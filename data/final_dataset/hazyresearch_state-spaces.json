{"home.repos.pwc.inspect_result.hazyresearch_state-spaces.None.example.S4Model.__init__": [[142, 178], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "range", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "example.S4Model.s4_layers.append", "example.S4Model.norms.append", "example.S4Model.dropouts.append", "src.models.sequence.ss.standalone.s4.S4", "torch.LayerNorm", "torch.LayerNorm", "torch.LayerNorm", "torch.LayerNorm", "torch.Dropout2d", "torch.Dropout2d", "torch.Dropout2d", "torch.Dropout2d"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "d_input", ",", "\n", "d_output", "=", "10", ",", "\n", "d_model", "=", "256", ",", "\n", "n_layers", "=", "4", ",", "\n", "dropout", "=", "0.2", ",", "\n", "prenorm", "=", "False", ",", "\n", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "prenorm", "=", "prenorm", "\n", "\n", "# Linear encoder (d_input = 1 for grayscale and 3 for RGB)", "\n", "self", ".", "encoder", "=", "nn", ".", "Linear", "(", "d_input", ",", "d_model", ")", "\n", "\n", "# Stack S4 layers as residual blocks", "\n", "self", ".", "s4_layers", "=", "nn", ".", "ModuleList", "(", ")", "\n", "self", ".", "norms", "=", "nn", ".", "ModuleList", "(", ")", "\n", "self", ".", "dropouts", "=", "nn", ".", "ModuleList", "(", ")", "\n", "for", "_", "in", "range", "(", "n_layers", ")", ":", "\n", "            ", "self", ".", "s4_layers", ".", "append", "(", "\n", "S4", "(", "\n", "d_model", "=", "d_model", ",", "\n", "l_max", "=", "1024", ",", "\n", "bidirectional", "=", "True", ",", "\n", "postact", "=", "'glu'", ",", "\n", "dropout", "=", "dropout", ",", "\n", "transposed", "=", "True", ",", "\n", ")", "\n", ")", "\n", "self", ".", "norms", ".", "append", "(", "nn", ".", "LayerNorm", "(", "d_model", ")", ")", "\n", "self", ".", "dropouts", ".", "append", "(", "nn", ".", "Dropout2d", "(", "dropout", ")", ")", "\n", "\n", "# Linear decoder", "\n", "", "self", ".", "decoder", "=", "nn", ".", "Linear", "(", "d_model", ",", "d_output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.None.example.S4Model.forward": [[179, 216], ["example.S4Model.encoder", "norm().transpose.transpose", "zip", "norm().transpose.transpose", "norm().transpose.mean", "example.S4Model.decoder", "layer", "dropout", "norm().transpose", "norm().transpose", "norm", "norm", "norm().transpose.transpose", "norm().transpose.transpose"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "\"\"\"\n        Input x is shape (B, L, d_input)\n        \"\"\"", "\n", "x", "=", "self", ".", "encoder", "(", "x", ")", "# (B, L, d_input) -> (B, L, d_model)", "\n", "\n", "x", "=", "x", ".", "transpose", "(", "-", "1", ",", "-", "2", ")", "# (B, L, d_model) -> (B, d_model, L)", "\n", "for", "layer", ",", "norm", ",", "dropout", "in", "zip", "(", "self", ".", "s4_layers", ",", "self", ".", "norms", ",", "self", ".", "dropouts", ")", ":", "\n", "# Each iteration of this loop will map (B, d_model, L) -> (B, d_model, L)", "\n", "\n", "            ", "z", "=", "x", "\n", "if", "self", ".", "prenorm", ":", "\n", "# Prenorm", "\n", "                ", "z", "=", "norm", "(", "z", ".", "transpose", "(", "-", "1", ",", "-", "2", ")", ")", ".", "transpose", "(", "-", "1", ",", "-", "2", ")", "\n", "\n", "# Apply S4 block: we ignore the state input and output", "\n", "", "z", ",", "_", "=", "layer", "(", "z", ")", "\n", "\n", "# Dropout on the output of the S4 block", "\n", "z", "=", "dropout", "(", "z", ")", "\n", "\n", "# Residual connection", "\n", "x", "=", "z", "+", "x", "\n", "\n", "if", "not", "self", ".", "prenorm", ":", "\n", "# Postnorm", "\n", "                ", "x", "=", "norm", "(", "x", ".", "transpose", "(", "-", "1", ",", "-", "2", ")", ")", ".", "transpose", "(", "-", "1", ",", "-", "2", ")", "\n", "\n", "", "", "x", "=", "x", ".", "transpose", "(", "-", "1", ",", "-", "2", ")", "\n", "\n", "# Pooling: average pooling over the sequence length", "\n", "x", "=", "x", ".", "mean", "(", "dim", "=", "1", ")", "\n", "\n", "# Decode the outputs", "\n", "x", "=", "self", ".", "decoder", "(", "x", ")", "# (B, d_model) -> (B, d_output)", "\n", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.None.example.split_train_val": [[68, 76], ["int", "torch.utils.data.random_split", "torch.utils.data.random_split", "torch.utils.data.random_split", "torch.utils.data.random_split", "len", "torch.Generator().manual_seed", "torch.Generator().manual_seed", "torch.Generator().manual_seed", "torch.Generator().manual_seed", "len", "torch.Generator", "torch.Generator", "torch.Generator", "torch.Generator"], "function", ["None"], ["def", "split_train_val", "(", "train", ",", "val_split", ")", ":", "\n", "    ", "train_len", "=", "int", "(", "len", "(", "train", ")", "*", "(", "1.0", "-", "val_split", ")", ")", "\n", "train", ",", "val", "=", "torch", ".", "utils", ".", "data", ".", "random_split", "(", "\n", "train", ",", "\n", "(", "train_len", ",", "len", "(", "train", ")", "-", "train_len", ")", ",", "\n", "generator", "=", "torch", ".", "Generator", "(", ")", ".", "manual_seed", "(", "42", ")", ",", "\n", ")", "\n", "return", "train", ",", "val", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.None.example.setup_optimizer": [[242, 290], ["list", "torch.AdamW", "torch.optim.lr_scheduler.ReduceLROnPlateau", "torch.optim.lr_scheduler.ReduceLROnPlateau", "torch.optim.lr_scheduler.ReduceLROnPlateau", "torch.optim.lr_scheduler.ReduceLROnPlateau", "sorted", "enumerate", "model.parameters", "getattr", "dict", "optim.AdamW.add_param_group", "set", "print", "hasattr", "set", "g.get", "hasattr", "frozenset", "getattr", "hp.keys", "hp.items", "group_hps.items", "len"], "function", ["None"], ["", "def", "setup_optimizer", "(", "model", ",", "lr", ",", "weight_decay", ",", "patience", ")", ":", "\n", "    ", "\"\"\"\n    S4 requires a specific optimizer setup.\n\n    The S4 layer (A, B, C, dt) parameters typically \n    require a smaller learning rate (typically 0.001), with no weight decay. \n\n    The rest of the model can be trained with a higher learning rate (e.g. 0.004, 0.01) \n    and weight decay (if desired).\n    \"\"\"", "\n", "\n", "# All parameters in the model", "\n", "all_parameters", "=", "list", "(", "model", ".", "parameters", "(", ")", ")", "\n", "\n", "# General parameters don't contain the special _optim key", "\n", "params", "=", "[", "p", "for", "p", "in", "all_parameters", "if", "not", "hasattr", "(", "p", ",", "\"_optim\"", ")", "]", "\n", "\n", "# Create an optimizer with the general parameters", "\n", "optimizer", "=", "optim", ".", "AdamW", "(", "\n", "params", ",", "\n", "lr", "=", "lr", ",", "\n", "weight_decay", "=", "weight_decay", ",", "\n", ")", "\n", "\n", "# Add parameters with special hyperparameters", "\n", "hps", "=", "[", "getattr", "(", "p", ",", "\"_optim\"", ")", "for", "p", "in", "all_parameters", "if", "hasattr", "(", "p", ",", "\"_optim\"", ")", "]", "\n", "hps", "=", "[", "\n", "dict", "(", "s", ")", "for", "s", "in", "set", "(", "frozenset", "(", "hp", ".", "items", "(", ")", ")", "for", "hp", "in", "hps", ")", "\n", "]", "# Unique dicts", "\n", "for", "hp", "in", "hps", ":", "\n", "        ", "params", "=", "[", "p", "for", "p", "in", "all_parameters", "if", "getattr", "(", "p", ",", "\"_optim\"", ",", "None", ")", "==", "hp", "]", "\n", "optimizer", ".", "add_param_group", "(", "\n", "{", "\"params\"", ":", "params", ",", "**", "hp", "}", "\n", ")", "\n", "\n", "# Create a lr scheduler", "\n", "", "scheduler", "=", "torch", ".", "optim", ".", "lr_scheduler", ".", "ReduceLROnPlateau", "(", "optimizer", ",", "patience", "=", "patience", ",", "factor", "=", "0.2", ")", "\n", "\n", "# Print optimizer info ", "\n", "keys", "=", "sorted", "(", "set", "(", "[", "k", "for", "hp", "in", "hps", "for", "k", "in", "hp", ".", "keys", "(", ")", "]", ")", ")", "\n", "for", "i", ",", "g", "in", "enumerate", "(", "optimizer", ".", "param_groups", ")", ":", "\n", "        ", "group_hps", "=", "{", "k", ":", "g", ".", "get", "(", "k", ",", "None", ")", "for", "k", "in", "keys", "}", "\n", "print", "(", "' | '", ".", "join", "(", "[", "\n", "f\"Optimizer group {i}\"", ",", "\n", "f\"{len(g['params'])} tensors\"", ",", "\n", "]", "+", "[", "f\"{k} {v}\"", "for", "k", ",", "v", "in", "group_hps", ".", "items", "(", ")", "]", ")", ")", "\n", "\n", "", "return", "optimizer", ",", "scheduler", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.None.example.train": [[301, 323], ["model.train", "tqdm.auto.tqdm", "enumerate", "optimizer.zero_grad", "model", "criterion", "criterion.backward", "optimizer.step", "criterion.item", "model.max", "targets.size", "predicted.eq().sum().item", "tqdm.auto.tqdm.set_description", "inputs.to", "targets.to", "predicted.eq().sum", "len", "predicted.eq"], "function", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.sc09_classifier.train_speech_commands.train", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.cauchy.cauchy.CauchyMultiplySymmetric.backward", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.sashimi.sashimi.Sashimi.step"], ["def", "train", "(", ")", ":", "\n", "    ", "model", ".", "train", "(", ")", "\n", "train_loss", "=", "0", "\n", "correct", "=", "0", "\n", "total", "=", "0", "\n", "pbar", "=", "tqdm", "(", "enumerate", "(", "trainloader", ")", ")", "\n", "for", "batch_idx", ",", "(", "inputs", ",", "targets", ")", "in", "pbar", ":", "\n", "        ", "inputs", ",", "targets", "=", "inputs", ".", "to", "(", "device", ")", ",", "targets", ".", "to", "(", "device", ")", "\n", "optimizer", ".", "zero_grad", "(", ")", "\n", "outputs", "=", "model", "(", "inputs", ")", "\n", "loss", "=", "criterion", "(", "outputs", ",", "targets", ")", "\n", "loss", ".", "backward", "(", ")", "\n", "optimizer", ".", "step", "(", ")", "\n", "\n", "train_loss", "+=", "loss", ".", "item", "(", ")", "\n", "_", ",", "predicted", "=", "outputs", ".", "max", "(", "1", ")", "\n", "total", "+=", "targets", ".", "size", "(", "0", ")", "\n", "correct", "+=", "predicted", ".", "eq", "(", "targets", ")", ".", "sum", "(", ")", ".", "item", "(", ")", "\n", "\n", "pbar", ".", "set_description", "(", "\n", "'Batch Idx: (%d/%d) | Loss: %.3f | Acc: %.3f%% (%d/%d)'", "%", "\n", "(", "batch_idx", ",", "len", "(", "trainloader", ")", ",", "train_loss", "/", "(", "batch_idx", "+", "1", ")", ",", "100.", "*", "correct", "/", "total", ",", "correct", ",", "total", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.None.example.eval": [[326, 364], ["model.eval", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "tqdm.auto.tqdm", "enumerate", "model", "criterion", "criterion.item", "model.max", "targets.size", "predicted.eq().sum().item", "tqdm.auto.tqdm.set_description", "torch.save", "torch.save", "torch.save", "torch.save", "inputs.to", "targets.to", "model.state_dict", "os.path.isdir", "os.mkdir", "predicted.eq().sum", "len", "predicted.eq"], "function", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.None.example.eval"], ["", "", "def", "eval", "(", "epoch", ",", "dataloader", ",", "checkpoint", "=", "False", ")", ":", "\n", "    ", "global", "best_acc", "\n", "model", ".", "eval", "(", ")", "\n", "eval_loss", "=", "0", "\n", "correct", "=", "0", "\n", "total", "=", "0", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "        ", "pbar", "=", "tqdm", "(", "enumerate", "(", "dataloader", ")", ")", "\n", "for", "batch_idx", ",", "(", "inputs", ",", "targets", ")", "in", "pbar", ":", "\n", "            ", "inputs", ",", "targets", "=", "inputs", ".", "to", "(", "device", ")", ",", "targets", ".", "to", "(", "device", ")", "\n", "outputs", "=", "model", "(", "inputs", ")", "\n", "loss", "=", "criterion", "(", "outputs", ",", "targets", ")", "\n", "\n", "eval_loss", "+=", "loss", ".", "item", "(", ")", "\n", "_", ",", "predicted", "=", "outputs", ".", "max", "(", "1", ")", "\n", "total", "+=", "targets", ".", "size", "(", "0", ")", "\n", "correct", "+=", "predicted", ".", "eq", "(", "targets", ")", ".", "sum", "(", ")", ".", "item", "(", ")", "\n", "\n", "pbar", ".", "set_description", "(", "\n", "'Batch Idx: (%d/%d) | Loss: %.3f | Acc: %.3f%% (%d/%d)'", "%", "\n", "(", "batch_idx", ",", "len", "(", "dataloader", ")", ",", "eval_loss", "/", "(", "batch_idx", "+", "1", ")", ",", "100.", "*", "correct", "/", "total", ",", "correct", ",", "total", ")", "\n", ")", "\n", "\n", "# Save checkpoint.", "\n", "", "", "if", "checkpoint", ":", "\n", "        ", "acc", "=", "100.", "*", "correct", "/", "total", "\n", "if", "acc", ">", "best_acc", ":", "\n", "            ", "state", "=", "{", "\n", "'model'", ":", "model", ".", "state_dict", "(", ")", ",", "\n", "'acc'", ":", "acc", ",", "\n", "'epoch'", ":", "epoch", ",", "\n", "}", "\n", "if", "not", "os", ".", "path", ".", "isdir", "(", "'checkpoint'", ")", ":", "\n", "                ", "os", ".", "mkdir", "(", "'checkpoint'", ")", "\n", "", "torch", ".", "save", "(", "state", ",", "'./checkpoint/ckpt.pth'", ")", "\n", "best_acc", "=", "acc", "\n", "\n", "", "return", "acc", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.utils.config.is_list": [[8, 10], ["isinstance", "isinstance"], "function", ["None"], ["def", "is_list", "(", "x", ")", ":", "\n", "    ", "return", "isinstance", "(", "x", ",", "Sequence", ")", "and", "not", "isinstance", "(", "x", ",", "str", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.utils.config.is_dict": [[12, 14], ["isinstance"], "function", ["None"], ["", "def", "is_dict", "(", "x", ")", ":", "\n", "    ", "return", "isinstance", "(", "x", ",", "Mapping", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.utils.config.to_dict": [[16, 30], ["config.is_list", "config.is_dict", "dict", "enumerate", "config.to_dict", "x.items"], "function", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.utils.config.is_list", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.utils.config.is_dict", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.utils.config.to_dict"], ["", "def", "to_dict", "(", "x", ",", "recursive", "=", "True", ")", ":", "\n", "    ", "\"\"\"Convert Sequence or Mapping object to dict\n\n    lists get converted to {0: x[0], 1: x[1], ...}\n    \"\"\"", "\n", "if", "is_list", "(", "x", ")", ":", "\n", "        ", "x", "=", "{", "i", ":", "v", "for", "i", ",", "v", "in", "enumerate", "(", "x", ")", "}", "\n", "", "if", "is_dict", "(", "x", ")", ":", "\n", "        ", "if", "recursive", ":", "\n", "            ", "return", "{", "k", ":", "to_dict", "(", "v", ",", "recursive", "=", "recursive", ")", "for", "k", ",", "v", "in", "x", ".", "items", "(", ")", "}", "\n", "", "else", ":", "\n", "            ", "return", "dict", "(", "x", ")", "\n", "", "", "else", ":", "\n", "        ", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.utils.config.to_list": [[32, 49], ["config.is_list", "list", "config.to_list"], "function", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.utils.config.is_list", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.utils.config.to_list"], ["", "", "def", "to_list", "(", "x", ",", "recursive", "=", "False", ")", ":", "\n", "    ", "\"\"\"Convert an object to list.\n\n    If Sequence (e.g. list, tuple, Listconfig): just return it\n\n    Special case: If non-recursive and not a list, wrap in list\n    \"\"\"", "\n", "if", "is_list", "(", "x", ")", ":", "\n", "        ", "if", "recursive", ":", "\n", "            ", "return", "[", "to_list", "(", "_x", ")", "for", "_x", "in", "x", "]", "\n", "", "else", ":", "\n", "            ", "return", "list", "(", "x", ")", "\n", "", "", "else", ":", "\n", "        ", "if", "recursive", ":", "\n", "            ", "return", "x", "\n", "", "else", ":", "\n", "            ", "return", "[", "x", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.utils.config.extract_attrs_from_obj": [[51, 56], ["getattr", "len"], "function", ["None"], ["", "", "", "def", "extract_attrs_from_obj", "(", "obj", ",", "*", "attrs", ")", ":", "\n", "    ", "if", "obj", "is", "None", ":", "\n", "        ", "assert", "len", "(", "attrs", ")", "==", "0", "\n", "return", "[", "]", "\n", "", "return", "[", "getattr", "(", "obj", ",", "attr", ",", "None", ")", "for", "attr", "in", "attrs", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.utils.config.instantiate": [[58, 100], ["isinstance", "isinstance", "functools.partial", "config.pop", "hydra.utils.get_method", "isinstance", "wrap", "functools.partial.", "NotImplementedError"], "function", ["None"], ["", "def", "instantiate", "(", "registry", ",", "config", ",", "*", "args", ",", "partial", "=", "False", ",", "wrap", "=", "None", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"\n    registry: Dictionary mapping names to functions or target paths (e.g. {'model': 'models.SequenceModel'})\n    config: Dictionary with a '_name_' key indicating which element of the registry to grab, and kwargs to be passed into the target constructor\n    wrap: wrap the target class (e.g. ema optimizer or tasks.wrap)\n    *args, **kwargs: additional arguments to override the config to pass into the target constructor\n    \"\"\"", "\n", "\n", "# Case 1: no config", "\n", "if", "config", "is", "None", ":", "\n", "        ", "return", "None", "\n", "# Case 2a: string means _name_ was overloaded", "\n", "", "if", "isinstance", "(", "config", ",", "str", ")", ":", "\n", "        ", "_name_", "=", "None", "\n", "_target_", "=", "registry", "[", "config", "]", "\n", "config", "=", "{", "}", "\n", "# Case 2b: grab the desired callable from name", "\n", "", "else", ":", "\n", "        ", "_name_", "=", "config", ".", "pop", "(", "\"_name_\"", ")", "\n", "_target_", "=", "registry", "[", "_name_", "]", "\n", "\n", "# Retrieve the right constructor automatically based on type", "\n", "", "if", "isinstance", "(", "_target_", ",", "str", ")", ":", "\n", "        ", "fn", "=", "hydra", ".", "utils", ".", "get_method", "(", "path", "=", "_target_", ")", "\n", "", "elif", "isinstance", "(", "_target_", ",", "Callable", ")", ":", "\n", "        ", "fn", "=", "_target_", "\n", "", "else", ":", "\n", "        ", "raise", "NotImplementedError", "(", "\"instantiate target must be string or callable\"", ")", "\n", "\n", "# Instantiate object", "\n", "", "if", "wrap", "is", "not", "None", ":", "\n", "        ", "fn", "=", "wrap", "(", "fn", ")", "\n", "", "obj", "=", "functools", ".", "partial", "(", "fn", ",", "*", "args", ",", "**", "config", ",", "**", "kwargs", ")", "\n", "\n", "# Restore _name_", "\n", "if", "_name_", "is", "not", "None", ":", "\n", "        ", "config", "[", "\"_name_\"", "]", "=", "_name_", "\n", "\n", "", "if", "partial", ":", "\n", "        ", "return", "obj", "\n", "", "else", ":", "\n", "        ", "return", "obj", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.utils.config.get_class": [[102, 104], ["hydra.utils.get_class"], "function", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.utils.config.get_class"], ["", "", "def", "get_class", "(", "registry", ",", "_name_", ")", ":", "\n", "    ", "return", "hydra", ".", "utils", ".", "get_class", "(", "path", "=", "registry", "[", "_name_", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.utils.config.omegaconf_filter_keys": [[106, 120], ["config.is_list", "omegaconf.ListConfig", "config.is_dict", "omegaconf.DictConfig", "config.omegaconf_filter_keys", "config.omegaconf_filter_keys", "d.items", "fn"], "function", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.utils.config.is_list", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.utils.config.is_dict", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.utils.config.omegaconf_filter_keys", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.utils.config.omegaconf_filter_keys"], ["", "def", "omegaconf_filter_keys", "(", "d", ",", "fn", "=", "None", ")", ":", "\n", "    ", "\"\"\"Only keep keys where fn(key) is True. Support nested DictConfig.\n    # TODO can make this inplace?\n    \"\"\"", "\n", "if", "fn", "is", "None", ":", "\n", "        ", "fn", "=", "lambda", "_", ":", "True", "\n", "", "if", "is_list", "(", "d", ")", ":", "\n", "        ", "return", "ListConfig", "(", "[", "omegaconf_filter_keys", "(", "v", ",", "fn", ")", "for", "v", "in", "d", "]", ")", "\n", "", "elif", "is_dict", "(", "d", ")", ":", "\n", "        ", "return", "DictConfig", "(", "\n", "{", "k", ":", "omegaconf_filter_keys", "(", "v", ",", "fn", ")", "for", "k", ",", "v", "in", "d", ".", "items", "(", ")", "if", "fn", "(", "k", ")", "}", "\n", ")", "\n", "", "else", ":", "\n", "        ", "return", "d", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.utils.config.instantiate_name": [[127, 138], ["isinstance", "config.pop", "hydra.utils.instantiate", "hydra.utils.instantiate"], "function", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.tasks.decoders.instantiate", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.tasks.decoders.instantiate"], ["def", "instantiate_name", "(", "registry", ",", "config", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "    ", "if", "config", "is", "None", ":", "\n", "        ", "return", "None", "\n", "", "if", "isinstance", "(", "config", ",", "str", ")", ":", "\n", "        ", "obj", "=", "hydra", ".", "utils", ".", "instantiate", "(", "{", "\"_target_\"", ":", "config", "}", ",", "*", "args", ",", "**", "kwargs", ")", "\n", "return", "obj", "\n", "", "name", "=", "config", ".", "pop", "(", "\"_name_\"", ")", "\n", "config", "[", "\"_target_\"", "]", "=", "registry", "[", "name", "]", "\n", "obj", "=", "hydra", ".", "utils", ".", "instantiate", "(", "config", ",", "*", "args", ",", "**", "kwargs", ")", "\n", "config", "[", "\"_name_\"", "]", "=", "name", "\n", "return", "obj", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.utils.config.instantiate_cls": [[140, 151], ["isinstance", "config.pop", "cls"], "function", ["None"], ["", "def", "instantiate_cls", "(", "registry", ",", "config", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "    ", "if", "config", "is", "None", ":", "\n", "        ", "return", "None", "\n", "", "if", "isinstance", "(", "config", ",", "str", ")", ":", "\n", "        ", "obj", "=", "registry", "[", "config", "]", "(", "*", "args", ",", "**", "kwargs", ")", "\n", "return", "obj", "\n", "", "name", "=", "config", ".", "pop", "(", "\"_name_\"", ")", "\n", "cls", "=", "registry", "[", "name", "]", "\n", "obj", "=", "cls", "(", "*", "args", ",", "**", "config", ",", "**", "kwargs", ")", "\n", "config", "[", "\"_name_\"", "]", "=", "name", "\n", "return", "obj", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.utils.config.instantiate_partial": [[154, 165], ["isinstance", "config.pop", "functools.partial", "functools.partial"], "function", ["None"], ["", "def", "instantiate_partial", "(", "registry", ",", "config", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "    ", "if", "config", "is", "None", ":", "\n", "        ", "return", "None", "\n", "", "if", "isinstance", "(", "config", ",", "str", ")", ":", "\n", "        ", "obj", "=", "functools", ".", "partial", "(", "registry", "[", "config", "]", ",", "*", "args", ",", "**", "kwargs", ")", "\n", "return", "obj", "\n", "", "name", "=", "config", ".", "pop", "(", "\"_name_\"", ")", "\n", "fn", "=", "registry", "[", "name", "]", "\n", "obj", "=", "functools", ".", "partial", "(", "fn", ",", "*", "args", ",", "**", "config", ",", "**", "kwargs", ")", "\n", "config", "[", "\"_name_\"", "]", "=", "name", "\n", "return", "obj", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.utils.config.dictconfig_filter_keys": [[170, 181], ["omegaconf.DictConfig", "isinstance", "config.dictconfig_filter_keys", "d.items", "fn"], "function", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.utils.config.dictconfig_filter_keys"], ["def", "dictconfig_filter_keys", "(", "d", ":", "DictConfig", ",", "fn", ":", "Optional", "[", "Callable", "]", "=", "None", ")", "->", "DictConfig", ":", "\n", "    ", "\"\"\"Only keep keys where fn(key) is True. Support nested DictConfig.\n    # TODO can make this inplace?\n    \"\"\"", "\n", "if", "fn", "is", "None", ":", "\n", "        ", "fn", "=", "lambda", "_", ":", "True", "\n", "", "return", "DictConfig", "(", "\n", "{", "\n", "k", ":", "dictconfig_filter_keys", "(", "v", ",", "fn", ")", "if", "isinstance", "(", "v", ",", "DictConfig", ")", "else", "v", "\n", "for", "k", ",", "v", "in", "d", ".", "items", "(", ")", "\n", "if", "fn", "(", "k", ")", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.utils.config.remove_postfix": [[186, 190], ["text.endswith", "len"], "function", ["None"], ["", "def", "remove_postfix", "(", "text", ",", "postfix", ")", ":", "\n", "    ", "if", "text", ".", "endswith", "(", "postfix", ")", ":", "\n", "        ", "return", "text", "[", ":", "-", "len", "(", "postfix", ")", "]", "\n", "", "return", "text", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.utils.config.to_scalar": [[193, 195], ["isinstance", "x.item"], "function", ["None"], ["", "def", "to_scalar", "(", "x", ")", ":", "\n", "    ", "return", "x", ".", "item", "(", ")", "if", "isinstance", "(", "x", ",", "torch", ".", "Tensor", ")", "else", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.utils.config.dictconfig_to_munch": [[197, 205], ["Munch", "isinstance", "config.dictconfig_to_munch", "d.items"], "function", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.utils.config.dictconfig_to_munch"], ["", "def", "dictconfig_to_munch", "(", "d", ")", ":", "\n", "    ", "\"\"\"Convert object of type OmegaConf to Munch so Wandb can log properly\n    Support nested dictionary.\n    \"\"\"", "\n", "return", "Munch", "(", "\n", "{", "\n", "k", ":", "dictconfig_to_munch", "(", "v", ")", "if", "isinstance", "(", "v", ",", "DictConfig", ")", "else", "v", "\n", "for", "k", ",", "v", "in", "d", ".", "items", "(", ")", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.utils.config.munch_to_dictconfig": [[209, 212], ["omegaconf.DictConfig", "isinstance", "config.munch_to_dictconfig", "m.items"], "function", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.utils.config.munch_to_dictconfig"], ["", "def", "munch_to_dictconfig", "(", "m", ")", ":", "\n", "    ", "return", "DictConfig", "(", "\n", "{", "k", ":", "munch_to_dictconfig", "(", "v", ")", "if", "isinstance", "(", "v", ",", "Munch", ")", "else", "v", "for", "k", ",", "v", "in", "m", ".", "items", "(", ")", "}", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.utils.distributed.init_distributed": [[21, 36], ["int", "os.environ.get", "torch.distributed.init_process_group", "torch.distributed.is_initialized"], "function", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.ss.lssl.Platypus.is_initialized"], ["def", "init_distributed", "(", "cuda", ")", ":", "\n", "    ", "\"\"\"\n    Initializes distributed backend.\n\n    :param cuda: (bool) if True initializes nccl backend, if False initializes\n        gloo backend\n    \"\"\"", "\n", "world_size", "=", "int", "(", "os", ".", "environ", ".", "get", "(", "'WORLD_SIZE'", ",", "1", ")", ")", "\n", "distributed", "=", "(", "world_size", ">", "1", ")", "\n", "if", "distributed", ":", "\n", "        ", "backend", "=", "'nccl'", "if", "cuda", "else", "'gloo'", "\n", "torch", ".", "distributed", ".", "init_process_group", "(", "backend", "=", "backend", ",", "\n", "init_method", "=", "'env://'", ")", "\n", "assert", "torch", ".", "distributed", ".", "is_initialized", "(", ")", "\n", "", "return", "distributed", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.utils.distributed.barrier": [[38, 44], ["torch.distributed.is_available", "torch.distributed.is_initialized", "torch.distributed.barrier"], "function", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.ss.lssl.Platypus.is_initialized", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.utils.distributed.barrier"], ["", "def", "barrier", "(", ")", ":", "\n", "    ", "\"\"\"\n    Call torch.distributed.barrier() if distritubed is in use\n    \"\"\"", "\n", "if", "torch", ".", "distributed", ".", "is_available", "(", ")", "and", "torch", ".", "distributed", ".", "is_initialized", "(", ")", ":", "\n", "        ", "torch", ".", "distributed", ".", "barrier", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.utils.distributed.get_rank": [[46, 55], ["torch.distributed.is_available", "torch.distributed.is_initialized", "torch.distributed.get_rank"], "function", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.ss.lssl.Platypus.is_initialized", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.utils.distributed.get_rank"], ["", "", "def", "get_rank", "(", ")", ":", "\n", "    ", "\"\"\"\n    Gets distributed rank or returns zero if distributed is not initialized.\n    \"\"\"", "\n", "if", "torch", ".", "distributed", ".", "is_available", "(", ")", "and", "torch", ".", "distributed", ".", "is_initialized", "(", ")", ":", "\n", "        ", "rank", "=", "torch", ".", "distributed", ".", "get_rank", "(", ")", "\n", "", "else", ":", "\n", "        ", "rank", "=", "0", "\n", "", "return", "rank", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.utils.distributed.get_world_size": [[57, 67], ["torch.distributed.is_available", "torch.distributed.is_initialized", "torch.distributed.get_world_size"], "function", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.ss.lssl.Platypus.is_initialized", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.utils.distributed.get_world_size"], ["", "def", "get_world_size", "(", ")", ":", "\n", "    ", "\"\"\"\n    Gets total number of distributed workers or returns one if distributed is\n    not initialized.\n    \"\"\"", "\n", "if", "torch", ".", "distributed", ".", "is_available", "(", ")", "and", "torch", ".", "distributed", ".", "is_initialized", "(", ")", ":", "\n", "        ", "world_size", "=", "torch", ".", "distributed", ".", "get_world_size", "(", ")", "\n", "", "else", ":", "\n", "        ", "world_size", "=", "1", "\n", "", "return", "world_size", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.utils.distributed.all_reduce_item": [[69, 101], ["torch.distributed.is_available", "torch.distributed.is_initialized", "torch.distributed.get_backend", "torch.tensor", "torch.distributed.all_reduce", "torch.tensor.item", "torch.device", "distributed.get_world_size", "torch.device", "RuntimeError", "RuntimeError"], "function", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.ss.lssl.Platypus.is_initialized", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.utils.distributed.get_world_size"], ["", "def", "all_reduce_item", "(", "value", ",", "op", "=", "'sum'", ")", ":", "\n", "    ", "\"\"\"\n    All-reduces single scalar value if distributed is in use\n    \"\"\"", "\n", "if", "torch", ".", "distributed", ".", "is_available", "(", ")", "and", "torch", ".", "distributed", ".", "is_initialized", "(", ")", ":", "\n", "        ", "if", "op", "==", "'sum'", "or", "op", "==", "'mean'", ":", "\n", "            ", "dop", "=", "torch", ".", "distributed", ".", "ReduceOp", ".", "SUM", "\n", "", "elif", "op", "==", "'min'", ":", "\n", "            ", "dop", "=", "torch", ".", "distributed", ".", "ReduceOp", ".", "MIN", "\n", "", "elif", "op", "==", "'max'", ":", "\n", "            ", "dop", "=", "torch", ".", "distributed", ".", "ReduceOp", ".", "MAX", "\n", "", "elif", "op", "==", "'product'", ":", "\n", "            ", "dop", "=", "torch", ".", "distributed", ".", "ReduceOp", ".", "PRODUCT", "\n", "", "else", ":", "\n", "            ", "raise", "RuntimeError", "(", "'Unsupported reduce op'", ")", "\n", "\n", "", "backend", "=", "torch", ".", "distributed", ".", "get_backend", "(", ")", "\n", "if", "backend", "==", "torch", ".", "distributed", ".", "Backend", ".", "NCCL", ":", "\n", "            ", "device", "=", "torch", ".", "device", "(", "'cuda'", ")", "\n", "", "elif", "backend", "==", "torch", ".", "distributed", ".", "Backend", ".", "GLOO", ":", "\n", "            ", "device", "=", "torch", ".", "device", "(", "'cpu'", ")", "\n", "", "else", ":", "\n", "            ", "raise", "RuntimeError", "(", "'Unsupported distributed backend'", ")", "\n", "\n", "", "tensor", "=", "torch", ".", "tensor", "(", "value", ",", "device", "=", "device", ")", "\n", "torch", ".", "distributed", ".", "all_reduce", "(", "tensor", ",", "dop", ")", "\n", "if", "op", "==", "'mean'", ":", "\n", "            ", "tensor", "/=", "get_world_size", "(", ")", "\n", "", "ret", "=", "tensor", ".", "item", "(", ")", "\n", "", "else", ":", "\n", "        ", "ret", "=", "value", "\n", "", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.utils.distributed.all_reduce_tensor": [[103, 135], ["torch.distributed.is_available", "torch.distributed.is_initialized", "torch.distributed.get_backend", "torch.distributed.all_reduce", "torch.device", "distributed.get_world_size", "torch.device", "RuntimeError", "RuntimeError"], "function", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.ss.lssl.Platypus.is_initialized", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.utils.distributed.get_world_size"], ["", "def", "all_reduce_tensor", "(", "value", ",", "op", "=", "'sum'", ")", ":", "\n", "    ", "\"\"\"\n    All-reduces single scalar value if distributed is in use\n    \"\"\"", "\n", "if", "torch", ".", "distributed", ".", "is_available", "(", ")", "and", "torch", ".", "distributed", ".", "is_initialized", "(", ")", ":", "\n", "        ", "if", "op", "==", "'sum'", "or", "op", "==", "'mean'", ":", "\n", "            ", "dop", "=", "torch", ".", "distributed", ".", "ReduceOp", ".", "SUM", "\n", "", "elif", "op", "==", "'min'", ":", "\n", "            ", "dop", "=", "torch", ".", "distributed", ".", "ReduceOp", ".", "MIN", "\n", "", "elif", "op", "==", "'max'", ":", "\n", "            ", "dop", "=", "torch", ".", "distributed", ".", "ReduceOp", ".", "MAX", "\n", "", "elif", "op", "==", "'product'", ":", "\n", "            ", "dop", "=", "torch", ".", "distributed", ".", "ReduceOp", ".", "PRODUCT", "\n", "", "else", ":", "\n", "            ", "raise", "RuntimeError", "(", "'Unsupported reduce op'", ")", "\n", "\n", "", "backend", "=", "torch", ".", "distributed", ".", "get_backend", "(", ")", "\n", "if", "backend", "==", "torch", ".", "distributed", ".", "Backend", ".", "NCCL", ":", "\n", "            ", "device", "=", "torch", ".", "device", "(", "'cuda'", ")", "\n", "", "elif", "backend", "==", "torch", ".", "distributed", ".", "Backend", ".", "GLOO", ":", "\n", "            ", "device", "=", "torch", ".", "device", "(", "'cpu'", ")", "\n", "", "else", ":", "\n", "            ", "raise", "RuntimeError", "(", "'Unsupported distributed backend'", ")", "\n", "\n", "", "tensor", "=", "value", "\n", "torch", ".", "distributed", ".", "all_reduce", "(", "tensor", ",", "dop", ")", "\n", "if", "op", "==", "'mean'", ":", "\n", "            ", "tensor", "/=", "get_world_size", "(", ")", "\n", "", "ret", "=", "tensor", "\n", "", "else", ":", "\n", "        ", "ret", "=", "value", "\n", "", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.utils.distributed.sync_workers": [[137, 145], ["distributed.get_rank", "distributed.barrier"], "function", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.utils.distributed.get_rank", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.utils.distributed.barrier"], ["", "@", "contextmanager", "\n", "def", "sync_workers", "(", ")", ":", "\n", "    ", "\"\"\"\n    Yields distributed rank and synchronizes all workers on exit.\n    \"\"\"", "\n", "rank", "=", "get_rank", "(", ")", "\n", "yield", "rank", "\n", "barrier", "(", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.utils.train.LoggingContext.__init__": [[19, 24], ["None"], "methods", ["None"], ["\n", "log", "=", "src", ".", "utils", ".", "train", ".", "get_logger", "(", "__name__", ")", "\n", "\n", "\n", "class", "SequenceLightningModule", "(", "pl", ".", "LightningModule", ")", ":", "\n", "    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.utils.train.LoggingContext.__enter__": [[25, 31], ["train.LoggingContext.logger.setLevel", "train.LoggingContext.logger.addHandler"], "methods", ["None"], ["# Disable profiling executor. This reduces memory and increases speed.", "\n", "        ", "try", ":", "\n", "            ", "torch", ".", "_C", ".", "_jit_set_profiling_executor", "(", "False", ")", "\n", "torch", ".", "_C", ".", "_jit_set_profiling_mode", "(", "False", ")", "\n", "", "except", "AttributeError", ":", "\n", "            ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.utils.train.LoggingContext.__exit__": [[32, 39], ["train.LoggingContext.logger.setLevel", "train.LoggingContext.logger.removeHandler", "train.LoggingContext.handler.close"], "methods", ["None"], ["", "super", "(", ")", ".", "__init__", "(", ")", "\n", "# Passing in config expands it one level, so can access by self.hparams.train instead of self.hparams.config.train", "\n", "self", ".", "save_hyperparameters", "(", "config", ",", "logger", "=", "False", ")", "\n", "\n", "self", ".", "dataset", "=", "SequenceDataset", ".", "registry", "[", "self", ".", "hparams", ".", "dataset", ".", "_name_", "]", "(", "\n", "**", "{", "\n", "# Arguments for configuring dataloader when using TBPTT", "\n", "\"tbptt\"", ":", "self", ".", "hparams", ".", "train", ".", "state", ".", "mode", "==", "'tbptt'", ",", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.utils.train.get_logger": [[42, 54], ["logging.getLogger", "logging.getLogger.setLevel", "setattr", "pytorch_lightning.utilities.rank_zero_only", "getattr"], "function", ["None"], ["# Dataset arguments", "\n", "**", "self", ".", "hparams", ".", "dataset", ",", "\n", "}", "\n", ")", "\n", "\n", "# Check hparams", "\n", "self", ".", "_check_config", "(", ")", "\n", "\n", "# PL has some bugs, so add hooks and make sure they're only called once", "\n", "self", ".", "_has_setup", "=", "False", "\n", "self", ".", "_has_on_post_move_to_device", "=", "False", "\n", "\n", "", "def", "setup", "(", "self", ",", "stage", "=", "None", ")", ":", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.utils.train.process_config": [[57, 102], ["train.get_logger", "omegaconf.OmegaConf.register_new_resolver", "src.utils.config.omegaconf_filter_keys", "omegaconf.OmegaConf.set_struct", "src.utils.config.omegaconf_filter_keys.get", "src.utils.config.omegaconf_filter_keys.get", "get_logger.info", "warnings.filterwarnings", "get_logger.info", "get_logger.info", "src.utils.config.omegaconf_filter_keys.trainer.get", "src.utils.config.omegaconf_filter_keys.loader.get", "src.utils.config.omegaconf_filter_keys.loader.get", "k.startswith"], "function", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.s4.get_logger", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.utils.config.omegaconf_filter_keys"], ["\n", "# We need to set up the model in setup() because for some reason when training with DDP, one GPU uses much more memory than the others", "\n", "# In order to not overwrite the model multiple times during different stages, we need this hack", "\n", "# TODO PL 1.5 seems to have an option to skip hooks to avoid this", "\n", "# https://github.com/PyTorchLightning/pytorch-lightning/issues/5410#issuecomment-762257024", "\n", "", "if", "self", ".", "_has_setup", ":", "\n", "            ", "return", "\n", "", "else", ":", "\n", "            ", "self", ".", "_has_setup", "=", "True", "\n", "\n", "# Convenience feature: if model specifies encoder, combine it with main encoder", "\n", "", "encoder_cfg", "=", "utils", ".", "to_list", "(", "self", ".", "hparams", ".", "encoder", ")", "+", "utils", ".", "to_list", "(", "\n", "self", ".", "hparams", ".", "model", ".", "pop", "(", "\"encoder\"", ",", "None", ")", "\n", ")", "\n", "decoder_cfg", "=", "utils", ".", "to_list", "(", "self", ".", "hparams", ".", "model", ".", "pop", "(", "\"decoder\"", ",", "None", ")", ")", "+", "utils", ".", "to_list", "(", "self", ".", "hparams", ".", "decoder", ")", "\n", "\n", "# Instantiate model", "\n", "self", ".", "model", "=", "utils", ".", "instantiate", "(", "registry", ".", "model", ",", "self", ".", "hparams", ".", "model", ")", "\n", "\n", "# Instantiate the task", "\n", "if", "\"task\"", "not", "in", "self", ".", "hparams", ":", "# TODO maybe don't need this?", "\n", "            ", "self", ".", "hparams", ".", "task", "=", "self", ".", "dataset", ".", "default_task", "\n", "", "self", ".", "task", "=", "task", "=", "utils", ".", "instantiate", "(", "\n", "tasks", ".", "registry", ",", "self", ".", "hparams", ".", "task", ",", "dataset", "=", "self", ".", "dataset", ",", "model", "=", "self", ".", "model", "\n", ")", "\n", "\n", "# Create encoders and decoders", "\n", "encoder", "=", "encoders", ".", "instantiate", "(", "\n", "encoder_cfg", ",", "dataset", "=", "self", ".", "dataset", ",", "model", "=", "self", ".", "model", "\n", ")", "\n", "decoder", "=", "decoders", ".", "instantiate", "(", "\n", "self", ".", "hparams", ".", "decoder", ",", "model", "=", "self", ".", "model", ",", "dataset", "=", "self", ".", "dataset", "\n", ")", "\n", "\n", "# Extract the modules so they show up in the top level parameter count", "\n", "self", ".", "encoder", "=", "U", ".", "TupleSequential", "(", "task", ".", "encoder", ",", "encoder", ")", "\n", "self", ".", "decoder", "=", "U", ".", "TupleSequential", "(", "decoder", ",", "task", ".", "decoder", ")", "\n", "self", ".", "loss", "=", "task", ".", "loss", "\n", "self", ".", "metrics", "=", "task", ".", "metrics", "\n", "\n", "# Handle state logic", "\n", "self", ".", "_initialize_state", "(", ")", "\n", "\n", "", "def", "_check_config", "(", "self", ")", ":", "\n", "        ", "assert", "self", ".", "hparams", ".", "train", ".", "state", ".", "mode", "in", "[", "None", ",", "\"none\"", ",", "\"null\"", ",", "\"reset\"", ",", "\"bptt\"", ",", "\"tbptt\"", "]", "\n", "assert", "(", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.utils.train.print_config": [[103, 143], ["rich.tree.Tree", "rich.tree.Tree", "config.keys", "rich.print", "rich.print", "rich.tree.Tree.add", "config.get", "str", "isinstance", "tree.add.add", "open", "rich.print", "rich.print", "omegaconf.OmegaConf.to_yaml", "rich.syntax.Syntax", "rich.syntax.Syntax"], "function", ["None"], ["(", "n", ":=", "self", ".", "hparams", ".", "train", ".", "state", ".", "n_context", ")", "is", "None", "\n", "or", "isinstance", "(", "n", ",", "int", ")", "\n", "and", "n", ">=", "0", "\n", ")", "\n", "assert", "(", "\n", "(", "n", ":=", "self", ".", "hparams", ".", "train", ".", "state", ".", "n_context_eval", ")", "is", "None", "\n", "or", "isinstance", "(", "n", ",", "int", ")", "\n", "and", "n", ">=", "0", "\n", ")", "\n", "assert", "(", "\n", "not", "(", "self", ".", "hparams", ".", "train", ".", "state", ".", "mode", "==", "'tbptt'", ")", "or", "\n", "(", "self", ".", "hparams", ".", "train", ".", "state", ".", "chunk_len", "is", "not", "None", "and", "\n", "self", ".", "hparams", ".", "train", ".", "state", ".", "overlap_len", "is", "not", "None", ")", "\n", ")", ",", "\"If tbptt is True, chunk_len and overlap_len must be specified.\"", "\n", "\n", "", "def", "_initialize_state", "(", "self", ")", ":", "\n", "        ", "self", ".", "_state", "=", "None", "\n", "self", ".", "_memory_chunks", "=", "[", "]", "\n", "\n", "", "def", "_reset_state", "(", "self", ",", "batch", ",", "device", "=", "None", ")", ":", "\n", "        ", "device", "=", "device", "or", "batch", "[", "0", "]", ".", "device", "\n", "self", ".", "_state", "=", "self", ".", "model", ".", "default_state", "(", "*", "batch", "[", "0", "]", ".", "shape", "[", ":", "1", "]", ",", "device", "=", "device", ")", "\n", "\n", "", "def", "_detach_state", "(", "self", ",", "state", ")", ":", "\n", "        ", "if", "isinstance", "(", "state", ",", "torch", ".", "Tensor", ")", ":", "\n", "            ", "return", "state", ".", "detach", "(", ")", "\n", "", "elif", "isinstance", "(", "state", ",", "tuple", ")", ":", "\n", "            ", "return", "tuple", "(", "self", ".", "_detach_state", "(", "s", ")", "for", "s", "in", "state", ")", "\n", "", "elif", "isinstance", "(", "state", ",", "list", ")", ":", "\n", "            ", "return", "[", "self", ".", "_detach_state", "(", "s", ")", "for", "s", "in", "state", "]", "\n", "", "elif", "isinstance", "(", "state", ",", "dict", ")", ":", "\n", "            ", "return", "{", "k", ":", "self", ".", "_detach_state", "(", "v", ")", "for", "k", ",", "v", "in", "state", ".", "items", "(", ")", "}", "\n", "", "elif", "state", "is", "None", ":", "\n", "            ", "return", "None", "\n", "", "else", ":", "\n", "            ", "raise", "NotImplementedError", "\n", "\n", "", "", "def", "_process_state", "(", "self", ",", "batch", ",", "batch_idx", ",", "train", "=", "True", ")", ":", "\n", "        ", "\"\"\"Handle logic for state context. This is unused for all current S3 experiments\"\"\"", "\n", "\n", "# Number of context steps", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.utils.train.log_optimizer": [[144, 153], ["sorted", "enumerate", "logger.info", "g.get", "group_hps.items", "len"], "function", ["None"], ["key", "=", "\"n_context\"", "if", "train", "else", "\"n_context_eval\"", "\n", "n_context", "=", "self", ".", "hparams", ".", "train", ".", "state", ".", "get", "(", "key", ")", "\n", "\n", "# Don't need to do anything if 0 context steps", "\n", "if", "n_context", "==", "0", "and", "self", ".", "hparams", ".", "train", ".", "state", ".", "mode", "not", "in", "[", "'tbptt'", "]", ":", "\n", "            ", "return", "\n", "\n", "# Reset state if needed", "\n", "", "if", "self", ".", "hparams", ".", "train", ".", "state", ".", "mode", "==", "\"reset\"", ":", "\n", "            ", "if", "batch_idx", "%", "(", "n_context", "+", "1", ")", "==", "0", ":", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.utils.train.resume": [[159, 186], ["pytorch_lightning.seed_everything", "create_trainer", "create_model", "model.to.setup", "model.to.to", "model.to.on_post_move_to_device", "create_trainer.fit"], "function", ["None"], ["with", "torch", ".", "no_grad", "(", ")", ":", "# should be unnecessary because individual modules should handle this", "\n", "                ", "for", "_batch", "in", "self", ".", "_memory_chunks", ":", "\n", "                    ", "self", ".", "forward", "(", "_batch", ")", "\n", "# Prepare for next step", "\n", "", "", "self", ".", "_memory_chunks", ".", "append", "(", "batch", ")", "\n", "self", ".", "_memory_chunks", "=", "self", ".", "_memory_chunks", "[", "-", "n_context", ":", "]", "\n", "\n", "", "elif", "self", ".", "hparams", ".", "train", ".", "state", ".", "mode", "==", "'tbptt'", ":", "\n", "            ", "_", ",", "_", ",", "*", "z", "=", "batch", "\n", "reset", "=", "z", "[", "-", "1", "]", "# if tbptt, last element of z should be whether to reset state!", "\n", "if", "reset", ":", "\n", "                ", "self", ".", "_reset_state", "(", "batch", ")", "\n", "", "else", ":", "\n", "                ", "self", ".", "_state", "=", "self", ".", "_detach_state", "(", "self", ".", "_state", ")", "\n", "\n", "", "", "", "def", "on_epoch_start", "(", "self", ")", ":", "\n", "        ", "self", ".", "_initialize_state", "(", ")", "\n", "\n", "", "def", "forward", "(", "self", ",", "batch", ")", ":", "\n", "        ", "\"\"\"Passes a batch through the encoder, backbone, and decoder\"\"\"", "\n", "# z holds arguments such as sequence length", "\n", "x", ",", "y", ",", "*", "z", "=", "batch", "\n", "# w can model-specific constructions such as key_padding_mask for transformers or state for RNNs", "\n", "x", ",", "*", "w", "=", "self", ".", "encoder", "(", "x", ",", "*", "z", ")", "\n", "x", ",", "state", "=", "self", ".", "model", "(", "x", ",", "*", "w", ",", "state", "=", "self", ".", "_state", ")", "\n", "self", ".", "_state", "=", "state", "\n", "x", ",", "*", "w", "=", "self", ".", "decoder", "(", "x", ",", "state", ",", "*", "z", ")", "\n", "return", "x", ",", "y", ",", "*", "w", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.utils.train.resume_manual": [[187, 214], ["pytorch_lightning.Trainer", "torch.load", "model.to.to", "model.to.setup", "model.to.modules", "model.to.load_state_dict", "pl.Trainer.init_optimizers", "zip", "pl.Trainer.test", "hasattr", "optimizer.load_state_dict", "Path().absolute", "module.setup", "Path"], "function", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.sc09_classifier.test_speech_commands.test"], ["\n", "", "@", "torch", ".", "inference_mode", "(", ")", "\n", "def", "forward_recurrence", "(", "self", ",", "batch", ",", "k", "=", "1", ")", ":", "\n", "        ", "\"\"\"This is a bit hacky; not part of the main train loop, only used to benchmark speed of recurrent view\"\"\"", "\n", "x", ",", "y", ",", "*", "z", "=", "batch", "\n", "T", "=", "x", ".", "shape", "[", "1", "]", "\n", "\n", "if", "k", ">", "1", ":", "\n", "            ", "x", "=", "torch", ".", "cat", "(", "[", "x", "]", "*", "k", ",", "dim", "=", "0", ")", "\n", "\n", "", "self", ".", "_state", "=", "self", ".", "model", ".", "default_state", "(", "*", "x", ".", "shape", "[", ":", "1", "]", ",", "device", "=", "\"cuda\"", ")", "\n", "\n", "x_all", "=", "[", "]", "\n", "w_all", "=", "[", "]", "\n", "for", "t", "in", "tqdm", "(", "range", "(", "T", ")", ")", ":", "\n", "\n", "            ", "x_t", "=", "x", "[", ":", ",", "t", "]", "\n", "x_t", "=", "x_t", ".", "to", "(", "\"cuda\"", ")", "\n", "\n", "x_t", ",", "*", "w_t", "=", "self", ".", "encoder", "(", "x_t", ")", "\n", "x_t", ",", "state", "=", "self", ".", "model", ".", "step", "(", "x_t", ",", "state", "=", "self", ".", "_state", ")", "\n", "self", ".", "_state", "=", "state", "\n", "x_t", ",", "*", "w_t", "=", "self", ".", "decoder", "(", "x_t", ",", "state", ")", "\n", "\n", "x_all", ".", "append", "(", "x_t", ")", "\n", "w_all", ".", "append", "(", "w_t", ")", "\n", "", "return", "torch", ".", "stack", "(", "x_all", ")", ",", "y", ",", "*", "[", "torch", ".", "stack", "(", "w_", ")", "for", "w_", "in", "zip", "(", "*", "w_all", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.utils.permutations.bitreversal_po2": [[8, 15], ["int", "numpy.arange().reshape", "range", "np.hstack.squeeze", "numpy.hstack", "math.log", "math.log", "numpy.arange"], "function", ["None"], ["def", "bitreversal_po2", "(", "n", ")", ":", "\n", "    ", "m", "=", "int", "(", "math", ".", "log", "(", "n", ")", "/", "math", ".", "log", "(", "2", ")", ")", "\n", "perm", "=", "np", ".", "arange", "(", "n", ")", ".", "reshape", "(", "n", ",", "1", ")", "\n", "for", "i", "in", "range", "(", "m", ")", ":", "\n", "        ", "n1", "=", "perm", ".", "shape", "[", "0", "]", "//", "2", "\n", "perm", "=", "np", ".", "hstack", "(", "(", "perm", "[", ":", "n1", "]", ",", "perm", "[", "n1", ":", "]", ")", ")", "\n", "", "return", "perm", ".", "squeeze", "(", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.utils.permutations.bitreversal_permutation": [[16, 21], ["int", "permutations.bitreversal_po2", "numpy.extract", "math.ceil", "math.log", "math.log"], "function", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.utils.permutations.bitreversal_po2"], ["", "def", "bitreversal_permutation", "(", "n", ")", ":", "\n", "    ", "m", "=", "int", "(", "math", ".", "ceil", "(", "math", ".", "log", "(", "n", ")", "/", "math", ".", "log", "(", "2", ")", ")", ")", "\n", "N", "=", "1", "<<", "m", "\n", "perm", "=", "bitreversal_po2", "(", "N", ")", "\n", "return", "np", ".", "extract", "(", "perm", "<", "n", ",", "perm", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.utils.permutations.transpose_permutation": [[22, 28], ["numpy.arange", "indices.reshape.reshape", "indices.reshape.reshape"], "function", ["None"], ["", "def", "transpose_permutation", "(", "h", ",", "w", ")", ":", "\n", "    ", "indices", "=", "np", ".", "arange", "(", "h", "*", "w", ")", "\n", "indices", "=", "indices", ".", "reshape", "(", "(", "h", ",", "w", ")", ")", "\n", "indices", "=", "indices", ".", "T", "\n", "indices", "=", "indices", ".", "reshape", "(", "h", "*", "w", ")", "\n", "return", "indices", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.utils.permutations.snake_permutation": [[29, 35], ["numpy.arange", "indices.reshape.reshape", "indices.reshape.reshape"], "function", ["None"], ["", "def", "snake_permutation", "(", "h", ",", "w", ")", ":", "\n", "    ", "indices", "=", "np", ".", "arange", "(", "h", "*", "w", ")", "\n", "indices", "=", "indices", ".", "reshape", "(", "(", "h", ",", "w", ")", ")", "\n", "indices", "[", "1", ":", ":", "2", ",", ":", "]", "=", "indices", "[", "1", ":", ":", "2", ",", ":", ":", "-", "1", "]", "\n", "indices", "=", "indices", ".", "reshape", "(", "h", "*", "w", ")", "\n", "return", "indices", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.utils.permutations.hilbert_permutation": [[36, 44], ["int", "permutations.decode", "numpy.arange().reshape", "math.log2", "list", "range", "numpy.arange"], "function", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.utils.permutations.decode"], ["", "def", "hilbert_permutation", "(", "n", ")", ":", "\n", "    ", "m", "=", "int", "(", "math", ".", "log2", "(", "n", ")", ")", "\n", "assert", "n", "==", "2", "**", "m", "\n", "inds", "=", "decode", "(", "list", "(", "range", "(", "n", "*", "n", ")", ")", ",", "2", ",", "m", ")", "\n", "ind_x", ",", "ind_y", "=", "inds", ".", "T", "\n", "indices", "=", "np", ".", "arange", "(", "n", "*", "n", ")", ".", "reshape", "(", "(", "n", ",", "n", ")", ")", "\n", "indices", "=", "indices", "[", "ind_x", ",", "ind_y", "]", "\n", "return", "(", "indices", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.utils.permutations.decode": [[46, 133], ["numpy.atleast_1d", "numpy.reshape", "permutations.binary2gray", "numpy.swapaxes", "range", "numpy.pad", "numpy.reshape", "numpy.squeeze", "np.squeeze.view", "numpy.reshape", "ValueError", "np.atleast_1d.ravel().astype().view", "numpy.unpackbits", "numpy.reshape", "range", "numpy.packbits", "numpy.logical_xor", "numpy.logical_and", "numpy.logical_xor", "numpy.logical_xor", "np.atleast_1d.ravel().astype", "numpy.logical_not", "numpy.logical_xor", "np.atleast_1d.ravel"], "function", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.utils.permutations.binary2gray", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.sc.pad"], ["def", "decode", "(", "hilberts", ",", "num_dims", ",", "num_bits", ")", ":", "\n", "  ", "''' Decode an array of Hilbert integers into locations in a hypercube.\n  This is a vectorized-ish version of the Hilbert curve implementation by John\n  Skilling as described in:\n  Skilling, J. (2004, April). Programming the Hilbert curve. In AIP Conference\n    Proceedings (Vol. 707, No. 1, pp. 381-387). American Institute of Physics.\n  Params:\n  -------\n   hilberts - An ndarray of Hilbert integers.  Must be an integer dtype and\n              cannot have fewer bits than num_dims * num_bits.\n   num_dims - The dimensionality of the hypercube. Integer.\n   num_bits - The number of bits for each dimension. Integer.\n  Returns:\n  --------\n   The output is an ndarray of unsigned integers with the same shape as hilberts\n   but with an additional dimension of size num_dims.\n  '''", "\n", "\n", "if", "num_dims", "*", "num_bits", ">", "64", ":", "\n", "    ", "raise", "ValueError", "(", "\n", "'''\n      num_dims=%d and num_bits=%d for %d bits total, which can't be encoded\n      into a uint64.  Are you sure you need that many points on your Hilbert\n      curve?\n      '''", "%", "(", "num_dims", ",", "num_bits", ")", "\n", ")", "\n", "\n", "# Handle the case where we got handed a naked integer.", "\n", "", "hilberts", "=", "np", ".", "atleast_1d", "(", "hilberts", ")", "\n", "\n", "# Keep around the shape for later.", "\n", "orig_shape", "=", "hilberts", ".", "shape", "\n", "\n", "# Treat each of the hilberts as a sequence of eight uint8.", "\n", "# This treats all of the inputs as uint64 and makes things uniform.", "\n", "hh_uint8", "=", "np", ".", "reshape", "(", "hilberts", ".", "ravel", "(", ")", ".", "astype", "(", "'>u8'", ")", ".", "view", "(", "np", ".", "uint8", ")", ",", "(", "-", "1", ",", "8", ")", ")", "\n", "\n", "# Turn these lists of uints into lists of bits and then truncate to the size", "\n", "# we actually need for using Skilling's procedure.", "\n", "hh_bits", "=", "np", ".", "unpackbits", "(", "hh_uint8", ",", "axis", "=", "1", ")", "[", ":", ",", "-", "num_dims", "*", "num_bits", ":", "]", "\n", "\n", "# Take the sequence of bits and Gray-code it.", "\n", "gray", "=", "binary2gray", "(", "hh_bits", ")", "\n", "\n", "# There has got to be a better way to do this.", "\n", "# I could index them differently, but the eventual packbits likes it this way.", "\n", "gray", "=", "np", ".", "swapaxes", "(", "\n", "np", ".", "reshape", "(", "gray", ",", "(", "-", "1", ",", "num_bits", ",", "num_dims", ")", ")", ",", "\n", "axis1", "=", "1", ",", "axis2", "=", "2", ",", "\n", ")", "\n", "\n", "# Iterate backwards through the bits.", "\n", "for", "bit", "in", "range", "(", "num_bits", "-", "1", ",", "-", "1", ",", "-", "1", ")", ":", "\n", "\n", "# Iterate backwards through the dimensions.", "\n", "    ", "for", "dim", "in", "range", "(", "num_dims", "-", "1", ",", "-", "1", ",", "-", "1", ")", ":", "\n", "\n", "# Identify which ones have this bit active.", "\n", "      ", "mask", "=", "gray", "[", ":", ",", "dim", ",", "bit", "]", "\n", "\n", "# Where this bit is on, invert the 0 dimension for lower bits.", "\n", "gray", "[", ":", ",", "0", ",", "bit", "+", "1", ":", "]", "=", "np", ".", "logical_xor", "(", "gray", "[", ":", ",", "0", ",", "bit", "+", "1", ":", "]", ",", "mask", "[", ":", ",", "np", ".", "newaxis", "]", ")", "\n", "\n", "# Where the bit is off, exchange the lower bits with the 0 dimension.", "\n", "to_flip", "=", "np", ".", "logical_and", "(", "\n", "np", ".", "logical_not", "(", "mask", "[", ":", ",", "np", ".", "newaxis", "]", ")", ",", "\n", "np", ".", "logical_xor", "(", "gray", "[", ":", ",", "0", ",", "bit", "+", "1", ":", "]", ",", "gray", "[", ":", ",", "dim", ",", "bit", "+", "1", ":", "]", ")", "\n", ")", "\n", "gray", "[", ":", ",", "dim", ",", "bit", "+", "1", ":", "]", "=", "np", ".", "logical_xor", "(", "gray", "[", ":", ",", "dim", ",", "bit", "+", "1", ":", "]", ",", "to_flip", ")", "\n", "gray", "[", ":", ",", "0", ",", "bit", "+", "1", ":", "]", "=", "np", ".", "logical_xor", "(", "gray", "[", ":", ",", "0", ",", "bit", "+", "1", ":", "]", ",", "to_flip", ")", "\n", "\n", "# Pad back out to 64 bits.", "\n", "", "", "extra_dims", "=", "64", "-", "num_bits", "\n", "padded", "=", "np", ".", "pad", "(", "gray", ",", "(", "(", "0", ",", "0", ")", ",", "(", "0", ",", "0", ")", ",", "(", "extra_dims", ",", "0", ")", ")", ",", "\n", "mode", "=", "'constant'", ",", "constant_values", "=", "0", ")", "\n", "\n", "# Now chop these up into blocks of 8.", "\n", "locs_chopped", "=", "np", ".", "reshape", "(", "padded", "[", ":", ",", ":", ",", ":", ":", "-", "1", "]", ",", "(", "-", "1", ",", "num_dims", ",", "8", ",", "8", ")", ")", "\n", "\n", "# Take those blocks and turn them unto uint8s.", "\n", "locs_uint8", "=", "np", ".", "squeeze", "(", "np", ".", "packbits", "(", "locs_chopped", ",", "bitorder", "=", "'little'", ",", "axis", "=", "3", ")", ")", "\n", "\n", "# Finally, treat these as uint64s.", "\n", "flat_locs", "=", "locs_uint8", ".", "view", "(", "np", ".", "uint64", ")", "\n", "\n", "# Return them in the expected shape.", "\n", "return", "np", ".", "reshape", "(", "flat_locs", ",", "(", "*", "orig_shape", ",", "num_dims", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.utils.permutations.right_shift": [[134, 163], ["slice", "numpy.pad", "numpy.zeros_like", "len", "len", "slice", "tuple"], "function", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.sc.pad"], ["", "def", "right_shift", "(", "binary", ",", "k", "=", "1", ",", "axis", "=", "-", "1", ")", ":", "\n", "  ", "''' Right shift an array of binary values.\n  Parameters:\n  -----------\n   binary: An ndarray of binary values.\n   k: The number of bits to shift. Default 1.\n   axis: The axis along which to shift.  Default -1.\n  Returns:\n  --------\n   Returns an ndarray with zero prepended and the ends truncated, along\n   whatever axis was specified.\n'''", "\n", "\n", "# If we're shifting the whole thing, just return zeros.", "\n", "if", "binary", ".", "shape", "[", "axis", "]", "<=", "k", ":", "\n", "    ", "return", "np", ".", "zeros_like", "(", "binary", ")", "\n", "\n", "# Determine the padding pattern.", "\n", "", "padding", "=", "[", "(", "0", ",", "0", ")", "]", "*", "len", "(", "binary", ".", "shape", ")", "\n", "padding", "[", "axis", "]", "=", "(", "k", ",", "0", ")", "\n", "\n", "# Determine the slicing pattern to eliminate just the last one.", "\n", "slicing", "=", "[", "slice", "(", "None", ")", "]", "*", "len", "(", "binary", ".", "shape", ")", "\n", "slicing", "[", "axis", "]", "=", "slice", "(", "None", ",", "-", "k", ")", "\n", "\n", "shifted", "=", "np", ".", "pad", "(", "binary", "[", "tuple", "(", "slicing", ")", "]", ",", "padding", ",", "\n", "mode", "=", "'constant'", ",", "constant_values", "=", "0", ")", "\n", "\n", "return", "shifted", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.utils.permutations.binary2gray": [[164, 181], ["permutations.right_shift", "numpy.logical_xor"], "function", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.utils.permutations.right_shift"], ["", "def", "binary2gray", "(", "binary", ",", "axis", "=", "-", "1", ")", ":", "\n", "  ", "''' Convert an array of binary values into Gray codes.\n  This uses the classic X ^ (X >> 1) trick to compute the Gray code.\n  Parameters:\n  -----------\n   binary: An ndarray of binary values.\n   axis: The axis along which to compute the gray code. Default=-1.\n  Returns:\n  --------\n   Returns an ndarray of Gray codes.\n  '''", "\n", "shifted", "=", "right_shift", "(", "binary", ",", "axis", "=", "axis", ")", "\n", "\n", "# Do the X ^ (X >> 1) trick.", "\n", "gray", "=", "np", ".", "logical_xor", "(", "binary", ",", "shifted", ")", "\n", "\n", "return", "gray", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.optim.schedulers.CosineWarmup.__init__": [[11, 14], ["super().__init__"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__"], ["    ", "def", "__init__", "(", "self", ",", "optimizer", ",", "T_max", ",", "eta_min", "=", "0", ",", "warmup_step", "=", "0", ",", "**", "kwargs", ")", ":", "\n", "        ", "self", ".", "warmup_step", "=", "warmup_step", "\n", "super", "(", ")", ".", "__init__", "(", "optimizer", ",", "T_max", "-", "warmup_step", ",", "eta_min", ",", "*", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.optim.schedulers.CosineWarmup.get_lr": [[17, 34], ["warnings.warn", "zip", "math.cos", "math.cos", "math.cos"], "methods", ["None"], ["", "def", "get_lr", "(", "self", ")", ":", "\n", "        ", "if", "not", "self", ".", "_get_lr_called_within_step", ":", "\n", "            ", "warnings", ".", "warn", "(", "\"To get the last learning rate computed by the scheduler, \"", "\n", "\"please use `get_last_lr()`.\"", ",", "UserWarning", ")", "\n", "\n", "", "if", "self", ".", "last_epoch", "==", "self", ".", "warmup_step", ":", "# also covers the case where both are 0", "\n", "            ", "return", "self", ".", "base_lrs", "\n", "", "elif", "self", ".", "last_epoch", "<", "self", ".", "warmup_step", ":", "\n", "            ", "return", "[", "base_lr", "*", "(", "self", ".", "last_epoch", "+", "1", ")", "/", "self", ".", "warmup_step", "for", "base_lr", "in", "self", ".", "base_lrs", "]", "\n", "", "elif", "(", "self", ".", "last_epoch", "-", "self", ".", "warmup_step", "-", "1", "-", "self", ".", "T_max", ")", "%", "(", "2", "*", "self", ".", "T_max", ")", "==", "0", ":", "\n", "            ", "return", "[", "group", "[", "'lr'", "]", "+", "(", "base_lr", "-", "self", ".", "eta_min", ")", "*", "\n", "(", "1", "-", "math", ".", "cos", "(", "math", ".", "pi", "/", "self", ".", "T_max", ")", ")", "/", "2", "\n", "for", "base_lr", ",", "group", "in", "zip", "(", "self", ".", "base_lrs", ",", "self", ".", "optimizer", ".", "param_groups", ")", "]", "\n", "", "return", "[", "(", "1", "+", "math", ".", "cos", "(", "math", ".", "pi", "*", "(", "self", ".", "last_epoch", "-", "self", ".", "warmup_step", ")", "/", "self", ".", "T_max", ")", ")", "/", "\n", "(", "1", "+", "math", ".", "cos", "(", "math", ".", "pi", "*", "(", "self", ".", "last_epoch", "-", "self", ".", "warmup_step", "-", "1", ")", "/", "self", ".", "T_max", ")", ")", "*", "\n", "(", "group", "[", "'lr'", "]", "-", "self", ".", "eta_min", ")", "+", "self", ".", "eta_min", "\n", "for", "group", "in", "self", ".", "optimizer", ".", "param_groups", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.optim.schedulers.InvSqrt": [[38, 50], ["torch.optim.lr_scheduler.LambdaLR"], "function", ["None"], ["", "def", "InvSqrt", "(", "optimizer", ",", "warmup_step", ")", ":", "\n", "    ", "\"\"\" Originally used for Transformer (in Attention is all you need)\n    \"\"\"", "\n", "\n", "def", "lr_lambda", "(", "step", ")", ":", "\n", "# return a multiplier instead of a learning rate", "\n", "        ", "if", "step", "==", "warmup_step", ":", "# also covers the case where both are 0", "\n", "            ", "return", "1.", "\n", "", "else", ":", "\n", "            ", "return", "1.", "/", "(", "step", "**", "0.5", ")", "if", "step", ">", "warmup_step", "else", "(", "step", "+", "1", ")", "/", "(", "warmup_step", "**", "1.5", ")", "\n", "\n", "", "", "return", "torch", ".", "optim", ".", "lr_scheduler", ".", "LambdaLR", "(", "optimizer", ",", "lr_lambda", "=", "lr_lambda", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.optim.schedulers.Constant": [[52, 61], ["torch.optim.lr_scheduler.LambdaLR"], "function", ["None"], ["", "def", "Constant", "(", "optimizer", ",", "warmup_step", ")", ":", "\n", "\n", "    ", "def", "lr_lambda", "(", "step", ")", ":", "\n", "        ", "if", "step", "==", "warmup_step", ":", "# also covers the case where both are 0", "\n", "            ", "return", "1.", "\n", "", "else", ":", "\n", "            ", "return", "1.", "if", "step", ">", "warmup_step", "else", "(", "step", "+", "1", ")", "/", "warmup_step", "\n", "\n", "", "", "return", "torch", ".", "optim", ".", "lr_scheduler", ".", "LambdaLR", "(", "optimizer", ",", "lr_lambda", "=", "lr_lambda", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.optim.ema.build_ema_optimizer": [[8, 49], ["super().__init__", "super().step", "super().__repr__", "ValueError", "super().__repr__.partition", "p.data.clone", "torch.optim.Adam", "torch.optim.RMSprop"], "function", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.sashimi.sashimi.Sashimi.step"], ["def", "build_ema_optimizer", "(", "optimizer_cls", ")", ":", "\n", "    ", "class", "Optimizer", "(", "optimizer_cls", ")", ":", "\n", "        ", "def", "__init__", "(", "self", ",", "*", "args", ",", "polyak", "=", "0.0", ",", "**", "kwargs", ")", ":", "\n", "            ", "if", "not", "0.0", "<=", "polyak", "<=", "1.0", ":", "\n", "                ", "raise", "ValueError", "(", "\"Invalid polyak decay rate: {}\"", ".", "format", "(", "polyak", ")", ")", "\n", "", "super", "(", ")", ".", "__init__", "(", "*", "args", ",", "**", "kwargs", ")", "\n", "self", ".", "defaults", "[", "'polyak'", "]", "=", "polyak", "\n", "self", ".", "stepped", "=", "False", "\n", "\n", "", "def", "step", "(", "self", ",", "closure", "=", "None", ")", ":", "\n", "            ", "super", "(", ")", ".", "step", "(", "closure", ")", "\n", "self", ".", "stepped", "=", "True", "\n", "\n", "# update exponential moving average after gradient update to parameters", "\n", "for", "group", "in", "self", ".", "param_groups", ":", "\n", "                ", "for", "p", "in", "group", "[", "'params'", "]", ":", "\n", "                    ", "state", "=", "self", ".", "state", "[", "p", "]", "\n", "\n", "# state initialization", "\n", "if", "'ema'", "not", "in", "state", ":", "\n", "                        ", "state", "[", "'ema'", "]", "=", "p", ".", "data", ".", "clone", "(", ")", "# torch.zeros_like(p.data)", "\n", "\n", "# ema update", "\n", "", "state", "[", "'ema'", "]", "-=", "(", "1", "-", "self", ".", "defaults", "[", "'polyak'", "]", ")", "*", "(", "state", "[", "'ema'", "]", "-", "p", ".", "data", ")", "\n", "\n", "\n", "", "", "", "def", "swap_ema", "(", "self", ")", ":", "\n", "            ", "\"\"\" substitute exponential moving average values into parameter values \"\"\"", "\n", "for", "group", "in", "self", ".", "param_groups", ":", "\n", "                ", "for", "p", "in", "group", "[", "'params'", "]", ":", "\n", "                    ", "data", "=", "p", ".", "data", "\n", "state", "=", "self", ".", "state", "[", "p", "]", "\n", "p", ".", "data", "=", "state", "[", "'ema'", "]", "\n", "state", "[", "'ema'", "]", "=", "data", "\n", "\n", "", "", "", "def", "__repr__", "(", "self", ")", ":", "\n", "            ", "s", "=", "super", "(", ")", ".", "__repr__", "(", ")", "\n", "return", "self", ".", "__class__", ".", "__mro__", "[", "1", "]", ".", "__name__", "+", "' (\\npolyak: {}\\n'", ".", "format", "(", "self", ".", "defaults", "[", "'polyak'", "]", ")", "+", "s", ".", "partition", "(", "'\\n'", ")", "[", "2", "]", "\n", "\n", "", "", "Optimizer", ".", "__name__", "=", "optimizer_cls", ".", "__name__", "\n", "return", "Optimizer", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.optim.lamb.Lamb.__init__": [[64, 78], ["dict", "torch.optim.Optimizer.__init__", "ValueError", "ValueError", "ValueError", "ValueError"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__"], ["def", "__init__", "(", "self", ",", "params", ",", "lr", "=", "1e-3", ",", "betas", "=", "(", "0.9", ",", "0.999", ")", ",", "eps", "=", "1e-6", ",", "\n", "weight_decay", "=", "0", ",", "adam", "=", "False", ")", ":", "\n", "        ", "if", "not", "0.0", "<=", "lr", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid learning rate: {}\"", ".", "format", "(", "lr", ")", ")", "\n", "", "if", "not", "0.0", "<=", "eps", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid epsilon value: {}\"", ".", "format", "(", "eps", ")", ")", "\n", "", "if", "not", "0.0", "<=", "betas", "[", "0", "]", "<", "1.0", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid beta parameter at index 0: {}\"", ".", "format", "(", "betas", "[", "0", "]", ")", ")", "\n", "", "if", "not", "0.0", "<=", "betas", "[", "1", "]", "<", "1.0", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid beta parameter at index 1: {}\"", ".", "format", "(", "betas", "[", "1", "]", ")", ")", "\n", "", "defaults", "=", "dict", "(", "lr", "=", "lr", ",", "betas", "=", "betas", ",", "eps", "=", "eps", ",", "\n", "weight_decay", "=", "weight_decay", ")", "\n", "self", ".", "adam", "=", "adam", "\n", "super", "(", "Lamb", ",", "self", ")", ".", "__init__", "(", "params", ",", "defaults", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.optim.lamb.Lamb.step": [[79, 147], ["closure", "exp_avg.mul_().add_", "exp_avg_sq.mul_().addcmul_", "p.data.norm().clamp_", "adam_step.norm", "p.data.add_", "RuntimeError", "len", "torch.zeros_like", "torch.zeros_like", "exp_avg_sq.sqrt().add", "adam_step.add_", "exp_avg.mul_", "exp_avg_sq.mul_", "p.data.norm", "exp_avg_sq.sqrt"], "methods", ["None"], ["", "def", "step", "(", "self", ",", "closure", "=", "None", ")", ":", "\n", "        ", "\"\"\"Performs a single optimization step.\n\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the model\n                and returns the loss.\n        \"\"\"", "\n", "loss", "=", "None", "\n", "if", "closure", "is", "not", "None", ":", "\n", "            ", "loss", "=", "closure", "(", ")", "\n", "\n", "", "for", "group", "in", "self", ".", "param_groups", ":", "\n", "            ", "for", "p", "in", "group", "[", "'params'", "]", ":", "\n", "                ", "if", "p", ".", "grad", "is", "None", ":", "\n", "                    ", "continue", "\n", "", "grad", "=", "p", ".", "grad", ".", "data", "\n", "if", "grad", ".", "is_sparse", ":", "\n", "                    ", "raise", "RuntimeError", "(", "'Lamb does not support sparse gradients.'", ")", "\n", "\n", "", "state", "=", "self", ".", "state", "[", "p", "]", "\n", "\n", "# State initialization", "\n", "if", "len", "(", "state", ")", "==", "0", ":", "\n", "                    ", "state", "[", "'step'", "]", "=", "0", "\n", "# Exponential moving average of gradient values", "\n", "state", "[", "'exp_avg'", "]", "=", "torch", ".", "zeros_like", "(", "p", ".", "data", ")", "\n", "# Exponential moving average of squared gradient values", "\n", "state", "[", "'exp_avg_sq'", "]", "=", "torch", ".", "zeros_like", "(", "p", ".", "data", ")", "\n", "\n", "", "exp_avg", ",", "exp_avg_sq", "=", "state", "[", "'exp_avg'", "]", ",", "state", "[", "'exp_avg_sq'", "]", "\n", "beta1", ",", "beta2", "=", "group", "[", "'betas'", "]", "\n", "\n", "state", "[", "'step'", "]", "+=", "1", "\n", "\n", "# Decay the first and second moment running average coefficient", "\n", "# m_t", "\n", "exp_avg", ".", "mul_", "(", "beta1", ")", ".", "add_", "(", "1", "-", "beta1", ",", "grad", ")", "\n", "# v_t", "\n", "exp_avg_sq", ".", "mul_", "(", "beta2", ")", ".", "addcmul_", "(", "1", "-", "beta2", ",", "grad", ",", "grad", ")", "\n", "\n", "# Paper v3 does not use debiasing.", "\n", "# bias_correction1 = 1 - beta1 ** state['step']", "\n", "# bias_correction2 = 1 - beta2 ** state['step']", "\n", "# Apply bias to lr to avoid broadcast.", "\n", "step_size", "=", "group", "[", "'lr'", "]", "# * math.sqrt(bias_correction2) / bias_correction1", "\n", "\n", "weight_norm", "=", "p", ".", "data", ".", "norm", "(", "p", "=", "2", ")", ".", "clamp_", "(", "0", ",", "10", ")", "\n", "\n", "adam_step", "=", "exp_avg", "/", "exp_avg_sq", ".", "sqrt", "(", ")", ".", "add", "(", "group", "[", "'eps'", "]", ")", "\n", "if", "group", "[", "'weight_decay'", "]", "!=", "0", ":", "\n", "                    ", "adam_step", ".", "add_", "(", "group", "[", "'weight_decay'", "]", ",", "p", ".", "data", ")", "\n", "\n", "", "adam_norm", "=", "adam_step", ".", "norm", "(", "p", "=", "2", ")", "\n", "\n", "if", "weight_norm", "==", "0.0", "or", "adam_norm", "==", "0.0", ":", "\n", "                    ", "trust_ratio", "=", "1", "\n", "", "else", ":", "\n", "                    ", "trust_ratio", "=", "weight_norm", "/", "(", "adam_norm", "+", "group", "[", "'eps'", "]", ")", "\n", "\n", "", "state", "[", "'weight_norm'", "]", "=", "weight_norm", "\n", "state", "[", "'adam_norm'", "]", "=", "adam_norm", "\n", "state", "[", "'trust_ratio'", "]", "=", "trust_ratio", "\n", "if", "self", ".", "adam", ":", "\n", "                    ", "trust_ratio", "=", "1", "\n", "\n", "", "p", ".", "data", ".", "add_", "(", "-", "step_size", "*", "trust_ratio", ",", "adam_step", ")", "\n", "\n", "", "", "return", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.optim.lamb.JITLamb.__init__": [[191, 205], ["dict", "torch.optim.Optimizer.__init__", "ValueError", "ValueError", "ValueError", "ValueError"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__"], ["def", "__init__", "(", "self", ",", "params", ",", "lr", "=", "1e-3", ",", "betas", "=", "(", "0.9", ",", "0.999", ")", ",", "eps", "=", "1e-6", ",", "\n", "weight_decay", "=", "0", ",", "adam", "=", "False", ")", ":", "\n", "        ", "if", "not", "0.0", "<=", "lr", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid learning rate: {}\"", ".", "format", "(", "lr", ")", ")", "\n", "", "if", "not", "0.0", "<=", "eps", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid epsilon value: {}\"", ".", "format", "(", "eps", ")", ")", "\n", "", "if", "not", "0.0", "<=", "betas", "[", "0", "]", "<", "1.0", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid beta parameter at index 0: {}\"", ".", "format", "(", "betas", "[", "0", "]", ")", ")", "\n", "", "if", "not", "0.0", "<=", "betas", "[", "1", "]", "<", "1.0", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid beta parameter at index 1: {}\"", ".", "format", "(", "betas", "[", "1", "]", ")", ")", "\n", "", "defaults", "=", "dict", "(", "lr", "=", "lr", ",", "betas", "=", "betas", ",", "eps", "=", "eps", ",", "\n", "weight_decay", "=", "weight_decay", ")", "\n", "self", ".", "adam", "=", "adam", "\n", "super", "(", ")", ".", "__init__", "(", "params", ",", "defaults", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.optim.lamb.JITLamb.step": [[206, 252], ["closure", "lamb.lamb_kernel", "RuntimeError", "len", "torch.zeros_like", "torch.zeros_like"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.optim.lamb.lamb_kernel"], ["", "def", "step", "(", "self", ",", "closure", "=", "None", ")", ":", "\n", "        ", "\"\"\"Performs a single optimization step.\n\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the model\n                and returns the loss.\n        \"\"\"", "\n", "loss", "=", "None", "\n", "if", "closure", "is", "not", "None", ":", "\n", "            ", "loss", "=", "closure", "(", ")", "\n", "\n", "", "for", "group", "in", "self", ".", "param_groups", ":", "\n", "            ", "for", "p", "in", "group", "[", "'params'", "]", ":", "\n", "                ", "if", "p", ".", "grad", "is", "None", ":", "\n", "                    ", "continue", "\n", "", "grad", "=", "p", ".", "grad", ".", "data", "\n", "if", "grad", ".", "is_sparse", ":", "\n", "                    ", "raise", "RuntimeError", "(", "'Lamb does not support sparse gradients.'", ")", "\n", "\n", "", "state", "=", "self", ".", "state", "[", "p", "]", "\n", "\n", "# State initialization", "\n", "if", "len", "(", "state", ")", "==", "0", ":", "\n", "                    ", "state", "[", "'step'", "]", "=", "0", "\n", "# Exponential moving average of gradient values", "\n", "state", "[", "'exp_avg'", "]", "=", "torch", ".", "zeros_like", "(", "p", ".", "data", ")", "\n", "# Exponential moving average of squared gradient values", "\n", "state", "[", "'exp_avg_sq'", "]", "=", "torch", ".", "zeros_like", "(", "p", ".", "data", ")", "\n", "\n", "", "exp_avg", ",", "exp_avg_sq", "=", "state", "[", "'exp_avg'", "]", ",", "state", "[", "'exp_avg_sq'", "]", "\n", "beta1", ",", "beta2", "=", "group", "[", "'betas'", "]", "\n", "\n", "state", "[", "'step'", "]", "+=", "1", "\n", "step_size", "=", "group", "[", "'lr'", "]", "\n", "\n", "param", ",", "exp_avg", ",", "exp_avg_sq", "=", "lamb_kernel", "(", "p", ".", "data", ",", "grad", ",", "exp_avg", ",", "\n", "exp_avg_sq", ",", "beta1", ",", "\n", "beta2", ",", "step_size", ",", "\n", "group", "[", "'eps'", "]", ",", "\n", "group", "[", "'weight_decay'", "]", ",", "\n", ")", "\n", "state", "[", "'exp_avg'", "]", "=", "exp_avg", "\n", "state", "[", "'exp_avg_sq'", "]", "=", "exp_avg_sq", "\n", "p", ".", "data", "=", "param", "\n", "\n", "", "", "return", "loss", "\n", "", "", ""]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.optim.lamb.lamb_kernel": [[149, 168], ["param.norm().clamp", "adam_step.norm", "trust_ratio.float.float", "exp_avg_sq.sqrt", "param.norm"], "function", ["None"], ["", "", "@", "torch", ".", "jit", ".", "script", "\n", "def", "lamb_kernel", "(", "param", ",", "grad", ",", "exp_avg", ",", "exp_avg_sq", ",", "beta1", ":", "float", ",", "\n", "beta2", ":", "float", ",", "step_size", ":", "float", ",", "eps", ":", "float", ",", "weight_decay", ":", "float", ")", ":", "\n", "    ", "exp_avg", "=", "exp_avg", "*", "beta1", "+", "(", "1", "-", "beta1", ")", "*", "grad", "\n", "exp_avg_sq", "=", "exp_avg_sq", "*", "beta2", "+", "(", "1", "-", "beta2", ")", "*", "(", "grad", "*", "grad", ")", "\n", "\n", "adam_step", "=", "exp_avg", "/", "(", "exp_avg_sq", ".", "sqrt", "(", ")", "+", "eps", ")", "\n", "adam_step", "=", "adam_step", "+", "weight_decay", "*", "param", "\n", "\n", "weight_norm", "=", "param", ".", "norm", "(", "p", "=", "2", ")", ".", "clamp", "(", "0", ",", "10", ")", "\n", "adam_norm", "=", "adam_step", ".", "norm", "(", "p", "=", "2", ")", "\n", "\n", "trust_ratio", "=", "weight_norm", "/", "(", "adam_norm", "+", "eps", ")", "\n", "trust_ratio", "=", "(", "weight_norm", "==", "0.0", ")", "*", "1.0", "+", "(", "weight_norm", "!=", "0.0", ")", "*", "trust_ratio", "\n", "trust_ratio", "=", "(", "adam_norm", "==", "0.0", ")", "*", "1.0", "+", "(", "adam_norm", "!=", "0.0", ")", "*", "trust_ratio", "\n", "trust_ratio", "=", "trust_ratio", ".", "float", "(", ")", "\n", "\n", "param", "=", "param", "-", "step_size", "*", "trust_ratio", "*", "adam_step", "\n", "return", "param", ",", "exp_avg", ",", "exp_avg_sq", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.tasks.tasks.BaseTask.__init__": [[28, 41], ["src.utils.config.to_list", "src.utils.config.to_list", "src.utils.config.to_list", "src.utils.config.to_list", "src.utils.config.instantiate", "src.utils.config.instantiate"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.utils.config.to_list", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.utils.config.to_list", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.utils.config.to_list", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.utils.config.to_list", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.tasks.decoders.instantiate", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.tasks.decoders.instantiate"], ["def", "__init__", "(", "self", ",", "dataset", "=", "None", ",", "model", "=", "None", ",", "loss", "=", "None", ",", "metrics", "=", "None", ",", "torchmetrics", "=", "None", ")", ":", "\n", "        ", "\"\"\" This class is allowed to grab attributes directly off a constructed dataset and model object \"\"\"", "\n", "self", ".", "dataset", "=", "dataset", "\n", "self", ".", "model", "=", "model", "\n", "if", "metrics", "is", "None", ":", "metrics", "=", "[", "]", "\n", "self", ".", "metric_names", "=", "to_list", "(", "metrics", ")", "\n", "\n", "if", "torchmetrics", "is", "None", ":", "torchmetrics", "=", "[", "]", "\n", "self", ".", "torchmetric_names", "=", "to_list", "(", "torchmetrics", ")", "\n", "self", ".", "_tracked_torchmetrics", "=", "{", "}", "\n", "\n", "# Create loss function", "\n", "self", ".", "loss", "=", "instantiate", "(", "M", ".", "output_metric_fns", ",", "loss", ",", "partial", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.tasks.tasks.BaseTask._init_torchmetrics": [[42, 56], ["int", "name.split", "getattr", "name.split", "getattr", "getattr"], "methods", ["None"], ["", "def", "_init_torchmetrics", "(", "self", ",", "prefix", ")", ":", "\n", "        ", "\"\"\"\n        Instantiate torchmetrics.\n        \"\"\"", "\n", "self", ".", "_tracked_torchmetrics", "[", "prefix", "]", "=", "{", "}", "\n", "for", "name", "in", "self", ".", "torchmetric_names", ":", "\n", "            ", "if", "name", "in", "[", "'AUROC'", ",", "'StatScores'", ",", "'Precision'", ",", "'Recall'", ",", "'F1'", "]", ":", "\n", "                ", "self", ".", "_tracked_torchmetrics", "[", "prefix", "]", "[", "name", "]", "=", "getattr", "(", "tm", ",", "name", ")", "(", "average", "=", "'macro'", ",", "num_classes", "=", "self", ".", "dataset", ".", "d_output", ",", "compute_on_step", "=", "False", ")", ".", "to", "(", "'cuda'", ")", "\n", "", "elif", "'@'", "in", "name", ":", "\n", "                ", "k", "=", "int", "(", "name", ".", "split", "(", "'@'", ")", "[", "1", "]", ")", "\n", "mname", "=", "name", ".", "split", "(", "'@'", ")", "[", "0", "]", "\n", "self", ".", "_tracked_torchmetrics", "[", "prefix", "]", "[", "name", "]", "=", "getattr", "(", "tm", ",", "mname", ")", "(", "average", "=", "'macro'", ",", "num_classes", "=", "self", ".", "dataset", ".", "d_output", ",", "compute_on_step", "=", "False", ",", "top_k", "=", "k", ")", ".", "to", "(", "'cuda'", ")", "\n", "", "else", ":", "\n", "                ", "self", ".", "_tracked_torchmetrics", "[", "prefix", "]", "[", "name", "]", "=", "getattr", "(", "tm", ",", "name", ")", "(", "compute_on_step", "=", "False", ")", ".", "to", "(", "'cuda'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.tasks.tasks.BaseTask._reset_torchmetrics": [[57, 71], ["[].reset"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.wavenet.DilatedQueue.reset"], ["", "", "", "def", "_reset_torchmetrics", "(", "self", ",", "prefix", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Reset torchmetrics for a prefix\n        associated with a particular dataloader (e.g. train, val, test).\n\n        Generally do this at the start of an epoch.\n        \"\"\"", "\n", "all_prefixes", "=", "[", "prefix", "]", "if", "prefix", "is", "not", "None", "else", "self", ".", "_tracked_torchmetrics", "\n", "for", "prefix", "in", "all_prefixes", ":", "\n", "            ", "for", "name", "in", "self", ".", "torchmetric_names", ":", "\n", "                ", "try", ":", "\n", "                    ", "self", ".", "_tracked_torchmetrics", "[", "prefix", "]", "[", "name", "]", ".", "reset", "(", ")", "\n", "", "except", "KeyError", ":", "# metrics don't exist yet", "\n", "                    ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.tasks.tasks.BaseTask.get_torchmetrics": [[72, 80], ["[].compute"], "methods", ["None"], ["", "", "", "", "def", "get_torchmetrics", "(", "self", ",", "prefix", ")", ":", "\n", "        ", "\"\"\"\n        Compute torchmetrics for a prefix associated with\n        a particular dataloader (e.g. train, val, test).\n\n        Generally do this at the end of an epoch.\n        \"\"\"", "\n", "return", "{", "name", ":", "self", ".", "_tracked_torchmetrics", "[", "prefix", "]", "[", "name", "]", ".", "compute", "(", ")", "for", "name", "in", "self", ".", "torchmetric_names", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.tasks.tasks.BaseTask.torchmetrics": [[81, 93], ["tasks.BaseTask._init_torchmetrics", "[].update"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.tasks.tasks.BaseTask._init_torchmetrics"], ["", "def", "torchmetrics", "(", "self", ",", "x", ",", "y", ",", "prefix", ")", ":", "\n", "        ", "\"\"\"\n        Update torchmetrics with new x, y .\n        Prefix corresponds to a particular dataloader (e.g. train, val, test).\n\n        Generally call this every batch.\n        \"\"\"", "\n", "if", "prefix", "not", "in", "self", ".", "_tracked_torchmetrics", ":", "\n", "            ", "self", ".", "_init_torchmetrics", "(", "prefix", ")", "\n", "\n", "", "for", "name", "in", "self", ".", "torchmetric_names", ":", "\n", "            ", "self", ".", "_tracked_torchmetrics", "[", "prefix", "]", "[", "name", "]", ".", "update", "(", "x", ",", "y", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.tasks.tasks.BaseTask.metrics": [[94, 109], ["None"], "methods", ["None"], ["", "", "def", "metrics", "(", "self", ",", "x", ",", "y", ")", ":", "\n", "        ", "\"\"\"\n        Metrics are just functions\n        output metrics are a function of output and target\n        loss metrics are a function of loss (e.g. perplexity)\n        \"\"\"", "\n", "output_metrics", "=", "{", "\n", "name", ":", "M", ".", "output_metric_fns", "[", "name", "]", "(", "x", ",", "y", ")", "\n", "for", "name", "in", "self", ".", "metric_names", "if", "name", "in", "M", ".", "output_metric_fns", "\n", "}", "\n", "loss_metrics", "=", "{", "\n", "name", ":", "M", ".", "loss_metric_fns", "[", "name", "]", "(", "x", ",", "y", ",", "self", ".", "loss", ")", "\n", "for", "name", "in", "self", ".", "metric_names", "if", "name", "in", "M", ".", "loss_metric_fns", "\n", "}", "\n", "return", "{", "**", "output_metrics", ",", "**", "loss_metrics", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.tasks.tasks.Scalar.__init__": [[112, 115], ["torch.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__"], ["    ", "def", "__init__", "(", "self", ",", "c", "=", "1", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "c", "=", "c", "\n", "", "def", "forward", "(", "self", ",", "x", ")", ":", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.tasks.tasks.Scalar.forward": [[115, 117], ["None"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "return", "x", "*", "self", ".", "c", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.tasks.tasks.LMTask.__init__": [[119, 146], ["tasks.BaseTask.__init__", "src.Embedding", "src.Embedding", "torch.init.normal_", "torch.init.normal_", "torch.init.normal_", "torch.Sequential", "torch.Sequential", "torch.Sequential", "src.Identity", "src.Identity", "src.TupleModule", "src.TupleModule", "tasks.LMTask.encoder.apply", "src.TupleModule", "src.TupleModule", "math.sqrt", "functools.partial"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__"], ["    ", "def", "__init__", "(", "self", ",", "tied", "=", "False", ",", "rescale", "=", "True", ",", "init", "=", "None", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "loss", "=", "'cross_entropy'", ",", "**", "kwargs", ")", "\n", "n_tokens", "=", "self", ".", "dataset", ".", "n_tokens", "\n", "d_model", "=", "self", ".", "model", ".", "d_model", "\n", "d_output", "=", "self", ".", "model", ".", "d_output", "\n", "\n", "if", "rescale", ":", "\n", "            ", "scale", "=", "U", ".", "TupleModule", "(", "Scalar", ")", "(", "math", ".", "sqrt", "(", "d_model", ")", ")", "\n", "", "else", ":", "\n", "            ", "scale", "=", "U", ".", "Identity", "(", ")", "\n", "\n", "", "embedding", "=", "U", ".", "Embedding", "(", "n_tokens", ",", "d_model", ")", "\n", "nn", ".", "init", ".", "normal_", "(", "embedding", ".", "weight", ",", "mean", "=", "0", ",", "std", "=", "d_model", "**", "-", ".5", ")", "\n", "encoder", "=", "nn", ".", "Sequential", "(", "\n", "embedding", ",", "\n", "scale", ",", "\n", ")", "\n", "self", ".", "encoder", "=", "encoder", "\n", "decoder", "=", "U", ".", "TupleModule", "(", "nn", ".", "Linear", ")", "(", "d_output", ",", "n_tokens", ")", "\n", "self", ".", "decoder", "=", "decoder", "\n", "\n", "if", "tied", ":", "\n", "            ", "assert", "d_model", "==", "d_output", "\n", "self", ".", "decoder", ".", "weight", "=", "self", ".", "encoder", "[", "0", "]", ".", "weight", "\n", "\n", "", "if", "init", "is", "not", "None", ":", "\n", "            ", "self", ".", "encoder", ".", "apply", "(", "functools", ".", "partial", "(", "weights_init_embedding", ",", "init_cfg", "=", "init", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.tasks.tasks.ForecastingTask.__init__": [[149, 151], ["tasks.BaseTask.__init__"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__"], ["    ", "def", "__init__", "(", "self", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.tasks.tasks.AdaptiveLMTask.__init__": [[154, 201], ["tasks.BaseTask.__init__", "src.models.nn.adaptive_softmax.ProjectedAdaptiveLogSoftmax", "src.models.nn.adaptive_softmax.ProjectedAdaptiveLogSoftmax", "src.TupleSequential", "src.TupleSequential", "src.TupleModule", "src.TupleModule"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "div_val", ",", "\n", "cutoffs", ":", "List", "[", "int", "]", ",", "\n", "tie_weights", ":", "bool", ",", "\n", "tie_projs", ":", "List", "[", "bool", "]", ",", "\n", "init_scale", "=", "1.0", ",", "\n", "bias_scale", "=", "0.0", ",", "\n", "dropemb", "=", "0.0", ",", "\n", "dropsoft", "=", "0.0", ",", "\n", "**", "kwargs", ",", "\n", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "**", "kwargs", ")", "\n", "n_tokens", "=", "self", ".", "dataset", ".", "n_tokens", "\n", "d_model", "=", "self", ".", "model", ".", "d_model", "\n", "d_output", "=", "self", ".", "model", ".", "d_output", "\n", "\n", "encoder", "=", "U", ".", "TupleModule", "(", "AdaptiveEmbedding", ")", "(", "\n", "n_tokens", ",", "\n", "d_model", ",", "\n", "d_model", ",", "\n", "cutoffs", "=", "cutoffs", ",", "\n", "div_val", "=", "div_val", ",", "\n", "init_scale", "=", "init_scale", ",", "\n", "dropout", "=", "dropemb", ",", "\n", ")", "\n", "\n", "if", "tie_weights", ":", "\n", "            ", "assert", "d_model", "==", "d_output", "\n", "emb_layers", "=", "[", "i", ".", "weight", "for", "i", "in", "encoder", ".", "emb_layers", "]", "\n", "", "else", ":", "\n", "            ", "emb_layers", "=", "None", "\n", "\n", "# Construct decoder/loss", "\n", "", "emb_projs", "=", "encoder", ".", "emb_projs", "\n", "loss", "=", "ProjectedAdaptiveLogSoftmax", "(", "\n", "n_tokens", ",", "d_output", ",", "d_output", ",", "\n", "cutoffs", ",", "div_val", "=", "div_val", ",", "\n", "tie_projs", "=", "tie_projs", ",", "\n", "out_projs", "=", "emb_projs", ",", "\n", "out_layers_weights", "=", "emb_layers", ",", "\n", "bias_scale", "=", "bias_scale", ",", "\n", "dropout", "=", "dropsoft", ",", "\n", ")", "\n", "\n", "self", ".", "encoder", "=", "U", ".", "TupleSequential", "(", "encoder", ",", "self", ".", "encoder", ")", "\n", "self", ".", "loss", "=", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.tasks.mixture.Mixture.__init__": [[329, 342], ["torch.Module.__init__", "mixture.Mixture.register_buffer", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__"], ["    ", "def", "__init__", "(", "self", ",", "b", ",", "a", ",", "cdf", "=", "'piecewise'", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "b", "=", "b", "\n", "self", ".", "a", "=", "a", "\n", "self", ".", "cdf_fn", "=", "{", "\n", "'piecewise'", ":", "piecewise_cdf", ",", "\n", "'sigmoid'", ":", "F", ".", "sigmoid", ",", "\n", "}", "[", "cdf", "]", "\n", "\n", "assert", "b", "%", "2", "==", "0", "\n", "buckets", "=", "torch", ".", "linspace", "(", "-", "1.0", ",", "1.0", ",", "b", "-", "1", ")", "*", "a", "\n", "# buckets = torch.linspace(-1.0+1/(b-1), 1.0-1/(b-1), b-1) * a", "\n", "self", ".", "register_buffer", "(", "'buckets'", ",", "buckets", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.tasks.mixture.Mixture.forward": [[343, 353], ["torch.unbind", "torch.unbind", "torch.unbind", "torch.unbind", "torch.unbind", "torch.unbind", "torch.unbind", "torch.unbind", "torch.unbind", "torch.unbind", "torch.unbind", "torch.unbind", "torch.unbind", "torch.unbind", "torch.unbind", "torch.unbind", "mixture.pdf", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "einops.rearrange", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.softmax.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.tasks.mixture.pdf"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "\"\"\"\n        x: (..., 3*k)\n        \"\"\"", "\n", "l", ",", "m", ",", "s", "=", "torch", ".", "unbind", "(", "rearrange", "(", "x", ",", "'... (z a) -> ... z a'", ",", "z", "=", "3", ")", ",", "dim", "=", "-", "2", ")", "\n", "p", "=", "pdf", "(", "m", ",", "torch", ".", "exp", "(", "s", ")", ",", "self", ".", "buckets", ",", "self", ".", "cdf_fn", ")", "# (..., k, b)", "\n", "weights", "=", "F", ".", "softmax", "(", "l", ",", "dim", "=", "-", "1", ")", "# (..., k)", "\n", "probs", "=", "torch", ".", "sum", "(", "weights", ".", "unsqueeze", "(", "-", "1", ")", "*", "p", ",", "dim", "=", "-", "2", ")", "# (..., b)", "\n", "logits", "=", "torch", ".", "log", "(", "probs", "+", "1e-8", ")", "\n", "return", "logits", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.tasks.mixture.log_prob_from_logits": [[10, 19], ["torch.max", "torch.max", "torch.max", "torch.max", "torch.logsumexp", "torch.logsumexp", "torch.logsumexp", "torch.logsumexp"], "function", ["None"], ["def", "log_prob_from_logits", "(", "x", ")", ":", "\n", "    ", "\"\"\" numerically stable log_softmax implementation that prevents overflow \"\"\"", "\n", "# TF ordering", "\n", "# axis = len(x.size()) - 1", "\n", "# m, _ = torch.max(x, dim=axis, keepdim=True)", "\n", "# return x - m - torch.log(torch.sum(torch.exp(x - m), dim=axis, keepdim=True))", "\n", "m", ",", "_", "=", "torch", ".", "max", "(", "x", ",", "dim", "=", "-", "1", ",", "keepdim", "=", "True", ")", "\n", "x", "=", "x", "-", "m", "\n", "return", "x", "-", "torch", ".", "logsumexp", "(", "x", ",", "dim", "=", "-", "1", ",", "keepdim", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.tasks.mixture.discretized_mix_logistic_loss_3d": [[20, 90], ["int", "l[].contiguous().view", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "x.unsqueeze.contiguous", "x.unsqueeze.unsqueeze", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.softplus", "torch.softplus", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "mixture.log_prob_from_logits", "l[].contiguous", "means[].unsqueeze", "torch.softplus", "torch.log", "torch.log", "torch.log", "torch.log", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "numpy.log", "torch.logsumexp", "torch.logsumexp", "torch.logsumexp", "torch.logsumexp"], "function", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.tasks.mixture.log_prob_from_logits"], ["", "def", "discretized_mix_logistic_loss_3d", "(", "x", ",", "l", ")", ":", "\n", "    ", "\"\"\"\n    log-likelihood for mixture of discretized logistics, specially for the 3 channel case\n    assumes the data has been rescaled to [-1,1] interval\n    \"\"\"", "\n", "# Pytorch ordering", "\n", "# x = x.permute(0, 2, 3, 1)", "\n", "# l = l.permute(0, 2, 3, 1)", "\n", "xs", "=", "x", ".", "shape", "# [int(y) for y in x.size()]", "\n", "ls", "=", "l", ".", "shape", "# [int(y) for y in l.size()]", "\n", "\n", "# here and below: unpacking the params of the mixture of logistics", "\n", "nr_mix", "=", "int", "(", "ls", "[", "-", "1", "]", "/", "10", ")", "\n", "logit_probs", "=", "l", "[", "...", ",", ":", "nr_mix", "]", "\n", "l", "=", "l", "[", "...", ",", "nr_mix", ":", "]", ".", "contiguous", "(", ")", ".", "view", "(", "xs", "+", "(", "3", "*", "nr_mix", ",", ")", ")", "# 3 for mean, scale, coef", "\n", "means", "=", "l", "[", "...", ",", ":", ",", ":", "nr_mix", "]", "\n", "# log_scales = torch.max(l[..., :, nr_mix:2 * nr_mix], -7.)", "\n", "log_scales", "=", "torch", ".", "clamp", "(", "l", "[", "...", ",", ":", ",", "nr_mix", ":", "2", "*", "nr_mix", "]", ",", "min", "=", "-", "7.", ")", "\n", "\n", "coeffs", "=", "torch", ".", "tanh", "(", "l", "[", "...", ",", ":", ",", "2", "*", "nr_mix", ":", "3", "*", "nr_mix", "]", ")", "\n", "# here and below: getting the means and adjusting them based on preceding", "\n", "# sub-pixels", "\n", "x", "=", "x", ".", "contiguous", "(", ")", "\n", "# x = x.unsqueeze(-1) + Variable(torch.zeros(xs + [nr_mix]).cuda(), requires_grad=False)", "\n", "x", "=", "x", ".", "unsqueeze", "(", "-", "1", ")", "\n", "m2", "=", "(", "means", "[", "...", ",", "1", ",", ":", "]", "+", "coeffs", "[", "...", ",", "0", ",", ":", "]", "*", "x", "[", "...", ",", "0", ",", ":", "]", ")", ".", "view", "(", "xs", "[", ":", "-", "1", "]", "+", "(", "1", ",", "nr_mix", ")", ")", "\n", "\n", "m3", "=", "(", "means", "[", "...", ",", "2", ",", ":", "]", "+", "coeffs", "[", "...", ",", "1", ",", ":", "]", "*", "x", "[", "...", ",", "0", ",", ":", "]", "+", "\n", "coeffs", "[", "...", ",", "2", ",", ":", "]", "*", "x", "[", "...", ",", "1", ",", ":", "]", ")", ".", "view", "(", "xs", "[", ":", "-", "1", "]", "+", "(", "1", ",", "nr_mix", ")", ")", "\n", "\n", "means", "=", "torch", ".", "cat", "(", "(", "means", "[", "...", ",", "0", ",", ":", "]", ".", "unsqueeze", "(", "-", "2", ")", ",", "m2", ",", "m3", ")", ",", "dim", "=", "-", "2", ")", "\n", "centered_x", "=", "x", "-", "means", "\n", "inv_stdv", "=", "torch", ".", "exp", "(", "-", "log_scales", ")", "\n", "plus_in", "=", "inv_stdv", "*", "(", "centered_x", "+", "1.", "/", "255.", ")", "\n", "cdf_plus", "=", "torch", ".", "sigmoid", "(", "plus_in", ")", "\n", "min_in", "=", "inv_stdv", "*", "(", "centered_x", "-", "1.", "/", "255.", ")", "\n", "cdf_min", "=", "torch", ".", "sigmoid", "(", "min_in", ")", "\n", "# log probability for edge case of 0 (before scaling)", "\n", "log_cdf_plus", "=", "plus_in", "-", "F", ".", "softplus", "(", "plus_in", ")", "\n", "# log probability for edge case of 255 (before scaling)", "\n", "log_one_minus_cdf_min", "=", "-", "F", ".", "softplus", "(", "min_in", ")", "\n", "cdf_delta", "=", "cdf_plus", "-", "cdf_min", "# probability for all other cases", "\n", "mid_in", "=", "inv_stdv", "*", "centered_x", "\n", "# log probability in the center of the bin, to be used in extreme cases", "\n", "# (not actually used in our code)", "\n", "log_pdf_mid", "=", "mid_in", "-", "log_scales", "-", "2.", "*", "F", ".", "softplus", "(", "mid_in", ")", "\n", "\n", "# now select the right output: left edge case, right edge case, normal", "\n", "# case, extremely low prob case (doesn't actually happen for us)", "\n", "\n", "# this is what we are really doing, but using the robust version below for extreme cases in other applications and to avoid NaN issue with tf.select()", "\n", "# log_probs = tf.select(x < -0.999, log_cdf_plus, tf.select(x > 0.999, log_one_minus_cdf_min, tf.log(cdf_delta)))", "\n", "\n", "# robust version, that still works if probabilities are below 1e-5 (which never happens in our code)", "\n", "# tensorflow backpropagates through tf.select() by multiplying with zero instead of selecting: this requires use to use some ugly tricks to avoid potential NaNs", "\n", "# the 1e-12 in tf.maximum(cdf_delta, 1e-12) is never actually used as output, it's purely there to get around the tf.select() gradient issue", "\n", "# if the probability on a sub-pixel is below 1e-5, we use an approximation", "\n", "# based on the assumption that the log-density is constant in the bin of", "\n", "# the observed sub-pixel value", "\n", "\n", "inner_inner_cond", "=", "(", "cdf_delta", ">", "1e-5", ")", ".", "float", "(", ")", "\n", "inner_inner_out", "=", "inner_inner_cond", "*", "torch", ".", "log", "(", "torch", ".", "clamp", "(", "cdf_delta", ",", "min", "=", "1e-12", ")", ")", "+", "(", "1.", "-", "inner_inner_cond", ")", "*", "(", "log_pdf_mid", "-", "np", ".", "log", "(", "127.5", ")", ")", "\n", "inner_cond", "=", "(", "x", ">", "0.999", ")", ".", "float", "(", ")", "\n", "inner_out", "=", "inner_cond", "*", "log_one_minus_cdf_min", "+", "(", "1.", "-", "inner_cond", ")", "*", "inner_inner_out", "\n", "cond", "=", "(", "x", "<", "-", "0.999", ")", ".", "float", "(", ")", "\n", "log_probs", "=", "cond", "*", "log_cdf_plus", "+", "(", "1.", "-", "cond", ")", "*", "inner_out", "\n", "log_probs", "=", "torch", ".", "sum", "(", "log_probs", ",", "dim", "=", "-", "2", ")", "+", "log_prob_from_logits", "(", "logit_probs", ")", "\n", "\n", "# return -torch.sum(log_sum_exp(log_probs))", "\n", "return", "-", "torch", ".", "mean", "(", "torch", ".", "logsumexp", "(", "log_probs", ",", "dim", "=", "-", "1", ")", ")", "/", "3.0", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.tasks.mixture.discretized_mix_logistic_loss_1d": [[91, 140], ["l[].contiguous().view", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "x.unsqueeze.contiguous", "x.unsqueeze.unsqueeze", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "int", "int", "torch.softplus", "torch.softplus", "log_probs.size", "torch.squeeze", "torch.squeeze", "torch.squeeze", "torch.squeeze", "mixture.log_prob_from_logits", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "x.unsqueeze.size", "l[].contiguous().view.size", "l[].contiguous", "torch.softplus", "torch.log", "torch.log", "torch.log", "torch.log", "torch.logsumexp", "torch.logsumexp", "torch.logsumexp", "torch.logsumexp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "numpy.log"], "function", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.tasks.mixture.log_prob_from_logits"], ["", "def", "discretized_mix_logistic_loss_1d", "(", "x", ",", "l", ")", ":", "\n", "    ", "\"\"\" log-likelihood for mixture of discretized logistics, assumes the data has been rescaled to [-1,1] interval \"\"\"", "\n", "# Pytorch ordering", "\n", "# x = x.permute(0, 2, 3, 1)", "\n", "# l = l.permute(0, 2, 3, 1)", "\n", "xs", "=", "[", "int", "(", "y", ")", "for", "y", "in", "x", ".", "size", "(", ")", "]", "\n", "ls", "=", "[", "int", "(", "y", ")", "for", "y", "in", "l", ".", "size", "(", ")", "]", "\n", "\n", "# here and below: unpacking the params of the mixture of logistics", "\n", "nr_mix", "=", "ls", "[", "-", "1", "]", "//", "3", "# k", "\n", "logit_probs", "=", "l", "[", "...", ",", ":", "nr_mix", "]", "# (b, h, w, k)", "\n", "l", "=", "l", "[", "...", ",", "nr_mix", ":", "]", ".", "contiguous", "(", ")", ".", "view", "(", "xs", "+", "[", "nr_mix", "*", "2", "]", ")", "# 2 for mean, scale", "\n", "means", "=", "l", "[", "...", ",", ":", ",", ":", "nr_mix", "]", "# (b, h, w, 1, k)", "\n", "log_scales", "=", "torch", ".", "clamp", "(", "l", "[", "...", ",", ":", ",", "nr_mix", ":", "2", "*", "nr_mix", "]", ",", "min", "=", "-", "7.", ")", "# (b, h, w, 1, k)", "\n", "# here and below: getting the means and adjusting them based on preceding", "\n", "# sub-pixels", "\n", "x", "=", "x", ".", "contiguous", "(", ")", "# (b, h, w, 1)", "\n", "# x = x.unsqueeze(-1) + nn.Variable(torch.zeros(xs + [nr_mix]).cuda(), requires_grad=False)", "\n", "x", "=", "x", ".", "unsqueeze", "(", "-", "1", ")", "\n", "\n", "# means = torch.cat((means[:, :, :, 0, :].unsqueeze(3), m2, m3), dim=3)", "\n", "centered_x", "=", "x", "-", "means", "# (b, h, w, 1, k)", "\n", "inv_stdv", "=", "torch", ".", "exp", "(", "-", "log_scales", ")", "# (b, h, w, 1, k)", "\n", "plus_in", "=", "inv_stdv", "*", "(", "centered_x", "+", "1.", "/", "255.", ")", "# (b, h, w, 1, k)", "\n", "cdf_plus", "=", "torch", ".", "sigmoid", "(", "plus_in", ")", "# (b, h, w, 1, k)", "\n", "min_in", "=", "inv_stdv", "*", "(", "centered_x", "-", "1.", "/", "255.", ")", "# (b, h, w, 1, k)", "\n", "cdf_min", "=", "torch", ".", "sigmoid", "(", "min_in", ")", "# (b, h, w, 1, k)", "\n", "# log probability for edge case of 0 (before scaling)", "\n", "log_cdf_plus", "=", "plus_in", "-", "F", ".", "softplus", "(", "plus_in", ")", "\n", "# log probability for edge case of 255 (before scaling)", "\n", "log_one_minus_cdf_min", "=", "-", "F", ".", "softplus", "(", "min_in", ")", "\n", "cdf_delta", "=", "cdf_plus", "-", "cdf_min", "# probability for all other cases", "\n", "mid_in", "=", "inv_stdv", "*", "centered_x", "\n", "# log probability in the center of the bin, to be used in extreme cases", "\n", "# (not actually used in our code)", "\n", "log_pdf_mid", "=", "mid_in", "-", "log_scales", "-", "2.", "*", "F", ".", "softplus", "(", "mid_in", ")", "\n", "\n", "inner_inner_cond", "=", "(", "cdf_delta", ">", "1e-5", ")", ".", "float", "(", ")", "\n", "inner_inner_out", "=", "inner_inner_cond", "*", "torch", ".", "log", "(", "torch", ".", "clamp", "(", "cdf_delta", ",", "min", "=", "1e-12", ")", ")", "+", "(", "1.", "-", "inner_inner_cond", ")", "*", "(", "log_pdf_mid", "-", "np", ".", "log", "(", "127.5", ")", ")", "\n", "inner_cond", "=", "(", "x", ">", "0.999", ")", ".", "float", "(", ")", "# (b, h, w, 1, 1)", "\n", "inner_out", "=", "inner_cond", "*", "log_one_minus_cdf_min", "+", "(", "1.", "-", "inner_cond", ")", "*", "inner_inner_out", "\n", "cond", "=", "(", "x", "<", "-", "0.999", ")", ".", "float", "(", ")", "# (b, h, w, 1, 1)", "\n", "log_probs", "=", "cond", "*", "log_cdf_plus", "+", "(", "1.", "-", "cond", ")", "*", "inner_out", "\n", "# log_probs        = torch.sum(log_probs, dim=3) + log_prob_from_logits(logit_probs)", "\n", "assert", "log_probs", ".", "size", "(", "-", "2", ")", "==", "1", "\n", "log_probs", "=", "torch", ".", "squeeze", "(", "log_probs", ",", "dim", "=", "-", "2", ")", "+", "log_prob_from_logits", "(", "logit_probs", ")", "# (b, h, w, k)", "\n", "\n", "# return -torch.sum(log_sum_exp(log_probs))", "\n", "return", "-", "torch", ".", "mean", "(", "torch", ".", "logsumexp", "(", "log_probs", ",", "dim", "=", "-", "1", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.tasks.mixture.mixture_loss": [[144, 205], ["y.unsqueeze.unsqueeze", "outs[].clamp", "cdf_fn", "cdf_fn", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.where", "torch.where", "torch.where", "torch.where", "torch.where", "torch.where", "torch.where", "torch.where", "torch.log", "torch.log", "torch.log", "torch.log", "torch.logsumexp", "torch.logsumexp", "torch.logsumexp", "torch.logsumexp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "mixture.log_prob_from_logits", "torch.logsumexp.mean"], "function", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.tasks.mixture.log_prob_from_logits"], ["def", "mixture_loss", "(", "outs", ",", "y", ",", "c", "=", "256", ",", "cdf_fn", "=", "torch", ".", "sigmoid", ",", "reduce", "=", "'mean'", ",", "scale", "=", "2.0", ")", ":", "\n", "    ", "\"\"\"\n    outs: (..., 3*k)\n    y: (...) int between 0 to c-1 (inclusive)\n    c: number of classes\n\n    scale: hyperparameter that increases the size of the buckets, i.e. increases entropy of distribution (less confident)\n    \"\"\"", "\n", "assert", "outs", ".", "shape", "[", "-", "1", "]", "%", "3", "==", "0", "\n", "k", "=", "outs", ".", "shape", "[", "-", "1", "]", "//", "3", "\n", "\n", "# Transform targets", "\n", "y", "=", "y", ".", "unsqueeze", "(", "-", "1", ")", "# (..., 1)", "\n", "# y_normalized = (2*y - (c-1)) / (c-1)", "\n", "# y_normalized = (y - (c-1)/2) / ((c-1)/2)", "\n", "# Buckets are slightly offset from normal implementation, to match Mixture() class below", "\n", "y_normalized", "=", "(", "y", "-", "(", "c", "-", "1", ")", "/", "2", ")", "/", "(", "(", "c", "-", "2", ")", "/", "2", ")", "\n", "\n", "# bin_max = y_normalized + 1./(c-1) # (..., 1)", "\n", "# bin_min = y_normalized - 1./(c-1) # (..., 1)", "\n", "bin_max", "=", "y_normalized", "+", "1.", "/", "(", "c", "-", "2", ")", "# (..., 1)", "\n", "bin_min", "=", "y_normalized", "-", "1.", "/", "(", "c", "-", "2", ")", "# (..., 1)", "\n", "\n", "bin_max", "=", "bin_max", "*", "scale", "\n", "bin_min", "=", "bin_min", "*", "scale", "\n", "\n", "# Unpack outputs", "\n", "mixture_logits", "=", "outs", "[", "...", ",", ":", "k", "]", "# (..., k)", "\n", "means", "=", "outs", "[", "...", ",", "k", ":", "2", "*", "k", "]", "# (..., k)", "\n", "scales", "=", "outs", "[", "...", ",", "2", "*", "k", ":", "3", "*", "k", "]", ".", "clamp", "(", "min", "=", "-", "7.", ")", "# (..., k)", "\n", "\n", "# Transform bins by mean and scale", "\n", "\n", "# equivalent to dividing by exp(scales) or negating scales; marginally easier for me to reason about multiply", "\n", "bin_min", "=", "(", "bin_min", "-", "means", ")", "*", "torch", ".", "exp", "(", "scales", ")", "# (..., k)", "\n", "bin_max", "=", "(", "bin_max", "-", "means", ")", "*", "torch", ".", "exp", "(", "scales", ")", "# (..., k)", "\n", "\n", "# Calculate probabilities", "\n", "cdf_max", "=", "cdf_fn", "(", "bin_max", ")", "\n", "cdf_min", "=", "cdf_fn", "(", "bin_min", ")", "\n", "\n", "# Edge cases for endpoints", "\n", "z", "=", "torch", ".", "zeros_like", "(", "y", ",", "dtype", "=", "torch", ".", "float", ")", "# torch.where doesn't support float32 scalar...", "\n", "tail_min", "=", "torch", ".", "where", "(", "y", "==", "0", ",", "cdf_min", ",", "z", ")", "\n", "tail_max", "=", "torch", ".", "where", "(", "y", "==", "c", "-", "1", ",", "1.", "-", "cdf_max", ",", "z", ")", "\n", "\n", "probs", "=", "cdf_max", "-", "cdf_min", "+", "tail_min", "+", "tail_max", "+", "1e-8", "# pad for numerical stability", "\n", "\n", "# Finish calculation in logit space; I doubt its more stable but previous implementations do this", "\n", "# Equivalent to working in probability space:", "\n", "#   probs = torch.sum(torch.softmax(mixture_logits, dim=-1) * probs, dim=-1)", "\n", "#   log_probs = torch.log(probs)", "\n", "log_probs", "=", "torch", ".", "log", "(", "probs", ")", "\n", "log_probs", "=", "torch", ".", "logsumexp", "(", "log_probs", "+", "log_prob_from_logits", "(", "mixture_logits", ")", ",", "dim", "=", "-", "1", ")", "\n", "\n", "\n", "if", "reduce", "==", "'mean'", ":", "\n", "        ", "return", "-", "log_probs", ".", "mean", "(", ")", "\n", "", "elif", "reduce", "==", "'none'", ":", "\n", "        ", "return", "-", "log_probs", "\n", "", "else", ":", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.tasks.mixture.mixture_loss_kd": [[206, 276], ["y.unsqueeze.unsqueeze", "einops.rearrange", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "range", "cdf_fn", "cdf_fn", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.where", "torch.where", "torch.where", "torch.where", "torch.where", "torch.where", "torch.where", "torch.where", "torch.log", "torch.log", "torch.log", "torch.log", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.logsumexp", "torch.logsumexp", "torch.logsumexp", "torch.logsumexp", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "mixture.log_prob_from_logits", "torch.logsumexp.mean"], "function", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.tasks.mixture.log_prob_from_logits"], ["", "def", "mixture_loss_kd", "(", "outs", ",", "y", ",", "c", "=", "256", ",", "cdf_fn", "=", "torch", ".", "sigmoid", ",", "reduce", "=", "'mean'", ")", ":", "\n", "    ", "\"\"\" Mixture loss for outputting multiple distributions at once, where later predictions can depend linearly on previous ones.\n\n    outs: (..., 3*k)\n    y: (..., d) int between 0 to c-1 (inclusive)\n    c: number of classes\n    \"\"\"", "\n", "d", "=", "y", ".", "shape", "[", "-", "1", "]", "\n", "factor", "=", "1", "+", "2", "*", "d", "+", "d", "*", "(", "d", "-", "1", ")", "//", "2", "\n", "assert", "outs", ".", "shape", "[", "-", "1", "]", "%", "factor", "==", "0", "\n", "k", "=", "outs", ".", "shape", "[", "-", "1", "]", "//", "factor", "\n", "\n", "# Transform targets", "\n", "y", "=", "y", ".", "unsqueeze", "(", "-", "1", ")", "\n", "y_normalized", "=", "(", "y", "-", "(", "c", "-", "1", ")", "/", "2", ")", "/", "(", "(", "c", "-", "1", ")", "/", "2", ")", "\n", "bin_max", "=", "y_normalized", "+", "1.", "/", "(", "c", "-", "1", ")", "# (..., d)", "\n", "bin_min", "=", "y_normalized", "-", "1.", "/", "(", "c", "-", "1", ")", "# (..., d)", "\n", "# y_normalized = (y - (c-1)/2) / ((c-2)/2)", "\n", "# bin_max = y_normalized + 1./(c-2) # (..., d)", "\n", "# bin_min = y_normalized - 1./(c-2) # (..., d)", "\n", "\n", "bin_max", "=", "bin_max", "*", "1.", "\n", "bin_min", "=", "bin_min", "*", "1.", "\n", "\n", "# Unpack outputs", "\n", "outs", "=", "rearrange", "(", "outs", ",", "'... (d k) -> ... d k'", ",", "k", "=", "k", ")", "\n", "mixture_logits", "=", "outs", "[", "...", ",", "0", ",", ":", "]", "# (..., k)", "\n", "means", "=", "outs", "[", "...", ",", "1", ":", "1", "+", "d", ",", ":", "]", "# (..., d*k)", "\n", "scales", "=", "outs", "[", "...", ",", "1", "+", "d", ":", "1", "+", "2", "*", "d", ",", ":", "]", "# (..., d*k)", "\n", "coeffs", "=", "torch", ".", "tanh", "(", "outs", "[", "...", ",", "1", "+", "2", "*", "d", ":", ",", ":", "]", ")", "\n", "\n", "# Transform means with linear combinations", "\n", "# means = rearrange(means, '... (d k) -> ... d k', k=k)", "\n", "# scales = rearrange(scales, '... (d k) -> ... d k', k=k)", "\n", "idx", "=", "0", "\n", "for", "i", "in", "range", "(", "1", ",", "d", ")", ":", "\n", "        ", "means", "[", "...", ",", "i", ",", ":", "]", "+=", "torch", ".", "sum", "(", "coeffs", "[", "...", ",", "idx", ":", "idx", "+", "i", ",", ":", "]", "*", "y_normalized", "[", "...", ",", ":", "i", ",", ":", "]", ",", "dim", "=", "-", "2", ")", "# (..., k)", "\n", "idx", "+=", "i", "\n", "\n", "# Transform bins by mean and scale", "\n", "\n", "# equivalent to dividing by exp(scales) or negating scales; marginally easier for me to reason about multiply", "\n", "", "bin_min", "=", "(", "bin_min", "-", "means", ")", "*", "torch", ".", "exp", "(", "scales", ")", "# (..., d, k)", "\n", "bin_max", "=", "(", "bin_max", "-", "means", ")", "*", "torch", ".", "exp", "(", "scales", ")", "# (..., d, k)", "\n", "\n", "# Calculate probabilities", "\n", "cdf_max", "=", "cdf_fn", "(", "bin_max", ")", "\n", "cdf_min", "=", "cdf_fn", "(", "bin_min", ")", "\n", "\n", "# Edge cases for endpoints", "\n", "z", "=", "torch", ".", "zeros_like", "(", "y", ",", "dtype", "=", "torch", ".", "float", ")", "# torch.where doesn't support float32 scalar...", "\n", "tail_min", "=", "torch", ".", "where", "(", "y", "==", "0", ",", "cdf_min", ",", "z", ")", "\n", "tail_max", "=", "torch", ".", "where", "(", "y", "==", "c", "-", "1", ",", "1.", "-", "cdf_max", ",", "z", ")", "\n", "\n", "probs", "=", "cdf_max", "-", "cdf_min", "+", "tail_min", "+", "tail_max", "+", "1e-8", "# pad for numerical stability", "\n", "\n", "# Finish calculation in logit space; I doubt its more stable but previous implementations do this", "\n", "# Equivalent to working in probability space:", "\n", "#   probs = torch.sum(torch.softmax(mixture_logits, dim=-1) * probs, dim=-1)", "\n", "#   log_probs = torch.log(probs)", "\n", "log_probs", "=", "torch", ".", "log", "(", "probs", ")", "# (..., d, k)", "\n", "log_probs", "=", "torch", ".", "sum", "(", "log_probs", ",", "dim", "=", "-", "2", ")", "# (..., k)", "\n", "log_probs", "=", "torch", ".", "logsumexp", "(", "log_probs", "+", "log_prob_from_logits", "(", "mixture_logits", ")", ",", "dim", "=", "-", "1", ")", "# (...)", "\n", "\n", "\n", "if", "reduce", "==", "'mean'", ":", "\n", "        ", "return", "-", "log_probs", ".", "mean", "(", ")", "/", "3.0", "\n", "", "elif", "reduce", "==", "'none'", ":", "\n", "        ", "return", "-", "log_probs", "\n", "", "else", ":", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.tasks.mixture.mixture_sample": [[277, 300], ["x[].clamp", "temp.max", "torch.gather().squeeze", "torch.gather().squeeze", "torch.gather().squeeze", "torch.gather().squeeze", "torch.gather().squeeze", "torch.gather().squeeze", "torch.gather().squeeze", "torch.gather().squeeze", "torch.log", "torch.log", "torch.log", "torch.log", "torch.rand_like", "torch.rand_like", "torch.rand_like", "torch.rand_like", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.rand_like", "torch.rand_like", "torch.rand_like", "torch.rand_like", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log"], "function", ["None"], ["", "def", "mixture_sample", "(", "x", ")", ":", "\n", "    ", "\"\"\" x: (..., 3*k) mixture params \"\"\"", "\n", "# Pytorch ordering", "\n", "assert", "x", ".", "shape", "[", "-", "1", "]", "%", "3", "==", "0", "\n", "k", "=", "x", ".", "shape", "[", "-", "1", "]", "//", "3", "\n", "\n", "# Unpack outputs", "\n", "mixture_logits", "=", "x", "[", "...", ",", ":", "k", "]", "# (..., k)", "\n", "means", "=", "x", "[", "...", ",", "k", ":", "2", "*", "k", "]", "# (..., k)", "\n", "scales", "=", "x", "[", "...", ",", "2", "*", "k", ":", "3", "*", "k", "]", ".", "clamp", "(", "min", "=", "-", "7.", ")", "# (..., k)", "\n", "\n", "# sample mixture indicator from softmax", "\n", "eps", "=", "1e-8", "\n", "temp", "=", "torch", ".", "rand_like", "(", "means", ")", "*", "(", "1", "-", "2", "*", "eps", ")", "+", "eps", "\n", "temp", "=", "mixture_logits", "-", "torch", ".", "log", "(", "-", "torch", ".", "log", "(", "temp", ")", ")", "\n", "_", ",", "argmax", "=", "temp", ".", "max", "(", "dim", "=", "-", "1", ",", "keepdim", "=", "True", ")", "# (..., 1)", "\n", "\n", "means", "=", "torch", ".", "gather", "(", "means", ",", "-", "1", ",", "argmax", ")", ".", "squeeze", "(", "-", "1", ")", "\n", "scales", "=", "torch", ".", "gather", "(", "scales", ",", "-", "1", ",", "argmax", ")", ".", "squeeze", "(", "-", "1", ")", "\n", "\n", "u", "=", "torch", ".", "rand_like", "(", "means", ")", "*", "(", "1", "-", "2", "*", "eps", ")", "+", "eps", "\n", "x", "=", "means", "+", "(", "torch", ".", "log", "(", "u", ")", "-", "torch", ".", "log", "(", "1.", "-", "u", ")", ")", "/", "torch", ".", "exp", "(", "scales", ")", "# (...)", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.tasks.mixture.piecewise_cdf": [[303, 309], ["torch.relu", "torch.relu"], "function", ["None"], ["", "def", "piecewise_cdf", "(", "x", ")", ":", "\n", "    ", "\"\"\" Piecewise linear function with nodes at (-1, 0) and (1, 1) \"\"\"", "\n", "x", "=", "F", ".", "relu", "(", "1", "+", "x", ")", "-", "1", "\n", "x", "=", "1", "-", "F", ".", "relu", "(", "1", "-", "x", ")", "\n", "x", "=", "(", "x", "+", "1", ")", "/", "2", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.tasks.mixture.pdf": [[310, 327], ["torch.cat", "torch.cat", "torch.cat", "torch.cat", "s.unsqueeze", "m.unsqueeze", "mixture.piecewise_cdf"], "function", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.tasks.mixture.piecewise_cdf"], ["", "def", "pdf", "(", "m", ",", "s", ",", "buckets", ",", "cdf_fn", ")", ":", "\n", "    ", "\"\"\"\n    m: (...) mean\n    s: (...) scale\n    buckets: (..., n-1)\n\n    returns: (..., n)\n    \"\"\"", "\n", "samples", "=", "s", ".", "unsqueeze", "(", "-", "1", ")", "*", "(", "buckets", "-", "m", ".", "unsqueeze", "(", "-", "1", ")", ")", "\n", "# samples = (buckets - m.unsqueeze(-1)) / s.unsqueeze(-1)", "\n", "# samples = s.unsqueeze(-1) * buckets + m.unsqueeze(-1)", "\n", "c", "=", "cdf_fn", "(", "samples", ")", "# (..., b) between 0, 1", "\n", "p0", "=", "c", "[", "...", ",", ":", "1", "]", "# (..., 1)", "\n", "pn", "=", "1.", "-", "c", "[", "...", ",", "-", "1", ":", "]", "# (..., 1)", "\n", "p", "=", "c", "[", "...", ",", "1", ":", "]", "-", "c", "[", "...", ",", ":", "-", "1", "]", "# (..., b-2)", "\n", "probs", "=", "torch", ".", "cat", "(", "[", "p0", ",", "p", ",", "pn", "]", ",", "dim", "=", "-", "1", ")", "# (..., b)", "\n", "return", "probs", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.tasks.mixture.test_mixture_loss": [[354, 366], ["torch.FloatTensor().normal_", "torch.FloatTensor().normal_", "torch.FloatTensor().normal_", "torch.FloatTensor().normal_", "torch.randint", "torch.randint", "torch.randint", "torch.randint", "range", "sum", "print", "print", "mixture.mixture_loss", "ans.append", "torch.max", "torch.max", "torch.max", "torch.max", "torch.min", "torch.min", "torch.min", "torch.min", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.exp", "torch.exp", "torch.exp", "torch.exp"], "function", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.tasks.mixture.mixture_loss"], ["", "", "def", "test_mixture_loss", "(", ")", ":", "\n", "    ", "logits", "=", "torch", ".", "FloatTensor", "(", "5", ",", "1024", ",", "30", ")", ".", "normal_", "(", ")", "\n", "y", "=", "torch", ".", "randint", "(", "0", ",", "256", ",", "(", "5", ",", "1024", ",", "1", ")", ")", "\n", "\n", "ans", "=", "[", "]", "\n", "for", "target", "in", "range", "(", "256", ")", ":", "\n", "        ", "y", "=", "torch", ".", "ones", "(", "5", ",", "1024", ",", "dtype", "=", "torch", ".", "long", ")", "*", "target", "\n", "loss", "=", "mixture_loss", "(", "logits", ",", "y", ",", "reduce", "=", "'none'", ")", "\n", "ans", ".", "append", "(", "torch", ".", "exp", "(", "-", "loss", ")", ")", "\n", "", "total_prob", "=", "sum", "(", "ans", ")", "\n", "print", "(", "torch", ".", "max", "(", "total_prob", ")", ")", "\n", "print", "(", "torch", ".", "min", "(", "total_prob", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.tasks.mixture.test_mixture_function": [[367, 381], ["torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "mixture.pdf", "print", "mixture.Mixture", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "Mixture.", "print", "torch.cat", "torch.cat", "torch.cat", "torch.cat"], "function", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.tasks.mixture.pdf"], ["", "def", "test_mixture_function", "(", ")", ":", "\n", "    ", "m", "=", "torch", ".", "tensor", "(", "[", "0.0", "]", ")", "\n", "s", "=", "torch", ".", "tensor", "(", "[", "1.0", "]", ")", "\n", "buckets", "=", "torch", ".", "tensor", "(", "[", "-", "1.0", ",", "0.0", ",", "1.0", "]", ")", "\n", "\n", "p", "=", "pdf", "(", "m", ",", "s", ",", "buckets", ",", "piecewise_cdf", ")", "\n", "print", "(", "p", ")", "\n", "\n", "mixture", "=", "Mixture", "(", "4", ",", "1.0", ",", "'piecewise'", ")", "\n", "\n", "s", "=", "torch", ".", "tensor", "(", "[", "0.0", "]", ")", "\n", "l", "=", "torch", ".", "tensor", "(", "[", "0.0", "]", ")", "\n", "p", "=", "mixture", "(", "torch", ".", "cat", "(", "[", "m", ",", "s", ",", "l", "]", ",", "dim", "=", "-", "1", ")", ")", "\n", "print", "(", "p", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.tasks.mixture.test_pixelcnn_mixture": [[382, 404], ["torch.randint", "torch.randint", "torch.randint", "torch.randint", "torch.FloatTensor().normal_", "torch.FloatTensor().normal_", "torch.FloatTensor().normal_", "torch.FloatTensor().normal_", "mixture.discretized_mix_logistic_loss_1d", "print", "mixture.mixture_loss", "print", "mixture.Mixture", "torch.cross_entropy", "print", "torch.randint", "torch.randint", "torch.randint", "torch.randint", "torch.FloatTensor().normal_", "torch.FloatTensor().normal_", "torch.FloatTensor().normal_", "torch.FloatTensor().normal_", "mixture.discretized_mix_logistic_loss_3d", "print", "mixture.mixture_loss_kd", "print", "torch.randint.squeeze", "Mixture.reshape", "torch.randint.view", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "Mixture."], "function", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.tasks.mixture.discretized_mix_logistic_loss_1d", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.tasks.mixture.mixture_loss", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.tasks.metrics.cross_entropy", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.tasks.mixture.discretized_mix_logistic_loss_3d", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.tasks.mixture.mixture_loss_kd"], ["", "def", "test_pixelcnn_mixture", "(", ")", ":", "\n", "# x = torch.FloatTensor(5, 1024, 1).uniform_(-1., 1.)", "\n", "    ", "y", "=", "torch", ".", "randint", "(", "0", ",", "256", ",", "(", "5", ",", "1024", ",", "1", ")", ")", "\n", "x", "=", "(", "y", "-", "255", "/", "2", ")", "/", "(", "255", "/", "2", ")", "\n", "logits", "=", "torch", ".", "FloatTensor", "(", "5", ",", "1024", ",", "30", ")", ".", "normal_", "(", ")", "\n", "loss", "=", "discretized_mix_logistic_loss_1d", "(", "x", ",", "logits", ")", "\n", "print", "(", "loss", ")", "\n", "loss", "=", "mixture_loss", "(", "logits", ",", "y", ".", "squeeze", "(", "-", "1", ")", ")", "\n", "print", "(", "loss", ")", "\n", "\n", "mixture", "=", "Mixture", "(", "256", ",", "2.0", ",", "'sigmoid'", ")", "\n", "loss", "=", "F", ".", "cross_entropy", "(", "mixture", "(", "logits", ")", ".", "reshape", "(", "-", "1", ",", "256", ")", ",", "y", ".", "view", "(", "-", "1", ")", ")", "\n", "print", "(", "loss", ")", "\n", "\n", "y", "=", "torch", ".", "randint", "(", "0", ",", "256", ",", "(", "5", ",", "32", ",", "32", ",", "3", ")", ")", "\n", "x", "=", "(", "y", "-", "255", "/", "2", ")", "/", "(", "255", "/", "2", ")", "\n", "# x = torch.FloatTensor(5, 32, 32, 3).uniform_(-1., 1.)", "\n", "logits", "=", "torch", ".", "FloatTensor", "(", "5", ",", "32", ",", "32", ",", "30", ")", ".", "normal_", "(", ")", "\n", "loss", "=", "discretized_mix_logistic_loss_3d", "(", "x", ",", "logits", ")", "\n", "print", "(", "loss", ")", "\n", "loss", "=", "mixture_loss_kd", "(", "logits", ",", "y", ")", "\n", "print", "(", "loss", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.tasks.mixture.test_mixture_sample": [[405, 415], ["torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.full", "torch.full", "torch.full", "torch.full", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "mixture.mixture_sample", "print", "torch.linspace.repeat"], "function", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.tasks.mixture.mixture_sample"], ["", "def", "test_mixture_sample", "(", ")", ":", "\n", "    ", "B", "=", "8", "\n", "k", "=", "5", "\n", "# x = torch.rand(B, 3*k)", "\n", "means", "=", "torch", ".", "linspace", "(", "-", "1.0", ",", "1.0", ",", "k", ")", "\n", "scales", "=", "torch", ".", "full", "(", "(", "B", ",", "k", ")", ",", "5.0", ")", "# Higher scale means more confident", "\n", "logits", "=", "torch", ".", "zeros", "(", "B", ",", "k", ")", "\n", "x", "=", "torch", ".", "cat", "(", "[", "logits", ",", "means", ".", "repeat", "(", "B", ",", "1", ")", ",", "scales", "]", ",", "dim", "=", "-", "1", ")", "\n", "samples", "=", "mixture_sample", "(", "x", ")", "\n", "print", "(", "samples", ".", "shape", ",", "samples", ")", "# Should see values close to -1, -.5, 0, .5, 1", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.tasks.metrics.binary_cross_entropy": [[8, 12], ["torch.binary_cross_entropy_with_logits", "logits.squeeze", "y.float"], "function", ["None"], ["def", "binary_cross_entropy", "(", "logits", ",", "y", ")", ":", "\n", "# BCE loss requires squeezing last dimension of logits so it has the same shape as y", "\n", "# requires y to be float, since it's overloaded to represent a probability", "\n", "    ", "return", "F", ".", "binary_cross_entropy_with_logits", "(", "logits", ".", "squeeze", "(", "-", "1", ")", ",", "y", ".", "float", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.tasks.metrics.binary_accuracy": [[14, 16], ["torch.eq().float().mean", "torch.eq().float().mean", "torch.eq().float", "torch.eq().float", "torch.eq", "torch.eq", "logits.squeeze"], "function", ["None"], ["", "def", "binary_accuracy", "(", "logits", ",", "y", ")", ":", "\n", "    ", "return", "torch", ".", "eq", "(", "logits", ".", "squeeze", "(", "-", "1", ")", ">=", "0", ",", "y", ")", ".", "float", "(", ")", ".", "mean", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.tasks.metrics.cross_entropy": [[18, 22], ["logits.view.view", "y.view.view", "torch.cross_entropy"], "function", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.tasks.metrics.cross_entropy"], ["", "def", "cross_entropy", "(", "logits", ",", "y", ")", ":", "\n", "    ", "logits", "=", "logits", ".", "view", "(", "-", "1", ",", "logits", ".", "shape", "[", "-", "1", "]", ")", "\n", "y", "=", "y", ".", "view", "(", "-", "1", ")", "\n", "return", "F", ".", "cross_entropy", "(", "logits", ",", "y", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.tasks.metrics.accuracy": [[24, 31], ["logits.view.view", "y.argmax.view", "torch.eq().float().mean", "torch.eq().float().mean", "y.argmax.numel", "y.argmax.argmax", "torch.eq().float", "torch.eq().float", "torch.eq", "torch.eq", "torch.argmax", "torch.argmax"], "function", ["None"], ["", "def", "accuracy", "(", "logits", ",", "y", ")", ":", "\n", "    ", "logits", "=", "logits", ".", "view", "(", "-", "1", ",", "logits", ".", "shape", "[", "-", "1", "]", ")", "\n", "if", "y", ".", "numel", "(", ")", ">", "logits", ".", "shape", "[", "0", "]", ":", "\n", "# Mixup leads to this case: use argmax class", "\n", "        ", "y", "=", "y", ".", "argmax", "(", "dim", "=", "-", "1", ")", "\n", "", "y", "=", "y", ".", "view", "(", "-", "1", ")", "\n", "return", "torch", ".", "eq", "(", "torch", ".", "argmax", "(", "logits", ",", "dim", "=", "-", "1", ")", ",", "y", ")", ".", "float", "(", ")", ".", "mean", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.tasks.metrics.accuracy_at_k": [[33, 40], ["logits.view.view", "y.argmax.view", "[].eq().any().float().mean", "y.argmax.numel", "y.argmax.argmax", "[].eq().any().float", "[].eq().any", "[].eq", "y.argmax.unsqueeze", "torch.topk", "torch.topk"], "function", ["None"], ["", "def", "accuracy_at_k", "(", "logits", ",", "y", ",", "k", "=", "1", ")", ":", "\n", "    ", "logits", "=", "logits", ".", "view", "(", "-", "1", ",", "logits", ".", "shape", "[", "-", "1", "]", ")", "\n", "if", "y", ".", "numel", "(", ")", ">", "logits", ".", "shape", "[", "0", "]", ":", "\n", "# Mixup leads to this case: use argmax class", "\n", "        ", "y", "=", "y", ".", "argmax", "(", "dim", "=", "-", "1", ")", "\n", "", "y", "=", "y", ".", "view", "(", "-", "1", ")", "\n", "return", "torch", ".", "topk", "(", "logits", ",", "k", ",", "dim", "=", "-", "1", ")", "[", "1", "]", ".", "eq", "(", "y", ".", "unsqueeze", "(", "-", "1", ")", ")", ".", "any", "(", "dim", "=", "-", "1", ")", ".", "float", "(", ")", ".", "mean", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.tasks.metrics.f1_binary": [[41, 46], ["logits.view.view", "y.view.view", "torch.argmax", "torch.argmax", "sklearn.metrics.f1_score", "y.view.cpu().numpy", "torch.argmax.cpu().numpy", "y.view.cpu", "torch.argmax.cpu"], "function", ["None"], ["", "def", "f1_binary", "(", "logits", ",", "y", ")", ":", "\n", "    ", "logits", "=", "logits", ".", "view", "(", "-", "1", ",", "logits", ".", "shape", "[", "-", "1", "]", ")", "\n", "y", "=", "y", ".", "view", "(", "-", "1", ")", "\n", "y_hat", "=", "torch", ".", "argmax", "(", "logits", ",", "dim", "=", "-", "1", ")", "\n", "return", "f1_score", "(", "y", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ",", "y_hat", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ",", "average", "=", "\"binary\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.tasks.metrics.f1_macro": [[48, 53], ["logits.view.view", "y.view.view", "torch.argmax", "torch.argmax", "sklearn.metrics.f1_score", "y.view.cpu().numpy", "torch.argmax.cpu().numpy", "y.view.cpu", "torch.argmax.cpu"], "function", ["None"], ["", "def", "f1_macro", "(", "logits", ",", "y", ")", ":", "\n", "    ", "logits", "=", "logits", ".", "view", "(", "-", "1", ",", "logits", ".", "shape", "[", "-", "1", "]", ")", "\n", "y", "=", "y", ".", "view", "(", "-", "1", ")", "\n", "y_hat", "=", "torch", ".", "argmax", "(", "logits", ",", "dim", "=", "-", "1", ")", "\n", "return", "f1_score", "(", "y", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ",", "y_hat", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ",", "average", "=", "\"macro\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.tasks.metrics.f1_micro": [[55, 60], ["logits.view.view", "y.view.view", "torch.argmax", "torch.argmax", "sklearn.metrics.f1_score", "y.view.cpu().numpy", "torch.argmax.cpu().numpy", "y.view.cpu", "torch.argmax.cpu"], "function", ["None"], ["", "def", "f1_micro", "(", "logits", ",", "y", ")", ":", "\n", "    ", "logits", "=", "logits", ".", "view", "(", "-", "1", ",", "logits", ".", "shape", "[", "-", "1", "]", ")", "\n", "y", "=", "y", ".", "view", "(", "-", "1", ")", "\n", "y_hat", "=", "torch", ".", "argmax", "(", "logits", ",", "dim", "=", "-", "1", ")", "\n", "return", "f1_score", "(", "y", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ",", "y_hat", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ",", "average", "=", "\"micro\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.tasks.metrics.roc_auc_macro": [[62, 69], ["logits.view().detach.view().detach", "y.view.view", "sklearn.metrics.roc_auc_score", "y.view.cpu().numpy", "logits.view().detach.view", "torch.softmax().cpu().numpy", "y.view.cpu", "torch.softmax().cpu", "torch.softmax"], "function", ["None"], ["", "def", "roc_auc_macro", "(", "logits", ",", "y", ")", ":", "\n", "    ", "logits", "=", "logits", ".", "view", "(", "\n", "-", "1", ",", "logits", ".", "shape", "[", "-", "1", "]", "\n", ")", ".", "detach", "(", ")", "# KS: had to add detach to eval while training", "\n", "y", "=", "y", ".", "view", "(", "-", "1", ")", "\n", "return", "roc_auc_score", "(", "\n", "y", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ",", "F", ".", "softmax", "(", "logits", ",", "dim", "=", "-", "1", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "[", ":", ",", "1", "]", ",", "average", "=", "\"macro\"", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.tasks.metrics.roc_auc_micro": [[72, 77], ["logits.view.view", "y.view.view", "sklearn.metrics.roc_auc_score", "y.view.cpu().numpy", "torch.softmax().cpu().numpy", "y.view.cpu", "torch.softmax().cpu", "torch.softmax"], "function", ["None"], ["", "def", "roc_auc_micro", "(", "logits", ",", "y", ")", ":", "\n", "    ", "logits", "=", "logits", ".", "view", "(", "-", "1", ",", "logits", ".", "shape", "[", "-", "1", "]", ")", "\n", "y", "=", "y", ".", "view", "(", "-", "1", ")", "\n", "return", "roc_auc_score", "(", "\n", "y", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ",", "F", ".", "softmax", "(", "logits", ",", "dim", "=", "-", "1", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "[", ":", ",", "1", "]", ",", "average", "=", "\"micro\"", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.tasks.metrics.mse": [[80, 96], ["len", "len", "outs.squeeze.squeeze", "torch.mse_loss", "torch.zeros_like", "torch.zeros_like", "enumerate", "torch.masked_select", "torch.masked_select", "torch.masked_select", "torch.masked_select", "torch.mse_loss"], "function", ["None"], ["", "def", "mse", "(", "outs", ",", "y", ",", "len_batch", "=", "None", ")", ":", "\n", "# assert outs.shape[:-1] == y.shape and outs.shape[-1] == 1", "\n", "# outs = outs.squeeze(-1)", "\n", "    ", "if", "len", "(", "y", ".", "shape", ")", "<", "len", "(", "outs", ".", "shape", ")", ":", "\n", "        ", "assert", "outs", ".", "shape", "[", "-", "1", "]", "==", "1", "\n", "outs", "=", "outs", ".", "squeeze", "(", "-", "1", ")", "\n", "", "if", "len_batch", "is", "None", ":", "\n", "        ", "return", "F", ".", "mse_loss", "(", "outs", ",", "y", ")", "\n", "", "else", ":", "\n", "# Computes the loss of the first `lens` items in the batches", "\n", "        ", "mask", "=", "torch", ".", "zeros_like", "(", "outs", ",", "dtype", "=", "torch", ".", "bool", ")", "\n", "for", "i", ",", "l", "in", "enumerate", "(", "len_batch", ")", ":", "\n", "            ", "mask", "[", "i", ",", ":", "l", ",", ":", "]", "=", "1", "\n", "", "outs_masked", "=", "torch", ".", "masked_select", "(", "outs", ",", "mask", ")", "\n", "y_masked", "=", "torch", ".", "masked_select", "(", "y", ",", "mask", ")", "\n", "return", "F", ".", "mse_loss", "(", "outs_masked", ",", "y_masked", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.tasks.metrics.mae": [[98, 114], ["len", "len", "outs.squeeze.squeeze", "torch.l1_loss", "torch.zeros_like", "torch.zeros_like", "enumerate", "torch.masked_select", "torch.masked_select", "torch.masked_select", "torch.masked_select", "torch.l1_loss"], "function", ["None"], ["", "", "def", "mae", "(", "outs", ",", "y", ",", "len_batch", "=", "None", ")", ":", "\n", "# assert outs.shape[:-1] == y.shape and outs.shape[-1] == 1", "\n", "# outs = outs.squeeze(-1)", "\n", "    ", "if", "len", "(", "y", ".", "shape", ")", "<", "len", "(", "outs", ".", "shape", ")", ":", "\n", "        ", "assert", "outs", ".", "shape", "[", "-", "1", "]", "==", "1", "\n", "outs", "=", "outs", ".", "squeeze", "(", "-", "1", ")", "\n", "", "if", "len_batch", "is", "None", ":", "\n", "        ", "return", "F", ".", "l1_loss", "(", "outs", ",", "y", ")", "\n", "", "else", ":", "\n", "# Computes the loss of the first `lens` items in the batches", "\n", "        ", "mask", "=", "torch", ".", "zeros_like", "(", "outs", ",", "dtype", "=", "torch", ".", "bool", ")", "\n", "for", "i", ",", "l", "in", "enumerate", "(", "len_batch", ")", ":", "\n", "            ", "mask", "[", "i", ",", ":", "l", ",", ":", "]", "=", "1", "\n", "", "outs_masked", "=", "torch", ".", "masked_select", "(", "outs", ",", "mask", ")", "\n", "y_masked", "=", "torch", ".", "masked_select", "(", "y", ",", "mask", ")", "\n", "return", "F", ".", "l1_loss", "(", "outs_masked", ",", "y_masked", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.tasks.metrics.loss": [[117, 120], ["loss_fn"], "function", ["None"], ["", "", "def", "loss", "(", "x", ",", "y", ",", "loss_fn", ")", ":", "\n", "    ", "\"\"\" This metric may be useful because the training loss may add extra regularization (e.g. weight decay implemented as L2 penalty), while adding this as a metric skips the additional losses \"\"\"", "\n", "return", "loss_fn", "(", "x", ",", "y", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.tasks.metrics.rmse": [[122, 124], ["loss_fn"], "function", ["None"], ["", "def", "rmse", "(", "x", ",", "y", ",", "loss_fn", ")", ":", "\n", "    ", "return", "loss_fn", "(", "x", ",", "y", ")", "**", "0.5", "# NOTE this isn't exactly correct", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.tasks.metrics.bpb": [[126, 129], ["loss_fn", "math.log"], "function", ["None"], ["", "def", "bpb", "(", "x", ",", "y", ",", "loss_fn", ")", ":", "\n", "    ", "\"\"\" bits per byte (image density estimation, speech generation, char LM) \"\"\"", "\n", "return", "loss_fn", "(", "x", ",", "y", ")", "/", "math", ".", "log", "(", "2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.tasks.metrics.ppl": [[131, 133], ["torch.exp", "torch.exp", "loss_fn"], "function", ["None"], ["", "def", "ppl", "(", "x", ",", "y", ",", "loss_fn", ")", ":", "\n", "    ", "return", "torch", ".", "exp", "(", "loss_fn", "(", "x", ",", "y", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.tasks.encoders.Encoder.forward": [[20, 30], ["None"], "methods", ["None"], ["def", "forward", "(", "self", ",", "x", ",", "*", "args", ")", ":", "\n", "        ", "\"\"\"\n        x: input tensor\n        *args: additional info from the dataset (e.g. sequence lengths)\n\n        Returns:\n        y: output tensor\n        *args: other arguments to pass into the model backbone\n        \"\"\"", "\n", "return", "(", "x", ",", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.tasks.encoders.PositionalEncoder.__init__": [[50, 69], ["torch.nn.Module.__init__", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.init.normal_", "torch.nn.init.normal_", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.sin", "torch.sin", "torch.sin", "torch.sin", "torch.cos", "torch.cos", "torch.cos", "torch.cos", "encoders.PositionalEncoder.register_buffer", "torch.empty", "torch.empty", "torch.empty", "torch.empty", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "math.log"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__"], ["def", "__init__", "(", "self", ",", "d_model", ",", "dropout", "=", "0.1", ",", "max_len", "=", "16384", ",", "pe_init", "=", "None", ",", "causal", "=", "True", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "p", "=", "dropout", ")", "\n", "if", "pe_init", "is", "not", "None", ":", "\n", "            ", "self", ".", "pe", "=", "nn", ".", "Parameter", "(", "torch", ".", "empty", "(", "max_len", ",", "1", ",", "d_model", ")", ")", "\n", "nn", ".", "init", ".", "normal_", "(", "self", ".", "pe", ",", "0", ",", "pe_init", ")", "\n", "# self.pe = pe.unsqueeze(1)", "\n", "", "else", ":", "\n", "            ", "pe", "=", "torch", ".", "zeros", "(", "max_len", ",", "d_model", ")", "\n", "position", "=", "torch", ".", "arange", "(", "0.0", ",", "max_len", ")", ".", "unsqueeze", "(", "1", ")", "\n", "div_term", "=", "torch", ".", "exp", "(", "\n", "-", "math", ".", "log", "(", "10000.0", ")", "*", "torch", ".", "arange", "(", "0.0", ",", "d_model", ",", "2.0", ")", "/", "d_model", "\n", ")", "\n", "pe", "[", ":", ",", "0", ":", ":", "2", "]", "=", "torch", ".", "sin", "(", "position", "*", "div_term", ")", "\n", "pe", "[", ":", ",", "1", ":", ":", "2", "]", "=", "torch", ".", "cos", "(", "position", "*", "div_term", ")", "\n", "# pe = pe.unsqueeze(1) # Comment this out to handle (B, L, D) instead of (L, B, D)", "\n", "self", ".", "register_buffer", "(", "\"pe\"", ",", "pe", ")", "\n", "\n", "", "self", ".", "attn_mask", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.tasks.encoders.PositionalEncoder.forward": [[70, 108], ["encoders.PositionalEncoder.dropout", "x.size"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ",", "seq_len", "=", "None", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "r\"\"\"Inputs of forward function\n        Args:\n            x: the sequence fed to the positional encoder model (required).\n            lens: actual lengths of sequences\n        Shape:\n            x: [l_sequence, n_batch, d_model]\n            Returns: [l_sequence, n_batch, d_model]\n            attn_mask: [l_sequence, l_sequence]\n            padding_mask:\n        \"\"\"", "\n", "\n", "# TODO currently not used, but maybe will be someday", "\n", "# e.g. attn_mask is defined directly in each attention layer", "\n", "\n", "# if self.attn_mask is None or self.attn_mask.shape[-1] != x.size(-2):", "\n", "#     # self.attn_mask = TriangularCausalMask(len(src), device=src.device)", "\n", "#     self.attn_mask = torch.triu(torch.ones(x.size(-2), x.size(-2),", "\n", "#                                           dtype=torch.bool, device=x.device),", "\n", "#                                diagonal=1)", "\n", "\n", "# padding_mask = None", "\n", "# if seq_len is not None and seq_len < x.size(-2):", "\n", "#     padding_mask = LengthMask(", "\n", "#         torch.full(", "\n", "#             (x.size(-2),),", "\n", "#             seq_len,", "\n", "#             device=x.device,", "\n", "#             dtype=torch.long,", "\n", "#         ),", "\n", "#         max_len=x.size(-2),", "\n", "#     )", "\n", "# else:", "\n", "#     padding_mask = None", "\n", "\n", "x", "=", "x", "+", "self", ".", "pe", "[", ":", "x", ".", "size", "(", "-", "2", ")", "]", "\n", "# return self.dropout(x), self.attn_mask, padding_mask", "\n", "return", "(", "self", ".", "dropout", "(", "x", ")", ",", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.tasks.encoders.ClassEmbedding.__init__": [[112, 115], ["torch.nn.Module.__init__", "torch.nn.Embedding", "torch.nn.Embedding"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__"], ["    ", "def", "__init__", "(", "self", ",", "n_classes", ",", "d_model", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "embedding", "=", "nn", ".", "Embedding", "(", "n_classes", ",", "d_model", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.tasks.encoders.ClassEmbedding.forward": [[116, 119], ["encoders.ClassEmbedding.embedding().unsqueeze", "encoders.ClassEmbedding.embedding"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ",", "y", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "x", "=", "x", "+", "self", ".", "embedding", "(", "y", ")", ".", "unsqueeze", "(", "-", "2", ")", "# (B, L, D)", "\n", "return", "(", "x", ",", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.tasks.encoders.Conv1DEncoder.__init__": [[122, 130], ["torch.nn.Module.__init__", "torch.nn.Conv1d", "torch.nn.Conv1d"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__"], ["    ", "def", "__init__", "(", "self", ",", "d_input", ",", "d_model", ",", "kernel_size", ",", "stride", ",", "padding", "=", "0", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "conv", "=", "nn", ".", "Conv1d", "(", "\n", "in_channels", "=", "d_input", ",", "\n", "out_channels", "=", "d_model", ",", "\n", "kernel_size", "=", "kernel_size", ",", "\n", "stride", "=", "stride", ",", "\n", "padding", "=", "padding", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.tasks.encoders.Conv1DEncoder.forward": [[132, 136], ["encoders.Conv1DEncoder.conv().transpose", "encoders.Conv1DEncoder.conv", "encoders.Conv1DEncoder.transpose"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ",", "*", "args", ")", ":", "\n", "# BLD -> BLD", "\n", "        ", "x", "=", "self", ".", "conv", "(", "x", ".", "transpose", "(", "1", ",", "2", ")", ")", ".", "transpose", "(", "1", ",", "2", ")", "\n", "return", "(", "x", ",", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.tasks.encoders.TimeEncoder.__init__": [[139, 150], ["torch.nn.Module.__init__", "torch.nn.Embedding", "torch.nn.Embedding", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.Linear", "torch.nn.Linear", "len", "torch.nn.Embedding", "torch.nn.Embedding"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__"], ["    ", "def", "__init__", "(", "self", ",", "n_tokens_time", ",", "d_model", ",", "timeenc", "=", "0", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "timeenc", "=", "timeenc", "\n", "if", "self", ".", "timeenc", "==", "0", ":", "\n", "            ", "self", ".", "encoders", "=", "nn", ".", "ModuleList", "(", "\n", "[", "nn", ".", "Embedding", "(", "v", ",", "d_model", ")", "for", "v", "in", "n_tokens_time", "]", "\n", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "encoders", "=", "nn", ".", "Linear", "(", "len", "(", "n_tokens_time", ")", ",", "d_model", ")", "\n", "", "self", ".", "mask_embed", "=", "nn", ".", "Embedding", "(", "2", ",", "d_model", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.tasks.encoders.TimeEncoder.forward": [[151, 162], ["encoders.TimeEncoder.mask_embed", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "encoders.TimeEncoder.encoders", "mask.squeeze", "mark.size", "len", "embed", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "zip", "torch.unbind", "torch.unbind", "torch.unbind", "torch.unbind"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ",", "mark", ",", "mask", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "if", "self", ".", "timeenc", "==", "0", ":", "\n", "            ", "assert", "mark", ".", "size", "(", "-", "1", ")", "==", "len", "(", "self", ".", "encoders", ")", "\n", "embeddings", "=", "[", "\n", "embed", "(", "z", ")", "for", "embed", ",", "z", "in", "zip", "(", "self", ".", "encoders", ",", "torch", ".", "unbind", "(", "mark", ",", "dim", "=", "-", "1", ")", ")", "\n", "]", "\n", "time_encode", "=", "torch", ".", "sum", "(", "torch", ".", "stack", "(", "embeddings", ")", ",", "dim", "=", "0", ")", "\n", "", "else", ":", "\n", "            ", "time_encode", "=", "self", ".", "encoders", "(", "mark", ")", "\n", "", "mask_encode", "=", "self", ".", "mask_embed", "(", "mask", ".", "squeeze", "(", "-", "1", ")", ")", "\n", "return", "(", "x", "+", "time_encode", "+", "mask_encode", ",", ")", "# (B, L, d_model)", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.tasks.encoders.PackedEncoder.forward": [[165, 174], ["torch.nn.utils.rnn.pack_padded_sequence", "torch.nn.utils.rnn.pack_padded_sequence", "len_batch.cpu"], "methods", ["None"], ["    ", "def", "forward", "(", "self", ",", "x", ",", "len_batch", "=", "None", ")", ":", "\n", "        ", "assert", "len_batch", "is", "not", "None", "\n", "x", "=", "nn", ".", "utils", ".", "rnn", ".", "pack_padded_sequence", "(", "\n", "x", ",", "\n", "len_batch", ".", "cpu", "(", ")", ",", "\n", "enforce_sorted", "=", "False", ",", "\n", "batch_first", "=", "True", ",", "\n", ")", "\n", "return", "(", "x", ",", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.tasks.encoders.OneHotEncoder.__init__": [[177, 181], ["torch.nn.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__"], ["    ", "def", "__init__", "(", "self", ",", "n_tokens", ",", "d_model", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "assert", "n_tokens", "<=", "d_model", "\n", "self", ".", "d_model", "=", "d_model", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.tasks.encoders.OneHotEncoder.forward": [[182, 184], ["torch.one_hot().float", "torch.one_hot().float", "torch.one_hot", "torch.one_hot", "x.squeeze"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "return", "(", "F", ".", "one_hot", "(", "x", ".", "squeeze", "(", "-", "1", ")", ",", "self", ".", "d_model", ")", ".", "float", "(", ")", ",", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.tasks.encoders._instantiate": [[221, 239], ["isinstance", "src.config.extract_attrs_from_obj", "src.config.extract_attrs_from_obj", "src.instantiate", "src.Identity", "dataset_attrs.get", "model_attrs.get"], "function", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.utils.config.extract_attrs_from_obj", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.utils.config.extract_attrs_from_obj", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.tasks.decoders.instantiate"], ["def", "_instantiate", "(", "encoder", ",", "dataset", "=", "None", ",", "model", "=", "None", ")", ":", "\n", "    ", "\"\"\"Instantiate a single encoder\"\"\"", "\n", "if", "encoder", "is", "None", ":", "\n", "        ", "return", "U", ".", "Identity", "(", ")", "\n", "", "if", "isinstance", "(", "encoder", ",", "str", ")", ":", "\n", "        ", "name", "=", "encoder", "\n", "", "else", ":", "\n", "        ", "name", "=", "encoder", "[", "\"_name_\"", "]", "\n", "\n", "# Extract dataset/model arguments from attribute names", "\n", "", "dataset_args", "=", "utils", ".", "config", ".", "extract_attrs_from_obj", "(", "\n", "dataset", ",", "*", "dataset_attrs", ".", "get", "(", "name", ",", "[", "]", ")", "\n", ")", "\n", "model_args", "=", "utils", ".", "config", ".", "extract_attrs_from_obj", "(", "model", ",", "*", "model_attrs", ".", "get", "(", "name", ",", "[", "]", ")", ")", "\n", "\n", "# Instantiate encoder", "\n", "obj", "=", "utils", ".", "instantiate", "(", "registry", ",", "encoder", ",", "*", "dataset_args", ",", "*", "model_args", ")", "\n", "return", "obj", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.tasks.encoders.instantiate": [[241, 245], ["src.to_list", "src.TupleSequential", "encoders._instantiate"], "function", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.utils.config.to_list", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.tasks.decoders._instantiate"], ["", "def", "instantiate", "(", "encoder", ",", "dataset", "=", "None", ",", "model", "=", "None", ")", ":", "\n", "    ", "encoder", "=", "utils", ".", "to_list", "(", "encoder", ")", "\n", "return", "U", ".", "TupleSequential", "(", "\n", "*", "[", "_instantiate", "(", "e", ",", "dataset", "=", "dataset", ",", "model", "=", "model", ")", "for", "e", "in", "encoder", "]", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.tasks.decoders.Decoder.forward": [[17, 28], ["None"], "methods", ["None"], ["def", "forward", "(", "x", ",", "y", ",", "state", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"\n        x: input tensor\n        state: additional state from the model backbone\n        *args, **kwargs: additional info from the dataset\n\n        Returns:\n        y: output tensor\n        *args: other arguments to pass into the loss function\n        \"\"\"", "\n", "return", "x", ",", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.tasks.decoders.SequenceDecoder.__init__": [[30, 52], ["torch.Module.__init__", "torch.Identity", "torch.Identity", "torch.Identity", "torch.Linear", "torch.Linear", "torch.Linear"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__"], ["    ", "def", "__init__", "(", "self", ",", "d_model", ",", "d_output", "=", "None", ",", "l_output", "=", "None", ",", "use_lengths", "=", "False", ",", "mode", "=", "'last'", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "output_transform", "=", "nn", ".", "Identity", "(", ")", "if", "d_output", "is", "None", "else", "nn", ".", "Linear", "(", "d_model", ",", "d_output", ")", "\n", "\n", "if", "l_output", "is", "None", ":", "\n", "            ", "self", ".", "l_output", "=", "None", "\n", "self", ".", "squeeze", "=", "False", "\n", "", "elif", "l_output", "==", "0", ":", "\n", "# Equivalent to getting an output of length 1 and then squeezing", "\n", "            ", "self", ".", "l_output", "=", "1", "\n", "self", ".", "squeeze", "=", "True", "\n", "", "else", ":", "\n", "            ", "assert", "l_output", ">", "0", "\n", "self", ".", "l_output", "=", "l_output", "\n", "self", ".", "squeeze", "=", "False", "\n", "\n", "", "self", ".", "use_lengths", "=", "use_lengths", "\n", "self", ".", "mode", "=", "mode", "\n", "\n", "if", "mode", "==", "'ragged'", ":", "\n", "            ", "assert", "not", "use_lengths", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.tasks.decoders.SequenceDecoder.forward": [[53, 114], ["decoders.SequenceDecoder.output_transform", "isinstance", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "decoders.SequenceDecoder.forward.restrict"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "x", ",", "state", ",", "l_batch", "=", "None", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"\n        x: (n_batch, l_seq, d_model)\n        Returns: (n_batch, l_output, d_output)\n        \"\"\"", "\n", "\n", "\n", "if", "self", ".", "l_output", "is", "None", ":", "\n", "            ", "if", "isinstance", "(", "l_batch", ",", "int", ")", ":", "# Override by pass in", "\n", "                ", "l_output", "=", "l_batch", "\n", "", "else", ":", "\n", "# Grab entire output", "\n", "                ", "l_output", "=", "x", ".", "size", "(", "-", "2", ")", "\n", "", "squeeze", "=", "False", "\n", "", "else", ":", "\n", "            ", "l_output", "=", "self", ".", "l_output", "\n", "squeeze", "=", "self", ".", "squeeze", "\n", "\n", "", "if", "self", ".", "mode", "==", "'last'", ":", "\n", "            ", "restrict", "=", "lambda", "x", ":", "x", "[", "...", ",", "-", "l_output", ":", ",", ":", "]", "\n", "", "elif", "self", ".", "mode", "==", "'first'", ":", "\n", "            ", "restrict", "=", "lambda", "x", ":", "x", "[", "...", ",", ":", "l_output", ",", ":", "]", "\n", "", "elif", "self", ".", "mode", "==", "'pool'", ":", "\n", "            ", "restrict", "=", "lambda", "x", ":", "(", "torch", ".", "cumsum", "(", "x", ",", "dim", "=", "-", "2", ")", "/", "torch", ".", "arange", "(", "1", ",", "1", "+", "x", ".", "size", "(", "-", "2", ")", ",", "device", "=", "x", ".", "device", ",", "dtype", "=", "x", ".", "dtype", ")", ".", "unsqueeze", "(", "-", "1", ")", ")", "[", "...", ",", "-", "l_output", ":", ",", ":", "]", "\n", "def", "restrict", "(", "x", ")", ":", "\n", "                ", "L", "=", "x", ".", "size", "(", "-", "2", ")", "\n", "s", "=", "x", ".", "sum", "(", "dim", "=", "-", "2", ",", "keepdim", "=", "True", ")", "\n", "if", "l_output", ">", "1", ":", "\n", "                    ", "c", "=", "torch", ".", "cumsum", "(", "x", "[", "...", ",", "-", "(", "l_output", "-", "1", ")", ":", ",", ":", "]", ".", "flip", "(", "-", "2", ")", ",", "dim", "=", "-", "2", ")", "\n", "c", "=", "F", ".", "pad", "(", "c", ",", "(", "0", ",", "0", ",", "1", ",", "0", ")", ")", "\n", "s", "=", "s", "-", "c", "# (B, l_output, D)", "\n", "s", "=", "s", ".", "flip", "(", "-", "2", ")", "\n", "", "denom", "=", "torch", ".", "arange", "(", "L", "-", "l_output", "+", "1", ",", "L", "+", "1", ",", "dtype", "=", "x", ".", "dtype", ",", "device", "=", "x", ".", "device", ")", "\n", "s", "=", "s", "/", "denom", "\n", "return", "s", "\n", "", "", "elif", "self", ".", "mode", "==", "'sum'", ":", "\n", "            ", "restrict", "=", "lambda", "x", ":", "torch", ".", "cumsum", "(", "x", ",", "dim", "=", "-", "2", ")", "[", "...", ",", "-", "l_output", ":", ",", ":", "]", "\n", "# TODO use same restrict function as pool case\\", "\n", "", "elif", "self", ".", "mode", "==", "'ragged'", ":", "\n", "            ", "assert", "l_batch", "is", "not", "None", ",", "\"l_batch must be provided for ragged mode\"", "\n", "# remove any additional padding (beyond max length of any sequence in the batch)", "\n", "restrict", "=", "lambda", "x", ":", "x", "[", "...", ",", ":", "max", "(", "l_batch", ")", ",", ":", "]", "\n", "", "else", ":", "raise", "NotImplementedError", "(", "\"Mode must be ['last' | 'first' | 'pool' | 'sum']\"", ")", "\n", "\n", "# Restrict to actual length of sequence", "\n", "if", "self", ".", "use_lengths", ":", "\n", "            ", "assert", "l_batch", "is", "not", "None", "\n", "x", "=", "torch", ".", "stack", "(", "[", "\n", "restrict", "(", "out", "[", "...", ",", ":", "length", ",", ":", "]", ")", "\n", "for", "out", ",", "length", "\n", "in", "zip", "(", "torch", ".", "unbind", "(", "x", ",", "dim", "=", "0", ")", ",", "l_batch", ")", "\n", "]", ",", "dim", "=", "0", ")", "\n", "", "else", ":", "\n", "            ", "x", "=", "restrict", "(", "x", ")", "\n", "\n", "", "if", "squeeze", ":", "\n", "            ", "assert", "x", ".", "size", "(", "-", "2", ")", "==", "1", "\n", "x", "=", "x", ".", "squeeze", "(", "-", "2", ")", "\n", "\n", "", "x", "=", "self", ".", "output_transform", "(", "x", ")", "\n", "return", "x", ",", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.tasks.decoders.StateDecoder.__init__": [[117, 121], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "torch.Linear"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__"], ["def", "__init__", "(", "self", ",", "d_model", ",", "state_to_tensor", ",", "d_output", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "output_transform", "=", "nn", ".", "Linear", "(", "d_model", ",", "d_output", ")", "\n", "self", ".", "state_transform", "=", "state_to_tensor", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.tasks.decoders.StateDecoder.forward": [[122, 124], ["decoders.StateDecoder.output_transform", "decoders.StateDecoder.state_transform"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ",", "state", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "return", "self", ".", "output_transform", "(", "self", ".", "state_transform", "(", "state", ")", ")", ",", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.tasks.decoders.RetrievalHead.__init__": [[126, 149], ["torch.Module.__init__", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.GELU", "torch.GELU", "torch.GELU", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__"], ["    ", "def", "__init__", "(", "self", ",", "d_input", ",", "d_model", ",", "n_classes", ",", "nli", "=", "True", ",", "activation", "=", "'relu'", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "nli", "=", "nli", "\n", "\n", "if", "activation", "==", "'relu'", ":", "\n", "            ", "activation_fn", "=", "nn", ".", "ReLU", "(", ")", "\n", "", "elif", "activation", "==", "'gelu'", ":", "\n", "            ", "activation_fn", "=", "nn", ".", "GELU", "(", ")", "\n", "", "else", ":", "raise", "NotImplementedError", "\n", "\n", "if", "self", ".", "nli", ":", "# Architecture from https://github.com/mlpen/Nystromformer/blob/6539b895fa5f798ea0509d19f336d4be787b5708/reorganized_code/LRA/model_wrapper.py#L74", "\n", "            ", "self", ".", "classifier", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Linear", "(", "4", "*", "d_input", ",", "d_model", ")", ",", "\n", "activation_fn", ",", "\n", "nn", ".", "Linear", "(", "d_model", ",", "n_classes", ")", ",", "\n", ")", "\n", "", "else", ":", "# Head from https://github.com/google-research/long-range-arena/blob/ad0ff01a5b3492ade621553a1caae383b347e0c1/lra_benchmarks/models/layers/common_layers.py#L232", "\n", "            ", "self", ".", "classifier", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Linear", "(", "2", "*", "d_input", ",", "d_model", ")", ",", "\n", "activation_fn", ",", "\n", "nn", ".", "Linear", "(", "d_model", ",", "d_model", "//", "2", ")", ",", "\n", "activation_fn", ",", "\n", "nn", ".", "Linear", "(", "d_model", "//", "2", ",", "n_classes", ")", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.tasks.decoders.RetrievalHead.forward": [[151, 163], ["einops.rearrange", "decoders.RetrievalHead.classifier", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "x", ")", ":", "# , state, *args, **kwargs):", "\n", "        ", "\"\"\"\n        x: (2*batch, dim)\n        \"\"\"", "\n", "outs", "=", "rearrange", "(", "x", ",", "'(z b) d -> z b d'", ",", "z", "=", "2", ")", "\n", "outs0", ",", "outs1", "=", "outs", "[", "0", "]", ",", "outs", "[", "1", "]", "# (n_batch, d_input)", "\n", "if", "self", ".", "nli", ":", "\n", "            ", "features", "=", "torch", ".", "cat", "(", "[", "outs0", ",", "outs1", ",", "outs0", "-", "outs1", ",", "outs0", "*", "outs1", "]", ",", "dim", "=", "-", "1", ")", "# (batch, dim)", "\n", "", "else", ":", "\n", "            ", "features", "=", "torch", ".", "cat", "(", "[", "outs0", ",", "outs1", "]", ",", "dim", "=", "-", "1", ")", "# (batch, dim)", "\n", "", "logits", "=", "self", ".", "classifier", "(", "features", ")", "\n", "return", "logits", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.tasks.decoders.RetrievalDecoder.__init__": [[166, 172], ["torch.Module.__init__", "decoders.SequenceDecoder", "decoders.RetrievalHead"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__"], ["def", "__init__", "(", "self", ",", "d_input", ",", "n_classes", ",", "d_model", "=", "None", ",", "nli", "=", "True", ",", "activation", "=", "'relu'", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "if", "d_model", "is", "None", ":", "d_model", "=", "d_input", "\n", "# self.feature = FeatureDecoder(d_input, None, *args, **kwargs)", "\n", "self", ".", "feature", "=", "SequenceDecoder", "(", "d_input", ",", "d_output", "=", "None", ",", "l_output", "=", "0", ",", "*", "args", ",", "**", "kwargs", ")", "\n", "self", ".", "retrieval", "=", "RetrievalHead", "(", "d_input", ",", "d_model", ",", "n_classes", ",", "nli", "=", "nli", ",", "activation", "=", "activation", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.tasks.decoders.RetrievalDecoder.forward": [[173, 177], ["decoders.RetrievalDecoder.feature", "decoders.RetrievalDecoder.retrieval"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ",", "state", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "x", ",", "=", "self", ".", "feature", "(", "x", ",", "state", ",", "*", "args", ",", "**", "kwargs", ")", "\n", "x", "=", "self", ".", "retrieval", "(", "x", ")", "\n", "return", "x", ",", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.tasks.decoders.PackedDecoder.forward": [[179, 183], ["torch.utils.rnn.pad_packed_sequence", "torch.utils.rnn.pad_packed_sequence", "torch.utils.rnn.pad_packed_sequence"], "methods", ["None"], ["    ", "def", "forward", "(", "self", ",", "x", ",", "state", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "x", ",", "_", "=", "nn", ".", "utils", ".", "rnn", ".", "pad_packed_sequence", "(", "x", "\n", ",", "batch_first", "=", "True", ")", "\n", "return", "x", ",", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.tasks.decoders._instantiate": [[211, 224], ["isinstance", "src.config.extract_attrs_from_obj", "src.config.extract_attrs_from_obj", "src.instantiate", "src.Identity", "dataset_attrs.get", "model_attrs.get"], "function", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.utils.config.extract_attrs_from_obj", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.utils.config.extract_attrs_from_obj", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.tasks.decoders.instantiate"], ["def", "_instantiate", "(", "decoder", ",", "model", "=", "None", ",", "dataset", "=", "None", ")", ":", "\n", "    ", "\"\"\" Instantiate a single decoder \"\"\"", "\n", "if", "decoder", "is", "None", ":", "return", "U", ".", "Identity", "(", ")", "\n", "\n", "if", "isinstance", "(", "decoder", ",", "str", ")", ":", "name", "=", "decoder", "\n", "else", ":", "name", "=", "decoder", "[", "'_name_'", "]", "\n", "\n", "# Extract arguments from attribute names", "\n", "dataset_args", "=", "utils", ".", "config", ".", "extract_attrs_from_obj", "(", "dataset", ",", "*", "dataset_attrs", ".", "get", "(", "name", ",", "[", "]", ")", ")", "\n", "model_args", "=", "utils", ".", "config", ".", "extract_attrs_from_obj", "(", "model", ",", "*", "model_attrs", ".", "get", "(", "name", ",", "[", "]", ")", ")", "\n", "# Instantiate decoder", "\n", "obj", "=", "utils", ".", "instantiate", "(", "registry", ",", "decoder", ",", "*", "model_args", ",", "*", "dataset_args", ")", "\n", "return", "obj", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.tasks.decoders.instantiate": [[225, 231], ["src.to_list", "src.TupleSequential", "decoders._instantiate"], "function", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.utils.config.to_list", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.tasks.decoders._instantiate"], ["", "def", "instantiate", "(", "decoder", ",", "model", "=", "None", ",", "dataset", "=", "None", ")", ":", "\n", "    ", "\"\"\" Instantiate a full decoder config, e.g. handle list of configs\n    Note that arguments are added in reverse order compared to encoder (model first, then dataset)\n    \"\"\"", "\n", "decoder", "=", "utils", ".", "to_list", "(", "decoder", ")", "\n", "return", "U", ".", "TupleSequential", "(", "*", "[", "_instantiate", "(", "d", ",", "model", "=", "model", ",", "dataset", "=", "dataset", ")", "for", "d", "in", "decoder", "]", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.callbacks.wandb.WatchModel.__init__": [[38, 41], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "log", ":", "str", "=", "\"gradients\"", ",", "log_freq", ":", "int", "=", "100", ")", ":", "\n", "        ", "self", ".", "log", "=", "log", "\n", "self", ".", "log_freq", "=", "log_freq", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.callbacks.wandb.WatchModel.on_train_start": [[42, 46], ["wandb.get_wandb_logger", "get_wandb_logger.watch"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.callbacks.wandb.get_wandb_logger"], ["", "@", "rank_zero_only", "\n", "def", "on_train_start", "(", "self", ",", "trainer", ",", "pl_module", ")", ":", "\n", "        ", "logger", "=", "get_wandb_logger", "(", "trainer", "=", "trainer", ")", "\n", "logger", ".", "watch", "(", "model", "=", "trainer", ".", "model", ",", "log", "=", "self", ".", "log", ",", "log_freq", "=", "self", ".", "log_freq", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.callbacks.wandb.UploadCodeAsArtifact.__init__": [[51, 53], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "code_dir", ":", "str", ")", ":", "\n", "        ", "self", ".", "code_dir", "=", "code_dir", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.callbacks.wandb.UploadCodeAsArtifact.on_train_start": [[54, 64], ["wandb.get_wandb_logger", "wandb.Artifact", "glob.glob", "experiment.log_artifact", "os.path.join", "wandb.Artifact.add_file"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.callbacks.wandb.get_wandb_logger"], ["", "@", "rank_zero_only", "\n", "def", "on_train_start", "(", "self", ",", "trainer", ",", "pl_module", ")", ":", "\n", "        ", "logger", "=", "get_wandb_logger", "(", "trainer", "=", "trainer", ")", "\n", "experiment", "=", "logger", ".", "experiment", "\n", "\n", "code", "=", "wandb", ".", "Artifact", "(", "\"project-source\"", ",", "type", "=", "\"code\"", ")", "\n", "for", "path", "in", "glob", ".", "glob", "(", "os", ".", "path", ".", "join", "(", "self", ".", "code_dir", ",", "\"**/*.py\"", ")", ",", "recursive", "=", "True", ")", ":", "\n", "            ", "code", ".", "add_file", "(", "path", ")", "\n", "\n", "", "experiment", ".", "log_artifact", "(", "code", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.callbacks.wandb.UploadCheckpointsAsArtifact.__init__": [[69, 72], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "ckpt_dir", ":", "str", "=", "\"checkpoints/\"", ",", "upload_best_only", ":", "bool", "=", "False", ")", ":", "\n", "        ", "self", ".", "ckpt_dir", "=", "ckpt_dir", "\n", "self", ".", "upload_best_only", "=", "upload_best_only", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.callbacks.wandb.UploadCheckpointsAsArtifact.on_train_end": [[73, 87], ["wandb.get_wandb_logger", "wandb.Artifact", "experiment.log_artifact", "wandb.Artifact.add_file", "glob.glob", "os.path.join", "wandb.Artifact.add_file"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.callbacks.wandb.get_wandb_logger"], ["", "@", "rank_zero_only", "\n", "def", "on_train_end", "(", "self", ",", "trainer", ",", "pl_module", ")", ":", "\n", "        ", "logger", "=", "get_wandb_logger", "(", "trainer", "=", "trainer", ")", "\n", "experiment", "=", "logger", ".", "experiment", "\n", "\n", "ckpts", "=", "wandb", ".", "Artifact", "(", "\"experiment-ckpts\"", ",", "type", "=", "\"checkpoints\"", ")", "\n", "\n", "if", "self", ".", "upload_best_only", ":", "\n", "            ", "ckpts", ".", "add_file", "(", "trainer", ".", "checkpoint_callback", ".", "best_model_path", ")", "\n", "", "else", ":", "\n", "            ", "for", "path", "in", "glob", ".", "glob", "(", "os", ".", "path", ".", "join", "(", "self", ".", "ckpt_dir", ",", "\"**/*.ckpt\"", ")", ",", "recursive", "=", "True", ")", ":", "\n", "                ", "ckpts", ".", "add_file", "(", "path", ")", "\n", "\n", "", "", "experiment", ".", "log_artifact", "(", "ckpts", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.callbacks.wandb.LogConfusionMatrix.__init__": [[94, 98], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ")", ":", "\n", "        ", "self", ".", "preds", "=", "[", "]", "\n", "self", ".", "targets", "=", "[", "]", "\n", "self", ".", "ready", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.callbacks.wandb.LogConfusionMatrix.on_sanity_check_start": [[99, 101], ["None"], "methods", ["None"], ["", "def", "on_sanity_check_start", "(", "self", ",", "trainer", ",", "pl_module", ")", "->", "None", ":", "\n", "        ", "self", ".", "ready", "=", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.callbacks.wandb.LogConfusionMatrix.on_sanity_check_end": [[102, 105], ["None"], "methods", ["None"], ["", "def", "on_sanity_check_end", "(", "self", ",", "trainer", ",", "pl_module", ")", ":", "\n", "        ", "\"\"\"Start executing this callback only after all validation sanity checks end.\"\"\"", "\n", "self", ".", "ready", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.callbacks.wandb.LogConfusionMatrix.on_validation_batch_end": [[106, 113], ["wandb.LogConfusionMatrix.preds.append", "wandb.LogConfusionMatrix.targets.append"], "methods", ["None"], ["", "def", "on_validation_batch_end", "(", "\n", "self", ",", "trainer", ",", "pl_module", ",", "outputs", ",", "batch", ",", "batch_idx", ",", "dataloader_idx", "\n", ")", ":", "\n", "        ", "\"\"\"Gather data from single batch.\"\"\"", "\n", "if", "self", ".", "ready", ":", "\n", "            ", "self", ".", "preds", ".", "append", "(", "outputs", "[", "\"preds\"", "]", ")", "\n", "self", ".", "targets", ".", "append", "(", "outputs", "[", "\"targets\"", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.callbacks.wandb.LogConfusionMatrix.on_validation_epoch_end": [[114, 145], ["wandb.get_wandb_logger", "torch.cat().cpu().numpy", "torch.cat().cpu().numpy", "sklearn.metrics.confusion_matrix", "matplotlib.figure", "seaborn.set", "seaborn.heatmap", "experiment.log", "matplotlib.clf", "wandb.LogConfusionMatrix.preds.clear", "wandb.LogConfusionMatrix.targets.clear", "torch.cat().cpu", "torch.cat().cpu", "wandb.Image", "torch.cat", "torch.cat"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.callbacks.wandb.get_wandb_logger"], ["", "", "def", "on_validation_epoch_end", "(", "self", ",", "trainer", ",", "pl_module", ")", ":", "\n", "        ", "\"\"\"Generate confusion matrix.\"\"\"", "\n", "if", "self", ".", "ready", ":", "\n", "            ", "logger", "=", "get_wandb_logger", "(", "trainer", ")", "\n", "experiment", "=", "logger", ".", "experiment", "\n", "\n", "preds", "=", "torch", ".", "cat", "(", "self", ".", "preds", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "targets", "=", "torch", ".", "cat", "(", "self", ".", "targets", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "\n", "confusion_matrix", "=", "metrics", ".", "confusion_matrix", "(", "y_true", "=", "targets", ",", "y_pred", "=", "preds", ")", "\n", "\n", "# set figure size", "\n", "plt", ".", "figure", "(", "figsize", "=", "(", "14", ",", "8", ")", ")", "\n", "\n", "# set labels size", "\n", "sn", ".", "set", "(", "font_scale", "=", "1.4", ")", "\n", "\n", "# set font size", "\n", "sn", ".", "heatmap", "(", "confusion_matrix", ",", "annot", "=", "True", ",", "annot_kws", "=", "{", "\"size\"", ":", "8", "}", ",", "fmt", "=", "\"g\"", ")", "\n", "\n", "# names should be uniqe or else charts from different experiments in wandb will overlap", "\n", "experiment", ".", "log", "(", "{", "f\"confusion_matrix/{experiment.name}\"", ":", "wandb", ".", "Image", "(", "plt", ")", "}", ",", "commit", "=", "False", ")", "\n", "\n", "# according to wandb docs this should also work but it crashes", "\n", "# experiment.log(f{\"confusion_matrix/{experiment.name}\": plt})", "\n", "\n", "# reset plot", "\n", "plt", ".", "clf", "(", ")", "\n", "\n", "self", ".", "preds", ".", "clear", "(", ")", "\n", "self", ".", "targets", ".", "clear", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.callbacks.wandb.LogF1PrecRecHeatmap.__init__": [[152, 156], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "class_names", ":", "List", "[", "str", "]", "=", "None", ")", ":", "\n", "        ", "self", ".", "preds", "=", "[", "]", "\n", "self", ".", "targets", "=", "[", "]", "\n", "self", ".", "ready", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.callbacks.wandb.LogF1PrecRecHeatmap.on_sanity_check_start": [[157, 159], ["None"], "methods", ["None"], ["", "def", "on_sanity_check_start", "(", "self", ",", "trainer", ",", "pl_module", ")", ":", "\n", "        ", "self", ".", "ready", "=", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.callbacks.wandb.LogF1PrecRecHeatmap.on_sanity_check_end": [[160, 163], ["None"], "methods", ["None"], ["", "def", "on_sanity_check_end", "(", "self", ",", "trainer", ",", "pl_module", ")", ":", "\n", "        ", "\"\"\"Start executing this callback only after all validation sanity checks end.\"\"\"", "\n", "self", ".", "ready", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.callbacks.wandb.LogF1PrecRecHeatmap.on_validation_batch_end": [[164, 171], ["wandb.LogF1PrecRecHeatmap.preds.append", "wandb.LogF1PrecRecHeatmap.targets.append"], "methods", ["None"], ["", "def", "on_validation_batch_end", "(", "\n", "self", ",", "trainer", ",", "pl_module", ",", "outputs", ",", "batch", ",", "batch_idx", ",", "dataloader_idx", "\n", ")", ":", "\n", "        ", "\"\"\"Gather data from single batch.\"\"\"", "\n", "if", "self", ".", "ready", ":", "\n", "            ", "self", ".", "preds", ".", "append", "(", "outputs", "[", "\"preds\"", "]", ")", "\n", "self", ".", "targets", ".", "append", "(", "outputs", "[", "\"targets\"", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.callbacks.wandb.LogF1PrecRecHeatmap.on_validation_epoch_end": [[172, 208], ["wandb.get_wandb_logger", "torch.cat().cpu().numpy", "torch.cat().cpu().numpy", "sklearn.metrics.f1_score", "sklearn.metrics.recall_score", "sklearn.metrics.precision_score", "matplotlib.figure", "seaborn.set", "seaborn.heatmap", "experiment.log", "matplotlib.clf", "wandb.LogF1PrecRecHeatmap.preds.clear", "wandb.LogF1PrecRecHeatmap.targets.clear", "torch.cat().cpu", "torch.cat().cpu", "wandb.Image", "torch.cat", "torch.cat"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.callbacks.wandb.get_wandb_logger"], ["", "", "def", "on_validation_epoch_end", "(", "self", ",", "trainer", ",", "pl_module", ")", ":", "\n", "        ", "\"\"\"Generate f1, precision and recall heatmap.\"\"\"", "\n", "if", "self", ".", "ready", ":", "\n", "            ", "logger", "=", "get_wandb_logger", "(", "trainer", "=", "trainer", ")", "\n", "experiment", "=", "logger", ".", "experiment", "\n", "\n", "preds", "=", "torch", ".", "cat", "(", "self", ".", "preds", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "targets", "=", "torch", ".", "cat", "(", "self", ".", "targets", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "f1", "=", "f1_score", "(", "preds", ",", "targets", ",", "average", "=", "None", ")", "\n", "r", "=", "recall_score", "(", "preds", ",", "targets", ",", "average", "=", "None", ")", "\n", "p", "=", "precision_score", "(", "preds", ",", "targets", ",", "average", "=", "None", ")", "\n", "data", "=", "[", "f1", ",", "p", ",", "r", "]", "\n", "\n", "# set figure size", "\n", "plt", ".", "figure", "(", "figsize", "=", "(", "14", ",", "3", ")", ")", "\n", "\n", "# set labels size", "\n", "sn", ".", "set", "(", "font_scale", "=", "1.2", ")", "\n", "\n", "# set font size", "\n", "sn", ".", "heatmap", "(", "\n", "data", ",", "\n", "annot", "=", "True", ",", "\n", "annot_kws", "=", "{", "\"size\"", ":", "10", "}", ",", "\n", "fmt", "=", "\".3f\"", ",", "\n", "yticklabels", "=", "[", "\"F1\"", ",", "\"Precision\"", ",", "\"Recall\"", "]", ",", "\n", ")", "\n", "\n", "# names should be uniqe or else charts from different experiments in wandb will overlap", "\n", "experiment", ".", "log", "(", "{", "f\"f1_p_r_heatmap/{experiment.name}\"", ":", "wandb", ".", "Image", "(", "plt", ")", "}", ",", "commit", "=", "False", ")", "\n", "\n", "# reset plot", "\n", "plt", ".", "clf", "(", ")", "\n", "\n", "self", ".", "preds", ".", "clear", "(", ")", "\n", "self", ".", "targets", ".", "clear", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.callbacks.wandb.LogImagePredictions.__init__": [[216, 220], ["pytorch_lightning.Callback.__init__"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__"], ["def", "__init__", "(", "self", ",", "num_samples", ":", "int", "=", "8", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "num_samples", "=", "num_samples", "\n", "self", ".", "ready", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.callbacks.wandb.LogImagePredictions.on_sanity_check_start": [[221, 223], ["None"], "methods", ["None"], ["", "def", "on_sanity_check_start", "(", "self", ",", "trainer", ",", "pl_module", ")", ":", "\n", "        ", "self", ".", "ready", "=", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.callbacks.wandb.LogImagePredictions.on_sanity_check_end": [[224, 227], ["None"], "methods", ["None"], ["", "def", "on_sanity_check_end", "(", "self", ",", "trainer", ",", "pl_module", ")", ":", "\n", "        ", "\"\"\"Start executing this callback only after all validation sanity checks end.\"\"\"", "\n", "self", ".", "ready", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.callbacks.wandb.LogImagePredictions.on_validation_epoch_end": [[228, 251], ["wandb.get_wandb_logger", "next", "val_imgs.to.to.to", "pl_module", "torch.argmax", "experiment.log", "iter", "trainer.datamodule.val_dataloader", "wandb.Image", "zip"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.callbacks.wandb.get_wandb_logger"], ["", "def", "on_validation_epoch_end", "(", "self", ",", "trainer", ",", "pl_module", ")", ":", "\n", "        ", "if", "self", ".", "ready", ":", "\n", "            ", "logger", "=", "get_wandb_logger", "(", "trainer", "=", "trainer", ")", "\n", "experiment", "=", "logger", ".", "experiment", "\n", "\n", "# get a validation batch from the validation dat loader", "\n", "val_samples", "=", "next", "(", "iter", "(", "trainer", ".", "datamodule", ".", "val_dataloader", "(", ")", ")", ")", "\n", "val_imgs", ",", "val_labels", "=", "val_samples", "\n", "\n", "# run the batch through the network", "\n", "val_imgs", "=", "val_imgs", ".", "to", "(", "device", "=", "pl_module", ".", "device", ")", "\n", "logits", "=", "pl_module", "(", "val_imgs", ")", "\n", "preds", "=", "torch", ".", "argmax", "(", "logits", ",", "axis", "=", "-", "1", ")", "\n", "\n", "# log the images as wandb Image", "\n", "experiment", ".", "log", "(", "\n", "{", "\n", "f\"Images/{experiment.name}\"", ":", "[", "\n", "wandb", ".", "Image", "(", "x", ",", "caption", "=", "f\"Pred:{pred}, Label:{y}\"", ")", "\n", "for", "x", ",", "pred", ",", "y", "in", "zip", "(", "\n", "val_imgs", "[", ":", "self", ".", "num_samples", "]", ",", "\n", "preds", "[", ":", "self", ".", "num_samples", "]", ",", "\n", "val_labels", "[", ":", "self", ".", "num_samples", "]", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.callbacks.wandb.LogDT.on_train_epoch_end": [[258, 278], ["pl_module.model.named_modules", "torch.distributed.is_initialized", "pl_module.hparams.train.get", "hasattr", "m.log_dt.detach().cpu().numpy().flatten", "wandb.Image", "wandb.Table", "torch.distributed.get_rank", "trainer.logger.experiment.log", "m.log_dt.detach().cpu().numpy().flatten().reshape", "m.log_dt.detach().cpu().numpy", "pandas.DataFrame", "m.log_dt.detach().cpu().numpy().flatten", "m.log_dt.detach().cpu", "m.log_dt.detach().cpu().numpy().flatten", "m.log_dt.detach().cpu().numpy", "m.log_dt.detach", "m.log_dt.detach().cpu().numpy", "m.log_dt.detach().cpu", "m.log_dt.detach().cpu", "m.log_dt.detach", "m.log_dt.detach"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.ss.lssl.Platypus.is_initialized", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.utils.distributed.get_rank"], ["def", "on_train_epoch_end", "(", "self", ",", "trainer", ",", "pl_module", ")", ":", "\n", "        ", "log_dict", "=", "{", "}", "\n", "for", "name", ",", "m", "in", "pl_module", ".", "model", ".", "named_modules", "(", ")", ":", "\n", "            ", "if", "pl_module", ".", "hparams", ".", "train", ".", "get", "(", "'log_dt'", ",", "False", ")", "and", "hasattr", "(", "m", ",", "\"log_dt\"", ")", ":", "\n", "                ", "log_dict", "[", "f\"{name}.log_dt\"", "]", "=", "(", "\n", "m", ".", "log_dt", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ".", "flatten", "(", ")", "\n", ")", "\n", "log_dict", "[", "f\"{name}.log_dt.image\"", "]", "=", "wandb", ".", "Image", "(", "\n", "m", ".", "log_dt", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ".", "flatten", "(", ")", ".", "reshape", "(", "1", ",", "-", "1", ")", "\n", ")", "\n", "log_dict", "[", "f\"{name}.log_dt\"", "]", "=", "wandb", ".", "Table", "(", "\n", "dataframe", "=", "pd", ".", "DataFrame", "(", "\n", "{", "\"log_dt\"", ":", "m", ".", "log_dt", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ".", "flatten", "(", ")", "}", "\n", ")", "\n", ")", "\n", "\n", "", "", "if", "torch", ".", "distributed", ".", "is_initialized", "(", ")", "and", "torch", ".", "distributed", ".", "get_rank", "(", ")", "==", "0", ":", "\n", "            ", "if", "trainer", ".", "logger", "is", "not", "None", ":", "\n", "                ", "trainer", ".", "logger", ".", "experiment", ".", "log", "(", "log_dict", ")", "\n", "", "", "", "", ""]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.callbacks.wandb.get_wandb_logger": [[19, 32], ["isinstance", "isinstance", "Exception", "isinstance"], "function", ["None"], ["def", "get_wandb_logger", "(", "trainer", ":", "Trainer", ")", "->", "WandbLogger", ":", "\n", "    ", "\"\"\"Safely get Weights&Biases logger from Trainer.\"\"\"", "\n", "\n", "if", "isinstance", "(", "trainer", ".", "logger", ",", "WandbLogger", ")", ":", "\n", "        ", "return", "trainer", ".", "logger", "\n", "\n", "", "if", "isinstance", "(", "trainer", ".", "logger", ",", "LoggerCollection", ")", ":", "\n", "        ", "for", "logger", "in", "trainer", ".", "logger", ":", "\n", "            ", "if", "isinstance", "(", "logger", ",", "WandbLogger", ")", ":", "\n", "                ", "return", "logger", "\n", "\n", "", "", "", "raise", "Exception", "(", "\n", "\"You are using wandb related callback, but WandbLogger was not found for some reason...\"", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.callbacks.timer.Timer.__init__": [[17, 30], ["pytorch_lightning.Callback.__init__", "pytorch_lightning.utilities.parsing.AttributeDict"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "step", ":", "bool", "=", "True", ",", "\n", "inter_step", ":", "bool", "=", "True", ",", "\n", "epoch", ":", "bool", "=", "True", ",", "\n", "val", ":", "bool", "=", "True", ",", "\n", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "_log_stats", "=", "AttributeDict", "(", "{", "\n", "'step_time'", ":", "step", ",", "\n", "'inter_step_time'", ":", "inter_step", ",", "\n", "'epoch_time'", ":", "epoch", ",", "\n", "'val_time'", ":", "val", ",", "\n", "}", ")", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.callbacks.timer.Timer.on_train_start": [[32, 34], ["None"], "methods", ["None"], ["", "def", "on_train_start", "(", "self", ",", "trainer", ":", "Trainer", ",", "pl_module", ":", "LightningModule", ")", "->", "None", ":", "\n", "        ", "self", ".", "_snap_epoch_time", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.callbacks.timer.Timer.on_train_epoch_start": [[35, 39], ["time.time"], "methods", ["None"], ["", "def", "on_train_epoch_start", "(", "self", ",", "trainer", ":", "Trainer", ",", "pl_module", ":", "LightningModule", ")", "->", "None", ":", "\n", "        ", "self", ".", "_snap_step_time", "=", "None", "\n", "self", ".", "_snap_inter_step_time", "=", "None", "\n", "self", ".", "_snap_epoch_time", "=", "time", ".", "time", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.callbacks.timer.Timer.on_train_batch_start": [[40, 60], ["time.time", "timer.Timer._should_log", "trainer.logger.log_metrics", "time.time"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.callbacks.timer.Timer._should_log"], ["", "def", "on_train_batch_start", "(", "\n", "self", ",", "\n", "trainer", ":", "Trainer", ",", "\n", "pl_module", ":", "LightningModule", ",", "\n", "batch", ":", "Any", ",", "\n", "batch_idx", ":", "int", ",", "\n", "dataloader_idx", ":", "int", "\n", ")", "->", "None", ":", "\n", "        ", "if", "self", ".", "_log_stats", ".", "step_time", ":", "\n", "            ", "self", ".", "_snap_step_time", "=", "time", ".", "time", "(", ")", "\n", "\n", "", "if", "not", "self", ".", "_should_log", "(", "trainer", ")", ":", "\n", "            ", "return", "\n", "\n", "", "logs", "=", "{", "}", "\n", "if", "self", ".", "_log_stats", ".", "inter_step_time", "and", "self", ".", "_snap_inter_step_time", ":", "\n", "# First log at beginning of second step", "\n", "            ", "logs", "[", "\"timer/inter_step\"", "]", "=", "(", "time", ".", "time", "(", ")", "-", "self", ".", "_snap_inter_step_time", ")", "# * 1000", "\n", "\n", "", "if", "trainer", ".", "logger", ":", "trainer", ".", "logger", ".", "log_metrics", "(", "logs", ",", "step", "=", "trainer", ".", "global_step", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.callbacks.timer.Timer.on_train_batch_end": [[61, 82], ["time.time", "timer.Timer._should_log", "trainer.logger.log_metrics", "time.time"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.callbacks.timer.Timer._should_log"], ["", "@", "rank_zero_only", "\n", "def", "on_train_batch_end", "(", "\n", "self", ",", "\n", "trainer", ":", "Trainer", ",", "\n", "pl_module", ":", "LightningModule", ",", "\n", "outputs", ":", "STEP_OUTPUT", ",", "\n", "batch", ":", "Any", ",", "\n", "batch_idx", ":", "int", ",", "\n", "dataloader_idx", ":", "int", ",", "\n", ")", "->", "None", ":", "\n", "        ", "if", "self", ".", "_log_stats", ".", "inter_step_time", ":", "\n", "            ", "self", ".", "_snap_inter_step_time", "=", "time", ".", "time", "(", ")", "\n", "\n", "", "if", "not", "self", ".", "_should_log", "(", "trainer", ")", ":", "\n", "            ", "return", "\n", "\n", "", "logs", "=", "{", "}", "\n", "if", "self", ".", "_log_stats", ".", "step_time", "and", "self", ".", "_snap_step_time", ":", "\n", "            ", "logs", "[", "\"timer/step\"", "]", "=", "(", "time", ".", "time", "(", ")", "-", "self", ".", "_snap_step_time", ")", "# * 1000", "\n", "\n", "", "if", "trainer", ".", "logger", ":", "trainer", ".", "logger", ".", "log_metrics", "(", "logs", ",", "step", "=", "trainer", ".", "global_step", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.callbacks.timer.Timer.on_train_epoch_end": [[83, 89], ["trainer.logger.log_metrics", "time.time"], "methods", ["None"], ["", "@", "rank_zero_only", "\n", "def", "on_train_epoch_end", "(", "self", ",", "trainer", ":", "Trainer", ",", "pl_module", ":", "LightningModule", ",", ")", "->", "None", ":", "\n", "        ", "logs", "=", "{", "}", "\n", "if", "self", ".", "_log_stats", ".", "epoch_time", "and", "self", ".", "_snap_epoch_time", ":", "\n", "            ", "logs", "[", "\"timer/epoch\"", "]", "=", "time", ".", "time", "(", ")", "-", "self", ".", "_snap_epoch_time", "\n", "", "if", "trainer", ".", "logger", ":", "trainer", ".", "logger", ".", "log_metrics", "(", "logs", ",", "step", "=", "trainer", ".", "global_step", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.callbacks.timer.Timer.on_validation_epoch_start": [[90, 92], ["time.time"], "methods", ["None"], ["", "def", "on_validation_epoch_start", "(", "self", ",", "trainer", ":", "Trainer", ",", "pl_module", ":", "LightningModule", ")", "->", "None", ":", "\n", "        ", "self", ".", "_snap_val_time", "=", "time", ".", "time", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.callbacks.timer.Timer.on_validation_epoch_end": [[93, 99], ["trainer.logger.log_metrics", "time.time"], "methods", ["None"], ["", "@", "rank_zero_only", "\n", "def", "on_validation_epoch_end", "(", "self", ",", "trainer", ":", "Trainer", ",", "pl_module", ":", "LightningModule", ",", ")", "->", "None", ":", "\n", "        ", "logs", "=", "{", "}", "\n", "if", "self", ".", "_log_stats", ".", "val_time", "and", "self", ".", "_snap_val_time", ":", "\n", "            ", "logs", "[", "\"timer/validation\"", "]", "=", "time", ".", "time", "(", ")", "-", "self", ".", "_snap_val_time", "\n", "", "if", "trainer", ".", "logger", ":", "trainer", ".", "logger", ".", "log_metrics", "(", "logs", ")", "# , step=trainer.global_step)", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.callbacks.timer.Timer._should_log": [[100, 103], ["None"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "_should_log", "(", "trainer", ")", "->", "bool", ":", "\n", "        ", "return", "(", "trainer", ".", "global_step", "+", "1", ")", "%", "trainer", ".", "log_every_n_steps", "==", "0", "or", "trainer", ".", "should_stop", "\n", "", "", ""]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.callbacks.norms.TrackNorms.on_after_training_step": [[10, 24], ["hasattr", "norms.TrackNorms.log_dict", "metrics.update"], "methods", ["None"], ["    ", "def", "on_after_training_step", "(", "self", ",", "batch", ",", "batch_idx", ",", "trainer", ":", "pl", ".", "Trainer", ",", "pl_module", ":", "pl", ".", "LightningModule", ")", ":", "\n", "# Log extra metrics", "\n", "        ", "metrics", "=", "{", "}", "\n", "\n", "if", "hasattr", "(", "pl_module", ",", "\"_grad_norms\"", ")", ":", "\n", "            ", "metrics", ".", "update", "(", "pl_module", ".", "_grad_norms", ")", "\n", "\n", "", "self", ".", "log_dict", "(", "\n", "metrics", ",", "\n", "on_step", "=", "True", ",", "\n", "on_epoch", "=", "False", ",", "\n", "prog_bar", "=", "False", ",", "\n", "add_dataloader_idx", "=", "False", ",", "\n", "sync_dist", "=", "True", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.callbacks.norms.TrackNorms.on_after_backward": [[27, 39], ["omegaconf.OmegaConf.select", "pl_module.named_parameters", "torch.mean"], "methods", ["None"], ["", "def", "on_after_backward", "(", "self", ",", "trainer", ":", "pl", ".", "Trainer", ",", "pl_module", ":", "pl", ".", "LightningModule", ")", ":", "\n", "# example to inspect gradient information in tensorboard", "\n", "        ", "if", "OmegaConf", ".", "select", "(", "trainer", ".", "hparams", ",", "'trainer.track_grad_norms'", ")", ":", "# TODO dot notation should work with omegaconf?", "\n", "            ", "norms", "=", "{", "}", "\n", "for", "name", ",", "p", "in", "pl_module", ".", "named_parameters", "(", ")", ":", "\n", "                ", "if", "p", ".", "grad", "is", "None", ":", "\n", "                    ", "continue", "\n", "\n", "# param_norm = float(p.grad.data.norm(norm_type))", "\n", "", "param_norm", "=", "torch", ".", "mean", "(", "p", ".", "grad", ".", "data", "**", "2", ")", "\n", "norms", "[", "f\"grad_norm.{name}\"", "]", "=", "param_norm", "\n", "", "pl_module", ".", "_grad_norms", "=", "norms", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.callbacks.params.ParamsLog.__init__": [[10, 22], ["pytorch_lightning.Callback.__init__", "pytorch_lightning.utilities.parsing.AttributeDict"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "total", ":", "bool", "=", "True", ",", "\n", "trainable", ":", "bool", "=", "True", ",", "\n", "fixed", ":", "bool", "=", "True", ",", "\n", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "_log_stats", "=", "AttributeDict", "(", "\n", "{", "\n", "'total_params_log'", ":", "total", ",", "\n", "'trainable_params_log'", ":", "trainable", ",", "\n", "'non_trainable_params_log'", ":", "fixed", ",", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.callbacks.params.ParamsLog.on_fit_start": [[25, 38], ["sum", "sum", "sum", "trainer.logger.log_hyperparams", "p.numel", "p.numel", "p.numel", "pl_module.parameters", "pl_module.parameters", "pl_module.parameters"], "methods", ["None"], ["", "@", "rank_zero_only", "\n", "def", "on_fit_start", "(", "self", ",", "trainer", ":", "pl", ".", "Trainer", ",", "pl_module", ":", "pl", ".", "LightningModule", ")", "->", "None", ":", "\n", "        ", "logs", "=", "{", "}", "\n", "if", "self", ".", "_log_stats", ".", "total_params_log", ":", "\n", "            ", "logs", "[", "\"params/total\"", "]", "=", "sum", "(", "p", ".", "numel", "(", ")", "for", "p", "in", "pl_module", ".", "parameters", "(", ")", ")", "\n", "", "if", "self", ".", "_log_stats", ".", "trainable_params_log", ":", "\n", "            ", "logs", "[", "\"params/trainable\"", "]", "=", "sum", "(", "p", ".", "numel", "(", ")", "for", "p", "in", "pl_module", ".", "parameters", "(", ")", "\n", "if", "p", ".", "requires_grad", ")", "\n", "", "if", "self", ".", "_log_stats", ".", "non_trainable_params_log", ":", "\n", "            ", "logs", "[", "\"params/fixed\"", "]", "=", "sum", "(", "p", ".", "numel", "(", ")", "for", "p", "in", "pl_module", ".", "parameters", "(", ")", "\n", "if", "not", "p", ".", "requires_grad", ")", "\n", "", "if", "trainer", ".", "logger", ":", "\n", "            ", "trainer", ".", "logger", ".", "log_hyperparams", "(", "logs", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.hippo.transition.AdaptiveTransition.__init__": [[33, 59], ["torch.Module.__init__", "torch.eye", "torch.eye", "torch.eye", "torch.eye", "torch.eye", "torch.eye", "torch.eye", "torch.eye", "torch.eye", "src.models.hippo.hippo.transition.AdaptiveTransition.register_buffer", "src.models.hippo.hippo.transition.AdaptiveTransition.register_buffer", "src.models.hippo.hippo.transition.AdaptiveTransition.register_buffer", "params.items", "params.items", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "p.repeat.repeat.repeat", "src.models.hippo.hippo.transition.AdaptiveTransition.register_parameter", "src.models.hippo.hippo.transition.AdaptiveTransition.register_buffer", "torch.Parameter", "torch.Parameter", "torch.Parameter", "getattr", "len"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__"], ["    ", "def", "__init__", "(", "self", ",", "N", ",", "params", ",", "trainable", "=", "False", ",", "lr", "=", "1.0", ",", "batch", "=", "(", ")", ")", ":", "\n", "        ", "\"\"\"\n        params: dict of Tensors that encode the parameters of the state system A, B\n        \"\"\"", "\n", "\n", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "N", "=", "N", "\n", "self", ".", "trainable", "=", "trainable", "\n", "self", ".", "batch", "=", "batch", "\n", "\n", "if", "self", ".", "trainable", ":", "\n", "            ", "for", "name", ",", "p", "in", "params", ".", "items", "(", ")", ":", "\n", "                ", "p", "=", "p", ".", "repeat", "(", "*", "batch", ",", "*", "[", "1", "]", "*", "len", "(", "p", ".", "shape", ")", ")", "\n", "self", ".", "register_parameter", "(", "name", ",", "nn", ".", "Parameter", "(", "p", ")", ")", "\n", "getattr", "(", "self", ",", "name", ")", ".", "_lr", "=", "lr", "\n", "", "", "else", ":", "\n", "            ", "assert", "batch", "==", "(", ")", ",", "\"If not learnable, Transition should not have a batch dimension\"", "\n", "for", "name", ",", "p", "in", "params", ".", "items", "(", ")", ":", "\n", "                ", "self", ".", "register_buffer", "(", "name", ",", "p", ")", "\n", "\n", "# Register some common buffers", "\n", "# (helps make sure every subclass has access to them on the right device)", "\n", "", "", "I", "=", "torch", ".", "eye", "(", "N", ")", "\n", "self", ".", "register_buffer", "(", "'I'", ",", "I", ")", "\n", "self", ".", "register_buffer", "(", "'ones'", ",", "torch", ".", "ones", "(", "N", ")", ")", "\n", "self", ".", "register_buffer", "(", "'arange'", ",", "torch", ".", "arange", "(", "N", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.hippo.transition.AdaptiveTransition.A": [[61, 71], ["src.models.hippo.hippo.transition.AdaptiveTransition._A", "hasattr", "src.models.hippo.hippo.transition.AdaptiveTransition._A"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.hippo.transition.ToeplitzAdaptiveTransition._A", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.hippo.transition.ToeplitzAdaptiveTransition._A"], ["", "@", "property", "\n", "def", "A", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "trainable", ":", "\n", "            ", "return", "self", ".", "_A", "(", ")", "\n", "# Cache it the first time this is called", "\n", "# this must be done here and not in __init__ so all tensors are on the right device", "\n", "", "else", ":", "\n", "            ", "if", "not", "hasattr", "(", "self", ",", "'_cached_A'", ")", ":", "\n", "                ", "self", ".", "_cached_A", "=", "self", ".", "_A", "(", ")", "\n", "", "return", "self", ".", "_cached_A", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.hippo.transition.AdaptiveTransition.B": [[72, 82], ["src.models.hippo.hippo.transition.AdaptiveTransition._B", "hasattr", "src.models.hippo.hippo.transition.AdaptiveTransition._B"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.hippo.transition.ToeplitzAdaptiveTransition._B", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.hippo.transition.ToeplitzAdaptiveTransition._B"], ["", "", "@", "property", "\n", "def", "B", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "trainable", ":", "\n", "            ", "return", "self", ".", "_B", "(", ")", "\n", "# Cache it the first time this is called", "\n", "# this must be done here and not in __init__ so all tensors are on the right device", "\n", "", "else", ":", "\n", "            ", "if", "not", "hasattr", "(", "self", ",", "'_cached_B'", ")", ":", "\n", "                ", "self", ".", "_cached_B", "=", "self", ".", "_B", "(", ")", "\n", "", "return", "self", ".", "_cached_B", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.hippo.transition.AdaptiveTransition.precompute_forward": [[83, 85], ["None"], "methods", ["None"], ["", "", "def", "precompute_forward", "(", "self", ")", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.hippo.transition.AdaptiveTransition.precompute_backward": [[86, 88], ["None"], "methods", ["None"], ["", "def", "precompute_backward", "(", "self", ")", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.hippo.transition.AdaptiveTransition.forward_mult": [[89, 99], ["None"], "methods", ["None"], ["", "def", "forward_mult", "(", "self", ",", "u", ",", "delta", ")", ":", "\n", "        ", "\"\"\" Computes (I + delta A) u\n\n        A: (n, n)\n        u: (..., n)\n        delta: (...) or scalar\n\n        output: (..., n)\n        \"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.hippo.transition.AdaptiveTransition.inverse_mult": [[100, 103], ["None"], "methods", ["None"], ["", "def", "inverse_mult", "(", "self", ",", "u", ",", "delta", ")", ":", "# TODO swap u, delta everywhere", "\n", "        ", "\"\"\" Computes (I - d A)^-1 u \"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.hippo.transition.AdaptiveTransition.forward_diff": [[104, 115], ["src.models.hippo.hippo.transition.AdaptiveTransition.forward_mult", "v.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.lssl.LegTTransition.forward_mult"], ["", "def", "forward_diff", "(", "self", ",", "d", ",", "u", ",", "v", ")", ":", "\n", "        ", "\"\"\" Computes the 'forward diff' or Euler update rule: (I - d A)^-1 u + d B v\n        d: (...)\n        u: (..., n)\n        v: (...)\n        \"\"\"", "\n", "v", "=", "d", "*", "v", "\n", "v", "=", "v", ".", "unsqueeze", "(", "-", "1", ")", "*", "self", ".", "B", "\n", "x", "=", "self", ".", "forward_mult", "(", "u", ",", "d", ")", "\n", "x", "=", "x", "+", "v", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.hippo.transition.AdaptiveTransition.backward_diff": [[116, 127], ["src.models.hippo.hippo.transition.AdaptiveTransition.inverse_mult", "v.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.lssl.LegTTransition.inverse_mult"], ["", "def", "backward_diff", "(", "self", ",", "d", ",", "u", ",", "v", ")", ":", "\n", "        ", "\"\"\" Computes the 'forward diff' or Euler update rule: (I - d A)^-1 u + d (I - d A)^-1 B v\n        d: (...)\n        u: (..., n)\n        v: (...)\n        \"\"\"", "\n", "v", "=", "d", "*", "v", "\n", "v", "=", "v", ".", "unsqueeze", "(", "-", "1", ")", "*", "self", ".", "B", "\n", "x", "=", "u", "+", "v", "\n", "x", "=", "self", ".", "inverse_mult", "(", "x", ",", "d", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.hippo.transition.AdaptiveTransition.bilinear": [[128, 143], ["src.models.hippo.hippo.transition.AdaptiveTransition.forward_mult", "src.models.hippo.hippo.transition.AdaptiveTransition.inverse_mult", "v.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.lssl.LegTTransition.forward_mult", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.lssl.LegTTransition.inverse_mult"], ["", "def", "bilinear", "(", "self", ",", "dt", ",", "u", ",", "v", ",", "alpha", "=", ".5", ")", ":", "\n", "        ", "\"\"\" Computes the bilinear (aka trapezoid or Tustin's) update rule.\n\n        (I - d/2 A)^-1 (I + d/2 A) u + d B (I - d/2 A)^-1 B v\n\n        dt: (...)\n        u: (..., N)\n        v: (...)\n        \"\"\"", "\n", "x", "=", "self", ".", "forward_mult", "(", "u", ",", "(", "1", "-", "alpha", ")", "*", "dt", ")", "\n", "v", "=", "dt", "*", "v", "\n", "v", "=", "v", ".", "unsqueeze", "(", "-", "1", ")", "*", "self", ".", "B", "\n", "x", "=", "x", "+", "v", "\n", "x", "=", "self", ".", "inverse_mult", "(", "x", ",", "(", "alpha", ")", "*", "dt", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.hippo.transition.AdaptiveTransition.zoh": [[144, 146], ["None"], "methods", ["None"], ["", "def", "zoh", "(", "self", ",", "dt", ",", "u", ",", "v", ")", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.hippo.transition.AdaptiveTransition.gbt_A": [[147, 159], ["max", "src.models.hippo.hippo.transition.AdaptiveTransition.I.view", "src.models.hippo.hippo.transition.AdaptiveTransition.bilinear", "einops.rearrange", "len", "len", "dt.new_zeros"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.lssl.AdaptiveTransition.bilinear"], ["", "def", "gbt_A", "(", "self", ",", "dt", ",", "alpha", "=", ".5", ")", ":", "\n", "        ", "\"\"\" Compute the transition matrices associated with bilinear transform\n\n        dt: (...) broadcastable with self.batch_shape\n        returns: (..., N, N)\n        \"\"\"", "\n", "# solve (N, ...) parallel problems of size N", "\n", "dims", "=", "max", "(", "len", "(", "dt", ".", "shape", ")", ",", "len", "(", "self", ".", "batch", ")", ")", "\n", "I", "=", "self", ".", "I", ".", "view", "(", "[", "self", ".", "N", "]", "+", "[", "1", "]", "*", "dims", "+", "[", "self", ".", "N", "]", ")", "\n", "A", "=", "self", ".", "bilinear", "(", "dt", ",", "I", ",", "dt", ".", "new_zeros", "(", "*", "dt", ".", "shape", ")", ",", "alpha", "=", "alpha", ")", "# (N, ..., N)", "\n", "A", "=", "rearrange", "(", "A", ",", "'n ... m -> ... m n'", ",", "n", "=", "self", ".", "N", ",", "m", "=", "self", ".", "N", ")", "\n", "return", "A", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.hippo.transition.AdaptiveTransition.gbt_B": [[160, 163], ["src.models.hippo.hippo.transition.AdaptiveTransition.bilinear", "dt.new_zeros", "dt.new_ones"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.lssl.AdaptiveTransition.bilinear"], ["", "def", "gbt_B", "(", "self", ",", "dt", ",", "alpha", "=", ".5", ")", ":", "\n", "        ", "B", "=", "self", ".", "bilinear", "(", "dt", ",", "dt", ".", "new_zeros", "(", "*", "dt", ".", "shape", ",", "self", ".", "N", ")", ",", "dt", ".", "new_ones", "(", "1", ")", ",", "alpha", "=", "alpha", ")", "# (..., N)", "\n", "return", "B", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.hippo.transition.ManualAdaptiveTransition.__init__": [[165, 172], ["src.models.hippo.hippo.transition.AdaptiveTransition.__init__"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__"], ["    ", "def", "__init__", "(", "self", ",", "N", ",", "A", ",", "B", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"\n        A: (N, N)\n        B: (N,)\n        \"\"\"", "\n", "\n", "super", "(", ")", ".", "__init__", "(", "N", ",", "{", "'a'", ":", "A", ",", "'b'", ":", "B", "}", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.hippo.transition.ManualAdaptiveTransition._A": [[173, 175], ["None"], "methods", ["None"], ["", "def", "_A", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "a", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.hippo.transition.ManualAdaptiveTransition._B": [[176, 178], ["None"], "methods", ["None"], ["", "def", "_B", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "b", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.hippo.transition.ManualAdaptiveTransition.precompute_forward": [[180, 182], ["None"], "methods", ["None"], ["", "def", "precompute_forward", "(", "self", ",", "delta", ")", ":", "\n", "        ", "return", "self", ".", "I", "+", "delta", "*", "self", ".", "A", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.hippo.transition.ManualAdaptiveTransition.precompute_backward": [[183, 185], ["torch.linalg.solve", "torch.linalg.solve", "torch.linalg.solve", "torch.linalg.solve", "torch.linalg.solve", "torch.linalg.solve", "torch.linalg.solve", "torch.linalg.solve", "torch.linalg.solve"], "methods", ["None"], ["", "def", "precompute_backward", "(", "self", ",", "delta", ")", ":", "\n", "        ", "return", "torch", ".", "linalg", ".", "solve", "(", "self", ".", "I", "-", "delta", "*", "self", ".", "A", ",", "self", ".", "I", ")", "[", "0", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.hippo.transition.ManualAdaptiveTransition.quadratic": [[187, 194], ["torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "y.unsqueeze"], "methods", ["None"], ["", "def", "quadratic", "(", "self", ",", "x", ",", "y", ")", ":", "\n", "        ", "\"\"\" Implements the quadratic form given by the A matrix\n        x : (..., N)\n        y : (..., N)\n        returns: x^T A y (...)\n        \"\"\"", "\n", "return", "torch", ".", "sum", "(", "(", "self", ".", "A", "@", "y", ".", "unsqueeze", "(", "-", "1", ")", ")", ".", "squeeze", "(", "-", "1", ")", "*", "x", ",", "dim", "=", "-", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.hippo.transition.ManualAdaptiveTransition.forward_mult": [[195, 213], ["isinstance", "delta.unsqueeze.unsqueeze.unsqueeze", "src.models.hippo.hippo.transition.ManualAdaptiveTransition.A.transpose", "u.unsqueeze"], "methods", ["None"], ["", "def", "forward_mult", "(", "self", ",", "u", ",", "delta", ",", "transpose", "=", "False", ")", ":", "\n", "        ", "\"\"\" Computes (I + d A) u\n\n        A: (n, n)\n        u: (b1* d, n) d represents memory_size\n        delta: (b2*, d) or scalar\n          Assume len(b2) <= len(b1)\n\n        output: (broadcast(b1, b2)*, d, n)\n        \"\"\"", "\n", "\n", "if", "isinstance", "(", "delta", ",", "torch", ".", "Tensor", ")", ":", "\n", "            ", "delta", "=", "delta", ".", "unsqueeze", "(", "-", "1", ")", "\n", "", "A_", "=", "self", ".", "A", ".", "transpose", "(", "-", "1", ",", "-", "2", ")", "if", "transpose", "else", "self", ".", "A", "\n", "x", "=", "(", "A_", "@", "u", ".", "unsqueeze", "(", "-", "1", ")", ")", ".", "squeeze", "(", "-", "1", ")", "\n", "x", "=", "u", "+", "delta", "*", "x", "\n", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.hippo.transition.ManualAdaptiveTransition.inverse_mult": [[215, 233], ["isinstance", "zip", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "delta.unsqueeze().unsqueeze.unsqueeze().unsqueeze.unsqueeze().unsqueeze", "_A.transpose.transpose.transpose", "torch.linalg.solve().squeeze", "torch.linalg.solve().squeeze", "torch.linalg.solve().squeeze", "torch.linalg.solve().squeeze", "torch.linalg.solve().squeeze", "torch.linalg.solve().squeeze", "torch.linalg.solve().squeeze", "torch.linalg.solve().squeeze", "torch.linalg.solve().squeeze", "xs.append", "torch.broadcast_tensors", "torch.broadcast_tensors", "torch.broadcast_tensors", "torch.broadcast_tensors", "torch.broadcast_tensors", "torch.broadcast_tensors", "torch.broadcast_tensors", "torch.broadcast_tensors", "torch.broadcast_tensors", "delta.unsqueeze().unsqueeze.unsqueeze().unsqueeze.unsqueeze", "u.unsqueeze", "torch.linalg.solve", "torch.linalg.solve", "torch.linalg.solve", "torch.linalg.solve", "torch.linalg.solve", "torch.linalg.solve", "torch.linalg.solve", "torch.linalg.solve", "torch.linalg.solve"], "methods", ["None"], ["", "def", "inverse_mult", "(", "self", ",", "u", ",", "delta", ",", "transpose", "=", "False", ")", ":", "\n", "        ", "\"\"\" Computes (I - d A)^-1 u \"\"\"", "\n", "\n", "if", "isinstance", "(", "delta", ",", "torch", ".", "Tensor", ")", ":", "\n", "            ", "delta", "=", "delta", ".", "unsqueeze", "(", "-", "1", ")", ".", "unsqueeze", "(", "-", "1", ")", "\n", "", "_A", "=", "self", ".", "I", "-", "delta", "*", "self", ".", "A", "\n", "if", "transpose", ":", "_A", "=", "_A", ".", "transpose", "(", "-", "1", ",", "-", "2", ")", "\n", "\n", "# x = torch.linalg.solve(_A, u.unsqueeze(-1)).squeeze(-1)", "\n", "\n", "# TODO pass in a flag to toggle the two codepaths depending on how big the problem is", "\n", "xs", "=", "[", "]", "\n", "for", "_A_", ",", "u_", "in", "zip", "(", "*", "torch", ".", "broadcast_tensors", "(", "_A", ",", "u", ".", "unsqueeze", "(", "-", "1", ")", ")", ")", ":", "\n", "            ", "x_", "=", "torch", ".", "linalg", ".", "solve", "(", "_A_", ",", "u_", "[", "...", ",", ":", "1", "]", ")", ".", "squeeze", "(", "-", "1", ")", "\n", "xs", ".", "append", "(", "x_", ")", "\n", "", "x", "=", "torch", ".", "stack", "(", "xs", ",", "dim", "=", "0", ")", "\n", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.hippo.transition.OPManualAdaptiveTransition.__init__": [[238, 254], ["src.models.hippo.hippo.transition", "torch.as_tensor", "torch.as_tensor", "torch.as_tensor", "torch.as_tensor", "torch.as_tensor", "torch.as_tensor", "torch.as_tensor", "torch.as_tensor", "torch.as_tensor", "src.models.hippo.hippo.transition.ManualAdaptiveTransition.__init__", "torch.as_tensor", "torch.as_tensor", "torch.as_tensor", "torch.as_tensor", "torch.as_tensor", "torch.as_tensor", "torch.as_tensor", "torch.as_tensor", "torch.as_tensor", "print", "type"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.s4.transition", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__"], ["def", "__init__", "(", "self", ",", "N", ",", "verbose", "=", "False", ",", "measure_args", "=", "{", "}", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\" Slow (n^3, or n^2 if step sizes are cached) version via manual matrix mult/inv\n\n        delta: optional list of step sizes to cache the transitions for\n        \"\"\"", "\n", "A", ",", "B", "=", "transition", "(", "type", "(", "self", ")", ".", "measure", ",", "N", ",", "**", "measure_args", ")", "\n", "# super().__init__(N, A, B[:, 0])", "\n", "A", "=", "torch", ".", "as_tensor", "(", "A", ",", "dtype", "=", "torch", ".", "float", ")", "\n", "B", "=", "torch", ".", "as_tensor", "(", "B", ",", "dtype", "=", "torch", ".", "float", ")", "[", ":", ",", "0", "]", "\n", "# A = torch.Tensor(A)", "\n", "# B = torch.Tensor(B)[:, 0]", "\n", "super", "(", ")", ".", "__init__", "(", "N", ",", "A", ",", "B", ",", "**", "kwargs", ")", "\n", "\n", "\n", "if", "verbose", ":", "\n", "            ", "print", "(", "f\"{self.__class__}\\n  A {self.A}\\nB {self.B}\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.hippo.transition.CumsumAdaptiveTransition.__init__": [[274, 296], ["src.models.hippo.hippo.transition.AdaptiveTransition.__init__", "src.models.hippo.hippo.transition.CumsumAdaptiveTransition.register_buffer", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__"], ["    ", "def", "__init__", "(", "self", ",", "N", ",", "a", ",", "b", ")", ":", "\n", "        ", "\"\"\" Implements update for matrix A = -(L+aI) for forward, backward, bilinear, zoh discretizations.\n        a: scalar, the element on the diagonal\n        b: scalar, so that B = b * ones vector\n        \"\"\"", "\n", "# can't wrap scalars with torch.Tensor(), while torch.tensor(a) gives double instead of float or something", "\n", "# super().__init__(N, {'a': [a], 'b': [b]}, **kwargs) # TODO this should register b and then construct self.B using a @property, like in Toeplitz (but is slightly slower in the non-learnable case)", "\n", "params", "=", "{", "\n", "'a'", ":", "torch", ".", "tensor", "(", "a", ",", "dtype", "=", "torch", ".", "float", ")", ",", "\n", "'b'", ":", "torch", ".", "tensor", "(", "b", ",", "dtype", "=", "torch", ".", "float", ")", ",", "\n", "}", "\n", "super", "(", ")", ".", "__init__", "(", "N", ",", "params", ")", "\n", "\n", "# self.N = N", "\n", "# self.a = a", "\n", "# self.b = b", "\n", "\n", "# self.register_buffer('A', self.construct_A())", "\n", "# self.register_buffer('B', b * torch.ones(N))", "\n", "\n", "# self.register_buffer('I', torch.eye(N))", "\n", "self", ".", "register_buffer", "(", "'arange'", ",", "torch", ".", "arange", "(", "N", "-", "1", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.hippo.transition.CumsumAdaptiveTransition._A": [[298, 302], ["torch.tril", "torch.tril", "torch.tril", "torch.tril", "torch.tril", "torch.tril", "torch.tril", "torch.tril", "torch.tril", "src.models.hippo.hippo.transition.CumsumAdaptiveTransition.ones.repeat"], "methods", ["None"], ["", "def", "_A", "(", "self", ")", ":", "\n", "        ", "L", "=", "torch", ".", "tril", "(", "self", ".", "ones", ".", "repeat", "(", "self", ".", "N", ",", "1", ")", ")", "\n", "D", "=", "self", ".", "a", "*", "self", ".", "I", "\n", "return", "-", "(", "L", "+", "D", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.hippo.transition.CumsumAdaptiveTransition._B": [[303, 305], ["None"], "methods", ["None"], ["", "def", "_B", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "b", "*", "self", ".", "ones", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.hippo.transition.CumsumAdaptiveTransition.quadratic": [[307, 314], ["torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "y.unsqueeze"], "methods", ["None"], ["", "def", "quadratic", "(", "self", ",", "x", ",", "y", ")", ":", "\n", "        ", "\"\"\"\n        x : (..., N)\n        y : (..., N)\n        returns: x^T A y (...)\n        \"\"\"", "\n", "return", "torch", ".", "sum", "(", "(", "self", ".", "A", "@", "y", ".", "unsqueeze", "(", "-", "1", ")", ")", ".", "squeeze", "(", "-", "1", ")", "*", "x", ",", "dim", "=", "-", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.hippo.transition.CumsumAdaptiveTransition.precompute_forward": [[315, 325], ["isinstance", "isinstance", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "delta.unsqueeze.unsqueeze.unsqueeze", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "delta.unsqueeze.unsqueeze.new_ones"], "methods", ["None"], ["", "def", "precompute_forward", "(", "self", ",", "delta", ")", ":", "\n", "        ", "\"\"\" Store elements along the diagonals of (I + d A) \"\"\"", "\n", "if", "isinstance", "(", "delta", ",", "float", ")", ":", "\n", "            ", "delta", "=", "torch", ".", "tensor", "(", "delta", ")", ".", "to", "(", "self", ".", "I", ")", "\n", "", "if", "isinstance", "(", "delta", ",", "torch", ".", "Tensor", ")", ":", "\n", "            ", "delta", "=", "delta", ".", "unsqueeze", "(", "-", "1", ")", "\n", "", "a_", "=", "1.", "-", "delta", "*", "self", ".", "a", "# (..., 1)", "\n", "if", "self", ".", "N", "==", "1", ":", "\n", "            ", "return", "a_", "\n", "", "return", "torch", ".", "cat", "(", "(", "a_", ",", "-", "delta", "*", "delta", ".", "new_ones", "(", "self", ".", "N", "-", "1", ")", ")", ",", "-", "1", ")", "# (..., N)", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.hippo.transition.CumsumAdaptiveTransition.precompute_backward": [[326, 351], ["isinstance", "isinstance", "denom.reciprocal", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "delta.unsqueeze.unsqueeze.unsqueeze", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor"], "methods", ["None"], ["", "def", "precompute_backward", "(", "self", ",", "delta", ")", ":", "# TODO should be called inverse?", "\n", "        ", "\"\"\" Store elements along the diagonals of (I - d A)^{-1}\n\n        # a' = a + 1/dt\n        delta: (...)\n        output: (..., N)\n        \"\"\"", "\n", "if", "isinstance", "(", "delta", ",", "float", ")", ":", "\n", "            ", "delta", "=", "torch", ".", "tensor", "(", "delta", ")", ".", "to", "(", "self", ".", "I", ")", "\n", "", "if", "isinstance", "(", "delta", ",", "torch", ".", "Tensor", ")", ":", "\n", "            ", "delta", "=", "delta", ".", "unsqueeze", "(", "-", "1", ")", "\n", "\n", "", "if", "self", ".", "N", "==", "1", ":", "\n", "            ", "return", "1.", "/", "(", "1.", "+", "self", ".", "a", "*", "delta", "+", "delta", ")", "\n", "\n", "", "ad", "=", "self", ".", "a", "*", "delta", "# (..., 1)", "\n", "ad_p1", "=", "1", "+", "ad", "\n", "denom", "=", "ad_p1", "+", "delta", "# 1 + a'", "\n", "denom_inv", "=", "denom", ".", "reciprocal", "(", ")", "# 1. / denom", "\n", "s", "=", "-", "delta", "*", "denom_inv", "*", "denom_inv", "# -1/(1+a')^2", "\n", "b", "=", "ad_p1", "*", "denom_inv", "# a' / (1 + a')", "\n", "pows", "=", "b", "**", "self", ".", "arange", "## TODO benchmark against cumprod or cumsum in log space", "\n", "tail", "=", "s", "*", "pows", "\n", "ret", "=", "torch", ".", "cat", "(", "(", "denom_inv", ",", "tail", ")", ",", "-", "1", ")", "\n", "return", "ret", "\n", "# ad = self.a*delta # (..., 1)", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.hippo.transition.CumsumAdaptiveTransition.precompute_gbt_A": [[362, 367], ["src.models.hippo.hippo.transition.CumsumAdaptiveTransition.precompute_forward", "src.models.hippo.hippo.transition.CumsumAdaptiveTransition.precompute_backward", "src.models.functional.toeplitz.causal_convolution"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.hippo.transition.CumsumAdaptiveTransition.precompute_forward", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.hippo.transition.CumsumAdaptiveTransition.precompute_backward", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.toeplitz.causal_convolution"], ["", "def", "precompute_gbt_A", "(", "self", ",", "delta", ",", "alpha", "=", "0.5", ")", ":", "\n", "        ", "\"\"\" Return the A matrix of the gbt discretization \"\"\"", "\n", "c", "=", "self", ".", "precompute_forward", "(", "(", "1.", "-", "alpha", ")", "*", "delta", ")", "\n", "d", "=", "self", ".", "precompute_backward", "(", "alpha", "*", "delta", ")", "\n", "return", "causal_convolution", "(", "c", ",", "d", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.hippo.transition.CumsumAdaptiveTransition.precompute_gbt_B": [[368, 373], ["src.models.hippo.hippo.transition.CumsumAdaptiveTransition.precompute_backward", "torch.cumsum", "torch.cumsum", "torch.cumsum", "torch.cumsum", "torch.cumsum", "torch.cumsum", "torch.cumsum", "torch.cumsum", "torch.cumsum"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.hippo.transition.CumsumAdaptiveTransition.precompute_backward"], ["", "def", "precompute_gbt_B", "(", "self", ",", "delta", ",", "alpha", "=", "0.5", ")", ":", "\n", "        ", "\"\"\" Return the B matrix of the gbt discretization \"\"\"", "\n", "d", "=", "self", ".", "precompute_backward", "(", "alpha", "*", "delta", ")", "\n", "# return causal_convolution(d, torch.ones_like(d)) * self.b", "\n", "return", "torch", ".", "cumsum", "(", "d", ",", "-", "1", ")", "*", "self", ".", "b", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.hippo.transition.CumsumAdaptiveTransition.forward_mult": [[374, 393], ["isinstance", "delta.unsqueeze.unsqueeze.unsqueeze", "torch.cumsum().flip", "torch.cumsum().flip", "torch.cumsum().flip", "torch.cumsum().flip", "torch.cumsum().flip", "torch.cumsum().flip", "torch.cumsum().flip", "torch.cumsum().flip", "torch.cumsum().flip", "torch.cumsum", "torch.cumsum", "torch.cumsum", "torch.cumsum", "torch.cumsum", "torch.cumsum", "torch.cumsum", "torch.cumsum", "torch.cumsum", "torch.cumsum", "torch.cumsum", "torch.cumsum", "torch.cumsum", "torch.cumsum", "torch.cumsum", "torch.cumsum", "torch.cumsum", "torch.cumsum", "u.flip"], "methods", ["None"], ["", "def", "forward_mult", "(", "self", ",", "u", ",", "delta", ",", "transpose", "=", "False", ")", ":", "\n", "        ", "\"\"\" Computes (I + delta A) u\n\n        A: (n, n)\n        u: (..., n)\n        delta: (...) or scalar\n\n        output: (..., n)\n        \"\"\"", "\n", "if", "isinstance", "(", "delta", ",", "torch", ".", "Tensor", ")", ":", "\n", "            ", "delta", "=", "delta", ".", "unsqueeze", "(", "-", "1", ")", "\n", "\n", "", "if", "transpose", ":", "\n", "            ", "x", "=", "torch", ".", "cumsum", "(", "u", ".", "flip", "(", "-", "1", ")", ",", "-", "1", ")", ".", "flip", "(", "-", "1", ")", "\n", "", "else", ":", "\n", "            ", "x", "=", "torch", ".", "cumsum", "(", "u", ",", "-", "1", ")", "\n", "", "x", "=", "x", "+", "u", "*", "self", ".", "a", "\n", "x", "=", "u", "-", "delta", "*", "x", "# Because A is negated in the representation", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.hippo.transition.CumsumAdaptiveTransition.inverse_mult": [[394, 408], ["src.models.hippo.hippo.transition.CumsumAdaptiveTransition.precompute_backward", "src.models.functional.toeplitz.causal_convolution().flip", "src.models.functional.toeplitz.causal_convolution", "src.models.functional.toeplitz.causal_convolution", "u.flip"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.hippo.transition.CumsumAdaptiveTransition.precompute_backward", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.toeplitz.causal_convolution", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.toeplitz.causal_convolution"], ["", "def", "inverse_mult", "(", "self", ",", "u", ",", "delta", ",", "transpose", "=", "False", ")", ":", "\n", "        ", "\"\"\" Computes (I - d A)^-1 u \"\"\"", "\n", "# if isinstance(delta, torch.Tensor):", "\n", "#     delta = delta.unsqueeze(-1)", "\n", "# if isinstance(delta, float) and delta in self.backward_cache:", "\n", "#     c = self.backward_cache[delta]", "\n", "# else:", "\n", "# c = self.precompute_backward(delta, **kwargs)", "\n", "c", "=", "self", ".", "precompute_backward", "(", "delta", ")", "\n", "if", "transpose", ":", "\n", "            ", "x", "=", "causal_convolution", "(", "c", ",", "u", ".", "flip", "(", "-", "1", ")", ")", ".", "flip", "(", "-", "1", ")", "\n", "", "else", ":", "\n", "            ", "x", "=", "causal_convolution", "(", "c", ",", "u", ")", "\n", "", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.hippo.transition.LagTCumsumAdaptiveTransition.__init__": [[411, 414], ["src.models.hippo.hippo.transition.CumsumAdaptiveTransition.__init__"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__"], ["def", "__init__", "(", "self", ",", "N", ",", "beta", "=", "1.0", ")", ":", "\n", "# super().__init__(N, -0.5, 1.0)", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "N", ",", "-", "0.5", ",", "beta", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.hippo.transition.TLagTCumsumAdaptiveTransition.__init__": [[419, 421], ["src.models.hippo.hippo.transition.CumsumAdaptiveTransition.__init__"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__"], ["def", "__init__", "(", "self", ",", "N", ",", "beta", "=", "1.0", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "N", ",", "-", "(", "1.", "-", "beta", ")", "/", "2", ",", "beta", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.hippo.transition.GLagTCumsumAdaptiveTransition.__init__": [[427, 430], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "N", ",", "alpha", "=", "0.0", ",", "beta", "=", "0.01", ")", ":", "\n", "# TODO this is completely broken", "\n", "        ", "raise", "NotImplementedError", "\n", "# super().__init__(N, -(1.-beta)/2, beta)", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.hippo.transition.LegTAdaptiveTransition.__init__": [[436, 441], ["src.models.hippo.hippo.transition", "torch.as_tensor", "torch.as_tensor", "torch.as_tensor", "torch.as_tensor", "torch.as_tensor", "torch.as_tensor", "torch.as_tensor", "torch.as_tensor", "torch.as_tensor", "src.models.hippo.hippo.transition.AdaptiveTransition.__init__", "torch.as_tensor", "torch.as_tensor", "torch.as_tensor", "torch.as_tensor", "torch.as_tensor", "torch.as_tensor", "torch.as_tensor", "torch.as_tensor", "torch.as_tensor"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.s4.transition", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__"], ["    ", "def", "__init__", "(", "self", ",", "N", ")", ":", "# this class is not trainable", "\n", "        ", "A", ",", "B", "=", "transition", "(", "'legt'", ",", "N", ")", "\n", "A", "=", "torch", ".", "as_tensor", "(", "A", ",", "dtype", "=", "torch", ".", "float", ")", "\n", "B", "=", "torch", ".", "as_tensor", "(", "B", ",", "dtype", "=", "torch", ".", "float", ")", "[", ":", ",", "0", "]", "\n", "super", "(", ")", ".", "__init__", "(", "N", ",", "{", "'a'", ":", "A", ",", "'b'", ":", "B", "}", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.hippo.transition.LegTAdaptiveTransition._A": [[442, 444], ["None"], "methods", ["None"], ["", "def", "_A", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "a", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.hippo.transition.LegTAdaptiveTransition._B": [[445, 447], ["None"], "methods", ["None"], ["", "def", "_B", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "b", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.hippo.transition.LegTAdaptiveTransition.forward_mult": [[448, 451], ["legt_gbt_forward_t", "legt_gbt_forward"], "methods", ["None"], ["", "def", "forward_mult", "(", "self", ",", "u", ",", "delta", ",", "transpose", "=", "False", ")", ":", "\n", "        ", "if", "transpose", ":", "return", "legt_gbt_forward_t", "(", "delta", ",", "u", ",", "transpose", "=", "True", ")", "# TODO this is all broken", "\n", "else", ":", "return", "legt_gbt_forward", "(", "delta", ",", "u", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.hippo.transition.LegTAdaptiveTransition.inverse_mult": [[452, 455], ["legt_gbt_backward_t", "legt_gbt_backward"], "methods", ["None"], ["", "def", "inverse_mult", "(", "self", ",", "u", ",", "delta", ",", "transpose", "=", "False", ")", ":", "\n", "        ", "if", "transpose", ":", "return", "legt_gbt_backward_t", "(", "-", "delta", ",", "u", ",", "transpose", "=", "True", ")", "\n", "else", ":", "return", "legt_gbt_backward", "(", "-", "delta", ",", "u", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.hippo.transition.LegTAdaptiveTransition.quadratic": [[456, 464], ["torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "y.unsqueeze"], "methods", ["None"], ["", "def", "quadratic", "(", "self", ",", "x", ",", "y", ")", ":", "\n", "# TODO should use fast mult... also check if we even need this anymore", "\n", "        ", "\"\"\"\n        x : (..., N)\n        y : (..., N)\n        returns: x^T A y (...)\n        \"\"\"", "\n", "return", "torch", ".", "sum", "(", "(", "self", ".", "A", "@", "y", ".", "unsqueeze", "(", "-", "1", ")", ")", ".", "squeeze", "(", "-", "1", ")", "*", "x", ",", "dim", "=", "-", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.hippo.transition.TriDInverseAdaptiveTransition.__init__": [[468, 479], ["src.models.hippo.hippo.transition.AdaptiveTransition.__init__"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__"], ["def", "__init__", "(", "self", ",", "N", ",", "dl", ",", "d", ",", "du", ",", "pl", ",", "pr", ",", "c", ",", "b", ",", "**", "kwargs", ")", ":", "\n", "        ", "params", "=", "{", "\n", "'dl'", ":", "dl", ",", "\n", "'d'", ":", "d", ",", "\n", "'du'", ":", "du", ",", "\n", "'pl'", ":", "pl", ",", "\n", "'pr'", ":", "pr", ",", "\n", "'c'", ":", "c", ",", "\n", "'b'", ":", "b", ",", "\n", "}", "\n", "super", "(", ")", ".", "__init__", "(", "N", ",", "params", ",", "**", "kwargs", ")", "\n", "", "def", "_A", "(", "self", ")", ":", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.hippo.transition.TriDInverseAdaptiveTransition._A": [[479, 485], ["trid_solve().transpose", "trid_solve", "src.models.hippo.hippo.transition.TriDInverseAdaptiveTransition.pl.unsqueeze"], "methods", ["None"], ["", "def", "_A", "(", "self", ")", ":", "\n", "        ", "\"\"\" The matrix A for system x' = -Ax + Bu \"\"\"", "\n", "A", "=", "trid_solve", "(", "self", ".", "I", ",", "self", ".", "dl", ",", "self", ".", "d", ",", "self", ".", "du", ")", ".", "transpose", "(", "-", "1", ",", "-", "2", ")", "\n", "A", "=", "A", "+", "self", ".", "c", "*", "self", ".", "I", "\n", "A", "=", "self", ".", "pl", ".", "unsqueeze", "(", "-", "1", ")", "*", "A", "*", "self", ".", "pr", "\n", "return", "A", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.hippo.transition.TriDInverseAdaptiveTransition._B": [[486, 488], ["None"], "methods", ["None"], ["", "def", "_B", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "pl", "*", "self", ".", "b", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.hippo.transition.TriDInverseAdaptiveTransition.forward_mult": [[489, 505], ["trid_gbt_forward", "trid_gbt_forward"], "methods", ["None"], ["", "def", "forward_mult", "(", "self", ",", "u", ",", "delta", ",", "transpose", "=", "False", ")", ":", "\n", "        ", "du", "=", "self", ".", "du", "\n", "d", "=", "self", ".", "d", "\n", "dl", "=", "self", ".", "dl", "\n", "pr", "=", "self", ".", "pr", "\n", "pl", "=", "self", ".", "pl", "\n", "c", "=", "self", ".", "c", "\n", "if", "transpose", ":", "\n", "            ", "return", "trid_gbt_forward", "(", "\n", "delta", ",", "u", ",", "\n", "du", ",", "d", ",", "dl", ",", "pr", ",", "pl", ",", "c", ",", "\n", ")", "\n", "", "else", ":", "\n", "            ", "return", "trid_gbt_forward", "(", "\n", "delta", ",", "u", ",", "\n", "dl", ",", "d", ",", "du", ",", "pl", ",", "pr", ",", "c", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.hippo.transition.TriDInverseAdaptiveTransition.inverse_mult": [[507, 523], ["trid_gbt_backward", "trid_gbt_backward"], "methods", ["None"], ["", "", "def", "inverse_mult", "(", "self", ",", "u", ",", "delta", ",", "transpose", "=", "False", ")", ":", "\n", "        ", "du", "=", "self", ".", "du", "\n", "d", "=", "self", ".", "d", "\n", "dl", "=", "self", ".", "dl", "\n", "pr", "=", "self", ".", "pr", "\n", "pl", "=", "self", ".", "pl", "\n", "c", "=", "self", ".", "c", "\n", "if", "transpose", ":", "\n", "            ", "return", "trid_gbt_backward", "(", "\n", "delta", ",", "u", ",", "\n", "du", ",", "d", ",", "dl", ",", "pr", ",", "pl", ",", "c", ",", "\n", ")", "\n", "", "else", ":", "\n", "            ", "return", "trid_gbt_backward", "(", "\n", "delta", ",", "u", ",", "\n", "dl", ",", "d", ",", "du", ",", "pl", ",", "pr", ",", "c", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.hippo.transition.LegTTriDInverseAdaptiveTransition.__init__": [[529, 548], ["torch.sqrt", "torch.sqrt", "torch.sqrt", "torch.sqrt", "torch.sqrt", "torch.sqrt", "torch.sqrt", "torch.sqrt", "torch.sqrt", "transition._diag", "transition._diag", "src.models.hippo.hippo.transition.TriDInverseAdaptiveTransition.__init__", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.hippo.transition._diag", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.hippo.transition._diag", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__"], ["    ", "def", "__init__", "(", "self", ",", "N", ",", "corners", "=", "3", ",", "**", "kwargs", ")", ":", "\n", "        ", "p", "=", "torch", ".", "sqrt", "(", "1", "+", "2", "*", "torch", ".", "arange", "(", "N", ")", ")", "\n", "# p = torch.ones(N)", "\n", "dl", "=", "_diag", "(", "N", ",", "-", ".5", ")", "# + F.pad(torch.randn(N-1)*1e-4, (1, 1))", "\n", "du", "=", "_diag", "(", "N", ",", ".5", ")", "# + F.pad(torch.randn(N-1)*1e-4, (1, 1))", "\n", "d", "=", "torch", ".", "zeros", "(", "N", ")", "+", "torch", ".", "randn", "(", "N", ")", "*", "1e-2", "\n", "if", "corners", "==", "0", ":", "\n", "            ", "pass", "\n", "", "elif", "corners", "==", "1", ":", "\n", "            ", "d", "[", "0", "]", "+=", ".5", "\n", "", "elif", "corners", "==", "2", ":", "\n", "            ", "d", "[", "-", "1", "]", "+=", ".5", "\n", "", "elif", "corners", "==", "3", ":", "\n", "            ", "d", "[", "0", "]", "+=", ".5", "\n", "d", "[", "-", "1", "]", "+=", ".5", "\n", "", "else", ":", "raise", "NotImplementedError", "\n", "c", "=", "torch", ".", "ones", "(", "N", ")", "*", "0.", "# + torch.randn(N)*1e-4", "\n", "\n", "super", "(", ")", ".", "__init__", "(", "N", ",", "dl", ",", "d", ",", "du", ",", "p", ",", "p", ",", "c", ",", "torch", ".", "ones", "(", "N", ")", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.hippo.transition.LagTTriDInverseAdaptiveTransition.__init__": [[550, 558], ["torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "transition._diag", "transition._diag", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "src.models.hippo.hippo.transition.TriDInverseAdaptiveTransition.__init__", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.hippo.transition._diag", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.hippo.transition._diag", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__"], ["    ", "def", "__init__", "(", "self", ",", "N", ",", "**", "kwargs", ")", ":", "\n", "        ", "p", "=", "torch", ".", "ones", "(", "N", ")", "\n", "dl", "=", "_diag", "(", "N", ",", "-", "1.", ")", "\n", "du", "=", "_diag", "(", "N", ",", "0.", ")", "\n", "d", "=", "torch", ".", "ones", "(", "N", ")", "\n", "c", "=", "torch", ".", "ones", "(", "N", ")", "*", "-", ".5", "\n", "\n", "super", "(", ")", ".", "__init__", "(", "N", ",", "dl", ",", "d", ",", "du", ",", "p", ",", "p", ",", "c", ",", "torch", ".", "ones", "(", "N", ")", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.hippo.transition.LegSTriDInverseAdaptiveTransition.__init__": [[560, 578], ["transition._diag", "transition._diag", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "src.models.hippo.hippo.transition.TriDInverseAdaptiveTransition.__init__", "torch.sqrt", "torch.sqrt", "torch.sqrt", "torch.sqrt", "torch.sqrt", "torch.sqrt", "torch.sqrt", "torch.sqrt", "torch.sqrt", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.sqrt", "torch.sqrt", "torch.sqrt", "torch.sqrt", "torch.sqrt", "torch.sqrt", "torch.sqrt", "torch.sqrt", "torch.sqrt", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.hippo.transition._diag", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.hippo.transition._diag", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__"], ["    ", "def", "__init__", "(", "self", ",", "N", ",", "diag_scale", "=", "2", ",", "diag_add", "=", "True", ",", "**", "kwargs", ")", ":", "\n", "# print(diag_scale, kwargs)", "\n", "        ", "if", "diag_scale", "==", "2", ":", "\n", "            ", "p", "=", "torch", ".", "sqrt", "(", "2", "*", "torch", ".", "arange", "(", "N", ")", "+", "1", ")", "\n", "", "elif", "diag_scale", "==", "1", ":", "\n", "            ", "p", "=", "torch", ".", "sqrt", "(", "torch", ".", "arange", "(", "N", ")", "+", "1", ")", "\n", "", "elif", "diag_scale", "==", "0", ":", "\n", "            ", "p", "=", "torch", ".", "ones", "(", "N", ")", "\n", "", "else", ":", "raise", "NotImplementedError", "\n", "dl", "=", "_diag", "(", "N", ",", "-", "1.", ")", "\n", "du", "=", "_diag", "(", "N", ",", "0.", ")", "\n", "d", "=", "torch", ".", "ones", "(", "N", ")", "\n", "if", "diag_add", ":", "\n", "            ", "c", "=", "-", "torch", ".", "arange", "(", "N", ")", "/", "(", "2", "*", "torch", ".", "arange", "(", "N", ")", "+", "1", ")", "\n", "", "else", ":", "\n", "            ", "c", "=", "-", ".5", "*", "torch", ".", "ones", "(", "N", ")", "\n", "\n", "", "super", "(", ")", ".", "__init__", "(", "N", ",", "dl", ",", "d", ",", "du", ",", "p", ",", "p", ",", "c", ",", "torch", ".", "ones", "(", "N", ")", ",", "**", "kwargs", ")", "\n", "# print(self.A)", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.hippo.transition.ChebITriDInverseAdaptiveTransition.__init__": [[582, 596], ["torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "transition._diag", "transition._diag", "src.models.hippo.hippo.transition.TriDInverseAdaptiveTransition.__init__", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.hippo.transition._diag", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.hippo.transition._diag", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__"], ["    ", "def", "__init__", "(", "self", ",", "N", ",", "**", "kwargs", ")", ":", "\n", "# p = torch.sqrt(1+2*torch.arange(N))", "\n", "        ", "p", "=", "torch", ".", "ones", "(", "N", ")", "\n", "dl", "=", "_diag", "(", "N", ",", "-", ".5", ")", "# + F.pad(torch.randn(N-1)*1e-4, (1, 1))", "\n", "du", "=", "_diag", "(", "N", ",", ".5", ")", "# + F.pad(torch.randn(N-1)*1e-4, (1, 1))", "\n", "d", "=", "torch", ".", "zeros", "(", "N", ")", "+", "torch", ".", "randn", "(", "N", ")", "*", "1e-3", "\n", "# d = torch.zeros(N)", "\n", "# d[0] += .5", "\n", "# d[-1] += .5", "\n", "dl", "[", "0", "]", "*=", "2.", "**", ".5", "\n", "du", "[", "0", "]", "*=", "2.", "**", ".5", "\n", "c", "=", "torch", ".", "ones", "(", "N", ")", "*", "0.", "# + torch.randn(N)*1e-4", "\n", "\n", "super", "(", ")", ".", "__init__", "(", "N", ",", "dl", ",", "d", ",", "du", ",", "p", ",", "p", ",", "c", ",", "torch", ".", "ones", "(", "N", ")", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.hippo.transition.ChebIITriDInverseAdaptiveTransition.__init__": [[598, 610], ["torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "transition._diag", "src.models.hippo.hippo.transition.TriDInverseAdaptiveTransition.__init__", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.hippo.transition._diag", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__"], ["    ", "def", "__init__", "(", "self", ",", "N", ",", "**", "kwargs", ")", ":", "\n", "        ", "p", "=", "torch", ".", "ones", "(", "N", ")", "\n", "du", "=", "_diag", "(", "N", ",", ".5", ")", "\n", "# du = 2.0 * du", "\n", "# dl = _diag(N, -.5) + F.pad(torch.randn(N-1)*2e-1, (1, 1))", "\n", "# dl = F.pad(torch.randn(N-1), (1,1)) * .5", "\n", "dl", "=", "-", "du", "\n", "d", "=", "torch", ".", "zeros", "(", "N", ")", "+", "torch", ".", "randn", "(", "N", ")", "*", "1e-3", "\n", "# d = torch.zeros(N)", "\n", "c", "=", "torch", ".", "ones", "(", "N", ")", "*", "0.", "# + torch.randn(N)*1e-4", "\n", "\n", "super", "(", ")", ".", "__init__", "(", "N", ",", "dl", ",", "d", ",", "du", ",", "p", ",", "p", ",", "c", ",", "torch", ".", "ones", "(", "N", ")", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.hippo.transition.ToeplitzAdaptiveTransition.__init__": [[614, 627], ["src.models.hippo.hippo.transition.AdaptiveTransition.__init__", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "src.models.hippo.hippo.transition.ToeplitzAdaptiveTransition.register_buffer"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__"], ["def", "__init__", "(", "self", ",", "N", ",", "a", ",", "b", ",", "c", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\" Implements update for lower triangular Toeplitz transitions A.\n\n        a: represents the diagonals of a lower triangular Toeplitz transition matrix\n        b: B transition matrix\n        c: scaling factors\n\n        A = c a c^{-1}, B = c b (note that c represents \\Lambda^{-1} in the HiPPO paper)\n        \"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", "N", ",", "{", "'a'", ":", "a", ",", "'c'", ":", "c", ",", "'b'", ":", "b", "}", ",", "**", "kwargs", ")", "\n", "e", "=", "torch", ".", "zeros", "(", "N", ")", "\n", "e", "[", "0", "]", "=", "1.0", "\n", "self", ".", "register_buffer", "(", "'e'", ",", "e", ")", "# for convenience", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.hippo.transition.ToeplitzAdaptiveTransition._A": [[630, 639], ["src.models.functional.toeplitz.construct_toeplitz", "A.transpose.transpose.transpose", "src.models.hippo.hippo.transition.ToeplitzAdaptiveTransition.c.reciprocal", "src.models.hippo.hippo.transition.ToeplitzAdaptiveTransition.c.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.toeplitz.construct_toeplitz"], ["", "def", "_A", "(", "self", ")", ":", "# TODO do this for all classes? how to know when to cache A or not?", "\n", "# Z = torch.diag_embed(torch.ones(self.N-1), -1).to(self.a)", "\n", "# [21-09-14 TODO] changed the krylov construction but haven't tested", "\n", "# Z = torch.diag_embed(self.ones[:-1], -1)", "\n", "# A = krylov(self.N, Z, self.a) # TODO use toeplitz.toeplitz_krylov_fast instead", "\n", "        ", "A", "=", "construct_toeplitz", "(", "self", ".", "a", ")", "\n", "A", "=", "A", ".", "transpose", "(", "0", ",", "1", ")", "\n", "A", "=", "self", ".", "c", ".", "unsqueeze", "(", "-", "1", ")", "*", "A", "*", "self", ".", "c", ".", "reciprocal", "(", ")", "\n", "return", "A", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.hippo.transition.ToeplitzAdaptiveTransition._B": [[641, 643], ["None"], "methods", ["None"], ["", "def", "_B", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "c", "*", "self", ".", "b", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.hippo.transition.ToeplitzAdaptiveTransition.quadratic": [[646, 653], ["torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "y.unsqueeze"], "methods", ["None"], ["", "def", "quadratic", "(", "self", ",", "x", ",", "y", ")", ":", "# TODO need this? also, move to main superclass", "\n", "        ", "\"\"\"\n        x : (..., N)\n        y : (..., N)\n        returns: x^T A y (...)\n        \"\"\"", "\n", "return", "torch", ".", "sum", "(", "(", "self", ".", "A", "@", "y", ".", "unsqueeze", "(", "-", "1", ")", ")", ".", "squeeze", "(", "-", "1", ")", "*", "x", ",", "dim", "=", "-", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.hippo.transition.ToeplitzAdaptiveTransition._mult": [[654, 664], ["src.models.functional.toeplitz.causal_convolution().flip", "src.models.functional.toeplitz.causal_convolution", "src.models.hippo.hippo.transition.ToeplitzAdaptiveTransition.c.reciprocal", "src.models.hippo.hippo.transition.ToeplitzAdaptiveTransition.c.reciprocal", "src.models.functional.toeplitz.causal_convolution", "src.models.functional.toeplitz.causal_convolution.flip"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.toeplitz.causal_convolution", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.toeplitz.causal_convolution"], ["", "def", "_mult", "(", "self", ",", "t", ",", "u", ",", "transpose", ")", ":", "\n", "        ", "if", "transpose", ":", "\n", "            ", "x", "=", "self", ".", "c", "*", "u", "\n", "x", "=", "causal_convolution", "(", "t", ",", "x", ".", "flip", "(", "-", "1", ")", ")", ".", "flip", "(", "-", "1", ")", "\n", "x", "=", "self", ".", "c", ".", "reciprocal", "(", ")", "*", "x", "\n", "", "else", ":", "\n", "            ", "x", "=", "self", ".", "c", ".", "reciprocal", "(", ")", "*", "u", "\n", "x", "=", "causal_convolution", "(", "t", ",", "x", ")", "\n", "x", "=", "self", ".", "c", "*", "x", "\n", "", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.hippo.transition.ToeplitzAdaptiveTransition.forward_mult": [[665, 677], ["src.models.hippo.hippo.transition.ToeplitzAdaptiveTransition._mult", "delta.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.hippo.transition.ToeplitzAdaptiveTransition._mult"], ["", "def", "forward_mult", "(", "self", ",", "u", ",", "delta", ",", "transpose", "=", "False", ")", ":", "\n", "        ", "\"\"\" Computes y = (I - delta A) u\n\n        self.a: (..., n)\n        u: (..., n)\n        delta: (...)\n\n        x: (..., n)\n        \"\"\"", "\n", "\n", "t", "=", "self", ".", "e", "-", "delta", ".", "unsqueeze", "(", "-", "1", ")", "*", "self", ".", "a", "# represents (I - delta A)", "\n", "return", "self", ".", "_mult", "(", "t", ",", "u", ",", "transpose", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.hippo.transition.ToeplitzAdaptiveTransition.inverse_mult": [[678, 685], ["src.models.functional.toeplitz.causal_convolution_inverse", "src.models.hippo.hippo.transition.ToeplitzAdaptiveTransition._mult", "delta.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.toeplitz.causal_convolution_inverse", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.hippo.transition.ToeplitzAdaptiveTransition._mult"], ["", "def", "inverse_mult", "(", "self", ",", "u", ",", "delta", ",", "transpose", "=", "False", ")", ":", "\n", "        ", "\"\"\" Computes (I + d A)^-1 u \"\"\"", "\n", "\n", "t", "=", "self", ".", "e", "+", "delta", ".", "unsqueeze", "(", "-", "1", ")", "*", "self", ".", "a", "\n", "# t_ = causal_convolution_inverse_wrong(t, self.e) # represents (I + delta A)^-1", "\n", "t_", "=", "causal_convolution_inverse", "(", "t", ")", "# represents (I + delta A)^-1", "\n", "return", "self", ".", "_mult", "(", "t_", ",", "u", ",", "transpose", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.hippo.transition.LagTToeplitzAdaptiveTransition.__init__": [[687, 693], ["torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "src.models.hippo.hippo.transition.ToeplitzAdaptiveTransition.__init__"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__"], ["    ", "def", "__init__", "(", "self", ",", "N", ",", "**", "kwargs", ")", ":", "\n", "        ", "a", "=", "torch", ".", "ones", "(", "N", ")", "\n", "a", "[", "...", ",", "0", "]", "=", ".5", "\n", "b", "=", "torch", ".", "ones", "(", "N", ")", "\n", "c", "=", "torch", ".", "ones", "(", "N", ")", "\n", "super", "(", ")", ".", "__init__", "(", "N", ",", "a", ",", "b", ",", "c", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.hippo.transition.GLagTToeplitzAdaptiveTransition.__init__": [[695, 706], ["torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.as_tensor", "torch.as_tensor", "torch.as_tensor", "torch.as_tensor", "torch.as_tensor", "torch.as_tensor", "torch.as_tensor", "torch.as_tensor", "torch.as_tensor", "numpy.exp", "torch.as_tensor", "torch.as_tensor", "torch.as_tensor", "torch.as_tensor", "torch.as_tensor", "torch.as_tensor", "torch.as_tensor", "torch.as_tensor", "torch.as_tensor", "src.models.hippo.hippo.transition.ToeplitzAdaptiveTransition.__init__", "scipy.special.binom", "numpy.exp", "numpy.arange", "scipy.special.gammaln", "scipy.special.gammaln", "numpy.arange", "scipy.special.gammaln", "numpy.arange", "numpy.arange"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__"], ["    ", "def", "__init__", "(", "self", ",", "N", ",", "alpha", "=", "0.0", ",", "beta", "=", "0.01", ",", "**", "kwargs", ")", ":", "\n", "        ", "a", "=", "torch", ".", "ones", "(", "N", ")", "\n", "a", "[", "...", ",", "0", "]", "=", "(", "1.", "+", "beta", ")", "/", "2.", "\n", "# b = torch.ones(N)", "\n", "b", "=", "ss", ".", "binom", "(", "alpha", "+", "np", ".", "arange", "(", "N", ")", ",", "np", ".", "arange", "(", "N", ")", ")", "*", "np", ".", "exp", "(", "-", ".5", "*", "ss", ".", "gammaln", "(", "1", "-", "alpha", ")", ")", "*", "beta", "**", "(", "(", "1", "-", "alpha", ")", "/", "2", ")", "\n", "b", "=", "torch", ".", "as_tensor", "(", "b", ",", "dtype", "=", "torch", ".", "float", ")", "\n", "# c = torch.ones(N)", "\n", "c", "=", "np", ".", "exp", "(", ".5", "*", "(", "ss", ".", "gammaln", "(", "np", ".", "arange", "(", "N", ")", "+", "alpha", "+", "1", ")", "-", "ss", ".", "gammaln", "(", "np", ".", "arange", "(", "N", ")", "+", "1", ")", ")", ")", "\n", "c", "=", "1.", "/", "c", "\n", "c", "=", "torch", ".", "as_tensor", "(", "c", ",", "dtype", "=", "torch", ".", "float", ")", "\n", "super", "(", ")", ".", "__init__", "(", "N", ",", "a", ",", "b", ",", "c", ",", "**", "kwargs", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.hippo.transition._diag": [[526, 527], ["torch.pad", "torch.ones", "torch.ones", "torch.ones"], "function", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.sc.pad"], ["", "", "", "def", "_diag", "(", "N", ",", "c", ")", ":", "return", "F", ".", "pad", "(", "torch", ".", "ones", "(", "N", "-", "1", ")", ",", "(", "1", ",", "1", ")", ")", "*", "c", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.hippo.standalone.HiPPO_LegT.__init__": [[38, 59], ["torch.Module.__init__", "src.models.hippo.hippo.transition", "numpy.ones", "numpy.zeros", "scipy.signal.cont2discrete", "B.squeeze.squeeze.squeeze", "standalone.HiPPO_LegT.register_buffer", "standalone.HiPPO_LegT.register_buffer", "numpy.arange", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "scipy.special.eval_legendre", "numpy.arange"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.s4.transition"], ["    ", "def", "__init__", "(", "self", ",", "N", ",", "dt", "=", "1.0", ",", "discretization", "=", "'bilinear'", ")", ":", "\n", "        ", "\"\"\"\n        N: the order of the HiPPO projection\n        dt: discretization step size - should be roughly inverse to the length of the sequence\n        \"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "N", "=", "N", "\n", "A", ",", "B", "=", "transition", "(", "'lmu'", ",", "N", ")", "\n", "C", "=", "np", ".", "ones", "(", "(", "1", ",", "N", ")", ")", "\n", "D", "=", "np", ".", "zeros", "(", "(", "1", ",", ")", ")", "\n", "# dt, discretization options", "\n", "A", ",", "B", ",", "_", ",", "_", ",", "_", "=", "signal", ".", "cont2discrete", "(", "(", "A", ",", "B", ",", "C", ",", "D", ")", ",", "dt", "=", "dt", ",", "method", "=", "discretization", ")", "\n", "\n", "B", "=", "B", ".", "squeeze", "(", "-", "1", ")", "\n", "\n", "self", ".", "register_buffer", "(", "'A'", ",", "torch", ".", "Tensor", "(", "A", ")", ")", "# (N, N)", "\n", "self", ".", "register_buffer", "(", "'B'", ",", "torch", ".", "Tensor", "(", "B", ")", ")", "# (N,)", "\n", "\n", "# vals = np.linspace(0.0, 1.0, 1./dt)", "\n", "vals", "=", "np", ".", "arange", "(", "0.0", ",", "1.0", ",", "dt", ")", "\n", "self", ".", "eval_matrix", "=", "torch", ".", "Tensor", "(", "ss", ".", "eval_legendre", "(", "np", ".", "arange", "(", "N", ")", "[", ":", ",", "None", "]", ",", "1", "-", "2", "*", "vals", ")", ".", "T", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.hippo.standalone.HiPPO_LegT.forward": [[60, 75], ["inputs.unsqueeze.unsqueeze.unsqueeze", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "cs.append", "torch.linear", "torch.linear", "torch.linear", "torch.linear"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "inputs", ")", ":", "\n", "        ", "\"\"\"\n        inputs : (length, ...)\n        output : (length, ..., N) where N is the order of the HiPPO projection\n        \"\"\"", "\n", "\n", "inputs", "=", "inputs", ".", "unsqueeze", "(", "-", "1", ")", "\n", "u", "=", "inputs", "*", "self", ".", "B", "# (length, ..., N)", "\n", "\n", "c", "=", "torch", ".", "zeros", "(", "u", ".", "shape", "[", "1", ":", "]", ")", "\n", "cs", "=", "[", "]", "\n", "for", "f", "in", "inputs", ":", "\n", "            ", "c", "=", "F", ".", "linear", "(", "c", ",", "self", ".", "A", ")", "+", "self", ".", "B", "*", "f", "\n", "cs", ".", "append", "(", "c", ")", "\n", "", "return", "torch", ".", "stack", "(", "cs", ",", "dim", "=", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.hippo.standalone.HiPPO_LegT.reconstruct": [[76, 78], ["c.unsqueeze"], "methods", ["None"], ["", "def", "reconstruct", "(", "self", ",", "c", ")", ":", "\n", "        ", "return", "(", "self", ".", "eval_matrix", "@", "c", ".", "unsqueeze", "(", "-", "1", ")", ")", ".", "squeeze", "(", "-", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.hippo.standalone.HiPPO_LegS.__init__": [[83, 114], ["torch.Module.__init__", "src.models.hippo.hippo.transition", "B.squeeze.squeeze.squeeze", "numpy.empty", "numpy.empty", "range", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "numpy.linspace", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "numpy.eye", "scipy.linalg.solve_triangular", "scipy.linalg.solve_triangular", "scipy.special.eval_legendre", "numpy.eye", "scipy.linalg.solve_triangular", "scipy.linalg.solve_triangular", "scipy.linalg.expm", "scipy.linalg.solve_triangular", "numpy.eye", "numpy.eye", "numpy.arange", "numpy.eye", "numpy.eye", "numpy.eye", "math.log", "math.log"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.s4.transition"], ["def", "__init__", "(", "self", ",", "N", ",", "max_length", "=", "1024", ",", "measure", "=", "'legs'", ",", "discretization", "=", "'bilinear'", ")", ":", "\n", "        ", "\"\"\"\n        max_length: maximum sequence length\n        \"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "N", "=", "N", "\n", "A", ",", "B", "=", "transition", "(", "measure", ",", "N", ")", "\n", "B", "=", "B", ".", "squeeze", "(", "-", "1", ")", "\n", "A_stacked", "=", "np", ".", "empty", "(", "(", "max_length", ",", "N", ",", "N", ")", ",", "dtype", "=", "A", ".", "dtype", ")", "\n", "B_stacked", "=", "np", ".", "empty", "(", "(", "max_length", ",", "N", ")", ",", "dtype", "=", "B", ".", "dtype", ")", "\n", "for", "t", "in", "range", "(", "1", ",", "max_length", "+", "1", ")", ":", "\n", "            ", "At", "=", "A", "/", "t", "\n", "Bt", "=", "B", "/", "t", "\n", "if", "discretization", "==", "'forward'", ":", "\n", "                ", "A_stacked", "[", "t", "-", "1", "]", "=", "np", ".", "eye", "(", "N", ")", "+", "At", "\n", "B_stacked", "[", "t", "-", "1", "]", "=", "Bt", "\n", "", "elif", "discretization", "==", "'backward'", ":", "\n", "                ", "A_stacked", "[", "t", "-", "1", "]", "=", "la", ".", "solve_triangular", "(", "np", ".", "eye", "(", "N", ")", "-", "At", ",", "np", ".", "eye", "(", "N", ")", ",", "lower", "=", "True", ")", "\n", "B_stacked", "[", "t", "-", "1", "]", "=", "la", ".", "solve_triangular", "(", "np", ".", "eye", "(", "N", ")", "-", "At", ",", "Bt", ",", "lower", "=", "True", ")", "\n", "", "elif", "discretization", "==", "'bilinear'", ":", "\n", "                ", "A_stacked", "[", "t", "-", "1", "]", "=", "la", ".", "solve_triangular", "(", "np", ".", "eye", "(", "N", ")", "-", "At", "/", "2", ",", "np", ".", "eye", "(", "N", ")", "+", "At", "/", "2", ",", "lower", "=", "True", ")", "\n", "B_stacked", "[", "t", "-", "1", "]", "=", "la", ".", "solve_triangular", "(", "np", ".", "eye", "(", "N", ")", "-", "At", "/", "2", ",", "Bt", ",", "lower", "=", "True", ")", "\n", "", "else", ":", "# ZOH", "\n", "                ", "A_stacked", "[", "t", "-", "1", "]", "=", "la", ".", "expm", "(", "A", "*", "(", "math", ".", "log", "(", "t", "+", "1", ")", "-", "math", ".", "log", "(", "t", ")", ")", ")", "\n", "B_stacked", "[", "t", "-", "1", "]", "=", "la", ".", "solve_triangular", "(", "A", ",", "A_stacked", "[", "t", "-", "1", "]", "@", "B", "-", "B", ",", "lower", "=", "True", ")", "\n", "", "", "self", ".", "A_stacked", "=", "torch", ".", "Tensor", "(", "A_stacked", ")", "# (max_length, N, N)", "\n", "self", ".", "B_stacked", "=", "torch", ".", "Tensor", "(", "B_stacked", ")", "# (max_length, N)", "\n", "# print(\"B_stacked shape\", B_stacked.shape)", "\n", "\n", "vals", "=", "np", ".", "linspace", "(", "0.0", ",", "1.0", ",", "max_length", ")", "\n", "self", ".", "eval_matrix", "=", "torch", ".", "Tensor", "(", "(", "B", "[", ":", ",", "None", "]", "*", "ss", ".", "eval_legendre", "(", "np", ".", "arange", "(", "N", ")", "[", ":", ",", "None", "]", ",", "2", "*", "vals", "-", "1", ")", ")", ".", "T", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.hippo.standalone.HiPPO_LegS.forward": [[115, 133], ["inputs.unsqueeze.unsqueeze.unsqueeze", "torch.transpose", "torch.transpose", "torch.transpose", "torch.transpose", "torch.transpose", "torch.transpose", "torch.transpose", "torch.transpose", "torch.transpose", "torch.transpose", "torch.transpose", "torch.transpose", "torch.transpose", "torch.transpose", "torch.transpose", "torch.transpose", "torch.transpose", "torch.transpose", "torch.transpose", "torch.transpose", "torch.transpose", "torch.transpose", "torch.transpose", "torch.transpose", "torch.transpose", "torch.transpose", "torch.transpose", "torch.transpose", "torch.transpose", "torch.transpose", "torch.transpose", "torch.transpose", "src.variable_unroll_matrix", "src.variable_unroll_matrix_sequential"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.unroll.variable_unroll_matrix", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.unroll.variable_unroll_matrix_sequential"], ["", "def", "forward", "(", "self", ",", "inputs", ",", "fast", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        inputs : (length, ...)\n        output : (length, ..., N) where N is the order of the HiPPO projection\n        \"\"\"", "\n", "\n", "L", "=", "inputs", ".", "shape", "[", "0", "]", "\n", "\n", "inputs", "=", "inputs", ".", "unsqueeze", "(", "-", "1", ")", "\n", "u", "=", "torch", ".", "transpose", "(", "inputs", ",", "0", ",", "-", "2", ")", "\n", "u", "=", "u", "*", "self", ".", "B_stacked", "[", ":", "L", "]", "\n", "u", "=", "torch", ".", "transpose", "(", "u", ",", "0", ",", "-", "2", ")", "# (length, ..., N)", "\n", "\n", "if", "fast", ":", "\n", "            ", "result", "=", "unroll", ".", "variable_unroll_matrix", "(", "self", ".", "A_stacked", "[", ":", "L", "]", ",", "u", ")", "\n", "", "else", ":", "\n", "            ", "result", "=", "unroll", ".", "variable_unroll_matrix_sequential", "(", "self", ".", "A_stacked", "[", ":", "L", "]", ",", "u", ")", "\n", "", "return", "result", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.hippo.standalone.HiPPO_LegS.reconstruct": [[134, 137], ["a.squeeze", "c.unsqueeze"], "methods", ["None"], ["", "def", "reconstruct", "(", "self", ",", "c", ")", ":", "\n", "        ", "a", "=", "self", ".", "eval_matrix", "@", "c", ".", "unsqueeze", "(", "-", "1", ")", "\n", "return", "a", ".", "squeeze", "(", "-", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.hippo.standalone.FunctionApprox.__init__": [[141, 150], ["numpy.random.RandomState", "nengo.processes.WhiteSignal", "numpy.empty", "range", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.TensorDataset.__init__", "nengo.processes.WhiteSignal.run_steps"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__"], ["    ", "def", "__init__", "(", "self", ",", "length", ",", "dt", ",", "nbatches", ",", "freq", "=", "10.0", ",", "seed", "=", "0", ")", ":", "\n", "        ", "rng", "=", "np", ".", "random", ".", "RandomState", "(", "seed", "=", "seed", ")", "\n", "process", "=", "nengo", ".", "processes", ".", "WhiteSignal", "(", "length", "*", "dt", ",", "high", "=", "freq", ",", "y0", "=", "0", ")", "\n", "X", "=", "np", ".", "empty", "(", "(", "nbatches", ",", "length", ",", "1", ")", ")", "\n", "for", "i", "in", "range", "(", "nbatches", ")", ":", "\n", "            ", "X", "[", "i", ",", ":", "]", "=", "process", ".", "run_steps", "(", "length", ",", "dt", "=", "dt", ",", "rng", "=", "rng", ")", "\n", "# X[i, :] /= np.max(np.abs(X[i, :]))", "\n", "", "X", "=", "torch", ".", "Tensor", "(", "X", ")", "\n", "super", "(", ")", ".", "__init__", "(", "X", ",", "X", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.hippo.standalone.test": [[152, 174], ["standalone.HiPPO_LegT", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "HiPPO_LegT.", "print", "standalone.HiPPO_LegT.reconstruct", "print", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "print", "standalone.HiPPO_LegS", "HiPPO_LegS.", "HiPPO_LegS.", "print", "standalone.HiPPO_LegS.reconstruct", "torch.randn.squeeze"], "function", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.hippo.standalone.HiPPO_LegS.reconstruct", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.hippo.standalone.HiPPO_LegS.reconstruct"], ["", "", "def", "test", "(", ")", ":", "\n", "    ", "N", "=", "256", "\n", "L", "=", "128", "\n", "hippo", "=", "HiPPO_LegT", "(", "N", ",", "dt", "=", "1.", "/", "L", ")", "\n", "\n", "x", "=", "torch", ".", "randn", "(", "L", ",", "1", ")", "\n", "\n", "y", "=", "hippo", "(", "x", ")", "\n", "print", "(", "y", ".", "shape", ")", "\n", "z", "=", "hippo", ".", "reconstruct", "(", "y", ")", "\n", "print", "(", "z", ".", "shape", ")", "\n", "\n", "# mse = torch.mean((z[-1,0,:L].flip(-1) - x.squeeze(-1))**2)", "\n", "mse", "=", "torch", ".", "mean", "(", "(", "z", "[", "-", "1", ",", "0", ",", ":", "L", "]", "-", "x", ".", "squeeze", "(", "-", "1", ")", ")", "**", "2", ")", "\n", "print", "(", "mse", ")", "\n", "\n", "# print(y.shape)", "\n", "hippo_legs", "=", "HiPPO_LegS", "(", "N", ",", "max_length", "=", "L", ")", "\n", "y", "=", "hippo_legs", "(", "x", ")", "\n", "# print(y.shape)", "\n", "z", "=", "hippo_legs", "(", "x", ",", "fast", "=", "True", ")", "\n", "print", "(", "hippo_legs", ".", "reconstruct", "(", "z", ")", ".", "shape", ")", "\n", "# print(y-z)", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.hippo.standalone.plot": [[177, 208], ["standalone.FunctionApprox", "standalone.FunctionApprox", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "iter", "next", "next", "f.squeeze().squeeze.squeeze().squeeze", "standalone.HiPPO_LegT", "standalone.HiPPO_LegS", "print", "print", "numpy.linspace", "matplotlib.figure", "matplotlib.plot", "matplotlib.plot", "matplotlib.plot", "matplotlib.xlabel", "matplotlib.xticks", "matplotlib.legend", "matplotlib.savefig", "matplotlib.close", "standalone.HiPPO_LegT.reconstruct", "standalone.HiPPO_LegS.reconstruct", "torch.mse_loss", "torch.mse_loss", "f.squeeze().squeeze.squeeze", "HiPPO_LegT.", "HiPPO_LegS."], "function", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.hippo.standalone.plot", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.hippo.standalone.plot", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.hippo.standalone.plot", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.hippo.standalone.HiPPO_LegS.reconstruct", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.hippo.standalone.HiPPO_LegS.reconstruct"], ["", "def", "plot", "(", ")", ":", "\n", "    ", "T", "=", "10000", "\n", "dt", "=", "1e-3", "\n", "N", "=", "256", "\n", "nbatches", "=", "10", "\n", "train", "=", "FunctionApprox", "(", "T", ",", "dt", ",", "nbatches", ",", "freq", "=", "1.0", ",", "seed", "=", "0", ")", "\n", "test", "=", "FunctionApprox", "(", "T", ",", "dt", ",", "nbatches", ",", "freq", "=", "1.0", ",", "seed", "=", "1", ")", "\n", "test_loader", "=", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "test", ",", "batch_size", "=", "1", ",", "shuffle", "=", "False", ")", "\n", "it", "=", "iter", "(", "test_loader", ")", "\n", "f", ",", "_", "=", "next", "(", "it", ")", "\n", "f", ",", "_", "=", "next", "(", "it", ")", "\n", "f", "=", "f", ".", "squeeze", "(", "0", ")", ".", "squeeze", "(", "-", "1", ")", "\n", "\n", "legt", "=", "HiPPO_LegT", "(", "N", ",", "1.", "/", "T", ")", "\n", "f_legt", "=", "legt", ".", "reconstruct", "(", "legt", "(", "f", ")", ")", "[", "-", "1", "]", "\n", "legs", "=", "HiPPO_LegS", "(", "N", ",", "T", ")", "\n", "f_legs", "=", "legs", ".", "reconstruct", "(", "legs", "(", "f", ")", ")", "[", "-", "1", "]", "\n", "print", "(", "F", ".", "mse_loss", "(", "f", ",", "f_legt", ")", ")", "\n", "print", "(", "F", ".", "mse_loss", "(", "f", ",", "f_legs", ")", ")", "\n", "\n", "vals", "=", "np", ".", "linspace", "(", "0.0", ",", "1.0", ",", "T", ")", "\n", "plt", ".", "figure", "(", "figsize", "=", "(", "6", ",", "2", ")", ")", "\n", "plt", ".", "plot", "(", "vals", ",", "f", "+", "0.1", ",", "'k'", ",", "linewidth", "=", "1.0", ")", "\n", "plt", ".", "plot", "(", "vals", "[", ":", "T", "//", "1", "]", ",", "f_legt", "[", ":", "T", "//", "1", "]", ")", "\n", "plt", ".", "plot", "(", "vals", "[", ":", "T", "//", "1", "]", ",", "f_legs", "[", ":", "T", "//", "1", "]", ")", "\n", "plt", ".", "xlabel", "(", "'Time (normalized)'", ",", "labelpad", "=", "-", "10", ")", "\n", "plt", ".", "xticks", "(", "[", "0", ",", "1", "]", ")", "\n", "plt", ".", "legend", "(", "[", "'f'", ",", "'legt'", ",", "'legs'", "]", ")", "\n", "plt", ".", "savefig", "(", "f'function_approx_whitenoise.pdf'", ",", "bbox_inches", "=", "'tight'", ")", "\n", "# plt.show()", "\n", "plt", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.nn.residual.Residual.__init__": [[9, 18], ["torch.nn.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__"], ["def", "__init__", "(", "self", ",", "i_layer", ",", "d_input", ",", "d_model", ",", "alpha", "=", "1.0", ",", "beta", "=", "1.0", ")", ":", "\n", "# print(\"ConstantResidual extra kwargs\", kwargs)", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "assert", "(", "d_input", "==", "d_model", ")", "or", "alpha", "==", "0.0", "\n", "self", ".", "i_layer", "=", "i_layer", "\n", "self", ".", "d_input", "=", "d_input", "\n", "self", ".", "d_model", "=", "d_model", "\n", "self", ".", "alpha", "=", "alpha", "\n", "self", ".", "beta", "=", "beta", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.nn.residual.Residual.d_output": [[19, 22], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "d_output", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "d_model", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.nn.residual.Residual.forward": [[23, 26], ["None"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ",", "y", ",", "transposed", ")", ":", "# TODO documentation of transposed", "\n", "        ", "y", "=", "self", ".", "beta", "*", "y", "if", "self", ".", "beta", "!=", "1.0", "else", "y", "\n", "return", "self", ".", "alpha", "*", "x", "+", "y", "if", "self", ".", "alpha", "else", "y", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.nn.residual.Affine.__init__": [[33, 42], ["residual.Residual.__init__", "torch.nn.Parameter", "torch.ones"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__"], ["def", "__init__", "(", "self", ",", "*", "args", ",", "scalar", "=", "True", ",", "gamma", "=", "0.0", ",", "**", "kwargs", ")", ":", "\n", "# print(\"ConstantResidual extra kwargs\", kwargs)", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "*", "args", ",", "**", "kwargs", ")", "\n", "self", ".", "scalar", "=", "scalar", "\n", "self", ".", "gamma", "=", "gamma", "\n", "\n", "c", "=", "self", ".", "beta", "*", "self", ".", "i_layer", "**", "(", "-", "self", ".", "gamma", ")", "\n", "d", "=", "1", "if", "self", ".", "scalar", "else", "self", ".", "d_input", "\n", "self", ".", "affine", "=", "nn", ".", "Parameter", "(", "c", "*", "torch", ".", "ones", "(", "d", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.nn.residual.Affine.forward": [[43, 47], ["c.unsqueeze.unsqueeze.unsqueeze"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ",", "y", ",", "transposed", ")", ":", "# TODO documentation of transposed", "\n", "        ", "c", "=", "self", ".", "affine", "\n", "if", "transposed", ":", "c", "=", "c", ".", "unsqueeze", "(", "-", "1", ")", "\n", "return", "self", ".", "alpha", "*", "x", "+", "c", "*", "y", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.nn.residual.Feedforward.__init__": [[50, 53], ["residual.Residual.__init__"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__"], ["    ", "def", "__init__", "(", "self", ",", "*", "args", ")", ":", "\n", "# print(\"Feedforward extra kwargs\", kwargs)", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "*", "args", ",", "alpha", "=", "0.0", ",", "beta", "=", "1.0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.nn.residual.Highway.__init__": [[56, 65], ["residual.Residual.__init__", "torch.nn.Linear", "torch.nn.Parameter", "torch.nn.Linear", "torch.randn"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__"], ["    ", "def", "__init__", "(", "self", ",", "*", "args", ",", "scaling_correction", "=", "False", ",", "elemwise", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "*", "args", ")", "\n", "self", ".", "scaling_correction", "=", "1.732", "if", "scaling_correction", "else", "1.0", "# TODO", "\n", "self", ".", "elemwise", "=", "elemwise", "\n", "self", ".", "Wx", "=", "nn", ".", "Linear", "(", "self", ".", "d_input", ",", "self", ".", "d_input", ")", "\n", "if", "self", ".", "elemwise", ":", "\n", "            ", "self", ".", "Wy", "=", "nn", ".", "Parameter", "(", "torch", ".", "randn", "(", "self", ".", "d_input", ")", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "Wy", "=", "nn", ".", "Linear", "(", "self", ".", "d_input", ",", "self", ".", "d_input", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.nn.residual.Highway.forward": [[66, 74], ["torch.sigmoid", "residual.Highway.Wy", "residual.Highway.Wx"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "x", ",", "y", ",", "transposed", "=", "False", ")", ":", "# TODO handle this case", "\n", "        ", "if", "self", ".", "elemwise", ":", "\n", "            ", "y", "=", "self", ".", "Wy", "*", "y", "\n", "", "else", ":", "\n", "            ", "y", "=", "self", ".", "Wy", "(", "y", ")", "\n", "", "r", "=", "torch", ".", "sigmoid", "(", "self", ".", "Wx", "(", "x", ")", "+", "y", ")", "\n", "z", "=", "self", ".", "scaling_correction", "*", "(", "1.", "-", "r", ")", "*", "x", "+", "r", "*", "y", "\n", "return", "z", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.nn.residual.DecayResidual.__init__": [[79, 84], ["residual.Residual.__init__"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__"], ["def", "__init__", "(", "self", ",", "*", "args", ",", "power", "=", "0.5", ",", "l2", "=", "True", ")", ":", "\n", "# print(\"DecayResidual extra kwargs\", kwargs)", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "*", "args", ")", "\n", "self", ".", "power", "=", "power", "\n", "self", ".", "l2", "=", "l2", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.nn.residual.DecayResidual.forward": [[85, 93], ["None"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ",", "y", ",", "transposed", ")", ":", "\n", "        ", "beta", "=", "self", ".", "i_layer", "**", "(", "-", "self", ".", "power", ")", "\n", "if", "self", ".", "l2", ":", "\n", "            ", "alpha", "=", "(", "1.", "-", "beta", "**", "2", ")", "**", "0.5", "\n", "", "else", ":", "\n", "            ", "alpha", "=", "1.", "-", "beta", "\n", "\n", "", "return", "alpha", "*", "x", "+", "beta", "*", "y", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.nn.initialization.init_weight": [[6, 17], ["torch.nn.init.uniform_", "torch.nn.init.normal_", "torch.nn.init.xavier_uniform_", "torch.nn.init.kaiming_normal_", "NotImplementedError"], "function", ["None"], ["def", "init_weight", "(", "weight", ",", "init_cfg", ")", ":", "\n", "    ", "if", "init_cfg", ".", "init", "==", "'uniform'", ":", "\n", "        ", "nn", ".", "init", ".", "uniform_", "(", "weight", ",", "-", "init_cfg", ".", "init_range", ",", "init_cfg", ".", "init_range", ")", "\n", "", "elif", "init_cfg", ".", "init", "==", "'normal'", ":", "\n", "        ", "nn", ".", "init", ".", "normal_", "(", "weight", ",", "0.0", ",", "init_cfg", ".", "init_std", ")", "\n", "", "elif", "init_cfg", ".", "init", "==", "'xavier'", ":", "\n", "        ", "nn", ".", "init", ".", "xavier_uniform_", "(", "weight", ")", "\n", "", "elif", "init_cfg", ".", "init", "==", "'kaiming'", ":", "\n", "        ", "nn", ".", "init", ".", "kaiming_normal_", "(", "weight", ",", "mode", "=", "'fan_in'", ",", "nonlinearity", "=", "'linear'", ")", "\n", "", "else", ":", "\n", "        ", "raise", "NotImplementedError", "(", "f\"initialization type {init_cfg.init} not supported\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.nn.initialization.init_bias": [[18, 24], ["hasattr", "torch.nn.init.constant_"], "function", ["None"], ["", "", "def", "init_bias", "(", "bias", ",", "init_cfg", ")", ":", "\n", "    ", "if", "hasattr", "(", "init_cfg", ",", "'zero_bias'", ")", "and", "init_cfg", ".", "zero_bias", "==", "False", ":", "\n", "# Keep the original bias init", "\n", "        ", "pass", "\n", "", "else", ":", "\n", "        ", "nn", ".", "init", ".", "constant_", "(", "bias", ",", "0.0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.nn.initialization.weights_init": [[25, 51], ["classname.find", "hasattr", "initialization.init_weight", "hasattr", "initialization.init_bias", "classname.find", "hasattr", "hasattr", "initialization.init_bias", "classname.find", "hasattr", "hasattr", "hasattr", "hasattr", "hasattr", "hasattr", "torch.nn.init.normal_", "initialization.init_weight", "initialization.init_weight", "initialization.init_weight", "initialization.init_bias", "initialization.init_bias"], "function", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.nn.initialization.init_weight", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.nn.initialization.init_bias", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.nn.initialization.init_bias", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.nn.initialization.init_weight", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.nn.initialization.init_weight", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.nn.initialization.init_weight", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.nn.initialization.init_bias", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.nn.initialization.init_bias"], ["", "", "def", "weights_init", "(", "m", ",", "init_cfg", ")", ":", "\n", "    ", "classname", "=", "m", ".", "__class__", ".", "__name__", "\n", "if", "classname", ".", "find", "(", "'Linear'", ")", "!=", "-", "1", ":", "\n", "        ", "if", "hasattr", "(", "m", ",", "'weight'", ")", "and", "m", ".", "weight", "is", "not", "None", ":", "\n", "            ", "init_weight", "(", "m", ".", "weight", ",", "init_cfg", ")", "\n", "", "if", "hasattr", "(", "m", ",", "'bias'", ")", "and", "m", ".", "bias", "is", "not", "None", ":", "\n", "            ", "init_bias", "(", "m", ".", "bias", ",", "init_cfg", ")", "\n", "", "", "elif", "classname", ".", "find", "(", "'LayerNorm'", ")", "!=", "-", "1", ":", "\n", "        ", "if", "hasattr", "(", "m", ",", "'weight'", ")", ":", "\n", "            ", "if", "hasattr", "(", "init_cfg", ",", "'ln'", ")", "and", "init_cfg", ".", "ln", "==", "False", ":", "\n", "                ", "pass", "\n", "", "else", ":", "\n", "                ", "nn", ".", "init", ".", "normal_", "(", "m", ".", "weight", ",", "1.0", ",", "init_cfg", ".", "init_std", ")", "\n", "", "", "if", "hasattr", "(", "m", ",", "'bias'", ")", "and", "m", ".", "bias", "is", "not", "None", ":", "\n", "            ", "init_bias", "(", "m", ".", "bias", ",", "init_cfg", ")", "\n", "", "", "elif", "classname", ".", "find", "(", "'TransformerLM'", ")", "!=", "-", "1", ":", "\n", "        ", "if", "hasattr", "(", "m", ",", "'r_emb'", ")", ":", "\n", "            ", "init_weight", "(", "m", ".", "r_emb", ",", "init_cfg", ")", "\n", "", "if", "hasattr", "(", "m", ",", "'r_w_bias'", ")", ":", "\n", "            ", "init_weight", "(", "m", ".", "r_w_bias", ",", "init_cfg", ")", "\n", "", "if", "hasattr", "(", "m", ",", "'r_r_bias'", ")", ":", "\n", "            ", "init_weight", "(", "m", ".", "r_r_bias", ",", "init_cfg", ")", "\n", "", "if", "hasattr", "(", "m", ",", "'r_bias'", ")", ":", "\n", "            ", "init_bias", "(", "m", ".", "r_bias", ",", "init_cfg", ")", "\n", "", "if", "hasattr", "(", "m", ",", "'initial_state'", ")", ":", "\n", "            ", "init_bias", "(", "m", ".", "initial_state", ",", "init_cfg", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.nn.initialization.weights_init_embedding": [[52, 75], ["classname.find", "hasattr", "range", "classname.find", "hasattr", "len", "initialization.init_weight", "classname.find", "hasattr", "hasattr", "torch.nn.init.normal_", "hasattr", "initialization.init_weight", "hasattr", "initialization.init_bias", "range", "range", "len", "len", "torch.nn.init.normal_", "initialization.init_weight"], "function", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.nn.initialization.init_weight", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.nn.initialization.init_weight", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.nn.initialization.init_bias", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.nn.initialization.init_weight"], ["", "", "", "def", "weights_init_embedding", "(", "m", ",", "init_cfg", ")", ":", "\n", "    ", "classname", "=", "m", ".", "__class__", ".", "__name__", "\n", "if", "classname", ".", "find", "(", "'AdaptiveEmbedding'", ")", "!=", "-", "1", ":", "\n", "        ", "if", "hasattr", "(", "m", ",", "'emb_projs'", ")", ":", "\n", "            ", "for", "i", "in", "range", "(", "len", "(", "m", ".", "emb_projs", ")", ")", ":", "\n", "                ", "if", "m", ".", "emb_projs", "[", "i", "]", "is", "not", "None", ":", "\n", "                    ", "nn", ".", "init", ".", "normal_", "(", "m", ".", "emb_projs", "[", "i", "]", ",", "0.0", ",", "init_cfg", ".", "proj_init_std", ")", "\n", "", "", "", "", "elif", "classname", ".", "find", "(", "'Embedding'", ")", "!=", "-", "1", ":", "\n", "        ", "if", "hasattr", "(", "m", ",", "'weight'", ")", ":", "\n", "            ", "init_weight", "(", "m", ".", "weight", ",", "init_cfg", ")", "\n", "", "", "elif", "classname", ".", "find", "(", "'ProjectedAdaptiveLogSoftmax'", ")", "!=", "-", "1", ":", "\n", "        ", "if", "hasattr", "(", "m", ",", "'cluster_weight'", ")", "and", "m", ".", "cluster_weight", "is", "not", "None", ":", "\n", "            ", "init_weight", "(", "m", ".", "cluster_weight", ",", "init_cfg", ")", "\n", "", "if", "hasattr", "(", "m", ",", "'cluster_bias'", ")", "and", "m", ".", "cluster_bias", "is", "not", "None", ":", "\n", "            ", "init_bias", "(", "m", ".", "cluster_bias", ",", "init_cfg", ")", "\n", "", "if", "hasattr", "(", "m", ",", "'out_projs'", ")", ":", "\n", "            ", "for", "i", "in", "range", "(", "len", "(", "m", ".", "out_projs", ")", ")", ":", "\n", "                ", "if", "m", ".", "out_projs", "[", "i", "]", "is", "not", "None", ":", "\n", "                    ", "nn", ".", "init", ".", "normal_", "(", "m", ".", "out_projs", "[", "i", "]", ",", "0.0", ",", "init_cfg", ".", "proj_init_std", ")", "\n", "", "", "", "if", "hasattr", "(", "m", ",", "'out_layers_weights'", ")", ":", "\n", "            ", "for", "i", "in", "range", "(", "len", "(", "m", ".", "out_layers_weights", ")", ")", ":", "\n", "                ", "if", "m", ".", "out_layers_weights", "[", "i", "]", "is", "not", "None", ":", "\n", "                    ", "init_weight", "(", "m", ".", "out_layers_weights", "[", "i", "]", ",", "init_cfg", ")", "\n", "", "", "", "", "", ""]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.nn.adaptive_softmax.OptionalParameterList.extra_repr": [[24, 35], ["adaptive_softmax.OptionalParameterList._parameters.items", "child_lines.append", "torch.typename", "torch.typename", "torch.typename", "torch.typename", "torch.typename", "torch.typename", "torch.typename", "torch.typename", "torch.typename", "str", "p.get_device", "p.size", "str"], "methods", ["None"], ["    ", "def", "extra_repr", "(", "self", ")", ":", "\n", "        ", "child_lines", "=", "[", "]", "\n", "for", "k", ",", "p", "in", "self", ".", "_parameters", ".", "items", "(", ")", ":", "\n", "            ", "if", "p", "is", "not", "None", ":", "\n", "                ", "size_str", "=", "'x'", ".", "join", "(", "str", "(", "size", ")", "for", "size", "in", "p", ".", "size", "(", ")", ")", "\n", "device_str", "=", "''", "if", "not", "p", ".", "is_cuda", "else", "' (GPU {})'", ".", "format", "(", "p", ".", "get_device", "(", ")", ")", "\n", "parastr", "=", "'Parameter containing: [{} of size {}{}]'", ".", "format", "(", "\n", "torch", ".", "typename", "(", "p", ")", ",", "size_str", ",", "device_str", ")", "\n", "child_lines", ".", "append", "(", "'  ('", "+", "str", "(", "k", ")", "+", "'): '", "+", "parastr", ")", "\n", "", "", "tmpstr", "=", "'\\n'", ".", "join", "(", "child_lines", ")", "\n", "return", "tmpstr", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.nn.adaptive_softmax.ProjectedAdaptiveLogSoftmax.__init__": [[38, 128], ["torch.Module.__init__", "torch.ParameterList", "torch.ParameterList", "torch.ParameterList", "adaptive_softmax.OptionalParameterList", "torch.Dropout", "torch.Dropout", "torch.Dropout", "list", "len", "isinstance", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.ParameterList", "torch.ParameterList", "torch.ParameterList", "adaptive_softmax.ProjectedAdaptiveLogSoftmax.out_layers_biases.append", "range", "torch.init.uniform_", "torch.init.uniform_", "torch.init.uniform_", "list", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "range", "adaptive_softmax.ProjectedAdaptiveLogSoftmax.out_projs.append", "torch.Parameter", "torch.Parameter", "torch.Parameter", "adaptive_softmax.ProjectedAdaptiveLogSoftmax.out_layers_weights.append", "len", "adaptive_softmax.ProjectedAdaptiveLogSoftmax.out_layers_biases.append", "len", "len", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.Parameter", "torch.Parameter", "torch.Parameter", "adaptive_softmax.ProjectedAdaptiveLogSoftmax.out_projs.append", "adaptive_softmax.ProjectedAdaptiveLogSoftmax.out_projs.append", "torch.Parameter", "torch.Parameter", "torch.Parameter", "adaptive_softmax.ProjectedAdaptiveLogSoftmax.out_layers_weights.append", "adaptive_softmax.ProjectedAdaptiveLogSoftmax.out_projs.append", "adaptive_softmax.ProjectedAdaptiveLogSoftmax.out_projs.append", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__"], ["    ", "def", "__init__", "(", "self", ",", "n_token", ",", "d_embed", ",", "d_proj", ",", "cutoffs", ",", "div_val", "=", "1", ",", "\n", "tie_projs", "=", "None", ",", "out_layers_weights", "=", "None", ",", "out_projs", "=", "None", ",", "\n", "keep_order", "=", "False", ",", "\n", "bias_scale", "=", "0.0", ",", "\n", "dropout", "=", "0.0", ",", "\n", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "n_token", "=", "n_token", "\n", "self", ".", "d_embed", "=", "d_embed", "\n", "self", ".", "d_proj", "=", "d_proj", "\n", "\n", "self", ".", "cutoffs", "=", "list", "(", "cutoffs", ")", "+", "[", "n_token", "]", "\n", "self", ".", "cutoff_ends", "=", "[", "0", "]", "+", "self", ".", "cutoffs", "\n", "self", ".", "div_val", "=", "div_val", "\n", "\n", "self", ".", "shortlist_size", "=", "self", ".", "cutoffs", "[", "0", "]", "\n", "self", ".", "n_clusters", "=", "len", "(", "self", ".", "cutoffs", ")", "-", "1", "\n", "self", ".", "head_size", "=", "self", ".", "shortlist_size", "+", "self", ".", "n_clusters", "\n", "\n", "# [21-09-15 AG]: bake the first False into the definition, just as [0] is built into the cutoffs", "\n", "if", "tie_projs", "is", "None", ":", "tie_projs", "=", "[", "]", "\n", "elif", "isinstance", "(", "tie_projs", ",", "bool", ")", ":", "tie_projs", "=", "[", "tie_projs", "]", "*", "len", "(", "cutoffs", ")", "\n", "else", ":", "tie_projs", "=", "list", "(", "tie_projs", ")", "\n", "tie_projs", "=", "[", "False", "]", "+", "tie_projs", "\n", "self", ".", "tie_projs", "=", "tie_projs", "\n", "\n", "if", "self", ".", "n_clusters", ">", "0", ":", "\n", "            ", "self", ".", "cluster_weight", "=", "nn", ".", "Parameter", "(", "torch", ".", "zeros", "(", "self", ".", "n_clusters", ",", "self", ".", "d_embed", ")", ")", "\n", "self", ".", "cluster_bias", "=", "nn", ".", "Parameter", "(", "torch", ".", "zeros", "(", "self", ".", "n_clusters", ")", ")", "\n", "\n", "", "if", "not", "out_layers_weights", ":", "\n", "            ", "self", ".", "out_layers_weights", "=", "nn", ".", "ParameterList", "(", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "out_layers_weights", "=", "out_layers_weights", "\n", "\n", "", "self", ".", "out_layers_biases", "=", "nn", ".", "ParameterList", "(", ")", "\n", "\n", "self", ".", "shared_out_projs", "=", "out_projs", "\n", "self", ".", "out_projs", "=", "OptionalParameterList", "(", ")", "\n", "\n", "self", ".", "dropout", "=", "dropout", "\n", "self", ".", "drop", "=", "nn", ".", "Dropout", "(", "dropout", ")", "\n", "\n", "if", "div_val", "==", "1", ":", "\n", "            ", "if", "d_proj", "!=", "d_embed", ":", "\n", "                ", "for", "i", "in", "range", "(", "len", "(", "self", ".", "cutoffs", ")", ")", ":", "\n", "                    ", "if", "tie_projs", "[", "i", "]", ":", "\n", "                        ", "self", ".", "out_projs", ".", "append", "(", "None", ")", "\n", "", "else", ":", "\n", "                        ", "self", ".", "out_projs", ".", "append", "(", "\n", "nn", ".", "Parameter", "(", "torch", ".", "zeros", "(", "d_proj", ",", "d_embed", ")", ")", "\n", ")", "\n", "", "", "", "else", ":", "\n", "# self.out_projs = [None] * len(self.cutoffs)", "\n", "                ", "self", ".", "out_projs", ".", "append", "(", "None", ")", "\n", "\n", "", "self", ".", "out_layers_biases", ".", "append", "(", "\n", "nn", ".", "Parameter", "(", "torch", ".", "zeros", "(", "n_token", ")", ")", "\n", ")", "\n", "\n", "if", "not", "out_layers_weights", ":", "\n", "                ", "self", ".", "out_layers_weights", ".", "append", "(", "\n", "nn", ".", "Parameter", "(", "torch", ".", "zeros", "(", "n_token", ",", "d_embed", ")", ")", "\n", ")", "\n", "", "", "else", ":", "\n", "            ", "for", "i", "in", "range", "(", "len", "(", "self", ".", "cutoffs", ")", ")", ":", "\n", "                ", "l_idx", ",", "r_idx", "=", "self", ".", "cutoff_ends", "[", "i", "]", ",", "self", ".", "cutoff_ends", "[", "i", "+", "1", "]", "\n", "d_emb_i", "=", "d_embed", "//", "(", "div_val", "**", "i", ")", "\n", "\n", "if", "tie_projs", "[", "i", "]", ":", "\n", "                    ", "self", ".", "out_projs", ".", "append", "(", "None", ")", "\n", "", "else", ":", "\n", "                    ", "self", ".", "out_projs", ".", "append", "(", "\n", "nn", ".", "Parameter", "(", "torch", ".", "zeros", "(", "d_proj", ",", "d_emb_i", ")", ")", "\n", ")", "\n", "\n", "", "self", ".", "out_layers_biases", ".", "append", "(", "\n", "nn", ".", "Parameter", "(", "torch", ".", "zeros", "(", "r_idx", "-", "l_idx", ")", ")", "\n", ")", "\n", "if", "not", "out_layers_weights", ":", "\n", "                    ", "self", ".", "out_layers_weights", ".", "append", "(", "\n", "nn", ".", "Parameter", "(", "torch", ".", "zeros", "(", "r_idx", "-", "l_idx", ",", "d_emb_i", ")", ")", "\n", ")", "\n", "", "", "", "for", "bias", "in", "self", ".", "out_layers_biases", ":", "\n", "            ", "bound", "=", "bias_scale", "*", "d_proj", "**", "-", ".5", "\n", "nn", ".", "init", ".", "uniform_", "(", "bias", ",", "-", "bound", ",", "bound", ")", "\n", "\n", "\n", "", "self", ".", "keep_order", "=", "keep_order", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.nn.adaptive_softmax.ProjectedAdaptiveLogSoftmax._compute_logit": [[129, 142], ["torch.linear", "torch.linear", "torch.linear", "adaptive_softmax.ProjectedAdaptiveLogSoftmax.drop", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "weight.t", "weight.t"], "methods", ["None"], ["", "def", "_compute_logit", "(", "self", ",", "hidden", ",", "weight", ",", "bias", ",", "proj", ")", ":", "\n", "        ", "if", "proj", "is", "None", ":", "\n", "            ", "logit", "=", "F", ".", "linear", "(", "hidden", ",", "weight", ",", "bias", "=", "bias", ")", "\n", "", "else", ":", "\n", "            ", "if", "self", ".", "dropout", ">", "0.0", ":", "\n", "                ", "logit", "=", "hidden", "@", "proj", "\n", "logit", "=", "self", ".", "drop", "(", "logit", ")", "\n", "logit", "=", "logit", "@", "weight", ".", "t", "(", ")", "\n", "", "else", ":", "\n", "                ", "logit", "=", "torch", ".", "einsum", "(", "'bd,de,ev->bv'", ",", "(", "hidden", ",", "proj", ",", "weight", ".", "t", "(", ")", ")", ")", "\n", "", "if", "bias", "is", "not", "None", ":", "\n", "                ", "logit", "=", "logit", "+", "bias", "\n", "", "", "return", "logit", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.nn.adaptive_softmax.ProjectedAdaptiveLogSoftmax.get_out_proj": [[143, 153], ["len", "len"], "methods", ["None"], ["", "def", "get_out_proj", "(", "self", ",", "i", ")", ":", "\n", "        ", "if", "self", ".", "tie_projs", "[", "i", "]", ":", "\n", "            ", "if", "len", "(", "self", ".", "shared_out_projs", ")", "==", "0", ":", "\n", "                ", "return", "None", "\n", "", "elif", "len", "(", "self", ".", "shared_out_projs", ")", "==", "1", ":", "\n", "                ", "return", "self", ".", "shared_out_projs", "[", "0", "]", "\n", "", "else", ":", "\n", "                ", "return", "self", ".", "shared_out_projs", "[", "i", "]", "\n", "", "", "else", ":", "\n", "            ", "return", "self", ".", "out_projs", "[", "i", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.nn.adaptive_softmax.ProjectedAdaptiveLogSoftmax.forward": [[154, 236], ["hidden.reshape.reshape.reshape", "target.reshape.reshape.reshape", "torch.zeros_like.mean", "torch.zeros_like.mean", "torch.zeros_like.mean", "hidden.reshape.reshape.size", "hidden.reshape.reshape.size", "target.reshape.reshape.size", "print", "RuntimeError", "adaptive_softmax.ProjectedAdaptiveLogSoftmax._compute_logit", "range", "adaptive_softmax.ProjectedAdaptiveLogSoftmax._compute_logit", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "range", "adaptive_softmax.ProjectedAdaptiveLogSoftmax.get_out_proj", "torch.log_softmax().gather().squeeze", "torch.log_softmax().gather().squeeze", "torch.log_softmax().gather().squeeze", "len", "weights.append", "biases.append", "adaptive_softmax.ProjectedAdaptiveLogSoftmax.get_out_proj", "mask_i.nonzero().squeeze", "torch.log_softmax.index_select", "head_logprob.index_select.gather().squeeze.size", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "len", "mask_i.nonzero().squeeze.numel", "target.reshape.reshape.index_select", "F.log_softmax.index_select.gather().squeeze", "hidden.reshape.reshape.index_select", "adaptive_softmax.ProjectedAdaptiveLogSoftmax._compute_logit", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.zeros_like.index_copy_", "torch.zeros_like.index_copy_", "torch.zeros_like.index_copy_", "nll[].copy_", "torch.log_softmax().gather", "torch.log_softmax().gather", "torch.log_softmax().gather", "mask_i.nonzero", "adaptive_softmax.ProjectedAdaptiveLogSoftmax.get_out_proj", "torch.log_softmax.gather().squeeze", "target.reshape.reshape.unsqueeze", "F.log_softmax.index_select.gather", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax.gather", "head_logprob.index_select.gather().squeeze.size"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.nn.adaptive_softmax.ProjectedAdaptiveLogSoftmax._compute_logit", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.nn.adaptive_softmax.ProjectedAdaptiveLogSoftmax._compute_logit", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.nn.adaptive_softmax.ProjectedAdaptiveLogSoftmax.get_out_proj", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.nn.adaptive_softmax.ProjectedAdaptiveLogSoftmax.get_out_proj", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.nn.adaptive_softmax.ProjectedAdaptiveLogSoftmax._compute_logit", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.nn.adaptive_softmax.ProjectedAdaptiveLogSoftmax.get_out_proj"], ["", "", "def", "forward", "(", "self", ",", "hidden", ",", "target", ",", "keep_order", "=", "False", ",", "key_padding_mask", "=", "None", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "# [21-09-15 AG]: TODO may need to handle key_padding_mask", "\n", "        ", "'''\n            hidden :: [len*bsz x d_proj]\n            target :: [len*bsz]\n        '''", "\n", "\n", "hidden", "=", "hidden", ".", "reshape", "(", "-", "1", ",", "hidden", ".", "size", "(", "-", "1", ")", ")", "\n", "target", "=", "target", ".", "reshape", "(", "-", "1", ")", "\n", "if", "hidden", ".", "size", "(", "0", ")", "!=", "target", ".", "size", "(", "0", ")", ":", "\n", "            ", "print", "(", "hidden", ".", "shape", ",", "target", ".", "shape", ")", "\n", "raise", "RuntimeError", "(", "'Input and target should have the same size '", "\n", "'in the batch dimension.'", ")", "\n", "\n", "", "if", "self", ".", "n_clusters", "==", "0", ":", "\n", "            ", "logit", "=", "self", ".", "_compute_logit", "(", "hidden", ",", "self", ".", "out_layers_weights", "[", "0", "]", ",", "\n", "self", ".", "out_layers_biases", "[", "0", "]", ",", "self", ".", "get_out_proj", "(", "0", ")", ")", "\n", "nll", "=", "-", "F", ".", "log_softmax", "(", "logit", ",", "dim", "=", "-", "1", ")", ".", "gather", "(", "1", ",", "target", ".", "unsqueeze", "(", "1", ")", ")", ".", "squeeze", "(", "1", ")", "\n", "", "else", ":", "\n", "# construct weights and biases", "\n", "            ", "weights", ",", "biases", "=", "[", "]", ",", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "self", ".", "cutoffs", ")", ")", ":", "\n", "                ", "if", "self", ".", "div_val", "==", "1", ":", "\n", "                    ", "l_idx", ",", "r_idx", "=", "self", ".", "cutoff_ends", "[", "i", "]", ",", "self", ".", "cutoff_ends", "[", "i", "+", "1", "]", "\n", "weight_i", "=", "self", ".", "out_layers_weights", "[", "0", "]", "[", "l_idx", ":", "r_idx", "]", "\n", "bias_i", "=", "self", ".", "out_layers_biases", "[", "0", "]", "[", "l_idx", ":", "r_idx", "]", "\n", "", "else", ":", "\n", "                    ", "weight_i", "=", "self", ".", "out_layers_weights", "[", "i", "]", "\n", "bias_i", "=", "self", ".", "out_layers_biases", "[", "i", "]", "\n", "\n", "", "if", "i", "==", "0", ":", "\n", "                    ", "weight_i", "=", "torch", ".", "cat", "(", "\n", "[", "weight_i", ",", "self", ".", "cluster_weight", "]", ",", "dim", "=", "0", ")", "\n", "bias_i", "=", "torch", ".", "cat", "(", "\n", "[", "bias_i", ",", "self", ".", "cluster_bias", "]", ",", "dim", "=", "0", ")", "\n", "\n", "", "weights", ".", "append", "(", "weight_i", ")", "\n", "biases", ".", "append", "(", "bias_i", ")", "\n", "\n", "", "head_weight", ",", "head_bias", ",", "head_proj", "=", "weights", "[", "0", "]", ",", "biases", "[", "0", "]", ",", "self", ".", "get_out_proj", "(", "0", ")", "\n", "\n", "head_logit", "=", "self", ".", "_compute_logit", "(", "hidden", ",", "head_weight", ",", "head_bias", ",", "head_proj", ")", "\n", "head_logprob", "=", "F", ".", "log_softmax", "(", "head_logit", ",", "dim", "=", "1", ")", "\n", "\n", "nll", "=", "torch", ".", "zeros_like", "(", "target", ",", "dtype", "=", "hidden", ".", "dtype", ",", "device", "=", "hidden", ".", "device", ")", "\n", "\n", "offset", "=", "0", "\n", "cutoff_values", "=", "[", "0", "]", "+", "self", ".", "cutoffs", "\n", "for", "i", "in", "range", "(", "len", "(", "cutoff_values", ")", "-", "1", ")", ":", "\n", "                ", "l_idx", ",", "r_idx", "=", "cutoff_values", "[", "i", "]", ",", "cutoff_values", "[", "i", "+", "1", "]", "\n", "\n", "mask_i", "=", "(", "target", ">=", "l_idx", ")", "&", "(", "target", "<", "r_idx", ")", "\n", "indices_i", "=", "mask_i", ".", "nonzero", "(", "as_tuple", "=", "False", ")", ".", "squeeze", "(", ")", "\n", "\n", "if", "indices_i", ".", "numel", "(", ")", "==", "0", ":", "\n", "                    ", "continue", "\n", "\n", "", "target_i", "=", "target", ".", "index_select", "(", "0", ",", "indices_i", ")", "-", "l_idx", "\n", "head_logprob_i", "=", "head_logprob", ".", "index_select", "(", "0", ",", "indices_i", ")", "\n", "\n", "if", "i", "==", "0", ":", "\n", "                    ", "logprob_i", "=", "head_logprob_i", ".", "gather", "(", "1", ",", "target_i", "[", ":", ",", "None", "]", ")", ".", "squeeze", "(", "1", ")", "\n", "", "else", ":", "\n", "                    ", "weight_i", ",", "bias_i", ",", "proj_i", "=", "weights", "[", "i", "]", ",", "biases", "[", "i", "]", ",", "self", ".", "get_out_proj", "(", "i", ")", "\n", "\n", "hidden_i", "=", "hidden", ".", "index_select", "(", "0", ",", "indices_i", ")", "\n", "\n", "tail_logit_i", "=", "self", ".", "_compute_logit", "(", "hidden_i", ",", "weight_i", ",", "bias_i", ",", "proj_i", ")", "\n", "tail_logprob_i", "=", "F", ".", "log_softmax", "(", "tail_logit_i", ",", "dim", "=", "1", ")", "\n", "\n", "logprob_i", "=", "head_logprob_i", "[", ":", ",", "-", "i", "]", "+", "tail_logprob_i", ".", "gather", "(", "1", ",", "target_i", "[", ":", ",", "None", "]", ")", ".", "squeeze", "(", "1", ")", "\n", "\n", "", "if", "self", ".", "keep_order", "or", "keep_order", ":", "\n", "                    ", "nll", ".", "index_copy_", "(", "0", ",", "indices_i", ",", "-", "logprob_i", ")", "\n", "", "else", ":", "\n", "                    ", "nll", "[", "offset", ":", "offset", "+", "logprob_i", ".", "size", "(", "0", ")", "]", ".", "copy_", "(", "-", "logprob_i", ")", "\n", "\n", "", "offset", "+=", "logprob_i", ".", "size", "(", "0", ")", "\n", "\n", "", "", "return", "nll", ".", "mean", "(", ")", "# TODO maybe cases for length or padding_mask", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.nn.adaptive_softmax.AdaptiveEmbedding.__init__": [[243, 279], ["torch.Module.__init__", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ParameterList", "torch.ParameterList", "torch.ParameterList", "list", "torch.Dropout", "torch.Dropout", "torch.Dropout", "torch.Identity", "torch.Identity", "torch.Identity", "adaptive_softmax.AdaptiveEmbedding.emb_layers.append", "_init_embed", "range", "torch.Embedding", "torch.Embedding", "torch.Embedding", "adaptive_softmax.AdaptiveEmbedding.emb_projs.append", "_init_proj", "len", "adaptive_softmax.AdaptiveEmbedding.emb_layers.append", "_init_embed", "adaptive_softmax.AdaptiveEmbedding.emb_projs.append", "_init_proj", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__"], ["def", "__init__", "(", "self", ",", "n_token", ",", "d_embed", ",", "d_proj", ",", "cutoffs", ":", "List", "[", "int", "]", ",", "div_val", "=", "1", ",", "init_scale", "=", "1.0", ",", "sample_softmax", "=", "False", ",", "dropout", "=", "0.0", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "n_token", "=", "n_token", "\n", "self", ".", "d_embed", "=", "d_embed", "\n", "\n", "self", ".", "cutoffs", "=", "list", "(", "cutoffs", ")", "+", "[", "n_token", "]", "\n", "self", ".", "div_val", "=", "div_val", "\n", "self", ".", "d_proj", "=", "d_proj", "\n", "self", ".", "drop", "=", "nn", ".", "Dropout", "(", "dropout", ")", "if", "dropout", ">", "0.0", "else", "nn", ".", "Identity", "(", ")", "\n", "\n", "self", ".", "emb_scale", "=", "d_proj", "**", "0.5", "\n", "\n", "self", ".", "cutoff_ends", "=", "[", "0", "]", "+", "self", ".", "cutoffs", "\n", "\n", "self", ".", "emb_layers", "=", "nn", ".", "ModuleList", "(", ")", "\n", "self", ".", "emb_projs", "=", "nn", ".", "ParameterList", "(", ")", "\n", "if", "div_val", "==", "1", ":", "\n", "            ", "self", ".", "emb_layers", ".", "append", "(", "nn", ".", "Embedding", "(", "n_token", ",", "d_embed", ",", "sparse", "=", "sample_softmax", ">", "0", ")", ")", "\n", "_init_embed", "(", "self", ".", "emb_layers", "[", "-", "1", "]", ".", "weight", ",", "d_embed", ",", "init_scale", ")", "\n", "# torch.nn.init.normal_(self.emb_layers[-1].weight, mean=0, std=init_scale * d_embed ** -.5)", "\n", "if", "d_proj", "!=", "d_embed", ":", "# TODO", "\n", "# self.emb_projs.append(nn.Parameter(torch.FloatTensor(d_proj, d_embed)))", "\n", "                ", "self", ".", "emb_projs", ".", "append", "(", "nn", ".", "Parameter", "(", "torch", ".", "FloatTensor", "(", "d_proj", ",", "d_embed", ")", ")", ")", "\n", "# torch.nn.init.normal_(self.emb_projs[-1], mean=0, std=init_scale * 1./self.emb_scale)", "\n", "_init_proj", "(", "self", ".", "emb_projs", "[", "-", "1", "]", ",", "d_proj", ",", "init_scale", ")", "\n", "", "", "else", ":", "\n", "            ", "for", "i", "in", "range", "(", "len", "(", "self", ".", "cutoffs", ")", ")", ":", "\n", "                ", "l_idx", ",", "r_idx", "=", "self", ".", "cutoff_ends", "[", "i", "]", ",", "self", ".", "cutoff_ends", "[", "i", "+", "1", "]", "\n", "d_emb_i", "=", "d_embed", "//", "(", "div_val", "**", "i", ")", "\n", "self", ".", "emb_layers", ".", "append", "(", "nn", ".", "Embedding", "(", "r_idx", "-", "l_idx", ",", "d_emb_i", ")", ")", "\n", "# torch.nn.init.normal_(self.emb_layers[-1].weight, mean=0, std=init_scale * d_emb_i ** -.5)", "\n", "_init_embed", "(", "self", ".", "emb_layers", "[", "-", "1", "]", ".", "weight", ",", "d_emb_i", ",", "init_scale", ")", "\n", "self", ".", "emb_projs", ".", "append", "(", "nn", ".", "Parameter", "(", "torch", ".", "FloatTensor", "(", "d_proj", ",", "d_emb_i", ")", ")", ")", "\n", "# torch.nn.init.normal_(self.emb_projs[-1], mean=0, std=init_scale * 1./self.emb_scale)", "\n", "_init_proj", "(", "self", ".", "emb_projs", "[", "-", "1", "]", ",", "d_proj", ",", "init_scale", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.nn.adaptive_softmax.AdaptiveEmbedding.forward": [[280, 331], ["torch.linear.mul_", "adaptive_softmax.AdaptiveEmbedding.drop", "next", "inp.view", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "range", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "emb_flat.view", "torch.linear", "torch.linear", "torch.linear", "adaptive_softmax.AdaptiveEmbedding.parameters", "len", "mask_i.nonzero().squeeze", "mask_i.nonzero().squeeze.numel", "adaptive_softmax.AdaptiveEmbedding.drop", "torch.linear", "torch.linear", "torch.linear", "torch.cat.append", "torch.cat.append", "torch.cat.append", "torch.zeros_like.index_put_", "torch.zeros_like.index_put_", "torch.zeros_like.index_put_", "inp.size", "inp.view.index_select", "mask_i.nonzero", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange"], "methods", ["None"], ["", "", "", "def", "forward", "(", "self", ",", "inp", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "if", "self", ".", "div_val", "==", "1", ":", "\n", "            ", "embed", "=", "self", ".", "emb_layers", "[", "0", "]", "(", "inp", ")", "\n", "embed", "=", "self", ".", "drop", "(", "embed", ")", "\n", "if", "self", ".", "d_proj", "!=", "self", ".", "d_embed", ":", "\n", "                ", "embed", "=", "F", ".", "linear", "(", "embed", ",", "self", ".", "emb_projs", "[", "0", "]", ")", "\n", "", "", "else", ":", "\n", "            ", "param", "=", "next", "(", "self", ".", "parameters", "(", ")", ")", "\n", "inp_flat", "=", "inp", ".", "view", "(", "-", "1", ")", "\n", "\n", "# Changes", "\n", "# emb_flat = torch.zeros([inp_flat.size(0), self.d_proj], dtype=param.dtype, device=param.device)", "\n", "embeddings", "=", "[", "]", "\n", "indices", "=", "torch", ".", "zeros_like", "(", "inp_flat", ")", "# empty should work as long as cutoffs[-1] > max token", "\n", "_total_tokens", "=", "0", "\n", "\n", "# emb_flat = inp.new_zeros(inp_flat.size(0), self.d_proj)", "\n", "for", "i", "in", "range", "(", "len", "(", "self", ".", "cutoffs", ")", ")", ":", "\n", "                ", "l_idx", ",", "r_idx", "=", "self", ".", "cutoff_ends", "[", "i", "]", ",", "self", ".", "cutoff_ends", "[", "i", "+", "1", "]", "\n", "\n", "mask_i", "=", "(", "inp_flat", ">=", "l_idx", ")", "&", "(", "inp_flat", "<", "r_idx", ")", "\n", "indices_i", "=", "mask_i", ".", "nonzero", "(", ")", ".", "squeeze", "(", "-", "1", ")", "# shape (_tokens,)", "\n", "\n", "_tokens", "=", "indices_i", ".", "numel", "(", ")", "\n", "if", "_tokens", "==", "0", ":", "\n", "                    ", "continue", "\n", "\n", "", "inp_i", "=", "inp_flat", ".", "index_select", "(", "0", ",", "indices_i", ")", "-", "l_idx", "\n", "emb_i", "=", "self", ".", "emb_layers", "[", "i", "]", "(", "inp_i", ")", "\n", "emb_i", "=", "self", ".", "drop", "(", "emb_i", ")", "\n", "emb_i", "=", "F", ".", "linear", "(", "emb_i", ",", "self", ".", "emb_projs", "[", "i", "]", ")", "\n", "\n", "# Changes", "\n", "embeddings", ".", "append", "(", "emb_i", ")", "\n", "indices", ".", "index_put_", "(", "\n", "(", "indices_i", ",", ")", ",", "\n", "torch", ".", "arange", "(", "_tokens", ",", "device", "=", "inp", ".", "device", ")", "+", "_total_tokens", "\n", ")", "\n", "_total_tokens", "+=", "_tokens", "\n", "\n", "# emb_flat.index_copy_(0, indices_i, emb_i)", "\n", "", "embeddings", "=", "torch", ".", "cat", "(", "embeddings", ",", "dim", "=", "0", ")", "\n", "emb_flat", "=", "embeddings", "[", "indices", "]", "\n", "\n", "embed_shape", "=", "inp", ".", "size", "(", ")", "+", "(", "self", ".", "d_proj", ",", ")", "\n", "embed", "=", "emb_flat", ".", "view", "(", "embed_shape", ")", "\n", "\n", "", "embed", ".", "mul_", "(", "self", ".", "emb_scale", ")", "\n", "# embed.div_(self.emb_scale)", "\n", "\n", "return", "embed", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.nn.adaptive_softmax._init_weight": [[333, 340], ["torch.init.normal_"], "function", ["None"], ["", "", "def", "_init_weight", "(", "weight", ",", "d", ":", "int", ",", "init_scale", ":", "Optional", "[", "float", "]", ",", "default", "=", "None", ")", ":", "\n", "    ", "assert", "init_scale", "or", "default", "\n", "if", "init_scale", "is", "None", ":", "\n", "        ", "std", "=", "default", "\n", "", "else", ":", "\n", "        ", "std", "=", "init_scale", "*", "(", "d", "**", "-", "0.5", ")", "\n", "", "nn", ".", "init", ".", "normal_", "(", "weight", ",", "mean", "=", "0", ",", "std", "=", "std", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.nn.dxt.DCT.__init__": [[12, 26], ["torch.Module.__init__", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "dxt.DCT.register_buffer", "numpy.exp", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "dxt.DCT.register_buffer", "scipy.fft.dct", "numpy.eye", "numpy.arange"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__"], ["def", "__init__", "(", "self", ",", "N", ",", "norm", "=", "'backward'", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "N", "=", "N", "\n", "\n", "# Materialize DCT matrix", "\n", "P", "=", "scipy", ".", "fft", ".", "dct", "(", "np", ".", "eye", "(", "N", ")", ",", "norm", "=", "norm", ",", "type", "=", "2", ")", ".", "T", "\n", "P", "=", "torch", ".", "tensor", "(", "P", ",", "dtype", "=", "torch", ".", "float", ")", "\n", "self", ".", "register_buffer", "(", "'P'", ",", "P", ")", "\n", "\n", "# TODO take care of normalization", "\n", "Q", "=", "np", ".", "exp", "(", "-", "1j", "*", "np", ".", "pi", "/", "(", "2", "*", "self", ".", "N", ")", "*", "np", ".", "arange", "(", "self", ".", "N", ")", ")", "\n", "Q", "=", "torch", ".", "tensor", "(", "Q", ",", "dtype", "=", "torch", ".", "cfloat", ")", "\n", "self", ".", "register_buffer", "(", "'Q'", ",", "Q", ")", "# half shift", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.nn.dxt.DCT.forward": [[27, 36], ["dxt.DCT.forward_dense", "dxt.DCT.forward_n", "dxt.DCT.forward_2n", "dxt.DCT.forward_4n"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.nn.dxt.IDCT.forward_dense", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.nn.dxt.IDCT.forward_n", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.nn.dxt.IDCT.forward_2n", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.nn.dxt.IDCT.forward_4n"], ["", "def", "forward", "(", "self", ",", "x", ",", "mode", "=", "2", ")", ":", "\n", "        ", "if", "mode", "==", "0", ":", "\n", "            ", "return", "self", ".", "forward_dense", "(", "x", ")", "\n", "", "elif", "mode", "==", "1", ":", "\n", "            ", "return", "self", ".", "forward_n", "(", "x", ")", "\n", "", "elif", "mode", "==", "2", ":", "\n", "            ", "return", "self", ".", "forward_2n", "(", "x", ")", "\n", "", "elif", "mode", "==", "4", ":", "\n", "            ", "return", "self", ".", "forward_4n", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.nn.dxt.DCT.forward_dense": [[37, 41], ["dxt.DCT.P.to", "x.unsqueeze"], "methods", ["None"], ["", "", "def", "forward_dense", "(", "self", ",", "x", ")", ":", "\n", "        ", "\"\"\" Baseline DCT type II - matmul by DCT matrix \"\"\"", "\n", "y", "=", "(", "self", ".", "P", ".", "to", "(", "x", ")", "@", "x", ".", "unsqueeze", "(", "-", "1", ")", ")", ".", "squeeze", "(", "-", "1", ")", "\n", "return", "y", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.nn.dxt.DCT.forward_4n": [[42, 55], ["torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "x.view.view.view", "torch.fft.fft", "torch.fft.fft", "torch.fft.fft", "torch.fft.fft", "torch.is_complex", "torch.is_complex", "torch.is_complex", "torch.is_complex", "torch.real", "torch.real", "torch.real", "torch.real", "x.view.view.flip"], "methods", ["None"], ["", "def", "forward_4n", "(", "self", ",", "x", ")", ":", "\n", "        ", "\"\"\" DCT type II - reduction to FFT size 4N \"\"\"", "\n", "assert", "self", ".", "N", "==", "x", ".", "shape", "[", "-", "1", "]", "\n", "x", "=", "torch", ".", "cat", "(", "[", "x", ",", "x", ".", "flip", "(", "-", "1", ")", "]", ",", "dim", "=", "-", "1", ")", "\n", "z", "=", "torch", ".", "zeros_like", "(", "x", ")", "\n", "x", "=", "torch", ".", "stack", "(", "[", "z", ",", "x", "]", ",", "dim", "=", "-", "1", ")", "\n", "x", "=", "x", ".", "view", "(", "x", ".", "shape", "[", ":", "-", "2", "]", "+", "(", "-", "1", ",", ")", ")", "\n", "y", "=", "torch", ".", "fft", ".", "fft", "(", "x", ")", "\n", "y", "=", "y", "[", "...", ",", ":", "self", ".", "N", "]", "\n", "if", "torch", ".", "is_complex", "(", "x", ")", ":", "\n", "            ", "return", "y", "\n", "", "else", ":", "\n", "            ", "return", "torch", ".", "real", "(", "y", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.nn.dxt.DCT.forward_2n": [[56, 71], ["torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.is_complex", "torch.is_complex", "torch.is_complex", "torch.is_complex", "torch.fft.fft", "torch.fft.fft", "torch.fft.fft", "torch.fft.fft", "torch.real", "torch.real", "torch.real", "torch.real", "torch.cat.flip", "torch.cat.flip"], "methods", ["None"], ["", "", "def", "forward_2n", "(", "self", ",", "x", ")", ":", "\n", "        ", "\"\"\" DCT type II - reduction to FFT size 2N mirrored\n\n        The reduction from the DSP forum is not quite correct in the complex input case.\n        halfshift(FFT[a, b, c, d, d, c, b, a]) -> [A, B, C, D, 0, -D, -C, -B]\n        In the case of real input, the intermediate step after FFT has form [A, B, C, D, 0, D*, C*, B*]\n        \"\"\"", "\n", "assert", "self", ".", "N", "==", "x", ".", "shape", "[", "-", "1", "]", "\n", "x", "=", "torch", ".", "cat", "(", "[", "x", ",", "x", ".", "flip", "(", "-", "1", ")", "]", ",", "dim", "=", "-", "1", ")", "\n", "y", "=", "torch", ".", "fft", ".", "fft", "(", "x", ")", "[", "...", ",", ":", "self", ".", "N", "]", "\n", "y", "=", "y", "*", "self", ".", "Q", "\n", "if", "torch", ".", "is_complex", "(", "x", ")", ":", "\n", "            ", "return", "y", "\n", "", "else", ":", "\n", "            ", "return", "torch", ".", "real", "(", "y", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.nn.dxt.DCT.forward_n": [[72, 83], ["torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.fft.fft", "torch.fft.fft", "torch.fft.fft", "torch.fft.fft", "torch.is_complex", "torch.is_complex", "torch.is_complex", "torch.is_complex", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.real", "torch.real", "torch.real", "torch.real", "x[].flip", "y[].flip"], "methods", ["None"], ["", "", "def", "forward_n", "(", "self", ",", "x", ")", ":", "\n", "        ", "\"\"\" DCT type II - reduction to size N \"\"\"", "\n", "assert", "self", ".", "N", "==", "x", ".", "shape", "[", "-", "1", "]", "\n", "x", "=", "torch", ".", "cat", "(", "[", "x", "[", "...", ",", "0", ":", ":", "2", "]", ",", "x", "[", "...", ",", "1", ":", ":", "2", "]", ".", "flip", "(", "-", "1", ")", "]", ",", "dim", "=", "-", "1", ")", "\n", "y", "=", "torch", ".", "fft", ".", "fft", "(", "x", ")", "\n", "y", "=", "y", "*", "2", "*", "self", ".", "Q", "\n", "if", "torch", ".", "is_complex", "(", "x", ")", ":", "\n", "            ", "y", "=", "torch", ".", "cat", "(", "[", "y", "[", "...", ",", ":", "1", "]", ",", "(", "y", "[", "...", ",", "1", ":", "]", "+", "1j", "*", "y", "[", "...", ",", "1", ":", "]", ".", "flip", "(", "-", "1", ")", ")", "/", "2", "]", ",", "dim", "=", "-", "1", ")", "# TODO in-place sum", "\n", "", "else", ":", "\n", "            ", "y", "=", "torch", ".", "real", "(", "y", ")", "\n", "", "return", "y", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.nn.dxt.IDCT.__init__": [[85, 99], ["torch.Module.__init__", "numpy.linalg.inv", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "dxt.IDCT.register_buffer", "numpy.exp", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "dxt.IDCT.register_buffer", "scipy.fft.dct", "numpy.arange", "numpy.eye"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__"], ["    ", "def", "__init__", "(", "self", ",", "N", ",", "norm", "=", "'backward'", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "N", "=", "N", "\n", "\n", "# Materialize DCT matrix", "\n", "P", "=", "np", ".", "linalg", ".", "inv", "(", "scipy", ".", "fft", ".", "dct", "(", "np", ".", "eye", "(", "N", ")", ",", "norm", "=", "norm", ",", "type", "=", "2", ")", ".", "T", ")", "\n", "P", "=", "torch", ".", "tensor", "(", "P", ",", "dtype", "=", "torch", ".", "float", ")", "\n", "self", ".", "register_buffer", "(", "'P'", ",", "P", ")", "\n", "\n", "# TODO take care of normalization", "\n", "Q", "=", "np", ".", "exp", "(", "-", "1j", "*", "np", ".", "pi", "/", "(", "2", "*", "self", ".", "N", ")", "*", "np", ".", "arange", "(", "2", "*", "self", ".", "N", ")", ")", "\n", "Q", "=", "torch", ".", "tensor", "(", "Q", ",", "dtype", "=", "torch", ".", "cfloat", ")", "\n", "self", ".", "register_buffer", "(", "'Q'", ",", "Q", ")", "# half shift", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.nn.dxt.IDCT.forward": [[100, 109], ["dxt.IDCT.forward_dense", "dxt.IDCT.forward_n", "dxt.IDCT.forward_2n", "dxt.IDCT.forward_4n"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.nn.dxt.IDCT.forward_dense", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.nn.dxt.IDCT.forward_n", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.nn.dxt.IDCT.forward_2n", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.nn.dxt.IDCT.forward_4n"], ["", "def", "forward", "(", "self", ",", "x", ",", "mode", "=", "2", ")", ":", "\n", "        ", "if", "mode", "==", "0", ":", "\n", "            ", "return", "self", ".", "forward_dense", "(", "x", ")", "\n", "", "elif", "mode", "==", "1", ":", "\n", "            ", "return", "self", ".", "forward_n", "(", "x", ")", "\n", "", "elif", "mode", "==", "2", ":", "\n", "            ", "return", "self", ".", "forward_2n", "(", "x", ")", "\n", "", "elif", "mode", "==", "4", ":", "\n", "            ", "return", "self", ".", "forward_4n", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.nn.dxt.IDCT.forward_dense": [[110, 114], ["dxt.IDCT.P.to", "x.unsqueeze"], "methods", ["None"], ["", "", "def", "forward_dense", "(", "self", ",", "x", ")", ":", "\n", "        ", "\"\"\" Baseline DCT type II - matmul by DCT matrix \"\"\"", "\n", "y", "=", "(", "self", ".", "P", ".", "to", "(", "x", ")", "@", "x", ".", "unsqueeze", "(", "-", "1", ")", ")", ".", "squeeze", "(", "-", "1", ")", "\n", "return", "y", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.nn.dxt.IDCT.forward_4n": [[115, 126], ["torch.cat.new_zeros", "torch.cat.new_zeros", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.fft.ifft", "torch.fft.ifft", "torch.fft.ifft", "torch.fft.ifft", "torch.is_complex", "torch.is_complex", "torch.is_complex", "torch.is_complex", "torch.real", "torch.real", "torch.real", "torch.real", "x[].flip", "torch.cat.flip", "torch.cat.flip"], "methods", ["None"], ["", "def", "forward_4n", "(", "self", ",", "x", ")", ":", "\n", "        ", "\"\"\" DCT type II - reduction to FFT size 4N \"\"\"", "\n", "assert", "self", ".", "N", "==", "x", ".", "shape", "[", "-", "1", "]", "\n", "z", "=", "x", ".", "new_zeros", "(", "x", ".", "shape", "[", ":", "-", "1", "]", "+", "(", "1", ",", ")", ")", "\n", "x", "=", "torch", ".", "cat", "(", "[", "x", ",", "z", ",", "-", "x", ".", "flip", "(", "-", "1", ")", ",", "-", "x", "[", "...", ",", "1", ":", "]", ",", "z", ",", "x", "[", "...", ",", "1", ":", "]", ".", "flip", "(", "-", "1", ")", "]", ",", "dim", "=", "-", "1", ")", "\n", "y", "=", "torch", ".", "fft", ".", "ifft", "(", "x", ")", "\n", "y", "=", "y", "[", "...", ",", "1", ":", "2", "*", "self", ".", "N", ":", "2", "]", "\n", "if", "torch", ".", "is_complex", "(", "x", ")", ":", "\n", "            ", "return", "y", "\n", "", "else", ":", "\n", "            ", "return", "torch", ".", "real", "(", "y", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.nn.dxt.IDCT.forward_2n": [[127, 138], ["torch.cat.new_zeros", "torch.cat.new_zeros", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.is_complex", "torch.is_complex", "torch.is_complex", "torch.is_complex", "torch.fft.ifft", "torch.fft.ifft", "torch.fft.ifft", "torch.fft.ifft", "torch.real", "torch.real", "torch.real", "torch.real", "x[].flip"], "methods", ["None"], ["", "", "def", "forward_2n", "(", "self", ",", "x", ")", ":", "\n", "        ", "\"\"\" DCT type II - reduction to FFT size 2N mirrored \"\"\"", "\n", "assert", "self", ".", "N", "==", "x", ".", "shape", "[", "-", "1", "]", "\n", "z", "=", "x", ".", "new_zeros", "(", "x", ".", "shape", "[", ":", "-", "1", "]", "+", "(", "1", ",", ")", ")", "\n", "x", "=", "torch", ".", "cat", "(", "[", "x", ",", "z", ",", "-", "x", "[", "...", ",", "1", ":", "]", ".", "flip", "(", "-", "1", ")", "]", ",", "dim", "=", "-", "1", ")", "\n", "x", "=", "x", "/", "self", ".", "Q", "\n", "y", "=", "torch", ".", "fft", ".", "ifft", "(", "x", ")", "[", "...", ",", ":", "self", ".", "N", "]", "\n", "if", "torch", ".", "is_complex", "(", "x", ")", ":", "\n", "            ", "return", "y", "\n", "", "else", ":", "\n", "            ", "return", "torch", ".", "real", "(", "y", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.nn.dxt.IDCT.forward_n": [[139, 143], ["None"], "methods", ["None"], ["", "", "def", "forward_n", "(", "self", ",", "x", ")", ":", "\n", "        ", "\"\"\" DCT type II - reduction to size N \"\"\"", "\n", "assert", "self", ".", "N", "==", "x", ".", "shape", "[", "-", "1", "]", "\n", "raise", "NotImplementedError", "# Straightforward by inverting operations of DCT-II reduction", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.nn.dxt.test_dct_ii": [[144, 168], ["dxt.DCT", "print", "torch.randn", "torch.randn", "baseline", "print", "print", "baseline", "print", "fn", "print", "torch.randn", "torch.randn", "fn", "print", "torch.norm", "torch.norm", "torch.randn", "torch.randn", "torch.norm", "torch.norm"], "function", ["None"], ["", "", "def", "test_dct_ii", "(", ")", ":", "\n", "    ", "N", "=", "8", "\n", "dct", "=", "DCT", "(", "N", ")", "\n", "\n", "baseline", "=", "dct", ".", "forward_dense", "\n", "methods", "=", "[", "dct", ".", "forward_4n", ",", "dct", ".", "forward_2n", ",", "dct", ".", "forward_n", "]", "\n", "\n", "# Real case", "\n", "print", "(", "\"DCT-II Real input\"", ")", "\n", "x", "=", "torch", ".", "randn", "(", "1", ",", "N", ")", "\n", "y", "=", "baseline", "(", "x", ")", "\n", "print", "(", "y", ")", "\n", "for", "fn", "in", "methods", ":", "\n", "        ", "y_", "=", "fn", "(", "x", ")", "\n", "print", "(", "\"err\"", ",", "torch", ".", "norm", "(", "y", "-", "y_", ")", ")", "\n", "\n", "# Complex case", "\n", "", "print", "(", "\"DCT-II Complex input\"", ")", "\n", "x", "=", "torch", ".", "randn", "(", "N", ")", "+", "1j", "*", "torch", ".", "randn", "(", "N", ")", "\n", "y", "=", "baseline", "(", "x", ")", "\n", "print", "(", "y", ")", "\n", "for", "fn", "in", "methods", ":", "\n", "        ", "y_", "=", "fn", "(", "x", ")", "\n", "print", "(", "\"err\"", ",", "torch", ".", "norm", "(", "y", "-", "y_", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.nn.dxt.test_dct_iii": [[169, 194], ["dxt.IDCT", "print", "torch.randn", "torch.randn", "baseline", "print", "print", "baseline", "print", "fn", "print", "torch.ones", "torch.ones", "fn", "print", "torch.norm", "torch.norm", "torch.norm", "torch.norm"], "function", ["None"], ["", "", "def", "test_dct_iii", "(", ")", ":", "\n", "    ", "N", "=", "8", "\n", "dct", "=", "IDCT", "(", "N", ")", "\n", "\n", "baseline", "=", "dct", ".", "forward_dense", "\n", "methods", "=", "[", "dct", ".", "forward_4n", ",", "dct", ".", "forward_2n", "]", "\n", "\n", "# Real case", "\n", "print", "(", "\"DCT-III Real input\"", ")", "\n", "x", "=", "torch", ".", "randn", "(", "1", ",", "N", ")", "\n", "y", "=", "baseline", "(", "x", ")", "\n", "print", "(", "y", ")", "\n", "for", "fn", "in", "methods", ":", "\n", "        ", "y_", "=", "fn", "(", "x", ")", "\n", "print", "(", "\"err\"", ",", "torch", ".", "norm", "(", "y", "-", "y_", ")", ")", "\n", "\n", "# Complex case", "\n", "", "print", "(", "\"DCT-III Complex input\"", ")", "\n", "# x = torch.randn(N) + 1j * torch.randn(N)", "\n", "x", "=", "1j", "*", "torch", ".", "ones", "(", "N", ")", "\n", "y", "=", "baseline", "(", "x", ")", "\n", "print", "(", "y", ")", "\n", "for", "fn", "in", "methods", ":", "\n", "        ", "y_", "=", "fn", "(", "x", ")", "\n", "print", "(", "\"err\"", ",", "torch", ".", "norm", "(", "y", "-", "y_", ")", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.nn.orthogonal.OrthogonalLinear.__init__": [[11, 33], ["exprnn.orthogonal.Orthogonal.__init__"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__"], ["    ", "def", "__init__", "(", "self", ",", "d_input", ",", "d_output", ",", "method", "=", "'dtriv'", ",", "init", "=", "'cayley'", ",", "K", "=", "100", ")", ":", "\n", "        ", "\"\"\" Wrapper around expRNN's Orthogonal class taking care of parameter names \"\"\"", "\n", "if", "method", "==", "\"exprnn\"", ":", "\n", "            ", "mode", "=", "\"static\"", "\n", "param", "=", "'expm'", "\n", "", "elif", "method", "==", "\"dtriv\"", ":", "\n", "# We use 100 as the default to project back to the manifold.", "\n", "# This parameter does not really affect the convergence of the algorithms, even for K=1", "\n", "            ", "mode", "=", "(", "\"dynamic\"", ",", "K", ",", "100", ")", "# TODO maybe K=30? check exprnn codebase", "\n", "param", "=", "'expm'", "\n", "", "elif", "method", "==", "\"cayley\"", ":", "\n", "            ", "mode", "=", "\"static\"", "\n", "param", "=", "'cayley'", "\n", "", "else", ":", "\n", "            ", "assert", "False", ",", "f\"OrthogonalLinear: orthogonal method {method} not supported\"", "\n", "\n", "", "param", "=", "param_name_to_param", "[", "param", "]", "\n", "init_A", "=", "init_name_to_init", "[", "init", "]", "\n", "super", "(", ")", ".", "__init__", "(", "d_input", ",", "d_output", ",", "init_A", ",", "mode", ",", "param", ")", "\n", "\n", "# Scale LR by factor of 10", "\n", "self", ".", "A", ".", "_lr_scale", "=", "0.1", "\n", "", "", ""]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.nn.gate.Gate.__init__": [[22, 50], ["torch.Module.__init__", "preact_ctor", "preact_ctor", "torch.empty", "torch.empty", "torch.empty", "torch.empty", "torch.nn.init.uniform_", "torch.nn.init.uniform_", "torch.nn.init.uniform_", "torch.nn.init.uniform_", "torch.Parameter", "torch.Parameter", "torch.log().detach", "torch.log().detach", "torch.log().detach", "torch.log().detach", "preact_ctor", "preact_ctor", "torch.empty", "torch.empty", "torch.empty", "torch.empty", "torch.nn.init.uniform_", "torch.nn.init.uniform_", "torch.nn.init.uniform_", "torch.nn.init.uniform_", "torch.Parameter", "torch.Parameter", "torch.log().detach", "torch.log().detach", "torch.log().detach", "torch.log().detach", "preact_ctor", "preact_ctor", "torch.log", "torch.log", "torch.log", "torch.log", "preact_ctor", "torch.log", "torch.log", "torch.log", "torch.log"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__"], ["def", "__init__", "(", "self", ",", "size", ",", "preact_ctor", ",", "preact_args", ",", "mechanism", "=", "'N'", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "size", "=", "size", "\n", "self", ".", "mechanism", "=", "mechanism", "\n", "\n", "if", "self", ".", "mechanism", "==", "'N'", ":", "\n", "            ", "pass", "\n", "", "elif", "self", ".", "mechanism", "in", "[", "'G'", ",", "'FS'", ",", "'BE'", ",", "'BR'", ",", "'TE'", ",", "'TR'", ",", "'TS'", ",", "'ZE'", ",", "'ZR'", ",", "'ZS'", "]", ":", "\n", "            ", "self", ".", "W_g", "=", "preact_ctor", "(", "*", "preact_args", ")", "\n", "", "elif", "self", ".", "mechanism", "in", "[", "'U'", ",", "'UT'", "]", ":", "\n", "            ", "self", ".", "W_g", "=", "preact_ctor", "(", "*", "preact_args", ")", "\n", "b_g_unif", "=", "torch", ".", "empty", "(", "size", ")", "\n", "torch", ".", "nn", ".", "init", ".", "uniform_", "(", "b_g_unif", ",", "1.", "/", "self", ".", "size", ",", "1.", "-", "1.", "/", "self", ".", "size", ")", "\n", "self", ".", "b_g", "=", "nn", ".", "Parameter", "(", "torch", ".", "log", "(", "1.", "/", "b_g_unif", "-", "1.", ")", ".", "detach", "(", ")", ",", "requires_grad", "=", "False", ")", "\n", "", "elif", "self", ".", "mechanism", "==", "'UR'", ":", "\n", "            ", "self", ".", "W_g", "=", "preact_ctor", "(", "*", "preact_args", ")", "\n", "self", ".", "W_r", "=", "preact_ctor", "(", "*", "preact_args", ")", "\n", "\n", "b_g_unif", "=", "torch", ".", "empty", "(", "size", ")", "\n", "torch", ".", "nn", ".", "init", ".", "uniform_", "(", "b_g_unif", ",", "1.", "/", "self", ".", "size", ",", "1.", "-", "1.", "/", "self", ".", "size", ")", "\n", "self", ".", "b_g", "=", "nn", ".", "Parameter", "(", "torch", ".", "log", "(", "1.", "/", "b_g_unif", "-", "1.", ")", ".", "detach", "(", ")", ",", "requires_grad", "=", "False", ")", "\n", "", "elif", "self", ".", "mechanism", "==", "'R'", ":", "\n", "            ", "self", ".", "W_g", "=", "preact_ctor", "(", "*", "preact_args", ")", "\n", "self", ".", "W_r", "=", "preact_ctor", "(", "*", "preact_args", ")", "\n", "", "elif", "self", ".", "mechanism", "in", "[", "'GT'", "]", ":", "\n", "            ", "self", ".", "W_g", "=", "preact_ctor", "(", "*", "preact_args", ")", "\n", "", "else", ":", "\n", "            ", "assert", "False", ",", "f'Gating type {self.mechanism} is not supported.'", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.nn.gate.Gate.forward": [[51, 117], ["gate.Gate.W_g", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "gate.Gate.W_g", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "gate.Gate.W_g", "gate.Gate.W_r", "gate.Gate.W_g", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "gate.Gate.W_r", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "gate.Gate.W_g", "gate.Gate.W_g", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "gate.Gate.W_g", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "gate.Gate.forward_diff", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "gate.Gate.backward_diff", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "gate.Gate.backward_diff", "gate.Gate.trapezoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "gate.Gate.trapezoid", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "gate.Gate.trapezoid", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "gate.Gate.zoh", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "gate.Gate.zoh", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "gate.Gate.zoh"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.lssl.AdaptiveTransition.forward_diff", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.lssl.AdaptiveTransition.backward_diff", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.lssl.AdaptiveTransition.backward_diff", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.nn.gate.Gate.trapezoid", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.nn.gate.Gate.trapezoid", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.nn.gate.Gate.trapezoid", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.nn.gate.Gate.zoh", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.nn.gate.Gate.zoh", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.nn.gate.Gate.zoh"], ["", "", "def", "forward", "(", "self", ",", "*", "inputs", ")", ":", "\n", "        ", "if", "self", ".", "mechanism", "==", "'N'", ":", "\n", "            ", "return", "1.0", "\n", "\n", "", "if", "self", ".", "mechanism", "==", "'G'", ":", "\n", "            ", "g_preact", "=", "self", ".", "W_g", "(", "*", "inputs", ")", "\n", "g", "=", "torch", ".", "sigmoid", "(", "g_preact", ")", "\n", "", "if", "self", ".", "mechanism", "==", "'U'", ":", "\n", "            ", "g_preact", "=", "self", ".", "W_g", "(", "*", "inputs", ")", "+", "self", ".", "b_g", "\n", "g", "=", "torch", ".", "sigmoid", "(", "g_preact", ")", "\n", "", "elif", "self", ".", "mechanism", "==", "'UR'", ":", "\n", "            ", "g_preact", "=", "self", ".", "W_g", "(", "*", "inputs", ")", "+", "self", ".", "b_g", "\n", "g", "=", "torch", ".", "sigmoid", "(", "g_preact", ")", "\n", "r", "=", "torch", ".", "sigmoid", "(", "self", ".", "W_r", "(", "*", "inputs", ")", ")", "\n", "g", "=", "(", "1", "-", "2", "*", "r", ")", "*", "g", "**", "2", "+", "2", "*", "r", "*", "g", "\n", "", "elif", "self", ".", "mechanism", "==", "'R'", ":", "\n", "            ", "g_preact", "=", "self", ".", "W_g", "(", "*", "inputs", ")", "\n", "g", "=", "torch", ".", "sigmoid", "(", "g_preact", ")", "\n", "r", "=", "torch", ".", "sigmoid", "(", "self", ".", "W_r", "(", "*", "inputs", ")", ")", "\n", "g", "=", "(", "1", "-", "2", "*", "r", ")", "*", "g", "**", "2", "+", "2", "*", "r", "*", "g", "\n", "", "elif", "self", ".", "mechanism", "==", "'UT'", ":", "\n", "            ", "g_preact", "=", "self", ".", "W_g", "(", "*", "inputs", ")", "+", "self", ".", "b_g", "\n", "g", "=", "torch", ".", "sigmoid", "(", "g_preact", ")", "\n", "r", "=", "g", "\n", "g", "=", "(", "1", "-", "2", "*", "r", ")", "*", "g", "**", "2", "+", "2", "*", "r", "*", "g", "\n", "", "elif", "self", ".", "mechanism", "==", "'GT'", ":", "\n", "            ", "g_preact", "=", "self", ".", "W_g", "(", "*", "inputs", ")", "\n", "g", "=", "torch", ".", "sigmoid", "(", "g_preact", ")", "\n", "r", "=", "g", "\n", "g", "=", "(", "1", "-", "2", "*", "r", ")", "*", "g", "**", "2", "+", "2", "*", "r", "*", "g", "\n", "", "else", ":", "\n", "            ", "g_preact", "=", "self", ".", "W_g", "(", "*", "inputs", ")", "\n", "# if self.mechanism[1] == 'S':", "\n", "#     g = torch.sigmoid(g_preact)", "\n", "# elif self.mechanism[1] == 'E':", "\n", "#     g = torch.exp(g_preact)", "\n", "# elif self.mechanism[1] == 'R':", "\n", "#     g = torch.relu(g_preact)", "\n", "if", "self", ".", "mechanism", "==", "'FS'", ":", "\n", "                ", "g", "=", "torch", ".", "sigmoid", "(", "g_preact", ")", "\n", "g", "=", "self", ".", "forward_diff", "(", "g", ")", "\n", "", "elif", "self", ".", "mechanism", "==", "'BE'", ":", "\n", "                ", "g", "=", "torch", ".", "exp", "(", "g_preact", ")", "\n", "g", "=", "self", ".", "backward_diff", "(", "g", ")", "\n", "", "elif", "self", ".", "mechanism", "==", "'BR'", ":", "\n", "                ", "g", "=", "torch", ".", "relu", "(", "g_preact", ")", "\n", "g", "=", "self", ".", "backward_diff", "(", "g", ")", "\n", "", "elif", "self", ".", "mechanism", "==", "'TS'", ":", "\n", "                ", "g", "=", "2", "*", "torch", ".", "sigmoid", "(", "g_preact", ")", "\n", "g", "=", "self", ".", "trapezoid", "(", "g", ")", "\n", "", "elif", "self", ".", "mechanism", "==", "'TE'", ":", "\n", "                ", "g", "=", "torch", ".", "exp", "(", "g_preact", ")", "\n", "g", "=", "self", ".", "trapezoid", "(", "g", ")", "\n", "", "elif", "self", ".", "mechanism", "==", "'TR'", ":", "\n", "                ", "g", "=", "torch", ".", "relu", "(", "g_preact", ")", "\n", "g", "=", "self", ".", "trapezoid", "(", "g", ")", "\n", "", "elif", "self", ".", "mechanism", "==", "'ZE'", ":", "\n", "                ", "g", "=", "torch", ".", "exp", "(", "g_preact", ")", "\n", "g", "=", "self", ".", "zoh", "(", "g", ")", "\n", "", "elif", "self", ".", "mechanism", "==", "'ZR'", ":", "\n", "                ", "g", "=", "torch", ".", "relu", "(", "g_preact", ")", "\n", "g", "=", "self", ".", "zoh", "(", "g", ")", "\n", "", "elif", "self", ".", "mechanism", "==", "'ZS'", ":", "\n", "                ", "g", "=", "torch", ".", "sigmoid", "(", "g_preact", ")", "\n", "g", "=", "self", ".", "zoh", "(", "g", ")", "\n", "", "", "return", "g", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.nn.gate.Gate.forward_diff": [[118, 120], ["None"], "methods", ["None"], ["", "def", "forward_diff", "(", "self", ",", "x", ")", ":", "\n", "        ", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.nn.gate.Gate.backward_diff": [[121, 123], ["None"], "methods", ["None"], ["", "def", "backward_diff", "(", "self", ",", "x", ")", ":", "\n", "        ", "return", "x", "/", "(", "1", "+", "x", ")", "\n", "# return 1 / (1+1/x)", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.nn.gate.Gate.trapezoid": [[125, 127], ["None"], "methods", ["None"], ["", "def", "trapezoid", "(", "self", ",", "x", ")", ":", "\n", "        ", "return", "x", "/", "(", "1", "+", "x", "/", "2", ")", "\n", "# return 1 / (.5 + 1/x)", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.nn.gate.Gate.zoh": [[129, 131], ["torch.exp", "torch.exp", "torch.exp", "torch.exp"], "methods", ["None"], ["", "def", "zoh", "(", "self", ",", "x", ")", ":", "\n", "        ", "return", "1", "-", "torch", ".", "exp", "(", "-", "x", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.nn.components.Modrelu.reset_parameters": [[58, 60], ["components.Modrelu.b.data.uniform_"], "methods", ["None"], ["    ", "def", "reset_parameters", "(", "self", ")", ":", "\n", "        ", "self", ".", "b", ".", "data", ".", "uniform_", "(", "-", "0.01", ",", "0.01", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.nn.components.TransposedLinear.__init__": [[99, 112], ["torch.Module.__init__", "torch.Parameter", "torch.Parameter", "torch.init.kaiming_uniform_", "torch.init.kaiming_uniform_", "torch.empty", "torch.empty", "torch.empty", "torch.empty", "torch.Parameter", "torch.Parameter", "torch.init.uniform_", "torch.init.uniform_", "math.sqrt", "torch.empty", "torch.empty", "torch.empty", "torch.empty", "math.sqrt"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__"], ["def", "__init__", "(", "self", ",", "d_input", ",", "d_output", ",", "bias", "=", "True", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "weight", "=", "nn", ".", "Parameter", "(", "torch", ".", "empty", "(", "d_output", ",", "d_input", ")", ")", "\n", "nn", ".", "init", ".", "kaiming_uniform_", "(", "self", ".", "weight", ",", "a", "=", "math", ".", "sqrt", "(", "5", ")", ")", "# nn.Linear default init", "\n", "# nn.init.kaiming_uniform_(self.weight, nonlinearity='linear') # should be equivalent", "\n", "\n", "if", "bias", ":", "\n", "            ", "self", ".", "bias", "=", "nn", ".", "Parameter", "(", "torch", ".", "empty", "(", "d_output", ",", "1", ")", ")", "\n", "bound", "=", "1", "/", "math", ".", "sqrt", "(", "d_input", ")", "\n", "nn", ".", "init", ".", "uniform_", "(", "self", ".", "bias", ",", "-", "bound", ",", "bound", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "bias", "=", "0.0", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.nn.components.TransposedLinear.forward": [[113, 115], ["opt_einsum.contract"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "return", "contract", "(", "'... u l, v u -> ... v l'", ",", "x", ",", "self", ".", "weight", ")", "+", "self", ".", "bias", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.nn.components.TransposedLN.__init__": [[122, 130], ["torch.Module.__init__", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.LayerNorm", "torch.LayerNorm", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.ones", "torch.ones", "torch.ones", "torch.ones"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__"], ["def", "__init__", "(", "self", ",", "d", ",", "scalar", "=", "True", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "scalar", "=", "scalar", "\n", "if", "self", ".", "scalar", ":", "\n", "            ", "self", ".", "m", "=", "nn", ".", "Parameter", "(", "torch", ".", "zeros", "(", "1", ")", ")", "\n", "self", ".", "s", "=", "nn", ".", "Parameter", "(", "torch", ".", "ones", "(", "1", ")", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "ln", "=", "nn", ".", "LayerNorm", "(", "d", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.nn.components.TransposedLN.forward": [[131, 138], ["torch.std_mean", "torch.std_mean", "torch.std_mean", "torch.std_mean", "components.TransposedLN.ln().transpose", "components.TransposedLN.ln", "x.transpose"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "if", "self", ".", "scalar", ":", "\n", "            ", "s", ",", "m", "=", "torch", ".", "std_mean", "(", "x", ",", "dim", "=", "-", "2", ",", "unbiased", "=", "False", ",", "keepdim", "=", "True", ")", "\n", "y", "=", "(", "self", ".", "s", "/", "s", ")", "*", "(", "x", "-", "m", "+", "self", ".", "m", ")", "\n", "", "else", ":", "\n", "            ", "y", "=", "self", ".", "ln", "(", "x", ".", "transpose", "(", "-", "1", ",", "-", "2", ")", ")", ".", "transpose", "(", "-", "1", ",", "-", "2", ")", "\n", "", "return", "y", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.nn.components.Normalization.__init__": [[140, 170], ["torch.Module.__init__", "components.TransposedLN", "torch.LayerNorm", "torch.LayerNorm", "norm_args.update", "torch.InstanceNorm1d", "torch.InstanceNorm1d", "norm_args.update", "torch.BatchNorm1d", "torch.BatchNorm1d", "torch.Identity", "torch.Identity"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "d", ",", "\n", "transposed", "=", "False", ",", "# Length dimension is -1 or -2", "\n", "_name_", "=", "'layer'", ",", "\n", "**", "kwargs", "\n", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "transposed", "=", "transposed", "\n", "\n", "if", "_name_", "==", "'layer'", ":", "\n", "            ", "self", ".", "channel", "=", "True", "# Normalize over channel dimension", "\n", "if", "self", ".", "transposed", ":", "\n", "                ", "self", ".", "norm", "=", "TransposedLN", "(", "d", ",", "**", "kwargs", ")", "\n", "", "else", ":", "\n", "                ", "self", ".", "norm", "=", "nn", ".", "LayerNorm", "(", "d", ",", "**", "kwargs", ")", "\n", "", "", "elif", "_name_", "==", "'instance'", ":", "\n", "            ", "self", ".", "channel", "=", "False", "\n", "norm_args", "=", "{", "'affine'", ":", "False", ",", "'track_running_stats'", ":", "False", "}", "\n", "norm_args", ".", "update", "(", "kwargs", ")", "\n", "self", ".", "norm", "=", "nn", ".", "InstanceNorm1d", "(", "d", ",", "**", "norm_args", ")", "# (True, True) performs very poorly", "\n", "", "elif", "_name_", "==", "'batch'", ":", "\n", "            ", "self", ".", "channel", "=", "False", "\n", "norm_args", "=", "{", "'affine'", ":", "True", ",", "'track_running_stats'", ":", "True", "}", "\n", "norm_args", ".", "update", "(", "kwargs", ")", "\n", "self", ".", "norm", "=", "nn", ".", "BatchNorm1d", "(", "d", ",", "**", "norm_args", ")", "\n", "", "elif", "_name_", "==", "'none'", ":", "\n", "            ", "self", ".", "channel", "=", "True", "\n", "self", ".", "norm", "=", "nn", ".", "Identity", "(", ")", "\n", "", "else", ":", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.nn.components.Normalization.forward": [[171, 181], ["components.Normalization.norm", "x.transpose.transpose.transpose", "components.Normalization.norm", "x.transpose.transpose.transpose"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "# The cases of LayerNorm / no normalization are automatically handled in all cases", "\n", "# Instance/Batch Norm work automatically with transposed axes", "\n", "        ", "if", "self", ".", "channel", "or", "self", ".", "transposed", ":", "\n", "            ", "return", "self", ".", "norm", "(", "x", ")", "\n", "", "else", ":", "\n", "            ", "x", "=", "x", ".", "transpose", "(", "-", "1", ",", "-", "2", ")", "\n", "x", "=", "self", ".", "norm", "(", "x", ")", "\n", "x", "=", "x", ".", "transpose", "(", "-", "1", ",", "-", "2", ")", "\n", "return", "x", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.nn.components.Activation": [[12, 31], ["torch.Identity", "torch.Tanh", "torch.ReLU", "torch.GELU", "torch.SiLU", "torch.GLU", "torch.Sigmoid", "components.Modrelu", "NotImplementedError"], "function", ["None"], ["def", "Activation", "(", "activation", "=", "None", ",", "size", "=", "None", ",", "dim", "=", "-", "1", ")", ":", "\n", "    ", "if", "activation", "in", "[", "None", ",", "'id'", ",", "'identity'", ",", "'linear'", "]", ":", "\n", "        ", "return", "nn", ".", "Identity", "(", ")", "\n", "", "elif", "activation", "==", "'tanh'", ":", "\n", "        ", "return", "nn", ".", "Tanh", "(", ")", "\n", "", "elif", "activation", "==", "'relu'", ":", "\n", "        ", "return", "nn", ".", "ReLU", "(", ")", "\n", "", "elif", "activation", "==", "'gelu'", ":", "\n", "        ", "return", "nn", ".", "GELU", "(", ")", "\n", "", "elif", "activation", "in", "[", "'swish'", ",", "'silu'", "]", ":", "\n", "        ", "return", "nn", ".", "SiLU", "(", ")", "\n", "", "elif", "activation", "==", "'glu'", ":", "\n", "        ", "return", "nn", ".", "GLU", "(", "dim", "=", "dim", ")", "\n", "", "elif", "activation", "==", "'sigmoid'", ":", "\n", "        ", "return", "nn", ".", "Sigmoid", "(", ")", "\n", "", "elif", "activation", "==", "'modrelu'", ":", "\n", "        ", "return", "Modrelu", "(", "size", ")", "\n", "", "else", ":", "\n", "        ", "raise", "NotImplementedError", "(", "\"hidden activation '{}' is not implemented\"", ".", "format", "(", "activation", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.nn.components.get_initializer": [[32, 56], ["functools.partial", "functools.partial", "NotImplementedError", "functools.partial", "functools.partial", "NotImplementedError"], "function", ["None"], ["", "", "def", "get_initializer", "(", "name", ",", "activation", "=", "None", ")", ":", "\n", "    ", "if", "activation", "in", "[", "None", ",", "'id'", ",", "'identity'", ",", "'linear'", ",", "'modrelu'", "]", ":", "\n", "        ", "nonlinearity", "=", "'linear'", "\n", "", "elif", "activation", "in", "[", "'relu'", ",", "'tanh'", ",", "'sigmoid'", "]", ":", "\n", "        ", "nonlinearity", "=", "activation", "\n", "", "elif", "activation", "in", "[", "'gelu'", ",", "'swish'", ",", "'silu'", "]", ":", "\n", "        ", "nonlinearity", "=", "'relu'", "# Close to ReLU so approximate with ReLU's gain", "\n", "", "else", ":", "\n", "        ", "raise", "NotImplementedError", "(", "f\"get_initializer: activation {activation} not supported\"", ")", "\n", "\n", "", "if", "name", "==", "'uniform'", ":", "\n", "        ", "initializer", "=", "partial", "(", "torch", ".", "nn", ".", "init", ".", "kaiming_uniform_", ",", "nonlinearity", "=", "nonlinearity", ")", "\n", "", "elif", "name", "==", "'normal'", ":", "\n", "        ", "initializer", "=", "partial", "(", "torch", ".", "nn", ".", "init", ".", "kaiming_normal_", ",", "nonlinearity", "=", "nonlinearity", ")", "\n", "", "elif", "name", "==", "'xavier'", ":", "\n", "        ", "initializer", "=", "torch", ".", "nn", ".", "init", ".", "xavier_normal_", "\n", "", "elif", "name", "==", "'zero'", ":", "\n", "        ", "initializer", "=", "partial", "(", "torch", ".", "nn", ".", "init", ".", "constant_", ",", "val", "=", "0", ")", "\n", "", "elif", "name", "==", "'one'", ":", "\n", "        ", "initializer", "=", "partial", "(", "torch", ".", "nn", ".", "init", ".", "constant_", ",", "val", "=", "1", ")", "\n", "", "else", ":", "\n", "        ", "raise", "NotImplementedError", "(", "f\"get_initializer: initializer type {name} not supported\"", ")", "\n", "\n", "", "return", "initializer", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.nn.components.LinearActivation": [[61, 94], ["linear_cls", "torch.init.zeros_", "torch.utils.weight_norm", "components.Activation", "torch.Sequential", "components.get_initializer"], "function", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.s4.Activation", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.nn.components.get_initializer"], ["", "", "def", "LinearActivation", "(", "\n", "d_input", ",", "d_output", ",", "bias", "=", "True", ",", "\n", "zero_bias_init", "=", "False", ",", "\n", "transposed", "=", "False", ",", "\n", "initializer", "=", "None", ",", "\n", "activation", "=", "None", ",", "\n", "activate", "=", "False", ",", "# Apply activation as part of this module", "\n", "weight_norm", "=", "False", ",", "\n", "**", "kwargs", ",", "\n", ")", ":", "\n", "    ", "\"\"\" Returns a linear nn.Module with control over axes order, initialization, and activation \"\"\"", "\n", "\n", "# Construct core module", "\n", "linear_cls", "=", "TransposedLinear", "if", "transposed", "else", "nn", ".", "Linear", "\n", "if", "activation", "==", "'glu'", ":", "d_output", "*=", "2", "\n", "linear", "=", "linear_cls", "(", "d_input", ",", "d_output", ",", "bias", "=", "bias", ",", "**", "kwargs", ")", "\n", "\n", "# Initialize weight", "\n", "if", "initializer", "is", "not", "None", ":", "\n", "        ", "get_initializer", "(", "initializer", ",", "activation", ")", "(", "linear", ".", "weight", ")", "\n", "\n", "# Initialize bias", "\n", "", "if", "bias", "and", "zero_bias_init", ":", "\n", "        ", "nn", ".", "init", ".", "zeros_", "(", "linear", ".", "bias", ")", "\n", "\n", "# Weight norm", "\n", "", "if", "weight_norm", ":", "\n", "        ", "linear", "=", "nn", ".", "utils", ".", "weight_norm", "(", "linear", ")", "\n", "\n", "", "if", "activate", "and", "activation", "is", "not", "None", ":", "\n", "        ", "activation", "=", "Activation", "(", "activation", ",", "d_output", ",", "dim", "=", "-", "2", "if", "transposed", "else", "-", "1", ")", "\n", "linear", "=", "nn", ".", "Sequential", "(", "linear", ",", "activation", ")", "\n", "", "return", "linear", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.exprnn.initialization.henaff_init_": [[8, 12], ["A.new().uniform_", "initialization.create_diag_", "A.size", "A.new"], "function", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.exprnn.initialization.create_diag_"], ["        ", "nn", ".", "init", ".", "uniform_", "(", "weight", ",", "-", "init_cfg", ".", "init_range", ",", "init_cfg", ".", "init_range", ")", "\n", "", "elif", "init_cfg", ".", "init", "==", "'normal'", ":", "\n", "        ", "nn", ".", "init", ".", "normal_", "(", "weight", ",", "0.0", ",", "init_cfg", ".", "init_std", ")", "\n", "", "elif", "init_cfg", ".", "init", "==", "'xavier'", ":", "\n", "        ", "nn", ".", "init", ".", "xavier_uniform_", "(", "weight", ")", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.exprnn.initialization.cayley_init_": [[14, 19], ["A.new().uniform_", "initialization.create_diag_", "A.size", "torch.sqrt", "A.new", "torch.cos", "torch.cos"], "function", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.exprnn.initialization.create_diag_"], ["        ", "nn", ".", "init", ".", "kaiming_normal_", "(", "weight", ",", "mode", "=", "'fan_in'", ",", "nonlinearity", "=", "'linear'", ")", "\n", "", "else", ":", "\n", "        ", "raise", "NotImplementedError", "(", "f\"initialization type {init_cfg.init} not supported\"", ")", "\n", "\n", "", "", "def", "init_bias", "(", "bias", ",", "init_cfg", ")", ":", "\n", "    ", "if", "hasattr", "(", "init_cfg", ",", "'zero_bias'", ")", "and", "init_cfg", ".", "zero_bias", "==", "False", ":", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.exprnn.initialization.haar_init_": [[21, 33], ["torch.nn.init.orthogonal_", "torch.no_grad", "A.copy_", "A.det", "numpy.random.randint", "scipy.logm", "torch.tensor", "A.size", "A.data.cpu().numpy", "A.data.cpu"], "function", ["None"], ["        ", "pass", "\n", "", "else", ":", "\n", "        ", "nn", ".", "init", ".", "constant_", "(", "bias", ",", "0.0", ")", "\n", "\n", "", "", "def", "weights_init", "(", "m", ",", "init_cfg", ")", ":", "\n", "    ", "classname", "=", "m", ".", "__class__", ".", "__name__", "\n", "if", "classname", ".", "find", "(", "'Linear'", ")", "!=", "-", "1", ":", "\n", "        ", "if", "hasattr", "(", "m", ",", "'weight'", ")", "and", "m", ".", "weight", "is", "not", "None", ":", "\n", "            ", "init_weight", "(", "m", ".", "weight", ",", "init_cfg", ")", "\n", "", "if", "hasattr", "(", "m", ",", "'bias'", ")", "and", "m", ".", "bias", "is", "not", "None", ":", "\n", "            ", "init_bias", "(", "m", ".", "bias", ",", "init_cfg", ")", "\n", "", "", "elif", "classname", ".", "find", "(", "'LayerNorm'", ")", "!=", "-", "1", ":", "\n", "        ", "if", "hasattr", "(", "m", ",", "'weight'", ")", ":", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.exprnn.initialization.haar_diag_init_": [[35, 46], ["initialization.haar_init_", "torch.no_grad", "A.data.cpu().numpy", "torch.tensor", "initialization.create_diag_", "scipy.eigvals", "A.data.cpu", "A.size"], "function", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.exprnn.initialization.haar_init_", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.exprnn.initialization.create_diag_"], ["                ", "pass", "\n", "", "else", ":", "\n", "                ", "nn", ".", "init", ".", "normal_", "(", "m", ".", "weight", ",", "1.0", ",", "init_cfg", ".", "init_std", ")", "\n", "", "", "if", "hasattr", "(", "m", ",", "'bias'", ")", "and", "m", ".", "bias", "is", "not", "None", ":", "\n", "            ", "init_bias", "(", "m", ".", "bias", ",", "init_cfg", ")", "\n", "", "", "elif", "classname", ".", "find", "(", "'TransformerLM'", ")", "!=", "-", "1", ":", "\n", "        ", "if", "hasattr", "(", "m", ",", "'r_emb'", ")", ":", "\n", "            ", "init_weight", "(", "m", ".", "r_emb", ",", "init_cfg", ")", "\n", "", "if", "hasattr", "(", "m", ",", "'r_w_bias'", ")", ":", "\n", "            ", "init_weight", "(", "m", ".", "r_w_bias", ",", "init_cfg", ")", "\n", "", "if", "hasattr", "(", "m", ",", "'r_r_bias'", ")", ":", "\n", "            ", "init_weight", "(", "m", ".", "r_r_bias", ",", "init_cfg", ")", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.exprnn.initialization.normal_squeeze_diag_init_": [[48, 52], ["A.new().normal_().fmod_", "initialization.create_diag_", "A.size", "A.new().normal_", "A.new"], "function", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.exprnn.initialization.create_diag_"], ["            ", "init_bias", "(", "m", ".", "r_bias", ",", "init_cfg", ")", "\n", "", "if", "hasattr", "(", "m", ",", "'initial_state'", ")", ":", "\n", "            ", "init_bias", "(", "m", ".", "initial_state", ",", "init_cfg", ")", "\n", "\n", "", "", "", "def", "weights_init_embedding", "(", "m", ",", "init_cfg", ")", ":", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.exprnn.initialization.normal_diag_init_": [[53, 57], ["A.new().normal_().fmod_", "initialization.create_diag_", "A.size", "A.new().normal_", "A.new"], "function", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.exprnn.initialization.create_diag_"], ["    ", "classname", "=", "m", ".", "__class__", ".", "__name__", "\n", "if", "classname", ".", "find", "(", "'AdaptiveEmbedding'", ")", "!=", "-", "1", ":", "\n", "        ", "if", "hasattr", "(", "m", ",", "'emb_projs'", ")", ":", "\n", "            ", "for", "i", "in", "range", "(", "len", "(", "m", ".", "emb_projs", ")", ")", ":", "\n", "                ", "if", "m", ".", "emb_projs", "[", "i", "]", "is", "not", "None", ":", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.exprnn.initialization.create_diag_": [[59, 68], ["A.size", "torch.zeros", "torch.diag", "torch.no_grad", "A.copy_"], "function", ["None"], ["", "", "", "", "elif", "classname", ".", "find", "(", "'Embedding'", ")", "!=", "-", "1", ":", "\n", "        ", "if", "hasattr", "(", "m", ",", "'weight'", ")", ":", "\n", "            ", "init_weight", "(", "m", ".", "weight", ",", "init_cfg", ")", "\n", "", "", "elif", "classname", ".", "find", "(", "'ProjectedAdaptiveLogSoftmax'", ")", "!=", "-", "1", ":", "\n", "        ", "if", "hasattr", "(", "m", ",", "'cluster_weight'", ")", "and", "m", ".", "cluster_weight", "is", "not", "None", ":", "\n", "            ", "init_weight", "(", "m", ".", "cluster_weight", ",", "init_cfg", ")", "\n", "", "if", "hasattr", "(", "m", ",", "'cluster_bias'", ")", "and", "m", ".", "cluster_bias", "is", "not", "None", ":", "\n", "            ", "init_bias", "(", "m", ".", "cluster_bias", ",", "init_cfg", ")", "\n", "", "if", "hasattr", "(", "m", ",", "'out_projs'", ")", ":", "\n", "            ", "for", "i", "in", "range", "(", "len", "(", "m", ".", "out_projs", ")", ")", ":", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.exprnn.trivializations.expm_class.forward": [[13, 17], ["ctx.save_for_backward", "expm32.expm32.expm32"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.exprnn.expm32.expm32"], ["    ", "@", "staticmethod", "\n", "def", "forward", "(", "ctx", ",", "A", ")", ":", "\n", "        ", "ctx", ".", "save_for_backward", "(", "A", ")", "\n", "return", "expm32", "(", "A", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.exprnn.trivializations.expm_class.backward": [[18, 22], ["expm32.expm32.differential", "A.t"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.exprnn.expm32.differential"], ["", "@", "staticmethod", "\n", "def", "backward", "(", "ctx", ",", "G", ")", ":", "\n", "        ", "(", "A", ",", ")", "=", "ctx", ".", "saved_tensors", "\n", "return", "differential", "(", "expm32", ",", "A", ".", "t", "(", ")", ",", "G", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.exprnn.trivializations.cayley_map": [[7, 11], ["X.size", "torch.eye", "torch.solve"], "function", ["None"], ["def", "cayley_map", "(", "X", ")", ":", "\n", "    ", "n", "=", "X", ".", "size", "(", "0", ")", "\n", "Id", "=", "torch", ".", "eye", "(", "n", ",", "dtype", "=", "X", ".", "dtype", ",", "device", "=", "X", ".", "device", ")", "\n", "return", "torch", ".", "solve", "(", "Id", "-", "X", ",", "Id", "+", "X", ")", "[", "0", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.exprnn.expm32._ExpmPadeHelper.__init__": [[71, 95], ["expm32._ident_like"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.exprnn.expm32._ident_like"], ["def", "__init__", "(", "self", ",", "A", ")", ":", "\n", "        ", "\"\"\"\n        Initialize the object.\n\n        Parameters\n        ----------\n        A : a dense or sparse square numpy matrix or ndarray\n            The matrix to be exponentiated.\n        \"\"\"", "\n", "self", ".", "A", "=", "A", "\n", "self", ".", "_A2", "=", "None", "\n", "self", ".", "_A4", "=", "None", "\n", "self", ".", "_A6", "=", "None", "\n", "self", ".", "_A8", "=", "None", "\n", "self", ".", "_A10", "=", "None", "\n", "self", ".", "_d4_exact", "=", "None", "\n", "self", ".", "_d6_exact", "=", "None", "\n", "self", ".", "_d8_exact", "=", "None", "\n", "self", ".", "_d10_exact", "=", "None", "\n", "self", ".", "_d4_approx", "=", "None", "\n", "self", ".", "_d6_approx", "=", "None", "\n", "self", ".", "_d8_approx", "=", "None", "\n", "self", ".", "_d10_approx", "=", "None", "\n", "self", ".", "ident", "=", "_ident_like", "(", "A", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.exprnn.expm32._ExpmPadeHelper.A2": [[96, 101], ["expm32._ExpmPadeHelper.A.mm"], "methods", ["None"], ["", "@", "property", "\n", "def", "A2", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "_A2", "is", "None", ":", "\n", "            ", "self", ".", "_A2", "=", "self", ".", "A", ".", "mm", "(", "self", ".", "A", ")", "\n", "", "return", "self", ".", "_A2", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.exprnn.expm32._ExpmPadeHelper.A4": [[102, 107], ["expm32._ExpmPadeHelper.A2.mm"], "methods", ["None"], ["", "@", "property", "\n", "def", "A4", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "_A4", "is", "None", ":", "\n", "            ", "self", ".", "_A4", "=", "self", ".", "A2", ".", "mm", "(", "self", ".", "A2", ")", "\n", "", "return", "self", ".", "_A4", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.exprnn.expm32._ExpmPadeHelper.A6": [[108, 113], ["expm32._ExpmPadeHelper.A4.mm"], "methods", ["None"], ["", "@", "property", "\n", "def", "A6", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "_A6", "is", "None", ":", "\n", "            ", "self", ".", "_A6", "=", "self", ".", "A4", ".", "mm", "(", "self", ".", "A2", ")", "\n", "", "return", "self", ".", "_A6", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.exprnn.expm32._ExpmPadeHelper.A8": [[114, 119], ["expm32._ExpmPadeHelper.A6.mm"], "methods", ["None"], ["", "@", "property", "\n", "def", "A8", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "_A8", "is", "None", ":", "\n", "            ", "self", ".", "_A8", "=", "self", ".", "A6", ".", "mm", "(", "self", ".", "A2", ")", "\n", "", "return", "self", ".", "_A8", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.exprnn.expm32._ExpmPadeHelper.A10": [[120, 125], ["expm32._ExpmPadeHelper.A4.mm"], "methods", ["None"], ["", "@", "property", "\n", "def", "A10", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "_A10", "is", "None", ":", "\n", "            ", "self", ".", "_A10", "=", "self", ".", "A4", ".", "mm", "(", "self", ".", "A6", ")", "\n", "", "return", "self", ".", "_A10", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.exprnn.expm32._ExpmPadeHelper.d4_tight": [[126, 131], ["expm32._onenorm"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.exprnn.expm32._onenorm"], ["", "@", "property", "\n", "def", "d4_tight", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "_d4_exact", "is", "None", ":", "\n", "            ", "self", ".", "_d4_exact", "=", "_onenorm", "(", "self", ".", "A4", ")", "**", "(", "1", "/", "4.", ")", "\n", "", "return", "self", ".", "_d4_exact", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.exprnn.expm32._ExpmPadeHelper.d6_tight": [[132, 137], ["expm32._onenorm"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.exprnn.expm32._onenorm"], ["", "@", "property", "\n", "def", "d6_tight", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "_d6_exact", "is", "None", ":", "\n", "            ", "self", ".", "_d6_exact", "=", "_onenorm", "(", "self", ".", "A6", ")", "**", "(", "1", "/", "6.", ")", "\n", "", "return", "self", ".", "_d6_exact", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.exprnn.expm32._ExpmPadeHelper.d8_tight": [[138, 143], ["expm32._onenorm"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.exprnn.expm32._onenorm"], ["", "@", "property", "\n", "def", "d8_tight", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "_d8_exact", "is", "None", ":", "\n", "            ", "self", ".", "_d8_exact", "=", "_onenorm", "(", "self", ".", "A8", ")", "**", "(", "1", "/", "8.", ")", "\n", "", "return", "self", ".", "_d8_exact", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.exprnn.expm32._ExpmPadeHelper.d10_tight": [[144, 149], ["expm32._onenorm"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.exprnn.expm32._onenorm"], ["", "@", "property", "\n", "def", "d10_tight", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "_d10_exact", "is", "None", ":", "\n", "            ", "self", ".", "_d10_exact", "=", "_onenorm", "(", "self", ".", "A10", ")", "**", "(", "1", "/", "10.", ")", "\n", "", "return", "self", ".", "_d10_exact", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.exprnn.expm32._ExpmPadeHelper.d4_loose": [[150, 153], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "d4_loose", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "d4_tight", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.exprnn.expm32._ExpmPadeHelper.d6_loose": [[154, 157], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "d6_loose", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "d6_tight", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.exprnn.expm32._ExpmPadeHelper.d8_loose": [[158, 161], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "d8_loose", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "d8_tight", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.exprnn.expm32._ExpmPadeHelper.d10_loose": [[162, 165], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "d10_loose", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "d10_tight", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.exprnn.expm32._ExpmPadeHelper.pade3": [[166, 171], ["expm32._ExpmPadeHelper.A.mm"], "methods", ["None"], ["", "def", "pade3", "(", "self", ")", ":", "\n", "        ", "b", "=", "(", "120.", ",", "60.", ",", "12.", ",", "1.", ")", "\n", "U", "=", "self", ".", "A", ".", "mm", "(", "b", "[", "3", "]", "*", "self", ".", "A2", "+", "b", "[", "1", "]", "*", "self", ".", "ident", ")", "\n", "V", "=", "b", "[", "2", "]", "*", "self", ".", "A2", "+", "b", "[", "0", "]", "*", "self", ".", "ident", "\n", "return", "U", ",", "V", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.exprnn.expm32._ExpmPadeHelper.pade5": [[172, 177], ["expm32._ExpmPadeHelper.A.mm"], "methods", ["None"], ["", "def", "pade5", "(", "self", ")", ":", "\n", "        ", "b", "=", "(", "30240.", ",", "15120.", ",", "3360.", ",", "420.", ",", "30.", ",", "1.", ")", "\n", "U", "=", "self", ".", "A", ".", "mm", "(", "b", "[", "5", "]", "*", "self", ".", "A4", "+", "b", "[", "3", "]", "*", "self", ".", "A2", "+", "b", "[", "1", "]", "*", "self", ".", "ident", ")", "\n", "V", "=", "b", "[", "4", "]", "*", "self", ".", "A4", "+", "b", "[", "2", "]", "*", "self", ".", "A2", "+", "b", "[", "0", "]", "*", "self", ".", "ident", "\n", "return", "U", ",", "V", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.exprnn.expm32._ExpmPadeHelper.pade7_scaled": [[178, 189], ["B.mm"], "methods", ["None"], ["", "def", "pade7_scaled", "(", "self", ",", "s", ")", ":", "\n", "        ", "b", "=", "(", "17297280.", ",", "8648640.", ",", "1995840.", ",", "277200.", ",", "25200.", ",", "1512.", ",", "56.", ",", "1.", ")", "\n", "\n", "B", "=", "self", ".", "A", "*", "2", "**", "-", "s", "\n", "B2", "=", "self", ".", "A2", "*", "2", "**", "(", "-", "2", "*", "s", ")", "\n", "B4", "=", "self", ".", "A4", "*", "2", "**", "(", "-", "4", "*", "s", ")", "\n", "B6", "=", "self", ".", "A6", "*", "2", "**", "(", "-", "6", "*", "s", ")", "\n", "\n", "U", "=", "B", ".", "mm", "(", "b", "[", "7", "]", "*", "B6", "+", "b", "[", "5", "]", "*", "B4", "+", "b", "[", "3", "]", "*", "B2", "+", "b", "[", "1", "]", "*", "self", ".", "ident", ")", "\n", "V", "=", "b", "[", "6", "]", "*", "B6", "+", "b", "[", "4", "]", "*", "B4", "+", "b", "[", "2", "]", "*", "B2", "+", "b", "[", "0", "]", "*", "self", ".", "ident", "\n", "return", "U", ",", "V", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.exprnn.expm32._onenorm_matrix_power_nnm": [[22, 53], ["int", "torch.ones", "A.t", "range", "torch.max().item", "ValueError", "ValueError", "A.t.mm", "int", "len", "torch.max"], "function", ["None"], ["def", "_onenorm_matrix_power_nnm", "(", "A", ",", "p", ")", ":", "\n", "    ", "\"\"\"\n    Compute the 1-norm of a non-negative integer power of a non-negative matrix.\n\n    Parameters\n    ----------\n    A : a square ndarray or matrix or sparse matrix\n        Input matrix with non-negative entries.\n    p : non-negative integer\n        The power to which the matrix is to be raised.\n\n    Returns\n    -------\n    out : float\n        The 1-norm of the matrix power p of A.\n\n    \"\"\"", "\n", "# check input", "\n", "if", "int", "(", "p", ")", "!=", "p", "or", "p", "<", "0", ":", "\n", "        ", "raise", "ValueError", "(", "'expected non-negative integer p'", ")", "\n", "", "p", "=", "int", "(", "p", ")", "\n", "if", "len", "(", "A", ".", "shape", ")", "!=", "2", "or", "A", ".", "shape", "[", "0", "]", "!=", "A", ".", "shape", "[", "1", "]", ":", "\n", "        ", "raise", "ValueError", "(", "'expected A to be like a square matrix'", ")", "\n", "\n", "# Explicitly make a column vector so that this works when A is a", "\n", "# numpy matrix (in addition to ndarray and sparse matrix).", "\n", "", "v", "=", "torch", ".", "ones", "(", "(", "A", ".", "shape", "[", "0", "]", ",", "1", ")", ",", "dtype", "=", "A", ".", "dtype", ",", "device", "=", "A", ".", "device", ")", "\n", "M", "=", "A", ".", "t", "(", ")", "\n", "for", "_", "in", "range", "(", "p", ")", ":", "\n", "        ", "v", "=", "M", ".", "mm", "(", "v", ")", "\n", "", "return", "torch", ".", "max", "(", "v", ")", ".", "item", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.exprnn.expm32._onenorm": [[55, 57], ["torch.norm().item", "torch.norm"], "function", ["None"], ["", "def", "_onenorm", "(", "A", ")", ":", "\n", "    ", "return", "torch", ".", "norm", "(", "A", ",", "1", ")", ".", "item", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.exprnn.expm32._ident_like": [[59, 61], ["torch.eye"], "function", ["None"], ["", "def", "_ident_like", "(", "A", ")", ":", "\n", "    ", "return", "torch", ".", "eye", "(", "A", ".", "shape", "[", "0", "]", ",", "A", ".", "shape", "[", "1", "]", ",", "dtype", "=", "A", ".", "dtype", ",", "device", "=", "A", ".", "device", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.exprnn.expm32.expm32": [[191, 220], ["expm32._expm"], "function", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.exprnn.expm32._expm"], ["", "", "def", "expm32", "(", "A", ")", ":", "\n", "    ", "\"\"\"\n    Compute the matrix exponential using Pade approximation.\n\n    Parameters\n    ----------\n    A : (M,M) array_like or sparse matrix\n        2D Array or Matrix (sparse or dense) to be exponentiated\n\n    Returns\n    -------\n    expA : (M,M) ndarray\n        Matrix exponential of `A`\n\n    Notes\n    -----\n    This is algorithm (6.1) which is a simplification of algorithm (5.1).\n\n    .. versionadded:: 0.12.0\n\n    References\n    ----------\n    .. [1] Awad H. Al-Mohy and Nicholas J. Higham (2009)\n           \"A New Scaling and Squaring Algorithm for the Matrix Exponential.\"\n           SIAM Journal on Matrix Analysis and Applications.\n           31 (3). pp. 970-989. ISSN 1095-7162\n\n    \"\"\"", "\n", "return", "_expm", "(", "A", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.exprnn.expm32._expm": [[222, 259], ["expm32._ExpmPadeHelper", "max", "max", "max", "max", "expm32._ell", "expm32._ExpmPadeHelper.pade7_scaled", "expm32._solve_P_Q", "torch.matrix_power", "ValueError", "torch.exp", "expm32._ExpmPadeHelper.pade3", "expm32._solve_P_Q", "expm32._ExpmPadeHelper.pade5", "expm32._solve_P_Q", "int", "len", "expm32._ell", "expm32._ell", "numpy.ceil", "numpy.log2"], "function", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.exprnn.expm32._ell", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.exprnn.expm32._ExpmPadeHelper.pade7_scaled", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.exprnn.expm32._solve_P_Q", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.exprnn.expm32._ExpmPadeHelper.pade3", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.exprnn.expm32._solve_P_Q", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.exprnn.expm32._ExpmPadeHelper.pade5", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.exprnn.expm32._solve_P_Q", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.exprnn.expm32._ell", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.exprnn.expm32._ell"], ["", "def", "_expm", "(", "A", ")", ":", "\n", "# Core of expm, separated to allow testing exact and approximate", "\n", "# algorithms.", "\n", "\n", "# Avoid indiscriminate asarray() to allow sparse or other strange arrays.", "\n", "    ", "if", "len", "(", "A", ".", "shape", ")", "!=", "2", "or", "A", ".", "shape", "[", "0", "]", "!=", "A", ".", "shape", "[", "1", "]", ":", "\n", "        ", "raise", "ValueError", "(", "'expected a square matrix'", ")", "\n", "\n", "# Trivial case", "\n", "", "if", "A", ".", "shape", "==", "(", "1", ",", "1", ")", ":", "\n", "        ", "return", "torch", ".", "exp", "(", "A", ")", "\n", "\n", "# Track functions of A to help compute the matrix exponential.", "\n", "", "h", "=", "_ExpmPadeHelper", "(", "A", ")", "\n", "\n", "# Try Pade order 3.", "\n", "eta_1", "=", "max", "(", "h", ".", "d4_loose", ",", "h", ".", "d6_loose", ")", "\n", "theta3", "=", "4.2587300348979312e-001", "\n", "if", "eta_1", "<", "theta3", "and", "_ell", "(", "h", ".", "A", ",", "3", ")", "==", "0", ":", "\n", "        ", "U", ",", "V", "=", "h", ".", "pade3", "(", ")", "\n", "return", "_solve_P_Q", "(", "U", ",", "V", ")", "\n", "\n", "# Try Pade order 5.", "\n", "", "eta_2", "=", "max", "(", "h", ".", "d4_tight", ",", "h", ".", "d6_loose", ")", "\n", "theta5", "=", "1.8801526985337688e+000", "\n", "if", "eta_2", "<", "theta5", "and", "_ell", "(", "h", ".", "A", ",", "5", ")", "==", "0", ":", "\n", "        ", "U", ",", "V", "=", "h", ".", "pade5", "(", ")", "\n", "return", "_solve_P_Q", "(", "U", ",", "V", ")", "\n", "\n", "", "theta_7", "=", "3.9257248464332842e+000", "\n", "eta_3", "=", "max", "(", "h", ".", "d6_tight", ",", "h", ".", "d8_loose", ")", "\n", "s", "=", "max", "(", "int", "(", "np", ".", "ceil", "(", "np", ".", "log2", "(", "eta_3", "/", "theta_7", ")", ")", ")", ",", "0", ")", "\n", "\n", "s", "+=", "_ell", "(", "2", "**", "-", "s", "*", "h", ".", "A", ",", "7", ")", "\n", "U", ",", "V", "=", "h", ".", "pade7_scaled", "(", "s", ")", "\n", "X", "=", "_solve_P_Q", "(", "U", ",", "V", ")", "\n", "return", "torch", ".", "matrix_power", "(", "X", ",", "2", "**", "s", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.exprnn.expm32._solve_P_Q": [[261, 265], ["torch.solve"], "function", ["None"], ["", "def", "_solve_P_Q", "(", "U", ",", "V", ")", ":", "\n", "    ", "P", "=", "U", "+", "V", "\n", "Q", "=", "-", "U", "+", "V", "\n", "return", "torch", ".", "solve", "(", "P", ",", "Q", ")", "[", "0", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.exprnn.expm32._ell": [[267, 307], ["scipy.special.comb", "float", "expm32._onenorm_matrix_power_nnm", "max", "ValueError", "abs", "int", "len", "math.factorial", "expm32._onenorm", "numpy.ceil", "numpy.log2"], "function", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.exprnn.expm32._onenorm_matrix_power_nnm", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.exprnn.expm32._onenorm"], ["", "def", "_ell", "(", "A", ",", "m", ")", ":", "\n", "    ", "\"\"\"\n    A helper function for expm_2009.\n\n    Parameters\n    ----------\n    A : linear operator\n        A linear operator whose norm of power we care about.\n    m : int\n        The power of the linear operator\n\n    Returns\n    -------\n    value : int\n        A value related to a bound.\n\n    \"\"\"", "\n", "if", "len", "(", "A", ".", "shape", ")", "!=", "2", "or", "A", ".", "shape", "[", "0", "]", "!=", "A", ".", "shape", "[", "1", "]", ":", "\n", "        ", "raise", "ValueError", "(", "'expected A to be like a square matrix'", ")", "\n", "\n", "", "p", "=", "2", "*", "m", "+", "1", "\n", "\n", "# The c_i are explained in (2.2) and (2.6) of the 2005 expm paper.", "\n", "# They are coefficients of terms of a generating function series expansion.", "\n", "choose_2p_p", "=", "scipy", ".", "special", ".", "comb", "(", "2", "*", "p", ",", "p", ",", "exact", "=", "True", ")", "\n", "abs_c_recip", "=", "float", "(", "choose_2p_p", "*", "math", ".", "factorial", "(", "2", "*", "p", "+", "1", ")", ")", "\n", "\n", "# This is explained after Eq. (1.2) of the 2009 expm paper.", "\n", "# It is the \"unit roundoff\" of IEEE double precision arithmetic.", "\n", "u", "=", "2.", "**", "-", "24", "\n", "\n", "# Compute the one-norm of matrix power p of abs(A).", "\n", "A_abs_onenorm", "=", "_onenorm_matrix_power_nnm", "(", "abs", "(", "A", ")", ",", "p", ")", "\n", "\n", "# Treat zero norm as a special case.", "\n", "if", "not", "A_abs_onenorm", ":", "\n", "        ", "return", "0", "\n", "\n", "", "alpha", "=", "A_abs_onenorm", "/", "(", "_onenorm", "(", "A", ")", "*", "abs_c_recip", ")", "\n", "return", "max", "(", "int", "(", "np", ".", "ceil", "(", "np", ".", "log2", "(", "alpha", "/", "u", ")", "/", "(", "2", "*", "m", ")", ")", ")", ",", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.exprnn.expm32.differential": [[308, 316], ["A.size", "torch.zeros", "f"], "function", ["None"], ["", "def", "differential", "(", "f", ",", "A", ",", "E", ")", ":", "\n", "    ", "\"\"\" Computes the differential of f at A when acting on E:  (df)_A(E) \"\"\"", "\n", "n", "=", "A", ".", "size", "(", "0", ")", "\n", "M", "=", "torch", ".", "zeros", "(", "2", "*", "n", ",", "2", "*", "n", ",", "dtype", "=", "A", ".", "dtype", ",", "device", "=", "A", ".", "device", ",", "requires_grad", "=", "False", ")", "\n", "M", "[", ":", "n", ",", ":", "n", "]", "=", "A", "\n", "M", "[", "n", ":", ",", "n", ":", "]", "=", "A", "\n", "M", "[", ":", "n", ",", "n", ":", "]", "=", "E", "\n", "return", "f", "(", "M", ")", "[", ":", "n", ",", "n", ":", "]", "\n", "", ""]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.exprnn.parametrization.Parametrization.__init__": [[34, 67], ["torch.Module.__init__", "torch.Parameter", "torch.Parameter", "parametrization.Parametrization.register_buffer", "parametrization.Parametrization.register_buffer", "parametrization.Parametrization.A.register_hook", "isinstance", "len"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__"], ["def", "__init__", "(", "self", ",", "A", ",", "base", ",", "mode", ")", ":", "\n", "        ", "\"\"\"\n        mode: \"static\" or a tuple such that:\n                mode[0] == \"dynamic\"\n                mode[1]: int, K, the number of steps after which we should change the basis of the dyn triv\n                mode[2]: int, M, the number of changes of basis after which we should project back onto the manifold the basis. This is particularly helpful for small values of K.\n        \"\"\"", "\n", "super", "(", "Parametrization", ",", "self", ")", ".", "__init__", "(", ")", "\n", "assert", "mode", "==", "\"static\"", "or", "(", "isinstance", "(", "mode", ",", "tuple", ")", "and", "len", "(", "mode", ")", "==", "3", "and", "mode", "[", "0", "]", "==", "\"dynamic\"", ")", "\n", "\n", "self", ".", "A", "=", "nn", ".", "Parameter", "(", "A", ")", "\n", "self", ".", "register_buffer", "(", "\"_B\"", ",", "None", ")", "\n", "self", ".", "register_buffer", "(", "'base'", ",", "base", ")", "\n", "# This is necessary, as it will be generated again the first time that self.B is called", "\n", "# We still need to register the buffer though", "\n", "\n", "if", "mode", "==", "\"static\"", ":", "\n", "            ", "self", ".", "mode", "=", "mode", "\n", "", "else", ":", "\n", "            ", "self", ".", "mode", "=", "mode", "[", "0", "]", "\n", "self", ".", "K", "=", "mode", "[", "1", "]", "\n", "self", ".", "M", "=", "mode", "[", "2", "]", "\n", "self", ".", "k", "=", "0", "\n", "self", ".", "m", "=", "0", "\n", "\n", "# This implements the parametrization trick in a rather slick way.", "\n", "# We put a hook on A, such that, whenever its gradients are computed, we", "\n", "#  get rid of self._B so that it has to be recomputed the next time that", "\n", "#  self.B is accessed", "\n", "", "def", "hook", "(", "grad", ")", ":", "\n", "            ", "nonlocal", "self", "\n", "self", ".", "_B", "=", "None", "\n", "", "self", ".", "A", ".", "register_hook", "(", "hook", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.exprnn.parametrization.Parametrization.rebase": [[69, 73], ["torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "parametrization.Parametrization.base.data.copy_", "parametrization.Parametrization.A.data.zero_"], "methods", ["None"], ["", "def", "rebase", "(", "self", ")", ":", "\n", "        ", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "self", ".", "base", ".", "data", ".", "copy_", "(", "self", ".", "_B", ".", "data", ")", "\n", "self", ".", "A", ".", "data", ".", "zero_", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.exprnn.parametrization.Parametrization.B": [[74, 105], ["parametrization.Parametrization.retraction", "parametrization.Parametrization._B.requires_grad_", "parametrization.Parametrization._B.retain_grad", "torch.is_grad_enabled", "torch.is_grad_enabled", "torch.is_grad_enabled", "torch.is_grad_enabled", "parametrization.Parametrization.rebase", "hasattr", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "parametrization.Parametrization.project"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.exprnn.orthogonal.Orthogonal.retraction", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.exprnn.parametrization.Parametrization.rebase", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.exprnn.orthogonal.Orthogonal.project"], ["", "", "@", "property", "\n", "def", "B", "(", "self", ")", ":", "\n", "        ", "not_B", "=", "self", ".", "_B", "is", "None", "\n", "if", "not_B", "or", "(", "not", "self", ".", "_B", ".", "grad_fn", "and", "torch", ".", "is_grad_enabled", "(", ")", ")", ":", "\n", "            ", "self", ".", "_B", "=", "self", ".", "retraction", "(", "self", ".", "A", ",", "self", ".", "base", ")", "\n", "# Just to be safe", "\n", "self", ".", "_B", ".", "requires_grad_", "(", ")", "\n", "# Now self._B it's not a leaf tensor, so we convert it into a leaf", "\n", "self", ".", "_B", ".", "retain_grad", "(", ")", "\n", "\n", "# Increment the counters for the dyntriv algorithm if we have generated B", "\n", "if", "self", ".", "mode", "==", "\"dynamic\"", "and", "not_B", ":", "\n", "                ", "if", "self", ".", "k", "==", "0", ":", "\n", "                    ", "self", ".", "rebase", "(", ")", "\n", "# Project the base back to the manifold every M changes of base", "\n", "# Increment the counter before as we don't project the first time", "\n", "self", ".", "m", "=", "(", "self", ".", "m", "+", "1", ")", "%", "self", ".", "M", "\n", "# It's optional to implement this method", "\n", "if", "self", ".", "m", "==", "0", "and", "hasattr", "(", "self", ",", "\"project\"", ")", ":", "\n", "                        ", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                            ", "self", ".", "base", "=", "self", ".", "project", "(", "self", ".", "base", ")", "\n", "# Change the basis after K optimization steps", "\n", "# Increment the counter afterwards as we change the basis in the first iteration", "\n", "", "", "", "if", "self", ".", "K", "!=", "\"infty\"", ":", "\n", "                    ", "self", ".", "k", "=", "(", "self", ".", "k", "+", "1", ")", "%", "self", ".", "K", "\n", "", "else", ":", "\n", "# Make sure that we just update the base once", "\n", "                    ", "if", "self", ".", "k", "==", "0", ":", "\n", "                        ", "self", ".", "k", "=", "1", "\n", "\n", "", "", "", "", "return", "self", ".", "_B", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.exprnn.parametrization.Parametrization.retraction": [[106, 115], ["None"], "methods", ["None"], ["", "def", "retraction", "(", "self", ",", "A", ",", "base", ")", ":", "\n", "        ", "\"\"\"\n        It computes r_{base}(A).\n        Notice that A will not always be in the tangent space of our manifold\n          For this reason, we first have to use A to parametrize the tangent space,\n          and then compute the retraction\n        When dealing with Lie groups, raw_A is always projected into the Lie algebra, as an optimization (cf. Section E in the paper)\n        \"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.exprnn.parametrization.Parametrization.project": [[116, 122], ["None"], "methods", ["None"], ["", "def", "project", "(", "self", ",", "base", ")", ":", "\n", "        ", "\"\"\"\n        This method is OPTIONAL\n        It returns the projected base back into the manifold\n        \"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.exprnn.parametrization.Parametrization.forward": [[123, 128], ["None"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input", ")", ":", "\n", "        ", "\"\"\"\n        It uses the attribute self.B to implement the layer itself (e.g. Linear, CNN, ...)\n        \"\"\"", "\n", "raise", "NotImplementedError", "\n", "", "", ""]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.exprnn.parametrization.get_parameters": [[7, 21], ["model.apply", "isinstance", "all", "parametrized_params.append", "model.parameters", "parametrization.get_parameters.not_in"], "function", ["None"], ["def", "get_parameters", "(", "model", ")", ":", "\n", "    ", "parametrized_params", "=", "[", "]", "\n", "\n", "def", "get_parametrized_params", "(", "mod", ")", ":", "\n", "        ", "nonlocal", "parametrized_params", "\n", "if", "isinstance", "(", "mod", ",", "Parametrization", ")", ":", "\n", "            ", "parametrized_params", ".", "append", "(", "mod", ".", "A", ")", "\n", "\n", "", "", "def", "not_in", "(", "elem", ",", "l", ")", ":", "\n", "        ", "return", "all", "(", "elem", "is", "not", "x", "for", "x", "in", "l", ")", "\n", "\n", "", "model", ".", "apply", "(", "get_parametrized_params", ")", "\n", "unconstrained_params", "=", "(", "param", "for", "param", "in", "model", ".", "parameters", "(", ")", "if", "not_in", "(", "param", ",", "parametrized_params", ")", ")", "\n", "return", "unconstrained_params", ",", "parametrized_params", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.exprnn.orthogonal.Orthogonal.__init__": [[11, 31], ["max", "torch.empty", "torch.empty", "torch.empty", "torch.empty", "torch.empty", "torch.empty", "torch.empty", "torch.empty", "parametrization.Parametrization.__init__", "orthogonal.Orthogonal.reset_parameters"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.cells.memory.MemoryCell.reset_parameters"], ["    ", "def", "__init__", "(", "self", ",", "d_input", ",", "d_output", ",", "method", "=", "'dtriv'", ",", "init", "=", "'cayley'", ",", "K", "=", "100", ")", ":", "\n", "        ", "\"\"\" Wrapper around expRNN's Orthogonal class taking care of parameter names \"\"\"", "\n", "if", "method", "==", "\"exprnn\"", ":", "\n", "            ", "mode", "=", "\"static\"", "\n", "param", "=", "'expm'", "\n", "", "elif", "method", "==", "\"dtriv\"", ":", "\n", "# We use 100 as the default to project back to the manifold.", "\n", "# This parameter does not really affect the convergence of the algorithms, even for K=1", "\n", "            ", "mode", "=", "(", "\"dynamic\"", ",", "K", ",", "100", ")", "# TODO maybe K=30? check exprnn codebase", "\n", "param", "=", "'expm'", "\n", "", "elif", "method", "==", "\"cayley\"", ":", "\n", "            ", "mode", "=", "\"static\"", "\n", "param", "=", "'cayley'", "\n", "", "else", ":", "\n", "            ", "assert", "False", ",", "f\"OrthogonalLinear: orthogonal method {method} not supported\"", "\n", "\n", "", "param", "=", "param_name_to_param", "[", "param", "]", "\n", "init_A", "=", "init_name_to_init", "[", "init", "]", "\n", "super", "(", ")", ".", "__init__", "(", "d_input", ",", "d_output", ",", "init_A", ",", "mode", ",", "param", ")", "\n", "\n", "# Scale LR by factor of 10", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.exprnn.orthogonal.Orthogonal.reset_parameters": [[32, 35], ["orthogonal.Orthogonal.init_A", "orthogonal.Orthogonal.init_base"], "methods", ["None"], ["self", ".", "A", ".", "_lr_scale", "=", "0.1", "\n", "", "", ""]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.exprnn.orthogonal.Orthogonal.forward": [[36, 38], ["input.matmul"], "methods", ["None"], []], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.exprnn.orthogonal.Orthogonal.retraction": [[39, 47], ["A.triu.triu.triu", "base.mm", "A.triu.triu.t", "orthogonal.Orthogonal.param"], "methods", ["None"], []], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.exprnn.orthogonal.Orthogonal.project": [[48, 62], ["torch.svd", "torch.svd", "torch.svd", "torch.svd", "U.mm", "V.t", "base.size", "base.size", "base.t", "torch.qr", "torch.qr", "torch.qr", "torch.qr", "base.size", "base.size", "ret.t.t.t"], "methods", ["None"], []], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.exprnn.orthogonal.modrelu.__init__": [[65, 71], ["torch.Module.__init__", "torch.Parameter", "torch.Parameter", "orthogonal.modrelu.reset_parameters", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.cells.memory.MemoryCell.reset_parameters"], []], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.exprnn.orthogonal.modrelu.reset_parameters": [[72, 74], ["orthogonal.modrelu.b.data.uniform_"], "methods", ["None"], []], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.exprnn.orthogonal.modrelu.forward": [[75, 82], ["torch.abs", "torch.abs", "torch.abs", "torch.abs", "torch.functional.relu", "torch.functional.relu", "torch.sign", "torch.sign", "torch.sign", "torch.sign"], "methods", ["None"], []], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.exprnn.orthogonal.OrthogonalRNN.__init__": [[85, 94], ["torch.Module.__init__", "orthogonal.Orthogonal", "torch.Linear", "torch.Linear", "orthogonal.modrelu", "orthogonal.OrthogonalRNN.reset_parameters"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.cells.memory.MemoryCell.reset_parameters"], []], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.exprnn.orthogonal.OrthogonalRNN.reset_parameters": [[95, 97], ["torch.init.kaiming_normal_", "torch.init.kaiming_normal_"], "methods", ["None"], []], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.exprnn.orthogonal.OrthogonalRNN.default_hidden": [[98, 100], ["input.new_zeros", "input.size"], "methods", ["None"], []], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.exprnn.orthogonal.OrthogonalRNN.forward": [[101, 108], ["orthogonal.OrthogonalRNN.input_kernel", "orthogonal.OrthogonalRNN.recurrent_kernel", "orthogonal.OrthogonalRNN.nonlinearity"], "methods", ["None"], []], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.cauchy._broadcast_dims": [[31, 35], ["max", "tensor.view", "len", "len"], "function", ["None"], ["def", "_broadcast_dims", "(", "*", "tensors", ")", ":", "\n", "    ", "max_dim", "=", "max", "(", "[", "len", "(", "tensor", ".", "shape", ")", "for", "tensor", "in", "tensors", "]", ")", "\n", "tensors", "=", "[", "tensor", ".", "view", "(", "(", "1", ",", ")", "*", "(", "max_dim", "-", "len", "(", "tensor", ".", "shape", ")", ")", "+", "tensor", ".", "shape", ")", "for", "tensor", "in", "tensors", "]", "\n", "return", "tensors", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.cauchy._c2r": [[36, 37], ["torch.view_as_real"], "function", ["None"], ["", "def", "_c2r", "(", "x", ")", ":", "return", "torch", ".", "view_as_real", "(", "x", ")", "\n", "def", "_r2c", "(", "x", ")", ":", "return", "torch", ".", "view_as_complex", "(", "x", ")", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.cauchy._r2c": [[37, 38], ["torch.view_as_complex"], "function", ["None"], ["def", "_r2c", "(", "x", ")", ":", "return", "torch", ".", "view_as_complex", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.cauchy.cauchy_slow": [[39, 51], ["torch.sum", "_conj", "_conj", "_conj.unsqueeze", "z.unsqueeze", "_conj.unsqueeze"], "function", ["None"], ["def", "cauchy_slow", "(", "v", ",", "z", ",", "w", ",", "conj", "=", "True", ")", ":", "\n", "    ", "\"\"\"\n    v: (..., N)\n    z: (..., L)\n    w: (..., N)\n    returns: (..., L) \\sum v/(z-w)\n    \"\"\"", "\n", "if", "conj", ":", "\n", "        ", "v", "=", "_conj", "(", "v", ")", "\n", "w", "=", "_conj", "(", "w", ")", "\n", "", "cauchy_matrix", "=", "v", ".", "unsqueeze", "(", "-", "1", ")", "/", "(", "z", ".", "unsqueeze", "(", "-", "2", ")", "-", "w", ".", "unsqueeze", "(", "-", "1", ")", ")", "# (... N L)", "\n", "return", "torch", ".", "sum", "(", "cauchy_matrix", ",", "dim", "=", "-", "2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.cauchy.cauchy_lazy": [[52, 64], ["cauchy._broadcast_dims", "LazyTensor", "LazyTensor", "LazyTensor", "div.sum", "div.sum.squeeze", "_conj", "_conj", "einops.rearrange", "einops.rearrange", "einops.rearrange", "len"], "function", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.s4._broadcast_dims"], ["", "def", "cauchy_lazy", "(", "v", ",", "z", ",", "w", ",", "conj", "=", "True", ")", ":", "\n", "    ", "if", "conj", ":", "\n", "        ", "v", "=", "_conj", "(", "v", ")", "\n", "w", "=", "_conj", "(", "w", ")", "\n", "", "v", ",", "z", ",", "w", "=", "_broadcast_dims", "(", "v", ",", "z", ",", "w", ")", "\n", "v_l", "=", "LazyTensor", "(", "rearrange", "(", "v", ",", "'... N -> ... N 1 1'", ")", ")", "\n", "w_l", "=", "LazyTensor", "(", "rearrange", "(", "w", ",", "'... N -> ... N 1 1'", ")", ")", "\n", "z_l", "=", "LazyTensor", "(", "rearrange", "(", "z", ",", "'... L -> ... 1 L 1'", ")", ")", "\n", "sub", "=", "z_l", "-", "w_l", "# (b N L 1), for some reason it doesn't display the last dimension", "\n", "div", "=", "v_l", "/", "sub", "\n", "s", "=", "div", ".", "sum", "(", "dim", "=", "len", "(", "v_l", ".", "shape", ")", "-", "2", ")", "\n", "return", "s", ".", "squeeze", "(", "-", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.cauchy.cauchy": [[65, 88], ["Genred", "cauchy._broadcast_dims", "cauchy._c2r", "cauchy._c2r", "cauchy._c2r", "Genred.", "cauchy._r2c", "_conj", "_conj"], "function", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.s4._broadcast_dims", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.cauchy._c2r", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.cauchy._c2r", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.cauchy._c2r", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.cauchy._r2c"], ["", "def", "cauchy", "(", "v", ",", "z", ",", "w", ",", "conj", "=", "False", ")", ":", "\n", "    ", "expr", "=", "'ComplexDivide(v, z-w)'", "\n", "cauchy_mult", "=", "Genred", "(", "\n", "expr", ",", "\n", "[", "\n", "'v = Vj(2)'", ",", "\n", "'z = Vi(2)'", ",", "\n", "'w = Vj(2)'", ",", "\n", "]", ",", "\n", "reduction_op", "=", "'Sum'", ",", "\n", "axis", "=", "1", ",", "\n", ")", "\n", "\n", "if", "conj", ":", "\n", "        ", "v", "=", "_conj", "(", "v", ")", "\n", "w", "=", "_conj", "(", "w", ")", "\n", "", "v", ",", "z", ",", "w", "=", "_broadcast_dims", "(", "v", ",", "z", ",", "w", ")", "\n", "v", "=", "_c2r", "(", "v", ")", "\n", "z", "=", "_c2r", "(", "z", ")", "\n", "w", "=", "_c2r", "(", "w", ")", "\n", "\n", "r", "=", "cauchy_mult", "(", "v", ",", "z", ",", "w", ",", "backend", "=", "'GPU'", ")", "\n", "return", "_r2c", "(", "r", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.cauchy.cauchy_real": [[89, 108], ["Genred", "cauchy._broadcast_dims", "v.unsqueeze.unsqueeze", "z.unsqueeze.unsqueeze", "w.unsqueeze.unsqueeze", "Genred."], "function", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.s4._broadcast_dims"], ["", "def", "cauchy_real", "(", "v", ",", "z", ",", "w", ")", ":", "\n", "    ", "expr", "=", "'v / (z - w)'", "\n", "cauchy_mult", "=", "Genred", "(", "\n", "expr", ",", "\n", "[", "\n", "'v = Vj(1)'", ",", "\n", "'z = Vi(1)'", ",", "\n", "'w = Vj(1)'", ",", "\n", "]", ",", "\n", "reduction_op", "=", "'Sum'", ",", "\n", "axis", "=", "1", ",", "\n", ")", "\n", "v", ",", "z", ",", "w", "=", "_broadcast_dims", "(", "v", ",", "z", ",", "w", ")", "\n", "v", "=", "v", ".", "unsqueeze", "(", "-", "1", ")", "\n", "z", "=", "z", ".", "unsqueeze", "(", "-", "1", ")", "\n", "w", "=", "w", ".", "unsqueeze", "(", "-", "1", ")", "\n", "\n", "r", "=", "cauchy_mult", "(", "v", ",", "z", ",", "w", ",", "backend", "=", "'GPU'", ")", "\n", "return", "r", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.cauchy.cauchy_conj": [[110, 141], ["Genred", "cauchy._broadcast_dims", "cauchy._c2r", "cauchy._c2r", "cauchy._c2r", "cauchy._r2c", "Genred."], "function", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.s4._broadcast_dims", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.cauchy._c2r", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.cauchy._c2r", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.cauchy._c2r", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.cauchy._r2c"], ["", "def", "cauchy_conj", "(", "v", ",", "z", ",", "w", ",", "num", "=", "2", ",", "denom", "=", "2", ")", ":", "\n", "    ", "if", "num", "==", "1", ":", "\n", "        ", "expr_num", "=", "'z * ComplexReal(v) - Real2Complex(ComplexReal(v)*ComplexReal(w) + ComplexImag(v)*ComplexImag(w))'", "\n", "", "elif", "num", "==", "2", ":", "\n", "        ", "expr_num", "=", "'z * ComplexReal(v) - Real2Complex(Sum(v * w))'", "\n", "", "else", ":", "raise", "NotImplementedError", "\n", "\n", "if", "denom", "==", "1", ":", "\n", "        ", "expr_denom", "=", "'ComplexMult(z-Real2Complex(ComplexReal(w)), z-Real2Complex(ComplexReal(w))) + Real2Complex(Square(ComplexImag(w)))'", "\n", "", "elif", "denom", "==", "2", ":", "\n", "        ", "expr_denom", "=", "'ComplexMult(z-w, z-Conj(w))'", "\n", "", "else", ":", "raise", "NotImplementedError", "\n", "\n", "cauchy_mult", "=", "Genred", "(", "\n", "f'ComplexDivide({expr_num}, {expr_denom})'", ",", "\n", "[", "\n", "'v = Vj(2)'", ",", "\n", "'z = Vi(2)'", ",", "\n", "'w = Vj(2)'", ",", "\n", "]", ",", "\n", "reduction_op", "=", "'Sum'", ",", "\n", "axis", "=", "1", ",", "\n", ")", "\n", "\n", "v", ",", "z", ",", "w", "=", "_broadcast_dims", "(", "v", ",", "z", ",", "w", ")", "\n", "v", "=", "_c2r", "(", "v", ")", "\n", "z", "=", "_c2r", "(", "z", ")", "\n", "w", "=", "_c2r", "(", "w", ")", "\n", "\n", "r", "=", "2", "*", "cauchy_mult", "(", "v", ",", "z", ",", "w", ",", "backend", "=", "'GPU'", ")", "\n", "return", "_r2c", "(", "r", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.cauchy.cauchy_conj_components": [[142, 171], ["Genred", "cauchy._broadcast_dims", "v.unsqueeze.unsqueeze", "z.unsqueeze.unsqueeze", "w.unsqueeze.unsqueeze", "z.unsqueeze.imag.contiguous", "cauchy._r2c", "v.unsqueeze.real.contiguous", "v.unsqueeze.imag.contiguous", "w.unsqueeze.real.contiguous", "w.unsqueeze.imag.contiguous", "Genred."], "function", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.s4._broadcast_dims", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.cauchy._r2c"], ["", "def", "cauchy_conj_components", "(", "v", ",", "z", ",", "w", ")", ":", "\n", "    ", "\"\"\" Assumes z is pure imaginary (as in S4 with bilinear) \"\"\"", "\n", "\n", "expr_num", "=", "'Imag2Complex(zi*vr) - Real2Complex(vr*wr + vi*wi)'", "\n", "expr_denom", "=", "'Real2Complex(Square(wr)+Square(wi)-Square(zi)) - Imag2Complex(IntCst(2)*zi*wr)'", "\n", "cauchy_mult", "=", "Genred", "(", "\n", "f'ComplexDivide({expr_num}, {expr_denom})'", ",", "\n", "[", "\n", "'vr = Vj(1)'", ",", "\n", "'vi = Vj(1)'", ",", "\n", "'wr = Vj(1)'", ",", "\n", "'wi = Vj(1)'", ",", "\n", "'zi = Vi(1)'", ",", "\n", "]", ",", "\n", "reduction_op", "=", "'Sum'", ",", "\n", "axis", "=", "1", ",", "\n", ")", "\n", "\n", "v", ",", "z", ",", "w", "=", "_broadcast_dims", "(", "v", ",", "z", ",", "w", ")", "\n", "v", "=", "v", ".", "unsqueeze", "(", "-", "1", ")", "\n", "z", "=", "z", ".", "unsqueeze", "(", "-", "1", ")", "\n", "w", "=", "w", ".", "unsqueeze", "(", "-", "1", ")", "\n", "\n", "v_r", ",", "v_i", "=", "v", ".", "real", ".", "contiguous", "(", ")", ",", "v", ".", "imag", ".", "contiguous", "(", ")", "\n", "w_r", ",", "w_i", "=", "w", ".", "real", ".", "contiguous", "(", ")", ",", "w", ".", "imag", ".", "contiguous", "(", ")", "\n", "z_i", "=", "z", ".", "imag", ".", "contiguous", "(", ")", "\n", "\n", "r", "=", "2", "*", "cauchy_mult", "(", "v_r", ",", "v_i", ",", "w_r", ",", "w_i", ",", "z_i", ",", "backend", "=", "'GPU'", ")", "\n", "return", "_r2c", "(", "r", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.cauchy.cauchy_conj_components_lazy": [[172, 197], ["cauchy._broadcast_dims", "z.imag.contiguous", "LazyTensor", "LazyTensor", "LazyTensor", "LazyTensor", "LazyTensor", "r.squeeze", "v.real.contiguous", "v.imag.contiguous", "w.real.contiguous", "w.imag.contiguous", "einops.rearrange", "einops.rearrange", "einops.rearrange", "einops.rearrange", "einops.rearrange", "r.sum", "len"], "function", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.s4._broadcast_dims"], ["", "def", "cauchy_conj_components_lazy", "(", "v", ",", "z", ",", "w", ",", "type", "=", "1", ")", ":", "\n", "    ", "v", ",", "z", ",", "w", "=", "_broadcast_dims", "(", "v", ",", "z", ",", "w", ")", "\n", "\n", "v_r", ",", "v_i", "=", "v", ".", "real", ".", "contiguous", "(", ")", ",", "v", ".", "imag", ".", "contiguous", "(", ")", "\n", "w_r", ",", "w_i", "=", "w", ".", "real", ".", "contiguous", "(", ")", ",", "w", ".", "imag", ".", "contiguous", "(", ")", "\n", "z_i", "=", "z", ".", "imag", ".", "contiguous", "(", ")", "\n", "\n", "v_r", "=", "LazyTensor", "(", "rearrange", "(", "v_r", ",", "'... N -> ... 1 N 1'", ")", ")", "\n", "v_i", "=", "LazyTensor", "(", "rearrange", "(", "v_i", ",", "'... N -> ... 1 N 1'", ")", ")", "\n", "w_r", "=", "LazyTensor", "(", "rearrange", "(", "w_r", ",", "'... N -> ... 1 N 1'", ")", ")", "\n", "w_i", "=", "LazyTensor", "(", "rearrange", "(", "w_i", ",", "'... N -> ... 1 N 1'", ")", ")", "\n", "z_i", "=", "LazyTensor", "(", "rearrange", "(", "z_i", ",", "'... L -> ... L 1 1'", ")", ")", "\n", "\n", "if", "type", "==", "1", ":", "\n", "        ", "num", "=", "-", "v_r", "*", "w_r", "-", "v_i", "*", "w_i", "+", "1j", "*", "z_i", "*", "v_r", "\n", "denom", "=", "w_r", "**", "2", "+", "w_i", "**", "2", "-", "z_i", "**", "2", "-", "2j", "*", "w_r", "*", "z_i", "\n", "", "else", ":", "\n", "# z = torch.complex(-w_r, z_i) # Not supported", "\n", "        ", "z", "=", "-", "w_r", "+", "1j", "*", "z_i", "\n", "num", "=", "v_r", "*", "z", "-", "v_i", "*", "w_i", "\n", "denom", "=", "z", "*", "z", "+", "w_i", "**", "2", "# z**2 is bugged for complex", "\n", "\n", "", "r", "=", "num", "/", "denom", "\n", "r", "=", "2", "*", "r", ".", "sum", "(", "dim", "=", "len", "(", "z_i", ".", "shape", ")", "-", "1", ")", "\n", "return", "r", ".", "squeeze", "(", "-", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.cauchy.cauchy_conj2": [[198, 220], ["Genred", "cauchy._broadcast_dims", "Genred.", "cauchy._r2c", "cauchy._c2r", "cauchy._c2r", "cauchy._c2r"], "function", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.s4._broadcast_dims", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.cauchy._r2c", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.cauchy._c2r", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.cauchy._c2r", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.cauchy._c2r"], ["", "def", "cauchy_conj2", "(", "v", ",", "z", ",", "w", ")", ":", "\n", "    ", "expr", "=", "'ComplexDivide(v, z-w) + ComplexDivide(Conj(v), z-Conj(w))'", "\n", "# expr = 'ComplexDivide(v, z-w)'", "\n", "cauchy_mult", "=", "Genred", "(", "\n", "expr", ",", "\n", "[", "\n", "'v = Vj(2)'", ",", "\n", "'z = Vi(2)'", ",", "\n", "'w = Vj(2)'", ",", "\n", "]", ",", "\n", "reduction_op", "=", "'Sum'", ",", "\n", "axis", "=", "1", ",", "\n", ")", "\n", "\n", "v", ",", "z", ",", "w", "=", "_broadcast_dims", "(", "v", ",", "z", ",", "w", ")", "\n", "if", "complex", ":", "\n", "        ", "v", "=", "_c2r", "(", "v", ")", "\n", "z", "=", "_c2r", "(", "z", ")", "\n", "w", "=", "_c2r", "(", "w", ")", "\n", "\n", "", "r", "=", "cauchy_mult", "(", "v", ",", "z", ",", "w", ",", "backend", "=", "'GPU'", ")", "\n", "return", "_r2c", "(", "r", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.cauchy.trigger_compilation": [[222, 237], ["torch.randn", "torch.randn", "torch.randn", "cauchy.cauchy_conj"], "function", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.cauchy.cauchy_conj"], ["", "def", "trigger_compilation", "(", ")", ":", "\n", "    ", "\"\"\" Small function to trigger the compilation of a pykeops kernel\n\n    Used in scenarios where we must manually control compilation, e.g. the multi-gpu case (https://github.com/getkeops/keops/issues/168) \"\"\"", "\n", "B", "=", "2", "\n", "N", "=", "4", "\n", "L", "=", "16", "\n", "\n", "w", "=", "torch", ".", "randn", "(", "B", ",", "N", "//", "2", ",", "dtype", "=", "torch", ".", "cfloat", ",", "device", "=", "'cuda'", ")", "\n", "v", "=", "torch", ".", "randn", "(", "B", ",", "N", "//", "2", ",", "dtype", "=", "torch", ".", "cfloat", ",", "device", "=", "'cuda'", ")", "\n", "z", "=", "torch", ".", "randn", "(", "B", ",", "L", ",", "dtype", "=", "torch", ".", "cfloat", ",", "device", "=", "'cuda'", ")", "\n", "w", ".", "requires_grad", "=", "True", "\n", "v", ".", "requires_grad", "=", "True", "\n", "\n", "cauchy_conj", "(", "v", ",", "z", ",", "w", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.toeplitz.TriangularToeplitzMult.forward": [[59, 63], ["ctx.save_for_backward", "toeplitz.triangular_toeplitz_multiply_"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.toeplitz.triangular_toeplitz_multiply_"], ["    ", "@", "staticmethod", "\n", "def", "forward", "(", "ctx", ",", "u", ",", "v", ")", ":", "\n", "        ", "ctx", ".", "save_for_backward", "(", "u", ",", "v", ")", "\n", "return", "triangular_toeplitz_multiply_", "(", "u", ",", "v", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.toeplitz.TriangularToeplitzMult.backward": [[64, 70], ["triangular_toeplitz_multiply_().flip", "triangular_toeplitz_multiply_().flip", "toeplitz.triangular_toeplitz_multiply_", "toeplitz.triangular_toeplitz_multiply_", "grad.flip", "grad.flip"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.toeplitz.triangular_toeplitz_multiply_", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.toeplitz.triangular_toeplitz_multiply_"], ["", "@", "staticmethod", "\n", "def", "backward", "(", "ctx", ",", "grad", ")", ":", "\n", "        ", "u", ",", "v", "=", "ctx", ".", "saved_tensors", "\n", "d_u", "=", "triangular_toeplitz_multiply_", "(", "grad", ".", "flip", "(", "-", "1", ")", ",", "v", ")", ".", "flip", "(", "-", "1", ")", "\n", "d_v", "=", "triangular_toeplitz_multiply_", "(", "grad", ".", "flip", "(", "-", "1", ")", ",", "u", ")", ".", "flip", "(", "-", "1", ")", "\n", "return", "d_u", ",", "d_v", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.toeplitz.TriangularToeplitzMultFast.forward": [[72, 85], ["torch.pad", "torch.pad", "torch.pad", "torch.pad", "torch.fft.rfft", "torch.fft.rfft", "torch.fft.rfft", "torch.fft.rfft", "torch.fft.rfft", "torch.fft.rfft", "torch.fft.rfft", "torch.fft.rfft", "ctx.save_for_backward", "torch.fft.irfft", "torch.fft.irfft", "torch.fft.irfft", "torch.fft.irfft"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.sc.pad", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.sc.pad", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.sc.pad", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.sc.pad"], ["    ", "@", "staticmethod", "\n", "def", "forward", "(", "ctx", ",", "u", ",", "v", ")", ":", "\n", "        ", "n", "=", "u", ".", "shape", "[", "-", "1", "]", "\n", "u_expand", "=", "F", ".", "pad", "(", "u", ",", "(", "0", ",", "n", ")", ")", "\n", "v_expand", "=", "F", ".", "pad", "(", "v", ",", "(", "0", ",", "n", ")", ")", "\n", "u_f", "=", "torch", ".", "fft", ".", "rfft", "(", "u_expand", ",", "n", "=", "2", "*", "n", ",", "dim", "=", "-", "1", ")", "\n", "v_f", "=", "torch", ".", "fft", ".", "rfft", "(", "v_expand", ",", "n", "=", "2", "*", "n", ",", "dim", "=", "-", "1", ")", "\n", "\n", "ctx", ".", "save_for_backward", "(", "u_f", ",", "v_f", ")", "\n", "\n", "uv_f", "=", "u_f", "*", "v_f", "\n", "output", "=", "torch", ".", "fft", ".", "irfft", "(", "uv_f", ",", "n", "=", "2", "*", "n", ",", "dim", "=", "-", "1", ")", "[", "...", ",", ":", "n", "]", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.toeplitz.TriangularToeplitzMultFast.backward": [[86, 99], ["torch.pad", "torch.pad", "torch.fft.rfft", "torch.fft.rfft", "torch.fft.rfft", "torch.fft.rfft", "d_u.flip.flip.flip", "d_v.flip.flip.flip", "grad.flip", "torch.fft.irfft", "torch.fft.irfft", "torch.fft.irfft", "torch.fft.irfft", "torch.fft.irfft", "torch.fft.irfft", "torch.fft.irfft", "torch.fft.irfft"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.sc.pad", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.sc.pad"], ["", "@", "staticmethod", "\n", "def", "backward", "(", "ctx", ",", "grad", ")", ":", "\n", "        ", "u_f", ",", "v_f", "=", "ctx", ".", "saved_tensors", "\n", "n", "=", "grad", ".", "shape", "[", "-", "1", "]", "\n", "g_expand", "=", "F", ".", "pad", "(", "grad", ".", "flip", "(", "-", "1", ")", ",", "(", "0", ",", "n", ")", ")", "\n", "g_f", "=", "torch", ".", "fft", ".", "rfft", "(", "g_expand", ",", "n", "=", "2", "*", "n", ",", "dim", "=", "-", "1", ")", "\n", "gu_f", "=", "g_f", "*", "u_f", "\n", "gv_f", "=", "g_f", "*", "v_f", "\n", "d_u", "=", "torch", ".", "fft", ".", "irfft", "(", "gv_f", ",", "n", "=", "2", "*", "n", ",", "dim", "=", "-", "1", ")", "[", "...", ",", ":", "n", "]", "\n", "d_v", "=", "torch", ".", "fft", ".", "irfft", "(", "gu_f", ",", "n", "=", "2", "*", "n", ",", "dim", "=", "-", "1", ")", "[", "...", ",", ":", "n", "]", "\n", "d_u", "=", "d_u", ".", "flip", "(", "-", "1", ")", "\n", "d_v", "=", "d_v", ".", "flip", "(", "-", "1", ")", "\n", "return", "d_u", ",", "d_v", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.toeplitz.TriangularToeplitzMultPadded.forward": [[101, 106], ["ctx.save_for_backward", "toeplitz.triangular_toeplitz_multiply_"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.toeplitz.triangular_toeplitz_multiply_"], ["    ", "@", "staticmethod", "\n", "def", "forward", "(", "ctx", ",", "u", ",", "v", ")", ":", "\n", "        ", "ctx", ".", "save_for_backward", "(", "u", ",", "v", ")", "\n", "output", "=", "triangular_toeplitz_multiply_", "(", "u", ",", "v", ")", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.toeplitz.TriangularToeplitzMultPadded.backward": [[107, 113], ["triangular_toeplitz_multiply_padded_().flip", "triangular_toeplitz_multiply_padded_().flip", "toeplitz.triangular_toeplitz_multiply_padded_", "toeplitz.triangular_toeplitz_multiply_padded_", "grad.flip", "grad.flip"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.toeplitz.triangular_toeplitz_multiply_padded_", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.toeplitz.triangular_toeplitz_multiply_padded_"], ["", "@", "staticmethod", "\n", "def", "backward", "(", "ctx", ",", "grad", ")", ":", "\n", "        ", "u", ",", "v", "=", "ctx", ".", "saved_tensors", "\n", "d_u", "=", "triangular_toeplitz_multiply_padded_", "(", "grad", ".", "flip", "(", "-", "1", ")", ",", "v", ")", ".", "flip", "(", "-", "1", ")", "\n", "d_v", "=", "triangular_toeplitz_multiply_padded_", "(", "grad", ".", "flip", "(", "-", "1", ")", ",", "u", ")", ".", "flip", "(", "-", "1", ")", "\n", "return", "d_u", ",", "d_v", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.toeplitz.TriangularToeplitzMultPaddedFast.forward": [[117, 129], ["torch.fft.rfft", "torch.fft.rfft", "torch.fft.rfft", "torch.fft.rfft", "torch.fft.rfft", "torch.fft.rfft", "torch.fft.rfft", "torch.fft.rfft", "ctx.save_for_backward", "torch.fft.irfft", "torch.fft.irfft", "torch.fft.irfft", "torch.fft.irfft", "output[].zero_"], "methods", ["None"], ["@", "staticmethod", "\n", "def", "forward", "(", "ctx", ",", "u", ",", "v", ")", ":", "\n", "        ", "n", "=", "u", ".", "shape", "[", "-", "1", "]", "\n", "u_f", "=", "torch", ".", "fft", ".", "rfft", "(", "u", ",", "n", "=", "n", ",", "dim", "=", "-", "1", ")", "\n", "v_f", "=", "torch", ".", "fft", ".", "rfft", "(", "v", ",", "n", "=", "n", ",", "dim", "=", "-", "1", ")", "\n", "\n", "ctx", ".", "save_for_backward", "(", "u_f", ",", "v_f", ")", "\n", "\n", "uv_f", "=", "u_f", "*", "v_f", "\n", "output", "=", "torch", ".", "fft", ".", "irfft", "(", "uv_f", ",", "n", "=", "n", ",", "dim", "=", "-", "1", ")", "\n", "output", "[", "...", ",", "n", "//", "2", ":", "]", ".", "zero_", "(", ")", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.toeplitz.TriangularToeplitzMultPaddedFast.backward": [[130, 145], ["torch.pad", "torch.pad", "torch.fft.rfft", "torch.fft.rfft", "torch.fft.rfft", "torch.fft.rfft", "torch.fft.irfft", "torch.fft.irfft", "torch.fft.irfft", "torch.fft.irfft", "torch.fft.irfft", "torch.fft.irfft", "torch.fft.irfft", "torch.fft.irfft", "d_u[].zero_", "d_v[].zero_", "d_u[].flip", "d_v[].flip", "grad[].flip"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.sc.pad", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.sc.pad"], ["", "@", "staticmethod", "\n", "def", "backward", "(", "ctx", ",", "grad", ")", ":", "\n", "        ", "u_f", ",", "v_f", "=", "ctx", ".", "saved_tensors", "\n", "n", "=", "grad", ".", "shape", "[", "-", "1", "]", "\n", "g_expand", "=", "F", ".", "pad", "(", "grad", "[", "...", ",", ":", "n", "//", "2", "]", ".", "flip", "(", "-", "1", ")", ",", "(", "0", ",", "n", "//", "2", ")", ")", "\n", "g_f", "=", "torch", ".", "fft", ".", "rfft", "(", "g_expand", ",", "n", "=", "n", ",", "dim", "=", "-", "1", ")", "\n", "gu_f", "=", "g_f", "*", "u_f", "\n", "gv_f", "=", "g_f", "*", "v_f", "\n", "d_u", "=", "torch", ".", "fft", ".", "irfft", "(", "gv_f", ",", "n", "=", "n", ",", "dim", "=", "-", "1", ")", "\n", "d_v", "=", "torch", ".", "fft", ".", "irfft", "(", "gu_f", ",", "n", "=", "n", ",", "dim", "=", "-", "1", ")", "\n", "d_u", "[", "...", ",", "n", "//", "2", ":", "]", ".", "zero_", "(", ")", "\n", "d_v", "[", "...", ",", "n", "//", "2", ":", "]", ".", "zero_", "(", ")", "\n", "d_u", "[", "...", ",", ":", "n", "//", "2", "]", "=", "d_u", "[", "...", ",", ":", "n", "//", "2", "]", ".", "flip", "(", "-", "1", ")", "# TODO", "\n", "d_v", "[", "...", ",", ":", "n", "//", "2", "]", "=", "d_v", "[", "...", ",", ":", "n", "//", "2", "]", ".", "flip", "(", "-", "1", ")", "# TODO", "\n", "return", "d_u", ",", "d_v", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.toeplitz.construct_toeplitz": [[17, 34], ["torch.arange", "torch.arange"], "function", ["None"], ["def", "construct_toeplitz", "(", "v", ",", "f", "=", "0.0", ")", ":", "\n", "    ", "\"\"\"Explicit construction of Krylov matrix [v  A @ v  A^2 @ v  ...  A^{n-1} @ v]\n    where A = Z_f. This uses vectorized indexing and cumprod so it's much\n    faster than using the Krylov function.\n    Parameters:\n        v: the starting vector of size n or (rank, n).\n        f: real number\n    Returns:\n        K: Krylov matrix of size (n, n) or (rank, n, n).\n    \"\"\"", "\n", "n", "=", "v", ".", "shape", "[", "-", "1", "]", "\n", "a", "=", "torch", ".", "arange", "(", "n", ",", "device", "=", "v", ".", "device", ")", "\n", "b", "=", "-", "a", "\n", "indices", "=", "a", "[", ":", ",", "None", "]", "+", "b", "[", "None", "]", "\n", "K", "=", "v", "[", "...", ",", "indices", "]", "\n", "K", "[", "...", ",", "indices", "<", "0", "]", "*=", "f", "\n", "return", "K", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.toeplitz.triangular_toeplitz_multiply_": [[35, 46], ["torch.pad", "torch.pad", "torch.fft.rfft", "torch.fft.rfft", "torch.fft.rfft", "torch.fft.rfft", "uv_f.sum.sum", "torch.fft.irfft", "torch.fft.irfft"], "function", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.sc.pad", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.sc.pad"], ["", "def", "triangular_toeplitz_multiply_", "(", "u", ",", "v", ",", "sum", "=", "None", ")", ":", "\n", "    ", "n", "=", "u", ".", "shape", "[", "-", "1", "]", "\n", "u_expand", "=", "F", ".", "pad", "(", "u", ",", "(", "0", ",", "n", ")", ")", "\n", "v_expand", "=", "F", ".", "pad", "(", "v", ",", "(", "0", ",", "n", ")", ")", "\n", "u_f", "=", "torch", ".", "fft", ".", "rfft", "(", "u_expand", ",", "n", "=", "2", "*", "n", ",", "dim", "=", "-", "1", ")", "\n", "v_f", "=", "torch", ".", "fft", ".", "rfft", "(", "v_expand", ",", "n", "=", "2", "*", "n", ",", "dim", "=", "-", "1", ")", "\n", "uv_f", "=", "u_f", "*", "v_f", "\n", "if", "sum", "is", "not", "None", ":", "\n", "        ", "uv_f", "=", "uv_f", ".", "sum", "(", "dim", "=", "sum", ")", "\n", "", "output", "=", "torch", ".", "fft", ".", "irfft", "(", "uv_f", ",", "n", "=", "2", "*", "n", ",", "dim", "=", "-", "1", ")", "[", "...", ",", ":", "n", "]", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.toeplitz.triangular_toeplitz_multiply_padded_": [[47, 57], ["torch.fft.rfft", "torch.fft.rfft", "torch.fft.rfft", "torch.fft.rfft", "torch.fft.irfft", "torch.fft.irfft"], "function", ["None"], ["", "def", "triangular_toeplitz_multiply_padded_", "(", "u", ",", "v", ")", ":", "\n", "    ", "\"\"\" Same as triangular_toeplitz_multiply but inputs and output assume to be 0-padded already. \"\"\"", "\n", "n", "=", "u", ".", "shape", "[", "-", "1", "]", "\n", "assert", "n", "%", "2", "==", "0", "\n", "u_f", "=", "torch", ".", "fft", ".", "rfft", "(", "u", ",", "n", "=", "n", ",", "dim", "=", "-", "1", ")", "\n", "v_f", "=", "torch", ".", "fft", ".", "rfft", "(", "v", ",", "n", "=", "n", ",", "dim", "=", "-", "1", ")", "\n", "uv_f", "=", "u_f", "*", "v_f", "\n", "output", "=", "torch", ".", "fft", ".", "irfft", "(", "uv_f", ",", "n", "=", "n", ",", "dim", "=", "-", "1", ")", "\n", "output", "[", "...", ",", "n", ":", "]", "=", "0", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.toeplitz.causal_convolution": [[152, 161], ["triangular_toeplitz_multiply", "triangular_toeplitz_multiply_fast", "triangular_toeplitz_multiply_padded", "triangular_toeplitz_multiply_padded_fast"], "function", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.lssl.triangular_toeplitz_multiply"], ["def", "causal_convolution", "(", "u", ",", "v", ",", "fast", "=", "True", ",", "pad", "=", "False", ")", ":", "\n", "    ", "if", "not", "pad", "and", "not", "fast", ":", "\n", "        ", "return", "triangular_toeplitz_multiply", "(", "u", ",", "v", ")", "\n", "", "if", "not", "pad", "and", "fast", ":", "\n", "        ", "return", "triangular_toeplitz_multiply_fast", "(", "u", ",", "v", ")", "\n", "", "if", "pad", "and", "not", "fast", ":", "\n", "        ", "return", "triangular_toeplitz_multiply_padded", "(", "u", ",", "v", ")", "\n", "", "if", "pad", "and", "fast", ":", "\n", "        ", "return", "triangular_toeplitz_multiply_padded_fast", "(", "u", ",", "v", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.toeplitz._fft": [[162, 163], ["torch.fft.rfft", "torch.fft.rfft", "torch.pad"], "function", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.sc.pad"], ["", "", "def", "_fft", "(", "x", ",", "N", ")", ":", "return", "torch", ".", "fft", ".", "rfft", "(", "F", ".", "pad", "(", "x", ",", "(", "0", ",", "2", "*", "N", "-", "x", ".", "shape", "[", "-", "1", "]", ")", ")", ",", "n", "=", "2", "*", "N", ",", "dim", "=", "-", "1", ")", "\n", "def", "_ifft", "(", "x", ",", "N", ")", ":", "return", "torch", ".", "fft", ".", "irfft", "(", "x", ",", "n", "=", "2", "*", "N", ",", "dim", "=", "-", "1", ")", "[", "...", ",", ":", "N", "]", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.toeplitz._ifft": [[163, 164], ["torch.fft.irfft", "torch.fft.irfft"], "function", ["None"], ["def", "_ifft", "(", "x", ",", "N", ")", ":", "return", "torch", ".", "fft", ".", "irfft", "(", "x", ",", "n", "=", "2", "*", "N", ",", "dim", "=", "-", "1", ")", "[", "...", ",", ":", "N", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.toeplitz.causal_convolution_inverse": [[165, 188], ["u[].reciprocal", "toeplitz._fft", "toeplitz._fft", "toeplitz._ifft"], "function", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.toeplitz._fft", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.toeplitz._fft", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.toeplitz._ifft"], ["def", "causal_convolution_inverse", "(", "u", ")", ":", "\n", "    ", "\"\"\" Invert the causal convolution/polynomial/triangular Toeplitz matrix represented by u.\n\n    This is easiest in the polynomial view:\n    https://www.csa.iisc.ac.in/~chandan/courses/CNT/notes/lec5.pdf\n    The idea is that\n    h = g^{-1} (mod x^m) => 2h - gh^2 = g^{-1} (mod x^{2m})\n\n    # TODO this can be numerically unstable if input is \"poorly conditioned\",\n    # for example if u[0] is magnitudes different from the rest of u\n    \"\"\"", "\n", "N", "=", "u", ".", "shape", "[", "-", "1", "]", "\n", "v", "=", "u", "[", "...", ",", ":", "1", "]", ".", "reciprocal", "(", ")", "\n", "while", "v", ".", "shape", "[", "-", "1", "]", "<", "N", ":", "\n", "        ", "M", "=", "v", ".", "shape", "[", "-", "1", "]", "\n", "v_f", "=", "_fft", "(", "v", ",", "2", "*", "M", ")", "\n", "u_f", "=", "_fft", "(", "u", "[", "...", ",", ":", "2", "*", "M", "]", ",", "2", "*", "M", ")", "\n", "_v", "=", "-", "_ifft", "(", "u_f", "*", "v_f", "**", "2", ",", "2", "*", "M", ")", "\n", "_v", "[", "...", ",", ":", "M", "]", "=", "_v", "[", "...", ",", ":", "M", "]", "+", "2", "*", "v", "\n", "v", "=", "_v", "\n", "# TODO contiguous?", "\n", "", "v", "=", "v", "[", "...", ",", ":", "N", "]", "\n", "return", "v", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.toeplitz.causal_convolution_inverse_wrong": [[191, 201], ["torch.pad", "torch.pad", "torch.fft.rfft", "torch.fft.rfft", "torch.fft.rfft", "torch.fft.rfft", "torch.fft.irfft", "torch.fft.irfft"], "function", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.sc.pad", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.sc.pad"], ["def", "causal_convolution_inverse_wrong", "(", "u", ",", "v", ")", ":", "\n", "    ", "\"\"\" Solve u * x = v. Initial attempt by inverting the multiplication algorithm, which I think doesn't work. \"\"\"", "\n", "n", "=", "u", ".", "shape", "[", "-", "1", "]", "\n", "u_expand", "=", "F", ".", "pad", "(", "u", ",", "(", "0", ",", "n", ")", ")", "\n", "v_expand", "=", "F", ".", "pad", "(", "v", ",", "(", "0", ",", "n", ")", ")", "\n", "u_f", "=", "torch", ".", "fft", ".", "rfft", "(", "u_expand", ",", "n", "=", "2", "*", "n", ",", "dim", "=", "-", "1", ")", "\n", "v_f", "=", "torch", ".", "fft", ".", "rfft", "(", "v_expand", ",", "n", "=", "2", "*", "n", ",", "dim", "=", "-", "1", ")", "\n", "uv_f", "=", "v_f", "/", "u_f", "\n", "x", "=", "torch", ".", "fft", ".", "irfft", "(", "uv_f", ",", "n", "=", "2", "*", "n", ",", "dim", "=", "-", "1", ")", "[", "...", ",", ":", "n", "]", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.toeplitz.construct_toeplitz_log": [[202, 210], ["torch.arange", "torch.arange"], "function", ["None"], ["", "def", "construct_toeplitz_log", "(", "v", ")", ":", "\n", "    ", "n", "=", "v", ".", "shape", "[", "-", "1", "]", "\n", "a", "=", "torch", ".", "arange", "(", "n", ",", "device", "=", "v", ".", "device", ")", "\n", "b", "=", "-", "a", "\n", "indices", "=", "a", "[", ":", ",", "None", "]", "+", "b", "[", "None", "]", "\n", "K", "=", "v", "[", "...", ",", "indices", "]", "\n", "K", "[", "...", ",", "indices", "<", "0", "]", "=", "-", "100.0", "\n", "return", "K", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.toeplitz._logsumexp": [[211, 218], ["torch.log", "torch.log", "torch.max", "torch.max", "torch.sum", "torch.sum", "m.squeeze", "torch.real", "torch.real", "torch.exp", "torch.exp"], "function", ["None"], ["", "def", "_logsumexp", "(", "x", ",", "dim", "=", "-", "1", ")", ":", "\n", "    ", "\"\"\" logsumexp for complex \"\"\"", "\n", "m", "=", "torch", ".", "max", "(", "torch", ".", "real", "(", "x", ")", ",", "dim", "=", "dim", ",", "keepdim", "=", "True", ")", "[", "0", "]", "\n", "x", "=", "x", "-", "m", "\n", "x", "=", "torch", ".", "log", "(", "torch", ".", "sum", "(", "torch", ".", "exp", "(", "x", ")", ",", "dim", "=", "dim", ")", ")", "\n", "x", "=", "x", "+", "m", ".", "squeeze", "(", "dim", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.toeplitz.causal_convolution_inverse_log": [[219, 249], ["toeplitz._logsumexp", "print", "torch.pad", "toeplitz.construct_toeplitz_log", "toeplitz._logsumexp", "toeplitz._logsumexp", "toeplitz._logsumexp", "torch.exp", "torch.exp", "torch.pad", "torch.log", "torch.log", "torch.log", "torch.log", "torch.stack", "torch.stack", "toeplitz.construct_toeplitz_log", "torch.pad", "torch.ones_like", "torch.ones_like", "torch.ones_like", "torch.ones_like"], "function", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.toeplitz._logsumexp", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.sc.pad", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.toeplitz.construct_toeplitz_log", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.toeplitz._logsumexp", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.toeplitz._logsumexp", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.toeplitz._logsumexp", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.sc.pad", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.toeplitz.construct_toeplitz_log", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.sc.pad"], ["", "def", "causal_convolution_inverse_log", "(", "u", ",", "N", "=", "-", "1", ")", ":", "\n", "    ", "\"\"\" Invert the causal convolution/polynomial/triangular Toeplitz matrix represented by u.\n\n    This is easiest in the polynomial view:\n    https://www.csa.iisc.ac.in/~chandan/courses/CNT/notes/lec5.pdf\n    The idea is that\n    h = g^{-1} (mod x^m) => 2h - gh^2 = g^{-1} (mod x^{2m})\n\n    # TODO this can be numerically unstable if input is \"poorly conditioned\",\n    # for example if u[0] is magnitudes different from the rest of u\n    \"\"\"", "\n", "if", "N", "<", "0", ":", "\n", "        ", "N", "=", "u", ".", "shape", "[", "-", "1", "]", "\n", "", "v", "=", "-", "u", "[", "...", ",", ":", "1", "]", "\n", "while", "v", ".", "shape", "[", "-", "1", "]", "<", "N", ":", "\n", "        ", "M", "=", "v", ".", "shape", "[", "-", "1", "]", "\n", "_v", "=", "F", ".", "pad", "(", "v", ",", "(", "0", ",", "M", ")", ",", "value", "=", "-", "100.0", ")", "\n", "_v_", "=", "construct_toeplitz_log", "(", "_v", ")", "\n", "u_", "=", "u", "[", "...", ",", ":", "2", "*", "M", "]", "if", "u", ".", "shape", "[", "-", "1", "]", ">=", "2", "*", "M", "else", "F", ".", "pad", "(", "u", ",", "(", "0", ",", "2", "*", "M", "-", "u", ".", "shape", "[", "-", "1", "]", ")", ",", "value", "=", "-", "100.0", ")", "\n", "_u", "=", "_logsumexp", "(", "_v_", "+", "u_", ",", "dim", "=", "-", "1", ")", "\n", "_u", "=", "_logsumexp", "(", "_v_", "+", "_u", ",", "dim", "=", "-", "1", ")", "\n", "_u", "=", "_u", "+", "torch", ".", "log", "(", "-", "torch", ".", "ones_like", "(", "_u", ")", ")", "\n", "_v", "=", "_v", "+", "torch", ".", "log", "(", "2.0", "*", "torch", ".", "ones_like", "(", "_u", ")", ")", "\n", "v", "=", "_logsumexp", "(", "torch", ".", "stack", "(", "[", "_v", ",", "_u", "]", ",", "dim", "=", "-", "1", ")", ",", "dim", "=", "-", "1", ")", "\n", "# TODO contiguous?", "\n", "", "v", "=", "v", "[", "...", ",", ":", "N", "]", "\n", "\n", "check", "=", "_logsumexp", "(", "construct_toeplitz_log", "(", "v", ")", "+", "F", ".", "pad", "(", "u", ",", "(", "0", ",", "N", "-", "u", ".", "shape", "[", "-", "1", "]", ")", ",", "value", "=", "-", "100.0", ")", ")", "\n", "print", "(", "\"check\"", ",", "check", ",", "torch", ".", "exp", "(", "check", ")", ")", "\n", "return", "v", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.complex.Conjugate.forward": [[63, 74], ["torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "complex.cupy2torch", "complex.conjugate_torch", "numpy.ascontiguousarray().view().conj().view", "torch2cupy().view().conj().view", "numpy.ascontiguousarray().view().conj", "torch2cupy().view().conj", "numpy.ascontiguousarray().view", "torch2cupy().view", "numpy.ascontiguousarray", "complex.torch2cupy", "complex.torch2numpy"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.complex.cupy2torch", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.complex.conjugate_torch", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.complex.torch2cupy", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.complex.torch2numpy"], ["@", "staticmethod", "\n", "def", "forward", "(", "ctx", ",", "X", ")", ":", "\n", "        ", "assert", "X", ".", "shape", "[", "-", "1", "]", "==", "2", ",", "'Last dimension must be 2'", "\n", "if", "X", ".", "is_cuda", ":", "\n", "            ", "if", "use_cupy", ":", "\n", "# TODO: do we need .contiguous here? I think it doesn't work if the last dimension isn't contiguous", "\n", "                ", "return", "cupy2torch", "(", "torch2cupy", "(", "X", ")", ".", "view", "(", "'complex64'", ")", ".", "conj", "(", ")", ".", "view", "(", "'float32'", ")", ")", "\n", "", "else", ":", "\n", "                ", "return", "conjugate_torch", "(", "X", ")", "\n", "", "", "else", ":", "\n", "            ", "return", "torch", ".", "from_numpy", "(", "np", ".", "ascontiguousarray", "(", "torch2numpy", "(", "X", ")", ")", ".", "view", "(", "'complex64'", ")", ".", "conj", "(", ")", ".", "view", "(", "'float32'", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.complex.Conjugate.backward": [[75, 78], ["Conjugate.apply"], "methods", ["None"], ["", "", "@", "staticmethod", "\n", "def", "backward", "(", "ctx", ",", "grad", ")", ":", "\n", "        ", "return", "Conjugate", ".", "apply", "(", "grad", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.complex.ComplexMul.forward": [[101, 117], ["ctx.save_for_backward", "numpy.ascontiguousarray().view", "numpy.ascontiguousarray().view", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "complex.cupy2torch", "complex.complex_mul_torch", "numpy.ascontiguousarray", "numpy.ascontiguousarray", "complex.torch2numpy", "complex.torch2numpy", "torch2cupy().view", "torch2cupy().view", "complex.torch2cupy", "complex.torch2cupy"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.complex.cupy2torch", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.complex.complex_mul_torch", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.complex.torch2numpy", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.complex.torch2numpy", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.complex.torch2cupy", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.complex.torch2cupy"], ["@", "staticmethod", "\n", "def", "forward", "(", "ctx", ",", "X", ",", "Y", ")", ":", "\n", "        ", "assert", "X", ".", "shape", "[", "-", "1", "]", "==", "2", "and", "Y", ".", "shape", "[", "-", "1", "]", "==", "2", ",", "'Last dimension must be 2'", "\n", "ctx", ".", "save_for_backward", "(", "X", ",", "Y", ")", "\n", "if", "X", ".", "is_cuda", ":", "\n", "            ", "assert", "Y", ".", "is_cuda", ",", "'X and Y must both be torch.cuda.FloatTensor'", "\n", "if", "use_cupy", ":", "\n", "# TODO: do we need .contiguous here? I think it doesn't work if the last dimension isn't contiguous", "\n", "                ", "return", "cupy2torch", "(", "(", "torch2cupy", "(", "X", ")", ".", "view", "(", "'complex64'", ")", "*", "torch2cupy", "(", "Y", ")", ".", "view", "(", "'complex64'", ")", ")", ".", "view", "(", "'float32'", ")", ")", "\n", "", "else", ":", "\n", "                ", "return", "complex_mul_torch", "(", "X", ",", "Y", ")", "\n", "", "", "else", ":", "\n", "            ", "assert", "not", "Y", ".", "is_cuda", ",", "'X and Y must both be torch.FloatTensor'", "\n", "X_np", "=", "np", ".", "ascontiguousarray", "(", "torch2numpy", "(", "X", ")", ")", ".", "view", "(", "'complex64'", ")", "\n", "Y_np", "=", "np", ".", "ascontiguousarray", "(", "torch2numpy", "(", "Y", ")", ")", ".", "view", "(", "'complex64'", ")", "\n", "return", "torch", ".", "from_numpy", "(", "(", "X_np", "*", "Y_np", ")", ".", "view", "(", "'float32'", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.complex.ComplexMul.backward": [[118, 141], ["ComplexMul.apply().sum_to_size", "ComplexMul.apply().sum_to_size", "ComplexMul.apply", "ComplexMul.apply", "conjugate", "conjugate"], "methods", ["None"], ["", "", "@", "staticmethod", "\n", "def", "backward", "(", "ctx", ",", "grad", ")", ":", "\n", "        ", "X", ",", "Y", "=", "ctx", ".", "saved_tensors", "\n", "grad_X", ",", "grad_Y", "=", "None", ",", "None", "\n", "if", "ctx", ".", "needs_input_grad", "[", "0", "]", ":", "\n", "            ", "grad_X", "=", "ComplexMul", ".", "apply", "(", "grad", ",", "conjugate", "(", "Y", ")", ")", ".", "sum_to_size", "(", "*", "X", ".", "shape", ")", "\n", "", "if", "ctx", ".", "needs_input_grad", "[", "1", "]", ":", "\n", "            ", "grad_Y", "=", "ComplexMul", ".", "apply", "(", "grad", ",", "conjugate", "(", "X", ")", ")", ".", "sum_to_size", "(", "*", "Y", ".", "shape", ")", "\n", "# grad_X, grad_Y = ComplexMul.apply(grad, conjugate(Y)), ComplexMul.apply(grad, conjugate(X))", "\n", "# # Need to sum over dimensions that were broadcasted", "\n", "# grad_X = grad_X.sum_to_size(*X.shape)", "\n", "# grad_Y = grad_Y.sum_to_size(*Y.shape)", "\n", "# dims_to_sum_X = [-i for i in range(1, X.dim() + 1) if X.shape[-i] != grad.shape[-i]]", "\n", "# dims_to_sum_Y = [-i for i in range(1, Y.dim() + 1) if Y.shape[-i] != grad.shape[-i]]", "\n", "# if dims_to_sum_X:  # If empty list is passed to sum, it sums all the dimensions", "\n", "#     grad_X = grad_X.sum(dim=dims_to_sum_X, keepdim=True)", "\n", "# if dims_to_sum_Y:  # If empty list is passed to sum, it sums all the dimensions", "\n", "#     grad_Y = grad_Y.sum(dim=dims_to_sum_Y, keepdim=True)", "\n", "# if grad.dim() > X.dim():", "\n", "#     grad_X = grad_X.sum(tuple(range(grad.dim() - X.dim())))", "\n", "# if grad.dim() > Y.dim():", "\n", "#     grad_Y = grad_Y.sum(tuple(range(grad.dim() - Y.dim())))", "\n", "", "return", "grad_X", ",", "grad_Y", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.complex.complex_mul_native": [[23, 25], ["torch.view_as_real", "torch.view_as_real", "torch.view_as_real", "torch.view_as_complex", "torch.view_as_complex", "torch.view_as_complex", "torch.view_as_complex", "torch.view_as_complex", "torch.view_as_complex"], "function", ["None"], ["def", "complex_mul_native", "(", "X", ",", "Y", ")", ":", "\n", "    ", "return", "torch", ".", "view_as_real", "(", "torch", ".", "view_as_complex", "(", "X", ")", "*", "torch", ".", "view_as_complex", "(", "Y", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.complex.conjugate_native": [[27, 29], ["torch.view_as_real", "torch.view_as_real", "torch.view_as_real", "torch.view_as_complex().conj", "torch.view_as_complex().conj", "torch.view_as_complex().conj", "torch.view_as_complex", "torch.view_as_complex", "torch.view_as_complex"], "function", ["None"], ["", "def", "conjugate_native", "(", "X", ")", ":", "\n", "    ", "return", "torch", ".", "view_as_real", "(", "torch", ".", "view_as_complex", "(", "X", ")", ".", "conj", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.complex.torch2numpy": [[31, 35], ["X.detach().numpy", "X.detach"], "function", ["None"], ["", "def", "torch2numpy", "(", "X", ")", ":", "\n", "    ", "\"\"\"Convert a torch float32 tensor to a numpy array, sharing the same memory.\n    \"\"\"", "\n", "return", "X", ".", "detach", "(", ")", ".", "numpy", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.complex.torch2cupy": [[37, 39], ["cp.fromDlpack", "src.torch.utils.dlpack.to_dlpack", "tensor.cuda"], "function", ["None"], ["", "def", "torch2cupy", "(", "tensor", ")", ":", "\n", "    ", "return", "cp", ".", "fromDlpack", "(", "to_dlpack", "(", "tensor", ".", "cuda", "(", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.complex.cupy2torch": [[41, 43], ["src.torch.utils.dlpack.from_dlpack", "tensor.toDlpack"], "function", ["None"], ["", "def", "cupy2torch", "(", "tensor", ")", ":", "\n", "    ", "return", "from_dlpack", "(", "tensor", ".", "toDlpack", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.complex.real_to_complex": [[45, 53], ["torch.stack", "torch.stack", "torch.stack", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like"], "function", ["None"], ["", "def", "real_to_complex", "(", "X", ")", ":", "\n", "    ", "\"\"\"A version of X that's complex (i.e., last dimension is 2).\n    Parameters:\n        X: (...) tensor\n    Return:\n        X_complex: (..., 2) tensor\n    \"\"\"", "\n", "return", "torch", ".", "stack", "(", "(", "X", ",", "torch", ".", "zeros_like", "(", "X", ")", ")", ",", "dim", "=", "-", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.complex.conjugate_torch": [[55, 58], ["torch.tensor", "torch.tensor", "torch.tensor"], "function", ["None"], ["", "def", "conjugate_torch", "(", "X", ")", ":", "\n", "    ", "assert", "X", ".", "shape", "[", "-", "1", "]", "==", "2", ",", "'Last dimension must be 2'", "\n", "return", "X", "*", "torch", ".", "tensor", "(", "(", "1", ",", "-", "1", ")", ",", "dtype", "=", "X", ".", "dtype", ",", "device", "=", "X", ".", "device", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.complex.complex_mul_torch": [[83, 89], ["torch.stack", "torch.stack", "torch.stack"], "function", ["None"], ["def", "complex_mul_torch", "(", "X", ",", "Y", ")", ":", "\n", "    ", "assert", "X", ".", "shape", "[", "-", "1", "]", "==", "2", "and", "Y", ".", "shape", "[", "-", "1", "]", "==", "2", ",", "'Last dimension must be 2'", "\n", "return", "torch", ".", "stack", "(", "\n", "(", "X", "[", "...", ",", "0", "]", "*", "Y", "[", "...", ",", "0", "]", "-", "X", "[", "...", ",", "1", "]", "*", "Y", "[", "...", ",", "1", "]", ",", "\n", "X", "[", "...", ",", "0", "]", "*", "Y", "[", "...", ",", "1", "]", "+", "X", "[", "...", ",", "1", "]", "*", "Y", "[", "...", ",", "0", "]", ")", ",", "\n", "dim", "=", "-", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.complex.complex_mul_numpy": [[91, 96], ["numpy.ascontiguousarray().view", "numpy.ascontiguousarray().view", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "numpy.ascontiguousarray", "numpy.ascontiguousarray", "complex.torch2numpy", "complex.torch2numpy"], "function", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.complex.torch2numpy", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.complex.torch2numpy"], ["", "def", "complex_mul_numpy", "(", "X", ",", "Y", ")", ":", "\n", "    ", "assert", "X", ".", "shape", "[", "-", "1", "]", "==", "2", "and", "Y", ".", "shape", "[", "-", "1", "]", "==", "2", ",", "'Last dimension must be 2'", "\n", "X_np", "=", "np", ".", "ascontiguousarray", "(", "torch2numpy", "(", "X", ")", ")", ".", "view", "(", "'complex64'", ")", "\n", "Y_np", "=", "np", ".", "ascontiguousarray", "(", "torch2numpy", "(", "Y", ")", ")", ".", "view", "(", "'complex64'", ")", "\n", "return", "torch", ".", "from_numpy", "(", "(", "X_np", "*", "Y_np", ")", ".", "view", "(", "'float32'", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.unroll.shift_up": [[18, 26], ["torch.zeros_like.unsqueeze", "torch.cat", "torch.cat", "torch.cat", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like"], "function", ["None"], ["def", "shift_up", "(", "a", ",", "s", "=", "None", ",", "drop", "=", "True", ",", "dim", "=", "0", ")", ":", "\n", "    ", "assert", "dim", "==", "0", "\n", "if", "s", "is", "None", ":", "\n", "        ", "s", "=", "torch", ".", "zeros_like", "(", "a", "[", "0", ",", "...", "]", ")", "\n", "", "s", "=", "s", ".", "unsqueeze", "(", "dim", ")", "\n", "if", "drop", ":", "\n", "        ", "a", "=", "a", "[", ":", "-", "1", ",", "...", "]", "\n", "", "return", "torch", ".", "cat", "(", "(", "s", ",", "a", ")", ",", "dim", "=", "dim", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.unroll.interleave": [[27, 43], ["torch.stack", "torch.stack", "torch.stack", "list", "torch.cat.view", "torch.cat", "torch.cat", "torch.cat"], "function", ["None"], ["", "def", "interleave", "(", "a", ",", "b", ",", "uneven", "=", "False", ",", "dim", "=", "0", ")", ":", "\n", "    ", "\"\"\" Interleave two tensors of same shape \"\"\"", "\n", "# assert(a.shape == b.shape)", "\n", "assert", "dim", "==", "0", "# TODO temporary to make handling uneven case easier", "\n", "if", "dim", "<", "0", ":", "\n", "        ", "dim", "=", "N", "+", "dim", "\n", "", "if", "uneven", ":", "\n", "        ", "a_", "=", "a", "[", "-", "1", ":", ",", "...", "]", "\n", "a", "=", "a", "[", ":", "-", "1", ",", "...", "]", "\n", "", "c", "=", "torch", ".", "stack", "(", "(", "a", ",", "b", ")", ",", "dim", "+", "1", ")", "\n", "out_shape", "=", "list", "(", "a", ".", "shape", ")", "\n", "out_shape", "[", "dim", "]", "*=", "2", "\n", "c", "=", "c", ".", "view", "(", "out_shape", ")", "\n", "if", "uneven", ":", "\n", "        ", "c", "=", "torch", ".", "cat", "(", "(", "c", ",", "a_", ")", ",", "dim", "=", "dim", ")", "\n", "", "return", "c", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.unroll.batch_mult": [[44, 70], ["u.unsqueeze.permute", "u.unsqueeze.unsqueeze", "v.permute.permute", "len", "len", "list", "list", "range", "range", "len", "len", "len"], "function", ["None"], ["", "def", "batch_mult", "(", "A", ",", "u", ",", "has_batch", "=", "None", ")", ":", "\n", "    ", "\"\"\" Matrix mult A @ u with special case to save memory if u has additional batch dim\n\n    The batch dimension is assumed to be the second dimension\n    A : (L, ..., N, N)\n    u : (L, [B], ..., N)\n    has_batch: True, False, or None. If None, determined automatically\n\n    Output:\n    x : (L, [B], ..., N)\n      A @ u broadcasted appropriately\n    \"\"\"", "\n", "\n", "if", "has_batch", "is", "None", ":", "\n", "        ", "has_batch", "=", "len", "(", "u", ".", "shape", ")", ">=", "len", "(", "A", ".", "shape", ")", "\n", "\n", "", "if", "has_batch", ":", "\n", "        ", "u", "=", "u", ".", "permute", "(", "[", "0", "]", "+", "list", "(", "range", "(", "2", ",", "len", "(", "u", ".", "shape", ")", ")", ")", "+", "[", "1", "]", ")", "\n", "", "else", ":", "\n", "        ", "u", "=", "u", ".", "unsqueeze", "(", "-", "1", ")", "\n", "", "v", "=", "(", "A", "@", "u", ")", "\n", "if", "has_batch", ":", "\n", "        ", "v", "=", "v", ".", "permute", "(", "[", "0", "]", "+", "[", "len", "(", "u", ".", "shape", ")", "-", "1", "]", "+", "list", "(", "range", "(", "1", ",", "len", "(", "u", ".", "shape", ")", "-", "1", ")", ")", ")", "\n", "", "else", ":", "\n", "        ", "v", "=", "v", "[", "...", ",", "0", "]", "\n", "", "return", "v", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.unroll.unroll": [[75, 91], ["u.new_zeros", "torch.unbind", "torch.unbind", "torch.unbind", "torch.stack", "torch.stack", "torch.stack", "outputs.append", "torch.linear"], "function", ["None"], ["", "def", "unroll", "(", "A", ",", "u", ")", ":", "\n", "    ", "\"\"\"\n    A : (..., N, N) # TODO I think this can't take batch dimension?\n    u : (L, ..., N)\n    output : x (..., N) # TODO a lot of these shapes are wrong\n    x[i, ...] = A^{i} @ u[0, ...] + ... + A @ u[i-1, ...] + u[i, ...]\n    \"\"\"", "\n", "\n", "m", "=", "u", ".", "new_zeros", "(", "u", ".", "shape", "[", "1", ":", "]", ")", "\n", "outputs", "=", "[", "]", "\n", "for", "u_", "in", "torch", ".", "unbind", "(", "u", ",", "dim", "=", "0", ")", ":", "\n", "        ", "m", "=", "F", ".", "linear", "(", "m", ",", "A", ")", "+", "u_", "\n", "outputs", ".", "append", "(", "m", ")", "\n", "\n", "", "output", "=", "torch", ".", "stack", "(", "outputs", ",", "dim", "=", "0", ")", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.unroll.parallel_unroll_recursive": [[93, 122], ["int", "torch.cat", "torch.cat", "torch.cat", "unroll.parallel_unroll_recursive.parallel_unroll_recursive_"], "function", ["None"], ["", "def", "parallel_unroll_recursive", "(", "A", ",", "u", ")", ":", "\n", "    ", "\"\"\" Bottom-up divide-and-conquer version of unroll. \"\"\"", "\n", "\n", "# Main recursive function", "\n", "def", "parallel_unroll_recursive_", "(", "A", ",", "u", ")", ":", "\n", "        ", "if", "u", ".", "shape", "[", "0", "]", "==", "1", ":", "\n", "            ", "return", "u", "\n", "\n", "", "u_evens", "=", "u", "[", "0", ":", ":", "2", ",", "...", "]", "\n", "u_odds", "=", "u", "[", "1", ":", ":", "2", ",", "...", "]", "\n", "\n", "# u2 = F.linear(u_evens, A) + u_odds", "\n", "u2", "=", "(", "A", "@", "u_evens", ".", "unsqueeze", "(", "-", "1", ")", ")", ".", "squeeze", "(", "-", "1", ")", "+", "u_odds", "\n", "A2", "=", "A", "@", "A", "\n", "\n", "x_odds", "=", "parallel_unroll_recursive_", "(", "A2", ",", "u2", ")", "\n", "# x_evens = F.linear(shift_up(x_odds), A) + u_evens", "\n", "x_evens", "=", "(", "A", "@", "shift_up", "(", "x_odds", ")", ".", "unsqueeze", "(", "-", "1", ")", ")", ".", "squeeze", "(", "-", "1", ")", "+", "u_evens", "\n", "\n", "x", "=", "interleave", "(", "x_evens", ",", "x_odds", ",", "dim", "=", "0", ")", "\n", "return", "x", "\n", "\n", "# Pad u to power of 2", "\n", "", "n", "=", "u", ".", "shape", "[", "0", "]", "\n", "m", "=", "int", "(", "math", ".", "ceil", "(", "math", ".", "log", "(", "n", ")", "/", "math", ".", "log", "(", "2", ")", ")", ")", "\n", "N", "=", "1", "<<", "m", "\n", "u", "=", "torch", ".", "cat", "(", "(", "u", ",", "u", ".", "new_zeros", "(", "(", "N", "-", "u", ".", "shape", "[", "0", "]", ",", ")", "+", "u", ".", "shape", "[", "1", ":", "]", ")", ")", ",", "dim", "=", "0", ")", "\n", "\n", "return", "parallel_unroll_recursive_", "(", "A", ",", "u", ")", "[", ":", "n", ",", "...", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.unroll.parallel_unroll_recursive_br": [[125, 160], ["int", "torch.cat", "torch.cat", "torch.cat", "src.utils.permutations.bitreversal_po2", "unroll.parallel_unroll_recursive_br.parallel_unroll_recursive_br_"], "function", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.utils.permutations.bitreversal_po2"], ["", "def", "parallel_unroll_recursive_br", "(", "A", ",", "u", ")", ":", "\n", "    ", "\"\"\" Same as parallel_unroll_recursive but uses bit reversal for locality. \"\"\"", "\n", "\n", "# Main recursive function", "\n", "def", "parallel_unroll_recursive_br_", "(", "A", ",", "u", ")", ":", "\n", "        ", "n", "=", "u", ".", "shape", "[", "0", "]", "\n", "if", "n", "==", "1", ":", "\n", "            ", "return", "u", "\n", "\n", "", "m", "=", "n", "//", "2", "\n", "u_0", "=", "u", "[", ":", "m", ",", "...", "]", "\n", "u_1", "=", "u", "[", "m", ":", ",", "...", "]", "\n", "\n", "u2", "=", "F", ".", "linear", "(", "u_0", ",", "A", ")", "+", "u_1", "\n", "A2", "=", "A", "@", "A", "\n", "\n", "x_1", "=", "parallel_unroll_recursive_br_", "(", "A2", ",", "u2", ")", "\n", "x_0", "=", "F", ".", "linear", "(", "shift_up", "(", "x_1", ")", ",", "A", ")", "+", "u_0", "\n", "\n", "# x = torch.cat((x_0, x_1), dim=0) # is there a way to do this with cat?", "\n", "x", "=", "interleave", "(", "x_0", ",", "x_1", ",", "dim", "=", "0", ")", "\n", "return", "x", "\n", "\n", "# Pad u to power of 2", "\n", "", "n", "=", "u", ".", "shape", "[", "0", "]", "\n", "m", "=", "int", "(", "math", ".", "ceil", "(", "math", ".", "log", "(", "n", ")", "/", "math", ".", "log", "(", "2", ")", ")", ")", "\n", "N", "=", "1", "<<", "m", "\n", "u", "=", "torch", ".", "cat", "(", "(", "u", ",", "u", ".", "new_zeros", "(", "(", "N", "-", "u", ".", "shape", "[", "0", "]", ",", ")", "+", "u", ".", "shape", "[", "1", ":", "]", ")", ")", ",", "dim", "=", "0", ")", "\n", "\n", "# Apply bit reversal", "\n", "br", "=", "bitreversal_po2", "(", "N", ")", "\n", "u", "=", "u", "[", "br", ",", "...", "]", "\n", "\n", "x", "=", "parallel_unroll_recursive_br_", "(", "A", ",", "u", ")", "\n", "return", "x", "[", ":", "n", ",", "...", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.unroll.parallel_unroll_iterative": [[161, 192], ["int", "torch.cat", "torch.cat", "torch.cat", "src.utils.permutations.bitreversal_po2", "range", "range", "math.ceil", "As.append", "us.append", "unroll.interleave", "torch.cat.new_zeros", "torch.linear", "torch.linear", "math.log", "math.log", "unroll.shift_up"], "function", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.utils.permutations.bitreversal_po2", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.unroll.interleave", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.unroll.shift_up"], ["", "def", "parallel_unroll_iterative", "(", "A", ",", "u", ")", ":", "\n", "    ", "\"\"\" Bottom-up divide-and-conquer version of unroll, implemented iteratively \"\"\"", "\n", "\n", "# Pad u to power of 2", "\n", "n", "=", "u", ".", "shape", "[", "0", "]", "\n", "m", "=", "int", "(", "math", ".", "ceil", "(", "math", ".", "log", "(", "n", ")", "/", "math", ".", "log", "(", "2", ")", ")", ")", "\n", "N", "=", "1", "<<", "m", "\n", "u", "=", "torch", ".", "cat", "(", "(", "u", ",", "u", ".", "new_zeros", "(", "(", "N", "-", "u", ".", "shape", "[", "0", "]", ",", ")", "+", "u", ".", "shape", "[", "1", ":", "]", ")", ")", ",", "dim", "=", "0", ")", "\n", "\n", "# Apply bit reversal", "\n", "br", "=", "bitreversal_po2", "(", "N", ")", "\n", "u", "=", "u", "[", "br", ",", "...", "]", "\n", "\n", "# Main recursive loop, flattened", "\n", "us", "=", "[", "]", "# stores the u_0 terms in the recursive version", "\n", "N_", "=", "N", "\n", "As", "=", "[", "]", "# stores the A matrices", "\n", "for", "l", "in", "range", "(", "m", ")", ":", "\n", "        ", "N_", "=", "N_", "//", "2", "\n", "As", ".", "append", "(", "A", ")", "\n", "u_0", "=", "u", "[", ":", "N_", ",", "...", "]", "\n", "us", ".", "append", "(", "u_0", ")", "\n", "u", "=", "F", ".", "linear", "(", "u_0", ",", "A", ")", "+", "u", "[", "N_", ":", ",", "...", "]", "\n", "A", "=", "A", "@", "A", "\n", "", "x_0", "=", "[", "]", "\n", "x", "=", "u", "# x_1", "\n", "for", "l", "in", "range", "(", "m", "-", "1", ",", "-", "1", ",", "-", "1", ")", ":", "\n", "        ", "x_0", "=", "F", ".", "linear", "(", "shift_up", "(", "x", ")", ",", "As", "[", "l", "]", ")", "+", "us", "[", "l", "]", "\n", "x", "=", "interleave", "(", "x_0", ",", "x", ",", "dim", "=", "0", ")", "\n", "\n", "", "return", "x", "[", ":", "n", ",", "...", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.unroll.variable_unroll_sequential": [[194, 222], ["zip", "torch.stack", "torch.stack", "torch.stack", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "A.expand.expand", "len", "len", "torch.unbind", "torch.unbind", "torch.unbind", "torch.unbind", "torch.unbind", "torch.unbind", "outputs.append", "unroll.batch_mult", "A_.unsqueeze", "torch.zeros_like.unsqueeze"], "function", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.unroll.batch_mult"], ["", "def", "variable_unroll_sequential", "(", "A", ",", "u", ",", "s", "=", "None", ",", "variable", "=", "True", ")", ":", "\n", "    ", "\"\"\" Unroll with variable (in time/length) transitions A.\n\n    A : ([L], ..., N, N) dimension L should exist iff variable is True\n    u : (L, [B], ..., N) updates\n    s : ([B], ..., N) start state\n    output : x (..., N)\n    x[i, ...] = A[i]..A[0] @ s + A[i..1] @ u[0] + ... + A[i] @ u[i-1] + u[i]\n    \"\"\"", "\n", "\n", "if", "s", "is", "None", ":", "\n", "        ", "s", "=", "torch", ".", "zeros_like", "(", "u", "[", "0", "]", ")", "\n", "\n", "", "if", "not", "variable", ":", "\n", "        ", "A", "=", "A", ".", "expand", "(", "(", "u", ".", "shape", "[", "0", "]", ",", ")", "+", "A", ".", "shape", ")", "\n", "", "has_batch", "=", "len", "(", "u", ".", "shape", ")", ">=", "len", "(", "A", ".", "shape", ")", "\n", "\n", "outputs", "=", "[", "]", "\n", "for", "(", "A_", ",", "u_", ")", "in", "zip", "(", "torch", ".", "unbind", "(", "A", ",", "dim", "=", "0", ")", ",", "torch", ".", "unbind", "(", "u", ",", "dim", "=", "0", ")", ")", ":", "\n", "# s = F.linear(s, A_) + u_", "\n", "# print(\"shapes\", A_.shape, s.shape, has_batch)", "\n", "        ", "s", "=", "batch_mult", "(", "A_", ".", "unsqueeze", "(", "0", ")", ",", "s", ".", "unsqueeze", "(", "0", ")", ",", "has_batch", ")", "[", "0", "]", "\n", "# breakpoint()", "\n", "s", "=", "s", "+", "u_", "\n", "outputs", ".", "append", "(", "s", ")", "\n", "\n", "", "output", "=", "torch", ".", "stack", "(", "outputs", ",", "dim", "=", "0", ")", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.unroll.variable_unroll": [[225, 268], ["unroll.batch_mult", "unroll.variable_unroll", "unroll.shift_up", "unroll.batch_mult", "unroll.interleave", "unroll.variable_unroll_sequential", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "len", "len"], "function", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.unroll.batch_mult", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.unroll.variable_unroll", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.unroll.shift_up", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.unroll.batch_mult", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.unroll.interleave", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.unroll.variable_unroll_sequential"], ["", "def", "variable_unroll", "(", "A", ",", "u", ",", "s", "=", "None", ",", "variable", "=", "True", ",", "recurse_limit", "=", "16", ")", ":", "\n", "    ", "\"\"\" Bottom-up divide-and-conquer version of variable_unroll. \"\"\"", "\n", "\n", "if", "u", ".", "shape", "[", "0", "]", "<=", "recurse_limit", ":", "\n", "        ", "return", "variable_unroll_sequential", "(", "A", ",", "u", ",", "s", ",", "variable", ")", "\n", "\n", "", "if", "s", "is", "None", ":", "\n", "        ", "s", "=", "torch", ".", "zeros_like", "(", "u", "[", "0", "]", ")", "\n", "\n", "", "uneven", "=", "u", ".", "shape", "[", "0", "]", "%", "2", "==", "1", "\n", "has_batch", "=", "len", "(", "u", ".", "shape", ")", ">=", "len", "(", "A", ".", "shape", ")", "\n", "\n", "u_0", "=", "u", "[", "0", ":", ":", "2", ",", "...", "]", "\n", "u_1", "=", "u", "[", "1", ":", ":", "2", ",", "...", "]", "\n", "\n", "if", "variable", ":", "\n", "        ", "A_0", "=", "A", "[", "0", ":", ":", "2", ",", "...", "]", "\n", "A_1", "=", "A", "[", "1", ":", ":", "2", ",", "...", "]", "\n", "", "else", ":", "\n", "        ", "A_0", "=", "A", "\n", "A_1", "=", "A", "\n", "\n", "", "u_0_", "=", "u_0", "\n", "A_0_", "=", "A_0", "\n", "if", "uneven", ":", "\n", "        ", "u_0_", "=", "u_0", "[", ":", "-", "1", ",", "...", "]", "\n", "if", "variable", ":", "\n", "            ", "A_0_", "=", "A_0", "[", ":", "-", "1", ",", "...", "]", "\n", "\n", "", "", "u_10", "=", "batch_mult", "(", "A_1", ",", "u_0_", ",", "has_batch", ")", "\n", "u_10", "=", "u_10", "+", "u_1", "\n", "A_10", "=", "A_1", "@", "A_0_", "\n", "\n", "# Recursive call", "\n", "x_1", "=", "variable_unroll", "(", "A_10", ",", "u_10", ",", "s", ",", "variable", ",", "recurse_limit", ")", "\n", "\n", "x_0", "=", "shift_up", "(", "x_1", ",", "s", ",", "drop", "=", "not", "uneven", ")", "\n", "x_0", "=", "batch_mult", "(", "A_0", ",", "x_0", ",", "has_batch", ")", "\n", "x_0", "=", "x_0", "+", "u_0", "\n", "\n", "\n", "x", "=", "interleave", "(", "x_0", ",", "x_1", ",", "uneven", ",", "dim", "=", "0", ")", "# For some reason this interleave is slower than in the (non-multi) unroll_recursive", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.unroll.variable_unroll_general_sequential": [[269, 290], ["zip", "torch.stack", "torch.stack", "torch.stack", "A.expand.expand", "torch.unbind", "torch.unbind", "torch.unbind", "torch.unbind", "torch.unbind", "torch.unbind", "outputs.append", "src.models.functional.toeplitz.triangular_toeplitz_multiply", "src.models.functional.toeplitz.triangular_toeplitz_multiply_padded"], "function", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.lssl.triangular_toeplitz_multiply"], ["", "def", "variable_unroll_general_sequential", "(", "A", ",", "u", ",", "s", ",", "op", ",", "variable", "=", "True", ")", ":", "\n", "    ", "\"\"\" Unroll with variable (in time/length) transitions A with general associative operation\n\n    A : ([L], ..., N, N) dimension L should exist iff variable is True\n    u : (L, [B], ..., N) updates\n    s : ([B], ..., N) start state\n    output : x (..., N)\n    x[i, ...] = A[i]..A[0] s + A[i..1] u[0] + ... + A[i] u[i-1] + u[i]\n    \"\"\"", "\n", "\n", "if", "not", "variable", ":", "\n", "        ", "A", "=", "A", ".", "expand", "(", "(", "u", ".", "shape", "[", "0", "]", ",", ")", "+", "A", ".", "shape", ")", "\n", "\n", "", "outputs", "=", "[", "]", "\n", "for", "(", "A_", ",", "u_", ")", "in", "zip", "(", "torch", ".", "unbind", "(", "A", ",", "dim", "=", "0", ")", ",", "torch", ".", "unbind", "(", "u", ",", "dim", "=", "0", ")", ")", ":", "\n", "        ", "s", "=", "op", "(", "A_", ",", "s", ")", "\n", "s", "=", "s", "+", "u_", "\n", "outputs", ".", "append", "(", "s", ")", "\n", "\n", "", "output", "=", "torch", ".", "stack", "(", "outputs", ",", "dim", "=", "0", ")", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.unroll.variable_unroll_matrix_sequential": [[291, 303], ["unroll.variable_unroll_general_sequential", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "A.expand.expand", "unroll.batch_mult", "x.unsqueeze", "y.unsqueeze"], "function", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.unroll.variable_unroll_general_sequential", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.unroll.batch_mult"], ["", "def", "variable_unroll_matrix_sequential", "(", "A", ",", "u", ",", "s", "=", "None", ",", "variable", "=", "True", ")", ":", "\n", "    ", "if", "s", "is", "None", ":", "\n", "        ", "s", "=", "torch", ".", "zeros_like", "(", "u", "[", "0", "]", ")", "\n", "\n", "", "if", "not", "variable", ":", "\n", "        ", "A", "=", "A", ".", "expand", "(", "(", "u", ".", "shape", "[", "0", "]", ",", ")", "+", "A", ".", "shape", ")", "\n", "# has_batch = len(u.shape) >= len(A.shape)", "\n", "\n", "# op = lambda x, y: batch_mult(x.unsqueeze(0), y.unsqueeze(0), has_batch)[0]", "\n", "", "op", "=", "lambda", "x", ",", "y", ":", "batch_mult", "(", "x", ".", "unsqueeze", "(", "0", ")", ",", "y", ".", "unsqueeze", "(", "0", ")", ")", "[", "0", "]", "\n", "\n", "return", "variable_unroll_general_sequential", "(", "A", ",", "u", ",", "s", ",", "op", ",", "variable", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.unroll.variable_unroll_toeplitz_sequential": [[304, 327], ["unroll.variable_unroll_general_sequential", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "F.pad.expand", "torch.pad", "torch.pad", "torch.pad", "unroll.variable_unroll_general_sequential"], "function", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.unroll.variable_unroll_general_sequential", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.sc.pad", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.sc.pad", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.sc.pad", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.unroll.variable_unroll_general_sequential"], ["", "def", "variable_unroll_toeplitz_sequential", "(", "A", ",", "u", ",", "s", "=", "None", ",", "variable", "=", "True", ",", "pad", "=", "False", ")", ":", "\n", "    ", "if", "s", "is", "None", ":", "\n", "        ", "s", "=", "torch", ".", "zeros_like", "(", "u", "[", "0", "]", ")", "\n", "\n", "", "if", "not", "variable", ":", "\n", "        ", "A", "=", "A", ".", "expand", "(", "(", "u", ".", "shape", "[", "0", "]", ",", ")", "+", "A", ".", "shape", ")", "\n", "# has_batch = len(u.shape) >= len(A.shape)", "\n", "\n", "# op = lambda x, y: batch_mult(x.unsqueeze(0), y.unsqueeze(0), has_batch)[0]", "\n", "# op = lambda x, y: batch_mult(x.unsqueeze(0), y.unsqueeze(0))[0]", "\n", "\n", "", "if", "pad", ":", "\n", "        ", "n", "=", "A", ".", "shape", "[", "-", "1", "]", "\n", "# print(\"shapes\", A.shape, u.shape)", "\n", "A", "=", "F", ".", "pad", "(", "A", ",", "(", "0", ",", "n", ")", ")", "\n", "u", "=", "F", ".", "pad", "(", "u", ",", "(", "0", ",", "n", ")", ")", "\n", "s", "=", "F", ".", "pad", "(", "s", ",", "(", "0", ",", "n", ")", ")", "\n", "# print(\"shapes\", A.shape, u.shape)", "\n", "ret", "=", "variable_unroll_general_sequential", "(", "A", ",", "u", ",", "s", ",", "triangular_toeplitz_multiply_padded", ",", "variable", "=", "True", ")", "\n", "ret", "=", "ret", "[", "...", ",", ":", "n", "]", "\n", "return", "ret", "\n", "\n", "", "return", "variable_unroll_general_sequential", "(", "A", ",", "u", ",", "s", ",", "triangular_toeplitz_multiply", ",", "variable", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.unroll.variable_unroll_general": [[332, 380], ["op", "compose_op", "unroll.variable_unroll_general", "unroll.shift_up", "op", "unroll.interleave", "unroll.variable_unroll_general_sequential", "len", "len"], "function", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.unroll.variable_unroll_general", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.unroll.shift_up", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.unroll.interleave", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.unroll.variable_unroll_general_sequential"], ["", "def", "variable_unroll_general", "(", "A", ",", "u", ",", "s", ",", "op", ",", "compose_op", "=", "None", ",", "sequential_op", "=", "None", ",", "variable", "=", "True", ",", "recurse_limit", "=", "16", ")", ":", "\n", "    ", "\"\"\" Bottom-up divide-and-conquer version of variable_unroll.\n\n    compose is an optional function that defines how to compose A without multiplying by a leaf u\n    \"\"\"", "\n", "\n", "if", "u", ".", "shape", "[", "0", "]", "<=", "recurse_limit", ":", "\n", "        ", "if", "sequential_op", "is", "None", ":", "\n", "            ", "sequential_op", "=", "op", "\n", "", "return", "variable_unroll_general_sequential", "(", "A", ",", "u", ",", "s", ",", "sequential_op", ",", "variable", ")", "\n", "\n", "", "if", "compose_op", "is", "None", ":", "\n", "        ", "compose_op", "=", "op", "\n", "\n", "", "uneven", "=", "u", ".", "shape", "[", "0", "]", "%", "2", "==", "1", "\n", "has_batch", "=", "len", "(", "u", ".", "shape", ")", ">=", "len", "(", "A", ".", "shape", ")", "\n", "\n", "u_0", "=", "u", "[", "0", ":", ":", "2", ",", "...", "]", "\n", "u_1", "=", "u", "[", "1", ":", ":", "2", ",", "...", "]", "\n", "\n", "if", "variable", ":", "\n", "        ", "A_0", "=", "A", "[", "0", ":", ":", "2", ",", "...", "]", "\n", "A_1", "=", "A", "[", "1", ":", ":", "2", ",", "...", "]", "\n", "", "else", ":", "\n", "        ", "A_0", "=", "A", "\n", "A_1", "=", "A", "\n", "\n", "", "u_0_", "=", "u_0", "\n", "A_0_", "=", "A_0", "\n", "if", "uneven", ":", "\n", "        ", "u_0_", "=", "u_0", "[", ":", "-", "1", ",", "...", "]", "\n", "if", "variable", ":", "\n", "            ", "A_0_", "=", "A_0", "[", ":", "-", "1", ",", "...", "]", "\n", "\n", "", "", "u_10", "=", "op", "(", "A_1", ",", "u_0_", ")", "# batch_mult(A_1, u_0_, has_batch)", "\n", "u_10", "=", "u_10", "+", "u_1", "\n", "A_10", "=", "compose_op", "(", "A_1", ",", "A_0_", ")", "\n", "\n", "# Recursive call", "\n", "x_1", "=", "variable_unroll_general", "(", "A_10", ",", "u_10", ",", "s", ",", "op", ",", "compose_op", ",", "sequential_op", ",", "variable", "=", "variable", ",", "recurse_limit", "=", "recurse_limit", ")", "\n", "\n", "x_0", "=", "shift_up", "(", "x_1", ",", "s", ",", "drop", "=", "not", "uneven", ")", "\n", "x_0", "=", "op", "(", "A_0", ",", "x_0", ")", "# batch_mult(A_0, x_0, has_batch)", "\n", "x_0", "=", "x_0", "+", "u_0", "\n", "\n", "\n", "x", "=", "interleave", "(", "x_0", ",", "x_1", ",", "uneven", ",", "dim", "=", "0", ")", "# For some reason this interleave is slower than in the (non-multi) unroll_recursive", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.unroll.variable_unroll_matrix": [[381, 389], ["unroll.variable_unroll_general", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "len", "len", "unroll.batch_mult", "unroll.batch_mult", "x.unsqueeze", "y.unsqueeze"], "function", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.unroll.variable_unroll_general", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.unroll.batch_mult", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.unroll.batch_mult"], ["", "def", "variable_unroll_matrix", "(", "A", ",", "u", ",", "s", "=", "None", ",", "variable", "=", "True", ",", "recurse_limit", "=", "16", ")", ":", "\n", "    ", "if", "s", "is", "None", ":", "\n", "        ", "s", "=", "torch", ".", "zeros_like", "(", "u", "[", "0", "]", ")", "\n", "", "has_batch", "=", "len", "(", "u", ".", "shape", ")", ">=", "len", "(", "A", ".", "shape", ")", "\n", "op", "=", "lambda", "x", ",", "y", ":", "batch_mult", "(", "x", ",", "y", ",", "has_batch", ")", "\n", "sequential_op", "=", "lambda", "x", ",", "y", ":", "batch_mult", "(", "x", ".", "unsqueeze", "(", "0", ")", ",", "y", ".", "unsqueeze", "(", "0", ")", ",", "has_batch", ")", "[", "0", "]", "\n", "matmul", "=", "lambda", "x", ",", "y", ":", "x", "@", "y", "\n", "return", "variable_unroll_general", "(", "A", ",", "u", ",", "s", ",", "op", ",", "compose_op", "=", "matmul", ",", "sequential_op", "=", "sequential_op", ",", "variable", "=", "variable", ",", "recurse_limit", "=", "recurse_limit", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.unroll.variable_unroll_toeplitz": [[390, 428], ["unroll.variable_unroll_general", "len", "int", "len", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.pad", "torch.pad", "torch.pad", "unroll.variable_unroll_general", "len", "len", "A.unsqueeze.unsqueeze"], "function", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.unroll.variable_unroll_general", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.sc.pad", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.sc.pad", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.sc.pad", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.unroll.variable_unroll_general"], ["", "def", "variable_unroll_toeplitz", "(", "A", ",", "u", ",", "s", "=", "None", ",", "variable", "=", "True", ",", "recurse_limit", "=", "8", ",", "pad", "=", "False", ")", ":", "\n", "    ", "\"\"\" Unroll with variable (in time/length) transitions A with general associative operation\n\n    A : ([L], ..., N) dimension L should exist iff variable is True\n    u : (L, [B], ..., N) updates\n    s : ([B], ..., N) start state\n    output : x (L, [B], ..., N) same shape as u\n    x[i, ...] = A[i]..A[0] s + A[i..1] u[0] + ... + A[i] u[i-1] + u[i]\n    \"\"\"", "\n", "# Add the batch dimension to A if necessary", "\n", "A_batch_dims", "=", "len", "(", "A", ".", "shape", ")", "-", "int", "(", "variable", ")", "\n", "u_batch_dims", "=", "len", "(", "u", ".", "shape", ")", "-", "1", "\n", "if", "u_batch_dims", ">", "A_batch_dims", ":", "\n", "# assert u_batch_dims == A_batch_dims + 1", "\n", "        ", "if", "variable", ":", "\n", "            ", "while", "len", "(", "A", ".", "shape", ")", "<", "len", "(", "u", ".", "shape", ")", ":", "\n", "                ", "A", "=", "A", ".", "unsqueeze", "(", "1", ")", "\n", "# else:", "\n", "#     A = A.unsqueeze(0)", "\n", "\n", "", "", "", "if", "s", "is", "None", ":", "\n", "        ", "s", "=", "torch", ".", "zeros_like", "(", "u", "[", "0", "]", ")", "\n", "\n", "", "if", "pad", ":", "\n", "        ", "n", "=", "A", ".", "shape", "[", "-", "1", "]", "\n", "# print(\"shapes\", A.shape, u.shape)", "\n", "A", "=", "F", ".", "pad", "(", "A", ",", "(", "0", ",", "n", ")", ")", "\n", "u", "=", "F", ".", "pad", "(", "u", ",", "(", "0", ",", "n", ")", ")", "\n", "s", "=", "F", ".", "pad", "(", "s", ",", "(", "0", ",", "n", ")", ")", "\n", "# print(\"shapes\", A.shape, u.shape)", "\n", "op", "=", "triangular_toeplitz_multiply_padded", "\n", "ret", "=", "variable_unroll_general", "(", "A", ",", "u", ",", "s", ",", "op", ",", "compose_op", "=", "op", ",", "variable", "=", "variable", ",", "recurse_limit", "=", "recurse_limit", ")", "\n", "ret", "=", "ret", "[", "...", ",", ":", "n", "]", "\n", "return", "ret", "\n", "\n", "", "op", "=", "triangular_toeplitz_multiply", "\n", "ret", "=", "variable_unroll_general", "(", "A", ",", "u", ",", "s", ",", "op", ",", "compose_op", "=", "op", ",", "variable", "=", "variable", ",", "recurse_limit", "=", "recurse_limit", ")", "\n", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.unroll.test_correctness": [[433, 470], ["print", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.ones", "torch.ones", "torch.ones", "unroll.unroll", "torch.isclose().all", "torch.isclose().all", "torch.isclose().all", "torch.isclose().all", "torch.isclose().all", "torch.isclose().all", "torch.isclose().all", "torch.isclose().all", "torch.isclose().all", "unroll.parallel_unroll_recursive", "torch.isclose().all", "torch.isclose().all", "torch.isclose().all", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.ones", "torch.ones", "torch.ones", "unroll.parallel_unroll_recursive", "print", "unroll.parallel_unroll_recursive_br", "print", "unroll.parallel_unroll_iterative", "print", "A.repeat.repeat", "torch.zeros", "torch.zeros", "torch.zeros", "print", "unroll.variable_unroll_sequential", "print", "unroll.variable_unroll", "print", "torch.isclose", "torch.isclose", "torch.isclose", "torch.isclose", "torch.isclose", "torch.isclose", "torch.isclose", "torch.isclose", "torch.isclose", "torch.isclose", "torch.isclose", "torch.isclose", "torch.Tensor", "torch.Tensor", "torch.Tensor", "unroll.shift_up", "torch.Tensor", "torch.Tensor", "torch.Tensor", "unroll.interleave", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor"], "function", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.unroll.unroll", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.unroll.parallel_unroll_recursive", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.unroll.parallel_unroll_recursive", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.unroll.parallel_unroll_recursive_br", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.unroll.parallel_unroll_iterative", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.unroll.variable_unroll_sequential", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.unroll.variable_unroll", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.unroll.shift_up", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.unroll.interleave"], ["", "def", "test_correctness", "(", ")", ":", "\n", "    ", "print", "(", "\"Testing Correctness\\n====================\"", ")", "\n", "\n", "# Test sequential unroll", "\n", "L", "=", "3", "\n", "A", "=", "torch", ".", "Tensor", "(", "[", "[", "1", ",", "1", "]", ",", "[", "1", ",", "0", "]", "]", ")", "\n", "u", "=", "torch", ".", "ones", "(", "(", "L", ",", "2", ")", ")", "\n", "x", "=", "unroll", "(", "A", ",", "u", ")", "\n", "assert", "torch", ".", "isclose", "(", "x", ",", "torch", ".", "Tensor", "(", "[", "[", "1.", ",", "1.", "]", ",", "[", "3.", ",", "2.", "]", ",", "[", "6.", ",", "4.", "]", "]", ")", ")", ".", "all", "(", ")", "\n", "\n", "# Test utilities", "\n", "assert", "torch", ".", "isclose", "(", "shift_up", "(", "x", ")", ",", "torch", ".", "Tensor", "(", "[", "[", "0.", ",", "0.", "]", ",", "[", "1.", ",", "1.", "]", ",", "[", "3.", ",", "2.", "]", "]", ")", ")", ".", "all", "(", ")", "\n", "assert", "torch", ".", "isclose", "(", "interleave", "(", "x", ",", "x", ")", ",", "torch", ".", "Tensor", "(", "[", "[", "1.", ",", "1.", "]", ",", "[", "1.", ",", "1.", "]", ",", "[", "3.", ",", "2.", "]", ",", "[", "3.", ",", "2.", "]", ",", "[", "6.", ",", "4.", "]", ",", "[", "6.", ",", "4.", "]", "]", ")", ")", ".", "all", "(", ")", "\n", "\n", "# Test parallel unroll", "\n", "x", "=", "parallel_unroll_recursive", "(", "A", ",", "u", ")", "\n", "assert", "torch", ".", "isclose", "(", "x", ",", "torch", ".", "Tensor", "(", "[", "[", "1.", ",", "1.", "]", ",", "[", "3.", ",", "2.", "]", ",", "[", "6.", ",", "4.", "]", "]", ")", ")", ".", "all", "(", ")", "\n", "\n", "# Powers", "\n", "L", "=", "12", "\n", "A", "=", "torch", ".", "Tensor", "(", "[", "[", "1", ",", "0", ",", "0", "]", ",", "[", "2", ",", "1", ",", "0", "]", ",", "[", "3", ",", "3", ",", "1", "]", "]", ")", "\n", "u", "=", "torch", ".", "ones", "(", "(", "L", ",", "3", ")", ")", "\n", "x", "=", "parallel_unroll_recursive", "(", "A", ",", "u", ")", "\n", "print", "(", "\"recursive\"", ",", "x", ")", "\n", "x", "=", "parallel_unroll_recursive_br", "(", "A", ",", "u", ")", "\n", "print", "(", "\"recursive_br\"", ",", "x", ")", "\n", "x", "=", "parallel_unroll_iterative", "(", "A", ",", "u", ")", "\n", "print", "(", "\"iterative_br\"", ",", "x", ")", "\n", "\n", "\n", "A", "=", "A", ".", "repeat", "(", "(", "L", ",", "1", ",", "1", ")", ")", "\n", "s", "=", "torch", ".", "zeros", "(", "3", ")", "\n", "print", "(", "\"A shape\"", ",", "A", ".", "shape", ")", "\n", "x", "=", "variable_unroll_sequential", "(", "A", ",", "u", ",", "s", ")", "\n", "print", "(", "\"variable_unroll\"", ",", "x", ")", "\n", "x", "=", "variable_unroll", "(", "A", ",", "u", ",", "s", ")", "\n", "print", "(", "\"parallel_variable_unroll\"", ",", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.unroll.generate_data": [[472, 482], ["torch.normal", "torch.normal", "torch.normal", "A.to.to", "u.to.to", "torch.eye", "torch.eye", "torch.eye", "torch.device", "torch.device", "torch.device", "torch.device", "torch.device", "torch.device", "torch.normal", "torch.normal", "torch.normal"], "function", ["None"], ["", "def", "generate_data", "(", "L", ",", "N", ",", "B", "=", "None", ",", "cuda", "=", "True", ")", ":", "\n", "    ", "A", "=", "torch", ".", "eye", "(", "N", ")", "+", "torch", ".", "normal", "(", "0", ",", "1", ",", "size", "=", "(", "N", ",", "N", ")", ")", "/", "(", "N", "**", ".5", ")", "/", "L", "\n", "u", "=", "torch", ".", "normal", "(", "0", ",", "1", ",", "size", "=", "(", "L", ",", "B", ",", "N", ")", ")", "\n", "\n", "\n", "# device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')", "\n", "device", "=", "torch", ".", "device", "(", "'cuda:0'", ")", "if", "cuda", "else", "torch", ".", "device", "(", "'cpu'", ")", "\n", "A", "=", "A", ".", "to", "(", "device", ")", "\n", "u", "=", "u", ".", "to", "(", "device", ")", "\n", "return", "A", ",", "u", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.unroll.test_stability": [[483, 516], ["print", "unroll.generate_data", "unroll.unroll", "unroll.parallel_unroll_recursive", "unroll.parallel_unroll_recursive_br", "unroll.parallel_unroll_iterative", "print", "print", "print", "print", "print", "print", "A.repeat.repeat", "unroll.variable_unroll_sequential", "unroll.variable_unroll", "unroll.variable_unroll_matrix", "print", "torch.abs", "torch.abs", "torch.abs", "print", "print", "print", "print", "torch.norm", "torch.norm", "torch.norm", "torch.norm", "torch.norm", "torch.norm", "torch.norm", "torch.norm", "torch.norm", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.norm", "torch.norm", "torch.norm", "torch.max", "torch.max", "torch.max", "torch.norm", "torch.norm", "torch.norm", "torch.max", "torch.max", "torch.max", "torch.abs", "torch.abs", "torch.abs", "torch.abs", "torch.abs", "torch.abs", "torch.abs", "torch.abs", "torch.abs", "torch.abs", "torch.abs", "torch.abs"], "function", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.cauchy.test_cauchy.generate_data", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.unroll.unroll", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.unroll.parallel_unroll_recursive", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.unroll.parallel_unroll_recursive_br", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.unroll.parallel_unroll_iterative", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.unroll.variable_unroll_sequential", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.unroll.variable_unroll", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.unroll.variable_unroll_matrix"], ["", "def", "test_stability", "(", ")", ":", "\n", "    ", "print", "(", "\"Testing Stability\\n====================\"", ")", "\n", "L", "=", "256", "\n", "N", "=", "L", "//", "2", "\n", "B", "=", "100", "\n", "A", ",", "u", "=", "generate_data", "(", "L", ",", "N", ",", "B", ")", "\n", "\n", "x", "=", "unroll", "(", "A", ",", "u", ")", "\n", "x1", "=", "parallel_unroll_recursive", "(", "A", ",", "u", ")", "\n", "x2", "=", "parallel_unroll_recursive_br", "(", "A", ",", "u", ")", "\n", "x3", "=", "parallel_unroll_iterative", "(", "A", ",", "u", ")", "\n", "print", "(", "\"norm error\"", ",", "torch", ".", "norm", "(", "x", "-", "x1", ")", ")", "\n", "print", "(", "\"norm error\"", ",", "torch", ".", "norm", "(", "x", "-", "x2", ")", ")", "\n", "print", "(", "\"norm error\"", ",", "torch", ".", "norm", "(", "x", "-", "x3", ")", ")", "\n", "# print(x-x1)", "\n", "# print(x-x2)", "\n", "# print(x-x3)", "\n", "print", "(", "\"max error\"", ",", "torch", ".", "max", "(", "torch", ".", "abs", "(", "x", "-", "x1", ")", ")", ")", "\n", "print", "(", "\"max error\"", ",", "torch", ".", "max", "(", "torch", ".", "abs", "(", "x", "-", "x2", ")", ")", ")", "\n", "print", "(", "\"max error\"", ",", "torch", ".", "max", "(", "torch", ".", "abs", "(", "x", "-", "x3", ")", ")", ")", "\n", "\n", "A", "=", "A", ".", "repeat", "(", "(", "L", ",", "1", ",", "1", ")", ")", "\n", "x", "=", "variable_unroll_sequential", "(", "A", ",", "u", ")", "\n", "x_", "=", "variable_unroll", "(", "A", ",", "u", ")", "\n", "# x_ = variable_unroll_matrix_sequential(A, u)", "\n", "x_", "=", "variable_unroll_matrix", "(", "A", ",", "u", ")", "\n", "print", "(", "x", "-", "x_", ")", "\n", "abserr", "=", "torch", ".", "abs", "(", "x", "-", "x_", ")", "\n", "relerr", "=", "abserr", "/", "(", "torch", ".", "abs", "(", "x", ")", "+", "1e-8", ")", "\n", "print", "(", "\"norm abs error\"", ",", "torch", ".", "norm", "(", "abserr", ")", ")", "\n", "print", "(", "\"max abs error\"", ",", "torch", ".", "max", "(", "abserr", ")", ")", "\n", "print", "(", "\"norm rel error\"", ",", "torch", ".", "norm", "(", "relerr", ")", ")", "\n", "print", "(", "\"max rel error\"", ",", "torch", ".", "max", "(", "relerr", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.unroll.test_toeplitz": [[517, 568], ["print", "unroll.generate_data", "construct_toeplitz", "unroll.unroll", "unroll.variable_unroll", "unroll.test_toeplitz.summarize"], "function", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.cauchy.test_cauchy.generate_data", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.toeplitz.construct_toeplitz", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.unroll.unroll", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.unroll.variable_unroll"], ["", "def", "test_toeplitz", "(", ")", ":", "\n", "    ", "from", "model", ".", "toeplitz", "import", "construct_toeplitz", "\n", "def", "summarize", "(", "name", ",", "x", ",", "x_", ",", "showdiff", "=", "False", ")", ":", "\n", "        ", "print", "(", "name", ",", "\"stats\"", ")", "\n", "if", "showdiff", ":", "\n", "            ", "print", "(", "x", "-", "x_", ")", "\n", "", "abserr", "=", "torch", ".", "abs", "(", "x", "-", "x_", ")", "\n", "relerr", "=", "abserr", "/", "(", "torch", ".", "abs", "(", "x", ")", "+", "1e-8", ")", "\n", "print", "(", "\"  norm abs error\"", ",", "torch", ".", "norm", "(", "abserr", ")", ")", "\n", "print", "(", "\"  max abs error\"", ",", "torch", ".", "max", "(", "abserr", ")", ")", "\n", "print", "(", "\"  norm rel error\"", ",", "torch", ".", "norm", "(", "relerr", ")", ")", "\n", "print", "(", "\"  max rel error\"", ",", "torch", ".", "max", "(", "relerr", ")", ")", "\n", "\n", "", "print", "(", "\"Testing Toeplitz\\n====================\"", ")", "\n", "L", "=", "512", "\n", "N", "=", "L", "//", "2", "\n", "B", "=", "100", "\n", "A", ",", "u", "=", "generate_data", "(", "L", ",", "N", ",", "B", ")", "\n", "\n", "A", "=", "A", "[", "...", ",", "0", "]", "\n", "A", "=", "construct_toeplitz", "(", "A", ")", "\n", "\n", "# print(\"SHAPES\", A.shape, u.shape)", "\n", "\n", "# Static A", "\n", "x", "=", "unroll", "(", "A", ",", "u", ")", "\n", "x_", "=", "variable_unroll", "(", "A", ",", "u", ",", "variable", "=", "False", ")", "\n", "summarize", "(", "\"nonvariable matrix original\"", ",", "x", ",", "x_", ",", "showdiff", "=", "False", ")", "\n", "x_", "=", "variable_unroll_matrix", "(", "A", ",", "u", ",", "variable", "=", "False", ")", "\n", "summarize", "(", "\"nonvariable matrix general\"", ",", "x", ",", "x_", ",", "showdiff", "=", "False", ")", "\n", "x_", "=", "variable_unroll_toeplitz", "(", "A", "[", "...", ",", "0", "]", ",", "u", ",", "variable", "=", "False", ")", "\n", "summarize", "(", "\"nonvariable toeplitz\"", ",", "x", ",", "x_", ",", "showdiff", "=", "False", ")", "\n", "\n", "# Sequential", "\n", "A", "=", "A", ".", "repeat", "(", "(", "L", ",", "1", ",", "1", ")", ")", "\n", "for", "_", "in", "range", "(", "1", ")", ":", "\n", "        ", "x_", "=", "variable_unroll_sequential", "(", "A", ",", "u", ")", "\n", "summarize", "(", "\"variable unroll sequential\"", ",", "x", ",", "x_", ",", "showdiff", "=", "False", ")", "\n", "x_", "=", "variable_unroll_matrix_sequential", "(", "A", ",", "u", ")", "\n", "summarize", "(", "\"variable matrix sequential\"", ",", "x", ",", "x_", ",", "showdiff", "=", "False", ")", "\n", "x_", "=", "variable_unroll_toeplitz_sequential", "(", "A", "[", "...", ",", "0", "]", ",", "u", ",", "pad", "=", "True", ")", "\n", "summarize", "(", "\"variable toeplitz sequential\"", ",", "x", ",", "x_", ",", "showdiff", "=", "False", ")", "\n", "\n", "# Parallel", "\n", "", "for", "_", "in", "range", "(", "1", ")", ":", "\n", "        ", "x_", "=", "variable_unroll", "(", "A", ",", "u", ")", "\n", "summarize", "(", "\"variable matrix original\"", ",", "x", ",", "x_", ",", "showdiff", "=", "False", ")", "\n", "x_", "=", "variable_unroll_matrix", "(", "A", ",", "u", ")", "\n", "summarize", "(", "\"variable matrix general\"", ",", "x", ",", "x_", ",", "showdiff", "=", "False", ")", "\n", "x_", "=", "variable_unroll_toeplitz", "(", "A", "[", "...", ",", "0", "]", ",", "u", ",", "pad", "=", "True", ",", "recurse_limit", "=", "8", ")", "\n", "summarize", "(", "\"variable toeplitz\"", ",", "x", ",", "x_", ",", "showdiff", "=", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.unroll.test_speed": [[569, 602], ["print", "unroll.generate_data", "A.repeat", "range", "range", "unroll.unroll", "torch.sum", "torch.sum", "torch.sum", "torch.sum.backward", "unroll.parallel_unroll_recursive", "torch.sum", "torch.sum", "torch.sum", "torch.sum.backward", "unroll.variable_unroll_sequential", "torch.sum", "torch.sum", "torch.sum", "torch.sum.backward", "unroll.variable_unroll", "torch.sum", "torch.sum", "torch.sum", "torch.sum.backward", "unroll.variable_unroll_sequential", "unroll.variable_unroll"], "function", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.cauchy.test_cauchy.generate_data", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.unroll.unroll", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.cauchy.cauchy.CauchyMultiplySymmetric.backward", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.unroll.parallel_unroll_recursive", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.cauchy.cauchy.CauchyMultiplySymmetric.backward", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.unroll.variable_unroll_sequential", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.cauchy.cauchy.CauchyMultiplySymmetric.backward", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.unroll.variable_unroll", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.cauchy.cauchy.CauchyMultiplySymmetric.backward", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.unroll.variable_unroll_sequential", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.unroll.variable_unroll"], ["", "", "def", "test_speed", "(", "variable", "=", "False", ",", "it", "=", "1", ")", ":", "\n", "    ", "print", "(", "\"Testing Speed\\n====================\"", ")", "\n", "N", "=", "256", "\n", "L", "=", "1024", "\n", "B", "=", "100", "\n", "A", ",", "u", "=", "generate_data", "(", "L", ",", "N", ",", "B", ")", "\n", "As", "=", "A", ".", "repeat", "(", "(", "L", ",", "1", ",", "1", ")", ")", "\n", "\n", "u", ".", "requires_grad", "=", "True", "\n", "As", ".", "requires_grad", "=", "True", "\n", "for", "_", "in", "range", "(", "it", ")", ":", "\n", "        ", "x", "=", "unroll", "(", "A", ",", "u", ")", "\n", "x", "=", "torch", ".", "sum", "(", "x", ")", "\n", "x", ".", "backward", "(", ")", "\n", "\n", "x", "=", "parallel_unroll_recursive", "(", "A", ",", "u", ")", "\n", "x", "=", "torch", ".", "sum", "(", "x", ")", "\n", "x", ".", "backward", "(", ")", "\n", "\n", "# parallel_unroll_recursive_br(A, u)", "\n", "# parallel_unroll_iterative(A, u)", "\n", "\n", "", "for", "_", "in", "range", "(", "it", ")", ":", "\n", "        ", "if", "variable", ":", "\n", "            ", "x", "=", "variable_unroll_sequential", "(", "As", ",", "u", ",", "variable", "=", "True", ",", "recurse_limit", "=", "16", ")", "\n", "x", "=", "torch", ".", "sum", "(", "x", ")", "\n", "x", ".", "backward", "(", ")", "\n", "x", "=", "variable_unroll", "(", "As", ",", "u", ",", "variable", "=", "True", ",", "recurse_limit", "=", "16", ")", "\n", "x", "=", "torch", ".", "sum", "(", "x", ")", "\n", "x", ".", "backward", "(", ")", "\n", "", "else", ":", "\n", "            ", "variable_unroll_sequential", "(", "A", ",", "u", ",", "variable", "=", "False", ",", "recurse_limit", "=", "16", ")", "\n", "variable_unroll", "(", "A", ",", "u", ",", "variable", "=", "False", ",", "recurse_limit", "=", "16", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.krylov.krylov_sequential": [[16, 49], ["range", "torch.stack", "torch.stack", "krylov.krylov_sequential", "torch.stack.append", "c.numel", "b.numel", "A.transpose", "torch.sum", "torch.sum", "b_.unsqueeze"], "function", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.krylov.krylov_sequential"], ["def", "krylov_sequential", "(", "L", ",", "A", ",", "b", ",", "c", "=", "None", ")", ":", "\n", "    ", "\"\"\" Constant matrix A\n\n    A : (..., N, N)\n    b : (..., N)\n    c : (..., N)\n\n    Returns\n    if c:\n    x : (..., L)\n    x[i, l] = c[i] @ A^l @ b[i]\n\n    else:\n    x : (..., N, L)\n    x[i, l] = A^l @ b[i]\n    \"\"\"", "\n", "\n", "# Check which of dim b and c is smaller to save memory", "\n", "if", "c", "is", "not", "None", "and", "c", ".", "numel", "(", ")", "<", "b", ".", "numel", "(", ")", ":", "\n", "        ", "return", "krylov_sequential", "(", "L", ",", "A", ".", "transpose", "(", "-", "1", ",", "-", "2", ")", ",", "c", ",", "b", ")", "\n", "\n", "", "b_", "=", "b", "\n", "x", "=", "[", "]", "\n", "for", "_", "in", "range", "(", "L", ")", ":", "\n", "        ", "if", "c", "is", "not", "None", ":", "\n", "            ", "x_", "=", "torch", ".", "sum", "(", "c", "*", "b_", ",", "dim", "=", "-", "1", ")", "# (...) # could be faster with matmul or einsum?", "\n", "", "else", ":", "\n", "            ", "x_", "=", "b_", "\n", "", "x", ".", "append", "(", "x_", ")", "\n", "b_", "=", "(", "A", "@", "b_", ".", "unsqueeze", "(", "-", "1", ")", ")", ".", "squeeze", "(", "-", "1", ")", "\n", "\n", "", "x", "=", "torch", ".", "stack", "(", "x", ",", "dim", "=", "-", "1", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.krylov.krylov": [[51, 94], ["b.unsqueeze", "torch.einsum.contiguous", "torch.eye", "torch.eye", "torch.cat", "torch.cat", "torch.einsum", "torch.einsum"], "function", ["None"], ["", "def", "krylov", "(", "L", ",", "A", ",", "b", ",", "c", "=", "None", ",", "return_power", "=", "False", ")", ":", "\n", "    ", "\"\"\"\n    Compute the Krylov matrix (b, Ab, A^2b, ...) using the squaring trick.\n\n    If return_power=True, return A^{L-1} as well\n    \"\"\"", "\n", "# TODO There is an edge case if L=1 where output doesn't get broadcasted, which might be an issue if caller is expecting broadcasting semantics... can deal with it if it arises", "\n", "\n", "x", "=", "b", ".", "unsqueeze", "(", "-", "1", ")", "# (..., N, 1)", "\n", "A_", "=", "A", "\n", "\n", "AL", "=", "None", "\n", "if", "return_power", ":", "\n", "        ", "AL", "=", "torch", ".", "eye", "(", "A", ".", "shape", "[", "-", "1", "]", ",", "dtype", "=", "A", ".", "dtype", ",", "device", "=", "A", ".", "device", ")", "\n", "_L", "=", "L", "-", "1", "\n", "\n", "", "done", "=", "L", "==", "1", "\n", "# loop invariant: _L represents how many indices left to compute", "\n", "while", "not", "done", ":", "\n", "        ", "if", "return_power", ":", "\n", "            ", "if", "_L", "%", "2", "==", "1", ":", "AL", "=", "A_", "@", "AL", "\n", "_L", "//=", "2", "\n", "\n", "# Save memory on last iteration", "\n", "", "l", "=", "x", ".", "shape", "[", "-", "1", "]", "\n", "if", "L", "-", "l", "<=", "l", ":", "\n", "            ", "done", "=", "True", "\n", "_x", "=", "x", "[", "...", ",", ":", "L", "-", "l", "]", "\n", "", "else", ":", "_x", "=", "x", "\n", "\n", "_x", "=", "A_", "@", "_x", "\n", "x", "=", "torch", ".", "cat", "(", "[", "x", ",", "_x", "]", ",", "dim", "=", "-", "1", ")", "# there might be a more efficient way of ordering axes", "\n", "if", "not", "done", ":", "A_", "=", "A_", "@", "A_", "\n", "\n", "", "assert", "x", ".", "shape", "[", "-", "1", "]", "==", "L", "\n", "\n", "if", "c", "is", "not", "None", ":", "\n", "        ", "x", "=", "torch", ".", "einsum", "(", "'...nl, ...n -> ...l'", ",", "x", ",", "c", ")", "\n", "", "x", "=", "x", ".", "contiguous", "(", ")", "# WOW!!", "\n", "if", "return_power", ":", "\n", "        ", "return", "x", ",", "AL", "\n", "", "else", ":", "\n", "        ", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.krylov.power": [[95, 136], ["torch.eye().to", "torch.eye().to", "powers.append", "einops.rearrange.size", "powers.pop", "einops.rearrange.size", "einops.rearrange", "einops.rearrange.squeeze", "torch.eye", "torch.eye", "powers.pop"], "function", ["None"], ["", "", "def", "power", "(", "L", ",", "A", ",", "v", "=", "None", ")", ":", "\n", "    ", "\"\"\" Compute A^L and the scan sum_i A^i v_i\n\n    A: (..., N, N)\n    v: (..., N, L)\n    \"\"\"", "\n", "\n", "I", "=", "torch", ".", "eye", "(", "A", ".", "shape", "[", "-", "1", "]", ")", ".", "to", "(", "A", ")", "# , dtype=A.dtype, device=A.device)", "\n", "\n", "powers", "=", "[", "A", "]", "\n", "l", "=", "1", "\n", "while", "True", ":", "\n", "        ", "if", "L", "%", "2", "==", "1", ":", "I", "=", "powers", "[", "-", "1", "]", "@", "I", "\n", "L", "//=", "2", "\n", "if", "L", "==", "0", ":", "break", "\n", "l", "*=", "2", "\n", "powers", ".", "append", "(", "powers", "[", "-", "1", "]", "@", "powers", "[", "-", "1", "]", ")", "\n", "\n", "", "if", "v", "is", "None", ":", "return", "I", "\n", "\n", "# Invariants:", "\n", "# powers[-1] := A^l", "\n", "# l := largest po2 at most L", "\n", "\n", "# Note that an alternative divide and conquer to compute the reduction is possible and can be embedded into the above loop without caching intermediate powers of A", "\n", "# We do this reverse divide-and-conquer for efficiency reasons:", "\n", "# 1) it involves fewer padding steps for non-po2 L", "\n", "# 2) it involves more contiguous arrays", "\n", "\n", "# Take care of edge case for non-po2 arrays", "\n", "# Note that this initial step is a no-op for the case of power of 2 (l == L)", "\n", "k", "=", "v", ".", "size", "(", "-", "1", ")", "-", "l", "\n", "v_", "=", "powers", ".", "pop", "(", ")", "@", "v", "[", "...", ",", "l", ":", "]", "\n", "v", "=", "v", "[", "...", ",", ":", "l", "]", "\n", "v", "[", "...", ",", ":", "k", "]", "=", "v", "[", "...", ",", ":", "k", "]", "+", "v_", "\n", "\n", "# Handle reduction for power of 2", "\n", "while", "v", ".", "size", "(", "-", "1", ")", ">", "1", ":", "\n", "        ", "v", "=", "rearrange", "(", "v", ",", "'... (z l) -> ... z l'", ",", "z", "=", "2", ")", "\n", "v", "=", "v", "[", "...", ",", "0", ",", ":", "]", "+", "powers", ".", "pop", "(", ")", "@", "v", "[", "...", ",", "1", ",", ":", "]", "\n", "", "return", "I", ",", "v", ".", "squeeze", "(", "-", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.krylov.krylov_toeplitz": [[137, 161], ["b.unsqueeze", "einops.rearrange.contiguous", "src.models.functional.toeplitz.causal_convolution", "torch.cat", "torch.cat", "src.models.functional.toeplitz.causal_convolution", "torch.einsum", "torch.einsum", "einops.rearrange"], "function", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.toeplitz.causal_convolution", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.toeplitz.causal_convolution"], ["", "def", "krylov_toeplitz", "(", "L", ",", "A", ",", "b", ",", "c", "=", "None", ")", ":", "\n", "    ", "\"\"\" Specializes to lower triangular Toeplitz matrix A represented by its diagonals\n\n    A : (..., N)\n    b : (..., N)\n    c : (..., N)\n\n    Returns\n    x : (..., N, L)\n    x[i, l] = A^l @ b[i]\n    \"\"\"", "\n", "x", "=", "b", ".", "unsqueeze", "(", "0", ")", "# (1, ..., N)", "\n", "A_", "=", "A", "\n", "while", "x", ".", "shape", "[", "0", "]", "<", "L", ":", "\n", "        ", "xx", "=", "causal_convolution", "(", "A_", ",", "x", ")", "\n", "x", "=", "torch", ".", "cat", "(", "[", "x", ",", "xx", "]", ",", "dim", "=", "0", ")", "# there might be a more efficient way of ordering axes", "\n", "A_", "=", "causal_convolution", "(", "A_", ",", "A_", ")", "\n", "", "x", "=", "x", "[", ":", "L", ",", "...", "]", "# (L, ..., N)", "\n", "if", "c", "is", "not", "None", ":", "\n", "        ", "x", "=", "torch", ".", "einsum", "(", "'l...n, ...n -> ...l'", ",", "x", ",", "c", ")", "\n", "", "else", ":", "\n", "        ", "x", "=", "rearrange", "(", "x", ",", "'l ... n -> ... n l'", ")", "\n", "", "x", "=", "x", ".", "contiguous", "(", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.krylov.krylov_toeplitz_": [[162, 196], ["b.unsqueeze", "torch.pad", "torch.pad", "einops.rearrange.contiguous", "torch.fft.rfft", "torch.fft.rfft", "torch.fft.rfft", "torch.fft.rfft", "torch.fft.irfft", "torch.fft.irfft", "torch.cat", "torch.cat", "torch.einsum", "torch.einsum", "einops.rearrange", "torch.fft.irfft", "torch.fft.irfft"], "function", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.sc.pad", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.sc.pad"], ["", "def", "krylov_toeplitz_", "(", "L", ",", "A", ",", "b", ",", "c", "=", "None", ")", ":", "\n", "    ", "\"\"\" Padded version of krylov_toeplitz that saves some fft's\n\n    TODO currently not faster than original version, not sure why\n    \"\"\"", "\n", "N", "=", "A", ".", "shape", "[", "-", "1", "]", "\n", "\n", "x", "=", "b", ".", "unsqueeze", "(", "0", ")", "# (1, ..., N)", "\n", "x", "=", "F", ".", "pad", "(", "x", ",", "(", "0", ",", "N", ")", ")", "\n", "A", "=", "F", ".", "pad", "(", "A", ",", "(", "0", ",", "N", ")", ")", "\n", "done", "=", "L", "==", "1", "\n", "while", "not", "done", ":", "\n", "        ", "l", "=", "x", ".", "shape", "[", "0", "]", "\n", "# Save memory on last iteration", "\n", "if", "L", "-", "l", "<=", "l", ":", "\n", "            ", "done", "=", "True", "\n", "_x", "=", "x", "[", ":", "L", "-", "l", "]", "\n", "", "else", ":", "_x", "=", "x", "\n", "Af", "=", "torch", ".", "fft", ".", "rfft", "(", "A", ",", "n", "=", "2", "*", "N", ",", "dim", "=", "-", "1", ")", "\n", "xf", "=", "torch", ".", "fft", ".", "rfft", "(", "_x", ",", "n", "=", "2", "*", "N", ",", "dim", "=", "-", "1", ")", "\n", "xf_", "=", "Af", "*", "xf", "\n", "x_", "=", "torch", ".", "fft", ".", "irfft", "(", "xf_", ",", "n", "=", "2", "*", "N", ",", "dim", "=", "-", "1", ")", "\n", "x_", "[", "...", ",", "N", ":", "]", "=", "0", "\n", "x", "=", "torch", ".", "cat", "(", "[", "x", ",", "x_", "]", ",", "dim", "=", "0", ")", "# there might be a more efficient way of ordering axes", "\n", "if", "not", "done", ":", "\n", "            ", "A", "=", "torch", ".", "fft", ".", "irfft", "(", "Af", "*", "Af", ",", "n", "=", "2", "*", "N", ",", "dim", "=", "-", "1", ")", "\n", "A", "[", "...", ",", "N", ":", "]", "=", "0", "\n", "", "", "x", "=", "x", "[", ":", "L", ",", "...", ",", ":", "N", "]", "# (L, ..., N)", "\n", "if", "c", "is", "not", "None", ":", "\n", "        ", "x", "=", "torch", ".", "einsum", "(", "'l...n, ...n -> ...l'", ",", "x", ",", "c", ")", "\n", "", "else", ":", "\n", "        ", "x", "=", "rearrange", "(", "x", ",", "'l ... n -> ... n l'", ")", "\n", "", "x", "=", "x", ".", "contiguous", "(", ")", "\n", "return", "x", "\n", "", ""]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.nrde._GetLogsignature.__init__": [[68, 71], ["range", "logsig.size"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "logsig", ")", ":", "\n", "        ", "self", ".", "knots", "=", "range", "(", "logsig", ".", "size", "(", "1", ")", ")", "\n", "self", ".", "logsig", "=", "logsig", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.nrde._GetLogsignature.__getitem__": [[72, 75], ["bisect.bisect"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "t", ")", ":", "\n", "        ", "index", "=", "bisect", ".", "bisect", "(", "self", ".", "knots", ",", "t", ")", "-", "1", "\n", "return", "self", ".", "logsig", "[", ":", ",", "index", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.nrde._NRDECell.__init__": [[84, 88], ["torch.nn.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__"], ["def", "__init__", "(", "self", ",", "logsig_getter", ",", "func", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "logsig_getter", "=", "logsig_getter", "\n", "self", ".", "func", "=", "func", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.nrde._NRDECell.forward": [[89, 93], ["nrde._NRDECell.func", "torch.bmm().squeeze", "torch.bmm", "nrde._NRDECell.logsig_getter[].unsqueeze"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "t", ",", "h", ")", ":", "\n", "        ", "A", "=", "self", ".", "func", "(", "h", ")", "\n", "output", "=", "torch", ".", "bmm", "(", "A", ",", "self", ".", "logsig_getter", "[", "t", "]", ".", "unsqueeze", "(", "2", ")", ")", ".", "squeeze", "(", "2", ")", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.nrde.NeuralRDE.__init__": [[104, 150], ["torch.nn.Module.__init__", "torch.nn.Linear", "nrde._NRDEFunc", "torch.nn.Linear"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__"], ["def", "__init__", "(", "self", ",", "\n", "initial_dim", ",", "\n", "logsig_dim", ",", "\n", "hidden_dim", ",", "\n", "output_dim", ",", "\n", "hidden_hidden_dim", "=", "15", ",", "\n", "num_layers", "=", "3", ",", "\n", "apply_final_linear", "=", "True", ",", "\n", "solver", "=", "'midpoint'", ",", "\n", "adjoint", "=", "False", ",", "\n", "return_sequences", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            initial_dim (int): We use the initial value (t_0 x_0) as an initial condition else we have translation\n                invariance.\n            logsig_dim (int): The dimension of the log-signature.\n            hidden_dim (int): The dimension of the hidden state.\n            output_dim (int): The dimension of the output.\n            hidden_hidden_dim (int): The dimension of the hidden layer in the RNN-like block.\n            num_layers (int): The number of hidden layers in the vector field. Set to 0 for a linear vector field.\n            apply_final_linear (bool): Set False to ignore the final linear output.\n            solver (str): ODE solver, must be implemented in torchdiffeq.\n            adjoint (bool): Set True to use odeint_adjoint.\n            return_sequences (bool): If True will return the linear function on the final layer, else linear function on\n                all layers.\n        \"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "initial_dim", "=", "initial_dim", "\n", "self", ".", "logsig_dim", "=", "logsig_dim", "\n", "self", ".", "hidden_dim", "=", "hidden_dim", "\n", "self", ".", "output_dim", "=", "output_dim", "\n", "self", ".", "hidden_hidden_dim", "=", "hidden_hidden_dim", "\n", "self", ".", "num_layers", "=", "num_layers", "\n", "self", ".", "apply_final_linear", "=", "apply_final_linear", "\n", "self", ".", "solver", "=", "solver", "\n", "self", ".", "adjoint", "=", "adjoint", "\n", "self", ".", "return_sequences", "=", "return_sequences", "\n", "\n", "# Initial to hidden", "\n", "self", ".", "initial_linear", "=", "nn", ".", "Linear", "(", "initial_dim", ",", "hidden_dim", ")", "\n", "\n", "# The net applied to h_prev", "\n", "self", ".", "func", "=", "_NRDEFunc", "(", "hidden_dim", ",", "logsig_dim", ",", "hidden_dim", "=", "hidden_hidden_dim", ",", "num_layers", "=", "num_layers", ")", "\n", "\n", "# Linear classifier to apply to final layer", "\n", "self", ".", "final_linear", "=", "nn", ".", "Linear", "(", "self", ".", "hidden_dim", ",", "self", ".", "output_dim", ")", "if", "apply_final_linear", "else", "lambda", "x", ":", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.nrde.NeuralRDE.forward": [[151, 166], ["nrde.NeuralRDE.initial_linear", "nrde.rdeint", "len", "nrde.NeuralRDE.final_linear", "nrde.NeuralRDE.final_linear"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.nrde.rdeint"], ["", "def", "forward", "(", "self", ",", "inputs", ")", ":", "\n", "# Setup the inital hidden layer", "\n", "        ", "assert", "len", "(", "inputs", ")", "==", "2", ",", "\"`inputs` must be a 2-tuple containing `(inital_values, logsig)`.\"", "\n", "initial", ",", "logsig", "=", "inputs", "\n", "h0", "=", "self", ".", "initial_linear", "(", "initial", ")", "\n", "\n", "# Perform the adjoint operation", "\n", "out", "=", "rdeint", "(", "\n", "logsig", ",", "h0", ",", "self", ".", "func", ",", "method", "=", "self", ".", "solver", ",", "adjoint", "=", "self", ".", "adjoint", ",", "return_sequences", "=", "self", ".", "return_sequences", "\n", ")", "\n", "\n", "# Outputs", "\n", "outputs", "=", "self", ".", "final_linear", "(", "out", "[", ":", ",", "-", "1", ",", ":", "]", ")", "if", "not", "self", ".", "return_sequences", "else", "self", ".", "final_linear", "(", "out", ")", "\n", "\n", "return", "outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.nrde._NRDEFunc.__init__": [[176, 193], ["torch.nn.Module.__init__", "torch.nn.Sequential", "torch.nn.Linear", "torch.nn.ReLU", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Tanh", "torch.nn.Linear"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__"], ["def", "__init__", "(", "self", ",", "input_dim", ",", "logsig_dim", ",", "num_layers", "=", "1", ",", "hidden_dim", "=", "15", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "input_dim", "=", "input_dim", "\n", "self", ".", "logsig_dim", "=", "logsig_dim", "\n", "self", ".", "hidden_dim", "=", "hidden_dim", "\n", "self", ".", "num_layers", "=", "num_layers", "\n", "\n", "# Additional layers are just hidden to hidden with relu activation", "\n", "additional_layers", "=", "[", "nn", ".", "ReLU", "(", ")", ",", "nn", ".", "Linear", "(", "hidden_dim", ",", "hidden_dim", ")", "]", "*", "(", "num_layers", "-", "1", ")", "if", "num_layers", ">", "1", "else", "[", "]", "\n", "\n", "# The net applied to h_prev", "\n", "self", ".", "net", "=", "nn", ".", "Sequential", "(", "*", "[", "\n", "nn", ".", "Linear", "(", "input_dim", ",", "hidden_dim", ")", ",", "\n", "*", "additional_layers", ",", "\n", "nn", ".", "Tanh", "(", ")", ",", "\n", "nn", ".", "Linear", "(", "hidden_dim", ",", "input_dim", "*", "logsig_dim", ")", ",", "\n", "]", ")", "if", "num_layers", ">", "0", "else", "nn", ".", "Linear", "(", "input_dim", ",", "input_dim", "*", "logsig_dim", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.nrde._NRDEFunc.forward": [[194, 196], ["nrde._NRDEFunc.net().view", "nrde._NRDEFunc.net"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "h", ")", ":", "\n", "        ", "return", "self", ".", "net", "(", "h", ")", ".", "view", "(", "-", "1", ",", "self", ".", "input_dim", ",", "self", ".", "logsig_dim", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.nrde.rdeint": [[9, 37], ["nrde._GetLogsignature", "nrde._NRDECell", "nrde.set_options", "odeint_func().transpose", "odeint_func"], "function", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.nrde.set_options"], ["def", "rdeint", "(", "logsig", ",", "h0", ",", "func", ",", "method", "=", "'rk4'", ",", "adjoint", "=", "False", ",", "return_sequences", "=", "False", ")", ":", "\n", "    ", "\"\"\"Analogous to odeint but for RDEs.\n    Note that we do not have time intervals here. This is because the log-ode method is always evaluated on [0, 1] and\n    thus are grid is always [0, 1, ..., num_intervals+1].\n    Args:\n        logsig (torch.Tensor): A tensor of logsignature of shape [N, L, logsig_dim]\n        h0 (torch.Tensor): The initial value of the hidden state.\n        func (nn.Module): The function to apply to the state h0.\n        method (str): The solver to use.\n        adjoint (bool): Set True to use the adjoint method.\n        return_sequences (bool): Set True to return a prediction at each step, else return just terminal time.\n    Returns:\n        torch.Tensor: The values of the hidden states at the specified times. This has shape [N, L, num_hidden].\n    \"\"\"", "\n", "# Method to get the logsig value", "\n", "logsig_getter", "=", "_GetLogsignature", "(", "logsig", ")", "\n", "\n", "# A cell to apply the output of the function linearly to correct log-signature piece.", "\n", "cell", "=", "_NRDECell", "(", "logsig_getter", ",", "func", ")", "\n", "\n", "# Set options", "\n", "t", ",", "options", ",", "=", "set_options", "(", "logsig", ",", "return_sequences", "=", "return_sequences", ")", "\n", "\n", "# Solve", "\n", "odeint_func", "=", "odeint_adjoint", "if", "adjoint", "else", "odeint", "\n", "output", "=", "odeint_func", "(", "func", "=", "cell", ",", "y0", "=", "h0", ",", "t", "=", "t", ",", "method", "=", "method", ",", "options", "=", "options", ")", ".", "transpose", "(", "0", ",", "1", ")", "\n", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.nrde.set_options": [[39, 58], ["logsig.size", "torch.arange().to", "torch.Tensor().to", "torch.arange", "torch.Tensor"], "function", ["None"], ["", "def", "set_options", "(", "logsig", ",", "return_sequences", "=", "False", ",", "eps", "=", "1e-5", ")", ":", "\n", "    ", "\"\"\"Sets the options to be passed to the relevant `odeint` function.\n    Args:\n        logsig (torch.Tensor): The logsignature of the path.\n        return_sequences (bool): Set True if a regression problem where we need the full sequence. This requires us\n            specifying the time grid as `torch.arange(0, T_final)` which is less memory efficient that specifying\n            the times `t = torch.Tensor([0, T_final])` along with an `step_size=1` in the options.\n        eps (float): The epsilon perturbation to make to integration points to distinguish the ends.\n    Returns:\n        torch.Tensor, dict: The integration times and the options dictionary.\n    \"\"\"", "\n", "length", "=", "logsig", ".", "size", "(", "1", ")", "+", "1", "\n", "if", "return_sequences", ":", "\n", "        ", "t", "=", "torch", ".", "arange", "(", "0", ",", "length", ",", "dtype", "=", "torch", ".", "float", ")", ".", "to", "(", "logsig", ".", "device", ")", "\n", "options", "=", "{", "'eps'", ":", "eps", "}", "\n", "", "else", ":", "\n", "        ", "options", "=", "{", "'step_size'", ":", "1", ",", "'eps'", ":", "eps", "}", "\n", "t", "=", "torch", ".", "Tensor", "(", "[", "0", ",", "length", "]", ")", ".", "to", "(", "logsig", ".", "device", ")", "\n", "", "return", "t", ",", "options", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.unicornn.UnICORNN_compile.__init__": [[122, 124], ["super().__init__"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__"], ["", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "super", "(", "UnICORNN_compile", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.unicornn.UnICORNN_compile.compile_functions": [[125, 137], ["torch.cuda.current_device", "torch.cuda.current_device", "torch.cuda.current_device", "torch.cuda.current_device", "function.Module", "function.Module.load", "function.Module.get_function", "function.Module.get_function", "collections.namedtuple", "collections.namedtuple.", "bytes", "unicornn.UnICORNN_compile._UnICORNN_PTX.encode", "torch.cuda.current_stream", "torch.cuda.current_stream", "torch.cuda.current_stream", "torch.cuda.current_stream"], "methods", ["None"], ["", "def", "compile_functions", "(", "self", ")", ":", "\n", "        ", "device", "=", "torch", ".", "cuda", ".", "current_device", "(", ")", "\n", "mod", "=", "function", ".", "Module", "(", ")", "\n", "mod", ".", "load", "(", "bytes", "(", "self", ".", "_UnICORNN_PTX", ".", "encode", "(", ")", ")", ")", "\n", "fwd_func", "=", "mod", ".", "get_function", "(", "\"unicornn_fwd\"", ")", "\n", "bwd_func", "=", "mod", ".", "get_function", "(", "\"unicornn_bwd\"", ")", "\n", "\n", "Stream", "=", "namedtuple", "(", "\"Stream\"", ",", "[", "\"ptr\"", "]", ")", "\n", "current_stream", "=", "Stream", "(", "ptr", "=", "torch", ".", "cuda", ".", "current_stream", "(", ")", ".", "cuda_stream", ")", "\n", "\n", "self", ".", "_DEVICE2FUNC", "[", "device", "]", "=", "(", "current_stream", ",", "fwd_func", ",", "bwd_func", ")", "\n", "return", "current_stream", ",", "fwd_func", ",", "bwd_func", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.unicornn.UnICORNN_compile.get_functions": [[138, 141], ["unicornn.UnICORNN_compile._DEVICE2FUNC.get", "torch.cuda.current_device", "torch.cuda.current_device", "torch.cuda.current_device", "torch.cuda.current_device", "unicornn.UnICORNN_compile.compile_functions"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.unicornn.UnICORNN_compile.compile_functions"], ["", "def", "get_functions", "(", "self", ")", ":", "\n", "        ", "res", "=", "self", ".", "_DEVICE2FUNC", ".", "get", "(", "torch", ".", "cuda", ".", "current_device", "(", ")", ",", "None", ")", "\n", "return", "res", "if", "res", "else", "self", ".", "compile_functions", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.unicornn.UnICORNN_Compute_GPU.forward": [[144, 185], ["unicornn.UnICORNN_compile", "x.size", "x.size", "min", "x.new", "x.new", "x.new", "unicornn.UnICORNN_compile.get_functions", "FUNC", "ctx.save_for_backward", "x.size", "x.dim", "x.dim", "x.contiguous().data_ptr", "weight_hh.contiguous().data_ptr", "hy_initial.contiguous().data_ptr", "hz_initial.contiguous().data_ptr", "x.new.contiguous().data_ptr", "x.new.contiguous().data_ptr", "c.contiguous().data_ptr", "dt.item", "alpha.item", "x.new.contiguous().data_ptr", "x.contiguous", "weight_hh.contiguous", "hy_initial.contiguous", "hz_initial.contiguous", "x.new.contiguous", "x.new.contiguous", "c.contiguous", "x.new.contiguous"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.unicornn.UnICORNN_compile.get_functions"], ["    ", "@", "staticmethod", "\n", "def", "forward", "(", "ctx", ",", "x", ",", "weight_hh", ",", "hy_initial", ",", "hz_initial", ",", "c", ",", "alpha", ",", "dt", ")", ":", "\n", "        ", "comp", "=", "UnICORNN_compile", "(", ")", "\n", "length", "=", "x", ".", "size", "(", "0", ")", "if", "x", ".", "dim", "(", ")", "==", "3", "else", "1", "\n", "batch", "=", "x", ".", "size", "(", "-", "2", ")", "\n", "d_model", "=", "x", ".", "size", "(", "-", "1", ")", "\n", "ncols", "=", "batch", "*", "d_model", "\n", "thread_per_block", "=", "min", "(", "512", ",", "ncols", ")", "\n", "num_block", "=", "(", "ncols", "-", "1", ")", "//", "thread_per_block", "+", "1", "\n", "\n", "size", "=", "(", "length", ",", "batch", ",", "d_model", ")", "if", "x", ".", "dim", "(", ")", "==", "3", "else", "(", "batch", ",", "d_model", ")", "\n", "hy_all", "=", "x", ".", "new", "(", "*", "size", ")", "\n", "\n", "hy_final", "=", "x", ".", "new", "(", "batch", ",", "d_model", ")", "\n", "hz_final", "=", "x", ".", "new", "(", "batch", ",", "d_model", ")", "\n", "\n", "stream", ",", "fwd_func", ",", "_", "=", "comp", ".", "get_functions", "(", ")", "\n", "FUNC", "=", "fwd_func", "\n", "FUNC", "(", "\n", "args", "=", "[", "\n", "x", ".", "contiguous", "(", ")", ".", "data_ptr", "(", ")", ",", "\n", "weight_hh", ".", "contiguous", "(", ")", ".", "data_ptr", "(", ")", ",", "\n", "hy_initial", ".", "contiguous", "(", ")", ".", "data_ptr", "(", ")", ",", "\n", "hz_initial", ".", "contiguous", "(", ")", ".", "data_ptr", "(", ")", ",", "\n", "hy_final", ".", "contiguous", "(", ")", ".", "data_ptr", "(", ")", ",", "\n", "hz_final", ".", "contiguous", "(", ")", ".", "data_ptr", "(", ")", ",", "\n", "length", ",", "\n", "batch", ",", "\n", "d_model", ",", "\n", "c", ".", "contiguous", "(", ")", ".", "data_ptr", "(", ")", ",", "\n", "dt", ".", "item", "(", ")", ",", "\n", "alpha", ".", "item", "(", ")", ",", "\n", "hy_all", ".", "contiguous", "(", ")", ".", "data_ptr", "(", ")", ",", "\n", "]", ",", "\n", "block", "=", "(", "thread_per_block", ",", "1", ",", "1", ")", ",", "\n", "grid", "=", "(", "num_block", ",", "1", ",", "1", ")", ",", "\n", "stream", "=", "stream", ",", "\n", ")", "\n", "\n", "ctx", ".", "save_for_backward", "(", "x", ",", "weight_hh", ",", "hy_final", ",", "hz_final", ",", "c", ",", "alpha", ",", "dt", ")", "\n", "return", "hy_all", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.unicornn.UnICORNN_Compute_GPU.backward": [[186, 226], ["unicornn.UnICORNN_compile", "x.size", "x.size", "min", "x.new", "x.new().zero_", "x.new().zero_", "unicornn.UnICORNN_compile.get_functions", "FUNC", "x.size", "x.dim", "x.size", "x.new", "x.new", "x.contiguous().data_ptr", "weight_hh.contiguous().data_ptr", "hy_final.contiguous().data_ptr", "hz_final.contiguous().data_ptr", "grad_h.contiguous().data_ptr", "c.contiguous().data_ptr", "dt.item", "alpha.item", "x.new.contiguous().data_ptr", "x.new().zero_.contiguous().data_ptr", "x.new().zero_.contiguous().data_ptr", "x.contiguous", "weight_hh.contiguous", "hy_final.contiguous", "hz_final.contiguous", "grad_h.contiguous", "c.contiguous", "x.new.contiguous", "x.new().zero_.contiguous", "x.new().zero_.contiguous"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.unicornn.UnICORNN_compile.get_functions"], ["", "@", "staticmethod", "\n", "def", "backward", "(", "ctx", ",", "grad_h", ")", ":", "\n", "        ", "x", ",", "weight_hh", ",", "hy_final", ",", "hz_final", ",", "c", ",", "alpha", ",", "dt", "=", "ctx", ".", "saved_tensors", "\n", "comp", "=", "UnICORNN_compile", "(", ")", "\n", "length", "=", "x", ".", "size", "(", "0", ")", "if", "x", ".", "dim", "(", ")", "==", "3", "else", "1", "\n", "batch", "=", "x", ".", "size", "(", "-", "2", ")", "\n", "d_model", "=", "x", ".", "size", "(", "-", "1", ")", "\n", "ncols", "=", "batch", "*", "d_model", "\n", "thread_per_block", "=", "min", "(", "256", ",", "ncols", ")", "\n", "num_block", "=", "(", "ncols", "-", "1", ")", "//", "thread_per_block", "+", "1", "\n", "\n", "grad_x", "=", "x", ".", "new", "(", "*", "x", ".", "size", "(", ")", ")", "\n", "grad_weight_hh", "=", "x", ".", "new", "(", "d_model", ")", ".", "zero_", "(", ")", "\n", "grad_c", "=", "x", ".", "new", "(", "d_model", ")", ".", "zero_", "(", ")", "\n", "\n", "stream", ",", "_", ",", "bwd_func", "=", "comp", ".", "get_functions", "(", ")", "\n", "FUNC", "=", "bwd_func", "\n", "FUNC", "(", "\n", "args", "=", "[", "\n", "x", ".", "contiguous", "(", ")", ".", "data_ptr", "(", ")", ",", "\n", "weight_hh", ".", "contiguous", "(", ")", ".", "data_ptr", "(", ")", ",", "\n", "hy_final", ".", "contiguous", "(", ")", ".", "data_ptr", "(", ")", ",", "\n", "hz_final", ".", "contiguous", "(", ")", ".", "data_ptr", "(", ")", ",", "\n", "grad_h", ".", "contiguous", "(", ")", ".", "data_ptr", "(", ")", ",", "\n", "length", ",", "\n", "batch", ",", "\n", "d_model", ",", "\n", "c", ".", "contiguous", "(", ")", ".", "data_ptr", "(", ")", ",", "\n", "dt", ".", "item", "(", ")", ",", "\n", "alpha", ".", "item", "(", ")", ",", "\n", "grad_x", ".", "contiguous", "(", ")", ".", "data_ptr", "(", ")", ",", "\n", "grad_weight_hh", ".", "contiguous", "(", ")", ".", "data_ptr", "(", ")", ",", "\n", "grad_c", ".", "contiguous", "(", ")", ".", "data_ptr", "(", ")", ",", "\n", "]", ",", "\n", "block", "=", "(", "thread_per_block", ",", "1", ",", "1", ")", ",", "\n", "grid", "=", "(", "num_block", ",", "1", ",", "1", ")", ",", "\n", "stream", "=", "stream", ",", "\n", ")", "\n", "\n", "return", "grad_x", ",", "grad_weight_hh", ",", "None", ",", "None", ",", "grad_c", ",", "None", ",", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.unicornn.UnICORNN_recurrence.__init__": [[229, 237], ["torch.Module.__init__", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.nn.Parameter", "torch.nn.Parameter", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.nn.Parameter", "torch.nn.Parameter", "unicornn.UnICORNN_recurrence.reset_parameters", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.cells.memory.MemoryCell.reset_parameters"], ["    ", "def", "__init__", "(", "self", ",", "d_model", ",", "dt", ",", "alpha", ")", ":", "\n", "        ", "super", "(", "UnICORNN_recurrence", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "d_model", "=", "d_model", "\n", "self", ".", "dt", "=", "torch", ".", "tensor", "(", "dt", ")", "\n", "self", ".", "c_", "=", "Parameter", "(", "torch", ".", "Tensor", "(", "d_model", ")", ")", "\n", "self", ".", "alpha", "=", "torch", ".", "tensor", "(", "alpha", ")", "\n", "self", ".", "weight_hh", "=", "Parameter", "(", "torch", ".", "Tensor", "(", "d_model", ")", ")", "\n", "self", ".", "reset_parameters", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.unicornn.UnICORNN_recurrence.reset_parameters": [[238, 244], ["unicornn.UnICORNN_recurrence.named_parameters", "torch.init.uniform_", "torch.init.uniform_", "torch.init.uniform_", "torch.init.uniform_"], "methods", ["None"], ["", "def", "reset_parameters", "(", "self", ")", ":", "\n", "        ", "for", "name", ",", "weight", "in", "self", ".", "named_parameters", "(", ")", ":", "\n", "            ", "if", "\"weight_hh\"", "in", "name", ":", "\n", "                ", "nn", ".", "init", ".", "uniform_", "(", "weight", ",", "a", "=", "0", ",", "b", "=", "1", ")", "\n", "", "if", "\"c_\"", "in", "name", ":", "\n", "                ", "nn", ".", "init", ".", "uniform_", "(", "weight", ",", "a", "=", "-", "0.1", ",", "b", "=", "0.1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.unicornn.UnICORNN_recurrence.forward": [[245, 252], ["UnICORNN_Compute_GPU.apply", "input.data.new().zero_", "input.data.new().zero_", "input.data.new", "input.data.new", "input.size", "input.size", "input.size", "input.size"], "methods", ["None"], ["", "", "", "def", "forward", "(", "self", ",", "input", ")", ":", "\n", "        ", "hy0", ",", "hz0", "=", "(", "\n", "input", ".", "data", ".", "new", "(", "input", ".", "size", "(", "-", "2", ")", ",", "input", ".", "size", "(", "-", "1", ")", ")", ".", "zero_", "(", ")", ",", "\n", "input", ".", "data", ".", "new", "(", "input", ".", "size", "(", "-", "2", ")", ",", "input", ".", "size", "(", "-", "1", ")", ")", ".", "zero_", "(", ")", ",", "\n", ")", "\n", "return", "UnICORNN_Compute_GPU", ".", "apply", "(", "\n", "input", ",", "self", ".", "weight_hh", ",", "hy0", ",", "hz0", ",", "self", ".", "c_", ",", "self", ".", "alpha", ",", "self", ".", "dt", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.unicornn.Dropout_overtime.forward": [[256, 267], ["input.clone", "input.data.new", "ctx.save_for_backward", "input.size", "input.size", "noise.unsqueeze().expand_as.unsqueeze().expand_as.bernoulli_().div_", "noise.unsqueeze().expand_as.unsqueeze().expand_as.unsqueeze().expand_as", "input.clone.mul_", "noise.unsqueeze().expand_as.unsqueeze().expand_as.bernoulli_", "noise.unsqueeze().expand_as.unsqueeze().expand_as.unsqueeze"], "methods", ["None"], ["    ", "@", "staticmethod", "\n", "def", "forward", "(", "ctx", ",", "input", ",", "p", "=", "0.5", ",", "training", "=", "False", ")", ":", "\n", "        ", "output", "=", "input", ".", "clone", "(", ")", "\n", "noise", "=", "input", ".", "data", ".", "new", "(", "input", ".", "size", "(", "-", "2", ")", ",", "input", ".", "size", "(", "-", "1", ")", ")", "\n", "if", "training", ":", "\n", "            ", "noise", ".", "bernoulli_", "(", "1", "-", "p", ")", ".", "div_", "(", "1", "-", "p", ")", "\n", "noise", "=", "noise", ".", "unsqueeze", "(", "0", ")", ".", "expand_as", "(", "input", ")", "\n", "output", ".", "mul_", "(", "noise", ")", "\n", "", "ctx", ".", "save_for_backward", "(", "noise", ")", "\n", "ctx", ".", "training", "=", "training", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.unicornn.Dropout_overtime.backward": [[268, 275], ["grad_output.mul"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "backward", "(", "ctx", ",", "grad_output", ")", ":", "\n", "        ", "(", "noise", ",", ")", "=", "ctx", ".", "saved_tensors", "\n", "if", "ctx", ".", "training", ":", "\n", "            ", "return", "grad_output", ".", "mul", "(", "noise", ")", ",", "None", ",", "None", "\n", "", "else", ":", "\n", "            ", "return", "grad_output", ",", "None", ",", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.unicornn.LinearInitovertime.__init__": [[281, 286], ["torch.Module.__init__", "torch.Linear", "torch.Linear"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__"], ["    ", "def", "__init__", "(", "self", ",", "d_input", ",", "d_model", ",", "bias", "=", "True", ")", ":", "\n", "        ", "super", "(", "LinearInitovertime", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "fc", "=", "nn", ".", "Linear", "(", "d_input", ",", "d_model", ",", "bias", "=", "bias", ")", "\n", "self", ".", "d_input", "=", "d_input", "\n", "self", ".", "d_model", "=", "d_model", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.unicornn.LinearInitovertime.forward": [[287, 292], ["x.contiguous().view", "unicornn.LinearInitovertime.fc", "y.view.view.view", "x.contiguous", "x.size", "x.size"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "y", "=", "x", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ",", "self", ".", "d_input", ")", "\n", "y", "=", "self", ".", "fc", "(", "y", ")", "\n", "y", "=", "y", ".", "view", "(", "x", ".", "size", "(", ")", "[", "0", "]", ",", "x", ".", "size", "(", ")", "[", "1", "]", ",", "self", ".", "d_model", ")", "\n", "return", "y", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.unicornn.UnICORNN.__init__": [[295, 332], ["torch.Module.__init__", "torch.ModuleList", "torch.ModuleList", "range", "unicornn.UnICORNN.init_weights", "range", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ModuleList", "ImportError", "unicornn.LinearInitovertime", "unicornn.UnICORNN.DIs.append", "unicornn.UnICORNN_recurrence", "unicornn.UnICORNN.RNNs.append"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.unicornn.UnICORNN.init_weights"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "# d_input,", "\n", "# d_output,", "\n", "# l_output,", "\n", "d_model", ",", "\n", "dt", ",", "\n", "alpha", ",", "\n", "n_layers", ",", "\n", "drop", "=", "0.1", ",", "\n", "**", "kwargs", "\n", ")", ":", "\n", "        ", "if", "not", "_unicornn_available", ":", "\n", "            ", "raise", "ImportError", "(", "\n", "\"Check unicornn codebase for install instructions. Requires cupy and pynvrtc.\"", "\n", ")", "\n", "\n", "", "super", "(", "UnICORNN", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "d_model", "=", "d_model", "\n", "self", ".", "d_output", "=", "d_model", "\n", "self", ".", "drop", "=", "drop", "\n", "self", ".", "nlayers", "=", "n_layers", "\n", "# self.l_output = l_output", "\n", "self", ".", "DIs", "=", "nn", ".", "ModuleList", "(", ")", "\n", "# denseinput = LinearInitovertime(d_input, nhid)", "\n", "# self.DIs.append(denseinput)", "\n", "# for x in range(self.nlayers - 1):", "\n", "for", "x", "in", "range", "(", "self", ".", "nlayers", ")", ":", "\n", "            ", "denseinput", "=", "LinearInitovertime", "(", "d_model", ",", "d_model", ")", "\n", "self", ".", "DIs", ".", "append", "(", "denseinput", ")", "\n", "# self.classifier = nn.Linear(nhid, d_output)", "\n", "", "self", ".", "init_weights", "(", ")", "\n", "self", ".", "RNNs", "=", "[", "]", "\n", "for", "x", "in", "range", "(", "self", ".", "nlayers", ")", ":", "\n", "            ", "rnn", "=", "UnICORNN_recurrence", "(", "d_model", ",", "dt", "[", "x", "]", ",", "alpha", ")", "\n", "self", ".", "RNNs", ".", "append", "(", "rnn", ")", "\n", "", "self", ".", "RNNs", "=", "torch", ".", "nn", ".", "ModuleList", "(", "self", ".", "RNNs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.unicornn.UnICORNN.init_weights": [[333, 341], ["unicornn.UnICORNN.named_parameters", "torch.init.kaiming_uniform_", "torch.init.kaiming_uniform_", "param.data.fill_"], "methods", ["None"], ["", "def", "init_weights", "(", "self", ")", ":", "\n", "        ", "for", "name", ",", "param", "in", "self", ".", "named_parameters", "(", ")", ":", "\n", "            ", "if", "(", "\"fc\"", "in", "name", ")", "and", "\"weight\"", "in", "name", ":", "\n", "                ", "nn", ".", "init", ".", "kaiming_uniform_", "(", "param", ",", "a", "=", "8", ",", "mode", "=", "\"fan_in\"", ")", "\n", "# if \"classifier\" in name and \"weight\" in name:", "\n", "#     nn.init.kaiming_normal_(param.data)", "\n", "", "if", "\"bias\"", "in", "name", ":", "\n", "                ", "param", ".", "data", ".", "fill_", "(", "0.0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.unicornn.UnICORNN.forward": [[342, 368], ["input.transpose.transpose.transpose", "range", "output.transpose.transpose.transpose", "len", "dropout_overtime", "len"], "methods", ["None"], ["", "", "", "def", "forward", "(", "self", ",", "input", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "\n", "        ", "input", "=", "input", ".", "transpose", "(", "0", ",", "1", ")", "\n", "\n", "rnnoutputs", "=", "{", "}", "\n", "rnnoutputs", "[", "\"outlayer-1\"", "]", "=", "input", "\n", "for", "x", "in", "range", "(", "len", "(", "self", ".", "RNNs", ")", ")", ":", "\n", "            ", "rnnoutputs", "[", "\"dilayer%d\"", "%", "x", "]", "=", "self", ".", "DIs", "[", "x", "]", "(", "\n", "rnnoutputs", "[", "\"outlayer%d\"", "%", "(", "x", "-", "1", ")", "]", "\n", ")", "\n", "rnnoutputs", "[", "\"outlayer%d\"", "%", "x", "]", "=", "self", ".", "RNNs", "[", "x", "]", "(", "rnnoutputs", "[", "\"dilayer%d\"", "%", "x", "]", ")", "\n", "rnnoutputs", "[", "\"outlayer%d\"", "%", "x", "]", "=", "dropout_overtime", "(", "\n", "rnnoutputs", "[", "\"outlayer%d\"", "%", "x", "]", ",", "self", ".", "drop", ",", "self", ".", "training", "\n", ")", "\n", "\n", "# temp = rnnoutputs[\"outlayer%d\" % (len(self.RNNs) - 1)][-1]", "\n", "# output = self.classifier(temp)", "\n", "", "output", "=", "rnnoutputs", "[", "\"outlayer%d\"", "%", "(", "len", "(", "self", ".", "RNNs", ")", "-", "1", ")", "]", "\n", "output", "=", "output", ".", "transpose", "(", "0", ",", "1", ")", "\n", "\n", "# if self.l_output == 0:", "\n", "#     output = output[:, -1]", "\n", "# else:", "\n", "#     output = output[:, -self.l_output :]", "\n", "\n", "return", "output", "\n", "", "", ""]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.odelstm.ODELSTMCell.__init__": [[12, 36], ["torch.Module.__init__", "solver_type.startswith", "torch.LSTMCell", "torch.LSTMCell", "torch.Sequential", "torch.Sequential", "torch.Linear", "torch.Linear", "torch.Tanh", "torch.Tanh", "torch.Linear", "torch.Linear", "torchdyn.models.NeuralDE", "ValueError", "options.keys"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__"], ["    ", "def", "__init__", "(", "self", ",", "d_input", ",", "d_model", ",", "solver_type", "=", "\"dopri5\"", ")", ":", "\n", "        ", "super", "(", "ODELSTMCell", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "solver_type", "=", "solver_type", "\n", "self", ".", "fixed_step_solver", "=", "solver_type", ".", "startswith", "(", "\"fixed_\"", ")", "\n", "self", ".", "lstm", "=", "nn", ".", "LSTMCell", "(", "d_input", ",", "d_model", ")", "\n", "# 1 hidden layer NODE", "\n", "self", ".", "f_node", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Linear", "(", "d_model", ",", "d_model", ")", ",", "\n", "nn", ".", "Tanh", "(", ")", ",", "\n", "nn", ".", "Linear", "(", "d_model", ",", "d_model", ")", ",", "\n", ")", "\n", "self", ".", "d_input", "=", "d_input", "\n", "self", ".", "d_model", "=", "d_model", "\n", "if", "not", "self", ".", "fixed_step_solver", ":", "\n", "            ", "self", ".", "node", "=", "NeuralDE", "(", "self", ".", "f_node", ",", "solver", "=", "solver_type", ")", "\n", "", "else", ":", "\n", "            ", "options", "=", "{", "\n", "\"fixed_euler\"", ":", "self", ".", "euler", ",", "\n", "\"fixed_heun\"", ":", "self", ".", "heun", ",", "\n", "\"fixed_rk4\"", ":", "self", ".", "rk4", ",", "\n", "}", "\n", "if", "not", "solver_type", "in", "options", ".", "keys", "(", ")", ":", "\n", "                ", "raise", "ValueError", "(", "\"Unknown solver type '{:}'\"", ".", "format", "(", "solver_type", ")", ")", "\n", "", "self", ".", "node", "=", "options", "[", "self", ".", "solver_type", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.odelstm.ODELSTMCell.forward": [[37, 52], ["odelstm.ODELSTMCell.lstm", "odelstm.ODELSTMCell.solve_fixed", "torch.argsort", "torch.argsort", "torch.argsort", "torch.argsort", "ts.size", "odelstm.ODELSTMCell.node.trajectory", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.arange", "torch.arange", "torch.arange", "torch.arange"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.odelstm.ODELSTMCell.solve_fixed"], ["", "", "def", "forward", "(", "self", ",", "input", ",", "hx", ",", "ts", ")", ":", "\n", "        ", "new_h", ",", "new_c", "=", "self", ".", "lstm", "(", "input", ",", "hx", ")", "\n", "if", "self", ".", "fixed_step_solver", ":", "\n", "            ", "new_h", "=", "self", ".", "solve_fixed", "(", "new_h", ",", "ts", ")", "\n", "", "else", ":", "\n", "            ", "indices", "=", "torch", ".", "argsort", "(", "ts", ")", "\n", "batch_size", "=", "ts", ".", "size", "(", "0", ")", "\n", "device", "=", "input", ".", "device", "\n", "s_sort", "=", "ts", "[", "indices", "]", "\n", "s_sort", "=", "s_sort", "+", "torch", ".", "linspace", "(", "0", ",", "1e-4", ",", "batch_size", ",", "device", "=", "device", ")", "\n", "# HACK: Make sure no two points are equal", "\n", "trajectory", "=", "self", ".", "node", ".", "trajectory", "(", "new_h", ",", "s_sort", ")", "\n", "new_h", "=", "trajectory", "[", "indices", ",", "torch", ".", "arange", "(", "batch_size", ",", "device", "=", "device", ")", "]", "\n", "\n", "", "return", "(", "new_h", ",", "new_c", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.odelstm.ODELSTMCell.solve_fixed": [[53, 58], ["ts.view.view.view", "range", "odelstm.ODELSTMCell.node"], "methods", ["None"], ["", "def", "solve_fixed", "(", "self", ",", "x", ",", "ts", ")", ":", "\n", "        ", "ts", "=", "ts", ".", "view", "(", "-", "1", ",", "1", ")", "\n", "for", "i", "in", "range", "(", "3", ")", ":", "# 3 unfolds", "\n", "            ", "x", "=", "self", ".", "node", "(", "x", ",", "ts", "*", "(", "1.0", "/", "3", ")", ")", "\n", "", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.odelstm.ODELSTMCell.euler": [[59, 62], ["odelstm.ODELSTMCell.f_node"], "methods", ["None"], ["", "def", "euler", "(", "self", ",", "y", ",", "delta_t", ")", ":", "\n", "        ", "dy", "=", "self", ".", "f_node", "(", "y", ")", "\n", "return", "y", "+", "delta_t", "*", "dy", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.odelstm.ODELSTMCell.heun": [[63, 67], ["odelstm.ODELSTMCell.f_node", "odelstm.ODELSTMCell.f_node"], "methods", ["None"], ["", "def", "heun", "(", "self", ",", "y", ",", "delta_t", ")", ":", "\n", "        ", "k1", "=", "self", ".", "f_node", "(", "y", ")", "\n", "k2", "=", "self", ".", "f_node", "(", "y", "+", "delta_t", "*", "k1", ")", "\n", "return", "y", "+", "delta_t", "*", "0.5", "*", "(", "k1", "+", "k2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.odelstm.ODELSTMCell.rk4": [[68, 75], ["odelstm.ODELSTMCell.f_node", "odelstm.ODELSTMCell.f_node", "odelstm.ODELSTMCell.f_node", "odelstm.ODELSTMCell.f_node"], "methods", ["None"], ["", "def", "rk4", "(", "self", ",", "y", ",", "delta_t", ")", ":", "\n", "        ", "k1", "=", "self", ".", "f_node", "(", "y", ")", "\n", "k2", "=", "self", ".", "f_node", "(", "y", "+", "k1", "*", "delta_t", "*", "0.5", ")", "\n", "k3", "=", "self", ".", "f_node", "(", "y", "+", "k2", "*", "delta_t", "*", "0.5", ")", "\n", "k4", "=", "self", ".", "f_node", "(", "y", "+", "k3", "*", "delta_t", ")", "\n", "\n", "return", "y", "+", "delta_t", "*", "(", "k1", "+", "2", "*", "k2", "+", "2", "*", "k3", "+", "k4", ")", "/", "6.0", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.odelstm.ODELSTM.__init__": [[78, 96], ["torch.Module.__init__", "odelstm.ODELSTMCell", "torch.Linear", "torch.Linear"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "d_input", ",", "\n", "d_output", ",", "\n", "d_model", ",", "\n", "return_sequences", "=", "True", ",", "\n", "solver_type", "=", "\"dopri5\"", ",", "\n", "l_output", "=", "None", ",", "\n", "l_max", "=", "None", ",", "\n", ")", ":", "\n", "        ", "super", "(", "ODELSTM", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "d_input", "=", "d_input", "\n", "self", ".", "d_model", "=", "d_model", "\n", "self", ".", "d_output", "=", "d_output", "\n", "self", ".", "return_sequences", "=", "return_sequences", "\n", "\n", "self", ".", "rnn_cell", "=", "ODELSTMCell", "(", "d_input", ",", "d_model", ",", "solver_type", "=", "solver_type", ")", "\n", "self", ".", "fc", "=", "nn", ".", "Linear", "(", "self", ".", "d_model", ",", "self", ".", "d_output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.odelstm.ODELSTM.forward": [[97, 127], ["x.size", "x.size", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "range", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "timespans[].squeeze", "odelstm.ODELSTM.rnn_cell.forward", "odelstm.ODELSTM.fc", "torch.stack.append", "torch.stack.append", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "x.new_ones", "mask[].view"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.models.resnext.CifarResNeXt.forward"], ["", "def", "forward", "(", "self", ",", "x", ",", "timespans", "=", "None", ",", "mask", "=", "None", ")", ":", "\n", "        ", "device", "=", "x", ".", "device", "\n", "batch_size", "=", "x", ".", "size", "(", "0", ")", "\n", "seq_len", "=", "x", ".", "size", "(", "1", ")", "\n", "hidden_state", "=", "[", "\n", "torch", ".", "zeros", "(", "(", "batch_size", ",", "self", ".", "d_model", ")", ",", "device", "=", "device", ")", ",", "\n", "torch", ".", "zeros", "(", "(", "batch_size", ",", "self", ".", "d_model", ")", ",", "device", "=", "device", ")", ",", "\n", "]", "\n", "outputs", "=", "[", "]", "\n", "last_output", "=", "torch", ".", "zeros", "(", "(", "batch_size", ",", "self", ".", "d_output", ")", ",", "device", "=", "device", ")", "\n", "\n", "if", "timespans", "is", "None", ":", "\n", "            ", "timespans", "=", "x", ".", "new_ones", "(", "x", ".", "shape", "[", ":", "-", "1", "]", "+", "(", "1", ",", ")", ")", "/", "x", ".", "shape", "[", "1", "]", "\n", "\n", "", "for", "t", "in", "range", "(", "seq_len", ")", ":", "\n", "            ", "inputs", "=", "x", "[", ":", ",", "t", "]", "\n", "ts", "=", "timespans", "[", ":", ",", "t", "]", ".", "squeeze", "(", ")", "\n", "hidden_state", "=", "self", ".", "rnn_cell", ".", "forward", "(", "inputs", ",", "hidden_state", ",", "ts", ")", "\n", "current_output", "=", "self", ".", "fc", "(", "hidden_state", "[", "0", "]", ")", "\n", "outputs", ".", "append", "(", "current_output", ")", "\n", "if", "mask", "is", "not", "None", ":", "\n", "                ", "cur_mask", "=", "mask", "[", ":", ",", "t", "]", ".", "view", "(", "batch_size", ",", "1", ")", "\n", "last_output", "=", "cur_mask", "*", "current_output", "+", "(", "1.0", "-", "cur_mask", ")", "*", "last_output", "\n", "", "else", ":", "\n", "                ", "last_output", "=", "current_output", "\n", "", "", "if", "self", ".", "return_sequences", ":", "\n", "            ", "outputs", "=", "torch", ".", "stack", "(", "outputs", ",", "dim", "=", "1", ")", "# return entire sequence", "\n", "", "else", ":", "\n", "            ", "outputs", "=", "last_output", "# only last item", "\n", "", "return", "outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.odelstm.IrregularSequenceLearner.__init__": [[130, 134], ["pytorch_lightning.LightningModule.__init__"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__"], ["    ", "def", "__init__", "(", "self", ",", "model", ",", "lr", "=", "0.005", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "model", "=", "model", "\n", "self", ".", "lr", "=", "lr", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.odelstm.IrregularSequenceLearner.training_step": [[135, 150], ["odelstm.IrregularSequenceLearner.model.forward", "y_hat.view.view.view", "y.view.view.view", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "pytorch_lightning.metrics.functional.accuracy", "odelstm.IrregularSequenceLearner.log", "odelstm.IrregularSequenceLearner.log", "len", "y_hat.view.view.size", "torch.CrossEntropyLoss", "torch.CrossEntropyLoss", "y_hat.view.view.detach"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.models.resnext.CifarResNeXt.forward", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.tasks.metrics.accuracy"], ["", "def", "training_step", "(", "self", ",", "batch", ",", "batch_idx", ")", ":", "\n", "        ", "if", "len", "(", "batch", ")", "==", "4", ":", "\n", "            ", "x", ",", "t", ",", "y", ",", "mask", "=", "batch", "\n", "", "else", ":", "\n", "            ", "x", ",", "t", ",", "y", "=", "batch", "\n", "mask", "=", "None", "\n", "", "y_hat", "=", "self", ".", "model", ".", "forward", "(", "x", ",", "t", ",", "mask", ")", "\n", "y_hat", "=", "y_hat", ".", "view", "(", "-", "1", ",", "y_hat", ".", "size", "(", "-", "1", ")", ")", "\n", "y", "=", "y", ".", "view", "(", "-", "1", ")", "\n", "loss", "=", "nn", ".", "CrossEntropyLoss", "(", ")", "(", "y_hat", ",", "y", ")", "\n", "preds", "=", "torch", ".", "argmax", "(", "y_hat", ".", "detach", "(", ")", ",", "dim", "=", "-", "1", ")", "\n", "acc", "=", "accuracy", "(", "preds", ",", "y", ")", "\n", "self", ".", "log", "(", "\"train_acc\"", ",", "acc", ",", "prog_bar", "=", "True", ")", "\n", "self", ".", "log", "(", "\"train_loss\"", ",", "loss", ",", "prog_bar", "=", "True", ")", "\n", "return", "{", "\"loss\"", ":", "loss", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.odelstm.IrregularSequenceLearner.validation_step": [[151, 169], ["odelstm.IrregularSequenceLearner.model.forward", "y_hat.view.view.view", "y.view.view.view", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "pytorch_lightning.metrics.functional.accuracy", "odelstm.IrregularSequenceLearner.log", "odelstm.IrregularSequenceLearner.log", "len", "y_hat.view.view.size", "torch.CrossEntropyLoss", "torch.CrossEntropyLoss"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.models.resnext.CifarResNeXt.forward", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.tasks.metrics.accuracy"], ["", "def", "validation_step", "(", "self", ",", "batch", ",", "batch_idx", ")", ":", "\n", "        ", "if", "len", "(", "batch", ")", "==", "4", ":", "\n", "            ", "x", ",", "t", ",", "y", ",", "mask", "=", "batch", "\n", "", "else", ":", "\n", "            ", "x", ",", "t", ",", "y", "=", "batch", "\n", "mask", "=", "None", "\n", "", "y_hat", "=", "self", ".", "model", ".", "forward", "(", "x", ",", "t", ",", "mask", ")", "\n", "y_hat", "=", "y_hat", ".", "view", "(", "-", "1", ",", "y_hat", ".", "size", "(", "-", "1", ")", ")", "\n", "y", "=", "y", ".", "view", "(", "-", "1", ")", "\n", "\n", "loss", "=", "nn", ".", "CrossEntropyLoss", "(", ")", "(", "y_hat", ",", "y", ")", "\n", "\n", "preds", "=", "torch", ".", "argmax", "(", "y_hat", ",", "dim", "=", "1", ")", "\n", "acc", "=", "accuracy", "(", "preds", ",", "y", ")", "\n", "\n", "self", ".", "log", "(", "\"val_loss\"", ",", "loss", ",", "prog_bar", "=", "True", ")", "\n", "self", ".", "log", "(", "\"val_acc\"", ",", "acc", ",", "prog_bar", "=", "True", ")", "\n", "return", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.odelstm.IrregularSequenceLearner.test_step": [[170, 173], ["odelstm.IrregularSequenceLearner.validation_step"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.odelstm.IrregularSequenceLearner.validation_step"], ["", "def", "test_step", "(", "self", ",", "batch", ",", "batch_idx", ")", ":", "\n", "# Here we just reuse the validation_step for testing", "\n", "        ", "return", "self", ".", "validation_step", "(", "batch", ",", "batch_idx", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.odelstm.IrregularSequenceLearner.configure_optimizers": [[174, 176], ["torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "odelstm.IrregularSequenceLearner.model.parameters"], "methods", ["None"], ["", "def", "configure_optimizers", "(", "self", ")", ":", "\n", "        ", "return", "torch", ".", "optim", ".", "Adam", "(", "self", ".", "model", ".", "parameters", "(", ")", ",", "lr", "=", "self", ".", "lr", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.lipschitzrnn.LipschitzRNN_ODE.__init__": [[28, 42], ["torch.Module.__init__", "get_device", "torch.Tanh", "torch.Tanh", "torch.Tanh", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.eye().to", "torch.eye().to", "torch.eye().to", "torch.eye().to", "torch.eye().to", "torch.eye().to", "torch.eye().to", "torch.eye().to", "torch.eye().to", "lipschitzrnn.gaussian_init_", "lipschitzrnn.gaussian_init_", "torch.eye", "torch.eye", "torch.eye", "torch.eye", "torch.eye", "torch.eye", "torch.eye", "torch.eye", "torch.eye"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.lipschitzrnn.gaussian_init_", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.lipschitzrnn.gaussian_init_"], ["def", "__init__", "(", "self", ",", "d_model", ",", "beta", ",", "gamma", ",", "init_std", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "device", "=", "get_device", "(", ")", "\n", "\n", "self", ".", "gamma", "=", "gamma", "\n", "self", ".", "beta", "=", "beta", "\n", "\n", "self", ".", "tanh", "=", "nn", ".", "Tanh", "(", ")", "\n", "\n", "self", ".", "z", "=", "torch", ".", "zeros", "(", "d_model", ")", "\n", "self", ".", "C", "=", "nn", ".", "Parameter", "(", "gaussian_init_", "(", "d_model", ",", "std", "=", "init_std", ")", ")", "\n", "self", ".", "B", "=", "nn", ".", "Parameter", "(", "gaussian_init_", "(", "d_model", ",", "std", "=", "init_std", ")", ")", "\n", "self", ".", "I", "=", "torch", ".", "eye", "(", "d_model", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "self", ".", "i", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.lipschitzrnn.LipschitzRNN_ODE.forward": [[43, 55], ["torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "lipschitzrnn.LipschitzRNN_ODE.tanh", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "lipschitzrnn.LipschitzRNN_ODE.B.transpose", "lipschitzrnn.LipschitzRNN_ODE.B.transpose", "lipschitzrnn.LipschitzRNN_ODE.C.transpose", "lipschitzrnn.LipschitzRNN_ODE.C.transpose"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "t", ",", "h", ")", ":", "\n", "        ", "\"\"\"dh/dt as a function of time and h(t).\"\"\"", "\n", "if", "self", ".", "i", "==", "0", ":", "\n", "            ", "self", ".", "A", "=", "self", ".", "beta", "*", "(", "self", ".", "B", "-", "self", ".", "B", ".", "transpose", "(", "1", ",", "0", ")", ")", "+", "(", "\n", "1", "-", "self", ".", "beta", ")", "*", "(", "self", ".", "B", "+", "\n", "self", ".", "B", ".", "transpose", "(", "1", ",", "0", ")", ")", "-", "self", ".", "gamma", "*", "self", ".", "I", "\n", "self", ".", "W", "=", "self", ".", "beta", "*", "(", "self", ".", "C", "-", "self", ".", "C", ".", "transpose", "(", "1", ",", "0", ")", ")", "+", "(", "\n", "1", "-", "self", ".", "beta", ")", "*", "(", "self", ".", "C", "+", "\n", "self", ".", "C", ".", "transpose", "(", "1", ",", "0", ")", ")", "-", "self", ".", "gamma", "*", "self", ".", "I", "\n", "\n", "", "return", "torch", ".", "matmul", "(", "\n", "h", ",", "self", ".", "A", ")", "+", "self", ".", "tanh", "(", "torch", ".", "matmul", "(", "h", ",", "self", ".", "W", ")", "+", "self", ".", "z", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.lipschitzrnn.RnnModels.d_output": [[59, 62], ["None"], "methods", ["None"], ["@", "property", "\n", "def", "d_output", "(", "self", ")", ":", "#TODO: check", "\n", "        ", "return", "self", ".", "d_model", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.lipschitzrnn.RnnModels.__init__": [[63, 129], ["src.models.sequence.base.SequenceModule.__init__", "torch.Tanh", "torch.Tanh", "torch.Tanh", "torch.Sigmoid", "torch.Sigmoid", "torch.Sigmoid", "lipschitzrnn.RnnModels.register_buffer", "torch.eye", "torch.eye", "torch.eye", "torch.eye", "torch.eye", "torch.eye", "torch.eye", "torch.eye", "torch.eye", "torch.Linear", "torch.Linear", "torch.Linear", "lipschitzrnn.gaussian_init_", "torch.Linear", "torch.Linear", "torch.Linear", "lipschitzrnn.gaussian_init_", "torch.Parameter", "torch.Parameter", "torch.Parameter", "lipschitzrnn.gaussian_init_", "torch.svd", "torch.svd", "torch.svd", "torch.svd", "torch.svd", "torch.svd", "torch.svd", "torch.svd", "torch.svd", "torch.Parameter", "torch.Parameter", "torch.Parameter", "lipschitzrnn.gaussian_init_", "torch.mm().float", "torch.mm().float", "torch.mm().float", "torch.mm().float", "torch.mm().float", "torch.mm().float", "torch.mm().float", "torch.mm().float", "torch.mm().float", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "lipschitzrnn.gaussian_init_", "lipschitzrnn.gaussian_init_", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.mm", "torch.mm", "torch.mm", "torch.mm", "torch.mm", "torch.mm", "torch.mm", "torch.mm", "torch.mm", "lipschitzrnn.gaussian_init_", "lipschitzrnn.gaussian_init_", "lipschitzrnn.LipschitzRNN_ODE", "print", "V.t"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.lipschitzrnn.gaussian_init_", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.lipschitzrnn.gaussian_init_", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.lipschitzrnn.gaussian_init_", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.lipschitzrnn.gaussian_init_", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.lipschitzrnn.gaussian_init_", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.lipschitzrnn.gaussian_init_", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.lipschitzrnn.gaussian_init_", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.lipschitzrnn.gaussian_init_"], ["", "def", "__init__", "(", "self", ",", "\n", "#  d_input,", "\n", "#  d_output,", "\n", "d_model", "=", "128", ",", "\n", "chunk", "=", "1", ",", "\n", "eps", "=", "0.01", ",", "\n", "beta", "=", "0.8", ",", "\n", "gamma", "=", "0.01", ",", "\n", "gated", "=", "False", ",", "\n", "init_std", "=", "1", ",", "\n", "alpha", "=", "1", ",", "\n", "model", "=", "'LipschitzRNN'", ",", "\n", "solver", "=", "'euler'", ",", "\n", "l_output", "=", "0", ",", "\n", "l_max", "=", "-", "1", ",", "\n", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "# self.d_input = d_input", "\n", "self", ".", "d_model", "=", "d_model", "\n", "# self.chunk = chunk", "\n", "self", ".", "eps", "=", "eps", "\n", "self", ".", "model", "=", "model", "\n", "self", ".", "solver", "=", "solver", "\n", "self", ".", "gamma", "=", "gamma", "\n", "self", ".", "beta", "=", "beta", "\n", "self", ".", "alpha", "=", "alpha", "\n", "\n", "\n", "self", ".", "tanh", "=", "nn", ".", "Tanh", "(", ")", "\n", "self", ".", "sigmoid", "=", "nn", ".", "Sigmoid", "(", ")", "\n", "\n", "# self.E = nn.Linear(d_input*self.chunk, d_model)", "\n", "# self.D = nn.Linear(d_model, d_output)", "\n", "self", ".", "register_buffer", "(", "'I'", ",", "torch", ".", "eye", "(", "d_model", ")", ")", "\n", "\n", "if", "self", ".", "model", "==", "'simpleRNN'", ":", "\n", "            ", "self", ".", "W", "=", "nn", ".", "Linear", "(", "d_model", ",", "d_model", ",", "bias", "=", "False", ")", "\n", "self", ".", "W", ".", "weight", ".", "data", "=", "gaussian_init_", "(", "d_model", ",", "std", "=", "init_std", ")", "\n", "\n", "", "elif", "self", ".", "model", "==", "'resRNN'", ":", "\n", "            ", "self", ".", "W", "=", "nn", ".", "Linear", "(", "d_model", ",", "d_model", ",", "bias", "=", "False", ")", "\n", "self", ".", "W", ".", "weight", ".", "data", "=", "gaussian_init_", "(", "d_model", ",", "std", "=", "init_std", ")", "\n", "\n", "", "elif", "self", ".", "model", "==", "'asymRNN'", ":", "\n", "            ", "self", ".", "C", "=", "nn", ".", "Parameter", "(", "gaussian_init_", "(", "d_model", ",", "std", "=", "init_std", ")", ")", "\n", "\n", "", "elif", "self", ".", "model", "==", "'calRNN'", ":", "\n", "            ", "U", ",", "_", ",", "V", "=", "torch", ".", "svd", "(", "gaussian_init_", "(", "d_model", ",", "std", "=", "init_std", ")", ")", "\n", "self", ".", "C", "=", "nn", ".", "Parameter", "(", "torch", ".", "mm", "(", "U", ",", "V", ".", "t", "(", ")", ")", ".", "float", "(", ")", ")", "\n", "\n", "", "elif", "self", ".", "model", "==", "'LipschitzRNN'", ":", "\n", "            ", "self", ".", "C", "=", "nn", ".", "Parameter", "(", "gaussian_init_", "(", "d_model", ",", "std", "=", "init_std", ")", ")", "\n", "self", ".", "B", "=", "nn", ".", "Parameter", "(", "gaussian_init_", "(", "d_model", ",", "std", "=", "init_std", ")", ")", "\n", "\n", "", "elif", "self", ".", "model", "==", "'LipschitzRNN_gated'", ":", "\n", "            ", "self", ".", "C", "=", "nn", ".", "Parameter", "(", "gaussian_init_", "(", "d_model", ",", "std", "=", "init_std", ")", ")", "\n", "self", ".", "B", "=", "nn", ".", "Parameter", "(", "gaussian_init_", "(", "d_model", ",", "std", "=", "init_std", ")", ")", "\n", "# self.E_gate = nn.Linear(d_input, d_model)", "\n", "\n", "", "elif", "self", ".", "model", "==", "'LipschitzRNN_ODE'", ":", "\n", "            ", "self", ".", "func", "=", "LipschitzRNN_ODE", "(", "d_model", ",", "beta", ",", "gamma", ",", "init_std", ")", "\n", "\n", "", "else", ":", "\n", "            ", "print", "(", "\"Unexpected model!\"", ")", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.lipschitzrnn.RnnModels.step": [[130, 143], ["lipschitzrnn.RnnModels.tanh", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "lipschitzrnn.RnnModels.B.transpose", "lipschitzrnn.RnnModels.B.transpose", "lipschitzrnn.RnnModels.C.transpose", "lipschitzrnn.RnnModels.C.transpose"], "methods", ["None"], ["", "", "def", "step", "(", "self", ",", "x", ",", "state", ")", ":", "\n", "# THIS CODE IS UNTESTED", "\n", "        ", "if", "self", ".", "model", "==", "'LipschitzRNN'", ":", "\n", "            ", "if", "state", "is", "None", ":", "\n", "                ", "A", "=", "self", ".", "beta", "*", "(", "self", ".", "B", "-", "self", ".", "B", ".", "transpose", "(", "1", ",", "0", ")", ")", "+", "(", "\n", "1", "-", "self", ".", "beta", ")", "*", "(", "self", ".", "B", "+", "self", ".", "B", ".", "transpose", "(", "\n", "1", ",", "0", ")", ")", "-", "self", ".", "gamma", "*", "self", ".", "I", "\n", "W", "=", "self", ".", "beta", "*", "(", "self", ".", "C", "-", "self", ".", "C", ".", "transpose", "(", "1", ",", "0", ")", ")", "+", "(", "\n", "1", "-", "self", ".", "beta", ")", "*", "(", "self", ".", "C", "+", "self", ".", "C", ".", "transpose", "(", "\n", "1", ",", "0", ")", ")", "-", "self", ".", "gamma", "*", "self", ".", "I", "\n", "", "state", "=", "state", "+", "self", ".", "eps", "*", "self", ".", "alpha", "*", "torch", ".", "matmul", "(", "state", ",", "A", ")", "+", "self", ".", "eps", "*", "self", ".", "tanh", "(", "torch", ".", "matmul", "(", "state", ",", "W", ")", "+", "x", ")", "\n", "", "return", "x", ",", "state", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.lipschitzrnn.RnnModels.forward": [[144, 210], ["torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "range", "lipschitzrnn.RnnModels.unsqueeze", "lipschitzrnn.RnnModels.tanh", "lipschitzrnn.RnnModels.W", "lipschitzrnn.RnnModels.tanh", "lipschitzrnn.RnnModels.tanh", "lipschitzrnn.RnnModels.tanh", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "lipschitzrnn.RnnModels.W", "lipschitzrnn.RnnModels.C.transpose", "lipschitzrnn.RnnModels.C.transpose", "torch.inverse", "torch.inverse", "torch.inverse", "torch.inverse", "torch.inverse", "torch.inverse", "torch.inverse", "torch.inverse", "torch.inverse", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "lipschitzrnn.RnnModels.E_gate", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "lipschitzrnn.RnnModels.sigmoid", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "lipschitzrnn.RnnModels.tanh", "lipschitzrnn.RnnModels.tanh", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torchdiffeq.odeint", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "lipschitzrnn.RnnModels.B.transpose", "lipschitzrnn.RnnModels.B.transpose", "lipschitzrnn.RnnModels.C.transpose", "lipschitzrnn.RnnModels.C.transpose", "lipschitzrnn.RnnModels.B.transpose", "lipschitzrnn.RnnModels.B.transpose", "lipschitzrnn.RnnModels.C.transpose", "lipschitzrnn.RnnModels.C.transpose", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "# x = x.reshape(x.shape[0], -1, self.d_input*self.chunk)", "\n", "        ", "T", "=", "x", ".", "shape", "[", "1", "]", "\n", "h", "=", "torch", ".", "zeros", "(", "x", ".", "shape", "[", "0", "]", ",", "self", ".", "d_model", ",", "device", "=", "x", ".", "device", ")", "\n", "\n", "for", "i", "in", "range", "(", "T", ")", ":", "\n", "# z = self.E(x[:, i, :])", "\n", "            ", "z", "=", "x", "[", ":", ",", "i", ",", ":", "]", "\n", "\n", "if", "self", ".", "model", "==", "'simpleRNN'", ":", "\n", "                ", "h", "=", "self", ".", "tanh", "(", "self", ".", "W", "(", "h", ")", "+", "z", ")", "\n", "\n", "", "elif", "self", ".", "model", "==", "'resRNN'", ":", "\n", "                ", "h", "=", "h", "+", "self", ".", "eps", "*", "self", ".", "tanh", "(", "self", ".", "W", "(", "h", ")", "+", "z", ")", "\n", "\n", "", "elif", "self", ".", "model", "==", "'asymRNN'", ":", "\n", "                ", "if", "i", "==", "0", ":", "\n", "                    ", "W", "=", "self", ".", "C", "-", "self", ".", "C", ".", "transpose", "(", "1", ",", "0", ")", "-", "self", ".", "gamma", "*", "self", ".", "I", "\n", "", "h", "=", "h", "+", "self", ".", "eps", "*", "self", ".", "tanh", "(", "torch", ".", "matmul", "(", "h", ",", "W", ")", "+", "z", ")", "\n", "\n", "", "elif", "self", ".", "model", "==", "'calRNN'", ":", "\n", "                ", "if", "i", "==", "0", ":", "\n", "                    ", "C", "=", "self", ".", "C", "-", "self", ".", "C", ".", "transpose", "(", "1", ",", "0", ")", "\n", "W", "=", "torch", ".", "matmul", "(", "torch", ".", "inverse", "(", "self", ".", "I", "+", "C", ")", ",", "self", ".", "I", "-", "C", ")", "\n", "", "h", "=", "self", ".", "tanh", "(", "torch", ".", "matmul", "(", "h", ",", "W", ")", "+", "z", ")", "\n", "\n", "", "elif", "self", ".", "model", "==", "'LipschitzRNN'", ":", "\n", "                ", "if", "i", "==", "0", ":", "\n", "                    ", "A", "=", "self", ".", "beta", "*", "(", "self", ".", "B", "-", "self", ".", "B", ".", "transpose", "(", "1", ",", "0", ")", ")", "+", "(", "\n", "1", "-", "self", ".", "beta", ")", "*", "(", "self", ".", "B", "+", "self", ".", "B", ".", "transpose", "(", "\n", "1", ",", "0", ")", ")", "-", "self", ".", "gamma", "*", "self", ".", "I", "\n", "W", "=", "self", ".", "beta", "*", "(", "self", ".", "C", "-", "self", ".", "C", ".", "transpose", "(", "1", ",", "0", ")", ")", "+", "(", "\n", "1", "-", "self", ".", "beta", ")", "*", "(", "self", ".", "C", "+", "self", ".", "C", ".", "transpose", "(", "\n", "1", ",", "0", ")", ")", "-", "self", ".", "gamma", "*", "self", ".", "I", "\n", "", "h", "=", "h", "+", "self", ".", "eps", "*", "self", ".", "alpha", "*", "torch", ".", "matmul", "(", "\n", "h", ",", "A", ")", "+", "self", ".", "eps", "*", "self", ".", "tanh", "(", "torch", ".", "matmul", "(", "h", ",", "W", ")", "+", "z", ")", "\n", "\n", "", "elif", "self", ".", "model", "==", "'LipschitzRNN_gated'", ":", "\n", "                ", "if", "i", "==", "0", ":", "\n", "                    ", "A", "=", "self", ".", "beta", "*", "(", "self", ".", "B", "-", "self", ".", "B", ".", "transpose", "(", "1", ",", "0", ")", ")", "+", "(", "\n", "1", "-", "self", ".", "beta", ")", "*", "(", "self", ".", "B", "+", "self", ".", "B", ".", "transpose", "(", "\n", "1", ",", "0", ")", ")", "-", "self", ".", "gamma", "*", "self", ".", "I", "\n", "W", "=", "self", ".", "beta", "*", "(", "self", ".", "C", "-", "self", ".", "C", ".", "transpose", "(", "1", ",", "0", ")", ")", "+", "(", "\n", "1", "-", "self", ".", "beta", ")", "*", "(", "self", ".", "C", "+", "self", ".", "C", ".", "transpose", "(", "\n", "1", ",", "0", ")", ")", "-", "self", ".", "gamma", "*", "self", ".", "I", "\n", "", "z_gate", "=", "self", ".", "E_gate", "(", "x", "[", ":", ",", "i", ",", ":", "]", ")", "\n", "Wh", "=", "torch", ".", "matmul", "(", "h", ",", "W", ")", "\n", "Ah", "=", "torch", ".", "matmul", "(", "h", ",", "A", ")", "\n", "q1", "=", "self", ".", "alpha", "*", "Ah", "+", "self", ".", "tanh", "(", "Wh", "+", "z", ")", "\n", "q2", "=", "self", ".", "sigmoid", "(", "Wh", "+", "z_gate", ")", "\n", "h", "=", "h", "+", "self", ".", "eps", "*", "q1", "*", "q2", "\n", "\n", "", "elif", "self", ".", "model", "==", "'LipschitzRNN_ODE'", ":", "\n", "                ", "self", ".", "func", ".", "z", "=", "z", "\n", "self", ".", "func", ".", "i", "=", "i", "\n", "h", "=", "odeint", "(", "self", ".", "func", ",", "\n", "h", ",", "\n", "torch", ".", "tensor", "(", "[", "0", ",", "self", ".", "eps", "]", ")", ".", "float", "(", ")", ",", "\n", "method", "=", "self", ".", "solver", ")", "[", "-", "1", ",", ":", ",", ":", "]", "\n", "\n", "# Decoder", "\n", "#----------", "\n", "# out = self.D(h)", "\n", "# return out", "\n", "\n", "", "", "return", "h", ".", "unsqueeze", "(", "1", ")", ",", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.lipschitzrnn.gaussian_init_": [[18, 23], ["torch.distributions.Normal", "torch.distributions.Normal", "torch.distributions.Normal", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.distributions.Normal.sample"], "function", ["None"], ["def", "gaussian_init_", "(", "n_units", ",", "std", "=", "1", ")", ":", "\n", "    ", "sampler", "=", "torch", ".", "distributions", ".", "Normal", "(", "torch", ".", "Tensor", "(", "[", "0", "]", ")", ",", "\n", "torch", ".", "Tensor", "(", "[", "std", "/", "n_units", "]", ")", ")", "\n", "A_init", "=", "sampler", ".", "sample", "(", "(", "n_units", ",", "n_units", ")", ")", "[", "...", ",", "0", "]", "\n", "return", "A_init", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.samplernn.StackedRNN.d_output": [[22, 25], ["None"], "methods", ["None"], ["@", "property", "\n", "def", "d_output", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "d_model", "if", "self", ".", "output_linear", "else", "self", ".", "d_hidden", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.samplernn.StackedRNN.__init__": [[26, 102], ["src.models.sequence.base.SequenceModule.__init__", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ModuleList", "range", "samplernn.StackedRNN.weight_norm", "torch.nn.Identity", "torch.nn.Identity", "torch.nn.Identity", "torch.nn.Identity", "rnn.named_modules", "ValueError", "samplernn.StackedRNN.rnn_layers.append", "samplernn.StackedRNN.lin_layers.append", "samplernn.StackedRNN.lin_layers.append", "samplernn.StackedRNN.dropout_layers.append", "samplernn.StackedRNN.dropout_layers.append", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "isinstance", "torch.nn.init.orthogonal_", "torch.nn.init.orthogonal_", "torch.nn.init.orthogonal_", "torch.nn.init.orthogonal_", "RNN", "samplernn.StackedRNN.rnn_layers.append", "samplernn.StackedRNN.rnn_layers.append", "samplernn.StackedRNN.weight_norm", "torch.nn.Identity", "torch.nn.Identity", "torch.nn.Identity", "torch.nn.Identity", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Identity", "torch.nn.Identity", "torch.nn.Identity", "torch.nn.Identity", "setattr", "RNN", "RNN", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "samplernn.StackedRNN.weight_norm"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__"], ["", "def", "__init__", "(", "\n", "self", ",", "\n", "d_model", ",", "\n", "d_hidden", ",", "\n", "n_layers", ",", "\n", "learn_h0", "=", "False", ",", "\n", "rnn_type", "=", "'gru'", ",", "\n", "skip_connections", "=", "False", ",", "\n", "weight_norm", "=", "False", ",", "\n", "dropout", "=", "0.0", ",", "\n", "output_linear", "=", "False", ",", "\n", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "d_model", "=", "d_model", "\n", "self", ".", "d_hidden", "=", "d_hidden", "\n", "self", ".", "n_layers", "=", "n_layers", "\n", "self", ".", "learn_h0", "=", "learn_h0", "\n", "self", ".", "skip_connections", "=", "skip_connections", "\n", "self", ".", "weight_norm", "=", "torch", ".", "nn", ".", "utils", ".", "weight_norm", "if", "weight_norm", "else", "lambda", "x", ":", "x", "\n", "\n", "self", ".", "output_linear", "=", "output_linear", "\n", "self", ".", "rnn_layers", "=", "torch", ".", "nn", ".", "ModuleList", "(", ")", "\n", "self", ".", "lin_layers", "=", "torch", ".", "nn", ".", "ModuleList", "(", ")", "\n", "self", ".", "dropout_layers", "=", "torch", ".", "nn", ".", "ModuleList", "(", ")", "\n", "self", ".", "rnn_type", "=", "rnn_type", "\n", "\n", "if", "rnn_type", "==", "'lstm'", ":", "\n", "            ", "RNN", "=", "TorchLSTM", "\n", "", "elif", "rnn_type", "==", "'gru'", ":", "\n", "            ", "RNN", "=", "TorchGRU", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "'rnn_type must be lstm or gru'", ")", "\n", "\n", "", "for", "i", "in", "range", "(", "n_layers", ")", ":", "\n", "\n", "            ", "if", "i", "==", "0", ":", "\n", "                ", "self", ".", "rnn_layers", ".", "append", "(", "\n", "RNN", "(", "d_model", "=", "d_model", ",", "d_hidden", "=", "d_hidden", ",", "n_layers", "=", "1", ",", "learn_h0", "=", "learn_h0", ")", ",", "\n", ")", "\n", "", "else", ":", "\n", "                ", "if", "skip_connections", ":", "\n", "                    ", "self", ".", "rnn_layers", ".", "append", "(", "\n", "RNN", "(", "d_model", "=", "d_model", "+", "d_hidden", ",", "d_hidden", "=", "d_hidden", ",", "n_layers", "=", "1", ",", "learn_h0", "=", "learn_h0", ")", ",", "\n", ")", "\n", "", "else", ":", "\n", "                    ", "self", ".", "rnn_layers", ".", "append", "(", "\n", "RNN", "(", "d_model", "=", "d_hidden", ",", "d_hidden", "=", "d_hidden", ",", "n_layers", "=", "1", ",", "learn_h0", "=", "learn_h0", ")", ",", "\n", ")", "\n", "\n", "", "", "if", "skip_connections", ":", "\n", "                ", "self", ".", "lin_layers", ".", "append", "(", "self", ".", "weight_norm", "(", "torch", ".", "nn", ".", "Linear", "(", "d_hidden", ",", "d_hidden", ")", ")", ")", "\n", "", "else", ":", "\n", "                ", "self", ".", "lin_layers", ".", "append", "(", "torch", ".", "nn", ".", "Identity", "(", ")", ")", "\n", "\n", "", "if", "dropout", ">", "0.0", "and", "i", "<", "n_layers", "-", "1", ":", "\n", "                ", "self", ".", "dropout_layers", ".", "append", "(", "torch", ".", "nn", ".", "Dropout", "(", "dropout", ")", ")", "\n", "", "else", ":", "\n", "                ", "self", ".", "dropout_layers", ".", "append", "(", "torch", ".", "nn", ".", "Identity", "(", ")", ")", "\n", "\n", "", "", "if", "output_linear", ":", "\n", "            ", "self", ".", "output_layer", "=", "self", ".", "weight_norm", "(", "torch", ".", "nn", ".", "Linear", "(", "d_hidden", ",", "d_model", ")", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "output_layer", "=", "torch", ".", "nn", ".", "Identity", "(", ")", "\n", "\n", "# Apply weight norm to all the RNN layers", "\n", "", "for", "rnn", "in", "self", ".", "rnn_layers", ":", "\n", "# Find all Linear layers in the RNN", "\n", "            ", "for", "name", ",", "module", "in", "rnn", ".", "named_modules", "(", ")", ":", "\n", "                ", "if", "isinstance", "(", "module", ",", "torch", ".", "nn", ".", "Linear", ")", ":", "\n", "                    ", "setattr", "(", "rnn", ",", "name", ",", "self", ".", "weight_norm", "(", "module", ")", ")", "\n", "\n", "# Use orthogonal initialization for W_hn if using GRU (weight_hh_l[0])", "\n", "", "", "", "if", "rnn_type", "==", "'gru'", ":", "\n", "            ", "for", "rnn", "in", "self", ".", "rnn_layers", ":", "\n", "                ", "torch", ".", "nn", ".", "init", ".", "orthogonal_", "(", "rnn", ".", "weight_hh_l0", "[", "2", "*", "d_hidden", ":", "]", ".", "data", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.samplernn.StackedRNN.default_state": [[104, 108], ["rnn.default_state"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.sashimi.sashimi.Sashimi.default_state"], ["", "", "", "def", "default_state", "(", "self", ",", "*", "batch_shape", ",", "device", "=", "None", ")", ":", "\n", "        ", "return", "[", "\n", "rnn", ".", "default_state", "(", "*", "batch_shape", ",", "device", "=", "device", ")", "\n", "for", "rnn", "in", "self", ".", "rnn_layers", "\n", "]", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.samplernn.StackedRNN.forward": [[110, 138], ["zip", "samplernn.StackedRNN.output_layer", "rnn", "next_states.append", "dropout", "lin", "len", "torch.cat", "torch.cat", "torch.cat", "torch.cat"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "inputs", ",", "*", "args", ",", "state", "=", "None", ",", "**", "kwargs", ")", ":", "\n", "\n", "        ", "outputs", "=", "inputs", "\n", "prev_states", "=", "[", "None", "]", "*", "len", "(", "self", ".", "rnn_layers", ")", "if", "state", "is", "None", "else", "state", "\n", "next_states", "=", "[", "]", "\n", "out", "=", "0.", "\n", "for", "rnn", ",", "prev_state", ",", "lin", ",", "dropout", "in", "zip", "(", "self", ".", "rnn_layers", ",", "prev_states", ",", "self", ".", "lin_layers", ",", "self", ".", "dropout_layers", ")", ":", "\n", "# Run RNN on inputs", "\n", "            ", "outputs", ",", "state", "=", "rnn", "(", "outputs", ",", "prev_state", ")", "\n", "next_states", ".", "append", "(", "state", ")", "\n", "\n", "# If dropout, only apply to the outputs of RNNs that are not the last one (like torch's LSTM)", "\n", "outputs", "=", "dropout", "(", "outputs", ")", "\n", "\n", "z", "=", "lin", "(", "outputs", ")", "\n", "\n", "if", "self", ".", "skip_connections", ":", "\n", "# If skip connections, add the outputs of all the RNNs to the outputs", "\n", "                ", "out", "+=", "z", "\n", "# Feed in the outputs of the previous RNN, and the original inputs to the next RNN", "\n", "outputs", "=", "torch", ".", "cat", "(", "[", "outputs", ",", "inputs", "]", ",", "dim", "=", "-", "1", ")", "\n", "", "else", ":", "\n", "                ", "out", "=", "z", "\n", "outputs", "=", "z", "\n", "\n", "", "", "out", "=", "self", ".", "output_layer", "(", "out", ")", "\n", "\n", "return", "out", ",", "next_states", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.samplernn.StackedRNNBaseline.d_output": [[160, 163], ["None"], "methods", ["None"], ["@", "property", "\n", "def", "d_output", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "d_hidden", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.samplernn.StackedRNNBaseline.__init__": [[164, 204], ["src.models.sequence.base.SequenceModule.__init__", "samplernn.StackedRNN", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.utils.weight_norm", "torch.nn.utils.weight_norm", "torch.nn.utils.weight_norm", "torch.nn.utils.weight_norm", "torch.nn.utils.weight_norm", "torch.nn.utils.weight_norm", "torch.nn.utils.weight_norm", "torch.nn.utils.weight_norm", "torch.nn.utils.weight_norm", "torch.nn.utils.weight_norm", "torch.nn.utils.weight_norm", "torch.nn.utils.weight_norm"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__"], ["", "def", "__init__", "(", "\n", "self", ",", "\n", "d_model", ",", "\n", "d_hidden", ",", "\n", "n_layers", ",", "\n", "learn_h0", "=", "False", ",", "\n", "rnn_type", "=", "'gru'", ",", "\n", "weight_norm", "=", "False", ",", "\n", "skip_connections", "=", "True", ",", "\n", "dropout", "=", "0.0", ",", "\n", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "d_model", "=", "d_model", "\n", "self", ".", "d_hidden", "=", "d_hidden", "\n", "self", ".", "n_layers", "=", "n_layers", "\n", "self", ".", "learn_h0", "=", "learn_h0", "\n", "self", ".", "weight_norm", "=", "weight_norm", "\n", "self", ".", "skip_connections", "=", "skip_connections", "\n", "self", ".", "rnn_type", "=", "rnn_type", "\n", "\n", "self", ".", "rnn", "=", "StackedRNN", "(", "\n", "d_model", "=", "d_model", ",", "\n", "d_hidden", "=", "d_hidden", ",", "\n", "n_layers", "=", "n_layers", ",", "\n", "rnn_type", "=", "rnn_type", ",", "\n", "skip_connections", "=", "skip_connections", ",", "\n", "weight_norm", "=", "weight_norm", ",", "\n", "dropout", "=", "dropout", ",", "\n", "output_linear", "=", "False", ",", "\n", ")", "\n", "\n", "self", ".", "lin1", "=", "torch", ".", "nn", ".", "Linear", "(", "d_hidden", ",", "d_hidden", ")", "\n", "self", ".", "lin2", "=", "torch", ".", "nn", ".", "Linear", "(", "d_hidden", ",", "d_hidden", ")", "\n", "self", ".", "lin3", "=", "torch", ".", "nn", ".", "Linear", "(", "d_hidden", ",", "d_hidden", ")", "\n", "\n", "if", "weight_norm", ":", "\n", "            ", "self", ".", "lin1", "=", "torch", ".", "nn", ".", "utils", ".", "weight_norm", "(", "self", ".", "lin1", ")", "\n", "self", ".", "lin2", "=", "torch", ".", "nn", ".", "utils", ".", "weight_norm", "(", "self", ".", "lin2", ")", "\n", "self", ".", "lin3", "=", "torch", ".", "nn", ".", "utils", ".", "weight_norm", "(", "self", ".", "lin3", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.samplernn.StackedRNNBaseline.default_state": [[205, 207], ["samplernn.StackedRNNBaseline.rnn.default_state"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.sashimi.sashimi.Sashimi.default_state"], ["", "", "def", "default_state", "(", "self", ",", "*", "batch_shape", ",", "device", "=", "None", ")", ":", "\n", "        ", "return", "self", ".", "rnn", ".", "default_state", "(", "*", "batch_shape", ",", "device", "=", "device", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.samplernn.StackedRNNBaseline.forward": [[208, 216], ["samplernn.StackedRNNBaseline.rnn", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "samplernn.StackedRNNBaseline.lin1", "samplernn.StackedRNNBaseline.lin2", "samplernn.StackedRNNBaseline.lin3"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "inputs", ",", "*", "args", ",", "state", "=", "None", ",", "**", "kwargs", ")", ":", "\n", "        ", "outputs", "=", "inputs", "\n", "outputs", ",", "state", "=", "self", ".", "rnn", "(", "outputs", ",", "state", ")", "\n", "outputs", "=", "F", ".", "relu", "(", "self", ".", "lin1", "(", "outputs", ")", ")", "\n", "outputs", "=", "F", ".", "relu", "(", "self", ".", "lin2", "(", "outputs", ")", ")", "\n", "outputs", "=", "F", ".", "relu", "(", "self", ".", "lin3", "(", "outputs", ")", ")", "\n", "\n", "return", "outputs", ",", "state", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.samplernn.LearnedUpsampling1d.__init__": [[220, 239], ["super().__init__", "torch.nn.ConvTranspose1d", "torch.nn.ConvTranspose1d", "torch.nn.ConvTranspose1d", "torch.nn.ConvTranspose1d", "samplernn.LearnedUpsampling1d.reset_parameters", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "samplernn.LearnedUpsampling1d.register_parameter", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.cells.memory.MemoryCell.reset_parameters"], ["    ", "def", "__init__", "(", "self", ",", "in_channels", ",", "out_channels", ",", "kernel_size", ",", "bias", "=", "True", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "conv_t", "=", "torch", ".", "nn", ".", "ConvTranspose1d", "(", "\n", "in_channels", "=", "in_channels", ",", "\n", "out_channels", "=", "out_channels", ",", "\n", "kernel_size", "=", "kernel_size", ",", "\n", "stride", "=", "kernel_size", ",", "\n", "bias", "=", "False", ",", "\n", ")", "\n", "\n", "if", "bias", ":", "\n", "            ", "self", ".", "bias", "=", "torch", ".", "nn", ".", "Parameter", "(", "\n", "torch", ".", "FloatTensor", "(", "out_channels", ",", "kernel_size", ")", "\n", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "register_parameter", "(", "'bias'", ",", "None", ")", "\n", "\n", "", "self", ".", "reset_parameters", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.samplernn.LearnedUpsampling1d.reset_parameters": [[240, 243], ["samplernn.LearnedUpsampling1d.conv_t.reset_parameters", "torch.nn.init.constant", "torch.nn.init.constant", "torch.nn.init.constant", "torch.nn.init.constant"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.cells.memory.MemoryCell.reset_parameters"], ["", "def", "reset_parameters", "(", "self", ")", ":", "\n", "        ", "self", ".", "conv_t", ".", "reset_parameters", "(", ")", "\n", "torch", ".", "nn", ".", "init", ".", "constant", "(", "self", ".", "bias", ",", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.samplernn.LearnedUpsampling1d.forward": [[244, 254], ["input.size", "samplernn.LearnedUpsampling1d.bias.unsqueeze().unsqueeze().expand().contiguous().view", "samplernn.LearnedUpsampling1d.conv_t", "samplernn.LearnedUpsampling1d.bias.unsqueeze().unsqueeze().expand().contiguous", "samplernn.LearnedUpsampling1d.bias.unsqueeze().unsqueeze().expand", "samplernn.LearnedUpsampling1d.bias.unsqueeze().unsqueeze", "samplernn.LearnedUpsampling1d.bias.unsqueeze"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input", ")", ":", "\n", "        ", "(", "batch_size", ",", "_", ",", "length", ")", "=", "input", ".", "size", "(", ")", "\n", "(", "kernel_size", ",", ")", "=", "self", ".", "conv_t", ".", "kernel_size", "\n", "bias", "=", "self", ".", "bias", ".", "unsqueeze", "(", "0", ")", ".", "unsqueeze", "(", "2", ")", ".", "expand", "(", "\n", "batch_size", ",", "self", ".", "conv_t", ".", "out_channels", ",", "length", ",", "kernel_size", "\n", ")", ".", "contiguous", "(", ")", ".", "view", "(", "\n", "batch_size", ",", "self", ".", "conv_t", ".", "out_channels", ",", "\n", "length", "*", "kernel_size", "\n", ")", "\n", "return", "self", ".", "conv_t", "(", "input", ")", "+", "bias", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.samplernn.SampleRNN.d_output": [[262, 265], ["None"], "methods", ["None"], ["@", "property", "\n", "def", "d_output", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "d_hidden", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.samplernn.SampleRNN.__init__": [[266, 321], ["src.models.sequence.base.SequenceModule.__init__", "map", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ModuleList", "samplernn.SampleLevelMLP", "torch.nn.Embedding", "torch.nn.Embedding", "torch.nn.Embedding", "torch.nn.Embedding", "numpy.cumprod", "ValueError", "samplernn.FrameLevelRNN", "zip"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__"], ["", "def", "__init__", "(", "\n", "self", ",", "\n", "frame_sizes", "=", "(", "16", ",", "4", ")", ",", "\n", "n_rnn", "=", "2", ",", "\n", "d_hidden", "=", "1024", ",", "\n", "bits", "=", "8", ",", "\n", "learn_h0", "=", "True", ",", "\n", "d_model", "=", "256", ",", "\n", "weight_norm", "=", "True", ",", "\n", "reproduce", "=", "True", ",", "\n", "quantization", "=", "'linear'", ",", "\n", "layer", "=", "'gru'", ",", "\n", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "d_hidden", "=", "d_hidden", "\n", "self", ".", "d_model", "=", "d_model", "\n", "self", ".", "reproduce", "=", "reproduce", "\n", "self", ".", "bits", "=", "bits", "\n", "self", ".", "quantization", "=", "quantization", "\n", "self", ".", "layer", "=", "layer", "\n", "\n", "if", "self", ".", "quantization", "==", "'linear'", ":", "\n", "            ", "self", ".", "dequantizer", "=", "linear_decode", "\n", "", "elif", "self", ".", "quantization", "==", "'mu-law'", ":", "\n", "            ", "self", ".", "dequantizer", "=", "mu_law_decode", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "f\"Unknown quantization type: {self.quantization}\"", ")", "\n", "\n", "", "if", "not", "self", ".", "reproduce", ":", "\n", "            ", "self", ".", "encoder", "=", "torch", ".", "nn", ".", "Embedding", "(", "1", "<<", "bits", ",", "d_model", ")", "\n", "\n", "", "ns_frame_samples", "=", "map", "(", "int", ",", "np", ".", "cumprod", "(", "frame_sizes", ")", ")", "# e.g. (16, 4) -> (16, 64)", "\n", "self", ".", "frame_level_rnns", "=", "torch", ".", "nn", ".", "ModuleList", "(", "[", "\n", "FrameLevelRNN", "(", "\n", "frame_size", "=", "frame_size", ",", "\n", "n_frame_samples", "=", "n_frame_samples", ",", "\n", "d_model", "=", "d_model", ",", "\n", "n_rnn", "=", "n_rnn", ",", "\n", "d_hidden", "=", "d_hidden", ",", "\n", "learn_h0", "=", "learn_h0", ",", "\n", "weight_norm", "=", "weight_norm", ",", "\n", "reproduce", "=", "reproduce", ",", "\n", "layer", "=", "layer", ",", "\n", ")", "\n", "for", "(", "frame_size", ",", "n_frame_samples", ")", "in", "zip", "(", "frame_sizes", ",", "ns_frame_samples", ")", "\n", "]", ")", "\n", "\n", "self", ".", "sample_level_mlp", "=", "SampleLevelMLP", "(", "\n", "frame_size", "=", "frame_sizes", "[", "0", "]", ",", "\n", "d_hidden", "=", "d_hidden", ",", "\n", "bits", "=", "bits", ",", "\n", "d_model", "=", "d_model", ",", "\n", "weight_norm", "=", "weight_norm", ",", "\n", "reproduce", "=", "reproduce", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.samplernn.SampleRNN.default_state": [[323, 325], ["rnn.default_state"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.sashimi.sashimi.Sashimi.default_state"], ["", "def", "default_state", "(", "self", ",", "batch_size", ",", "device", "=", "None", ")", ":", "\n", "        ", "return", "[", "rnn", ".", "default_state", "(", "batch_size", ",", "device", "=", "device", ")", "for", "rnn", "in", "self", ".", "frame_level_rnns", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.samplernn.SampleRNN.step": [[326, 408], ["zip", "list", "[].unsqueeze", "samplernn.SampleRNN.sample_level_mlp", "len", "x.unsqueeze.unsqueeze.unsqueeze", "samplernn.SampleRNN.default_state", "samplernn.SampleRNN._window[].clone", "reversed", "reversed", "rnn", "list.append", "reversed", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "src.dataloaders.audio.q_zero", "len", "list", "list.append", "samplernn.SampleRNN.dequantizer", "prev_samples.view.view.view", "samplernn.SampleRNN.encoder", "prev_samples.view.view.contiguous", "prev_samples.view.view.view", "[].unsqueeze", "enumerate", "prev_samples.view.view.contiguous", "len", "len"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.sashimi.sashimi.Sashimi.default_state"], ["", "def", "step", "(", "self", ",", "x", ",", "state", "=", "None", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "if", "len", "(", "x", ".", "shape", ")", "==", "1", ":", "\n", "            ", "x", "=", "x", ".", "unsqueeze", "(", "1", ")", "\n", "", "batch_size", "=", "x", ".", "shape", "[", "0", "]", "\n", "\n", "if", "state", "is", "None", ":", "\n", "            ", "state", "=", "self", ".", "default_state", "(", "batch_size", ",", "device", "=", "x", ".", "device", ")", "\n", "self", ".", "_frame_level_outputs", "=", "[", "None", "for", "_", "in", "self", ".", "frame_level_rnns", "]", "\n", "self", ".", "_window", "=", "torch", ".", "zeros", "(", "\n", "batch_size", ",", "\n", "self", ".", "lookback", ",", "\n", "x", ".", "shape", "[", "1", "]", "if", "len", "(", "x", ".", "shape", ")", "==", "2", "else", "x", ".", "shape", "[", "2", "]", ",", "\n", "dtype", "=", "x", ".", "dtype", ",", "\n", "device", "=", "x", ".", "device", ",", "\n", ")", "+", "q_zero", "(", "bits", "=", "self", ".", "bits", ")", "\n", "self", ".", "_step_idx", "=", "self", ".", "lookback", "\n", "\n", "if", "len", "(", "x", ".", "shape", ")", "==", "3", ":", "\n", "                ", "assert", "x", ".", "shape", "[", "1", "]", "==", "self", ".", "lookback", "\n", "self", ".", "_window", "=", "x", "\n", "\n", "", "", "if", "self", ".", "_step_idx", ">", "self", ".", "lookback", ":", "\n", "# Update window (but on the first step)", "\n", "            ", "self", ".", "_window", "[", ":", ",", ":", "-", "1", "]", "=", "self", ".", "_window", "[", ":", ",", "1", ":", "]", ".", "clone", "(", ")", "\n", "self", ".", "_window", "[", ":", ",", "-", "1", "]", "=", "x", "\n", "\n", "", "new_states", "=", "[", "]", "\n", "\n", "for", "(", "i", ",", "rnn", ")", ",", "state_", "in", "zip", "(", "reversed", "(", "list", "(", "enumerate", "(", "self", ".", "frame_level_rnns", ")", ")", ")", ",", "reversed", "(", "state", ")", ")", ":", "\n", "            ", "if", "self", ".", "_step_idx", "%", "rnn", ".", "n_frame_samples", "!=", "0", ":", "\n", "# Don't need to process this rnn", "\n", "                ", "new_states", ".", "append", "(", "state_", ")", "\n", "continue", "\n", "\n", "# prev_samples shape: (B, CHUNK_SIZE, D) e.g. (16, 16384, 1)", "\n", "", "prev_samples", "=", "self", ".", "_window", "[", ":", ",", "-", "rnn", ".", "n_frame_samples", ":", "]", "\n", "\n", "if", "self", ".", "reproduce", ":", "\n", "# SampleRNN dequantizes to recover the raw audio signal before passing this to the RNN", "\n", "                ", "prev_samples", "=", "self", ".", "dequantizer", "(", "prev_samples", ",", "bits", "=", "self", ".", "bits", ")", "\n", "prev_samples", "=", "2", "*", "prev_samples", ".", "contiguous", "(", ")", "\n", "# Below, reshape from (B, CHUNK_SIZE, D) -> (B, -1, rnn.n_frame_samples) = (B, M_i, F_i)", "\n", "# e.g. (16, 16384, 1) -> (16, 256, 64) [first rnn] | (16, 1024, 16) [second rnn]", "\n", "prev_samples", "=", "prev_samples", ".", "view", "(", "batch_size", ",", "-", "1", ",", "rnn", ".", "n_frame_samples", ")", "\n", "\n", "", "else", ":", "\n", "                ", "raise", "NotImplementedError", "\n", "# More generally, we can use an Embedding encoder instead", "\n", "prev_samples", "=", "self", ".", "encoder", "(", "prev_samples", ")", "\n", "prev_samples", "=", "prev_samples", ".", "contiguous", "(", ")", "\n", "prev_samples", "=", "prev_samples", ".", "view", "(", "batch_size", ",", "-", "1", ",", "rnn", ".", "n_frame_samples", ",", "self", ".", "d_model", ")", "\n", "\n", "# upper_tier_conditioning shape: None -> (B, M, D_HIDDEN) [first rnn]", "\n", "# (B, M_{i-1}, D_HIDDEN) -> (B, M_i, D_HIDDEN) [second rnn]", "\n", "", "if", "i", "==", "len", "(", "self", ".", "frame_level_rnns", ")", "-", "1", ":", "\n", "                ", "upper_tier_conditioning", "=", "None", "\n", "", "else", ":", "\n", "                ", "frame_index", "=", "(", "self", ".", "_step_idx", "//", "rnn", ".", "n_frame_samples", ")", "%", "self", ".", "frame_level_rnns", "[", "i", "+", "1", "]", ".", "frame_size", "\n", "upper_tier_conditioning", "=", "self", ".", "_frame_level_outputs", "[", "i", "+", "1", "]", "[", ":", ",", "frame_index", ",", ":", "]", ".", "unsqueeze", "(", "1", ")", "\n", "\n", "", "upper_tier_conditioning", ",", "new_state", "=", "rnn", "(", "prev_samples", ",", "upper_tier_conditioning", ",", "state_", ")", "\n", "\n", "self", ".", "_frame_level_outputs", "[", "i", "]", "=", "upper_tier_conditioning", "\n", "\n", "new_states", ".", "append", "(", "new_state", ")", "\n", "\n", "# Make sure new states are in the right order", "\n", "", "new_states", "=", "list", "(", "reversed", "(", "new_states", ")", ")", "\n", "\n", "bottom_frame_size", "=", "self", ".", "frame_level_rnns", "[", "0", "]", ".", "frame_size", "\n", "mlp_input_sequences", "=", "self", ".", "_window", "[", ":", ",", "-", "bottom_frame_size", ":", "]", "\n", "\n", "# Upper tier conditioning for the bottom", "\n", "upper_tier_conditioning", "=", "self", ".", "_frame_level_outputs", "[", "0", "]", "[", ":", ",", "self", ".", "_step_idx", "%", "bottom_frame_size", ",", ":", "]", ".", "unsqueeze", "(", "1", ")", "\n", "y", "=", "self", ".", "sample_level_mlp", "(", "mlp_input_sequences", ",", "upper_tier_conditioning", ")", "\n", "\n", "# Update window and step", "\n", "self", ".", "_step_idx", "+=", "1", "\n", "\n", "# mlp_input_sequences shape: (B, L - _, D) e.g. (16, 16399, 1)", "\n", "# upper_tier_conditioning shape: (B, M_{last_rnn}, D_HIDDEN) [last rnn]", "\n", "return", "y", ",", "new_states", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.samplernn.SampleRNN.lookback": [[409, 412], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "lookback", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "frame_level_rnns", "[", "-", "1", "]", ".", "n_frame_samples", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.samplernn.SampleRNN.forward": [[413, 463], ["zip", "list", "samplernn.SampleRNN.default_state", "reversed", "reversed", "rnn", "list.append", "reversed", "samplernn.SampleRNN.sample_level_mlp", "samplernn.SampleRNN.dequantizer", "prev_samples.view.view.view", "samplernn.SampleRNN.encoder", "prev_samples.view.view.contiguous", "prev_samples.view.view.view", "prev_samples.view.view.contiguous"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.sashimi.sashimi.Sashimi.default_state"], ["", "def", "forward", "(", "self", ",", "inputs", ",", "*", "args", ",", "state", "=", "None", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"\n        inputs shape: (B, L, D) e.g. (16, 16447, 1)\n\n        For SampleRNN, inputs contains quantized audio samples (e.g. B elements of length L)\n        \"\"\"", "\n", "batch_size", "=", "inputs", ".", "shape", "[", "0", "]", "\n", "\n", "if", "state", "is", "None", ":", "\n", "            ", "state", "=", "self", ".", "default_state", "(", "batch_size", ",", "inputs", ".", "device", ")", "\n", "\n", "", "upper_tier_conditioning", "=", "None", "\n", "new_states", "=", "[", "]", "\n", "for", "rnn", ",", "state_", "in", "zip", "(", "reversed", "(", "self", ".", "frame_level_rnns", ")", ",", "reversed", "(", "state", ")", ")", ":", "\n", "# TODO: explain this", "\n", "            ", "from_index", "=", "self", ".", "lookback", "-", "rnn", ".", "n_frame_samples", "\n", "to_index", "=", "-", "rnn", ".", "n_frame_samples", "+", "1", "\n", "\n", "# prev_samples shape: (B, CHUNK_SIZE, D) e.g. (16, 16384, 1)", "\n", "prev_samples", "=", "inputs", "[", ":", ",", "from_index", ":", "to_index", "]", "\n", "\n", "if", "self", ".", "reproduce", ":", "\n", "# SampleRNN dequantizes to recover the raw audio signal before passing this to the RNN", "\n", "                ", "prev_samples", "=", "self", ".", "dequantizer", "(", "prev_samples", ",", "bits", "=", "self", ".", "bits", ")", "\n", "prev_samples", "=", "2", "*", "prev_samples", ".", "contiguous", "(", ")", "\n", "# Below, reshape from (B, CHUNK_SIZE, D) -> (B, -1, rnn.n_frame_samples) = (B, M_i, F_i)", "\n", "# e.g. (16, 16384, 1) -> (16, 256, 64) [first rnn] | (16, 1024, 16) [second rnn]", "\n", "prev_samples", "=", "prev_samples", ".", "view", "(", "batch_size", ",", "-", "1", ",", "rnn", ".", "n_frame_samples", ")", "\n", "\n", "", "else", ":", "\n", "# More generally, we can use an Embedding encoder instead", "\n", "                ", "prev_samples", "=", "self", ".", "encoder", "(", "prev_samples", ")", "\n", "prev_samples", "=", "prev_samples", ".", "contiguous", "(", ")", "\n", "prev_samples", "=", "prev_samples", ".", "view", "(", "batch_size", ",", "-", "1", ",", "rnn", ".", "n_frame_samples", ",", "self", ".", "d_model", ")", "\n", "\n", "# upper_tier_conditioning shape: None -> (B, M, D_HIDDEN) [first rnn]", "\n", "# (B, M_{i-1}, D_HIDDEN) -> (B, M_i, D_HIDDEN) [second rnn]", "\n", "", "upper_tier_conditioning", ",", "new_state", "=", "rnn", "(", "prev_samples", ",", "upper_tier_conditioning", ",", "state_", ")", "\n", "\n", "new_states", ".", "append", "(", "new_state", ")", "\n", "\n", "# Make sure new states are in the right order", "\n", "", "new_states", "=", "list", "(", "reversed", "(", "new_states", ")", ")", "\n", "\n", "bottom_frame_size", "=", "self", ".", "frame_level_rnns", "[", "0", "]", ".", "frame_size", "\n", "mlp_input_sequences", "=", "inputs", "[", ":", ",", "self", ".", "lookback", "-", "bottom_frame_size", ":", "]", "\n", "\n", "# mlp_input_sequences shape: (B, L - _, D) e.g. (16, 16399, 1)", "\n", "# upper_tier_conditioning shape: (B, M_{last_rnn}, D_HIDDEN) [last rnn]", "\n", "return", "self", ".", "sample_level_mlp", "(", "mlp_input_sequences", ",", "upper_tier_conditioning", ")", ",", "new_states", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.samplernn.FrameLevelRNN.__init__": [[484, 584], ["super().__init__", "torch.nn.Conv1d", "torch.nn.Conv1d", "torch.nn.Conv1d", "torch.nn.Conv1d", "torch.nn.init.kaiming_uniform", "torch.nn.init.kaiming_uniform", "torch.nn.init.kaiming_uniform", "torch.nn.init.kaiming_uniform", "torch.nn.init.constant", "torch.nn.init.constant", "torch.nn.init.constant", "torch.nn.init.constant", "torch.nn.Conv1d", "torch.nn.Conv1d", "torch.nn.Conv1d", "torch.nn.Conv1d", "src.models.sequence.rnns.gru.TorchGRU", "samplernn.LearnedUpsampling1d", "torch.nn.init.uniform", "torch.nn.init.uniform", "torch.nn.init.uniform", "torch.nn.init.uniform", "torch.nn.init.constant", "torch.nn.init.constant", "torch.nn.init.constant", "torch.nn.init.constant", "torch.nn.ConvTranspose1d", "torch.nn.ConvTranspose1d", "torch.nn.ConvTranspose1d", "torch.nn.ConvTranspose1d", "torch.nn.utils.weight_norm", "torch.nn.utils.weight_norm", "torch.nn.utils.weight_norm", "torch.nn.utils.weight_norm", "torch.nn.utils.weight_norm", "torch.nn.utils.weight_norm", "torch.nn.utils.weight_norm", "torch.nn.utils.weight_norm", "torch.nn.utils.weight_norm", "torch.nn.utils.weight_norm", "torch.nn.utils.weight_norm", "torch.nn.utils.weight_norm", "torch.nn.utils.weight_norm", "torch.nn.utils.weight_norm", "torch.nn.utils.weight_norm", "torch.nn.utils.weight_norm", "src.models.sequence.ss.s4.S4", "range", "numpy.sqrt", "samplernn.concat_init", "torch.nn.init.constant", "torch.nn.init.constant", "torch.nn.init.constant", "torch.nn.init.constant", "samplernn.concat_init", "torch.nn.init.constant", "torch.nn.init.constant", "torch.nn.init.constant", "torch.nn.init.constant", "numpy.sqrt", "getattr", "getattr", "getattr", "getattr"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.samplernn.concat_init", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.samplernn.concat_init"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "frame_size", ",", "\n", "n_frame_samples", ",", "\n", "d_model", ",", "\n", "n_rnn", ",", "\n", "d_hidden", ",", "\n", "learn_h0", "=", "True", ",", "\n", "weight_norm", "=", "True", ",", "\n", "reproduce", "=", "False", ",", "\n", "layer", "=", "'gru'", ",", "\n", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "frame_size", "=", "frame_size", "\n", "self", ".", "n_frame_samples", "=", "n_frame_samples", "\n", "self", ".", "d_model", "=", "d_model", "\n", "self", ".", "d_hidden", "=", "d_hidden", "\n", "self", ".", "n_rnn", "=", "n_rnn", "\n", "self", ".", "learn_h0", "=", "learn_h0", "\n", "self", ".", "weight_norm", "=", "weight_norm", "\n", "self", ".", "reproduce", "=", "reproduce", "\n", "self", ".", "layer", "=", "layer", "\n", "\n", "if", "self", ".", "reproduce", ":", "\n", "            ", "assert", "learn_h0", ",", "\"Original SampleRNN FrameLevelRNN learns h0.\"", "\n", "assert", "weight_norm", ",", "\"Original SampleRNN FrameLevelRNN uses weight norm.\"", "\n", "\n", "", "if", "reproduce", ":", "\n", "            ", "self", ".", "input_expand", "=", "torch", ".", "nn", ".", "Conv1d", "(", "\n", "in_channels", "=", "n_frame_samples", ",", "\n", "out_channels", "=", "d_hidden", ",", "\n", "kernel_size", "=", "1", ",", "\n", ")", "\n", "torch", ".", "nn", ".", "init", ".", "kaiming_uniform", "(", "self", ".", "input_expand", ".", "weight", ")", "\n", "torch", ".", "nn", ".", "init", ".", "constant", "(", "self", ".", "input_expand", ".", "bias", ",", "0", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "input_expand", "=", "torch", ".", "nn", ".", "Conv1d", "(", "\n", "in_channels", "=", "d_model", ",", "\n", "out_channels", "=", "d_hidden", ",", "\n", "kernel_size", "=", "n_frame_samples", ",", "\n", "stride", "=", "n_frame_samples", ",", "\n", ")", "\n", "\n", "", "if", "self", ".", "layer", "==", "'gru'", ":", "\n", "            ", "self", ".", "rnn", "=", "TorchGRU", "(", "\n", "d_model", "=", "d_hidden", ",", "\n", "d_hidden", "=", "d_hidden", ",", "\n", "n_layers", "=", "n_rnn", ",", "\n", "learn_h0", "=", "learn_h0", ",", "\n", ")", "\n", "", "elif", "self", ".", "layer", "==", "'s4'", ":", "\n", "            ", "self", ".", "rnn", "=", "S4", "(", "\n", "H", "=", "d_hidden", ",", "\n", "d_state", "=", "64", ",", "\n", "use_state", "=", "False", ",", "\n", ")", "\n", "\n", "", "if", "reproduce", ":", "\n", "\n", "            ", "if", "self", ".", "layer", "==", "'gru'", ":", "\n", "                ", "for", "i", "in", "range", "(", "n_rnn", ")", ":", "\n", "                    ", "concat_init", "(", "\n", "getattr", "(", "self", ".", "rnn", ",", "'weight_ih_l{}'", ".", "format", "(", "i", ")", ")", ",", "\n", "[", "lecun_uniform", ",", "lecun_uniform", ",", "lecun_uniform", "]", "\n", ")", "\n", "torch", ".", "nn", ".", "init", ".", "constant", "(", "getattr", "(", "self", ".", "rnn", ",", "'bias_ih_l{}'", ".", "format", "(", "i", ")", ")", ",", "0", ")", "\n", "\n", "concat_init", "(", "\n", "getattr", "(", "self", ".", "rnn", ",", "'weight_hh_l{}'", ".", "format", "(", "i", ")", ")", ",", "\n", "[", "lecun_uniform", ",", "lecun_uniform", ",", "torch", ".", "nn", ".", "init", ".", "orthogonal", "]", "\n", ")", "\n", "torch", ".", "nn", ".", "init", ".", "constant", "(", "getattr", "(", "self", ".", "rnn", ",", "'bias_hh_l{}'", ".", "format", "(", "i", ")", ")", ",", "0", ")", "\n", "\n", "", "", "self", ".", "upsampling", "=", "LearnedUpsampling1d", "(", "\n", "in_channels", "=", "d_hidden", ",", "\n", "out_channels", "=", "d_hidden", ",", "\n", "kernel_size", "=", "frame_size", ",", "\n", ")", "\n", "\n", "torch", ".", "nn", ".", "init", ".", "uniform", "(", "\n", "self", ".", "upsampling", ".", "conv_t", ".", "weight", ",", "-", "np", ".", "sqrt", "(", "6", "/", "d_hidden", ")", ",", "np", ".", "sqrt", "(", "6", "/", "d_hidden", ")", "\n", ")", "\n", "torch", ".", "nn", ".", "init", ".", "constant", "(", "self", ".", "upsampling", ".", "bias", ",", "0", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "upsampling", "=", "torch", ".", "nn", ".", "ConvTranspose1d", "(", "\n", "in_channels", "=", "d_hidden", ",", "\n", "out_channels", "=", "d_hidden", ",", "\n", "kernel_size", "=", "frame_size", ",", "\n", "stride", "=", "frame_size", ",", "\n", "bias", "=", "True", ",", "\n", ")", "\n", "\n", "\n", "", "if", "weight_norm", "and", "reproduce", ":", "\n", "            ", "self", ".", "input_expand", "=", "torch", ".", "nn", ".", "utils", ".", "weight_norm", "(", "self", ".", "input_expand", ")", "\n", "self", ".", "upsampling", ".", "conv_t", "=", "torch", ".", "nn", ".", "utils", ".", "weight_norm", "(", "self", ".", "upsampling", ".", "conv_t", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "input_expand", "=", "torch", ".", "nn", ".", "utils", ".", "weight_norm", "(", "self", ".", "input_expand", ")", "\n", "self", ".", "upsampling", "=", "torch", ".", "nn", ".", "utils", ".", "weight_norm", "(", "self", ".", "upsampling", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.samplernn.FrameLevelRNN.default_state": [[585, 590], ["samplernn.FrameLevelRNN.rnn.default_state"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.sashimi.sashimi.Sashimi.default_state"], ["", "", "def", "default_state", "(", "self", ",", "batch_size", ",", "device", "=", "None", ")", ":", "\n", "        ", "if", "self", ".", "layer", "==", "'gru'", ":", "\n", "            ", "return", "self", ".", "rnn", ".", "default_state", "(", "batch_size", ",", "device", "=", "device", ")", "\n", "", "elif", "self", ".", "layer", "==", "'s4'", ":", "\n", "            ", "return", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.samplernn.FrameLevelRNN.forward": [[591, 623], ["samplernn.FrameLevelRNN.upsampling().permute", "prev_samples.view.view.view", "samplernn.FrameLevelRNN.input_expand().permute", "samplernn.FrameLevelRNN.input_expand().permute", "samplernn.FrameLevelRNN.rnn", "state.contiguous", "samplernn.FrameLevelRNN.rnn", "output.transpose.transpose.transpose", "samplernn.FrameLevelRNN.upsampling", "samplernn.FrameLevelRNN.input_expand", "samplernn.FrameLevelRNN.input_expand", "samplernn.FrameLevelRNN.transpose", "output.transpose.transpose.permute", "prev_samples.view.view.permute", "prev_samples.view.view.permute"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "prev_samples", ",", "upper_tier_conditioning", ",", "state", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        prev_samples: (B, M_i, D_MODEL) if self.reproduce else (B, M_i, FRAME, D_MODEL)\n        upper_tier_conditioning: (B, M_i, D_HIDDEN) or None\n        \"\"\"", "\n", "if", "not", "self", ".", "reproduce", ":", "\n", "# Use strided convolutions to get frame embeddings", "\n", "# This generalizes the SampleRNN operation to handle non-1D signals ", "\n", "# This reshapes from (B, M_i, FRAME, D_MODEL) -> (B, M_i, D_HIDDEN)", "\n", "            ", "prev_samples", "=", "prev_samples", ".", "view", "(", "prev_samples", ".", "shape", "[", "0", "]", ",", "-", "1", ",", "self", ".", "d_model", ")", "\n", "input", "=", "self", ".", "input_expand", "(", "prev_samples", ".", "permute", "(", "0", ",", "2", ",", "1", ")", ")", ".", "permute", "(", "0", ",", "2", ",", "1", ")", "\n", "", "else", ":", "\n", "# SampleRNN uses an MLP (implemented as 1D Conv) to map (FRAME_SIZE, 1) to D_HIDDEN", "\n", "# This reshapes from (B, M_i, FRAME) -> (B, M_i, D_HIDDEN)", "\n", "            ", "input", "=", "self", ".", "input_expand", "(", "prev_samples", ".", "permute", "(", "0", ",", "2", ",", "1", ")", ")", ".", "permute", "(", "0", ",", "2", ",", "1", ")", "\n", "\n", "", "if", "upper_tier_conditioning", "is", "not", "None", ":", "\n", "            ", "input", "+=", "upper_tier_conditioning", "\n", "\n", "# Run RNN: (B, M_i, D_HIDDEN) -> (B, M_i, D_HIDDEN)", "\n", "", "if", "self", ".", "layer", "==", "'gru'", ":", "\n", "            ", "output", ",", "state", "=", "self", ".", "rnn", "(", "input", ",", "state", ".", "contiguous", "(", ")", ")", "\n", "", "elif", "self", ".", "layer", "==", "'s4'", ":", "\n", "# TODO: not working", "\n", "            ", "output", ",", "state", "=", "self", ".", "rnn", "(", "input", ".", "transpose", "(", "1", ",", "2", ")", ",", "state", ")", "\n", "output", "=", "output", ".", "transpose", "(", "1", ",", "2", ")", "\n", "\n", "# Run 1D transposed convolution to upsample: (B, M_i, D_HIDDEN) -> (B, M', D_HIDDEN)", "\n", "# TODO: make M' more precise", "\n", "", "output", "=", "self", ".", "upsampling", "(", "output", ".", "permute", "(", "0", ",", "2", ",", "1", ")", ")", ".", "permute", "(", "0", ",", "2", ",", "1", ")", "\n", "\n", "return", "output", ",", "state", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.samplernn.SampleLevelMLP.__init__": [[627, 685], ["super().__init__", "torch.nn.Conv1d", "torch.nn.Conv1d", "torch.nn.Conv1d", "torch.nn.Conv1d", "torch.nn.Embedding", "torch.nn.Embedding", "torch.nn.Embedding", "torch.nn.Embedding", "torch.nn.Conv1d", "torch.nn.Conv1d", "torch.nn.Conv1d", "torch.nn.Conv1d", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Conv1d", "torch.nn.Conv1d", "torch.nn.Conv1d", "torch.nn.Conv1d", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.init.kaiming_uniform", "torch.nn.init.kaiming_uniform", "torch.nn.init.kaiming_uniform", "torch.nn.init.kaiming_uniform", "torch.nn.init.kaiming_uniform", "torch.nn.init.kaiming_uniform", "torch.nn.init.kaiming_uniform", "torch.nn.init.kaiming_uniform", "torch.nn.init.constant", "torch.nn.init.constant", "torch.nn.init.constant", "torch.nn.init.constant", "samplernn.lecun_uniform", "torch.nn.init.constant", "torch.nn.init.constant", "torch.nn.init.constant", "torch.nn.init.constant", "torch.nn.utils.weight_norm", "torch.nn.utils.weight_norm", "torch.nn.utils.weight_norm", "torch.nn.utils.weight_norm", "torch.nn.utils.weight_norm", "torch.nn.utils.weight_norm", "torch.nn.utils.weight_norm", "torch.nn.utils.weight_norm", "torch.nn.utils.weight_norm", "torch.nn.utils.weight_norm", "torch.nn.utils.weight_norm", "torch.nn.utils.weight_norm"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.samplernn.lecun_uniform"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "frame_size", ",", "\n", "d_hidden", ",", "\n", "bits", "=", "8", ",", "\n", "d_model", "=", "256", ",", "\n", "weight_norm", "=", "True", ",", "\n", "embedding", "=", "True", ",", "\n", "reproduce", "=", "False", ",", "\n", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "d_model", "=", "d_model", "\n", "self", ".", "reproduce", "=", "reproduce", "\n", "\n", "if", "self", ".", "reproduce", ":", "\n", "            ", "assert", "embedding", ",", "\"Original SampleRNN SampleLevelMLP uses an embedding layer.\"", "\n", "assert", "weight_norm", ",", "\"Original SampleRNN SampleLevelMLP uses weight norm.\"", "\n", "\n", "", "if", "embedding", ":", "\n", "            ", "self", ".", "embedding", "=", "torch", ".", "nn", ".", "Embedding", "(", "1", "<<", "bits", ",", "d_model", ")", "\n", "\n", "", "self", ".", "input", "=", "torch", ".", "nn", ".", "Conv1d", "(", "\n", "in_channels", "=", "d_model", ",", "\n", "out_channels", "=", "d_hidden", ",", "\n", "kernel_size", "=", "frame_size", ",", "\n", "bias", "=", "False", ",", "\n", ")", "\n", "\n", "if", "self", ".", "reproduce", ":", "\n", "            ", "self", ".", "hidden", "=", "torch", ".", "nn", ".", "Conv1d", "(", "\n", "in_channels", "=", "d_hidden", ",", "\n", "out_channels", "=", "d_hidden", ",", "\n", "kernel_size", "=", "1", ",", "\n", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "hidden", "=", "torch", ".", "nn", ".", "Linear", "(", "d_hidden", ",", "d_hidden", ")", "\n", "\n", "", "if", "self", ".", "reproduce", ":", "\n", "            ", "self", ".", "output", "=", "torch", ".", "nn", ".", "Conv1d", "(", "\n", "in_channels", "=", "d_hidden", ",", "\n", "out_channels", "=", "256", ",", "\n", "kernel_size", "=", "1", ",", "\n", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "output", "=", "torch", ".", "nn", ".", "Linear", "(", "d_hidden", ",", "256", ")", "\n", "\n", "", "if", "self", ".", "reproduce", ":", "\n", "            ", "torch", ".", "nn", ".", "init", ".", "kaiming_uniform", "(", "self", ".", "input", ".", "weight", ")", "\n", "torch", ".", "nn", ".", "init", ".", "kaiming_uniform", "(", "self", ".", "hidden", ".", "weight", ")", "\n", "torch", ".", "nn", ".", "init", ".", "constant", "(", "self", ".", "hidden", ".", "bias", ",", "0", ")", "\n", "lecun_uniform", "(", "self", ".", "output", ".", "weight", ")", "\n", "torch", ".", "nn", ".", "init", ".", "constant", "(", "self", ".", "output", ".", "bias", ",", "0", ")", "\n", "\n", "", "if", "weight_norm", ":", "\n", "            ", "self", ".", "input", "=", "torch", ".", "nn", ".", "utils", ".", "weight_norm", "(", "self", ".", "input", ")", "\n", "self", ".", "hidden", "=", "torch", ".", "nn", ".", "utils", ".", "weight_norm", "(", "self", ".", "hidden", ")", "\n", "self", ".", "output", "=", "torch", ".", "nn", ".", "utils", ".", "weight_norm", "(", "self", ".", "output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.samplernn.SampleLevelMLP.forward": [[686, 715], ["samplernn.SampleLevelMLP.permute", "upper_tier_conditioning.permute.permute.permute", "samplernn.SampleLevelMLP.contiguous", "samplernn.SampleLevelMLP.embedding().view", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "samplernn.SampleLevelMLP.output().permute", "torch.relu", "torch.relu", "samplernn.SampleLevelMLP.permute", "torch.relu", "torch.relu", "samplernn.SampleLevelMLP.output", "samplernn.SampleLevelMLP.hidden", "samplernn.SampleLevelMLP.hidden", "samplernn.SampleLevelMLP.embedding", "samplernn.SampleLevelMLP.input", "samplernn.SampleLevelMLP.output", "samplernn.SampleLevelMLP.input", "samplernn.SampleLevelMLP.contiguous().view", "samplernn.SampleLevelMLP.contiguous"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "prev_samples", ",", "upper_tier_conditioning", ")", ":", "\n", "        ", "if", "self", ".", "embedding", ":", "\n", "# Embed the input samples (which are quantized)", "\n", "# This reshapes from (B, L, 1) -> (B, L, D_MODEL)", "\n", "            ", "prev_samples", "=", "self", ".", "embedding", "(", "\n", "prev_samples", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ")", "\n", ")", ".", "view", "(", "prev_samples", ".", "shape", "[", "0", "]", ",", "-", "1", ",", "self", ".", "d_model", ")", "\n", "\n", "", "assert", "prev_samples", ".", "shape", "[", "-", "1", "]", "==", "self", ".", "d_model", ",", "\"`prev_samples` shape should be (B, L', D_MODEL)\"", "\n", "\n", "# prev_samples: (B, L', D_MODEL) -> (B, D_MODEL, L')", "\n", "# upper_tier_conditioning: (B, L, D_HIDDEN) -> (B, D_HIDDEN, L)", "\n", "prev_samples", "=", "prev_samples", ".", "permute", "(", "0", ",", "2", ",", "1", ")", "\n", "upper_tier_conditioning", "=", "upper_tier_conditioning", ".", "permute", "(", "0", ",", "2", ",", "1", ")", "\n", "\n", "if", "self", ".", "reproduce", ":", "\n", "# Take (B, L', D_MODEL), (B, L, D_HIDDEN) -> (B, D_HIDDEN, L)", "\n", "            ", "x", "=", "F", ".", "relu", "(", "self", ".", "input", "(", "prev_samples", ")", "+", "upper_tier_conditioning", ")", "\n", "x", "=", "F", ".", "relu", "(", "self", ".", "hidden", "(", "x", ")", ")", "\n", "x", "=", "self", ".", "output", "(", "x", ")", ".", "permute", "(", "0", ",", "2", ",", "1", ")", "\n", "", "else", ":", "\n", "# Take (B, L', D_MODEL), (B, L, D_HIDDEN) -> (B, D_HIDDEN, L)", "\n", "            ", "x", "=", "F", ".", "relu", "(", "self", ".", "input", "(", "prev_samples", ")", "+", "upper_tier_conditioning", ")", "\n", "# x: (B, D_HIDDEN, L) -> (B, L, D_HIDDEN)", "\n", "x", "=", "x", ".", "permute", "(", "0", ",", "2", ",", "1", ")", "\n", "x", "=", "F", ".", "relu", "(", "self", ".", "hidden", "(", "x", ")", ")", "\n", "x", "=", "self", ".", "output", "(", "x", ")", "\n", "\n", "", "return", "x", ".", "contiguous", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.samplernn.lecun_uniform": [[464, 467], ["torch.nn.init._calculate_correct_fan", "torch.nn.init._calculate_correct_fan", "torch.nn.init.uniform", "torch.nn.init.uniform", "math.sqrt", "math.sqrt"], "function", ["None"], ["", "", "def", "lecun_uniform", "(", "tensor", ")", ":", "\n", "    ", "fan_in", "=", "torch", ".", "nn", ".", "init", ".", "_calculate_correct_fan", "(", "tensor", ",", "'fan_in'", ")", "\n", "torch", ".", "nn", ".", "init", ".", "uniform", "(", "tensor", ",", "-", "math", ".", "sqrt", "(", "3", "/", "fan_in", ")", ",", "math", ".", "sqrt", "(", "3", "/", "fan_in", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.samplernn.concat_init": [[468, 481], ["tensor.size", "tensor.new", "enumerate", "len", "torch.nn.init"], "function", ["None"], ["", "def", "concat_init", "(", "tensor", ",", "inits", ")", ":", "\n", "    ", "try", ":", "\n", "        ", "tensor", "=", "tensor", ".", "data", "\n", "", "except", "AttributeError", ":", "\n", "        ", "pass", "\n", "\n", "", "(", "length", ",", "fan_out", ")", "=", "tensor", ".", "size", "(", ")", "\n", "fan_in", "=", "length", "//", "len", "(", "inits", ")", "\n", "\n", "chunk", "=", "tensor", ".", "new", "(", "fan_in", ",", "fan_out", ")", "\n", "for", "(", "i", ",", "init", ")", "in", "enumerate", "(", "inits", ")", ":", "\n", "        ", "init", "(", "chunk", ")", "\n", "tensor", "[", "i", "*", "fan_in", ":", "(", "i", "+", "1", ")", "*", "fan_in", ",", ":", "]", "=", "chunk", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.samplernn.test_stacked_rnn": [[717, 730], ["samplernn.StackedRNN", "torch.randn", "torch.randn", "StackedRNN.", "len"], "function", ["None"], ["", "", "def", "test_stacked_rnn", "(", ")", ":", "\n", "    ", "rnn", "=", "StackedRNN", "(", "\n", "d_model", "=", "256", ",", "\n", "d_hidden", "=", "32", ",", "\n", "n_layers", "=", "4", ",", "\n", "skip_connections", "=", "True", ",", "\n", "dropout", "=", "0.0", ",", "\n", "output_linear", "=", "False", ",", "\n", ")", "\n", "x", "=", "torch", ".", "randn", "(", "8", ",", "100", ",", "256", ")", "\n", "y", ",", "states", "=", "rnn", "(", "x", ")", "\n", "assert", "y", ".", "shape", "==", "(", "8", ",", "100", ",", "32", ")", "\n", "assert", "len", "(", "states", ")", "==", "4", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.samplernn.test_rnn_baseline": [[731, 745], ["samplernn.StackedRNNBaseline", "torch.randn", "torch.randn", "StackedRNNBaseline.", "len"], "function", ["None"], ["", "def", "test_rnn_baseline", "(", ")", ":", "\n", "    ", "rnn", "=", "StackedRNNBaseline", "(", "\n", "d_model", "=", "256", ",", "\n", "d_hidden", "=", "32", ",", "\n", "n_layers", "=", "4", ",", "\n", "learned_h0", "=", "True", ",", "\n", "weight_norm", "=", "True", ",", "\n", "skip_connections", "=", "True", ",", "\n", "dropout", "=", "0.0", ",", "\n", ")", "\n", "x", "=", "torch", ".", "randn", "(", "8", ",", "100", ",", "256", ")", "\n", "y", ",", "states", "=", "rnn", "(", "x", ")", "\n", "assert", "y", ".", "shape", "==", "(", "8", ",", "100", ",", "256", ")", "\n", "assert", "len", "(", "states", ")", "==", "4", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.samplernn.test_sample_rnn": [[746, 772], ["SampleRNN().cuda", "torch.randint().cuda", "torch.randint().cuda", "SampleRNN().cuda.", "breakpoint", "torch.no_grad", "torch.no_grad", "SampleRNN().cuda.step", "range", "torch.stack().squeeze().transpose", "torch.stack().squeeze().transpose", "samplernn.SampleRNN", "torch.randint", "torch.randint", "SampleRNN().cuda.step", "ys.append", "torch.stack().squeeze", "torch.stack().squeeze", "torch.stack", "torch.stack"], "function", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.sashimi.sashimi.Sashimi.step", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.sashimi.sashimi.Sashimi.step"], ["", "def", "test_sample_rnn", "(", ")", ":", "\n", "    ", "rnn", "=", "SampleRNN", "(", "\n", "frame_sizes", "=", "(", "16", ",", "4", ")", ",", "\n", "n_rnn", "=", "1", ",", "\n", "d_hidden", "=", "1024", ",", "\n", "bits", "=", "8", ",", "\n", "learn_h0", "=", "True", ",", "\n", "d_model", "=", "256", ",", "\n", "weight_norm", "=", "True", ",", "\n", "reproduce", "=", "True", ",", "\n", "quantization", "=", "'linear'", ",", "\n", ")", ".", "cuda", "(", ")", "\n", "x", "=", "torch", ".", "randint", "(", "0", ",", "255", ",", "(", "2", ",", "1023", ",", "1", ")", ",", "dtype", "=", "torch", ".", "long", ")", ".", "cuda", "(", ")", "\n", "y", ",", "states", "=", "rnn", "(", "x", ")", "\n", "# assert y.shape == (8, 960, 256)", "\n", "# assert len(states) == 4", "\n", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "        ", "y_i", ",", "state", "=", "rnn", ".", "step", "(", "x", "[", ":", ",", ":", "rnn", ".", "lookback", ",", ":", "]", ",", "state", "=", "None", ")", "\n", "ys", "=", "[", "y_i", "]", "\n", "for", "i", "in", "range", "(", "rnn", ".", "lookback", ",", "x", ".", "shape", "[", "1", "]", ")", ":", "\n", "            ", "x_i", "=", "x", "[", ":", ",", "i", ",", ":", "]", "\n", "y_i", ",", "state", "=", "rnn", ".", "step", "(", "x_i", ",", "state", ")", "\n", "ys", ".", "append", "(", "y_i", ")", "\n", "", "y_", "=", "torch", ".", "stack", "(", "ys", ")", ".", "squeeze", "(", ")", ".", "transpose", "(", "0", ",", "1", ")", "\n", "", "breakpoint", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.wavegan.ResidualBlock.__init__": [[18, 24], ["torch.Module.__init__", "src.models.nn.components.Normalization", "torch.Dropout2d", "torch.Dropout2d", "torch.Dropout2d", "torch.Dropout2d"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__"], ["    ", "def", "__init__", "(", "self", ",", "d", ",", "layer", ",", "norm", "=", "'none'", ",", "dropout", "=", "0.0", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "d", "=", "d", "\n", "self", ".", "layer", "=", "layer", "\n", "self", ".", "norm", "=", "Normalization", "(", "d", ",", "transposed", "=", "True", ",", "_name_", "=", "norm", ")", "\n", "self", ".", "drop", "=", "nn", ".", "Dropout2d", "(", "dropout", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.wavegan.ResidualBlock.forward": [[25, 32], ["wavegan.ResidualBlock.layer", "torch.leaky_relu", "torch.leaky_relu", "torch.leaky_relu", "torch.leaky_relu", "wavegan.ResidualBlock.drop", "wavegan.ResidualBlock.norm"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "y", "=", "self", ".", "layer", "(", "x", ")", "\n", "y", "=", "F", ".", "leaky_relu", "(", "y", ",", "negative_slope", "=", "0.2", ")", "\n", "y", "=", "self", ".", "drop", "(", "y", ")", "\n", "y", "=", "x", "+", "y", "\n", "y", "=", "self", ".", "norm", "(", "y", ")", "\n", "return", "y", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.wavegan.Conv1DBlock.__init__": [[34, 73], ["torch.Module.__init__", "range", "layers.append", "layers.append", "layers.append", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "layers.append", "torch.Conv1d", "torch.Conv1d", "torch.Conv1d", "torch.Conv1d", "src.models.nn.components.Normalization", "torch.Dropout2d", "torch.Dropout2d", "torch.Dropout2d", "torch.Dropout2d", "wavegan.ResidualBlock", "torch.Conv1d", "torch.Conv1d", "torch.Conv1d", "torch.Conv1d"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "input_channels", ",", "\n", "output_channels", ",", "\n", "kernel_size", ",", "\n", "causal", "=", "True", ",", "\n", "stride", "=", "4", ",", "\n", "# padding=12,", "\n", "# alpha=0.2,", "\n", "n_layers", "=", "1", ",", "\n", "norm", "=", "'none'", ",", "\n", "dropout", "=", "0", ",", "\n", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "layers", "=", "[", "]", "\n", "# Residual convolution layers", "\n", "# padding = (kernel_size-1, 0) if causal else (kernel_size-1)//2", "\n", "padding", "=", "(", "kernel_size", "-", "1", ")", "//", "2", "\n", "for", "_", "in", "range", "(", "n_layers", "-", "1", ")", ":", "\n", "            ", "layers", ".", "append", "(", "ResidualBlock", "(", "\n", "input_channels", ",", "\n", "nn", ".", "Conv1d", "(", "input_channels", ",", "input_channels", ",", "kernel_size", ",", "stride", "=", "1", ",", "padding", "=", "padding", ")", ",", "\n", "norm", "=", "norm", ",", "\n", "dropout", "=", "dropout", ",", "\n", ")", ")", "\n", "\n", "# Final non-residual conv layer with channel upsizing", "\n", "", "layers", ".", "append", "(", "nn", ".", "Conv1d", "(", "\n", "input_channels", ",", "output_channels", ",", "kernel_size", ",", "stride", "=", "stride", ",", "padding", "=", "padding", ",", "\n", ")", ")", "\n", "layers", ".", "append", "(", "\n", "Normalization", "(", "output_channels", ",", "True", ",", "norm", ")", "\n", "# nn.BatchNorm1d(output_channels)", "\n", "# if use_batch_norm", "\n", "# else nn.Identity()", "\n", ")", "\n", "# self.alpha = alpha", "\n", "layers", ".", "append", "(", "nn", ".", "Dropout2d", "(", "dropout", ")", ")", "\n", "self", ".", "layers", "=", "nn", ".", "Sequential", "(", "*", "layers", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.wavegan.Conv1DBlock.forward": [[74, 76], ["wavegan.Conv1DBlock.layers"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "return", "self", ".", "layers", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.wavegan.WaveGANDiscriminator.__init__": [[79, 222], ["src.models.sequence.SequenceModule.__init__", "print", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "wavegan.WaveGANDiscriminator.modules", "math.ceil", "wavegan.Conv1DBlock", "wavegan.Conv1DBlock", "wavegan.Conv1DBlock", "wavegan.Conv1DBlock", "wavegan.Conv1DBlock", "math.log2", "isinstance", "isinstance", "torch.init.kaiming_normal_", "torch.init.kaiming_normal_", "torch.init.kaiming_normal_", "torch.init.kaiming_normal_"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "d_model", "=", "1", ",", "\n", "d_output", "=", "10", ",", "\n", "l_output", "=", "0", ",", "# Unused, absorbs argument from sequence", "\n", "model_size", "=", "64", ",", "\n", "n_layers", "=", "1", ",", "\n", "kernel_size", "=", "25", ",", "\n", "# alpha=0.2,", "\n", "norm", "=", "'none'", ",", "\n", "causal", "=", "True", ",", "# Currently doesn't work", "\n", "verbose", "=", "False", ",", "\n", "l_max", "=", "16384", ",", "\n", "# use_batch_norm=False,", "\n", "dropout", "=", "0.0", ",", "\n", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "assert", "kernel_size", "%", "2", "==", "1", ",", "f\"Only odd kernel sizes supported\"", "\n", "# assert l_max in [16384, 32768, 65536]  # used to predict longer utterances", "\n", "# assert l_max == 16384 # only support up to 16k sequences for now", "\n", "\n", "self", ".", "d_model", "=", "d_model", "# c", "\n", "self", ".", "d_output", "=", "d_output", "\n", "# assert l_output == 0, \"WaveGAN Discriminator should only be used on classification tasks with l_output == 0\"", "\n", "self", ".", "l_output", "=", "l_output", "\n", "\n", "self", ".", "l_max", "=", "2", "**", "math", ".", "ceil", "(", "math", ".", "log2", "(", "l_max", ")", ")", "\n", "print", "(", "self", ".", "l_max", ")", "\n", "\n", "self", ".", "model_size", "=", "model_size", "# d", "\n", "# self.use_batch_norm = use_batch_norm", "\n", "# self.alpha = alpha", "\n", "self", ".", "verbose", "=", "verbose", "\n", "\n", "conv_layers", "=", "[", "\n", "Conv1DBlock", "(", "\n", "d_model", ",", "\n", "model_size", ",", "\n", "kernel_size", ",", "\n", "stride", "=", "4", ",", "\n", "# padding=12,", "\n", "# use_batch_norm=use_batch_norm,", "\n", "causal", "=", "causal", ",", "\n", "norm", "=", "norm", ",", "\n", "n_layers", "=", "n_layers", ",", "\n", "# alpha=alpha,", "\n", "dropout", "=", "dropout", ",", "\n", ")", ",", "\n", "Conv1DBlock", "(", "\n", "model_size", ",", "\n", "2", "*", "model_size", ",", "\n", "kernel_size", ",", "\n", "stride", "=", "4", ",", "\n", "# padding=12,", "\n", "# use_batch_norm=use_batch_norm,", "\n", "causal", "=", "causal", ",", "\n", "norm", "=", "norm", ",", "\n", "n_layers", "=", "n_layers", ",", "\n", "# alpha=alpha,", "\n", "dropout", "=", "dropout", ",", "\n", ")", ",", "\n", "Conv1DBlock", "(", "\n", "2", "*", "model_size", ",", "\n", "4", "*", "model_size", ",", "\n", "kernel_size", ",", "\n", "stride", "=", "4", ",", "\n", "# padding=12,", "\n", "# use_batch_norm=use_batch_norm,", "\n", "causal", "=", "causal", ",", "\n", "norm", "=", "norm", ",", "\n", "n_layers", "=", "n_layers", ",", "\n", "# alpha=alpha,", "\n", "dropout", "=", "dropout", ",", "\n", ")", ",", "\n", "Conv1DBlock", "(", "\n", "4", "*", "model_size", ",", "\n", "8", "*", "model_size", ",", "\n", "kernel_size", ",", "\n", "stride", "=", "4", ",", "\n", "# padding=12,", "\n", "# use_batch_norm=use_batch_norm,", "\n", "causal", "=", "causal", ",", "\n", "norm", "=", "norm", ",", "\n", "n_layers", "=", "n_layers", ",", "\n", "# alpha=alpha,", "\n", "dropout", "=", "dropout", ",", "\n", ")", ",", "\n", "Conv1DBlock", "(", "\n", "8", "*", "model_size", ",", "\n", "16", "*", "model_size", ",", "\n", "kernel_size", ",", "\n", "stride", "=", "4", ",", "\n", "# padding=12,", "\n", "# use_batch_norm=use_batch_norm,", "\n", "causal", "=", "causal", ",", "\n", "norm", "=", "norm", ",", "\n", "n_layers", "=", "n_layers", ",", "\n", "# alpha=alpha,", "\n", "dropout", "=", "dropout", ",", "\n", ")", ",", "\n", "]", "\n", "self", ".", "causal", "=", "causal", "\n", "# self.fc_d_input = 256 * model_size", "\n", "if", "self", ".", "causal", ":", "\n", "            ", "self", ".", "fc_d_input", "=", "16", "*", "model_size", "\n", "", "else", ":", "\n", "            ", "self", ".", "fc_d_input", "=", "self", ".", "l_max", "//", "64", "*", "model_size", "\n", "\n", "# Logic for very long sequences from WaveGAN code", "\n", "# if l_max == 32768:", "\n", "#     conv_layers.append(", "\n", "#         Conv1D(", "\n", "#             16 * model_size,", "\n", "#             32 * model_size,", "\n", "#             kernel_size,", "\n", "#             stride=2,", "\n", "#             padding=12,", "\n", "#             use_batch_norm=use_batch_norm,", "\n", "#             alpha=alpha,", "\n", "#         )", "\n", "#     )", "\n", "#     self.fc_d_input = 480 * model_size", "\n", "# elif l_max == 65536:", "\n", "#     conv_layers.append(", "\n", "#         Conv1D(", "\n", "#             16 * model_size,", "\n", "#             32 * model_size,", "\n", "#             kernel_size,", "\n", "#             stride=4,", "\n", "#             padding=12,", "\n", "#             use_batch_norm=use_batch_norm,", "\n", "#             alpha=alpha,", "\n", "#         )", "\n", "#     )", "\n", "#     self.fc_d_input = 512 * model_size", "\n", "\n", "", "self", ".", "conv_layers", "=", "nn", ".", "ModuleList", "(", "conv_layers", ")", "\n", "\n", "self", ".", "fc1", "=", "nn", ".", "Linear", "(", "self", ".", "fc_d_input", ",", "self", ".", "d_output", ")", "\n", "\n", "for", "m", "in", "self", ".", "modules", "(", ")", ":", "\n", "            ", "if", "isinstance", "(", "m", ",", "nn", ".", "Conv1d", ")", "or", "isinstance", "(", "m", ",", "nn", ".", "Linear", ")", ":", "\n", "                ", "nn", ".", "init", ".", "kaiming_normal_", "(", "m", ".", "weight", ".", "data", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.wavegan.WaveGANDiscriminator.forward": [[223, 247], ["x.reshape.reshape.permute", "torch.pad", "torch.pad", "torch.pad", "torch.pad", "conv", "wavegan.WaveGANDiscriminator.fc1", "x.reshape.reshape.reshape", "print", "x.reshape.reshape.transpose", "print", "wavegan.WaveGANDiscriminator.fc1"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.sc.pad", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.sc.pad", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.sc.pad", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.sc.pad"], ["", "", "", "def", "forward", "(", "self", ",", "x", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"\n        x: (batch, length, channels)\n        y: (batch, 1, d_output)\n        \"\"\"", "\n", "x", "=", "x", ".", "permute", "(", "0", ",", "2", ",", "1", ")", "\n", "x", "=", "F", ".", "pad", "(", "x", ",", "(", "0", ",", "self", ".", "l_max", "-", "x", ".", "shape", "[", "-", "1", "]", ")", ")", "\n", "for", "conv", "in", "self", ".", "conv_layers", ":", "\n", "            ", "x", "=", "conv", "(", "x", ")", "\n", "if", "self", ".", "verbose", ":", "\n", "                ", "print", "(", "x", ".", "shape", ")", "\n", "", "", "if", "self", ".", "causal", ":", "\n", "            ", "x", "=", "self", ".", "fc1", "(", "x", ".", "transpose", "(", "-", "1", ",", "-", "2", ")", ")", "# (B, L, output)", "\n", "\n", "if", "self", ".", "l_output", "==", "0", ":", "\n", "                ", "return", "x", "[", ":", ",", "-", "1", ",", ":", "]", ",", "None", "\n", "", "else", ":", "\n", "                ", "return", "x", "[", ":", ",", "-", "self", ".", "l_output", ":", ",", ":", "]", ",", "None", "\n", "", "", "else", ":", "\n", "            ", "assert", "self", ".", "l_output", "==", "0", "\n", "x", "=", "x", ".", "reshape", "(", "-", "1", ",", "self", ".", "fc_d_input", ")", "\n", "if", "self", ".", "verbose", ":", "\n", "                ", "print", "(", "x", ".", "shape", ")", "\n", "", "return", "self", ".", "fc1", "(", "x", ")", ",", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.resnet.Resnet18CelebA.__init__": [[10, 21], ["torch.Module.__init__", "resnet18", "torch.Linear"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "d_output", ",", "\n", "**", "kwargs", ",", "\n", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "if", "'l_output'", "in", "kwargs", "and", "kwargs", "[", "'l_output'", "]", ">", "1", ":", "\n", "            ", "d_output", "=", "kwargs", "[", "'l_output'", "]", "\n", "\n", "", "self", ".", "resnet", "=", "resnet18", "(", "pretrained", "=", "False", ")", "\n", "self", ".", "resnet", ".", "fc", "=", "nn", ".", "Linear", "(", "512", ",", "d_output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.resnet.Resnet18CelebA.forward": [[22, 28], ["x.view.view.transpose", "x.view.view.view", "resnet.Resnet18CelebA.resnet.forward"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.models.resnext.CifarResNeXt.forward"], ["", "def", "forward", "(", "self", ",", "x", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "# BSC -> BCS", "\n", "        ", "x", "=", "x", ".", "transpose", "(", "1", ",", "2", ")", "\n", "# BCS -> BCHW", "\n", "x", "=", "x", ".", "view", "(", "x", ".", "shape", "[", "0", "]", ",", "3", ",", "178", ",", "218", ")", "\n", "return", "self", ".", "resnet", ".", "forward", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.resnet.ResnetSquare.__init__": [[31, 58], ["torch.Module.__init__", "torch.Identity"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "d_input", ",", "\n", "variant", "=", "'18'", ",", "\n", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "d_input", "=", "d_input", "\n", "self", ".", "resnet", "=", "{", "\n", "'18'", ":", "models", ".", "resnet18", ",", "\n", "'34'", ":", "models", ".", "resnet34", ",", "\n", "'50'", ":", "models", ".", "resnet50", ",", "\n", "18", ":", "models", ".", "resnet18", ",", "\n", "34", ":", "models", ".", "resnet34", ",", "\n", "50", ":", "models", ".", "resnet50", ",", "\n", "'wrn'", ":", "models", ".", "wide_resnet50_2", ",", "\n", "}", "[", "variant", "]", "(", "pretrained", "=", "False", ")", "\n", "self", ".", "resnet", ".", "fc", "=", "nn", ".", "Identity", "(", ")", "\n", "self", ".", "d_output", "=", "{", "\n", "'18'", ":", "512", ",", "\n", "'34'", ":", "512", ",", "\n", "'50'", ":", "2048", ",", "\n", "18", ":", "512", ",", "\n", "34", ":", "512", ",", "\n", "50", ":", "2048", ",", "\n", "'wrn'", ":", "2048", ",", "\n", "}", "[", "variant", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.resnet.ResnetSquare.forward": [[59, 73], ["x.repeat.repeat.transpose", "int", "x.repeat.repeat.view", "resnet.ResnetSquare.resnet.forward", "y.unsqueeze.unsqueeze.unsqueeze", "x.repeat.repeat.repeat", "x.repeat.repeat.size"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.models.resnext.CifarResNeXt.forward"], ["", "def", "forward", "(", "self", ",", "x", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "# BSC -> BCS", "\n", "        ", "x", "=", "x", ".", "transpose", "(", "1", ",", "2", ")", "\n", "# BCS -> BCHW", "\n", "n", "=", "int", "(", "x", ".", "size", "(", "-", "1", ")", "**", ".5", ")", "\n", "x", "=", "x", ".", "view", "(", "x", ".", "shape", "[", "0", "]", ",", "self", ".", "d_input", ",", "n", ",", "n", ")", "\n", "if", "self", ".", "d_input", "==", "1", ":", "\n", "            ", "x", "=", "x", ".", "repeat", "(", "1", ",", "3", ",", "1", ",", "1", ")", "\n", "", "elif", "self", ".", "d_input", "==", "3", ":", "\n", "            ", "pass", "\n", "", "else", ":", "raise", "NotImplementedError", "\n", "y", "=", "self", ".", "resnet", ".", "forward", "(", "x", ")", "\n", "y", "=", "y", ".", "unsqueeze", "(", "-", "2", ")", "# (B 1 C)", "\n", "return", "y", ",", "None", "\n", "", "", ""]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.ckconv.LayerNorm.__init__": [[55, 64], ["torch.Module.__init__", "torch.nn.GroupNorm", "torch.nn.GroupNorm", "torch.nn.GroupNorm", "torch.nn.GroupNorm", "torch.nn.GroupNorm", "torch.nn.GroupNorm", "torch.nn.GroupNorm", "torch.nn.GroupNorm", "torch.nn.GroupNorm", "torch.nn.GroupNorm", "torch.nn.GroupNorm", "torch.nn.GroupNorm", "torch.nn.GroupNorm", "torch.nn.GroupNorm", "torch.nn.GroupNorm", "torch.nn.GroupNorm", "torch.nn.GroupNorm", "torch.nn.GroupNorm", "torch.nn.GroupNorm", "torch.nn.GroupNorm", "torch.nn.GroupNorm", "torch.nn.GroupNorm", "torch.nn.GroupNorm", "torch.nn.GroupNorm", "torch.nn.GroupNorm"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "num_channels", ":", "int", ",", "\n", "eps", ":", "float", "=", "1e-12", ",", "\n", ")", ":", "\n", "        ", "\"\"\"Uses GroupNorm implementation with group=1 for speed.\"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", ")", "\n", "# we use GroupNorm to implement this efficiently and fast.", "\n", "self", ".", "layer_norm", "=", "torch", ".", "nn", ".", "GroupNorm", "(", "1", ",", "num_channels", "=", "num_channels", ",", "eps", "=", "eps", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.ckconv.LayerNorm.forward": [[65, 67], ["ckconv.LayerNorm.layer_norm"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "return", "self", ".", "layer_norm", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.ckconv.Expression.__init__": [[71, 78], ["super().__init__"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__"], ["    ", "def", "__init__", "(", "self", ",", "func", ")", ":", "\n", "        ", "\"\"\"\n        Creates a torch.nn.Module that applies the function func.\n        :param func: lambda function\n        \"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "func", "=", "func", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.ckconv.Expression.forward": [[79, 81], ["ckconv.Expression.func"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "return", "self", ".", "func", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.ckconv.KernelNet.__init__": [[179, 242], ["super().__init__", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "ckconv.KernelNet.initialize", "torch.nn.utils.weight_norm", "torch.nn.utils.weight_norm", "torch.nn.utils.weight_norm", "torch.nn.utils.weight_norm", "torch.nn.utils.weight_norm", "ActivationFunction", "torch.nn.utils.weight_norm", "torch.nn.utils.weight_norm", "torch.nn.utils.weight_norm", "torch.nn.utils.weight_norm", "torch.nn.utils.weight_norm", "ActivationFunction", "torch.nn.utils.weight_norm", "torch.nn.utils.weight_norm", "torch.nn.utils.weight_norm", "torch.nn.utils.weight_norm", "torch.nn.utils.weight_norm", "Linear", "ckconv.Multiply", "torch.nn.Identity", "torch.nn.Identity", "torch.nn.Identity", "torch.nn.Identity", "torch.nn.Identity", "torch.nn.Identity", "torch.nn.Identity", "torch.nn.Identity", "torch.nn.Identity", "torch.nn.Identity", "torch.nn.Identity", "torch.nn.Identity", "torch.nn.Identity", "torch.nn.Identity", "torch.nn.Identity", "torch.nn.Identity", "torch.nn.Identity", "torch.nn.Identity", "torch.nn.Identity", "torch.nn.Identity", "torch.nn.Identity", "torch.nn.Identity", "torch.nn.Identity", "torch.nn.Identity", "torch.nn.Identity", "Norm", "torch.nn.Identity", "torch.nn.Identity", "torch.nn.Identity", "torch.nn.Identity", "torch.nn.Identity", "torch.nn.Identity", "torch.nn.Identity", "torch.nn.Identity", "torch.nn.Identity", "torch.nn.Identity", "torch.nn.Identity", "torch.nn.Identity", "torch.nn.Identity", "torch.nn.Identity", "torch.nn.Identity", "torch.nn.Identity", "torch.nn.Identity", "torch.nn.Identity", "torch.nn.Identity", "torch.nn.Identity", "torch.nn.Identity", "torch.nn.Identity", "torch.nn.Identity", "torch.nn.Identity", "torch.nn.Identity", "Linear", "ckconv.Multiply", "torch.nn.Identity", "torch.nn.Identity", "torch.nn.Identity", "torch.nn.Identity", "torch.nn.Identity", "torch.nn.Identity", "torch.nn.Identity", "torch.nn.Identity", "torch.nn.Identity", "torch.nn.Identity", "torch.nn.Identity", "torch.nn.Identity", "torch.nn.Identity", "torch.nn.Identity", "torch.nn.Identity", "torch.nn.Identity", "torch.nn.Identity", "torch.nn.Identity", "torch.nn.Identity", "torch.nn.Identity", "torch.nn.Identity", "torch.nn.Identity", "torch.nn.Identity", "torch.nn.Identity", "torch.nn.Identity", "Norm", "torch.nn.Identity", "torch.nn.Identity", "torch.nn.Identity", "torch.nn.Identity", "torch.nn.Identity", "torch.nn.Identity", "torch.nn.Identity", "torch.nn.Identity", "torch.nn.Identity", "torch.nn.Identity", "torch.nn.Identity", "torch.nn.Identity", "torch.nn.Identity", "torch.nn.Identity", "torch.nn.Identity", "torch.nn.Identity", "torch.nn.Identity", "torch.nn.Identity", "torch.nn.Identity", "torch.nn.Identity", "torch.nn.Identity", "torch.nn.Identity", "torch.nn.Identity", "torch.nn.Identity", "torch.nn.Identity", "Linear", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Identity", "torch.nn.Identity", "torch.nn.Identity", "torch.nn.Identity", "torch.nn.Identity", "torch.nn.Identity", "torch.nn.Identity", "torch.nn.Identity", "torch.nn.Identity", "torch.nn.Identity", "torch.nn.Identity", "torch.nn.Identity", "torch.nn.Identity", "torch.nn.Identity", "torch.nn.Identity", "torch.nn.Identity", "torch.nn.Identity", "torch.nn.Identity", "torch.nn.Identity", "torch.nn.Identity", "torch.nn.Identity", "torch.nn.Identity", "torch.nn.Identity", "torch.nn.Identity", "torch.nn.Identity"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.ss.lssl.Platypus.initialize", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.ckconv.Multiply", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.ckconv.Multiply"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "in_channels", ":", "int", ",", "\n", "out_channels", ":", "int", ",", "\n", "hidden_channels", ":", "int", ",", "\n", "activation_function", ":", "str", ",", "\n", "norm_type", ":", "str", ",", "\n", "dim_linear", ":", "int", ",", "\n", "bias", ":", "bool", ",", "\n", "omega_0", ":", "float", ",", "\n", "weight_dropout", ":", "float", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Creates an 3-layer MLP, which parameterizes a convolutional kernel as:\n        relative positions -> hidden_channels -> hidden_channels -> in_channels * out_channels\n        :param in_channels:  Dimensionality of the relative positions (Default: 1).\n        :param out_channels:  input channels * output channels of the resulting convolutional kernel.\n        :param hidden_channels: Number of hidden units.\n        :param activation_function: Activation function used.\n        :param norm_type: Normalization type used.\n        :param dim_linear:  Spatial dimension of the input, e.g., for audio = 1, images = 2 (only 1 suported).\n        :param bias:  If True, adds a learnable bias to the layers.\n        :param omega_0: Value of the omega_0 value (only used in Sine networks).\n        :param weight_dropout: Dropout rate applied to the sampled convolutional kernel.\n        \"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "is_siren", "=", "activation_function", "==", "\"Sine\"", "\n", "w_dp", "=", "weight_dropout", "!=", "0.0", "\n", "\n", "Norm", "=", "{", "\n", "\"BatchNorm\"", ":", "torch", ".", "nn", ".", "BatchNorm1d", ",", "\n", "\"LayerNorm\"", ":", "LayerNorm", ",", "\n", "\"\"", ":", "torch", ".", "nn", ".", "Identity", ",", "\n", "}", "[", "norm_type", "]", "\n", "ActivationFunction", "=", "{", "\n", "\"ReLU\"", ":", "torch", ".", "nn", ".", "ReLU", ",", "\n", "\"LeakyReLU\"", ":", "torch", ".", "nn", ".", "LeakyReLU", ",", "\n", "\"Swish\"", ":", "Swish", ",", "\n", "\"Sine\"", ":", "Sine", ",", "\n", "}", "[", "activation_function", "]", "\n", "Linear", "=", "{", "1", ":", "Linear1d", ",", "2", ":", "Linear2d", "}", "[", "dim_linear", "]", "\n", "\n", "self", ".", "kernel_net", "=", "torch", ".", "nn", ".", "Sequential", "(", "\n", "weight_norm", "(", "Linear", "(", "in_channels", ",", "hidden_channels", ",", "bias", "=", "bias", ")", ")", ",", "\n", "Multiply", "(", "omega_0", ")", "if", "is_siren", "else", "torch", ".", "nn", ".", "Identity", "(", ")", ",", "\n", "Norm", "(", "hidden_channels", ")", "if", "not", "is_siren", "else", "torch", ".", "nn", ".", "Identity", "(", ")", ",", "\n", "ActivationFunction", "(", ")", ",", "\n", "weight_norm", "(", "Linear", "(", "hidden_channels", ",", "hidden_channels", ",", "bias", "=", "bias", ")", ")", ",", "\n", "Multiply", "(", "omega_0", ")", "if", "is_siren", "else", "torch", ".", "nn", ".", "Identity", "(", ")", ",", "\n", "Norm", "(", "hidden_channels", ")", "if", "not", "is_siren", "else", "torch", ".", "nn", ".", "Identity", "(", ")", ",", "\n", "ActivationFunction", "(", ")", ",", "\n", "weight_norm", "(", "Linear", "(", "hidden_channels", ",", "out_channels", ",", "bias", "=", "bias", ")", ")", ",", "\n", "torch", ".", "nn", ".", "Dropout", "(", "p", "=", "weight_dropout", ")", "if", "w_dp", "else", "torch", ".", "nn", ".", "Identity", "(", ")", ",", "\n", ")", "\n", "\n", "# initialize the kernel function", "\n", "self", ".", "initialize", "(", "\n", "mean", "=", "0.0", ",", "\n", "variance", "=", "0.01", ",", "\n", "bias_value", "=", "0.0", ",", "\n", "is_siren", "=", "(", "activation_function", "==", "\"Sine\"", ")", ",", "\n", "omega_0", "=", "omega_0", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.ckconv.KernelNet.forward": [[244, 246], ["ckconv.KernelNet.kernel_net"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "return", "self", ".", "kernel_net", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.ckconv.KernelNet.initialize": [[247, 318], ["enumerate", "enumerate", "ckconv.KernelNet.modules", "ckconv.KernelNet.modules", "isinstance", "isinstance", "isinstance", "isinstance", "isinstance", "isinstance", "m.weight.data.normal_", "m.weight.data.uniform_", "m.weight.data.uniform_", "m.bias.data.uniform_", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "numpy.sqrt", "m.weight.data.clone().squeeze", "m.weight.data.clone", "m.bias.data.clone", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "m.bias.data.fill_", "numpy.sqrt", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "m.weight.data.clone", "intermediate_response[].squeeze", "m.weight.data.clone().squeeze", "m.weight.data.clone"], "methods", ["None"], ["", "def", "initialize", "(", "self", ",", "mean", ",", "variance", ",", "bias_value", ",", "is_siren", ",", "omega_0", ")", ":", "\n", "\n", "        ", "if", "is_siren", ":", "\n", "# Initialization of SIRENs", "\n", "            ", "net_layer", "=", "1", "\n", "for", "(", "i", ",", "m", ")", "in", "enumerate", "(", "self", ".", "modules", "(", ")", ")", ":", "\n", "                ", "if", "(", "\n", "isinstance", "(", "m", ",", "torch", ".", "nn", ".", "Conv1d", ")", "\n", "or", "isinstance", "(", "m", ",", "torch", ".", "nn", ".", "Conv2d", ")", "\n", "or", "isinstance", "(", "m", ",", "torch", ".", "nn", ".", "Linear", ")", "\n", ")", ":", "\n", "                    ", "if", "net_layer", "==", "1", ":", "\n", "                        ", "m", ".", "weight", ".", "data", ".", "uniform_", "(", "\n", "-", "1", ",", "1", "\n", ")", "# Normally (-1, 1) / in_dim but we only use 1D inputs.", "\n", "# Important! Bias is not defined in original SIREN implementation!", "\n", "net_layer", "+=", "1", "\n", "", "else", ":", "\n", "                        ", "m", ".", "weight", ".", "data", ".", "uniform_", "(", "\n", "-", "np", ".", "sqrt", "(", "6.0", "/", "m", ".", "weight", ".", "shape", "[", "1", "]", ")", "/", "omega_0", ",", "\n", "# the in_size is dim 2 in the weights of Linear and Conv layers", "\n", "np", ".", "sqrt", "(", "6.0", "/", "m", ".", "weight", ".", "shape", "[", "1", "]", ")", "/", "omega_0", ",", "\n", ")", "\n", "# Important! Bias is not defined in original SIREN implementation", "\n", "", "if", "m", ".", "bias", "is", "not", "None", ":", "\n", "                        ", "m", ".", "bias", ".", "data", ".", "uniform_", "(", "-", "1.0", ",", "1.0", ")", "\n", "", "", "", "", "else", ":", "\n", "# Initialization of ReLUs", "\n", "            ", "net_layer", "=", "1", "\n", "intermediate_response", "=", "None", "\n", "for", "(", "i", ",", "m", ")", "in", "enumerate", "(", "self", ".", "modules", "(", ")", ")", ":", "\n", "                ", "if", "(", "\n", "isinstance", "(", "m", ",", "torch", ".", "nn", ".", "Conv1d", ")", "\n", "or", "isinstance", "(", "m", ",", "torch", ".", "nn", ".", "Conv2d", ")", "\n", "or", "isinstance", "(", "m", ",", "torch", ".", "nn", ".", "Linear", ")", "\n", ")", ":", "\n", "                    ", "m", ".", "weight", ".", "data", ".", "normal_", "(", "\n", "mean", ",", "\n", "variance", ",", "\n", ")", "\n", "if", "m", ".", "bias", "is", "not", "None", ":", "\n", "\n", "                        ", "if", "net_layer", "==", "1", ":", "\n", "# m.bias.data.fill_(bias_value)", "\n", "                            ", "range", "=", "torch", ".", "linspace", "(", "-", "1.0", ",", "1.0", ",", "steps", "=", "m", ".", "weight", ".", "shape", "[", "0", "]", ")", "\n", "bias", "=", "-", "range", "*", "m", ".", "weight", ".", "data", ".", "clone", "(", ")", ".", "squeeze", "(", ")", "\n", "m", ".", "bias", "=", "torch", ".", "nn", ".", "Parameter", "(", "bias", ")", "\n", "\n", "intermediate_response", "=", "[", "\n", "m", ".", "weight", ".", "data", ".", "clone", "(", ")", ",", "\n", "m", ".", "bias", ".", "data", ".", "clone", "(", ")", ",", "\n", "]", "\n", "net_layer", "+=", "1", "\n", "\n", "", "elif", "net_layer", "==", "2", ":", "\n", "                            ", "range", "=", "torch", ".", "linspace", "(", "-", "1.0", ",", "1.0", ",", "steps", "=", "m", ".", "weight", ".", "shape", "[", "0", "]", ")", "\n", "range", "=", "range", "+", "(", "range", "[", "1", "]", "-", "range", "[", "0", "]", ")", "\n", "range", "=", "(", "\n", "range", "*", "intermediate_response", "[", "0", "]", ".", "squeeze", "(", ")", "\n", "+", "intermediate_response", "[", "1", "]", "\n", ")", "\n", "\n", "bias", "=", "-", "torch", ".", "einsum", "(", "\n", "\"oi, i -> o\"", ",", "m", ".", "weight", ".", "data", ".", "clone", "(", ")", ".", "squeeze", "(", ")", ",", "range", "\n", ")", "\n", "m", ".", "bias", "=", "torch", ".", "nn", ".", "Parameter", "(", "bias", ")", "\n", "\n", "net_layer", "+=", "1", "\n", "\n", "", "else", ":", "\n", "                            ", "m", ".", "bias", ".", "data", ".", "fill_", "(", "bias_value", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.ckconv.CKConv.__init__": [[321, 371], ["super().__init__", "ckconv.KernelNet", "ckconv.CKConv.register_buffer", "ckconv.CKConv.register_buffer", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "ckconv.CKConv.bias.data.fill_", "torch.zeros().int", "torch.zeros().int", "torch.zeros().int", "torch.zeros().int", "torch.zeros().int", "torch.zeros().int", "torch.zeros().int", "torch.zeros().int", "torch.zeros().int", "torch.zeros().int", "torch.zeros().int", "torch.zeros().int", "torch.zeros().int", "torch.zeros().int", "torch.zeros().int", "torch.zeros().int", "torch.zeros().int", "torch.zeros().int", "torch.zeros().int", "torch.zeros().int", "torch.zeros().int", "torch.zeros().int", "torch.zeros().int", "torch.zeros().int", "torch.zeros().int", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "in_channels", ":", "int", ",", "\n", "out_channels", ":", "int", ",", "\n", "hidden_channels", ":", "int", ",", "\n", "activation_function", ":", "str", ",", "\n", "norm_type", ":", "str", ",", "\n", "dim_linear", ":", "int", ",", "\n", "bias", ":", "bool", ",", "\n", "omega_0", ":", "float", ",", "\n", "weight_dropout", ":", "float", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Creates a Continuous Kernel Convolution.\n        :param in_channels: Number of channels in the input signal\n        :param out_channels: Number of channels produced by the convolution\n        :param hidden_channels: Number of hidden units in the network parameterizing the ConvKernel (KernelNet).\n        :param activation_function: Activation function used in KernelNet.\n        :param norm_type: Normalization type used in KernelNet. (only for non-Sine KernelNets).\n        :param dim_linear: patial dimension of the input, e.g., for audio = 1, images = 2 (only 1 suported).\n        :param bias: If True, adds a learnable bias to the output.\n        :param omega_0: Value of the omega_0 value of the KernelNet. (only for non-Sine KernelNets).\n        :param weight_dropout: Dropout rate applied to the sampled convolutional kernels.\n        \"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "Kernel", "=", "KernelNet", "(", "\n", "dim_linear", ",", "\n", "out_channels", "*", "in_channels", ",", "\n", "hidden_channels", ",", "\n", "activation_function", ",", "\n", "norm_type", ",", "\n", "dim_linear", ",", "\n", "bias", ",", "\n", "omega_0", ",", "\n", "weight_dropout", ",", "\n", ")", "\n", "\n", "if", "bias", ":", "\n", "            ", "self", ".", "bias", "=", "torch", ".", "nn", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "out_channels", ")", ")", "\n", "self", ".", "bias", ".", "data", ".", "fill_", "(", "value", "=", "0.0", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "bias", "=", "None", "\n", "\n", "# Non-persistent values", "\n", "", "self", ".", "rel_positions", "=", "None", "\n", "self", ".", "sigma", "=", "None", "\n", "self", ".", "sr_change", "=", "1.0", "\n", "\n", "self", ".", "register_buffer", "(", "\"train_length\"", ",", "torch", ".", "zeros", "(", "1", ")", ".", "int", "(", ")", ",", "persistent", "=", "True", ")", "\n", "self", ".", "register_buffer", "(", "\"conv_kernel\"", ",", "torch", ".", "zeros", "(", "in_channels", ")", ",", "persistent", "=", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.ckconv.CKConv.forward": [[372, 414], ["ckconv.CKConv.handle_rel_positions", "ckconv.CKConv.Kernel().view", "torch.Tensor().cuda().unsqueeze().unsqueeze", "torch.Tensor().cuda().unsqueeze().unsqueeze", "torch.Tensor().cuda().unsqueeze().unsqueeze", "torch.Tensor().cuda().unsqueeze().unsqueeze", "torch.Tensor().cuda().unsqueeze().unsqueeze", "torch.Tensor().cuda().unsqueeze().unsqueeze", "torch.Tensor().cuda().unsqueeze().unsqueeze", "torch.Tensor().cuda().unsqueeze().unsqueeze", "torch.Tensor().cuda().unsqueeze().unsqueeze", "torch.Tensor().cuda().unsqueeze().unsqueeze", "torch.Tensor().cuda().unsqueeze().unsqueeze", "torch.Tensor().cuda().unsqueeze().unsqueeze", "torch.Tensor().cuda().unsqueeze().unsqueeze", "torch.Tensor().cuda().unsqueeze().unsqueeze", "torch.Tensor().cuda().unsqueeze().unsqueeze", "torch.Tensor().cuda().unsqueeze().unsqueeze", "torch.Tensor().cuda().unsqueeze().unsqueeze", "torch.Tensor().cuda().unsqueeze().unsqueeze", "torch.Tensor().cuda().unsqueeze().unsqueeze", "torch.Tensor().cuda().unsqueeze().unsqueeze", "torch.Tensor().cuda().unsqueeze().unsqueeze", "torch.Tensor().cuda().unsqueeze().unsqueeze", "torch.Tensor().cuda().unsqueeze().unsqueeze", "torch.Tensor().cuda().unsqueeze().unsqueeze", "torch.Tensor().cuda().unsqueeze().unsqueeze", "torch.conv1d().view", "torch.conv1d().view", "torch.conv1d().view", "torch.conv1d().view", "torch.conv1d().view", "torch.conv1d().view", "torch.conv1d().view", "torch.conv1d().view", "torch.conv1d().view", "torch.conv1d().view", "torch.conv1d().view", "torch.conv1d().view", "torch.conv1d().view", "torch.conv1d().view", "torch.conv1d().view", "torch.conv1d().view", "torch.conv1d().view", "torch.conv1d().view", "torch.conv1d().view", "torch.conv1d().view", "torch.conv1d().view", "torch.conv1d().view", "torch.conv1d().view", "torch.conv1d().view", "torch.conv1d().view", "ckconv.CKConv.train_length.item", "ckconv.causal_conv", "ckconv.causal_fftconv", "ckconv.CKConv.Kernel", "G", "int", "exp", "range", "torch.Tensor().cuda().unsqueeze", "torch.Tensor().cuda().unsqueeze", "torch.Tensor().cuda().unsqueeze", "torch.Tensor().cuda().unsqueeze", "torch.Tensor().cuda().unsqueeze", "torch.Tensor().cuda().unsqueeze", "torch.Tensor().cuda().unsqueeze", "torch.Tensor().cuda().unsqueeze", "torch.Tensor().cuda().unsqueeze", "torch.Tensor().cuda().unsqueeze", "torch.Tensor().cuda().unsqueeze", "torch.Tensor().cuda().unsqueeze", "torch.Tensor().cuda().unsqueeze", "torch.Tensor().cuda().unsqueeze", "torch.Tensor().cuda().unsqueeze", "torch.Tensor().cuda().unsqueeze", "torch.Tensor().cuda().unsqueeze", "torch.Tensor().cuda().unsqueeze", "torch.Tensor().cuda().unsqueeze", "torch.Tensor().cuda().unsqueeze", "torch.Tensor().cuda().unsqueeze", "torch.Tensor().cuda().unsqueeze", "torch.Tensor().cuda().unsqueeze", "torch.Tensor().cuda().unsqueeze", "torch.Tensor().cuda().unsqueeze", "torch.conv1d", "torch.conv1d", "torch.conv1d", "torch.conv1d", "torch.conv1d", "torch.conv1d", "torch.conv1d", "torch.conv1d", "torch.conv1d", "torch.conv1d", "torch.conv1d", "torch.conv1d", "torch.conv1d", "torch.conv1d", "torch.conv1d", "torch.conv1d", "torch.conv1d", "torch.conv1d", "torch.conv1d", "torch.conv1d", "torch.conv1d", "torch.conv1d", "torch.conv1d", "torch.conv1d", "torch.conv1d", "ckconv.CKConv.view", "sqrt", "torch.Tensor().cuda", "torch.Tensor().cuda", "torch.Tensor().cuda", "torch.Tensor().cuda", "torch.Tensor().cuda", "torch.Tensor().cuda", "torch.Tensor().cuda", "torch.Tensor().cuda", "torch.Tensor().cuda", "torch.Tensor().cuda", "torch.Tensor().cuda", "torch.Tensor().cuda", "torch.Tensor().cuda", "torch.Tensor().cuda", "torch.Tensor().cuda", "torch.Tensor().cuda", "torch.Tensor().cuda", "torch.Tensor().cuda", "torch.Tensor().cuda", "torch.Tensor().cuda", "torch.Tensor().cuda", "torch.Tensor().cuda", "torch.Tensor().cuda", "torch.Tensor().cuda", "torch.Tensor().cuda", "float", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.ckconv.CKConv.handle_rel_positions", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.ckconv.causal_conv", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.ckconv.causal_fftconv"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "# Construct kernel", "\n", "        ", "x_shape", "=", "x", ".", "shape", "\n", "\n", "rel_pos", "=", "self", ".", "handle_rel_positions", "(", "x", ")", "\n", "conv_kernel", "=", "self", ".", "Kernel", "(", "rel_pos", ")", ".", "view", "(", "-", "1", ",", "x_shape", "[", "1", "]", ",", "*", "x_shape", "[", "2", ":", "]", ")", "\n", "\n", "# ---- Different samling rate --------", "\n", "# If freq test > freq test, smooth out high-freq elements.", "\n", "if", "self", ".", "sigma", "is", "not", "None", ":", "\n", "            ", "from", "math", "import", "pi", ",", "sqrt", ",", "exp", "\n", "\n", "n", "=", "int", "(", "1", "/", "self", ".", "sr_change", ")", "*", "2", "+", "1", "\n", "h", "=", "n", "//", "2", "\n", "G", "=", "(", "\n", "lambda", "x", ":", "1", "\n", "/", "(", "self", ".", "sigma", "*", "sqrt", "(", "2", "*", "pi", ")", ")", "\n", "*", "exp", "(", "-", "float", "(", "x", ")", "**", "2", "/", "(", "2", "*", "self", ".", "sigma", "**", "2", ")", ")", "\n", ")", "\n", "\n", "smoothing_ker", "=", "[", "G", "(", "x", ")", "for", "x", "in", "range", "(", "-", "h", ",", "h", "+", "1", ")", "]", "\n", "smoothing_ker", "=", "torch", ".", "Tensor", "(", "smoothing_ker", ")", ".", "cuda", "(", ")", ".", "unsqueeze", "(", "0", ")", ".", "unsqueeze", "(", "0", ")", "\n", "conv_kernel", "[", ":", ",", ":", ",", "h", ":", "-", "h", "]", "=", "torch", ".", "conv1d", "(", "\n", "conv_kernel", ".", "view", "(", "-", "1", ",", "1", ",", "*", "x_shape", "[", "2", ":", "]", ")", ",", "smoothing_ker", ",", "padding", "=", "0", "\n", ")", ".", "view", "(", "*", "conv_kernel", ".", "shape", "[", ":", "-", "1", "]", ",", "-", "1", ")", "\n", "# multiply by the sr_train / sr_test", "\n", "", "if", "self", ".", "sr_change", "!=", "1.0", ":", "\n", "            ", "conv_kernel", "*=", "self", ".", "sr_change", "\n", "# ------------------------------------", "\n", "\n", "# For computation of \"weight_decay\"", "\n", "", "self", ".", "conv_kernel", "=", "conv_kernel", "\n", "\n", "# We have noticed that the results of fftconv become very noisy when the length of", "\n", "# the input is very small ( < 50 samples). As this might occur when we use subsampling,", "\n", "# we replace causal_fftconv by causal_conv in settings where this occurs.", "\n", "if", "x_shape", "[", "-", "1", "]", "<", "self", ".", "train_length", ".", "item", "(", ")", ":", "\n", "# Use spatial convolution:", "\n", "            ", "return", "causal_conv", "(", "x", ",", "conv_kernel", ",", "self", ".", "bias", ")", "\n", "", "else", ":", "\n", "# Otherwise use fft convolution:", "\n", "            ", "return", "causal_fftconv", "(", "x", ",", "conv_kernel", ",", "self", ".", "bias", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.ckconv.CKConv.handle_rel_positions": [[415, 449], ["ckconv.CKConv.calculate_max", "torch.linspace().cuda().unsqueeze().unsqueeze", "torch.linspace().cuda().unsqueeze().unsqueeze", "torch.linspace().cuda().unsqueeze().unsqueeze", "torch.linspace().cuda().unsqueeze().unsqueeze", "torch.linspace().cuda().unsqueeze().unsqueeze", "torch.linspace().cuda().unsqueeze().unsqueeze", "torch.linspace().cuda().unsqueeze().unsqueeze", "torch.linspace().cuda().unsqueeze().unsqueeze", "torch.linspace().cuda().unsqueeze().unsqueeze", "torch.linspace().cuda().unsqueeze().unsqueeze", "torch.linspace().cuda().unsqueeze().unsqueeze", "torch.linspace().cuda().unsqueeze().unsqueeze", "torch.linspace().cuda().unsqueeze().unsqueeze", "torch.linspace().cuda().unsqueeze().unsqueeze", "torch.linspace().cuda().unsqueeze().unsqueeze", "torch.linspace().cuda().unsqueeze().unsqueeze", "torch.linspace().cuda().unsqueeze().unsqueeze", "torch.linspace().cuda().unsqueeze().unsqueeze", "torch.linspace().cuda().unsqueeze().unsqueeze", "torch.linspace().cuda().unsqueeze().unsqueeze", "torch.linspace().cuda().unsqueeze().unsqueeze", "torch.linspace().cuda().unsqueeze().unsqueeze", "torch.linspace().cuda().unsqueeze().unsqueeze", "torch.linspace().cuda().unsqueeze().unsqueeze", "torch.linspace().cuda().unsqueeze().unsqueeze", "ckconv.CKConv.train_length.item", "ckconv.CKConv.train_length.item", "round", "torch.linspace().cuda().unsqueeze", "torch.linspace().cuda().unsqueeze", "torch.linspace().cuda().unsqueeze", "torch.linspace().cuda().unsqueeze", "torch.linspace().cuda().unsqueeze", "torch.linspace().cuda().unsqueeze", "torch.linspace().cuda().unsqueeze", "torch.linspace().cuda().unsqueeze", "torch.linspace().cuda().unsqueeze", "torch.linspace().cuda().unsqueeze", "torch.linspace().cuda().unsqueeze", "torch.linspace().cuda().unsqueeze", "torch.linspace().cuda().unsqueeze", "torch.linspace().cuda().unsqueeze", "torch.linspace().cuda().unsqueeze", "torch.linspace().cuda().unsqueeze", "torch.linspace().cuda().unsqueeze", "torch.linspace().cuda().unsqueeze", "torch.linspace().cuda().unsqueeze", "torch.linspace().cuda().unsqueeze", "torch.linspace().cuda().unsqueeze", "torch.linspace().cuda().unsqueeze", "torch.linspace().cuda().unsqueeze", "torch.linspace().cuda().unsqueeze", "torch.linspace().cuda().unsqueeze", "round", "ckconv.CKConv.train_length.item", "torch.linspace().cuda", "torch.linspace().cuda", "torch.linspace().cuda", "torch.linspace().cuda", "torch.linspace().cuda", "torch.linspace().cuda", "torch.linspace().cuda", "torch.linspace().cuda", "torch.linspace().cuda", "torch.linspace().cuda", "torch.linspace().cuda", "torch.linspace().cuda", "torch.linspace().cuda", "torch.linspace().cuda", "torch.linspace().cuda", "torch.linspace().cuda", "torch.linspace().cuda", "torch.linspace().cuda", "torch.linspace().cuda", "torch.linspace().cuda", "torch.linspace().cuda", "torch.linspace().cuda", "torch.linspace().cuda", "torch.linspace().cuda", "torch.linspace().cuda", "ckconv.CKConv.train_length.item", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.ckconv.CKConv.calculate_max"], ["", "", "def", "handle_rel_positions", "(", "self", ",", "x", ")", ":", "\n", "        ", "\"\"\"\n        Handles the vector or relative positions which is given to KernelNet.\n        \"\"\"", "\n", "if", "self", ".", "rel_positions", "is", "None", ":", "\n", "            ", "if", "self", ".", "train_length", "[", "0", "]", "==", "0", ":", "\n", "# The ckconv has not been trained yet. Set maximum length to be 1.", "\n", "                ", "self", ".", "train_length", "[", "0", "]", "=", "x", ".", "shape", "[", "-", "1", "]", "\n", "\n", "# Calculate the maximum relative position based on the length of the train set,", "\n", "# and the current length of the input.", "\n", "", "max_relative_pos", "=", "self", ".", "calculate_max", "(", "\n", "self", ".", "train_length", ".", "item", "(", ")", ",", "current_length", "=", "x", ".", "shape", "[", "-", "1", "]", "\n", ")", "\n", "\n", "# Creates the vector of relative positions.", "\n", "self", ".", "rel_positions", "=", "(", "\n", "torch", ".", "linspace", "(", "-", "1.0", ",", "max_relative_pos", ",", "x", ".", "shape", "[", "-", "1", "]", ")", "\n", ".", "cuda", "(", ")", "\n", ".", "unsqueeze", "(", "0", ")", "\n", ".", "unsqueeze", "(", "0", ")", "\n", ")", "# -> With form: [batch_size=1, in_channels=1, x_dimension]", "\n", "\n", "# calculate and save the sr ratio for later", "\n", "if", "self", ".", "train_length", ".", "item", "(", ")", ">", "x", ".", "shape", "[", "-", "1", "]", ":", "\n", "                ", "self", ".", "sr_change", "=", "round", "(", "self", ".", "train_length", ".", "item", "(", ")", "/", "x", ".", "shape", "[", "-", "1", "]", ")", "\n", "", "else", ":", "\n", "                ", "self", ".", "sr_change", "=", "1", "/", "round", "(", "x", ".", "shape", "[", "-", "1", "]", "/", "self", ".", "train_length", ".", "item", "(", ")", ")", "\n", "\n", "# if new signal has higher frequency", "\n", "", "if", "self", ".", "sr_change", "<", "1", ":", "\n", "                ", "self", ".", "sigma", "=", "0.5", "\n", "\n", "", "", "return", "self", ".", "rel_positions", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.ckconv.CKConv.calculate_max": [[450, 481], ["round", "round"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "calculate_max", "(", "\n", "train_length", ":", "int", ",", "\n", "current_length", ":", "int", ",", "\n", ")", "->", "float", ":", "\n", "        ", "\"\"\"\n        Calculates the maximum relative position for the current length based on the input length.\n        This is used to avoid kernel misalignment (see Appx. D.2).\n        :param train_length: Input length during training.\n        :param current_length: Current input length.\n        :return: Returns the max relative position for the calculation of the relative\n                 positions vector. The max. of train is always equal to 1.\n        \"\"\"", "\n", "# get sampling rate ratio", "\n", "if", "train_length", ">", "current_length", ":", "\n", "            ", "sr_change", "=", "round", "(", "train_length", "/", "current_length", ")", "\n", "", "else", ":", "\n", "            ", "sr_change", "=", "1", "/", "round", "(", "current_length", "/", "train_length", ")", "\n", "\n", "# get step sizes (The third parameter of torch.linspace).", "\n", "", "train_step", "=", "2.0", "/", "(", "train_length", "-", "1", ")", "\n", "current_step", "=", "train_step", "*", "sr_change", "\n", "\n", "# Calculate the maximum relative position.", "\n", "if", "sr_change", ">", "1", ":", "\n", "            ", "substract", "=", "(", "train_length", "-", "1", ")", "%", "sr_change", "\n", "max_relative_pos", "=", "1", "-", "substract", "*", "train_step", "\n", "", "else", ":", "\n", "            ", "add", "=", "(", "current_length", "-", "1", ")", "%", "(", "1", "/", "sr_change", ")", "\n", "max_relative_pos", "=", "1", "+", "add", "*", "current_step", "\n", "", "return", "max_relative_pos", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.ckconv.CKBlock.__init__": [[484, 563], ["super().__init__", "ckconv.CKConv", "ckconv.CKConv", "ckconv.LayerNorm", "ckconv.LayerNorm", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "shortcut.append", "ckconv.Linear1d"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.ckconv.Linear1d"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "in_channels", ":", "int", ",", "\n", "out_channels", ":", "int", ",", "\n", "kernelnet_hidden_channels", ":", "int", ",", "\n", "kernelnet_activation_function", ":", "str", ",", "\n", "kernelnet_norm_type", ":", "str", ",", "\n", "dim_linear", ":", "int", ",", "\n", "bias", ":", "bool", ",", "\n", "omega_0", ":", "bool", ",", "\n", "dropout", ":", "float", ",", "\n", "weight_dropout", ":", "float", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Creates a Residual Block with CKConvs as:\n        ( Follows the Residual Block of Bai et. al., 2017 )\n        input\n         | ---------------|\n         CKConv           |\n         LayerNorm        |\n         ReLU             |\n         DropOut          |\n         |                |\n         CKConv           |\n         LayerNorm        |\n         ReLU             |\n         DropOut          |\n         + <--------------|\n         |\n         ReLU\n         |\n         output\n        :param in_channels:  Number of channels in the input signal\n        :param out_channels:  Number of output (and hidden) channels of the block.\n        :param kernelnet_hidden_channels: Number of hidden units in the KernelNets of the CKConvs.\n        :param kernelnet_activation_function: Activation function used in the KernelNets of the CKConvs.\n        :param kernelnet_norm_type: Normalization type used in the KernelNets of the CKConvs (only for non-Sine KernelNets).\n        :param dim_linear:  Spatial dimension of the input, e.g., for audio = 1, images = 2 (only 1 suported).\n        :param bias:  If True, adds a learnable bias to the output.\n        :param omega_0: Value of the omega_0 value of the KernelNets. (only for non-Sine KernelNets).\n        :param dropout: Dropout rate of the block\n        :param weight_dropout: Dropout rate applied to the sampled convolutional kernels.\n        \"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "# CKConv layers", "\n", "self", ".", "cconv1", "=", "CKConv", "(", "\n", "in_channels", ",", "\n", "out_channels", ",", "\n", "kernelnet_hidden_channels", ",", "\n", "kernelnet_activation_function", ",", "\n", "kernelnet_norm_type", ",", "\n", "dim_linear", ",", "\n", "bias", ",", "\n", "omega_0", ",", "\n", "weight_dropout", ",", "\n", ")", "\n", "self", ".", "cconv2", "=", "CKConv", "(", "\n", "out_channels", ",", "\n", "out_channels", ",", "\n", "kernelnet_hidden_channels", ",", "\n", "kernelnet_activation_function", ",", "\n", "kernelnet_norm_type", ",", "\n", "dim_linear", ",", "\n", "bias", ",", "\n", "omega_0", ",", "\n", "weight_dropout", ",", "\n", ")", "\n", "# Norm layers", "\n", "self", ".", "norm1", "=", "LayerNorm", "(", "out_channels", ")", "\n", "self", ".", "norm2", "=", "LayerNorm", "(", "out_channels", ")", "\n", "\n", "# Dropout", "\n", "self", ".", "dp", "=", "torch", ".", "nn", ".", "Dropout", "(", "dropout", ")", "\n", "\n", "shortcut", "=", "[", "]", "\n", "if", "in_channels", "!=", "out_channels", ":", "\n", "            ", "shortcut", ".", "append", "(", "Linear1d", "(", "in_channels", ",", "out_channels", ")", ")", "\n", "", "self", ".", "shortcut", "=", "torch", ".", "nn", ".", "Sequential", "(", "*", "shortcut", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.ckconv.CKBlock.forward": [[564, 569], ["ckconv.CKBlock.shortcut", "ckconv.CKBlock.dp", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "ckconv.CKBlock.norm1", "ckconv.CKBlock.dp", "ckconv.CKBlock.cconv1", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "ckconv.CKBlock.norm2", "ckconv.CKBlock.cconv2"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "shortcut", "=", "self", ".", "shortcut", "(", "x", ")", "\n", "out", "=", "self", ".", "dp", "(", "torch", ".", "relu", "(", "self", ".", "norm1", "(", "self", ".", "cconv1", "(", "x", ")", ")", ")", ")", "\n", "out", "=", "torch", ".", "relu", "(", "self", ".", "dp", "(", "torch", ".", "relu", "(", "self", ".", "norm2", "(", "self", ".", "cconv2", "(", "out", ")", ")", ")", ")", "+", "shortcut", ")", "\n", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.ckconv.CKCNN.__init__": [[572, 608], ["super().__init__", "range", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "blocks.append", "ckconv.CKBlock", "blocks.append", "torch.nn.MaxPool1d", "torch.nn.MaxPool1d", "torch.nn.MaxPool1d", "torch.nn.MaxPool1d", "torch.nn.MaxPool1d", "torch.nn.MaxPool1d", "torch.nn.MaxPool1d", "torch.nn.MaxPool1d", "torch.nn.MaxPool1d", "torch.nn.MaxPool1d", "torch.nn.MaxPool1d", "torch.nn.MaxPool1d", "torch.nn.MaxPool1d", "torch.nn.MaxPool1d", "torch.nn.MaxPool1d", "torch.nn.MaxPool1d", "torch.nn.MaxPool1d", "torch.nn.MaxPool1d", "torch.nn.MaxPool1d", "torch.nn.MaxPool1d", "torch.nn.MaxPool1d", "torch.nn.MaxPool1d", "torch.nn.MaxPool1d", "torch.nn.MaxPool1d", "torch.nn.MaxPool1d"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "hidden_channels", ":", "int", ",", "\n", "num_blocks", ":", "int", ",", "# 2", "\n", "kernelnet_hidden_channels", ":", "int", ",", "\n", "kernelnet_activation_function", ":", "str", ",", "\n", "kernelnet_norm_type", ":", "str", ",", "\n", "dim_linear", ":", "int", ",", "\n", "bias", ":", "bool", ",", "\n", "omega_0", ":", "bool", ",", "# sensitive to this param: good values <= 70", "\n", "dropout", ":", "float", ",", "\n", "weight_dropout", ":", "float", ",", "\n", "pool", ":", "bool", ",", "# Always False in our experiments.", "\n", ")", ":", "\n", "        ", "super", "(", "CKCNN", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "blocks", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "num_blocks", ")", ":", "\n", "            ", "blocks", ".", "append", "(", "\n", "CKBlock", "(", "\n", "# block_in_channels,", "\n", "hidden_channels", ",", "\n", "hidden_channels", ",", "\n", "kernelnet_hidden_channels", ",", "\n", "kernelnet_activation_function", ",", "\n", "kernelnet_norm_type", ",", "\n", "dim_linear", ",", "\n", "bias", ",", "\n", "omega_0", ",", "\n", "dropout", ",", "\n", "weight_dropout", ",", "\n", ")", "\n", ")", "\n", "if", "pool", ":", "\n", "                ", "blocks", ".", "append", "(", "torch", ".", "nn", ".", "MaxPool1d", "(", "kernel_size", "=", "2", ")", ")", "\n", "", "", "self", ".", "backbone", "=", "torch", ".", "nn", ".", "Sequential", "(", "*", "blocks", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.ckconv.CKCNN.forward": [[609, 615], ["x.transpose.transpose.transpose", "ckconv.CKCNN.backbone", "x.transpose.transpose.transpose"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "# Change from (B, L, H) -> (B, H, L)", "\n", "        ", "x", "=", "x", ".", "transpose", "(", "1", ",", "2", ")", "\n", "x", "=", "self", ".", "backbone", "(", "x", ")", "\n", "x", "=", "x", ".", "transpose", "(", "1", ",", "2", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.ckconv.CopyMemory_CKCNN.__init__": [[618, 655], ["ckconv.CKCNN.__init__", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "ckconv.CopyMemory_CKCNN.finallyr.weight.data.normal_", "ckconv.CopyMemory_CKCNN.finallyr.bias.data.fill_"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "in_channels", ":", "int", ",", "\n", "hidden_channels", ":", "int", ",", "\n", "num_blocks", ":", "int", ",", "\n", "kernelnet_hidden_channels", ":", "int", ",", "\n", "kernelnet_activation_function", ":", "str", ",", "\n", "kernelnet_norm_type", ":", "str", ",", "\n", "dim_linear", ":", "int", ",", "\n", "bias", ":", "bool", ",", "\n", "omega_0", ":", "bool", ",", "\n", "dropout", ":", "float", ",", "\n", "weight_dropout", ":", "float", ",", "\n", "pool", ":", "bool", ",", "\n", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "\n", "in_channels", ",", "\n", "hidden_channels", ",", "\n", "num_blocks", ",", "\n", "kernelnet_hidden_channels", ",", "\n", "kernelnet_activation_function", ",", "\n", "kernelnet_norm_type", ",", "\n", "dim_linear", ",", "\n", "bias", ",", "\n", "omega_0", ",", "\n", "dropout", ",", "\n", "weight_dropout", ",", "\n", "pool", ",", "\n", ")", "\n", "\n", "self", ".", "finallyr", "=", "torch", ".", "nn", ".", "Linear", "(", "in_features", "=", "hidden_channels", ",", "out_features", "=", "10", ")", "\n", "# Initialize finallyr", "\n", "self", ".", "finallyr", ".", "weight", ".", "data", ".", "normal_", "(", "\n", "mean", "=", "0.0", ",", "\n", "std", "=", "0.01", ",", "\n", ")", "\n", "self", ".", "finallyr", ".", "bias", ".", "data", ".", "fill_", "(", "value", "=", "0.0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.ckconv.CopyMemory_CKCNN.forward": [[656, 662], ["x.transpose.transpose.transpose", "ckconv.CopyMemory_CKCNN.backbone", "ckconv.CopyMemory_CKCNN.finallyr", "ckconv.CopyMemory_CKCNN.transpose"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "# Change from (B, S, C) -> (B, C, S)", "\n", "        ", "x", "=", "x", ".", "transpose", "(", "1", ",", "2", ")", "\n", "out", "=", "self", ".", "backbone", "(", "x", ")", "\n", "out", "=", "self", ".", "finallyr", "(", "out", ".", "transpose", "(", "1", ",", "2", ")", ")", "\n", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.ckconv.AddProblem_CKCNN.__init__": [[665, 702], ["ckconv.CKCNN.__init__", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "ckconv.AddProblem_CKCNN.finallyr.weight.data.normal_", "ckconv.AddProblem_CKCNN.finallyr.bias.data.fill_"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "in_channels", ":", "int", ",", "\n", "hidden_channels", ":", "int", ",", "\n", "num_blocks", ":", "int", ",", "\n", "kernelnet_hidden_channels", ":", "int", ",", "\n", "kernelnet_activation_function", ":", "str", ",", "\n", "kernelnet_norm_type", ":", "str", ",", "\n", "dim_linear", ":", "int", ",", "\n", "bias", ":", "bool", ",", "\n", "omega_0", ":", "bool", ",", "\n", "dropout", ":", "float", ",", "\n", "weight_dropout", ":", "float", ",", "\n", "pool", ":", "bool", ",", "\n", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "\n", "in_channels", ",", "\n", "hidden_channels", ",", "\n", "num_blocks", ",", "\n", "kernelnet_hidden_channels", ",", "\n", "kernelnet_activation_function", ",", "\n", "kernelnet_norm_type", ",", "\n", "dim_linear", ",", "\n", "bias", ",", "\n", "omega_0", ",", "\n", "dropout", ",", "\n", "weight_dropout", ",", "\n", "pool", ",", "\n", ")", "\n", "\n", "self", ".", "finallyr", "=", "torch", ".", "nn", ".", "Linear", "(", "in_features", "=", "hidden_channels", ",", "out_features", "=", "1", ")", "\n", "# Initialize finallyr", "\n", "self", ".", "finallyr", ".", "weight", ".", "data", ".", "normal_", "(", "\n", "mean", "=", "0.0", ",", "\n", "std", "=", "0.01", ",", "\n", ")", "\n", "self", ".", "finallyr", ".", "bias", ".", "data", ".", "fill_", "(", "value", "=", "0.0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.ckconv.AddProblem_CKCNN.forward": [[703, 709], ["x.transpose.transpose.transpose", "ckconv.AddProblem_CKCNN.backbone", "ckconv.AddProblem_CKCNN.finallyr"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "# Change from (B, S, C) -> (B, C, S)", "\n", "        ", "x", "=", "x", ".", "transpose", "(", "1", ",", "2", ")", "\n", "out", "=", "self", ".", "backbone", "(", "x", ")", "\n", "out", "=", "self", ".", "finallyr", "(", "out", "[", ":", ",", ":", ",", "-", "1", "]", ")", "\n", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.ckconv.ClassificationCKCNN.__init__": [[712, 748], ["ckconv.CKCNN.__init__", "ckconv.LnLoss"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "# d_input: int,", "\n", "# d_output: int,", "\n", "d_model", ":", "int", ",", "\n", "num_blocks", ":", "int", ",", "\n", "kernelnet_hidden_channels", ":", "int", ",", "\n", "kernelnet_activation_function", ":", "str", ",", "\n", "kernelnet_norm_type", ":", "str", ",", "\n", "dim_linear", ":", "int", ",", "\n", "bias", ":", "bool", ",", "\n", "omega_0", ":", "bool", ",", "\n", "dropout", ":", "float", ",", "\n", "weight_dropout", ":", "float", ",", "\n", "pool", ":", "bool", ",", "\n", "wd", ":", "float", ",", "\n", "# **kwargs,", "\n", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "\n", "# d_input,", "\n", "d_model", ",", "\n", "num_blocks", ",", "\n", "kernelnet_hidden_channels", ",", "\n", "kernelnet_activation_function", ",", "\n", "kernelnet_norm_type", ",", "\n", "dim_linear", ",", "\n", "bias", ",", "\n", "omega_0", ",", "\n", "dropout", ",", "\n", "weight_dropout", ",", "\n", "pool", ",", "\n", ")", "\n", "self", ".", "d_model", "=", "d_model", "\n", "self", ".", "d_output", "=", "d_model", "\n", "\n", "self", ".", "wd", "=", "LnLoss", "(", "wd", ",", "2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.ckconv.ClassificationCKCNN.forward": [[749, 755], ["x.transpose.transpose.transpose", "ckconv.ClassificationCKCNN.backbone", "x.transpose.transpose.transpose"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "# Change from (B, S, C) -> (B, C, S)", "\n", "        ", "x", "=", "x", ".", "transpose", "(", "1", ",", "2", ")", "\n", "x", "=", "self", ".", "backbone", "(", "x", ")", "\n", "x", "=", "x", ".", "transpose", "(", "1", ",", "2", ")", "\n", "return", "x", ",", "None", "# Have to return a state", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.ckconv.ClassificationCKCNN.loss": [[756, 758], ["ckconv.ClassificationCKCNN.wd.forward"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.models.resnext.CifarResNeXt.forward"], ["", "def", "loss", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "wd", ".", "forward", "(", "model", "=", "self", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.ckconv.LnLoss.__init__": [[760, 773], ["super().__init__"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "weight_loss", ":", "float", ",", "\n", "norm_type", ":", "int", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Computes the Ln loss on the CKConv kernels in a CKCNN.\n        :param weight_loss: Specifies the weight with which the loss will be summed to the total loss.\n        :param norm_type: Type of norm, e.g., 1 = L1 loss, 2 = L2 loss, ...\n        \"\"\"", "\n", "super", "(", "LnLoss", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "weight_loss", "=", "weight_loss", "\n", "self", ".", "norm_type", "=", "norm_type", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.ckconv.LnLoss.forward": [[774, 788], ["model.modules", "isinstance", "m.conv_kernel.norm", "m.bias.norm"], "methods", ["None"], ["", "def", "forward", "(", "\n", "self", ",", "\n", "model", ":", "CKConv", ",", "\n", ")", ":", "\n", "        ", "loss", "=", "0.0", "\n", "# Go through modules that are instances of CKConvs and gather the sampled filters", "\n", "for", "m", "in", "model", ".", "modules", "(", ")", ":", "\n", "            ", "if", "not", "isinstance", "(", "m", ",", "CKConv", ")", ":", "\n", "                ", "continue", "\n", "", "loss", "=", "loss", "+", "m", ".", "conv_kernel", ".", "norm", "(", "self", ".", "norm_type", ")", "\n", "loss", "=", "loss", "+", "m", ".", "bias", ".", "norm", "(", "self", ".", "norm_type", ")", "\n", "\n", "", "loss", "=", "self", ".", "weight_loss", "*", "loss", "\n", "return", "loss", "\n", "", "", ""]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.ckconv.Linear1d": [[16, 26], ["torch.Conv1d"], "function", ["None"], ["def", "Linear1d", "(", "\n", "in_channels", ":", "int", ",", "\n", "out_channels", ":", "int", ",", "\n", "stride", ":", "int", "=", "1", ",", "\n", "bias", ":", "bool", "=", "True", ",", "\n", ")", "->", "torch", ".", "nn", ".", "Module", ":", "\n", "    ", "\"\"\"\n    Implements a Linear Layer in terms of a point-wise convolution.\n    \"\"\"", "\n", "return", "nn", ".", "Conv1d", "(", "in_channels", ",", "out_channels", ",", "kernel_size", "=", "1", ",", "stride", "=", "stride", ",", "bias", "=", "bias", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.ckconv.Linear2d": [[28, 38], ["torch.Conv2d"], "function", ["None"], ["", "def", "Linear2d", "(", "\n", "in_channels", ":", "int", ",", "\n", "out_channels", ":", "int", ",", "\n", "stride", ":", "int", "=", "1", ",", "\n", "bias", ":", "bool", "=", "True", ",", "\n", ")", "->", "torch", ".", "nn", ".", "Module", ":", "\n", "    ", "\"\"\"\n    Implements a Linear Layer in terms of a point-wise convolution.\n    \"\"\"", "\n", "return", "nn", ".", "Conv2d", "(", "in_channels", ",", "out_channels", ",", "kernel_size", "=", "1", ",", "stride", "=", "stride", ",", "bias", "=", "bias", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.ckconv.Swish": [[40, 45], ["ckconv.Expression", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid"], "function", ["None"], ["", "def", "Swish", "(", ")", ":", "\n", "    ", "\"\"\"\n    out = x * sigmoid(x)\n    \"\"\"", "\n", "return", "Expression", "(", "lambda", "x", ":", "x", "*", "torch", ".", "sigmoid", "(", "x", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.ckconv.Sine": [[47, 52], ["ckconv.Expression", "torch.sin", "torch.sin", "torch.sin", "torch.sin", "torch.sin"], "function", ["None"], ["", "def", "Sine", "(", ")", ":", "\n", "    ", "\"\"\"\n    out = sin(x)\n    \"\"\"", "\n", "return", "Expression", "(", "lambda", "x", ":", "torch", ".", "sin", "(", "x", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.ckconv.Multiply": [[83, 90], ["ckconv.Expression"], "function", ["None"], ["", "", "def", "Multiply", "(", "\n", "omega_0", ":", "float", ",", "\n", ")", ":", "\n", "    ", "\"\"\"\n    out = omega_0 * x\n    \"\"\"", "\n", "return", "Expression", "(", "lambda", "x", ":", "omega_0", "*", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.ckconv.causal_padding": [[92, 105], ["torch.pad", "torch.pad"], "function", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.sc.pad", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.sc.pad"], ["", "def", "causal_padding", "(", "\n", "x", ":", "torch", ".", "Tensor", ",", "\n", "kernel", ":", "torch", ".", "Tensor", ",", "\n", ")", "->", "Tuple", "[", "torch", ".", "Tensor", ",", "torch", ".", "Tensor", "]", ":", "\n", "# 1. Pad the input signal & kernel tensors.", "\n", "# Check if sizes are odd. If not, add a pad of zero to make them odd.", "\n", "    ", "if", "kernel", ".", "shape", "[", "-", "1", "]", "%", "2", "==", "0", ":", "\n", "        ", "kernel", "=", "f", ".", "pad", "(", "kernel", ",", "[", "1", ",", "0", "]", ",", "value", "=", "0.0", ")", "\n", "# x = torch.nn.functional.pad(x, [1, 0], value=0.0)", "\n", "# 2. Perform padding on the input so that output equals input in length", "\n", "", "x", "=", "f", ".", "pad", "(", "x", ",", "[", "kernel", ".", "shape", "[", "-", "1", "]", "-", "1", ",", "0", "]", ",", "value", "=", "0.0", ")", "\n", "\n", "return", "x", ",", "kernel", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.ckconv.causal_conv": [[107, 124], ["ckconv.causal_padding", "torch.nn.functional.conv1d", "torch.nn.functional.conv1d", "torch.nn.functional.conv1d", "torch.nn.functional.conv1d", "torch.nn.functional.conv1d"], "function", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.ckconv.causal_padding"], ["", "def", "causal_conv", "(", "\n", "x", ":", "torch", ".", "Tensor", ",", "\n", "kernel", ":", "torch", ".", "Tensor", ",", "\n", "bias", ":", "Optional", "[", "torch", ".", "Tensor", "]", "=", "None", ",", "\n", ")", "->", "torch", ".", "Tensor", ":", "\n", "    ", "\"\"\"\n    Args:\n        x: (Tensor) Input tensor to be convolved with the kernel.\n        kernel: (Tensor) Convolution kernel.\n        bias: (Optional, Tensor) Bias tensor to add to the output.\n        padding: (int) Number of zero samples to pad the input on the last dimension.\n    Returns:\n        (Tensor) Convolved tensor\n    \"\"\"", "\n", "\n", "x", ",", "kernel", "=", "causal_padding", "(", "x", ",", "kernel", ")", "\n", "return", "torch", ".", "nn", ".", "functional", ".", "conv1d", "(", "x", ",", "kernel", ",", "bias", "=", "bias", ",", "padding", "=", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.ckconv.causal_fftconv": [[126, 176], ["ckconv.causal_padding", "torch.pad", "torch.fft.rfft", "torch.fft.rfft", "torch.fft.rfft", "torch.fft.rfft", "torch.fft.rfft", "torch.fft.rfft", "torch.fft.rfft", "torch.fft.rfft", "torch.fft.rfft", "torch.fft.rfft", "torch.conj", "torch.conj", "torch.conj", "torch.conj", "torch.conj", "torch.fft.irfft().float", "torch.fft.irfft().float", "torch.fft.irfft().float", "torch.fft.irfft().float", "torch.fft.irfft().float", "x.double.double", "kernel.double.double", "torch.fft.irfft", "torch.fft.irfft", "torch.fft.irfft", "torch.fft.irfft", "torch.fft.irfft", "bias.view", "x.double.size", "kernel.double.size", "torch.fft.rfft.unsqueeze", "torch.conj.unsqueeze"], "function", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.ckconv.causal_padding", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.sc.pad"], ["", "def", "causal_fftconv", "(", "\n", "x", ":", "torch", ".", "Tensor", ",", "\n", "kernel", ":", "torch", ".", "Tensor", ",", "\n", "bias", ":", "Optional", "[", "torch", ".", "Tensor", "]", "=", "None", ",", "\n", "double_precision", ":", "bool", "=", "False", ",", "\n", ")", "->", "torch", ".", "Tensor", ":", "\n", "    ", "\"\"\"\n    Args:\n        x: (Tensor) Input tensor to be convolved with the kernel.\n        kernel: (Tensor) Convolution kernel.\n        bias: (Optional, Tensor) Bias tensor to add to the output.\n        padding: (int) Number of zero samples to pad the input on the last dimension.\n    Returns:\n        (Tensor) Convolved tensor\n    \"\"\"", "\n", "\n", "x_shape", "=", "x", ".", "shape", "\n", "# 1. Handle padding of the input and the kernel to make them odd.", "\n", "x", ",", "kernel", "=", "causal_padding", "(", "x", ",", "kernel", ")", "\n", "\n", "# 2. Pad the kernel tensor to make them equally big. Required for fft.", "\n", "kernel", "=", "f", ".", "pad", "(", "kernel", ",", "[", "0", ",", "x", ".", "size", "(", "-", "1", ")", "-", "kernel", ".", "size", "(", "-", "1", ")", "]", ")", "\n", "\n", "# 3. Perform fourier transform", "\n", "if", "double_precision", ":", "\n", "# We can make usage of double precision to make more accurate approximations of the convolution response.", "\n", "        ", "x", "=", "x", ".", "double", "(", ")", "\n", "kernel", "=", "kernel", ".", "double", "(", ")", "\n", "\n", "", "x_fr", "=", "torch", ".", "fft", ".", "rfft", "(", "x", ",", "dim", "=", "-", "1", ")", "\n", "kernel_fr", "=", "torch", ".", "fft", ".", "rfft", "(", "kernel", ",", "dim", "=", "-", "1", ")", "\n", "\n", "# 4. Multiply the transformed matrices:", "\n", "# (Input * Conj(Kernel)) = Correlation(Input, Kernel)", "\n", "kernel_fr", "=", "torch", ".", "conj", "(", "kernel_fr", ")", "\n", "output_fr", "=", "(", "x_fr", ".", "unsqueeze", "(", "1", ")", "*", "kernel_fr", ".", "unsqueeze", "(", "0", ")", ")", ".", "sum", "(", "\n", "2", "\n", ")", "# 'ab..., cb... -> ac...'", "\n", "\n", "# 5. Compute inverse FFT, and remove extra padded values", "\n", "# Once we are back in the spatial domain, we can go back to float precision, if double used.", "\n", "out", "=", "torch", ".", "fft", ".", "irfft", "(", "output_fr", ",", "dim", "=", "-", "1", ")", ".", "float", "(", ")", "\n", "\n", "out", "=", "out", "[", ":", ",", ":", ",", ":", "x_shape", "[", "-", "1", "]", "]", "\n", "\n", "# 6. Optionally, add a bias term before returning.", "\n", "if", "bias", "is", "not", "None", ":", "\n", "        ", "out", "=", "out", "+", "bias", ".", "view", "(", "1", ",", "-", "1", ",", "1", ")", "\n", "\n", "", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.transformer.ClassificationTransformer.__init__": [[23, 62], ["torch.nn.Module.__init__", "torch.nn.Linear", "transformer.TransformerEncoder", "torch.nn.Linear", "transformer.ClassificationTransformer._reset_parameters", "transformer.TransformerEncoderLayer", "torch.nn.modules.normalization.LayerNorm"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.transformer.Transformer._reset_parameters"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "d_input", ",", "\n", "d_output", ",", "\n", "d_model", ":", "int", "=", "512", ",", "\n", "nhead", ":", "int", "=", "8", ",", "\n", "num_encoder_layers", ":", "int", "=", "6", ",", "\n", "dim_feedforward", ":", "int", "=", "2048", ",", "\n", "dropout", ":", "float", "=", "0.1", ",", "\n", "activation", ":", "str", "=", "\"gelu\"", ",", "\n", "prenorm", ":", "bool", "=", "False", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "# Input projection to make the number of channels `d_model`", "\n", "self", ".", "input_proj", "=", "torch", ".", "nn", ".", "Linear", "(", "\n", "in_features", "=", "d_input", ",", "\n", "out_features", "=", "d_model", ",", "\n", ")", "\n", "\n", "# Create the TransformerEncoder blocks", "\n", "self", ".", "encoder", "=", "TransformerEncoder", "(", "\n", "TransformerEncoderLayer", "(", "\n", "d_model", ",", "nhead", ",", "dim_feedforward", ",", "dropout", ",", "activation", ",", "share_qk", "=", "False", ",", "prenorm", "=", "prenorm", "\n", ")", ",", "\n", "num_encoder_layers", ",", "\n", "LayerNorm", "(", "d_model", ")", "\n", ")", "\n", "\n", "# Output projection", "\n", "self", ".", "output_proj", "=", "torch", ".", "nn", ".", "Linear", "(", "\n", "in_features", "=", "d_model", ",", "\n", "out_features", "=", "d_output", ",", "\n", ")", "\n", "self", ".", "_reset_parameters", "(", ")", "\n", "\n", "self", ".", "d_model", "=", "d_model", "\n", "self", ".", "nhead", "=", "nhead", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.transformer.ClassificationTransformer.forward": [[63, 74], ["transformer.ClassificationTransformer.input_proj", "transformer.ClassificationTransformer.encoder.forward", "transformer.ClassificationTransformer.output_proj"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.models.resnext.CifarResNeXt.forward"], ["", "def", "forward", "(", "\n", "self", ",", "\n", "src", ":", "Tensor", ",", "\n", "*", "args", ",", "\n", "**", "kwargs", "\n", ")", "->", "Tensor", ":", "\n", "\n", "# Encode the input (B, S, C)", "\n", "        ", "x", "=", "self", ".", "input_proj", "(", "src", ")", "\n", "x", "=", "self", ".", "encoder", ".", "forward", "(", "x", ")", "\n", "return", "self", ".", "output_proj", "(", "x", "[", ":", ",", "-", "1", ",", ":", "]", ")", "# uses the encoding of the last \"token\" to classify", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.transformer.ClassificationTransformer._reset_parameters": [[75, 81], ["transformer.ClassificationTransformer.parameters", "p.dim", "torch.nn.init.xavier_uniform_"], "methods", ["None"], ["", "def", "_reset_parameters", "(", "self", ")", ":", "\n", "        ", "r\"\"\"Initiate parameters in the transformer model.\"\"\"", "\n", "\n", "for", "p", "in", "self", ".", "parameters", "(", ")", ":", "\n", "            ", "if", "p", ".", "dim", "(", ")", ">", "1", ":", "\n", "                ", "xavier_uniform_", "(", "p", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.transformer.Transformer.__init__": [[109, 135], ["torch.nn.Module.__init__", "transformer.Transformer._reset_parameters", "transformer.TransformerEncoderLayer", "torch.nn.modules.normalization.LayerNorm", "transformer.TransformerEncoder", "transformer.TransformerDecoderLayer", "torch.nn.modules.normalization.LayerNorm", "transformer.TransformerDecoder"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.transformer.Transformer._reset_parameters"], ["def", "__init__", "(", "self", ",", "d_model", ":", "int", "=", "512", ",", "nhead", ":", "int", "=", "8", ",", "num_encoder_layers", ":", "int", "=", "6", ",", "\n", "num_decoder_layers", ":", "int", "=", "6", ",", "dim_feedforward", ":", "int", "=", "2048", ",", "dropout", ":", "float", "=", "0.1", ",", "\n", "activation", ":", "str", "=", "\"relu\"", ",", "custom_encoder", ":", "Optional", "[", "Any", "]", "=", "None", ",", "\n", "custom_decoder", ":", "Optional", "[", "Any", "]", "=", "None", ",", "approx", ":", "dict", "=", "None", ")", "->", "None", ":", "\n", "        ", "super", "(", "Transformer", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "if", "custom_encoder", "is", "not", "None", ":", "\n", "            ", "self", ".", "encoder", "=", "custom_encoder", "\n", "", "else", ":", "\n", "            ", "encoder_layer", "=", "TransformerEncoderLayer", "(", "d_model", ",", "nhead", ",", "dim_feedforward", ",", "dropout", ",", "activation", ",", "\n", "share_qk", "=", "False", ")", "\n", "encoder_norm", "=", "LayerNorm", "(", "d_model", ")", "\n", "self", ".", "encoder", "=", "TransformerEncoder", "(", "encoder_layer", ",", "num_encoder_layers", ",", "encoder_norm", ")", "\n", "\n", "", "if", "custom_decoder", "is", "not", "None", ":", "\n", "            ", "self", ".", "decoder", "=", "custom_decoder", "\n", "", "else", ":", "\n", "            ", "decoder_layer", "=", "TransformerDecoderLayer", "(", "d_model", ",", "nhead", ",", "dim_feedforward", ",", "dropout", ",", "activation", ",", "\n", "share_qk", "=", "False", ")", "\n", "decoder_norm", "=", "LayerNorm", "(", "d_model", ")", "\n", "self", ".", "decoder", "=", "TransformerDecoder", "(", "decoder_layer", ",", "num_decoder_layers", ",", "decoder_norm", ")", "\n", "\n", "", "self", ".", "_reset_parameters", "(", ")", "\n", "\n", "self", ".", "d_model", "=", "d_model", "\n", "self", ".", "nhead", "=", "nhead", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.transformer.Transformer.forward": [[136, 188], ["transformer.Transformer.encoder", "transformer.Transformer.decoder", "src.size", "tgt.size", "RuntimeError", "RuntimeError", "src.size", "tgt.size"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "src", ":", "Tensor", ",", "tgt", ":", "Tensor", ",", "src_mask", ":", "Optional", "[", "Tensor", "]", "=", "None", ",", "tgt_mask", ":", "Optional", "[", "Tensor", "]", "=", "None", ",", "\n", "memory_mask", ":", "Optional", "[", "Tensor", "]", "=", "None", ",", "src_key_padding_mask", ":", "Optional", "[", "Tensor", "]", "=", "None", ",", "\n", "tgt_key_padding_mask", ":", "Optional", "[", "Tensor", "]", "=", "None", ",", "\n", "memory_key_padding_mask", ":", "Optional", "[", "Tensor", "]", "=", "None", ")", "->", "Tensor", ":", "\n", "        ", "r\"\"\"Take in and process masked source/target sequences.\n            src: the sequence to the encoder (required).\n            tgt: the sequence to the decoder (required).\n            src_mask: the additive mask for the src sequence (optional).\n            tgt_mask: the additive mask for the tgt sequence (optional).\n            memory_mask: the additive mask for the encoder output (optional).\n            src_key_padding_mask: the ByteTensor mask for src keys per batch (optional).\n            tgt_key_padding_mask: the ByteTensor mask for tgt keys per batch (optional).\n            memory_key_padding_mask: the ByteTensor mask for memory keys per batch (optional).\n        Shape:\n            - src: :math:`(S, N, E)`.\n            - tgt: :math:`(T, N, E)`.\n            - src_mask: :math:`(S, S)`.\n            - tgt_mask: :math:`(T, T)`.\n            - memory_mask: :math:`(T, S)`.\n            - src_key_padding_mask: :math:`(N, S)`.\n            - tgt_key_padding_mask: :math:`(N, T)`.\n            - memory_key_padding_mask: :math:`(N, S)`.\n            Note: [src/tgt/memory]_mask ensures that position i is allowed to attend the unmasked\n            positions. If a ByteTensor is provided, the non-zero positions are not allowed to attend\n            while the zero positions will be unchanged. If a BoolTensor is provided, positions with ``True``\n            are not allowed to attend while ``False`` values will be unchanged. If a FloatTensor\n            is provided, it will be added to the attention weight.\n            [src/tgt/memory]_key_padding_mask provides specified elements in the key to be ignored by\n            the attention. If a ByteTensor is provided, the non-zero positions will be ignored while the zero\n            positions will be unchanged. If a BoolTensor is provided, the positions with the\n            value of ``True`` will be ignored while the position with the value of ``False`` will be unchanged.\n            - output: :math:`(T, N, E)`.\n            Note: Due to the multi-head attention architecture in the transformer model,\n            the output sequence length of a transformer is same as the input sequence\n            (i.e. target) length of the decode.\n            where S is the source sequence length, T is the target sequence length, N is the\n            batch size, E is the feature number\n        Examples:\n            >>> output = transformer_model(src, tgt, src_mask=src_mask, tgt_mask=tgt_mask)\n        \"\"\"", "\n", "\n", "if", "src", ".", "size", "(", "1", ")", "!=", "tgt", ".", "size", "(", "1", ")", ":", "\n", "            ", "raise", "RuntimeError", "(", "\"the batch number of src and tgt must be equal\"", ")", "\n", "\n", "", "if", "src", ".", "size", "(", "2", ")", "!=", "self", ".", "d_model", "or", "tgt", ".", "size", "(", "2", ")", "!=", "self", ".", "d_model", ":", "\n", "            ", "raise", "RuntimeError", "(", "\"the feature number of src and tgt must be equal to d_model\"", ")", "\n", "\n", "", "memory", "=", "self", ".", "encoder", "(", "src", ",", "mask", "=", "src_mask", ",", "src_key_padding_mask", "=", "src_key_padding_mask", ")", "\n", "output", "=", "self", ".", "decoder", "(", "tgt", ",", "memory", ",", "tgt_mask", "=", "tgt_mask", ",", "memory_mask", "=", "memory_mask", ",", "\n", "tgt_key_padding_mask", "=", "tgt_key_padding_mask", ",", "\n", "memory_key_padding_mask", "=", "memory_key_padding_mask", ")", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.transformer.Transformer.generate_square_subsequent_mask": [[189, 196], ["mask.float().masked_fill().masked_fill.float().masked_fill().masked_fill.float().masked_fill().masked_fill", "float", "mask.float().masked_fill().masked_fill.float().masked_fill().masked_fill.float().masked_fill", "torch.triu", "float", "torch.ones", "mask.float().masked_fill().masked_fill.float().masked_fill().masked_fill.float"], "methods", ["None"], ["", "def", "generate_square_subsequent_mask", "(", "self", ",", "sz", ":", "int", ")", "->", "Tensor", ":", "\n", "        ", "r\"\"\"Generate a square mask for the sequence. The masked positions are filled with float('-inf').\n            Unmasked positions are filled with float(0.0).\n        \"\"\"", "\n", "mask", "=", "(", "torch", ".", "triu", "(", "torch", ".", "ones", "(", "sz", ",", "sz", ")", ")", "==", "1", ")", ".", "transpose", "(", "0", ",", "1", ")", "\n", "mask", "=", "mask", ".", "float", "(", ")", ".", "masked_fill", "(", "mask", "==", "0", ",", "float", "(", "'-inf'", ")", ")", ".", "masked_fill", "(", "mask", "==", "1", ",", "float", "(", "0.0", ")", ")", "\n", "return", "mask", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.transformer.Transformer._reset_parameters": [[197, 203], ["transformer.Transformer.parameters", "p.dim", "torch.nn.init.xavier_uniform_"], "methods", ["None"], ["", "def", "_reset_parameters", "(", "self", ")", ":", "\n", "        ", "r\"\"\"Initiate parameters in the transformer model.\"\"\"", "\n", "\n", "for", "p", "in", "self", ".", "parameters", "(", ")", ":", "\n", "            ", "if", "p", ".", "dim", "(", ")", ">", "1", ":", "\n", "                ", "xavier_uniform_", "(", "p", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.transformer.TransformerEncoder.__init__": [[219, 224], ["torch.nn.Module.__init__", "transformer._get_clones"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.transformer._get_clones"], ["def", "__init__", "(", "self", ",", "encoder_layer", ",", "num_layers", ",", "norm", "=", "None", ")", ":", "\n", "        ", "super", "(", "TransformerEncoder", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "layers", "=", "_get_clones", "(", "encoder_layer", ",", "num_layers", ")", "\n", "self", ".", "num_layers", "=", "num_layers", "\n", "self", ".", "norm", "=", "norm", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.transformer.TransformerEncoder.forward": [[225, 243], ["mod", "transformer.TransformerEncoder.norm"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "src", ":", "Tensor", ",", "mask", ":", "Optional", "[", "Tensor", "]", "=", "None", ",", "types", ":", "Optional", "[", "dict", "]", "=", "None", ",", "\n", "src_key_padding_mask", ":", "Optional", "[", "Tensor", "]", "=", "None", ")", "->", "Tensor", ":", "\n", "        ", "r\"\"\"Pass the input through the encoder layers in turn.\n        Args:\n            src: the sequence to the encoder (required).\n            mask: the mask for the src sequence (optional).\n            src_key_padding_mask: the mask for the src keys per batch (optional).\n        Shape:\n            see the docs in Transformer class.\n        \"\"\"", "\n", "output", "=", "src", "\n", "for", "mod", "in", "self", ".", "layers", ":", "\n", "            ", "output", "=", "mod", "(", "output", ",", "types", "=", "types", ",", "src_mask", "=", "mask", ",", "src_key_padding_mask", "=", "src_key_padding_mask", ")", "\n", "\n", "", "if", "self", ".", "norm", "is", "not", "None", ":", "\n", "            ", "output", "=", "self", ".", "norm", "(", "output", ")", "\n", "\n", "", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.transformer.TransformerDecoder.__init__": [[260, 265], ["torch.nn.Module.__init__", "transformer._get_clones"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.transformer._get_clones"], ["def", "__init__", "(", "self", ",", "decoder_layer", ",", "num_layers", ",", "norm", "=", "None", ")", ":", "\n", "        ", "super", "(", "TransformerDecoder", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "layers", "=", "_get_clones", "(", "decoder_layer", ",", "num_layers", ")", "\n", "self", ".", "num_layers", "=", "num_layers", "\n", "self", ".", "norm", "=", "norm", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.transformer.TransformerDecoder.forward": [[266, 292], ["mod", "transformer.TransformerDecoder.norm"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "tgt", ":", "Tensor", ",", "memory", ":", "Tensor", ",", "types", ":", "Optional", "[", "dict", "]", "=", "None", ",", "tgt_mask", ":", "Optional", "[", "Tensor", "]", "=", "None", ",", "\n", "memory_mask", ":", "Optional", "[", "Tensor", "]", "=", "None", ",", "tgt_key_padding_mask", ":", "Optional", "[", "Tensor", "]", "=", "None", ",", "\n", "memory_key_padding_mask", ":", "Optional", "[", "Tensor", "]", "=", "None", ")", "->", "Tensor", ":", "\n", "        ", "r\"\"\"Pass the inputs (and mask) through the decoder layer in turn.\n        Args:\n            tgt: the sequence to the decoder (required).\n            memory: the sequence from the last layer of the encoder (required).\n            tgt_mask: the mask for the tgt sequence (optional).\n            memory_mask: the mask for the memory sequence (optional).\n            tgt_key_padding_mask: the mask for the tgt keys per batch (optional).\n            memory_key_padding_mask: the mask for the memory keys per batch (optional).\n        Shape:\n            see the docs in Transformer class.\n        \"\"\"", "\n", "output", "=", "tgt", "\n", "\n", "for", "mod", "in", "self", ".", "layers", ":", "\n", "            ", "output", "=", "mod", "(", "output", ",", "memory", ",", "types", "=", "types", ",", "tgt_mask", "=", "tgt_mask", ",", "\n", "memory_mask", "=", "memory_mask", ",", "\n", "tgt_key_padding_mask", "=", "tgt_key_padding_mask", ",", "\n", "memory_key_padding_mask", "=", "memory_key_padding_mask", ")", "\n", "\n", "", "if", "self", ".", "norm", "is", "not", "None", ":", "\n", "            ", "output", "=", "self", ".", "norm", "(", "output", ")", "\n", "\n", "", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.transformer.TransformerEncoderLayer.__init__": [[313, 337], ["torch.nn.Module.__init__", "transformer.MultiheadAttention", "torch.nn.modules.linear.Linear", "torch.nn.modules.dropout.Dropout", "torch.nn.modules.linear.Linear", "torch.nn.modules.normalization.LayerNorm", "torch.nn.modules.normalization.LayerNorm", "torch.nn.modules.dropout.Dropout", "torch.nn.modules.dropout.Dropout", "transformer._get_activation_fn"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.transformer._get_activation_fn"], ["def", "__init__", "(", "\n", "self", ",", "\n", "d_model", ",", "\n", "nhead", ",", "\n", "dim_feedforward", "=", "2048", ",", "\n", "dropout", "=", "0.1", ",", "\n", "activation", "=", "\"relu\"", ",", "\n", "share_qk", "=", "False", ",", "\n", "prenorm", "=", "False", ",", "\n", ")", ":", "\n", "        ", "super", "(", "TransformerEncoderLayer", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "self_attn", "=", "MultiheadAttention", "(", "d_model", ",", "nhead", ",", "dropout", "=", "dropout", ",", "share_qk", "=", "share_qk", ")", "\n", "# Implementation of Feedforward model", "\n", "self", ".", "linear1", "=", "Linear", "(", "d_model", ",", "dim_feedforward", ")", "\n", "self", ".", "dropout", "=", "Dropout", "(", "dropout", ")", "\n", "self", ".", "linear2", "=", "Linear", "(", "dim_feedforward", ",", "d_model", ")", "\n", "\n", "self", ".", "norm1", "=", "LayerNorm", "(", "d_model", ")", "\n", "self", ".", "norm2", "=", "LayerNorm", "(", "d_model", ")", "\n", "self", ".", "dropout1", "=", "Dropout", "(", "dropout", ")", "\n", "self", ".", "dropout2", "=", "Dropout", "(", "dropout", ")", "\n", "\n", "self", ".", "activation", "=", "_get_activation_fn", "(", "activation", ")", "\n", "self", ".", "prenorm", "=", "prenorm", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.transformer.TransformerEncoderLayer.__setstate__": [[338, 342], ["super().__setstate__"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.transformer.TransformerDecoderLayer.__setstate__"], ["", "def", "__setstate__", "(", "self", ",", "state", ")", ":", "\n", "        ", "if", "'activation'", "not", "in", "state", ":", "\n", "            ", "state", "[", "'activation'", "]", "=", "F", ".", "relu", "\n", "", "super", "(", "TransformerEncoderLayer", ",", "self", ")", ".", "__setstate__", "(", "state", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.transformer.TransformerEncoderLayer.forward": [[343, 377], ["transformer.TransformerEncoderLayer.norm1", "transformer.TransformerEncoderLayer.norm1", "transformer.TransformerEncoderLayer.norm2", "transformer.TransformerEncoderLayer.linear2", "transformer.TransformerEncoderLayer.linear2", "transformer.TransformerEncoderLayer.norm2", "transformer.TransformerEncoderLayer.self_attn", "transformer.TransformerEncoderLayer.dropout1", "transformer.TransformerEncoderLayer.self_attn", "transformer.TransformerEncoderLayer.dropout1", "transformer.TransformerEncoderLayer.dropout", "transformer.TransformerEncoderLayer.dropout2", "transformer.TransformerEncoderLayer.dropout", "transformer.TransformerEncoderLayer.dropout2", "transformer.TransformerEncoderLayer.activation", "transformer.TransformerEncoderLayer.activation", "transformer.TransformerEncoderLayer.linear1", "transformer.TransformerEncoderLayer.linear1"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "src", ":", "Tensor", ",", "types", ":", "Optional", "[", "dict", "]", "=", "None", ",", "src_mask", ":", "Optional", "[", "Tensor", "]", "=", "None", ",", "\n", "src_key_padding_mask", ":", "Optional", "[", "Tensor", "]", "=", "None", ")", "->", "Tensor", ":", "\n", "        ", "r\"\"\"Pass the input through the encoder layer.\n        Args:\n            src: the sequence to the encoder layer (required).\n            src_mask: the mask for the src sequence (optional).\n            src_key_padding_mask: the mask for the src keys per batch (optional).\n        Shape:\n            see the docs in Transformer class.\n        \"\"\"", "\n", "\n", "if", "self", ".", "prenorm", ":", "\n", "# src = self.norm1(src)", "\n", "            ", "src2", "=", "self", ".", "norm1", "(", "src", ")", "\n", "src2", "=", "self", ".", "self_attn", "(", "src2", ",", "src2", ",", "src2", ",", "types", "=", "types", ",", "attn_mask", "=", "src_mask", ",", "\n", "key_padding_mask", "=", "src_key_padding_mask", ")", "[", "0", "]", "\n", "src", "=", "src", "+", "self", ".", "dropout1", "(", "src2", ")", "\n", "", "else", ":", "\n", "# Old code", "\n", "            ", "src2", "=", "self", ".", "self_attn", "(", "src", ",", "src", ",", "src", ",", "types", "=", "types", ",", "attn_mask", "=", "src_mask", ",", "\n", "key_padding_mask", "=", "src_key_padding_mask", ")", "[", "0", "]", "\n", "src", "=", "src", "+", "self", ".", "dropout1", "(", "src2", ")", "\n", "src", "=", "self", ".", "norm1", "(", "src", ")", "\n", "\n", "", "if", "self", ".", "prenorm", ":", "\n", "            ", "src2", "=", "self", ".", "norm2", "(", "src", ")", "\n", "src2", "=", "self", ".", "linear2", "(", "self", ".", "dropout", "(", "self", ".", "activation", "(", "self", ".", "linear1", "(", "src2", ")", ")", ")", ")", "\n", "src", "=", "src", "+", "self", ".", "dropout2", "(", "src2", ")", "\n", "", "else", ":", "\n", "# Old code", "\n", "            ", "src2", "=", "self", ".", "linear2", "(", "self", ".", "dropout", "(", "self", ".", "activation", "(", "self", ".", "linear1", "(", "src", ")", ")", ")", ")", "\n", "src", "=", "src", "+", "self", ".", "dropout2", "(", "src2", ")", "\n", "src", "=", "self", ".", "norm2", "(", "src", ")", "\n", "", "return", "src", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.transformer.TransformerDecoderLayer.__init__": [[399, 417], ["torch.nn.Module.__init__", "transformer.MultiheadAttention", "transformer.MultiheadAttention", "torch.nn.modules.linear.Linear", "torch.nn.modules.dropout.Dropout", "torch.nn.modules.linear.Linear", "torch.nn.modules.normalization.LayerNorm", "torch.nn.modules.normalization.LayerNorm", "torch.nn.modules.normalization.LayerNorm", "torch.nn.modules.dropout.Dropout", "torch.nn.modules.dropout.Dropout", "torch.nn.modules.dropout.Dropout", "transformer._get_activation_fn"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.transformer._get_activation_fn"], ["def", "__init__", "(", "self", ",", "d_model", ",", "nhead", ",", "dim_feedforward", "=", "2048", ",", "dropout", "=", "0.1", ",", "activation", "=", "\"relu\"", ",", "share_qk", "=", "False", ",", "\n", "approx", "=", "None", ")", ":", "\n", "        ", "super", "(", "TransformerDecoderLayer", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "self_attn", "=", "MultiheadAttention", "(", "d_model", ",", "nhead", ",", "dropout", "=", "dropout", ")", "\n", "self", ".", "multihead_attn", "=", "MultiheadAttention", "(", "d_model", ",", "nhead", ",", "dropout", "=", "dropout", ")", "\n", "# Implementation of Feedforward model", "\n", "self", ".", "linear1", "=", "Linear", "(", "d_model", ",", "dim_feedforward", ")", "\n", "self", ".", "dropout", "=", "Dropout", "(", "dropout", ")", "\n", "self", ".", "linear2", "=", "Linear", "(", "dim_feedforward", ",", "d_model", ")", "\n", "\n", "self", ".", "norm1", "=", "LayerNorm", "(", "d_model", ")", "\n", "self", ".", "norm2", "=", "LayerNorm", "(", "d_model", ")", "\n", "self", ".", "norm3", "=", "LayerNorm", "(", "d_model", ")", "\n", "self", ".", "dropout1", "=", "Dropout", "(", "dropout", ")", "\n", "self", ".", "dropout2", "=", "Dropout", "(", "dropout", ")", "\n", "self", ".", "dropout3", "=", "Dropout", "(", "dropout", ")", "\n", "\n", "self", ".", "activation", "=", "_get_activation_fn", "(", "activation", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.transformer.TransformerDecoderLayer.__setstate__": [[418, 422], ["super().__setstate__"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.transformer.TransformerDecoderLayer.__setstate__"], ["", "def", "__setstate__", "(", "self", ",", "state", ")", ":", "\n", "        ", "if", "'activation'", "not", "in", "state", ":", "\n", "            ", "state", "[", "'activation'", "]", "=", "F", ".", "relu", "\n", "", "super", "(", "TransformerDecoderLayer", ",", "self", ")", ".", "__setstate__", "(", "state", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.transformer.TransformerDecoderLayer.forward": [[423, 450], ["transformer.TransformerDecoderLayer.norm1", "transformer.TransformerDecoderLayer.norm2", "transformer.TransformerDecoderLayer.linear2", "transformer.TransformerDecoderLayer.norm3", "transformer.TransformerDecoderLayer.self_attn", "transformer.TransformerDecoderLayer.dropout1", "transformer.TransformerDecoderLayer.multihead_attn", "transformer.TransformerDecoderLayer.dropout2", "transformer.TransformerDecoderLayer.dropout", "transformer.TransformerDecoderLayer.dropout3", "transformer.TransformerDecoderLayer.activation", "transformer.TransformerDecoderLayer.linear1"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "tgt", ":", "Tensor", ",", "memory", ":", "Tensor", ",", "tgt_mask", ":", "Optional", "[", "Tensor", "]", "=", "None", ",", "\n", "memory_mask", ":", "Optional", "[", "Tensor", "]", "=", "None", ",", "\n", "tgt_key_padding_mask", ":", "Optional", "[", "Tensor", "]", "=", "None", ",", "\n", "memory_key_padding_mask", ":", "Optional", "[", "Tensor", "]", "=", "None", ")", "->", "Tensor", ":", "\n", "        ", "r\"\"\"Pass the inputs (and mask) through the decoder layer.\n        Args:\n            tgt: the sequence to the decoder layer (required).\n            memory: the sequence from the last layer of the encoder (required).\n            tgt_mask: the mask for the tgt sequence (optional).\n            memory_mask: the mask for the memory sequence (optional).\n            tgt_key_padding_mask: the mask for the tgt keys per batch (optional).\n            memory_key_padding_mask: the mask for the memory keys per batch (optional).\n        Shape:\n            see the docs in Transformer class.\n        \"\"\"", "\n", "tgt2", "=", "self", ".", "self_attn", "(", "tgt", ",", "tgt", ",", "tgt", ",", "attn_mask", "=", "tgt_mask", ",", "\n", "key_padding_mask", "=", "tgt_key_padding_mask", ")", "[", "0", "]", "\n", "tgt", "=", "tgt", "+", "self", ".", "dropout1", "(", "tgt2", ")", "\n", "tgt", "=", "self", ".", "norm1", "(", "tgt", ")", "\n", "tgt2", "=", "self", ".", "multihead_attn", "(", "tgt", ",", "memory", ",", "memory", ",", "attn_mask", "=", "memory_mask", ",", "\n", "key_padding_mask", "=", "memory_key_padding_mask", ")", "[", "0", "]", "\n", "tgt", "=", "tgt", "+", "self", ".", "dropout2", "(", "tgt2", ")", "\n", "tgt", "=", "self", ".", "norm2", "(", "tgt", ")", "\n", "tgt2", "=", "self", ".", "linear2", "(", "self", ".", "dropout", "(", "self", ".", "activation", "(", "self", ".", "linear1", "(", "tgt", ")", ")", ")", ")", "\n", "tgt", "=", "tgt", "+", "self", ".", "dropout3", "(", "tgt2", ")", "\n", "tgt", "=", "self", ".", "norm3", "(", "tgt", ")", "\n", "return", "tgt", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.transformer.MultiheadAttention.__init__": [[570, 621], ["super().__init__", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.init.xavier_uniform_", "torch.nn.init.xavier_uniform_", "torch.nn.init.xavier_uniform_", "torch.nn.Linear", "transformer.MultiheadAttentionContainer", "torch.nn.init.constant_", "torch.nn.init.constant_", "torch.nn.init.constant_", "torch.nn.parameter.Parameter", "torch.nn.parameter.Parameter", "torch.nn.init.xavier_normal_", "torch.nn.init.xavier_normal_", "transformer.SharedQK_Proj", "transformer.InProjContainer", "transformer.ScaledDotProduct", "torch.empty", "torch.empty"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__"], ["    ", "def", "__init__", "(", "self", ",", "embed_dim", ",", "num_heads", ",", "dropout", "=", "0.", ",", "bias", "=", "True", ",", "add_bias_kv", "=", "False", ",", "add_zero_attn", "=", "False", ",", "kdim", "=", "None", ",", "\n", "vdim", "=", "None", ",", "share_qk", "=", "False", ")", ":", "\n", "        ", "super", "(", "MultiheadAttention", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "embed_dim", "=", "embed_dim", "\n", "self", ".", "kdim", "=", "kdim", "if", "kdim", "is", "not", "None", "else", "embed_dim", "\n", "self", ".", "vdim", "=", "vdim", "if", "vdim", "is", "not", "None", "else", "embed_dim", "\n", "self", ".", "_qkv_same_embed_dim", "=", "self", ".", "kdim", "==", "embed_dim", "and", "self", ".", "vdim", "==", "embed_dim", "\n", "\n", "self", ".", "num_heads", "=", "num_heads", "\n", "self", ".", "dropout", "=", "dropout", "\n", "self", ".", "head_dim", "=", "embed_dim", "//", "num_heads", "\n", "assert", "self", ".", "head_dim", "*", "num_heads", "==", "self", ".", "embed_dim", ",", "\"embed_dim must be divisible by num_heads\"", "\n", "\n", "self", ".", "bias", "=", "bias", "\n", "self", ".", "add_bias_kv", "=", "add_bias_kv", "\n", "self", ".", "add_zero_attn", "=", "add_zero_attn", "\n", "\n", "self", ".", "q_proj_weight", "=", "torch", ".", "nn", ".", "Linear", "(", "embed_dim", ",", "embed_dim", ",", "bias", "=", "self", ".", "bias", ")", "\n", "self", ".", "k_proj_weight", "=", "torch", ".", "nn", ".", "Linear", "(", "embed_dim", ",", "self", ".", "kdim", ",", "bias", "=", "self", ".", "bias", ")", "\n", "self", ".", "v_proj_weight", "=", "torch", ".", "nn", ".", "Linear", "(", "embed_dim", ",", "self", ".", "vdim", ",", "bias", "=", "self", ".", "bias", ")", "\n", "xavier_uniform_", "(", "self", ".", "q_proj_weight", ".", "weight", ")", "\n", "xavier_uniform_", "(", "self", ".", "k_proj_weight", ".", "weight", ")", "\n", "xavier_uniform_", "(", "self", ".", "v_proj_weight", ".", "weight", ")", "\n", "self", ".", "out_proj", "=", "torch", ".", "nn", ".", "Linear", "(", "embed_dim", ",", "self", ".", "vdim", ")", "\n", "\n", "#         self._reset_parameters()", "\n", "\n", "if", "self", ".", "bias", ":", "\n", "            ", "constant_", "(", "self", ".", "q_proj_weight", ".", "bias", ",", "0.", ")", "\n", "constant_", "(", "self", ".", "v_proj_weight", ".", "bias", ",", "0.", ")", "\n", "constant_", "(", "self", ".", "out_proj", ".", "bias", ",", "0.", ")", "\n", "\n", "", "if", "add_bias_kv", ":", "\n", "            ", "self", ".", "bias_k", "=", "Parameter", "(", "torch", ".", "empty", "(", "1", ",", "1", ",", "embed_dim", ")", ")", "\n", "self", ".", "bias_v", "=", "Parameter", "(", "torch", ".", "empty", "(", "1", ",", "1", ",", "embed_dim", ")", ")", "\n", "xavier_normal_", "(", "self", ".", "bias_k", ")", "\n", "xavier_normal_", "(", "self", ".", "bias_v", ")", "\n", "\n", "", "else", ":", "\n", "            ", "self", ".", "bias_k", "=", "self", ".", "bias_v", "=", "None", "\n", "\n", "", "self", ".", "add_zero_attn", "=", "add_zero_attn", "\n", "\n", "if", "share_qk", ":", "\n", "            ", "self", ".", "in_proj_container", "=", "SharedQK_Proj", "(", "self", ".", "q_proj_weight", ",", "self", ".", "v_proj_weight", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "in_proj_container", "=", "InProjContainer", "(", "self", ".", "q_proj_weight", ",", "self", ".", "k_proj_weight", ",", "self", ".", "v_proj_weight", ")", "\n", "", "self", ".", "multihead_attention", "=", "MultiheadAttentionContainer", "(", "num_heads", ",", "\n", "self", ".", "in_proj_container", ",", "\n", "ScaledDotProduct", "(", "self", ".", "dropout", ")", ",", "\n", "self", ".", "out_proj", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.transformer.MultiheadAttention.forward": [[622, 628], ["transformer.MultiheadAttention.multihead_attention", "attn_mask.view.view.bool", "attn_mask.view.view.dim", "attn_mask.view.view.view", "attn_mask.view.view.size", "attn_mask.view.view.size"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "query", ",", "key", ",", "value", ",", "types", "=", "None", ",", "key_padding_mask", "=", "None", ",", "need_weights", "=", "True", ",", "attn_mask", "=", "None", ")", ":", "\n", "        ", "if", "attn_mask", "is", "not", "None", ":", "\n", "            ", "if", "attn_mask", ".", "dim", "(", ")", "==", "2", ":", "\n", "                ", "attn_mask", "=", "attn_mask", ".", "view", "(", "-", "1", ",", "attn_mask", ".", "size", "(", "0", ")", ",", "attn_mask", ".", "size", "(", "1", ")", ")", "\n", "", "attn_mask", "=", "attn_mask", ".", "bool", "(", ")", "\n", "", "return", "self", ".", "multihead_attention", "(", "query", ",", "key", ",", "value", ",", "types", ",", "attn_mask", ",", "self", ".", "bias_k", ",", "self", ".", "bias_v", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.transformer.MultiheadAttentionContainer.__init__": [[631, 660], ["super().__init__"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__"], ["    ", "def", "__init__", "(", "self", ",", "nhead", ",", "in_proj_container", ",", "attention_layer", ",", "out_proj", ")", ":", "\n", "        ", "r\"\"\" A multi-head attention container\n        Args:\n            nhead: the number of heads in the multiheadattention model\n            in_proj_container: A container of multi-head in-projection linear layers (a.k.a nn.Linear).\n            attention_layer: The attention layer.\n            out_proj: The multi-head out-projection layer (a.k.a nn.Linear).\n        Examples::\n            >>> import torch\n            >>> embed_dim, num_heads, bsz = 10, 5, 64\n            >>> in_proj_container = InProjContainer(torch.nn.Linear(embed_dim, embed_dim),\n                                                    torch.nn.Linear(embed_dim, embed_dim),\n                                                    torch.nn.Linear(embed_dim, embed_dim))\n            >>> MHA = MultiheadAttentionContainer(num_heads,\n                                                  in_proj_container,\n                                                  ScaledDotProduct(),\n                                                  torch.nn.Linear(embed_dim, embed_dim))\n            >>> query = torch.rand((21, bsz, embed_dim))\n            >>> key = value = torch.rand((16, bsz, embed_dim))\n            >>> attn_output, attn_weights = MHA(query, key, value)\n            >>> print(attn_output.shape)\n            >>> torch.Size([21, 64, 10])\n        \"\"\"", "\n", "super", "(", "MultiheadAttentionContainer", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "nhead", "=", "nhead", "\n", "self", ".", "in_proj_container", "=", "in_proj_container", "\n", "self", ".", "attention_layer", "=", "attention_layer", "\n", "self", ".", "out_proj", "=", "out_proj", "\n", "self", ".", "attn_map", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.transformer.MultiheadAttentionContainer.forward": [[661, 704], ["transformer.MultiheadAttentionContainer.in_proj_container", "q.reshape.reshape.reshape", "k.reshape.reshape.reshape", "v.reshape.reshape.reshape", "transformer.MultiheadAttentionContainer.attention_layer", "transformer.MultiheadAttentionContainer.reshape", "transformer.MultiheadAttentionContainer.out_proj", "query.size", "key.size", "query.size", "query.size", "q.reshape.reshape.size", "k.reshape.reshape.size", "v.reshape.reshape.size", "q.reshape.reshape.size", "k.reshape.reshape.size", "v.reshape.reshape.size"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "query", ":", "torch", ".", "Tensor", ",", "key", ":", "torch", ".", "Tensor", ",", "value", ":", "torch", ".", "Tensor", ",", "\n", "types", ":", "Optional", "[", "dict", "]", "=", "None", ",", "\n", "attn_mask", ":", "Optional", "[", "torch", ".", "Tensor", "]", "=", "None", ",", "\n", "bias_k", ":", "Optional", "[", "torch", ".", "Tensor", "]", "=", "None", ",", "\n", "bias_v", ":", "Optional", "[", "torch", ".", "Tensor", "]", "=", "None", ")", "->", "Tuple", "[", "torch", ".", "Tensor", ",", "torch", ".", "Tensor", "]", ":", "\n", "        ", "r\"\"\"\n        Args:\n            query, key, value (Tensor): map a query and a set of key-value pairs to an output.\n                See \"Attention Is All You Need\" for more details.\n            attn_mask, bias_k and bias_v (Tensor, optional): keyword arguments passed to the attention layer.\n                See the definitions in the attention.\n        Shape:\n            - Inputs:\n            - query: :math:`(L, N, E)`\n            - key: :math:`(S, N, E)`\n            - value: :math:`(S, N, E)`\n            - attn_mask, bias_k and bias_v: same with the shape of the corresponding args in attention layer.\n            - Outputs:\n            - attn_output: :math:`(L, N, E)`\n            - attn_output_weights: :math:`(N * H, L, S)`\n            where where L is the target length, S is the sequence length, H is the number of attention heads,\n                N is the batch size, and E is the embedding dimension.\n        \"\"\"", "\n", "tgt_len", ",", "src_len", ",", "bsz", ",", "embed_dim", "=", "query", ".", "size", "(", "-", "3", ")", ",", "key", ".", "size", "(", "-", "3", ")", ",", "query", ".", "size", "(", "-", "2", ")", ",", "query", ".", "size", "(", "-", "1", ")", "\n", "q", ",", "k", ",", "v", "=", "self", ".", "in_proj_container", "(", "query", ",", "key", ",", "value", ")", "\n", "assert", "q", ".", "size", "(", "-", "1", ")", "%", "self", ".", "nhead", "==", "0", ",", "\"query's embed_dim must be divisible by the number of heads\"", "\n", "head_dim", "=", "q", ".", "size", "(", "-", "1", ")", "//", "self", ".", "nhead", "\n", "q", "=", "q", ".", "reshape", "(", "tgt_len", ",", "bsz", "*", "self", ".", "nhead", ",", "head_dim", ")", "\n", "\n", "assert", "k", ".", "size", "(", "-", "1", ")", "%", "self", ".", "nhead", "==", "0", ",", "\"key's embed_dim must be divisible by the number of heads\"", "\n", "head_dim", "=", "k", ".", "size", "(", "-", "1", ")", "//", "self", ".", "nhead", "\n", "k", "=", "k", ".", "reshape", "(", "src_len", ",", "bsz", "*", "self", ".", "nhead", ",", "head_dim", ")", "\n", "\n", "assert", "v", ".", "size", "(", "-", "1", ")", "%", "self", ".", "nhead", "==", "0", ",", "\"value's embed_dim must be divisible by the number of heads\"", "\n", "head_dim", "=", "v", ".", "size", "(", "-", "1", ")", "//", "self", ".", "nhead", "\n", "v", "=", "v", ".", "reshape", "(", "src_len", ",", "bsz", "*", "self", ".", "nhead", ",", "head_dim", ")", "\n", "\n", "attn_output", ",", "attn_output_weights", ",", "self", ".", "attn_map", "=", "self", ".", "attention_layer", "(", "q", ",", "k", ",", "v", ",", "\n", "types", "=", "types", ",", "attn_mask", "=", "attn_mask", ",", "\n", "bias_k", "=", "bias_k", ",", "bias_v", "=", "bias_v", ")", "\n", "attn_output", "=", "attn_output", ".", "reshape", "(", "tgt_len", ",", "bsz", ",", "embed_dim", ")", "\n", "attn_output", "=", "self", ".", "out_proj", "(", "attn_output", ")", "\n", "return", "attn_output", ",", "attn_output_weights", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.transformer.ScaledDotProduct.__init__": [[708, 723], ["super().__init__"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__"], ["    ", "def", "__init__", "(", "self", ",", "dropout", "=", "0.0", ")", ":", "\n", "        ", "r\"\"\"Processes a projected query and key-value pair to apply\n        scaled dot product attention.\n        Args:\n            dropout (float): probability of dropping an attention weight.\n        Examples::\n            >>> SDP = torchtext.models.ScaledDotProduct(0.1)\n            >>> q = torch.randn(256, 21, 3)\n            >>> k = v = torch.randn(256, 21, 3)\n            >>> attn_output, attn_weights = SDP(q, k, v)\n            >>> print(attn_output.shape, attn_weights.shape)\n            torch.Size([256, 21, 3]) torch.Size([256, 21, 21])\n        \"\"\"", "\n", "super", "(", "ScaledDotProduct", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dropout", "=", "dropout", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.transformer.ScaledDotProduct.forward": [[724, 799], ["torch.cat.size", "max", "torch.matmul", "torch.nn.functional.dropout", "torch.matmul", "torch.cat", "torch.cat", "query.size", "query.size", "query.size", "torch.cat.size", "torch.cat.size", "torch.cat.size", "torch.cat.size", "query.size", "torch.cat.size", "query.transpose", "torch.cat.transpose", "torch.cat.transpose", "torch.cat.transpose", "torch.nn.functional.softmax.masked_fill_", "transformer.compute_single_distance", "torch.nn.functional.softmax", "torch.matmul.transpose", "torch.nn.functional.pad", "float", "torch.nn.functional.pad.dim", "RuntimeError", "RuntimeError", "RuntimeError", "torch.cat.size", "bias_k.size", "torch.cat.size", "bias_k.size", "bias_k.size", "torch.cat.size", "bias_v.size", "torch.cat.size", "bias_v.size", "bias_v.size", "torch.nn.functional.pad.size", "torch.nn.functional.pad.size", "torch.nn.functional.pad.size", "torch.nn.functional.pad.size"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.transformer.compute_single_distance", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.sc.pad"], ["", "def", "forward", "(", "self", ",", "query", ":", "torch", ".", "Tensor", ",", "key", ":", "torch", ".", "Tensor", ",", "value", ":", "torch", ".", "Tensor", ",", "\n", "types", ":", "Optional", "[", "dict", "]", "=", "None", ",", "\n", "attn_mask", ":", "Optional", "[", "torch", ".", "Tensor", "]", "=", "None", ",", "\n", "bias_k", ":", "Optional", "[", "torch", ".", "Tensor", "]", "=", "None", ",", "\n", "bias_v", ":", "Optional", "[", "torch", ".", "Tensor", "]", "=", "None", ")", "->", "Tuple", "[", "torch", ".", "Tensor", ",", "torch", ".", "Tensor", "]", ":", "\n", "        ", "r\"\"\"Uses a scaled dot product with the projected key-value pair to update\n        the projected query.\n        Args:\n            query (Tensor): Projected query\n            key (Tensor): Projected key\n            value (Tensor): Projected value\n            attn_mask (BoolTensor, optional): 3D mask that prevents attention to certain positions.\n            bias_k and bias_v: (Tensor, optional): one more key and value sequence to be added at\n                sequence dim (dim=-3). Those are used for incremental decoding. Users should provide\n                non-None to both arguments in order to activate them.\n        Shape:\n            - query: :math:`(L, N * H, E / H)`\n            - key: :math:`(S, N * H, E / H)`\n            - value: :math:`(S, N * H, E / H)`\n            - attn_mask: :math:`(N * H, L, S)`, positions with ``True`` are not allowed to attend\n                while ``False`` values will be unchanged.\n            - bias_k and bias_v:bias: :math:`(1, N * H, E / H)`\n            - Output: :math:`(L, N * H, E / H)`, :math:`(N * H, L, S)`\n            where L is the target length, S is the source length, H is the number\n            of attention heads, N is the batch size, and E is the embedding dimension.\n        \"\"\"", "\n", "if", "bias_k", "is", "not", "None", "and", "bias_v", "is", "not", "None", ":", "\n", "            ", "assert", "key", ".", "size", "(", "-", "1", ")", "==", "bias_k", ".", "size", "(", "-", "1", ")", "and", "key", ".", "size", "(", "-", "2", ")", "==", "bias_k", ".", "size", "(", "-", "2", ")", "and", "bias_k", ".", "size", "(", "-", "3", ")", "==", "1", ",", "\"Shape of bias_k is not supported\"", "\n", "assert", "value", ".", "size", "(", "-", "1", ")", "==", "bias_v", ".", "size", "(", "-", "1", ")", "and", "value", ".", "size", "(", "-", "2", ")", "==", "bias_v", ".", "size", "(", "-", "2", ")", "and", "bias_v", ".", "size", "(", "-", "3", ")", "==", "1", ",", "\"Shape of bias_v is not supported\"", "\n", "key", "=", "torch", ".", "cat", "(", "[", "key", ",", "bias_k", "]", ")", "\n", "value", "=", "torch", ".", "cat", "(", "[", "value", ",", "bias_v", "]", ")", "\n", "if", "attn_mask", "is", "not", "None", ":", "\n", "                ", "_attn_mask", "=", "attn_mask", "\n", "attn_mask", "=", "torch", ".", "nn", ".", "functional", ".", "pad", "(", "_attn_mask", ",", "(", "0", ",", "1", ")", ")", "\n", "\n", "", "", "tgt_len", ",", "head_dim", "=", "query", ".", "size", "(", "-", "3", ")", ",", "query", ".", "size", "(", "-", "1", ")", "\n", "assert", "query", ".", "size", "(", "-", "1", ")", "==", "key", ".", "size", "(", "-", "1", ")", "==", "value", ".", "size", "(", "-", "1", ")", ",", "\"The feature dim of query, key, value must be equal.\"", "\n", "assert", "key", ".", "size", "(", ")", "==", "value", ".", "size", "(", ")", ",", "\"Shape of key, value must match\"", "\n", "src_len", "=", "key", ".", "size", "(", "-", "3", ")", "\n", "batch_heads", "=", "max", "(", "query", ".", "size", "(", "-", "2", ")", ",", "key", ".", "size", "(", "-", "2", ")", ")", "\n", "\n", "# Scale query", "\n", "query", ",", "key", ",", "value", "=", "query", ".", "transpose", "(", "-", "2", ",", "-", "3", ")", ",", "key", ".", "transpose", "(", "-", "2", ",", "-", "3", ")", ",", "value", ".", "transpose", "(", "-", "2", ",", "-", "3", ")", "\n", "query", "=", "query", "*", "(", "float", "(", "head_dim", ")", "**", "-", "0.5", ")", "\n", "if", "attn_mask", "is", "not", "None", ":", "\n", "            ", "if", "attn_mask", ".", "dim", "(", ")", "!=", "3", ":", "\n", "                ", "raise", "RuntimeError", "(", "'attn_mask must be a 3D tensor.'", ")", "\n", "", "if", "(", "attn_mask", ".", "size", "(", "-", "1", ")", "!=", "src_len", ")", "or", "(", "attn_mask", ".", "size", "(", "-", "2", ")", "!=", "tgt_len", ")", "or", "(", "attn_mask", ".", "size", "(", "-", "3", ")", "!=", "1", "and", "attn_mask", ".", "size", "(", "-", "3", ")", "!=", "batch_heads", ")", ":", "\n", "                ", "raise", "RuntimeError", "(", "'The size of the attn_mask is not correct.'", ")", "\n", "", "if", "attn_mask", ".", "dtype", "!=", "torch", ".", "bool", ":", "\n", "                ", "raise", "RuntimeError", "(", "'Only bool tensor is supported for attn_mask'", ")", "\n", "\n", "# Dot product of q, k", "\n", "", "", "attn_output_weights", "=", "torch", ".", "matmul", "(", "query", ",", "key", ".", "transpose", "(", "-", "2", ",", "-", "1", ")", ")", "\n", "if", "attn_mask", "is", "not", "None", ":", "\n", "            ", "attn_output_weights", ".", "masked_fill_", "(", "attn_mask", ",", "-", "1e8", ",", ")", "\n", "", "attn_map", "=", "{", "}", "\n", "attn_map", "[", "'attn'", "]", "=", "attn_output_weights", "\n", "attn_map", "[", "'stat'", "]", "=", "None", "\n", "attn_map", "[", "'succeed'", "]", "=", "None", "\n", "\n", "# approx attn weights", "\n", "if", "(", "types", "is", "not", "None", ")", "and", "(", "not", "self", ".", "training", ")", ":", "\n", "            ", "attn_output_weights", ",", "attn_map", "[", "'stat'", "]", ",", "attn_map", "[", "'succeed'", "]", "=", "compute_single_distance", "(", "attn_map", "[", "'attn'", "]", ",", "attn_mask", ",", "types", "[", "'params_reduction'", "]", ",", "\n", "types", "[", "'approx_type'", "]", ",", "alpha", "=", "types", "[", "'alpha'", "]", ")", "\n", "", "else", ":", "\n", "            ", "attn_output_weights", "=", "torch", ".", "nn", ".", "functional", ".", "softmax", "(", "attn_output_weights", ",", "dim", "=", "-", "1", ")", "\n", "\n", "", "attn_output_weights", "=", "torch", ".", "nn", ".", "functional", ".", "dropout", "(", "attn_output_weights", ",", "p", "=", "self", ".", "dropout", ",", "training", "=", "self", ".", "training", ")", "\n", "attn_output", "=", "torch", ".", "matmul", "(", "attn_output_weights", ",", "value", ")", "\n", "return", "attn_output", ".", "transpose", "(", "-", "2", ",", "-", "3", ")", ",", "attn_output_weights", ",", "attn_map", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.transformer.SharedQK_Proj.__init__": [[802, 806], ["super().__init__"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__"], ["    ", "def", "__init__", "(", "self", ",", "qk_proj", ",", "v_proj", ")", ":", "\n", "        ", "super", "(", "SharedQK_Proj", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "qk_proj", "=", "qk_proj", "\n", "self", ".", "v_proj", "=", "qk_proj", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.transformer.SharedQK_Proj.forward": [[807, 809], ["transformer.SharedQK_Proj.qk_proj", "transformer.SharedQK_Proj.qk_proj", "transformer.SharedQK_Proj.v_proj"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "q", ",", "k", ",", "v", ")", ":", "\n", "        ", "return", "self", ".", "qk_proj", "(", "q", ")", ",", "self", ".", "qk_proj", "(", "k", ")", ",", "self", ".", "v_proj", "(", "v", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.transformer.InProjContainer.__init__": [[812, 824], ["super().__init__"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__"], ["    ", "def", "__init__", "(", "self", ",", "query_proj", ",", "key_proj", ",", "value_proj", ")", ":", "\n", "        ", "r\"\"\"A in-proj container to process inputs.\n        Args:\n            query_proj: a proj layer for query.\n            key_proj: a proj layer for key.\n            value_proj: a proj layer for value.\n        \"\"\"", "\n", "\n", "super", "(", "InProjContainer", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "query_proj", "=", "query_proj", "\n", "self", ".", "key_proj", "=", "key_proj", "\n", "self", ".", "value_proj", "=", "value_proj", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.transformer.InProjContainer.forward": [[825, 838], ["transformer.InProjContainer.query_proj", "transformer.InProjContainer.key_proj", "transformer.InProjContainer.value_proj"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "\n", "query", ":", "torch", ".", "Tensor", ",", "\n", "key", ":", "torch", ".", "Tensor", ",", "\n", "value", ":", "torch", ".", "Tensor", ")", "->", "Tuple", "[", "torch", ".", "Tensor", ",", "torch", ".", "Tensor", ",", "torch", ".", "Tensor", "]", ":", "\n", "        ", "r\"\"\"Projects the input sequences using in-proj layers.\n        Args:\n            query, key, value (Tensors): sequence to be projected\n        Shape:\n            - query, key, value: :math:`(S, N, E)`\n            - Output: :math:`(S, N, E)`\n            where S is the sequence length, N is the batch size, and E is the embedding dimension.\n        \"\"\"", "\n", "return", "self", ".", "query_proj", "(", "query", ")", ",", "self", ".", "key_proj", "(", "key", ")", ",", "self", ".", "value_proj", "(", "value", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.transformer._get_clones": [[452, 454], ["torch.nn.modules.container.ModuleList", "copy.deepcopy", "range"], "function", ["None"], ["", "", "def", "_get_clones", "(", "module", ",", "N", ")", ":", "\n", "    ", "return", "ModuleList", "(", "[", "copy", ".", "deepcopy", "(", "module", ")", "for", "i", "in", "range", "(", "N", ")", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.transformer._get_activation_fn": [[456, 463], ["RuntimeError"], "function", ["None"], ["", "def", "_get_activation_fn", "(", "activation", ")", ":", "\n", "    ", "if", "activation", "==", "\"relu\"", ":", "\n", "        ", "return", "F", ".", "relu", "\n", "", "elif", "activation", "==", "\"gelu\"", ":", "\n", "        ", "return", "F", ".", "gelu", "\n", "\n", "", "raise", "RuntimeError", "(", "\"activation should be relu/gelu, not {}\"", ".", "format", "(", "activation", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.transformer.kl": [[465, 468], ["torch.nn.functional.kl_div"], "function", ["None"], ["", "def", "kl", "(", "p", ",", "q", ")", ":", "\n", "    ", "kl_dis", "=", "F", ".", "kl_div", "(", "p", ",", "q", ")", "\n", "return", "kl_dis", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.transformer.mse": [[470, 473], ["torch.nn.functional.mse_loss"], "function", ["None"], ["", "def", "mse", "(", "p", ",", "q", ")", ":", "\n", "    ", "mse_loss", "=", "F", ".", "mse_loss", "(", "p", ",", "q", ")", "\n", "return", "mse_loss", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.transformer.l1": [[475, 478], ["torch.nn.functional.l1_loss"], "function", ["None"], ["", "def", "l1", "(", "p", ",", "q", ")", ":", "\n", "    ", "l1_loss", "=", "F", ".", "l1_loss", "(", "p", ",", "q", ")", "\n", "return", "l1_loss", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.transformer.smart_sort": [[480, 487], ["x.size", "x[].view", "torch.arange().unsqueeze().repeat().flatten", "permutation.flatten", "torch.arange().unsqueeze().repeat", "torch.arange().unsqueeze", "torch.arange"], "function", ["None"], ["", "def", "smart_sort", "(", "x", ",", "permutation", ")", ":", "\n", "    ", "d1", ",", "d2", "=", "x", ".", "size", "(", ")", "\n", "ret", "=", "x", "[", "\n", "torch", ".", "arange", "(", "d1", ")", ".", "unsqueeze", "(", "1", ")", ".", "repeat", "(", "(", "1", ",", "d2", ")", ")", ".", "flatten", "(", ")", ",", "\n", "permutation", ".", "flatten", "(", ")", "\n", "]", ".", "view", "(", "d1", ",", "d2", ")", "\n", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.transformer.sparsify": [[489, 498], ["target.clone", "torch.sort", "int", "torch.zeros_like().scatter_", "float", "round", "torch.zeros_like().scatter_.bool", "torch.zeros_like"], "function", ["None"], ["", "def", "sparsify", "(", "target", ",", "params_reduction", ")", ":", "\n", "    ", "target_sparse", "=", "target", ".", "clone", "(", ")", "\n", "N", ",", "target_l", ",", "seq_l", "=", "target_sparse", ".", "shape", "\n", "sorted_tensor", ",", "indices_tensor", "=", "torch", ".", "sort", "(", "target_sparse", ",", "dim", "=", "-", "1", ",", "descending", "=", "True", ")", "\n", "topk", "=", "int", "(", "round", "(", "seq_l", "*", "(", "1", "-", "params_reduction", ")", ")", ")", "\n", "mask", "=", "torch", ".", "zeros_like", "(", "target_sparse", ",", "dtype", "=", "torch", ".", "bool", ")", ".", "scatter_", "(", "-", "1", ",", "indices_tensor", "[", ":", ",", ":", ",", ":", "topk", "]", ",", "1", ")", "\n", "target_sparse", "[", "~", "mask", "]", "=", "float", "(", "\n", "'-inf'", ")", "# To zero out these values, we set their logit to be -inf, so that after softmax they are zero", "\n", "return", "target_sparse", ",", "mask", ".", "bool", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.transformer.low_rank": [[500, 512], ["target.clone", "torch.svd", "int", "torch.matmul", "round", "torch.matmul", "v.transpose", "torch.diag_embed"], "function", ["None"], ["", "def", "low_rank", "(", "target", ",", "sparsity", ")", ":", "\n", "    ", "N", ",", "target_l", ",", "seq_l", "=", "target", ".", "shape", "\n", "target_lr", "=", "target", ".", "clone", "(", ")", "\n", "try", ":", "\n", "        ", "u", ",", "s", ",", "v", "=", "torch", ".", "svd", "(", "target_lr", ")", "\n", "topk", "=", "int", "(", "round", "(", "seq_l", "*", "(", "1", "-", "sparsity", ")", ")", ")", "\n", "# assert torch.dist(target_lr, torch.matmul(torch.matmul(u, torch.diag_embed(s)), v.transpose(-2, -1)))<1e-2", "\n", "s", "[", ":", ",", "topk", ":", "]", "=", "0", "\n", "target_lr", "=", "torch", ".", "matmul", "(", "torch", ".", "matmul", "(", "u", ",", "torch", ".", "diag_embed", "(", "s", ")", ")", ",", "v", ".", "transpose", "(", "-", "2", ",", "-", "1", ")", ")", "\n", "return", "target_lr", ",", "True", "\n", "", "except", ":", "# torch.svd may have convergence issues for GPU and CPU.", "\n", "        ", "return", "target_lr", ",", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.transformer.log_stats": [[514, 520], ["transformer.l1", "transformer.kl", "transformer.kl", "torch.cat", "torch.log", "torch.log", "l1.view", "kl.view", "kl.view"], "function", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.transformer.l1", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.transformer.kl", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.transformer.kl"], ["", "", "def", "log_stats", "(", "approx", ",", "target", ")", ":", "\n", "    ", "eps", "=", "1e-5", "\n", "sparse_l1", "=", "l1", "(", "approx", ",", "target", ")", "\n", "sparse_kl", "=", "kl", "(", "torch", ".", "log", "(", "approx", "+", "eps", ")", ",", "target", "+", "eps", ")", "\n", "sparse_kl_inverse", "=", "kl", "(", "torch", ".", "log", "(", "target", "+", "eps", ")", ",", "approx", "+", "eps", ")", "\n", "return", "torch", ".", "cat", "(", "[", "sparse_l1", ".", "view", "(", "1", ")", ",", "sparse_kl", ".", "view", "(", "1", ")", ",", "sparse_kl_inverse", ".", "view", "(", "1", ")", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.transformer.compute_single_distance": [[522, 567], ["torch.zeros", "float", "torch.nn.functional.softmax", "transformer.sparsify", "torch.softmax", "transformer.log_stats", "target.clone.masked_fill_", "transformer.low_rank", "float", "torch.nn.functional.normalize", "transformer.log_stats", "F.softmax.clone", "transformer.sparsify", "transformer.low_rank", "print", "target_lr.masked_fill_", "torch.nn.functional.normalize", "transformer.log_stats", "target_sparse_lr.masked_fill_"], "function", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.transformer.sparsify", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.transformer.log_stats", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.transformer.low_rank", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.transformer.log_stats", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.transformer.sparsify", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.transformer.low_rank", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.transformer.log_stats"], ["", "def", "compute_single_distance", "(", "target_raw", ",", "attn_mask", ",", "params_reduction", ",", "approx_type", ",", "alpha", "=", "0.5", ")", ":", "\n", "    ", "stats", "=", "torch", ".", "zeros", "(", "[", "1", ",", "3", "]", ")", "\n", "target_raw", "[", "target_raw", "<", "-", "1e7", "]", "=", "float", "(", "'-inf'", ")", "\n", "target", "=", "F", ".", "softmax", "(", "target_raw", ",", "dim", "=", "-", "1", ")", "\n", "succeed", "=", "True", "\n", "approx_target", "=", "0", "\n", "\n", "# sparse", "\n", "if", "approx_type", "==", "\"sparse\"", ":", "\n", "        ", "target_sparse", ",", "mask", "=", "sparsify", "(", "target_raw", ",", "params_reduction", ")", "\n", "if", "attn_mask", "is", "not", "None", ":", "\n", "            ", "target_sparse", ".", "masked_fill_", "(", "attn_mask", ",", "float", "(", "'-inf'", ")", ",", ")", "\n", "", "approx_target", "=", "torch", ".", "softmax", "(", "target_sparse", ",", "dim", "=", "-", "1", ")", "\n", "stats", "=", "log_stats", "(", "approx_target", ",", "target", ")", "\n", "\n", "# low_rank", "\n", "", "elif", "approx_type", "==", "\"low_rank\"", ":", "\n", "        ", "new_sparsity", "=", "1", "-", "(", "1", "-", "params_reduction", ")", "/", "2", "\n", "target_lr", ",", "succeed", "=", "low_rank", "(", "target", ",", "new_sparsity", ")", "\n", "if", "succeed", ":", "\n", "            ", "target_lr", "[", "target_lr", "<", "0", "]", "=", "0.0", "\n", "if", "attn_mask", "is", "not", "None", ":", "\n", "                ", "target_lr", ".", "masked_fill_", "(", "attn_mask", ",", "0.0", ",", ")", "\n", "", "approx_target", "=", "F", ".", "normalize", "(", "target_lr", ",", "p", "=", "1", ",", "dim", "=", "-", "1", ")", "\n", "stats", "=", "log_stats", "(", "approx_target", ",", "target", ")", "\n", "\n", "# sparse+low_rank", "\n", "", "", "elif", "approx_type", "==", "\"sparse_low_rank\"", ":", "\n", "        ", "target_sparse", "=", "target", ".", "clone", "(", ")", "\n", "params_sparse", "=", "alpha", "*", "(", "1", "-", "params_reduction", ")", "\n", "_", ",", "mask", "=", "sparsify", "(", "target", ",", "1", "-", "params_sparse", ")", "\n", "target_sparse", "[", "~", "mask", "]", "=", "0.0", "\n", "target_sparse_lr", "=", "target", "-", "target_sparse", "\n", "params_lr", "=", "(", "1", "-", "alpha", ")", "*", "(", "1", "-", "params_reduction", ")", "/", "2", "\n", "target_sparse_lr", ",", "succeed", "=", "low_rank", "(", "target_sparse_lr", ",", "1", "-", "params_lr", ")", "\n", "if", "succeed", ":", "\n", "            ", "target_sparse_lr", "[", "target_sparse_lr", "<", "0", "]", "=", "0.0", "\n", "target_sparse_lr", "+=", "target_sparse", "\n", "if", "attn_mask", "is", "not", "None", ":", "\n", "                ", "target_sparse_lr", ".", "masked_fill_", "(", "attn_mask", ",", "0.0", ",", ")", "\n", "", "approx_target", "=", "F", ".", "normalize", "(", "target_sparse_lr", ",", "p", "=", "1", ",", "dim", "=", "-", "1", ")", "\n", "stats", "=", "log_stats", "(", "approx_target", ",", "target", ")", "\n", "", "", "else", ":", "\n", "        ", "print", "(", "\"Approximation type is not implemented\"", ")", "\n", "", "return", "approx_target", ",", "stats", ",", "succeed", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.transformer.generate_square_subsequent_mask": [[840, 849], ["torch.triu", "torch.ones"], "function", ["None"], ["", "", "def", "generate_square_subsequent_mask", "(", "nbatch", ",", "sz", ")", ":", "\n", "    ", "r\"\"\"Generate a square mask for the sequence. The masked positions are filled with True.\n        Unmasked positions are filled with False.\n    Args:\n        nbatch: the number of batch size\n        sz: the size of square mask\n    \"\"\"", "\n", "mask", "=", "(", "torch", ".", "triu", "(", "torch", ".", "ones", "(", "sz", ",", "sz", ")", ")", "==", "1", ")", ".", "transpose", "(", "0", ",", "1", ")", ".", "repeat", "(", "nbatch", ",", "1", ",", "1", ")", "\n", "return", "mask", "\n", "", ""]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.wavenet.DilatedQueue.__init__": [[55, 66], ["torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "dtype().zero_", "dtype"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "max_length", ",", "data", "=", "None", ",", "dilation", "=", "1", ",", "num_deq", "=", "1", ",", "num_channels", "=", "1", ",", "dtype", "=", "torch", ".", "FloatTensor", ")", ":", "\n", "        ", "self", ".", "in_pos", "=", "0", "\n", "self", ".", "out_pos", "=", "0", "\n", "self", ".", "num_deq", "=", "num_deq", "\n", "self", ".", "num_channels", "=", "num_channels", "\n", "self", ".", "dilation", "=", "dilation", "\n", "self", ".", "max_length", "=", "max_length", "\n", "self", ".", "data", "=", "data", "\n", "self", ".", "dtype", "=", "dtype", "\n", "if", "data", "==", "None", ":", "\n", "            ", "self", ".", "data", "=", "Variable", "(", "dtype", "(", "num_channels", ",", "max_length", ")", ".", "zero_", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.wavenet.DilatedQueue.enqueue": [[67, 73], ["input.squeeze", "len", "len", "wavenet.DilatedQueue.data.unsqueeze().repeat", "wavenet.DilatedQueue.data.unsqueeze"], "methods", ["None"], ["", "", "def", "enqueue", "(", "self", ",", "input", ")", ":", "\n", "        ", "assert", "len", "(", "input", ".", "shape", ")", "==", "3", "\n", "if", "len", "(", "self", ".", "data", ".", "shape", ")", "==", "2", ":", "\n", "            ", "self", ".", "data", "=", "self", ".", "data", ".", "unsqueeze", "(", "0", ")", ".", "repeat", "(", "input", ".", "shape", "[", "0", "]", ",", "1", ",", "1", ")", "\n", "", "self", ".", "data", "[", ":", ",", ":", ",", "self", ".", "in_pos", "]", "=", "input", ".", "squeeze", "(", "2", ")", "\n", "self", ".", "in_pos", "=", "(", "self", ".", "in_pos", "+", "1", ")", "%", "self", ".", "max_length", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.wavenet.DilatedQueue.dequeue": [[74, 88], ["torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat"], "methods", ["None"], ["", "def", "dequeue", "(", "self", ",", "num_deq", "=", "1", ",", "dilation", "=", "1", ")", ":", "\n", "#       |", "\n", "#  |6|7|8|1|2|3|4|5|", "\n", "#         |", "\n", "        ", "start", "=", "self", ".", "out_pos", "-", "(", "(", "num_deq", "-", "1", ")", "*", "dilation", ")", "\n", "if", "start", "<", "0", ":", "\n", "            ", "t1", "=", "self", ".", "data", "[", ":", ",", ":", ",", "start", ":", ":", "dilation", "]", "\n", "t2", "=", "self", ".", "data", "[", ":", ",", ":", ",", "self", ".", "out_pos", "%", "dilation", ":", "self", ".", "out_pos", "+", "1", ":", "dilation", "]", "\n", "t", "=", "torch", ".", "cat", "(", "(", "t1", ",", "t2", ")", ",", "2", ")", "\n", "", "else", ":", "\n", "            ", "t", "=", "self", ".", "data", "[", ":", ",", ":", ",", "start", ":", "self", ".", "out_pos", "+", "1", ":", "dilation", "]", "\n", "\n", "", "self", ".", "out_pos", "=", "(", "self", ".", "out_pos", "+", "1", ")", "%", "self", ".", "max_length", "\n", "return", "t", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.wavenet.DilatedQueue.reset": [[89, 93], ["torch.autograd.Variable().to", "torch.autograd.Variable().to", "torch.autograd.Variable().to", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "wavenet.DilatedQueue.dtype().zero_", "wavenet.DilatedQueue.dtype"], "methods", ["None"], ["", "def", "reset", "(", "self", ",", "device", ")", ":", "\n", "        ", "self", ".", "data", "=", "Variable", "(", "self", ".", "dtype", "(", "self", ".", "num_channels", ",", "self", ".", "max_length", ")", ".", "zero_", "(", ")", ")", ".", "to", "(", "device", ")", "\n", "self", ".", "in_pos", "=", "0", "\n", "self", ".", "out_pos", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.wavenet.WaveNetModel.d_output": [[120, 123], ["None"], "methods", ["None"], ["@", "property", "\n", "def", "d_output", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "classes", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.wavenet.WaveNetModel.default_state": [[125, 127], ["None"], "methods", ["None"], ["", "def", "default_state", "(", "self", ",", "*", "batch_shape", ",", "device", "=", "None", ")", ":", "\n", "        ", "return", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.wavenet.WaveNetModel.__init__": [[128, 224], ["src.models.sequence.base.SequenceModule.__init__", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.Conv1d", "torch.Conv1d", "torch.Conv1d", "range", "torch.Conv1d", "torch.Conv1d", "torch.Conv1d", "torch.Conv1d", "torch.Conv1d", "torch.Conv1d", "range", "wavenet.WaveNetModel.dilations.append", "wavenet.WaveNetModel.dilated_queues.append", "wavenet.WaveNetModel.filter_convs.append", "wavenet.WaveNetModel.gate_convs.append", "wavenet.WaveNetModel.residual_convs.append", "wavenet.WaveNetModel.skip_convs.append", "wavenet.DilatedQueue", "torch.Conv1d", "torch.Conv1d", "torch.Conv1d", "torch.Conv1d", "torch.Conv1d", "torch.Conv1d", "torch.Conv1d", "torch.Conv1d", "torch.Conv1d", "torch.Conv1d", "torch.Conv1d", "torch.Conv1d"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__"], ["", "def", "__init__", "(", "\n", "self", ",", "\n", "layers", "=", "10", ",", "\n", "blocks", "=", "4", ",", "\n", "dilation_channels", "=", "32", ",", "\n", "residual_channels", "=", "32", ",", "\n", "skip_channels", "=", "256", ",", "\n", "end_channels", "=", "256", ",", "\n", "classes", "=", "256", ",", "\n", "kernel_size", "=", "2", ",", "\n", "dtype", "=", "torch", ".", "FloatTensor", ",", "\n", "bias", "=", "False", ",", "\n", ")", ":", "\n", "\n", "        ", "super", "(", "WaveNetModel", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "layers", "=", "layers", "\n", "self", ".", "blocks", "=", "blocks", "\n", "self", ".", "dilation_channels", "=", "dilation_channels", "\n", "self", ".", "residual_channels", "=", "residual_channels", "\n", "self", ".", "skip_channels", "=", "skip_channels", "\n", "self", ".", "classes", "=", "classes", "\n", "self", ".", "kernel_size", "=", "kernel_size", "\n", "self", ".", "dtype", "=", "dtype", "\n", "\n", "self", ".", "d_model", "=", "256", "\n", "\n", "# build model", "\n", "receptive_field", "=", "1", "\n", "init_dilation", "=", "1", "\n", "\n", "self", ".", "dilations", "=", "[", "]", "\n", "self", ".", "dilated_queues", "=", "[", "]", "\n", "self", ".", "filter_convs", "=", "nn", ".", "ModuleList", "(", ")", "\n", "self", ".", "gate_convs", "=", "nn", ".", "ModuleList", "(", ")", "\n", "self", ".", "residual_convs", "=", "nn", ".", "ModuleList", "(", ")", "\n", "self", ".", "skip_convs", "=", "nn", ".", "ModuleList", "(", ")", "\n", "\n", "# 1x1 convolution to create channels", "\n", "self", ".", "start_conv", "=", "nn", ".", "Conv1d", "(", "in_channels", "=", "self", ".", "classes", ",", "\n", "out_channels", "=", "residual_channels", ",", "\n", "kernel_size", "=", "1", ",", "\n", "bias", "=", "bias", ")", "\n", "\n", "for", "b", "in", "range", "(", "blocks", ")", ":", "\n", "            ", "additional_scope", "=", "kernel_size", "-", "1", "\n", "new_dilation", "=", "1", "\n", "for", "i", "in", "range", "(", "layers", ")", ":", "\n", "# dilations of this layer", "\n", "                ", "self", ".", "dilations", ".", "append", "(", "(", "new_dilation", ",", "init_dilation", ")", ")", "\n", "\n", "# dilated queues for fast generation", "\n", "self", ".", "dilated_queues", ".", "append", "(", "DilatedQueue", "(", "max_length", "=", "(", "kernel_size", "-", "1", ")", "*", "new_dilation", "+", "1", ",", "\n", "num_channels", "=", "residual_channels", ",", "\n", "dilation", "=", "new_dilation", ",", "\n", "dtype", "=", "dtype", ")", ")", "\n", "\n", "# dilated convolutions", "\n", "self", ".", "filter_convs", ".", "append", "(", "nn", ".", "Conv1d", "(", "in_channels", "=", "residual_channels", ",", "\n", "out_channels", "=", "dilation_channels", ",", "\n", "kernel_size", "=", "kernel_size", ",", "\n", "bias", "=", "bias", ")", ")", "\n", "\n", "self", ".", "gate_convs", ".", "append", "(", "nn", ".", "Conv1d", "(", "in_channels", "=", "residual_channels", ",", "\n", "out_channels", "=", "dilation_channels", ",", "\n", "kernel_size", "=", "kernel_size", ",", "\n", "bias", "=", "bias", ")", ")", "\n", "\n", "# 1x1 convolution for residual connection", "\n", "self", ".", "residual_convs", ".", "append", "(", "nn", ".", "Conv1d", "(", "in_channels", "=", "dilation_channels", ",", "\n", "out_channels", "=", "residual_channels", ",", "\n", "kernel_size", "=", "1", ",", "\n", "bias", "=", "bias", ")", ")", "\n", "\n", "# 1x1 convolution for skip connection", "\n", "self", ".", "skip_convs", ".", "append", "(", "nn", ".", "Conv1d", "(", "in_channels", "=", "dilation_channels", ",", "\n", "out_channels", "=", "skip_channels", ",", "\n", "kernel_size", "=", "1", ",", "\n", "bias", "=", "bias", ")", ")", "\n", "\n", "receptive_field", "+=", "additional_scope", "\n", "additional_scope", "*=", "2", "\n", "init_dilation", "=", "new_dilation", "\n", "new_dilation", "*=", "2", "\n", "\n", "", "", "self", ".", "end_conv_1", "=", "nn", ".", "Conv1d", "(", "in_channels", "=", "skip_channels", ",", "\n", "out_channels", "=", "end_channels", ",", "\n", "kernel_size", "=", "1", ",", "\n", "bias", "=", "True", ")", "\n", "\n", "self", ".", "end_conv_2", "=", "nn", ".", "Conv1d", "(", "in_channels", "=", "end_channels", ",", "\n", "out_channels", "=", "classes", ",", "\n", "kernel_size", "=", "1", ",", "\n", "bias", "=", "True", ")", "\n", "\n", "self", ".", "receptive_field", "=", "receptive_field", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.wavenet.WaveNetModel.wavenet": [[226, 273], ["wavenet.WaveNetModel.start_conv", "range", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "wavenet.WaveNetModel.end_conv_2", "dilation_func", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "wavenet.WaveNetModel.end_conv_1", "wavenet.WaveNetModel.size", "wavenet.dilate", "dilate.size"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.wavenet.dilate"], ["", "def", "wavenet", "(", "self", ",", "input", ",", "dilation_func", ")", ":", "\n", "\n", "        ", "x", "=", "self", ".", "start_conv", "(", "input", ")", "\n", "skip", "=", "0", "\n", "\n", "# WaveNet layers", "\n", "for", "i", "in", "range", "(", "self", ".", "blocks", "*", "self", ".", "layers", ")", ":", "\n", "\n", "#            |----------------------------------------|     *residual*", "\n", "#            |                                        |", "\n", "#            |    |-- conv -- tanh --|                |", "\n", "# -> dilate -|----|                  * ----|-- 1x1 -- + -->\t*input*", "\n", "#                 |-- conv -- sigm --|     |", "\n", "#                                         1x1", "\n", "#                                          |", "\n", "# ---------------------------------------> + ------------->\t*skip*", "\n", "\n", "            ", "(", "dilation", ",", "init_dilation", ")", "=", "self", ".", "dilations", "[", "i", "]", "\n", "\n", "residual", "=", "dilation_func", "(", "x", ",", "dilation", ",", "init_dilation", ",", "i", ")", "\n", "\n", "# dilated convolution", "\n", "filter", "=", "self", ".", "filter_convs", "[", "i", "]", "(", "residual", ")", "\n", "filter", "=", "torch", ".", "tanh", "(", "filter", ")", "\n", "gate", "=", "self", ".", "gate_convs", "[", "i", "]", "(", "residual", ")", "\n", "gate", "=", "torch", ".", "sigmoid", "(", "gate", ")", "\n", "x", "=", "filter", "*", "gate", "\n", "\n", "# parametrized skip connection", "\n", "s", "=", "x", "\n", "if", "x", ".", "size", "(", "2", ")", "!=", "1", ":", "\n", "                 ", "s", "=", "dilate", "(", "x", ",", "1", ",", "init_dilation", "=", "dilation", ")", "\n", "", "s", "=", "self", ".", "skip_convs", "[", "i", "]", "(", "s", ")", "\n", "try", ":", "\n", "                ", "skip", "=", "skip", "[", ":", ",", ":", ",", "-", "s", ".", "size", "(", "2", ")", ":", "]", "\n", "", "except", ":", "\n", "                ", "skip", "=", "0", "\n", "", "skip", "=", "s", "+", "skip", "\n", "\n", "x", "=", "self", ".", "residual_convs", "[", "i", "]", "(", "x", ")", "\n", "x", "=", "x", "+", "residual", "[", ":", ",", ":", ",", "(", "self", ".", "kernel_size", "-", "1", ")", ":", "]", "\n", "\n", "", "x", "=", "F", ".", "relu", "(", "skip", ")", "\n", "x", "=", "F", ".", "relu", "(", "self", ".", "end_conv_1", "(", "x", ")", ")", "\n", "x", "=", "self", ".", "end_conv_2", "(", "x", ")", "\n", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.wavenet.WaveNetModel.wavenet_dilate": [[274, 277], ["wavenet.dilate"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.wavenet.dilate"], ["", "def", "wavenet_dilate", "(", "self", ",", "input", ",", "dilation", ",", "init_dilation", ",", "i", ")", ":", "\n", "        ", "x", "=", "dilate", "(", "input", ",", "dilation", ",", "init_dilation", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.wavenet.WaveNetModel.queue_dilate": [[278, 285], ["queue.enqueue", "queue.dequeue"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.wavenet.DilatedQueue.enqueue", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.wavenet.DilatedQueue.dequeue"], ["", "def", "queue_dilate", "(", "self", ",", "input", ",", "dilation", ",", "init_dilation", ",", "i", ")", ":", "\n", "        ", "queue", "=", "self", ".", "dilated_queues", "[", "i", "]", "\n", "queue", ".", "enqueue", "(", "input", ")", "\n", "x", "=", "queue", ".", "dequeue", "(", "num_deq", "=", "self", ".", "kernel_size", ",", "\n", "dilation", "=", "dilation", ")", "\n", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.wavenet.WaveNetModel.forward": [[286, 299], ["input.transpose().contiguous.transpose().contiguous.transpose().contiguous", "wavenet.WaveNetModel.wavenet", "x.transpose().contiguous.transpose().contiguous.transpose().contiguous", "input.transpose().contiguous.transpose().contiguous.transpose", "x.transpose().contiguous.transpose().contiguous.transpose"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.wavenet.WaveNetModel.wavenet"], ["", "def", "forward", "(", "self", ",", "input", ",", "state", "=", "None", ")", ":", "\n", "# BLD -> BDL", "\n", "        ", "input", "=", "input", ".", "transpose", "(", "1", ",", "2", ")", ".", "contiguous", "(", ")", "\n", "\n", "x", "=", "self", ".", "wavenet", "(", "\n", "input", ",", "\n", "dilation_func", "=", "self", ".", "wavenet_dilate", ",", "\n", ")", "\n", "\n", "# reshape output", "\n", "x", "=", "x", ".", "transpose", "(", "1", ",", "2", ")", ".", "contiguous", "(", ")", "\n", "x", "=", "x", "[", ":", ",", "-", "(", "input", ".", "shape", "[", "2", "]", "-", "self", ".", "receptive_field", ")", ":", "]", "\n", "return", "x", ",", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.wavenet.WaveNetModel.step": [[300, 316], ["x.unsqueeze.unsqueeze.transpose().contiguous", "wavenet.WaveNetModel.wavenet", "x.unsqueeze.unsqueeze.transpose().contiguous", "len", "x.unsqueeze.unsqueeze.unsqueeze().unsqueeze", "len", "x.unsqueeze.unsqueeze.unsqueeze", "queue.reset", "x.unsqueeze.unsqueeze.transpose", "x.unsqueeze.unsqueeze.transpose", "x.unsqueeze.unsqueeze.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.wavenet.WaveNetModel.wavenet", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.wavenet.DilatedQueue.reset"], ["", "def", "step", "(", "self", ",", "x", ",", "state", "=", "None", ")", ":", "\n", "        ", "if", "len", "(", "x", ".", "shape", ")", "==", "1", ":", "\n", "            ", "x", "=", "x", ".", "unsqueeze", "(", "1", ")", ".", "unsqueeze", "(", "1", ")", "\n", "", "elif", "len", "(", "x", ".", "shape", ")", "==", "2", ":", "\n", "            ", "x", "=", "x", ".", "unsqueeze", "(", "1", ")", "\n", "\n", "", "if", "state", "is", "None", ":", "\n", "# Reset dilated queues", "\n", "            ", "for", "queue", "in", "self", ".", "dilated_queues", ":", "\n", "                ", "queue", ".", "reset", "(", "device", "=", "x", ".", "device", ")", "\n", "\n", "", "", "x", "=", "x", ".", "transpose", "(", "1", ",", "2", ")", ".", "contiguous", "(", ")", "\n", "x", "=", "self", ".", "wavenet", "(", "x", ",", "dilation_func", "=", "self", ".", "queue_dilate", ")", "\n", "x", "=", "x", ".", "transpose", "(", "1", ",", "2", ")", ".", "contiguous", "(", ")", "\n", "\n", "return", "x", ",", "self", ".", "dilated_queues", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.wavenet.mu_law_expansion": [[16, 19], ["numpy.sign", "numpy.exp", "numpy.abs", "numpy.log"], "function", ["None"], ["def", "mu_law_expansion", "(", "data", ",", "mu", ")", ":", "\n", "    ", "s", "=", "np", ".", "sign", "(", "data", ")", "*", "(", "np", ".", "exp", "(", "np", ".", "abs", "(", "data", ")", "*", "np", ".", "log", "(", "mu", "+", "1", ")", ")", "-", "1", ")", "/", "mu", "\n", "return", "s", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.wavenet.dilate": [[21, 52], ["constant_pad_1d.size", "int", "int", "int", "math.ceil", "math.ceil", "constant_pad_1d.permute().contiguous", "constant_pad_1d.view", "constant_pad_1d.permute().contiguous", "wavenet.constant_pad_1d", "round", "round", "numpy.ceil", "constant_pad_1d.permute", "constant_pad_1d.permute"], "function", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.wavenet.constant_pad_1d"], ["", "def", "dilate", "(", "x", ",", "dilation", ",", "init_dilation", "=", "1", ")", ":", "\n", "    ", "\"\"\"\n    :param x: Tensor of size (N, C, L), where N is the input dilation, C is the number of channels, and L is the input length\n    :param dilation: Target dilation. Will be the size of the first dimension of the output tensor.\n    :param pad_start: If the input length is not compatible with the specified dilation, zero padding is used. This parameter determines wether the zeros are added at the start or at the end.\n    :return: The dilated tensor of size (dilation, C, L*N / dilation). The output might be zero padded at the start\n    \"\"\"", "\n", "\n", "[", "n", ",", "c", ",", "l", "]", "=", "x", ".", "size", "(", ")", "\n", "dilation_factor", "=", "dilation", "/", "init_dilation", "\n", "if", "dilation_factor", "==", "1", ":", "\n", "        ", "return", "x", "\n", "\n", "# zero padding for reshaping", "\n", "", "new_l", "=", "int", "(", "np", ".", "ceil", "(", "l", "/", "dilation_factor", ")", "*", "dilation_factor", ")", "\n", "if", "new_l", "!=", "l", ":", "\n", "        ", "l", "=", "new_l", "\n", "# x = constant_pad_1d(x, new_l, dimension=2, pad_start=pad_start)", "\n", "x", "=", "constant_pad_1d", "(", "x", ",", "new_l", ")", "\n", "\n", "", "l_old", "=", "int", "(", "round", "(", "l", "/", "dilation_factor", ")", ")", "\n", "n_old", "=", "int", "(", "round", "(", "n", "*", "dilation_factor", ")", ")", "\n", "l", "=", "math", ".", "ceil", "(", "l", "*", "init_dilation", "/", "dilation", ")", "\n", "n", "=", "math", ".", "ceil", "(", "n", "*", "dilation", "/", "init_dilation", ")", "\n", "\n", "# reshape according to dilation", "\n", "x", "=", "x", ".", "permute", "(", "1", ",", "2", ",", "0", ")", ".", "contiguous", "(", ")", "# (n, c, l) -> (c, l, n)", "\n", "x", "=", "x", ".", "view", "(", "c", ",", "l", ",", "n", ")", "\n", "x", "=", "x", ".", "permute", "(", "2", ",", "0", ",", "1", ")", ".", "contiguous", "(", ")", "# (c, l, n) -> (n, c, l)", "\n", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.wavenet.constant_pad_1d": [[94, 100], ["torch.nn.ConstantPad1d", "torch.nn.ConstantPad1d", "torch.nn.ConstantPad1d", "torch.nn.ConstantPad1d.", "input.size"], "function", ["None"], ["", "", "def", "constant_pad_1d", "(", "\n", "input", ",", "\n", "target_size", ",", "\n", ")", ":", "\n", "    ", "cp1d", "=", "torch", ".", "nn", ".", "ConstantPad1d", "(", "(", "target_size", "-", "input", ".", "size", "(", "-", "1", ")", ",", "0", ")", ",", "0", ")", "\n", "return", "cp1d", "(", "input", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.baselines.wavenet.test_wavenet": [[317, 346], ["WaveNetModel().cuda", "print", "print", "print", "torch.randn().cuda", "torch.randn().cuda", "torch.randn().cuda", "WaveNetModel().cuda.", "print", "breakpoint", "WaveNetModel().cuda.parameter_count", "torch.no_grad", "torch.no_grad", "torch.no_grad", "range", "torch.stack().squeeze().transpose", "torch.stack().squeeze().transpose", "torch.stack().squeeze().transpose", "wavenet.WaveNetModel", "torch.randn", "torch.randn", "torch.randn", "WaveNetModel().cuda.step", "ys.append", "torch.stack().squeeze", "torch.stack().squeeze", "torch.stack().squeeze", "torch.stack", "torch.stack", "torch.stack"], "function", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.sashimi.sashimi.Sashimi.step"], ["", "", "def", "test_wavenet", "(", ")", ":", "\n", "    ", "wavenet", "=", "WaveNetModel", "(", "\n", "layers", "=", "10", ",", "\n", "blocks", "=", "4", ",", "\n", "dilation_channels", "=", "32", ",", "\n", "residual_channels", "=", "32", ",", "\n", "skip_channels", "=", "256", ",", "\n", "end_channels", "=", "256", ",", "\n", "classes", "=", "256", ",", "\n", "# output_length=16000,", "\n", "kernel_size", "=", "2", ",", "\n", ")", ".", "cuda", "(", ")", "\n", "\n", "print", "(", "wavenet", ")", "\n", "print", "(", "wavenet", ".", "parameter_count", "(", ")", ")", "\n", "print", "(", "wavenet", ".", "receptive_field", ")", "\n", "# BLD", "\n", "x", "=", "torch", ".", "randn", "(", "7", ",", "4093", "+", "16", ",", "256", ")", ".", "cuda", "(", ")", "\n", "y", ",", "_", "=", "wavenet", "(", "x", ")", "\n", "print", "(", "y", ".", "shape", ")", "\n", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "        ", "state", "=", "None", "\n", "ys", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "x", ".", "shape", "[", "1", "]", ")", ":", "\n", "            ", "y_i", ",", "state", "=", "wavenet", ".", "step", "(", "x", "[", ":", ",", "i", ",", ":", "]", ",", "state", ")", "\n", "ys", ".", "append", "(", "y_i", ")", "\n", "", "y_", "=", "torch", ".", "stack", "(", "ys", ")", ".", "squeeze", "(", ")", ".", "transpose", "(", "0", ",", "1", ")", "\n", "", "breakpoint", "(", ")", "\n", "# assert y.shape == (8, 16000, 256)", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.sequence.base.SequenceModule.d_output": [[25, 28], ["None"], "methods", ["None"], ["", "@", "d_output", ".", "setter", "\n", "def", "d_output", "(", "self", ",", "d", ")", ":", "\n", "        ", "self", ".", "_d_output", "=", "d", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.sequence.base.SequenceModule.state_to_tensor": [[29, 33], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "state_to_tensor", "(", "self", ")", ":", "\n", "        ", "\"\"\" Returns a function mapping a state to a single tensor, in case one wants to use the hidden state instead of the output for final prediction \"\"\"", "\n", "return", "lambda", "_", ":", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.sequence.base.SequenceModule.d_state": [[34, 38], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "d_state", "(", "self", ")", ":", "\n", "        ", "\"\"\" Returns dimension of output of self.state_to_tensor \"\"\"", "\n", "return", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.sequence.base.SequenceModule.transposed": [[42, 45], ["None"], "methods", ["None"], ["", "@", "transposed", ".", "setter", "\n", "def", "transposed", "(", "self", ",", "x", ")", ":", "\n", "        ", "self", ".", "_transposed", "=", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.sequence.base.SequenceModule.default_state": [[47, 49], ["None"], "methods", ["None"], ["", "def", "default_state", "(", "self", ",", "*", "batch_shape", ",", "device", "=", "None", ")", ":", "# TODO device shouldn't be needed; models should store their own initial state at initialization", "\n", "        ", "return", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.sequence.base.SequenceModule.step": [[50, 52], ["None"], "methods", ["None"], ["", "def", "step", "(", "self", ",", "x", ",", "state", "=", "None", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "return", "x", ",", "state", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.sequence.base.SequenceModule.forward": [[53, 55], ["None"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ",", "state", "=", "None", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "return", "x", ",", "state", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.sequence.base.SequenceIdentity.__init__": [[74, 77], ["torch.nn.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__"], ["    ", "def", "__init__", "(", "self", ",", "d_model", ",", "dropout", "=", "0.0", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "d_output", "=", "d_model", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.sequence.base.SequenceIdentity.forward": [[78, 80], ["None"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ",", "state", "=", "None", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "return", "x", ",", "state", "\n", "", "", "SequenceIdentity", "=", "Transpose", "(", "SequenceIdentity", ")", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.sequence.base.Transpose": [[56, 72], ["super().__init__", "super().forward", "x.transpose.transpose", "x.transpose.transpose"], "function", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.models.resnext.CifarResNeXt.forward"], ["", "", "def", "Transpose", "(", "module", ")", ":", "\n", "    ", "\"\"\" Wrap a SequenceModule class to transpose the forward pass \"\"\"", "\n", "# TODO maybe possible with functools.wraps", "\n", "class", "WrappedModule", "(", "module", ")", ":", "\n", "        ", "def", "__init__", "(", "self", ",", "*", "args", ",", "transposed", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "            ", "super", "(", ")", ".", "__init__", "(", "*", "args", ",", "**", "kwargs", ")", "\n", "self", ".", "transposed", "=", "transposed", "\n", "\n", "", "def", "forward", "(", "self", ",", "x", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "            ", "if", "self", ".", "transposed", ":", "x", "=", "x", ".", "transpose", "(", "-", "1", ",", "-", "2", ")", "\n", "x", ",", "state", "=", "super", "(", ")", ".", "forward", "(", "x", ")", "\n", "if", "self", ".", "transposed", ":", "x", "=", "x", ".", "transpose", "(", "-", "1", ",", "-", "2", ")", "\n", "return", "x", ",", "state", "\n", "# https://stackoverflow.com/questions/5352781/how-to-set-class-names-dynamically", "\n", "", "", "WrappedModule", ".", "__name__", "=", "module", ".", "__name__", "\n", "return", "WrappedModule", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.sequence.pool.DownSample.__init__": [[53, 60], ["src.models.sequence.SequenceModule.__init__"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__"], ["    ", "def", "__init__", "(", "self", ",", "d_input", ",", "stride", "=", "1", ",", "expand", "=", "1", ",", "transposed", "=", "True", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "d_input", "=", "d_input", "\n", "self", ".", "stride", "=", "stride", "\n", "self", ".", "expand", "=", "expand", "\n", "# self.average = average", "\n", "self", ".", "transposed", "=", "transposed", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.sequence.pool.DownSample.forward": [[61, 63], ["pool.downsample"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.sequence.pool.downsample"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "return", "downsample", "(", "x", ",", "self", ".", "stride", ",", "self", ".", "expand", ",", "False", ",", "self", ".", "transposed", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.sequence.pool.DownSample.step": [[64, 68], ["None"], "methods", ["None"], ["", "def", "step", "(", "self", ",", "x", ",", "state", ",", "**", "kwargs", ")", ":", "\n", "        ", "if", "self", ".", "stride", ">", "1", "or", "self", ".", "expand", ">", "1", ":", "\n", "            ", "raise", "NotImplementedError", "\n", "", "return", "x", ",", "state", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.sequence.pool.DownSample.d_output": [[69, 72], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "d_output", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "d_input", "*", "self", ".", "expand", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.sequence.pool.DownAvgPool.__init__": [[74, 81], ["src.models.sequence.SequenceModule.__init__"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__"], ["    ", "def", "__init__", "(", "self", ",", "d_input", ",", "stride", "=", "1", ",", "expand", "=", "1", ",", "transposed", "=", "True", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "d_input", "=", "d_input", "\n", "self", ".", "stride", "=", "stride", "\n", "self", ".", "expand", "=", "expand", "\n", "# self.average = average", "\n", "self", ".", "transposed", "=", "transposed", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.sequence.pool.DownAvgPool.forward": [[82, 84], ["pool.downsample"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.sequence.pool.downsample"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "return", "downsample", "(", "x", ",", "self", ".", "stride", ",", "self", ".", "expand", ",", "True", ",", "self", ".", "transposed", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.sequence.pool.DownAvgPool.step": [[85, 89], ["None"], "methods", ["None"], ["", "def", "step", "(", "self", ",", "x", ",", "state", ",", "**", "kwargs", ")", ":", "\n", "        ", "if", "self", ".", "stride", ">", "1", "or", "self", ".", "expand", ">", "1", ":", "\n", "            ", "raise", "NotImplementedError", "\n", "", "return", "x", ",", "state", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.sequence.pool.DownAvgPool.d_output": [[90, 93], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "d_output", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "d_input", "*", "self", ".", "expand", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.sequence.pool.UpSample.__init__": [[95, 101], ["torch.nn.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__"], ["    ", "def", "__init__", "(", "self", ",", "d_input", ",", "stride", "=", "1", ",", "expand", "=", "1", ",", "transposed", "=", "True", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "d_input", "=", "d_input", "\n", "self", ".", "stride", "=", "stride", "\n", "self", ".", "expand", "=", "expand", "\n", "self", ".", "transposed", "=", "transposed", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.sequence.pool.UpSample.forward": [[102, 104], ["pool.upsample"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.sequence.pool.upsample"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "return", "upsample", "(", "x", ",", "self", ".", "stride", ",", "self", ".", "expand", ",", "self", ".", "transposed", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.sequence.pool.UpSample.d_output": [[105, 108], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "d_output", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "d_input", "//", "self", ".", "expand", "\n", "", "def", "step", "(", "self", ",", "x", ",", "state", ",", "**", "kwargs", ")", ":", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.sequence.pool.UpSample.step": [[108, 112], ["None"], "methods", ["None"], ["", "def", "step", "(", "self", ",", "x", ",", "state", ",", "**", "kwargs", ")", ":", "\n", "        ", "if", "self", ".", "stride", ">", "1", "or", "self", ".", "expand", ">", "1", ":", "\n", "            ", "raise", "NotImplementedError", "\n", "", "return", "x", ",", "state", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.sequence.pool.DownLinearPool.__init__": [[115, 127], ["src.models.sequence.SequenceModule.__init__", "src.models.nn.LinearActivation"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.s4.LinearActivation"], ["    ", "def", "__init__", "(", "self", ",", "d_input", ",", "stride", "=", "1", ",", "expand", "=", "1", ",", "transposed", "=", "True", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "d_input", "=", "d_input", "\n", "self", ".", "stride", "=", "stride", "\n", "self", ".", "expand", "=", "expand", "\n", "self", ".", "transposed", "=", "transposed", "\n", "\n", "self", ".", "linear", "=", "LinearActivation", "(", "\n", "d_input", "*", "stride", ",", "\n", "d_input", "*", "expand", ",", "\n", "transposed", "=", "transposed", ",", "\n", "# initializer=initializer,", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.sequence.pool.DownLinearPool.forward": [[133, 140], ["pool.DownLinearPool.linear", "einops.rearrange", "einops.rearrange"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "if", "self", ".", "transposed", ":", "\n", "            ", "x", "=", "rearrange", "(", "x", ",", "'... h (l s) -> ... (h s) l'", ",", "s", "=", "self", ".", "stride", ")", "\n", "", "else", ":", "\n", "            ", "x", "=", "rearrange", "(", "x", ",", "'... (l s) h -> ... l (h s)'", ",", "s", "=", "self", ".", "stride", ")", "\n", "", "x", "=", "self", ".", "linear", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.sequence.pool.DownLinearPool.step": [[141, 145], ["None"], "methods", ["None"], ["", "def", "step", "(", "self", ",", "x", ",", "state", ",", "**", "kwargs", ")", ":", "\n", "        ", "if", "self", ".", "stride", ">", "1", "or", "self", ".", "expand", ">", "1", ":", "\n", "            ", "raise", "NotImplementedError", "\n", "", "return", "x", ",", "state", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.sequence.pool.DownLinearPool.d_output": [[146, 149], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "d_output", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "d_input", "*", "self", ".", "expand", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.sequence.pool.DownPool2d.__init__": [[155, 166], ["src.models.sequence.SequenceModule.__init__", "src.models.nn.LinearActivation", "torch.nn.AvgPool2d", "torch.nn.AvgPool2d"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.s4.LinearActivation"], ["    ", "def", "__init__", "(", "self", ",", "d_input", ",", "d_output", ",", "stride", "=", "1", ",", "transposed", "=", "True", ",", "weight_norm", "=", "True", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "linear", "=", "LinearActivation", "(", "\n", "d_input", ",", "\n", "d_output", ",", "\n", "transposed", "=", "transposed", ",", "\n", "weight_norm", "=", "weight_norm", ",", "\n", ")", "\n", "\n", "self", ".", "pool", "=", "nn", ".", "AvgPool2d", "(", "kernel_size", "=", "stride", ",", "stride", "=", "stride", ")", ",", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.sequence.pool.DownPool2d.forward": [[167, 170], ["pool.DownPool2d.pool"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "if", "self", ".", "transposed", ":", "\n", "            ", "x", "=", "self", ".", "pool", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.sequence.pool.DownPool.__init__": [[172, 189], ["src.models.sequence.SequenceModule.__init__", "src.models.nn.LinearActivation"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.s4.LinearActivation"], ["    ", "def", "__init__", "(", "self", ",", "d_input", ",", "d_output", "=", "None", ",", "expand", "=", "None", ",", "stride", "=", "1", ",", "transposed", "=", "True", ",", "weight_norm", "=", "True", ",", "initializer", "=", "None", ",", "activation", "=", "None", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "assert", "(", "d_output", "is", "None", ")", "+", "(", "expand", "is", "None", ")", "==", "1", "\n", "if", "d_output", "is", "None", ":", "d_output", "=", "d_input", "*", "expand", "\n", "\n", "self", ".", "_d_output", "=", "d_output", "\n", "self", ".", "stride", "=", "stride", "\n", "self", ".", "transposed", "=", "transposed", "\n", "\n", "self", ".", "linear", "=", "LinearActivation", "(", "\n", "d_input", "*", "stride", ",", "\n", "d_output", ",", "\n", "transposed", "=", "transposed", ",", "\n", "initializer", "=", "initializer", ",", "\n", "weight_norm", "=", "weight_norm", ",", "\n", "activation", "=", "activation", ",", "\n", "activate", "=", "True", "if", "activation", "is", "not", "None", "else", "False", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.sequence.pool.DownPool.forward": [[191, 198], ["pool.DownPool.linear", "einops.rearrange", "einops.rearrange"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "if", "self", ".", "transposed", ":", "\n", "            ", "x", "=", "rearrange", "(", "x", ",", "'... h (l s) -> ... (h s) l'", ",", "s", "=", "self", ".", "stride", ")", "\n", "", "else", ":", "\n", "            ", "x", "=", "rearrange", "(", "x", ",", "'... (l s) h -> ... l (h s)'", ",", "s", "=", "self", ".", "stride", ")", "\n", "", "x", "=", "self", ".", "linear", "(", "x", ")", "\n", "return", "x", ",", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.sequence.pool.DownPool.step": [[199, 214], ["state.append", "len", "einops.rearrange", "pool.DownPool.linear", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "x.squeeze.squeeze.unsqueeze", "x.squeeze.squeeze.squeeze"], "methods", ["None"], ["", "def", "step", "(", "self", ",", "x", ",", "state", ",", "**", "kwargs", ")", ":", "# TODO needs fix in transpose ca, **kwargsse", "\n", "        ", "\"\"\"\n        x: (..., H)\n        \"\"\"", "\n", "\n", "if", "x", "is", "None", ":", "return", "None", ",", "state", "\n", "state", ".", "append", "(", "x", ")", "\n", "if", "len", "(", "state", ")", "==", "self", ".", "stride", ":", "\n", "            ", "x", "=", "rearrange", "(", "torch", ".", "stack", "(", "state", ",", "dim", "=", "-", "1", ")", ",", "'... h s -> ... (h s)'", ")", "\n", "if", "self", ".", "transposed", ":", "x", "=", "x", ".", "unsqueeze", "(", "-", "1", ")", "\n", "x", "=", "self", ".", "linear", "(", "x", ")", "\n", "if", "self", ".", "transposed", ":", "x", "=", "x", ".", "squeeze", "(", "-", "1", ")", "\n", "return", "x", ",", "[", "]", "\n", "", "else", ":", "\n", "            ", "return", "None", ",", "state", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.sequence.pool.DownPool.default_state": [[215, 217], ["None"], "methods", ["None"], ["", "", "def", "default_state", "(", "self", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "return", "[", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.sequence.pool.DownPool.d_output": [[218, 220], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "d_output", "(", "self", ")", ":", "return", "self", ".", "_d_output", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.sequence.pool.UpPool.__init__": [[223, 239], ["src.models.sequence.SequenceModule.__init__", "src.models.nn.LinearActivation"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.s4.LinearActivation"], ["    ", "def", "__init__", "(", "self", ",", "d_input", ",", "d_output", ",", "stride", ",", "transposed", "=", "True", ",", "weight_norm", "=", "True", ",", "initializer", "=", "None", ",", "activation", "=", "None", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "d_input", "=", "d_input", "\n", "self", ".", "_d_output", "=", "d_output", "\n", "self", ".", "stride", "=", "stride", "\n", "self", ".", "transposed", "=", "transposed", "\n", "\n", "self", ".", "linear", "=", "LinearActivation", "(", "\n", "d_input", ",", "\n", "d_output", "*", "stride", ",", "\n", "transposed", "=", "transposed", ",", "\n", "initializer", "=", "initializer", ",", "\n", "weight_norm", "=", "weight_norm", ",", "\n", "activation", "=", "activation", ",", "\n", "activate", "=", "True", "if", "activation", "is", "not", "None", "else", "False", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.sequence.pool.UpPool.forward": [[241, 252], ["pool.UpPool.linear", "torch.pad", "torch.pad", "einops.rearrange", "torch.pad", "torch.pad", "einops.rearrange"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.sc.pad", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.sc.pad", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.sc.pad", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.sc.pad"], ["", "def", "forward", "(", "self", ",", "x", ",", "skip", "=", "None", ")", ":", "\n", "        ", "x", "=", "self", ".", "linear", "(", "x", ")", "\n", "if", "self", ".", "transposed", ":", "\n", "            ", "x", "=", "F", ".", "pad", "(", "x", "[", "...", ",", ":", "-", "1", "]", ",", "(", "1", ",", "0", ")", ")", "# Shift to ensure causality", "\n", "x", "=", "rearrange", "(", "x", ",", "'... (h s) l -> ... h (l s)'", ",", "s", "=", "self", ".", "stride", ")", "\n", "", "else", ":", "\n", "            ", "x", "=", "F", ".", "pad", "(", "x", "[", "...", ",", ":", "-", "1", ",", ":", "]", ",", "(", "0", ",", "0", ",", "1", ",", "0", ")", ")", "# Shift to ensure causality", "\n", "x", "=", "rearrange", "(", "x", ",", "'... l (h s) -> ... (l s) h'", ",", "s", "=", "self", ".", "stride", ")", "\n", "", "if", "skip", "is", "not", "None", ":", "\n", "            ", "x", "=", "x", "+", "skip", "\n", "", "return", "x", ",", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.sequence.pool.UpPool.step": [[253, 269], ["len", "len", "pool.UpPool.linear", "einops.rearrange", "list", "x.squeeze.squeeze.unsqueeze", "x.squeeze.squeeze.squeeze", "torch.unbind", "torch.unbind", "torch.unbind", "torch.unbind"], "methods", ["None"], ["", "def", "step", "(", "self", ",", "x", ",", "state", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"\n        x: (..., H)\n        \"\"\"", "\n", "\n", "assert", "len", "(", "state", ")", ">", "0", "\n", "y", ",", "state", "=", "state", "[", "0", "]", ",", "state", "[", "1", ":", "]", "\n", "if", "len", "(", "state", ")", "==", "0", ":", "\n", "            ", "assert", "x", "is", "not", "None", "\n", "if", "self", ".", "transposed", ":", "x", "=", "x", ".", "unsqueeze", "(", "-", "1", ")", "\n", "x", "=", "self", ".", "linear", "(", "x", ")", "\n", "if", "self", ".", "transposed", ":", "x", "=", "x", ".", "squeeze", "(", "-", "1", ")", "\n", "x", "=", "rearrange", "(", "x", ",", "'... (h s) -> ... h s'", ",", "s", "=", "self", ".", "stride", ")", "\n", "state", "=", "list", "(", "torch", ".", "unbind", "(", "x", ",", "dim", "=", "-", "1", ")", ")", "\n", "", "else", ":", "assert", "x", "is", "None", "\n", "return", "y", ",", "state", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.sequence.pool.UpPool.default_state": [[270, 274], ["torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "list", "torch.unbind", "torch.unbind", "torch.unbind", "torch.unbind"], "methods", ["None"], ["", "def", "default_state", "(", "self", ",", "*", "batch_shape", ",", "device", "=", "None", ")", ":", "\n", "        ", "state", "=", "torch", ".", "zeros", "(", "batch_shape", "+", "(", "self", ".", "d_output", ",", "self", ".", "stride", ")", ",", "device", "=", "device", ")", "# (batch, h, s)", "\n", "state", "=", "list", "(", "torch", ".", "unbind", "(", "state", ",", "dim", "=", "-", "1", ")", ")", "# List of (..., H)", "\n", "return", "state", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.sequence.pool.UpPool.d_output": [[275, 277], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "d_output", "(", "self", ")", ":", "return", "self", ".", "_d_output", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.sequence.pool.downsample": [[17, 37], ["einops.repeat", "einops.repeat", "torch.avg_pool1d", "einops.reduce"], "function", ["None"], ["def", "downsample", "(", "x", ",", "stride", "=", "1", ",", "expand", "=", "1", ",", "average", "=", "False", ",", "transposed", "=", "False", ")", ":", "\n", "    ", "if", "x", "is", "None", ":", "return", "None", "\n", "if", "stride", ">", "1", ":", "\n", "# TODO higher dimension stuff", "\n", "        ", "if", "transposed", ":", "\n", "# einops appears slower than F", "\n", "# if average: x = reduce(x, '... (l s) -> ... l', 'mean', s=stride)", "\n", "            ", "if", "average", ":", "x", "=", "F", ".", "avg_pool1d", "(", "x", ",", "stride", ",", "stride", ")", "\n", "else", ":", "x", "=", "x", "[", "...", ",", "0", ":", ":", "stride", "]", "\n", "", "else", ":", "\n", "            ", "if", "average", ":", "x", "=", "reduce", "(", "x", ",", "'... (l s) h -> ... l h'", ",", "'mean'", ",", "s", "=", "stride", ")", "\n", "else", ":", "x", "=", "x", "[", "...", ",", "0", ":", ":", "stride", ",", ":", "]", "\n", "\n", "", "", "if", "expand", ">", "1", ":", "\n", "        ", "if", "transposed", ":", "\n", "            ", "x", "=", "repeat", "(", "x", ",", "'... d l -> ... (d e) l'", ",", "e", "=", "expand", ")", "\n", "", "else", ":", "\n", "            ", "x", "=", "repeat", "(", "x", ",", "'... d -> ... (d e)'", ",", "e", "=", "expand", ")", "\n", "\n", "", "", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.sequence.pool.upsample": [[38, 51], ["einops.reduce", "einops.reduce", "einops.repeat", "einops.repeat"], "function", ["None"], ["", "def", "upsample", "(", "x", ",", "stride", "=", "1", ",", "expand", "=", "1", ",", "transposed", "=", "False", ")", ":", "\n", "    ", "if", "x", "is", "None", ":", "return", "None", "\n", "if", "expand", ">", "1", ":", "\n", "        ", "if", "transposed", ":", "\n", "            ", "x", "=", "reduce", "(", "x", ",", "'... (d e) l -> ... d l'", ",", "'mean'", ",", "e", "=", "expand", ")", "\n", "", "else", ":", "\n", "            ", "x", "=", "reduce", "(", "x", ",", "'... (d e) -> ... d'", ",", "'mean'", ",", "e", "=", "expand", ")", "\n", "", "", "if", "stride", ">", "1", ":", "\n", "        ", "if", "transposed", ":", "\n", "            ", "x", "=", "repeat", "(", "x", ",", "'... l -> ... (l e)'", ",", "e", "=", "stride", ")", "\n", "", "else", ":", "\n", "            ", "x", "=", "repeat", "(", "x", ",", "'... l d -> ... (l e) d'", ",", "e", "=", "stride", ")", "\n", "", "", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.sequence.sashimi.Sashimi.__init__": [[12, 124], ["src.models.sequence.base.SequenceModule.__init__", "layer.copy", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "range", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.LayerNorm", "torch.LayerNorm", "torch.LayerNorm", "src.models.sequence.block.SequenceResidualBlock", "d_layers.append", "c_layers.append", "block.append", "range", "u_layers.append", "range", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "src.models.sequence.pool.DownPool", "sashimi.Sashimi.__init__._residual"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "d_model", ",", "\n", "n_layers", ",", "\n", "pool", "=", "[", "]", ",", "\n", "expand", "=", "1", ",", "\n", "ff", "=", "2", ",", "\n", "prenorm", "=", "False", ",", "\n", "dropout", "=", "0.0", ",", "\n", "dropres", "=", "0.0", ",", "\n", "layer", "=", "None", ",", "\n", "residual", "=", "None", ",", "\n", "norm", "=", "None", ",", "\n", "initializer", "=", "None", ",", "\n", "l_max", "=", "-", "1", ",", "\n", "transposed", "=", "True", ",", "\n", "interp", "=", "0", ",", "\n", "act_pool", "=", "None", ",", "\n", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "assert", "l_max", ">", "0", ",", "\"SaShiMi must have length passed in\"", "\n", "\n", "self", ".", "d_model", "=", "d_model", "\n", "H", "=", "d_model", "\n", "\n", "self", ".", "interp", "=", "interp", "\n", "if", "interp", ">", "0", ":", "\n", "            ", "assert", "l_max", "%", "interp", "==", "0", ",", "\"Interpolation level must be a factor of the length\"", "\n", "l_max", "=", "l_max", "//", "interp", "\n", "\n", "", "L", "=", "l_max", "\n", "self", ".", "L", "=", "L", "\n", "self", ".", "transposed", "=", "transposed", "\n", "\n", "# Layer arguments", "\n", "layer_cfg", "=", "layer", ".", "copy", "(", ")", "\n", "layer_cfg", "[", "'dropout'", "]", "=", "dropout", "\n", "layer_cfg", "[", "'transposed'", "]", "=", "self", ".", "transposed", "\n", "# layer_cfg['initializer'] = initializer", "\n", "layer_cfg", "[", "'l_max'", "]", "=", "L", "\n", "\n", "ff_cfg", "=", "{", "\n", "'_name_'", ":", "'ff'", ",", "\n", "'expand'", ":", "ff", ",", "\n", "'transposed'", ":", "self", ".", "transposed", ",", "\n", "'activation'", ":", "'gelu'", ",", "\n", "'initializer'", ":", "initializer", ",", "\n", "'dropout'", ":", "dropout", ",", "\n", "}", "\n", "\n", "def", "_residual", "(", "d", ",", "i", ",", "layer", ")", ":", "\n", "            ", "return", "SequenceResidualBlock", "(", "\n", "d", ",", "\n", "i", ",", "\n", "prenorm", "=", "prenorm", ",", "\n", "dropout", "=", "dropres", ",", "\n", "layer", "=", "layer", ",", "\n", "residual", "=", "residual", "if", "residual", "is", "not", "None", "else", "'R'", ",", "\n", "norm", "=", "norm", ",", "\n", "pool", "=", "None", ",", "\n", ")", "\n", "\n", "# Down blocks", "\n", "", "d_layers", "=", "[", "]", "\n", "for", "p", "in", "pool", ":", "\n", "# Add sequence downsampling and feature expanding", "\n", "            ", "d_layers", ".", "append", "(", "DownPool", "(", "H", ",", "H", "*", "expand", ",", "stride", "=", "p", ",", "transposed", "=", "self", ".", "transposed", ",", "activation", "=", "act_pool", ")", ")", "\n", "L", "//=", "p", "\n", "layer_cfg", "[", "'l_max'", "]", "=", "L", "\n", "H", "*=", "expand", "\n", "", "self", ".", "d_layers", "=", "nn", ".", "ModuleList", "(", "d_layers", ")", "\n", "\n", "# Center block", "\n", "c_layers", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "n_layers", ")", ":", "\n", "            ", "c_layers", ".", "append", "(", "_residual", "(", "H", ",", "i", "+", "1", ",", "layer_cfg", ")", ")", "\n", "if", "ff", ">", "0", ":", "c_layers", ".", "append", "(", "_residual", "(", "H", ",", "i", "+", "1", ",", "ff_cfg", ")", ")", "\n", "", "self", ".", "c_layers", "=", "nn", ".", "ModuleList", "(", "c_layers", ")", "\n", "\n", "# Up blocks", "\n", "u_layers", "=", "[", "]", "\n", "for", "p", "in", "pool", "[", ":", ":", "-", "1", "]", ":", "\n", "            ", "block", "=", "[", "]", "\n", "H", "//=", "expand", "\n", "L", "*=", "p", "\n", "layer_cfg", "[", "'l_max'", "]", "=", "L", "\n", "block", ".", "append", "(", "UpPool", "(", "H", "*", "expand", ",", "H", ",", "stride", "=", "p", ",", "transposed", "=", "self", ".", "transposed", ",", "activation", "=", "act_pool", ")", ")", "\n", "\n", "for", "i", "in", "range", "(", "n_layers", ")", ":", "\n", "                ", "block", ".", "append", "(", "_residual", "(", "H", ",", "i", "+", "1", ",", "layer_cfg", ")", ")", "\n", "if", "ff", ">", "0", ":", "block", ".", "append", "(", "_residual", "(", "H", ",", "i", "+", "1", ",", "ff_cfg", ")", ")", "\n", "\n", "", "u_layers", ".", "append", "(", "nn", ".", "ModuleList", "(", "block", ")", ")", "\n", "\n", "", "self", ".", "u_layers", "=", "nn", ".", "ModuleList", "(", "u_layers", ")", "\n", "\n", "assert", "H", "==", "d_model", "\n", "\n", "self", ".", "norm", "=", "nn", ".", "LayerNorm", "(", "H", ")", "\n", "\n", "if", "interp", ">", "0", ":", "\n", "            ", "interp_layers", "=", "[", "]", "\n", "assert", "interp", "%", "2", "==", "0", "\n", "for", "i", "in", "range", "(", "int", "(", "math", ".", "log2", "(", "interp", ")", ")", ")", ":", "\n", "                ", "block", "=", "[", "]", "\n", "for", "j", "in", "range", "(", "2", ")", ":", "\n", "                    ", "block", ".", "append", "(", "_residual", "(", "H", ",", "i", "+", "1", ",", "layer_cfg", ")", ")", "\n", "if", "ff", ">", "0", ":", "block", ".", "append", "(", "_residual", "(", "H", ",", "i", "+", "1", ",", "ff_cfg", ")", ")", "\n", "\n", "", "interp_layers", ".", "append", "(", "nn", ".", "ModuleList", "(", "block", ")", ")", "\n", "\n", "", "self", ".", "interp_layers", "=", "nn", ".", "ModuleList", "(", "interp_layers", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.sequence.sashimi.Sashimi.d_output": [[125, 128], ["None"], "methods", ["None"], ["", "", "@", "property", "\n", "def", "d_output", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "d_model", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.sequence.sashimi.Sashimi.forward": [[129, 188], ["outputs.append", "sashimi.Sashimi.norm", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "x.transpose.transpose.transpose", "layer", "outputs.append", "layer", "outputs.pop", "x.transpose.transpose.transpose", "torch.pad", "torch.pad", "torch.pad", "int", "layer", "isinstance", "outputs.pop", "z.transpose.transpose.transpose", "layer", "z.transpose.transpose.transpose", "outputs.append", "outputs.pop"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.sc.pad", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.sc.pad", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.sc.pad"], ["", "def", "forward", "(", "self", ",", "x", ",", "state", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        input: (batch, length, d_input)\n        output: (batch, length, d_output)\n        \"\"\"", "\n", "if", "self", ".", "interp", ">", "0", ":", "\n", "# Interpolation will be used to reconstruct \"missing\" frames", "\n", "# Subsample the input sequence and run the SNet on that", "\n", "            ", "x_all", "=", "x", "\n", "x", "=", "x", "[", ":", ",", ":", ":", "self", ".", "interp", ",", ":", "]", "\n", "\n", "y", "=", "torch", ".", "zeros_like", "(", "x_all", ")", "\n", "# Run the interpolating layers", "\n", "interp_level", "=", "self", ".", "interp", "\n", "for", "block", "in", "self", ".", "interp_layers", ":", "\n", "# Pad to the right and discard the output of the first input", "\n", "# (creates dependence on the next time step for interpolation)", "\n", "                ", "z", "=", "x_all", "[", ":", ",", ":", ":", "interp_level", ",", ":", "]", "\n", "if", "self", ".", "transposed", ":", "z", "=", "z", ".", "transpose", "(", "1", ",", "2", ")", "\n", "for", "layer", "in", "block", ":", "\n", "                    ", "z", ",", "_", "=", "layer", "(", "z", ")", "\n", "\n", "", "z", "=", "F", ".", "pad", "(", "z", "[", ":", ",", ":", ",", "1", ":", "]", ",", "(", "0", ",", "1", ")", ",", "mode", "=", "'replicate'", ")", "\n", "if", "self", ".", "transposed", ":", "z", "=", "z", ".", "transpose", "(", "1", ",", "2", ")", "\n", "y", "[", ":", ",", "interp_level", "//", "2", "-", "1", ":", ":", "interp_level", ",", ":", "]", "+=", "z", "\n", "interp_level", "=", "int", "(", "interp_level", "//", "2", ")", "\n", "\n", "", "", "if", "self", ".", "transposed", ":", "x", "=", "x", ".", "transpose", "(", "1", ",", "2", ")", "\n", "\n", "# Down blocks", "\n", "outputs", "=", "[", "]", "\n", "outputs", ".", "append", "(", "x", ")", "\n", "for", "layer", "in", "self", ".", "d_layers", ":", "\n", "            ", "x", ",", "_", "=", "layer", "(", "x", ")", "\n", "outputs", ".", "append", "(", "x", ")", "\n", "\n", "# Center block", "\n", "", "for", "layer", "in", "self", ".", "c_layers", ":", "\n", "            ", "x", ",", "_", "=", "layer", "(", "x", ")", "\n", "", "x", "=", "x", "+", "outputs", ".", "pop", "(", ")", "# add a skip connection to the last output of the down block", "\n", "\n", "for", "block", "in", "self", ".", "u_layers", ":", "\n", "            ", "for", "layer", "in", "block", ":", "\n", "                ", "x", ",", "_", "=", "layer", "(", "x", ")", "\n", "if", "isinstance", "(", "layer", ",", "UpPool", ")", ":", "\n", "# Before modeling layer in the block", "\n", "                    ", "x", "=", "x", "+", "outputs", ".", "pop", "(", ")", "\n", "outputs", ".", "append", "(", "x", ")", "\n", "", "", "x", "=", "x", "+", "outputs", ".", "pop", "(", ")", "# add a skip connection from the input of the modeling part of this up block", "\n", "\n", "# feature projection", "\n", "", "if", "self", ".", "transposed", ":", "x", "=", "x", ".", "transpose", "(", "1", ",", "2", ")", "# (batch, length, expand)", "\n", "x", "=", "self", ".", "norm", "(", "x", ")", "\n", "\n", "if", "self", ".", "interp", ">", "0", ":", "\n", "            ", "y", "[", ":", ",", "self", ".", "interp", "-", "1", ":", ":", "self", ".", "interp", ",", ":", "]", "=", "x", "\n", "x", "=", "y", "\n", "\n", "", "return", "x", ",", "None", "# required to return a state", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.sequence.sashimi.Sashimi.default_state": [[189, 193], ["layer.default_state", "list", "list"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.sashimi.sashimi.Sashimi.default_state"], ["", "def", "default_state", "(", "self", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\" x: (batch) \"\"\"", "\n", "layers", "=", "list", "(", "self", ".", "d_layers", ")", "+", "list", "(", "self", ".", "c_layers", ")", "+", "[", "layer", "for", "block", "in", "self", ".", "u_layers", "for", "layer", "in", "block", "]", "\n", "return", "[", "layer", ".", "default_state", "(", "*", "args", ",", "**", "kwargs", ")", "for", "layer", "in", "layers", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.sequence.sashimi.Sashimi.step": [[194, 242], ["sashimi.Sashimi.norm", "outputs.append", "layer.step", "next_state.append", "range", "range", "outputs.append", "len", "len", "next_state.append", "range", "list", "layer.step", "next_state.append", "outputs.pop", "layer.step", "next_state.append", "isinstance", "outputs.pop", "state.pop", "len", "state.pop", "len", "next_state.append", "outputs.append", "state.pop", "state.pop", "state.pop", "outputs.pop"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.sashimi.sashimi.Sashimi.step", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.sashimi.sashimi.Sashimi.step", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.sashimi.sashimi.Sashimi.step"], ["", "def", "step", "(", "self", ",", "x", ",", "state", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"\n        input: (batch, d_input)\n        output: (batch, d_output)\n        \"\"\"", "\n", "# States will be popped in reverse order for convenience", "\n", "state", "=", "state", "[", ":", ":", "-", "1", "]", "\n", "\n", "# Down blocks", "\n", "outputs", "=", "[", "]", "# Store all layers for SaShiMi", "\n", "next_state", "=", "[", "]", "\n", "for", "layer", "in", "self", ".", "d_layers", ":", "\n", "            ", "outputs", ".", "append", "(", "x", ")", "\n", "x", ",", "_next_state", "=", "layer", ".", "step", "(", "x", ",", "state", "=", "state", ".", "pop", "(", ")", ",", "**", "kwargs", ")", "\n", "next_state", ".", "append", "(", "_next_state", ")", "\n", "if", "x", "is", "None", ":", "break", "\n", "\n", "# Center block", "\n", "", "if", "x", "is", "None", ":", "\n", "# Skip computations since we've downsized", "\n", "            ", "skipped", "=", "len", "(", "self", ".", "d_layers", ")", "-", "len", "(", "outputs", ")", "\n", "for", "_", "in", "range", "(", "skipped", "+", "len", "(", "self", ".", "c_layers", ")", ")", ":", "\n", "                ", "next_state", ".", "append", "(", "state", ".", "pop", "(", ")", ")", "\n", "", "for", "i", "in", "range", "(", "skipped", ")", ":", "\n", "                ", "for", "_", "in", "range", "(", "len", "(", "self", ".", "u_layers", "[", "i", "]", ")", ")", ":", "\n", "                    ", "next_state", ".", "append", "(", "state", ".", "pop", "(", ")", ")", "\n", "", "", "u_layers", "=", "list", "(", "self", ".", "u_layers", ")", "[", "skipped", ":", "]", "\n", "", "else", ":", "\n", "            ", "outputs", ".", "append", "(", "x", ")", "\n", "for", "layer", "in", "self", ".", "c_layers", ":", "\n", "                ", "x", ",", "_next_state", "=", "layer", ".", "step", "(", "x", ",", "state", "=", "state", ".", "pop", "(", ")", ",", "**", "kwargs", ")", "\n", "next_state", ".", "append", "(", "_next_state", ")", "\n", "", "x", "=", "x", "+", "outputs", ".", "pop", "(", ")", "\n", "u_layers", "=", "self", ".", "u_layers", "\n", "\n", "", "for", "block", "in", "u_layers", ":", "\n", "            ", "for", "layer", "in", "block", ":", "\n", "                ", "x", ",", "_next_state", "=", "layer", ".", "step", "(", "x", ",", "state", "=", "state", ".", "pop", "(", ")", ",", "**", "kwargs", ")", "\n", "next_state", ".", "append", "(", "_next_state", ")", "\n", "if", "isinstance", "(", "layer", ",", "UpPool", ")", ":", "\n", "# Before modeling layer in the block", "\n", "                    ", "x", "=", "x", "+", "outputs", ".", "pop", "(", ")", "\n", "outputs", ".", "append", "(", "x", ")", "\n", "", "", "x", "=", "x", "+", "outputs", ".", "pop", "(", ")", "\n", "\n", "# feature projection", "\n", "", "x", "=", "self", ".", "norm", "(", "x", ")", "\n", "return", "x", ",", "next_state", "\n", "", "", ""]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.sequence.unet.SequenceUNet.__init__": [[26, 113], ["src.models.sequence.base.SequenceModule.__init__", "layer.copy", "print", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "range", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.LayerNorm", "torch.LayerNorm", "torch.LayerNorm", "src.models.sequence.block.SequenceResidualBlock", "range", "d_layers.append", "c_layers.append", "u_layers.append", "range", "d_layers.append", "src.models.sequence.pool.DownPool", "unet.SequenceUNet.__init__._residual"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "d_model", ",", "n_layers", ",", "pool", "=", "[", "]", ",", "expand", "=", "1", ",", "ff", "=", "2", ",", "cff", "=", "0", ",", "\n", "prenorm", "=", "False", ",", "\n", "dropout", "=", "0.0", ",", "\n", "dropres", "=", "0.0", ",", "\n", "layer", "=", "None", ",", "\n", "residual", "=", "None", ",", "\n", "norm", "=", "None", ",", "\n", "initializer", "=", "None", ",", "\n", "l_max", "=", "-", "1", ",", "\n", "transposed", "=", "True", ",", "\n", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "d_model", "=", "d_model", "\n", "H", "=", "d_model", "\n", "L", "=", "l_max", "\n", "self", ".", "L", "=", "L", "\n", "self", ".", "transposed", "=", "transposed", "\n", "assert", "l_max", ">", "0", ",", "\"UNet must have length passed in\"", "\n", "\n", "# Layer arguments", "\n", "layer_cfg", "=", "layer", ".", "copy", "(", ")", "\n", "layer_cfg", "[", "'dropout'", "]", "=", "dropout", "\n", "layer_cfg", "[", "'transposed'", "]", "=", "self", ".", "transposed", "\n", "layer_cfg", "[", "'initializer'", "]", "=", "initializer", "\n", "layer_cfg", "[", "'l_max'", "]", "=", "L", "\n", "print", "(", "\"layer config\"", ",", "layer_cfg", ")", "\n", "\n", "ff_cfg", "=", "{", "\n", "'_name_'", ":", "'ff'", ",", "\n", "'expand'", ":", "ff", ",", "\n", "'transposed'", ":", "self", ".", "transposed", ",", "\n", "'activation'", ":", "'gelu'", ",", "\n", "'initializer'", ":", "initializer", ",", "# TODO", "\n", "'dropout'", ":", "dropout", ",", "# TODO untie dropout", "\n", "}", "\n", "\n", "def", "_residual", "(", "d", ",", "i", ",", "layer", ")", ":", "\n", "            ", "return", "SequenceResidualBlock", "(", "\n", "d", ",", "\n", "i", ",", "# temporary placeholder for i_layer", "\n", "prenorm", "=", "prenorm", ",", "\n", "dropout", "=", "dropres", ",", "\n", "layer", "=", "layer", ",", "\n", "residual", "=", "residual", "if", "residual", "is", "not", "None", "else", "'R'", ",", "\n", "norm", "=", "norm", ",", "\n", "pool", "=", "None", ",", "\n", ")", "\n", "\n", "# Down blocks", "\n", "", "d_layers", "=", "[", "]", "\n", "for", "p", "in", "pool", ":", "\n", "            ", "for", "i", "in", "range", "(", "n_layers", ")", ":", "\n", "                ", "d_layers", ".", "append", "(", "_residual", "(", "H", ",", "i", "+", "1", ",", "layer_cfg", ")", ")", "\n", "if", "ff", ">", "0", ":", "d_layers", ".", "append", "(", "_residual", "(", "H", ",", "i", "+", "1", ",", "ff_cfg", ")", ")", "\n", "\n", "# Add sequence downsampling and feature expanding", "\n", "", "d_layers", ".", "append", "(", "DownPool", "(", "H", ",", "H", "*", "expand", ",", "pool", "=", "p", ",", "transposed", "=", "self", ".", "transposed", ")", ")", "# TODO take expansion argument instead", "\n", "L", "//=", "p", "\n", "layer_cfg", "[", "'l_max'", "]", "=", "L", "\n", "H", "*=", "expand", "\n", "", "self", ".", "d_layers", "=", "nn", ".", "ModuleList", "(", "d_layers", ")", "\n", "\n", "# Center block", "\n", "c_layers", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "n_layers", ")", ":", "\n", "            ", "c_layers", ".", "append", "(", "_residual", "(", "H", ",", "i", "+", "1", ",", "layer_cfg", ")", ")", "\n", "if", "cff", ">", "0", ":", "c_layers", ".", "append", "(", "_residual", "(", "H", ",", "i", "+", "1", ",", "ff_cfg", ")", ")", "\n", "", "self", ".", "c_layers", "=", "nn", ".", "ModuleList", "(", "c_layers", ")", "\n", "\n", "# Up blocks", "\n", "u_layers", "=", "[", "]", "\n", "for", "p", "in", "pool", "[", ":", ":", "-", "1", "]", ":", "\n", "            ", "H", "//=", "expand", "\n", "L", "*=", "p", "\n", "layer_cfg", "[", "'l_max'", "]", "=", "L", "\n", "u_layers", ".", "append", "(", "UpPool", "(", "H", "*", "expand", ",", "H", ",", "pool", "=", "p", ",", "transposed", "=", "self", ".", "transposed", ")", ")", "# TODO", "\n", "\n", "for", "i", "in", "range", "(", "n_layers", ")", ":", "\n", "                ", "u_layers", ".", "append", "(", "_residual", "(", "H", ",", "i", "+", "1", ",", "layer_cfg", ")", ")", "\n", "if", "ff", ">", "0", ":", "u_layers", ".", "append", "(", "_residual", "(", "H", ",", "i", "+", "1", ",", "ff_cfg", ")", ")", "\n", "", "", "self", ".", "u_layers", "=", "nn", ".", "ModuleList", "(", "u_layers", ")", "\n", "\n", "assert", "H", "==", "d_model", "\n", "\n", "self", ".", "norm", "=", "nn", ".", "LayerNorm", "(", "H", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.sequence.unet.SequenceUNet.d_output": [[117, 120], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "d_output", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "d_model", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.sequence.unet.SequenceUNet.forward": [[121, 149], ["outputs.append", "unet.SequenceUNet.norm", "x.transpose.transpose.transpose", "outputs.append", "layer", "layer", "outputs.pop", "layer", "x.transpose.transpose.transpose", "outputs.pop"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ",", "state", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        input: (batch, length, d_input)\n        output: (batch, length, d_output)\n        \"\"\"", "\n", "if", "self", ".", "transposed", ":", "x", "=", "x", ".", "transpose", "(", "1", ",", "2", ")", "\n", "\n", "# Down blocks", "\n", "outputs", "=", "[", "]", "# Store all layers for SequenceUNet structure", "\n", "for", "layer", "in", "self", ".", "d_layers", ":", "\n", "            ", "outputs", ".", "append", "(", "x", ")", "\n", "x", ",", "_", "=", "layer", "(", "x", ")", "\n", "\n", "# Center block", "\n", "", "outputs", ".", "append", "(", "x", ")", "\n", "for", "layer", "in", "self", ".", "c_layers", ":", "\n", "            ", "x", ",", "_", "=", "layer", "(", "x", ")", "\n", "", "x", "=", "x", "+", "outputs", ".", "pop", "(", ")", "\n", "\n", "for", "layer", "in", "self", ".", "u_layers", ":", "\n", "            ", "x", ",", "_", "=", "layer", "(", "x", ")", "\n", "x", "=", "x", "+", "outputs", ".", "pop", "(", ")", "\n", "\n", "# feature projection", "\n", "", "if", "self", ".", "transposed", ":", "x", "=", "x", ".", "transpose", "(", "1", ",", "2", ")", "# (batch, length, expand)", "\n", "x", "=", "self", ".", "norm", "(", "x", ")", "\n", "\n", "return", "x", ",", "None", "# required to return a state", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.sequence.unet.SequenceUNet.default_state": [[150, 154], ["list", "layer.default_state", "list", "list"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.sashimi.sashimi.Sashimi.default_state"], ["", "def", "default_state", "(", "self", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\" x: (batch) \"\"\"", "\n", "layers", "=", "list", "(", "self", ".", "d_layers", ")", "+", "list", "(", "self", ".", "c_layers", ")", "+", "list", "(", "self", ".", "u_layers", ")", "\n", "return", "[", "layer", ".", "default_state", "(", "*", "args", ",", "**", "kwargs", ")", "for", "layer", "in", "layers", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.sequence.unet.SequenceUNet.step": [[155, 195], ["unet.SequenceUNet.norm", "outputs.append", "layer.step", "next_state.append", "range", "outputs.append", "layer.step", "next_state.append", "len", "len", "next_state.append", "list", "layer.step", "next_state.append", "outputs.pop", "outputs.pop", "state.pop", "state.pop", "state.pop", "len", "state.pop"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.sashimi.sashimi.Sashimi.step", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.sashimi.sashimi.Sashimi.step", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.sashimi.sashimi.Sashimi.step"], ["", "def", "step", "(", "self", ",", "x", ",", "state", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"\n        input: (batch, d_input)\n        output: (batch, d_output)\n        \"\"\"", "\n", "# States will be popped in reverse order for convenience", "\n", "state", "=", "state", "[", ":", ":", "-", "1", "]", "\n", "\n", "# Down blocks", "\n", "outputs", "=", "[", "]", "# Store all layers for SequenceUNet structure", "\n", "next_state", "=", "[", "]", "\n", "for", "layer", "in", "self", ".", "d_layers", ":", "\n", "            ", "outputs", ".", "append", "(", "x", ")", "\n", "x", ",", "_next_state", "=", "layer", ".", "step", "(", "x", ",", "state", "=", "state", ".", "pop", "(", ")", ",", "**", "kwargs", ")", "\n", "next_state", ".", "append", "(", "_next_state", ")", "\n", "if", "x", "is", "None", ":", "break", "\n", "\n", "# Center block", "\n", "", "if", "x", "is", "None", ":", "\n", "# Skip computations since we've downsized", "\n", "            ", "skipped", "=", "len", "(", "self", ".", "d_layers", ")", "-", "len", "(", "outputs", ")", "\n", "for", "_", "in", "range", "(", "skipped", "+", "len", "(", "self", ".", "c_layers", ")", "+", "skipped", ")", ":", "\n", "                ", "next_state", ".", "append", "(", "state", ".", "pop", "(", ")", ")", "\n", "", "u_layers", "=", "list", "(", "self", ".", "u_layers", ")", "[", "skipped", ":", "]", "\n", "", "else", ":", "\n", "            ", "outputs", ".", "append", "(", "x", ")", "\n", "for", "layer", "in", "self", ".", "c_layers", ":", "\n", "                ", "x", ",", "_next_state", "=", "layer", ".", "step", "(", "x", ",", "state", "=", "state", ".", "pop", "(", ")", ",", "**", "kwargs", ")", "\n", "next_state", ".", "append", "(", "_next_state", ")", "\n", "", "x", "=", "x", "+", "outputs", ".", "pop", "(", ")", "\n", "u_layers", "=", "self", ".", "u_layers", "\n", "\n", "", "for", "layer", "in", "u_layers", ":", "\n", "            ", "x", ",", "_next_state", "=", "layer", ".", "step", "(", "x", ",", "state", "=", "state", ".", "pop", "(", ")", ",", "**", "kwargs", ")", "\n", "next_state", ".", "append", "(", "_next_state", ")", "\n", "x", "=", "x", "+", "outputs", ".", "pop", "(", ")", "\n", "\n", "# feature projection", "\n", "", "x", "=", "self", ".", "norm", "(", "x", ")", "\n", "return", "x", ",", "next_state", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.sequence.unet.SequenceUNet.cache_all": [[196, 201], ["unet.SequenceUNet.modules", "next", "hasattr", "layer.cache_all"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.sequence.unet.SequenceUNet.cache_all"], ["", "def", "cache_all", "(", "self", ")", ":", "\n", "        ", "modules", "=", "self", ".", "modules", "(", ")", "\n", "next", "(", "modules", ")", "\n", "for", "layer", "in", "modules", ":", "\n", "            ", "if", "hasattr", "(", "layer", ",", "'cache_all'", ")", ":", "layer", ".", "cache_all", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.sequence.unet.prepare_generation": [[202, 205], ["model.eval", "hasattr", "model.cache_all"], "function", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.None.example.eval", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.sequence.unet.SequenceUNet.cache_all"], ["", "", "", "def", "prepare_generation", "(", "model", ")", ":", "\n", "    ", "model", ".", "eval", "(", ")", "\n", "if", "hasattr", "(", "model", ",", "'cache_all'", ")", ":", "model", ".", "cache_all", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.sequence.unet.generate_recurrent": [[206, 229], ["torch.inference_mode", "torch.inference_mode", "torch.inference_mode", "range", "torch.stack", "torch.stack", "torch.stack", "print", "torch.zeros", "torch.zeros", "torch.zeros", "model.default_state", "NotImplementedError", "print", "model.step", "mixture_sample", "x.unsqueeze.unsqueeze", "xs.append"], "function", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.sashimi.sashimi.Sashimi.default_state", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.sashimi.sashimi.Sashimi.step", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.tasks.mixture.mixture_sample"], ["", "@", "torch", ".", "inference_mode", "(", ")", "\n", "def", "generate_recurrent", "(", "model", ",", "batch_size", "=", "None", ",", "x", "=", "None", ")", ":", "\n", "    ", "from", "src", ".", "tasks", ".", "mixture", "import", "mixture_sample", "\n", "# TODO incorporate normalization function for dataset", "\n", "# TODO handle or document non-mixture case", "\n", "\"\"\" generate remaining L-L' samples given x: (B, L', C) a context for the model \"\"\"", "\n", "\n", "if", "x", "is", "None", ":", "\n", "        ", "assert", "batch_size", "is", "not", "None", "\n", "x", "=", "torch", ".", "zeros", "(", "batch_size", ",", "model", ".", "d_model", ",", "device", "=", "device", ")", "\n", "state", "=", "model", ".", "default_state", "(", "batch_size", ",", "device", "=", "device", ")", "\n", "", "else", ":", "raise", "NotImplementedError", "(", "\"Conditional generation not implemented yet\"", ")", "\n", "\n", "xs", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "model", ".", "L", ")", ":", "\n", "        ", "print", "(", "\"pixel\"", ",", "i", ")", "\n", "x", ",", "state", "=", "model", ".", "step", "(", "x", ",", "state", ")", "\n", "x", "=", "mixture_sample", "(", "x", ")", "\n", "# TODO postprocess: clamp, divide into buckets, renormalize", "\n", "x", "=", "x", ".", "unsqueeze", "(", "-", "1", ")", "\n", "xs", ".", "append", "(", "x", ")", "\n", "", "sample", "=", "torch", ".", "stack", "(", "xs", ",", "dim", "=", "1", ")", "\n", "print", "(", "\"recurrent sample shape\"", ",", "sample", ".", "shape", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.sequence.unet.generate_global": [[230, 250], ["torch.no_grad", "torch.no_grad", "torch.no_grad", "range", "print", "torch.zeros", "torch.zeros", "torch.zeros", "NotImplementedError", "print", "model", "torch.cat", "torch.cat", "torch.cat", "mixture_sample", "z.unsqueeze.unsqueeze", "torch.cat.new_zeros"], "function", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.tasks.mixture.mixture_sample"], ["", "@", "torch", ".", "no_grad", "(", ")", "\n", "def", "generate_global", "(", "model", ",", "batch_size", "=", "None", ",", "x", "=", "None", ",", "length", "=", "None", ")", ":", "\n", "    ", "from", "tasks", ".", "mixture", "import", "mixture_sample", "\n", "\"\"\" generate remaining L-L' samples given x: (B, L', C) a context for the model \"\"\"", "\n", "\n", "if", "x", "is", "None", ":", "\n", "        ", "assert", "batch_size", "is", "not", "None", "\n", "x", "=", "torch", ".", "zeros", "(", "batch_size", ",", "model", ".", "L", ",", "model", ".", "d_input", ",", "device", "=", "device", ")", "\n", "", "else", ":", "raise", "NotImplementedError", "(", "\"Conditional generation not implemented yet\"", ")", "\n", "\n", "if", "length", "is", "None", ":", "length", "=", "model", ".", "L", "\n", "for", "i", "in", "range", "(", "length", ")", ":", "\n", "        ", "print", "(", "\"pixel\"", ",", "i", ")", "\n", "y", "=", "model", "(", "x", ")", "\n", "y", "=", "torch", ".", "cat", "(", "[", "y", ",", "y", ".", "new_zeros", "(", "batch_size", ",", "1", ",", "model", ".", "d_output", ")", "]", ",", "dim", "=", "1", ")", "# TODO handle sequence shape properly", "\n", "z", "=", "mixture_sample", "(", "y", "[", ":", ",", "i", ",", ":", "]", ")", "\n", "# TODO postprocess: clamp, divide into buckets, renormalize", "\n", "z", "=", "z", ".", "unsqueeze", "(", "-", "1", ")", "\n", "x", "[", ":", ",", "i", ",", ":", "]", "=", "z", "\n", "", "print", "(", "\"global sample shape\"", ",", "x", ".", "shape", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.sequence.ff.FF.__init__": [[8, 36], ["src.models.sequence.base.SequenceModule.__init__", "src.models.nn.LinearActivation", "src.models.nn.LinearActivation", "torch.nn.Sequential", "dropout_cls", "torch.nn.Identity"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.s4.LinearActivation", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.s4.LinearActivation"], ["    ", "def", "__init__", "(", "self", ",", "d_input", ",", "expand", "=", "2", ",", "d_output", "=", "None", ",", "transposed", "=", "False", ",", "activation", "=", "'gelu'", ",", "initializer", "=", "None", ",", "dropout", "=", "0.0", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "d_output", "=", "d_input", "if", "d_output", "is", "None", "else", "d_output", "\n", "self", ".", "transposed", "=", "transposed", "\n", "d_inner", "=", "expand", "*", "d_input", "\n", "\n", "linear1", "=", "LinearActivation", "(", "\n", "d_input", ",", "d_inner", ",", "\n", "transposed", "=", "transposed", ",", "\n", "activation", "=", "activation", ",", "\n", "initializer", "=", "initializer", ",", "\n", "activate", "=", "True", ",", "\n", ")", "\n", "dropout_cls", "=", "nn", ".", "Dropout2d", "if", "self", ".", "transposed", "else", "nn", ".", "Dropout", "\n", "drop", "=", "dropout_cls", "(", "dropout", ")", "if", "dropout", ">", "0.0", "else", "nn", ".", "Identity", "(", ")", "\n", "\n", "linear2", "=", "LinearActivation", "(", "\n", "d_inner", ",", "self", ".", "d_output", ",", "\n", "transposed", "=", "transposed", ",", "\n", "activation", "=", "None", ",", "\n", "initializer", "=", "initializer", ",", "\n", "activate", "=", "False", ",", "\n", ")", "\n", "\n", "self", ".", "ff", "=", "nn", ".", "Sequential", "(", "\n", "linear1", ",", "\n", "drop", ",", "\n", "linear2", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.sequence.ff.FF.forward": [[38, 40], ["ff.FF.ff"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "return", "self", ".", "ff", "(", "x", ")", ",", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.sequence.ff.FF.step": [[41, 48], ["ff.FF.ff().squeeze", "ff.FF.ff", "ff.FF.ff", "x.unsqueeze"], "methods", ["None"], ["", "def", "step", "(", "self", ",", "x", ",", "state", ")", ":", "\n", "# x: [batch, d_input]", "\n", "        ", "if", "self", ".", "transposed", ":", "\n", "# expects: [batch, d_input, seq_len]", "\n", "            ", "return", "self", ".", "ff", "(", "x", ".", "unsqueeze", "(", "-", "1", ")", ")", ".", "squeeze", "(", "-", "1", ")", ",", "state", "\n", "", "else", ":", "\n", "            ", "return", "self", ".", "ff", "(", "x", ")", ",", "state", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.sequence.mha.MultiheadAttention.__init__": [[10, 16], ["models.sequence.base.SequenceModule.__init__", "torch.nn.MultiheadAttention"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__"], ["def", "__init__", "(", "self", ",", "d_model", ",", "n_heads", ",", "*", "args", ",", "causal", "=", "True", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "d_model", "=", "d_model", "\n", "self", ".", "d_output", "=", "d_model", "\n", "self", ".", "mha", "=", "nn", ".", "MultiheadAttention", "(", "d_model", ",", "n_heads", ",", "*", "args", ",", "batch_first", "=", "True", ",", "**", "kwargs", ")", "\n", "self", ".", "causal", "=", "causal", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.sequence.mha.MultiheadAttention.forward": [[17, 27], ["mha.MultiheadAttention.mha", "torch.triu", "torch.ones", "src.size", "src.size"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "src", ",", "attn_mask", "=", "None", ",", "key_padding_mask", "=", "None", ",", "state", "=", "None", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\" state should represent a mask and key padding mask \"\"\"", "\n", "if", "self", ".", "causal", "and", "attn_mask", "is", "None", ":", "\n", "            ", "attn_mask", "=", "torch", ".", "triu", "(", "torch", ".", "ones", "(", "src", ".", "size", "(", "-", "2", ")", ",", "src", ".", "size", "(", "-", "2", ")", ",", "\n", "dtype", "=", "torch", ".", "bool", ",", "device", "=", "src", ".", "device", ")", ",", "\n", "diagonal", "=", "1", ")", "\n", "# attn_mask, key_padding_mask = state", "\n", "# Note that this returns None for the second argument", "\n", "", "y", ",", "z", "=", "self", ".", "mha", "(", "src", ",", "src", ",", "src", ",", "attn_mask", "=", "attn_mask", ",", "key_padding_mask", "=", "key_padding_mask", ",", "need_weights", "=", "False", ",", "**", "kwargs", ")", "\n", "return", "y", ",", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.sequence.mha.MultiheadAttention.step": [[28, 32], ["mha.MultiheadAttention.mha"], "methods", ["None"], ["", "def", "step", "(", "self", ",", "x", ",", "state", ")", ":", "\n", "# TODO proper cached inference", "\n", "# x: (B, D)", "\n", "        ", "y", ",", "z", "=", "self", ".", "mha", "(", "src", ",", "src", ",", "src", ",", "attn_mask", "=", "attn_mask", ",", "key_padding_mask", "=", "key_padding_mask", ",", "need_weights", "=", "False", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.sequence.block.SequenceResidualBlock.__init__": [[21, 64], ["src.models.sequence.SequenceModule.__init__", "src.instantiate", "src.instantiate", "src.instantiate", "src.instantiate", "src.instantiate", "src.instantiate", "isinstance", "drop_cls", "torch.nn.Identity", "src.models.nn.components.Normalization", "src.models.nn.components.Normalization", "src.models.nn.components.Normalization", "src.models.nn.components.Normalization"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.tasks.decoders.instantiate", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.tasks.decoders.instantiate", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.tasks.decoders.instantiate", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.tasks.decoders.instantiate", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.tasks.decoders.instantiate", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.tasks.decoders.instantiate"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "d_input", ",", "\n", "i_layer", "=", "None", ",", "# Only needs to be passed into certain residuals like Decay", "\n", "prenorm", "=", "True", ",", "\n", "dropout", "=", "0.0", ",", "\n", "layer", "=", "None", ",", "# Config for black box module", "\n", "residual", "=", "None", ",", "# Config for residual function", "\n", "norm", "=", "None", ",", "# Config for normalization layer", "\n", "pool", "=", "None", ",", "\n", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "i_layer", "=", "i_layer", "\n", "self", ".", "d_input", "=", "d_input", "\n", "self", ".", "layer", "=", "utils", ".", "instantiate", "(", "registry", ".", "layer", ",", "layer", ",", "d_input", ")", "\n", "self", ".", "prenorm", "=", "prenorm", "\n", "\n", "# Residual", "\n", "# d_residual is the output dimension after residual", "\n", "if", "residual", "is", "None", ":", "\n", "            ", "self", ".", "residual", "=", "None", "\n", "self", ".", "d_residual", "=", "self", ".", "layer", ".", "d_output", "\n", "", "else", ":", "\n", "            ", "self", ".", "residual", "=", "utils", ".", "instantiate", "(", "residual_registry", ",", "residual", ",", "i_layer", ",", "d_input", ",", "self", ".", "layer", ".", "d_output", ")", "\n", "self", ".", "d_residual", "=", "self", ".", "residual", ".", "d_output", "\n", "\n", "# Normalization", "\n", "", "d_norm", "=", "d_input", "if", "self", ".", "prenorm", "else", "self", ".", "d_residual", "\n", "# We don't use config to directly instantiate since Normalization has some special cases", "\n", "if", "norm", "is", "None", ":", "\n", "            ", "self", ".", "norm", "=", "None", "\n", "", "elif", "isinstance", "(", "norm", ",", "str", ")", ":", "\n", "            ", "self", ".", "norm", "=", "Normalization", "(", "d_norm", ",", "transposed", "=", "self", ".", "transposed", ",", "_name_", "=", "norm", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "norm", "=", "Normalization", "(", "d_norm", ",", "transposed", "=", "self", ".", "transposed", ",", "**", "norm", ")", "\n", "\n", "# Pool", "\n", "", "self", ".", "pool", "=", "utils", ".", "instantiate", "(", "pool_registry", ",", "pool", ",", "self", ".", "d_residual", ",", "transposed", "=", "self", ".", "transposed", ")", "\n", "\n", "# Dropout", "\n", "drop_cls", "=", "nn", ".", "Dropout2d", "if", "self", ".", "transposed", "else", "nn", ".", "Dropout", "\n", "self", ".", "drop", "=", "drop_cls", "(", "dropout", ")", "if", "dropout", ">", "0.0", "else", "nn", ".", "Identity", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.sequence.block.SequenceResidualBlock.transposed": [[66, 69], ["getattr"], "methods", ["None"], ["", "@", "property", "\n", "def", "transposed", "(", "self", ")", ":", "\n", "        ", "return", "getattr", "(", "self", ".", "layer", ",", "'transposed'", ",", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.sequence.block.SequenceResidualBlock.d_output": [[70, 73], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "d_output", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "pool", ".", "d_output", "if", "self", ".", "pool", "is", "not", "None", "else", "self", ".", "d_residual", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.sequence.block.SequenceResidualBlock.d_state": [[74, 77], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "d_state", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "layer", ".", "d_state", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.sequence.block.SequenceResidualBlock.state_to_tensor": [[78, 81], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "state_to_tensor", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "layer", ".", "state_to_tensor", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.sequence.block.SequenceResidualBlock.default_state": [[82, 84], ["block.SequenceResidualBlock.layer.default_state"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.sashimi.sashimi.Sashimi.default_state"], ["", "def", "default_state", "(", "self", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "return", "self", ".", "layer", ".", "default_state", "(", "*", "args", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.sequence.block.SequenceResidualBlock.forward": [[85, 105], ["block.SequenceResidualBlock.layer", "block.SequenceResidualBlock.norm", "block.SequenceResidualBlock.residual", "block.SequenceResidualBlock.norm", "block.SequenceResidualBlock.pool", "block.SequenceResidualBlock.drop"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ",", "*", "args", ",", "state", "=", "None", ",", "**", "kwargs", ")", ":", "\n", "        ", "y", "=", "x", "\n", "\n", "# Pre-norm", "\n", "if", "self", ".", "norm", "is", "not", "None", "and", "self", ".", "prenorm", ":", "y", "=", "self", ".", "norm", "(", "y", ")", "\n", "\n", "# Black box module", "\n", "y", ",", "state", "=", "self", ".", "layer", "(", "y", ",", "*", "args", ",", "state", "=", "state", ",", "**", "kwargs", ")", "\n", "\n", "# Residual", "\n", "if", "self", ".", "residual", "is", "not", "None", ":", "y", "=", "self", ".", "residual", "(", "x", ",", "self", ".", "drop", "(", "y", ")", ",", "self", ".", "transposed", ")", "\n", "\n", "# Post-norm", "\n", "if", "self", ".", "norm", "is", "not", "None", "and", "not", "self", ".", "prenorm", ":", "y", "=", "self", ".", "norm", "(", "y", ")", "\n", "\n", "# Pool", "\n", "# x = pool.downpool(x, self.pool, self.expand, self.transposed)", "\n", "if", "self", ".", "pool", "is", "not", "None", ":", "y", "=", "self", ".", "pool", "(", "y", ")", "\n", "\n", "return", "y", ",", "state", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.sequence.block.SequenceResidualBlock.step": [[106, 131], ["block.SequenceResidualBlock.layer.step", "block.SequenceResidualBlock.norm", "block.SequenceResidualBlock.residual", "block.SequenceResidualBlock.norm", "block.SequenceResidualBlock.pool", "y.squeeze.squeeze.unsqueeze", "y.squeeze.squeeze.squeeze", "y.squeeze.squeeze.unsqueeze", "y.squeeze.squeeze.squeeze"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.sashimi.sashimi.Sashimi.step"], ["", "def", "step", "(", "self", ",", "x", ",", "state", ",", "*", "args", ",", "**", "kwargs", ")", ":", "# TODO needs fix for transpose logic", "\n", "        ", "y", "=", "x", "\n", "\n", "# Pre-norm", "\n", "if", "self", ".", "norm", "is", "not", "None", "and", "self", ".", "prenorm", ":", "\n", "            ", "if", "self", ".", "transposed", ":", "y", "=", "y", ".", "unsqueeze", "(", "-", "1", ")", "\n", "y", "=", "self", ".", "norm", "(", "y", ")", "# TODO transpose seems wrong", "\n", "if", "self", ".", "transposed", ":", "y", "=", "y", ".", "squeeze", "(", "-", "1", ")", "\n", "\n", "# Black box module", "\n", "", "y", ",", "state", "=", "self", ".", "layer", ".", "step", "(", "y", ",", "state", ",", "*", "args", ",", "**", "kwargs", ")", "\n", "\n", "# Residual", "\n", "if", "self", ".", "residual", "is", "not", "None", ":", "y", "=", "self", ".", "residual", "(", "x", ",", "y", ",", "transposed", "=", "False", ")", "# TODO this would not work with concat", "\n", "\n", "# Post-norm", "\n", "if", "self", ".", "norm", "is", "not", "None", "and", "not", "self", ".", "prenorm", ":", "\n", "            ", "if", "self", ".", "transposed", ":", "y", "=", "y", ".", "unsqueeze", "(", "-", "1", ")", "\n", "y", "=", "self", ".", "norm", "(", "y", ")", "#.step(x)", "\n", "if", "self", ".", "transposed", ":", "y", "=", "y", ".", "squeeze", "(", "-", "1", ")", "\n", "\n", "# Pool", "\n", "", "if", "self", ".", "pool", "is", "not", "None", ":", "y", "=", "self", ".", "pool", "(", "y", ")", "\n", "\n", "return", "y", ",", "state", "\n", "", "", ""]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.sequence.model.SequenceModel.__init__": [[23, 89], ["src.models.sequence.base.SequenceModule.__init__", "src.utils.config.to_list", "enumerate", "torch.ModuleList", "torch.ModuleList", "torch.Identity", "torch.Identity", "src.models.sequence.block.SequenceResidualBlock", "_layers.append", "torch.Identity", "torch.Identity", "model.SequenceModel.apply", "torch.Dropout2d", "torch.Dropout2d", "torch.Dropout", "torch.Dropout", "_layer.get", "isinstance", "functools.partial", "src.models.nn.components.Normalization", "src.models.nn.components.Normalization"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.utils.config.to_list"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "d_model", ",", "# Resize input (useful for deep models with residuals)", "\n", "n_layers", "=", "1", ",", "# Number of layers", "\n", "transposed", "=", "False", ",", "\n", "dropout", "=", "0.0", ",", "# Residual dropout parameter", "\n", "prenorm", "=", "True", ",", "\n", "layer", "=", "None", ",", "# layer config, must be specified", "\n", "residual", "=", "None", ",", "# Residual config", "\n", "norm", "=", "None", ",", "# Normalization config (e.g. layer vs batch)", "\n", "pool", "=", "None", ",", "\n", "init", "=", "None", ",", "\n", "verbose", "=", "False", ",", "\n", "track_norms", "=", "True", ",", "\n", "dropinp", "=", "0.0", ",", "\n", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "# Save arguments needed for forward pass", "\n", "self", ".", "d_model", "=", "d_model", "\n", "self", ".", "transposed", "=", "transposed", "\n", "self", ".", "verbose", "=", "verbose", "\n", "self", ".", "track_norms", "=", "track_norms", "\n", "self", ".", "_forward", "=", "False", "\n", "\n", "if", "dropinp", ">", "0.0", ":", "\n", "            ", "self", ".", "drop", "=", "nn", ".", "Dropout2d", "(", "dropinp", ")", "if", "self", ".", "transposed", "else", "nn", ".", "Dropout", "(", "dropinp", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "drop", "=", "nn", ".", "Identity", "(", ")", "\n", "\n", "", "layer", "=", "to_list", "(", "layer", ",", "recursive", "=", "False", ")", "\n", "\n", "# Some special arguments are passed into each layer", "\n", "for", "_layer", "in", "layer", ":", "\n", "# If layers don't specify dropout, add it", "\n", "            ", "if", "_layer", ".", "get", "(", "'dropout'", ",", "None", ")", "is", "None", ":", "\n", "                ", "_layer", "[", "'dropout'", "]", "=", "dropout", "\n", "# Ensure all layers are shaped the same way", "\n", "", "_layer", "[", "'transposed'", "]", "=", "transposed", "\n", "\n", "# Duplicate layers", "\n", "", "layers", "=", "layer", "*", "n_layers", "\n", "\n", "# Instantiate layers", "\n", "_layers", "=", "[", "]", "\n", "d", "=", "d_model", "\n", "for", "l", ",", "layer", "in", "enumerate", "(", "layers", ")", ":", "\n", "            ", "block", "=", "SequenceResidualBlock", "(", "d", ",", "l", "+", "1", ",", "prenorm", "=", "prenorm", ",", "dropout", "=", "dropout", ",", "layer", "=", "layer", ",", "residual", "=", "residual", ",", "norm", "=", "norm", ",", "pool", "=", "pool", ")", "\n", "_layers", ".", "append", "(", "block", ")", "\n", "d", "=", "block", ".", "d_output", "\n", "\n", "", "self", ".", "d_output", "=", "d", "\n", "self", ".", "layers", "=", "nn", ".", "ModuleList", "(", "_layers", ")", "\n", "\n", "if", "prenorm", ":", "\n", "            ", "if", "norm", "is", "None", ":", "\n", "                ", "self", ".", "norm", "=", "None", "\n", "", "elif", "isinstance", "(", "norm", ",", "str", ")", ":", "\n", "                ", "self", ".", "norm", "=", "Normalization", "(", "self", ".", "d_output", ",", "transposed", "=", "self", ".", "transposed", ",", "_name_", "=", "norm", ")", "\n", "", "else", ":", "\n", "                ", "self", ".", "norm", "=", "Normalization", "(", "self", ".", "d_output", ",", "transposed", "=", "self", ".", "transposed", ",", "**", "norm", ")", "\n", "", "", "else", ":", "\n", "            ", "self", ".", "norm", "=", "nn", ".", "Identity", "(", ")", "\n", "\n", "# Initializer hook", "\n", "", "if", "init", "is", "not", "None", ":", "\n", "            ", "self", ".", "apply", "(", "functools", ".", "partial", "(", "weights_init", ",", "init_cfg", "=", "init", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.sequence.model.SequenceModel.forward": [[90, 120], ["model.SequenceModel.drop", "zip", "model.SequenceModel.norm", "print", "einops.rearrange", "layer", "next_states.append", "einops.rearrange", "src.utils.config.to_dict", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "len", "output_norms.append", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "src.utils.config.to_dict.items", "einops.rearrange.detach", "einops.rearrange.detach"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.utils.config.to_dict"], ["", "", "def", "forward", "(", "self", ",", "inputs", ",", "*", "args", ",", "state", "=", "None", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\" Inputs assumed to be (batch, sequence, dim) \"\"\"", "\n", "# Debug", "\n", "if", "self", ".", "verbose", "and", "not", "self", ".", "_forward", ":", "\n", "            ", "print", "(", "\"Model: unused kwargs\"", ",", "kwargs", ")", "\n", "self", ".", "_forward", "=", "True", "\n", "\n", "", "if", "self", ".", "transposed", ":", "inputs", "=", "rearrange", "(", "inputs", ",", "'b l d -> b d l'", ")", "\n", "inputs", "=", "self", ".", "drop", "(", "inputs", ")", "\n", "\n", "# Track norms", "\n", "if", "self", ".", "track_norms", ":", "output_norms", "=", "[", "torch", ".", "mean", "(", "inputs", ".", "detach", "(", ")", "**", "2", ")", "]", "\n", "\n", "# Apply layers", "\n", "outputs", "=", "inputs", "\n", "prev_states", "=", "[", "None", "]", "*", "len", "(", "self", ".", "layers", ")", "if", "state", "is", "None", "else", "state", "\n", "next_states", "=", "[", "]", "\n", "for", "layer", ",", "prev_state", "in", "zip", "(", "self", ".", "layers", ",", "prev_states", ")", ":", "\n", "            ", "outputs", ",", "state", "=", "layer", "(", "outputs", ",", "*", "args", ",", "state", "=", "prev_state", ",", "**", "kwargs", ")", "# TODO handle state", "\n", "next_states", ".", "append", "(", "state", ")", "\n", "if", "self", ".", "track_norms", ":", "output_norms", ".", "append", "(", "torch", ".", "mean", "(", "outputs", ".", "detach", "(", ")", "**", "2", ")", ")", "\n", "", "outputs", "=", "self", ".", "norm", "(", "outputs", ")", "\n", "\n", "if", "self", ".", "transposed", ":", "outputs", "=", "rearrange", "(", "outputs", ",", "'b d l -> b l d'", ")", "\n", "\n", "if", "self", ".", "track_norms", ":", "\n", "            ", "metrics", "=", "to_dict", "(", "output_norms", ",", "recursive", "=", "False", ")", "\n", "self", ".", "metrics", "=", "{", "f'norm/{i}'", ":", "v", "for", "i", ",", "v", "in", "metrics", ".", "items", "(", ")", "}", "\n", "\n", "", "return", "outputs", ",", "next_states", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.sequence.model.SequenceModel.d_state": [[121, 125], ["sum"], "methods", ["None"], ["", "@", "property", "\n", "def", "d_state", "(", "self", ")", ":", "\n", "        ", "d_states", "=", "[", "layer", ".", "d_state", "for", "layer", "in", "self", ".", "layers", "]", "\n", "return", "sum", "(", "[", "d", "for", "d", "in", "d_states", "if", "d", "is", "not", "None", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.sequence.model.SequenceModel.state_to_tensor": [[126, 135], ["torch.cat", "torch.cat", "torch.cat", "torch.cat", "_layer.state_to_tensor", "zip"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.s4.S4.state_to_tensor"], ["", "@", "property", "\n", "def", "state_to_tensor", "(", "self", ")", ":", "\n", "# Slightly hacky way to implement this in a curried manner (so that the function can be extracted from an instance)", "\n", "# Somewhat more sound may be to turn this into a @staticmethod and grab subclasses using hydra.utils.get_class", "\n", "        ", "def", "fn", "(", "state", ")", ":", "\n", "            ", "x", "=", "[", "_layer", ".", "state_to_tensor", "(", "_state", ")", "for", "(", "_layer", ",", "_state", ")", "in", "zip", "(", "self", ".", "layers", ",", "state", ")", "]", "\n", "x", "=", "[", "_x", "for", "_x", "in", "x", "if", "_x", "is", "not", "None", "]", "\n", "return", "torch", ".", "cat", "(", "x", ",", "dim", "=", "-", "1", ")", "\n", "", "return", "fn", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.sequence.model.SequenceModel.default_state": [[136, 138], ["layer.default_state"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.sashimi.sashimi.Sashimi.default_state"], ["", "def", "default_state", "(", "self", ",", "*", "batch_shape", ",", "device", "=", "None", ")", ":", "\n", "        ", "return", "[", "layer", ".", "default_state", "(", "*", "batch_shape", ",", "device", "=", "device", ")", "for", "layer", "in", "self", ".", "layers", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.sequence.model.SequenceModel.step": [[139, 162], ["zip", "model.SequenceModel.norm", "layer.step", "next_states.append", "len"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.sashimi.sashimi.Sashimi.step"], ["", "def", "step", "(", "self", ",", "x", ",", "state", ")", ":", "\n", "        ", "\"\"\" \n        Step one time step as a recurrent model. Intended to be used during validation.\n\n        u: (B H)\n        state: (B H N)\n        Returns: output (B H), state (B H N)\n        \"\"\"", "\n", "\n", "# if self.transposed: x = rearrange(x, 'b l d -> b d l')", "\n", "\n", "# Apply layers", "\n", "prev_states", "=", "[", "None", "]", "*", "len", "(", "self", ".", "layers", ")", "if", "state", "is", "None", "else", "state", "\n", "next_states", "=", "[", "]", "\n", "for", "layer", ",", "prev_state", "in", "zip", "(", "self", ".", "layers", ",", "prev_states", ")", ":", "\n", "            ", "x", ",", "state", "=", "layer", ".", "step", "(", "x", ",", "state", "=", "prev_state", ")", "\n", "next_states", ".", "append", "(", "state", ")", "\n", "\n", "", "x", "=", "self", ".", "norm", "(", "x", ")", "\n", "\n", "# if self.transposed: x = rearrange(x, 'b d l -> b l d')", "\n", "\n", "return", "x", ",", "next_states", "\n", "", "", ""]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.rnns.rnn.RNN.__init__": [[19, 30], ["src.models.sequence.SequenceModule.__init__", "src.instantiate", "NotImplementedError"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.tasks.decoders.instantiate"], ["    ", "def", "__init__", "(", "self", ",", "d_input", ",", "cell", "=", "None", ",", "return_output", "=", "True", ",", "transposed", "=", "False", ",", "dropout", "=", "0.0", ")", ":", "\n", "        ", "\"\"\"\n        return_output: if False, only returns the state\n        \"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "transposed", "=", "transposed", "\n", "if", "dropout", ">", "0.0", ":", "\n", "            ", "raise", "NotImplementedError", "(", "\"Dropout currently not supported for custom RNNs\"", ")", "\n", "", "self", ".", "return_output", "=", "return_output", "\n", "\n", "self", ".", "cell", "=", "utils", ".", "instantiate", "(", "cell_registry", ",", "cell", ",", "d_input", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.rnns.rnn.RNN.forward": [[31, 55], ["isinstance", "rnn.RNN.cell.default_state", "torch.unbind", "torch.unbind", "torch.unbind", "torch.unbind", "torch.unbind", "torch.unbind", "torch.unbind", "torch.unbind", "torch.unbind", "inputs.transpose.transpose.transpose", "rnn.PackedRNN.forward", "rnn.RNN.step", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "outputs.transpose.transpose.transpose", "outputs.transpose.transpose.append"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.sashimi.sashimi.Sashimi.default_state", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.models.resnext.CifarResNeXt.forward", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.sashimi.sashimi.Sashimi.step"], ["", "def", "forward", "(", "self", ",", "inputs", ",", "state", "=", "None", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"\n        cell.forward : (input, state) -> (output, state)\n        inputs : [n_batch, l_seq, d]\n        \"\"\"", "\n", "\n", "if", "self", ".", "transposed", ":", "inputs", "=", "inputs", ".", "transpose", "(", "-", "1", ",", "-", "2", ")", "\n", "\n", "# Automatically detect PackedSequence", "\n", "if", "isinstance", "(", "inputs", ",", "nn", ".", "utils", ".", "rnn", ".", "PackedSequence", ")", ":", "\n", "            ", "return", "PackedRNN", ".", "forward", "(", "self", ",", "inputs", ")", "\n", "\n", "# Construct initial state", "\n", "", "state", "=", "self", ".", "cell", ".", "default_state", "(", "*", "inputs", ".", "shape", "[", ":", "-", "2", "]", ",", "device", "=", "inputs", ".", "device", ")", "\n", "outputs", "=", "[", "]", "\n", "\n", "for", "input", "in", "torch", ".", "unbind", "(", "inputs", ",", "dim", "=", "-", "2", ")", ":", "\n", "            ", "output", ",", "new_state", "=", "self", ".", "step", "(", "input", ",", "state", ")", "\n", "state", "=", "new_state", "\n", "if", "self", ".", "return_output", ":", "\n", "                ", "outputs", ".", "append", "(", "output", ")", "\n", "", "", "outputs", "=", "torch", ".", "stack", "(", "outputs", ",", "dim", "=", "-", "2", ")", "if", "self", ".", "return_output", "else", "None", "\n", "if", "self", ".", "transposed", "and", "outputs", "is", "not", "None", ":", "outputs", "=", "outputs", ".", "transpose", "(", "-", "1", ",", "-", "2", ")", "\n", "return", "outputs", ",", "state", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.rnns.rnn.RNN.step": [[56, 58], ["rnn.RNN.cell.step"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.sashimi.sashimi.Sashimi.step"], ["", "def", "step", "(", "self", ",", "x", ",", "state", ")", ":", "\n", "        ", "return", "self", ".", "cell", ".", "step", "(", "x", ",", "state", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.rnns.rnn.RNN.default_state": [[59, 61], ["rnn.RNN.cell.default_state"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.sashimi.sashimi.Sashimi.default_state"], ["", "def", "default_state", "(", "self", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "return", "self", ".", "cell", ".", "default_state", "(", "*", "args", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.rnns.rnn.RNN.d_state": [[62, 66], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "d_state", "(", "self", ")", ":", "\n", "        ", "\"\"\" Size after converting state to a single tensor \"\"\"", "\n", "return", "self", ".", "cell", ".", "d_state", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.rnns.rnn.RNN.d_output": [[67, 71], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "d_output", "(", "self", ")", ":", "\n", "        ", "\"\"\" Size of output \"\"\"", "\n", "return", "self", ".", "cell", ".", "d_output", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.rnns.rnn.RNN.state_to_tensor": [[72, 77], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "state_to_tensor", "(", "self", ")", ":", "\n", "        ", "\"\"\" Convert state into a single tensor output \"\"\"", "\n", "# return self.cell.state_to_tensor(state)", "\n", "return", "self", ".", "cell", ".", "state_to_tensor", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.rnns.rnn.PackedRNN.apply_tuple": [[82, 89], ["isinstance", "tuple", "fn", "isinstance", "fn"], "methods", ["None"], ["@", "staticmethod", "\n", "def", "apply_tuple", "(", "tup", ",", "fn", ")", ":", "\n", "        ", "\"\"\"Apply a function to a Tensor or a tuple of Tensor\"\"\"", "\n", "if", "isinstance", "(", "tup", ",", "tuple", ")", ":", "\n", "            ", "return", "tuple", "(", "(", "fn", "(", "x", ")", "if", "isinstance", "(", "x", ",", "torch", ".", "Tensor", ")", "else", "x", ")", "for", "x", "in", "tup", ")", "\n", "", "else", ":", "\n", "            ", "return", "fn", "(", "tup", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.rnns.rnn.PackedRNN.concat_tuple": [[90, 100], ["isinstance", "tuple", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "isinstance", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "zip"], "methods", ["None"], ["", "", "@", "staticmethod", "\n", "def", "concat_tuple", "(", "tups", ",", "dim", "=", "0", ")", ":", "\n", "        ", "\"\"\"Concat a list of Tensors or a list of tuples of Tensor\"\"\"", "\n", "if", "isinstance", "(", "tups", "[", "0", "]", ",", "tuple", ")", ":", "\n", "            ", "return", "tuple", "(", "\n", "(", "torch", ".", "cat", "(", "xs", ",", "dim", ")", "if", "isinstance", "(", "xs", "[", "0", "]", ",", "torch", ".", "Tensor", ")", "else", "xs", "[", "0", "]", ")", "\n", "for", "xs", "in", "zip", "(", "*", "tups", ")", "\n", ")", "\n", "", "else", ":", "\n", "            ", "return", "torch", ".", "cat", "(", "tups", ",", "dim", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.rnns.rnn.PackedRNN.forward": [[101, 159], ["isinstance", "rnn.PackedRNN.cell.default_state", "batch_sizes.detach().cpu().numpy.detach().cpu().numpy.detach().cpu().numpy", "saved_states.append", "saved_states.reverse", "rnn.PackedRNN.concat_tuple", "rnn.PackedRNN.apply_tuple", "rnn.PackedRNN.cell.forward", "torch.utils.rnn.PackedSequence", "torch.utils.rnn.PackedSequence", "torch.utils.rnn.PackedSequence", "batch_sizes.detach().cpu().numpy.detach().cpu().numpy.detach().cpu", "rnn.PackedRNN.apply_tuple", "rnn.PackedRNN.apply_tuple", "saved_states.append", "torch.utils.rnn.PackedSequence.append", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "batch_sizes.detach().cpu().numpy.detach().cpu().numpy.detach"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.sashimi.sashimi.Sashimi.default_state", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.rnns.rnn.PackedRNN.concat_tuple", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.rnns.rnn.PackedRNN.apply_tuple", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.models.resnext.CifarResNeXt.forward", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.rnns.rnn.PackedRNN.apply_tuple", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.rnns.rnn.PackedRNN.apply_tuple"], ["", "", "def", "forward", "(", "self", ",", "inputs", ",", "len_batch", "=", "None", ")", ":", "\n", "# assert len_batch is not None", "\n", "# inputs = nn.utils.rnn.pack_padded_sequence(", "\n", "#     inputs, len_batch.cpu(), enforce_sorted=False", "\n", "# )", "\n", "        ", "assert", "isinstance", "(", "inputs", ",", "nn", ".", "utils", ".", "rnn", ".", "PackedSequence", ")", "\n", "\n", "# Similar implementation to https://github.com/pytorch/pytorch/blob/9e94e464535e768ad3444525aecd78893504811f/torch/nn/modules/rnn.py#L202", "\n", "inputs", ",", "batch_sizes", ",", "sorted_indices", ",", "unsorted_indices", "=", "inputs", "\n", "max_batch_size", "=", "batch_sizes", "[", "0", "]", "\n", "\n", "# Construct initial state", "\n", "state", "=", "self", ".", "cell", ".", "default_state", "(", "max_batch_size", ",", "device", "=", "inputs", ".", "device", ")", "\n", "outputs", "=", "[", "]", "\n", "\n", "# Following implementation at https://github.com/pytorch/pytorch/blob/9e94e464535e768ad3444525aecd78893504811f/aten/src/ATen/native/RNN.cpp#L621", "\n", "# Batch sizes is a sequence of decreasing lengths, which are offsets", "\n", "# into a 1D list of inputs. At every step we slice out batch_size elements,", "\n", "# and possibly account for the decrease in the batch size since the last step,", "\n", "# which requires us to slice the hidden state (since some sequences", "\n", "# are completed now). The sliced parts are also saved, because we will need", "\n", "# to return a tensor of final hidden state.", "\n", "batch_sizes_og", "=", "batch_sizes", "\n", "batch_sizes", "=", "batch_sizes", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "input_offset", "=", "0", "\n", "last_batch_size", "=", "batch_sizes", "[", "0", "]", "\n", "saved_states", "=", "[", "]", "\n", "for", "batch_size", "in", "batch_sizes", ":", "\n", "            ", "step_input", "=", "inputs", "[", "input_offset", ":", "input_offset", "+", "batch_size", "]", "\n", "input_offset", "+=", "batch_size", "\n", "dec", "=", "last_batch_size", "-", "batch_size", "\n", "if", "dec", ">", "0", ":", "\n", "                ", "saved_state", "=", "PackedRNN", ".", "apply_tuple", "(", "state", ",", "lambda", "x", ":", "x", "[", "batch_size", ":", "]", ")", "\n", "state", "=", "PackedRNN", ".", "apply_tuple", "(", "state", ",", "lambda", "x", ":", "x", "[", ":", "batch_size", "]", ")", "\n", "saved_states", ".", "append", "(", "saved_state", ")", "\n", "", "last_batch_size", "=", "batch_size", "\n", "output", ",", "new_state", "=", "self", ".", "cell", ".", "forward", "(", "step_input", ",", "state", ")", "\n", "state", "=", "new_state", "\n", "if", "self", ".", "return_output", ":", "\n", "                ", "outputs", ".", "append", "(", "output", ")", "\n", "", "", "saved_states", ".", "append", "(", "state", ")", "\n", "saved_states", ".", "reverse", "(", ")", "\n", "state", "=", "PackedRNN", ".", "concat_tuple", "(", "saved_states", ")", "\n", "state", "=", "PackedRNN", ".", "apply_tuple", "(", "\n", "state", ",", "\n", "lambda", "x", ":", "x", "[", "unsorted_indices", "]", "if", "unsorted_indices", "is", "not", "None", "else", "x", ",", "\n", ")", "\n", "if", "self", ".", "return_output", ":", "\n", "            ", "outputs", "=", "nn", ".", "utils", ".", "rnn", ".", "PackedSequence", "(", "\n", "torch", ".", "cat", "(", "outputs", ",", "dim", "=", "0", ")", ",", "\n", "batch_sizes_og", ",", "\n", "sorted_indices", ",", "\n", "unsorted_indices", ",", "\n", ")", "\n", "# outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs)", "\n", "", "else", ":", "\n", "            ", "outputs", "=", "None", "\n", "", "return", "outputs", ",", "state", "\n", "", "", ""]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.rnns.gru.TorchGRU.__init__": [[12, 25], ["super().__init__", "torch.nn.Parameter", "torch.zeros"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__"], ["def", "__init__", "(", "self", ",", "d_model", ",", "d_hidden", ",", "n_layers", "=", "1", ",", "learn_h0", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "# Rename input_size, hidden_size to d_input, d_model", "\n", "# Set batch_first as default as per this codebase's convention", "\n", "        ", "self", ".", "d_model", "=", "d_model", "\n", "self", ".", "d_hidden", "=", "d_hidden", "\n", "self", ".", "n_layers", "=", "n_layers", "\n", "self", ".", "learn_h0", "=", "learn_h0", "\n", "super", "(", ")", ".", "__init__", "(", "d_model", ",", "d_hidden", ",", "num_layers", "=", "n_layers", ",", "batch_first", "=", "True", ",", "**", "kwargs", ")", "\n", "\n", "self", ".", "num_directions", "=", "2", "if", "self", ".", "bidirectional", "else", "1", "\n", "\n", "if", "self", ".", "learn_h0", ":", "\n", "            ", "self", ".", "h0", "=", "nn", ".", "Parameter", "(", "torch", ".", "zeros", "(", "self", ".", "num_layers", "*", "self", ".", "num_directions", ",", "1", ",", "self", ".", "hidden_size", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.rnns.gru.TorchGRU.step": [[26, 28], ["NotImplementedError"], "methods", ["None"], ["", "", "def", "step", "(", "self", ",", "x", ",", "state", ")", ":", "\n", "        ", "raise", "NotImplementedError", "(", "\"Needs to be implemented.\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.rnns.gru.TorchGRU.default_state": [[29, 42], ["torch.zeros", "gru.TorchGRU.h0.expand"], "methods", ["None"], ["", "def", "default_state", "(", "self", ",", "*", "batch_shape", ",", "device", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Snippet from nn.LSTM source\n        # https://pytorch.org/docs/stable/_modules/torch/nn/modules/rnn.html#LSTM\n        \"\"\"", "\n", "if", "not", "self", ".", "learn_h0", ":", "\n", "            ", "h_zeros", "=", "torch", ".", "zeros", "(", "self", ".", "num_layers", "*", "self", ".", "num_directions", ",", "\n", "*", "batch_shape", ",", "self", ".", "hidden_size", ",", "\n", "dtype", "=", "torch", ".", "float", ",", "device", "=", "device", ")", "\n", "", "else", ":", "\n", "            ", "h_zeros", "=", "self", ".", "h0", ".", "expand", "(", "self", ".", "num_layers", "*", "self", ".", "num_directions", ",", "*", "batch_shape", ",", "self", ".", "hidden_size", ")", "\n", "\n", "", "return", "h_zeros", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.rnns.gru.TorchGRU.d_state": [[43, 46], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "d_state", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "n_layers", "*", "self", ".", "d_hidden", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.rnns.gru.TorchGRU.d_output": [[47, 50], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "d_output", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "d_hidden", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.rnns.gru.TorchGRU.state_to_tensor": [[51, 57], ["einops.rearrange"], "methods", ["None"], ["", "@", "property", "\n", "def", "state_to_tensor", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "n_layers", "==", "1", ":", "\n", "            ", "return", "lambda", "state", ":", "state", "[", "0", "]", "\n", "", "else", ":", "\n", "            ", "return", "lambda", "state", ":", "rearrange", "(", "state", "[", "0", "]", ",", "'d b h -> b (d h)'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.rnns.lstm.TorchLSTM.__init__": [[12, 27], ["super().__init__", "torch.nn.Parameter", "torch.nn.Parameter", "torch.zeros", "torch.zeros"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__"], ["def", "__init__", "(", "self", ",", "d_model", ",", "d_hidden", ",", "n_layers", "=", "1", ",", "learn_h0", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "# Rename input_size, hidden_size to d_input, d_model", "\n", "# Set batch_first as default as per this codebase's convention", "\n", "        ", "self", ".", "d_model", "=", "d_model", "\n", "self", ".", "d_hidden", "=", "d_hidden", "\n", "self", ".", "n_layers", "=", "n_layers", "\n", "self", ".", "learn_h0", "=", "learn_h0", "\n", "super", "(", ")", ".", "__init__", "(", "d_model", ",", "d_hidden", ",", "num_layers", "=", "n_layers", ",", "batch_first", "=", "True", ",", "**", "kwargs", ")", "\n", "\n", "self", ".", "num_directions", "=", "2", "if", "self", ".", "bidirectional", "else", "1", "\n", "self", ".", "real_hidden_size", "=", "self", ".", "proj_size", "if", "self", ".", "proj_size", ">", "0", "else", "self", ".", "hidden_size", "\n", "\n", "if", "learn_h0", ":", "\n", "            ", "self", ".", "h0", "=", "nn", ".", "Parameter", "(", "torch", ".", "zeros", "(", "self", ".", "num_layers", "*", "self", ".", "num_directions", ",", "1", ",", "self", ".", "real_hidden_size", ")", ")", "\n", "self", ".", "c0", "=", "nn", ".", "Parameter", "(", "torch", ".", "zeros", "(", "self", ".", "num_layers", "*", "self", ".", "num_directions", ",", "1", ",", "self", ".", "hidden_size", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.rnns.lstm.TorchLSTM.step": [[32, 34], ["NotImplementedError"], "methods", ["None"], ["", "", "def", "step", "(", "self", ",", "x", ",", "state", ")", ":", "\n", "        ", "raise", "NotImplementedError", "(", "\"Needs to be implemented.\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.rnns.lstm.TorchLSTM.default_state": [[35, 51], ["torch.zeros", "torch.zeros", "lstm.TorchLSTM.h0.expand", "lstm.TorchLSTM.c0.expand"], "methods", ["None"], ["", "def", "default_state", "(", "self", ",", "*", "batch_shape", ",", "device", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Snippet from nn.LSTM source\n        # https://pytorch.org/docs/stable/_modules/torch/nn/modules/rnn.html#LSTM\n        \"\"\"", "\n", "if", "not", "self", ".", "learn_h0", ":", "\n", "            ", "h_zeros", "=", "torch", ".", "zeros", "(", "self", ".", "num_layers", "*", "self", ".", "num_directions", ",", "\n", "*", "batch_shape", ",", "self", ".", "real_hidden_size", ",", "\n", "dtype", "=", "torch", ".", "float", ",", "device", "=", "device", ")", "\n", "c_zeros", "=", "torch", ".", "zeros", "(", "self", ".", "num_layers", "*", "self", ".", "num_directions", ",", "\n", "*", "batch_shape", ",", "self", ".", "hidden_size", ",", "\n", "dtype", "=", "torch", ".", "float", ",", "device", "=", "device", ")", "\n", "", "else", ":", "\n", "            ", "h_zeros", "=", "self", ".", "h0", ".", "expand", "(", "self", ".", "num_layers", "*", "self", ".", "num_directions", ",", "*", "batch_shape", ",", "self", ".", "real_hidden_size", ")", "\n", "c_zeros", "=", "self", ".", "c0", ".", "expand", "(", "self", ".", "num_layers", "*", "self", ".", "num_directions", ",", "*", "batch_shape", ",", "self", ".", "hidden_size", ")", "\n", "", "return", "(", "h_zeros", ",", "c_zeros", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.rnns.lstm.TorchLSTM.d_state": [[52, 55], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "d_state", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "n_layers", "*", "self", ".", "d_model", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.rnns.lstm.TorchLSTM.d_output": [[56, 59], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "d_output", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "d_model", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.rnns.lstm.TorchLSTM.state_to_tensor": [[60, 66], ["einops.rearrange"], "methods", ["None"], ["", "@", "property", "\n", "def", "state_to_tensor", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "n_layers", "==", "1", ":", "\n", "            ", "return", "lambda", "state", ":", "state", "[", "0", "]", "\n", "", "else", ":", "\n", "            ", "return", "lambda", "state", ":", "rearrange", "(", "state", "[", "0", "]", ",", "'d b h -> b (d h)'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.rnns.qrnn.MemoryProjection.__init__": [[21, 34], ["torch.Module.__init__", "src.models.hippo.hippo.transition", "numpy.ones", "numpy.zeros", "scipy.signal.cont2discrete", "qrnn.MemoryProjection.register_buffer", "qrnn.MemoryProjection.register_buffer", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.s4.transition"], ["def", "__init__", "(", "self", ",", "order", ",", "measure", ",", "dt", ",", "discretization", "=", "'bilinear'", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "order", "=", "order", "\n", "A", ",", "B", "=", "transition", "(", "measure", ",", "order", ")", "\n", "C", "=", "np", ".", "ones", "(", "(", "1", ",", "order", ")", ")", "\n", "D", "=", "np", ".", "zeros", "(", "(", "1", ",", ")", ")", "\n", "# dt, discretization options", "\n", "A", ",", "B", ",", "_", ",", "_", ",", "_", "=", "signal", ".", "cont2discrete", "(", "(", "A", ",", "B", ",", "C", ",", "D", ")", ",", "dt", "=", "dt", ",", "method", "=", "discretization", ")", "\n", "\n", "\n", "# self.register_buffer('A', torch.Tensor(A-np.eye(self.order)))", "\n", "self", ".", "register_buffer", "(", "'A'", ",", "torch", ".", "Tensor", "(", "A", ")", ")", "\n", "self", ".", "register_buffer", "(", "'B'", ",", "torch", ".", "Tensor", "(", "B", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.rnns.qrnn.MemoryProjection.forward": [[35, 57], ["inputs.unsqueeze.unsqueeze.unsqueeze", "torch.linear", "torch.linear", "torch.linear", "src.models.functional.unroll.parallel_unroll_recursive"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.unroll.parallel_unroll_recursive"], ["", "def", "forward", "(", "self", ",", "inputs", ")", ":", "\n", "        ", "\"\"\"\n        inputs : (length, batch, size)\n        output : (length, batch, size, order)\n        # TODO this puts the unsqueeze inside here rather than outside, should make RNN versions the same\n        \"\"\"", "\n", "\n", "# L, B, S = inputs.shape", "\n", "inputs", "=", "inputs", ".", "unsqueeze", "(", "-", "1", ")", "\n", "u", "=", "F", ".", "linear", "(", "inputs", ",", "self", ".", "B", ")", "\n", "# output = unroll.unroll(self.A, u)", "\n", "output", "=", "unroll", ".", "parallel_unroll_recursive", "(", "self", ".", "A", ",", "u", ")", "\n", "# output = unroll.parallel_unroll_iterative(self.A, u)", "\n", "# output = unroll.variable_unroll(self.A, u, variable=False)", "\n", "\n", "# m = inputs.new_zeros(B, S, self.order)", "\n", "# outputs = []", "\n", "# for input in torch.unbind(inputs, dim=0):", "\n", "#     m = m + F.linear(m, self.A) + F.linear(input, self.B)", "\n", "\n", "# output = torch.stack(outputs, dim=0)", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.rnns.qrnn.VariableMemoryProjection.__init__": [[63, 84], ["torch.Module.__init__", "qrnn.VariableMemoryProjection.register_buffer", "qrnn.VariableMemoryProjection.register_buffer", "src.models.hippo.transition.LegSAdaptiveTransitionManual", "torch.eye", "torch.eye", "torch.eye", "torch.eye", "torch.eye", "torch.eye", "torch.eye", "torch.eye", "torch.eye", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "src.models.hippo.transition.LegTAdaptiveTransitionManual", "src.models.hippo.transition.LagTAdaptiveTransitionManual", "src.models.hippo.transition.TLagTAdaptiveTransitionManual"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__"], ["def", "__init__", "(", "self", ",", "order", "=", "1", ",", "measure", "=", "'legs'", ",", "dt", "=", "None", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "order", "=", "order", "\n", "self", ".", "measure", "=", "measure", "\n", "self", ".", "dt", "=", "dt", "\n", "\n", "# TODO incorporate measure", "\n", "if", "self", ".", "measure", "==", "'legs'", ":", "\n", "            ", "self", ".", "transition", "=", "LegSAdaptiveTransitionManual", "(", "self", ".", "order", ")", "\n", "", "elif", "self", ".", "measure", "==", "'legt'", ":", "\n", "            ", "self", ".", "transition", "=", "LegTAdaptiveTransitionManual", "(", "self", ".", "order", ")", "\n", "", "elif", "self", ".", "measure", "==", "'lagt'", ":", "\n", "            ", "self", ".", "transition", "=", "LagTAdaptiveTransitionManual", "(", "self", ".", "order", ")", "\n", "", "elif", "self", ".", "measure", "==", "'tlagt'", ":", "\n", "            ", "self", ".", "transition", "=", "TLagTAdaptiveTransitionManual", "(", "self", ".", "order", ")", "\n", "", "else", ":", "\n", "            ", "assert", "False", ",", "f\"VariableMemoryProjection: measure {measure} not allowed\"", "\n", "\n", "# Cached tensors", "\n", "", "self", ".", "register_buffer", "(", "'I'", ",", "torch", ".", "eye", "(", "self", ".", "order", ")", ")", "\n", "self", ".", "register_buffer", "(", "'zero'", ",", "torch", ".", "zeros", "(", "self", ".", "order", ",", "self", ".", "order", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.rnns.qrnn.VariableMemoryProjection.forward": [[85, 122], ["qrnn.VariableMemoryProjection.I[].repeat", "qrnn.VariableMemoryProjection.transition.bilinear", "As.permute.permute.permute", "qrnn.VariableMemoryProjection.transition.bilinear", "inputs.unsqueeze.unsqueeze.unsqueeze", "src.models.functional.unroll.variable_unroll", "inputs.unsqueeze.unsqueeze.new_zeros", "torch.cumsum", "torch.cumsum", "torch.cumsum", "torch.cumsum", "torch.cumsum", "torch.cumsum", "torch.cumsum", "torch.cumsum", "torch.cumsum", "torch.full().to", "torch.full().to", "torch.full().to", "torch.full().to", "torch.full().to", "torch.full().to", "torch.full().to", "torch.full().to", "torch.full().to", "inputs.unsqueeze.unsqueeze.new_ones", "torch.full", "torch.full", "torch.full", "torch.full", "torch.full", "torch.full", "torch.full", "torch.full", "torch.full"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.lssl.AdaptiveTransition.bilinear", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.lssl.AdaptiveTransition.bilinear", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.unroll.variable_unroll"], ["", "def", "forward", "(", "self", ",", "inputs", ",", "dt", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        inputs : (L, B, M)\n        dt     : (L, B, M)\n        output : (L, B, M, N) [length, batch, size, order]\n        # TODO this puts the input unsqueeze inside here rather than outside, should make RNN versions the same\n        \"\"\"", "\n", "\n", "L", ",", "B", ",", "M", "=", "inputs", ".", "shape", "\n", "\n", "# Construct discretization if necessary", "\n", "if", "dt", "is", "None", ":", "\n", "            ", "if", "self", ".", "dt", "is", "None", ":", "\n", "                ", "dt", "=", "torch", ".", "cumsum", "(", "inputs", ".", "new_ones", "(", "L", ")", ",", "dim", "=", "0", ")", "# no new_arange", "\n", "dt", "=", "(", "1.", "/", "dt", ")", "[", ":", ",", "None", ",", "None", "]", "# (L, 1, 1)", "\n", "", "else", ":", "\n", "                ", "dt", "=", "torch", ".", "full", "(", "(", "L", ",", "1", ",", "1", ")", ",", "self", ".", "dt", ")", ".", "to", "(", "inputs", ")", "# fixed dt", "\n", "\n", "# Create transition matrices", "\n", "# I = self.I[:, None, None, None, :].expand((self.order, L, B, M, self.order)) # (N, L, B, M, N)", "\n", "", "", "I", "=", "self", ".", "I", "[", ":", ",", "None", ",", "None", ",", "None", ",", ":", "]", ".", "repeat", "(", "(", "1", ",", "L", ",", "B", ",", "M", ",", "1", ")", ")", "# (N, L, B, M, N)", "\n", "As", "=", "self", ".", "transition", ".", "bilinear", "(", "dt", ",", "I", ",", "0", ")", "# (N, L, B, M, N) # NOTE due to the broadcasting here, the ManualTransition actually swaps axes back for efficiency; can potential save if this axis reordering is too slow [probably not a bottleneck]", "\n", "As", "=", "As", ".", "permute", "(", "(", "1", ",", "2", ",", "3", ",", "0", ",", "4", ")", ")", "# (L, B, M, N, N)", "\n", "# TODO this A might be transposed; should print to compare", "\n", "# print(As.shape)", "\n", "Bs", "=", "self", ".", "transition", ".", "bilinear", "(", "dt", ",", "inputs", ".", "new_zeros", "(", "self", ".", "order", ")", ",", "1", ")", "# (L, B, M, N)", "\n", "\n", "inputs", "=", "inputs", ".", "unsqueeze", "(", "-", "1", ")", "\n", "# u = F.linear(inputs, self.transition.B) # (L, B, M, N)", "\n", "# u = F.linear(inputs, Bs) # (L, B, M, N)", "\n", "u", "=", "inputs", "*", "Bs", "# (L, B, M, N)", "\n", "# output = unroll.unroll(self.A, u)", "\n", "# output = unroll.parallel_unroll_recursive(self.A, u)", "\n", "output", "=", "unroll", ".", "variable_unroll", "(", "As", ",", "u", ",", "variable", "=", "True", ")", "\n", "# output = unroll.parallel_unroll_iterative(self.A, u)", "\n", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.rnns.qrnn.ToeplitzMemoryProjection.__init__": [[124, 141], ["torch.Module.__init__", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "qrnn.ToeplitzMemoryProjection.register_buffer", "src.models.hippo.transition.LagTCumsumAdaptiveTransition", "measure_args.get", "measure_args.get", "GLagTCumsumAdaptiveTransition"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__"], ["    ", "def", "__init__", "(", "self", ",", "order", ",", "measure", ",", "measure_args", "=", "{", "}", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "N", "=", "order", "\n", "\n", "if", "measure", "==", "'lagt'", ":", "\n", "            ", "self", ".", "transition", "=", "LagTCumsumAdaptiveTransition", "(", "self", ".", "N", ")", "\n", "", "elif", "measure", "==", "'glagt'", ":", "\n", "# TODO this is broken", "\n", "            ", "alpha", "=", "measure_args", ".", "get", "(", "'alpha'", ",", "0.0", ")", "\n", "beta", "=", "measure_args", ".", "get", "(", "'beta'", ",", "0.01", ")", "\n", "self", ".", "transition", "=", "GLagTCumsumAdaptiveTransition", "(", "self", ".", "N", ",", "alpha", ",", "beta", ")", "\n", "", "else", ":", "\n", "            ", "assert", "False", ",", "f\"ToeplitzMemoryProjection: measure {measure} not supported\"", "\n", "\n", "", "e", "=", "torch", ".", "zeros", "(", "self", ".", "N", ")", "\n", "e", "[", "0", "]", "=", "1", "\n", "self", ".", "register_buffer", "(", "'e'", ",", "e", ")", "# the e_0 basis vector", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.rnns.qrnn.ToeplitzMemoryProjection.forward": [[142, 159], ["qrnn.ToeplitzMemoryProjection.e.repeat", "qrnn.ToeplitzMemoryProjection.transition.bilinear", "qrnn.ToeplitzMemoryProjection.transition.bilinear", "src.models.functional.unroll.variable_unroll_toeplitz", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.lssl.AdaptiveTransition.bilinear", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.lssl.AdaptiveTransition.bilinear", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.unroll.variable_unroll_toeplitz"], ["", "def", "forward", "(", "self", ",", "inputs", ",", "dt", ")", ":", "\n", "        ", "\"\"\"\n        inputs : (L, B, M)\n        dt     : (L, B, M)\n        output : (L, B, M, N) [length, batch, size, order]\n        # TODO this puts the unsqueeze inside here rather than outside, should make RNN versions the same\n        \"\"\"", "\n", "\n", "L", ",", "B", ",", "M", "=", "inputs", ".", "shape", "\n", "I", "=", "self", ".", "e", ".", "repeat", "(", "(", "L", ",", "B", ",", "M", ",", "1", ")", ")", "# (L, B, M, N)", "\n", "# I = self.e.repeat(inputs.shape+(1,)) # (L, B, M, N)", "\n", "As", "=", "self", ".", "transition", ".", "bilinear", "(", "dt", ",", "I", ",", "torch", ".", "zeros_like", "(", "dt", ")", ")", "# (L, B, M, N)", "\n", "# Bs = self.transition.bilinear(dt, torch.zeros_like(I), torch.ones_like(dt)) # (L, B, M, N)", "\n", "Bs", "=", "self", ".", "transition", ".", "bilinear", "(", "dt", ",", "torch", ".", "zeros_like", "(", "I", ")", ",", "inputs", ")", "# (L, B, M, N)", "\n", "output", "=", "unroll", ".", "variable_unroll_toeplitz", "(", "As", ",", "Bs", ",", "pad", "=", "False", ")", "\n", "# print(\"HERE\")", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.rnns.qrnn.HiPPOQRNN.__init__": [[163, 202], ["src.models.sequence.base.SequenceModule.__init__", "preact_ctor", "torch.Identity", "torch.Identity", "torch.Identity", "NotImplementedError", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "qrnn.MemoryProjection", "qrnn.ToeplitzMemoryProjection", "qrnn.VariableMemoryProjection"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "d_input", ",", "d_model", "=", "256", ",", "memory_size", "=", "1", ",", "memory_order", "=", "-", "1", ",", "\n", "variable", "=", "False", ",", "dt", "=", "0.01", ",", "\n", "measure", "=", "'lagt'", ",", "measure_args", "=", "{", "}", ",", "\n", "dropout", "=", "0.0", ",", "\n", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "if", "dropout", ">", "0.0", ":", "\n", "            ", "raise", "NotImplementedError", "(", "\"Dropout currently not supported for QRNN\"", ")", "\n", "\n", "", "if", "memory_order", "<", "0", ":", "\n", "            ", "memory_order", "=", "d_model", "\n", "", "self", ".", "d_input", "=", "d_input", "\n", "self", ".", "d_model", "=", "d_model", "\n", "self", ".", "memory_size", "=", "memory_size", "\n", "self", ".", "memory_order", "=", "memory_order", "\n", "self", ".", "variable", "=", "variable", "\n", "self", ".", "dt", "=", "dt", "\n", "\n", "# TODO deal with initializers", "\n", "\n", "preact_ctor", "=", "LinearActivation", "\n", "preact_args", "=", "[", "self", ".", "d_input", "+", "self", ".", "memory_size", "*", "self", ".", "memory_order", ",", "self", ".", "d_model", ",", "True", "]", "\n", "self", ".", "W_hmx", "=", "preact_ctor", "(", "*", "preact_args", ")", "\n", "\n", "if", "self", ".", "variable", ":", "\n", "            ", "self", ".", "W_uh", "=", "nn", ".", "Linear", "(", "self", ".", "d_input", ",", "2", "*", "self", ".", "memory_size", ")", "\n", "if", "measure", "in", "[", "'lagt'", ",", "'tlagt'", "]", ":", "\n", "                ", "self", ".", "memory_proj", "=", "ToeplitzMemoryProjection", "(", "memory_order", ",", "measure", ",", "measure_args", ")", "\n", "", "else", ":", "\n", "                ", "self", ".", "memory_proj", "=", "VariableMemoryProjection", "(", "memory_order", ",", "measure", ")", "\n", "", "", "else", ":", "\n", "            ", "self", ".", "W_uh", "=", "nn", ".", "Linear", "(", "self", ".", "d_input", ",", "self", ".", "memory_size", ")", "\n", "self", ".", "memory_proj", "=", "MemoryProjection", "(", "memory_order", ",", "measure", ",", "dt", ")", "\n", "\n", "", "self", ".", "hidden_activation_fn", "=", "torch", ".", "tanh", "\n", "self", ".", "memory_activation_fn", "=", "nn", ".", "Identity", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.rnns.qrnn.HiPPOQRNN.forward": [[204, 232], ["qrnn.HiPPOQRNN.memory_activation_fn", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "qrnn.HiPPOQRNN.hidden_activation_fn", "qrnn.HiPPOQRNN.W_uh", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "qrnn.HiPPOQRNN.memory_proj", "qrnn.HiPPOQRNN.memory_proj", "qrnn.HiPPOQRNN.W_hmx", "qrnn.HiPPOQRNN.view"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "inputs", ",", "return_output", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        inputs : [length, batch, dim]\n        \"\"\"", "\n", "L", ",", "B", ",", "d_input", "=", "inputs", ".", "shape", "\n", "assert", "d_input", "==", "self", ".", "d_input", "\n", "\n", "u", "=", "self", ".", "memory_activation_fn", "(", "self", ".", "W_uh", "(", "inputs", ")", ")", "# (L, B, memory_size)", "\n", "\n", "if", "self", ".", "variable", ":", "\n", "# Automatic scaling dt", "\n", "            ", "M", "=", "self", ".", "memory_size", "\n", "# dt = torch.full((L, 1, 1), self.dt).to(inputs) # fixed dt to test", "\n", "dt", "=", "torch", ".", "sigmoid", "(", "u", "[", "...", ",", "M", ":", "]", ")", "# variable dt", "\n", "u", "=", "u", "[", "...", ",", ":", "M", "]", "\n", "m", "=", "self", ".", "memory_proj", "(", "u", ",", "dt", ")", "\n", "", "else", ":", "\n", "            ", "m", "=", "self", ".", "memory_proj", "(", "u", ")", "# (L, B, M, N)", "\n", "\n", "\n", "", "mx", "=", "torch", ".", "cat", "(", "(", "m", ".", "view", "(", "L", ",", "B", ",", "self", ".", "memory_size", "*", "self", ".", "memory_order", ")", ",", "inputs", ")", ",", "dim", "=", "-", "1", ")", "# length, batch, d_input", "\n", "h", "=", "self", ".", "hidden_activation_fn", "(", "self", ".", "W_hmx", "(", "mx", ")", ")", "# length, batch, d_model", "\n", "\n", "\n", "if", "return_output", ":", "\n", "            ", "return", "h", ",", "h", "[", "-", "1", ",", "...", "]", "\n", "", "else", ":", "\n", "            ", "return", "None", ",", "h", "[", "-", "1", ",", "...", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.rnns.qrnn.HiPPOQRNN.default_state": [[233, 235], ["NotImplementedError"], "methods", ["None"], ["", "", "def", "default_state", "(", "self", ",", "x", ",", "batch_shape", ")", ":", "\n", "        ", "raise", "NotImplementedError", "(", "\"Needs to be implemented.\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.rnns.qrnn.HiPPOQRNN.step": [[236, 238], ["NotImplementedError"], "methods", ["None"], ["", "def", "step", "(", "self", ",", "x", ",", "state", ")", ":", "\n", "        ", "raise", "NotImplementedError", "(", "\"Needs to be implemented.\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.rnns.qrnn.HiPPOQRNN.d_state": [[239, 242], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "d_state", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "d_model", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.rnns.qrnn.HiPPOQRNN.d_output": [[243, 246], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "d_output", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "d_model", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.rnns.qrnn.HiPPOQRNN.state_to_tensor": [[247, 250], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "state_to_tensor", "(", "self", ")", ":", "\n", "        ", "return", "lambda", "state", ":", "state", "\n", "", "", ""]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.rnns.sru.SRUCell.default_initializers": [[21, 26], ["None"], "methods", ["None"], ["@", "property", "\n", "def", "default_initializers", "(", "self", ")", ":", "\n", "        ", "return", "{", "\n", "'fx'", ":", "'xavier'", ",", "\n", "'rx'", ":", "'xavier'", ",", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.rnns.sru.SRUCell.default_architecture": [[28, 32], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "default_architecture", "(", "self", ")", ":", "\n", "        ", "return", "{", "\n", "'bias'", ":", "True", ",", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.rnns.sru.SRUCell.__init__": [[34, 46], ["src.models.sequence.rnns.cells.CellBase.__init__"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__"], ["", "def", "__init__", "(", "\n", "self", ",", "d_input", ",", "d_model", ",", "\n", "skip", "=", "'H'", ",", "# Highway, Residual, None", "\n", "offset", "=", "True", ",", "# whether to use previous or current cell to compute highway gate", "\n", "**", "kwargs", "\n", ")", ":", "\n", "\n", "        ", "self", ".", "offset", "=", "offset", "\n", "self", ".", "skip", "=", "skip", "\n", "assert", "self", ".", "skip", "in", "[", "'H'", ",", "'R'", ",", "'N'", "]", "\n", "\n", "super", "(", ")", ".", "__init__", "(", "d_input", ",", "d_model", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.rnns.sru.SRUCell.reset_parameters": [[47, 63], ["src.models.nn.LinearActivation", "src.models.nn.LinearActivation", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "src.models.nn.LinearActivation", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Identity", "torch.Identity", "torch.Identity", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.s4.LinearActivation", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.s4.LinearActivation", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.s4.LinearActivation"], ["", "def", "reset_parameters", "(", "self", ")", ":", "\n", "        ", "self", ".", "W", "=", "LinearActivation", "(", "self", ".", "d_input", ",", "self", ".", "d_model", ",", "bias", "=", "self", ".", "architecture", "[", "'bias'", "]", ")", "\n", "# gate", "\n", "self", ".", "W_fx", "=", "LinearActivation", "(", "self", ".", "d_input", ",", "self", ".", "d_model", ",", "bias", "=", "True", ",", "initializer", "=", "self", ".", "initializers", "[", "'fx'", "]", ",", "activation", "=", "'sigmoid'", ")", "\n", "self", ".", "W_fc", "=", "nn", ".", "Parameter", "(", "torch", ".", "randn", "(", "self", ".", "d_model", ")", ")", "\n", "\n", "# highway", "\n", "if", "self", ".", "skip", "==", "'H'", ":", "\n", "            ", "self", ".", "W_rx", "=", "LinearActivation", "(", "self", ".", "d_input", ",", "self", ".", "d_model", ",", "bias", "=", "True", ",", "initializer", "=", "self", ".", "initializers", "[", "'rx'", "]", ",", "activation", "=", "'sigmoid'", ")", "\n", "self", ".", "W_rc", "=", "nn", ".", "Parameter", "(", "torch", ".", "randn", "(", "self", ".", "d_model", ")", ")", "\n", "\n", "# resize input", "\n", "", "if", "self", ".", "d_input", "!=", "self", ".", "d_model", ":", "\n", "            ", "self", ".", "skip_transform", "=", "nn", ".", "Linear", "(", "self", ".", "d_input", ",", "self", ".", "d_model", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "skip_transform", "=", "nn", ".", "Identity", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.rnns.sru.SRUCell.forward": [[65, 82], ["torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "sru.SRUCell.W_fx", "sru.SRUCell.W", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "sru.SRUCell.skip_transform", "sru.SRUCell.skip_transform", "sru.SRUCell.W_rx", "sru.SRUCell.W_rx"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "x", ",", "c", ")", ":", "\n", "### Update hidden state", "\n", "        ", "g", "=", "torch", ".", "sigmoid", "(", "self", ".", "W_fx", "(", "x", ")", "+", "self", ".", "W_fc", "*", "c", ")", "\n", "c_", "=", "(", "1.", "-", "g", ")", "*", "c", "+", "g", "*", "self", ".", "W", "(", "x", ")", "\n", "\n", "if", "self", ".", "skip", "==", "'H'", ":", "\n", "            ", "if", "self", ".", "offset", ":", "\n", "                ", "r", "=", "torch", ".", "sigmoid", "(", "self", ".", "W_rx", "(", "x", ")", "+", "self", ".", "W_rc", "*", "c", ")", "\n", "", "else", ":", "\n", "                ", "r", "=", "torch", ".", "sigmoid", "(", "self", ".", "W_rx", "(", "x", ")", "+", "self", ".", "W_rc", "*", "c_", ")", "\n", "", "h", "=", "(", "1", "-", "r", ")", "*", "self", ".", "skip_transform", "(", "x", ")", "+", "r", "*", "c_", "\n", "", "elif", "self", ".", "skip", "==", "'R'", ":", "\n", "            ", "h", "=", "c_", "+", "self", ".", "skip_transform", "(", "x", ")", "\n", "", "else", ":", "\n", "            ", "h", "=", "c_", "\n", "\n", "", "return", "h", ",", "c_", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.rnns.sru.SRURNNGate.__init__": [[85, 94], ["torch.Module.__init__", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__"], ["def", "__init__", "(", "self", ",", "d_model", ",", "feedback", "=", "True", ")", ":", "\n", "        ", "\"\"\"\n        feedback: control whether cell state feeds back into itself. If False, this is essentially a QRNN reduce\n        \"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "d_model", "=", "d_model", "\n", "self", ".", "feedback", "=", "feedback", "\n", "if", "self", ".", "feedback", ":", "\n", "            ", "self", ".", "W_fc", "=", "nn", ".", "Parameter", "(", "torch", ".", "randn", "(", "self", ".", "d_model", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.rnns.sru.SRURNNGate.forward": [[95, 112], ["torch.sigmoid.new_zeros", "torch.sigmoid.new_zeros", "torch.sigmoid.new_zeros", "zip", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.unbind", "torch.unbind", "torch.unbind", "torch.unbind", "torch.unbind", "torch.unbind", "torch.unbind", "torch.unbind", "torch.unbind", "torch.unbind", "torch.unbind", "torch.unbind", "torch.unbind", "torch.unbind", "torch.unbind", "torch.unbind", "torch.unbind", "torch.unbind", "cs.append", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "f", ",", "u", ")", ":", "\n", "        ", "\"\"\"\n        f, u: (batch, length, dim)\n        \"\"\"", "\n", "\n", "# If no feedback, batch the sigmoid computation", "\n", "if", "not", "self", ".", "feedback", ":", "\n", "            ", "f", "=", "torch", ".", "sigmoid", "(", "f", ")", "\n", "\n", "", "c", "=", "f", ".", "new_zeros", "(", "f", ".", "shape", "[", "...", ",", "1", ":", ",", ":", "]", ",", "requires_grad", "=", "False", ")", "\n", "cs", "=", "[", "]", "\n", "for", "f_", ",", "u_", "in", "zip", "(", "torch", ".", "unbind", "(", "f", ",", "dim", "=", "-", "2", ")", ",", "torch", ".", "unbind", "(", "u", ",", "dim", "=", "-", "2", ")", ")", ":", "\n", "            ", "if", "self", ".", "feedback", ":", "\n", "                ", "f_", "=", "torch", ".", "sigmoid", "(", "f_", "+", "self", ".", "W_fc", "*", "c", ")", "\n", "", "c", "=", "(", "1.", "-", "f_", ")", "*", "c", "+", "f_", "*", "u_", "\n", "cs", ".", "append", "(", "c", ")", "\n", "", "return", "torch", ".", "stack", "(", "cs", ",", "dim", "=", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.rnns.sru.SRURNN.__init__": [[117, 128], ["src.models.sequence.base.SequenceModule.__init__", "src.models.nn.LinearActivation", "sru.SRURNNGate", "NotImplementedError"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.s4.LinearActivation"], ["def", "__init__", "(", "self", ",", "d_input", ",", "d_model", ",", "feedback", "=", "True", ",", "return_output", "=", "True", ",", "dropout", "=", "0.0", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "d_input", "=", "d_input", "\n", "self", ".", "d_model", "=", "d_model", "\n", "self", ".", "return_output", "=", "return_output", "\n", "\n", "self", ".", "W_fused", "=", "LinearActivation", "(", "d_input", ",", "2", "*", "self", ".", "d_model", ",", "bias", "=", "True", ")", "\n", "self", ".", "C", "=", "SRURNNGate", "(", "d_model", ",", "feedback", ")", "\n", "\n", "if", "dropout", ">", "0.0", ":", "\n", "            ", "raise", "NotImplementedError", "(", "\"Dropout currently not supported for SRU\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.rnns.sru.SRURNN.forward": [[129, 139], ["sru.SRURNN.W_fused", "einops.rearrange", "torch.unbind", "torch.unbind", "torch.unbind", "torch.unbind", "torch.unbind", "torch.unbind", "torch.unbind", "torch.unbind", "torch.unbind", "sru.SRURNN.C"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "x", ",", "return_output", "=", "True", ")", ":", "\n", "        ", "ufr", "=", "self", ".", "W_fused", "(", "x", ")", "\n", "ufr", "=", "rearrange", "(", "ufr", ",", "'b l (c d) -> b l c d'", ",", "c", "=", "2", ")", "\n", "u", ",", "fx", "=", "torch", ".", "unbind", "(", "ufr", ",", "dim", "=", "2", ")", "# (B, L, H)", "\n", "c", "=", "self", ".", "C", "(", "fx", ",", "u", ")", "# (B, L, H)", "\n", "state", "=", "c", "[", "...", ",", "-", "1", ",", ":", "]", "\n", "if", "self", ".", "return_output", ":", "\n", "            ", "return", "c", ",", "state", "\n", "", "else", ":", "\n", "            ", "return", "None", ",", "state", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.rnns.sru.SRURNN.d_state": [[140, 143], ["None"], "methods", ["None"], ["", "", "@", "property", "\n", "def", "d_state", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "d_model", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.rnns.sru.SRURNN.d_output": [[144, 147], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "d_output", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "d_model", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.rnns.sru.SRURNN.state_to_tensor": [[148, 151], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "state_to_tensor", "(", "self", ")", ":", "\n", "        ", "return", "lambda", "state", ":", "state", "\n", "# TODO haven't checked the default_state, step functions", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.cells.hippo.HiPPOLTICell.__init__": [[13, 23], ["src.models.hippo.hippo.transition", "src.models.sequence.rnns.cells.memory.LTICell.__init__", "type"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.s4.transition", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__"], ["A", "=", "np", ".", "pad", "(", "A", ",", "(", "(", "0", ",", "0", ")", ",", "(", "0", ",", "1", ")", ",", "(", "0", ",", "0", ")", ",", "(", "0", ",", "1", ")", ")", ")", "+", "np", ".", "pad", "(", "A", ",", "(", "(", "0", ",", "0", ")", ",", "(", "1", ",", "0", ")", ",", "(", "0", ",", "0", ")", ",", "(", "1", ",", "0", ")", ")", ")", "\n", "return", "rearrange", "(", "A", ",", "'m x n y -> (m x) (n y)'", ")", "\n", "\n", "# TODO take in 'torch' option to return torch instead of numpy, which converts the shape of B from (N, 1) to (N)", "\n", "# TODO remove tlagt", "\n", "", "def", "transition", "(", "measure", ",", "N", ",", "**", "measure_args", ")", ":", "\n", "    "]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.cells.hippo.HiPPOLSICell.__init__": [[27, 37], ["src.models.hippo.hippo.transition", "src.models.sequence.rnns.cells.memory.LSICell.__init__", "type"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.s4.transition", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__"], ["\n", "# Laguerre (translated)", "\n", "if", "measure", "==", "'lagt'", ":", "\n", "        ", "b", "=", "measure_args", ".", "get", "(", "'beta'", ",", "1.0", ")", "\n", "A", "=", "np", ".", "eye", "(", "N", ")", "/", "2", "-", "np", ".", "tril", "(", "np", ".", "ones", "(", "(", "N", ",", "N", ")", ")", ")", "\n", "B", "=", "b", "*", "np", ".", "ones", "(", "(", "N", ",", "1", ")", ")", "\n", "", "elif", "measure", "==", "'tlagt'", ":", "\n", "# beta = 1 corresponds to no tilt", "\n", "        ", "b", "=", "measure_args", ".", "get", "(", "'beta'", ",", "1.0", ")", "\n", "A", "=", "(", "1.", "-", "b", ")", "/", "2", "*", "np", ".", "eye", "(", "N", ")", "-", "np", ".", "tril", "(", "np", ".", "ones", "(", "(", "N", ",", "N", ")", ")", ")", "\n", "B", "=", "b", "*", "np", ".", "ones", "(", "(", "N", ",", "1", ")", ")", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.cells.hippo.LagTCell.__init__": [[53, 55], ["hippo.HiPPOLTICell.__init__"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__"], ["R", "=", "(", "2", "*", "Q", "+", "1", ")", "**", ".5", "\n", "j", ",", "i", "=", "np", ".", "meshgrid", "(", "Q", ",", "Q", ")", "\n", "A", "=", "R", "[", ":", ",", "None", "]", "*", "np", ".", "where", "(", "i", "<", "j", ",", "(", "-", "1.", ")", "**", "(", "i", "-", "j", ")", ",", "1", ")", "*", "R", "[", "None", ",", ":", "]", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.cells.hippo.GLagTCell.__init__": [[61, 63], ["hippo.HiPPOLTICell.__init__"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__"], ["A", "*=", "0.5", "\n", "B", "*=", "0.5", "\n", "# LMU: equivalent to LegT up to normalization", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.cells.hippo.LMUCell.default_initializers": [[72, 83], ["None"], "methods", ["None"], ["        ", "q", "=", "np", ".", "arange", "(", "N", ",", "dtype", "=", "np", ".", "float64", ")", "\n", "col", ",", "row", "=", "np", ".", "meshgrid", "(", "q", ",", "q", ")", "\n", "r", "=", "2", "*", "q", "+", "1", "\n", "M", "=", "-", "(", "np", ".", "where", "(", "row", ">=", "col", ",", "r", ",", "0", ")", "-", "np", ".", "diag", "(", "q", ")", ")", "\n", "T", "=", "np", ".", "sqrt", "(", "np", ".", "diag", "(", "2", "*", "q", "+", "1", ")", ")", "\n", "A", "=", "T", "@", "M", "@", "np", ".", "linalg", ".", "inv", "(", "T", ")", "\n", "B", "=", "np", ".", "diag", "(", "T", ")", "[", ":", ",", "None", "]", "\n", "B", "=", "B", ".", "copy", "(", ")", "# Otherwise \"UserWarning: given NumPY array is not writeable...\" after torch.as_tensor(B)", "\n", "", "elif", "measure", "==", "'legsd'", ":", "\n", "        ", "q", "=", "np", ".", "arange", "(", "N", ",", "dtype", "=", "np", ".", "float64", ")", "\n", "col", ",", "row", "=", "np", ".", "meshgrid", "(", "q", ",", "q", ")", "\n", "r", "=", "2", "*", "q", "+", "1", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.cells.hippo.LMUCell.default_architecture": [[85, 94], ["None"], "methods", ["None"], ["T", "=", "np", ".", "sqrt", "(", "np", ".", "diag", "(", "2", "*", "q", "+", "1", ")", ")", "\n", "A", "=", "T", "@", "M", "@", "np", ".", "linalg", ".", "inv", "(", "T", ")", "\n", "B", "=", "np", ".", "diag", "(", "T", ")", "[", ":", ",", "None", "]", "\n", "B", "=", "B", ".", "copy", "(", ")", "# Otherwise \"UserWarning: given NumPY array is not writeable...\" after torch.as_tensor(B)", "\n", "A", "+=", ".5", "*", "B", "*", "B", "[", "None", ",", ":", ",", "0", "]", "\n", "B", "=", "B", "/", "2.0", "\n", "", "elif", "measure", "==", "'fourier_old'", ":", "\n", "        ", "freqs", "=", "np", ".", "arange", "(", "N", "//", "2", ")", "\n", "d", "=", "np", ".", "stack", "(", "[", "freqs", ",", "np", ".", "zeros", "(", "N", "//", "2", ")", "]", ",", "axis", "=", "-", "1", ")", ".", "reshape", "(", "-", "1", ")", "[", ":", "-", "1", "]", "\n", "A", "=", "2", "*", "np", ".", "pi", "*", "(", "np", ".", "diag", "(", "d", ",", "1", ")", "-", "np", ".", "diag", "(", "d", ",", "-", "1", ")", ")", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.cells.hippo.LMUCell.__init__": [[96, 98], ["hippo.HiPPOLTICell.__init__"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__"], ["B", "=", "embed_c2r", "(", "np", ".", "ones", "(", "(", "N", "//", "2", ",", "1", ")", ")", ")", "[", "...", ",", ":", "1", "]", "\n", "", "elif", "measure", "==", "'fourier_diag'", ":", "\n", "        ", "freqs", "=", "np", ".", "arange", "(", "N", "//", "2", ")", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.cells.hippo.RandomCell.__init__": [[104, 116], ["numpy.random.normal", "src.models.sequence.rnns.cells.memory.LTICell.__init__", "numpy.random.normal"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__"], ["", "elif", "measure", "==", "'fourier'", ":", "\n", "        ", "freqs", "=", "np", ".", "arange", "(", "N", "//", "2", ")", "\n", "d", "=", "np", ".", "stack", "(", "[", "np", ".", "zeros", "(", "N", "//", "2", ")", ",", "freqs", "]", ",", "axis", "=", "-", "1", ")", ".", "reshape", "(", "-", "1", ")", "[", "1", ":", "]", "\n", "A", "=", "np", ".", "pi", "*", "(", "-", "np", ".", "diag", "(", "d", ",", "1", ")", "+", "np", ".", "diag", "(", "d", ",", "-", "1", ")", ")", "\n", "B", "=", "np", ".", "zeros", "(", "N", ")", "\n", "B", "[", "0", ":", ":", "2", "]", "=", "2", "**", ".5", "\n", "B", "[", "0", "]", "=", "1", "\n", "\n", "# Subtract off rank correction - this corresponds to the other endpoint u(t-1) in this case", "\n", "A", "=", "A", "-", "B", "[", ":", ",", "None", "]", "*", "B", "[", "None", ",", ":", "]", "\n", "B", "=", "B", "[", ":", ",", "None", "]", "\n", "", "elif", "measure", "==", "'fourier_decay'", ":", "\n", "        ", "freqs", "=", "np", ".", "arange", "(", "N", "//", "2", ")", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.cells.timestamp.TimeMemoryCell.__init__": [[24, 60], ["src.models.sequence.rnns.cells.memory.MemoryCell.__init__", "functools.partial", "functools.partial", "functools.partial"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "d_input", ",", "d_model", ",", "memory_size", ",", "memory_order", ",", "\n", "measure", "=", "'legs'", ",", "\n", "method", "=", "'trid'", ",", "\n", "discretization", "=", "'bilinear'", ",", "\n", "**", "kwargs", "\n", ")", ":", "\n", "        ", "if", "memory_order", "<", "0", ":", "\n", "            ", "memory_order", "=", "d_model", "\n", "\n", "", "super", "(", ")", ".", "__init__", "(", "d_input", "-", "1", ",", "d_model", ",", "memory_size", ",", "memory_order", ",", "**", "kwargs", ")", "\n", "\n", "assert", "measure", "in", "[", "'legs'", ",", "'lagt'", ",", "'legt'", "]", "\n", "assert", "method", "in", "[", "'dense'", ",", "'trid'", "]", "\n", "transitions", "=", "{", "\n", "'dense'", ":", "{", "\n", "'legs'", ":", "LegSAdaptiveTransitionManual", ",", "\n", "'legt'", ":", "LegTAdaptiveTransitionManual", ",", "\n", "'lagt'", ":", "LagTAdaptiveTransitionManual", ",", "\n", "}", ",", "\n", "'trid'", ":", "{", "\n", "'legs'", ":", "LegSTriDInverseAdaptiveTransition", ",", "\n", "'legt'", ":", "LegTTriDInverseAdaptiveTransition", ",", "\n", "'lagt'", ":", "LagTTriDInverseAdaptiveTransition", ",", "\n", "}", ",", "\n", "}", "\n", "self", ".", "transition", "=", "transitions", "[", "method", "]", "[", "measure", "]", "(", "self", ".", "memory_order", ")", "\n", "\n", "if", "discretization", "in", "forward_aliases", ":", "\n", "            ", "self", ".", "transition_fn", "=", "partial", "(", "self", ".", "transition", ".", "forward_diff", ",", "**", "kwargs", ")", "\n", "", "elif", "discretization", "in", "backward_aliases", ":", "\n", "            ", "self", ".", "transition_fn", "=", "partial", "(", "self", ".", "transition", ".", "backward_diff", ",", "**", "kwargs", ")", "\n", "", "elif", "discretization", "in", "bilinear_aliases", ":", "\n", "            ", "self", ".", "transition_fn", "=", "partial", "(", "self", ".", "transition", ".", "bilinear", ",", "**", "kwargs", ")", "\n", "", "else", ":", "assert", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.cells.timestamp.TimeMemoryCell.update_memory": [[61, 64], ["None"], "methods", ["None"], ["", "def", "update_memory", "(", "self", ",", "m", ",", "u", ",", "t0", ",", "t1", ")", ":", "\n", "        ", "\"\"\" This class is intended to be subclassed to the LTI or LSI cases \"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.cells.timestamp.TimeMemoryCell.forward": [[65, 80], ["timestamp.TimeMemoryCell.forward_memory", "timestamp.TimeMemoryCell.update_memory", "timestamp.TimeMemoryCell.forward_hidden", "timestamp.TimeMemoryCell.state_to_tensor"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.cells.memory.MemoryCell.forward_memory", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.cells.memory.LSICell.update_memory", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.cells.memory.MemoryCell.forward_hidden", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.s4.S4.state_to_tensor"], ["", "def", "forward", "(", "self", ",", "input", ",", "state", ")", ":", "\n", "        ", "h", ",", "m", ",", "prev_timestamp", "=", "state", "\n", "timestamp", ",", "input", "=", "input", "[", ":", ",", "0", "]", ",", "input", "[", ":", ",", "1", ":", "]", "\n", "\n", "# Update the memory", "\n", "u", "=", "self", ".", "forward_memory", "(", "input", ",", "h", ",", "m", ")", "\n", "m", "=", "self", ".", "update_memory", "(", "m", ",", "u", ",", "prev_timestamp", ",", "timestamp", ")", "# (batch, memory_size, memory_order)", "\n", "\n", "# Update hidden", "\n", "h", "=", "self", ".", "forward_hidden", "(", "input", ",", "h", ",", "m", ")", "\n", "\n", "next_state", "=", "(", "h", ",", "m", ",", "timestamp", ")", "\n", "output", "=", "self", ".", "state_to_tensor", "(", "next_state", ")", "\n", "\n", "return", "output", ",", "next_state", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.cells.timestamp.TimeLSICell.update_memory": [[89, 103], ["torch.eq().any", "torch.eq().any", "torch.eq().any", "torch.eq().any", "torch.eq().any", "torch.eq().any", "torch.eq().any", "torch.eq().any", "torch.eq().any", "torch.pad", "torch.pad", "torch.pad", "timestamp.TimeLSICell.transition_fn", "torch.eq", "torch.eq", "torch.eq", "torch.eq", "torch.eq", "torch.eq", "torch.eq", "torch.eq", "torch.eq", "u.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.sc.pad", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.sc.pad", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.sc.pad"], ["def", "update_memory", "(", "self", ",", "m", ",", "u", ",", "t0", ",", "t1", ")", ":", "\n", "        ", "\"\"\"\n        m: (B, M, N) [batch, memory_size, memory_order]\n        u: (B, M)\n        t0: (B,) previous time\n        t1: (B,) current time\n        \"\"\"", "\n", "\n", "if", "torch", ".", "eq", "(", "t1", ",", "0.", ")", ".", "any", "(", ")", ":", "\n", "            ", "return", "F", ".", "pad", "(", "u", ".", "unsqueeze", "(", "-", "1", ")", ",", "(", "0", ",", "self", ".", "memory_order", "-", "1", ")", ")", "\n", "", "else", ":", "\n", "            ", "dt", "=", "(", "(", "t1", "-", "t0", ")", "/", "t1", ")", ".", "unsqueeze", "(", "-", "1", ")", "\n", "m", "=", "self", ".", "transition_fn", "(", "dt", ",", "m", ",", "u", ")", "\n", "", "return", "m", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.cells.timestamp.TimeLTICell.__init__": [[113, 126], ["timestamp.TimeMemoryCell.__init__"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "d_input", ",", "d_model", ",", "memory_size", "=", "1", ",", "memory_order", "=", "-", "1", ",", "\n", "measure", "=", "'legt'", ",", "\n", "dt", "=", "1.0", ",", "\n", "**", "kwargs", "\n", ")", ":", "\n", "        ", "if", "memory_order", "<", "0", ":", "\n", "            ", "memory_order", "=", "d_model", "\n", "\n", "", "self", ".", "dt", "=", "dt", "\n", "\n", "super", "(", ")", ".", "__init__", "(", "d_input", ",", "d_model", ",", "memory_size", ",", "memory_order", ",", "measure", "=", "measure", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.cells.timestamp.TimeLTICell.update_memory": [[127, 138], ["timestamp.TimeLTICell.transition_fn"], "methods", ["None"], ["", "def", "update_memory", "(", "self", ",", "m", ",", "u", ",", "t0", ",", "t1", ")", ":", "\n", "        ", "\"\"\"\n        m: (B, M, N) [batch, memory_size, memory_order]\n        u: (B, M)\n        t0: (B,) previous time\n        t1: (B,) current time\n        \"\"\"", "\n", "\n", "dt", "=", "self", ".", "dt", "*", "(", "t1", "-", "t0", ")", ".", "unsqueeze", "(", "-", "1", ")", "\n", "m", "=", "self", ".", "transition_fn", "(", "dt", ",", "m", ",", "u", ")", "\n", "return", "m", "\n", "", "", ""]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.cells.minimalrnn.MinimalRNNCell.default_initializers": [[17, 21], ["None"], "methods", ["None"], ["@", "property", "\n", "def", "default_initializers", "(", "self", ")", ":", "\n", "        ", "return", "{", "\n", "'hx'", ":", "'xavier'", ",", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.cells.minimalrnn.MinimalRNNCell.default_architecture": [[23, 27], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "default_architecture", "(", "self", ")", ":", "\n", "        ", "return", "{", "\n", "'bias'", ":", "True", ",", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.cells.minimalrnn.MinimalRNNCell.__init__": [[30, 41], ["src.models.sequence.rnns.cells.basic.CellBase.__init__"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__"], ["", "def", "__init__", "(", "\n", "self", ",", "d_input", ",", "d_model", ",", "\n", "hidden_activation", "=", "'tanh'", ",", "\n", "zero_bias_init", "=", "False", ",", "\n", "**", "kwargs", "\n", ")", ":", "\n", "\n", "        ", "self", ".", "hidden_activation", "=", "hidden_activation", "\n", "self", ".", "zero_bias_init", "=", "zero_bias_init", "\n", "\n", "super", "(", ")", ".", "__init__", "(", "d_input", ",", "d_model", ",", "**", "kwargs", ",", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.cells.minimalrnn.MinimalRNNCell.reset_parameters": [[42, 55], ["src.models.nn.LinearActivation", "src.models.nn.gate.Gate"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.s4.LinearActivation"], ["", "def", "reset_parameters", "(", "self", ")", ":", "\n", "        ", "self", ".", "W_hx", "=", "LinearActivation", "(", "\n", "self", ".", "d_input", ",", "self", ".", "d_model", ",", "\n", "bias", "=", "self", ".", "architecture", "[", "'bias'", "]", ",", "zero_bias_init", "=", "self", ".", "zero_bias_init", ",", "\n", "initializer", "=", "self", ".", "initializers", "[", "'hx'", "]", ",", "activation", "=", "self", ".", "hidden_activation", ",", "\n", "activate", "=", "True", ",", "\n", ")", "\n", "# get_initializer(self.initializers['hx'], self.hidden_activation)(self.W_hx.weight)", "\n", "# self.hidden_activation_fn = Activate(self.hidden_activation, self.d_model)", "\n", "\n", "preact_ctor", "=", "LinearActivation", "\n", "preact_args", "=", "[", "self", ".", "d_input", "+", "self", ".", "d_model", ",", "self", ".", "d_model", ",", "self", ".", "architecture", "[", "'bias'", "]", "]", "\n", "self", ".", "W_g", "=", "Gate", "(", "self", ".", "d_model", ",", "preact_ctor", ",", "preact_args", ",", "mechanism", "=", "'G'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.cells.minimalrnn.MinimalRNNCell.forward": [[57, 67], ["minimalrnn.MinimalRNNCell.W_hx", "torch.cat", "minimalrnn.MinimalRNNCell.W_g"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input", ",", "h", ")", ":", "\n", "# Update hidden state", "\n", "# hidden_preact = self.W_hx(input)", "\n", "# hidden = self.hidden_activation_fn(hidden_preact)", "\n", "        ", "hidden", "=", "self", ".", "W_hx", "(", "input", ")", "\n", "hx", "=", "torch", ".", "cat", "(", "(", "input", ",", "h", ")", ",", "dim", "=", "-", "1", ")", "\n", "g", "=", "self", ".", "W_g", "(", "hx", ")", "\n", "h", "=", "(", "1.", "-", "g", ")", "*", "h", "+", "g", "*", "hidden", "\n", "\n", "return", "h", ",", "h", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.cells.basic.CellBase.__init_subclass__": [[20, 25], ["src.models.sequence.base.SequenceModule.__init_subclass__", "hasattr"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.cells.basic.CellBase.__init_subclass__"], ["def", "__init_subclass__", "(", "cls", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init_subclass__", "(", "**", "kwargs", ")", "\n", "# Only register classes with @name attribute", "\n", "if", "hasattr", "(", "cls", ",", "'name'", ")", "and", "cls", ".", "name", "is", "not", "None", ":", "\n", "            ", "cls", ".", "registry", "[", "cls", ".", "name", "]", "=", "cls", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.cells.basic.CellBase.default_initializers": [[29, 32], ["None"], "methods", ["None"], ["@", "property", "\n", "def", "default_initializers", "(", "self", ")", ":", "\n", "        ", "return", "{", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.cells.basic.CellBase.default_architecture": [[33, 36], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "default_architecture", "(", "self", ")", ":", "\n", "        ", "return", "{", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.cells.basic.CellBase.__init__": [[37, 54], ["src.models.sequence.base.SequenceModule.__init__", "set().issubset", "set().issubset", "basic.CellBase.reset_parameters", "basic.CellBase.initializers.update", "print", "basic.CellBase.architecture.update", "set", "set", "basic.CellBase.initializers.keys", "basic.CellBase.architecture.keys"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.cells.memory.MemoryCell.reset_parameters"], ["", "def", "__init__", "(", "self", ",", "d_input", ",", "d_model", ",", "initializers", "=", "None", ",", "architecture", "=", "None", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "d_input", "=", "d_input", "\n", "self", ".", "d_model", "=", "d_model", "\n", "\n", "self", ".", "architecture", "=", "self", ".", "default_architecture", "\n", "self", ".", "initializers", "=", "self", ".", "default_initializers", "\n", "if", "initializers", "is", "not", "None", ":", "\n", "            ", "self", ".", "initializers", ".", "update", "(", "initializers", ")", "\n", "print", "(", "\"Initializers:\"", ",", "initializers", ")", "\n", "", "if", "architecture", "is", "not", "None", ":", "\n", "            ", "self", ".", "architecture", ".", "update", "(", "architecture", ")", "\n", "\n", "", "assert", "set", "(", "self", ".", "initializers", ".", "keys", "(", ")", ")", ".", "issubset", "(", "self", ".", "valid_keys", ")", "\n", "assert", "set", "(", "self", ".", "architecture", ".", "keys", "(", ")", ")", ".", "issubset", "(", "self", ".", "valid_keys", ")", "\n", "\n", "self", ".", "reset_parameters", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.cells.basic.CellBase.reset_parameters": [[55, 57], ["None"], "methods", ["None"], ["", "def", "reset_parameters", "(", "self", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.cells.basic.CellBase.forward": [[58, 61], ["None"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input", ",", "hidden", ")", ":", "\n", "        ", "\"\"\" Returns output, next_state \"\"\"", "\n", "return", "input", ",", "input", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.cells.basic.CellBase.default_state": [[62, 67], ["torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros"], "methods", ["None"], ["", "def", "default_state", "(", "self", ",", "*", "batch_shape", ",", "device", "=", "None", ")", ":", "\n", "        ", "return", "torch", ".", "zeros", "(", "\n", "*", "batch_shape", ",", "self", ".", "d_model", ",", "\n", "device", "=", "device", ",", "\n", "requires_grad", "=", "False", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.cells.basic.CellBase.step": [[69, 71], ["basic.CellBase.forward"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.models.resnext.CifarResNeXt.forward"], ["", "def", "step", "(", "self", ",", "x", ",", "state", ")", ":", "\n", "        ", "return", "self", ".", "forward", "(", "x", ",", "state", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.cells.basic.CellBase.state_to_tensor": [[72, 75], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "state_to_tensor", "(", "self", ")", ":", "\n", "        ", "return", "lambda", "state", ":", "state", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.cells.basic.CellBase.d_state": [[76, 79], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "d_state", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "d_model", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.cells.basic.CellBase.d_output": [[80, 83], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "d_output", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "d_model", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.cells.basic.RNNCell.__init__": [[99, 114], ["basic.CellBase.__init__"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__"], ["def", "__init__", "(", "\n", "self", ",", "d_input", ",", "d_model", ",", "\n", "hidden_activation", "=", "'tanh'", ",", "\n", "orthogonal", "=", "False", ",", "\n", "ortho_args", "=", "None", ",", "\n", "zero_bias_init", "=", "False", ",", "\n", "**", "kwargs", "\n", ")", ":", "\n", "\n", "        ", "self", ".", "hidden_activation", "=", "hidden_activation", "\n", "self", ".", "orthogonal", "=", "orthogonal", "\n", "self", ".", "ortho_args", "=", "ortho_args", "\n", "self", ".", "zero_bias_init", "=", "zero_bias_init", "\n", "\n", "super", "(", ")", ".", "__init__", "(", "d_input", ",", "d_model", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.cells.basic.RNNCell.reset_parameters": [[115, 128], ["src.models.nn.LinearActivation", "src.models.nn.Activation", "basic.RNNCell.reset_hidden_to_hidden"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.s4.LinearActivation", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.s4.Activation", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.cells.basic.RNNCell.reset_hidden_to_hidden"], ["", "def", "reset_parameters", "(", "self", ")", ":", "\n", "        ", "self", ".", "W_hx", "=", "LinearActivation", "(", "\n", "self", ".", "d_input", ",", "self", ".", "d_model", ",", "\n", "bias", "=", "self", ".", "architecture", "[", "'bias'", "]", ",", "\n", "zero_bias_init", "=", "self", ".", "zero_bias_init", ",", "\n", "initializer", "=", "self", ".", "initializers", "[", "'hx'", "]", ",", "\n", "activation", "=", "self", ".", "hidden_activation", ",", "\n", "# apply_activation=False,", "\n", "activate", "=", "False", ",", "\n", ")", "\n", "self", ".", "activate", "=", "Activation", "(", "self", ".", "hidden_activation", ",", "self", ".", "d_model", ")", "\n", "\n", "self", ".", "reset_hidden_to_hidden", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.cells.basic.RNNCell.reset_hidden_to_hidden": [[129, 147], ["src.models.nn.orthogonal.OrthogonalLinear", "src.models.nn.LinearActivation"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.s4.LinearActivation"], ["", "def", "reset_hidden_to_hidden", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "orthogonal", ":", "\n", "\n", "            ", "if", "self", ".", "ortho_args", "is", "None", ":", "\n", "                ", "self", ".", "ortho_args", "=", "{", "}", "\n", "", "self", ".", "ortho_args", "[", "'d_input'", "]", "=", "self", ".", "d_model", "\n", "self", ".", "ortho_args", "[", "'d_output'", "]", "=", "self", ".", "d_model", "\n", "\n", "self", ".", "W_hh", "=", "OrthogonalLinear", "(", "**", "self", ".", "ortho_args", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "W_hh", "=", "LinearActivation", "(", "\n", "self", ".", "d_model", ",", "self", ".", "d_model", ",", "\n", "bias", "=", "self", ".", "architecture", "[", "'bias'", "]", ",", "\n", "zero_bias_init", "=", "self", ".", "zero_bias_init", ",", "\n", "initializer", "=", "self", ".", "initializers", "[", "'hh'", "]", ",", "\n", "activation", "=", "self", ".", "hidden_activation", ",", "\n", "# apply_activation=False,", "\n", "activate", "=", "False", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.cells.basic.RNNCell.forward": [[151, 157], ["basic.RNNCell.activate", "basic.RNNCell.W_hx", "basic.RNNCell.W_hh"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "input", ",", "h", ")", ":", "\n", "# Update hidden state", "\n", "        ", "hidden_preact", "=", "self", ".", "W_hx", "(", "input", ")", "+", "self", ".", "W_hh", "(", "h", ")", "\n", "hidden", "=", "self", ".", "activate", "(", "hidden_preact", ")", "\n", "\n", "return", "hidden", ",", "hidden", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.cells.basic.GatedRNNCell.__init__": [[161, 170], ["basic.RNNCell.__init__"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__"], ["def", "__init__", "(", "\n", "self", ",", "d_input", ",", "d_model", ",", "\n", "gate", "=", "'G'", ",", "# 'N' | 'G' | 'R' | 'UR'", "\n", "reset", "=", "'G'", ",", "\n", "**", "kwargs", "\n", ")", ":", "\n", "        ", "self", ".", "gate", "=", "gate", "\n", "self", ".", "reset", "=", "reset", "\n", "super", "(", ")", ".", "__init__", "(", "d_input", ",", "d_model", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.cells.basic.GatedRNNCell.reset_parameters": [[171, 180], ["basic.RNNCell.reset_parameters", "src.models.nn.gate.Gate", "src.models.nn.gate.Gate"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.cells.memory.MemoryCell.reset_parameters"], ["", "def", "reset_parameters", "(", "self", ")", ":", "\n", "        ", "super", "(", ")", ".", "reset_parameters", "(", ")", "\n", "# self.reset_gate()", "\n", "\n", "# def reset_gate(self):", "\n", "preact_ctor", "=", "LinearActivation", "\n", "preact_args", "=", "[", "self", ".", "d_input", "+", "self", ".", "d_model", ",", "self", ".", "d_model", ",", "self", ".", "architecture", "[", "'bias'", "]", "]", "\n", "self", ".", "W_g", "=", "Gate", "(", "self", ".", "d_model", ",", "preact_ctor", ",", "preact_args", ",", "mechanism", "=", "self", ".", "gate", ")", "\n", "self", ".", "W_reset", "=", "Gate", "(", "self", ".", "d_model", ",", "preact_ctor", ",", "preact_args", ",", "mechanism", "=", "self", ".", "reset", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.cells.basic.GatedRNNCell.forward": [[181, 191], ["torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "basic.GatedRNNCell.W_reset", "basic.RNNCell.forward", "basic.GatedRNNCell.W_g"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.models.resnext.CifarResNeXt.forward"], ["", "def", "forward", "(", "self", ",", "input", ",", "h", ")", ":", "\n", "        ", "hx", "=", "torch", ".", "cat", "(", "(", "input", ",", "h", ")", ",", "dim", "=", "-", "1", ")", "\n", "reset", "=", "self", ".", "W_reset", "(", "hx", ")", "\n", "\n", "_", ",", "update", "=", "super", "(", ")", ".", "forward", "(", "input", ",", "reset", "*", "h", ")", "\n", "\n", "g", "=", "self", ".", "W_g", "(", "hx", ")", "\n", "h", "=", "(", "1.", "-", "g", ")", "*", "h", "+", "g", "*", "update", "\n", "\n", "return", "h", ",", "h", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.cells.basic.ExpRNNCell.__init__": [[200, 202], ["basic.RNNCell.__init__"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__"], ["def", "__init__", "(", "self", ",", "d_input", ",", "d_model", ",", "orthogonal", "=", "True", ",", "hidden_activation", "=", "'modrelu'", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "d_input", ",", "d_model", ",", "orthogonal", "=", "orthogonal", ",", "hidden_activation", "=", "hidden_activation", ",", "**", "kwargs", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.cells.memory.MemoryCell.default_initializers": [[31, 38], ["None"], "methods", ["None"], ["@", "property", "\n", "def", "default_initializers", "(", "self", ")", ":", "\n", "        ", "return", "{", "\n", "'uxh'", ":", "'uniform'", ",", "\n", "'hxm'", ":", "'xavier'", ",", "\n", "'um'", ":", "'zero'", ",", "\n", "'hh'", ":", "'xavier'", ",", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.cells.memory.MemoryCell.default_architecture": [[41, 49], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "default_architecture", "(", "self", ")", ":", "\n", "        ", "return", "{", "\n", "'ux'", ":", "True", ",", "\n", "'hx'", ":", "True", ",", "\n", "'hm'", ":", "True", ",", "\n", "'hh'", ":", "False", ",", "\n", "'bias'", ":", "True", ",", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.cells.memory.MemoryCell.__init__": [[52, 110], ["src.models.sequence.rnns.cells.basic.RNNCell.__init__", "src.models.nn.components.LinearActivation", "src.models.nn.components.LinearActivation", "memory.MemoryCell.reset_hidden_to_hidden", "src.models.nn.gate.Gate", "print"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.s4.LinearActivation", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.s4.LinearActivation", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.cells.basic.RNNCell.reset_hidden_to_hidden"], ["", "def", "__init__", "(", "\n", "self", ",", "d_input", ",", "d_model", ",", "memory_size", ",", "memory_order", ",", "\n", "memory_activation", "=", "'id'", ",", "\n", "gate", "=", "'G'", ",", "# 'N' | 'G' | UR'", "\n", "**", "kwargs", "\n", ")", ":", "\n", "        ", "self", ".", "memory_size", "=", "memory_size", "\n", "self", ".", "memory_order", "=", "memory_order", "\n", "\n", "self", ".", "memory_activation", "=", "memory_activation", "\n", "self", ".", "gate", "=", "gate", "\n", "\n", "super", "(", "MemoryCell", ",", "self", ")", ".", "__init__", "(", "d_input", ",", "d_model", ",", "**", "kwargs", ")", "\n", "\n", "\n", "self", ".", "input_to_d_model", "=", "self", ".", "d_input", "if", "self", ".", "architecture", "[", "'hx'", "]", "else", "0", "\n", "self", ".", "input_to_memory_size", "=", "self", ".", "d_input", "if", "self", ".", "architecture", "[", "'ux'", "]", "else", "0", "\n", "\n", "# Hidden to memory", "\n", "self", ".", "W_uxh", "=", "LinearActivation", "(", "\n", "self", ".", "input_to_memory_size", "+", "self", ".", "d_model", ",", "\n", "self", ".", "memory_size", ",", "\n", "bias", "=", "self", ".", "architecture", "[", "'bias'", "]", ",", "\n", "initializer", "=", "self", ".", "initializers", "[", "'uxh'", "]", ",", "\n", "activation", "=", "self", ".", "memory_activation", ",", "\n", "activate", "=", "True", ",", "\n", ")", "\n", "\n", "\n", "self", ".", "memory_to_d_model", "=", "self", ".", "memory_size", "*", "self", ".", "memory_order", "if", "self", ".", "architecture", "[", "'hm'", "]", "else", "0", "\n", "\n", "# Memory to hidden", "\n", "self", ".", "W_hxm", "=", "LinearActivation", "(", "\n", "self", ".", "input_to_d_model", "+", "self", ".", "memory_to_d_model", ",", "\n", "self", ".", "d_model", ",", "\n", "self", ".", "architecture", "[", "'bias'", "]", ",", "\n", "initializer", "=", "self", ".", "initializers", "[", "'hxm'", "]", ",", "\n", "activation", "=", "self", ".", "hidden_activation", ",", "\n", "activate", "=", "False", ",", "\n", ")", "\n", "\n", "if", "self", ".", "architecture", "[", "'hh'", "]", ":", "\n", "            ", "self", ".", "reset_hidden_to_hidden", "(", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "W_hh", "=", "None", "\n", "\n", "# Construct gate with options", "\n", "", "if", "self", ".", "gate", "is", "not", "None", ":", "\n", "            ", "preact_ctor", "=", "LinearActivation", "\n", "preact_args", "=", "[", "\n", "self", ".", "input_to_d_model", "+", "self", ".", "memory_to_d_model", ",", "\n", "self", ".", "d_model", ",", "\n", "self", ".", "architecture", "[", "'bias'", "]", ",", "\n", "]", "\n", "if", "self", ".", "architecture", "[", "'hh'", "]", ":", "\n", "                ", "print", "(", "\"input to hidden size, memory to hidden size, hidden size:\"", ",", "self", ".", "input_to_d_model", ",", "self", ".", "memory_to_d_model", ",", "self", ".", "d_model", ")", "\n", "preact_args", "[", "0", "]", "+=", "self", ".", "d_model", "\n", "", "self", ".", "W_gxm", "=", "Gate", "(", "self", ".", "d_model", ",", "preact_ctor", ",", "preact_args", ",", "mechanism", "=", "self", ".", "gate", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.cells.memory.MemoryCell.reset_parameters": [[111, 114], ["src.models.nn.components.Activation"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.s4.Activation"], ["", "", "def", "reset_parameters", "(", "self", ")", ":", "\n", "# super().reset_parameters() # TODO find a way to refactor to call super()", "\n", "        ", "self", ".", "activate", "=", "Activation", "(", "self", ".", "hidden_activation", ",", "self", ".", "d_model", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.cells.memory.MemoryCell.forward": [[115, 129], ["memory.MemoryCell.forward_memory", "memory.MemoryCell.update_memory", "memory.MemoryCell.forward_hidden", "memory.MemoryCell.state_to_tensor"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.cells.memory.MemoryCell.forward_memory", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.cells.memory.LSICell.update_memory", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.cells.memory.MemoryCell.forward_hidden", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.s4.S4.state_to_tensor"], ["", "def", "forward", "(", "self", ",", "input", ",", "state", ")", ":", "\n", "        ", "h", ",", "m", ",", "time_step", "=", "state", "\n", "\n", "# Update the memory", "\n", "u", "=", "self", ".", "forward_memory", "(", "input", ",", "h", ",", "m", ")", "\n", "m", "=", "self", ".", "update_memory", "(", "m", ",", "u", ",", "time_step", ")", "# (batch, memory_size, memory_order)", "\n", "\n", "# Update hidden", "\n", "h", "=", "self", ".", "forward_hidden", "(", "input", ",", "h", ",", "m", ")", "\n", "\n", "next_state", "=", "(", "h", ",", "m", ",", "time_step", "+", "1", ")", "\n", "output", "=", "self", ".", "state_to_tensor", "(", "next_state", ")", "\n", "\n", "return", "output", ",", "next_state", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.cells.memory.MemoryCell.forward_memory": [[131, 140], ["torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "memory.MemoryCell.W_uxh", "input.new_empty"], "methods", ["None"], ["", "def", "forward_memory", "(", "self", ",", "input", ",", "h", ",", "m", ")", ":", "\n", "        ", "\"\"\" First part of forward pass to construct the memory state update \"\"\"", "\n", "\n", "input_to_memory", "=", "input", "if", "self", ".", "architecture", "[", "'ux'", "]", "else", "input", ".", "new_empty", "(", "(", "0", ",", ")", ")", "\n", "xh", "=", "torch", ".", "cat", "(", "(", "input_to_memory", ",", "h", ")", ",", "dim", "=", "-", "1", ")", "\n", "\n", "# Construct the update features", "\n", "u", "=", "self", ".", "W_uxh", "(", "xh", ")", "# (batch, memory_size)", "\n", "return", "u", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.cells.memory.MemoryCell.forward_hidden": [[141, 164], ["m.view", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "memory.MemoryCell.W_hxm", "memory.MemoryCell.activate", "input.new_empty", "memory.MemoryCell.W_gxm", "memory.MemoryCell.W_hh", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat"], "methods", ["None"], ["", "def", "forward_hidden", "(", "self", ",", "input", ",", "h", ",", "m", ")", ":", "\n", "        ", "input_to_hidden", "=", "input", "if", "self", ".", "architecture", "[", "'hx'", "]", "else", "input", ".", "new_empty", "(", "(", "0", ",", ")", ")", "\n", "\n", "# Update hidden state from memory", "\n", "memory_to_hidden", "=", "m", ".", "view", "(", "input", ".", "shape", "[", "0", "]", ",", "self", ".", "memory_size", "*", "self", ".", "memory_order", ")", "\n", "xm", "=", "torch", ".", "cat", "(", "(", "input_to_hidden", ",", "memory_to_hidden", ")", ",", "dim", "=", "-", "1", ")", "\n", "hidden_preact", "=", "self", ".", "W_hxm", "(", "xm", ")", "\n", "\n", "if", "self", ".", "architecture", "[", "'hh'", "]", ":", "\n", "            ", "hidden_preact", "=", "hidden_preact", "+", "self", ".", "W_hh", "(", "h", ")", "\n", "", "hidden", "=", "self", ".", "activate", "(", "hidden_preact", ")", "\n", "\n", "\n", "# Construct gate if necessary", "\n", "if", "self", ".", "gate", "is", "None", ":", "\n", "            ", "h", "=", "hidden", "\n", "", "else", ":", "\n", "            ", "if", "self", ".", "architecture", "[", "'hh'", "]", ":", "\n", "                ", "xm", "=", "torch", ".", "cat", "(", "(", "xm", ",", "h", ")", ",", "dim", "=", "-", "1", ")", "\n", "", "g", "=", "self", ".", "W_gxm", "(", "xm", ")", "\n", "h", "=", "(", "1.", "-", "g", ")", "*", "h", "+", "g", "*", "hidden", "\n", "\n", "", "return", "h", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.cells.memory.MemoryCell.update_memory": [[166, 174], ["None"], "methods", ["None"], ["", "def", "update_memory", "(", "self", ",", "m", ",", "u", ",", "time_step", ")", ":", "\n", "        ", "\"\"\"\n        m: (B, M, N) [batch size, memory size, memory order]\n        u: (B, M)\n\n        Output: (B, M, N)\n        \"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.cells.memory.MemoryCell.default_state": [[175, 180], ["torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros"], "methods", ["None"], ["", "def", "default_state", "(", "self", ",", "*", "batch_shape", ",", "device", "=", "None", ")", ":", "\n", "        ", "return", "(", "\n", "torch", ".", "zeros", "(", "*", "batch_shape", ",", "self", ".", "d_model", ",", "device", "=", "device", ",", "requires_grad", "=", "False", ")", ",", "\n", "torch", ".", "zeros", "(", "*", "batch_shape", ",", "self", ".", "memory_size", ",", "self", ".", "memory_order", ",", "device", "=", "device", ",", "requires_grad", "=", "False", ")", ",", "\n", "0", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.cells.memory.MemoryCell.state_to_tensor": [[182, 189], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "state_to_tensor", "(", "self", ")", ":", "\n", "        ", "\"\"\" Converts a state into a single output (tensor) \"\"\"", "\n", "def", "fn", "(", "state", ")", ":", "\n", "            ", "h", ",", "m", ",", "time_step", "=", "state", "\n", "return", "h", "\n", "", "return", "fn", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.cells.memory.MemoryCell.d_state": [[190, 193], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "d_state", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "d_model", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.cells.memory.MemoryCell.d_output": [[194, 197], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "d_output", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "d_model", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.cells.memory.LTICell.__init__": [[202, 220], ["memory.MemoryCell.__init__", "numpy.ones", "numpy.zeros", "scipy.signal.cont2discrete", "memory.LTICell.register_buffer", "memory.LTICell.register_buffer", "numpy.eye", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__"], ["def", "__init__", "(", "\n", "self", ",", "d_input", ",", "d_model", ",", "memory_size", ",", "memory_order", ",", "\n", "A", ",", "B", ",", "\n", "dt", "=", "0.01", ",", "\n", "discretization", "=", "'zoh'", ",", "\n", "**", "kwargs", "\n", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "d_input", ",", "d_model", ",", "memory_size", ",", "memory_order", ",", "**", "kwargs", ")", "\n", "\n", "\n", "C", "=", "np", ".", "ones", "(", "(", "1", ",", "memory_order", ")", ")", "\n", "D", "=", "np", ".", "zeros", "(", "(", "1", ",", ")", ")", "\n", "dA", ",", "dB", ",", "_", ",", "_", ",", "_", "=", "signal", ".", "cont2discrete", "(", "(", "A", ",", "B", ",", "C", ",", "D", ")", ",", "dt", "=", "dt", ",", "method", "=", "discretization", ")", "\n", "\n", "\n", "dA", "=", "dA", "-", "np", ".", "eye", "(", "memory_order", ")", "# puts into form: x += Ax", "\n", "self", ".", "register_buffer", "(", "'A'", ",", "torch", ".", "Tensor", "(", "dA", ")", ")", "\n", "self", ".", "register_buffer", "(", "'B'", ",", "torch", ".", "Tensor", "(", "dB", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.cells.memory.LTICell.update_memory": [[221, 224], ["u.unsqueeze.unsqueeze.unsqueeze", "torch.linear", "torch.linear", "torch.linear", "torch.linear", "torch.linear", "torch.linear"], "methods", ["None"], ["", "def", "update_memory", "(", "self", ",", "m", ",", "u", ",", "time_step", ")", ":", "\n", "        ", "u", "=", "u", ".", "unsqueeze", "(", "-", "1", ")", "# (B, M, 1)", "\n", "return", "m", "+", "F", ".", "linear", "(", "m", ",", "self", ".", "A", ")", "+", "F", ".", "linear", "(", "u", ",", "self", ".", "B", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.cells.memory.LSICell.__init__": [[228, 273], ["memory.MemoryCell.__init__", "isinstance", "numpy.empty", "numpy.empty", "range", "numpy.eye", "memory.LSICell.register_buffer", "memory.LSICell.register_buffer", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "len", "numpy.eye", "scipy.linalg.solve_triangular", "scipy.linalg.solve_triangular", "numpy.eye", "scipy.linalg.solve_triangular", "scipy.linalg.solve_triangular", "numpy.eye", "numpy.eye", "scipy.linalg.expm", "scipy.linalg.solve_triangular", "numpy.eye", "numpy.eye", "numpy.eye", "math.log", "math.log"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__"], ["def", "__init__", "(", "\n", "self", ",", "d_input", ",", "d_model", ",", "memory_size", ",", "memory_order", ",", "\n", "A", ",", "B", ",", "\n", "init_t", "=", "0", ",", "# 0 for special case at t=0 (new code), else old code without special case", "\n", "l_max", "=", "1024", ",", "\n", "discretization", "=", "'bilinear'", ",", "\n", "**", "kwargs", "\n", ")", ":", "\n", "        ", "\"\"\"\n        # TODO: make init_t start at arbitrary time (instead of 0 or 1)\n        \"\"\"", "\n", "\n", "# B should have shape (N, 1)", "\n", "assert", "len", "(", "B", ".", "shape", ")", "==", "2", "and", "B", ".", "shape", "[", "1", "]", "==", "1", "\n", "\n", "super", "(", ")", ".", "__init__", "(", "d_input", ",", "d_model", ",", "memory_size", ",", "memory_order", ",", "**", "kwargs", ")", "\n", "\n", "assert", "isinstance", "(", "init_t", ",", "int", ")", "\n", "self", ".", "init_t", "=", "init_t", "\n", "self", ".", "l_max", "=", "l_max", "\n", "\n", "A_stacked", "=", "np", ".", "empty", "(", "(", "l_max", ",", "memory_order", ",", "memory_order", ")", ",", "dtype", "=", "A", ".", "dtype", ")", "\n", "B_stacked", "=", "np", ".", "empty", "(", "(", "l_max", ",", "memory_order", ")", ",", "dtype", "=", "B", ".", "dtype", ")", "\n", "B", "=", "B", "[", ":", ",", "0", "]", "\n", "N", "=", "memory_order", "\n", "for", "t", "in", "range", "(", "1", ",", "l_max", "+", "1", ")", ":", "\n", "            ", "At", "=", "A", "/", "t", "\n", "Bt", "=", "B", "/", "t", "\n", "if", "discretization", "in", "forward_aliases", ":", "\n", "                ", "A_stacked", "[", "t", "-", "1", "]", "=", "np", ".", "eye", "(", "N", ")", "+", "At", "\n", "B_stacked", "[", "t", "-", "1", "]", "=", "Bt", "\n", "", "elif", "discretization", "in", "backward_aliases", ":", "\n", "                ", "A_stacked", "[", "t", "-", "1", "]", "=", "la", ".", "solve_triangular", "(", "np", ".", "eye", "(", "N", ")", "-", "At", ",", "np", ".", "eye", "(", "N", ")", ",", "lower", "=", "True", ")", "\n", "B_stacked", "[", "t", "-", "1", "]", "=", "la", ".", "solve_triangular", "(", "np", ".", "eye", "(", "N", ")", "-", "At", ",", "Bt", ",", "lower", "=", "True", ")", "\n", "", "elif", "discretization", "in", "bilinear_aliases", ":", "\n", "                ", "A_stacked", "[", "t", "-", "1", "]", "=", "la", ".", "solve_triangular", "(", "np", ".", "eye", "(", "N", ")", "-", "At", "/", "2", ",", "np", ".", "eye", "(", "N", ")", "+", "At", "/", "2", ",", "lower", "=", "True", ")", "\n", "B_stacked", "[", "t", "-", "1", "]", "=", "la", ".", "solve_triangular", "(", "np", ".", "eye", "(", "N", ")", "-", "At", "/", "2", ",", "Bt", ",", "lower", "=", "True", ")", "\n", "", "elif", "discretization", "in", "zoh_aliases", ":", "\n", "                ", "A_stacked", "[", "t", "-", "1", "]", "=", "la", ".", "expm", "(", "A", "*", "(", "math", ".", "log", "(", "t", "+", "1", ")", "-", "math", ".", "log", "(", "t", ")", ")", ")", "\n", "B_stacked", "[", "t", "-", "1", "]", "=", "la", ".", "solve_triangular", "(", "A", ",", "A_stacked", "[", "t", "-", "1", "]", "@", "B", "-", "B", ",", "lower", "=", "True", ")", "\n", "", "", "B_stacked", "=", "B_stacked", "[", ":", ",", ":", ",", "None", "]", "\n", "\n", "A_stacked", "-=", "np", ".", "eye", "(", "memory_order", ")", "# puts into form: x += Ax", "\n", "self", ".", "register_buffer", "(", "'A'", ",", "torch", ".", "Tensor", "(", "A_stacked", ")", ")", "\n", "self", ".", "register_buffer", "(", "'B'", ",", "torch", ".", "Tensor", "(", "B_stacked", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.cells.memory.LSICell.update_memory": [[275, 283], ["u.unsqueeze.unsqueeze.unsqueeze", "torch.pad", "torch.pad", "torch.pad", "torch.linear", "torch.linear", "torch.linear", "torch.linear", "torch.linear", "torch.linear"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.sc.pad", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.sc.pad", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.sc.pad"], ["", "def", "update_memory", "(", "self", ",", "m", ",", "u", ",", "time_step", ")", ":", "\n", "        ", "u", "=", "u", ".", "unsqueeze", "(", "-", "1", ")", "# (B, M, 1)", "\n", "t", "=", "time_step", "-", "1", "+", "self", ".", "init_t", "\n", "if", "t", "<", "0", ":", "\n", "            ", "return", "F", ".", "pad", "(", "u", ",", "(", "0", ",", "self", ".", "memory_order", "-", "1", ")", ")", "\n", "", "else", ":", "\n", "            ", "if", "t", ">=", "self", ".", "l_max", ":", "t", "=", "self", ".", "l_max", "-", "1", "\n", "return", "m", "+", "F", ".", "linear", "(", "m", ",", "self", ".", "A", "[", "t", "]", ")", "+", "F", ".", "linear", "(", "u", ",", "self", ".", "B", "[", "t", "]", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.attention.performer.PerformerFeatures.__init__": [[100, 117], ["fast_transformers.feature_maps.base.FeatureMap.__init__", "performer.PerformerFeatures.register_buffer", "int", "torch.zeros", "math.sqrt", "math.log"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__"], ["def", "__init__", "(", "self", ",", "query_dims", ",", "n_features", "=", "None", ",", "ortho_scaling", "=", "0", ",", "softmax_temp", "=", "None", ",", "\n", "orthogonal", "=", "False", ",", "redraw", "=", "1", ",", "deterministic_eval", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "query_dims", ")", "\n", "self", ".", "n_features", "=", "n_features", "or", "int", "(", "query_dims", "*", "math", ".", "log", "(", "query_dims", ")", ")", "\n", "self", ".", "ortho_scaling", "=", "ortho_scaling", "\n", "# TODO: we're not using @orthogonal atm", "\n", "self", ".", "orthogonal", "=", "orthogonal", "\n", "# TODO: we're not using @softmax_temp atm", "\n", "self", ".", "softmax_temp", "=", "1", "/", "math", ".", "sqrt", "(", "query_dims", ")", "if", "softmax_temp", "is", "None", "else", "softmax_temp", "\n", "# self.redraw = redraw", "\n", "# TODO: not redrawing atm, so I'm setting it to an irrational number", "\n", "self", ".", "redraw", "=", "math", ".", "pi", "\n", "self", ".", "deterministic_eval", "=", "deterministic_eval", "\n", "\n", "# Make a buffer for storing the sampled projection_matrix", "\n", "self", ".", "register_buffer", "(", "\"projection_matrix\"", ",", "torch", ".", "zeros", "(", "self", ".", "query_dims", ",", "self", ".", "n_features", ")", ")", "\n", "self", ".", "_calls", "=", "-", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.attention.performer.PerformerFeatures.new_feature_map": [[118, 132], ["performer.gaussian_orthogonal_random_matrix", "performer.PerformerFeatures.register_buffer", "gaussian_orthogonal_random_matrix.to"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.attention.performer.gaussian_orthogonal_random_matrix"], ["", "def", "new_feature_map", "(", "self", ",", "device", ")", ":", "\n", "# If we are not training skip the generation of a new feature map", "\n", "        ", "if", "self", ".", "deterministic_eval", "and", "not", "self", ".", "training", ":", "\n", "            ", "return", "\n", "\n", "# Only redraw the new feature map every self.redraw times", "\n", "", "self", ".", "_calls", "+=", "1", "\n", "if", "(", "self", ".", "_calls", "%", "self", ".", "redraw", ")", "!=", "0", ":", "\n", "            ", "return", "\n", "\n", "", "projection_matrix", "=", "gaussian_orthogonal_random_matrix", "(", "nb_rows", "=", "self", ".", "n_features", ",", "\n", "nb_columns", "=", "self", ".", "query_dims", ",", "\n", "scaling", "=", "self", ".", "ortho_scaling", ")", "\n", "self", ".", "register_buffer", "(", "\"projection_matrix\"", ",", "projection_matrix", ".", "to", "(", "device", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.attention.performer.PerformerFeatures.forward_queries": [[133, 135], ["performer.softmax_kernel"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.attention.performer.softmax_kernel"], ["", "def", "forward_queries", "(", "self", ",", "x", ")", ":", "\n", "        ", "return", "softmax_kernel", "(", "x", ",", "projection_matrix", "=", "self", ".", "projection_matrix", ",", "is_query", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.attention.performer.PerformerFeatures.forward_keys": [[136, 138], ["performer.softmax_kernel"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.attention.performer.softmax_kernel"], ["", "def", "forward_keys", "(", "self", ",", "x", ")", ":", "\n", "        ", "return", "softmax_kernel", "(", "x", ",", "projection_matrix", "=", "self", ".", "projection_matrix", ",", "is_query", "=", "False", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.attention.performer.orthogonal_matrix_chunk": [[13, 17], ["torch.randn", "torch.linalg.qr", "q.t"], "function", ["None"], ["def", "orthogonal_matrix_chunk", "(", "cols", ",", "device", "=", "None", ")", ":", "\n", "    ", "unstructured_block", "=", "torch", ".", "randn", "(", "(", "cols", ",", "cols", ")", ",", "device", "=", "device", ")", "\n", "q", ",", "r", "=", "torch", ".", "linalg", ".", "qr", "(", "unstructured_block", ")", "\n", "return", "q", ".", "t", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.attention.performer.gaussian_orthogonal_random_matrix": [[18, 42], ["int", "range", "torch.cat", "performer.orthogonal_matrix_chunk", "block_list.append", "performer.orthogonal_matrix_chunk", "block_list.append", "torch.randn().norm", "torch.diag", "ValueError", "torch.randn", "math.sqrt", "torch.ones", "float"], "function", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.attention.performer.orthogonal_matrix_chunk", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.attention.performer.orthogonal_matrix_chunk"], ["", "def", "gaussian_orthogonal_random_matrix", "(", "nb_rows", ",", "nb_columns", ",", "scaling", "=", "0", ",", "device", "=", "None", ")", ":", "\n", "    ", "nb_full_blocks", "=", "int", "(", "nb_rows", "/", "nb_columns", ")", "\n", "\n", "block_list", "=", "[", "]", "\n", "\n", "for", "_", "in", "range", "(", "nb_full_blocks", ")", ":", "\n", "        ", "q", "=", "orthogonal_matrix_chunk", "(", "nb_columns", ",", "device", "=", "device", ")", "\n", "block_list", ".", "append", "(", "q", ")", "\n", "\n", "", "remaining_rows", "=", "nb_rows", "-", "nb_full_blocks", "*", "nb_columns", "\n", "if", "remaining_rows", ">", "0", ":", "\n", "        ", "q", "=", "orthogonal_matrix_chunk", "(", "nb_columns", ",", "device", "=", "device", ")", "\n", "block_list", ".", "append", "(", "q", "[", ":", "remaining_rows", "]", ")", "\n", "\n", "", "final_matrix", "=", "torch", ".", "cat", "(", "block_list", ")", "\n", "\n", "if", "scaling", "==", "0", ":", "\n", "        ", "multiplier", "=", "torch", ".", "randn", "(", "(", "nb_rows", ",", "nb_columns", ")", ",", "device", "=", "device", ")", ".", "norm", "(", "dim", "=", "1", ")", "\n", "", "elif", "scaling", "==", "1", ":", "\n", "        ", "multiplier", "=", "math", ".", "sqrt", "(", "(", "float", "(", "nb_columns", ")", ")", ")", "*", "torch", ".", "ones", "(", "(", "nb_rows", ",", ")", ",", "device", "=", "device", ")", "\n", "", "else", ":", "\n", "        ", "raise", "ValueError", "(", "f'Invalid scaling {scaling}'", ")", "\n", "\n", "", "return", "torch", ".", "diag", "(", "multiplier", ")", "@", "final_matrix", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.attention.performer.softmax_kernel": [[47, 77], ["math.sqrt", "einops.repeat", "projection.type_as.type_as", "torch.einsum", "torch.sum", "diag_data.unsqueeze.unsqueeze", "torch.einsum.type_as", "math.sqrt", "torch.exp", "torch.exp", "torch.max", "torch.max"], "function", ["None"], ["", "def", "softmax_kernel", "(", "data", ",", "*", ",", "projection_matrix", ",", "is_query", ",", "softmax_temp", "=", "None", ",", "eps", "=", "1e-4", ")", ":", "\n", "    ", "\"\"\"For key, we expect shape (b, h, s, d) where s is the sequence dimension\n    \"\"\"", "\n", "b", ",", "h", ",", "_", ",", "d", "=", "data", ".", "shape", "\n", "\n", "if", "softmax_temp", "is", "None", ":", "\n", "        ", "softmax_temp", "=", "1", "/", "math", ".", "sqrt", "(", "d", ")", "\n", "", "data_normalizer", "=", "math", ".", "sqrt", "(", "softmax_temp", ")", "\n", "\n", "ratio", "=", "(", "projection_matrix", ".", "shape", "[", "0", "]", "**", "-", "0.5", ")", "\n", "\n", "projection", "=", "repeat", "(", "projection_matrix", ",", "'j d -> b h j d'", ",", "b", "=", "b", ",", "h", "=", "h", ")", "\n", "projection", "=", "projection", ".", "type_as", "(", "data", ")", "\n", "\n", "data_dash", "=", "torch", ".", "einsum", "(", "'...id,...jd->...ij'", ",", "(", "data_normalizer", "*", "data", ")", ",", "projection", ")", "\n", "\n", "diag_data", "=", "data", "**", "2", "\n", "diag_data", "=", "torch", ".", "sum", "(", "diag_data", ",", "dim", "=", "-", "1", ")", "\n", "diag_data", "=", "(", "diag_data", "/", "2.0", ")", "*", "(", "data_normalizer", "**", "2", ")", "\n", "diag_data", "=", "diag_data", ".", "unsqueeze", "(", "dim", "=", "-", "1", ")", "\n", "\n", "if", "is_query", ":", "\n", "        ", "data_dash", "=", "ratio", "*", "(", "\n", "torch", ".", "exp", "(", "data_dash", "-", "diag_data", "-", "\n", "torch", ".", "max", "(", "data_dash", ",", "dim", "=", "-", "1", ",", "keepdim", "=", "True", ")", ".", "values", ")", "+", "eps", ")", "\n", "", "else", ":", "\n", "        ", "data_dash", "=", "ratio", "*", "(", "\n", "torch", ".", "exp", "(", "data_dash", "-", "diag_data", "-", "torch", ".", "max", "(", "data_dash", ")", ")", "+", "eps", ")", "\n", "\n", "", "return", "data_dash", ".", "type_as", "(", "data", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.attention.linear.LinearAttention.__init__": [[92, 101], ["torch.Module.__init__", "hydra.utils.instantiate", "fast_transformers.feature_maps.elu_feature_map"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.tasks.decoders.instantiate"], ["def", "__init__", "(", "self", ",", "d_model", ",", "n_heads", ",", "feature_map_cfg", "=", "None", ",", "eps", "=", "1e-6", ",", "dropout", "=", "0.0", ")", ":", "# TODO dropout not used", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "query_dims", "=", "d_model", "//", "n_heads", "\n", "self", ".", "n_heads", "=", "n_heads", "\n", "self", ".", "feature_map", "=", "(", "\n", "hydra", ".", "utils", ".", "instantiate", "(", "feature_map_cfg", ",", "query_dims", ")", "if", "feature_map_cfg", "is", "not", "None", "\n", "else", "elu_feature_map", "(", "query_dims", ")", "\n", ")", "\n", "self", ".", "eps", "=", "eps", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.attention.linear.LinearAttention.forward": [[102, 127], ["einops.rearrange", "einops.rearrange", "einops.rearrange", "linear.LinearAttention.feature_map.new_feature_map", "linear.LinearAttention.feature_map.forward_queries", "linear.LinearAttention.feature_map.forward_keys", "attn_fn", "einops.rearrange", "RuntimeError", "linear.LinearAttention.masked_fill_", "einops.rearrange"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.attention.performer.PerformerFeatures.new_feature_map", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.attention.performer.PerformerFeatures.forward_queries", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.attention.performer.PerformerFeatures.forward_keys"], ["", "def", "forward", "(", "self", ",", "query", ",", "key", ",", "value", ",", "attn_mask", "=", "None", ",", "key_padding_mask", "=", "None", ",", "need_weights", "=", "False", ")", ":", "\n", "# Permute the dimensions to BHTE instead of BTHE", "\n", "        ", "query", "=", "rearrange", "(", "query", ",", "'b t (h e) -> b h t e'", ",", "h", "=", "self", ".", "n_heads", ")", "\n", "key", "=", "rearrange", "(", "key", ",", "'b s (h e) -> b h s e'", ",", "h", "=", "self", ".", "n_heads", ")", "\n", "value", "=", "rearrange", "(", "value", ",", "'b s (h d) -> b h s d'", ",", "h", "=", "self", ".", "n_heads", ")", "\n", "\n", "# Apply the feature map to the query and key", "\n", "self", ".", "feature_map", ".", "new_feature_map", "(", "query", ".", "device", ")", "\n", "Q", "=", "self", ".", "feature_map", ".", "forward_queries", "(", "query", ")", "\n", "K", "=", "self", ".", "feature_map", ".", "forward_keys", "(", "key", ")", "\n", "\n", "# Apply the key padding mask and make sure that the attn_mask is", "\n", "# all_ones or is causal", "\n", "causal", "=", "attn_mask", "is", "not", "None", "and", "attn_mask", ".", "lower_triangular", "\n", "if", "not", "(", "attn_mask", "is", "None", "or", "attn_mask", ".", "all_ones", "or", "causal", ")", ":", "\n", "            ", "raise", "RuntimeError", "(", "(", "\"LinearAttention does not support arbitrary attention masks\"", ")", ")", "\n", "", "if", "causal", ":", "\n", "            ", "assert", "query", ".", "shape", "[", "1", "]", "==", "key", ".", "shape", "[", "1", "]", ",", "'query and key must have the same sequence length'", "\n", "\n", "", "if", "key_padding_mask", "is", "not", "None", ":", "\n", "            ", "K", ".", "masked_fill_", "(", "~", "rearrange", "(", "key_padding_mask", ".", "bool_matrix", ",", "'b s -> b 1 s 1'", ")", ",", "0.0", ")", "\n", "", "attn_fn", "=", "causal_linear_attention", "if", "causal", "else", "linear_attention", "\n", "out", ",", "attn", "=", "attn_fn", "(", "Q", ",", "K", ",", "value", ",", "eps", "=", "self", ".", "eps", ",", "need_weights", "=", "need_weights", ")", "\n", "out", "=", "rearrange", "(", "out", ",", "'b h s d -> b s (h d)'", ")", "\n", "return", "out", ",", "attn", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.attention.linear.Performer.__init__": [[130, 136], ["models.sequence.base.SequenceModule.__init__", "linear.LinearAttention"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__"], ["def", "__init__", "(", "self", ",", "d_model", ",", "n_heads", ",", "*", "args", ",", "causal", "=", "True", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "d_model", "=", "d_model", "\n", "self", ".", "d_output", "=", "d_model", "\n", "self", ".", "mha", "=", "LinearAttention", "(", "d_model", ",", "n_heads", ",", "*", "args", ",", "**", "kwargs", ")", "\n", "self", ".", "causal", "=", "causal", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.attention.linear.Performer.forward": [[137, 145], ["linear.Performer.mha", "fast_transformers.masking.TriangularCausalMask", "src.size"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "src", ",", "attn_mask", "=", "None", ",", "key_padding_mask", "=", "None", ",", "state", "=", "None", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\" state should represent a mask and key padding mask \"\"\"", "\n", "if", "self", ".", "causal", "and", "attn_mask", "is", "None", ":", "\n", "            ", "attn_mask", "=", "TriangularCausalMask", "(", "src", ".", "size", "(", "-", "2", ")", ",", "device", "=", "src", ".", "device", ")", "\n", "# attn_mask, key_padding_mask = state", "\n", "# Note that this returns None for the second argument", "\n", "", "y", ",", "z", "=", "self", ".", "mha", "(", "src", ",", "src", ",", "src", ",", "attn_mask", "=", "attn_mask", ",", "key_padding_mask", "=", "key_padding_mask", ",", "need_weights", "=", "False", ")", "\n", "return", "y", ",", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.attention.linear.Performer.step": [[146, 148], ["None"], "methods", ["None"], ["", "def", "step", "(", "self", ",", "x", ",", "state", ")", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.attention.linear.linear_attention": [[26, 33], ["k.sum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "k.sum.type_as"], "function", ["None"], ["", "def", "linear_attention", "(", "q", ",", "k", ",", "v", ",", "eps", "=", "0.0", ",", "need_weights", "=", "False", ")", ":", "\n", "    ", "k_cumsum", "=", "k", ".", "sum", "(", "dim", "=", "-", "2", ")", "\n", "D_inv", "=", "1.", "/", "(", "torch", ".", "einsum", "(", "'...nd,...d->...n'", ",", "q", ",", "k_cumsum", ".", "type_as", "(", "q", ")", ")", "+", "eps", ")", "\n", "context", "=", "torch", ".", "einsum", "(", "'...nd,...ne->...de'", ",", "k", ",", "v", ")", "\n", "out", "=", "torch", ".", "einsum", "(", "'...de,...nd,...n->...ne'", ",", "context", ",", "q", ",", "D_inv", ")", "\n", "attn", "=", "None", "if", "not", "need_weights", "else", "torch", ".", "einsum", "(", "'...te,...se,...s->...ts'", ",", "q", ",", "k", ",", "D_inv", ")", "\n", "return", "out", ",", "attn", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.attention.linear.null_context": [[35, 38], ["None"], "function", ["None"], ["", "@", "contextmanager", "\n", "def", "null_context", "(", ")", ":", "\n", "    ", "yield", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.attention.linear.causal_linear_attention": [[41, 67], ["torch.is_autocast_enabled", "torch.is_autocast_enabled", "isinstance", "torch.einsum", "torch.einsum", "functools.partial", "amp.float_function", "k.cumsum", "torch.einsum", "torch.einsum", "cuda_context", "causal_dot_product_fn", "k_cumsum.type_as", "map", "torch.einsum", "torch.einsum", "torch.triu", "torch.triu", "torch.einsum.masked_fill_", "torch.ones", "torch.ones", "t.float"], "function", ["None"], ["", "def", "causal_linear_attention", "(", "q", ",", "k", ",", "v", ",", "eps", "=", "1e-6", ",", "need_weights", "=", "False", ")", ":", "\n", "    ", "from", "fast_transformers", ".", "causal_product", "import", "CausalDotProduct", "\n", "autocast_enabled", "=", "torch", ".", "is_autocast_enabled", "(", ")", "\n", "is_half", "=", "isinstance", "(", "q", ",", "torch", ".", "cuda", ".", "HalfTensor", ")", "\n", "assert", "not", "is_half", "or", "APEX_AVAILABLE", ",", "'half tensors can only be used if nvidia apex is available'", "\n", "cuda_context", "=", "null_context", "if", "not", "autocast_enabled", "else", "partial", "(", "autocast", ",", "enabled", "=", "False", ")", "\n", "\n", "causal_dot_product_fn", "=", "amp", ".", "float_function", "(", "CausalDotProduct", ".", "apply", ")", "if", "is_half", "else", "CausalDotProduct", ".", "apply", "\n", "\n", "k_cumsum", "=", "k", ".", "cumsum", "(", "dim", "=", "-", "2", ")", "+", "eps", "\n", "D_inv", "=", "1.", "/", "torch", ".", "einsum", "(", "'...nd,...nd->...n'", ",", "q", ",", "k_cumsum", ".", "type_as", "(", "q", ")", ")", "\n", "\n", "with", "cuda_context", "(", ")", ":", "\n", "        ", "if", "autocast_enabled", ":", "\n", "            ", "q", ",", "k", ",", "v", "=", "map", "(", "lambda", "t", ":", "t", ".", "float", "(", ")", ",", "(", "q", ",", "k", ",", "v", ")", ")", "\n", "", "out", "=", "causal_dot_product_fn", "(", "q", ",", "k", ",", "v", ")", "\n", "if", "need_weights", ":", "\n", "            ", "attn", "=", "torch", ".", "einsum", "(", "'...te,...se,...s'", ",", "q", ",", "k", ",", "D_inv", ")", "\n", "causal_mask", "=", "torch", ".", "triu", "(", "torch", ".", "ones", "(", "q", ".", "shape", "[", "-", "2", "]", ",", "k", ".", "shape", "[", "-", "2", "]", ",", "dtype", "=", "torch", ".", "bool", ",", "\n", "device", "=", "k", ".", "device", ")", ",", "diagonal", "=", "1", ")", "\n", "attn", ".", "masked_fill_", "(", "causal_mask", ",", "0.0", ")", "\n", "", "else", ":", "\n", "            ", "attn", "=", "None", "\n", "\n", "", "", "out", "=", "torch", ".", "einsum", "(", "'...nd,...n->...nd'", ",", "out", ",", "D_inv", ")", "\n", "return", "out", ",", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.ss.linear_system_recurrence.LinearSystem.__init__": [[12, 31], ["torch.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "N", ",", "\n", "transition", ",", "\n", "C", ",", "\n", "D", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        N: the order of the HiPPO projection\n        dt: discretization step size - should be roughly inverse to the length of the sequence\n        C: (..., M, N)\n        D: (..., M)\n        \"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "N", "=", "N", "\n", "self", ".", "transition", "=", "transition", "\n", "\n", "self", ".", "C", "=", "C", "\n", "self", ".", "D", "=", "D", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.ss.linear_system_recurrence.LinearSystem.forward": [[32, 53], ["zip", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "u.new_zeros", "linear_system_recurrence.LinearSystem.transition.bilinear", "ys.append", "u.unsqueeze", "linear_system_recurrence.LinearSystem.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.lssl.AdaptiveTransition.bilinear"], ["", "def", "forward", "(", "self", ",", "dt", ",", "u", ",", "x_", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        u : (length, ...)\n        x : (..., N)\n        Returns\n        y : (length, ..., M)\n        \"\"\"", "\n", "\n", "if", "x_", "is", "None", ":", "\n", "            ", "x_", "=", "u", ".", "new_zeros", "(", "u", ".", "shape", "[", "1", ":", "]", "+", "(", "self", ".", "N", ",", ")", ")", "\n", "", "ys", "=", "[", "]", "\n", "for", "dt_", ",", "u_", "in", "zip", "(", "dt", ",", "u", ")", ":", "\n", "            ", "x_", "=", "self", ".", "transition", ".", "bilinear", "(", "dt_", ",", "x_", ",", "u_", ")", "# (..., N)", "\n", "y", "=", "(", "self", ".", "C", "@", "x_", ".", "unsqueeze", "(", "-", "1", ")", ")", ".", "squeeze", "(", "\n", "-", "1", "\n", ")", "# TODO can use sum instead of matmul if M = 1", "\n", "ys", ".", "append", "(", "y", ")", "\n", "", "y", "=", "torch", ".", "stack", "(", "ys", ",", "dim", "=", "0", ")", "\n", "v", "=", "u", ".", "unsqueeze", "(", "-", "1", ")", "*", "self", ".", "D", "# (L, ..., M)", "\n", "y", "=", "y", "+", "v", "# (L, ..., M)", "\n", "return", "y", ",", "x_", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.ss.linear_system_recurrence.LinearSystem.adjoint_input": [[54, 81], ["torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "zip", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "du.flip.flip.flip", "dt.flip", "dyC.flip", "linear_system_recurrence.LinearSystem.transition.inverse_mult", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "du.flip.flip.append", "dy[].unsqueeze", "linear_system_recurrence.LinearSystem.transition.forward_mult", "linear_system_recurrence.LinearSystem.C.transpose", "dy.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.lssl.LegTTransition.inverse_mult", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.lssl.LegTTransition.forward_mult"], ["", "def", "adjoint_input", "(", "self", ",", "dy", ",", "dt", ")", ":", "\n", "        ", "\"\"\"Computes adjoint to the input u\n\n        dy: (L, ..., M)\n        dt: (L, ...)\n        \"\"\"", "\n", "\n", "# Compute dx_", "\n", "dx_", "=", "torch", ".", "sum", "(", "dy", "[", "-", "1", "]", ".", "unsqueeze", "(", "-", "1", ")", "*", "self", ".", "C", ",", "dim", "=", "-", "2", ")", "# (..., N)", "\n", "\n", "dyC", "=", "(", "self", ".", "C", ".", "transpose", "(", "-", "1", ",", "-", "2", ")", "@", "dy", ".", "unsqueeze", "(", "-", "1", ")", ")", ".", "squeeze", "(", "\n", "-", "1", "\n", ")", "# C^T dy (L, ..., N)", "\n", "dyD", "=", "torch", ".", "sum", "(", "dy", "*", "self", ".", "D", ",", "dim", "=", "-", "1", ")", "# D^T dy (L, ...)", "\n", "du", "=", "[", "]", "\n", "for", "dt_", ",", "dyC_", "in", "zip", "(", "dt", ".", "flip", "(", "0", ")", ",", "dyC", ".", "flip", "(", "0", ")", ")", ":", "\n", "            ", "dx_", "=", "self", ".", "transition", ".", "inverse_mult", "(", "dx_", ",", "dt_", "/", "2", ",", "transpose", "=", "True", ")", "# (..., N)", "\n", "du_", "=", "torch", ".", "sum", "(", "self", ".", "transition", ".", "B", "*", "dx_", ",", "dim", "=", "-", "1", ")", "# (...)", "\n", "du_", "=", "dt_", "*", "du_", "# (...)", "\n", "dx_", "=", "(", "\n", "self", ".", "transition", ".", "forward_mult", "(", "dx_", ",", "dt_", "/", "2", ",", "transpose", "=", "True", ")", "+", "dyC_", "\n", ")", "# (..., N)", "\n", "du", ".", "append", "(", "du_", ")", "\n", "", "du", "=", "torch", ".", "stack", "(", "du", ",", "dim", "=", "0", ")", "# (L, ...)", "\n", "du", "=", "du", ".", "flip", "(", "0", ")", "\n", "du", "=", "du", "+", "dyD", "\n", "return", "du", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.ss.linear_system_recurrence.LinearSystem.adjoint_projection": [[82, 99], ["torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "u.new_zeros", "zip", "dD.view().sum.view().sum.view().sum", "linear_system_recurrence.LinearSystem.transition.bilinear", "dC_.view().sum", "u.unsqueeze", "dy_.unsqueeze", "linear_system_recurrence.LinearSystem.unsqueeze", "dD.view().sum.view().sum.view", "dC_.view"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.lssl.AdaptiveTransition.bilinear"], ["", "def", "adjoint_projection", "(", "self", ",", "dy", ",", "dt", ",", "u", ")", ":", "\n", "        ", "\"\"\"Computes adjoint to the projection parameters C, D\n\n        dy: (L, ..., M)\n        u: (L, ...)\n        dt: (L, ...)\n        \"\"\"", "\n", "\n", "dC", "=", "torch", ".", "zeros_like", "(", "self", ".", "C", ")", "\n", "x_", "=", "u", ".", "new_zeros", "(", "u", ".", "shape", "[", "1", ":", "]", "+", "(", "self", ".", "N", ",", ")", ")", "\n", "for", "dt_", ",", "u_", ",", "dy_", "in", "zip", "(", "dt", ",", "u", ",", "dy", ")", ":", "\n", "            ", "x_", "=", "self", ".", "transition", ".", "bilinear", "(", "dt_", ",", "x_", ",", "u_", ")", "# (..., N)", "\n", "dC_", "=", "dy_", ".", "unsqueeze", "(", "-", "1", ")", "*", "x_", ".", "unsqueeze", "(", "-", "2", ")", "# (..., M, N)", "\n", "dC", "+=", "dC_", ".", "view", "(", "(", "-", "1", ",", ")", "+", "self", ".", "C", ".", "shape", ")", ".", "sum", "(", "dim", "=", "0", ")", "# (M, N)", "\n", "", "dD", "=", "dy", "*", "u", ".", "unsqueeze", "(", "-", "1", ")", "# (L, ..., M)", "\n", "dD", "=", "dD", ".", "view", "(", "(", "-", "1", ",", ")", "+", "self", ".", "D", ".", "shape", ")", ".", "sum", "(", "dim", "=", "0", ")", "# (M,)", "\n", "return", "dC", ",", "dD", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.ss.linear_system_recurrence.LinearSystemStepsize.__init__": [[102, 119], ["torch.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "N", ",", "\n", "transition", ",", "\n", "C", ",", "\n", "D", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        N: the order of the HiPPO projection\n        dt: discretization step size - should be roughly inverse to the length of the sequence\n        \"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "N", "=", "N", "\n", "self", ".", "transition", "=", "transition", "\n", "\n", "self", ".", "C", "=", "C", "\n", "self", ".", "D", "=", "D", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.ss.linear_system_recurrence.LinearSystemStepsize.forward": [[120, 142], ["zip", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "u.unsqueeze", "u.new_zeros", "linear_system_recurrence.LinearSystemStepsize.transition.bilinear", "ys.append", "linear_system_recurrence.LinearSystemStepsize.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.lssl.AdaptiveTransition.bilinear"], ["", "def", "forward", "(", "self", ",", "dt", ",", "u", ",", "x", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        u : (length, ...)\n        x : (..., N)\n        Returns\n        y : (length, ..., M)\n        \"\"\"", "\n", "\n", "v", "=", "u", ".", "unsqueeze", "(", "-", "1", ")", "*", "self", ".", "D", "# (L, ..., M)", "\n", "\n", "if", "x", "is", "None", ":", "\n", "            ", "x", "=", "u", ".", "new_zeros", "(", "u", ".", "shape", "[", "1", ":", "]", "+", "(", "self", ".", "N", ",", ")", ")", "\n", "", "ys", "=", "[", "]", "\n", "for", "dt_", ",", "u_", "in", "zip", "(", "dt", ",", "u", ")", ":", "\n", "            ", "x", "=", "self", ".", "transition", ".", "bilinear", "(", "dt_", ",", "x", ",", "u_", ")", "# (..., N)", "\n", "y", "=", "(", "self", ".", "C", "@", "x", ".", "unsqueeze", "(", "-", "1", ")", ")", ".", "squeeze", "(", "\n", "-", "1", "\n", ")", "# TODO can use sum instead of matmul if M = 1", "\n", "ys", ".", "append", "(", "y", ")", "\n", "", "y", "=", "torch", ".", "stack", "(", "ys", ",", "dim", "=", "0", ")", "\n", "y", "=", "y", "+", "v", "# (L, ..., M)", "\n", "return", "y", ",", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.ss.linear_system_recurrence.LinearSystemStepsize.adjoint": [[143, 203], ["torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "zip", "torch.stack().flip", "torch.stack().flip", "torch.stack().flip", "torch.stack().flip", "torch.stack().flip", "torch.stack().flip", "torch.stack().flip", "torch.stack().flip", "torch.stack().flip", "torch.stack().flip", "torch.stack().flip", "torch.stack().flip", "torch.stack().flip", "torch.stack().flip", "torch.stack().flip", "torch.stack().flip", "torch.stack().flip", "torch.stack().flip", "dt.flip", "dyC.flip", "u.flip", "dy.flip", "dC_.view().sum", "dD_.view().sum", "linear_system_recurrence.LinearSystemStepsize.transition.inverse_mult", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.stack().flip.append", "torch.stack().flip.append", "torch.stack().flip.append", "linear_system_recurrence.LinearSystemStepsize.transition.bilinear", "linear_system_recurrence.LinearSystemStepsize.transition.quadratic", "torch.stack().flip.append", "torch.stack().flip.append", "torch.stack().flip.append", "dy[].unsqueeze", "dy_.unsqueeze", "x_.unsqueeze", "u_.unsqueeze", "linear_system_recurrence.LinearSystemStepsize.transition.forward_mult", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "linear_system_recurrence.LinearSystemStepsize.C.transpose", "dy.unsqueeze", "dC_.view", "dD_.view", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.lssl.LegTTransition.inverse_mult", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.lssl.AdaptiveTransition.bilinear", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.hippo.transition.ToeplitzAdaptiveTransition.quadratic", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.lssl.LegTTransition.forward_mult"], ["", "def", "adjoint", "(", "self", ",", "dy", ",", "x_", ",", "dt", ",", "u", ")", ":", "\n", "        ", "\"\"\"\n        gradient:\n        dy: (L, ..., M)\n\n        state:\n        # dx_: (..., N)\n        x: (..., N)\n\n        cached arguments:\n        dt: (L, ...)\n        u: (L, ...)\n        \"\"\"", "\n", "\n", "dx_", "=", "torch", ".", "sum", "(", "dy", "[", "-", "1", "]", ".", "unsqueeze", "(", "-", "1", ")", "*", "self", ".", "C", ",", "dim", "=", "-", "2", ")", "# (..., N)", "\n", "\n", "dyC", "=", "(", "self", ".", "C", ".", "transpose", "(", "-", "1", ",", "-", "2", ")", "@", "dy", ".", "unsqueeze", "(", "-", "1", ")", ")", ".", "squeeze", "(", "\n", "-", "1", "\n", ")", "# C^T dy (L, ..., N)", "\n", "dyD", "=", "torch", ".", "sum", "(", "dy", "*", "self", ".", "D", ",", "dim", "=", "-", "1", ")", "# D^T dy (L, ...)", "\n", "\n", "dC", "=", "torch", ".", "zeros_like", "(", "self", ".", "C", ")", "\n", "dD", "=", "torch", ".", "zeros_like", "(", "self", ".", "D", ")", "\n", "du", "=", "[", "]", "\n", "ddt", "=", "[", "]", "\n", "for", "dt_", ",", "dyC_", ",", "u_", ",", "dy_", "in", "zip", "(", "dt", ".", "flip", "(", "0", ")", ",", "dyC", ".", "flip", "(", "0", ")", ",", "u", ".", "flip", "(", "0", ")", ",", "dy", ".", "flip", "(", "0", ")", ")", ":", "\n", "# dy_: (..., M)", "\n", "# x_: (..., N)", "\n", "# u_, dt_: (...)", "\n", "            ", "dC_", "=", "dy_", ".", "unsqueeze", "(", "-", "1", ")", "*", "x_", ".", "unsqueeze", "(", "-", "2", ")", "# (..., M, N)", "\n", "dC", "+=", "dC_", ".", "view", "(", "(", "-", "1", ",", ")", "+", "self", ".", "C", ".", "shape", ")", ".", "sum", "(", "dim", "=", "0", ")", "# (M, N)", "\n", "dD_", "=", "dy_", "*", "u_", ".", "unsqueeze", "(", "-", "1", ")", "# (..., M)", "\n", "dD", "+=", "dD_", ".", "view", "(", "(", "-", "1", ",", ")", "+", "self", ".", "D", ".", "shape", ")", ".", "sum", "(", "dim", "=", "0", ")", "# (M,)", "\n", "\n", "dx_", "=", "self", ".", "transition", ".", "inverse_mult", "(", "dx_", ",", "dt_", "/", "2", ",", "transpose", "=", "True", ")", "# (..., N)", "\n", "\n", "# Compute du", "\n", "du_", "=", "torch", ".", "sum", "(", "self", ".", "transition", ".", "B", "*", "dx_", ",", "dim", "=", "-", "1", ")", "# (...)", "\n", "du_", "=", "dt_", "*", "du_", "# (...)", "\n", "du", ".", "append", "(", "du_", ")", "\n", "\n", "x_prev", "=", "self", ".", "transition", ".", "bilinear", "(", "-", "dt_", ",", "x_", ",", "u_", ")", "# (..., N)", "\n", "ddt_", "=", "self", ".", "transition", ".", "quadratic", "(", "dx_", ",", "0.5", "*", "(", "x_prev", "+", "x_", ")", ")", "# (...)", "\n", "ddt_", "=", "ddt_", "+", "torch", ".", "sum", "(", "self", ".", "transition", ".", "B", "*", "dx_", ",", "dim", "=", "-", "1", ")", "*", "u_", "\n", "ddt", ".", "append", "(", "ddt_", ")", "# (...)", "\n", "x_", "=", "x_prev", "\n", "\n", "dx_", "=", "(", "\n", "self", ".", "transition", ".", "forward_mult", "(", "dx_", ",", "dt_", "/", "2", ",", "transpose", "=", "True", ")", "+", "dyC_", "\n", ")", "# (..., N)", "\n", "\n", "", "du", "=", "torch", ".", "stack", "(", "du", ",", "dim", "=", "0", ")", ".", "flip", "(", "0", ")", "# (L, ...)", "\n", "du", "=", "du", "+", "dyD", "\n", "\n", "ddt", "=", "torch", ".", "stack", "(", "ddt", ",", "dim", "=", "0", ")", ".", "flip", "(", "0", ")", "# (L, ...)", "\n", "\n", "# Sanity check", "\n", "# print(f\"{x_=}\") # should be 0 (initial state)", "\n", "\n", "return", "du", ",", "ddt", ",", "dC", ",", "dD", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.ss.linear_system_recurrence.LinearSystemFunction.forward": [[206, 237], ["ctx.save_for_backward", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "zip", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "u.new_zeros", "transition.bilinear", "ys.append", "u.unsqueeze", "transition.bilinear.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.lssl.AdaptiveTransition.bilinear"], ["    ", "@", "staticmethod", "\n", "def", "forward", "(", "ctx", ",", "x", ",", "dt", ",", "u", ",", "C", ",", "D", ",", "transition", ")", ":", "\n", "        ", "\"\"\"\n        dt : (L, ...)\n        u : (L, ...)\n        C : (M, N)\n        D : (M,)\n        transition: Transition objective implementing forward_mult, inverse_mult, bilinear, quadratic\n\n        Returns:\n        y : (L, ..., M)\n        \"\"\"", "\n", "ctx", ".", "transition", "=", "transition", "\n", "ctx", ".", "save_for_backward", "(", "dt", ",", "u", ",", "C", ",", "D", ")", "\n", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "if", "x", "is", "None", ":", "\n", "                ", "x", "=", "u", ".", "new_zeros", "(", "u", ".", "shape", "[", "1", ":", "]", "+", "(", "transition", ".", "N", ",", ")", ")", "\n", "", "ys", "=", "[", "]", "\n", "for", "dt_", ",", "u_", "in", "zip", "(", "dt", ",", "u", ")", ":", "\n", "# breakpoint()", "\n", "                ", "x", "=", "transition", ".", "bilinear", "(", "dt_", ",", "x", ",", "u_", ")", "# (..., N)", "\n", "y", "=", "(", "C", "@", "x", ".", "unsqueeze", "(", "-", "1", ")", ")", ".", "squeeze", "(", "\n", "-", "1", "\n", ")", "# TODO can use sum instead of matmul if M = 1", "\n", "ys", ".", "append", "(", "y", ")", "\n", "", "y", "=", "torch", ".", "stack", "(", "ys", ",", "dim", "=", "0", ")", "\n", "# breakpoint()", "\n", "v", "=", "u", ".", "unsqueeze", "(", "-", "1", ")", "*", "D", "# (L, ..., M)", "\n", "y", "=", "y", "+", "v", "# (L, ..., M)", "\n", "", "return", "y", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.ss.linear_system_recurrence.LinearSystemFunction.backward": [[238, 283], ["torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "zip", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "du.flip.flip.flip", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "u.new_zeros", "zip", "dD.view().sum.view().sum.view().sum", "dt.flip", "dyC.flip", "transition.inverse_mult", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "du.flip.flip.append", "transition.bilinear", "dC_.view().sum", "u.unsqueeze", "dy[].unsqueeze", "transition.forward_mult", "dy_.unsqueeze", "transition.bilinear.unsqueeze", "dD.view().sum.view().sum.view", "C.transpose", "dy.unsqueeze", "dC_.view"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.lssl.LegTTransition.inverse_mult", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.lssl.AdaptiveTransition.bilinear", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.lssl.LegTTransition.forward_mult"], ["", "@", "staticmethod", "\n", "def", "backward", "(", "ctx", ",", "dy", ")", ":", "\n", "        ", "\"\"\"Computes adjoint to the input u\n\n        dy: (L, ..., M)\n        \"\"\"", "\n", "dt", ",", "u", ",", "C", ",", "D", "=", "ctx", ".", "saved_tensors", "\n", "transition", "=", "ctx", ".", "transition", "\n", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "\n", "# Compute dx_", "\n", "            ", "dx_", "=", "torch", ".", "sum", "(", "dy", "[", "-", "1", "]", ".", "unsqueeze", "(", "-", "1", ")", "*", "C", ",", "dim", "=", "-", "2", ")", "# (..., N)", "\n", "\n", "# Compute du", "\n", "dyC", "=", "(", "C", ".", "transpose", "(", "-", "1", ",", "-", "2", ")", "@", "dy", ".", "unsqueeze", "(", "-", "1", ")", ")", ".", "squeeze", "(", "\n", "-", "1", "\n", ")", "# C^T dy (L, ..., N)", "\n", "dyD", "=", "torch", ".", "sum", "(", "dy", "*", "D", ",", "dim", "=", "-", "1", ")", "# D^T dy (L, ...)", "\n", "du", "=", "[", "]", "\n", "for", "dt_", ",", "dyC_", "in", "zip", "(", "dt", ".", "flip", "(", "0", ")", ",", "dyC", ".", "flip", "(", "0", ")", ")", ":", "\n", "                ", "dx_", "=", "transition", ".", "inverse_mult", "(", "dx_", ",", "dt_", "/", "2", ",", "transpose", "=", "True", ")", "# (..., N)", "\n", "du_", "=", "torch", ".", "sum", "(", "transition", ".", "B", "*", "dx_", ",", "dim", "=", "-", "1", ")", "# (...)", "\n", "du_", "=", "dt_", "*", "du_", "# (...)", "\n", "dx_", "=", "(", "\n", "transition", ".", "forward_mult", "(", "dx_", ",", "dt_", "/", "2", ",", "transpose", "=", "True", ")", "+", "dyC_", "\n", ")", "# (..., N)", "\n", "du", ".", "append", "(", "du_", ")", "\n", "", "du", "=", "torch", ".", "stack", "(", "du", ",", "dim", "=", "0", ")", "# (L, ...)", "\n", "du", "=", "du", ".", "flip", "(", "0", ")", "\n", "du", "=", "du", "+", "dyD", "\n", "\n", "# Compute dC, dD", "\n", "dC", "=", "torch", ".", "zeros_like", "(", "C", ")", "\n", "x_", "=", "u", ".", "new_zeros", "(", "u", ".", "shape", "[", "1", ":", "]", "+", "(", "transition", ".", "N", ",", ")", ")", "\n", "for", "dt_", ",", "u_", ",", "dy_", "in", "zip", "(", "dt", ",", "u", ",", "dy", ")", ":", "\n", "                ", "x_", "=", "transition", ".", "bilinear", "(", "dt_", ",", "x_", ",", "u_", ")", "# (..., N)", "\n", "dC_", "=", "dy_", ".", "unsqueeze", "(", "-", "1", ")", "*", "x_", ".", "unsqueeze", "(", "-", "2", ")", "# (..., M, N)", "\n", "dC", "+=", "dC_", ".", "view", "(", "(", "-", "1", ",", ")", "+", "C", ".", "shape", ")", ".", "sum", "(", "dim", "=", "0", ")", "# (M, N)", "\n", "", "dD", "=", "dy", "*", "u", ".", "unsqueeze", "(", "-", "1", ")", "# (L, ..., M)", "\n", "dD", "=", "dD", ".", "view", "(", "(", "-", "1", ",", ")", "+", "D", ".", "shape", ")", ".", "sum", "(", "dim", "=", "0", ")", "# (M,)", "\n", "\n", "", "if", "not", "ctx", ".", "needs_input_grad", "[", "0", "]", ":", "\n", "            ", "dx_", "=", "None", "\n", "", "return", "dx_", ",", "None", ",", "du", ",", "dC", ",", "dD", ",", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.ss.linear_system_recurrence.LinearSystemStepsizeFunction.forward": [[289, 320], ["zip", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "ctx.save_for_backward", "u.unsqueeze", "u.new_zeros", "transition.bilinear", "ys.append", "transition.bilinear.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.lssl.AdaptiveTransition.bilinear"], ["    ", "@", "staticmethod", "\n", "def", "forward", "(", "ctx", ",", "x", ",", "dt", ",", "u", ",", "C", ",", "D", ",", "transition", ")", ":", "\n", "        ", "\"\"\"\n        dt : (L, ...)\n        u : (L, ...)\n        C : (M, N)\n        D : (M,)\n        transition: Transition objective implementing forward_mult, inverse_mult, bilinear, quadratic\n\n        Returns:\n        y : (L, ..., M)\n        \"\"\"", "\n", "ctx", ".", "transition", "=", "transition", "\n", "# ctx.save_for_backward(dt, u, C, D)", "\n", "\n", "v", "=", "u", ".", "unsqueeze", "(", "-", "1", ")", "*", "D", "# (L, ..., M)", "\n", "\n", "if", "x", "is", "None", ":", "\n", "            ", "x", "=", "u", ".", "new_zeros", "(", "u", ".", "shape", "[", "1", ":", "]", "+", "(", "transition", ".", "N", ",", ")", ")", "\n", "", "ys", "=", "[", "]", "\n", "for", "dt_", ",", "u_", "in", "zip", "(", "dt", ",", "u", ")", ":", "\n", "            ", "x", "=", "transition", ".", "bilinear", "(", "dt_", ",", "x", ",", "u_", ")", "# (..., N)", "\n", "y", "=", "(", "C", "@", "x", ".", "unsqueeze", "(", "-", "1", ")", ")", ".", "squeeze", "(", "\n", "-", "1", "\n", ")", "# TODO can use sum instead of matmul if M = 1", "\n", "ys", ".", "append", "(", "y", ")", "\n", "", "y", "=", "torch", ".", "stack", "(", "ys", ",", "dim", "=", "0", ")", "\n", "y", "=", "y", "+", "v", "# (L, ..., M)", "\n", "\n", "ctx", ".", "save_for_backward", "(", "dt", ",", "u", ",", "C", ",", "D", ",", "x", ")", "\n", "return", "y", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.ss.linear_system_recurrence.LinearSystemStepsizeFunction.backward": [[321, 387], ["torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "zip", "torch.stack().flip", "torch.stack().flip", "torch.stack().flip", "torch.stack().flip", "torch.stack().flip", "torch.stack().flip", "torch.stack().flip", "torch.stack().flip", "torch.stack().flip", "torch.stack().flip", "torch.stack().flip", "torch.stack().flip", "torch.stack().flip", "torch.stack().flip", "torch.stack().flip", "torch.stack().flip", "torch.stack().flip", "torch.stack().flip", "dt.flip", "dyC.flip", "u.flip", "dy.flip", "dC_.view().sum", "dD_.view().sum", "transition.inverse_mult", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.stack().flip.append", "torch.stack().flip.append", "torch.stack().flip.append", "transition.bilinear", "transition.quadratic", "torch.stack().flip.append", "torch.stack().flip.append", "torch.stack().flip.append", "dy[].unsqueeze", "dy_.unsqueeze", "x_.unsqueeze", "u_.unsqueeze", "transition.forward_mult", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "C.transpose", "dy.unsqueeze", "dC_.view", "dD_.view", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.lssl.LegTTransition.inverse_mult", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.lssl.AdaptiveTransition.bilinear", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.hippo.transition.ToeplitzAdaptiveTransition.quadratic", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.lssl.LegTTransition.forward_mult"], ["", "@", "staticmethod", "\n", "def", "backward", "(", "ctx", ",", "dy", ")", ":", "\n", "        ", "\"\"\"\n        gradient:\n        dy: (L, ..., M)\n\n        state:\n        # dx_: (..., N)\n        x: (..., N)\n\n        cached arguments:\n        dt: (L, ...)\n        u: (L, ...)\n        \"\"\"", "\n", "\n", "# dt, u, C, D = ctx.saved_tensors", "\n", "dt", ",", "u", ",", "C", ",", "D", ",", "x_", "=", "ctx", ".", "saved_tensors", "\n", "transition", "=", "ctx", ".", "transition", "\n", "\n", "# Compute dx_", "\n", "dx_", "=", "torch", ".", "sum", "(", "dy", "[", "-", "1", "]", ".", "unsqueeze", "(", "-", "1", ")", "*", "C", ",", "dim", "=", "-", "2", ")", "# (..., N)", "\n", "\n", "dyC", "=", "(", "C", ".", "transpose", "(", "-", "1", ",", "-", "2", ")", "@", "dy", ".", "unsqueeze", "(", "-", "1", ")", ")", ".", "squeeze", "(", "-", "1", ")", "# C^T dy (L, ..., N)", "\n", "dyD", "=", "torch", ".", "sum", "(", "dy", "*", "D", ",", "dim", "=", "-", "1", ")", "# D^T dy (L, ...)", "\n", "\n", "dC", "=", "torch", ".", "zeros_like", "(", "C", ")", "\n", "dD", "=", "torch", ".", "zeros_like", "(", "D", ")", "\n", "du", "=", "[", "]", "\n", "ddt", "=", "[", "]", "\n", "for", "dt_", ",", "dyC_", ",", "u_", ",", "dy_", "in", "zip", "(", "dt", ".", "flip", "(", "0", ")", ",", "dyC", ".", "flip", "(", "0", ")", ",", "u", ".", "flip", "(", "0", ")", ",", "dy", ".", "flip", "(", "0", ")", ")", ":", "\n", "# dy_: (..., M)", "\n", "# x_: (..., N)", "\n", "# u_, dt_: (...)", "\n", "            ", "dC_", "=", "dy_", ".", "unsqueeze", "(", "-", "1", ")", "*", "x_", ".", "unsqueeze", "(", "-", "2", ")", "# (..., M, N)", "\n", "dC", "+=", "dC_", ".", "view", "(", "(", "-", "1", ",", ")", "+", "C", ".", "shape", ")", ".", "sum", "(", "dim", "=", "0", ")", "# (M, N)", "\n", "dD_", "=", "dy_", "*", "u_", ".", "unsqueeze", "(", "-", "1", ")", "# (..., M)", "\n", "dD", "+=", "dD_", ".", "view", "(", "(", "-", "1", ",", ")", "+", "D", ".", "shape", ")", ".", "sum", "(", "dim", "=", "0", ")", "# (M,)", "\n", "\n", "dx_", "=", "transition", ".", "inverse_mult", "(", "dx_", ",", "dt_", "/", "2", ",", "transpose", "=", "True", ")", "# (..., N)", "\n", "\n", "# Compute du", "\n", "du_", "=", "torch", ".", "sum", "(", "transition", ".", "B", "*", "dx_", ",", "dim", "=", "-", "1", ")", "# (...)", "\n", "du_", "=", "dt_", "*", "du_", "# (...)", "\n", "du", ".", "append", "(", "du_", ")", "\n", "\n", "x_prev", "=", "transition", ".", "bilinear", "(", "-", "dt_", ",", "x_", ",", "u_", ")", "# (..., N)", "\n", "ddt_", "=", "transition", ".", "quadratic", "(", "dx_", ",", "0.5", "*", "(", "x_prev", "+", "x_", ")", ")", "# (...)", "\n", "ddt_", "=", "ddt_", "+", "torch", ".", "sum", "(", "transition", ".", "B", "*", "dx_", ",", "dim", "=", "-", "1", ")", "*", "u_", "\n", "ddt", ".", "append", "(", "ddt_", ")", "# (...)", "\n", "x_", "=", "x_prev", "\n", "\n", "dx_", "=", "(", "\n", "transition", ".", "forward_mult", "(", "dx_", ",", "dt_", "/", "2", ",", "transpose", "=", "True", ")", "+", "dyC_", "\n", ")", "# (..., N)", "\n", "\n", "", "du", "=", "torch", ".", "stack", "(", "du", ",", "dim", "=", "0", ")", ".", "flip", "(", "0", ")", "# (L, ...)", "\n", "du", "=", "du", "+", "dyD", "\n", "\n", "ddt", "=", "torch", ".", "stack", "(", "ddt", ",", "dim", "=", "0", ")", ".", "flip", "(", "0", ")", "# (L, ...)", "\n", "\n", "# Sanity check", "\n", "# print(f\"{x_=}\") # should be 0 (initial state)", "\n", "\n", "if", "not", "ctx", ".", "needs_input_grad", "[", "0", "]", ":", "\n", "            ", "dx_", "=", "None", "\n", "", "return", "dx_", ",", "ddt", ",", "du", ",", "dC", ",", "dD", ",", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.ss.linear_system_recurrence._abs_err": [[392, 396], ["x.detach().cpu().numpy", "y.detach().cpu().numpy", "x.detach().cpu", "y.detach().cpu", "x.detach", "y.detach"], "function", ["None"], ["def", "_abs_err", "(", "x", ",", "y", ")", ":", "\n", "    ", "x_", "=", "x", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "y_", "=", "y", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "return", "(", "y_", "-", "x_", ")", "/", "x_", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.ss.linear_system_recurrence.test_linear_system": [[398, 480], ["torch.eye", "torch.eye", "torch.eye", "torch.ones", "torch.ones", "torch.ones", "torch.ones().to", "torch.ones().to", "torch.ones().to", "torch.ones().to", "torch.ones().to", "torch.ones().to", "torch.ones().to.retain_grad", "torch.ones().to.retain_grad", "torch.arange().to", "torch.arange().to", "torch.arange().to", "u.unsqueeze().unsqueeze().repeat.unsqueeze().unsqueeze().repeat", "u.unsqueeze().unsqueeze().repeat.retain_grad", "transition.ManualAdaptiveTransition().to.ManualAdaptiveTransition().to", "dt.to.to", "x.retain_grad", "y.retain_grad", "y.sum", "y.sum.backward", "torch.ones", "torch.ones", "torch.ones", "linear_system_recurrence.LinearSystemStepsize", "dt.to.requires_grad_", "dt.to.retain_grad", "linear_system_recurrence.LinearSystem", "linear_system_recurrence.LinearSystem.forward", "linear_system_recurrence.LinearSystem.forward", "LinearSystem.adjoint", "print", "print", "print", "print", "print", "u.unsqueeze().unsqueeze().repeat.grad.zero_", "dt.to.grad.zero_", "torch.ones().to.grad.zero_", "torch.ones().to.grad.zero_", "linearsystemstepsize", "print", "linearsystem.sum().backward", "print", "print", "print", "print", "linear_system_recurrence.LinearSystem.adjoint_input", "linear_system_recurrence.LinearSystem.adjoint_projection", "print", "print", "print", "print", "u.unsqueeze().unsqueeze().repeat.grad.zero_", "torch.ones().to.grad.zero_", "torch.ones().to.grad.zero_", "linearsystem", "print", "linearsystem.sum().backward", "print", "print", "print", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.arange", "torch.arange", "torch.arange", "u.unsqueeze().unsqueeze().repeat.unsqueeze().unsqueeze", "transition.ManualAdaptiveTransition().to.ManualAdaptiveTransition", "linear_system_recurrence._abs_err", "linear_system_recurrence._abs_err", "linear_system_recurrence._abs_err", "linear_system_recurrence._abs_err", "linear_system_recurrence._abs_err", "linear_system_recurrence._abs_err", "linear_system_recurrence._abs_err", "linearsystem.sum", "linearsystem.sum", "u.unsqueeze().unsqueeze().repeat.unsqueeze"], "function", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.cauchy.cauchy.CauchyMultiplySymmetric.backward", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.models.resnext.CifarResNeXt.forward", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.models.resnext.CifarResNeXt.forward", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.ss.linear_system_recurrence.LinearSystemStepsize.adjoint", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.cauchy.cauchy.CauchyMultiplySymmetric.backward", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.ss.linear_system_recurrence.LinearSystem.adjoint_input", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.ss.linear_system_recurrence.LinearSystem.adjoint_projection", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.cauchy.cauchy.CauchyMultiplySymmetric.backward", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.ss.linear_system_recurrence._abs_err", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.ss.linear_system_recurrence._abs_err", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.ss.linear_system_recurrence._abs_err", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.ss.linear_system_recurrence._abs_err", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.ss.linear_system_recurrence._abs_err", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.ss.linear_system_recurrence._abs_err", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.ss.linear_system_recurrence._abs_err"], ["", "def", "test_linear_system", "(", "L", ",", "batch", ",", "dim", ",", "N", ",", "M", ",", "stepsize", "=", "False", ")", ":", "\n", "    ", "from", "models", ".", "hippo", "import", "transition", "# for testing", "\n", "\n", "# Define A, B, C, D", "\n", "A", "=", "torch", ".", "eye", "(", "N", ")", "\n", "B", "=", "torch", ".", "ones", "(", "N", ")", "\n", "C", "=", "torch", ".", "ones", "(", "dim", ",", "M", ",", "N", ",", "requires_grad", "=", "True", ")", ".", "to", "(", "device", ")", "\n", "D", "=", "torch", ".", "ones", "(", "dim", ",", "M", ",", "requires_grad", "=", "True", ")", ".", "to", "(", "device", ")", "\n", "C", ".", "retain_grad", "(", ")", "\n", "D", ".", "retain_grad", "(", ")", "\n", "\n", "# Create u and dt", "\n", "u", "=", "torch", ".", "arange", "(", "L", ",", "dtype", "=", "torch", ".", "float", ",", "requires_grad", "=", "True", ")", ".", "to", "(", "device", ")", "\n", "u", "=", "u", ".", "unsqueeze", "(", "-", "1", ")", ".", "unsqueeze", "(", "-", "1", ")", ".", "repeat", "(", "(", "1", ",", "batch", ",", "dim", ")", ")", "# (L, B, D)", "\n", "u", ".", "retain_grad", "(", ")", "\n", "\n", "dt", "=", "torch", ".", "ones", "(", "L", ",", "batch", ",", "dim", ")", "*", "0.001", "# for LegT", "\n", "# dt = torch.ones_like(u, requires_grad=True).to(device) * 0.001 # for LegT", "\n", "# dt = torch.ones_like(u, requires_grad=True).to(device) * 0.1 # for LagT", "\n", "# dt.retain_grad()", "\n", "\n", "# Construct model", "\n", "transition", "=", "transition", ".", "ManualAdaptiveTransition", "(", "N", ",", "A", ",", "B", ")", ".", "to", "(", "device", ")", "\n", "# transition = transition.ConstantBilinearTransition(N, A, B, dt[0]).to(device)", "\n", "# transition = transition.LegTAdaptiveTransition(N).to(device)", "\n", "# transition = transition.LagTCumsumAdaptiveTransition(N).to(device)", "\n", "dt", "=", "dt", ".", "to", "(", "device", ")", "\n", "if", "stepsize", ":", "\n", "        ", "hippo", "=", "LinearSystemStepsize", "(", "N", ",", "transition", ",", "C", ",", "D", ")", "# .to(device)", "\n", "dt", ".", "requires_grad_", "(", "True", ")", "\n", "dt", ".", "retain_grad", "(", ")", "\n", "", "else", ":", "\n", "        ", "hippo", "=", "LinearSystem", "(", "N", ",", "transition", ",", "C", ",", "D", ")", "# .to(device)", "\n", "\n", "# Autograd", "\n", "", "if", "stepsize", ":", "\n", "        ", "y", ",", "x", "=", "hippo", ".", "forward", "(", "dt", ",", "u", ")", "\n", "", "else", ":", "\n", "        ", "y", ",", "x", "=", "hippo", ".", "forward", "(", "dt", ",", "u", ")", "\n", "", "x", ".", "retain_grad", "(", ")", "\n", "y", ".", "retain_grad", "(", ")", "\n", "z", "=", "y", ".", "sum", "(", ")", "\n", "z", ".", "backward", "(", "retain_graph", "=", "True", ")", "\n", "# print(f\"{y=}\")", "\n", "\n", "# Manual adjoint", "\n", "if", "stepsize", ":", "\n", "        ", "du", ",", "ddt", ",", "dC", ",", "dD", "=", "hippo", ".", "adjoint", "(", "y", ".", "grad", ",", "x", ",", "dt", ",", "u", ")", "\n", "print", "(", "\"du\"", ",", "u", ".", "grad", ",", "\"\\nerror\"", ",", "_abs_err", "(", "u", ".", "grad", ",", "du", ")", ")", "\n", "print", "(", "\"ddt\"", ",", "dt", ".", "grad", ",", "\"\\nerror\"", ",", "_abs_err", "(", "dt", ".", "grad", ",", "ddt", ")", ")", "\n", "print", "(", "\"dC\"", ",", "C", ".", "grad", ",", "\"\\nerror\"", ",", "_abs_err", "(", "C", ".", "grad", ",", "dC", ")", ")", "\n", "print", "(", "\"dD\"", ",", "D", ".", "grad", ",", "\"\\nerror\"", ",", "_abs_err", "(", "D", ".", "grad", ",", "dD", ")", ")", "\n", "\n", "print", "(", "\"Function vs Module abs error\"", ")", "\n", "u", ".", "grad", ".", "zero_", "(", ")", "\n", "dt", ".", "grad", ".", "zero_", "(", ")", "\n", "C", ".", "grad", ".", "zero_", "(", ")", "\n", "D", ".", "grad", ".", "zero_", "(", ")", "\n", "y_", "=", "linearsystemstepsize", "(", "None", ",", "dt", ",", "u", ",", "C", ",", "D", ",", "transition", ")", "\n", "print", "(", "f\"y\"", ",", "y_", "-", "y", ")", "\n", "y_", ".", "sum", "(", ")", ".", "backward", "(", ")", "\n", "print", "(", "\"du\"", ",", "u", ".", "grad", "-", "du", ")", "\n", "print", "(", "\"ddt\"", ",", "dt", ".", "grad", "-", "ddt", ")", "\n", "print", "(", "\"dC\"", ",", "C", ".", "grad", "-", "dC", ")", "\n", "print", "(", "\"dD\"", ",", "D", ".", "grad", "-", "dD", ")", "\n", "", "else", ":", "\n", "        ", "du", "=", "hippo", ".", "adjoint_input", "(", "y", ".", "grad", ",", "dt", ")", "\n", "dC", ",", "dD", "=", "hippo", ".", "adjoint_projection", "(", "y", ".", "grad", ",", "dt", ",", "u", ")", "\n", "print", "(", "\"du\"", ",", "u", ".", "grad", ",", "\"\\nerror\"", ",", "_abs_err", "(", "u", ".", "grad", ",", "du", ")", ")", "\n", "print", "(", "\"dC\"", ",", "C", ".", "grad", ",", "\"\\nerror\"", ",", "_abs_err", "(", "C", ".", "grad", ",", "dC", ")", ")", "\n", "print", "(", "\"dD\"", ",", "D", ".", "grad", ",", "\"\\nerror\"", ",", "_abs_err", "(", "D", ".", "grad", ",", "dD", ")", ")", "\n", "\n", "print", "(", "\"Function vs Module abs error\"", ")", "\n", "u", ".", "grad", ".", "zero_", "(", ")", "\n", "C", ".", "grad", ".", "zero_", "(", ")", "\n", "D", ".", "grad", ".", "zero_", "(", ")", "\n", "y_", "=", "linearsystem", "(", "None", ",", "dt", ",", "u", ",", "C", ",", "D", ",", "transition", ")", "\n", "print", "(", "f\"y\"", ",", "y_", "-", "y", ")", "\n", "y_", ".", "sum", "(", ")", ".", "backward", "(", ")", "\n", "print", "(", "\"du\"", ",", "u", ".", "grad", "-", "du", ")", "\n", "print", "(", "\"dC\"", ",", "C", ".", "grad", "-", "dC", ")", "\n", "print", "(", "\"dD\"", ",", "D", ".", "grad", "-", "dD", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.ss.lssl_recurrent.RecurrentLSSL.__init__": [[24, 75], ["src.models.sequence.base.SequenceModule.__init__", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "lssl_recurrent.RecurrentLSSL.register_buffer", "NotImplementedError", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "src.models.hippo.transition.ManualAdaptiveTransition", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "math.log", "math.log", "torch.eye", "torch.eye", "torch.eye", "torch.eye", "torch.eye", "torch.eye", "torch.eye", "torch.eye", "torch.eye", "torch.eye", "torch.eye", "torch.eye", "torch.eye", "torch.eye", "torch.eye", "torch.eye", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "src.models.hippo.transition.LegTAdaptiveTransition", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "src.models.hippo.transition.LagTCumsumAdaptiveTransition", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__"], ["def", "__init__", "(", "\n", "self", ",", "d", ",", "\n", "# memory_order,", "\n", "d_model", ",", "# overloading this term", "\n", "dt_min", "=", "0.01", ",", "\n", "dt_max", "=", "1.0", ",", "\n", "measure", "=", "'legt'", ",", "\n", "channels", "=", "None", ",", "\n", "# discretization='bilinear',", "\n", "init", "=", "'normal'", ",", "# for debugging, but might be useful?", "\n", "dropout", "=", "0.0", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        N: the order of the HiPPO projection\n        dt: discretization step size - should be roughly inverse to the length of the sequence\n        \"\"\"", "\n", "\n", "if", "dropout", ">", "0.0", ":", "\n", "            ", "raise", "NotImplementedError", "(", "\"Dropout currently not supported for Recurrent LSSL\"", ")", "\n", "\n", "", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "N", "=", "d_model", "\n", "self", ".", "d", "=", "d", "\n", "self", ".", "channels", "=", "channels", "\n", "\n", "dt", "=", "torch", ".", "exp", "(", "torch", ".", "linspace", "(", "math", ".", "log", "(", "dt_min", ")", ",", "math", ".", "log", "(", "dt_max", ")", ",", "self", ".", "d", ")", ")", "\n", "self", ".", "register_buffer", "(", "'dt'", ",", "dt", ")", "\n", "# self.dt = dt", "\n", "\n", "# Construct transition", "\n", "if", "measure", "==", "'identity'", ":", "\n", "            ", "A", ",", "B", "=", "torch", ".", "eye", "(", "self", ".", "N", ")", ",", "torch", ".", "ones", "(", "self", ".", "N", ")", "\n", "self", ".", "transition", "=", "transition", ".", "ManualAdaptiveTransition", "(", "self", ".", "N", ",", "A", ",", "B", ")", "\n", "", "elif", "measure", "==", "'legt'", ":", "\n", "            ", "self", ".", "transition", "=", "transition", ".", "LegTAdaptiveTransition", "(", "self", ".", "N", ")", "\n", "", "elif", "measure", "==", "'lagt'", ":", "\n", "            ", "self", ".", "transition", "=", "transition", ".", "LagTCumsumAdaptiveTransition", "(", "self", ".", "N", ")", "\n", "", "else", ":", "\n", "            ", "raise", "NotImplementedError", "\n", "\n", "", "if", "self", ".", "channels", "is", "None", ":", "\n", "            ", "self", ".", "m", "=", "1", "\n", "", "else", ":", "\n", "            ", "self", ".", "m", "=", "self", ".", "channels", "\n", "\n", "", "if", "init", "==", "'normal'", ":", "\n", "            ", "self", ".", "C", "=", "nn", ".", "Parameter", "(", "torch", ".", "randn", "(", "self", ".", "d", ",", "self", ".", "m", ",", "self", ".", "N", ")", ")", "\n", "self", ".", "D", "=", "nn", ".", "Parameter", "(", "torch", ".", "randn", "(", "self", ".", "d", ",", "self", ".", "m", ")", ")", "\n", "", "elif", "init", "==", "'constant'", ":", "\n", "            ", "self", ".", "C", "=", "nn", ".", "Parameter", "(", "torch", ".", "ones", "(", "self", ".", "d", ",", "self", ".", "m", ",", "self", ".", "N", ")", ")", "\n", "self", ".", "D", "=", "nn", ".", "Parameter", "(", "torch", ".", "ones", "(", "self", ".", "d", ",", "self", ".", "m", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.ss.lssl_recurrent.RecurrentLSSL.forward": [[76, 88], ["lssl_recurrent.RecurrentLSSL.dt.repeat", "src.models.sequence.ss.linear_system_recurrence.linearsystem", "src.models.sequence.ss.linear_system_recurrence.linearsystem.sum", "src.models.sequence.ss.linear_system_recurrence.linearsystem.squeeze"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "u", ",", "return_output", "=", "True", ")", ":", "\n", "        ", "\"\"\"\n        u: (L, B, D)\n        \"\"\"", "\n", "dt", "=", "self", ".", "dt", ".", "repeat", "(", "(", "u", ".", "shape", "[", "0", "]", ",", "u", ".", "shape", "[", "1", "]", ",", "1", ")", ")", "\n", "y", "=", "linearsystem", "(", "None", ",", "dt", ",", "u", ",", "self", ".", "C", ",", "self", ".", "D", ",", "self", ".", "transition", ")", "# (L, B, D, M)", "\n", "\n", "if", "self", ".", "channels", ":", "\n", "            ", "output", "=", "y", ".", "sum", "(", "dim", "=", "-", "2", ")", "# (L, B, M)", "\n", "", "else", ":", "\n", "            ", "output", "=", "y", ".", "squeeze", "(", "-", "1", ")", "# (L, B, D)", "\n", "", "return", "output", ",", "output", "[", "-", "1", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.ss.lssl_recurrent.RecurrentLSSL.default_state": [[89, 91], ["torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros"], "methods", ["None"], ["", "def", "default_state", "(", "self", ",", "*", "batch_shape", ",", "device", "=", "None", ")", ":", "\n", "        ", "return", "torch", ".", "zeros", "(", "*", "batch_shape", ",", "self", ".", "N", ",", "device", "=", "device", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.ss.lssl_recurrent.RecurrentLSSL.step": [[92, 94], ["NotImplementedError"], "methods", ["None"], ["", "def", "step", "(", "self", ",", "x", ",", "state", ")", ":", "\n", "        ", "raise", "NotImplementedError", "(", "\"Needs to be implemented.\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.ss.lssl_recurrent.RecurrentLSSL.d_state": [[95, 98], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "d_state", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "d", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.ss.lssl_recurrent.RecurrentLSSL.d_output": [[99, 102], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "d_output", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "d", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.ss.lssl_recurrent.RecurrentLSSL.state_to_tensor": [[103, 106], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "state_to_tensor", "(", "self", ")", ":", "\n", "        ", "return", "lambda", "state", ":", "state", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.ss.lssl.Platypus.__init__": [[51, 187], ["src.models.sequence.base.SequenceModule.__init__", "omegaconf.DictConfig", "src.models.nn.Activation", "torch.Dropout", "torch.Dropout", "torch.Dropout", "lssl.Platypus.dt.update", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.Parameter", "torch.Parameter", "torch.Parameter", "lssl.Platypus.register_buffer", "torch.Linear", "torch.Linear", "torch.Linear", "src.models.hippo.transition.ManualAdaptiveTransition", "kwargs.update", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "math.log", "math.log", "torch.utils.weight_norm", "torch.utils.weight_norm", "torch.utils.weight_norm", "torch.eye", "torch.eye", "torch.eye", "torch.eye", "torch.eye", "torch.eye", "torch.eye", "torch.eye", "torch.eye", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "src.models.hippo.transition.ManualAdaptiveTransition", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "src.models.hippo.transition.ManualAdaptiveTransition", "src.models.hippo.hippo.transition", "src.models.hippo.transition.ManualAdaptiveTransition", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "math.log", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "src.models.hippo.transition.LegTTriDInverseAdaptiveTransition", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "src.models.hippo.transition.LegTTriDInverseAdaptiveTransition", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "src.models.hippo.transition.ChebITriDInverseAdaptiveTransition", "src.models.hippo.transition.LagTTriDInverseAdaptiveTransition", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "math.log", "math.log", "src.models.hippo.transition.ChebIITriDInverseAdaptiveTransition", "src.models.hippo.transition.LegSTriDInverseAdaptiveTransition", "src.models.hippo.transition.LagTCumsumAdaptiveTransition", "src.models.hippo.transition.ChebITriDInverseAdaptiveTransition", "src.models.hippo.transition.GLagTToeplitzAdaptiveTransition", "src.models.hippo.transition.ChebIITriDInverseAdaptiveTransition", "src.models.hippo.transition.LegSTriDInverseAdaptiveTransition", "src.models.hippo.transition.LagTToeplitzAdaptiveTransition", "src.models.hippo.transition.JacTriDInverseAdaptiveTransition"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.s4.Activation", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.s4.transition"], ["def", "__init__", "(", "\n", "self", ",", "\n", "d", ",", "\n", "d_model", "=", "-", "1", ",", "# overloading this term, same as memory_order or N", "\n", "measure", "=", "'legs'", ",", "# 'legs', 'legt' main ones; can also try 'lagt'", "\n", "measure_args", "=", "{", "}", ",", "\n", "learn", "=", "0", ",", "# 0 means no learn, 1 means same A matrix for each hidden feature H, 2 means different A matrix per feature. 1 does not change parameter count. 2 adds parameters but does not slow down", "\n", "lr", "=", "0.0001", ",", "# controls learning rate of transition parameters", "\n", "noise", "=", "0.0", ",", "# injects input noise to the state space system", "\n", "init", "=", "'normal'", ",", "# for debugging, but might be useful?", "\n", "dt", "=", "None", ",", "\n", "channels", "=", "1", ",", "# denoted by M below", "\n", "bias", "=", "False", ",", "\n", "activation", "=", "'gelu'", ",", "\n", "ff", "=", "True", ",", "\n", "weight_norm", "=", "False", ",", "\n", "dropout", "=", "0.0", ",", "\n", "l_max", "=", "-", "1", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        N: the order of the HiPPO projection\n        dt: discretization step size - should be roughly inverse to the length of the sequence\n        \"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "d", "=", "d", "\n", "self", ".", "N", "=", "d_model", "if", "d_model", ">", "0", "else", "d", "\n", "self", ".", "dt", "=", "DictConfig", "(", "{", "\n", "'min'", ":", "0.001", ",", "\n", "'max'", ":", "0.1", ",", "\n", "'learn'", ":", "False", ",", "\n", "'lr'", ":", "0.001", ",", "\n", "'init'", ":", "'random'", ",", "\n", "}", ")", "\n", "if", "dt", "is", "not", "None", ":", "self", ".", "dt", ".", "update", "(", "dt", ")", "\n", "self", ".", "ff", "=", "ff", "\n", "self", ".", "bias", "=", "bias", "\n", "\n", "\n", "# Construct transition", "\n", "self", ".", "learn", "=", "learn", "\n", "if", "self", ".", "learn", "==", "0", ":", "\n", "            ", "if", "measure", "==", "'identity'", ":", "# for testing", "\n", "                ", "A", ",", "B", "=", "torch", ".", "eye", "(", "self", ".", "N", ")", ",", "torch", ".", "ones", "(", "self", ".", "N", ")", "\n", "self", ".", "transition", "=", "transition", ".", "ManualAdaptiveTransition", "(", "self", ".", "N", ",", "A", ",", "B", ")", "\n", "", "elif", "measure", "==", "'random'", ":", "\n", "                ", "A", "=", "torch", ".", "randn", "(", "self", ".", "N", ",", "self", ".", "N", ")", "/", "self", ".", "N", "# E[AA^T] = (1/N)I -- empirically I nans out", "\n", "B", "=", "torch", ".", "ones", "(", "self", ".", "N", ")", "# based on HiPPO matrices; worth trying random, haven't tried", "\n", "self", ".", "transition", "=", "transition", ".", "ManualAdaptiveTransition", "(", "self", ".", "N", ",", "A", ",", "B", ")", "\n", "", "elif", "measure", "==", "'legt'", ":", "\n", "# self.transition = transition.LegTAdaptiveTransition(self.N)", "\n", "                ", "self", ".", "transition", "=", "transition", ".", "LegTTriDInverseAdaptiveTransition", "(", "self", ".", "N", ",", "**", "measure_args", ")", "\n", "", "elif", "measure", "==", "'cheb'", ":", "\n", "                ", "self", ".", "transition", "=", "transition", ".", "ChebITriDInverseAdaptiveTransition", "(", "self", ".", "N", ",", "**", "measure_args", ")", "\n", "", "elif", "measure", "==", "'chebii'", ":", "\n", "                ", "self", ".", "transition", "=", "transition", ".", "ChebIITriDInverseAdaptiveTransition", "(", "self", ".", "N", ",", "**", "measure_args", ")", "\n", "", "elif", "measure", "==", "'lagt'", ":", "\n", "                ", "self", ".", "transition", "=", "transition", ".", "LagTCumsumAdaptiveTransition", "(", "self", ".", "N", ",", "**", "measure_args", ")", "\n", "", "elif", "measure", "==", "'glagt'", ":", "\n", "                ", "self", ".", "transition", "=", "transition", ".", "GLagTToeplitzAdaptiveTransition", "(", "self", ".", "N", ",", "**", "measure_args", ")", "\n", "", "elif", "measure", "==", "'legs'", ":", "\n", "                ", "self", ".", "transition", "=", "transition", ".", "LegSTriDInverseAdaptiveTransition", "(", "self", ".", "N", ",", "**", "measure_args", ")", "\n", "", "elif", "measure", "==", "'jac'", ":", "\n", "                ", "self", ".", "transition", "=", "transition", ".", "JacTriDInverseAdaptiveTransition", "(", "self", ".", "N", ",", "**", "measure_args", ")", "\n", "", "else", ":", "\n", "                ", "raise", "NotImplementedError", "\n", "", "", "elif", "self", ".", "learn", "==", "1", "or", "self", ".", "learn", "==", "2", ":", "\n", "            ", "kwargs", "=", "{", "'trainable'", ":", "True", ",", "'lr'", ":", "lr", "}", "\n", "kwargs", ".", "update", "(", "measure_args", ")", "\n", "if", "self", ".", "learn", "==", "2", ":", "\n", "                ", "kwargs", "[", "'batch'", "]", "=", "(", "self", ".", "d", ",", ")", "\n", "", "if", "measure", "==", "'random'", ":", "\n", "                ", "A", "=", "torch", ".", "randn", "(", "self", ".", "N", ",", "self", ".", "N", ")", "/", "self", ".", "N", "# E[AA^T] = (1/N)I . empirically I doesn't work, dunno why", "\n", "B", "=", "torch", ".", "ones", "(", "self", ".", "N", ")", "# based on HiPPO matrices; worth trying random, haven't tried", "\n", "self", ".", "transition", "=", "transition", ".", "ManualAdaptiveTransition", "(", "self", ".", "N", ",", "A", ",", "B", ",", "**", "kwargs", ")", "\n", "", "elif", "measure", "==", "'legt'", ":", "\n", "                ", "self", ".", "transition", "=", "transition", ".", "LegTTriDInverseAdaptiveTransition", "(", "self", ".", "N", ",", "**", "kwargs", ")", "\n", "", "elif", "measure", "==", "'lagt'", ":", "\n", "                ", "self", ".", "transition", "=", "transition", ".", "LagTTriDInverseAdaptiveTransition", "(", "self", ".", "N", ",", "**", "kwargs", ")", "\n", "", "elif", "measure", "==", "'legs'", ":", "\n", "                ", "self", ".", "transition", "=", "transition", ".", "LegSTriDInverseAdaptiveTransition", "(", "self", ".", "N", ",", "**", "kwargs", ")", "\n", "", "elif", "measure", "==", "'cheb'", ":", "\n", "                ", "self", ".", "transition", "=", "transition", ".", "ChebITriDInverseAdaptiveTransition", "(", "self", ".", "N", ",", "**", "kwargs", ")", "\n", "", "elif", "measure", "==", "'chebii'", ":", "\n", "                ", "self", ".", "transition", "=", "transition", ".", "ChebIITriDInverseAdaptiveTransition", "(", "self", ".", "N", ",", "**", "kwargs", ")", "\n", "", "elif", "measure", "==", "'toep'", ":", "\n", "                ", "self", ".", "transition", "=", "transition", ".", "LagTToeplitzAdaptiveTransition", "(", "self", ".", "N", ",", "**", "kwargs", ")", "\n", "", "else", ":", "raise", "NotImplementedError", "\n", "", "elif", "self", ".", "learn", "==", "3", ":", "# for debugging", "\n", "            ", "A", ",", "B", "=", "hippo", ".", "transition", "(", "measure", ",", "self", ".", "N", ")", "\n", "B", "=", "B", "[", ":", ",", "0", "]", "\n", "self", ".", "transition", "=", "transition", ".", "ManualAdaptiveTransition", "(", "self", ".", "N", ",", "A", ",", "B", ",", "trainable", "=", "True", ",", "lr", "=", "lr", ")", "\n", "", "else", ":", "\n", "            ", "raise", "NotImplementedError", "\n", "\n", "\n", "", "self", ".", "m", "=", "channels", "\n", "\n", "if", "init", "==", "'normal'", ":", "\n", "            ", "self", ".", "C", "=", "nn", ".", "Parameter", "(", "torch", ".", "randn", "(", "self", ".", "d", ",", "self", ".", "m", ",", "self", ".", "N", ")", ")", "\n", "self", ".", "D", "=", "nn", ".", "Parameter", "(", "torch", ".", "randn", "(", "self", ".", "d", ",", "self", ".", "m", ")", ")", "\n", "", "elif", "init", "==", "'constant'", ":", "\n", "            ", "self", ".", "C", "=", "nn", ".", "Parameter", "(", "torch", ".", "ones", "(", "self", ".", "d", ",", "self", ".", "m", ",", "self", ".", "N", ")", ")", "\n", "self", ".", "D", "=", "nn", ".", "Parameter", "(", "torch", ".", "ones", "(", "self", ".", "d", ",", "self", ".", "m", ")", ")", "\n", "", "elif", "init", "==", "'uniform'", ":", "\n", "            ", "self", ".", "C", "=", "nn", ".", "Parameter", "(", "1.732", "*", "torch", ".", "rand", "(", "self", ".", "d", ",", "self", ".", "m", ",", "self", ".", "N", ")", ")", "\n", "self", ".", "D", "=", "nn", ".", "Parameter", "(", "torch", ".", "randn", "(", "self", ".", "d", ",", "self", ".", "m", ")", ")", "\n", "", "else", ":", "raise", "NotImplementedError", "\n", "if", "self", ".", "bias", ":", "\n", "            ", "self", ".", "E", "=", "nn", ".", "Parameter", "(", "torch", ".", "zeros", "(", "self", ".", "d", ",", "self", ".", "m", ")", ")", "\n", "\n", "", "if", "self", ".", "dt", ".", "init", "==", "'uniform'", ":", "\n", "            ", "log_dt", "=", "torch", ".", "linspace", "(", "math", ".", "log", "(", "self", ".", "dt", ".", "min", ")", ",", "math", ".", "log", "(", "self", ".", "dt", ".", "max", ")", ",", "self", ".", "d", ")", "\n", "", "elif", "self", ".", "dt", ".", "init", "==", "'random'", ":", "\n", "            ", "log_dt", "=", "torch", ".", "rand", "(", "self", ".", "d", ")", "*", "(", "math", ".", "log", "(", "self", ".", "dt", ".", "max", ")", "-", "math", ".", "log", "(", "self", ".", "dt", ".", "min", ")", ")", "+", "math", ".", "log", "(", "self", ".", "dt", ".", "min", ")", "\n", "", "else", ":", "raise", "NotImplementedError", "\n", "\n", "if", "self", ".", "dt", ".", "learn", ":", "\n", "            ", "self", ".", "log_dt", "=", "nn", ".", "Parameter", "(", "log_dt", ")", "# (H)", "\n", "self", ".", "log_dt", ".", "_lr", "=", "self", ".", "dt", ".", "lr", "# register the parameter for the optimizer to reduce lr", "\n", "", "else", ":", "\n", "            ", "self", ".", "register_buffer", "(", "'log_dt'", ",", "log_dt", ")", "\n", "", "self", ".", "k", "=", "None", "\n", "self", ".", "noise", "=", "noise", "\n", "\n", "self", ".", "activate", "=", "Activation", "(", "activation", ")", "\n", "self", ".", "drop", "=", "nn", ".", "Dropout", "(", "dropout", ")", "\n", "\n", "if", "self", ".", "ff", ":", "\n", "            ", "self", ".", "output_linear", "=", "nn", ".", "Linear", "(", "self", ".", "m", "*", "self", ".", "d", ",", "self", ".", "d", ")", "\n", "\n", "if", "weight_norm", ":", "\n", "                ", "self", ".", "output_linear", "=", "nn", ".", "utils", ".", "weight_norm", "(", "self", ".", "output_linear", ")", "\n", "\n", "# For test time shift", "\n", "", "", "self", ".", "l_max", "=", "l_max", "\n", "self", ".", "last_len", "=", "-", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.ss.lssl.Platypus.forward": [[188, 309], ["u.transpose.transpose.transpose", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "lssl.linear_system_from_krylov", "lssl.Platypus.drop", "einops.rearrange", "lssl.Platypus.transpose", "lssl.Platypus.transition.gbt_B", "torch.stack.append", "torch.stack.append", "torch.stack.append", "torch.stack.append", "torch.stack.append", "torch.stack.append", "len", "lssl.Platypus.transition.gbt_A", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "src.models.functional.krylov.krylov", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "torch.cumsum", "torch.cumsum", "torch.cumsum", "torch.cumsum", "torch.cumsum", "torch.cumsum", "torch.cumsum", "torch.cumsum", "torch.cumsum", "contract", "src.models.functional.krylov.krylov", "lssl.Platypus.activate", "lssl.Platypus.output_linear", "round", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "b.broadcast_to", "torch.broadcast_tensors", "torch.broadcast_tensors", "torch.broadcast_tensors", "torch.broadcast_tensors", "torch.broadcast_tensors", "torch.broadcast_tensors", "torch.broadcast_tensors", "torch.broadcast_tensors", "torch.broadcast_tensors", "k_conv.squeeze", "k_noise.squeeze.squeeze.squeeze", "k_noise.squeeze.squeeze.unsqueeze", "lssl.Platypus.transition.gbt_A", "lssl.Platypus.transpose", "lssl.Platypus.C.transpose", "contract", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "contract", "contract", "next_state.detach.detach.detach", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "contract", "int", "int", "u.transpose.transpose.flip"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.lssl.StateSpace.linear_system_from_krylov", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.lssl.AdaptiveTransition.gbt_B", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.lssl.AdaptiveTransition.gbt_A", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.lssl.krylov", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.lssl.krylov", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.lssl.AdaptiveTransition.gbt_A"], ["", "def", "forward", "(", "self", ",", "u", ",", "*", "args", ",", "state", "=", "None", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"\n        u: (L, B, H) [21-09-29] Our backbone now passes inputs as (B, L, H). This calss originally expected (L, B, H) so we transpose accordingly\n        state: (B, H, N) previous hidden state of the recurrence\n        \"\"\"", "\n", "next_state", "=", "None", "\n", "\n", "u", "=", "u", ".", "transpose", "(", "0", ",", "1", ")", "\n", "\n", "# Construct dt (H)", "\n", "dt", "=", "torch", ".", "exp", "(", "self", ".", "log_dt", ")", "# Note: if dt is not learnable this slightly wastes computation, but it isn't a bottleneck", "\n", "\n", "## # Calculate test-time shift", "\n", "# changed sampling rate; uncache Krylov", "\n", "if", "self", ".", "last_len", "!=", "u", ".", "shape", "[", "0", "]", ":", "\n", "            ", "self", ".", "k", "=", "None", "\n", "self", ".", "last_len", "=", "u", ".", "shape", "[", "0", "]", "\n", "# Calculate change from train sampling rate", "\n", "", "if", "self", ".", "l_max", ">", "0", ":", "\n", "            ", "rate", "=", "self", ".", "l_max", "/", "u", ".", "shape", "[", "0", "]", "\n", "# if rate != 1.0: dt = dt * rate", "\n", "if", "rate", "!=", "1.0", ":", "rate", "=", "round", "(", "rate", ")", "\n", "else", ":", "rate", "=", "None", "\n", "", "else", ":", "\n", "            ", "rate", "=", "None", "\n", "\n", "\n", "# We need to compute the \"recurrence\" if", "\n", "# (*) there is noise or an initial state", "\n", "# (*) we're learning the system A, B", "\n", "# (*) first pass", "\n", "", "kb", "=", "[", "]", "# will store the B vectors for Krylov computation", "\n", "_learn", "=", "(", "self", ".", "dt", ".", "learn", "or", "self", ".", "learn", ")", "and", "self", ".", "training", "# need to learn and it's training time # TODO this ignores the last training minibatch if no test time shift (prev batch's K gets cached)... should recalculate A in the last_len check ideally", "\n", "_conv", "=", "_learn", "or", "self", ".", "k", "is", "None", "or", "u", ".", "shape", "[", "0", "]", ">", "self", ".", "k", ".", "shape", "[", "-", "1", "]", "# or rate", "\n", "_noise", "=", "self", ".", "noise", ">", "0.0", "and", "self", ".", "training", "\n", "if", "_conv", ":", "\n", "            ", "B", "=", "self", ".", "transition", ".", "gbt_B", "(", "dt", ")", "# (..., N) depending if learn=2", "\n", "kb", ".", "append", "(", "B", ")", "\n", "", "if", "_noise", ":", "\n", "            ", "noise", "=", "self", ".", "noise", "*", "torch", ".", "randn", "(", "self", ".", "d", ",", "self", ".", "N", ",", "dtype", "=", "u", ".", "dtype", ",", "device", "=", "u", ".", "device", ")", "# (H, N)", "\n", "kb", ".", "append", "(", "noise", ")", "\n", "\n", "", "A", "=", "None", "\n", "if", "len", "(", "kb", ")", ">", "0", ":", "\n", "            ", "if", "rate", "is", "not", "None", ":", "\n", "                ", "dt", "=", "dt", "*", "rate", "\n", "\n", "", "A", "=", "self", ".", "transition", ".", "gbt_A", "(", "dt", ")", "# (..., N, N) (..., N)", "\n", "\n", "# Adjust by rate", "\n", "# if _conv and rate is not None:", "\n", "#     while rate > 1:", "\n", "#         B = B + torch.sum(A * B.unsqueeze(-2), dim=-1) # (I + A) @ B", "\n", "#         A = A @ A", "\n", "#         rate //= 2", "\n", "\n", "kb", "=", "[", "b", ".", "broadcast_to", "(", "dt", ".", "shape", "+", "(", "self", ".", "N", ",", ")", ")", "for", "b", "in", "kb", "]", "\n", "kb", "=", "torch", ".", "stack", "(", "torch", ".", "broadcast_tensors", "(", "*", "kb", ")", ",", "dim", "=", "0", ")", "# each (..., N)", "\n", "krylovs", "=", "krylov", "(", "u", ".", "shape", "[", "0", "]", ",", "A", ",", "kb", ")", "# (H, N, L) each", "\n", "k_noise", ",", "k_conv", "=", "torch", ".", "split", "(", "\n", "krylovs", ",", "\n", "split_size_or_sections", "=", "[", "int", "(", "_noise", ")", ",", "int", "(", "_conv", ")", "]", ",", "\n", "dim", "=", "0", "\n", ")", "\n", "if", "_conv", ":", "# Cache the Krylov matrix K(A, B)", "\n", "                ", "self", ".", "k", "=", "k_conv", ".", "squeeze", "(", "0", ")", "# (H, N, L)", "\n", "", "if", "_noise", ":", "\n", "                ", "k_noise", "=", "k_noise", ".", "squeeze", "(", "0", ")", "# (H, N, L)", "\n", "\n", "# Convolution", "\n", "", "", "y", "=", "linear_system_from_krylov", "(", "u", ",", "self", ".", "C", ",", "self", ".", "D", ",", "self", ".", "k", "[", "...", ",", ":", "u", ".", "shape", "[", "0", "]", "]", ")", "# (L, B, H, M)", "\n", "if", "_noise", ":", "\n", "            ", "k_noise", "=", "torch", ".", "cumsum", "(", "k_noise", ",", "dim", "=", "-", "1", ")", "# (H, N, L) w + Aw + A^2w + ...", "\n", "k_noise", "=", "contract", "(", "'h m n, h n l -> l h m'", ",", "self", ".", "C", ",", "k_noise", ")", "# C @ k", "\n", "y", "=", "y", "+", "k_noise", ".", "unsqueeze", "(", "1", ")", "# (L, B, H, M)", "\n", "y", "=", "y", "+", "self", ".", "noise", "*", "torch", ".", "randn", "(", "y", ".", "shape", ",", "dtype", "=", "u", ".", "dtype", ",", "device", "=", "u", ".", "device", ")", "\n", "\n", "# State needs a special case because it has a batch dimension", "\n", "", "if", "state", "is", "not", "None", ":", "# (B, H, N)", "\n", "            ", "if", "A", "is", "None", ":", "A", "=", "self", ".", "transition", ".", "gbt_A", "(", "dt", ")", "# (..., N, N) (..., N)", "\n", "\n", "ATC", ",", "ATL", "=", "krylov", "(", "u", ".", "shape", "[", "0", "]", ",", "A", ".", "transpose", "(", "-", "1", ",", "-", "2", ")", ",", "self", ".", "C", ".", "transpose", "(", "0", ",", "1", ")", ",", "return_power", "=", "True", ")", "# (M, H, N, L), (H, N, N) represents A^T C and (A^T)^L", "\n", "y", "=", "y", "+", "contract", "(", "'mhnl, bhn -> lbhm'", ",", "ATC", ",", "state", ")", "\n", "\n", "# Compute next state", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                ", "next_state", "=", "contract", "(", "'hnp, bhn -> bhp'", ",", "ATL", ",", "state", ")", "\n", "if", "_noise", ":", "\n", "                    ", "next_state", "=", "next_state", "+", "k_noise", "[", "...", ",", "-", "1", "]", "\n", "", "next_state", "=", "next_state", "+", "contract", "(", "'lbh, hnl -> bhn'", ",", "u", ".", "flip", "(", "0", ")", ",", "self", ".", "k", "[", ":", "...", ",", "u", ".", "shape", "[", "0", "]", "]", ")", "# (B, H, N)", "\n", "next_state", "=", "contract", "(", "'hnp, bhp -> bhn'", ",", "A", ",", "next_state", ")", "\n", "next_state", "=", "next_state", ".", "detach", "(", ")", "# TODO necessary?", "\n", "\n", "# Debugging code useful for checking if state computation is correct", "\n", "# from models.functional.unroll import variable_unroll_sequential, variable_unroll", "\n", "# B = self.transition.gbt_B(dt)", "\n", "# inps = B*u.unsqueeze(-1) # (L, B, H, N)", "\n", "# inps[0] = inps[0] + state", "\n", "# xx = variable_unroll(A, inps, variable=False)", "\n", "# yy = torch.sum(self.C * xx.unsqueeze(-2), dim=-1)", "\n", "# yy = yy + u.unsqueeze(-1) * self.D # true output y; should equal y", "\n", "# xx_ = variable_unroll(A, B*u.unsqueeze(-1), variable=False)", "\n", "# yy_ = torch.sum(self.C * xx_.unsqueeze(-2), dim=-1)", "\n", "# yy_ = yy_ + u.unsqueeze(-1) * self.D # output without state; should equal y before the C A^T S term was added", "\n", "# ss = (A @ xx[-1].unsqueeze(-1)).squeeze(-1) # should equal next_state", "\n", "# breakpoint()", "\n", "# y = z", "\n", "\n", "# bias term", "\n", "", "", "if", "self", ".", "bias", ":", "\n", "            ", "y", "=", "y", "+", "self", ".", "E", "\n", "\n", "", "y", "=", "self", ".", "drop", "(", "self", ".", "activate", "(", "y", ")", ")", "\n", "\n", "y", "=", "rearrange", "(", "y", ",", "'l b h m -> l b (h m)'", ")", "# (L, B, H*M)", "\n", "\n", "if", "self", ".", "ff", ":", "\n", "            ", "y", "=", "self", ".", "output_linear", "(", "y", ")", "# (L, B, H)", "\n", "# y = self.drop(y) # moved to residual", "\n", "", "y", "=", "y", ".", "transpose", "(", "0", ",", "1", ")", "# Back to (B, L, H) as expected", "\n", "return", "y", ",", "next_state", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.ss.lssl.Platypus.is_initialized": [[310, 312], ["None"], "methods", ["None"], ["", "def", "is_initialized", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "k", "is", "not", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.ss.lssl.Platypus.initialize": [[313, 322], ["torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "lssl.Platypus.transition.gbt_A", "lssl.Platypus.transition.gbt_B", "src.models.functional.krylov.krylov"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.lssl.AdaptiveTransition.gbt_A", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.lssl.AdaptiveTransition.gbt_B", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.lssl.krylov"], ["", "def", "initialize", "(", "self", ",", "shared_params", ")", ":", "\n", "        ", "if", "'k'", "in", "shared_params", ":", "\n", "            ", "self", ".", "k", "=", "shared_params", "[", "'k'", "]", "\n", "", "else", ":", "\n", "            ", "dt", "=", "torch", ".", "exp", "(", "self", ".", "log_dt", ")", "\n", "A", "=", "self", ".", "transition", ".", "gbt_A", "(", "dt", ")", "# (..., N, N)", "\n", "B", "=", "self", ".", "transition", ".", "gbt_B", "(", "dt", ")", "# (..., N)", "\n", "self", ".", "k", "=", "krylov", "(", "1024", ",", "A", ",", "B", ")", "# (L, H, N) each", "\n", "shared_params", "[", "'k'", "]", "=", "self", ".", "k", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.ss.lssl.Platypus.default_state": [[323, 325], ["torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros"], "methods", ["None"], ["", "", "def", "default_state", "(", "self", ",", "*", "batch_shape", ",", "device", "=", "None", ")", ":", "\n", "        ", "return", "torch", ".", "zeros", "(", "*", "batch_shape", ",", "self", ".", "N", ",", "device", "=", "device", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.ss.lssl.Platypus.step": [[326, 328], ["NotImplementedError"], "methods", ["None"], ["", "def", "step", "(", "self", ",", "x", ",", "state", ")", ":", "\n", "        ", "raise", "NotImplementedError", "(", "\"Needs to be implemented.\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.ss.lssl.Platypus.d_state": [[329, 332], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "d_state", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "d", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.ss.lssl.Platypus.d_output": [[333, 336], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "d_output", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "d", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.ss.lssl.Platypus.state_to_tensor": [[337, 340], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "state_to_tensor", "(", "self", ")", ":", "\n", "        ", "return", "lambda", "state", ":", "state", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.ss.lssl.linear_system_from_krylov": [[17, 44], ["einops.rearrange", "k.unsqueeze.to", "k.unsqueeze.unsqueeze", "u.unsqueeze().transpose", "src.models.functional.toeplitz.causal_convolution", "y.transpose.transpose", "u.unsqueeze", "u.unsqueeze"], "function", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.toeplitz.causal_convolution"], ["def", "linear_system_from_krylov", "(", "u", ",", "C", ",", "D", ",", "k", ")", ":", "\n", "    ", "\"\"\"\n    Computes the state-space system y = Cx + Du from Krylov matrix K(A, B)\n\n    u: (L, B, ...) ... = H\n    C: (..., M, N) ... = H\n    D: (..., M)\n    k: (..., N, L) Krylov matrix representing b, Ab, A^2b...\n\n    y: (L, B, ..., M)\n    \"\"\"", "\n", "\n", "\n", "# Equivalent ways to perform C @ k, slight speed differences", "\n", "k", "=", "C", "@", "k", "# (..., M, L)", "\n", "# k = torch.einsum('... m n, ... n l -> ... m l', C, k) # C @ k", "\n", "# k = torch.sum(k.unsqueeze(-3) * C.unsqueeze(-1), dim=-2) # (..., M, L) C @ k", "\n", "\n", "k", "=", "rearrange", "(", "k", ",", "'... m l -> m ... l'", ")", "\n", "k", "=", "k", ".", "to", "(", "u", ")", "# if training in half precision, need to go back to float32 for the fft", "\n", "k", "=", "k", ".", "unsqueeze", "(", "1", ")", "# (M, 1, ..., L)", "\n", "\n", "v", "=", "u", ".", "unsqueeze", "(", "-", "1", ")", ".", "transpose", "(", "0", ",", "-", "1", ")", "# (1, B, ..., L)", "\n", "y", "=", "causal_convolution", "(", "k", ",", "v", ",", "fast", "=", "True", ")", "# (M, B, ..., L)", "\n", "y", "=", "y", ".", "transpose", "(", "0", ",", "-", "1", ")", "# (L, B, ..., M)", "\n", "y", "=", "y", "+", "u", ".", "unsqueeze", "(", "-", "1", ")", "*", "D", "# (L, B, ..., M)", "\n", "return", "y", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.ss.s4.S4.__init__": [[31, 116], ["torch.Module.__init__", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "src.models.sequence.ss.kernel.HippoSSKernel", "src.utils.train.get_logger", "src.utils.train.get_logger.info", "src.models.nn.Activation", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "src.models.nn.Activation", "src.models.nn.LinearActivation", "dropout_fn", "torch.Identity", "torch.Identity", "torch.Identity", "torch.Identity", "src.models.nn.Normalization", "torch.Identity", "torch.Identity", "torch.Identity", "torch.Identity"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.s4.get_logger", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.s4.Activation", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.s4.Activation", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.s4.LinearActivation"], ["def", "__init__", "(", "\n", "self", ",", "\n", "d_model", ",", "\n", "d_state", "=", "64", ",", "\n", "l_max", "=", "1", ",", "# Maximum length of sequence. Fine if not provided: the kernel will keep doubling in length until longer than sequence. However, this can be marginally slower if the true length is not a power of 2", "\n", "channels", "=", "1", ",", "# maps 1-dim to C-dim", "\n", "bidirectional", "=", "False", ",", "\n", "# Arguments for FF", "\n", "activation", "=", "'gelu'", ",", "# activation in between SS and FF", "\n", "ln", "=", "False", ",", "# Extra normalization", "\n", "postact", "=", "None", ",", "# activation after FF", "\n", "initializer", "=", "None", ",", "# initializer on FF", "\n", "weight_norm", "=", "False", ",", "# weight normalization on FF", "\n", "hyper_act", "=", "None", ",", "# Use a \"hypernetwork\" multiplication", "\n", "dropout", "=", "0.0", ",", "\n", "transposed", "=", "True", ",", "# axis ordering (B, L, D) or (B, D, L)", "\n", "verbose", "=", "False", ",", "\n", "shift", "=", "False", ",", "\n", "linear", "=", "False", ",", "\n", "# SSM Kernel arguments", "\n", "**", "kernel_args", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        d_state: the dimension of the state, also denoted by N\n        l_max: the maximum sequence length, also denoted by L\n          if this is not known at model creation, set l_max=1\n        channels: can be interpreted as a number of \"heads\"\n        bidirectional: bidirectional\n        dropout: standard dropout argument\n        transposed: choose backbone axis ordering of (B, L, H) or (B, H, L) [B=batch size, L=sequence length, H=hidden dimension]\n\n        Other options are all experimental and should not need to be configured\n        \"\"\"", "\n", "\n", "super", "(", ")", ".", "__init__", "(", ")", "\n", "if", "verbose", ":", "\n", "            ", "import", "src", ".", "utils", ".", "train", "\n", "log", "=", "src", ".", "utils", ".", "train", ".", "get_logger", "(", "__name__", ")", "\n", "log", ".", "info", "(", "f\"Constructing S4 (H, N, L) = ({d_model}, {d_state}, {l_max})\"", ")", "\n", "\n", "", "self", ".", "h", "=", "d_model", "\n", "self", ".", "n", "=", "d_state", "\n", "self", ".", "bidirectional", "=", "bidirectional", "\n", "self", ".", "ln", "=", "ln", "\n", "self", ".", "channels", "=", "channels", "\n", "self", ".", "transposed", "=", "transposed", "\n", "self", ".", "shift", "=", "shift", "\n", "self", ".", "linear", "=", "linear", "\n", "\n", "# optional multiplicative modulation GLU-style", "\n", "# https://arxiv.org/abs/2002.05202", "\n", "self", ".", "hyper", "=", "hyper_act", "is", "not", "None", "\n", "if", "self", ".", "hyper", ":", "\n", "            ", "channels", "*=", "2", "\n", "self", ".", "hyper_activation", "=", "Activation", "(", "hyper_act", ")", "\n", "\n", "", "self", ".", "D", "=", "nn", ".", "Parameter", "(", "torch", ".", "randn", "(", "channels", ",", "self", ".", "h", ")", ")", "\n", "\n", "if", "self", ".", "bidirectional", ":", "\n", "            ", "channels", "*=", "2", "\n", "\n", "\n", "# SSM Kernel", "\n", "", "self", ".", "kernel", "=", "HippoSSKernel", "(", "self", ".", "h", ",", "N", "=", "self", ".", "n", ",", "L", "=", "l_max", ",", "channels", "=", "channels", ",", "verbose", "=", "verbose", ",", "**", "kernel_args", ")", "\n", "\n", "# Pointwise", "\n", "if", "not", "self", ".", "linear", ":", "\n", "            ", "self", ".", "activation", "=", "Activation", "(", "activation", ")", "\n", "dropout_fn", "=", "nn", ".", "Dropout2d", "if", "self", ".", "transposed", "else", "nn", ".", "Dropout", "\n", "self", ".", "dropout", "=", "dropout_fn", "(", "dropout", ")", "if", "dropout", ">", "0.0", "else", "nn", ".", "Identity", "(", ")", "\n", "if", "self", ".", "ln", ":", "\n", "                ", "self", ".", "norm", "=", "Normalization", "(", "self", ".", "h", "*", "self", ".", "channels", ",", "transposed", "=", "transposed", ")", "\n", "", "else", ":", "\n", "                ", "self", ".", "norm", "=", "nn", ".", "Identity", "(", ")", "\n", "\n", "# position-wise output transform to mix features", "\n", "", "", "if", "not", "self", ".", "linear", ":", "\n", "            ", "self", ".", "output_linear", "=", "LinearActivation", "(", "\n", "self", ".", "h", "*", "self", ".", "channels", ",", "\n", "self", ".", "h", ",", "\n", "transposed", "=", "self", ".", "transposed", ",", "\n", "initializer", "=", "initializer", ",", "\n", "activation", "=", "postact", ",", "\n", "activate", "=", "True", ",", "\n", "weight_norm", "=", "weight_norm", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.ss.s4.S4.forward": [[119, 181], ["u.transpose.transpose.size", "s4.S4.kernel", "einops.rearrange", "u.transpose.transpose.transpose", "einops.rearrange", "torch.fft.rfft", "torch.fft.rfft", "torch.fft.rfft", "torch.fft.rfft", "torch.fft.rfft", "torch.fft.rfft", "torch.fft.rfft", "torch.fft.rfft", "torch.fft.rfft", "torch.fft.rfft", "torch.fft.rfft", "torch.fft.rfft", "torch.fft.rfft", "torch.fft.rfft", "torch.fft.rfft", "torch.fft.rfft", "torch.fft.rfft", "torch.fft.rfft", "torch.fft.rfft", "torch.fft.rfft", "torch.fft.rfft", "torch.fft.rfft", "torch.fft.rfft", "torch.fft.rfft", "torch.fft.rfft", "torch.fft.rfft", "torch.fft.rfft", "torch.fft.rfft", "torch.fft.rfft", "torch.fft.rfft", "torch.fft.rfft", "torch.fft.rfft", "contract", "[].flip", "torch.fft.rfft", "torch.fft.rfft", "torch.fft.rfft", "torch.fft.rfft", "torch.fft.rfft", "torch.fft.rfft", "torch.fft.rfft", "torch.fft.rfft", "torch.fft.rfft", "torch.fft.rfft", "torch.fft.rfft", "torch.fft.rfft", "torch.fft.rfft", "torch.fft.rfft", "torch.fft.rfft", "torch.fft.rfft", "torch.fft.rfft", "torch.fft.rfft", "torch.fft.rfft", "torch.fft.rfft", "torch.fft.rfft", "torch.fft.rfft", "torch.fft.rfft", "torch.fft.rfft", "torch.fft.rfft", "torch.fft.rfft", "torch.fft.rfft", "torch.fft.rfft", "torch.fft.rfft", "torch.fft.rfft", "torch.fft.rfft", "torch.fft.rfft", "contract", "contract", "s4.S4.kernel.forward_state", "einops.rearrange", "s4.S4.dropout", "s4.S4.transpose", "s4.S4.norm", "s4.S4.output_linear", "torch.pad", "torch.pad", "torch.pad", "torch.pad", "torch.pad", "torch.pad", "torch.pad", "torch.pad", "torch.pad", "torch.pad", "torch.pad", "torch.pad", "torch.pad", "torch.pad", "torch.pad", "torch.pad", "torch.fft.irfft", "torch.fft.irfft", "torch.fft.irfft", "torch.fft.irfft", "torch.fft.irfft", "torch.fft.irfft", "torch.fft.irfft", "torch.fft.irfft", "torch.fft.irfft", "torch.fft.irfft", "torch.fft.irfft", "torch.fft.irfft", "torch.fft.irfft", "torch.fft.irfft", "torch.fft.irfft", "torch.fft.irfft", "s4.S4.hyper_activation", "s4.S4.activation", "k1.flip", "k.flip", "u.transpose.transpose.flip", "torch.fft.irfft", "torch.fft.irfft", "torch.fft.irfft", "torch.fft.irfft", "torch.fft.irfft", "torch.fft.irfft", "torch.fft.irfft", "torch.fft.irfft", "torch.fft.irfft", "torch.fft.irfft", "torch.fft.irfft", "torch.fft.irfft", "torch.fft.irfft", "torch.fft.irfft", "torch.fft.irfft", "torch.fft.irfft"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.sc.pad", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.sc.pad", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.sc.pad", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.sc.pad", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.sc.pad", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.sc.pad", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.sc.pad", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.sc.pad", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.sc.pad", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.sc.pad", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.sc.pad", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.sc.pad", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.sc.pad", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.sc.pad", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.sc.pad", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.sc.pad"], ["", "", "def", "forward", "(", "self", ",", "u", ",", "state", "=", "None", ",", "**", "kwargs", ")", ":", "# absorbs return_output and transformer src mask", "\n", "        ", "\"\"\"\n        u: (B H L) if self.transposed else (B L H)\n        state: (H N) never needed unless you know what you're doing\n\n        Returns: same shape as u\n        \"\"\"", "\n", "if", "not", "self", ".", "transposed", ":", "u", "=", "u", ".", "transpose", "(", "-", "1", ",", "-", "2", ")", "\n", "L", "=", "u", ".", "size", "(", "-", "1", ")", "\n", "\n", "# Compute SS Kernel", "\n", "k", ",", "k_state", "=", "self", ".", "kernel", "(", "L", "=", "L", ",", "state", "=", "state", ")", "# (C H L) (B C H L)", "\n", "\n", "# Convolution", "\n", "if", "self", ".", "bidirectional", ":", "\n", "            ", "k0", ",", "k1", "=", "rearrange", "(", "k", ",", "'(s c) h l -> s c h l'", ",", "s", "=", "2", ")", "\n", "k", "=", "F", ".", "pad", "(", "k0", ",", "(", "0", ",", "L", ")", ")", "+", "F", ".", "pad", "(", "k1", ".", "flip", "(", "-", "1", ")", ",", "(", "L", ",", "0", ")", ")", "\n", "", "if", "self", ".", "shift", ":", "\n", "# Try flip and pad to correct for potential off-by-one", "\n", "            ", "k_f", "=", "torch", ".", "fft", ".", "rfft", "(", "F", ".", "pad", "(", "k", ".", "flip", "(", "-", "1", ")", ",", "(", "L", ",", "0", ")", ")", ",", "n", "=", "2", "*", "L", ")", "# (C H L)", "\n", "u_f", "=", "torch", ".", "fft", ".", "rfft", "(", "F", ".", "pad", "(", "u", ".", "flip", "(", "-", "1", ")", ",", "(", "L", ",", "0", ")", ")", ",", "n", "=", "2", "*", "L", ")", "# (B H L)", "\n", "y_f", "=", "contract", "(", "'bhl,chl->bchl'", ",", "u_f", ",", "k_f", ")", "# k_f.unsqueeze(-4) * u_f.unsqueeze(-3) # (B C H L)", "\n", "y", "=", "torch", ".", "fft", ".", "irfft", "(", "y_f", ",", "n", "=", "2", "*", "L", ")", "[", "...", ",", "L", ":", "]", ".", "flip", "(", "-", "1", ")", "# (B C H L)", "\n", "", "else", ":", "\n", "            ", "k_f", "=", "torch", ".", "fft", ".", "rfft", "(", "k", ",", "n", "=", "2", "*", "L", ")", "# (C H L)", "\n", "u_f", "=", "torch", ".", "fft", ".", "rfft", "(", "u", ",", "n", "=", "2", "*", "L", ")", "# (B H L)", "\n", "y_f", "=", "contract", "(", "'bhl,chl->bchl'", ",", "u_f", ",", "k_f", ")", "# k_f.unsqueeze(-4) * u_f.unsqueeze(-3) # (B C H L)", "\n", "y", "=", "torch", ".", "fft", ".", "irfft", "(", "y_f", ",", "n", "=", "2", "*", "L", ")", "[", "...", ",", ":", "L", "]", "# (B C H L)", "\n", "\n", "\n", "\n", "# Compute D term in state space equation - essentially a skip connection", "\n", "", "y", "=", "y", "+", "contract", "(", "'bhl,ch->bchl'", ",", "u", ",", "self", ".", "D", ")", "# u.unsqueeze(-3) * self.D.unsqueeze(-1)", "\n", "\n", "# Compute state update", "\n", "if", "state", "is", "not", "None", ":", "\n", "            ", "assert", "not", "self", ".", "bidirectional", ",", "\"Bidirectional not supported with state forwarding\"", "\n", "y", "=", "y", "+", "k_state", "\n", "next_state", "=", "self", ".", "kernel", ".", "forward_state", "(", "u", ",", "state", ")", "\n", "", "else", ":", "\n", "            ", "next_state", "=", "None", "\n", "\n", "# Optional hyper-network multiplication", "\n", "", "if", "self", ".", "hyper", ":", "\n", "            ", "y", ",", "yh", "=", "rearrange", "(", "y", ",", "'b (s c) h l -> s b c h l'", ",", "s", "=", "2", ")", "\n", "y", "=", "self", ".", "hyper_activation", "(", "yh", ")", "*", "y", "\n", "\n", "# Reshape to flatten channels", "\n", "", "y", "=", "rearrange", "(", "y", ",", "'... c h l -> ... (c h) l'", ")", "\n", "\n", "if", "not", "self", ".", "linear", ":", "\n", "            ", "y", "=", "self", ".", "dropout", "(", "self", ".", "activation", "(", "y", ")", ")", "\n", "\n", "", "if", "not", "self", ".", "transposed", ":", "y", "=", "y", ".", "transpose", "(", "-", "1", ",", "-", "2", ")", "\n", "\n", "if", "not", "self", ".", "linear", ":", "\n", "            ", "y", "=", "self", ".", "norm", "(", "y", ")", "\n", "y", "=", "self", ".", "output_linear", "(", "y", ")", "\n", "\n", "", "return", "y", ",", "next_state", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.ss.s4.S4.setup_step": [[182, 184], ["s4.S4.kernel.setup_step"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.s4.S4.setup_step"], ["", "def", "setup_step", "(", "self", ")", ":", "\n", "        ", "self", ".", "kernel", ".", "setup_step", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.ss.s4.S4.step": [[185, 203], ["s4.S4.kernel.step", "einops.rearrange", "s4.S4.activation", "s4.S4.output_linear().squeeze", "s4.S4.output_linear", "u.unsqueeze", "s4.S4.output_linear", "s4.S4.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.sashimi.sashimi.Sashimi.step"], ["", "def", "step", "(", "self", ",", "u", ",", "state", ")", ":", "\n", "        ", "\"\"\" Step one time step as a recurrent model. Intended to be used during validation.\n\n        u: (B H)\n        state: (B H N)\n        Returns: output (B H), state (B H N)\n        \"\"\"", "\n", "assert", "not", "self", ".", "training", "\n", "\n", "y", ",", "next_state", "=", "self", ".", "kernel", ".", "step", "(", "u", ",", "state", ")", "# (B C H)", "\n", "y", "=", "y", "+", "u", ".", "unsqueeze", "(", "-", "2", ")", "*", "self", ".", "D", "\n", "y", "=", "rearrange", "(", "y", ",", "'... c h -> ... (c h)'", ")", "\n", "y", "=", "self", ".", "activation", "(", "y", ")", "\n", "if", "self", ".", "transposed", ":", "\n", "            ", "y", "=", "self", ".", "output_linear", "(", "y", ".", "unsqueeze", "(", "-", "1", ")", ")", ".", "squeeze", "(", "-", "1", ")", "\n", "", "else", ":", "\n", "            ", "y", "=", "self", ".", "output_linear", "(", "y", ")", "\n", "", "return", "y", ",", "next_state", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.ss.s4.S4.default_state": [[204, 206], ["s4.S4.kernel.default_state"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.sashimi.sashimi.Sashimi.default_state"], ["", "def", "default_state", "(", "self", ",", "*", "batch_shape", ",", "device", "=", "None", ")", ":", "\n", "        ", "return", "self", ".", "kernel", ".", "default_state", "(", "*", "batch_shape", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.ss.s4.S4.d_state": [[207, 210], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "d_state", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "h", "*", "self", ".", "n", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.ss.s4.S4.d_output": [[211, 214], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "d_output", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "h", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.ss.s4.S4.state_to_tensor": [[215, 218], ["einops.rearrange"], "methods", ["None"], ["", "@", "property", "\n", "def", "state_to_tensor", "(", "self", ")", ":", "\n", "        ", "return", "lambda", "state", ":", "rearrange", "(", "'... h n -> ... (h n)'", ",", "state", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.ss.s4.test_state": [[220, 273], ["s4.S4", "S4.to", "S4.eval", "S4.modules", "torch.ones().to", "torch.ones().to", "torch.ones().to", "torch.ones().to", "s4.S4.default_state", "torch.cat.clone", "S4.", "print", "print", "torch.cat.clone", "torch.unbind", "torch.unbind", "torch.unbind", "torch.unbind", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "print", "print", "torch.cat.clone", "torch.ones().to.chunk", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "print", "print", "print", "utils.compare_outputs", "print", "utils.compare_outputs", "hasattr", "torch.randn_like", "torch.randn_like", "torch.randn_like", "torch.randn_like", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "s4.S4.step", "torch.cat.append", "S4.", "torch.cat.append", "module.setup_step", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.cat.size", "torch.cat.conj"], "function", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.None.example.eval", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.sashimi.sashimi.Sashimi.default_state", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.sashimi.sashimi.Sashimi.step", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.s4.S4.setup_step"], ["", "", "def", "test_state", "(", "random_init", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "# B = 1", "\n", "# H = 64", "\n", "# N = 64", "\n", "# L = 1024", "\n", "    ", "B", "=", "2", "\n", "H", "=", "3", "\n", "N", "=", "4", "\n", "L", "=", "8", "\n", "s4", "=", "S4", "(", "H", ",", "d_state", "=", "N", ",", "l_max", "=", "L", ",", "**", "kwargs", ")", "\n", "s4", ".", "to", "(", "device", ")", "\n", "s4", ".", "eval", "(", ")", "\n", "for", "module", "in", "s4", ".", "modules", "(", ")", ":", "\n", "        ", "if", "hasattr", "(", "module", ",", "'setup_step'", ")", ":", "module", ".", "setup_step", "(", ")", "\n", "\n", "", "u", "=", "torch", ".", "ones", "(", "B", ",", "H", ",", "L", ")", ".", "to", "(", "device", ")", "\n", "initial_state", "=", "s4", ".", "default_state", "(", "B", ")", "\n", "if", "random_init", ":", "\n", "        ", "if", "initial_state", ".", "size", "(", "-", "1", ")", "==", "N", ":", "\n", "            ", "initial_state", "=", "initial_state", "[", "...", ",", ":", "N", "//", "2", "]", "\n", "", "initial_state", "=", "torch", ".", "randn_like", "(", "initial_state", ")", "\n", "initial_state", "=", "torch", ".", "cat", "(", "[", "initial_state", ",", "initial_state", ".", "conj", "(", ")", "]", ",", "dim", "=", "-", "1", ")", "\n", "\n", "", "state", "=", "initial_state", ".", "clone", "(", ")", "\n", "y", ",", "final_state", "=", "s4", "(", "u", ",", "state", "=", "state", ")", "\n", "print", "(", "\"output:\\n\"", ",", "y", ",", "y", ".", "shape", ")", "\n", "print", "(", "\"final state:\\n\"", ",", "final_state", ",", "final_state", ".", "shape", ")", "\n", "\n", "# Use Stepping", "\n", "state", "=", "initial_state", ".", "clone", "(", ")", "\n", "ys", "=", "[", "]", "\n", "for", "u_", "in", "torch", ".", "unbind", "(", "u", ",", "dim", "=", "-", "1", ")", ":", "\n", "        ", "y_", ",", "state", "=", "s4", ".", "step", "(", "u_", ",", "state", "=", "state", ")", "\n", "ys", ".", "append", "(", "y_", ")", "\n", "", "ys", "=", "torch", ".", "stack", "(", "ys", ",", "dim", "=", "-", "1", ")", "\n", "print", "(", "\"step outputs:\\n\"", ",", "ys", ")", "\n", "print", "(", "\"step final state:\\n\"", ",", "state", ")", "\n", "\n", "# Use Chunking", "\n", "\n", "chunks", "=", "4", "\n", "state", "=", "initial_state", ".", "clone", "(", ")", "\n", "ys", "=", "[", "]", "\n", "for", "u_", "in", "u", ".", "chunk", "(", "chunks", ",", "dim", "=", "-", "1", ")", ":", "\n", "        ", "y_", ",", "state", "=", "s4", "(", "u_", ",", "state", "=", "state", ")", "\n", "ys", ".", "append", "(", "y_", ")", "\n", "", "ys", "=", "torch", ".", "cat", "(", "ys", ",", "dim", "=", "-", "1", ")", "\n", "print", "(", "\"chunk outputs:\\n\"", ",", "ys", ")", "\n", "print", "(", "\"chunk final state:\\n\"", ",", "state", ")", "\n", "print", "(", "\"chunk output error:\"", ")", "\n", "utils", ".", "compare_outputs", "(", "y", ",", "ys", ")", "\n", "print", "(", "\"chunk final state error:\"", ")", "\n", "utils", ".", "compare_outputs", "(", "final_state", ",", "state", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.s4d.SSKernelDiag.__init__": [[104, 130], ["torch.Module.__init__", "log_dt.size", "w.size", "C.expand.expand.expand", "torch.Parameter", "torch.Parameter", "torch.Parameter", "s4d.SSKernelDiag.register", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "s4d.SSKernelDiag.register", "s4d.SSKernelDiag.register", "w.size", "C.expand.expand.size", "w.size", "torch.broadcast_shapes", "torch.broadcast_shapes", "torch.broadcast_shapes", "torch.broadcast_shapes", "torch.broadcast_shapes", "torch.broadcast_shapes", "torch.broadcast_shapes", "torch.broadcast_shapes", "torch.broadcast_shapes", "_c2r", "w.size", "_resolve_conj"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.s4.SSKernelNPLR.register", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.s4.SSKernelNPLR.register", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.s4.SSKernelNPLR.register", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.cauchy._c2r"], ["def", "__init__", "(", "\n", "self", ",", "\n", "w", ",", "C", ",", "log_dt", ",", "\n", "lr", "=", "None", ",", "\n", ")", ":", "\n", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "# Rank of low-rank correction", "\n", "assert", "w", ".", "size", "(", "-", "1", ")", "==", "C", ".", "size", "(", "-", "1", ")", "\n", "self", ".", "H", "=", "log_dt", ".", "size", "(", "-", "1", ")", "\n", "self", ".", "N", "=", "w", ".", "size", "(", "-", "1", ")", "\n", "assert", "self", ".", "H", "%", "w", ".", "size", "(", "0", ")", "==", "0", "\n", "self", ".", "copies", "=", "self", ".", "H", "//", "w", ".", "size", "(", "0", ")", "\n", "\n", "# Broadcast everything to correct shapes", "\n", "C", "=", "C", ".", "expand", "(", "torch", ".", "broadcast_shapes", "(", "C", ".", "shape", ",", "(", "1", ",", "self", ".", "H", ",", "self", ".", "N", ")", ")", ")", "# (H, C, N)", "\n", "\n", "# Register parameters", "\n", "self", ".", "C", "=", "nn", ".", "Parameter", "(", "_c2r", "(", "_resolve_conj", "(", "C", ")", ")", ")", "\n", "self", ".", "register", "(", "\"log_dt\"", ",", "log_dt", ",", "True", ",", "lr", ",", "0.0", ")", "\n", "\n", "log_w_real", "=", "torch", ".", "log", "(", "-", "w", ".", "real", "+", "1e-4", ")", "\n", "w_imag", "=", "w", ".", "imag", "\n", "self", ".", "register", "(", "\"log_w_real\"", ",", "log_w_real", ",", "True", ",", "lr", ",", "0.0", ")", "\n", "self", ".", "register", "(", "\"w_imag\"", ",", "w_imag", ",", "True", ",", "lr", ",", "0.0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.s4d.SSKernelDiag._w": [[132, 139], ["einops.repeat", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp"], "methods", ["None"], ["", "def", "_w", "(", "self", ")", ":", "\n", "# Get the internal w (diagonal) parameter", "\n", "        ", "w_real", "=", "-", "torch", ".", "exp", "(", "self", ".", "log_w_real", ")", "\n", "w_imag", "=", "self", ".", "w_imag", "\n", "w", "=", "w_real", "+", "1j", "*", "w_imag", "\n", "w", "=", "repeat", "(", "w", ",", "'t n -> (v t) n'", ",", "v", "=", "self", ".", "copies", ")", "# (H N)", "\n", "return", "w", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.s4d.SSKernelDiag.forward": [[140, 159], ["torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "_r2c", "s4d.SSKernelDiag._w", "contract", "torch.exp.unsqueeze", "torch.exp.unsqueeze", "torch.exp.unsqueeze", "dtA.unsqueeze", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.cauchy._r2c", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.s4.SSKernelNPLR._w"], ["", "def", "forward", "(", "self", ",", "L", ")", ":", "\n", "        ", "\"\"\"\n        returns: (..., c, L) where c is number of channels (default 1)\n        \"\"\"", "\n", "\n", "dt", "=", "torch", ".", "exp", "(", "self", ".", "log_dt", ")", "# (H)", "\n", "C", "=", "_r2c", "(", "self", ".", "C", ")", "# (C H N)", "\n", "w", "=", "self", ".", "_w", "(", ")", "# (H N)", "\n", "\n", "# Incorporate dt into A", "\n", "dtA", "=", "w", "*", "dt", ".", "unsqueeze", "(", "-", "1", ")", "# (H N)", "\n", "\n", "# Power up", "\n", "K", "=", "dtA", ".", "unsqueeze", "(", "-", "1", ")", "*", "torch", ".", "arange", "(", "L", ",", "device", "=", "w", ".", "device", ")", "# (H N L)", "\n", "C", "=", "C", "*", "(", "torch", ".", "exp", "(", "dtA", ")", "-", "1.", ")", "/", "w", "\n", "K", "=", "contract", "(", "'chn, hnl -> chl'", ",", "C", ",", "torch", ".", "exp", "(", "K", ")", ")", "\n", "K", "=", "2", "*", "K", ".", "real", "\n", "\n", "return", "K", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.s4d.SSKernelDiag.setup_step": [[160, 170], ["torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "_r2c", "s4d.SSKernelDiag._w", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "s4d.SSKernelDiag.dC.new_ones", "torch.exp.unsqueeze", "torch.exp.unsqueeze", "torch.exp.unsqueeze", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.cauchy._r2c", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.s4.SSKernelNPLR._w"], ["", "def", "setup_step", "(", "self", ")", ":", "\n", "        ", "dt", "=", "torch", ".", "exp", "(", "self", ".", "log_dt", ")", "# (H)", "\n", "C", "=", "_r2c", "(", "self", ".", "C", ")", "# (C H N)", "\n", "w", "=", "self", ".", "_w", "(", ")", "# (H N)", "\n", "\n", "# Incorporate dt into A", "\n", "dtA", "=", "w", "*", "dt", ".", "unsqueeze", "(", "-", "1", ")", "# (H N)", "\n", "self", ".", "dA", "=", "torch", ".", "exp", "(", "dtA", ")", "# (H N)", "\n", "self", ".", "dC", "=", "C", "*", "(", "torch", ".", "exp", "(", "dtA", ")", "-", "1.", ")", "/", "w", "# (C H N)", "\n", "self", ".", "dB", "=", "self", ".", "dC", ".", "new_ones", "(", "self", ".", "H", ",", "self", ".", "N", ")", "# (H N)", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.s4d.SSKernelDiag.default_state": [[171, 175], ["_r2c", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.cauchy._r2c"], ["", "def", "default_state", "(", "self", ",", "*", "batch_shape", ")", ":", "\n", "        ", "C", "=", "_r2c", "(", "self", ".", "C", ")", "\n", "state", "=", "torch", ".", "zeros", "(", "*", "batch_shape", ",", "self", ".", "H", ",", "self", ".", "N", ",", "dtype", "=", "C", ".", "dtype", ",", "device", "=", "C", ".", "device", ")", "\n", "return", "state", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.s4d.SSKernelDiag.step": [[176, 181], ["contract", "contract", "contract"], "methods", ["None"], ["", "def", "step", "(", "self", ",", "u", ",", "state", ")", ":", "\n", "        ", "next_state", "=", "contract", "(", "\"h n, b h n -> b h n\"", ",", "self", ".", "dA", ",", "state", ")", "+", "contract", "(", "\"h n, b h -> b h n\"", ",", "self", ".", "dB", ",", "u", ")", "\n", "y", "=", "contract", "(", "\"c h n, b h n -> b c h\"", ",", "self", ".", "dC", ",", "next_state", ")", "\n", "return", "2", "*", "y", ".", "real", ",", "next_state", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.s4d.SSKernelDiag.register": [[183, 198], ["s4d.SSKernelDiag.register_parameter", "s4d.SSKernelDiag.register_buffer", "len", "setattr", "torch.Parameter", "torch.Parameter", "torch.Parameter", "getattr"], "methods", ["None"], ["", "def", "register", "(", "self", ",", "name", ",", "tensor", ",", "trainable", "=", "False", ",", "lr", "=", "None", ",", "wd", "=", "None", ")", ":", "\n", "        ", "\"\"\"Utility method: register a tensor as a buffer or trainable parameter\"\"\"", "\n", "\n", "if", "trainable", ":", "\n", "            ", "self", ".", "register_parameter", "(", "name", ",", "nn", ".", "Parameter", "(", "tensor", ")", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "register_buffer", "(", "name", ",", "tensor", ")", "\n", "\n", "", "optim", "=", "{", "}", "\n", "if", "trainable", "and", "lr", "is", "not", "None", ":", "\n", "            ", "optim", "[", "\"lr\"", "]", "=", "lr", "\n", "", "if", "trainable", "and", "wd", "is", "not", "None", ":", "\n", "            ", "optim", "[", "\"weight_decay\"", "]", "=", "wd", "\n", "", "if", "len", "(", "optim", ")", ">", "0", ":", "\n", "            ", "setattr", "(", "getattr", "(", "self", ",", "name", ")", ",", "\"_optim\"", ",", "optim", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.s4d.S4DKernel.__init__": [[203, 245], ["torch.Module.__init__", "s4d.random_dplr", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "einops.repeat().clone().contiguous", "einops.repeat().clone().contiguous", "s4d.SSKernelDiag", "math.log", "einops.repeat", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "einops.repeat().clone", "einops.repeat().clone", "math.log", "math.log", "einops.repeat", "einops.repeat", "einops.repeat().clone().contiguous.size", "einops.repeat().clone().contiguous.size"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.s4d.random_dplr"], ["def", "__init__", "(", "\n", "self", ",", "\n", "H", ",", "\n", "N", "=", "64", ",", "\n", "scaling", "=", "\"inverse\"", ",", "\n", "channels", "=", "1", ",", "# 1-dim to C-dim map; can think of C as having separate \"heads\"", "\n", "dt_min", "=", "0.001", ",", "\n", "dt_max", "=", "0.1", ",", "\n", "lr", "=", "None", ",", "# Hook to set LR of SSM parameters differently", "\n", "n_ssm", "=", "1", ",", "# Copies of the ODE parameters A and B. Must divide H", "\n", "**", "kernel_args", ",", "\n", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "N", "=", "N", "\n", "self", ".", "H", "=", "H", "\n", "dtype", "=", "torch", ".", "float", "\n", "cdtype", "=", "torch", ".", "cfloat", "\n", "self", ".", "channels", "=", "channels", "\n", "self", ".", "n_ssm", "=", "n_ssm", "\n", "\n", "# Generate dt", "\n", "log_dt", "=", "torch", ".", "rand", "(", "self", ".", "H", ",", "dtype", "=", "dtype", ")", "*", "(", "\n", "math", ".", "log", "(", "dt_max", ")", "-", "math", ".", "log", "(", "dt_min", ")", "\n", ")", "+", "math", ".", "log", "(", "dt_min", ")", "\n", "\n", "# Compute the preprocessed representation", "\n", "# Generate low rank correction p for the measure", "\n", "w", ",", "B", "=", "random_dplr", "(", "self", ".", "N", ",", "H", "=", "n_ssm", ",", "scaling", "=", "scaling", ")", "\n", "\n", "C", "=", "torch", ".", "randn", "(", "channels", ",", "self", ".", "H", ",", "self", ".", "N", "//", "2", ",", "dtype", "=", "cdtype", ")", "\n", "\n", "# Broadcast tensors to n_ssm copies", "\n", "# These will be the parameters, so make sure tensors are materialized and contiguous", "\n", "B", "=", "repeat", "(", "B", ",", "'t n -> (v t) n'", ",", "v", "=", "self", ".", "n_ssm", "//", "B", ".", "size", "(", "-", "2", ")", ")", ".", "clone", "(", ")", ".", "contiguous", "(", ")", "\n", "w", "=", "repeat", "(", "w", ",", "'t n -> (v t) n'", ",", "v", "=", "self", ".", "n_ssm", "//", "w", ".", "size", "(", "-", "2", ")", ")", ".", "clone", "(", ")", ".", "contiguous", "(", ")", "\n", "\n", "# Combine B and C using structure of diagonal SSM", "\n", "C", "=", "C", "*", "repeat", "(", "B", ",", "'t n -> (v t) n'", ",", "v", "=", "H", "//", "self", ".", "n_ssm", ")", "\n", "self", ".", "kernel", "=", "SSKernelDiag", "(", "\n", "w", ",", "C", ",", "log_dt", ",", "\n", "lr", "=", "lr", ",", "\n", "**", "kernel_args", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.s4d.S4DKernel.forward": [[247, 250], ["s4d.S4DKernel.kernel", "s4d.S4DKernel.float"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "L", "=", "None", ")", ":", "\n", "        ", "k", "=", "self", ".", "kernel", "(", "L", "=", "L", ")", "\n", "return", "k", ".", "float", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.s4d.S4DKernel.setup_step": [[251, 253], ["s4d.S4DKernel.kernel.setup_step"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.s4.S4.setup_step"], ["", "def", "setup_step", "(", "self", ")", ":", "\n", "        ", "self", ".", "kernel", ".", "setup_step", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.s4d.S4DKernel.step": [[254, 257], ["s4d.S4DKernel.kernel.step", "u.float"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.sashimi.sashimi.Sashimi.step"], ["", "def", "step", "(", "self", ",", "u", ",", "state", ",", "**", "kwargs", ")", ":", "\n", "        ", "u", ",", "state", "=", "self", ".", "kernel", ".", "step", "(", "u", ",", "state", ",", "**", "kwargs", ")", "\n", "return", "u", ".", "float", "(", ")", ",", "state", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.s4d.S4DKernel.default_state": [[258, 260], ["s4d.S4DKernel.kernel.default_state"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.sashimi.sashimi.Sashimi.default_state"], ["", "def", "default_state", "(", "self", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "return", "self", ".", "kernel", ".", "default_state", "(", "*", "args", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.s4d.S4D.__init__": [[264, 316], ["torch.Module.__init__", "torch.Parameter", "torch.Parameter", "torch.Parameter", "s4d.S4DKernel", "s4d.Activation", "s4d.LinearActivation", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "dropout_fn", "torch.Identity", "torch.Identity", "torch.Identity"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.s4.Activation", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.s4.LinearActivation"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "d_model", ",", "\n", "d_state", "=", "64", ",", "\n", "channels", "=", "1", ",", "# maps 1-dim to C-dim", "\n", "bidirectional", "=", "False", ",", "\n", "# Arguments for FF", "\n", "activation", "=", "'gelu'", ",", "# activation in between SS and FF", "\n", "postact", "=", "None", ",", "# activation after FF", "\n", "dropout", "=", "0.0", ",", "\n", "transposed", "=", "True", ",", "# axis ordering (B, L, D) or (B, D, L)", "\n", "# SSM Kernel arguments", "\n", "**", "kernel_args", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        d_state: the dimension of the state, also denoted by N\n        channels: can be interpreted as a number of \"heads\"\n        bidirectional: bidirectional\n        dropout: standard dropout argument\n        transposed: choose backbone axis ordering of (B, L, H) or (B, H, L) [B=batch size, L=sequence length, H=hidden dimension]\n\n        Other options are all experimental and should not need to be configured\n        \"\"\"", "\n", "\n", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "h", "=", "d_model", "\n", "self", ".", "n", "=", "d_state", "\n", "self", ".", "bidirectional", "=", "bidirectional", "\n", "self", ".", "channels", "=", "channels", "\n", "self", ".", "transposed", "=", "transposed", "\n", "\n", "self", ".", "D", "=", "nn", ".", "Parameter", "(", "torch", ".", "randn", "(", "channels", ",", "self", ".", "h", ")", ")", "\n", "\n", "if", "self", ".", "bidirectional", ":", "\n", "            ", "channels", "*=", "2", "\n", "\n", "# SSM Kernel", "\n", "", "self", ".", "kernel", "=", "S4DKernel", "(", "self", ".", "h", ",", "N", "=", "self", ".", "n", ",", "channels", "=", "channels", ",", "**", "kernel_args", ")", "\n", "\n", "# Pointwise", "\n", "self", ".", "activation", "=", "Activation", "(", "activation", ")", "\n", "dropout_fn", "=", "nn", ".", "Dropout2d", "if", "self", ".", "transposed", "else", "nn", ".", "Dropout", "\n", "self", ".", "dropout", "=", "dropout_fn", "(", "dropout", ")", "if", "dropout", ">", "0.0", "else", "nn", ".", "Identity", "(", ")", "\n", "\n", "# position-wise output transform to mix features", "\n", "self", ".", "output_linear", "=", "LinearActivation", "(", "\n", "self", ".", "h", "*", "self", ".", "channels", ",", "\n", "self", ".", "h", ",", "\n", "transposed", "=", "self", ".", "transposed", ",", "\n", "activation", "=", "postact", ",", "\n", "activate", "=", "True", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.s4d.S4D.forward": [[319, 357], ["u.transpose.transpose.size", "s4d.S4D.kernel", "torch.fft.rfft", "torch.fft.rfft", "torch.fft.rfft", "torch.fft.rfft", "torch.fft.rfft", "torch.fft.rfft", "torch.fft.rfft", "torch.fft.rfft", "torch.fft.rfft", "torch.fft.rfft", "torch.fft.rfft", "torch.fft.rfft", "torch.fft.rfft", "torch.fft.rfft", "torch.fft.rfft", "torch.fft.rfft", "torch.fft.rfft", "torch.fft.rfft", "contract", "einops.rearrange", "s4d.S4D.dropout", "s4d.S4D.output_linear", "u.transpose.transpose.transpose", "einops.rearrange", "torch.fft.irfft", "torch.fft.irfft", "torch.fft.irfft", "torch.fft.irfft", "torch.fft.irfft", "torch.fft.irfft", "torch.fft.irfft", "torch.fft.irfft", "torch.fft.irfft", "contract", "s4d.S4D.activation", "y.transpose.transpose.transpose", "torch.pad", "torch.pad", "torch.pad", "torch.pad", "torch.pad", "torch.pad", "k1.flip"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.sc.pad", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.sc.pad", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.sc.pad", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.sc.pad", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.sc.pad", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.sc.pad"], ["", "def", "forward", "(", "self", ",", "u", ",", "**", "kwargs", ")", ":", "# absorbs return_output and transformer src mask", "\n", "        ", "\"\"\"\n        u: (B H L) if self.transposed else (B L H)\n        state: (H N) never needed unless you know what you're doing\n\n        Returns: same shape as u\n        \"\"\"", "\n", "if", "not", "self", ".", "transposed", ":", "u", "=", "u", ".", "transpose", "(", "-", "1", ",", "-", "2", ")", "\n", "L", "=", "u", ".", "size", "(", "-", "1", ")", "\n", "\n", "# Compute SS Kernel", "\n", "k", "=", "self", ".", "kernel", "(", "L", "=", "L", ")", "# (C H L) (B C H L)", "\n", "\n", "# Convolution", "\n", "if", "self", ".", "bidirectional", ":", "\n", "            ", "k0", ",", "k1", "=", "rearrange", "(", "k", ",", "'(s c) h l -> s c h l'", ",", "s", "=", "2", ")", "\n", "k", "=", "F", ".", "pad", "(", "k0", ",", "(", "0", ",", "L", ")", ")", "+", "F", ".", "pad", "(", "k1", ".", "flip", "(", "-", "1", ")", ",", "(", "L", ",", "0", ")", ")", "\n", "", "k_f", "=", "torch", ".", "fft", ".", "rfft", "(", "k", ",", "n", "=", "2", "*", "L", ")", "# (C H L)", "\n", "u_f", "=", "torch", ".", "fft", ".", "rfft", "(", "u", ",", "n", "=", "2", "*", "L", ")", "# (B H L)", "\n", "y_f", "=", "contract", "(", "'bhl,chl->bchl'", ",", "u_f", ",", "k_f", ")", "# k_f.unsqueeze(-4) * u_f.unsqueeze(-3) # (B C H L)", "\n", "y", "=", "torch", ".", "fft", ".", "irfft", "(", "y_f", ",", "n", "=", "2", "*", "L", ")", "[", "...", ",", ":", "L", "]", "# (B C H L)", "\n", "\n", "\n", "# Compute D term in state space equation - essentially a skip connection", "\n", "y", "=", "y", "+", "contract", "(", "'bhl,ch->bchl'", ",", "u", ",", "self", ".", "D", ")", "# u.unsqueeze(-3) * self.D.unsqueeze(-1)", "\n", "\n", "# Reshape to flatten channels", "\n", "y", "=", "rearrange", "(", "y", ",", "'... c h l -> ... (c h) l'", ")", "\n", "\n", "y", "=", "self", ".", "dropout", "(", "self", ".", "activation", "(", "y", ")", ")", "\n", "\n", "if", "not", "self", ".", "transposed", ":", "y", "=", "y", ".", "transpose", "(", "-", "1", ",", "-", "2", ")", "\n", "\n", "y", "=", "self", ".", "output_linear", "(", "y", ")", "\n", "\n", "return", "y", ",", "None", "# Return a None to satisfy this repo's interface, but this can be modified", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.s4d.S4D.setup_step": [[358, 360], ["s4d.S4D.kernel.setup_step"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.s4.S4.setup_step"], ["", "def", "setup_step", "(", "self", ")", ":", "\n", "        ", "self", ".", "kernel", ".", "setup_step", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.s4d.S4D.step": [[361, 379], ["s4d.S4D.kernel.step", "einops.rearrange", "s4d.S4D.activation", "s4d.S4D.output_linear().squeeze", "s4d.S4D.output_linear", "u.unsqueeze", "s4d.S4D.output_linear", "s4d.S4D.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.sashimi.sashimi.Sashimi.step"], ["", "def", "step", "(", "self", ",", "u", ",", "state", ")", ":", "\n", "        ", "\"\"\" Step one time step as a recurrent model. Intended to be used during validation.\n\n        u: (B H)\n        state: (B H N)\n        Returns: output (B H), state (B H N)\n        \"\"\"", "\n", "assert", "not", "self", ".", "training", "\n", "\n", "y", ",", "next_state", "=", "self", ".", "kernel", ".", "step", "(", "u", ",", "state", ")", "# (B C H)", "\n", "y", "=", "y", "+", "u", ".", "unsqueeze", "(", "-", "2", ")", "*", "self", ".", "D", "\n", "y", "=", "rearrange", "(", "y", ",", "'... c h -> ... (c h)'", ")", "\n", "y", "=", "self", ".", "activation", "(", "y", ")", "\n", "if", "self", ".", "transposed", ":", "\n", "            ", "y", "=", "self", ".", "output_linear", "(", "y", ".", "unsqueeze", "(", "-", "1", ")", ")", ".", "squeeze", "(", "-", "1", ")", "\n", "", "else", ":", "\n", "            ", "y", "=", "self", ".", "output_linear", "(", "y", ")", "\n", "", "return", "y", ",", "next_state", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.s4d.S4D.default_state": [[380, 382], ["s4d.S4D.kernel.default_state"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.sashimi.sashimi.Sashimi.default_state"], ["", "def", "default_state", "(", "self", ",", "*", "batch_shape", ",", "device", "=", "None", ")", ":", "\n", "        ", "return", "self", ".", "kernel", ".", "default_state", "(", "*", "batch_shape", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.s4d.S4D.d_state": [[383, 386], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "d_state", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "h", "*", "self", ".", "n", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.s4d.S4D.d_output": [[387, 390], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "d_output", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "h", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.s4d.S4D.state_to_tensor": [[391, 394], ["einops.rearrange"], "methods", ["None"], ["", "@", "property", "\n", "def", "state_to_tensor", "(", "self", ")", ":", "\n", "        ", "return", "lambda", "state", ":", "rearrange", "(", "'... h n -> ... (h n)'", ",", "state", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.s4d.Activation": [[31, 48], ["torch.Identity", "torch.Tanh", "torch.ReLU", "torch.GELU", "torch.SiLU", "torch.GLU", "torch.Sigmoid", "NotImplementedError"], "function", ["None"], ["def", "Activation", "(", "activation", "=", "None", ",", "dim", "=", "-", "1", ")", ":", "\n", "    ", "if", "activation", "in", "[", "None", ",", "'id'", ",", "'identity'", ",", "'linear'", "]", ":", "\n", "        ", "return", "nn", ".", "Identity", "(", ")", "\n", "", "elif", "activation", "==", "'tanh'", ":", "\n", "        ", "return", "nn", ".", "Tanh", "(", ")", "\n", "", "elif", "activation", "==", "'relu'", ":", "\n", "        ", "return", "nn", ".", "ReLU", "(", ")", "\n", "", "elif", "activation", "==", "'gelu'", ":", "\n", "        ", "return", "nn", ".", "GELU", "(", ")", "\n", "", "elif", "activation", "in", "[", "'swish'", ",", "'silu'", "]", ":", "\n", "        ", "return", "nn", ".", "SiLU", "(", ")", "\n", "", "elif", "activation", "==", "'glu'", ":", "\n", "        ", "return", "nn", ".", "GLU", "(", "dim", "=", "dim", ")", "\n", "", "elif", "activation", "==", "'sigmoid'", ":", "\n", "        ", "return", "nn", ".", "Sigmoid", "(", ")", "\n", "", "else", ":", "\n", "        ", "raise", "NotImplementedError", "(", "\"hidden activation '{}' is not implemented\"", ".", "format", "(", "activation", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.s4d.LinearActivation": [[49, 67], ["linear_cls", "functools.partial", "s4d.Activation", "torch.Sequential"], "function", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.s4.Activation"], ["", "", "def", "LinearActivation", "(", "\n", "d_input", ",", "d_output", ",", "bias", "=", "True", ",", "\n", "transposed", "=", "False", ",", "\n", "activation", "=", "None", ",", "\n", "activate", "=", "False", ",", "# Apply activation as part of this module", "\n", "**", "kwargs", ",", "\n", ")", ":", "\n", "    ", "\"\"\" Returns a linear nn.Module with control over axes order, initialization, and activation \"\"\"", "\n", "\n", "# Construct core module", "\n", "linear_cls", "=", "partial", "(", "nn", ".", "Conv1d", ",", "kernel_size", "=", "1", ")", "if", "transposed", "else", "nn", ".", "Linear", "\n", "if", "activation", "==", "'glu'", ":", "d_output", "*=", "2", "\n", "linear", "=", "linear_cls", "(", "d_input", ",", "d_output", ",", "bias", "=", "bias", ",", "**", "kwargs", ")", "\n", "\n", "if", "activate", "and", "activation", "is", "not", "None", ":", "\n", "        ", "activation", "=", "Activation", "(", "activation", ",", "dim", "=", "-", "2", "if", "transposed", "else", "-", "1", ")", "\n", "linear", "=", "nn", ".", "Sequential", "(", "linear", ",", "activation", ")", "\n", "", "return", "linear", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.s4d.random_dplr": [[71, 97], ["torch.tensor", "torch.tensor", "torch.tensor", "einops.repeat", "torch.randn", "torch.randn", "torch.randn", "torch.ones", "torch.ones", "torch.ones", "torch.arange", "torch.arange", "torch.arange", "torch.randn", "torch.randn", "torch.randn", "torch.sum", "torch.sum", "torch.sum", "torch.abs", "torch.abs", "torch.abs"], "function", ["None"], ["def", "random_dplr", "(", "N", ",", "H", "=", "1", ",", "scaling", "=", "'inverse'", ",", "real_scale", "=", "1.0", ",", "imag_scale", "=", "1.0", ")", ":", "\n", "    ", "dtype", "=", "torch", ".", "cfloat", "\n", "\n", "pi", "=", "torch", ".", "tensor", "(", "np", ".", "pi", ")", "\n", "real_part", "=", ".5", "*", "torch", ".", "ones", "(", "H", ",", "N", "//", "2", ")", "\n", "imag_part", "=", "repeat", "(", "torch", ".", "arange", "(", "N", "//", "2", ")", ",", "'n -> h n'", ",", "h", "=", "H", ")", "\n", "\n", "real_part", "=", "real_scale", "*", "real_part", "\n", "if", "scaling", "==", "'random'", ":", "\n", "        ", "imag_part", "=", "torch", ".", "randn", "(", "H", ",", "N", "//", "2", ")", "\n", "", "elif", "scaling", "==", "'linear'", ":", "\n", "        ", "imag_part", "=", "pi", "*", "imag_part", "\n", "", "elif", "scaling", "==", "'inverse'", ":", "# Based on asymptotics of the default HiPPO matrix", "\n", "        ", "imag_part", "=", "1", "/", "pi", "*", "N", "*", "(", "N", "/", "(", "1", "+", "2", "*", "imag_part", ")", "-", "1", ")", "\n", "", "else", ":", "raise", "NotImplementedError", "\n", "imag_part", "=", "imag_scale", "*", "imag_part", "\n", "w", "=", "-", "real_part", "+", "1j", "*", "imag_part", "\n", "\n", "\n", "B", "=", "torch", ".", "randn", "(", "H", ",", "N", "//", "2", ",", "dtype", "=", "dtype", ")", "\n", "\n", "norm", "=", "-", "B", "/", "w", "# (H, N) # Result if you integrate the kernel with constant 1 function", "\n", "zeta", "=", "2", "*", "torch", ".", "sum", "(", "torch", ".", "abs", "(", "norm", ")", "**", "2", ",", "dim", "=", "-", "1", ",", "keepdim", "=", "True", ")", "# Variance with a random C vector", "\n", "B", "=", "B", "/", "zeta", "**", ".5", "\n", "\n", "return", "w", ",", "B", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.lssl.AdaptiveTransition.__init__": [[75, 93], ["torch.Module.__init__", "lssl.hippo", "torch.as_tensor", "torch.as_tensor", "torch.as_tensor", "torch.as_tensor", "torch.as_tensor", "torch.as_tensor", "torch.as_tensor", "torch.as_tensor", "torch.as_tensor", "lssl.AdaptiveTransition.register_buffer", "lssl.AdaptiveTransition.register_buffer", "torch.eye", "torch.eye", "torch.eye", "torch.eye", "torch.eye", "torch.eye", "torch.eye", "torch.eye", "torch.eye", "lssl.AdaptiveTransition.register_buffer", "torch.as_tensor", "torch.as_tensor", "torch.as_tensor", "torch.as_tensor", "torch.as_tensor", "torch.as_tensor", "torch.as_tensor", "torch.as_tensor", "torch.as_tensor"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.lssl.hippo"], ["self", ".", "d", "=", "d", "\n", "self", ".", "N", "=", "d_model", "if", "d_model", ">", "0", "else", "d", "\n", "self", ".", "dt", "=", "DictConfig", "(", "{", "\n", "'min'", ":", "0.001", ",", "\n", "'max'", ":", "0.1", ",", "\n", "'learn'", ":", "False", ",", "\n", "'lr'", ":", "0.001", ",", "\n", "'init'", ":", "'random'", ",", "\n", "}", ")", "\n", "if", "dt", "is", "not", "None", ":", "self", ".", "dt", ".", "update", "(", "dt", ")", "\n", "self", ".", "ff", "=", "ff", "\n", "self", ".", "bias", "=", "bias", "\n", "\n", "\n", "# Construct transition", "\n", "self", ".", "learn", "=", "learn", "\n", "if", "self", ".", "learn", "==", "0", ":", "\n", "            ", "if", "measure", "==", "'identity'", ":", "# for testing", "\n", "                ", "A", ",", "B", "=", "torch", ".", "eye", "(", "self", ".", "N", ")", ",", "torch", ".", "ones", "(", "self", ".", "N", ")", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.lssl.AdaptiveTransition.forward_mult": [[95, 105], ["None"], "methods", ["None"], ["", "elif", "measure", "==", "'random'", ":", "\n", "                ", "A", "=", "torch", ".", "randn", "(", "self", ".", "N", ",", "self", ".", "N", ")", "/", "self", ".", "N", "# E[AA^T] = (1/N)I -- empirically I nans out", "\n", "B", "=", "torch", ".", "ones", "(", "self", ".", "N", ")", "# based on HiPPO matrices; worth trying random, haven't tried", "\n", "self", ".", "transition", "=", "transition", ".", "ManualAdaptiveTransition", "(", "self", ".", "N", ",", "A", ",", "B", ")", "\n", "", "elif", "measure", "==", "'legt'", ":", "\n", "# self.transition = transition.LegTAdaptiveTransition(self.N)", "\n", "                ", "self", ".", "transition", "=", "transition", ".", "LegTTriDInverseAdaptiveTransition", "(", "self", ".", "N", ",", "**", "measure_args", ")", "\n", "", "elif", "measure", "==", "'cheb'", ":", "\n", "                ", "self", ".", "transition", "=", "transition", ".", "ChebITriDInverseAdaptiveTransition", "(", "self", ".", "N", ",", "**", "measure_args", ")", "\n", "", "elif", "measure", "==", "'chebii'", ":", "\n", "                ", "self", ".", "transition", "=", "transition", ".", "ChebIITriDInverseAdaptiveTransition", "(", "self", ".", "N", ",", "**", "measure_args", ")", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.lssl.AdaptiveTransition.inverse_mult": [[106, 109], ["None"], "methods", ["None"], ["", "elif", "measure", "==", "'lagt'", ":", "\n", "                ", "self", ".", "transition", "=", "transition", ".", "LagTCumsumAdaptiveTransition", "(", "self", ".", "N", ",", "**", "measure_args", ")", "\n", "", "elif", "measure", "==", "'glagt'", ":", "\n", "                ", "self", ".", "transition", "=", "transition", ".", "GLagTToeplitzAdaptiveTransition", "(", "self", ".", "N", ",", "**", "measure_args", ")", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.lssl.AdaptiveTransition.forward_diff": [[110, 121], ["lssl.AdaptiveTransition.forward_mult", "v.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.lssl.LegTTransition.forward_mult"], ["", "elif", "measure", "==", "'legs'", ":", "\n", "                ", "self", ".", "transition", "=", "transition", ".", "LegSTriDInverseAdaptiveTransition", "(", "self", ".", "N", ",", "**", "measure_args", ")", "\n", "", "elif", "measure", "==", "'jac'", ":", "\n", "                ", "self", ".", "transition", "=", "transition", ".", "JacTriDInverseAdaptiveTransition", "(", "self", ".", "N", ",", "**", "measure_args", ")", "\n", "", "else", ":", "\n", "                ", "raise", "NotImplementedError", "\n", "", "", "elif", "self", ".", "learn", "==", "1", "or", "self", ".", "learn", "==", "2", ":", "\n", "            ", "kwargs", "=", "{", "'trainable'", ":", "True", ",", "'lr'", ":", "lr", "}", "\n", "kwargs", ".", "update", "(", "measure_args", ")", "\n", "if", "self", ".", "learn", "==", "2", ":", "\n", "                ", "kwargs", "[", "'batch'", "]", "=", "(", "self", ".", "d", ",", ")", "\n", "", "if", "measure", "==", "'random'", ":", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.lssl.AdaptiveTransition.backward_diff": [[122, 133], ["lssl.AdaptiveTransition.inverse_mult", "v.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.lssl.LegTTransition.inverse_mult"], ["                ", "A", "=", "torch", ".", "randn", "(", "self", ".", "N", ",", "self", ".", "N", ")", "/", "self", ".", "N", "# E[AA^T] = (1/N)I . empirically I doesn't work, dunno why", "\n", "B", "=", "torch", ".", "ones", "(", "self", ".", "N", ")", "# based on HiPPO matrices; worth trying random, haven't tried", "\n", "self", ".", "transition", "=", "transition", ".", "ManualAdaptiveTransition", "(", "self", ".", "N", ",", "A", ",", "B", ",", "**", "kwargs", ")", "\n", "", "elif", "measure", "==", "'legt'", ":", "\n", "                ", "self", ".", "transition", "=", "transition", ".", "LegTTriDInverseAdaptiveTransition", "(", "self", ".", "N", ",", "**", "kwargs", ")", "\n", "", "elif", "measure", "==", "'lagt'", ":", "\n", "                ", "self", ".", "transition", "=", "transition", ".", "LagTTriDInverseAdaptiveTransition", "(", "self", ".", "N", ",", "**", "kwargs", ")", "\n", "", "elif", "measure", "==", "'legs'", ":", "\n", "                ", "self", ".", "transition", "=", "transition", ".", "LegSTriDInverseAdaptiveTransition", "(", "self", ".", "N", ",", "**", "kwargs", ")", "\n", "", "elif", "measure", "==", "'cheb'", ":", "\n", "                ", "self", ".", "transition", "=", "transition", ".", "ChebITriDInverseAdaptiveTransition", "(", "self", ".", "N", ",", "**", "kwargs", ")", "\n", "", "elif", "measure", "==", "'chebii'", ":", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.lssl.AdaptiveTransition.bilinear": [[134, 149], ["lssl.AdaptiveTransition.forward_mult", "lssl.AdaptiveTransition.inverse_mult", "v.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.lssl.LegTTransition.forward_mult", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.lssl.LegTTransition.inverse_mult"], ["                ", "self", ".", "transition", "=", "transition", ".", "ChebIITriDInverseAdaptiveTransition", "(", "self", ".", "N", ",", "**", "kwargs", ")", "\n", "", "elif", "measure", "==", "'toep'", ":", "\n", "                ", "self", ".", "transition", "=", "transition", ".", "LagTToeplitzAdaptiveTransition", "(", "self", ".", "N", ",", "**", "kwargs", ")", "\n", "", "else", ":", "raise", "NotImplementedError", "\n", "", "elif", "self", ".", "learn", "==", "3", ":", "# for debugging", "\n", "            ", "A", ",", "B", "=", "hippo", ".", "transition", "(", "measure", ",", "self", ".", "N", ")", "\n", "B", "=", "B", "[", ":", ",", "0", "]", "\n", "self", ".", "transition", "=", "transition", ".", "ManualAdaptiveTransition", "(", "self", ".", "N", ",", "A", ",", "B", ",", "trainable", "=", "True", ",", "lr", "=", "lr", ")", "\n", "", "else", ":", "\n", "            ", "raise", "NotImplementedError", "\n", "\n", "\n", "", "self", ".", "m", "=", "channels", "\n", "\n", "if", "init", "==", "'normal'", ":", "\n", "            ", "self", ".", "C", "=", "nn", ".", "Parameter", "(", "torch", ".", "randn", "(", "self", ".", "d", ",", "self", ".", "m", ",", "self", ".", "N", ")", ")", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.lssl.AdaptiveTransition.gbt_A": [[150, 162], ["len", "lssl.AdaptiveTransition.I.view", "lssl.AdaptiveTransition.bilinear", "einops.rearrange", "dt.new_zeros"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.lssl.AdaptiveTransition.bilinear"], ["self", ".", "D", "=", "nn", ".", "Parameter", "(", "torch", ".", "randn", "(", "self", ".", "d", ",", "self", ".", "m", ")", ")", "\n", "", "elif", "init", "==", "'constant'", ":", "\n", "            ", "self", ".", "C", "=", "nn", ".", "Parameter", "(", "torch", ".", "ones", "(", "self", ".", "d", ",", "self", ".", "m", ",", "self", ".", "N", ")", ")", "\n", "self", ".", "D", "=", "nn", ".", "Parameter", "(", "torch", ".", "ones", "(", "self", ".", "d", ",", "self", ".", "m", ")", ")", "\n", "", "elif", "init", "==", "'uniform'", ":", "\n", "            ", "self", ".", "C", "=", "nn", ".", "Parameter", "(", "1.732", "*", "torch", ".", "rand", "(", "self", ".", "d", ",", "self", ".", "m", ",", "self", ".", "N", ")", ")", "\n", "self", ".", "D", "=", "nn", ".", "Parameter", "(", "torch", ".", "randn", "(", "self", ".", "d", ",", "self", ".", "m", ")", ")", "\n", "", "else", ":", "raise", "NotImplementedError", "\n", "if", "self", ".", "bias", ":", "\n", "            ", "self", ".", "E", "=", "nn", ".", "Parameter", "(", "torch", ".", "zeros", "(", "self", ".", "d", ",", "self", ".", "m", ")", ")", "\n", "\n", "", "if", "self", ".", "dt", ".", "init", "==", "'uniform'", ":", "\n", "            ", "log_dt", "=", "torch", ".", "linspace", "(", "math", ".", "log", "(", "self", ".", "dt", ".", "min", ")", ",", "math", ".", "log", "(", "self", ".", "dt", ".", "max", ")", ",", "self", ".", "d", ")", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.lssl.AdaptiveTransition.gbt_B": [[163, 166], ["lssl.AdaptiveTransition.bilinear", "dt.new_zeros", "dt.new_ones"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.lssl.AdaptiveTransition.bilinear"], ["", "elif", "self", ".", "dt", ".", "init", "==", "'random'", ":", "\n", "            ", "log_dt", "=", "torch", ".", "rand", "(", "self", ".", "d", ")", "*", "(", "math", ".", "log", "(", "self", ".", "dt", ".", "max", ")", "-", "math", ".", "log", "(", "self", ".", "dt", ".", "min", ")", ")", "+", "math", ".", "log", "(", "self", ".", "dt", ".", "min", ")", "\n", "", "else", ":", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.lssl.LegTTransitionDense.forward_mult": [[171, 179], ["isinstance", "delta.unsqueeze.unsqueeze.unsqueeze", "lssl.LegTTransitionDense.A.transpose", "u.unsqueeze"], "methods", ["None"], ["            ", "self", ".", "register_buffer", "(", "'log_dt'", ",", "log_dt", ")", "\n", "", "self", ".", "k", "=", "None", "\n", "self", ".", "noise", "=", "noise", "\n", "\n", "self", ".", "activate", "=", "Activation", "(", "activation", ")", "\n", "self", ".", "drop", "=", "nn", ".", "Dropout", "(", "dropout", ")", "\n", "\n", "if", "self", ".", "ff", ":", "\n", "            ", "self", ".", "output_linear", "=", "nn", ".", "Linear", "(", "self", ".", "m", "*", "self", ".", "d", ",", "self", ".", "d", ")", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.lssl.LegTTransitionDense.inverse_mult": [[180, 196], ["isinstance", "zip", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "delta.unsqueeze().unsqueeze.unsqueeze().unsqueeze.unsqueeze().unsqueeze", "_A.transpose.transpose.transpose", "torch.linalg.solve().squeeze", "torch.linalg.solve().squeeze", "torch.linalg.solve().squeeze", "torch.linalg.solve().squeeze", "torch.linalg.solve().squeeze", "torch.linalg.solve().squeeze", "torch.linalg.solve().squeeze", "torch.linalg.solve().squeeze", "torch.linalg.solve().squeeze", "xs.append", "torch.broadcast_tensors", "torch.broadcast_tensors", "torch.broadcast_tensors", "torch.broadcast_tensors", "torch.broadcast_tensors", "torch.broadcast_tensors", "torch.broadcast_tensors", "torch.broadcast_tensors", "torch.broadcast_tensors", "delta.unsqueeze().unsqueeze.unsqueeze().unsqueeze.unsqueeze", "u.unsqueeze", "torch.linalg.solve", "torch.linalg.solve", "torch.linalg.solve", "torch.linalg.solve", "torch.linalg.solve", "torch.linalg.solve", "torch.linalg.solve", "torch.linalg.solve", "torch.linalg.solve"], "methods", ["None"], ["\n", "if", "weight_norm", ":", "\n", "                ", "self", ".", "output_linear", "=", "nn", ".", "utils", ".", "weight_norm", "(", "self", ".", "output_linear", ")", "\n", "\n", "# For test time shift", "\n", "", "", "self", ".", "l_max", "=", "l_max", "\n", "self", ".", "last_len", "=", "-", "1", "\n", "\n", "", "def", "forward", "(", "self", ",", "u", ",", "*", "args", ",", "state", "=", "None", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"\n        u: (L, B, H) [21-09-29] Our backbone now passes inputs as (B, L, H). This calss originally expected (L, B, H) so we transpose accordingly\n        state: (B, H, N) previous hidden state of the recurrence\n        \"\"\"", "\n", "next_state", "=", "None", "\n", "\n", "u", "=", "u", ".", "transpose", "(", "0", ",", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.lssl.LegTTransition.forward_mult": [[201, 204], ["legt_gbt_forward_t", "legt_gbt_forward"], "methods", ["None"], ["# changed sampling rate; uncache Krylov", "\n", "if", "self", ".", "last_len", "!=", "u", ".", "shape", "[", "0", "]", ":", "\n", "            ", "self", ".", "k", "=", "None", "\n", "self", ".", "last_len", "=", "u", ".", "shape", "[", "0", "]", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.lssl.LegTTransition.inverse_mult": [[205, 208], ["legt_gbt_backward_t", "legt_gbt_backward"], "methods", ["None"], ["# Calculate change from train sampling rate", "\n", "", "if", "self", ".", "l_max", ">", "0", ":", "\n", "            ", "rate", "=", "self", ".", "l_max", "/", "u", ".", "shape", "[", "0", "]", "\n", "# if rate != 1.0: dt = dt * rate", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.lssl.StateSpace.__init__": [[221, 254], ["torch.Module.__init__", "lssl.LegTTransitionDense", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "lssl.StateSpace.register_buffer", "torch.GELU", "torch.GELU", "torch.GELU", "torch.Dropout", "torch.Dropout", "torch.Dropout", "torch.Linear", "torch.Linear", "torch.Linear", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "math.log", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "math.log", "math.log"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__"], ["_conv", "=", "_learn", "or", "self", ".", "k", "is", "None", "or", "u", ".", "shape", "[", "0", "]", ">", "self", ".", "k", ".", "shape", "[", "-", "1", "]", "# or rate", "\n", "_noise", "=", "self", ".", "noise", ">", "0.0", "and", "self", ".", "training", "\n", "if", "_conv", ":", "\n", "            ", "B", "=", "self", ".", "transition", ".", "gbt_B", "(", "dt", ")", "# (..., N) depending if learn=2", "\n", "kb", ".", "append", "(", "B", ")", "\n", "", "if", "_noise", ":", "\n", "            ", "noise", "=", "self", ".", "noise", "*", "torch", ".", "randn", "(", "self", ".", "d", ",", "self", ".", "N", ",", "dtype", "=", "u", ".", "dtype", ",", "device", "=", "u", ".", "device", ")", "# (H, N)", "\n", "kb", ".", "append", "(", "noise", ")", "\n", "\n", "", "A", "=", "None", "\n", "if", "len", "(", "kb", ")", ">", "0", ":", "\n", "            ", "if", "rate", "is", "not", "None", ":", "\n", "                ", "dt", "=", "dt", "*", "rate", "\n", "\n", "", "A", "=", "self", ".", "transition", ".", "gbt_A", "(", "dt", ")", "# (..., N, N) (..., N)", "\n", "\n", "# Adjust by rate", "\n", "# if _conv and rate is not None:", "\n", "#     while rate > 1:", "\n", "#         B = B + torch.sum(A * B.unsqueeze(-2), dim=-1) # (I + A) @ B", "\n", "#         A = A @ A", "\n", "#         rate //= 2", "\n", "\n", "kb", "=", "[", "b", ".", "broadcast_to", "(", "dt", ".", "shape", "+", "(", "self", ".", "N", ",", ")", ")", "for", "b", "in", "kb", "]", "\n", "kb", "=", "torch", ".", "stack", "(", "torch", ".", "broadcast_tensors", "(", "*", "kb", ")", ",", "dim", "=", "0", ")", "# each (..., N)", "\n", "krylovs", "=", "krylov", "(", "u", ".", "shape", "[", "0", "]", ",", "A", ",", "kb", ")", "# (H, N, L) each", "\n", "k_noise", ",", "k_conv", "=", "torch", ".", "split", "(", "\n", "krylovs", ",", "\n", "split_size_or_sections", "=", "[", "int", "(", "_noise", ")", ",", "int", "(", "_conv", ")", "]", ",", "\n", "dim", "=", "0", "\n", ")", "\n", "if", "_conv", ":", "# Cache the Krylov matrix K(A, B)", "\n", "                ", "self", ".", "k", "=", "k_conv", ".", "squeeze", "(", "0", ")", "# (H, N, L)", "\n", "", "if", "_noise", ":", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.lssl.StateSpace.forward": [[256, 278], ["lssl.StateSpace.linear_system_from_krylov", "lssl.StateSpace.dropout", "einops.rearrange", "lssl.StateSpace.output_linear", "lssl.StateSpace.transition.gbt_A", "lssl.StateSpace.transition.gbt_B", "lssl.krylov", "lssl.StateSpace.activation_fn"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.lssl.StateSpace.linear_system_from_krylov", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.lssl.AdaptiveTransition.gbt_A", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.lssl.AdaptiveTransition.gbt_B", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.lssl.krylov"], ["\n", "# Convolution", "\n", "", "", "y", "=", "linear_system_from_krylov", "(", "u", ",", "self", ".", "C", ",", "self", ".", "D", ",", "self", ".", "k", "[", "...", ",", ":", "u", ".", "shape", "[", "0", "]", "]", ")", "# (L, B, H, M)", "\n", "if", "_noise", ":", "\n", "            ", "k_noise", "=", "torch", ".", "cumsum", "(", "k_noise", ",", "dim", "=", "-", "1", ")", "# (H, N, L) w + Aw + A^2w + ...", "\n", "k_noise", "=", "contract", "(", "'h m n, h n l -> l h m'", ",", "self", ".", "C", ",", "k_noise", ")", "# C @ k", "\n", "y", "=", "y", "+", "k_noise", ".", "unsqueeze", "(", "1", ")", "# (L, B, H, M)", "\n", "y", "=", "y", "+", "self", ".", "noise", "*", "torch", ".", "randn", "(", "y", ".", "shape", ",", "dtype", "=", "u", ".", "dtype", ",", "device", "=", "u", ".", "device", ")", "\n", "\n", "# State needs a special case because it has a batch dimension", "\n", "", "if", "state", "is", "not", "None", ":", "# (B, H, N)", "\n", "            ", "if", "A", "is", "None", ":", "A", "=", "self", ".", "transition", ".", "gbt_A", "(", "dt", ")", "# (..., N, N) (..., N)", "\n", "\n", "ATC", ",", "ATL", "=", "krylov", "(", "u", ".", "shape", "[", "0", "]", ",", "A", ".", "transpose", "(", "-", "1", ",", "-", "2", ")", ",", "self", ".", "C", ".", "transpose", "(", "0", ",", "1", ")", ",", "return_power", "=", "True", ")", "# (M, H, N, L), (H, N, N) represents A^T C and (A^T)^L", "\n", "y", "=", "y", "+", "contract", "(", "'mhnl, bhn -> lbhm'", ",", "ATC", ",", "state", ")", "\n", "\n", "# Compute next state", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                ", "next_state", "=", "contract", "(", "'hnp, bhn -> bhp'", ",", "ATL", ",", "state", ")", "\n", "if", "_noise", ":", "\n", "                    ", "next_state", "=", "next_state", "+", "k_noise", "[", "...", ",", "-", "1", "]", "\n", "", "next_state", "=", "next_state", "+", "contract", "(", "'lbh, hnl -> bhn'", ",", "u", ".", "flip", "(", "0", ")", ",", "self", ".", "k", "[", ":", "...", ",", "u", ".", "shape", "[", "0", "]", "]", ")", "# (B, H, N)", "\n", "next_state", "=", "contract", "(", "'hnp, bhp -> bhn'", ",", "A", ",", "next_state", ")", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.lssl.StateSpace.linear_system_from_krylov": [[279, 303], ["einops.rearrange", "k.unsqueeze.unsqueeze.to", "k.unsqueeze.unsqueeze.unsqueeze", "u.unsqueeze().transpose", "lssl.triangular_toeplitz_multiply", "y.transpose.transpose.transpose", "u.unsqueeze", "u.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.lssl.triangular_toeplitz_multiply"], ["next_state", "=", "next_state", ".", "detach", "(", ")", "# TODO necessary?", "\n", "\n", "# Debugging code useful for checking if state computation is correct", "\n", "# from models.functional.unroll import variable_unroll_sequential, variable_unroll", "\n", "# B = self.transition.gbt_B(dt)", "\n", "# inps = B*u.unsqueeze(-1) # (L, B, H, N)", "\n", "# inps[0] = inps[0] + state", "\n", "# xx = variable_unroll(A, inps, variable=False)", "\n", "# yy = torch.sum(self.C * xx.unsqueeze(-2), dim=-1)", "\n", "# yy = yy + u.unsqueeze(-1) * self.D # true output y; should equal y", "\n", "# xx_ = variable_unroll(A, B*u.unsqueeze(-1), variable=False)", "\n", "# yy_ = torch.sum(self.C * xx_.unsqueeze(-2), dim=-1)", "\n", "# yy_ = yy_ + u.unsqueeze(-1) * self.D # output without state; should equal y before the C A^T S term was added", "\n", "# ss = (A @ xx[-1].unsqueeze(-1)).squeeze(-1) # should equal next_state", "\n", "# breakpoint()", "\n", "# y = z", "\n", "\n", "# bias term", "\n", "", "", "if", "self", ".", "bias", ":", "\n", "            ", "y", "=", "y", "+", "self", ".", "E", "\n", "\n", "", "y", "=", "self", ".", "drop", "(", "self", ".", "activate", "(", "y", ")", ")", "\n", "\n", "y", "=", "rearrange", "(", "y", ",", "'l b h m -> l b (h m)'", ")", "# (L, B, H*M)", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.lssl.triangular_toeplitz_multiply": [[23, 32], ["torch.pad", "torch.pad", "torch.fft.rfft", "torch.fft.rfft", "torch.fft.rfft", "torch.fft.rfft", "torch.fft.rfft", "torch.fft.rfft", "torch.fft.irfft", "torch.fft.irfft", "torch.fft.irfft"], "function", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.sc.pad", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.sc.pad"], ["\n", "\n", "\n", "# Equivalent ways to perform C @ k, slight speed differences", "\n", "k", "=", "C", "@", "k", "# (..., M, L)", "\n", "# k = torch.einsum('... m n, ... n l -> ... m l', C, k) # C @ k", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.lssl.krylov": [[33, 56], ["b.unsqueeze", "torch.cat.contiguous", "torch.cat", "torch.cat", "torch.cat"], "function", ["None"], ["# k = torch.sum(k.unsqueeze(-3) * C.unsqueeze(-1), dim=-2) # (..., M, L) C @ k", "\n", "\n", "k", "=", "rearrange", "(", "k", ",", "'... m l -> m ... l'", ")", "\n", "k", "=", "k", ".", "to", "(", "u", ")", "# if training in half precision, need to go back to float32 for the fft", "\n", "k", "=", "k", ".", "unsqueeze", "(", "1", ")", "# (M, 1, ..., L)", "\n", "\n", "v", "=", "u", ".", "unsqueeze", "(", "-", "1", ")", ".", "transpose", "(", "0", ",", "-", "1", ")", "# (1, B, ..., L)", "\n", "y", "=", "causal_convolution", "(", "k", ",", "v", ",", "fast", "=", "True", ")", "# (M, B, ..., L)", "\n", "y", "=", "y", ".", "transpose", "(", "0", ",", "-", "1", ")", "# (L, B, ..., M)", "\n", "y", "=", "y", "+", "u", ".", "unsqueeze", "(", "-", "1", ")", "*", "D", "# (L, B, ..., M)", "\n", "return", "y", "\n", "\n", "", "class", "Platypus", "(", "SequenceModule", ")", ":", "\n", "    ", "\"\"\" Implementation of LSSL module.\n    # TODO this expects (length, batch) but this codebase is now (batch, length)\n    \"\"\"", "\n", "requires_length", "=", "True", "\n", "\n", "def", "__init__", "(", "\n", "self", ",", "\n", "d", ",", "\n", "d_model", "=", "-", "1", ",", "# overloading this term, same as memory_order or N", "\n", "measure", "=", "'legs'", ",", "# 'legs', 'legt' main ones; can also try 'lagt'", "\n", "measure_args", "=", "{", "}", ",", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.lssl.hippo": [[58, 67], ["numpy.arange", "numpy.meshgrid", "numpy.where"], "function", ["None"], ["lr", "=", "0.0001", ",", "# controls learning rate of transition parameters", "\n", "noise", "=", "0.0", ",", "# injects input noise to the state space system", "\n", "init", "=", "'normal'", ",", "# for debugging, but might be useful?", "\n", "dt", "=", "None", ",", "\n", "channels", "=", "1", ",", "# denoted by M below", "\n", "bias", "=", "False", ",", "\n", "activation", "=", "'gelu'", ",", "\n", "ff", "=", "True", ",", "\n", "weight_norm", "=", "False", ",", "\n", "dropout", "=", "0.0", ",", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.s4.SSKernelNPLR._setup_C": [[290, 318], ["torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "_r2c", "s4.SSKernelNPLR._setup_state", "s4.power", "_conj", "contract", "s4.SSKernelNPLR.C.copy_", "s4.SSKernelNPLR.L.item", "power.transpose", "_c2r", "log.info", "s4.SSKernelNPLR.L.item", "s4.SSKernelNPLR.L.item", "log.info", "s4.SSKernelNPLR.L.item", "s4.SSKernelNPLR.L.item"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.cauchy._r2c", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.s4.SSKernelNPLR._setup_state", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.s4.power", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.cauchy._c2r"], []], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.s4.SSKernelNPLR._omega": [[320, 339], ["torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "hasattr", "numpy.exp", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "s4.SSKernelNPLR.omega.size"], "methods", ["None"], []], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.s4.SSKernelNPLR.__init__": [[340, 399], ["torch.Module.__init__", "log_dt.size", "w.size", "w.size", "C.expand.expand.expand", "B.unsqueeze.unsqueeze.unsqueeze", "torch.Parameter", "torch.Parameter", "torch.Parameter", "s4.SSKernelNPLR.register", "s4.SSKernelNPLR.register", "s4.SSKernelNPLR.register", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "s4.SSKernelNPLR.register", "s4.SSKernelNPLR.register", "s4.SSKernelNPLR.register_buffer", "w.size", "P.size", "B.unsqueeze.unsqueeze.size", "C.expand.expand.size", "w.size", "P.size", "B.unsqueeze.unsqueeze.size", "w.size", "torch.broadcast_shapes", "torch.broadcast_shapes", "torch.broadcast_shapes", "torch.broadcast_shapes", "torch.broadcast_shapes", "torch.broadcast_shapes", "torch.broadcast_shapes", "torch.broadcast_shapes", "torch.broadcast_shapes", "_c2r", "trainable.get", "_c2r", "trainable.get", "_c2r", "trainable.get", "trainable.get", "trainable.get", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "w.size", "_resolve_conj"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.s4.SSKernelNPLR.register", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.s4.SSKernelNPLR.register", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.s4.SSKernelNPLR.register", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.s4.SSKernelNPLR.register", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.s4.SSKernelNPLR.register", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.cauchy._c2r", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.cauchy._c2r", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.cauchy._c2r"], []], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.s4.SSKernelNPLR._w": [[401, 407], ["torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp"], "methods", ["None"], []], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.s4.SSKernelNPLR.forward": [[408, 477], ["s4.SSKernelNPLR.L.item", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "_r2c", "_r2c", "_r2c", "einops.repeat.conj", "s4.SSKernelNPLR._w", "s4.SSKernelNPLR._omega", "einops.repeat", "einops.repeat", "einops.repeat", "einops.repeat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.fft.irfft", "torch.fft.irfft", "torch.fft.irfft", "torch.fft.irfft", "torch.fft.irfft", "torch.fft.irfft", "torch.fft.irfft", "torch.fft.irfft", "torch.fft.irfft", "s4.SSKernelNPLR._setup_C", "s4.SSKernelNPLR.L.item", "s4.SSKernelNPLR.L.item", "s4.SSKernelNPLR._setup_C", "torch.exp.unsqueeze", "torch.exp.unsqueeze", "torch.exp.unsqueeze", "torch.cat.unsqueeze", "torch.cat.unsqueeze", "torch.cat.unsqueeze", "torch.cat.unsqueeze", "torch.cat.unsqueeze", "torch.cat.unsqueeze", "cauchy_mult", "s4.SSKernelNPLR.L.item", "cauchy_conj", "cauchy_slow"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.cauchy._r2c", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.cauchy._r2c", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.cauchy._r2c", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.s4.SSKernelNPLR._w", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.s4.SSKernelNPLR._omega", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.s4.SSKernelNPLR._setup_C", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.s4.SSKernelNPLR._setup_C", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.cauchy.cauchy.cauchy_mult", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.cauchy.cauchy_conj", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.cauchy.cauchy_slow"], []], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.s4.SSKernelNPLR.double_length": [[478, 482], ["torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "s4.SSKernelNPLR._setup_C"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.s4.SSKernelNPLR._setup_C"], []], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.s4.SSKernelNPLR._setup_linear": [[483, 515], ["torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "s4.SSKernelNPLR._w", "_r2c", "_r2c", "einops.repeat.conj", "einops.repeat", "einops.repeat", "einops.repeat", "einops.repeat", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "einops.rearrange", "einops.rearrange", "torch.eye", "torch.eye", "torch.eye", "torch.eye", "torch.eye", "torch.eye", "torch.eye", "torch.eye", "torch.eye", "torch.linalg.solve", "torch.linalg.solve", "torch.linalg.solve", "torch.linalg.solve", "torch.linalg.solve", "torch.linalg.solve", "torch.linalg.solve", "torch.linalg.solve", "torch.linalg.solve", "torch.tensor().to.to", "torch.tensor().to.to", "torch.tensor().to.to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "contract", "torch.exp.unsqueeze", "torch.exp.unsqueeze", "torch.exp.unsqueeze", "torch.exp.unsqueeze", "torch.exp.unsqueeze", "torch.exp.unsqueeze", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "numpy.linalg.solve", "torch.tensor().to.to().cpu", "torch.tensor().to.to().cpu", "torch.tensor().to.to().cpu", "einops.rearrange.cpu", "torch.tensor().to.to", "torch.tensor().to.to", "torch.tensor().to.to"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.s4.SSKernelNPLR._w", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.cauchy._r2c", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.cauchy._r2c"], []], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.s4.SSKernelNPLR._step_state_linear": [[517, 556], ["_r2c", "s4.SSKernelNPLR.step_params.copy", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros.size", "torch.zeros.size", "torch.zeros.size", "contract_fn", "torch.zeros.size", "torch.zeros.size", "torch.zeros.size", "_conj", "contract", "torch.zeros.unsqueeze", "torch.zeros.unsqueeze", "torch.zeros.unsqueeze", "contract_fn", "contract", "s4.SSKernelNPLR.items", "_conj", "_conj", "_conj"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.cauchy._r2c"], []], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.s4.SSKernelNPLR._setup_state": [[557, 573], ["s4.SSKernelNPLR._setup_linear", "_r2c", "torch.eye().unsqueeze", "torch.eye().unsqueeze", "torch.eye().unsqueeze", "torch.eye().unsqueeze", "torch.eye().unsqueeze", "torch.eye().unsqueeze", "torch.eye().unsqueeze", "torch.eye().unsqueeze", "torch.eye().unsqueeze", "s4.SSKernelNPLR._step_state_linear", "einops.rearrange", "_r2c.new_ones", "s4.SSKernelNPLR._step_state_linear", "_conj", "einops.rearrange", "torch.eye", "torch.eye", "torch.eye", "torch.eye", "torch.eye", "torch.eye", "torch.eye", "torch.eye", "torch.eye"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.s4.SSKernelNPLR._setup_linear", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.cauchy._r2c", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.s4.SSKernelNPLR._step_state_linear", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.s4.SSKernelNPLR._step_state_linear"], []], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.s4.SSKernelNPLR._step_state": [[574, 578], ["s4.SSKernelNPLR.state_contraction", "s4.SSKernelNPLR.input_contraction"], "methods", ["None"], []], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.s4.SSKernelNPLR.setup_step": [[580, 618], ["s4.SSKernelNPLR._setup_state", "s4.power", "torch.eye().to", "torch.eye().to", "torch.eye().to", "torch.eye().to", "torch.eye().to", "torch.eye().to", "torch.eye().to", "torch.eye().to", "torch.eye().to", "_conj", "torch.linalg.solve().squeeze", "torch.linalg.solve().squeeze", "torch.linalg.solve().squeeze", "torch.linalg.solve().squeeze", "torch.linalg.solve().squeeze", "torch.linalg.solve().squeeze", "torch.linalg.solve().squeeze", "torch.linalg.solve().squeeze", "torch.linalg.solve().squeeze", "_r2c", "torch.eye", "torch.eye", "torch.eye", "torch.eye", "torch.eye", "torch.eye", "torch.eye", "torch.eye", "torch.eye", "torch.linalg.solve", "torch.linalg.solve", "torch.linalg.solve", "torch.linalg.solve", "torch.linalg.solve", "torch.linalg.solve", "torch.linalg.solve", "torch.linalg.solve", "torch.linalg.solve", "torch.linalg.eig", "torch.linalg.eig", "torch.linalg.eig", "torch.linalg.eig", "torch.linalg.eig", "torch.linalg.eig", "torch.linalg.eig", "torch.linalg.eig", "torch.linalg.eig", "torch.linalg.inv", "torch.linalg.inv", "torch.linalg.inv", "torch.linalg.inv", "torch.linalg.inv", "torch.linalg.inv", "torch.linalg.inv", "torch.linalg.inv", "torch.linalg.inv", "contract", "contract", "s4.SSKernelNPLR.dA.size", "_conj.unsqueeze", "print", "NotImplementedError", "power.transpose", "torch.dist", "torch.dist", "torch.dist", "torch.dist", "torch.dist", "torch.dist", "torch.dist", "torch.dist", "torch.dist", "torch.diag_embed", "torch.diag_embed", "torch.diag_embed", "torch.diag_embed", "torch.diag_embed", "torch.diag_embed", "torch.diag_embed", "torch.diag_embed", "torch.diag_embed"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.s4.SSKernelNPLR._setup_state", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.s4.power", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.cauchy._r2c"], []], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.s4.SSKernelNPLR.default_state": [[620, 658], ["_r2c", "_r2c.size", "_r2c.size", "contract_expression", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "contract_expression", "contract_expression", "contract_expression"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.functional.cauchy._r2c"], []], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.s4.SSKernelNPLR.step": [[659, 668], ["s4.SSKernelNPLR.output_contraction", "s4.SSKernelNPLR._step_state_linear", "s4.SSKernelNPLR._step_state"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.s4.SSKernelNPLR._step_state_linear", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.s4.SSKernelNPLR._step_state"], []], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.s4.SSKernelNPLR.register": [[669, 684], ["s4.SSKernelNPLR.register_parameter", "s4.SSKernelNPLR.register_buffer", "len", "setattr", "torch.Parameter", "torch.Parameter", "torch.Parameter", "getattr"], "methods", ["None"], []], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.s4.HippoSSKernel.__init__": [[695, 749], ["torch.Module.__init__", "s4.nplr", "einops.repeat().clone().contiguous.unsqueeze", "einops.repeat().clone().contiguous.unsqueeze", "einops.repeat().clone().contiguous.unsqueeze", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "einops.repeat().clone().contiguous", "einops.repeat().clone().contiguous", "einops.repeat().clone().contiguous", "s4.SSKernelNPLR", "math.log", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "einops.repeat().clone", "einops.repeat().clone", "einops.repeat().clone", "math.log", "math.log", "einops.repeat().clone().contiguous.size", "einops.repeat().clone().contiguous.size", "einops.repeat().clone().contiguous.size", "einops.repeat", "einops.repeat", "einops.repeat", "einops.repeat().clone().contiguous.size", "einops.repeat().clone().contiguous.size", "einops.repeat().clone().contiguous.size"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.s4.nplr"], []], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.s4.HippoSSKernel.forward": [[751, 754], ["s4.HippoSSKernel.kernel", "s4.HippoSSKernel.float"], "methods", ["None"], []], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.s4.HippoSSKernel.setup_step": [[755, 757], ["s4.HippoSSKernel.kernel.setup_step"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.s4.S4.setup_step"], []], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.s4.HippoSSKernel.step": [[758, 761], ["s4.HippoSSKernel.kernel.step", "u.float"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.sashimi.sashimi.Sashimi.step"], []], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.s4.HippoSSKernel.default_state": [[762, 764], ["s4.HippoSSKernel.kernel.default_state"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.sashimi.sashimi.Sashimi.default_state"], []], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.s4.S4.__init__": [[768, 828], ["torch.Module.__init__", "torch.Parameter", "torch.Parameter", "torch.Parameter", "s4.HippoSSKernel", "s4.Activation", "s4.LinearActivation", "src.utils.train.get_logger", "src.utils.train.get_logger.info", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "dropout_fn", "torch.Identity", "torch.Identity", "torch.Identity"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.s4.Activation", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.s4.LinearActivation", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.s4.get_logger"], []], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.s4.S4.forward": [[831, 869], ["u.transpose.transpose.size", "s4.S4.kernel", "torch.fft.rfft", "torch.fft.rfft", "torch.fft.rfft", "torch.fft.rfft", "torch.fft.rfft", "torch.fft.rfft", "torch.fft.rfft", "torch.fft.rfft", "torch.fft.rfft", "torch.fft.rfft", "torch.fft.rfft", "torch.fft.rfft", "torch.fft.rfft", "torch.fft.rfft", "torch.fft.rfft", "torch.fft.rfft", "torch.fft.rfft", "torch.fft.rfft", "contract", "einops.rearrange", "s4.S4.dropout", "s4.S4.output_linear", "u.transpose.transpose.transpose", "einops.rearrange", "torch.fft.irfft", "torch.fft.irfft", "torch.fft.irfft", "torch.fft.irfft", "torch.fft.irfft", "torch.fft.irfft", "torch.fft.irfft", "torch.fft.irfft", "torch.fft.irfft", "contract", "s4.S4.activation", "y.transpose.transpose.transpose", "torch.pad", "torch.pad", "torch.pad", "torch.pad", "torch.pad", "torch.pad", "k1.flip"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.sc.pad", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.sc.pad", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.sc.pad", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.sc.pad", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.sc.pad", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.sc.pad"], []], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.s4.S4.setup_step": [[870, 872], ["s4.S4.kernel.setup_step"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.s4.S4.setup_step"], []], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.s4.S4.step": [[873, 891], ["s4.S4.kernel.step", "einops.rearrange", "s4.S4.activation", "s4.S4.output_linear().squeeze", "s4.S4.output_linear", "u.unsqueeze", "s4.S4.output_linear", "s4.S4.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.sashimi.sashimi.Sashimi.step"], []], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.s4.S4.default_state": [[892, 894], ["s4.S4.kernel.default_state"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.sashimi.sashimi.Sashimi.default_state"], []], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.s4.S4.d_state": [[895, 898], ["None"], "methods", ["None"], []], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.s4.S4.d_output": [[899, 902], ["None"], "methods", ["None"], []], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.s4.S4.state_to_tensor": [[903, 906], ["einops.rearrange"], "methods", ["None"], []], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.s4.get_logger": [[20, 32], ["logging.getLogger", "logging.getLogger.setLevel", "setattr", "pytorch_lightning.utilities.rank_zero_only", "getattr"], "function", ["None"], ["if", "optimized", ":", "\n", "    ", "contract", "=", "oe", ".", "contract", "\n", "", "else", ":", "\n", "    ", "contract", "=", "torch", ".", "einsum", "\n", "\n", "", "from", "src", ".", "models", ".", "sequence", ".", "ss", ".", "kernel", "import", "HippoSSKernel", "\n", "from", "src", ".", "models", ".", "nn", "import", "LinearActivation", ",", "Activation", ",", "Normalization", "\n", "\n", "class", "S4", "(", "nn", ".", "Module", ")", ":", "\n", "    ", "requires_length", "=", "True", "\n", "\n", "def", "__init__", "(", "\n", "self", ",", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.s4._broadcast_dims": [[90, 94], ["max", "tensor.view", "len", "len"], "function", ["None"], ["            ", "channels", "*=", "2", "\n", "\n", "\n", "# SSM Kernel", "\n", "", "self", ".", "kernel", "=", "HippoSSKernel", "(", "self", ".", "h", ",", "N", "=", "self", ".", "n", ",", "L", "=", "l_max", ",", "channels", "=", "channels", ",", "verbose", "=", "verbose", ",", "**", "kernel_args", ")", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.s4.Activation": [[107, 124], ["torch.Identity", "torch.Tanh", "torch.ReLU", "torch.GELU", "torch.SiLU", "torch.GLU", "torch.Sigmoid", "NotImplementedError"], "function", ["None"], ["", "", "if", "not", "self", ".", "linear", ":", "\n", "            ", "self", ".", "output_linear", "=", "LinearActivation", "(", "\n", "self", ".", "h", "*", "self", ".", "channels", ",", "\n", "self", ".", "h", ",", "\n", "transposed", "=", "self", ".", "transposed", ",", "\n", "initializer", "=", "initializer", ",", "\n", "activation", "=", "postact", ",", "\n", "activate", "=", "True", ",", "\n", "weight_norm", "=", "weight_norm", ",", "\n", ")", "\n", "\n", "\n", "", "", "def", "forward", "(", "self", ",", "u", ",", "state", "=", "None", ",", "**", "kwargs", ")", ":", "# absorbs return_output and transformer src mask", "\n", "        "]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.s4.LinearActivation": [[125, 143], ["linear_cls", "functools.partial", "s4.Activation", "torch.Sequential"], "function", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.s4.Activation"], ["\n", "if", "not", "self", ".", "transposed", ":", "u", "=", "u", ".", "transpose", "(", "-", "1", ",", "-", "2", ")", "\n", "L", "=", "u", ".", "size", "(", "-", "1", ")", "\n", "\n", "# Compute SS Kernel", "\n", "k", ",", "k_state", "=", "self", ".", "kernel", "(", "L", "=", "L", ",", "state", "=", "state", ")", "# (C H L) (B C H L)", "\n", "\n", "# Convolution", "\n", "if", "self", ".", "bidirectional", ":", "\n", "            ", "k0", ",", "k1", "=", "rearrange", "(", "k", ",", "'(s c) h l -> s c h l'", ",", "s", "=", "2", ")", "\n", "k", "=", "F", ".", "pad", "(", "k0", ",", "(", "0", ",", "L", ")", ")", "+", "F", ".", "pad", "(", "k1", ".", "flip", "(", "-", "1", ")", ",", "(", "L", ",", "0", ")", ")", "\n", "", "if", "self", ".", "shift", ":", "\n", "# Try flip and pad to correct for potential off-by-one", "\n", "            ", "k_f", "=", "torch", ".", "fft", ".", "rfft", "(", "F", ".", "pad", "(", "k", ".", "flip", "(", "-", "1", ")", ",", "(", "L", ",", "0", ")", ")", ",", "n", "=", "2", "*", "L", ")", "# (C H L)", "\n", "u_f", "=", "torch", ".", "fft", ".", "rfft", "(", "F", ".", "pad", "(", "u", ".", "flip", "(", "-", "1", ")", ",", "(", "L", ",", "0", ")", ")", ",", "n", "=", "2", "*", "L", ")", "# (B H L)", "\n", "y_f", "=", "contract", "(", "'bhl,chl->bchl'", ",", "u_f", ",", "k_f", ")", "# k_f.unsqueeze(-4) * u_f.unsqueeze(-3) # (B C H L)", "\n", "y", "=", "torch", ".", "fft", ".", "irfft", "(", "y_f", ",", "n", "=", "2", "*", "L", ")", "[", "...", ",", "L", ":", "]", ".", "flip", "(", "-", "1", ")", "# (B C H L)", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.s4.power": [[146, 187], ["torch.eye().to", "torch.eye().to", "torch.eye().to", "powers.append", "einops.rearrange.size", "powers.pop", "einops.rearrange.size", "einops.rearrange", "einops.rearrange.squeeze", "torch.eye", "torch.eye", "torch.eye", "powers.pop"], "function", ["None"], ["u_f", "=", "torch", ".", "fft", ".", "rfft", "(", "u", ",", "n", "=", "2", "*", "L", ")", "# (B H L)", "\n", "y_f", "=", "contract", "(", "'bhl,chl->bchl'", ",", "u_f", ",", "k_f", ")", "# k_f.unsqueeze(-4) * u_f.unsqueeze(-3) # (B C H L)", "\n", "y", "=", "torch", ".", "fft", ".", "irfft", "(", "y_f", ",", "n", "=", "2", "*", "L", ")", "[", "...", ",", ":", "L", "]", "# (B C H L)", "\n", "\n", "\n", "\n", "# Compute D term in state space equation - essentially a skip connection", "\n", "", "y", "=", "y", "+", "contract", "(", "'bhl,ch->bchl'", ",", "u", ",", "self", ".", "D", ")", "# u.unsqueeze(-3) * self.D.unsqueeze(-1)", "\n", "\n", "# Compute state update", "\n", "if", "state", "is", "not", "None", ":", "\n", "            ", "assert", "not", "self", ".", "bidirectional", ",", "\"Bidirectional not supported with state forwarding\"", "\n", "y", "=", "y", "+", "k_state", "\n", "next_state", "=", "self", ".", "kernel", ".", "forward_state", "(", "u", ",", "state", ")", "\n", "", "else", ":", "\n", "            ", "next_state", "=", "None", "\n", "\n", "# Optional hyper-network multiplication", "\n", "", "if", "self", ".", "hyper", ":", "\n", "            ", "y", ",", "yh", "=", "rearrange", "(", "y", ",", "'b (s c) h l -> s b c h l'", ",", "s", "=", "2", ")", "\n", "y", "=", "self", ".", "hyper_activation", "(", "yh", ")", "*", "y", "\n", "\n", "# Reshape to flatten channels", "\n", "", "y", "=", "rearrange", "(", "y", ",", "'... c h l -> ... (c h) l'", ")", "\n", "\n", "if", "not", "self", ".", "linear", ":", "\n", "            ", "y", "=", "self", ".", "dropout", "(", "self", ".", "activation", "(", "y", ")", ")", "\n", "\n", "", "if", "not", "self", ".", "transposed", ":", "y", "=", "y", ".", "transpose", "(", "-", "1", ",", "-", "2", ")", "\n", "\n", "if", "not", "self", ".", "linear", ":", "\n", "            ", "y", "=", "self", ".", "norm", "(", "y", ")", "\n", "y", "=", "self", ".", "output_linear", "(", "y", ")", "\n", "\n", "", "return", "y", ",", "next_state", "\n", "\n", "", "def", "setup_step", "(", "self", ")", ":", "\n", "        ", "self", ".", "kernel", ".", "setup_step", "(", ")", "\n", "\n", "", "def", "step", "(", "self", ",", "u", ",", "state", ")", ":", "\n", "        "]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.s4.transition": [[191, 223], ["numpy.arange", "numpy.meshgrid", "numpy.sqrt", "np.zeros.copy", "numpy.diag", "numpy.linalg.inv", "numpy.diag", "numpy.arange", "numpy.zeros", "numpy.where", "numpy.diag", "numpy.stack().reshape", "numpy.diag", "numpy.stack", "numpy.diag", "numpy.zeros"], "function", ["None"], ["\n", "assert", "not", "self", ".", "training", "\n", "\n", "y", ",", "next_state", "=", "self", ".", "kernel", ".", "step", "(", "u", ",", "state", ")", "# (B C H)", "\n", "y", "=", "y", "+", "u", ".", "unsqueeze", "(", "-", "2", ")", "*", "self", ".", "D", "\n", "y", "=", "rearrange", "(", "y", ",", "'... c h -> ... (c h)'", ")", "\n", "y", "=", "self", ".", "activation", "(", "y", ")", "\n", "if", "self", ".", "transposed", ":", "\n", "            ", "y", "=", "self", ".", "output_linear", "(", "y", ".", "unsqueeze", "(", "-", "1", ")", ")", ".", "squeeze", "(", "-", "1", ")", "\n", "", "else", ":", "\n", "            ", "y", "=", "self", ".", "output_linear", "(", "y", ")", "\n", "", "return", "y", ",", "next_state", "\n", "\n", "", "def", "default_state", "(", "self", ",", "*", "batch_shape", ",", "device", "=", "None", ")", ":", "\n", "        ", "return", "self", ".", "kernel", ".", "default_state", "(", "*", "batch_shape", ")", "\n", "\n", "", "@", "property", "\n", "def", "d_state", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "h", "*", "self", ".", "n", "\n", "\n", "", "@", "property", "\n", "def", "d_output", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "h", "\n", "\n", "", "@", "property", "\n", "def", "state_to_tensor", "(", "self", ")", ":", "\n", "        ", "return", "lambda", "state", ":", "rearrange", "(", "'... h n -> ... (h n)'", ",", "state", ")", "\n", "\n", "\n", "", "", "def", "test_state", "(", "random_init", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "# B = 1", "\n", "# H = 64", "\n", "# N = 64", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.s4.rank_correction": [[224, 243], ["torch.stack.size", "torch.sqrt().unsqueeze", "torch.sqrt().unsqueeze", "torch.sqrt().unsqueeze", "torch.cat", "torch.cat", "torch.cat", "torch.ones", "torch.ones", "torch.ones", "torch.stack.clone", "torch.stack.clone", "torch.stack", "torch.stack", "torch.stack", "torch.sqrt", "torch.sqrt", "torch.sqrt", "torch.zeros", "torch.zeros", "torch.zeros", "torch.arange", "torch.arange", "torch.arange"], "function", ["None"], ["# L = 1024", "\n", "    ", "B", "=", "2", "\n", "H", "=", "3", "\n", "N", "=", "4", "\n", "L", "=", "8", "\n", "s4", "=", "S4", "(", "H", ",", "d_state", "=", "N", ",", "l_max", "=", "L", ",", "**", "kwargs", ")", "\n", "s4", ".", "to", "(", "device", ")", "\n", "s4", ".", "eval", "(", ")", "\n", "for", "module", "in", "s4", ".", "modules", "(", ")", ":", "\n", "        ", "if", "hasattr", "(", "module", ",", "'setup_step'", ")", ":", "module", ".", "setup_step", "(", ")", "\n", "\n", "", "u", "=", "torch", ".", "ones", "(", "B", ",", "H", ",", "L", ")", ".", "to", "(", "device", ")", "\n", "initial_state", "=", "s4", ".", "default_state", "(", "B", ")", "\n", "if", "random_init", ":", "\n", "        ", "if", "initial_state", ".", "size", "(", "-", "1", ")", "==", "N", ":", "\n", "            ", "initial_state", "=", "initial_state", "[", "...", ",", ":", "N", "//", "2", "]", "\n", "", "initial_state", "=", "torch", ".", "randn_like", "(", "initial_state", ")", "\n", "initial_state", "=", "torch", ".", "cat", "(", "[", "initial_state", ",", "initial_state", ".", "conj", "(", ")", "]", ",", "dim", "=", "-", "1", ")", "\n", "\n", "", "state", "=", "initial_state", ".", "clone", "(", ")", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.s4.nplr": [[244, 271], ["s4.transition", "torch.as_tensor", "torch.as_tensor", "torch.as_tensor", "s4.rank_correction", "torch.linalg.eig", "torch.linalg.eig", "torch.linalg.eig", "w[].contiguous", "V[].contiguous", "V[].contiguous.conj().transpose", "contract", "contract", "torch.as_tensor", "torch.as_tensor", "torch.as_tensor", "torch.sum", "torch.sum", "torch.sum", "contract.to", "contract.to", "V[].contiguous.conj", "contract.unsqueeze", "contract.unsqueeze"], "function", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.s4.transition", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.s4.rank_correction"], ["y", ",", "final_state", "=", "s4", "(", "u", ",", "state", "=", "state", ")", "\n", "print", "(", "\"output:\\n\"", ",", "y", ",", "y", ".", "shape", ")", "\n", "print", "(", "\"final state:\\n\"", ",", "final_state", ",", "final_state", ".", "shape", ")", "\n", "\n", "# Use Stepping", "\n", "state", "=", "initial_state", ".", "clone", "(", ")", "\n", "ys", "=", "[", "]", "\n", "for", "u_", "in", "torch", ".", "unbind", "(", "u", ",", "dim", "=", "-", "1", ")", ":", "\n", "        ", "y_", ",", "state", "=", "s4", ".", "step", "(", "u_", ",", "state", "=", "state", ")", "\n", "ys", ".", "append", "(", "y_", ")", "\n", "", "ys", "=", "torch", ".", "stack", "(", "ys", ",", "dim", "=", "-", "1", ")", "\n", "print", "(", "\"step outputs:\\n\"", ",", "ys", ")", "\n", "print", "(", "\"step final state:\\n\"", ",", "state", ")", "\n", "\n", "# Use Chunking", "\n", "\n", "chunks", "=", "4", "\n", "state", "=", "initial_state", ".", "clone", "(", ")", "\n", "ys", "=", "[", "]", "\n", "for", "u_", "in", "u", ".", "chunk", "(", "chunks", ",", "dim", "=", "-", "1", ")", ":", "\n", "        ", "y_", ",", "state", "=", "s4", "(", "u_", ",", "state", "=", "state", ")", "\n", "ys", ".", "append", "(", "y_", ")", "\n", "", "ys", "=", "torch", ".", "cat", "(", "ys", ",", "dim", "=", "-", "1", ")", "\n", "print", "(", "\"chunk outputs:\\n\"", ",", "ys", ")", "\n", "print", "(", "\"chunk final state:\\n\"", ",", "state", ")", "\n", "print", "(", "\"chunk output error:\"", ")", "\n", "utils", ".", "compare_outputs", "(", "y", ",", "ys", ")", "\n", "print", "(", "\"chunk final state error:\"", ")", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.uea.download": [[51, 67], ["os.path.exists", "urllib.request.urlretrieve", "os.path.exists", "os.mkdir", "os.path.exists", "os.mkdir", "str", "zipfile.ZipFile", "f.extractall", "str"], "function", ["None"], ["def", "download", "(", ")", ":", "\n", "    ", "\"\"\" Download data if not exists \"\"\"", "\n", "base_base_loc", "=", "here", "# / 'data'", "\n", "base_loc", "=", "base_base_loc", "/", "'uea'", "\n", "loc", "=", "base_loc", "/", "'Multivariate2018_ts.zip'", "\n", "if", "os", ".", "path", ".", "exists", "(", "loc", ")", ":", "\n", "        ", "return", "\n", "", "if", "not", "os", ".", "path", ".", "exists", "(", "base_base_loc", ")", ":", "\n", "        ", "os", ".", "mkdir", "(", "base_base_loc", ")", "\n", "", "if", "not", "os", ".", "path", ".", "exists", "(", "base_loc", ")", ":", "\n", "        ", "os", ".", "mkdir", "(", "base_loc", ")", "\n", "", "urllib", ".", "request", ".", "urlretrieve", "(", "'http://www.timeseriesclassification.com/Downloads/Archives/Multivariate2018_ts.zip'", ",", "\n", "str", "(", "loc", ")", ")", "\n", "\n", "with", "zipfile", ".", "ZipFile", "(", "loc", ",", "'r'", ")", "as", "f", ":", "\n", "        ", "f", ".", "extractall", "(", "str", "(", "base_loc", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.uea.load_data": [[68, 81], ["sktime.utils.data_io.load_from_tsfile_to_dataframe", "sktime.utils.data_io.load_from_tsfile_to_dataframe", "train_X.to_numpy.to_numpy", "test_X.to_numpy.to_numpy", "numpy.concatenate", "numpy.concatenate", "str", "str"], "function", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.bidmc.data_loader.load_from_tsfile_to_dataframe", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.bidmc.data_loader.load_from_tsfile_to_dataframe"], ["", "", "def", "load_data", "(", "dataset_name", ")", ":", "\n", "    ", "\"\"\" Load X, y numpy data for given dataset \"\"\"", "\n", "assert", "dataset_name", "in", "valid_dataset_names", ",", "\"Must specify a valid dataset name.\"", "\n", "\n", "# base_filename = here / 'data' / 'UEA' / 'Multivariate_ts' / dataset_name / dataset_name", "\n", "base_filename", "=", "here", "/", "'uea'", "/", "'Multivariate_ts'", "/", "dataset_name", "/", "dataset_name", "\n", "train_X", ",", "train_y", "=", "load_from_tsfile_to_dataframe", "(", "str", "(", "base_filename", ")", "+", "'_TRAIN.ts'", ")", "\n", "test_X", ",", "test_y", "=", "load_from_tsfile_to_dataframe", "(", "str", "(", "base_filename", ")", "+", "'_TEST.ts'", ")", "\n", "train_X", "=", "train_X", ".", "to_numpy", "(", ")", "\n", "test_X", "=", "test_X", ".", "to_numpy", "(", ")", "\n", "X", "=", "np", ".", "concatenate", "(", "(", "train_X", ",", "test_X", ")", ",", "axis", "=", "0", ")", "\n", "y", "=", "np", ".", "concatenate", "(", "(", "train_y", ",", "test_y", ")", ",", "axis", "=", "0", ")", "\n", "return", "X", ",", "y", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.uea.save_data": [[82, 85], ["tensors.items", "torch.save", "str"], "function", ["None"], ["", "def", "save_data", "(", "dir", ",", "**", "tensors", ")", ":", "\n", "    ", "for", "tensor_name", ",", "tensor_value", "in", "tensors", ".", "items", "(", ")", ":", "\n", "        ", "torch", ".", "save", "(", "tensor_value", ",", "str", "(", "dir", "/", "tensor_name", ")", "+", "'.pt'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.uea.load_processed_data": [[86, 94], ["os.listdir", "filename.endswith", "torch.load", "filename.split", "str"], "function", ["None"], ["", "", "def", "load_processed_data", "(", "dir", ")", ":", "\n", "    ", "tensors", "=", "{", "}", "\n", "for", "filename", "in", "os", ".", "listdir", "(", "dir", ")", ":", "\n", "        ", "if", "filename", ".", "endswith", "(", "'.pt'", ")", ":", "\n", "            ", "tensor_name", "=", "filename", ".", "split", "(", "'.'", ")", "[", "0", "]", "\n", "tensor_value", "=", "torch", ".", "load", "(", "str", "(", "dir", "/", "filename", ")", ")", "\n", "tensors", "[", "tensor_name", "]", "=", "tensor_value", "\n", "", "", "return", "tensors", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.uea.wrap_data": [[95, 111], ["torch.utils.data.TensorDataset", "torch.utils.data.TensorDataset", "torch.utils.data.TensorDataset"], "function", ["None"], ["", "def", "wrap_data", "(", "train_X", ",", "val_X", ",", "test_X", ",", "train_y", ",", "val_y", ",", "test_y", ",", "train_final_index", ",", "val_final_index", ",", "\n", "test_final_index", ",", "\n", ")", ":", "\n", "    ", "\"\"\" Wrap data into Pytorch Dataset. \"\"\"", "\n", "\n", "train_dataset", "=", "torch", ".", "utils", ".", "data", ".", "TensorDataset", "(", "train_X", ",", "train_y", ",", "\n", "# train_final_index", "\n", ")", "\n", "val_dataset", "=", "torch", ".", "utils", ".", "data", ".", "TensorDataset", "(", "val_X", ",", "val_y", ",", "\n", "# val_final_index", "\n", ")", "\n", "test_dataset", "=", "torch", ".", "utils", ".", "data", ".", "TensorDataset", "(", "test_X", ",", "test_y", ",", "\n", "# test_final_index", "\n", ")", "\n", "\n", "return", "train_dataset", ",", "val_dataset", ",", "test_dataset", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.uea.split_data": [[112, 127], ["sklearn.model_selection.train_test_split", "sklearn.model_selection.train_test_split"], "function", ["None"], ["", "def", "split_data", "(", "tensor", ",", "stratify", ")", ":", "\n", "# 0.7/0.15/0.15 train/val/test split", "\n", "    ", "(", "train_tensor", ",", "testval_tensor", ",", "\n", "train_stratify", ",", "testval_stratify", ")", "=", "sklearn", ".", "model_selection", ".", "train_test_split", "(", "tensor", ",", "stratify", ",", "\n", "train_size", "=", "0.7", ",", "\n", "random_state", "=", "0", ",", "\n", "shuffle", "=", "True", ",", "\n", "stratify", "=", "stratify", ")", "\n", "\n", "val_tensor", ",", "test_tensor", "=", "sklearn", ".", "model_selection", ".", "train_test_split", "(", "testval_tensor", ",", "\n", "train_size", "=", "0.5", ",", "\n", "random_state", "=", "1", ",", "\n", "shuffle", "=", "True", ",", "\n", "stratify", "=", "testval_stratify", ")", "\n", "return", "train_tensor", ",", "val_tensor", ",", "test_tensor", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.uea.normalize_data": [[129, 143], ["uea.split_data", "zip", "torch.stack", "X.unbind", "train_X.unbind", "train_Xi.masked_select", "train_Xi.masked_select.mean", "train_Xi.masked_select.std", "torch.stack.append", "torch.isnan"], "function", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.bidmc.process_data.split_data"], ["", "def", "normalize_data", "(", "X", ",", "y", ")", ":", "\n", "    ", "\"\"\" Normalize data by training statistics per channel.\n\n    X: data tensor with channels as last dimension\n    \"\"\"", "\n", "train_X", ",", "_", ",", "_", "=", "split_data", "(", "X", ",", "y", ")", "\n", "out", "=", "[", "]", "\n", "for", "Xi", ",", "train_Xi", "in", "zip", "(", "X", ".", "unbind", "(", "dim", "=", "-", "1", ")", ",", "train_X", ".", "unbind", "(", "dim", "=", "-", "1", ")", ")", ":", "\n", "        ", "train_Xi_nonan", "=", "train_Xi", ".", "masked_select", "(", "~", "torch", ".", "isnan", "(", "train_Xi", ")", ")", "\n", "mean", "=", "train_Xi_nonan", ".", "mean", "(", ")", "# compute statistics using only training data.", "\n", "std", "=", "train_Xi_nonan", ".", "std", "(", ")", "\n", "out", ".", "append", "(", "(", "Xi", "-", "mean", ")", "/", "(", "std", "+", "1e-5", ")", ")", "\n", "", "out", "=", "torch", ".", "stack", "(", "out", ",", "dim", "=", "-", "1", ")", "\n", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.uea.preprocess_data": [[145, 185], ["uea.normalize_data", "augmented_X.append", "uea.split_data", "uea.split_data", "uea.split_data", "torch.cat.size", "intensity.to().cumsum.to().cumsum", "augmented_X.append", "len", "torch.cat", "torch.isnan", "intensity.to().cumsum.to"], "function", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.uea.normalize_data", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.bidmc.process_data.split_data", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.bidmc.process_data.split_data", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.bidmc.process_data.split_data"], ["", "def", "preprocess_data", "(", "\n", "X", ",", "y", ",", "\n", "final_index", ",", "\n", "# append_times,", "\n", "append_intensity", ",", "\n", ")", ":", "\n", "    ", "X", "=", "normalize_data", "(", "X", ",", "y", ")", "\n", "\n", "# Append extra channels together. Note that the order here: time, intensity, original, is important, and some models", "\n", "# depend on that order.", "\n", "augmented_X", "=", "[", "]", "\n", "# if append_times:", "\n", "#     augmented_X.append(times.unsqueeze(0).repeat(X.size(0), 1).unsqueeze(-1))", "\n", "if", "append_intensity", ":", "# Note this will append #channels copies of the same intensity", "\n", "        ", "intensity", "=", "~", "torch", ".", "isnan", "(", "X", ")", "# of size (batch, stream, channels)", "\n", "intensity", "=", "intensity", ".", "to", "(", "X", ".", "dtype", ")", ".", "cumsum", "(", "dim", "=", "1", ")", "\n", "augmented_X", ".", "append", "(", "intensity", ")", "\n", "", "augmented_X", ".", "append", "(", "X", ")", "\n", "if", "len", "(", "augmented_X", ")", "==", "1", ":", "\n", "        ", "X", "=", "augmented_X", "[", "0", "]", "\n", "", "else", ":", "\n", "        ", "X", "=", "torch", ".", "cat", "(", "augmented_X", ",", "dim", "=", "2", ")", "\n", "\n", "", "train_X", ",", "val_X", ",", "test_X", "=", "split_data", "(", "X", ",", "y", ")", "# TODO split data should just return y? or list of indices corresponding to splits", "\n", "train_y", ",", "val_y", ",", "test_y", "=", "split_data", "(", "y", ",", "y", ")", "\n", "train_final_index", ",", "val_final_index", ",", "test_final_index", "=", "split_data", "(", "final_index", ",", "y", ")", "\n", "\n", "# train_coeffs = controldiffeq.natural_cubic_spline_coeffs(times, train_X)", "\n", "# val_coeffs = controldiffeq.natural_cubic_spline_coeffs(times, val_X)", "\n", "# test_coeffs = controldiffeq.natural_cubic_spline_coeffs(times, test_X)", "\n", "\n", "in_channels", "=", "X", ".", "size", "(", "-", "1", ")", "\n", "\n", "return", "(", "\n", "# times,", "\n", "# train_coeffs, val_coeffs, test_coeffs,", "\n", "train_X", ",", "val_X", ",", "test_X", ",", "\n", "train_y", ",", "val_y", ",", "test_y", ",", "\n", "train_final_index", ",", "val_final_index", ",", "test_final_index", ",", "\n", "in_channels", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.uea.process_data": [[187, 247], ["uea.load_data", "torch.tensor", "torch.tensor.max", "torch.stack", "X.transpose.transpose", "torch.linspace", "collections.OrderedDict", "torch.tensor", "uea.preprocess_data", "torch.tensor", "torch.full", "X.transpose.size", "len", "torch.stack", "X.transpose.size", "torch.tensor.size", "uea.process_data._pad"], "function", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.sc.load_data", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.uea.preprocess_data"], ["", "def", "process_data", "(", "dataset_name", ",", "intensity", ")", ":", "\n", "# We begin by loading both the train and test data and using our own train/val/test split.", "\n", "# The reason for this is that (a) by default there is no val split and (b) the sizes of the train/test splits are", "\n", "# really janky by default. (e.g. LSST has 2459 training samples and 2466 test samples.)", "\n", "\n", "\n", "    ", "X", ",", "y", "=", "load_data", "(", "dataset_name", ")", "\n", "\n", "lengths", "=", "torch", ".", "tensor", "(", "[", "len", "(", "Xi", "[", "0", "]", ")", "for", "Xi", "in", "X", "]", ")", "\n", "final_index", "=", "lengths", "-", "1", "\n", "maxlen", "=", "lengths", ".", "max", "(", ")", "\n", "# X is now a numpy array of shape (batch, channel)", "\n", "# Each channel is a pandas.core.series.Series object of length corresponding to the length of the time series", "\n", "def", "_pad", "(", "channel", ",", "maxlen", ")", ":", "\n", "        ", "channel", "=", "torch", ".", "tensor", "(", "channel", ")", "\n", "out", "=", "torch", ".", "full", "(", "(", "maxlen", ",", ")", ",", "channel", "[", "-", "1", "]", ")", "\n", "out", "[", ":", "channel", ".", "size", "(", "0", ")", "]", "=", "channel", "\n", "return", "out", "\n", "", "X", "=", "torch", ".", "stack", "(", "[", "torch", ".", "stack", "(", "[", "_pad", "(", "channel", ",", "maxlen", ")", "for", "channel", "in", "batch", "]", ",", "dim", "=", "0", ")", "for", "batch", "in", "X", "]", ",", "dim", "=", "0", ")", "\n", "# X is now a tensor of shape (batch, channel, length)", "\n", "X", "=", "X", ".", "transpose", "(", "-", "1", ",", "-", "2", ")", "\n", "# X is now a tensor of shape (batch, length, channel)", "\n", "times", "=", "torch", ".", "linspace", "(", "0", ",", "X", ".", "size", "(", "1", ")", "-", "1", ",", "X", ".", "size", "(", "1", ")", ")", "\n", "\n", "\n", "# generator = torch.Generator().manual_seed(56789)", "\n", "# for Xi in X:", "\n", "#     removed_points = torch.randperm(X.size(1), generator=generator)[:int(X.size(1) * missing_rate)].sort().values", "\n", "#     Xi[removed_points] = float('nan')", "\n", "\n", "# Now fix the labels to be integers from 0 upwards", "\n", "targets", "=", "co", ".", "OrderedDict", "(", ")", "\n", "counter", "=", "0", "\n", "for", "yi", "in", "y", ":", "\n", "        ", "if", "yi", "not", "in", "targets", ":", "\n", "            ", "targets", "[", "yi", "]", "=", "counter", "\n", "counter", "+=", "1", "\n", "", "", "y", "=", "torch", ".", "tensor", "(", "[", "targets", "[", "yi", "]", "for", "yi", "in", "y", "]", ")", "\n", "\n", "\n", "(", "train_X", ",", "val_X", ",", "test_X", ",", "\n", "train_y", ",", "val_y", ",", "test_y", ",", "\n", "train_final_index", ",", "val_final_index", ",", "\n", "test_final_index", ",", "\n", "input_channels", ")", "=", "preprocess_data", "(", "\n", "X", ",", "y", ",", "final_index", ",", "\n", "# append_times=True,", "\n", "append_intensity", "=", "intensity", ",", "\n", ")", "\n", "\n", "num_classes", "=", "counter", "\n", "\n", "assert", "num_classes", ">=", "2", ",", "f\"Have only {num_classes} classes.\"", "\n", "\n", "return", "(", "\n", "# times,", "\n", "train_X", ",", "val_X", ",", "test_X", ",", "\n", "train_y", ",", "val_y", ",", "test_y", ",", "\n", "train_final_index", ",", "val_final_index", ",", "test_final_index", ",", "\n", "num_classes", ",", "input_channels", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.uea.get_data": [[249, 304], ["uea.load_processed_data", "int", "int", "print", "uea.download", "uea.process_data", "uea.save_data", "os.path.exists", "os.mkdir", "os.path.exists", "os.mkdir", "os.path.exists", "os.mkdir", "torch.as_tensor", "torch.as_tensor"], "function", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.uea.load_processed_data", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.sc._SpeechCommands.download", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.process_data", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.sc.save_data"], ["", "def", "get_data", "(", "\n", "dataset_name", ",", "\n", "intensity", ",", "\n", "train_hz", "=", "1", ",", "\n", "eval_hz", "=", "1", ",", "\n", "timestamp", "=", "False", ",", "\n", "train_ts", "=", "1", ",", "\n", "eval_ts", "=", "1", ",", "\n", ")", ":", "\n", "# We begin by loading both the train and test data and using our own train/val/test split.", "\n", "# The reason for this is that (a) by default there is no val split and (b) the sizes of the train/test splits are", "\n", "# really janky by default. (e.g. LSST has 2459 training samples and 2466 test samples.)", "\n", "\n", "    ", "assert", "dataset_name", "in", "valid_dataset_names", ",", "\"Must specify a valid dataset name.\"", "\n", "\n", "base_base_loc", "=", "here", "/", "'processed_data'", "\n", "base_loc", "=", "base_base_loc", "/", "'uea'", "\n", "loc", "=", "base_loc", "/", "(", "dataset_name", "+", "(", "'_intensity'", "if", "intensity", "else", "''", ")", ")", "\n", "try", ":", "\n", "        ", "tensors", "=", "load_processed_data", "(", "loc", ")", "\n", "train_X", "=", "tensors", "[", "'train_X'", "]", "\n", "val_X", "=", "tensors", "[", "'val_X'", "]", "\n", "test_X", "=", "tensors", "[", "'test_X'", "]", "\n", "train_y", "=", "tensors", "[", "'train_y'", "]", "\n", "val_y", "=", "tensors", "[", "'val_y'", "]", "\n", "test_y", "=", "tensors", "[", "'test_y'", "]", "\n", "train_final_index", "=", "tensors", "[", "'train_final_index'", "]", "\n", "val_final_index", "=", "tensors", "[", "'val_final_index'", "]", "\n", "test_final_index", "=", "tensors", "[", "'test_final_index'", "]", "\n", "num_classes", "=", "int", "(", "tensors", "[", "'num_classes'", "]", ")", "\n", "input_channels", "=", "int", "(", "tensors", "[", "'input_channels'", "]", ")", "\n", "", "except", ":", "\n", "        ", "print", "(", "f\"Could not find preprocessed data. Loading {dataset_name}...\"", ")", "\n", "download", "(", ")", "# download the UEA data if necessary", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "base_base_loc", ")", ":", "\n", "            ", "os", ".", "mkdir", "(", "base_base_loc", ")", "\n", "", "if", "not", "os", ".", "path", ".", "exists", "(", "base_loc", ")", ":", "\n", "            ", "os", ".", "mkdir", "(", "base_loc", ")", "\n", "", "if", "not", "os", ".", "path", ".", "exists", "(", "loc", ")", ":", "\n", "            ", "os", ".", "mkdir", "(", "loc", ")", "\n", "", "(", "train_X", ",", "val_X", ",", "test_X", ",", "train_y", ",", "val_y", ",", "test_y", ",", "train_final_index", ",", "val_final_index", ",", "\n", "test_final_index", ",", "num_classes", ",", "input_channels", ")", "=", "process_data", "(", "dataset_name", ",", "intensity", ")", "\n", "save_data", "(", "\n", "loc", ",", "\n", "train_X", "=", "train_X", ",", "val_X", "=", "val_X", ",", "test_X", "=", "test_X", ",", "\n", "train_y", "=", "train_y", ",", "val_y", "=", "val_y", ",", "test_y", "=", "test_y", ",", "train_final_index", "=", "train_final_index", ",", "\n", "val_final_index", "=", "val_final_index", ",", "test_final_index", "=", "test_final_index", ",", "\n", "num_classes", "=", "torch", ".", "as_tensor", "(", "num_classes", ")", ",", "input_channels", "=", "torch", ".", "as_tensor", "(", "input_channels", ")", ",", "\n", ")", "\n", "\n", "", "return", "(", "\n", "train_X", ",", "val_X", ",", "test_X", ",", "\n", "train_y", ",", "val_y", ",", "test_y", ",", "\n", "train_final_index", ",", "val_final_index", ",", "test_final_index", ",", "\n", "num_classes", ",", "input_channels", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.uea._subsample": [[307, 347], ["removed_points.to.to", "torch.Generator().manual_seed", "time_gen", "time_gen.to().unsqueeze", "X_.append", "T_.append", "torch.stack", "torch.stack", "torch.arange", "Xi.clone", "torch.zeros_like", "torch.cat", "int", "torch.Generator", "[].sort", "Xi.clone", "time_gen.to", "torch.randperm", "int"], "function", ["None"], ["", "def", "_subsample", "(", "X", ",", "hz", "=", "1", ",", "uniform", "=", "True", ",", "mask", "=", "False", ")", ":", "\n", "    ", "\"\"\" Subsample X non-uniformly at hz frequency\n\n    Input:\n    - X : (dim, length)\n\n    Returns:\n    - Subsampled X\n    - Original timestamps of the preserved data\n\n    If mask=True, then the dimensionality of X is preserved; dropped inputs are replaced by 0,\n    and an additional channel is appended indicating the positions original elements\n    \"\"\"", "\n", "L", "=", "X", ".", "shape", "[", "1", "]", "\n", "# create subsampler", "\n", "if", "uniform", ":", "\n", "        ", "removed_points", "=", "torch", ".", "arange", "(", "int", "(", "L", "*", "hz", ")", ")", "//", "hz", "\n", "removed_points", "=", "removed_points", ".", "to", "(", "int", ")", "\n", "time_gen", "=", "lambda", ":", "removed_points", "\n", "", "else", ":", "\n", "        ", "generator", "=", "torch", ".", "Generator", "(", ")", ".", "manual_seed", "(", "56789", ")", "\n", "time_gen", "=", "lambda", ":", "torch", ".", "randperm", "(", "L", ",", "generator", "=", "generator", ")", "[", ":", "int", "(", "L", "*", "hz", ")", "]", ".", "sort", "(", ")", ".", "values", "\n", "\n", "", "X_", "=", "[", "]", "\n", "T_", "=", "[", "]", "\n", "for", "Xi", "in", "X", ":", "\n", "        ", "times", "=", "time_gen", "(", ")", "\n", "if", "mask", ":", "\n", "            ", "Xi_copy", "=", "Xi", ".", "clone", "(", ")", "\n", "Xi_copy", "[", "times", "]", "=", "0.0", "\n", "Xi_", "=", "Xi", ".", "clone", "(", ")", "-", "Xi_copy", "# Keep original data at [times]", "\n", "mask_", "=", "torch", ".", "zeros_like", "(", "Xi_", "[", ":", ",", ":", "1", "]", ")", "\n", "mask_", "[", "times", "]", "=", "1.0", "\n", "Xi_", "=", "torch", ".", "cat", "(", "[", "Xi_", ",", "mask_", "]", ",", "dim", "=", "1", ")", "\n", "", "else", ":", "\n", "            ", "Xi_", "=", "Xi", "[", "times", "]", "\n", "", "times_", "=", "times", ".", "to", "(", "torch", ".", "float32", ")", ".", "unsqueeze", "(", "-", "1", ")", "\n", "X_", ".", "append", "(", "Xi_", ")", "\n", "T_", ".", "append", "(", "times_", ")", "\n", "", "return", "torch", ".", "stack", "(", "X_", ",", "dim", "=", "0", ")", ",", "torch", ".", "stack", "(", "T_", ",", "dim", "=", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.uea.postprocess_data": [[348, 397], ["uea._subsample", "uea._subsample", "uea._subsample", "uea.wrap_data", "torch.cat", "torch.cat", "torch.cat"], "function", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.uea._subsample", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.uea._subsample", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.uea._subsample", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.uea.wrap_data"], ["", "def", "postprocess_data", "(", "\n", "train_X", ",", "val_X", ",", "test_X", ",", "\n", "train_y", ",", "val_y", ",", "test_y", ",", "\n", "train_final_index", ",", "val_final_index", ",", "test_final_index", ",", "\n", "train_hz", "=", "1", ",", "\n", "eval_hz", "=", "1", ",", "\n", "train_uniform", "=", "True", ",", "\n", "eval_uniform", "=", "True", ",", "\n", "timestamp", "=", "False", ",", "\n", "train_ts", "=", "1", ",", "\n", "eval_ts", "=", "1", ",", "\n", "mask", "=", "False", ",", "\n", ")", ":", "\n", "    ", "\"\"\"\n    train_hz, eval_hz: subsampling multiplier of original data\n        e.g. train_hz=0.5 means data is sampled at half speed, so remove every other element of the sequence\n        Since the original data is sampled from a trajectory at 200Hz, this corresponds to a sampling rate of 100Hz\n    train_uniform, eval_uniform: whether subsampling is uniformly spaced or random\n    timestamp: data comes with timestamps\n    train_ts, eval_ts: timestamp multiplier\n    mask: Whether to mask the data instead of removing. Should not be simultaneously True with timestamp\n\n    Example configurations:\n    train_hz=1.0, eval_hz=0.5, {train,eval}_uniform=True, timestamp=False\n    - non-timestamped, uniformly sampled data, where evaluation sequences have every other element removed\n\n    {train,eval}_uniform=False, timestamp=True, train_ts=1.0, eval_ts=0.5\n    - timestamped, randomly sampled data, where evaluation sequences have timestamps halved\n\n    Both of the above configurations test train->evaluation generalization of halving the timescale frequency, either from the measurement sampling rate decreasing (from 200Hz -> 100hz), or the subject drawing half as fast.\n    \"\"\"", "\n", "\n", "\n", "train_X", ",", "train_T", "=", "_subsample", "(", "train_X", ",", "train_hz", ",", "train_uniform", ",", "mask", ")", "\n", "val_X", ",", "val_T", "=", "_subsample", "(", "val_X", ",", "eval_hz", ",", "eval_uniform", ",", "mask", ")", "\n", "test_X", ",", "test_T", "=", "_subsample", "(", "test_X", ",", "eval_hz", ",", "eval_uniform", ",", "mask", ")", "\n", "\n", "if", "timestamp", ":", "\n", "        ", "train_X", "=", "torch", ".", "cat", "(", "[", "train_ts", "*", "train_T", ",", "train_X", "]", ",", "dim", "=", "-", "1", ")", "\n", "val_X", "=", "torch", ".", "cat", "(", "[", "eval_ts", "*", "val_T", ",", "val_X", "]", ",", "dim", "=", "-", "1", ")", "\n", "test_X", "=", "torch", ".", "cat", "(", "[", "eval_ts", "*", "test_T", ",", "test_X", "]", ",", "dim", "=", "-", "1", ")", "\n", "\n", "", "train_dataset", ",", "val_dataset", ",", "test_dataset", "=", "wrap_data", "(", "\n", "# times,", "\n", "train_X", ",", "val_X", ",", "test_X", ",", "\n", "train_y", ",", "val_y", ",", "test_y", ",", "\n", "train_final_index", ",", "val_final_index", ",", "test_final_index", "\n", ")", "\n", "return", "train_dataset", ",", "val_dataset", ",", "test_dataset", "# , num_classes, input_channels", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.vocabulary.Vocab.__init__": [[26, 35], ["collections.Counter"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "special", "=", "[", "]", ",", "min_freq", "=", "0", ",", "max_size", "=", "None", ",", "lower_case", "=", "True", ",", "\n", "delimiter", "=", "None", ",", "vocab_file", "=", "None", ")", ":", "\n", "        ", "self", ".", "counter", "=", "Counter", "(", ")", "\n", "self", ".", "special", "=", "special", "\n", "self", ".", "min_freq", "=", "min_freq", "\n", "self", ".", "max_size", "=", "max_size", "\n", "self", ".", "lower_case", "=", "lower_case", "\n", "self", ".", "delimiter", "=", "delimiter", "\n", "self", ".", "vocab_file", "=", "vocab_file", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.vocabulary.Vocab.tokenize": [[36, 54], ["line.lower.lower.strip", "line.lower.lower.lower", "line.lower.lower.split"], "methods", ["None"], ["", "def", "tokenize", "(", "self", ",", "line", ",", "add_eos", "=", "False", ",", "add_double_eos", "=", "False", ")", ":", "\n", "        ", "line", "=", "line", ".", "strip", "(", ")", "\n", "# convert to lower case", "\n", "if", "self", ".", "lower_case", ":", "\n", "            ", "line", "=", "line", ".", "lower", "(", ")", "\n", "\n", "# empty delimiter '' will evaluate False", "\n", "", "if", "self", ".", "delimiter", "==", "''", ":", "\n", "            ", "symbols", "=", "line", "\n", "", "else", ":", "\n", "            ", "symbols", "=", "line", ".", "split", "(", "self", ".", "delimiter", ")", "\n", "\n", "", "if", "add_double_eos", ":", "# lm1b", "\n", "            ", "return", "[", "'<S>'", "]", "+", "symbols", "+", "[", "'<S>'", "]", "\n", "", "elif", "add_eos", ":", "\n", "            ", "return", "symbols", "+", "[", "'<eos>'", "]", "\n", "", "else", ":", "\n", "            ", "return", "symbols", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.vocabulary.Vocab.count_file": [[55, 70], ["os.path.exists", "print", "open", "enumerate", "vocabulary.Vocab.tokenize", "vocabulary.Vocab.counter.update", "sents.append", "print"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.vocabulary.OpenAIVocab.tokenize"], ["", "", "def", "count_file", "(", "self", ",", "path", ",", "verbose", "=", "False", ",", "add_eos", "=", "False", ")", ":", "\n", "        ", "if", "verbose", ":", "\n", "            ", "print", "(", "'counting file {} ...'", ".", "format", "(", "path", ")", ")", "\n", "", "assert", "os", ".", "path", ".", "exists", "(", "path", ")", "\n", "\n", "sents", "=", "[", "]", "\n", "with", "open", "(", "path", ",", "'r'", ",", "encoding", "=", "'utf-8'", ")", "as", "f", ":", "\n", "            ", "for", "idx", ",", "line", "in", "enumerate", "(", "f", ")", ":", "\n", "                ", "if", "verbose", "and", "idx", ">", "0", "and", "idx", "%", "500000", "==", "0", ":", "\n", "                    ", "print", "(", "'    line {}'", ".", "format", "(", "idx", ")", ")", "\n", "", "symbols", "=", "self", ".", "tokenize", "(", "line", ",", "add_eos", "=", "add_eos", ")", "\n", "self", ".", "counter", ".", "update", "(", "symbols", ")", "\n", "sents", ".", "append", "(", "symbols", ")", "\n", "\n", "", "", "return", "sents", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.vocabulary.Vocab.count_sents": [[71, 81], ["enumerate", "print", "vocabulary.Vocab.counter.update", "print", "len"], "methods", ["None"], ["", "def", "count_sents", "(", "self", ",", "sents", ",", "verbose", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n            sents : a list of sentences, each a list of tokenized symbols\n        \"\"\"", "\n", "if", "verbose", ":", "\n", "            ", "print", "(", "'counting {} sents ...'", ".", "format", "(", "len", "(", "sents", ")", ")", ")", "\n", "", "for", "idx", ",", "symbols", "in", "enumerate", "(", "sents", ")", ":", "\n", "            ", "if", "verbose", "and", "idx", ">", "0", "and", "idx", "%", "500000", "==", "0", ":", "\n", "                ", "print", "(", "'    line {}'", ".", "format", "(", "idx", ")", ")", "\n", "", "self", ".", "counter", ".", "update", "(", "symbols", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.vocabulary.Vocab._build_from_file": [[82, 91], ["collections.OrderedDict", "open", "vocabulary.Vocab.add_symbol", "line.strip().split", "line.strip"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.vocabulary.Vocab.add_symbol"], ["", "", "def", "_build_from_file", "(", "self", ",", "vocab_file", ")", ":", "\n", "        ", "self", ".", "idx2sym", "=", "[", "]", "\n", "self", ".", "sym2idx", "=", "OrderedDict", "(", ")", "\n", "\n", "with", "open", "(", "vocab_file", ",", "'r'", ",", "encoding", "=", "'utf-8'", ")", "as", "f", ":", "\n", "            ", "for", "line", "in", "f", ":", "\n", "                ", "symb", "=", "line", ".", "strip", "(", ")", ".", "split", "(", ")", "[", "0", "]", "\n", "self", ".", "add_symbol", "(", "symb", ")", "\n", "", "", "self", ".", "unk_idx", "=", "self", ".", "sym2idx", "[", "'<UNK>'", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.vocabulary.Vocab.build_vocab": [[92, 113], ["print", "vocabulary.Vocab._build_from_file", "print", "print", "collections.OrderedDict", "vocabulary.Vocab.counter.most_common", "print", "vocabulary.Vocab.add_special", "vocabulary.Vocab.add_symbol", "len", "len", "len"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.vocabulary.Vocab._build_from_file", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.vocabulary.Vocab.add_special", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.vocabulary.Vocab.add_symbol"], ["", "def", "build_vocab", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "vocab_file", ":", "\n", "            ", "print", "(", "'building vocab from {}'", ".", "format", "(", "self", ".", "vocab_file", ")", ")", "\n", "self", ".", "_build_from_file", "(", "self", ".", "vocab_file", ")", "\n", "print", "(", "'final vocab size {}'", ".", "format", "(", "len", "(", "self", ")", ")", ")", "\n", "", "else", ":", "\n", "            ", "print", "(", "'building vocab with min_freq={}, max_size={}'", ".", "format", "(", "\n", "self", ".", "min_freq", ",", "self", ".", "max_size", ")", ")", "\n", "self", ".", "idx2sym", "=", "[", "]", "\n", "self", ".", "sym2idx", "=", "OrderedDict", "(", ")", "\n", "\n", "for", "sym", "in", "self", ".", "special", ":", "\n", "                ", "self", ".", "add_special", "(", "sym", ")", "\n", "\n", "", "for", "sym", ",", "cnt", "in", "self", ".", "counter", ".", "most_common", "(", "self", ".", "max_size", ")", ":", "\n", "                ", "if", "cnt", "<", "self", ".", "min_freq", ":", "\n", "                    ", "break", "\n", "", "self", ".", "add_symbol", "(", "sym", ")", "\n", "\n", "", "print", "(", "'final vocab size {} from {} unique tokens'", ".", "format", "(", "\n", "len", "(", "self", ")", ",", "len", "(", "self", ".", "counter", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.vocabulary.Vocab.encode_file": [[114, 132], ["os.path.exists", "print", "open", "enumerate", "torch.cat", "vocabulary.Vocab.tokenize", "torch.cat.append", "print", "vocabulary.Vocab.convert_to_tensor"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.vocabulary.OpenAIVocab.tokenize", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.vocabulary.OpenAIVocab.convert_to_tensor"], ["", "", "def", "encode_file", "(", "self", ",", "path", ",", "ordered", "=", "False", ",", "verbose", "=", "False", ",", "add_eos", "=", "True", ",", "\n", "add_double_eos", "=", "False", ")", ":", "\n", "        ", "if", "verbose", ":", "\n", "            ", "print", "(", "'encoding file {} ...'", ".", "format", "(", "path", ")", ")", "\n", "", "assert", "os", ".", "path", ".", "exists", "(", "path", ")", "\n", "encoded", "=", "[", "]", "\n", "with", "open", "(", "path", ",", "'r'", ",", "encoding", "=", "'utf-8'", ")", "as", "f", ":", "\n", "            ", "for", "idx", ",", "line", "in", "enumerate", "(", "f", ")", ":", "\n", "                ", "if", "verbose", "and", "idx", ">", "0", "and", "idx", "%", "500000", "==", "0", ":", "\n", "                    ", "print", "(", "'    line {}'", ".", "format", "(", "idx", ")", ")", "\n", "", "symbols", "=", "self", ".", "tokenize", "(", "line", ",", "add_eos", "=", "add_eos", ",", "\n", "add_double_eos", "=", "add_double_eos", ")", "\n", "encoded", ".", "append", "(", "self", ".", "convert_to_tensor", "(", "symbols", ")", ")", "\n", "\n", "", "", "if", "ordered", ":", "\n", "            ", "encoded", "=", "torch", ".", "cat", "(", "encoded", ")", "\n", "\n", "", "return", "encoded", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.vocabulary.Vocab.encode_sents": [[133, 146], ["enumerate", "print", "torch.cat.append", "torch.cat", "print", "vocabulary.Vocab.convert_to_tensor", "len"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.vocabulary.OpenAIVocab.convert_to_tensor"], ["", "def", "encode_sents", "(", "self", ",", "sents", ",", "ordered", "=", "False", ",", "verbose", "=", "False", ")", ":", "\n", "        ", "if", "verbose", ":", "\n", "            ", "print", "(", "'encoding {} sents ...'", ".", "format", "(", "len", "(", "sents", ")", ")", ")", "\n", "", "encoded", "=", "[", "]", "\n", "for", "idx", ",", "symbols", "in", "enumerate", "(", "sents", ")", ":", "\n", "            ", "if", "verbose", "and", "idx", ">", "0", "and", "idx", "%", "500000", "==", "0", ":", "\n", "                ", "print", "(", "'    line {}'", ".", "format", "(", "idx", ")", ")", "\n", "", "encoded", ".", "append", "(", "self", ".", "convert_to_tensor", "(", "symbols", ")", ")", "\n", "\n", "", "if", "ordered", ":", "\n", "            ", "encoded", "=", "torch", ".", "cat", "(", "encoded", ")", "\n", "\n", "", "return", "encoded", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.vocabulary.Vocab.add_special": [[147, 152], ["vocabulary.Vocab.idx2sym.append", "setattr", "len", "sym.strip"], "methods", ["None"], ["", "def", "add_special", "(", "self", ",", "sym", ")", ":", "\n", "        ", "if", "sym", "not", "in", "self", ".", "sym2idx", ":", "\n", "            ", "self", ".", "idx2sym", ".", "append", "(", "sym", ")", "\n", "self", ".", "sym2idx", "[", "sym", "]", "=", "len", "(", "self", ".", "idx2sym", ")", "-", "1", "\n", "setattr", "(", "self", ",", "'{}_idx'", ".", "format", "(", "sym", ".", "strip", "(", "'<>'", ")", ")", ",", "self", ".", "sym2idx", "[", "sym", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.vocabulary.Vocab.add_symbol": [[153, 157], ["vocabulary.Vocab.idx2sym.append", "len"], "methods", ["None"], ["", "", "def", "add_symbol", "(", "self", ",", "sym", ")", ":", "\n", "        ", "if", "sym", "not", "in", "self", ".", "sym2idx", ":", "\n", "            ", "self", ".", "idx2sym", ".", "append", "(", "sym", ")", "\n", "self", ".", "sym2idx", "[", "sym", "]", "=", "len", "(", "self", ".", "idx2sym", ")", "-", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.vocabulary.Vocab.get_sym": [[158, 161], ["len"], "methods", ["None"], ["", "", "def", "get_sym", "(", "self", ",", "idx", ")", ":", "\n", "        ", "assert", "0", "<=", "idx", "<", "len", "(", "self", ")", ",", "'Index {} out of range'", ".", "format", "(", "idx", ")", "\n", "return", "self", ".", "idx2sym", "[", "idx", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.vocabulary.Vocab.get_idx": [[162, 170], ["hasattr", "vocabulary.Vocab.sym2idx.get"], "methods", ["None"], ["", "def", "get_idx", "(", "self", ",", "sym", ")", ":", "\n", "        ", "if", "sym", "in", "self", ".", "sym2idx", ":", "\n", "            ", "return", "self", ".", "sym2idx", "[", "sym", "]", "\n", "", "else", ":", "\n", "# print('encounter unk {}'.format(sym))", "\n", "            ", "assert", "'<eos>'", "not", "in", "sym", "\n", "assert", "hasattr", "(", "self", ",", "'unk_idx'", ")", "\n", "return", "self", ".", "sym2idx", ".", "get", "(", "sym", ",", "self", ".", "unk_idx", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.vocabulary.Vocab.get_symbols": [[171, 173], ["vocabulary.Vocab.get_sym"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.vocabulary.Vocab.get_sym"], ["", "", "def", "get_symbols", "(", "self", ",", "indices", ")", ":", "\n", "        ", "return", "[", "self", ".", "get_sym", "(", "idx", ")", "for", "idx", "in", "indices", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.vocabulary.Vocab.get_indices": [[174, 176], ["vocabulary.Vocab.get_idx"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.vocabulary.Vocab.get_idx"], ["", "def", "get_indices", "(", "self", ",", "symbols", ")", ":", "\n", "        ", "return", "[", "self", ".", "get_idx", "(", "sym", ")", "for", "sym", "in", "symbols", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.vocabulary.Vocab.convert_to_tensor": [[177, 179], ["torch.LongTensor", "vocabulary.Vocab.get_indices"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.vocabulary.Vocab.get_indices"], ["", "def", "convert_to_tensor", "(", "self", ",", "symbols", ")", ":", "\n", "        ", "return", "torch", ".", "LongTensor", "(", "self", ".", "get_indices", "(", "symbols", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.vocabulary.Vocab.convert_to_sent": [[180, 185], ["vocabulary.Vocab.get_sym", "vocabulary.Vocab.get_sym"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.vocabulary.Vocab.get_sym", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.vocabulary.Vocab.get_sym"], ["", "def", "convert_to_sent", "(", "self", ",", "indices", ",", "exclude", "=", "None", ")", ":", "\n", "        ", "if", "exclude", "is", "None", ":", "\n", "            ", "return", "' '", ".", "join", "(", "[", "self", ".", "get_sym", "(", "idx", ")", "for", "idx", "in", "indices", "]", ")", "\n", "", "else", ":", "\n", "            ", "return", "' '", ".", "join", "(", "[", "self", ".", "get_sym", "(", "idx", ")", "for", "idx", "in", "indices", "if", "idx", "not", "in", "exclude", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.vocabulary.Vocab.__len__": [[186, 188], ["len"], "methods", ["None"], ["", "", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "idx2sym", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.vocabulary.OpenAIVocab.__init__": [[193, 206], ["GPT2Tokenizer.from_pretrained", "len", "range", "vocabulary.OpenAIVocab.tokenizer.add_tokens"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "max_size", "=", "None", ",", "vocab_file", "=", "None", ")", ":", "\n", "        ", "from", "transformers", "import", "GPT2Tokenizer", "\n", "self", ".", "tokenizer", "=", "GPT2Tokenizer", ".", "from_pretrained", "(", "'gpt2'", ")", "\n", "self", ".", "EOT", "=", "self", ".", "tokenizer", ".", "encoder", "[", "'<|endoftext|>'", "]", "\n", "self", ".", "max_size", "=", "max_size", "\n", "self", ".", "vocab_file", "=", "vocab_file", "\n", "\n", "pad", "=", "8", "\n", "vocab_size", "=", "len", "(", "self", ".", "tokenizer", ")", "\n", "padded_vocab_size", "=", "(", "vocab_size", "+", "pad", "-", "1", ")", "//", "pad", "*", "pad", "\n", "for", "i", "in", "range", "(", "0", ",", "padded_vocab_size", "-", "vocab_size", ")", ":", "\n", "            ", "token", "=", "f'madeupword{i:09d}'", "\n", "self", ".", "tokenizer", ".", "add_tokens", "(", "[", "token", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.vocabulary.OpenAIVocab.__len__": [[207, 209], ["len"], "methods", ["None"], ["", "", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "tokenizer", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.vocabulary.OpenAIVocab.count_file": [[210, 213], ["None"], "methods", ["None"], ["", "def", "count_file", "(", "self", ",", "path", ",", "verbose", "=", "False", ",", "add_eos", "=", "False", ")", ":", "\n", "# TODO: train from scratch, respect self.max_size", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.vocabulary.OpenAIVocab.build_vocab": [[214, 216], ["None"], "methods", ["None"], ["", "def", "build_vocab", "(", "self", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.vocabulary.OpenAIVocab.encode_file": [[217, 232], ["os.path.exists", "print", "os.path.exists", "torch.load", "open", "open", "contextlib.redirect_stderr", "torch.LongTensor", "src.distributed.sync_workers", "vocabulary.OpenAIVocab.tokenizer.encode", "torch.save", "f.read"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.utils.distributed.sync_workers"], ["", "def", "encode_file", "(", "self", ",", "path", ",", "ordered", "=", "False", ",", "verbose", "=", "False", ",", "add_eos", "=", "True", ",", "add_double_eos", "=", "False", ")", "->", "torch", ".", "LongTensor", ":", "\n", "        ", "cached", "=", "path", "+", "'.bpe'", "\n", "if", "os", ".", "path", ".", "exists", "(", "cached", ")", ":", "\n", "            ", "return", "torch", ".", "load", "(", "cached", ")", "\n", "", "print", "(", "f'encoding file {path} ...'", ")", "\n", "assert", "os", ".", "path", ".", "exists", "(", "path", ")", ",", "f\"{path} doesn't exist\"", "\n", "\n", "with", "open", "(", "path", ",", "encoding", "=", "'utf-8'", ")", "as", "f", ":", "\n", "# Suppress warnings about length.", "\n", "            ", "with", "open", "(", "os", ".", "devnull", ",", "\"w\"", ")", "as", "devnull", ",", "contextlib", ".", "redirect_stderr", "(", "devnull", ")", ":", "\n", "                ", "out", "=", "torch", ".", "LongTensor", "(", "self", ".", "tokenizer", ".", "encode", "(", "f", ".", "read", "(", ")", ")", "+", "[", "self", ".", "EOT", "]", ")", "\n", "with", "utils", ".", "distributed", ".", "sync_workers", "(", ")", "as", "rank", ":", "\n", "                    ", "if", "rank", "==", "0", ":", "\n", "                        ", "torch", ".", "save", "(", "out", ",", "cached", ")", "\n", "", "", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.vocabulary.OpenAIVocab.tokenize": [[233, 235], ["vocabulary.OpenAIVocab.tokenizer.encode"], "methods", ["None"], ["", "", "", "def", "tokenize", "(", "self", ",", "line", ",", "add_eos", "=", "False", ",", "add_double_eos", "=", "False", ")", ":", "\n", "        ", "return", "self", ".", "tokenizer", ".", "encode", "(", "line", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.vocabulary.OpenAIVocab.convert_to_tensor": [[236, 238], ["torch.LongTensor"], "methods", ["None"], ["", "def", "convert_to_tensor", "(", "self", ",", "symbols", ")", ":", "\n", "        ", "return", "torch", ".", "LongTensor", "(", "symbols", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.sc._SpeechCommands.__init__": [[179, 255], ["pathlib.Path", "os.path.exists", "sc._SpeechCommands.load_data", "super().__init__", "pathlib.Path", "pathlib.Path", "sc._SpeechCommands.download", "sc.save_data", "y.transpose.transpose.transpose", "torch.pad", "torch.pad", "sc.subsample", "X.long().squeeze.long().squeeze.long().squeeze", "sc._SpeechCommands._process_data", "sc._SpeechCommands._process_all", "os.path.exists", "os.mkdir", "os.path.exists", "os.mkdir", "pathlib.Path", "str", "str", "X.long().squeeze.long().squeeze.long", "str"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.sc.load_data", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.sc._SpeechCommands.download", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.sc.save_data", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.sc.pad", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.sc.pad", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.sc.subsample", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.sc._SpeechCommands._process_data", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.sc._SpeechCommands._process_all"], ["def", "__init__", "(", "\n", "self", ",", "\n", "partition", ":", "str", ",", "# `train`, `val`, `test`", "\n", "length", ":", "int", ",", "# sequence length", "\n", "mfcc", ":", "bool", ",", "# whether to use MFCC features (`True`) or raw features", "\n", "sr", ":", "int", ",", "# subsampling rate: default should be 1 (no subsampling); keeps every kth sample", "\n", "dropped_rate", ":", "float", ",", "# rate at which samples are dropped, lies in [0, 100.]", "\n", "path", ":", "str", ",", "\n", "all_classes", ":", "bool", "=", "False", ",", "\n", "gen", ":", "bool", "=", "False", ",", "# whether we are doing speech generation", "\n", "discrete_input", ":", "bool", "=", "False", ",", "# whether we are using discrete inputs", "\n", ")", ":", "\n", "        ", "self", ".", "dropped_rate", "=", "dropped_rate", "\n", "self", ".", "all_classes", "=", "all_classes", "\n", "self", ".", "gen", "=", "gen", "\n", "self", ".", "discrete_input", "=", "discrete_input", "\n", "\n", "self", ".", "root", "=", "pathlib", ".", "Path", "(", "path", ")", "# pathlib.Path(\"./data\")", "\n", "base_loc", "=", "self", ".", "root", "/", "\"SpeechCommands\"", "/", "\"processed_data\"", "\n", "\n", "\n", "if", "mfcc", ":", "\n", "            ", "data_loc", "=", "base_loc", "/", "\"mfcc\"", "\n", "", "elif", "gen", ":", "\n", "            ", "data_loc", "=", "base_loc", "/", "\"gen\"", "\n", "", "else", ":", "\n", "            ", "data_loc", "=", "base_loc", "/", "\"raw\"", "\n", "\n", "if", "self", ".", "dropped_rate", "!=", "0", ":", "\n", "                ", "data_loc", "=", "pathlib", ".", "Path", "(", "\n", "str", "(", "data_loc", ")", "+", "\"_dropped{}\"", ".", "format", "(", "self", ".", "dropped_rate", ")", "\n", ")", "\n", "\n", "", "", "if", "self", ".", "all_classes", ":", "\n", "            ", "data_loc", "=", "pathlib", ".", "Path", "(", "str", "(", "data_loc", ")", "+", "\"_all_classes\"", ")", "\n", "\n", "", "if", "self", ".", "discrete_input", ":", "\n", "            ", "data_loc", "=", "pathlib", ".", "Path", "(", "str", "(", "data_loc", ")", "+", "\"_discrete\"", ")", "\n", "\n", "", "if", "os", ".", "path", ".", "exists", "(", "data_loc", ")", ":", "\n", "            ", "pass", "\n", "", "else", ":", "\n", "            ", "self", ".", "download", "(", ")", "\n", "if", "not", "self", ".", "all_classes", ":", "\n", "                ", "train_X", ",", "val_X", ",", "test_X", ",", "train_y", ",", "val_y", ",", "test_y", "=", "self", ".", "_process_data", "(", "mfcc", ")", "\n", "", "else", ":", "\n", "                ", "train_X", ",", "val_X", ",", "test_X", ",", "train_y", ",", "val_y", ",", "test_y", "=", "self", ".", "_process_all", "(", "mfcc", ")", "\n", "\n", "", "if", "not", "os", ".", "path", ".", "exists", "(", "base_loc", ")", ":", "\n", "                ", "os", ".", "mkdir", "(", "base_loc", ")", "\n", "", "if", "not", "os", ".", "path", ".", "exists", "(", "data_loc", ")", ":", "\n", "                ", "os", ".", "mkdir", "(", "data_loc", ")", "\n", "", "save_data", "(", "\n", "data_loc", ",", "\n", "train_X", "=", "train_X", ",", "\n", "val_X", "=", "val_X", ",", "\n", "test_X", "=", "test_X", ",", "\n", "train_y", "=", "train_y", ",", "\n", "val_y", "=", "val_y", ",", "\n", "test_y", "=", "test_y", ",", "\n", ")", "\n", "\n", "", "X", ",", "y", "=", "self", ".", "load_data", "(", "data_loc", ",", "partition", ")", "# (batch, length, 1)", "\n", "if", "self", ".", "gen", ":", "y", "=", "y", ".", "transpose", "(", "1", ",", "2", ")", "\n", "\n", "if", "not", "mfcc", "and", "not", "self", ".", "gen", ":", "\n", "            ", "X", "=", "F", ".", "pad", "(", "X", ",", "(", "0", ",", "0", ",", "0", ",", "length", "-", "16000", ")", ")", "\n", "\n", "# Subsample", "\n", "", "if", "not", "mfcc", ":", "\n", "            ", "X", ",", "y", "=", "subsample", "(", "X", ",", "y", ",", "sr", ")", "\n", "\n", "", "if", "self", ".", "discrete_input", ":", "\n", "            ", "X", "=", "X", ".", "long", "(", ")", ".", "squeeze", "(", ")", "\n", "\n", "", "super", "(", "_SpeechCommands", ",", "self", ")", ".", "__init__", "(", "X", ",", "y", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.sc._SpeechCommands.download": [[256, 271], ["os.path.exists", "urllib.request.urlretrieve", "os.path.exists", "os.mkdir", "os.path.exists", "os.mkdir", "tarfile.open", "f.extractall"], "methods", ["None"], ["", "def", "download", "(", "self", ")", ":", "\n", "        ", "root", "=", "self", ".", "root", "\n", "base_loc", "=", "root", "/", "\"SpeechCommands\"", "\n", "loc", "=", "base_loc", "/", "\"speech_commands.tar.gz\"", "\n", "if", "os", ".", "path", ".", "exists", "(", "loc", ")", ":", "\n", "            ", "return", "\n", "", "if", "not", "os", ".", "path", ".", "exists", "(", "root", ")", ":", "\n", "            ", "os", ".", "mkdir", "(", "root", ")", "\n", "", "if", "not", "os", ".", "path", ".", "exists", "(", "base_loc", ")", ":", "\n", "            ", "os", ".", "mkdir", "(", "base_loc", ")", "\n", "", "urllib", ".", "request", ".", "urlretrieve", "(", "\n", "\"http://download.tensorflow.org/data/speech_commands_v0.02.tar.gz\"", ",", "loc", "\n", ")", "# TODO: Add progress bar", "\n", "with", "tarfile", ".", "open", "(", "loc", ",", "\"r\"", ")", "as", "f", ":", "\n", "            ", "f", ".", "extractall", "(", "base_loc", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.sc._SpeechCommands._process_all": [[272, 397], ["torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "print", "open", "set", "open", "set", "print", "os.listdir", "train_X.transpose.transpose.unsqueeze().squeeze", "val_X.transpose.transpose.unsqueeze().squeeze", "test_X.transpose.transpose.unsqueeze().squeeze", "sc.normalize_all_data", "train_X.transpose.transpose.transpose", "val_X.transpose.transpose.transpose", "test_X.transpose.transpose.transpose", "sc.normalize_all_data", "torchaudio.load", "torch.pad", "torch.pad", "train_X.transpose.transpose.transpose", "val_X.transpose.transpose.transpose", "test_X.transpose.transpose.transpose", "line.rstrip", "line.rstrip", "str", "val_X.transpose.transpose.append", "torch.tensor.append", "torch.tensor.append", "train_X.transpose.transpose.unsqueeze", "val_X.transpose.transpose.unsqueeze", "test_X.transpose.transpose.unsqueeze", "str", "test_X.transpose.transpose.append", "torch.tensor.append", "torch.tensor.append", "train_X.transpose.transpose.append", "torch.tensor.append", "torch.tensor.append", "torchaudio.transforms.MFCC", "train_X.transpose.transpose.squeeze", "torchaudio.transforms.MFCC", "val_X.transpose.transpose.squeeze", "torchaudio.transforms.MFCC", "test_X.transpose.transpose.squeeze", "dict", "dict", "dict"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.sc.normalize_all_data", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.sc.normalize_all_data", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.sc.pad", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.sc.pad"], ["", "", "def", "_process_all", "(", "self", ",", "mfcc", ")", ":", "\n", "        ", "assert", "self", ".", "dropped_rate", "==", "0", ",", "\"Dropped rate must be 0 for all classes\"", "\n", "base_loc", "=", "self", ".", "root", "/", "\"SpeechCommands\"", "\n", "\n", "with", "open", "(", "base_loc", "/", "\"validation_list.txt\"", ",", "\"r\"", ")", "as", "f", ":", "\n", "            ", "validation_list", "=", "set", "(", "[", "line", ".", "rstrip", "(", ")", "for", "line", "in", "f", "]", ")", "\n", "\n", "", "with", "open", "(", "base_loc", "/", "\"testing_list.txt\"", ",", "\"r\"", ")", "as", "f", ":", "\n", "            ", "testing_list", "=", "set", "(", "[", "line", ".", "rstrip", "(", ")", "for", "line", "in", "f", "]", ")", "\n", "\n", "# X = torch.empty(105829, 16000, 1)", "\n", "# y = torch.empty(105829, dtype=torch.long)", "\n", "", "train_X", ",", "val_X", ",", "test_X", "=", "[", "]", ",", "[", "]", ",", "[", "]", "\n", "train_y", ",", "val_y", ",", "test_y", "=", "[", "]", ",", "[", "]", ",", "[", "]", "\n", "\n", "# val_indices = []", "\n", "# test_indices = []", "\n", "\n", "batch_index", "=", "0", "\n", "y_index", "=", "0", "\n", "for", "foldername", "in", "self", ".", "ALL_CLASSES", ":", "\n", "            ", "print", "(", "foldername", ")", "\n", "loc", "=", "base_loc", "/", "foldername", "\n", "for", "filename", "in", "os", ".", "listdir", "(", "loc", ")", ":", "\n", "                ", "audio", ",", "_", "=", "torchaudio", ".", "load", "(", "\n", "loc", "/", "filename", ",", "channels_first", "=", "False", ",", "\n", ")", "\n", "audio", "=", "(", "\n", "audio", "/", "2", "**", "15", "\n", ")", "\n", "# Pad", "\n", "audio", "=", "F", ".", "pad", "(", "audio", ",", "(", "0", ",", "0", ",", "0", ",", "16000", "-", "audio", ".", "shape", "[", "0", "]", ")", ")", "\n", "# # A few samples are shorter than the full length; for simplicity we discard them.", "\n", "# if len(audio) != 16000:", "\n", "#     continue", "\n", "# breakpoint()", "\n", "\n", "if", "str", "(", "foldername", "+", "'/'", "+", "filename", ")", "in", "validation_list", ":", "\n", "                    ", "val_X", ".", "append", "(", "audio", ")", "\n", "val_y", ".", "append", "(", "y_index", ")", "\n", "", "elif", "str", "(", "foldername", "+", "'/'", "+", "filename", ")", "in", "testing_list", ":", "\n", "                    ", "test_X", ".", "append", "(", "audio", ")", "\n", "test_y", ".", "append", "(", "y_index", ")", "\n", "", "else", ":", "\n", "                    ", "train_X", ".", "append", "(", "audio", ")", "\n", "train_y", ".", "append", "(", "y_index", ")", "\n", "\n", "# X[batch_index] = audio", "\n", "# y[batch_index] = y_index", "\n", "\n", "# if filename in validation_list:", "\n", "#     val_indices.append(1)", "\n", "# else:", "\n", "#     val_indices.append(0)", "\n", "# if filename in testing_list:", "\n", "#     test_indices.append(1)", "\n", "# else:", "\n", "#     test_indices.append(0)", "\n", "\n", "", "batch_index", "+=", "1", "\n", "", "y_index", "+=", "1", "\n", "# print(\"Full data: {} samples\".format(len(X)))", "\n", "", "train_X", "=", "torch", ".", "stack", "(", "train_X", ")", "\n", "val_X", "=", "torch", ".", "stack", "(", "val_X", ")", "\n", "test_X", "=", "torch", ".", "stack", "(", "test_X", ")", "\n", "train_y", "=", "torch", ".", "tensor", "(", "train_y", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "val_y", "=", "torch", ".", "tensor", "(", "val_y", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "test_y", "=", "torch", ".", "tensor", "(", "test_y", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "\n", "# y = torch.tensor(y, dtype=torch.long)", "\n", "# val_indices = torch.tensor(val_indices, dtype=torch.bool)", "\n", "# test_indices = torch.tensor(test_indices, dtype=torch.bool)", "\n", "\n", "# If MFCC, then we compute these coefficients.", "\n", "if", "mfcc", ":", "\n", "            ", "train_X", "=", "torchaudio", ".", "transforms", ".", "MFCC", "(", "\n", "log_mels", "=", "True", ",", "n_mfcc", "=", "20", ",", "melkwargs", "=", "dict", "(", "n_fft", "=", "200", ",", "n_mels", "=", "64", ")", "\n", ")", "(", "train_X", ".", "squeeze", "(", "-", "1", ")", ")", ".", "detach", "(", ")", "\n", "\n", "val_X", "=", "torchaudio", ".", "transforms", ".", "MFCC", "(", "\n", "log_mels", "=", "True", ",", "n_mfcc", "=", "20", ",", "melkwargs", "=", "dict", "(", "n_fft", "=", "200", ",", "n_mels", "=", "64", ")", "\n", ")", "(", "val_X", ".", "squeeze", "(", "-", "1", ")", ")", ".", "detach", "(", ")", "\n", "\n", "test_X", "=", "torchaudio", ".", "transforms", ".", "MFCC", "(", "\n", "log_mels", "=", "True", ",", "n_mfcc", "=", "20", ",", "melkwargs", "=", "dict", "(", "n_fft", "=", "200", ",", "n_mels", "=", "64", ")", "\n", ")", "(", "test_X", ".", "squeeze", "(", "-", "1", ")", ")", ".", "detach", "(", ")", "\n", "# X is of shape (batch, channels=20, length=161)", "\n", "", "else", ":", "\n", "            ", "train_X", "=", "train_X", ".", "unsqueeze", "(", "1", ")", ".", "squeeze", "(", "-", "1", ")", "\n", "val_X", "=", "val_X", ".", "unsqueeze", "(", "1", ")", ".", "squeeze", "(", "-", "1", ")", "\n", "test_X", "=", "test_X", ".", "unsqueeze", "(", "1", ")", ".", "squeeze", "(", "-", "1", ")", "\n", "\n", "# X is of shape (batch, channels=1, length=16000)", "\n", "\n", "# breakpoint()", "\n", "# train_X = torch.masked_select(X, ~(val_indices + test_indices))", "\n", "# val_X = torch.masked_select(X, val_indices)", "\n", "# test_X = torch.masked_select(X, test_indices)", "\n", "# train_y = torch.masked_select(y, ~(val_indices + test_indices))", "\n", "# val_y = torch.masked_select(y, val_indices)", "\n", "# test_y = torch.masked_select(y, test_indices)", "\n", "\n", "# Normalize data", "\n", "", "if", "mfcc", ":", "\n", "            ", "train_X", ",", "val_X", ",", "test_X", "=", "normalize_all_data", "(", "train_X", ".", "transpose", "(", "1", ",", "2", ")", ",", "val_X", ".", "transpose", "(", "1", ",", "2", ")", ",", "test_X", ".", "transpose", "(", "1", ",", "2", ")", ")", "\n", "train_X", "=", "train_X", ".", "transpose", "(", "1", ",", "2", ")", "\n", "val_X", "=", "val_X", ".", "transpose", "(", "1", ",", "2", ")", "\n", "test_X", "=", "test_X", ".", "transpose", "(", "1", ",", "2", ")", "\n", "", "else", ":", "\n", "            ", "train_X", ",", "val_X", ",", "test_X", "=", "normalize_all_data", "(", "train_X", ",", "val_X", ",", "test_X", ")", "\n", "\n", "# Print the shape of all tensors in one line", "\n", "", "print", "(", "\n", "\"Train: {}, Val: {}, Test: {}\"", ".", "format", "(", "\n", "train_X", ".", "shape", ",", "val_X", ".", "shape", ",", "test_X", ".", "shape", "\n", ")", "\n", ")", "\n", "\n", "return", "(", "\n", "train_X", ",", "\n", "val_X", ",", "\n", "test_X", ",", "\n", "train_y", ",", "\n", "val_y", ",", "\n", "test_y", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.sc._SpeechCommands._process_data": [[400, 503], ["sc.split_data", "sc.split_data", "torch.empty", "torch.empty", "torch.empty", "torch.empty", "torch.empty", "torch.empty", "torch.empty", "torch.empty", "torch.empty", "torch.empty", "torch.empty", "torch.empty", "torch.empty", "torch.empty", "torch.empty", "torch.empty", "os.listdir", "torch.cat.unsqueeze().squeeze", "torch.cat.unsqueeze().squeeze", "torch.Generator().manual_seed", "torch.Generator().manual_seed", "torch.Generator().manual_seed", "torch.Generator().manual_seed", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "normalise_data().transpose", "sc.normalise_data", "torch.where", "torch.where", "torch.where", "torch.where", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torchaudio.load", "Xi.clone", "float", "X_removed.append", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "sc.mu_law_encode", "sc.mu_law_encode", "sc.mu_law_encode", "len", "torch.cat.unsqueeze", "torch.cat.unsqueeze", "torch.Generator", "torch.Generator", "torch.Generator", "torch.Generator", "[].sort", "sc.normalise_data", "torch.isnan", "torch.isnan", "torch.isnan", "torch.isnan", "torch.roll", "torch.roll", "torch.roll", "torch.roll", "torch.roll", "torch.roll", "torch.roll", "torch.roll", "torch.roll", "torch.roll", "torch.roll", "torch.roll", "torch.roll", "torch.roll", "torch.roll", "torch.roll", "torch.roll", "torch.roll", "torch.roll", "torch.roll", "torch.roll", "torch.roll", "torch.roll", "torch.roll", "torchaudio.transforms.MFCC", "torch.cat.squeeze", "torch.cat.squeeze", "torch.cat.transpose", "torch.cat.transpose", "torch.isnan", "torch.isnan", "torch.isnan", "torch.isnan", "sc.mu_law_decode", "sc.mu_law_decode", "sc.mu_law_decode", "dict", "torch.randperm", "torch.randperm", "torch.randperm", "torch.randperm", "int", "float"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.bidmc.process_data.split_data", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.bidmc.process_data.split_data", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.sc.normalise_data", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.sc.mu_law_encode", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.sc.mu_law_encode", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.sc.mu_law_encode", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.sc.normalise_data", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.copying.CopyOrderedIterator.roll", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.copying.CopyOrderedIterator.roll", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.copying.CopyOrderedIterator.roll", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.copying.CopyOrderedIterator.roll", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.copying.CopyOrderedIterator.roll", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.copying.CopyOrderedIterator.roll", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.copying.CopyOrderedIterator.roll", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.copying.CopyOrderedIterator.roll", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.copying.CopyOrderedIterator.roll", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.copying.CopyOrderedIterator.roll", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.copying.CopyOrderedIterator.roll", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.copying.CopyOrderedIterator.roll", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.copying.CopyOrderedIterator.roll", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.copying.CopyOrderedIterator.roll", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.copying.CopyOrderedIterator.roll", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.copying.CopyOrderedIterator.roll", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.copying.CopyOrderedIterator.roll", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.copying.CopyOrderedIterator.roll", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.copying.CopyOrderedIterator.roll", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.copying.CopyOrderedIterator.roll", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.copying.CopyOrderedIterator.roll", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.copying.CopyOrderedIterator.roll", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.copying.CopyOrderedIterator.roll", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.copying.CopyOrderedIterator.roll", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.sc.mu_law_decode", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.sc.mu_law_decode", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.sc.mu_law_decode"], ["", "def", "_process_data", "(", "self", ",", "mfcc", ")", ":", "\n", "        ", "base_loc", "=", "self", ".", "root", "/", "\"SpeechCommands\"", "\n", "if", "self", ".", "gen", ":", "\n", "            ", "X", "=", "torch", ".", "empty", "(", "35628", ",", "16000", ",", "1", ")", "\n", "y", "=", "torch", ".", "empty", "(", "35628", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "", "else", ":", "\n", "            ", "X", "=", "torch", ".", "empty", "(", "34975", ",", "16000", ",", "1", ")", "\n", "y", "=", "torch", ".", "empty", "(", "34975", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "\n", "", "batch_index", "=", "0", "\n", "y_index", "=", "0", "\n", "for", "foldername", "in", "self", ".", "SUBSET_CLASSES", ":", "\n", "            ", "loc", "=", "base_loc", "/", "foldername", "\n", "for", "filename", "in", "os", ".", "listdir", "(", "loc", ")", ":", "\n", "                ", "audio", ",", "_", "=", "torchaudio", ".", "load", "(", "\n", "loc", "/", "filename", ",", "channels_first", "=", "False", ",", "\n", ")", "\n", "# audio, _ = torchaudio.load_wav(", "\n", "#     loc / filename, channels_first=False, normalization=False", "\n", "# )  # for forward compatbility if they fix it", "\n", "audio", "=", "(", "\n", "audio", "/", "2", "**", "15", "\n", ")", "# Normalization argument doesn't seem to work so we do it manually.", "\n", "\n", "# A few samples are shorter than the full length; for simplicity we discard them.", "\n", "if", "len", "(", "audio", ")", "!=", "16000", ":", "\n", "                    ", "continue", "\n", "\n", "", "X", "[", "batch_index", "]", "=", "audio", "\n", "y", "[", "batch_index", "]", "=", "y_index", "\n", "batch_index", "+=", "1", "\n", "", "y_index", "+=", "1", "\n", "", "if", "self", ".", "gen", ":", "\n", "            ", "assert", "batch_index", "==", "35628", ",", "\"batch_index is {}\"", ".", "format", "(", "batch_index", ")", "\n", "", "else", ":", "\n", "            ", "assert", "batch_index", "==", "34975", ",", "\"batch_index is {}\"", ".", "format", "(", "batch_index", ")", "\n", "\n", "# If MFCC, then we compute these coefficients.", "\n", "", "if", "mfcc", ":", "\n", "            ", "X", "=", "torchaudio", ".", "transforms", ".", "MFCC", "(", "\n", "log_mels", "=", "True", ",", "n_mfcc", "=", "20", ",", "melkwargs", "=", "dict", "(", "n_fft", "=", "200", ",", "n_mels", "=", "64", ")", "\n", ")", "(", "X", ".", "squeeze", "(", "-", "1", ")", ")", ".", "detach", "(", ")", "\n", "# X is of shape (batch=34975, channels=20, length=161)", "\n", "", "else", ":", "\n", "            ", "X", "=", "X", ".", "unsqueeze", "(", "1", ")", ".", "squeeze", "(", "-", "1", ")", "\n", "# X is of shape (batch=34975, channels=1, length=16000)", "\n", "\n", "# If dropped is different than zero, randomly drop that quantity of data from the dataset.", "\n", "", "if", "self", ".", "dropped_rate", "!=", "0", ":", "\n", "            ", "generator", "=", "torch", ".", "Generator", "(", ")", ".", "manual_seed", "(", "56789", ")", "\n", "X_removed", "=", "[", "]", "\n", "for", "Xi", "in", "X", ":", "\n", "                ", "removed_points", "=", "(", "\n", "torch", ".", "randperm", "(", "X", ".", "shape", "[", "-", "1", "]", ",", "generator", "=", "generator", ")", "[", "\n", ":", "int", "(", "X", ".", "shape", "[", "-", "1", "]", "*", "float", "(", "self", ".", "dropped_rate", ")", "/", "100.0", ")", "\n", "]", "\n", ".", "sort", "(", ")", "\n", ".", "values", "\n", ")", "\n", "Xi_removed", "=", "Xi", ".", "clone", "(", ")", "\n", "Xi_removed", "[", ":", ",", "removed_points", "]", "=", "float", "(", "\"nan\"", ")", "\n", "X_removed", ".", "append", "(", "Xi_removed", ")", "\n", "", "X", "=", "torch", ".", "stack", "(", "X_removed", ",", "dim", "=", "0", ")", "\n", "\n", "# Normalize data", "\n", "", "if", "mfcc", ":", "\n", "            ", "X", "=", "normalise_data", "(", "X", ".", "transpose", "(", "1", ",", "2", ")", ",", "y", ")", ".", "transpose", "(", "1", ",", "2", ")", "\n", "", "else", ":", "\n", "            ", "X", "=", "normalise_data", "(", "X", ",", "y", ")", "\n", "\n", "# Once the data is normalized append times and mask values if required.", "\n", "", "if", "self", ".", "dropped_rate", "!=", "0", ":", "\n", "# Get mask of possitions that are deleted", "\n", "            ", "mask_exists", "=", "(", "~", "torch", ".", "isnan", "(", "X", "[", ":", ",", ":", "1", ",", ":", "]", ")", ")", ".", "float", "(", ")", "\n", "X", "=", "torch", ".", "where", "(", "~", "torch", ".", "isnan", "(", "X", ")", ",", "X", ",", "torch", ".", "Tensor", "(", "[", "0.0", "]", ")", ")", "\n", "X", "=", "torch", ".", "cat", "(", "[", "X", ",", "mask_exists", "]", ",", "dim", "=", "1", ")", "\n", "\n", "", "train_X", ",", "val_X", ",", "test_X", "=", "split_data", "(", "X", ",", "y", ")", "\n", "train_y", ",", "val_y", ",", "test_y", "=", "split_data", "(", "y", ",", "y", ")", "\n", "\n", "if", "self", ".", "gen", ":", "\n", "            ", "train_y", ",", "val_y", ",", "test_y", "=", "train_X", ",", "val_X", ",", "test_X", "\n", "train_y", ",", "val_y", ",", "test_y", "=", "mu_law_encode", "(", "train_y", ")", ",", "mu_law_encode", "(", "val_y", ")", ",", "mu_law_encode", "(", "test_y", ")", "\n", "# train_X, val_X, test_X = train_X[..., :-1], val_X[..., :-1], test_X[..., :-1]", "\n", "# # Prepend zero to train_X, val_X, test_X", "\n", "# train_X = torch.cat([torch.zeros(train_X.shape[0], 1, train_X.shape[2]), train_X], dim=1)", "\n", "\n", "# train_X, val_X, test_X = torch.roll(train_X, 1, 2), torch.roll(val_X, 1, 2), torch.roll(test_X, 1, 2)", "\n", "if", "not", "self", ".", "discrete_input", ":", "\n", "                ", "train_X", ",", "val_X", ",", "test_X", "=", "torch", ".", "roll", "(", "mu_law_decode", "(", "train_y", ")", ",", "1", ",", "2", ")", ",", "torch", ".", "roll", "(", "mu_law_decode", "(", "val_y", ")", ",", "1", ",", "2", ")", ",", "torch", ".", "roll", "(", "mu_law_decode", "(", "test_y", ")", ",", "1", ",", "2", ")", "\n", "", "else", ":", "\n", "                ", "train_X", ",", "val_X", ",", "test_X", "=", "torch", ".", "roll", "(", "train_y", ",", "1", ",", "2", ")", ",", "torch", ".", "roll", "(", "val_y", ",", "1", ",", "2", ")", ",", "torch", ".", "roll", "(", "test_y", ",", "1", ",", "2", ")", "\n", "", "train_X", "[", "...", ",", "0", "]", ",", "val_X", "[", "...", ",", "0", "]", ",", "test_X", "[", "...", ",", "0", "]", "=", "0", ",", "0", ",", "0", "\n", "\n", "assert", "(", "train_y", ".", "shape", "==", "train_X", ".", "shape", ")", "\n", "\n", "", "return", "(", "\n", "train_X", ",", "\n", "val_X", ",", "\n", "test_X", ",", "\n", "train_y", ",", "\n", "val_y", ",", "\n", "test_y", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.sc._SpeechCommands.load_data": [[505, 522], ["sc._SpeechCommands.load_data"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.sc.load_data"], ["", "@", "staticmethod", "\n", "def", "load_data", "(", "data_loc", ",", "partition", ")", ":", "\n", "\n", "        ", "tensors", "=", "load_data", "(", "data_loc", ")", "\n", "if", "partition", "==", "\"train\"", ":", "\n", "            ", "X", "=", "tensors", "[", "\"train_X\"", "]", "\n", "y", "=", "tensors", "[", "\"train_y\"", "]", "\n", "", "elif", "partition", "==", "\"val\"", ":", "\n", "            ", "X", "=", "tensors", "[", "\"val_X\"", "]", "\n", "y", "=", "tensors", "[", "\"val_y\"", "]", "\n", "", "elif", "partition", "==", "\"test\"", ":", "\n", "            ", "X", "=", "tensors", "[", "\"test_X\"", "]", "\n", "y", "=", "tensors", "[", "\"test_y\"", "]", "\n", "", "else", ":", "\n", "            ", "raise", "NotImplementedError", "(", "\"the set {} is not implemented.\"", ".", "format", "(", "set", ")", ")", "\n", "\n", "", "return", "X", ".", "transpose", "(", "1", ",", "2", ")", ",", "y", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.sc._SpeechCommandsGeneration.__init__": [[537, 558], ["sc._SpeechCommands.__init__"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "partition", ":", "str", ",", "# `train`, `val`, `test`", "\n", "length", ":", "int", ",", "# sequence length", "\n", "mfcc", ":", "bool", ",", "# whether to use MFCC features (`True`) or raw features", "\n", "sr", ":", "int", ",", "# subsampling rate: default should be 1 (no subsampling); keeps every kth sample", "\n", "dropped_rate", ":", "float", ",", "# rate at which samples are dropped, lies in [0, 100.]", "\n", "path", ":", "str", ",", "\n", "all_classes", ":", "bool", "=", "False", ",", "\n", "discrete_input", ":", "bool", "=", "False", ",", "\n", ")", ":", "\n", "        ", "super", "(", "_SpeechCommandsGeneration", ",", "self", ")", ".", "__init__", "(", "\n", "partition", "=", "partition", ",", "\n", "length", "=", "length", ",", "\n", "mfcc", "=", "mfcc", ",", "\n", "sr", "=", "sr", ",", "\n", "dropped_rate", "=", "dropped_rate", ",", "\n", "path", "=", "path", ",", "\n", "all_classes", "=", "all_classes", ",", "\n", "gen", "=", "True", ",", "\n", "discrete_input", "=", "discrete_input", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.sc.pad": [[17, 22], ["torch.tensor", "torch.tensor", "torch.full", "torch.full", "torch.tensor.size"], "function", ["None"], ["def", "pad", "(", "channel", ",", "maxlen", ")", ":", "\n", "    ", "channel", "=", "torch", ".", "tensor", "(", "channel", ")", "\n", "out", "=", "torch", ".", "full", "(", "(", "maxlen", ",", ")", ",", "channel", "[", "-", "1", "]", ")", "\n", "out", "[", ":", "channel", ".", "size", "(", "0", ")", "]", "=", "channel", "\n", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.sc.subsample": [[24, 28], ["None"], "function", ["None"], ["", "def", "subsample", "(", "X", ",", "y", ",", "subsample_rate", ")", ":", "\n", "    ", "if", "subsample_rate", "!=", "1", ":", "\n", "        ", "X", "=", "X", "[", ":", ",", ":", ":", "subsample_rate", ",", ":", "]", "\n", "", "return", "X", ",", "y", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.sc.save_data": [[30, 33], ["tensors.items", "torch.save", "torch.save", "str"], "function", ["None"], ["", "def", "save_data", "(", "dir", ",", "**", "tensors", ")", ":", "\n", "    ", "for", "tensor_name", ",", "tensor_value", "in", "tensors", ".", "items", "(", ")", ":", "\n", "        ", "torch", ".", "save", "(", "tensor_value", ",", "str", "(", "dir", "/", "tensor_name", ")", "+", "\".pt\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.sc.load_data": [[35, 43], ["os.listdir", "filename.endswith", "torch.load", "torch.load", "filename.split", "str"], "function", ["None"], ["", "", "def", "load_data", "(", "dir", ")", ":", "\n", "    ", "tensors", "=", "{", "}", "\n", "for", "filename", "in", "os", ".", "listdir", "(", "dir", ")", ":", "\n", "        ", "if", "filename", ".", "endswith", "(", "\".pt\"", ")", ":", "\n", "            ", "tensor_name", "=", "filename", ".", "split", "(", "\".\"", ")", "[", "0", "]", "\n", "tensor_value", "=", "torch", ".", "load", "(", "str", "(", "dir", "/", "filename", ")", ")", "\n", "tensors", "[", "tensor_name", "]", "=", "tensor_value", "\n", "", "", "return", "tensors", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.sc.normalise_data": [[45, 55], ["sc.split_data", "zip", "torch.stack", "torch.stack", "X.unbind", "train_X.unbind", "train_Xi.masked_select", "train_Xi.masked_select.mean", "train_Xi.masked_select.std", "torch.stack.append", "torch.isnan", "torch.isnan"], "function", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.bidmc.process_data.split_data"], ["", "def", "normalise_data", "(", "X", ",", "y", ")", ":", "\n", "    ", "train_X", ",", "_", ",", "_", "=", "split_data", "(", "X", ",", "y", ")", "\n", "out", "=", "[", "]", "\n", "for", "Xi", ",", "train_Xi", "in", "zip", "(", "X", ".", "unbind", "(", "dim", "=", "-", "1", ")", ",", "train_X", ".", "unbind", "(", "dim", "=", "-", "1", ")", ")", ":", "\n", "        ", "train_Xi_nonan", "=", "train_Xi", ".", "masked_select", "(", "~", "torch", ".", "isnan", "(", "train_Xi", ")", ")", "\n", "mean", "=", "train_Xi_nonan", ".", "mean", "(", ")", "# compute statistics using only training data.", "\n", "std", "=", "train_Xi_nonan", ".", "std", "(", ")", "\n", "out", ".", "append", "(", "(", "Xi", "-", "mean", ")", "/", "(", "std", "+", "1e-5", ")", ")", "\n", "", "out", "=", "torch", ".", "stack", "(", "out", ",", "dim", "=", "-", "1", ")", "\n", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.sc.normalize_all_data": [[56, 66], ["range", "X_train[].mean", "X_train[].std"], "function", ["None"], ["", "def", "normalize_all_data", "(", "X_train", ",", "X_val", ",", "X_test", ")", ":", "\n", "\n", "    ", "for", "i", "in", "range", "(", "X_train", ".", "shape", "[", "-", "1", "]", ")", ":", "\n", "        ", "mean", "=", "X_train", "[", ":", ",", ":", ",", "i", "]", ".", "mean", "(", ")", "\n", "std", "=", "X_train", "[", ":", ",", ":", ",", "i", "]", ".", "std", "(", ")", "\n", "X_train", "[", ":", ",", ":", ",", "i", "]", "=", "(", "X_train", "[", ":", ",", ":", ",", "i", "]", "-", "mean", ")", "/", "(", "std", "+", "1e-5", ")", "\n", "X_val", "[", ":", ",", ":", ",", "i", "]", "=", "(", "X_val", "[", ":", ",", ":", ",", "i", "]", "-", "mean", ")", "/", "(", "std", "+", "1e-5", ")", "\n", "X_test", "[", ":", ",", ":", ",", "i", "]", "=", "(", "X_test", "[", ":", ",", ":", ",", "i", "]", "-", "mean", ")", "/", "(", "std", "+", "1e-5", ")", "\n", "\n", "", "return", "X_train", ",", "X_val", ",", "X_test", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.sc.minmax_scale": [[67, 71], ["torch.amin", "torch.amin", "torch.amax", "torch.amax"], "function", ["None"], ["", "def", "minmax_scale", "(", "tensor", ")", ":", "\n", "    ", "min_val", "=", "torch", ".", "amin", "(", "tensor", ",", "dim", "=", "(", "1", ",", "2", ")", ",", "keepdim", "=", "True", ")", "\n", "max_val", "=", "torch", ".", "amax", "(", "tensor", ",", "dim", "=", "(", "1", ",", "2", ")", ",", "keepdim", "=", "True", ")", "\n", "return", "(", "tensor", "-", "min_val", ")", "/", "(", "max_val", "-", "min_val", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.sc.mu_law_encode": [[72, 88], ["torch.tensor", "torch.tensor", "torch.log1p", "torch.log1p", "torch.log1p", "torch.log1p", "torch.sign", "torch.sign", "sc.minmax_scale", "torch.abs", "torch.abs"], "function", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.sc.minmax_scale"], ["", "def", "mu_law_encode", "(", "audio", ",", "bits", "=", "8", ")", ":", "\n", "    ", "\"\"\"\n    Perform mu-law companding transformation.\n    \"\"\"", "\n", "mu", "=", "torch", ".", "tensor", "(", "2", "**", "bits", "-", "1", ")", "\n", "\n", "# Audio must be min-max scaled between -1 and 1", "\n", "audio", "=", "2", "*", "minmax_scale", "(", "audio", ")", "-", "1", "\n", "\n", "# Perform mu-law companding transformation.", "\n", "numerator", "=", "torch", ".", "log1p", "(", "mu", "*", "torch", ".", "abs", "(", "audio", ")", ")", "\n", "denominator", "=", "torch", ".", "log1p", "(", "mu", ")", "\n", "encoded", "=", "torch", ".", "sign", "(", "audio", ")", "*", "(", "numerator", "/", "denominator", ")", "\n", "\n", "# Quantize signal to the specified number of levels.", "\n", "return", "(", "(", "encoded", "+", "1", ")", "/", "2", "*", "mu", "+", "0.5", ")", ".", "to", "(", "torch", ".", "int32", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.sc.mu_law_decode": [[89, 100], ["torch.sign", "torch.sign", "torch.abs", "torch.abs"], "function", ["None"], ["", "def", "mu_law_decode", "(", "encoded", ",", "bits", "=", "8", ")", ":", "\n", "    ", "\"\"\"\n    Perform inverse mu-law transformation.\n    \"\"\"", "\n", "mu", "=", "2", "**", "bits", "-", "1", "\n", "# Invert the quantization", "\n", "x", "=", "(", "encoded", "/", "mu", ")", "*", "2", "-", "1", "\n", "\n", "# Invert the mu-law transformation", "\n", "x", "=", "torch", ".", "sign", "(", "x", ")", "*", "(", "(", "1", "+", "mu", ")", "**", "(", "torch", ".", "abs", "(", "x", ")", ")", "-", "1", ")", "/", "mu", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.sc.split_data": [[101, 125], ["sklearn.model_selection.train_test_split", "sklearn.model_selection.train_test_split"], "function", ["None"], ["", "def", "split_data", "(", "tensor", ",", "stratify", ")", ":", "\n", "# 0.7/0.15/0.15 train/val/test split", "\n", "    ", "(", "\n", "train_tensor", ",", "\n", "testval_tensor", ",", "\n", "train_stratify", ",", "\n", "testval_stratify", ",", "\n", ")", "=", "sklearn", ".", "model_selection", ".", "train_test_split", "(", "\n", "tensor", ",", "\n", "stratify", ",", "\n", "train_size", "=", "0.7", ",", "\n", "random_state", "=", "0", ",", "\n", "shuffle", "=", "True", ",", "\n", "stratify", "=", "stratify", ",", "\n", ")", "\n", "\n", "val_tensor", ",", "test_tensor", "=", "sklearn", ".", "model_selection", ".", "train_test_split", "(", "\n", "testval_tensor", ",", "\n", "train_size", "=", "0.5", ",", "\n", "random_state", "=", "1", ",", "\n", "shuffle", "=", "True", ",", "\n", "stratify", "=", "testval_stratify", ",", "\n", ")", "\n", "return", "train_tensor", ",", "val_tensor", ",", "test_tensor", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.copying.CopyingTrainDataset.__init__": [[51, 62], ["super().__init__"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__"], ["    ", "def", "__init__", "(", "self", ",", "L", ",", "M", ",", "A", ",", "samples_per_epoch", "=", "-", "1", ")", ":", "\n", "        ", "\"\"\"\n        L: number of noise tokens\n        M: number of memorization tokens\n        A: size of dictionary\n        \"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "L", "=", "L", "\n", "self", ".", "M", "=", "M", "\n", "self", ".", "A", "=", "A", "\n", "self", ".", "samples_per_epoch", "=", "samples_per_epoch", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.copying.CopyingTrainDataset.__iter__": [[63, 72], ["range", "copying.torch_copying_data", "copying.torch_copying_data"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.copying.torch_copying_data", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.copying.torch_copying_data"], ["", "def", "__iter__", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "samples_per_epoch", "<", "0", ":", "\n", "            ", "while", "True", ":", "\n", "                ", "x", ",", "y", "=", "torch_copying_data", "(", "self", ".", "L", ",", "self", ".", "M", ",", "self", ".", "A", ")", "\n", "yield", "x", ",", "y", "\n", "", "", "else", ":", "\n", "            ", "for", "_", "in", "range", "(", "self", ".", "samples_per_epoch", ")", ":", "\n", "                ", "x", ",", "y", "=", "torch_copying_data", "(", "self", ".", "L", ",", "self", ".", "M", ",", "self", ".", "A", ")", "\n", "yield", "x", ",", "y", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.copying.CopyingEvalDataset.__init__": [[75, 82], ["copying.torch_copying_data", "super().__init__"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.copying.torch_copying_data", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__"], ["    ", "def", "__init__", "(", "self", ",", "L", ",", "M", ",", "A", ",", "samples", ")", ":", "\n", "        ", "self", ".", "L", "=", "L", "\n", "self", ".", "M", "=", "M", "\n", "self", ".", "A", "=", "A", "\n", "self", ".", "samples", "=", "samples", "\n", "all_x", ",", "all_y", "=", "torch_copying_data", "(", "self", ".", "L", ",", "self", ".", "M", ",", "self", ".", "A", ",", "batch_shape", "=", "(", "self", ".", "samples", ",", ")", ")", "\n", "super", "(", ")", ".", "__init__", "(", "all_x", ",", "all_y", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.copying.CopyOrderedIterator.__init__": [[91, 124], ["torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "src.utils.distributed.get_world_size", "src.utils.distributed.get_rank", "copying.CopyOrderedIterator.process", "len"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.utils.distributed.get_world_size", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.utils.distributed.get_rank", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.copying.CopyOrderedIterator.process"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "data", ",", "\n", "batch_size", ",", "\n", "l_max", ",", "\n", "offset", "=", "0", ",", "\n", "# device=\"cpu\",", "\n", "# mem_len=None,", "\n", "# ext_len=None,", "\n", "# warmup=True,", "\n", "# roll_seed=None, # roll data based on seed", "\n", ")", ":", "\n", "        ", "\"\"\"\n        data -- LongTensor -- the LongTensor is strictly ordered\n        pad_last: whether to pad the last sequence in the batch so that all sequences\n            have the same length (l_max).\n        \"\"\"", "\n", "assert", "len", "(", "data", ".", "shape", ")", "==", "1", "\n", "self", ".", "data_x", "=", "data", "[", "offset", ":", "]", "\n", "self", ".", "data_y", "=", "data", "[", ":", "-", "offset", "]", "\n", "self", ".", "data", "=", "torch", ".", "stack", "(", "[", "data_x", ",", "data_y", "]", ",", "dim", "=", "0", ")", "# (2, L, D)", "\n", "\n", "self", ".", "batch_size", "=", "batch_size", "\n", "self", ".", "l_max", "=", "l_max", "\n", "# self.roll_seed = roll_seed", "\n", "\n", "self", ".", "epoch", "=", "0", "\n", "\n", "# DDP", "\n", "self", ".", "world_size", "=", "distributed", ".", "get_world_size", "(", ")", "\n", "self", ".", "rank", "=", "distributed", ".", "get_rank", "(", ")", "\n", "\n", "self", ".", "process", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.copying.CopyOrderedIterator.process": [[125, 149], ["copying.CopyOrderedIterator.data.view().contiguous().pin_memory", "copying.CopyOrderedIterator.data.size", "copying.CopyOrderedIterator.data.chunk", "copying.CopyOrderedIterator.data.size", "copying.CopyOrderedIterator.data.view().contiguous", "copying.CopyOrderedIterator.data.size", "copying.CopyOrderedIterator.data.view", "copying.CopyOrderedIterator.data.size"], "methods", ["None"], ["", "def", "process", "(", "self", ")", ":", "\n", "        ", "\"\"\" Process the data. All logic involving sequence length and batch size should go here \"\"\"", "\n", "assert", "self", ".", "l_max", "%", "self", ".", "n_overlaps", "==", "0", "\n", "self", ".", "l_inc", "=", "self", ".", "l_max", "//", "self", ".", "n_overlaps", "\n", "\n", "global_batch_size", "=", "self", ".", "world_size", "*", "self", ".", "batch_size", "\n", "\n", "# Work out how cleanly we can divide the dataset into batch_size parts.", "\n", "n_tokens_per_batch", "=", "global_batch_size", "*", "self", ".", "l_max", "\n", "n_step", "=", "self", ".", "data", ".", "size", "(", "-", "2", ")", "//", "n_tokens_per_batch", "\n", "\n", "# Trim off any extra elements that wouldn't cleanly fit (remainders).", "\n", "self", ".", "data", "=", "self", ".", "data", "[", ":", "n_step", "*", "global_batch_size", "]", "\n", "\n", "# Evenly divide the data across the batches.", "\n", "self", ".", "data", "=", "self", ".", "data", ".", "view", "(", "2", ",", "global_batch_size", ",", "-", "1", ",", "self", ".", "data", ".", "size", "(", "-", "1", ")", ")", ".", "contiguous", "(", ")", ".", "pin_memory", "(", ")", "# (2, global_batch_size, length)", "\n", "\n", "# Partition data for DistributedDataParallel", "\n", "self", ".", "data", "=", "self", ".", "data", ".", "chunk", "(", "self", ".", "world_size", ",", "dim", "=", "-", "3", ")", "[", "self", ".", "rank", "]", "# (2, batch_size, length, dim)", "\n", "\n", "# Number of mini-batches", "\n", "# Need to subtract 1 because target is data shifted by 1", "\n", "assert", "self", ".", "data", ".", "size", "(", "-", "2", ")", "%", "self", ".", "l_max", "==", "0", "\n", "self", ".", "n_batch", "=", "(", "self", ".", "data", ".", "size", "(", "-", "2", ")", ")", "//", "self", ".", "l_max", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.copying.CopyOrderedIterator.roll": [[151, 159], ["torch.Generator", "torch.Generator", "torch.Generator", "torch.Generator", "torch.Generator", "torch.Generator", "torch.Generator", "torch.Generator", "torch.Generator", "torch.Generator.manual_seed", "torch.Generator.manual_seed", "torch.Generator.manual_seed", "range", "copying.CopyOrderedIterator.data.size", "torch.randint", "torch.randint", "torch.randint", "torch.randint", "torch.randint", "torch.randint", "torch.randint", "torch.randint", "torch.randint", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "copying.CopyOrderedIterator.data.size"], "methods", ["None"], ["", "def", "roll", "(", "self", ",", "seed", ")", ":", "\n", "        ", "rng", "=", "torch", ".", "Generator", "(", ")", "\n", "rng", ".", "manual_seed", "(", "seed", ")", "\n", "for", "i", "in", "range", "(", "self", ".", "data", ".", "size", "(", "1", ")", ")", ":", "\n", "            ", "row", "=", "self", ".", "data", "[", ":", ",", "i", "]", "\n", "shift", "=", "torch", ".", "randint", "(", "0", ",", "self", ".", "data", ".", "size", "(", "0", ")", ",", "(", "1", ",", ")", ",", "generator", "=", "rng", ")", "\n", "row", "=", "torch", ".", "cat", "(", "(", "row", "[", "shift", ":", "]", ",", "row", "[", ":", "shift", "]", ")", ")", "\n", "self", ".", "data", "[", ":", ",", "i", "]", "=", "row", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.copying.CopyOrderedIterator.get_batch": [[160, 173], ["None"], "methods", ["None"], ["", "", "def", "get_batch", "(", "self", ",", "i", ",", "l_max", "=", "None", ")", ":", "\n", "        ", "\"\"\" Get batch starting at token index i \"\"\"", "\n", "if", "l_max", "is", "None", ":", "l_max", "=", "self", ".", "l_max", "\n", "seq_len", "=", "l_max", "# min(l_max, self.data.size(0) - 1 - i)", "\n", "\n", "end_idx", "=", "i", "+", "seq_len", "\n", "# beg_idx = max(0, i - self.ext_len)", "\n", "beg_idx", "=", "i", "\n", "\n", "data", "=", "self", ".", "data", "[", ":", ",", ":", ",", "beg_idx", ":", "end_idx", ",", ":", "]", "# (2, B, L, D)", "\n", "x", ",", "y", "=", "data", "# (B, L, D)", "\n", "\n", "return", "x", ",", "y", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.copying.CopyOrderedIterator.get_fixlen_iter": [[174, 177], ["range", "copying.CopyOrderedIterator.data.size", "copying.CopyOrderedIterator.get_batch"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.copying.CopyOrderedIterator.get_batch"], ["", "def", "get_fixlen_iter", "(", "self", ")", ":", "\n", "        ", "for", "i", "in", "range", "(", "0", ",", "self", ".", "data", ".", "size", "(", "-", "1", ")", ",", "self", ".", "l_max", ")", ":", "\n", "            ", "yield", "self", ".", "get_batch", "(", "i", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.copying.CopyOrderedIterator.__iter__": [[178, 183], ["copying.CopyOrderedIterator.get_fixlen_iter"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.copying.CopyOrderedIterator.get_fixlen_iter"], ["", "", "def", "__iter__", "(", "self", ")", ":", "\n", "        ", "self", ".", "epoch", "+=", "1", "\n", "# if self.roll_seed is not None:", "\n", "#     self.roll(self.roll_seed + self.epoch)", "\n", "return", "self", ".", "get_fixlen_iter", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.copying.CopyOrderedIterator.__len__": [[184, 186], ["None"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "n_batch", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.copying.np_copying_data": [[10, 21], ["numpy.random.randint", "numpy.zeros", "numpy.zeros", "numpy.concatenate", "numpy.concatenate", "torch.one_hot().float", "torch.tensor", "torch.tensor", "torch.tensor", "numpy.ones", "torch.one_hot", "torch.tensor", "torch.tensor", "torch.tensor"], "function", ["None"], ["def", "np_copying_data", "(", "L", ",", "M", ",", "A", ",", "batch_shape", "=", "(", ")", ")", ":", "\n", "    ", "seq", "=", "np", ".", "random", ".", "randint", "(", "low", "=", "1", ",", "high", "=", "A", "-", "1", ",", "size", "=", "batch_shape", "+", "(", "M", ",", ")", ")", "\n", "zeros_x", "=", "np", ".", "zeros", "(", "batch_shape", "+", "(", "L", ",", ")", ")", "\n", "markers", "=", "(", "A", "-", "1", ")", "*", "np", ".", "ones", "(", "batch_shape", "+", "(", "M", ",", ")", ")", "\n", "zeros_y", "=", "np", ".", "zeros", "(", "batch_shape", "+", "(", "M", "+", "L", ",", ")", ")", "\n", "\n", "x_", "=", "np", ".", "concatenate", "(", "[", "seq", ",", "zeros_x", ",", "markers", "]", ",", "axis", "=", "-", "1", ")", "\n", "y_", "=", "np", ".", "concatenate", "(", "[", "zeros_y", ",", "seq", "]", ",", "axis", "=", "-", "1", ")", "\n", "x", "=", "F", ".", "one_hot", "(", "torch", ".", "tensor", "(", "x_", ",", "dtype", "=", "torch", ".", "int64", ")", ",", "A", ")", ".", "float", "(", ")", "\n", "y", "=", "torch", ".", "tensor", "(", "y_", ",", "dtype", "=", "torch", ".", "int64", ")", "\n", "return", "x", ",", "y", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.copying.torch_copying_data": [[22, 49], ["torch.randint", "torch.randint", "torch.randint", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros.scatter_", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.one_hot().float", "numpy.prod", "torch.stack", "torch.stack", "torch.stack", "torch.arange().repeat.reshape", "torch.arange().repeat.sort", "torch.arange().repeat", "torch.arange().repeat", "torch.arange().repeat", "torch.ones", "torch.ones", "torch.ones", "torch.one_hot", "torch.arange", "torch.arange", "torch.arange", "torch.randperm", "torch.randperm", "torch.randperm", "range"], "function", ["None"], ["", "def", "torch_copying_data", "(", "L", ",", "M", ",", "A", ",", "variable", "=", "False", ",", "batch_shape", "=", "(", ")", ")", ":", "\n", "    ", "tokens", "=", "torch", ".", "randint", "(", "low", "=", "1", ",", "high", "=", "A", "-", "1", ",", "size", "=", "batch_shape", "+", "(", "M", ",", ")", ")", "\n", "# zeros_x = torch.zeros(batch_shape+(L,), dtype=torch.long)", "\n", "if", "variable", ":", "\n", "# inds = torch.randint(low=0, high=L+M, size=batch_shape+(M,))", "\n", "        ", "total_batch", "=", "np", ".", "prod", "(", "batch_shape", ")", "\n", "inds", "=", "torch", ".", "stack", "(", "[", "\n", "torch", ".", "randperm", "(", "L", "+", "M", ")", "[", ":", "M", "]", "\n", "for", "_", "in", "range", "(", "total_batch", ")", "\n", "]", ",", "0", ")", "\n", "inds", "=", "inds", ".", "reshape", "(", "batch_shape", "+", "(", "M", ",", ")", ")", "\n", "inds", ",", "_", "=", "inds", ".", "sort", "(", ")", "\n", "", "else", ":", "\n", "        ", "inds", "=", "torch", ".", "arange", "(", "M", ")", ".", "repeat", "(", "batch_shape", "+", "(", "1", ",", ")", ")", "\n", "", "zeros_x", "=", "torch", ".", "zeros", "(", "batch_shape", "+", "(", "M", "+", "L", ",", ")", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "zeros_x", ".", "scatter_", "(", "-", "1", ",", "inds", ",", "tokens", ")", "\n", "markers", "=", "(", "A", "-", "1", ")", "*", "torch", ".", "ones", "(", "batch_shape", "+", "(", "M", ",", ")", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "# zeros_y = torch.zeros(batch_shape+(M+L,), dtype=torch.long)", "\n", "\n", "x_", "=", "torch", ".", "cat", "(", "[", "zeros_x", ",", "markers", "]", ",", "dim", "=", "-", "1", ")", "\n", "y_", "=", "torch", ".", "cat", "(", "[", "tokens", "]", ",", "dim", "=", "-", "1", ")", "\n", "x", "=", "F", ".", "one_hot", "(", "x_", ",", "A", ")", ".", "float", "(", ")", "\n", "# y = torch.tensor(y_, dtype=torch.int64)", "\n", "# y = y_.long()", "\n", "y", "=", "y_", "\n", "# print(\"TYPE\", x.dtype, y.dtype)", "\n", "return", "x", ",", "y", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.copying.copying_static_dataset": [[83, 88], ["copying.torch_copying_data", "print", "torch.utils.data.TensorDataset", "torch.utils.data.TensorDataset", "torch.utils.data.TensorDataset"], "function", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.copying.torch_copying_data"], ["", "", "def", "copying_static_dataset", "(", "L", ",", "M", ",", "A", ",", "variable", ",", "samples", ")", ":", "\n", "    ", "all_x", ",", "all_y", "=", "torch_copying_data", "(", "L", ",", "M", ",", "A", ",", "variable", ",", "batch_shape", "=", "(", "samples", ",", ")", ")", "\n", "print", "(", "\"Constructing Copying dataset of shape\"", ",", "all_x", ".", "shape", ")", "\n", "ds", "=", "torch", ".", "utils", ".", "data", ".", "TensorDataset", "(", "all_x", ",", "all_y", ")", "\n", "return", "ds", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.adding.torch_adding_data": [[8, 23], ["torch.randint", "torch.randint", "torch.randint", "torch.randint", "torch.randint", "torch.randint", "torch.cat().float", "torch.cat().float", "torch.cat().float", "torch.empty", "torch.empty", "torch.empty", "torch.empty.uniform_", "torch.stack", "torch.stack", "torch.stack", "torch.sum", "torch.sum", "torch.sum", "torch.cat", "torch.cat", "torch.cat", "torch.one_hot", "torch.one_hot"], "function", ["None"], ["def", "torch_adding_data", "(", "L", ",", "batch_shape", "=", "(", ")", ")", ":", "\n", "    ", "assert", "L", ">=", "2", "\n", "# tokens = torch.randint(low=1, high=A-1, size=batch_shape+(M,))", "\n", "mid", "=", "L", "//", "2", "\n", "idx0", "=", "torch", ".", "randint", "(", "low", "=", "0", ",", "high", "=", "mid", ",", "size", "=", "batch_shape", ")", "\n", "idx1", "=", "torch", ".", "randint", "(", "low", "=", "0", ",", "high", "=", "L", "-", "mid", ",", "size", "=", "batch_shape", ")", "\n", "\n", "idx", "=", "torch", ".", "cat", "(", "(", "F", ".", "one_hot", "(", "idx0", ",", "mid", ")", ",", "F", ".", "one_hot", "(", "idx1", ",", "L", "-", "mid", ")", ")", ",", "dim", "=", "-", "1", ")", ".", "float", "(", ")", "# (batch_shape, L)", "\n", "unif", "=", "torch", ".", "empty", "(", "batch_shape", "+", "(", "L", ",", ")", ")", "\n", "unif", ".", "uniform_", "(", "0.", ",", "1.", ")", "\n", "\n", "x", "=", "torch", ".", "stack", "(", "(", "unif", ",", "idx", ")", ",", "dim", "=", "-", "1", ")", "# (batch_shape, L, 2)", "\n", "y", "=", "torch", ".", "sum", "(", "unif", "*", "idx", ",", "dim", "=", "-", "1", ",", "keepdim", "=", "True", ")", "# (batch_shape, 1)", "\n", "\n", "return", "x", ",", "y", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.adding.adding_static_dataset": [[24, 29], ["adding.torch_adding_data", "print", "torch.utils.data.TensorDataset", "torch.utils.data.TensorDataset", "torch.utils.data.TensorDataset"], "function", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.adding.torch_adding_data"], ["", "def", "adding_static_dataset", "(", "L", ",", "samples", ")", ":", "\n", "    ", "all_x", ",", "all_y", "=", "torch_adding_data", "(", "L", ",", "batch_shape", "=", "(", "samples", ",", ")", ")", "\n", "print", "(", "\"Constructing Adding dataset of shape\"", ",", "all_x", ".", "shape", ")", "\n", "ds", "=", "torch", ".", "utils", ".", "data", ".", "TensorDataset", "(", "all_x", ",", "all_y", ")", "\n", "return", "ds", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.eigenworms.process_data.split_data": [[9, 19], ["pandas.concat", "numpy.concatenate", "sklearn.model_selection.train_test_split", "sklearn.model_selection.train_test_split", "sklearn.model_selection.train_test_split"], "function", ["None"], ["def", "split_data", "(", "X_train_orig", ",", "y_train_orig", ",", "X_test_orig", ",", "y_test_orig", ",", "shuffle", "=", "True", ",", "seed", "=", "0", ")", ":", "\n", "    ", "if", "shuffle", ":", "\n", "        ", "X_all", "=", "pd", ".", "concat", "(", "(", "X_train_orig", ",", "X_test_orig", ")", ")", "\n", "y_all", "=", "np", ".", "concatenate", "(", "(", "y_train_orig", ",", "y_test_orig", ")", ")", "\n", "X_train", ",", "X_eval", ",", "y_train", ",", "y_eval", "=", "train_test_split", "(", "X_all", ",", "y_all", ",", "test_size", "=", "0.3", ",", "random_state", "=", "seed", ")", "\n", "X_val", ",", "X_test", ",", "y_val", ",", "y_test", "=", "train_test_split", "(", "X_eval", ",", "y_eval", ",", "test_size", "=", "0.5", ",", "random_state", "=", "seed", "+", "1", ")", "\n", "", "else", ":", "\n", "        ", "X_test", ",", "y_test", "=", "X_test_orig", ",", "y_test_orig", "\n", "X_train", ",", "X_val", ",", "y_train", ",", "y_val", "=", "train_test_split", "(", "X_train_orig", ",", "y_train_orig", ",", "test_size", "=", "0.20", ",", "random_state", "=", "seed", ")", "\n", "", "return", "X_train", ",", "y_train", ",", "X_val", ",", "y_val", ",", "X_test", ",", "y_test", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.eigenworms.process_data._to_numpy": [[29, 31], ["numpy.stack().swapaxes", "numpy.stack", "numpy.stack", "X.to_numpy"], "function", ["None"], ["def", "_to_numpy", "(", "X", ")", ":", "\n", "    ", "return", "np", ".", "stack", "(", "[", "np", ".", "stack", "(", "x", ")", "for", "x", "in", "X", ".", "to_numpy", "(", ")", "]", ")", ".", "swapaxes", "(", "-", "1", ",", "-", "2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.eeg.data_utils.StandardScaler.__init__": [[189, 192], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "mean", ",", "std", ")", ":", "\n", "        ", "self", ".", "mean", "=", "mean", "# (1,num_nodes,1)", "\n", "self", ".", "std", "=", "std", "# (1,num_nodes,1)", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.eeg.data_utils.StandardScaler.transform": [[193, 195], ["None"], "methods", ["None"], ["", "def", "transform", "(", "self", ",", "data", ")", ":", "\n", "        ", "return", "(", "data", "-", "self", ".", "mean", ")", "/", "self", ".", "std", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.eeg.data_utils.StandardScaler.inverse_transform": [[196, 205], ["None"], "methods", ["None"], ["", "def", "inverse_transform", "(", "self", ",", "data", ")", ":", "\n", "        ", "\"\"\"\n        Masked inverse transform\n        Args:\n            data: data for inverse scaling\n            is_tensor: whether data is a tensor\n            device: device\n        \"\"\"", "\n", "return", "data", "*", "self", ".", "std", "+", "self", ".", "mean", "\n", "", "", ""]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.eeg.data_utils.computeFFT": [[38, 60], ["scipy.fftpack.fft", "int", "numpy.abs", "numpy.log", "numpy.angle", "numpy.floor"], "function", ["None"], ["def", "computeFFT", "(", "signals", ",", "n", ")", ":", "\n", "    ", "\"\"\"\n    Args:\n        signals: EEG signals, (number of channels, number of data points)\n        n: length of positive frequency terms of fourier transform\n    Returns:\n        FT: log amplitude of FFT of signals, (number of channels, number of data points)\n        P: phase spectrum of FFT of signals, (number of channels, number of data points)\n    \"\"\"", "\n", "# fourier transform", "\n", "fourier_signal", "=", "fft", "(", "signals", ",", "n", "=", "n", ",", "axis", "=", "-", "1", ")", "# FFT on the last dimension", "\n", "\n", "# only take the positive freq part", "\n", "idx_pos", "=", "int", "(", "np", ".", "floor", "(", "n", "/", "2", ")", ")", "\n", "fourier_signal", "=", "fourier_signal", "[", ":", ",", ":", "idx_pos", "]", "\n", "amp", "=", "np", ".", "abs", "(", "fourier_signal", ")", "\n", "amp", "[", "amp", "==", "0.0", "]", "=", "1e-8", "# avoid log of 0", "\n", "\n", "FT", "=", "np", ".", "log", "(", "amp", ")", "\n", "P", "=", "np", ".", "angle", "(", "fourier_signal", ")", "\n", "\n", "return", "FT", ",", "P", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.eeg.data_utils.get_swap_pairs": [[62, 89], ["swap_pairs.append", "swap_pairs.append", "swap_pairs.append", "swap_pairs.append", "swap_pairs.append", "swap_pairs.append", "swap_pairs.append", "swap_pairs.append", "channels.index", "channels.index", "channels.index", "channels.index", "channels.index", "channels.index", "channels.index", "channels.index", "channels.index", "channels.index", "channels.index", "channels.index", "channels.index", "channels.index", "channels.index", "channels.index"], "function", ["None"], ["", "def", "get_swap_pairs", "(", "channels", ")", ":", "\n", "    ", "\"\"\"\n    Swap select adjacenet channels\n    Args:\n        channels: list of channel names\n    Returns:\n        list of tuples, each a pair of channel indices being swapped\n    \"\"\"", "\n", "swap_pairs", "=", "[", "]", "\n", "if", "(", "\"EEG FP1\"", "in", "channels", ")", "and", "(", "\"EEG FP2\"", "in", "channels", ")", ":", "\n", "        ", "swap_pairs", ".", "append", "(", "(", "channels", ".", "index", "(", "\"EEG FP1\"", ")", ",", "channels", ".", "index", "(", "\"EEG FP2\"", ")", ")", ")", "\n", "", "if", "(", "\"EEG Fp1\"", "in", "channels", ")", "and", "(", "\"EEG Fp2\"", "in", "channels", ")", ":", "\n", "        ", "swap_pairs", ".", "append", "(", "(", "channels", ".", "index", "(", "\"EEG Fp1\"", ")", ",", "channels", ".", "index", "(", "\"EEG Fp2\"", ")", ")", ")", "\n", "", "if", "(", "\"EEG F3\"", "in", "channels", ")", "and", "(", "\"EEG F4\"", "in", "channels", ")", ":", "\n", "        ", "swap_pairs", ".", "append", "(", "(", "channels", ".", "index", "(", "\"EEG F3\"", ")", ",", "channels", ".", "index", "(", "\"EEG F4\"", ")", ")", ")", "\n", "", "if", "(", "\"EEG F7\"", "in", "channels", ")", "and", "(", "\"EEG F8\"", "in", "channels", ")", ":", "\n", "        ", "swap_pairs", ".", "append", "(", "(", "channels", ".", "index", "(", "\"EEG F7\"", ")", ",", "channels", ".", "index", "(", "\"EEG F8\"", ")", ")", ")", "\n", "", "if", "(", "\"EEG C3\"", "in", "channels", ")", "and", "(", "\"EEG C4\"", "in", "channels", ")", ":", "\n", "        ", "swap_pairs", ".", "append", "(", "(", "channels", ".", "index", "(", "\"EEG C3\"", ")", ",", "channels", ".", "index", "(", "\"EEG C4\"", ")", ")", ")", "\n", "", "if", "(", "\"EEG T3\"", "in", "channels", ")", "and", "(", "\"EEG T4\"", "in", "channels", ")", ":", "\n", "        ", "swap_pairs", ".", "append", "(", "(", "channels", ".", "index", "(", "\"EEG T3\"", ")", ",", "channels", ".", "index", "(", "\"EEG T4\"", ")", ")", ")", "\n", "", "if", "(", "\"EEG T5\"", "in", "channels", ")", "and", "(", "\"EEG T6\"", "in", "channels", ")", ":", "\n", "        ", "swap_pairs", ".", "append", "(", "(", "channels", ".", "index", "(", "\"EEG T5\"", ")", ",", "channels", ".", "index", "(", "\"EEG T6\"", ")", ")", ")", "\n", "", "if", "(", "\"EEG O1\"", "in", "channels", ")", "and", "(", "\"EEG O2\"", "in", "channels", ")", ":", "\n", "        ", "swap_pairs", ".", "append", "(", "(", "channels", ".", "index", "(", "\"EEG O1\"", ")", ",", "channels", ".", "index", "(", "\"EEG O2\"", ")", ")", ")", "\n", "\n", "", "return", "swap_pairs", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.eeg.data_utils.random_augmentation": [[91, 106], ["data_utils.get_swap_pairs", "random.choice", "random.choice", "numpy.random.uniform"], "function", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.eeg.data_utils.get_swap_pairs"], ["", "def", "random_augmentation", "(", "EEG_seq", ")", ":", "\n", "    ", "\"\"\"\n    Random augmentation of EEG sequence by randomly swapping channels\n    and randomly scaling the signals.\n    Args:\n        EEG_seq: shape (seq_length, num_nodes, num_data_point)\n    Returns:\n        augmented signals, same shape as input\n    \"\"\"", "\n", "for", "pair", "in", "get_swap_pairs", "(", "INCLUDED_CHANNELS", ")", ":", "\n", "        ", "if", "random", ".", "choice", "(", "[", "True", ",", "False", "]", ")", ":", "\n", "            ", "EEG_seq", "[", ":", ",", "[", "pair", "[", "0", "]", ",", "pair", "[", "1", "]", "]", ",", ":", "]", "=", "EEG_seq", "[", ":", ",", "[", "pair", "[", "1", "]", ",", "pair", "[", "0", "]", "]", ",", ":", "]", "\n", "", "", "if", "random", ".", "choice", "(", "[", "True", ",", "False", "]", ")", ":", "\n", "        ", "EEG_seq", "=", "EEG_seq", "*", "np", ".", "random", ".", "uniform", "(", "0.8", ",", "1.2", ")", "\n", "", "return", "EEG_seq", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.eeg.data_utils.getOrderedChannels": [[108, 122], ["list", "range", "len", "labels[].split", "ordered_channels.append", "list.index", "Exception", "print"], "function", ["None"], ["", "def", "getOrderedChannels", "(", "file_name", ",", "verbose", ",", "labels_object", ",", "channel_names", ")", ":", "\n", "    ", "labels", "=", "list", "(", "labels_object", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "labels", ")", ")", ":", "\n", "        ", "labels", "[", "i", "]", "=", "labels", "[", "i", "]", ".", "split", "(", "\"-\"", ")", "[", "0", "]", "\n", "\n", "", "ordered_channels", "=", "[", "]", "\n", "for", "ch", "in", "channel_names", ":", "\n", "        ", "try", ":", "\n", "            ", "ordered_channels", ".", "append", "(", "labels", ".", "index", "(", "ch", ")", ")", "\n", "", "except", ":", "\n", "            ", "if", "verbose", ":", "\n", "                ", "print", "(", "file_name", "+", "\" failed to get channel \"", "+", "ch", ")", "\n", "", "raise", "Exception", "(", "\"channel not match\"", ")", "\n", "", "", "return", "ordered_channels", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.eeg.data_utils.getChannelIndices": [[124, 132], ["labels.index"], "function", ["None"], ["", "def", "getChannelIndices", "(", "labelsObject", ",", "channel_names", ")", ":", "\n", "    ", "\"\"\"\n    labelsObject: labels object from eeghdf file\n    channel_names: list of channel names\n    \"\"\"", "\n", "labels", "=", "labelsObject", "\n", "channelIndices", "=", "[", "labels", ".", "index", "(", "ch", ")", "for", "ch", "in", "channel_names", "]", "\n", "return", "channelIndices", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.eeg.data_utils.getSeizureTimes": [[134, 155], ["open", "f.readlines", "file_name.split", "seizure_times.append", "float", "float", "line.strip().split", "line.strip().split", "line.strip", "line.strip"], "function", ["None"], ["", "def", "getSeizureTimes", "(", "file_name", ")", ":", "\n", "    ", "\"\"\"\n    Args:\n        file_name: file name of .edf file etc.\n    Returns:\n        seizure_times: list of times of seizure onset in seconds\n    \"\"\"", "\n", "tse_file", "=", "file_name", ".", "split", "(", "\".edf\"", ")", "[", "0", "]", "+", "\".tse_bi\"", "\n", "\n", "seizure_times", "=", "[", "]", "\n", "with", "open", "(", "tse_file", ")", "as", "f", ":", "\n", "        ", "for", "line", "in", "f", ".", "readlines", "(", ")", ":", "\n", "            ", "if", "\"seiz\"", "in", "line", ":", "# if seizure", "\n", "# seizure start and end time", "\n", "                ", "seizure_times", ".", "append", "(", "\n", "[", "\n", "float", "(", "line", ".", "strip", "(", ")", ".", "split", "(", "\" \"", ")", "[", "0", "]", ")", ",", "\n", "float", "(", "line", ".", "strip", "(", ")", ".", "split", "(", "\" \"", ")", "[", "1", "]", ")", ",", "\n", "]", "\n", ")", "\n", "", "", "", "return", "seizure_times", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.eeg.data_utils.getEDFsignals": [[157, 167], ["numpy.zeros", "range", "edf.getNSamples", "edf.readSignal"], "function", ["None"], ["", "def", "getEDFsignals", "(", "edf", ")", ":", "\n", "    ", "n", "=", "edf", ".", "signals_in_file", "\n", "samples", "=", "edf", ".", "getNSamples", "(", ")", "[", "0", "]", "\n", "signals", "=", "np", ".", "zeros", "(", "(", "n", ",", "samples", ")", ")", "\n", "for", "i", "in", "range", "(", "n", ")", ":", "\n", "        ", "try", ":", "\n", "            ", "signals", "[", "i", ",", ":", "]", "=", "edf", ".", "readSignal", "(", "i", ")", "\n", "", "except", ":", "\n", "            ", "pass", "\n", "", "", "return", "signals", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.eeg.data_utils.resampleData": [[169, 182], ["int", "scipy.signal.resample"], "function", ["None"], ["", "def", "resampleData", "(", "signals", ",", "to_freq", "=", "200", ",", "window_size", "=", "4", ")", ":", "\n", "    ", "\"\"\"\n    Resample signals from its original sampling freq to another freq\n    Args:\n        signals: EEG signal slice, (num_channels, num_data_points)\n        to_freq: Re-sampled frequency in Hz\n        window_size: time window in seconds\n    Returns:\n        resampled: (num_channels, resampled_data_points)\n    \"\"\"", "\n", "num", "=", "int", "(", "to_freq", "*", "window_size", ")", "\n", "resampled", "=", "resample", "(", "signals", ",", "num", "=", "num", ",", "axis", "=", "1", ")", "\n", "return", "resampled", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.eeg.dataloader.SeizureDataset.__init__": [[165, 306], ["print", "len", "ValueError", "ValueError", "dataset_name.lower", "os.walk", "open", "f.readlines", "curr_str.strip().split", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "x.split", "os.path.join", "os.path.join", "curr_str.strip", "h5py.File", "dataloader.SeizureDataset.file_tuples.append", "dataloader.SeizureDataset.edf_files.append", "os.path.join", "str", "str", "str", "str", "edf_fn.split", "str", "str", "str", "str", "str", "str"], "methods", ["None"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "input_dir", ",", "\n", "raw_data_dir", ",", "\n", "clip_len", "=", "60", ",", "\n", "time_step_size", "=", "1", ",", "\n", "stride", "=", "60", ",", "\n", "standardize", "=", "False", ",", "\n", "scaler", "=", "None", ",", "\n", "split", "=", "\"train\"", ",", "\n", "balance_train", "=", "False", ",", "\n", "data_augment", "=", "False", ",", "\n", "use_fft", "=", "False", ",", "\n", "use_cnn_features", "=", "False", ",", "\n", "padding", "=", "False", ",", "\n", "padding_val", "=", "0", ",", "\n", "dataset_name", "=", "\"TUH\"", ",", "\n", "min_sz_len", "=", "5", ",", "\n", "cv_fold", "=", "None", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            input_dir: dir to resampled signals h5 files\n            raw_data_dir: dir to raw edf files\n            clip_len: EEG clip length, in seconds, int\n            time_step_size: how many seconds is one time step? int\n            stride: how many seconds between subsequent clips, int\n            standardize: if True, will z-normalize wrt train set mean and std\n            scaler: scaler object for standardization\n            split: train, dev or test\n            data_augment: if True, perform random scaling & random reflecting along midline\n            use_fft: if True, will perform Fourier transform to raw EEG signals\n            padding: if True, will pad to clip_len//time_step_size\n            padding_val: int, value used for padding to clip_len\n            dataset_name: name of the dataset, \"TUH\", \"Stanford\" or \"LPCH\"\n            min_sz_len: minimum length to be considered as a seizure, in seconds\n            cv_fold: cv fold, int\n        \"\"\"", "\n", "if", "standardize", "and", "(", "scaler", "is", "None", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\"To standardize, please provide scaler.\"", ")", "\n", "", "if", "use_fft", "and", "use_cnn_features", ":", "\n", "            ", "raise", "ValueError", "(", "\"Either use_fft or use_cnn_features.\"", ")", "\n", "\n", "", "self", ".", "input_dir", "=", "input_dir", "\n", "self", ".", "raw_data_dir", "=", "raw_data_dir", "\n", "self", ".", "time_step_size", "=", "time_step_size", "\n", "self", ".", "clip_len", "=", "clip_len", "\n", "# only use smaller strides for train set", "\n", "self", ".", "stride", "=", "stride", "if", "split", "==", "\"train\"", "else", "clip_len", "\n", "self", ".", "standardize", "=", "standardize", "\n", "self", ".", "scaler", "=", "scaler", "\n", "self", ".", "split", "=", "split", "\n", "self", ".", "balance_train", "=", "balance_train", "\n", "self", ".", "data_augment", "=", "data_augment", "\n", "self", ".", "use_fft", "=", "use_fft", "\n", "self", ".", "use_cnn_features", "=", "use_cnn_features", "\n", "self", ".", "padding_val", "=", "padding_val", "\n", "self", ".", "padding", "=", "padding", "\n", "self", ".", "dataset_name", "=", "dataset_name", "\n", "self", ".", "min_sz_len", "=", "min_sz_len", "\n", "self", ".", "cv_fold", "=", "cv_fold", "\n", "\n", "# get full paths to all raw edf files", "\n", "self", ".", "edf_files", "=", "[", "]", "\n", "if", "dataset_name", ".", "lower", "(", ")", "==", "\"tuh\"", ":", "\n", "            ", "for", "path", ",", "subdirs", ",", "files", "in", "os", ".", "walk", "(", "raw_data_dir", ")", ":", "\n", "                ", "for", "name", "in", "files", ":", "\n", "                    ", "if", "\".edf\"", "in", "name", ":", "\n", "                        ", "self", ".", "edf_files", ".", "append", "(", "os", ".", "path", ".", "join", "(", "path", ",", "name", ")", ")", "\n", "\n", "# get number of clips for each eeg file", "\n", "", "", "", "", "self", ".", "file_tuples", "=", "[", "]", "# list of tuples (h5_file_name, clip_index)", "\n", "if", "split", "==", "\"train\"", "and", "balance_train", ":", "\n", "            ", "if", "padding", ":", "\n", "                ", "file_marker", "=", "os", ".", "path", ".", "join", "(", "\n", "os", ".", "path", ".", "join", "(", "FILEMARKER_DIR", ",", "\"variable_length\"", ")", ",", "\n", "split", "\n", "+", "\"_cliplen\"", "\n", "+", "str", "(", "clip_len", ")", "\n", "+", "\"_stride\"", "\n", "+", "str", "(", "self", ".", "stride", ")", "\n", "+", "\"_timestep\"", "\n", "+", "str", "(", "time_step_size", ")", "\n", "+", "\"_balanced.txt\"", ",", "\n", ")", "\n", "", "else", ":", "\n", "                ", "file_marker", "=", "os", ".", "path", ".", "join", "(", "\n", "FILEMARKER_DIR", ",", "\n", "split", "\n", "+", "\"_cliplen\"", "\n", "+", "str", "(", "clip_len", ")", "\n", "+", "\"_stride\"", "\n", "+", "str", "(", "self", ".", "stride", ")", "\n", "+", "\"_balanced.txt\"", ",", "\n", ")", "\n", "", "", "else", ":", "\n", "            ", "if", "padding", ":", "\n", "                ", "file_marker", "=", "os", ".", "path", ".", "join", "(", "\n", "os", ".", "path", ".", "join", "(", "FILEMARKER_DIR", ",", "\"variable_length\"", ")", ",", "\n", "split", "\n", "+", "\"_cliplen\"", "\n", "+", "str", "(", "clip_len", ")", "\n", "+", "\"_stride\"", "\n", "+", "str", "(", "self", ".", "stride", ")", "\n", "+", "\"_timestep\"", "\n", "+", "str", "(", "time_step_size", ")", "\n", "+", "\".txt\"", ",", "\n", ")", "\n", "", "else", ":", "\n", "                ", "file_marker", "=", "os", ".", "path", ".", "join", "(", "\n", "FILEMARKER_DIR", ",", "\n", "split", "\n", "+", "\"_cliplen\"", "\n", "+", "str", "(", "clip_len", ")", "\n", "+", "\"_stride\"", "\n", "+", "str", "(", "self", ".", "stride", ")", "\n", "+", "\".txt\"", ",", "\n", ")", "\n", "\n", "", "", "print", "(", "file_marker", ")", "\n", "with", "open", "(", "file_marker", ",", "\"r\"", ")", "as", "f", ":", "\n", "            ", "file_str", "=", "f", ".", "readlines", "(", ")", "\n", "", "file_tuples", "=", "[", "curr_str", ".", "strip", "(", "\"\\n\"", ")", ".", "split", "(", "\",\"", ")", "for", "curr_str", "in", "file_str", "]", "\n", "\n", "# filter out signals shorter than time_step_size", "\n", "self", ".", "file_tuples", "=", "[", "]", "\n", "if", "self", ".", "use_cnn_features", ":", "\n", "            ", "for", "tup", "in", "file_tuples", ":", "\n", "                ", "edf_fn", "=", "tup", "[", "0", "]", "\n", "h5_fn", "=", "os", ".", "path", ".", "join", "(", "input_dir", ",", "edf_fn", ".", "split", "(", "\".edf\"", ")", "[", "0", "]", "+", "\".h5\"", ")", "\n", "with", "h5py", ".", "File", "(", "h5_fn", ",", "\"r\"", ")", "as", "hf", ":", "\n", "                    ", "feature", "=", "hf", "[", "\"cnn_features\"", "]", "[", "(", ")", "]", "\n", "", "if", "feature", ".", "shape", "[", "0", "]", ">", "0", ":", "\n", "                    ", "self", ".", "file_tuples", ".", "append", "(", "tup", ")", "\n", "", "", "", "else", ":", "\n", "            ", "self", ".", "file_tuples", "=", "file_tuples", "\n", "\n", "", "self", ".", "size", "=", "len", "(", "self", ".", "file_tuples", ")", "\n", "\n", "# Get sensor ids", "\n", "self", ".", "sensor_ids", "=", "[", "x", ".", "split", "(", "\" \"", ")", "[", "-", "1", "]", "for", "x", "in", "INCLUDED_CHANNELS", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.eeg.dataloader.SeizureDataset.__len__": [[307, 309], ["None"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "size", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.eeg.dataloader.SeizureDataset._random_reflect": [[310, 329], ["datasets.eeg.data_utils.get_swap_pairs", "EEG_seq.copy", "numpy.random.choice"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.eeg.data_utils.get_swap_pairs"], ["", "def", "_random_reflect", "(", "self", ",", "EEG_seq", ")", ":", "\n", "        ", "\"\"\"\n        Randomly reflect along midline\n        Args:\n            EEG_seq: shape (seq_length, num_nodes, feature_dim)\n        Returns:\n            EEG_seq_reflect: augmented signal, same shape as EEG_seq\n            swap_pairs: swapped EEG channel pairs\n        \"\"\"", "\n", "swap_pairs", "=", "get_swap_pairs", "(", "INCLUDED_CHANNELS", ")", "\n", "EEG_seq_reflect", "=", "EEG_seq", ".", "copy", "(", ")", "\n", "if", "np", ".", "random", ".", "choice", "(", "[", "True", ",", "False", "]", ")", ":", "\n", "            ", "for", "pair", "in", "swap_pairs", ":", "\n", "                ", "EEG_seq_reflect", "[", ":", ",", "[", "pair", "[", "0", "]", ",", "pair", "[", "1", "]", "]", ",", ":", "]", "=", "EEG_seq", "[", "\n", ":", ",", "[", "pair", "[", "1", "]", ",", "pair", "[", "0", "]", "]", ",", ":", "\n", "]", "\n", "", "", "else", ":", "\n", "            ", "swap_pairs", "=", "None", "\n", "", "return", "EEG_seq_reflect", ",", "swap_pairs", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.eeg.dataloader.SeizureDataset._random_scale": [[330, 344], ["numpy.random.uniform", "numpy.log"], "methods", ["None"], ["", "def", "_random_scale", "(", "self", ",", "EEG_seq", ")", ":", "\n", "        ", "\"\"\"\n        Scale EEG signals by a random number between 0.8 and 1.2\n        Args:\n            EEG_seq: shape (seq_length, num_nodes, feature_dim)\n        Returns:\n            augmented signals, same shape as input\n        \"\"\"", "\n", "scale_factor", "=", "np", ".", "random", ".", "uniform", "(", "0.8", ",", "1.2", ")", "\n", "if", "self", ".", "use_fft", ":", "\n", "            ", "EEG_seq", "+=", "np", ".", "log", "(", "scale_factor", ")", "\n", "", "else", ":", "\n", "            ", "EEG_seq", "*=", "scale_factor", "\n", "", "return", "EEG_seq", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.eeg.dataloader.SeizureDataset.__getitem__": [[345, 434], ["int", "os.path.join", "dataloader.computeSliceMatrix", "numpy.minimum", "torch.FloatTensor", "torch.FloatTensor", "torch.LongTensor", "len", "str", "dataloader.SeizureDataset._random_reflect", "dataloader.SeizureDataset._random_scale", "dataloader.SeizureDataset.scaler.transform", "numpy.concatenate", "numpy.concatenate", "dataloader.SeizureDataset.copy", "seizure_labels.copy", "x.reshape.reshape.view", "x.reshape.reshape.transpose", "x.reshape.reshape.reshape", "numpy.ones", "edf_fn.split", "numpy.ones", "numpy.ones"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.eeg.dataloader.computeSliceMatrix", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.eeg.dataloader.SeizureDataset._random_reflect", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.eeg.dataloader.SeizureDataset._random_scale", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.eeg.data_utils.StandardScaler.transform"], ["", "def", "__getitem__", "(", "self", ",", "idx", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            idx: (int) index in [0, 1, ..., size_of_dataset-1]\n        Returns:\n            a tuple of (feature, label, writeout_file_name)\n        \"\"\"", "\n", "edf_fn", ",", "clip_idx", ",", "_", "=", "self", ".", "file_tuples", "[", "idx", "]", "\n", "edf_file", "=", "[", "file", "for", "file", "in", "self", ".", "edf_files", "if", "edf_fn", "in", "file", "]", "\n", "assert", "len", "(", "edf_file", ")", "==", "1", "\n", "edf_file", "=", "edf_file", "[", "0", "]", "\n", "clip_idx", "=", "int", "(", "clip_idx", ")", "\n", "writeout_fn", "=", "edf_fn", "+", "\"_\"", "+", "str", "(", "clip_idx", ")", "\n", "h5_fn", "=", "os", ".", "path", ".", "join", "(", "self", ".", "input_dir", ",", "edf_fn", ".", "split", "(", "\".edf\"", ")", "[", "0", "]", "+", "\".h5\"", ")", "\n", "\n", "# get the clip", "\n", "eeg_clip", ",", "seizure_labels", ",", "is_seizure", "=", "computeSliceMatrix", "(", "\n", "h5_fn", "=", "h5_fn", ",", "\n", "edf_fn", "=", "edf_file", ",", "\n", "channel_names", "=", "INCLUDED_CHANNELS", ",", "\n", "clip_idx", "=", "clip_idx", ",", "\n", "time_step_size", "=", "self", ".", "time_step_size", ",", "\n", "clip_len", "=", "self", ".", "clip_len", ",", "\n", "stride", "=", "self", ".", "stride", ",", "\n", "is_fft", "=", "self", ".", "use_fft", ",", "\n", "use_cnn_features", "=", "self", ".", "use_cnn_features", ",", "\n", "dataset_name", "=", "self", ".", "dataset_name", ",", "\n", "min_sz_len", "=", "self", ".", "min_sz_len", ",", "\n", ")", "\n", "\n", "# data augmentation", "\n", "if", "self", ".", "data_augment", "and", "(", "\n", "not", "self", ".", "use_cnn_features", "\n", ")", ":", "# no augmentation with cnn features", "\n", "            ", "curr_feature", ",", "swap_nodes", "=", "self", ".", "_random_reflect", "(", "eeg_clip", ")", "\n", "curr_feature", "=", "self", ".", "_random_scale", "(", "curr_feature", ")", "\n", "", "else", ":", "\n", "            ", "swap_nodes", "=", "None", "\n", "curr_feature", "=", "eeg_clip", "\n", "\n", "# standardize wrt train mean and std", "\n", "", "if", "(", "\n", "self", ".", "standardize", "\n", "and", "(", "not", "self", ".", "use_cnn_features", ")", "\n", "and", "(", "not", "self", ".", "use_hippo_proj", ")", "\n", ")", ":", "\n", "            ", "curr_feature", "=", "self", ".", "scaler", ".", "transform", "(", "curr_feature", ")", "\n", "\n", "# padding, will be useful for full signal later", "\n", "", "curr_len", "=", "curr_feature", ".", "shape", "[", "0", "]", "\n", "seq_len", "=", "np", ".", "minimum", "(", "curr_len", ",", "self", ".", "clip_len", "//", "self", ".", "time_step_size", ")", "\n", "if", "self", ".", "padding", "and", "(", "curr_len", "<", "(", "self", ".", "clip_len", "//", "self", ".", "time_step_size", ")", ")", ":", "\n", "            ", "len_pad", "=", "self", ".", "clip_len", "//", "self", ".", "time_step_size", "-", "curr_len", "\n", "if", "not", "self", ".", "use_cnn_features", ":", "\n", "                ", "padded_feature", "=", "(", "\n", "np", ".", "ones", "(", "(", "len_pad", ",", "curr_feature", ".", "shape", "[", "1", "]", ",", "curr_feature", ".", "shape", "[", "2", "]", ")", ")", "\n", "*", "self", ".", "padding_val", "\n", ")", "\n", "", "else", ":", "\n", "                ", "padded_feature", "=", "(", "\n", "np", ".", "ones", "(", "(", "len_pad", ",", "curr_feature", ".", "shape", "[", "1", "]", ")", ")", "*", "self", ".", "padding_val", "\n", ")", "\n", "", "padded_feature", "=", "np", ".", "concatenate", "(", "(", "curr_feature", ",", "padded_feature", ")", ",", "axis", "=", "0", ")", "\n", "\n", "# for labels, pad with -1 ?", "\n", "padded_label", "=", "np", ".", "ones", "(", "(", "len_pad", ")", ")", "*", "-", "1.0", "\n", "padded_label", "=", "np", ".", "concatenate", "(", "(", "seizure_labels", ",", "padded_label", ")", ",", "axis", "=", "0", ")", "\n", "", "else", ":", "\n", "            ", "padded_feature", "=", "curr_feature", ".", "copy", "(", ")", "\n", "padded_label", "=", "seizure_labels", ".", "copy", "(", ")", "\n", "\n", "# convert to tensors", "\n", "# (clip_len//time_step_size, num_channels, input_dim) or (clip_len//time_step_size, cnn_feature_dim)", "\n", "", "x", "=", "torch", ".", "FloatTensor", "(", "padded_feature", ")", "\n", "y", "=", "torch", ".", "FloatTensor", "(", "padded_label", ")", "# (clip_len//time_step_size,)", "\n", "seq_len", "=", "torch", ".", "LongTensor", "(", "[", "seq_len", "]", ")", "# (1,)", "\n", "\n", "# return (x, y, seq_len, writeout_fn)", "\n", "# return (x, y, seq_len, is_seizure)", "\n", "\n", "if", "self", ".", "use_fft", ":", "\n", "# reshape such that x is (L,19*100)", "\n", "            ", "x", "=", "x", ".", "view", "(", "x", ".", "shape", "[", "0", "]", ",", "-", "1", ")", "\n", "", "else", ":", "\n", "# if raw, reshape such that x is (L,19) where L = clip_len*200", "\n", "            ", "x", "=", "x", ".", "transpose", "(", "1", ",", "2", ")", "\n", "x", "=", "x", ".", "reshape", "(", "-", "1", ",", "x", ".", "shape", "[", "2", "]", ")", "\n", "\n", "", "return", "(", "x", ",", "is_seizure", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.eeg.dataloader.computeSliceMatrix": [[40, 162], ["int", "numpy.zeros().astype", "dataset_name.lower", "numpy.minimum", "int", "numpy.stack", "datasets.eeg.data_utils.getSeizureTimes", "int", "int", "h5py.File", "time_steps.append", "numpy.minimum", "numpy.zeros", "int", "int", "numpy.maximum", "numpy.minimum", "int", "int", "datasets.eeg.data_utils.computeFFT", "h5py.File", "edf_fn.split", "numpy.floor", "numpy.ceil", "int", "int"], "function", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.eeg.data_utils.getSeizureTimes", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.eeg.data_utils.computeFFT"], ["def", "computeSliceMatrix", "(", "\n", "edf_fn", ",", "\n", "channel_names", ",", "\n", "clip_idx", ",", "\n", "h5_fn", "=", "None", ",", "\n", "time_step_size", "=", "1", ",", "\n", "clip_len", "=", "60", ",", "\n", "stride", "=", "60", ",", "\n", "is_fft", "=", "False", ",", "\n", "use_cnn_features", "=", "False", ",", "\n", "dataset_name", "=", "\"TUH\"", ",", "\n", "min_sz_len", "=", "5", ",", "\n", ")", ":", "\n", "    ", "\"\"\"\n    Comvert entire EEG sequence into clips of length clip_len\n    Args:\n        edf_fn: edf/eeghdf file name, full path\n        channel_names: list of channel names\n        clip_idx: index of current clip/sliding window, int\n        h5_fn: file name of resampled signal h5 file (full path)\n        time_step_size: length of each time step, in seconds, int\n        clip_len: sliding window size or EEG clip length, in seconds, int\n        stride: stride size, by how many seconds the sliding window moves, int\n        is_fft: whether to perform Fourier Transform on raw EEG data\n        use_cnn_features: whether or not the input is CNN features\n        dataset_name: name of the dataset, \"TUH\", \"Stanford\" or \"LPCH\"\n        min_sz_len: minimum length to be considered as a seizure, in seconds\n    Returns:\n        eeg_clip: EEG clip, shape (clip_len//time_step_size, num_channels, time_step_size*freq)\n        seizure_labels: per-time-step seizure labels, shape (clip_len//time_step_size,)\n        is_seizure: overall label, 1 if at least one seizure within this clip, otherwise 0\n    \"\"\"", "\n", "if", "dataset_name", ".", "lower", "(", ")", "not", "in", "[", "\"tuh\"", ",", "\"stanford\"", ",", "\"lpch\"", "]", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n", "", "physical_clip_len", "=", "int", "(", "FREQUENCY", "*", "clip_len", ")", "\n", "\n", "start_window", "=", "clip_idx", "*", "FREQUENCY", "*", "stride", "\n", "\n", "if", "not", "use_cnn_features", ":", "\n", "        ", "with", "h5py", ".", "File", "(", "h5_fn", ",", "\"r\"", ")", "as", "f", ":", "\n", "            ", "signal_array", "=", "f", "[", "\"resampled_signal\"", "]", "[", "(", ")", "]", "\n", "resampled_freq", "=", "f", "[", "\"resample_freq\"", "]", "[", "(", ")", "]", "\n", "assert", "resampled_freq", "==", "FREQUENCY", "\n", "\n", "# (num_channels, physical_clip_len)", "\n", "", "end_window", "=", "np", ".", "minimum", "(", "\n", "signal_array", ".", "shape", "[", "-", "1", "]", ",", "start_window", "+", "physical_clip_len", "\n", ")", "\n", "curr_slc", "=", "signal_array", "[", "\n", ":", ",", "start_window", ":", "end_window", "\n", "]", "# (num_channels, FREQ*clip_len)", "\n", "physical_time_step_size", "=", "int", "(", "FREQUENCY", "*", "time_step_size", ")", "\n", "\n", "start_time_step", "=", "0", "\n", "time_steps", "=", "[", "]", "\n", "while", "start_time_step", "<=", "curr_slc", ".", "shape", "[", "1", "]", "-", "physical_time_step_size", ":", "\n", "            ", "end_time_step", "=", "start_time_step", "+", "physical_time_step_size", "\n", "# (num_channels, physical_time_step_size)", "\n", "curr_time_step", "=", "curr_slc", "[", ":", ",", "start_time_step", ":", "end_time_step", "]", "\n", "if", "is_fft", ":", "\n", "# curr_time_step, _ = computeFFT(curr_time_step, n=FREQUENCY)", "\n", "                ", "curr_time_step", ",", "_", "=", "computeFFT", "(", "\n", "curr_time_step", ",", "n", "=", "curr_time_step", ".", "shape", "[", "-", "1", "]", "\n", ")", "\n", "\n", "", "time_steps", ".", "append", "(", "curr_time_step", ")", "\n", "start_time_step", "=", "end_time_step", "\n", "\n", "", "eeg_clip", "=", "np", ".", "stack", "(", "time_steps", ",", "axis", "=", "0", ")", "\n", "", "elif", "use_cnn_features", ":", "\n", "        ", "with", "h5py", ".", "File", "(", "h5_fn", ",", "\"r\"", ")", "as", "hf", ":", "\n", "            ", "cnn_features", "=", "hf", "[", "\"cnn_features\"", "]", "[", "\n", "(", ")", "\n", "]", "# (num_total_time_steps, cnn_feature_dim)", "\n", "# start_ts_idx = clip_idx * (clip_len // time_step_size)", "\n", "", "start_ts_idx", "=", "clip_idx", "*", "(", "stride", "//", "time_step_size", ")", "\n", "end_ts_idx", "=", "np", ".", "minimum", "(", "\n", "cnn_features", ".", "shape", "[", "0", "]", ",", "start_ts_idx", "+", "clip_len", "//", "time_step_size", "\n", ")", "\n", "eeg_clip", "=", "cnn_features", "[", "\n", "start_ts_idx", ":", "end_ts_idx", ",", ":", "\n", "]", "# (num_time_steps, cnn_feature_dim)", "\n", "end_window", "=", "end_ts_idx", "*", "time_step_size", "*", "FREQUENCY", "\n", "\n", "# get seizure times, take min_sz_len into account", "\n", "", "if", "\".edf\"", "in", "edf_fn", ":", "\n", "# TODO: Also enforce a min_sz_len for TUH annotations?", "\n", "        ", "seizure_times_raw", "=", "getSeizureTimes", "(", "edf_fn", ".", "split", "(", "\".edf\"", ")", "[", "0", "]", ")", "\n", "seizure_times", "=", "[", "\n", "sz_time", "\n", "for", "sz_time", "in", "seizure_times_raw", "\n", "if", "(", "sz_time", "[", "1", "]", "-", "sz_time", "[", "0", "]", ")", ">", "min_sz_len", "\n", "]", "\n", "", "else", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n", "# get per-time-step seizure labels", "\n", "", "num_time_steps", "=", "eeg_clip", ".", "shape", "[", "0", "]", "\n", "seizure_labels", "=", "np", ".", "zeros", "(", "(", "num_time_steps", ")", ")", ".", "astype", "(", "int", ")", "\n", "is_seizure", "=", "0", "\n", "for", "t", "in", "seizure_times", ":", "\n", "        ", "start_t", "=", "int", "(", "t", "[", "0", "]", "*", "FREQUENCY", ")", "\n", "end_t", "=", "int", "(", "t", "[", "1", "]", "*", "FREQUENCY", ")", "\n", "if", "not", "(", "(", "end_window", "<", "start_t", ")", "or", "(", "start_window", ">", "end_t", ")", ")", ":", "\n", "            ", "is_seizure", "=", "1", "\n", "\n", "start_t_sec", "=", "int", "(", "t", "[", "0", "]", ")", "# start of seizure in int seconds", "\n", "end_t_sec", "=", "int", "(", "t", "[", "1", "]", ")", "# end of seizure in int seconds", "\n", "\n", "# shift start_t_sec and end_t_sec so that they start at current clip", "\n", "start_t_sec", "=", "np", ".", "maximum", "(", "0", ",", "start_t_sec", "-", "int", "(", "start_window", "/", "FREQUENCY", ")", ")", "\n", "end_t_sec", "=", "np", ".", "minimum", "(", "clip_len", ",", "end_t_sec", "-", "int", "(", "start_window", "/", "FREQUENCY", ")", ")", "\n", "# print(\"start_t_sec: {}; end_t_sec: {}\".format(start_t_sec, end_t_sec))", "\n", "\n", "# time step size may not be 1-sec", "\n", "start_time_step", "=", "int", "(", "np", ".", "floor", "(", "start_t_sec", "/", "time_step_size", ")", ")", "\n", "end_time_step", "=", "int", "(", "np", ".", "ceil", "(", "end_t_sec", "/", "time_step_size", ")", ")", "\n", "\n", "seizure_labels", "[", "start_time_step", ":", "end_time_step", "]", "=", "1", "\n", "\n", "", "", "return", "eeg_clip", ",", "seizure_labels", ",", "is_seizure", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.eeg.dataloader.load_dataset": [[436, 539], ["datasets.eeg.data_utils.StandardScaler", "dataset_name.lower", "dataloader.SeizureDataset", "torch.utils.data.DataLoader", "open", "pickle.load", "open", "pickle.load"], "function", ["None"], ["", "", "def", "load_dataset", "(", "\n", "input_dir", ",", "\n", "raw_data_dir", ",", "\n", "train_batch_size", "=", "64", ",", "\n", "test_batch_size", "=", "64", ",", "\n", "clip_len", "=", "60", ",", "\n", "time_step_size", "=", "1", ",", "\n", "stride", "=", "60", ",", "\n", "standardize", "=", "False", ",", "\n", "num_workers", "=", "8", ",", "\n", "padding", "=", "False", ",", "\n", "padding_val", "=", "0.0", ",", "\n", "augmentation", "=", "False", ",", "\n", "balance_train", "=", "False", ",", "\n", "use_fft", "=", "False", ",", "\n", "use_cnn_features", "=", "False", ",", "\n", "eval_clip_len", "=", "None", ",", "\n", "dataset_name", "=", "\"TUH\"", ",", "\n", "min_sz_len", "=", "5", ",", "\n", "cv_fold", "=", "None", ",", "\n", "means_dir", "=", "None", ",", "\n", "stds_dir", "=", "None", ",", "\n", ")", ":", "\n", "    ", "\"\"\"\n    Args:\n        input_dir: dir to preprocessed h5 file\n        train_batch_size: int\n        test_batch_size: int\n        standardize: if True, will z-normalize wrt train set\n        num_workers: int\n        padding_val: value used for padding\n        augmentation: if True, perform random augmentation of EEG seq\n    Returns:\n        dataloaders, datasets: dictionaries of train/dev/test dataloaders and datasets\n    \"\"\"", "\n", "\n", "if", "standardize", ":", "\n", "        ", "if", "(", "means_dir", "is", "not", "None", ")", "and", "(", "stds_dir", "is", "not", "None", ")", ":", "\n", "            ", "means_file", "=", "means_dir", "\n", "stds_file", "=", "stds_dir", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "\n", "", "with", "open", "(", "means_file", ",", "\"rb\"", ")", "as", "f", ":", "\n", "            ", "means", "=", "pickle", ".", "load", "(", "f", ")", "\n", "", "with", "open", "(", "stds_file", ",", "\"rb\"", ")", "as", "f", ":", "\n", "            ", "stds", "=", "pickle", ".", "load", "(", "f", ")", "\n", "\n", "", "scaler", "=", "StandardScaler", "(", "mean", "=", "means", ",", "std", "=", "stds", ")", "\n", "", "else", ":", "\n", "        ", "scaler", "=", "None", "\n", "\n", "", "dataloaders", "=", "{", "}", "\n", "if", "dataset_name", ".", "lower", "(", ")", "==", "\"tuh\"", ":", "\n", "        ", "splits", "=", "[", "\"train\"", ",", "\"dev\"", ",", "\"test\"", "]", "\n", "", "else", ":", "\n", "        ", "splits", "=", "[", "\"train\"", ",", "\"test\"", "]", "\n", "", "for", "split", "in", "splits", ":", "\n", "        ", "if", "split", "==", "\"train\"", ":", "\n", "            ", "data_augment", "=", "augmentation", "\n", "", "else", ":", "\n", "            ", "data_augment", "=", "False", "# never do augmentation on dev/test sets", "\n", "\n", "# allows different clip length for evaluation", "\n", "", "if", "(", "eval_clip_len", "is", "not", "None", ")", "and", "(", "split", "!=", "\"train\"", ")", ":", "\n", "            ", "curr_clip_len", "=", "eval_clip_len", "\n", "", "else", ":", "\n", "            ", "curr_clip_len", "=", "clip_len", "\n", "", "dataset", "=", "SeizureDataset", "(", "\n", "input_dir", "=", "input_dir", ",", "\n", "raw_data_dir", "=", "raw_data_dir", ",", "\n", "time_step_size", "=", "time_step_size", ",", "\n", "clip_len", "=", "curr_clip_len", ",", "\n", "stride", "=", "stride", ",", "\n", "standardize", "=", "standardize", ",", "\n", "scaler", "=", "scaler", ",", "\n", "split", "=", "split", ",", "\n", "balance_train", "=", "balance_train", ",", "\n", "data_augment", "=", "data_augment", ",", "\n", "use_fft", "=", "use_fft", ",", "\n", "use_cnn_features", "=", "use_cnn_features", ",", "\n", "padding", "=", "padding", ",", "\n", "padding_val", "=", "padding_val", ",", "\n", "dataset_name", "=", "dataset_name", ",", "\n", "min_sz_len", "=", "min_sz_len", ",", "\n", "cv_fold", "=", "cv_fold", ",", "\n", ")", "\n", "\n", "if", "split", "==", "\"train\"", ":", "\n", "            ", "shuffle", "=", "True", "\n", "batch_size", "=", "train_batch_size", "\n", "", "else", ":", "\n", "            ", "shuffle", "=", "False", "\n", "batch_size", "=", "test_batch_size", "\n", "\n", "", "loader", "=", "DataLoader", "(", "\n", "dataset", "=", "dataset", ",", "\n", "shuffle", "=", "shuffle", ",", "\n", "batch_size", "=", "batch_size", ",", "\n", "num_workers", "=", "num_workers", ",", "\n", ")", "\n", "dataloaders", "[", "split", "]", "=", "loader", "\n", "\n", "", "return", "dataloaders", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.bidmc.data_loader.load_from_tsfile_to_dataframe": [[41, 547], ["open", "tqdm.tqdm", "pandas.DataFrame", "range", "data_loader.TsFileParseException", "line.replace.strip().lower", "data_loader.TsFileParseException", "line.replace.startswith", "data_loader.TsFileParseException", "pandas.Series", "line.replace.strip", "line.replace.split", "len", "line.replace.startswith", "data_loader.TsFileParseException", "numpy.asarray", "data_loader.TsFileParseException", "data_loader.TsFileParseException", "line.replace.split", "len", "line.replace.startswith", "len", "str", "data_loader.TsFileParseException", "data_loader.TsFileParseException", "line.replace.split", "len", "line.replace.startswith", "len", "data_loader.TsFileParseException", "data_loader.TsFileParseException", "line.replace.split", "len", "line.replace.startswith", "data_loader.TsFileParseException", "data_loader.TsFileParseException", "data_loader.TsFileParseException", "data_loader.TsFileParseException", "token.strip", "line.replace.split", "len", "line.replace.startswith", "data_loader.TsFileParseException", "data_loader.TsFileParseException", "data_loader.TsFileParseException", "data_loader.TsFileParseException", "data_loader.TsFileParseException", "data_loader.TsFileParseException", "data_loader.TsFileParseException", "line.replace.replace", "data_loader.TsFileParseException", "len", "line.replace.split", "len", "range", "data_loader.TsFileParseException", "data_loader.TsFileParseException", "data_loader.TsFileParseException", "len", "range", "data_loader.TsFileParseException", "dimensions[].strip", "class_val_list.append", "str.isspace", "data_loader.TsFileParseException", "len", "instance_list.append", "dimensions[].strip.split", "instance_list[].append", "instance_list[].append", "float", "instance_list[].append", "data_loader.TsFileParseException", "data_loader.TsFileParseException", "instance_list[].append", "str", "float", "pandas.Series", "pandas.Series", "dimensions[].strip", "len", "instance_list.append", "pandas.Series", "line[].strip", "class_val_list.append", "tuple_data.rfind", "data_loader.TsFileParseException", "str", "len", "instance_list.append", "pandas.Series", "str", "float", "data_loader.TsFileParseException", "data_loader.TsFileParseException", "str.isspace", "data_loader.TsFileParseException", "float", "int", "data_loader.TsFileParseException", "data_loader.TsFileParseException", "data_loader.TsFileParseException", "data_loader.TsFileParseException", "instance_list[].append", "instance_list[].append", "str", "str", "data_loader.TsFileParseException", "float", "timestamp.strip.strip", "len", "instance_list.append", "pandas.DatetimeIndex", "pandas.Series", "str", "len", "instance_list.append", "pandas.Series", "str", "str", "str", "str", "str", "str", "str", "str", "str", "str", "str", "str", "str", "str", "str", "str", "str", "str", "str", "str", "str", "str"], "function", ["None"], ["", "def", "load_from_tsfile_to_dataframe", "(", "full_file_path_and_name", ",", "return_separate_X_and_y", "=", "True", ",", "\n", "replace_missing_vals_with", "=", "'NaN'", ")", ":", "\n", "    ", "\"\"\"Loads data from a .ts file into a Pandas DataFrame.\n    Parameters\n    ----------\n    full_file_path_and_name: str\n        The full pathname of the .ts file to read.\n    return_separate_X_and_y: bool\n        true if X and Y values should be returned as separate Data Frames (X) and a numpy array (y), false otherwise.\n        This is only relevant for data that\n    replace_missing_vals_with: str\n       The value that missing values in the text file should be replaced with prior to parsing.\n    Returns\n    -------\n    DataFrame, ndarray\n        If return_separate_X_and_y then a tuple containing a DataFrame and a numpy array containing the relevant time-series and corresponding class values.\n    DataFrame\n        If not return_separate_X_and_y then a single DataFrame containing all time-series and (if relevant) a column \"class_vals\" the associated class values.\n    \"\"\"", "\n", "\n", "# Initialize flags and variables used when parsing the file", "\n", "metadata_started", "=", "False", "\n", "data_started", "=", "False", "\n", "\n", "has_problem_name_tag", "=", "False", "\n", "has_timestamps_tag", "=", "False", "\n", "has_univariate_tag", "=", "False", "\n", "has_class_labels_tag", "=", "False", "\n", "has_target_labels_tag", "=", "False", "\n", "has_data_tag", "=", "False", "\n", "\n", "previous_timestamp_was_float", "=", "None", "\n", "previous_timestamp_was_int", "=", "None", "\n", "previous_timestamp_was_timestamp", "=", "None", "\n", "num_dimensions", "=", "None", "\n", "is_first_case", "=", "True", "\n", "instance_list", "=", "[", "]", "\n", "class_val_list", "=", "[", "]", "\n", "line_num", "=", "0", "\n", "\n", "# Parse the file", "\n", "# print(full_file_path_and_name)", "\n", "# with open(full_file_path_and_name, 'r', encoding='utf-8') as file:", "\n", "with", "open", "(", "full_file_path_and_name", ",", "'r'", ",", "encoding", "=", "'latin1'", ")", "as", "file", ":", "\n", "        ", "for", "line", "in", "tqdm", "(", "file", ")", ":", "\n", "# print(\".\", end='')", "\n", "# Strip white space from start/end of line and change to lowercase for use below", "\n", "            ", "line", "=", "line", ".", "strip", "(", ")", ".", "lower", "(", ")", "\n", "# Empty lines are valid at any point in a file", "\n", "if", "line", ":", "\n", "# Check if this line contains metadata", "\n", "# Please note that even though metadata is stored in this function it is not currently published externally", "\n", "                ", "if", "line", ".", "startswith", "(", "\"@problemname\"", ")", ":", "\n", "# Check that the data has not started", "\n", "                    ", "if", "data_started", ":", "\n", "                        ", "raise", "TsFileParseException", "(", "\"metadata must come before data\"", ")", "\n", "# Check that the associated value is valid", "\n", "", "tokens", "=", "line", ".", "split", "(", "' '", ")", "\n", "token_len", "=", "len", "(", "tokens", ")", "\n", "\n", "if", "token_len", "==", "1", ":", "\n", "                        ", "raise", "TsFileParseException", "(", "\"problemname tag requires an associated value\"", ")", "\n", "\n", "", "problem_name", "=", "line", "[", "len", "(", "\"@problemname\"", ")", "+", "1", ":", "]", "\n", "has_problem_name_tag", "=", "True", "\n", "metadata_started", "=", "True", "\n", "", "elif", "line", ".", "startswith", "(", "\"@timestamps\"", ")", ":", "\n", "# Check that the data has not started", "\n", "                    ", "if", "data_started", ":", "\n", "                        ", "raise", "TsFileParseException", "(", "\"metadata must come before data\"", ")", "\n", "\n", "# Check that the associated value is valid", "\n", "", "tokens", "=", "line", ".", "split", "(", "' '", ")", "\n", "token_len", "=", "len", "(", "tokens", ")", "\n", "\n", "if", "token_len", "!=", "2", ":", "\n", "                        ", "raise", "TsFileParseException", "(", "\"timestamps tag requires an associated Boolean value\"", ")", "\n", "", "elif", "tokens", "[", "1", "]", "==", "\"true\"", ":", "\n", "                        ", "timestamps", "=", "True", "\n", "", "elif", "tokens", "[", "1", "]", "==", "\"false\"", ":", "\n", "                        ", "timestamps", "=", "False", "\n", "", "else", ":", "\n", "                        ", "raise", "TsFileParseException", "(", "\"invalid timestamps value\"", ")", "\n", "", "has_timestamps_tag", "=", "True", "\n", "metadata_started", "=", "True", "\n", "", "elif", "line", ".", "startswith", "(", "\"@univariate\"", ")", ":", "\n", "# Check that the data has not started", "\n", "                    ", "if", "data_started", ":", "\n", "                        ", "raise", "TsFileParseException", "(", "\"metadata must come before data\"", ")", "\n", "\n", "# Check that the associated value is valid", "\n", "", "tokens", "=", "line", ".", "split", "(", "' '", ")", "\n", "token_len", "=", "len", "(", "tokens", ")", "\n", "if", "token_len", "!=", "2", ":", "\n", "                        ", "raise", "TsFileParseException", "(", "\"univariate tag requires an associated Boolean value\"", ")", "\n", "", "elif", "tokens", "[", "1", "]", "==", "\"true\"", ":", "\n", "                        ", "univariate", "=", "True", "\n", "", "elif", "tokens", "[", "1", "]", "==", "\"false\"", ":", "\n", "                        ", "univariate", "=", "False", "\n", "", "else", ":", "\n", "                        ", "raise", "TsFileParseException", "(", "\"invalid univariate value\"", ")", "\n", "\n", "", "has_univariate_tag", "=", "True", "\n", "metadata_started", "=", "True", "\n", "", "elif", "line", ".", "startswith", "(", "\"@classlabel\"", ")", ":", "\n", "# Check that the data has not started", "\n", "                    ", "if", "data_started", ":", "\n", "                        ", "raise", "TsFileParseException", "(", "\"metadata must come before data\"", ")", "\n", "\n", "# Check that the associated value is valid", "\n", "", "tokens", "=", "line", ".", "split", "(", "' '", ")", "\n", "token_len", "=", "len", "(", "tokens", ")", "\n", "\n", "if", "token_len", "==", "1", ":", "\n", "                        ", "raise", "TsFileParseException", "(", "\"classlabel tag requires an associated Boolean value\"", ")", "\n", "\n", "", "if", "tokens", "[", "1", "]", "==", "\"true\"", ":", "\n", "                        ", "class_labels", "=", "True", "\n", "", "elif", "tokens", "[", "1", "]", "==", "\"false\"", ":", "\n", "                        ", "class_labels", "=", "False", "\n", "", "else", ":", "\n", "                        ", "raise", "TsFileParseException", "(", "\"invalid classLabel value\"", ")", "\n", "\n", "# Check if we have any associated class values", "\n", "", "if", "token_len", "==", "2", "and", "class_labels", ":", "\n", "                        ", "raise", "TsFileParseException", "(", "\"if the classlabel tag is true then class values must be supplied\"", ")", "\n", "\n", "", "has_class_labels_tag", "=", "True", "\n", "class_label_list", "=", "[", "token", ".", "strip", "(", ")", "for", "token", "in", "tokens", "[", "2", ":", "]", "]", "\n", "metadata_started", "=", "True", "\n", "", "elif", "line", ".", "startswith", "(", "\"@targetlabel\"", ")", ":", "\n", "# Check that the data has not started", "\n", "                    ", "if", "data_started", ":", "\n", "                        ", "raise", "TsFileParseException", "(", "\"metadata must come before data\"", ")", "\n", "\n", "# Check that the associated value is valid", "\n", "", "tokens", "=", "line", ".", "split", "(", "' '", ")", "\n", "token_len", "=", "len", "(", "tokens", ")", "\n", "\n", "if", "token_len", "==", "1", ":", "\n", "                        ", "raise", "TsFileParseException", "(", "\"targetlabel tag requires an associated Boolean value\"", ")", "\n", "\n", "", "if", "tokens", "[", "1", "]", "==", "\"true\"", ":", "\n", "                        ", "target_labels", "=", "True", "\n", "", "elif", "tokens", "[", "1", "]", "==", "\"false\"", ":", "\n", "                        ", "target_labels", "=", "False", "\n", "", "else", ":", "\n", "                        ", "raise", "TsFileParseException", "(", "\"invalid targetLabel value\"", ")", "\n", "\n", "", "has_target_labels_tag", "=", "True", "\n", "class_val_list", "=", "[", "]", "\n", "metadata_started", "=", "True", "\n", "# Check if this line contains the start of data", "\n", "", "elif", "line", ".", "startswith", "(", "\"@data\"", ")", ":", "\n", "                    ", "if", "line", "!=", "\"@data\"", ":", "\n", "                        ", "raise", "TsFileParseException", "(", "\"data tag should not have an associated value\"", ")", "\n", "\n", "", "if", "data_started", "and", "not", "metadata_started", ":", "\n", "                        ", "raise", "TsFileParseException", "(", "\"metadata must come before data\"", ")", "\n", "", "else", ":", "\n", "                        ", "has_data_tag", "=", "True", "\n", "data_started", "=", "True", "\n", "# If the 'data tag has been found then metadata has been parsed and data can be loaded", "\n", "", "", "elif", "data_started", ":", "\n", "# Check that a full set of metadata has been provided", "\n", "                    ", "incomplete_regression_meta_data", "=", "not", "has_problem_name_tag", "or", "not", "has_timestamps_tag", "or", "not", "has_univariate_tag", "or", "not", "has_target_labels_tag", "or", "not", "has_data_tag", "\n", "incomplete_classification_meta_data", "=", "not", "has_problem_name_tag", "or", "not", "has_timestamps_tag", "or", "not", "has_univariate_tag", "or", "not", "has_class_labels_tag", "or", "not", "has_data_tag", "\n", "if", "incomplete_regression_meta_data", "and", "incomplete_classification_meta_data", ":", "\n", "                        ", "raise", "TsFileParseException", "(", "\"a full set of metadata has not been provided before the data\"", ")", "\n", "\n", "# Replace any missing values with the value specified", "\n", "", "line", "=", "line", ".", "replace", "(", "\"?\"", ",", "replace_missing_vals_with", ")", "\n", "\n", "# Check if we dealing with data that has timestamps", "\n", "if", "timestamps", ":", "\n", "# We're dealing with timestamps so cannot just split line on ':' as timestamps may contain one", "\n", "                        ", "has_another_value", "=", "False", "\n", "has_another_dimension", "=", "False", "\n", "\n", "timestamps_for_dimension", "=", "[", "]", "\n", "values_for_dimension", "=", "[", "]", "\n", "\n", "this_line_num_dimensions", "=", "0", "\n", "line_len", "=", "len", "(", "line", ")", "\n", "char_num", "=", "0", "\n", "\n", "while", "char_num", "<", "line_len", ":", "\n", "# Move through any spaces", "\n", "                            ", "while", "char_num", "<", "line_len", "and", "str", ".", "isspace", "(", "line", "[", "char_num", "]", ")", ":", "\n", "                                ", "char_num", "+=", "1", "\n", "\n", "# See if there is any more data to read in or if we should validate that read thus far", "\n", "\n", "", "if", "char_num", "<", "line_len", ":", "\n", "\n", "# See if we have an empty dimension (i.e. no values)", "\n", "                                ", "if", "line", "[", "char_num", "]", "==", "\":\"", ":", "\n", "                                    ", "if", "len", "(", "instance_list", ")", "<", "(", "this_line_num_dimensions", "+", "1", ")", ":", "\n", "                                        ", "instance_list", ".", "append", "(", "[", "]", ")", "\n", "\n", "", "instance_list", "[", "this_line_num_dimensions", "]", ".", "append", "(", "pd", ".", "Series", "(", ")", ")", "\n", "this_line_num_dimensions", "+=", "1", "\n", "\n", "has_another_value", "=", "False", "\n", "has_another_dimension", "=", "True", "\n", "\n", "timestamps_for_dimension", "=", "[", "]", "\n", "values_for_dimension", "=", "[", "]", "\n", "\n", "char_num", "+=", "1", "\n", "", "else", ":", "\n", "# Check if we have reached a class label", "\n", "                                    ", "if", "line", "[", "char_num", "]", "!=", "\"(\"", "and", "target_labels", ":", "\n", "                                        ", "class_val", "=", "line", "[", "char_num", ":", "]", ".", "strip", "(", ")", "\n", "\n", "# if class_val not in class_val_list:", "\n", "#     raise TsFileParseException(", "\n", "#         \"the class value '\" + class_val + \"' on line \" + str(", "\n", "#             line_num + 1) + \" is not valid\")", "\n", "\n", "class_val_list", ".", "append", "(", "float", "(", "class_val", ")", ")", "\n", "char_num", "=", "line_len", "\n", "\n", "has_another_value", "=", "False", "\n", "has_another_dimension", "=", "False", "\n", "\n", "timestamps_for_dimension", "=", "[", "]", "\n", "values_for_dimension", "=", "[", "]", "\n", "\n", "", "else", ":", "\n", "\n", "# Read in the data contained within the next tuple", "\n", "\n", "                                        ", "if", "line", "[", "char_num", "]", "!=", "\"(\"", "and", "not", "target_labels", ":", "\n", "                                            ", "raise", "TsFileParseException", "(", "\n", "\"dimension \"", "+", "str", "(", "this_line_num_dimensions", "+", "1", ")", "+", "\" on line \"", "+", "str", "(", "\n", "line_num", "+", "1", ")", "+", "\" does not start with a '('\"", ")", "\n", "\n", "", "char_num", "+=", "1", "\n", "tuple_data", "=", "\"\"", "\n", "\n", "while", "char_num", "<", "line_len", "and", "line", "[", "char_num", "]", "!=", "\")\"", ":", "\n", "                                            ", "tuple_data", "+=", "line", "[", "char_num", "]", "\n", "char_num", "+=", "1", "\n", "\n", "", "if", "char_num", ">=", "line_len", "or", "line", "[", "char_num", "]", "!=", "\")\"", ":", "\n", "                                            ", "raise", "TsFileParseException", "(", "\n", "\"dimension \"", "+", "str", "(", "this_line_num_dimensions", "+", "1", ")", "+", "\" on line \"", "+", "str", "(", "\n", "line_num", "+", "1", ")", "+", "\" does not end with a ')'\"", ")", "\n", "\n", "# Read in any spaces immediately after the current tuple", "\n", "\n", "", "char_num", "+=", "1", "\n", "\n", "while", "char_num", "<", "line_len", "and", "str", ".", "isspace", "(", "line", "[", "char_num", "]", ")", ":", "\n", "                                            ", "char_num", "+=", "1", "\n", "\n", "# Check if there is another value or dimension to process after this tuple", "\n", "\n", "", "if", "char_num", ">=", "line_len", ":", "\n", "                                            ", "has_another_value", "=", "False", "\n", "has_another_dimension", "=", "False", "\n", "\n", "", "elif", "line", "[", "char_num", "]", "==", "\",\"", ":", "\n", "                                            ", "has_another_value", "=", "True", "\n", "has_another_dimension", "=", "False", "\n", "\n", "", "elif", "line", "[", "char_num", "]", "==", "\":\"", ":", "\n", "                                            ", "has_another_value", "=", "False", "\n", "has_another_dimension", "=", "True", "\n", "\n", "", "char_num", "+=", "1", "\n", "\n", "# Get the numeric value for the tuple by reading from the end of the tuple data backwards to the last comma", "\n", "\n", "last_comma_index", "=", "tuple_data", ".", "rfind", "(", "','", ")", "\n", "\n", "if", "last_comma_index", "==", "-", "1", ":", "\n", "                                            ", "raise", "TsFileParseException", "(", "\n", "\"dimension \"", "+", "str", "(", "this_line_num_dimensions", "+", "1", ")", "+", "\" on line \"", "+", "str", "(", "\n", "line_num", "+", "1", ")", "+", "\" contains a tuple that has no comma inside of it\"", ")", "\n", "\n", "", "try", ":", "\n", "                                            ", "value", "=", "tuple_data", "[", "last_comma_index", "+", "1", ":", "]", "\n", "value", "=", "float", "(", "value", ")", "\n", "\n", "", "except", "ValueError", ":", "\n", "                                            ", "raise", "TsFileParseException", "(", "\n", "\"dimension \"", "+", "str", "(", "this_line_num_dimensions", "+", "1", ")", "+", "\" on line \"", "+", "str", "(", "\n", "line_num", "+", "1", ")", "+", "\" contains a tuple that does not have a valid numeric value\"", ")", "\n", "\n", "# Check the type of timestamp that we have", "\n", "\n", "", "timestamp", "=", "tuple_data", "[", "0", ":", "last_comma_index", "]", "\n", "\n", "try", ":", "\n", "                                            ", "timestamp", "=", "int", "(", "timestamp", ")", "\n", "timestamp_is_int", "=", "True", "\n", "timestamp_is_timestamp", "=", "False", "\n", "", "except", "ValueError", ":", "\n", "                                            ", "timestamp_is_int", "=", "False", "\n", "\n", "", "if", "not", "timestamp_is_int", ":", "\n", "                                            ", "try", ":", "\n", "                                                ", "timestamp", "=", "float", "(", "timestamp", ")", "\n", "timestamp_is_float", "=", "True", "\n", "timestamp_is_timestamp", "=", "False", "\n", "", "except", "ValueError", ":", "\n", "                                                ", "timestamp_is_float", "=", "False", "\n", "\n", "", "", "if", "not", "timestamp_is_int", "and", "not", "timestamp_is_float", ":", "\n", "                                            ", "try", ":", "\n", "                                                ", "timestamp", "=", "timestamp", ".", "strip", "(", ")", "\n", "timestamp_is_timestamp", "=", "True", "\n", "", "except", "ValueError", ":", "\n", "                                                ", "timestamp_is_timestamp", "=", "False", "\n", "\n", "# Make sure that the timestamps in the file (not just this dimension or case) are consistent", "\n", "\n", "", "", "if", "not", "timestamp_is_timestamp", "and", "not", "timestamp_is_int", "and", "not", "timestamp_is_float", ":", "\n", "                                            ", "raise", "TsFileParseException", "(", "\n", "\"dimension \"", "+", "str", "(", "this_line_num_dimensions", "+", "1", ")", "+", "\" on line \"", "+", "str", "(", "\n", "line_num", "+", "1", ")", "+", "\" contains a tuple that has an invalid timestamp '\"", "+", "timestamp", "+", "\"'\"", ")", "\n", "\n", "", "if", "previous_timestamp_was_float", "is", "not", "None", "and", "previous_timestamp_was_float", "and", "not", "timestamp_is_float", ":", "\n", "                                            ", "raise", "TsFileParseException", "(", "\n", "\"dimension \"", "+", "str", "(", "this_line_num_dimensions", "+", "1", ")", "+", "\" on line \"", "+", "str", "(", "\n", "line_num", "+", "1", ")", "+", "\" contains tuples where the timestamp format is inconsistent\"", ")", "\n", "\n", "", "if", "previous_timestamp_was_int", "is", "not", "None", "and", "previous_timestamp_was_int", "and", "not", "timestamp_is_int", ":", "\n", "                                            ", "raise", "TsFileParseException", "(", "\n", "\"dimension \"", "+", "str", "(", "this_line_num_dimensions", "+", "1", ")", "+", "\" on line \"", "+", "str", "(", "\n", "line_num", "+", "1", ")", "+", "\" contains tuples where the timestamp format is inconsistent\"", ")", "\n", "\n", "", "if", "previous_timestamp_was_timestamp", "is", "not", "None", "and", "previous_timestamp_was_timestamp", "and", "not", "timestamp_is_timestamp", ":", "\n", "                                            ", "raise", "TsFileParseException", "(", "\n", "\"dimension \"", "+", "str", "(", "this_line_num_dimensions", "+", "1", ")", "+", "\" on line \"", "+", "str", "(", "\n", "line_num", "+", "1", ")", "+", "\" contains tuples where the timestamp format is inconsistent\"", ")", "\n", "\n", "# Store the values", "\n", "\n", "", "timestamps_for_dimension", "+=", "[", "timestamp", "]", "\n", "values_for_dimension", "+=", "[", "value", "]", "\n", "\n", "#  If this was our first tuple then we store the type of timestamp we had", "\n", "\n", "if", "previous_timestamp_was_timestamp", "is", "None", "and", "timestamp_is_timestamp", ":", "\n", "                                            ", "previous_timestamp_was_timestamp", "=", "True", "\n", "previous_timestamp_was_int", "=", "False", "\n", "previous_timestamp_was_float", "=", "False", "\n", "\n", "", "if", "previous_timestamp_was_int", "is", "None", "and", "timestamp_is_int", ":", "\n", "                                            ", "previous_timestamp_was_timestamp", "=", "False", "\n", "previous_timestamp_was_int", "=", "True", "\n", "previous_timestamp_was_float", "=", "False", "\n", "\n", "", "if", "previous_timestamp_was_float", "is", "None", "and", "timestamp_is_float", ":", "\n", "                                            ", "previous_timestamp_was_timestamp", "=", "False", "\n", "previous_timestamp_was_int", "=", "False", "\n", "previous_timestamp_was_float", "=", "True", "\n", "\n", "# See if we should add the data for this dimension", "\n", "\n", "", "if", "not", "has_another_value", ":", "\n", "                                            ", "if", "len", "(", "instance_list", ")", "<", "(", "this_line_num_dimensions", "+", "1", ")", ":", "\n", "                                                ", "instance_list", ".", "append", "(", "[", "]", ")", "\n", "\n", "", "if", "timestamp_is_timestamp", ":", "\n", "                                                ", "timestamps_for_dimension", "=", "pd", ".", "DatetimeIndex", "(", "timestamps_for_dimension", ")", "\n", "\n", "", "instance_list", "[", "this_line_num_dimensions", "]", ".", "append", "(", "\n", "pd", ".", "Series", "(", "index", "=", "timestamps_for_dimension", ",", "data", "=", "values_for_dimension", ")", ")", "\n", "this_line_num_dimensions", "+=", "1", "\n", "\n", "timestamps_for_dimension", "=", "[", "]", "\n", "values_for_dimension", "=", "[", "]", "\n", "\n", "", "", "", "", "elif", "has_another_value", ":", "\n", "                                ", "raise", "TsFileParseException", "(", "\n", "\"dimension \"", "+", "str", "(", "this_line_num_dimensions", "+", "1", ")", "+", "\" on line \"", "+", "str", "(", "\n", "line_num", "+", "1", ")", "+", "\" ends with a ',' that is not followed by another tuple\"", ")", "\n", "\n", "", "elif", "has_another_dimension", "and", "target_labels", ":", "\n", "                                ", "raise", "TsFileParseException", "(", "\n", "\"dimension \"", "+", "str", "(", "this_line_num_dimensions", "+", "1", ")", "+", "\" on line \"", "+", "str", "(", "\n", "line_num", "+", "1", ")", "+", "\" ends with a ':' while it should list a class value\"", ")", "\n", "\n", "", "elif", "has_another_dimension", "and", "not", "target_labels", ":", "\n", "                                ", "if", "len", "(", "instance_list", ")", "<", "(", "this_line_num_dimensions", "+", "1", ")", ":", "\n", "                                    ", "instance_list", ".", "append", "(", "[", "]", ")", "\n", "\n", "", "instance_list", "[", "this_line_num_dimensions", "]", ".", "append", "(", "pd", ".", "Series", "(", "dtype", "=", "np", ".", "float32", ")", ")", "\n", "this_line_num_dimensions", "+=", "1", "\n", "num_dimensions", "=", "this_line_num_dimensions", "\n", "\n", "# If this is the 1st line of data we have seen then note the dimensions", "\n", "\n", "", "if", "not", "has_another_value", "and", "not", "has_another_dimension", ":", "\n", "                                ", "if", "num_dimensions", "is", "None", ":", "\n", "                                    ", "num_dimensions", "=", "this_line_num_dimensions", "\n", "\n", "", "if", "num_dimensions", "!=", "this_line_num_dimensions", ":", "\n", "                                    ", "raise", "TsFileParseException", "(", "\"line \"", "+", "str", "(", "\n", "line_num", "+", "1", ")", "+", "\" does not have the same number of dimensions as the previous line of data\"", ")", "\n", "\n", "# Check that we are not expecting some more data, and if not, store that processed above", "\n", "\n", "", "", "", "if", "has_another_value", ":", "\n", "                            ", "raise", "TsFileParseException", "(", "\n", "\"dimension \"", "+", "str", "(", "this_line_num_dimensions", "+", "1", ")", "+", "\" on line \"", "+", "str", "(", "\n", "line_num", "+", "1", ")", "+", "\" ends with a ',' that is not followed by another tuple\"", ")", "\n", "\n", "", "elif", "has_another_dimension", "and", "target_labels", ":", "\n", "                            ", "raise", "TsFileParseException", "(", "\n", "\"dimension \"", "+", "str", "(", "this_line_num_dimensions", "+", "1", ")", "+", "\" on line \"", "+", "str", "(", "\n", "line_num", "+", "1", ")", "+", "\" ends with a ':' while it should list a class value\"", ")", "\n", "\n", "", "elif", "has_another_dimension", "and", "not", "target_labels", ":", "\n", "                            ", "if", "len", "(", "instance_list", ")", "<", "(", "this_line_num_dimensions", "+", "1", ")", ":", "\n", "                                ", "instance_list", ".", "append", "(", "[", "]", ")", "\n", "\n", "", "instance_list", "[", "this_line_num_dimensions", "]", ".", "append", "(", "pd", ".", "Series", "(", ")", ")", "\n", "this_line_num_dimensions", "+=", "1", "\n", "num_dimensions", "=", "this_line_num_dimensions", "\n", "\n", "# If this is the 1st line of data we have seen then note the dimensions", "\n", "\n", "", "if", "not", "has_another_value", "and", "num_dimensions", "!=", "this_line_num_dimensions", ":", "\n", "                            ", "raise", "TsFileParseException", "(", "\"line \"", "+", "str", "(", "\n", "line_num", "+", "1", ")", "+", "\" does not have the same number of dimensions as the previous line of data\"", ")", "\n", "\n", "# Check if we should have class values, and if so that they are contained in those listed in the metadata", "\n", "\n", "", "if", "target_labels", "and", "len", "(", "class_val_list", ")", "==", "0", ":", "\n", "                            ", "raise", "TsFileParseException", "(", "\"the cases have no associated class values\"", ")", "\n", "", "", "else", ":", "\n", "                        ", "dimensions", "=", "line", ".", "split", "(", "\":\"", ")", "\n", "# If first row then note the number of dimensions (that must be the same for all cases)", "\n", "if", "is_first_case", ":", "\n", "                            ", "num_dimensions", "=", "len", "(", "dimensions", ")", "\n", "\n", "if", "target_labels", ":", "\n", "                                ", "num_dimensions", "-=", "1", "\n", "\n", "", "for", "dim", "in", "range", "(", "0", ",", "num_dimensions", ")", ":", "\n", "                                ", "instance_list", ".", "append", "(", "[", "]", ")", "\n", "", "is_first_case", "=", "False", "\n", "\n", "# See how many dimensions that the case whose data in represented in this line has", "\n", "", "this_line_num_dimensions", "=", "len", "(", "dimensions", ")", "\n", "\n", "if", "target_labels", ":", "\n", "                            ", "this_line_num_dimensions", "-=", "1", "\n", "\n", "# All dimensions should be included for all series, even if they are empty", "\n", "", "if", "this_line_num_dimensions", "!=", "num_dimensions", ":", "\n", "                            ", "raise", "TsFileParseException", "(", "\"inconsistent number of dimensions. Expecting \"", "+", "str", "(", "\n", "num_dimensions", ")", "+", "\" but have read \"", "+", "str", "(", "this_line_num_dimensions", ")", ")", "\n", "\n", "# Process the data for each dimension", "\n", "", "for", "dim", "in", "range", "(", "0", ",", "num_dimensions", ")", ":", "\n", "                            ", "dimension", "=", "dimensions", "[", "dim", "]", ".", "strip", "(", ")", "\n", "\n", "if", "dimension", ":", "\n", "                                ", "data_series", "=", "dimension", ".", "split", "(", "\",\"", ")", "\n", "data_series", "=", "[", "float", "(", "i", ")", "for", "i", "in", "data_series", "]", "\n", "instance_list", "[", "dim", "]", ".", "append", "(", "pd", ".", "Series", "(", "data_series", ")", ")", "\n", "", "else", ":", "\n", "                                ", "instance_list", "[", "dim", "]", ".", "append", "(", "pd", ".", "Series", "(", ")", ")", "\n", "\n", "", "", "if", "target_labels", ":", "\n", "                            ", "class_val_list", ".", "append", "(", "float", "(", "dimensions", "[", "num_dimensions", "]", ".", "strip", "(", ")", ")", ")", "\n", "\n", "", "", "", "", "line_num", "+=", "1", "\n", "\n", "# Check that the file was not empty", "\n", "", "", "if", "line_num", ":", "\n", "# Check that the file contained both metadata and data", "\n", "        ", "complete_regression_meta_data", "=", "has_problem_name_tag", "and", "has_timestamps_tag", "and", "has_univariate_tag", "and", "has_target_labels_tag", "and", "has_data_tag", "\n", "complete_classification_meta_data", "=", "has_problem_name_tag", "and", "has_timestamps_tag", "and", "has_univariate_tag", "and", "has_class_labels_tag", "and", "has_data_tag", "\n", "\n", "if", "metadata_started", "and", "not", "complete_regression_meta_data", "and", "not", "complete_classification_meta_data", ":", "\n", "            ", "raise", "TsFileParseException", "(", "\"metadata incomplete\"", ")", "\n", "", "elif", "metadata_started", "and", "not", "data_started", ":", "\n", "            ", "raise", "TsFileParseException", "(", "\"file contained metadata but no data\"", ")", "\n", "", "elif", "metadata_started", "and", "data_started", "and", "len", "(", "instance_list", ")", "==", "0", ":", "\n", "            ", "raise", "TsFileParseException", "(", "\"file contained metadata but no data\"", ")", "\n", "\n", "# Create a DataFrame from the data parsed above", "\n", "", "data", "=", "pd", ".", "DataFrame", "(", "dtype", "=", "np", ".", "float32", ")", "\n", "\n", "for", "dim", "in", "range", "(", "0", ",", "num_dimensions", ")", ":", "\n", "            ", "data", "[", "'dim_'", "+", "str", "(", "dim", ")", "]", "=", "instance_list", "[", "dim", "]", "\n", "\n", "# Check if we should return any associated class labels separately", "\n", "\n", "", "if", "target_labels", ":", "\n", "            ", "if", "return_separate_X_and_y", ":", "\n", "                ", "return", "data", ",", "np", ".", "asarray", "(", "class_val_list", ")", "\n", "", "else", ":", "\n", "                ", "data", "[", "'class_vals'", "]", "=", "pd", ".", "Series", "(", "class_val_list", ")", "\n", "return", "data", "\n", "", "", "else", ":", "\n", "            ", "return", "data", "\n", "", "", "else", ":", "\n", "        ", "raise", "TsFileParseException", "(", "\"empty file\"", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.bidmc.process_data.split_data": [[14, 33], ["pandas.concat", "numpy.concatenate", "sklearn.model_selection.train_test_split", "sklearn.model_selection.train_test_split", "int"], "function", ["None"], ["X_val", ",", "X_test", ",", "y_val", ",", "y_test", "=", "train_test_split", "(", "X_eval", ",", "y_eval", ",", "test_size", "=", "0.5", ",", "random_state", "=", "seed", "+", "1", ")", "\n", "", "else", ":", "\n", "        ", "X_test", ",", "y_test", "=", "X_test_orig", ",", "y_test_orig", "\n", "X_train", ",", "X_val", ",", "y_train", ",", "y_val", "=", "train_test_split", "(", "X_train_orig", ",", "y_train_orig", ",", "test_size", "=", "0.20", ",", "random_state", "=", "seed", ")", "\n", "", "return", "X_train", ",", "y_train", ",", "X_val", ",", "y_val", ",", "X_test", ",", "y_test", "\n", "\n", "", "X_train_orig", ",", "y_train_orig", "=", "load_from_arff_to_dataframe", "(", "\n", "os", ".", "path", ".", "join", "(", "DATA_PATH", ",", "\"EigenWorms_TRAIN.arff\"", ")", "\n", ")", "\n", "X_test_orig", ",", "y_test_orig", "=", "load_from_arff_to_dataframe", "(", "\n", "os", ".", "path", ".", "join", "(", "DATA_PATH", ",", "\"EigenWorms_TEST.arff\"", ")", "\n", ")", "\n", "\n", "X_train", ",", "y_train", ",", "X_val", ",", "y_val", ",", "X_test", ",", "y_test", "=", "split_data", "(", "X_train_orig", ",", "y_train_orig", ",", "X_test_orig", ",", "y_test_orig", ",", "shuffle", "=", "True", ",", "seed", "=", "0", ")", "\n", "\n", "def", "_to_numpy", "(", "X", ")", ":", "\n", "    ", "return", "np", ".", "stack", "(", "[", "np", ".", "stack", "(", "x", ")", "for", "x", "in", "X", ".", "to_numpy", "(", ")", "]", ")", ".", "swapaxes", "(", "-", "1", ",", "-", "2", ")", "\n", "\n", "", "X_train", "=", "_to_numpy", "(", "X_train", ")", "\n", "X_val", "=", "_to_numpy", "(", "X_val", ")", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.bidmc.process_data._to_numpy": [[35, 38], ["numpy.stack().swapaxes", "numpy.stack", "numpy.stack", "X.to_numpy"], "function", ["None"], ["\n", "mean", "=", "np", ".", "mean", "(", "X_train", ".", "reshape", "(", "(", "-", "1", ",", "6", ")", ")", ",", "axis", "=", "0", ")", "\n", "std", "=", "np", ".", "std", "(", "X_train", ".", "reshape", "(", "(", "-", "1", ",", "6", ")", ")", ",", "axis", "=", "0", ")", "\n", "print", "(", "mean", ".", "shape", ",", "std", ".", "shape", ")", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.bidmc.process_data.process_data": [[40, 68], ["data_loader.load_from_tsfile_to_dataframe", "data_loader.load_from_tsfile_to_dataframe", "process_data.split_data", "os.path.join", "os.makedirs", "numpy.save", "numpy.save", "numpy.save", "numpy.save", "numpy.save", "numpy.save", "os.path.join", "os.path.join", "os.path.join", "process_data._to_numpy", "os.path.join", "os.path.join", "process_data._to_numpy", "os.path.join", "os.path.join", "process_data._to_numpy", "os.path.join", "numpy.load", "print"], "function", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.bidmc.data_loader.load_from_tsfile_to_dataframe", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.bidmc.data_loader.load_from_tsfile_to_dataframe", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.bidmc.process_data.split_data", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.bidmc.process_data._to_numpy", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.bidmc.process_data._to_numpy", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.bidmc.process_data._to_numpy"], ["print", "(", "std", ")", "\n", "X_train", "=", "(", "X_train", "-", "mean", ")", "/", "std", "\n", "X_val", "=", "(", "X_val", "-", "mean", ")", "/", "std", "\n", "X_test", "=", "(", "X_test", "-", "mean", ")", "/", "std", "\n", "\n", "np", ".", "save", "(", "os", ".", "path", ".", "join", "(", "DATA_PATH", ",", "\"trainx.npy\"", ")", ",", "X_train", ")", "\n", "np", ".", "save", "(", "os", ".", "path", ".", "join", "(", "DATA_PATH", ",", "\"trainy.npy\"", ")", ",", "y_train", ".", "astype", "(", "int", ")", "-", "1", ")", "\n", "np", ".", "save", "(", "os", ".", "path", ".", "join", "(", "DATA_PATH", ",", "\"validx.npy\"", ")", ",", "X_val", ")", "\n", "np", ".", "save", "(", "os", ".", "path", ".", "join", "(", "DATA_PATH", ",", "\"validy.npy\"", ")", ",", "y_val", ".", "astype", "(", "int", ")", "-", "1", ")", "\n", "np", ".", "save", "(", "os", ".", "path", ".", "join", "(", "DATA_PATH", ",", "\"testx.npy\"", ")", ",", "X_test", ")", "\n", "np", ".", "save", "(", "os", ".", "path", ".", "join", "(", "DATA_PATH", ",", "\"testy.npy\"", ")", ",", "y_test", ".", "astype", "(", "int", ")", "-", "1", ")", "\n", "\n", "\n", "for", "f", "in", "[", "'trainx'", ",", "'trainy'", ",", "'validx'", ",", "'validy'", ",", "'testx'", ",", "'testy'", "]", ":", "\n", "    ", "df", "=", "np", ".", "load", "(", "f\"data/{f}.npy\"", ")", "\n", "print", "(", "f", ",", "df", ".", "shape", ",", "df", ".", "dtype", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.monash.links.download_file": [[189, 195], ["requests.get", "open", "shutil.copyfileobj"], "function", ["None"], ["def", "download_file", "(", "url", ",", "filename", ")", ":", "\n", "    ", "\"\"\"Download a file from a url and save it to a filename.\"\"\"", "\n", "r", "=", "requests", ".", "get", "(", "url", ",", "stream", "=", "True", ")", "\n", "with", "open", "(", "filename", ",", "'wb'", ")", "as", "f", ":", "\n", "        ", "shutil", ".", "copyfileobj", "(", "r", ".", "raw", ",", "f", ")", "\n", "", "del", "r", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.monash.links.download_all_links": [[197, 227], ["urls.items", "os.path.exists", "os.mkdir", "print", "dataset_urls.items", "os.path.exists", "os.mkdir", "print", "links.download_file", "os.remove", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "zipfile.ZipFile", "zip_ref.extractall", "print", "os.path.join", "os.path.join"], "function", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.monash.links.download_file"], ["", "def", "download_all_links", "(", "urls", ")", ":", "\n", "    ", "\"\"\"Download all links inside the URLS dictionary, and save them in a folder named 'monash', with a subfolder for each dataset.\"\"\"", "\n", "# Create the 'monash' folder if it doesn't exist", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "'monash'", ")", ":", "\n", "        ", "os", ".", "mkdir", "(", "'monash'", ")", "\n", "\n", "# Iterate over the datasets", "\n", "", "for", "dataset_name", ",", "dataset_urls", "in", "urls", ".", "items", "(", ")", ":", "\n", "        ", "print", "(", "'Downloading dataset {}...'", ".", "format", "(", "dataset_name", ")", ")", "\n", "# Create the subfolder for the dataset", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "os", ".", "path", ".", "join", "(", "'monash'", ",", "dataset_name", ")", ")", ":", "\n", "            ", "os", ".", "mkdir", "(", "os", ".", "path", ".", "join", "(", "'monash'", ",", "dataset_name", ")", ")", "\n", "", "else", ":", "\n", "            ", "print", "(", "'The dataset {} already exists. Skipping...'", ".", "format", "(", "dataset_name", ")", ")", "\n", "continue", "\n", "\n", "# Iterate over the urls for the dataset", "\n", "", "for", "url_name", ",", "url", "in", "dataset_urls", ".", "items", "(", ")", ":", "\n", "# Download the file", "\n", "            ", "download_file", "(", "url", ",", "os", ".", "path", ".", "join", "(", "'monash'", ",", "dataset_name", ",", "url_name", "+", "'.zip'", ")", ")", "\n", "\n", "# Unzip the file", "\n", "try", ":", "\n", "                ", "with", "zipfile", ".", "ZipFile", "(", "os", ".", "path", ".", "join", "(", "'monash'", ",", "dataset_name", ",", "url_name", "+", "'.zip'", ")", ",", "'r'", ")", "as", "zip_ref", ":", "\n", "                    ", "zip_ref", ".", "extractall", "(", "os", ".", "path", ".", "join", "(", "'monash'", ",", "dataset_name", ")", ")", "\n", "", "", "except", "zipfile", ".", "BadZipFile", ":", "\n", "                ", "print", "(", "'The file {}.zip is not a valid zip file. Skipping...'", ".", "format", "(", "url_name", ")", ")", "\n", "\n", "# Delete the zip file", "\n", "", "os", ".", "remove", "(", "os", ".", "path", ".", "join", "(", "'monash'", ",", "dataset_name", ",", "url_name", "+", "'.zip'", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.cauchy.tune_cauchy.forward_params_list": [[10, 26], ["itertools.product", "int", "math.log2", "list", "thread_values.append", "str"], "function", ["None"], ["def", "forward_params_list", "(", "N", ")", ":", "\n", "    ", "blocksize_params", "=", "(", "'MAX_BLOCK_SIZE_VALUE'", ",", "[", "64", ",", "128", ",", "256", ",", "512", ",", "1024", "]", ")", "\n", "thread_value_default", "=", "[", "2", ",", "4", ",", "8", ",", "16", ",", "32", ",", "32", ",", "32", ",", "32", ",", "32", ",", "32", "]", "\n", "thread_values_supported", "=", "[", "2", ",", "4", ",", "8", ",", "16", ",", "32", ",", "64", ",", "128", "]", "\n", "log_N_half", "=", "int", "(", "math", ".", "log2", "(", "N", ")", ")", "-", "1", "\n", "thread_values", "=", "[", "]", "\n", "for", "val", "in", "thread_values_supported", ":", "\n", "        ", "if", "val", "<=", "N", "//", "2", ":", "\n", "            ", "array", "=", "list", "(", "thread_value_default", ")", "\n", "array", "[", "log_N_half", "-", "1", "]", "=", "val", "\n", "thread_values", ".", "append", "(", "'{'", "+", "', '", ".", "join", "(", "str", "(", "v", ")", "for", "v", "in", "array", ")", "+", "'}'", ")", "\n", "", "", "thread_params", "=", "(", "'ITEMS_PER_THREAD_SYM_FWD_VALUES'", ",", "thread_values", ")", "\n", "value_prod", "=", "itertools", ".", "product", "(", "thread_params", "[", "1", "]", ",", "blocksize_params", "[", "1", "]", ")", "\n", "params_list", "=", "[", "{", "thread_params", "[", "0", "]", ":", "value", "[", "0", "]", ",", "blocksize_params", "[", "0", "]", ":", "value", "[", "1", "]", "}", "\n", "for", "value", "in", "value_prod", "]", "\n", "return", "params_list", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.cauchy.tune_cauchy.backward_params_list": [[28, 34], ["None"], "function", ["None"], ["", "def", "backward_params_list", "(", "L", ")", ":", "\n", "    ", "thread_value_supported", "=", "[", "8", ",", "16", ",", "32", ",", "64", ",", "128", "]", "\n", "thread_params", "=", "(", "'ITEMS_PER_THREAD_SYM_BWD_VALUE'", ",", "[", "v", "for", "v", "in", "thread_value_supported", "\n", "if", "(", "L", "+", "v", "-", "1", ")", "//", "v", "<=", "1024", "]", ")", "\n", "params_list", "=", "[", "{", "thread_params", "[", "0", "]", ":", "value", "}", "for", "value", "in", "thread_params", "[", "1", "]", "]", "\n", "return", "params_list", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.cauchy.cauchy.CauchyMultiply.forward": [[68, 82], ["ctx.save_for_backward", "cauchy_mult.cauchy_mult_fwd", "NotImplementedError", "NotImplementedError", "NotImplementedError"], "methods", ["None"], ["expr", ",", "\n", "[", "\n", "'v = Vj(2)'", ",", "\n", "'z = Vi(2)'", ",", "\n", "'w = Vj(2)'", ",", "\n", "]", ",", "\n", "reduction_op", "=", "'Sum'", ",", "\n", "axis", "=", "1", ",", "\n", ")", "\n", "\n", "if", "conj", ":", "\n", "        ", "v", "=", "_conj", "(", "v", ")", "\n", "w", "=", "_conj", "(", "w", ")", "\n", "", "v", ",", "z", ",", "w", "=", "_broadcast_dims", "(", "v", ",", "z", ",", "w", ")", "\n", "v", "=", "_c2r", "(", "v", ")", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.cauchy.cauchy.CauchyMultiply.backward": [[83, 88], ["cauchy_mult.cauchy_mult_bwd"], "methods", ["None"], ["z", "=", "_c2r", "(", "z", ")", "\n", "w", "=", "_c2r", "(", "w", ")", "\n", "\n", "r", "=", "cauchy_mult", "(", "v", ",", "z", ",", "w", ",", "backend", "=", "'GPU'", ")", "\n", "return", "_r2c", "(", "r", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.cauchy.cauchy.CauchyMultiplySymmetric.forward": [[92, 106], ["ctx.save_for_backward", "cauchy_mult.cauchy_mult_sym_fwd", "NotImplementedError", "NotImplementedError", "NotImplementedError"], "methods", ["None"], ["expr", ",", "\n", "[", "\n", "'v = Vj(1)'", ",", "\n", "'z = Vi(1)'", ",", "\n", "'w = Vj(1)'", ",", "\n", "]", ",", "\n", "reduction_op", "=", "'Sum'", ",", "\n", "axis", "=", "1", ",", "\n", ")", "\n", "v", ",", "z", ",", "w", "=", "_broadcast_dims", "(", "v", ",", "z", ",", "w", ")", "\n", "v", "=", "v", ".", "unsqueeze", "(", "-", "1", ")", "\n", "z", "=", "z", ".", "unsqueeze", "(", "-", "1", ")", "\n", "w", "=", "w", ".", "unsqueeze", "(", "-", "1", ")", "\n", "\n", "r", "=", "cauchy_mult", "(", "v", ",", "z", ",", "w", ",", "backend", "=", "'GPU'", ")", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.cauchy.cauchy.CauchyMultiplySymmetric.backward": [[107, 112], ["cauchy_mult.cauchy_mult_sym_bwd"], "methods", ["None"], ["return", "r", "\n", "\n", "\n", "", "def", "cauchy_conj", "(", "v", ",", "z", ",", "w", ",", "num", "=", "2", ",", "denom", "=", "2", ")", ":", "\n", "    ", "if", "num", "==", "1", ":", "\n", "        ", "expr_num", "=", "'z * ComplexReal(v) - Real2Complex(ComplexReal(v)*ComplexReal(w) + ComplexImag(v)*ComplexImag(w))'", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.cauchy.cauchy.cauchy_mult_torch": [[8, 27], ["einops.rearrange", "einops.rearrange", "einops.rearrange", "einops.rearrange", "einops.rearrange", "einops.rearrange", "einops.rearrange.abs().square", "einops.rearrange.abs"], "function", ["None"], ["\n", "if", "__name__", "==", "'__main__'", ":", "\n", "    ", "import", "sys", "\n", "import", "pathlib", "\n", "p", "=", "pathlib", ".", "Path", "(", ")", ".", "absolute", "(", ")", "\n", "print", "(", "\"Adding path: \"", ",", "p", ")", "\n", "sys", ".", "path", ".", "append", "(", "str", "(", "p", ")", ")", "\n", "\n", "", "import", "math", "\n", "import", "torch", "\n", "\n", "from", "einops", "import", "rearrange", "\n", "\n", "import", "os", "\n", "\n", "try", ":", "\n", "    ", "import", "pykeops", "\n", "from", "pykeops", ".", "torch", "import", "LazyTensor", ",", "Genred", "\n", "", "except", ":", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.cauchy.cauchy.cauchy_mult_keops": [[29, 38], ["LazyTensor", "LazyTensor", "LazyTensor", "div.sum", "div.sum.squeeze", "einops.rearrange", "einops.rearrange", "einops.rearrange"], "function", ["None"], ["\n", "", "_conj", "=", "lambda", "x", ":", "torch", ".", "cat", "(", "[", "x", ",", "x", ".", "conj", "(", ")", "]", ",", "dim", "=", "-", "1", ")", "\n", "def", "_broadcast_dims", "(", "*", "tensors", ")", ":", "\n", "    ", "max_dim", "=", "max", "(", "[", "len", "(", "tensor", ".", "shape", ")", "for", "tensor", "in", "tensors", "]", ")", "\n", "tensors", "=", "[", "tensor", ".", "view", "(", "(", "1", ",", ")", "*", "(", "max_dim", "-", "len", "(", "tensor", ".", "shape", ")", ")", "+", "tensor", ".", "shape", ")", "for", "tensor", "in", "tensors", "]", "\n", "return", "tensors", "\n", "\n", "", "def", "_c2r", "(", "x", ")", ":", "return", "torch", ".", "view_as_real", "(", "x", ")", "\n", "def", "_r2c", "(", "x", ")", ":", "return", "torch", ".", "view_as_complex", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.cauchy.cauchy._cauchy_mult": [[40, 45], ["CauchyMultiply.apply", "CauchyMultiplySymmetric.apply"], "function", ["None"], ["    ", "\"\"\"\n    v: (..., N)\n    z: (..., L)\n    w: (..., N)\n    returns: (..., L) \\sum v/(z-w)\n    \"\"\"", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.cauchy.cauchy.cauchy_mult": [[46, 64], ["torch.broadcast_tensors", "z.contiguous.squeeze", "v.contiguous.contiguous", "w.contiguous.contiguous", "z.contiguous.contiguous", "v.contiguous.size", "cauchy._cauchy_mult", "y.view.view", "len", "w.contiguous.size", "v.contiguous.view", "w.contiguous.view", "z.contiguous.size"], "function", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.cauchy.cauchy._cauchy_mult"], ["if", "conj", ":", "\n", "        ", "v", "=", "_conj", "(", "v", ")", "\n", "w", "=", "_conj", "(", "w", ")", "\n", "", "cauchy_matrix", "=", "v", ".", "unsqueeze", "(", "-", "1", ")", "/", "(", "z", ".", "unsqueeze", "(", "-", "2", ")", "-", "w", ".", "unsqueeze", "(", "-", "1", ")", ")", "# (... N L)", "\n", "return", "torch", ".", "sum", "(", "cauchy_matrix", ",", "dim", "=", "-", "2", ")", "\n", "\n", "", "def", "cauchy_lazy", "(", "v", ",", "z", ",", "w", ",", "conj", "=", "True", ")", ":", "\n", "    ", "if", "conj", ":", "\n", "        ", "v", "=", "_conj", "(", "v", ")", "\n", "w", "=", "_conj", "(", "w", ")", "\n", "", "v", ",", "z", ",", "w", "=", "_broadcast_dims", "(", "v", ",", "z", ",", "w", ")", "\n", "v_l", "=", "LazyTensor", "(", "rearrange", "(", "v", ",", "'... N -> ... N 1 1'", ")", ")", "\n", "w_l", "=", "LazyTensor", "(", "rearrange", "(", "w", ",", "'... N -> ... N 1 1'", ")", ")", "\n", "z_l", "=", "LazyTensor", "(", "rearrange", "(", "z", ",", "'... L -> ... 1 L 1'", ")", ")", "\n", "sub", "=", "z_l", "-", "w_l", "# (b N L 1), for some reason it doesn't display the last dimension", "\n", "div", "=", "v_l", "/", "sub", "\n", "s", "=", "div", ".", "sum", "(", "dim", "=", "len", "(", "v_l", ".", "shape", ")", "-", "2", ")", "\n", "return", "s", ".", "squeeze", "(", "-", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.cauchy.test_cauchy.generate_data": [[11, 24], ["torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.cat().requires_grad_", "torch.randn", "torch.cat().requires_grad_", "torch.exp", "torch.cat", "torch.cat", "torch.randn", "torch.randn.conj", "torch.randn.conj"], "function", ["None"], ["def", "generate_data", "(", "batch_size", ",", "N", ",", "L", ",", "symmetric", "=", "True", ",", "device", "=", "'cuda'", ")", ":", "\n", "    ", "if", "not", "symmetric", ":", "\n", "        ", "v", "=", "torch", ".", "randn", "(", "batch_size", ",", "N", ",", "dtype", "=", "torch", ".", "complex64", ",", "device", "=", "device", ",", "requires_grad", "=", "True", ")", "\n", "w", "=", "torch", ".", "randn", "(", "batch_size", ",", "N", ",", "dtype", "=", "torch", ".", "complex64", ",", "device", "=", "device", ",", "requires_grad", "=", "True", ")", "\n", "z", "=", "torch", ".", "randn", "(", "L", ",", "dtype", "=", "torch", ".", "complex64", ",", "device", "=", "device", ")", "\n", "", "else", ":", "\n", "        ", "assert", "N", "%", "2", "==", "0", "\n", "v_half", "=", "torch", ".", "randn", "(", "batch_size", ",", "N", "//", "2", ",", "dtype", "=", "torch", ".", "complex64", ",", "device", "=", "device", ")", "\n", "v", "=", "torch", ".", "cat", "(", "[", "v_half", ",", "v_half", ".", "conj", "(", ")", "]", ",", "dim", "=", "-", "1", ")", ".", "requires_grad_", "(", "True", ")", "\n", "w_half", "=", "torch", ".", "randn", "(", "batch_size", ",", "N", "//", "2", ",", "dtype", "=", "torch", ".", "complex64", ",", "device", "=", "device", ")", "\n", "w", "=", "torch", ".", "cat", "(", "[", "w_half", ",", "w_half", ".", "conj", "(", ")", "]", ",", "dim", "=", "-", "1", ")", ".", "requires_grad_", "(", "True", ")", "\n", "z", "=", "torch", ".", "exp", "(", "1j", "*", "torch", ".", "randn", "(", "L", ",", "dtype", "=", "torch", ".", "float32", ",", "device", "=", "device", ")", ")", "\n", "", "return", "v", ",", "z", ",", "w", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.cauchy.test_cauchy.grad_to_half_grad": [[26, 29], ["dx.chunk", "dx_half_conj.conj"], "function", ["None"], ["", "def", "grad_to_half_grad", "(", "dx", ")", ":", "\n", "    ", "dx_half", ",", "dx_half_conj", "=", "dx", ".", "chunk", "(", "2", ",", "dim", "=", "-", "1", ")", "\n", "return", "dx_half", "+", "dx_half_conj", ".", "conj", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.cauchy.test_cauchy.test_cauchy_mult_symmetric": [[53, 96], ["pytest.mark.parametrize", "pytest.mark.parametrize", "torch.random.manual_seed", "test_cauchy.generate_data", "v[].clone().detach().requires_grad_", "w[].clone().detach().requires_grad_", "cauchy.cauchy_mult_torch().cfloat", "cauchy.cauchy_mult_keops", "cauchy.cauchy_mult", "torch.randn_like", "torch.autograd.grad", "torch.autograd.grad", "torch.autograd.grad", "print", "print", "print", "print", "print", "print", "cauchy_mult_torch().cfloat.abs", "cauchy_mult_torch().cfloat.abs", "test_cauchy.grad_to_half_grad", "test_cauchy.grad_to_half_grad", "dv_torch.abs", "dv_torch.abs", "dw_torch.abs", "dw_torch.abs", "relerr_out.amax", "relerr_out.mean", "relerr_dv.amax", "relerr_dv.mean", "relerr_dw.amax", "relerr_dw.mean", "v[].clone().detach", "w[].clone().detach", "cauchy.cauchy_mult_torch", "v.cdouble", "z.cdouble", "w.cdouble", "relerr_out_keops.amax().item", "relerr_out_keops.mean().item", "relerr_out.amax().item", "relerr_out.mean().item", "relerr_dv_keops.amax().item", "relerr_dv_keops.mean().item", "relerr_dv.amax().item", "relerr_dv.mean().item", "relerr_dw_keops.amax().item", "relerr_dw_keops.mean().item", "relerr_dw.amax().item", "relerr_dw.mean().item", "relerr_out_keops.amax", "relerr_out_keops.mean", "relerr_dv_keops.amax", "relerr_dv_keops.mean", "relerr_dw_keops.amax", "relerr_dw_keops.mean", "v[].clone", "w[].clone", "relerr_out_keops.amax", "relerr_out_keops.mean", "relerr_out.amax", "relerr_out.mean", "relerr_dv_keops.amax", "relerr_dv_keops.mean", "relerr_dv.amax", "relerr_dv.mean", "relerr_dw_keops.amax", "relerr_dw_keops.mean", "relerr_dw.amax", "relerr_dw.mean"], "function", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.cauchy.test_cauchy.generate_data", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.cauchy.cauchy.cauchy_mult_keops", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.cauchy.cauchy.cauchy_mult", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.cauchy.test_cauchy.grad_to_half_grad", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.cauchy.test_cauchy.grad_to_half_grad", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.cauchy.cauchy.cauchy_mult_torch"], ["", "@", "pytest", ".", "mark", ".", "parametrize", "(", "'L'", ",", "[", "3", ",", "17", ",", "489", ",", "2", "**", "10", ",", "1047", ",", "2", "**", "11", ",", "2", "**", "12", ",", "2", "**", "13", ",", "2", "**", "14", ",", "2", "**", "18", "]", ")", "\n", "@", "pytest", ".", "mark", ".", "parametrize", "(", "'N'", ",", "[", "4", ",", "8", ",", "16", ",", "32", ",", "64", ",", "128", ",", "256", ",", "512", ",", "1024", ",", "2048", "]", ")", "\n", "def", "test_cauchy_mult_symmetric", "(", "N", ",", "L", ")", ":", "\n", "# rtol, atol = (1e-4, 1e-4) if N <= 64 and L <= 1024 else(1e-3, 1e-3)", "\n", "    ", "atol", "=", "1e-4", "\n", "tol_factor", "=", "10.0", "# Our error shouldn't be this much higher than Keops' error", "\n", "device", "=", "'cuda'", "\n", "batch_size", "=", "4", "\n", "torch", ".", "random", ".", "manual_seed", "(", "2357", ")", "\n", "v", ",", "z", ",", "w", "=", "generate_data", "(", "batch_size", ",", "N", ",", "L", ",", "symmetric", "=", "True", ",", "device", "=", "device", ")", "\n", "v_half", "=", "v", "[", ":", ",", ":", "N", "//", "2", "]", ".", "clone", "(", ")", ".", "detach", "(", ")", ".", "requires_grad_", "(", "True", ")", "\n", "w_half", "=", "w", "[", ":", ",", ":", "N", "//", "2", "]", ".", "clone", "(", ")", ".", "detach", "(", ")", ".", "requires_grad_", "(", "True", ")", "\n", "# out_torch = cauchy_mult_torch(v, z, w, symmetric=True)", "\n", "out_torch", "=", "cauchy_mult_torch", "(", "v", ".", "cdouble", "(", ")", ",", "z", ".", "cdouble", "(", ")", ",", "w", ".", "cdouble", "(", ")", ",", "symmetric", "=", "True", ")", ".", "cfloat", "(", ")", "\n", "out_keops", "=", "cauchy_mult_keops", "(", "v", ",", "z", ",", "w", ")", "\n", "out", "=", "cauchy_mult", "(", "v_half", ",", "z", ",", "w_half", ",", "symmetric", "=", "True", ")", "\n", "relerr_out_keops", "=", "(", "out_keops", "-", "out_torch", ")", ".", "abs", "(", ")", "/", "out_torch", ".", "abs", "(", ")", "\n", "relerr_out", "=", "(", "out", "-", "out_torch", ")", ".", "abs", "(", ")", "/", "out_torch", ".", "abs", "(", ")", "\n", "\n", "dout", "=", "torch", ".", "randn_like", "(", "out", ")", "\n", "dv_torch", ",", "dw_torch", "=", "torch", ".", "autograd", ".", "grad", "(", "out_torch", ",", "(", "v", ",", "w", ")", ",", "dout", ",", "retain_graph", "=", "True", ")", "\n", "dv_torch", ",", "dw_torch", "=", "dv_torch", "[", ":", ",", ":", "N", "//", "2", "]", ",", "dw_torch", "[", ":", ",", ":", "N", "//", "2", "]", "\n", "dv_keops", ",", "dw_keops", "=", "torch", ".", "autograd", ".", "grad", "(", "out_keops", ",", "(", "v", ",", "w", ")", ",", "dout", ",", "retain_graph", "=", "True", ")", "\n", "dv_keops", ",", "dw_keops", "=", "grad_to_half_grad", "(", "dv_keops", ")", ",", "grad_to_half_grad", "(", "dw_keops", ")", "\n", "dv", ",", "dw", "=", "torch", ".", "autograd", ".", "grad", "(", "out", ",", "(", "v_half", ",", "w_half", ")", ",", "dout", ",", "retain_graph", "=", "True", ")", "\n", "relerr_dv_keops", "=", "(", "dv_keops", "-", "dv_torch", ")", ".", "abs", "(", ")", "/", "dv_torch", ".", "abs", "(", ")", "\n", "relerr_dv", "=", "(", "dv", "-", "dv_torch", ")", ".", "abs", "(", ")", "/", "dv_torch", ".", "abs", "(", ")", "\n", "relerr_dw_keops", "=", "(", "dw_keops", "-", "dw_torch", ")", ".", "abs", "(", ")", "/", "dw_torch", ".", "abs", "(", ")", "\n", "relerr_dw", "=", "(", "dw", "-", "dw_torch", ")", ".", "abs", "(", ")", "/", "dw_torch", ".", "abs", "(", ")", "\n", "print", "(", "f'Keops out relative error: max {relerr_out_keops.amax().item():.6f}, mean {relerr_out_keops.mean().item():6f}'", ")", "\n", "print", "(", "f'out relative error: max {relerr_out.amax().item():.6f}, mean {relerr_out.mean().item():.6f}'", ")", "\n", "print", "(", "f'Keops dv relative error: max {relerr_dv_keops.amax().item():.6f}, mean {relerr_dv_keops.mean().item():6f}'", ")", "\n", "print", "(", "f'dv relative error: max {relerr_dv.amax().item():.6f}, mean {relerr_dv.mean().item():.6f}'", ")", "\n", "print", "(", "f'Keops dw relative error: max {relerr_dw_keops.amax().item():.6f}, mean {relerr_dw_keops.mean().item():6f}'", ")", "\n", "print", "(", "f'dw relative error: max {relerr_dw.amax().item():.6f}, mean {relerr_dw.mean().item():.6f}'", ")", "\n", "assert", "(", "relerr_out", ".", "amax", "(", ")", "<=", "relerr_out_keops", ".", "amax", "(", ")", "*", "tol_factor", "+", "atol", ")", "\n", "assert", "(", "relerr_out", ".", "mean", "(", ")", "<=", "relerr_out_keops", ".", "mean", "(", ")", "*", "tol_factor", "+", "atol", ")", "\n", "# assert torch.allclose(out, out_torch, rtol=rtol, atol=atol)", "\n", "# assert torch.allclose(out, out_keops, rtol=rtol, atol=atol)", "\n", "assert", "(", "relerr_dv", ".", "amax", "(", ")", "<=", "relerr_dv_keops", ".", "amax", "(", ")", "*", "tol_factor", "+", "atol", ")", "\n", "assert", "(", "relerr_dv", ".", "mean", "(", ")", "<=", "relerr_dv_keops", ".", "mean", "(", ")", "*", "tol_factor", "+", "atol", ")", "\n", "assert", "(", "relerr_dw", ".", "amax", "(", ")", "<=", "relerr_dw_keops", ".", "amax", "(", ")", "*", "tol_factor", "+", "atol", ")", "\n", "assert", "(", "relerr_dw", ".", "mean", "(", ")", "<=", "relerr_dw_keops", ".", "mean", "(", ")", "*", "tol_factor", "+", "atol", ")", "\n", "# assert torch.allclose(dv, dv_torch, rtol=1e-4, atol=1e-4)", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.cauchy.tuner.KernelTuner.__init__": [[137, 146], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "extension_dir", ",", "source_files", ",", "params_list", ",", "benchmark_script", ",", "\n", "benchmark_args", ",", "npool", "=", "8", ",", "verbose", "=", "True", ")", ":", "\n", "        ", "self", ".", "extension_dir", "=", "extension_dir", "\n", "self", ".", "source_files", "=", "source_files", "\n", "self", ".", "params_list", "=", "params_list", "\n", "self", ".", "benchmark_script", "=", "benchmark_script", "\n", "self", ".", "benchmark_args", "=", "benchmark_args", "\n", "self", ".", "npool", "=", "npool", "\n", "self", ".", "verbose", "=", "verbose", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.cauchy.tuner.KernelTuner.tune": [[147, 183], ["tqdm.tqdm.tqdm", "tqdm.tqdm.tqdm", "print", "tuner.uninstall_extensions", "tuner.set_up_tuning_temp_dir", "multiprocessing.Pool", "p.map", "list", "shutil.rmtree", "tuner.compile_extension", "zip", "results.append", "tuner.benchmark_extension"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.cauchy.tuner.uninstall_extensions", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.cauchy.tuner.set_up_tuning_temp_dir", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.cauchy.tuner.compile_extension", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.cauchy.tuner.benchmark_extension"], ["", "def", "tune", "(", "self", ")", ":", "\n", "        ", "temp_dirs", "=", "[", "set_up_tuning_temp_dir", "(", "params", ",", "self", ".", "source_files", ",", "self", ".", "extension_dir", ",", "\n", "verbose", "=", "self", ".", "verbose", ")", "\n", "for", "params", "in", "self", ".", "params_list", "]", "\n", "# Compile in parallel (for speed), then install sequentially to ensure correctness", "\n", "with", "Pool", "(", "self", ".", "npool", ")", "as", "p", ":", "\n", "            ", "p", ".", "map", "(", "compile_extension", ",", "temp_dirs", ")", "\n", "# with Pool(1) as p:", "\n", "#     p.map(partial(compile_extension, install=True), [temp_dirs])", "\n", "", "for", "temp_dir", "in", "tqdm", "(", "temp_dirs", ")", ":", "\n", "            ", "try", ":", "\n", "                ", "compile_extension", "(", "temp_dir", ",", "install", "=", "True", ")", "\n", "", "except", ":", "\n", "                ", "pass", "\n", "# # We benchmark on a separate process so that they can import the extension that just got compiled.", "\n", "# for params, temp_dir in params_tempdir:", "\n", "#     print('Benchmarking: ', params)", "\n", "#     recv_conn, send_conn = Pipe(duplex=False)", "\n", "#     benchmark_process = Process(target=benchmark_fwd, args=(send_conn, str(temp_dir.stem)))", "\n", "#     benchmark_process.start()", "\n", "#     result = recv_conn.recv()", "\n", "#     benchmark_process.join()", "\n", "#     print('result', result)", "\n", "", "", "results", "=", "[", "]", "\n", "for", "params", ",", "temp_dir", "in", "tqdm", "(", "list", "(", "zip", "(", "self", ".", "params_list", ",", "temp_dirs", ")", ")", ")", ":", "\n", "            ", "try", ":", "\n", "                ", "results", ".", "append", "(", "(", "params", ",", "\n", "benchmark_extension", "(", "self", ".", "benchmark_script", ",", "\n", "*", "[", "'--name'", ",", "temp_dir", ".", "stem", "]", "+", "self", ".", "benchmark_args", ")", ")", ")", "\n", "", "except", ":", "\n", "                ", "pass", "\n", "", "", "print", "(", "results", ")", "\n", "uninstall_extensions", "(", "[", "temp_dir", ".", "stem", "for", "temp_dir", "in", "temp_dirs", "]", ")", "\n", "for", "temp_dir", "in", "temp_dirs", ":", "\n", "            ", "shutil", ".", "rmtree", "(", "temp_dir", ")", "\n", "", "return", "results", "\n", "", "", ""]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.cauchy.tuner.read_file": [[21, 26], ["os.path.isfile", "open", "f.read"], "function", ["None"], ["def", "read_file", "(", "filename", ")", ":", "\n", "    ", "\"\"\" return the contents of the file named filename or None if file not found \"\"\"", "\n", "if", "os", ".", "path", ".", "isfile", "(", "filename", ")", ":", "\n", "        ", "with", "open", "(", "filename", ",", "'r'", ")", "as", "f", ":", "\n", "            ", "return", "f", ".", "read", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.cauchy.tuner.write_file": [[28, 32], ["open", "f.write"], "function", ["None"], ["", "", "", "def", "write_file", "(", "filename", ",", "string", ")", ":", "\n", "    ", "\"\"\"dump the contents of string to a file called filename\"\"\"", "\n", "with", "open", "(", "filename", ",", "'w'", ",", "encoding", "=", "\"utf-8\"", ")", "as", "f", ":", "\n", "        ", "f", ".", "write", "(", "string", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.cauchy.tuner.prepare_kernel_string": [[34, 38], ["params.items", "str"], "function", ["None"], ["", "", "def", "prepare_kernel_string", "(", "kernel_string", ",", "params", ")", ":", "\n", "    ", "for", "k", ",", "v", "in", "params", ".", "items", "(", ")", ":", "\n", "        ", "kernel_string", "=", "\"#define \"", "+", "k", "+", "\" \"", "+", "str", "(", "v", ")", "+", "\"\\n\"", "+", "kernel_string", "\n", "", "return", "kernel_string", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.cauchy.tuner.compile_extension": [[40, 59], ["subprocess.run", "str", "str", "print", "print"], "function", ["None"], ["", "def", "compile_extension", "(", "temp_dir", ",", "install", "=", "False", ",", "verbose", "=", "True", ")", ":", "\n", "# Need to copy this process's environments, otherwise it can't find the compilers", "\n", "    ", "env", "=", "{", "**", "os", ".", "environ", ",", "\n", "'TUNING_SOURCE_DIR'", ":", "str", "(", "temp_dir", ")", ",", "\n", "'TUNING_EXTENSION_NAME'", ":", "str", "(", "temp_dir", ".", "stem", ")", "}", "\n", "# https://stackoverflow.com/questions/53173314/how-to-change-distutils-output-directory", "\n", "# Need separate build directories for parallel compilation", "\n", "output", "=", "subprocess", ".", "run", "(", "\n", "# [sys.executable, \"tuning_setup.py\", 'build', f'--build-base={str(temp_dir)}',", "\n", "#  f'--build-lib={str(temp_dir)}'],", "\n", "[", "sys", ".", "executable", ",", "\"tuning_setup.py\"", ",", "'build'", "if", "not", "install", "else", "'develop'", "]", ",", "\n", "cwd", "=", "temp_dir", ",", "\n", "env", "=", "env", ",", "\n", "capture_output", "=", "True", ",", "\n", "# check=True", "\n", ")", "\n", "if", "verbose", ":", "\n", "        ", "print", "(", "output", ")", "\n", "print", "(", "'Done compiling'", "if", "not", "install", "else", "'Done installing'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.cauchy.tuner.uninstall_extensions": [[61, 73], ["subprocess.run", "print", "print"], "function", ["None"], ["", "", "def", "uninstall_extensions", "(", "tuning_extension_names", ",", "verbose", "=", "True", ")", ":", "\n", "# Need to copy this process's environments, otherwise it can't find the compilers", "\n", "    ", "env", "=", "{", "**", "os", ".", "environ", "}", "\n", "output", "=", "subprocess", ".", "run", "(", "\n", "[", "sys", ".", "executable", ",", "'-m'", ",", "'pip'", ",", "'uninstall'", ",", "'-y'", ",", "*", "tuning_extension_names", "]", ",", "\n", "env", "=", "env", ",", "\n", "capture_output", "=", "True", ",", "\n", "# check=True", "\n", ")", "\n", "if", "verbose", ":", "\n", "        ", "print", "(", "output", ")", "\n", "print", "(", "'Done uninstalling'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.cauchy.tuner.benchmark_extension": [[75, 90], ["subprocess.run", "json.loads", "print", "print", "subprocess.run.stdout.decode"], "function", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.utils.permutations.decode"], ["", "", "def", "benchmark_extension", "(", "benchmark_script", ",", "*", "benchmark_args", ",", "verbose", "=", "True", ")", ":", "\n", "# Need to copy this process's environments, otherwise it can't find the compilers", "\n", "    ", "env", "=", "os", ".", "environ", "\n", "# https://stackoverflow.com/questions/53173314/how-to-change-distutils-output-directory", "\n", "# Need separate build directories for parallel compilation", "\n", "process", "=", "subprocess", ".", "run", "(", "\n", "[", "sys", ".", "executable", ",", "benchmark_script", ",", "*", "benchmark_args", "]", ",", "\n", "env", "=", "os", ".", "environ", ",", "\n", "capture_output", "=", "True", ",", "\n", "# check=True", "\n", ")", "\n", "if", "verbose", ":", "\n", "        ", "print", "(", "process", ")", "\n", "print", "(", "'Done benchmarking'", ")", "\n", "", "return", "json", ".", "loads", "(", "process", ".", "stdout", ".", "decode", "(", "sys", ".", "stdout", ".", "encoding", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.cauchy.tuner.set_up_tuning_temp_dir": [[116, 133], ["temp_dir.exists", "shutil.copytree", "print", "shutil.rmtree", "tuner.read_file", "tuner.prepare_kernel_string", "tuner.write_file", "random.choices", "pathlib.Path.cwd"], "function", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.cauchy.tuner.read_file", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.cauchy.tuner.prepare_kernel_string", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.cauchy.tuner.write_file"], ["", "def", "set_up_tuning_temp_dir", "(", "params", ":", "dict", ",", "source_files", ",", "extension_dir", ",", "verbose", "=", "True", ")", ":", "\n", "    ", "if", "verbose", ":", "\n", "        ", "print", "(", "'params: '", ",", "params", ")", "\n", "# TD [2021-10-22]: tempfile.mkdtemp sometimes create dir name with '_' in it, thus messing up", "\n", "# the extension name.", "\n", "# temp_dir = Path(tempfile.mkdtemp(prefix=\"temp_\", dir=Path.cwd().parent)).absolute()", "\n", "", "tuning_extension_name", "=", "'temp_'", "+", "''", ".", "join", "(", "random", ".", "choices", "(", "string", ".", "ascii_uppercase", "+", "string", ".", "digits", ",", "k", "=", "10", ")", ")", "\n", "temp_dir", "=", "(", "Path", ".", "cwd", "(", ")", ".", "parent", "/", "tuning_extension_name", ")", ".", "absolute", "(", ")", "\n", "if", "temp_dir", ".", "exists", "(", ")", ":", "\n", "        ", "shutil", ".", "rmtree", "(", "temp_dir", ")", "# shutil.copytree doesn't want directory that already exists", "\n", "", "shutil", ".", "copytree", "(", "extension_dir", ",", "temp_dir", ")", "\n", "sources", "=", "[", "temp_dir", "/", "name", "for", "name", "in", "source_files", "]", "\n", "for", "kernel_source", "in", "sources", ":", "\n", "        ", "ks", "=", "read_file", "(", "kernel_source", ")", "\n", "ks", "=", "prepare_kernel_string", "(", "ks", ",", "params", ")", "\n", "write_file", "(", "kernel_source", ",", "ks", ")", "\n", "", "return", "temp_dir", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.sashimi.sashimi.DownPool.__init__": [[23, 33], ["torch.Module.__init__", "src.models.sequence.ss.standalone.s4.LinearActivation"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.s4.LinearActivation"], ["residual", "=", "None", ",", "\n", "norm", "=", "None", ",", "\n", "initializer", "=", "None", ",", "\n", "l_max", "=", "-", "1", ",", "\n", "transposed", "=", "True", ",", "\n", "interp", "=", "0", ",", "\n", "act_pool", "=", "None", ",", "\n", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "assert", "l_max", ">", "0", ",", "\"SaShiMi must have length passed in\"", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.sashimi.sashimi.DownPool.forward": [[35, 39], ["einops.rearrange", "sashimi.DownPool.linear"], "methods", ["None"], ["H", "=", "d_model", "\n", "\n", "self", ".", "interp", "=", "interp", "\n", "if", "interp", ">", "0", ":", "\n", "            ", "assert", "l_max", "%", "interp", "==", "0", ",", "\"Interpolation level must be a factor of the length\"", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.sashimi.sashimi.DownPool.step": [[40, 55], ["state.append", "len", "einops.rearrange", "x.squeeze.squeeze.unsqueeze", "sashimi.DownPool.linear", "x.squeeze.squeeze.squeeze", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack"], "methods", ["None"], ["l_max", "=", "l_max", "//", "interp", "\n", "\n", "", "L", "=", "l_max", "\n", "self", ".", "L", "=", "L", "\n", "self", ".", "transposed", "=", "transposed", "\n", "\n", "# Layer arguments", "\n", "layer_cfg", "=", "layer", ".", "copy", "(", ")", "\n", "layer_cfg", "[", "'dropout'", "]", "=", "dropout", "\n", "layer_cfg", "[", "'transposed'", "]", "=", "self", ".", "transposed", "\n", "# layer_cfg['initializer'] = initializer", "\n", "layer_cfg", "[", "'l_max'", "]", "=", "L", "\n", "\n", "ff_cfg", "=", "{", "\n", "'_name_'", ":", "'ff'", ",", "\n", "'expand'", ":", "ff", ",", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.sashimi.sashimi.DownPool.default_state": [[56, 58], ["None"], "methods", ["None"], ["'transposed'", ":", "self", ".", "transposed", ",", "\n", "'activation'", ":", "'gelu'", ",", "\n", "'initializer'", ":", "initializer", ",", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.sashimi.sashimi.UpPool.__init__": [[61, 71], ["torch.Module.__init__", "src.models.sequence.ss.standalone.s4.LinearActivation"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.s4.LinearActivation"], ["\n", "def", "_residual", "(", "d", ",", "i", ",", "layer", ")", ":", "\n", "            ", "return", "SequenceResidualBlock", "(", "\n", "d", ",", "\n", "i", ",", "\n", "prenorm", "=", "prenorm", ",", "\n", "dropout", "=", "dropres", ",", "\n", "layer", "=", "layer", ",", "\n", "residual", "=", "residual", "if", "residual", "is", "not", "None", "else", "'R'", ",", "\n", "norm", "=", "norm", ",", "\n", "pool", "=", "None", ",", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.sashimi.sashimi.UpPool.forward": [[73, 82], ["sashimi.UpPool.linear", "torch.pad", "torch.pad", "torch.pad", "einops.rearrange"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.sc.pad", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.sc.pad", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.sc.pad"], ["\n", "# Down blocks", "\n", "", "d_layers", "=", "[", "]", "\n", "for", "p", "in", "pool", ":", "\n", "# Add sequence downsampling and feature expanding", "\n", "            ", "d_layers", ".", "append", "(", "DownPool", "(", "H", ",", "H", "*", "expand", ",", "stride", "=", "p", ",", "transposed", "=", "self", ".", "transposed", ",", "activation", "=", "act_pool", ")", ")", "\n", "L", "//=", "p", "\n", "layer_cfg", "[", "'l_max'", "]", "=", "L", "\n", "H", "*=", "expand", "\n", "", "self", ".", "d_layers", "=", "nn", ".", "ModuleList", "(", "d_layers", ")", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.sashimi.sashimi.UpPool.step": [[83, 98], ["len", "len", "einops.rearrange.unsqueeze", "sashimi.UpPool.linear", "einops.rearrange.squeeze", "einops.rearrange", "list", "torch.unbind", "torch.unbind", "torch.unbind", "torch.unbind", "torch.unbind", "torch.unbind", "torch.unbind", "torch.unbind", "torch.unbind"], "methods", ["None"], ["\n", "# Center block", "\n", "c_layers", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "n_layers", ")", ":", "\n", "            ", "c_layers", ".", "append", "(", "_residual", "(", "H", ",", "i", "+", "1", ",", "layer_cfg", ")", ")", "\n", "if", "ff", ">", "0", ":", "c_layers", ".", "append", "(", "_residual", "(", "H", ",", "i", "+", "1", ",", "ff_cfg", ")", ")", "\n", "", "self", ".", "c_layers", "=", "nn", ".", "ModuleList", "(", "c_layers", ")", "\n", "\n", "# Up blocks", "\n", "u_layers", "=", "[", "]", "\n", "for", "p", "in", "pool", "[", ":", ":", "-", "1", "]", ":", "\n", "            ", "block", "=", "[", "]", "\n", "H", "//=", "expand", "\n", "L", "*=", "p", "\n", "layer_cfg", "[", "'l_max'", "]", "=", "L", "\n", "block", ".", "append", "(", "UpPool", "(", "H", "*", "expand", ",", "H", ",", "stride", "=", "p", ",", "transposed", "=", "self", ".", "transposed", ",", "activation", "=", "act_pool", ")", ")", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.sashimi.sashimi.UpPool.default_state": [[99, 103], ["torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "list", "torch.unbind", "torch.unbind", "torch.unbind", "torch.unbind", "torch.unbind", "torch.unbind", "torch.unbind", "torch.unbind", "torch.unbind"], "methods", ["None"], ["\n", "for", "i", "in", "range", "(", "n_layers", ")", ":", "\n", "                ", "block", ".", "append", "(", "_residual", "(", "H", ",", "i", "+", "1", ",", "layer_cfg", ")", ")", "\n", "if", "ff", ">", "0", ":", "block", ".", "append", "(", "_residual", "(", "H", ",", "i", "+", "1", ",", "ff_cfg", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.sashimi.sashimi.FFBlock.__init__": [[107, 138], ["torch.Module.__init__", "src.models.sequence.ss.standalone.s4.LinearActivation", "src.models.sequence.ss.standalone.s4.LinearActivation", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Dropout2d", "torch.Dropout2d", "torch.Dropout2d", "torch.Identity", "torch.Identity", "torch.Identity"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.s4.LinearActivation", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.s4.LinearActivation"], ["\n", "assert", "H", "==", "d_model", "\n", "\n", "self", ".", "norm", "=", "nn", ".", "LayerNorm", "(", "H", ")", "\n", "\n", "if", "interp", ">", "0", ":", "\n", "            ", "interp_layers", "=", "[", "]", "\n", "assert", "interp", "%", "2", "==", "0", "\n", "for", "i", "in", "range", "(", "int", "(", "math", ".", "log2", "(", "interp", ")", ")", ")", ":", "\n", "                ", "block", "=", "[", "]", "\n", "for", "j", "in", "range", "(", "2", ")", ":", "\n", "                    ", "block", ".", "append", "(", "_residual", "(", "H", ",", "i", "+", "1", ",", "layer_cfg", ")", ")", "\n", "if", "ff", ">", "0", ":", "block", ".", "append", "(", "_residual", "(", "H", ",", "i", "+", "1", ",", "ff_cfg", ")", ")", "\n", "\n", "", "interp_layers", ".", "append", "(", "nn", ".", "ModuleList", "(", "block", ")", ")", "\n", "\n", "", "self", ".", "interp_layers", "=", "nn", ".", "ModuleList", "(", "interp_layers", ")", "\n", "\n", "", "", "@", "property", "\n", "def", "d_output", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "d_model", "\n", "\n", "", "def", "forward", "(", "self", ",", "x", ",", "state", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        input: (batch, length, d_input)\n        output: (batch, length, d_output)\n        \"\"\"", "\n", "if", "self", ".", "interp", ">", "0", ":", "\n", "# Interpolation will be used to reconstruct \"missing\" frames", "\n", "# Subsample the input sequence and run the SNet on that", "\n", "            ", "x_all", "=", "x", "\n", "x", "=", "x", "[", ":", ",", ":", ":", "self", ".", "interp", ",", ":", "]", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.sashimi.sashimi.FFBlock.forward": [[140, 142], ["sashimi.FFBlock.ff"], "methods", ["None"], ["y", "=", "torch", ".", "zeros_like", "(", "x_all", ")", "\n", "# Run the interpolating layers", "\n", "interp_level", "=", "self", ".", "interp", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.sashimi.sashimi.FFBlock.default_state": [[143, 145], ["None"], "methods", ["None"], ["for", "block", "in", "self", ".", "interp_layers", ":", "\n", "# Pad to the right and discard the output of the first input", "\n", "# (creates dependence on the next time step for interpolation)", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.sashimi.sashimi.FFBlock.step": [[146, 149], ["sashimi.FFBlock.ff().squeeze", "sashimi.FFBlock.ff", "x.unsqueeze"], "methods", ["None"], ["                ", "z", "=", "x_all", "[", ":", ",", ":", ":", "interp_level", ",", ":", "]", "\n", "if", "self", ".", "transposed", ":", "z", "=", "z", ".", "transpose", "(", "1", ",", "2", ")", "\n", "for", "layer", "in", "block", ":", "\n", "                    ", "z", ",", "_", "=", "layer", "(", "z", ")", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.sashimi.sashimi.ResidualBlock.__init__": [[153, 173], ["torch.Module.__init__", "torch.LayerNorm", "torch.LayerNorm", "torch.LayerNorm", "torch.Dropout2d", "torch.Dropout2d", "torch.Dropout2d", "torch.Identity", "torch.Identity", "torch.Identity"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__"], ["y", "[", ":", ",", "interp_level", "//", "2", "-", "1", ":", ":", "interp_level", ",", ":", "]", "+=", "z", "\n", "interp_level", "=", "int", "(", "interp_level", "//", "2", ")", "\n", "\n", "", "", "if", "self", ".", "transposed", ":", "x", "=", "x", ".", "transpose", "(", "1", ",", "2", ")", "\n", "\n", "# Down blocks", "\n", "outputs", "=", "[", "]", "\n", "outputs", ".", "append", "(", "x", ")", "\n", "for", "layer", "in", "self", ".", "d_layers", ":", "\n", "            ", "x", ",", "_", "=", "layer", "(", "x", ")", "\n", "outputs", ".", "append", "(", "x", ")", "\n", "\n", "# Center block", "\n", "", "for", "layer", "in", "self", ".", "c_layers", ":", "\n", "            ", "x", ",", "_", "=", "layer", "(", "x", ")", "\n", "", "x", "=", "x", "+", "outputs", ".", "pop", "(", ")", "# add a skip connection to the last output of the down block", "\n", "\n", "for", "block", "in", "self", ".", "u_layers", ":", "\n", "            ", "for", "layer", "in", "block", ":", "\n", "                ", "x", ",", "_", "=", "layer", "(", "x", ")", "\n", "if", "isinstance", "(", "layer", ",", "UpPool", ")", ":", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.sashimi.sashimi.ResidualBlock.forward": [[174, 193], ["sashimi.ResidualBlock.norm().transpose", "sashimi.ResidualBlock.layer", "sashimi.ResidualBlock.dropout", "sashimi.ResidualBlock.norm", "sashimi.ResidualBlock.transpose"], "methods", ["None"], ["# Before modeling layer in the block", "\n", "                    ", "x", "=", "x", "+", "outputs", ".", "pop", "(", ")", "\n", "outputs", ".", "append", "(", "x", ")", "\n", "", "", "x", "=", "x", "+", "outputs", ".", "pop", "(", ")", "# add a skip connection from the input of the modeling part of this up block", "\n", "\n", "# feature projection", "\n", "", "if", "self", ".", "transposed", ":", "x", "=", "x", ".", "transpose", "(", "1", ",", "2", ")", "# (batch, length, expand)", "\n", "x", "=", "self", ".", "norm", "(", "x", ")", "\n", "\n", "if", "self", ".", "interp", ">", "0", ":", "\n", "            ", "y", "[", ":", ",", "self", ".", "interp", "-", "1", ":", ":", "self", ".", "interp", ",", ":", "]", "=", "x", "\n", "x", "=", "y", "\n", "\n", "", "return", "x", ",", "None", "# required to return a state", "\n", "\n", "", "def", "default_state", "(", "self", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\" x: (batch) \"\"\"", "\n", "layers", "=", "list", "(", "self", ".", "d_layers", ")", "+", "list", "(", "self", ".", "c_layers", ")", "+", "[", "layer", "for", "block", "in", "self", ".", "u_layers", "for", "layer", "in", "block", "]", "\n", "return", "[", "layer", ".", "default_state", "(", "*", "args", ",", "**", "kwargs", ")", "for", "layer", "in", "layers", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.sashimi.sashimi.ResidualBlock.default_state": [[194, 196], ["sashimi.ResidualBlock.layer.default_state"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.sashimi.sashimi.Sashimi.default_state"], ["", "def", "step", "(", "self", ",", "x", ",", "state", ",", "**", "kwargs", ")", ":", "\n", "        "]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.sashimi.sashimi.ResidualBlock.step": [[197, 210], ["sashimi.ResidualBlock.norm", "sashimi.ResidualBlock.layer.step"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.sashimi.sashimi.Sashimi.step"], ["\n", "# States will be popped in reverse order for convenience", "\n", "state", "=", "state", "[", ":", ":", "-", "1", "]", "\n", "\n", "# Down blocks", "\n", "outputs", "=", "[", "]", "# Store all layers for SaShiMi", "\n", "next_state", "=", "[", "]", "\n", "for", "layer", "in", "self", ".", "d_layers", ":", "\n", "            ", "outputs", ".", "append", "(", "x", ")", "\n", "x", ",", "_next_state", "=", "layer", ".", "step", "(", "x", ",", "state", "=", "state", ".", "pop", "(", ")", ",", "**", "kwargs", ")", "\n", "next_state", ".", "append", "(", "_next_state", ")", "\n", "if", "x", "is", "None", ":", "break", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.sashimi.sashimi.Sashimi.__init__": [[213, 326], ["torch.Module.__init__", "range", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.LayerNorm", "torch.LayerNorm", "torch.LayerNorm", "src.models.sequence.ss.standalone.s4.S4", "sashimi.ResidualBlock", "sashimi.FFBlock", "sashimi.ResidualBlock", "d_layers.append", "c_layers.append", "block.append", "range", "u_layers.append", "range", "sashimi.DownPool", "sashimi.Sashimi.__init__.s4_block"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__"], ["# Skip computations since we've downsized", "\n", "            ", "skipped", "=", "len", "(", "self", ".", "d_layers", ")", "-", "len", "(", "outputs", ")", "\n", "for", "_", "in", "range", "(", "skipped", "+", "len", "(", "self", ".", "c_layers", ")", ")", ":", "\n", "                ", "next_state", ".", "append", "(", "state", ".", "pop", "(", ")", ")", "\n", "", "for", "i", "in", "range", "(", "skipped", ")", ":", "\n", "                ", "for", "_", "in", "range", "(", "len", "(", "self", ".", "u_layers", "[", "i", "]", ")", ")", ":", "\n", "                    ", "next_state", ".", "append", "(", "state", ".", "pop", "(", ")", ")", "\n", "", "", "u_layers", "=", "list", "(", "self", ".", "u_layers", ")", "[", "skipped", ":", "]", "\n", "", "else", ":", "\n", "            ", "outputs", ".", "append", "(", "x", ")", "\n", "for", "layer", "in", "self", ".", "c_layers", ":", "\n", "                ", "x", ",", "_next_state", "=", "layer", ".", "step", "(", "x", ",", "state", "=", "state", ".", "pop", "(", ")", ",", "**", "kwargs", ")", "\n", "next_state", ".", "append", "(", "_next_state", ")", "\n", "", "x", "=", "x", "+", "outputs", ".", "pop", "(", ")", "\n", "u_layers", "=", "self", ".", "u_layers", "\n", "\n", "", "for", "block", "in", "u_layers", ":", "\n", "            ", "for", "layer", "in", "block", ":", "\n", "                ", "x", ",", "_next_state", "=", "layer", ".", "step", "(", "x", ",", "state", "=", "state", ".", "pop", "(", ")", ",", "**", "kwargs", ")", "\n", "next_state", ".", "append", "(", "_next_state", ")", "\n", "if", "isinstance", "(", "layer", ",", "UpPool", ")", ":", "\n", "# Before modeling layer in the block", "\n", "                    ", "x", "=", "x", "+", "outputs", ".", "pop", "(", ")", "\n", "outputs", ".", "append", "(", "x", ")", "\n", "", "", "x", "=", "x", "+", "outputs", ".", "pop", "(", ")", "\n", "\n", "# feature projection", "\n", "", "x", "=", "self", ".", "norm", "(", "x", ")", "\n", "return", "x", ",", "next_state", "\n", "", "", ""]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.sashimi.sashimi.Sashimi.forward": [[327, 366], ["sashimi.Sashimi.transpose", "outputs.append", "sashimi.Sashimi.transpose", "sashimi.Sashimi.norm", "layer", "outputs.append", "layer", "outputs.pop", "layer", "layer", "isinstance", "outputs.pop", "outputs.pop", "outputs.append", "outputs.pop"], "methods", ["None"], []], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.sashimi.sashimi.Sashimi.default_state": [[367, 370], ["layer.default_state", "list", "list"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.sashimi.sashimi.Sashimi.default_state"], []], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.sashimi.sashimi.Sashimi.step": [[371, 430], ["sashimi.Sashimi.norm", "outputs.append", "layer.step", "next_state.append", "range", "outputs.append", "len", "len", "next_state.append", "range", "range", "layer.step", "next_state.append", "outputs.pop", "state.pop", "len", "state.pop", "next_state.append", "list", "range", "list", "layer.step", "next_state.append", "layer.step", "next_state.append", "isinstance", "outputs.pop", "state.pop", "len", "next_state.append", "state.pop", "outputs.pop", "outputs.append", "state.pop", "state.pop", "state.pop", "outputs.pop"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.sashimi.sashimi.Sashimi.step", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.sashimi.sashimi.Sashimi.step", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.sashimi.sashimi.Sashimi.step", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.sashimi.sashimi.Sashimi.step"], []], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.sashimi.sashimi.Sashimi.setup_rnn": [[431, 445], ["sashimi.Sashimi.modules", "hasattr", "module.setup_step"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.s4.S4.setup_step"], []], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.sashimi.generation.generate": [[20, 116], ["torch.inference_mode", "torch.inference_mode", "isinstance", "numpy.zeros", "numpy.zeros", "tqdm.auto.tqdm", "torch.stack().squeeze", "torch.stack().squeeze", "isinstance", "range", "x_t.to.to", "model.encoder", "model.model.step", "model.decoder", "torch.softmax", "torch.distributions.Categorical().sample", "torch.stack().squeeze.append", "isinstance", "model.model.default_state", "torch.stack.append", "probs.squeeze.sort", "csum_probs[].clone", "torch.zeros_like", "torch.zeros_like", "csum_probs.flatten", "x_t.to.cpu().squeeze", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "Categorical().sample.detach().cpu", "probs.sort.values.cumsum", "torch.distributions.Categorical", "probs.squeeze.squeeze", "x.repeat.repeat", "torch.zeros_like.int", "len", "torch.log().cpu().numpy", "torch.log().cpu().numpy", "torch.log().cpu().numpy", "torch.log().cpu().numpy", "x_t.to.cpu", "Categorical().sample.detach", "[].repeat().flatten", "probs.sort.indices.flatten", "torch.log().cpu", "torch.log().cpu", "torch.log().cpu", "torch.log().cpu", "[].repeat", "torch.log", "torch.log", "torch.log", "torch.log", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "Categorical().sample.squeeze", "torch.arange", "torch.arange"], "function", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.sashimi.sashimi.Sashimi.step", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.sashimi.sashimi.Sashimi.default_state"], ["@", "torch", ".", "inference_mode", "(", ")", "\n", "def", "generate", "(", "\n", "model", ",", "\n", "batch", ",", "\n", "tau", "=", "1.0", ",", "\n", "prefix", "=", "0", ",", "\n", "T", "=", "None", ",", "\n", "debug", "=", "False", ",", "\n", "top_p", "=", "1.0", ",", "\n", "benchmark", "=", "False", ",", "\n", "calc_logprobs", "=", "False", ",", "\n", ")", ":", "\n", "    ", "x", ",", "_", ",", "*", "_", "=", "batch", "\n", "T", "=", "x", ".", "shape", "[", "1", "]", "if", "T", "is", "None", "else", "T", "\n", "\n", "# Set up the initial state", "\n", "if", "isinstance", "(", "model", ".", "model", ",", "SampleRNN", ")", ":", "\n", "        ", "model", ".", "_state", "=", "None", "\n", "", "elif", "isinstance", "(", "model", ".", "model", ",", "WaveNetModel", ")", ":", "\n", "        ", "model", ".", "_state", "=", "None", "\n", "if", "not", "benchmark", ":", "\n", "            ", "prefix", "+=", "model", ".", "model", ".", "receptive_field", "\n", "T", "+=", "model", ".", "model", ".", "receptive_field", "\n", "if", "x", ".", "shape", "[", "1", "]", "==", "1", ":", "\n", "                ", "x", "=", "x", ".", "repeat", "(", "1", ",", "prefix", "+", "1", ")", "\n", "", "", "", "else", ":", "\n", "        ", "model", ".", "_state", "=", "model", ".", "model", ".", "default_state", "(", "*", "x", ".", "shape", "[", ":", "1", "]", ",", "device", "=", "'cuda'", ")", "\n", "\n", "# First sample", "\n", "", "x_t", "=", "x", "[", ":", ",", "0", "]", "\n", "y_all", "=", "[", "]", "\n", "logprobs", "=", "np", ".", "zeros", "(", "x", ".", "shape", "[", "0", "]", ")", "\n", "entropy", "=", "np", ".", "zeros", "(", "x", ".", "shape", "[", "0", "]", ")", "\n", "\n", "if", "debug", ":", "\n", "        ", "y_raw", "=", "[", "]", "\n", "\n", "# Generation loop", "\n", "", "for", "t", "in", "tqdm", "(", "range", "(", "T", ")", ")", ":", "\n", "\n", "# Step through the model with the current sample", "\n", "        ", "x_t", "=", "x_t", ".", "to", "(", "'cuda'", ")", "\n", "y_t", ",", "*", "_", "=", "model", ".", "encoder", "(", "x_t", ")", "\n", "y_t", ",", "state", "=", "model", ".", "model", ".", "step", "(", "y_t", ",", "state", "=", "model", ".", "_state", ")", "\n", "model", ".", "_state", "=", "state", "\n", "y_t", ",", "*", "_", "=", "model", ".", "decoder", "(", "y_t", ",", "state", ")", "\n", "\n", "if", "debug", ":", "\n", "            ", "y_raw", ".", "append", "(", "y_t", ".", "detach", "(", ")", ".", "cpu", "(", ")", ")", "\n", "\n", "# Output distribution", "\n", "", "probs", "=", "F", ".", "softmax", "(", "y_t", ",", "dim", "=", "-", "1", ")", "\n", "\n", "# Optional: nucleus sampling", "\n", "if", "top_p", "<", "1.0", ":", "\n", "            ", "sorted_probs", "=", "probs", ".", "sort", "(", "dim", "=", "-", "1", ",", "descending", "=", "True", ")", "\n", "csum_probs", "=", "sorted_probs", ".", "values", ".", "cumsum", "(", "dim", "=", "-", "1", ")", ">", "top_p", "\n", "csum_probs", "[", "...", ",", "1", ":", "]", "=", "csum_probs", "[", "...", ",", ":", "-", "1", "]", ".", "clone", "(", ")", "\n", "csum_probs", "[", "...", ",", "0", "]", "=", "0", "\n", "indices_to_remove", "=", "torch", ".", "zeros_like", "(", "csum_probs", ")", "\n", "indices_to_remove", "[", "torch", ".", "arange", "(", "sorted_probs", ".", "indices", ".", "shape", "[", "0", "]", ")", "[", ":", ",", "None", "]", ".", "repeat", "(", "1", ",", "sorted_probs", ".", "indices", ".", "shape", "[", "1", "]", ")", ".", "flatten", "(", ")", ",", "sorted_probs", ".", "indices", ".", "flatten", "(", ")", "]", "=", "csum_probs", ".", "flatten", "(", ")", "\n", "y_t", "=", "y_t", "+", "indices_to_remove", ".", "int", "(", ")", "*", "(", "-", "1e20", ")", "\n", "\n", "# Sample from the distribution", "\n", "", "y_t", "=", "Categorical", "(", "logits", "=", "y_t", "/", "tau", ")", ".", "sample", "(", ")", "\n", "\n", "# Feed back to the model", "\n", "if", "t", "<", "prefix", ":", "\n", "            ", "x_t", "=", "x", "[", ":", ",", "t", "+", "1", "]", "\n", "", "else", ":", "\n", "            ", "x_t", "=", "y_t", "\n", "\n", "# Calculate the log-likelihood", "\n", "if", "calc_logprobs", ":", "\n", "                ", "probs", "=", "probs", ".", "squeeze", "(", "1", ")", "\n", "if", "len", "(", "y_t", ".", "shape", ")", ">", "1", ":", "\n", "                    ", "logprobs", "+=", "torch", ".", "log", "(", "probs", "[", "torch", ".", "arange", "(", "probs", ".", "shape", "[", "0", "]", ")", ",", "y_t", ".", "squeeze", "(", "1", ")", "]", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "", "else", ":", "\n", "                    ", "logprobs", "+=", "torch", ".", "log", "(", "probs", "[", "torch", ".", "arange", "(", "probs", ".", "shape", "[", "0", "]", ")", ",", "y_t", "]", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "", "entropy", "+=", "-", "(", "probs", "*", "(", "probs", "+", "1e-6", ")", ".", "log", "(", ")", ")", ".", "sum", "(", "dim", "=", "-", "1", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "\n", "", "", "y_all", ".", "append", "(", "x_t", ".", "cpu", "(", ")", ".", "squeeze", "(", ")", ")", "\n", "\n", "", "y_all", "=", "torch", ".", "stack", "(", "y_all", ")", ".", "squeeze", "(", ")", "\n", "\n", "if", "isinstance", "(", "model", ".", "model", ",", "WaveNetModel", ")", "and", "not", "benchmark", ":", "\n", "        ", "y_all", "=", "y_all", "[", "model", ".", "model", ".", "receptive_field", ":", "]", "\n", "\n", "", "if", "not", "calc_logprobs", ":", "\n", "        ", "if", "debug", ":", "\n", "            ", "y_raw", "=", "torch", ".", "stack", "(", "y_raw", ")", "\n", "return", "y_all", ",", "y_raw", "\n", "", "return", "y_all", "\n", "", "else", ":", "\n", "        ", "assert", "not", "debug", "\n", "return", "y_all", ",", "logprobs", ",", "entropy", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.sashimi.generation.main": [[118, 197], ["hydra.initialize", "print", "overrides.append", "overrides.append", "src.utils.train.process_config", "src.utils.train.print_config", "pytorch_lightning.seed_everything", "torch.cuda.is_available", "torch.cuda.is_available", "train.SequenceLightningModule", "train.SequenceLightningModule.setup", "train.SequenceLightningModule.to", "torch.load", "torch.load", "train.SequenceLightningModule.load_state_dict", "train.SequenceLightningModule.modules", "train.SequenceLightningModule.eval", "generation.generate", "src.dataloaders.audio.mu_law_decode", "enumerate", "numpy.save", "overrides.append", "hasattr", "train.SequenceLightningModule.val_dataloader", "next", "torchaudio.save", "hydra.compose", "hydra.compose", "hydra.compose", "module.setup_step", "iter", "x.repeat", "numpy.argsort", "d.unsqueeze", "hydra.compose", "torch.zeros().to", "torch.zeros().to", "logprobs.flatten", "torch.zeros", "torch.zeros"], "function", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.ss.lssl.Platypus.initialize", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.utils.train.process_config", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.utils.train.print_config", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.None.example.eval", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.sashimi.generation.generate", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.sc.mu_law_decode", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.standalone.s4.S4.setup_step"], ["", "", "def", "main", "(", "args", ")", ":", "\n", "# Initialize hydra", "\n", "    ", "initialize", "(", "config_path", "=", "\"../configs\"", ")", "\n", "\n", "print", "(", "\"Loading model...\"", ")", "\n", "\n", "overrides", "=", "[", "]", "\n", "if", "not", "args", ".", "load_data", ":", "\n", "        ", "overrides", ".", "append", "(", "'train.disable_dataset=true'", ")", "\n", "\n", "", "overrides", ".", "append", "(", "f'loader.batch_size={args.n_samples}'", ")", "\n", "overrides", ".", "append", "(", "f'experiment={args.model}-{args.dataset}'", ")", "\n", "\n", "# Load in the model config", "\n", "if", "args", ".", "model", "==", "'sashimi'", ":", "\n", "        ", "if", "args", ".", "dataset", "==", "'sc09'", ":", "\n", "            ", "config", "=", "compose", "(", "config_name", "=", "\"config.yaml\"", ",", "overrides", "=", "overrides", "+", "[", "'model.layer.hurwitz=false'", ",", "'decoder.mode=last'", "]", ")", "\n", "", "else", ":", "\n", "            ", "config", "=", "compose", "(", "config_name", "=", "\"config.yaml\"", ",", "overrides", "=", "overrides", "+", "[", "'model.layer.hurwitz=false'", ",", "'model.layer.postact=null'", ",", "'decoder.mode=last'", "]", ")", "\n", "\n", "", "", "elif", "args", ".", "model", "==", "'wavenet'", ":", "\n", "        ", "config", "=", "compose", "(", "config_name", "=", "\"config.yaml\"", ",", "overrides", "=", "overrides", ")", "\n", "\n", "", "elif", "args", ".", "model", "==", "'samplernn'", ":", "\n", "        ", "config", "=", "compose", "(", "config_name", "=", "\"config.yaml\"", ",", "overrides", "=", "overrides", ")", "\n", "\n", "", "config", "=", "utils", ".", "train", ".", "process_config", "(", "config", ")", "\n", "utils", ".", "train", ".", "print_config", "(", "config", ",", "resolve", "=", "True", ")", "\n", "\n", "# Seed ", "\n", "pl", ".", "seed_everything", "(", "config", ".", "train", ".", "seed", ",", "workers", "=", "True", ")", "\n", "\n", "assert", "torch", ".", "cuda", ".", "is_available", "(", ")", ",", "'Use a GPU for generation.'", "\n", "\n", "# Create the Lightning Module", "\n", "model", "=", "SequenceLightningModule", "(", "config", ")", "\n", "model", ".", "setup", "(", ")", "\n", "model", ".", "to", "(", "'cuda'", ")", "\n", "\n", "# Load checkpoint", "\n", "state_dict", "=", "torch", ".", "load", "(", "f'sashimi/checkpoints/{args.model}_{args.dataset}.pt'", ",", "map_location", "=", "'cuda'", ")", "\n", "model", ".", "load_state_dict", "(", "state_dict", ")", "\n", "\n", "# Setup: required for S4 modules in SaShiMi", "\n", "for", "module", "in", "model", ".", "modules", "(", ")", ":", "\n", "        ", "if", "hasattr", "(", "module", ",", "'setup_step'", ")", ":", "module", ".", "setup_step", "(", "mode", "=", "'dense'", ")", "\n", "", "model", ".", "eval", "(", ")", "\n", "\n", "if", "args", ".", "load_data", ":", "\n", "# Get the eval dataloaders", "\n", "        ", "eval_dataloaders", "=", "model", ".", "val_dataloader", "(", ")", "\n", "dl", "=", "eval_dataloaders", "[", "0", "]", "if", "args", ".", "split", "==", "'val'", "else", "eval_dataloaders", "[", "1", "]", "\n", "\n", "# Construct a batch", "\n", "x", ",", "_", ",", "*", "_", "=", "next", "(", "iter", "(", "dl", ")", ")", "\n", "batch", "=", "(", "x", ".", "repeat", "(", "args", ".", "n_reps", ",", "1", ")", ",", "None", ",", "None", ")", "\n", "", "else", ":", "\n", "        ", "assert", "args", ".", "prefix", "==", "0", ",", "'Only unconditional generation when data is not loaded.'", "\n", "batch", "=", "(", "torch", ".", "zeros", "(", "args", ".", "n_samples", "*", "args", ".", "n_reps", ",", "1", ")", ".", "to", "(", "torch", ".", "long", ")", "+", "128", ",", "None", ",", "None", ")", "\n", "\n", "# Generate", "\n", "", "y", ",", "logprobs", ",", "_", "=", "generate", "(", "\n", "model", ",", "# lightning module (SequenceLightningModule from `train.py`)", "\n", "batch", ",", "# pass data to condition the generation", "\n", "prefix", "=", "args", ".", "prefix", ",", "# length of conditioning prefix", "\n", "T", "=", "args", ".", "sample_len", ",", "# length of generated sequence", "\n", "top_p", "=", "args", ".", "top_p", ",", "# nucleus sampling: always set to 1.0 for SaShiMi experiments", "\n", "tau", "=", "args", ".", "temp", ",", "# temperature: always set to 1.0 for SaShiMi experiments", "\n", "calc_logprobs", "=", "True", ",", "# calc exact likelihoods", "\n", ")", "\n", "\n", "# Decode quantization", "\n", "y", "=", "mu_law_decode", "(", "y", ".", "T", ")", "\n", "\n", "# Sort based on likelihoods and save", "\n", "y", "=", "y", "[", "np", ".", "argsort", "(", "logprobs", ".", "flatten", "(", ")", ")", "]", "\n", "for", "i", ",", "d", "in", "enumerate", "(", "y", ")", ":", "\n", "        ", "torchaudio", ".", "save", "(", "f'{args.save_dir}/unconditional_{args.dataset}_{args.model}_len_{args.sample_len/16000.:.2f}s_gen_{i+1}.wav'", ",", "d", ".", "unsqueeze", "(", "0", ")", ",", "16000", ")", "\n", "", "np", ".", "save", "(", "f'{args.save_dir}/unconditional_{args.dataset}_{args.model}_len_{args.sample_len/16000.:.2f}s_logprobs.npy'", ",", "logprobs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.sashimi.metrics.fid": [[7, 32], ["numpy.mean", "numpy.mean", "numpy.cov", "numpy.cov", "scipy.linalg.sqrtm", "np.cov.dot", "numpy.isfinite().all", "print", "scipy.linalg.sqrtm", "numpy.sum", "numpy.trace", "numpy.eye", "numpy.square", "numpy.isfinite"], "function", ["None"], ["\n", "def", "binary_cross_entropy", "(", "logits", ",", "y", ")", ":", "\n", "# BCE loss requires squeezing last dimension of logits so it has the same shape as y", "\n", "# requires y to be float, since it's overloaded to represent a probability", "\n", "    ", "return", "F", ".", "binary_cross_entropy_with_logits", "(", "logits", ".", "squeeze", "(", "-", "1", ")", ",", "y", ".", "float", "(", ")", ")", "\n", "\n", "\n", "", "def", "binary_accuracy", "(", "logits", ",", "y", ")", ":", "\n", "    ", "return", "torch", ".", "eq", "(", "logits", ".", "squeeze", "(", "-", "1", ")", ">=", "0", ",", "y", ")", ".", "float", "(", ")", ".", "mean", "(", ")", "\n", "\n", "\n", "", "def", "cross_entropy", "(", "logits", ",", "y", ")", ":", "\n", "    ", "logits", "=", "logits", ".", "view", "(", "-", "1", ",", "logits", ".", "shape", "[", "-", "1", "]", ")", "\n", "y", "=", "y", ".", "view", "(", "-", "1", ")", "\n", "return", "F", ".", "cross_entropy", "(", "logits", ",", "y", ")", "\n", "\n", "\n", "", "def", "accuracy", "(", "logits", ",", "y", ")", ":", "\n", "    ", "logits", "=", "logits", ".", "view", "(", "-", "1", ",", "logits", ".", "shape", "[", "-", "1", "]", ")", "\n", "if", "y", ".", "numel", "(", ")", ">", "logits", ".", "shape", "[", "0", "]", ":", "\n", "# Mixup leads to this case: use argmax class", "\n", "        ", "y", "=", "y", ".", "argmax", "(", "dim", "=", "-", "1", ")", "\n", "", "y", "=", "y", ".", "view", "(", "-", "1", ")", "\n", "return", "torch", ".", "eq", "(", "torch", ".", "argmax", "(", "logits", ",", "dim", "=", "-", "1", ")", ",", "y", ")", ".", "float", "(", ")", ".", "mean", "(", ")", "\n", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.sashimi.metrics.inception_score": [[33, 58], ["numpy.random.seed", "numpy.mean", "scipy.stats.entropy().mean", "numpy.exp", "numpy.random.permutation", "scipy.stats.entropy", "len", "len", "len", "numpy.repeat", "len"], "function", ["None"], ["", "def", "accuracy_at_k", "(", "logits", ",", "y", ",", "k", "=", "1", ")", ":", "\n", "    ", "logits", "=", "logits", ".", "view", "(", "-", "1", ",", "logits", ".", "shape", "[", "-", "1", "]", ")", "\n", "if", "y", ".", "numel", "(", ")", ">", "logits", ".", "shape", "[", "0", "]", ":", "\n", "# Mixup leads to this case: use argmax class", "\n", "        ", "y", "=", "y", ".", "argmax", "(", "dim", "=", "-", "1", ")", "\n", "", "y", "=", "y", ".", "view", "(", "-", "1", ")", "\n", "return", "torch", ".", "topk", "(", "logits", ",", "k", ",", "dim", "=", "-", "1", ")", "[", "1", "]", ".", "eq", "(", "y", ".", "unsqueeze", "(", "-", "1", ")", ")", ".", "any", "(", "dim", "=", "-", "1", ")", ".", "float", "(", ")", ".", "mean", "(", ")", "\n", "\n", "", "def", "f1_binary", "(", "logits", ",", "y", ")", ":", "\n", "    ", "logits", "=", "logits", ".", "view", "(", "-", "1", ",", "logits", ".", "shape", "[", "-", "1", "]", ")", "\n", "y", "=", "y", ".", "view", "(", "-", "1", ")", "\n", "y_hat", "=", "torch", ".", "argmax", "(", "logits", ",", "dim", "=", "-", "1", ")", "\n", "return", "f1_score", "(", "y", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ",", "y_hat", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ",", "average", "=", "\"binary\"", ")", "\n", "\n", "\n", "", "def", "f1_macro", "(", "logits", ",", "y", ")", ":", "\n", "    ", "logits", "=", "logits", ".", "view", "(", "-", "1", ",", "logits", ".", "shape", "[", "-", "1", "]", ")", "\n", "y", "=", "y", ".", "view", "(", "-", "1", ")", "\n", "y_hat", "=", "torch", ".", "argmax", "(", "logits", ",", "dim", "=", "-", "1", ")", "\n", "return", "f1_score", "(", "y", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ",", "y_hat", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ",", "average", "=", "\"macro\"", ")", "\n", "\n", "\n", "", "def", "f1_micro", "(", "logits", ",", "y", ")", ":", "\n", "    ", "logits", "=", "logits", ".", "view", "(", "-", "1", ",", "logits", ".", "shape", "[", "-", "1", "]", ")", "\n", "y", "=", "y", ".", "view", "(", "-", "1", ")", "\n", "y_hat", "=", "torch", ".", "argmax", "(", "logits", ",", "dim", "=", "-", "1", ")", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.sashimi.metrics.modified_inception_score": [[59, 84], ["numpy.random.seed", "len", "range", "numpy.exp", "numpy.random.choice", "scipy.stats.entropy", "all_kls.append", "numpy.mean", "numpy.arange"], "function", ["None"], ["return", "f1_score", "(", "y", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ",", "y_hat", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ",", "average", "=", "\"micro\"", ")", "\n", "\n", "\n", "", "def", "roc_auc_macro", "(", "logits", ",", "y", ")", ":", "\n", "    ", "logits", "=", "logits", ".", "view", "(", "\n", "-", "1", ",", "logits", ".", "shape", "[", "-", "1", "]", "\n", ")", ".", "detach", "(", ")", "# KS: had to add detach to eval while training", "\n", "y", "=", "y", ".", "view", "(", "-", "1", ")", "\n", "return", "roc_auc_score", "(", "\n", "y", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ",", "F", ".", "softmax", "(", "logits", ",", "dim", "=", "-", "1", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "[", ":", ",", "1", "]", ",", "average", "=", "\"macro\"", "\n", ")", "\n", "\n", "\n", "", "def", "roc_auc_micro", "(", "logits", ",", "y", ")", ":", "\n", "    ", "logits", "=", "logits", ".", "view", "(", "-", "1", ",", "logits", ".", "shape", "[", "-", "1", "]", ")", "\n", "y", "=", "y", ".", "view", "(", "-", "1", ")", "\n", "return", "roc_auc_score", "(", "\n", "y", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ",", "F", ".", "softmax", "(", "logits", ",", "dim", "=", "-", "1", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "[", ":", ",", "1", "]", ",", "average", "=", "\"micro\"", "\n", ")", "\n", "\n", "\n", "", "def", "mse", "(", "outs", ",", "y", ",", "len_batch", "=", "None", ")", ":", "\n", "# assert outs.shape[:-1] == y.shape and outs.shape[-1] == 1", "\n", "# outs = outs.squeeze(-1)", "\n", "    ", "if", "len", "(", "y", ".", "shape", ")", "<", "len", "(", "outs", ".", "shape", ")", ":", "\n", "        ", "assert", "outs", ".", "shape", "[", "-", "1", "]", "==", "1", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.sashimi.metrics.am_score": [[85, 95], ["numpy.mean", "numpy.mean", "numpy.mean", "scipy.stats.entropy", "scipy.stats.entropy"], "function", ["None"], ["outs", "=", "outs", ".", "squeeze", "(", "-", "1", ")", "\n", "", "if", "len_batch", "is", "None", ":", "\n", "        ", "return", "F", ".", "mse_loss", "(", "outs", ",", "y", ")", "\n", "", "else", ":", "\n", "# Computes the loss of the first `lens` items in the batches", "\n", "        ", "mask", "=", "torch", ".", "zeros_like", "(", "outs", ",", "dtype", "=", "torch", ".", "bool", ")", "\n", "for", "i", ",", "l", "in", "enumerate", "(", "len_batch", ")", ":", "\n", "            ", "mask", "[", "i", ",", ":", "l", ",", ":", "]", "=", "1", "\n", "", "outs_masked", "=", "torch", ".", "masked_select", "(", "outs", ",", "mask", ")", "\n", "y_masked", "=", "torch", ".", "masked_select", "(", "y", ",", "mask", ")", "\n", "return", "F", ".", "mse_loss", "(", "outs_masked", ",", "y_masked", ")", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.sashimi.metrics.two_proportions_z_test": [[97, 109], ["numpy.sqrt", "scipy.stats.norm.cdf", "abs", "numpy.abs"], "function", ["None"], ["\n", "", "", "def", "mae", "(", "outs", ",", "y", ",", "len_batch", "=", "None", ")", ":", "\n", "# assert outs.shape[:-1] == y.shape and outs.shape[-1] == 1", "\n", "# outs = outs.squeeze(-1)", "\n", "    ", "if", "len", "(", "y", ".", "shape", ")", "<", "len", "(", "outs", ".", "shape", ")", ":", "\n", "        ", "assert", "outs", ".", "shape", "[", "-", "1", "]", "==", "1", "\n", "outs", "=", "outs", ".", "squeeze", "(", "-", "1", ")", "\n", "", "if", "len_batch", "is", "None", ":", "\n", "        ", "return", "F", ".", "l1_loss", "(", "outs", ",", "y", ")", "\n", "", "else", ":", "\n", "# Computes the loss of the first `lens` items in the batches", "\n", "        ", "mask", "=", "torch", ".", "zeros_like", "(", "outs", ",", "dtype", "=", "torch", ".", "bool", ")", "\n", "for", "i", ",", "l", "in", "enumerate", "(", "len_batch", ")", ":", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.sashimi.metrics.ndb_score": [[111, 134], ["sklearn.cluster.KMeans().fit", "KMeans().fit.predict", "KMeans().fit.predict", "numpy.zeros_like", "numpy.unique", "metrics.two_proportions_z_test", "numpy.count_nonzero", "numpy.unique", "len", "len", "len", "len", "sklearn.cluster.KMeans"], "function", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.sc09_classifier.test_speech_commands.two_proportions_z_test"], ["", "outs_masked", "=", "torch", ".", "masked_select", "(", "outs", ",", "mask", ")", "\n", "y_masked", "=", "torch", ".", "masked_select", "(", "y", ",", "mask", ")", "\n", "return", "F", ".", "l1_loss", "(", "outs_masked", ",", "y_masked", ")", "\n", "\n", "\n", "# Metrics that can depend on the loss", "\n", "", "", "def", "loss", "(", "x", ",", "y", ",", "loss_fn", ")", ":", "\n", "    ", "\"\"\" This metric may be useful because the training loss may add extra regularization (e.g. weight decay implemented as L2 penalty), while adding this as a metric skips the additional losses \"\"\"", "\n", "return", "loss_fn", "(", "x", ",", "y", ")", "\n", "\n", "\n", "", "def", "rmse", "(", "x", ",", "y", ",", "loss_fn", ")", ":", "\n", "    ", "return", "loss_fn", "(", "x", ",", "y", ")", "**", "0.5", "# NOTE this isn't exactly correct", "\n", "\n", "\n", "", "def", "bpb", "(", "x", ",", "y", ",", "loss_fn", ")", ":", "\n", "    ", "\"\"\" bits per byte (image density estimation, speech generation, char LM) \"\"\"", "\n", "return", "loss_fn", "(", "x", ",", "y", ")", "/", "math", ".", "log", "(", "2", ")", "\n", "\n", "\n", "", "def", "ppl", "(", "x", ",", "y", ",", "loss_fn", ")", ":", "\n", "    ", "return", "torch", ".", "exp", "(", "loss_fn", "(", "x", ",", "y", ")", ")", "\n", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.sc09_classifier.test_speech_commands.ActivationExtractor.__init__": [[168, 171], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ")", ":", "\n", "        ", "self", ".", "input", "=", "None", "\n", "self", ".", "output", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.sc09_classifier.test_speech_commands.ActivationExtractor.add_hook": [[172, 175], ["None"], "methods", ["None"], ["", "def", "add_hook", "(", "self", ",", "module", ",", "input", ",", "output", ")", ":", "\n", "        ", "self", ".", "input", "=", "input", "\n", "self", ".", "output", "=", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.sc09_classifier.test_speech_commands.ActivationOp.__init__": [[177, 192], ["test_speech_commands.ActivationExtractor", "_nested_getattr.register_forward_hook", "test_speech_commands._nested_getattr", "ValueError"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.sc09_classifier.test_speech_commands._nested_getattr"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "model", ":", "torch", ".", "nn", ".", "Module", ",", "\n", "target_module", ":", "str", ",", "\n", ")", ":", "\n", "        ", "self", ".", "model", "=", "model", "\n", "self", ".", "target_module", "=", "target_module", "\n", "\n", "try", ":", "\n", "            ", "target_module", "=", "_nested_getattr", "(", "model", ",", "target_module", ")", "\n", "", "except", "torch", ".", "nn", ".", "modules", ".", "module", ".", "ModuleAttributeError", ":", "\n", "            ", "raise", "ValueError", "(", "f\"`model` does not have a submodule {target_module}\"", ")", "\n", "\n", "", "self", ".", "extractor", "=", "ActivationExtractor", "(", ")", "\n", "target_module", ".", "register_forward_hook", "(", "self", ".", "extractor", ".", "add_hook", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.sc09_classifier.test_speech_commands.SpeechCommandsDataset.__init__": [[199, 224], ["natsort.natsorted", "os.path.join", "natsort.natsorted", "os.listdir", "f.endswith", "range", "os.listdir", "f.endswith", "os.path.join", "data.append", "len", "os.path.join", "data.append"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "folder", ",", "transform", "=", "None", ",", "classes", "=", "CLASSES", ",", "samples", "=", "False", ")", ":", "\n", "\n", "        ", "self", ".", "classes", "=", "classes", "\n", "self", ".", "transform", "=", "transform", "\n", "\n", "if", "not", "samples", ":", "\n", "            ", "class_to_idx", "=", "{", "classes", "[", "i", "]", ":", "i", "for", "i", "in", "range", "(", "len", "(", "classes", ")", ")", "}", "\n", "\n", "data", "=", "[", "]", "\n", "for", "c", "in", "classes", ":", "\n", "                ", "d", "=", "os", ".", "path", ".", "join", "(", "folder", ",", "c", ")", "\n", "target", "=", "class_to_idx", "[", "c", "]", "\n", "for", "f", "in", "natsorted", "(", "os", ".", "listdir", "(", "d", ")", ")", ":", "\n", "                    ", "if", "f", ".", "endswith", "(", "\".wav\"", ")", ":", "\n", "                        ", "path", "=", "os", ".", "path", ".", "join", "(", "d", ",", "f", ")", "\n", "data", ".", "append", "(", "(", "path", ",", "target", ")", ")", "\n", "\n", "", "", "", "", "else", ":", "\n", "            ", "data", "=", "[", "]", "\n", "for", "f", "in", "natsorted", "(", "os", ".", "listdir", "(", "folder", ")", ")", ":", "\n", "                ", "if", "f", ".", "endswith", "(", "\".wav\"", ")", ":", "\n", "                    ", "path", "=", "os", ".", "path", ".", "join", "(", "folder", ",", "f", ")", "\n", "data", ".", "append", "(", "(", "path", ",", "-", "1", ")", ")", "\n", "\n", "", "", "", "self", ".", "data", "=", "data", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.sc09_classifier.test_speech_commands.SpeechCommandsDataset.__len__": [[225, 227], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "data", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.sc09_classifier.test_speech_commands.SpeechCommandsDataset.__getitem__": [[228, 236], ["test_speech_commands.SpeechCommandsDataset.transform"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.eeg.data_utils.StandardScaler.transform"], ["", "def", "__getitem__", "(", "self", ",", "index", ")", ":", "\n", "        ", "path", ",", "target", "=", "self", ".", "data", "[", "index", "]", "\n", "data", "=", "{", "'path'", ":", "path", ",", "'target'", ":", "target", "}", "\n", "\n", "if", "self", ".", "transform", "is", "not", "None", ":", "\n", "            ", "data", "=", "self", ".", "transform", "(", "data", ")", "\n", "\n", "", "return", "data", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.sc09_classifier.test_speech_commands.fid": [[24, 49], ["numpy.mean", "numpy.mean", "numpy.cov", "numpy.cov", "scipy.linalg.sqrtm", "np.cov.dot", "numpy.isfinite().all", "print", "scipy.linalg.sqrtm", "numpy.sum", "numpy.trace", "numpy.eye", "numpy.square", "numpy.isfinite"], "function", ["None"], ["def", "fid", "(", "feat_data", ",", "feat_gen", ")", ":", "\n", "    ", "\"\"\"\n    Calculate Frechet Inception Distance\n    \"\"\"", "\n", "# Means", "\n", "mu_data", "=", "np", ".", "mean", "(", "feat_data", ",", "axis", "=", "0", ")", "\n", "mu_gen", "=", "np", ".", "mean", "(", "feat_gen", ",", "axis", "=", "0", ")", "\n", "\n", "# Covariances", "\n", "try", ":", "\n", "        ", "sigma_data", "=", "np", ".", "cov", "(", "feat_data", ",", "rowvar", "=", "False", ")", "\n", "sigma_gen", "=", "np", ".", "cov", "(", "feat_gen", ",", "rowvar", "=", "False", ")", "\n", "\n", "covmean", ",", "_", "=", "linalg", ".", "sqrtm", "(", "sigma_data", ".", "dot", "(", "sigma_gen", ")", ",", "disp", "=", "False", ")", "\n", "if", "not", "np", ".", "isfinite", "(", "covmean", ")", ".", "all", "(", ")", ":", "\n", "            ", "print", "(", "\"fid calculation produces singular product; adding perturbation to diagonal of cov estimates\"", ")", "\n", "offset", "=", "np", ".", "eye", "(", "sigma_data", ".", "shape", "[", "0", "]", ")", "*", "1e-4", "\n", "covmean", ",", "_", "=", "linalg", ".", "sqrtm", "(", "(", "sigma_data", "+", "offset", ")", ".", "dot", "(", "sigma_gen", "+", "offset", ")", ")", "\n", "\n", "# Now calculate the FID", "\n", "", "fid_value", "=", "np", ".", "sum", "(", "np", ".", "square", "(", "mu_gen", "-", "mu_data", ")", ")", "+", "np", ".", "trace", "(", "sigma_gen", "+", "sigma_data", "-", "2", "*", "covmean", ")", "\n", "\n", "return", "fid_value", "\n", "", "except", "ValueError", ":", "\n", "        ", "return", "np", ".", "inf", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.sc09_classifier.test_speech_commands.inception_score": [[50, 75], ["numpy.random.seed", "numpy.mean", "scipy.stats.entropy().mean", "numpy.exp", "numpy.random.permutation", "scipy.stats.entropy", "len", "len", "len", "numpy.repeat", "len"], "function", ["None"], ["", "", "def", "inception_score", "(", "probs_gen", ")", ":", "\n", "    ", "\"\"\"\n    Calculate Inception Score\n    \"\"\"", "\n", "# Set seed", "\n", "np", ".", "random", ".", "seed", "(", "0", ")", "\n", "\n", "# Shuffle probs_gen", "\n", "probs_gen", "=", "probs_gen", "[", "np", ".", "random", ".", "permutation", "(", "len", "(", "probs_gen", ")", ")", "]", "\n", "\n", "# Split probs_gen into two halves", "\n", "probs_gen_1", "=", "probs_gen", "[", ":", "len", "(", "probs_gen", ")", "//", "2", "]", "\n", "probs_gen_2", "=", "probs_gen", "[", "len", "(", "probs_gen", ")", "//", "2", ":", "]", "\n", "\n", "# Calculate average label distribution for split 2", "\n", "mean_2", "=", "np", ".", "mean", "(", "probs_gen_2", ",", "axis", "=", "0", ")", "\n", "\n", "# Compute the mean kl-divergence between the probability distributions", "\n", "# of the generated and average label distributions", "\n", "kl", "=", "entropy", "(", "probs_gen_1", ",", "np", ".", "repeat", "(", "mean_2", "[", "None", ",", ":", "]", ",", "len", "(", "probs_gen_1", ")", ",", "axis", "=", "0", ")", ")", ".", "mean", "(", ")", "\n", "\n", "# Compute the expected score", "\n", "is_score", "=", "np", ".", "exp", "(", "kl", ")", "\n", "\n", "return", "is_score", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.sc09_classifier.test_speech_commands.modified_inception_score": [[76, 101], ["numpy.random.seed", "len", "range", "numpy.exp", "numpy.random.choice", "scipy.stats.entropy", "all_kls.append", "numpy.mean", "numpy.arange"], "function", ["None"], ["", "def", "modified_inception_score", "(", "probs_gen", ",", "n", "=", "10000", ")", ":", "\n", "    ", "\"\"\"\n    Calculate Modified Inception Score\n    \"\"\"", "\n", "# Set seed", "\n", "np", ".", "random", ".", "seed", "(", "0", ")", "\n", "\n", "n_samples", "=", "len", "(", "probs_gen", ")", "\n", "\n", "all_kls", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "n", ")", ":", "\n", "# Sample two prob vectors", "\n", "        ", "indices", "=", "np", ".", "random", ".", "choice", "(", "np", ".", "arange", "(", "n_samples", ")", ",", "size", "=", "2", ",", "replace", "=", "True", ")", "\n", "probs_gen_1", "=", "probs_gen", "[", "indices", "[", "0", "]", "]", "\n", "probs_gen_2", "=", "probs_gen", "[", "indices", "[", "1", "]", "]", "\n", "\n", "# Calculate their KL", "\n", "kl", "=", "entropy", "(", "probs_gen_1", ",", "probs_gen_2", ")", "\n", "\n", "all_kls", ".", "append", "(", "kl", ")", "\n", "\n", "# Compute the score", "\n", "", "mis_score", "=", "np", ".", "exp", "(", "np", ".", "mean", "(", "all_kls", ")", ")", "\n", "\n", "return", "mis_score", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.sc09_classifier.test_speech_commands.am_score": [[102, 112], ["numpy.mean", "numpy.mean", "numpy.mean", "scipy.stats.entropy", "scipy.stats.entropy"], "function", ["None"], ["", "def", "am_score", "(", "probs_data", ",", "probs_gen", ")", ":", "\n", "    ", "\"\"\"\n    Calculate AM Score\n    \"\"\"", "\n", "mean_data", "=", "np", ".", "mean", "(", "probs_data", ",", "axis", "=", "0", ")", "\n", "mean_gen", "=", "np", ".", "mean", "(", "probs_gen", ",", "axis", "=", "0", ")", "\n", "entropy_gen", "=", "np", ".", "mean", "(", "entropy", "(", "probs_gen", ",", "axis", "=", "1", ")", ")", "\n", "am_score", "=", "entropy", "(", "mean_data", ",", "mean_gen", ")", "+", "entropy_gen", "\n", "\n", "return", "am_score", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.sc09_classifier.test_speech_commands.two_proportions_z_test": [[114, 126], ["numpy.sqrt", "scipy.stats.norm.cdf", "abs", "numpy.abs"], "function", ["None"], ["", "def", "two_proportions_z_test", "(", "p1", ",", "n1", ",", "p2", ",", "n2", ",", "significance_level", ",", "z_threshold", "=", "None", ")", ":", "\n", "# Taken from https://github.com/eitanrich/gans-n-gmms/blob/master/utils/ndb.py", "\n", "# Per http://stattrek.com/hypothesis-test/difference-in-proportions.aspx", "\n", "# See also http://www.itl.nist.gov/div898/software/dataplot/refman1/auxillar/binotest.htm", "\n", "    ", "p", "=", "(", "p1", "*", "n1", "+", "p2", "*", "n2", ")", "/", "(", "n1", "+", "n2", ")", "\n", "se", "=", "np", ".", "sqrt", "(", "p", "*", "(", "1", "-", "p", ")", "*", "(", "1", "/", "n1", "+", "1", "/", "n2", ")", ")", "\n", "z", "=", "(", "p1", "-", "p2", ")", "/", "se", "\n", "# Allow defining a threshold in terms as Z (difference relative to the SE) rather than in p-values.", "\n", "if", "z_threshold", "is", "not", "None", ":", "\n", "        ", "return", "abs", "(", "z", ")", ">", "z_threshold", "\n", "", "p_values", "=", "2.0", "*", "norm", ".", "cdf", "(", "-", "1.0", "*", "np", ".", "abs", "(", "z", ")", ")", "# Two-tailed test", "\n", "return", "p_values", "<", "significance_level", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.sc09_classifier.test_speech_commands.ndb_score": [[128, 151], ["sklearn.cluster.KMeans().fit", "KMeans().fit.predict", "KMeans().fit.predict", "numpy.zeros_like", "numpy.unique", "test_speech_commands.two_proportions_z_test", "numpy.count_nonzero", "numpy.unique", "len", "len", "len", "len", "sklearn.cluster.KMeans"], "function", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.sc09_classifier.test_speech_commands.two_proportions_z_test"], ["", "def", "ndb_score", "(", "feat_data", ",", "feat_gen", ")", ":", "\n", "# Run K-Means cluster on feat_data with K=50", "\n", "    ", "kmeans", "=", "KMeans", "(", "n_clusters", "=", "50", ",", "random_state", "=", "0", ")", ".", "fit", "(", "feat_data", ")", "\n", "\n", "# Get cluster labels for feat_data and feat_gen", "\n", "labels_data", "=", "kmeans", ".", "predict", "(", "feat_data", ")", "\n", "labels_gen", "=", "kmeans", ".", "predict", "(", "feat_gen", ")", "\n", "\n", "# Calculate number of data points in each cluster using np.unique", "\n", "counts_data", "=", "np", ".", "unique", "(", "labels_data", ",", "return_counts", "=", "True", ")", "[", "1", "]", "\n", "counts_gen", "=", "np", ".", "zeros_like", "(", "counts_data", ")", "\n", "values", ",", "counts", "=", "np", ".", "unique", "(", "labels_gen", ",", "return_counts", "=", "True", ")", "\n", "counts_gen", "[", "values", "]", "=", "counts", "\n", "\n", "# Calculate proportion of data points in each cluster", "\n", "prop_data", "=", "counts_data", "/", "len", "(", "labels_data", ")", "\n", "prop_gen", "=", "counts_gen", "/", "len", "(", "labels_gen", ")", "\n", "\n", "# Calculate number of bins with statistically different proportions", "\n", "different_bins", "=", "two_proportions_z_test", "(", "prop_data", ",", "len", "(", "labels_data", ")", ",", "prop_gen", ",", "len", "(", "labels_gen", ")", ",", "0.05", ")", "\n", "ndb", "=", "np", ".", "count_nonzero", "(", "different_bins", ")", "\n", "\n", "return", "ndb", "/", "50.", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.sc09_classifier.test_speech_commands._nested_getattr": [[154, 163], ["functools.reduce", "getattr", "attr.split"], "function", ["None"], ["", "def", "_nested_getattr", "(", "obj", ",", "attr", ",", "*", "args", ")", ":", "\n", "    ", "\"\"\"Get a nested property from an object.\n    Example:\n    ```\n        model = ...\n        weights = _nested_getattr(model, \"layer4.weights\")\n    ```\n    \"\"\"", "\n", "return", "reduce", "(", "lambda", "o", ",", "a", ":", "getattr", "(", "o", ",", "a", ",", "*", "args", ")", ",", "[", "obj", "]", "+", "attr", ".", "split", "(", "\".\"", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.sc09_classifier.test_speech_commands.test": [[302, 342], ["torch.no_grad", "model.eval", "test_speech_commands.ActivationExtractor", "module.register_forward_hook", "tqdm.auto.tqdm", "numpy.concatenate", "numpy.concatenate", "print", "inputs.cuda.unsqueeze", "model", "torch.nn.functional.softmax", "pred.eq().sum", "targets.cuda.size", "np.concatenate.append", "np.concatenate.append", "inputs.cuda.cuda", "targets.cuda.cuda", "torch.nn.functional.softmax.data.max", "torch.nn.functional.softmax.cpu().numpy", "ActivationExtractor.input[].cpu().numpy", "pred.eq", "targets.cuda.data.view_as", "torch.nn.functional.softmax.cpu", "ActivationExtractor.input[].cpu"], "function", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.None.example.eval"], ["@", "torch", ".", "no_grad", "(", ")", "\n", "def", "test", "(", "dataloader", ")", ":", "\n", "    ", "model", ".", "eval", "(", ")", "# Set model to evaluate mode", "\n", "\n", "extractor", "=", "ActivationExtractor", "(", ")", "\n", "module", "=", "model", ".", "module", ".", "classifier", "\n", "module", ".", "register_forward_hook", "(", "extractor", ".", "add_hook", ")", "\n", "\n", "correct", "=", "0", "\n", "total", "=", "0", "\n", "\n", "probs", "=", "[", "]", "\n", "activations", "=", "[", "]", "\n", "\n", "pbar", "=", "tqdm", "(", "dataloader", ",", "unit", "=", "\"audios\"", ",", "unit_scale", "=", "dataloader", ".", "batch_size", ")", "\n", "for", "batch", "in", "pbar", ":", "\n", "        ", "inputs", "=", "batch", "[", "'input'", "]", "\n", "inputs", "=", "inputs", ".", "unsqueeze", "(", "1", ")", "\n", "targets", "=", "batch", "[", "'target'", "]", "\n", "\n", "if", "use_gpu", ":", "\n", "            ", "inputs", "=", "inputs", ".", "cuda", "(", ")", "\n", "targets", "=", "targets", ".", "cuda", "(", ")", "\n", "\n", "# forward", "\n", "", "outputs", "=", "model", "(", "inputs", ")", "\n", "outputs", "=", "torch", ".", "nn", ".", "functional", ".", "softmax", "(", "outputs", ",", "dim", "=", "1", ")", "\n", "\n", "pred", "=", "outputs", ".", "data", ".", "max", "(", "1", ",", "keepdim", "=", "True", ")", "[", "1", "]", "\n", "correct", "+=", "pred", ".", "eq", "(", "targets", ".", "data", ".", "view_as", "(", "pred", ")", ")", ".", "sum", "(", ")", "\n", "total", "+=", "targets", ".", "size", "(", "0", ")", "\n", "\n", "probs", ".", "append", "(", "outputs", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ")", "\n", "activations", ".", "append", "(", "extractor", ".", "input", "[", "0", "]", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ")", "\n", "\n", "", "probs", "=", "np", ".", "concatenate", "(", "probs", ")", "\n", "activations", "=", "np", ".", "concatenate", "(", "activations", ")", "\n", "accuracy", "=", "correct", "/", "total", "\n", "print", "(", "\"accuracy: %f%%\"", "%", "(", "100", "*", "accuracy", ")", ")", "\n", "return", "probs", ",", "activations", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.sc09_classifier.speech_commands_dataset.SpeechCommandsDataset.__init__": [[23, 38], ["os.path.join", "os.listdir", "range", "os.path.join", "data.append", "len"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "folder", ",", "transform", "=", "None", ",", "classes", "=", "CLASSES", ")", ":", "\n", "        ", "all_classes", "=", "classes", "\n", "class_to_idx", "=", "{", "classes", "[", "i", "]", ":", "i", "for", "i", "in", "range", "(", "len", "(", "classes", ")", ")", "}", "\n", "\n", "data", "=", "[", "]", "\n", "for", "c", "in", "all_classes", ":", "\n", "            ", "d", "=", "os", ".", "path", ".", "join", "(", "folder", ",", "c", ")", "\n", "target", "=", "class_to_idx", "[", "c", "]", "\n", "for", "f", "in", "os", ".", "listdir", "(", "d", ")", ":", "\n", "                ", "path", "=", "os", ".", "path", ".", "join", "(", "d", ",", "f", ")", "\n", "data", ".", "append", "(", "(", "path", ",", "target", ")", ")", "\n", "\n", "", "", "self", ".", "classes", "=", "classes", "\n", "self", ".", "data", "=", "data", "\n", "self", ".", "transform", "=", "transform", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.sc09_classifier.speech_commands_dataset.SpeechCommandsDataset.__len__": [[39, 41], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "data", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.sc09_classifier.speech_commands_dataset.SpeechCommandsDataset.__getitem__": [[42, 50], ["speech_commands_dataset.SpeechCommandsDataset.transform"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.eeg.data_utils.StandardScaler.transform"], ["", "def", "__getitem__", "(", "self", ",", "index", ")", ":", "\n", "        ", "path", ",", "target", "=", "self", ".", "data", "[", "index", "]", "\n", "data", "=", "{", "'path'", ":", "path", ",", "'target'", ":", "target", "}", "\n", "\n", "if", "self", ".", "transform", "is", "not", "None", ":", "\n", "            ", "data", "=", "self", ".", "transform", "(", "data", ")", "\n", "\n", "", "return", "data", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.sc09_classifier.speech_commands_dataset.SpeechCommandsDataset.make_weights_for_balanced_classes": [[51, 65], ["len", "numpy.zeros", "float", "numpy.zeros", "enumerate", "sum", "len"], "methods", ["None"], ["", "def", "make_weights_for_balanced_classes", "(", "self", ")", ":", "\n", "        ", "\"\"\"adopted from https://discuss.pytorch.org/t/balanced-sampling-between-classes-with-torchvision-dataloader/2703/3\"\"\"", "\n", "\n", "nclasses", "=", "len", "(", "self", ".", "classes", ")", "\n", "count", "=", "np", ".", "zeros", "(", "nclasses", ")", "\n", "for", "item", "in", "self", ".", "data", ":", "\n", "            ", "count", "[", "item", "[", "1", "]", "]", "+=", "1", "\n", "\n", "", "N", "=", "float", "(", "sum", "(", "count", ")", ")", "\n", "weight_per_class", "=", "N", "/", "count", "\n", "weight", "=", "np", ".", "zeros", "(", "len", "(", "self", ")", ")", "\n", "for", "idx", ",", "item", "in", "enumerate", "(", "self", ".", "data", ")", ":", "\n", "            ", "weight", "[", "idx", "]", "=", "weight_per_class", "[", "item", "[", "1", "]", "]", "\n", "", "return", "weight", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.sc09_classifier.speech_commands_dataset.BackgroundNoiseDataset.__init__": [[69, 85], ["numpy.hstack", "int", "samples[].reshape", "os.path.join", "librosa.load", "numpy.hstack.append", "len", "os.listdir", "os.path.isfile", "d.endswith", "os.path.join"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "folder", ",", "transform", "=", "None", ",", "sample_rate", "=", "16000", ",", "sample_length", "=", "1", ")", ":", "\n", "        ", "audio_files", "=", "[", "d", "for", "d", "in", "os", ".", "listdir", "(", "folder", ")", "if", "os", ".", "path", ".", "isfile", "(", "os", ".", "path", ".", "join", "(", "folder", ",", "d", ")", ")", "and", "d", ".", "endswith", "(", "'.wav'", ")", "]", "\n", "samples", "=", "[", "]", "\n", "for", "f", "in", "audio_files", ":", "\n", "            ", "path", "=", "os", ".", "path", ".", "join", "(", "folder", ",", "f", ")", "\n", "s", ",", "sr", "=", "librosa", ".", "load", "(", "path", ",", "sample_rate", ")", "\n", "samples", ".", "append", "(", "s", ")", "\n", "\n", "", "samples", "=", "np", ".", "hstack", "(", "samples", ")", "\n", "c", "=", "int", "(", "sample_rate", "*", "sample_length", ")", "\n", "r", "=", "len", "(", "samples", ")", "//", "c", "\n", "self", ".", "samples", "=", "samples", "[", ":", "r", "*", "c", "]", ".", "reshape", "(", "-", "1", ",", "c", ")", "\n", "self", ".", "sample_rate", "=", "sample_rate", "\n", "self", ".", "classes", "=", "CLASSES", "\n", "self", ".", "transform", "=", "transform", "\n", "self", ".", "path", "=", "folder", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.sc09_classifier.speech_commands_dataset.BackgroundNoiseDataset.__len__": [[86, 88], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "samples", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.sc09_classifier.speech_commands_dataset.BackgroundNoiseDataset.__getitem__": [[89, 96], ["speech_commands_dataset.BackgroundNoiseDataset.transform"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.eeg.data_utils.StandardScaler.transform"], ["", "def", "__getitem__", "(", "self", ",", "index", ")", ":", "\n", "        ", "data", "=", "{", "'samples'", ":", "self", ".", "samples", "[", "index", "]", ",", "'sample_rate'", ":", "self", ".", "sample_rate", ",", "'target'", ":", "1", ",", "'path'", ":", "self", ".", "path", "}", "\n", "\n", "if", "self", ".", "transform", "is", "not", "None", ":", "\n", "            ", "data", "=", "self", ".", "transform", "(", "data", ")", "\n", "\n", "", "return", "data", "\n", "", "", ""]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.sc09_classifier.train_speech_commands.get_lr": [[132, 134], ["None"], "function", ["None"], ["", "def", "get_lr", "(", ")", ":", "\n", "    ", "return", "optimizer", ".", "param_groups", "[", "0", "]", "[", "'lr'", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.sc09_classifier.train_speech_commands.train": [[137, 191], ["print", "writer.add_scalar", "model.train", "tqdm.auto.tqdm", "writer.add_scalar", "writer.add_scalar", "train_speech_commands.get_lr", "torch.unsqueeze", "torch.autograd.Variable", "torch.autograd.Variable", "model", "criterion", "optimizer.zero_grad", "criterion.backward", "optimizer.step", "criterion.item", "pred.eq().sum", "targets.cuda.size", "writer.add_scalar", "tqdm.auto.tqdm.set_postfix", "inputs.cuda.cuda", "targets.cuda.cuda", "model.data.max", "criterion.item", "train_speech_commands.get_lr", "pred.eq", "targets.cuda.data.view_as"], "function", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.sc09_classifier.train_speech_commands.train", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.sc09_classifier.train_speech_commands.get_lr", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.cauchy.cauchy.CauchyMultiplySymmetric.backward", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.sashimi.sashimi.Sashimi.step", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.sc09_classifier.train_speech_commands.get_lr"], ["def", "train", "(", "epoch", ")", ":", "\n", "    ", "global", "global_step", "\n", "\n", "print", "(", "\"epoch %3d with lr=%.02e\"", "%", "(", "epoch", ",", "get_lr", "(", ")", ")", ")", "\n", "phase", "=", "'train'", "\n", "writer", ".", "add_scalar", "(", "'%s/learning_rate'", "%", "phase", ",", "get_lr", "(", ")", ",", "epoch", ")", "\n", "\n", "model", ".", "train", "(", ")", "# Set model to training mode", "\n", "\n", "running_loss", "=", "0.0", "\n", "it", "=", "0", "\n", "correct", "=", "0", "\n", "total", "=", "0", "\n", "\n", "pbar", "=", "tqdm", "(", "train_dataloader", ",", "unit", "=", "\"audios\"", ",", "unit_scale", "=", "train_dataloader", ".", "batch_size", ")", "\n", "for", "batch", "in", "pbar", ":", "\n", "        ", "inputs", "=", "batch", "[", "'input'", "]", "\n", "inputs", "=", "torch", ".", "unsqueeze", "(", "inputs", ",", "1", ")", "\n", "targets", "=", "batch", "[", "'target'", "]", "\n", "\n", "inputs", "=", "Variable", "(", "inputs", ",", "requires_grad", "=", "True", ")", "\n", "targets", "=", "Variable", "(", "targets", ",", "requires_grad", "=", "False", ")", "\n", "\n", "if", "use_gpu", ":", "\n", "            ", "inputs", "=", "inputs", ".", "cuda", "(", ")", "\n", "targets", "=", "targets", ".", "cuda", "(", ")", "\n", "\n", "# forward/backward", "\n", "", "outputs", "=", "model", "(", "inputs", ")", "\n", "loss", "=", "criterion", "(", "outputs", ",", "targets", ")", "\n", "optimizer", ".", "zero_grad", "(", ")", "\n", "loss", ".", "backward", "(", ")", "\n", "optimizer", ".", "step", "(", ")", "\n", "\n", "# statistics", "\n", "it", "+=", "1", "\n", "global_step", "+=", "1", "\n", "running_loss", "+=", "loss", ".", "item", "(", ")", "# loss.data[0]", "\n", "pred", "=", "outputs", ".", "data", ".", "max", "(", "1", ",", "keepdim", "=", "True", ")", "[", "1", "]", "\n", "correct", "+=", "pred", ".", "eq", "(", "targets", ".", "data", ".", "view_as", "(", "pred", ")", ")", ".", "sum", "(", ")", "\n", "total", "+=", "targets", ".", "size", "(", "0", ")", "\n", "\n", "writer", ".", "add_scalar", "(", "'%s/loss'", "%", "phase", ",", "loss", ".", "item", "(", ")", ",", "global_step", ")", "\n", "\n", "# update the progress bar", "\n", "pbar", ".", "set_postfix", "(", "{", "\n", "'loss'", ":", "\"%.05f\"", "%", "(", "running_loss", "/", "it", ")", ",", "\n", "'acc'", ":", "\"%.02f%%\"", "%", "(", "100", "*", "correct", "/", "total", ")", "\n", "}", ")", "\n", "\n", "", "accuracy", "=", "correct", "/", "total", "\n", "epoch_loss", "=", "running_loss", "/", "it", "\n", "writer", ".", "add_scalar", "(", "'%s/accuracy'", "%", "phase", ",", "100", "*", "accuracy", ",", "epoch", ")", "\n", "writer", ".", "add_scalar", "(", "'%s/epoch_loss'", "%", "phase", ",", "epoch_loss", ",", "epoch", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.sc09_classifier.train_speech_commands.valid": [[192, 263], ["model.eval", "tqdm.auto.tqdm", "writer.add_scalar", "writer.add_scalar", "torch.save", "torch.unsqueeze", "torch.autograd.Variable", "torch.autograd.Variable", "model", "criterion", "criterion.item", "pred.eq().sum", "targets.cuda.size", "writer.add_scalar", "tqdm.auto.tqdm.set_postfix", "model.state_dict", "optimizer.state_dict", "torch.save", "torch.save", "torch.save", "torch.save", "inputs.cuda.cuda", "targets.cuda.cuda", "model.data.max", "criterion.item", "pred.eq", "targets.cuda.data.view_as"], "function", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.None.example.eval"], ["", "def", "valid", "(", "epoch", ")", ":", "\n", "    ", "global", "best_accuracy", ",", "best_loss", ",", "global_step", "\n", "\n", "phase", "=", "'valid'", "\n", "model", ".", "eval", "(", ")", "# Set model to evaluate mode", "\n", "\n", "running_loss", "=", "0.0", "\n", "it", "=", "0", "\n", "correct", "=", "0", "\n", "total", "=", "0", "\n", "\n", "pbar", "=", "tqdm", "(", "valid_dataloader", ",", "unit", "=", "\"audios\"", ",", "unit_scale", "=", "valid_dataloader", ".", "batch_size", ")", "\n", "for", "batch", "in", "pbar", ":", "\n", "        ", "inputs", "=", "batch", "[", "'input'", "]", "\n", "inputs", "=", "torch", ".", "unsqueeze", "(", "inputs", ",", "1", ")", "\n", "targets", "=", "batch", "[", "'target'", "]", "\n", "\n", "inputs", "=", "Variable", "(", "inputs", ",", "volatile", "=", "True", ")", "\n", "targets", "=", "Variable", "(", "targets", ",", "requires_grad", "=", "False", ")", "\n", "\n", "if", "use_gpu", ":", "\n", "            ", "inputs", "=", "inputs", ".", "cuda", "(", ")", "\n", "targets", "=", "targets", ".", "cuda", "(", ")", "\n", "\n", "# forward", "\n", "", "outputs", "=", "model", "(", "inputs", ")", "\n", "loss", "=", "criterion", "(", "outputs", ",", "targets", ")", "\n", "\n", "# statistics", "\n", "it", "+=", "1", "\n", "global_step", "+=", "1", "\n", "running_loss", "+=", "loss", ".", "item", "(", ")", "\n", "pred", "=", "outputs", ".", "data", ".", "max", "(", "1", ",", "keepdim", "=", "True", ")", "[", "1", "]", "\n", "correct", "+=", "pred", ".", "eq", "(", "targets", ".", "data", ".", "view_as", "(", "pred", ")", ")", ".", "sum", "(", ")", "\n", "total", "+=", "targets", ".", "size", "(", "0", ")", "\n", "\n", "writer", ".", "add_scalar", "(", "'%s/loss'", "%", "phase", ",", "loss", ".", "item", "(", ")", ",", "global_step", ")", "\n", "\n", "# update the progress bar", "\n", "pbar", ".", "set_postfix", "(", "{", "\n", "'loss'", ":", "\"%.05f\"", "%", "(", "running_loss", "/", "it", ")", ",", "\n", "'acc'", ":", "\"%.02f%%\"", "%", "(", "100", "*", "correct", "/", "total", ")", "\n", "}", ")", "\n", "\n", "", "accuracy", "=", "correct", "/", "total", "\n", "epoch_loss", "=", "running_loss", "/", "it", "\n", "writer", ".", "add_scalar", "(", "'%s/accuracy'", "%", "phase", ",", "100", "*", "accuracy", ",", "epoch", ")", "\n", "writer", ".", "add_scalar", "(", "'%s/epoch_loss'", "%", "phase", ",", "epoch_loss", ",", "epoch", ")", "\n", "\n", "checkpoint", "=", "{", "\n", "'epoch'", ":", "epoch", ",", "\n", "'step'", ":", "global_step", ",", "\n", "'state_dict'", ":", "model", ".", "state_dict", "(", ")", ",", "\n", "'loss'", ":", "epoch_loss", ",", "\n", "'accuracy'", ":", "accuracy", ",", "\n", "'optimizer'", ":", "optimizer", ".", "state_dict", "(", ")", ",", "\n", "}", "\n", "\n", "if", "accuracy", ">", "best_accuracy", ":", "\n", "        ", "best_accuracy", "=", "accuracy", "\n", "torch", ".", "save", "(", "checkpoint", ",", "'checkpoints/best-loss-speech-commands-checkpoint-%s.pth'", "%", "full_name", ")", "\n", "torch", ".", "save", "(", "model", ",", "'%d-%s-best-loss.pth'", "%", "(", "start_timestamp", ",", "full_name", ")", ")", "\n", "", "if", "epoch_loss", "<", "best_loss", ":", "\n", "        ", "best_loss", "=", "epoch_loss", "\n", "torch", ".", "save", "(", "checkpoint", ",", "'checkpoints/best-acc-speech-commands-checkpoint-%s.pth'", "%", "full_name", ")", "\n", "torch", ".", "save", "(", "model", ",", "'%d-%s-best-acc.pth'", "%", "(", "start_timestamp", ",", "full_name", ")", ")", "\n", "\n", "", "torch", ".", "save", "(", "checkpoint", ",", "'checkpoints/last-speech-commands-checkpoint.pth'", ")", "\n", "del", "checkpoint", "# reduce memory", "\n", "\n", "return", "epoch_loss", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.speech_commands.split_dataset.move_files": [[8, 17], ["open", "f.readlines", "line.rstrip.rstrip", "os.path.dirname", "os.path.join", "shutil.move", "os.path.exists", "os.mkdir", "os.path.join"], "function", ["None"], ["def", "move_files", "(", "src_folder", ",", "to_folder", ",", "list_file", ")", ":", "\n", "    ", "with", "open", "(", "list_file", ")", "as", "f", ":", "\n", "        ", "for", "line", "in", "f", ".", "readlines", "(", ")", ":", "\n", "            ", "line", "=", "line", ".", "rstrip", "(", ")", "\n", "dirname", "=", "os", ".", "path", ".", "dirname", "(", "line", ")", "\n", "dest", "=", "os", ".", "path", ".", "join", "(", "to_folder", ",", "dirname", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "dest", ")", ":", "\n", "                ", "os", ".", "mkdir", "(", "dest", ")", "\n", "", "shutil", ".", "move", "(", "os", ".", "path", ".", "join", "(", "src_folder", ",", "line", ")", ",", "dest", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.models.resnext.ResNeXtBottleneck.__init__": [[28, 55], ["torch.Module.__init__", "torch.Conv2d", "torch.Conv2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.Conv2d", "torch.Conv2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.Conv2d", "torch.Conv2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.Sequential", "torch.Sequential", "int", "resnext.ResNeXtBottleneck.shortcut.add_module", "resnext.ResNeXtBottleneck.shortcut.add_module", "torch.Conv2d", "torch.Conv2d", "torch.BatchNorm2d", "torch.BatchNorm2d"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__"], ["def", "__init__", "(", "self", ",", "in_channels", ",", "out_channels", ",", "stride", ",", "cardinality", ",", "base_width", ",", "widen_factor", ")", ":", "\n", "        ", "\"\"\" Constructor\n\n        Args:\n            in_channels: input channel dimensionality\n            out_channels: output channel dimensionality\n            stride: conv stride. Replaces pooling layer.\n            cardinality: num of convolution groups.\n            base_width: base number of channels in each group.\n            widen_factor: factor to reduce the input dimensionality before convolution.\n        \"\"\"", "\n", "super", "(", "ResNeXtBottleneck", ",", "self", ")", ".", "__init__", "(", ")", "\n", "width_ratio", "=", "out_channels", "/", "(", "widen_factor", "*", "64.", ")", "\n", "D", "=", "cardinality", "*", "int", "(", "base_width", "*", "width_ratio", ")", "\n", "self", ".", "conv_reduce", "=", "nn", ".", "Conv2d", "(", "in_channels", ",", "D", ",", "kernel_size", "=", "1", ",", "stride", "=", "1", ",", "padding", "=", "0", ",", "bias", "=", "False", ")", "\n", "self", ".", "bn_reduce", "=", "nn", ".", "BatchNorm2d", "(", "D", ")", "\n", "self", ".", "conv_conv", "=", "nn", ".", "Conv2d", "(", "D", ",", "D", ",", "kernel_size", "=", "3", ",", "stride", "=", "stride", ",", "padding", "=", "1", ",", "groups", "=", "cardinality", ",", "bias", "=", "False", ")", "\n", "self", ".", "bn", "=", "nn", ".", "BatchNorm2d", "(", "D", ")", "\n", "self", ".", "conv_expand", "=", "nn", ".", "Conv2d", "(", "D", ",", "out_channels", ",", "kernel_size", "=", "1", ",", "stride", "=", "1", ",", "padding", "=", "0", ",", "bias", "=", "False", ")", "\n", "self", ".", "bn_expand", "=", "nn", ".", "BatchNorm2d", "(", "out_channels", ")", "\n", "\n", "self", ".", "shortcut", "=", "nn", ".", "Sequential", "(", ")", "\n", "if", "in_channels", "!=", "out_channels", ":", "\n", "            ", "self", ".", "shortcut", ".", "add_module", "(", "'shortcut_conv'", ",", "\n", "nn", ".", "Conv2d", "(", "in_channels", ",", "out_channels", ",", "kernel_size", "=", "1", ",", "stride", "=", "stride", ",", "padding", "=", "0", ",", "\n", "bias", "=", "False", ")", ")", "\n", "self", ".", "shortcut", ".", "add_module", "(", "'shortcut_bn'", ",", "nn", ".", "BatchNorm2d", "(", "out_channels", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.models.resnext.ResNeXtBottleneck.forward": [[56, 65], ["resnext.ResNeXtBottleneck.conv_reduce.forward", "torch.relu", "torch.relu", "resnext.ResNeXtBottleneck.conv_conv.forward", "torch.relu", "torch.relu", "resnext.ResNeXtBottleneck.conv_expand.forward", "resnext.ResNeXtBottleneck.bn_expand.forward", "resnext.ResNeXtBottleneck.shortcut.forward", "torch.relu", "torch.relu", "resnext.ResNeXtBottleneck.bn_reduce.forward", "resnext.ResNeXtBottleneck.bn.forward"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.models.resnext.CifarResNeXt.forward", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.models.resnext.CifarResNeXt.forward", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.models.resnext.CifarResNeXt.forward", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.models.resnext.CifarResNeXt.forward", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.models.resnext.CifarResNeXt.forward", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.models.resnext.CifarResNeXt.forward", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.models.resnext.CifarResNeXt.forward"], ["", "", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "bottleneck", "=", "self", ".", "conv_reduce", ".", "forward", "(", "x", ")", "\n", "bottleneck", "=", "F", ".", "relu", "(", "self", ".", "bn_reduce", ".", "forward", "(", "bottleneck", ")", ",", "inplace", "=", "True", ")", "\n", "bottleneck", "=", "self", ".", "conv_conv", ".", "forward", "(", "bottleneck", ")", "\n", "bottleneck", "=", "F", ".", "relu", "(", "self", ".", "bn", ".", "forward", "(", "bottleneck", ")", ",", "inplace", "=", "True", ")", "\n", "bottleneck", "=", "self", ".", "conv_expand", ".", "forward", "(", "bottleneck", ")", "\n", "bottleneck", "=", "self", ".", "bn_expand", ".", "forward", "(", "bottleneck", ")", "\n", "residual", "=", "self", ".", "shortcut", ".", "forward", "(", "x", ")", "\n", "return", "F", ".", "relu", "(", "residual", "+", "bottleneck", ",", "inplace", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.models.resnext.CifarResNeXt.__init__": [[73, 109], ["torch.Module.__init__", "torch.Conv2d", "torch.Conv2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "resnext.CifarResNeXt.block", "resnext.CifarResNeXt.block", "resnext.CifarResNeXt.block", "torch.Linear", "torch.Linear", "torch.nn.init.kaiming_normal", "torch.nn.init.kaiming_normal", "resnext.CifarResNeXt.state_dict", "key.split", "torch.nn.init.kaiming_normal", "torch.nn.init.kaiming_normal", "key.split", "resnext.CifarResNeXt.state_dict", "resnext.CifarResNeXt.state_dict", "resnext.CifarResNeXt.state_dict"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.models.resnext.CifarResNeXt.block", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.models.resnext.CifarResNeXt.block", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.models.resnext.CifarResNeXt.block"], ["def", "__init__", "(", "self", ",", "nlabels", ",", "cardinality", "=", "8", ",", "depth", "=", "29", ",", "base_width", "=", "64", ",", "widen_factor", "=", "4", ",", "in_channels", "=", "3", ")", ":", "\n", "        ", "\"\"\" Constructor\n\n        Args:\n            cardinality: number of convolution groups.\n            depth: number of layers.\n            nlabels: number of classes\n            base_width: base number of channels in each group.\n            widen_factor: factor to adjust the channel dimensionality\n        \"\"\"", "\n", "super", "(", "CifarResNeXt", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "cardinality", "=", "cardinality", "\n", "self", ".", "depth", "=", "depth", "\n", "self", ".", "block_depth", "=", "(", "self", ".", "depth", "-", "2", ")", "//", "9", "\n", "self", ".", "base_width", "=", "base_width", "\n", "self", ".", "widen_factor", "=", "widen_factor", "\n", "self", ".", "nlabels", "=", "nlabels", "\n", "self", ".", "output_size", "=", "64", "\n", "self", ".", "stages", "=", "[", "64", ",", "64", "*", "self", ".", "widen_factor", ",", "128", "*", "self", ".", "widen_factor", ",", "256", "*", "self", ".", "widen_factor", "]", "\n", "\n", "self", ".", "conv_1_3x3", "=", "nn", ".", "Conv2d", "(", "in_channels", ",", "64", ",", "3", ",", "1", ",", "1", ",", "bias", "=", "False", ")", "\n", "self", ".", "bn_1", "=", "nn", ".", "BatchNorm2d", "(", "64", ")", "\n", "self", ".", "stage_1", "=", "self", ".", "block", "(", "'stage_1'", ",", "self", ".", "stages", "[", "0", "]", ",", "self", ".", "stages", "[", "1", "]", ",", "1", ")", "\n", "self", ".", "stage_2", "=", "self", ".", "block", "(", "'stage_2'", ",", "self", ".", "stages", "[", "1", "]", ",", "self", ".", "stages", "[", "2", "]", ",", "2", ")", "\n", "self", ".", "stage_3", "=", "self", ".", "block", "(", "'stage_3'", ",", "self", ".", "stages", "[", "2", "]", ",", "self", ".", "stages", "[", "3", "]", ",", "2", ")", "\n", "self", ".", "classifier", "=", "nn", ".", "Linear", "(", "self", ".", "stages", "[", "3", "]", ",", "nlabels", ")", "\n", "init", ".", "kaiming_normal", "(", "self", ".", "classifier", ".", "weight", ")", "\n", "\n", "for", "key", "in", "self", ".", "state_dict", "(", ")", ":", "\n", "            ", "if", "key", ".", "split", "(", "'.'", ")", "[", "-", "1", "]", "==", "'weight'", ":", "\n", "                ", "if", "'conv'", "in", "key", ":", "\n", "                    ", "init", ".", "kaiming_normal", "(", "self", ".", "state_dict", "(", ")", "[", "key", "]", ",", "mode", "=", "'fan_out'", ")", "\n", "", "if", "'bn'", "in", "key", ":", "\n", "                    ", "self", ".", "state_dict", "(", ")", "[", "key", "]", "[", "...", "]", "=", "1", "\n", "", "", "elif", "key", ".", "split", "(", "'.'", ")", "[", "-", "1", "]", "==", "'bias'", ":", "\n", "                ", "self", ".", "state_dict", "(", ")", "[", "key", "]", "[", "...", "]", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.models.resnext.CifarResNeXt.block": [[110, 133], ["torch.Sequential", "torch.Sequential", "range", "torch.Sequential.add_module", "torch.Sequential.add_module", "resnext.ResNeXtBottleneck", "resnext.ResNeXtBottleneck"], "methods", ["None"], ["", "", "", "def", "block", "(", "self", ",", "name", ",", "in_channels", ",", "out_channels", ",", "pool_stride", "=", "2", ")", ":", "\n", "        ", "\"\"\" Stack n bottleneck modules where n is inferred from the depth of the network.\n\n        Args:\n            name: string name of the current block.\n            in_channels: number of input channels\n            out_channels: number of output channels\n            pool_stride: factor to reduce the spatial dimensionality in the first bottleneck of the block.\n\n        Returns: a Module consisting of n sequential bottlenecks.\n\n        \"\"\"", "\n", "block", "=", "nn", ".", "Sequential", "(", ")", "\n", "for", "bottleneck", "in", "range", "(", "self", ".", "block_depth", ")", ":", "\n", "            ", "name_", "=", "'%s_bottleneck_%d'", "%", "(", "name", ",", "bottleneck", ")", "\n", "if", "bottleneck", "==", "0", ":", "\n", "                ", "block", ".", "add_module", "(", "name_", ",", "ResNeXtBottleneck", "(", "in_channels", ",", "out_channels", ",", "pool_stride", ",", "self", ".", "cardinality", ",", "\n", "self", ".", "base_width", ",", "self", ".", "widen_factor", ")", ")", "\n", "", "else", ":", "\n", "                ", "block", ".", "add_module", "(", "name_", ",", "\n", "ResNeXtBottleneck", "(", "out_channels", ",", "out_channels", ",", "1", ",", "self", ".", "cardinality", ",", "self", ".", "base_width", ",", "\n", "self", ".", "widen_factor", ")", ")", "\n", "", "", "return", "block", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.models.resnext.CifarResNeXt.forward": [[134, 143], ["resnext.CifarResNeXt.conv_1_3x3.forward", "torch.relu", "torch.relu", "resnext.CifarResNeXt.stage_1.forward", "resnext.CifarResNeXt.stage_2.forward", "resnext.CifarResNeXt.stage_3.forward", "torch.avg_pool2d", "torch.avg_pool2d", "x.view.view.view", "resnext.CifarResNeXt.classifier", "resnext.CifarResNeXt.bn_1.forward"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.models.resnext.CifarResNeXt.forward", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.models.resnext.CifarResNeXt.forward", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.models.resnext.CifarResNeXt.forward", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.models.resnext.CifarResNeXt.forward", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.models.resnext.CifarResNeXt.forward"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "conv_1_3x3", ".", "forward", "(", "x", ")", "\n", "x", "=", "F", ".", "relu", "(", "self", ".", "bn_1", ".", "forward", "(", "x", ")", ",", "inplace", "=", "True", ")", "\n", "x", "=", "self", ".", "stage_1", ".", "forward", "(", "x", ")", "\n", "x", "=", "self", ".", "stage_2", ".", "forward", "(", "x", ")", "\n", "x", "=", "self", ".", "stage_3", ".", "forward", "(", "x", ")", "\n", "x", "=", "F", ".", "avg_pool2d", "(", "x", ",", "8", ",", "1", ")", "\n", "x", "=", "x", ".", "view", "(", "-", "1", ",", "self", ".", "stages", "[", "3", "]", ")", "\n", "return", "self", ".", "classifier", "(", "x", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.transforms.transforms_wav.LoadAudio.__init__": [[19, 21], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "sample_rate", "=", "16000", ")", ":", "\n", "        ", "self", ".", "sample_rate", "=", "sample_rate", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.transforms.transforms_wav.LoadAudio.__call__": [[22, 33], ["librosa.load", "numpy.zeros"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "data", ")", ":", "\n", "        ", "path", "=", "data", "[", "'path'", "]", "\n", "if", "path", ":", "\n", "            ", "samples", ",", "sample_rate", "=", "librosa", ".", "load", "(", "path", ",", "self", ".", "sample_rate", ")", "\n", "", "else", ":", "\n", "# silence", "\n", "            ", "sample_rate", "=", "self", ".", "sample_rate", "\n", "samples", "=", "np", ".", "zeros", "(", "sample_rate", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "", "data", "[", "'samples'", "]", "=", "samples", "\n", "data", "[", "'sample_rate'", "]", "=", "sample_rate", "\n", "return", "data", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.transforms.transforms_wav.FixAudioLength.__init__": [[37, 39], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "time", "=", "1", ")", ":", "\n", "        ", "self", ".", "time", "=", "time", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.transforms.transforms_wav.FixAudioLength.__call__": [[40, 49], ["int", "len", "len", "numpy.pad", "len"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.sc.pad"], ["", "def", "__call__", "(", "self", ",", "data", ")", ":", "\n", "        ", "samples", "=", "data", "[", "'samples'", "]", "\n", "sample_rate", "=", "data", "[", "'sample_rate'", "]", "\n", "length", "=", "int", "(", "self", ".", "time", "*", "sample_rate", ")", "\n", "if", "length", "<", "len", "(", "samples", ")", ":", "\n", "            ", "data", "[", "'samples'", "]", "=", "samples", "[", ":", "length", "]", "\n", "", "elif", "length", ">", "len", "(", "samples", ")", ":", "\n", "            ", "data", "[", "'samples'", "]", "=", "np", ".", "pad", "(", "samples", ",", "(", "0", ",", "length", "-", "len", "(", "samples", ")", ")", ",", "\"constant\"", ")", "\n", "", "return", "data", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.transforms.transforms_wav.ChangeAmplitude.__init__": [[53, 55], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "amplitude_range", "=", "(", "0.7", ",", "1.1", ")", ")", ":", "\n", "        ", "self", ".", "amplitude_range", "=", "amplitude_range", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.transforms.transforms_wav.ChangeAmplitude.__call__": [[56, 62], ["transforms_wav.should_apply_transform", "random.uniform"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.transforms.transforms_wav.should_apply_transform"], ["", "def", "__call__", "(", "self", ",", "data", ")", ":", "\n", "        ", "if", "not", "should_apply_transform", "(", ")", ":", "\n", "            ", "return", "data", "\n", "\n", "", "data", "[", "'samples'", "]", "=", "data", "[", "'samples'", "]", "*", "random", ".", "uniform", "(", "*", "self", ".", "amplitude_range", ")", "\n", "return", "data", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.transforms.transforms_wav.ChangeSpeedAndPitchAudio.__init__": [[66, 68], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "max_scale", "=", "0.2", ")", ":", "\n", "        ", "self", ".", "max_scale", "=", "max_scale", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.transforms.transforms_wav.ChangeSpeedAndPitchAudio.__call__": [[69, 79], ["random.uniform", "numpy.interp().astype", "transforms_wav.should_apply_transform", "numpy.interp", "numpy.arange", "numpy.arange", "len", "len"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.transforms.transforms_wav.should_apply_transform"], ["", "def", "__call__", "(", "self", ",", "data", ")", ":", "\n", "        ", "if", "not", "should_apply_transform", "(", ")", ":", "\n", "            ", "return", "data", "\n", "\n", "", "samples", "=", "data", "[", "'samples'", "]", "\n", "sample_rate", "=", "data", "[", "'sample_rate'", "]", "\n", "scale", "=", "random", ".", "uniform", "(", "-", "self", ".", "max_scale", ",", "self", ".", "max_scale", ")", "\n", "speed_fac", "=", "1.0", "/", "(", "1", "+", "scale", ")", "\n", "data", "[", "'samples'", "]", "=", "np", ".", "interp", "(", "np", ".", "arange", "(", "0", ",", "len", "(", "samples", ")", ",", "speed_fac", ")", ",", "np", ".", "arange", "(", "0", ",", "len", "(", "samples", ")", ")", ",", "samples", ")", ".", "astype", "(", "np", ".", "float32", ")", "\n", "return", "data", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.transforms.transforms_wav.StretchAudio.__init__": [[83, 85], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "max_scale", "=", "0.2", ")", ":", "\n", "        ", "self", ".", "max_scale", "=", "max_scale", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.transforms.transforms_wav.StretchAudio.__call__": [[86, 93], ["random.uniform", "librosa.effects.time_stretch", "transforms_wav.should_apply_transform"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.transforms.transforms_wav.should_apply_transform"], ["", "def", "__call__", "(", "self", ",", "data", ")", ":", "\n", "        ", "if", "not", "should_apply_transform", "(", ")", ":", "\n", "            ", "return", "data", "\n", "\n", "", "scale", "=", "random", ".", "uniform", "(", "-", "self", ".", "max_scale", ",", "self", ".", "max_scale", ")", "\n", "data", "[", "'samples'", "]", "=", "librosa", ".", "effects", ".", "time_stretch", "(", "data", "[", "'samples'", "]", ",", "1", "+", "scale", ")", "\n", "return", "data", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.transforms.transforms_wav.TimeshiftAudio.__init__": [[97, 99], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "max_shift_seconds", "=", "0.2", ")", ":", "\n", "        ", "self", ".", "max_shift_seconds", "=", "max_shift_seconds", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.transforms.transforms_wav.TimeshiftAudio.__call__": [[100, 113], ["random.randint", "max", "numpy.pad", "transforms_wav.should_apply_transform", "min", "len"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.sc.pad", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.transforms.transforms_wav.should_apply_transform"], ["", "def", "__call__", "(", "self", ",", "data", ")", ":", "\n", "        ", "if", "not", "should_apply_transform", "(", ")", ":", "\n", "            ", "return", "data", "\n", "\n", "", "samples", "=", "data", "[", "'samples'", "]", "\n", "sample_rate", "=", "data", "[", "'sample_rate'", "]", "\n", "max_shift", "=", "(", "sample_rate", "*", "self", ".", "max_shift_seconds", ")", "\n", "shift", "=", "random", ".", "randint", "(", "-", "max_shift", ",", "max_shift", ")", "\n", "a", "=", "-", "min", "(", "0", ",", "shift", ")", "\n", "b", "=", "max", "(", "0", ",", "shift", ")", "\n", "samples", "=", "np", ".", "pad", "(", "samples", ",", "(", "a", ",", "b", ")", ",", "\"constant\"", ")", "\n", "data", "[", "'samples'", "]", "=", "samples", "[", ":", "len", "(", "samples", ")", "-", "a", "]", "if", "a", "else", "samples", "[", "b", ":", "]", "\n", "return", "data", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.transforms.transforms_wav.AddBackgroundNoise.__init__": [[117, 120], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "bg_dataset", ",", "max_percentage", "=", "0.45", ")", ":", "\n", "        ", "self", ".", "bg_dataset", "=", "bg_dataset", "\n", "self", ".", "max_percentage", "=", "max_percentage", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.transforms.transforms_wav.AddBackgroundNoise.__call__": [[121, 130], ["random.uniform", "transforms_wav.should_apply_transform", "random.choice"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.transforms.transforms_wav.should_apply_transform"], ["", "def", "__call__", "(", "self", ",", "data", ")", ":", "\n", "        ", "if", "not", "should_apply_transform", "(", ")", ":", "\n", "            ", "return", "data", "\n", "\n", "", "samples", "=", "data", "[", "'samples'", "]", "\n", "noise", "=", "random", ".", "choice", "(", "self", ".", "bg_dataset", ")", "[", "'samples'", "]", "\n", "percentage", "=", "random", ".", "uniform", "(", "0", ",", "self", ".", "max_percentage", ")", "\n", "data", "[", "'samples'", "]", "=", "samples", "*", "(", "1", "-", "percentage", ")", "+", "noise", "*", "percentage", "\n", "return", "data", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.transforms.transforms_wav.ToMelSpectrogram.__init__": [[134, 136], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "n_mels", "=", "32", ")", ":", "\n", "        ", "self", ".", "n_mels", "=", "n_mels", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.transforms.transforms_wav.ToMelSpectrogram.__call__": [[137, 143], ["librosa.feature.melspectrogram", "librosa.power_to_db"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "data", ")", ":", "\n", "        ", "samples", "=", "data", "[", "'samples'", "]", "\n", "sample_rate", "=", "data", "[", "'sample_rate'", "]", "\n", "s", "=", "librosa", ".", "feature", ".", "melspectrogram", "(", "samples", ",", "sr", "=", "sample_rate", ",", "n_mels", "=", "self", ".", "n_mels", ")", "\n", "data", "[", "'mel_spectrogram'", "]", "=", "librosa", ".", "power_to_db", "(", "s", ",", "ref", "=", "np", ".", "max", ")", "\n", "return", "data", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.transforms.transforms_wav.ToTensor.__init__": [[147, 151], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "np_name", ",", "tensor_name", ",", "normalize", "=", "None", ")", ":", "\n", "        ", "self", ".", "np_name", "=", "np_name", "\n", "self", ".", "tensor_name", "=", "tensor_name", "\n", "self", ".", "normalize", "=", "normalize", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.transforms.transforms_wav.ToTensor.__call__": [[152, 160], ["torch.FloatTensor"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "data", ")", ":", "\n", "        ", "tensor", "=", "torch", ".", "FloatTensor", "(", "data", "[", "self", ".", "np_name", "]", ")", "\n", "if", "self", ".", "normalize", "is", "not", "None", ":", "\n", "            ", "mean", ",", "std", "=", "self", ".", "normalize", "\n", "tensor", "-=", "mean", "\n", "tensor", "/=", "std", "\n", "", "data", "[", "self", ".", "tensor_name", "]", "=", "tensor", "\n", "return", "data", "\n", "", "", ""]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.transforms.transforms_wav.should_apply_transform": [[12, 15], ["random.random"], "function", ["None"], ["def", "should_apply_transform", "(", "prob", "=", "0.5", ")", ":", "\n", "    ", "\"\"\"Transforms are only randomly applied with the given probability.\"\"\"", "\n", "return", "random", ".", "random", "(", ")", "<", "prob", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.transforms.transforms_stft.ToSTFT.__init__": [[17, 20], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "n_fft", "=", "2048", ",", "hop_length", "=", "512", ")", ":", "\n", "        ", "self", ".", "n_fft", "=", "n_fft", "\n", "self", ".", "hop_length", "=", "hop_length", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.transforms.transforms_stft.ToSTFT.__call__": [[21, 29], ["librosa.stft"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "data", ")", ":", "\n", "        ", "samples", "=", "data", "[", "'samples'", "]", "\n", "sample_rate", "=", "data", "[", "'sample_rate'", "]", "\n", "data", "[", "'n_fft'", "]", "=", "self", ".", "n_fft", "\n", "data", "[", "'hop_length'", "]", "=", "self", ".", "hop_length", "\n", "data", "[", "'stft'", "]", "=", "librosa", ".", "stft", "(", "samples", ",", "n_fft", "=", "self", ".", "n_fft", ",", "hop_length", "=", "self", ".", "hop_length", ")", "\n", "data", "[", "'stft_shape'", "]", "=", "data", "[", "'stft'", "]", ".", "shape", "\n", "return", "data", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.transforms.transforms_stft.StretchAudioOnSTFT.__init__": [[33, 35], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "max_scale", "=", "0.2", ")", ":", "\n", "        ", "self", ".", "max_scale", "=", "max_scale", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.transforms.transforms_stft.StretchAudioOnSTFT.__call__": [[36, 47], ["random.uniform", "librosa.core.phase_vocoder", "transforms_wav.should_apply_transform"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.transforms.transforms_wav.should_apply_transform"], ["", "def", "__call__", "(", "self", ",", "data", ")", ":", "\n", "        ", "if", "not", "should_apply_transform", "(", ")", ":", "\n", "            ", "return", "data", "\n", "\n", "", "stft", "=", "data", "[", "'stft'", "]", "\n", "sample_rate", "=", "data", "[", "'sample_rate'", "]", "\n", "hop_length", "=", "data", "[", "'hop_length'", "]", "\n", "scale", "=", "random", ".", "uniform", "(", "-", "self", ".", "max_scale", ",", "self", ".", "max_scale", ")", "\n", "stft_stretch", "=", "librosa", ".", "core", ".", "phase_vocoder", "(", "stft", ",", "1", "+", "scale", ",", "hop_length", "=", "hop_length", ")", "\n", "data", "[", "'stft'", "]", "=", "stft_stretch", "\n", "return", "data", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.transforms.transforms_stft.TimeshiftAudioOnSTFT.__init__": [[51, 53], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "max_shift", "=", "8", ")", ":", "\n", "        ", "self", ".", "max_shift", "=", "max_shift", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.transforms.transforms_stft.TimeshiftAudioOnSTFT.__call__": [[54, 69], ["random.randint", "max", "numpy.pad", "transforms_wav.should_apply_transform", "min"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.sc.pad", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.transforms.transforms_wav.should_apply_transform"], ["", "def", "__call__", "(", "self", ",", "data", ")", ":", "\n", "        ", "if", "not", "should_apply_transform", "(", ")", ":", "\n", "            ", "return", "data", "\n", "\n", "", "stft", "=", "data", "[", "'stft'", "]", "\n", "shift", "=", "random", ".", "randint", "(", "-", "self", ".", "max_shift", ",", "self", ".", "max_shift", ")", "\n", "a", "=", "-", "min", "(", "0", ",", "shift", ")", "\n", "b", "=", "max", "(", "0", ",", "shift", ")", "\n", "stft", "=", "np", ".", "pad", "(", "stft", ",", "(", "(", "0", ",", "0", ")", ",", "(", "a", ",", "b", ")", ")", ",", "\"constant\"", ")", "\n", "if", "a", "==", "0", ":", "\n", "            ", "stft", "=", "stft", "[", ":", ",", "b", ":", "]", "\n", "", "else", ":", "\n", "            ", "stft", "=", "stft", "[", ":", ",", "0", ":", "-", "a", "]", "\n", "", "data", "[", "'stft'", "]", "=", "stft", "\n", "return", "data", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.transforms.transforms_stft.AddBackgroundNoiseOnSTFT.__init__": [[73, 76], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "bg_dataset", ",", "max_percentage", "=", "0.45", ")", ":", "\n", "        ", "self", ".", "bg_dataset", "=", "bg_dataset", "\n", "self", ".", "max_percentage", "=", "max_percentage", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.transforms.transforms_stft.AddBackgroundNoiseOnSTFT.__call__": [[77, 85], ["random.uniform", "transforms_wav.should_apply_transform", "random.choice"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.transforms.transforms_wav.should_apply_transform"], ["", "def", "__call__", "(", "self", ",", "data", ")", ":", "\n", "        ", "if", "not", "should_apply_transform", "(", ")", ":", "\n", "            ", "return", "data", "\n", "\n", "", "noise", "=", "random", ".", "choice", "(", "self", ".", "bg_dataset", ")", "[", "'stft'", "]", "\n", "percentage", "=", "random", ".", "uniform", "(", "0", ",", "self", ".", "max_percentage", ")", "\n", "data", "[", "'stft'", "]", "=", "data", "[", "'stft'", "]", "*", "(", "1", "-", "percentage", ")", "+", "noise", "*", "percentage", "\n", "return", "data", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.transforms.transforms_stft.FixSTFTDimension.__call__": [[89, 100], ["numpy.pad"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.dataloaders.sc.pad"], ["def", "__call__", "(", "self", ",", "data", ")", ":", "\n", "        ", "stft", "=", "data", "[", "'stft'", "]", "\n", "t_len", "=", "stft", ".", "shape", "[", "1", "]", "\n", "orig_t_len", "=", "data", "[", "'stft_shape'", "]", "[", "1", "]", "\n", "if", "t_len", ">", "orig_t_len", ":", "\n", "            ", "stft", "=", "stft", "[", ":", ",", "0", ":", "orig_t_len", "]", "\n", "", "elif", "t_len", "<", "orig_t_len", ":", "\n", "            ", "stft", "=", "np", ".", "pad", "(", "stft", ",", "(", "(", "0", ",", "0", ")", ",", "(", "0", ",", "orig_t_len", "-", "t_len", ")", ")", ",", "\"constant\"", ")", "\n", "\n", "", "data", "[", "'stft'", "]", "=", "stft", "\n", "return", "data", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.transforms.transforms_stft.ToMelSpectrogramFromSTFT.__init__": [[104, 106], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "n_mels", "=", "32", ")", ":", "\n", "        ", "self", ".", "n_mels", "=", "n_mels", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.transforms.transforms_stft.ToMelSpectrogramFromSTFT.__call__": [[107, 115], ["librosa.filters.mel", "numpy.dot", "librosa.power_to_db", "numpy.abs"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "data", ")", ":", "\n", "        ", "stft", "=", "data", "[", "'stft'", "]", "\n", "sample_rate", "=", "data", "[", "'sample_rate'", "]", "\n", "n_fft", "=", "data", "[", "'n_fft'", "]", "\n", "mel_basis", "=", "librosa", ".", "filters", ".", "mel", "(", "sample_rate", ",", "n_fft", ",", "self", ".", "n_mels", ")", "\n", "s", "=", "np", ".", "dot", "(", "mel_basis", ",", "np", ".", "abs", "(", "stft", ")", "**", "2.0", ")", "\n", "data", "[", "'mel_spectrogram'", "]", "=", "librosa", ".", "power_to_db", "(", "s", ",", "ref", "=", "np", ".", "max", ")", "\n", "return", "data", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.transforms.transforms_stft.DeleteSTFT.__call__": [[119, 122], ["None"], "methods", ["None"], ["def", "__call__", "(", "self", ",", "data", ")", ":", "\n", "        ", "del", "data", "[", "'stft'", "]", "\n", "return", "data", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.transforms.transforms_stft.AudioFromSTFT.__call__": [[126, 130], ["librosa.core.istft"], "methods", ["None"], ["def", "__call__", "(", "self", ",", "data", ")", ":", "\n", "        ", "stft", "=", "data", "[", "'stft'", "]", "\n", "data", "[", "'istft_samples'", "]", "=", "librosa", ".", "core", ".", "istft", "(", "stft", ",", "dtype", "=", "data", "[", "'samples'", "]", ".", "dtype", ")", "\n", "return", "data", "\n", "", "", ""]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.__init__": [[16, 34], ["pathlib.Path", "pathlib.Path", "turk_create_batch.Experiment._verify_input_dir", "turk_create_batch.Experiment._verify_input_filenames", "turk_create_batch.Experiment._shuffle_files", "turk_create_batch.Experiment.create_uids"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment._verify_input_dir", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment._verify_input_filenames", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment._shuffle_files", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.create_uids"], ["  ", "def", "__init__", "(", "\n", "self", ",", "\n", "condition", ",", "\n", "input_dir", ",", "\n", "output_dir", ",", "\n", "url_templ", ",", "\n", "methods", ",", "\n", ")", ":", "\n", "      ", "self", ".", "condition", "=", "condition", "\n", "self", ".", "input_dir", "=", "Path", "(", "input_dir", ")", "\n", "self", ".", "output_dir", "=", "Path", "(", "output_dir", ")", "\n", "self", ".", "url_templ", "=", "url_templ", "\n", "self", ".", "methods", "=", "methods", "\n", "\n", "self", ".", "_verify_input_dir", "(", ")", "\n", "self", ".", "_verify_input_filenames", "(", ")", "\n", "self", ".", "_shuffle_files", "(", ")", "\n", "self", ".", "uids", "=", "self", ".", "create_uids", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment._verify_input_dir": [[35, 39], ["turk_create_batch.Experiment.input_dir.joinpath().exists", "turk_create_batch.Experiment.input_dir.joinpath"], "methods", ["None"], ["", "def", "_verify_input_dir", "(", "self", ")", ":", "\n", "# Check that input_dir exists and contains folders for each method", "\n", "    ", "for", "method", "in", "self", ".", "methods", ":", "\n", "      ", "assert", "self", ".", "input_dir", ".", "joinpath", "(", "method", ")", ".", "exists", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment._verify_input_filenames": [[40, 51], ["set", "list", "print", "set", "natsort.natsorted", "turk_create_batch.Experiment.input_dir.joinpath().glob", "len", "len", "len", "turk_create_batch.Experiment.input_dir.joinpath().glob", "turk_create_batch.Experiment.input_dir.joinpath", "turk_create_batch.Experiment.input_dir.joinpath"], "methods", ["None"], ["", "", "def", "_verify_input_filenames", "(", "self", ")", ":", "\n", "# Check that each method has the same number of files and identical filenames", "\n", "    ", "filenames", "=", "set", "(", "[", "file", ".", "name", "for", "file", "in", "self", ".", "input_dir", ".", "joinpath", "(", "self", ".", "methods", "[", "0", "]", ")", ".", "glob", "(", "'*.wav'", ")", "]", ")", "\n", "for", "method", "in", "self", ".", "methods", "[", "1", ":", "]", ":", "\n", "      ", "files", "=", "set", "(", "self", ".", "input_dir", ".", "joinpath", "(", "method", ")", ".", "glob", "(", "'*.wav'", ")", ")", "\n", "assert", "len", "(", "files", ")", "==", "len", "(", "filenames", ")", "\n", "for", "file", "in", "files", ":", "\n", "        ", "assert", "file", ".", "name", "in", "filenames", ",", "f'{file.name} is not in the set of filenames'", "\n", "\n", "", "", "self", ".", "filenames", "=", "list", "(", "natsorted", "(", "filenames", ")", ")", "\n", "print", "(", "\"Found {} files\"", ".", "format", "(", "len", "(", "self", ".", "filenames", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment._shuffle_files": [[52, 56], ["random.seed", "random.shuffle"], "methods", ["None"], ["", "def", "_shuffle_files", "(", "self", ")", ":", "\n", "# Shuffle the filenames", "\n", "    ", "random", ".", "seed", "(", "42", ")", "\n", "random", ".", "shuffle", "(", "self", ".", "filenames", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.create_uids": [[57, 72], ["random.seed", "set", "set.add", "uuid.uuid4", "uuid.uuid4"], "methods", ["None"], ["", "def", "create_uids", "(", "self", ")", ":", "\n", "# Construct a table mapping each (method, filename) to a unique ID", "\n", "    ", "random", ".", "seed", "(", "42", ")", "\n", "uids", "=", "{", "}", "\n", "_uuids", "=", "set", "(", ")", "\n", "for", "method", "in", "self", ".", "methods", ":", "\n", "      ", "for", "filename", "in", "self", ".", "filenames", ":", "\n", "# Generate a unique ID", "\n", "        ", "uid", "=", "uuid", ".", "uuid4", "(", ")", ".", "hex", "\n", "while", "uid", "in", "_uuids", ":", "\n", "          ", "uid", "=", "uuid", ".", "uuid4", "(", ")", ".", "hex", "\n", "", "_uuids", ".", "add", "(", "uid", ")", "\n", "\n", "uids", "[", "(", "method", ",", "filename", ")", "]", "=", "uid", "\n", "", "", "return", "uids", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.construct_batches": [[73, 81], ["len", "range", "len"], "methods", ["None"], ["", "def", "construct_batches", "(", "self", ",", "batch_size", ":", "int", ")", ":", "\n", "    ", "assert", "batch_size", ">", "0", "\n", "assert", "len", "(", "self", ".", "filenames", ")", "%", "batch_size", "==", "0", ",", "'batch_size must evenly divide the number of files'", "\n", "\n", "# Split the files into batches", "\n", "batches", "=", "[", "self", ".", "filenames", "[", "i", ":", "i", "+", "batch_size", "]", "for", "i", "in", "range", "(", "0", ",", "len", "(", "self", ".", "filenames", ")", ",", "batch_size", ")", "]", "\n", "\n", "return", "batches", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.create_output_dir": [[82, 88], ["turk_create_batch.Experiment.output_dir.mkdir", "turk_create_batch.Experiment.output_dir.joinpath().mkdir", "turk_create_batch.Experiment.output_dir.joinpath"], "methods", ["None"], ["", "def", "create_output_dir", "(", "self", ")", ":", "\n", "# Create the output directory", "\n", "    ", "self", ".", "output_dir", ".", "mkdir", "(", "parents", "=", "True", ",", "exist_ok", "=", "True", ")", "\n", "\n", "# Create a subdirectory for the condition", "\n", "self", ".", "output_dir", ".", "joinpath", "(", "self", ".", "condition", ")", ".", "mkdir", "(", "parents", "=", "True", ",", "exist_ok", "=", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.process_data": [[89, 148], ["random.seed", "turk_create_batch.Experiment.create_output_dir", "turk_create_batch.Experiment.construct_batches", "enumerate", "urls_by_batch.items", "types.SimpleNamespace", "turk_create_batch.Experiment.output_dir.joinpath().joinpath", "turk_create_batch.Experiment.mkdir", "random.shuffle", "open", "open", "f.write", "open", "turk_create_batch.Experiment.uids.items", "str", "turk_create_batch.Experiment.output_dir.joinpath().joinpath", "f.write", "turk_create_batch.Experiment.output_dir.joinpath().joinpath", "open", "f.write", "turk_create_batch.Experiment.output_dir.joinpath().joinpath", "f.write", "turk_create_batch.Experiment.output_dir.joinpath", "turk_create_batch.Experiment.input_dir.joinpath().joinpath", "turk_create_batch.Experiment.joinpath", "shutil.copy", "urls.append", "str", "turk_create_batch.Experiment.output_dir.joinpath().joinpath", "turk_create_batch.Experiment.get_url", "turk_create_batch.Experiment.output_dir.joinpath", "turk_create_batch.Experiment.output_dir.joinpath", "urls_by_batch.items", "turk_create_batch.Experiment.output_dir.joinpath", "turk_create_batch.Experiment.input_dir.joinpath", "str", "turk_create_batch.Experiment.output_dir.joinpath", "range", "range", "len", "len"], "methods", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.create_output_dir", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.construct_batches", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.get_url"], ["", "def", "process_data", "(", "self", ",", "batch_size", ":", "int", ")", ":", "\n", "# Set random seed", "\n", "    ", "random", ".", "seed", "(", "42", ")", "\n", "\n", "# Create output directory", "\n", "self", ".", "create_output_dir", "(", ")", "\n", "\n", "# Construct batches", "\n", "# Each batch contains a fixed set of filenames, and includes those filenames for all methods", "\n", "batches", "=", "self", ".", "construct_batches", "(", "batch_size", ")", "\n", "\n", "# 1. Create a subdirectory for each batch", "\n", "# 2. For each method, copy the waveforms into the subdirectory after renaming them using self.uids", "\n", "# 3. Create a list of URLs for each batch", "\n", "urls_by_batch", "=", "{", "}", "\n", "for", "i", ",", "batch", "in", "enumerate", "(", "batches", ")", ":", "\n", "      ", "urls", "=", "[", "]", "\n", "\n", "# Create output directory", "\n", "batch_dir", "=", "self", ".", "output_dir", ".", "joinpath", "(", "self", ".", "condition", ")", ".", "joinpath", "(", "str", "(", "i", ")", ")", "\n", "batch_dir", ".", "mkdir", "(", "parents", "=", "True", ",", "exist_ok", "=", "False", ")", "\n", "\n", "# Copy files into the batch directory", "\n", "for", "method", "in", "self", ".", "methods", ":", "\n", "        ", "for", "filename", "in", "batch", ":", "\n", "          ", "src", "=", "self", ".", "input_dir", ".", "joinpath", "(", "method", ")", ".", "joinpath", "(", "filename", ")", "\n", "dst", "=", "batch_dir", ".", "joinpath", "(", "f'{self.uids[(method, filename)]}.wav'", ")", "\n", "shutil", ".", "copy", "(", "src", ",", "dst", ")", "\n", "\n", "urls", ".", "append", "(", "self", ".", "get_url", "(", "self", ".", "condition", ",", "str", "(", "i", ")", ",", "self", ".", "uids", "[", "(", "method", ",", "filename", ")", "]", ")", ")", "\n", "\n", "# Shuffle the URLs to randomize the order of the waveforms", "\n", "", "", "random", ".", "shuffle", "(", "urls", ")", "\n", "urls_by_batch", "[", "str", "(", "i", ")", "]", "=", "urls", "\n", "\n", "# Store `batches` to disk", "\n", "", "with", "open", "(", "self", ".", "output_dir", ".", "joinpath", "(", "self", ".", "condition", ")", ".", "joinpath", "(", "'batches.txt'", ")", ",", "'w'", ")", "as", "f", ":", "\n", "      ", "for", "batch", "in", "batches", ":", "\n", "        ", "f", ".", "write", "(", "' '", ".", "join", "(", "batch", ")", "+", "'\\n'", ")", "\n", "\n", "# Store `urls_by_batch` to disk", "\n", "", "", "with", "open", "(", "self", ".", "output_dir", ".", "joinpath", "(", "self", ".", "condition", ")", ".", "joinpath", "(", "'urls.csv'", ")", ",", "'w'", ")", "as", "f", ":", "\n", "      ", "urls", "=", "[", "','", ".", "join", "(", "[", "f'recording_{i}_url'", "for", "i", "in", "range", "(", "len", "(", "self", ".", "methods", ")", "*", "batch_size", ")", "]", ")", "]", "+", "[", "\",\"", ".", "join", "(", "urls", ")", "for", "_", ",", "urls", "in", "urls_by_batch", ".", "items", "(", ")", "]", "\n", "f", ".", "write", "(", "'\\n'", ".", "join", "(", "urls", ")", ")", "\n", "\n", "# Store `urls_by_batch` by batch", "\n", "", "for", "batch", ",", "urls", "in", "urls_by_batch", ".", "items", "(", ")", ":", "\n", "      ", "with", "open", "(", "self", ".", "output_dir", ".", "joinpath", "(", "self", ".", "condition", ")", ".", "joinpath", "(", "f'urls_{batch}.csv'", ")", ",", "'w'", ")", "as", "f", ":", "\n", "        ", "urls", "=", "[", "','", ".", "join", "(", "[", "f'recording_{i}_url'", "for", "i", "in", "range", "(", "len", "(", "self", ".", "methods", ")", "*", "batch_size", ")", "]", ")", "]", "+", "[", "\",\"", ".", "join", "(", "urls", ")", "]", "\n", "f", ".", "write", "(", "'\\n'", ".", "join", "(", "urls", ")", ")", "\n", "\n", "# Store information in `self.uids` to disk", "\n", "", "", "with", "open", "(", "self", ".", "output_dir", ".", "joinpath", "(", "self", ".", "condition", ")", ".", "joinpath", "(", "'uids.txt'", ")", ",", "'w'", ")", "as", "f", ":", "\n", "      ", "for", "(", "method", ",", "filename", ")", ",", "uid", "in", "self", ".", "uids", ".", "items", "(", ")", ":", "\n", "        ", "f", ".", "write", "(", "f'{method} {filename} {uid}\\n'", ")", "\n", "\n", "", "", "return", "SimpleNamespace", "(", "\n", "batches", "=", "batches", ",", "\n", "urls_by_batch", "=", "urls_by_batch", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.get_url": [[150, 152], ["turk_create_batch.Experiment.url_templ.format"], "methods", ["None"], ["", "def", "get_url", "(", "self", ",", "condition", ",", "batch", ",", "uid", ")", ":", "\n", "    ", "return", "self", ".", "url_templ", ".", "format", "(", "condition", ",", "batch", ",", "uid", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.turk_create_batch.Experiment.upload_data": [[153, 155], ["None"], "methods", ["None"], ["", "def", "upload_data", "(", ")", ":", "\n", "    ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.prepare_sc09.move_files": [[10, 22], ["os.makedirs", "enumerate", "os.mkdir", "FileExistsError", "shutil.copy", "shutil.copy", "src_files[].split"], "function", ["None"], ["def", "move_files", "(", "src_dir", ",", "src_files", ",", "target_dir", ",", "indices", ",", "discard", "=", "False", ")", ":", "\n", "    ", "os", ".", "makedirs", "(", "target_dir", ",", "exist_ok", "=", "True", ")", "\n", "for", "i", ",", "digit", "in", "enumerate", "(", "digits", ")", ":", "\n", "        ", "try", ":", "\n", "            ", "os", ".", "mkdir", "(", "target_dir", "+", "f'{digit}'", ")", "\n", "", "except", "FileExistsError", ":", "\n", "            ", "raise", "FileExistsError", "(", "f'{target_dir}/{digit} already exists, please delete it before running this script.'", ")", "\n", "", "for", "index", "in", "indices", "[", "i", "]", ":", "\n", "            ", "if", "not", "discard", ":", "\n", "                ", "shutil", ".", "copy", "(", "f'{src_dir}/{src_files[index]}'", ",", "f'{target_dir}/{digit}/{src_files[index]}'", ")", "\n", "", "else", ":", "\n", "                ", "shutil", ".", "copy", "(", "f'{src_dir}/{src_files[index]}'", ",", "f'{target_dir}/{digit}/{src_files[index].split(\"/\")[-1]}'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.prepare_sc09.standardize_filenames": [[23, 32], ["natsort.natsorted", "shutil.rmtree", "os.path.exists", "os.listdir", "f.endswith", "shutil.move"], "function", ["None"], ["", "", "", "", "def", "standardize_filenames", "(", "target_dir", ")", ":", "\n", "    ", "i", "=", "0", "\n", "for", "digit", "in", "digits", ":", "\n", "        ", "if", "not", "os", ".", "path", ".", "exists", "(", "f'{target_dir}/{digit}/'", ")", ":", "continue", "\n", "for", "f", "in", "natsorted", "(", "os", ".", "listdir", "(", "f'{target_dir}/{digit}/'", ")", ")", ":", "\n", "            ", "if", "f", ".", "endswith", "(", "'.wav'", ")", ":", "\n", "                ", "shutil", ".", "move", "(", "f'{target_dir}/{digit}/{f}'", ",", "f'{target_dir}/{i}.wav'", ")", "\n", "i", "+=", "1", "\n", "", "", "shutil", ".", "rmtree", "(", "f'{target_dir}/{digit}/'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.prepare_sc09.grab_indices": [[33, 45], ["range", "numpy.zeros_like", "numpy.random.choice", "numpy.argsort", "numpy.where", "probs.argmax", "probs.argmax", "probs.argmax"], "function", ["None"], ["", "", "def", "grab_indices", "(", "probs", ",", "samples_per_class", "=", "50", ")", ":", "\n", "    ", "confident_indices", "=", "{", "}", "\n", "random_indices", "=", "{", "}", "\n", "for", "digit", "in", "range", "(", "10", ")", ":", "\n", "# Rows with prediction = digit", "\n", "        ", "rows", "=", "np", ".", "zeros_like", "(", "probs", ")", "\n", "rows", "[", "probs", ".", "argmax", "(", "1", ")", "==", "digit", "]", "=", "probs", "[", "probs", ".", "argmax", "(", "1", ")", "==", "digit", "]", "\n", "# Sort rows by confidence and take the last 50 indices", "\n", "confident_indices", "[", "digit", "]", "=", "np", ".", "argsort", "(", "rows", "[", ":", ",", "digit", "]", ")", "[", "-", "samples_per_class", ":", "]", "\n", "# Take a random sample of 50 digits", "\n", "random_indices", "[", "digit", "]", "=", "np", ".", "random", ".", "choice", "(", "np", ".", "where", "(", "probs", ".", "argmax", "(", "1", ")", "==", "digit", ")", "[", "0", "]", ",", "samples_per_class", ",", "replace", "=", "False", ")", "\n", "", "return", "confident_indices", ",", "random_indices", "\n", "\n"]], "home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.prepare_sc09.prepare": [[46, 76], ["numpy.load", "list", "prepare_sc09.grab_indices", "prepare_sc09.move_files", "prepare_sc09.standardize_filenames", "natsort.natsorted", "os.listdir", "e.endswith"], "function", ["home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.prepare_sc09.grab_indices", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.prepare_sc09.move_files", "home.repos.pwc.inspect_result.hazyresearch_state-spaces.mturk.prepare_sc09.standardize_filenames"], ["", "def", "prepare", "(", "\n", "method", ",", "\n", "cache_dir", "=", "'../sc09_classifier/cache/'", ",", "\n", "sample_dir", "=", "'../samples/sc09/'", ",", "\n", "target_dir", "=", "'sc09/sc09-unconditional-exp-confident/'", ",", "\n", "n_samples", "=", "2048", ",", "\n", "samples_per_class", "=", "50", ",", "\n", ")", ":", "\n", "\n", "# Load outputs of SC09 classifier (ResNeXt)", "\n", "# Example: `2048-sashimi-diffwave-small-500k-resnext-probs.npy` (where method is `sashimi-diffwave-small-500k`)", "\n", "    ", "probs", "=", "np", ".", "load", "(", "f'{cache_dir}/{n_samples}-{method}-resnext-probs.npy'", ")", "\n", "\n", "# List all .wav sample files in the method directory", "\n", "# Example: all .wav files in `../samples/sc09/sashimi-diffwave-small-500k/`", "\n", "files", "=", "list", "(", "natsorted", "(", "[", "e", "for", "e", "in", "os", ".", "listdir", "(", "f'{sample_dir}/{n_samples}-{method}'", ")", "if", "e", ".", "endswith", "(", "'.wav'", ")", "]", ")", ")", "\n", "\n", "# Grab indices of the top 50 most-confident samples for each digit", "\n", "indices", ",", "_", "=", "grab_indices", "(", "probs", ",", "samples_per_class", ")", "\n", "\n", "# Move the top 50 confident samples for each digit to the method directory", "\n", "move_files", "(", "\n", "f'{sample_dir}/{n_samples}-{method}'", ",", "\n", "files", ",", "\n", "f'{target_dir}/{method}/'", ",", "\n", "indices", ",", "\n", ")", "\n", "\n", "# Rename the files to `0.wav, 1.wav, ...` and flatten the target directory structure", "\n", "standardize_filenames", "(", "f'{target_dir}/{method}'", ")", "\n", "\n"]]}