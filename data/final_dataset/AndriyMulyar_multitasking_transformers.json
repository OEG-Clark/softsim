{"home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.None.setup.PyTest.initialize_options": [[21, 24], ["setuptools.command.test.test.initialize_options"], "methods", ["home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.None.setup.PyTest.initialize_options"], ["def", "initialize_options", "(", "self", ")", ":", "\n", "        ", "TestCommand", ".", "initialize_options", "(", "self", ")", "\n", "self", ".", "pytest_args", "=", "\"\"", "\n", "\n"]], "home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.None.setup.PyTest.run_tests": [[25, 32], ["pytest.main", "sys.exit", "shlex.split"], "methods", ["None"], ["", "def", "run_tests", "(", "self", ")", ":", "\n", "        ", "import", "shlex", "\n", "# import here, cause outside the eggs aren't loaded", "\n", "import", "pytest", "\n", "\n", "errno", "=", "pytest", ".", "main", "(", "shlex", ".", "split", "(", "self", ".", "pytest_args", ")", ")", "\n", "sys", ".", "exit", "(", "errno", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.None.setup.readme": [[8, 11], ["open", "f.read"], "function", ["None"], ["def", "readme", "(", ")", ":", "\n", "    ", "with", "open", "(", "'README.md'", ")", "as", "f", ":", "\n", "        ", "return", "f", ".", "read", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.examples.predict_ner.visualize": [[41, 67], ["experiment_replication.raw_datasets.language.get_language", "experiment_replication.raw_datasets.language.get_language.create_pipe", "print", "displacy.serve", "experiment_replication.raw_datasets.language.get_language.", "docs.append", "language.create_pipe.add_label", "language.char_span", "spans.append"], "function", ["home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.raw_datasets.language.get_language"], ["def", "visualize", "(", "data_generator", ")", ":", "\n", "    ", "from", "spacy", "import", "displacy", "\n", "from", "spacy", ".", "gold", "import", "biluo_tags_from_offsets", "\n", "from", "spacy", ".", "tokens", "import", "Span", "\n", "language", "=", "get_language", "(", ")", "\n", "ner", "=", "language", ".", "create_pipe", "(", "\"ner\"", ")", "\n", "# language.add_pipe(ner, last=True)", "\n", "docs", "=", "[", "]", "\n", "print", "(", "data_generator", ")", "\n", "for", "text", ",", "annotation", "in", "data_generator", ":", "\n", "        ", "doc", "=", "language", "(", "text", ")", "\n", "for", "label", "in", "annotation", "[", "'entity_labels'", "]", ":", "\n", "            ", "ner", ".", "add_label", "(", "label", ")", "\n", "\n", "", "spans", "=", "[", "]", "\n", "for", "key", "in", "annotation", "[", "'entities'", "]", ":", "\n", "            ", "for", "start", ",", "stop", ",", "label", "in", "annotation", "[", "'entities'", "]", "[", "key", "]", ":", "\n", "                ", "span", "=", "doc", ".", "char_span", "(", "start", ",", "stop", ",", "label", "=", "label", ")", "\n", "if", "span", "is", "None", ":", "\n", "                    ", "continue", "\n", "", "spans", ".", "append", "(", "span", ")", "\n", "\n", "", "", "doc", ".", "ents", "=", "spans", "\n", "docs", ".", "append", "(", "doc", ")", "\n", "\n", "", "displacy", ".", "serve", "(", "docs", ",", "style", "=", "\"ent\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.examples.predict_ner.prepare_encoding": [[88, 113], ["predict_ner.prepare_encoding.chunk_encoding"], "function", ["None"], ["def", "prepare_encoding", "(", "encoding", ":", "Encoding", ")", ":", "\n", "    ", "\"\"\"\n    Given a arbitrarily long text (>512 subwords), chunks it into the BERT context window.\n    :param encoding:\n    :return:\n    \"\"\"", "\n", "def", "chunk_encoding", "(", "tensor", ":", "torch", ".", "Tensor", ")", ":", "\n", "        ", "chunks", "=", "tensor", ".", "split", "(", "max_sequence_length", ")", "\n", "batch", "=", "torch", ".", "zeros", "(", "size", "=", "(", "len", "(", "chunks", ")", ",", "max_sequence_length", ")", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "#we don't include special tokens during prediction (empirically, doesn't look like it hurts!)", "\n", "for", "index", ",", "chunk", "in", "enumerate", "(", "chunks", ")", ":", "\n", "            ", "batch", "[", "index", "]", "[", "0", ":", "len", "(", "chunk", ")", "]", "=", "torch", ".", "clone", "(", "chunk", ")", "\n", "# batch[index][0] = tokenizer.cls_token", "\n", "# batch[index][chunk.shape[0] + 1] = tokenizer.sep_token", "\n", "\n", "", "return", "batch", ",", "[", "len", "(", "chunk", ")", "for", "chunk", "in", "chunks", "]", "\n", "\n", "", "input_ids", ",", "num_tokens_in_instance", "=", "chunk_encoding", "(", "torch", ".", "tensor", "(", "encoding", ".", "ids", ",", "dtype", "=", "torch", ".", "long", ")", ")", "\n", "attention_mask", ",", "_", "=", "chunk_encoding", "(", "torch", ".", "tensor", "(", "encoding", ".", "attention_mask", ",", "dtype", "=", "torch", ".", "long", ")", ")", "\n", "token_type_ids", ",", "_", "=", "chunk_encoding", "(", "torch", ".", "tensor", "(", "encoding", ".", "type_ids", ",", "dtype", "=", "torch", ".", "long", ")", ")", "\n", "\n", "\n", "return", "(", "input_ids", ",", "attention_mask", ",", "token_type_ids", ")", ",", "[", "encoding", ".", "offsets", "[", "i", ":", "i", "+", "max_sequence_length", "]", "for", "i", "in", "range", "(", "0", ",", "len", "(", "encoding", ".", "offsets", ")", ",", "max_sequence_length", ")", "]", ",", "num_tokens_in_instance", "\n", "\n"]], "home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.raw_datasets.language.get_language": [[7, 21], ["spacy.load", "spacy.load.add_pipe", "spacy.tokens.Doc.set_extension", "spacy.tokens.Span.set_extension", "language._add_tokenization_exceptions"], "function", ["home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.raw_datasets.language._add_tokenization_exceptions"], ["def", "get_language", "(", "use_tokenizer_exceptions", "=", "True", ")", ":", "\n", "    ", "\"\"\"\n    Retrieves a customized spaCy language\n    :return:\n    \"\"\"", "\n", "\n", "language", "=", "spacy", ".", "load", "(", "'en_core_sci_sm'", ",", "disable", "=", "[", "\"tagger\"", ",", "\"ner\"", "]", ")", "\n", "if", "use_tokenizer_exceptions", ":", "\n", "        ", "_add_tokenization_exceptions", "(", "language", ")", "\n", "", "language", ".", "add_pipe", "(", "_set_sentence_parse_exceptions", ",", "before", "=", "'parser'", ")", "\n", "\n", "Doc", ".", "set_extension", "(", "'id'", ",", "default", "=", "None", ",", "force", "=", "True", ")", "\n", "Span", ".", "set_extension", "(", "'cui'", ",", "default", "=", "None", ",", "force", "=", "True", ")", "\n", "return", "language", "\n", "\n"]], "home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.raw_datasets.language._add_tokenization_exceptions": [[22, 222], ["language.tokenizer.add_special_case", "language.tokenizer.add_special_case", "language.tokenizer.add_special_case", "language.tokenizer.add_special_case", "language.tokenizer.add_special_case", "language.tokenizer.add_special_case", "language.tokenizer.add_special_case", "language.tokenizer.add_special_case", "language.tokenizer.add_special_case", "language.tokenizer.add_special_case", "language.tokenizer.add_special_case", "language.tokenizer.add_special_case", "language.tokenizer.add_special_case", "language.tokenizer.add_special_case", "language.tokenizer.add_special_case", "language.tokenizer.add_special_case", "language.tokenizer.add_special_case", "language.tokenizer.add_special_case", "language.tokenizer.add_special_case", "language.tokenizer.add_special_case", "language.tokenizer.add_special_case", "language.tokenizer.add_special_case", "language.tokenizer.add_special_case", "language.tokenizer.add_special_case", "language.tokenizer.add_special_case", "language.tokenizer.add_special_case", "language.tokenizer.add_special_case", "language.tokenizer.add_special_case", "language.tokenizer.add_special_case", "language.tokenizer.add_special_case", "language.tokenizer.add_special_case", "language.tokenizer.add_special_case", "language.tokenizer.add_special_case", "language.tokenizer.add_special_case", "language.tokenizer.add_special_case", "language.tokenizer.add_special_case", "language.tokenizer.add_special_case", "language.tokenizer.add_special_case", "language.tokenizer.add_special_case", "language.tokenizer.add_special_case", "language.tokenizer.add_special_case", "language.tokenizer.add_special_case", "language.tokenizer.add_special_case", "language.tokenizer.add_special_case", "language.tokenizer.add_special_case", "language.tokenizer.add_special_case", "language.tokenizer.add_special_case", "language.tokenizer.add_special_case", "language.tokenizer.add_special_case", "language.tokenizer.add_special_case", "language.tokenizer.add_special_case", "language.tokenizer.add_special_case", "language.tokenizer.add_special_case", "language.tokenizer.add_special_case", "language.tokenizer.add_special_case", "language.tokenizer.add_special_case", "language.tokenizer.add_special_case", "language.tokenizer.add_special_case", "language.tokenizer.add_special_case", "language.tokenizer.add_special_case", "language.tokenizer.add_special_case", "language.tokenizer.add_special_case", "language.tokenizer.add_special_case", "language.tokenizer.add_special_case", "language.tokenizer.add_special_case", "language.tokenizer.add_special_case", "language.tokenizer.add_special_case", "language.tokenizer.add_special_case", "language.tokenizer.add_special_case", "language.tokenizer.add_special_case", "language.tokenizer.add_special_case", "language.tokenizer.add_special_case", "language.tokenizer.add_special_case", "language.tokenizer.add_special_case", "language.tokenizer.add_special_case", "language.tokenizer.add_special_case", "language.tokenizer.add_special_case", "language.tokenizer.add_special_case", "language.tokenizer.add_special_case", "language.tokenizer.add_special_case", "language.tokenizer.add_special_case", "language.tokenizer.add_special_case", "language.tokenizer.add_special_case", "language.tokenizer.add_special_case", "language.tokenizer.add_special_case", "language.tokenizer.add_special_case", "language.tokenizer.add_special_case", "language.tokenizer.add_special_case", "language.tokenizer.add_special_case", "language.tokenizer.add_special_case", "language.tokenizer.add_special_case", "language.tokenizer.add_special_case", "language.tokenizer.add_special_case", "language.tokenizer.add_special_case", "language.tokenizer.add_special_case", "language.tokenizer.add_special_case", "language.tokenizer.add_special_case", "language.tokenizer.add_special_case", "language.tokenizer.add_special_case", "language.tokenizer.add_special_case", "language.tokenizer.add_special_case", "language.tokenizer.add_special_case", "language.tokenizer.add_special_case", "language.tokenizer.add_special_case", "language.tokenizer.add_special_case", "language.tokenizer.add_special_case", "language.tokenizer.add_special_case", "language.tokenizer.add_special_case", "language.tokenizer.add_special_case", "language.tokenizer.add_special_case", "language.tokenizer.add_special_case", "language.tokenizer.add_special_case", "language.tokenizer.add_special_case", "language.tokenizer.add_special_case", "language.tokenizer.add_special_case", "language.tokenizer.add_special_case", "language.tokenizer.add_special_case", "language.tokenizer.add_special_case", "language.tokenizer.add_special_case", "language.tokenizer.add_special_case", "language.tokenizer.add_special_case", "language.tokenizer.add_special_case", "language.tokenizer.add_special_case", "language.tokenizer.add_special_case", "language.tokenizer.add_special_case", "language.tokenizer.add_special_case", "language.tokenizer.add_special_case", "language.tokenizer.add_special_case", "language.tokenizer.add_special_case", "language.tokenizer.add_special_case", "language.tokenizer.add_special_case", "language.tokenizer.add_special_case", "language.tokenizer.add_special_case", "language.tokenizer.add_special_case", "language.tokenizer.add_special_case", "language.tokenizer.add_special_case", "language.tokenizer.add_special_case", "language.tokenizer.add_special_case", "language.tokenizer.add_special_case", "language.tokenizer.add_special_case", "language.tokenizer.add_special_case", "language.tokenizer.add_special_case", "language.tokenizer.add_special_case", "language.tokenizer.add_special_case", "language.tokenizer.add_special_case", "language.tokenizer.add_special_case", "language.tokenizer.add_special_case", "language.tokenizer.add_special_case", "language.tokenizer.add_special_case", "language.tokenizer.add_special_case", "language.tokenizer.add_special_case", "language.tokenizer.add_special_case", "language.tokenizer.add_special_case", "language.tokenizer.add_special_case", "spacy.util.compile_infix_regex", "spacy.util.compile_prefix_regex", "spacy.util.compile_suffix_regex", "tuple", "tuple", "tuple", "list", "list", "list"], "function", ["None"], ["", "def", "_add_tokenization_exceptions", "(", "language", ")", ":", "\n", "    ", "\"\"\"\n    Tons of tokenization exceptions for this dataset\n    :param language:\n    :return:\n    \"\"\"", "\n", "\n", "#N2C2 2019 and Share 2013 Concept Normalization", "\n", "language", ".", "tokenizer", ".", "add_special_case", "(", "'empiricvancomycin'", ",", "[", "{", "ORTH", ":", "\"empiric\"", "}", ",", "{", "ORTH", ":", "\"vancomycin\"", "}", "]", ")", "\n", "language", ".", "tokenizer", ".", "add_special_case", "(", "'dobutamine-MIBI'", ",", "[", "{", "ORTH", ":", "'dobutamine'", "}", ",", "{", "ORTH", ":", "'-'", "}", ",", "{", "ORTH", ":", "'MIBI'", "}", "]", ")", "\n", "language", ".", "tokenizer", ".", "add_special_case", "(", "'cLVH'", ",", "[", "{", "ORTH", ":", "'c'", "}", ",", "{", "ORTH", ":", "'LVH'", "}", "]", ")", "\n", "language", ".", "tokenizer", ".", "add_special_case", "(", "'UPEP/Beta'", ",", "[", "{", "ORTH", ":", "'UPEP'", "}", ",", "{", "ORTH", ":", "'/'", "}", ",", "{", "ORTH", ":", "'Beta'", "}", "]", ")", "\n", "language", ".", "tokenizer", ".", "add_special_case", "(", "'constipation-related'", ",", "[", "{", "ORTH", ":", "'constipation'", "}", ",", "{", "ORTH", ":", "'-'", "}", ",", "{", "ORTH", ":", "'related'", "}", "]", ")", "\n", "language", ".", "tokenizer", ".", "add_special_case", "(", "'anteriordysplasia'", ",", "[", "{", "ORTH", ":", "'anterior'", "}", ",", "{", "ORTH", ":", "'dysplasia'", "}", "]", ")", "\n", "language", ".", "tokenizer", ".", "add_special_case", "(", "'F.'", ",", "[", "{", "ORTH", ":", "'F'", "}", ",", "{", "ORTH", ":", "'.'", "}", "]", ")", "\n", "language", ".", "tokenizer", ".", "add_special_case", "(", "'extrapleural-pleural'", ",", "[", "{", "ORTH", ":", "'extrapleural'", "}", ",", "{", "ORTH", ":", "'-'", "}", ",", "{", "ORTH", ":", "'pleural'", "}", "]", ")", "\n", "language", ".", "tokenizer", ".", "add_special_case", "(", "'saphenous'", ",", "[", "{", "ORTH", ":", "'sap'", "}", ",", "{", "ORTH", ":", "'henous'", "}", "]", ")", "\n", "language", ".", "tokenizer", ".", "add_special_case", "(", "'T97.5'", ",", "[", "{", "ORTH", ":", "'T'", "}", ",", "{", "ORTH", ":", "'97.5'", "}", "]", ")", "\n", "language", ".", "tokenizer", ".", "add_special_case", "(", "'P59'", ",", "[", "{", "ORTH", ":", "'P'", "}", ",", "{", "ORTH", ":", "'59'", "}", "]", ")", "\n", "language", ".", "tokenizer", ".", "add_special_case", "(", "'RR20'", ",", "[", "{", "ORTH", ":", "'RR'", "}", ",", "{", "ORTH", ":", "'20'", "}", "]", ")", "\n", "language", ".", "tokenizer", ".", "add_special_case", "(", "'Demerolprn'", ",", "[", "{", "ORTH", ":", "'Demerol'", "}", ",", "{", "ORTH", ":", "'prn'", "}", "]", ")", "\n", "language", ".", "tokenizer", ".", "add_special_case", "(", "'Carboplatin-Taxolchemo'", ",", "[", "{", "ORTH", ":", "'Carboplatin'", "}", ",", "{", "ORTH", ":", "'-'", "}", ",", "{", "ORTH", ":", "'Taxol'", "}", ",", "{", "ORTH", ":", "'chemo'", "}", "]", ")", "\n", "language", ".", "tokenizer", ".", "add_special_case", "(", "'midepigastric'", ",", "[", "{", "ORTH", ":", "'mid'", "}", ",", "{", "ORTH", ":", "'epigastric'", "}", "]", ")", "\n", "language", ".", "tokenizer", ".", "add_special_case", "(", "'BRBPR/melena'", ",", "[", "{", "ORTH", ":", "'BRBPR'", "}", ",", "{", "ORTH", ":", "'/'", "}", ",", "{", "ORTH", ":", "'melena'", "}", "]", ")", "\n", "language", ".", "tokenizer", ".", "add_special_case", "(", "'CPAP+PS'", ",", "[", "{", "ORTH", ":", "'CPAP'", "}", ",", "{", "ORTH", ":", "'+'", "}", ",", "{", "ORTH", ":", "'PS'", "}", "]", ")", "\n", "language", ".", "tokenizer", ".", "add_special_case", "(", "'medications'", ",", "[", "{", "ORTH", ":", "'medication'", "}", ",", "{", "ORTH", ":", "'s'", "}", "]", ")", "\n", "language", ".", "tokenizer", ".", "add_special_case", "(", "'mass-'", ",", "[", "{", "ORTH", ":", "'mass'", "}", ",", "{", "ORTH", ":", "'-'", "}", "]", ")", "\n", "language", ".", "tokenizer", ".", "add_special_case", "(", "'1.ampullary'", ",", "[", "{", "ORTH", ":", "'1'", "}", ",", "{", "ORTH", ":", "'.'", "}", ",", "{", "ORTH", ":", "'ampullary'", "}", "]", ")", "\n", "language", ".", "tokenizer", ".", "add_special_case", "(", "'membranes'", ",", "[", "{", "ORTH", ":", "'membrane'", "}", ",", "{", "ORTH", ":", "'s'", "}", "]", ")", "\n", "language", ".", "tokenizer", ".", "add_special_case", "(", "'SOBx'", ",", "[", "{", "ORTH", ":", "'SOB'", "}", ",", "{", "ORTH", ":", "'x'", "}", "]", ")", "\n", "language", ".", "tokenizer", ".", "add_special_case", "(", "'Mass.'", ",", "[", "{", "ORTH", ":", "'Mass'", "}", ",", "{", "ORTH", ":", "'.'", "}", "]", ")", "\n", "language", ".", "tokenizer", ".", "add_special_case", "(", "'Atroventnebulizer'", ",", "[", "{", "ORTH", ":", "'Atrovent'", "}", ",", "{", "ORTH", ":", "'nebulizer'", "}", "]", ")", "\n", "language", ".", "tokenizer", ".", "add_special_case", "(", "'PCO227'", ",", "[", "{", "ORTH", ":", "'PC02'", "}", ",", "{", "ORTH", ":", "'27'", "}", "]", ")", "\n", "language", ".", "tokenizer", ".", "add_special_case", "(", "'PCO227'", ",", "[", "{", "ORTH", ":", "'PC02'", "}", ",", "{", "ORTH", ":", "'27'", "}", "]", ")", "\n", "language", ".", "tokenizer", ".", "add_special_case", "(", "'MB&apos;s'", ",", "[", "{", "ORTH", ":", "'MB'", "}", ",", "{", "ORTH", ":", "'&apos;s'", "}", "]", ")", "\n", "language", ".", "tokenizer", ".", "add_special_case", "(", "'Q&apos;s'", ",", "[", "{", "ORTH", ":", "'Q'", "}", ",", "{", "ORTH", ":", "'&apos;s'", "}", "]", ")", "\n", "language", ".", "tokenizer", ".", "add_special_case", "(", "'predischarge'", ",", "[", "{", "ORTH", ":", "'pre'", "}", ",", "{", "ORTH", ":", "'discharge'", "}", "]", ")", "\n", "language", ".", "tokenizer", ".", "add_special_case", "(", "'1.  Diabetes mellitus type 2.'", ",", "[", "{", "ORTH", ":", "'1.  '", "}", ",", "{", "ORTH", ":", "'Diabetes mellitus type 2'", "}", ",", "{", "ORTH", ":", "'.'", "}", "]", ")", "\n", "\n", "#N2C2 2018 NER", "\n", "language", ".", "tokenizer", ".", "add_special_case", "(", "'ons'", ",", "[", "{", "ORTH", ":", "'on'", "}", ",", "{", "ORTH", ":", "'s'", "}", "]", ")", "\n", "language", ".", "tokenizer", ".", "add_special_case", "(", "'DAILY16'", ",", "[", "{", "ORTH", ":", "'DAILY'", "}", ",", "{", "ORTH", ":", "'16'", "}", "]", ")", "\n", "language", ".", "tokenizer", ".", "add_special_case", "(", "'2uRBCs,'", ",", "[", "{", "ORTH", ":", "'2u'", "}", ",", "{", "ORTH", ":", "'RBCs'", "}", ",", "{", "ORTH", ":", "','", "}", "]", ")", "\n", "language", ".", "tokenizer", ".", "add_special_case", "(", "'1.amlodipine'", ",", "[", "{", "ORTH", ":", "'1'", "}", ",", "{", "ORTH", ":", "'.'", "}", ",", "{", "ORTH", ":", "'amlodipine'", "}", "]", ")", "\n", "language", ".", "tokenizer", ".", "add_special_case", "(", "'2.fexofenadine'", ",", "[", "{", "ORTH", ":", "'2'", "}", ",", "{", "ORTH", ":", "'.'", "}", ",", "{", "ORTH", ":", "'fexofenadine'", "}", "]", ")", "\n", "language", ".", "tokenizer", ".", "add_special_case", "(", "'3.levothyroxine'", ",", "[", "{", "ORTH", ":", "'3'", "}", ",", "{", "ORTH", ":", "'.'", "}", ",", "{", "ORTH", ":", "'levothyroxine'", "}", "]", ")", "\n", "language", ".", "tokenizer", ".", "add_special_case", "(", "'4.omeprazole'", ",", "[", "{", "ORTH", ":", "'4'", "}", ",", "{", "ORTH", ":", "'.'", "}", ",", "{", "ORTH", ":", "'omeprazole'", "}", "]", ")", "\n", "language", ".", "tokenizer", ".", "add_special_case", "(", "'5.multivitamin'", ",", "[", "{", "ORTH", ":", "'5'", "}", ",", "{", "ORTH", ":", "'.'", "}", ",", "{", "ORTH", ":", "'multivitamin'", "}", "]", ")", "\n", "language", ".", "tokenizer", ".", "add_special_case", "(", "'6.tiotropium'", ",", "[", "{", "ORTH", ":", "'6'", "}", ",", "{", "ORTH", ":", "'.'", "}", ",", "{", "ORTH", ":", "'tiotropium'", "}", "]", ")", "\n", "language", ".", "tokenizer", ".", "add_special_case", "(", "'7.atorvastatin'", ",", "[", "{", "ORTH", ":", "'7'", "}", ",", "{", "ORTH", ":", "'.'", "}", ",", "{", "ORTH", ":", "'atorvastatin'", "}", "]", ")", "\n", "language", ".", "tokenizer", ".", "add_special_case", "(", "'8.docusate'", ",", "[", "{", "ORTH", ":", "'8'", "}", ",", "{", "ORTH", ":", "'.'", "}", ",", "{", "ORTH", ":", "'docusate'", "}", "]", ")", "\n", "language", ".", "tokenizer", ".", "add_special_case", "(", "'9.dofetilide'", ",", "[", "{", "ORTH", ":", "'9'", "}", ",", "{", "ORTH", ":", "'.'", "}", ",", "{", "ORTH", ":", "'dofetilide'", "}", "]", ")", "\n", "language", ".", "tokenizer", ".", "add_special_case", "(", "'10.albuterol'", ",", "[", "{", "ORTH", ":", "'10'", "}", ",", "{", "ORTH", ":", "'.'", "}", ",", "{", "ORTH", ":", "'albuterol'", "}", "]", ")", "\n", "language", ".", "tokenizer", ".", "add_special_case", "(", "'11.cholecalciferol'", ",", "[", "{", "ORTH", ":", "'11'", "}", ",", "{", "ORTH", ":", "'.'", "}", ",", "{", "ORTH", ":", "'cholecalciferol'", "}", "]", ")", "\n", "language", ".", "tokenizer", ".", "add_special_case", "(", "'12.fluticasone'", ",", "[", "{", "ORTH", ":", "'12'", "}", ",", "{", "ORTH", ":", "'.'", "}", ",", "{", "ORTH", ":", "'fluticasone'", "}", "]", ")", "\n", "language", ".", "tokenizer", ".", "add_special_case", "(", "'13.morphine'", ",", "[", "{", "ORTH", ":", "'13'", "}", ",", "{", "ORTH", ":", "'.'", "}", ",", "{", "ORTH", ":", "'morphine'", "}", "]", ")", "\n", "language", ".", "tokenizer", ".", "add_special_case", "(", "'14.morphine'", ",", "[", "{", "ORTH", ":", "'14'", "}", ",", "{", "ORTH", ":", "'.'", "}", ",", "{", "ORTH", ":", "'morphine'", "}", "]", ")", "\n", "language", ".", "tokenizer", ".", "add_special_case", "(", "'15.calcium'", ",", "[", "{", "ORTH", ":", "'15'", "}", ",", "{", "ORTH", ":", "'.'", "}", ",", "{", "ORTH", ":", "'calcium'", "}", "]", ")", "\n", "language", ".", "tokenizer", ".", "add_special_case", "(", "'16.warfarin'", ",", "[", "{", "ORTH", ":", "'16'", "}", ",", "{", "ORTH", ":", "'.'", "}", ",", "{", "ORTH", ":", "'warfarin'", "}", "]", ")", "\n", "language", ".", "tokenizer", ".", "add_special_case", "(", "'17.warfarin'", ",", "[", "{", "ORTH", ":", "'17'", "}", ",", "{", "ORTH", ":", "'.'", "}", ",", "{", "ORTH", ":", "'warfarin'", "}", "]", ")", "\n", "language", ".", "tokenizer", ".", "add_special_case", "(", "'18.Epogen'", ",", "[", "{", "ORTH", ":", "'18'", "}", ",", "{", "ORTH", ":", "'.'", "}", ",", "{", "ORTH", ":", "'Epogen'", "}", "]", ")", "\n", "language", ".", "tokenizer", ".", "add_special_case", "(", "'19.guaifenesin'", ",", "[", "{", "ORTH", ":", "'19'", "}", ",", "{", "ORTH", ":", "'.'", "}", ",", "{", "ORTH", ":", "'guaifenesin'", "}", "]", ")", "\n", "language", ".", "tokenizer", ".", "add_special_case", "(", "'20.bumetanide'", ",", "[", "{", "ORTH", ":", "'20'", "}", ",", "{", "ORTH", ":", "'.'", "}", ",", "{", "ORTH", ":", "'bumetanide'", "}", "]", ")", "\n", "language", ".", "tokenizer", ".", "add_special_case", "(", "'21.prednisone'", ",", "[", "{", "ORTH", ":", "'21'", "}", ",", "{", "ORTH", ":", "'.'", "}", ",", "{", "ORTH", ":", "'prednisone'", "}", "]", ")", "\n", "language", ".", "tokenizer", ".", "add_special_case", "(", "'22.ferrous'", ",", "[", "{", "ORTH", ":", "'22'", "}", ",", "{", "ORTH", ":", "'.'", "}", ",", "{", "ORTH", ":", "'ferrous'", "}", "]", ")", "\n", "language", ".", "tokenizer", ".", "add_special_case", "(", "'23.spironolactone'", ",", "[", "{", "ORTH", ":", "'23'", "}", ",", "{", "ORTH", ":", "'.'", "}", ",", "{", "ORTH", ":", "'spironolactone'", "}", "]", ")", "\n", "language", ".", "tokenizer", ".", "add_special_case", "(", "'1.lasix'", ",", "[", "{", "ORTH", ":", "'1'", "}", ",", "{", "ORTH", ":", "'.'", "}", ",", "{", "ORTH", ":", "'lasix'", "}", "]", ")", "\n", "language", ".", "tokenizer", ".", "add_special_case", "(", "'6.lasix'", ",", "[", "{", "ORTH", ":", "'6'", "}", ",", "{", "ORTH", ":", "'.'", "}", ",", "{", "ORTH", ":", "'lasix'", "}", "]", ")", "\n", "language", ".", "tokenizer", ".", "add_special_case", "(", "'10.citalopram'", ",", "[", "{", "ORTH", ":", "'10'", "}", ",", "{", "ORTH", ":", "'.'", "}", ",", "{", "ORTH", ":", "'citalopram'", "}", "]", ")", "\n", "language", ".", "tokenizer", ".", "add_special_case", "(", "'2.haloperidol'", ",", "[", "{", "ORTH", ":", "'2'", "}", ",", "{", "ORTH", ":", "'.'", "}", ",", "{", "ORTH", ":", "'haloperidol'", "}", "]", ")", "\n", "language", ".", "tokenizer", ".", "add_special_case", "(", "'4.tiotropium'", ",", "[", "{", "ORTH", ":", "'4'", "}", ",", "{", "ORTH", ":", "'.'", "}", ",", "{", "ORTH", ":", "'tiotropium'", "}", "]", ")", "\n", "language", ".", "tokenizer", ".", "add_special_case", "(", "'8.omeprazole'", ",", "[", "{", "ORTH", ":", "'8'", "}", ",", "{", "ORTH", ":", "'.'", "}", ",", "{", "ORTH", ":", "'omeprazole'", "}", "]", ")", "\n", "language", ".", "tokenizer", ".", "add_special_case", "(", "'3.tamsulosin'", ",", "[", "{", "ORTH", ":", "'3'", "}", ",", "{", "ORTH", ":", "'.'", "}", ",", "{", "ORTH", ":", "'tamsulosin'", "}", "]", ")", "\n", "#language.tokenizer.add_special_case('.atorvastatin', [{ORTH:'.'},{ORTH: 'atorvastatin'}])", "\n", "language", ".", "tokenizer", ".", "add_special_case", "(", "'5.atorvastatin'", ",", "[", "{", "ORTH", ":", "'5'", "}", ",", "{", "ORTH", ":", "'.'", "}", ",", "{", "ORTH", ":", "'atorvastatin'", "}", "]", ")", "\n", "language", ".", "tokenizer", ".", "add_special_case", "(", "'9.aspirin'", ",", "[", "{", "ORTH", ":", "'5'", "}", ",", "{", "ORTH", ":", "'.'", "}", ",", "{", "ORTH", ":", "'atorvastatin'", "}", "]", ")", "\n", "language", ".", "tokenizer", ".", "add_special_case", "(", "'10.citalopram'", ",", "[", "{", "ORTH", ":", "'10'", "}", ",", "{", "ORTH", ":", "'.'", "}", ",", "{", "ORTH", ":", "'citalopram'", "}", "]", ")", "\n", "language", ".", "tokenizer", ".", "add_special_case", "(", "'1.fluticasone-salmeterol'", ",", "[", "{", "ORTH", ":", "'1'", "}", ",", "{", "ORTH", ":", "'.'", "}", ",", "{", "ORTH", ":", "'fluticasone'", "}", ",", "{", "ORTH", ":", "'-'", "}", ",", "{", "ORTH", ":", "'salmeterol'", "}", "]", ")", "\n", "language", ".", "tokenizer", ".", "add_special_case", "(", "'6.lisinopril'", ",", "[", "{", "ORTH", ":", "'6'", "}", ",", "{", "ORTH", ":", "'.'", "}", ",", "{", "ORTH", ":", "'lisinopril'", "}", "]", ")", "\n", "language", ".", "tokenizer", ".", "add_special_case", "(", "'7.senna'", ",", "[", "{", "ORTH", ":", "'7'", "}", ",", "{", "ORTH", ":", "'.'", "}", ",", "{", "ORTH", ":", "'senna'", "}", "]", ")", "\n", "language", ".", "tokenizer", ".", "add_special_case", "(", "'hours).'", ",", "[", "{", "ORTH", ":", "'hours'", "}", ",", "{", "ORTH", ":", "')'", "}", ",", "{", "ORTH", ":", "'.'", "}", "]", ")", "\n", "language", ".", "tokenizer", ".", "add_special_case", "(", "'.Talon'", ",", "[", "{", "ORTH", ":", "'.'", "}", ",", "{", "ORTH", ":", "'Talon'", "}", "]", ")", "\n", "\n", "language", ".", "tokenizer", ".", "add_special_case", "(", "'RR<'", ",", "[", "{", "ORTH", ":", "'RR'", "}", ",", "{", "ORTH", ":", "'<'", "}", "]", ")", "\n", "language", ".", "tokenizer", ".", "add_special_case", "(", "'(2'", ",", "[", "{", "ORTH", ":", "'('", "}", ",", "{", "ORTH", ":", "'2'", "}", "]", ")", "\n", "language", ".", "tokenizer", ".", "add_special_case", "(", "'IDDM:'", ",", "[", "{", "ORTH", ":", "'ID'", "}", ",", "{", "ORTH", ":", "'DM'", "}", ",", "{", "ORTH", ":", "':'", "}", "]", ")", "\n", "language", ".", "tokenizer", ".", "add_special_case", "(", "'@HS,tramadol'", ",", "[", "{", "ORTH", ":", "'@'", "}", ",", "{", "ORTH", ":", "'HS'", "}", ",", "{", "ORTH", ":", "','", "}", ",", "{", "ORTH", ":", "'tramadol'", "}", "]", ")", "\n", "language", ".", "tokenizer", ".", "add_special_case", "(", "'1-2Lnc'", ",", "[", "{", "ORTH", ":", "'1-2L'", "}", ",", "{", "ORTH", ":", "'nc'", "}", "]", ")", "\n", "language", ".", "tokenizer", ".", "add_special_case", "(", "'withantibiotic'", ",", "[", "{", "ORTH", ":", "'with'", "}", ",", "{", "ORTH", ":", "'antibiotic'", "}", "]", ")", "\n", "\n", "language", ".", "tokenizer", ".", "add_special_case", "(", "'startingKeppra,'", ",", "[", "{", "ORTH", ":", "'starting'", "}", ",", "{", "ORTH", ":", "'Keppra'", "}", "]", ")", "\n", "language", ".", "tokenizer", ".", "add_special_case", "(", "'Warfarin5'", ",", "[", "{", "ORTH", ":", "'Warfarin'", "}", ",", "{", "ORTH", ":", "'5'", "}", "]", ")", "\n", "language", ".", "tokenizer", ".", "add_special_case", "(", "'IDDM'", ",", "[", "{", "ORTH", ":", "'I'", "}", ",", "{", "ORTH", ":", "'DDM'", "}", "]", ")", "\n", "language", ".", "tokenizer", ".", "add_special_case", "(", "'1u'", ",", "[", "{", "ORTH", ":", "'1'", "}", ",", "{", "ORTH", ":", "'u'", "}", "]", ")", "\n", "language", ".", "tokenizer", ".", "add_special_case", "(", "'6U'", ",", "[", "{", "ORTH", ":", "'6'", "}", ",", "{", "ORTH", ":", "'U'", "}", "]", ")", "\n", "language", ".", "tokenizer", ".", "add_special_case", "(", "'HSQ'", ",", "[", "{", "ORTH", ":", "'H'", "}", ",", "{", "ORTH", ":", "'SQ'", "}", "]", ")", "\n", "language", ".", "tokenizer", ".", "add_special_case", "(", "'GD20'", ",", "[", "{", "ORTH", ":", "'GD'", "}", ",", "{", "ORTH", ":", "'20'", "}", "]", ")", "\n", "language", ".", "tokenizer", ".", "add_special_case", "(", "'FAFA'", ",", "[", "{", "ORTH", ":", "'FA'", "}", ",", "{", "ORTH", ":", "'FA'", "}", "]", ")", "\n", "language", ".", "tokenizer", ".", "add_special_case", "(", "'FACB'", ",", "[", "{", "ORTH", ":", "'FA'", "}", ",", "{", "ORTH", ":", "'CB'", "}", "]", ")", "\n", "language", ".", "tokenizer", ".", "add_special_case", "(", "'O3CB'", ",", "[", "{", "ORTH", ":", "'O3'", "}", ",", "{", "ORTH", ":", "'CB'", "}", "]", ")", "\n", "language", ".", "tokenizer", ".", "add_special_case", "(", "'O3FA'", ",", "[", "{", "ORTH", ":", "'03'", "}", ",", "{", "ORTH", ":", "'FA'", "}", "]", ")", "\n", "language", ".", "tokenizer", ".", "add_special_case", "(", "'PND5'", ",", "[", "{", "ORTH", ":", "'PND'", "}", ",", "{", "ORTH", ":", "'5'", "}", "]", ")", "\n", "language", ".", "tokenizer", ".", "add_special_case", "(", "'PND60:'", ",", "[", "{", "ORTH", ":", "'PND'", "}", ",", "{", "ORTH", ":", "'60'", "}", ",", "{", "ORTH", ":", "':'", "}", "]", ")", "\n", "language", ".", "tokenizer", ".", "add_special_case", "(", "'mice/treatment)'", ",", "[", "{", "ORTH", ":", "'mice'", "}", ",", "{", "ORTH", ":", "'/'", "}", ",", "{", "ORTH", ":", "'treatment'", "}", ",", "{", "ORTH", ":", "')'", "}", "]", ")", "\n", "\n", "\n", "#TAC 2018", "\n", "language", ".", "tokenizer", ".", "add_special_case", "(", "'Kunmingmouse'", ",", "[", "{", "ORTH", ":", "'Kunming'", "}", ",", "{", "ORTH", ":", "'mouse'", "}", "]", ")", "\n", "language", ".", "tokenizer", ".", "add_special_case", "(", "'24h'", ",", "[", "{", "ORTH", ":", "'24'", "}", ",", "{", "ORTH", ":", "'h'", "}", "]", ")", "\n", "language", ".", "tokenizer", ".", "add_special_case", "(", "'72h'", ",", "[", "{", "ORTH", ":", "'72'", "}", ",", "{", "ORTH", ":", "'h'", "}", "]", ")", "\n", "language", ".", "tokenizer", ".", "add_special_case", "(", "'[15N5]8-oxodG'", ",", "[", "{", "ORTH", ":", "'[15N5]'", "}", ",", "{", "ORTH", ":", "'8-oxodG'", "}", "]", ")", "\n", "language", ".", "tokenizer", ".", "add_special_case", "(", "'ratswerepermitted'", ",", "[", "{", "ORTH", ":", "'rats'", "}", ",", "{", "ORTH", ":", "'were'", "}", ",", "{", "ORTH", ":", "'permitted'", "}", "]", ")", "\n", "language", ".", "tokenizer", ".", "add_special_case", "(", "'mgTi'", ",", "[", "{", "ORTH", ":", "'mg'", "}", ",", "{", "ORTH", ":", "'Ti'", "}", "]", ")", "\n", "language", ".", "tokenizer", ".", "add_special_case", "(", "'ND60'", ",", "[", "{", "ORTH", ":", "'ND'", "}", ",", "{", "ORTH", ":", "'60'", "}", "]", ")", "\n", "# language.tokenizer.add_special_case('PND30\u201335', [{ORTH: 'PND'}, {ORTH: '30\u201335'}])", "\n", "language", ".", "tokenizer", ".", "add_special_case", "(", "'198Au'", ",", "[", "{", "ORTH", ":", "'198'", "}", ",", "{", "ORTH", ":", "'Au'", "}", "]", ")", "\n", "language", ".", "tokenizer", ".", "add_special_case", "(", "'8weeks,'", ",", "[", "{", "ORTH", ":", "'8'", "}", ",", "{", "ORTH", ":", "'weeks'", "}", ",", "{", "ORTH", ":", "','", "}", "]", ")", "\n", "language", ".", "tokenizer", ".", "add_special_case", "(", "'weeks:55'", ",", "[", "{", "ORTH", ":", "'weeks'", "}", ",", "{", "ORTH", ":", "':'", "}", ",", "{", "ORTH", ":", "'55'", "}", "]", ")", "\n", "language", ".", "tokenizer", ".", "add_special_case", "(", "'ininfected'", ",", "[", "{", "ORTH", ":", "'in'", "}", ",", "{", "ORTH", ":", "'infected'", "}", "]", ")", "\n", "language", ".", "tokenizer", ".", "add_special_case", "(", "'15days.'", ",", "[", "{", "ORTH", ":", "'15'", "}", ",", "{", "ORTH", ":", "'days'", "}", ",", "{", "ORTH", ":", "'.'", "}", "]", ")", "\n", "language", ".", "tokenizer", ".", "add_special_case", "(", "'GD18'", ",", "[", "{", "ORTH", ":", "'GD'", "}", ",", "{", "ORTH", ":", "'18'", "}", "]", ")", "\n", "language", ".", "tokenizer", ".", "add_special_case", "(", "'day).'", ",", "[", "{", "ORTH", ":", "'day'", "}", ",", "{", "ORTH", ":", "')'", "}", ",", "{", "ORTH", ":", "'.'", "}", "]", ")", "\n", "language", ".", "tokenizer", ".", "add_special_case", "(", "'x11days).'", ",", "[", "{", "ORTH", ":", "'x11'", "}", ",", "{", "ORTH", ":", "'days'", "}", ",", "{", "ORTH", ":", "')'", "}", ",", "{", "ORTH", ":", "'.'", "}", "]", ")", "\n", "language", ".", "tokenizer", ".", "add_special_case", "(", "'4.5hours'", ",", "[", "{", "ORTH", ":", "'4.5'", "}", ",", "{", "ORTH", ":", "'hours'", "}", "]", ")", "\n", "language", ".", "tokenizer", ".", "add_special_case", "(", "'0.5mg'", ",", "[", "{", "ORTH", ":", "'0.5'", "}", ",", "{", "ORTH", ":", "'mg'", "}", "]", ")", "\n", "\n", "\n", "#N2C2 2010", "\n", "language", ".", "tokenizer", ".", "add_special_case", "(", "'periprosthetic'", ",", "[", "{", "ORTH", ":", "'peri'", "}", ",", "{", "ORTH", ":", "'prosthetic'", "}", "]", ")", "\n", "language", ".", "tokenizer", ".", "add_special_case", "(", "'MIER'", ",", "[", "{", "ORTH", ":", "'MI'", "}", ",", "{", "ORTH", ":", "'ER'", "}", "]", ")", "\n", "\n", "#END 2017", "\n", "language", ".", "tokenizer", ".", "add_special_case", "(", "'PeripheralPeripheral'", ",", "[", "{", "ORTH", ":", "'Peripheral'", "}", ",", "{", "ORTH", ":", "'Peripheral'", "}", "]", ")", "\n", "language", ".", "tokenizer", ".", "add_special_case", "(", "'SeriousSerious'", ",", "[", "{", "ORTH", ":", "'Serious'", "}", ",", "{", "ORTH", ":", "'Serious'", "}", "]", ")", "\n", "language", ".", "tokenizer", ".", "add_special_case", "(", "'ADC-CD30'", ",", "[", "{", "ORTH", ":", "'ADC-CD'", "}", ",", "{", "ORTH", ":", "'30'", "}", "]", ")", "\n", "language", ".", "tokenizer", ".", "add_special_case", "(", "'MCC-DM1'", ",", "[", "{", "ORTH", ":", "'MCC-DM'", "}", ",", "{", "ORTH", ":", "'1'", "}", "]", ")", "\n", "language", ".", "tokenizer", ".", "add_special_case", "(", "'syndrome[see'", ",", "[", "{", "ORTH", ":", "'syndrome'", "}", ",", "{", "ORTH", ":", "'['", "}", ",", "{", "ORTH", ":", "'see'", "}", "]", ")", "\n", "language", ".", "tokenizer", ".", "add_special_case", "(", "'5.1Anaphylaxis'", ",", "[", "{", "ORTH", ":", "'5.1'", "}", ",", "{", "ORTH", ":", "'Anaphylaxis'", "}", "]", ")", "\n", "language", ".", "tokenizer", ".", "add_special_case", "(", "'HIGHLIGHTSPEGINTRON'", ",", "[", "{", "ORTH", ":", "'HIGHLIGHTS'", "}", ",", "{", "ORTH", ":", "'PEGINTRON'", "}", "]", ")", "\n", "language", ".", "tokenizer", ".", "add_special_case", "(", "'HIGHLIGHTSRibavirin'", ",", "[", "{", "ORTH", ":", "'HIGHLIGHTS'", "}", ",", "{", "ORTH", ":", "'Ribavirin'", "}", "]", ")", "\n", "language", ".", "tokenizer", ".", "add_special_case", "(", "'COPEGUS[see'", ",", "[", "{", "ORTH", ":", "'COPEGUS'", "}", ",", "{", "ORTH", ":", "'[see'", "}", "]", ")", "\n", "\n", "\n", "\n", "#I2B2 2014", "\n", "language", ".", "tokenizer", ".", "add_special_case", "(", "'FAT'", ",", "[", "{", "ORTH", ":", "'F'", "}", ",", "{", "ORTH", ":", "'A'", "}", ",", "{", "ORTH", ":", "'T'", "}", "]", ")", "\n", "language", ".", "tokenizer", ".", "add_special_case", "(", "'TTS'", ",", "[", "{", "ORTH", ":", "'T'", "}", ",", "{", "ORTH", ":", "'T'", "}", ",", "{", "ORTH", ":", "'S'", "}", "]", ")", "\n", "language", ".", "tokenizer", ".", "add_special_case", "(", "'STTh'", ",", "[", "{", "ORTH", ":", "'S'", "}", ",", "{", "ORTH", ":", "'T'", "}", ",", "{", "ORTH", ":", "'Th'", "}", "]", ")", "\n", "language", ".", "tokenizer", ".", "add_special_case", "(", "'TThSa'", ",", "[", "{", "ORTH", ":", "'T'", "}", ",", "{", "ORTH", ":", "'h'", "}", ",", "{", "ORTH", ":", "'Sa'", "}", "]", ")", "\n", "language", ".", "tokenizer", ".", "add_special_case", "(", "'MWFS'", ",", "[", "{", "ORTH", ":", "'M'", "}", ",", "{", "ORTH", ":", "'W'", "}", ",", "{", "ORTH", ":", "'F'", "}", ",", "{", "ORTH", ":", "'S'", "}", "]", ")", "\n", "language", ".", "tokenizer", ".", "add_special_case", "(", "'MWF'", ",", "[", "{", "ORTH", ":", "'M'", "}", ",", "{", "ORTH", ":", "'W'", "}", ",", "{", "ORTH", ":", "'F'", "}", "]", ")", "\n", "language", ".", "tokenizer", ".", "add_special_case", "(", "'ThisRoberta'", ",", "[", "{", "ORTH", ":", "'This'", "}", ",", "{", "ORTH", ":", "'Roberta'", "}", "]", ")", "\n", "language", ".", "tokenizer", ".", "add_special_case", "(", "'GambiaHome'", ",", "[", "{", "ORTH", ":", "'Gambia'", "}", ",", "{", "ORTH", ":", "'Home'", "}", "]", ")", "\n", "language", ".", "tokenizer", ".", "add_special_case", "(", "'SupervisorSupport'", ",", "[", "{", "ORTH", ":", "'Supervisor'", "}", ",", "{", "ORTH", ":", "'Support'", "}", "]", ")", "\n", "language", ".", "tokenizer", ".", "add_special_case", "(", "'inhartsville'", ",", "[", "{", "ORTH", ":", "'in'", "}", ",", "{", "ORTH", ":", "'hartsville'", "}", "]", ")", "\n", "language", ".", "tokenizer", ".", "add_special_case", "(", "'ELLENMRN:'", ",", "[", "{", "ORTH", ":", "'ELLEN'", "}", ",", "{", "ORTH", ":", "'MRN:'", "}", "]", ")", "\n", "language", ".", "tokenizer", ".", "add_special_case", "(", "'0.411/29/2088'", ",", "[", "{", "ORTH", ":", "'0.4'", "}", ",", "{", "ORTH", ":", "'11/29/2088'", "}", "]", ")", "\n", "language", ".", "tokenizer", ".", "add_special_case", "(", "'past11/29/2088'", ",", "[", "{", "ORTH", ":", "'past'", "}", ",", "{", "ORTH", ":", "'11/29/2088'", "}", "]", ")", "\n", "language", ".", "tokenizer", ".", "add_special_case", "(", "'Hospital0021'", ",", "[", "{", "ORTH", ":", "'Hospital'", "}", ",", "{", "ORTH", ":", "'0021'", "}", "]", ")", "\n", "language", ".", "tokenizer", ".", "add_special_case", "(", "'HospitalAdmission'", ",", "[", "{", "ORTH", ":", "'Hospital'", "}", ",", "{", "ORTH", ":", "'Admission'", "}", "]", ")", "\n", "language", ".", "tokenizer", ".", "add_special_case", "(", "'AvenueKigali,'", ",", "[", "{", "ORTH", ":", "'Avenue'", "}", ",", "{", "ORTH", ":", "'Kigali,'", "}", "]", ")", "\n", "language", ".", "tokenizer", ".", "add_special_case", "(", "'47798497-045-1949'", ",", "[", "{", "ORTH", ":", "'47798'", "}", ",", "{", "ORTH", ":", "'497-045-1949'", "}", "]", ")", "\n", "language", ".", "tokenizer", ".", "add_special_case", "(", "'.02/23/2077:'", ",", "[", "{", "ORTH", ":", "'.'", "}", ",", "{", "ORTH", ":", "'02/23/2077:'", "}", "]", ")", "\n", "language", ".", "tokenizer", ".", "add_special_case", "(", "'34712RadiologyExam'", ",", "[", "{", "ORTH", ":", "'34712'", "}", ",", "{", "ORTH", ":", "'RadiologyExam'", "}", "]", ")", "\n", "language", ".", "tokenizer", ".", "add_special_case", "(", "'3041038MARY'", ",", "[", "{", "ORTH", ":", "'3041038'", "}", ",", "{", "ORTH", ":", "'MARY'", "}", "]", ")", "\n", "language", ".", "tokenizer", ".", "add_special_case", "(", "'PLAN88F'", ",", "[", "{", "ORTH", ":", "'PLAN'", "}", ",", "{", "ORTH", ":", "'88'", "}", ",", "{", "ORTH", ":", "'F'", "}", "]", ")", "\n", "language", ".", "tokenizer", ".", "add_special_case", "(", "'~2112Hypothyroidism'", ",", "[", "{", "ORTH", ":", "'~'", "}", ",", "{", "ORTH", ":", "'2112'", "}", ",", "{", "ORTH", ":", "'Hypothyroidism'", "}", "]", ")", "\n", "language", ".", "tokenizer", ".", "add_special_case", "(", "'97198841PGH'", ",", "[", "{", "ORTH", ":", "'97198841'", "}", ",", "{", "ORTH", ":", "'PGH'", "}", "]", ")", "\n", "language", ".", "tokenizer", ".", "add_special_case", "(", "'5694653MEDIQUIK'", ",", "[", "{", "ORTH", ":", "'5694653'", "}", ",", "{", "ORTH", ":", "'MEDIQUIK'", "}", "]", ")", "\n", "language", ".", "tokenizer", ".", "add_special_case", "(", "'0083716SNH'", ",", "[", "{", "ORTH", ":", "'0083716'", "}", ",", "{", "ORTH", ":", "'SNH'", "}", "]", ")", "\n", "language", ".", "tokenizer", ".", "add_special_case", "(", "'20626842267'", ",", "[", "{", "ORTH", ":", "'2062'", "}", ",", "{", "ORTH", ":", "'6842267'", "}", "]", ")", "\n", "language", ".", "tokenizer", ".", "add_special_case", "(", "'0370149RSC'", ",", "[", "{", "ORTH", ":", "'0370149'", "}", ",", "{", "ORTH", ":", "'RSC'", "}", "]", ")", "\n", "language", ".", "tokenizer", ".", "add_special_case", "(", "'4832978HOB'", ",", "[", "{", "ORTH", ":", "'4832978'", "}", ",", "{", "ORTH", ":", "'HOB'", "}", "]", ")", "\n", "language", ".", "tokenizer", ".", "add_special_case", "(", "'0907307PCC'", ",", "[", "{", "ORTH", ":", "'0907307'", "}", ",", "{", "ORTH", ":", "'PCC'", "}", "]", ")", "\n", "language", ".", "tokenizer", ".", "add_special_case", "(", "'LittletonColonoscopy'", ",", "[", "{", "ORTH", ":", "'Littleton'", "}", ",", "{", "ORTH", ":", "'Colonoscopy'", "}", "]", ")", "\n", "language", ".", "tokenizer", ".", "add_special_case", "(", "'34674TSH'", ",", "[", "{", "ORTH", ":", "'34674'", "}", ",", "{", "ORTH", ":", "'TSH'", "}", "]", ")", "\n", "language", ".", "tokenizer", ".", "add_special_case", "(", "'b93D'", ",", "[", "{", "ORTH", ":", "'b'", "}", ",", "{", "ORTH", ":", "'93'", "}", ",", "{", "ORTH", ":", "'D'", "}", "]", ")", "\n", "language", ".", "tokenizer", ".", "add_special_case", "(", "'due22D'", ",", "[", "{", "ORTH", ":", "'due'", "}", ",", "{", "ORTH", ":", "'22'", "}", ",", "{", "ORTH", ":", "'D'", "}", "]", ")", "\n", "language", ".", "tokenizer", ".", "add_special_case", "(", "'33182William'", ",", "[", "{", "ORTH", ":", "'33182'", "}", ",", "{", "ORTH", ":", "'William'", "}", "]", ")", "\n", "\n", "#French 2014 NER", "\n", "language", ".", "tokenizer", ".", "add_special_case", "(", "'postop\u00e9ratoires'", ",", "[", "{", "ORTH", ":", "'post'", "}", ",", "{", "ORTH", ":", "'op\u00e9ratoires'", "}", "]", ")", "\n", "\n", "\n", "custom_infixes", "=", "[", "r'\\d+\\.\\d+'", ",", "'[P|p]RBCs?'", ",", "'cap|CAP'", ",", "'qhs|QHS|mg'", ",", "'tab|TAB|BPA'", ",", "'BB'", ",", "'yo'", ",", "'ASA'", ",", "'gtt|GTT'", ",", "'iv|IV'", ",", "'FFP'", ",", "\n", "'inh|INH'", ",", "'pf|PF'", ",", "'bid|BID|PND'", ",", "'prn|PRN'", ",", "'puffs?'", ",", "r'\\dL'", ",", "\n", "'QD|qd'", ",", "'Q?(AM|PM)'", ",", "'O2'", ",", "'MWF'", ",", "r'q\\d+'", ",", "'HS'", ",", "'ye423zc'", ",", "\n", "'-|;|:|\u2013|#|<|{|}'", ",", "r'\\-'", "]", "+", "[", "r\"\\\\\"", ",", "\"/\"", ",", "\"%\"", ",", "r\"\\+\"", ",", "r\"\\,\"", ",", "r\"\\(\"", ",", "r\"\\)\"", ",", "r\"\\.\"", ",", "r\"\\d\\d/\\d\\d/\\d\\d\\d\\d\"", ",", "r\"\\d\\d\\d\\d\\d\\d\\d\"", ",", "r\"\\^\"", "]", "\n", "language", ".", "tokenizer", ".", "infix_finditer", "=", "compile_infix_regex", "(", "tuple", "(", "list", "(", "language", ".", "Defaults", ".", "infixes", ")", "+", "custom_infixes", ")", ")", ".", "finditer", "\n", "#language.tokenizer.infix_finditer = compile_infix_regex(custom_infixes).finditer", "\n", "language", ".", "tokenizer", ".", "prefix_search", "=", "compile_prefix_regex", "(", "tuple", "(", "list", "(", "language", ".", "Defaults", ".", "prefixes", ")", "\n", "+", "[", "'-|:|_+'", ",", "'/'", ",", "'~'", ",", "'x|X'", ",", "r'\\dL'", ",", "'O2'", ",", "'VN'", ",", "\"Coumadin\"", ",", "\n", "\"HIGHLIGHTS\"", ",", "\"PROMPTCARE\"", ",", "\"VISUDYNE\"", ",", "\"weeks\"", ",", "\"week\"", ",", "'ASA'", ",", "'pap'", ",", "\n", "\"ZT\"", ",", "\"BaP\"", ",", "\"PND|BID\"", ",", "\"BPA\"", ",", "\"GD\"", ",", "'BB'", ",", "'PBS'", ",", "\n", "\"days\"", ",", "\"day\"", ",", "\"Kx\"", ",", "'mg'", ",", "r\"\\d\\.'\"", ",", "\"Results\"", ",", "\"RoeID\"", ",", "\"at\"", ",", "r\"\\d/\\d\\d\\d\\d\"", "]", ")", ")", ".", "search", "\n", "language", ".", "tokenizer", ".", "suffix_search", "=", "compile_suffix_regex", "(", "tuple", "(", "list", "(", "language", ".", "Defaults", ".", "suffixes", ")", "\n", "+", "[", "'weeks|minutes|day|days|hours|year'", ",", "\n", "'Au'", ",", "r\"\\[see\"", ",", "'induced|IMA|HBMC|mice|MARY|SLO|RHM|solutions|\u2014and|for|III|FINAL|The|Scattered|Intern|Left|Emergency|Staff|Chief'", ",", "\n", "'\u03bcl|\u03bcL|\u03bcg/L|AGH|HCC|RHN|MC|yM|GMH|Code|Hyperlipidemia|Adenomatous|greater|Drug|MEDIQUIK|and|Date|Procedure|Problems|Ordering|CLOSURE|Total|Status'", ",", "\n", "':|_+'", ",", "r\"\\.\\w+\"", ",", "'U|D'", ",", "\"Mouse\"", ",", "r\"\\d\\d/\\d\\d/\\d\\d\\d\\d\"", ",", "r\"\\d\\d/\\d\\d\"", "]", ")", ")", ".", "search", "\n", "#exit()", "\n"]], "home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.raw_datasets.language._set_sentence_parse_exceptions": [[226, 241], ["len", "len"], "function", ["None"], ["", "def", "_set_sentence_parse_exceptions", "(", "doc", ")", ":", "\n", "    ", "for", "token", "in", "doc", "[", ":", "-", "1", "]", ":", "\n", "# if token.text == '\\n':", "\n", "#     doc[token.i+1].is_sent_start = True", "\n", "        ", "if", "token", ".", "text", "==", "'B-3'", ":", "\n", "            ", "doc", "[", "token", ".", "i", "-", "1", "]", ".", "is_sent_start", "=", "False", "\n", "doc", "[", "token", ".", "i", "]", ".", "is_sent_start", "=", "False", "\n", "", "if", "token", ".", "text", "==", "'Troponin'", ":", "\n", "            ", "doc", "[", "token", ".", "i", "+", "1", "]", ".", "is_sent_start", "=", "False", "\n", "doc", "[", "token", ".", "i", "-", "1", "]", ".", "is_sent_start", "=", "False", "\n", "", "if", "token", ".", "text", "==", "'p.o'", "or", "token", ".", "text", "==", "'po'", "or", "token", ".", "text", "==", "'.q'", "or", "token", ".", "text", "==", "'VIT'", "and", "len", "(", "doc", ")", ">", "2", ":", "\n", "            ", "doc", "[", "token", ".", "i", "+", "2", "]", ".", "is_sent_start", "=", "False", "\n", "", "if", "token", ".", "text", "==", "'12.7'", "and", "len", "(", "doc", ")", ">", "token", ".", "i", "+", "2", ":", "\n", "            ", "doc", "[", "token", ".", "i", "+", "2", "]", ".", "is_sent_start", "=", "False", "\n", "", "", "return", "doc", "\n", "", ""]], "home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.raw_datasets.con_to_brat.Line.__init__": [[27, 31], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "line_text", ":", "str", ",", "line_num", ":", "int", ",", "line_index", ":", "int", ")", ":", "\n", "        ", "self", ".", "text", "=", "line_text", "\n", "self", ".", "num", "=", "line_num", "\n", "self", ".", "index", "=", "line_index", "\n", "\n"]], "home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.raw_datasets.con_to_brat.Line.init_lines": [[32, 71], ["full_text.split", "con_to_brat.Line", "text_lines.append", "full_text.index", "len", "search_text.index", "matches.append"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "init_lines", "(", "full_text", ":", "str", ")", ":", "\n", "        ", "\"\"\"\n        Creates all the Line objects for a given text file, storing them in a list where index n is the nth - 1\n        line of the document.\n        :param full_text: The entire text of the document.\n        :return: The list of Lines.\n        \"\"\"", "\n", "global_start_ind", "=", "0", "\n", "global_line_num", "=", "0", "\n", "\n", "full_text_lines", "=", "full_text", ".", "split", "(", "'\\n'", ")", "\n", "text_lines", "=", "[", "]", "\n", "\n", "for", "given_line", "in", "full_text_lines", ":", "\n", "\n", "            ", "sub_index", "=", "0", "\n", "matches", "=", "[", "]", "\n", "while", "sub_index", "<", "global_start_ind", ":", "\n", "                ", "for", "previous_line", "in", "text_lines", ":", "\n", "                    ", "if", "given_line", "==", "previous_line", ".", "text", ":", "\n", "                        ", "matches", ".", "append", "(", "previous_line", ")", "\n", "", "sub_index", "+=", "previous_line", ".", "index", "\n", "\n", "", "", "if", "matches", ":", "\n", "# Get the text from the end of the last match onward", "\n", "                ", "search_text_start", "=", "matches", "[", "-", "1", "]", ".", "index", "+", "len", "(", "matches", "[", "-", "1", "]", ".", "text", ")", "\n", "search_text", "=", "full_text", "[", "search_text_start", ":", "]", "\n", "start_ind", "=", "search_text", ".", "index", "(", "given_line", ")", "+", "search_text_start", "\n", "", "else", ":", "# The line is unique so str.index() will be accurate", "\n", "                ", "start_ind", "=", "full_text", ".", "index", "(", "given_line", ")", "\n", "\n", "", "new_line", "=", "Line", "(", "given_line", ",", "global_line_num", ",", "start_ind", ")", "\n", "text_lines", ".", "append", "(", "new_line", ")", "\n", "\n", "global_start_ind", "=", "text_lines", "[", "-", "1", "]", ".", "index", "\n", "global_line_num", "+=", "1", "\n", "\n", "", "return", "text_lines", "\n", "\n"]], "home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.raw_datasets.con_to_brat.Line.__str__": [[72, 75], ["None"], "methods", ["None"], ["", "def", "__str__", "(", "self", ")", ":", "\n", "        ", "\"\"\"String representation of a line, with its index and text separated by a pipe.\"\"\"", "\n", "return", "\"%i | %s\"", "%", "(", "self", ".", "index", ",", "self", ".", "text", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.raw_datasets.con_to_brat.is_valid_con": [[86, 93], ["isinstance", "re.fullmatch"], "function", ["None"], ["def", "is_valid_con", "(", "item", ":", "str", ")", ":", "\n", "    ", "\"\"\"\n    Comprehensively tests to see if a given line is in valid con format. Returns respective boolean value.\n    :param item: A string that is a line of text, hopefully in the con format.\n    :return: Boolean of whether or not the line matches a con regular expression.\n    \"\"\"", "\n", "return", "isinstance", "(", "item", ",", "str", ")", "and", "fullmatch", "(", "con_pattern", ",", "item", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.raw_datasets.con_to_brat.line_to_dict": [[95, 107], ["re.findall", "re.findall", "re.findall"], "function", ["None"], ["", "def", "line_to_dict", "(", "item", ")", ":", "\n", "    ", "\"\"\"\n    Converts a string that is a line in con format to a dictionary representation of that data.\n    Keys are: data_item; start_ind; end_ind; data_type.\n    :param item: The line of con text (str).\n    :return: The dictionary containing that data.\n    \"\"\"", "\n", "c_item", "=", "findall", "(", "r'c=\"([^\"]*)\"'", ",", "item", ")", "\n", "t_item", "=", "findall", "(", "r't=\"([^\"]*)\"'", ",", "item", ")", "\n", "spans", "=", "findall", "(", "r'\\d+:\\d+'", ",", "item", ")", "\n", "items", "=", "{", "\"data_item\"", ":", "c_item", "[", "0", "]", ",", "\"start_ind\"", ":", "spans", "[", "0", "]", ",", "\"end_ind\"", ":", "spans", "[", "1", "]", ",", "\"data_type\"", ":", "t_item", "[", "0", "]", "}", "\n", "return", "items", "\n", "\n"]], "home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.raw_datasets.con_to_brat.switch_extension": [[109, 112], ["os.path.splitext"], "function", ["None"], ["", "def", "switch_extension", "(", "name", ",", "ext", ")", ":", "\n", "    ", "\"\"\"Takes the name of a file (str) and changes the extension to the one provided (str)\"\"\"", "\n", "return", "os", ".", "path", ".", "splitext", "(", "name", ")", "[", "0", "]", "+", "ext", "\n", "\n"]], "home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.raw_datasets.con_to_brat.check_same_text": [[114, 135], ["None"], "function", ["None"], ["", "def", "check_same_text", "(", "ent_text", ",", "start_ind", ",", "end_ind", ",", "doc_text", ")", ":", "\n", "    ", "\"\"\"\n    Checks that the entity text in the BRAT annotations matches the text of the document at\n    the indices calculated in this program; for example, if the casing is different in the\n    txt document than in the ann file, this function would return the casing used in the txt\n    document.\n    :param ent_text: the text of the annotation in the CON annotation\n    :param start_ind: the character index where the annotation starts\n    :param end_ind: the character index where the annotaiton ends\n    :param doc_text: the text of the txt document\n    :return: the text between the indices in the document\n    \"\"\"", "\n", "\n", "# Get the text between the annotations in the document,", "\n", "# even if it's different from the provided text", "\n", "text_in_doc", "=", "doc_text", "[", "start_ind", ":", "end_ind", "]", "\n", "\n", "if", "ent_text", "==", "text_in_doc", ":", "\n", "        ", "return", "True", "\n", "\n", "", "return", "text_in_doc", "\n", "\n"]], "home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.raw_datasets.con_to_brat.get_absolute_index": [[137, 189], ["re.split", "int", "re.split", "split_by_whitespace[].isspace", "sum", "sum", "re.escape", "re.sub", "int", "line_to_start_index.index", "s.isspace", "len", "len", "re.search", "logging.warning", "s.isspace"], "function", ["None"], ["", "def", "get_absolute_index", "(", "txt_lns", ",", "ind", ",", "entity", ")", ":", "\n", "    ", "\"\"\"\n    Given one of the \\d+:\\d+ spans, which represent the index of a word relative to the start of the line it's on,\n    returns the index of that char relative to the start of the file.\n    :param txt_lns: The list of Line objects for that file.\n    :param ind: The string in format \\d+:\\d+\n    :param entity: The text of the entity\n    :return: The absolute index\n    \"\"\"", "\n", "\n", "# Convert ind to line_num and char_num", "\n", "nums", "=", "split", "(", "\":\"", ",", "ind", ")", "\n", "line_num", "=", "int", "(", "nums", "[", "0", "]", ")", "-", "1", "# line nums in con start at 1 and not 0", "\n", "word_num", "=", "int", "(", "nums", "[", "1", "]", ")", "\n", "\n", "this_line", "=", "txt_lns", "[", "line_num", "]", "\n", "line_index", "=", "this_line", ".", "index", "\n", "\n", "# Get index of word following n space", "\n", "split_by_whitespace", "=", "split", "(", "whitespace_pattern", ",", "this_line", ".", "text", ")", "\n", "split_by_whitespace", "=", "[", "s", "for", "s", "in", "split_by_whitespace", "if", "s", "!=", "''", "]", "\n", "split_by_ws_no_ws", "=", "[", "s", "for", "s", "in", "split_by_whitespace", "if", "not", "s", ".", "isspace", "(", ")", "]", "\n", "all_whitespace", "=", "[", "s", "for", "s", "in", "split_by_whitespace", "if", "s", ".", "isspace", "(", ")", "]", "\n", "\n", "# Adjust word_num if first character cluster is whitespace", "\n", "if", "split_by_whitespace", "[", "0", "]", ".", "isspace", "(", ")", ":", "\n", "        ", "line_to_target_word", "=", "split_by_ws_no_ws", "[", ":", "word_num", "-", "1", "]", "\n", "", "else", ":", "\n", "        ", "line_to_target_word", "=", "split_by_ws_no_ws", "[", ":", "word_num", "]", "\n", "\n", "", "num_non_whitespace", "=", "sum", "(", "[", "len", "(", "w", ")", "for", "w", "in", "line_to_target_word", "]", ")", "\n", "num_whitespace", "=", "sum", "(", "[", "len", "(", "w", ")", "for", "w", "in", "all_whitespace", "[", ":", "word_num", "]", "]", ")", "\n", "\n", "index_within_line", "=", "num_whitespace", "+", "num_non_whitespace", "\n", "line_to_start_index", "=", "this_line", ".", "text", "[", "index_within_line", ":", "]", "\n", "entity_pattern_escaped", "=", "re", ".", "escape", "(", "entity", ")", "\n", "entity_pattern_spaced", "=", "re", ".", "sub", "(", "r\"\\\\\\s+\"", ",", "\"\\\\\\s+\"", ",", "entity_pattern_escaped", ")", "\n", "\n", "try", ":", "\n", "# Search for entity regardless of case or composition of intermediate spaces", "\n", "# match = re.search(entity_pattern_spaced, this_line.text, re.IGNORECASE)[0]", "\n", "        ", "match", "=", "re", ".", "search", "(", "entity_pattern_spaced", ",", "line_to_start_index", ",", "re", ".", "IGNORECASE", ")", "[", "0", "]", "\n", "offset", "=", "line_to_start_index", ".", "index", "(", "match", ")", "# adjusts if entity is not the first char in its \"word\"", "\n", "", "except", "(", "ValueError", ",", "TypeError", ")", ":", "\n", "        ", "logging", ".", "warning", "(", "\"\"\"Entity not found in its expected line:\n        \\t\"%s\"\n        \\t\"%s\"\n        \\tRevision of input data may be required; conversion for this item was skipped\"\"\"", "%", "(", "entity", ",", "this_line", ")", "\n", ")", "\n", "return", "-", "1", "\n", "\n", "", "return", "index_within_line", "+", "line_index", "+", "offset", "\n", "\n"]], "home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.raw_datasets.con_to_brat.convert_con_to_brat": [[191, 260], ["len", "os.path.isfile", "con_to_brat.switch_extension", "os.path.isfile", "con_file.read.split", "con_to_brat.line_to_dict", "con_to_brat.get_absolute_index", "len", "con_to_brat.check_same_text", "isinstance", "os.path.isfile", "FileNotFoundError", "open", "text_file.read", "con_to_brat.Line.init_lines", "FileNotFoundError", "open", "con_file.read", "con_file.read.split", "line.startswith", "logging.info", "open", "text_file.read", "con_to_brat.Line.init_lines", "con_to_brat.is_valid_con", "logging.warning", "str", "str", "str"], "function", ["home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.raw_datasets.con_to_brat.switch_extension", "home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.raw_datasets.con_to_brat.line_to_dict", "home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.raw_datasets.con_to_brat.get_absolute_index", "home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.raw_datasets.con_to_brat.check_same_text", "home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.raw_datasets.con_to_brat.Line.init_lines", "home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.raw_datasets.con_to_brat.Line.init_lines", "home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.raw_datasets.con_to_brat.is_valid_con"], ["", "def", "convert_con_to_brat", "(", "con_file_path", ",", "text_file_path", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    Converts a con file to a string representation of a brat file.\n    :param con_file_path: Path to the con file being converted. If a valid path is not provided but the argument is a\n        string, it will be parsed as if it were a representation of the con file itself.\n    :param text_file_path: Path to the text file associated with the con file. If not provided, the function will look\n        for a text file in the same directory with the same name except for the extention switched to 'txt'.\n        Else, raises error. Note that no conversion can be performed without the text file.\n    :return: A string representation of the brat file, which can then be written to file if desired.\n    \"\"\"", "\n", "\n", "global", "num_lines", ",", "num_skipped_regex", ",", "num_skipped_value_error", "\n", "\n", "# By default, find txt file with equivalent name", "\n", "if", "text_file_path", "is", "None", ":", "\n", "        ", "text_file_path", "=", "switch_extension", "(", "con_file_path", ",", "\".txt\"", ")", "\n", "if", "not", "os", ".", "path", ".", "isfile", "(", "text_file_path", ")", ":", "\n", "            ", "raise", "FileNotFoundError", "(", "\"No text file path was provided and no matching text file was found in the input\"", "\n", "\" directory\"", ")", "\n", "", "with", "open", "(", "text_file_path", ",", "'r'", ")", "as", "text_file", ":", "\n", "            ", "text", "=", "text_file", ".", "read", "(", ")", "\n", "text_lines", "=", "Line", ".", "init_lines", "(", "text", ")", "\n", "# Else, open the file with the path passed to the function", "\n", "", "", "elif", "os", ".", "path", ".", "isfile", "(", "text_file_path", ")", ":", "\n", "        ", "with", "open", "(", "text_file_path", ",", "'r'", ")", "as", "text_file", ":", "\n", "            ", "text", "=", "text_file", ".", "read", "(", ")", "\n", "text_lines", "=", "Line", ".", "init_lines", "(", "text", ")", "\n", "", "", "else", ":", "raise", "FileNotFoundError", "(", "\"No text file path was provided or the file was not found.\"", "\n", "\" Note that direct string input of the source text is not supported.\"", ")", "\n", "\n", "num_lines", "+=", "len", "(", "text_lines", ")", "\n", "\n", "# If con_file_path is actually a path, open it and split it into lines", "\n", "if", "os", ".", "path", ".", "isfile", "(", "con_file_path", ")", ":", "\n", "        ", "with", "open", "(", "con_file_path", ",", "'r'", ")", "as", "con_file", ":", "\n", "            ", "con_text", "=", "con_file", ".", "read", "(", ")", "\n", "con_text_lines", "=", "con_text", ".", "split", "(", "'\\n'", ")", "\n", "# Else, read whatever string is passed to the function as if it were the file itself", "\n", "", "", "else", ":", "\n", "        ", "con_text", "=", "con_file_path", "\n", "con_text_lines", "=", "con_text", ".", "split", "(", "'\\n'", ")", "\n", "\n", "", "output_text", "=", "\"\"", "\n", "t", "=", "1", "\n", "for", "line", "in", "con_text_lines", ":", "\n", "        ", "if", "line", "==", "\"\"", "or", "line", ".", "startswith", "(", "\"#\"", ")", ":", "continue", "\n", "elif", "not", "is_valid_con", "(", "line", ")", ":", "\n", "            ", "logging", ".", "warning", "(", "\"Incorrectly formatted line in %s was skipped: \\\"%s\\\".\"", "%", "(", "con_file_path", ",", "line", ")", ")", "\n", "num_skipped_regex", "+=", "1", "\n", "continue", "\n", "", "d", "=", "line_to_dict", "(", "line", ")", "\n", "start_ind", "=", "get_absolute_index", "(", "text_lines", ",", "d", "[", "\"start_ind\"", "]", ",", "d", "[", "\"data_item\"", "]", ")", "\n", "if", "start_ind", "==", "-", "1", ":", "\n", "            ", "num_skipped_value_error", "+=", "1", "\n", "continue", "# skips data that could not be converted", "\n", "", "span_length", "=", "len", "(", "d", "[", "\"data_item\"", "]", ")", "\n", "end_ind", "=", "start_ind", "+", "span_length", "\n", "\n", "# Check that the text of the annotation matches what's between its spans in the text document", "\n", "is_match", "=", "check_same_text", "(", "d", "[", "'data_item'", "]", ",", "start_ind", ",", "end_ind", ",", "text", ")", "\n", "if", "isinstance", "(", "is_match", ",", "str", ")", ":", "\n", "            ", "logging", ".", "info", "(", "f\"Annotation in file '{con_file_path}' did not match text between spans: '{d['data_item']}' != '{is_match}'\"", ")", "\n", "d", "[", "'data_item'", "]", "=", "is_match", "\n", "\n", "", "output_line", "=", "\"T%s\\t%s %s %s\\t%s\\n\"", "%", "(", "str", "(", "t", ")", ",", "d", "[", "\"data_type\"", "]", ",", "str", "(", "start_ind", ")", ",", "str", "(", "end_ind", ")", ",", "d", "[", "\"data_item\"", "]", ")", "\n", "output_text", "+=", "output_line", "\n", "t", "+=", "1", "\n", "\n", "", "return", "output_text", "\n", "\n"]], "home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.nli.data.load_mednli_2018": [[11, 38], ["language.get_language", "language.get_language.remove_pipe", "language.get_language.remove_pipe", "os.path.join", "open().read().strip", "enumerate", "os.path.exists", "FileNotFoundError", "open().read().strip.split", "json.loads", "MEDNLI_LABELS.index", "instances.append", "os.path.join", "open().read", "open", "os.path.join"], "function", ["home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.raw_datasets.language.get_language"], ["def", "load_mednli_2018", "(", "partition", "=", "'train'", ")", ":", "\n", "    ", "\"\"\"\n\n    :param partition:\n    :return:\n    \"\"\"", "\n", "assert", "partition", "in", "[", "'train'", ",", "'test'", "]", "\n", "language", "=", "get_language", "(", ")", "\n", "\n", "language", ".", "remove_pipe", "(", "'parser'", ")", "\n", "language", ".", "remove_pipe", "(", "'_set_sentence_parse_exceptions'", ")", "\n", "\n", "annotation_dir", "=", "os", ".", "path", ".", "join", "(", "base_path", ",", "'nli'", ",", "'med_nli'", ")", "\n", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "os", ".", "path", ".", "join", "(", "annotation_dir", ",", "f'mli_{partition}_v1.jsonl'", ")", ")", ":", "\n", "        ", "raise", "FileNotFoundError", "(", "'Cannot find %s data'", "%", "partition", ")", "\n", "\n", "\n", "", "file", "=", "open", "(", "os", ".", "path", ".", "join", "(", "annotation_dir", ",", "f'mli_{partition}_v1.jsonl'", ")", ",", "'r'", ")", ".", "read", "(", ")", ".", "strip", "(", ")", "\n", "\n", "instances", "=", "[", "]", "\n", "for", "idx", ",", "line", "in", "enumerate", "(", "file", ".", "split", "(", "'\\n'", ")", ")", ":", "\n", "        ", "instance", "=", "json", ".", "loads", "(", "line", ")", "\n", "label", "=", "MEDNLI_LABELS", ".", "index", "(", "instance", "[", "'gold_label'", "]", ")", "\n", "instances", ".", "append", "(", "(", "idx", ",", "instance", "[", "'sentence1'", "]", ",", "instance", "[", "'sentence2'", "]", ",", "label", ")", ")", "\n", "\n", "", "return", "instances", ",", "MEDNLI_LABELS", "\n", "\n"]], "home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.nli.data.load_medrqe_2016": [[42, 76], ["language.get_language", "language.get_language.remove_pipe", "language.get_language.remove_pipe", "os.path.join", "open().read().strip", "xml.fromstring", "ET.fromstring.findall", "os.path.exists", "FileNotFoundError", "MEDRQE_LABELS.index", "instances.append", "os.path.join", "open().read", "open", "os.path.join", "instance.findall", "instance.findall"], "function", ["home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.raw_datasets.language.get_language"], ["", "def", "load_medrqe_2016", "(", "partition", "=", "'train'", ")", ":", "\n", "    ", "\"\"\"\n\n    :param partition:\n    :return:\n    \"\"\"", "\n", "assert", "partition", "in", "[", "'train'", ",", "'test'", "]", "\n", "language", "=", "get_language", "(", ")", "\n", "\n", "language", ".", "remove_pipe", "(", "'parser'", ")", "\n", "language", ".", "remove_pipe", "(", "'_set_sentence_parse_exceptions'", ")", "\n", "\n", "annotation_dir", "=", "os", ".", "path", ".", "join", "(", "base_path", ",", "'nli'", ",", "'med_rqe'", ")", "\n", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "os", ".", "path", ".", "join", "(", "annotation_dir", ",", "f'RQE_{partition}_pairs_AMIA2016.xml'", ")", ")", ":", "\n", "        ", "raise", "FileNotFoundError", "(", "'Cannot find %s data'", "%", "partition", ")", "\n", "\n", "\n", "# if not resource_exists('clinical_data', f\"nli/med_rqe/RQE_{partition}_pairs_AMIA2016.xml\"):", "\n", "#     raise FileNotFoundError(f\"Cannot find {partition} data\")", "\n", "\n", "#file = resource_string('clinical_data', f\"nli/med_rqe/RQE_{partition}_pairs_AMIA2016.xml\").decode('utf-8').strip()", "\n", "", "file", "=", "open", "(", "os", ".", "path", ".", "join", "(", "annotation_dir", ",", "f'RQE_{partition}_pairs_AMIA2016.xml'", ")", ")", ".", "read", "(", ")", ".", "strip", "(", ")", "\n", "root", "=", "ET", ".", "fromstring", "(", "file", ")", "\n", "\n", "pairs", "=", "root", ".", "findall", "(", "\"./pair\"", ")", "\n", "\n", "instances", "=", "[", "]", "\n", "for", "instance", "in", "pairs", ":", "\n", "        ", "id", "=", "instance", ".", "attrib", "[", "'pid'", "]", "\n", "label", "=", "MEDRQE_LABELS", ".", "index", "(", "instance", ".", "attrib", "[", "'value'", "]", ")", "\n", "instances", ".", "append", "(", "(", "id", ",", "instance", ".", "findall", "(", "'./chq'", ")", "[", "0", "]", ".", "text", ",", "instance", ".", "findall", "(", "'./faq'", ")", "[", "0", "]", ".", "text", ",", "label", ")", ")", "\n", "\n", "", "return", "instances", ",", "MEDRQE_LABELS", "\n", "\n"]], "home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.ner.data.load_n2c2_2018": [[40, 147], ["language.get_language", "pkg_resources.os.listdir", "sorted", "set", "set", "list", "enumerate", "pkg_resources.os.path.join", "pkg_resources.os.path.join", "set", "list.append", "open().read().strip", "open().read().strip.split", "annotations.append", "language.get_language.pipe", "zip", "enumerate", "open().read().strip", "line.split.split", "open().read", "set.add", "set.add", "annotation[].append", "doc.char_span", "enumerate", "open().read", "line[].split", "int", "[].append", "range", "tuple", "print", "print", "[].append", "open", "x.split", "len", "tuple", "len", "[].append", "line[].split", "[].split", "[].split", "doc.char_span", "doc.char_span", "tuple", "open", "pkg_resources.os.path.join", "line[].split", "tuple", "doc.char_span", "doc.char_span", "str", "str", "pkg_resources.os.path.join", "doc.char_span", "doc.char_span", "line[].split", "line[].split", "doc.char_span", "doc.char_span"], "function", ["home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.raw_datasets.language.get_language"], ["\n", "\n", "", "def", "load_medrqe_2016", "(", "partition", "=", "'train'", ")", ":", "\n", "    ", "\"\"\"\n\n    :param partition:\n    :return:\n    \"\"\"", "\n", "assert", "partition", "in", "[", "'train'", ",", "'test'", "]", "\n", "language", "=", "get_language", "(", ")", "\n", "\n", "language", ".", "remove_pipe", "(", "'parser'", ")", "\n", "language", ".", "remove_pipe", "(", "'_set_sentence_parse_exceptions'", ")", "\n", "\n", "annotation_dir", "=", "os", ".", "path", ".", "join", "(", "base_path", ",", "'nli'", ",", "'med_rqe'", ")", "\n", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "os", ".", "path", ".", "join", "(", "annotation_dir", ",", "f'RQE_{partition}_pairs_AMIA2016.xml'", ")", ")", ":", "\n", "        ", "raise", "FileNotFoundError", "(", "'Cannot find %s data'", "%", "partition", ")", "\n", "\n", "\n", "# if not resource_exists('clinical_data', f\"nli/med_rqe/RQE_{partition}_pairs_AMIA2016.xml\"):", "\n", "#     raise FileNotFoundError(f\"Cannot find {partition} data\")", "\n", "\n", "#file = resource_string('clinical_data', f\"nli/med_rqe/RQE_{partition}_pairs_AMIA2016.xml\").decode('utf-8').strip()", "\n", "", "file", "=", "open", "(", "os", ".", "path", ".", "join", "(", "annotation_dir", ",", "f'RQE_{partition}_pairs_AMIA2016.xml'", ")", ")", ".", "read", "(", ")", ".", "strip", "(", ")", "\n", "root", "=", "ET", ".", "fromstring", "(", "file", ")", "\n", "\n", "pairs", "=", "root", ".", "findall", "(", "\"./pair\"", ")", "\n", "\n", "instances", "=", "[", "]", "\n", "for", "instance", "in", "pairs", ":", "\n", "        ", "id", "=", "instance", ".", "attrib", "[", "'pid'", "]", "\n", "label", "=", "MEDRQE_LABELS", ".", "index", "(", "instance", ".", "attrib", "[", "'value'", "]", ")", "\n", "instances", ".", "append", "(", "(", "id", ",", "instance", ".", "findall", "(", "'./chq'", ")", "[", "0", "]", ".", "text", ",", "instance", ".", "findall", "(", "'./faq'", ")", "[", "0", "]", ".", "text", ",", "label", ")", ")", "\n", "\n", "", "return", "instances", ",", "MEDRQE_LABELS", "\n", "\n", "\n", "\n", "", ""]], "home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.ner.data.load_i2b2_2014": [[148, 272], ["language.get_language", "sorted", "set", "list", "enumerate", "pkg_resources.os.listdir", "pkg_resources.os.listdir", "pkg_resources.os.path.join", "pkg_resources.os.listdir", "set", "list.append", "annotations.append", "language.get_language.pipe", "zip", "enumerate", "pkg_resources.os.path.join", "pkg_resources.os.path.join", "pkg_resources.os.path.exists", "xml.fromstring", "ET.fromstring.findall", "isinstance", "set.add", "[].append", "pkg_resources.os.path.join", "xml.fromstring", "xml.fromstring", "open().read().strip", "int", "int", "tuple", "doc.char_span", "enumerate", "open().read().strip", "open().read().strip", "ET.fromstring.findall", "[].append", "open().read", "doc.char_span", "doc.char_span", "tuple", "open().read", "open().read", "doc.char_span", "doc.char_span", "open", "doc.char_span", "doc.char_span", "open", "open", "pkg_resources.os.path.join", "doc.char_span", "doc.char_span", "pkg_resources.os.path.join", "pkg_resources.os.path.join", "doc.char_span", "doc.char_span", "doc.char_span", "doc.char_span", "doc.char_span", "doc.char_span", "doc.char_span", "doc.char_span", "doc.char_span", "doc.char_span", "doc.char_span", "doc.char_span", "doc.char_span", "doc.char_span", "doc.char_span", "doc.char_span"], "function", ["home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.raw_datasets.language.get_language"], []], "home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.ner.data.load_i2b2_2010": [[273, 397], ["language.get_language", "sorted", "set", "list", "enumerate", "set", "open().read().strip.strip().split", "annotations.append", "language.get_language.pipe", "zip", "enumerate", "pkg_resources.os.path.join", "pkg_resources.os.path.join", "pkg_resources.os.path.join", "pkg_resources.os.listdir", "file.endswith", "pkg_resources.os.path.exists", "pkg_resources.os.path.join", "list.append", "open().read().strip", "line.split.split", "pkg_resources.os.path.join", "list.append", "open().read().strip", "list.append", "open().read().strip", "open().read().strip", "open().read().strip.strip", "set.add", "unique_relation_labels.add", "annotation[].append", "doc.char_span", "enumerate", "open().read().strip", "open().read().strip", "open().read", "line[].split", "int", "[].append", "range", "tuple", "print", "[].append", "open().read", "open().read", "open().read", "x.split", "len", "tuple", "len", "[].append", "line[].split", "[].split", "[].split", "doc.char_span", "doc.char_span", "print", "tuple", "open().read", "open().read", "open", "line[].split", "tuple", "doc.char_span", "doc.char_span", "str", "open", "open", "open", "pkg_resources.os.path.join", "doc.char_span", "doc.char_span", "open", "pkg_resources.os.path.join", "open", "pkg_resources.os.path.join", "pkg_resources.os.path.join", "line[].split", "line[].split", "doc.char_span", "doc.char_span", "pkg_resources.os.path.join", "pkg_resources.os.path.join"], "function", ["home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.raw_datasets.language.get_language"], []], "home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.ner.data.load_i2b2_2012": [[401, 521], ["language.get_language", "sorted", "set", "list", "enumerate", "pkg_resources.os.path.join", "pkg_resources.os.path.join", "set", "xml.fromstring", "list.append", "annotations.append", "language.get_language.pipe", "zip", "enumerate", "pkg_resources.os.listdir", "file.endswith", "open().read().strip().replace", "ET.fromstring.findall", "isinstance", "set.add", "[].append", "annotation_dict.attrib[].startswith", "int", "int", "tuple", "doc.char_span", "enumerate", "open().read().strip", "ET.fromstring.findall", "print", "print", "RuntimeError", "[].append", "doc.char_span", "doc.char_span", "print", "tuple", "open().read", "doc.char_span", "doc.char_span", "str", "str", "doc.char_span", "doc.char_span", "open", "doc.char_span", "doc.char_span", "str", "pkg_resources.os.path.join", "doc.char_span", "doc.char_span", "int", "int", "doc.char_span", "doc.char_span", "doc.char_span", "doc.char_span", "doc.char_span", "doc.char_span", "doc.char_span", "doc.char_span", "doc.char_span", "doc.char_span", "doc.char_span", "doc.char_span", "doc.char_span", "doc.char_span"], "function", ["home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.raw_datasets.language.get_language"], []], "home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.ner.data.load_quaero_frenchmed": [[522, 650], ["language.get_language", "pkg_resources.os.listdir", "sorted", "set", "set", "list", "enumerate", "pkg_resources.os.path.join", "pkg_resources.os.path.join", "set", "list.append", "open().read().strip", "open().read().strip.strip().split", "annotations.append", "language.get_language.pipe", "zip", "enumerate", "open().read().strip", "line.split.split", "open().read", "open().read().strip.strip", "set.add", "set.add", "annotation[].append", "doc.char_span", "enumerate", "open().read", "line[].split", "int", "[].append", "range", "tuple", "[].append", "open", "x.split", "len", "tuple", "len", "[].append", "line[].split", "[].split", "[].split", "doc.char_span", "doc.char_span", "tuple", "open", "pkg_resources.os.path.join", "line[].split", "tuple", "doc.char_span", "doc.char_span", "pkg_resources.os.path.join", "doc.char_span", "doc.char_span", "line[].split", "line[].split", "doc.char_span", "doc.char_span", "doc.char_span", "doc.char_span", "doc.char_span", "doc.char_span", "doc.char_span", "doc.char_span", "doc.char_span", "doc.char_span", "doc.char_span", "doc.char_span", "doc.char_span", "doc.char_span", "doc.char_span", "doc.char_span", "doc.char_span", "doc.char_span", "doc.char_span", "doc.char_span"], "function", ["home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.raw_datasets.language.get_language"], []], "home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.similarity.data.load_n2c2_2019_train_dev": [[7, 11], ["data.load_n2c2_2019", "random.Random().shuffle", "random.Random", "int", "int", "len", "len"], "function", ["home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.similarity.data.load_n2c2_2019"], ["MEDRQE_LABELS", "=", "sorted", "(", "[", "'false'", ",", "'true'", "]", ")", "\n", "base_path", "=", "os", ".", "path", ".", "join", "(", "os", ".", "getcwd", "(", ")", ",", "'raw_datasets'", ")", "\n", "\n", "\n", "def", "load_mednli_2018", "(", "partition", "=", "'train'", ")", ":", "\n"]], "home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.similarity.data.load_n2c2_2019": [[13, 55], ["language.get_language", "language.get_language.remove_pipe", "language.get_language.remove_pipe", "os.path.join", "open().read().strip", "open().read().strip().split", "enumerate", "os.path.exists", "FileNotFoundError", "open().read().strip.split", "os.path.join", "open().read", "open().read().strip", "tuple", "data.append", "line.split", "tuple", "data.append", "open", "open().read", "float", "line.split", "os.path.join", "float", "open", "os.path.join"], "function", ["home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.raw_datasets.language.get_language"], ["\n", "assert", "partition", "in", "[", "'train'", ",", "'test'", "]", "\n", "language", "=", "get_language", "(", ")", "\n", "\n", "language", ".", "remove_pipe", "(", "'parser'", ")", "\n", "language", ".", "remove_pipe", "(", "'_set_sentence_parse_exceptions'", ")", "\n", "\n", "annotation_dir", "=", "os", ".", "path", ".", "join", "(", "base_path", ",", "'nli'", ",", "'med_nli'", ")", "\n", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "os", ".", "path", ".", "join", "(", "annotation_dir", ",", "f'mli_{partition}_v1.jsonl'", ")", ")", ":", "\n", "        ", "raise", "FileNotFoundError", "(", "'Cannot find %s data'", "%", "partition", ")", "\n", "\n", "\n", "", "file", "=", "open", "(", "os", ".", "path", ".", "join", "(", "annotation_dir", ",", "f'mli_{partition}_v1.jsonl'", ")", ",", "'r'", ")", ".", "read", "(", ")", ".", "strip", "(", ")", "\n", "\n", "instances", "=", "[", "]", "\n", "for", "idx", ",", "line", "in", "enumerate", "(", "file", ".", "split", "(", "'\\n'", ")", ")", ":", "\n", "        ", "instance", "=", "json", ".", "loads", "(", "line", ")", "\n", "label", "=", "MEDNLI_LABELS", ".", "index", "(", "instance", "[", "'gold_label'", "]", ")", "\n", "instances", ".", "append", "(", "(", "idx", ",", "instance", "[", "'sentence1'", "]", ",", "instance", "[", "'sentence2'", "]", ",", "label", ")", ")", "\n", "\n", "", "return", "instances", ",", "MEDNLI_LABELS", "\n", "\n", "\n", "\n", "\n", "", "def", "load_medrqe_2016", "(", "partition", "=", "'train'", ")", ":", "\n", "    ", "\"\"\"\n\n    :param partition:\n    :return:\n    \"\"\"", "\n", "assert", "partition", "in", "[", "'train'", ",", "'test'", "]", "\n", "language", "=", "get_language", "(", ")", "\n", "\n", "language", ".", "remove_pipe", "(", "'parser'", ")", "\n", "language", ".", "remove_pipe", "(", "'_set_sentence_parse_exceptions'", ")", "\n", "\n", "annotation_dir", "=", "os", ".", "path", ".", "join", "(", "base_path", ",", "'nli'", ",", "'med_rqe'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.mt.predict.setup_logger": [[15, 29], ["log.handlers.clear", "logging.Formatter", "logging.StreamHandler", "logging.StreamHandler.setLevel", "logging.StreamHandler.setFormatter", "log.setLevel", "log.addHandler", "time.strftime", "socket.gethostname"], "function", ["None"], ["def", "setup_logger", "(", ")", ":", "\n", "\n", "#Set run specific envirorment configurations", "\n", "    ", "timestamp", "=", "time", ".", "strftime", "(", "\"run_%Y_%m_%d_%H_%M_%S\"", ")", "+", "\"_{machine}\"", ".", "format", "(", "machine", "=", "socket", ".", "gethostname", "(", ")", ")", "\n", "\n", "\n", "log", ".", "handlers", ".", "clear", "(", ")", "\n", "formatter", "=", "logging", ".", "Formatter", "(", "'%(message)s'", ")", "\n", "\n", "ch", "=", "logging", ".", "StreamHandler", "(", ")", "\n", "ch", ".", "setLevel", "(", "logging", ".", "INFO", ")", "\n", "ch", ".", "setFormatter", "(", "formatter", ")", "\n", "log", ".", "setLevel", "(", "logging", ".", "INFO", ")", "\n", "log", ".", "addHandler", "(", "ch", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.mt.predict.load_clinical_configured_tasks": [[75, 116], ["os.path.join", "os.getcwd", "os.path.exists", "os.path.exists", "os.path.exists", "[].update", "[].update", "[].update"], "function", ["None"], ["def", "load_clinical_configured_tasks", "(", "preprocessed_directory", "=", "os", ".", "path", ".", "join", "(", "os", ".", "getcwd", "(", ")", ",", "'..'", ",", "'..'", ",", "'data'", ")", ")", ":", "\n", "\n", "    ", "for", "dataset", "in", "clinical_ner", ":", "\n", "        ", "if", "os", ".", "path", ".", "exists", "(", "f\"{preprocessed_directory}/{dataset}/ner/train\"", ")", ":", "\n", "            ", "if", "dataset", "not", "in", "TASKS", "[", "'ner'", "]", ":", "\n", "                ", "TASKS", "[", "'ner'", "]", "[", "dataset", "]", "=", "{", "}", "\n", "", "TASKS", "[", "'ner'", "]", "[", "dataset", "]", ".", "update", "(", "\n", "{", "\n", "'head'", ":", "'subword_classification'", ",", "\n", "'batch_size'", ":", "25", ",", "\n", "'train'", ":", "f\"{preprocessed_directory}/{dataset}/ner/train\"", ",", "\n", "'test'", ":", "f\"{preprocessed_directory}/{dataset}/ner/test\"", "\n", "}", "\n", ")", "\n", "\n", "", "", "for", "dataset", "in", "clinical_sts", ":", "\n", "        ", "if", "os", ".", "path", ".", "exists", "(", "f\"{preprocessed_directory}/{dataset}/similarity/train\"", ")", ":", "\n", "            ", "if", "dataset", "not", "in", "TASKS", "[", "'sts'", "]", ":", "\n", "                ", "TASKS", "[", "'sts'", "]", "[", "dataset", "]", "=", "{", "}", "\n", "", "TASKS", "[", "'sts'", "]", "[", "dataset", "]", ".", "update", "(", "\n", "{", "\n", "'head'", ":", "'cls_regression'", ",", "\n", "'batch_size'", ":", "40", ",", "\n", "'train'", ":", "f\"{preprocessed_directory}/{dataset}/similarity/train\"", ",", "\n", "'test'", ":", "f\"{preprocessed_directory}/{dataset}/similarity/test\"", "\n", "}", "\n", ")", "\n", "", "", "for", "dataset", "in", "clinical_nli", ":", "\n", "        ", "if", "os", ".", "path", ".", "exists", "(", "f\"{preprocessed_directory}/{dataset}/nli/train\"", ")", ":", "\n", "            ", "if", "dataset", "not", "in", "TASKS", "[", "'nli'", "]", ":", "\n", "                ", "TASKS", "[", "'nli'", "]", "[", "dataset", "]", "=", "{", "}", "\n", "", "TASKS", "[", "'nli'", "]", "[", "dataset", "]", ".", "update", "(", "\n", "{", "\n", "'head'", ":", "'cls_classification'", ",", "\n", "'batch_size'", ":", "40", ",", "\n", "'train'", ":", "f\"{preprocessed_directory}/{dataset}/nli/train\"", ",", "\n", "'test'", ":", "f\"{preprocessed_directory}/{dataset}/nli/test\"", "\n", "}", "\n", ")", "\n", "\n", "", "", "return", "TASKS", "\n", "\n"]], "home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.mt.predict.predict": [[118, 180], ["torch.random.manual_seed", "predict.load_clinical_configured_tasks", "print", "pprint.pprint", "multitasking_transformers.multitaskers.MultiTaskingBert", "multitasking_transformers.multitaskers.MultiTaskingBert.predict", "hasattr", "heads_and_datasets.append", "hasattr", "torch.utils.data.DataLoader"], "function", ["home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.mt.train.load_clinical_configured_tasks", "home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.multitaskers.multi_tasking_bert.MultiTaskingBert.predict"], ["", "def", "predict", "(", "\n", "transformer_weights", "=", "'mt_clinical_bert_8_tasks'", ",", "\n", "model_storage_directory", "=", "''", ",", "\n", "device", "=", "'cuda'", ",", "\n", "learning_rate", "=", "5e-5", ",", "\n", "seed", "=", "5", ",", "\n", "shuffle", "=", "True", ",", "\n", "num_workers", "=", "1", ",", "\n", "transformer_hidden_size", "=", "768", ",", "\n", "transformer_dropout_prob", "=", ".1", "\n", ")", ":", "\n", "\n", "# log.info(gin.config_str())", "\n", "\n", "    ", "torch", ".", "random", ".", "manual_seed", "(", "seed", ")", "\n", "heads_and_datasets", "=", "[", "]", "\n", "load_clinical_configured_tasks", "(", ")", "\n", "\n", "print", "(", "\"MT Prediction over the following tasks:\"", ")", "\n", "pprint", "(", "TASKS", ")", "\n", "\n", "for", "task", "in", "TASKS", ":", "\n", "        ", "for", "dataset", "in", "TASKS", "[", "task", "]", ":", "\n", "            ", "train_dataset", "=", "DATASETS", "[", "task", "]", "(", "TASKS", "[", "task", "]", "[", "dataset", "]", "[", "'train'", "]", ")", "\n", "test_dataset", "=", "DATASETS", "[", "task", "]", "(", "TASKS", "[", "task", "]", "[", "dataset", "]", "[", "'test'", "]", ")", "\n", "\n", "labels", "=", "train_dataset", ".", "entity_labels", "if", "hasattr", "(", "train_dataset", ",", "'entity_labels'", ")", "else", "None", "\n", "if", "hasattr", "(", "train_dataset", ",", "'class_labels'", ")", ":", "\n", "                ", "labels", "=", "train_dataset", ".", "class_labels", "\n", "\n", "", "head", "=", "HEADS", "[", "TASKS", "[", "task", "]", "[", "dataset", "]", "[", "'head'", "]", "]", "(", "dataset", ",", "\n", "labels", "=", "labels", ",", "\n", "hidden_size", "=", "transformer_hidden_size", ",", "\n", "hidden_dropout_prob", "=", "transformer_dropout_prob", "\n", ")", "\n", "\n", "if", "TASKS", "[", "task", "]", "[", "dataset", "]", "[", "'head'", "]", "==", "'subword_classification'", ":", "\n", "                ", "if", "'evaluate_biluo'", "in", "TASKS", "[", "task", "]", "[", "dataset", "]", ":", "\n", "                    ", "head", ".", "config", ".", "evaluate_biluo", "=", "TASKS", "[", "task", "]", "[", "dataset", "]", "[", "'evaluate_biluo'", "]", "\n", "", "else", ":", "\n", "                    ", "head", ".", "config", ".", "evaluate_biluo", "=", "False", "\n", "", "", "heads_and_datasets", ".", "append", "(", "(", "head", ",", "\n", "DataLoader", "(", "test_dataset", ",", "\n", "batch_size", "=", "TASKS", "[", "task", "]", "[", "dataset", "]", "[", "'batch_size'", "]", ",", "\n", "shuffle", "=", "shuffle", ",", "\n", "num_workers", "=", "num_workers", "\n", ")", "\n", ")", "\n", ")", "\n", "\n", "\n", "\n", "", "", "heads", "=", "[", "head", "for", "head", ",", "_", "in", "heads_and_datasets", "]", "\n", "# mlflow.set_tag('number_tasks', str(len(heads)))", "\n", "mtb", "=", "MultiTaskingBert", "(", "heads", ",", "\n", "model_storage_directory", "=", "model_storage_directory", ",", "\n", "transformer_weights", "=", "transformer_weights", ",", "\n", "device", "=", "device", ",", "\n", "learning_rate", "=", "learning_rate", "\n", ")", "\n", "\n", "mtb", ".", "predict", "(", "heads_and_datasets", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.mt.train.setup_logger": [[15, 56], ["gin.bind_parameter", "os.makedirs", "log.handlers.clear", "logging.Formatter", "logging.FileHandler", "logging.FileHandler.setLevel", "logging.FileHandler.setFormatter", "log.addHandler", "logging.StreamHandler", "logging.StreamHandler.setLevel", "logging.StreamHandler.setFormatter", "log.setLevel", "log.addHandler", "mlflow.set_tracking_uri", "mlflow.set_experiment", "mlflow.start_run", "gin.config._CONFIG.get", "mlflow.log_params", "time.strftime", "os.path.join", "gin.query_parameter", "os.path.join", "torch.cuda.is_available", "log.info", "gin.query_parameter", "gin.query_parameter", "gin.query_parameter", "gin.query_parameter", "log.info", "Exception", "list", "socket.gethostname", "torch.cuda.current_device", "gin.query_parameter", "gin.query_parameter", "gin.config._CONFIG.keys", "gin.query_parameter"], "function", ["None"], ["def", "setup_logger", "(", ")", ":", "\n", "\n", "#Set run specific envirorment configurations", "\n", "    ", "timestamp", "=", "time", ".", "strftime", "(", "\"run_%Y_%m_%d_%H_%M_%S\"", ")", "+", "\"_{machine}\"", ".", "format", "(", "machine", "=", "socket", ".", "gethostname", "(", ")", ")", "\n", "\n", "gin", ".", "bind_parameter", "(", "'multi_tasking_train.model_storage_directory'", ",", "\n", "os", ".", "path", ".", "join", "(", "gin", ".", "query_parameter", "(", "'multi_tasking_train.model_storage_directory'", ")", ",", "timestamp", ")", ")", "\n", "\n", "os", ".", "makedirs", "(", "gin", ".", "query_parameter", "(", "'multi_tasking_train.model_storage_directory'", ")", ",", "exist_ok", "=", "True", ")", "\n", "\n", "log", ".", "handlers", ".", "clear", "(", ")", "\n", "formatter", "=", "logging", ".", "Formatter", "(", "'%(message)s'", ")", "\n", "fh", "=", "logging", ".", "FileHandler", "(", "os", ".", "path", ".", "join", "(", "gin", ".", "query_parameter", "(", "'multi_tasking_train.model_storage_directory'", ")", ",", "\"log.txt\"", ")", ")", "\n", "fh", ".", "setLevel", "(", "logging", ".", "INFO", ")", "\n", "fh", ".", "setFormatter", "(", "formatter", ")", "\n", "log", ".", "addHandler", "(", "fh", ")", "\n", "\n", "ch", "=", "logging", ".", "StreamHandler", "(", ")", "\n", "ch", ".", "setLevel", "(", "logging", ".", "INFO", ")", "\n", "ch", ".", "setFormatter", "(", "formatter", ")", "\n", "log", ".", "setLevel", "(", "logging", ".", "INFO", ")", "\n", "log", ".", "addHandler", "(", "ch", ")", "\n", "\n", "#Set global GPU state", "\n", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "and", "gin", ".", "query_parameter", "(", "'multi_tasking_train.device'", ")", "==", "'cuda'", ":", "\n", "        ", "log", ".", "info", "(", "\"Using CUDA device:{0}\"", ".", "format", "(", "torch", ".", "cuda", ".", "current_device", "(", ")", ")", ")", "\n", "", "else", ":", "\n", "        ", "if", "gin", ".", "query_parameter", "(", "'multi_tasking_train.device'", ")", "==", "'cpu'", ":", "\n", "            ", "log", ".", "info", "(", "\"Utilizing CPU\"", ")", "\n", "", "else", ":", "\n", "            ", "raise", "Exception", "(", "f\"Unrecognized device: {gin.query_parameter('multi_tasking_train.device')}\"", ")", "\n", "\n", "#ML-Flow", "\n", "", "", "mlflow", ".", "set_tracking_uri", "(", "f\"{gin.query_parameter('multi_tasking_train.ml_flow_directory')}\"", ")", "\n", "mlflow", ".", "set_experiment", "(", "f\"/{gin.query_parameter('multi_tasking_train.experiment_name')}\"", ")", "\n", "\n", "mlflow", ".", "start_run", "(", ")", "\n", "gin_parameters", "=", "gin", ".", "config", ".", "_CONFIG", ".", "get", "(", "list", "(", "gin", ".", "config", ".", "_CONFIG", ".", "keys", "(", ")", ")", "[", "0", "]", ")", "\n", "#gin_parameters['tasks'] = TASKS", "\n", "\n", "mlflow", ".", "log_params", "(", "gin_parameters", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.mt.train.load_clinical_configured_tasks": [[83, 124], ["os.path.join", "os.getcwd", "os.path.exists", "os.path.exists", "os.path.exists", "[].update", "[].update", "[].update"], "function", ["None"], ["def", "load_clinical_configured_tasks", "(", "preprocessed_directory", "=", "os", ".", "path", ".", "join", "(", "os", ".", "getcwd", "(", ")", ",", "'..'", ",", "'..'", ",", "'data'", ")", ")", ":", "\n", "\n", "    ", "for", "dataset", "in", "clinical_ner", ":", "\n", "        ", "if", "os", ".", "path", ".", "exists", "(", "f\"{preprocessed_directory}/{dataset}/ner/train\"", ")", ":", "\n", "            ", "if", "dataset", "not", "in", "TASKS", "[", "'ner'", "]", ":", "\n", "                ", "TASKS", "[", "'ner'", "]", "[", "dataset", "]", "=", "{", "}", "\n", "", "TASKS", "[", "'ner'", "]", "[", "dataset", "]", ".", "update", "(", "\n", "{", "\n", "'head'", ":", "'subword_classification'", ",", "\n", "'batch_size'", ":", "25", ",", "\n", "'train'", ":", "f\"{preprocessed_directory}/{dataset}/ner/train\"", ",", "\n", "'test'", ":", "f\"{preprocessed_directory}/{dataset}/ner/test\"", "\n", "}", "\n", ")", "\n", "\n", "", "", "for", "dataset", "in", "clinical_sts", ":", "\n", "        ", "if", "os", ".", "path", ".", "exists", "(", "f\"{preprocessed_directory}/{dataset}/similarity/train\"", ")", ":", "\n", "            ", "if", "dataset", "not", "in", "TASKS", "[", "'sts'", "]", ":", "\n", "                ", "TASKS", "[", "'sts'", "]", "[", "dataset", "]", "=", "{", "}", "\n", "", "TASKS", "[", "'sts'", "]", "[", "dataset", "]", ".", "update", "(", "\n", "{", "\n", "'head'", ":", "'cls_regression'", ",", "\n", "'batch_size'", ":", "40", ",", "\n", "'train'", ":", "f\"{preprocessed_directory}/{dataset}/similarity/train\"", ",", "\n", "'test'", ":", "f\"{preprocessed_directory}/{dataset}/similarity/test\"", "\n", "}", "\n", ")", "\n", "", "", "for", "dataset", "in", "clinical_nli", ":", "\n", "        ", "if", "os", ".", "path", ".", "exists", "(", "f\"{preprocessed_directory}/{dataset}/nli/train\"", ")", ":", "\n", "            ", "if", "dataset", "not", "in", "TASKS", "[", "'nli'", "]", ":", "\n", "                ", "TASKS", "[", "'nli'", "]", "[", "dataset", "]", "=", "{", "}", "\n", "", "TASKS", "[", "'nli'", "]", "[", "dataset", "]", ".", "update", "(", "\n", "{", "\n", "'head'", ":", "'cls_classification'", ",", "\n", "'batch_size'", ":", "40", ",", "\n", "'train'", ":", "f\"{preprocessed_directory}/{dataset}/nli/train\"", ",", "\n", "'test'", ":", "f\"{preprocessed_directory}/{dataset}/nli/test\"", "\n", "}", "\n", ")", "\n", "\n", "", "", "return", "TASKS", "\n", "\n"]], "home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.mt.train.train": [[127, 202], ["gin.configurable", "log.info", "torch.random.manual_seed", "train.load_clinical_configured_tasks", "print", "pprint.pprint", "mlflow.set_tag", "multitasking_transformers.multitaskers.MultiTaskingBert", "multitasking_transformers.multitaskers.MultiTaskingBert.fit", "gin.config_str", "str", "hasattr", "heads_and_datasets.append", "len", "hasattr", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader"], "function", ["home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.mt.train.load_clinical_configured_tasks", "home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.multitaskers.multi_tasking_bert.MultiTaskingBert.fit"], ["", "@", "gin", ".", "configurable", "(", "'multi_tasking_train'", ")", "\n", "def", "train", "(", "experiment_name", ",", "\n", "ml_flow_directory", ",", "\n", "transformer_weights", ",", "\n", "model_storage_directory", ",", "\n", "device", ",", "\n", "repeat_in_epoch_sampling", ",", "\n", "learning_rate", ",", "\n", "seed", ",", "\n", "evaluation_interval", "=", "1", ",", "\n", "checkpoint_interval", "=", "1", ",", "\n", "shuffle", "=", "True", ",", "\n", "num_workers", "=", "1", ",", "\n", "num_epochs", "=", "1", ",", "\n", "transformer_hidden_size", "=", "768", ",", "\n", "transformer_dropout_prob", "=", ".1", "\n", ")", ":", "\n", "\n", "    ", "log", ".", "info", "(", "gin", ".", "config_str", "(", ")", ")", "\n", "\n", "torch", ".", "random", ".", "manual_seed", "(", "seed", ")", "\n", "heads_and_datasets", "=", "[", "]", "\n", "load_clinical_configured_tasks", "(", ")", "\n", "\n", "print", "(", "\"MT training with the following tasks:\"", ")", "\n", "pprint", "(", "TASKS", ")", "\n", "\n", "for", "task", "in", "TASKS", ":", "\n", "        ", "for", "dataset", "in", "TASKS", "[", "task", "]", ":", "\n", "            ", "train_dataset", "=", "DATASETS", "[", "task", "]", "(", "TASKS", "[", "task", "]", "[", "dataset", "]", "[", "'train'", "]", ")", "\n", "test_dataset", "=", "DATASETS", "[", "task", "]", "(", "TASKS", "[", "task", "]", "[", "dataset", "]", "[", "'test'", "]", ")", "\n", "\n", "labels", "=", "train_dataset", ".", "entity_labels", "if", "hasattr", "(", "train_dataset", ",", "'entity_labels'", ")", "else", "None", "\n", "if", "hasattr", "(", "train_dataset", ",", "'class_labels'", ")", ":", "\n", "                ", "labels", "=", "train_dataset", ".", "class_labels", "\n", "\n", "", "head", "=", "HEADS", "[", "TASKS", "[", "task", "]", "[", "dataset", "]", "[", "'head'", "]", "]", "(", "dataset", ",", "\n", "labels", "=", "labels", ",", "\n", "hidden_size", "=", "transformer_hidden_size", ",", "\n", "hidden_dropout_prob", "=", "transformer_dropout_prob", "\n", ")", "\n", "\n", "if", "TASKS", "[", "task", "]", "[", "dataset", "]", "[", "'head'", "]", "==", "'subword_classification'", ":", "\n", "                ", "if", "'evaluate_biluo'", "in", "TASKS", "[", "task", "]", "[", "dataset", "]", ":", "\n", "                    ", "head", ".", "config", ".", "evaluate_biluo", "=", "TASKS", "[", "task", "]", "[", "dataset", "]", "[", "'evaluate_biluo'", "]", "\n", "", "else", ":", "\n", "                    ", "head", ".", "config", ".", "evaluate_biluo", "=", "False", "\n", "", "", "heads_and_datasets", ".", "append", "(", "(", "head", ",", "\n", "DataLoader", "(", "train_dataset", ",", "\n", "batch_size", "=", "TASKS", "[", "task", "]", "[", "dataset", "]", "[", "'batch_size'", "]", ",", "\n", "num_workers", "=", "num_workers", "\n", ")", ",", "\n", "DataLoader", "(", "test_dataset", ",", "\n", "batch_size", "=", "TASKS", "[", "task", "]", "[", "dataset", "]", "[", "'batch_size'", "]", ",", "\n", "shuffle", "=", "shuffle", ",", "\n", "num_workers", "=", "num_workers", "\n", ")", "\n", ")", "\n", ")", "\n", "\n", "\n", "\n", "", "", "heads", "=", "[", "head", "for", "head", ",", "_", ",", "_", "in", "heads_and_datasets", "]", "\n", "mlflow", ".", "set_tag", "(", "'number_tasks'", ",", "str", "(", "len", "(", "heads", ")", ")", ")", "\n", "mtb", "=", "MultiTaskingBert", "(", "heads", ",", "\n", "model_storage_directory", "=", "model_storage_directory", ",", "\n", "transformer_weights", "=", "transformer_weights", ",", "\n", "device", "=", "device", ",", "\n", "learning_rate", "=", "learning_rate", "\n", ")", "\n", "mtb", ".", "fit", "(", "heads_and_datasets", ",", "\n", "num_epochs", "=", "num_epochs", ",", "\n", "evaluation_interval", "=", "evaluation_interval", ",", "\n", "checkpoint_interval", "=", "checkpoint_interval", ",", "\n", "repeat_in_epoch_sampling", "=", "repeat_in_epoch_sampling", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.mt.train.cleanup_on_kill": [[210, 214], ["log.info", "mlflow.end_run"], "function", ["None"], ["", "@", "atexit", ".", "register", "\n", "def", "cleanup_on_kill", "(", ")", ":", "\n", "    ", "log", ".", "info", "(", "\"Training was abruptly killed.\"", ")", "\n", "mlflow", ".", "end_run", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.multitaskers.multi_tasking_bert.MultiTaskingBert.__init__": [[16, 69], ["util.get_model_path", "transformers.BertTokenizer.from_pretrained", "os.path.exists", "transformers.BertModel.from_pretrained", "torch.optim.Adam", "os.path.exists", "transformers.BertConfig.from_pretrained", "hasattr", "multi_tasking_bert.MultiTaskingBert.bert.parameters", "os.path.join", "transformers.BertConfig.from_json_file", "os.path.exists", "head.from_pretrained", "getattr", "log.info", "os.path.join", "os.path.join", "transformers.BertConfig.from_json_file", "ValueError", "log.info", "log.info", "head._init_mlm_head", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.multitaskers.util.get_model_path", "home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.heads.heads.TransformerHead.from_pretrained", "home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.heads.heads.TransformerHead.from_pretrained", "home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.heads.heads.TransformerHead.from_pretrained", "home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.heads.heads.TransformerHeadConfig.from_json_file", "home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.heads.heads.TransformerHead.from_pretrained", "home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.heads.heads.TransformerHeadConfig.from_json_file", "home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.heads.heads.MaskedLMHead._init_mlm_head"], ["    ", "def", "__init__", "(", "self", ",", "\n", "heads", ":", "List", "[", "TransformerHead", "]", ",", "\n", "transformer_weights", ":", "str", ",", "\n", "model_storage_directory", "=", "None", ",", "\n", "device", "=", "'cpu'", ",", "\n", "learning_rate", "=", "5e-5", ",", "\n", "transformer_layers", "=", "12", ",", "\n", "use_pretrained_heads", "=", "True", ")", ":", "\n", "        ", "\"\"\"\n        \n        :param model_directory: a directory path to the multi-tasking model. This contains bert weights and head weights.\n        \"\"\"", "\n", "self", ".", "transformer_weights", "=", "get_model_path", "(", "transformer_weights", ")", "\n", "self", ".", "heads", "=", "heads", "\n", "self", ".", "model_storage_directory", "=", "model_storage_directory", "\n", "self", ".", "transformer_layers", "=", "transformer_layers", "\n", "\n", "self", ".", "device", "=", "device", "\n", "\n", "self", ".", "bert_tokenizer", "=", "BertTokenizer", ".", "from_pretrained", "(", "self", ".", "transformer_weights", ")", "\n", "if", "os", ".", "path", ".", "exists", "(", "self", ".", "transformer_weights", ")", ":", "\n", "            ", "if", "os", ".", "path", ".", "exists", "(", "os", ".", "path", ".", "join", "(", "self", ".", "transformer_weights", ",", "CONFIG_NAME", ")", ")", ":", "\n", "                ", "config", "=", "BertConfig", ".", "from_json_file", "(", "os", ".", "path", ".", "join", "(", "self", ".", "transformer_weights", ",", "CONFIG_NAME", ")", ")", "\n", "", "elif", "os", ".", "path", ".", "exists", "(", "os", ".", "path", ".", "join", "(", "self", ".", "transformer_weights", ",", "'bert_config.json'", ")", ")", ":", "\n", "                ", "config", "=", "BertConfig", ".", "from_json_file", "(", "os", ".", "path", ".", "join", "(", "self", ".", "transformer_weights", ",", "'bert_config.json'", ")", ")", "\n", "", "else", ":", "\n", "                ", "raise", "ValueError", "(", "\"Cannot find a configuration for the BERT based model you are attempting to load.\"", ")", "\n", "", "", "else", ":", "\n", "            ", "config", "=", "BertConfig", ".", "from_pretrained", "(", "self", ".", "transformer_weights", ")", "\n", "\n", "", "config", ".", "output_hidden_states", "=", "True", "\n", "\n", "use_tf_model", "=", "'biobert_v1'", "in", "self", ".", "transformer_weights", "\n", "self", ".", "bert", "=", "BertModel", ".", "from_pretrained", "(", "self", ".", "transformer_weights", ",", "config", "=", "config", ",", "from_tf", "=", "use_tf_model", ")", "\n", "\n", "for", "head", "in", "heads", ":", "\n", "            ", "if", "use_pretrained_heads", ":", "\n", "                ", "if", "head", ".", "from_pretrained", "(", "self", ".", "transformer_weights", ")", ":", "\n", "                    ", "log", ".", "info", "(", "f\"Loading pretrained head: {head}\"", ")", "\n", "", "else", ":", "\n", "                    ", "log", ".", "info", "(", "f\"Training new head: {head}\"", ")", "\n", "", "if", "getattr", "(", "head", ",", "'_init_mlm_head'", ",", "None", ")", ":", "#lm heads required bert model configurations.", "\n", "                    ", "head", ".", "_init_mlm_head", "(", "config", ")", "\n", "", "", "else", ":", "\n", "                ", "log", ".", "info", "(", "f\"Training new head: {head}\"", ")", "\n", "\n", "", "", "if", "not", "hasattr", "(", "self", ",", "'epoch'", ")", ":", "\n", "            ", "self", ".", "epoch", "=", "0", "\n", "\n", "", "self", ".", "optimizer", "=", "torch", ".", "optim", ".", "Adam", "(", "\n", "self", ".", "bert", ".", "parameters", "(", ")", ",", "\n", "weight_decay", "=", "0", ",", "\n", "lr", "=", "learning_rate", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.multitaskers.multi_tasking_bert.MultiTaskingBert.save_checkpoint": [[72, 98], ["log.info", "torch.save", "base.config.to_json_file", "multi_tasking_bert.MultiTaskingBert.bert_tokenizer.save_vocabulary", "os.path.exists", "os.mkdir", "ValueError", "base.state_dict", "os.path.join", "os.path.join", "head.save", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.heads.heads.TransformerHead.save", "home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.heads.heads.TransformerHeadConfig.to_json_file", "home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.heads.heads.TransformerHead.save"], ["", "def", "save_checkpoint", "(", "self", ",", "checkpoint_path", ":", "str", ")", ":", "\n", "        ", "\"\"\"\n        Saves a checkpoint of the multi-tasking model.\n\n        A check point includes:\n        Base model weights, tokenizer and configuration.\n        Head weights and configurations.\n\n        :param checkpoint_path: the directory to save the checkpoint\n        :return: The directory containing the saved model.\n        \"\"\"", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "checkpoint_path", ")", ":", "\n", "            ", "os", ".", "mkdir", "(", "checkpoint_path", ")", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"Attempting to save checkpoint to an existing directory\"", ")", "\n", "", "log", ".", "info", "(", "f\"Saving checkpoint: {checkpoint_path}\"", ")", "\n", "base", "=", "self", ".", "bert", "\n", "\n", "#Save base model", "\n", "torch", ".", "save", "(", "base", ".", "state_dict", "(", ")", ",", "os", ".", "path", ".", "join", "(", "checkpoint_path", ",", "WEIGHTS_NAME", ")", ")", "\n", "base", ".", "config", ".", "to_json_file", "(", "os", ".", "path", ".", "join", "(", "checkpoint_path", ",", "CONFIG_NAME", ")", ")", "\n", "self", ".", "bert_tokenizer", ".", "save_vocabulary", "(", "checkpoint_path", ")", "\n", "\n", "#Save heads", "\n", "for", "head", "in", "self", ".", "heads", ":", "\n", "            ", "head", ".", "save", "(", "os", ".", "path", ".", "join", "(", "checkpoint_path", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.multitaskers.multi_tasking_bert.MultiTaskingBert.fit": [[101, 201], ["multitasking_transformers.dataloaders.RoundRobinDataLoader", "multi_tasking_bert.MultiTaskingBert.bert.train", "multi_tasking_bert.MultiTaskingBert.bert.to", "range", "head.to", "head.train", "enumerate", "log.info", "str", "str", "bert_input_ids.to.to.to", "isinstance", "isinstance", "labels.to.to.to", "multi_tasking_bert.MultiTaskingBert.bert", "head", "float", "loss.backward", "multi_tasking_bert.MultiTaskingBert.optimizer.step", "multi_tasking_bert.MultiTaskingBert.optimizer.zero_grad", "mlflow.log_metric", "multi_tasking_bert.MultiTaskingBert.predict", "multi_tasking_bert.MultiTaskingBert.save_checkpoint", "bert_attention_masks.to.to.to", "bert_token_type_ids.to.to.to", "log.info", "loss.item", "float", "os.path.join", "loss_weights[].to", "mlflow.log_metric", "multi_tasking_bert.MultiTaskingBert.save_checkpoint", "str", "str", "str", "str", "os.path.join", "float", "str", "str", "str"], "methods", ["home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.mt.train.train", "home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.mt.train.train", "home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.multitaskers.multi_tasking_bert.MultiTaskingBert.predict", "home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.multitaskers.multi_tasking_bert.MultiTaskingBert.save_checkpoint", "home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.multitaskers.multi_tasking_bert.MultiTaskingBert.save_checkpoint"], ["", "", "def", "fit", "(", "self", ",", "\n", "heads_and_dataloaders", ":", "List", "[", "Tuple", "[", "TransformerHead", ",", "DataLoader", ",", "DataLoader", "]", "]", ",", "\n", "num_epochs", "=", "1", ",", "\n", "evaluation_interval", "=", "1", ",", "\n", "checkpoint_interval", "=", "1", ",", "\n", "repeat_in_epoch_sampling", "=", "True", ",", "\n", "use_loss_weight", "=", "False", ",", "\n", "in_epoch_logging_and_saving", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        Fits a multi tasking model on the given heads and dataloaders\n        :param datasets:\n        \"\"\"", "\n", "\n", "train_loader", "=", "RoundRobinDataLoader", "(", "[", "(", "head", ",", "train_loader", ")", "for", "head", ",", "train_loader", ",", "_", "in", "heads_and_dataloaders", "]", ",", "\n", "repeat_in_epoch_sampling", "=", "repeat_in_epoch_sampling", ")", "\n", "\n", "self", ".", "bert", ".", "train", "(", ")", "\n", "self", ".", "bert", ".", "to", "(", "device", "=", "self", ".", "device", ")", "\n", "for", "head", "in", "self", ".", "heads", ":", "\n", "            ", "head", ".", "to", "(", "device", "=", "self", ".", "device", ")", "\n", "head", ".", "train", "(", ")", "\n", "\n", "\n", "", "for", "epoch", "in", "range", "(", "1", ",", "num_epochs", "+", "1", ")", ":", "\n", "            ", "self", ".", "epoch", "+=", "1", "\n", "\n", "task_epoch_loss", "=", "{", "str", "(", "head", ")", ":", "0.0", "for", "head", "in", "self", ".", "heads", "}", "\n", "# depending on round robin scheme, some tasks maybe passed more than once. we must keep count.", "\n", "task_batches", "=", "{", "str", "(", "head", ")", ":", "0", "for", "head", "in", "self", ".", "heads", "}", "\n", "\n", "for", "training_batch_idx", ",", "(", "head", ",", "dataset_batch_idx", ",", "batch", ")", "in", "enumerate", "(", "train_loader", ")", ":", "\n", "\n", "                ", "if", "head", ".", "__class__", ".", "__name__", "==", "\"SubwordClassificationHead\"", ":", "\n", "                    ", "bert_input_ids", ",", "bert_token_type_ids", ",", "bert_attention_masks", ",", "bert_sequence_lengths", ",", "labels", ",", "_", ",", "_", ",", "loss_weights", "=", "batch", "\n", "\n", "if", "use_loss_weight", ":", "\n", "                        ", "loss_weights", "=", "loss_weights", "[", "0", "]", ".", "to", "(", "device", "=", "self", ".", "device", ")", "\n", "#log.info(loss_weights)", "\n", "", "", "if", "head", ".", "__class__", ".", "__name__", "==", "\"MaskedLMHead\"", ":", "\n", "                    ", "bert_input_ids", ",", "labels", "=", "batch", "\n", "bert_token_type_ids", "=", "None", "\n", "bert_attention_masks", "=", "None", "\n", "\n", "", "if", "head", ".", "__class__", ".", "__name__", "in", "[", "\"CLSRegressionHead\"", ",", "\"CLSClassificationHead\"", "]", ":", "\n", "                    ", "bert_input_ids", ",", "bert_token_type_ids", ",", "bert_attention_masks", ",", "labels", "=", "batch", "\n", "\n", "", "if", "not", "use_loss_weight", ":", "\n", "                    ", "loss_weights", "=", "None", "\n", "\n", "", "bert_input_ids", "=", "bert_input_ids", ".", "to", "(", "device", "=", "self", ".", "device", ")", "\n", "\n", "if", "isinstance", "(", "bert_attention_masks", ",", "torch", ".", "Tensor", ")", ":", "#some tasks (ie. NER) do not need token types.", "\n", "                    ", "bert_attention_masks", "=", "bert_attention_masks", ".", "to", "(", "device", "=", "self", ".", "device", ")", "\n", "", "else", ":", "\n", "                    ", "bert_attention_masks", "=", "None", "\n", "\n", "", "if", "isinstance", "(", "bert_token_type_ids", ",", "torch", ".", "Tensor", ")", ":", "#some tasks (ie. NER) do not need token types.", "\n", "                    ", "bert_token_type_ids", "=", "bert_token_type_ids", ".", "to", "(", "device", "=", "self", ".", "device", ")", "\n", "", "else", ":", "\n", "                    ", "bert_token_type_ids", "=", "None", "\n", "\n", "", "labels", "=", "labels", ".", "to", "(", "device", "=", "self", ".", "device", ")", "\n", "\n", "\n", "hidden_states", ",", "_", ",", "all_hidden", "=", "self", ".", "bert", "(", "bert_input_ids", ",", "\n", "attention_mask", "=", "bert_attention_masks", ",", "\n", "token_type_ids", "=", "bert_token_type_ids", "\n", ")", "\n", "\n", "loss", ",", "_", "=", "head", "(", "hidden_states", ",", "labels", "=", "labels", ",", "loss_weight", "=", "loss_weights", ")", "\n", "\n", "if", "dataset_batch_idx", "%", "5", "==", "0", ":", "\n", "                    ", "log", ".", "info", "(", "f\"|{training_batch_idx}|{dataset_batch_idx}|: {head}, {loss}\"", ")", "\n", "\n", "", "if", "in_epoch_logging_and_saving", ":", "#useful for language modeling.", "\n", "                    ", "if", "(", "training_batch_idx", "+", "1", ")", "%", "100", "==", "0", ":", "\n", "                        ", "mlflow", ".", "log_metric", "(", "f\"{head}/epoch_train_loss\"", ",", "float", "(", "task_epoch_loss", "[", "str", "(", "head", ")", "]", ")", "/", "task_batches", "[", "str", "(", "head", ")", "]", ",", "step", "=", "training_batch_idx", ")", "\n", "\n", "", "if", "(", "training_batch_idx", "+", "1", ")", "%", "16000", "==", "0", ":", "\n", "                        ", "self", ".", "save_checkpoint", "(", "os", ".", "path", ".", "join", "(", "self", ".", "model_storage_directory", ",", "f'checkpoint_{epoch}_{training_batch_idx+1}'", ")", ")", "\n", "\n", "\n", "", "", "task_epoch_loss", "[", "str", "(", "head", ")", "]", "+=", "float", "(", "loss", ".", "item", "(", ")", ")", "\n", "task_batches", "[", "str", "(", "head", ")", "]", "+=", "1", "\n", "\n", "loss", ".", "backward", "(", ")", "\n", "self", ".", "optimizer", ".", "step", "(", ")", "\n", "self", ".", "optimizer", ".", "zero_grad", "(", ")", "\n", "\n", "", "for", "head", "in", "self", ".", "heads", ":", "\n", "                ", "task_epoch_loss", "[", "str", "(", "head", ")", "]", "/=", "task_batches", "[", "str", "(", "head", ")", "]", "\n", "mlflow", ".", "log_metric", "(", "f\"{head}/train_loss\"", ",", "float", "(", "task_epoch_loss", "[", "str", "(", "head", ")", "]", ")", ",", "step", "=", "epoch", ")", "\n", "", "log", ".", "info", "(", "f\"Epoch {self.epoch} Loss: {task_epoch_loss}\"", ")", "\n", "\n", "if", "epoch", "%", "evaluation_interval", "==", "0", ":", "\n", "                ", "self", ".", "predict", "(", "[", "(", "head", ",", "test_loader", ")", "for", "head", ",", "_", ",", "test_loader", "in", "heads_and_dataloaders", "]", ")", "\n", "", "if", "epoch", "%", "checkpoint_interval", "==", "0", ":", "\n", "                ", "self", ".", "save_checkpoint", "(", "os", ".", "path", ".", "join", "(", "self", ".", "model_storage_directory", ",", "f'checkpoint_{epoch}'", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.multitaskers.multi_tasking_bert.MultiTaskingBert.predict": [[202, 323], ["multi_tasking_bert.MultiTaskingBert.bert.eval", "multi_tasking_bert.MultiTaskingBert.bert.to", "multi_tasking_bert.MultiTaskingBert.bert.train", "head.to", "head.eval", "head.eval", "torch.no_grad", "head.train", "log.info", "isinstance", "isinstance", "isinstance", "enumerate", "multitasking_transformers.evaluation.evaluate_ner", "enumerate", "multitasking_transformers.evaluation.evaluate_sts", "enumerate", "multitasking_transformers.evaluation.evaluate_classification", "bert_input_ids.to.to.to", "bert_attention_masks.to.to.to", "isinstance", "multi_tasking_bert.MultiTaskingBert.bert", "range", "str", "bert_input_ids.to.to.to", "bert_attention_masks.to.to.to", "isinstance", "multi_tasking_bert.MultiTaskingBert.bert", "str", "bert_input_ids.to.to.to", "bert_attention_masks.to.to.to", "isinstance", "multi_tasking_bert.MultiTaskingBert.bert", "str", "bert_token_type_ids.to.to.to", "head", "subword_scores.max", "[].cpu().numpy", "range", "spacy_predictions_and_correct_labels.append", "bert_token_type_ids.to.to.to", "head", "scores.cpu().squeeze().tolist", "labels.squeeze().tolist", "numpy.asarray", "numpy.asarray", "bert_token_type_ids.to.to.to", "head", "[].cpu().squeeze().tolist", "[].squeeze().tolist", "numpy.asarray", "numpy.asarray", "len", "len", "[].cpu", "head.config.labels.index", "alignments[].max().item", "len", "[].tolist", "[].tolist", "scores.cpu().squeeze", "labels.squeeze", "[].cpu().squeeze", "[].squeeze", "alignments[].max", "scores.cpu", "[].cpu", "labels.max", "int", "bert_sequence_lengths[].item", "alignments[].max().item", "alignments[].max().item", "predictions.max", "alignments[].max", "alignments[].max"], "methods", ["home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.mt.train.train", "home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.mt.train.train", "home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.evaluation.evaluate.evaluate_ner", "home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.evaluation.evaluate.evaluate_sts", "home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.evaluation.evaluate.evaluate_classification"], ["", "", "", "def", "predict", "(", "self", ",", "heads_and_dataloaders", ":", "List", "[", "Tuple", "[", "TransformerHead", ",", "DataLoader", "]", "]", ")", ":", "\n", "        ", "\"\"\"\n        Predicts over the heads and dataloaders present.\n        :param heads_and_dataloaders:\n        :return:\n        \"\"\"", "\n", "\n", "self", ".", "bert", ".", "eval", "(", ")", "\n", "self", ".", "bert", ".", "to", "(", "device", "=", "self", ".", "device", ")", "\n", "for", "head", "in", "self", ".", "heads", ":", "\n", "            ", "head", ".", "to", "(", "device", "=", "self", ".", "device", ")", "\n", "head", ".", "eval", "(", ")", "\n", "\n", "", "for", "head", "in", "self", ".", "heads", ":", "\n", "            ", "head", ".", "eval", "(", ")", "\n", "", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "for", "head", ",", "dataloader", "in", "heads_and_dataloaders", ":", "\n", "                ", "log", ".", "info", "(", "f\"Predicting {head}\"", ")", "\n", "if", "isinstance", "(", "head", ",", "SubwordClassificationHead", ")", ":", "\n", "                    ", "spacy_predictions_and_correct_labels", "=", "[", "]", "\n", "for", "idx", ",", "batch", "in", "enumerate", "(", "dataloader", ")", ":", "\n", "                        ", "bert_input_ids", ",", "bert_token_type_ids", ",", "bert_attention_masks", ",", "bert_sequence_lengths", ",", "correct_bert_labels", ",", "correct_spacy_labels", ",", "alignments", ",", "_", "=", "batch", "\n", "\n", "bert_input_ids", "=", "bert_input_ids", ".", "to", "(", "device", "=", "self", ".", "device", ")", "\n", "bert_attention_masks", "=", "bert_attention_masks", ".", "to", "(", "device", "=", "self", ".", "device", ")", "\n", "\n", "if", "isinstance", "(", "bert_token_type_ids", ",", "torch", ".", "Tensor", ")", ":", "# some tasks (ie. NER) do not need token types.", "\n", "                            ", "bert_token_type_ids", "=", "bert_token_type_ids", ".", "to", "(", "device", "=", "self", ".", "device", ")", "\n", "", "else", ":", "\n", "                            ", "bert_token_type_ids", "=", "None", "\n", "\n", "", "hidden_states", ",", "_", ",", "all_hidden", "=", "self", ".", "bert", "(", "bert_input_ids", ",", "\n", "attention_mask", "=", "bert_attention_masks", ",", "\n", "token_type_ids", "=", "bert_token_type_ids", "\n", ")", "\n", "\n", "subword_scores", "=", "head", "(", "all_hidden", "[", "self", ".", "transformer_layers", "]", ")", "[", "0", "]", "\n", "batch_sequence_predictions", "=", "subword_scores", ".", "max", "(", "2", ")", "[", "1", "]", "#subword predictions", "\n", "\n", "\n", "for", "j", "in", "range", "(", "batch_sequence_predictions", ".", "shape", "[", "0", "]", ")", ":", "\n", "# retrieve only tokens from the sequence we care about", "\n", "                            ", "bert_sequence_predictions", "=", "batch_sequence_predictions", "[", "j", "]", "[", ":", "int", "(", "bert_sequence_lengths", "[", "j", "]", ".", "item", "(", ")", ")", "]", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "\n", "# array to store predictions relative to initial (spaCy) tokenization", "\n", "spacy_sequence_predictions", "=", "[", "head", ".", "config", ".", "labels", ".", "index", "(", "'O'", ")", "]", "*", "(", "alignments", "[", "j", "]", ".", "max", "(", ")", ".", "item", "(", ")", "+", "1", ")", "\n", "\n", "\n", "for", "token_index", "in", "range", "(", "1", ",", "len", "(", "bert_sequence_predictions", ")", "-", "1", ")", ":", "# account for bert padding tokens", "\n", "                                ", "spacy_sequence_predictions", "[", "alignments", "[", "j", "]", "[", "token_index", "]", "]", "=", "bert_sequence_predictions", "[", "token_index", "]", "\n", "\n", "\n", "", "spacy_predictions_and_correct_labels", ".", "append", "(", "\n", "(", "spacy_sequence_predictions", ",", "correct_spacy_labels", "[", "j", "]", "[", ":", "alignments", "[", "j", "]", ".", "max", "(", ")", ".", "item", "(", ")", "+", "1", "]", ".", "tolist", "(", ")", ")", "\n", ")", "\n", "assert", "(", "len", "(", "spacy_sequence_predictions", ")", "==", "len", "(", "correct_spacy_labels", "[", "j", "]", "[", ":", "alignments", "[", "j", "]", ".", "max", "(", ")", ".", "item", "(", ")", "+", "1", "]", ".", "tolist", "(", ")", ")", ")", "\n", "\n", "", "", "evaluate_ner", "(", "spacy_predictions_and_correct_labels", ",", "\n", "head", ".", "config", ".", "labels", ",", "str", "(", "head", ")", ",", "step", "=", "self", ".", "epoch", ",", "evaluate_bilou", "=", "head", ".", "config", ".", "evaluate_biluo", ")", "\n", "\n", "", "if", "isinstance", "(", "head", ",", "CLSRegressionHead", ")", ":", "\n", "\n", "                    ", "predicted_scores", "=", "[", "]", "\n", "correct_scores", "=", "[", "]", "\n", "for", "idx", ",", "batch", "in", "enumerate", "(", "dataloader", ")", ":", "\n", "                        ", "bert_input_ids", ",", "bert_token_type_ids", ",", "bert_attention_masks", ",", "labels", "=", "batch", "\n", "\n", "bert_input_ids", "=", "bert_input_ids", ".", "to", "(", "device", "=", "self", ".", "device", ")", "\n", "bert_attention_masks", "=", "bert_attention_masks", ".", "to", "(", "device", "=", "self", ".", "device", ")", "\n", "\n", "if", "isinstance", "(", "bert_token_type_ids", ",", "torch", ".", "Tensor", ")", ":", "# some tasks (ie. NER) do not need token types.", "\n", "                            ", "bert_token_type_ids", "=", "bert_token_type_ids", ".", "to", "(", "device", "=", "self", ".", "device", ")", "\n", "", "else", ":", "\n", "                            ", "bert_token_type_ids", "=", "None", "\n", "\n", "", "hidden_states", ",", "_", ",", "all_hidden", "=", "self", ".", "bert", "(", "bert_input_ids", ",", "\n", "attention_mask", "=", "bert_attention_masks", ",", "\n", "token_type_ids", "=", "bert_token_type_ids", "\n", ")", "\n", "scores", "=", "head", "(", "all_hidden", "[", "self", ".", "transformer_layers", "]", ")", "[", "0", "]", "\n", "\n", "predicted_scores", "=", "predicted_scores", "+", "scores", ".", "cpu", "(", ")", ".", "squeeze", "(", ")", ".", "tolist", "(", ")", "\n", "correct_scores", "=", "correct_scores", "+", "labels", ".", "squeeze", "(", ")", ".", "tolist", "(", ")", "\n", "\n", "", "evaluate_sts", "(", "(", "np", ".", "asarray", "(", "predicted_scores", ",", "dtype", "=", "np", ".", "float", ")", ",", "\n", "np", ".", "asarray", "(", "correct_scores", ",", "dtype", "=", "np", ".", "float", ")", ")", ",", "\n", "str", "(", "head", ")", ",", "step", "=", "self", ".", "epoch", ")", "\n", "\n", "", "if", "isinstance", "(", "head", ",", "CLSClassificationHead", ")", ":", "\n", "\n", "                    ", "predicted_labels", "=", "[", "]", "\n", "correct_labels", "=", "[", "]", "\n", "for", "idx", ",", "batch", "in", "enumerate", "(", "dataloader", ")", ":", "\n", "                        ", "bert_input_ids", ",", "bert_token_type_ids", ",", "bert_attention_masks", ",", "labels", "=", "batch", "\n", "\n", "bert_input_ids", "=", "bert_input_ids", ".", "to", "(", "device", "=", "self", ".", "device", ")", "\n", "bert_attention_masks", "=", "bert_attention_masks", ".", "to", "(", "device", "=", "self", ".", "device", ")", "\n", "\n", "if", "isinstance", "(", "bert_token_type_ids", ",", "torch", ".", "Tensor", ")", ":", "# some tasks (ie. NER) do not need token types.", "\n", "                            ", "bert_token_type_ids", "=", "bert_token_type_ids", ".", "to", "(", "device", "=", "self", ".", "device", ")", "\n", "", "else", ":", "\n", "                            ", "bert_token_type_ids", "=", "None", "\n", "\n", "", "hidden_states", ",", "_", ",", "all_hidden", "=", "self", ".", "bert", "(", "bert_input_ids", ",", "\n", "attention_mask", "=", "bert_attention_masks", ",", "\n", "token_type_ids", "=", "bert_token_type_ids", "\n", ")", "\n", "predictions", "=", "head", "(", "hidden_states", ")", "[", "0", "]", "\n", "\n", "#.max will select the label index we want.", "\n", "predicted_labels", "=", "predicted_labels", "+", "predictions", ".", "max", "(", "1", ")", "[", "1", "]", ".", "cpu", "(", ")", ".", "squeeze", "(", ")", ".", "tolist", "(", ")", "\n", "correct_labels", "=", "correct_labels", "+", "labels", ".", "max", "(", "1", ")", "[", "1", "]", ".", "squeeze", "(", ")", ".", "tolist", "(", ")", "\n", "\n", "", "evaluate_classification", "(", "(", "np", ".", "asarray", "(", "predicted_labels", ",", "dtype", "=", "np", ".", "int", ")", ",", "\n", "np", ".", "asarray", "(", "correct_labels", ",", "dtype", "=", "np", ".", "int", ")", ")", ",", "head", ".", "config", ".", "labels", ",", "\n", "str", "(", "head", ")", ",", "step", "=", "self", ".", "epoch", ")", "\n", "\n", "", "", "", "self", ".", "bert", ".", "train", "(", ")", "\n", "for", "head", "in", "self", ".", "heads", ":", "\n", "            ", "head", ".", "train", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.multitaskers.util.get_model_path": [[17, 47], ["transformers.file_utils.url_to_filename", "os.path.join", "os.path.exists", "os.makedirs", "tempfile.NamedTemporaryFile", "print", "temp_file.flush", "temp_file.seek", "tarfile.open.extractall", "transformers.file_utils.http_get", "tarfile.open", "print", "os.rmdir"], "function", ["None"], ["def", "get_model_path", "(", "model_name", ":", "str", ")", ":", "\n", "\n", "    ", "if", "model_name", "in", "MODEL_URL", ":", "\n", "        ", "model_url", "=", "MODEL_URL", "[", "model_name", "]", "\n", "", "else", ":", "\n", "        ", "return", "model_name", "\n", "", "model_url_hash", "=", "url_to_filename", "(", "model_url", ")", "\n", "\n", "model_path", "=", "os", ".", "path", ".", "join", "(", "default_cache_path", ",", "model_url_hash", ")", "\n", "\n", "if", "os", ".", "path", ".", "exists", "(", "model_path", ")", ":", "\n", "        ", "return", "model_path", "\n", "", "else", ":", "\n", "        ", "os", ".", "makedirs", "(", "model_path", ")", "\n", "with", "tempfile", ".", "NamedTemporaryFile", "(", ")", "as", "temp_file", ":", "\n", "            ", "print", "(", "\"Downloading model: %s from %s\"", "%", "(", "model_name", ",", "model_url", ")", ")", "\n", "try", ":", "\n", "                ", "http_get", "(", "model_url", ",", "temp_file", ")", "\n", "tar", "=", "tarfile", ".", "open", "(", "temp_file", ".", "name", ")", "\n", "", "except", "BaseException", "as", "exc", ":", "\n", "                ", "print", "(", "\"Failed to download model: %s\"", "%", "model_name", ")", "\n", "os", ".", "rmdir", "(", "model_path", ")", "\n", "raise", "exc", "\n", "\n", "\n", "", "temp_file", ".", "flush", "(", ")", "\n", "temp_file", ".", "seek", "(", "0", ")", "\n", "tar", ".", "extractall", "(", "model_path", ")", "\n", "\n", "return", "model_path", "\n", "\n"]], "home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.heads.test_heads.TestTransformerHeadConfig.test_config": [[5, 16], ["TransformerHeadConfig", "test_heads.TestTransformerHeadConfig.assertEqual", "test_heads.TestTransformerHeadConfig.assertEqual", "tempfile.TemporaryDirectory", "TransformerHeadConfig.to_json_file", "TransformerHeadConfig.from_json_file", "test_heads.TestTransformerHeadConfig.assertEqual", "os.path.join", "os.path.join", "TransformerHeadConfig.from_json_file.to_json_string", "TransformerHeadConfig.to_json_string"], "methods", ["home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.heads.heads.TransformerHeadConfig.to_json_file", "home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.heads.heads.TransformerHeadConfig.from_json_file", "home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.heads.heads.TransformerHeadConfig.to_json_string", "home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.heads.heads.TransformerHeadConfig.to_json_string"], ["    ", "def", "test_config", "(", "self", ")", ":", "\n", "        ", "config", "=", "TransformerHeadConfig", "(", "{", "'head_name'", ":", "\"subword_classification_head\"", ",", "'head_task'", ":", "\"Random NER Dataset\"", "}", ")", "\n", "\n", "self", ".", "assertEqual", "(", "config", ".", "head_name", ",", "\"subword_classification_head\"", ")", "\n", "self", ".", "assertEqual", "(", "config", ".", "head_task", ",", "\"Random NER Dataset\"", ")", "\n", "\n", "with", "tempfile", ".", "TemporaryDirectory", "(", ")", "as", "temp_dir", ":", "\n", "            ", "config", ".", "to_json_file", "(", "os", ".", "path", ".", "join", "(", "temp_dir", ",", "f\"{config}.json\"", ")", ")", "\n", "loaded_config", "=", "TransformerHeadConfig", ".", "from_json_file", "(", "os", ".", "path", ".", "join", "(", "temp_dir", ",", "f\"{config}.json\"", ")", ")", "\n", "#print(os.path.join(temp_dir, f\"{config}.json\"))", "\n", "self", ".", "assertEqual", "(", "loaded_config", ".", "to_json_string", "(", ")", ",", "config", ".", "to_json_string", "(", ")", ")", "\n", "#", "\n"]], "home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.heads.test_heads.TestSubwordClassificationHead.test_loading_head": [[20, 31], ["tempfile.TemporaryDirectory", "SubwordClassificationHead", "SubwordClassificationHead.save", "SubwordClassificationHead", "SubwordClassificationHead.from_pretrained", "test_heads.TestSubwordClassificationHead.assertEqual", "str", "dict", "dict"], "methods", ["home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.heads.heads.TransformerHead.save", "home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.heads.heads.TransformerHead.from_pretrained"], ["    ", "def", "test_loading_head", "(", "self", ")", ":", "\n", "\n", "        ", "with", "tempfile", ".", "TemporaryDirectory", "(", ")", "as", "temp_dir", ":", "\n", "            ", "head", "=", "SubwordClassificationHead", "(", "'test_task'", ",", "\n", "labels", "=", "[", "'O'", ",", "'B-Drug'", "]", ",", "\n", "hidden_size", "=", "768", ",", "\n", "hidden_dropout_prob", "=", ".1", ")", "\n", "head", ".", "save", "(", "str", "(", "temp_dir", ")", ")", "\n", "head2", "=", "SubwordClassificationHead", "(", "'test_task'", ",", "labels", "=", "head", ".", "config", ".", "labels", ")", "\n", "head2", ".", "from_pretrained", "(", "temp_dir", ")", "\n", "self", ".", "assertEqual", "(", "dict", "(", "head", ".", "config", ")", ",", "dict", "(", "head2", ".", "config", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.heads.test_heads.TestCLSRegressionHead.test_loading_head": [[34, 43], ["tempfile.TemporaryDirectory", "CLSRegressionHead", "CLSRegressionHead.save", "CLSRegressionHead", "CLSRegressionHead.from_pretrained", "test_heads.TestCLSRegressionHead.assertEqual", "str", "dict", "dict"], "methods", ["home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.heads.heads.TransformerHead.save", "home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.heads.heads.TransformerHead.from_pretrained"], ["    ", "def", "test_loading_head", "(", "self", ")", ":", "\n", "        ", "with", "tempfile", ".", "TemporaryDirectory", "(", ")", "as", "temp_dir", ":", "\n", "            ", "head", "=", "CLSRegressionHead", "(", "'test_task'", ",", "\n", "hidden_size", "=", "768", ",", "\n", "hidden_dropout_prob", "=", ".1", ")", "\n", "head", ".", "save", "(", "str", "(", "temp_dir", ")", ")", "\n", "head2", "=", "CLSRegressionHead", "(", "'test_task'", ")", "\n", "head2", ".", "from_pretrained", "(", "temp_dir", ")", "\n", "self", ".", "assertEqual", "(", "dict", "(", "head", ".", "config", ")", ",", "dict", "(", "head2", ".", "config", ")", ")", "", "", "", "", ""]], "home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.heads.heads.TransformerHeadConfig.__init__": [[20, 25], ["dict.__init__", "hasattr", "hasattr"], "methods", ["home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.dataloaders.round_robin.RoundRobinDataLoader.__init__"], ["def", "__init__", "(", "self", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", "TransformerHeadConfig", ",", "self", ")", ".", "__init__", "(", "*", "args", ",", "**", "kwargs", ")", "\n", "self", ".", "__dict__", "=", "self", "\n", "assert", "hasattr", "(", "self", ",", "'head_name'", ")", ",", "\"TransformerHeads require names.\"", "\n", "assert", "hasattr", "(", "self", ",", "'head_task'", ")", ",", "\"TransformerHeads require tasks.\"", "\n", "\n"]], "home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.heads.heads.TransformerHeadConfig.to_json_file": [[26, 30], ["open", "writer.write", "heads.TransformerHeadConfig.to_json_string"], "methods", ["home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.heads.heads.TransformerHeadConfig.to_json_string"], ["", "def", "to_json_file", "(", "self", ",", "json_file_path", ")", ":", "\n", "        ", "\"\"\" Save this instance to a json file.\"\"\"", "\n", "with", "open", "(", "json_file_path", ",", "\"w\"", ",", "encoding", "=", "\"utf-8\"", ")", "as", "writer", ":", "\n", "            ", "writer", ".", "write", "(", "self", ".", "to_json_string", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.heads.heads.TransformerHeadConfig.to_json_string": [[31, 34], ["json.dumps"], "methods", ["None"], ["", "", "def", "to_json_string", "(", "self", ")", ":", "\n", "        ", "\"\"\"Serializes this instance to a JSON string.\"\"\"", "\n", "return", "json", ".", "dumps", "(", "self", ",", "indent", "=", "2", ",", "sort_keys", "=", "True", ")", "+", "\"\\n\"", "\n", "\n"]], "home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.heads.heads.TransformerHeadConfig.from_json_file": [[35, 42], ["json.loads", "cls", "open", "reader.read"], "methods", ["None"], ["", "@", "classmethod", "\n", "def", "from_json_file", "(", "cls", ",", "json_file", ")", ":", "\n", "        ", "\"\"\"Constructs a `Config` from a json file of parameters.\"\"\"", "\n", "with", "open", "(", "json_file", ",", "\"r\"", ",", "encoding", "=", "\"utf-8\"", ")", "as", "reader", ":", "\n", "            ", "text", "=", "reader", ".", "read", "(", ")", "\n", "", "dict_obj", "=", "json", ".", "loads", "(", "text", ")", "\n", "return", "cls", "(", "**", "dict_obj", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.heads.heads.TransformerHeadConfig.__str__": [[43, 45], ["None"], "methods", ["None"], ["", "def", "__str__", "(", "self", ")", ":", "\n", "        ", "return", "f\"{self.head_name}_{self.head_task}\"", ".", "replace", "(", "' '", ",", "'_'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.heads.heads.TransformerHead.__init__": [[48, 61], ["torch.nn.Module.__init__", "heads.TransformerHeadConfig", "str"], "methods", ["home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.dataloaders.round_robin.RoundRobinDataLoader.__init__"], ["    ", "def", "__init__", "(", "self", ",", "head_name", ",", "head_task", ",", "hidden_size", "=", "None", ",", "hidden_dropout_prob", "=", "None", ",", "labels", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n\n        :param hidden_size: the dimension of the hidden state of the transformer.\n        :param hidden_dropout_prob: dropout rate during training\n        \"\"\"", "\n", "super", "(", "TransformerHead", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "config", "=", "TransformerHeadConfig", "(", "{", "\n", "'head_name'", ":", "head_name", ",", "\n", "'head_task'", ":", "str", "(", "head_task", ")", ",", "\n", "'labels'", ":", "labels", ",", "\n", "'hidden_size'", ":", "hidden_size", ",", "\n", "'hidden_dropout_prob'", ":", "hidden_dropout_prob", "\n", "}", ")", "\n"]], "home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.heads.heads.TransformerHead.from_pretrained": [[65, 90], ["os.path.join", "os.path.join", "heads.TransformerHead.config.update", "torch.cuda.is_available", "heads.TransformerHeadConfig.from_json_file", "heads.TransformerHead.load_state_dict", "heads.TransformerHead.load_state_dict", "torch.load", "torch.load", "torch.device"], "methods", ["home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.heads.heads.TransformerHeadConfig.from_json_file"], ["", "def", "from_pretrained", "(", "self", ",", "head_directory", ":", "str", ")", ":", "\n", "        ", "\"\"\"\n        Loads a transformer head from pretrained weights\n        :param head_directory: the directory of the head.\n        :param head_name: the name of the head.\n        :param head_task: the task of the head.\n        :return: a pretrained head.\n        \"\"\"", "\n", "config_filepath", "=", "os", ".", "path", ".", "join", "(", "head_directory", ",", "f\"{self.config.head_name}_{self.config.head_task}.json\"", ".", "replace", "(", "' '", ",", "'_'", ")", ")", "\n", "weight_filepath", "=", "os", ".", "path", ".", "join", "(", "head_directory", ",", "f\"{self.config.head_name}_{self.config.head_task}.pt\"", ".", "replace", "(", "' '", ",", "'_'", ")", ")", "\n", "\n", "try", ":", "\n", "            ", "loaded_config", "=", "TransformerHeadConfig", ".", "from_json_file", "(", "config_filepath", ")", "\n", "", "except", "FileNotFoundError", ":", "\n", "            ", "return", "False", "\n", "\n", "", "assert", "loaded_config", ".", "head_name", "==", "self", ".", "config", ".", "head_name", "\n", "assert", "loaded_config", ".", "head_task", "==", "self", ".", "config", ".", "head_task", "\n", "\n", "self", ".", "config", ".", "update", "(", "loaded_config", ".", "__dict__", ")", "\n", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", ":", "\n", "            ", "self", ".", "load_state_dict", "(", "torch", ".", "load", "(", "weight_filepath", ")", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "load_state_dict", "(", "torch", ".", "load", "(", "weight_filepath", ",", "map_location", "=", "torch", ".", "device", "(", "'cpu'", ")", ")", ")", "\n", "", "return", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.heads.heads.TransformerHead.save": [[91, 102], ["os.path.join", "os.path.join", "heads.TransformerHead.config.to_json_file", "torch.save", "heads.TransformerHead.state_dict"], "methods", ["home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.heads.heads.TransformerHeadConfig.to_json_file", "home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.heads.heads.TransformerHead.save"], ["", "def", "save", "(", "self", ",", "head_directory", ":", "str", ")", ":", "\n", "        ", "\"\"\"\n        Saves a head to a given directory.\n        :param head_directory:\n        :return:\n        \"\"\"", "\n", "config_filepath", "=", "os", ".", "path", ".", "join", "(", "head_directory", ",", "f\"{self.config.head_name}_{self.config.head_task}.json\"", ".", "replace", "(", "' '", ",", "'_'", ")", ")", "\n", "weight_filepath", "=", "os", ".", "path", ".", "join", "(", "head_directory", ",", "f\"{self.config.head_name}_{self.config.head_task}.pt\"", ".", "replace", "(", "' '", ",", "'_'", ")", ")", "\n", "\n", "self", ".", "config", ".", "to_json_file", "(", "config_filepath", ")", "\n", "torch", ".", "save", "(", "self", ".", "state_dict", "(", ")", ",", "weight_filepath", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.heads.heads.TransformerHead.__str__": [[104, 106], ["heads.TransformerHead.config.__str__"], "methods", ["home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.heads.heads.TransformerHead.__str__"], ["", "def", "__str__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "config", ".", "__str__", "(", ")", "\n", "", "def", "__repr__", "(", "self", ")", ":", "\n"]], "home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.heads.heads.TransformerHead.__repr__": [[106, 108], ["heads.TransformerHead.config.__repr__"], "methods", ["home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.heads.heads.TransformerHead.__repr__"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "config", ".", "__repr__", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.heads.heads.SubwordClassificationHead.__init__": [[115, 124], ["heads.TransformerHead.__init__", "torch.nn.Linear", "len", "type"], "methods", ["home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.dataloaders.round_robin.RoundRobinDataLoader.__init__"], ["def", "__init__", "(", "self", ",", "head_task", ",", "labels", "=", "None", ",", "hidden_size", "=", "768", ",", "hidden_dropout_prob", "=", ".1", ")", ":", "\n", "        ", "super", "(", "SubwordClassificationHead", ",", "self", ")", ".", "__init__", "(", "type", "(", "self", ")", ".", "__name__", ",", "\n", "head_task", ",", "\n", "labels", "=", "labels", ",", "\n", "hidden_size", "=", "hidden_size", ",", "\n", "hidden_dropout_prob", "=", "hidden_dropout_prob", ")", "\n", "self", ".", "entity_labels", "=", "self", ".", "config", ".", "labels", "\n", "self", ".", "config", ".", "evaluate_biluo", "=", "False", "\n", "self", ".", "classifier", "=", "nn", ".", "Linear", "(", "hidden_size", ",", "len", "(", "self", ".", "entity_labels", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.heads.heads.SubwordClassificationHead.forward": [[126, 149], ["heads.SubwordClassificationHead.classifier", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "attention_mask.view", "heads.SubwordClassificationHead.view", "labels.view", "heads.SubwordClassificationHead.view", "labels.view", "len", "len"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ":", "torch", ".", "Tensor", ",", "attention_mask", "=", "None", ",", "labels", "=", "None", ",", "loss_weight", "=", "None", ")", ":", "\n", "        ", "logits", "=", "self", ".", "classifier", "(", "hidden_states", ")", "\n", "\n", "outputs", "=", "(", "logits", ",", ")", "# add hidden states and attention if they are here", "\n", "if", "labels", "is", "not", "None", ":", "\n", "            ", "loss_fct", "=", "CrossEntropyLoss", "(", "weight", "=", "loss_weight", ")", "\n", "# Only keep active parts of the loss", "\n", "if", "attention_mask", "is", "not", "None", ":", "\n", "                ", "active_loss", "=", "attention_mask", ".", "view", "(", "-", "1", ")", "==", "1", "\n", "#print(active_loss)", "\n", "#print(active_loss.shape)", "\n", "#print(\"Sequence Lengths: \" + str(sequence_lengths))", "\n", "active_logits", "=", "logits", ".", "view", "(", "-", "1", ",", "len", "(", "self", ".", "entity_labels", ")", ")", "[", "active_loss", "]", "\n", "#print(active_logits.shape)", "\n", "active_labels", "=", "labels", ".", "view", "(", "-", "1", ")", "[", "active_loss", "]", "\n", "#print(active_labels.shape)", "\n", "#print(active_labels)", "\n", "loss", "=", "loss_fct", "(", "active_logits", ",", "active_labels", ")", "\n", "", "else", ":", "\n", "                ", "loss", "=", "loss_fct", "(", "logits", ".", "view", "(", "-", "1", ",", "len", "(", "self", ".", "entity_labels", ")", ")", ",", "labels", ".", "view", "(", "-", "1", ")", ")", "\n", "", "outputs", "=", "(", "loss", ",", ")", "+", "outputs", "\n", "\n", "", "return", "outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.heads.heads.BertForMaskedLM.__init__": [[153, 157], ["transformers.BertPreTrainedModel.__init__", "transformers.modeling_bert.BertOnlyMLMHead"], "methods", ["home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.dataloaders.round_robin.RoundRobinDataLoader.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "config", ")", "\n", "\n", "self", ".", "cls", "=", "BertOnlyMLMHead", "(", "config", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.heads.heads.BertForMaskedLM.get_output_embeddings": [[160, 162], ["None"], "methods", ["None"], ["", "def", "get_output_embeddings", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "cls", ".", "predictions", ".", "decoder", "\n", "\n"]], "home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.heads.heads.BertForMaskedLM.forward": [[163, 188], ["heads.BertForMaskedLM.cls", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss.", "heads.BertForMaskedLM.view", "masked_lm_labels.view"], "methods", ["None"], ["", "def", "forward", "(", "\n", "self", ",", "\n", "hidden_states", ":", "torch", ".", "Tensor", ",", "\n", "attention_mask", "=", "None", ",", "\n", "token_type_ids", "=", "None", ",", "\n", "position_ids", "=", "None", ",", "\n", "head_mask", "=", "None", ",", "\n", "inputs_embeds", "=", "None", ",", "\n", "masked_lm_labels", "=", "None", ",", "\n", "encoder_hidden_states", "=", "None", ",", "\n", "encoder_attention_mask", "=", "None", ",", "\n", "lm_labels", "=", "None", ",", "\n", ")", ":", "\n", "\n", "        ", "sequence_output", "=", "hidden_states", "\n", "prediction_scores", "=", "self", ".", "cls", "(", "sequence_output", ")", "\n", "\n", "outputs", "=", "(", "prediction_scores", ",", ")", "# Add hidden states and attention if they are here", "\n", "\n", "if", "masked_lm_labels", "is", "not", "None", ":", "\n", "            ", "loss_fct", "=", "CrossEntropyLoss", "(", ")", "# -100 index = padding token", "\n", "masked_lm_loss", "=", "loss_fct", "(", "prediction_scores", ".", "view", "(", "-", "1", ",", "self", ".", "config", ".", "vocab_size", ")", ",", "masked_lm_labels", ".", "view", "(", "-", "1", ")", ")", "\n", "outputs", "=", "(", "masked_lm_loss", ",", ")", "+", "outputs", "\n", "\n", "", "return", "outputs", "# (masked_lm_loss), (ltr_lm_loss), prediction_scores, (hidden_states), (attentions)", "\n", "\n"]], "home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.heads.heads.MaskedLMHead.__init__": [[194, 201], ["heads.TransformerHead.__init__", "type"], "methods", ["home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.dataloaders.round_robin.RoundRobinDataLoader.__init__"], ["def", "__init__", "(", "self", ",", "head_task", ",", "labels", "=", "None", ",", "hidden_size", "=", "768", ",", "hidden_dropout_prob", "=", ".1", ")", ":", "\n", "        ", "super", "(", "MaskedLMHead", ",", "self", ")", ".", "__init__", "(", "type", "(", "self", ")", ".", "__name__", ",", "\n", "head_task", ",", "\n", "labels", "=", "labels", ",", "\n", "hidden_size", "=", "hidden_size", ",", "\n", "hidden_dropout_prob", "=", "hidden_dropout_prob", ")", "\n", "self", ".", "masked_lm", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.heads.heads.MaskedLMHead._init_mlm_head": [[203, 205], ["heads.BertForMaskedLM"], "methods", ["None"], ["", "def", "_init_mlm_head", "(", "self", ",", "transformer_config", ")", ":", "\n", "        ", "self", ".", "masked_lm", "=", "BertForMaskedLM", "(", "transformer_config", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.heads.heads.MaskedLMHead.forward": [[206, 210], ["heads.MaskedLMHead.masked_lm", "Exception"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ":", "torch", ".", "Tensor", ",", "attention_mask", "=", "None", ",", "labels", "=", "None", ",", "loss_weight", "=", "None", ")", ":", "\n", "        ", "if", "not", "self", ".", "masked_lm", ":", "\n", "            ", "raise", "Exception", "(", "\"Masked Language Model Head is not initialized.\"", ")", "\n", "", "return", "self", ".", "masked_lm", "(", "hidden_states", ",", "masked_lm_labels", "=", "labels", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.heads.heads.CLSRegressionHead.__init__": [[213, 221], ["heads.TransformerHead.__init__", "torch.nn.Dropout", "torch.nn.Linear", "type"], "methods", ["home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.dataloaders.round_robin.RoundRobinDataLoader.__init__"], ["    ", "def", "__init__", "(", "self", ",", "head_task", ",", "hidden_size", "=", "768", ",", "hidden_dropout_prob", "=", ".1", ",", "labels", "=", "None", ")", ":", "\n", "        ", "super", "(", "CLSRegressionHead", ",", "self", ")", ".", "__init__", "(", "type", "(", "self", ")", ".", "__name__", ",", "\n", "head_task", ",", "\n", "hidden_size", "=", "hidden_size", ",", "\n", "hidden_dropout_prob", "=", "hidden_dropout_prob", ")", "\n", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "self", ".", "config", ".", "hidden_dropout_prob", ")", "\n", "self", ".", "classifier", "=", "nn", ".", "Linear", "(", "self", ".", "config", ".", "hidden_size", ",", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.heads.heads.CLSRegressionHead.forward": [[222, 234], ["heads.CLSRegressionHead.classifier", "torch.nn.MSELoss", "torch.nn.MSELoss.", "heads.CLSRegressionHead.view", "labels.view"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ":", "torch", ".", "Tensor", ",", "attention_mask", "=", "None", ",", "labels", "=", "None", ",", "loss_weight", "=", "None", ")", ":", "\n", "\n", "        ", "logits", "=", "self", ".", "classifier", "(", "hidden_states", "[", ":", ",", "0", ",", ":", "]", ")", "#take CLS token", "\n", "\n", "outputs", "=", "(", "logits", ",", ")", "# add hidden states and attention if they are here", "\n", "if", "labels", "is", "not", "None", ":", "\n", "            ", "loss_function", "=", "torch", ".", "nn", ".", "MSELoss", "(", ")", "\n", "\n", "loss", "=", "loss_function", "(", "logits", ".", "view", "(", "-", "1", ")", ",", "labels", ".", "view", "(", "-", "1", ")", ")", "\n", "outputs", "=", "(", "loss", ",", ")", "+", "outputs", "\n", "\n", "", "return", "outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.heads.heads.CLSClassificationHead.__init__": [[236, 247], ["heads.TransformerHead.__init__", "torch.nn.Dropout", "torch.nn.Linear", "len", "type"], "methods", ["home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.dataloaders.round_robin.RoundRobinDataLoader.__init__"], ["    ", "def", "__init__", "(", "self", ",", "head_task", ",", "hidden_size", "=", "768", ",", "hidden_dropout_prob", "=", ".1", ",", "labels", "=", "None", ")", ":", "\n", "        ", "super", "(", "CLSClassificationHead", ",", "self", ")", ".", "__init__", "(", "type", "(", "self", ")", ".", "__name__", ",", "\n", "head_task", ",", "\n", "labels", "=", "labels", ",", "\n", "hidden_size", "=", "hidden_size", ",", "\n", "hidden_dropout_prob", "=", "hidden_dropout_prob", ")", "\n", "\n", "self", ".", "class_labels", "=", "self", ".", "config", ".", "labels", "\n", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "self", ".", "config", ".", "hidden_dropout_prob", ")", "\n", "self", ".", "classifier", "=", "nn", ".", "Linear", "(", "self", ".", "config", ".", "hidden_size", ",", "len", "(", "self", ".", "class_labels", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.heads.heads.CLSClassificationHead.forward": [[248, 264], ["heads.CLSClassificationHead.classifier", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss.", "heads.CLSClassificationHead.view", "len", "labels.view().max", "labels.view", "len"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ":", "torch", ".", "Tensor", ",", "attention_mask", "=", "None", ",", "labels", "=", "None", ",", "loss_weight", "=", "None", ")", ":", "\n", "        ", "logits", "=", "self", ".", "classifier", "(", "hidden_states", "[", ":", ",", "0", ",", ":", "]", ")", "# take CLS token", "\n", "\n", "outputs", "=", "(", "logits", ",", ")", "# add hidden states and attention if they are here", "\n", "if", "labels", "is", "not", "None", ":", "\n", "            ", "loss_function", "=", "torch", ".", "nn", ".", "CrossEntropyLoss", "(", ")", "\n", "# log.info(logits.view(-1, len(self.class_labels)).shape, labels.view(-1, len(self.class_labels)).shape)", "\n", "# exit()", "\n", "\n", "# log.info(labels.view(-1, len(self.class_labels)))", "\n", "# log.info(labels.view(-1, len(self.class_labels)).max(1)[1])", "\n", "# exit()", "\n", "loss", "=", "loss_function", "(", "logits", ".", "view", "(", "-", "1", ",", "len", "(", "self", ".", "class_labels", ")", ")", ",", "labels", ".", "view", "(", "-", "1", ",", "len", "(", "self", ".", "class_labels", ")", ")", ".", "max", "(", "1", ")", "[", "1", "]", ")", "\n", "outputs", "=", "(", "loss", ",", ")", "+", "outputs", "\n", "\n", "", "return", "outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.data.test_configured_tasks.TestConfiguredTasks.test_configured_tasks": [[9, 15], ["get_all_configured_tasks", "test_configured_tasks.TestConfiguredTasks.assertIsInstance"], "methods", ["home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.data.configured_tasks.get_all_configured_tasks"], ["    ", "def", "test_configured_tasks", "(", "self", ")", ":", "\n", "        ", "from", "multi_tasking_transformers", ".", "data", ".", "configured_tasks", "import", "get_all_configured_tasks", "\n", "\n", "TASKS", "=", "get_all_configured_tasks", "(", "\"/home/aymulyar/development/multitasking_transformers/experiments/data\"", ")", "\n", "\n", "self", ".", "assertIsInstance", "(", "TASKS", ",", "dict", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.data.configured_tasks.get_misc_configured_tasks": [[25, 39], ["[].update"], "function", ["None"], ["def", "get_misc_configured_tasks", "(", "preprocessed_directory", ")", ":", "\n", "    ", "for", "dataset", "in", "misc_ner", ":", "\n", "        ", "if", "dataset", "not", "in", "TASKS", "[", "'ner'", "]", ":", "\n", "            ", "TASKS", "[", "'ner'", "]", "[", "dataset", "]", "=", "{", "}", "\n", "", "TASKS", "[", "'ner'", "]", "[", "dataset", "]", ".", "update", "(", "\n", "{", "\n", "'head'", ":", "'subword_classification'", ",", "\n", "'batch_size'", ":", "25", ",", "\n", "'train'", ":", "f\"{preprocessed_directory}/{dataset}/ner/train\"", ",", "\n", "'test'", ":", "f\"{preprocessed_directory}/{dataset}/ner/test\"", "\n", "}", "\n", ")", "\n", "\n", "", "return", "TASKS", "\n", "\n"]], "home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.data.configured_tasks.get_biomedical_configured_tasks": [[42, 57], ["[].update"], "function", ["None"], ["", "def", "get_biomedical_configured_tasks", "(", "preprocessed_directory", ")", ":", "\n", "# Update with biomedical ner tasks", "\n", "    ", "for", "dataset", "in", "biomedical_ner", ":", "\n", "        ", "if", "dataset", "not", "in", "TASKS", "[", "'ner'", "]", ":", "\n", "            ", "TASKS", "[", "'ner'", "]", "[", "dataset", "]", "=", "{", "}", "\n", "", "TASKS", "[", "'ner'", "]", "[", "dataset", "]", ".", "update", "(", "\n", "{", "\n", "'head'", ":", "'subword_classification'", ",", "\n", "'batch_size'", ":", "20", ",", "\n", "'train'", ":", "f\"{preprocessed_directory}/biomedical/ner/{dataset}/train\"", ",", "\n", "'test'", ":", "f\"{preprocessed_directory}/biomedical/ner/{dataset}/test\"", ",", "\n", "'evaluate_biluo'", ":", "False", "\n", "}", "\n", ")", "\n", "", "return", "TASKS", "\n", "\n"]], "home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.data.configured_tasks.get_clinical_configured_tasks": [[58, 95], ["[].update", "[].update", "[].update"], "function", ["None"], ["", "def", "get_clinical_configured_tasks", "(", "preprocessed_directory", ")", ":", "\n", "    ", "for", "dataset", "in", "clinical_ner", ":", "\n", "        ", "if", "dataset", "not", "in", "TASKS", "[", "'ner'", "]", ":", "\n", "            ", "TASKS", "[", "'ner'", "]", "[", "dataset", "]", "=", "{", "}", "\n", "", "TASKS", "[", "'ner'", "]", "[", "dataset", "]", ".", "update", "(", "\n", "{", "\n", "'head'", ":", "'subword_classification'", ",", "\n", "'batch_size'", ":", "25", ",", "\n", "'train'", ":", "f\"{preprocessed_directory}/{dataset}/ner/train\"", ",", "\n", "'test'", ":", "f\"{preprocessed_directory}/{dataset}/ner/test\"", "\n", "}", "\n", ")", "\n", "\n", "", "for", "dataset", "in", "clinical_sts", ":", "\n", "        ", "if", "dataset", "not", "in", "TASKS", "[", "'sts'", "]", ":", "\n", "            ", "TASKS", "[", "'sts'", "]", "[", "dataset", "]", "=", "{", "}", "\n", "", "TASKS", "[", "'sts'", "]", "[", "dataset", "]", ".", "update", "(", "\n", "{", "\n", "'head'", ":", "'cls_regression'", ",", "\n", "'batch_size'", ":", "40", ",", "\n", "'train'", ":", "f\"{preprocessed_directory}/{dataset}/similarity/train\"", ",", "\n", "'test'", ":", "f\"{preprocessed_directory}/{dataset}/similarity/test\"", "\n", "}", "\n", ")", "\n", "", "for", "dataset", "in", "clinical_nli", ":", "\n", "        ", "if", "dataset", "not", "in", "TASKS", "[", "'nli'", "]", ":", "\n", "            ", "TASKS", "[", "'nli'", "]", "[", "dataset", "]", "=", "{", "}", "\n", "", "TASKS", "[", "'nli'", "]", "[", "dataset", "]", ".", "update", "(", "\n", "{", "\n", "'head'", ":", "'cls_classification'", ",", "\n", "'batch_size'", ":", "40", ",", "\n", "'train'", ":", "f\"{preprocessed_directory}/{dataset}/nli/train\"", ",", "\n", "'test'", ":", "f\"{preprocessed_directory}/{dataset}/nli/test\"", "\n", "}", "\n", ")", "\n", "\n", "", "return", "TASKS", "\n", "\n"]], "home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.data.configured_tasks.get_huner_tasks": [[106, 122], ["[].update"], "function", ["None"], ["def", "get_huner_tasks", "(", "preprocessed_directory", ")", ":", "\n", "    ", "for", "task", "in", "huner_datasets", ":", "\n", "        ", "for", "dataset", "in", "huner_datasets", "[", "task", "]", ":", "\n", "            ", "if", "dataset", "in", "[", "'cellfinder'", ",", "'biosemantics'", "]", ":", "# TODO figure out the right batch size and delete this line -cellfinder. biosemantics is large. ignored for preliminary results.", "\n", "                ", "continue", "\n", "", "if", "dataset", "not", "in", "TASKS", "[", "'ner'", "]", ":", "\n", "                ", "TASKS", "[", "'ner'", "]", "[", "dataset", "]", "=", "{", "}", "\n", "", "TASKS", "[", "'ner'", "]", "[", "dataset", "]", ".", "update", "(", "\n", "{", "\n", "'head'", ":", "'subword_classification'", ",", "\n", "'batch_size'", ":", "20", ",", "\n", "'train'", ":", "f\"{preprocessed_directory}/biomedical/huner/{dataset}/train\"", ",", "\n", "'test'", ":", "f\"{preprocessed_directory}/biomedical/huner/{dataset}/test\"", "\n", "}", "\n", ")", "\n", "", "", "return", "TASKS", "\n", "\n"]], "home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.data.configured_tasks.get_all_configured_tasks": [[124, 130], ["configured_tasks.get_biomedical_configured_tasks", "configured_tasks.get_clinical_configured_tasks", "configured_tasks.get_misc_configured_tasks"], "function", ["home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.data.configured_tasks.get_biomedical_configured_tasks", "home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.data.configured_tasks.get_clinical_configured_tasks", "home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.data.configured_tasks.get_misc_configured_tasks"], ["", "def", "get_all_configured_tasks", "(", "preprocessed_directory", ")", ":", "\n", "    ", "get_biomedical_configured_tasks", "(", "preprocessed_directory", ")", "\n", "get_clinical_configured_tasks", "(", "preprocessed_directory", ")", "\n", "get_misc_configured_tasks", "(", "preprocessed_directory", ")", "\n", "\n", "return", "TASKS", "\n", "", ""]], "home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.data.test_sentence_pair_dataset.TestCreateSentencePairRegressionDataset.test_medreq_2016_test": [[20, 29], ["unittest.skipUnless", "list", "SentencePairClassificationDataset.create_sentence_pair_dataset", "print", "print", "load_medrqe_2016", "transformers.BertTokenizer.from_pretrained"], "methods", ["home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.data.sentence_pair_regression_dataset.SentencePairRegressionDataset.create_sentence_pair_dataset", "home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.nli.data.load_medrqe_2016", "home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.heads.heads.TransformerHead.from_pretrained"], ["    ", "@", "unittest", ".", "skipUnless", "(", "CLINICAL_DATA_PRESENT", ",", "\"Clinical data package not installed (a private preprocessing package)\"", ")", "\n", "def", "test_medreq_2016_test", "(", "self", ")", ":", "\n", "        ", "from", "clinical_data", ".", "entailment", "import", "load_medrqe_2016", "\n", "medreq_2016", "=", "list", "(", "load_medrqe_2016", "(", "partition", "=", "'train'", ")", ")", "\n", "bert_inputs", ",", "bert_labels", ",", "class_labels", "=", "SentencePairClassificationDataset", ".", "create_sentence_pair_dataset", "(", "medreq_2016", ",", "BertTokenizer", ".", "from_pretrained", "(", "'bert-base-uncased'", ")", ")", "\n", "bert_input_ids", ",", "bert_token_type_ids", ",", "bert_attention_masks", "=", "bert_inputs", "\n", "print", "(", "bert_labels", ")", "\n", "print", "(", "class_labels", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.data.test_sentence_pair_dataset.TestCreateSentencePairRegressionDataset.test_ner": [[31, 37], ["unittest.skipIf", "list", "SentencePairRegressionDataset.create_sentence_pair_dataset", "test_sentence_pair_dataset.TestCreateSentencePairRegressionDataset.assertEqual", "load_n2c2_2019", "transformers.BertTokenizer.from_pretrained"], "methods", ["home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.data.sentence_pair_regression_dataset.SentencePairRegressionDataset.create_sentence_pair_dataset", "home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.similarity.data.load_n2c2_2019", "home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.heads.heads.TransformerHead.from_pretrained"], ["", "@", "unittest", ".", "skipIf", "(", "CLINICAL_DATA_PRESENT", ",", "\"Clinical data package is installed (a private preprocessing package)\"", ")", "\n", "def", "test_ner", "(", "self", ")", ":", "\n", "        ", "n2c2_train", "=", "list", "(", "load_n2c2_2019", "(", "partition", "=", "'train'", ")", ")", "\n", "bert_input_ids", ",", "bert_attention_masks", ",", "bert_sequence_lengths", ",", "bert_labels", ",", "biluo_ordered_labels", "=", "SentencePairRegressionDataset", ".", "create_sentence_pair_dataset", "(", "n2c2_train", ",", "BertTokenizer", ".", "from_pretrained", "(", "'bert-base-uncased'", ")", ")", "\n", "self", ".", "assertEqual", "(", "biluo_ordered_labels", ".", "index", "[", "'BERT_TOKEN'", "]", ",", "bert_labels", ",", "bert_labels", "[", "0", "]", "[", "0", "]", ")", "", "", "", ""]], "home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.data.test_ner_dataset.TestCreateNERDataset.test_n2c2_2018_ner": [[11, 22], ["unittest.skipUnless", "list", "NERDataset.create_ner_dataset", "print", "print", "print", "test_ner_dataset.TestCreateNERDataset.assertEqual", "load_n2c2_2018", "transformers.BertTokenizer.from_pretrained", "biluo_ordered_labels.index"], "methods", ["home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.data.ner_dataset.NERDataset.create_ner_dataset", "home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.ner.data.load_n2c2_2018", "home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.heads.heads.TransformerHead.from_pretrained"], ["    ", "@", "unittest", ".", "skipUnless", "(", "CLINICAL_DATA_PRESENT", ",", "\"Clinical data package not installed (a private preprocessing package)\"", ")", "\n", "def", "test_n2c2_2018_ner", "(", "self", ")", ":", "\n", "        ", "from", "clinical_data", ".", "ner", "import", "load_n2c2_2018_train_dev", ",", "load_n2c2_2018", "\n", "n2c2_train", "=", "list", "(", "load_n2c2_2018", "(", "partition", "=", "'train'", ")", ")", "\n", "bert_inputs", ",", "bert_sequence_lengths", ",", "bert_labels", ",", "spacy_labels", ",", "alignments", ",", "biluo_ordered_labels", ",", "class_counts", "=", "NERDataset", ".", "create_ner_dataset", "(", "n2c2_train", ",", "BertTokenizer", ".", "from_pretrained", "(", "'bert-base-uncased'", ")", ")", "\n", "bert_input_ids", ",", "bert_token_masks", ",", "bert_attention_masks", "=", "bert_inputs", "\n", "print", "(", "bert_labels", "[", "1", "]", ")", "\n", "print", "(", "alignments", ")", "\n", "print", "(", "class_counts", ")", "\n", "self", ".", "assertEqual", "(", "biluo_ordered_labels", ".", "index", "(", "'BERT_TOKEN'", ")", ",", "bert_labels", "[", "0", "]", "[", "0", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.data.sentence_pair_classification_dataset.SentencePairClassificationDataset.__init__": [[25, 33], ["sorted", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.path.join"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "dataset_directory", ":", "str", ")", ":", "\n", "        ", "self", ".", "dataset_directory", "=", "dataset_directory", "\n", "self", ".", "class_labels", "=", "sorted", "(", "torch", ".", "load", "(", "os", ".", "path", ".", "join", "(", "self", ".", "dataset_directory", ",", "'class_labels.pt'", ")", ")", ")", "\n", "\n", "self", ".", "bert_input_ids", "=", "torch", ".", "load", "(", "os", ".", "path", ".", "join", "(", "self", ".", "dataset_directory", ",", "f\"bert_input_ids.pt\"", ")", ")", "\n", "self", ".", "bert_token_type_ids", "=", "torch", ".", "load", "(", "os", ".", "path", ".", "join", "(", "self", ".", "dataset_directory", ",", "f\"bert_token_type_ids.pt\"", ")", ")", "\n", "self", ".", "bert_attention_masks", "=", "torch", ".", "load", "(", "os", ".", "path", ".", "join", "(", "self", ".", "dataset_directory", ",", "f\"bert_attention_masks.pt\"", ")", ")", "\n", "self", ".", "labels", "=", "torch", ".", "load", "(", "os", ".", "path", ".", "join", "(", "self", ".", "dataset_directory", ",", "f\"labels.pt\"", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.data.sentence_pair_classification_dataset.SentencePairClassificationDataset.__len__": [[34, 37], ["None"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "'Denotes the total number of samples'", "\n", "return", "self", ".", "bert_input_ids", ".", "shape", "[", "0", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.data.sentence_pair_classification_dataset.SentencePairClassificationDataset.__getitem__": [[38, 42], ["None"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "index", ":", "int", ")", ":", "\n", "# Load data and get label", "\n", "        ", "return", "self", ".", "bert_input_ids", "[", "index", "]", ",", "self", ".", "bert_token_type_ids", "[", "index", "]", ",", "self", ".", "bert_attention_masks", "[", "index", "]", ",", "self", ".", "labels", "[", "index", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.data.sentence_pair_classification_dataset.SentencePairClassificationDataset.create_sentence_pair_dataset": [[43, 118], ["torch.empty", "torch.empty", "torch.empty", "torch.empty", "enumerate", "sentence_pair_classification_dataset._truncate_seq_pair", "max", "tokens.append", "input_type_ids.append", "tokenizer.tokenize", "tokenizer.tokenize", "sentence_pair_classification_dataset._truncate_seq_pair", "tokens.append", "input_type_ids.append", "tokens.append", "input_type_ids.append", "tokenizer.convert_tokens_to_ids", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "sorted", "tokenizer.tokenize", "tokenizer.tokenize", "len", "len", "len", "len", "len", "tokens.append", "input_type_ids.append", "tokens.append", "input_type_ids.append", "len", "len", "tokenizer.convert_tokens_to_ids.append", "attention_masks.append", "input_type_ids.append", "len", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "sorted", "os.path.join", "len", "len"], "methods", ["home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.data.sentence_pair_regression_dataset._truncate_seq_pair", "home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.data.sentence_pair_regression_dataset._truncate_seq_pair", "home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.heads.heads.TransformerHead.save", "home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.heads.heads.TransformerHead.save", "home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.heads.heads.TransformerHead.save", "home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.heads.heads.TransformerHead.save", "home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.heads.heads.TransformerHead.save"], ["", "def", "create_sentence_pair_dataset", "(", "data", ":", "List", "[", "Tuple", "]", ",", "\n", "tokenizer", ":", "Union", "[", "BertTokenizer", ",", "AlbertTokenizer", "]", ",", "\n", "save_directory", "=", "None", ",", "\n", "max_sequence_length", "=", "128", ")", ":", "\n", "        ", "\"\"\"\n        data is tuple where 1st element is dataset and second is a sorted list of label strings\n        dataset is a list of instances (tuples) where first index is id, sent1, sent2, label_index\n        :param tokenizer: \n        :param save_directory:\n        :param max_sequence_length:\n        :return:\n        \"\"\"", "\n", "max_bert_input_length", "=", "0", "\n", "\n", "data", ",", "class_labels", "=", "data", "[", "0", "]", ",", "data", "[", "1", "]", "\n", "\n", "#label is index of label in class_labels or if single label 1 if true or 0 if false.", "\n", "for", "id", ",", "sent1", ",", "sent2", ",", "label", "in", "data", ":", "\n", "            ", "sentence_1_tokenized", ",", "sentence_2_tokenized", "=", "tokenizer", ".", "tokenize", "(", "sent1", ")", ",", "tokenizer", ".", "tokenize", "(", "sent2", ")", "\n", "_truncate_seq_pair", "(", "sentence_1_tokenized", ",", "sentence_2_tokenized", ",", "max_sequence_length", "-", "3", ")", "# accounting for positioning tokens", "\n", "\n", "max_bert_input_length", "=", "max", "(", "max_bert_input_length", ",", "\n", "len", "(", "sentence_1_tokenized", ")", "+", "len", "(", "sentence_2_tokenized", ")", "+", "3", ")", "\n", "\n", "", "bert_input_ids", "=", "torch", ".", "empty", "(", "(", "len", "(", "data", ")", ",", "max_bert_input_length", ")", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "bert_token_type_ids", "=", "torch", ".", "empty", "(", "(", "len", "(", "data", ")", ",", "max_bert_input_length", ")", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "bert_attention_masks", "=", "torch", ".", "empty", "(", "(", "len", "(", "data", ")", ",", "max_bert_input_length", ")", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "labels", "=", "torch", ".", "empty", "(", "(", "len", "(", "data", ")", ",", "len", "(", "class_labels", ")", ")", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "\n", "for", "idx", ",", "(", "id", ",", "sent1", ",", "sent2", ",", "label", ")", "in", "enumerate", "(", "data", ")", ":", "\n", "            ", "tokens", "=", "[", "]", "\n", "input_type_ids", "=", "[", "]", "\n", "\n", "tokens", ".", "append", "(", "\"[CLS]\"", ")", "\n", "input_type_ids", ".", "append", "(", "0", ")", "\n", "sentence_1_tokenized", "=", "tokenizer", ".", "tokenize", "(", "sent1", ")", "\n", "sentence_2_tokenized", "=", "tokenizer", ".", "tokenize", "(", "sent2", ")", "\n", "_truncate_seq_pair", "(", "sentence_1_tokenized", ",", "sentence_2_tokenized", ",", "max_sequence_length", "-", "3", ")", "\n", "for", "token", "in", "sentence_1_tokenized", ":", "\n", "                ", "tokens", ".", "append", "(", "token", ")", "\n", "input_type_ids", ".", "append", "(", "0", ")", "\n", "", "tokens", ".", "append", "(", "\"[SEP]\"", ")", "\n", "input_type_ids", ".", "append", "(", "0", ")", "\n", "\n", "for", "token", "in", "sentence_2_tokenized", ":", "\n", "                ", "tokens", ".", "append", "(", "token", ")", "\n", "input_type_ids", ".", "append", "(", "1", ")", "\n", "", "tokens", ".", "append", "(", "\"[SEP]\"", ")", "\n", "input_type_ids", ".", "append", "(", "1", ")", "\n", "\n", "input_ids", "=", "tokenizer", ".", "convert_tokens_to_ids", "(", "tokens", ")", "\n", "\n", "attention_masks", "=", "[", "1", "]", "*", "len", "(", "input_ids", ")", "\n", "while", "len", "(", "input_ids", ")", "<", "max_bert_input_length", ":", "\n", "                ", "input_ids", ".", "append", "(", "0", ")", "\n", "attention_masks", ".", "append", "(", "0", ")", "\n", "input_type_ids", ".", "append", "(", "0", ")", "\n", "\n", "", "bert_input_ids", "[", "idx", "]", "=", "torch", ".", "tensor", "(", "input_ids", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "bert_token_type_ids", "[", "idx", "]", "=", "torch", ".", "tensor", "(", "input_type_ids", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "bert_attention_masks", "[", "idx", "]", "=", "torch", ".", "tensor", "(", "attention_masks", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "\n", "one_hot", "=", "[", "0", "]", "*", "len", "(", "class_labels", ")", "\n", "one_hot", "[", "label", "]", "=", "1", "\n", "\n", "labels", "[", "idx", "]", "=", "torch", ".", "tensor", "(", "one_hot", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "\n", "", "if", "save_directory", ":", "\n", "            ", "torch", ".", "save", "(", "bert_input_ids", ",", "os", ".", "path", ".", "join", "(", "save_directory", ",", "f\"bert_input_ids.pt\"", ")", ")", "\n", "torch", ".", "save", "(", "bert_token_type_ids", ",", "os", ".", "path", ".", "join", "(", "save_directory", ",", "f\"bert_token_type_ids.pt\"", ")", ")", "\n", "torch", ".", "save", "(", "bert_attention_masks", ",", "os", ".", "path", ".", "join", "(", "save_directory", ",", "f\"bert_attention_masks.pt\"", ")", ")", "\n", "torch", ".", "save", "(", "labels", ",", "os", ".", "path", ".", "join", "(", "save_directory", ",", "f\"labels.pt\"", ")", ")", "\n", "torch", ".", "save", "(", "sorted", "(", "class_labels", ")", ",", "os", ".", "path", ".", "join", "(", "save_directory", ",", "f\"class_labels.pt\"", ")", ")", "\n", "\n", "", "return", "(", "bert_input_ids", ",", "bert_token_type_ids", ",", "bert_attention_masks", ")", ",", "labels", ",", "sorted", "(", "class_labels", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.data.sentence_pair_classification_dataset._truncate_seq_pair": [[5, 23], ["len", "len", "len", "len", "tokens_a.pop", "tokens_b.pop"], "function", ["None"], ["def", "_truncate_seq_pair", "(", "tokens_a", ",", "tokens_b", ",", "max_length", ":", "int", ")", ":", "\n", "    ", "\"\"\"\n    Copied exactly from: https://github.com/huggingface/pytorch-pretrained-BERT/blob/78462aad6113d50063d8251e27dbaadb7f44fbf0/examples/extract_features.py#L150\n    Truncates a sequence pair in place to the maximum length.\n    \"\"\"", "\n", "\n", "# This is a simple heuristic which will always truncate the longer sequence", "\n", "# one token at a time. This makes more sense than truncating an equal percent", "\n", "# of tokens from each, since if one sequence is very short then each token", "\n", "# that's truncated likely contains more information than a longer sequence.", "\n", "while", "True", ":", "\n", "        ", "total_length", "=", "len", "(", "tokens_a", ")", "+", "len", "(", "tokens_b", ")", "\n", "if", "total_length", "<=", "max_length", ":", "\n", "            ", "break", "\n", "", "if", "len", "(", "tokens_a", ")", ">", "len", "(", "tokens_b", ")", ":", "\n", "            ", "tokens_a", ".", "pop", "(", ")", "\n", "", "else", ":", "\n", "            ", "tokens_b", ".", "pop", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.data.ner_dataset.NERDataset.__init__": [[15, 26], ["torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.path.join"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "dataset_directory", ":", "str", ",", ")", ":", "\n", "        ", "self", ".", "dataset_directory", "=", "dataset_directory", "\n", "self", ".", "entity_labels", "=", "torch", ".", "load", "(", "os", ".", "path", ".", "join", "(", "self", ".", "dataset_directory", ",", "'entity_names.pl'", ")", ")", "\n", "\n", "self", ".", "bert_input_ids", "=", "torch", ".", "load", "(", "os", ".", "path", ".", "join", "(", "self", ".", "dataset_directory", ",", "f\"bert_input.pt\"", ")", ")", "\n", "self", ".", "bert_attention_masks", "=", "torch", ".", "load", "(", "os", ".", "path", ".", "join", "(", "self", ".", "dataset_directory", ",", "f\"bert_attention_mask.pt\"", ")", ")", "\n", "self", ".", "bert_sequence_lengths", "=", "torch", ".", "load", "(", "os", ".", "path", ".", "join", "(", "self", ".", "dataset_directory", ",", "f\"bert_sequence_length.pt\"", ")", ")", "\n", "self", ".", "bert_labels", "=", "torch", ".", "load", "(", "os", ".", "path", ".", "join", "(", "self", ".", "dataset_directory", ",", "f\"bert_labels.pt\"", ")", ")", "\n", "self", ".", "spacy_labels", "=", "torch", ".", "load", "(", "os", ".", "path", ".", "join", "(", "self", ".", "dataset_directory", ",", "f\"spacy_labels.pt\"", ")", ")", "\n", "self", ".", "bert_alignment", "=", "torch", ".", "load", "(", "os", ".", "path", ".", "join", "(", "self", ".", "dataset_directory", ",", "f\"subword_to_spacy_alignment.pt\"", ")", ")", "\n", "self", ".", "loss_weights", "=", "torch", ".", "load", "(", "os", ".", "path", ".", "join", "(", "self", ".", "dataset_directory", ",", "f\"loss_weights.pt\"", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.data.ner_dataset.NERDataset.__len__": [[27, 30], ["None"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "'Denotes the total number of samples'", "\n", "return", "self", ".", "bert_input_ids", ".", "shape", "[", "0", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.data.ner_dataset.NERDataset.__getitem__": [[31, 36], ["None"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "index", ":", "int", ")", ":", "\n", "# Load data and get label, pad with [] (cannot use None) to align with LM inputs (we don't need token types for subword classification).", "\n", "        ", "return", "self", ".", "bert_input_ids", "[", "index", "]", ",", "[", "]", ",", "self", ".", "bert_attention_masks", "[", "index", "]", ",", "self", ".", "bert_sequence_lengths", "[", "index", "]", ",", "self", ".", "bert_labels", "[", "index", "]", ",", "self", ".", "spacy_labels", "[", "index", "]", ",", "self", ".", "bert_alignment", "[", "index", "]", ",", "self", ".", "loss_weights", "\n", "\n"]], "home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.data.ner_dataset.NERDataset.create_ner_dataset": [[39, 235], ["torch.FloatTensor", "torch.abs", "zip", "max", "torch.zeros", "torch.zeros_like", "torch.zeros", "torch.zeros_like", "torch.zeros_like", "torch.zeros", "enumerate", "token_sequences.append", "label_sequences.append", "sorted", "set", "set", "sorted", "sorted", "len", "warnings.warn", "zip", "tokenizer.convert_tokens_to_ids", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "range", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "len", "len", "spacy.gold.biluo_tags_from_offsets", "token_sequences.append", "label_sequences.append", "sorted.index", "len", "len", "token_idx_to_subwords.append", "sorted.index", "sorted.index", "len", "all_bert_subword_sequences.append", "all_bert_label_sequences.append", "all_bert_sequence_alignments.append", "original_tokenization_labels.append", "len", "len", "BaseException", "len", "len", "tokenizer.convert_tokens_to_ids.append", "attention_masks.append", "bert_label_sequence.append", "alignment.append", "len", "original_tokenization_label.append", "sum", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "set.add", "sorted.add", "list", "len", "original_tokens_processed.append", "all_bert_subword_sequences.append", "all_bert_label_sequences.append", "all_bert_sequence_alignments.append", "original_tokenization_labels.append", "len", "len", "len", "len", "annotations[].values", "len", "len", "len", "sorted.index", "sorted.index", "sorted.index", "tokenizer.tokenize", "len", "str"], "methods", ["home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.heads.heads.TransformerHead.save", "home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.heads.heads.TransformerHead.save", "home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.heads.heads.TransformerHead.save", "home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.heads.heads.TransformerHead.save", "home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.heads.heads.TransformerHead.save", "home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.heads.heads.TransformerHead.save", "home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.heads.heads.TransformerHead.save", "home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.heads.heads.TransformerHead.save"], ["", "@", "staticmethod", "\n", "def", "create_ner_dataset", "(", "data", ",", "\n", "tokenizer", ":", "Union", "[", "BertTokenizer", ",", "AlbertTokenizer", "]", ",", "\n", "save_directory", "=", "None", ",", "\n", "max_sequence_length", "=", "512", ",", "\n", "conll_format", "=", "False", ",", ")", "->", "Tuple", "[", "Tuple", "[", "torch", ".", "LongTensor", ",", "torch", ".", "LongTensor", ",", "\n", "torch", ".", "LongTensor", "]", ",", "torch", ".", "LongTensor", ",", "List", "[", "str", "]", "]", ":", "\n", "        ", "\"\"\"\n            Given a list of tuples of document with span level annotations, saves bert input and labels onto disk.\n            This method is designed as a pre-processing step to be utilized with a pytorch Dataset and Dataloader.\n\n            :param data:  a list of tuples relating a document to its set of annotations.\n            :param tokenizer: the transformers tokenizer to utilize.\n            :param conll_format: set true if data is a tuple containing parallel arrays of tokens and labels and list of entities\n            :return the location the dataset was saved\n            \"\"\"", "\n", "# TODO insure sequences are not split on token boundaries.", "\n", "if", "conll_format", ":", "\n", "            ", "assert", "len", "(", "data", ")", "==", "3", ",", "\"Should contain list of tokens, tags and list of bilou entities\"", "\n", "token_sequences", "=", "[", "]", "\n", "label_sequences", "=", "[", "]", "\n", "token_sequence", "=", "data", "[", "0", "]", "\n", "label_sequence", "=", "data", "[", "1", "]", "\n", "token_sequences", ".", "append", "(", "token_sequence", ")", "\n", "label_sequences", ".", "append", "(", "label_sequence", ")", "\n", "biluo_ordered_labels", "=", "sorted", "(", "[", "entity_label", "for", "entity_label", "in", "data", "[", "2", "]", "if", "entity_label", "!=", "'O'", "]", "+", "[", "'O'", ",", "'BERT_TOKEN'", "]", ")", "\n", "tags_from_annotations", "=", "biluo_ordered_labels", "\n", "\n", "", "else", ":", "#custom spacy format", "\n", "            ", "assert", "len", "(", "data", ")", ">", "1", "\n", "assert", "'entities'", "in", "data", "[", "0", "]", "[", "1", "]", "\n", "assert", "'entity_labels'", "in", "data", "[", "0", "]", "[", "1", "]", "\n", "token_sequences", "=", "[", "]", "\n", "label_sequences", "=", "[", "]", "\n", "\n", "entity_labels", "=", "set", "(", ")", "\n", "tags_from_annotations", "=", "set", "(", ")", "\n", "\n", "for", "doc", ",", "annotations", "in", "data", ":", "\n", "                ", "for", "label", "in", "annotations", "[", "'entity_labels'", "]", ":", "\n", "                    ", "entity_labels", ".", "add", "(", "label", ")", "\n", "", "offsets", "=", "[", "offset", "for", "annotation", "in", "annotations", "[", "'entities'", "]", ".", "values", "(", ")", "for", "offset", "in", "annotation", "]", "\n", "tags", "=", "biluo_tags_from_offsets", "(", "doc", ",", "offsets", ")", "\n", "for", "tag", "in", "tags", ":", "\n", "                    ", "tags_from_annotations", ".", "add", "(", "tag", ")", "\n", "\n", "", "token_sequences", ".", "append", "(", "[", "x", "for", "x", "in", "doc", "]", ")", "\n", "label_sequences", ".", "append", "(", "tags", ")", "\n", "\n", "", "biluo_ordered_labels", "=", "sorted", "(", "[", "f\"{prefix}-{entity_label}\"", "for", "prefix", "in", "[", "'B'", ",", "'I'", ",", "'L'", ",", "'U'", "]", "\n", "for", "entity_label", "in", "entity_labels", "if", "entity_label", "!=", "'O'", "]", "+", "[", "'O'", ",", "'BERT_TOKEN'", "]", ")", "\n", "tags_from_annotations", "=", "sorted", "(", "list", "(", "tags_from_annotations", ")", "+", "[", "'BERT_TOKEN'", "]", ")", "\n", "\n", "\n", "# convert each string label to a unique id with respect to the biluo_labels of the tokenization", "\n", "", "encoded_label_sequences", "=", "[", "[", "biluo_ordered_labels", ".", "index", "(", "label", ")", "for", "label", "in", "seq", "]", "for", "seq", "in", "label_sequences", "]", "\n", "\n", "class_counts", "=", "[", "0", "]", "*", "len", "(", "biluo_ordered_labels", ")", "\n", "\n", "for", "seq", "in", "encoded_label_sequences", ":", "\n", "            ", "for", "id", "in", "seq", ":", "\n", "                ", "class_counts", "[", "id", "]", "+=", "1", "\n", "\n", "", "", "class_counts", "=", "torch", ".", "FloatTensor", "(", "class_counts", ")", "\n", "loss_weights", "=", "torch", ".", "abs", "(", "1", "-", "(", "class_counts", "/", "len", "(", "[", "x", "for", "x", "in", "seq", "for", "seq", "in", "encoded_label_sequences", "]", ")", ")", ")", "\n", "# Assert that all labels appear in the annotations. This could occur if annotation processing could not align", "\n", "# all annotations into the defined spacy tokenization.", "\n", "if", "biluo_ordered_labels", "!=", "tags_from_annotations", ":", "\n", "            ", "warnings", ".", "warn", "(", "\"Processed dataset does not contain instances from all labels when converted to BILOU scheme.\"", ")", "\n", "\n", "# Now generate bert input tensors", "\n", "", "all_bert_sequence_alignments", ",", "all_bert_subword_sequences", ",", "all_bert_label_sequences", ",", "original_tokenization_labels", "=", "[", "]", ",", "[", "]", ",", "[", "]", ",", "[", "]", "\n", "\n", "\n", "for", "sequence", ",", "labels", "in", "zip", "(", "token_sequences", ",", "encoded_label_sequences", ")", ":", "\n", "\n", "# alignment from the bert tokenization to spaCy tokenization", "\n", "            ", "assert", "len", "(", "sequence", ")", "==", "len", "(", "labels", ")", "\n", "\n", "#maps each original token to it's subwords", "\n", "token_idx_to_subwords", "=", "[", "]", "\n", "for", "token", "in", "sequence", ":", "\n", "                ", "token_idx_to_subwords", ".", "append", "(", "[", "subword", "for", "subword", "in", "tokenizer", ".", "tokenize", "(", "str", "(", "token", ")", ")", "]", ")", "\n", "\n", "#token_idx_to_subwords = [seq for seq in token_idx_to_subwords if seq]", "\n", "", "bert_subwords", "=", "[", "'[CLS]'", ",", "'[SEP]'", "]", "\n", "bert_subword_labels", "=", "[", "biluo_ordered_labels", ".", "index", "(", "'BERT_TOKEN'", ")", ",", "biluo_ordered_labels", ".", "index", "(", "'BERT_TOKEN'", ")", "]", "\n", "bert_subword_to_original_tokenization_alignment", "=", "[", "-", "1", ",", "-", "1", "]", "\n", "original_tokens_processed", "=", "[", "]", "\n", "\n", "# print(token_idx_to_subwords[:10])", "\n", "# print([str(token) for token in sequence][:10])", "\n", "# exit()", "\n", "idx", "=", "0", "\n", "chunk_start", "=", "0", "\n", "while", "idx", "<", "len", "(", "sequence", ")", ":", "\n", "\n", "                ", "start_next_buffer", "=", "False", "\n", "token_in_buffer_size", "=", "len", "(", "bert_subwords", ")", "+", "len", "(", "token_idx_to_subwords", "[", "idx", "]", ")", "<=", "max_sequence_length", "\n", "\n", "if", "token_in_buffer_size", ":", "\n", "#build a sequence", "\n", "                    ", "bert_subwords", "[", "-", "1", ":", "-", "1", "]", "=", "[", "subword", "for", "subword", "in", "token_idx_to_subwords", "[", "idx", "]", "]", "\n", "bert_subword_labels", "[", "-", "1", ":", "-", "1", "]", "=", "[", "labels", "[", "idx", "]", "for", "_", "in", "token_idx_to_subwords", "[", "idx", "]", "]", "\n", "bert_subword_to_original_tokenization_alignment", "[", "-", "1", ":", "-", "1", "]", "=", "[", "idx", "-", "chunk_start", "for", "_", "in", "token_idx_to_subwords", "[", "idx", "]", "]", "\n", "original_tokens_processed", ".", "append", "(", "idx", ")", "\n", "idx", "+=", "1", "\n", "\n", "#Insure we aren't splitting on a label by greedily splitting on 'O' labels once the buffer gets very full (>500 subwords)", "\n", "", "if", "len", "(", "bert_subwords", ")", ">", "500", "and", "labels", "[", "idx", "-", "1", "]", "==", "biluo_ordered_labels", ".", "index", "(", "'O'", ")", ":", "\n", "                    ", "start_next_buffer", "=", "True", "\n", "\n", "", "if", "not", "token_in_buffer_size", "or", "start_next_buffer", ":", "\n", "                    ", "all_bert_subword_sequences", ".", "append", "(", "bert_subwords", ")", "\n", "all_bert_label_sequences", ".", "append", "(", "bert_subword_labels", ")", "\n", "all_bert_sequence_alignments", ".", "append", "(", "bert_subword_to_original_tokenization_alignment", ")", "\n", "\n", "original_tokenization_labels", ".", "append", "(", "[", "labels", "[", "i", "]", "for", "i", "in", "original_tokens_processed", "]", ")", "\n", "\n", "#reset sequence builders", "\n", "bert_subwords", "=", "[", "'[CLS]'", ",", "'[SEP]'", "]", "\n", "bert_subword_labels", "=", "[", "biluo_ordered_labels", ".", "index", "(", "'BERT_TOKEN'", ")", ",", "biluo_ordered_labels", ".", "index", "(", "'BERT_TOKEN'", ")", "]", "\n", "bert_subword_to_original_tokenization_alignment", "=", "[", "-", "1", ",", "-", "1", "]", "\n", "original_tokens_processed", "=", "[", "]", "\n", "chunk_start", "=", "idx", "\n", "\n", "", "", "if", "bert_subwords", "!=", "[", "'[CLS]'", ",", "'[SEP]'", "]", ":", "\n", "#Add the remaining", "\n", "                ", "all_bert_subword_sequences", ".", "append", "(", "bert_subwords", ")", "\n", "all_bert_label_sequences", ".", "append", "(", "bert_subword_labels", ")", "\n", "all_bert_sequence_alignments", ".", "append", "(", "bert_subword_to_original_tokenization_alignment", ")", "\n", "original_tokenization_labels", ".", "append", "(", "[", "labels", "[", "i", "]", "for", "i", "in", "original_tokens_processed", "]", ")", "\n", "\n", "", "", "for", "seq", "in", "original_tokenization_labels", ":", "\n", "            ", "for", "label", "in", "seq", ":", "\n", "                ", "assert", "label", "!=", "-", "1", "\n", "\n", "", "", "max_num_spacy_labels", "=", "max", "(", "[", "len", "(", "seq", ")", "for", "seq", "in", "original_tokenization_labels", "]", ")", "\n", "\n", "bert_input_ids", "=", "torch", ".", "zeros", "(", "size", "=", "(", "len", "(", "all_bert_subword_sequences", ")", ",", "max_sequence_length", ")", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "bert_attention_masks", "=", "torch", ".", "zeros_like", "(", "bert_input_ids", ")", "\n", "bert_sequence_lengths", "=", "torch", ".", "zeros", "(", "size", "=", "(", "len", "(", "all_bert_subword_sequences", ")", ",", "1", ")", ")", "\n", "\n", "bert_labels", "=", "torch", ".", "zeros_like", "(", "bert_input_ids", ")", "\n", "bert_alignment", "=", "torch", ".", "zeros_like", "(", "bert_input_ids", ")", "\n", "gold_original_token_labels", "=", "torch", ".", "zeros", "(", "size", "=", "(", "len", "(", "all_bert_subword_sequences", ")", ",", "max_num_spacy_labels", ")", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "\n", "for", "idx", ",", "(", "bert_subword_sequence", ",", "bert_label_sequence", ",", "alignment", ",", "original_tokenization_label", ")", "in", "enumerate", "(", "zip", "(", "all_bert_subword_sequences", ",", "all_bert_label_sequences", ",", "all_bert_sequence_alignments", ",", "original_tokenization_labels", ")", ")", ":", "\n", "            ", "if", "len", "(", "bert_subword_sequence", ")", ">", "512", ":", "\n", "                ", "raise", "BaseException", "(", "\"Error sequence at index %i as it is to long (%i tokens)\"", "%", "(", "idx", ",", "len", "(", "bert_subword_sequence", ")", ")", ")", "\n", "", "input_ids", "=", "tokenizer", ".", "convert_tokens_to_ids", "(", "bert_subword_sequence", ")", "\n", "attention_masks", "=", "[", "1", "]", "*", "len", "(", "input_ids", ")", "\n", "\n", "while", "len", "(", "input_ids", ")", "<", "max_sequence_length", ":", "#pad bert aligned input until max length", "\n", "                ", "input_ids", ".", "append", "(", "0", ")", "\n", "attention_masks", ".", "append", "(", "0", ")", "\n", "bert_label_sequence", ".", "append", "(", "0", ")", "\n", "alignment", ".", "append", "(", "-", "1", ")", "\n", "", "while", "len", "(", "original_tokenization_label", ")", "<", "max_num_spacy_labels", ":", "#pad spacy aligned input with -1", "\n", "                ", "original_tokenization_label", ".", "append", "(", "-", "1", ")", "\n", "\n", "", "bert_input_ids", "[", "idx", "]", "=", "torch", ".", "tensor", "(", "input_ids", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "bert_attention_masks", "[", "idx", "]", "=", "torch", ".", "tensor", "(", "attention_masks", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "bert_alignment", "[", "idx", "]", "=", "torch", ".", "tensor", "(", "alignment", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "bert_sequence_lengths", "[", "idx", "]", "=", "torch", ".", "tensor", "(", "sum", "(", "[", "1", "for", "x", "in", "input_ids", "if", "x", "!=", "0", "]", ")", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "gold_original_token_labels", "[", "idx", "]", "=", "torch", ".", "tensor", "(", "original_tokenization_label", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "bert_labels", "[", "idx", "]", "=", "torch", ".", "tensor", "(", "bert_label_sequence", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "\n", "\n", "for", "i", "in", "range", "(", "1", ",", "len", "(", "bert_labels", "[", "idx", "]", ")", "-", "1", ")", ":", "\n", "# print()", "\n", "# print(f\"Bert Labels | {i} | {bert_labels[idx][i]}\")", "\n", "# print(f\"Correct Original Labels | {i} | {gold_original_token_labels[idx][bert_alignment[idx][i]]}\")", "\n", "# print(f\"Bert Labels: {bert_labels[idx]}\")", "\n", "# print(f\"Spacy Labels: {gold_original_token_labels[idx]}\")", "\n", "# print(f\"Bert Alignment: {bert_alignment[idx]}\")", "\n", "                ", "try", ":", "\n", "                    ", "assert", "bert_labels", "[", "idx", "]", "[", "i", "]", "==", "gold_original_token_labels", "[", "idx", "]", "[", "bert_alignment", "[", "idx", "]", "[", "i", "]", "]", "\n", "", "except", "BaseException", ":", "\n", "                    ", "pass", "\n", "\n", "\n", "\n", "", "", "", "if", "save_directory", ":", "\n", "            ", "torch", ".", "save", "(", "bert_input_ids", ",", "os", ".", "path", ".", "join", "(", "save_directory", ",", "f\"bert_input.pt\"", ")", ")", "#bert input ids", "\n", "torch", ".", "save", "(", "bert_attention_masks", ",", "os", ".", "path", ".", "join", "(", "save_directory", ",", "f\"bert_attention_mask.pt\"", ")", ")", "#bert attention masks", "\n", "torch", ".", "save", "(", "bert_sequence_lengths", ",", "os", ".", "path", ".", "join", "(", "save_directory", ",", "f\"bert_sequence_length.pt\"", ")", ")", "#length of actual bert sequence", "\n", "torch", ".", "save", "(", "bert_labels", ",", "os", ".", "path", ".", "join", "(", "save_directory", ",", "f\"bert_labels.pt\"", ")", ")", "#correct labels relative to bert tokenization", "\n", "torch", ".", "save", "(", "gold_original_token_labels", ",", "os", ".", "path", ".", "join", "(", "save_directory", ",", "f\"spacy_labels.pt\"", ")", ")", "#correct labels relative to spacy tokenization", "\n", "torch", ".", "save", "(", "bert_alignment", ",", "os", ".", "path", ".", "join", "(", "save_directory", ",", "f\"subword_to_spacy_alignment.pt\"", ")", ")", "#alignment between bert and spacy sequences", "\n", "torch", ".", "save", "(", "biluo_ordered_labels", ",", "os", ".", "path", ".", "join", "(", "save_directory", ",", "'entity_names.pl'", ")", ")", "#entity labels", "\n", "torch", ".", "save", "(", "loss_weights", ",", "os", ".", "path", ".", "join", "(", "save_directory", ",", "'loss_weights.pt'", ")", ")", "#global entity class counts", "\n", "\n", "", "return", "(", "bert_input_ids", ",", "None", ",", "bert_attention_masks", ")", ",", "bert_sequence_lengths", ",", "bert_labels", ",", "original_tokenization_labels", ",", "bert_alignment", ",", "biluo_ordered_labels", ",", "loss_weights", "\n", "\n"]], "home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.data.sentence_pair_regression_dataset.SentencePairRegressionDataset.__init__": [[25, 32], ["torch.load", "torch.load", "torch.load", "torch.load", "os.path.join", "os.path.join", "os.path.join", "os.path.join"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "dataset_directory", ":", "str", ")", ":", "\n", "        ", "self", ".", "dataset_directory", "=", "dataset_directory", "\n", "\n", "self", ".", "bert_input_ids", "=", "torch", ".", "load", "(", "os", ".", "path", ".", "join", "(", "self", ".", "dataset_directory", ",", "f\"bert_input_ids.pt\"", ")", ")", "\n", "self", ".", "bert_token_type_ids", "=", "torch", ".", "load", "(", "os", ".", "path", ".", "join", "(", "self", ".", "dataset_directory", ",", "f\"bert_token_type_ids.pt\"", ")", ")", "\n", "self", ".", "bert_attention_masks", "=", "torch", ".", "load", "(", "os", ".", "path", ".", "join", "(", "self", ".", "dataset_directory", ",", "f\"bert_attention_masks.pt\"", ")", ")", "\n", "self", ".", "scores", "=", "torch", ".", "load", "(", "os", ".", "path", ".", "join", "(", "self", ".", "dataset_directory", ",", "f\"scores.pt\"", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.data.sentence_pair_regression_dataset.SentencePairRegressionDataset.__len__": [[33, 36], ["None"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "'Denotes the total number of samples'", "\n", "return", "self", ".", "bert_input_ids", ".", "shape", "[", "0", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.data.sentence_pair_regression_dataset.SentencePairRegressionDataset.__getitem__": [[37, 41], ["None"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "index", ":", "int", ")", ":", "\n", "# Load data and get label", "\n", "        ", "return", "self", ".", "bert_input_ids", "[", "index", "]", ",", "self", ".", "bert_token_type_ids", "[", "index", "]", ",", "self", ".", "bert_attention_masks", "[", "index", "]", ",", "self", ".", "scores", "[", "index", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.data.sentence_pair_regression_dataset.SentencePairRegressionDataset.create_sentence_pair_dataset": [[42, 104], ["torch.empty", "torch.empty", "torch.empty", "torch.empty", "enumerate", "sentence_pair_regression_dataset._truncate_seq_pair", "max", "tokens.append", "input_type_ids.append", "tokens.append", "input_type_ids.append", "tokens.append", "input_type_ids.append", "tokenizer.convert_tokens_to_ids", "torch.tensor", "torch.tensor", "torch.tensor", "torch.save", "torch.save", "torch.save", "torch.save", "tokenizer.tokenize", "tokenizer.tokenize", "len", "len", "len", "len", "tokens.append", "input_type_ids.append", "tokens.append", "input_type_ids.append", "len", "len", "tokenizer.convert_tokens_to_ids.append", "attention_masks.append", "input_type_ids.append", "torch.tensor", "torch.tensor", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "float", "len", "len"], "methods", ["home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.data.sentence_pair_regression_dataset._truncate_seq_pair", "home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.heads.heads.TransformerHead.save", "home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.heads.heads.TransformerHead.save", "home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.heads.heads.TransformerHead.save", "home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.heads.heads.TransformerHead.save"], ["", "def", "create_sentence_pair_dataset", "(", "data", ":", "list", ",", "\n", "tokenizer", ":", "Union", "[", "BertTokenizer", ",", "AlbertTokenizer", "]", ",", "\n", "save_directory", "=", "None", ",", "\n", "max_sequence_length", "=", "128", ")", ":", "\n", "        ", "max_bert_input_length", "=", "0", "\n", "for", "sentence_pair", "in", "data", ":", "\n", "            ", "sentence_1_tokenized", ",", "sentence_2_tokenized", "=", "tokenizer", ".", "tokenize", "(", "\n", "sentence_pair", "[", "'sentence_1'", "]", ")", ",", "tokenizer", ".", "tokenize", "(", "sentence_pair", "[", "'sentence_2'", "]", ")", "\n", "_truncate_seq_pair", "(", "sentence_1_tokenized", ",", "sentence_2_tokenized", ",", "\n", "max_sequence_length", "-", "3", ")", "# accounting for positioning tokens", "\n", "\n", "max_bert_input_length", "=", "max", "(", "max_bert_input_length", ",", "\n", "len", "(", "sentence_1_tokenized", ")", "+", "len", "(", "sentence_2_tokenized", ")", "+", "3", ")", "\n", "sentence_pair", "[", "'sentence_1_tokenized'", "]", "=", "sentence_1_tokenized", "\n", "sentence_pair", "[", "'sentence_2_tokenized'", "]", "=", "sentence_2_tokenized", "\n", "\n", "", "bert_input_ids", "=", "torch", ".", "empty", "(", "(", "len", "(", "data", ")", ",", "max_bert_input_length", ")", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "bert_token_type_ids", "=", "torch", ".", "empty", "(", "(", "len", "(", "data", ")", ",", "max_bert_input_length", ")", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "bert_attention_masks", "=", "torch", ".", "empty", "(", "(", "len", "(", "data", ")", ",", "max_bert_input_length", ")", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "scores", "=", "torch", ".", "empty", "(", "(", "len", "(", "data", ")", ",", "1", ")", ",", "dtype", "=", "torch", ".", "float", ")", "\n", "\n", "for", "idx", ",", "sentence_pair", "in", "enumerate", "(", "data", ")", ":", "\n", "            ", "tokens", "=", "[", "]", "\n", "input_type_ids", "=", "[", "]", "\n", "\n", "tokens", ".", "append", "(", "\"[CLS]\"", ")", "\n", "input_type_ids", ".", "append", "(", "0", ")", "\n", "for", "token", "in", "sentence_pair", "[", "'sentence_1_tokenized'", "]", ":", "\n", "                ", "tokens", ".", "append", "(", "token", ")", "\n", "input_type_ids", ".", "append", "(", "0", ")", "\n", "", "tokens", ".", "append", "(", "\"[SEP]\"", ")", "\n", "input_type_ids", ".", "append", "(", "0", ")", "\n", "\n", "for", "token", "in", "sentence_pair", "[", "'sentence_2_tokenized'", "]", ":", "\n", "                ", "tokens", ".", "append", "(", "token", ")", "\n", "input_type_ids", ".", "append", "(", "1", ")", "\n", "", "tokens", ".", "append", "(", "\"[SEP]\"", ")", "\n", "input_type_ids", ".", "append", "(", "1", ")", "\n", "\n", "input_ids", "=", "tokenizer", ".", "convert_tokens_to_ids", "(", "tokens", ")", "\n", "\n", "attention_masks", "=", "[", "1", "]", "*", "len", "(", "input_ids", ")", "\n", "while", "len", "(", "input_ids", ")", "<", "max_bert_input_length", ":", "\n", "                ", "input_ids", ".", "append", "(", "0", ")", "\n", "attention_masks", ".", "append", "(", "0", ")", "\n", "input_type_ids", ".", "append", "(", "0", ")", "\n", "\n", "", "bert_input_ids", "[", "idx", "]", "=", "torch", ".", "tensor", "(", "input_ids", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "bert_token_type_ids", "[", "idx", "]", "=", "torch", ".", "tensor", "(", "input_type_ids", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "bert_attention_masks", "[", "idx", "]", "=", "torch", ".", "tensor", "(", "attention_masks", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "if", "'similarity'", "not", "in", "sentence_pair", "or", "sentence_pair", "[", "'similarity'", "]", "is", "None", ":", "\n", "                ", "scores", "[", "idx", "]", "=", "torch", ".", "tensor", "(", "float", "(", "'nan'", ")", ",", "dtype", "=", "torch", ".", "float", ")", "\n", "", "else", ":", "\n", "                ", "scores", "[", "idx", "]", "=", "torch", ".", "tensor", "(", "sentence_pair", "[", "'similarity'", "]", ",", "dtype", "=", "torch", ".", "float", ")", "\n", "\n", "", "", "if", "save_directory", ":", "\n", "            ", "torch", ".", "save", "(", "bert_input_ids", ",", "os", ".", "path", ".", "join", "(", "save_directory", ",", "f\"bert_input_ids.pt\"", ")", ")", "\n", "torch", ".", "save", "(", "bert_token_type_ids", ",", "os", ".", "path", ".", "join", "(", "save_directory", ",", "f\"bert_token_type_ids.pt\"", ")", ")", "\n", "torch", ".", "save", "(", "bert_attention_masks", ",", "os", ".", "path", ".", "join", "(", "save_directory", ",", "f\"bert_attention_masks.pt\"", ")", ")", "\n", "torch", ".", "save", "(", "scores", ",", "os", ".", "path", ".", "join", "(", "save_directory", ",", "f\"scores.pt\"", ")", ")", "\n", "\n", "", "return", "(", "bert_input_ids", ",", "bert_token_type_ids", ",", "bert_attention_masks", ")", ",", "scores", "\n", "", "", ""]], "home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.data.sentence_pair_regression_dataset._truncate_seq_pair": [[5, 23], ["len", "len", "len", "len", "tokens_a.pop", "tokens_b.pop"], "function", ["None"], ["def", "_truncate_seq_pair", "(", "tokens_a", ",", "tokens_b", ",", "max_length", ":", "int", ")", ":", "\n", "    ", "\"\"\"\n    Copied exactly from: https://github.com/huggingface/pytorch-pretrained-BERT/blob/78462aad6113d50063d8251e27dbaadb7f44fbf0/examples/extract_features.py#L150\n    Truncates a sequence pair in place to the maximum length.\n    \"\"\"", "\n", "\n", "# This is a simple heuristic which will always truncate the longer sequence", "\n", "# one token at a time. This makes more sense than truncating an equal percent", "\n", "# of tokens from each, since if one sequence is very short then each token", "\n", "# that's truncated likely contains more information than a longer sequence.", "\n", "while", "True", ":", "\n", "        ", "total_length", "=", "len", "(", "tokens_a", ")", "+", "len", "(", "tokens_b", ")", "\n", "if", "total_length", "<=", "max_length", ":", "\n", "            ", "break", "\n", "", "if", "len", "(", "tokens_a", ")", ">", "len", "(", "tokens_b", ")", ":", "\n", "            ", "tokens_a", ".", "pop", "(", ")", "\n", "", "else", ":", "\n", "            ", "tokens_b", ".", "pop", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.evaluation.evaluate.evaluate_ner": [[9, 79], ["enumerate", "sklearn.metrics.f1_score", "sklearn.metrics.precision_score", "sklearn.metrics.recall_score", "sklearn.metrics.f1_score", "sklearn.metrics.f1_score", "mlflow.log_metric", "mlflow.log_metric", "log.info", "log.info", "int", "len", "len", "enumerate", "enumerate", "mlflow.log_metric", "mlflow.log_metric", "mlflow.log_metric", "float", "float", "entity_labels.index", "filtered_labels_indices.append", "str", "str", "entity_labels.append", "entity_labels.index", "entity_labels.index", "entity_labels.append", "enumerate", "enumerate", "enumerate", "entity_labels[].replace", "entity_labels[].replace", "entity_labels[].replace"], "function", ["None"], ["def", "evaluate_ner", "(", "predicted_and_correct", ":", "List", "[", "Tuple", "[", "List", ",", "List", "]", "]", ",", "\n", "bilou_labels", ":", "List", ",", "\n", "head_identifier", ",", "\n", "evaluate_bilou", "=", "False", ",", "\n", "step", "=", "1", ")", ":", "\n", "    ", "\"\"\"\n\n    :param predicted_and_correct:\n    :param bilou_labels: list of bilou labels\n    :param head_identifier: head name\n    :param evaluate_bilou: If true, will evaluate over bilou labels, otherwise will strip prefixes and eval over original.\n    :param step: current epoch for logging\n    :return:\n    \"\"\"", "\n", "flattened_predictions", "=", "[", "int", "(", "x", ")", "for", "tuple", "in", "predicted_and_correct", "for", "x", "in", "tuple", "[", "0", "]", "]", "\n", "flattened_gold_labels", "=", "[", "x", "for", "tuple", "in", "predicted_and_correct", "for", "x", "in", "tuple", "[", "1", "]", "]", "\n", "\n", "assert", "len", "(", "flattened_predictions", ")", "==", "len", "(", "flattened_gold_labels", ")", "\n", "\n", "\n", "#convert BILOU labels back to original by mapping all BILOU tags to original entity tags by stripping prefixes.", "\n", "entity_labels", "=", "[", "]", "\n", "bilou_to_entity", "=", "{", "}", "# maps biluou indices to entity indices.", "\n", "if", "evaluate_bilou", ":", "\n", "        ", "entity_labels", "=", "bilou_labels", "\n", "for", "idx", ",", "label", "in", "enumerate", "(", "bilou_labels", ")", ":", "\n", "            ", "bilou_to_entity", "[", "idx", "]", "=", "entity_labels", ".", "index", "(", "label", ")", "\n", "", "", "else", ":", "\n", "        ", "for", "idx", ",", "label", "in", "enumerate", "(", "bilou_labels", ")", ":", "\n", "            ", "if", "label", "==", "'O'", "or", "label", "==", "'BERT_TOKEN'", ":", "\n", "                ", "entity_labels", ".", "append", "(", "label", ")", "\n", "bilou_to_entity", "[", "idx", "]", "=", "entity_labels", ".", "index", "(", "label", ")", "\n", "", "else", ":", "\n", "                ", "if", "label", "[", "2", ":", "]", "not", "in", "entity_labels", ":", "# remove prefix", "\n", "                    ", "entity_labels", ".", "append", "(", "label", "[", "2", ":", "]", ")", "\n", "", "bilou_to_entity", "[", "idx", "]", "=", "entity_labels", ".", "index", "(", "label", "[", "2", ":", "]", ")", "\n", "\n", "\n", "", "", "", "filtered_labels_indices", "=", "[", "]", "\n", "for", "idx", ",", "label", "in", "enumerate", "(", "entity_labels", ")", ":", "\n", "        ", "if", "not", "label", "in", "[", "'BERT_TOKEN'", ",", "'O'", "]", ":", "\n", "            ", "filtered_labels_indices", ".", "append", "(", "idx", ")", "\n", "\n", "\n", "#map back to original", "\n", "", "", "flattened_predictions", "=", "[", "bilou_to_entity", "[", "label_idx", "]", "for", "label_idx", "in", "flattened_predictions", "]", "\n", "flattened_gold_labels", "=", "[", "bilou_to_entity", "[", "label_idx", "]", "for", "label_idx", "in", "flattened_gold_labels", "]", "\n", "\n", "\n", "#Compute metrics", "\n", "f_measures", "=", "f1_score", "(", "flattened_gold_labels", ",", "flattened_predictions", ",", "average", "=", "None", ",", "\n", "labels", "=", "[", "idx", "for", "idx", ",", "label", "in", "enumerate", "(", "entity_labels", ")", "]", ")", "\n", "precisions", "=", "precision_score", "(", "flattened_gold_labels", ",", "flattened_predictions", ",", "average", "=", "None", ",", "\n", "labels", "=", "[", "idx", "for", "idx", ",", "label", "in", "enumerate", "(", "entity_labels", ")", "]", ")", "\n", "recalls", "=", "recall_score", "(", "flattened_gold_labels", ",", "flattened_predictions", ",", "average", "=", "None", ",", "\n", "labels", "=", "[", "idx", "for", "idx", ",", "label", "in", "enumerate", "(", "entity_labels", ")", "]", ")", "\n", "\n", "for", "label_idx", "in", "filtered_labels_indices", ":", "\n", "        ", "mlflow", ".", "log_metric", "(", "f\"Precision/{entity_labels[label_idx].replace(' ', '_')}/{head_identifier}\"", ",", "precisions", "[", "label_idx", "]", ",", "step", "=", "step", ")", "\n", "mlflow", ".", "log_metric", "(", "f\"Recall/{entity_labels[label_idx].replace(' ', '_')}/{head_identifier}\"", ",", "recalls", "[", "label_idx", "]", ",", "step", "=", "step", ")", "\n", "mlflow", ".", "log_metric", "(", "f\"F1/{entity_labels[label_idx].replace(' ', '_')}/{head_identifier}\"", ",", "f_measures", "[", "label_idx", "]", ",", "step", "=", "step", ")", "\n", "\n", "# exclude bert_token, other in macro/micro calculation", "\n", "", "micro_f1", "=", "f1_score", "(", "flattened_gold_labels", ",", "flattened_predictions", ",", "average", "=", "'micro'", ",", "labels", "=", "filtered_labels_indices", ")", "\n", "macro_f1", "=", "f1_score", "(", "flattened_gold_labels", ",", "flattened_predictions", ",", "average", "=", "'macro'", ",", "labels", "=", "filtered_labels_indices", ")", "\n", "\n", "mlflow", ".", "log_metric", "(", "f\"Macro-F1/{head_identifier}\"", ",", "float", "(", "macro_f1", ")", ",", "step", "=", "step", ")", "\n", "mlflow", ".", "log_metric", "(", "f\"Micro-F1/{head_identifier}\"", ",", "float", "(", "micro_f1", ")", ",", "step", "=", "step", ")", "\n", "log", ".", "info", "(", "\"Micro-F1: \"", "+", "str", "(", "micro_f1", ")", ")", "\n", "log", ".", "info", "(", "\"Macro-F1: \"", "+", "str", "(", "macro_f1", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.evaluation.evaluate.evaluate_sts": [[81, 85], ["mlflow.log_metric", "log.info", "scipy.stats.pearsonr", "float"], "function", ["None"], ["", "def", "evaluate_sts", "(", "predicted_and_correct", ":", "Tuple", "[", "List", ",", "List", "]", ",", "head_identifier", ",", "step", "=", "1", ")", ":", "\n", "    ", "correlation", "=", "pearsonr", "(", "predicted_and_correct", "[", "0", "]", ",", "predicted_and_correct", "[", "1", "]", ")", "[", "0", "]", "\n", "mlflow", ".", "log_metric", "(", "f\"PearsonRho/{head_identifier}\"", ",", "float", "(", "correlation", ")", ",", "step", "=", "step", ")", "\n", "log", ".", "info", "(", "f\"Correlation: {correlation}\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.evaluation.evaluate.evaluate_classification": [[86, 117], ["sklearn.metrics.f1_score", "sklearn.metrics.precision_score", "sklearn.metrics.recall_score", "sklearn.metrics.f1_score", "sklearn.metrics.f1_score", "sklearn.metrics.accuracy_score", "mlflow.log_metric", "mlflow.log_metric", "mlflow.log_metric", "log.info", "log.info", "log.info", "mlflow.log_metric", "mlflow.log_metric", "mlflow.log_metric", "float", "float", "float", "enumerate", "str", "str", "str", "enumerate", "enumerate", "enumerate", "labels[].replace", "labels[].replace", "labels[].replace"], "function", ["None"], ["", "def", "evaluate_classification", "(", "predicted_and_correct", ":", "Tuple", "[", "List", ",", "List", "]", ",", "labels", ":", "List", ",", "head_identifier", ",", "step", "=", "1", ")", ":", "\n", "\n", "    ", "label_indices", "=", "[", "idx", "for", "idx", ",", "label", "in", "enumerate", "(", "labels", ")", "]", "\n", "\n", "f_measures", "=", "f1_score", "(", "predicted_and_correct", "[", "1", "]", ",", "predicted_and_correct", "[", "0", "]", ",", "average", "=", "None", ",", "\n", "labels", "=", "[", "idx", "for", "idx", ",", "label", "in", "enumerate", "(", "labels", ")", "]", ")", "\n", "precisions", "=", "precision_score", "(", "predicted_and_correct", "[", "1", "]", ",", "predicted_and_correct", "[", "0", "]", ",", "average", "=", "None", ",", "\n", "labels", "=", "[", "idx", "for", "idx", ",", "label", "in", "enumerate", "(", "labels", ")", "]", ")", "\n", "recalls", "=", "recall_score", "(", "predicted_and_correct", "[", "1", "]", ",", "predicted_and_correct", "[", "0", "]", ",", "average", "=", "None", ",", "\n", "labels", "=", "[", "idx", "for", "idx", ",", "label", "in", "enumerate", "(", "labels", ")", "]", ")", "\n", "\n", "\n", "for", "label_idx", "in", "label_indices", ":", "\n", "        ", "mlflow", ".", "log_metric", "(", "f\"Precision/{labels[label_idx].replace(' ', '_')}/{head_identifier}\"", ",", "precisions", "[", "label_idx", "]", ",", "\n", "step", "=", "step", ")", "\n", "mlflow", ".", "log_metric", "(", "f\"Recall/{labels[label_idx].replace(' ', '_')}/{head_identifier}\"", ",", "recalls", "[", "label_idx", "]", ",", "\n", "step", "=", "step", ")", "\n", "mlflow", ".", "log_metric", "(", "f\"F1/{labels[label_idx].replace(' ', '_')}/{head_identifier}\"", ",", "f_measures", "[", "label_idx", "]", ",", "\n", "step", "=", "step", ")", "\n", "\n", "# exclude bert_token, other in macro/micro calculation", "\n", "", "micro_f1", "=", "f1_score", "(", "predicted_and_correct", "[", "1", "]", ",", "predicted_and_correct", "[", "0", "]", ",", "average", "=", "'micro'", ",", "labels", "=", "label_indices", ")", "\n", "macro_f1", "=", "f1_score", "(", "predicted_and_correct", "[", "1", "]", ",", "predicted_and_correct", "[", "0", "]", ",", "average", "=", "'macro'", ",", "labels", "=", "label_indices", ")", "\n", "accuracy", "=", "accuracy_score", "(", "predicted_and_correct", "[", "1", "]", ",", "predicted_and_correct", "[", "0", "]", ")", "\n", "\n", "mlflow", ".", "log_metric", "(", "f\"Macro-F1/{head_identifier}\"", ",", "float", "(", "macro_f1", ")", ",", "step", "=", "step", ")", "\n", "mlflow", ".", "log_metric", "(", "f\"Micro-F1/{head_identifier}\"", ",", "float", "(", "micro_f1", ")", ",", "step", "=", "step", ")", "\n", "mlflow", ".", "log_metric", "(", "f\"Accuracy/{head_identifier}\"", ",", "float", "(", "accuracy", ")", ",", "step", "=", "step", ")", "\n", "log", ".", "info", "(", "\"Micro-F1: \"", "+", "str", "(", "micro_f1", ")", ")", "\n", "log", ".", "info", "(", "\"Macro-F1: \"", "+", "str", "(", "macro_f1", ")", ")", "\n", "log", ".", "info", "(", "\"Accuracy: \"", "+", "str", "(", "accuracy", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.evaluation.mlflow.MLFlowRun.__init__": [[8, 21], ["mlflow.MLFlowRun.compute_run_statistics"], "methods", ["home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.evaluation.mlflow.MLFlowRun.compute_run_statistics"], ["def", "__init__", "(", "self", ",", "tracking_uri", "=", "None", ",", "experiment_name", "=", "None", ",", "run_id", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Pulls all statistics from tracking servers in an organized manner.\n        :param tracking_uri: the url of the tracking server\n        :param experiment_name: the name of the tracking server experiment\n        :param run_id: the uuid of the mlflow run\n        :return: a dictionary organized by task -> dataset -> task specific metrics\n        \"\"\"", "\n", "self", ".", "tracking_uri", "=", "tracking_uri", "\n", "self", ".", "experiment_name", "=", "experiment_name", "\n", "self", ".", "run_id", "=", "run_id", "\n", "\n", "self", ".", "metrics", "=", "self", ".", "compute_run_statistics", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.evaluation.mlflow.MLFlowRun.compute_run_statistics": [[22, 109], ["MlflowClient", "MlflowClient.search_runs", "list", "str", "run.data.metrics.keys", "[].startswith", "[].startswith", "[].startswith", "MlflowClient.get_metric_history", "[].startswith", "[].startswith", "[].startswith", "MlflowClient.list_experiments", "Exception", "len", "metric.split", "metric.split", "len", "metric.split", "metric.split", "metric.split", "metric.split", "metric.split", "metric.split", "metric.split", "metric.split", "[].split", "metric.split", "[].split", "metric.split", "metric.split", "metric.split", "metric.split", "metric.split", "metric.split", "[].split", "metric.split", "[].split", "[].split", "[].split", "[].split", "[].split", "metric.split", "metric.split", "metric.split", "metric.split", "metric.split", "metric.split", "metric.split", "metric.split"], "methods", ["None"], ["", "def", "compute_run_statistics", "(", "self", ")", ":", "\n", "        ", "from", "mlflow", ".", "tracking", ".", "client", "import", "MlflowClient", "\n", "\n", "client", "=", "MlflowClient", "(", "tracking_uri", "=", "self", ".", "tracking_uri", ")", "\n", "experiments", "=", "[", "str", "(", "exp", ".", "experiment_id", ")", "for", "exp", "in", "client", ".", "list_experiments", "(", ")", "if", "exp", ".", "name", "==", "f\"/{self.experiment_name}\"", "]", "\n", "\n", "runs", "=", "client", ".", "search_runs", "(", "\n", "experiment_ids", "=", "experiments", "\n", ")", "\n", "try", ":", "\n", "            ", "run", "=", "[", "run", "for", "run", "in", "runs", "if", "run", ".", "info", ".", "run_id", "==", "self", ".", "run_id", "]", "[", "0", "]", "\n", "", "except", "Exception", "as", "e", ":", "\n", "            ", "raise", "Exception", "(", "f\"No run with id '{self.run_id}' under experiment '{self.experiment_name}'\"", ")", "\n", "", "metrics", "=", "list", "(", "run", ".", "data", ".", "metrics", ".", "keys", "(", ")", ")", "\n", "\n", "METRICS", "=", "{", "\n", "\n", "}", "\n", "for", "metric", "in", "metrics", ":", "\n", "            ", "if", "metric", ".", "split", "(", "'/'", ")", "[", "-", "1", "]", ".", "startswith", "(", "'SubwordClassificationHead'", ")", ":", "\n", "                ", "if", "'ner'", "not", "in", "METRICS", ":", "\n", "                    ", "METRICS", "[", "'ner'", "]", "=", "{", "}", "\n", "", "if", "len", "(", "metric", ".", "split", "(", "'/'", ")", ")", "==", "2", ":", "#cross entity level metric", "\n", "                    ", "dataset_name", "=", "'_'", ".", "join", "(", "metric", ".", "split", "(", "'/'", ")", "[", "1", "]", ".", "split", "(", "'_'", ")", "[", "1", ":", "]", ")", "\n", "metric_name", "=", "metric", ".", "split", "(", "'/'", ")", "[", "0", "]", "\n", "if", "dataset_name", "not", "in", "METRICS", "[", "'ner'", "]", ":", "\n", "                        ", "METRICS", "[", "'ner'", "]", "[", "dataset_name", "]", "=", "{", "'GLOBAL_METRICS'", ":", "{", "}", ",", "'ENTITY_METRICS'", ":", "{", "}", "}", "\n", "", "METRICS", "[", "'ner'", "]", "[", "dataset_name", "]", "[", "'GLOBAL_METRICS'", "]", "[", "metric_name", "]", "=", "[", "]", "\n", "", "else", ":", "\n", "                    ", "dataset_name", "=", "'_'", ".", "join", "(", "metric", ".", "split", "(", "'/'", ")", "[", "-", "1", "]", ".", "split", "(", "'_'", ")", "[", "1", ":", "]", ")", "\n", "if", "dataset_name", "not", "in", "METRICS", "[", "'ner'", "]", ":", "\n", "                        ", "METRICS", "[", "'ner'", "]", "[", "dataset_name", "]", "=", "{", "'GLOBAL_METRICS'", ":", "{", "}", ",", "'ENTITY_METRICS'", ":", "{", "}", "}", "\n", "", "entity_name", "=", "metric", ".", "split", "(", "'/'", ")", "[", "1", "]", "\n", "metric_name", "=", "metric", ".", "split", "(", "'/'", ")", "[", "0", "]", "\n", "if", "entity_name", "not", "in", "METRICS", "[", "'ner'", "]", "[", "dataset_name", "]", "[", "'ENTITY_METRICS'", "]", ":", "\n", "                        ", "METRICS", "[", "'ner'", "]", "[", "dataset_name", "]", "[", "'ENTITY_METRICS'", "]", "[", "entity_name", "]", "=", "{", "}", "\n", "", "if", "metric_name", "not", "in", "METRICS", "[", "'ner'", "]", "[", "dataset_name", "]", "[", "'ENTITY_METRICS'", "]", "[", "entity_name", "]", ":", "\n", "                        ", "METRICS", "[", "'ner'", "]", "[", "dataset_name", "]", "[", "'ENTITY_METRICS'", "]", "[", "entity_name", "]", "[", "metric_name", "]", "=", "[", "]", "\n", "", "", "", "if", "metric", ".", "split", "(", "'/'", ")", "[", "-", "1", "]", ".", "startswith", "(", "'CLSRegressionHead'", ")", ":", "\n", "                ", "if", "'sts'", "not", "in", "METRICS", ":", "\n", "                    ", "METRICS", "[", "'sts'", "]", "=", "{", "}", "\n", "", "dataset_name", "=", "'_'", ".", "join", "(", "metric", ".", "split", "(", "'/'", ")", "[", "-", "1", "]", ".", "split", "(", "'_'", ")", "[", "1", ":", "]", ")", "\n", "metric_name", "=", "metric", ".", "split", "(", "'/'", ")", "[", "0", "]", "\n", "if", "dataset_name", "not", "in", "METRICS", "[", "'sts'", "]", ":", "\n", "                    ", "METRICS", "[", "'sts'", "]", "[", "dataset_name", "]", "=", "{", "}", "\n", "", "if", "metric_name", "not", "in", "METRICS", "[", "'sts'", "]", "[", "dataset_name", "]", ":", "\n", "                    ", "METRICS", "[", "'sts'", "]", "[", "dataset_name", "]", "[", "metric_name", "]", "=", "[", "]", "\n", "", "", "if", "metric", ".", "split", "(", "'/'", ")", "[", "-", "1", "]", ".", "startswith", "(", "'CLSClassificationHead'", ")", ":", "\n", "                ", "if", "'nli'", "not", "in", "METRICS", ":", "\n", "                    ", "METRICS", "[", "'nli'", "]", "=", "{", "}", "\n", "", "dataset_name", "=", "'_'", ".", "join", "(", "metric", ".", "split", "(", "'/'", ")", "[", "-", "1", "]", ".", "split", "(", "'_'", ")", "[", "1", ":", "]", ")", "\n", "metric_name", "=", "metric", ".", "split", "(", "'/'", ")", "[", "0", "]", "\n", "if", "dataset_name", "not", "in", "METRICS", "[", "'nli'", "]", ":", "\n", "                    ", "METRICS", "[", "'nli'", "]", "[", "dataset_name", "]", "=", "{", "}", "\n", "", "if", "metric_name", "not", "in", "METRICS", "[", "'nli'", "]", "[", "dataset_name", "]", ":", "\n", "                    ", "METRICS", "[", "'nli'", "]", "[", "dataset_name", "]", "[", "metric_name", "]", "=", "[", "]", "\n", "\n", "\n", "", "", "", "metric_history", "=", "{", "metric", ":", "{", "}", "for", "metric", "in", "metrics", "}", "\n", "\n", "for", "metric", "in", "metrics", ":", "\n", "            ", "time_steps", "=", "client", ".", "get_metric_history", "(", "run_id", "=", "self", ".", "run_id", ",", "key", "=", "metric", ")", "\n", "\n", "# metric_by_time = {metric.step: {'value': metric.value, 'timestamp': metric.timestamp} for metric in time_steps}", "\n", "metric_by_time", "=", "[", "metric", ".", "value", "for", "metric", "in", "time_steps", "]", "\n", "metric_history", "[", "metric", "]", "=", "metric_by_time", "\n", "\n", "if", "metric", ".", "split", "(", "'/'", ")", "[", "-", "1", "]", ".", "startswith", "(", "'SubwordClassificationHead'", ")", ":", "\n", "                ", "if", "len", "(", "metric", ".", "split", "(", "'/'", ")", ")", "==", "2", ":", "# cross entity level metric", "\n", "                    ", "metric_name", "=", "metric", ".", "split", "(", "'/'", ")", "[", "0", "]", "\n", "dataset_name", "=", "'_'", ".", "join", "(", "metric", ".", "split", "(", "'/'", ")", "[", "1", "]", ".", "split", "(", "'_'", ")", "[", "1", ":", "]", ")", "\n", "METRICS", "[", "'ner'", "]", "[", "dataset_name", "]", "[", "'GLOBAL_METRICS'", "]", "[", "metric_name", "]", "=", "metric_by_time", "\n", "", "else", ":", "\n", "                    ", "dataset_name", "=", "'_'", ".", "join", "(", "metric", ".", "split", "(", "'/'", ")", "[", "-", "1", "]", ".", "split", "(", "'_'", ")", "[", "1", ":", "]", ")", "\n", "entity_name", "=", "metric", ".", "split", "(", "'/'", ")", "[", "1", "]", "\n", "metric_name", "=", "metric", ".", "split", "(", "'/'", ")", "[", "0", "]", "\n", "METRICS", "[", "'ner'", "]", "[", "dataset_name", "]", "[", "'ENTITY_METRICS'", "]", "[", "entity_name", "]", "[", "metric_name", "]", "=", "metric_by_time", "\n", "", "", "if", "metric", ".", "split", "(", "'/'", ")", "[", "-", "1", "]", ".", "startswith", "(", "'CLSRegressionHead'", ")", ":", "\n", "                ", "dataset_name", "=", "'_'", ".", "join", "(", "metric", ".", "split", "(", "'/'", ")", "[", "-", "1", "]", ".", "split", "(", "'_'", ")", "[", "1", ":", "]", ")", "\n", "metric_name", "=", "metric", ".", "split", "(", "'/'", ")", "[", "0", "]", "\n", "METRICS", "[", "'sts'", "]", "[", "dataset_name", "]", "[", "metric_name", "]", "=", "metric_by_time", "\n", "\n", "", "if", "metric", ".", "split", "(", "'/'", ")", "[", "-", "1", "]", ".", "startswith", "(", "'CLSClassificationHead'", ")", ":", "\n", "                ", "dataset_name", "=", "'_'", ".", "join", "(", "metric", ".", "split", "(", "'/'", ")", "[", "-", "1", "]", ".", "split", "(", "'_'", ")", "[", "1", ":", "]", ")", "\n", "metric_name", "=", "metric", ".", "split", "(", "'/'", ")", "[", "0", "]", "\n", "METRICS", "[", "'nli'", "]", "[", "dataset_name", "]", "[", "metric_name", "]", "=", "metric_by_time", "\n", "", "", "return", "METRICS", "\n", "\n"]], "home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.evaluation.mlflow.MLFlowRun.compare_against_run": [[110, 172], ["sorted", "list", "my_metrics.keys", "ignored_datasets.append", "print", "comparison.append", "comparison.append", "comparison.append", "len", "[].keys", "list", "[].keys"], "methods", ["None"], ["", "def", "compare_against_run", "(", "self", ",", "run", ",", "task", ")", ":", "\n", "        ", "\"\"\"\n        Compares this run with 'run' on the given (task,dataset) along the given metric (or all).\n        For NER:\n            - Single class tasks will be evaluated with F1\n            - Multi-class tasks with micro-averaged F1\n        :param run: an instance of a processed MLFlowRun\n        :param task: the task to compare on\n        :param dataset: the dataset name\n        :param metric: the metric to compare along.\n        :return: a list [dataset, task metric, my_performance, other_performance]\n        \"\"\"", "\n", "my_metrics", "=", "self", ".", "metrics", "[", "task", "]", "\n", "other_metrics", "=", "run", ".", "metrics", "[", "task", "]", "\n", "\n", "single_entity_datasets", "=", "{", "}", "\n", "ignored_datasets", "=", "[", "]", "\n", "\n", "#ignore un-matching datasets, single out single label ner TASKS", "\n", "for", "dataset", "in", "sorted", "(", "list", "(", "my_metrics", ".", "keys", "(", ")", ")", ")", ":", "\n", "            ", "if", "dataset", "not", "in", "other_metrics", ":", "\n", "                ", "ignored_datasets", ".", "append", "(", "dataset", ")", "\n", "print", "(", "f\"Ignoring metrics from: {dataset}\"", ")", "\n", "", "if", "task", "==", "'ner'", ":", "\n", "                ", "if", "len", "(", "my_metrics", "[", "dataset", "]", "[", "'ENTITY_METRICS'", "]", ".", "keys", "(", ")", ")", "==", "1", ":", "\n", "                    ", "single_entity_datasets", "[", "dataset", "]", "=", "list", "(", "my_metrics", "[", "dataset", "]", "[", "'ENTITY_METRICS'", "]", ".", "keys", "(", ")", ")", "[", "0", "]", "\n", "\n", "\n", "", "", "", "comparison", "=", "[", "]", "\n", "\n", "for", "dataset", "in", "my_metrics", ":", "\n", "            ", "if", "dataset", "in", "ignored_datasets", ":", "\n", "                ", "continue", "\n", "", "if", "task", "==", "'ner'", ":", "\n", "                ", "my_performance", "=", "None", "\n", "other_performance", "=", "None", "\n", "if", "dataset", "in", "single_entity_datasets", ":", "\n", "                    ", "metric", "=", "'F1'", "\n", "my_performance", "=", "my_metrics", "[", "dataset", "]", "[", "'ENTITY_METRICS'", "]", "[", "single_entity_datasets", "[", "dataset", "]", "]", "[", "metric", "]", "\n", "other_performance", "=", "other_metrics", "[", "dataset", "]", "[", "'ENTITY_METRICS'", "]", "[", "single_entity_datasets", "[", "dataset", "]", "]", "[", "metric", "]", "\n", "#print(dataset, metric, single_entity_datasets[dataset])", "\n", "", "else", ":", "\n", "                    ", "metric", "=", "'Micro-F1'", "\n", "#print(dataset, metric)", "\n", "my_performance", "=", "my_metrics", "[", "dataset", "]", "[", "'GLOBAL_METRICS'", "]", "[", "metric", "]", "\n", "other_performance", "=", "other_metrics", "[", "dataset", "]", "[", "'GLOBAL_METRICS'", "]", "[", "metric", "]", "\n", "\n", "", "comparison", ".", "append", "(", "(", "dataset", ",", "task", ",", "metric", ",", "my_performance", ",", "other_performance", ")", ")", "\n", "", "if", "task", "==", "'nli'", ":", "\n", "                ", "metric", "=", "'Accuracy'", "\n", "my_performance", "=", "my_metrics", "[", "dataset", "]", "[", "metric", "]", "\n", "other_performance", "=", "other_metrics", "[", "dataset", "]", "[", "metric", "]", "\n", "comparison", ".", "append", "(", "(", "dataset", ",", "task", ",", "metric", ",", "my_performance", ",", "other_performance", ")", ")", "\n", "", "if", "task", "==", "'sts'", ":", "\n", "                ", "metric", "=", "'PearsonRho'", "\n", "my_performance", "=", "my_metrics", "[", "dataset", "]", "[", "metric", "]", "\n", "other_performance", "=", "other_metrics", "[", "dataset", "]", "[", "metric", "]", "\n", "comparison", ".", "append", "(", "(", "dataset", ",", "task", ",", "metric", ",", "my_performance", ",", "other_performance", ")", ")", "\n", "\n", "\n", "\n", "", "", "return", "comparison", "\n", "", "", ""]], "home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.dataloaders.round_robin.RoundRobinDataLoader.__init__": [[12, 16], ["str"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "heads_and_dataloaders", ":", "List", "[", "Tuple", "[", "TransformerHead", ",", "DataLoader", "]", "]", ",", "repeat_in_epoch_sampling", "=", "True", ")", ":", "\n", "        ", "self", ".", "heads_and_dataloaders", "=", "heads_and_dataloaders", "\n", "self", ".", "repeat_in_epoch_sampling", "=", "repeat_in_epoch_sampling", "\n", "self", ".", "epoch_batch_counts", "=", "{", "str", "(", "head", ")", ":", "0", "for", "head", ",", "train", "in", "self", ".", "heads_and_dataloaders", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.AndriyMulyar_multitasking_transformers.dataloaders.round_robin.RoundRobinDataLoader.__iter__": [[18, 48], ["len", "max", "str", "range", "enumerate", "len", "any", "len", "range", "len", "iter", "len", "next", "isinstance", "str", "next", "enumerate", "iter"], "methods", ["None"], ["", "def", "__iter__", "(", "self", ")", ":", "\n", "        ", "epoch_generators", "=", "[", "(", "head", ",", "enumerate", "(", "iter", "(", "train", ")", ")", ")", "for", "head", ",", "train", "in", "self", ".", "heads_and_dataloaders", "]", "\n", "epoch_complete", "=", "[", "False", "]", "*", "len", "(", "epoch_generators", ")", "\n", "if", "len", "(", "self", ".", "heads_and_dataloaders", ")", ">", "1", "and", "not", "any", "(", "[", "isinstance", "(", "head", ",", "MaskedLMHead", ")", "for", "head", ",", "_", "in", "self", ".", "heads_and_dataloaders", "]", ")", ":", "\n", "            ", "dataset_num_mini_batches", "=", "[", "len", "(", "train", ")", "for", "_", ",", "train", "in", "self", ".", "heads_and_dataloaders", "]", "\n", "dataset_with_most_batches", "=", "max", "(", "range", "(", "len", "(", "dataset_num_mini_batches", ")", ")", ",", "\n", "key", "=", "dataset_num_mini_batches", ".", "__getitem__", ")", "\n", "", "else", ":", "\n", "            ", "dataset_with_most_batches", "=", "0", "\n", "\n", "", "self", ".", "epoch_batch_counts", "=", "{", "str", "(", "head", ")", ":", "0", "for", "head", ",", "train", "in", "self", ".", "heads_and_dataloaders", "}", "\n", "\n", "# keep sampling batches until the dataset with the most batches is complete.", "\n", "while", "not", "epoch_complete", "[", "dataset_with_most_batches", "]", ":", "\n", "            ", "for", "i", "in", "range", "(", "len", "(", "epoch_generators", ")", ")", ":", "\n", "                ", "if", "epoch_complete", "[", "dataset_with_most_batches", "]", ":", "\n", "                    ", "break", "\n", "", "head", ",", "train_iter", "=", "epoch_generators", "[", "i", "]", "\n", "try", ":", "\n", "                    ", "batch_idx", ",", "batch", "=", "next", "(", "train_iter", ")", "\n", "self", ".", "epoch_batch_counts", "[", "str", "(", "head", ")", "]", "+=", "1", "\n", "yield", "(", "head", ",", "batch_idx", ",", "batch", ")", "\n", "", "except", "StopIteration", ":", "\n", "                    ", "epoch_complete", "[", "i", "]", "=", "True", "\n", "if", "self", ".", "repeat_in_epoch_sampling", ":", "#resets dataloader to continue sampling from task.", "\n", "                        ", "epoch_generators", "[", "i", "]", "=", "(", "self", ".", "heads_and_dataloaders", "[", "i", "]", "[", "0", "]", ",", "\n", "enumerate", "(", "iter", "(", "self", ".", "heads_and_dataloaders", "[", "i", "]", "[", "1", "]", ")", ")", ")", "\n", "head", ",", "train_iter", "=", "epoch_generators", "[", "i", "]", "\n", "batch_idx", ",", "batch", "=", "next", "(", "train_iter", ")", "\n", "yield", "(", "head", ",", "batch_idx", ",", "batch", ")", "\n", "", "", "", "", "", "", ""]]}