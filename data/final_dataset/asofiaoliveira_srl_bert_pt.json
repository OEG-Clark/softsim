{"home.repos.pwc.inspect_result.asofiaoliveira_srl_bert_pt.None.my_model.SrlMyModel.__init__": [[46, 73], ["allennlp.nn.InitializerApplicator", "allennlp.models.model.Model.__init__", "transformers.AutoModel.from_pretrained", "my_model.SrlMyModel.vocab.get_vocab_size", "allennlp_models.structured_prediction.metrics.srl_eval_scorer.SrlEvalScorer", "torch.nn.modules.Linear", "torch.nn.modules.Linear", "torch.nn.modules.Dropout", "torch.nn.modules.Dropout", "initializer"], "methods", ["home.repos.pwc.inspect_result.asofiaoliveira_srl_bert_pt.None.conll_reader.SrlMineReader.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "vocab", ":", "Vocabulary", ",", "\n", "bert_model", ":", "Union", "[", "str", ",", "AutoModel", "]", ",", "\n", "embedding_dropout", ":", "float", "=", "0.0", ",", "\n", "initializer", ":", "InitializerApplicator", "=", "InitializerApplicator", "(", ")", ",", "\n", "label_smoothing", ":", "float", "=", "None", ",", "\n", "verbose_metrics", ":", "bool", "=", "False", ",", "\n", "ignore_span_metric", ":", "bool", "=", "False", ",", "\n", "srl_eval_path", ":", "str", "=", "DEFAULT_SRL_EVAL_PATH", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "vocab", ",", "**", "kwargs", ")", "\n", "\n", "self", ".", "verbose_metrics", "=", "verbose_metrics", "\n", "\n", "self", ".", "bert_model", "=", "AutoModel", ".", "from_pretrained", "(", "bert_model", ")", "\n", "\n", "\n", "self", ".", "num_classes", "=", "self", ".", "vocab", ".", "get_vocab_size", "(", "\"labels\"", ")", "\n", "self", ".", "span_metric", "=", "SrlEvalScorer", "(", "\"srl-eval.pl\"", ",", "ignore_classes", "=", "[", "\"V\"", "]", ")", "\n", "self", ".", "tag_projection_layer", "=", "Linear", "(", "self", ".", "bert_model", ".", "config", ".", "hidden_size", ",", "self", ".", "num_classes", ")", "\n", "\n", "self", ".", "embedding_dropout", "=", "Dropout", "(", "p", "=", "embedding_dropout", ")", "\n", "self", ".", "_label_smoothing", "=", "label_smoothing", "\n", "self", ".", "ignore_span_metric", "=", "ignore_span_metric", "\n", "initializer", "(", "self", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.asofiaoliveira_srl_bert_pt.None.my_model.SrlMyModel.forward": [[74, 176], ["allennlp.nn.util.get_text_field_mask", "my_model.SrlMyModel.bert_model", "my_model.SrlMyModel.embedding_dropout", "my_model.SrlMyModel.size", "my_model.SrlMyModel.tag_projection_layer", "my_model.SrlMyModel.view", "torch.softmax().view", "torch.softmax().view", "zip", "list", "list", "list", "allennlp.nn.util.sequence_cross_entropy_with_logits", "allennlp.nn.util.get_token_ids_from_text_field_tensors", "torch.softmax", "torch.softmax", "open", "my_model.SrlMyModel.make_output_human_readable().pop", "my_model.SrlMyModel.span_metric", "allennlp_models.structured_prediction.models.srl.convert_bio_tags_to_conll_format", "allennlp_models.structured_prediction.models.srl.convert_bio_tags_to_conll_format", "open.writelines", "my_model.SrlMyModel.make_output_human_readable", "range", "zip", "len", "list"], "methods", ["home.repos.pwc.inspect_result.asofiaoliveira_srl_bert_pt.None.my_model.SrlMyModel.make_output_human_readable"], ["", "def", "forward", "(", "# type: ignore", "\n", "self", ",", "\n", "tokens", ":", "TextFieldTensors", ",", "\n", "verb_indicator", ":", "torch", ".", "Tensor", ",", "\n", "metadata", ":", "List", "[", "Any", "]", ",", "\n", "tags", ":", "torch", ".", "LongTensor", "=", "None", ",", "\n", ")", ":", "\n", "\n", "        ", "\"\"\"\n        # Parameters\n\n        tokens : TextFieldTensors, required\n            The output of `TextField.as_array()`, which should typically be passed directly to a\n            `TextFieldEmbedder`. For this model, this must be a `SingleIdTokenIndexer` which\n            indexes wordpieces from the BERT vocabulary.\n        verb_indicator: torch.LongTensor, required.\n            An integer `SequenceFeatureField` representation of the position of the verb\n            in the sentence. This should have shape (batch_size, num_tokens) and importantly, can be\n            all zeros, in the case that the sentence has no verbal predicate.\n        tags : torch.LongTensor, optional (default = None)\n            A torch tensor representing the sequence of integer gold class labels\n            of shape `(batch_size, num_tokens)`\n        metadata : `List[Dict[str, Any]]`, optional, (default = None)\n            metadata containg the original words in the sentence, the verb to compute the\n            frame for, and start offsets for converting wordpieces back to a sequence of words,\n            under 'words', 'verb' and 'offsets' keys, respectively.\n\n        # Returns\n\n        An output dictionary consisting of:\n        logits : torch.FloatTensor\n            A tensor of shape `(batch_size, num_tokens, tag_vocab_size)` representing\n            unnormalised log probabilities of the tag classes.\n        class_probabilities : torch.FloatTensor\n            A tensor of shape `(batch_size, num_tokens, tag_vocab_size)` representing\n            a distribution of the tag classes per word.\n        loss : torch.FloatTensor, optional\n            A scalar loss to be optimised.\n        \"\"\"", "\n", "mask", "=", "get_text_field_mask", "(", "tokens", ")", "\n", "bert_embeddings", ",", "_", "=", "self", ".", "bert_model", "(", "\n", "input_ids", "=", "util", ".", "get_token_ids_from_text_field_tensors", "(", "tokens", ")", ",", "\n", "token_type_ids", "=", "verb_indicator", ",", "\n", "attention_mask", "=", "mask", ",", "\n", "return_dict", "=", "False", "\n", ")", "\n", "\n", "embedded_text_input", "=", "self", ".", "embedding_dropout", "(", "bert_embeddings", ")", "\n", "batch_size", ",", "sequence_length", ",", "_", "=", "embedded_text_input", ".", "size", "(", ")", "\n", "logits", "=", "self", ".", "tag_projection_layer", "(", "embedded_text_input", ")", "\n", "\n", "reshaped_log_probs", "=", "logits", ".", "view", "(", "-", "1", ",", "self", ".", "num_classes", ")", "\n", "class_probabilities", "=", "F", ".", "softmax", "(", "reshaped_log_probs", ",", "dim", "=", "-", "1", ")", ".", "view", "(", "\n", "[", "batch_size", ",", "sequence_length", ",", "self", ".", "num_classes", "]", "\n", ")", "\n", "output_dict", "=", "{", "\"logits\"", ":", "logits", ",", "\"class_probabilities\"", ":", "class_probabilities", "}", "\n", "# We need to retain the mask in the output dictionary", "\n", "# so that we can crop the sequences to remove padding", "\n", "# when we do viterbi inference in self.make_output_human_readable.", "\n", "output_dict", "[", "\"mask\"", "]", "=", "mask", "\n", "# We add in the offsets here so we can compute the un-wordpieced tags.", "\n", "words", ",", "verbs", ",", "offsets", "=", "zip", "(", "*", "[", "(", "x", "[", "\"words\"", "]", ",", "x", "[", "\"verb\"", "]", ",", "x", "[", "\"offsets\"", "]", ")", "for", "x", "in", "metadata", "]", ")", "\n", "output_dict", "[", "\"words\"", "]", "=", "list", "(", "words", ")", "\n", "output_dict", "[", "\"verb\"", "]", "=", "list", "(", "verbs", ")", "\n", "output_dict", "[", "\"wordpiece_offsets\"", "]", "=", "list", "(", "offsets", ")", "\n", "try", ":", "\n", "            ", "from", "__main__", "import", "file_path", "\n", "self", ".", "file_path", "=", "file_path", "\n", "if", "self", ".", "file_path", "is", "not", "None", ":", "\n", "                ", "f", "=", "open", "(", "self", ".", "file_path", ",", "\"a\"", ")", "\n", "", "", "except", ":", "\n", "            ", "self", ".", "file_path", "=", "None", "\n", "", "if", "tags", "is", "not", "None", ":", "\n", "            ", "loss", "=", "sequence_cross_entropy_with_logits", "(", "\n", "logits", ",", "tags", ",", "mask", ",", "label_smoothing", "=", "self", ".", "_label_smoothing", "\n", ")", "\n", "if", "not", "self", ".", "ignore_span_metric", "and", "self", ".", "span_metric", "is", "not", "None", "and", "not", "self", ".", "training", ":", "\n", "                ", "batch_verb_indices", "=", "[", "\n", "example_metadata", "[", "\"verb_index\"", "]", "for", "example_metadata", "in", "metadata", "\n", "]", "\n", "batch_sentences", "=", "[", "example_metadata", "[", "\"words\"", "]", "for", "example_metadata", "in", "metadata", "]", "\n", "batch_bio_predicted_tags", "=", "self", ".", "make_output_human_readable", "(", "output_dict", ")", ".", "pop", "(", "\"tags\"", ")", "\n", "batch_conll_predicted_tags", "=", "[", "\n", "convert_bio_tags_to_conll_format", "(", "tags", ")", "for", "tags", "in", "batch_bio_predicted_tags", "\n", "]", "\n", "batch_bio_gold_tags", "=", "[", "\n", "example_metadata", "[", "\"gold_tags\"", "]", "for", "example_metadata", "in", "metadata", "\n", "]", "\n", "batch_conll_gold_tags", "=", "[", "\n", "convert_bio_tags_to_conll_format", "(", "tags", ")", "for", "tags", "in", "batch_bio_gold_tags", "\n", "]", "\n", "if", "self", ".", "file_path", "is", "not", "None", ":", "\n", "                    ", "f", ".", "writelines", "(", "\"%s\\t%s\\t%s\\n\"", "%", "(", "s", ",", "x1", ",", "x2", ")", "for", "i", "in", "range", "(", "len", "(", "batch_conll_predicted_tags", ")", ")", "for", "s", ",", "x1", ",", "x2", "in", "zip", "(", "list", "(", "words", ")", "[", "i", "]", "+", "[", "\"-\"", "]", ",", "batch_conll_gold_tags", "[", "i", "]", "+", "[", "\"-\"", "]", ",", "batch_conll_predicted_tags", "[", "i", "]", "+", "[", "\"-\"", "]", ")", ")", "\n", "", "self", ".", "span_metric", "(", "\n", "batch_verb_indices", ",", "\n", "batch_sentences", ",", "\n", "batch_conll_predicted_tags", ",", "\n", "batch_conll_gold_tags", ",", "\n", ")", "\n", "", "output_dict", "[", "\"loss\"", "]", "=", "loss", "\n", "", "return", "output_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.asofiaoliveira_srl_bert_pt.None.my_model.SrlMyModel.make_output_human_readable": [[177, 229], ["allennlp.nn.util.get_lengths_from_binary_sequence_mask().data.tolist", "my_model.SrlMyModel.get_viterbi_pairwise_potentials", "my_model.SrlMyModel.get_start_transitions", "zip", "all_predictions.dim", "allennlp.nn.util.viterbi_decode", "wordpiece_tags.append", "word_tags.append", "all_predictions[].detach().cpu", "my_model.SrlMyModel.vocab.get_token_from_index", "allennlp.nn.util.get_lengths_from_binary_sequence_mask", "range", "all_predictions[].detach", "all_predictions.size"], "methods", ["home.repos.pwc.inspect_result.asofiaoliveira_srl_bert_pt.None.my_model.SrlMyModel.get_viterbi_pairwise_potentials", "home.repos.pwc.inspect_result.asofiaoliveira_srl_bert_pt.None.my_model.SrlMyModel.get_start_transitions"], ["", "@", "overrides", "\n", "def", "make_output_human_readable", "(", "\n", "self", ",", "output_dict", ":", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", "\n", ")", "->", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ":", "\n", "        ", "\"\"\"\n        Does constrained viterbi decoding on class probabilities output in :func:`forward`.  The\n        constraint simply specifies that the output tags must be a valid BIO sequence.  We add a\n        `\"tags\"` key to the dictionary with the result.\n\n        NOTE: First, we decode a BIO sequence on top of the wordpieces. This is important; viterbi\n        decoding produces low quality output if you decode on top of word representations directly,\n        because the model gets confused by the 'missing' positions (which is sensible as it is trained\n        to perform tagging on wordpieces, not words).\n\n        Secondly, it's important that the indices we use to recover words from the wordpieces are the\n        start_offsets (i.e offsets which correspond to using the first wordpiece of words which are\n        tokenized into multiple wordpieces) as otherwise, we might get an ill-formed BIO sequence\n        when we select out the word tags from the wordpiece tags. This happens in the case that a word\n        is split into multiple word pieces, and then we take the last tag of the word, which might\n        correspond to, e.g, I-V, which would not be allowed as it is not preceeded by a B tag.\n        \"\"\"", "\n", "all_predictions", "=", "output_dict", "[", "\"class_probabilities\"", "]", "\n", "sequence_lengths", "=", "get_lengths_from_binary_sequence_mask", "(", "output_dict", "[", "\"mask\"", "]", ")", ".", "data", ".", "tolist", "(", ")", "\n", "\n", "if", "all_predictions", ".", "dim", "(", ")", "==", "3", ":", "\n", "            ", "predictions_list", "=", "[", "\n", "all_predictions", "[", "i", "]", ".", "detach", "(", ")", ".", "cpu", "(", ")", "for", "i", "in", "range", "(", "all_predictions", ".", "size", "(", "0", ")", ")", "\n", "]", "\n", "", "else", ":", "\n", "            ", "predictions_list", "=", "[", "all_predictions", "]", "\n", "", "wordpiece_tags", "=", "[", "]", "\n", "word_tags", "=", "[", "]", "\n", "transition_matrix", "=", "self", ".", "get_viterbi_pairwise_potentials", "(", ")", "\n", "start_transitions", "=", "self", ".", "get_start_transitions", "(", ")", "\n", "# **************** Different ********************", "\n", "# We add in the offsets here so we can compute the un-wordpieced tags.", "\n", "for", "predictions", ",", "length", ",", "offsets", "in", "zip", "(", "\n", "predictions_list", ",", "sequence_lengths", ",", "output_dict", "[", "\"wordpiece_offsets\"", "]", "\n", ")", ":", "\n", "            ", "max_likelihood_sequence", ",", "_", "=", "viterbi_decode", "(", "\n", "predictions", "[", ":", "length", "]", ",", "transition_matrix", ",", "allowed_start_transitions", "=", "start_transitions", "\n", ")", "\n", "tags", "=", "[", "\n", "self", ".", "vocab", ".", "get_token_from_index", "(", "x", ",", "namespace", "=", "\"labels\"", ")", "\n", "for", "x", "in", "max_likelihood_sequence", "\n", "]", "\n", "\n", "wordpiece_tags", ".", "append", "(", "tags", ")", "\n", "word_tags", ".", "append", "(", "[", "tags", "[", "i", "]", "for", "i", "in", "offsets", "]", ")", "\n", "", "output_dict", "[", "\"wordpiece_tags\"", "]", "=", "wordpiece_tags", "\n", "output_dict", "[", "\"tags\"", "]", "=", "word_tags", "\n", "return", "output_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.asofiaoliveira_srl_bert_pt.None.my_model.SrlMyModel.get_metrics": [[230, 244], ["my_model.SrlMyModel.span_metric.get_metric", "my_model.SrlMyModel.items"], "methods", ["None"], ["", "def", "get_metrics", "(", "self", ",", "reset", ":", "bool", "=", "False", ")", ":", "\n", "        ", "if", "self", ".", "ignore_span_metric", ":", "\n", "# Return an empty dictionary if ignoring the", "\n", "# span metric", "\n", "            ", "return", "{", "}", "\n", "\n", "", "metric_dict", "=", "self", ".", "span_metric", ".", "get_metric", "(", "reset", "=", "reset", ")", "\n", "\n", "if", "self", ".", "verbose_metrics", ":", "\n", "            ", "return", "metric_dict", "\n", "", "else", ":", "\n", "# This can be a lot of metrics, as there are 3 per class.", "\n", "# we only really care about the overall metrics, so we filter for them here.", "\n", "            ", "return", "{", "x", ":", "y", "for", "x", ",", "y", "in", "metric_dict", ".", "items", "(", ")", "if", "\"overall\"", "in", "x", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.asofiaoliveira_srl_bert_pt.None.my_model.SrlMyModel.get_viterbi_pairwise_potentials": [[245, 269], ["my_model.SrlMyModel.vocab.get_index_to_token_vocabulary", "len", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "my_model.SrlMyModel.items", "my_model.SrlMyModel.items", "float"], "methods", ["None"], ["", "", "def", "get_viterbi_pairwise_potentials", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Generate a matrix of pairwise transition potentials for the BIO labels.\n        The only constraint implemented here is that I-XXX labels must be preceded\n        by either an identical I-XXX tag or a B-XXX tag. In order to achieve this\n        constraint, pairs of labels which do not satisfy this constraint have a\n        pairwise potential of -inf.\n\n        # Returns\n\n        transition_matrix : torch.Tensor\n            A (num_labels, num_labels) matrix of pairwise potentials.\n        \"\"\"", "\n", "all_labels", "=", "self", ".", "vocab", ".", "get_index_to_token_vocabulary", "(", "\"labels\"", ")", "\n", "num_labels", "=", "len", "(", "all_labels", ")", "\n", "transition_matrix", "=", "torch", ".", "zeros", "(", "[", "num_labels", ",", "num_labels", "]", ")", "\n", "\n", "for", "i", ",", "previous_label", "in", "all_labels", ".", "items", "(", ")", ":", "\n", "            ", "for", "j", ",", "label", "in", "all_labels", ".", "items", "(", ")", ":", "\n", "# I labels can only be preceded by themselves or", "\n", "# their corresponding B tag.", "\n", "                ", "if", "i", "!=", "j", "and", "label", "[", "0", "]", "==", "\"I\"", "and", "not", "previous_label", "==", "\"B\"", "+", "label", "[", "1", ":", "]", ":", "\n", "                    ", "transition_matrix", "[", "i", ",", "j", "]", "=", "float", "(", "\"-inf\"", ")", "\n", "", "", "", "return", "transition_matrix", "\n", "\n"]], "home.repos.pwc.inspect_result.asofiaoliveira_srl_bert_pt.None.my_model.SrlMyModel.get_start_transitions": [[270, 291], ["my_model.SrlMyModel.vocab.get_index_to_token_vocabulary", "len", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "my_model.SrlMyModel.items", "float"], "methods", ["None"], ["", "def", "get_start_transitions", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        In the BIO sequence, we cannot start the sequence with an I-XXX tag.\n        This transition sequence is passed to viterbi_decode to specify this constraint.\n\n        # Returns\n\n        start_transitions : torch.Tensor\n            The pairwise potentials between a START token and\n            the first token of the sequence.\n        \"\"\"", "\n", "all_labels", "=", "self", ".", "vocab", ".", "get_index_to_token_vocabulary", "(", "\"labels\"", ")", "\n", "num_labels", "=", "len", "(", "all_labels", ")", "\n", "\n", "start_transitions", "=", "torch", ".", "zeros", "(", "num_labels", ")", "\n", "\n", "for", "i", ",", "label", "in", "all_labels", ".", "items", "(", ")", ":", "\n", "            ", "if", "label", "[", "0", "]", "==", "\"I\"", ":", "\n", "                ", "start_transitions", "[", "i", "]", "=", "float", "(", "\"-inf\"", ")", "\n", "\n", "", "", "return", "start_transitions", "\n", "", "", ""]], "home.repos.pwc.inspect_result.asofiaoliveira_srl_bert_pt.None.my_reader.SrlReader.__init__": [[129, 149], ["allennlp.data.dataset_readers.dataset_reader.DatasetReader.__init__", "transformers.AutoTokenizer.from_pretrained", "my_reader.SrlReader.vocab.update", "allennlp.data.token_indexers.SingleIdTokenIndexer", "my_reader.SrlReader.bert_tokenizer.convert_ids_to_tokens", "range"], "methods", ["home.repos.pwc.inspect_result.asofiaoliveira_srl_bert_pt.None.conll_reader.SrlMineReader.__init__"], ["def", "__init__", "(", "self", ",", "\n", "token_indexers", ":", "Dict", "[", "str", ",", "TokenIndexer", "]", "=", "None", ",", "\n", "domain_identifier", ":", "str", "=", "None", ",", "\n", "lazy", ":", "bool", "=", "False", ",", "\n", "bert_model_name", ":", "str", "=", "None", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "lazy", ")", "\n", "self", ".", "_token_indexers", "=", "token_indexers", "or", "{", "\"tokens\"", ":", "SingleIdTokenIndexer", "(", ")", "}", "\n", "self", ".", "_domain_identifier", "=", "domain_identifier", "\n", "self", ".", "xlm", "=", "\"xlm\"", "in", "bert_model_name", "\n", "\n", "self", ".", "bert_tokenizer", "=", "AutoTokenizer", ".", "from_pretrained", "(", "bert_model_name", ",", "use_fast", "=", "False", ")", "\n", "\n", "# the model class is different in xlmr models ", "\n", "# the vocab is not in the same place as in bert", "\n", "# so we create the vocab in the xlmr", "\n", "if", "self", ".", "xlm", ":", "\n", "            ", "self", ".", "vocab", "=", "{", "self", ".", "bert_tokenizer", ".", "convert_ids_to_tokens", "(", "i", ")", ":", "i", "for", "i", "in", "range", "(", "250001", ")", "}", "\n", "self", ".", "vocab", ".", "update", "(", "self", ".", "bert_tokenizer", ".", "added_tokens_encoder", ")", "\n", "\n", "", "self", ".", "lowercase_input", "=", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.asofiaoliveira_srl_bert_pt.None.my_reader.SrlReader._wordpiece_tokenize_input": [[150, 199], ["start_offsets.append", "len", "end_offsets.append", "word_piece_tokens.extend", "my_reader.SrlReader.bert_tokenizer._tokenize", "my_reader.SrlReader.bert_tokenizer.wordpiece_tokenizer.tokenize"], "methods", ["None"], ["", "def", "_wordpiece_tokenize_input", "(", "self", ",", "tokens", ":", "List", "[", "str", "]", ")", "->", "Tuple", "[", "List", "[", "str", "]", ",", "List", "[", "int", "]", ",", "List", "[", "int", "]", "]", ":", "\n", "        ", "\"\"\"\n        Convert a list of tokens to wordpiece tokens and offsets, as well as adding\n        BERT CLS and SEP tokens to the begining and end of the sentence.\n\n        A slight oddity with this function is that it also returns the wordpiece offsets\n        corresponding to the _start_ of words as well as the end.\n\n        We need both of these offsets (or at least, it's easiest to use both), because we need\n        to convert the labels to tags using the end_offsets. However, when we are decoding a\n        BIO sequence inside the SRL model itself, it's important that we use the start_offsets,\n        because otherwise we might select an ill-formed BIO sequence from the BIO sequence on top of\n        wordpieces (this happens in the case that a word is split into multiple word pieces,\n        and then we take the last tag of the word, which might correspond to, e.g, I-V, which\n        would not be allowed as it is not preceeded by a B tag).\n\n        For example:\n\n        `annotate` will be bert tokenized as [\"anno\", \"##tate\"].\n        If this is tagged as [B-V, I-V] as it should be, we need to select the\n        _first_ wordpiece label to be the label for the token, because otherwise\n        we may end up with invalid tag sequences (we cannot start a new tag with an I).\n\n        Returns\n        -------\n        wordpieces : List[str]\n            The BERT wordpieces from the words in the sentence.\n        end_offsets : List[int]\n            Indices into wordpieces such that `[wordpieces[i] for i in end_offsets]`\n            results in the end wordpiece of each word being chosen.\n        start_offsets : List[int]\n            Indices into wordpieces such that `[wordpieces[i] for i in start_offsets]`\n            results in the start wordpiece of each word being chosen.\n        \"\"\"", "\n", "word_piece_tokens", ":", "List", "[", "str", "]", "=", "[", "]", "\n", "end_offsets", "=", "[", "]", "\n", "start_offsets", "=", "[", "]", "\n", "cumulative", "=", "0", "\n", "for", "token", "in", "tokens", ":", "\n", "            ", "if", "self", ".", "xlm", ":", "\n", "                ", "word_pieces", "=", "self", ".", "bert_tokenizer", ".", "_tokenize", "(", "token", ")", "\n", "", "else", ":", "\n", "                ", "word_pieces", "=", "self", ".", "bert_tokenizer", ".", "wordpiece_tokenizer", ".", "tokenize", "(", "token", ")", "\n", "", "start_offsets", ".", "append", "(", "cumulative", "+", "1", ")", "\n", "cumulative", "+=", "len", "(", "word_pieces", ")", "\n", "end_offsets", ".", "append", "(", "cumulative", ")", "\n", "word_piece_tokens", ".", "extend", "(", "word_pieces", ")", "\n", "\n", "", "return", "word_piece_tokens", ",", "end_offsets", ",", "start_offsets", "\n", "\n"]], "home.repos.pwc.inspect_result.asofiaoliveira_srl_bert_pt.None.my_reader.SrlReader._read": [[200, 215], ["allennlp.common.file_utils.cached_path", "logger.info", "codecs.open", "line.split", "words.split", "tags.strip().split", "allennlp.data.tokenizers.Token", "my_reader.SrlReader.text_to_instance", "tags.strip"], "methods", ["home.repos.pwc.inspect_result.asofiaoliveira_srl_bert_pt.None.conll_reader.SrlMineReader.text_to_instance"], ["", "@", "overrides", "\n", "def", "_read", "(", "self", ",", "file_path", ":", "str", ")", ":", "\n", "# This reads the folds data", "\n", "\n", "# if `file_path` is a URL, redirect to the cache", "\n", "        ", "file_path", "=", "cached_path", "(", "file_path", ")", "\n", "logger", ".", "info", "(", "\"Reading SRL instances from dataset files at: %s\"", ",", "file_path", ")", "\n", "\n", "with", "codecs", ".", "open", "(", "file_path", ",", "'r'", ",", "encoding", "=", "'utf8'", ")", "as", "open_file", ":", "\n", "            ", "for", "line", "in", "open_file", ":", "\n", "                ", "words", ",", "tags", "=", "line", ".", "split", "(", "'\\t'", ")", "\n", "words", ",", "tags", "=", "words", ".", "split", "(", "' '", ")", ",", "tags", ".", "strip", "(", ")", ".", "split", "(", "' '", ")", "\n", "verb_indicator", "=", "[", "1", "if", "label", "[", "-", "2", ":", "]", "==", "\"-V\"", "else", "0", "for", "label", "in", "tags", "]", "\n", "tokens", "=", "[", "Token", "(", "t", ")", "for", "t", "in", "words", "]", "\n", "yield", "self", ".", "text_to_instance", "(", "tokens", ",", "verb_indicator", ",", "tags", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.asofiaoliveira_srl_bert_pt.None.my_reader.SrlReader.text_to_instance": [[219, 278], ["all", "allennlp.data.fields.MetadataField", "allennlp.data.instance.Instance", "my_reader.SrlReader._wordpiece_tokenize_input", "my_reader._convert_verb_indices_to_wordpiece_indices", "allennlp.data.fields.SequenceLabelField", "allennlp.data.fields.TextField", "allennlp.data.fields.SequenceLabelField", "verb_label.index", "allennlp.data.fields.TextField", "allennlp.data.fields.TextField", "my_reader._convert_tags_to_wordpiece_tags", "allennlp.data.fields.SequenceLabelField", "allennlp.data.fields.SequenceLabelField", "allennlp.data.tokenizers.Token", "allennlp.data.tokenizers.Token"], "methods", ["home.repos.pwc.inspect_result.asofiaoliveira_srl_bert_pt.None.conll_reader.SrlMineReader._wordpiece_tokenize_input", "home.repos.pwc.inspect_result.asofiaoliveira_srl_bert_pt.None.my_reader._convert_verb_indices_to_wordpiece_indices", "home.repos.pwc.inspect_result.asofiaoliveira_srl_bert_pt.None.my_reader._convert_tags_to_wordpiece_tags"], ["", "", "", "def", "text_to_instance", "(", "self", ",", "# type: ignore", "\n", "tokens", ":", "List", "[", "Token", "]", ",", "\n", "verb_label", ":", "List", "[", "int", "]", ",", "\n", "tags", ":", "List", "[", "str", "]", "=", "None", ")", "->", "Instance", ":", "\n", "        ", "\"\"\"\n        We take `pre-tokenized` input here, along with a verb label.  The verb label should be a\n        one-hot binary vector, the same length as the tokens, indicating the position of the verb\n        to find arguments for.\n        \"\"\"", "\n", "# pylint: disable=arguments-differ", "\n", "metadata_dict", ":", "Dict", "[", "str", ",", "Any", "]", "=", "{", "}", "\n", "if", "self", ".", "bert_tokenizer", "is", "not", "None", ":", "\n", "            ", "wordpieces", ",", "offsets", ",", "start_offsets", "=", "self", ".", "_wordpiece_tokenize_input", "(", "[", "t", ".", "text", "for", "t", "in", "tokens", "]", ")", "\n", "\n", "if", "self", ".", "xlm", ":", "\n", "                ", "wordpieces", "=", "[", "\"<s>\"", "]", "+", "wordpieces", "+", "[", "\"</s>\"", "]", "\n", "", "else", ":", "\n", "                ", "wordpieces", "=", "[", "\"[CLS]\"", "]", "+", "wordpieces", "+", "[", "\"[SEP]\"", "]", "\n", "", "new_verbs", "=", "_convert_verb_indices_to_wordpiece_indices", "(", "verb_label", ",", "offsets", ")", "\n", "metadata_dict", "[", "\"offsets\"", "]", "=", "start_offsets", "\n", "# In order to override the indexing mechanism, we need to set the `text_id`", "\n", "# attribute directly. This causes the indexing to use this id.", "\n", "if", "self", ".", "xlm", ":", "\n", "                ", "text_field", "=", "TextField", "(", "[", "Token", "(", "t", ",", "text_id", "=", "self", ".", "vocab", "[", "t", "]", ")", "for", "t", "in", "wordpieces", "]", ",", "\n", "token_indexers", "=", "self", ".", "_token_indexers", ")", "\n", "", "else", ":", "\n", "                ", "text_field", "=", "TextField", "(", "[", "Token", "(", "t", ",", "text_id", "=", "self", ".", "bert_tokenizer", ".", "vocab", "[", "t", "]", ")", "for", "t", "in", "wordpieces", "]", ",", "\n", "token_indexers", "=", "self", ".", "_token_indexers", ")", "\n", "\n", "", "verb_indicator", "=", "SequenceLabelField", "(", "new_verbs", ",", "text_field", ")", "\n", "\n", "", "else", ":", "\n", "            ", "text_field", "=", "TextField", "(", "tokens", ",", "token_indexers", "=", "self", ".", "_token_indexers", ")", "\n", "verb_indicator", "=", "SequenceLabelField", "(", "verb_label", ",", "text_field", ")", "\n", "\n", "", "fields", ":", "Dict", "[", "str", ",", "Field", "]", "=", "{", "}", "\n", "fields", "[", "'tokens'", "]", "=", "text_field", "\n", "fields", "[", "'verb_indicator'", "]", "=", "verb_indicator", "\n", "\n", "if", "all", "(", "[", "x", "==", "0", "for", "x", "in", "verb_label", "]", ")", ":", "\n", "            ", "verb", "=", "None", "\n", "verb_index", "=", "None", "\n", "", "else", ":", "\n", "            ", "verb_index", "=", "verb_label", ".", "index", "(", "1", ")", "\n", "verb", "=", "tokens", "[", "verb_index", "]", ".", "text", "\n", "\n", "", "metadata_dict", "[", "\"words\"", "]", "=", "[", "x", ".", "text", "for", "x", "in", "tokens", "]", "\n", "metadata_dict", "[", "\"verb\"", "]", "=", "verb", "\n", "metadata_dict", "[", "\"verb_index\"", "]", "=", "verb_index", "\n", "\n", "if", "tags", ":", "\n", "            ", "if", "self", ".", "bert_tokenizer", "is", "not", "None", ":", "\n", "                ", "new_tags", "=", "_convert_tags_to_wordpiece_tags", "(", "tags", ",", "offsets", ")", "\n", "fields", "[", "'tags'", "]", "=", "SequenceLabelField", "(", "new_tags", ",", "text_field", ")", "\n", "", "else", ":", "\n", "                ", "fields", "[", "'tags'", "]", "=", "SequenceLabelField", "(", "tags", ",", "text_field", ")", "\n", "", "metadata_dict", "[", "\"gold_tags\"", "]", "=", "tags", "\n", "", "fields", "[", "\"metadata\"", "]", "=", "MetadataField", "(", "metadata_dict", ")", "\n", "return", "Instance", "(", "fields", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.asofiaoliveira_srl_bert_pt.None.my_reader.SimpleReader.__init__": [[288, 293], ["allennlp.data.dataset_readers.dataset_reader.DatasetReader.__init__"], "methods", ["home.repos.pwc.inspect_result.asofiaoliveira_srl_bert_pt.None.conll_reader.SrlMineReader.__init__"], ["def", "__init__", "(", "self", ",", "\n", "lazy", ":", "bool", "=", "False", ",", "\n", "remove_c", ":", "bool", "=", "False", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "lazy", ")", "\n", "self", ".", "remove_c", "=", "remove_c", "\n", "\n"]], "home.repos.pwc.inspect_result.asofiaoliveira_srl_bert_pt.None.my_reader.SimpleReader._read": [[294, 311], ["allennlp.common.file_utils.cached_path", "PropBankBr.PropBankBr.PropBankBr", "logger.info", "PropBankBr.PropBankBr.PropBankBr.dataset_iterator", "preprocess.Preprocess().preprocess", "preprocess.Preprocess().preprocess", "preprocess.Preprocess", "preprocess.Preprocess"], "methods", ["home.repos.pwc.inspect_result.asofiaoliveira_srl_bert_pt.None.conll_reader.Ontonotes_mine.dataset_iterator", "home.repos.pwc.inspect_result.asofiaoliveira_srl_bert_pt.None.preprocess.Preprocess.preprocess", "home.repos.pwc.inspect_result.asofiaoliveira_srl_bert_pt.None.preprocess.Preprocess.preprocess"], ["", "@", "overrides", "\n", "def", "_read", "(", "self", ",", "file_path", ":", "str", ")", ":", "\n", "# if `file_path` is a URL, redirect to the cache", "\n", "        ", "file_path", "=", "cached_path", "(", "file_path", ")", "\n", "PBBr_reader", "=", "PropBankBr", "(", "remove_c", "=", "self", ".", "remove_c", ")", "\n", "logger", ".", "info", "(", "\"Reading SRL instances from dataset files at: %s\"", ",", "file_path", ")", "\n", "\n", "for", "sentence", "in", "PBBr_reader", ".", "dataset_iterator", "(", "file_path", ")", ":", "\n", "            ", "if", "not", "sentence", ".", "srl_frames", ":", "\n", "# Sentence contains no predicates.", "\n", "                ", "tags", "=", "[", "\"O\"", "for", "_", "in", "sentence", ".", "words", "]", "\n", "words", ",", "tags", "=", "preprocess", ".", "Preprocess", "(", "sentence", ".", "words", ",", "tags", ")", ".", "preprocess", "(", ")", "\n", "yield", "[", "words", ",", "tags", "]", "\n", "", "else", ":", "\n", "                ", "for", "(", "_", ",", "tags", ")", "in", "sentence", ".", "srl_frames", ":", "\n", "                    ", "words", ",", "tags", "=", "preprocess", ".", "Preprocess", "(", "sentence", ".", "words", ",", "tags", ")", ".", "preprocess", "(", ")", "\n", "yield", "[", "words", ",", "tags", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.asofiaoliveira_srl_bert_pt.None.my_reader.UDTransformerDatasetReader.__init__": [[333, 341], ["allennlp_models.structured_prediction.dataset_readers.UniversalDependenciesDatasetReader.__init__"], "methods", ["home.repos.pwc.inspect_result.asofiaoliveira_srl_bert_pt.None.conll_reader.SrlMineReader.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "token_indexers", ":", "Dict", "[", "str", ",", "TokenIndexer", "]", "=", "None", ",", "\n", "use_language_specific_pos", ":", "bool", "=", "False", ",", "\n", "tokenizer", ":", "Tokenizer", "=", "None", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "token_indexers", ",", "use_language_specific_pos", ",", "tokenizer", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.asofiaoliveira_srl_bert_pt.None.my_reader.UDTransformerDatasetReader._read": [[342, 366], ["allennlp.common.file_utils.cached_path", "open", "logger.info", "conllu.parse_incr", "my_reader.UDTransformerDatasetReader.text_to_instance", "isinstance", "list", "zip"], "methods", ["home.repos.pwc.inspect_result.asofiaoliveira_srl_bert_pt.None.conll_reader.SrlMineReader.text_to_instance"], ["", "@", "overrides", "\n", "def", "_read", "(", "self", ",", "file_path", ":", "str", ")", ":", "\n", "# if `file_path` is a URL, redirect to the cache", "\n", "        ", "file_path", "=", "cached_path", "(", "file_path", ")", "\n", "\n", "with", "open", "(", "file_path", ",", "\"r\"", ",", "encoding", "=", "\"utf-8\"", ")", "as", "conllu_file", ":", "\n", "            ", "logger", ".", "info", "(", "\"Reading UD instances from conllu dataset at: %s\"", ",", "file_path", ")", "\n", "\n", "for", "annotation", "in", "parse_incr", "(", "conllu_file", ")", ":", "\n", "# CoNLLU annotations sometimes add back in words that have been elided", "\n", "# in the original sentence; we remove these, as we're just predicting", "\n", "# dependencies for the original sentence.", "\n", "# We filter by integers here as elided words have a non-integer word id,", "\n", "# as parsed by the conllu python library.", "\n", "                ", "annotation", "=", "[", "x", "for", "x", "in", "annotation", "if", "isinstance", "(", "x", "[", "\"id\"", "]", ",", "int", ")", "]", "\n", "\n", "heads", "=", "[", "x", "[", "\"head\"", "]", "for", "x", "in", "annotation", "]", "\n", "tags", "=", "[", "x", "[", "\"deprel\"", "]", "for", "x", "in", "annotation", "]", "\n", "words", "=", "[", "x", "[", "\"form\"", "]", "for", "x", "in", "annotation", "]", "\n", "if", "self", ".", "use_language_specific_pos", ":", "\n", "                    ", "pos_tags", "=", "[", "x", "[", "\"xpostag\"", "]", "for", "x", "in", "annotation", "]", "\n", "", "else", ":", "\n", "                    ", "pos_tags", "=", "[", "x", "[", "\"upostag\"", "]", "for", "x", "in", "annotation", "]", "\n", "", "yield", "self", ".", "text_to_instance", "(", "words", ",", "pos_tags", ",", "list", "(", "zip", "(", "tags", ",", "heads", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.asofiaoliveira_srl_bert_pt.None.my_reader.UDTransformerDatasetReader._wordpiece_tokenize_input": [[368, 417], ["start_offsets.append", "len", "end_offsets.append", "word_piece_tokens.extend", "my_reader.UDTransformerDatasetReader.bert_tokenizer._tokenize", "my_reader.UDTransformerDatasetReader.bert_tokenizer.wordpiece_tokenizer.tokenize"], "methods", ["None"], ["", "", "", "def", "_wordpiece_tokenize_input", "(", "self", ",", "tokens", ":", "List", "[", "str", "]", ")", "->", "Tuple", "[", "List", "[", "str", "]", ",", "List", "[", "int", "]", ",", "List", "[", "int", "]", "]", ":", "\n", "        ", "\"\"\"\n        Convert a list of tokens to wordpiece tokens and offsets, as well as adding\n        BERT CLS and SEP tokens to the begining and end of the sentence.\n\n        A slight oddity with this function is that it also returns the wordpiece offsets\n        corresponding to the _start_ of words as well as the end.\n\n        We need both of these offsets (or at least, it's easiest to use both), because we need\n        to convert the labels to tags using the end_offsets. However, when we are decoding a\n        BIO sequence inside the SRL model itself, it's important that we use the start_offsets,\n        because otherwise we might select an ill-formed BIO sequence from the BIO sequence on top of\n        wordpieces (this happens in the case that a word is split into multiple word pieces,\n        and then we take the last tag of the word, which might correspond to, e.g, I-V, which\n        would not be allowed as it is not preceeded by a B tag).\n\n        For example:\n\n        `annotate` will be bert tokenized as [\"anno\", \"##tate\"].\n        If this is tagged as [B-V, I-V] as it should be, we need to select the\n        _first_ wordpiece label to be the label for the token, because otherwise\n        we may end up with invalid tag sequences (we cannot start a new tag with an I).\n\n        Returns\n        -------\n        wordpieces : List[str]\n            The BERT wordpieces from the words in the sentence.\n        end_offsets : List[int]\n            Indices into wordpieces such that `[wordpieces[i] for i in end_offsets]`\n            results in the end wordpiece of each word being chosen.\n        start_offsets : List[int]\n            Indices into wordpieces such that `[wordpieces[i] for i in start_offsets]`\n            results in the start wordpiece of each word being chosen.\n        \"\"\"", "\n", "word_piece_tokens", ":", "List", "[", "str", "]", "=", "[", "]", "\n", "end_offsets", "=", "[", "]", "\n", "start_offsets", "=", "[", "]", "\n", "cumulative", "=", "0", "\n", "for", "token", "in", "tokens", ":", "\n", "            ", "if", "self", ".", "xlm", ":", "\n", "                ", "word_pieces", "=", "self", ".", "bert_tokenizer", ".", "_tokenize", "(", "token", ")", "\n", "", "else", ":", "\n", "                ", "word_pieces", "=", "self", ".", "bert_tokenizer", ".", "wordpiece_tokenizer", ".", "tokenize", "(", "token", ")", "\n", "", "start_offsets", ".", "append", "(", "cumulative", "+", "1", ")", "\n", "cumulative", "+=", "len", "(", "word_pieces", ")", "\n", "end_offsets", ".", "append", "(", "cumulative", ")", "\n", "word_piece_tokens", ".", "extend", "(", "word_pieces", ")", "\n", "\n", "", "return", "word_piece_tokens", ",", "end_offsets", ",", "start_offsets", "\n", "\n"]], "home.repos.pwc.inspect_result.asofiaoliveira_srl_bert_pt.None.my_reader.UDTransformerDatasetReader.text_to_instance": [[419, 477], ["allennlp.data.fields.SequenceLabelField", "allennlp.data.fields.MetadataField", "allennlp.data.instance.Instance", "my_reader.UDTransformerDatasetReader._wordpiece_tokenize_input", "allennlp.data.fields.TextField", "allennlp.data.fields.SequenceLabelField", "allennlp.data.fields.SequenceLabelField", "allennlp.data.fields.TextField", "allennlp.data.fields.TextField", "allennlp.data.tokenizers.Token", "allennlp.data.tokenizers.Token", "allennlp.data.tokenizers.Token"], "methods", ["home.repos.pwc.inspect_result.asofiaoliveira_srl_bert_pt.None.conll_reader.SrlMineReader._wordpiece_tokenize_input"], ["", "@", "overrides", "\n", "def", "text_to_instance", "(", "\n", "self", ",", "# type: ignore", "\n", "words", ":", "List", "[", "str", "]", ",", "\n", "upos_tags", ":", "List", "[", "str", "]", ",", "\n", "dependencies", ":", "List", "[", "Tuple", "[", "str", ",", "int", "]", "]", "=", "None", ",", "\n", ")", "->", "Instance", ":", "\n", "\n", "        ", "\"\"\"\n        # Parameters\n\n        words : `List[str]`, required.\n            The words in the sentence to be encoded.\n        upos_tags : `List[str]`, required.\n            The universal dependencies POS tags for each word.\n        dependencies : `List[Tuple[str, int]]`, optional (default = None)\n            A list of  (head tag, head index) tuples. Indices are 1 indexed,\n            meaning an index of 0 corresponds to that word being the root of\n            the dependency tree.\n\n        # Returns\n\n        An instance containing words, upos tags, dependency head tags and head\n        indices as fields.\n        \"\"\"", "\n", "fields", ":", "Dict", "[", "str", ",", "Field", "]", "=", "{", "}", "\n", "\n", "if", "self", ".", "tokenizer", "is", "not", "None", ":", "\n", "            ", "wordpieces", ",", "offsets", ",", "start_offsets", "=", "self", ".", "_wordpiece_tokenize_input", "(", "[", "t", ".", "text", "for", "t", "in", "tokens", "]", ")", "\n", "\n", "if", "self", ".", "xlm", ":", "\n", "                ", "wordpieces", "=", "[", "\"<s>\"", "]", "+", "wordpieces", "+", "[", "\"</s>\"", "]", "\n", "", "else", ":", "\n", "                ", "wordpieces", "=", "[", "\"[CLS]\"", "]", "+", "wordpieces", "+", "[", "\"[SEP]\"", "]", "\n", "", "if", "self", ".", "xlm", ":", "\n", "                ", "text_field", "=", "TextField", "(", "[", "Token", "(", "t", ",", "text_id", "=", "self", ".", "vocab", "[", "t", "]", ")", "for", "t", "in", "wordpieces", "]", ",", "\n", "token_indexers", "=", "self", ".", "_token_indexers", ")", "\n", "", "else", ":", "\n", "                ", "text_field", "=", "TextField", "(", "[", "Token", "(", "t", ",", "text_id", "=", "self", ".", "bert_tokenizer", ".", "vocab", "[", "t", "]", ")", "for", "t", "in", "wordpieces", "]", ",", "\n", "token_indexers", "=", "self", ".", "_token_indexers", ")", "\n", "", "", "else", ":", "\n", "            ", "tokens", "=", "[", "Token", "(", "t", ")", "for", "t", "in", "words", "]", "\n", "text_field", "=", "TextField", "(", "tokens", ",", "token_indexers", "=", "self", ".", "_token_indexers", ")", "\n", "\n", "", "fields", "[", "\"words\"", "]", "=", "text_field", "\n", "fields", "[", "\"pos_tags\"", "]", "=", "SequenceLabelField", "(", "upos_tags", ",", "text_field", ",", "label_namespace", "=", "\"pos\"", ")", "\n", "if", "dependencies", "is", "not", "None", ":", "\n", "# We don't want to expand the label namespace with an additional dummy token, so we'll", "\n", "# always give the 'ROOT_HEAD' token a label of 'root'.", "\n", "            ", "fields", "[", "\"head_tags\"", "]", "=", "SequenceLabelField", "(", "\n", "[", "x", "[", "0", "]", "for", "x", "in", "dependencies", "]", ",", "text_field", ",", "label_namespace", "=", "\"head_tags\"", "\n", ")", "\n", "fields", "[", "\"head_indices\"", "]", "=", "SequenceLabelField", "(", "\n", "[", "x", "[", "1", "]", "for", "x", "in", "dependencies", "]", ",", "text_field", ",", "label_namespace", "=", "\"head_index_tags\"", "\n", ")", "\n", "\n", "", "fields", "[", "\"metadata\"", "]", "=", "MetadataField", "(", "{", "\"words\"", ":", "words", ",", "\"pos\"", ":", "upos_tags", ",", "\"offsets\"", ":", "start_offsets", "}", ")", "\n", "return", "Instance", "(", "fields", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.asofiaoliveira_srl_bert_pt.None.my_reader._convert_tags_to_wordpiece_tags": [[27, 70], ["enumerate", "new_tags.append", "tag.startswith", "new_tags.append", "tag.startswith", "new_tags.append", "tag.startswith", "tag.split", "new_tags.append"], "function", ["None"], ["def", "_convert_tags_to_wordpiece_tags", "(", "tags", ":", "List", "[", "str", "]", ",", "offsets", ":", "List", "[", "int", "]", ")", "->", "List", "[", "str", "]", ":", "\n", "    ", "\"\"\"\n    Converts a series of BIO tags to account for a wordpiece tokenizer,\n    extending/modifying BIO tags where appropriate to deal with words which\n    are split into multiple wordpieces by the tokenizer.\n\n    This is only used if you pass a `bert_model_name` to the dataset reader below.\n\n    # Parameters\n\n    tags : `List[str]`\n        The BIO formatted tags to convert to BIO tags for wordpieces\n    offsets : `List[int]`\n        The wordpiece offsets.\n\n    # Returns\n\n    The new BIO tags.\n    \"\"\"", "\n", "new_tags", "=", "[", "]", "\n", "j", "=", "0", "\n", "for", "i", ",", "offset", "in", "enumerate", "(", "offsets", ")", ":", "\n", "        ", "tag", "=", "tags", "[", "i", "]", "\n", "is_o", "=", "tag", "==", "\"O\"", "\n", "is_start", "=", "True", "\n", "while", "j", "<", "offset", ":", "\n", "            ", "if", "is_o", ":", "\n", "                ", "new_tags", ".", "append", "(", "\"O\"", ")", "\n", "\n", "", "elif", "tag", ".", "startswith", "(", "\"I\"", ")", ":", "\n", "                ", "new_tags", ".", "append", "(", "tag", ")", "\n", "\n", "", "elif", "is_start", "and", "tag", ".", "startswith", "(", "\"B\"", ")", ":", "\n", "                ", "new_tags", ".", "append", "(", "tag", ")", "\n", "is_start", "=", "False", "\n", "\n", "", "elif", "tag", ".", "startswith", "(", "\"B\"", ")", ":", "\n", "                ", "_", ",", "label", "=", "tag", ".", "split", "(", "\"-\"", ",", "1", ")", "\n", "new_tags", ".", "append", "(", "\"I-\"", "+", "label", ")", "\n", "", "j", "+=", "1", "\n", "\n", "# Add O tags for cls and sep tokens.", "\n", "", "", "return", "[", "\"O\"", "]", "+", "new_tags", "+", "[", "\"O\"", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.asofiaoliveira_srl_bert_pt.None.my_reader._convert_verb_indices_to_wordpiece_indices": [[72, 101], ["enumerate", "new_verb_indices.append"], "function", ["None"], ["", "def", "_convert_verb_indices_to_wordpiece_indices", "(", "verb_indices", ":", "List", "[", "int", "]", ",", "offsets", ":", "List", "[", "int", "]", ")", ":", "\n", "    ", "\"\"\"\n    Converts binary verb indicators to account for a wordpiece tokenizer,\n    extending/modifying BIO tags where appropriate to deal with words which\n    are split into multiple wordpieces by the tokenizer.\n\n    This is only used if you pass a `bert_model_name` to the dataset reader below.\n\n    # Parameters\n\n    verb_indices : `List[int]`\n        The binary verb indicators, 0 for not a verb, 1 for verb.\n    offsets : `List[int]`\n        The wordpiece offsets.\n\n    # Returns\n\n    The new verb indices.\n    \"\"\"", "\n", "j", "=", "0", "\n", "new_verb_indices", "=", "[", "]", "\n", "for", "i", ",", "offset", "in", "enumerate", "(", "offsets", ")", ":", "\n", "        ", "indicator", "=", "verb_indices", "[", "i", "]", "\n", "while", "j", "<", "offset", ":", "\n", "            ", "new_verb_indices", ".", "append", "(", "indicator", ")", "\n", "j", "+=", "1", "\n", "\n", "# Add 0 indicators for cls and sep tokens.", "\n", "", "", "return", "[", "0", "]", "+", "new_verb_indices", "+", "[", "0", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.asofiaoliveira_srl_bert_pt.None.PropBankBr.PropBankBrSentence.__init__": [[33, 43], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "\n", "words", ":", "List", "[", "str", "]", ",", "\n", "predicate_senses", ":", "List", "[", "Optional", "[", "str", "]", "]", ",", "\n", "predicate_lemmas", ":", "List", "[", "Optional", "[", "str", "]", "]", ",", "\n", "srl_frames", ":", "List", "[", "Tuple", "[", "str", ",", "List", "[", "str", "]", "]", "]", ")", "->", "None", ":", "\n", "\n", "        ", "self", ".", "words", "=", "words", "\n", "self", ".", "predicate_senses", "=", "predicate_senses", "\n", "self", ".", "predicate_lemmas", "=", "predicate_lemmas", "\n", "self", ".", "srl_frames", "=", "srl_frames", "\n", "\n"]], "home.repos.pwc.inspect_result.asofiaoliveira_srl_bert_pt.None.PropBankBr.PropBankBr.__init__": [[65, 69], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "remove_c", ")", ":", "\n", "        ", "self", ".", "flag", "=", "[", "]", "\n", "# Flag to remove the continuation arguments (to compare with Falci et al.):", "\n", "self", ".", "remove_c", "=", "remove_c", "\n", "\n"]], "home.repos.pwc.inspect_result.asofiaoliveira_srl_bert_pt.None.PropBankBr.PropBankBr.dataset_iterator": [[70, 77], ["PropBankBr.PropBankBr.dataset_document_iterator"], "methods", ["home.repos.pwc.inspect_result.asofiaoliveira_srl_bert_pt.None.conll_reader.Ontonotes_mine.dataset_document_iterator"], ["", "def", "dataset_iterator", "(", "self", ",", "file_path", ":", "str", ")", "->", "Iterator", "[", "PropBankBrSentence", "]", ":", "\n", "        ", "\"\"\"\n        An iterator over the entire dataset, yielding all sentences processed.\n        \"\"\"", "\n", "\n", "for", "sentence", "in", "self", ".", "dataset_document_iterator", "(", "file_path", ")", ":", "\n", "            ", "yield", "sentence", "\n", "\n"]], "home.repos.pwc.inspect_result.asofiaoliveira_srl_bert_pt.None.PropBankBr.PropBankBr.dataset_document_iterator": [[78, 113], ["codecs.open", "line.strip.strip.strip", "line.strip.strip.startswith", "conll_rows.append", "line.strip.strip.startswith", "PropBankBr.PropBankBr._conll_rows_to_sentence", "document.append", "len", "len", "enumerate"], "methods", ["home.repos.pwc.inspect_result.asofiaoliveira_srl_bert_pt.None.conll_reader.Ontonotes_mine._conll_rows_to_sentence"], ["", "", "def", "dataset_document_iterator", "(", "self", ",", "file_path", ":", "str", ")", "->", "List", "[", "PropBankBrSentence", "]", ":", "\n", "        ", "\"\"\"\n        An iterator over CONLL formatted files which yields documents, regardless\n        of the number of document annotations in a particular file. This is useful\n        for conll data which has been preprocessed, such as the preprocessing which\n        takes place for the 2012 CONLL Coreference Resolution task.\n        \"\"\"", "\n", "with", "codecs", ".", "open", "(", "file_path", ",", "'r'", ",", "encoding", "=", "'UTF-8'", ")", "as", "open_file", ":", "\n", "            ", "conll_rows", "=", "[", "]", "\n", "document", ":", "List", "[", "PropBankBrSentence", "]", "=", "[", "]", "\n", "for", "line", "in", "open_file", ":", "\n", "                ", "line", "=", "line", ".", "strip", "(", ")", "\n", "if", "line", "!=", "''", "and", "not", "line", ".", "startswith", "(", "'#'", ")", ":", "\n", "# Non-empty line. Collect the annotation.", "\n", "                    ", "conll_rows", ".", "append", "(", "line", ")", "\n", "", "else", ":", "\n", "                    ", "if", "conll_rows", ":", "\n", "                        ", "sentence", "=", "self", ".", "_conll_rows_to_sentence", "(", "conll_rows", ")", "\n", "if", "len", "(", "self", ".", "flag", ")", "!=", "0", ":", "\n", "# Remove instances that have either C- tags ", "\n", "# (if we want to remove them) or double labels", "\n", "                            ", "sentence", ".", "srl_frames", "=", "[", "srl_frame", "for", "ind", ",", "srl_frame", "in", "enumerate", "(", "sentence", ".", "srl_frames", ")", "if", "ind", "not", "in", "self", ".", "flag", "]", "\n", "self", ".", "flag", "=", "[", "]", "\n", "if", "len", "(", "sentence", ".", "srl_frames", ")", "==", "0", ":", "\n", "# If there are no annotated verbs left", "\n", "                                ", "conll_rows", "=", "[", "]", "\n", "continue", "\n", "", "", "document", ".", "append", "(", "sentence", ")", "\n", "conll_rows", "=", "[", "]", "\n", "", "", "if", "line", ".", "startswith", "(", "\"#end document\"", ")", ":", "\n", "                    ", "return", "document", "\n", "", "", "if", "document", ":", "\n", "# Collect any stragglers or files which might not", "\n", "# have the '#end document' format for the end of the file.", "\n", "                ", "return", "document", "\n", "\n"]], "home.repos.pwc.inspect_result.asofiaoliveira_srl_bert_pt.None.PropBankBr.PropBankBr._conll_rows_to_sentence": [[115, 170], ["enumerate", "PropBankBr.PropBankBrSentence", "row.split", "PropBankBr.PropBankBr._process_span_annotations_for_word", "any", "sentence.append", "predicate_lemmas.append", "predicate_senses.append", "verbal_predicates.append", "zip"], "methods", ["home.repos.pwc.inspect_result.asofiaoliveira_srl_bert_pt.None.conll_reader.Ontonotes_mine._process_span_annotations_for_word"], ["", "", "", "def", "_conll_rows_to_sentence", "(", "self", ",", "conll_rows", ":", "List", "[", "str", "]", ")", "->", "PropBankBrSentence", ":", "\n", "\n", "# The words in the sentence.", "\n", "        ", "sentence", ":", "List", "[", "str", "]", "=", "[", "]", "\n", "# The VerboBrasil ID of the predicate.", "\n", "predicate_senses", ":", "List", "[", "str", "]", "=", "[", "]", "\n", "# The lemmatised form of the predicates in the sentence ", "\n", "predicate_lemmas", ":", "List", "[", "str", "]", "=", "[", "]", "\n", "\n", "verbal_predicates", ":", "List", "[", "str", "]", "=", "[", "]", "\n", "span_labels", ":", "List", "[", "List", "[", "str", "]", "]", "=", "[", "]", "\n", "current_span_labels", ":", "List", "[", "str", "]", "=", "[", "]", "\n", "\n", "for", "_", ",", "row", "in", "enumerate", "(", "conll_rows", ")", ":", "\n", "            ", "conll_components", "=", "row", ".", "split", "(", ")", "\n", "\n", "word", "=", "conll_components", "[", "1", "]", "\n", "lemmatised_predicate", "=", "conll_components", "[", "9", "]", "\n", "sense", "=", "conll_components", "[", "8", "]", "\n", "\n", "if", "not", "span_labels", ":", "\n", "# If this is the first word in the sentence, create", "\n", "# empty lists to collect the SRL BIO labels.", "\n", "# We can't do this upfront, because we don't know how many", "\n", "# components we are collecting, as a sentence can have", "\n", "# variable numbers of SRL frames.", "\n", "                ", "span_labels", "=", "[", "[", "]", "for", "_", "in", "conll_components", "[", "10", ":", "]", "]", "\n", "# Create variables representing the current label for each label", "\n", "# sequence we are collecting.", "\n", "current_span_labels", "=", "[", "None", "for", "_", "in", "conll_components", "[", "10", ":", "]", "]", "\n", "\n", "", "self", ".", "_process_span_annotations_for_word", "(", "self", ",", "\n", "conll_components", "[", "10", ":", "]", ",", "\n", "span_labels", ",", "\n", "current_span_labels", ")", "\n", "\n", "# If any annotation marks this word as a verb predicate,", "\n", "# we need to record its index. This also has the side effect", "\n", "# of ordering the verbal predicates by their location in the", "\n", "# sentence, automatically aligning them with the annotations.", "\n", "# !! perhaps change this to only adding if it has a predicate sense", "\n", "word_is_verbal_predicate", "=", "any", "(", "[", "\"(V\"", "in", "x", "for", "x", "in", "conll_components", "[", "10", ":", "]", "]", ")", "\n", "if", "word_is_verbal_predicate", ":", "\n", "                ", "verbal_predicates", ".", "append", "(", "word", ")", "\n", "\n", "", "sentence", ".", "append", "(", "word", ")", "\n", "predicate_lemmas", ".", "append", "(", "lemmatised_predicate", "if", "lemmatised_predicate", "!=", "\"-\"", "else", "None", ")", "\n", "predicate_senses", ".", "append", "(", "sense", "if", "sense", "!=", "\"-\"", "else", "None", ")", "\n", "\n", "", "srl_frames", "=", "[", "(", "predicate", ",", "labels", ")", "for", "predicate", ",", "labels", "\n", "in", "zip", "(", "verbal_predicates", ",", "span_labels", ")", "]", "\n", "return", "PropBankBrSentence", "(", "sentence", ",", "\n", "predicate_senses", ",", "\n", "predicate_lemmas", ",", "\n", "srl_frames", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.asofiaoliveira_srl_bert_pt.None.PropBankBr.PropBankBr._process_span_annotations_for_word": [[171, 219], ["enumerate", "annotation.strip", "PropBankBr.PropBankBr.flag.append", "span_labels[].append", "PropBankBr.PropBankBr.flag.append", "span_labels[].append", "span_labels[].append"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "_process_span_annotations_for_word", "(", "self", ",", "\n", "annotations", ":", "List", "[", "str", "]", ",", "\n", "span_labels", ":", "List", "[", "List", "[", "str", "]", "]", ",", "\n", "current_span_labels", ":", "List", "[", "Optional", "[", "str", "]", "]", ")", "->", "None", ":", "\n", "        ", "\"\"\"\n        Given a sequence of different label types for a single word and the current\n        span label we are inside, compute the BIO tag for each label and append to a list.\n\n        Parameters\n        ----------\n        annotations: ``List[str]``\n            A list of labels to compute BIO tags for.\n        span_labels : ``List[List[str]]``\n            A list of lists, one for each annotation, to incrementally collect\n            the BIO tags for a sequence.\n        current_span_labels : ``List[Optional[str]]``\n            The currently open span per annotation type, or ``None`` if there is no open span.\n        \"\"\"", "\n", "for", "annotation_index", ",", "annotation", "in", "enumerate", "(", "annotations", ")", ":", "\n", "# strip all bracketing information to", "\n", "# get the actual propbank label.", "\n", "            ", "label", "=", "annotation", ".", "strip", "(", "\"()*\"", ")", "\n", "if", "self", ".", "remove_c", "and", "\"C-\"", "in", "label", ":", "\n", "                ", "self", ".", "flag", ".", "append", "(", "annotation_index", ")", "\n", "", "if", "\"(\"", "in", "annotation", ":", "\n", "# Entering into a span for a particular semantic role label.", "\n", "# We append the label and set the current span for this annotation.", "\n", "                ", "if", "current_span_labels", "[", "annotation_index", "]", "is", "not", "None", ":", "\n", "# If there is already a current label but the label has a \"(\", ", "\n", "# this token has 2 annotations and it is to be considered an annotation error.", "\n", "# Do not append to the document.", "\n", "                    ", "self", ".", "flag", ".", "append", "(", "annotation_index", ")", "\n", "", "bio_label", "=", "\"B-\"", "+", "label", "\n", "span_labels", "[", "annotation_index", "]", ".", "append", "(", "bio_label", ")", "\n", "current_span_labels", "[", "annotation_index", "]", "=", "label", "\n", "\n", "", "elif", "current_span_labels", "[", "annotation_index", "]", "is", "not", "None", ":", "\n", "# If there's no '(' token, but the current_span_label is not None,", "\n", "# then we are inside a span.", "\n", "                ", "bio_label", "=", "\"I-\"", "+", "current_span_labels", "[", "annotation_index", "]", "\n", "span_labels", "[", "annotation_index", "]", ".", "append", "(", "bio_label", ")", "\n", "", "else", ":", "\n", "# We're outside a span.", "\n", "                ", "span_labels", "[", "annotation_index", "]", ".", "append", "(", "\"O\"", ")", "\n", "# Exiting a span, so we reset the current span label for this annotation.", "\n", "", "if", "\")\"", "in", "annotation", ":", "\n", "                ", "current_span_labels", "[", "annotation_index", "]", "=", "None", "\n", "", "", "", "", ""]], "home.repos.pwc.inspect_result.asofiaoliveira_srl_bert_pt.None.preprocess.Preprocess.__init__": [[25, 44], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "\n", "tokens", ":", "List", "[", "str", "]", ",", "\n", "tags", ":", "List", "[", "str", "]", "=", "None", ",", "\n", "separate", ":", "bool", "=", "True", ",", "\n", "contract", ":", "bool", "=", "True", "\n", ")", "->", "None", ":", "\n", "        ", "self", ".", "tokens", "=", "tokens", "\n", "self", ".", "tags", "=", "tags", "\n", "self", ".", "has_tags", "=", "tags", "!=", "None", "\n", "self", ".", "art_def", "=", "[", "'a'", ",", "'as'", ",", "'o'", ",", "'os'", "]", "\n", "self", ".", "art_def_masc", "=", "[", "'o'", ",", "'os'", "]", "\n", "self", ".", "adverb", "=", "[", "'a\u00ed'", ",", "'aqui'", ",", "'ali'", "]", "\n", "self", ".", "pron", "=", "[", "'ele'", ",", "'eles'", ",", "'ela'", ",", "'elas'", ",", "'esse'", ",", "'esses'", ",", "'essa'", ",", "'essas'", ",", "'isso'", ",", "'este'", ",", "'estes'", ",", "'esta'", ",", "'estas'", ",", "'isto'", "]", "\n", "self", ".", "pron2", "=", "[", "'aquele'", ",", "'aqueles'", ",", "'aquela'", ",", "'aquelas'", ",", "'aquilo'", "]", "\n", "self", ".", "ref", "=", "[", "'o'", ",", "'os'", ",", "'a'", ",", "'as'", ",", "'me'", ",", "'se'", ",", "'te'", ",", "'vos'", ",", "'lhe'", ",", "'lho'", ",", "'lhas'", ",", "'lhos'", ",", "'lha'", ",", "'lo'", ",", "'la'", ",", "'los'", ",", "'las'", ",", "'lhes'", ",", "'no'", ",", "'na'", ",", "'nos'", "]", "\n", "self", ".", "change", "=", "self", ".", "art_def", "+", "self", ".", "adverb", "+", "self", ".", "pron", "+", "self", ".", "pron2", "+", "self", ".", "ref", "\n", "self", ".", "prepositions", "=", "[", "'de'", ",", "'a'", ",", "'em'", ",", "'por'", ",", "'-'", "]", "\n", "self", ".", "_separate", "=", "separate", "\n", "self", ".", "_contract", "=", "contract", "\n", "\n"]], "home.repos.pwc.inspect_result.asofiaoliveira_srl_bert_pt.None.preprocess.Preprocess.preprocess": [[45, 54], ["preprocess.Preprocess.separate", "preprocess.Preprocess.contractions", "Exception", "len", "len", "len", "len"], "methods", ["home.repos.pwc.inspect_result.asofiaoliveira_srl_bert_pt.None.preprocess.Preprocess.separate", "home.repos.pwc.inspect_result.asofiaoliveira_srl_bert_pt.None.preprocess.Preprocess.contractions"], ["", "def", "preprocess", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "_separate", ":", "\n", "            ", "self", ".", "separate", "(", ")", "\n", "", "if", "self", ".", "_contract", ":", "\n", "            ", "self", ".", "contractions", "(", ")", "\n", "#Check that tokens and tags list has same size", "\n", "", "if", "self", ".", "has_tags", "and", "len", "(", "self", ".", "tokens", ")", "!=", "len", "(", "self", ".", "tags", ")", ":", "\n", "            ", "raise", "Exception", "(", "\"Lengths of token list %i and tag list %i don't match.\"", "%", "(", "len", "(", "self", ".", "tokens", ")", ",", "len", "(", "self", ".", "tags", ")", ")", ")", "\n", "", "return", "self", ".", "tokens", ",", "self", ".", "tags", "\n", "\n"]], "home.repos.pwc.inspect_result.asofiaoliveira_srl_bert_pt.None.preprocess.Preprocess.separate": [[55, 72], ["len", "preprocess.Preprocess.tokens[].split", "len", "len"], "methods", ["None"], ["", "def", "separate", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        We want to separate multiword nouns and expressions such as \n        \"Campeonato_Brasileiro\" and \"em_termos_de\" into several tokens\n        (removing the \"_\")\n        \"\"\"", "\n", "ind", "=", "0", "\n", "while", "ind", "<", "len", "(", "self", ".", "tokens", ")", ":", "\n", "            ", "if", "'_'", "in", "self", ".", "tokens", "[", "ind", "]", "and", "'_'", "!=", "self", ".", "tokens", "[", "ind", "]", ":", "#sometimes '_' represents '--'", "\n", "                ", "new_tokens", "=", "self", ".", "tokens", "[", "ind", "]", ".", "split", "(", "'_'", ")", "\n", "self", ".", "tokens", "=", "self", ".", "tokens", "[", ":", "ind", "]", "+", "new_tokens", "+", "self", ".", "tokens", "[", "ind", "+", "1", ":", "]", "\n", "if", "self", ".", "has_tags", ":", "\n", "                    ", "if", "self", ".", "tags", "[", "ind", "]", "[", ":", "2", "]", "==", "'B-'", ":", "\n", "                        ", "self", ".", "tags", "=", "self", ".", "tags", "[", ":", "ind", "+", "1", "]", "+", "[", "'I-'", "+", "self", ".", "tags", "[", "ind", "]", "[", "2", ":", "]", "]", "*", "(", "len", "(", "new_tokens", ")", "-", "1", ")", "+", "self", ".", "tags", "[", "ind", "+", "1", ":", "]", "\n", "", "else", ":", "\n", "                        ", "self", ".", "tags", "=", "self", ".", "tags", "[", ":", "ind", "]", "+", "[", "self", ".", "tags", "[", "ind", "]", "]", "*", "len", "(", "new_tokens", ")", "+", "self", ".", "tags", "[", "ind", "+", "1", ":", "]", "\n", "", "", "", "ind", "+=", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.asofiaoliveira_srl_bert_pt.None.preprocess.Preprocess.determine_tags": [[73, 92], ["enumerate", "Exception"], "methods", ["None"], ["", "", "def", "determine_tags", "(", "self", ",", "ind", ",", "new_token", ")", ":", "\n", "        ", "\"\"\"\n        Given a new, contracted token at position ind, \n        change tags list to reflect the new tokens list.\n        \"\"\"", "\n", "if", "self", ".", "tags", "[", "ind", "]", "==", "self", ".", "tags", "[", "ind", "+", "1", "]", ":", "\n", "            ", "del", "self", ".", "tags", "[", "ind", "]", "\n", "", "elif", "self", ".", "tags", "[", "ind", "]", "==", "'O'", "and", "self", ".", "tags", "[", "ind", "+", "1", "]", "!=", "'O'", ":", "\n", "            ", "del", "self", ".", "tags", "[", "ind", "]", "\n", "", "elif", "self", ".", "tags", "[", "ind", "]", "!=", "'O'", "and", "self", ".", "tags", "[", "ind", "+", "1", "]", "==", "'O'", ":", "\n", "            ", "del", "self", ".", "tags", "[", "ind", "+", "1", "]", "\n", "if", "\"C-\"", "in", "self", ".", "tags", "[", "ind", "+", "1", "]", "and", "self", ".", "tags", "[", "ind", "+", "1", "]", "[", "4", ":", "]", "==", "self", ".", "tags", "[", "ind", "]", "[", "2", ":", "]", ":", "\n", "                ", "for", "i", ",", "tag", "in", "enumerate", "(", "self", ".", "tags", "[", "ind", "+", "1", ":", "]", ",", "ind", "+", "1", ")", ":", "\n", "                    ", "if", "\"C-\"", "+", "self", ".", "tags", "[", "ind", "]", "[", "2", ":", "]", "in", "tag", ":", "\n", "                        ", "self", ".", "tags", "[", "i", "]", "=", "\"I-\"", "+", "tag", "[", "4", ":", "]", "\n", "", "", "", "", "elif", "self", ".", "tags", "[", "ind", "]", "[", ":", "2", "]", "==", "\"B-\"", "and", "self", ".", "tags", "[", "ind", "+", "1", "]", "[", ":", "2", "]", "==", "\"I-\"", "and", "self", ".", "tags", "[", "ind", "]", "[", "2", ":", "]", "==", "self", ".", "tags", "[", "ind", "+", "1", "]", "[", "2", ":", "]", ":", "\n", "            ", "del", "self", ".", "tags", "[", "ind", "+", "1", "]", "\n", "", "else", ":", "\n", "            ", "raise", "Exception", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.asofiaoliveira_srl_bert_pt.None.preprocess.Preprocess.contractions": [[93, 132], ["any", "any", "[].isupper", "preprocess.Preprocess.tokens[].lower", "preprocess.Preprocess.tokens[].lower", "len", "token.lower", "new_token.capitalize.capitalize.capitalize", "preprocess.Preprocess.determine_tags"], "methods", ["home.repos.pwc.inspect_result.asofiaoliveira_srl_bert_pt.None.preprocess.Preprocess.determine_tags"], ["", "", "def", "contractions", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        We want to contract prepositions and articles, pronouns and adverbs\n        These contractions are widely used in the portuguese language and a lot of times\n        are used instead of the separated form even in formal settings\n        \"\"\"", "\n", "if", "any", "(", "token", "in", "self", ".", "change", "for", "token", "in", "self", ".", "tokens", ")", "and", "any", "(", "token", ".", "lower", "(", ")", "in", "self", ".", "prepositions", "for", "token", "in", "self", ".", "tokens", ")", ":", "\n", "            ", "ind", "=", "0", "\n", "while", "ind", "<", "len", "(", "self", ".", "tokens", ")", "-", "1", ":", "\n", "# The only captalization that matters is in the first word of the contraction", "\n", "                ", "is_cap", "=", "self", ".", "tokens", "[", "ind", "]", "[", "0", "]", ".", "isupper", "(", ")", "\n", "\n", "prev_token", "=", "self", ".", "tokens", "[", "ind", "]", ".", "lower", "(", ")", "\n", "cur_token", "=", "self", ".", "tokens", "[", "ind", "+", "1", "]", ".", "lower", "(", ")", "\n", "\n", "if", "prev_token", "in", "self", ".", "prepositions", "and", "cur_token", "in", "self", ".", "change", ":", "\n", "                    ", "if", "prev_token", "==", "'de'", "and", "cur_token", "in", "self", ".", "art_def", "+", "self", ".", "adverb", "+", "self", ".", "pron", "+", "self", ".", "pron2", ":", "\n", "                        ", "new_token", "=", "'d'", "+", "cur_token", "\n", "", "elif", "prev_token", "==", "'em'", "and", "cur_token", "in", "self", ".", "art_def", "+", "self", ".", "pron", "+", "self", ".", "pron2", ":", "\n", "                        ", "new_token", "=", "'n'", "+", "cur_token", "\n", "", "elif", "prev_token", "==", "'a'", "and", "cur_token", "in", "self", ".", "art_def", "+", "self", ".", "pron2", "and", "cur_token", "not", "in", "self", ".", "art_def_masc", ":", "\n", "                        ", "new_token", "=", "'\u00e0'", "+", "cur_token", "[", "1", ":", "]", "\n", "", "elif", "prev_token", "==", "'a'", "and", "cur_token", "in", "self", ".", "art_def_masc", ":", "\n", "                        ", "new_token", "=", "'a'", "+", "cur_token", "\n", "", "elif", "prev_token", "==", "'por'", "and", "cur_token", "in", "self", ".", "art_def", ":", "\n", "                        ", "new_token", "=", "'pel'", "+", "cur_token", "\n", "", "elif", "prev_token", "==", "'-'", "and", "cur_token", "in", "self", ".", "ref", ":", "\n", "                        ", "new_token", "=", "'-'", "+", "cur_token", "\n", "", "else", ":", "\n", "                        ", "ind", "+=", "1", "\n", "continue", "\n", "\n", "", "if", "is_cap", ":", "\n", "                        ", "new_token", "=", "new_token", ".", "capitalize", "(", ")", "\n", "", "self", ".", "tokens", "=", "self", ".", "tokens", "[", ":", "ind", "]", "+", "[", "new_token", "]", "+", "self", ".", "tokens", "[", "ind", "+", "2", ":", "]", "\n", "\n", "if", "self", ".", "has_tags", ":", "\n", "                        ", "self", ".", "determine_tags", "(", "ind", ",", "new_token", ")", "\n", "", "", "ind", "+=", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.asofiaoliveira_srl_bert_pt.None.my_predict.predict.__init__": [[23, 27], ["allennlp_models.structured_prediction.predictors.SemanticRoleLabelerPredictor.__init__"], "methods", ["home.repos.pwc.inspect_result.asofiaoliveira_srl_bert_pt.None.conll_reader.SrlMineReader.__init__"], ["    ", "def", "__init__", "(", "\n", "self", ",", "model", ":", "Model", ",", "dataset_reader", ":", "DatasetReader", "\n", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "model", ",", "dataset_reader", ",", "spacy_model", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.asofiaoliveira_srl_bert_pt.None.my_predict.predict.load_line": [[28, 32], ["nltk.tokenize.sent_tokenize"], "methods", ["None"], ["", "@", "overrides", "\n", "def", "load_line", "(", "self", ",", "line", ":", "str", ")", "->", "Iterator", "[", "str", "]", ":", "\n", "        ", "for", "sentence", "in", "tokenize", ".", "sent_tokenize", "(", "line", ")", ":", "\n", "            ", "yield", "sentence", "\n", "\n"]], "home.repos.pwc.inspect_result.asofiaoliveira_srl_bert_pt.None.my_predict.predict.dump_line": [[33, 37], ["open", "json.dump"], "methods", ["None"], ["", "", "@", "overrides", "\n", "def", "dump_line", "(", "self", ",", "outputs", ")", "->", "str", ":", "\n", "        ", "output_file", "=", "open", "(", "\"output.txt\"", ",", "\"a\"", ",", "encoding", "=", "\"utf-8\"", ")", "\n", "return", "json", ".", "dump", "(", "outputs", ",", "output_file", ",", "ensure_ascii", "=", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.asofiaoliveira_srl_bert_pt.None.my_predict.predict._sentence_to_srl_instances": [[40, 47], ["my_predict.predict._tokenizer.tokenize", "my_predict.predict.tokens_to_instances"], "methods", ["None"], ["", "@", "overrides", "\n", "def", "_sentence_to_srl_instances", "(", "self", ",", "sentence", ")", ":", "\n", "        ", "new_sent", "=", "\"\"", "\n", "for", "char", "in", "sentence", ":", "\n", "            ", "new_sent", "+=", "char", "if", "char", "!=", "\"-\"", "else", "\" -\"", "\n", "", "tokens", "=", "self", ".", "_tokenizer", ".", "tokenize", "(", "new_sent", ")", "\n", "return", "self", ".", "tokens_to_instances", "(", "tokens", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.asofiaoliveira_srl_bert_pt.None.my_predict.predict.predict_json": [[48, 54], ["my_predict.predict._sentence_to_srl_instances", "my_predict.predict.predict_instances", "inputs.split"], "methods", ["home.repos.pwc.inspect_result.asofiaoliveira_srl_bert_pt.None.my_predict.predict._sentence_to_srl_instances", "home.repos.pwc.inspect_result.asofiaoliveira_srl_bert_pt.None.my_predict.predict.predict_instances"], ["", "@", "overrides", "\n", "def", "predict_json", "(", "self", ",", "inputs", ":", "str", ")", ":", "\n", "        ", "instances", "=", "self", ".", "_sentence_to_srl_instances", "(", "inputs", ")", "\n", "if", "not", "instances", ":", "\n", "            ", "return", "inputs", ".", "split", "(", ")", "\n", "", "return", "self", ".", "predict_instances", "(", "instances", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.asofiaoliveira_srl_bert_pt.None.my_predict.predict.predict_instances": [[55, 66], ["my_predict.predict._model.forward_on_instances", "my_predict.predict.make_srl_string", "results[].append"], "methods", ["None"], ["", "@", "overrides", "\n", "def", "predict_instances", "(", "self", ",", "instances", ":", "List", "[", "Instance", "]", ")", "->", "JsonDict", ":", "\n", "        ", "outputs", "=", "self", ".", "_model", ".", "forward_on_instances", "(", "instances", ")", "\n", "results", "=", "{", "\"verbs\"", ":", "[", "]", ",", "\"words\"", ":", "outputs", "[", "0", "]", "[", "\"words\"", "]", "}", "\n", "for", "output", "in", "outputs", ":", "\n", "            ", "tags", "=", "output", "[", "\"tags\"", "]", "\n", "description", "=", "self", ".", "make_srl_string", "(", "output", "[", "\"words\"", "]", ",", "tags", ")", "\n", "results", "[", "\"verbs\"", "]", ".", "append", "(", "\n", "{", "\"verb\"", ":", "output", "[", "\"verb\"", "]", ",", "\"description\"", ":", "description", ",", "\"tags\"", ":", "tags", "}", "\n", ")", "\n", "", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.asofiaoliveira_srl_bert_pt.None.my_predict.predictManager.__init__": [[70, 79], ["allennlp.commands.predict._PredictManager.__init__"], "methods", ["home.repos.pwc.inspect_result.asofiaoliveira_srl_bert_pt.None.conll_reader.SrlMineReader.__init__"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "predictor", ":", "Predictor", ",", "\n", "input_file", ":", "str", ",", "\n", "output_file", ":", "Optional", "[", "str", "]", ",", "\n", "batch_size", ":", "int", ",", "\n", "print_to_console", ":", "bool", ",", "\n", "has_dataset_reader", ":", "bool", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "predictor", ",", "input_file", ",", "output_file", ",", "batch_size", ",", "print_to_console", ",", "has_dataset_reader", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.asofiaoliveira_srl_bert_pt.None.my_predict.predictManager._get_json_data": [[80, 92], ["allennlp.common.file_utils.cached_path", "open", "line.isspace", "my_predict.predictManager._predictor.load_line", "line.isspace", "my_predict.predictManager._predictor.load_line"], "methods", ["home.repos.pwc.inspect_result.asofiaoliveira_srl_bert_pt.None.my_predict.predict.load_line", "home.repos.pwc.inspect_result.asofiaoliveira_srl_bert_pt.None.my_predict.predict.load_line"], ["", "@", "overrides", "\n", "def", "_get_json_data", "(", "self", ")", "->", "Iterator", "[", "str", "]", ":", "\n", "        ", "if", "self", ".", "_input_file", "==", "\"-\"", ":", "\n", "            ", "for", "line", "in", "sys", ".", "stdin", ":", "\n", "                ", "if", "not", "line", ".", "isspace", "(", ")", ":", "\n", "                    ", "yield", "from", "self", ".", "_predictor", ".", "load_line", "(", "line", ")", "\n", "", "", "", "else", ":", "\n", "            ", "input_file", "=", "cached_path", "(", "self", ".", "_input_file", ")", "\n", "with", "open", "(", "input_file", ",", "\"r\"", ",", "encoding", "=", "\"utf-8\"", ")", "as", "file_input", ":", "\n", "                ", "for", "line", "in", "file_input", ":", "\n", "                    ", "if", "not", "line", ".", "isspace", "(", ")", ":", "\n", "                        ", "yield", "from", "self", ".", "_predictor", ".", "load_line", "(", "line", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.asofiaoliveira_srl_bert_pt.None.conll_reader.Ontonotes_mine.__init__": [[30, 32], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "self", ".", "flag", "=", "[", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.asofiaoliveira_srl_bert_pt.None.conll_reader.Ontonotes_mine.dataset_iterator": [[33, 39], ["conll_reader.Ontonotes_mine.dataset_path_iterator", "conll_reader.Ontonotes_mine.sentence_iterator"], "methods", ["home.repos.pwc.inspect_result.asofiaoliveira_srl_bert_pt.None.conll_reader.Ontonotes_mine.dataset_path_iterator", "home.repos.pwc.inspect_result.asofiaoliveira_srl_bert_pt.None.conll_reader.Ontonotes_mine.sentence_iterator"], ["", "def", "dataset_iterator", "(", "self", ",", "file_path", ":", "str", ")", "->", "Iterator", "[", "OntonotesSentence", "]", ":", "\n", "        ", "\"\"\"\n        An iterator over the entire dataset, yielding all sentences processed.\n        \"\"\"", "\n", "for", "conll_file", "in", "self", ".", "dataset_path_iterator", "(", "file_path", ")", ":", "\n", "            ", "yield", "from", "self", ".", "sentence_iterator", "(", "conll_file", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.asofiaoliveira_srl_bert_pt.None.conll_reader.Ontonotes_mine.dataset_path_iterator": [[40, 56], ["logger.info", "list", "os.walk", "data_file.endswith", "os.path.join"], "methods", ["None"], ["", "", "@", "staticmethod", "\n", "def", "dataset_path_iterator", "(", "file_path", ":", "str", ")", "->", "Iterator", "[", "str", "]", ":", "\n", "        ", "\"\"\"\n        An iterator returning file_paths in a directory\n        containing CONLL-formatted files.\n        \"\"\"", "\n", "logger", ".", "info", "(", "\"Reading CONLL sentences from dataset files at: %s\"", ",", "file_path", ")", "\n", "for", "root", ",", "_", ",", "files", "in", "list", "(", "os", ".", "walk", "(", "file_path", ")", ")", ":", "\n", "            ", "for", "data_file", "in", "files", ":", "\n", "# These are a relic of the dataset pre-processing. Every", "\n", "# file will be duplicated - one file called filename.gold_skel", "\n", "# and one generated from the preprocessing called filename.gold_conll.", "\n", "                ", "if", "not", "data_file", ".", "endswith", "(", "\"gold_conll\"", ")", ":", "\n", "                    ", "continue", "\n", "\n", "", "yield", "os", ".", "path", ".", "join", "(", "root", ",", "data_file", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.asofiaoliveira_srl_bert_pt.None.conll_reader.Ontonotes_mine.dataset_document_iterator": [[57, 92], ["codecs.open", "line.strip.strip.strip", "line.strip.strip.startswith", "conll_rows.append", "line.strip.strip.startswith", "conll_reader.Ontonotes_mine._conll_rows_to_sentence", "document.append", "len", "len", "enumerate"], "methods", ["home.repos.pwc.inspect_result.asofiaoliveira_srl_bert_pt.None.conll_reader.Ontonotes_mine._conll_rows_to_sentence"], ["", "", "", "def", "dataset_document_iterator", "(", "self", ",", "file_path", ":", "str", ")", "->", "Iterator", "[", "List", "[", "OntonotesSentence", "]", "]", ":", "\n", "        ", "\"\"\"\n        An iterator over CONLL formatted files which yields documents, regardless\n        of the number of document annotations in a particular file. This is useful\n        for conll data which has been preprocessed, such as the preprocessing which\n        takes place for the 2012 CONLL Coreference Resolution task.\n        \"\"\"", "\n", "with", "codecs", ".", "open", "(", "file_path", ",", "\"r\"", ",", "encoding", "=", "\"utf8\"", ")", "as", "open_file", ":", "\n", "            ", "conll_rows", "=", "[", "]", "\n", "document", ":", "List", "[", "OntonotesSentence", "]", "=", "[", "]", "\n", "for", "line", "in", "open_file", ":", "\n", "                ", "line", "=", "line", ".", "strip", "(", ")", "\n", "if", "line", "!=", "\"\"", "and", "not", "line", ".", "startswith", "(", "\"#\"", ")", ":", "\n", "# Non-empty line. Collect the annotation.", "\n", "                    ", "conll_rows", ".", "append", "(", "line", ")", "\n", "", "else", ":", "\n", "                    ", "if", "conll_rows", ":", "\n", "                        ", "sentence", "=", "self", ".", "_conll_rows_to_sentence", "(", "conll_rows", ")", "\n", "if", "len", "(", "self", ".", "flag", ")", "!=", "0", ":", "\n", "# Remove instances that have tags we don't care about", "\n", "                            ", "sentence", ".", "srl_frames", "=", "[", "srl_frame", "for", "ind", ",", "srl_frame", "in", "enumerate", "(", "sentence", ".", "srl_frames", ")", "if", "ind", "not", "in", "self", ".", "flag", "]", "\n", "self", ".", "flag", "=", "[", "]", "\n", "if", "len", "(", "sentence", ".", "srl_frames", ")", "==", "0", ":", "\n", "# If there are no annotated verbs left", "\n", "                                ", "conll_rows", "=", "[", "]", "\n", "continue", "\n", "", "", "document", ".", "append", "(", "sentence", ")", "\n", "conll_rows", "=", "[", "]", "\n", "", "", "if", "line", ".", "startswith", "(", "\"#end document\"", ")", ":", "\n", "                    ", "yield", "document", "\n", "document", "=", "[", "]", "\n", "", "", "if", "document", ":", "\n", "# Collect any stragglers or files which might not", "\n", "# have the '#end document' format for the end of the file.", "\n", "                ", "yield", "document", "\n", "\n"]], "home.repos.pwc.inspect_result.asofiaoliveira_srl_bert_pt.None.conll_reader.Ontonotes_mine.sentence_iterator": [[93, 100], ["conll_reader.Ontonotes_mine.dataset_document_iterator"], "methods", ["home.repos.pwc.inspect_result.asofiaoliveira_srl_bert_pt.None.conll_reader.Ontonotes_mine.dataset_document_iterator"], ["", "", "", "def", "sentence_iterator", "(", "self", ",", "file_path", ":", "str", ")", "->", "Iterator", "[", "OntonotesSentence", "]", ":", "\n", "        ", "\"\"\"\n        An iterator over the sentences in an individual CONLL formatted file.\n        \"\"\"", "\n", "for", "document", "in", "self", ".", "dataset_document_iterator", "(", "file_path", ")", ":", "\n", "            ", "for", "sentence", "in", "document", ":", "\n", "                ", "yield", "sentence", "\n", "\n"]], "home.repos.pwc.inspect_result.asofiaoliveira_srl_bert_pt.None.conll_reader.Ontonotes_mine._conll_rows_to_sentence": [[101, 241], ["collections.defaultdict", "collections.defaultdict", "enumerate", "range", "all", "allennlp_models.common.ontonotes.OntonotesSentence", "row.split", "int", "conll_reader.Ontonotes_mine._process_span_annotations_for_word", "any", "conll_reader.Ontonotes_mine._process_coref_span_annotations_for_word", "sentence.append", "pos_tags.append", "parse_pieces.append", "predicate_lemmas.append", "predicate_framenet_ids.append", "word_senses.append", "speakers.append", "len", "any", "nltk.Tree.fromstring", "parse_piece.split", "verbal_predicates.append", "zip", "clusters.items", "right_hand_side.count", "float", "range", "len"], "methods", ["home.repos.pwc.inspect_result.asofiaoliveira_srl_bert_pt.None.conll_reader.Ontonotes_mine._process_span_annotations_for_word", "home.repos.pwc.inspect_result.asofiaoliveira_srl_bert_pt.None.conll_reader.Ontonotes_mine._process_coref_span_annotations_for_word"], ["", "", "", "def", "_conll_rows_to_sentence", "(", "self", ",", "conll_rows", ":", "List", "[", "str", "]", ")", "->", "OntonotesSentence", ":", "\n", "        ", "document_id", ":", "str", "=", "None", "\n", "sentence_id", ":", "int", "=", "None", "\n", "# The words in the sentence.", "\n", "sentence", ":", "List", "[", "str", "]", "=", "[", "]", "\n", "# The pos tags of the words in the sentence.", "\n", "pos_tags", ":", "List", "[", "str", "]", "=", "[", "]", "\n", "# the pieces of the parse tree.", "\n", "parse_pieces", ":", "List", "[", "str", "]", "=", "[", "]", "\n", "# The lemmatised form of the words in the sentence which", "\n", "# have SRL or word sense information.", "\n", "predicate_lemmas", ":", "List", "[", "str", "]", "=", "[", "]", "\n", "# The FrameNet ID of the predicate.", "\n", "predicate_framenet_ids", ":", "List", "[", "str", "]", "=", "[", "]", "\n", "# The sense of the word, if available.", "\n", "word_senses", ":", "List", "[", "float", "]", "=", "[", "]", "\n", "# The current speaker, if available.", "\n", "speakers", ":", "List", "[", "str", "]", "=", "[", "]", "\n", "\n", "verbal_predicates", ":", "List", "[", "str", "]", "=", "[", "]", "\n", "span_labels", ":", "List", "[", "List", "[", "str", "]", "]", "=", "[", "]", "\n", "current_span_labels", ":", "List", "[", "str", "]", "=", "[", "]", "\n", "\n", "# Cluster id -> List of (start_index, end_index) spans.", "\n", "clusters", ":", "DefaultDict", "[", "int", ",", "List", "[", "Tuple", "[", "int", ",", "int", "]", "]", "]", "=", "defaultdict", "(", "list", ")", "\n", "# Cluster id -> List of start_indices which are open for this id.", "\n", "coref_stacks", ":", "DefaultDict", "[", "int", ",", "List", "[", "int", "]", "]", "=", "defaultdict", "(", "list", ")", "\n", "\n", "for", "index", ",", "row", "in", "enumerate", "(", "conll_rows", ")", ":", "\n", "            ", "conll_components", "=", "row", ".", "split", "(", ")", "\n", "\n", "document_id", "=", "conll_components", "[", "0", "]", "\n", "sentence_id", "=", "int", "(", "conll_components", "[", "1", "]", ")", "\n", "word", "=", "conll_components", "[", "3", "]", "\n", "pos_tag", "=", "conll_components", "[", "4", "]", "\n", "parse_piece", "=", "conll_components", "[", "5", "]", "\n", "\n", "# Replace brackets in text and pos tags", "\n", "# with a different token for parse trees.", "\n", "if", "pos_tag", "!=", "\"XX\"", "and", "word", "!=", "\"XX\"", ":", "\n", "                ", "if", "word", "==", "\"(\"", ":", "\n", "                    ", "parse_word", "=", "\"-LRB-\"", "\n", "", "elif", "word", "==", "\")\"", ":", "\n", "                    ", "parse_word", "=", "\"-RRB-\"", "\n", "", "else", ":", "\n", "                    ", "parse_word", "=", "word", "\n", "", "if", "pos_tag", "==", "\"(\"", ":", "\n", "                    ", "pos_tag", "=", "\"-LRB-\"", "\n", "", "if", "pos_tag", "==", "\")\"", ":", "\n", "                    ", "pos_tag", "=", "\"-RRB-\"", "\n", "", "(", "left_brackets", ",", "right_hand_side", ")", "=", "parse_piece", ".", "split", "(", "\"*\"", ")", "\n", "# only keep ')' if there are nested brackets with nothing in them.", "\n", "right_brackets", "=", "right_hand_side", ".", "count", "(", "\")\"", ")", "*", "\")\"", "\n", "parse_piece", "=", "f\"{left_brackets} ({pos_tag} {parse_word}) {right_brackets}\"", "\n", "", "else", ":", "\n", "# There are some bad annotations in the CONLL data.", "\n", "# They contain no information, so to make this explicit,", "\n", "# we just set the parse piece to be None which will result", "\n", "# in the overall parse tree being None.", "\n", "                ", "parse_piece", "=", "None", "\n", "\n", "", "lemmatised_word", "=", "conll_components", "[", "6", "]", "\n", "framenet_id", "=", "conll_components", "[", "7", "]", "\n", "word_sense", "=", "conll_components", "[", "8", "]", "\n", "speaker", "=", "conll_components", "[", "9", "]", "\n", "\n", "if", "not", "span_labels", ":", "\n", "# If this is the first word in the sentence, create", "\n", "# empty lists to collect the NER and SRL BIO labels.", "\n", "# We can't do this upfront, because we don't know how many", "\n", "# components we are collecting, as a sentence can have", "\n", "# variable numbers of SRL frames.", "\n", "                ", "span_labels", "=", "[", "[", "]", "for", "_", "in", "conll_components", "[", "10", ":", "-", "1", "]", "]", "\n", "# Create variables representing the current label for each label", "\n", "# sequence we are collecting.", "\n", "current_span_labels", "=", "[", "None", "for", "_", "in", "conll_components", "[", "10", ":", "-", "1", "]", "]", "\n", "\n", "", "self", ".", "_process_span_annotations_for_word", "(", "\n", "conll_components", "[", "10", ":", "-", "1", "]", ",", "span_labels", ",", "current_span_labels", "\n", ")", "\n", "\n", "# If any annotation marks this word as a verb predicate,", "\n", "# we need to record its index. This also has the side effect", "\n", "# of ordering the verbal predicates by their location in the", "\n", "# sentence, automatically aligning them with the annotations.", "\n", "word_is_verbal_predicate", "=", "any", "(", "\"(V\"", "in", "x", "for", "x", "in", "conll_components", "[", "11", ":", "-", "1", "]", ")", "\n", "if", "word_is_verbal_predicate", ":", "\n", "                ", "verbal_predicates", ".", "append", "(", "word", ")", "\n", "\n", "", "self", ".", "_process_coref_span_annotations_for_word", "(", "\n", "conll_components", "[", "-", "1", "]", ",", "index", ",", "clusters", ",", "coref_stacks", "\n", ")", "\n", "\n", "sentence", ".", "append", "(", "word", ")", "\n", "pos_tags", ".", "append", "(", "pos_tag", ")", "\n", "parse_pieces", ".", "append", "(", "parse_piece", ")", "\n", "predicate_lemmas", ".", "append", "(", "lemmatised_word", "if", "lemmatised_word", "!=", "\"-\"", "else", "None", ")", "\n", "predicate_framenet_ids", ".", "append", "(", "framenet_id", "if", "framenet_id", "!=", "\"-\"", "else", "None", ")", "\n", "word_senses", ".", "append", "(", "float", "(", "word_sense", ")", "if", "word_sense", "!=", "\"-\"", "else", "None", ")", "\n", "speakers", ".", "append", "(", "speaker", "if", "speaker", "!=", "\"-\"", "else", "None", ")", "\n", "\n", "", "named_entities", "=", "span_labels", "[", "0", "]", "\n", "\n", "for", "j", "in", "range", "(", "1", ",", "len", "(", "span_labels", ")", ")", ":", "\n", "            ", "labs", "=", "[", "\"A0\"", ",", "\"A1\"", ",", "\"A2\"", ",", "\"A3\"", ",", "\"A4\"", ",", "\"A5\"", "]", "\n", "if", "any", "(", "x", "in", "span_labels", "[", "j", "]", "for", "x", "in", "[", "\"B-R-A0\"", ",", "\"B-R-A1\"", ",", "\"B-R-A2\"", ",", "\"B-R-A3\"", ",", "\"B-R-A4\"", ",", "\"B-R-A5\"", "]", ")", ":", "\n", "                ", "for", "lab", "in", "labs", ":", "\n", "                    ", "if", "\"B-R-\"", "+", "lab", "in", "span_labels", "[", "j", "]", ":", "\n", "                        ", "for", "i", "in", "range", "(", "len", "(", "span_labels", "[", "j", "]", ")", ")", ":", "\n", "                            ", "if", "lab", "in", "span_labels", "[", "j", "]", "[", "i", "]", "and", "\"R\"", "not", "in", "span_labels", "[", "j", "]", "[", "i", "]", ":", "\n", "                                ", "span_labels", "[", "j", "]", "[", "i", "]", "=", "\"O\"", "\n", "", "elif", "\"R-\"", "+", "lab", "in", "span_labels", "[", "j", "]", "[", "i", "]", ":", "\n", "                                ", "span_labels", "[", "j", "]", "[", "i", "]", "=", "span_labels", "[", "j", "]", "[", "i", "]", "[", "0", "]", "+", "span_labels", "[", "j", "]", "[", "i", "]", "[", "3", ":", "]", "\n", "\n", "\n", "", "", "", "", "", "", "srl_frames", "=", "[", "\n", "(", "predicate", ",", "labels", ")", "for", "predicate", ",", "labels", "in", "zip", "(", "verbal_predicates", ",", "span_labels", "[", "1", ":", "]", ")", "\n", "]", "\n", "\n", "\n", "if", "all", "(", "parse_pieces", ")", ":", "\n", "            ", "parse_tree", "=", "Tree", ".", "fromstring", "(", "\"\"", ".", "join", "(", "parse_pieces", ")", ")", "\n", "", "else", ":", "\n", "            ", "parse_tree", "=", "None", "\n", "", "coref_span_tuples", ":", "Set", "[", "TypedSpan", "]", "=", "{", "\n", "(", "cluster_id", ",", "span", ")", "for", "cluster_id", ",", "span_list", "in", "clusters", ".", "items", "(", ")", "for", "span", "in", "span_list", "\n", "}", "\n", "return", "OntonotesSentence", "(", "\n", "document_id", ",", "\n", "sentence_id", ",", "\n", "sentence", ",", "\n", "pos_tags", ",", "\n", "parse_tree", ",", "\n", "predicate_lemmas", ",", "\n", "predicate_framenet_ids", ",", "\n", "word_senses", ",", "\n", "speakers", ",", "\n", "named_entities", ",", "\n", "srl_frames", ",", "\n", "coref_span_tuples", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.asofiaoliveira_srl_bert_pt.None.conll_reader.Ontonotes_mine._process_coref_span_annotations_for_word": [[243, 293], ["label.split", "int", "coref_stacks[].pop", "clusters[].append", "int", "clusters[].append", "int", "coref_stacks[].append"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "_process_coref_span_annotations_for_word", "(", "\n", "label", ":", "str", ",", "\n", "word_index", ":", "int", ",", "\n", "clusters", ":", "DefaultDict", "[", "int", ",", "List", "[", "Tuple", "[", "int", ",", "int", "]", "]", "]", ",", "\n", "coref_stacks", ":", "DefaultDict", "[", "int", ",", "List", "[", "int", "]", "]", ",", "\n", ")", "->", "None", ":", "\n", "        ", "\"\"\"\n        For a given coref label, add it to a currently open span(s), complete a span(s) or\n        ignore it, if it is outside of all spans. This method mutates the clusters and coref_stacks\n        dictionaries.\n\n        # Parameters\n\n        label : `str`\n            The coref label for this word.\n        word_index : `int`\n            The word index into the sentence.\n        clusters : `DefaultDict[int, List[Tuple[int, int]]]`\n            A dictionary mapping cluster ids to lists of inclusive spans into the\n            sentence.\n        coref_stacks : `DefaultDict[int, List[int]]`\n            Stacks for each cluster id to hold the start indices of active spans (spans\n            which we are inside of when processing a given word). Spans with the same id\n            can be nested, which is why we collect these opening spans on a stack, e.g:\n\n            [Greg, the baker who referred to [himself]_ID1 as 'the bread man']_ID1\n        \"\"\"", "\n", "if", "label", "!=", "\"-\"", ":", "\n", "            ", "for", "segment", "in", "label", ".", "split", "(", "\"|\"", ")", ":", "\n", "# The conll representation of coref spans allows spans to", "\n", "# overlap. If spans end or begin at the same word, they are", "\n", "# separated by a \"|\".", "\n", "                ", "if", "segment", "[", "0", "]", "==", "\"(\"", ":", "\n", "# The span begins at this word.", "\n", "                    ", "if", "segment", "[", "-", "1", "]", "==", "\")\"", ":", "\n", "# The span begins and ends at this word (single word span).", "\n", "                        ", "cluster_id", "=", "int", "(", "segment", "[", "1", ":", "-", "1", "]", ")", "\n", "clusters", "[", "cluster_id", "]", ".", "append", "(", "(", "word_index", ",", "word_index", ")", ")", "\n", "", "else", ":", "\n", "# The span is starting, so we record the index of the word.", "\n", "                        ", "cluster_id", "=", "int", "(", "segment", "[", "1", ":", "]", ")", "\n", "coref_stacks", "[", "cluster_id", "]", ".", "append", "(", "word_index", ")", "\n", "", "", "else", ":", "\n", "# The span for this id is ending, but didn't start at this word.", "\n", "# Retrieve the start index from the document state and", "\n", "# add the span to the clusters for this id.", "\n", "                    ", "cluster_id", "=", "int", "(", "segment", "[", ":", "-", "1", "]", ")", "\n", "start", "=", "coref_stacks", "[", "cluster_id", "]", ".", "pop", "(", ")", "\n", "clusters", "[", "cluster_id", "]", ".", "append", "(", "(", "start", ",", "word_index", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.asofiaoliveira_srl_bert_pt.None.conll_reader.Ontonotes_mine._process_span_annotations_for_word": [[295, 351], ["enumerate", "annotation.strip", "span_labels[].append", "span_labels[].append", "span_labels[].append", "span_labels[].append"], "methods", ["None"], ["", "", "", "", "def", "_process_span_annotations_for_word", "(", "\n", "self", ",", "\n", "annotations", ":", "List", "[", "str", "]", ",", "\n", "span_labels", ":", "List", "[", "List", "[", "str", "]", "]", ",", "\n", "current_span_labels", ":", "List", "[", "Optional", "[", "str", "]", "]", ",", "\n", ")", "->", "None", ":", "\n", "        ", "\"\"\"\n        Given a sequence of different label types for a single word and the current\n        span label we are inside, compute the BIO tag for each label and append to a list.\n\n        # Parameters\n\n        annotations : `List[str]`\n            A list of labels to compute BIO tags for.\n        span_labels : `List[List[str]]`\n            A list of lists, one for each annotation, to incrementally collect\n            the BIO tags for a sequence.\n        current_span_labels : `List[Optional[str]]`\n            The currently open span per annotation type, or `None` if there is no open span.\n        \"\"\"", "\n", "for", "annotation_index", ",", "annotation", "in", "enumerate", "(", "annotations", ")", ":", "\n", "# strip all bracketing information to", "\n", "# get the actual propbank label.", "\n", "            ", "label", "=", "annotation", ".", "strip", "(", "\"()*\"", ")", "\n", "\n", "if", "annotation_index", "!=", "0", ":", "\n", "                ", "if", "\"C-\"", "in", "label", "or", "\"R-\"", "in", "label", ":", "\n", "                    ", "label", "=", "label", "[", ":", "3", "]", "+", "label", "[", "5", ":", "]", "\n", "", "elif", "\"A\"", "in", "label", ":", "\n", "                    ", "label", "=", "label", "[", "0", "]", "+", "label", "[", "3", ":", "]", "\n", "", "", "if", "label", "in", "[", "\"AM-ADJ\"", ",", "\"R-AM-LOC\"", ",", "\"R-AM-TMP\"", ",", "\"AM-LVB\"", ",", "\"R-AM-MNR\"", ",", "\"AM-DSP\"", ",", "\"AA\"", ",", "\"R-AM-CAU\"", ",", "\"R-AM-ADV\"", ",", "\"R-AM-DIR\"", ",", "\"R-AM-PRP\"", ",", "\"R-AM-EXT\"", ",", "\"AM-PRR\"", ",", "\"AM-PRX\"", ",", "\"R-AM-GOL\"", ",", "\"R-AM-PNC\"", ",", "\"R-AM-COM\"", ",", "\"R-AM-PRD\"", ",", "\"R-AM-MOD\"", "]", ":", "\n", "                ", "span_labels", "[", "annotation_index", "]", ".", "append", "(", "\"O\"", ")", "\n", "continue", "\n", "\n", "", "if", "label", "==", "\"AM-PNC\"", ":", "\n", "                ", "label", "=", "\"AM-PRP\"", "\n", "", "if", "label", "==", "\"C-AM-PNC\"", ":", "\n", "                ", "label", "=", "\"C-AM-PRP\"", "\n", "\n", "", "if", "\"(\"", "in", "annotation", ":", "\n", "# Entering into a span for a particular semantic role label.", "\n", "# We append the label and set the current span for this annotation.", "\n", "                ", "bio_label", "=", "\"B-\"", "+", "label", "\n", "span_labels", "[", "annotation_index", "]", ".", "append", "(", "bio_label", ")", "\n", "current_span_labels", "[", "annotation_index", "]", "=", "label", "\n", "", "elif", "current_span_labels", "[", "annotation_index", "]", "is", "not", "None", ":", "\n", "# If there's no '(' token, but the current_span_label is not None,", "\n", "# then we are inside a span.", "\n", "                ", "bio_label", "=", "\"I-\"", "+", "current_span_labels", "[", "annotation_index", "]", "\n", "span_labels", "[", "annotation_index", "]", ".", "append", "(", "bio_label", ")", "\n", "", "else", ":", "\n", "# We're outside a span.", "\n", "                ", "span_labels", "[", "annotation_index", "]", ".", "append", "(", "\"O\"", ")", "\n", "# Exiting a span, so we reset the current span label for this annotation.", "\n", "", "if", "\")\"", "in", "annotation", ":", "\n", "                ", "current_span_labels", "[", "annotation_index", "]", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.asofiaoliveira_srl_bert_pt.None.conll_reader.SrlMineReader.__init__": [[386, 404], ["allennlp.data.dataset_readers.dataset_reader.DatasetReader.__init__", "transformers.AutoTokenizer.from_pretrained", "conll_reader.SrlMineReader.vocab.update", "allennlp.data.token_indexers.SingleIdTokenIndexer", "conll_reader.SrlMineReader.bert_tokenizer.convert_ids_to_tokens", "range"], "methods", ["home.repos.pwc.inspect_result.asofiaoliveira_srl_bert_pt.None.conll_reader.SrlMineReader.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "token_indexers", ":", "Dict", "[", "str", ",", "TokenIndexer", "]", "=", "None", ",", "\n", "domain_identifier", ":", "str", "=", "None", ",", "\n", "bert_model_name", ":", "str", "=", "None", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "**", "kwargs", ")", "\n", "self", ".", "_token_indexers", "=", "token_indexers", "or", "{", "\"tokens\"", ":", "SingleIdTokenIndexer", "(", ")", "}", "\n", "self", ".", "_domain_identifier", "=", "domain_identifier", "\n", "self", ".", "xlm", "=", "\"xlm\"", "in", "bert_model_name", "\n", "self", ".", "bert_tokenizer", "=", "AutoTokenizer", ".", "from_pretrained", "(", "bert_model_name", ")", "\n", "\n", "if", "self", ".", "xlm", ":", "\n", "            ", "self", ".", "vocab", "=", "{", "self", ".", "bert_tokenizer", ".", "convert_ids_to_tokens", "(", "i", ")", ":", "i", "for", "i", "in", "range", "(", "250001", ")", "}", "\n", "self", ".", "vocab", ".", "update", "(", "self", ".", "bert_tokenizer", ".", "added_tokens_encoder", ")", "\n", "\n", "", "self", ".", "lowercase_input", "=", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.asofiaoliveira_srl_bert_pt.None.conll_reader.SrlMineReader._wordpiece_tokenize_input": [[405, 456], ["start_offsets.append", "len", "end_offsets.append", "word_piece_tokens.extend", "conll_reader.SrlMineReader.bert_tokenizer._tokenize", "conll_reader.SrlMineReader.bert_tokenizer.wordpiece_tokenizer.tokenize"], "methods", ["None"], ["", "def", "_wordpiece_tokenize_input", "(", "\n", "self", ",", "tokens", ":", "List", "[", "str", "]", "\n", ")", "->", "Tuple", "[", "List", "[", "str", "]", ",", "List", "[", "int", "]", ",", "List", "[", "int", "]", "]", ":", "\n", "        ", "\"\"\"\n        Convert a list of tokens to wordpiece tokens and offsets, as well as adding\n        BERT CLS and SEP tokens to the begining and end of the sentence.\n\n        A slight oddity with this function is that it also returns the wordpiece offsets\n        corresponding to the _start_ of words as well as the end.\n\n        We need both of these offsets (or at least, it's easiest to use both), because we need\n        to convert the labels to tags using the end_offsets. However, when we are decoding a\n        BIO sequence inside the SRL model itself, it's important that we use the start_offsets,\n        because otherwise we might select an ill-formed BIO sequence from the BIO sequence on top of\n        wordpieces (this happens in the case that a word is split into multiple word pieces,\n        and then we take the last tag of the word, which might correspond to, e.g, I-V, which\n        would not be allowed as it is not preceeded by a B tag).\n\n        For example:\n\n        `annotate` will be bert tokenized as [\"anno\", \"##tate\"].\n        If this is tagged as [B-V, I-V] as it should be, we need to select the\n        _first_ wordpiece label to be the label for the token, because otherwise\n        we may end up with invalid tag sequences (we cannot start a new tag with an I).\n\n        # Returns\n\n        wordpieces : List[str]\n            The BERT wordpieces from the words in the sentence.\n        end_offsets : List[int]\n            Indices into wordpieces such that `[wordpieces[i] for i in end_offsets]`\n            results in the end wordpiece of each word being chosen.\n        start_offsets : List[int]\n            Indices into wordpieces such that `[wordpieces[i] for i in start_offsets]`\n            results in the start wordpiece of each word being chosen.\n        \"\"\"", "\n", "word_piece_tokens", ":", "List", "[", "str", "]", "=", "[", "]", "\n", "end_offsets", "=", "[", "]", "\n", "start_offsets", "=", "[", "]", "\n", "cumulative", "=", "0", "\n", "for", "token", "in", "tokens", ":", "\n", "            ", "if", "self", ".", "xlm", ":", "\n", "                ", "word_pieces", "=", "self", ".", "bert_tokenizer", ".", "_tokenize", "(", "token", ")", "\n", "", "else", ":", "\n", "                ", "word_pieces", "=", "self", ".", "bert_tokenizer", ".", "wordpiece_tokenizer", ".", "tokenize", "(", "token", ")", "\n", "", "start_offsets", ".", "append", "(", "cumulative", "+", "1", ")", "\n", "cumulative", "+=", "len", "(", "word_pieces", ")", "\n", "end_offsets", ".", "append", "(", "cumulative", ")", "\n", "word_piece_tokens", ".", "extend", "(", "word_pieces", ")", "\n", "\n", "", "return", "word_piece_tokens", ",", "end_offsets", ",", "start_offsets", "\n", "\n"]], "home.repos.pwc.inspect_result.asofiaoliveira_srl_bert_pt.None.conll_reader.SrlMineReader._read": [[457, 489], ["allennlp.common.file_utils.cached_path", "conll_reader.Ontonotes_mine", "logger.info", "conll_reader.SrlMineReader._ontonotes_subset", "logger.info", "allennlp.data.tokenizers.Token", "conll_reader.SrlMineReader.text_to_instance", "conll_reader.SrlMineReader.text_to_instance"], "methods", ["home.repos.pwc.inspect_result.asofiaoliveira_srl_bert_pt.None.conll_reader.SrlMineReader._ontonotes_subset", "home.repos.pwc.inspect_result.asofiaoliveira_srl_bert_pt.None.conll_reader.SrlMineReader.text_to_instance", "home.repos.pwc.inspect_result.asofiaoliveira_srl_bert_pt.None.conll_reader.SrlMineReader.text_to_instance"], ["", "@", "overrides", "\n", "def", "_read", "(", "self", ",", "file_path", ":", "str", ")", ":", "\n", "# if `file_path` is a URL, redirect to the cache", "\n", "        ", "file_path", "=", "cached_path", "(", "file_path", ")", "\n", "ontonotes_reader", "=", "Ontonotes_mine", "(", ")", "\n", "logger", ".", "info", "(", "\"Reading SRL instances from dataset files at: %s\"", ",", "file_path", ")", "\n", "if", "self", ".", "_domain_identifier", "is", "not", "None", ":", "\n", "            ", "logger", ".", "info", "(", "\n", "\"Filtering to only include file paths containing the %s domain\"", ",", "\n", "self", ".", "_domain_identifier", ",", "\n", ")", "\n", "\n", "", "for", "sentence", "in", "self", ".", "_ontonotes_subset", "(", "\n", "ontonotes_reader", ",", "file_path", ",", "self", ".", "_domain_identifier", "\n", ")", ":", "\n", "            ", "tokens", "=", "[", "Token", "(", "t", ")", "for", "t", "in", "sentence", ".", "words", "]", "\n", "if", "not", "sentence", ".", "srl_frames", ":", "\n", "# Sentence contains no predicates.", "\n", "                ", "tags", "=", "[", "\"O\"", "for", "_", "in", "tokens", "]", "\n", "verb_label", "=", "[", "0", "for", "_", "in", "tokens", "]", "\n", "s", "=", "self", ".", "text_to_instance", "(", "tokens", ",", "verb_label", ",", "tags", ")", "\n", "if", "s", "is", "None", ":", "\n", "                    ", "continue", "\n", "", "yield", "s", "\n", "\n", "", "else", ":", "\n", "                ", "for", "(", "_", ",", "tags", ")", "in", "sentence", ".", "srl_frames", ":", "\n", "                    ", "verb_indicator", "=", "[", "1", "if", "label", "[", "-", "2", ":", "]", "==", "\"-V\"", "else", "0", "for", "label", "in", "tags", "]", "\n", "s", "=", "self", ".", "text_to_instance", "(", "tokens", ",", "verb_indicator", ",", "tags", ")", "\n", "if", "s", "is", "None", ":", "\n", "                        ", "continue", "\n", "", "yield", "s", "\n", "\n"]], "home.repos.pwc.inspect_result.asofiaoliveira_srl_bert_pt.None.conll_reader.SrlMineReader._ontonotes_subset": [[490, 502], ["ontonotes_reader.dataset_path_iterator", "ontonotes_reader.sentence_iterator"], "methods", ["home.repos.pwc.inspect_result.asofiaoliveira_srl_bert_pt.None.conll_reader.Ontonotes_mine.dataset_path_iterator", "home.repos.pwc.inspect_result.asofiaoliveira_srl_bert_pt.None.conll_reader.Ontonotes_mine.sentence_iterator"], ["", "", "", "", "@", "staticmethod", "\n", "def", "_ontonotes_subset", "(", "\n", "ontonotes_reader", ":", "Ontonotes_mine", ",", "file_path", ":", "str", ",", "domain_identifier", ":", "str", "\n", ")", "->", "Iterable", "[", "OntonotesSentence", "]", ":", "\n", "        ", "\"\"\"\n        Iterates over the Ontonotes 5.0 dataset using an optional domain identifier.\n        If the domain identifier is present, only examples which contain the domain\n        identifier in the file path are yielded.\n        \"\"\"", "\n", "for", "conll_file", "in", "ontonotes_reader", ".", "dataset_path_iterator", "(", "file_path", ")", ":", "\n", "            ", "if", "domain_identifier", "is", "None", "or", "f\"/{domain_identifier}/\"", "in", "conll_file", ":", "\n", "                ", "yield", "from", "ontonotes_reader", ".", "sentence_iterator", "(", "conll_file", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.asofiaoliveira_srl_bert_pt.None.conll_reader.SrlMineReader.text_to_instance": [[503, 565], ["all", "allennlp.data.fields.MetadataField", "allennlp.data.instance.Instance", "conll_reader.SrlMineReader._wordpiece_tokenize_input", "allennlp_models.structured_prediction.dataset_readers.srl._convert_verb_indices_to_wordpiece_indices", "allennlp.data.fields.SequenceLabelField", "allennlp.data.fields.TextField", "allennlp.data.fields.SequenceLabelField", "verb_label.index", "len", "logger.info", "allennlp.data.fields.TextField", "allennlp.data.fields.TextField", "allennlp_models.structured_prediction.dataset_readers.srl._convert_tags_to_wordpiece_tags", "allennlp.data.fields.SequenceLabelField", "allennlp.data.fields.SequenceLabelField", "allennlp.data.tokenizers.Token", "allennlp.data.tokenizers.Token", "len"], "methods", ["home.repos.pwc.inspect_result.asofiaoliveira_srl_bert_pt.None.conll_reader.SrlMineReader._wordpiece_tokenize_input", "home.repos.pwc.inspect_result.asofiaoliveira_srl_bert_pt.None.my_reader._convert_verb_indices_to_wordpiece_indices", "home.repos.pwc.inspect_result.asofiaoliveira_srl_bert_pt.None.my_reader._convert_tags_to_wordpiece_tags"], ["", "", "", "def", "text_to_instance", "(", "# type: ignore", "\n", "self", ",", "tokens", ":", "List", "[", "Token", "]", ",", "verb_label", ":", "List", "[", "int", "]", ",", "tags", ":", "List", "[", "str", "]", "=", "None", "\n", ")", "->", "Instance", ":", "\n", "        ", "\"\"\"\n        We take `pre-tokenized` input here, along with a verb label.  The verb label should be a\n        one-hot binary vector, the same length as the tokens, indicating the position of the verb\n        to find arguments for.\n        \"\"\"", "\n", "\n", "metadata_dict", ":", "Dict", "[", "str", ",", "Any", "]", "=", "{", "}", "\n", "if", "self", ".", "bert_tokenizer", "is", "not", "None", ":", "\n", "            ", "wordpieces", ",", "offsets", ",", "start_offsets", "=", "self", ".", "_wordpiece_tokenize_input", "(", "[", "t", ".", "text", "for", "t", "in", "tokens", "]", ")", "\n", "if", "self", ".", "xlm", ":", "\n", "                ", "wordpieces", "=", "[", "\"<s>\"", "]", "+", "wordpieces", "+", "[", "\"</s>\"", "]", "\n", "", "else", ":", "\n", "                ", "wordpieces", "=", "[", "\"[CLS]\"", "]", "+", "wordpieces", "+", "[", "\"[SEP]\"", "]", "\n", "", "new_verbs", "=", "_convert_verb_indices_to_wordpiece_indices", "(", "verb_label", ",", "offsets", ")", "\n", "metadata_dict", "[", "\"offsets\"", "]", "=", "start_offsets", "\n", "# In order to override the indexing mechanism, we need to set the `text_id`", "\n", "# attribute directly. This causes the indexing to use this id.", "\n", "if", "self", ".", "xlm", ":", "\n", "                ", "text_field", "=", "TextField", "(", "[", "Token", "(", "t", ",", "text_id", "=", "self", ".", "vocab", "[", "t", "]", ")", "for", "t", "in", "wordpieces", "]", ",", "\n", "token_indexers", "=", "self", ".", "_token_indexers", ")", "\n", "", "else", ":", "\n", "                ", "text_field", "=", "TextField", "(", "[", "Token", "(", "t", ",", "text_id", "=", "self", ".", "bert_tokenizer", ".", "vocab", "[", "t", "]", ")", "for", "t", "in", "wordpieces", "]", ",", "\n", "token_indexers", "=", "self", ".", "_token_indexers", ")", "\n", "\n", "", "verb_indicator", "=", "SequenceLabelField", "(", "new_verbs", ",", "text_field", ")", "\n", "\n", "", "else", ":", "\n", "            ", "text_field", "=", "TextField", "(", "tokens", ",", "token_indexers", "=", "self", ".", "_token_indexers", ")", "\n", "verb_indicator", "=", "SequenceLabelField", "(", "verb_label", ",", "text_field", ")", "\n", "\n", "", "fields", ":", "Dict", "[", "str", ",", "Field", "]", "=", "{", "}", "\n", "fields", "[", "\"tokens\"", "]", "=", "text_field", "\n", "fields", "[", "\"verb_indicator\"", "]", "=", "verb_indicator", "\n", "\n", "if", "all", "(", "x", "==", "0", "for", "x", "in", "verb_label", ")", ":", "\n", "            ", "verb", "=", "None", "\n", "verb_index", "=", "None", "\n", "", "else", ":", "\n", "            ", "verb_index", "=", "verb_label", ".", "index", "(", "1", ")", "\n", "verb", "=", "tokens", "[", "verb_index", "]", ".", "text", "\n", "\n", "", "metadata_dict", "[", "\"words\"", "]", "=", "[", "x", ".", "text", "for", "x", "in", "tokens", "]", "\n", "metadata_dict", "[", "\"verb\"", "]", "=", "verb", "\n", "metadata_dict", "[", "\"verb_index\"", "]", "=", "verb_index", "\n", "\n", "if", "tags", ":", "\n", "            ", "if", "self", ".", "bert_tokenizer", "is", "not", "None", ":", "\n", "                ", "new_tags", "=", "_convert_tags_to_wordpiece_tags", "(", "tags", ",", "offsets", ")", "\n", "fields", "[", "\"tags\"", "]", "=", "SequenceLabelField", "(", "new_tags", ",", "text_field", ")", "\n", "", "else", ":", "\n", "                ", "fields", "[", "\"tags\"", "]", "=", "SequenceLabelField", "(", "tags", ",", "text_field", ")", "\n", "", "metadata_dict", "[", "\"gold_tags\"", "]", "=", "tags", "\n", "\n", "", "fields", "[", "\"metadata\"", "]", "=", "MetadataField", "(", "metadata_dict", ")", "\n", "\n", "if", "len", "(", "wordpieces", ")", ">", "190", ":", "\n", "            ", "logger", ".", "info", "(", "\"%s %i\"", "%", "(", "wordpieces", ",", "len", "(", "wordpieces", ")", ")", ")", "\n", "return", "None", "\n", "", "return", "Instance", "(", "fields", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.asofiaoliveira_srl_bert_pt.Choose Best Model.tool.calc_for_model": [[45, 74], ["range", "dict", "range", "metrics_avg.items", "[].copy", "evalu.get_metric", "zip", "metrics[].items", "metrics[].keys", "str", "str", "len", "metrics_avg.get", "dict.get", "str", "str", "str", "str", "str", "str", "str"], "function", ["None"], ["def", "calc_for_model", "(", "model", ")", ":", "\n", "    ", "metrics", "=", "{", "}", "\n", "scores", "=", "{", "}", "\n", "for", "fold", "in", "range", "(", "10", ")", ":", "\n", "        ", "tmp", "=", "data_to_use", "[", "model", "]", "[", "str", "(", "fold", ")", "]", ".", "copy", "(", ")", "\n", "scores", "[", "str", "(", "fold", ")", "]", "=", "{", "}", "\n", "for", "sc", "in", "[", "\"true_positives\"", ",", "\"false_positives\"", ",", "\"false_negatives\"", "]", ":", "\n", "            ", "scores", "[", "str", "(", "fold", ")", "]", "[", "sc", "]", "=", "{", "}", "\n", "for", "tag", "in", "tmp", "[", "sc", "]", ":", "\n", "                ", "if", "tag", "in", "not_ignore", "+", "[", "\"V\"", "]", ":", "\n", "                    ", "scores", "[", "str", "(", "fold", ")", "]", "[", "sc", "]", "[", "tag", "]", "=", "tmp", "[", "sc", "]", "[", "tag", "]", "\n", "", "", "", "evalu", ".", "_true_positives", "=", "scores", "[", "str", "(", "fold", ")", "]", "[", "\"true_positives\"", "]", "\n", "evalu", ".", "_false_positives", "=", "scores", "[", "str", "(", "fold", ")", "]", "[", "\"false_positives\"", "]", "\n", "evalu", ".", "_false_negatives", "=", "scores", "[", "str", "(", "fold", ")", "]", "[", "\"false_negatives\"", "]", "\n", "metrics", "[", "str", "(", "fold", ")", "]", "=", "evalu", ".", "get_metric", "(", ")", "\n", "\n", "", "metrics_avg", "=", "metrics", "[", "\"0\"", "]", "\n", "\n", "count", "=", "dict", "(", "zip", "(", "metrics", "[", "\"0\"", "]", ".", "keys", "(", ")", ",", "[", "1", "]", "*", "len", "(", "metrics", "[", "\"0\"", "]", ")", ")", ")", "\n", "\n", "for", "fold", "in", "range", "(", "1", ",", "10", ")", ":", "\n", "        ", "for", "key", ",", "value", "in", "metrics", "[", "str", "(", "fold", ")", "]", ".", "items", "(", ")", ":", "\n", "            ", "metrics_avg", "[", "key", "]", "=", "metrics_avg", ".", "get", "(", "key", ",", "0", ")", "+", "value", "\n", "count", "[", "key", "]", "=", "count", ".", "get", "(", "key", ",", "0", ")", "+", "1", "\n", "\n", "", "", "for", "key", ",", "value", "in", "metrics_avg", ".", "items", "(", ")", ":", "\n", "        ", "metrics_avg", "[", "key", "]", "/=", "count", "[", "key", "]", "\n", "\n", "", "return", "metrics_avg", "\n", "\n"]], "home.repos.pwc.inspect_result.asofiaoliveira_srl_bert_pt.Choose Best Model.tool.process_metrics": [[75, 81], ["data.items", "round", "key.split"], "function", ["None"], ["", "def", "process_metrics", "(", "data", ",", "metric", "=", "\"f1-measure\"", ")", ":", "\n", "    ", "dic", "=", "{", "}", "\n", "for", "key", ",", "value", "in", "data", ".", "items", "(", ")", ":", "\n", "        ", "if", "metric", "in", "key", ":", "\n", "            ", "dic", "[", "key", ".", "split", "(", "metric", "+", "\"-\"", ")", "[", "1", "]", "]", "=", "round", "(", "value", "*", "100", ",", "2", ")", "\n", "", "", "return", "dic", "\n", "\n"]]}