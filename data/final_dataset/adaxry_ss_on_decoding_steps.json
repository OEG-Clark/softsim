{"home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet_decoding_step_ss_mle_absolute.translation.TranslationProphetnetTask.__init__": [[10, 12], ["fairseq.tasks.translation.TranslationTask.__init__"], "methods", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.transformer_lrp.TransformerLRP.__init__"], ["    ", "def", "__init__", "(", "self", ",", "args", ",", "src_dict", ",", "tgt_dict", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "args", ",", "src_dict", ",", "tgt_dict", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet_decoding_step_ss_mle_absolute.translation.TranslationProphetnetTask.load_dictionary": [[13, 16], ["bert_dictionary.BertDictionary.load_from_file"], "methods", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.bert_dictionary.BertDictionary.load_from_file"], ["", "@", "classmethod", "\n", "def", "load_dictionary", "(", "cls", ",", "filename", ")", ":", "\n", "        ", "return", "BertDictionary", ".", "load_from_file", "(", "filename", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet_decoding_step_ss_mle_absolute.translation.TranslationProphetnetTask.max_positions": [[17, 20], ["None"], "methods", ["None"], ["", "def", "max_positions", "(", "self", ")", ":", "\n", "        ", "\"\"\"Return the max sentence length allowed by the task.\"\"\"", "\n", "return", "(", "self", ".", "args", ".", "max_source_positions", ",", "self", ".", "args", ".", "max_target_positions", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet_decoding_step_ss_mle_absolute.translation.TranslationProphetnetTask.begin_epoch": [[21, 24], ["hasattr", "model.set_epoch"], "methods", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet_decoding_step_ss_mle_absolute.ngram_s2s_model.NgramTransformerProphetModel.set_epoch"], ["", "def", "begin_epoch", "(", "self", ",", "epoch", ",", "model", ",", "max_epoch", ")", ":", "\n", "        ", "if", "hasattr", "(", "model", ",", "'set_epoch'", ")", "and", "model", "!=", "self", ":", "\n", "            ", "model", ".", "set_epoch", "(", "epoch", ",", "max_epoch", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet_decoding_step_ss_mle_absolute.learned_positional_embedding.LearnedPositionalEmbedding.__init__": [[14, 22], ["torch.Embedding.__init__"], "methods", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.transformer_lrp.TransformerLRP.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "num_embeddings", ":", "int", ",", "\n", "embedding_dim", ":", "int", ",", "\n", "padding_idx", ":", "int", ",", "\n", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "num_embeddings", ",", "embedding_dim", ",", "padding_idx", ")", "\n", "self", ".", "onnx_trace", "=", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet_decoding_step_ss_mle_absolute.learned_positional_embedding.LearnedPositionalEmbedding.forward": [[23, 42], ["super().forward", "input.data.new().fill_", "fairseq.utils.make_positions", "int", "input.data.new", "input.size"], "methods", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_multihead_attention.NgramMultiheadAttention.forward", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.distribute.size"], ["", "def", "forward", "(", "self", ",", "input", ",", "incremental_state", "=", "None", ",", "positions", "=", "None", ")", ":", "\n", "        ", "\"\"\"Input is expected to be of size [bsz x seqlen].\"\"\"", "\n", "assert", "(", "\n", "(", "positions", "is", "None", ")", "or", "(", "self", ".", "padding_idx", "is", "None", ")", "\n", ")", ",", "\"If positions is pre-computed then padding_idx should not be set.\"", "\n", "\n", "if", "positions", "is", "None", ":", "\n", "            ", "if", "incremental_state", "is", "not", "None", ":", "\n", "# positions is the same for every token when decoding a single step", "\n", "# Without the int() cast, it doesn't work in some cases when exporting to ONNX", "\n", "                ", "positions", "=", "input", ".", "data", ".", "new", "(", "1", ",", "1", ")", ".", "fill_", "(", "int", "(", "self", ".", "padding_idx", "+", "input", ".", "size", "(", "1", ")", ")", ")", "\n", "", "else", ":", "\n", "                ", "positions", "=", "utils", ".", "make_positions", "(", "\n", "input", ".", "data", ",", "self", ".", "padding_idx", ",", "onnx_trace", "=", "self", ".", "onnx_trace", ",", "\n", ")", "\n", "", "real_positions", "=", "positions", "\n", "", "else", ":", "\n", "            ", "real_positions", "=", "positions", "\n", "", "return", "super", "(", ")", ".", "forward", "(", "positions", ")", ",", "real_positions", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet_decoding_step_ss_mle_absolute.learned_positional_embedding.LearnedPositionalEmbedding.max_positions": [[43, 49], ["None"], "methods", ["None"], ["", "def", "max_positions", "(", "self", ")", ":", "\n", "        ", "\"\"\"Maximum number of supported positions.\"\"\"", "\n", "if", "self", ".", "padding_idx", "is", "not", "None", ":", "\n", "            ", "return", "self", ".", "num_embeddings", "-", "self", ".", "padding_idx", "-", "1", "\n", "", "else", ":", "\n", "            ", "return", "self", ".", "num_embeddings", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet_decoding_step_ss_mle_absolute.learned_positional_embedding.LearnedPositionalEmbedding._forward": [[50, 52], ["super().forward"], "methods", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_multihead_attention.NgramMultiheadAttention.forward"], ["", "", "def", "_forward", "(", "self", ",", "positions", ")", ":", "\n", "        ", "return", "super", "(", ")", ".", "forward", "(", "positions", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet_decoding_step_ss_mle_absolute.ngram_criterions.NgramLmLoss.__init__": [[21, 25], ["fairseq.criterions.FairseqCriterion.__init__"], "methods", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.transformer_lrp.TransformerLRP.__init__"], ["def", "__init__", "(", "self", ",", "args", ",", "task", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "args", ",", "task", ")", "\n", "self", ".", "eps", "=", "args", ".", "label_smoothing", "\n", "self", ".", "disable_ngram_loss", "=", "args", ".", "disable_ngram_loss", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet_decoding_step_ss_mle_absolute.ngram_criterions.NgramLmLoss.add_args": [[26, 34], ["parser.add_argument", "parser.add_argument"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "add_args", "(", "parser", ")", ":", "\n", "        ", "\"\"\"Add criterion-specific arguments to the parser.\"\"\"", "\n", "# fmt: off", "\n", "parser", ".", "add_argument", "(", "'--label-smoothing'", ",", "default", "=", "0.", ",", "type", "=", "float", ",", "metavar", "=", "'D'", ",", "\n", "help", "=", "'epsilon for label smoothing, 0 means no label smoothing'", ")", "\n", "parser", ".", "add_argument", "(", "'--disable-ngram-loss'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'only comput basic stat'", ")", "\n", "# fmt: on", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet_decoding_step_ss_mle_absolute.ngram_criterions.NgramLmLoss.forward": [[36, 100], ["model", "model.get_targets", "model.size", "targets.unsqueeze_().repeat.unsqueeze_().repeat.new_zeros().fill_", "range", "torch.log_softmax", "torch.log_softmax", "targets.unsqueeze_().repeat.unsqueeze_().repeat.unsqueeze_().repeat", "torch.nll_loss", "torch.nll_loss", "targets.unsqueeze_().repeat.unsqueeze_().repeat.ne().int().sum().item", "torch.zeros_like().fill_", "torch.zeros_like().fill_", "torch.zeros_like().fill_", "torch.zeros_like().fill_", "logits.view", "targets.unsqueeze_().repeat.unsqueeze_().repeat.view", "targets.unsqueeze_().repeat.unsqueeze_().repeat.ne().view", "smooth_loss.sum.sum.sum", "targets.unsqueeze_().repeat.unsqueeze_().repeat.new_zeros", "torch.where", "torch.where", "torch.where", "torch.where", "logits.size", "targets.unsqueeze_().repeat.unsqueeze_().repeat.unsqueeze_", "torch.log_softmax.sum", "torch.log_softmax.size", "targets.unsqueeze_().repeat.unsqueeze_().repeat.ne().int().sum", "fairseq.utils.item", "targets.unsqueeze_().repeat.unsqueeze_().repeat.size", "targets.unsqueeze_().repeat.unsqueeze_().repeat.size", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "targets.unsqueeze_().repeat.unsqueeze_().repeat.ne", "targets.unsqueeze_().repeat.unsqueeze_().repeat.ne().int", "targets.unsqueeze_().repeat.unsqueeze_().repeat.ne"], "methods", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.distribute.size", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.distribute.size", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.distribute.size", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.distribute.size", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.distribute.size"], ["", "def", "forward", "(", "self", ",", "model", ",", "sample", ",", "reduce", "=", "True", ")", ":", "\n", "        ", "\"\"\"Compute the loss for the given sample.\n        Returns a tuple with three elements:\n        1) the loss\n        2) the sample size, which is used as the denominator for the gradient\n        3) logging outputs to display while training\n        \"\"\"", "\n", "# compute MLM loss", "\n", "#  [2, ngram, B, T, V]", "\n", "#print(sample.keys(), sample['net_input']['tgt_lengths'])", "\n", "logits_list", "=", "model", "(", "**", "sample", "[", "'net_input'", "]", ",", "return_all_hiddens", "=", "False", ")", "\n", "targets", "=", "model", ".", "get_targets", "(", "sample", ",", "[", "logits_list", "]", ")", "\n", "\n", "\n", "#ngram = len(logits_list)", "\n", "ngram", "=", "logits_list", ".", "size", "(", "1", ")", "\n", "# [B, ngram, T]", "\n", "expend_targets", "=", "targets", ".", "new_zeros", "(", "ngram", ",", "targets", ".", "size", "(", "0", ")", ",", "targets", ".", "size", "(", "1", ")", ")", ".", "fill_", "(", "self", ".", "padding_idx", ")", "\n", "for", "i", "in", "range", "(", "ngram", ")", ":", "\n", "            ", "if", "i", ">", "0", "and", "self", ".", "disable_ngram_loss", ":", "\n", "                ", "break", "\n", "\n", "", "padding_targets", "=", "torch", ".", "zeros_like", "(", "targets", ")", ".", "fill_", "(", "self", ".", "padding_idx", ")", "\n", "if", "'target_idx'", "in", "sample", ":", "\n", "                ", "expend_targets", "[", "i", ",", ":", ",", ":", "]", "=", "torch", ".", "where", "(", "sample", "[", "'target_idx'", "]", ">=", "i", ",", "targets", ",", "padding_targets", ")", "\n", "", "else", ":", "\n", "                ", "expend_targets", "[", "i", ",", ":", ",", ":", "]", "=", "targets", "\n", "# [B, ngram, T]", "\n", "", "", "targets", "=", "expend_targets", "\n", "\n", "#logits = torch.cat(logits_list, dim=0) #.view(ngram, *logits_list[0].size())", "\n", "logits", "=", "logits_list", "\n", "\n", "lprobs", "=", "F", ".", "log_softmax", "(", "\n", "logits", ".", "view", "(", "-", "1", ",", "logits", ".", "size", "(", "-", "1", ")", ")", ",", "\n", "dim", "=", "-", "1", ",", "\n", "dtype", "=", "torch", ".", "float32", ",", "\n", ")", "\n", "targets", "=", "targets", ".", "unsqueeze_", "(", "0", ")", ".", "repeat", "(", "[", "2", ",", "1", ",", "1", ",", "1", "]", ")", "\n", "loss", "=", "F", ".", "nll_loss", "(", "\n", "lprobs", ",", "\n", "targets", ".", "view", "(", "-", "1", ")", ",", "\n", "reduction", "=", "'sum'", ",", "\n", "ignore_index", "=", "self", ".", "padding_idx", ",", "\n", ")", "\n", "\n", "if", "self", ".", "eps", ">", "0.", ":", "\n", "            ", "smooth_loss", "=", "-", "lprobs", ".", "sum", "(", "dim", "=", "-", "1", ",", "keepdim", "=", "True", ")", "\n", "non_pad_mask", "=", "targets", ".", "ne", "(", "self", ".", "padding_idx", ")", ".", "view", "(", "-", "1", ")", "\n", "smooth_loss", "=", "smooth_loss", "[", "non_pad_mask", "]", "\n", "smooth_loss", "=", "smooth_loss", ".", "sum", "(", ")", "\n", "\n", "eps_i", "=", "self", ".", "eps", "/", "lprobs", ".", "size", "(", "-", "1", ")", "\n", "loss", "=", "(", "1.", "-", "self", ".", "eps", ")", "*", "loss", "+", "eps_i", "*", "smooth_loss", "\n", "\n", "", "sample_size", "=", "targets", ".", "ne", "(", "self", ".", "padding_idx", ")", ".", "int", "(", ")", ".", "sum", "(", ")", ".", "item", "(", ")", "\n", "\n", "logging_output", "=", "{", "\n", "'loss'", ":", "utils", ".", "item", "(", "loss", ".", "data", ")", "if", "reduce", "else", "loss", ".", "data", ",", "\n", "'ntokens'", ":", "sample", "[", "'ntokens'", "]", ",", "\n", "'nsentences'", ":", "sample", "[", "'nsentences'", "]", ",", "\n", "'sample_size'", ":", "sample_size", ",", "\n", "}", "\n", "return", "loss", ",", "sample_size", ",", "logging_output", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet_decoding_step_ss_mle_absolute.ngram_criterions.NgramLmLoss.aggregate_logging_outputs": [[101, 116], ["sum", "sum", "sum", "sum", "log.get", "log.get", "log.get", "log.get", "math.log"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "aggregate_logging_outputs", "(", "logging_outputs", ")", ":", "\n", "        ", "\"\"\"Aggregate logging outputs from data parallel training.\"\"\"", "\n", "loss", "=", "sum", "(", "log", ".", "get", "(", "'loss'", ",", "0", ")", "for", "log", "in", "logging_outputs", ")", "\n", "ntokens", "=", "sum", "(", "log", ".", "get", "(", "'ntokens'", ",", "0", ")", "for", "log", "in", "logging_outputs", ")", "\n", "nsentences", "=", "sum", "(", "log", ".", "get", "(", "'nsentences'", ",", "0", ")", "for", "log", "in", "logging_outputs", ")", "\n", "sample_size", "=", "sum", "(", "log", ".", "get", "(", "'sample_size'", ",", "0", ")", "for", "log", "in", "logging_outputs", ")", "\n", "\n", "agg_output", "=", "{", "\n", "'loss'", ":", "loss", "/", "sample_size", "/", "math", ".", "log", "(", "2", ")", ",", "\n", "'ntokens'", ":", "ntokens", ",", "\n", "'nsentences'", ":", "nsentences", ",", "\n", "'sample_size'", ":", "sample_size", ",", "\n", "}", "\n", "return", "agg_output", "\n", "", "", ""]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet_decoding_step_ss_mle_absolute.ngram_s2s_model.NgramTransformerProphetModel.__init__": [[39, 41], ["fairseq.models.FairseqEncoderDecoderModel.__init__"], "methods", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.transformer_lrp.TransformerLRP.__init__"], ["def", "__init__", "(", "self", ",", "encoder", ",", "decoder", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "encoder", ",", "decoder", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet_decoding_step_ss_mle_absolute.ngram_s2s_model.NgramTransformerProphetModel.add_args": [[42, 100], ["parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "fairseq.utils.get_available_activation_fns"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "add_args", "(", "parser", ")", ":", "\n", "        ", "\"\"\"Add model-specific arguments to the parser.\"\"\"", "\n", "parser", ".", "add_argument", "(", "'--ngram'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'num of predicting grams'", ")", "\n", "parser", ".", "add_argument", "(", "'--num_buckets'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'num of buckets for relative position'", ")", "\n", "parser", ".", "add_argument", "(", "'--relative_max_distance'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'num of bucket for relative position'", ")", "\n", "# fmt: off", "\n", "parser", ".", "add_argument", "(", "'--activation-fn'", ",", "\n", "choices", "=", "utils", ".", "get_available_activation_fns", "(", ")", ",", "\n", "help", "=", "'activation function to use'", ")", "\n", "parser", ".", "add_argument", "(", "'--dropout'", ",", "type", "=", "float", ",", "metavar", "=", "'D'", ",", "\n", "help", "=", "'dropout probability'", ")", "\n", "parser", ".", "add_argument", "(", "'--attention-dropout'", ",", "type", "=", "float", ",", "metavar", "=", "'D'", ",", "\n", "help", "=", "'dropout probability for attention weights'", ")", "\n", "parser", ".", "add_argument", "(", "'--activation-dropout'", ",", "type", "=", "float", ",", "metavar", "=", "'D'", ",", "\n", "help", "=", "'dropout probability after activation in FFN.'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--encoder-embed-dim'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'encoder embedding dimension'", ")", "\n", "parser", ".", "add_argument", "(", "'--encoder-ffn-embed-dim'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'encoder embedding dimension for FFN'", ")", "\n", "parser", ".", "add_argument", "(", "'--encoder-layers'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'num encoder layers'", ")", "\n", "parser", ".", "add_argument", "(", "'--encoder-attention-heads'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'num encoder attention heads'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--decoder-embed-dim'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'decoder embedding dimension'", ")", "\n", "parser", ".", "add_argument", "(", "'--decoder-ffn-embed-dim'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'decoder embedding dimension for FFN'", ")", "\n", "parser", ".", "add_argument", "(", "'--decoder-layers'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'num decoder layers'", ")", "\n", "parser", ".", "add_argument", "(", "'--decoder-attention-heads'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'num decoder attention heads'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--share-all-embeddings'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'share encoder, decoder and output embeddings'", "\n", "' (requires shared dictionary and embed dim)'", ")", "\n", "parser", ".", "add_argument", "(", "'--load-from-pretrained-model'", ",", "type", "=", "str", ",", "default", "=", "None", ",", "\n", "help", "=", "'Load from pretrained model'", ")", "\n", "parser", ".", "add_argument", "(", "'--load-sep'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'load pretrained [SEP] weight into [X_SEP]. ([SEP] used as eos in fine tuning)'", ")", "\n", "# fmt: on", "\n", "parser", ".", "add_argument", "(", "'--fixed_scheduled_sampling_rate'", ",", "type", "=", "float", ",", "default", "=", "0.5", ",", "\n", "help", "=", "''", ")", "\n", "parser", ".", "add_argument", "(", "'--trainstep_schduled_sampling_strategy'", ",", "type", "=", "str", ",", "default", "=", "\"linear\"", ",", "\n", "help", "=", "'trainstep_schduled_sampling_strategy linear or exp'", ")", "\n", "parser", ".", "add_argument", "(", "'--decodingstep_schduled_sampling_strategy'", ",", "type", "=", "str", ",", "default", "=", "\"linear\"", ",", "\n", "help", "=", "'decodingstep_schduled_sampling_strategy linear or exp'", ")", "\n", "parser", ".", "add_argument", "(", "'--exp_radix'", ",", "type", "=", "float", ",", "default", "=", "0.8", ",", "\n", "help", "=", "'trainstep_schduled_sampling_strategy linear or exp'", ")", "\n", "parser", ".", "add_argument", "(", "'--decodingstep_exp_radix'", ",", "type", "=", "float", ",", "default", "=", "0.99", ",", "\n", "help", "=", "'0.99 for 512'", ")", "\n", "parser", ".", "add_argument", "(", "'--decodingstep_sigmoid_k'", ",", "type", "=", "float", ",", "default", "=", "60", ",", "\n", "help", "=", "'60 for 512'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet_decoding_step_ss_mle_absolute.ngram_s2s_model.NgramTransformerProphetModel.get_normalized_probs": [[101, 113], ["hasattr", "ngram_s2s_model.NgramTransformerProphetModel.decoder.get_normalized_probs", "ngram_s2s_model.NgramTransformerProphetModel.decoder.get_normalized_probs", "torch.is_tensor", "torch.is_tensor", "torch.is_tensor", "torch.is_tensor", "torch.is_tensor", "torch.is_tensor", "torch.is_tensor", "torch.is_tensor", "torch.is_tensor", "net_output.float", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.softmax", "torch.softmax", "torch.softmax"], "methods", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_s2s_model.NgramTransformerDecoder.get_normalized_probs", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_s2s_model.NgramTransformerDecoder.get_normalized_probs"], ["", "def", "get_normalized_probs", "(", "self", ",", "net_output", ",", "log_probs", ",", "sample", "=", "None", ")", ":", "\n", "        ", "\"\"\"Get normalized probabilities (or log probs) from a net's output.\"\"\"", "\n", "if", "hasattr", "(", "self", ",", "'decoder'", ")", ":", "\n", "            ", "return", "self", ".", "decoder", ".", "get_normalized_probs", "(", "net_output", ",", "log_probs", ",", "sample", ")", "\n", "return", "self", ".", "decoder", ".", "get_normalized_probs", "(", "net_output", ",", "log_probs", ",", "sample", ")", "\n", "", "elif", "torch", ".", "is_tensor", "(", "net_output", ")", ":", "\n", "            ", "logits", "=", "net_output", ".", "float", "(", ")", "\n", "if", "log_probs", ":", "\n", "                ", "return", "F", ".", "log_softmax", "(", "logits", ",", "dim", "=", "-", "1", ")", "\n", "", "else", ":", "\n", "                ", "return", "F", ".", "softmax", "(", "logits", ",", "dim", "=", "-", "1", ")", "\n", "", "", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet_decoding_step_ss_mle_absolute.ngram_s2s_model.NgramTransformerProphetModel.build_model": [[114, 213], ["ngram_s2s_model.base_architecture", "ngram_s2s_model.TransformerEncoder", "ngram_s2s_model.NgramTransformerDecoder", "ngram_s2s_model.NgramTransformerProphetModel", "ngram_s2s_model.NgramTransformerProphetModel", "hasattr", "hasattr", "len", "dictionary.pad", "ngram_s2s_model.Embedding", "ngram_s2s_model.NgramTransformerProphetModel.build_model.build_embedding"], "methods", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_s2s_model.base_architecture", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_s2s_model.Embedding"], ["", "@", "classmethod", "\n", "def", "build_model", "(", "cls", ",", "args", ",", "task", ")", ":", "\n", "        ", "\"\"\"Build a new model instance.\"\"\"", "\n", "cls", ".", "fixed_scheduled_sampling_rate", "=", "args", ".", "fixed_scheduled_sampling_rate", "\n", "cls", ".", "trainstep_schduled_sampling_strategy", "=", "args", ".", "trainstep_schduled_sampling_strategy", "\n", "cls", ".", "decodingstep_schduled_sampling_strategy", "=", "args", ".", "decodingstep_schduled_sampling_strategy", "\n", "cls", ".", "exp_radix", "=", "args", ".", "exp_radix", "\n", "cls", ".", "decodingstep_exp_radix", "=", "args", ".", "decodingstep_exp_radix", "\n", "cls", ".", "decodingstep_sigmoid_k", "=", "args", ".", "decodingstep_sigmoid_k", "\n", "#cls.sampling_golden_prob = 1.0 # default", "\n", "# make sure all arguments are present in older models", "\n", "base_architecture", "(", "args", ")", "\n", "\n", "if", "not", "hasattr", "(", "args", ",", "'max_source_positions'", ")", ":", "\n", "            ", "args", ".", "max_source_positions", "=", "DEFAULT_MAX_SOURCE_POSITIONS", "\n", "", "if", "not", "hasattr", "(", "args", ",", "'max_target_positions'", ")", ":", "\n", "            ", "args", ".", "max_target_positions", "=", "DEFAULT_MAX_TARGET_POSITIONS", "\n", "\n", "", "src_dict", ",", "tgt_dict", "=", "task", ".", "source_dictionary", ",", "task", ".", "target_dictionary", "\n", "\n", "def", "build_embedding", "(", "dictionary", ",", "embed_dim", ")", ":", "\n", "            ", "num_embeddings", "=", "len", "(", "dictionary", ")", "\n", "padding_idx", "=", "dictionary", ".", "pad", "(", ")", "\n", "emb", "=", "Embedding", "(", "num_embeddings", ",", "embed_dim", ",", "padding_idx", ")", "\n", "return", "emb", "\n", "\n", "", "if", "args", ".", "share_all_embeddings", ":", "\n", "            ", "if", "src_dict", "!=", "tgt_dict", ":", "\n", "                ", "raise", "ValueError", "(", "'--share-all-embeddings requires a joined dictionary'", ")", "\n", "", "if", "args", ".", "encoder_embed_dim", "!=", "args", ".", "decoder_embed_dim", ":", "\n", "                ", "raise", "ValueError", "(", "\n", "'--share-all-embeddings requires --encoder-embed-dim to match --decoder-embed-dim'", ")", "\n", "", "encoder_embed_tokens", "=", "build_embedding", "(", "\n", "src_dict", ",", "args", ".", "encoder_embed_dim", "\n", ")", "\n", "cls", ".", "decoder_embed_tokens", "=", "encoder_embed_tokens", "\n", "args", ".", "share_decoder_input_output_embed", "=", "True", "\n", "", "else", ":", "\n", "            ", "encoder_embed_tokens", "=", "build_embedding", "(", "\n", "src_dict", ",", "args", ".", "encoder_embed_dim", "\n", ")", "\n", "cls", ".", "decoder_embed_tokens", "=", "build_embedding", "(", "\n", "tgt_dict", ",", "args", ".", "decoder_embed_dim", "\n", ")", "\n", "\n", "", "encoder", "=", "TransformerEncoder", "(", "args", ",", "src_dict", ",", "encoder_embed_tokens", ")", "\n", "decoder", "=", "NgramTransformerDecoder", "(", "args", ",", "tgt_dict", ",", "cls", ".", "decoder_embed_tokens", ")", "\n", "model", "=", "NgramTransformerProphetModel", "(", "encoder", ",", "decoder", ")", "\n", "\n", "if", "args", ".", "load_from_pretrained_model", "is", "not", "None", ":", "\n", "            ", "states", "=", "torch", ".", "load", "(", "args", ".", "load_from_pretrained_model", ",", "map_location", "=", "'cpu'", ")", "\n", "if", "'model'", "in", "states", "and", "'args'", "in", "states", ":", "\n", "                ", "states", "=", "states", "[", "'model'", "]", "\n", "", "if", "args", ".", "load_sep", ":", "\n", "                ", "encoder_token_weight", "=", "states", "[", "'encoder.embed_tokens.weight'", "]", "\n", "decoder_token_weight", "=", "states", "[", "'decoder.embed_tokens.weight'", "]", "\n", "encoder_token_weight", "[", "2", "]", "=", "encoder_token_weight", "[", "102", "]", "\n", "decoder_token_weight", "[", "2", "]", "=", "decoder_token_weight", "[", "102", "]", "\n", "states", "[", "'encoder.embed_tokens.weight'", "]", "=", "encoder_token_weight", "\n", "states", "[", "'decoder.embed_tokens.weight'", "]", "=", "decoder_token_weight", "\n", "\n", "", "loaded_dict_size", "=", "states", "[", "'encoder.embed_tokens.weight'", "]", ".", "size", "(", "0", ")", "\n", "num_langids_to_add", "=", "len", "(", "encoder", ".", "dictionary", ")", "-", "loaded_dict_size", "\n", "embed_dim", "=", "states", "[", "'encoder.embed_tokens.weight'", "]", ".", "size", "(", "1", ")", "\n", "\n", "if", "num_langids_to_add", ">", "0", ":", "\n", "                ", "new_lang_embed_to_add", "=", "torch", ".", "zeros", "(", "num_langids_to_add", ",", "embed_dim", ")", "\n", "nn", ".", "init", ".", "normal_", "(", "\n", "new_lang_embed_to_add", ",", "\n", "mean", "=", "0", ",", "\n", "std", "=", "embed_dim", "**", "-", "0.5", "\n", ")", "\n", "new_lang_embed_to_add", "=", "new_lang_embed_to_add", ".", "to", "(", "\n", "dtype", "=", "states", "[", "'encoder.embed_tokens.weight'", "]", ".", "dtype", ",", "\n", ")", "\n", "\n", "states", "[", "'encoder.embed_tokens.weight'", "]", "=", "torch", ".", "cat", "(", "[", "\n", "states", "[", "'encoder.embed_tokens.weight'", "]", ",", "\n", "new_lang_embed_to_add", "]", "\n", ")", "\n", "states", "[", "'decoder.embed_tokens.weight'", "]", "=", "torch", ".", "cat", "(", "[", "\n", "states", "[", "'decoder.embed_tokens.weight'", "]", ",", "\n", "new_lang_embed_to_add", "]", "\n", ")", "\n", "\n", "", "for", "position_name", ",", "target_position_length", "in", "[", "(", "'encoder.embed_positions.weight'", ",", "model", ".", "encoder", ".", "embed_positions", ".", "weight", ".", "size", "(", "0", ")", ")", ",", "(", "'decoder.embed_positions.weight'", ",", "model", ".", "decoder", ".", "embed_positions", ".", "weight", ".", "size", "(", "0", ")", ")", "]", ":", "\n", "                ", "if", "states", "[", "position_name", "]", ".", "size", "(", "0", ")", "<", "target_position_length", ":", "\n", "                    ", "_index", "=", "torch", ".", "arange", "(", "states", "[", "position_name", "]", ".", "size", "(", "1", ")", ")", "\n", "expend_position_states", "=", "states", "[", "position_name", "]", ".", "clone", "(", ")", "\n", "while", "states", "[", "position_name", "]", ".", "size", "(", "0", ")", "<", "target_position_length", ":", "\n", "                        ", "_index", "=", "torch", ".", "cat", "(", "(", "_index", "[", "1", ":", "]", ",", "_index", "[", ":", "1", "]", ")", ",", "dim", "=", "0", ")", "\n", "states", "[", "position_name", "]", "=", "torch", ".", "cat", "(", "[", "states", "[", "position_name", "]", ",", "expend_position_states", "[", ":", ",", "_index", "]", "]", ",", "dim", "=", "0", ")", "\n", "", "", "if", "states", "[", "position_name", "]", ".", "size", "(", "0", ")", ">", "target_position_length", ":", "\n", "                    ", "states", "[", "position_name", "]", "=", "states", "[", "position_name", "]", "[", ":", "target_position_length", "]", "\n", "", "", "model", ".", "load_state_dict", "(", "states", ")", "\n", "args", ".", "load_from_pretrained_model", "=", "None", "# Clear this param", "\n", "\n", "", "return", "NgramTransformerProphetModel", "(", "encoder", ",", "decoder", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet_decoding_step_ss_mle_absolute.ngram_s2s_model.NgramTransformerProphetModel.max_positions": [[214, 216], ["ngram_s2s_model.NgramTransformerProphetModel.encoder.max_positions", "ngram_s2s_model.NgramTransformerProphetModel.decoder.max_positions"], "methods", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_s2s_model.NgramTransformerDecoder.max_positions", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_s2s_model.NgramTransformerDecoder.max_positions"], ["", "def", "max_positions", "(", "self", ")", ":", "\n", "        ", "return", "(", "self", ".", "encoder", ".", "max_positions", "(", ")", ",", "self", ".", "decoder", ".", "max_positions", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet_decoding_step_ss_mle_absolute.ngram_s2s_model.NgramTransformerProphetModel.get_current_gampling_threshod": [[217, 238], ["ngram_s2s_model.NgramTransformerProphetModel.decoder.max_positions", "torch.range", "torch.range", "torch.range", "torch.range", "torch.range", "torch.range", "torch.range", "torch.range", "torch.range", "threshold_table.tril.tril.unsqueeze_().repeat", "threshold_table.tril.tril.tril", "threshold_table[].view", "threshold_table.tril.tril.unsqueeze_", "ValuraiseeError", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp"], "methods", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_s2s_model.NgramTransformerDecoder.max_positions"], ["", "def", "get_current_gampling_threshod", "(", "self", ",", "input_size", ",", "tgt_lengths", ")", ":", "\n", "        ", "max_len", "=", "self", ".", "decoder", ".", "max_positions", "(", ")", "\n", "t", "=", "torch", ".", "range", "(", "0", ",", "max_len", "-", "1", ")", "\n", "if", "self", ".", "decodingstep_schduled_sampling_strategy", "==", "\"linear\"", ":", "\n", "            ", "threshold_table", "=", "1", "-", "t", "/", "max_len", "\n", "", "elif", "self", ".", "decodingstep_schduled_sampling_strategy", "==", "\"exp\"", ":", "\n", "            ", "threshold_table", "=", "self", ".", "decodingstep_exp_radix", "**", "t", "\n", "", "elif", "self", ".", "decodingstep_schduled_sampling_strategy", "==", "\"sigmoid\"", ":", "\n", "            ", "threshold_table", "=", "self", ".", "decodingstep_sigmoid_k", "/", "(", "self", ".", "decodingstep_sigmoid_k", "+", "torch", ".", "exp", "(", "t", "/", "self", ".", "decodingstep_sigmoid_k", ")", ")", "\n", "", "else", ":", "\n", "             ", "ValuraiseeError", "(", "\"Unknown decodingstep_schduled_sampling_strategy  %s\"", "%", "decodingstep_schduled_sampling_strategy", ")", "\n", "# list to matrix", "\n", "", "threshold_table", "=", "threshold_table", ".", "unsqueeze_", "(", "0", ")", ".", "repeat", "(", "max_len", ",", "1", ")", "\n", "threshold_table", "=", "threshold_table", ".", "tril", "(", ")", "\n", "batch_size", ",", "seq_len", "=", "input_size", "\n", "# [b, max_len]", "\n", "tgt_lengths", "-=", "1", "# for index", "\n", "current_threshold", "=", "threshold_table", "[", "tgt_lengths", "]", ".", "view", "(", "batch_size", ",", "max_len", ")", "\n", "# [b, s]", "\n", "current_threshold", "=", "current_threshold", "[", ":", ",", ":", "seq_len", "]", "\n", "return", "current_threshold", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet_decoding_step_ss_mle_absolute.ngram_s2s_model.NgramTransformerProphetModel.forward": [[239, 279], ["ngram_s2s_model.NgramTransformerProphetModel.encoder", "ngram_s2s_model.NgramTransformerProphetModel.decoder", "ngram_s2s_model.NgramTransformerProphetModel.decoder", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "prev_output_tokens.size", "ngram_s2s_model.NgramTransformerProphetModel.decoder", "[].detach", "torch.softmax", "torch.softmax", "torch.softmax", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "prev_output_tokens[].cuda", "ngram_s2s_model.NgramTransformerProphetModel.decoder_embed_tokens", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "ngram_s2s_model.NgramTransformerProphetModel.decoder_embed_tokens", "ngram_s2s_model.NgramTransformerProphetModel.get_current_gampling_threshod", "cond.unsqueeze_().repeat().cuda.unsqueeze_().repeat().cuda.unsqueeze_().repeat().cuda", "torch.where", "torch.where", "torch.where", "torch.where", "torch.where", "torch.where", "torch.where", "torch.where", "torch.where", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "prev_output_tokens.size", "ngram_s2s_model.NgramTransformerProphetModel.size", "torch.matmul.size", "torch.matmul.size", "torch.matmul.size", "prev_output_tokens.size", "cond.unsqueeze_().repeat().cuda.unsqueeze_().repeat().cuda.unsqueeze_().repeat", "cond.unsqueeze_().repeat().cuda.unsqueeze_().repeat().cuda.unsqueeze_", "ngram_s2s_model.NgramTransformerProphetModel.size", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones"], "methods", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.distribute.size", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet_decoding_step_ss_mle_absolute.ngram_s2s_model.NgramTransformerProphetModel.get_current_gampling_threshod", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.distribute.size", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.distribute.size", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.distribute.size", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.distribute.size", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.distribute.size", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.distribute.size", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.distribute.size"], ["", "def", "forward", "(", "self", ",", "src_tokens", "=", "None", ",", "src_lengths", "=", "None", ",", "prev_output_tokens", "=", "None", ",", "tgt_lengths", "=", "None", ",", "**", "kwargs", ")", ":", "\n", "\n", "        ", "encoder_out", "=", "self", ".", "encoder", "(", "src_tokens", ",", "src_lengths", "=", "src_lengths", ",", "**", "kwargs", ")", "\n", "# get output ", "\n", "decoder_out", "=", "self", ".", "decoder", "(", "prev_output_tokens", ",", "encoder_out", "=", "encoder_out", ",", "src_lengths", "=", "src_lengths", ")", "\n", "new_decoder_input_emb", "=", "None", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "if", "self", ".", "training", ":", "\n", "                ", "batch_size", ",", "_", "=", "prev_output_tokens", ".", "size", "(", ")", "\n", "# get output ", "\n", "decoder_out", "=", "self", ".", "decoder", "(", "prev_output_tokens", ",", "encoder_out", "=", "encoder_out", ",", "src_lengths", "=", "src_lengths", ")", "\n", "# [B, T, C], only use 1ngram for model prediciton", "\n", "ngrams_decoder_out", "=", "decoder_out", "[", "0", "]", "[", "0", "]", ".", "detach", "(", ")", "\n", "# apply softmax # list of [B, T, V] ", "\n", "ngrams_decoder_prob", "=", "F", ".", "softmax", "(", "ngrams_decoder_out", ",", "dim", "=", "-", "1", ")", "\n", "# # [B, T, V] * [V, C] = [B, T ,C]", "\n", "soft_decoder_out_emb", "=", "torch", ".", "matmul", "(", "ngrams_decoder_prob", ",", "self", ".", "decoder_embed_tokens", ".", "weight", ")", "\n", "# appending bos embd [1, C]", "\n", "bos_idx", "=", "prev_output_tokens", "[", "0", ",", "0", "]", ".", "cuda", "(", ")", "\n", "# tile to [B, 1]", "\n", "bos_tokens", "=", "(", "bos_idx", "*", "torch", ".", "ones", "(", "(", "batch_size", ",", "1", ")", ")", ")", ".", "to", "(", "torch", ".", "long", ")", ".", "cuda", "(", ")", "\n", "# bos embedding , [B, 1, C]", "\n", "bos_embeddings", "=", "self", ".", "decoder_embed_tokens", "(", "bos_tokens", ")", "\n", "# [[B, T, C]; [B, 1, C]][:, :-1] => [B, T, C]", "\n", "soft_decoder_out_emb", "=", "torch", ".", "cat", "(", "[", "soft_decoder_out_emb", ",", "bos_embeddings", "]", ",", "dim", "=", "1", ")", "[", ":", ",", ":", "-", "1", ",", ":", "]", "\n", "# random select decoder input between goldens and predictions", "\n", "random_select_seed", "=", "torch", ".", "rand", "(", "prev_output_tokens", ".", "size", "(", ")", ")", "# shape sampe as prev_output_tokens", "\n", "prev_output_tokens_emb", "=", "self", ".", "decoder_embed_tokens", "(", "prev_output_tokens", ")", "\n", "assert", "prev_output_tokens_emb", ".", "size", "(", ")", "==", "soft_decoder_out_emb", ".", "size", "(", ")", "\n", "thresholds", "=", "self", ".", "get_current_gampling_threshod", "(", "prev_output_tokens", ".", "size", "(", ")", ",", "tgt_lengths", ")", "\n", "cond", "=", "random_select_seed", "<", "thresholds", "\n", "cond", "=", "cond", ".", "unsqueeze_", "(", "-", "1", ")", ".", "repeat", "(", "1", ",", "1", ",", "prev_output_tokens_emb", ".", "size", "(", ")", "[", "-", "1", "]", ")", ".", "cuda", "(", ")", "\n", "new_decoder_input_emb", "=", "torch", ".", "where", "(", "cond", ",", "prev_output_tokens_emb", ",", "soft_decoder_out_emb", ")", "\n", "\n", "# ATT:", "\n", "", "", "decoder_out_with_sampling", "=", "self", ".", "decoder", "(", "prev_output_tokens", ",", "encoder_out", "=", "encoder_out", ",", "new_decoder_input_emb", "=", "new_decoder_input_emb", ")", "\n", "# list of [B, T, V] * (ngram) => [ngram, B, T, V]", "\n", "decoder_out", "=", "torch", ".", "stack", "(", "decoder_out", "[", "0", "]", ",", "dim", "=", "0", ")", "\n", "decoder_out_with_sampling", "=", "torch", ".", "stack", "(", "decoder_out_with_sampling", "[", "0", "]", ",", "dim", "=", "0", ")", "\n", "return", "torch", ".", "stack", "(", "[", "decoder_out", ",", "decoder_out_with_sampling", "]", ",", "dim", "=", "0", ")", "# [2, ngram, B, T, V]", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet_decoding_step_ss_mle_absolute.ngram_s2s_model.NgramTransformerProphetModel.set_epoch": [[281, 283], ["None"], "methods", ["None"], ["", "def", "set_epoch", "(", "self", ",", "epoch", ",", "max_epoch", ")", ":", "\n", "        ", "self", ".", "epoch", "=", "epoch", "\n", "#self.decay_prob(epoch, max_epoch, self.trainstep_schduled_sampling_strategy, self.exp_radix)", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet_decoding_step_ss_mle_absolute.ngram_s2s_model.NgramTransformerProphetModel.decay_prob": [[286, 293], ["ValuraiseeError"], "methods", ["None"], ["", "def", "decay_prob", "(", "self", ",", "epoch_index", ",", "max_epoch", ",", "trainstep_schduled_sampling_strategy", "=", "None", ",", "exp_radix", "=", "0.8", ",", "sigmoid_k", "=", "0", ")", ":", "\n", "        ", "if", "trainstep_schduled_sampling_strategy", "==", "\"linear\"", ":", "\n", "            ", "self", ".", "sampling_golden_prob", "=", "-", "(", "epoch_index", "-", "1", ")", "/", "max_epoch", "+", "1.0", "# [0.1, 1]", "\n", "", "elif", "trainstep_schduled_sampling_strategy", "==", "\"exp\"", ":", "\n", "            ", "self", ".", "sampling_golden_prob", "=", "exp_radix", "**", "epoch_index", "# [0.1, 0.8]", "\n", "", "else", ":", "\n", "             ", "ValuraiseeError", "(", "\"Unknown trainstep_schduled_sampling_strategy  %s\"", "%", "trainstep_schduled_sampling_strategy", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet_decoding_step_ss_mle_absolute.ngram_s2s_model.TransformerEncoderLayer.__init__": [[301, 338], ["torch.Module.__init__", "fairseq.utils.get_activation_fn", "fairseq.modules.MultiheadAttention", "fairseq.modules.LayerNorm", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "fairseq.modules.LayerNorm"], "methods", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.transformer_lrp.TransformerLRP.__init__", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_s2s_model.Linear", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_s2s_model.Linear", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_s2s_model.Linear", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_s2s_model.Linear", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_s2s_model.Linear", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_s2s_model.Linear"], ["def", "__init__", "(", "\n", "self", ",", "\n", "embedding_dim", ":", "float", "=", "768", ",", "\n", "ffn_embedding_dim", ":", "float", "=", "3072", ",", "\n", "num_attention_heads", ":", "float", "=", "8", ",", "\n", "dropout", ":", "float", "=", "0.1", ",", "\n", "attention_dropout", ":", "float", "=", "0.1", ",", "\n", "activation_dropout", ":", "float", "=", "0.1", ",", "\n", "activation_fn", ":", "str", "=", "'relu'", ",", "\n", "add_bias_kv", ":", "bool", "=", "False", ",", "\n", "add_zero_attn", ":", "bool", "=", "False", ",", "\n", "export", ":", "bool", "=", "False", ",", "\n", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "# Initialize parameters", "\n", "self", ".", "embedding_dim", "=", "embedding_dim", "\n", "self", ".", "dropout", "=", "dropout", "\n", "self", ".", "activation_dropout", "=", "activation_dropout", "\n", "\n", "# Initialize blocks", "\n", "self", ".", "activation_fn", "=", "utils", ".", "get_activation_fn", "(", "activation_fn", ")", "\n", "self", ".", "self_attn", "=", "MultiheadAttention", "(", "\n", "self", ".", "embedding_dim", ",", "\n", "num_attention_heads", ",", "\n", "dropout", "=", "attention_dropout", ",", "\n", "add_bias_kv", "=", "add_bias_kv", ",", "\n", "add_zero_attn", "=", "add_zero_attn", ",", "\n", "self_attention", "=", "True", ",", "\n", ")", "\n", "\n", "# layer norm associated with the self attention layer", "\n", "self", ".", "self_attn_layer_norm", "=", "LayerNorm", "(", "self", ".", "embedding_dim", ",", "export", "=", "export", ")", "\n", "self", ".", "fc1", "=", "nn", ".", "Linear", "(", "self", ".", "embedding_dim", ",", "ffn_embedding_dim", ")", "\n", "self", ".", "fc2", "=", "nn", ".", "Linear", "(", "ffn_embedding_dim", ",", "self", ".", "embedding_dim", ")", "\n", "\n", "# layer norm associated with the position wise feed-forward NN", "\n", "self", ".", "final_layer_norm", "=", "LayerNorm", "(", "self", ".", "embedding_dim", ",", "export", "=", "export", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet_decoding_step_ss_mle_absolute.ngram_s2s_model.TransformerEncoderLayer.forward": [[339, 370], ["ngram_s2s_model.TransformerEncoderLayer.self_attn", "torch.dropout", "torch.dropout", "torch.dropout", "ngram_s2s_model.TransformerEncoderLayer.self_attn_layer_norm", "ngram_s2s_model.TransformerEncoderLayer.activation_fn", "torch.dropout", "torch.dropout", "torch.dropout", "ngram_s2s_model.TransformerEncoderLayer.fc2", "torch.dropout", "torch.dropout", "torch.dropout", "ngram_s2s_model.TransformerEncoderLayer.final_layer_norm", "ngram_s2s_model.TransformerEncoderLayer.fc1"], "methods", ["None"], ["", "def", "forward", "(", "\n", "self", ",", "\n", "x", ":", "torch", ".", "Tensor", ",", "\n", "self_attn_mask", ":", "torch", ".", "Tensor", "=", "None", ",", "\n", "self_attn_padding_mask", ":", "torch", ".", "Tensor", "=", "None", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        LayerNorm is applied either before or after the self-attention/ffn\n        modules similar to the original Transformer imlementation.\n        \"\"\"", "\n", "residual", "=", "x", "\n", "x", ",", "attn", "=", "self", ".", "self_attn", "(", "\n", "query", "=", "x", ",", "\n", "key", "=", "x", ",", "\n", "value", "=", "x", ",", "\n", "key_padding_mask", "=", "self_attn_padding_mask", ",", "\n", "need_weights", "=", "False", ",", "\n", "attn_mask", "=", "self_attn_mask", ",", "\n", ")", "\n", "x", "=", "F", ".", "dropout", "(", "x", ",", "p", "=", "self", ".", "dropout", ",", "training", "=", "self", ".", "training", ")", "\n", "x", "=", "residual", "+", "x", "\n", "x", "=", "self", ".", "self_attn_layer_norm", "(", "x", ")", "\n", "\n", "residual", "=", "x", "\n", "x", "=", "self", ".", "activation_fn", "(", "self", ".", "fc1", "(", "x", ")", ")", "\n", "x", "=", "F", ".", "dropout", "(", "x", ",", "p", "=", "self", ".", "activation_dropout", ",", "training", "=", "self", ".", "training", ")", "\n", "x", "=", "self", ".", "fc2", "(", "x", ")", "\n", "x", "=", "F", ".", "dropout", "(", "x", ",", "p", "=", "self", ".", "dropout", ",", "training", "=", "self", ".", "training", ")", "\n", "x", "=", "residual", "+", "x", "\n", "x", "=", "self", ".", "final_layer_norm", "(", "x", ")", "\n", "return", "x", ",", "attn", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet_decoding_step_ss_mle_absolute.ngram_s2s_model.NgramTransformerDecoderLayer.__init__": [[373, 426], ["torch.Module.__init__", "fairseq.utils.get_activation_fn", "ngram_multihead_attention.NgramMultiheadAttention", "fairseq.modules.LayerNorm", "fairseq.modules.MultiheadAttention", "fairseq.modules.LayerNorm", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "fairseq.modules.LayerNorm"], "methods", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.transformer_lrp.TransformerLRP.__init__", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_s2s_model.Linear", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_s2s_model.Linear", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_s2s_model.Linear", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_s2s_model.Linear", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_s2s_model.Linear", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_s2s_model.Linear"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "ngram", "=", "2", ",", "\n", "embedding_dim", ":", "float", "=", "768", ",", "\n", "ffn_embedding_dim", ":", "float", "=", "3072", ",", "\n", "num_attention_heads", ":", "float", "=", "8", ",", "\n", "dropout", ":", "float", "=", "0.1", ",", "\n", "attention_dropout", ":", "float", "=", "0.1", ",", "\n", "activation_dropout", ":", "float", "=", "0.1", ",", "\n", "activation_fn", ":", "str", "=", "'relu'", ",", "\n", "add_bias_kv", ":", "bool", "=", "False", ",", "\n", "add_zero_attn", ":", "bool", "=", "False", ",", "\n", "export", ":", "bool", "=", "False", ",", "\n", "\n", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "embedding_dim", "=", "embedding_dim", "\n", "self", ".", "dropout", "=", "dropout", "\n", "self", ".", "activation_dropout", "=", "activation_dropout", "\n", "\n", "# Initialize blocks", "\n", "self", ".", "activation_fn", "=", "utils", ".", "get_activation_fn", "(", "activation_fn", ")", "\n", "self", ".", "ngram_self_attn", "=", "NgramMultiheadAttention", "(", "\n", "self", ".", "embedding_dim", ",", "\n", "num_attention_heads", ",", "\n", "dropout", "=", "attention_dropout", ",", "\n", "add_bias_kv", "=", "add_bias_kv", ",", "\n", "add_zero_attn", "=", "add_zero_attn", ",", "\n", "self_attention", "=", "True", ",", "\n", "ngram", "=", "ngram", "\n", ")", "\n", "self", ".", "ngram", "=", "ngram", "\n", "\n", "# layer norm associated with the self attention layer", "\n", "self", ".", "self_attn_layer_norm", "=", "LayerNorm", "(", "self", ".", "embedding_dim", ",", "export", "=", "export", ")", "\n", "\n", "self", ".", "encoder_attn", "=", "MultiheadAttention", "(", "\n", "self", ".", "embedding_dim", ",", "\n", "num_attention_heads", ",", "\n", "kdim", "=", "embedding_dim", ",", "\n", "vdim", "=", "embedding_dim", ",", "\n", "dropout", "=", "attention_dropout", ",", "\n", "encoder_decoder_attention", "=", "True", ",", "\n", ")", "\n", "self", ".", "encoder_attn_layer_norm", "=", "LayerNorm", "(", "self", ".", "embedding_dim", ",", "export", "=", "export", ")", "\n", "\n", "self", ".", "fc1", "=", "nn", ".", "Linear", "(", "self", ".", "embedding_dim", ",", "ffn_embedding_dim", ")", "\n", "self", ".", "fc2", "=", "nn", ".", "Linear", "(", "ffn_embedding_dim", ",", "self", ".", "embedding_dim", ")", "\n", "\n", "# layer norm associated with the position wise feed-forward NN", "\n", "self", ".", "final_layer_norm", "=", "LayerNorm", "(", "self", ".", "embedding_dim", ",", "export", "=", "export", ")", "\n", "self", ".", "need_attn", "=", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet_decoding_step_ss_mle_absolute.ngram_s2s_model.NgramTransformerDecoderLayer.forward": [[427, 497], ["ngram_s2s_model.NgramTransformerDecoderLayer.ngram_self_attn", "torch.dropout", "torch.dropout", "torch.dropout", "ngram_s2s_model.NgramTransformerDecoderLayer.self_attn_layer_norm", "ngram_s2s_model.NgramTransformerDecoderLayer.encoder_attn", "torch.dropout", "torch.dropout", "torch.dropout", "ngram_s2s_model.NgramTransformerDecoderLayer.encoder_attn_layer_norm", "ngram_s2s_model.NgramTransformerDecoderLayer.activation_fn", "torch.dropout", "torch.dropout", "torch.dropout", "ngram_s2s_model.NgramTransformerDecoderLayer.fc2", "torch.dropout", "torch.dropout", "torch.dropout", "ngram_s2s_model.NgramTransformerDecoderLayer.final_layer_norm", "ngram_s2s_model.NgramTransformerDecoderLayer.self_attn._set_input_buffer", "ngram_s2s_model.NgramTransformerDecoderLayer.encoder_attn._set_input_buffer", "ngram_s2s_model.NgramTransformerDecoderLayer.fc1"], "methods", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_multihead_attention.NgramMultiheadAttention._set_input_buffer", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_multihead_attention.NgramMultiheadAttention._set_input_buffer"], ["", "def", "forward", "(", "\n", "self", ",", "\n", "x", ",", "\n", "encoder_out", "=", "None", ",", "\n", "encoder_mask", "=", "None", ",", "\n", "incremental_state", "=", "None", ",", "\n", "prev_self_attn_state", "=", "None", ",", "\n", "prev_attn_state", "=", "None", ",", "\n", "self_attn_mask", "=", "None", ",", "\n", "ngram_mask_matrix", "=", "None", ",", "\n", "i_buckets_main_stream", "=", "None", ",", "\n", "i_bucket_relative_stream", "=", "None", ",", "\n", "real_positions", "=", "None", "\n", ")", ":", "\n", "# one main stream and ngram predicting streams", "\n", "        ", "residual", "=", "x", "\n", "\n", "if", "prev_self_attn_state", "is", "not", "None", ":", "\n", "            ", "if", "incremental_state", "is", "None", ":", "\n", "                ", "incremental_state", "=", "{", "}", "\n", "", "prev_key", ",", "prev_value", "=", "prev_self_attn_state", "\n", "saved_state", "=", "{", "\"prev_key\"", ":", "prev_key", ",", "\"prev_value\"", ":", "prev_value", "}", "\n", "self", ".", "self_attn", ".", "_set_input_buffer", "(", "incremental_state", ",", "saved_state", ")", "\n", "\n", "", "x", ",", "attn", "=", "self", ".", "ngram_self_attn", "(", "\n", "query", "=", "x", ",", "\n", "key", "=", "x", ",", "\n", "value", "=", "x", ",", "\n", "incremental_state", "=", "incremental_state", ",", "\n", "need_weights", "=", "False", ",", "\n", "self_attn_mask", "=", "self_attn_mask", ",", "\n", "ngram_mask_matrix", "=", "ngram_mask_matrix", ",", "\n", "i_buckets_main_stream", "=", "i_buckets_main_stream", ",", "\n", "i_bucket_relative_stream", "=", "i_bucket_relative_stream", ",", "\n", "real_positions", "=", "real_positions", "\n", ")", "\n", "# [(1+ngram)*T, B, C]", "\n", "\n", "x", "=", "F", ".", "dropout", "(", "x", ",", "p", "=", "self", ".", "dropout", ",", "training", "=", "self", ".", "training", ")", "\n", "x", "=", "residual", "+", "x", "\n", "x", "=", "self", ".", "self_attn_layer_norm", "(", "x", ")", "\n", "\n", "residual", "=", "x", "\n", "if", "prev_attn_state", "is", "not", "None", ":", "\n", "            ", "if", "incremental_state", "is", "None", ":", "\n", "                ", "incremental_state", "=", "{", "}", "\n", "", "prev_key", ",", "prev_value", "=", "prev_attn_state", "\n", "saved_state", "=", "{", "\"prev_key\"", ":", "prev_key", ",", "\"prev_value\"", ":", "prev_value", "}", "\n", "self", ".", "encoder_attn", ".", "_set_input_buffer", "(", "incremental_state", ",", "saved_state", ")", "\n", "", "x", ",", "attn", "=", "self", ".", "encoder_attn", "(", "\n", "query", "=", "x", ",", "\n", "key", "=", "encoder_out", ",", "\n", "value", "=", "encoder_out", ",", "\n", "key_padding_mask", "=", "encoder_mask", ",", "\n", "incremental_state", "=", "incremental_state", ",", "\n", "static_kv", "=", "True", ",", "\n", "need_weights", "=", "(", "not", "self", ".", "training", "and", "self", ".", "need_attn", ")", ",", "\n", ")", "\n", "x", "=", "F", ".", "dropout", "(", "x", ",", "p", "=", "self", ".", "dropout", ",", "training", "=", "self", ".", "training", ")", "\n", "x", "=", "residual", "+", "x", "\n", "x", "=", "self", ".", "encoder_attn_layer_norm", "(", "x", ")", "\n", "\n", "residual", "=", "x", "\n", "x", "=", "self", ".", "activation_fn", "(", "self", ".", "fc1", "(", "x", ")", ")", "\n", "x", "=", "F", ".", "dropout", "(", "x", ",", "p", "=", "self", ".", "activation_dropout", ",", "training", "=", "self", ".", "training", ")", "\n", "x", "=", "self", ".", "fc2", "(", "x", ")", "\n", "x", "=", "F", ".", "dropout", "(", "x", ",", "p", "=", "self", ".", "dropout", ",", "training", "=", "self", ".", "training", ")", "\n", "x", "=", "residual", "+", "x", "\n", "x", "=", "self", ".", "final_layer_norm", "(", "x", ")", "\n", "return", "x", ",", "attn", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet_decoding_step_ss_mle_absolute.ngram_s2s_model.NgramTransformerDecoderLayer.make_generation_fast_": [[498, 500], ["None"], "methods", ["None"], ["", "def", "make_generation_fast_", "(", "self", ",", "need_attn", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "        ", "self", ".", "need_attn", "=", "need_attn", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet_decoding_step_ss_mle_absolute.ngram_s2s_model.TransformerEncoder.__init__": [[513, 547], ["fairseq.models.FairseqEncoder.__init__", "ngram_s2s_model.TransformerEncoder.register_buffer", "learned_positional_embedding.LearnedPositionalEmbedding", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "ngram_s2s_model.TransformerEncoder.layers.extend", "fairseq.modules.LayerNorm", "ngram_s2s_model.TransformerEncoder.apply", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "ngram_s2s_model.TransformerEncoderLayer", "range"], "methods", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.transformer_lrp.TransformerLRP.__init__"], ["def", "__init__", "(", "self", ",", "args", ",", "dictionary", ",", "embed_tokens", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "dictionary", ")", "\n", "self", ".", "register_buffer", "(", "'version'", ",", "torch", ".", "Tensor", "(", "[", "3", "]", ")", ")", "\n", "\n", "self", ".", "dropout", "=", "args", ".", "dropout", "\n", "\n", "embed_dim", "=", "embed_tokens", ".", "embedding_dim", "\n", "self", ".", "padding_idx", "=", "embed_tokens", ".", "padding_idx", "\n", "self", ".", "max_source_positions", "=", "args", ".", "max_source_positions", "\n", "\n", "self", ".", "embed_tokens", "=", "embed_tokens", "\n", "self", ".", "embed_scale", "=", "None", "#math.sqrt(embed_dim)", "\n", "self", ".", "embed_positions", "=", "LearnedPositionalEmbedding", "(", "\n", "args", ".", "max_source_positions", "+", "1", "+", "self", ".", "padding_idx", ",", "embed_dim", ",", "self", ".", "padding_idx", ",", "\n", ")", "\n", "\n", "self", ".", "layers", "=", "nn", ".", "ModuleList", "(", "[", "]", ")", "\n", "\n", "self", ".", "layers", ".", "extend", "(", "[", "\n", "TransformerEncoderLayer", "(", "\n", "args", ".", "encoder_embed_dim", ",", "\n", "args", ".", "encoder_ffn_embed_dim", ",", "\n", "args", ".", "encoder_attention_heads", ",", "\n", "args", ".", "dropout", ",", "\n", "args", ".", "attention_dropout", ",", "\n", "args", ".", "activation_dropout", ",", "\n", "args", ".", "activation_fn", ",", "\n", ")", "\n", "for", "i", "in", "range", "(", "args", ".", "encoder_layers", ")", "\n", "]", ")", "\n", "\n", "self", ".", "emb_layer_norm", "=", "LayerNorm", "(", "embed_dim", ")", "\n", "\n", "self", ".", "apply", "(", "init_bert_params", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet_decoding_step_ss_mle_absolute.ngram_s2s_model.TransformerEncoder.forward": [[548, 594], ["src_tokens.eq", "ngram_s2s_model.TransformerEncoder.embed_tokens", "torch.dropout", "torch.dropout", "torch.dropout", "ngram_s2s_model.TransformerEncoder.transpose", "src_tokens.eq.any", "ngram_s2s_model.TransformerEncoder.embed_positions", "ngram_s2s_model.TransformerEncoder.emb_layer_norm", "layer", "src_tokens.eq.unsqueeze().type_as", "src_tokens.eq.unsqueeze"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "src_tokens", ",", "src_lengths", ",", "**", "unused", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            src_tokens (LongTensor): tokens in the source language of shape\n                `(batch, src_len)`\n            src_lengths (torch.LongTensor): lengths of each source sentence of\n                shape `(batch)`\n        Returns:\n            dict:\n                - **encoder_out** (Tensor): the last encoder layer's output of\n                  shape `(src_len, batch, embed_dim)`\n                - **encoder_padding_mask** (ByteTensor): the positions of\n                  padding elements of shape `(batch, src_len)`\n        \"\"\"", "\n", "# compute padding mask", "\n", "encoder_padding_mask", "=", "src_tokens", ".", "eq", "(", "self", ".", "padding_idx", ")", "\n", "if", "not", "encoder_padding_mask", ".", "any", "(", ")", ":", "\n", "            ", "encoder_padding_mask", "=", "None", "\n", "\n", "", "x", "=", "self", ".", "embed_tokens", "(", "src_tokens", ")", "\n", "# embed tokens and positions", "\n", "if", "self", ".", "embed_scale", "is", "not", "None", ":", "\n", "            ", "x", "*=", "self", ".", "embed_scale", "\n", "\n", "", "if", "self", ".", "embed_positions", "is", "not", "None", ":", "\n", "            ", "pos_emb", ",", "real_positions", "=", "self", ".", "embed_positions", "(", "src_tokens", ")", "\n", "x", "+=", "pos_emb", "\n", "\n", "", "if", "self", ".", "emb_layer_norm", ":", "\n", "            ", "x", "=", "self", ".", "emb_layer_norm", "(", "x", ")", "\n", "\n", "", "x", "=", "F", ".", "dropout", "(", "x", ",", "p", "=", "self", ".", "dropout", ",", "training", "=", "self", ".", "training", ")", "\n", "\n", "if", "encoder_padding_mask", "is", "not", "None", ":", "\n", "            ", "x", "*=", "1", "-", "encoder_padding_mask", ".", "unsqueeze", "(", "-", "1", ")", ".", "type_as", "(", "x", ")", "\n", "# B x T x C -> T x B x C", "\n", "", "x", "=", "x", ".", "transpose", "(", "0", ",", "1", ")", "\n", "\n", "# encoder layers", "\n", "for", "layer", "in", "self", ".", "layers", ":", "\n", "# x, _ = layer(x, self_attn_padding_mask=encoder_padding_mask, real_positions=real_positions)", "\n", "            ", "x", ",", "_", "=", "layer", "(", "x", ",", "self_attn_padding_mask", "=", "encoder_padding_mask", ",", ")", "\n", "\n", "", "return", "{", "\n", "'encoder_out'", ":", "x", ",", "# T x B x C", "\n", "'encoder_padding_mask'", ":", "encoder_padding_mask", ",", "# B x T", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet_decoding_step_ss_mle_absolute.ngram_s2s_model.TransformerEncoder.reorder_encoder_out": [[596, 612], ["encoder_out[].index_select", "encoder_out[].index_select"], "methods", ["None"], ["", "def", "reorder_encoder_out", "(", "self", ",", "encoder_out", ",", "new_order", ")", ":", "\n", "        ", "\"\"\"\n        Reorder encoder output according to *new_order*.\n        Args:\n            encoder_out: output from the ``forward()`` method\n            new_order (LongTensor): desired order\n        Returns:\n            *encoder_out* rearranged according to *new_order*\n        \"\"\"", "\n", "if", "encoder_out", "[", "'encoder_out'", "]", "is", "not", "None", ":", "\n", "            ", "encoder_out", "[", "'encoder_out'", "]", "=", "encoder_out", "[", "'encoder_out'", "]", ".", "index_select", "(", "1", ",", "new_order", ")", "\n", "", "if", "encoder_out", "[", "'encoder_padding_mask'", "]", "is", "not", "None", ":", "\n", "            ", "encoder_out", "[", "'encoder_padding_mask'", "]", "=", "encoder_out", "[", "'encoder_padding_mask'", "]", ".", "index_select", "(", "0", ",", "new_order", ")", "\n", "", "return", "encoder_out", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet_decoding_step_ss_mle_absolute.ngram_s2s_model.TransformerEncoder.max_positions": [[613, 618], ["min", "ngram_s2s_model.TransformerEncoder.embed_positions.max_positions"], "methods", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_s2s_model.NgramTransformerDecoder.max_positions"], ["", "def", "max_positions", "(", "self", ")", ":", "\n", "        ", "\"\"\"Maximum input length supported by the encoder.\"\"\"", "\n", "if", "self", ".", "embed_positions", "is", "None", ":", "\n", "            ", "return", "self", ".", "max_source_positions", "\n", "", "return", "min", "(", "self", ".", "max_source_positions", ",", "self", ".", "embed_positions", ".", "max_positions", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet_decoding_step_ss_mle_absolute.ngram_s2s_model.NgramTransformerDecoder.__init__": [[632, 680], ["fairseq.models.FairseqIncrementalDecoder.__init__", "ngram_s2s_model.NgramTransformerDecoder.register_buffer", "learned_positional_embedding.LearnedPositionalEmbedding", "ngram_s2s_model.Embedding", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "ngram_s2s_model.NgramTransformerDecoder.layers.extend", "fairseq.modules.LayerNorm", "ngram_s2s_model.NgramTransformerDecoder.apply", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.init.normal_", "torch.init.normal_", "torch.init.normal_", "ngram_s2s_model.NgramTransformerDecoderLayer", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "range", "len"], "methods", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.transformer_lrp.TransformerLRP.__init__", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_s2s_model.Embedding"], ["def", "__init__", "(", "self", ",", "args", ",", "dictionary", ",", "embed_tokens", ",", "no_encoder_attn", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "dictionary", ")", "\n", "self", ".", "register_buffer", "(", "'version'", ",", "torch", ".", "Tensor", "(", "[", "3", "]", ")", ")", "\n", "self", ".", "ngram", "=", "args", ".", "ngram", "\n", "self", ".", "num_buckets", "=", "args", ".", "num_buckets", "\n", "self", ".", "relative_max_distance", "=", "args", ".", "relative_max_distance", "\n", "\n", "self", ".", "dropout", "=", "args", ".", "dropout", "\n", "self", ".", "share_input_output_embed", "=", "args", ".", "share_decoder_input_output_embed", "\n", "\n", "input_embed_dim", "=", "embed_tokens", ".", "embedding_dim", "\n", "embed_dim", "=", "args", ".", "decoder_embed_dim", "\n", "\n", "self", ".", "padding_idx", "=", "embed_tokens", ".", "padding_idx", "\n", "self", ".", "max_target_positions", "=", "args", ".", "max_target_positions", "\n", "self", ".", "embed_dim", "=", "embed_dim", "\n", "self", ".", "embed_tokens", "=", "embed_tokens", "\n", "self", ".", "embed_scale", "=", "None", "#math.sqrt(embed_dim)  # todo: try with input_embed_dim", "\n", "\n", "self", ".", "embed_positions", "=", "LearnedPositionalEmbedding", "(", "\n", "args", ".", "max_target_positions", "+", "2", "+", "self", ".", "padding_idx", ",", "embed_dim", ",", "self", ".", "padding_idx", ",", "\n", ")", "\n", "\n", "self", ".", "ngram_input_embed", "=", "Embedding", "(", "self", ".", "ngram", ",", "input_embed_dim", ",", "None", ")", "\n", "\n", "self", ".", "layers", "=", "nn", ".", "ModuleList", "(", "[", "]", ")", "\n", "\n", "self", ".", "layers", ".", "extend", "(", "[", "\n", "NgramTransformerDecoderLayer", "(", "\n", "args", ".", "ngram", ",", "\n", "args", ".", "decoder_embed_dim", ",", "\n", "args", ".", "decoder_ffn_embed_dim", ",", "\n", "args", ".", "decoder_attention_heads", ",", "\n", "args", ".", "dropout", ",", "\n", "args", ".", "attention_dropout", ",", "\n", "args", ".", "activation_dropout", ",", "\n", "args", ".", "activation_fn", ",", "\n", "\n", ")", "\n", "for", "_", "in", "range", "(", "args", ".", "decoder_layers", ")", "\n", "]", ")", "\n", "\n", "if", "not", "self", ".", "share_input_output_embed", ":", "\n", "            ", "self", ".", "embed_out", "=", "nn", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "len", "(", "dictionary", ")", ",", "self", ".", "embed_dim", ")", ")", "\n", "nn", ".", "init", ".", "normal_", "(", "self", ".", "embed_out", ",", "mean", "=", "0", ",", "std", "=", "self", ".", "embed_dim", "**", "-", "0.5", ")", "\n", "\n", "", "self", ".", "emb_layer_norm", "=", "LayerNorm", "(", "embed_dim", ")", "\n", "self", ".", "apply", "(", "init_bert_params", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet_decoding_step_ss_mle_absolute.ngram_s2s_model.NgramTransformerDecoder.forward": [[681, 699], ["prev_output_tokens.size", "ngram_s2s_model.NgramTransformerDecoder.extract_features", "ngram_s2s_model.NgramTransformerDecoder.output_layer"], "methods", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.distribute.size", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_s2s_model.NgramTransformerDecoder.extract_features", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_s2s_model.NgramTransformerDecoder.output_layer"], ["", "def", "forward", "(", "self", ",", "\n", "prev_output_tokens", ",", "\n", "encoder_out", "=", "None", ",", "\n", "incremental_state", "=", "None", ",", "\n", "**", "unused", ")", ":", "\n", "# T  [B, T]", "\n", "        ", "T", "=", "prev_output_tokens", ".", "size", "(", "1", ")", "\n", "# more exactly, [B, T, C] * (1+ngram)", "\n", "x_list", ",", "extra", "=", "self", ".", "extract_features", "(", "prev_output_tokens", ",", "encoder_out", ",", "incremental_state", ",", "**", "unused", ")", "\n", "x_predicted", "=", "x_list", "[", "1", ":", "]", "\n", "x_predicted", "=", "[", "self", ".", "output_layer", "(", "x", ")", "for", "x", "in", "x_predicted", "]", "\n", "# list of [B, T, V] * (ngram)", "\n", "if", "incremental_state", "is", "not", "None", ":", "\n", "            ", "x_predicted", "=", "x_predicted", "[", "0", "]", "\n", "for", "k", "in", "extra", ":", "\n", "                ", "if", "extra", "[", "k", "]", "is", "not", "None", ":", "\n", "                    ", "extra", "[", "k", "]", "=", "extra", "[", "k", "]", "[", "0", "]", "\n", "", "", "", "return", "x_predicted", ",", "extra", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet_decoding_step_ss_mle_absolute.ngram_s2s_model.NgramTransformerDecoder._relative_positions_bucket": [[700, 719], ["torch.lt", "torch.lt", "torch.lt", "torch.lt", "torch.lt", "torch.lt", "torch.lt", "torch.lt", "torch.lt", "torch.min", "torch.min", "torch.min", "torch.min", "torch.min", "torch.min", "torch.min", "torch.min", "torch.min", "val_if_large.int.int.int", "torch.abs", "torch.abs", "torch.abs", "torch.abs", "torch.abs", "torch.abs", "torch.abs", "torch.abs", "torch.abs", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.where", "torch.where", "torch.where", "torch.where", "torch.where", "torch.where", "torch.where", "torch.where", "torch.where", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.ones_like", "torch.ones_like", "torch.ones_like", "torch.ones_like", "torch.ones_like", "torch.ones_like", "torch.ones_like", "torch.ones_like", "torch.ones_like", "torch.max.int", "torch.max.int", "torch.max.int", "torch.lt().int", "torch.lt().int", "torch.lt().int", "torch.lt().int", "torch.lt().int", "torch.lt().int", "torch.lt().int", "torch.lt().int", "torch.lt().int", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "math.log", "torch.lt", "torch.lt", "torch.lt", "torch.lt", "torch.lt", "torch.lt", "torch.lt", "torch.lt", "torch.lt", "torch.max.float", "torch.max.float", "torch.max.float", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like"], "methods", ["None"], ["", "def", "_relative_positions_bucket", "(", "self", ",", "relative_positions", ",", "bidirectional", "=", "False", ")", ":", "\n", "        ", "num_buckets", "=", "self", ".", "num_buckets", "\n", "max_distance", "=", "self", ".", "relative_max_distance", "\n", "n", "=", "-", "relative_positions", "\n", "result", "=", "0", "\n", "if", "bidirectional", ":", "\n", "            ", "num_buckets", "=", "num_buckets", "//", "2", "\n", "result", "=", "result", "+", "torch", ".", "lt", "(", "n", ",", "torch", ".", "zeros_like", "(", "n", ")", ")", ".", "int", "(", ")", "*", "num_buckets", "\n", "n", "=", "torch", ".", "abs", "(", "n", ")", "\n", "", "else", ":", "\n", "            ", "n", "=", "torch", ".", "max", "(", "n", ",", "torch", ".", "zeros_like", "(", "n", ")", ")", "\n", "", "max_exact", "=", "num_buckets", "//", "2", "\n", "is_small", "=", "torch", ".", "lt", "(", "n", ",", "max_exact", ")", "\n", "val_if_large", "=", "max_exact", "+", "torch", ".", "log", "(", "n", ".", "float", "(", ")", "/", "max_exact", ")", "/", "math", ".", "log", "(", "max_distance", "/", "max_exact", ")", "*", "(", "\n", "num_buckets", "-", "max_exact", ")", "\n", "val_if_large", "=", "torch", ".", "min", "(", "val_if_large", ",", "torch", ".", "ones_like", "(", "val_if_large", ")", "*", "(", "num_buckets", "-", "1", ")", ")", "\n", "val_if_large", "=", "val_if_large", ".", "int", "(", ")", "\n", "result", "=", "result", "+", "torch", ".", "where", "(", "is_small", ",", "n", ".", "int", "(", ")", ",", "val_if_large", ")", "\n", "return", "result", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet_decoding_step_ss_mle_absolute.ngram_s2s_model.NgramTransformerDecoder.cal_pretrain_relative_positions": [[720, 745], ["real_positions.unsqueeze", "main_stream_relative_positions.repeat.repeat.repeat", "real_positions.unsqueeze", "torch.cat().unsqueeze", "torch.cat().unsqueeze", "torch.cat().unsqueeze", "torch.cat().unsqueeze", "torch.cat().unsqueeze", "torch.cat().unsqueeze", "torch.cat().unsqueeze", "torch.cat().unsqueeze", "torch.cat().unsqueeze", "predicting_stream_relative_positions.repeat.repeat.repeat", "real_positions.unsqueeze", "ngram_s2s_model.NgramTransformerDecoder._relative_positions_bucket", "ngram_s2s_model.NgramTransformerDecoder._relative_positions_bucket", "real_positions.size", "real_positions.size", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat"], "methods", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_multihead_attention.NgramMultiheadAttention._relative_positions_bucket", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_multihead_attention.NgramMultiheadAttention._relative_positions_bucket", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.distribute.size", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.distribute.size"], ["", "def", "cal_pretrain_relative_positions", "(", "self", ",", "real_positions", ")", ":", "\n", "# main stream", "\n", "        ", "main_stream_relative_positions", "=", "real_positions", ".", "unsqueeze", "(", "1", ")", "\n", "# [B,T,T/S]", "\n", "main_stream_relative_positions", "=", "main_stream_relative_positions", ".", "repeat", "(", "1", ",", "real_positions", ".", "size", "(", "-", "1", ")", ",", "1", ")", "\n", "# [B,T,1]", "\n", "real_positions_main", "=", "real_positions", ".", "unsqueeze", "(", "-", "1", ")", "\n", "main_stream_relative_positions", "=", "main_stream_relative_positions", "-", "real_positions_main", "\n", "\n", "# predicting stream", "\n", "# input shift", "\n", "real_positions_shift_predicting_stream", "=", "real_positions", "-", "1", "\n", "# [B,1, 2*T]", "\n", "predicting_stream_relative_positions", "=", "torch", ".", "cat", "(", "(", "real_positions_shift_predicting_stream", ",", "real_positions", ")", ",", "\n", "dim", "=", "-", "1", ")", ".", "unsqueeze", "(", "1", ")", "\n", "# [B,T, 2*T]", "\n", "predicting_stream_relative_positions", "=", "predicting_stream_relative_positions", ".", "repeat", "(", "1", ",", "real_positions", ".", "size", "(", "-", "1", ")", ",", "\n", "1", ")", "\n", "# [B,T, 1]", "\n", "real_positions_predicting_stream", "=", "real_positions", ".", "unsqueeze", "(", "-", "1", ")", "\n", "predicting_stream_relative_positions", "=", "predicting_stream_relative_positions", "-", "real_positions_predicting_stream", "\n", "i_buckets_main_stream", "=", "self", ".", "_relative_positions_bucket", "(", "main_stream_relative_positions", ",", "bidirectional", "=", "False", ")", "\n", "i_bucket_relative_stream", "=", "self", ".", "_relative_positions_bucket", "(", "predicting_stream_relative_positions", ",", "\n", "bidirectional", "=", "False", ")", "\n", "return", "i_buckets_main_stream", ",", "i_bucket_relative_stream", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet_decoding_step_ss_mle_absolute.ngram_s2s_model.NgramTransformerDecoder.cal_finetune_relative_positions": [[746, 766], ["real_positions.size", "real_positions.size", "ngram_s2s_model.NgramTransformerDecoder._finetune_i_bucket_main_stream[].repeat", "torch.cat().repeat", "torch.cat().repeat", "torch.cat().repeat", "torch.cat().repeat", "torch.cat().repeat", "torch.cat().repeat", "torch.cat().repeat", "torch.cat().repeat", "torch.cat().repeat", "torch.arange().repeat", "torch.arange().repeat", "torch.arange().repeat", "torch.arange().repeat", "torch.arange().repeat", "torch.arange().repeat", "torch.arange().repeat", "torch.arange().repeat", "torch.arange().repeat", "ngram_s2s_model.NgramTransformerDecoder.cal_pretrain_relative_positions", "ngram_s2s_model.NgramTransformerDecoder.to", "torch.cat().repeat.to", "torch.cat().repeat.to", "torch.cat().repeat.to", "hasattr", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange"], "methods", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.distribute.size", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.distribute.size", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_s2s_model.NgramTransformerDecoder.cal_pretrain_relative_positions"], ["", "def", "cal_finetune_relative_positions", "(", "self", ",", "real_positions", ")", ":", "\n", "        ", "n_tokens", "=", "real_positions", ".", "size", "(", "-", "1", ")", "\n", "batch_size", "=", "real_positions", ".", "size", "(", "0", ")", "\n", "if", "not", "hasattr", "(", "self", ",", "\n", "'_finetune_i_bucket_main_stream'", ")", "or", "self", ".", "_finetune_i_bucket_main_stream", "is", "None", "or", "self", ".", "_finetune_i_bucket_main_stream", ".", "device", "!=", "real_positions", ".", "device", ":", "\n", "            ", "fake_positions", "=", "torch", ".", "arange", "(", "1", ",", "self", ".", "max_target_positions", "+", "1", ")", ".", "repeat", "(", "1", ",", "1", ")", "\n", "finetune_i_bucket_main_stream", ",", "finetune_i_bucket_predicting_stream", "=", "self", ".", "cal_pretrain_relative_positions", "(", "fake_positions", ")", "\n", "self", ".", "_finetune_i_bucket_main_stream", "=", "finetune_i_bucket_main_stream", ".", "to", "(", "real_positions", ".", "device", ")", "\n", "self", ".", "_finetune_i_bucket_predicting_stream", "=", "finetune_i_bucket_predicting_stream", ".", "to", "(", "real_positions", ".", "device", ")", "\n", "", "finetune_i_bucket_main_stream", "=", "self", ".", "_finetune_i_bucket_main_stream", "[", ":", ",", ":", "n_tokens", ",", ":", "n_tokens", "]", ".", "repeat", "(", "batch_size", ",", "\n", "1", ",", "1", ")", "\n", "finetune_i_bucket_predicting_stream", "=", "torch", ".", "cat", "(", "[", "\n", "self", ".", "_finetune_i_bucket_predicting_stream", "[", ":", ",", ":", "n_tokens", ",", ":", "n_tokens", "]", ",", "\n", "self", ".", "_finetune_i_bucket_predicting_stream", "[", ":", ",", ":", "n_tokens", ",", "\n", "self", ".", "max_target_positions", ":", "self", ".", "max_target_positions", "+", "n_tokens", "]", "\n", "]", ",", "2", ")", ".", "repeat", "(", "batch_size", ",", "1", ",", "1", ")", "\n", "return", "finetune_i_bucket_main_stream", ",", "finetune_i_bucket_predicting_stream", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet_decoding_step_ss_mle_absolute.ngram_s2s_model.NgramTransformerDecoder.extract_features": [[767, 867], ["ngram_s2s_model.NgramTransformerDecoder.embed_positions._forward", "ngram_s2s_model.NgramTransformerDecoder.transpose", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.dropout", "torch.dropout", "torch.dropout", "ngram_s2s_model.NgramTransformerDecoder.transpose().chunk", "ngram_s2s_model.NgramTransformerDecoder.embed_positions._forward", "ngram_s2s_model.NgramTransformerDecoder.cal_pretrain_relative_positions", "ngram_s2s_model.NgramTransformerDecoder.embed_tokens", "print", "Exception", "ngram_s2s_model.NgramTransformerDecoder.size", "ngram_s2s_model.NgramTransformerDecoder.buffered_future_mask", "ngram_s2s_model.NgramTransformerDecoder.buffered_future_mask_ngram", "ngram_s2s_model.NgramTransformerDecoder.emb_layer_norm", "layer", "inner_states.append", "attn.transpose().chunk", "ngram_s2s_model.NgramTransformerDecoder.embed_positions", "ngram_s2s_model.NgramTransformerDecoder.cal_finetune_relative_positions", "ngram_s2s_model.NgramTransformerDecoder.transpose", "range", "range", "attn.transpose"], "methods", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.learned_positional_embedding.LearnedPositionalEmbedding._forward", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.learned_positional_embedding.LearnedPositionalEmbedding._forward", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_s2s_model.NgramTransformerDecoder.cal_pretrain_relative_positions", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.distribute.size", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_s2s_model.NgramTransformerDecoder.buffered_future_mask", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_s2s_model.NgramTransformerDecoder.buffered_future_mask_ngram", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_s2s_model.NgramTransformerDecoder.cal_finetune_relative_positions"], ["", "def", "extract_features", "(", "self", ",", "prev_output_tokens", ",", "encoder_out", "=", "None", ",", "incremental_state", "=", "None", ",", "**", "unused", ")", ":", "\n", "# embed positions", "\n", "# [bos, A, B, C, D, eos] with real positions [1,2,3,4,5,6](main stream), [2,3,4,5,6,7](predicting stream)", "\n", "# target [B,C,D] with prev [A,B,C] from [A,B,C,D] as pretraining span with real positions [2,3,4],", "\n", "# but target actually [3,4,5] for fine tune with another [bos].", "\n", "# thus [2,3,4] used for main stream shifted prev [A,B,C], [3,4,5] used for predicting [B,C,D]", "\n", "        ", "if", "'positions'", "in", "unused", ":", "\n", "# pretrain procedure", "\n", "            ", "main_stream_pos_embed", "=", "self", ".", "embed_positions", ".", "_forward", "(", "unused", "[", "'positions'", "]", ")", "\n", "real_positions", "=", "unused", "[", "'positions'", "]", "\n", "i_buckets_main_stream", ",", "i_bucket_relative_stream", "=", "self", ".", "cal_pretrain_relative_positions", "(", "real_positions", ")", "\n", "", "else", ":", "\n", "# fine tune procedure", "\n", "            ", "main_stream_pos_embed", ",", "real_positions", "=", "self", ".", "embed_positions", "(", "\n", "prev_output_tokens", ",", "\n", "incremental_state", "=", "incremental_state", ",", "\n", ")", "if", "self", ".", "embed_positions", "is", "not", "None", "else", "None", "\n", "if", "incremental_state", "is", "not", "None", ":", "\n", "                ", "i_buckets_main_stream", ",", "i_bucket_relative_stream", "=", "None", ",", "None", "\n", "", "else", ":", "\n", "                ", "i_buckets_main_stream", ",", "i_bucket_relative_stream", "=", "self", ".", "cal_finetune_relative_positions", "(", "real_positions", ")", "\n", "\n", "", "", "predicting_stream_pos_embed", "=", "self", ".", "embed_positions", ".", "_forward", "(", "real_positions", "+", "1", ")", "\n", "\n", "if", "incremental_state", "is", "not", "None", ":", "\n", "            ", "prev_output_tokens", "=", "prev_output_tokens", "[", ":", ",", "-", "1", ":", "]", "\n", "if", "main_stream_pos_embed", "is", "not", "None", ":", "\n", "                ", "main_stream_pos_embed", "=", "main_stream_pos_embed", "[", ":", ",", "-", "1", ":", "]", "\n", "\n", "# ATT:", "\n", "", "", "if", "'new_decoder_input_emb'", "in", "unused", "and", "not", "unused", "[", "\"new_decoder_input_emb\"", "]", "is", "None", ":", "\n", "            ", "x", "=", "unused", "[", "\"new_decoder_input_emb\"", "]", "\n", "", "else", ":", "\n", "            ", "x", "=", "self", ".", "embed_tokens", "(", "prev_output_tokens", ")", "\n", "# embed tokens and positions", "\n", "", "if", "self", ".", "embed_scale", "is", "not", "None", ":", "\n", "            ", "x", "*=", "self", ".", "embed_scale", "\n", "\n", "", "if", "main_stream_pos_embed", "is", "not", "None", ":", "\n", "            ", "x", "+=", "main_stream_pos_embed", "\n", "\n", "# B x T x C -> T x B x C", "\n", "", "x", "=", "x", ".", "transpose", "(", "0", ",", "1", ")", "\n", "attn", "=", "None", "\n", "\n", "inner_states", "=", "[", "x", "]", "\n", "if", "main_stream_pos_embed", "is", "None", ":", "\n", "            ", "print", "(", "'positions should be used to predict ngrams'", ")", "\n", "raise", "Exception", "(", ")", "\n", "\n", "", "if", "self", ".", "embed_scale", "is", "not", "None", ":", "\n", "            ", "ngram_input_embed", "=", "self", ".", "embed_scale", "*", "self", ".", "ngram_input_embed", ".", "weight", "\n", "", "else", ":", "\n", "            ", "ngram_input_embed", "=", "self", ".", "ngram_input_embed", ".", "weight", "\n", "\n", "", "if", "incremental_state", "is", "not", "None", ":", "\n", "            ", "B", "=", "x", ".", "size", "(", "1", ")", "\n", "ngram_masks", "=", "[", "\n", "(", "ngram_input_embed", "[", "ngram", "-", "1", "]", "+", "predicting_stream_pos_embed", ")", ".", "transpose", "(", "0", ",", "1", ")", ".", "repeat", "(", "1", ",", "B", ",", "1", ")", "\n", "for", "ngram", "in", "range", "(", "self", ".", "ngram", ")", "]", "\n", "", "else", ":", "\n", "            ", "ngram_masks", "=", "[", "(", "ngram_input_embed", "[", "ngram", "-", "1", "]", "+", "predicting_stream_pos_embed", ")", ".", "transpose", "(", "0", ",", "1", ")", "for", "\n", "ngram", "in", "range", "(", "self", ".", "ngram", ")", "]", "\n", "\n", "", "self_attn_mask", "=", "self", ".", "buffered_future_mask", "(", "x", ")", "if", "incremental_state", "is", "None", "else", "None", "\n", "ngram_mask_matrix", "=", "self", ".", "buffered_future_mask_ngram", "(", "x", ")", "if", "incremental_state", "is", "None", "else", "None", "\n", "\n", "# TODO in train [(1+ngram)*T, B, C], in inference [T+ngram, B, C]", "\n", "x", "=", "torch", ".", "cat", "(", "[", "x", "]", "+", "ngram_masks", ",", "0", ")", "\n", "\n", "if", "self", ".", "emb_layer_norm", ":", "\n", "            ", "x", "=", "self", ".", "emb_layer_norm", "(", "x", ")", "\n", "\n", "", "x", "=", "F", ".", "dropout", "(", "x", ",", "p", "=", "self", ".", "dropout", ",", "training", "=", "self", ".", "training", ")", "\n", "\n", "# decoder layers", "\n", "for", "layer", "in", "self", ".", "layers", ":", "\n", "            ", "x", ",", "attn", "=", "layer", "(", "\n", "x", ",", "\n", "encoder_out", "[", "'encoder_out'", "]", "if", "encoder_out", "is", "not", "None", "else", "None", ",", "\n", "encoder_out", "[", "'encoder_padding_mask'", "]", "if", "encoder_out", "is", "not", "None", "else", "None", ",", "\n", "incremental_state", ",", "\n", "self_attn_mask", "=", "self_attn_mask", ",", "\n", "ngram_mask_matrix", "=", "ngram_mask_matrix", ",", "\n", "i_buckets_main_stream", "=", "i_buckets_main_stream", ",", "\n", "i_bucket_relative_stream", "=", "i_bucket_relative_stream", ",", "\n", "real_positions", "=", "real_positions", "\n", ")", "\n", "inner_states", ".", "append", "(", "x", ")", "\n", "\n", "# TODO [(1+ngram)*T, B, C] -> [B, (1+ngram)*T, C]", "\n", "", "x_list", "=", "x", ".", "transpose", "(", "0", ",", "1", ")", ".", "chunk", "(", "1", "+", "self", ".", "ngram", ",", "1", ")", "\n", "if", "attn", "is", "not", "None", ":", "\n", "            ", "attn_list", "=", "attn", ".", "transpose", "(", "0", ",", "1", ")", ".", "chunk", "(", "1", "+", "self", ".", "ngram", ",", "1", ")", "\n", "", "else", ":", "\n", "            ", "attn_list", "=", "None", "\n", "\n", "", "return", "x_list", ",", "{", "'attn'", ":", "attn_list", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet_decoding_step_ss_mle_absolute.ngram_s2s_model.NgramTransformerDecoder.get_normalized_probs": [[868, 891], ["hasattr", "ngram_s2s_model.NgramTransformerDecoder.adaptive_softmax.get_log_prob", "fairseq.utils.log_softmax", "fairseq.utils.softmax", "ngram_s2s_model.NgramTransformerDecoder.exp_"], "methods", ["None"], ["", "def", "get_normalized_probs", "(", "self", ",", "net_output", ",", "log_probs", ",", "sample", ")", ":", "\n", "        ", "\"\"\"Get normalized probabilities (or log probs) from a net's output.\"\"\"", "\n", "\n", "if", "hasattr", "(", "self", ",", "'adaptive_softmax'", ")", "and", "self", ".", "adaptive_softmax", "is", "not", "None", ":", "\n", "            ", "if", "sample", "is", "not", "None", ":", "\n", "                ", "assert", "'target'", "in", "sample", "\n", "target", "=", "sample", "[", "'target'", "]", "\n", "", "else", ":", "\n", "                ", "target", "=", "None", "\n", "", "out", "=", "self", ".", "adaptive_softmax", ".", "get_log_prob", "(", "net_output", "[", "0", "]", ",", "target", "=", "target", ")", "\n", "return", "out", ".", "exp_", "(", ")", "if", "not", "log_probs", "else", "out", "\n", "", "'''\n        logits_list = net_output[0]\n        if log_probs:\n            return [utils.log_softmax(logits, dim=-1, onnx_trace=self.onnx_trace) for logits in logits_list][0]\n        else:\n            return [utils.softmax(logits, dim=-1, onnx_trace=self.onnx_trace) for logits in logits_list][0]\n        '''", "\n", "logits", "=", "net_output", "[", "0", "]", "\n", "if", "log_probs", ":", "\n", "            ", "return", "utils", ".", "log_softmax", "(", "logits", ",", "dim", "=", "-", "1", ",", "onnx_trace", "=", "self", ".", "onnx_trace", ")", "\n", "", "else", ":", "\n", "            ", "return", "utils", ".", "softmax", "(", "logits", ",", "dim", "=", "-", "1", ",", "onnx_trace", "=", "self", ".", "onnx_trace", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet_decoding_step_ss_mle_absolute.ngram_s2s_model.NgramTransformerDecoder.output_layer": [[893, 900], ["torch.linear", "torch.linear", "torch.linear", "torch.linear", "torch.linear", "torch.linear"], "methods", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.layers.nn.linear", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.layers.nn.linear", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.layers.nn.linear", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.layers.nn.linear", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.layers.nn.linear", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.layers.nn.linear"], ["", "", "def", "output_layer", "(", "self", ",", "features", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"Project features to the vocabulary size.\"\"\"", "\n", "# project back to size of vocabulary", "\n", "if", "self", ".", "share_input_output_embed", ":", "\n", "            ", "return", "F", ".", "linear", "(", "features", ",", "self", ".", "embed_tokens", ".", "weight", ")", "\n", "", "else", ":", "\n", "            ", "return", "F", ".", "linear", "(", "features", ",", "self", ".", "embed_out", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet_decoding_step_ss_mle_absolute.ngram_s2s_model.NgramTransformerDecoder.max_positions": [[901, 906], ["min", "ngram_s2s_model.NgramTransformerDecoder.embed_positions.max_positions"], "methods", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_s2s_model.NgramTransformerDecoder.max_positions"], ["", "", "def", "max_positions", "(", "self", ")", ":", "\n", "        ", "\"\"\"Maximum output length supported by the decoder.\"\"\"", "\n", "if", "self", ".", "embed_positions", "is", "None", ":", "\n", "            ", "return", "self", ".", "max_target_positions", "\n", "", "return", "min", "(", "self", ".", "max_target_positions", ",", "self", ".", "embed_positions", ".", "max_positions", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet_decoding_step_ss_mle_absolute.ngram_s2s_model.NgramTransformerDecoder.buffered_future_mask": [[907, 914], ["tensor.size", "torch.triu", "torch.triu", "torch.triu", "torch.triu", "torch.triu", "torch.triu", "torch.triu", "torch.triu", "torch.triu", "hasattr", "ngram_s2s_model.NgramTransformerDecoder._future_mask.size", "fairseq.utils.fill_with_neg_inf", "tensor.new"], "methods", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.distribute.size", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.distribute.size"], ["", "def", "buffered_future_mask", "(", "self", ",", "tensor", ")", ":", "\n", "        ", "dim", "=", "tensor", ".", "size", "(", "0", ")", "\n", "if", "not", "hasattr", "(", "self", ",", "\n", "'_future_mask'", ")", "or", "self", ".", "_future_mask", "is", "None", "or", "self", ".", "_future_mask", ".", "device", "!=", "tensor", ".", "device", "or", "self", ".", "_future_mask", ".", "size", "(", "\n", "0", ")", "<", "dim", ":", "\n", "            ", "self", ".", "_future_mask", "=", "torch", ".", "triu", "(", "utils", ".", "fill_with_neg_inf", "(", "tensor", ".", "new", "(", "dim", ",", "dim", ")", ")", ",", "1", ")", "\n", "", "return", "self", ".", "_future_mask", "[", ":", "dim", ",", ":", "dim", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet_decoding_step_ss_mle_absolute.ngram_s2s_model.NgramTransformerDecoder.buffered_future_mask_ngram": [[915, 926], ["tensor.size", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "ngram_multihead_attention.ngram_attention_bias().type().to", "hasattr", "ngram_multihead_attention.ngram_attention_bias().type", "ngram_multihead_attention.ngram_attention_bias"], "methods", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.distribute.size", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_multihead_attention.ngram_attention_bias"], ["", "def", "buffered_future_mask_ngram", "(", "self", ",", "tensor", ")", ":", "\n", "        ", "dim", "=", "tensor", ".", "size", "(", "0", ")", "\n", "if", "not", "hasattr", "(", "self", ",", "\n", "'_ngram_future_mask'", ")", "or", "self", ".", "_ngram_future_mask", "is", "None", "or", "self", ".", "_ngram_future_mask", ".", "device", "!=", "tensor", ".", "device", ":", "\n", "            ", "self", ".", "_ngram_future_mask", "=", "ngram_attention_bias", "(", "self", ".", "max_target_positions", ",", "self", ".", "ngram", ")", ".", "type", "(", "tensor", ".", "dtype", ")", ".", "to", "(", "\n", "tensor", ".", "device", ")", "\n", "", "ngram_future_mask", "=", "torch", ".", "cat", "(", "[", "self", ".", "_ngram_future_mask", "[", ":", ",", ":", "dim", ",", ":", "dim", "]", ",", "\n", "self", ".", "_ngram_future_mask", "[", ":", ",", ":", "dim", ",", "\n", "self", ".", "max_target_positions", ":", "self", ".", "max_target_positions", "+", "dim", "]", "\n", "]", ",", "2", ")", "\n", "return", "ngram_future_mask", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet_decoding_step_ss_mle_absolute.ngram_s2s_model.Embedding": [[928, 933], ["torch.Embedding", "torch.init.normal_", "torch.init.constant_"], "function", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_s2s_model.Embedding"], ["", "", "def", "Embedding", "(", "num_embeddings", ",", "embedding_dim", ",", "padding_idx", ")", ":", "\n", "    ", "m", "=", "nn", ".", "Embedding", "(", "num_embeddings", ",", "embedding_dim", ",", "padding_idx", "=", "padding_idx", ")", "\n", "nn", ".", "init", ".", "normal_", "(", "m", ".", "weight", ",", "mean", "=", "0", ",", "std", "=", "embedding_dim", "**", "-", "0.5", ")", "\n", "nn", ".", "init", ".", "constant_", "(", "m", ".", "weight", "[", "padding_idx", "]", ",", "0", ")", "\n", "return", "m", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet_decoding_step_ss_mle_absolute.ngram_s2s_model.Linear": [[935, 941], ["torch.Linear", "torch.init.xavier_uniform_", "torch.init.constant_"], "function", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_s2s_model.Linear"], ["", "def", "Linear", "(", "in_features", ",", "out_features", ",", "bias", "=", "True", ")", ":", "\n", "    ", "m", "=", "nn", ".", "Linear", "(", "in_features", ",", "out_features", ",", "bias", ")", "\n", "nn", ".", "init", ".", "xavier_uniform_", "(", "m", ".", "weight", ")", "\n", "if", "bias", ":", "\n", "        ", "nn", ".", "init", ".", "constant_", "(", "m", ".", "bias", ",", "0.", ")", "\n", "", "return", "m", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet_decoding_step_ss_mle_absolute.ngram_s2s_model.base_architecture": [[943, 967], ["fairseq.models.register_model_architecture", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr"], "function", ["None"], ["", "@", "register_model_architecture", "(", "'ngram_transformer_prophet'", ",", "'ngram_transformer_prophet'", ")", "\n", "def", "base_architecture", "(", "args", ")", ":", "\n", "    ", "args", ".", "ngram", "=", "getattr", "(", "args", ",", "'ngram'", ",", "2", ")", "\n", "args", ".", "num_buckets", "=", "getattr", "(", "args", ",", "'num_buckets'", ",", "32", ")", "\n", "args", ".", "relative_max_distance", "=", "getattr", "(", "args", ",", "'relative_max_distance'", ",", "128", ")", "\n", "\n", "args", ".", "activation_fn", "=", "getattr", "(", "args", ",", "'activation_fn'", ",", "'relu'", ")", "\n", "args", ".", "dropout", "=", "getattr", "(", "args", ",", "'dropout'", ",", "0.1", ")", "\n", "args", ".", "attention_dropout", "=", "getattr", "(", "args", ",", "'attention_dropout'", ",", "0.", ")", "\n", "args", ".", "activation_dropout", "=", "getattr", "(", "args", ",", "'activation_dropout'", ",", "0.", ")", "\n", "\n", "args", ".", "encoder_embed_dim", "=", "getattr", "(", "args", ",", "'encoder_embed_dim'", ",", "512", ")", "\n", "args", ".", "encoder_ffn_embed_dim", "=", "getattr", "(", "args", ",", "'encoder_ffn_embed_dim'", ",", "2048", ")", "\n", "args", ".", "encoder_layers", "=", "getattr", "(", "args", ",", "'encoder_layers'", ",", "6", ")", "\n", "args", ".", "encoder_attention_heads", "=", "getattr", "(", "args", ",", "'encoder_attention_heads'", ",", "8", ")", "\n", "\n", "args", ".", "decoder_embed_dim", "=", "getattr", "(", "args", ",", "'decoder_embed_dim'", ",", "512", ")", "\n", "args", ".", "decoder_ffn_embed_dim", "=", "getattr", "(", "args", ",", "'decoder_ffn_embed_dim'", ",", "2048", ")", "\n", "args", ".", "decoder_layers", "=", "getattr", "(", "args", ",", "'decoder_layers'", ",", "6", ")", "\n", "args", ".", "decoder_attention_heads", "=", "getattr", "(", "args", ",", "'decoder_attention_heads'", ",", "8", ")", "\n", "\n", "args", ".", "share_decoder_input_output_embed", "=", "getattr", "(", "args", ",", "'share_decoder_input_output_embed'", ",", "False", ")", "\n", "args", ".", "share_all_embeddings", "=", "getattr", "(", "args", ",", "'share_all_embeddings'", ",", "False", ")", "\n", "args", ".", "load_sep", "=", "getattr", "(", "args", ",", "'load_sep'", ",", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet_decoding_step_ss_mle_absolute.ngram_s2s_model.transformer_base": [[969, 993], ["fairseq.models.register_model_architecture", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "ngram_s2s_model.base_architecture"], "function", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_s2s_model.base_architecture"], ["", "@", "register_model_architecture", "(", "'ngram_transformer_prophet'", ",", "'ngram_transformer_prophet_base'", ")", "\n", "def", "transformer_base", "(", "args", ")", ":", "\n", "    ", "args", ".", "ngram", "=", "getattr", "(", "args", ",", "'ngram'", ",", "2", ")", "\n", "args", ".", "num_buckets", "=", "getattr", "(", "args", ",", "'num_buckets'", ",", "32", ")", "\n", "args", ".", "relative_max_distance", "=", "getattr", "(", "args", ",", "'relative_max_distance'", ",", "128", ")", "\n", "\n", "args", ".", "encoder_embed_dim", "=", "getattr", "(", "args", ",", "'encoder_embed_dim'", ",", "768", ")", "\n", "args", ".", "encoder_ffn_embed_dim", "=", "getattr", "(", "args", ",", "'encoder_ffn_embed_dim'", ",", "3072", ")", "\n", "args", ".", "encoder_attention_heads", "=", "getattr", "(", "args", ",", "'encoder_attention_heads'", ",", "12", ")", "\n", "args", ".", "encoder_layers", "=", "getattr", "(", "args", ",", "'encoder_layers'", ",", "6", ")", "\n", "\n", "args", ".", "dropout", "=", "getattr", "(", "args", ",", "'dropout'", ",", "0.1", ")", "\n", "args", ".", "attention_dropout", "=", "getattr", "(", "args", ",", "'attention_dropout'", ",", "0.1", ")", "\n", "args", ".", "activation_dropout", "=", "getattr", "(", "args", ",", "'activation_dropout'", ",", "0.1", ")", "\n", "args", ".", "activation_fn", "=", "getattr", "(", "args", ",", "'activation_fn'", ",", "'gelu'", ")", "\n", "\n", "args", ".", "decoder_embed_dim", "=", "getattr", "(", "args", ",", "'decoder_embed_dim'", ",", "768", ")", "\n", "args", ".", "decoder_ffn_embed_dim", "=", "getattr", "(", "args", ",", "'decoder_ffn_embed_dim'", ",", "3072", ")", "\n", "args", ".", "decoder_attention_heads", "=", "getattr", "(", "args", ",", "'decoder_attention_heads'", ",", "12", ")", "\n", "args", ".", "decoder_layers", "=", "getattr", "(", "args", ",", "'decoder_layers'", ",", "6", ")", "\n", "\n", "args", ".", "share_decoder_input_output_embed", "=", "getattr", "(", "args", ",", "'share_decoder_input_output_embed'", ",", "True", ")", "\n", "args", ".", "share_all_embeddings", "=", "getattr", "(", "args", ",", "'share_all_embeddings'", ",", "True", ")", "\n", "base_architecture", "(", "args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet_decoding_step_ss_mle_absolute.ngram_s2s_model.transformer_middle": [[995, 1011], ["fairseq.models.register_model_architecture", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "ngram_s2s_model.transformer_base"], "function", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_s2s_model.transformer_base"], ["", "@", "register_model_architecture", "(", "'ngram_transformer_prophet'", ",", "'ngram_transformer_prophet_middle'", ")", "\n", "def", "transformer_middle", "(", "args", ")", ":", "\n", "    ", "args", ".", "ngram", "=", "getattr", "(", "args", ",", "'ngram'", ",", "2", ")", "\n", "args", ".", "num_buckets", "=", "getattr", "(", "args", ",", "'num_buckets'", ",", "32", ")", "\n", "args", ".", "relative_max_distance", "=", "getattr", "(", "args", ",", "'relative_max_distance'", ",", "128", ")", "\n", "\n", "args", ".", "encoder_embed_dim", "=", "getattr", "(", "args", ",", "'encoder_embed_dim'", ",", "1024", ")", "\n", "args", ".", "encoder_ffn_embed_dim", "=", "getattr", "(", "args", ",", "'encoder_ffn_embed_dim'", ",", "4096", ")", "\n", "args", ".", "encoder_attention_heads", "=", "getattr", "(", "args", ",", "'encoder_attention_heads'", ",", "16", ")", "\n", "args", ".", "encoder_layers", "=", "getattr", "(", "args", ",", "'encoder_layers'", ",", "6", ")", "\n", "\n", "args", ".", "decoder_embed_dim", "=", "getattr", "(", "args", ",", "'decoder_embed_dim'", ",", "1024", ")", "\n", "args", ".", "decoder_ffn_embed_dim", "=", "getattr", "(", "args", ",", "'decoder_ffn_embed_dim'", ",", "4096", ")", "\n", "args", ".", "decoder_attention_heads", "=", "getattr", "(", "args", ",", "'decoder_attention_heads'", ",", "16", ")", "\n", "args", ".", "decoder_layers", "=", "getattr", "(", "args", ",", "'decoder_layers'", ",", "6", ")", "\n", "transformer_base", "(", "args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet_decoding_step_ss_mle_absolute.ngram_s2s_model.transformer_big": [[1013, 1022], ["fairseq.models.register_model_architecture", "getattr", "getattr", "getattr", "getattr", "getattr", "ngram_s2s_model.transformer_middle"], "function", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_s2s_model.transformer_middle"], ["", "@", "register_model_architecture", "(", "'ngram_transformer_prophet'", ",", "'ngram_transformer_prophet_large'", ")", "\n", "def", "transformer_big", "(", "args", ")", ":", "\n", "    ", "args", ".", "ngram", "=", "getattr", "(", "args", ",", "'ngram'", ",", "2", ")", "\n", "args", ".", "num_buckets", "=", "getattr", "(", "args", ",", "'num_buckets'", ",", "32", ")", "\n", "args", ".", "relative_max_distance", "=", "getattr", "(", "args", ",", "'relative_max_distance'", ",", "128", ")", "\n", "\n", "args", ".", "encoder_layers", "=", "getattr", "(", "args", ",", "'encoder_layers'", ",", "12", ")", "\n", "args", ".", "decoder_layers", "=", "getattr", "(", "args", ",", "'decoder_layers'", ",", "12", ")", "\n", "transformer_middle", "(", "args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet_decoding_step_ss_mle_absolute.ngram_multihead_attention.NgramMultiheadAttention.__init__": [[38, 89], ["torch.nn.Module.__init__", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "ngram_multihead_attention.NgramMultiheadAttention.reset_parameters", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "ngram_multihead_attention.NgramMultiheadAttention.register_parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor"], "methods", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.transformer_lrp.TransformerLRP.__init__", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_s2s_model.Linear", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_s2s_model.Linear", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_s2s_model.Linear", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_s2s_model.Linear", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_multihead_attention.NgramMultiheadAttention.reset_parameters"], ["def", "__init__", "(", "self", ",", "embed_dim", ",", "num_heads", ",", "kdim", "=", "None", ",", "vdim", "=", "None", ",", "dropout", "=", "0.", ",", "bias", "=", "True", ",", "\n", "add_bias_kv", "=", "False", ",", "add_zero_attn", "=", "False", ",", "self_attention", "=", "False", ",", "\n", "encoder_decoder_attention", "=", "False", ",", "ngram", "=", "2", ",", "num_buckets", "=", "32", ",", "relative_max_distance", "=", "128", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "embed_dim", "=", "embed_dim", "\n", "self", ".", "kdim", "=", "kdim", "if", "kdim", "is", "not", "None", "else", "embed_dim", "\n", "self", ".", "vdim", "=", "vdim", "if", "vdim", "is", "not", "None", "else", "embed_dim", "\n", "self", ".", "qkv_same_dim", "=", "self", ".", "kdim", "==", "embed_dim", "and", "self", ".", "vdim", "==", "embed_dim", "\n", "\n", "self", ".", "num_buckets", "=", "num_buckets", "\n", "self", ".", "relative_max_distance", "=", "relative_max_distance", "\n", "self", ".", "num_heads", "=", "num_heads", "\n", "self", ".", "dropout", "=", "dropout", "\n", "self", ".", "head_dim", "=", "embed_dim", "//", "num_heads", "\n", "self", ".", "ngram", "=", "ngram", "\n", "\n", "assert", "self", ".", "head_dim", "*", "num_heads", "==", "self", ".", "embed_dim", ",", "\"embed_dim must be divisible by num_heads\"", "\n", "self", ".", "scaling", "=", "self", ".", "head_dim", "**", "-", "0.5", "\n", "\n", "self", ".", "self_attention", "=", "self_attention", "\n", "self", ".", "encoder_decoder_attention", "=", "encoder_decoder_attention", "\n", "\n", "assert", "not", "self", ".", "self_attention", "or", "self", ".", "qkv_same_dim", ",", "'Self-attention requires query, key and '", "'value to be of the same size'", "\n", "\n", "self", ".", "relative_linear", "=", "nn", ".", "Linear", "(", "embed_dim", ",", "num_buckets", "*", "num_heads", ")", "\n", "if", "self", ".", "qkv_same_dim", ":", "\n", "            ", "self", ".", "in_proj_weight", "=", "Parameter", "(", "torch", ".", "Tensor", "(", "3", "*", "embed_dim", ",", "embed_dim", ")", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "k_proj_weight", "=", "Parameter", "(", "torch", ".", "Tensor", "(", "embed_dim", ",", "self", ".", "kdim", ")", ")", "\n", "self", ".", "v_proj_weight", "=", "Parameter", "(", "torch", ".", "Tensor", "(", "embed_dim", ",", "self", ".", "vdim", ")", ")", "\n", "self", ".", "q_proj_weight", "=", "Parameter", "(", "torch", ".", "Tensor", "(", "embed_dim", ",", "embed_dim", ")", ")", "\n", "\n", "", "if", "bias", ":", "\n", "            ", "self", ".", "in_proj_bias", "=", "Parameter", "(", "torch", ".", "Tensor", "(", "3", "*", "embed_dim", ")", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "register_parameter", "(", "'in_proj_bias'", ",", "None", ")", "\n", "\n", "", "self", ".", "out_proj", "=", "nn", ".", "Linear", "(", "embed_dim", ",", "embed_dim", ",", "bias", "=", "bias", ")", "\n", "\n", "if", "add_bias_kv", ":", "\n", "            ", "self", ".", "bias_k", "=", "Parameter", "(", "torch", ".", "Tensor", "(", "1", ",", "1", ",", "embed_dim", ")", ")", "\n", "self", ".", "bias_v", "=", "Parameter", "(", "torch", ".", "Tensor", "(", "1", ",", "1", ",", "embed_dim", ")", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "bias_k", "=", "self", ".", "bias_v", "=", "None", "\n", "\n", "", "self", ".", "add_zero_attn", "=", "add_zero_attn", "\n", "\n", "self", ".", "reset_parameters", "(", ")", "\n", "\n", "self", ".", "onnx_trace", "=", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet_decoding_step_ss_mle_absolute.ngram_multihead_attention.NgramMultiheadAttention.prepare_for_onnx_export_": [[90, 92], ["None"], "methods", ["None"], ["", "def", "prepare_for_onnx_export_", "(", "self", ")", ":", "\n", "        ", "self", ".", "onnx_trace", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet_decoding_step_ss_mle_absolute.ngram_multihead_attention.NgramMultiheadAttention.reset_parameters": [[93, 109], ["torch.nn.init.xavier_uniform_", "torch.nn.init.xavier_uniform_", "torch.nn.init.xavier_uniform_", "torch.nn.init.xavier_uniform_", "torch.nn.init.xavier_uniform_", "torch.nn.init.xavier_uniform_", "torch.nn.init.xavier_uniform_", "torch.nn.init.xavier_uniform_", "torch.nn.init.xavier_uniform_", "torch.nn.init.xavier_uniform_", "torch.nn.init.constant_", "torch.nn.init.constant_", "torch.nn.init.constant_", "torch.nn.init.constant_", "torch.nn.init.xavier_normal_", "torch.nn.init.xavier_normal_", "torch.nn.init.xavier_normal_", "torch.nn.init.xavier_normal_"], "methods", ["None"], ["", "def", "reset_parameters", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "qkv_same_dim", ":", "\n", "            ", "nn", ".", "init", ".", "xavier_uniform_", "(", "self", ".", "in_proj_weight", ")", "\n", "", "else", ":", "\n", "            ", "nn", ".", "init", ".", "xavier_uniform_", "(", "self", ".", "k_proj_weight", ")", "\n", "nn", ".", "init", ".", "xavier_uniform_", "(", "self", ".", "v_proj_weight", ")", "\n", "nn", ".", "init", ".", "xavier_uniform_", "(", "self", ".", "q_proj_weight", ")", "\n", "\n", "", "nn", ".", "init", ".", "xavier_uniform_", "(", "self", ".", "out_proj", ".", "weight", ")", "\n", "if", "self", ".", "in_proj_bias", "is", "not", "None", ":", "\n", "            ", "nn", ".", "init", ".", "constant_", "(", "self", ".", "in_proj_bias", ",", "0.", ")", "\n", "nn", ".", "init", ".", "constant_", "(", "self", ".", "out_proj", ".", "bias", ",", "0.", ")", "\n", "", "if", "self", ".", "bias_k", "is", "not", "None", ":", "\n", "            ", "nn", ".", "init", ".", "xavier_normal_", "(", "self", ".", "bias_k", ")", "\n", "", "if", "self", ".", "bias_v", "is", "not", "None", ":", "\n", "            ", "nn", ".", "init", ".", "xavier_normal_", "(", "self", ".", "bias_v", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet_decoding_step_ss_mle_absolute.ngram_multihead_attention.NgramMultiheadAttention._relative_positions_bucket": [[110, 129], ["torch.lt", "torch.lt", "torch.lt", "torch.lt", "torch.min", "torch.min", "torch.min", "torch.min", "val_if_large.int.int.int", "torch.abs", "torch.abs", "torch.abs", "torch.abs", "torch.max", "torch.max", "torch.max", "torch.max", "torch.where", "torch.where", "torch.where", "torch.where", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.ones_like", "torch.ones_like", "torch.ones_like", "torch.ones_like", "torch.max.int", "torch.max.int", "torch.lt().int", "torch.lt().int", "torch.lt().int", "torch.lt().int", "torch.log", "torch.log", "torch.log", "torch.log", "math.log", "torch.lt", "torch.lt", "torch.lt", "torch.lt", "torch.max.float", "torch.max.float", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like"], "methods", ["None"], ["", "", "def", "_relative_positions_bucket", "(", "self", ",", "relative_positions", ",", "bidirectional", "=", "False", ")", ":", "\n", "        ", "num_buckets", "=", "self", ".", "num_buckets", "\n", "max_distance", "=", "self", ".", "relative_max_distance", "\n", "n", "=", "-", "relative_positions", "\n", "result", "=", "0", "\n", "if", "bidirectional", ":", "\n", "            ", "num_buckets", "=", "num_buckets", "//", "2", "\n", "result", "=", "result", "+", "torch", ".", "lt", "(", "n", ",", "torch", ".", "zeros_like", "(", "n", ")", ")", ".", "int", "(", ")", "*", "num_buckets", "\n", "n", "=", "torch", ".", "abs", "(", "n", ")", "\n", "", "else", ":", "\n", "            ", "n", "=", "torch", ".", "max", "(", "n", ",", "torch", ".", "zeros_like", "(", "n", ")", ")", "\n", "", "max_exact", "=", "num_buckets", "//", "2", "\n", "is_small", "=", "torch", ".", "lt", "(", "n", ",", "max_exact", ")", "\n", "val_if_large", "=", "max_exact", "+", "torch", ".", "log", "(", "n", ".", "float", "(", ")", "/", "max_exact", ")", "/", "math", ".", "log", "(", "max_distance", "/", "max_exact", ")", "*", "(", "\n", "num_buckets", "-", "max_exact", ")", "\n", "val_if_large", "=", "torch", ".", "min", "(", "val_if_large", ",", "torch", ".", "ones_like", "(", "val_if_large", ")", "*", "(", "num_buckets", "-", "1", ")", ")", "\n", "val_if_large", "=", "val_if_large", ".", "int", "(", ")", "\n", "result", "=", "result", "+", "torch", ".", "where", "(", "is_small", ",", "n", ".", "int", "(", ")", ",", "val_if_large", ")", "\n", "return", "result", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet_decoding_step_ss_mle_absolute.ngram_multihead_attention.NgramMultiheadAttention.main_stream_relative_logits": [[131, 176], ["query.transpose.transpose.size", "attn_weights.size", "query.transpose.transpose.transpose", "ngram_multihead_attention.NgramMultiheadAttention.relative_linear", "values.reshape.reshape.view", "values.reshape.reshape.transpose", "values.reshape.reshape.transpose", "values.reshape.reshape.reshape", "ngram_multihead_attention.NgramMultiheadAttention.repeat().view", "values.reshape.reshape.reshape", "ngram_multihead_attention.NgramMultiheadAttention.view().long", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "result.view.view.view", "torch.arange().unsqueeze().unsqueeze().repeat().to", "torch.arange().unsqueeze().unsqueeze().repeat().to", "torch.arange().unsqueeze().unsqueeze().repeat().to", "torch.arange().unsqueeze().unsqueeze().repeat().to", "real_positions.unsqueeze().repeat.unsqueeze().repeat.unsqueeze().repeat", "ngram_multihead_attention.NgramMultiheadAttention._relative_positions_bucket", "values.reshape.reshape.size", "values.reshape.reshape.size", "attn_weights.size", "attn_weights.size", "attn_weights.size", "attn_weights.size", "values.reshape.reshape.size", "attn_weights.size", "attn_weights.size", "ngram_multihead_attention.NgramMultiheadAttention.repeat", "ngram_multihead_attention.NgramMultiheadAttention.view", "torch.arange().unsqueeze().unsqueeze().repeat", "torch.arange().unsqueeze().unsqueeze().repeat", "torch.arange().unsqueeze().unsqueeze().repeat", "torch.arange().unsqueeze().unsqueeze().repeat", "real_positions.unsqueeze().repeat.unsqueeze().repeat.unsqueeze", "ngram_multihead_attention.NgramMultiheadAttention.size", "torch.arange().unsqueeze().unsqueeze", "torch.arange().unsqueeze().unsqueeze", "torch.arange().unsqueeze().unsqueeze", "torch.arange().unsqueeze().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange", "torch.arange", "torch.arange", "torch.arange"], "methods", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.distribute.size", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.distribute.size", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_multihead_attention.NgramMultiheadAttention._relative_positions_bucket", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.distribute.size", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.distribute.size", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.distribute.size", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.distribute.size", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.distribute.size", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.distribute.size", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.distribute.size", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.distribute.size", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.distribute.size", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.distribute.size"], ["", "def", "main_stream_relative_logits", "(", "self", ",", "query", ",", "attn_weights", ",", "real_positions", ",", "i_bucket_main_stream", ")", ":", "\n", "# input query [T,B,C]", "\n", "# input attn_weights [T*head,T,S]", "\n", "# input real_positions [B,T] or [1,1]", "\n", "\n", "        ", "T", ",", "B", ",", "_", "=", "query", ".", "size", "(", ")", "\n", "S", "=", "attn_weights", ".", "size", "(", "-", "1", ")", "\n", "\n", "if", "i_bucket_main_stream", "is", "not", "None", ":", "\n", "            ", "i_buckets", "=", "i_bucket_main_stream", "\n", "", "else", ":", "\n", "# [B,T,S]", "\n", "            ", "relative_positions", "=", "torch", ".", "arange", "(", "1", ",", "S", "+", "1", ")", ".", "unsqueeze", "(", "0", ")", ".", "unsqueeze", "(", "0", ")", ".", "repeat", "(", "B", ",", "T", ",", "1", ")", ".", "to", "(", "real_positions", ".", "device", ")", "\n", "# [B,T,1]", "\n", "real_positions", "=", "real_positions", ".", "unsqueeze", "(", "0", ")", ".", "repeat", "(", "B", ",", "T", ",", "1", ")", "\n", "# [B,T,S]", "\n", "relative_positions", "=", "relative_positions", "-", "real_positions", "\n", "# [B,T,T]", "\n", "i_buckets", "=", "self", ".", "_relative_positions_bucket", "(", "relative_positions", ",", "False", ")", "\n", "\n", "# [B,T,C]", "\n", "", "query", "=", "query", ".", "transpose", "(", "0", ",", "1", ")", "\n", "# [B,T,Buckets*head]", "\n", "values", "=", "self", ".", "relative_linear", "(", "query", ")", "\n", "# [B,T,Buckets,head]", "\n", "values", "=", "values", ".", "view", "(", "values", ".", "size", "(", "0", ")", ",", "values", ".", "size", "(", "1", ")", ",", "self", ".", "num_buckets", ",", "self", ".", "num_heads", ")", "\n", "# [B,head,Buckets,T]", "\n", "values", "=", "values", ".", "transpose", "(", "1", ",", "3", ")", "\n", "# [B,head,T,Buckets]", "\n", "values", "=", "values", ".", "transpose", "(", "2", ",", "3", ")", "\n", "# [B*head,T,Buckets]", "\n", "values", "=", "values", ".", "reshape", "(", "attn_weights", ".", "size", "(", "0", ")", ",", "attn_weights", ".", "size", "(", "1", ")", ",", "-", "1", ")", "\n", "\n", "# => [B,head*T,T] => [B*head,T,T]", "\n", "i_buckets", "=", "i_buckets", ".", "repeat", "(", "1", ",", "self", ".", "num_heads", ",", "1", ")", ".", "view", "(", "attn_weights", ".", "size", "(", "0", ")", ",", "attn_weights", ".", "size", "(", "1", ")", ",", "-", "1", ")", "\n", "# [B*head*T,Buckets]", "\n", "values", "=", "values", ".", "reshape", "(", "-", "1", ",", "values", ".", "size", "(", "-", "1", ")", ")", "\n", "# [B*head*T,T]", "\n", "i_buckets", "=", "i_buckets", ".", "view", "(", "-", "1", ",", "i_buckets", ".", "size", "(", "-", "1", ")", ")", ".", "long", "(", ")", "\n", "# [B*head*T,T]", "\n", "result", "=", "torch", ".", "gather", "(", "values", ",", "dim", "=", "1", ",", "index", "=", "i_buckets", ")", "\n", "# [B*head,T,T]", "\n", "result", "=", "result", ".", "view", "(", "attn_weights", ".", "size", "(", "0", ")", ",", "attn_weights", ".", "size", "(", "1", ")", ",", "-", "1", ")", "\n", "\n", "return", "result", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet_decoding_step_ss_mle_absolute.ngram_multihead_attention.NgramMultiheadAttention.ngram_relative_logits": [[177, 222], ["query.transpose.transpose.size", "attn_weights.size", "query.transpose.transpose.transpose", "ngram_multihead_attention.NgramMultiheadAttention.relative_linear", "values.reshape.reshape.view", "values.reshape.reshape.permute", "values.reshape.reshape.reshape", "ngram_multihead_attention.NgramMultiheadAttention.unsqueeze().repeat", "values.reshape.reshape.reshape", "ngram_multihead_attention.NgramMultiheadAttention.view().long", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "result.view.view.view", "torch.arange().unsqueeze().unsqueeze().repeat().to", "torch.arange().unsqueeze().unsqueeze().repeat().to", "torch.arange().unsqueeze().unsqueeze().repeat().to", "torch.arange().unsqueeze().unsqueeze().repeat().to", "real_positions.unsqueeze().repeat.unsqueeze().repeat.unsqueeze().repeat", "ngram_multihead_attention.NgramMultiheadAttention._relative_positions_bucket", "values.reshape.reshape.size", "ngram_multihead_attention.NgramMultiheadAttention.unsqueeze", "ngram_multihead_attention.NgramMultiheadAttention.view", "torch.arange().unsqueeze().unsqueeze().repeat", "torch.arange().unsqueeze().unsqueeze().repeat", "torch.arange().unsqueeze().unsqueeze().repeat", "torch.arange().unsqueeze().unsqueeze().repeat", "real_positions.unsqueeze().repeat.unsqueeze().repeat.unsqueeze", "values.reshape.reshape.size", "ngram_multihead_attention.NgramMultiheadAttention.size", "torch.arange().unsqueeze().unsqueeze", "torch.arange().unsqueeze().unsqueeze", "torch.arange().unsqueeze().unsqueeze", "torch.arange().unsqueeze().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange", "torch.arange", "torch.arange", "torch.arange"], "methods", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.distribute.size", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.distribute.size", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_multihead_attention.NgramMultiheadAttention._relative_positions_bucket", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.distribute.size", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.distribute.size", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.distribute.size"], ["", "def", "ngram_relative_logits", "(", "self", ",", "query", ",", "attn_weights", ",", "real_positions", ",", "i_bucket_relative_stream", ")", ":", "\n", "# input query [ngram, T,B,C]", "\n", "# input attn_weights [ngram, B*head,T,S]", "\n", "# input real_positions [B,T] or [1,1]", "\n", "# input i_bucket_relative_stream [B,T, 2*T] or None", "\n", "\n", "        ", "N", ",", "T", ",", "B", ",", "_", "=", "query", ".", "size", "(", ")", "\n", "_", ",", "BH", ",", "_", ",", "S", "=", "attn_weights", ".", "size", "(", ")", "\n", "\n", "if", "i_bucket_relative_stream", "is", "not", "None", ":", "\n", "            ", "i_buckets", "=", "i_bucket_relative_stream", "\n", "", "else", ":", "\n", "# [B,T,S]", "\n", "            ", "assert", "real_positions", "[", "0", "]", "[", "0", "]", "==", "S", "-", "1", ",", "'memory position is 1 2 3 4 5(S-1)'", "\n", "relative_positions", "=", "torch", ".", "arange", "(", "0", ",", "S", ")", ".", "unsqueeze", "(", "0", ")", ".", "unsqueeze", "(", "0", ")", ".", "repeat", "(", "B", ",", "T", ",", "1", ")", ".", "to", "(", "real_positions", ".", "device", ")", "\n", "# print('relative_positions', relative_positions)", "\n", "# [B,T,1]", "\n", "real_positions", "=", "real_positions", ".", "unsqueeze", "(", "0", ")", ".", "repeat", "(", "B", ",", "T", ",", "1", ")", "\n", "relative_positions", "=", "relative_positions", "\n", "# [B,T,2*T] or [B,T,S]", "\n", "relative_positions", "=", "relative_positions", "-", "real_positions", "\n", "i_buckets", "=", "self", ".", "_relative_positions_bucket", "(", "relative_positions", ",", "False", ")", "\n", "\n", "# [ngram, B, T, C]", "\n", "", "query", "=", "query", ".", "transpose", "(", "1", ",", "2", ")", "\n", "# [ngram, B, T, bucket*head]", "\n", "values", "=", "self", ".", "relative_linear", "(", "query", ")", "\n", "# [ngram, B, T, bucket, head]", "\n", "values", "=", "values", ".", "view", "(", "*", "values", ".", "size", "(", ")", "[", ":", "-", "1", "]", ",", "self", ".", "num_buckets", ",", "self", ".", "num_heads", ")", "\n", "# [ngram, B, head, T, bucket]", "\n", "values", "=", "values", ".", "permute", "(", "0", ",", "1", ",", "4", ",", "2", ",", "3", ")", "\n", "# [ngram*B*head, T, bucket]", "\n", "values", "=", "values", ".", "reshape", "(", "N", "*", "BH", ",", "T", ",", "-", "1", ")", "\n", "\n", "# [ngram, B, head*T, S]", "\n", "i_buckets", "=", "i_buckets", ".", "unsqueeze", "(", "0", ")", ".", "repeat", "(", "N", ",", "1", ",", "self", ".", "num_heads", ",", "1", ")", "\n", "\n", "values", "=", "values", ".", "reshape", "(", "-", "1", ",", "values", ".", "size", "(", "-", "1", ")", ")", "\n", "i_buckets", "=", "i_buckets", ".", "view", "(", "-", "1", ",", "i_buckets", ".", "size", "(", "-", "1", ")", ")", ".", "long", "(", ")", "\n", "# [ngram*B*head*T, S]", "\n", "result", "=", "torch", ".", "gather", "(", "values", ",", "dim", "=", "1", ",", "index", "=", "i_buckets", ")", "\n", "# [ngram, B*head, T, S]", "\n", "result", "=", "result", ".", "view", "(", "N", ",", "BH", ",", "T", ",", "-", "1", ")", "\n", "\n", "return", "result", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet_decoding_step_ss_mle_absolute.ngram_multihead_attention.NgramMultiheadAttention.forward": [[223, 365], ["query.size", "ngram_multihead_attention.NgramMultiheadAttention.in_proj_qkv", "q.contiguous().view().transpose.contiguous().view().transpose.contiguous().view().transpose", "query.chunk", "q.contiguous().view().transpose.contiguous().view().transpose.chunk", "k.contiguous().view().transpose.contiguous().view().transpose.chunk", "v.contiguous().view().transpose.contiguous().view().transpose.chunk", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "ngram_multihead_attention.NgramMultiheadAttention.main_stream_relative_logits", "fairseq.utils.softmax().type_as", "torch.dropout", "torch.dropout", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "ngram_multihead_attention.NgramMultiheadAttention.transpose().contiguous().view", "ngram_multihead_attention.NgramMultiheadAttention.out_proj", "torch.cat().view", "torch.cat().view", "torch.cat().view", "torch.cat().view", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat().view", "torch.cat().view", "torch.cat().view", "torch.cat().view", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "ngram_multihead_attention.NgramMultiheadAttention.ngram_relative_logits", "fairseq.utils.softmax().type_as", "torch.dropout", "torch.dropout", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "ngram_multihead_attention.NgramMultiheadAttention.transpose().contiguous().view", "ngram_multihead_attention.NgramMultiheadAttention.out_proj", "attn_result.append", "attn_result.append", "torch.cat().view", "torch.cat().view", "torch.cat().view", "torch.cat().view", "list", "ngram_multihead_attention.NgramMultiheadAttention._get_input_buffer", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "k.contiguous().view().transpose.contiguous().view().transpose.contiguous().view().transpose", "v.contiguous().view().transpose.contiguous().view().transpose.contiguous().view().transpose", "torch.cat.view", "torch.cat.view", "torch.cat.view", "torch.cat.view", "ngram_multihead_attention.NgramMultiheadAttention._set_input_buffer", "torch.cat.transpose", "torch.cat.transpose", "ngram_multihead_attention.NgramMultiheadAttention.unsqueeze", "ngram_mask_matrix.unsqueeze.unsqueeze.unsqueeze", "query.size", "q.contiguous().view().transpose.contiguous().view().transpose.contiguous().view", "saved_state[].view", "saved_state[].view", "fairseq.utils.softmax", "ngram_multihead_attention.NgramMultiheadAttention.transpose().contiguous", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat().unsqueeze", "torch.cat().unsqueeze", "torch.cat().unsqueeze", "torch.cat().unsqueeze", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat().unsqueeze", "torch.cat().unsqueeze", "torch.cat().unsqueeze", "torch.cat().unsqueeze", "fairseq.utils.softmax", "ngram_multihead_attention.NgramMultiheadAttention.transpose().contiguous", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "ngram_multihead_attention.NgramMultiheadAttention.bias_k.repeat", "ngram_multihead_attention.NgramMultiheadAttention.bias_v.repeat", "k.contiguous().view().transpose.contiguous().view().transpose.contiguous().view", "v.contiguous().view().transpose.contiguous().view().transpose.contiguous().view", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "q.contiguous().view().transpose.contiguous().view().transpose.contiguous", "ngram_multihead_attention.NgramMultiheadAttention.transpose", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "ngram_multihead_attention.NgramMultiheadAttention.transpose", "k.contiguous().view().transpose.contiguous().view().transpose.contiguous", "v.contiguous().view().transpose.contiguous().view().transpose.contiguous"], "methods", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.distribute.size", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_multihead_attention.NgramMultiheadAttention.in_proj_qkv", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_multihead_attention.NgramMultiheadAttention.main_stream_relative_logits", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_multihead_attention.NgramMultiheadAttention.ngram_relative_logits", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_multihead_attention.NgramMultiheadAttention._get_input_buffer", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_multihead_attention.NgramMultiheadAttention._set_input_buffer", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.distribute.size"], ["", "def", "forward", "(", "self", ",", "query", ",", "key", ",", "value", ",", "key_padding_mask", "=", "None", ",", "incremental_state", "=", "None", ",", "\n", "need_weights", "=", "True", ",", "static_kv", "=", "False", ",", "\n", "self_attn_mask", "=", "None", ",", "\n", "ngram_mask_matrix", "=", "None", ",", "\n", "i_buckets_main_stream", "=", "None", ",", "\n", "i_bucket_relative_stream", "=", "None", ",", "\n", "real_positions", "=", "None", "\n", ")", ":", "\n", "        ", "\"\"\"Input shape: Time x Batch x Channel\n\n        Timesteps can be masked by supplying a T x T mask in the\n        `attn_mask` argument. Padding elements can be excluded from\n        the key by passing a binary ByteTensor (`key_padding_mask`) with shape:\n        batch x src_len, where padding elements are indicated by 1s.\n        \"\"\"", "\n", "\n", "tgt_len", ",", "bsz", ",", "embed_dim", "=", "query", ".", "size", "(", ")", "\n", "assert", "embed_dim", "==", "self", ".", "embed_dim", "\n", "assert", "list", "(", "query", ".", "size", "(", ")", ")", "==", "[", "tgt_len", ",", "bsz", ",", "embed_dim", "]", "\n", "\n", "if", "incremental_state", "is", "not", "None", ":", "\n", "            ", "saved_state", "=", "self", ".", "_get_input_buffer", "(", "incremental_state", ")", "\n", "if", "'prev_key'", "in", "saved_state", ":", "\n", "# previous time steps are cached - no need to recompute", "\n", "# key and value if they are static", "\n", "                ", "if", "static_kv", ":", "\n", "                    ", "assert", "self", ".", "encoder_decoder_attention", "and", "not", "self", ".", "self_attention", "\n", "key", "=", "value", "=", "None", "\n", "", "", "", "else", ":", "\n", "            ", "saved_state", "=", "None", "\n", "\n", "", "q", ",", "k", ",", "v", "=", "self", ".", "in_proj_qkv", "(", "query", ")", "\n", "q", "*=", "self", ".", "scaling", "\n", "\n", "if", "self", ".", "bias_k", "is", "not", "None", ":", "\n", "            ", "assert", "self", ".", "bias_v", "is", "not", "None", "\n", "k", "=", "torch", ".", "cat", "(", "[", "k", ",", "self", ".", "bias_k", ".", "repeat", "(", "1", ",", "bsz", ",", "1", ")", "]", ")", "\n", "v", "=", "torch", ".", "cat", "(", "[", "v", ",", "self", ".", "bias_v", ".", "repeat", "(", "1", ",", "bsz", ",", "1", ")", "]", ")", "\n", "", "q", "=", "q", ".", "contiguous", "(", ")", ".", "view", "(", "tgt_len", ",", "bsz", "*", "self", ".", "num_heads", ",", "self", ".", "head_dim", ")", ".", "transpose", "(", "0", ",", "1", ")", "\n", "if", "k", "is", "not", "None", ":", "\n", "            ", "k", "=", "k", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ",", "bsz", "*", "self", ".", "num_heads", ",", "self", ".", "head_dim", ")", ".", "transpose", "(", "0", ",", "1", ")", "\n", "", "if", "v", "is", "not", "None", ":", "\n", "            ", "v", "=", "v", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ",", "bsz", "*", "self", ".", "num_heads", ",", "self", ".", "head_dim", ")", ".", "transpose", "(", "0", ",", "1", ")", "\n", "\n", "# h: \u8f93\u5165\u7684\u9690\u72b6\u6001", "\n", "", "h_list", "=", "query", ".", "chunk", "(", "1", "+", "self", ".", "ngram", ",", "dim", "=", "0", ")", "\n", "\n", "q_list", "=", "q", ".", "chunk", "(", "1", "+", "self", ".", "ngram", ",", "dim", "=", "1", ")", "\n", "k_list", "=", "k", ".", "chunk", "(", "1", "+", "self", ".", "ngram", ",", "dim", "=", "1", ")", "\n", "v_list", "=", "v", ".", "chunk", "(", "1", "+", "self", ".", "ngram", ",", "dim", "=", "1", ")", "\n", "\n", "h_main", ",", "h_predict_list", "=", "h_list", "[", "0", "]", ",", "h_list", "[", "1", ":", "]", "\n", "q_main", ",", "q_predict_list", "=", "q_list", "[", "0", "]", ",", "q_list", "[", "1", ":", "]", "\n", "k_main", ",", "k_predict_list", "=", "k_list", "[", "0", "]", ",", "k_list", "[", "1", ":", "]", "\n", "v_main", ",", "v_predict_list", "=", "v_list", "[", "0", "]", ",", "v_list", "[", "1", ":", "]", "\n", "\n", "if", "saved_state", "is", "not", "None", ":", "\n", "# saved states are stored with shape (bsz, num_heads, seq_len, head_dim)", "\n", "            ", "if", "'prev_key'", "in", "saved_state", ":", "\n", "                ", "prev_key", "=", "saved_state", "[", "'prev_key'", "]", ".", "view", "(", "bsz", "*", "self", ".", "num_heads", ",", "-", "1", ",", "self", ".", "head_dim", ")", "\n", "if", "static_kv", ":", "\n", "                    ", "assert", "False", ",", "'static_kv not supprt in ngram decoder'", "\n", "k", "=", "prev_key", "\n", "", "else", ":", "\n", "                    ", "k_main", "=", "torch", ".", "cat", "(", "(", "prev_key", ",", "k_main", ")", ",", "dim", "=", "1", ")", "\n", "", "", "if", "'prev_value'", "in", "saved_state", ":", "\n", "                ", "prev_value", "=", "saved_state", "[", "'prev_value'", "]", ".", "view", "(", "bsz", "*", "self", ".", "num_heads", ",", "-", "1", ",", "self", ".", "head_dim", ")", "\n", "if", "static_kv", ":", "\n", "                    ", "v", "=", "prev_value", "\n", "", "else", ":", "\n", "                    ", "v_main", "=", "torch", ".", "cat", "(", "(", "prev_value", ",", "v_main", ")", ",", "dim", "=", "1", ")", "\n", "", "", "saved_state", "[", "'prev_key'", "]", "=", "k_main", ".", "view", "(", "bsz", ",", "self", ".", "num_heads", ",", "-", "1", ",", "self", ".", "head_dim", ")", "\n", "saved_state", "[", "'prev_value'", "]", "=", "v_main", ".", "view", "(", "bsz", ",", "self", ".", "num_heads", ",", "-", "1", ",", "self", ".", "head_dim", ")", "\n", "\n", "self", ".", "_set_input_buffer", "(", "incremental_state", ",", "saved_state", ")", "\n", "\n", "", "real_tgt_len", "=", "tgt_len", "//", "(", "1", "+", "self", ".", "ngram", ")", "\n", "\n", "attn_weights_main", "=", "torch", ".", "bmm", "(", "q_main", ",", "k_main", ".", "transpose", "(", "1", ",", "2", ")", ")", "\n", "\n", "main_relative_logits", "=", "self", ".", "main_stream_relative_logits", "(", "h_main", ",", "attn_weights_main", ",", "real_positions", ",", "i_buckets_main_stream", ")", "\n", "attn_weights_main", "=", "attn_weights_main", "+", "main_relative_logits", "\n", "\n", "if", "self_attn_mask", "is", "not", "None", ":", "\n", "            ", "self_attn_mask", "=", "self_attn_mask", ".", "unsqueeze", "(", "0", ")", "\n", "attn_weights_main", "=", "attn_weights_main", "+", "self_attn_mask", "\n", "\n", "", "attn_weights_main", "=", "utils", ".", "softmax", "(", "\n", "attn_weights_main", ",", "dim", "=", "-", "1", ",", "onnx_trace", "=", "self", ".", "onnx_trace", ",", "\n", ")", ".", "type_as", "(", "attn_weights_main", ")", "\n", "attn_weights_main", "=", "F", ".", "dropout", "(", "attn_weights_main", ",", "p", "=", "self", ".", "dropout", ",", "training", "=", "self", ".", "training", ")", "\n", "\n", "attn_main", "=", "torch", ".", "bmm", "(", "attn_weights_main", ",", "v_main", ")", "\n", "attn_main", "=", "attn_main", ".", "transpose", "(", "0", ",", "1", ")", ".", "contiguous", "(", ")", ".", "view", "(", "1", ",", "real_tgt_len", ",", "bsz", ",", "embed_dim", ")", "\n", "attn_main", "=", "self", ".", "out_proj", "(", "attn_main", ")", "\n", "\n", "\n", "# [ngram, B*head, T, c]", "\n", "q_ngram", "=", "torch", ".", "cat", "(", "q_predict_list", ",", "0", ")", ".", "view", "(", "self", ".", "ngram", ",", "-", "1", ",", "real_tgt_len", ",", "self", ".", "head_dim", ")", "\n", "# [ngram, B*head, 2*T, c]", "\n", "k_ngram", "=", "torch", ".", "cat", "(", "[", "torch", ".", "cat", "(", "[", "k_main", ",", "k_p", "]", ",", "1", ")", ".", "unsqueeze", "(", "0", ")", "for", "k_p", "in", "k_predict_list", "]", ",", "0", ")", "\n", "# below code slower than above for loop", "\n", "# k_ngram = torch.cat([k_main.unsqueeze(0).repeat(self.ngram, 1, 1, 1) , torch.cat(k_predict_list).view(self.ngram, -1, real_tgt_len, self.head_dim)], 2)", "\n", "\n", "# [ngram, T, B, C]", "\n", "h_ngram", "=", "torch", ".", "cat", "(", "h_predict_list", ",", "0", ")", ".", "view", "(", "self", ".", "ngram", ",", "real_tgt_len", ",", "bsz", ",", "embed_dim", ")", "\n", "\n", "# [ngram, B*head, 2*T, c]", "\n", "v_ngram", "=", "torch", ".", "cat", "(", "[", "torch", ".", "cat", "(", "[", "v_main", ",", "v_p", "]", ",", "1", ")", ".", "unsqueeze", "(", "0", ")", "for", "v_p", "in", "v_predict_list", "]", ",", "0", ")", "\n", "# below code slower than above for loop", "\n", "# v_ngram = torch.cat([v_main.unsqueeze(0).repeat(self.ngram, 1, 1, 1) , torch.cat(v_predict_list).view(self.ngram, -1, real_tgt_len, self.head_dim)], 2)", "\n", "\n", "# [ngram, B*head, T, 2*T]", "\n", "attn_weights_ngram", "=", "torch", ".", "einsum", "(", "'nbtc,nbsc->nbts'", ",", "(", "q_ngram", ",", "k_ngram", ")", ")", "\n", "\n", "# [ngram, B*head, T, S]", "\n", "predict_relative_logits", "=", "self", ".", "ngram_relative_logits", "(", "h_ngram", ",", "attn_weights_ngram", ",", "real_positions", ",", "i_bucket_relative_stream", ")", "\n", "# [ngram, B*head, T, 2*T]", "\n", "attn_weights_ngram", "=", "attn_weights_ngram", "+", "predict_relative_logits", "\n", "\n", "if", "ngram_mask_matrix", "is", "not", "None", ":", "\n", "            ", "ngram_mask_matrix", "=", "ngram_mask_matrix", ".", "unsqueeze", "(", "1", ")", "\n", "attn_weights_ngram", "=", "attn_weights_ngram", "+", "ngram_mask_matrix", "\n", "\n", "", "attn_weights_ngram", "=", "utils", ".", "softmax", "(", "\n", "attn_weights_ngram", ",", "dim", "=", "-", "1", ",", "onnx_trace", "=", "self", ".", "onnx_trace", ",", "\n", ")", ".", "type_as", "(", "attn_weights_ngram", ")", "\n", "attn_weights_ngram", "=", "F", ".", "dropout", "(", "attn_weights_ngram", ",", "p", "=", "self", ".", "dropout", ",", "training", "=", "self", ".", "training", ")", "\n", "\n", "# [ngram, B*head, T, c]", "\n", "attn_ngram", "=", "torch", ".", "einsum", "(", "'nbts,nbsc->nbtc'", ",", "(", "attn_weights_ngram", ",", "v_ngram", ")", ")", "\n", "# [ngram, T, B, C]", "\n", "attn_ngram", "=", "attn_ngram", ".", "transpose", "(", "1", ",", "2", ")", ".", "contiguous", "(", ")", ".", "view", "(", "self", ".", "ngram", ",", "real_tgt_len", ",", "bsz", ",", "embed_dim", ")", "\n", "attn_ngram", "=", "self", ".", "out_proj", "(", "attn_ngram", ")", "\n", "\n", "attn_result", "=", "[", "]", "\n", "attn_result", ".", "append", "(", "attn_main", ")", "\n", "attn_result", ".", "append", "(", "attn_ngram", ")", "\n", "\n", "# [1+ngram*T, B, C]", "\n", "attn", "=", "torch", ".", "cat", "(", "attn_result", ",", "0", ")", ".", "view", "(", "-", "1", ",", "bsz", ",", "embed_dim", ")", "\n", "return", "attn", ",", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet_decoding_step_ss_mle_absolute.ngram_multihead_attention.NgramMultiheadAttention.in_proj_qkv": [[366, 368], ["ngram_multihead_attention.NgramMultiheadAttention._in_proj().chunk", "ngram_multihead_attention.NgramMultiheadAttention._in_proj"], "methods", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_multihead_attention.NgramMultiheadAttention._in_proj"], ["", "def", "in_proj_qkv", "(", "self", ",", "query", ")", ":", "\n", "        ", "return", "self", ".", "_in_proj", "(", "query", ")", ".", "chunk", "(", "3", ",", "dim", "=", "-", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet_decoding_step_ss_mle_absolute.ngram_multihead_attention.NgramMultiheadAttention.in_proj_q": [[369, 377], ["ngram_multihead_attention.NgramMultiheadAttention._in_proj", "torch.linear", "torch.linear"], "methods", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_multihead_attention.NgramMultiheadAttention._in_proj", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.layers.nn.linear", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.layers.nn.linear"], ["", "def", "in_proj_q", "(", "self", ",", "query", ")", ":", "\n", "        ", "if", "self", ".", "qkv_same_dim", ":", "\n", "            ", "return", "self", ".", "_in_proj", "(", "query", ",", "end", "=", "self", ".", "embed_dim", ")", "\n", "", "else", ":", "\n", "            ", "bias", "=", "self", ".", "in_proj_bias", "\n", "if", "bias", "is", "not", "None", ":", "\n", "                ", "bias", "=", "bias", "[", ":", "self", ".", "embed_dim", "]", "\n", "", "return", "F", ".", "linear", "(", "query", ",", "self", ".", "q_proj_weight", ",", "bias", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet_decoding_step_ss_mle_absolute.ngram_multihead_attention.NgramMultiheadAttention.in_proj_k": [[378, 387], ["ngram_multihead_attention.NgramMultiheadAttention._in_proj", "torch.linear", "torch.linear"], "methods", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_multihead_attention.NgramMultiheadAttention._in_proj", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.layers.nn.linear", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.layers.nn.linear"], ["", "", "def", "in_proj_k", "(", "self", ",", "key", ")", ":", "\n", "        ", "if", "self", ".", "qkv_same_dim", ":", "\n", "            ", "return", "self", ".", "_in_proj", "(", "key", ",", "start", "=", "self", ".", "embed_dim", ",", "end", "=", "2", "*", "self", ".", "embed_dim", ")", "\n", "", "else", ":", "\n", "            ", "weight", "=", "self", ".", "k_proj_weight", "\n", "bias", "=", "self", ".", "in_proj_bias", "\n", "if", "bias", "is", "not", "None", ":", "\n", "                ", "bias", "=", "bias", "[", "self", ".", "embed_dim", ":", "2", "*", "self", ".", "embed_dim", "]", "\n", "", "return", "F", ".", "linear", "(", "key", ",", "weight", ",", "bias", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet_decoding_step_ss_mle_absolute.ngram_multihead_attention.NgramMultiheadAttention.in_proj_v": [[388, 397], ["ngram_multihead_attention.NgramMultiheadAttention._in_proj", "torch.linear", "torch.linear"], "methods", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_multihead_attention.NgramMultiheadAttention._in_proj", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.layers.nn.linear", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.layers.nn.linear"], ["", "", "def", "in_proj_v", "(", "self", ",", "value", ")", ":", "\n", "        ", "if", "self", ".", "qkv_same_dim", ":", "\n", "            ", "return", "self", ".", "_in_proj", "(", "value", ",", "start", "=", "2", "*", "self", ".", "embed_dim", ")", "\n", "", "else", ":", "\n", "            ", "weight", "=", "self", ".", "v_proj_weight", "\n", "bias", "=", "self", ".", "in_proj_bias", "\n", "if", "bias", "is", "not", "None", ":", "\n", "                ", "bias", "=", "bias", "[", "2", "*", "self", ".", "embed_dim", ":", "]", "\n", "", "return", "F", ".", "linear", "(", "value", ",", "weight", ",", "bias", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet_decoding_step_ss_mle_absolute.ngram_multihead_attention.NgramMultiheadAttention._in_proj": [[398, 405], ["torch.linear", "torch.linear"], "methods", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.layers.nn.linear", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.layers.nn.linear"], ["", "", "def", "_in_proj", "(", "self", ",", "input", ",", "start", "=", "0", ",", "end", "=", "None", ")", ":", "\n", "        ", "weight", "=", "self", ".", "in_proj_weight", "\n", "bias", "=", "self", ".", "in_proj_bias", "\n", "weight", "=", "weight", "[", "start", ":", "end", ",", ":", "]", "\n", "if", "bias", "is", "not", "None", ":", "\n", "            ", "bias", "=", "bias", "[", "start", ":", "end", "]", "\n", "", "return", "F", ".", "linear", "(", "input", ",", "weight", ",", "bias", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet_decoding_step_ss_mle_absolute.ngram_multihead_attention.NgramMultiheadAttention.reorder_incremental_state": [[406, 413], ["ngram_multihead_attention.NgramMultiheadAttention._get_input_buffer", "ngram_multihead_attention.NgramMultiheadAttention.keys", "ngram_multihead_attention.NgramMultiheadAttention._set_input_buffer", "input_buffer[].index_select"], "methods", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_multihead_attention.NgramMultiheadAttention._get_input_buffer", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_multihead_attention.NgramMultiheadAttention._set_input_buffer"], ["", "def", "reorder_incremental_state", "(", "self", ",", "incremental_state", ",", "new_order", ")", ":", "\n", "        ", "\"\"\"Reorder buffered internal state (for incremental generation).\"\"\"", "\n", "input_buffer", "=", "self", ".", "_get_input_buffer", "(", "incremental_state", ")", "\n", "if", "input_buffer", "is", "not", "None", ":", "\n", "            ", "for", "k", "in", "input_buffer", ".", "keys", "(", ")", ":", "\n", "                ", "input_buffer", "[", "k", "]", "=", "input_buffer", "[", "k", "]", ".", "index_select", "(", "0", ",", "new_order", ")", "\n", "", "self", ".", "_set_input_buffer", "(", "incremental_state", ",", "input_buffer", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet_decoding_step_ss_mle_absolute.ngram_multihead_attention.NgramMultiheadAttention._get_input_buffer": [[414, 420], ["fairseq.utils.get_incremental_state"], "methods", ["None"], ["", "", "def", "_get_input_buffer", "(", "self", ",", "incremental_state", ")", ":", "\n", "        ", "return", "utils", ".", "get_incremental_state", "(", "\n", "self", ",", "\n", "incremental_state", ",", "\n", "'attn_state'", ",", "\n", ")", "or", "{", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet_decoding_step_ss_mle_absolute.ngram_multihead_attention.NgramMultiheadAttention._set_input_buffer": [[421, 427], ["fairseq.utils.set_incremental_state"], "methods", ["None"], ["", "def", "_set_input_buffer", "(", "self", ",", "incremental_state", ",", "buffer", ")", ":", "\n", "        ", "utils", ".", "set_incremental_state", "(", "\n", "self", ",", "\n", "incremental_state", ",", "\n", "'attn_state'", ",", "\n", "buffer", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet_decoding_step_ss_mle_absolute.ngram_multihead_attention.ngram_attention_bias": [[16, 30], ["range", "torch.from_numpy", "torch.from_numpy", "range", "bias_result.append", "numpy.array", "range", "bias_n_skip.append", "float"], "function", ["None"], ["def", "ngram_attention_bias", "(", "length", ",", "num_skip", ")", ":", "\n", "        ", "bias_result", "=", "[", "]", "\n", "for", "n_skip", "in", "range", "(", "num_skip", ")", ":", "\n", "            ", "bias_n_skip", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "length", ")", ":", "\n", "                ", "bias_this", "=", "[", "float", "(", "'-inf'", ")", "]", "*", "(", "2", "*", "length", ")", "\n", "bias_this", "[", "length", "+", "i", "]", "=", "0", "\n", "first_k", "=", "i", "-", "n_skip", "\n", "first_k", "=", "first_k", "if", "first_k", ">", "0", "else", "0", "\n", "for", "j", "in", "range", "(", "first_k", "+", "1", ")", ":", "\n", "                    ", "bias_this", "[", "j", "]", "=", "0", "\n", "", "bias_n_skip", ".", "append", "(", "bias_this", ")", "\n", "", "bias_result", ".", "append", "(", "bias_n_skip", ")", "\n", "", "return", "torch", ".", "from_numpy", "(", "np", ".", "array", "(", "bias_result", ",", "dtype", "=", "np", ".", "float32", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet_decoding_step_ss_mle_absolute.bert_dictionary.BertDictionary.__init__": [[20, 29], ["fairseq.data.Dictionary.__init__"], "methods", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.transformer_lrp.TransformerLRP.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "pad", "=", "'<pad>'", ",", "\n", "eos", "=", "'</s>'", ",", "\n", "unk", "=", "'<unk>'", ",", "\n", "bos", "=", "'<s>'", ",", "\n", "extra_special_symbols", "=", "None", ",", "\n", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "pad", ",", "eos", ",", "unk", ",", "bos", ",", "extra_special_symbols", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet_decoding_step_ss_mle_absolute.bert_dictionary.BertDictionary.load_from_file": [[30, 54], ["cls", "cls.add_symbol", "cls.add_symbol", "cls.add_symbol", "cls.add_symbol", "open", "line.split", "cls.add_symbol"], "methods", ["None"], ["", "@", "classmethod", "\n", "def", "load_from_file", "(", "cls", ",", "filename", ")", ":", "\n", "        ", "d", "=", "cls", "(", ")", "\n", "d", ".", "symbols", "=", "[", "]", "\n", "d", ".", "count", "=", "[", "]", "\n", "d", ".", "indices", "=", "{", "}", "\n", "\n", "with", "open", "(", "filename", ",", "'r'", ",", "encoding", "=", "'utf-8'", ",", "errors", "=", "'ignore'", ")", "as", "input_file", ":", "\n", "            ", "for", "line", "in", "input_file", ":", "\n", "                ", "k", ",", "v", "=", "line", ".", "split", "(", ")", "\n", "d", ".", "add_symbol", "(", "k", ")", "\n", "\n", "", "", "d", ".", "unk_word", "=", "'[UNK]'", "\n", "d", ".", "pad_word", "=", "'[PAD]'", "\n", "d", ".", "eos_word", "=", "'[SEP]'", "\n", "d", ".", "bos_word", "=", "'[CLS]'", "\n", "\n", "d", ".", "bos_index", "=", "d", ".", "add_symbol", "(", "'[CLS]'", ")", "\n", "d", ".", "pad_index", "=", "d", ".", "add_symbol", "(", "'[PAD]'", ")", "\n", "d", ".", "eos_index", "=", "d", ".", "add_symbol", "(", "'[SEP]'", ")", "\n", "d", ".", "unk_index", "=", "d", ".", "add_symbol", "(", "'[UNK]'", ")", "\n", "\n", "d", ".", "nspecial", "=", "999", "\n", "return", "d", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet_decoding_step_ss_mle_absolute.bert_dictionary.BertDictionary.save": [[55, 59], ["bert_dictionary.BertDictionary._get_meta", "bert_dictionary.BertDictionary._save", "zip"], "methods", ["None"], ["", "def", "save", "(", "self", ",", "f", ")", ":", "\n", "        ", "\"\"\"Stores dictionary into a text file\"\"\"", "\n", "ex_keys", ",", "ex_vals", "=", "self", ".", "_get_meta", "(", ")", "\n", "self", ".", "_save", "(", "f", ",", "zip", "(", "ex_keys", "+", "self", ".", "symbols", ",", "ex_vals", "+", "self", ".", "count", ")", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.translation.TranslationProphetnetTask.__init__": [[10, 12], ["fairseq.tasks.translation.TranslationTask.__init__"], "methods", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.transformer_lrp.TransformerLRP.__init__"], ["    ", "def", "__init__", "(", "self", ",", "args", ",", "src_dict", ",", "tgt_dict", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "args", ",", "src_dict", ",", "tgt_dict", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.translation.TranslationProphetnetTask.load_dictionary": [[13, 16], ["bert_dictionary.BertDictionary.load_from_file"], "methods", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.bert_dictionary.BertDictionary.load_from_file"], ["", "@", "classmethod", "\n", "def", "load_dictionary", "(", "cls", ",", "filename", ")", ":", "\n", "        ", "return", "BertDictionary", ".", "load_from_file", "(", "filename", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.translation.TranslationProphetnetTask.max_positions": [[17, 20], ["None"], "methods", ["None"], ["", "def", "max_positions", "(", "self", ")", ":", "\n", "        ", "\"\"\"Return the max sentence length allowed by the task.\"\"\"", "\n", "return", "(", "self", ".", "args", ".", "max_source_positions", ",", "self", ".", "args", ".", "max_target_positions", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.learned_positional_embedding.LearnedPositionalEmbedding.__init__": [[14, 22], ["torch.Embedding.__init__"], "methods", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.transformer_lrp.TransformerLRP.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "num_embeddings", ":", "int", ",", "\n", "embedding_dim", ":", "int", ",", "\n", "padding_idx", ":", "int", ",", "\n", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "num_embeddings", ",", "embedding_dim", ",", "padding_idx", ")", "\n", "self", ".", "onnx_trace", "=", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.learned_positional_embedding.LearnedPositionalEmbedding.forward": [[23, 42], ["super().forward", "input.data.new().fill_", "fairseq.utils.make_positions", "int", "input.data.new", "input.size"], "methods", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_multihead_attention.NgramMultiheadAttention.forward", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.distribute.size"], ["", "def", "forward", "(", "self", ",", "input", ",", "incremental_state", "=", "None", ",", "positions", "=", "None", ")", ":", "\n", "        ", "\"\"\"Input is expected to be of size [bsz x seqlen].\"\"\"", "\n", "assert", "(", "\n", "(", "positions", "is", "None", ")", "or", "(", "self", ".", "padding_idx", "is", "None", ")", "\n", ")", ",", "\"If positions is pre-computed then padding_idx should not be set.\"", "\n", "\n", "if", "positions", "is", "None", ":", "\n", "            ", "if", "incremental_state", "is", "not", "None", ":", "\n", "# positions is the same for every token when decoding a single step", "\n", "# Without the int() cast, it doesn't work in some cases when exporting to ONNX", "\n", "                ", "positions", "=", "input", ".", "data", ".", "new", "(", "1", ",", "1", ")", ".", "fill_", "(", "int", "(", "self", ".", "padding_idx", "+", "input", ".", "size", "(", "1", ")", ")", ")", "\n", "", "else", ":", "\n", "                ", "positions", "=", "utils", ".", "make_positions", "(", "\n", "input", ".", "data", ",", "self", ".", "padding_idx", ",", "onnx_trace", "=", "self", ".", "onnx_trace", ",", "\n", ")", "\n", "", "real_positions", "=", "positions", "\n", "", "else", ":", "\n", "            ", "real_positions", "=", "positions", "\n", "", "return", "super", "(", ")", ".", "forward", "(", "positions", ")", ",", "real_positions", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.learned_positional_embedding.LearnedPositionalEmbedding.max_positions": [[43, 49], ["None"], "methods", ["None"], ["", "def", "max_positions", "(", "self", ")", ":", "\n", "        ", "\"\"\"Maximum number of supported positions.\"\"\"", "\n", "if", "self", ".", "padding_idx", "is", "not", "None", ":", "\n", "            ", "return", "self", ".", "num_embeddings", "-", "self", ".", "padding_idx", "-", "1", "\n", "", "else", ":", "\n", "            ", "return", "self", ".", "num_embeddings", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.learned_positional_embedding.LearnedPositionalEmbedding._forward": [[50, 52], ["super().forward"], "methods", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_multihead_attention.NgramMultiheadAttention.forward"], ["", "", "def", "_forward", "(", "self", ",", "positions", ")", ":", "\n", "        ", "return", "super", "(", ")", ".", "forward", "(", "positions", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_criterions.NgramLmLoss.__init__": [[21, 25], ["fairseq.criterions.FairseqCriterion.__init__"], "methods", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.transformer_lrp.TransformerLRP.__init__"], ["def", "__init__", "(", "self", ",", "args", ",", "task", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "args", ",", "task", ")", "\n", "self", ".", "eps", "=", "args", ".", "label_smoothing", "\n", "self", ".", "disable_ngram_loss", "=", "args", ".", "disable_ngram_loss", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_criterions.NgramLmLoss.add_args": [[26, 34], ["parser.add_argument", "parser.add_argument"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "add_args", "(", "parser", ")", ":", "\n", "        ", "\"\"\"Add criterion-specific arguments to the parser.\"\"\"", "\n", "# fmt: off", "\n", "parser", ".", "add_argument", "(", "'--label-smoothing'", ",", "default", "=", "0.", ",", "type", "=", "float", ",", "metavar", "=", "'D'", ",", "\n", "help", "=", "'epsilon for label smoothing, 0 means no label smoothing'", ")", "\n", "parser", ".", "add_argument", "(", "'--disable-ngram-loss'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'only comput basic stat'", ")", "\n", "# fmt: on", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_criterions.NgramLmLoss.forward": [[36, 95], ["model.get_targets", "len", "model.get_targets.new_zeros().fill_", "range", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.log_softmax", "torch.log_softmax", "torch.nll_loss", "torch.nll_loss", "model.get_targets.ne().int().sum().item", "model", "torch.zeros_like().fill_", "torch.zeros_like().fill_", "torch.zeros_like().fill_", "torch.zeros_like().fill_", "torch.cat.view", "torch.cat.view", "model.get_targets.view", "model.get_targets.ne().view", "smooth_loss.sum.sum.sum", "model.get_targets.new_zeros", "torch.where", "torch.where", "torch.where", "torch.where", "torch.cat.size", "torch.cat.size", "torch.log_softmax.sum", "torch.log_softmax.size", "model.get_targets.ne().int().sum", "fairseq.utils.item", "model.get_targets.size", "model.get_targets.size", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "model.get_targets.ne", "model.get_targets.ne().int", "model.get_targets.ne"], "methods", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.distribute.size", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.distribute.size", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.distribute.size", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.distribute.size", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.distribute.size"], ["", "def", "forward", "(", "self", ",", "model", ",", "sample", ",", "reduce", "=", "True", ")", ":", "\n", "        ", "\"\"\"Compute the loss for the given sample.\n        Returns a tuple with three elements:\n        1) the loss\n        2) the sample size, which is used as the denominator for the gradient\n        3) logging outputs to display while training\n        \"\"\"", "\n", "# compute MLM loss", "\n", "#  [2, ngram, B, T, V]", "\n", "#print(sample.keys(), sample['net_input']['tgt_lengths'])", "\n", "logits_list", "=", "model", "(", "**", "sample", "[", "'net_input'", "]", ",", "return_all_hiddens", "=", "False", ")", "\n", "targets", "=", "model", ".", "get_targets", "(", "sample", ",", "[", "logits_list", "]", ")", "\n", "\n", "\n", "#ngram = len(logits_list)", "\n", "ngram", "=", "logits_list", ".", "size", "(", "1", ")", "\n", "# [B, ngram, T]", "\n", "expend_targets", "=", "targets", ".", "new_zeros", "(", "ngram", ",", "targets", ".", "size", "(", "0", ")", ",", "targets", ".", "size", "(", "1", ")", ")", ".", "fill_", "(", "self", ".", "padding_idx", ")", "\n", "for", "i", "in", "range", "(", "ngram", ")", ":", "\n", "            ", "if", "i", ">", "0", "and", "self", ".", "disable_ngram_loss", ":", "\n", "                ", "break", "\n", "\n", "", "padding_targets", "=", "torch", ".", "zeros_like", "(", "targets", ")", ".", "fill_", "(", "self", ".", "padding_idx", ")", "\n", "if", "'target_idx'", "in", "sample", ":", "\n", "                ", "expend_targets", "[", "i", ",", ":", ",", ":", "]", "=", "torch", ".", "where", "(", "sample", "[", "'target_idx'", "]", ">=", "i", ",", "targets", ",", "padding_targets", ")", "\n", "", "else", ":", "\n", "                ", "expend_targets", "[", "i", ",", ":", ",", ":", "]", "=", "targets", "\n", "# [B, ngram, T]", "\n", "", "", "targets", "=", "expend_targets", "\n", "\n", "#logits = torch.cat(logits_list, dim=0) #.view(ngram, *logits_list[0].size())", "\n", "logits", "=", "logits_list", "\n", "\n", "lprobs", "=", "F", ".", "log_softmax", "(", "\n", "logits", ".", "view", "(", "-", "1", ",", "logits", ".", "size", "(", "-", "1", ")", ")", ",", "\n", "dim", "=", "-", "1", ",", "\n", "dtype", "=", "torch", ".", "float32", ",", "\n", ")", "\n", "targets", "=", "targets", ".", "unsqueeze_", "(", "0", ")", ".", "repeat", "(", "[", "2", ",", "1", ",", "1", ",", "1", "]", ")", "\n", "loss", "=", "F", ".", "nll_loss", "(", "\n", "lprobs", ",", "\n", "targets", ".", "view", "(", "-", "1", ")", ",", "\n", "reduction", "=", "'sum'", ",", "\n", "ignore_index", "=", "self", ".", "padding_idx", ",", "\n", ")", "\n", "\n", "if", "self", ".", "eps", ">", "0.", ":", "\n", "            ", "smooth_loss", "=", "-", "lprobs", ".", "sum", "(", "dim", "=", "-", "1", ",", "keepdim", "=", "True", ")", "\n", "non_pad_mask", "=", "targets", ".", "ne", "(", "self", ".", "padding_idx", ")", ".", "view", "(", "-", "1", ")", "\n", "smooth_loss", "=", "smooth_loss", "[", "non_pad_mask", "]", "\n", "smooth_loss", "=", "smooth_loss", ".", "sum", "(", ")", "\n", "\n", "eps_i", "=", "self", ".", "eps", "/", "lprobs", ".", "size", "(", "-", "1", ")", "\n", "loss", "=", "(", "1.", "-", "self", ".", "eps", ")", "*", "loss", "+", "eps_i", "*", "smooth_loss", "\n", "\n", "", "sample_size", "=", "targets", ".", "ne", "(", "self", ".", "padding_idx", ")", ".", "int", "(", ")", ".", "sum", "(", ")", ".", "item", "(", ")", "\n", "\n", "logging_output", "=", "{", "\n", "'loss'", ":", "utils", ".", "item", "(", "loss", ".", "data", ")", "if", "reduce", "else", "loss", ".", "data", ",", "\n", "'ntokens'", ":", "sample", "[", "'ntokens'", "]", ",", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_criterions.NgramLmLoss.aggregate_logging_outputs": [[96, 111], ["sum", "sum", "sum", "sum", "log.get", "log.get", "log.get", "log.get", "math.log"], "methods", ["None"], ["'nsentences'", ":", "sample", "[", "'nsentences'", "]", ",", "\n", "'sample_size'", ":", "sample_size", ",", "\n", "}", "\n", "return", "loss", ",", "sample_size", ",", "logging_output", "\n", "\n", "", "@", "staticmethod", "\n", "def", "aggregate_logging_outputs", "(", "logging_outputs", ")", ":", "\n", "        ", "\"\"\"Aggregate logging outputs from data parallel training.\"\"\"", "\n", "loss", "=", "sum", "(", "log", ".", "get", "(", "'loss'", ",", "0", ")", "for", "log", "in", "logging_outputs", ")", "\n", "ntokens", "=", "sum", "(", "log", ".", "get", "(", "'ntokens'", ",", "0", ")", "for", "log", "in", "logging_outputs", ")", "\n", "nsentences", "=", "sum", "(", "log", ".", "get", "(", "'nsentences'", ",", "0", ")", "for", "log", "in", "logging_outputs", ")", "\n", "sample_size", "=", "sum", "(", "log", ".", "get", "(", "'sample_size'", ",", "0", ")", "for", "log", "in", "logging_outputs", ")", "\n", "\n", "agg_output", "=", "{", "\n", "'loss'", ":", "loss", "/", "sample_size", "/", "math", ".", "log", "(", "2", ")", ",", "\n", "'ntokens'", ":", "ntokens", ",", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_s2s_model.NgramTransformerProphetModel.__init__": [[40, 42], ["fairseq.models.FairseqEncoderDecoderModel.__init__"], "methods", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.transformer_lrp.TransformerLRP.__init__"], ["        ", "super", "(", ")", ".", "__init__", "(", "encoder", ",", "decoder", ")", "\n", "\n", "", "@", "staticmethod", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_s2s_model.NgramTransformerProphetModel.add_args": [[43, 88], ["parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "fairseq.utils.get_available_activation_fns"], "methods", ["None"], ["def", "add_args", "(", "parser", ")", ":", "\n", "        ", "\"\"\"Add model-specific arguments to the parser.\"\"\"", "\n", "parser", ".", "add_argument", "(", "'--ngram'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'num of predicting grams'", ")", "\n", "parser", ".", "add_argument", "(", "'--num_buckets'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'num of buckets for relative position'", ")", "\n", "parser", ".", "add_argument", "(", "'--relative_max_distance'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'num of bucket for relative position'", ")", "\n", "# fmt: off", "\n", "parser", ".", "add_argument", "(", "'--activation-fn'", ",", "\n", "choices", "=", "utils", ".", "get_available_activation_fns", "(", ")", ",", "\n", "help", "=", "'activation function to use'", ")", "\n", "parser", ".", "add_argument", "(", "'--dropout'", ",", "type", "=", "float", ",", "metavar", "=", "'D'", ",", "\n", "help", "=", "'dropout probability'", ")", "\n", "parser", ".", "add_argument", "(", "'--attention-dropout'", ",", "type", "=", "float", ",", "metavar", "=", "'D'", ",", "\n", "help", "=", "'dropout probability for attention weights'", ")", "\n", "parser", ".", "add_argument", "(", "'--activation-dropout'", ",", "type", "=", "float", ",", "metavar", "=", "'D'", ",", "\n", "help", "=", "'dropout probability after activation in FFN.'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--encoder-embed-dim'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'encoder embedding dimension'", ")", "\n", "parser", ".", "add_argument", "(", "'--encoder-ffn-embed-dim'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'encoder embedding dimension for FFN'", ")", "\n", "parser", ".", "add_argument", "(", "'--encoder-layers'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'num encoder layers'", ")", "\n", "parser", ".", "add_argument", "(", "'--encoder-attention-heads'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'num encoder attention heads'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--decoder-embed-dim'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'decoder embedding dimension'", ")", "\n", "parser", ".", "add_argument", "(", "'--decoder-ffn-embed-dim'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'decoder embedding dimension for FFN'", ")", "\n", "parser", ".", "add_argument", "(", "'--decoder-layers'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'num decoder layers'", ")", "\n", "parser", ".", "add_argument", "(", "'--decoder-attention-heads'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'num decoder attention heads'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--share-all-embeddings'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'share encoder, decoder and output embeddings'", "\n", "' (requires shared dictionary and embed dim)'", ")", "\n", "parser", ".", "add_argument", "(", "'--load-from-pretrained-model'", ",", "type", "=", "str", ",", "default", "=", "None", ",", "\n", "help", "=", "'Load from pretrained model'", ")", "\n", "parser", ".", "add_argument", "(", "'--load-sep'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'load pretrained [SEP] weight into [X_SEP]. ([SEP] used as eos in fine tuning)'", ")", "\n", "# fmt: on", "\n", "parser", ".", "add_argument", "(", "'--fixed_scheduled_sampling_rate'", ",", "type", "=", "float", ",", "default", "=", "0.5", ",", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_s2s_model.NgramTransformerProphetModel.get_normalized_probs": [[90, 101], ["hasattr", "ngram_s2s_model.NgramTransformerProphetModel.decoder.get_normalized_probs", "torch.is_tensor", "torch.is_tensor", "torch.is_tensor", "torch.is_tensor", "torch.is_tensor", "torch.is_tensor", "torch.is_tensor", "torch.is_tensor", "torch.is_tensor", "net_output.float", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.softmax", "torch.softmax", "torch.softmax"], "methods", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_s2s_model.NgramTransformerDecoder.get_normalized_probs"], ["parser", ".", "add_argument", "(", "'--trainstep_schduled_sampling_strategy'", ",", "type", "=", "str", ",", "default", "=", "\"linear\"", ",", "\n", "help", "=", "'trainstep_schduled_sampling_strategy linear or exp'", ")", "\n", "parser", ".", "add_argument", "(", "'--decodingstep_schduled_sampling_strategy'", ",", "type", "=", "str", ",", "default", "=", "\"linear\"", ",", "\n", "help", "=", "'decodingstep_schduled_sampling_strategy linear or exp'", ")", "\n", "parser", ".", "add_argument", "(", "'--exp_radix'", ",", "type", "=", "float", ",", "default", "=", "0.8", ",", "\n", "help", "=", "'trainstep_schduled_sampling_strategy linear or exp'", ")", "\n", "parser", ".", "add_argument", "(", "'--decodingstep_exp_radix'", ",", "type", "=", "float", ",", "default", "=", "0.99", ",", "\n", "help", "=", "'0.99 for 512'", ")", "\n", "parser", ".", "add_argument", "(", "'--decodingstep_sigmoid_k'", ",", "type", "=", "float", ",", "default", "=", "60", ",", "\n", "help", "=", "'60 for 512'", ")", "\n", "\n", "", "def", "get_normalized_probs", "(", "self", ",", "net_output", ",", "log_probs", ",", "sample", "=", "None", ")", ":", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_s2s_model.NgramTransformerProphetModel.build_model": [[102, 196], ["ngram_s2s_model.base_architecture", "ngram_s2s_model.TransformerEncoder", "ngram_s2s_model.NgramTransformerDecoder", "ngram_s2s_model.NgramTransformerProphetModel", "ngram_s2s_model.NgramTransformerProphetModel", "hasattr", "hasattr", "len", "dictionary.pad", "ngram_s2s_model.Embedding", "ngram_s2s_model.NgramTransformerProphetModel.build_model.build_embedding"], "methods", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_s2s_model.base_architecture", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_s2s_model.Embedding"], ["        ", "\"\"\"Get normalized probabilities (or log probs) from a net's output.\"\"\"", "\n", "if", "hasattr", "(", "self", ",", "'decoder'", ")", ":", "\n", "            ", "return", "self", ".", "decoder", ".", "get_normalized_probs", "(", "net_output", ",", "log_probs", ",", "sample", ")", "\n", "return", "self", ".", "decoder", ".", "get_normalized_probs", "(", "net_output", ",", "log_probs", ",", "sample", ")", "\n", "", "elif", "torch", ".", "is_tensor", "(", "net_output", ")", ":", "\n", "            ", "logits", "=", "net_output", ".", "float", "(", ")", "\n", "if", "log_probs", ":", "\n", "                ", "return", "F", ".", "log_softmax", "(", "logits", ",", "dim", "=", "-", "1", ")", "\n", "", "else", ":", "\n", "                ", "return", "F", ".", "softmax", "(", "logits", ",", "dim", "=", "-", "1", ")", "\n", "", "", "raise", "NotImplementedError", "\n", "\n", "", "@", "classmethod", "\n", "def", "build_model", "(", "cls", ",", "args", ",", "task", ")", ":", "\n", "        ", "\"\"\"Build a new model instance.\"\"\"", "\n", "cls", ".", "fixed_scheduled_sampling_rate", "=", "args", ".", "fixed_scheduled_sampling_rate", "\n", "cls", ".", "trainstep_schduled_sampling_strategy", "=", "args", ".", "trainstep_schduled_sampling_strategy", "\n", "cls", ".", "decodingstep_schduled_sampling_strategy", "=", "args", ".", "decodingstep_schduled_sampling_strategy", "\n", "cls", ".", "exp_radix", "=", "args", ".", "exp_radix", "\n", "cls", ".", "decodingstep_exp_radix", "=", "args", ".", "decodingstep_exp_radix", "\n", "cls", ".", "decodingstep_sigmoid_k", "=", "args", ".", "decodingstep_sigmoid_k", "\n", "#cls.sampling_golden_prob = 1.0 # default", "\n", "# make sure all arguments are present in older models", "\n", "base_architecture", "(", "args", ")", "\n", "\n", "if", "not", "hasattr", "(", "args", ",", "'max_source_positions'", ")", ":", "\n", "            ", "args", ".", "max_source_positions", "=", "DEFAULT_MAX_SOURCE_POSITIONS", "\n", "", "if", "not", "hasattr", "(", "args", ",", "'max_target_positions'", ")", ":", "\n", "            ", "args", ".", "max_target_positions", "=", "DEFAULT_MAX_TARGET_POSITIONS", "\n", "\n", "", "src_dict", ",", "tgt_dict", "=", "task", ".", "source_dictionary", ",", "task", ".", "target_dictionary", "\n", "\n", "def", "build_embedding", "(", "dictionary", ",", "embed_dim", ")", ":", "\n", "            ", "num_embeddings", "=", "len", "(", "dictionary", ")", "\n", "padding_idx", "=", "dictionary", ".", "pad", "(", ")", "\n", "emb", "=", "Embedding", "(", "num_embeddings", ",", "embed_dim", ",", "padding_idx", ")", "\n", "return", "emb", "\n", "\n", "", "if", "args", ".", "share_all_embeddings", ":", "\n", "            ", "if", "src_dict", "!=", "tgt_dict", ":", "\n", "                ", "raise", "ValueError", "(", "'--share-all-embeddings requires a joined dictionary'", ")", "\n", "", "if", "args", ".", "encoder_embed_dim", "!=", "args", ".", "decoder_embed_dim", ":", "\n", "                ", "raise", "ValueError", "(", "\n", "'--share-all-embeddings requires --encoder-embed-dim to match --decoder-embed-dim'", ")", "\n", "", "encoder_embed_tokens", "=", "build_embedding", "(", "\n", "src_dict", ",", "args", ".", "encoder_embed_dim", "\n", ")", "\n", "cls", ".", "decoder_embed_tokens", "=", "encoder_embed_tokens", "\n", "args", ".", "share_decoder_input_output_embed", "=", "True", "\n", "", "else", ":", "\n", "            ", "encoder_embed_tokens", "=", "build_embedding", "(", "\n", "src_dict", ",", "args", ".", "encoder_embed_dim", "\n", ")", "\n", "cls", ".", "decoder_embed_tokens", "=", "build_embedding", "(", "\n", "tgt_dict", ",", "args", ".", "decoder_embed_dim", "\n", ")", "\n", "\n", "", "encoder", "=", "TransformerEncoder", "(", "args", ",", "src_dict", ",", "encoder_embed_tokens", ")", "\n", "decoder", "=", "NgramTransformerDecoder", "(", "args", ",", "tgt_dict", ",", "cls", ".", "decoder_embed_tokens", ")", "\n", "model", "=", "NgramTransformerProphetModel", "(", "encoder", ",", "decoder", ")", "\n", "\n", "if", "args", ".", "load_from_pretrained_model", "is", "not", "None", ":", "\n", "            ", "states", "=", "torch", ".", "load", "(", "args", ".", "load_from_pretrained_model", ",", "map_location", "=", "'cpu'", ")", "\n", "if", "'model'", "in", "states", "and", "'args'", "in", "states", ":", "\n", "                ", "states", "=", "states", "[", "'model'", "]", "\n", "", "if", "args", ".", "load_sep", ":", "\n", "                ", "encoder_token_weight", "=", "states", "[", "'encoder.embed_tokens.weight'", "]", "\n", "decoder_token_weight", "=", "states", "[", "'decoder.embed_tokens.weight'", "]", "\n", "encoder_token_weight", "[", "2", "]", "=", "encoder_token_weight", "[", "102", "]", "\n", "decoder_token_weight", "[", "2", "]", "=", "decoder_token_weight", "[", "102", "]", "\n", "states", "[", "'encoder.embed_tokens.weight'", "]", "=", "encoder_token_weight", "\n", "states", "[", "'decoder.embed_tokens.weight'", "]", "=", "decoder_token_weight", "\n", "\n", "", "loaded_dict_size", "=", "states", "[", "'encoder.embed_tokens.weight'", "]", ".", "size", "(", "0", ")", "\n", "num_langids_to_add", "=", "len", "(", "encoder", ".", "dictionary", ")", "-", "loaded_dict_size", "\n", "embed_dim", "=", "states", "[", "'encoder.embed_tokens.weight'", "]", ".", "size", "(", "1", ")", "\n", "\n", "if", "num_langids_to_add", ">", "0", ":", "\n", "                ", "new_lang_embed_to_add", "=", "torch", ".", "zeros", "(", "num_langids_to_add", ",", "embed_dim", ")", "\n", "nn", ".", "init", ".", "normal_", "(", "\n", "new_lang_embed_to_add", ",", "\n", "mean", "=", "0", ",", "\n", "std", "=", "embed_dim", "**", "-", "0.5", "\n", ")", "\n", "new_lang_embed_to_add", "=", "new_lang_embed_to_add", ".", "to", "(", "\n", "dtype", "=", "states", "[", "'encoder.embed_tokens.weight'", "]", ".", "dtype", ",", "\n", ")", "\n", "\n", "states", "[", "'encoder.embed_tokens.weight'", "]", "=", "torch", ".", "cat", "(", "[", "\n", "states", "[", "'encoder.embed_tokens.weight'", "]", ",", "\n", "new_lang_embed_to_add", "]", "\n", ")", "\n", "states", "[", "'decoder.embed_tokens.weight'", "]", "=", "torch", ".", "cat", "(", "[", "\n", "states", "[", "'decoder.embed_tokens.weight'", "]", ",", "\n", "new_lang_embed_to_add", "]", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_s2s_model.NgramTransformerProphetModel.max_positions": [[197, 199], ["ngram_s2s_model.NgramTransformerProphetModel.encoder.max_positions", "ngram_s2s_model.NgramTransformerProphetModel.decoder.max_positions"], "methods", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_s2s_model.NgramTransformerDecoder.max_positions", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_s2s_model.NgramTransformerDecoder.max_positions"], [")", "\n", "\n", "", "for", "position_name", ",", "target_position_length", "in", "[", "(", "'encoder.embed_positions.weight'", ",", "model", ".", "encoder", ".", "embed_positions", ".", "weight", ".", "size", "(", "0", ")", ")", ","]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_s2s_model.NgramTransformerProphetModel.forward": [[200, 222], ["ngram_s2s_model.NgramTransformerProphetModel.encoder", "ngram_s2s_model.NgramTransformerProphetModel.decoder"], "methods", ["None"], ["(", "'decoder.embed_positions.weight'", ",", "model", ".", "decoder", ".", "embed_positions", ".", "weight", ".", "size", "(", "0", ")", ")", "]", ":", "\n", "                ", "if", "states", "[", "position_name", "]", ".", "size", "(", "0", ")", "<", "target_position_length", ":", "\n", "                    ", "_index", "=", "torch", ".", "arange", "(", "states", "[", "position_name", "]", ".", "size", "(", "1", ")", ")", "\n", "expend_position_states", "=", "states", "[", "position_name", "]", ".", "clone", "(", ")", "\n", "while", "states", "[", "position_name", "]", ".", "size", "(", "0", ")", "<", "target_position_length", ":", "\n", "                        ", "_index", "=", "torch", ".", "cat", "(", "(", "_index", "[", "1", ":", "]", ",", "_index", "[", ":", "1", "]", ")", ",", "dim", "=", "0", ")", "\n", "states", "[", "position_name", "]", "=", "torch", ".", "cat", "(", "[", "states", "[", "position_name", "]", ",", "expend_position_states", "[", ":", ",", "_index", "]", "]", ",", "dim", "=", "0", ")", "\n", "", "", "if", "states", "[", "position_name", "]", ".", "size", "(", "0", ")", ">", "target_position_length", ":", "\n", "                    ", "states", "[", "position_name", "]", "=", "states", "[", "position_name", "]", "[", ":", "target_position_length", "]", "\n", "", "", "model", ".", "load_state_dict", "(", "states", ")", "\n", "args", ".", "load_from_pretrained_model", "=", "None", "# Clear this param", "\n", "\n", "", "return", "NgramTransformerProphetModel", "(", "encoder", ",", "decoder", ")", "\n", "\n", "", "def", "max_positions", "(", "self", ")", ":", "\n", "        ", "return", "(", "self", ".", "encoder", ".", "max_positions", "(", ")", ",", "self", ".", "decoder", ".", "max_positions", "(", ")", ")", "\n", "\n", "", "def", "get_current_gampling_threshod", "(", "self", ",", "input_size", ",", "tgt_lengths", ")", ":", "\n", "        ", "max_len", "=", "self", ".", "decoder", ".", "max_positions", "(", ")", "\n", "t", "=", "torch", ".", "range", "(", "0", ",", "max_len", "-", "1", ")", "\n", "if", "self", ".", "decodingstep_schduled_sampling_strategy", "==", "\"linear\"", ":", "\n", "            ", "threshold_table", "=", "1", "-", "t", "/", "max_len", "\n", "", "elif", "self", ".", "decodingstep_schduled_sampling_strategy", "==", "\"exp\"", ":", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_s2s_model.TransformerEncoderLayer.__init__": [[230, 267], ["torch.Module.__init__", "fairseq.utils.get_activation_fn", "fairseq.modules.MultiheadAttention", "fairseq.modules.LayerNorm", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "fairseq.modules.LayerNorm"], "methods", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.transformer_lrp.TransformerLRP.__init__", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_s2s_model.Linear", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_s2s_model.Linear", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_s2s_model.Linear", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_s2s_model.Linear", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_s2s_model.Linear", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_s2s_model.Linear"], ["threshold_table", "=", "threshold_table", ".", "tril", "(", ")", "\n", "batch_size", ",", "seq_len", "=", "input_size", "\n", "# [b, max_len]", "\n", "tgt_lengths", "-=", "1", "# for index", "\n", "current_threshold", "=", "threshold_table", "[", "tgt_lengths", "]", ".", "view", "(", "batch_size", ",", "max_len", ")", "\n", "# [b, s]", "\n", "current_threshold", "=", "current_threshold", "[", ":", ",", ":", "seq_len", "]", "\n", "return", "current_threshold", "\n", "\n", "", "def", "forward", "(", "self", ",", "src_tokens", "=", "None", ",", "src_lengths", "=", "None", ",", "prev_output_tokens", "=", "None", ",", "tgt_lengths", "=", "None", ",", "**", "kwargs", ")", ":", "\n", "\n", "        ", "encoder_out", "=", "self", ".", "encoder", "(", "src_tokens", ",", "src_lengths", "=", "src_lengths", ",", "**", "kwargs", ")", "\n", "# get output ", "\n", "decoder_out", "=", "self", ".", "decoder", "(", "prev_output_tokens", ",", "encoder_out", "=", "encoder_out", ",", "src_lengths", "=", "src_lengths", ")", "\n", "new_decoder_input_emb", "=", "None", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "if", "self", ".", "training", ":", "\n", "                ", "batch_size", ",", "_", "=", "prev_output_tokens", ".", "size", "(", ")", "\n", "# get output ", "\n", "decoder_out", "=", "self", ".", "decoder", "(", "prev_output_tokens", ",", "encoder_out", "=", "encoder_out", ",", "src_lengths", "=", "src_lengths", ")", "\n", "# [B, T, C], only use 1ngram for model prediciton", "\n", "ngrams_decoder_out", "=", "decoder_out", "[", "0", "]", "[", "0", "]", ".", "detach", "(", ")", "\n", "# apply softmax # list of [B, T, V] ", "\n", "ngrams_decoder_prob", "=", "F", ".", "softmax", "(", "ngrams_decoder_out", ",", "dim", "=", "-", "1", ")", "\n", "# # [B, T, V] * [V, C] = [B, T ,C]", "\n", "soft_decoder_out_emb", "=", "torch", ".", "matmul", "(", "ngrams_decoder_prob", ",", "self", ".", "decoder_embed_tokens", ".", "weight", ")", "\n", "# appending bos embd [1, C]", "\n", "bos_idx", "=", "prev_output_tokens", "[", "0", ",", "0", "]", ".", "cuda", "(", ")", "\n", "# tile to [B, 1]", "\n", "bos_tokens", "=", "(", "bos_idx", "*", "torch", ".", "ones", "(", "(", "batch_size", ",", "1", ")", ")", ")", ".", "to", "(", "torch", ".", "long", ")", ".", "cuda", "(", ")", "\n", "# bos embedding , [B, 1, C]", "\n", "bos_embeddings", "=", "self", ".", "decoder_embed_tokens", "(", "bos_tokens", ")", "\n", "# [[B, T, C]; [B, 1, C]][:, :-1] => [B, T, C]", "\n", "soft_decoder_out_emb", "=", "torch", ".", "cat", "(", "[", "soft_decoder_out_emb", ",", "bos_embeddings", "]", ",", "dim", "=", "1", ")", "[", ":", ",", ":", "-", "1", ",", ":", "]", "\n", "# random select decoder input between goldens and predictions", "\n", "random_select_seed", "=", "torch", ".", "rand", "(", "prev_output_tokens", ".", "size", "(", ")", ")", "# shape sampe as prev_output_tokens", "\n", "prev_output_tokens_emb", "=", "self", ".", "decoder_embed_tokens", "(", "prev_output_tokens", ")", "\n", "assert", "prev_output_tokens_emb", ".", "size", "(", ")", "==", "soft_decoder_out_emb", ".", "size", "(", ")", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_s2s_model.TransformerEncoderLayer.forward": [[268, 299], ["ngram_s2s_model.TransformerEncoderLayer.self_attn", "torch.dropout", "torch.dropout", "torch.dropout", "ngram_s2s_model.TransformerEncoderLayer.self_attn_layer_norm", "ngram_s2s_model.TransformerEncoderLayer.activation_fn", "torch.dropout", "torch.dropout", "torch.dropout", "ngram_s2s_model.TransformerEncoderLayer.fc2", "torch.dropout", "torch.dropout", "torch.dropout", "ngram_s2s_model.TransformerEncoderLayer.final_layer_norm", "ngram_s2s_model.TransformerEncoderLayer.fc1"], "methods", ["None"], ["thresholds", "=", "self", ".", "get_current_gampling_threshod", "(", "prev_output_tokens", ".", "size", "(", ")", ",", "tgt_lengths", ")", "\n", "cond", "=", "random_select_seed", "<", "thresholds", "\n", "cond", "=", "cond", ".", "unsqueeze_", "(", "-", "1", ")", ".", "repeat", "(", "1", ",", "1", ",", "prev_output_tokens_emb", ".", "size", "(", ")", "[", "-", "1", "]", ")", ".", "cuda", "(", ")", "\n", "new_decoder_input_emb", "=", "torch", ".", "where", "(", "cond", ",", "prev_output_tokens_emb", ",", "soft_decoder_out_emb", ")", "\n", "\n", "# ATT:", "\n", "", "", "decoder_out_with_sampling", "=", "self", ".", "decoder", "(", "prev_output_tokens", ",", "encoder_out", "=", "encoder_out", ",", "new_decoder_input_emb", "=", "new_decoder_input_emb", ")", "\n", "# list of [B, T, V] * (ngram) => [ngram, B, T, V]", "\n", "decoder_out", "=", "torch", ".", "stack", "(", "decoder_out", "[", "0", "]", ",", "dim", "=", "0", ")", "\n", "decoder_out_with_sampling", "=", "torch", ".", "stack", "(", "decoder_out_with_sampling", "[", "0", "]", ",", "dim", "=", "0", ")", "\n", "return", "torch", ".", "stack", "(", "[", "decoder_out", ",", "decoder_out_with_sampling", "]", ",", "dim", "=", "0", ")", "# [2, ngram, B, T, V]", "\n", "\n", "\n", "", "def", "set_epoch", "(", "self", ",", "epoch", ",", "max_epoch", ")", ":", "\n", "        ", "self", ".", "epoch", "=", "epoch", "\n", "#self.decay_prob(epoch, max_epoch, self.trainstep_schduled_sampling_strategy, self.exp_radix)", "\n", "#print('swith to epoch {}, sampling golden with prob. -> {}'.format(epoch, self.sampling_golden_prob))", "\n", "\n", "", "def", "decay_prob", "(", "self", ",", "epoch_index", ",", "max_epoch", ",", "trainstep_schduled_sampling_strategy", "=", "None", ",", "exp_radix", "=", "0.8", ",", "sigmoid_k", "=", "0", ")", ":", "\n", "        ", "if", "trainstep_schduled_sampling_strategy", "==", "\"linear\"", ":", "\n", "            ", "self", ".", "sampling_golden_prob", "=", "-", "(", "epoch_index", "-", "1", ")", "/", "max_epoch", "+", "1.0", "# [0.1, 1]", "\n", "", "elif", "trainstep_schduled_sampling_strategy", "==", "\"exp\"", ":", "\n", "            ", "self", ".", "sampling_golden_prob", "=", "exp_radix", "**", "epoch_index", "# [0.1, 0.8]", "\n", "", "else", ":", "\n", "             ", "ValuraiseeError", "(", "\"Unknown trainstep_schduled_sampling_strategy  %s\"", "%", "trainstep_schduled_sampling_strategy", ")", "\n", "\n", "\n", "", "", "", "class", "TransformerEncoderLayer", "(", "nn", ".", "Module", ")", ":", "\n", "    ", "\"\"\"\n    Implements a Transformer Encoder Layer used in BERT/XLM style pre-trained\n    models.\n    \"\"\"", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_s2s_model.NgramTransformerDecoderLayer.__init__": [[302, 355], ["torch.Module.__init__", "fairseq.utils.get_activation_fn", "ngram_multihead_attention.NgramMultiheadAttention", "fairseq.modules.LayerNorm", "fairseq.modules.MultiheadAttention", "fairseq.modules.LayerNorm", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "fairseq.modules.LayerNorm"], "methods", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.transformer_lrp.TransformerLRP.__init__", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_s2s_model.Linear", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_s2s_model.Linear", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_s2s_model.Linear", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_s2s_model.Linear", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_s2s_model.Linear", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_s2s_model.Linear"], ["self", ",", "\n", "embedding_dim", ":", "float", "=", "768", ",", "\n", "ffn_embedding_dim", ":", "float", "=", "3072", ",", "\n", "num_attention_heads", ":", "float", "=", "8", ",", "\n", "dropout", ":", "float", "=", "0.1", ",", "\n", "attention_dropout", ":", "float", "=", "0.1", ",", "\n", "activation_dropout", ":", "float", "=", "0.1", ",", "\n", "activation_fn", ":", "str", "=", "'relu'", ",", "\n", "add_bias_kv", ":", "bool", "=", "False", ",", "\n", "add_zero_attn", ":", "bool", "=", "False", ",", "\n", "export", ":", "bool", "=", "False", ",", "\n", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "# Initialize parameters", "\n", "self", ".", "embedding_dim", "=", "embedding_dim", "\n", "self", ".", "dropout", "=", "dropout", "\n", "self", ".", "activation_dropout", "=", "activation_dropout", "\n", "\n", "# Initialize blocks", "\n", "self", ".", "activation_fn", "=", "utils", ".", "get_activation_fn", "(", "activation_fn", ")", "\n", "self", ".", "self_attn", "=", "MultiheadAttention", "(", "\n", "self", ".", "embedding_dim", ",", "\n", "num_attention_heads", ",", "\n", "dropout", "=", "attention_dropout", ",", "\n", "add_bias_kv", "=", "add_bias_kv", ",", "\n", "add_zero_attn", "=", "add_zero_attn", ",", "\n", "self_attention", "=", "True", ",", "\n", ")", "\n", "\n", "# layer norm associated with the self attention layer", "\n", "self", ".", "self_attn_layer_norm", "=", "LayerNorm", "(", "self", ".", "embedding_dim", ",", "export", "=", "export", ")", "\n", "self", ".", "fc1", "=", "nn", ".", "Linear", "(", "self", ".", "embedding_dim", ",", "ffn_embedding_dim", ")", "\n", "self", ".", "fc2", "=", "nn", ".", "Linear", "(", "ffn_embedding_dim", ",", "self", ".", "embedding_dim", ")", "\n", "\n", "# layer norm associated with the position wise feed-forward NN", "\n", "self", ".", "final_layer_norm", "=", "LayerNorm", "(", "self", ".", "embedding_dim", ",", "export", "=", "export", ")", "\n", "\n", "", "def", "forward", "(", "\n", "self", ",", "\n", "x", ":", "torch", ".", "Tensor", ",", "\n", "self_attn_mask", ":", "torch", ".", "Tensor", "=", "None", ",", "\n", "self_attn_padding_mask", ":", "torch", ".", "Tensor", "=", "None", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        LayerNorm is applied either before or after the self-attention/ffn\n        modules similar to the original Transformer imlementation.\n        \"\"\"", "\n", "residual", "=", "x", "\n", "x", ",", "attn", "=", "self", ".", "self_attn", "(", "\n", "query", "=", "x", ",", "\n", "key", "=", "x", ",", "\n", "value", "=", "x", ",", "\n", "key_padding_mask", "=", "self_attn_padding_mask", ",", "\n", "need_weights", "=", "False", ",", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_s2s_model.NgramTransformerDecoderLayer.forward": [[356, 425], ["ngram_s2s_model.NgramTransformerDecoderLayer.ngram_self_attn", "torch.dropout", "torch.dropout", "torch.dropout", "ngram_s2s_model.NgramTransformerDecoderLayer.self_attn_layer_norm", "ngram_s2s_model.NgramTransformerDecoderLayer.encoder_attn", "torch.dropout", "torch.dropout", "torch.dropout", "ngram_s2s_model.NgramTransformerDecoderLayer.encoder_attn_layer_norm", "ngram_s2s_model.NgramTransformerDecoderLayer.activation_fn", "torch.dropout", "torch.dropout", "torch.dropout", "ngram_s2s_model.NgramTransformerDecoderLayer.fc2", "torch.dropout", "torch.dropout", "torch.dropout", "ngram_s2s_model.NgramTransformerDecoderLayer.final_layer_norm", "ngram_s2s_model.NgramTransformerDecoderLayer.self_attn._set_input_buffer", "ngram_s2s_model.NgramTransformerDecoderLayer.encoder_attn._set_input_buffer", "ngram_s2s_model.NgramTransformerDecoderLayer.fc1"], "methods", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_multihead_attention.NgramMultiheadAttention._set_input_buffer", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_multihead_attention.NgramMultiheadAttention._set_input_buffer"], ["attn_mask", "=", "self_attn_mask", ",", "\n", ")", "\n", "x", "=", "F", ".", "dropout", "(", "x", ",", "p", "=", "self", ".", "dropout", ",", "training", "=", "self", ".", "training", ")", "\n", "x", "=", "residual", "+", "x", "\n", "x", "=", "self", ".", "self_attn_layer_norm", "(", "x", ")", "\n", "\n", "residual", "=", "x", "\n", "x", "=", "self", ".", "activation_fn", "(", "self", ".", "fc1", "(", "x", ")", ")", "\n", "x", "=", "F", ".", "dropout", "(", "x", ",", "p", "=", "self", ".", "activation_dropout", ",", "training", "=", "self", ".", "training", ")", "\n", "x", "=", "self", ".", "fc2", "(", "x", ")", "\n", "x", "=", "F", ".", "dropout", "(", "x", ",", "p", "=", "self", ".", "dropout", ",", "training", "=", "self", ".", "training", ")", "\n", "x", "=", "residual", "+", "x", "\n", "x", "=", "self", ".", "final_layer_norm", "(", "x", ")", "\n", "return", "x", ",", "attn", "\n", "\n", "\n", "", "", "class", "NgramTransformerDecoderLayer", "(", "nn", ".", "Module", ")", ":", "\n", "    ", "def", "__init__", "(", "\n", "self", ",", "\n", "ngram", "=", "2", ",", "\n", "embedding_dim", ":", "float", "=", "768", ",", "\n", "ffn_embedding_dim", ":", "float", "=", "3072", ",", "\n", "num_attention_heads", ":", "float", "=", "8", ",", "\n", "dropout", ":", "float", "=", "0.1", ",", "\n", "attention_dropout", ":", "float", "=", "0.1", ",", "\n", "activation_dropout", ":", "float", "=", "0.1", ",", "\n", "activation_fn", ":", "str", "=", "'relu'", ",", "\n", "add_bias_kv", ":", "bool", "=", "False", ",", "\n", "add_zero_attn", ":", "bool", "=", "False", ",", "\n", "export", ":", "bool", "=", "False", ",", "\n", "\n", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "embedding_dim", "=", "embedding_dim", "\n", "self", ".", "dropout", "=", "dropout", "\n", "self", ".", "activation_dropout", "=", "activation_dropout", "\n", "\n", "# Initialize blocks", "\n", "self", ".", "activation_fn", "=", "utils", ".", "get_activation_fn", "(", "activation_fn", ")", "\n", "self", ".", "ngram_self_attn", "=", "NgramMultiheadAttention", "(", "\n", "self", ".", "embedding_dim", ",", "\n", "num_attention_heads", ",", "\n", "dropout", "=", "attention_dropout", ",", "\n", "add_bias_kv", "=", "add_bias_kv", ",", "\n", "add_zero_attn", "=", "add_zero_attn", ",", "\n", "self_attention", "=", "True", ",", "\n", "ngram", "=", "ngram", "\n", ")", "\n", "self", ".", "ngram", "=", "ngram", "\n", "\n", "# layer norm associated with the self attention layer", "\n", "self", ".", "self_attn_layer_norm", "=", "LayerNorm", "(", "self", ".", "embedding_dim", ",", "export", "=", "export", ")", "\n", "\n", "self", ".", "encoder_attn", "=", "MultiheadAttention", "(", "\n", "self", ".", "embedding_dim", ",", "\n", "num_attention_heads", ",", "\n", "kdim", "=", "embedding_dim", ",", "\n", "vdim", "=", "embedding_dim", ",", "\n", "dropout", "=", "attention_dropout", ",", "\n", "encoder_decoder_attention", "=", "True", ",", "\n", ")", "\n", "self", ".", "encoder_attn_layer_norm", "=", "LayerNorm", "(", "self", ".", "embedding_dim", ",", "export", "=", "export", ")", "\n", "\n", "self", ".", "fc1", "=", "nn", ".", "Linear", "(", "self", ".", "embedding_dim", ",", "ffn_embedding_dim", ")", "\n", "self", ".", "fc2", "=", "nn", ".", "Linear", "(", "ffn_embedding_dim", ",", "self", ".", "embedding_dim", ")", "\n", "\n", "# layer norm associated with the position wise feed-forward NN", "\n", "self", ".", "final_layer_norm", "=", "LayerNorm", "(", "self", ".", "embedding_dim", ",", "export", "=", "export", ")", "\n", "self", ".", "need_attn", "=", "False", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_s2s_model.NgramTransformerDecoderLayer.make_generation_fast_": [[426, 428], ["None"], "methods", ["None"], ["\n", "", "def", "forward", "(", "\n", "self", ",", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_s2s_model.TransformerEncoder.__init__": [[439, 473], ["fairseq.models.FairseqEncoder.__init__", "ngram_s2s_model.TransformerEncoder.register_buffer", "learned_positional_embedding.LearnedPositionalEmbedding", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "ngram_s2s_model.TransformerEncoder.layers.extend", "fairseq.modules.LayerNorm", "ngram_s2s_model.TransformerEncoder.apply", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "ngram_s2s_model.TransformerEncoderLayer", "range"], "methods", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.transformer_lrp.TransformerLRP.__init__"], ["real_positions", "=", "None", "\n", ")", ":", "\n", "# one main stream and ngram predicting streams", "\n", "        ", "residual", "=", "x", "\n", "\n", "if", "prev_self_attn_state", "is", "not", "None", ":", "\n", "            ", "if", "incremental_state", "is", "None", ":", "\n", "                ", "incremental_state", "=", "{", "}", "\n", "", "prev_key", ",", "prev_value", "=", "prev_self_attn_state", "\n", "saved_state", "=", "{", "\"prev_key\"", ":", "prev_key", ",", "\"prev_value\"", ":", "prev_value", "}", "\n", "self", ".", "self_attn", ".", "_set_input_buffer", "(", "incremental_state", ",", "saved_state", ")", "\n", "\n", "", "x", ",", "attn", "=", "self", ".", "ngram_self_attn", "(", "\n", "query", "=", "x", ",", "\n", "key", "=", "x", ",", "\n", "value", "=", "x", ",", "\n", "incremental_state", "=", "incremental_state", ",", "\n", "need_weights", "=", "False", ",", "\n", "self_attn_mask", "=", "self_attn_mask", ",", "\n", "ngram_mask_matrix", "=", "ngram_mask_matrix", ",", "\n", "i_buckets_main_stream", "=", "i_buckets_main_stream", ",", "\n", "i_bucket_relative_stream", "=", "i_bucket_relative_stream", ",", "\n", "real_positions", "=", "real_positions", "\n", ")", "\n", "# [(1+ngram)*T, B, C]", "\n", "\n", "x", "=", "F", ".", "dropout", "(", "x", ",", "p", "=", "self", ".", "dropout", ",", "training", "=", "self", ".", "training", ")", "\n", "x", "=", "residual", "+", "x", "\n", "x", "=", "self", ".", "self_attn_layer_norm", "(", "x", ")", "\n", "\n", "residual", "=", "x", "\n", "if", "prev_attn_state", "is", "not", "None", ":", "\n", "            ", "if", "incremental_state", "is", "None", ":", "\n", "                ", "incremental_state", "=", "{", "}", "\n", "", "prev_key", ",", "prev_value", "=", "prev_attn_state", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_s2s_model.TransformerEncoder.forward": [[474, 520], ["src_tokens.eq", "ngram_s2s_model.TransformerEncoder.embed_tokens", "torch.dropout", "torch.dropout", "torch.dropout", "ngram_s2s_model.TransformerEncoder.transpose", "src_tokens.eq.any", "ngram_s2s_model.TransformerEncoder.embed_positions", "ngram_s2s_model.TransformerEncoder.emb_layer_norm", "layer", "src_tokens.eq.unsqueeze().type_as", "src_tokens.eq.unsqueeze"], "methods", ["None"], ["saved_state", "=", "{", "\"prev_key\"", ":", "prev_key", ",", "\"prev_value\"", ":", "prev_value", "}", "\n", "self", ".", "encoder_attn", ".", "_set_input_buffer", "(", "incremental_state", ",", "saved_state", ")", "\n", "", "x", ",", "attn", "=", "self", ".", "encoder_attn", "(", "\n", "query", "=", "x", ",", "\n", "key", "=", "encoder_out", ",", "\n", "value", "=", "encoder_out", ",", "\n", "key_padding_mask", "=", "encoder_mask", ",", "\n", "incremental_state", "=", "incremental_state", ",", "\n", "static_kv", "=", "True", ",", "\n", "need_weights", "=", "(", "not", "self", ".", "training", "and", "self", ".", "need_attn", ")", ",", "\n", ")", "\n", "x", "=", "F", ".", "dropout", "(", "x", ",", "p", "=", "self", ".", "dropout", ",", "training", "=", "self", ".", "training", ")", "\n", "x", "=", "residual", "+", "x", "\n", "x", "=", "self", ".", "encoder_attn_layer_norm", "(", "x", ")", "\n", "\n", "residual", "=", "x", "\n", "x", "=", "self", ".", "activation_fn", "(", "self", ".", "fc1", "(", "x", ")", ")", "\n", "x", "=", "F", ".", "dropout", "(", "x", ",", "p", "=", "self", ".", "activation_dropout", ",", "training", "=", "self", ".", "training", ")", "\n", "x", "=", "self", ".", "fc2", "(", "x", ")", "\n", "x", "=", "F", ".", "dropout", "(", "x", ",", "p", "=", "self", ".", "dropout", ",", "training", "=", "self", ".", "training", ")", "\n", "x", "=", "residual", "+", "x", "\n", "x", "=", "self", ".", "final_layer_norm", "(", "x", ")", "\n", "return", "x", ",", "attn", "\n", "\n", "", "def", "make_generation_fast_", "(", "self", ",", "need_attn", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "        ", "self", ".", "need_attn", "=", "need_attn", "\n", "\n", "\n", "\n", "", "", "class", "TransformerEncoder", "(", "FairseqEncoder", ")", ":", "\n", "    ", "\"\"\"\n    Transformer encoder consisting of *args.encoder_layers* layers. Each layer\n    is a :class:`TransformerEncoderLayer`.\n    Args:\n        args (argparse.Namespace): parsed command-line arguments\n        dictionary (~fairseq.data.Dictionary): encoding dictionary\n        embed_tokens (torch.nn.Embedding): input embedding\n    \"\"\"", "\n", "\n", "def", "__init__", "(", "self", ",", "args", ",", "dictionary", ",", "embed_tokens", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "dictionary", ")", "\n", "self", ".", "register_buffer", "(", "'version'", ",", "torch", ".", "Tensor", "(", "[", "3", "]", ")", ")", "\n", "\n", "self", ".", "dropout", "=", "args", ".", "dropout", "\n", "\n", "embed_dim", "=", "embed_tokens", ".", "embedding_dim", "\n", "self", ".", "padding_idx", "=", "embed_tokens", ".", "padding_idx", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_s2s_model.TransformerEncoder.reorder_encoder_out": [[522, 538], ["encoder_out[].index_select", "encoder_out[].index_select"], "methods", ["None"], ["\n", "self", ".", "embed_tokens", "=", "embed_tokens", "\n", "self", ".", "embed_scale", "=", "None", "#math.sqrt(embed_dim)", "\n", "self", ".", "embed_positions", "=", "LearnedPositionalEmbedding", "(", "\n", "args", ".", "max_source_positions", "+", "1", "+", "self", ".", "padding_idx", ",", "embed_dim", ",", "self", ".", "padding_idx", ",", "\n", ")", "\n", "\n", "self", ".", "layers", "=", "nn", ".", "ModuleList", "(", "[", "]", ")", "\n", "\n", "self", ".", "layers", ".", "extend", "(", "[", "\n", "TransformerEncoderLayer", "(", "\n", "args", ".", "encoder_embed_dim", ",", "\n", "args", ".", "encoder_ffn_embed_dim", ",", "\n", "args", ".", "encoder_attention_heads", ",", "\n", "args", ".", "dropout", ",", "\n", "args", ".", "attention_dropout", ",", "\n", "args", ".", "activation_dropout", ",", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_s2s_model.TransformerEncoder.max_positions": [[539, 544], ["min", "ngram_s2s_model.TransformerEncoder.embed_positions.max_positions"], "methods", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_s2s_model.NgramTransformerDecoder.max_positions"], ["args", ".", "activation_fn", ",", "\n", ")", "\n", "for", "i", "in", "range", "(", "args", ".", "encoder_layers", ")", "\n", "]", ")", "\n", "\n", "self", ".", "emb_layer_norm", "=", "LayerNorm", "(", "embed_dim", ")", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_s2s_model.NgramTransformerDecoder.__init__": [[558, 606], ["fairseq.models.FairseqIncrementalDecoder.__init__", "ngram_s2s_model.NgramTransformerDecoder.register_buffer", "learned_positional_embedding.LearnedPositionalEmbedding", "ngram_s2s_model.Embedding", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "ngram_s2s_model.NgramTransformerDecoder.layers.extend", "fairseq.modules.LayerNorm", "ngram_s2s_model.NgramTransformerDecoder.apply", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.init.normal_", "torch.init.normal_", "torch.init.normal_", "ngram_s2s_model.NgramTransformerDecoderLayer", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "range", "len"], "methods", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.transformer_lrp.TransformerLRP.__init__", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_s2s_model.Embedding"], ["\n", "# compute padding mask", "\n", "encoder_padding_mask", "=", "src_tokens", ".", "eq", "(", "self", ".", "padding_idx", ")", "\n", "if", "not", "encoder_padding_mask", ".", "any", "(", ")", ":", "\n", "            ", "encoder_padding_mask", "=", "None", "\n", "\n", "", "x", "=", "self", ".", "embed_tokens", "(", "src_tokens", ")", "\n", "# embed tokens and positions", "\n", "if", "self", ".", "embed_scale", "is", "not", "None", ":", "\n", "            ", "x", "*=", "self", ".", "embed_scale", "\n", "\n", "", "if", "self", ".", "embed_positions", "is", "not", "None", ":", "\n", "            ", "pos_emb", ",", "real_positions", "=", "self", ".", "embed_positions", "(", "src_tokens", ")", "\n", "x", "+=", "pos_emb", "\n", "\n", "", "if", "self", ".", "emb_layer_norm", ":", "\n", "            ", "x", "=", "self", ".", "emb_layer_norm", "(", "x", ")", "\n", "\n", "", "x", "=", "F", ".", "dropout", "(", "x", ",", "p", "=", "self", ".", "dropout", ",", "training", "=", "self", ".", "training", ")", "\n", "\n", "if", "encoder_padding_mask", "is", "not", "None", ":", "\n", "            ", "x", "*=", "1", "-", "encoder_padding_mask", ".", "unsqueeze", "(", "-", "1", ")", ".", "type_as", "(", "x", ")", "\n", "# B x T x C -> T x B x C", "\n", "", "x", "=", "x", ".", "transpose", "(", "0", ",", "1", ")", "\n", "\n", "# encoder layers", "\n", "for", "layer", "in", "self", ".", "layers", ":", "\n", "# x, _ = layer(x, self_attn_padding_mask=encoder_padding_mask, real_positions=real_positions)", "\n", "            ", "x", ",", "_", "=", "layer", "(", "x", ",", "self_attn_padding_mask", "=", "encoder_padding_mask", ",", ")", "\n", "\n", "", "return", "{", "\n", "'encoder_out'", ":", "x", ",", "# T x B x C", "\n", "'encoder_padding_mask'", ":", "encoder_padding_mask", ",", "# B x T", "\n", "}", "\n", "\n", "", "def", "reorder_encoder_out", "(", "self", ",", "encoder_out", ",", "new_order", ")", ":", "\n", "        ", "\"\"\"\n        Reorder encoder output according to *new_order*.\n        Args:\n            encoder_out: output from the ``forward()`` method\n            new_order (LongTensor): desired order\n        Returns:\n            *encoder_out* rearranged according to *new_order*\n        \"\"\"", "\n", "if", "encoder_out", "[", "'encoder_out'", "]", "is", "not", "None", ":", "\n", "            ", "encoder_out", "[", "'encoder_out'", "]", "="]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_s2s_model.NgramTransformerDecoder.forward": [[607, 624], ["prev_output_tokens.size", "ngram_s2s_model.NgramTransformerDecoder.extract_features", "ngram_s2s_model.NgramTransformerDecoder.output_layer"], "methods", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.distribute.size", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_s2s_model.NgramTransformerDecoder.extract_features", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_s2s_model.NgramTransformerDecoder.output_layer"], ["encoder_out", "[", "'encoder_out'", "]", ".", "index_select", "(", "1", ",", "new_order", ")", "\n", "", "if", "encoder_out", "[", "'encoder_padding_mask'", "]", "is", "not", "None", ":", "\n", "            ", "encoder_out", "[", "'encoder_padding_mask'", "]", "=", "encoder_out", "[", "'encoder_padding_mask'", "]", ".", "index_select", "(", "0", ",", "new_order", ")", "\n", "", "return", "encoder_out", "\n", "\n", "", "def", "max_positions", "(", "self", ")", ":", "\n", "        ", "\"\"\"Maximum input length supported by the encoder.\"\"\"", "\n", "if", "self", ".", "embed_positions", "is", "None", ":", "\n", "            ", "return", "self", ".", "max_source_positions", "\n", "", "return", "min", "(", "self", ".", "max_source_positions", ",", "self", ".", "embed_positions", ".", "max_positions", "(", ")", ")", "\n", "\n", "\n", "", "", "class", "NgramTransformerDecoder", "(", "FairseqIncrementalDecoder", ")", ":", "\n", "    "]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_s2s_model.NgramTransformerDecoder._relative_positions_bucket": [[625, 644], ["torch.lt", "torch.lt", "torch.lt", "torch.lt", "torch.lt", "torch.lt", "torch.lt", "torch.lt", "torch.lt", "torch.min", "torch.min", "torch.min", "torch.min", "torch.min", "torch.min", "torch.min", "torch.min", "torch.min", "val_if_large.int.int.int", "torch.abs", "torch.abs", "torch.abs", "torch.abs", "torch.abs", "torch.abs", "torch.abs", "torch.abs", "torch.abs", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.where", "torch.where", "torch.where", "torch.where", "torch.where", "torch.where", "torch.where", "torch.where", "torch.where", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.ones_like", "torch.ones_like", "torch.ones_like", "torch.ones_like", "torch.ones_like", "torch.ones_like", "torch.ones_like", "torch.ones_like", "torch.ones_like", "torch.max.int", "torch.max.int", "torch.max.int", "torch.lt().int", "torch.lt().int", "torch.lt().int", "torch.lt().int", "torch.lt().int", "torch.lt().int", "torch.lt().int", "torch.lt().int", "torch.lt().int", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "math.log", "torch.lt", "torch.lt", "torch.lt", "torch.lt", "torch.lt", "torch.lt", "torch.lt", "torch.lt", "torch.lt", "torch.max.float", "torch.max.float", "torch.max.float", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like"], "methods", ["None"], ["\n", "\n", "def", "__init__", "(", "self", ",", "args", ",", "dictionary", ",", "embed_tokens", ",", "no_encoder_attn", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "dictionary", ")", "\n", "self", ".", "register_buffer", "(", "'version'", ",", "torch", ".", "Tensor", "(", "[", "3", "]", ")", ")", "\n", "self", ".", "ngram", "=", "args", ".", "ngram", "\n", "self", ".", "num_buckets", "=", "args", ".", "num_buckets", "\n", "self", ".", "relative_max_distance", "=", "args", ".", "relative_max_distance", "\n", "\n", "self", ".", "dropout", "=", "args", ".", "dropout", "\n", "self", ".", "share_input_output_embed", "=", "args", ".", "share_decoder_input_output_embed", "\n", "\n", "input_embed_dim", "=", "embed_tokens", ".", "embedding_dim", "\n", "embed_dim", "=", "args", ".", "decoder_embed_dim", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_s2s_model.NgramTransformerDecoder.cal_pretrain_relative_positions": [[645, 670], ["real_positions.unsqueeze", "main_stream_relative_positions.repeat.repeat.repeat", "real_positions.unsqueeze", "torch.cat().unsqueeze", "torch.cat().unsqueeze", "torch.cat().unsqueeze", "torch.cat().unsqueeze", "torch.cat().unsqueeze", "torch.cat().unsqueeze", "torch.cat().unsqueeze", "torch.cat().unsqueeze", "torch.cat().unsqueeze", "predicting_stream_relative_positions.repeat.repeat.repeat", "real_positions.unsqueeze", "ngram_s2s_model.NgramTransformerDecoder._relative_positions_bucket", "ngram_s2s_model.NgramTransformerDecoder._relative_positions_bucket", "real_positions.size", "real_positions.size", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat"], "methods", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_multihead_attention.NgramMultiheadAttention._relative_positions_bucket", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_multihead_attention.NgramMultiheadAttention._relative_positions_bucket", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.distribute.size", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.distribute.size"], ["self", ".", "padding_idx", "=", "embed_tokens", ".", "padding_idx", "\n", "self", ".", "max_target_positions", "=", "args", ".", "max_target_positions", "\n", "self", ".", "embed_dim", "=", "embed_dim", "\n", "self", ".", "embed_tokens", "=", "embed_tokens", "\n", "self", ".", "embed_scale", "=", "None", "#math.sqrt(embed_dim)  # todo: try with input_embed_dim", "\n", "\n", "self", ".", "embed_positions", "=", "LearnedPositionalEmbedding", "(", "\n", "args", ".", "max_target_positions", "+", "2", "+", "self", ".", "padding_idx", ",", "embed_dim", ",", "self", ".", "padding_idx", ",", "\n", ")", "\n", "\n", "self", ".", "ngram_input_embed", "=", "Embedding", "(", "self", ".", "ngram", ",", "input_embed_dim", ",", "None", ")", "\n", "\n", "self", ".", "layers", "=", "nn", ".", "ModuleList", "(", "[", "]", ")", "\n", "\n", "self", ".", "layers", ".", "extend", "(", "[", "\n", "NgramTransformerDecoderLayer", "(", "\n", "args", ".", "ngram", ",", "\n", "args", ".", "decoder_embed_dim", ",", "\n", "args", ".", "decoder_ffn_embed_dim", ",", "\n", "args", ".", "decoder_attention_heads", ",", "\n", "args", ".", "dropout", ",", "\n", "args", ".", "attention_dropout", ",", "\n", "args", ".", "activation_dropout", ",", "\n", "args", ".", "activation_fn", ",", "\n", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_s2s_model.NgramTransformerDecoder.cal_finetune_relative_positions": [[671, 691], ["real_positions.size", "real_positions.size", "ngram_s2s_model.NgramTransformerDecoder._finetune_i_bucket_main_stream[].repeat", "torch.cat().repeat", "torch.cat().repeat", "torch.cat().repeat", "torch.cat().repeat", "torch.cat().repeat", "torch.cat().repeat", "torch.cat().repeat", "torch.cat().repeat", "torch.cat().repeat", "torch.arange().repeat", "torch.arange().repeat", "torch.arange().repeat", "torch.arange().repeat", "torch.arange().repeat", "torch.arange().repeat", "torch.arange().repeat", "torch.arange().repeat", "torch.arange().repeat", "ngram_s2s_model.NgramTransformerDecoder.cal_pretrain_relative_positions", "ngram_s2s_model.NgramTransformerDecoder.to", "torch.cat().repeat.to", "torch.cat().repeat.to", "torch.cat().repeat.to", "hasattr", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange"], "methods", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.distribute.size", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.distribute.size", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_s2s_model.NgramTransformerDecoder.cal_pretrain_relative_positions"], ["for", "_", "in", "range", "(", "args", ".", "decoder_layers", ")", "\n", "]", ")", "\n", "\n", "if", "not", "self", ".", "share_input_output_embed", ":", "\n", "            ", "self", ".", "embed_out", "=", "nn", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "len", "(", "dictionary", ")", ",", "self", ".", "embed_dim", ")", ")", "\n", "nn", ".", "init", ".", "normal_", "(", "self", ".", "embed_out", ",", "mean", "=", "0", ",", "std", "=", "self", ".", "embed_dim", "**", "-", "0.5", ")", "\n", "\n", "", "self", ".", "emb_layer_norm", "=", "LayerNorm", "(", "embed_dim", ")", "\n", "self", ".", "apply", "(", "init_bert_params", ")", "\n", "\n", "", "def", "forward", "(", "self", ",", "\n", "prev_output_tokens", ",", "\n", "encoder_out", "=", "None", ",", "\n", "incremental_state", "=", "None", ",", "\n", "**", "unused", ")", ":", "\n", "# T  [B, T]", "\n", "        ", "T", "=", "prev_output_tokens", ".", "size", "(", "1", ")", "\n", "# more exactly, [B, T, C] * (1+ngram)", "\n", "x_list", ",", "extra", "=", "self", ".", "extract_features", "(", "prev_output_tokens", ",", "encoder_out", ",", "incremental_state", ",", "**", "unused", ")", "\n", "x_predicted", "=", "x_list", "[", "1", ":", "]", "\n", "x_predicted", "=", "[", "self", ".", "output_layer", "(", "x", ")", "for", "x", "in", "x_predicted", "]", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_s2s_model.NgramTransformerDecoder.extract_features": [[692, 788], ["ngram_s2s_model.NgramTransformerDecoder.embed_positions._forward", "ngram_s2s_model.NgramTransformerDecoder.embed_tokens", "ngram_s2s_model.NgramTransformerDecoder.transpose", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.dropout", "torch.dropout", "torch.dropout", "ngram_s2s_model.NgramTransformerDecoder.transpose().chunk", "ngram_s2s_model.NgramTransformerDecoder.embed_positions._forward", "ngram_s2s_model.NgramTransformerDecoder.cal_pretrain_relative_positions", "print", "Exception", "ngram_s2s_model.NgramTransformerDecoder.size", "ngram_s2s_model.NgramTransformerDecoder.buffered_future_mask", "ngram_s2s_model.NgramTransformerDecoder.buffered_future_mask_ngram", "ngram_s2s_model.NgramTransformerDecoder.emb_layer_norm", "layer", "inner_states.append", "attn.transpose().chunk", "ngram_s2s_model.NgramTransformerDecoder.embed_positions", "ngram_s2s_model.NgramTransformerDecoder.cal_finetune_relative_positions", "ngram_s2s_model.NgramTransformerDecoder.transpose", "range", "range", "attn.transpose"], "methods", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.learned_positional_embedding.LearnedPositionalEmbedding._forward", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.learned_positional_embedding.LearnedPositionalEmbedding._forward", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_s2s_model.NgramTransformerDecoder.cal_pretrain_relative_positions", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.distribute.size", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_s2s_model.NgramTransformerDecoder.buffered_future_mask", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_s2s_model.NgramTransformerDecoder.buffered_future_mask_ngram", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_s2s_model.NgramTransformerDecoder.cal_finetune_relative_positions"], ["# list of [B, T, V] * (ngram)", "\n", "if", "incremental_state", "is", "not", "None", ":", "\n", "            ", "x_predicted", "=", "x_predicted", "[", "0", "]", "\n", "for", "k", "in", "extra", ":", "\n", "                ", "if", "extra", "[", "k", "]", "is", "not", "None", ":", "\n", "                    ", "extra", "[", "k", "]", "=", "extra", "[", "k", "]", "[", "0", "]", "\n", "", "", "", "return", "x_predicted", ",", "extra", "\n", "\n", "", "def", "_relative_positions_bucket", "(", "self", ",", "relative_positions", ",", "bidirectional", "=", "False", ")", ":", "\n", "        ", "num_buckets", "=", "self", ".", "num_buckets", "\n", "max_distance", "=", "self", ".", "relative_max_distance", "\n", "n", "=", "-", "relative_positions", "\n", "result", "=", "0", "\n", "if", "bidirectional", ":", "\n", "            ", "num_buckets", "=", "num_buckets", "//", "2", "\n", "result", "=", "result", "+", "torch", ".", "lt", "(", "n", ",", "torch", ".", "zeros_like", "(", "n", ")", ")", ".", "int", "(", ")", "*", "num_buckets", "\n", "n", "=", "torch", ".", "abs", "(", "n", ")", "\n", "", "else", ":", "\n", "            ", "n", "=", "torch", ".", "max", "(", "n", ",", "torch", ".", "zeros_like", "(", "n", ")", ")", "\n", "", "max_exact", "=", "num_buckets", "//", "2", "\n", "is_small", "=", "torch", ".", "lt", "(", "n", ",", "max_exact", ")", "\n", "val_if_large", "=", "max_exact", "+", "torch", ".", "log", "(", "n", ".", "float", "(", ")", "/", "max_exact", ")", "/", "math", ".", "log", "(", "max_distance", "/", "max_exact", ")", "*", "(", "\n", "num_buckets", "-", "max_exact", ")", "\n", "val_if_large", "=", "torch", ".", "min", "(", "val_if_large", ",", "torch", ".", "ones_like", "(", "val_if_large", ")", "*", "(", "num_buckets", "-", "1", ")", ")", "\n", "val_if_large", "=", "val_if_large", ".", "int", "(", ")", "\n", "result", "=", "result", "+", "torch", ".", "where", "(", "is_small", ",", "n", ".", "int", "(", ")", ",", "val_if_large", ")", "\n", "return", "result", "\n", "\n", "", "def", "cal_pretrain_relative_positions", "(", "self", ",", "real_positions", ")", ":", "\n", "# main stream", "\n", "        ", "main_stream_relative_positions", "=", "real_positions", ".", "unsqueeze", "(", "1", ")", "\n", "# [B,T,T/S]", "\n", "main_stream_relative_positions", "=", "main_stream_relative_positions", ".", "repeat", "(", "1", ",", "real_positions", ".", "size", "(", "-", "1", ")", ",", "1", ")", "\n", "# [B,T,1]", "\n", "real_positions_main", "=", "real_positions", ".", "unsqueeze", "(", "-", "1", ")", "\n", "main_stream_relative_positions", "=", "main_stream_relative_positions", "-", "real_positions_main", "\n", "\n", "# predicting stream", "\n", "# input shift", "\n", "real_positions_shift_predicting_stream", "=", "real_positions", "-", "1", "\n", "# [B,1, 2*T]", "\n", "predicting_stream_relative_positions", "=", "torch", ".", "cat", "(", "(", "real_positions_shift_predicting_stream", ",", "real_positions", ")", ",", "\n", "dim", "=", "-", "1", ")", ".", "unsqueeze", "(", "1", ")", "\n", "# [B,T, 2*T]", "\n", "predicting_stream_relative_positions", "=", "predicting_stream_relative_positions", ".", "repeat", "(", "1", ",", "real_positions", ".", "size", "(", "-", "1", ")", ",", "\n", "1", ")", "\n", "# [B,T, 1]", "\n", "real_positions_predicting_stream", "=", "real_positions", ".", "unsqueeze", "(", "-", "1", ")", "\n", "predicting_stream_relative_positions", "=", "predicting_stream_relative_positions", "-", "real_positions_predicting_stream", "\n", "i_buckets_main_stream", "=", "self", ".", "_relative_positions_bucket", "(", "main_stream_relative_positions", ",", "bidirectional", "=", "False", ")", "\n", "i_bucket_relative_stream", "=", "self", ".", "_relative_positions_bucket", "(", "predicting_stream_relative_positions", ",", "\n", "bidirectional", "=", "False", ")", "\n", "return", "i_buckets_main_stream", ",", "i_bucket_relative_stream", "\n", "\n", "", "def", "cal_finetune_relative_positions", "(", "self", ",", "real_positions", ")", ":", "\n", "        ", "n_tokens", "=", "real_positions", ".", "size", "(", "-", "1", ")", "\n", "batch_size", "=", "real_positions", ".", "size", "(", "0", ")", "\n", "if", "not", "hasattr", "(", "self", ",", "\n", "'_finetune_i_bucket_main_stream'", ")", "or", "self", ".", "_finetune_i_bucket_main_stream", "is", "None", "or", "self", ".", "_finetune_i_bucket_main_stream", ".", "device", "!=", "real_positions", ".", "device", ":", "\n", "            ", "fake_positions", "=", "torch", ".", "arange", "(", "1", ",", "self", ".", "max_target_positions", "+", "1", ")", ".", "repeat", "(", "1", ",", "1", ")", "\n", "finetune_i_bucket_main_stream", ",", "finetune_i_bucket_predicting_stream", "=", "self", ".", "cal_pretrain_relative_positions", "(", "fake_positions", ")", "\n", "self", ".", "_finetune_i_bucket_main_stream", "=", "finetune_i_bucket_main_stream", ".", "to", "(", "real_positions", ".", "device", ")", "\n", "self", ".", "_finetune_i_bucket_predicting_stream", "=", "finetune_i_bucket_predicting_stream", ".", "to", "(", "real_positions", ".", "device", ")", "\n", "", "finetune_i_bucket_main_stream", "=", "self", ".", "_finetune_i_bucket_main_stream", "[", ":", ",", ":", "n_tokens", ",", ":", "n_tokens", "]", ".", "repeat", "(", "batch_size", ",", "\n", "1", ",", "1", ")", "\n", "finetune_i_bucket_predicting_stream", "=", "torch", ".", "cat", "(", "[", "\n", "self", ".", "_finetune_i_bucket_predicting_stream", "[", ":", ",", ":", "n_tokens", ",", ":", "n_tokens", "]", ",", "\n", "self", ".", "_finetune_i_bucket_predicting_stream", "[", ":", ",", ":", "n_tokens", ",", "\n", "self", ".", "max_target_positions", ":", "self", ".", "max_target_positions", "+", "n_tokens", "]", "\n", "]", ",", "2", ")", ".", "repeat", "(", "batch_size", ",", "1", ",", "1", ")", "\n", "return", "finetune_i_bucket_main_stream", ",", "finetune_i_bucket_predicting_stream", "\n", "\n", "", "def", "extract_features", "(", "self", ",", "prev_output_tokens", ",", "encoder_out", "=", "None", ",", "incremental_state", "=", "None", ",", "**", "unused", ")", ":", "\n", "# embed positions", "\n", "# [bos, A, B, C, D, eos] with real positions [1,2,3,4,5,6](main stream), [2,3,4,5,6,7](predicting stream)", "\n", "# target [B,C,D] with prev [A,B,C] from [A,B,C,D] as pretraining span with real positions [2,3,4],", "\n", "# but target actually [3,4,5] for fine tune with another [bos].", "\n", "# thus [2,3,4] used for main stream shifted prev [A,B,C], [3,4,5] used for predicting [B,C,D]", "\n", "        ", "if", "'positions'", "in", "unused", ":", "\n", "# pretrain procedure", "\n", "            ", "main_stream_pos_embed", "=", "self", ".", "embed_positions", ".", "_forward", "(", "unused", "[", "'positions'", "]", ")", "\n", "real_positions", "=", "unused", "[", "'positions'", "]", "\n", "i_buckets_main_stream", ",", "i_bucket_relative_stream", "=", "self", ".", "cal_pretrain_relative_positions", "(", "real_positions", ")", "\n", "", "else", ":", "\n", "# fine tune procedure", "\n", "            ", "main_stream_pos_embed", ",", "real_positions", "=", "self", ".", "embed_positions", "(", "\n", "prev_output_tokens", ",", "\n", "incremental_state", "=", "incremental_state", ",", "\n", ")", "if", "self", ".", "embed_positions", "is", "not", "None", "else", "None", "\n", "if", "incremental_state", "is", "not", "None", ":", "\n", "                ", "i_buckets_main_stream", ",", "i_bucket_relative_stream", "=", "None", ",", "None", "\n", "", "else", ":", "\n", "                ", "i_buckets_main_stream", ",", "i_bucket_relative_stream", "="]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_s2s_model.NgramTransformerDecoder.get_normalized_probs": [[789, 812], ["hasattr", "ngram_s2s_model.NgramTransformerDecoder.adaptive_softmax.get_log_prob", "fairseq.utils.log_softmax", "fairseq.utils.softmax", "ngram_s2s_model.NgramTransformerDecoder.exp_"], "methods", ["None"], ["self", ".", "cal_finetune_relative_positions", "(", "real_positions", ")", "\n", "\n", "", "", "predicting_stream_pos_embed", "=", "self", ".", "embed_positions", ".", "_forward", "(", "real_positions", "+", "1", ")", "\n", "\n", "if", "incremental_state", "is", "not", "None", ":", "\n", "            ", "prev_output_tokens", "=", "prev_output_tokens", "[", ":", ",", "-", "1", ":", "]", "\n", "if", "main_stream_pos_embed", "is", "not", "None", ":", "\n", "                ", "main_stream_pos_embed", "=", "main_stream_pos_embed", "[", ":", ",", "-", "1", ":", "]", "\n", "\n", "# ATT:", "\n", "", "", "if", "'new_decoder_input_emb'", "in", "unused", "and", "not", "unused", "[", "\"new_decoder_input_emb\"", "]", "is", "None", ":", "\n", "            ", "x", "=", "unused", "[", "\"new_decoder_input_emb\"", "]", "\n", "", "else", ":", "\n", "            ", "x", "=", "self", ".", "embed_tokens", "(", "prev_output_tokens", ")", "\n", "# embed tokens and positions", "\n", "", "if", "self", ".", "embed_scale", "is", "not", "None", ":", "\n", "            ", "x", "*=", "self", ".", "embed_scale", "\n", "\n", "", "if", "main_stream_pos_embed", "is", "not", "None", ":", "\n", "            ", "x", "+=", "main_stream_pos_embed", "\n", "\n", "# B x T x C -> T x B x C", "\n", "", "x", "=", "x", ".", "transpose", "(", "0", ",", "1", ")", "\n", "attn", "=", "None", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_s2s_model.NgramTransformerDecoder.output_layer": [[814, 821], ["torch.linear", "torch.linear", "torch.linear", "torch.linear", "torch.linear", "torch.linear"], "methods", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.layers.nn.linear", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.layers.nn.linear", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.layers.nn.linear", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.layers.nn.linear", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.layers.nn.linear", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.layers.nn.linear"], ["inner_states", "=", "[", "x", "]", "\n", "if", "main_stream_pos_embed", "is", "None", ":", "\n", "            ", "print", "(", "'positions should be used to predict ngrams'", ")", "\n", "raise", "Exception", "(", ")", "\n", "\n", "", "if", "self", ".", "embed_scale", "is", "not", "None", ":", "\n", "            ", "ngram_input_embed", "=", "self", ".", "embed_scale", "*", "self", ".", "ngram_input_embed", ".", "weight", "\n", "", "else", ":", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_s2s_model.NgramTransformerDecoder.max_positions": [[822, 827], ["min", "ngram_s2s_model.NgramTransformerDecoder.embed_positions.max_positions"], "methods", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_s2s_model.NgramTransformerDecoder.max_positions"], ["            ", "ngram_input_embed", "=", "self", ".", "ngram_input_embed", ".", "weight", "\n", "\n", "", "if", "incremental_state", "is", "not", "None", ":", "\n", "            ", "B", "=", "x", ".", "size", "(", "1", ")", "\n", "ngram_masks", "=", "[", "\n", "(", "ngram_input_embed", "[", "ngram", "-", "1", "]", "+", "predicting_stream_pos_embed", ")", ".", "transpose", "(", "0", ",", "1", ")", ".", "repeat", "(", "1", ",", "B", ",", "1", ")", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_s2s_model.NgramTransformerDecoder.buffered_future_mask": [[828, 835], ["tensor.size", "torch.triu", "torch.triu", "torch.triu", "torch.triu", "torch.triu", "torch.triu", "torch.triu", "torch.triu", "torch.triu", "hasattr", "ngram_s2s_model.NgramTransformerDecoder._future_mask.size", "fairseq.utils.fill_with_neg_inf", "tensor.new"], "methods", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.distribute.size", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.distribute.size"], ["for", "ngram", "in", "range", "(", "self", ".", "ngram", ")", "]", "\n", "", "else", ":", "\n", "            ", "ngram_masks", "=", "[", "(", "ngram_input_embed", "[", "ngram", "-", "1", "]", "+", "predicting_stream_pos_embed", ")", ".", "transpose", "(", "0", ",", "1", ")", "for", "\n", "ngram", "in", "range", "(", "self", ".", "ngram", ")", "]", "\n", "\n", "", "self_attn_mask", "=", "self", ".", "buffered_future_mask", "(", "x", ")", "if", "incremental_state", "is", "None", "else", "None", "\n", "ngram_mask_matrix", "=", "self", ".", "buffered_future_mask_ngram", "(", "x", ")", "if", "incremental_state", "is", "None", "else", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_s2s_model.NgramTransformerDecoder.buffered_future_mask_ngram": [[836, 847], ["tensor.size", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "ngram_multihead_attention.ngram_attention_bias().type().to", "hasattr", "ngram_multihead_attention.ngram_attention_bias().type", "ngram_multihead_attention.ngram_attention_bias"], "methods", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.distribute.size", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_multihead_attention.ngram_attention_bias"], ["# TODO in train [(1+ngram)*T, B, C], in inference [T+ngram, B, C]", "\n", "x", "=", "torch", ".", "cat", "(", "[", "x", "]", "+", "ngram_masks", ",", "0", ")", "\n", "\n", "if", "self", ".", "emb_layer_norm", ":", "\n", "            ", "x", "=", "self", ".", "emb_layer_norm", "(", "x", ")", "\n", "\n", "", "x", "=", "F", ".", "dropout", "(", "x", ",", "p", "=", "self", ".", "dropout", ",", "training", "=", "self", ".", "training", ")", "\n", "\n", "# decoder layers", "\n", "for", "layer", "in", "self", ".", "layers", ":", "\n", "            ", "x", ",", "attn", "=", "layer", "(", "\n", "x", ",", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_s2s_model.Embedding": [[849, 854], ["torch.Embedding", "torch.init.normal_", "torch.init.constant_"], "function", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_s2s_model.Embedding"], ["encoder_out", "[", "'encoder_padding_mask'", "]", "if", "encoder_out", "is", "not", "None", "else", "None", ",", "\n", "incremental_state", ",", "\n", "self_attn_mask", "=", "self_attn_mask", ",", "\n", "ngram_mask_matrix", "=", "ngram_mask_matrix", ",", "\n", "i_buckets_main_stream", "=", "i_buckets_main_stream", ",", "\n", "i_bucket_relative_stream", "=", "i_bucket_relative_stream", ",", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_s2s_model.Linear": [[856, 862], ["torch.Linear", "torch.init.xavier_uniform_", "torch.init.constant_"], "function", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_s2s_model.Linear"], [")", "\n", "inner_states", ".", "append", "(", "x", ")", "\n", "\n", "# TODO [(1+ngram)*T, B, C] -> [B, (1+ngram)*T, C]", "\n", "", "x_list", "=", "x", ".", "transpose", "(", "0", ",", "1", ")", ".", "chunk", "(", "1", "+", "self", ".", "ngram", ",", "1", ")", "\n", "if", "attn", "is", "not", "None", ":", "\n", "            ", "attn_list", "=", "attn", ".", "transpose", "(", "0", ",", "1", ")", ".", "chunk", "(", "1", "+", "self", ".", "ngram", ",", "1", ")", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_s2s_model.base_architecture": [[864, 888], ["fairseq.models.register_model_architecture", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr"], "function", ["None"], ["            ", "attn_list", "=", "None", "\n", "\n", "", "return", "x_list", ",", "{", "'attn'", ":", "attn_list", "}", "\n", "\n", "", "def", "get_normalized_probs", "(", "self", ",", "net_output", ",", "log_probs", ",", "sample", ")", ":", "\n", "        ", "\"\"\"Get normalized probabilities (or log probs) from a net's output.\"\"\"", "\n", "\n", "if", "hasattr", "(", "self", ",", "'adaptive_softmax'", ")", "and", "self", ".", "adaptive_softmax", "is", "not", "None", ":", "\n", "            ", "if", "sample", "is", "not", "None", ":", "\n", "                ", "assert", "'target'", "in", "sample", "\n", "target", "=", "sample", "[", "'target'", "]", "\n", "", "else", ":", "\n", "                ", "target", "=", "None", "\n", "", "out", "=", "self", ".", "adaptive_softmax", ".", "get_log_prob", "(", "net_output", "[", "0", "]", ",", "target", "=", "target", ")", "\n", "return", "out", ".", "exp_", "(", ")", "if", "not", "log_probs", "else", "out", "\n", "", "'''\n        logits_list = net_output[0]\n        if log_probs:\n            return [utils.log_softmax(logits, dim=-1, onnx_trace=self.onnx_trace) for logits in logits_list][0]\n        else:\n            return [utils.softmax(logits, dim=-1, onnx_trace=self.onnx_trace) for logits in logits_list][0]\n        '''", "\n", "logits", "=", "net_output", "[", "0", "]", "\n", "if", "log_probs", ":", "\n", "            ", "return", "utils", ".", "log_softmax", "(", "logits", ",", "dim", "=", "-", "1", ",", "onnx_trace", "=", "self", ".", "onnx_trace", ")", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_s2s_model.transformer_base": [[890, 914], ["fairseq.models.register_model_architecture", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "ngram_s2s_model.base_architecture"], "function", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_s2s_model.base_architecture"], ["            ", "return", "utils", ".", "softmax", "(", "logits", ",", "dim", "=", "-", "1", ",", "onnx_trace", "=", "self", ".", "onnx_trace", ")", "\n", "\n", "\n", "", "", "def", "output_layer", "(", "self", ",", "features", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"Project features to the vocabulary size.\"\"\"", "\n", "# project back to size of vocabulary", "\n", "if", "self", ".", "share_input_output_embed", ":", "\n", "            ", "return", "F", ".", "linear", "(", "features", ",", "self", ".", "embed_tokens", ".", "weight", ")", "\n", "", "else", ":", "\n", "            ", "return", "F", ".", "linear", "(", "features", ",", "self", ".", "embed_out", ")", "\n", "\n", "", "", "def", "max_positions", "(", "self", ")", ":", "\n", "        ", "\"\"\"Maximum output length supported by the decoder.\"\"\"", "\n", "if", "self", ".", "embed_positions", "is", "None", ":", "\n", "            ", "return", "self", ".", "max_target_positions", "\n", "", "return", "min", "(", "self", ".", "max_target_positions", ",", "self", ".", "embed_positions", ".", "max_positions", "(", ")", ")", "\n", "\n", "", "def", "buffered_future_mask", "(", "self", ",", "tensor", ")", ":", "\n", "        ", "dim", "=", "tensor", ".", "size", "(", "0", ")", "\n", "if", "not", "hasattr", "(", "self", ",", "\n", "'_future_mask'", ")", "or", "self", ".", "_future_mask", "is", "None", "or", "self", ".", "_future_mask", ".", "device", "!=", "tensor", ".", "device", "or", "self", ".", "_future_mask", ".", "size", "(", "\n", "0", ")", "<", "dim", ":", "\n", "            ", "self", ".", "_future_mask", "=", "torch", ".", "triu", "(", "utils", ".", "fill_with_neg_inf", "(", "tensor", ".", "new", "(", "dim", ",", "dim", ")", ")", ",", "1", ")", "\n", "", "return", "self", ".", "_future_mask", "[", ":", "dim", ",", ":", "dim", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_s2s_model.transformer_middle": [[916, 932], ["fairseq.models.register_model_architecture", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "ngram_s2s_model.transformer_base"], "function", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_s2s_model.transformer_base"], ["        ", "dim", "=", "tensor", ".", "size", "(", "0", ")", "\n", "if", "not", "hasattr", "(", "self", ",", "\n", "'_ngram_future_mask'", ")", "or", "self", ".", "_ngram_future_mask", "is", "None", "or", "self", ".", "_ngram_future_mask", ".", "device", "!=", "tensor", ".", "device", ":", "\n", "            ", "self", ".", "_ngram_future_mask", "=", "ngram_attention_bias", "(", "self", ".", "max_target_positions", ",", "self", ".", "ngram", ")", ".", "type", "(", "tensor", ".", "dtype", ")", ".", "to", "(", "\n", "tensor", ".", "device", ")", "\n", "", "ngram_future_mask", "=", "torch", ".", "cat", "(", "[", "self", ".", "_ngram_future_mask", "[", ":", ",", ":", "dim", ",", ":", "dim", "]", ",", "\n", "self", ".", "_ngram_future_mask", "[", ":", ",", ":", "dim", ",", "\n", "self", ".", "max_target_positions", ":", "self", ".", "max_target_positions", "+", "dim", "]", "\n", "]", ",", "2", ")", "\n", "return", "ngram_future_mask", "\n", "\n", "\n", "", "", "def", "Embedding", "(", "num_embeddings", ",", "embedding_dim", ",", "padding_idx", ")", ":", "\n", "    ", "m", "=", "nn", ".", "Embedding", "(", "num_embeddings", ",", "embedding_dim", ",", "padding_idx", "=", "padding_idx", ")", "\n", "nn", ".", "init", ".", "normal_", "(", "m", ".", "weight", ",", "mean", "=", "0", ",", "std", "=", "embedding_dim", "**", "-", "0.5", ")", "\n", "nn", ".", "init", ".", "constant_", "(", "m", ".", "weight", "[", "padding_idx", "]", ",", "0", ")", "\n", "return", "m", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_s2s_model.transformer_big": [[934, 943], ["fairseq.models.register_model_architecture", "getattr", "getattr", "getattr", "getattr", "getattr", "ngram_s2s_model.transformer_middle"], "function", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_s2s_model.transformer_middle"], ["\n", "", "def", "Linear", "(", "in_features", ",", "out_features", ",", "bias", "=", "True", ")", ":", "\n", "    ", "m", "=", "nn", ".", "Linear", "(", "in_features", ",", "out_features", ",", "bias", ")", "\n", "nn", ".", "init", ".", "xavier_uniform_", "(", "m", ".", "weight", ")", "\n", "if", "bias", ":", "\n", "        ", "nn", ".", "init", ".", "constant_", "(", "m", ".", "bias", ",", "0.", ")", "\n", "", "return", "m", "\n", "\n", "\n", "", "@", "register_model_architecture", "(", "'ngram_transformer_prophet'", ",", "'ngram_transformer_prophet'", ")", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_multihead_attention.NgramMultiheadAttention.__init__": [[38, 89], ["torch.nn.Module.__init__", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "ngram_multihead_attention.NgramMultiheadAttention.reset_parameters", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "ngram_multihead_attention.NgramMultiheadAttention.register_parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor"], "methods", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.transformer_lrp.TransformerLRP.__init__", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_s2s_model.Linear", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_s2s_model.Linear", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_s2s_model.Linear", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_s2s_model.Linear", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_multihead_attention.NgramMultiheadAttention.reset_parameters"], ["def", "__init__", "(", "self", ",", "embed_dim", ",", "num_heads", ",", "kdim", "=", "None", ",", "vdim", "=", "None", ",", "dropout", "=", "0.", ",", "bias", "=", "True", ",", "\n", "add_bias_kv", "=", "False", ",", "add_zero_attn", "=", "False", ",", "self_attention", "=", "False", ",", "\n", "encoder_decoder_attention", "=", "False", ",", "ngram", "=", "2", ",", "num_buckets", "=", "32", ",", "relative_max_distance", "=", "128", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "embed_dim", "=", "embed_dim", "\n", "self", ".", "kdim", "=", "kdim", "if", "kdim", "is", "not", "None", "else", "embed_dim", "\n", "self", ".", "vdim", "=", "vdim", "if", "vdim", "is", "not", "None", "else", "embed_dim", "\n", "self", ".", "qkv_same_dim", "=", "self", ".", "kdim", "==", "embed_dim", "and", "self", ".", "vdim", "==", "embed_dim", "\n", "\n", "self", ".", "num_buckets", "=", "num_buckets", "\n", "self", ".", "relative_max_distance", "=", "relative_max_distance", "\n", "self", ".", "num_heads", "=", "num_heads", "\n", "self", ".", "dropout", "=", "dropout", "\n", "self", ".", "head_dim", "=", "embed_dim", "//", "num_heads", "\n", "self", ".", "ngram", "=", "ngram", "\n", "\n", "assert", "self", ".", "head_dim", "*", "num_heads", "==", "self", ".", "embed_dim", ",", "\"embed_dim must be divisible by num_heads\"", "\n", "self", ".", "scaling", "=", "self", ".", "head_dim", "**", "-", "0.5", "\n", "\n", "self", ".", "self_attention", "=", "self_attention", "\n", "self", ".", "encoder_decoder_attention", "=", "encoder_decoder_attention", "\n", "\n", "assert", "not", "self", ".", "self_attention", "or", "self", ".", "qkv_same_dim", ",", "'Self-attention requires query, key and '", "'value to be of the same size'", "\n", "\n", "self", ".", "relative_linear", "=", "nn", ".", "Linear", "(", "embed_dim", ",", "num_buckets", "*", "num_heads", ")", "\n", "if", "self", ".", "qkv_same_dim", ":", "\n", "            ", "self", ".", "in_proj_weight", "=", "Parameter", "(", "torch", ".", "Tensor", "(", "3", "*", "embed_dim", ",", "embed_dim", ")", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "k_proj_weight", "=", "Parameter", "(", "torch", ".", "Tensor", "(", "embed_dim", ",", "self", ".", "kdim", ")", ")", "\n", "self", ".", "v_proj_weight", "=", "Parameter", "(", "torch", ".", "Tensor", "(", "embed_dim", ",", "self", ".", "vdim", ")", ")", "\n", "self", ".", "q_proj_weight", "=", "Parameter", "(", "torch", ".", "Tensor", "(", "embed_dim", ",", "embed_dim", ")", ")", "\n", "\n", "", "if", "bias", ":", "\n", "            ", "self", ".", "in_proj_bias", "=", "Parameter", "(", "torch", ".", "Tensor", "(", "3", "*", "embed_dim", ")", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "register_parameter", "(", "'in_proj_bias'", ",", "None", ")", "\n", "\n", "", "self", ".", "out_proj", "=", "nn", ".", "Linear", "(", "embed_dim", ",", "embed_dim", ",", "bias", "=", "bias", ")", "\n", "\n", "if", "add_bias_kv", ":", "\n", "            ", "self", ".", "bias_k", "=", "Parameter", "(", "torch", ".", "Tensor", "(", "1", ",", "1", ",", "embed_dim", ")", ")", "\n", "self", ".", "bias_v", "=", "Parameter", "(", "torch", ".", "Tensor", "(", "1", ",", "1", ",", "embed_dim", ")", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "bias_k", "=", "self", ".", "bias_v", "=", "None", "\n", "\n", "", "self", ".", "add_zero_attn", "=", "add_zero_attn", "\n", "\n", "self", ".", "reset_parameters", "(", ")", "\n", "\n", "self", ".", "onnx_trace", "=", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_multihead_attention.NgramMultiheadAttention.prepare_for_onnx_export_": [[90, 92], ["None"], "methods", ["None"], ["", "def", "prepare_for_onnx_export_", "(", "self", ")", ":", "\n", "        ", "self", ".", "onnx_trace", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_multihead_attention.NgramMultiheadAttention.reset_parameters": [[93, 109], ["torch.nn.init.xavier_uniform_", "torch.nn.init.xavier_uniform_", "torch.nn.init.xavier_uniform_", "torch.nn.init.xavier_uniform_", "torch.nn.init.xavier_uniform_", "torch.nn.init.xavier_uniform_", "torch.nn.init.xavier_uniform_", "torch.nn.init.xavier_uniform_", "torch.nn.init.xavier_uniform_", "torch.nn.init.xavier_uniform_", "torch.nn.init.constant_", "torch.nn.init.constant_", "torch.nn.init.constant_", "torch.nn.init.constant_", "torch.nn.init.xavier_normal_", "torch.nn.init.xavier_normal_", "torch.nn.init.xavier_normal_", "torch.nn.init.xavier_normal_"], "methods", ["None"], ["", "def", "reset_parameters", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "qkv_same_dim", ":", "\n", "            ", "nn", ".", "init", ".", "xavier_uniform_", "(", "self", ".", "in_proj_weight", ")", "\n", "", "else", ":", "\n", "            ", "nn", ".", "init", ".", "xavier_uniform_", "(", "self", ".", "k_proj_weight", ")", "\n", "nn", ".", "init", ".", "xavier_uniform_", "(", "self", ".", "v_proj_weight", ")", "\n", "nn", ".", "init", ".", "xavier_uniform_", "(", "self", ".", "q_proj_weight", ")", "\n", "\n", "", "nn", ".", "init", ".", "xavier_uniform_", "(", "self", ".", "out_proj", ".", "weight", ")", "\n", "if", "self", ".", "in_proj_bias", "is", "not", "None", ":", "\n", "            ", "nn", ".", "init", ".", "constant_", "(", "self", ".", "in_proj_bias", ",", "0.", ")", "\n", "nn", ".", "init", ".", "constant_", "(", "self", ".", "out_proj", ".", "bias", ",", "0.", ")", "\n", "", "if", "self", ".", "bias_k", "is", "not", "None", ":", "\n", "            ", "nn", ".", "init", ".", "xavier_normal_", "(", "self", ".", "bias_k", ")", "\n", "", "if", "self", ".", "bias_v", "is", "not", "None", ":", "\n", "            ", "nn", ".", "init", ".", "xavier_normal_", "(", "self", ".", "bias_v", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_multihead_attention.NgramMultiheadAttention._relative_positions_bucket": [[110, 129], ["torch.lt", "torch.lt", "torch.lt", "torch.lt", "torch.min", "torch.min", "torch.min", "torch.min", "val_if_large.int.int.int", "torch.abs", "torch.abs", "torch.abs", "torch.abs", "torch.max", "torch.max", "torch.max", "torch.max", "torch.where", "torch.where", "torch.where", "torch.where", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.ones_like", "torch.ones_like", "torch.ones_like", "torch.ones_like", "torch.max.int", "torch.max.int", "torch.lt().int", "torch.lt().int", "torch.lt().int", "torch.lt().int", "torch.log", "torch.log", "torch.log", "torch.log", "math.log", "torch.lt", "torch.lt", "torch.lt", "torch.lt", "torch.max.float", "torch.max.float", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like"], "methods", ["None"], ["", "", "def", "_relative_positions_bucket", "(", "self", ",", "relative_positions", ",", "bidirectional", "=", "False", ")", ":", "\n", "        ", "num_buckets", "=", "self", ".", "num_buckets", "\n", "max_distance", "=", "self", ".", "relative_max_distance", "\n", "n", "=", "-", "relative_positions", "\n", "result", "=", "0", "\n", "if", "bidirectional", ":", "\n", "            ", "num_buckets", "=", "num_buckets", "//", "2", "\n", "result", "=", "result", "+", "torch", ".", "lt", "(", "n", ",", "torch", ".", "zeros_like", "(", "n", ")", ")", ".", "int", "(", ")", "*", "num_buckets", "\n", "n", "=", "torch", ".", "abs", "(", "n", ")", "\n", "", "else", ":", "\n", "            ", "n", "=", "torch", ".", "max", "(", "n", ",", "torch", ".", "zeros_like", "(", "n", ")", ")", "\n", "", "max_exact", "=", "num_buckets", "//", "2", "\n", "is_small", "=", "torch", ".", "lt", "(", "n", ",", "max_exact", ")", "\n", "val_if_large", "=", "max_exact", "+", "torch", ".", "log", "(", "n", ".", "float", "(", ")", "/", "max_exact", ")", "/", "math", ".", "log", "(", "max_distance", "/", "max_exact", ")", "*", "(", "\n", "num_buckets", "-", "max_exact", ")", "\n", "val_if_large", "=", "torch", ".", "min", "(", "val_if_large", ",", "torch", ".", "ones_like", "(", "val_if_large", ")", "*", "(", "num_buckets", "-", "1", ")", ")", "\n", "val_if_large", "=", "val_if_large", ".", "int", "(", ")", "\n", "result", "=", "result", "+", "torch", ".", "where", "(", "is_small", ",", "n", ".", "int", "(", ")", ",", "val_if_large", ")", "\n", "return", "result", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_multihead_attention.NgramMultiheadAttention.main_stream_relative_logits": [[131, 176], ["query.transpose.transpose.size", "attn_weights.size", "query.transpose.transpose.transpose", "ngram_multihead_attention.NgramMultiheadAttention.relative_linear", "values.reshape.reshape.view", "values.reshape.reshape.transpose", "values.reshape.reshape.transpose", "values.reshape.reshape.reshape", "ngram_multihead_attention.NgramMultiheadAttention.repeat().view", "values.reshape.reshape.reshape", "ngram_multihead_attention.NgramMultiheadAttention.view().long", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "result.view.view.view", "torch.arange().unsqueeze().unsqueeze().repeat().to", "torch.arange().unsqueeze().unsqueeze().repeat().to", "torch.arange().unsqueeze().unsqueeze().repeat().to", "torch.arange().unsqueeze().unsqueeze().repeat().to", "real_positions.unsqueeze().repeat.unsqueeze().repeat.unsqueeze().repeat", "ngram_multihead_attention.NgramMultiheadAttention._relative_positions_bucket", "values.reshape.reshape.size", "values.reshape.reshape.size", "attn_weights.size", "attn_weights.size", "attn_weights.size", "attn_weights.size", "values.reshape.reshape.size", "attn_weights.size", "attn_weights.size", "ngram_multihead_attention.NgramMultiheadAttention.repeat", "ngram_multihead_attention.NgramMultiheadAttention.view", "torch.arange().unsqueeze().unsqueeze().repeat", "torch.arange().unsqueeze().unsqueeze().repeat", "torch.arange().unsqueeze().unsqueeze().repeat", "torch.arange().unsqueeze().unsqueeze().repeat", "real_positions.unsqueeze().repeat.unsqueeze().repeat.unsqueeze", "ngram_multihead_attention.NgramMultiheadAttention.size", "torch.arange().unsqueeze().unsqueeze", "torch.arange().unsqueeze().unsqueeze", "torch.arange().unsqueeze().unsqueeze", "torch.arange().unsqueeze().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange", "torch.arange", "torch.arange", "torch.arange"], "methods", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.distribute.size", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.distribute.size", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_multihead_attention.NgramMultiheadAttention._relative_positions_bucket", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.distribute.size", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.distribute.size", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.distribute.size", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.distribute.size", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.distribute.size", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.distribute.size", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.distribute.size", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.distribute.size", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.distribute.size", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.distribute.size"], ["", "def", "main_stream_relative_logits", "(", "self", ",", "query", ",", "attn_weights", ",", "real_positions", ",", "i_bucket_main_stream", ")", ":", "\n", "# input query [T,B,C]", "\n", "# input attn_weights [T*head,T,S]", "\n", "# input real_positions [B,T] or [1,1]", "\n", "\n", "        ", "T", ",", "B", ",", "_", "=", "query", ".", "size", "(", ")", "\n", "S", "=", "attn_weights", ".", "size", "(", "-", "1", ")", "\n", "\n", "if", "i_bucket_main_stream", "is", "not", "None", ":", "\n", "            ", "i_buckets", "=", "i_bucket_main_stream", "\n", "", "else", ":", "\n", "# [B,T,S]", "\n", "            ", "relative_positions", "=", "torch", ".", "arange", "(", "1", ",", "S", "+", "1", ")", ".", "unsqueeze", "(", "0", ")", ".", "unsqueeze", "(", "0", ")", ".", "repeat", "(", "B", ",", "T", ",", "1", ")", ".", "to", "(", "real_positions", ".", "device", ")", "\n", "# [B,T,1]", "\n", "real_positions", "=", "real_positions", ".", "unsqueeze", "(", "0", ")", ".", "repeat", "(", "B", ",", "T", ",", "1", ")", "\n", "# [B,T,S]", "\n", "relative_positions", "=", "relative_positions", "-", "real_positions", "\n", "# [B,T,T]", "\n", "i_buckets", "=", "self", ".", "_relative_positions_bucket", "(", "relative_positions", ",", "False", ")", "\n", "\n", "# [B,T,C]", "\n", "", "query", "=", "query", ".", "transpose", "(", "0", ",", "1", ")", "\n", "# [B,T,Buckets*head]", "\n", "values", "=", "self", ".", "relative_linear", "(", "query", ")", "\n", "# [B,T,Buckets,head]", "\n", "values", "=", "values", ".", "view", "(", "values", ".", "size", "(", "0", ")", ",", "values", ".", "size", "(", "1", ")", ",", "self", ".", "num_buckets", ",", "self", ".", "num_heads", ")", "\n", "# [B,head,Buckets,T]", "\n", "values", "=", "values", ".", "transpose", "(", "1", ",", "3", ")", "\n", "# [B,head,T,Buckets]", "\n", "values", "=", "values", ".", "transpose", "(", "2", ",", "3", ")", "\n", "# [B*head,T,Buckets]", "\n", "values", "=", "values", ".", "reshape", "(", "attn_weights", ".", "size", "(", "0", ")", ",", "attn_weights", ".", "size", "(", "1", ")", ",", "-", "1", ")", "\n", "\n", "# => [B,head*T,T] => [B*head,T,T]", "\n", "i_buckets", "=", "i_buckets", ".", "repeat", "(", "1", ",", "self", ".", "num_heads", ",", "1", ")", ".", "view", "(", "attn_weights", ".", "size", "(", "0", ")", ",", "attn_weights", ".", "size", "(", "1", ")", ",", "-", "1", ")", "\n", "# [B*head*T,Buckets]", "\n", "values", "=", "values", ".", "reshape", "(", "-", "1", ",", "values", ".", "size", "(", "-", "1", ")", ")", "\n", "# [B*head*T,T]", "\n", "i_buckets", "=", "i_buckets", ".", "view", "(", "-", "1", ",", "i_buckets", ".", "size", "(", "-", "1", ")", ")", ".", "long", "(", ")", "\n", "# [B*head*T,T]", "\n", "result", "=", "torch", ".", "gather", "(", "values", ",", "dim", "=", "1", ",", "index", "=", "i_buckets", ")", "\n", "# [B*head,T,T]", "\n", "result", "=", "result", ".", "view", "(", "attn_weights", ".", "size", "(", "0", ")", ",", "attn_weights", ".", "size", "(", "1", ")", ",", "-", "1", ")", "\n", "\n", "return", "result", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_multihead_attention.NgramMultiheadAttention.ngram_relative_logits": [[177, 222], ["query.transpose.transpose.size", "attn_weights.size", "query.transpose.transpose.transpose", "ngram_multihead_attention.NgramMultiheadAttention.relative_linear", "values.reshape.reshape.view", "values.reshape.reshape.permute", "values.reshape.reshape.reshape", "ngram_multihead_attention.NgramMultiheadAttention.unsqueeze().repeat", "values.reshape.reshape.reshape", "ngram_multihead_attention.NgramMultiheadAttention.view().long", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "result.view.view.view", "torch.arange().unsqueeze().unsqueeze().repeat().to", "torch.arange().unsqueeze().unsqueeze().repeat().to", "torch.arange().unsqueeze().unsqueeze().repeat().to", "torch.arange().unsqueeze().unsqueeze().repeat().to", "real_positions.unsqueeze().repeat.unsqueeze().repeat.unsqueeze().repeat", "ngram_multihead_attention.NgramMultiheadAttention._relative_positions_bucket", "values.reshape.reshape.size", "ngram_multihead_attention.NgramMultiheadAttention.unsqueeze", "ngram_multihead_attention.NgramMultiheadAttention.view", "torch.arange().unsqueeze().unsqueeze().repeat", "torch.arange().unsqueeze().unsqueeze().repeat", "torch.arange().unsqueeze().unsqueeze().repeat", "torch.arange().unsqueeze().unsqueeze().repeat", "real_positions.unsqueeze().repeat.unsqueeze().repeat.unsqueeze", "values.reshape.reshape.size", "ngram_multihead_attention.NgramMultiheadAttention.size", "torch.arange().unsqueeze().unsqueeze", "torch.arange().unsqueeze().unsqueeze", "torch.arange().unsqueeze().unsqueeze", "torch.arange().unsqueeze().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange", "torch.arange", "torch.arange", "torch.arange"], "methods", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.distribute.size", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.distribute.size", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_multihead_attention.NgramMultiheadAttention._relative_positions_bucket", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.distribute.size", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.distribute.size", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.distribute.size"], ["", "def", "ngram_relative_logits", "(", "self", ",", "query", ",", "attn_weights", ",", "real_positions", ",", "i_bucket_relative_stream", ")", ":", "\n", "# input query [ngram, T,B,C]", "\n", "# input attn_weights [ngram, B*head,T,S]", "\n", "# input real_positions [B,T] or [1,1]", "\n", "# input i_bucket_relative_stream [B,T, 2*T] or None", "\n", "\n", "        ", "N", ",", "T", ",", "B", ",", "_", "=", "query", ".", "size", "(", ")", "\n", "_", ",", "BH", ",", "_", ",", "S", "=", "attn_weights", ".", "size", "(", ")", "\n", "\n", "if", "i_bucket_relative_stream", "is", "not", "None", ":", "\n", "            ", "i_buckets", "=", "i_bucket_relative_stream", "\n", "", "else", ":", "\n", "# [B,T,S]", "\n", "            ", "assert", "real_positions", "[", "0", "]", "[", "0", "]", "==", "S", "-", "1", ",", "'memory position is 1 2 3 4 5(S-1)'", "\n", "relative_positions", "=", "torch", ".", "arange", "(", "0", ",", "S", ")", ".", "unsqueeze", "(", "0", ")", ".", "unsqueeze", "(", "0", ")", ".", "repeat", "(", "B", ",", "T", ",", "1", ")", ".", "to", "(", "real_positions", ".", "device", ")", "\n", "# print('relative_positions', relative_positions)", "\n", "# [B,T,1]", "\n", "real_positions", "=", "real_positions", ".", "unsqueeze", "(", "0", ")", ".", "repeat", "(", "B", ",", "T", ",", "1", ")", "\n", "relative_positions", "=", "relative_positions", "\n", "# [B,T,2*T] or [B,T,S]", "\n", "relative_positions", "=", "relative_positions", "-", "real_positions", "\n", "i_buckets", "=", "self", ".", "_relative_positions_bucket", "(", "relative_positions", ",", "False", ")", "\n", "\n", "# [ngram, B, T, C]", "\n", "", "query", "=", "query", ".", "transpose", "(", "1", ",", "2", ")", "\n", "# [ngram, B, T, bucket*head]", "\n", "values", "=", "self", ".", "relative_linear", "(", "query", ")", "\n", "# [ngram, B, T, bucket, head]", "\n", "values", "=", "values", ".", "view", "(", "*", "values", ".", "size", "(", ")", "[", ":", "-", "1", "]", ",", "self", ".", "num_buckets", ",", "self", ".", "num_heads", ")", "\n", "# [ngram, B, head, T, bucket]", "\n", "values", "=", "values", ".", "permute", "(", "0", ",", "1", ",", "4", ",", "2", ",", "3", ")", "\n", "# [ngram*B*head, T, bucket]", "\n", "values", "=", "values", ".", "reshape", "(", "N", "*", "BH", ",", "T", ",", "-", "1", ")", "\n", "\n", "# [ngram, B, head*T, S]", "\n", "i_buckets", "=", "i_buckets", ".", "unsqueeze", "(", "0", ")", ".", "repeat", "(", "N", ",", "1", ",", "self", ".", "num_heads", ",", "1", ")", "\n", "\n", "values", "=", "values", ".", "reshape", "(", "-", "1", ",", "values", ".", "size", "(", "-", "1", ")", ")", "\n", "i_buckets", "=", "i_buckets", ".", "view", "(", "-", "1", ",", "i_buckets", ".", "size", "(", "-", "1", ")", ")", ".", "long", "(", ")", "\n", "# [ngram*B*head*T, S]", "\n", "result", "=", "torch", ".", "gather", "(", "values", ",", "dim", "=", "1", ",", "index", "=", "i_buckets", ")", "\n", "# [ngram, B*head, T, S]", "\n", "result", "=", "result", ".", "view", "(", "N", ",", "BH", ",", "T", ",", "-", "1", ")", "\n", "\n", "return", "result", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_multihead_attention.NgramMultiheadAttention.forward": [[223, 365], ["query.size", "ngram_multihead_attention.NgramMultiheadAttention.in_proj_qkv", "q.contiguous().view().transpose.contiguous().view().transpose.contiguous().view().transpose", "query.chunk", "q.contiguous().view().transpose.contiguous().view().transpose.chunk", "k.contiguous().view().transpose.contiguous().view().transpose.chunk", "v.contiguous().view().transpose.contiguous().view().transpose.chunk", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "ngram_multihead_attention.NgramMultiheadAttention.main_stream_relative_logits", "fairseq.utils.softmax().type_as", "torch.dropout", "torch.dropout", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "ngram_multihead_attention.NgramMultiheadAttention.transpose().contiguous().view", "ngram_multihead_attention.NgramMultiheadAttention.out_proj", "torch.cat().view", "torch.cat().view", "torch.cat().view", "torch.cat().view", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat().view", "torch.cat().view", "torch.cat().view", "torch.cat().view", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "ngram_multihead_attention.NgramMultiheadAttention.ngram_relative_logits", "fairseq.utils.softmax().type_as", "torch.dropout", "torch.dropout", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "ngram_multihead_attention.NgramMultiheadAttention.transpose().contiguous().view", "ngram_multihead_attention.NgramMultiheadAttention.out_proj", "attn_result.append", "attn_result.append", "torch.cat().view", "torch.cat().view", "torch.cat().view", "torch.cat().view", "list", "ngram_multihead_attention.NgramMultiheadAttention._get_input_buffer", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "k.contiguous().view().transpose.contiguous().view().transpose.contiguous().view().transpose", "v.contiguous().view().transpose.contiguous().view().transpose.contiguous().view().transpose", "torch.cat.view", "torch.cat.view", "torch.cat.view", "torch.cat.view", "ngram_multihead_attention.NgramMultiheadAttention._set_input_buffer", "torch.cat.transpose", "torch.cat.transpose", "ngram_multihead_attention.NgramMultiheadAttention.unsqueeze", "ngram_mask_matrix.unsqueeze.unsqueeze.unsqueeze", "query.size", "q.contiguous().view().transpose.contiguous().view().transpose.contiguous().view", "saved_state[].view", "saved_state[].view", "fairseq.utils.softmax", "ngram_multihead_attention.NgramMultiheadAttention.transpose().contiguous", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat().unsqueeze", "torch.cat().unsqueeze", "torch.cat().unsqueeze", "torch.cat().unsqueeze", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat().unsqueeze", "torch.cat().unsqueeze", "torch.cat().unsqueeze", "torch.cat().unsqueeze", "fairseq.utils.softmax", "ngram_multihead_attention.NgramMultiheadAttention.transpose().contiguous", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "ngram_multihead_attention.NgramMultiheadAttention.bias_k.repeat", "ngram_multihead_attention.NgramMultiheadAttention.bias_v.repeat", "k.contiguous().view().transpose.contiguous().view().transpose.contiguous().view", "v.contiguous().view().transpose.contiguous().view().transpose.contiguous().view", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "q.contiguous().view().transpose.contiguous().view().transpose.contiguous", "ngram_multihead_attention.NgramMultiheadAttention.transpose", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "ngram_multihead_attention.NgramMultiheadAttention.transpose", "k.contiguous().view().transpose.contiguous().view().transpose.contiguous", "v.contiguous().view().transpose.contiguous().view().transpose.contiguous"], "methods", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.distribute.size", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_multihead_attention.NgramMultiheadAttention.in_proj_qkv", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_multihead_attention.NgramMultiheadAttention.main_stream_relative_logits", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_multihead_attention.NgramMultiheadAttention.ngram_relative_logits", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_multihead_attention.NgramMultiheadAttention._get_input_buffer", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_multihead_attention.NgramMultiheadAttention._set_input_buffer", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.distribute.size"], ["", "def", "forward", "(", "self", ",", "query", ",", "key", ",", "value", ",", "key_padding_mask", "=", "None", ",", "incremental_state", "=", "None", ",", "\n", "need_weights", "=", "True", ",", "static_kv", "=", "False", ",", "\n", "self_attn_mask", "=", "None", ",", "\n", "ngram_mask_matrix", "=", "None", ",", "\n", "i_buckets_main_stream", "=", "None", ",", "\n", "i_bucket_relative_stream", "=", "None", ",", "\n", "real_positions", "=", "None", "\n", ")", ":", "\n", "        ", "\"\"\"Input shape: Time x Batch x Channel\n\n        Timesteps can be masked by supplying a T x T mask in the\n        `attn_mask` argument. Padding elements can be excluded from\n        the key by passing a binary ByteTensor (`key_padding_mask`) with shape:\n        batch x src_len, where padding elements are indicated by 1s.\n        \"\"\"", "\n", "\n", "tgt_len", ",", "bsz", ",", "embed_dim", "=", "query", ".", "size", "(", ")", "\n", "assert", "embed_dim", "==", "self", ".", "embed_dim", "\n", "assert", "list", "(", "query", ".", "size", "(", ")", ")", "==", "[", "tgt_len", ",", "bsz", ",", "embed_dim", "]", "\n", "\n", "if", "incremental_state", "is", "not", "None", ":", "\n", "            ", "saved_state", "=", "self", ".", "_get_input_buffer", "(", "incremental_state", ")", "\n", "if", "'prev_key'", "in", "saved_state", ":", "\n", "# previous time steps are cached - no need to recompute", "\n", "# key and value if they are static", "\n", "                ", "if", "static_kv", ":", "\n", "                    ", "assert", "self", ".", "encoder_decoder_attention", "and", "not", "self", ".", "self_attention", "\n", "key", "=", "value", "=", "None", "\n", "", "", "", "else", ":", "\n", "            ", "saved_state", "=", "None", "\n", "\n", "", "q", ",", "k", ",", "v", "=", "self", ".", "in_proj_qkv", "(", "query", ")", "\n", "q", "*=", "self", ".", "scaling", "\n", "\n", "if", "self", ".", "bias_k", "is", "not", "None", ":", "\n", "            ", "assert", "self", ".", "bias_v", "is", "not", "None", "\n", "k", "=", "torch", ".", "cat", "(", "[", "k", ",", "self", ".", "bias_k", ".", "repeat", "(", "1", ",", "bsz", ",", "1", ")", "]", ")", "\n", "v", "=", "torch", ".", "cat", "(", "[", "v", ",", "self", ".", "bias_v", ".", "repeat", "(", "1", ",", "bsz", ",", "1", ")", "]", ")", "\n", "", "q", "=", "q", ".", "contiguous", "(", ")", ".", "view", "(", "tgt_len", ",", "bsz", "*", "self", ".", "num_heads", ",", "self", ".", "head_dim", ")", ".", "transpose", "(", "0", ",", "1", ")", "\n", "if", "k", "is", "not", "None", ":", "\n", "            ", "k", "=", "k", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ",", "bsz", "*", "self", ".", "num_heads", ",", "self", ".", "head_dim", ")", ".", "transpose", "(", "0", ",", "1", ")", "\n", "", "if", "v", "is", "not", "None", ":", "\n", "            ", "v", "=", "v", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ",", "bsz", "*", "self", ".", "num_heads", ",", "self", ".", "head_dim", ")", ".", "transpose", "(", "0", ",", "1", ")", "\n", "\n", "# h: \u8f93\u5165\u7684\u9690\u72b6\u6001", "\n", "", "h_list", "=", "query", ".", "chunk", "(", "1", "+", "self", ".", "ngram", ",", "dim", "=", "0", ")", "\n", "\n", "q_list", "=", "q", ".", "chunk", "(", "1", "+", "self", ".", "ngram", ",", "dim", "=", "1", ")", "\n", "k_list", "=", "k", ".", "chunk", "(", "1", "+", "self", ".", "ngram", ",", "dim", "=", "1", ")", "\n", "v_list", "=", "v", ".", "chunk", "(", "1", "+", "self", ".", "ngram", ",", "dim", "=", "1", ")", "\n", "\n", "h_main", ",", "h_predict_list", "=", "h_list", "[", "0", "]", ",", "h_list", "[", "1", ":", "]", "\n", "q_main", ",", "q_predict_list", "=", "q_list", "[", "0", "]", ",", "q_list", "[", "1", ":", "]", "\n", "k_main", ",", "k_predict_list", "=", "k_list", "[", "0", "]", ",", "k_list", "[", "1", ":", "]", "\n", "v_main", ",", "v_predict_list", "=", "v_list", "[", "0", "]", ",", "v_list", "[", "1", ":", "]", "\n", "\n", "if", "saved_state", "is", "not", "None", ":", "\n", "# saved states are stored with shape (bsz, num_heads, seq_len, head_dim)", "\n", "            ", "if", "'prev_key'", "in", "saved_state", ":", "\n", "                ", "prev_key", "=", "saved_state", "[", "'prev_key'", "]", ".", "view", "(", "bsz", "*", "self", ".", "num_heads", ",", "-", "1", ",", "self", ".", "head_dim", ")", "\n", "if", "static_kv", ":", "\n", "                    ", "assert", "False", ",", "'static_kv not supprt in ngram decoder'", "\n", "k", "=", "prev_key", "\n", "", "else", ":", "\n", "                    ", "k_main", "=", "torch", ".", "cat", "(", "(", "prev_key", ",", "k_main", ")", ",", "dim", "=", "1", ")", "\n", "", "", "if", "'prev_value'", "in", "saved_state", ":", "\n", "                ", "prev_value", "=", "saved_state", "[", "'prev_value'", "]", ".", "view", "(", "bsz", "*", "self", ".", "num_heads", ",", "-", "1", ",", "self", ".", "head_dim", ")", "\n", "if", "static_kv", ":", "\n", "                    ", "v", "=", "prev_value", "\n", "", "else", ":", "\n", "                    ", "v_main", "=", "torch", ".", "cat", "(", "(", "prev_value", ",", "v_main", ")", ",", "dim", "=", "1", ")", "\n", "", "", "saved_state", "[", "'prev_key'", "]", "=", "k_main", ".", "view", "(", "bsz", ",", "self", ".", "num_heads", ",", "-", "1", ",", "self", ".", "head_dim", ")", "\n", "saved_state", "[", "'prev_value'", "]", "=", "v_main", ".", "view", "(", "bsz", ",", "self", ".", "num_heads", ",", "-", "1", ",", "self", ".", "head_dim", ")", "\n", "\n", "self", ".", "_set_input_buffer", "(", "incremental_state", ",", "saved_state", ")", "\n", "\n", "", "real_tgt_len", "=", "tgt_len", "//", "(", "1", "+", "self", ".", "ngram", ")", "\n", "\n", "attn_weights_main", "=", "torch", ".", "bmm", "(", "q_main", ",", "k_main", ".", "transpose", "(", "1", ",", "2", ")", ")", "\n", "\n", "main_relative_logits", "=", "self", ".", "main_stream_relative_logits", "(", "h_main", ",", "attn_weights_main", ",", "real_positions", ",", "i_buckets_main_stream", ")", "\n", "attn_weights_main", "=", "attn_weights_main", "+", "main_relative_logits", "\n", "\n", "if", "self_attn_mask", "is", "not", "None", ":", "\n", "            ", "self_attn_mask", "=", "self_attn_mask", ".", "unsqueeze", "(", "0", ")", "\n", "attn_weights_main", "=", "attn_weights_main", "+", "self_attn_mask", "\n", "\n", "", "attn_weights_main", "=", "utils", ".", "softmax", "(", "\n", "attn_weights_main", ",", "dim", "=", "-", "1", ",", "onnx_trace", "=", "self", ".", "onnx_trace", ",", "\n", ")", ".", "type_as", "(", "attn_weights_main", ")", "\n", "attn_weights_main", "=", "F", ".", "dropout", "(", "attn_weights_main", ",", "p", "=", "self", ".", "dropout", ",", "training", "=", "self", ".", "training", ")", "\n", "\n", "attn_main", "=", "torch", ".", "bmm", "(", "attn_weights_main", ",", "v_main", ")", "\n", "attn_main", "=", "attn_main", ".", "transpose", "(", "0", ",", "1", ")", ".", "contiguous", "(", ")", ".", "view", "(", "1", ",", "real_tgt_len", ",", "bsz", ",", "embed_dim", ")", "\n", "attn_main", "=", "self", ".", "out_proj", "(", "attn_main", ")", "\n", "\n", "\n", "# [ngram, B*head, T, c]", "\n", "q_ngram", "=", "torch", ".", "cat", "(", "q_predict_list", ",", "0", ")", ".", "view", "(", "self", ".", "ngram", ",", "-", "1", ",", "real_tgt_len", ",", "self", ".", "head_dim", ")", "\n", "# [ngram, B*head, 2*T, c]", "\n", "k_ngram", "=", "torch", ".", "cat", "(", "[", "torch", ".", "cat", "(", "[", "k_main", ",", "k_p", "]", ",", "1", ")", ".", "unsqueeze", "(", "0", ")", "for", "k_p", "in", "k_predict_list", "]", ",", "0", ")", "\n", "# below code slower than above for loop", "\n", "# k_ngram = torch.cat([k_main.unsqueeze(0).repeat(self.ngram, 1, 1, 1) , torch.cat(k_predict_list).view(self.ngram, -1, real_tgt_len, self.head_dim)], 2)", "\n", "\n", "# [ngram, T, B, C]", "\n", "h_ngram", "=", "torch", ".", "cat", "(", "h_predict_list", ",", "0", ")", ".", "view", "(", "self", ".", "ngram", ",", "real_tgt_len", ",", "bsz", ",", "embed_dim", ")", "\n", "\n", "# [ngram, B*head, 2*T, c]", "\n", "v_ngram", "=", "torch", ".", "cat", "(", "[", "torch", ".", "cat", "(", "[", "v_main", ",", "v_p", "]", ",", "1", ")", ".", "unsqueeze", "(", "0", ")", "for", "v_p", "in", "v_predict_list", "]", ",", "0", ")", "\n", "# below code slower than above for loop", "\n", "# v_ngram = torch.cat([v_main.unsqueeze(0).repeat(self.ngram, 1, 1, 1) , torch.cat(v_predict_list).view(self.ngram, -1, real_tgt_len, self.head_dim)], 2)", "\n", "\n", "# [ngram, B*head, T, 2*T]", "\n", "attn_weights_ngram", "=", "torch", ".", "einsum", "(", "'nbtc,nbsc->nbts'", ",", "(", "q_ngram", ",", "k_ngram", ")", ")", "\n", "\n", "# [ngram, B*head, T, S]", "\n", "predict_relative_logits", "=", "self", ".", "ngram_relative_logits", "(", "h_ngram", ",", "attn_weights_ngram", ",", "real_positions", ",", "i_bucket_relative_stream", ")", "\n", "# [ngram, B*head, T, 2*T]", "\n", "attn_weights_ngram", "=", "attn_weights_ngram", "+", "predict_relative_logits", "\n", "\n", "if", "ngram_mask_matrix", "is", "not", "None", ":", "\n", "            ", "ngram_mask_matrix", "=", "ngram_mask_matrix", ".", "unsqueeze", "(", "1", ")", "\n", "attn_weights_ngram", "=", "attn_weights_ngram", "+", "ngram_mask_matrix", "\n", "\n", "", "attn_weights_ngram", "=", "utils", ".", "softmax", "(", "\n", "attn_weights_ngram", ",", "dim", "=", "-", "1", ",", "onnx_trace", "=", "self", ".", "onnx_trace", ",", "\n", ")", ".", "type_as", "(", "attn_weights_ngram", ")", "\n", "attn_weights_ngram", "=", "F", ".", "dropout", "(", "attn_weights_ngram", ",", "p", "=", "self", ".", "dropout", ",", "training", "=", "self", ".", "training", ")", "\n", "\n", "# [ngram, B*head, T, c]", "\n", "attn_ngram", "=", "torch", ".", "einsum", "(", "'nbts,nbsc->nbtc'", ",", "(", "attn_weights_ngram", ",", "v_ngram", ")", ")", "\n", "# [ngram, T, B, C]", "\n", "attn_ngram", "=", "attn_ngram", ".", "transpose", "(", "1", ",", "2", ")", ".", "contiguous", "(", ")", ".", "view", "(", "self", ".", "ngram", ",", "real_tgt_len", ",", "bsz", ",", "embed_dim", ")", "\n", "attn_ngram", "=", "self", ".", "out_proj", "(", "attn_ngram", ")", "\n", "\n", "attn_result", "=", "[", "]", "\n", "attn_result", ".", "append", "(", "attn_main", ")", "\n", "attn_result", ".", "append", "(", "attn_ngram", ")", "\n", "\n", "# [1+ngram*T, B, C]", "\n", "attn", "=", "torch", ".", "cat", "(", "attn_result", ",", "0", ")", ".", "view", "(", "-", "1", ",", "bsz", ",", "embed_dim", ")", "\n", "return", "attn", ",", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_multihead_attention.NgramMultiheadAttention.in_proj_qkv": [[366, 368], ["ngram_multihead_attention.NgramMultiheadAttention._in_proj().chunk", "ngram_multihead_attention.NgramMultiheadAttention._in_proj"], "methods", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_multihead_attention.NgramMultiheadAttention._in_proj"], ["", "def", "in_proj_qkv", "(", "self", ",", "query", ")", ":", "\n", "        ", "return", "self", ".", "_in_proj", "(", "query", ")", ".", "chunk", "(", "3", ",", "dim", "=", "-", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_multihead_attention.NgramMultiheadAttention.in_proj_q": [[369, 377], ["ngram_multihead_attention.NgramMultiheadAttention._in_proj", "torch.linear", "torch.linear"], "methods", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_multihead_attention.NgramMultiheadAttention._in_proj", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.layers.nn.linear", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.layers.nn.linear"], ["", "def", "in_proj_q", "(", "self", ",", "query", ")", ":", "\n", "        ", "if", "self", ".", "qkv_same_dim", ":", "\n", "            ", "return", "self", ".", "_in_proj", "(", "query", ",", "end", "=", "self", ".", "embed_dim", ")", "\n", "", "else", ":", "\n", "            ", "bias", "=", "self", ".", "in_proj_bias", "\n", "if", "bias", "is", "not", "None", ":", "\n", "                ", "bias", "=", "bias", "[", ":", "self", ".", "embed_dim", "]", "\n", "", "return", "F", ".", "linear", "(", "query", ",", "self", ".", "q_proj_weight", ",", "bias", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_multihead_attention.NgramMultiheadAttention.in_proj_k": [[378, 387], ["ngram_multihead_attention.NgramMultiheadAttention._in_proj", "torch.linear", "torch.linear"], "methods", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_multihead_attention.NgramMultiheadAttention._in_proj", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.layers.nn.linear", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.layers.nn.linear"], ["", "", "def", "in_proj_k", "(", "self", ",", "key", ")", ":", "\n", "        ", "if", "self", ".", "qkv_same_dim", ":", "\n", "            ", "return", "self", ".", "_in_proj", "(", "key", ",", "start", "=", "self", ".", "embed_dim", ",", "end", "=", "2", "*", "self", ".", "embed_dim", ")", "\n", "", "else", ":", "\n", "            ", "weight", "=", "self", ".", "k_proj_weight", "\n", "bias", "=", "self", ".", "in_proj_bias", "\n", "if", "bias", "is", "not", "None", ":", "\n", "                ", "bias", "=", "bias", "[", "self", ".", "embed_dim", ":", "2", "*", "self", ".", "embed_dim", "]", "\n", "", "return", "F", ".", "linear", "(", "key", ",", "weight", ",", "bias", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_multihead_attention.NgramMultiheadAttention.in_proj_v": [[388, 397], ["ngram_multihead_attention.NgramMultiheadAttention._in_proj", "torch.linear", "torch.linear"], "methods", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_multihead_attention.NgramMultiheadAttention._in_proj", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.layers.nn.linear", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.layers.nn.linear"], ["", "", "def", "in_proj_v", "(", "self", ",", "value", ")", ":", "\n", "        ", "if", "self", ".", "qkv_same_dim", ":", "\n", "            ", "return", "self", ".", "_in_proj", "(", "value", ",", "start", "=", "2", "*", "self", ".", "embed_dim", ")", "\n", "", "else", ":", "\n", "            ", "weight", "=", "self", ".", "v_proj_weight", "\n", "bias", "=", "self", ".", "in_proj_bias", "\n", "if", "bias", "is", "not", "None", ":", "\n", "                ", "bias", "=", "bias", "[", "2", "*", "self", ".", "embed_dim", ":", "]", "\n", "", "return", "F", ".", "linear", "(", "value", ",", "weight", ",", "bias", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_multihead_attention.NgramMultiheadAttention._in_proj": [[398, 405], ["torch.linear", "torch.linear"], "methods", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.layers.nn.linear", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.layers.nn.linear"], ["", "", "def", "_in_proj", "(", "self", ",", "input", ",", "start", "=", "0", ",", "end", "=", "None", ")", ":", "\n", "        ", "weight", "=", "self", ".", "in_proj_weight", "\n", "bias", "=", "self", ".", "in_proj_bias", "\n", "weight", "=", "weight", "[", "start", ":", "end", ",", ":", "]", "\n", "if", "bias", "is", "not", "None", ":", "\n", "            ", "bias", "=", "bias", "[", "start", ":", "end", "]", "\n", "", "return", "F", ".", "linear", "(", "input", ",", "weight", ",", "bias", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_multihead_attention.NgramMultiheadAttention.reorder_incremental_state": [[406, 413], ["ngram_multihead_attention.NgramMultiheadAttention._get_input_buffer", "ngram_multihead_attention.NgramMultiheadAttention.keys", "ngram_multihead_attention.NgramMultiheadAttention._set_input_buffer", "input_buffer[].index_select"], "methods", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_multihead_attention.NgramMultiheadAttention._get_input_buffer", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_multihead_attention.NgramMultiheadAttention._set_input_buffer"], ["", "def", "reorder_incremental_state", "(", "self", ",", "incremental_state", ",", "new_order", ")", ":", "\n", "        ", "\"\"\"Reorder buffered internal state (for incremental generation).\"\"\"", "\n", "input_buffer", "=", "self", ".", "_get_input_buffer", "(", "incremental_state", ")", "\n", "if", "input_buffer", "is", "not", "None", ":", "\n", "            ", "for", "k", "in", "input_buffer", ".", "keys", "(", ")", ":", "\n", "                ", "input_buffer", "[", "k", "]", "=", "input_buffer", "[", "k", "]", ".", "index_select", "(", "0", ",", "new_order", ")", "\n", "", "self", ".", "_set_input_buffer", "(", "incremental_state", ",", "input_buffer", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_multihead_attention.NgramMultiheadAttention._get_input_buffer": [[414, 420], ["fairseq.utils.get_incremental_state"], "methods", ["None"], ["", "", "def", "_get_input_buffer", "(", "self", ",", "incremental_state", ")", ":", "\n", "        ", "return", "utils", ".", "get_incremental_state", "(", "\n", "self", ",", "\n", "incremental_state", ",", "\n", "'attn_state'", ",", "\n", ")", "or", "{", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_multihead_attention.NgramMultiheadAttention._set_input_buffer": [[421, 427], ["fairseq.utils.set_incremental_state"], "methods", ["None"], ["", "def", "_set_input_buffer", "(", "self", ",", "incremental_state", ",", "buffer", ")", ":", "\n", "        ", "utils", ".", "set_incremental_state", "(", "\n", "self", ",", "\n", "incremental_state", ",", "\n", "'attn_state'", ",", "\n", "buffer", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.ngram_multihead_attention.ngram_attention_bias": [[16, 30], ["range", "torch.from_numpy", "torch.from_numpy", "range", "bias_result.append", "numpy.array", "range", "bias_n_skip.append", "float"], "function", ["None"], ["def", "ngram_attention_bias", "(", "length", ",", "num_skip", ")", ":", "\n", "        ", "bias_result", "=", "[", "]", "\n", "for", "n_skip", "in", "range", "(", "num_skip", ")", ":", "\n", "            ", "bias_n_skip", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "length", ")", ":", "\n", "                ", "bias_this", "=", "[", "float", "(", "'-inf'", ")", "]", "*", "(", "2", "*", "length", ")", "\n", "bias_this", "[", "length", "+", "i", "]", "=", "0", "\n", "first_k", "=", "i", "-", "n_skip", "\n", "first_k", "=", "first_k", "if", "first_k", ">", "0", "else", "0", "\n", "for", "j", "in", "range", "(", "first_k", "+", "1", ")", ":", "\n", "                    ", "bias_this", "[", "j", "]", "=", "0", "\n", "", "bias_n_skip", ".", "append", "(", "bias_this", ")", "\n", "", "bias_result", ".", "append", "(", "bias_n_skip", ")", "\n", "", "return", "torch", ".", "from_numpy", "(", "np", ".", "array", "(", "bias_result", ",", "dtype", "=", "np", ".", "float32", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.bert_dictionary.BertDictionary.__init__": [[20, 29], ["fairseq.data.Dictionary.__init__"], "methods", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.transformer_lrp.TransformerLRP.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "pad", "=", "'<pad>'", ",", "\n", "eos", "=", "'</s>'", ",", "\n", "unk", "=", "'<unk>'", ",", "\n", "bos", "=", "'<s>'", ",", "\n", "extra_special_symbols", "=", "None", ",", "\n", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "pad", ",", "eos", ",", "unk", ",", "bos", ",", "extra_special_symbols", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.bert_dictionary.BertDictionary.load_from_file": [[30, 54], ["cls", "cls.add_symbol", "cls.add_symbol", "cls.add_symbol", "cls.add_symbol", "open", "line.split", "cls.add_symbol"], "methods", ["None"], ["", "@", "classmethod", "\n", "def", "load_from_file", "(", "cls", ",", "filename", ")", ":", "\n", "        ", "d", "=", "cls", "(", ")", "\n", "d", ".", "symbols", "=", "[", "]", "\n", "d", ".", "count", "=", "[", "]", "\n", "d", ".", "indices", "=", "{", "}", "\n", "\n", "with", "open", "(", "filename", ",", "'r'", ",", "encoding", "=", "'utf-8'", ",", "errors", "=", "'ignore'", ")", "as", "input_file", ":", "\n", "            ", "for", "line", "in", "input_file", ":", "\n", "                ", "k", ",", "v", "=", "line", ".", "split", "(", ")", "\n", "d", ".", "add_symbol", "(", "k", ")", "\n", "\n", "", "", "d", ".", "unk_word", "=", "'[UNK]'", "\n", "d", ".", "pad_word", "=", "'[PAD]'", "\n", "d", ".", "eos_word", "=", "'[SEP]'", "\n", "d", ".", "bos_word", "=", "'[CLS]'", "\n", "\n", "d", ".", "bos_index", "=", "d", ".", "add_symbol", "(", "'[CLS]'", ")", "\n", "d", ".", "pad_index", "=", "d", ".", "add_symbol", "(", "'[PAD]'", ")", "\n", "d", ".", "eos_index", "=", "d", ".", "add_symbol", "(", "'[SEP]'", ")", "\n", "d", ".", "unk_index", "=", "d", ".", "add_symbol", "(", "'[UNK]'", ")", "\n", "\n", "d", ".", "nspecial", "=", "999", "\n", "return", "d", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.bert_dictionary.BertDictionary.save": [[55, 59], ["bert_dictionary.BertDictionary._get_meta", "bert_dictionary.BertDictionary._save", "zip"], "methods", ["None"], ["", "def", "save", "(", "self", ",", "f", ")", ":", "\n", "        ", "\"\"\"Stores dictionary into a text file\"\"\"", "\n", "ex_keys", ",", "ex_vals", "=", "self", ".", "_get_meta", "(", ")", "\n", "self", ".", "_save", "(", "f", ",", "zip", "(", "ex_keys", "+", "self", ".", "symbols", ",", "ex_vals", "+", "self", ".", "count", ")", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.ggw.eval_ggw.test_rouge": [[61, 96], ["tempfile.mkdtemp", "len", "time.strftime", "os.path.join", "len", "len", "time.localtime", "os.path.isdir", "os.mkdir", "os.mkdir", "os.mkdir", "range", "bs_pyrouge.Rouge155", "bs_pyrouge.Rouge155.convert_and_evaluate", "print", "bs_pyrouge.Rouge155.output_to_dict", "os.path.isdir", "shutil.rmtree", "len", "open", "f.write", "open", "f.write"], "function", ["None"], ["def", "test_rouge", "(", "cand", ",", "ref", ")", ":", "\n", "    ", "temp_dir", "=", "tempfile", ".", "mkdtemp", "(", ")", "\n", "candidates", "=", "cand", "\n", "references", "=", "ref", "\n", "assert", "len", "(", "candidates", ")", "==", "len", "(", "references", ")", "\n", "\n", "cnt", "=", "len", "(", "candidates", ")", "\n", "current_time", "=", "time", ".", "strftime", "(", "'%Y-%m-%d-%H-%M-%S'", ",", "time", ".", "localtime", "(", ")", ")", "\n", "tmp_dir", "=", "os", ".", "path", ".", "join", "(", "temp_dir", ",", "\"rouge-tmp-{}\"", ".", "format", "(", "current_time", ")", ")", "\n", "if", "not", "os", ".", "path", ".", "isdir", "(", "tmp_dir", ")", ":", "\n", "        ", "os", ".", "mkdir", "(", "tmp_dir", ")", "\n", "os", ".", "mkdir", "(", "tmp_dir", "+", "\"/candidate\"", ")", "\n", "os", ".", "mkdir", "(", "tmp_dir", "+", "\"/reference\"", ")", "\n", "", "try", ":", "\n", "        ", "for", "i", "in", "range", "(", "cnt", ")", ":", "\n", "            ", "if", "len", "(", "references", "[", "i", "]", ")", "<", "1", ":", "\n", "                ", "continue", "\n", "", "with", "open", "(", "tmp_dir", "+", "\"/candidate/cand.{}.txt\"", ".", "format", "(", "i", ")", ",", "\"w\"", ",", "\n", "encoding", "=", "\"utf-8\"", ")", "as", "f", ":", "\n", "                ", "f", ".", "write", "(", "candidates", "[", "i", "]", ")", "\n", "", "with", "open", "(", "tmp_dir", "+", "\"/reference/ref.{}.txt\"", ".", "format", "(", "i", ")", ",", "\"w\"", ",", "\n", "encoding", "=", "\"utf-8\"", ")", "as", "f", ":", "\n", "                ", "f", ".", "write", "(", "references", "[", "i", "]", ")", "\n", "", "", "r", "=", "Rouge155", "(", "temp_dir", "=", "temp_dir", ")", "\n", "r", ".", "model_dir", "=", "tmp_dir", "+", "\"/reference/\"", "\n", "r", ".", "system_dir", "=", "tmp_dir", "+", "\"/candidate/\"", "\n", "r", ".", "model_filename_pattern", "=", "'ref.#ID#.txt'", "\n", "r", ".", "system_filename_pattern", "=", "r'cand.(\\d+).txt'", "\n", "rouge_results", "=", "r", ".", "convert_and_evaluate", "(", ")", "\n", "print", "(", "rouge_results", ")", "\n", "results_dict", "=", "r", ".", "output_to_dict", "(", "rouge_results", ")", "\n", "", "finally", ":", "\n", "        ", "if", "os", ".", "path", ".", "isdir", "(", "tmp_dir", ")", ":", "\n", "            ", "shutil", ".", "rmtree", "(", "tmp_dir", ")", "\n", "", "", "return", "results_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.ggw.eval_ggw.rouge_results_to_str": [[98, 106], ["None"], "function", ["None"], ["", "def", "rouge_results_to_str", "(", "results_dict", ")", ":", "\n", "    ", "return", "\">> ROUGE-F(1/2/l): {:.2f}/{:.2f}/{:.2f}\\nROUGE-R(1/2/3/l): {:.2f}/{:.2f}/{:.2f}\\n\"", ".", "format", "(", "\n", "results_dict", "[", "\"rouge_1_f_score\"", "]", "*", "100", ",", "\n", "results_dict", "[", "\"rouge_2_f_score\"", "]", "*", "100", ",", "\n", "results_dict", "[", "\"rouge_l_f_score\"", "]", "*", "100", ",", "\n", "results_dict", "[", "\"rouge_1_recall\"", "]", "*", "100", ",", "\n", "results_dict", "[", "\"rouge_2_recall\"", "]", "*", "100", ",", "\n", "results_dict", "[", "\"rouge_l_recall\"", "]", "*", "100", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.ggw.eval_ggw.count_tokens": [[109, 117], ["counter.keys"], "function", ["None"], ["", "def", "count_tokens", "(", "tokens", ")", ":", "\n", "    ", "counter", "=", "{", "}", "\n", "for", "t", "in", "tokens", ":", "\n", "        ", "if", "t", "in", "counter", ".", "keys", "(", ")", ":", "\n", "            ", "counter", "[", "t", "]", "+=", "1", "\n", "", "else", ":", "\n", "            ", "counter", "[", "t", "]", "=", "1", "\n", "", "", "return", "counter", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.ggw.eval_ggw.get_f1": [[119, 133], ["text_a.lower().split", "text_b.lower().split", "eval_ggw.count_tokens", "eval_ggw.count_tokens", "count_tokens.keys", "len", "len", "text_a.lower", "text_b.lower", "len", "len", "count_tokens.keys", "min", "len", "len"], "function", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.cnndm.postprocess_cnn_dm.count_tokens", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.cnndm.postprocess_cnn_dm.count_tokens"], ["", "def", "get_f1", "(", "text_a", ",", "text_b", ")", ":", "\n", "    ", "tokens_a", "=", "text_a", ".", "lower", "(", ")", ".", "split", "(", ")", "\n", "tokens_b", "=", "text_b", ".", "lower", "(", ")", ".", "split", "(", ")", "\n", "if", "len", "(", "tokens_a", ")", "==", "0", "or", "len", "(", "tokens_b", ")", "==", "0", ":", "\n", "        ", "return", "1", "if", "len", "(", "tokens_a", ")", "==", "len", "(", "tokens_b", ")", "else", "0", "\n", "", "set_a", "=", "count_tokens", "(", "tokens_a", ")", "\n", "set_b", "=", "count_tokens", "(", "tokens_b", ")", "\n", "match", "=", "0", "\n", "for", "token", "in", "set_a", ".", "keys", "(", ")", ":", "\n", "        ", "if", "token", "in", "set_b", ".", "keys", "(", ")", ":", "\n", "            ", "match", "+=", "min", "(", "set_a", "[", "token", "]", ",", "set_b", "[", "token", "]", ")", "\n", "", "", "p", "=", "match", "/", "len", "(", "tokens_a", ")", "\n", "r", "=", "match", "/", "len", "(", "tokens_b", ")", "\n", "return", "2.0", "*", "p", "*", "r", "/", "(", "p", "+", "r", "+", "1e-5", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.ggw.eval_ggw._is_digit": [[141, 146], ["ch.isdigit"], "function", ["None"], ["def", "_is_digit", "(", "w", ")", ":", "\n", "    ", "for", "ch", "in", "w", ":", "\n", "        ", "if", "not", "(", "ch", ".", "isdigit", "(", ")", "or", "ch", "==", "','", ")", ":", "\n", "            ", "return", "False", "\n", "", "", "return", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.ggw.eval_ggw.fix_tokenization": [[148, 226], ["text.split", "len", "_tok_dict.keys", "output_tokens.append", "output_tokens.append", "output_tokens.append", "output_tokens[].endswith", "output_tokens.append", "len", "output_tokens.append", "len", "len", "output_tokens.append", "output_tokens.append", "output_tokens.append", "eval_ggw._is_digit", "eval_ggw._is_digit", "len", "len", "output_tokens[].isdigit", "input_tokens[].isdigit", "len", "len", "output_tokens[].isupper", "input_tokens[].isupper", "len", "len", "len", "len", "len", "len", "input_tokens[].isupper", "output_tokens.append", "output_tokens.append", "len", "output_tokens.append", "len", "len", "output_tokens.append", "len"], "function", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.cnndm.postprocess_cnn_dm._is_digit", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.cnndm.postprocess_cnn_dm._is_digit"], ["", "def", "fix_tokenization", "(", "text", ")", ":", "\n", "    ", "input_tokens", "=", "text", ".", "split", "(", ")", "\n", "output_tokens", "=", "[", "]", "\n", "has_left_quote", "=", "False", "\n", "has_left_single_quote", "=", "False", "\n", "\n", "i", "=", "0", "\n", "prev_dash", "=", "False", "\n", "while", "i", "<", "len", "(", "input_tokens", ")", ":", "\n", "        ", "tok", "=", "input_tokens", "[", "i", "]", "\n", "flag_prev_dash", "=", "False", "\n", "if", "tok", "in", "_tok_dict", ".", "keys", "(", ")", ":", "\n", "            ", "output_tokens", ".", "append", "(", "_tok_dict", "[", "tok", "]", ")", "\n", "i", "+=", "1", "\n", "", "elif", "tok", "==", "\"\\\"\"", ":", "\n", "            ", "if", "has_left_quote", ":", "\n", "                ", "output_tokens", ".", "append", "(", "\"''\"", ")", "\n", "", "else", ":", "\n", "                ", "output_tokens", ".", "append", "(", "\"``\"", ")", "\n", "", "has_left_quote", "=", "not", "has_left_quote", "\n", "i", "+=", "1", "\n", "", "elif", "tok", "==", "\"'\"", "and", "len", "(", "output_tokens", ")", ">", "0", "and", "output_tokens", "[", "-", "1", "]", ".", "endswith", "(", "\"n\"", ")", "and", "i", "<", "len", "(", "input_tokens", ")", "-", "1", "and", "input_tokens", "[", "i", "+", "1", "]", "==", "\"t\"", ":", "\n", "            ", "output_tokens", "[", "-", "1", "]", "=", "output_tokens", "[", "-", "1", "]", "[", ":", "-", "1", "]", "\n", "output_tokens", ".", "append", "(", "\"n't\"", ")", "\n", "i", "+=", "2", "\n", "", "elif", "tok", "==", "\"'\"", "and", "i", "<", "len", "(", "input_tokens", ")", "-", "1", "and", "input_tokens", "[", "i", "+", "1", "]", "in", "(", "\"s\"", ",", "\"d\"", ",", "\"ll\"", ")", ":", "\n", "            ", "output_tokens", ".", "append", "(", "\"'\"", "+", "input_tokens", "[", "i", "+", "1", "]", ")", "\n", "i", "+=", "2", "\n", "", "elif", "tok", "==", "\"'\"", ":", "\n", "            ", "if", "has_left_single_quote", ":", "\n", "                ", "output_tokens", ".", "append", "(", "\"'\"", ")", "\n", "", "else", ":", "\n", "                ", "output_tokens", ".", "append", "(", "\"`\"", ")", "\n", "", "has_left_single_quote", "=", "not", "has_left_single_quote", "\n", "i", "+=", "1", "\n", "", "elif", "tok", "==", "\".\"", "and", "i", "<", "len", "(", "input_tokens", ")", "-", "2", "and", "input_tokens", "[", "i", "+", "1", "]", "==", "\".\"", "and", "input_tokens", "[", "i", "+", "2", "]", "==", "\".\"", ":", "\n", "            ", "output_tokens", ".", "append", "(", "\"...\"", ")", "\n", "i", "+=", "3", "\n", "", "elif", "tok", "==", "\",\"", "and", "len", "(", "output_tokens", ")", ">", "0", "and", "_is_digit", "(", "output_tokens", "[", "-", "1", "]", ")", "and", "i", "<", "len", "(", "input_tokens", ")", "-", "1", "and", "_is_digit", "(", "input_tokens", "[", "i", "+", "1", "]", ")", ":", "\n", "# $ 3 , 000 -> $ 3,000", "\n", "            ", "output_tokens", "[", "-", "1", "]", "+=", "','", "+", "input_tokens", "[", "i", "+", "1", "]", "\n", "i", "+=", "2", "\n", "", "elif", "tok", "==", "\".\"", "and", "len", "(", "output_tokens", ")", ">", "0", "and", "output_tokens", "[", "-", "1", "]", ".", "isdigit", "(", ")", "and", "i", "<", "len", "(", "input_tokens", ")", "-", "1", "and", "input_tokens", "[", "i", "+", "1", "]", ".", "isdigit", "(", ")", ":", "\n", "# 3 . 03 -> $ 3.03", "\n", "            ", "output_tokens", "[", "-", "1", "]", "+=", "'.'", "+", "input_tokens", "[", "i", "+", "1", "]", "\n", "i", "+=", "2", "\n", "", "elif", "tok", "==", "\".\"", "and", "len", "(", "output_tokens", ")", ">", "0", "and", "len", "(", "output_tokens", "[", "-", "1", "]", ")", "==", "1", "and", "output_tokens", "[", "-", "1", "]", ".", "isupper", "(", ")", "and", "i", "<", "len", "(", "input_tokens", ")", "-", "2", "and", "len", "(", "input_tokens", "[", "i", "+", "1", "]", ")", "==", "1", "and", "input_tokens", "[", "i", "+", "1", "]", ".", "isupper", "(", ")", "and", "input_tokens", "[", "i", "+", "2", "]", "==", "'.'", ":", "\n", "# U . N . -> U.N.", "\n", "            ", "k", "=", "i", "+", "3", "\n", "while", "k", "+", "2", "<", "len", "(", "input_tokens", ")", ":", "\n", "                ", "if", "len", "(", "input_tokens", "[", "k", "+", "1", "]", ")", "==", "1", "and", "input_tokens", "[", "k", "+", "1", "]", ".", "isupper", "(", ")", "and", "input_tokens", "[", "k", "+", "2", "]", "==", "'.'", ":", "\n", "                    ", "k", "+=", "2", "\n", "", "else", ":", "\n", "                    ", "break", "\n", "", "", "output_tokens", "[", "-", "1", "]", "+=", "''", ".", "join", "(", "input_tokens", "[", "i", ":", "k", "]", ")", "\n", "i", "+=", "2", "\n", "", "elif", "tok", "==", "\"-\"", ":", "\n", "            ", "if", "i", "<", "len", "(", "input_tokens", ")", "-", "1", "and", "input_tokens", "[", "i", "+", "1", "]", "==", "\"-\"", ":", "\n", "                ", "output_tokens", ".", "append", "(", "\"--\"", ")", "\n", "i", "+=", "2", "\n", "", "elif", "i", "==", "len", "(", "input_tokens", ")", "-", "1", "or", "i", "==", "0", ":", "\n", "                ", "output_tokens", ".", "append", "(", "\"-\"", ")", "\n", "i", "+=", "1", "\n", "", "elif", "output_tokens", "[", "-", "1", "]", "not", "in", "string", ".", "punctuation", "and", "input_tokens", "[", "i", "+", "1", "]", "[", "0", "]", "not", "in", "string", ".", "punctuation", ":", "\n", "                ", "output_tokens", "[", "-", "1", "]", "+=", "\"-\"", "\n", "i", "+=", "1", "\n", "flag_prev_dash", "=", "True", "\n", "", "else", ":", "\n", "                ", "output_tokens", ".", "append", "(", "\"-\"", ")", "\n", "i", "+=", "1", "\n", "", "", "elif", "prev_dash", "and", "len", "(", "output_tokens", ")", ">", "0", "and", "tok", "[", "0", "]", "not", "in", "string", ".", "punctuation", ":", "\n", "            ", "output_tokens", "[", "-", "1", "]", "+=", "tok", "\n", "i", "+=", "1", "\n", "", "else", ":", "\n", "            ", "output_tokens", ".", "append", "(", "tok", ")", "\n", "i", "+=", "1", "\n", "", "prev_dash", "=", "flag_prev_dash", "\n", "", "return", "\" \"", ".", "join", "(", "output_tokens", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.ggw.eval_ggw.process_eval": [[228, 269], ["open", "open", "open", "len", "len", "len", "len", "eval_ggw.test_rouge", "evaluator.get_scores", "l.strip", "gold_list.append", "fix_tokenization().replace", "buf.append", "pred_list.append", "f_out.write", "f_out.write", "l.strip", "len", "eval_ggw.fix_tokenization", "bit.split", "min", "trunc_list.append", "l.strip", "len"], "function", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.cnndm.postprocess_cnn_dm.test_rouge", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.cnndm.postprocess_cnn_dm.fix_tokenization"], ["", "def", "process_eval", "(", "eval_fn", ")", ":", "\n", "    ", "gold_list", "=", "[", "]", "\n", "with", "open", "(", "args", ".", "gold", ",", "\"r\"", ",", "encoding", "=", "\"utf-8\"", ")", "as", "f_in", ":", "\n", "        ", "for", "l", "in", "f_in", ":", "\n", "            ", "line", "=", "l", ".", "strip", "(", ")", "\n", "gold_list", ".", "append", "(", "line", ")", "\n", "\n", "", "", "pred_list", "=", "[", "]", "\n", "with", "open", "(", "eval_fn", ",", "\"r\"", ",", "encoding", "=", "\"utf-8\"", ")", "as", "f_in", ":", "\n", "        ", "for", "l", "in", "f_in", ":", "\n", "            ", "buf", "=", "[", "]", "\n", "sentence", "=", "fix_tokenization", "(", "l", ".", "strip", "(", ")", ")", ".", "replace", "(", "'1'", ",", "'#'", ")", "\n", "buf", ".", "append", "(", "sentence", ")", "\n", "if", "args", ".", "trunc_len", ":", "\n", "                ", "num_left", "=", "args", ".", "trunc_len", "\n", "trunc_list", "=", "[", "]", "\n", "for", "bit", "in", "buf", ":", "\n", "                    ", "tk_list", "=", "bit", ".", "split", "(", ")", "\n", "n", "=", "min", "(", "len", "(", "tk_list", ")", ",", "num_left", ")", "\n", "trunc_list", ".", "append", "(", "' '", ".", "join", "(", "tk_list", "[", ":", "n", "]", ")", ")", "\n", "num_left", "-=", "n", "\n", "if", "num_left", "<=", "0", ":", "\n", "                        ", "break", "\n", "", "", "", "else", ":", "\n", "                ", "trunc_list", "=", "buf", "\n", "", "line", "=", "\"\\n\"", ".", "join", "(", "trunc_list", ")", "\n", "pred_list", ".", "append", "(", "line", ")", "\n", "", "", "with", "open", "(", "eval_fn", "+", "'.post'", ",", "'w'", ",", "encoding", "=", "'utf-8'", ")", "as", "f_out", ":", "\n", "        ", "for", "l", "in", "pred_list", ":", "\n", "            ", "f_out", ".", "write", "(", "l", ".", "strip", "(", ")", ")", "\n", "f_out", ".", "write", "(", "'\\n'", ")", "\n", "# rouge scores", "\n", "", "", "if", "len", "(", "pred_list", ")", "<", "len", "(", "gold_list", ")", ":", "\n", "# evaluate subset", "\n", "        ", "gold_list", "=", "gold_list", "[", ":", "len", "(", "pred_list", ")", "]", "\n", "", "assert", "len", "(", "pred_list", ")", "==", "len", "(", "gold_list", ")", "\n", "if", "args", ".", "perl", ":", "\n", "        ", "scores", "=", "test_rouge", "(", "pred_list", ",", "gold_list", ")", "\n", "", "else", ":", "\n", "        ", "scores", "=", "evaluator", ".", "get_scores", "(", "pred_list", ",", "[", "[", "it", "]", "for", "it", "in", "gold_list", "]", ")", "\n", "", "return", "eval_fn", ",", "scores", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.ggw.eval_ggw.main": [[271, 338], ["list", "logger.info", "min", "multiprocessing.Pool", "multiprocessing.Pool.imap_unordered", "sorted", "multiprocessing.Pool.close", "multiprocessing.Pool.join", "list", "filter", "set", "len", "print", "rg2_dict.items", "group_dict.items", "glob.glob", "os.path.join", "pathlib.Path().exists", "print", "print", "os.path.join", "pathlib.Path().exists", "glob.glob", "pathlib.Path", "eval_ggw.rouge_results_to_str", "open", "f_out.write", "pathlib.Path", "pathlib.Path", "open", "f_in.read().strip().split", "new_eval_fn_list.append", "json.dumps", "pathlib.Path", "pathlib.Path", "pathlib.Path", "open", "float", "open", "f_out.write", "pathlib.Path().exists", "os.path.join.endswith", "os.path.join.endswith", "o_name.split", "pathlib.Path().name.split", "f_in.read().strip", "f_in.read().strip().split", "pathlib.Path", "f_in.read", "pathlib.Path", "f_in.read().strip", "f_in.read"], "function", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.cnndm.postprocess_cnn_dm.rouge_results_to_str"], ["", "def", "main", "(", ")", ":", "\n", "    ", "if", "args", ".", "perl", ":", "\n", "        ", "eval_fn_list", "=", "list", "(", "glob", ".", "glob", "(", "args", ".", "pred", ")", ")", "\n", "", "else", ":", "\n", "        ", "eval_fn_list", "=", "[", "eval_fn", "for", "eval_fn", "in", "glob", ".", "glob", "(", "args", ".", "pred", ")", "if", "not", "(", "\n", "args", ".", "lazy_eval", "and", "Path", "(", "eval_fn", "+", "\".rouge\"", ")", ".", "exists", "(", ")", ")", "]", "\n", "", "eval_fn_list", "=", "list", "(", "filter", "(", "lambda", "fn", ":", "not", "(", "fn", ".", "endswith", "(", "\n", "'.post'", ")", "or", "fn", ".", "endswith", "(", "'.rouge'", ")", ")", ",", "eval_fn_list", ")", ")", "\n", "\n", "if", "args", ".", "only_eval_best", ":", "\n", "        ", "best_epoch_dict", "=", "{", "}", "\n", "for", "dir_path", "in", "set", "(", "Path", "(", "fn", ")", ".", "parent", "for", "fn", "in", "eval_fn_list", ")", ":", "\n", "            ", "fn_save", "=", "os", ".", "path", ".", "join", "(", "dir_path", ",", "'save_best.dev'", ")", "\n", "if", "Path", "(", "fn_save", ")", ".", "exists", "(", ")", ":", "\n", "                ", "with", "open", "(", "fn_save", ",", "'r'", ")", "as", "f_in", ":", "\n", "                    ", "__", ",", "o_name", ",", "__", "=", "f_in", ".", "read", "(", ")", ".", "strip", "(", ")", ".", "split", "(", "'\\n'", ")", "\n", "epoch", "=", "o_name", ".", "split", "(", "'.'", ")", "[", "1", "]", "\n", "best_epoch_dict", "[", "dir_path", "]", "=", "epoch", "\n", "", "", "", "new_eval_fn_list", "=", "[", "]", "\n", "for", "fn", "in", "eval_fn_list", ":", "\n", "            ", "dir_path", "=", "Path", "(", "fn", ")", ".", "parent", "\n", "if", "dir_path", "in", "best_epoch_dict", ":", "\n", "                ", "if", "Path", "(", "fn", ")", ".", "name", ".", "split", "(", "'.'", ")", "[", "1", "]", "==", "best_epoch_dict", "[", "dir_path", "]", ":", "\n", "                    ", "new_eval_fn_list", ".", "append", "(", "fn", ")", "\n", "", "", "", "eval_fn_list", "=", "new_eval_fn_list", "\n", "\n", "", "logger", ".", "info", "(", "\"***** Evaluation: %s *****\"", ",", "','", ".", "join", "(", "eval_fn_list", ")", ")", "\n", "num_pool", "=", "min", "(", "args", ".", "processes", ",", "len", "(", "eval_fn_list", ")", ")", "\n", "p", "=", "Pool", "(", "num_pool", ")", "\n", "r_list", "=", "p", ".", "imap_unordered", "(", "process_eval", ",", "eval_fn_list", ")", "\n", "r_list", "=", "sorted", "(", "[", "(", "fn", ",", "scores", ")", "\n", "for", "fn", ",", "scores", "in", "r_list", "]", ",", "key", "=", "lambda", "x", ":", "x", "[", "0", "]", ")", "\n", "rg2_dict", "=", "{", "}", "\n", "for", "fn", ",", "scores", "in", "r_list", ":", "\n", "        ", "print", "(", "fn", ")", "\n", "if", "args", ".", "perl", ":", "\n", "            ", "print", "(", "rouge_results_to_str", "(", "scores", ")", ")", "\n", "", "else", ":", "\n", "            ", "rg2_dict", "[", "fn", "]", "=", "scores", "[", "'rouge-2'", "]", "[", "'f'", "]", "\n", "print", "(", "\n", "\"ROUGE-1: {}\\tROUGE-2: {}\\n\"", ".", "format", "(", "scores", "[", "'rouge-1'", "]", "[", "'f'", "]", ",", "scores", "[", "'rouge-2'", "]", "[", "'f'", "]", ")", ")", "\n", "with", "open", "(", "fn", "+", "\".rouge\"", ",", "'w'", ")", "as", "f_out", ":", "\n", "                ", "f_out", ".", "write", "(", "json", ".", "dumps", "(", "\n", "{", "'rg1'", ":", "scores", "[", "'rouge-1'", "]", "[", "'f'", "]", ",", "'rg2'", ":", "scores", "[", "'rouge-2'", "]", "[", "'f'", "]", "}", ")", ")", "\n", "", "", "", "p", ".", "close", "(", ")", "\n", "p", ".", "join", "(", ")", "\n", "\n", "if", "args", ".", "save_best", ":", "\n", "# find best results", "\n", "        ", "group_dict", "=", "{", "}", "\n", "for", "k", ",", "v", "in", "rg2_dict", ".", "items", "(", ")", ":", "\n", "            ", "d_name", ",", "o_name", "=", "Path", "(", "k", ")", ".", "parent", ",", "Path", "(", "k", ")", ".", "name", "\n", "if", "(", "d_name", "not", "in", "group_dict", ")", "or", "(", "v", ">", "group_dict", "[", "d_name", "]", "[", "1", "]", ")", ":", "\n", "                ", "group_dict", "[", "d_name", "]", "=", "(", "o_name", ",", "v", ")", "\n", "# compare and save the best result", "\n", "", "", "for", "k", ",", "v", "in", "group_dict", ".", "items", "(", ")", ":", "\n", "            ", "fn", "=", "os", ".", "path", ".", "join", "(", "k", ",", "'save_best.'", "+", "args", ".", "split", ")", "\n", "o_name_s", ",", "rst_s", "=", "v", "\n", "should_save", "=", "True", "\n", "if", "Path", "(", "fn", ")", ".", "exists", "(", ")", ":", "\n", "                ", "with", "open", "(", "fn", ",", "'r'", ")", "as", "f_in", ":", "\n", "                    ", "rst_f", "=", "float", "(", "f_in", ".", "read", "(", ")", ".", "strip", "(", ")", ".", "split", "(", "'\\n'", ")", "[", "-", "1", "]", ")", "\n", "", "if", "rst_s", "<=", "rst_f", ":", "\n", "                    ", "should_save", "=", "False", "\n", "", "", "if", "should_save", ":", "\n", "                ", "with", "open", "(", "fn", ",", "'w'", ")", "as", "f_out", ":", "\n", "                    ", "f_out", ".", "write", "(", "'{0}\\n{1}\\n{2}\\n'", ".", "format", "(", "k", ",", "o_name_s", ",", "rst_s", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.cnndm.postprocess_cnn_dm._is_digit": [[28, 33], ["ch.isdigit"], "function", ["None"], ["def", "_is_digit", "(", "w", ")", ":", "\n", "    ", "for", "ch", "in", "w", ":", "\n", "        ", "if", "not", "(", "ch", ".", "isdigit", "(", ")", "or", "ch", "==", "','", ")", ":", "\n", "            ", "return", "False", "\n", "", "", "return", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.cnndm.postprocess_cnn_dm.fix_tokenization": [[34, 112], ["text.split", "len", "_tok_dict.keys", "output_tokens.append", "output_tokens.append", "output_tokens.append", "output_tokens[].endswith", "output_tokens.append", "len", "output_tokens.append", "len", "len", "output_tokens.append", "output_tokens.append", "output_tokens.append", "postprocess_cnn_dm._is_digit", "postprocess_cnn_dm._is_digit", "len", "len", "output_tokens[].isdigit", "input_tokens[].isdigit", "len", "len", "output_tokens[].isupper", "input_tokens[].isupper", "len", "len", "len", "len", "len", "len", "input_tokens[].isupper", "output_tokens.append", "output_tokens.append", "len", "output_tokens.append", "len", "len", "output_tokens.append", "len"], "function", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.cnndm.postprocess_cnn_dm._is_digit", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.cnndm.postprocess_cnn_dm._is_digit"], ["", "def", "fix_tokenization", "(", "text", ")", ":", "\n", "    ", "input_tokens", "=", "text", ".", "split", "(", ")", "\n", "output_tokens", "=", "[", "]", "\n", "has_left_quote", "=", "False", "\n", "has_left_single_quote", "=", "False", "\n", "\n", "i", "=", "0", "\n", "prev_dash", "=", "False", "\n", "while", "i", "<", "len", "(", "input_tokens", ")", ":", "\n", "        ", "tok", "=", "input_tokens", "[", "i", "]", "\n", "flag_prev_dash", "=", "False", "\n", "if", "tok", "in", "_tok_dict", ".", "keys", "(", ")", ":", "\n", "            ", "output_tokens", ".", "append", "(", "_tok_dict", "[", "tok", "]", ")", "\n", "i", "+=", "1", "\n", "", "elif", "tok", "==", "\"\\\"\"", ":", "\n", "            ", "if", "has_left_quote", ":", "\n", "                ", "output_tokens", ".", "append", "(", "\"''\"", ")", "\n", "", "else", ":", "\n", "                ", "output_tokens", ".", "append", "(", "\"``\"", ")", "\n", "", "has_left_quote", "=", "not", "has_left_quote", "\n", "i", "+=", "1", "\n", "", "elif", "tok", "==", "\"'\"", "and", "len", "(", "output_tokens", ")", ">", "0", "and", "output_tokens", "[", "-", "1", "]", ".", "endswith", "(", "\"n\"", ")", "and", "i", "<", "len", "(", "input_tokens", ")", "-", "1", "and", "input_tokens", "[", "i", "+", "1", "]", "==", "\"t\"", ":", "\n", "            ", "output_tokens", "[", "-", "1", "]", "=", "output_tokens", "[", "-", "1", "]", "[", ":", "-", "1", "]", "\n", "output_tokens", ".", "append", "(", "\"n't\"", ")", "\n", "i", "+=", "2", "\n", "", "elif", "tok", "==", "\"'\"", "and", "i", "<", "len", "(", "input_tokens", ")", "-", "1", "and", "input_tokens", "[", "i", "+", "1", "]", "in", "(", "\"s\"", ",", "\"d\"", ",", "\"ll\"", ")", ":", "\n", "            ", "output_tokens", ".", "append", "(", "\"'\"", "+", "input_tokens", "[", "i", "+", "1", "]", ")", "\n", "i", "+=", "2", "\n", "", "elif", "tok", "==", "\"'\"", ":", "\n", "            ", "if", "has_left_single_quote", ":", "\n", "                ", "output_tokens", ".", "append", "(", "\"'\"", ")", "\n", "", "else", ":", "\n", "                ", "output_tokens", ".", "append", "(", "\"`\"", ")", "\n", "", "has_left_single_quote", "=", "not", "has_left_single_quote", "\n", "i", "+=", "1", "\n", "", "elif", "tok", "==", "\".\"", "and", "i", "<", "len", "(", "input_tokens", ")", "-", "2", "and", "input_tokens", "[", "i", "+", "1", "]", "==", "\".\"", "and", "input_tokens", "[", "i", "+", "2", "]", "==", "\".\"", ":", "\n", "            ", "output_tokens", ".", "append", "(", "\"...\"", ")", "\n", "i", "+=", "3", "\n", "", "elif", "tok", "==", "\",\"", "and", "len", "(", "output_tokens", ")", ">", "0", "and", "_is_digit", "(", "output_tokens", "[", "-", "1", "]", ")", "and", "i", "<", "len", "(", "input_tokens", ")", "-", "1", "and", "_is_digit", "(", "input_tokens", "[", "i", "+", "1", "]", ")", ":", "\n", "# $ 3 , 000 -> $ 3,000", "\n", "            ", "output_tokens", "[", "-", "1", "]", "+=", "','", "+", "input_tokens", "[", "i", "+", "1", "]", "\n", "i", "+=", "2", "\n", "", "elif", "tok", "==", "\".\"", "and", "len", "(", "output_tokens", ")", ">", "0", "and", "output_tokens", "[", "-", "1", "]", ".", "isdigit", "(", ")", "and", "i", "<", "len", "(", "input_tokens", ")", "-", "1", "and", "input_tokens", "[", "i", "+", "1", "]", ".", "isdigit", "(", ")", ":", "\n", "# 3 . 03 -> $ 3.03", "\n", "            ", "output_tokens", "[", "-", "1", "]", "+=", "'.'", "+", "input_tokens", "[", "i", "+", "1", "]", "\n", "i", "+=", "2", "\n", "", "elif", "tok", "==", "\".\"", "and", "len", "(", "output_tokens", ")", ">", "0", "and", "len", "(", "output_tokens", "[", "-", "1", "]", ")", "==", "1", "and", "output_tokens", "[", "-", "1", "]", ".", "isupper", "(", ")", "and", "i", "<", "len", "(", "input_tokens", ")", "-", "2", "and", "len", "(", "input_tokens", "[", "i", "+", "1", "]", ")", "==", "1", "and", "input_tokens", "[", "i", "+", "1", "]", ".", "isupper", "(", ")", "and", "input_tokens", "[", "i", "+", "2", "]", "==", "'.'", ":", "\n", "# U . N . -> U.N.", "\n", "            ", "k", "=", "i", "+", "3", "\n", "while", "k", "+", "2", "<", "len", "(", "input_tokens", ")", ":", "\n", "                ", "if", "len", "(", "input_tokens", "[", "k", "+", "1", "]", ")", "==", "1", "and", "input_tokens", "[", "k", "+", "1", "]", ".", "isupper", "(", ")", "and", "input_tokens", "[", "k", "+", "2", "]", "==", "'.'", ":", "\n", "                    ", "k", "+=", "2", "\n", "", "else", ":", "\n", "                    ", "break", "\n", "", "", "output_tokens", "[", "-", "1", "]", "+=", "''", ".", "join", "(", "input_tokens", "[", "i", ":", "k", "]", ")", "\n", "i", "+=", "2", "\n", "", "elif", "tok", "==", "\"-\"", ":", "\n", "            ", "if", "i", "<", "len", "(", "input_tokens", ")", "-", "1", "and", "input_tokens", "[", "i", "+", "1", "]", "==", "\"-\"", ":", "\n", "                ", "output_tokens", ".", "append", "(", "\"--\"", ")", "\n", "i", "+=", "2", "\n", "", "elif", "i", "==", "len", "(", "input_tokens", ")", "-", "1", "or", "i", "==", "0", ":", "\n", "                ", "output_tokens", ".", "append", "(", "\"-\"", ")", "\n", "i", "+=", "1", "\n", "", "elif", "output_tokens", "[", "-", "1", "]", "not", "in", "string", ".", "punctuation", "and", "input_tokens", "[", "i", "+", "1", "]", "[", "0", "]", "not", "in", "string", ".", "punctuation", ":", "\n", "                ", "output_tokens", "[", "-", "1", "]", "+=", "\"-\"", "\n", "i", "+=", "1", "\n", "flag_prev_dash", "=", "True", "\n", "", "else", ":", "\n", "                ", "output_tokens", ".", "append", "(", "\"-\"", ")", "\n", "i", "+=", "1", "\n", "", "", "elif", "prev_dash", "and", "len", "(", "output_tokens", ")", ">", "0", "and", "tok", "[", "0", "]", "not", "in", "string", ".", "punctuation", ":", "\n", "            ", "output_tokens", "[", "-", "1", "]", "+=", "tok", "\n", "i", "+=", "1", "\n", "", "else", ":", "\n", "            ", "output_tokens", ".", "append", "(", "tok", ")", "\n", "i", "+=", "1", "\n", "", "prev_dash", "=", "flag_prev_dash", "\n", "", "return", "\" \"", ".", "join", "(", "output_tokens", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.cnndm.postprocess_cnn_dm.remove_duplicate": [[113, 123], ["set", "enumerate", "l.lower().split", "set", "r_list.append", "l.lower", "len", "len"], "function", ["None"], ["", "def", "remove_duplicate", "(", "l_list", ",", "duplicate_rate", ")", ":", "\n", "    ", "tk_list", "=", "[", "l", ".", "lower", "(", ")", ".", "split", "(", ")", "for", "l", "in", "l_list", "]", "\n", "r_list", "=", "[", "]", "\n", "history_set", "=", "set", "(", ")", "\n", "for", "i", ",", "w_list", "in", "enumerate", "(", "tk_list", ")", ":", "\n", "        ", "w_set", "=", "set", "(", "w_list", ")", "\n", "if", "len", "(", "w_set", "&", "history_set", ")", "/", "len", "(", "w_set", ")", "<=", "duplicate_rate", ":", "\n", "            ", "r_list", ".", "append", "(", "l_list", "[", "i", "]", ")", "\n", "", "history_set", "|=", "w_set", "\n", "", "return", "r_list", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.cnndm.postprocess_cnn_dm.test_rouge": [[124, 159], ["tempfile.mkdtemp", "len", "time.strftime", "os.path.join", "len", "len", "time.localtime", "os.path.isdir", "os.mkdir", "os.mkdir", "os.mkdir", "range", "bs_pyrouge.Rouge155", "bs_pyrouge.Rouge155.convert_and_evaluate", "print", "bs_pyrouge.Rouge155.output_to_dict", "os.path.isdir", "shutil.rmtree", "len", "open", "f.write", "open", "f.write"], "function", ["None"], ["", "def", "test_rouge", "(", "cand", ",", "ref", ")", ":", "\n", "    ", "temp_dir", "=", "tempfile", ".", "mkdtemp", "(", ")", "\n", "candidates", "=", "cand", "\n", "references", "=", "ref", "\n", "assert", "len", "(", "candidates", ")", "==", "len", "(", "references", ")", "\n", "\n", "cnt", "=", "len", "(", "candidates", ")", "\n", "current_time", "=", "time", ".", "strftime", "(", "'%Y-%m-%d-%H-%M-%S'", ",", "time", ".", "localtime", "(", ")", ")", "\n", "tmp_dir", "=", "os", ".", "path", ".", "join", "(", "temp_dir", ",", "\"rouge-tmp-{}\"", ".", "format", "(", "current_time", ")", ")", "\n", "if", "not", "os", ".", "path", ".", "isdir", "(", "tmp_dir", ")", ":", "\n", "        ", "os", ".", "mkdir", "(", "tmp_dir", ")", "\n", "os", ".", "mkdir", "(", "tmp_dir", "+", "\"/candidate\"", ")", "\n", "os", ".", "mkdir", "(", "tmp_dir", "+", "\"/reference\"", ")", "\n", "", "try", ":", "\n", "        ", "for", "i", "in", "range", "(", "cnt", ")", ":", "\n", "            ", "if", "len", "(", "references", "[", "i", "]", ")", "<", "1", ":", "\n", "                ", "continue", "\n", "", "with", "open", "(", "tmp_dir", "+", "\"/candidate/cand.{}.txt\"", ".", "format", "(", "i", ")", ",", "\"w\"", ",", "\n", "encoding", "=", "\"utf-8\"", ")", "as", "f", ":", "\n", "                ", "f", ".", "write", "(", "candidates", "[", "i", "]", ")", "\n", "", "with", "open", "(", "tmp_dir", "+", "\"/reference/ref.{}.txt\"", ".", "format", "(", "i", ")", ",", "\"w\"", ",", "\n", "encoding", "=", "\"utf-8\"", ")", "as", "f", ":", "\n", "                ", "f", ".", "write", "(", "references", "[", "i", "]", ")", "\n", "", "", "r", "=", "Rouge155", "(", "temp_dir", "=", "temp_dir", ")", "\n", "r", ".", "model_dir", "=", "tmp_dir", "+", "\"/reference/\"", "\n", "r", ".", "system_dir", "=", "tmp_dir", "+", "\"/candidate/\"", "\n", "r", ".", "model_filename_pattern", "=", "'ref.#ID#.txt'", "\n", "r", ".", "system_filename_pattern", "=", "r'cand.(\\d+).txt'", "\n", "rouge_results", "=", "r", ".", "convert_and_evaluate", "(", ")", "\n", "print", "(", "rouge_results", ")", "\n", "results_dict", "=", "r", ".", "output_to_dict", "(", "rouge_results", ")", "\n", "", "finally", ":", "\n", "        ", "if", "os", ".", "path", ".", "isdir", "(", "tmp_dir", ")", ":", "\n", "            ", "shutil", ".", "rmtree", "(", "tmp_dir", ")", "\n", "", "", "return", "results_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.cnndm.postprocess_cnn_dm.rouge_results_to_str": [[160, 168], ["None"], "function", ["None"], ["", "def", "rouge_results_to_str", "(", "results_dict", ")", ":", "\n", "    ", "return", "\">> ROUGE-F(1/2/l): {:.2f}/{:.2f}/{:.2f}\\nROUGE-R(1/2/3/l): {:.2f}/{:.2f}/{:.2f}\\n\"", ".", "format", "(", "\n", "results_dict", "[", "\"rouge_1_f_score\"", "]", "*", "100", ",", "\n", "results_dict", "[", "\"rouge_2_f_score\"", "]", "*", "100", ",", "\n", "results_dict", "[", "\"rouge_l_f_score\"", "]", "*", "100", ",", "\n", "results_dict", "[", "\"rouge_1_recall\"", "]", "*", "100", ",", "\n", "results_dict", "[", "\"rouge_2_recall\"", "]", "*", "100", ",", "\n", "results_dict", "[", "\"rouge_l_recall\"", "]", "*", "100", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.cnndm.postprocess_cnn_dm.count_tokens": [[170, 178], ["counter.keys"], "function", ["None"], ["", "def", "count_tokens", "(", "tokens", ")", ":", "\n", "    ", "counter", "=", "{", "}", "\n", "for", "t", "in", "tokens", ":", "\n", "        ", "if", "t", "in", "counter", ".", "keys", "(", ")", ":", "\n", "            ", "counter", "[", "t", "]", "+=", "1", "\n", "", "else", ":", "\n", "            ", "counter", "[", "t", "]", "=", "1", "\n", "", "", "return", "counter", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.cnndm.postprocess_cnn_dm.get_f1": [[179, 193], ["text_a.lower().split", "text_b.lower().split", "postprocess_cnn_dm.count_tokens", "postprocess_cnn_dm.count_tokens", "count_tokens.keys", "len", "len", "text_a.lower", "text_b.lower", "len", "len", "count_tokens.keys", "min", "len", "len"], "function", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.cnndm.postprocess_cnn_dm.count_tokens", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.cnndm.postprocess_cnn_dm.count_tokens"], ["", "def", "get_f1", "(", "text_a", ",", "text_b", ")", ":", "\n", "    ", "tokens_a", "=", "text_a", ".", "lower", "(", ")", ".", "split", "(", ")", "\n", "tokens_b", "=", "text_b", ".", "lower", "(", ")", ".", "split", "(", ")", "\n", "if", "len", "(", "tokens_a", ")", "==", "0", "or", "len", "(", "tokens_b", ")", "==", "0", ":", "\n", "        ", "return", "1", "if", "len", "(", "tokens_a", ")", "==", "len", "(", "tokens_b", ")", "else", "0", "\n", "", "set_a", "=", "count_tokens", "(", "tokens_a", ")", "\n", "set_b", "=", "count_tokens", "(", "tokens_b", ")", "\n", "match", "=", "0", "\n", "for", "token", "in", "set_a", ".", "keys", "(", ")", ":", "\n", "        ", "if", "token", "in", "set_b", ".", "keys", "(", ")", ":", "\n", "            ", "match", "+=", "min", "(", "set_a", "[", "token", "]", ",", "set_b", "[", "token", "]", ")", "\n", "", "", "p", "=", "match", "/", "len", "(", "tokens_a", ")", "\n", "r", "=", "match", "/", "len", "(", "tokens_b", ")", "\n", "return", "2.0", "*", "p", "*", "r", "/", "(", "p", "+", "r", "+", "1e-5", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.distribute.enable_distributed_training": [[13, 23], ["hvd.init", "sys.stderr.write", "exit"], "function", ["None"], ["def", "enable_distributed_training", "(", ")", ":", "\n", "    ", "global", "_ENGINE", "\n", "try", ":", "\n", "        ", "import", "horovod", ".", "tensorflow", "as", "hvd", "\n", "_ENGINE", "=", "hvd", "\n", "hvd", ".", "init", "(", ")", "\n", "", "except", "ImportError", ":", "\n", "        ", "sys", ".", "stderr", ".", "write", "(", "\"Error: You must install horovod first in order to\"", "\n", "\" enable distributed training.\\n\"", ")", "\n", "exit", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.distribute.is_distributed_training_mode": [[25, 27], ["None"], "function", ["None"], ["", "", "def", "is_distributed_training_mode", "(", ")", ":", "\n", "    ", "return", "_ENGINE", "is", "not", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.distribute.rank": [[29, 31], ["_ENGINE.rank"], "function", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.distribute.rank"], ["", "def", "rank", "(", ")", ":", "\n", "    ", "return", "_ENGINE", ".", "rank", "(", ")", "if", "_ENGINE", "is", "not", "None", "else", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.distribute.local_rank": [[33, 35], ["_ENGINE.local_rank"], "function", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.distribute.local_rank"], ["", "def", "local_rank", "(", ")", ":", "\n", "    ", "return", "_ENGINE", ".", "local_rank", "(", ")", "if", "_ENGINE", "is", "not", "None", "else", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.distribute.size": [[37, 39], ["_ENGINE.size"], "function", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.distribute.size"], ["", "def", "size", "(", ")", ":", "\n", "    ", "return", "_ENGINE", ".", "size", "(", ")", "if", "_ENGINE", "is", "not", "None", "else", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.distribute.all_reduce": [[41, 46], ["_ENGINE.allreduce"], "function", ["None"], ["", "def", "all_reduce", "(", "tensor", ")", ":", "\n", "    ", "if", "_ENGINE", "is", "None", ":", "\n", "        ", "return", "tensor", "\n", "\n", "", "return", "_ENGINE", ".", "allreduce", "(", "tensor", ",", "compression", "=", "_ENGINE", ".", "Compression", ".", "fp16", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.distribute.get_broadcast_hook": [[48, 52], ["_ENGINE.BroadcastGlobalVariablesHook"], "function", ["None"], ["", "def", "get_broadcast_hook", "(", ")", ":", "\n", "    ", "if", "not", "_ENGINE", ":", "\n", "        ", "return", "None", "\n", "", "return", "_ENGINE", ".", "BroadcastGlobalVariablesHook", "(", "0", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.common.infer_shape": [[11, 29], ["tensorflow.convert_to_tensor", "tf.convert_to_tensor.shape.as_list", "tensorflow.shape", "range", "tensorflow.shape", "len", "ret.append"], "function", ["None"], ["def", "infer_shape", "(", "x", ")", ":", "\n", "    ", "x", "=", "tf", ".", "convert_to_tensor", "(", "x", ")", "\n", "\n", "# If unknown rank, return dynamic shape", "\n", "if", "x", ".", "shape", ".", "dims", "is", "None", ":", "\n", "        ", "return", "tf", ".", "shape", "(", "x", ")", "\n", "\n", "", "static_shape", "=", "x", ".", "shape", ".", "as_list", "(", ")", "\n", "dynamic_shape", "=", "tf", ".", "shape", "(", "x", ")", "\n", "\n", "ret", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "static_shape", ")", ")", ":", "\n", "        ", "dim", "=", "static_shape", "[", "i", "]", "\n", "if", "dim", "is", "None", ":", "\n", "            ", "dim", "=", "dynamic_shape", "[", "i", "]", "\n", "", "ret", ".", "append", "(", "dim", ")", "\n", "\n", "", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.common.infer_shape_invariants": [[31, 36], ["tensor.shape.as_list", "range", "tensorflow.TensorShape", "len"], "function", ["None"], ["", "def", "infer_shape_invariants", "(", "tensor", ")", ":", "\n", "    ", "shape", "=", "tensor", ".", "shape", ".", "as_list", "(", ")", "\n", "for", "i", "in", "range", "(", "1", ",", "len", "(", "shape", ")", "-", "1", ")", ":", "\n", "        ", "shape", "[", "i", "]", "=", "None", "\n", "", "return", "tf", ".", "TensorShape", "(", "shape", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.common.merge_first_two_dims": [[38, 43], ["common.infer_shape", "infer_shape.pop", "tensorflow.reshape"], "function", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.common.infer_shape"], ["", "def", "merge_first_two_dims", "(", "tensor", ")", ":", "\n", "    ", "shape", "=", "infer_shape", "(", "tensor", ")", "\n", "shape", "[", "0", "]", "*=", "shape", "[", "1", "]", "\n", "shape", ".", "pop", "(", "1", ")", "\n", "return", "tf", ".", "reshape", "(", "tensor", ",", "shape", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.common.split_first_two_dims": [[45, 49], ["common.infer_shape", "tensorflow.reshape"], "function", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.common.infer_shape"], ["", "def", "split_first_two_dims", "(", "tensor", ",", "dim_0", ",", "dim_1", ")", ":", "\n", "    ", "shape", "=", "infer_shape", "(", "tensor", ")", "\n", "new_shape", "=", "[", "dim_0", "]", "+", "[", "dim_1", "]", "+", "shape", "[", "1", ":", "]", "\n", "return", "tf", ".", "reshape", "(", "tensor", ",", "new_shape", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.common.tile_to_beam_size": [[51, 58], ["tensorflow.expand_dims", "tensorflow.tile"], "function", ["None"], ["", "def", "tile_to_beam_size", "(", "tensor", ",", "beam_size", ")", ":", "\n", "    ", "\"\"\"Tiles a given tensor by beam_size. \"\"\"", "\n", "tensor", "=", "tf", ".", "expand_dims", "(", "tensor", ",", "axis", "=", "1", ")", "\n", "tile_dims", "=", "[", "1", "]", "*", "tensor", ".", "shape", ".", "ndims", "\n", "tile_dims", "[", "1", "]", "=", "beam_size", "\n", "\n", "return", "tf", ".", "tile", "(", "tensor", ",", "tile_dims", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.common.tile_batch": [[60, 69], ["common.infer_shape", "tensorflow.tile", "tensorflow.reshape", "tensorflow.expand_dims"], "function", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.common.infer_shape"], ["", "def", "tile_batch", "(", "tensor", ",", "batch_size", ")", ":", "\n", "    ", "shape", "=", "infer_shape", "(", "tensor", ")", "\n", "tile_dims", "=", "[", "1", "]", "*", "(", "tensor", ".", "shape", ".", "ndims", "+", "1", ")", "\n", "tile_dims", "[", "1", "]", "=", "batch_size", "\n", "\n", "tensor", "=", "tf", ".", "tile", "(", "tf", ".", "expand_dims", "(", "tensor", ",", "axis", "=", "1", ")", ",", "tile_dims", ")", "\n", "shape", "[", "0", "]", "=", "shape", "[", "0", "]", "*", "batch_size", "\n", "\n", "return", "tf", ".", "reshape", "(", "tensor", ",", "shape", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.common.gather_2d": [[71, 86], ["tensorflow.reshape", "tensorflow.stack", "tensorflow.gather_nd", "tensorflow.shape", "tensorflow.shape", "tensorflow.range"], "function", ["None"], ["", "def", "gather_2d", "(", "params", ",", "indices", ",", "name", "=", "None", ")", ":", "\n", "    ", "\"\"\" Gather the 2nd dimension given indices\n    :param params: A tensor with shape [batch_size, M, ...]\n    :param indices: A tensor with shape [batch_size, N]\n    :param name: An optional string\n    :return: A tensor with shape [batch_size, N, ...]\n    \"\"\"", "\n", "batch_size", "=", "tf", ".", "shape", "(", "params", ")", "[", "0", "]", "\n", "range_size", "=", "tf", ".", "shape", "(", "indices", ")", "[", "1", "]", "\n", "batch_pos", "=", "tf", ".", "range", "(", "batch_size", "*", "range_size", ")", "//", "range_size", "\n", "batch_pos", "=", "tf", ".", "reshape", "(", "batch_pos", ",", "[", "batch_size", ",", "range_size", "]", ")", "\n", "indices", "=", "tf", ".", "stack", "(", "[", "batch_pos", ",", "indices", "]", ",", "axis", "=", "-", "1", ")", "\n", "output", "=", "tf", ".", "gather_nd", "(", "params", ",", "indices", ",", "name", "=", "name", ")", "\n", "\n", "return", "output", "\n", "", ""]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.inference._get_inference_fn": [[21, 51], ["zip", "tensorflow.pad", "tensorflow.fill", "tensorflow.add_n", "float", "model_fn", "outputs.append", "next_state.append", "model_fn", "outputs.append", "next_state.append", "len", "tensorflow.shape", "tensorflow.shape"], "function", ["None"], ["", "def", "_get_inference_fn", "(", "model_fns", ",", "features", ")", ":", "\n", "    ", "def", "inference_fn", "(", "inputs", ",", "state", ")", ":", "\n", "        ", "local_features", "=", "{", "\n", "\"source\"", ":", "features", "[", "\"source\"", "]", ",", "\n", "\"source_length\"", ":", "features", "[", "\"source_length\"", "]", ",", "\n", "# [bos_id, ...] => [..., 0]", "\n", "\"target\"", ":", "tf", ".", "pad", "(", "inputs", "[", ":", ",", "1", ":", "]", ",", "[", "[", "0", ",", "0", "]", ",", "[", "0", ",", "1", "]", "]", ")", ",", "\n", "\"target_length\"", ":", "tf", ".", "fill", "(", "[", "tf", ".", "shape", "(", "inputs", ")", "[", "0", "]", "]", ",", "\n", "tf", ".", "shape", "(", "inputs", ")", "[", "1", "]", ")", "\n", "}", "\n", "\n", "outputs", "=", "[", "]", "\n", "next_state", "=", "[", "]", "\n", "\n", "for", "(", "model_fn", ",", "model_state", ")", "in", "zip", "(", "model_fns", ",", "state", ")", ":", "\n", "            ", "if", "model_state", ":", "\n", "                ", "output", ",", "new_state", "=", "model_fn", "(", "local_features", ",", "model_state", ")", "\n", "outputs", ".", "append", "(", "output", ")", "\n", "next_state", ".", "append", "(", "new_state", ")", "\n", "", "else", ":", "\n", "                ", "output", "=", "model_fn", "(", "local_features", ")", "\n", "outputs", ".", "append", "(", "output", ")", "\n", "next_state", ".", "append", "(", "{", "}", ")", "\n", "\n", "# Ensemble", "\n", "", "", "log_prob", "=", "tf", ".", "add_n", "(", "outputs", ")", "/", "float", "(", "len", "(", "outputs", ")", ")", "\n", "\n", "return", "log_prob", ",", "next_state", "\n", "\n", "", "return", "inference_fn", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.inference._beam_search_step": [[53, 127], ["thumt.merge_first_two_dims", "tensorflow.python.util.nest.map_structure", "func", "thumt.split_first_two_dims", "tensorflow.python.util.nest.map_structure", "tensorflow.pow", "tensorflow.reshape", "tensorflow.nn.top_k", "thumt.gather_2d", "tensorflow.concat", "tensorflow.equal", "tensorflow.nn.top_k", "thumt.gather_2d", "thumt.gather_2d", "thumt.gather_2d", "tensorflow.concat", "tensorflow.python.util.nest.map_structure", "tensorflow.concat", "tensorflow.concat", "tensorflow.nn.top_k", "thumt.gather_2d", "tensorflow.fill", "tensorflow.concat", "tensorflow.concat", "thumt.gather_2d", "inference.BeamSearchState", "tensorflow.expand_dims", "tensorflow.constant", "thumt.merge_first_two_dims", "thumt.split_first_two_dims", "tensorflow.shape", "tensorflow.expand_dims", "tensorflow.to_float", "tensorflow.expand_dims", "thumt.gather_2d", "tensorflow.to_float", "tensorflow.to_float"], "function", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.common.merge_first_two_dims", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.common.split_first_two_dims", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.common.gather_2d", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.common.gather_2d", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.common.gather_2d", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.common.gather_2d", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.common.gather_2d", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.common.gather_2d", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.common.merge_first_two_dims", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.common.split_first_two_dims", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.common.gather_2d"], ["", "def", "_beam_search_step", "(", "time", ",", "func", ",", "state", ",", "batch_size", ",", "beam_size", ",", "alpha", ",", "\n", "pad_id", ",", "eos_id", ")", ":", "\n", "    ", "\"\"\"\n      Before beam search, source's shape is [batch_size*beam_size, length],\n    state and seqs shape is [batch_size, beam_size, length]\n    \"\"\"", "\n", "# Compute log probabilities", "\n", "seqs", ",", "log_probs", "=", "state", ".", "inputs", "[", ":", "2", "]", "\n", "# [batch_size, beam_size, length] => [batch_size*beam_size, length]", "\n", "flat_seqs", "=", "utils", ".", "merge_first_two_dims", "(", "seqs", ")", "\n", "flat_state", "=", "nest", ".", "map_structure", "(", "lambda", "x", ":", "utils", ".", "merge_first_two_dims", "(", "x", ")", ",", "\n", "state", ".", "state", ")", "\n", "step_log_probs", ",", "next_state", "=", "func", "(", "flat_seqs", ",", "flat_state", ")", "\n", "# [batch_size*beam_size, vocab_size] => [batch_size, beam_size, vocab_size]", "\n", "step_log_probs", "=", "utils", ".", "split_first_two_dims", "(", "step_log_probs", ",", "batch_size", ",", "\n", "beam_size", ")", "\n", "next_state", "=", "nest", ".", "map_structure", "(", "\n", "lambda", "x", ":", "utils", ".", "split_first_two_dims", "(", "x", ",", "batch_size", ",", "beam_size", ")", ",", "\n", "next_state", ")", "\n", "# [batch_size, beam_size, 1]+[batch_size, beam_size, vocab_size]", "\n", "curr_log_probs", "=", "tf", ".", "expand_dims", "(", "log_probs", ",", "2", ")", "+", "step_log_probs", "\n", "\n", "# Apply length penalty", "\n", "length_penalty", "=", "tf", ".", "pow", "(", "(", "5.0", "+", "tf", ".", "to_float", "(", "time", "+", "1", ")", ")", "/", "6.0", ",", "alpha", ")", "\n", "curr_scores", "=", "curr_log_probs", "/", "length_penalty", "\n", "vocab_size", "=", "curr_scores", ".", "shape", "[", "-", "1", "]", ".", "value", "or", "tf", ".", "shape", "(", "curr_scores", ")", "[", "-", "1", "]", "\n", "\n", "# Select top-k candidates", "\n", "# [batch_size, beam_size * vocab_size]", "\n", "curr_scores", "=", "tf", ".", "reshape", "(", "curr_scores", ",", "[", "-", "1", ",", "beam_size", "*", "vocab_size", "]", ")", "\n", "# [batch_size, 2 * beam_size]", "\n", "top_scores", ",", "top_indices", "=", "tf", ".", "nn", ".", "top_k", "(", "curr_scores", ",", "k", "=", "2", "*", "beam_size", ")", "\n", "# Shape: [batch_size, 2 * beam_size]", "\n", "beam_indices", "=", "top_indices", "//", "vocab_size", "\n", "symbol_indices", "=", "top_indices", "%", "vocab_size", "\n", "# Expand sequences", "\n", "# [batch_size, 2 * beam_size, time]", "\n", "candidate_seqs", "=", "utils", ".", "gather_2d", "(", "seqs", ",", "beam_indices", ")", "\n", "candidate_seqs", "=", "tf", ".", "concat", "(", "[", "candidate_seqs", ",", "\n", "tf", ".", "expand_dims", "(", "symbol_indices", ",", "2", ")", "]", ",", "2", ")", "\n", "\n", "# Expand sequences", "\n", "# Suppress finished sequences", "\n", "flags", "=", "tf", ".", "equal", "(", "symbol_indices", ",", "eos_id", ")", "\n", "# [batch, 2 * beam_size]", "\n", "alive_scores", "=", "top_scores", "+", "tf", ".", "to_float", "(", "flags", ")", "*", "tf", ".", "float32", ".", "min", "\n", "# [batch, beam_size]", "\n", "alive_scores", ",", "alive_indices", "=", "tf", ".", "nn", ".", "top_k", "(", "alive_scores", ",", "beam_size", ")", "\n", "alive_symbols", "=", "utils", ".", "gather_2d", "(", "symbol_indices", ",", "alive_indices", ")", "\n", "alive_indices", "=", "utils", ".", "gather_2d", "(", "beam_indices", ",", "alive_indices", ")", "\n", "alive_seqs", "=", "utils", ".", "gather_2d", "(", "seqs", ",", "alive_indices", ")", "\n", "# [batch_size, beam_size, time + 1]", "\n", "alive_seqs", "=", "tf", ".", "concat", "(", "[", "alive_seqs", ",", "tf", ".", "expand_dims", "(", "alive_symbols", ",", "2", ")", "]", ",", "2", ")", "\n", "alive_state", "=", "nest", ".", "map_structure", "(", "\n", "lambda", "x", ":", "utils", ".", "gather_2d", "(", "x", ",", "alive_indices", ")", ",", "\n", "next_state", ")", "\n", "alive_log_probs", "=", "alive_scores", "*", "length_penalty", "\n", "\n", "# Select finished sequences", "\n", "prev_fin_flags", ",", "prev_fin_seqs", ",", "prev_fin_scores", "=", "state", ".", "finish", "\n", "# [batch, 2 * beam_size]", "\n", "step_fin_scores", "=", "top_scores", "+", "(", "1.0", "-", "tf", ".", "to_float", "(", "flags", ")", ")", "*", "tf", ".", "float32", ".", "min", "\n", "# [batch, 3 * beam_size]", "\n", "fin_flags", "=", "tf", ".", "concat", "(", "[", "prev_fin_flags", ",", "flags", "]", ",", "axis", "=", "1", ")", "\n", "fin_scores", "=", "tf", ".", "concat", "(", "[", "prev_fin_scores", ",", "step_fin_scores", "]", ",", "axis", "=", "1", ")", "\n", "# [batch, beam_size]", "\n", "fin_scores", ",", "fin_indices", "=", "tf", ".", "nn", ".", "top_k", "(", "fin_scores", ",", "beam_size", ")", "\n", "fin_flags", "=", "utils", ".", "gather_2d", "(", "fin_flags", ",", "fin_indices", ")", "\n", "pad_seqs", "=", "tf", ".", "fill", "(", "[", "batch_size", ",", "beam_size", ",", "1", "]", ",", "\n", "tf", ".", "constant", "(", "pad_id", ",", "tf", ".", "int32", ")", ")", "\n", "prev_fin_seqs", "=", "tf", ".", "concat", "(", "[", "prev_fin_seqs", ",", "pad_seqs", "]", ",", "axis", "=", "2", ")", "\n", "fin_seqs", "=", "tf", ".", "concat", "(", "[", "prev_fin_seqs", ",", "candidate_seqs", "]", ",", "axis", "=", "1", ")", "\n", "fin_seqs", "=", "utils", ".", "gather_2d", "(", "fin_seqs", ",", "fin_indices", ")", "\n", "\n", "new_state", "=", "BeamSearchState", "(", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.inference.beam_search": [[129, 202], ["tensorflow.fill", "tensorflow.constant", "tensorflow.tile", "tensorflow.zeros_like", "tensorflow.zeros", "tensorflow.fill", "tensorflow.zeros", "inference.BeamSearchState", "tensorflow.reduce_max", "tensorflow.constant", "inference.BeamSearchState", "tensorflow.while_loop", "alive_seqs.set_shape", "tf.where.set_shape", "tensorflow.where", "tensorflow.where", "tensorflow.pow", "tensorflow.reduce_min", "tensorflow.reduce_all", "tensorflow.logical_and", "inference._beam_search_step", "tensorflow.reduce_any", "tensorflow.reduce_any", "tensorflow.to_float", "tensorflow.greater", "tensorflow.less", "tensorflow.logical_not", "tensorflow.python.util.nest.map_structure", "tensorflow.to_float", "tensorflow.reduce_any", "tensorflow.TensorShape", "tensorflow.TensorShape", "tensorflow.TensorShape", "tensorflow.TensorShape", "tensorflow.TensorShape", "tensorflow.TensorShape", "tensorflow.TensorShape", "tensorflow.to_float"], "function", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.inference._beam_search_step"], ["state", "=", "alive_state", ",", "\n", "finish", "=", "(", "fin_flags", ",", "fin_seqs", ",", "fin_scores", ")", ",", "\n", ")", "\n", "\n", "return", "time", "+", "1", ",", "new_state", "\n", "\n", "\n", "", "def", "beam_search", "(", "func", ",", "state", ",", "batch_size", ",", "beam_size", ",", "max_length", ",", "alpha", ",", "\n", "pad_id", ",", "bos_id", ",", "eos_id", ")", ":", "\n", "# func is get_inference_func/decoding_fn", "\n", "# state is get_inference_func/encoding_fn's output", "\n", "# states has been expand [batch_size, beam_size, ...]", "\n", "# {\"encoder\":encoder_output, \"decoder\":{\"layer_%d\":{\"key\":  ,\"value\":  }}}", "\n", "    ", "init_seqs", "=", "tf", ".", "fill", "(", "[", "batch_size", ",", "beam_size", ",", "1", "]", ",", "bos_id", ")", "\n", "init_log_probs", "=", "tf", ".", "constant", "(", "[", "[", "0.", "]", "+", "[", "tf", ".", "float32", ".", "min", "]", "*", "(", "beam_size", "-", "1", ")", "]", ")", "\n", "init_log_probs", "=", "tf", ".", "tile", "(", "init_log_probs", ",", "[", "batch_size", ",", "1", "]", ")", "\n", "init_scores", "=", "tf", ".", "zeros_like", "(", "init_log_probs", ")", "\n", "fin_seqs", "=", "tf", ".", "zeros", "(", "[", "batch_size", ",", "beam_size", ",", "1", "]", ",", "tf", ".", "int32", ")", "\n", "fin_scores", "=", "tf", ".", "fill", "(", "[", "batch_size", ",", "beam_size", "]", ",", "tf", ".", "float32", ".", "min", ")", "\n", "fin_flags", "=", "tf", ".", "zeros", "(", "[", "batch_size", ",", "beam_size", "]", ",", "tf", ".", "bool", ")", "\n", "\n", "state", "=", "BeamSearchState", "(", "\n", "inputs", "=", "(", "init_seqs", ",", "init_log_probs", ",", "init_scores", ")", ",", "\n", "state", "=", "state", ",", "\n", "finish", "=", "(", "fin_flags", ",", "fin_seqs", ",", "fin_scores", ")", ",", "\n", ")", "\n", "\n", "max_step", "=", "tf", ".", "reduce_max", "(", "max_length", ")", "\n", "\n", "def", "_is_finished", "(", "t", ",", "s", ")", ":", "\n", "        ", "log_probs", "=", "s", ".", "inputs", "[", "1", "]", "\n", "finished_flags", "=", "s", ".", "finish", "[", "0", "]", "\n", "finished_scores", "=", "s", ".", "finish", "[", "2", "]", "\n", "max_lp", "=", "tf", ".", "pow", "(", "(", "(", "5.0", "+", "tf", ".", "to_float", "(", "max_step", ")", ")", "/", "6.0", ")", ",", "alpha", ")", "\n", "best_alive_score", "=", "log_probs", "[", ":", ",", "0", "]", "/", "max_lp", "\n", "worst_finished_score", "=", "tf", ".", "reduce_min", "(", "\n", "finished_scores", "*", "tf", ".", "to_float", "(", "finished_flags", ")", ",", "axis", "=", "1", ")", "\n", "add_mask", "=", "1.0", "-", "tf", ".", "to_float", "(", "tf", ".", "reduce_any", "(", "finished_flags", ",", "1", ")", ")", "\n", "worst_finished_score", "+=", "tf", ".", "float32", ".", "min", "*", "add_mask", "\n", "bound_is_met", "=", "tf", ".", "reduce_all", "(", "tf", ".", "greater", "(", "worst_finished_score", ",", "\n", "best_alive_score", ")", ")", "\n", "\n", "cond", "=", "tf", ".", "logical_and", "(", "tf", ".", "less", "(", "t", ",", "max_step", ")", ",", "\n", "tf", ".", "logical_not", "(", "bound_is_met", ")", ")", "\n", "\n", "return", "cond", "\n", "\n", "", "def", "_loop_fn", "(", "t", ",", "s", ")", ":", "\n", "        ", "outs", "=", "_beam_search_step", "(", "t", ",", "func", ",", "s", ",", "batch_size", ",", "beam_size", ",", "alpha", ",", "\n", "pad_id", ",", "eos_id", ")", "\n", "return", "outs", "\n", "\n", "", "time", "=", "tf", ".", "constant", "(", "0", ",", "name", "=", "\"time\"", ")", "\n", "shape_invariants", "=", "BeamSearchState", "(", "\n", "inputs", "=", "(", "tf", ".", "TensorShape", "(", "[", "None", ",", "None", ",", "None", "]", ")", ",", "\n", "tf", ".", "TensorShape", "(", "[", "None", ",", "None", "]", ")", ",", "\n", "tf", ".", "TensorShape", "(", "[", "None", ",", "None", "]", ")", ")", ",", "\n", "state", "=", "nest", ".", "map_structure", "(", "utils", ".", "infer_shape_invariants", ",", "state", ".", "state", ")", ",", "\n", "finish", "=", "(", "tf", ".", "TensorShape", "(", "[", "None", ",", "None", "]", ")", ",", "\n", "tf", ".", "TensorShape", "(", "[", "None", ",", "None", ",", "None", "]", ")", ",", "\n", "tf", ".", "TensorShape", "(", "[", "None", ",", "None", "]", ")", ")", "\n", ")", "\n", "outputs", "=", "tf", ".", "while_loop", "(", "_is_finished", ",", "_loop_fn", ",", "[", "time", ",", "state", "]", ",", "\n", "shape_invariants", "=", "[", "tf", ".", "TensorShape", "(", "[", "]", ")", ",", "\n", "shape_invariants", "]", ",", "\n", "parallel_iterations", "=", "1", ",", "\n", "back_prop", "=", "False", ")", "\n", "\n", "final_state", "=", "outputs", "[", "1", "]", "\n", "alive_seqs", "=", "final_state", ".", "inputs", "[", "0", "]", "\n", "alive_scores", "=", "final_state", ".", "inputs", "[", "2", "]", "\n", "final_flags", "=", "final_state", ".", "finish", "[", "0", "]", "\n", "final_seqs", "=", "final_state", ".", "finish", "[", "1", "]", "\n", "final_scores", "=", "final_state", ".", "finish", "[", "2", "]", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.inference.create_inference_graph": [[204, 266], ["copy.copy", "tensorflow.expand_dims", "tensorflow.tile", "tensorflow.shape", "tensorflow.reshape", "tensorflow.expand_dims", "tensorflow.tile", "tensorflow.shape", "tensorflow.reshape", "inference._get_inference_fn", "tensorflow.python.util.nest.map_structure", "inference.beam_search", "isinstance", "ValueError", "model.get_inference_func", "callable", "tensorflow.shape", "nest.map_structure.append", "funcs.append", "nest.map_structure.append", "funcs.append", "thumt.tile_to_beam_size"], "function", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.sampling._get_inference_fn", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.inference.beam_search", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.transformer_lrp.TransformerLRP.get_inference_func", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.common.tile_to_beam_size"], ["alive_seqs", ".", "set_shape", "(", "[", "None", ",", "beam_size", ",", "None", "]", ")", "\n", "final_seqs", ".", "set_shape", "(", "[", "None", ",", "beam_size", ",", "None", "]", ")", "\n", "\n", "final_seqs", "=", "tf", ".", "where", "(", "tf", ".", "reduce_any", "(", "final_flags", ",", "1", ")", ",", "final_seqs", ",", "\n", "alive_seqs", ")", "\n", "final_scores", "=", "tf", ".", "where", "(", "tf", ".", "reduce_any", "(", "final_flags", ",", "1", ")", ",", "final_scores", ",", "\n", "alive_scores", ")", "\n", "\n", "return", "final_seqs", ",", "final_scores", "\n", "\n", "\n", "", "def", "create_inference_graph", "(", "models", ",", "features", ",", "params", ")", ":", "\n", "    ", "if", "not", "isinstance", "(", "models", ",", "(", "list", ",", "tuple", ")", ")", ":", "\n", "        ", "raise", "ValueError", "(", "\"'models' must be a list or tuple\"", ")", "\n", "\n", "", "features", "=", "copy", ".", "copy", "(", "features", ")", "\n", "model_fns", "=", "[", "model", ".", "get_inference_func", "(", ")", "for", "model", "in", "models", "]", "\n", "\n", "decode_length", "=", "params", ".", "decode_length", "\n", "beam_size", "=", "params", ".", "beam_size", "\n", "top_beams", "=", "params", ".", "top_beams", "\n", "alpha", "=", "params", ".", "decode_alpha", "\n", "\n", "# Compute initial state if necessary", "\n", "states", "=", "[", "]", "\n", "funcs", "=", "[", "]", "\n", "\n", "for", "model_fn", "in", "model_fns", ":", "\n", "        ", "if", "callable", "(", "model_fn", ")", ":", "\n", "# For non-incremental decoding", "\n", "            ", "states", ".", "append", "(", "{", "}", ")", "\n", "funcs", ".", "append", "(", "model_fn", ")", "\n", "", "else", ":", "\n", "# For incremental decoding where model_fn is a tuple:", "\n", "# (encoding_fn, decoding_fn)", "\n", "            ", "states", ".", "append", "(", "model_fn", "[", "0", "]", "(", "features", ")", ")", "\n", "funcs", ".", "append", "(", "model_fn", "[", "1", "]", ")", "\n", "\n", "", "", "batch_size", "=", "tf", ".", "shape", "(", "features", "[", "\"source\"", "]", ")", "[", "0", "]", "\n", "pad_id", "=", "params", ".", "mapping", "[", "\"target\"", "]", "[", "params", ".", "pad", "]", "\n", "bos_id", "=", "params", ".", "mapping", "[", "\"target\"", "]", "[", "params", ".", "bos", "]", "\n", "eos_id", "=", "params", ".", "mapping", "[", "\"target\"", "]", "[", "params", ".", "eos", "]", "\n", "\n", "# Expand the inputs", "\n", "# [batch, length] => [batch, beam_size, length]", "\n", "features", "[", "\"source\"", "]", "=", "tf", ".", "expand_dims", "(", "features", "[", "\"source\"", "]", ",", "1", ")", "\n", "features", "[", "\"source\"", "]", "=", "tf", ".", "tile", "(", "features", "[", "\"source\"", "]", ",", "[", "1", ",", "beam_size", ",", "1", "]", ")", "\n", "shape", "=", "tf", ".", "shape", "(", "features", "[", "\"source\"", "]", ")", "\n", "\n", "# [batch, beam_size, length] => [batch * beam_size, length]", "\n", "features", "[", "\"source\"", "]", "=", "tf", ".", "reshape", "(", "features", "[", "\"source\"", "]", ",", "\n", "[", "shape", "[", "0", "]", "*", "shape", "[", "1", "]", ",", "shape", "[", "2", "]", "]", ")", "\n", "\n", "# For source sequence length", "\n", "features", "[", "\"source_length\"", "]", "=", "tf", ".", "expand_dims", "(", "features", "[", "\"source_length\"", "]", ",", "1", ")", "\n", "features", "[", "\"source_length\"", "]", "=", "tf", ".", "tile", "(", "features", "[", "\"source_length\"", "]", ",", "\n", "[", "1", ",", "beam_size", "]", ")", "\n", "shape", "=", "tf", ".", "shape", "(", "features", "[", "\"source_length\"", "]", ")", "\n", "\n", "max_length", "=", "features", "[", "\"source_length\"", "]", "+", "decode_length", "\n", "\n", "# [batch, beam_size, length] => [batch * beam_size, length]", "\n", "features", "[", "\"source_length\"", "]", "=", "tf", ".", "reshape", "(", "features", "[", "\"source_length\"", "]", ",", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.bleu.closest_length": [[13, 29], ["len", "len", "abs"], "function", ["None"], ["def", "closest_length", "(", "candidate", ",", "references", ")", ":", "\n", "    ", "clen", "=", "len", "(", "candidate", ")", "\n", "closest_diff", "=", "9999", "\n", "closest_len", "=", "9999", "\n", "\n", "for", "reference", "in", "references", ":", "\n", "        ", "rlen", "=", "len", "(", "reference", ")", "\n", "diff", "=", "abs", "(", "rlen", "-", "clen", ")", "\n", "\n", "if", "diff", "<", "closest_diff", ":", "\n", "            ", "closest_diff", "=", "diff", "\n", "closest_len", "=", "rlen", "\n", "", "elif", "diff", "==", "closest_diff", ":", "\n", "            ", "closest_len", "=", "rlen", "if", "rlen", "<", "closest_len", "else", "closest_len", "\n", "\n", "", "", "return", "closest_len", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.bleu.shortest_length": [[31, 33], ["min", "len"], "function", ["None"], ["", "def", "shortest_length", "(", "references", ")", ":", "\n", "    ", "return", "min", "(", "[", "len", "(", "ref", ")", "for", "ref", "in", "references", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.bleu.modified_precision": [[35, 58], ["collections.Counter", "collections.Counter.items", "len", "collections.Counter", "min", "float", "float", "len", "tuple", "tuple", "max", "sum", "sum", "range", "len", "range", "clipped_counts.values", "collections.Counter.values"], "function", ["None"], ["", "def", "modified_precision", "(", "candidate", ",", "references", ",", "n", ")", ":", "\n", "    ", "tngrams", "=", "len", "(", "candidate", ")", "+", "1", "-", "n", "\n", "counts", "=", "Counter", "(", "[", "tuple", "(", "candidate", "[", "i", ":", "i", "+", "n", "]", ")", "for", "i", "in", "range", "(", "tngrams", ")", "]", ")", "\n", "\n", "if", "len", "(", "counts", ")", "==", "0", ":", "\n", "        ", "return", "0", ",", "0", "\n", "\n", "", "max_counts", "=", "{", "}", "\n", "for", "reference", "in", "references", ":", "\n", "        ", "rngrams", "=", "len", "(", "reference", ")", "+", "1", "-", "n", "\n", "ngrams", "=", "[", "tuple", "(", "reference", "[", "i", ":", "i", "+", "n", "]", ")", "for", "i", "in", "range", "(", "rngrams", ")", "]", "\n", "ref_counts", "=", "Counter", "(", "ngrams", ")", "\n", "for", "ngram", "in", "counts", ":", "\n", "            ", "mcount", "=", "0", "if", "ngram", "not", "in", "max_counts", "else", "max_counts", "[", "ngram", "]", "\n", "rcount", "=", "0", "if", "ngram", "not", "in", "ref_counts", "else", "ref_counts", "[", "ngram", "]", "\n", "max_counts", "[", "ngram", "]", "=", "max", "(", "mcount", ",", "rcount", ")", "\n", "\n", "", "", "clipped_counts", "=", "{", "}", "\n", "\n", "for", "ngram", ",", "count", "in", "counts", ".", "items", "(", ")", ":", "\n", "        ", "clipped_counts", "[", "ngram", "]", "=", "min", "(", "count", ",", "max_counts", "[", "ngram", "]", ")", "\n", "\n", "", "return", "float", "(", "sum", "(", "clipped_counts", ".", "values", "(", ")", ")", ")", ",", "float", "(", "sum", "(", "counts", ".", "values", "(", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.bleu.brevity_penalty": [[60, 76], ["zip", "math.exp", "len", "min", "bleu.shortest_length", "bleu.closest_length"], "function", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.bleu.shortest_length", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.bleu.closest_length"], ["", "def", "brevity_penalty", "(", "trans", ",", "refs", ",", "mode", "=", "\"closest\"", ")", ":", "\n", "    ", "bp_c", "=", "0.0", "\n", "bp_r", "=", "0.0", "\n", "\n", "for", "candidate", ",", "references", "in", "zip", "(", "trans", ",", "refs", ")", ":", "\n", "        ", "bp_c", "+=", "len", "(", "candidate", ")", "\n", "\n", "if", "mode", "==", "\"shortest\"", ":", "\n", "            ", "bp_r", "+=", "shortest_length", "(", "references", ")", "\n", "", "else", ":", "\n", "            ", "bp_r", "+=", "closest_length", "(", "candidate", ",", "references", ")", "\n", "\n", "# Prevent zero divide", "\n", "", "", "bp_c", "=", "bp_c", "or", "1.0", "\n", "\n", "return", "math", ".", "exp", "(", "min", "(", "0", ",", "1.0", "-", "bp_r", "/", "bp_c", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.bleu.bleu": [[78, 113], ["zip", "range", "bleu.brevity_penalty", "range", "sum", "math.exp", "range", "range", "bleu.modified_precision", "range", "math.log", "len", "ValueError", "sum", "float", "float", "float", "range"], "function", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.bleu.brevity_penalty", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.bleu.modified_precision"], ["", "def", "bleu", "(", "trans", ",", "refs", ",", "bp", "=", "\"closest\"", ",", "smooth", "=", "False", ",", "n", "=", "4", ",", "weights", "=", "None", ")", ":", "\n", "    ", "p_norm", "=", "[", "0", "for", "_", "in", "range", "(", "n", ")", "]", "\n", "p_denorm", "=", "[", "0", "for", "_", "in", "range", "(", "n", ")", "]", "\n", "\n", "for", "candidate", ",", "references", "in", "zip", "(", "trans", ",", "refs", ")", ":", "\n", "        ", "for", "i", "in", "range", "(", "n", ")", ":", "\n", "            ", "ccount", ",", "tcount", "=", "modified_precision", "(", "candidate", ",", "references", ",", "i", "+", "1", ")", "\n", "p_norm", "[", "i", "]", "+=", "ccount", "\n", "p_denorm", "[", "i", "]", "+=", "tcount", "\n", "\n", "", "", "bleu_n", "=", "[", "0", "for", "_", "in", "range", "(", "n", ")", "]", "\n", "\n", "for", "i", "in", "range", "(", "n", ")", ":", "\n", "# add one smoothing", "\n", "        ", "if", "smooth", "and", "i", ">", "0", ":", "\n", "            ", "p_norm", "[", "i", "]", "+=", "1", "\n", "p_denorm", "[", "i", "]", "+=", "1", "\n", "\n", "", "if", "p_norm", "[", "i", "]", "==", "0", "or", "p_denorm", "[", "i", "]", "==", "0", ":", "\n", "            ", "bleu_n", "[", "i", "]", "=", "-", "9999", "\n", "", "else", ":", "\n", "            ", "bleu_n", "[", "i", "]", "=", "math", ".", "log", "(", "float", "(", "p_norm", "[", "i", "]", ")", "/", "float", "(", "p_denorm", "[", "i", "]", ")", ")", "\n", "\n", "", "", "if", "weights", ":", "\n", "        ", "if", "len", "(", "weights", ")", "!=", "n", ":", "\n", "            ", "raise", "ValueError", "(", "\"len(weights) != n: invalid weight number\"", ")", "\n", "", "log_precision", "=", "sum", "(", "[", "bleu_n", "[", "i", "]", "*", "weights", "[", "i", "]", "for", "i", "in", "range", "(", "n", ")", "]", ")", "\n", "", "else", ":", "\n", "        ", "log_precision", "=", "sum", "(", "bleu_n", ")", "/", "float", "(", "n", ")", "\n", "\n", "", "bp", "=", "brevity_penalty", "(", "trans", ",", "refs", ",", "bp", ")", "\n", "\n", "score", "=", "bp", "*", "math", ".", "exp", "(", "log_precision", ")", "\n", "\n", "return", "score", "\n", "", ""]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.weight_ratio.create_diagonal": [[17, 28], ["tensorflow.diag", "tensorflow.expand_dims", "tensorflow.tile", "tensorflow.shape", "tensorflow.shape", "tensorflow.ones"], "function", ["None"], ["def", "create_diagonal", "(", "output", ")", ":", "\n", "    ", "'''\n        output: (batchsize, dim)\n        diagonal matrix (batchsize, length, length)\n    '''", "\n", "length", "=", "tf", ".", "shape", "(", "output", ")", "[", "1", "]", "\n", "batchsize", "=", "tf", ".", "shape", "(", "output", ")", "[", "0", "]", "\n", "result", "=", "tf", ".", "diag", "(", "tf", ".", "ones", "(", "[", "length", "]", ")", ")", "\n", "result", "=", "tf", ".", "expand_dims", "(", "result", ",", "0", ")", "\n", "result", "=", "tf", ".", "tile", "(", "result", ",", "[", "batchsize", ",", "1", ",", "1", "]", ")", "\n", "return", "result", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.weight_ratio.weight_ratio_mean": [[30, 45], ["tensorflow.cast", "tensorflow.shape", "tensorflow.reshape", "tensorflow.reshape", "tensorflow.reshape", "weight_ratio.stabilize", "tensorflow.shape"], "function", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.rnnsearch_lrp.stabilize"], ["", "def", "weight_ratio_mean", "(", "input", ",", "output", ",", "stab", "=", "0", ")", ":", "\n", "    ", "'''\n        inputs: (..., dim)\n        output: (..., 1)\n        weight ratios: [(..., dim)]\n    '''", "\n", "dim", "=", "tf", ".", "cast", "(", "tf", ".", "shape", "(", "input", ")", "[", "-", "1", "]", ",", "tf", ".", "float32", ")", "\n", "output_shape", "=", "tf", ".", "shape", "(", "input", ")", "\n", "# Flatten to 2D", "\n", "inputs", "=", "tf", ".", "reshape", "(", "input", ",", "[", "-", "1", ",", "input", ".", "shape", "[", "-", "1", "]", ".", "value", "]", ")", "\n", "output", "=", "tf", ".", "reshape", "(", "output", ",", "[", "-", "1", ",", "output", ".", "shape", "[", "-", "1", "]", ".", "value", "]", ")", "\n", "\n", "w", "=", "inputs", "/", "dim", "/", "stabilize", "(", "output", ",", "stab", ")", "\n", "\n", "return", "tf", ".", "reshape", "(", "w", ",", "output_shape", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.weight_ratio.stabilize": [[47, 54], ["tensorflow.sign", "tensorflow.equal", "tensorflow.cast", "tensorflow.zeros", "tensorflow.shape"], "function", ["None"], ["", "def", "stabilize", "(", "matrix", ",", "stab", ")", ":", "\n", "    ", "sign", "=", "tf", ".", "sign", "(", "matrix", ")", "\n", "zero_pos", "=", "tf", ".", "equal", "(", "sign", ",", "tf", ".", "zeros", "(", "tf", ".", "shape", "(", "sign", ")", ")", ")", "\n", "zero_pos", "=", "tf", ".", "cast", "(", "zero_pos", ",", "tf", ".", "float32", ")", "\n", "sign", "+=", "zero_pos", "\n", "result", "=", "matrix", "+", "stab", "*", "sign", "\n", "return", "result", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.weight_ratio.weight_ratio_linear": [[56, 84], ["range", "tensorflow.reshape", "range", "len", "len", "len", "tensorflow.concat", "output_shape.append", "tensorflow.reshape", "len", "weight_ratios.append", "tensorflow.reshape", "tensorflow.expand_dims", "tensorflow.expand_dims", "tensorflow.expand_dims", "zip", "tensorflow.shape", "weight_ratio.stabilize", "tensorflow.shape"], "function", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.rnnsearch_lrp.stabilize"], ["", "def", "weight_ratio_linear", "(", "inputs", ",", "weights", ",", "output", ",", "bias", "=", "None", ",", "stab", "=", "0", ")", ":", "\n", "    ", "'''\n        inputs: [(..., dim_in_i)]\n        weights: [(dim_in_i, dim_out)]\n        bias: [(dim_out)]\n        output: (..., dim_out)\n        weight ratios: [(..., dim_in_i, dim_out)]\n    '''", "\n", "assert", "len", "(", "inputs", ")", "==", "len", "(", "weights", ")", "\n", "output_shape", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "inputs", ")", ")", ":", "\n", "        ", "os", "=", "tf", ".", "concat", "(", "[", "tf", ".", "shape", "(", "inputs", "[", "i", "]", ")", ",", "tf", ".", "shape", "(", "weights", "[", "i", "]", ")", "[", "-", "1", ":", "]", "]", ",", "-", "1", ")", "\n", "output_shape", ".", "append", "(", "os", ")", "\n", "# Flatten to 2D", "\n", "", "inputs", "=", "[", "tf", ".", "reshape", "(", "inp", ",", "[", "-", "1", ",", "inp", ".", "shape", "[", "-", "1", "]", ".", "value", "]", ")", "for", "inp", "in", "inputs", "]", "\n", "output", "=", "tf", ".", "reshape", "(", "output", ",", "[", "-", "1", ",", "output", ".", "shape", "[", "-", "1", "]", ".", "value", "]", ")", "\n", "\n", "weight_ratios", "=", "[", "]", "\n", "\n", "for", "i", "in", "range", "(", "len", "(", "inputs", ")", ")", ":", "\n", "        ", "r", "=", "tf", ".", "expand_dims", "(", "inputs", "[", "i", "]", ",", "-", "1", ")", "*", "tf", ".", "expand_dims", "(", "weights", "[", "i", "]", ",", "-", "3", ")", "\n", "w", "=", "r", "/", "tf", ".", "expand_dims", "(", "stabilize", "(", "output", ",", "stab", ")", ",", "-", "2", ")", "\n", "weight_ratios", ".", "append", "(", "w", ")", "\n", "\n", "", "weight_ratios", "=", "[", "tf", ".", "reshape", "(", "wr", ",", "os", ")", "\n", "for", "os", ",", "wr", "in", "zip", "(", "output_shape", ",", "weight_ratios", ")", "]", "\n", "\n", "return", "weight_ratios", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.weight_ratio.weight_ratio_weighted_sum": [[86, 112], ["tensorflow.reshape", "weight_ratio.create_diagonal", "range", "len", "len", "tensorflow.shape", "tensorflow.concat", "tensorflow.reshape", "len", "weight_ratios.append", "tensorflow.reshape", "weight_ratio.stabilize", "tensorflow.shape", "tensorflow.shape", "tensorflow.expand_dims", "tensorflow.shape", "tensorflow.shape"], "function", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.weight_ratio.create_diagonal", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.rnnsearch_lrp.stabilize"], ["", "def", "weight_ratio_weighted_sum", "(", "inputs", ",", "weights", ",", "output", ",", "stab", "=", "0", ",", "flatten", "=", "False", ")", ":", "\n", "    ", "'''\n        inputs: [(..., dim)]\n        weights: [scalar]\n        output: (..., dim)\n        weight_ratios: [(..., dim, dim)]\n    '''", "\n", "assert", "len", "(", "inputs", ")", "==", "len", "(", "weights", ")", "\n", "if", "flatten", ":", "\n", "        ", "output_shape", "=", "tf", ".", "shape", "(", "output", ")", "\n", "", "else", ":", "\n", "        ", "output_shape", "=", "tf", ".", "concat", "(", "[", "tf", ".", "shape", "(", "output", ")", ",", "tf", ".", "shape", "(", "output", ")", "[", "-", "1", ":", "]", "]", ",", "-", "1", ")", "\n", "# Flatten to 2D", "\n", "", "inputs", "=", "[", "tf", ".", "reshape", "(", "inp", ",", "[", "-", "1", ",", "tf", ".", "shape", "(", "inp", ")", "[", "-", "1", "]", "]", ")", "for", "inp", "in", "inputs", "]", "\n", "output", "=", "tf", ".", "reshape", "(", "output", ",", "[", "-", "1", ",", "tf", ".", "shape", "(", "output", ")", "[", "-", "1", "]", "]", ")", "\n", "\n", "weight_ratios", "=", "[", "]", "\n", "diag", "=", "create_diagonal", "(", "output", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "inputs", ")", ")", ":", "\n", "        ", "wr", "=", "inputs", "[", "i", "]", "*", "weights", "[", "i", "]", "/", "stabilize", "(", "output", ",", "stab", ")", "\n", "if", "not", "flatten", ":", "\n", "            ", "wr", "=", "tf", ".", "expand_dims", "(", "wr", ",", "-", "1", ")", "*", "diag", "\n", "", "weight_ratios", ".", "append", "(", "wr", ")", "\n", "\n", "", "weight_ratios", "=", "[", "tf", ".", "reshape", "(", "wr", ",", "output_shape", ")", "for", "wr", "in", "weight_ratios", "]", "\n", "return", "weight_ratios", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.weight_ratio.weight_ratio_maxpool": [[114, 154], ["tensorflow.constant", "tensorflow.concat", "tensorflow.reshape", "tensorflow.reshape", "tensorflow.shape", "tensorflow.concat", "tensorflow.reshape", "tensorflow.argmax", "tensorflow.cast", "tensorflow.reshape", "tensorflow.sparse_to_dense", "tensorflow.reshape", "tensorflow.reshape", "tensorflow.expand_dims", "tensorflow.reshape", "tensorflow.sparse_to_dense", "tensorflow.reshape", "tensorflow.shape", "tensorflow.ones", "tensorflow.range", "tensorflow.ones", "tensorflow.shape", "tensorflow.range", "tensorflow.shape", "tensorflow.range", "tensorflow.shape", "tensorflow.shape"], "function", ["None"], ["", "def", "weight_ratio_maxpool", "(", "input", ",", "output", ",", "maxnum", ",", "flatten", "=", "False", ")", ":", "\n", "    ", "'''\n        inputs: (..., dim)\n        output: (..., dim/maxpart)\n        weight_ratios: (..., dim, dim/maxnum)\n    '''", "\n", "# Flatten to 2D", "\n", "maxnum", "=", "tf", ".", "constant", "(", "maxnum", ",", "dtype", "=", "tf", ".", "int32", ")", "\n", "weight_shape", "=", "tf", ".", "concat", "(", "[", "tf", ".", "shape", "(", "input", ")", ",", "tf", ".", "shape", "(", "output", ")", "[", "-", "1", ":", "]", "]", ",", "axis", "=", "-", "1", ")", "\n", "input", "=", "tf", ".", "reshape", "(", "input", ",", "[", "-", "1", ",", "input", ".", "shape", "[", "-", "1", "]", ".", "value", "]", ")", "\n", "output", "=", "tf", ".", "reshape", "(", "output", ",", "[", "-", "1", ",", "output", ".", "shape", "[", "-", "1", "]", ".", "value", "]", ")", "\n", "\n", "shape_inp", "=", "tf", ".", "shape", "(", "input", ")", "\n", "batch", "=", "shape_inp", "[", "0", "]", "\n", "dim_in", "=", "shape_inp", "[", "-", "1", "]", "\n", "shape", "=", "tf", ".", "concat", "(", "[", "shape_inp", "[", ":", "-", "1", "]", ",", "[", "shape_inp", "[", "-", "1", "]", "//", "maxnum", ",", "maxnum", "]", "]", ",", "\n", "axis", "=", "0", ")", "\n", "dim_out", "=", "shape", "[", "-", "2", "]", "\n", "value", "=", "tf", ".", "reshape", "(", "input", ",", "shape", ")", "\n", "\n", "pos", "=", "tf", ".", "argmax", "(", "value", ",", "-", "1", ")", "\n", "pos", "=", "tf", ".", "cast", "(", "pos", ",", "tf", ".", "int32", ")", "\n", "pos", "=", "tf", ".", "reshape", "(", "pos", ",", "[", "-", "1", "]", ")", "\n", "if", "flatten", ":", "\n", "        ", "indices", "=", "tf", ".", "range", "(", "tf", ".", "shape", "(", "pos", ")", "[", "0", "]", ")", "*", "maxnum", "+", "pos", "\n", "weight_ratio", "=", "tf", ".", "sparse_to_dense", "(", "indices", ",", "[", "batch", "*", "dim_in", "]", ",", "\n", "tf", ".", "ones", "(", "tf", ".", "shape", "(", "indices", ")", ")", ")", "\n", "weight_ratio", "=", "tf", ".", "reshape", "(", "weight_ratio", ",", "weight_shape", "[", ":", "-", "1", "]", ")", "\n", "", "else", ":", "\n", "        ", "indices", "=", "dim_out", "*", "pos", "+", "dim_in", "*", "tf", ".", "range", "(", "batch", "*", "dim_out", ",", "\n", "dtype", "=", "tf", ".", "int32", ")", "\n", "indices", "=", "tf", ".", "reshape", "(", "indices", ",", "[", "-", "1", ",", "dim_out", "]", ")", "\n", "indices", "+=", "tf", ".", "expand_dims", "(", "tf", ".", "range", "(", "dim_out", ",", "dtype", "=", "tf", ".", "int32", ")", ",", "0", ")", "\n", "indices", "=", "tf", ".", "reshape", "(", "indices", ",", "[", "-", "1", "]", ")", "\n", "\n", "weight_ratio", "=", "tf", ".", "sparse_to_dense", "(", "indices", ",", "[", "batch", "*", "dim_in", "*", "dim_out", "]", ",", "\n", "tf", ".", "ones", "(", "tf", ".", "shape", "(", "indices", ")", ")", ")", "\n", "weight_ratio", "=", "tf", ".", "reshape", "(", "weight_ratio", ",", "weight_shape", ")", "\n", "\n", "", "return", "weight_ratio", "\n", "", ""]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.half.custom_getter": [[11, 21], ["getter", "tensorflow.cast"], "function", ["None"], ["def", "custom_getter", "(", "getter", ",", "name", ",", "shape", "=", "None", ",", "dtype", "=", "None", ",", "initializer", "=", "None", ",", "\n", "regularizer", "=", "None", ",", "trainable", "=", "True", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "    ", "var_dtype", "=", "tf", ".", "float32", "if", "trainable", "else", "dtype", "\n", "variable", "=", "getter", "(", "name", ",", "shape", ",", "dtype", "=", "var_dtype", ",", "initializer", "=", "initializer", ",", "\n", "regularizer", "=", "regularizer", ",", "trainable", "=", "trainable", ",", "\n", "*", "args", ",", "**", "kwargs", ")", "\n", "if", "trainable", "and", "dtype", "!=", "tf", ".", "float32", ":", "\n", "        ", "variable", "=", "tf", ".", "cast", "(", "variable", ",", "dtype", ")", "\n", "\n", "", "return", "variable", "\n", "", ""]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.lrp.LegacyGRUCell_encoder_v2n.__init__": [[267, 270], ["super().__init__"], "methods", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.transformer_lrp.TransformerLRP.__init__"], ["def", "__init__", "(", "self", ",", "num_units", ",", "reuse", "=", "None", ")", ":", "\n", "        ", "super", "(", "LegacyGRUCell_encoder_v2n", ",", "self", ")", ".", "__init__", "(", "_reuse", "=", "reuse", ")", "\n", "self", ".", "_num_units", "=", "num_units", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.lrp.LegacyGRUCell_encoder_v2n.__call__": [[271, 318], ["tensorflow.variable_scope", "tensorflow.ones", "lrp.linear_v2n", "tensorflow.nn.sigmoid", "lrp.linear_v2n", "tensorflow.nn.sigmoid", "tensorflow.concat", "lrp.linear_v2n", "tensorflow.split", "lrp.stabilize", "isinstance", "tensorflow.shape", "tensorflow.shape", "list", "list", "tensorflow.tanh", "tensorflow.expand_dims", "tensorflow.expand_dims", "tensorflow.expand_dims", "tensorflow.shape"], "methods", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.lrp.linear_v2n", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.lrp.linear_v2n", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.lrp.linear_v2n", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.rnnsearch_lrp.stabilize"], ["", "def", "__call__", "(", "self", ",", "inputs", ",", "state", ",", "w_x_h_last", ",", "params", ",", "scope", "=", "None", ")", ":", "\n", "        ", "with", "tf", ".", "variable_scope", "(", "scope", ",", "default_name", "=", "\"gru_cell\"", ",", "\n", "values", "=", "[", "inputs", ",", "state", "]", ")", ":", "\n", "            ", "if", "not", "isinstance", "(", "inputs", ",", "(", "list", ",", "tuple", ")", ")", ":", "\n", "                ", "inputs", "=", "[", "inputs", "]", "\n", "\n", "", "bs", "=", "tf", ".", "shape", "(", "w_x_h_last", ")", "[", "0", "]", "\n", "emb", "=", "tf", ".", "shape", "(", "inputs", ")", "[", "-", "1", "]", "\n", "w_x_x", "=", "tf", ".", "ones", "(", "[", "bs", ",", "1", ",", "emb", "]", ",", "dtype", "=", "tf", ".", "float32", ")", "\n", "all_inputs", "=", "list", "(", "inputs", ")", "+", "[", "state", "]", "\n", "r_linear", "=", "linear_v2n", "(", "all_inputs", ",", "self", ".", "_num_units", ",", "False", ",", "\n", "[", "w_x_x", ",", "w_x_h_last", "]", ",", "params", ",", "False", ",", "\n", "scope", "=", "\"reset_gate\"", ",", "d2", "=", "True", ")", "\n", "w_x_r", ",", "w_xlast_r", "=", "r_linear", "[", "\"weight_ratios\"", "]", "\n", "r", "=", "tf", ".", "nn", ".", "sigmoid", "(", "r_linear", "[", "\"output\"", "]", ")", "\n", "u_linear", "=", "linear_v2n", "(", "all_inputs", ",", "self", ".", "_num_units", ",", "False", ",", "\n", "[", "w_x_x", ",", "w_x_h_last", "]", ",", "params", ",", "False", ",", "\n", "scope", "=", "\"update_gate\"", ",", "d2", "=", "True", ")", "\n", "w_x_u", ",", "w_xlast_u", "=", "u_linear", "[", "\"weight_ratios\"", "]", "\n", "u", "=", "tf", ".", "nn", ".", "sigmoid", "(", "u_linear", "[", "\"output\"", "]", ")", "\n", "\n", "reseted", "=", "r", "*", "state", "\n", "w_x_reseted", "=", "w_x_r", "\n", "w_xlast_reseted", "=", "w_xlast_r", "\n", "\n", "w_tx_reseted", "=", "tf", ".", "concat", "(", "[", "w_x_reseted", ",", "w_xlast_reseted", "]", ",", "1", ")", "\n", "all_inputs", "=", "list", "(", "inputs", ")", "+", "[", "reseted", "]", "\n", "c_linear", "=", "linear_v2n", "(", "all_inputs", ",", "self", ".", "_num_units", ",", "True", ",", "\n", "[", "w_x_x", ",", "w_tx_reseted", "]", ",", "params", ",", "False", ",", "\n", "scope", "=", "\"candidate\"", ",", "d2", "=", "True", ")", "\n", "w_x_c_direct", ",", "w_tx_reseted_c", "=", "c_linear", "[", "\"weight_ratios\"", "]", "\n", "w_x_reseted_c", ",", "w_xlast_c", "=", "tf", ".", "split", "(", "w_tx_reseted_c", ",", "\n", "[", "1", ",", "tf", ".", "shape", "(", "w_tx_reseted_c", ")", "[", "1", "]", "-", "1", "]", ",", "\n", "axis", "=", "1", ")", "\n", "w_x_c", "=", "w_x_c_direct", "+", "w_x_reseted_c", "\n", "c", "=", "c_linear", "[", "\"output\"", "]", "\n", "\n", "h1", "=", "u", "*", "tf", ".", "tanh", "(", "c", ")", "\n", "h2", "=", "(", "1.0", "-", "u", ")", "*", "state", "\n", "new_state", "=", "h1", "+", "h2", "\n", "new_state_stab", "=", "stabilize", "(", "new_state", ",", "params", ".", "stab", ")", "\n", "w_x_newh", "=", "w_x_c", "*", "tf", ".", "expand_dims", "(", "h1", "/", "new_state_stab", ",", "axis", "=", "1", ")", "\n", "w_xlast_newh", "=", "w_xlast_c", "*", "tf", ".", "expand_dims", "(", "h1", "/", "new_state_stab", ",", "axis", "=", "1", ")", "+", "w_x_h_last", "*", "tf", ".", "expand_dims", "(", "h2", "/", "new_state_stab", ",", "axis", "=", "1", ")", "\n", "\n", "", "return", "new_state", ",", "new_state", ",", "w_xlast_newh", ",", "w_x_newh", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.lrp.LegacyGRUCell_encoder_v2n.state_size": [[319, 322], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "state_size", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "_num_units", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.lrp.LegacyGRUCell_encoder_v2n.output_size": [[323, 326], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "output_size", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "_num_units", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.lrp.LegacyGRUCell_decoder_v2n.__init__": [[336, 339], ["super().__init__"], "methods", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.transformer_lrp.TransformerLRP.__init__"], ["def", "__init__", "(", "self", ",", "num_units", ",", "reuse", "=", "None", ")", ":", "\n", "        ", "super", "(", "LegacyGRUCell_decoder_v2n", ",", "self", ")", ".", "__init__", "(", "_reuse", "=", "reuse", ")", "\n", "self", ".", "_num_units", "=", "num_units", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.lrp.LegacyGRUCell_decoder_v2n.__call__": [[340, 390], ["tensorflow.variable_scope", "tensorflow.zeros", "lrp.linear_v2n", "tensorflow.nn.sigmoid", "lrp.linear_v2n", "tensorflow.nn.sigmoid", "lrp.linear_v2n", "lrp.weighted_sum", "tensorflow.expand_dims", "tensorflow.expand_dims", "isinstance", "tensorflow.shape", "tensorflow.shape", "list", "list", "tensorflow.tanh"], "methods", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.lrp.linear_v2n", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.lrp.linear_v2n", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.lrp.linear_v2n", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.lrp.weighted_sum"], ["", "def", "__call__", "(", "self", ",", "inputs", ",", "state", ",", "w_x_h_last", ",", "w_x_c", ",", "params", ",", "scope", "=", "None", ")", ":", "\n", "        ", "with", "tf", ".", "variable_scope", "(", "scope", ",", "default_name", "=", "\"gru_cell\"", ",", "\n", "values", "=", "[", "inputs", ",", "state", "]", ")", ":", "\n", "            ", "if", "not", "isinstance", "(", "inputs", ",", "(", "list", ",", "tuple", ")", ")", ":", "\n", "                ", "inputs", "=", "[", "inputs", "]", "\n", "\n", "", "bs", "=", "tf", ".", "shape", "(", "w_x_h_last", ")", "[", "0", "]", "\n", "emb", "=", "tf", ".", "shape", "(", "inputs", "[", "0", "]", ")", "[", "-", "1", "]", "\n", "w_x_y", "=", "tf", ".", "zeros", "(", "[", "bs", ",", "1", ",", "emb", "]", ",", "dtype", "=", "tf", ".", "float32", ")", "\n", "all_inputs", "=", "list", "(", "inputs", ")", "+", "[", "state", "]", "\n", "w_x_h", "=", "w_x_h_last", "\n", "w_x_ctx", "=", "w_x_c", "\n", "r_linear", "=", "linear_v2n", "(", "all_inputs", ",", "self", ".", "_num_units", ",", "False", ",", "\n", "[", "w_x_y", ",", "w_x_c", ",", "w_x_h_last", "]", ",", "params", ",", "False", ",", "\n", "scope", "=", "\"reset_gate\"", ",", "d2", "=", "True", ")", "\n", "_", ",", "w_x_ctx_r", ",", "w_x_h_r", "=", "r_linear", "[", "\"weight_ratios\"", "]", "\n", "w_x_r", "=", "w_x_ctx_r", "+", "w_x_h_r", "\n", "r", "=", "tf", ".", "nn", ".", "sigmoid", "(", "r_linear", "[", "\"output\"", "]", ")", "\n", "\n", "u_linear", "=", "linear_v2n", "(", "all_inputs", ",", "self", ".", "_num_units", ",", "False", ",", "\n", "[", "w_x_y", ",", "w_x_c", ",", "w_x_h_last", "]", ",", "params", ",", "False", ",", "\n", "scope", "=", "\"update_gate\"", ",", "d2", "=", "True", ")", "\n", "_", ",", "w_x_ctx_u", ",", "w_x_h_u", "=", "u_linear", "[", "\"weight_ratios\"", "]", "\n", "w_x_u", "=", "w_x_ctx_u", "+", "w_x_h_u", "\n", "u", "=", "tf", ".", "nn", ".", "sigmoid", "(", "u_linear", "[", "\"output\"", "]", ")", "\n", "\n", "reseted", "=", "r", "*", "state", "\n", "w_x_reseted", "=", "0.5", "*", "w_x_r", "+", "0.5", "*", "w_x_h", "\n", "\n", "all_inputs", "=", "list", "(", "inputs", ")", "+", "[", "reseted", "]", "\n", "c_linear", "=", "linear_v2n", "(", "all_inputs", ",", "self", ".", "_num_units", ",", "True", ",", "\n", "[", "w_x_y", ",", "w_x_c", ",", "w_x_reseted", "]", ",", "params", ",", "False", ",", "\n", "scope", "=", "\"candidate\"", ",", "d2", "=", "True", ")", "\n", "_", ",", "w_x_c_state", ",", "w_x_resetes_state", "=", "c_linear", "[", "\"weight_ratios\"", "]", "\n", "w_x_state", "=", "w_x_c_state", "+", "w_x_resetes_state", "\n", "c", "=", "c_linear", "[", "\"output\"", "]", "\n", "\n", "h1", "=", "u", "*", "tf", ".", "tanh", "(", "c", ")", "\n", "h2", "=", "(", "1.0", "-", "u", ")", "*", "state", "\n", "w_x_h1", "=", "0.5", "*", "w_x_state", "+", "0.5", "*", "w_x_u", "\n", "w_x_h2", "=", "0.5", "*", "w_x_u", "+", "0.5", "*", "w_x_h", "\n", "\n", "newh_ws", "=", "weighted_sum", "(", "[", "h1", ",", "h2", "]", ",", "[", "1.", ",", "1.", "]", ",", "params", ",", "flatten", "=", "True", ")", "\n", "new_state", "=", "newh_ws", "[", "\"output\"", "]", "\n", "w_h1_newh", ",", "w_h2_newh", "=", "newh_ws", "[", "\"weight_ratios\"", "]", "\n", "w_h1_newh", "=", "tf", ".", "expand_dims", "(", "w_h1_newh", ",", "1", ")", "\n", "w_h2_newh", "=", "tf", ".", "expand_dims", "(", "w_h2_newh", ",", "1", ")", "\n", "w_x_newh", "=", "w_x_h1", "*", "w_h1_newh", "+", "w_x_h2", "*", "w_h2_newh", "\n", "\n", "", "return", "new_state", ",", "new_state", ",", "w_x_newh", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.lrp.LegacyGRUCell_decoder_v2n.state_size": [[391, 394], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "state_size", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "_num_units", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.lrp.LegacyGRUCell_decoder_v2n.output_size": [[395, 398], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "output_size", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "_num_units", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.lrp.reduce_reserve": [[23, 25], ["tensorflow.expand_dims", "tensorflow.reduce_sum"], "function", ["None"], ["def", "reduce_reserve", "(", "matrix", ",", "axis", ")", ":", "\n", "    ", "return", "tf", ".", "expand_dims", "(", "tf", ".", "reduce_sum", "(", "matrix", ",", "axis", ")", ",", "axis", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.lrp.v2n_propagate_linear": [[27, 40], ["tensorflow.expand_dims", "tensorflow.expand_dims", "tensorflow.tile", "tensorflow.matmul", "tensorflow.squeeze", "tensorflow.shape"], "function", ["None"], ["", "def", "v2n_propagate_linear", "(", "w_x_last", ",", "w_linear", ")", ":", "\n", "    ", "'''\n        w_x_last: [bs, len_src, len, d]\n        w_linear: [b, len, d, d]\n        return: [b, len_src, len, d]\n    '''", "\n", "len_src", "=", "tf", ".", "shape", "(", "w_x_last", ")", "[", "1", "]", "\n", "w_x_last", "=", "tf", ".", "expand_dims", "(", "w_x_last", ",", "-", "2", ")", "\n", "w_linear", "=", "tf", ".", "expand_dims", "(", "w_linear", ",", "1", ")", "\n", "w_linear", "=", "tf", ".", "tile", "(", "w_linear", ",", "[", "1", ",", "len_src", ",", "1", ",", "1", ",", "1", "]", ")", "\n", "result", "=", "tf", ".", "matmul", "(", "w_x_last", ",", "w_linear", ")", "\n", "result", "=", "tf", ".", "squeeze", "(", "result", ",", "-", "2", ")", "\n", "return", "result", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.lrp.create_diagonal_v2n": [[42, 52], ["tensorflow.diag", "tensorflow.expand_dims", "tensorflow.expand_dims", "tensorflow.tile", "tensorflow.ones"], "function", ["None"], ["", "def", "create_diagonal_v2n", "(", "batchsize", ",", "length", ",", "dim", ")", ":", "\n", "    ", "'''\n        diagonal matrix (batchsize, len_src, len_src, dim)\n\tresult[bs, len_src, len_src, dim] = 1\n    '''", "\n", "result", "=", "tf", ".", "diag", "(", "tf", ".", "ones", "(", "[", "length", "]", ",", "dtype", "=", "tf", ".", "float32", ")", ")", "\n", "result", "=", "tf", ".", "expand_dims", "(", "result", ",", "0", ")", "\n", "result", "=", "tf", ".", "expand_dims", "(", "result", ",", "-", "1", ")", "\n", "result", "=", "tf", ".", "tile", "(", "result", ",", "[", "batchsize", ",", "1", ",", "1", ",", "dim", "]", ")", "\n", "return", "result", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.lrp.stabilize": [[54, 61], ["tensorflow.sign", "tensorflow.equal", "tensorflow.cast", "tensorflow.zeros", "tensorflow.shape"], "function", ["None"], ["", "def", "stabilize", "(", "matrix", ",", "stab", ")", ":", "\n", "    ", "sign", "=", "tf", ".", "sign", "(", "matrix", ")", "\n", "zero_pos", "=", "tf", ".", "equal", "(", "sign", ",", "tf", ".", "zeros", "(", "tf", ".", "shape", "(", "sign", ")", ")", ")", "\n", "zero_pos", "=", "tf", ".", "cast", "(", "zero_pos", ",", "tf", ".", "float32", ")", "\n", "sign", "+=", "zero_pos", "\n", "result", "=", "matrix", "+", "stab", "*", "sign", "\n", "return", "result", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.lrp.normalize": [[63, 66], ["tensorflow.reduce_sum", "tensorflow.abs", "tensorflow.expand_dims"], "function", ["None"], ["", "def", "normalize", "(", "matrix", ")", ":", "\n", "    ", "total", "=", "tf", ".", "reduce_sum", "(", "tf", ".", "abs", "(", "matrix", ")", ",", "axis", "=", "-", "1", ")", "\n", "return", "matrix", "/", "tf", ".", "expand_dims", "(", "total", ",", "axis", "=", "-", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.lrp.dot_product": [[68, 76], ["tensorflow.identity", "range", "thumt.weight_ratio_dot_product", "len"], "function", ["None"], ["", "def", "dot_product", "(", "inputs", ",", "params", ")", ":", "\n", "    ", "output", "=", "tf", ".", "identity", "(", "inputs", "[", "0", "]", ")", "\n", "for", "i", "in", "range", "(", "1", ",", "len", "(", "inputs", ")", ")", ":", "\n", "        ", "output", "*=", "inputs", "[", "i", "]", "\n", "\n", "", "weight_ratios", "=", "wr", ".", "weight_ratio_dot_product", "(", "inputs", ",", "output", ")", "\n", "\n", "return", "{", "\"output\"", ":", "output", ",", "\"weight_ratios\"", ":", "weight_ratios", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.lrp.weighted_sum": [[78, 87], ["tensorflow.add_n", "thumt.weight_ratio_weighted_sum", "len", "len", "range", "len"], "function", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.weight_ratio.weight_ratio_weighted_sum"], ["", "def", "weighted_sum", "(", "inputs", ",", "weights", ",", "params", ",", "flatten", "=", "False", ")", ":", "\n", "    ", "assert", "len", "(", "inputs", ")", "==", "len", "(", "weights", ")", "\n", "output", "=", "tf", ".", "add_n", "(", "[", "inputs", "[", "i", "]", "*", "weights", "[", "i", "]", "for", "i", "in", "range", "(", "len", "(", "inputs", ")", ")", "]", ")", "\n", "\n", "weight_ratios", "=", "wr", ".", "weight_ratio_weighted_sum", "(", "inputs", ",", "weights", ",", "output", ",", "\n", "stab", "=", "params", ".", "stab", ",", "\n", "flatten", "=", "flatten", ")", "\n", "\n", "return", "{", "\"output\"", ":", "output", ",", "\"weight_ratios\"", ":", "weight_ratios", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.lrp.maxpool": [[89, 97], ["tensorflow.concat", "tensorflow.reshape", "tensorflow.reduce_max", "thumt.weight_ratio_maxpool", "tensorflow.shape"], "function", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.weight_ratio.weight_ratio_maxpool"], ["", "def", "maxpool", "(", "input", ",", "output_size", ",", "params", ",", "flatten", "=", "False", ")", ":", "\n", "    ", "shape", "=", "tf", ".", "concat", "(", "[", "tf", ".", "shape", "(", "input", ")", "[", ":", "-", "1", "]", ",", "[", "output_size", ",", "params", ".", "maxnum", "]", "]", ",", "\n", "axis", "=", "0", ")", "\n", "value", "=", "tf", ".", "reshape", "(", "input", ",", "shape", ")", "\n", "output", "=", "tf", ".", "reduce_max", "(", "value", ",", "-", "1", ")", "\n", "weight_ratio", "=", "wr", ".", "weight_ratio_maxpool", "(", "input", ",", "output", ",", "params", ".", "maxnum", ",", "\n", "flatten", "=", "flatten", ")", "\n", "return", "{", "\"output\"", ":", "output", ",", "\"weight_ratio\"", ":", "weight_ratio", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.lrp.weight_ratio_linear_v2n_2d": [[99, 108], ["tensorflow.expand_dims", "lrp.weight_ratio_linear_v2n", "tensorflow.expand_dims", "tensorflow.expand_dims", "tensorflow.squeeze"], "function", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.lrp.weight_ratio_linear_v2n"], ["", "def", "weight_ratio_linear_v2n_2d", "(", "inputs", ",", "weights", ",", "output", ",", "w_x_inp", ",", "bias", "=", "None", ",", "\n", "stab", "=", "0", ")", ":", "\n", "    ", "inputs_ex", "=", "[", "tf", ".", "expand_dims", "(", "inp", ",", "1", ")", "for", "inp", "in", "inputs", "]", "\n", "output_ex", "=", "tf", ".", "expand_dims", "(", "output", ",", "1", ")", "\n", "w_x_inp_ex", "=", "[", "tf", ".", "expand_dims", "(", "w", ",", "2", ")", "for", "w", "in", "w_x_inp", "]", "\n", "result", "=", "weight_ratio_linear_v2n", "(", "inputs_ex", ",", "weights", ",", "output_ex", ",", "\n", "w_x_inp_ex", ",", "bias", "=", "bias", ",", "stab", "=", "stab", ")", "\n", "result", "=", "[", "tf", ".", "squeeze", "(", "res", ",", "2", ")", "for", "res", "in", "result", "]", "\n", "return", "result", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.lrp.weight_ratio_linear_v2n": [[110, 139], ["tensorflow.expand_dims", "tensorflow.reshape", "range", "len", "len", "tensorflow.shape", "tensorflow.shape", "lrp.stabilize", "len", "tensorflow.reshape", "tensorflow.reshape", "tensorflow.matmul", "tensorflow.reshape", "weight_ratios.append", "tensorflow.shape", "tensorflow.shape"], "function", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.rnnsearch_lrp.stabilize"], ["", "def", "weight_ratio_linear_v2n", "(", "inputs", ",", "weights", ",", "output", ",", "w_x_inp", ",", "bias", "=", "None", ",", "\n", "stab", "=", "0", ")", ":", "\n", "    ", "'''\n        inputs: [(bs, lq, di)]\n        weights: [(di, do)]\n        bias: (do)\n        output: (bs, lq, do)\n        w_x_inp: [(bs, ls, lq, di)]\n        weight ratios: [(bs, ls, lq, do)]\n    '''", "\n", "assert", "len", "(", "inputs", ")", "==", "len", "(", "weights", ")", "\n", "weight_ratios", "=", "[", "]", "\n", "bs", "=", "tf", ".", "shape", "(", "w_x_inp", "[", "0", "]", ")", "[", "0", "]", "\n", "lq", "=", "tf", ".", "shape", "(", "w_x_inp", "[", "0", "]", ")", "[", "2", "]", "\n", "outp", "=", "tf", ".", "expand_dims", "(", "stabilize", "(", "output", ",", "stab", ")", ",", "1", ")", "\n", "outp", "=", "tf", ".", "reshape", "(", "outp", ",", "[", "bs", ",", "1", ",", "lq", ",", "-", "1", "]", ")", "\n", "\n", "for", "i", "in", "range", "(", "len", "(", "inputs", ")", ")", ":", "\n", "        ", "di", "=", "tf", ".", "shape", "(", "w_x_inp", "[", "i", "]", ")", "[", "3", "]", "\n", "ls", "=", "tf", ".", "shape", "(", "w_x_inp", "[", "i", "]", ")", "[", "1", "]", "\n", "inp", "=", "tf", ".", "reshape", "(", "inputs", "[", "i", "]", ",", "[", "bs", ",", "1", ",", "lq", ",", "-", "1", "]", ")", "\n", "w", "=", "w_x_inp", "[", "i", "]", "*", "inp", "\n", "w", "=", "tf", ".", "reshape", "(", "w", ",", "[", "-", "1", ",", "di", "]", ")", "\n", "w", "=", "tf", ".", "matmul", "(", "w", ",", "weights", "[", "i", "]", ")", "\n", "w", "=", "tf", ".", "reshape", "(", "w", ",", "[", "bs", ",", "ls", ",", "lq", ",", "-", "1", "]", ")", "\n", "w", "=", "w", "/", "outp", "\n", "weight_ratios", ".", "append", "(", "w", ")", "\n", "\n", "", "return", "weight_ratios", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.lrp.linear_v2n": [[141, 214], ["tensorflow.variable_scope", "tensorflow.concat", "tensorflow.add_n", "tensorflow.reshape", "isinstance", "tensorflow.shape", "len", "len", "RuntimeError", "tensorflow.reshape", "sum", "tensorflow.concat", "tensorflow.concat", "tensorflow.get_variable", "results.append", "range", "tensorflow.get_variable", "tensorflow.nn.bias_add", "operator", "operator", "tensorflow.matmul", "len", "weight_shapes.append", "tensorflow.get_variable", "matrixs.append", "results.append", "item.get_shape", "tensorflow.shape", "tensorflow.concat", "tensorflow.matmul"], "function", ["None"], ["", "def", "linear_v2n", "(", "inputs", ",", "output_size", ",", "bias", ",", "w_x_inp", ",", "params", ",", "concat", "=", "False", ",", "\n", "dtype", "=", "None", ",", "scope", "=", "None", ",", "d2", "=", "False", ")", ":", "\n", "    ", "\"\"\"\n    Linear layer\n    :param inputs: A Tensor or a list of Tensors with shape [batch, input_size]\n    :param output_size: An integer specify the output size\n    :param bias: a boolean value indicate whether to use bias term\n    :param concat: a boolean value indicate whether to concatenate all inputs\n    :param dtype: an instance of tf.DType, the default value is ``tf.float32''\n    :param scope: the scope of this layer, the default value is ``linear''\n    :returns: a Tensor with shape [batch, output_size]\n    :raises RuntimeError: raises ``RuntimeError'' when input sizes do not\n                          compatible with each other\n    \"\"\"", "\n", "\n", "with", "tf", ".", "variable_scope", "(", "scope", ",", "default_name", "=", "\"linear\"", ",", "values", "=", "[", "inputs", "]", ")", ":", "\n", "#assert not concat", "\n", "        ", "if", "not", "isinstance", "(", "inputs", ",", "(", "list", ",", "tuple", ")", ")", ":", "\n", "            ", "inputs", "=", "[", "inputs", "]", "\n", "\n", "", "batch_shape", "=", "tf", ".", "shape", "(", "inputs", "[", "0", "]", ")", "[", ":", "-", "1", "]", "\n", "input_size", "=", "[", "item", ".", "get_shape", "(", ")", "[", "-", "1", "]", ".", "value", "for", "item", "in", "inputs", "]", "\n", "\n", "if", "len", "(", "inputs", ")", "!=", "len", "(", "input_size", ")", ":", "\n", "            ", "raise", "RuntimeError", "(", "\"inputs and input_size unmatched!\"", ")", "\n", "\n", "", "output_shape", "=", "tf", ".", "concat", "(", "[", "tf", ".", "shape", "(", "inputs", "[", "0", "]", ")", "[", ":", "-", "1", "]", ",", "[", "output_size", "]", "]", ",", "\n", "axis", "=", "0", ")", "\n", "# Flatten to 2D", "\n", "inputs", "=", "[", "tf", ".", "reshape", "(", "inp", ",", "[", "-", "1", ",", "inp", ".", "shape", "[", "-", "1", "]", ".", "value", "]", ")", "for", "inp", "in", "inputs", "]", "\n", "\n", "results", "=", "[", "]", "\n", "weight_ratios", "=", "[", "]", "\n", "weight_shapes", "=", "[", "]", "\n", "matrixs", "=", "[", "]", "\n", "\n", "if", "concat", ":", "\n", "            ", "input_size", "=", "sum", "(", "input_size", ")", "\n", "inputs", "=", "tf", ".", "concat", "(", "inputs", ",", "1", ")", "\n", "shape", "=", "[", "input_size", ",", "output_size", "]", "\n", "weight_shape", "=", "tf", ".", "concat", "(", "[", "batch_shape", ",", "shape", "]", ",", "-", "1", ")", "\n", "matrix", "=", "tf", ".", "get_variable", "(", "\"matrix\"", ",", "shape", ",", "dtype", "=", "dtype", ")", "\n", "results", ".", "append", "(", "tf", ".", "matmul", "(", "inputs", ",", "matrix", ")", ")", "\n", "", "else", ":", "\n", "            ", "for", "i", "in", "range", "(", "len", "(", "input_size", ")", ")", ":", "\n", "                ", "shape", "=", "[", "input_size", "[", "i", "]", ",", "output_size", "]", "\n", "weight_shapes", ".", "append", "(", "tf", ".", "concat", "(", "[", "batch_shape", ",", "shape", "]", ",", "-", "1", ")", ")", "\n", "name", "=", "\"matrix_%d\"", "%", "i", "\n", "matrix", "=", "tf", ".", "get_variable", "(", "name", ",", "shape", ",", "dtype", "=", "dtype", ")", "\n", "matrixs", ".", "append", "(", "matrix", ")", "\n", "results", ".", "append", "(", "tf", ".", "matmul", "(", "inputs", "[", "i", "]", ",", "matrix", ")", ")", "\n", "\n", "", "", "output", "=", "tf", ".", "add_n", "(", "results", ")", "\n", "\n", "if", "bias", ":", "\n", "            ", "shape", "=", "[", "output_size", "]", "\n", "bias", "=", "tf", ".", "get_variable", "(", "\"bias\"", ",", "shape", ",", "dtype", "=", "dtype", ")", "\n", "output", "=", "tf", ".", "nn", ".", "bias_add", "(", "output", ",", "bias", ")", "\n", "\n", "# calculate weight ratio", "\n", "", "operator", "=", "weight_ratio_linear_v2n", "\n", "if", "d2", ":", "\n", "            ", "operator", "=", "weight_ratio_linear_v2n_2d", "\n", "", "if", "concat", ":", "\n", "            ", "weight_ratios", "=", "operator", "(", "[", "inputs", "]", ",", "[", "matrix", "]", ",", "output", ",", "w_x_inp", ",", "\n", "bias", "=", "bias", ",", "stab", "=", "params", ".", "stab", ")", "\n", "", "else", ":", "\n", "            ", "weight_ratios", "=", "operator", "(", "inputs", ",", "matrixs", ",", "output", ",", "w_x_inp", ",", "\n", "bias", "=", "bias", ",", "stab", "=", "params", ".", "stab", ")", "\n", "\n", "", "output", "=", "tf", ".", "reshape", "(", "output", ",", "output_shape", ")", "\n", "\n", "return", "{", "\"output\"", ":", "output", ",", "\"weight_ratios\"", ":", "weight_ratios", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.lrp.maxout_v2n": [[216, 257], ["tensorflow.transpose", "tensorflow.transpose", "tensorflow.zeros", "lrp.linear_v2n", "tensorflow.transpose", "lrp.maxpool", "propagater", "tensorflow.shape"], "function", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.lrp.linear_v2n", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.lrp.maxpool"], ["", "", "def", "maxout_v2n", "(", "inputs", ",", "output_size", ",", "maxpart", ",", "w", ",", "params", ",", "use_bias", "=", "True", ",", "\n", "concat", "=", "True", ",", "dtype", "=", "None", ",", "scope", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    Maxout layer\n    :param inputs: see the corresponding description of ``linear''\n    :param output_size: see the corresponding description of ``linear''\n    :param maxpart: an integer, the default value is 2\n    :param use_bias: a boolean value indicate whether to use bias term\n    :param concat: concat all tensors if inputs is a list of tensors\n    :param dtype: an optional instance of tf.Dtype\n    :param scope: the scope of this layer, the default value is ``maxout''\n    :returns: a Tensor with shape [batch, output_size]\n    :raises RuntimeError: see the corresponding description of ``linear''\n    \"\"\"", "\n", "\n", "w_x_dec", ",", "w_x_ctx", "=", "w", "\n", "w_x_dec", "=", "tf", ".", "transpose", "(", "w_x_dec", ",", "[", "1", ",", "2", ",", "0", ",", "3", "]", ")", "\n", "w_x_ctx", "=", "tf", ".", "transpose", "(", "w_x_ctx", ",", "[", "1", ",", "2", ",", "0", ",", "3", "]", ")", "\n", "w_x_y", "=", "tf", ".", "zeros", "(", "tf", ".", "shape", "(", "w_x_dec", ")", ",", "dtype", "=", "tf", ".", "float32", ")", "\n", "candidate_linear", "=", "linear_v2n", "(", "inputs", ",", "output_size", "*", "maxpart", ",", "use_bias", ",", "\n", "[", "w_x_y", ",", "w_x_dec", ",", "w_x_ctx", "]", ",", "params", ",", "concat", ",", "\n", "dtype", "=", "dtype", ",", "scope", "=", "scope", "or", "\"maxout\"", ")", "\n", "candidate", "=", "candidate_linear", "[", "\"output\"", "]", "\n", "_", ",", "w_x_dec_readout", ",", "w_x_ctx_readout", "=", "candidate_linear", "[", "\"weight_ratios\"", "]", "\n", "w_x_readout", "=", "w_x_dec_readout", "+", "w_x_ctx_readout", "\n", "w_x_readout", "=", "tf", ".", "transpose", "(", "w_x_readout", ",", "[", "0", ",", "2", ",", "1", ",", "3", "]", ")", "\n", "\n", "output_maxout", "=", "maxpool", "(", "candidate", ",", "output_size", ",", "params", ")", "\n", "output", "=", "output_maxout", "[", "\"output\"", "]", "\n", "\n", "# direct", "\n", "w_readout_maxout", "=", "output_maxout", "[", "\"weight_ratio\"", "]", "\n", "\n", "#propagate", "\n", "propagater", "=", "tf", ".", "matmul", "\n", "\n", "w_x_maxout", "=", "propagater", "(", "w_x_readout", ",", "w_readout_maxout", ")", "\n", "\n", "weight_ratios", "=", "[", "w_x_maxout", "]", "\n", "\n", "return", "{", "\"output\"", ":", "output", ",", "\"weight_ratios\"", ":", "weight_ratios", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.lrp.combine_heads_v2n": [[400, 411], ["tensorflow.name_scope", "tensorflow.transpose", "tensorflow.reshape", "tf.reshape.set_shape", "tf.reshape.get_shape", "tensorflow.concat", "tensorflow.shape"], "function", ["None"], ["", "", "def", "combine_heads_v2n", "(", "inputs", ",", "name", "=", "None", ")", ":", "\n", "    ", "with", "tf", ".", "name_scope", "(", "name", ",", "default_name", "=", "\"combine_heads\"", ",", "values", "=", "[", "inputs", "]", ")", ":", "\n", "        ", "x", "=", "inputs", "\n", "x", "=", "tf", ".", "transpose", "(", "x", ",", "[", "0", ",", "2", ",", "3", ",", "1", ",", "4", "]", ")", "\n", "old_shape", "=", "x", ".", "get_shape", "(", ")", ".", "dims", "\n", "a", ",", "b", "=", "old_shape", "[", "-", "2", ":", "]", "\n", "new_shape", "=", "old_shape", "[", ":", "-", "2", "]", "+", "[", "a", "*", "b", "if", "a", "and", "b", "else", "None", "]", "\n", "x", "=", "tf", ".", "reshape", "(", "x", ",", "tf", ".", "concat", "(", "[", "tf", ".", "shape", "(", "x", ")", "[", ":", "-", "2", "]", ",", "[", "-", "1", "]", "]", ",", "0", ")", ")", "\n", "x", ".", "set_shape", "(", "new_shape", ")", "\n", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.lrp.split_heads": [[413, 424], ["tensorflow.name_scope", "tensorflow.reshape", "tf.reshape.set_shape", "tensorflow.transpose", "x.get_shape", "tensorflow.concat", "tensorflow.shape"], "function", ["None"], ["", "", "def", "split_heads", "(", "inputs", ",", "num_heads", ",", "name", "=", "None", ")", ":", "\n", "    ", "with", "tf", ".", "name_scope", "(", "name", ",", "default_name", "=", "\"split_heads\"", ",", "values", "=", "[", "inputs", "]", ")", ":", "\n", "        ", "x", "=", "inputs", "\n", "n", "=", "num_heads", "\n", "old_shape", "=", "x", ".", "get_shape", "(", ")", ".", "dims", "\n", "\n", "last", "=", "old_shape", "[", "-", "1", "]", "\n", "new_shape", "=", "old_shape", "[", ":", "-", "1", "]", "+", "[", "n", "]", "+", "[", "last", "//", "n", "if", "last", "else", "None", "]", "\n", "ret", "=", "tf", ".", "reshape", "(", "x", ",", "tf", ".", "concat", "(", "[", "tf", ".", "shape", "(", "x", ")", "[", ":", "-", "1", "]", ",", "[", "n", ",", "-", "1", "]", "]", ",", "0", ")", ")", "\n", "ret", ".", "set_shape", "(", "new_shape", ")", "\n", "return", "tf", ".", "transpose", "(", "ret", ",", "[", "0", ",", "3", ",", "1", ",", "2", ",", "4", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.lrp.combine_heads": [[425, 436], ["tensorflow.name_scope", "tensorflow.transpose", "tensorflow.reshape", "tf.reshape.set_shape", "tf.reshape.get_shape", "tensorflow.concat", "tensorflow.shape"], "function", ["None"], ["", "", "def", "combine_heads", "(", "inputs", ",", "name", "=", "None", ")", ":", "\n", "    ", "with", "tf", ".", "name_scope", "(", "name", ",", "default_name", "=", "\"combine_heads\"", ",", "values", "=", "[", "inputs", "]", ")", ":", "\n", "        ", "x", "=", "inputs", "\n", "x", "=", "tf", ".", "transpose", "(", "x", ",", "[", "0", ",", "2", ",", "3", ",", "4", ",", "1", ",", "5", "]", ")", "\n", "old_shape", "=", "x", ".", "get_shape", "(", ")", ".", "dims", "\n", "a", ",", "b", "=", "old_shape", "[", "-", "2", ":", "]", "\n", "new_shape", "=", "old_shape", "[", ":", "-", "2", "]", "+", "[", "a", "*", "b", "if", "a", "and", "b", "else", "None", "]", "\n", "x", "=", "tf", ".", "reshape", "(", "x", ",", "tf", ".", "concat", "(", "[", "tf", ".", "shape", "(", "x", ")", "[", ":", "-", "2", "]", ",", "[", "-", "1", "]", "]", ",", "0", ")", ")", "\n", "x", ".", "set_shape", "(", "new_shape", ")", "\n", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.lrp.layer_process": [[438, 446], ["lrp.layer_norm", "ValueError"], "function", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.layers.nn.layer_norm"], ["", "", "def", "layer_process", "(", "x", ",", "mode", ",", "w_x_inp", ",", "params", ")", ":", "\n", "    ", "if", "not", "mode", "or", "mode", "==", "\"none\"", ":", "\n", "        ", "return", "{", "\"outputs\"", ":", "x", ",", "\"weight_ratios\"", ":", "w_x_inp", "}", "\n", "", "elif", "mode", "==", "\"layer_norm\"", ":", "\n", "        ", "norm", "=", "layer_norm", "(", "x", ",", "w_x_inp", ",", "params", ")", "\n", "return", "norm", "\n", "", "else", ":", "\n", "        ", "raise", "ValueError", "(", "\"Unknown mode %s\"", "%", "mode", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.lrp.layer_norm": [[448, 490], ["tensorflow.variable_scope", "tensorflow.get_variable", "tensorflow.get_variable", "tensorflow.reduce_mean", "tensorflow.reduce_mean", "thumt.weight_ratio_mean", "thumt.weight_ratio_weighted_sum", "tensorflow.reduce_sum", "tensorflow.expand_dims", "tensorflow.expand_dims", "inputs.get_shape().as_list", "tensorflow.square", "tensorflow.rsqrt", "tensorflow.expand_dims", "tensorflow.ones_initializer", "tensorflow.zeros_initializer", "tensorflow.expand_dims", "inputs.get_shape"], "function", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.weight_ratio.weight_ratio_mean", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.weight_ratio.weight_ratio_weighted_sum"], ["", "", "def", "layer_norm", "(", "inputs", ",", "w_x_inp", ",", "params", ",", "epsilon", "=", "1e-6", ",", "dtype", "=", "None", ",", "scope", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    Layer Normalization\n    :param inputs: A Tensor of shape [..., channel_size]\n    :param epsilon: A floating number\n    :param dtype: An optional instance of tf.DType\n    :param scope: An optional string\n    :returns: A Tensor with the same shape as inputs\n\n    w_x_inp: [bs, len_src, len, dim]\n    \"\"\"", "\n", "with", "tf", ".", "variable_scope", "(", "scope", ",", "default_name", "=", "\"layer_norm\"", ",", "values", "=", "[", "inputs", "]", ",", "\n", "dtype", "=", "dtype", ")", ":", "\n", "        ", "channel_size", "=", "inputs", ".", "get_shape", "(", ")", ".", "as_list", "(", ")", "[", "-", "1", "]", "\n", "\n", "scale", "=", "tf", ".", "get_variable", "(", "\"scale\"", ",", "shape", "=", "[", "channel_size", "]", ",", "\n", "initializer", "=", "tf", ".", "ones_initializer", "(", ")", ")", "\n", "\n", "offset", "=", "tf", ".", "get_variable", "(", "\"offset\"", ",", "shape", "=", "[", "channel_size", "]", ",", "\n", "initializer", "=", "tf", ".", "zeros_initializer", "(", ")", ")", "\n", "\n", "mean", "=", "tf", ".", "reduce_mean", "(", "inputs", ",", "axis", "=", "-", "1", ",", "keep_dims", "=", "True", ")", "\n", "variance", "=", "tf", ".", "reduce_mean", "(", "tf", ".", "square", "(", "inputs", "-", "mean", ")", ",", "axis", "=", "-", "1", ",", "\n", "keep_dims", "=", "True", ")", "\n", "\n", "averaged", "=", "(", "inputs", "-", "mean", ")", "\n", "norm_inputs", "=", "averaged", "*", "tf", ".", "rsqrt", "(", "variance", "+", "epsilon", ")", "\n", "\n", "w_inp_mean", "=", "wr", ".", "weight_ratio_mean", "(", "inputs", ",", "mean", ",", "stab", "=", "params", ".", "stab", ")", "\n", "w_inp_out", ",", "w_mean_out", "=", "wr", ".", "weight_ratio_weighted_sum", "(", "[", "inputs", ",", "mean", "]", ",", "\n", "[", "1.", ",", "-", "1.", "]", ",", "\n", "averaged", ",", "\n", "stab", "=", "params", ".", "stab", ",", "\n", "flatten", "=", "True", ")", "\n", "w_x_mean", "=", "tf", ".", "reduce_sum", "(", "w_x_inp", "*", "tf", ".", "expand_dims", "(", "w_inp_mean", ",", "1", ")", ",", "-", "1", ")", "\n", "w_inp_out", "=", "tf", ".", "expand_dims", "(", "w_inp_out", ",", "1", ")", "\n", "w_mean_out", "=", "tf", ".", "expand_dims", "(", "w_mean_out", ",", "1", ")", "\n", "w_x_out", "=", "w_x_inp", "*", "w_inp_out", "\n", "w_x_out", "+=", "tf", ".", "expand_dims", "(", "w_x_mean", ",", "-", "1", ")", "*", "w_mean_out", "\n", "\n", "return", "{", "\"outputs\"", ":", "norm_inputs", "*", "scale", "+", "offset", ",", "\n", "\"weight_ratios\"", ":", "w_x_out", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.lrp.multihead_attention_v2n": [[491, 593], ["ValueError", "ValueError", "tensorflow.variable_scope", "thumt.split_heads", "thumt.split_heads", "thumt.split_heads", "lrp.split_heads", "thumt.multiplicative_attention", "thumt.combine_heads", "tensorflow.transpose", "tensorflow.reshape", "tensorflow.matmul", "tensorflow.reshape", "tensorflow.transpose", "lrp.combine_heads_v2n", "tensorflow.shape", "tensorflow.shape", "tensorflow.shape", "tensorflow.shape", "lrp.linear_v2n", "tensorflow.split", "tensorflow.split", "thumt.linear", "lrp.linear_v2n", "tensorflow.split", "tensorflow.split", "lrp.linear_v2n", "tensorflow.shape"], "function", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.layers.attention.split_heads", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.layers.attention.split_heads", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.layers.attention.split_heads", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.layers.attention.split_heads", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.layers.attention.multiplicative_attention", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.layers.attention.combine_heads", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.lrp.combine_heads_v2n", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.lrp.linear_v2n", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.layers.nn.linear", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.lrp.linear_v2n", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.lrp.linear_v2n"], ["", "", "def", "multihead_attention_v2n", "(", "queries", ",", "memories", ",", "bias", ",", "w_x_inp", ",", "num_heads", ",", "\n", "key_size", ",", "value_size", ",", "output_size", ",", "params", ",", "\n", "keep_prob", "=", "None", ",", "output", "=", "True", ",", "dtype", "=", "None", ",", "\n", "scope", "=", "None", ")", ":", "\n", "    ", "\"\"\" Multi-head scaled-dot-product attention with input/output\n        transformations.\n\n    :param queries: A tensor with shape [batch, length_q, depth_q] if\n    :param memories: A tensor with shape [batch, length_m, depth_m]\n    :param bias: A tensor (see attention_bias)\n    :param num_heads: An integer dividing key_size and value_size\n    :param key_size: An integer\n    :param value_size: An integer\n    :param output_size: An integer\n    :param keep_prob: A floating point number in (0, 1]\n    :param output: Whether to use output transformation\n    :param dtype: An optional instance of tf.DType\n    :param scope: An optional string\n\n\n    :returns: A dict with the following keys:\n        weights: A tensor with shape [batch, heads, length_q, length_v]\n        outputs: A tensor with shape [batch, length_q, depth_v]\n        weight_ratio: [batch. length_q, d, length_v, d]\n\n        w_x_inp: [batch, len_src, len_src, d] or [batch, len_trg, len_trg, d]\n    \"\"\"", "\n", "\n", "if", "key_size", "%", "num_heads", "!=", "0", ":", "\n", "        ", "raise", "ValueError", "(", "\"Key size (%d) must be divisible by the number of \"", "\n", "\"attention heads (%d).\"", "%", "(", "key_size", ",", "num_heads", ")", ")", "\n", "\n", "", "if", "value_size", "%", "num_heads", "!=", "0", ":", "\n", "        ", "raise", "ValueError", "(", "\"Value size (%d) must be divisible by the number of \"", "\n", "\"attention heads (%d).\"", "%", "(", "value_size", ",", "num_heads", ")", ")", "\n", "\n", "", "with", "tf", ".", "variable_scope", "(", "scope", ",", "default_name", "=", "\"multihead_attention\"", ",", "\n", "values", "=", "[", "queries", ",", "memories", "]", ",", "dtype", "=", "dtype", ")", ":", "\n", "        ", "bs", "=", "tf", ".", "shape", "(", "w_x_inp", ")", "[", "0", "]", "\n", "len_q", "=", "tf", ".", "shape", "(", "queries", ")", "[", "1", "]", "\n", "len_src", "=", "tf", ".", "shape", "(", "w_x_inp", ")", "[", "1", "]", "\n", "dim", "=", "tf", ".", "shape", "(", "w_x_inp", ")", "[", "3", "]", "\n", "if", "memories", "is", "None", ":", "\n", "# self attention", "\n", "            ", "size", "=", "key_size", "*", "2", "+", "value_size", "\n", "combined_linear", "=", "linear_v2n", "(", "queries", ",", "size", ",", "True", ",", "[", "w_x_inp", "]", ",", "\n", "params", ",", "True", ",", "scope", "=", "\"qkv_transform\"", ")", "\n", "combined", "=", "combined_linear", "[", "\"output\"", "]", "\n", "q", ",", "k", ",", "v", "=", "tf", ".", "split", "(", "combined", ",", "[", "key_size", ",", "key_size", ",", "value_size", "]", ",", "\n", "axis", "=", "-", "1", ")", "\n", "w_x_combined", "=", "combined_linear", "[", "\"weight_ratios\"", "]", "[", "0", "]", "\n", "w_x_q", ",", "w_x_k", ",", "w_x_v", "=", "tf", ".", "split", "(", "w_x_combined", ",", "\n", "[", "key_size", ",", "key_size", ",", "value_size", "]", ",", "\n", "axis", "=", "-", "1", ")", "\n", "", "else", ":", "\n", "            ", "q", "=", "nn", ".", "linear", "(", "queries", ",", "key_size", ",", "True", ",", "params", ",", "True", ",", "\n", "scope", "=", "\"q_transform\"", ")", "\n", "combined_linear", "=", "linear_v2n", "(", "memories", ",", "key_size", "+", "value_size", ",", "True", ",", "\n", "[", "w_x_inp", "]", ",", "params", ",", "True", ",", "\n", "scope", "=", "\"kv_transform\"", ")", "\n", "combined", "=", "combined_linear", "[", "\"output\"", "]", "\n", "k", ",", "v", "=", "tf", ".", "split", "(", "combined", ",", "[", "key_size", ",", "value_size", "]", ",", "axis", "=", "-", "1", ")", "\n", "w_x_combined", "=", "combined_linear", "[", "\"weight_ratios\"", "]", "[", "0", "]", "\n", "w_x_k", ",", "w_x_v", "=", "tf", ".", "split", "(", "w_x_combined", ",", "[", "key_size", ",", "value_size", "]", ",", "\n", "axis", "=", "-", "1", ")", "\n", "\n", "# split heads", "\n", "", "q", "=", "attention", ".", "split_heads", "(", "q", ",", "num_heads", ")", "\n", "k", "=", "attention", ".", "split_heads", "(", "k", ",", "num_heads", ")", "\n", "v", "=", "attention", ".", "split_heads", "(", "v", ",", "num_heads", ")", "\n", "w_x_v", "=", "split_heads", "(", "w_x_v", ",", "num_heads", ")", "\n", "\n", "# scale query", "\n", "key_depth_per_head", "=", "key_size", "//", "num_heads", "\n", "q", "*=", "key_depth_per_head", "**", "-", "0.5", "\n", "\n", "# attention", "\n", "results", "=", "attention", ".", "multiplicative_attention", "(", "q", ",", "k", ",", "v", ",", "bias", ",", "keep_prob", ")", "\n", "\n", "# combine heads", "\n", "weights", "=", "results", "[", "\"weights\"", "]", "\n", "x", "=", "attention", ".", "combine_heads", "(", "results", "[", "\"outputs\"", "]", ")", "\n", "\n", "w_x_v", "=", "tf", ".", "transpose", "(", "w_x_v", ",", "[", "0", ",", "1", ",", "3", ",", "2", ",", "4", "]", ")", "\n", "w_x_v", "=", "tf", ".", "reshape", "(", "w_x_v", ",", "[", "bs", ",", "num_heads", ",", "tf", ".", "shape", "(", "w_x_v", ")", "[", "2", "]", ",", "-", "1", "]", ")", "\n", "w_x_att", "=", "tf", ".", "matmul", "(", "weights", ",", "w_x_v", ")", "\n", "w_x_att", "=", "tf", ".", "reshape", "(", "w_x_att", ",", "\n", "[", "bs", ",", "num_heads", ",", "len_q", ",", "len_src", ",", "key_depth_per_head", "]", ")", "\n", "w_x_att", "=", "tf", ".", "transpose", "(", "w_x_att", ",", "[", "0", ",", "1", ",", "3", ",", "2", ",", "4", "]", ")", "\n", "w_x_att", "=", "combine_heads_v2n", "(", "w_x_att", ")", "\n", "\n", "if", "output", ":", "\n", "            ", "outputs_linear", "=", "linear_v2n", "(", "x", ",", "output_size", ",", "True", ",", "[", "w_x_att", "]", ",", "\n", "params", ",", "True", ",", "\n", "scope", "=", "\"output_transform\"", ")", "\n", "outputs", "=", "outputs_linear", "[", "\"output\"", "]", "\n", "w_x_out", "=", "outputs_linear", "[", "\"weight_ratios\"", "]", "[", "0", "]", "\n", "", "else", ":", "\n", "            ", "outputs", "=", "x", "\n", "w_x_out", "=", "w_x_att", "\n", "\n", "", "return", "{", "\"weights\"", ":", "weights", ",", "\"outputs\"", ":", "outputs", ",", "\"weight_ratio\"", ":", "w_x_out", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.parallel._maybe_repeat": [[15, 21], ["isinstance", "len"], "function", ["None"], ["    ", "if", "isinstance", "(", "x", ",", "list", ")", ":", "\n", "        ", "assert", "len", "(", "x", ")", "==", "n", "\n", "return", "x", "\n", "", "else", ":", "\n", "        ", "return", "[", "x", "]", "*", "n", "\n", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.parallel.data_parallelism": [[24, 56], ["len", "six.iteritems", "parallel._maybe_repeat", "range", "parallel._maybe_repeat", "range", "parallel._maybe_repeat", "list", "range", "tensorflow.variable_scope", "zip", "range", "tensorflow.get_variable_scope", "tensorflow.name_scope", "tensorflow.device", "outputs.append"], "function", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.parallel._maybe_repeat", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.parallel._maybe_repeat", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.parallel._maybe_repeat"], ["    ", "num_worker", "=", "len", "(", "devices", ")", "\n", "devices", "=", "[", "\"gpu:%d\"", "%", "d", "for", "d", "in", "devices", "]", "\n", "\n", "# Replicate args and kwargs", "\n", "if", "args", ":", "\n", "        ", "new_args", "=", "[", "_maybe_repeat", "(", "arg", ",", "num_worker", ")", "for", "arg", "in", "args", "]", "\n", "# Transpose", "\n", "# transpose tuple to list", "\n", "new_args", "=", "[", "list", "(", "x", ")", "for", "x", "in", "zip", "(", "*", "new_args", ")", "]", "\n", "", "else", ":", "\n", "        ", "new_args", "=", "[", "[", "]", "for", "_", "in", "range", "(", "num_worker", ")", "]", "\n", "\n", "", "new_kwargs", "=", "[", "{", "}", "for", "_", "in", "range", "(", "num_worker", ")", "]", "\n", "\n", "for", "k", ",", "v", "in", "six", ".", "iteritems", "(", "kwargs", ")", ":", "\n", "        ", "vals", "=", "_maybe_repeat", "(", "v", ",", "num_worker", ")", "\n", "\n", "for", "i", "in", "range", "(", "num_worker", ")", ":", "\n", "            ", "new_kwargs", "[", "i", "]", "[", "k", "]", "=", "vals", "[", "i", "]", "\n", "\n", "", "", "fns", "=", "_maybe_repeat", "(", "fn", ",", "num_worker", ")", "\n", "\n", "# Now make the parallel call.", "\n", "outputs", "=", "[", "]", "\n", "\n", "for", "i", "in", "range", "(", "num_worker", ")", ":", "\n", "        ", "with", "tf", ".", "variable_scope", "(", "tf", ".", "get_variable_scope", "(", ")", ",", "reuse", "=", "(", "i", "!=", "0", ")", ")", ":", "\n", "            ", "with", "tf", ".", "name_scope", "(", "\"parallel_%d\"", "%", "i", ")", ":", "\n", "                ", "with", "tf", ".", "device", "(", "devices", "[", "i", "]", ")", ":", "\n", "# when call functions, '*' means unpack a tuple, '**' means unpack a dict", "\n", "                    ", "outputs", ".", "append", "(", "fns", "[", "i", "]", "(", "*", "new_args", "[", "i", "]", ",", "**", "new_kwargs", "[", "i", "]", ")", ")", "\n", "\n", "", "", "", "", "return", "outputs", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.parallel.shard_features": [[58, 91], ["len", "range", "tensorflow.device", "six.iteritems", "datashard_to_features.append", "tensorflow.convert_to_tensor", "range", "tensorflow.split", "tf.tile.shape.as_list", "tensorflow.expand_dims", "tensorflow.tile", "tensorflow.shape", "size_splits.append", "six.iteritems", "tensorflow.cond", "tensorflow.greater", "tensorflow.mod"], "function", ["None"], ["\n", "", "def", "shard_features", "(", "features", ",", "device_list", ")", ":", "\n", "    ", "num_datashards", "=", "len", "(", "device_list", ")", "\n", "sharded_features", "=", "{", "}", "\n", "\n", "with", "tf", ".", "device", "(", "\"/cpu:0\"", ")", ":", "\n", "        ", "for", "k", ",", "v", "in", "six", ".", "iteritems", "(", "features", ")", ":", "\n", "            ", "v", "=", "tf", ".", "convert_to_tensor", "(", "v", ")", "\n", "\n", "# if v is scalar, then expand_dims and tile", "\n", "if", "not", "v", ".", "shape", ".", "as_list", "(", ")", ":", "\n", "                ", "v", "=", "tf", ".", "expand_dims", "(", "v", ",", "axis", "=", "-", "1", ")", "\n", "v", "=", "tf", ".", "tile", "(", "v", ",", "[", "num_datashards", "]", ")", "\n", "\n", "", "batch_size", "=", "tf", ".", "shape", "(", "v", ")", "[", "0", "]", "\n", "size_splits", "=", "[", "]", "\n", "# number of batch on each device according to batch_size mod num_datashards", "\n", "for", "i", "in", "range", "(", "num_datashards", ")", ":", "\n", "                ", "size_splits", ".", "append", "(", "\n", "tf", ".", "cond", "(", "tf", ".", "greater", "(", "tf", ".", "mod", "(", "batch_size", ",", "num_datashards", ")", ",", "i", ")", ",", "\n", "lambda", ":", "batch_size", "//", "num_datashards", "+", "1", ",", "\n", "lambda", ":", "batch_size", "//", "num_datashards", ")", "\n", ")", "\n", "\n", "", "sharded_features", "[", "k", "]", "=", "tf", ".", "split", "(", "v", ",", "size_splits", ",", "0", ")", "\n", "\n", "", "", "datashard_to_features", "=", "[", "]", "\n", "# convert to dict nested in list", "\n", "for", "d", "in", "range", "(", "num_datashards", ")", ":", "\n", "        ", "feat", "=", "{", "\n", "k", ":", "v", "[", "d", "]", "for", "k", ",", "v", "in", "six", ".", "iteritems", "(", "sharded_features", ")", "\n", "}", "\n", "datashard_to_features", ".", "append", "(", "feat", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.parallel.parallel_model": [[93, 104], ["thumt.Parallelism", "parallel.shard_features", "expert_utils.Parallelism.", "len", "model_fn"], "function", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.bin.translator.shard_features"], ["\n", "\n", "", "def", "parallel_model", "(", "model_fn", ",", "features", ",", "devices", ")", ":", "\n", "    ", "if", "len", "(", "devices", ")", "==", "1", ":", "\n", "        ", "return", "[", "model_fn", "(", "features", ")", "]", "\n", "\n", "", "devices", "=", "[", "\"gpu:%d\"", "%", "d", "for", "d", "in", "devices", "]", "\n", "dp", "=", "expert_utils", ".", "Parallelism", "(", "devices", ",", "reuse", "=", "True", ",", "daisy_chain_variables", "=", "True", ")", "\n", "\n", "features", "=", "shard_features", "(", "features", ",", "devices", ")", "\n", "outputs", "=", "dp", "(", "model_fn", ",", "features", ")", "\n", "#outputs = data_parallelism(devices, model_fn, features)", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.hooks.EvaluationHook.__init__": [[186, 220], ["tensorflow.logging.info", "base_dir.rstrip", "os.path.join", "os.path.join", "os.path.join", "tensorflow.train.SecondOrStepTimer", "ValueError"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "eval_fn", ",", "eval_input_fn", ",", "eval_decode_fn", ",", "base_dir", ",", "\n", "session_config", ",", "max_to_keep", "=", "5", ",", "eval_secs", "=", "None", ",", "\n", "eval_steps", "=", "None", ",", "metric", "=", "\"BLEU\"", ")", ":", "\n", "        ", "\"\"\" Initializes a `EvaluationHook`.\n        :param eval_fn: A function with signature (feature)\n        :param eval_input_fn: A function with signature ()\n        :param eval_decode_fn: A function with signature (inputs)\n        :param base_dir: A string. Base directory for the checkpoint files.\n        :param session_config: An instance of tf.ConfigProto\n        :param max_to_keep: An integer. The maximum of checkpoints to save\n        :param eval_secs: An integer, eval every N secs.\n        :param eval_steps: An integer, eval every N steps.\n        :param checkpoint_basename: `str`, base name for the checkpoint files.\n        :raises ValueError: One of `save_steps` or `save_secs` should be set.\n        :raises ValueError: At most one of saver or scaffold should be set.\n        \"\"\"", "\n", "tf", ".", "logging", ".", "info", "(", "\"Create EvaluationHook.\"", ")", "\n", "\n", "if", "metric", "!=", "\"BLEU\"", ":", "\n", "            ", "raise", "ValueError", "(", "\"Currently, EvaluationHook only support BLEU\"", ")", "\n", "\n", "", "self", ".", "_base_dir", "=", "base_dir", ".", "rstrip", "(", "\"/\"", ")", "\n", "self", ".", "_session_config", "=", "session_config", "\n", "self", ".", "_save_path", "=", "os", ".", "path", ".", "join", "(", "base_dir", ",", "\"eval\"", ")", "\n", "self", ".", "_record_name", "=", "os", ".", "path", ".", "join", "(", "self", ".", "_save_path", ",", "\"record\"", ")", "\n", "self", ".", "_log_name", "=", "os", ".", "path", ".", "join", "(", "self", ".", "_save_path", ",", "\"log\"", ")", "\n", "self", ".", "_eval_fn", "=", "eval_fn", "\n", "self", ".", "_eval_input_fn", "=", "eval_input_fn", "\n", "self", ".", "_eval_decode_fn", "=", "eval_decode_fn", "\n", "self", ".", "_max_to_keep", "=", "max_to_keep", "\n", "self", ".", "_metric", "=", "metric", "\n", "self", ".", "_global_step", "=", "None", "\n", "self", ".", "_timer", "=", "tf", ".", "train", ".", "SecondOrStepTimer", "(", "\n", "every_secs", "=", "eval_secs", "or", "None", ",", "every_steps", "=", "eval_steps", "or", "None", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.hooks.EvaluationHook.begin": [[222, 243], ["tensorflow.train.get_global_step", "os.path.join", "tensorflow.gfile.Glob", "hooks.EvaluationHook._timer.last_triggered_step", "hooks.EvaluationHook._timer.update_last_triggered_step", "tensorflow.gfile.Exists", "tensorflow.logging.info", "tensorflow.gfile.MakeDirs", "name.replace", "tensorflow.gfile.Copy", "RuntimeError"], "methods", ["None"], ["", "def", "begin", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "_timer", ".", "last_triggered_step", "(", ")", "is", "None", ":", "\n", "            ", "self", ".", "_timer", ".", "update_last_triggered_step", "(", "0", ")", "\n", "\n", "", "global_step", "=", "tf", ".", "train", ".", "get_global_step", "(", ")", "\n", "\n", "if", "not", "tf", ".", "gfile", ".", "Exists", "(", "self", ".", "_save_path", ")", ":", "\n", "            ", "tf", ".", "logging", ".", "info", "(", "\"Making dir: %s\"", "%", "self", ".", "_save_path", ")", "\n", "tf", ".", "gfile", ".", "MakeDirs", "(", "self", ".", "_save_path", ")", "\n", "\n", "", "params_pattern", "=", "os", ".", "path", ".", "join", "(", "self", ".", "_base_dir", ",", "\"*.json\"", ")", "\n", "params_files", "=", "tf", ".", "gfile", ".", "Glob", "(", "params_pattern", ")", "\n", "\n", "for", "name", "in", "params_files", ":", "\n", "            ", "new_name", "=", "name", ".", "replace", "(", "self", ".", "_base_dir", ",", "self", ".", "_save_path", ")", "\n", "tf", ".", "gfile", ".", "Copy", "(", "name", ",", "new_name", ",", "overwrite", "=", "True", ")", "\n", "\n", "", "if", "global_step", "is", "None", ":", "\n", "            ", "raise", "RuntimeError", "(", "\"Global step should be created first\"", ")", "\n", "\n", "", "self", ".", "_global_step", "=", "global_step", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.hooks.EvaluationHook.before_run": [[244, 247], ["tensorflow.train.SessionRunArgs"], "methods", ["None"], ["", "def", "before_run", "(", "self", ",", "run_context", ")", ":", "\n", "        ", "args", "=", "tf", ".", "train", ".", "SessionRunArgs", "(", "self", ".", "_global_step", ")", "\n", "return", "args", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.hooks.EvaluationHook.after_run": [[248, 313], ["hooks.EvaluationHook._timer.should_trigger_for_step", "run_context.session.run", "hooks.EvaluationHook._timer.should_trigger_for_step", "hooks.EvaluationHook._timer.update_last_triggered_step", "os.path.join", "hooks._get_saver", "tensorflow.logging.info", "_get_saver.save", "tensorflow.logging.info", "hooks._evaluate", "tensorflow.logging.info", "hooks._save_log", "os.path.join", "hooks._read_checkpoint_def", "hooks._read_score_record", "hooks._add_to_record", "hooks._save_score_record", "checkpoint_filename.replace.replace.replace", "hooks._save_checkpoint_def", "tensorflow.logging.info", "os.path.join", "os.path.join", "tensorflow.gfile.Glob", "tensorflow.logging.info", "os.path.join", "tensorflow.logging.info", "tensorflow.gfile.Glob", "o_file.replace", "tensorflow.gfile.Copy", "tensorflow.gfile.Remove"], "methods", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.hooks._get_saver", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.bert_dictionary.BertDictionary.save", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.hooks._evaluate", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.hooks._save_log", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.hooks._read_checkpoint_def", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.hooks._read_score_record", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.hooks._add_to_record", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.hooks._save_score_record", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.hooks._save_checkpoint_def"], ["", "def", "after_run", "(", "self", ",", "run_context", ",", "run_values", ")", ":", "\n", "        ", "stale_global_step", "=", "run_values", ".", "results", "\n", "\n", "if", "self", ".", "_timer", ".", "should_trigger_for_step", "(", "stale_global_step", "+", "1", ")", ":", "\n", "            ", "global_step", "=", "run_context", ".", "session", ".", "run", "(", "self", ".", "_global_step", ")", "\n", "\n", "# Get the real value", "\n", "if", "self", ".", "_timer", ".", "should_trigger_for_step", "(", "global_step", ")", ":", "\n", "                ", "self", ".", "_timer", ".", "update_last_triggered_step", "(", "global_step", ")", "\n", "# Save model", "\n", "save_path", "=", "os", ".", "path", ".", "join", "(", "self", ".", "_base_dir", ",", "\"model.ckpt\"", ")", "\n", "saver", "=", "_get_saver", "(", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"Saving checkpoints for %d into %s.\"", "%", "\n", "(", "global_step", ",", "save_path", ")", ")", "\n", "saver", ".", "save", "(", "run_context", ".", "session", ",", "\n", "save_path", ",", "\n", "global_step", "=", "global_step", ")", "\n", "# Do validation here", "\n", "tf", ".", "logging", ".", "info", "(", "\"Validating model at step %d\"", "%", "global_step", ")", "\n", "score", "=", "_evaluate", "(", "self", ".", "_eval_fn", ",", "self", ".", "_eval_input_fn", ",", "\n", "self", ".", "_eval_decode_fn", ",", "\n", "self", ".", "_base_dir", ",", "\n", "self", ".", "_session_config", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"%s at step %d: %f\"", "%", "\n", "(", "self", ".", "_metric", ",", "global_step", ",", "score", ")", ")", "\n", "\n", "_save_log", "(", "self", ".", "_log_name", ",", "(", "self", ".", "_metric", ",", "global_step", ",", "score", ")", ")", "\n", "\n", "checkpoint_filename", "=", "os", ".", "path", ".", "join", "(", "self", ".", "_base_dir", ",", "\n", "\"checkpoint\"", ")", "\n", "all_checkpoints", "=", "_read_checkpoint_def", "(", "checkpoint_filename", ")", "\n", "records", "=", "_read_score_record", "(", "self", ".", "_record_name", ")", "\n", "latest_checkpoint", "=", "all_checkpoints", "[", "-", "1", "]", "\n", "record", "=", "[", "latest_checkpoint", ",", "score", "]", "\n", "added", ",", "removed", ",", "records", "=", "_add_to_record", "(", "records", ",", "record", ",", "\n", "self", ".", "_max_to_keep", ")", "\n", "\n", "if", "added", "is", "not", "None", ":", "\n", "                    ", "old_path", "=", "os", ".", "path", ".", "join", "(", "self", ".", "_base_dir", ",", "added", ")", "\n", "new_path", "=", "os", ".", "path", ".", "join", "(", "self", ".", "_save_path", ",", "added", ")", "\n", "old_files", "=", "tf", ".", "gfile", ".", "Glob", "(", "old_path", "+", "\"*\"", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"Copying %s to %s\"", "%", "(", "old_path", ",", "new_path", ")", ")", "\n", "\n", "for", "o_file", "in", "old_files", ":", "\n", "                        ", "n_file", "=", "o_file", ".", "replace", "(", "old_path", ",", "new_path", ")", "\n", "tf", ".", "gfile", ".", "Copy", "(", "o_file", ",", "n_file", ",", "overwrite", "=", "True", ")", "\n", "\n", "", "", "if", "removed", "is", "not", "None", ":", "\n", "                    ", "filename", "=", "os", ".", "path", ".", "join", "(", "self", ".", "_save_path", ",", "removed", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"Removing %s\"", "%", "filename", ")", "\n", "files", "=", "tf", ".", "gfile", ".", "Glob", "(", "filename", "+", "\"*\"", ")", "\n", "\n", "for", "name", "in", "files", ":", "\n", "                        ", "tf", ".", "gfile", ".", "Remove", "(", "name", ")", "\n", "\n", "", "", "_save_score_record", "(", "self", ".", "_record_name", ",", "records", ")", "\n", "checkpoint_filename", "=", "checkpoint_filename", ".", "replace", "(", "\n", "self", ".", "_base_dir", ",", "self", ".", "_save_path", "\n", ")", "\n", "_save_checkpoint_def", "(", "checkpoint_filename", ",", "\n", "[", "item", "[", "0", "]", "for", "item", "in", "records", "]", ")", "\n", "\n", "best_score", "=", "records", "[", "0", "]", "[", "1", "]", "\n", "tf", ".", "logging", ".", "info", "(", "\"Best score at step %d: %f\"", "%", "\n", "(", "global_step", ",", "best_score", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.hooks.EvaluationHook.end": [[314, 363], ["session.run", "hooks.EvaluationHook._timer.last_triggered_step", "tensorflow.logging.info", "hooks._evaluate", "tensorflow.logging.info", "os.path.join", "hooks._read_checkpoint_def", "hooks._read_score_record", "hooks._add_to_record", "hooks._save_score_record", "checkpoint_filename.replace.replace.replace", "hooks._save_checkpoint_def", "tensorflow.logging.info", "os.path.join", "os.path.join", "tensorflow.gfile.Glob", "tensorflow.logging.info", "os.path.join", "tensorflow.logging.info", "tensorflow.gfile.Glob", "o_file.replace", "tensorflow.gfile.Copy", "tensorflow.gfile.Remove"], "methods", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.hooks._evaluate", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.hooks._read_checkpoint_def", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.hooks._read_score_record", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.hooks._add_to_record", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.hooks._save_score_record", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.hooks._save_checkpoint_def"], ["", "", "", "def", "end", "(", "self", ",", "session", ")", ":", "\n", "        ", "last_step", "=", "session", ".", "run", "(", "self", ".", "_global_step", ")", "\n", "\n", "if", "last_step", "!=", "self", ".", "_timer", ".", "last_triggered_step", "(", ")", ":", "\n", "            ", "global_step", "=", "last_step", "\n", "tf", ".", "logging", ".", "info", "(", "\"Validating model at step %d\"", "%", "global_step", ")", "\n", "score", "=", "_evaluate", "(", "self", ".", "_eval_fn", ",", "self", ".", "_eval_input_fn", ",", "\n", "self", ".", "_eval_decode_fn", ",", "\n", "self", ".", "_base_dir", ",", "\n", "self", ".", "_session_config", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"%s at step %d: %f\"", "%", "\n", "(", "self", ".", "_metric", ",", "global_step", ",", "score", ")", ")", "\n", "\n", "checkpoint_filename", "=", "os", ".", "path", ".", "join", "(", "self", ".", "_base_dir", ",", "\n", "\"checkpoint\"", ")", "\n", "all_checkpoints", "=", "_read_checkpoint_def", "(", "checkpoint_filename", ")", "\n", "records", "=", "_read_score_record", "(", "self", ".", "_record_name", ")", "\n", "latest_checkpoint", "=", "all_checkpoints", "[", "-", "1", "]", "\n", "record", "=", "[", "latest_checkpoint", ",", "score", "]", "\n", "added", ",", "removed", ",", "records", "=", "_add_to_record", "(", "records", ",", "record", ",", "\n", "self", ".", "_max_to_keep", ")", "\n", "\n", "if", "added", "is", "not", "None", ":", "\n", "                ", "old_path", "=", "os", ".", "path", ".", "join", "(", "self", ".", "_base_dir", ",", "added", ")", "\n", "new_path", "=", "os", ".", "path", ".", "join", "(", "self", ".", "_save_path", ",", "added", ")", "\n", "old_files", "=", "tf", ".", "gfile", ".", "Glob", "(", "old_path", "+", "\"*\"", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"Copying %s to %s\"", "%", "(", "old_path", ",", "new_path", ")", ")", "\n", "\n", "for", "o_file", "in", "old_files", ":", "\n", "                    ", "n_file", "=", "o_file", ".", "replace", "(", "old_path", ",", "new_path", ")", "\n", "tf", ".", "gfile", ".", "Copy", "(", "o_file", ",", "n_file", ",", "overwrite", "=", "True", ")", "\n", "\n", "", "", "if", "removed", "is", "not", "None", ":", "\n", "                ", "filename", "=", "os", ".", "path", ".", "join", "(", "self", ".", "_save_path", ",", "removed", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"Removing %s\"", "%", "filename", ")", "\n", "files", "=", "tf", ".", "gfile", ".", "Glob", "(", "filename", "+", "\"*\"", ")", "\n", "\n", "for", "name", "in", "files", ":", "\n", "                    ", "tf", ".", "gfile", ".", "Remove", "(", "name", ")", "\n", "\n", "", "", "_save_score_record", "(", "self", ".", "_record_name", ",", "records", ")", "\n", "checkpoint_filename", "=", "checkpoint_filename", ".", "replace", "(", "\n", "self", ".", "_base_dir", ",", "self", ".", "_save_path", "\n", ")", "\n", "_save_checkpoint_def", "(", "checkpoint_filename", ",", "\n", "[", "item", "[", "0", "]", "for", "item", "in", "records", "]", ")", "\n", "\n", "best_score", "=", "records", "[", "0", "]", "[", "1", "]", "\n", "tf", ".", "logging", ".", "info", "(", "\"Best score: %f\"", "%", "best_score", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.hooks.MultiStepHook.__init__": [[367, 371], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "hook", ",", "step", "=", "1", ")", ":", "\n", "        ", "self", ".", "_hook", "=", "hook", "\n", "self", ".", "_step", "=", "step", "\n", "self", ".", "_iter", "=", "0", "if", "step", "==", "1", "else", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.hooks.MultiStepHook.begin": [[372, 374], ["hooks.MultiStepHook._hook.begin"], "methods", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.hooks.MultiStepHook.begin"], ["", "def", "begin", "(", "self", ")", ":", "\n", "        ", "self", ".", "_hook", ".", "begin", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.hooks.MultiStepHook.after_create_session": [[375, 377], ["hooks.MultiStepHook._hook.after_create_session"], "methods", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.hooks.MultiStepHook.after_create_session"], ["", "def", "after_create_session", "(", "self", ",", "session", ",", "coord", ")", ":", "\n", "        ", "self", ".", "_hook", ".", "after_create_session", "(", "session", ",", "coord", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.hooks.MultiStepHook.before_run": [[378, 380], ["hooks.MultiStepHook._hook.before_run"], "methods", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.hooks.MultiStepHook.before_run"], ["", "def", "before_run", "(", "self", ",", "run_context", ")", ":", "\n", "        ", "return", "self", ".", "_hook", ".", "before_run", "(", "run_context", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.hooks.MultiStepHook.after_run": [[381, 385], ["hooks.MultiStepHook._hook.after_run"], "methods", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.hooks.MultiStepHook.after_run"], ["", "def", "after_run", "(", "self", ",", "run_context", ",", "run_values", ")", ":", "\n", "        ", "if", "self", ".", "_iter", "%", "self", ".", "_step", "==", "0", ":", "\n", "            ", "self", ".", "_hook", ".", "after_run", "(", "run_context", ",", "run_values", ")", "\n", "", "self", ".", "_iter", "=", "(", "self", ".", "_iter", "+", "1", ")", "%", "self", ".", "_step", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.hooks.MultiStepHook.end": [[386, 388], ["hooks.MultiStepHook._hook.end"], "methods", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.hooks.MultiStepHook.end"], ["", "def", "end", "(", "self", ",", "session", ")", ":", "\n", "        ", "self", ".", "_hook", ".", "end", "(", "session", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.hooks._get_saver": [[16, 28], ["tensorflow.get_collection", "RuntimeError", "len", "RuntimeError"], "function", ["None"], ["def", "_get_saver", "(", ")", ":", "\n", "# Get saver from the SAVERS collection if present.", "\n", "    ", "collection_key", "=", "tf", ".", "GraphKeys", ".", "SAVERS", "\n", "savers", "=", "tf", ".", "get_collection", "(", "collection_key", ")", "\n", "\n", "if", "not", "savers", ":", "\n", "        ", "raise", "RuntimeError", "(", "\"No items in collection {}. \"", "\n", "\"Please add a saver to the collection \"", ")", "\n", "", "elif", "len", "(", "savers", ")", ">", "1", ":", "\n", "        ", "raise", "RuntimeError", "(", "\"More than one item in collection\"", ")", "\n", "\n", "", "return", "savers", "[", "0", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.hooks._save_log": [[30, 37], ["open", "datetime.datetime.now", "fd.write"], "function", ["None"], ["", "def", "_save_log", "(", "filename", ",", "result", ")", ":", "\n", "    ", "metric", ",", "global_step", ",", "score", "=", "result", "\n", "\n", "with", "open", "(", "filename", ",", "\"a\"", ")", "as", "fd", ":", "\n", "        ", "time", "=", "datetime", ".", "datetime", ".", "now", "(", ")", "\n", "msg", "=", "\"%s: %s at step %d: %f\\n\"", "%", "(", "time", ",", "metric", ",", "global_step", ",", "score", ")", "\n", "fd", ".", "write", "(", "msg", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.hooks._read_checkpoint_def": [[39, 49], ["tensorflow.gfile.GFile", "fd.readline", "records.append", "[].split", "[].strip", "line.strip().split", "line.strip"], "function", ["None"], ["", "", "def", "_read_checkpoint_def", "(", "filename", ")", ":", "\n", "    ", "records", "=", "[", "]", "\n", "\n", "with", "tf", ".", "gfile", ".", "GFile", "(", "filename", ")", "as", "fd", ":", "\n", "        ", "fd", ".", "readline", "(", ")", "\n", "\n", "for", "line", "in", "fd", ":", "\n", "            ", "records", ".", "append", "(", "line", ".", "strip", "(", ")", ".", "split", "(", "\":\"", ")", "[", "-", "1", "]", ".", "strip", "(", ")", "[", "1", ":", "-", "1", "]", ".", "split", "(", "\"/\"", ")", "[", "-", "1", "]", ")", "\n", "\n", "", "", "return", "records", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.hooks._save_checkpoint_def": [[51, 67], ["sorted", "int", "keys.append", "tensorflow.gfile.GFile", "fd.write", "operator.itemgetter", "fd.write", "checkpoint_name.strip().split", "checkpoint_name.strip"], "function", ["None"], ["", "def", "_save_checkpoint_def", "(", "filename", ",", "checkpoint_names", ")", ":", "\n", "    ", "keys", "=", "[", "]", "\n", "\n", "for", "checkpoint_name", "in", "checkpoint_names", ":", "\n", "        ", "step", "=", "int", "(", "checkpoint_name", ".", "strip", "(", ")", ".", "split", "(", "\"-\"", ")", "[", "-", "1", "]", ")", "\n", "keys", ".", "append", "(", "(", "step", ",", "checkpoint_name", ")", ")", "\n", "\n", "", "sorted_names", "=", "sorted", "(", "keys", ",", "key", "=", "operator", ".", "itemgetter", "(", "0", ")", ",", "\n", "reverse", "=", "True", ")", "\n", "\n", "with", "tf", ".", "gfile", ".", "GFile", "(", "filename", ",", "\"w\"", ")", "as", "fd", ":", "\n", "        ", "fd", ".", "write", "(", "\"model_checkpoint_path: \\\"%s\\\"\\n\"", "%", "checkpoint_names", "[", "0", "]", ")", "\n", "\n", "for", "checkpoint_name", "in", "sorted_names", ":", "\n", "            ", "checkpoint_name", "=", "checkpoint_name", "[", "1", "]", "\n", "fd", ".", "write", "(", "\"all_model_checkpoint_paths: \\\"%s\\\"\\n\"", "%", "checkpoint_name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.hooks._read_score_record": [[69, 84], ["tensorflow.gfile.Exists", "tensorflow.gfile.GFile", "line.strip().split", "float", "records.append", "name.strip", "line.strip"], "function", ["None"], ["", "", "", "def", "_read_score_record", "(", "filename", ")", ":", "\n", "# \"checkpoint_name\": score", "\n", "    ", "records", "=", "[", "]", "\n", "\n", "if", "not", "tf", ".", "gfile", ".", "Exists", "(", "filename", ")", ":", "\n", "        ", "return", "records", "\n", "\n", "", "with", "tf", ".", "gfile", ".", "GFile", "(", "filename", ")", "as", "fd", ":", "\n", "        ", "for", "line", "in", "fd", ":", "\n", "            ", "name", ",", "score", "=", "line", ".", "strip", "(", ")", ".", "split", "(", "\":\"", ")", "\n", "name", "=", "name", ".", "strip", "(", ")", "[", "1", ":", "-", "1", "]", "\n", "score", "=", "float", "(", "score", ")", "\n", "records", ".", "append", "(", "[", "name", ",", "score", "]", ")", "\n", "\n", "", "", "return", "records", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.hooks._save_score_record": [[86, 102], ["sorted", "int", "keys.append", "tensorflow.gfile.GFile", "operator.itemgetter", "fd.write", "checkpoint_name.strip().split", "checkpoint_name.strip"], "function", ["None"], ["", "def", "_save_score_record", "(", "filename", ",", "records", ")", ":", "\n", "    ", "keys", "=", "[", "]", "\n", "\n", "for", "record", "in", "records", ":", "\n", "        ", "checkpoint_name", "=", "record", "[", "0", "]", "\n", "step", "=", "int", "(", "checkpoint_name", ".", "strip", "(", ")", ".", "split", "(", "\"-\"", ")", "[", "-", "1", "]", ")", "\n", "keys", ".", "append", "(", "(", "step", ",", "record", ")", ")", "\n", "\n", "", "sorted_keys", "=", "sorted", "(", "keys", ",", "key", "=", "operator", ".", "itemgetter", "(", "0", ")", ",", "\n", "reverse", "=", "True", ")", "\n", "sorted_records", "=", "[", "item", "[", "1", "]", "for", "item", "in", "sorted_keys", "]", "\n", "\n", "with", "tf", ".", "gfile", ".", "GFile", "(", "filename", ",", "\"w\"", ")", "as", "fd", ":", "\n", "        ", "for", "record", "in", "sorted_records", ":", "\n", "            ", "checkpoint_name", ",", "score", "=", "record", "\n", "fd", ".", "write", "(", "\"\\\"%s\\\": %f\\n\"", "%", "(", "checkpoint_name", ",", "score", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.hooks._add_to_record": [[104, 131], ["sorted", "len", "sorted", "sorted.append"], "function", ["None"], ["", "", "", "def", "_add_to_record", "(", "records", ",", "record", ",", "max_to_keep", ")", ":", "\n", "    ", "added", "=", "None", "\n", "removed", "=", "None", "\n", "models", "=", "{", "}", "\n", "\n", "for", "(", "name", ",", "score", ")", "in", "records", ":", "\n", "        ", "models", "[", "name", "]", "=", "score", "\n", "\n", "", "if", "len", "(", "records", ")", "<", "max_to_keep", ":", "\n", "        ", "if", "record", "[", "0", "]", "not", "in", "models", ":", "\n", "            ", "added", "=", "record", "[", "0", "]", "\n", "records", ".", "append", "(", "record", ")", "\n", "", "", "else", ":", "\n", "        ", "sorted_records", "=", "sorted", "(", "records", ",", "key", "=", "lambda", "x", ":", "-", "x", "[", "1", "]", ")", "\n", "worst_score", "=", "sorted_records", "[", "-", "1", "]", "[", "1", "]", "\n", "current_score", "=", "record", "[", "1", "]", "\n", "\n", "if", "current_score", ">=", "worst_score", ":", "\n", "            ", "if", "record", "[", "0", "]", "not", "in", "models", ":", "\n", "                ", "added", "=", "record", "[", "0", "]", "\n", "removed", "=", "sorted_records", "[", "-", "1", "]", "[", "0", "]", "\n", "records", "=", "sorted_records", "[", ":", "-", "1", "]", "+", "[", "record", "]", "\n", "\n", "# Sort", "\n", "", "", "", "records", "=", "sorted", "(", "records", ",", "key", "=", "lambda", "x", ":", "-", "x", "[", "1", "]", ")", "\n", "\n", "return", "added", ",", "removed", ",", "records", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.hooks._evaluate": [[133, 179], ["tensorflow.Graph", "tf.Graph.as_default", "input_fn", "eval_fn", "tensorflow.train.ChiefSessionCreator", "decode_fn", "enumerate", "thumt.bleu", "tensorflow.placeholder", "tensorflow.placeholder", "tensorflow.train.MonitoredSession", "decode_fn", "list", "range", "sess.should_stop", "sess.run", "sess.run", "outputs.tolist.tolist", "all_outputs.extend", "range", "zip", "len", "item.tolist", "len", "all_refs[].extend"], "function", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.bleu.bleu"], ["", "def", "_evaluate", "(", "eval_fn", ",", "input_fn", ",", "decode_fn", ",", "path", ",", "config", ")", ":", "\n", "    ", "graph", "=", "tf", ".", "Graph", "(", ")", "\n", "with", "graph", ".", "as_default", "(", ")", ":", "\n", "        ", "features", "=", "input_fn", "(", ")", "\n", "refs", "=", "features", "[", "\"references\"", "]", "\n", "placeholders", "=", "{", "\n", "\"source\"", ":", "tf", ".", "placeholder", "(", "tf", ".", "int32", ",", "[", "None", ",", "None", "]", ",", "\"source\"", ")", ",", "\n", "\"source_length\"", ":", "tf", ".", "placeholder", "(", "tf", ".", "int32", ",", "[", "None", "]", ",", "\"source_length\"", ")", "\n", "}", "\n", "predictions", "=", "eval_fn", "(", "placeholders", ")", "\n", "predictions", "=", "predictions", "[", "0", "]", "[", ":", ",", "0", ",", ":", "]", "\n", "\n", "all_refs", "=", "[", "[", "]", "for", "_", "in", "range", "(", "len", "(", "refs", ")", ")", "]", "\n", "all_outputs", "=", "[", "]", "\n", "\n", "sess_creator", "=", "tf", ".", "train", ".", "ChiefSessionCreator", "(", "\n", "checkpoint_dir", "=", "path", ",", "\n", "config", "=", "config", "\n", ")", "\n", "\n", "with", "tf", ".", "train", ".", "MonitoredSession", "(", "session_creator", "=", "sess_creator", ")", "as", "sess", ":", "\n", "            ", "while", "not", "sess", ".", "should_stop", "(", ")", ":", "\n", "                ", "feats", "=", "sess", ".", "run", "(", "features", ")", "\n", "outputs", "=", "sess", ".", "run", "(", "predictions", ",", "feed_dict", "=", "{", "\n", "placeholders", "[", "\"source\"", "]", ":", "feats", "[", "\"source\"", "]", ",", "\n", "placeholders", "[", "\"source_length\"", "]", ":", "feats", "[", "\"source_length\"", "]", "\n", "}", ")", "\n", "# shape: [batch, len]", "\n", "outputs", "=", "outputs", ".", "tolist", "(", ")", "\n", "# shape: ([batch, len], ..., [batch, len])", "\n", "references", "=", "[", "item", ".", "tolist", "(", ")", "for", "item", "in", "feats", "[", "\"references\"", "]", "]", "\n", "\n", "all_outputs", ".", "extend", "(", "outputs", ")", "\n", "\n", "for", "i", "in", "range", "(", "len", "(", "refs", ")", ")", ":", "\n", "                    ", "all_refs", "[", "i", "]", ".", "extend", "(", "references", "[", "i", "]", ")", "\n", "\n", "", "", "", "decoded_symbols", "=", "decode_fn", "(", "all_outputs", ")", "\n", "\n", "for", "i", ",", "l", "in", "enumerate", "(", "decoded_symbols", ")", ":", "\n", "            ", "decoded_symbols", "[", "i", "]", "=", "\" \"", ".", "join", "(", "l", ")", ".", "replace", "(", "\"@@ \"", ",", "\"\"", ")", ".", "split", "(", ")", "\n", "\n", "", "decoded_refs", "=", "[", "decode_fn", "(", "refs", ")", "for", "refs", "in", "all_refs", "]", "\n", "decoded_refs", "=", "[", "list", "(", "x", ")", "for", "x", "in", "zip", "(", "*", "decoded_refs", ")", "]", "\n", "\n", "return", "bleu", ".", "bleu", "(", "decoded_symbols", ",", "decoded_refs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.radam.RAdamOptimizer.__init__": [[27, 90], ["tensorflow.python.training.optimizer.Optimizer.__init__", "float"], "methods", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.transformer_lrp.TransformerLRP.__init__"], ["def", "__init__", "(", "self", ",", "\n", "learning_rate", "=", "0.001", ",", "\n", "beta1", "=", "0.9", ",", "\n", "beta2", "=", "0.999", ",", "\n", "epsilon", "=", "1e-7", ",", "\n", "weight_decay", "=", "0.", ",", "\n", "amsgrad", "=", "False", ",", "\n", "total_steps", "=", "0", ",", "\n", "warmup_proportion", "=", "0.1", ",", "\n", "min_lr", "=", "0.", ",", "\n", "use_locking", "=", "False", ",", "\n", "name", "=", "\"RAdam\"", ")", ":", "\n", "        ", "r\"\"\"Construct a new Adam optimizer.\n        Args:\n            learning_rate: A Tensor or a floating point value.    The learning rate.\n            beta1: A float value or a constant float tensor. The exponential decay\n                rate for the 1st moment estimates.\n            beta2: A float value or a constant float tensor. The exponential decay\n                rate for the 2nd moment estimates.\n            epsilon: A small constant for numerical stability. This epsilon is\n                \"epsilon hat\" in the Kingma and Ba paper (in the formula just before\n                Section 2.1), not the epsilon in Algorithm 1 of the paper.\n            weight_decay: A floating point value. Weight decay for each param.\n            amsgrad: boolean. Whether to apply AMSGrad variant of this algorithm from\n                the paper \"On the Convergence of Adam and beyond\".\n            total_steps: An integer. Total number of training steps.\n                Enable warmup by setting a positive value.\n            warmup_proportion: A floating point value. The proportion of increasing steps.\n            min_lr: A floating point value. Minimum learning rate after warmup.\n            name: Optional name for the operations created when applying gradients.\n                Defaults to \"Adam\".    @compatibility(eager) When eager execution is\n                enabled, `learning_rate`, `beta_1`, `beta_2`, and `epsilon` can each be\n                a callable that takes no arguments and returns the actual value to use.\n                This can be useful for changing these values across different\n                invocations of optimizer functions. @end_compatibility\n            **kwargs: keyword arguments. Allowed to be {`clipnorm`, `clipvalue`, `lr`,\n                `decay`}. `clipnorm` is clip gradients by norm; `clipvalue` is clip\n                gradients by value, `decay` is included for backward compatibility to\n                allow time inverse decay of learning rate. `lr` is included for backward\n                compatibility, recommended to use `learning_rate` instead.\n        \"\"\"", "\n", "super", "(", "RAdamOptimizer", ",", "self", ")", ".", "__init__", "(", "use_locking", ",", "name", ")", "\n", "self", ".", "_lr", "=", "learning_rate", "\n", "self", ".", "_beta1", "=", "beta1", "\n", "self", ".", "_beta2", "=", "beta2", "\n", "self", ".", "_epsilon", "=", "epsilon", "\n", "self", ".", "_weight_decay", "=", "weight_decay", "\n", "self", ".", "_amsgrad", "=", "amsgrad", "\n", "self", ".", "_total_steps", "=", "float", "(", "total_steps", ")", "\n", "self", ".", "_warmup_proportion", "=", "warmup_proportion", "\n", "self", ".", "_min_lr", "=", "min_lr", "\n", "self", ".", "_initial_weight_decay", "=", "weight_decay", "\n", "self", ".", "_initial_total_steps", "=", "total_steps", "\n", "\n", "self", ".", "_lr_t", "=", "None", "\n", "self", ".", "_step_t", "=", "None", "\n", "self", ".", "_beta1_t", "=", "None", "\n", "self", ".", "_beta2_t", "=", "None", "\n", "self", ".", "_epsilon_t", "=", "None", "\n", "self", ".", "_weight_decay_t", "=", "None", "\n", "self", ".", "_total_steps_t", "=", "None", "\n", "self", ".", "_warmup_proportion_t", "=", "None", "\n", "self", ".", "_min_lr_t", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.radam.RAdamOptimizer._get_beta_accumulators": [[91, 100], ["tensorflow.python.framework.ops.init_scope", "tensorflow.python.eager.context.executing_eagerly", "tensorflow.python.framework.ops.get_default_graph", "radam.RAdamOptimizer._get_non_slot_variable", "radam.RAdamOptimizer._get_non_slot_variable", "radam.RAdamOptimizer._get_non_slot_variable"], "methods", ["None"], ["", "def", "_get_beta_accumulators", "(", "self", ")", ":", "\n", "        ", "with", "ops", ".", "init_scope", "(", ")", ":", "\n", "            ", "if", "context", ".", "executing_eagerly", "(", ")", ":", "\n", "                ", "graph", "=", "None", "\n", "", "else", ":", "\n", "                ", "graph", "=", "ops", ".", "get_default_graph", "(", ")", "\n", "", "return", "(", "self", ".", "_get_non_slot_variable", "(", "\"step\"", ",", "graph", "=", "graph", ")", ",", "\n", "self", ".", "_get_non_slot_variable", "(", "\"beta1_power\"", ",", "graph", "=", "graph", ")", ",", "\n", "self", ".", "_get_non_slot_variable", "(", "\"beta2_power\"", ",", "graph", "=", "graph", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.radam.RAdamOptimizer._create_slots": [[101, 111], ["min", "radam.RAdamOptimizer._create_non_slot_variable", "radam.RAdamOptimizer._create_non_slot_variable", "radam.RAdamOptimizer._create_non_slot_variable", "radam.RAdamOptimizer._zeros_slot", "radam.RAdamOptimizer._zeros_slot", "radam.RAdamOptimizer._zeros_slot"], "methods", ["None"], ["", "", "def", "_create_slots", "(", "self", ",", "var_list", ")", ":", "\n", "        ", "first_var", "=", "min", "(", "var_list", ",", "key", "=", "lambda", "x", ":", "x", ".", "name", ")", "\n", "self", ".", "_create_non_slot_variable", "(", "initial_value", "=", "1.0", ",", "name", "=", "\"step\"", ",", "colocate_with", "=", "first_var", ")", "\n", "self", ".", "_create_non_slot_variable", "(", "initial_value", "=", "self", ".", "_beta1", ",", "name", "=", "\"beta1_power\"", ",", "colocate_with", "=", "first_var", ")", "\n", "self", ".", "_create_non_slot_variable", "(", "initial_value", "=", "self", ".", "_beta2", ",", "name", "=", "\"beta2_power\"", ",", "colocate_with", "=", "first_var", ")", "\n", "for", "v", "in", "var_list", ":", "\n", "            ", "self", ".", "_zeros_slot", "(", "v", ",", "\"m\"", ",", "self", ".", "_name", ")", "\n", "self", ".", "_zeros_slot", "(", "v", ",", "\"v\"", ",", "self", ".", "_name", ")", "\n", "if", "self", ".", "_amsgrad", ":", "\n", "                ", "self", ".", "_zeros_slot", "(", "v", ",", "\"vhat\"", ",", "self", ".", "_name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.radam.RAdamOptimizer._prepare": [[112, 130], ["tensorflow.python.framework.ops.convert_to_tensor", "tensorflow.python.framework.ops.convert_to_tensor", "tensorflow.python.framework.ops.convert_to_tensor", "tensorflow.python.framework.ops.convert_to_tensor", "tensorflow.python.framework.ops.convert_to_tensor", "tensorflow.python.framework.ops.convert_to_tensor", "tensorflow.python.framework.ops.convert_to_tensor", "tensorflow.python.framework.ops.convert_to_tensor"], "methods", ["None"], ["", "", "", "def", "_prepare", "(", "self", ")", ":", "\n", "        ", "lr", "=", "self", ".", "_lr", "\n", "beta1", "=", "self", ".", "_beta1", "\n", "beta2", "=", "self", ".", "_beta2", "\n", "epsilon", "=", "self", ".", "_epsilon", "\n", "weight_decay", "=", "self", ".", "_weight_decay", "\n", "total_steps", "=", "self", ".", "_total_steps", "\n", "warmup_proportion", "=", "self", ".", "_warmup_proportion", "\n", "min_lr", "=", "self", ".", "_min_lr", "\n", "\n", "self", ".", "_lr_t", "=", "ops", ".", "convert_to_tensor", "(", "lr", ",", "name", "=", "\"learning_rate\"", ")", "\n", "self", ".", "_beta1_t", "=", "ops", ".", "convert_to_tensor", "(", "beta1", ",", "name", "=", "\"beta1\"", ")", "\n", "self", ".", "_beta2_t", "=", "ops", ".", "convert_to_tensor", "(", "beta2", ",", "name", "=", "\"beta2\"", ")", "\n", "self", ".", "_epsilon_t", "=", "ops", ".", "convert_to_tensor", "(", "epsilon", ",", "name", "=", "\"epsilon\"", ")", "\n", "self", ".", "_weight_decay_t", "=", "ops", ".", "convert_to_tensor", "(", "weight_decay", ",", "name", "=", "\"weight_decay\"", ")", "\n", "self", ".", "_total_steps_t", "=", "ops", ".", "convert_to_tensor", "(", "total_steps", ",", "name", "=", "\"total_steps\"", ")", "\n", "self", ".", "_warmup_proportion_t", "=", "ops", ".", "convert_to_tensor", "(", "warmup_proportion", ",", "name", "=", "\"warmup_proportion\"", ")", "\n", "self", ".", "_min_lr_t", "=", "ops", ".", "convert_to_tensor", "(", "min_lr", ",", "name", "=", "\"min_lr\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.radam.RAdamOptimizer._apply_dense": [[131, 133], ["radam.RAdamOptimizer._resource_apply_dense"], "methods", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.bin.radam.RAdamOptimizer._resource_apply_dense"], ["", "def", "_apply_dense", "(", "self", ",", "grad", ",", "var", ")", ":", "\n", "        ", "return", "self", ".", "_resource_apply_dense", "(", "grad", ",", "var", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.radam.RAdamOptimizer._resource_apply_dense": [[134, 188], ["radam.RAdamOptimizer._get_beta_accumulators", "tensorflow.python.ops.math_ops.cast", "tensorflow.python.ops.math_ops.cast", "tensorflow.python.ops.math_ops.cast", "tensorflow.python.ops.math_ops.cast", "tensorflow.python.ops.math_ops.cast", "tensorflow.python.ops.math_ops.cast", "radam.RAdamOptimizer.get_slot", "tensorflow.python.ops.state_ops.assign", "radam.RAdamOptimizer.get_slot", "tensorflow.python.ops.state_ops.assign", "tensorflow.python.ops.math_ops.sqrt", "tensorflow.where", "tensorflow.python.ops.state_ops.assign_sub", "tensorflow.python.ops.control_flow_ops.group", "tensorflow.python.ops.math_ops.cast", "tensorflow.python.ops.math_ops.cast", "tensorflow.python.ops.math_ops.cast", "tensorflow.python.ops.math_ops.maximum", "tensorflow.where", "radam.RAdamOptimizer.get_slot", "tensorflow.python.ops.state_ops.assign", "tensorflow.python.ops.math_ops.sqrt", "tensorflow.python.ops.math_ops.sqrt", "updates.append", "tensorflow.python.ops.math_ops.maximum", "tensorflow.python.ops.math_ops.cast", "tensorflow.python.ops.math_ops.square", "tensorflow.python.ops.math_ops.minimum"], "methods", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.bin.radam.RAdamOptimizer._get_beta_accumulators"], ["", "def", "_resource_apply_dense", "(", "self", ",", "grad", ",", "var", ")", ":", "\n", "        ", "step", ",", "beta1_power", ",", "beta2_power", "=", "self", ".", "_get_beta_accumulators", "(", ")", "\n", "beta1_power", "=", "math_ops", ".", "cast", "(", "beta1_power", ",", "var", ".", "dtype", ".", "base_dtype", ")", "\n", "beta2_power", "=", "math_ops", ".", "cast", "(", "beta2_power", ",", "var", ".", "dtype", ".", "base_dtype", ")", "\n", "lr_t", "=", "math_ops", ".", "cast", "(", "self", ".", "_lr_t", ",", "var", ".", "dtype", ".", "base_dtype", ")", "\n", "\n", "if", "self", ".", "_initial_total_steps", ">", "0", ":", "\n", "            ", "total_steps", "=", "math_ops", ".", "cast", "(", "self", ".", "_total_steps_t", ",", "var", ".", "dtype", ".", "base_dtype", ")", "\n", "warmup_proportion", "=", "math_ops", ".", "cast", "(", "self", ".", "_warmup_proportion_t", ",", "var", ".", "dtype", ".", "base_dtype", ")", "\n", "min_lr", "=", "math_ops", ".", "cast", "(", "self", ".", "_min_lr_t", ",", "var", ".", "dtype", ".", "base_dtype", ")", "\n", "warmup_steps", "=", "total_steps", "*", "warmup_proportion", "\n", "decay_steps", "=", "math_ops", ".", "maximum", "(", "total_steps", "-", "warmup_steps", ",", "1", ")", "\n", "decay_rate", "=", "(", "min_lr", "-", "lr_t", ")", "/", "decay_steps", "\n", "lr_t", "=", "tf", ".", "where", "(", "\n", "step", "<=", "warmup_steps", ",", "\n", "lr_t", "*", "(", "step", "/", "warmup_steps", ")", ",", "\n", "lr_t", "+", "decay_rate", "*", "math_ops", ".", "minimum", "(", "step", "-", "warmup_steps", ",", "decay_steps", ")", ",", "\n", ")", "\n", "\n", "", "beta1_t", "=", "math_ops", ".", "cast", "(", "self", ".", "_beta1_t", ",", "var", ".", "dtype", ".", "base_dtype", ")", "\n", "beta2_t", "=", "math_ops", ".", "cast", "(", "self", ".", "_beta2_t", ",", "var", ".", "dtype", ".", "base_dtype", ")", "\n", "epsilon_t", "=", "math_ops", ".", "cast", "(", "self", ".", "_epsilon_t", ",", "var", ".", "dtype", ".", "base_dtype", ")", "\n", "\n", "sma_inf", "=", "2.0", "/", "(", "1.0", "-", "beta2_t", ")", "-", "1.0", "\n", "sma_t", "=", "sma_inf", "-", "2.0", "*", "step", "*", "beta2_power", "/", "(", "1.0", "-", "beta2_power", ")", "\n", "\n", "m", "=", "self", ".", "get_slot", "(", "var", ",", "\"m\"", ")", "\n", "m_t", "=", "state_ops", ".", "assign", "(", "m", ",", "beta1_t", "*", "m", "+", "(", "1.0", "-", "beta1_t", ")", "*", "grad", ",", "use_locking", "=", "self", ".", "_use_locking", ")", "\n", "m_corr_t", "=", "m_t", "/", "(", "1.0", "-", "beta1_power", ")", "\n", "\n", "v", "=", "self", ".", "get_slot", "(", "var", ",", "\"v\"", ")", "\n", "v_t", "=", "state_ops", ".", "assign", "(", "v", ",", "beta2_t", "*", "v", "+", "(", "1.0", "-", "beta2_t", ")", "*", "math_ops", ".", "square", "(", "grad", ")", ",", "use_locking", "=", "self", ".", "_use_locking", ")", "\n", "if", "self", ".", "_amsgrad", ":", "\n", "            ", "vhat", "=", "self", ".", "get_slot", "(", "var", ",", "'vhat'", ")", "\n", "vhat_t", "=", "state_ops", ".", "assign", "(", "vhat", ",", "math_ops", ".", "maximum", "(", "vhat", ",", "v_t", ")", ",", "use_locking", "=", "self", ".", "_use_locking", ")", "\n", "v_corr_t", "=", "math_ops", ".", "sqrt", "(", "vhat_t", "/", "(", "1.0", "-", "beta2_power", ")", ")", "\n", "", "else", ":", "\n", "            ", "v_corr_t", "=", "math_ops", ".", "sqrt", "(", "v_t", "/", "(", "1.0", "-", "beta2_power", ")", ")", "\n", "\n", "", "r_t", "=", "math_ops", ".", "sqrt", "(", "(", "sma_t", "-", "4.0", ")", "/", "(", "sma_inf", "-", "4.0", ")", "*", "\n", "(", "sma_t", "-", "2.0", ")", "/", "(", "sma_inf", "-", "2.0", ")", "*", "\n", "sma_inf", "/", "sma_t", ")", "\n", "\n", "var_t", "=", "tf", ".", "where", "(", "sma_t", ">=", "5.0", ",", "r_t", "*", "m_corr_t", "/", "(", "v_corr_t", "+", "epsilon_t", ")", ",", "m_corr_t", ")", "\n", "\n", "if", "self", ".", "_initial_weight_decay", ">", "0.0", ":", "\n", "            ", "var_t", "+=", "math_ops", ".", "cast", "(", "self", ".", "_weight_decay_t", ",", "var", ".", "dtype", ".", "base_dtype", ")", "*", "var", "\n", "\n", "", "var_update", "=", "state_ops", ".", "assign_sub", "(", "var", ",", "lr_t", "*", "var_t", ",", "use_locking", "=", "self", ".", "_use_locking", ")", "\n", "\n", "updates", "=", "[", "var_update", ",", "m_t", ",", "v_t", "]", "\n", "if", "self", ".", "_amsgrad", ":", "\n", "            ", "updates", ".", "append", "(", "vhat_t", ")", "\n", "", "return", "control_flow_ops", ".", "group", "(", "*", "updates", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.radam.RAdamOptimizer._apply_sparse_shared": [[189, 254], ["radam.RAdamOptimizer._get_beta_accumulators", "tensorflow.python.ops.math_ops.cast", "tensorflow.python.ops.math_ops.cast", "tensorflow.python.ops.math_ops.cast", "tensorflow.python.ops.math_ops.cast", "tensorflow.python.ops.math_ops.cast", "tensorflow.python.ops.math_ops.cast", "radam.RAdamOptimizer.get_slot", "tensorflow.python.ops.state_ops.assign", "radam.RAdamOptimizer.get_slot", "tensorflow.python.ops.state_ops.assign", "tensorflow.python.ops.math_ops.sqrt", "tensorflow.where", "tensorflow.python.ops.state_ops.scatter_sub", "tensorflow.python.ops.control_flow_ops.group", "tensorflow.python.ops.math_ops.cast", "tensorflow.python.ops.math_ops.cast", "tensorflow.python.ops.math_ops.cast", "tensorflow.python.ops.math_ops.maximum", "tensorflow.where", "tensorflow.python.framework.ops.control_dependencies", "scatter_add", "tensorflow.python.framework.ops.control_dependencies", "scatter_add", "radam.RAdamOptimizer.get_slot", "tensorflow.python.ops.state_ops.assign", "tensorflow.python.ops.math_ops.sqrt", "tensorflow.python.ops.math_ops.sqrt", "tensorflow.python.ops.array_ops.gather", "updates.append", "tensorflow.python.ops.math_ops.maximum", "tensorflow.python.ops.math_ops.cast", "tensorflow.python.ops.math_ops.minimum"], "methods", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.bin.radam.RAdamOptimizer._get_beta_accumulators"], ["", "def", "_apply_sparse_shared", "(", "self", ",", "grad", ",", "var", ",", "indices", ",", "scatter_add", ")", ":", "\n", "        ", "step", ",", "beta1_power", ",", "beta2_power", "=", "self", ".", "_get_beta_accumulators", "(", ")", "\n", "beta1_power", "=", "math_ops", ".", "cast", "(", "beta1_power", ",", "var", ".", "dtype", ".", "base_dtype", ")", "\n", "beta2_power", "=", "math_ops", ".", "cast", "(", "beta2_power", ",", "var", ".", "dtype", ".", "base_dtype", ")", "\n", "lr_t", "=", "math_ops", ".", "cast", "(", "self", ".", "_lr_t", ",", "var", ".", "dtype", ".", "base_dtype", ")", "\n", "\n", "if", "self", ".", "_initial_total_steps", ">", "0", ":", "\n", "            ", "total_steps", "=", "math_ops", ".", "cast", "(", "self", ".", "_total_steps_t", ",", "var", ".", "dtype", ".", "base_dtype", ")", "\n", "warmup_proportion", "=", "math_ops", ".", "cast", "(", "self", ".", "_warmup_proportion_t", ",", "var", ".", "dtype", ".", "base_dtype", ")", "\n", "min_lr", "=", "math_ops", ".", "cast", "(", "self", ".", "_min_lr_t", ",", "var", ".", "dtype", ".", "base_dtype", ")", "\n", "warmup_steps", "=", "total_steps", "*", "warmup_proportion", "\n", "decay_steps", "=", "math_ops", ".", "maximum", "(", "total_steps", "-", "warmup_steps", ",", "1", ")", "\n", "decay_rate", "=", "(", "min_lr", "-", "lr_t", ")", "/", "decay_steps", "\n", "lr_t", "=", "tf", ".", "where", "(", "\n", "step", "<=", "warmup_steps", ",", "\n", "lr_t", "*", "(", "step", "/", "warmup_steps", ")", ",", "\n", "lr_t", "+", "decay_rate", "*", "math_ops", ".", "minimum", "(", "step", "-", "warmup_steps", ",", "decay_steps", ")", ",", "\n", ")", "\n", "\n", "", "beta1_t", "=", "math_ops", ".", "cast", "(", "self", ".", "_beta1_t", ",", "var", ".", "dtype", ".", "base_dtype", ")", "\n", "beta2_t", "=", "math_ops", ".", "cast", "(", "self", ".", "_beta2_t", ",", "var", ".", "dtype", ".", "base_dtype", ")", "\n", "epsilon_t", "=", "math_ops", ".", "cast", "(", "self", ".", "_epsilon_t", ",", "var", ".", "dtype", ".", "base_dtype", ")", "\n", "\n", "sma_inf", "=", "2.0", "/", "(", "1.0", "-", "beta2_t", ")", "-", "1.0", "\n", "sma_t", "=", "sma_inf", "-", "2.0", "*", "step", "*", "beta2_power", "/", "(", "1.0", "-", "beta2_power", ")", "\n", "\n", "m", "=", "self", ".", "get_slot", "(", "var", ",", "\"m\"", ")", "\n", "m_scaled_g_values", "=", "grad", "*", "(", "1", "-", "beta1_t", ")", "\n", "m_t", "=", "state_ops", ".", "assign", "(", "m", ",", "m", "*", "beta1_t", ",", "use_locking", "=", "self", ".", "_use_locking", ")", "\n", "with", "ops", ".", "control_dependencies", "(", "[", "m_t", "]", ")", ":", "\n", "            ", "m_t", "=", "scatter_add", "(", "m", ",", "indices", ",", "m_scaled_g_values", ")", "\n", "", "m_corr_t", "=", "m_t", "/", "(", "1.0", "-", "beta1_power", ")", "\n", "\n", "v", "=", "self", ".", "get_slot", "(", "var", ",", "\"v\"", ")", "\n", "v_scaled_g_values", "=", "(", "grad", "*", "grad", ")", "*", "(", "1", "-", "beta2_t", ")", "\n", "v_t", "=", "state_ops", ".", "assign", "(", "v", ",", "v", "*", "beta2_t", ",", "use_locking", "=", "self", ".", "_use_locking", ")", "\n", "with", "ops", ".", "control_dependencies", "(", "[", "v_t", "]", ")", ":", "\n", "            ", "v_t", "=", "scatter_add", "(", "v", ",", "indices", ",", "v_scaled_g_values", ")", "\n", "", "if", "self", ".", "_amsgrad", ":", "\n", "            ", "vhat", "=", "self", ".", "get_slot", "(", "var", ",", "'vhat'", ")", "\n", "vhat_t", "=", "state_ops", ".", "assign", "(", "vhat", ",", "math_ops", ".", "maximum", "(", "vhat", ",", "v_t", ")", ",", "use_locking", "=", "self", ".", "_use_locking", ")", "\n", "v_corr_t", "=", "math_ops", ".", "sqrt", "(", "vhat_t", "/", "(", "1.0", "-", "beta2_power", ")", ")", "\n", "", "else", ":", "\n", "            ", "v_corr_t", "=", "math_ops", ".", "sqrt", "(", "v_t", "/", "(", "1.0", "-", "beta2_power", ")", ")", "\n", "\n", "", "r_t", "=", "math_ops", ".", "sqrt", "(", "(", "sma_t", "-", "4.0", ")", "/", "(", "sma_inf", "-", "4.0", ")", "*", "\n", "(", "sma_t", "-", "2.0", ")", "/", "(", "sma_inf", "-", "2.0", ")", "*", "\n", "sma_inf", "/", "sma_t", ")", "\n", "\n", "var_t", "=", "tf", ".", "where", "(", "sma_t", ">=", "5.0", ",", "r_t", "*", "m_corr_t", "/", "(", "v_corr_t", "+", "epsilon_t", ")", ",", "m_corr_t", ")", "\n", "\n", "if", "self", ".", "_initial_weight_decay", ">", "0.0", ":", "\n", "            ", "var_t", "+=", "math_ops", ".", "cast", "(", "self", ".", "_weight_decay_t", ",", "var", ".", "dtype", ".", "base_dtype", ")", "*", "var", "\n", "\n", "", "var_t", "=", "lr_t", "*", "var_t", "\n", "var_update", "=", "state_ops", ".", "scatter_sub", "(", "\n", "var", ",", "\n", "indices", ",", "\n", "array_ops", ".", "gather", "(", "var_t", ",", "indices", ")", ",", "\n", "use_locking", "=", "self", ".", "_use_locking", ")", "\n", "\n", "updates", "=", "[", "var_update", ",", "m_t", ",", "v_t", "]", "\n", "if", "self", ".", "_amsgrad", ":", "\n", "            ", "updates", ".", "append", "(", "vhat_t", ")", "\n", "", "return", "control_flow_ops", ".", "group", "(", "*", "updates", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.radam.RAdamOptimizer._apply_sparse": [[255, 261], ["radam.RAdamOptimizer._apply_sparse_shared", "tensorflow.python.ops.state_ops.scatter_add"], "methods", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.bin.radam.RAdamOptimizer._apply_sparse_shared"], ["", "def", "_apply_sparse", "(", "self", ",", "grad", ",", "var", ")", ":", "\n", "        ", "return", "self", ".", "_apply_sparse_shared", "(", "\n", "grad", ".", "values", ",", "\n", "var", ",", "\n", "grad", ".", "indices", ",", "\n", "lambda", "x", ",", "i", ",", "v", ":", "state_ops", ".", "scatter_add", "(", "x", ",", "i", ",", "v", ",", "use_locking", "=", "self", ".", "_use_locking", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.radam.RAdamOptimizer._resource_scatter_add": [[262, 265], ["tensorflow.python.framework.ops.control_dependencies", "x.value", "tensorflow.python.ops.resource_variable_ops.resource_scatter_add"], "methods", ["None"], ["", "def", "_resource_scatter_add", "(", "self", ",", "x", ",", "i", ",", "v", ")", ":", "\n", "        ", "with", "ops", ".", "control_dependencies", "(", "[", "resource_variable_ops", ".", "resource_scatter_add", "(", "x", ".", "handle", ",", "i", ",", "v", ")", "]", ")", ":", "\n", "            ", "return", "x", ".", "value", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.radam.RAdamOptimizer._resource_apply_sparse": [[266, 268], ["radam.RAdamOptimizer._apply_sparse_shared"], "methods", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.bin.radam.RAdamOptimizer._apply_sparse_shared"], ["", "", "def", "_resource_apply_sparse", "(", "self", ",", "grad", ",", "var", ",", "indices", ")", ":", "\n", "        ", "return", "self", ".", "_apply_sparse_shared", "(", "grad", ",", "var", ",", "indices", ",", "self", ".", "_resource_scatter_add", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.radam.RAdamOptimizer._finish": [[269, 277], ["tensorflow.python.ops.control_flow_ops.group", "tensorflow.python.framework.ops.control_dependencies", "radam.RAdamOptimizer._get_beta_accumulators", "tensorflow.python.framework.ops.colocate_with", "step.assign", "beta1_power.assign", "beta2_power.assign"], "methods", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.bin.radam.RAdamOptimizer._get_beta_accumulators"], ["", "def", "_finish", "(", "self", ",", "update_ops", ",", "name_scope", ")", ":", "\n", "        ", "with", "ops", ".", "control_dependencies", "(", "update_ops", ")", ":", "\n", "            ", "step", ",", "beta1_power", ",", "beta2_power", "=", "self", ".", "_get_beta_accumulators", "(", ")", "\n", "with", "ops", ".", "colocate_with", "(", "beta1_power", ")", ":", "\n", "                ", "update_step", "=", "step", ".", "assign", "(", "step", "+", "1.0", ",", "use_locking", "=", "self", ".", "_use_locking", ")", "\n", "update_beta1", "=", "beta1_power", ".", "assign", "(", "beta1_power", "*", "self", ".", "_beta1_t", ",", "use_locking", "=", "self", ".", "_use_locking", ")", "\n", "update_beta2", "=", "beta2_power", ".", "assign", "(", "beta2_power", "*", "self", ".", "_beta2_t", ",", "use_locking", "=", "self", ".", "_use_locking", ")", "\n", "", "", "return", "control_flow_ops", ".", "group", "(", "*", "update_ops", "+", "[", "update_step", ",", "update_beta1", ",", "update_beta2", "]", ",", "name", "=", "name_scope", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.expert_utils.NoisyTopKGatingParams.__init__": [[40, 49], ["tensorflow.zeros_initializer"], "methods", ["None"], ["  ", "def", "__init__", "(", "self", ")", ":", "\n", "    ", "self", ".", "gating_class", "=", "NoisyTopKGating", "\n", "self", ".", "num_experts", "=", "16", "# The number of experts", "\n", "self", ".", "k", "=", "2", "# 'The number of experts to use per example", "\n", "self", ".", "input_size", "=", "None", "# size of input to MoE.  Set by MoE class", "\n", "self", ".", "dtype", "=", "tf", ".", "float32", "# floating point data type", "\n", "self", ".", "initializer", "=", "tf", ".", "zeros_initializer", "(", ")", "# initializer for weight matrices", "\n", "self", ".", "noisy_gating", "=", "True", "# Add tunable noise (necessary for load-balancing)", "\n", "self", ".", "noise_epsilon", "=", "1e-2", "# Added to noise stddev for numerical stability", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.expert_utils.FeedForwardExpertParams.__init__": [[53, 73], ["None"], "methods", ["None"], ["  ", "def", "__init__", "(", "self", ")", ":", "\n", "\n", "    ", "self", ".", "relu_dropout", "=", "0.2", "\n", "\n", "# The class that implements the expert network", "\n", "self", ".", "expert_class", "=", "FeedForwardExpert", "\n", "self", ".", "input_size", "=", "None", "# Size of input to MoE.  Set by MoE class.", "\n", "# List of hidden layer sizes, or None for no hidden layers.", "\n", "# The length of this list determines the number of hidden layers", "\n", "self", ".", "hidden_layer_sizes", "=", "None", "\n", "self", ".", "output_size", "=", "None", "# Size of output from MoE.  Set by MoE class.", "\n", "self", ".", "dtype", "=", "tf", ".", "float32", "# Floating point data type)", "\n", "# Activation function applied at each hidden layer)", "\n", "self", ".", "hidden_activation", "=", "tf", ".", "nn", ".", "relu6", "\n", "self", ".", "output_activation", "=", "None", "\n", "self", ".", "initializer", "=", "None", "# Optional initializer for weight matrices.)", "\n", "# If autoscale = True, At each hidden/output layer, multiply by", "\n", "# rsqrt(prev_layer_size / input_size).  This scaling happens", "\n", "# before application of hidden_activation)", "\n", "self", ".", "autoscale", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.expert_utils.FeedForwardExpert.__init__": [[102, 120], ["range", "len", "expert_utils.FeedForwardExpert._w.append", "tensorflow.get_variable"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "hp", ",", "name", ")", ":", "\n", "    ", "\"\"\"Creates a FeedForwardExpert.\n\n    Args:\n      hp: hyperparameters.  Call FeedForwardExpertParams() to create these.\n      name: a string.\n    \"\"\"", "\n", "self", ".", "_hp", "=", "hp", "\n", "hidden_layer_sizes", "=", "hp", ".", "hidden_layer_sizes", "or", "[", "]", "\n", "num_layers", "=", "1", "+", "len", "(", "hidden_layer_sizes", ")", "\n", "layer_sizes", "=", "[", "hp", ".", "input_size", "]", "+", "hidden_layer_sizes", "+", "[", "hp", ".", "output_size", "]", "\n", "self", ".", "_layer_sizes", "=", "layer_sizes", "\n", "self", ".", "_w", "=", "[", "]", "\n", "for", "layer", "in", "range", "(", "num_layers", ")", ":", "\n", "      ", "shape", "=", "layer_sizes", "[", "layer", ":", "layer", "+", "2", "]", "\n", "self", ".", "_w", ".", "append", "(", "\n", "tf", ".", "get_variable", "(", "'%s_layer_%d'", "%", "(", "name", ",", "layer", ")", ",", "shape", ",", "hp", ".", "dtype", ",", "\n", "hp", ".", "initializer", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.expert_utils.FeedForwardExpert.Eval": [[121, 144], ["len", "six.moves.xrange", "tensorflow.matmul", "tensorflow.clip_by_value", "hp.output_activation", "hp.hidden_activation", "tensorflow.nn.dropout"], "methods", ["None"], ["", "", "def", "Eval", "(", "self", ",", "x", ")", ":", "\n", "    ", "\"\"\"Evaluate the FeedForwardExpert on the given input.\n\n    Args:\n      x: a `Tensor` of shape `[batch_size, hp.input_size]`\n\n    Returns:\n      a `Tensor` of shape `[batch_size, hp.output_size]`\n    \"\"\"", "\n", "hp", "=", "self", ".", "_hp", "\n", "num_layers", "=", "len", "(", "self", ".", "_w", ")", "\n", "for", "i", "in", "xrange", "(", "num_layers", ")", ":", "\n", "      ", "x", "=", "tf", ".", "matmul", "(", "x", ",", "self", ".", "_w", "[", "i", "]", ")", "\n", "if", "hp", ".", "autoscale", "and", "self", ".", "_layer_sizes", "[", "i", "]", "!=", "hp", ".", "input_size", ":", "\n", "        ", "x", "*=", "(", "self", ".", "_layer_sizes", "[", "i", "]", "/", "hp", ".", "input_size", ")", "**", "-", "0.5", "\n", "", "if", "i", "+", "1", "<", "num_layers", "and", "hp", ".", "hidden_activation", ":", "\n", "        ", "x", "=", "hp", ".", "hidden_activation", "(", "x", ")", "\n", "x", "=", "tf", ".", "nn", ".", "dropout", "(", "x", ",", "1.0", "-", "hp", ".", "relu_dropout", ")", "\n", "", "", "if", "hp", ".", "output_activation", "is", "None", ":", "\n", "      ", "x", "=", "tf", ".", "clip_by_value", "(", "x", ",", "-", "6.0", ",", "6.0", ")", "\n", "", "else", ":", "\n", "      ", "x", "=", "hp", ".", "output_activation", "(", "x", ")", "\n", "", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.expert_utils.FeedForwardExpert.vars": [[145, 148], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "vars", "(", "self", ")", ":", "\n", "    ", "return", "self", ".", "_w", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.expert_utils.Parallelism.__init__": [[191, 217], ["len", "expert_utils.Parallelism._MaybeRepeat"], "methods", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.expert_utils.Parallelism._MaybeRepeat"], ["def", "__init__", "(", "self", ",", "\n", "device_names_or_functions", ",", "\n", "reuse", "=", "None", ",", "\n", "caching_devices", "=", "None", ",", "\n", "daisy_chain_variables", "=", "False", ")", ":", "\n", "    ", "\"\"\"Create a Parallelism.\n\n    Args:\n      device_names_or_functions: A list of of length n, containing device names\n        or device functions (see `tf.device`)\n      reuse: True or None.  Whether to reuse variables created in the first\n        replica in the subsequent replicas.\n      caching_devices: Either `None`, or a list of length n containing device\n        names.\n      daisy_chain_variables: a boolean - if true, then copies variables in a\n        daisy chain between devices.\n\n    Returns:\n      a Parallelism.\n    \"\"\"", "\n", "assert", "device_names_or_functions", "\n", "self", ".", "_devices", "=", "device_names_or_functions", "\n", "self", ".", "_n", "=", "len", "(", "device_names_or_functions", ")", "\n", "self", ".", "_reuse", "=", "reuse", "\n", "self", ".", "_caching_devices", "=", "self", ".", "_MaybeRepeat", "(", "caching_devices", ")", "\n", "self", ".", "_daisy_chain_variables", "=", "daisy_chain_variables", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.expert_utils.Parallelism.__call__": [[218, 303], ["six.iteritems", "expert_utils.Parallelism._MaybeRepeat", "six.moves.xrange", "isinstance", "expert_utils.TransposeListOfLists", "expert_utils.Parallelism._MaybeRepeat", "six.moves.xrange", "list", "tuple", "six.moves.xrange", "getter", "six.moves.zip", "expert_utils.Parallelism._MaybeRepeat", "six.moves.xrange", "tensorflow.identity", "getter", "tensorflow.identity", "tensorflow.device", "tensorflow.identity", "tensorflow.name_scope", "tensorflow.device", "tuple.append", "list", "getter._ref", "tensorflow.identity._ref", "tensorflow.variable_scope", "tensorflow.get_variable_scope", "tensorflow.device", "tuple.append"], "methods", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.expert_utils.Parallelism._MaybeRepeat", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.expert_utils.TransposeListOfLists", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.expert_utils.Parallelism._MaybeRepeat", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.expert_utils.Parallelism._MaybeRepeat"], ["", "def", "__call__", "(", "self", ",", "fn", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"A parallel set of function calls (using the specified devices).\n\n    Args:\n      fn: a function or a list of n functions.\n      *args: additional args.  Each arg should either be not a list, or a list\n         of length n.\n      **kwargs: additional keyword args.  Each arg should either be not a\n         list, or a list of length n.\n\n    Returns:\n      either a single list of length n (if fn does not return a tuple), or a\n      tuple of lists of length n (if fn returns a tuple).\n    \"\"\"", "\n", "# Construct lists or args and kwargs for each function.", "\n", "if", "args", ":", "\n", "      ", "my_args", "=", "TransposeListOfLists", "(", "[", "self", ".", "_MaybeRepeat", "(", "arg", ")", "for", "arg", "in", "args", "]", ")", "\n", "", "else", ":", "\n", "      ", "my_args", "=", "[", "[", "]", "for", "_", "in", "xrange", "(", "self", ".", "n", ")", "]", "\n", "", "my_kwargs", "=", "[", "{", "}", "for", "_", "in", "xrange", "(", "self", ".", "n", ")", "]", "\n", "for", "k", ",", "v", "in", "six", ".", "iteritems", "(", "kwargs", ")", ":", "\n", "      ", "vals", "=", "self", ".", "_MaybeRepeat", "(", "v", ")", "\n", "for", "i", "in", "xrange", "(", "self", ".", "n", ")", ":", "\n", "        ", "my_kwargs", "[", "i", "]", "[", "k", "]", "=", "vals", "[", "i", "]", "\n", "\n", "# Construct lists of functions.", "\n", "", "", "fns", "=", "self", ".", "_MaybeRepeat", "(", "fn", ")", "\n", "\n", "# Now make the parallel call.", "\n", "outputs", "=", "[", "]", "\n", "cache", "=", "{", "}", "\n", "for", "i", "in", "xrange", "(", "self", ".", "n", ")", ":", "\n", "      ", "def", "DaisyChainGetter", "(", "getter", ",", "name", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"Get a variable and cache in a daisy chain.\"\"\"", "\n", "device_var_key", "=", "(", "self", ".", "_devices", "[", "i", "]", ",", "name", ")", "\n", "if", "device_var_key", "in", "cache", ":", "\n", "# if we have the variable on the correct device, return it.", "\n", "          ", "return", "cache", "[", "device_var_key", "]", "\n", "", "if", "name", "in", "cache", ":", "\n", "# if we have it on a different device, copy it from the last device", "\n", "          ", "v", "=", "tf", ".", "identity", "(", "cache", "[", "name", "]", ")", "\n", "", "else", ":", "\n", "          ", "var", "=", "getter", "(", "name", ",", "*", "args", ",", "**", "kwargs", ")", "\n", "v", "=", "tf", ".", "identity", "(", "var", ".", "_ref", "(", ")", ")", "# pylint: disable=protected-access", "\n", "# update the cache", "\n", "", "cache", "[", "name", "]", "=", "v", "\n", "cache", "[", "device_var_key", "]", "=", "v", "\n", "return", "v", "\n", "\n", "# Variable scope will not reset caching_device on reused variables,", "\n", "# so we make a custom getter that uses identity to cache the variable.", "\n", "# pylint: disable=cell-var-from-loop", "\n", "", "def", "CachingGetter", "(", "getter", ",", "name", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "v", "=", "getter", "(", "name", ",", "*", "args", ",", "**", "kwargs", ")", "\n", "key", "=", "(", "self", ".", "_caching_devices", "[", "i", "]", ",", "name", ")", "\n", "if", "key", "in", "cache", ":", "\n", "          ", "return", "cache", "[", "key", "]", "\n", "", "with", "tf", ".", "device", "(", "self", ".", "_caching_devices", "[", "i", "]", ")", ":", "\n", "          ", "ret", "=", "tf", ".", "identity", "(", "v", ".", "_ref", "(", ")", ")", "# pylint: disable=protected-access", "\n", "", "cache", "[", "key", "]", "=", "ret", "\n", "return", "ret", "\n", "\n", "", "if", "self", ".", "_daisy_chain_variables", ":", "\n", "        ", "custom_getter", "=", "DaisyChainGetter", "\n", "", "elif", "self", ".", "_caching_devices", "[", "i", "]", ":", "\n", "        ", "custom_getter", "=", "CachingGetter", "\n", "", "else", ":", "\n", "        ", "custom_getter", "=", "None", "\n", "# pylint: enable=cell-var-from-loop", "\n", "", "if", "self", ".", "n", ">", "1", ":", "\n", "        ", "with", "tf", ".", "name_scope", "(", "'parallel_%d'", "%", "i", ")", ":", "\n", "          ", "with", "tf", ".", "variable_scope", "(", "\n", "tf", ".", "get_variable_scope", "(", ")", ",", "\n", "reuse", "=", "True", "if", "i", ">", "0", "and", "self", ".", "_reuse", "else", "None", ",", "\n", "caching_device", "=", "self", ".", "_caching_devices", "[", "i", "]", ",", "\n", "custom_getter", "=", "custom_getter", ")", ":", "\n", "            ", "with", "tf", ".", "device", "(", "self", ".", "_devices", "[", "i", "]", ")", ":", "\n", "              ", "outputs", ".", "append", "(", "fns", "[", "i", "]", "(", "*", "my_args", "[", "i", "]", ",", "**", "my_kwargs", "[", "i", "]", ")", ")", "\n", "", "", "", "", "else", ":", "\n", "        ", "with", "tf", ".", "device", "(", "self", ".", "_devices", "[", "i", "]", ")", ":", "\n", "          ", "outputs", ".", "append", "(", "fns", "[", "i", "]", "(", "*", "my_args", "[", "i", "]", ",", "**", "my_kwargs", "[", "i", "]", ")", ")", "\n", "", "", "", "if", "isinstance", "(", "outputs", "[", "0", "]", ",", "tuple", ")", ":", "\n", "      ", "outputs", "=", "list", "(", "zip", "(", "*", "outputs", ")", ")", "\n", "outputs", "=", "tuple", "(", "[", "list", "(", "o", ")", "for", "o", "in", "outputs", "]", ")", "\n", "", "return", "outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.expert_utils.Parallelism.n": [[304, 307], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "n", "(", "self", ")", ":", "\n", "    ", "return", "self", ".", "_n", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.expert_utils.Parallelism.devices": [[308, 311], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "devices", "(", "self", ")", ":", "\n", "    ", "return", "self", ".", "_devices", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.expert_utils.Parallelism._MaybeRepeat": [[312, 326], ["isinstance", "len"], "methods", ["None"], ["", "def", "_MaybeRepeat", "(", "self", ",", "x", ")", ":", "\n", "    ", "\"\"\"Utility function for processing arguments that are singletons or lists.\n\n    Args:\n      x: either a list of self.n elements, or not a list.\n\n    Returns:\n      a list of self.n elements.\n    \"\"\"", "\n", "if", "isinstance", "(", "x", ",", "list", ")", ":", "\n", "      ", "assert", "len", "(", "x", ")", "==", "self", ".", "n", "\n", "return", "x", "\n", "", "else", ":", "\n", "      ", "return", "[", "x", "]", "*", "self", ".", "n", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.expert_utils.NoisyTopKGating.__init__": [[521, 539], ["tensorflow.get_variable", "expert_utils.NoisyTopKGating._vars.append", "tensorflow.get_variable", "expert_utils.NoisyTopKGating._vars.append"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "hp", ",", "name", ")", ":", "\n", "    ", "\"\"\"Create a NoisyTopKGating network.\n\n    Args:\n      hp: a hyperparameters created by NoisyTopKGatingParams()\n      name: a string\n    \"\"\"", "\n", "self", ".", "_vars", "=", "[", "]", "\n", "self", ".", "_hp", "=", "hp", "\n", "self", ".", "_w_gate", "=", "tf", ".", "get_variable", "(", "'%s_gate'", "%", "name", ",", "\n", "[", "hp", ".", "input_size", ",", "\n", "hp", ".", "num_experts", "]", ",", "hp", ".", "dtype", ",", "hp", ".", "initializer", ")", "\n", "self", ".", "_vars", ".", "append", "(", "self", ".", "_w_gate", ")", "\n", "if", "hp", ".", "noisy_gating", ":", "\n", "      ", "self", ".", "_w_noise", "=", "tf", ".", "get_variable", "(", "'%s_noise'", "%", "name", ",", "\n", "[", "hp", ".", "input_size", ",", "hp", ".", "num_experts", "]", ",", "hp", ".", "dtype", ",", "\n", "hp", ".", "initializer", ")", "\n", "self", ".", "_vars", ".", "append", "(", "self", ".", "_w_noise", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.expert_utils.NoisyTopKGating.Eval": [[540, 585], ["tensorflow.variable_scope", "tensorflow.matmul", "expert_utils._MyTopK", "tensorflow.slice", "tensorflow.slice", "tensorflow.nn.softmax", "expert_utils._RowwiseUnsortedSegmentSum", "tensorflow.matmul", "min", "tensorflow.reduce_sum", "expert_utils._GatesToLoad", "tensorflow.summary.histogram", "tensorflow.summary.histogram", "tensorflow.nn.softplus", "tensorflow.summary.histogram", "tensorflow.summary.histogram", "expert_utils._ProbInTopK", "tensorflow.reduce_sum", "tensorflow.random_normal", "tensorflow.shape"], "methods", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.expert_utils._MyTopK", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.expert_utils._RowwiseUnsortedSegmentSum", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.expert_utils._GatesToLoad", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.expert_utils._ProbInTopK"], ["", "", "def", "Eval", "(", "self", ",", "x", ",", "train", "=", "True", ",", "summaries", "=", "False", ")", ":", "\n", "    ", "\"\"\"Compute noisy top-k gating.\n\n    Args:\n      x: a `Tensor` of shape `[batch_size, input_size]`.\n      train: a boolean `Scalar`.   Setting this to false turns off noise.\n      summaries: a boolean.  Whether to add summaries.\n    Returns:\n      gates: a `Tensor` of shape `[batch_size, n]`\n      load: a `Tensor` of shape `[n]`.\n        If we are using noise, this is a smooth approximation of the load,\n        and you can define a loss in terms of it to help with load-balancing.\n    \"\"\"", "\n", "with", "tf", ".", "variable_scope", "(", "'NoisyTopKGating'", ")", ":", "\n", "      ", "hp", "=", "self", ".", "_hp", "\n", "clean_logits", "=", "tf", ".", "matmul", "(", "x", ",", "self", ".", "_w_gate", ")", "\n", "if", "hp", ".", "noisy_gating", "and", "train", ":", "\n", "        ", "raw_noise_stddev", "=", "tf", ".", "matmul", "(", "x", ",", "self", ".", "_w_noise", ")", "\n", "noise_stddev", "=", "tf", ".", "nn", ".", "softplus", "(", "raw_noise_stddev", ")", "+", "hp", ".", "noise_epsilon", "\n", "noisy_logits", "=", "clean_logits", "+", "(", "\n", "tf", ".", "random_normal", "(", "tf", ".", "shape", "(", "clean_logits", ")", ")", "*", "noise_stddev", ")", "\n", "logits", "=", "noisy_logits", "\n", "if", "summaries", ":", "\n", "          ", "tf", ".", "summary", ".", "histogram", "(", "'noisy_logits'", ",", "noisy_logits", ")", "\n", "tf", ".", "summary", ".", "histogram", "(", "'noise_stddev'", ",", "noise_stddev", ")", "\n", "", "", "else", ":", "\n", "        ", "logits", "=", "clean_logits", "\n", "", "top_logits", ",", "top_indices", "=", "_MyTopK", "(", "logits", ",", "min", "(", "hp", ".", "k", "+", "1", ",", "hp", ".", "num_experts", ")", ")", "\n", "top_k_logits", "=", "tf", ".", "slice", "(", "top_logits", ",", "[", "0", ",", "0", "]", ",", "[", "-", "1", ",", "hp", ".", "k", "]", ")", "\n", "top_k_indices", "=", "tf", ".", "slice", "(", "top_indices", ",", "[", "0", ",", "0", "]", ",", "[", "-", "1", ",", "hp", ".", "k", "]", ")", "\n", "top_k_gates", "=", "tf", ".", "nn", ".", "softmax", "(", "top_k_logits", ")", "\n", "# This will be a `Tensor` of shape `[batch_size, n]`, with zeros in the", "\n", "# positions corresponding to all but the top k experts per example.", "\n", "gates", "=", "_RowwiseUnsortedSegmentSum", "(", "top_k_gates", ",", "top_k_indices", ",", "\n", "hp", ".", "num_experts", ")", "\n", "if", "hp", ".", "noisy_gating", "and", "train", "and", "hp", ".", "k", "<", "hp", ".", "num_experts", ":", "\n", "        ", "load", "=", "tf", ".", "reduce_sum", "(", "\n", "_ProbInTopK", "(", "clean_logits", ",", "noisy_logits", ",", "noise_stddev", ",", "top_logits", ",", "\n", "hp", ".", "k", ")", ",", "0", ")", "\n", "", "else", ":", "\n", "        ", "load", "=", "_GatesToLoad", "(", "gates", ")", "\n", "", "if", "summaries", ":", "\n", "        ", "tf", ".", "summary", ".", "histogram", "(", "'importance'", ",", "tf", ".", "reduce_sum", "(", "gates", ",", "0", ")", ")", "\n", "tf", ".", "summary", ".", "histogram", "(", "'load'", ",", "load", ")", "\n", "", "return", "gates", ",", "load", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.expert_utils.NoisyTopKGating.vars": [[586, 589], ["None"], "methods", ["None"], ["", "", "@", "property", "\n", "def", "vars", "(", "self", ")", ":", "\n", "    ", "return", "self", ".", "_vars", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.expert_utils.LocalMixtureOfExperts.__init__": [[595, 616], ["expert_utils._SetInputOutputSizes", "expert_utils._SetInputOutputSizes", "gating_hp.gating_class", "expert_hp.expert_class", "six.moves.xrange"], "methods", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.expert_utils._SetInputOutputSizes", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.expert_utils._SetInputOutputSizes"], ["def", "__init__", "(", "self", ",", "gating_hp", ",", "expert_hp", ",", "input_size", ",", "output_size", ",", "name", ")", ":", "\n", "    ", "\"\"\"Create a LocalMixtureOfExperts.\n\n    Args:\n      gating_hp: hyperparameters for the gating network.\n        e.g. NoisyTopKGatingParams()\n      expert_hp: hyperparameters for the expert networks.\n        e.g. FeedForwardExpertParams()\n      input_size: an integer.\n      output_size: an integer.\n      name: a string.\n    \"\"\"", "\n", "self", ".", "_name", "=", "name", "\n", "_SetInputOutputSizes", "(", "gating_hp", ",", "input_size", ",", "None", ")", "\n", "_SetInputOutputSizes", "(", "expert_hp", ",", "input_size", ",", "output_size", ")", "\n", "self", ".", "_gating_hp", "=", "gating_hp", "\n", "self", ".", "_gating", "=", "gating_hp", ".", "gating_class", "(", "gating_hp", ",", "name", "+", "'_gating'", ")", "\n", "self", ".", "_expert_hp", "=", "expert_hp", "\n", "self", ".", "_experts", "=", "[", "\n", "expert_hp", ".", "expert_class", "(", "expert_hp", ",", "name", "+", "'_%d'", "%", "i", ")", "\n", "for", "i", "in", "xrange", "(", "gating_hp", ".", "num_experts", ")", "\n", "]", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.expert_utils.LocalMixtureOfExperts.Eval": [[618, 680], ["expert_utils.LocalMixtureOfExperts._gating.Eval", "expert_utils.SparseDispatcher", "expert_utils.SparseDispatcher.Dispatch", "expert_utils.SparseDispatcher.Combine", "tensorflow.expand_dims", "expert_utils.LocalMixtureOfExperts._experts[].Eval", "expert_utils.SparseDispatcher.Dispatch", "tensorflow.reduce_sum", "expert_utils.SparseDispatcher.ExpertToGates", "six.moves.xrange"], "methods", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.expert_utils.DistributedMixtureOfExperts.Eval", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.expert_utils.DistributedSingleDispatcher.Dispatch", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.expert_utils.DistributedSingleDispatcher.Combine", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.expert_utils.DistributedMixtureOfExperts.Eval", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.expert_utils.DistributedSingleDispatcher.Dispatch", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.expert_utils.DistributedSparseDispatcher.ExpertToGates"], ["", "def", "Eval", "(", "self", ",", "\n", "x", ",", "\n", "train", "=", "True", ",", "\n", "per_example_multiplier", "=", "None", ",", "\n", "summaries", "=", "False", ",", "\n", "identifiers", "=", "None", ")", ":", "\n", "    ", "\"\"\"Evaluate mixture of experts.\n\n    We provide a convenient debugging tool for determining the set of examples\n    that we passed to each expert.  The caller may provide a `Tensor` of\n    \"identifiers\", of any type whose first dimension matches the number of\n    input examples. The function will then return a list\n    \"expert_to_identifiers\", with one `Tensor` for each expert containing the\n    identifiers for all examples assigned to that expert.  A parallel list of\n    `Tensor`s, \"expert_to_gates\", is also returned, containing the\n    corresponding gate values.\n\n    Args:\n      x: a `Tensor` of shape `[batch_size, input_size]`\n      train: a boolean Scalar.  Are we in training mode?\n      per_example_multiplier: an optional `Tensor` of shape `[batch_size]` which\n        gets multiplied into the gate values.  If this LocalMixtureOfExperts\n        represents one secondary MoE in a hierarchical MoE, then we pass in\n        in the gate values from the primary gating function here.  This causes\n        the computed values (`y`, `importance` and `expert_to_gates`) to also\n        reflect the primary gate values.\n      summaries: an boolean.  Enable summaries.\n      identifiers: an optional `Tensor` whose first dimension is equal to\n        batch_size.\n\n    Returns:\n      y: a `Tensor` of shape `[batch_size, output_size]`.  Output of the MoE.\n      importance: a `Tensor` of shape `[n]`.  Batchwise sum of gates.\n      load: a `Tensor` of shape `[n]`.  Smooth estimator of the number of\n        examples passed to each expert.  This is useful for load-balancing,\n        as any gradient on this `Tensor` will back-propagate to the gating\n        network.\n      expert_to_identifiers:  if `identifiers` was passed in, a list of\n        length `num_experts`.  Each element is a `Tensor` whose shape matches\n        that of `identifiers` in all but the first dimension.  Contains the\n        slices of `identifiers` corresponding to the batch elements that were\n        dispatched to that expert.\n      expert_to_gates:  A list of length `num_experts`.  Each element contains\n        a 1-dimensional tensor\n    \"\"\"", "\n", "gating_hp", "=", "self", ".", "_gating_hp", "\n", "gates", ",", "load", "=", "self", ".", "_gating", ".", "Eval", "(", "x", ",", "train", ",", "summaries", ")", "\n", "if", "per_example_multiplier", "is", "not", "None", ":", "\n", "      ", "gates", "*=", "tf", ".", "expand_dims", "(", "per_example_multiplier", ",", "1", ")", "\n", "", "dispatcher", "=", "SparseDispatcher", "(", "gating_hp", ".", "num_experts", ",", "gates", ")", "\n", "expert_input", "=", "dispatcher", ".", "Dispatch", "(", "x", ")", "\n", "expert_output", "=", "[", "\n", "self", ".", "_experts", "[", "i", "]", ".", "Eval", "(", "expert_input", "[", "i", "]", ")", "\n", "for", "i", "in", "xrange", "(", "gating_hp", ".", "num_experts", ")", "\n", "]", "\n", "y", "=", "dispatcher", ".", "Combine", "(", "expert_output", ")", "\n", "if", "identifiers", "is", "not", "None", ":", "\n", "      ", "expert_to_identifiers", "=", "dispatcher", ".", "Dispatch", "(", "identifiers", ")", "\n", "", "else", ":", "\n", "      ", "expert_to_identifiers", "=", "None", "\n", "", "return", "(", "y", ",", "tf", ".", "reduce_sum", "(", "gates", ",", "0", ")", ",", "load", ",", "expert_to_identifiers", ",", "\n", "dispatcher", ".", "ExpertToGates", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.expert_utils.LocalMixtureOfExperts.vars": [[681, 688], ["ret.extend", "ret.extend"], "methods", ["None"], ["", "@", "property", "\n", "def", "vars", "(", "self", ")", ":", "\n", "    ", "ret", "=", "[", "]", "\n", "for", "x", "in", "self", ".", "_experts", ":", "\n", "      ", "ret", ".", "extend", "(", "x", ".", "vars", ")", "\n", "", "ret", ".", "extend", "(", "self", ".", "_gating", ".", "vars", ")", "\n", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.expert_utils.DistributedMixtureOfExperts.__init__": [[708, 769], ["expert_utils._SetInputOutputSizes", "expert_utils._SetInputOutputSizes", "primary_gating_hp.gating_class", "expert_utils.DistributedMixtureOfExperts._all_vars.extend", "six.moves.xrange", "six.moves.xrange", "six.moves.xrange", "tensorflow.device", "expert_utils.LocalMixtureOfExperts", "expert_utils.DistributedMixtureOfExperts._secondary_moe.append", "expert_utils.DistributedMixtureOfExperts._all_vars.extend", "tensorflow.device", "expert_hp.expert_class", "expert_utils.DistributedMixtureOfExperts._experts.append", "expert_utils.DistributedMixtureOfExperts._all_vars.extend", "len"], "methods", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.expert_utils._SetInputOutputSizes", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.expert_utils._SetInputOutputSizes"], ["def", "__init__", "(", "self", ",", "primary_gating_hp", ",", "secondary_gating_hp", ",", "expert_hp", ",", "\n", "input_size", ",", "output_size", ",", "expert_devices", ",", "name", ")", ":", "\n", "    ", "\"\"\"Create a DistributedMixtureOfExperts.\n\n    If `secondary_gating_hp` is `None`, then this is a flat MoE with\n    `primary_gating_hp.num_experts` experts. Otherwise, this is a hierarchical\n    MoE with `primary_gating_hp.num_experts` groups of\n    `secondary_gating_hp.num_experts` experts.\n\n    The assignemnt of experts (or groups of experts) to devices is by\n    round-robin.   So to make equal use of all the devices, one should set\n    `primary_gating_hp.num_experts` to the number of devices or a multiple\n    thereof.\n\n    Args:\n      primary_gating_hp: hyperparameters for the primary gating network.\n        e.g. NoisyTopKGatingParams().\n      secondary_gating_hp: hyperparameters for the secondary gating network.\n        e.g. NoisyTopKGatingParams().  None indicates a flat MoE.\n      expert_hp: hyperparameters for the expert networks.\n        e.g. FeedForwardExpertParams()\n      input_size: an integer.\n      output_size: an integer.\n      expert_devices: a list of device strings.  The devices to be used for\n        the experts.\n      name: a string.\n    \"\"\"", "\n", "self", ".", "_name", "=", "name", "\n", "# fill in the missing values in the hyperparameters", "\n", "_SetInputOutputSizes", "(", "primary_gating_hp", ",", "input_size", ",", "None", ")", "\n", "_SetInputOutputSizes", "(", "expert_hp", ",", "input_size", ",", "output_size", ")", "\n", "self", ".", "_is_hierarchical", "=", "secondary_gating_hp", "is", "not", "None", "\n", "self", ".", "_primary_gating_hp", "=", "primary_gating_hp", "\n", "self", ".", "_primary_gating", "=", "primary_gating_hp", ".", "gating_class", "(", "\n", "primary_gating_hp", ",", "name", "+", "'_primary_gating'", ")", "\n", "n1", "=", "self", ".", "_primary_gating_hp", ".", "num_experts", "\n", "# round robin assignment of experts to devices.", "\n", "expert_devices", "=", "[", "\n", "expert_devices", "[", "i", "%", "len", "(", "expert_devices", ")", "]", "for", "i", "in", "xrange", "(", "n1", ")", "\n", "]", "\n", "self", ".", "_expert_devices", "=", "expert_devices", "\n", "self", ".", "_all_vars", "=", "[", "]", "\n", "self", ".", "_all_vars", ".", "extend", "(", "self", ".", "_primary_gating", ".", "vars", ")", "\n", "if", "self", ".", "_is_hierarchical", ":", "\n", "# hierarchical MoE", "\n", "      ", "self", ".", "_secondary_moe", "=", "[", "]", "\n", "for", "i", "in", "xrange", "(", "n1", ")", ":", "\n", "        ", "with", "tf", ".", "device", "(", "expert_devices", "[", "i", "]", ")", ":", "\n", "          ", "secondary_moe", "=", "LocalMixtureOfExperts", "(", "secondary_gating_hp", ",", "expert_hp", ",", "\n", "input_size", ",", "output_size", ",", "\n", "'%s_secondary_%d'", "%", "(", "name", ",", "i", ")", ")", "\n", "self", ".", "_secondary_moe", ".", "append", "(", "secondary_moe", ")", "\n", "self", ".", "_all_vars", ".", "extend", "(", "secondary_moe", ".", "vars", ")", "\n", "", "", "", "else", ":", "\n", "# flat MoE", "\n", "      ", "self", ".", "_experts", "=", "[", "]", "\n", "for", "i", "in", "xrange", "(", "n1", ")", ":", "\n", "        ", "with", "tf", ".", "device", "(", "expert_devices", "[", "i", "]", ")", ":", "\n", "          ", "expert", "=", "expert_hp", ".", "expert_class", "(", "expert_hp", ",", "name", "+", "'_%d'", "%", "i", ")", "\n", "self", ".", "_experts", ".", "append", "(", "expert", ")", "\n", "self", ".", "_all_vars", ".", "extend", "(", "expert", ".", "vars", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.expert_utils.DistributedMixtureOfExperts.Eval": [[770, 878], ["len", "expert_utils.Parallel", "tensorflow.add_n", "tensorflow.add_n", "tensorflow.add_n", "expert_utils.DistributedSparseDispatcher", "expert_utils.DistributedSparseDispatcher.ExpertToGates", "expert_utils.Parallel", "expert_utils.DistributedSparseDispatcher.Combine", "tensorflow.stack", "len", "len", "expert_utils.Parallel", "expert_utils.Parallel", "expert_utils.DistributedSparseDispatcher.Dispatch", "expert_utils.DistributedSparseDispatcher.Dispatch", "expert_utils.DistributedSparseDispatcher.Dispatch", "expert_utils.Parallel", "expert_utils.DistributedSparseDispatcher.Combine", "tensorflow.stack", "tensorflow.expand_dims", "expert_to_gates.extend", "expert_to_identifiers.extend", "a.Eval"], "methods", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.expert_utils.Parallel", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.expert_utils.DistributedSparseDispatcher.ExpertToGates", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.expert_utils.Parallel", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.expert_utils.DistributedSingleDispatcher.Combine", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.expert_utils.Parallel", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.expert_utils.Parallel", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.expert_utils.DistributedSingleDispatcher.Dispatch", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.expert_utils.DistributedSingleDispatcher.Dispatch", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.expert_utils.DistributedSingleDispatcher.Dispatch", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.expert_utils.Parallel", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.expert_utils.DistributedSingleDispatcher.Combine", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.expert_utils.DistributedMixtureOfExperts.Eval"], ["", "", "", "", "def", "Eval", "(", "self", ",", "\n", "datashard_devices", ",", "\n", "xs", ",", "\n", "train", "=", "True", ",", "\n", "summaries", "=", "False", ",", "\n", "identifiers", "=", "None", ",", "\n", "shadow_xs", "=", "None", ")", ":", "\n", "    ", "\"\"\"Evaluate MoE on given inputs.\n\n    This class is designed for the case where the rest of the model is using\n    data parallelism.   We receive an array of input `Tensor`s, one per\n    datashard, and we produce a list of output Tensors, one per datashard.\n\n    We provide a convenient debugging tool for determining the set of examples\n    that we passed to each expert.  The caller may provide a `Tensor` of\n    \"identifiers\", of any type whose first dimension matches the number of\n    input examples. The function will then return a list\n    \"expert_to_identifiers\", with one `Tensor` for each expert containing the\n    identifiers for all examples assigned to that expert.  A parallel list of\n    `Tensor`s, \"expert_to_gates\", is also returned, containing the\n    corresponding gate values.\n\n    Args:\n      datashard_devices: a `list` of device strings of length `num_datashards`.\n        Which devices to use for the output tensors.\n      xs: A `list` of `Tensor`s of length `num_datashards`.  Each has shape\n        `[batch_size[d], input_size].\n      train: a boolean `Scalar`.   When train=`True`, noise is added to the\n        gating function.\n      summaries: a boolean.  Whether to write summaries.\n      identifiers: an optional list of tensors.\n        Each tensor has shape [<batch_size[datashard]>, extra_dims]\n      shadow_xs: Optional `list` of `Tensor`s of length `num_datashards`.  Each\n        has shape `[batch_size[d], input_size]. Shadow_xs is useful if you want\n        to dispatch a transformed version of xs to the experts, but you want\n        untransformed xs for the gating network.\n\n    Returns:\n      ys: the output (a list of one tensor per datashard).  Each has shape\n         `[batch_size[d], output_size].\n      importance: a `Tensor` of shape `[n]` for a flat MoE or `[n1, n2]` for a\n         hierarchical MoE.  Batchwise sum of gates.\n      load:  a `Tensor` of shape `[n]` for a flat MoE or `[n1, n2]` for a\n         hierarchical MoE.  Smooth estimator of the number of\n         examples passed to each expert.  This is useful for load-balancing,\n         as any gradient on this `Tensor` will back-propagate to the gating\n         network.\n      expert_to_identifiers:  if `identifiers` was passed in, a list of\n         length `num_experts`.  Each element is a `Tensor` whose shape matches\n         that of `identifiers` in all but the first dimension.  Contains the\n         slices of `identifiers` corresponding to the batch elements that were\n         dispatched to that expert.\n      expert_to_gates: a list of one tensor per expert.\n         Each tensor has shape [<num_examples[expert]>]\n\n    \"\"\"", "\n", "n1", "=", "self", ".", "_primary_gating_hp", ".", "num_experts", "\n", "epsilon", "=", "1e-10", "\n", "assert", "len", "(", "datashard_devices", ")", "==", "len", "(", "xs", ")", "\n", "num_datashards", "=", "len", "(", "xs", ")", "\n", "expert_devices", "=", "self", ".", "_expert_devices", "\n", "has_identifiers", "=", "identifiers", "is", "not", "None", "\n", "# pylint: disable=unbalanced-tuple-unpacking", "\n", "primary_gates", ",", "primary_smooth_load", "=", "Parallel", "(", "\n", "datashard_devices", ",", "self", ".", "_primary_gating", ".", "Eval", ",", "xs", ",", "train", ",", "\n", "[", "summaries", "]", "+", "[", "False", "]", "*", "(", "num_datashards", "-", "1", ")", ")", "\n", "primary_importance", "=", "tf", ".", "add_n", "(", "\n", "Parallel", "(", "datashard_devices", ",", "tf", ".", "reduce_sum", ",", "primary_gates", ",", "0", ")", ")", "\n", "primary_smooth_load", "=", "tf", ".", "add_n", "(", "primary_smooth_load", ")", "\n", "primary_true_load", "=", "tf", ".", "add_n", "(", "\n", "Parallel", "(", "datashard_devices", ",", "_GatesToLoad", ",", "primary_gates", ")", ")", "\n", "primary_dispatcher", "=", "DistributedSparseDispatcher", "(", "\n", "datashard_devices", ",", "expert_devices", ",", "primary_gates", ")", "\n", "\n", "if", "shadow_xs", "is", "None", ":", "\n", "      ", "secondary_input", "=", "primary_dispatcher", ".", "Dispatch", "(", "xs", ")", "\n", "", "else", ":", "\n", "      ", "secondary_input", "=", "primary_dispatcher", ".", "Dispatch", "(", "shadow_xs", ")", "\n", "\n", "", "primary_expert_to_identifiers", "=", "(", "primary_dispatcher", ".", "Dispatch", "(", "identifiers", ")", "\n", "if", "has_identifiers", "else", "None", ")", "\n", "primary_expert_to_gates", "=", "primary_dispatcher", ".", "ExpertToGates", "(", ")", "\n", "if", "not", "self", ".", "_is_hierarchical", ":", "\n", "# one-level distributed mixture of experts", "\n", "      ", "secondary_output", "=", "Parallel", "(", "expert_devices", ",", "lambda", "a", ",", "b", ":", "a", ".", "Eval", "(", "b", ")", ",", "\n", "self", ".", "_experts", ",", "secondary_input", ")", "\n", "ys", "=", "primary_dispatcher", ".", "Combine", "(", "secondary_output", ")", "\n", "return", "(", "ys", ",", "primary_importance", ",", "primary_smooth_load", ",", "\n", "primary_expert_to_identifiers", ",", "primary_expert_to_gates", ")", "\n", "# two-level hierarchical MoE", "\n", "", "(", "secondary_output", ",", "secondary_importance", ",", "secondary_load", ",", "\n", "secondary_expert_to_identifiers", ",", "secondary_expert_to_gates", ")", "=", "(", "Parallel", "(", "\n", "expert_devices", ",", "[", "m", ".", "Eval", "for", "m", "in", "self", ".", "_secondary_moe", "]", ",", "secondary_input", ",", "\n", "train", ",", "primary_expert_to_gates", ",", "[", "summaries", "]", "+", "[", "False", "]", "*", "(", "n1", "-", "1", ")", ",", "\n", "primary_expert_to_identifiers", ")", ")", "\n", "# pylint: enable=unbalanced-tuple-unpacking", "\n", "ys", "=", "primary_dispatcher", ".", "Combine", "(", "secondary_output", ",", "multiply_by_gates", "=", "False", ")", "\n", "importance", "=", "tf", ".", "stack", "(", "secondary_importance", ")", "\n", "load", "=", "tf", ".", "stack", "(", "secondary_load", ")", "*", "tf", ".", "expand_dims", "(", "primary_smooth_load", "/", "(", "\n", "primary_true_load", "+", "epsilon", ")", ",", "1", ")", "\n", "expert_to_identifiers", "=", "[", "]", "\n", "if", "identifiers", "is", "not", "None", ":", "\n", "      ", "for", "el", "in", "secondary_expert_to_identifiers", ":", "\n", "        ", "expert_to_identifiers", ".", "extend", "(", "el", ")", "\n", "", "", "expert_to_gates", "=", "[", "]", "\n", "for", "el", "in", "secondary_expert_to_gates", ":", "\n", "      ", "expert_to_gates", ".", "extend", "(", "el", ")", "\n", "", "return", "(", "ys", ",", "importance", ",", "load", ",", "expert_to_identifiers", ",", "expert_to_gates", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.expert_utils.DistributedMixtureOfExperts.vars": [[879, 882], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "vars", "(", "self", ")", ":", "\n", "    ", "return", "self", ".", "_all_vars", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.expert_utils.SparseDispatcher.__init__": [[905, 924], ["tensorflow.to_int32", "tensorflow.unstack", "tensorflow.reduce_sum", "tensorflow.gather", "tensorflow.where", "tensorflow.to_int32", "tensorflow.reshape", "tensorflow.transpose"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "num_experts", ",", "gates", ")", ":", "\n", "    ", "\"\"\"Create a SparseDispatcher.\n\n    Args:\n      num_experts: an integer.\n      gates: a `Tensor` of shape `[batch_size, num_experts]`.\n\n    Returns:\n      a SparseDispatcher\n    \"\"\"", "\n", "self", ".", "_gates", "=", "gates", "\n", "self", ".", "_num_experts", "=", "num_experts", "\n", "\n", "where", "=", "tf", ".", "to_int32", "(", "tf", ".", "where", "(", "tf", ".", "transpose", "(", "gates", ")", ">", "0", ")", ")", "\n", "self", ".", "_expert_index", ",", "self", ".", "_batch_index", "=", "tf", ".", "unstack", "(", "where", ",", "num", "=", "2", ",", "axis", "=", "1", ")", "\n", "self", ".", "_part_sizes_tensor", "=", "tf", ".", "reduce_sum", "(", "tf", ".", "to_int32", "(", "gates", ">", "0", ")", ",", "[", "0", "]", ")", "\n", "self", ".", "_nonzero_gates", "=", "tf", ".", "gather", "(", "\n", "tf", ".", "reshape", "(", "self", ".", "_gates", ",", "[", "-", "1", "]", ")", ",", "\n", "self", ".", "_batch_index", "*", "num_experts", "+", "self", ".", "_expert_index", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.expert_utils.SparseDispatcher.Dispatch": [[925, 939], ["tensorflow.gather", "tensorflow.split"], "methods", ["None"], ["", "def", "Dispatch", "(", "self", ",", "inp", ")", ":", "\n", "    ", "\"\"\"Create one input Tensor for each expert.\n\n    The `Tensor` for a expert `i` contains the slices of `inp` corresponding\n    to the batch elements `b` where `gates[b, i] > 0`.\n\n    Args:\n      inp: a `Tensor` of shape '[batch_size, <extra_input_dims>]`\n    Returns:\n      a list of `num_experts` `Tensor`s with shapes\n        `[expert_batch_size_i, <extra_input_dims>]`.\n    \"\"\"", "\n", "inp", "=", "tf", ".", "gather", "(", "inp", ",", "self", ".", "_batch_index", ")", "\n", "return", "tf", ".", "split", "(", "inp", ",", "self", ".", "_part_sizes_tensor", ",", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.expert_utils.SparseDispatcher.Combine": [[940, 963], ["expert_utils.ConvertGradientToTensor", "tensorflow.unsorted_segment_sum", "tensorflow.concat", "tensorflow.expand_dims", "tensorflow.shape"], "methods", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.expert_utils.ConvertGradientToTensor"], ["", "def", "Combine", "(", "self", ",", "expert_out", ",", "multiply_by_gates", "=", "True", ")", ":", "\n", "    ", "\"\"\"Sum together the expert output, weighted by the gates.\n\n    The slice corresponding to a particular batch element `b` is computed\n    as the sum over all experts `i` of the expert output, weighted by the\n    corresponding gate values.  If `multiply_by_gates` is set to False, the\n    gate values are ignored.\n\n    Args:\n      expert_out: a list of `num_experts` `Tensor`s, each with shape\n        `[expert_batch_size_i, <extra_output_dims>]`.\n      multiply_by_gates: a boolean\n\n    Returns:\n      a `Tensor` with shape `[batch_size, <extra_output_dims>]`.\n    \"\"\"", "\n", "# see comments on ConvertGradientToTensor", "\n", "stitched", "=", "ConvertGradientToTensor", "(", "tf", ".", "concat", "(", "expert_out", ",", "0", ")", ")", "\n", "if", "multiply_by_gates", ":", "\n", "      ", "stitched", "*=", "tf", ".", "expand_dims", "(", "self", ".", "_nonzero_gates", ",", "1", ")", "\n", "", "combined", "=", "tf", ".", "unsorted_segment_sum", "(", "stitched", ",", "self", ".", "_batch_index", ",", "\n", "tf", ".", "shape", "(", "self", ".", "_gates", ")", "[", "0", "]", ")", "\n", "return", "combined", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.expert_utils.SparseDispatcher.ExpertToGates": [[964, 972], ["tensorflow.split"], "methods", ["None"], ["", "def", "ExpertToGates", "(", "self", ")", ":", "\n", "    ", "\"\"\"Gate values corresponding to the examples in the per-expert `Tensor`s.\n\n    Returns:\n      a list of `num_experts` one-dimensional `Tensor`s with type `tf.float32`\n          and shapes `[expert_batch_size_i]`\n    \"\"\"", "\n", "return", "tf", ".", "split", "(", "self", ".", "_nonzero_gates", ",", "self", ".", "_part_sizes_tensor", ",", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.expert_utils.SparseDispatcher.part_sizes": [[973, 976], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "part_sizes", "(", "self", ")", ":", "\n", "    ", "return", "self", ".", "_part_sizes_tensor", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.expert_utils.DistributedSparseDispatcher.__init__": [[990, 1010], ["len", "len", "expert_utils.Parallel", "len", "len"], "methods", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.expert_utils.Parallel"], ["def", "__init__", "(", "self", ",", "datashard_devices", ",", "expert_devices", ",", "gates", ")", ":", "\n", "    ", "\"\"\"Create a DistributedSparseDispatcher.\n\n    Args:\n      datashard_devices: a list of num_datashards device strings.\n      expert_devices: a list of num_experts device strings.\n      gates: a list of num_datashards `Tensor`s of shapes\n        `[batch_size[d], num_experts]`.\n\n    Returns:\n      a DistributedSparseDispatcher\n    \"\"\"", "\n", "self", ".", "_gates", "=", "gates", "\n", "self", ".", "_num_experts", "=", "len", "(", "expert_devices", ")", "\n", "assert", "len", "(", "gates", ")", "==", "len", "(", "datashard_devices", ")", "\n", "self", ".", "_num_datashards", "=", "len", "(", "gates", ")", "\n", "self", ".", "_datashard_devices", "=", "datashard_devices", "\n", "self", ".", "_expert_devices", "=", "expert_devices", "\n", "self", ".", "_dispatchers", "=", "Parallel", "(", "self", ".", "_datashard_devices", ",", "SparseDispatcher", ",", "\n", "self", ".", "_num_experts", ",", "gates", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.expert_utils.DistributedSparseDispatcher.Dispatch": [[1011, 1029], ["expert_utils.Parallel", "expert_utils.Parallel", "expert_utils.TransposeListOfLists", "expert_utils.Parallel", "a.Dispatch"], "methods", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.expert_utils.Parallel", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.expert_utils.Parallel", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.expert_utils.TransposeListOfLists", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.expert_utils.Parallel", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.expert_utils.DistributedSingleDispatcher.Dispatch"], ["", "def", "Dispatch", "(", "self", ",", "inp", ")", ":", "\n", "    ", "\"\"\"Create one input Tensor for each expert.\n\n    Args:\n      inp: a list of length num_datashards `Tensor`s with shapes\n        `[batch_size[d], <extra_input_dims>]`.\n    Returns:\n      a list of `num_experts` `Tensor`s with shapes\n        `[num_examples[i], <extra_input_dims>]`.\n    \"\"\"", "\n", "dispatched", "=", "Parallel", "(", "self", ".", "_datashard_devices", ",", "lambda", "a", ",", "b", ":", "a", ".", "Dispatch", "(", "b", ")", ",", "\n", "self", ".", "_dispatchers", ",", "inp", ")", "\n", "ret", "=", "Parallel", "(", "self", ".", "_expert_devices", ",", "tf", ".", "concat", ",", "\n", "TransposeListOfLists", "(", "dispatched", ")", ",", "0", ")", "\n", "if", "ret", "[", "0", "]", ".", "dtype", "==", "tf", ".", "float32", ":", "\n", "# see comments on ConvertGradientToTensor", "\n", "      ", "ret", "=", "Parallel", "(", "self", ".", "_expert_devices", ",", "ConvertGradientToTensor", ",", "ret", ")", "\n", "", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.expert_utils.DistributedSparseDispatcher.Combine": [[1030, 1061], ["tensorflow.unstack", "expert_utils.Parallel", "expert_utils.TransposeListOfLists", "six.moves.xrange", "tensorflow.stack", "tensorflow.device", "ret.append", "expert_utils.DistributedSparseDispatcher._dispatchers[].Combine", "six.moves.xrange", "expert_utils.ConvertGradientToTensor", "tensorflow.concat"], "methods", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.expert_utils.Parallel", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.expert_utils.TransposeListOfLists", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.expert_utils.DistributedSingleDispatcher.Combine", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.expert_utils.ConvertGradientToTensor"], ["", "def", "Combine", "(", "self", ",", "expert_out", ",", "multiply_by_gates", "=", "True", ")", ":", "\n", "    ", "\"\"\"Sum together the expert output, multiplied by the corresponding gates.\n\n    Args:\n      expert_out: a list of `num_experts` `Tensor`s, each with shape\n        `[expert_batch_size_i, <extra_output_dims>]`.\n      multiply_by_gates: a boolean.\n\n    Returns:\n      a list of num_datashards `Tensor`s with shapes\n        `[batch_size[d], <extra_output_dims>]`.\n    \"\"\"", "\n", "expert_part_sizes", "=", "tf", ".", "unstack", "(", "\n", "tf", ".", "stack", "(", "[", "\n", "self", ".", "_dispatchers", "[", "d", "]", ".", "part_sizes", "\n", "for", "d", "in", "xrange", "(", "self", ".", "_num_datashards", ")", "\n", "]", ")", ",", "\n", "num", "=", "self", ".", "_num_experts", ",", "\n", "axis", "=", "1", ")", "\n", "# list of lists of shape [num_experts][num_datashards]", "\n", "expert_output_parts", "=", "Parallel", "(", "self", ".", "_expert_devices", ",", "tf", ".", "split", ",", "expert_out", ",", "\n", "expert_part_sizes", ")", "\n", "expert_output_parts_t", "=", "TransposeListOfLists", "(", "expert_output_parts", ")", "\n", "ret", "=", "[", "]", "\n", "for", "d", "in", "xrange", "(", "self", ".", "_num_datashards", ")", ":", "\n", "      ", "with", "tf", ".", "device", "(", "self", ".", "_datashard_devices", "[", "d", "]", ")", ":", "\n", "        ", "ret", ".", "append", "(", "self", ".", "_dispatchers", "[", "d", "]", ".", "Combine", "(", "\n", "# see comments on ConvertGradientToTensor", "\n", "ConvertGradientToTensor", "(", "tf", ".", "concat", "(", "expert_output_parts_t", "[", "d", "]", ",", "0", ")", ")", ",", "\n", "multiply_by_gates", "=", "multiply_by_gates", ")", ")", "\n", "", "", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.expert_utils.DistributedSparseDispatcher.ExpertToGates": [[1062, 1074], ["expert_utils.Parallel", "expert_utils.TransposeListOfLists", "expert_utils.Parallel", "six.moves.xrange"], "methods", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.expert_utils.Parallel", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.expert_utils.TransposeListOfLists", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.expert_utils.Parallel"], ["", "def", "ExpertToGates", "(", "self", ")", ":", "\n", "    ", "\"\"\"Gate values corresponding to the examples in the per-expert `Tensor`s.\n\n    Returns:\n      a list of `num_experts` one-dimensional `Tensor`s of type `tf.float32`.\n    \"\"\"", "\n", "return", "Parallel", "(", "self", ".", "_expert_devices", ",", "tf", ".", "concat", ",", "\n", "TransposeListOfLists", "(", "\n", "Parallel", "(", "self", ".", "_datashard_devices", ",", "[", "\n", "self", ".", "_dispatchers", "[", "d", "]", ".", "ExpertToGates", "\n", "for", "d", "in", "xrange", "(", "self", ".", "_num_datashards", ")", "\n", "]", ")", ")", ",", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.expert_utils.DistributedSingleDispatcher.__init__": [[1097, 1130], ["data_parallelism", "data_parallelism", "tensorflow.unstack", "data_parallelism", "tensorflow.unsorted_segment_sum", "tensorflow.stack", "tensorflow.dynamic_partition", "tensorflow.ones_like", "tensorflow.range", "tensorflow.size"], "methods", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.parallel.data_parallelism", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.parallel.data_parallelism", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.parallel.data_parallelism", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.distribute.size"], ["def", "__init__", "(", "self", ",", "data_parallelism", ",", "model_parallelism", ",", "gates", ")", ":", "\n", "    ", "\"\"\"Constructs a Dispatcher.\n\n    Args:\n      data_parallelism: a Parallelism object.\n      model_parallelism: a Parallelism object.\n      gates: a list of 1d integer `Tensor`s, one per datashard.\n        Says which expert to use for each batch element.\n\n    Returns:\n      a DistributedSingleDispatcher\n    \"\"\"", "\n", "gates", "=", "data_parallelism", "(", "tf", ".", "to_int32", ",", "gates", ")", "\n", "self", ".", "_gates", "=", "gates", "\n", "self", ".", "_data_parallelism", "=", "data_parallelism", "\n", "self", ".", "_model_parallelism", "=", "model_parallelism", "\n", "\n", "# Compute the sizes number of examples going from each datashard to each", "\n", "# expert.", "\n", "def", "_PartSizes", "(", "gates", ")", ":", "\n", "      ", "return", "tf", ".", "unsorted_segment_sum", "(", "\n", "tf", ".", "ones_like", "(", "gates", ")", ",", "gates", ",", "model_parallelism", ".", "n", ")", "\n", "\n", "", "part_sizes_by_datashard", "=", "data_parallelism", "(", "_PartSizes", ",", "gates", ")", "\n", "self", ".", "_part_sizes_by_expert", "=", "tf", ".", "unstack", "(", "\n", "tf", ".", "stack", "(", "part_sizes_by_datashard", ")", ",", "num", "=", "model_parallelism", ".", "n", ",", "axis", "=", "1", ")", "\n", "\n", "# These indices will be used to combine the output on the datashards.", "\n", "def", "_StitchIndices", "(", "gates", ")", ":", "\n", "      ", "return", "tf", ".", "dynamic_partition", "(", "\n", "tf", ".", "range", "(", "tf", ".", "size", "(", "gates", ")", ")", ",", "gates", ",", "model_parallelism", ".", "n", ")", "\n", "\n", "", "self", ".", "_stitch_indices", "=", "data_parallelism", "(", "_StitchIndices", ",", "gates", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.expert_utils.DistributedSingleDispatcher.Dispatch": [[1131, 1150], ["expert_utils.DistributedSingleDispatcher._data_parallelism", "expert_utils.TransposeListOfLists", "expert_utils.DistributedSingleDispatcher._model_parallelism"], "methods", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.expert_utils.TransposeListOfLists"], ["", "def", "Dispatch", "(", "self", ",", "d_tensors", ")", ":", "\n", "    ", "\"\"\"Reshuffles input `Tensor`s to produce output `Tensor`s.\n\n    The dimensions of all input and output `Tensor`s match, except for\n    dimension 0.  In dimension 0, the input `Tensor`s match the corresponding\n    `gates` `Tensor`s which were passed to the constructor.\n\n    Args:\n      d_tensors: a list of `Tensor`s, one per datashard.\n\n    Returns:\n      a list of `Tensor`s, one per expert.\n\n    \"\"\"", "\n", "parts", "=", "self", ".", "_data_parallelism", "(", "tf", ".", "dynamic_partition", ",", "d_tensors", ",", "self", ".", "_gates", ",", "\n", "self", ".", "_model_parallelism", ".", "n", ")", "\n", "parts_by_expert", "=", "TransposeListOfLists", "(", "parts", ")", "\n", "x_tensors", "=", "self", ".", "_model_parallelism", "(", "tf", ".", "concat", ",", "parts_by_expert", ",", "0", ")", "\n", "return", "x_tensors", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.expert_utils.DistributedSingleDispatcher.Combine": [[1151, 1172], ["expert_utils.DistributedSingleDispatcher._model_parallelism", "expert_utils.DistributedSingleDispatcher._data_parallelism", "expert_utils.TransposeListOfLists"], "methods", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.expert_utils.TransposeListOfLists"], ["", "def", "Combine", "(", "self", ",", "x_tensors", ")", ":", "\n", "    ", "\"\"\"Reshuffles per-expert `Tensor`s to produce per-datashard `Tensor`s.\n\n    Dispatch must have been called at least once first.\n\n    The dimensions of all input and output `Tensor`s match, except for\n    dimension 0.  In dimension 0, the input `Tensor`s match the corresponding\n    outputs of `Dispatch`, and the output `Tensor`s match the corresponding\n    `gates` `Tensor`s which were passed to the constructor.\n\n    Args:\n      x_tensors: a list of `Tensor`s, one per expert.\n\n    Returns:\n      a list of `Tensor`s, one per datashard.\n    \"\"\"", "\n", "parts", "=", "self", ".", "_model_parallelism", "(", "tf", ".", "split", ",", "x_tensors", ",", "\n", "self", ".", "_part_sizes_by_expert", ")", "\n", "d_tensors", "=", "self", ".", "_data_parallelism", "(", "tf", ".", "dynamic_stitch", ",", "self", ".", "_stitch_indices", ",", "\n", "TransposeListOfLists", "(", "parts", ")", ")", "\n", "return", "d_tensors", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.expert_utils._SetInputOutputSizes": [[75, 96], ["None"], "function", ["None"], ["", "", "def", "_SetInputOutputSizes", "(", "hp", ",", "input_size", ",", "output_size", ")", ":", "\n", "  ", "\"\"\"Fill in the input_size and output_size hyperparameters.\n\n  This is used by LocalMixtureOfExperts and DistributedMixtureOfExperts to\n  fill in the input_size and output_size on the gating parameters and expert\n  parameters so that the user does not have to set them in multiple places.\n\n  Args:\n    hp: a hyperparameters\n    input_size: an integer\n    output_size: an integer\n  \"\"\"", "\n", "if", "hp", ".", "input_size", "is", "None", ":", "\n", "    ", "hp", ".", "input_size", "=", "input_size", "\n", "", "else", ":", "\n", "    ", "assert", "hp", ".", "input_size", "==", "input_size", "\n", "", "if", "output_size", "is", "not", "None", ":", "\n", "    ", "if", "hp", ".", "output_size", "is", "None", ":", "\n", "      ", "hp", ".", "output_size", "=", "output_size", "\n", "", "else", ":", "\n", "      ", "assert", "hp", ".", "output_size", "==", "output_size", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.expert_utils.ConvertGradientToTensor": [[150, 171], ["tensorflow.python.framework.function.Defun", "tensorflow.convert_to_tensor", "op.inputs[].get_shape"], "function", ["None"], ["", "", "@", "function", ".", "Defun", "(", "\n", "python_grad_func", "=", "lambda", "x", ",", "dy", ":", "tf", ".", "convert_to_tensor", "(", "dy", ")", ",", "\n", "shape_func", "=", "lambda", "op", ":", "[", "op", ".", "inputs", "[", "0", "]", ".", "get_shape", "(", ")", "]", ")", "\n", "def", "ConvertGradientToTensor", "(", "x", ")", ":", "\n", "  ", "\"\"\"Identity operation whose gradient is converted to a `Tensor`.\n\n  Currently, the gradient to `tf.concat` is particularly expensive to\n  compute if dy is an `IndexedSlices` (a lack of GPU implementation\n  forces the gradient operation onto CPU).  This situation occurs when\n  the output of the `tf.concat` is eventually passed to `tf.gather`.\n  It is sometimes faster to convert the gradient to a `Tensor`, so as\n  to get the cheaper gradient for `tf.concat`.  To do this, replace\n  `tf.concat(x)` with `ConvertGradientToTensor(tf.concat(x))`.\n\n  Args:\n    x: A `Tensor`.\n\n  Returns:\n    The input `Tensor`.\n  \"\"\"", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.expert_utils.Parallel": [[328, 344], ["expert_utils.Parallelism", "tensorflow.tf.reduce_sum", "expert_utils._GatesToLoad", "tensorflow.tf.concat", "tensorflow.tf.split", "tensorflow.tf.concat", "expert_utils.ConvertGradientToTensor"], "function", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.expert_utils._GatesToLoad", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.expert_utils.ConvertGradientToTensor"], ["", "", "", "def", "Parallel", "(", "device_names_or_functions", ",", "fn", ",", "*", "args", ")", ":", "\n", "  ", "\"\"\"Deprecated interface.\n\n  Use `Parallelism(device_names_or_functions)(fn, *args)` instead.\n\n  Args:\n    device_names_or_functions: A list of length n.\n    fn: a function or a list of n functions.\n    *args: additional args.  Each arg should either be not a list, or a list\n       of length n.\n\n  Returns:\n    either a single list of length n (if fn does not return a tuple), or a\n    tuple of lists of length n (if fn returns a tuple).\n  \"\"\"", "\n", "return", "Parallelism", "(", "device_names_or_functions", ")", "(", "fn", ",", "*", "args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.expert_utils._RowwiseUnsortedSegmentSum": [[346, 361], ["tensorflow.unstack", "tensorflow.unsorted_segment_sum", "tensorflow.reshape", "tensorflow.shape", "tensorflow.reshape", "tensorflow.reshape", "tensorflow.div", "tensorflow.range"], "function", ["None"], ["", "def", "_RowwiseUnsortedSegmentSum", "(", "values", ",", "indices", ",", "n", ")", ":", "\n", "  ", "\"\"\"UnsortedSegmentSum on each row.\n\n  Args:\n    values: a `Tensor` with shape `[batch_size, k]`.\n    indices: an integer `Tensor` with shape `[batch_size, k]`.\n    n: an integer.\n  Returns:\n    A `Tensor` with the same type as `values` and shape `[batch_size, n]`.\n  \"\"\"", "\n", "batch", ",", "k", "=", "tf", ".", "unstack", "(", "tf", ".", "shape", "(", "indices", ")", ",", "num", "=", "2", ")", "\n", "indices_flat", "=", "tf", ".", "reshape", "(", "indices", ",", "[", "-", "1", "]", ")", "+", "tf", ".", "div", "(", "tf", ".", "range", "(", "batch", "*", "k", ")", ",", "k", ")", "*", "n", "\n", "ret_flat", "=", "tf", ".", "unsorted_segment_sum", "(", "\n", "tf", ".", "reshape", "(", "values", ",", "[", "-", "1", "]", ")", ",", "indices_flat", ",", "batch", "*", "n", ")", "\n", "return", "tf", ".", "reshape", "(", "ret_flat", ",", "[", "batch", ",", "n", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.expert_utils._NormalDistributionCDF": [[363, 380], ["tensorflow.erf", "math.sqrt"], "function", ["None"], ["", "def", "_NormalDistributionCDF", "(", "x", ",", "stddev", ")", ":", "\n", "  ", "\"\"\"Evaluates the CDF of the normal distribution.\n\n  Normal distribution with mean 0 and standard deviation stddev,\n  evaluated at x=x.\n\n  input and output `Tensor`s have matching shapes.\n\n  Args:\n    x: a `Tensor`\n    stddev: a `Tensor` with the same shape as `x`.\n\n  Returns:\n    a `Tensor` with the same shape as `x`.\n\n  \"\"\"", "\n", "return", "0.5", "*", "(", "1.0", "+", "tf", ".", "erf", "(", "x", "/", "(", "math", ".", "sqrt", "(", "2", ")", "*", "stddev", "+", "1e-20", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.expert_utils._ProbInTopK": [[382, 427], ["tensorflow.reshape", "tensorflow.expand_dims", "tensorflow.greater", "tensorflow.expand_dims", "expert_utils._NormalDistributionCDF", "expert_utils._NormalDistributionCDF", "tensorflow.where", "tensorflow.shape", "tensorflow.shape", "tensorflow.gather", "tensorflow.to_float", "tensorflow.gather", "tensorflow.range"], "function", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.expert_utils._NormalDistributionCDF", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.expert_utils._NormalDistributionCDF"], ["", "def", "_ProbInTopK", "(", "clean_values", ",", "noisy_values", ",", "noise_stddev", ",", "noisy_top_values", ",", "k", ")", ":", "\n", "  ", "\"\"\"Helper function to NoisyTopKGating.\n\n  Computes the probability that value is in top k, given different random noise.\n\n  This gives us a way of backpropagating from a loss that balances the number\n  of times each expert is in the top k experts per example.\n\n  In the case of no noise, pass in None for noise_stddev, and the result will\n  not be differentiable.\n\n  Args:\n    clean_values: a `Tensor` of shape [batch, n].\n    noisy_values: a `Tensor` of shape [batch, n].  Equal to clean values plus\n      normally distributed noise with standard deviation noise_stddev.\n    noise_stddev: a `Tensor` of shape [batch, n], or None\n    noisy_top_values: a `Tensor` of shape [batch, m].\n       'values' Output of tf.top_k(noisy_top_values, m).  m >= k+1\n    k: an integer.\n\n  Returns:\n    a `Tensor` of shape [batch, n].\n  \"\"\"", "\n", "batch", "=", "tf", ".", "shape", "(", "clean_values", ")", "[", "0", "]", "\n", "m", "=", "tf", ".", "shape", "(", "noisy_top_values", ")", "[", "1", "]", "\n", "top_values_flat", "=", "tf", ".", "reshape", "(", "noisy_top_values", ",", "[", "-", "1", "]", ")", "\n", "# we want to compute the threshold that a particular value would have to", "\n", "# exceed in order to make the top k.  This computation differs depending", "\n", "# on whether the value is already in the top k.", "\n", "threshold_positions_if_in", "=", "tf", ".", "range", "(", "batch", ")", "*", "m", "+", "k", "\n", "threshold_if_in", "=", "tf", ".", "expand_dims", "(", "\n", "tf", ".", "gather", "(", "top_values_flat", ",", "threshold_positions_if_in", ")", ",", "1", ")", "\n", "is_in", "=", "tf", ".", "greater", "(", "noisy_values", ",", "threshold_if_in", ")", "\n", "if", "noise_stddev", "is", "None", ":", "\n", "    ", "return", "tf", ".", "to_float", "(", "is_in", ")", "\n", "", "threshold_positions_if_out", "=", "threshold_positions_if_in", "-", "1", "\n", "threshold_if_out", "=", "tf", ".", "expand_dims", "(", "\n", "tf", ".", "gather", "(", "top_values_flat", ",", "threshold_positions_if_out", ")", ",", "1", ")", "\n", "# is each value currently in the top k.", "\n", "prob_if_in", "=", "_NormalDistributionCDF", "(", "clean_values", "-", "threshold_if_in", ",", "\n", "noise_stddev", ")", "\n", "prob_if_out", "=", "_NormalDistributionCDF", "(", "clean_values", "-", "threshold_if_out", ",", "\n", "noise_stddev", ")", "\n", "prob", "=", "tf", ".", "where", "(", "is_in", ",", "prob_if_in", ",", "prob_if_out", ")", "\n", "return", "prob", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.expert_utils.CVSquared": [[429, 447], ["tensorflow.to_float", "tensorflow.reduce_sum", "tensorflow.reduce_sum", "tensorflow.size", "tensorflow.square", "tensorflow.square"], "function", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.distribute.size"], ["", "def", "CVSquared", "(", "x", ")", ":", "\n", "  ", "\"\"\"The squared coefficient of variation of a sample.\n\n  Useful as a loss to encourage a positive distribution to be more uniform.\n  Epsilons added for numerical stability.\n  Returns 0 for an empty Tensor.\n\n  Args:\n    x: a `Tensor`.\n\n  Returns:\n    a `Scalar`.\n  \"\"\"", "\n", "epsilon", "=", "1e-10", "\n", "float_size", "=", "tf", ".", "to_float", "(", "tf", ".", "size", "(", "x", ")", ")", "+", "epsilon", "\n", "mean", "=", "tf", ".", "reduce_sum", "(", "x", ")", "/", "float_size", "\n", "variance", "=", "tf", ".", "reduce_sum", "(", "tf", ".", "square", "(", "x", "-", "mean", ")", ")", "/", "float_size", "\n", "return", "variance", "/", "(", "tf", ".", "square", "(", "mean", ")", "+", "epsilon", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.expert_utils.MaxOverload": [[449, 473], ["tensorflow.reduce_sum", "tensorflow.reshape", "tensorflow.reduce_max", "tensorflow.reduce_mean", "tensorflow.shape"], "function", ["None"], ["", "def", "MaxOverload", "(", "load", ")", ":", "\n", "  ", "\"\"\"The load of the hardest-hit device relative to average.\n\n  This is useful for monitoring the performance of MoEs.\n\n  The load of an expert is the number of examples assigned to that expert.\n  The load of a device is the sum of the loads of all experts on that device.\n\n  The input to this function is generally the 'load' output of\n  DistributedMixtureOfExperts.Eval(), which is either a 1d or 2d `Tensor` of\n  per-expert loads.  In either case, the fist dimension corresponds to devices.\n\n  This function sums over all dimensions other than dimension zero, then\n  computes the ratio of the maxmium value to the mean value.\n\n  Args:\n    load: a 1d or 2d `Tensor`.\n\n  Returns:\n    a `Scalar`.\n  \"\"\"", "\n", "per_device_load", "=", "tf", ".", "reduce_sum", "(", "tf", ".", "reshape", "(", "load", ",", "[", "tf", ".", "shape", "(", "load", ")", "[", "0", "]", ",", "-", "1", "]", ")", ",", "1", ")", "\n", "return", "(", "tf", ".", "reduce_max", "(", "per_device_load", ")", "/", "\n", "(", "tf", ".", "reduce_mean", "(", "per_device_load", ")", "+", "1e-10", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.expert_utils._GatesToLoad": [[475, 486], ["tensorflow.reduce_sum", "tensorflow.to_float"], "function", ["None"], ["", "def", "_GatesToLoad", "(", "gates", ")", ":", "\n", "  ", "\"\"\"Compute the true load per expert, given the gates.\n\n  The load is the number of examples for which the corresponding gate is >0.\n\n  Args:\n    gates: a `Tensor` of shape [batch_size, n]\n  Returns:\n    a float32 `Tensor` of shape [n]\n  \"\"\"", "\n", "return", "tf", ".", "reduce_sum", "(", "tf", ".", "to_float", "(", "gates", ">", "0", ")", ",", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.expert_utils._MyTopK": [[488, 513], ["six.moves.xrange", "tensorflow.nn.top_k", "tensorflow.shape", "values.append", "tensorflow.argmax", "indices.append", "tensorflow.stack", "tensorflow.to_int32", "tensorflow.reduce_max", "tensorflow.one_hot", "tensorflow.stack"], "function", ["None"], ["", "def", "_MyTopK", "(", "x", ",", "k", ")", ":", "\n", "  ", "\"\"\"GPU-compatible version of top-k that works for very small constant k.\n\n  Calls argmax repeatedly.\n\n  Args:\n    x: a 2d Tensor.\n    k: a small integer.\n\n  Returns:\n    values: a Tensor of shape [batch_size, k]\n    indices: a int32 Tensor of shape [batch_size, k]\n  \"\"\"", "\n", "if", "k", ">", "10", ":", "\n", "    ", "return", "tf", ".", "nn", ".", "top_k", "(", "x", ",", "k", ")", "\n", "", "values", "=", "[", "]", "\n", "indices", "=", "[", "]", "\n", "depth", "=", "tf", ".", "shape", "(", "x", ")", "[", "1", "]", "\n", "for", "i", "in", "xrange", "(", "k", ")", ":", "\n", "    ", "values", ".", "append", "(", "tf", ".", "reduce_max", "(", "x", ",", "1", ")", ")", "\n", "argmax", "=", "tf", ".", "argmax", "(", "x", ",", "1", ")", "\n", "indices", ".", "append", "(", "argmax", ")", "\n", "if", "i", "+", "1", "<", "k", ":", "\n", "      ", "x", "+=", "tf", ".", "one_hot", "(", "argmax", ",", "depth", ",", "-", "1e9", ")", "\n", "", "", "return", "tf", ".", "stack", "(", "values", ",", "axis", "=", "1", ")", ",", "tf", ".", "to_int32", "(", "tf", ".", "stack", "(", "indices", ",", "axis", "=", "1", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.expert_utils.TransposeListOfLists": [[1076, 1086], ["list", "six.moves.zip"], "function", ["None"], ["", "", "def", "TransposeListOfLists", "(", "lol", ")", ":", "\n", "  ", "\"\"\"Transpose a list of equally-sized python lists.\n\n  Args:\n    lol: a list of lists\n  Returns:\n    a list of lists\n  \"\"\"", "\n", "assert", "lol", ",", "'cannot pass the empty list'", "\n", "return", "[", "list", "(", "x", ")", "for", "x", "in", "zip", "(", "*", "lol", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.expert_utils.ParallelEmbeddingLookup": [[1174, 1206], ["expert_utils.Parallelism", "len", "data_parallelism", "data_parallelism", "data_parallelism", "expert_utils.DistributedSingleDispatcher", "expert_utils.DistributedSingleDispatcher.Dispatch", "Parallelism.", "Parallelism.", "expert_utils.DistributedSingleDispatcher.Combine", "data_parallelism"], "function", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.parallel.data_parallelism", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.parallel.data_parallelism", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.parallel.data_parallelism", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.expert_utils.DistributedSingleDispatcher.Dispatch", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.expert_utils.DistributedSingleDispatcher.Combine", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.parallel.data_parallelism"], ["", "", "def", "ParallelEmbeddingLookup", "(", "params", ",", "ids", ",", "data_parallelism", ")", ":", "\n", "  ", "\"\"\"Mod-sharded embedding lookup with multiple datashards.\n\n  TODO(noam): does this work when vocab_size is not a multiple of `num_shards`?\n\n  Args:\n    params:  A list of `num_shards` `Tensors`, each with shapes\n       `[vocab_size / num_params, depth]`.\n    ids: A list of `num_datashards` one-dimensional ineger `Tensors`,\n       with shapes `[batch_size[i]]`\n    data_parallelism: A Parallelism object.\n\n  Returns:\n    a list of `num_datashards` `Tensors`, each with shape\n       `[batch_size[i], depth]`.\n  \"\"\"", "\n", "param_devices", "=", "[", "x", ".", "device", "for", "x", "in", "params", "]", "\n", "model_parallelism", "=", "Parallelism", "(", "param_devices", ")", "\n", "num_shards", "=", "len", "(", "param_devices", ")", "\n", "# pylint: disable=unbalanced-tuple-unpacking", "\n", "ids", ",", "unique_idx", "=", "data_parallelism", "(", "tf", ".", "unique", ",", "ids", ")", "\n", "# pylint: enable=unbalanced-tuple-unpacking", "\n", "gates", "=", "data_parallelism", "(", "tf", ".", "mod", ",", "ids", ",", "num_shards", ")", "\n", "ids_div", "=", "data_parallelism", "(", "tf", ".", "div", ",", "ids", ",", "num_shards", ")", "\n", "dispatcher", "=", "DistributedSingleDispatcher", "(", "data_parallelism", ",", "model_parallelism", ",", "\n", "gates", ")", "\n", "x_ids_div", "=", "dispatcher", ".", "Dispatch", "(", "ids_div", ")", "\n", "params", "=", "model_parallelism", "(", "ConvertGradientToTensor", ",", "params", ")", "\n", "x_emb", "=", "model_parallelism", "(", "tf", ".", "gather", ",", "params", ",", "x_ids_div", ")", "\n", "r_emb", "=", "dispatcher", ".", "Combine", "(", "x_emb", ")", "\n", "r_emb", "=", "data_parallelism", "(", "tf", ".", "gather", ",", "r_emb", ",", "unique_idx", ")", "\n", "return", "r_emb", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.expert_utils.SampledSoftmaxLoss": [[1208, 1244], ["tensorflow.reduce_logsumexp", "tensorflow.unsorted_segment_sum", "tensorflow.reduce_logsumexp", "tensorflow.matmul", "sampler.log_expected_count", "tensorflow.reduce_sum", "sampler.log_expected_count", "tensorflow.fill", "tensorflow.gather", "tensorflow.stack", "tensorflow.shape", "float"], "function", ["None"], ["", "def", "SampledSoftmaxLoss", "(", "features", ",", "sampler", ",", "num_classes", ",", "target_classes", ",", "\n", "target_params", ",", "sampled_classes", ",", "sampled_params", ")", ":", "\n", "  ", "\"\"\"Loss for training softmax classifiers on large label vocabulary.\n\n  This function assumes that we have already chosen the sampled classes and\n  fetched the parameters for the target classes and the sampled classes.\n\n  Args:\n    features: a Tensor with shape [batch_size, hidden_size]\n    sampler: a candidate sampler object\n    num_classes: an integer\n    target_classes: an integer Tensor with shape [batch_size]\n    target_params: a Tensor with shape [batch_size, hidden_size]\n      The parameters corresponding to the target classes.\n    sampled_classes: an integer tensor with shape [num_sampled_classes]\n    sampled_params: a Tensor with shape [num_sampled_classes, hidden_size]\n      The parameters corresponding to the sampled classes.\n\n  Returns:\n    a Tensor with shape [batch_size]\n  \"\"\"", "\n", "sampled_logits", "=", "(", "tf", ".", "matmul", "(", "features", ",", "sampled_params", ",", "transpose_b", "=", "True", ")", "-", "\n", "sampler", ".", "log_expected_count", "(", "sampled_classes", ")", ")", "\n", "target_logits", "=", "(", "tf", ".", "reduce_sum", "(", "target_params", "*", "features", ",", "1", ")", "-", "\n", "sampler", ".", "log_expected_count", "(", "target_classes", ")", ")", "\n", "sampled_log_denominator", "=", "tf", ".", "reduce_logsumexp", "(", "\n", "sampled_logits", ",", "[", "1", "]", ",", "name", "=", "'SampledLogDenominator'", ")", "\n", "sampled_classes_mask", "=", "tf", ".", "unsorted_segment_sum", "(", "\n", "tf", ".", "fill", "(", "tf", ".", "shape", "(", "sampled_classes", ")", ",", "float", "(", "'-inf'", ")", ")", ",", "sampled_classes", ",", "\n", "num_classes", ")", "\n", "target_log_denominator", "=", "(", "\n", "target_logits", "+", "tf", ".", "gather", "(", "sampled_classes_mask", ",", "target_classes", ")", ")", "\n", "combined_log_denominator", "=", "tf", ".", "reduce_logsumexp", "(", "\n", "tf", ".", "stack", "(", "[", "sampled_log_denominator", ",", "target_log_denominator", "]", ")", ",", "[", "0", "]", ")", "\n", "loss", "=", "combined_log_denominator", "-", "target_logits", "\n", "return", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.expert_utils.ParallelSampledSoftmaxLoss": [[1246, 1286], ["data_parallelism", "expert_utils.ParallelEmbeddingLookup", "expert_utils.ParallelEmbeddingLookup", "data_parallelism", "data_parallelism", "tensorflow.add_n", "data_parallelism"], "function", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.parallel.data_parallelism", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.expert_utils.ParallelEmbeddingLookup", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.expert_utils.ParallelEmbeddingLookup", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.parallel.data_parallelism", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.parallel.data_parallelism", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.parallel.data_parallelism"], ["", "def", "ParallelSampledSoftmaxLoss", "(", "params", ",", "\n", "features", ",", "\n", "target_classes", ",", "\n", "sampler", ",", "\n", "num_classes", ",", "\n", "data_parallelism", ",", "\n", "target_weights", "=", "None", ")", ":", "\n", "  ", "\"\"\"Computes sampled softmax loss across many datashards.\n\n  This is used during training to efficiently train a softmax classifier layer.\n\n  Args:\n    params: A list of num_param_shards Tensors, each with shape\n      [num_classes / num_param_shards, num_features].\n      The parameters are assumed to be mod-sharded by class.\n    features: a list of num_datashards Tensors, each with shape\n      [batch_size_i, num_features]\n    target_classes: A list of num_datashards integer Tensors each with shape\n       [batch_size_i]\n    sampler: a candidate sampler object\n    num_classes: an Integer\n    data_parallelism: a Parallelism object\n    target_weights: an optional list of num_datashards Tensors each with\n      shape [batch_size_i]\n  Returns:\n     a Scalar.\n  \"\"\"", "\n", "sampled_classes", "=", "data_parallelism", "(", "sampler", ".", "sample", ")", "\n", "sampled_params", "=", "ParallelEmbeddingLookup", "(", "params", ",", "sampled_classes", ",", "\n", "data_parallelism", ")", "\n", "target_params", "=", "ParallelEmbeddingLookup", "(", "params", ",", "target_classes", ",", "\n", "data_parallelism", ")", "\n", "ret", "=", "data_parallelism", "(", "SampledSoftmaxLoss", ",", "features", ",", "sampler", ",", "num_classes", ",", "\n", "target_classes", ",", "target_params", ",", "sampled_classes", ",", "\n", "sampled_params", ")", "\n", "if", "target_weights", "is", "not", "None", ":", "\n", "    ", "ret", "=", "data_parallelism", "(", "tf", ".", "multiply", ",", "ret", ",", "target_weights", ")", "\n", "", "ret", "=", "data_parallelism", "(", "tf", ".", "reduce_sum", ",", "ret", ")", "\n", "ret", "=", "tf", ".", "add_n", "(", "ret", ")", "\n", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.expert_utils.moe_layer": [[1288, 1364], ["tensorflow.variable_scope", "expert_utils.NoisyTopKGatingParams", "expert_utils.FeedForwardExpertParams", "expert_utils.DistributedMixtureOfExperts", "dp", "expert_utils.DistributedMixtureOfExperts.Eval", "dp", "expert_utils.NoisyTopKGatingParams", "dp", "expert_utils.CVSquared", "expert_utils.CVSquared"], "function", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.expert_utils.DistributedMixtureOfExperts.Eval", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.expert_utils.CVSquared", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.expert_utils.CVSquared"], ["", "def", "moe_layer", "(", "data_parallelism", ",", "\n", "ps_devices", ",", "\n", "xs", ",", "\n", "train", ",", "\n", "model_hidden_size", ",", "\n", "expert_hidden_size", ",", "\n", "n1", ",", "\n", "n2", ",", "\n", "loss_coef", ",", "\n", "num_active_experts", "=", "2", ",", "\n", "activation", "=", "'relu6'", ",", "\n", "output_activation", "=", "None", ",", "\n", "moe_relu_dropout", "=", "0.2", ",", "\n", "autoscale", "=", "True", ",", "\n", "name", "=", "None", ")", ":", "\n", "  ", "\"\"\"A mixture of experts layer.\n\n  Args:\n    data_parallelism: a expert_utils.Parallelism object.\n    ps_devices: a list of strings\n    xs: a list of input tensors.\n    train: a boolean scalar.\n    model_hidden_size: an integer (input/output size for this layer)\n    expert_hidden_size: an integer (size of each expert's hidden layer)\n    n1: an integer - number of experts (or # of groups for hierarchical MoE)\n    n2: optional integer - size of each group of experts for hierarchical MoE\n    loss_coef: a scalar - multiplier on load-balancing losses\n    autoscale: a boolean\n    name: a string\n\n  Returns:\n    ys: a list of tensors:\n    extra_training_loss: a scalar\n  \"\"\"", "\n", "dp", "=", "data_parallelism", "\n", "with", "tf", ".", "variable_scope", "(", "name", ",", "default_name", "=", "\"moe\"", ")", ":", "\n", "# Set up the hyperparameters for the gating networks.", "\n", "    ", "primary_gating_hp", "=", "NoisyTopKGatingParams", "(", ")", "\n", "primary_gating_hp", ".", "num_experts", "=", "n1", "\n", "primary_gating_hp", ".", "k", "=", "num_active_experts", "\n", "if", "n2", ":", "\n", "# hierarchical MoE containing moe_n1 groups of moe_n2 experts.", "\n", "      ", "assert", "n2", ">", "1", "\n", "secondary_gating_hp", "=", "NoisyTopKGatingParams", "(", ")", "\n", "secondary_gating_hp", ".", "num_experts", "=", "n2", "\n", "secondary_gating_hp", ".", "k", "=", "num_active_experts", "\n", "", "else", ":", "\n", "# flat mixture of moe_n1 experts.", "\n", "      ", "secondary_gating_hp", "=", "None", "\n", "# Set up the hyperparameters for the expert networks.", "\n", "# Each expert contains a hidden RELU layer of size filter_size", "\n", "", "expert_hp", "=", "FeedForwardExpertParams", "(", ")", "\n", "expert_hp", ".", "autoscale", "=", "autoscale", "\n", "expert_hp", ".", "relu_dropout", "=", "moe_relu_dropout", "\n", "expert_hp", ".", "hidden_layer_sizes", "=", "[", "expert_hidden_size", "]", "\n", "if", "activation", "==", "'elu'", ":", "\n", "      ", "expert_hp", ".", "hidden_activation", "=", "tf", ".", "nn", ".", "elu", "\n", "", "elif", "activation", "==", "'relu'", ":", "\n", "      ", "expert_hp", ".", "hidden_activation", "=", "tf", ".", "nn", ".", "relu", "\n", "", "if", "output_activation", "==", "'tanh'", ":", "\n", "      ", "expert_hp", ".", "output_activation", "=", "tf", ".", "tanh", "\n", "# Create the mixture of experts.", "\n", "", "moe", "=", "DistributedMixtureOfExperts", "(", "primary_gating_hp", ",", "secondary_gating_hp", ",", "\n", "expert_hp", ",", "model_hidden_size", ",", "\n", "model_hidden_size", ",", "ps_devices", ",", "\"moe\"", ")", "\n", "# MoE expects input tensors to be 2d.", "\n", "#  Flatten out spatial dimensions.", "\n", "xs_2d", "=", "dp", "(", "tf", ".", "reshape", ",", "xs", ",", "[", "[", "-", "1", ",", "model_hidden_size", "]", "]", "*", "dp", ".", "n", ")", "\n", "# Call the MoE", "\n", "moe_out_2d", ",", "importance", ",", "load", ",", "_", ",", "_", "=", "moe", ".", "Eval", "(", "\n", "dp", ".", "devices", ",", "xs_2d", ",", "train", ",", "identifiers", "=", "None", ",", "summaries", "=", "train", ")", "\n", "# Reshape the output to the original shape.", "\n", "moe_out", "=", "dp", "(", "tf", ".", "reshape", ",", "moe_out_2d", ",", "dp", "(", "tf", ".", "shape", ",", "xs", ")", ")", "\n", "# These losses encourage equal load on the different experts.", "\n", "loss", "=", "loss_coef", "*", "(", "CVSquared", "(", "importance", ")", "+", "CVSquared", "(", "load", ")", ")", "\n", "return", "moe_out", ",", "loss", "\n", "", "", ""]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.sampling._get_inference_fn": [[21, 51], ["zip", "tensorflow.pad", "tensorflow.fill", "tensorflow.add_n", "float", "model_fn", "outputs.append", "next_state.append", "model_fn", "outputs.append", "next_state.append", "len", "tensorflow.shape", "tensorflow.shape"], "function", ["None"], ["", "def", "_get_inference_fn", "(", "model_fns", ",", "features", ")", ":", "\n", "    ", "def", "inference_fn", "(", "inputs", ",", "state", ")", ":", "\n", "        ", "local_features", "=", "{", "\n", "\"source\"", ":", "features", "[", "\"source\"", "]", ",", "\n", "\"source_length\"", ":", "features", "[", "\"source_length\"", "]", ",", "\n", "# [bos_id, ...] => [..., 0]", "\n", "\"target\"", ":", "tf", ".", "pad", "(", "inputs", "[", ":", ",", "1", ":", "]", ",", "[", "[", "0", ",", "0", "]", ",", "[", "0", ",", "1", "]", "]", ")", ",", "\n", "\"target_length\"", ":", "tf", ".", "fill", "(", "[", "tf", ".", "shape", "(", "inputs", ")", "[", "0", "]", "]", ",", "\n", "tf", ".", "shape", "(", "inputs", ")", "[", "1", "]", ")", "\n", "}", "\n", "\n", "outputs", "=", "[", "]", "\n", "next_state", "=", "[", "]", "\n", "\n", "for", "(", "model_fn", ",", "model_state", ")", "in", "zip", "(", "model_fns", ",", "state", ")", ":", "\n", "            ", "if", "model_state", ":", "\n", "                ", "output", ",", "new_state", "=", "model_fn", "(", "local_features", ",", "model_state", ")", "\n", "outputs", ".", "append", "(", "output", ")", "\n", "next_state", ".", "append", "(", "new_state", ")", "\n", "", "else", ":", "\n", "                ", "output", "=", "model_fn", "(", "local_features", ")", "\n", "outputs", ".", "append", "(", "output", ")", "\n", "next_state", ".", "append", "(", "{", "}", ")", "\n", "\n", "# Ensemble", "\n", "", "", "log_prob", "=", "tf", ".", "add_n", "(", "outputs", ")", "/", "float", "(", "len", "(", "outputs", ")", ")", "\n", "\n", "return", "log_prob", ",", "next_state", "\n", "\n", "", "return", "inference_fn", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.sampling._sampling_step": [[53, 102], ["func", "tensorflow.one_hot", "thumt.tile_batch", "tensorflow.where", "tensorflow.multinomial", "tensorflow.squeeze", "tensorflow.squeeze", "tensorflow.logical_or", "tensorflow.where", "tensorflow.where", "tensorflow.where", "tensorflow.squeeze", "tensorflow.fill", "tensorflow.logical_and", "tensorflow.logical_or", "tensorflow.where", "tensorflow.where", "sampling.SamplerState", "tensorflow.shape", "tensorflow.reshape", "tensorflow.zeros_like", "thumt.gather_2d", "tensorflow.equal", "tensorflow.fill", "tensorflow.zeros", "tensorflow.ones", "tensorflow.zeros", "thumt.gather_2d", "tensorflow.logical_not", "tensorflow.shape", "tensorflow.fill", "tensorflow.concat"], "function", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.common.tile_batch", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.common.gather_2d", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.common.gather_2d"], ["", "def", "_sampling_step", "(", "time", ",", "func", ",", "state", ",", "min_length", ",", "max_length", ",", "pad_id", ",", "eos_id", ")", ":", "\n", "# Compute log probabilities", "\n", "    ", "seqs", "=", "state", ".", "inputs", "\n", "# [batch_size * num_samples, vocab_size]", "\n", "# func is decoding_fn", "\n", "step_log_probs", ",", "next_state", "=", "func", "(", "seqs", ",", "state", ".", "state", ")", "\n", "\n", "# Suppress <eos> if needed", "\n", "# When time<min_length, suppress <eos>", "\n", "# batch_size [batch_size * num_samples, vocab_size] ", "\n", "batch_size", "=", "tf", ".", "shape", "(", "step_log_probs", ")", "[", "0", "]", "\n", "vocab_size", "=", "step_log_probs", ".", "shape", "[", "-", "1", "]", ".", "value", "or", "tf", ".", "shape", "(", "step_log_probs", ")", "[", "1", "]", "\n", "add_mask", "=", "tf", ".", "one_hot", "(", "eos_id", ",", "vocab_size", ",", "dtype", "=", "step_log_probs", ".", "dtype", ",", "\n", "on_value", "=", "step_log_probs", ".", "dtype", ".", "min", ",", "\n", "off_value", "=", "0.0", ")", "\n", "add_mask", "=", "utils", ".", "tile_batch", "(", "tf", ".", "reshape", "(", "add_mask", ",", "[", "1", ",", "-", "1", "]", ")", ",", "batch_size", ")", "\n", "add_mask", "=", "tf", ".", "where", "(", "time", "<", "min_length", ",", "add_mask", ",", "tf", ".", "zeros_like", "(", "add_mask", ")", ")", "\n", "step_log_probs", "=", "step_log_probs", "+", "add_mask", "\n", "\n", "# sample from distribution", "\n", "symbol_indices", "=", "tf", ".", "multinomial", "(", "step_log_probs", ",", "1", ",", "output_dtype", "=", "tf", ".", "int32", ")", "\n", "symbol_scores", "=", "tf", ".", "squeeze", "(", "utils", ".", "gather_2d", "(", "step_log_probs", ",", "symbol_indices", ")", ",", "\n", "axis", "=", "1", ")", "\n", "# when sample the <eos>, curr_flags=True", "\n", "curr_flags", "=", "tf", ".", "squeeze", "(", "tf", ".", "equal", "(", "symbol_indices", ",", "eos_id", ")", ",", "axis", "=", "1", ")", "\n", "# seq after <eos> curr_flags=True", "\n", "curr_flags", "=", "tf", ".", "logical_or", "(", "state", ".", "flags", ",", "curr_flags", ")", "\n", "\n", "# Append <pad> to finished samples", "\n", "symbol_indices", "=", "tf", ".", "where", "(", "state", ".", "flags", ",", "tf", ".", "fill", "(", "[", "batch_size", ",", "1", "]", ",", "pad_id", ")", ",", "\n", "symbol_indices", ")", "\n", "symbol_scores", "=", "tf", ".", "where", "(", "state", ".", "flags", ",", "tf", ".", "zeros", "(", "[", "batch_size", "]", ")", ",", "\n", "symbol_scores", ")", "\n", "\n", "# Force sampler to generate <eos> if length exceed max_length", "\n", "eos_flags", "=", "tf", ".", "where", "(", "time", ">", "max_length", ",", "tf", ".", "ones", "(", "[", "batch_size", "]", ",", "tf", ".", "bool", ")", ",", "\n", "tf", ".", "zeros", "(", "[", "batch_size", "]", ",", "tf", ".", "bool", ")", ")", "\n", "eos_scores", "=", "tf", ".", "squeeze", "(", "utils", ".", "gather_2d", "(", "step_log_probs", ",", "\n", "tf", ".", "fill", "(", "[", "batch_size", ",", "1", "]", ",", "eos_id", ")", ")", ",", "\n", "axis", "=", "1", ")", "\n", "eos_indices", "=", "tf", ".", "fill", "(", "[", "batch_size", ",", "1", "]", ",", "eos_id", ")", "\n", "# not sample the <eos>, but exceed max_length", "\n", "cond", "=", "tf", ".", "logical_and", "(", "tf", ".", "logical_not", "(", "curr_flags", ")", ",", "eos_flags", ")", "\n", "curr_flags", "=", "tf", ".", "logical_or", "(", "curr_flags", ",", "eos_flags", ")", "\n", "symbol_indices", "=", "tf", ".", "where", "(", "cond", ",", "eos_indices", ",", "symbol_indices", ")", "\n", "symbol_scores", "=", "tf", ".", "where", "(", "cond", ",", "eos_scores", ",", "symbol_scores", ")", "\n", "\n", "new_state", "=", "SamplerState", "(", "\n", "inputs", "=", "tf", ".", "concat", "(", "[", "seqs", ",", "symbol_indices", "]", ",", "axis", "=", "1", ")", ",", "\n", "state", "=", "next_state", ",", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.sampling.random_sample": [[104, 149], ["tensorflow.fill", "tensorflow.zeros", "tensorflow.zeros", "sampling.SamplerState", "tensorflow.reduce_max", "tensorflow.constant", "sampling.SamplerState", "tensorflow.while_loop", "tensorflow.reduce_all", "tensorflow.logical_and", "sampling._sampling_step", "tensorflow.less", "tensorflow.logical_not", "tensorflow.TensorShape", "tensorflow.python.util.nest.map_structure", "tensorflow.TensorShape", "tensorflow.TensorShape", "tensorflow.TensorShape"], "function", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.sampling._sampling_step"], ["flags", "=", "curr_flags", "\n", ")", "\n", "\n", "return", "time", "+", "1", ",", "new_state", "\n", "\n", "\n", "", "def", "random_sample", "(", "func", ",", "state", ",", "batch_size", ",", "min_length", ",", "max_length", ",", "pad_id", ",", "\n", "bos_id", ",", "eos_id", ")", ":", "\n", "    ", "init_seqs", "=", "tf", ".", "fill", "(", "[", "batch_size", ",", "1", "]", ",", "bos_id", ")", "\n", "init_scores", "=", "tf", ".", "zeros", "(", "[", "batch_size", "]", ")", "\n", "init_flags", "=", "tf", ".", "zeros", "(", "[", "batch_size", "]", ",", "tf", ".", "bool", ")", "\n", "\n", "# state.inputs record the infered seq.", "\n", "state", "=", "SamplerState", "(", "\n", "inputs", "=", "init_seqs", ",", "\n", "state", "=", "state", ",", "\n", "scores", "=", "init_scores", ",", "\n", "flags", "=", "init_flags", "\n", ")", "\n", "\n", "max_step", "=", "tf", ".", "reduce_max", "(", "max_length", ")", "\n", "# When True, the loop will continue", "\n", "def", "_is_finished", "(", "t", ",", "s", ")", ":", "\n", "        ", "all_finished", "=", "tf", ".", "reduce_all", "(", "s", ".", "flags", ")", "\n", "cond", "=", "tf", ".", "logical_and", "(", "tf", ".", "less", "(", "t", ",", "max_step", ")", ",", "\n", "tf", ".", "logical_not", "(", "all_finished", ")", ")", "\n", "\n", "return", "cond", "\n", "\n", "", "def", "_loop_fn", "(", "t", ",", "s", ")", ":", "\n", "        ", "outs", "=", "_sampling_step", "(", "t", ",", "func", ",", "s", ",", "min_length", ",", "max_length", ",", "pad_id", ",", "\n", "eos_id", ")", "\n", "return", "outs", "\n", "\n", "", "time", "=", "tf", ".", "constant", "(", "0", ",", "name", "=", "\"time\"", ")", "\n", "# shape_invaariants enforces shape invariants for the loop variables.", "\n", "#An error will be raised if the shape of a loop variable after an ", "\n", "#iteration is determined to be more general than or incompatible with ", "\n", "#its shape invariant.  Using tf.TensorShape class to assign the shape. ", "\n", "shape_invariants", "=", "SamplerState", "(", "\n", "inputs", "=", "tf", ".", "TensorShape", "(", "[", "None", ",", "None", "]", ")", ",", "\n", "state", "=", "nest", ".", "map_structure", "(", "utils", ".", "infer_shape_invariants", ",", "state", ".", "state", ")", ",", "\n", "scores", "=", "tf", ".", "TensorShape", "(", "[", "None", "]", ")", ",", "\n", "flags", "=", "tf", ".", "TensorShape", "(", "[", "None", "]", ")", "\n", ")", "\n", "outputs", "=", "tf", ".", "while_loop", "(", "_is_finished", ",", "_loop_fn", ",", "[", "time", ",", "state", "]", ",", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.sampling.create_sampling_graph": [[151, 215], ["copy.copy", "thumt.tile_batch", "thumt.tile_batch", "tensorflow.to_float", "tensorflow.to_float", "tensorflow.to_int32", "tensorflow.to_int32", "sampling._get_inference_fn", "tensorflow.python.util.nest.map_structure", "sampling.random_sample", "tensorflow.reshape", "tensorflow.reshape", "isinstance", "ValueError", "model.get_inference_func", "callable", "tensorflow.shape", "nest.map_structure.append", "funcs.append", "nest.map_structure.append", "funcs.append", "thumt.tile_batch"], "function", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.common.tile_batch", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.common.tile_batch", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.sampling._get_inference_fn", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.sampling.random_sample", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.transformer_lrp.TransformerLRP.get_inference_func", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.common.tile_batch"], ["shape_invariants", "]", ",", "\n", "parallel_iterations", "=", "1", ",", "\n", "back_prop", "=", "False", ")", "\n", "\n", "final_state", "=", "outputs", "[", "1", "]", "\n", "final_seqs", "=", "final_state", ".", "inputs", "\n", "final_scores", "=", "final_state", ".", "scores", "\n", "\n", "return", "final_seqs", ",", "final_scores", "\n", "\n", "\n", "", "def", "create_sampling_graph", "(", "models", ",", "features", ",", "params", ")", ":", "\n", "    ", "if", "not", "isinstance", "(", "models", ",", "(", "list", ",", "tuple", ")", ")", ":", "\n", "        ", "raise", "ValueError", "(", "\"'models' must be a list or tuple\"", ")", "\n", "\n", "", "features", "=", "copy", ".", "copy", "(", "features", ")", "\n", "model_fns", "=", "[", "model", ".", "get_inference_func", "(", ")", "for", "model", "in", "models", "]", "\n", "\n", "# Expand times(tile batch) for sampling", "\n", "num_samples", "=", "params", ".", "num_samples", "\n", "\n", "# Compute initial state if necessary", "\n", "states", "=", "[", "]", "\n", "funcs", "=", "[", "]", "\n", "\n", "for", "model_fn", "in", "model_fns", ":", "\n", "        ", "if", "callable", "(", "model_fn", ")", ":", "\n", "# For non-incremental decoding", "\n", "            ", "states", ".", "append", "(", "{", "}", ")", "\n", "funcs", ".", "append", "(", "model_fn", ")", "\n", "", "else", ":", "\n", "# For incremental decoding where model_fn is a tuple:", "\n", "# (encoding_fn, decoding_fn)", "\n", "            ", "states", ".", "append", "(", "model_fn", "[", "0", "]", "(", "features", ")", ")", "\n", "funcs", ".", "append", "(", "model_fn", "[", "1", "]", ")", "\n", "\n", "", "", "batch_size", "=", "tf", ".", "shape", "(", "features", "[", "\"source\"", "]", ")", "[", "0", "]", "\n", "pad_id", "=", "params", ".", "mapping", "[", "\"target\"", "]", "[", "params", ".", "pad", "]", "\n", "bos_id", "=", "params", ".", "mapping", "[", "\"target\"", "]", "[", "params", ".", "bos", "]", "\n", "eos_id", "=", "params", ".", "mapping", "[", "\"target\"", "]", "[", "params", ".", "eos", "]", "\n", "\n", "# Expand the inputs", "\n", "# [batch_size*num_samples, length] after tile_batch ", "\n", "features", "[", "\"source\"", "]", "=", "utils", ".", "tile_batch", "(", "features", "[", "\"source\"", "]", ",", "num_samples", ")", "\n", "features", "[", "\"source_length\"", "]", "=", "utils", ".", "tile_batch", "(", "features", "[", "\"source_length\"", "]", ",", "\n", "num_samples", ")", "\n", "\n", "min_length", "=", "tf", ".", "to_float", "(", "features", "[", "\"source_length\"", "]", ")", "\n", "max_length", "=", "tf", ".", "to_float", "(", "features", "[", "\"source_length\"", "]", ")", "\n", "\n", "if", "params", ".", "min_length_ratio", ":", "\n", "        ", "min_length", "=", "min_length", "*", "params", ".", "min_length_ratio", "\n", "\n", "", "if", "params", ".", "max_length_ratio", ":", "\n", "        ", "max_length", "=", "max_length", "*", "params", ".", "max_length_ratio", "\n", "\n", "", "if", "params", ".", "min_sample_length", ":", "\n", "        ", "min_length", "=", "min_length", "-", "params", ".", "min_sample_length", "\n", "\n", "", "if", "params", ".", "max_sample_length", ":", "\n", "        ", "max_length", "=", "max_length", "+", "params", ".", "max_sample_length", "\n", "\n", "", "min_length", "=", "tf", ".", "to_int32", "(", "min_length", ")", "\n", "max_length", "=", "tf", ".", "to_int32", "(", "max_length", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.optimizers.StaticLossScalingOptimizer.__init__": [[15, 20], ["super().__init__"], "methods", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.transformer_lrp.TransformerLRP.__init__"], ["    ", "def", "__init__", "(", "self", ",", "optimizer", ",", "scale", "=", "128.0", ",", "use_locking", "=", "False", ",", "\n", "name", "=", "\"StaticLossScalingOptimizer\"", ")", ":", "\n", "        ", "super", "(", "StaticLossScalingOptimizer", ",", "self", ")", ".", "__init__", "(", "use_locking", ",", "name", ")", "\n", "self", ".", "_optimizer", "=", "optimizer", "\n", "self", ".", "_scale", "=", "scale", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.optimizers.StaticLossScalingOptimizer.compute_gradients": [[21, 42], ["optimizers.StaticLossScalingOptimizer._optimizer.compute_gradients", "isinstance", "scaled_grads_and_vars.append", "tensorflow.IndexedSlices", "isinstance"], "methods", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.optimizers.MultiStepOptimizer.compute_gradients"], ["", "def", "compute_gradients", "(", "self", ",", "loss", ",", "var_list", "=", "None", ",", "\n", "gate_gradients", "=", "tf", ".", "train", ".", "Optimizer", ".", "GATE_OP", ",", "\n", "aggregation_method", "=", "None", ",", "\n", "colocate_gradients_with_ops", "=", "False", ",", "\n", "grad_loss", "=", "None", ")", ":", "\n", "        ", "grads_and_vars", "=", "self", ".", "_optimizer", ".", "compute_gradients", "(", "\n", "loss", "*", "self", ".", "_scale", ",", "var_list", ",", "gate_gradients", ",", "\n", "aggregation_method", ",", "colocate_gradients_with_ops", ",", "grad_loss", ")", "\n", "\n", "scaled_grads_and_vars", "=", "[", "]", "\n", "\n", "for", "grad", ",", "var", "in", "grads_and_vars", ":", "\n", "            ", "if", "isinstance", "(", "grad", ",", "tf", ".", "IndexedSlices", ")", ":", "\n", "                ", "grad", "=", "tf", ".", "IndexedSlices", "(", "grad", ".", "values", "/", "self", ".", "_scale", ",", "\n", "grad", ".", "indices", ",", "grad", ".", "dense_shape", ")", "\n", "", "elif", "isinstance", "(", "grad", ",", "tf", ".", "Tensor", ")", ":", "\n", "                ", "grad", "=", "grad", "/", "self", ".", "_scale", "\n", "\n", "", "scaled_grads_and_vars", ".", "append", "(", "(", "grad", ",", "var", ")", ")", "\n", "\n", "", "return", "scaled_grads_and_vars", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.optimizers.StaticLossScalingOptimizer.apply_gradients": [[43, 46], ["optimizers.StaticLossScalingOptimizer._optimizer.apply_gradients"], "methods", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.optimizers.MultiStepOptimizer.apply_gradients"], ["", "def", "apply_gradients", "(", "self", ",", "grads_and_vars", ",", "global_step", "=", "None", ",", "name", "=", "None", ")", ":", "\n", "        ", "return", "self", ".", "_optimizer", ".", "apply_gradients", "(", "grads_and_vars", ",", "global_step", ",", "\n", "name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.optimizers.LossScalingOptimizer.__init__": [[50, 68], ["super().__init__", "tensorflow.convert_to_tensor", "tensorflow.convert_to_tensor", "tensorflow.convert_to_tensor", "tensorflow.convert_to_tensor", "tensorflow.convert_to_tensor"], "methods", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.transformer_lrp.TransformerLRP.__init__"], ["    ", "def", "__init__", "(", "self", ",", "optimizer", ",", "scale", "=", "2.0", "**", "15", ",", "scale_factor", "=", "2.0", ",", "\n", "scale_window", "=", "2000", ",", "tolerance", "=", "0.05", ",", "threshold", "=", "None", ",", "\n", "use_locking", "=", "False", ",", "name", "=", "\"LossScalingOptimizer\"", ")", ":", "\n", "        ", "super", "(", "LossScalingOptimizer", ",", "self", ")", ".", "__init__", "(", "use_locking", ",", "name", ")", "\n", "self", ".", "_optimizer", "=", "optimizer", "\n", "self", ".", "_scale", "=", "tf", ".", "convert_to_tensor", "(", "scale", ",", "dtype", "=", "tf", ".", "float32", ")", "\n", "self", ".", "_scale_factor", "=", "tf", ".", "convert_to_tensor", "(", "scale_factor", ",", "\n", "dtype", "=", "tf", ".", "float32", ")", "\n", "self", ".", "_scale_window", "=", "tf", ".", "convert_to_tensor", "(", "scale_window", ",", "\n", "dtype", "=", "tf", ".", "int32", ")", "\n", "self", ".", "_tolerance", "=", "tf", ".", "convert_to_tensor", "(", "tolerance", ",", "\n", "dtype", "=", "tf", ".", "float32", ")", "\n", "\n", "if", "threshold", ":", "\n", "            ", "self", ".", "_threshold", "=", "tf", ".", "convert_to_tensor", "(", "threshold", ",", "\n", "dtype", "=", "tf", ".", "float32", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "_threshold", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.optimizers.LossScalingOptimizer.compute_gradients": [[69, 93], ["optimizers.LossScalingOptimizer._create_non_slot_variable", "optimizers.LossScalingOptimizer._optimizer.compute_gradients", "isinstance", "scaled_grads_and_vars.append", "tensorflow.IndexedSlices", "isinstance"], "methods", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.optimizers.MultiStepOptimizer.compute_gradients"], ["", "", "def", "compute_gradients", "(", "self", ",", "loss", ",", "var_list", "=", "None", ",", "\n", "gate_gradients", "=", "tf", ".", "train", ".", "Optimizer", ".", "GATE_OP", ",", "\n", "aggregation_method", "=", "None", ",", "\n", "colocate_gradients_with_ops", "=", "False", ",", "\n", "grad_loss", "=", "None", ")", ":", "\n", "        ", "scale_var", "=", "self", ".", "_create_non_slot_variable", "(", "\n", "initial_value", "=", "self", ".", "_scale", ",", "name", "=", "\"scale\"", ",", "colocate_with", "=", "loss", ")", "\n", "\n", "grads_and_vars", "=", "self", ".", "_optimizer", ".", "compute_gradients", "(", "\n", "loss", "*", "scale_var", ",", "var_list", ",", "gate_gradients", ",", "\n", "aggregation_method", ",", "colocate_gradients_with_ops", ",", "grad_loss", ")", "\n", "\n", "scaled_grads_and_vars", "=", "[", "]", "\n", "\n", "for", "grad", ",", "var", "in", "grads_and_vars", ":", "\n", "            ", "if", "isinstance", "(", "grad", ",", "tf", ".", "IndexedSlices", ")", ":", "\n", "                ", "grad", "=", "tf", ".", "IndexedSlices", "(", "grad", ".", "values", "/", "scale_var", ",", "\n", "grad", ".", "indices", ",", "grad", ".", "dense_shape", ")", "\n", "", "elif", "isinstance", "(", "grad", ",", "tf", ".", "Tensor", ")", ":", "\n", "                ", "grad", "=", "grad", "/", "scale_var", "\n", "\n", "", "scaled_grads_and_vars", ".", "append", "(", "(", "grad", ",", "var", ")", ")", "\n", "\n", "", "return", "scaled_grads_and_vars", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.optimizers.LossScalingOptimizer.apply_gradients": [[94, 173], ["list", "tensorflow.global_norm", "tensorflow.logical_not", "list", "optimizers.LossScalingOptimizer._optimizer.apply_gradients", "min", "optimizers.LossScalingOptimizer._create_non_slot_variable", "optimizers.LossScalingOptimizer._create_non_slot_variable", "optimizers.LossScalingOptimizer._create_non_slot_variable", "optimizers.LossScalingOptimizer._create_non_slot_variable", "optimizers.LossScalingOptimizer._get_non_slot_variable", "tensorflow.assign_add", "tensorflow.cond", "tensorflow.cond", "tensorflow.group", "zip", "tensorflow.is_finite", "new_grads.append", "zip", "tensorflow.get_default_graph", "tensorflow.assign", "tensorflow.assign", "tensorflow.group", "tensorflow.assign", "tensorflow.assign", "tensorflow.assign", "tensorflow.group", "tensorflow.control_dependencies", "tensorflow.div", "tensorflow.cond", "tensorflow.cond", "tensorflow.cond", "tensorflow.assign_add", "tensorflow.assign", "tensorflow.assign", "tensorflow.cast", "tensorflow.cast", "tensorflow.logical_and", "tensorflow.logical_and", "tensorflow.maximum", "tensorflow.greater", "tensorflow.no_op", "tensorflow.logical_not", "tensorflow.equal", "tensorflow.no_op", "tensorflow.zeros_like"], "methods", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.optimizers.MultiStepOptimizer.apply_gradients"], ["", "def", "apply_gradients", "(", "self", ",", "grads_and_vars", ",", "global_step", "=", "None", ",", "name", "=", "None", ")", ":", "\n", "        ", "grads", ",", "var_list", "=", "list", "(", "zip", "(", "*", "grads_and_vars", ")", ")", "\n", "grad_norm", "=", "tf", ".", "global_norm", "(", "grads", ")", "\n", "new_grads", "=", "[", "]", "\n", "\n", "is_overflow", "=", "tf", ".", "logical_not", "(", "tf", ".", "is_finite", "(", "grad_norm", ")", ")", "\n", "\n", "for", "grad", "in", "grads", ":", "\n", "            ", "if", "grad", "is", "not", "None", ":", "\n", "                ", "grad", "=", "tf", ".", "cond", "(", "is_overflow", ",", "\n", "lambda", ":", "tf", ".", "zeros_like", "(", "grad", ")", ",", "lambda", ":", "grad", ")", "\n", "", "new_grads", ".", "append", "(", "grad", ")", "\n", "\n", "", "grads_and_vars", "=", "list", "(", "zip", "(", "new_grads", ",", "var_list", ")", ")", "\n", "update_op", "=", "self", ".", "_optimizer", ".", "apply_gradients", "(", "grads_and_vars", ",", "\n", "global_step", ",", "name", ")", "\n", "\n", "first_var", "=", "min", "(", "var_list", ",", "key", "=", "lambda", "x", ":", "x", ".", "name", ")", "\n", "iter_var", "=", "self", ".", "_create_non_slot_variable", "(", "\n", "initial_value", "=", "0", ",", "name", "=", "\"iter\"", ",", "colocate_with", "=", "first_var", ")", "\n", "rescale_iter", "=", "self", ".", "_create_non_slot_variable", "(", "\n", "initial_value", "=", "0", ",", "name", "=", "\"rescale_iter\"", ",", "colocate_with", "=", "first_var", ")", "\n", "overflow_iter", "=", "self", ".", "_create_non_slot_variable", "(", "\n", "initial_value", "=", "-", "1", ",", "name", "=", "\"overflow_iter\"", ",", "colocate_with", "=", "first_var", ")", "\n", "overflow_var", "=", "self", ".", "_create_non_slot_variable", "(", "\n", "initial_value", "=", "0", ",", "name", "=", "\"overflow\"", ",", "colocate_with", "=", "first_var", ")", "\n", "scale_var", "=", "self", ".", "_get_non_slot_variable", "(", "\"scale\"", ",", "\n", "tf", ".", "get_default_graph", "(", ")", ")", "\n", "\n", "iter_op", "=", "tf", ".", "assign_add", "(", "iter_var", ",", "1", ")", "\n", "overflow_count_op", "=", "tf", ".", "cond", "(", "\n", "is_overflow", ",", "\n", "lambda", ":", "tf", ".", "assign_add", "(", "overflow_var", ",", "1", ")", ",", "\n", "lambda", ":", "overflow_var", ")", "\n", "overflow_iter_op", "=", "tf", ".", "cond", "(", "\n", "is_overflow", ",", "\n", "lambda", ":", "tf", ".", "assign", "(", "overflow_iter", ",", "iter_var", ")", ",", "\n", "lambda", ":", "iter_var", ")", "\n", "\n", "def", "increase_scale", "(", ")", ":", "\n", "            ", "scale_op", "=", "tf", ".", "assign", "(", "scale_var", ",", "scale_var", "*", "self", ".", "_scale_factor", ",", "\n", "use_locking", "=", "self", ".", "_use_locking", ")", "\n", "iter_op", "=", "tf", ".", "assign", "(", "rescale_iter", ",", "iter_var", ",", "\n", "use_locking", "=", "self", ".", "_use_locking", ")", "\n", "return", "tf", ".", "group", "(", "*", "[", "scale_op", ",", "iter_op", "]", ")", "\n", "\n", "", "def", "decrease_scale", "(", ")", ":", "\n", "            ", "scale_op", "=", "tf", ".", "assign", "(", "scale_var", ",", "scale_var", "/", "self", ".", "_scale_factor", ",", "\n", "use_locking", "=", "self", ".", "_use_locking", ")", "\n", "if", "self", ".", "_threshold", "is", "not", "None", ":", "\n", "                ", "scale_op", "=", "tf", ".", "assign", "(", "scale_op", ",", "tf", ".", "maximum", "(", "scale_var", ",", "\n", "self", ".", "_threshold", ")", ")", "\n", "\n", "", "iter_op", "=", "tf", ".", "assign", "(", "rescale_iter", ",", "iter_var", ",", "\n", "use_locking", "=", "self", ".", "_use_locking", ")", "\n", "overflow_op", "=", "tf", ".", "assign", "(", "overflow_var", ",", "0", ")", "\n", "\n", "return", "tf", ".", "group", "(", "*", "[", "scale_op", ",", "iter_op", ",", "overflow_op", "]", ")", "\n", "\n", "", "with", "tf", ".", "control_dependencies", "(", "[", "overflow_count_op", ",", "overflow_iter_op", "]", ")", ":", "\n", "            ", "percentage", "=", "tf", ".", "div", "(", "tf", ".", "cast", "(", "overflow_var", ",", "tf", ".", "float32", ")", ",", "\n", "tf", ".", "cast", "(", "iter_var", "-", "rescale_iter", ",", "tf", ".", "float32", ")", ")", "\n", "decrease_scale_op", "=", "tf", ".", "cond", "(", "\n", "tf", ".", "logical_and", "(", "is_overflow", ",", "\n", "tf", ".", "greater", "(", "percentage", ",", "self", ".", "_tolerance", ")", ")", ",", "\n", "decrease_scale", ",", "lambda", ":", "tf", ".", "no_op", "(", ")", ")", "\n", "increase_scale_op", "=", "tf", ".", "cond", "(", "\n", "tf", ".", "logical_and", "(", "\n", "tf", ".", "logical_not", "(", "is_overflow", ")", ",", "\n", "tf", ".", "equal", "(", "\n", "(", "iter_var", "-", "overflow_iter", ")", "%", "self", ".", "_scale_window", ",", "0", ")", "\n", ")", ",", "increase_scale", ",", "lambda", ":", "tf", ".", "no_op", "(", ")", ")", "\n", "\n", "", "ops", "=", "[", "\n", "update_op", ",", "iter_op", ",", "overflow_count_op", ",", "overflow_iter_op", ",", "\n", "increase_scale_op", ",", "decrease_scale_op", "\n", "]", "\n", "\n", "return", "tf", ".", "group", "(", "*", "ops", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.optimizers.MultiStepOptimizer.__init__": [[177, 183], ["super().__init__", "tensorflow.convert_to_tensor"], "methods", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.transformer_lrp.TransformerLRP.__init__"], ["    ", "def", "__init__", "(", "self", ",", "optimizer", ",", "step", "=", "1", ",", "use_locking", "=", "False", ",", "\n", "name", "=", "\"MultiStepOptimizer\"", ")", ":", "\n", "        ", "super", "(", "MultiStepOptimizer", ",", "self", ")", ".", "__init__", "(", "use_locking", ",", "name", ")", "\n", "self", ".", "_optimizer", "=", "optimizer", "\n", "self", ".", "_step", "=", "step", "\n", "self", ".", "_step_t", "=", "tf", ".", "convert_to_tensor", "(", "step", ",", "name", "=", "\"step\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.optimizers.MultiStepOptimizer._all_reduce": [[184, 193], ["tensorflow.name_scope", "isinstance", "thumt.utils.distribute.all_reduce", "tensorflow.convert_to_tensor"], "methods", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.distribute.all_reduce"], ["", "def", "_all_reduce", "(", "self", ",", "tensor", ")", ":", "\n", "        ", "with", "tf", ".", "name_scope", "(", "self", ".", "_name", "+", "\"_Allreduce\"", ")", ":", "\n", "            ", "if", "tensor", "is", "None", ":", "\n", "                ", "return", "tensor", "\n", "\n", "", "if", "isinstance", "(", "tensor", ",", "tf", ".", "IndexedSlices", ")", ":", "\n", "                ", "tensor", "=", "tf", ".", "convert_to_tensor", "(", "tensor", ")", "\n", "\n", "", "return", "all_reduce", "(", "tensor", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.optimizers.MultiStepOptimizer.compute_gradients": [[194, 237], ["optimizers.MultiStepOptimizer._optimizer.compute_gradients", "list", "min", "optimizers.MultiStepOptimizer._create_non_slot_variable", "zip", "list", "zip", "list", "optimizers.MultiStepOptimizer._zeros_slot", "isinstance", "tensorflow.cond", "new_grads.append", "zip", "optimizers.MultiStepOptimizer._all_reduce", "zip", "tensorflow.scatter_add", "tensorflow.assign_add", "optimizers.MultiStepOptimizer._all_reduce", "tensorflow.equal"], "methods", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.optimizers.MultiStepOptimizer.compute_gradients", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.optimizers.MultiStepOptimizer._all_reduce", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.optimizers.MultiStepOptimizer._all_reduce"], ["", "", "def", "compute_gradients", "(", "self", ",", "loss", ",", "var_list", "=", "None", ",", "\n", "gate_gradients", "=", "tf", ".", "train", ".", "Optimizer", ".", "GATE_OP", ",", "\n", "aggregation_method", "=", "None", ",", "\n", "colocate_gradients_with_ops", "=", "False", ",", "\n", "grad_loss", "=", "None", ")", ":", "\n", "        ", "grads_and_vars", "=", "self", ".", "_optimizer", ".", "compute_gradients", "(", "loss", ",", "var_list", ",", "\n", "gate_gradients", ",", "aggregation_method", ",", "colocate_gradients_with_ops", ",", "\n", "grad_loss", ")", "\n", "\n", "grads", ",", "var_list", "=", "list", "(", "zip", "(", "*", "grads_and_vars", ")", ")", "\n", "\n", "# Do not create extra variables when step is 1", "\n", "if", "self", ".", "_step", "==", "1", ":", "\n", "            ", "grads", "=", "[", "self", ".", "_all_reduce", "(", "t", ")", "for", "t", "in", "grads", "]", "\n", "return", "list", "(", "zip", "(", "grads", ",", "var_list", ")", ")", "\n", "\n", "", "first_var", "=", "min", "(", "var_list", ",", "key", "=", "lambda", "x", ":", "x", ".", "name", ")", "\n", "iter_var", "=", "self", ".", "_create_non_slot_variable", "(", "\n", "initial_value", "=", "0", "if", "self", ".", "_step", "==", "1", "else", "1", ",", "name", "=", "\"iter\"", ",", "\n", "colocate_with", "=", "first_var", ")", "\n", "\n", "new_grads", "=", "[", "]", "\n", "\n", "for", "grad", ",", "var", "in", "zip", "(", "grads", ",", "var_list", ")", ":", "\n", "            ", "grad_acc", "=", "self", ".", "_zeros_slot", "(", "var", ",", "\"grad_acc\"", ",", "self", ".", "_name", ")", "\n", "\n", "if", "isinstance", "(", "grad", ",", "tf", ".", "IndexedSlices", ")", ":", "\n", "                ", "grad_acc", "=", "tf", ".", "scatter_add", "(", "grad_acc", ",", "grad", ".", "indices", ",", "grad", ".", "values", ",", "\n", "use_locking", "=", "self", ".", "_use_locking", ")", "\n", "", "else", ":", "\n", "                ", "grad_acc", "=", "tf", ".", "assign_add", "(", "grad_acc", ",", "grad", ",", "\n", "use_locking", "=", "self", ".", "_use_locking", ")", "\n", "\n", "", "def", "_acc_grad", "(", ")", ":", "\n", "                ", "return", "grad_acc", "\n", "\n", "", "def", "_avg_grad", "(", ")", ":", "\n", "                ", "return", "self", ".", "_all_reduce", "(", "grad_acc", "/", "self", ".", "_step", ")", "\n", "\n", "", "grad", "=", "tf", ".", "cond", "(", "tf", ".", "equal", "(", "iter_var", ",", "0", ")", ",", "_avg_grad", ",", "_acc_grad", ")", "\n", "new_grads", ".", "append", "(", "grad", ")", "\n", "\n", "", "return", "list", "(", "zip", "(", "new_grads", ",", "var_list", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.optimizers.MultiStepOptimizer.apply_gradients": [[238, 270], ["list", "optimizers.MultiStepOptimizer._get_non_slot_variable", "tensorflow.cond", "tensorflow.group", "optimizers.MultiStepOptimizer._optimizer.apply_gradients", "zip", "tensorflow.group", "optimizers.MultiStepOptimizer._optimizer.apply_gradients", "tensorflow.group", "tensorflow.get_default_graph", "tensorflow.equal", "tensorflow.control_dependencies", "optimizers.MultiStepOptimizer.assign", "zip", "tensorflow.control_dependencies", "tensorflow.group", "tensorflow.mod", "optimizers.MultiStepOptimizer.get_slot", "zero_ops.append", "optimizers.MultiStepOptimizer.assign", "tensorflow.zeros_like"], "methods", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.optimizers.MultiStepOptimizer.apply_gradients", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.optimizers.MultiStepOptimizer.apply_gradients"], ["", "def", "apply_gradients", "(", "self", ",", "grads_and_vars", ",", "global_step", "=", "None", ",", "name", "=", "None", ")", ":", "\n", "        ", "if", "self", ".", "_step", "==", "1", ":", "\n", "            ", "return", "self", ".", "_optimizer", ".", "apply_gradients", "(", "grads_and_vars", ",", "global_step", ",", "\n", "name", "=", "name", ")", "\n", "\n", "", "grads", ",", "var_list", "=", "list", "(", "zip", "(", "*", "grads_and_vars", ")", ")", "\n", "\n", "def", "_pass_gradients", "(", ")", ":", "\n", "            ", "return", "tf", ".", "group", "(", "*", "grads", ")", "\n", "\n", "", "def", "_apply_gradients", "(", ")", ":", "\n", "            ", "op", "=", "self", ".", "_optimizer", ".", "apply_gradients", "(", "zip", "(", "grads", ",", "var_list", ")", ",", "\n", "global_step", ",", "name", ")", "\n", "with", "tf", ".", "control_dependencies", "(", "[", "op", "]", ")", ":", "\n", "                ", "zero_ops", "=", "[", "]", "\n", "for", "var", "in", "var_list", ":", "\n", "                    ", "grad_acc", "=", "self", ".", "get_slot", "(", "var", ",", "\"grad_acc\"", ")", "\n", "zero_ops", ".", "append", "(", "\n", "grad_acc", ".", "assign", "(", "tf", ".", "zeros_like", "(", "grad_acc", ")", ",", "\n", "use_locking", "=", "self", ".", "_use_locking", ")", ")", "\n", "", "zero_op", "=", "tf", ".", "group", "(", "*", "zero_ops", ")", "\n", "", "return", "tf", ".", "group", "(", "*", "[", "op", ",", "zero_op", "]", ")", "\n", "\n", "", "iter_var", "=", "self", ".", "_get_non_slot_variable", "(", "\"iter\"", ",", "tf", ".", "get_default_graph", "(", ")", ")", "\n", "update_op", "=", "tf", ".", "cond", "(", "tf", ".", "equal", "(", "iter_var", ",", "0", ")", ",", "_apply_gradients", ",", "\n", "_pass_gradients", ")", "\n", "\n", "with", "tf", ".", "control_dependencies", "(", "[", "update_op", "]", ")", ":", "\n", "            ", "iter_op", "=", "iter_var", ".", "assign", "(", "tf", ".", "mod", "(", "iter_var", "+", "1", ",", "self", ".", "_step_t", ")", ",", "\n", "use_locking", "=", "self", ".", "_use_locking", ")", "\n", "\n", "", "return", "tf", ".", "group", "(", "*", "[", "update_op", ",", "iter_op", "]", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.bin.get_relevance.to_text": [[25, 34], ["decoded.append"], "function", ["None"], ["def", "to_text", "(", "vocab", ",", "mapping", ",", "indice", ",", "params", ")", ":", "\n", "    ", "decoded", "=", "[", "]", "\n", "for", "idx", "in", "indice", ":", "\n", "        ", "if", "idx", "==", "mapping", "[", "params", ".", "eos", "]", ":", "\n", "            ", "break", "\n", "", "decoded", ".", "append", "(", "vocab", "[", "idx", "]", ")", "\n", "\n", "", "decoded", "=", "\" \"", ".", "join", "(", "decoded", ")", "\n", "return", "decoded", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.bin.get_relevance.parse_args": [[36, 61], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args"], "function", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.scripts.build_vocab.parse_args"], ["", "def", "parse_args", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "\n", "description", "=", "\"Translate using existing NMT models\"", ",", "\n", "usage", "=", "\"translator.py [<args>] [-h | --help]\"", "\n", ")", "\n", "\n", "# input files", "\n", "parser", ".", "add_argument", "(", "\"--input\"", ",", "type", "=", "str", ",", "required", "=", "True", ",", "\n", "help", "=", "\"Path of input file\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--output\"", ",", "type", "=", "str", ",", "required", "=", "True", ",", "\n", "help", "=", "\"Path of output file\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--relevances\"", ",", "type", "=", "str", ",", "required", "=", "True", ",", "\n", "help", "=", "\"Path of relevance result\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--checkpoints\"", ",", "type", "=", "str", ",", "nargs", "=", "\"+\"", ",", "required", "=", "True", ",", "\n", "help", "=", "\"Path of trained models\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--vocabulary\"", ",", "type", "=", "str", ",", "nargs", "=", "2", ",", "required", "=", "True", ",", "\n", "help", "=", "\"Path of source and target vocabulary\"", ")", "\n", "\n", "# model and configuration", "\n", "parser", ".", "add_argument", "(", "\"--models\"", ",", "type", "=", "str", ",", "required", "=", "True", ",", "nargs", "=", "\"+\"", ",", "\n", "help", "=", "\"Name of the model\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--parameters\"", ",", "type", "=", "str", ",", "\n", "help", "=", "\"Additional hyper parameters\"", ")", "\n", "\n", "return", "parser", ".", "parse_args", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.bin.get_relevance.default_parameters": [[63, 90], ["tensorflow.contrib.training.HParams"], "function", ["None"], ["", "def", "default_parameters", "(", ")", ":", "\n", "    ", "params", "=", "tf", ".", "contrib", ".", "training", ".", "HParams", "(", "\n", "input", "=", "None", ",", "\n", "output", "=", "None", ",", "\n", "vocabulary", "=", "None", ",", "\n", "model", "=", "None", ",", "\n", "# vocabulary specific", "\n", "pad", "=", "\"<pad>\"", ",", "\n", "bos", "=", "\"<bos>\"", ",", "\n", "eos", "=", "\"<eos>\"", ",", "\n", "unk", "=", "\"<unk>\"", ",", "\n", "mapping", "=", "None", ",", "\n", "append_eos", "=", "False", ",", "\n", "# decoding", "\n", "top_beams", "=", "1", ",", "\n", "beam_size", "=", "4", ",", "\n", "decode_alpha", "=", "0.6", ",", "\n", "decode_beta", "=", "0.2", ",", "\n", "decode_length", "=", "50", ",", "\n", "decode_batch_size", "=", "1", ",", "\n", "decode_constant", "=", "5.0", ",", "\n", "decode_normalize", "=", "False", ",", "\n", "device_list", "=", "[", "0", "]", ",", "\n", "num_threads", "=", "6", "\n", ")", "\n", "\n", "return", "params", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.bin.get_relevance.merge_parameters": [[92, 108], ["tensorflow.contrib.training.HParams", "six.iteritems", "tf.contrib.training.HParams.values", "six.iteritems", "params1.values", "tf.contrib.training.HParams.add_hparam", "params2.values", "setattr", "tf.contrib.training.HParams.add_hparam"], "function", ["None"], ["", "def", "merge_parameters", "(", "params1", ",", "params2", ")", ":", "\n", "    ", "params", "=", "tf", ".", "contrib", ".", "training", ".", "HParams", "(", ")", "\n", "\n", "for", "(", "k", ",", "v", ")", "in", "six", ".", "iteritems", "(", "params1", ".", "values", "(", ")", ")", ":", "\n", "        ", "params", ".", "add_hparam", "(", "k", ",", "v", ")", "\n", "\n", "", "params_dict", "=", "params", ".", "values", "(", ")", "\n", "\n", "for", "(", "k", ",", "v", ")", "in", "six", ".", "iteritems", "(", "params2", ".", "values", "(", ")", ")", ":", "\n", "        ", "if", "k", "in", "params_dict", ":", "\n", "# Override", "\n", "            ", "setattr", "(", "params", ",", "k", ",", "v", ")", "\n", "", "else", ":", "\n", "            ", "params", ".", "add_hparam", "(", "k", ",", "v", ")", "\n", "\n", "", "", "return", "params", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.bin.get_relevance.import_params": [[110, 123], ["os.path.abspath", "os.path.join", "tensorflow.gfile.Exists", "tensorflow.gfile.Open", "tensorflow.logging.info", "fd.readline", "params.parse_json"], "function", ["None"], ["", "def", "import_params", "(", "model_dir", ",", "model_name", ",", "params", ")", ":", "\n", "    ", "model_dir", "=", "os", ".", "path", ".", "abspath", "(", "model_dir", ")", "\n", "m_name", "=", "os", ".", "path", ".", "join", "(", "model_dir", ",", "model_name", "+", "\".json\"", ")", "\n", "\n", "if", "not", "tf", ".", "gfile", ".", "Exists", "(", "m_name", ")", ":", "\n", "        ", "return", "params", "\n", "\n", "", "with", "tf", ".", "gfile", ".", "Open", "(", "m_name", ")", "as", "fd", ":", "\n", "        ", "tf", ".", "logging", ".", "info", "(", "\"Restoring model parameters from %s\"", "%", "m_name", ")", "\n", "json_str", "=", "fd", ".", "readline", "(", ")", "\n", "params", ".", "parse_json", "(", "json_str", ")", "\n", "\n", "", "return", "params", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.bin.get_relevance.override_parameters": [[125, 154], ["thumt.process_vocabulary", "thumt.process_vocabulary", "params.parse", "thumt.load_vocabulary", "thumt.load_vocabulary", "thumt.get_control_mapping", "thumt.get_control_mapping"], "function", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.data.vocab.process_vocabulary", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.data.vocab.process_vocabulary", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.data.vocab.load_vocabulary", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.data.vocab.load_vocabulary", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.data.vocab.get_control_mapping", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.data.vocab.get_control_mapping"], ["", "def", "override_parameters", "(", "params", ",", "args", ")", ":", "\n", "    ", "if", "args", ".", "parameters", ":", "\n", "        ", "params", ".", "parse", "(", "args", ".", "parameters", ")", "\n", "\n", "", "params", ".", "vocabulary", "=", "{", "\n", "\"source\"", ":", "vocabulary", ".", "load_vocabulary", "(", "args", ".", "vocabulary", "[", "0", "]", ")", ",", "\n", "\"target\"", ":", "vocabulary", ".", "load_vocabulary", "(", "args", ".", "vocabulary", "[", "1", "]", ")", "\n", "}", "\n", "params", ".", "vocabulary", "[", "\"source\"", "]", "=", "vocabulary", ".", "process_vocabulary", "(", "\n", "params", ".", "vocabulary", "[", "\"source\"", "]", ",", "params", "\n", ")", "\n", "params", ".", "vocabulary", "[", "\"target\"", "]", "=", "vocabulary", ".", "process_vocabulary", "(", "\n", "params", ".", "vocabulary", "[", "\"target\"", "]", ",", "params", "\n", ")", "\n", "\n", "control_symbols", "=", "[", "params", ".", "pad", ",", "params", ".", "bos", ",", "params", ".", "eos", ",", "params", ".", "unk", "]", "\n", "\n", "params", ".", "mapping", "=", "{", "\n", "\"source\"", ":", "vocabulary", ".", "get_control_mapping", "(", "\n", "params", ".", "vocabulary", "[", "\"source\"", "]", ",", "\n", "control_symbols", "\n", ")", ",", "\n", "\"target\"", ":", "vocabulary", ".", "get_control_mapping", "(", "\n", "params", ".", "vocabulary", "[", "\"target\"", "]", ",", "\n", "control_symbols", "\n", ")", "\n", "}", "\n", "\n", "return", "params", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.bin.get_relevance.session_config": [[156, 169], ["tensorflow.OptimizerOptions", "tensorflow.GraphOptions", "tensorflow.ConfigProto", "str"], "function", ["None"], ["", "def", "session_config", "(", "params", ")", ":", "\n", "    ", "optimizer_options", "=", "tf", ".", "OptimizerOptions", "(", "opt_level", "=", "tf", ".", "OptimizerOptions", ".", "L1", ",", "\n", "do_function_inlining", "=", "False", ")", "\n", "graph_options", "=", "tf", ".", "GraphOptions", "(", "optimizer_options", "=", "optimizer_options", ")", "\n", "config", "=", "tf", ".", "ConfigProto", "(", "allow_soft_placement", "=", "True", ",", "\n", "graph_options", "=", "graph_options", ",", "\n", "intra_op_parallelism_threads", "=", "16", ",", "\n", "inter_op_parallelism_threads", "=", "16", ")", "\n", "if", "params", ".", "device_list", ":", "\n", "        ", "device_str", "=", "\",\"", ".", "join", "(", "[", "str", "(", "i", ")", "for", "i", "in", "params", ".", "device_list", "]", ")", "\n", "config", ".", "gpu_options", ".", "visible_device_list", "=", "device_str", "\n", "\n", "", "return", "config", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.bin.get_relevance.set_variables": [[171, 185], ["tensorflow.logging.info", "list", "tensorflow.device", "tensorflow.assign", "ops.append", "name.split"], "function", ["None"], ["", "def", "set_variables", "(", "var_list", ",", "value_dict", ",", "prefix", ")", ":", "\n", "    ", "ops", "=", "[", "]", "\n", "for", "var", "in", "var_list", ":", "\n", "        ", "for", "name", "in", "value_dict", ":", "\n", "            ", "var_name", "=", "\"/\"", ".", "join", "(", "[", "prefix", "]", "+", "list", "(", "name", ".", "split", "(", "\"/\"", ")", "[", "1", ":", "]", ")", ")", "\n", "\n", "if", "var", ".", "name", "[", ":", "-", "2", "]", "==", "var_name", ":", "\n", "                ", "tf", ".", "logging", ".", "info", "(", "\"restoring %s -> %s\"", "%", "(", "name", ",", "var", ".", "name", ")", ")", "\n", "with", "tf", ".", "device", "(", "\"/cpu:0\"", ")", ":", "\n", "                    ", "op", "=", "tf", ".", "assign", "(", "var", ",", "value_dict", "[", "name", "]", ")", "\n", "ops", ".", "append", "(", "op", ")", "\n", "", "break", "\n", "\n", "", "", "", "return", "ops", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.bin.get_relevance.main": [[187, 295], ["tensorflow.logging.set_verbosity", "thumt.get_model", "get_relevance.default_parameters", "get_relevance.merge_parameters", "get_relevance.import_params", "get_relevance.override_parameters", "tensorflow.Graph().as_default", "enumerate", "range", "thumt.get_relevance_input", "tensorflow.trainable_variables", "range", "tensorflow.group", "params.add_hparam", "params.add_hparam", "tensorflow.train.ChiefSessionCreator", "range", "model_cls.get_parameters", "zip", "range", "range", "print", "tensorflow.train.list_variables", "tensorflow.train.load_checkpoint", "model_var_lists.append", "len", "model_cls_list[].get_name", "model.get_relevance_func", "model_fns.append", "tensorflow.gfile.Open", "tensorflow.gfile.Open", "len", "model_cls_list[].get_name", "get_relevance.set_variables", "assign_ops.extend", "tensorflow.train.MonitoredSession", "sess.run", "len", "len", "len", "tensorflow.Graph", "tf.train.load_checkpoint.get_tensor", "line.strip", "line.strip", "v.name.startswith", "get_relevance.session_config", "os.path.exists", "os.makedirs", "sess.should_stop", "sess.run", "range", "tensorflow.logging.log", "model_cls_list[].get_name.startswith", "model_cls_list[].get_name.find", "un_init_var_list.append", "get_relevance.to_text", "get_relevance.to_text", "open", "open.write", "open.write", "open.write", "model_cls_list[].get_name", "str", "str"], "function", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.__init__.get_model", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.bin.scorer_token_level.default_parameters", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.bin.scorer_token_level.merge_parameters", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.bin.scorer_token_level.import_params", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.bin.scorer_token_level.override_parameters", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.data.dataset.get_relevance_input", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.transformer_lrp.TransformerLRP.get_parameters", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.transformer_lrp.TransformerLRP.get_name", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.transformer_lrp.TransformerLRP.get_relevance_func", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.transformer_lrp.TransformerLRP.get_name", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.bin.scorer_token_level.set_variables", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.bin.scorer_token_level.session_config", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.bin.get_relevance.to_text", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.bin.get_relevance.to_text", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.transformer_lrp.TransformerLRP.get_name"], ["", "def", "main", "(", "args", ")", ":", "\n", "    ", "tf", ".", "logging", ".", "set_verbosity", "(", "tf", ".", "logging", ".", "INFO", ")", "\n", "# Load configs", "\n", "model_cls_list", "=", "[", "models", ".", "get_model", "(", "model", ",", "lrp", "=", "True", ")", "\n", "for", "model", "in", "args", ".", "models", "]", "\n", "params_list", "=", "[", "default_parameters", "(", ")", "for", "_", "in", "range", "(", "len", "(", "model_cls_list", ")", ")", "]", "\n", "params_list", "=", "[", "\n", "merge_parameters", "(", "params", ",", "model_cls", ".", "get_parameters", "(", ")", ")", "\n", "for", "params", ",", "model_cls", "in", "zip", "(", "params_list", ",", "model_cls_list", ")", "\n", "]", "\n", "params_list", "=", "[", "\n", "import_params", "(", "args", ".", "checkpoints", "[", "i", "]", ",", "args", ".", "models", "[", "i", "]", ",", "params_list", "[", "i", "]", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "args", ".", "checkpoints", ")", ")", "\n", "]", "\n", "params_list", "=", "[", "\n", "override_parameters", "(", "params_list", "[", "i", "]", ",", "args", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "model_cls_list", ")", ")", "\n", "]", "\n", "\n", "# Build Graph", "\n", "with", "tf", ".", "Graph", "(", ")", ".", "as_default", "(", ")", ":", "\n", "        ", "model_var_lists", "=", "[", "]", "\n", "\n", "# Load checkpoints", "\n", "for", "i", ",", "checkpoint", "in", "enumerate", "(", "args", ".", "checkpoints", ")", ":", "\n", "            ", "print", "(", "\"Loading %s\"", "%", "checkpoint", ")", "\n", "var_list", "=", "tf", ".", "train", ".", "list_variables", "(", "checkpoint", ")", "\n", "values", "=", "{", "}", "\n", "reader", "=", "tf", ".", "train", ".", "load_checkpoint", "(", "checkpoint", ")", "\n", "\n", "for", "(", "name", ",", "shape", ")", "in", "var_list", ":", "\n", "                ", "if", "not", "name", ".", "startswith", "(", "model_cls_list", "[", "i", "]", ".", "get_name", "(", ")", ")", ":", "\n", "                    ", "continue", "\n", "\n", "", "if", "name", ".", "find", "(", "\"losses_avg\"", ")", ">=", "0", ":", "\n", "                    ", "continue", "\n", "\n", "", "tensor", "=", "reader", ".", "get_tensor", "(", "name", ")", "\n", "values", "[", "name", "]", "=", "tensor", "\n", "\n", "", "model_var_lists", ".", "append", "(", "values", ")", "\n", "\n", "# Build models", "\n", "", "model_fns", "=", "[", "]", "\n", "\n", "for", "i", "in", "range", "(", "len", "(", "args", ".", "checkpoints", ")", ")", ":", "\n", "            ", "name", "=", "model_cls_list", "[", "i", "]", ".", "get_name", "(", ")", "\n", "model", "=", "model_cls_list", "[", "i", "]", "(", "params_list", "[", "i", "]", ",", "name", "+", "\"_%d\"", "%", "i", ")", "\n", "model_fn", "=", "model", ".", "get_relevance_func", "(", ")", "\n", "model_fns", ".", "append", "(", "model_fn", ")", "\n", "\n", "", "params", "=", "params_list", "[", "0", "]", "\n", "# Read input file", "\n", "with", "tf", ".", "gfile", ".", "Open", "(", "args", ".", "input", ")", "as", "fd", ":", "\n", "            ", "inputs", "=", "[", "line", ".", "strip", "(", ")", "for", "line", "in", "fd", "]", "\n", "", "with", "tf", ".", "gfile", ".", "Open", "(", "args", ".", "output", ")", "as", "fd", ":", "\n", "            ", "outputs", "=", "[", "line", ".", "strip", "(", ")", "for", "line", "in", "fd", "]", "\n", "# Build input queue", "\n", "", "features", "=", "dataset", ".", "get_relevance_input", "(", "inputs", ",", "outputs", ",", "params", ")", "\n", "relevances", "=", "model_fns", "[", "0", "]", "(", "features", ",", "params", ")", "\n", "\n", "assign_ops", "=", "[", "]", "\n", "\n", "all_var_list", "=", "tf", ".", "trainable_variables", "(", ")", "\n", "\n", "for", "i", "in", "range", "(", "len", "(", "args", ".", "checkpoints", ")", ")", ":", "\n", "            ", "un_init_var_list", "=", "[", "]", "\n", "name", "=", "model_cls_list", "[", "i", "]", ".", "get_name", "(", ")", "\n", "\n", "for", "v", "in", "all_var_list", ":", "\n", "                ", "if", "v", ".", "name", ".", "startswith", "(", "name", "+", "\"_%d\"", "%", "i", ")", ":", "\n", "                    ", "un_init_var_list", ".", "append", "(", "v", ")", "\n", "\n", "", "", "ops", "=", "set_variables", "(", "un_init_var_list", ",", "model_var_lists", "[", "i", "]", ",", "\n", "name", "+", "\"_%d\"", "%", "i", ")", "\n", "assign_ops", ".", "extend", "(", "ops", ")", "\n", "\n", "", "assign_op", "=", "tf", ".", "group", "(", "*", "assign_ops", ")", "\n", "\n", "params", ".", "add_hparam", "(", "\"intra_op_parallelism_threads\"", ",", "1", ")", "\n", "params", ".", "add_hparam", "(", "\"inter_op_parallelism_threads\"", ",", "1", ")", "\n", "sess_creator", "=", "tf", ".", "train", ".", "ChiefSessionCreator", "(", "\n", "config", "=", "session_config", "(", "params", ")", "\n", ")", "\n", "\n", "results", "=", "[", "]", "\n", "\n", "# Create session", "\n", "with", "tf", ".", "train", ".", "MonitoredSession", "(", "session_creator", "=", "sess_creator", ")", "as", "sess", ":", "\n", "# Restore variables", "\n", "            ", "sess", ".", "run", "(", "assign_op", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "args", ".", "relevances", ")", ":", "\n", "                ", "os", ".", "makedirs", "(", "args", ".", "relevances", ")", "\n", "", "count", "=", "0", "\n", "while", "not", "sess", ".", "should_stop", "(", ")", ":", "\n", "                ", "src_seq", ",", "trg_seq", ",", "rlv_info", ",", "loss", "=", "sess", ".", "run", "(", "relevances", ")", "\n", "message", "=", "\"Finished batch\"", "\" %d\"", "%", "count", "\n", "for", "i", "in", "range", "(", "src_seq", ".", "shape", "[", "0", "]", ")", ":", "\n", "                    ", "count", "+=", "1", "\n", "src", "=", "to_text", "(", "params", ".", "vocabulary", "[", "\"source\"", "]", ",", "\n", "params", ".", "mapping", "[", "\"source\"", "]", ",", "src_seq", "[", "i", "]", ",", "params", ")", "\n", "trg", "=", "to_text", "(", "params", ".", "vocabulary", "[", "\"target\"", "]", ",", "\n", "params", ".", "mapping", "[", "\"target\"", "]", ",", "trg_seq", "[", "i", "]", ",", "params", ")", "\n", "output", "=", "open", "(", "args", ".", "relevances", "+", "\"/\"", "+", "str", "(", "count", ")", ",", "\"w\"", ")", "\n", "output", ".", "write", "(", "\"src: \"", "+", "src", "+", "\"\\n\"", ")", "\n", "output", ".", "write", "(", "\"trg: \"", "+", "trg", "+", "\"\\n\"", ")", "\n", "output", ".", "write", "(", "\"result: %s\\n\"", "%", "str", "(", "rlv_info", "[", "\"result\"", "]", "[", "i", "]", ")", ")", "\n", "", "tf", ".", "logging", ".", "log", "(", "tf", ".", "logging", ".", "INFO", ",", "message", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.bin.translator.parse_args": [[25, 50], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args"], "function", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.scripts.build_vocab.parse_args"], ["def", "parse_args", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "\n", "description", "=", "\"Translate using existing NMT models\"", ",", "\n", "usage", "=", "\"translator.py [<args>] [-h | --help]\"", "\n", ")", "\n", "\n", "# input files", "\n", "parser", ".", "add_argument", "(", "\"--input\"", ",", "type", "=", "str", ",", "required", "=", "True", ",", "\n", "help", "=", "\"Path of input file\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--output\"", ",", "type", "=", "str", ",", "required", "=", "True", ",", "\n", "help", "=", "\"Path of output file\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--checkpoints\"", ",", "type", "=", "str", ",", "nargs", "=", "\"+\"", ",", "required", "=", "True", ",", "\n", "help", "=", "\"Path of trained models\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--vocabulary\"", ",", "type", "=", "str", ",", "nargs", "=", "2", ",", "required", "=", "True", ",", "\n", "help", "=", "\"Path of source and target vocabulary\"", ")", "\n", "\n", "# model and configuration", "\n", "parser", ".", "add_argument", "(", "\"--models\"", ",", "type", "=", "str", ",", "required", "=", "True", ",", "nargs", "=", "\"+\"", ",", "\n", "help", "=", "\"Name of the model\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--parameters\"", ",", "type", "=", "str", ",", "\n", "help", "=", "\"Additional hyper parameters\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--verbose\"", ",", "action", "=", "\"store_true\"", ",", "\n", "help", "=", "\"Enable verbose output\"", ")", "\n", "\n", "return", "parser", ".", "parse_args", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.bin.translator.default_parameters": [[52, 83], ["tensorflow.contrib.training.HParams"], "function", ["None"], ["", "def", "default_parameters", "(", ")", ":", "\n", "    ", "params", "=", "tf", ".", "contrib", ".", "training", ".", "HParams", "(", "\n", "input", "=", "None", ",", "\n", "output", "=", "None", ",", "\n", "vocabulary", "=", "None", ",", "\n", "# vocabulary specific", "\n", "pad", "=", "\"<pad>\"", ",", "\n", "bos", "=", "\"<bos>\"", ",", "\n", "eos", "=", "\"<eos>\"", ",", "\n", "unk", "=", "\"<unk>\"", ",", "\n", "mapping", "=", "None", ",", "\n", "append_eos", "=", "False", ",", "\n", "device_list", "=", "[", "0", "]", ",", "\n", "num_threads", "=", "1", ",", "\n", "# decoding", "\n", "top_beams", "=", "1", ",", "\n", "beam_size", "=", "4", ",", "\n", "decode_alpha", "=", "0.6", ",", "\n", "decode_length", "=", "50", ",", "\n", "decode_batch_size", "=", "32", ",", "\n", "# sampling", "\n", "generate_samples", "=", "False", ",", "\n", "num_samples", "=", "1", ",", "\n", "min_length_ratio", "=", "0.0", ",", "\n", "max_length_ratio", "=", "1.5", ",", "\n", "min_sample_length", "=", "0", ",", "\n", "max_sample_length", "=", "0", ",", "\n", "sample_batch_size", "=", "32", "\n", ")", "\n", "\n", "return", "params", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.bin.translator.merge_parameters": [[85, 101], ["tensorflow.contrib.training.HParams", "six.iteritems", "tf.contrib.training.HParams.values", "six.iteritems", "params1.values", "tf.contrib.training.HParams.add_hparam", "params2.values", "setattr", "tf.contrib.training.HParams.add_hparam"], "function", ["None"], ["", "def", "merge_parameters", "(", "params1", ",", "params2", ")", ":", "\n", "    ", "params", "=", "tf", ".", "contrib", ".", "training", ".", "HParams", "(", ")", "\n", "\n", "for", "(", "k", ",", "v", ")", "in", "six", ".", "iteritems", "(", "params1", ".", "values", "(", ")", ")", ":", "\n", "        ", "params", ".", "add_hparam", "(", "k", ",", "v", ")", "\n", "\n", "", "params_dict", "=", "params", ".", "values", "(", ")", "\n", "\n", "for", "(", "k", ",", "v", ")", "in", "six", ".", "iteritems", "(", "params2", ".", "values", "(", ")", ")", ":", "\n", "        ", "if", "k", "in", "params_dict", ":", "\n", "# Override", "\n", "            ", "setattr", "(", "params", ",", "k", ",", "v", ")", "\n", "", "else", ":", "\n", "            ", "params", ".", "add_hparam", "(", "k", ",", "v", ")", "\n", "\n", "", "", "return", "params", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.bin.translator.import_params": [[103, 119], ["model_name.startswith", "os.path.abspath", "os.path.join", "tensorflow.gfile.Exists", "tensorflow.gfile.Open", "tensorflow.logging.info", "fd.readline", "params.parse_json"], "function", ["None"], ["", "def", "import_params", "(", "model_dir", ",", "model_name", ",", "params", ")", ":", "\n", "    ", "if", "model_name", ".", "startswith", "(", "\"experimental_\"", ")", ":", "\n", "        ", "model_name", "=", "model_name", "[", "13", ":", "]", "\n", "\n", "", "model_dir", "=", "os", ".", "path", ".", "abspath", "(", "model_dir", ")", "\n", "m_name", "=", "os", ".", "path", ".", "join", "(", "model_dir", ",", "model_name", "+", "\".json\"", ")", "\n", "\n", "if", "not", "tf", ".", "gfile", ".", "Exists", "(", "m_name", ")", ":", "\n", "        ", "return", "params", "\n", "\n", "", "with", "tf", ".", "gfile", ".", "Open", "(", "m_name", ")", "as", "fd", ":", "\n", "        ", "tf", ".", "logging", ".", "info", "(", "\"Restoring model parameters from %s\"", "%", "m_name", ")", "\n", "json_str", "=", "fd", ".", "readline", "(", ")", "\n", "params", ".", "parse_json", "(", "json_str", ")", "\n", "\n", "", "return", "params", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.bin.translator.override_parameters": [[121, 150], ["thumt.process_vocabulary", "thumt.process_vocabulary", "params.parse", "thumt.load_vocabulary", "thumt.load_vocabulary", "thumt.get_control_mapping", "thumt.get_control_mapping"], "function", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.data.vocab.process_vocabulary", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.data.vocab.process_vocabulary", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.data.vocab.load_vocabulary", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.data.vocab.load_vocabulary", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.data.vocab.get_control_mapping", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.data.vocab.get_control_mapping"], ["", "def", "override_parameters", "(", "params", ",", "args", ")", ":", "\n", "    ", "if", "args", ".", "parameters", ":", "\n", "        ", "params", ".", "parse", "(", "args", ".", "parameters", ")", "\n", "\n", "", "params", ".", "vocabulary", "=", "{", "\n", "\"source\"", ":", "vocabulary", ".", "load_vocabulary", "(", "args", ".", "vocabulary", "[", "0", "]", ")", ",", "\n", "\"target\"", ":", "vocabulary", ".", "load_vocabulary", "(", "args", ".", "vocabulary", "[", "1", "]", ")", "\n", "}", "\n", "params", ".", "vocabulary", "[", "\"source\"", "]", "=", "vocabulary", ".", "process_vocabulary", "(", "\n", "params", ".", "vocabulary", "[", "\"source\"", "]", ",", "params", "\n", ")", "\n", "params", ".", "vocabulary", "[", "\"target\"", "]", "=", "vocabulary", ".", "process_vocabulary", "(", "\n", "params", ".", "vocabulary", "[", "\"target\"", "]", ",", "params", "\n", ")", "\n", "\n", "control_symbols", "=", "[", "params", ".", "pad", ",", "params", ".", "bos", ",", "params", ".", "eos", ",", "params", ".", "unk", "]", "\n", "\n", "params", ".", "mapping", "=", "{", "\n", "\"source\"", ":", "vocabulary", ".", "get_control_mapping", "(", "\n", "params", ".", "vocabulary", "[", "\"source\"", "]", ",", "\n", "control_symbols", "\n", ")", ",", "\n", "\"target\"", ":", "vocabulary", ".", "get_control_mapping", "(", "\n", "params", ".", "vocabulary", "[", "\"target\"", "]", ",", "\n", "control_symbols", "\n", ")", "\n", "}", "\n", "\n", "return", "params", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.bin.translator.session_config": [[152, 163], ["tensorflow.OptimizerOptions", "tensorflow.GraphOptions", "tensorflow.ConfigProto", "str"], "function", ["None"], ["", "def", "session_config", "(", "params", ")", ":", "\n", "    ", "optimizer_options", "=", "tf", ".", "OptimizerOptions", "(", "opt_level", "=", "tf", ".", "OptimizerOptions", ".", "L1", ",", "\n", "do_function_inlining", "=", "False", ")", "\n", "graph_options", "=", "tf", ".", "GraphOptions", "(", "optimizer_options", "=", "optimizer_options", ")", "\n", "config", "=", "tf", ".", "ConfigProto", "(", "allow_soft_placement", "=", "True", ",", "\n", "graph_options", "=", "graph_options", ")", "\n", "if", "params", ".", "device_list", ":", "\n", "        ", "device_str", "=", "\",\"", ".", "join", "(", "[", "str", "(", "i", ")", "for", "i", "in", "params", ".", "device_list", "]", ")", "\n", "config", ".", "gpu_options", ".", "visible_device_list", "=", "device_str", "\n", "\n", "", "return", "config", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.bin.translator.set_variables": [[165, 182], ["tensorflow.logging.debug", "tensorflow.placeholder", "list", "tensorflow.device", "tensorflow.assign", "ops.append", "name.split"], "function", ["None"], ["", "def", "set_variables", "(", "var_list", ",", "value_dict", ",", "prefix", ",", "feed_dict", ")", ":", "\n", "    ", "ops", "=", "[", "]", "\n", "for", "var", "in", "var_list", ":", "\n", "        ", "for", "name", "in", "value_dict", ":", "\n", "            ", "var_name", "=", "\"/\"", ".", "join", "(", "[", "prefix", "]", "+", "list", "(", "name", ".", "split", "(", "\"/\"", ")", "[", "1", ":", "]", ")", ")", "\n", "\n", "if", "var", ".", "name", "[", ":", "-", "2", "]", "==", "var_name", ":", "\n", "                ", "tf", ".", "logging", ".", "debug", "(", "\"restoring %s -> %s\"", "%", "(", "name", ",", "var", ".", "name", ")", ")", "\n", "placeholder", "=", "tf", ".", "placeholder", "(", "tf", ".", "float32", ",", "\n", "name", "=", "\"placeholder/\"", "+", "var_name", ")", "\n", "with", "tf", ".", "device", "(", "\"/cpu:0\"", ")", ":", "\n", "                    ", "op", "=", "tf", ".", "assign", "(", "var", ",", "placeholder", ")", "\n", "ops", ".", "append", "(", "op", ")", "\n", "", "feed_dict", "[", "placeholder", "]", "=", "value_dict", "[", "name", "]", "\n", "break", "\n", "\n", "", "", "", "return", "ops", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.bin.translator.shard_features": [[184, 207], ["len", "isinstance", "range"], "function", ["None"], ["", "def", "shard_features", "(", "features", ",", "placeholders", ",", "predictions", ")", ":", "\n", "    ", "num_shards", "=", "len", "(", "placeholders", ")", "\n", "feed_dict", "=", "{", "}", "\n", "n", "=", "0", "\n", "\n", "for", "name", "in", "features", ":", "\n", "        ", "feat", "=", "features", "[", "name", "]", "\n", "batch", "=", "feat", ".", "shape", "[", "0", "]", "\n", "shard_size", "=", "(", "batch", "+", "num_shards", "-", "1", ")", "//", "num_shards", "\n", "\n", "for", "i", "in", "range", "(", "num_shards", ")", ":", "\n", "            ", "shard_feat", "=", "feat", "[", "i", "*", "shard_size", ":", "(", "i", "+", "1", ")", "*", "shard_size", "]", "\n", "\n", "if", "shard_feat", ".", "shape", "[", "0", "]", "!=", "0", ":", "\n", "                ", "feed_dict", "[", "placeholders", "[", "i", "]", "[", "name", "]", "]", "=", "shard_feat", "\n", "n", "=", "i", "+", "1", "\n", "", "else", ":", "\n", "                ", "break", "\n", "\n", "", "", "", "if", "isinstance", "(", "predictions", ",", "(", "list", ",", "tuple", ")", ")", ":", "\n", "        ", "predictions", "=", "predictions", "[", ":", "n", "]", "\n", "\n", "", "return", "predictions", ",", "feed_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.bin.translator.main": [[209, 375], ["tensorflow.logging.set_verbosity", "thumt.get_model", "translator.default_parameters", "translator.merge_parameters", "translator.import_params", "translator.override_parameters", "tensorflow.Graph().as_default", "enumerate", "range", "thumt.sort_input_file", "thumt.get_inference_input", "range", "thumt.data_parallelism", "tensorflow.trainable_variables", "range", "tensorflow.group", "tensorflow.tables_initializer", "tensorflow.get_default_graph().finalize", "range", "zip", "open.close", "range", "model_cls.get_parameters", "zip", "range", "range", "tensorflow.logging.info", "tensorflow.train.list_variables", "tensorflow.train.load_checkpoint", "model_var_lists.append", "len", "model_cls_list[].get_name", "model_list.append", "len", "placeholders.append", "len", "model_cls_list[].get_name", "translator.set_variables", "assign_ops.extend", "tensorflow.Session", "sess.run", "sess.run", "len", "restored_inputs.append", "restored_outputs.append", "restored_scores.append", "open", "zip", "len", "len", "len", "tensorflow.Graph", "tf.train.load_checkpoint.get_tensor", "inference_fn", "v.name.startswith", "tensorflow.get_default_graph", "open", "ValueError", "model_cls_list[].get_name.startswith", "model_cls_list[].get_name.find", "tensorflow.placeholder", "tensorflow.placeholder", "un_init_var_list.append", "translator.session_config", "sess.run", "translator.shard_features", "results.append", "tensorflow.logging.log", "outputs.append", "scores.append", "decoded.append", "open.write", "open.write", "model_cls_list[].get_name", "sess.run", "len", "item.tolist", "item.tolist"], "function", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.__init__.get_model", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.bin.scorer_token_level.default_parameters", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.bin.scorer_token_level.merge_parameters", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.bin.scorer_token_level.import_params", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.bin.scorer_token_level.override_parameters", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.data.dataset.sort_input_file", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.data.dataset.get_inference_input", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.parallel.data_parallelism", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.transformer_lrp.TransformerLRP.get_parameters", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.transformer_lrp.TransformerLRP.get_name", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.transformer_lrp.TransformerLRP.get_name", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.bin.scorer_token_level.set_variables", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.bin.scorer_token_level.session_config", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.bin.translator.shard_features", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.transformer_lrp.TransformerLRP.get_name"], ["", "def", "main", "(", "args", ")", ":", "\n", "    ", "tf", ".", "logging", ".", "set_verbosity", "(", "tf", ".", "logging", ".", "INFO", ")", "\n", "# Load configs", "\n", "model_cls_list", "=", "[", "models", ".", "get_model", "(", "model", ")", "for", "model", "in", "args", ".", "models", "]", "\n", "params_list", "=", "[", "default_parameters", "(", ")", "for", "_", "in", "range", "(", "len", "(", "model_cls_list", ")", ")", "]", "\n", "params_list", "=", "[", "\n", "merge_parameters", "(", "params", ",", "model_cls", ".", "get_parameters", "(", ")", ")", "\n", "for", "params", ",", "model_cls", "in", "zip", "(", "params_list", ",", "model_cls_list", ")", "\n", "]", "\n", "params_list", "=", "[", "\n", "import_params", "(", "args", ".", "checkpoints", "[", "i", "]", ",", "args", ".", "models", "[", "i", "]", ",", "params_list", "[", "i", "]", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "args", ".", "checkpoints", ")", ")", "\n", "]", "\n", "params_list", "=", "[", "\n", "override_parameters", "(", "params_list", "[", "i", "]", ",", "args", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "model_cls_list", ")", ")", "\n", "]", "\n", "\n", "# Build Graph", "\n", "with", "tf", ".", "Graph", "(", ")", ".", "as_default", "(", ")", ":", "\n", "        ", "model_var_lists", "=", "[", "]", "\n", "\n", "# Load checkpoints", "\n", "for", "i", ",", "checkpoint", "in", "enumerate", "(", "args", ".", "checkpoints", ")", ":", "\n", "            ", "tf", ".", "logging", ".", "info", "(", "\"Loading %s\"", "%", "checkpoint", ")", "\n", "var_list", "=", "tf", ".", "train", ".", "list_variables", "(", "checkpoint", ")", "\n", "values", "=", "{", "}", "\n", "reader", "=", "tf", ".", "train", ".", "load_checkpoint", "(", "checkpoint", ")", "\n", "\n", "for", "(", "name", ",", "shape", ")", "in", "var_list", ":", "\n", "                ", "if", "not", "name", ".", "startswith", "(", "model_cls_list", "[", "i", "]", ".", "get_name", "(", ")", ")", ":", "\n", "                    ", "continue", "\n", "\n", "", "if", "name", ".", "find", "(", "\"losses_avg\"", ")", ">=", "0", ":", "\n", "                    ", "continue", "\n", "\n", "", "tensor", "=", "reader", ".", "get_tensor", "(", "name", ")", "\n", "values", "[", "name", "]", "=", "tensor", "\n", "\n", "", "model_var_lists", ".", "append", "(", "values", ")", "\n", "\n", "# Build models", "\n", "", "model_list", "=", "[", "]", "\n", "\n", "for", "i", "in", "range", "(", "len", "(", "args", ".", "checkpoints", ")", ")", ":", "\n", "            ", "name", "=", "model_cls_list", "[", "i", "]", ".", "get_name", "(", ")", "\n", "model", "=", "model_cls_list", "[", "i", "]", "(", "params_list", "[", "i", "]", ",", "name", "+", "\"_%d\"", "%", "i", ")", "\n", "model_list", ".", "append", "(", "model", ")", "\n", "\n", "", "params", "=", "params_list", "[", "0", "]", "\n", "# Read input file", "\n", "sorted_keys", ",", "sorted_inputs", "=", "dataset", ".", "sort_input_file", "(", "args", ".", "input", ")", "\n", "# Build input queue", "\n", "features", "=", "dataset", ".", "get_inference_input", "(", "sorted_inputs", ",", "params", ")", "\n", "# Create placeholders", "\n", "placeholders", "=", "[", "]", "\n", "\n", "for", "i", "in", "range", "(", "len", "(", "params", ".", "device_list", ")", ")", ":", "\n", "            ", "placeholders", ".", "append", "(", "{", "\n", "\"source\"", ":", "tf", ".", "placeholder", "(", "tf", ".", "int32", ",", "[", "None", ",", "None", "]", ",", "\n", "\"source_%d\"", "%", "i", ")", ",", "\n", "\"source_length\"", ":", "tf", ".", "placeholder", "(", "tf", ".", "int32", ",", "[", "None", "]", ",", "\n", "\"source_length_%d\"", "%", "i", ")", "\n", "}", ")", "\n", "\n", "# A list of outputs", "\n", "", "if", "params", ".", "generate_samples", ":", "\n", "            ", "inference_fn", "=", "sampling", ".", "create_sampling_graph", "\n", "", "else", ":", "\n", "            ", "inference_fn", "=", "inference", ".", "create_inference_graph", "\n", "\n", "", "predictions", "=", "parallel", ".", "data_parallelism", "(", "\n", "params", ".", "device_list", ",", "lambda", "f", ":", "inference_fn", "(", "model_list", ",", "f", ",", "params", ")", ",", "\n", "placeholders", ")", "\n", "\n", "# Create assign ops", "\n", "assign_ops", "=", "[", "]", "\n", "feed_dict", "=", "{", "}", "\n", "\n", "all_var_list", "=", "tf", ".", "trainable_variables", "(", ")", "\n", "\n", "for", "i", "in", "range", "(", "len", "(", "args", ".", "checkpoints", ")", ")", ":", "\n", "            ", "un_init_var_list", "=", "[", "]", "\n", "name", "=", "model_cls_list", "[", "i", "]", ".", "get_name", "(", ")", "\n", "\n", "for", "v", "in", "all_var_list", ":", "\n", "                ", "if", "v", ".", "name", ".", "startswith", "(", "name", "+", "\"_%d\"", "%", "i", ")", ":", "\n", "                    ", "un_init_var_list", ".", "append", "(", "v", ")", "\n", "\n", "", "", "ops", "=", "set_variables", "(", "un_init_var_list", ",", "model_var_lists", "[", "i", "]", ",", "\n", "name", "+", "\"_%d\"", "%", "i", ",", "feed_dict", ")", "\n", "assign_ops", ".", "extend", "(", "ops", ")", "\n", "\n", "", "assign_op", "=", "tf", ".", "group", "(", "*", "assign_ops", ")", "\n", "init_op", "=", "tf", ".", "tables_initializer", "(", ")", "\n", "results", "=", "[", "]", "\n", "\n", "tf", ".", "get_default_graph", "(", ")", ".", "finalize", "(", ")", "\n", "\n", "# Create session", "\n", "with", "tf", ".", "Session", "(", "config", "=", "session_config", "(", "params", ")", ")", "as", "sess", ":", "\n", "# Restore variables", "\n", "            ", "sess", ".", "run", "(", "assign_op", ",", "feed_dict", "=", "feed_dict", ")", "\n", "sess", ".", "run", "(", "init_op", ")", "\n", "\n", "while", "True", ":", "\n", "                ", "try", ":", "\n", "                    ", "feats", "=", "sess", ".", "run", "(", "features", ")", "\n", "op", ",", "feed_dict", "=", "shard_features", "(", "feats", ",", "placeholders", ",", "\n", "predictions", ")", "\n", "results", ".", "append", "(", "sess", ".", "run", "(", "op", ",", "feed_dict", "=", "feed_dict", ")", ")", "\n", "message", "=", "\"Finished batch %d\"", "%", "len", "(", "results", ")", "\n", "tf", ".", "logging", ".", "log", "(", "tf", ".", "logging", ".", "INFO", ",", "message", ")", "\n", "", "except", "tf", ".", "errors", ".", "OutOfRangeError", ":", "\n", "                    ", "break", "\n", "\n", "# Convert to plain text", "\n", "", "", "", "vocab", "=", "params", ".", "vocabulary", "[", "\"target\"", "]", "\n", "outputs", "=", "[", "]", "\n", "scores", "=", "[", "]", "\n", "for", "result", "in", "results", ":", "\n", "            ", "for", "shard", "in", "result", ":", "\n", "                ", "for", "item", "in", "shard", "[", "0", "]", ":", "\n", "                    ", "outputs", ".", "append", "(", "item", ".", "tolist", "(", ")", ")", "\n", "", "for", "item", "in", "shard", "[", "1", "]", ":", "\n", "                    ", "scores", ".", "append", "(", "item", ".", "tolist", "(", ")", ")", "\n", "\n", "", "", "", "restored_inputs", "=", "[", "]", "\n", "restored_outputs", "=", "[", "]", "\n", "restored_scores", "=", "[", "]", "\n", "for", "index", "in", "range", "(", "len", "(", "sorted_inputs", ")", ")", ":", "\n", "            ", "restored_inputs", ".", "append", "(", "sorted_inputs", "[", "sorted_keys", "[", "index", "]", "]", ")", "\n", "restored_outputs", ".", "append", "(", "outputs", "[", "sorted_keys", "[", "index", "]", "]", ")", "\n", "restored_scores", ".", "append", "(", "scores", "[", "sorted_keys", "[", "index", "]", "]", ")", "\n", "\n", "# Write to file", "\n", "", "if", "sys", ".", "version_info", ".", "major", "==", "2", ":", "\n", "            ", "outfile", "=", "open", "(", "args", ".", "output", ",", "\"w\"", ")", "\n", "", "elif", "sys", ".", "version_info", ".", "major", "==", "3", ":", "\n", "            ", "outfile", "=", "open", "(", "args", ".", "output", ",", "\"w\"", ",", "encoding", "=", "\"utf-8\"", ")", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"Unkown python running environment!\"", ")", "\n", "\n", "", "count", "=", "0", "\n", "for", "outputs", ",", "scores", "in", "zip", "(", "restored_outputs", ",", "restored_scores", ")", ":", "\n", "            ", "for", "output", ",", "score", "in", "zip", "(", "outputs", ",", "scores", ")", ":", "\n", "                ", "decoded", "=", "[", "]", "\n", "for", "idx", "in", "output", ":", "\n", "                    ", "if", "idx", "==", "params", ".", "mapping", "[", "\"target\"", "]", "[", "params", ".", "eos", "]", ":", "\n", "                        ", "break", "\n", "", "decoded", ".", "append", "(", "vocab", "[", "idx", "]", ")", "\n", "\n", "", "decoded", "=", "\" \"", ".", "join", "(", "decoded", ")", "\n", "\n", "if", "not", "args", ".", "verbose", ":", "\n", "                    ", "outfile", ".", "write", "(", "\"%s\\n\"", "%", "decoded", ")", "\n", "", "else", ":", "\n", "                    ", "pattern", "=", "\"%d ||| %s ||| %s ||| %f\\n\"", "\n", "source", "=", "restored_inputs", "[", "count", "]", "\n", "values", "=", "(", "count", ",", "source", ",", "decoded", ",", "score", ")", "\n", "outfile", ".", "write", "(", "pattern", "%", "values", ")", "\n", "\n", "", "", "count", "+=", "1", "\n", "", "outfile", ".", "close", "(", ")", "\n", "\n", "", "", "if", "__name__", "==", "\"__main__\"", ":", "\n", "    ", "main", "(", "parse_args", "(", ")", ")", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.bin.trainer.parse_args": [[29, 62], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args"], "function", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.scripts.build_vocab.parse_args"], ["def", "parse_args", "(", "args", "=", "None", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "\n", "description", "=", "\"Training neural machine translation models\"", ",", "\n", "usage", "=", "\"trainer.py [<args>] [-h | --help]\"", "\n", ")", "\n", "\n", "# input files", "\n", "parser", ".", "add_argument", "(", "\"--input\"", ",", "type", "=", "str", ",", "nargs", "=", "2", ",", "\n", "help", "=", "\"Path of source and target corpus\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--record\"", ",", "type", "=", "str", ",", "\n", "help", "=", "\"Path to tf.Record data\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--output\"", ",", "type", "=", "str", ",", "default", "=", "\"train\"", ",", "\n", "help", "=", "\"Path to saved models\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--vocabulary\"", ",", "type", "=", "str", ",", "nargs", "=", "2", ",", "\n", "help", "=", "\"Path of source and target vocabulary\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--validation\"", ",", "type", "=", "str", ",", "\n", "help", "=", "\"Path of validation file\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--references\"", ",", "type", "=", "str", ",", "nargs", "=", "\"+\"", ",", "\n", "help", "=", "\"Path of reference files\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--checkpoint\"", ",", "type", "=", "str", ",", "\n", "help", "=", "\"Path to pre-trained checkpoint\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--half\"", ",", "action", "=", "\"store_true\"", ",", "\n", "help", "=", "\"Enable FP16 training\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--distribute\"", ",", "action", "=", "\"store_true\"", ",", "\n", "help", "=", "\"Enable distributed training\"", ")", "\n", "\n", "# model and configuration", "\n", "parser", ".", "add_argument", "(", "\"--model\"", ",", "type", "=", "str", ",", "required", "=", "True", ",", "\n", "help", "=", "\"Name of the model\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--parameters\"", ",", "type", "=", "str", ",", "default", "=", "\"\"", ",", "\n", "help", "=", "\"Additional hyper parameters\"", ")", "\n", "\n", "return", "parser", ".", "parse_args", "(", "args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.bin.trainer.default_parameters": [[64, 120], ["tensorflow.contrib.training.HParams"], "function", ["None"], ["", "def", "default_parameters", "(", ")", ":", "\n", "    ", "params", "=", "tf", ".", "contrib", ".", "training", ".", "HParams", "(", "\n", "input", "=", "[", "\"\"", ",", "\"\"", "]", ",", "\n", "output", "=", "\"\"", ",", "\n", "record", "=", "\"\"", ",", "\n", "model", "=", "\"transformer\"", ",", "\n", "vocab", "=", "[", "\"\"", ",", "\"\"", "]", ",", "\n", "# Default training hyper parameters", "\n", "bridge_threshold", "=", "1.00", ",", "\n", "start_step", "=", "100000", ",", "\n", "num_threads", "=", "6", ",", "\n", "batch_size", "=", "4096", ",", "\n", "max_length", "=", "256", ",", "\n", "length_multiplier", "=", "1", ",", "\n", "mantissa_bits", "=", "2", ",", "\n", "warmup_steps", "=", "4000", ",", "\n", "train_steps", "=", "1000000", ",", "\n", "buffer_size", "=", "200000", ",", "\n", "constant_batch_size", "=", "False", ",", "\n", "device_list", "=", "[", "0", "]", ",", "\n", "update_cycle", "=", "1", ",", "\n", "initializer", "=", "\"uniform_unit_scaling\"", ",", "\n", "initializer_gain", "=", "1.0", ",", "\n", "loss_scale", "=", "128", ",", "\n", "scale_l1", "=", "0.0", ",", "\n", "scale_l2", "=", "0.0", ",", "\n", "optimizer", "=", "\"Adam\"", ",", "\n", "adam_beta1", "=", "0.9", ",", "\n", "adam_beta2", "=", "0.999", ",", "\n", "adam_epsilon", "=", "1e-8", ",", "\n", "clip_grad_norm", "=", "5.0", ",", "\n", "learning_rate", "=", "1.0", ",", "\n", "learning_rate_decay", "=", "\"linear_warmup_rsqrt_decay\"", ",", "\n", "learning_rate_boundaries", "=", "[", "0", "]", ",", "\n", "learning_rate_values", "=", "[", "0.0", "]", ",", "\n", "keep_checkpoint_max", "=", "100", ",", "\n", "keep_top_checkpoint_max", "=", "5", ",", "\n", "# Validation", "\n", "eval_steps", "=", "5000", ",", "\n", "eval_secs", "=", "0", ",", "\n", "eval_batch_size", "=", "32", ",", "\n", "top_beams", "=", "1", ",", "\n", "beam_size", "=", "4", ",", "\n", "decode_alpha", "=", "0.6", ",", "\n", "decode_length", "=", "50", ",", "\n", "validation", "=", "\"\"", ",", "\n", "references", "=", "[", "\"\"", "]", ",", "\n", "save_checkpoint_secs", "=", "0", ",", "\n", "save_checkpoint_steps", "=", "100", ",", "\n", "# Setting this to True can save disk spaces, but cannot restore", "\n", "# training using the saved checkpoint", "\n", "only_save_trainable", "=", "False", ",", "\n", "zero_step", "=", "True", ",", "\n", "happen_prob", "=", "0.5", ",", "\n", "replace_prob", "=", "0.3", ",", "\n", "mle_rate", "=", "1.0", ",", "\n", "bridge_rate", "=", "1.0", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.bin.trainer.import_params": [[122, 141], ["os.path.abspath", "os.path.join", "os.path.join", "tensorflow.gfile.Open", "tensorflow.logging.info", "fd.readline", "params.parse_json", "tensorflow.gfile.Open", "tensorflow.logging.info", "fd.readline", "params.parse_json", "tensorflow.gfile.Exists", "tensorflow.gfile.Exists"], "function", ["None"], ["\n", "return", "params", "\n", "\n", "\n", "", "def", "import_params", "(", "model_dir", ",", "model_name", ",", "params", ")", ":", "\n", "    ", "model_dir", "=", "os", ".", "path", ".", "abspath", "(", "model_dir", ")", "\n", "p_name", "=", "os", ".", "path", ".", "join", "(", "model_dir", ",", "\"params.json\"", ")", "\n", "m_name", "=", "os", ".", "path", ".", "join", "(", "model_dir", ",", "model_name", "+", "\".json\"", ")", "\n", "\n", "if", "not", "tf", ".", "gfile", ".", "Exists", "(", "p_name", ")", "or", "not", "tf", ".", "gfile", ".", "Exists", "(", "m_name", ")", ":", "\n", "        ", "return", "params", "\n", "\n", "", "with", "tf", ".", "gfile", ".", "Open", "(", "p_name", ")", "as", "fd", ":", "\n", "        ", "tf", ".", "logging", ".", "info", "(", "\"Restoring hyper parameters from %s\"", "%", "p_name", ")", "\n", "json_str", "=", "fd", ".", "readline", "(", ")", "\n", "params", ".", "parse_json", "(", "json_str", ")", "\n", "\n", "", "with", "tf", ".", "gfile", ".", "Open", "(", "m_name", ")", "as", "fd", ":", "\n", "        ", "tf", ".", "logging", ".", "info", "(", "\"Restoring model parameters from %s\"", "%", "m_name", ")", "\n", "json_str", "=", "fd", ".", "readline", "(", ")", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.bin.trainer.export_params": [[143, 151], ["os.path.join", "tensorflow.gfile.Exists", "tensorflow.gfile.MkDir", "tensorflow.gfile.Open", "fd.write", "params.to_json"], "function", ["None"], ["\n", "", "return", "params", "\n", "\n", "\n", "", "def", "export_params", "(", "output_dir", ",", "name", ",", "params", ")", ":", "\n", "    ", "if", "not", "tf", ".", "gfile", ".", "Exists", "(", "output_dir", ")", ":", "\n", "        ", "tf", ".", "gfile", ".", "MkDir", "(", "output_dir", ")", "\n", "\n", "# Save params as params.json", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.bin.trainer.collect_params": [[153, 160], ["tensorflow.contrib.training.HParams", "six.iterkeys", "params.values", "tf.contrib.training.HParams.add_hparam", "getattr"], "function", ["None"], ["with", "tf", ".", "gfile", ".", "Open", "(", "filename", ",", "\"w\"", ")", "as", "fd", ":", "\n", "        ", "fd", ".", "write", "(", "params", ".", "to_json", "(", ")", ")", "\n", "\n", "\n", "", "", "def", "collect_params", "(", "all_params", ",", "params", ")", ":", "\n", "    ", "collected", "=", "tf", ".", "contrib", ".", "training", ".", "HParams", "(", ")", "\n", "\n", "for", "k", "in", "six", ".", "iterkeys", "(", "params", ".", "values", "(", ")", ")", ":", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.bin.trainer.merge_parameters": [[162, 178], ["tensorflow.contrib.training.HParams", "six.iteritems", "tf.contrib.training.HParams.values", "six.iteritems", "params1.values", "tf.contrib.training.HParams.add_hparam", "params2.values", "setattr", "tf.contrib.training.HParams.add_hparam"], "function", ["None"], ["\n", "", "return", "collected", "\n", "\n", "\n", "", "def", "merge_parameters", "(", "params1", ",", "params2", ")", ":", "\n", "    ", "params", "=", "tf", ".", "contrib", ".", "training", ".", "HParams", "(", ")", "\n", "\n", "for", "(", "k", ",", "v", ")", "in", "six", ".", "iteritems", "(", "params1", ".", "values", "(", ")", ")", ":", "\n", "        ", "params", ".", "add_hparam", "(", "k", ",", "v", ")", "\n", "\n", "", "params_dict", "=", "params", ".", "values", "(", ")", "\n", "\n", "for", "(", "k", ",", "v", ")", "in", "six", ".", "iteritems", "(", "params2", ".", "values", "(", ")", ")", ":", "\n", "        ", "if", "k", "in", "params_dict", ":", "\n", "# Override", "\n", "            ", "setattr", "(", "params", ",", "k", ",", "v", ")", "\n", "", "else", ":", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.bin.trainer.override_parameters": [[180, 215], ["params.parse", "thumt.process_vocabulary", "thumt.process_vocabulary", "thumt.load_vocabulary", "thumt.load_vocabulary", "thumt.get_control_mapping", "thumt.get_control_mapping"], "function", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.data.vocab.process_vocabulary", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.data.vocab.process_vocabulary", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.data.vocab.load_vocabulary", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.data.vocab.load_vocabulary", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.data.vocab.get_control_mapping", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.data.vocab.get_control_mapping"], ["\n", "", "", "return", "params", "\n", "\n", "\n", "", "def", "override_parameters", "(", "params", ",", "args", ")", ":", "\n", "    ", "params", ".", "model", "=", "args", ".", "model", "\n", "params", ".", "input", "=", "args", ".", "input", "or", "params", ".", "input", "\n", "params", ".", "output", "=", "args", ".", "output", "or", "params", ".", "output", "\n", "params", ".", "record", "=", "args", ".", "record", "or", "params", ".", "record", "\n", "params", ".", "vocab", "=", "args", ".", "vocabulary", "or", "params", ".", "vocab", "\n", "params", ".", "validation", "=", "args", ".", "validation", "or", "params", ".", "validation", "\n", "params", ".", "references", "=", "args", ".", "references", "or", "params", ".", "references", "\n", "params", ".", "parse", "(", "args", ".", "parameters", ")", "\n", "\n", "params", ".", "vocabulary", "=", "{", "\n", "\"source\"", ":", "vocabulary", ".", "load_vocabulary", "(", "params", ".", "vocab", "[", "0", "]", ")", ",", "\n", "\"target\"", ":", "vocabulary", ".", "load_vocabulary", "(", "params", ".", "vocab", "[", "1", "]", ")", "\n", "}", "\n", "params", ".", "vocabulary", "[", "\"source\"", "]", "=", "vocabulary", ".", "process_vocabulary", "(", "\n", "params", ".", "vocabulary", "[", "\"source\"", "]", ",", "params", "\n", ")", "\n", "params", ".", "vocabulary", "[", "\"target\"", "]", "=", "vocabulary", ".", "process_vocabulary", "(", "\n", "params", ".", "vocabulary", "[", "\"target\"", "]", ",", "params", "\n", ")", "\n", "\n", "control_symbols", "=", "[", "params", ".", "pad", ",", "params", ".", "bos", ",", "params", ".", "eos", ",", "params", ".", "unk", "]", "\n", "\n", "params", ".", "mapping", "=", "{", "\n", "\"source\"", ":", "vocabulary", ".", "get_control_mapping", "(", "\n", "params", ".", "vocabulary", "[", "\"source\"", "]", ",", "\n", "control_symbols", "\n", ")", ",", "\n", "\"target\"", ":", "vocabulary", ".", "get_control_mapping", "(", "\n", "params", ".", "vocabulary", "[", "\"target\"", "]", ",", "\n", "control_symbols", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.bin.trainer.get_initializer": [[217, 233], ["tensorflow.random_uniform_initializer", "tensorflow.random_normal_initializer", "tensorflow.variance_scaling_initializer", "tensorflow.variance_scaling_initializer", "ValueError"], "function", ["None"], ["\n", "return", "params", "\n", "\n", "\n", "", "def", "get_initializer", "(", "params", ")", ":", "\n", "    ", "if", "params", ".", "initializer", "==", "\"uniform\"", ":", "\n", "        ", "max_val", "=", "params", ".", "initializer_gain", "\n", "return", "tf", ".", "random_uniform_initializer", "(", "-", "max_val", ",", "max_val", ")", "\n", "", "elif", "params", ".", "initializer", "==", "\"normal\"", ":", "\n", "        ", "return", "tf", ".", "random_normal_initializer", "(", "0.0", ",", "params", ".", "initializer_gain", ")", "\n", "", "elif", "params", ".", "initializer", "==", "\"normal_unit_scaling\"", ":", "\n", "        ", "return", "tf", ".", "variance_scaling_initializer", "(", "params", ".", "initializer_gain", ",", "\n", "mode", "=", "\"fan_avg\"", ",", "\n", "distribution", "=", "\"normal\"", ")", "\n", "", "elif", "params", ".", "initializer", "==", "\"uniform_unit_scaling\"", ":", "\n", "        ", "return", "tf", ".", "variance_scaling_initializer", "(", "params", ".", "initializer_gain", ",", "\n", "mode", "=", "\"fan_avg\"", ",", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.bin.trainer.get_learning_rate_decay": [[235, 252], ["tensorflow.to_float", "tensorflow.to_float", "tensorflow.minimum", "tensorflow.train.piecewise_constant", "tensorflow.to_int32", "ValueError"], "function", ["None"], ["", "else", ":", "\n", "        ", "raise", "ValueError", "(", "\"Unrecognized initializer: %s\"", "%", "params", ".", "initializer", ")", "\n", "\n", "\n", "", "", "def", "get_learning_rate_decay", "(", "learning_rate", ",", "global_step", ",", "params", ")", ":", "\n", "    ", "if", "params", ".", "learning_rate_decay", "in", "[", "\"linear_warmup_rsqrt_decay\"", ",", "\"noam\"", "]", ":", "\n", "        ", "step", "=", "tf", ".", "to_float", "(", "global_step", ")", "\n", "warmup_steps", "=", "tf", ".", "to_float", "(", "params", ".", "warmup_steps", ")", "\n", "multiplier", "=", "params", ".", "hidden_size", "**", "-", "0.5", "\n", "decay", "=", "multiplier", "*", "tf", ".", "minimum", "(", "(", "step", "+", "1", ")", "*", "(", "warmup_steps", "**", "-", "1.5", ")", ",", "\n", "(", "step", "+", "1", ")", "**", "-", "0.5", ")", "\n", "\n", "return", "learning_rate", "*", "decay", "\n", "", "elif", "params", ".", "learning_rate_decay", "==", "\"piecewise_constant\"", ":", "\n", "        ", "return", "tf", ".", "train", ".", "piecewise_constant", "(", "tf", ".", "to_int32", "(", "global_step", ")", ",", "\n", "params", ".", "learning_rate_boundaries", ",", "\n", "params", ".", "learning_rate_values", ")", "\n", "", "elif", "params", ".", "learning_rate_decay", "==", "\"none\"", ":", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.bin.trainer.session_config": [[254, 268], ["tensorflow.OptimizerOptions", "tensorflow.GraphOptions", "tensorflow.ConfigProto", "thumt.is_distributed_training_mode", "str", "thumt.local_rank", "str"], "function", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.distribute.is_distributed_training_mode", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.distribute.local_rank"], ["", "else", ":", "\n", "        ", "raise", "ValueError", "(", "\"Unknown learning_rate_decay\"", ")", "\n", "\n", "\n", "", "", "def", "session_config", "(", "params", ")", ":", "\n", "    ", "optimizer_options", "=", "tf", ".", "OptimizerOptions", "(", "opt_level", "=", "tf", ".", "OptimizerOptions", ".", "L1", ",", "\n", "do_function_inlining", "=", "True", ")", "\n", "graph_options", "=", "tf", ".", "GraphOptions", "(", "optimizer_options", "=", "optimizer_options", ")", "\n", "config", "=", "tf", ".", "ConfigProto", "(", "allow_soft_placement", "=", "True", ",", "\n", "graph_options", "=", "graph_options", ")", "\n", "\n", "if", "distribute", ".", "is_distributed_training_mode", "(", ")", ":", "\n", "        ", "config", ".", "gpu_options", ".", "visible_device_list", "=", "str", "(", "distribute", ".", "local_rank", "(", ")", ")", "\n", "", "elif", "params", ".", "device_list", ":", "\n", "        ", "device_str", "=", "\",\"", ".", "join", "(", "[", "str", "(", "i", ")", "for", "i", "in", "params", ".", "device_list", "]", ")", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.bin.trainer.decode_target_ids": [[270, 294], ["decoded.append", "isinstance", "syms.append", "isinstance", "sym.decode.decode"], "function", ["None"], ["\n", "", "return", "config", "\n", "\n", "\n", "", "def", "decode_target_ids", "(", "inputs", ",", "params", ")", ":", "\n", "    ", "decoded", "=", "[", "]", "\n", "vocab", "=", "params", ".", "vocabulary", "[", "\"target\"", "]", "\n", "\n", "for", "item", "in", "inputs", ":", "\n", "        ", "syms", "=", "[", "]", "\n", "for", "idx", "in", "item", ":", "\n", "            ", "if", "isinstance", "(", "idx", ",", "six", ".", "integer_types", ")", ":", "\n", "                ", "sym", "=", "vocab", "[", "idx", "]", "\n", "", "else", ":", "\n", "                ", "sym", "=", "idx", "\n", "if", "not", "isinstance", "(", "sym", ",", "six", ".", "string_types", ")", ":", "\n", "                    ", "sym", "=", "sym", ".", "decode", "(", "\"utf-8\"", ")", "\n", "\n", "", "", "if", "sym", "==", "params", ".", "eos", ":", "\n", "                ", "break", "\n", "\n", "", "if", "sym", "==", "params", ".", "pad", ":", "\n", "                ", "break", "\n", "\n", "", "syms", ".", "append", "(", "sym", ")", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.bin.trainer.restore_variables": [[296, 333], ["tensorflow.logging.info", "tensorflow.train.list_variables", "tensorflow.train.load_checkpoint", "tensorflow.get_collection", "tensorflow.group", "tensorflow.no_op", "tf.train.load_checkpoint.get_tensor", "name.endswith", "name.split", "re.search().group", "var.name.split", "tensorflow.logging.info", "ops.append", "name.endswith", "tensorflow.assign", "re.search().group", "tensorflow.logging.info", "ops.append", "tensorflow.logging.info", "re.search", "tensorflow.assign", "re.search"], "function", ["None"], ["\n", "", "return", "decoded", "\n", "\n", "\n", "", "def", "restore_variables", "(", "checkpoint", ")", ":", "\n", "    ", "if", "not", "checkpoint", ":", "\n", "        ", "return", "tf", ".", "no_op", "(", "\"restore_op\"", ")", "\n", "\n", "# Load checkpoints", "\n", "", "tf", ".", "logging", ".", "info", "(", "\"Loading %s\"", "%", "checkpoint", ")", "\n", "var_list", "=", "tf", ".", "train", ".", "list_variables", "(", "checkpoint", ")", "\n", "reader", "=", "tf", ".", "train", ".", "load_checkpoint", "(", "checkpoint", ")", "\n", "values", "=", "{", "}", "\n", "\n", "for", "(", "name", ",", "shape", ")", "in", "var_list", ":", "\n", "        ", "tensor", "=", "reader", ".", "get_tensor", "(", "name", ")", "\n", "name", "=", "name", ".", "split", "(", "\":\"", ")", "[", "0", "]", "\n", "# additionally save beta*_power for all cases", "\n", "if", "name", ".", "endswith", "(", "\"power\"", ")", ":", "\n", "            ", "beta_power", "=", "re", ".", "search", "(", "r'(beta[1-2]_power)'", ",", "name", ")", ".", "group", "(", ")", "\n", "values", "[", "beta_power", "]", "=", "tensor", "\n", "", "values", "[", "name", "]", "=", "tensor", "\n", "\n", "#var_list = tf.trainable_variables()", "\n", "", "var_list", "=", "tf", ".", "get_collection", "(", "tf", ".", "GraphKeys", ".", "GLOBAL_VARIABLES", ",", "scope", "=", "''", ")", "\n", "ops", "=", "[", "]", "\n", "for", "var", "in", "var_list", ":", "\n", "        ", "name", "=", "var", ".", "name", ".", "split", "(", "\":\"", ")", "[", "0", "]", "\n", "\n", "if", "name", "in", "values", ":", "\n", "            ", "tf", ".", "logging", ".", "info", "(", "\"Restore %s\"", "%", "var", ".", "name", ")", "\n", "ops", ".", "append", "(", "tf", ".", "assign", "(", "var", ",", "values", "[", "name", "]", ")", ")", "\n", "", "elif", "name", ".", "endswith", "(", "\"power\"", ")", ":", "\n", "            ", "beta_power", "=", "re", ".", "search", "(", "r'(beta[1-2]_power)'", ",", "name", ")", ".", "group", "(", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"Restore %s with %s\"", "%", "(", "var", ".", "name", ",", "beta_power", ")", ")", "\n", "ops", ".", "append", "(", "tf", ".", "assign", "(", "var", ",", "values", "[", "beta_power", "]", ")", ")", "\n", "", "else", ":", "\n", "            ", "tf", ".", "logging", ".", "info", "(", "\"Not restore %s\"", "%", "var", ".", "name", ")", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.bin.trainer.print_variables": [[335, 346], ["sorted", "tensorflow.logging.info", "list", "tensorflow.logging.info", "numpy.prod().tolist", "tensorflow.trainable_variables", "v.name[].ljust", "str().ljust", "numpy.prod", "str", "numpy.array", "v.shape.as_list"], "function", ["None"], ["\n", "", "", "return", "tf", ".", "group", "(", "*", "ops", ",", "name", "=", "\"restore_op\"", ")", "\n", "\n", "\n", "", "def", "print_variables", "(", ")", ":", "\n", "    ", "all_weights", "=", "{", "v", ".", "name", ":", "v", "for", "v", "in", "tf", ".", "trainable_variables", "(", ")", "}", "\n", "total_size", "=", "0", "\n", "\n", "for", "v_name", "in", "sorted", "(", "list", "(", "all_weights", ")", ")", ":", "\n", "        ", "v", "=", "all_weights", "[", "v_name", "]", "\n", "tf", ".", "logging", ".", "info", "(", "\"%s\\tshape    %s\"", ",", "v", ".", "name", "[", ":", "-", "2", "]", ".", "ljust", "(", "80", ")", ",", "\n", "str", "(", "v", ".", "shape", ")", ".", "ljust", "(", "20", ")", ")", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.bin.trainer.main": [[348, 541], ["tensorflow.logging.set_verbosity", "thumt.get_model", "trainer.default_parameters", "trainer.merge_parameters", "trainer.import_params", "trainer.override_parameters", "thumt.enable_distributed_training", "models.get_model.get_parameters", "thumt.rank", "trainer.export_params", "trainer.export_params", "tensorflow.Graph().as_default", "trainer.get_initializer", "tensorflow.contrib.layers.l1_l2_regularizer", "models.get_model.", "tensorflow.train.get_or_create_global_step", "thumt.parallel_model", "trainer.get_learning_rate_decay", "tensorflow.convert_to_tensor", "tensorflow.summary.scalar", "tensorflow.summary.scalar", "thumt.MultiStepOptimizer", "radam.RAdamOptimizer.compute_gradients", "radam.RAdamOptimizer.apply_gradients", "thumt.get_broadcast_hook", "print", "print", "sys.stdout.flush", "trainer.restore_variables", "tf.train.get_or_create_global_step.assign", "trainer.collect_params", "thumt.get_training_input", "thumt.get_input_features", "model_cls.get_training_func", "tensorflow.add_n", "len", "tensorflow.losses.get_regularization_loss", "thumt.rank", "trainer.print_variables", "tensorflow.train.AdamOptimizer", "thumt.LossScalingOptimizer", "list", "tensorflow.clip_by_global_norm", "zip", "thumt.sort_and_zip_files", "tensorflow.train.StopAtStepHook", "tensorflow.train.NanTensorHook", "tensorflow.train.LoggingTensorHook", "train_hooks.append", "thumt.rank", "tensorflow.train.Saver", "tensorflow.add_to_collection", "train_hooks.append", "step_context.session.run", "tf.train.Saver.save", "tensorflow.train.MonitoredTrainingSession", "sess.run_step_fn", "models.get_model.get_parameters", "tensorflow.Graph", "os.path.join", "tensorflow.contrib.opt.LazyAdamOptimizer", "zip", "list", "tensorflow.trainable_variables", "thumt.MultiStepHook", "train_hooks.append", "sess.run", "sess.should_stop", "sess.run", "thumt.RAdamOptimizer", "RuntimeError", "tensorflow.train.CheckpointSaverHook", "thumt.MultiStepHook", "trainer.session_config", "sess.run_step_fn", "thumt.EvaluationHook", "trainer.session_config", "thumt.create_inference_graph", "eval_input_fn", "trainer.decode_target_ids"], "function", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.__init__.get_model", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.bin.scorer_token_level.default_parameters", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.bin.scorer_token_level.merge_parameters", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.bin.scorer_token_level.import_params", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.bin.scorer_token_level.override_parameters", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.distribute.enable_distributed_training", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.transformer_lrp.TransformerLRP.get_parameters", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.distribute.rank", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.bin.trainer.export_params", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.bin.trainer.export_params", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.bin.trainer.get_initializer", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.parallel.parallel_model", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.bin.trainer.get_learning_rate_decay", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.optimizers.MultiStepOptimizer.compute_gradients", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.optimizers.MultiStepOptimizer.apply_gradients", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.distribute.get_broadcast_hook", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.bin.trainer.restore_variables", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.bin.trainer.collect_params", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.data.dataset.get_training_input", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.data.record.get_input_features", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.transformer_lrp.TransformerLRP.get_training_func", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.distribute.rank", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.bin.trainer.print_variables", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.data.dataset.sort_and_zip_files", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.distribute.rank", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.bert_dictionary.BertDictionary.save", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.transformer_lrp.TransformerLRP.get_parameters", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.bin.scorer_token_level.session_config", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.bin.scorer_token_level.session_config", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.inference.create_inference_graph", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.bin.trainer.decode_target_ids"], ["total_size", "+=", "v_size", "\n", "", "tf", ".", "logging", ".", "info", "(", "\"Total trainable variables size: %d\"", ",", "total_size", ")", "\n", "\n", "\n", "", "def", "main", "(", "args", ")", ":", "\n", "    ", "if", "args", ".", "distribute", ":", "\n", "        ", "distribute", ".", "enable_distributed_training", "(", ")", "\n", "\n", "", "tf", ".", "logging", ".", "set_verbosity", "(", "tf", ".", "logging", ".", "INFO", ")", "\n", "model_cls", "=", "models", ".", "get_model", "(", "args", ".", "model", ")", "\n", "params", "=", "default_parameters", "(", ")", "\n", "\n", "# Import and override parameters", "\n", "# Priorities (low -> high):", "\n", "# default -> saved -> command", "\n", "params", "=", "merge_parameters", "(", "params", ",", "model_cls", ".", "get_parameters", "(", ")", ")", "\n", "params", "=", "import_params", "(", "args", ".", "output", ",", "args", ".", "model", ",", "params", ")", "\n", "override_parameters", "(", "params", ",", "args", ")", "\n", "\n", "# Export all parameters and model specific parameters", "\n", "if", "distribute", ".", "rank", "(", ")", "==", "0", ":", "\n", "        ", "export_params", "(", "params", ".", "output", ",", "\"params.json\"", ",", "params", ")", "\n", "export_params", "(", "\n", "params", ".", "output", ",", "\n", "\"%s.json\"", "%", "args", ".", "model", ",", "\n", "collect_params", "(", "params", ",", "model_cls", ".", "get_parameters", "(", ")", ")", "\n", ")", "\n", "\n", "# Build Graph", "\n", "", "with", "tf", ".", "Graph", "(", ")", ".", "as_default", "(", ")", ":", "\n", "        ", "if", "not", "params", ".", "record", ":", "\n", "# Build input queue", "\n", "            ", "features", "=", "dataset", ".", "get_training_input", "(", "params", ".", "input", ",", "params", ")", "\n", "", "else", ":", "\n", "            ", "features", "=", "record", ".", "get_input_features", "(", "\n", "os", ".", "path", ".", "join", "(", "params", ".", "record", ",", "\"*train*\"", ")", ",", "\"train\"", ",", "params", "\n", ")", "\n", "\n", "# Build model", "\n", "", "initializer", "=", "get_initializer", "(", "params", ")", "\n", "regularizer", "=", "tf", ".", "contrib", ".", "layers", ".", "l1_l2_regularizer", "(", "\n", "scale_l1", "=", "params", ".", "scale_l1", ",", "scale_l2", "=", "params", ".", "scale_l2", ")", "\n", "model", "=", "model_cls", "(", "params", ")", "\n", "# Create global step", "\n", "global_step", "=", "tf", ".", "train", ".", "get_or_create_global_step", "(", ")", "\n", "dtype", "=", "tf", ".", "float16", "if", "args", ".", "half", "else", "None", "\n", "\n", "# Multi-GPU setting", "\n", "sharded_mle_losses", ",", "sharded_bridge_losses", "=", "parallel", ".", "parallel_model", "(", "\n", "model", ".", "get_training_func", "(", "initializer", ",", "regularizer", ",", "dtype", ")", ",", "\n", "features", ",", "\n", "params", ".", "device_list", "\n", ")", "\n", "mle_loss", "=", "tf", ".", "add_n", "(", "sharded_mle_losses", ")", "/", "len", "(", "sharded_mle_losses", ")", "\n", "bridge_loss", "=", "tf", ".", "add_n", "(", "sharded_bridge_losses", ")", "/", "len", "(", "sharded_bridge_losses", ")", "\n", "loss", "=", "params", ".", "mle_rate", "*", "mle_loss", "+", "params", ".", "bridge_rate", "*", "bridge_loss", "+", "tf", ".", "losses", ".", "get_regularization_loss", "(", ")", "\n", "\n", "if", "distribute", ".", "rank", "(", ")", "==", "0", ":", "\n", "            ", "print_variables", "(", ")", "\n", "\n", "", "learning_rate", "=", "get_learning_rate_decay", "(", "params", ".", "learning_rate", ",", "\n", "global_step", ",", "params", ")", "\n", "learning_rate", "=", "tf", ".", "convert_to_tensor", "(", "learning_rate", ",", "dtype", "=", "tf", ".", "float32", ")", "\n", "\n", "tf", ".", "summary", ".", "scalar", "(", "\"loss\"", ",", "loss", ")", "\n", "tf", ".", "summary", ".", "scalar", "(", "\"learning_rate\"", ",", "learning_rate", ")", "\n", "\n", "# Create optimizer", "\n", "if", "params", ".", "optimizer", "==", "\"Adam\"", ":", "\n", "            ", "opt", "=", "tf", ".", "train", ".", "AdamOptimizer", "(", "learning_rate", ",", "\n", "beta1", "=", "params", ".", "adam_beta1", ",", "\n", "beta2", "=", "params", ".", "adam_beta2", ",", "\n", "epsilon", "=", "params", ".", "adam_epsilon", ")", "\n", "", "elif", "params", ".", "optimizer", "==", "\"LazyAdam\"", ":", "\n", "            ", "opt", "=", "tf", ".", "contrib", ".", "opt", ".", "LazyAdamOptimizer", "(", "learning_rate", ",", "\n", "beta1", "=", "params", ".", "adam_beta1", ",", "\n", "beta2", "=", "params", ".", "adam_beta2", ",", "\n", "epsilon", "=", "params", ".", "adam_epsilon", ")", "\n", "", "elif", "params", ".", "optimizer", "==", "\"RAdam\"", ":", "\n", "            ", "opt", "=", "radam", ".", "RAdamOptimizer", "(", "learning_rate", "=", "learning_rate", ",", "\n", "beta1", "=", "params", ".", "adam_beta1", ",", "\n", "beta2", "=", "params", ".", "adam_beta2", ",", "\n", "epsilon", "=", "params", ".", "adam_epsilon", ")", "\n", "", "else", ":", "\n", "            ", "raise", "RuntimeError", "(", "\"Optimizer %s not supported\"", "%", "params", ".", "optimizer", ")", "\n", "\n", "", "opt", "=", "optimizers", ".", "MultiStepOptimizer", "(", "opt", ",", "params", ".", "update_cycle", ")", "\n", "\n", "if", "args", ".", "half", ":", "\n", "            ", "opt", "=", "optimizers", ".", "LossScalingOptimizer", "(", "opt", ",", "params", ".", "loss_scale", ")", "\n", "\n", "# Optimization", "\n", "", "grads_and_vars", "=", "opt", ".", "compute_gradients", "(", "\n", "loss", ",", "colocate_gradients_with_ops", "=", "True", ")", "\n", "\n", "if", "params", ".", "clip_grad_norm", ":", "\n", "            ", "grads", ",", "var_list", "=", "list", "(", "zip", "(", "*", "grads_and_vars", ")", ")", "\n", "grads", ",", "_", "=", "tf", ".", "clip_by_global_norm", "(", "grads", ",", "params", ".", "clip_grad_norm", ")", "\n", "grads_and_vars", "=", "zip", "(", "grads", ",", "var_list", ")", "\n", "\n", "", "train_op", "=", "opt", ".", "apply_gradients", "(", "grads_and_vars", ",", "\n", "global_step", "=", "global_step", ")", "\n", "\n", "# Validation", "\n", "if", "params", ".", "validation", "and", "params", ".", "references", "[", "0", "]", ":", "\n", "            ", "files", "=", "[", "params", ".", "validation", "]", "+", "list", "(", "params", ".", "references", ")", "\n", "eval_inputs", "=", "dataset", ".", "sort_and_zip_files", "(", "files", ")", "\n", "eval_input_fn", "=", "dataset", ".", "get_evaluation_input", "\n", "", "else", ":", "\n", "            ", "eval_input_fn", "=", "None", "\n", "\n", "# Hooks", "\n", "", "train_hooks", "=", "[", "\n", "tf", ".", "train", ".", "StopAtStepHook", "(", "last_step", "=", "params", ".", "train_steps", ")", ",", "\n", "tf", ".", "train", ".", "NanTensorHook", "(", "loss", ")", ",", "\n", "tf", ".", "train", ".", "LoggingTensorHook", "(", "\n", "{", "\n", "\"step\"", ":", "global_step", ",", "\n", "\"loss\"", ":", "loss", ",", "\n", "\"mle_loss\"", ":", "mle_loss", ",", "\n", "\"bridge_loss\"", ":", "bridge_loss", "\n", "}", ",", "\n", "every_n_iter", "=", "1", "\n", ")", "\n", "]", "\n", "\n", "broadcast_hook", "=", "distribute", ".", "get_broadcast_hook", "(", ")", "\n", "\n", "if", "broadcast_hook", ":", "\n", "            ", "train_hooks", ".", "append", "(", "broadcast_hook", ")", "\n", "\n", "", "print", "(", "params", ".", "save_checkpoint_steps", ")", "\n", "print", "(", "params", ".", "output", ")", "\n", "sys", ".", "stdout", ".", "flush", "(", ")", "\n", "if", "distribute", ".", "rank", "(", ")", "==", "0", ":", "\n", "# Add hooks", "\n", "            ", "save_vars", "=", "tf", ".", "trainable_variables", "(", ")", "+", "[", "global_step", "]", "\n", "saver", "=", "tf", ".", "train", ".", "Saver", "(", "\n", "var_list", "=", "save_vars", "if", "params", ".", "only_save_trainable", "else", "None", ",", "\n", "max_to_keep", "=", "params", ".", "keep_checkpoint_max", ",", "\n", "sharded", "=", "False", "\n", ")", "\n", "tf", ".", "add_to_collection", "(", "tf", ".", "GraphKeys", ".", "SAVERS", ",", "saver", ")", "\n", "train_hooks", ".", "append", "(", "\n", "hooks", ".", "MultiStepHook", "(", "\n", "tf", ".", "train", ".", "CheckpointSaverHook", "(", "\n", "checkpoint_dir", "=", "params", ".", "output", ",", "\n", "save_secs", "=", "params", ".", "save_checkpoint_secs", "or", "None", ",", "\n", "save_steps", "=", "params", ".", "save_checkpoint_steps", "or", "None", ",", "\n", "saver", "=", "saver", ")", ",", "\n", "step", "=", "params", ".", "update_cycle", ")", "\n", ")", "\n", "\n", "if", "eval_input_fn", "is", "not", "None", ":", "\n", "                ", "train_hooks", ".", "append", "(", "\n", "hooks", ".", "MultiStepHook", "(", "\n", "hooks", ".", "EvaluationHook", "(", "\n", "lambda", "f", ":", "inference", ".", "create_inference_graph", "(", "\n", "[", "model", "]", ",", "f", ",", "params", "\n", ")", ",", "\n", "lambda", ":", "eval_input_fn", "(", "eval_inputs", ",", "params", ")", ",", "\n", "lambda", "x", ":", "decode_target_ids", "(", "x", ",", "params", ")", ",", "\n", "params", ".", "output", ",", "\n", "session_config", "(", "params", ")", ",", "\n", "params", ".", "keep_top_checkpoint_max", ",", "\n", "eval_secs", "=", "params", ".", "eval_secs", ",", "\n", "eval_steps", "=", "params", ".", "eval_steps", "\n", ")", ",", "\n", "step", "=", "params", ".", "update_cycle", "\n", ")", "\n", ")", "\n", "", "checkpoint_dir", "=", "params", ".", "output", "\n", "", "else", ":", "\n", "            ", "checkpoint_dir", "=", "None", "\n", "\n", "", "restore_op", "=", "restore_variables", "(", "args", ".", "checkpoint", ")", "\n", "zero_step_op", "=", "global_step", ".", "assign", "(", "1", ")", "\n", "step_count", "=", "0", "\n", "\n", "def", "restore_fn", "(", "step_context", ")", ":", "\n", "            ", "step_context", ".", "session", ".", "run", "(", "restore_op", ")", "\n", "", "def", "save_fn", "(", "step_context", ")", ":", "\n", "            ", "saver", ".", "save", "(", "step_context", ".", "session", ",", "params", ".", "output", "+", "\"/model.ckpt\"", ",", "global_step", ")", "\n", "\n", "# Create session, do not use default CheckpointSaverHook", "\n", "", "with", "tf", ".", "train", ".", "MonitoredTrainingSession", "(", "\n", "checkpoint_dir", "=", "checkpoint_dir", ",", "hooks", "=", "train_hooks", ",", "\n", "save_checkpoint_secs", "=", "None", ",", "\n", "config", "=", "session_config", "(", "params", ")", ")", "as", "sess", ":", "\n", "# Restore pre-trained variables", "\n", "            ", "sess", ".", "run_step_fn", "(", "restore_fn", ")", "\n", "if", "params", ".", "zero_step", ":", "\n", "                ", "sess", ".", "run", "(", "zero_step_op", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.bin.scorer.parse_args": [[19, 42], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args"], "function", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.scripts.build_vocab.parse_args"], ["    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "\n", "description", "=", "\"Translate using existing NMT models\"", ",", "\n", "usage", "=", "\"translator.py [<args>] [-h | --help]\"", "\n", ")", "\n", "\n", "# input files", "\n", "parser", ".", "add_argument", "(", "\"--input\"", ",", "type", "=", "str", ",", "required", "=", "True", ",", "nargs", "=", "2", ",", "\n", "help", "=", "\"Path of input file\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--output\"", ",", "type", "=", "str", ",", "required", "=", "True", ",", "\n", "help", "=", "\"Path of output file\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--checkpoint\"", ",", "type", "=", "str", ",", "required", "=", "True", ",", "\n", "help", "=", "\"Path of trained models\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--vocabulary\"", ",", "type", "=", "str", ",", "nargs", "=", "2", ",", "required", "=", "True", ",", "\n", "help", "=", "\"Path of source and target vocabulary\"", ")", "\n", "\n", "# model and configuration", "\n", "parser", ".", "add_argument", "(", "\"--model\"", ",", "type", "=", "str", ",", "required", "=", "True", ",", "\n", "help", "=", "\"Name of the model\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--parameters\"", ",", "type", "=", "str", ",", "\n", "help", "=", "\"Additional hyper parameters\"", ")", "\n", "\n", "return", "parser", ".", "parse_args", "(", ")", "\n", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.bin.scorer.default_parameters": [[44, 63], ["tensorflow.contrib.training.HParams"], "function", ["None"], ["    ", "params", "=", "tf", ".", "contrib", ".", "training", ".", "HParams", "(", "\n", "input", "=", "None", ",", "\n", "output", "=", "None", ",", "\n", "vocabulary", "=", "None", ",", "\n", "model", "=", "None", ",", "\n", "# vocabulary specific", "\n", "pad", "=", "\"<pad>\"", ",", "\n", "bos", "=", "\"<bos>\"", ",", "\n", "eos", "=", "\"<eos>\"", ",", "\n", "unk", "=", "\"<unk>\"", ",", "\n", "mapping", "=", "None", ",", "\n", "append_eos", "=", "False", ",", "\n", "device_list", "=", "[", "0", "]", ",", "\n", "num_threads", "=", "6", ",", "\n", "eval_batch_size", "=", "32", "\n", ")", "\n", "\n", "return", "params", "\n", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.bin.scorer.merge_parameters": [[65, 81], ["tensorflow.contrib.training.HParams", "six.iteritems", "tf.contrib.training.HParams.values", "six.iteritems", "params1.values", "tf.contrib.training.HParams.add_hparam", "params2.values", "setattr", "tf.contrib.training.HParams.add_hparam"], "function", ["None"], ["    ", "params", "=", "tf", ".", "contrib", ".", "training", ".", "HParams", "(", ")", "\n", "\n", "for", "(", "k", ",", "v", ")", "in", "six", ".", "iteritems", "(", "params1", ".", "values", "(", ")", ")", ":", "\n", "        ", "params", ".", "add_hparam", "(", "k", ",", "v", ")", "\n", "\n", "", "params_dict", "=", "params", ".", "values", "(", ")", "\n", "\n", "for", "(", "k", ",", "v", ")", "in", "six", ".", "iteritems", "(", "params2", ".", "values", "(", ")", ")", ":", "\n", "        ", "if", "k", "in", "params_dict", ":", "\n", "# Override", "\n", "            ", "setattr", "(", "params", ",", "k", ",", "v", ")", "\n", "", "else", ":", "\n", "            ", "params", ".", "add_hparam", "(", "k", ",", "v", ")", "\n", "\n", "", "", "return", "params", "\n", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.bin.scorer.import_params": [[83, 96], ["os.path.abspath", "os.path.join", "tensorflow.gfile.Exists", "tensorflow.gfile.Open", "tensorflow.logging.info", "fd.readline", "params.parse_json"], "function", ["None"], ["    ", "model_dir", "=", "os", ".", "path", ".", "abspath", "(", "model_dir", ")", "\n", "m_name", "=", "os", ".", "path", ".", "join", "(", "model_dir", ",", "model_name", "+", "\".json\"", ")", "\n", "\n", "if", "not", "tf", ".", "gfile", ".", "Exists", "(", "m_name", ")", ":", "\n", "        ", "return", "params", "\n", "\n", "", "with", "tf", ".", "gfile", ".", "Open", "(", "m_name", ")", "as", "fd", ":", "\n", "        ", "tf", ".", "logging", ".", "info", "(", "\"Restoring model parameters from %s\"", "%", "m_name", ")", "\n", "json_str", "=", "fd", ".", "readline", "(", ")", "\n", "params", ".", "parse_json", "(", "json_str", ")", "\n", "\n", "", "return", "params", "\n", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.bin.scorer.override_parameters": [[98, 127], ["thumt.process_vocabulary", "thumt.process_vocabulary", "params.parse", "thumt.load_vocabulary", "thumt.load_vocabulary", "thumt.get_control_mapping", "thumt.get_control_mapping"], "function", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.data.vocab.process_vocabulary", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.data.vocab.process_vocabulary", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.data.vocab.load_vocabulary", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.data.vocab.load_vocabulary", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.data.vocab.get_control_mapping", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.data.vocab.get_control_mapping"], ["    ", "if", "args", ".", "parameters", ":", "\n", "        ", "params", ".", "parse", "(", "args", ".", "parameters", ")", "\n", "\n", "", "params", ".", "vocabulary", "=", "{", "\n", "\"source\"", ":", "vocabulary", ".", "load_vocabulary", "(", "args", ".", "vocabulary", "[", "0", "]", ")", ",", "\n", "\"target\"", ":", "vocabulary", ".", "load_vocabulary", "(", "args", ".", "vocabulary", "[", "1", "]", ")", "\n", "}", "\n", "params", ".", "vocabulary", "[", "\"source\"", "]", "=", "vocabulary", ".", "process_vocabulary", "(", "\n", "params", ".", "vocabulary", "[", "\"source\"", "]", ",", "params", "\n", ")", "\n", "params", ".", "vocabulary", "[", "\"target\"", "]", "=", "vocabulary", ".", "process_vocabulary", "(", "\n", "params", ".", "vocabulary", "[", "\"target\"", "]", ",", "params", "\n", ")", "\n", "\n", "control_symbols", "=", "[", "params", ".", "pad", ",", "params", ".", "bos", ",", "params", ".", "eos", ",", "params", ".", "unk", "]", "\n", "\n", "params", ".", "mapping", "=", "{", "\n", "\"source\"", ":", "vocabulary", ".", "get_control_mapping", "(", "\n", "params", ".", "vocabulary", "[", "\"source\"", "]", ",", "\n", "control_symbols", "\n", ")", ",", "\n", "\"target\"", ":", "vocabulary", ".", "get_control_mapping", "(", "\n", "params", ".", "vocabulary", "[", "\"target\"", "]", ",", "\n", "control_symbols", "\n", ")", "\n", "}", "\n", "\n", "return", "params", "\n", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.bin.scorer.session_config": [[129, 140], ["tensorflow.OptimizerOptions", "tensorflow.GraphOptions", "tensorflow.ConfigProto", "str"], "function", ["None"], ["    ", "optimizer_options", "=", "tf", ".", "OptimizerOptions", "(", "opt_level", "=", "tf", ".", "OptimizerOptions", ".", "L1", ",", "\n", "do_function_inlining", "=", "False", ")", "\n", "graph_options", "=", "tf", ".", "GraphOptions", "(", "optimizer_options", "=", "optimizer_options", ")", "\n", "config", "=", "tf", ".", "ConfigProto", "(", "allow_soft_placement", "=", "True", ",", "\n", "graph_options", "=", "graph_options", ")", "\n", "if", "params", ".", "device_list", ":", "\n", "        ", "device_str", "=", "\",\"", ".", "join", "(", "[", "str", "(", "i", ")", "for", "i", "in", "params", ".", "device_list", "]", ")", "\n", "config", ".", "gpu_options", ".", "visible_device_list", "=", "device_str", "\n", "\n", "", "return", "config", "\n", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.bin.scorer.set_variables": [[142, 156], ["tensorflow.logging.debug", "list", "tensorflow.device", "tensorflow.assign", "ops.append", "name.split"], "function", ["None"], ["    ", "ops", "=", "[", "]", "\n", "for", "var", "in", "var_list", ":", "\n", "        ", "for", "name", "in", "value_dict", ":", "\n", "            ", "var_name", "=", "\"/\"", ".", "join", "(", "[", "prefix", "]", "+", "list", "(", "name", ".", "split", "(", "\"/\"", ")", "[", "1", ":", "]", ")", ")", "\n", "\n", "if", "var", ".", "name", "[", ":", "-", "2", "]", "==", "var_name", ":", "\n", "                ", "tf", ".", "logging", ".", "debug", "(", "\"restoring %s -> %s\"", "%", "(", "name", ",", "var", ".", "name", ")", ")", "\n", "with", "tf", ".", "device", "(", "\"/cpu:0\"", ")", ":", "\n", "                    ", "op", "=", "tf", ".", "assign", "(", "var", ",", "value_dict", "[", "name", "]", ")", "\n", "ops", ".", "append", "(", "op", ")", "\n", "", "break", "\n", "\n", "", "", "", "return", "ops", "\n", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.bin.scorer.read_files": [[158, 177], ["zip", "tensorflow.gfile.GFile", "enumerate", "fd.close", "range", "line.strip", "inputs[].append", "len"], "function", ["None"], ["    ", "inputs", "=", "[", "[", "]", "for", "_", "in", "range", "(", "len", "(", "names", ")", ")", "]", "\n", "files", "=", "[", "tf", ".", "gfile", ".", "GFile", "(", "name", ")", "for", "name", "in", "names", "]", "\n", "\n", "count", "=", "0", "\n", "\n", "for", "lines", "in", "zip", "(", "*", "files", ")", ":", "\n", "        ", "lines", "=", "[", "line", ".", "strip", "(", ")", "for", "line", "in", "lines", "]", "\n", "\n", "for", "i", ",", "line", "in", "enumerate", "(", "lines", ")", ":", "\n", "            ", "inputs", "[", "i", "]", ".", "append", "(", "line", ")", "\n", "\n", "", "count", "+=", "1", "\n", "\n", "# Close files", "\n", "", "for", "fd", "in", "files", ":", "\n", "        ", "fd", ".", "close", "(", ")", "\n", "\n", "", "return", "inputs", "\n", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.bin.scorer.get_features": [[179, 240], ["tensorflow.device", "tensorflow.data.Dataset.zip", "dataset.map.map", "dataset.map.padded_batch", "dataset.map.make_one_shot_iterator", "dataset.make_one_shot_iterator.get_next", "tensorflow.contrib.lookup.index_table_from_tensor", "tensorflow.contrib.lookup.index_table_from_tensor", "tf.contrib.lookup.index_table_from_tensor.lookup", "tf.contrib.lookup.index_table_from_tensor.lookup", "tensorflow.data.Dataset.from_tensor_slices", "dataset.map.map", "dataset.map.map", "datasets.append", "tuple", "tensorflow.constant", "tensorflow.constant", "tensorflow.concat", "tensorflow.Dimension", "tensorflow.Dimension", "tensorflow.string_split", "tensorflow.shape", "tensorflow.shape", "tensorflow.constant"], "function", ["None"], ["    ", "with", "tf", ".", "device", "(", "\"/cpu:0\"", ")", ":", "\n", "# Create datasets", "\n", "        ", "datasets", "=", "[", "]", "\n", "\n", "for", "data", "in", "inputs", ":", "\n", "            ", "dataset", "=", "tf", ".", "data", ".", "Dataset", ".", "from_tensor_slices", "(", "data", ")", "\n", "# Split string", "\n", "dataset", "=", "dataset", ".", "map", "(", "lambda", "x", ":", "tf", ".", "string_split", "(", "[", "x", "]", ")", ".", "values", ",", "\n", "num_parallel_calls", "=", "params", ".", "num_threads", ")", "\n", "# Append <eos>", "\n", "dataset", "=", "dataset", ".", "map", "(", "\n", "lambda", "x", ":", "tf", ".", "concat", "(", "[", "x", ",", "[", "tf", ".", "constant", "(", "params", ".", "eos", ")", "]", "]", ",", "axis", "=", "0", ")", ",", "\n", "num_parallel_calls", "=", "params", ".", "num_threads", "\n", ")", "\n", "datasets", ".", "append", "(", "dataset", ")", "\n", "\n", "", "dataset", "=", "tf", ".", "data", ".", "Dataset", ".", "zip", "(", "tuple", "(", "datasets", ")", ")", "\n", "\n", "# Convert tuple to dictionary", "\n", "dataset", "=", "dataset", ".", "map", "(", "\n", "lambda", "*", "x", ":", "{", "\n", "\"source\"", ":", "x", "[", "0", "]", ",", "\n", "\"source_length\"", ":", "tf", ".", "shape", "(", "x", "[", "0", "]", ")", "[", "0", "]", ",", "\n", "\"target\"", ":", "x", "[", "1", "]", ",", "\n", "\"target_length\"", ":", "tf", ".", "shape", "(", "x", "[", "1", "]", ")", "[", "0", "]", "\n", "}", ",", "\n", "num_parallel_calls", "=", "params", ".", "num_threads", "\n", ")", "\n", "\n", "dataset", "=", "dataset", ".", "padded_batch", "(", "\n", "params", ".", "eval_batch_size", ",", "\n", "{", "\n", "\"source\"", ":", "[", "tf", ".", "Dimension", "(", "None", ")", "]", ",", "\n", "\"source_length\"", ":", "[", "]", ",", "\n", "\"target\"", ":", "[", "tf", ".", "Dimension", "(", "None", ")", "]", ",", "\n", "\"target_length\"", ":", "[", "]", "\n", "}", ",", "\n", "{", "\n", "\"source\"", ":", "params", ".", "pad", ",", "\n", "\"source_length\"", ":", "0", ",", "\n", "\"target\"", ":", "params", ".", "pad", ",", "\n", "\"target_length\"", ":", "0", "\n", "}", "\n", ")", "\n", "\n", "iterator", "=", "dataset", ".", "make_one_shot_iterator", "(", ")", "\n", "features", "=", "iterator", ".", "get_next", "(", ")", "\n", "\n", "src_table", "=", "tf", ".", "contrib", ".", "lookup", ".", "index_table_from_tensor", "(", "\n", "tf", ".", "constant", "(", "params", ".", "vocabulary", "[", "\"source\"", "]", ")", ",", "\n", "default_value", "=", "params", ".", "mapping", "[", "\"source\"", "]", "[", "params", ".", "unk", "]", "\n", ")", "\n", "tgt_table", "=", "tf", ".", "contrib", ".", "lookup", ".", "index_table_from_tensor", "(", "\n", "tf", ".", "constant", "(", "params", ".", "vocabulary", "[", "\"target\"", "]", ")", ",", "\n", "default_value", "=", "params", ".", "mapping", "[", "\"target\"", "]", "[", "params", ".", "unk", "]", "\n", ")", "\n", "features", "[", "\"source\"", "]", "=", "src_table", ".", "lookup", "(", "features", "[", "\"source\"", "]", ")", "\n", "features", "[", "\"target\"", "]", "=", "tgt_table", ".", "lookup", "(", "features", "[", "\"target\"", "]", ")", "\n", "\n", "", "return", "features", "\n", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.bin.scorer.main": [[242, 295], ["tensorflow.logging.set_verbosity", "thumt.get_model", "scorer.default_parameters", "scorer.merge_parameters", "scorer.import_params", "scorer.override_parameters", "models.get_model.get_parameters", "tensorflow.Graph().as_default", "models.get_model.", "scorer.read_files", "scorer.get_features", "model_cls.get_evaluation_func", "model.get_evaluation_func.", "tensorflow.train.ChiefSessionCreator", "tensorflow.logging.info", "tensorflow.train.list_variables", "tensorflow.train.load_checkpoint", "scorer.set_variables", "tensorflow.group", "tf.train.load_checkpoint.get_tensor", "tensorflow.trainable_variables", "models.get_model.get_name", "tensorflow.train.MonitoredSession", "sess.run", "tensorflow.gfile.Open", "tf.gfile.Open.close", "tensorflow.Graph", "scorer.session_config", "name.startswith", "sess.should_stop", "sess.run", "models.get_model.get_name", "tf.gfile.Open.write"], "function", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.__init__.get_model", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.bin.scorer_token_level.default_parameters", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.bin.scorer_token_level.merge_parameters", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.bin.scorer_token_level.import_params", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.bin.scorer_token_level.override_parameters", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.transformer_lrp.TransformerLRP.get_parameters", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.bin.scorer_token_level.read_files", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.bin.scorer_token_level.get_features", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.transformer_lrp.TransformerLRP.get_evaluation_func", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.bin.scorer_token_level.set_variables", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.transformer_lrp.TransformerLRP.get_name", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.bin.scorer_token_level.session_config", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.transformer_lrp.TransformerLRP.get_name"], ["    ", "tf", ".", "logging", ".", "set_verbosity", "(", "tf", ".", "logging", ".", "INFO", ")", "\n", "model_cls", "=", "models", ".", "get_model", "(", "args", ".", "model", ")", "\n", "params", "=", "default_parameters", "(", ")", "\n", "\n", "# Import and override parameters", "\n", "# Priorities (low -> high):", "\n", "# default -> saved -> command", "\n", "params", "=", "merge_parameters", "(", "params", ",", "model_cls", ".", "get_parameters", "(", ")", ")", "\n", "params", "=", "import_params", "(", "args", ".", "checkpoint", ",", "args", ".", "model", ",", "params", ")", "\n", "override_parameters", "(", "params", ",", "args", ")", "\n", "\n", "# Build Graph", "\n", "with", "tf", ".", "Graph", "(", ")", ".", "as_default", "(", ")", ":", "\n", "        ", "model", "=", "model_cls", "(", "params", ")", "\n", "inputs", "=", "read_files", "(", "args", ".", "input", ")", "\n", "features", "=", "get_features", "(", "inputs", ",", "params", ")", "\n", "score_fn", "=", "model", ".", "get_evaluation_func", "(", ")", "\n", "scores", "=", "score_fn", "(", "features", ",", "params", ")", "\n", "\n", "sess_creator", "=", "tf", ".", "train", ".", "ChiefSessionCreator", "(", "\n", "config", "=", "session_config", "(", "params", ")", "\n", ")", "\n", "\n", "# Load checkpoint", "\n", "tf", ".", "logging", ".", "info", "(", "\"Loading %s\"", "%", "args", ".", "checkpoint", ")", "\n", "var_list", "=", "tf", ".", "train", ".", "list_variables", "(", "args", ".", "checkpoint", ")", "\n", "values", "=", "{", "}", "\n", "reader", "=", "tf", ".", "train", ".", "load_checkpoint", "(", "args", ".", "checkpoint", ")", "\n", "\n", "for", "(", "name", ",", "shape", ")", "in", "var_list", ":", "\n", "            ", "if", "not", "name", ".", "startswith", "(", "model_cls", ".", "get_name", "(", ")", ")", ":", "\n", "                ", "continue", "\n", "\n", "", "tensor", "=", "reader", ".", "get_tensor", "(", "name", ")", "\n", "values", "[", "name", "]", "=", "tensor", "\n", "\n", "", "ops", "=", "set_variables", "(", "tf", ".", "trainable_variables", "(", ")", ",", "values", ",", "\n", "model_cls", ".", "get_name", "(", ")", ")", "\n", "assign_op", "=", "tf", ".", "group", "(", "*", "ops", ")", "\n", "\n", "# Create session", "\n", "with", "tf", ".", "train", ".", "MonitoredSession", "(", "session_creator", "=", "sess_creator", ")", "as", "sess", ":", "\n", "# Restore variables", "\n", "            ", "sess", ".", "run", "(", "assign_op", ")", "\n", "fd", "=", "tf", ".", "gfile", ".", "Open", "(", "args", ".", "output", ",", "\"w\"", ")", "\n", "\n", "while", "not", "sess", ".", "should_stop", "(", ")", ":", "\n", "                ", "results", "=", "sess", ".", "run", "(", "scores", ")", "\n", "for", "value", "in", "results", ":", "\n", "                    ", "fd", ".", "write", "(", "\"%f\\n\"", "%", "value", ")", "\n", "\n", "", "", "fd", ".", "close", "(", ")", "\n", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.bin.scorer_token_level.parse_args": [[19, 42], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args"], "function", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.scripts.build_vocab.parse_args"], ["def", "parse_args", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "\n", "description", "=", "\"Translate using existing NMT models\"", ",", "\n", "usage", "=", "\"translator.py [<args>] [-h | --help]\"", "\n", ")", "\n", "\n", "# input files", "\n", "parser", ".", "add_argument", "(", "\"--input\"", ",", "type", "=", "str", ",", "required", "=", "True", ",", "nargs", "=", "2", ",", "\n", "help", "=", "\"Path of input file\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--output\"", ",", "type", "=", "str", ",", "required", "=", "True", ",", "\n", "help", "=", "\"Path of output file\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--checkpoint\"", ",", "type", "=", "str", ",", "required", "=", "True", ",", "\n", "help", "=", "\"Path of trained models\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--vocabulary\"", ",", "type", "=", "str", ",", "nargs", "=", "2", ",", "required", "=", "True", ",", "\n", "help", "=", "\"Path of source and target vocabulary\"", ")", "\n", "\n", "# model and configuration", "\n", "parser", ".", "add_argument", "(", "\"--model\"", ",", "type", "=", "str", ",", "required", "=", "True", ",", "\n", "help", "=", "\"Name of the model\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--parameters\"", ",", "type", "=", "str", ",", "\n", "help", "=", "\"Additional hyper parameters\"", ")", "\n", "\n", "return", "parser", ".", "parse_args", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.bin.scorer_token_level.default_parameters": [[44, 63], ["tensorflow.contrib.training.HParams"], "function", ["None"], ["", "def", "default_parameters", "(", ")", ":", "\n", "    ", "params", "=", "tf", ".", "contrib", ".", "training", ".", "HParams", "(", "\n", "input", "=", "None", ",", "\n", "output", "=", "None", ",", "\n", "vocabulary", "=", "None", ",", "\n", "model", "=", "None", ",", "\n", "# vocabulary specific", "\n", "pad", "=", "\"<pad>\"", ",", "\n", "bos", "=", "\"<bos>\"", ",", "\n", "eos", "=", "\"<eos>\"", ",", "\n", "unk", "=", "\"<unk>\"", ",", "\n", "mapping", "=", "None", ",", "\n", "append_eos", "=", "False", ",", "\n", "device_list", "=", "[", "0", "]", ",", "\n", "num_threads", "=", "6", ",", "\n", "eval_batch_size", "=", "32", "\n", ")", "\n", "\n", "return", "params", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.bin.scorer_token_level.merge_parameters": [[65, 81], ["tensorflow.contrib.training.HParams", "six.iteritems", "tf.contrib.training.HParams.values", "six.iteritems", "params1.values", "tf.contrib.training.HParams.add_hparam", "params2.values", "setattr", "tf.contrib.training.HParams.add_hparam"], "function", ["None"], ["", "def", "merge_parameters", "(", "params1", ",", "params2", ")", ":", "\n", "    ", "params", "=", "tf", ".", "contrib", ".", "training", ".", "HParams", "(", ")", "\n", "\n", "for", "(", "k", ",", "v", ")", "in", "six", ".", "iteritems", "(", "params1", ".", "values", "(", ")", ")", ":", "\n", "        ", "params", ".", "add_hparam", "(", "k", ",", "v", ")", "\n", "\n", "", "params_dict", "=", "params", ".", "values", "(", ")", "\n", "\n", "for", "(", "k", ",", "v", ")", "in", "six", ".", "iteritems", "(", "params2", ".", "values", "(", ")", ")", ":", "\n", "        ", "if", "k", "in", "params_dict", ":", "\n", "# Override", "\n", "            ", "setattr", "(", "params", ",", "k", ",", "v", ")", "\n", "", "else", ":", "\n", "            ", "params", ".", "add_hparam", "(", "k", ",", "v", ")", "\n", "\n", "", "", "return", "params", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.bin.scorer_token_level.import_params": [[83, 96], ["os.path.abspath", "os.path.join", "tensorflow.gfile.Exists", "tensorflow.gfile.Open", "tensorflow.logging.info", "fd.readline", "params.parse_json"], "function", ["None"], ["", "def", "import_params", "(", "model_dir", ",", "model_name", ",", "params", ")", ":", "\n", "    ", "model_dir", "=", "os", ".", "path", ".", "abspath", "(", "model_dir", ")", "\n", "m_name", "=", "os", ".", "path", ".", "join", "(", "model_dir", ",", "model_name", "+", "\".json\"", ")", "\n", "\n", "if", "not", "tf", ".", "gfile", ".", "Exists", "(", "m_name", ")", ":", "\n", "        ", "return", "params", "\n", "\n", "", "with", "tf", ".", "gfile", ".", "Open", "(", "m_name", ")", "as", "fd", ":", "\n", "        ", "tf", ".", "logging", ".", "info", "(", "\"Restoring model parameters from %s\"", "%", "m_name", ")", "\n", "json_str", "=", "fd", ".", "readline", "(", ")", "\n", "params", ".", "parse_json", "(", "json_str", ")", "\n", "\n", "", "return", "params", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.bin.scorer_token_level.override_parameters": [[98, 127], ["thumt.process_vocabulary", "thumt.process_vocabulary", "params.parse", "thumt.load_vocabulary", "thumt.load_vocabulary", "thumt.get_control_mapping", "thumt.get_control_mapping"], "function", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.data.vocab.process_vocabulary", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.data.vocab.process_vocabulary", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.data.vocab.load_vocabulary", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.data.vocab.load_vocabulary", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.data.vocab.get_control_mapping", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.data.vocab.get_control_mapping"], ["", "def", "override_parameters", "(", "params", ",", "args", ")", ":", "\n", "    ", "if", "args", ".", "parameters", ":", "\n", "        ", "params", ".", "parse", "(", "args", ".", "parameters", ")", "\n", "\n", "", "params", ".", "vocabulary", "=", "{", "\n", "\"source\"", ":", "vocabulary", ".", "load_vocabulary", "(", "args", ".", "vocabulary", "[", "0", "]", ")", ",", "\n", "\"target\"", ":", "vocabulary", ".", "load_vocabulary", "(", "args", ".", "vocabulary", "[", "1", "]", ")", "\n", "}", "\n", "params", ".", "vocabulary", "[", "\"source\"", "]", "=", "vocabulary", ".", "process_vocabulary", "(", "\n", "params", ".", "vocabulary", "[", "\"source\"", "]", ",", "params", "\n", ")", "\n", "params", ".", "vocabulary", "[", "\"target\"", "]", "=", "vocabulary", ".", "process_vocabulary", "(", "\n", "params", ".", "vocabulary", "[", "\"target\"", "]", ",", "params", "\n", ")", "\n", "\n", "control_symbols", "=", "[", "params", ".", "pad", ",", "params", ".", "bos", ",", "params", ".", "eos", ",", "params", ".", "unk", "]", "\n", "\n", "params", ".", "mapping", "=", "{", "\n", "\"source\"", ":", "vocabulary", ".", "get_control_mapping", "(", "\n", "params", ".", "vocabulary", "[", "\"source\"", "]", ",", "\n", "control_symbols", "\n", ")", ",", "\n", "\"target\"", ":", "vocabulary", ".", "get_control_mapping", "(", "\n", "params", ".", "vocabulary", "[", "\"target\"", "]", ",", "\n", "control_symbols", "\n", ")", "\n", "}", "\n", "\n", "return", "params", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.bin.scorer_token_level.session_config": [[129, 140], ["tensorflow.OptimizerOptions", "tensorflow.GraphOptions", "tensorflow.ConfigProto", "str"], "function", ["None"], ["", "def", "session_config", "(", "params", ")", ":", "\n", "    ", "optimizer_options", "=", "tf", ".", "OptimizerOptions", "(", "opt_level", "=", "tf", ".", "OptimizerOptions", ".", "L1", ",", "\n", "do_function_inlining", "=", "False", ")", "\n", "graph_options", "=", "tf", ".", "GraphOptions", "(", "optimizer_options", "=", "optimizer_options", ")", "\n", "config", "=", "tf", ".", "ConfigProto", "(", "allow_soft_placement", "=", "True", ",", "\n", "graph_options", "=", "graph_options", ")", "\n", "if", "params", ".", "device_list", ":", "\n", "        ", "device_str", "=", "\",\"", ".", "join", "(", "[", "str", "(", "i", ")", "for", "i", "in", "params", ".", "device_list", "]", ")", "\n", "config", ".", "gpu_options", ".", "visible_device_list", "=", "device_str", "\n", "\n", "", "return", "config", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.bin.scorer_token_level.set_variables": [[142, 156], ["tensorflow.logging.debug", "list", "tensorflow.device", "tensorflow.assign", "ops.append", "name.split"], "function", ["None"], ["", "def", "set_variables", "(", "var_list", ",", "value_dict", ",", "prefix", ")", ":", "\n", "    ", "ops", "=", "[", "]", "\n", "for", "var", "in", "var_list", ":", "\n", "        ", "for", "name", "in", "value_dict", ":", "\n", "            ", "var_name", "=", "\"/\"", ".", "join", "(", "[", "prefix", "]", "+", "list", "(", "name", ".", "split", "(", "\"/\"", ")", "[", "1", ":", "]", ")", ")", "\n", "\n", "if", "var", ".", "name", "[", ":", "-", "2", "]", "==", "var_name", ":", "\n", "                ", "tf", ".", "logging", ".", "debug", "(", "\"restoring %s -> %s\"", "%", "(", "name", ",", "var", ".", "name", ")", ")", "\n", "with", "tf", ".", "device", "(", "\"/cpu:0\"", ")", ":", "\n", "                    ", "op", "=", "tf", ".", "assign", "(", "var", ",", "value_dict", "[", "name", "]", ")", "\n", "ops", ".", "append", "(", "op", ")", "\n", "", "break", "\n", "\n", "", "", "", "return", "ops", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.bin.scorer_token_level.read_files": [[158, 177], ["zip", "tensorflow.gfile.GFile", "enumerate", "fd.close", "range", "line.strip", "inputs[].append", "len"], "function", ["None"], ["", "def", "read_files", "(", "names", ")", ":", "\n", "    ", "inputs", "=", "[", "[", "]", "for", "_", "in", "range", "(", "len", "(", "names", ")", ")", "]", "\n", "files", "=", "[", "tf", ".", "gfile", ".", "GFile", "(", "name", ")", "for", "name", "in", "names", "]", "\n", "\n", "count", "=", "0", "\n", "\n", "for", "lines", "in", "zip", "(", "*", "files", ")", ":", "\n", "        ", "lines", "=", "[", "line", ".", "strip", "(", ")", "for", "line", "in", "lines", "]", "\n", "\n", "for", "i", ",", "line", "in", "enumerate", "(", "lines", ")", ":", "\n", "            ", "inputs", "[", "i", "]", ".", "append", "(", "line", ")", "\n", "\n", "", "count", "+=", "1", "\n", "\n", "# Close files", "\n", "", "for", "fd", "in", "files", ":", "\n", "        ", "fd", ".", "close", "(", ")", "\n", "\n", "", "return", "inputs", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.bin.scorer_token_level.get_features": [[179, 240], ["tensorflow.device", "tensorflow.data.Dataset.zip", "dataset.map.map", "dataset.map.padded_batch", "dataset.map.make_one_shot_iterator", "dataset.make_one_shot_iterator.get_next", "tensorflow.contrib.lookup.index_table_from_tensor", "tensorflow.contrib.lookup.index_table_from_tensor", "tf.contrib.lookup.index_table_from_tensor.lookup", "tf.contrib.lookup.index_table_from_tensor.lookup", "tensorflow.data.Dataset.from_tensor_slices", "dataset.map.map", "dataset.map.map", "datasets.append", "tuple", "tensorflow.constant", "tensorflow.constant", "tensorflow.concat", "tensorflow.Dimension", "tensorflow.Dimension", "tensorflow.string_split", "tensorflow.shape", "tensorflow.shape", "tensorflow.constant"], "function", ["None"], ["", "def", "get_features", "(", "inputs", ",", "params", ")", ":", "\n", "    ", "with", "tf", ".", "device", "(", "\"/cpu:0\"", ")", ":", "\n", "# Create datasets", "\n", "        ", "datasets", "=", "[", "]", "\n", "\n", "for", "data", "in", "inputs", ":", "\n", "            ", "dataset", "=", "tf", ".", "data", ".", "Dataset", ".", "from_tensor_slices", "(", "data", ")", "\n", "# Split string", "\n", "dataset", "=", "dataset", ".", "map", "(", "lambda", "x", ":", "tf", ".", "string_split", "(", "[", "x", "]", ")", ".", "values", ",", "\n", "num_parallel_calls", "=", "params", ".", "num_threads", ")", "\n", "# Append <eos>", "\n", "dataset", "=", "dataset", ".", "map", "(", "\n", "lambda", "x", ":", "tf", ".", "concat", "(", "[", "x", ",", "[", "tf", ".", "constant", "(", "params", ".", "eos", ")", "]", "]", ",", "axis", "=", "0", ")", ",", "\n", "num_parallel_calls", "=", "params", ".", "num_threads", "\n", ")", "\n", "datasets", ".", "append", "(", "dataset", ")", "\n", "\n", "", "dataset", "=", "tf", ".", "data", ".", "Dataset", ".", "zip", "(", "tuple", "(", "datasets", ")", ")", "\n", "\n", "# Convert tuple to dictionary", "\n", "dataset", "=", "dataset", ".", "map", "(", "\n", "lambda", "*", "x", ":", "{", "\n", "\"source\"", ":", "x", "[", "0", "]", ",", "\n", "\"source_length\"", ":", "tf", ".", "shape", "(", "x", "[", "0", "]", ")", "[", "0", "]", ",", "\n", "\"target\"", ":", "x", "[", "1", "]", ",", "\n", "\"target_length\"", ":", "tf", ".", "shape", "(", "x", "[", "1", "]", ")", "[", "0", "]", "\n", "}", ",", "\n", "num_parallel_calls", "=", "params", ".", "num_threads", "\n", ")", "\n", "\n", "dataset", "=", "dataset", ".", "padded_batch", "(", "\n", "params", ".", "eval_batch_size", ",", "\n", "{", "\n", "\"source\"", ":", "[", "tf", ".", "Dimension", "(", "None", ")", "]", ",", "\n", "\"source_length\"", ":", "[", "]", ",", "\n", "\"target\"", ":", "[", "tf", ".", "Dimension", "(", "None", ")", "]", ",", "\n", "\"target_length\"", ":", "[", "]", "\n", "}", ",", "\n", "{", "\n", "\"source\"", ":", "params", ".", "pad", ",", "\n", "\"source_length\"", ":", "0", ",", "\n", "\"target\"", ":", "params", ".", "pad", ",", "\n", "\"target_length\"", ":", "0", "\n", "}", "\n", ")", "\n", "\n", "iterator", "=", "dataset", ".", "make_one_shot_iterator", "(", ")", "\n", "features", "=", "iterator", ".", "get_next", "(", ")", "\n", "\n", "src_table", "=", "tf", ".", "contrib", ".", "lookup", ".", "index_table_from_tensor", "(", "\n", "tf", ".", "constant", "(", "params", ".", "vocabulary", "[", "\"source\"", "]", ")", ",", "\n", "default_value", "=", "params", ".", "mapping", "[", "\"source\"", "]", "[", "params", ".", "unk", "]", "\n", ")", "\n", "tgt_table", "=", "tf", ".", "contrib", ".", "lookup", ".", "index_table_from_tensor", "(", "\n", "tf", ".", "constant", "(", "params", ".", "vocabulary", "[", "\"target\"", "]", ")", ",", "\n", "default_value", "=", "params", ".", "mapping", "[", "\"target\"", "]", "[", "params", ".", "unk", "]", "\n", ")", "\n", "features", "[", "\"source\"", "]", "=", "src_table", ".", "lookup", "(", "features", "[", "\"source\"", "]", ")", "\n", "features", "[", "\"target\"", "]", "=", "tgt_table", ".", "lookup", "(", "features", "[", "\"target\"", "]", ")", "\n", "\n", "", "return", "features", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.bin.scorer_token_level.main": [[242, 297], ["tensorflow.logging.set_verbosity", "thumt.get_model", "scorer_token_level.default_parameters", "scorer_token_level.merge_parameters", "scorer_token_level.import_params", "scorer_token_level.override_parameters", "models.get_model.get_parameters", "tensorflow.Graph().as_default", "models.get_model.", "scorer_token_level.read_files", "scorer_token_level.get_features", "model_cls.get_evaluation_func", "model.get_evaluation_func.", "tensorflow.train.ChiefSessionCreator", "tensorflow.logging.info", "tensorflow.train.list_variables", "tensorflow.train.load_checkpoint", "scorer_token_level.set_variables", "tensorflow.group", "tf.train.load_checkpoint.get_tensor", "tensorflow.trainable_variables", "models.get_model.get_name", "tensorflow.train.MonitoredSession", "sess.run", "tensorflow.gfile.Open", "tf.gfile.Open.close", "tensorflow.Graph", "scorer_token_level.session_config", "name.startswith", "sess.should_stop", "sess.run", "models.get_model.get_name", "tf.gfile.Open.write", "tf.gfile.Open.write"], "function", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.__init__.get_model", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.bin.scorer_token_level.default_parameters", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.bin.scorer_token_level.merge_parameters", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.bin.scorer_token_level.import_params", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.bin.scorer_token_level.override_parameters", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.transformer_lrp.TransformerLRP.get_parameters", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.bin.scorer_token_level.read_files", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.bin.scorer_token_level.get_features", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.transformer_lrp.TransformerLRP.get_evaluation_func", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.bin.scorer_token_level.set_variables", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.transformer_lrp.TransformerLRP.get_name", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.bin.scorer_token_level.session_config", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.transformer_lrp.TransformerLRP.get_name"], ["", "def", "main", "(", "args", ")", ":", "\n", "    ", "tf", ".", "logging", ".", "set_verbosity", "(", "tf", ".", "logging", ".", "INFO", ")", "\n", "model_cls", "=", "models", ".", "get_model", "(", "args", ".", "model", ")", "\n", "params", "=", "default_parameters", "(", ")", "\n", "\n", "# Import and override parameters", "\n", "# Priorities (low -> high):", "\n", "# default -> saved -> command", "\n", "params", "=", "merge_parameters", "(", "params", ",", "model_cls", ".", "get_parameters", "(", ")", ")", "\n", "params", "=", "import_params", "(", "args", ".", "checkpoint", ",", "args", ".", "model", ",", "params", ")", "\n", "override_parameters", "(", "params", ",", "args", ")", "\n", "\n", "# Build Graph", "\n", "with", "tf", ".", "Graph", "(", ")", ".", "as_default", "(", ")", ":", "\n", "        ", "model", "=", "model_cls", "(", "params", ")", "\n", "inputs", "=", "read_files", "(", "args", ".", "input", ")", "\n", "features", "=", "get_features", "(", "inputs", ",", "params", ")", "\n", "score_fn", "=", "model", ".", "get_evaluation_func", "(", ")", "\n", "scores", "=", "score_fn", "(", "features", ",", "params", ")", "\n", "\n", "sess_creator", "=", "tf", ".", "train", ".", "ChiefSessionCreator", "(", "\n", "config", "=", "session_config", "(", "params", ")", "\n", ")", "\n", "\n", "# Load checkpoint", "\n", "tf", ".", "logging", ".", "info", "(", "\"Loading %s\"", "%", "args", ".", "checkpoint", ")", "\n", "var_list", "=", "tf", ".", "train", ".", "list_variables", "(", "args", ".", "checkpoint", ")", "\n", "values", "=", "{", "}", "\n", "reader", "=", "tf", ".", "train", ".", "load_checkpoint", "(", "args", ".", "checkpoint", ")", "\n", "\n", "for", "(", "name", ",", "shape", ")", "in", "var_list", ":", "\n", "            ", "if", "not", "name", ".", "startswith", "(", "model_cls", ".", "get_name", "(", ")", ")", ":", "\n", "                ", "continue", "\n", "\n", "", "tensor", "=", "reader", ".", "get_tensor", "(", "name", ")", "\n", "values", "[", "name", "]", "=", "tensor", "\n", "\n", "", "ops", "=", "set_variables", "(", "tf", ".", "trainable_variables", "(", ")", ",", "values", ",", "\n", "model_cls", ".", "get_name", "(", ")", ")", "\n", "assign_op", "=", "tf", ".", "group", "(", "*", "ops", ")", "\n", "\n", "# Create session", "\n", "with", "tf", ".", "train", ".", "MonitoredSession", "(", "session_creator", "=", "sess_creator", ")", "as", "sess", ":", "\n", "# Restore variables", "\n", "            ", "sess", ".", "run", "(", "assign_op", ")", "\n", "fd", "=", "tf", ".", "gfile", ".", "Open", "(", "args", ".", "output", ",", "\"w\"", ")", "\n", "\n", "while", "not", "sess", ".", "should_stop", "(", ")", ":", "\n", "                ", "results", "=", "sess", ".", "run", "(", "scores", ")", "\n", "for", "value", "in", "results", ":", "\n", "                    ", "for", "item", "in", "value", ":", "\n", "                        ", "fd", ".", "write", "(", "\"%f \"", "%", "item", ")", "\n", "", "fd", ".", "write", "(", "\"\\n\"", ")", "\n", "\n", "", "", "fd", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.bin.radam.RAdamOptimizer.__init__": [[27, 90], ["tensorflow.python.training.optimizer.Optimizer.__init__", "float"], "methods", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.transformer_lrp.TransformerLRP.__init__"], ["def", "__init__", "(", "self", ",", "\n", "learning_rate", "=", "0.001", ",", "\n", "beta1", "=", "0.9", ",", "\n", "beta2", "=", "0.999", ",", "\n", "epsilon", "=", "1e-7", ",", "\n", "weight_decay", "=", "0.", ",", "\n", "amsgrad", "=", "False", ",", "\n", "total_steps", "=", "0", ",", "\n", "warmup_proportion", "=", "0.1", ",", "\n", "min_lr", "=", "0.", ",", "\n", "use_locking", "=", "False", ",", "\n", "name", "=", "\"RAdam\"", ")", ":", "\n", "        ", "r\"\"\"Construct a new Adam optimizer.\n        Args:\n            learning_rate: A Tensor or a floating point value.    The learning rate.\n            beta1: A float value or a constant float tensor. The exponential decay\n                rate for the 1st moment estimates.\n            beta2: A float value or a constant float tensor. The exponential decay\n                rate for the 2nd moment estimates.\n            epsilon: A small constant for numerical stability. This epsilon is\n                \"epsilon hat\" in the Kingma and Ba paper (in the formula just before\n                Section 2.1), not the epsilon in Algorithm 1 of the paper.\n            weight_decay: A floating point value. Weight decay for each param.\n            amsgrad: boolean. Whether to apply AMSGrad variant of this algorithm from\n                the paper \"On the Convergence of Adam and beyond\".\n            total_steps: An integer. Total number of training steps.\n                Enable warmup by setting a positive value.\n            warmup_proportion: A floating point value. The proportion of increasing steps.\n            min_lr: A floating point value. Minimum learning rate after warmup.\n            name: Optional name for the operations created when applying gradients.\n                Defaults to \"Adam\".    @compatibility(eager) When eager execution is\n                enabled, `learning_rate`, `beta_1`, `beta_2`, and `epsilon` can each be\n                a callable that takes no arguments and returns the actual value to use.\n                This can be useful for changing these values across different\n                invocations of optimizer functions. @end_compatibility\n            **kwargs: keyword arguments. Allowed to be {`clipnorm`, `clipvalue`, `lr`,\n                `decay`}. `clipnorm` is clip gradients by norm; `clipvalue` is clip\n                gradients by value, `decay` is included for backward compatibility to\n                allow time inverse decay of learning rate. `lr` is included for backward\n                compatibility, recommended to use `learning_rate` instead.\n        \"\"\"", "\n", "super", "(", "RAdamOptimizer", ",", "self", ")", ".", "__init__", "(", "use_locking", ",", "name", ")", "\n", "self", ".", "_lr", "=", "learning_rate", "\n", "self", ".", "_beta1", "=", "beta1", "\n", "self", ".", "_beta2", "=", "beta2", "\n", "self", ".", "_epsilon", "=", "epsilon", "\n", "self", ".", "_weight_decay", "=", "weight_decay", "\n", "self", ".", "_amsgrad", "=", "amsgrad", "\n", "self", ".", "_total_steps", "=", "float", "(", "total_steps", ")", "\n", "self", ".", "_warmup_proportion", "=", "warmup_proportion", "\n", "self", ".", "_min_lr", "=", "min_lr", "\n", "self", ".", "_initial_weight_decay", "=", "weight_decay", "\n", "self", ".", "_initial_total_steps", "=", "total_steps", "\n", "\n", "self", ".", "_lr_t", "=", "None", "\n", "self", ".", "_step_t", "=", "None", "\n", "self", ".", "_beta1_t", "=", "None", "\n", "self", ".", "_beta2_t", "=", "None", "\n", "self", ".", "_epsilon_t", "=", "None", "\n", "self", ".", "_weight_decay_t", "=", "None", "\n", "self", ".", "_total_steps_t", "=", "None", "\n", "self", ".", "_warmup_proportion_t", "=", "None", "\n", "self", ".", "_min_lr_t", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.bin.radam.RAdamOptimizer._get_beta_accumulators": [[91, 100], ["tensorflow.python.framework.ops.init_scope", "tensorflow.python.eager.context.executing_eagerly", "tensorflow.python.framework.ops.get_default_graph", "radam.RAdamOptimizer._get_non_slot_variable", "radam.RAdamOptimizer._get_non_slot_variable", "radam.RAdamOptimizer._get_non_slot_variable"], "methods", ["None"], ["", "def", "_get_beta_accumulators", "(", "self", ")", ":", "\n", "        ", "with", "ops", ".", "init_scope", "(", ")", ":", "\n", "            ", "if", "context", ".", "executing_eagerly", "(", ")", ":", "\n", "                ", "graph", "=", "None", "\n", "", "else", ":", "\n", "                ", "graph", "=", "ops", ".", "get_default_graph", "(", ")", "\n", "", "return", "(", "self", ".", "_get_non_slot_variable", "(", "\"step\"", ",", "graph", "=", "graph", ")", ",", "\n", "self", ".", "_get_non_slot_variable", "(", "\"beta1_power\"", ",", "graph", "=", "graph", ")", ",", "\n", "self", ".", "_get_non_slot_variable", "(", "\"beta2_power\"", ",", "graph", "=", "graph", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.bin.radam.RAdamOptimizer._create_slots": [[101, 111], ["min", "radam.RAdamOptimizer._create_non_slot_variable", "radam.RAdamOptimizer._create_non_slot_variable", "radam.RAdamOptimizer._create_non_slot_variable", "radam.RAdamOptimizer._zeros_slot", "radam.RAdamOptimizer._zeros_slot", "radam.RAdamOptimizer._zeros_slot"], "methods", ["None"], ["", "", "def", "_create_slots", "(", "self", ",", "var_list", ")", ":", "\n", "        ", "first_var", "=", "min", "(", "var_list", ",", "key", "=", "lambda", "x", ":", "x", ".", "name", ")", "\n", "self", ".", "_create_non_slot_variable", "(", "initial_value", "=", "1.0", ",", "name", "=", "\"step\"", ",", "colocate_with", "=", "first_var", ")", "\n", "self", ".", "_create_non_slot_variable", "(", "initial_value", "=", "self", ".", "_beta1", ",", "name", "=", "\"beta1_power\"", ",", "colocate_with", "=", "first_var", ")", "\n", "self", ".", "_create_non_slot_variable", "(", "initial_value", "=", "self", ".", "_beta2", ",", "name", "=", "\"beta2_power\"", ",", "colocate_with", "=", "first_var", ")", "\n", "for", "v", "in", "var_list", ":", "\n", "            ", "self", ".", "_zeros_slot", "(", "v", ",", "\"m\"", ",", "self", ".", "_name", ")", "\n", "self", ".", "_zeros_slot", "(", "v", ",", "\"v\"", ",", "self", ".", "_name", ")", "\n", "if", "self", ".", "_amsgrad", ":", "\n", "                ", "self", ".", "_zeros_slot", "(", "v", ",", "\"vhat\"", ",", "self", ".", "_name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.bin.radam.RAdamOptimizer._prepare": [[112, 130], ["tensorflow.python.framework.ops.convert_to_tensor", "tensorflow.python.framework.ops.convert_to_tensor", "tensorflow.python.framework.ops.convert_to_tensor", "tensorflow.python.framework.ops.convert_to_tensor", "tensorflow.python.framework.ops.convert_to_tensor", "tensorflow.python.framework.ops.convert_to_tensor", "tensorflow.python.framework.ops.convert_to_tensor", "tensorflow.python.framework.ops.convert_to_tensor"], "methods", ["None"], ["", "", "", "def", "_prepare", "(", "self", ")", ":", "\n", "        ", "lr", "=", "self", ".", "_lr", "\n", "beta1", "=", "self", ".", "_beta1", "\n", "beta2", "=", "self", ".", "_beta2", "\n", "epsilon", "=", "self", ".", "_epsilon", "\n", "weight_decay", "=", "self", ".", "_weight_decay", "\n", "total_steps", "=", "self", ".", "_total_steps", "\n", "warmup_proportion", "=", "self", ".", "_warmup_proportion", "\n", "min_lr", "=", "self", ".", "_min_lr", "\n", "\n", "self", ".", "_lr_t", "=", "ops", ".", "convert_to_tensor", "(", "lr", ",", "name", "=", "\"learning_rate\"", ")", "\n", "self", ".", "_beta1_t", "=", "ops", ".", "convert_to_tensor", "(", "beta1", ",", "name", "=", "\"beta1\"", ")", "\n", "self", ".", "_beta2_t", "=", "ops", ".", "convert_to_tensor", "(", "beta2", ",", "name", "=", "\"beta2\"", ")", "\n", "self", ".", "_epsilon_t", "=", "ops", ".", "convert_to_tensor", "(", "epsilon", ",", "name", "=", "\"epsilon\"", ")", "\n", "self", ".", "_weight_decay_t", "=", "ops", ".", "convert_to_tensor", "(", "weight_decay", ",", "name", "=", "\"weight_decay\"", ")", "\n", "self", ".", "_total_steps_t", "=", "ops", ".", "convert_to_tensor", "(", "total_steps", ",", "name", "=", "\"total_steps\"", ")", "\n", "self", ".", "_warmup_proportion_t", "=", "ops", ".", "convert_to_tensor", "(", "warmup_proportion", ",", "name", "=", "\"warmup_proportion\"", ")", "\n", "self", ".", "_min_lr_t", "=", "ops", ".", "convert_to_tensor", "(", "min_lr", ",", "name", "=", "\"min_lr\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.bin.radam.RAdamOptimizer._apply_dense": [[131, 133], ["radam.RAdamOptimizer._resource_apply_dense"], "methods", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.bin.radam.RAdamOptimizer._resource_apply_dense"], ["", "def", "_apply_dense", "(", "self", ",", "grad", ",", "var", ")", ":", "\n", "        ", "return", "self", ".", "_resource_apply_dense", "(", "grad", ",", "var", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.bin.radam.RAdamOptimizer._resource_apply_dense": [[134, 188], ["radam.RAdamOptimizer._get_beta_accumulators", "tensorflow.python.ops.math_ops.cast", "tensorflow.python.ops.math_ops.cast", "tensorflow.python.ops.math_ops.cast", "tensorflow.python.ops.math_ops.cast", "tensorflow.python.ops.math_ops.cast", "tensorflow.python.ops.math_ops.cast", "radam.RAdamOptimizer.get_slot", "tensorflow.python.ops.state_ops.assign", "radam.RAdamOptimizer.get_slot", "tensorflow.python.ops.state_ops.assign", "tensorflow.python.ops.math_ops.sqrt", "tensorflow.where", "tensorflow.python.ops.state_ops.assign_sub", "tensorflow.python.ops.control_flow_ops.group", "tensorflow.python.ops.math_ops.cast", "tensorflow.python.ops.math_ops.cast", "tensorflow.python.ops.math_ops.cast", "tensorflow.python.ops.math_ops.maximum", "tensorflow.where", "radam.RAdamOptimizer.get_slot", "tensorflow.python.ops.state_ops.assign", "tensorflow.python.ops.math_ops.sqrt", "tensorflow.python.ops.math_ops.sqrt", "updates.append", "tensorflow.python.ops.math_ops.maximum", "tensorflow.python.ops.math_ops.cast", "tensorflow.python.ops.math_ops.square", "tensorflow.python.ops.math_ops.minimum"], "methods", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.bin.radam.RAdamOptimizer._get_beta_accumulators"], ["", "def", "_resource_apply_dense", "(", "self", ",", "grad", ",", "var", ")", ":", "\n", "        ", "step", ",", "beta1_power", ",", "beta2_power", "=", "self", ".", "_get_beta_accumulators", "(", ")", "\n", "beta1_power", "=", "math_ops", ".", "cast", "(", "beta1_power", ",", "var", ".", "dtype", ".", "base_dtype", ")", "\n", "beta2_power", "=", "math_ops", ".", "cast", "(", "beta2_power", ",", "var", ".", "dtype", ".", "base_dtype", ")", "\n", "lr_t", "=", "math_ops", ".", "cast", "(", "self", ".", "_lr_t", ",", "var", ".", "dtype", ".", "base_dtype", ")", "\n", "\n", "if", "self", ".", "_initial_total_steps", ">", "0", ":", "\n", "            ", "total_steps", "=", "math_ops", ".", "cast", "(", "self", ".", "_total_steps_t", ",", "var", ".", "dtype", ".", "base_dtype", ")", "\n", "warmup_proportion", "=", "math_ops", ".", "cast", "(", "self", ".", "_warmup_proportion_t", ",", "var", ".", "dtype", ".", "base_dtype", ")", "\n", "min_lr", "=", "math_ops", ".", "cast", "(", "self", ".", "_min_lr_t", ",", "var", ".", "dtype", ".", "base_dtype", ")", "\n", "warmup_steps", "=", "total_steps", "*", "warmup_proportion", "\n", "decay_steps", "=", "math_ops", ".", "maximum", "(", "total_steps", "-", "warmup_steps", ",", "1", ")", "\n", "decay_rate", "=", "(", "min_lr", "-", "lr_t", ")", "/", "decay_steps", "\n", "lr_t", "=", "tf", ".", "where", "(", "\n", "step", "<=", "warmup_steps", ",", "\n", "lr_t", "*", "(", "step", "/", "warmup_steps", ")", ",", "\n", "lr_t", "+", "decay_rate", "*", "math_ops", ".", "minimum", "(", "step", "-", "warmup_steps", ",", "decay_steps", ")", ",", "\n", ")", "\n", "\n", "", "beta1_t", "=", "math_ops", ".", "cast", "(", "self", ".", "_beta1_t", ",", "var", ".", "dtype", ".", "base_dtype", ")", "\n", "beta2_t", "=", "math_ops", ".", "cast", "(", "self", ".", "_beta2_t", ",", "var", ".", "dtype", ".", "base_dtype", ")", "\n", "epsilon_t", "=", "math_ops", ".", "cast", "(", "self", ".", "_epsilon_t", ",", "var", ".", "dtype", ".", "base_dtype", ")", "\n", "\n", "sma_inf", "=", "2.0", "/", "(", "1.0", "-", "beta2_t", ")", "-", "1.0", "\n", "sma_t", "=", "sma_inf", "-", "2.0", "*", "step", "*", "beta2_power", "/", "(", "1.0", "-", "beta2_power", ")", "\n", "\n", "m", "=", "self", ".", "get_slot", "(", "var", ",", "\"m\"", ")", "\n", "m_t", "=", "state_ops", ".", "assign", "(", "m", ",", "beta1_t", "*", "m", "+", "(", "1.0", "-", "beta1_t", ")", "*", "grad", ",", "use_locking", "=", "self", ".", "_use_locking", ")", "\n", "m_corr_t", "=", "m_t", "/", "(", "1.0", "-", "beta1_power", ")", "\n", "\n", "v", "=", "self", ".", "get_slot", "(", "var", ",", "\"v\"", ")", "\n", "v_t", "=", "state_ops", ".", "assign", "(", "v", ",", "beta2_t", "*", "v", "+", "(", "1.0", "-", "beta2_t", ")", "*", "math_ops", ".", "square", "(", "grad", ")", ",", "use_locking", "=", "self", ".", "_use_locking", ")", "\n", "if", "self", ".", "_amsgrad", ":", "\n", "            ", "vhat", "=", "self", ".", "get_slot", "(", "var", ",", "'vhat'", ")", "\n", "vhat_t", "=", "state_ops", ".", "assign", "(", "vhat", ",", "math_ops", ".", "maximum", "(", "vhat", ",", "v_t", ")", ",", "use_locking", "=", "self", ".", "_use_locking", ")", "\n", "v_corr_t", "=", "math_ops", ".", "sqrt", "(", "vhat_t", "/", "(", "1.0", "-", "beta2_power", ")", ")", "\n", "", "else", ":", "\n", "            ", "v_corr_t", "=", "math_ops", ".", "sqrt", "(", "v_t", "/", "(", "1.0", "-", "beta2_power", ")", ")", "\n", "\n", "", "r_t", "=", "math_ops", ".", "sqrt", "(", "(", "sma_t", "-", "4.0", ")", "/", "(", "sma_inf", "-", "4.0", ")", "*", "\n", "(", "sma_t", "-", "2.0", ")", "/", "(", "sma_inf", "-", "2.0", ")", "*", "\n", "sma_inf", "/", "sma_t", ")", "\n", "\n", "var_t", "=", "tf", ".", "where", "(", "sma_t", ">=", "5.0", ",", "r_t", "*", "m_corr_t", "/", "(", "v_corr_t", "+", "epsilon_t", ")", ",", "m_corr_t", ")", "\n", "\n", "if", "self", ".", "_initial_weight_decay", ">", "0.0", ":", "\n", "            ", "var_t", "+=", "math_ops", ".", "cast", "(", "self", ".", "_weight_decay_t", ",", "var", ".", "dtype", ".", "base_dtype", ")", "*", "var", "\n", "\n", "", "var_update", "=", "state_ops", ".", "assign_sub", "(", "var", ",", "lr_t", "*", "var_t", ",", "use_locking", "=", "self", ".", "_use_locking", ")", "\n", "\n", "updates", "=", "[", "var_update", ",", "m_t", ",", "v_t", "]", "\n", "if", "self", ".", "_amsgrad", ":", "\n", "            ", "updates", ".", "append", "(", "vhat_t", ")", "\n", "", "return", "control_flow_ops", ".", "group", "(", "*", "updates", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.bin.radam.RAdamOptimizer._apply_sparse_shared": [[189, 254], ["radam.RAdamOptimizer._get_beta_accumulators", "tensorflow.python.ops.math_ops.cast", "tensorflow.python.ops.math_ops.cast", "tensorflow.python.ops.math_ops.cast", "tensorflow.python.ops.math_ops.cast", "tensorflow.python.ops.math_ops.cast", "tensorflow.python.ops.math_ops.cast", "radam.RAdamOptimizer.get_slot", "tensorflow.python.ops.state_ops.assign", "radam.RAdamOptimizer.get_slot", "tensorflow.python.ops.state_ops.assign", "tensorflow.python.ops.math_ops.sqrt", "tensorflow.where", "tensorflow.python.ops.state_ops.scatter_sub", "tensorflow.python.ops.control_flow_ops.group", "tensorflow.python.ops.math_ops.cast", "tensorflow.python.ops.math_ops.cast", "tensorflow.python.ops.math_ops.cast", "tensorflow.python.ops.math_ops.maximum", "tensorflow.where", "tensorflow.python.framework.ops.control_dependencies", "scatter_add", "tensorflow.python.framework.ops.control_dependencies", "scatter_add", "radam.RAdamOptimizer.get_slot", "tensorflow.python.ops.state_ops.assign", "tensorflow.python.ops.math_ops.sqrt", "tensorflow.python.ops.math_ops.sqrt", "tensorflow.python.ops.array_ops.gather", "updates.append", "tensorflow.python.ops.math_ops.maximum", "tensorflow.python.ops.math_ops.cast", "tensorflow.python.ops.math_ops.minimum"], "methods", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.bin.radam.RAdamOptimizer._get_beta_accumulators"], ["", "def", "_apply_sparse_shared", "(", "self", ",", "grad", ",", "var", ",", "indices", ",", "scatter_add", ")", ":", "\n", "        ", "step", ",", "beta1_power", ",", "beta2_power", "=", "self", ".", "_get_beta_accumulators", "(", ")", "\n", "beta1_power", "=", "math_ops", ".", "cast", "(", "beta1_power", ",", "var", ".", "dtype", ".", "base_dtype", ")", "\n", "beta2_power", "=", "math_ops", ".", "cast", "(", "beta2_power", ",", "var", ".", "dtype", ".", "base_dtype", ")", "\n", "lr_t", "=", "math_ops", ".", "cast", "(", "self", ".", "_lr_t", ",", "var", ".", "dtype", ".", "base_dtype", ")", "\n", "\n", "if", "self", ".", "_initial_total_steps", ">", "0", ":", "\n", "            ", "total_steps", "=", "math_ops", ".", "cast", "(", "self", ".", "_total_steps_t", ",", "var", ".", "dtype", ".", "base_dtype", ")", "\n", "warmup_proportion", "=", "math_ops", ".", "cast", "(", "self", ".", "_warmup_proportion_t", ",", "var", ".", "dtype", ".", "base_dtype", ")", "\n", "min_lr", "=", "math_ops", ".", "cast", "(", "self", ".", "_min_lr_t", ",", "var", ".", "dtype", ".", "base_dtype", ")", "\n", "warmup_steps", "=", "total_steps", "*", "warmup_proportion", "\n", "decay_steps", "=", "math_ops", ".", "maximum", "(", "total_steps", "-", "warmup_steps", ",", "1", ")", "\n", "decay_rate", "=", "(", "min_lr", "-", "lr_t", ")", "/", "decay_steps", "\n", "lr_t", "=", "tf", ".", "where", "(", "\n", "step", "<=", "warmup_steps", ",", "\n", "lr_t", "*", "(", "step", "/", "warmup_steps", ")", ",", "\n", "lr_t", "+", "decay_rate", "*", "math_ops", ".", "minimum", "(", "step", "-", "warmup_steps", ",", "decay_steps", ")", ",", "\n", ")", "\n", "\n", "", "beta1_t", "=", "math_ops", ".", "cast", "(", "self", ".", "_beta1_t", ",", "var", ".", "dtype", ".", "base_dtype", ")", "\n", "beta2_t", "=", "math_ops", ".", "cast", "(", "self", ".", "_beta2_t", ",", "var", ".", "dtype", ".", "base_dtype", ")", "\n", "epsilon_t", "=", "math_ops", ".", "cast", "(", "self", ".", "_epsilon_t", ",", "var", ".", "dtype", ".", "base_dtype", ")", "\n", "\n", "sma_inf", "=", "2.0", "/", "(", "1.0", "-", "beta2_t", ")", "-", "1.0", "\n", "sma_t", "=", "sma_inf", "-", "2.0", "*", "step", "*", "beta2_power", "/", "(", "1.0", "-", "beta2_power", ")", "\n", "\n", "m", "=", "self", ".", "get_slot", "(", "var", ",", "\"m\"", ")", "\n", "m_scaled_g_values", "=", "grad", "*", "(", "1", "-", "beta1_t", ")", "\n", "m_t", "=", "state_ops", ".", "assign", "(", "m", ",", "m", "*", "beta1_t", ",", "use_locking", "=", "self", ".", "_use_locking", ")", "\n", "with", "ops", ".", "control_dependencies", "(", "[", "m_t", "]", ")", ":", "\n", "            ", "m_t", "=", "scatter_add", "(", "m", ",", "indices", ",", "m_scaled_g_values", ")", "\n", "", "m_corr_t", "=", "m_t", "/", "(", "1.0", "-", "beta1_power", ")", "\n", "\n", "v", "=", "self", ".", "get_slot", "(", "var", ",", "\"v\"", ")", "\n", "v_scaled_g_values", "=", "(", "grad", "*", "grad", ")", "*", "(", "1", "-", "beta2_t", ")", "\n", "v_t", "=", "state_ops", ".", "assign", "(", "v", ",", "v", "*", "beta2_t", ",", "use_locking", "=", "self", ".", "_use_locking", ")", "\n", "with", "ops", ".", "control_dependencies", "(", "[", "v_t", "]", ")", ":", "\n", "            ", "v_t", "=", "scatter_add", "(", "v", ",", "indices", ",", "v_scaled_g_values", ")", "\n", "", "if", "self", ".", "_amsgrad", ":", "\n", "            ", "vhat", "=", "self", ".", "get_slot", "(", "var", ",", "'vhat'", ")", "\n", "vhat_t", "=", "state_ops", ".", "assign", "(", "vhat", ",", "math_ops", ".", "maximum", "(", "vhat", ",", "v_t", ")", ",", "use_locking", "=", "self", ".", "_use_locking", ")", "\n", "v_corr_t", "=", "math_ops", ".", "sqrt", "(", "vhat_t", "/", "(", "1.0", "-", "beta2_power", ")", ")", "\n", "", "else", ":", "\n", "            ", "v_corr_t", "=", "math_ops", ".", "sqrt", "(", "v_t", "/", "(", "1.0", "-", "beta2_power", ")", ")", "\n", "\n", "", "r_t", "=", "math_ops", ".", "sqrt", "(", "(", "sma_t", "-", "4.0", ")", "/", "(", "sma_inf", "-", "4.0", ")", "*", "\n", "(", "sma_t", "-", "2.0", ")", "/", "(", "sma_inf", "-", "2.0", ")", "*", "\n", "sma_inf", "/", "sma_t", ")", "\n", "\n", "var_t", "=", "tf", ".", "where", "(", "sma_t", ">=", "5.0", ",", "r_t", "*", "m_corr_t", "/", "(", "v_corr_t", "+", "epsilon_t", ")", ",", "m_corr_t", ")", "\n", "\n", "if", "self", ".", "_initial_weight_decay", ">", "0.0", ":", "\n", "            ", "var_t", "+=", "math_ops", ".", "cast", "(", "self", ".", "_weight_decay_t", ",", "var", ".", "dtype", ".", "base_dtype", ")", "*", "var", "\n", "\n", "", "var_t", "=", "lr_t", "*", "var_t", "\n", "var_update", "=", "state_ops", ".", "scatter_sub", "(", "\n", "var", ",", "\n", "indices", ",", "\n", "array_ops", ".", "gather", "(", "var_t", ",", "indices", ")", ",", "\n", "use_locking", "=", "self", ".", "_use_locking", ")", "\n", "\n", "updates", "=", "[", "var_update", ",", "m_t", ",", "v_t", "]", "\n", "if", "self", ".", "_amsgrad", ":", "\n", "            ", "updates", ".", "append", "(", "vhat_t", ")", "\n", "", "return", "control_flow_ops", ".", "group", "(", "*", "updates", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.bin.radam.RAdamOptimizer._apply_sparse": [[255, 261], ["radam.RAdamOptimizer._apply_sparse_shared", "tensorflow.python.ops.state_ops.scatter_add"], "methods", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.bin.radam.RAdamOptimizer._apply_sparse_shared"], ["", "def", "_apply_sparse", "(", "self", ",", "grad", ",", "var", ")", ":", "\n", "        ", "return", "self", ".", "_apply_sparse_shared", "(", "\n", "grad", ".", "values", ",", "\n", "var", ",", "\n", "grad", ".", "indices", ",", "\n", "lambda", "x", ",", "i", ",", "v", ":", "state_ops", ".", "scatter_add", "(", "x", ",", "i", ",", "v", ",", "use_locking", "=", "self", ".", "_use_locking", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.bin.radam.RAdamOptimizer._resource_scatter_add": [[262, 265], ["tensorflow.python.framework.ops.control_dependencies", "x.value", "tensorflow.python.ops.resource_variable_ops.resource_scatter_add"], "methods", ["None"], ["", "def", "_resource_scatter_add", "(", "self", ",", "x", ",", "i", ",", "v", ")", ":", "\n", "        ", "with", "ops", ".", "control_dependencies", "(", "[", "resource_variable_ops", ".", "resource_scatter_add", "(", "x", ".", "handle", ",", "i", ",", "v", ")", "]", ")", ":", "\n", "            ", "return", "x", ".", "value", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.bin.radam.RAdamOptimizer._resource_apply_sparse": [[266, 268], ["radam.RAdamOptimizer._apply_sparse_shared"], "methods", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.bin.radam.RAdamOptimizer._apply_sparse_shared"], ["", "", "def", "_resource_apply_sparse", "(", "self", ",", "grad", ",", "var", ",", "indices", ")", ":", "\n", "        ", "return", "self", ".", "_apply_sparse_shared", "(", "grad", ",", "var", ",", "indices", ",", "self", ".", "_resource_scatter_add", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.bin.radam.RAdamOptimizer._finish": [[269, 277], ["tensorflow.python.ops.control_flow_ops.group", "tensorflow.python.framework.ops.control_dependencies", "radam.RAdamOptimizer._get_beta_accumulators", "tensorflow.python.framework.ops.colocate_with", "step.assign", "beta1_power.assign", "beta2_power.assign"], "methods", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.bin.radam.RAdamOptimizer._get_beta_accumulators"], ["", "def", "_finish", "(", "self", ",", "update_ops", ",", "name_scope", ")", ":", "\n", "        ", "with", "ops", ".", "control_dependencies", "(", "update_ops", ")", ":", "\n", "            ", "step", ",", "beta1_power", ",", "beta2_power", "=", "self", ".", "_get_beta_accumulators", "(", ")", "\n", "with", "ops", ".", "colocate_with", "(", "beta1_power", ")", ":", "\n", "                ", "update_step", "=", "step", ".", "assign", "(", "step", "+", "1.0", ",", "use_locking", "=", "self", ".", "_use_locking", ")", "\n", "update_beta1", "=", "beta1_power", ".", "assign", "(", "beta1_power", "*", "self", ".", "_beta1_t", ",", "use_locking", "=", "self", ".", "_use_locking", ")", "\n", "update_beta2", "=", "beta2_power", ".", "assign", "(", "beta2_power", "*", "self", ".", "_beta2_t", ",", "use_locking", "=", "self", ".", "_use_locking", ")", "\n", "", "", "return", "control_flow_ops", ".", "group", "(", "*", "update_ops", "+", "[", "update_step", ",", "update_beta1", ",", "update_beta2", "]", ",", "name", "=", "name_scope", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.layers.rnn_cell.LegacyGRUCell.__init__": [[22, 25], ["super().__init__"], "methods", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.transformer_lrp.TransformerLRP.__init__"], ["def", "__init__", "(", "self", ",", "num_units", ",", "reuse", "=", "None", ")", ":", "\n", "        ", "super", "(", "LegacyGRUCell", ",", "self", ")", ".", "__init__", "(", "_reuse", "=", "reuse", ")", "\n", "self", ".", "_num_units", "=", "num_units", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.layers.rnn_cell.LegacyGRUCell.__call__": [[26, 44], ["tensorflow.variable_scope", "tensorflow.nn.sigmoid", "tensorflow.nn.sigmoid", "thumt.layers.nn.linear", "isinstance", "list", "thumt.layers.nn.linear", "thumt.layers.nn.linear", "list", "tensorflow.tanh"], "methods", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.layers.nn.linear", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.layers.nn.linear", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.layers.nn.linear"], ["", "def", "__call__", "(", "self", ",", "inputs", ",", "state", ",", "scope", "=", "None", ")", ":", "\n", "        ", "with", "tf", ".", "variable_scope", "(", "scope", ",", "default_name", "=", "\"gru_cell\"", ",", "\n", "values", "=", "[", "inputs", ",", "state", "]", ")", ":", "\n", "            ", "if", "not", "isinstance", "(", "inputs", ",", "(", "list", ",", "tuple", ")", ")", ":", "\n", "                ", "inputs", "=", "[", "inputs", "]", "\n", "\n", "", "all_inputs", "=", "list", "(", "inputs", ")", "+", "[", "state", "]", "\n", "r", "=", "tf", ".", "nn", ".", "sigmoid", "(", "linear", "(", "all_inputs", ",", "self", ".", "_num_units", ",", "False", ",", "False", ",", "\n", "scope", "=", "\"reset_gate\"", ")", ")", "\n", "u", "=", "tf", ".", "nn", ".", "sigmoid", "(", "linear", "(", "all_inputs", ",", "self", ".", "_num_units", ",", "False", ",", "False", ",", "\n", "scope", "=", "\"update_gate\"", ")", ")", "\n", "all_inputs", "=", "list", "(", "inputs", ")", "+", "[", "r", "*", "state", "]", "\n", "c", "=", "linear", "(", "all_inputs", ",", "self", ".", "_num_units", ",", "True", ",", "False", ",", "\n", "scope", "=", "\"candidate\"", ")", "\n", "\n", "new_state", "=", "(", "1.0", "-", "u", ")", "*", "state", "+", "u", "*", "tf", ".", "tanh", "(", "c", ")", "\n", "\n", "", "return", "new_state", ",", "new_state", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.layers.rnn_cell.LegacyGRUCell.state_size": [[45, 48], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "state_size", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "_num_units", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.layers.rnn_cell.LegacyGRUCell.output_size": [[49, 52], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "output_size", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "_num_units", "\n", "", "", ""]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.layers.attention.add_timing_signal": [[14, 48], ["tensorflow.name_scope", "tensorflow.to_float", "tensorflow.concat", "tensorflow.pad", "tensorflow.reshape", "tensorflow.shape", "tensorflow.shape", "tensorflow.range", "math.log", "tensorflow.exp", "tensorflow.expand_dims", "tensorflow.expand_dims", "tensorflow.cast", "tensorflow.to_float", "tensorflow.sin", "tensorflow.cos", "float", "float", "tensorflow.to_float", "tensorflow.mod", "tensorflow.range"], "function", ["None"], ["def", "add_timing_signal", "(", "x", ",", "min_timescale", "=", "1.0", ",", "max_timescale", "=", "1.0e4", ",", "name", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    This function adds a bunch of sinusoids of different frequencies to a\n    Tensor. See paper: `Attention is all you need'\n\n    :param x: A tensor with shape [batch, length, channels]\n    :param min_timescale: A floating point number\n    :param max_timescale: A floating point number\n    :param name: An optional string\n\n    :returns: a Tensor the same shape as x.\n    \"\"\"", "\n", "\n", "with", "tf", ".", "name_scope", "(", "name", ",", "default_name", "=", "\"add_timing_signal\"", ",", "values", "=", "[", "x", "]", ")", ":", "\n", "        ", "length", "=", "tf", ".", "shape", "(", "x", ")", "[", "1", "]", "\n", "channels", "=", "tf", ".", "shape", "(", "x", ")", "[", "2", "]", "\n", "position", "=", "tf", ".", "to_float", "(", "tf", ".", "range", "(", "length", ")", ")", "\n", "num_timescales", "=", "channels", "//", "2", "\n", "\n", "log_timescale_increment", "=", "(", "\n", "math", ".", "log", "(", "float", "(", "max_timescale", ")", "/", "float", "(", "min_timescale", ")", ")", "/", "\n", "(", "tf", ".", "to_float", "(", "num_timescales", ")", "-", "1", ")", "\n", ")", "\n", "inv_timescales", "=", "min_timescale", "*", "tf", ".", "exp", "(", "\n", "tf", ".", "to_float", "(", "tf", ".", "range", "(", "num_timescales", ")", ")", "*", "-", "log_timescale_increment", "\n", ")", "\n", "\n", "scaled_time", "=", "(", "tf", ".", "expand_dims", "(", "position", ",", "1", ")", "*", "\n", "tf", ".", "expand_dims", "(", "inv_timescales", ",", "0", ")", ")", "\n", "signal", "=", "tf", ".", "concat", "(", "[", "tf", ".", "sin", "(", "scaled_time", ")", ",", "tf", ".", "cos", "(", "scaled_time", ")", "]", ",", "axis", "=", "1", ")", "\n", "signal", "=", "tf", ".", "pad", "(", "signal", ",", "[", "[", "0", ",", "0", "]", ",", "[", "0", ",", "tf", ".", "mod", "(", "channels", ",", "2", ")", "]", "]", ")", "\n", "signal", "=", "tf", ".", "reshape", "(", "signal", ",", "[", "1", ",", "length", ",", "channels", "]", ")", "\n", "\n", "return", "x", "+", "tf", ".", "cast", "(", "signal", ",", "x", ".", "dtype", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.layers.attention.split_heads": [[50, 70], ["tensorflow.name_scope", "tensorflow.reshape", "tf.reshape.set_shape", "tensorflow.transpose", "x.get_shape", "tensorflow.concat", "tensorflow.shape", "range"], "function", ["None"], ["", "", "def", "split_heads", "(", "inputs", ",", "num_heads", ",", "name", "=", "None", ")", ":", "\n", "    ", "\"\"\" Split heads\n    :param inputs: A tensor with shape [batch, ..., channels]\n    :param num_heads: An integer\n    :param name: An optional string\n    :returns: A tensor with shape [batch, heads, ..., channels / heads]\n    \"\"\"", "\n", "\n", "with", "tf", ".", "name_scope", "(", "name", ",", "default_name", "=", "\"split_heads\"", ",", "values", "=", "[", "inputs", "]", ")", ":", "\n", "        ", "x", "=", "inputs", "\n", "n", "=", "num_heads", "\n", "old_shape", "=", "x", ".", "get_shape", "(", ")", ".", "dims", "\n", "ndims", "=", "x", ".", "shape", ".", "ndims", "\n", "\n", "last", "=", "old_shape", "[", "-", "1", "]", "\n", "new_shape", "=", "old_shape", "[", ":", "-", "1", "]", "+", "[", "n", "]", "+", "[", "last", "//", "n", "if", "last", "else", "None", "]", "\n", "ret", "=", "tf", ".", "reshape", "(", "x", ",", "tf", ".", "concat", "(", "[", "tf", ".", "shape", "(", "x", ")", "[", ":", "-", "1", "]", ",", "[", "n", ",", "-", "1", "]", "]", ",", "0", ")", ")", "\n", "ret", ".", "set_shape", "(", "new_shape", ")", "\n", "perm", "=", "[", "0", ",", "ndims", "-", "1", "]", "+", "[", "i", "for", "i", "in", "range", "(", "1", ",", "ndims", "-", "1", ")", "]", "+", "[", "ndims", "]", "\n", "return", "tf", ".", "transpose", "(", "ret", ",", "perm", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.layers.attention.combine_heads": [[72, 89], ["tensorflow.name_scope", "tensorflow.transpose", "tensorflow.reshape", "tf.reshape.set_shape", "tf.reshape.get_shape", "tensorflow.concat", "tensorflow.shape"], "function", ["None"], ["", "", "def", "combine_heads", "(", "inputs", ",", "name", "=", "None", ")", ":", "\n", "    ", "\"\"\" Combine heads\n    :param inputs: A tensor with shape [batch, heads, length, channels]\n    :param name: An optional string\n    :returns: A tensor with shape [batch, length, heads * channels]\n    \"\"\"", "\n", "\n", "with", "tf", ".", "name_scope", "(", "name", ",", "default_name", "=", "\"combine_heads\"", ",", "values", "=", "[", "inputs", "]", ")", ":", "\n", "        ", "x", "=", "inputs", "\n", "x", "=", "tf", ".", "transpose", "(", "x", ",", "[", "0", ",", "2", ",", "1", ",", "3", "]", ")", "\n", "old_shape", "=", "x", ".", "get_shape", "(", ")", ".", "dims", "\n", "a", ",", "b", "=", "old_shape", "[", "-", "2", ":", "]", "\n", "new_shape", "=", "old_shape", "[", ":", "-", "2", "]", "+", "[", "a", "*", "b", "if", "a", "and", "b", "else", "None", "]", "\n", "x", "=", "tf", ".", "reshape", "(", "x", ",", "tf", ".", "concat", "(", "[", "tf", ".", "shape", "(", "x", ")", "[", ":", "-", "2", "]", ",", "[", "-", "1", "]", "]", ",", "0", ")", ")", "\n", "x", ".", "set_shape", "(", "new_shape", ")", "\n", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.layers.attention.create_rpr": [[91, 110], ["tensorflow.name_scope", "tensorflow.reshape", "tensorflow.reshape", "tensorflow.maximum", "tensorflow.minimum", "tensorflow.gather", "tensorflow.range", "tensorflow.range"], "function", ["None"], ["", "", "def", "create_rpr", "(", "orginal_var", ",", "length_q", ",", "length_kv", ",", "max_relative_dis", ",", "name", "=", "None", ")", ":", "\n", "    ", "\"\"\" Create relative positional representation \n    :param orginal_var: A tensor with shape [2*max_relative_dis+1, depth]\n    :param length_q: An integer\n    :param length_kv: An integer\n    :param max_relative_dis: An integer\n    :returns: A tensor with shape [length_q, length_kv, depth]\n    \"\"\"", "\n", "\n", "with", "tf", ".", "name_scope", "(", "name", ",", "default_name", "=", "\"create_rpr\"", ",", "values", "=", "[", "orginal_var", "]", ")", ":", "\n", "        ", "idxs", "=", "tf", ".", "reshape", "(", "tf", ".", "range", "(", "length_kv", ")", ",", "[", "-", "1", ",", "1", "]", ")", "# only self-attention", "\n", "idys", "=", "tf", ".", "reshape", "(", "tf", ".", "range", "(", "length_kv", ")", ",", "[", "1", ",", "-", "1", "]", ")", "\n", "ids", "=", "idxs", "-", "idys", "\n", "ids", "=", "ids", "+", "max_relative_dis", "\n", "ids", "=", "tf", ".", "maximum", "(", "ids", ",", "0", ")", "\n", "ids", "=", "tf", ".", "minimum", "(", "ids", ",", "2", "*", "max_relative_dis", ")", "\n", "ids", "=", "ids", "[", "-", "length_q", ":", ",", ":", "]", "\n", "rpr", "=", "tf", ".", "gather", "(", "orginal_var", ",", "ids", ")", "\n", "return", "rpr", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.layers.attention.attention_bias": [[112, 162], ["tensorflow.name_scope", "tensorflow.cast", "tensorflow.matrix_band_part", "tensorflow.reshape", "tensorflow.ones", "tensorflow.expand_dims", "tensorflow.expand_dims", "tensorflow.to_float", "tensorflow.expand_dims", "tensorflow.range", "tensorflow.expand_dims", "tensorflow.expand_dims", "tensorflow.expand_dims", "tensorflow.where", "tensorflow.cast", "tensorflow.matrix_band_part", "tensorflow.reshape", "ValueError", "tensorflow.ones", "tensorflow.matrix_band_part", "tensorflow.log", "tensorflow.ones", "tensorflow.abs"], "function", ["None"], ["", "", "def", "attention_bias", "(", "inputs", ",", "mode", ",", "inf", "=", "-", "1e9", ",", "dtype", "=", "None", ",", "name", "=", "None", ")", ":", "\n", "    ", "\"\"\" A bias tensor used in attention mechanism\n    :param inputs: A tensor\n    :param mode: one of \"causal\", \"masking\", \"proximal\" or \"distance\"\n    :param inf: A floating value\n    :param dtype: An instance of tf.DType\n    :param name: optional string\n    :returns: A 4D tensor with shape [batch, heads, queries, memories]\n    \"\"\"", "\n", "\n", "with", "tf", ".", "name_scope", "(", "name", ",", "default_name", "=", "\"attention_bias\"", ",", "values", "=", "[", "inputs", "]", ")", ":", "\n", "        ", "if", "dtype", "is", "None", ":", "\n", "            ", "dtype", "=", "tf", ".", "float32", "\n", "\n", "", "if", "dtype", "!=", "tf", ".", "float32", ":", "\n", "            ", "inf", "=", "dtype", ".", "min", "\n", "\n", "", "if", "mode", "==", "\"causal\"", ":", "\n", "            ", "length", "=", "inputs", "\n", "lower_triangle", "=", "tf", ".", "matrix_band_part", "(", "\n", "tf", ".", "ones", "(", "[", "length", ",", "length", "]", ")", ",", "-", "1", ",", "0", "\n", ")", "\n", "ret", "=", "inf", "*", "(", "1.0", "-", "lower_triangle", ")", "\n", "ret", "=", "tf", ".", "reshape", "(", "ret", ",", "[", "1", ",", "1", ",", "length", ",", "length", "]", ")", "\n", "", "elif", "mode", "==", "\"masking\"", ":", "\n", "            ", "mask", "=", "inputs", "\n", "ret", "=", "(", "1.0", "-", "mask", ")", "*", "inf", "\n", "ret", "=", "tf", ".", "expand_dims", "(", "tf", ".", "expand_dims", "(", "ret", ",", "1", ")", ",", "1", ")", "\n", "", "elif", "mode", "==", "\"proximal\"", ":", "\n", "            ", "length", "=", "inputs", "\n", "r", "=", "tf", ".", "to_float", "(", "tf", ".", "range", "(", "length", ")", ")", "\n", "diff", "=", "tf", ".", "expand_dims", "(", "r", ",", "0", ")", "-", "tf", ".", "expand_dims", "(", "r", ",", "1", ")", "\n", "ret", "=", "tf", ".", "expand_dims", "(", "tf", ".", "expand_dims", "(", "-", "tf", ".", "log", "(", "1", "+", "tf", ".", "abs", "(", "diff", ")", ")", ",", "0", ")", ",", "\n", "0", ")", "\n", "", "elif", "mode", "==", "\"distance\"", ":", "\n", "            ", "length", ",", "distance", "=", "inputs", "\n", "distance", "=", "tf", ".", "where", "(", "distance", ">", "length", ",", "0", ",", "distance", ")", "\n", "distance", "=", "tf", ".", "cast", "(", "distance", ",", "tf", ".", "int64", ")", "\n", "lower_triangle", "=", "tf", ".", "matrix_band_part", "(", "\n", "tf", ".", "ones", "(", "[", "length", ",", "length", "]", ")", ",", "-", "1", ",", "0", "\n", ")", "\n", "mask_triangle", "=", "1.0", "-", "tf", ".", "matrix_band_part", "(", "\n", "tf", ".", "ones", "(", "[", "length", ",", "length", "]", ")", ",", "distance", "-", "1", ",", "0", "\n", ")", "\n", "ret", "=", "inf", "*", "(", "1.0", "-", "lower_triangle", "+", "mask_triangle", ")", "\n", "ret", "=", "tf", ".", "reshape", "(", "ret", ",", "[", "1", ",", "1", ",", "length", ",", "length", "]", ")", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"Unknown mode %s\"", "%", "mode", ")", "\n", "\n", "", "return", "tf", ".", "cast", "(", "ret", ",", "dtype", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.layers.attention.should_generate_summaries": [[164, 173], ["tensorflow.contrib.framework.get_name_scope", "tensorflow.get_variable_scope"], "function", ["None"], ["", "", "def", "should_generate_summaries", "(", ")", ":", "\n", "    ", "\"\"\"Is this an appropriate context to generate summaries.\n    :returns: a boolean\n    \"\"\"", "\n", "if", "\"while/\"", "in", "tf", ".", "contrib", ".", "framework", ".", "get_name_scope", "(", ")", ":", "\n", "        ", "return", "False", "\n", "", "if", "tf", ".", "get_variable_scope", "(", ")", ".", "reuse", ":", "\n", "        ", "return", "False", "\n", "", "return", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.layers.attention.attention_image_summary": [[175, 204], ["tensorflow.shape", "tensorflow.summary.image", "tensorflow.transpose", "tensorflow.pow", "tensorflow.pad", "tensorflow.shape", "tensorflow.reshape", "tensorflow.reduce_max", "tensorflow.reshape", "tensorflow.mod"], "function", ["None"], ["", "def", "attention_image_summary", "(", "weights", ",", "rgb", "=", "True", ")", ":", "\n", "    ", "\"\"\"Compute attention image summary.\n    :param weights: a Tensor with shape [batch, heads, queries, memories]\n    :param rgb: use RGB color to represent a head\n    \"\"\"", "\n", "shape", "=", "tf", ".", "shape", "(", "weights", ")", "\n", "batch_size", "=", "shape", "[", "0", "]", "\n", "num_heads", "=", "shape", "[", "1", "]", "\n", "num_queries", "=", "shape", "[", "2", "]", "\n", "num_memories", "=", "shape", "[", "3", "]", "\n", "\n", "if", "rgb", ":", "\n", "# [batch, queries, memories, heads]", "\n", "        ", "image", "=", "tf", ".", "transpose", "(", "weights", ",", "[", "0", ",", "2", ",", "3", ",", "1", "]", ")", "\n", "# for high-dynamic-range", "\n", "image", "=", "tf", ".", "pow", "(", "image", ",", "0.2", ")", "\n", "# Each head will correspond to one of RGB", "\n", "image", "=", "tf", ".", "pad", "(", "image", ",", "[", "[", "0", ",", "0", "]", ",", "[", "0", ",", "0", "]", ",", "[", "0", ",", "0", "]", ",", "\n", "[", "0", ",", "tf", ".", "mod", "(", "-", "num_heads", ",", "3", ")", "]", "]", ")", "\n", "shape", "=", "tf", ".", "shape", "(", "image", ")", "\n", "# [batch, queries, memories, 3, heads]", "\n", "image", "=", "tf", ".", "reshape", "(", "image", ",", "[", "batch_size", ",", "num_queries", ",", "num_memories", ",", "\n", "3", ",", "shape", "[", "-", "1", "]", "//", "3", "]", ")", "\n", "image", "=", "tf", ".", "reduce_max", "(", "image", ",", "4", ")", "\n", "", "else", ":", "\n", "        ", "image", "=", "tf", ".", "reshape", "(", "weights", ",", "[", "-", "1", ",", "num_queries", ",", "num_memories", ",", "1", "]", ")", "\n", "\n", "# [batch, height, width, channel]", "\n", "", "tf", ".", "summary", ".", "image", "(", "\"attention\"", ",", "image", ",", "max_outputs", "=", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.layers.attention.attention": [[206, 257], ["tensorflow.variable_scope", "tensorflow.shape", "thumt.layers.nn.linear", "tensorflow.reshape", "tensorflow.tanh", "tensorflow.reshape", "thumt.layers.nn.linear", "tensorflow.reshape", "tensorflow.nn.softmax", "memories.get_shape().as_list", "tensorflow.reshape", "thumt.layers.nn.linear", "tensorflow.reduce_sum", "memories.get_shape"], "function", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.layers.nn.linear", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.layers.nn.linear", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.layers.nn.linear"], ["", "def", "attention", "(", "query", ",", "memories", ",", "bias", ",", "hidden_size", ",", "cache", "=", "None", ",", "reuse", "=", "None", ",", "\n", "dtype", "=", "None", ",", "scope", "=", "None", ")", ":", "\n", "    ", "\"\"\" Standard attention layer\n\n    :param query: A tensor with shape [batch, key_size]\n    :param memories: A tensor with shape [batch, memory_size, key_size]\n    :param bias: A tensor with shape [batch, memory_size]\n    :param hidden_size: An integer\n    :param cache: A dictionary of precomputed value\n    :param reuse: A boolean value, whether to reuse the scope\n    :param dtype: An optional instance of tf.DType\n    :param scope: An optional string, the scope of this layer\n    :return: A tensor with shape [batch, value_size] and\n        a Tensor with shape [batch, memory_size]\n    \"\"\"", "\n", "\n", "with", "tf", ".", "variable_scope", "(", "scope", "or", "\"attention\"", ",", "reuse", "=", "reuse", ",", "\n", "values", "=", "[", "query", ",", "memories", ",", "bias", "]", ",", "dtype", "=", "dtype", ")", ":", "\n", "        ", "mem_shape", "=", "tf", ".", "shape", "(", "memories", ")", "\n", "key_size", "=", "memories", ".", "get_shape", "(", ")", ".", "as_list", "(", ")", "[", "-", "1", "]", "\n", "\n", "if", "cache", "is", "None", ":", "\n", "            ", "k", "=", "tf", ".", "reshape", "(", "memories", ",", "[", "-", "1", ",", "key_size", "]", ")", "\n", "k", "=", "linear", "(", "k", ",", "hidden_size", ",", "False", ",", "False", ",", "scope", "=", "\"k_transform\"", ")", "\n", "\n", "if", "query", "is", "None", ":", "\n", "                ", "return", "{", "\"key\"", ":", "k", "}", "\n", "", "", "else", ":", "\n", "            ", "k", "=", "cache", "[", "\"key\"", "]", "\n", "\n", "", "q", "=", "linear", "(", "query", ",", "hidden_size", ",", "False", ",", "False", ",", "scope", "=", "\"q_transform\"", ")", "\n", "k", "=", "tf", ".", "reshape", "(", "k", ",", "[", "mem_shape", "[", "0", "]", ",", "mem_shape", "[", "1", "]", ",", "hidden_size", "]", ")", "\n", "\n", "hidden", "=", "tf", ".", "tanh", "(", "q", "[", ":", ",", "None", ",", ":", "]", "+", "k", ")", "\n", "hidden", "=", "tf", ".", "reshape", "(", "hidden", ",", "[", "-", "1", ",", "hidden_size", "]", ")", "\n", "\n", "# Shape: [batch, mem_size, 1]", "\n", "logits", "=", "linear", "(", "hidden", ",", "1", ",", "False", ",", "False", ",", "scope", "=", "\"logits\"", ")", "\n", "logits", "=", "tf", ".", "reshape", "(", "logits", ",", "[", "-", "1", ",", "mem_shape", "[", "1", "]", "]", ")", "\n", "\n", "if", "bias", "is", "not", "None", ":", "\n", "            ", "logits", "=", "logits", "+", "bias", "\n", "\n", "", "alpha", "=", "tf", ".", "nn", ".", "softmax", "(", "logits", ")", "\n", "\n", "outputs", "=", "{", "\n", "\"value\"", ":", "tf", ".", "reduce_sum", "(", "alpha", "[", ":", ",", ":", ",", "None", "]", "*", "memories", ",", "axis", "=", "1", ")", ",", "\n", "\"weight\"", ":", "alpha", "\n", "}", "\n", "\n", "", "return", "outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.layers.attention.additive_attention": [[259, 312], ["tensorflow.variable_scope", "tensorflow.tile", "tensorflow.tile", "tensorflow.squeeze", "tensorflow.nn.softmax", "tensorflow.matmul", "tensorflow.shape", "tensorflow.shape", "tensorflow.expand_dims", "tensorflow.expand_dims", "tensorflow.tanh", "thumt.layers.nn.linear", "thumt.layers.nn.linear", "tensorflow.tanh", "thumt.layers.nn.linear", "tensorflow.nn.dropout", "thumt.layers.nn.linear", "tensorflow.concat"], "function", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.layers.nn.linear", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.layers.nn.linear", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.layers.nn.linear", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.layers.nn.linear"], ["", "def", "additive_attention", "(", "queries", ",", "keys", ",", "values", ",", "bias", ",", "hidden_size", ",", "concat", "=", "False", ",", "\n", "keep_prob", "=", "None", ",", "dtype", "=", "None", ",", "scope", "=", "None", ")", ":", "\n", "    ", "\"\"\" Additive attention mechanism. This layer is implemented using a\n        one layer feed forward neural network\n\n    :param queries: A tensor with shape [batch, heads, length_q, depth_k]\n    :param keys: A tensor with shape [batch, heads, length_kv, depth_k]\n    :param values: A tensor with shape [batch, heads, length_kv, depth_v]\n    :param bias: A tensor\n    :param hidden_size: An integer\n    :param concat: A boolean value. If ``concat'' is set to True, then\n        the computation of attention mechanism is following $tanh(W[q, k])$.\n        When ``concat'' is set to False, the computation is following\n        $tanh(Wq + Vk)$\n    :param keep_prob: a scalar in [0, 1]\n    :param dtype: An optional instance of tf.DType\n    :param scope: An optional string, the scope of this layer\n\n    :returns: A dict with the following keys:\n        weights: A tensor with shape [batch, length_q]\n        outputs: A tensor with shape [batch, length_q, depth_v]\n    \"\"\"", "\n", "\n", "with", "tf", ".", "variable_scope", "(", "scope", ",", "default_name", "=", "\"additive_attention\"", ",", "\n", "values", "=", "[", "queries", ",", "keys", ",", "values", ",", "bias", "]", ",", "dtype", "=", "dtype", ")", ":", "\n", "        ", "length_q", "=", "tf", ".", "shape", "(", "queries", ")", "[", "2", "]", "\n", "length_kv", "=", "tf", ".", "shape", "(", "keys", ")", "[", "2", "]", "\n", "q", "=", "tf", ".", "tile", "(", "tf", ".", "expand_dims", "(", "queries", ",", "3", ")", ",", "[", "1", ",", "1", ",", "1", ",", "length_kv", ",", "1", "]", ")", "\n", "k", "=", "tf", ".", "tile", "(", "tf", ".", "expand_dims", "(", "keys", ",", "2", ")", ",", "[", "1", ",", "1", ",", "length_q", ",", "1", ",", "1", "]", ")", "\n", "\n", "if", "concat", ":", "\n", "            ", "combined", "=", "tf", ".", "tanh", "(", "linear", "(", "tf", ".", "concat", "(", "[", "q", ",", "k", "]", ",", "axis", "=", "-", "1", ")", ",", "hidden_size", ",", "\n", "True", ",", "True", ",", "name", "=", "\"qk_transform\"", ")", ")", "\n", "", "else", ":", "\n", "            ", "q", "=", "linear", "(", "queries", ",", "hidden_size", ",", "True", ",", "True", ",", "name", "=", "\"q_transform\"", ")", "\n", "k", "=", "linear", "(", "keys", ",", "hidden_size", ",", "True", ",", "True", ",", "name", "=", "\"key_transform\"", ")", "\n", "combined", "=", "tf", ".", "tanh", "(", "q", "+", "k", ")", "\n", "\n", "# shape: [batch, heads, length_q, length_kv]", "\n", "", "logits", "=", "tf", ".", "squeeze", "(", "linear", "(", "combined", ",", "1", ",", "True", ",", "True", ",", "name", "=", "\"logits\"", ")", ",", "\n", "axis", "=", "-", "1", ")", "\n", "\n", "if", "bias", "is", "not", "None", ":", "\n", "            ", "logits", "+=", "bias", "\n", "\n", "", "weights", "=", "tf", ".", "nn", ".", "softmax", "(", "logits", ",", "name", "=", "\"attention_weights\"", ")", "\n", "\n", "if", "keep_prob", "or", "keep_prob", "<", "1.0", ":", "\n", "            ", "weights", "=", "tf", ".", "nn", ".", "dropout", "(", "weights", ",", "keep_prob", ")", "\n", "\n", "", "outputs", "=", "tf", ".", "matmul", "(", "weights", ",", "values", ")", "\n", "\n", "return", "{", "\"weights\"", ":", "weights", ",", "\"outputs\"", ":", "outputs", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.layers.attention.multiplicative_attention": [[314, 376], ["tensorflow.name_scope", "tensorflow.shape", "tensorflow.nn.softmax", "tensorflow.shape", "tensorflow.shape", "tensorflow.matmul", "tensorflow.matmul", "tensorflow.reshape", "tensorflow.matmul", "tensorflow.reshape", "tensorflow.nn.dropout", "tensorflow.matmul", "tensorflow.matmul", "tensorflow.reshape", "tensorflow.matmul", "tensorflow.reshape", "tensorflow.reshape", "tensorflow.transpose", "tensorflow.transpose", "tensorflow.transpose", "tensorflow.transpose", "tensorflow.transpose", "tensorflow.transpose"], "function", ["None"], ["", "", "def", "multiplicative_attention", "(", "queries", ",", "keys", ",", "values", ",", "bias", ",", "keep_prob", "=", "None", ",", "\n", "name", "=", "None", ",", "rpr", "=", "None", ")", ":", "\n", "    ", "\"\"\" Multiplicative attention mechanism. This layer is implemented using\n        dot-product operation.\n\n    :param queries: A tensor with shape [batch, heads, length_q, depth_k]\n    :param keys: A tensor with shape [batch, heads, length_kv, depth_k]\n    :param values: A tensor with shape [batch, heads, length_kv, depth_v]\n    :param bias: A tensor\n    :param keep_prob: a scalar in (0, 1]\n    :param name: the name of this operation\n    :param rpr: the name of this operation\n\n    :returns: A dict with the following keys:\n        weights: A tensor with shape [batch, heads, length_q, length_kv]\n        outputs: A tensor with shape [batch, heads, length_q, depth_v]\n    \"\"\"", "\n", "\n", "with", "tf", ".", "name_scope", "(", "name", ",", "default_name", "=", "\"multiplicative_attention\"", ",", "\n", "values", "=", "[", "queries", ",", "keys", ",", "values", ",", "bias", "]", ")", ":", "\n", "\n", "        ", "q_shape", "=", "tf", ".", "shape", "(", "queries", ")", "\n", "bs", ",", "hd", ",", "lq", ",", "dk", "=", "q_shape", "[", "0", "]", ",", "q_shape", "[", "1", "]", ",", "q_shape", "[", "2", "]", ",", "q_shape", "[", "3", "]", "\n", "lk", "=", "tf", ".", "shape", "(", "keys", ")", "[", "2", "]", "\n", "dv", "=", "tf", ".", "shape", "(", "values", ")", "[", "3", "]", "\n", "\n", "if", "rpr", "is", "not", "None", ":", "\n", "            ", "rpr_k", ",", "rpr_v", "=", "rpr", "[", "'rpr_k'", "]", ",", "rpr", "[", "'rpr_v'", "]", "# (lq, lk, dk), (lq, lk, dv)", "\n", "\n", "", "if", "rpr", "is", "None", ":", "\n", "            ", "logits", "=", "tf", ".", "matmul", "(", "queries", ",", "keys", ",", "transpose_b", "=", "True", ")", "\n", "", "else", ":", "# self-attention with relative position representaion", "\n", "            ", "logits_part1", "=", "tf", ".", "matmul", "(", "queries", ",", "keys", ",", "transpose_b", "=", "True", ")", "# bs, hd, lq, lk", "\n", "\n", "queries", "=", "tf", ".", "reshape", "(", "tf", ".", "transpose", "(", "queries", ",", "[", "2", ",", "0", ",", "1", ",", "3", "]", ")", ",", "[", "lq", ",", "bs", "*", "hd", ",", "dk", "]", ")", "# lq, bs*hd, dk", "\n", "logits_part2", "=", "tf", ".", "matmul", "(", "queries", ",", "tf", ".", "transpose", "(", "rpr_k", ",", "[", "0", ",", "2", ",", "1", "]", ")", ")", "# lq, bs*hd, lk", "\n", "logits_part2", "=", "tf", ".", "reshape", "(", "tf", ".", "transpose", "(", "logits_part2", ",", "[", "1", ",", "0", ",", "2", "]", ")", ",", "[", "bs", ",", "hd", ",", "lq", ",", "lk", "]", ")", "\n", "\n", "logits", "=", "logits_part1", "+", "logits_part2", "# bs, hd, lq, lk", "\n", "\n", "# shape: [batch, heads, length_q, length_kv]", "\n", "", "if", "bias", "is", "not", "None", ":", "\n", "            ", "logits", "+=", "bias", "\n", "\n", "", "weights", "=", "tf", ".", "nn", ".", "softmax", "(", "logits", ",", "name", "=", "\"attention_weights\"", ")", "\n", "\n", "if", "keep_prob", "is", "not", "None", "and", "keep_prob", "<", "1.0", ":", "\n", "            ", "weights", "=", "tf", ".", "nn", ".", "dropout", "(", "weights", ",", "keep_prob", ")", "\n", "\n", "", "if", "rpr", "is", "None", ":", "\n", "            ", "outputs", "=", "tf", ".", "matmul", "(", "weights", ",", "values", ")", "# bs, hd, lq, dv", "\n", "", "else", ":", "# self-attention with relative position representaion", "\n", "            ", "outputs_part1", "=", "tf", ".", "matmul", "(", "weights", ",", "values", ")", "# bs, hd, lq, dv", "\n", "\n", "weights", "=", "tf", ".", "reshape", "(", "tf", ".", "transpose", "(", "weights", ",", "[", "2", ",", "0", ",", "1", ",", "3", "]", ")", ",", "[", "lq", ",", "bs", "*", "hd", ",", "lk", "]", ")", "# lq, bs*hd, lk", "\n", "outputs_part2", "=", "tf", ".", "matmul", "(", "weights", ",", "rpr_v", ")", "# lq, bs*hd, dv", "\n", "outputs_part2", "=", "tf", ".", "reshape", "(", "tf", ".", "transpose", "(", "outputs_part2", ",", "[", "1", ",", "0", ",", "2", "]", ")", ",", "[", "bs", ",", "hd", ",", "lq", ",", "dv", "]", ")", "\n", "\n", "outputs", "=", "outputs_part1", "+", "outputs_part2", "# bs, hd, lq, dv", "\n", "weights", "=", "tf", ".", "reshape", "(", "tf", ".", "transpose", "(", "weights", ",", "[", "1", ",", "0", ",", "2", "]", ")", ",", "[", "bs", ",", "hd", ",", "lq", ",", "lk", "]", ")", "# bs, hd, lq, lk", "\n", "\n", "", "return", "{", "\"weights\"", ":", "weights", ",", "\"outputs\"", ":", "outputs", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.layers.attention.multihead_attention": [[378, 480], ["ValueError", "ValueError", "tensorflow.variable_scope", "attention.split_heads", "attention.split_heads", "attention.split_heads", "attention.combine_heads", "thumt.layers.nn.linear", "tensorflow.split", "thumt.layers.nn.linear", "thumt.layers.nn.linear", "tensorflow.split", "tensorflow.shape", "tensorflow.shape", "tensorflow.get_variable", "tensorflow.get_variable", "attention.create_rpr", "attention.create_rpr", "attention.multiplicative_attention", "attention.multiplicative_attention", "thumt.layers.nn.linear", "attention.should_generate_summaries", "attention.attention_image_summary", "tensorflow.concat", "tensorflow.concat"], "function", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.layers.attention.split_heads", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.layers.attention.split_heads", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.layers.attention.split_heads", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.layers.attention.combine_heads", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.layers.nn.linear", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.layers.nn.linear", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.layers.nn.linear", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.layers.attention.create_rpr", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.layers.attention.create_rpr", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.layers.attention.multiplicative_attention", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.layers.attention.multiplicative_attention", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.layers.nn.linear", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.layers.attention.should_generate_summaries", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.layers.attention.attention_image_summary"], ["", "", "def", "multihead_attention", "(", "queries", ",", "memories", ",", "bias", ",", "num_heads", ",", "key_size", ",", "\n", "value_size", ",", "output_size", ",", "keep_prob", "=", "None", ",", "output", "=", "True", ",", "\n", "state", "=", "None", ",", "summary", "=", "True", ",", "dtype", "=", "None", ",", "scope", "=", "None", ",", "\n", "max_relative_dis", "=", "None", ")", ":", "\n", "    ", "\"\"\" Multi-head scaled-dot-product attention with input/output\n        transformations.\n\n    :param queries: A tensor with shape [batch, length_q, depth_q]\n    :param memories: A tensor with shape [batch, length_m, depth_m]\n    :param bias: A tensor (see attention_bias)\n    :param num_heads: An integer dividing key_size and value_size\n    :param key_size: An integer\n    :param value_size: An integer\n    :param output_size: An integer\n    :param keep_prob: A floating point number in (0, 1]\n    :param output: Whether to use output transformation\n    :param state: An optional dictionary used for incremental decoding\n    :param summary: Use image summary\n    :param dtype: An optional instance of tf.DType\n    :param scope: An optional string\n    :param max_relative_dis: An integer\n\n    :returns: A dict with the following keys:\n        weights: A tensor with shape [batch, heads, length_q, length_kv]\n        outputs: A tensor with shape [batch, length_q, depth_v]\n    \"\"\"", "\n", "\n", "if", "key_size", "%", "num_heads", "!=", "0", ":", "\n", "        ", "raise", "ValueError", "(", "\"Key size (%d) must be divisible by the number of \"", "\n", "\"attention heads (%d).\"", "%", "(", "key_size", ",", "num_heads", ")", ")", "\n", "\n", "", "if", "value_size", "%", "num_heads", "!=", "0", ":", "\n", "        ", "raise", "ValueError", "(", "\"Value size (%d) must be divisible by the number of \"", "\n", "\"attention heads (%d).\"", "%", "(", "value_size", ",", "num_heads", ")", ")", "\n", "\n", "", "with", "tf", ".", "variable_scope", "(", "scope", ",", "default_name", "=", "\"multihead_attention\"", ",", "\n", "values", "=", "[", "queries", ",", "memories", "]", ",", "dtype", "=", "dtype", ")", ":", "\n", "        ", "next_state", "=", "{", "}", "\n", "\n", "if", "memories", "is", "None", ":", "\n", "# self attention", "\n", "            ", "size", "=", "key_size", "*", "2", "+", "value_size", "\n", "combined", "=", "linear", "(", "queries", ",", "size", ",", "True", ",", "True", ",", "scope", "=", "\"qkv_transform\"", ")", "\n", "q", ",", "k", ",", "v", "=", "tf", ".", "split", "(", "combined", ",", "[", "key_size", ",", "key_size", ",", "value_size", "]", ",", "\n", "axis", "=", "-", "1", ")", "\n", "\n", "if", "state", "is", "not", "None", ":", "\n", "                ", "k", "=", "tf", ".", "concat", "(", "[", "state", "[", "\"key\"", "]", ",", "k", "]", ",", "axis", "=", "1", ")", "\n", "v", "=", "tf", ".", "concat", "(", "[", "state", "[", "\"value\"", "]", ",", "v", "]", ",", "axis", "=", "1", ")", "\n", "next_state", "[", "\"key\"", "]", "=", "k", "\n", "next_state", "[", "\"value\"", "]", "=", "v", "\n", "", "", "else", ":", "\n", "            ", "q", "=", "linear", "(", "queries", ",", "key_size", ",", "True", ",", "True", ",", "scope", "=", "\"q_transform\"", ")", "\n", "combined", "=", "linear", "(", "memories", ",", "key_size", "+", "value_size", ",", "True", ",", "\n", "scope", "=", "\"kv_transform\"", ")", "\n", "k", ",", "v", "=", "tf", ".", "split", "(", "combined", ",", "[", "key_size", ",", "value_size", "]", ",", "axis", "=", "-", "1", ")", "\n", "\n", "# split heads", "\n", "", "q", "=", "split_heads", "(", "q", ",", "num_heads", ")", "\n", "k", "=", "split_heads", "(", "k", ",", "num_heads", ")", "\n", "v", "=", "split_heads", "(", "v", ",", "num_heads", ")", "\n", "\n", "# get length", "\n", "length_q", "=", "tf", ".", "shape", "(", "q", ")", "[", "2", "]", "\n", "length_kv", "=", "tf", ".", "shape", "(", "k", ")", "[", "2", "]", "\n", "\n", "# scale query", "\n", "key_depth_per_head", "=", "key_size", "//", "num_heads", "\n", "q", "*=", "key_depth_per_head", "**", "-", "0.5", "\n", "\n", "# relative position representation (only in self-attention)", "\n", "if", "max_relative_dis", "and", "memories", "is", "None", ":", "\n", "            ", "rpr_k", "=", "tf", ".", "get_variable", "(", "'rpr_k'", ",", "[", "2", "*", "max_relative_dis", "+", "1", ",", "key_size", "//", "num_heads", "]", ")", "\n", "rpr_v", "=", "tf", ".", "get_variable", "(", "'rpr_v'", ",", "[", "2", "*", "max_relative_dis", "+", "1", ",", "value_size", "//", "num_heads", "]", ")", "\n", "rpr_k", "=", "create_rpr", "(", "rpr_k", ",", "length_q", ",", "length_kv", ",", "max_relative_dis", ")", "\n", "rpr_v", "=", "create_rpr", "(", "rpr_v", ",", "length_q", ",", "length_kv", ",", "max_relative_dis", ")", "\n", "rpr", "=", "{", "'rpr_k'", ":", "rpr_k", ",", "'rpr_v'", ":", "rpr_v", "}", "\n", "# attention", "\n", "results", "=", "multiplicative_attention", "(", "q", ",", "k", ",", "v", ",", "bias", ",", "keep_prob", ",", "rpr", "=", "rpr", ")", "\n", "", "else", ":", "\n", "# attention", "\n", "            ", "results", "=", "multiplicative_attention", "(", "q", ",", "k", ",", "v", ",", "bias", ",", "keep_prob", ")", "\n", "\n", "# combine heads", "\n", "", "weights", "=", "results", "[", "\"weights\"", "]", "\n", "x", "=", "combine_heads", "(", "results", "[", "\"outputs\"", "]", ")", "\n", "\n", "if", "output", ":", "\n", "            ", "outputs", "=", "linear", "(", "x", ",", "output_size", ",", "True", ",", "True", ",", "\n", "scope", "=", "\"output_transform\"", ")", "\n", "", "else", ":", "\n", "            ", "outputs", "=", "x", "\n", "\n", "", "if", "should_generate_summaries", "(", ")", "and", "summary", ":", "\n", "            ", "attention_image_summary", "(", "weights", ")", "\n", "\n", "", "outputs", "=", "{", "\"weights\"", ":", "weights", ",", "\"outputs\"", ":", "outputs", "}", "\n", "\n", "if", "state", "is", "not", "None", ":", "\n", "            ", "outputs", "[", "\"state\"", "]", "=", "next_state", "\n", "\n", "", "return", "outputs", "\n", "", "", ""]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.layers.nn.linear": [[11, 66], ["tensorflow.variable_scope", "tensorflow.concat", "tensorflow.add_n", "tensorflow.reshape", "isinstance", "len", "len", "RuntimeError", "tensorflow.reshape", "sum", "tensorflow.concat", "tensorflow.get_variable", "results.append", "range", "tensorflow.get_variable", "tensorflow.nn.bias_add", "tensorflow.matmul", "len", "tensorflow.get_variable", "results.append", "item.get_shape", "tensorflow.shape", "tensorflow.matmul"], "function", ["None"], ["def", "linear", "(", "inputs", ",", "output_size", ",", "bias", ",", "concat", "=", "True", ",", "dtype", "=", "None", ",", "scope", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    Linear layer\n    :param inputs: A Tensor or a list of Tensors with shape [batch, input_size]\n    :param output_size: An integer specify the output size\n    :param bias: a boolean value indicate whether to use bias term\n    :param concat: a boolean value indicate whether to concatenate all inputs\n    :param dtype: an instance of tf.DType\n    :param scope: the scope of this layer, the default value is ``linear''\n    :returns: a Tensor with shape [batch, output_size]\n    :raises RuntimeError: raises ``RuntimeError'' when input sizes do not\n                          compatible with each other\n    \"\"\"", "\n", "\n", "with", "tf", ".", "variable_scope", "(", "scope", ",", "default_name", "=", "\"linear\"", ",", "values", "=", "[", "inputs", "]", ",", "\n", "dtype", "=", "dtype", ")", ":", "\n", "        ", "if", "not", "isinstance", "(", "inputs", ",", "(", "list", ",", "tuple", ")", ")", ":", "\n", "            ", "inputs", "=", "[", "inputs", "]", "\n", "\n", "", "input_size", "=", "[", "item", ".", "get_shape", "(", ")", "[", "-", "1", "]", ".", "value", "for", "item", "in", "inputs", "]", "\n", "\n", "if", "len", "(", "inputs", ")", "!=", "len", "(", "input_size", ")", ":", "\n", "            ", "raise", "RuntimeError", "(", "\"inputs and input_size unmatched!\"", ")", "\n", "\n", "", "output_shape", "=", "tf", ".", "concat", "(", "[", "tf", ".", "shape", "(", "inputs", "[", "0", "]", ")", "[", ":", "-", "1", "]", ",", "[", "output_size", "]", "]", ",", "\n", "axis", "=", "0", ")", "\n", "# Flatten to 2D", "\n", "inputs", "=", "[", "tf", ".", "reshape", "(", "inp", ",", "[", "-", "1", ",", "inp", ".", "shape", "[", "-", "1", "]", ".", "value", "]", ")", "for", "inp", "in", "inputs", "]", "\n", "\n", "results", "=", "[", "]", "\n", "\n", "if", "concat", ":", "\n", "            ", "input_size", "=", "sum", "(", "input_size", ")", "\n", "inputs", "=", "tf", ".", "concat", "(", "inputs", ",", "1", ")", "\n", "\n", "shape", "=", "[", "input_size", ",", "output_size", "]", "\n", "matrix", "=", "tf", ".", "get_variable", "(", "\"matrix\"", ",", "shape", ")", "\n", "results", ".", "append", "(", "tf", ".", "matmul", "(", "inputs", ",", "matrix", ")", ")", "\n", "", "else", ":", "\n", "            ", "for", "i", "in", "range", "(", "len", "(", "input_size", ")", ")", ":", "\n", "                ", "shape", "=", "[", "input_size", "[", "i", "]", ",", "output_size", "]", "\n", "name", "=", "\"matrix_%d\"", "%", "i", "\n", "matrix", "=", "tf", ".", "get_variable", "(", "name", ",", "shape", ")", "\n", "results", ".", "append", "(", "tf", ".", "matmul", "(", "inputs", "[", "i", "]", ",", "matrix", ")", ")", "\n", "\n", "", "", "output", "=", "tf", ".", "add_n", "(", "results", ")", "\n", "\n", "if", "bias", ":", "\n", "            ", "shape", "=", "[", "output_size", "]", "\n", "bias", "=", "tf", ".", "get_variable", "(", "\"bias\"", ",", "shape", ")", "\n", "output", "=", "tf", ".", "nn", ".", "bias_add", "(", "output", ",", "bias", ")", "\n", "\n", "", "output", "=", "tf", ".", "reshape", "(", "output", ",", "output_shape", ")", "\n", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.layers.nn.maxout": [[68, 91], ["nn.linear", "tensorflow.concat", "tensorflow.reshape", "tensorflow.reduce_max", "tensorflow.shape"], "function", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.layers.nn.linear"], ["", "", "def", "maxout", "(", "inputs", ",", "output_size", ",", "maxpart", "=", "2", ",", "use_bias", "=", "True", ",", "concat", "=", "True", ",", "\n", "dtype", "=", "None", ",", "scope", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    Maxout layer\n    :param inputs: see the corresponding description of ``linear''\n    :param output_size: see the corresponding description of ``linear''\n    :param maxpart: an integer, the default value is 2\n    :param use_bias: a boolean value indicate whether to use bias term\n    :param concat: concat all tensors if inputs is a list of tensors\n    :param dtype: an optional instance of tf.Dtype\n    :param scope: the scope of this layer, the default value is ``maxout''\n    :returns: a Tensor with shape [batch, output_size]\n    :raises RuntimeError: see the corresponding description of ``linear''\n    \"\"\"", "\n", "\n", "candidate", "=", "linear", "(", "inputs", ",", "output_size", "*", "maxpart", ",", "use_bias", ",", "concat", ",", "\n", "dtype", "=", "dtype", ",", "scope", "=", "scope", "or", "\"maxout\"", ")", "\n", "shape", "=", "tf", ".", "concat", "(", "[", "tf", ".", "shape", "(", "candidate", ")", "[", ":", "-", "1", "]", ",", "[", "output_size", ",", "maxpart", "]", "]", ",", "\n", "axis", "=", "0", ")", "\n", "value", "=", "tf", ".", "reshape", "(", "candidate", ",", "shape", ")", "\n", "output", "=", "tf", ".", "reduce_max", "(", "value", ",", "-", "1", ")", "\n", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.layers.nn.layer_norm": [[93, 118], ["tensorflow.variable_scope", "tensorflow.get_variable", "tensorflow.get_variable", "tensorflow.reduce_mean", "tensorflow.reduce_mean", "inputs.get_shape().as_list", "tensorflow.square", "tensorflow.rsqrt", "tensorflow.ones_initializer", "tensorflow.zeros_initializer", "inputs.get_shape"], "function", ["None"], ["", "def", "layer_norm", "(", "inputs", ",", "epsilon", "=", "1e-6", ",", "dtype", "=", "None", ",", "scope", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    Layer Normalization\n    :param inputs: A Tensor of shape [..., channel_size]\n    :param epsilon: A floating number\n    :param dtype: An optional instance of tf.DType\n    :param scope: An optional string\n    :returns: A Tensor with the same shape as inputs\n    \"\"\"", "\n", "with", "tf", ".", "variable_scope", "(", "scope", ",", "default_name", "=", "\"layer_norm\"", ",", "values", "=", "[", "inputs", "]", ",", "\n", "dtype", "=", "dtype", ")", ":", "\n", "        ", "channel_size", "=", "inputs", ".", "get_shape", "(", ")", ".", "as_list", "(", ")", "[", "-", "1", "]", "\n", "\n", "scale", "=", "tf", ".", "get_variable", "(", "\"scale\"", ",", "shape", "=", "[", "channel_size", "]", ",", "\n", "initializer", "=", "tf", ".", "ones_initializer", "(", ")", ")", "\n", "\n", "offset", "=", "tf", ".", "get_variable", "(", "\"offset\"", ",", "shape", "=", "[", "channel_size", "]", ",", "\n", "initializer", "=", "tf", ".", "zeros_initializer", "(", ")", ")", "\n", "\n", "mean", "=", "tf", ".", "reduce_mean", "(", "inputs", ",", "-", "1", ",", "True", ")", "\n", "variance", "=", "tf", ".", "reduce_mean", "(", "tf", ".", "square", "(", "inputs", "-", "mean", ")", ",", "-", "1", ",", "True", ")", "\n", "\n", "norm_inputs", "=", "(", "inputs", "-", "mean", ")", "*", "tf", ".", "rsqrt", "(", "variance", "+", "epsilon", ")", "\n", "\n", "return", "norm_inputs", "*", "scale", "+", "offset", "\n", "", "", ""]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.scripts.convert_old_model.parseargs": [[15, 24], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args"], "function", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.scripts.build_vocab.parse_args"], ["def", "parseargs", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "description", "=", "\"Convert old models\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--input\"", ",", "type", "=", "str", ",", "required", "=", "True", ",", "\n", "help", "=", "\"Path of old model\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--output\"", ",", "type", "=", "str", ",", "required", "=", "True", ",", "\n", "help", "=", "\"Path of output checkpoint\"", ")", "\n", "\n", "return", "parser", ".", "parse_args", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.scripts.convert_old_model.old_keys": [[26, 71], ["None"], "function", ["None"], ["", "def", "old_keys", "(", ")", ":", "\n", "    ", "keys", "=", "[", "\n", "\"GRU_dec_attcontext\"", ",", "\n", "\"GRU_dec_att\"", ",", "\n", "\"GRU_dec_atthidden\"", ",", "\n", "\"GRU_dec_inputoffset\"", ",", "\n", "\"GRU_dec_inputemb\"", ",", "\n", "\"GRU_dec_inputcontext\"", ",", "\n", "\"GRU_dec_inputhidden\"", ",", "\n", "\"GRU_dec_resetemb\"", ",", "\n", "\"GRU_dec_resetcontext\"", ",", "\n", "\"GRU_dec_resethidden\"", ",", "\n", "\"GRU_dec_gateemb\"", ",", "\n", "\"GRU_dec_gatecontext\"", ",", "\n", "\"GRU_dec_gatehidden\"", ",", "\n", "\"initer_b\"", ",", "\n", "\"initer_W\"", ",", "\n", "\"GRU_dec_probsemb\"", ",", "\n", "\"GRU_enc_back_inputoffset\"", ",", "\n", "\"GRU_enc_back_inputemb\"", ",", "\n", "\"GRU_enc_back_inputhidden\"", ",", "\n", "\"GRU_enc_back_resetemb\"", ",", "\n", "\"GRU_enc_back_resethidden\"", ",", "\n", "\"GRU_enc_back_gateemb\"", ",", "\n", "\"GRU_enc_back_gatehidden\"", ",", "\n", "\"GRU_enc_inputoffset\"", ",", "\n", "\"GRU_enc_inputemb\"", ",", "\n", "\"GRU_enc_inputhidden\"", ",", "\n", "\"GRU_enc_resetemb\"", ",", "\n", "\"GRU_enc_resethidden\"", ",", "\n", "\"GRU_enc_gateemb\"", ",", "\n", "\"GRU_enc_gatehidden\"", ",", "\n", "\"GRU_dec_readoutoffset\"", ",", "\n", "\"GRU_dec_readoutemb\"", ",", "\n", "\"GRU_dec_readouthidden\"", ",", "\n", "\"GRU_dec_readoutcontext\"", ",", "\n", "\"GRU_dec_probsoffset\"", ",", "\n", "\"GRU_dec_probs\"", ",", "\n", "\"emb_src_b\"", ",", "\n", "\"emb_src_emb\"", ",", "\n", "\"emb_trg_b\"", ",", "\n", "\"emb_trg_emb\"", "\n", "]", "\n", "\n", "return", "keys", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.scripts.convert_old_model.new_keys": [[73, 118], ["None"], "function", ["None"], ["", "def", "new_keys", "(", ")", ":", "\n", "    ", "keys", "=", "[", "\n", "\"rnnsearch/decoder/attention/k_transform/matrix_0\"", ",", "\n", "\"rnnsearch/decoder/attention/logits/matrix_0\"", ",", "\n", "\"rnnsearch/decoder/attention/q_transform/matrix_0\"", ",", "\n", "\"rnnsearch/decoder/gru_cell/candidate/bias\"", ",", "\n", "\"rnnsearch/decoder/gru_cell/candidate/matrix_0\"", ",", "\n", "\"rnnsearch/decoder/gru_cell/candidate/matrix_1\"", ",", "\n", "\"rnnsearch/decoder/gru_cell/candidate/matrix_2\"", ",", "\n", "\"rnnsearch/decoder/gru_cell/reset_gate/matrix_0\"", ",", "\n", "\"rnnsearch/decoder/gru_cell/reset_gate/matrix_1\"", ",", "\n", "\"rnnsearch/decoder/gru_cell/reset_gate/matrix_2\"", ",", "\n", "\"rnnsearch/decoder/gru_cell/update_gate/matrix_0\"", ",", "\n", "\"rnnsearch/decoder/gru_cell/update_gate/matrix_1\"", ",", "\n", "\"rnnsearch/decoder/gru_cell/update_gate/matrix_2\"", ",", "\n", "\"rnnsearch/decoder/s_transform/bias\"", ",", "\n", "\"rnnsearch/decoder/s_transform/matrix_0\"", ",", "\n", "\"rnnsearch/deepout/matrix_0\"", ",", "\n", "\"rnnsearch/encoder/backward/gru_cell/candidate/bias\"", ",", "\n", "\"rnnsearch/encoder/backward/gru_cell/candidate/matrix_0\"", ",", "\n", "\"rnnsearch/encoder/backward/gru_cell/candidate/matrix_1\"", ",", "\n", "\"rnnsearch/encoder/backward/gru_cell/reset_gate/matrix_0\"", ",", "\n", "\"rnnsearch/encoder/backward/gru_cell/reset_gate/matrix_1\"", ",", "\n", "\"rnnsearch/encoder/backward/gru_cell/update_gate/matrix_0\"", ",", "\n", "\"rnnsearch/encoder/backward/gru_cell/update_gate/matrix_1\"", ",", "\n", "\"rnnsearch/encoder/forward/gru_cell/candidate/bias\"", ",", "\n", "\"rnnsearch/encoder/forward/gru_cell/candidate/matrix_0\"", ",", "\n", "\"rnnsearch/encoder/forward/gru_cell/candidate/matrix_1\"", ",", "\n", "\"rnnsearch/encoder/forward/gru_cell/reset_gate/matrix_0\"", ",", "\n", "\"rnnsearch/encoder/forward/gru_cell/reset_gate/matrix_1\"", ",", "\n", "\"rnnsearch/encoder/forward/gru_cell/update_gate/matrix_0\"", ",", "\n", "\"rnnsearch/encoder/forward/gru_cell/update_gate/matrix_1\"", ",", "\n", "\"rnnsearch/maxout/bias\"", ",", "\n", "\"rnnsearch/maxout/matrix_0\"", ",", "\n", "\"rnnsearch/maxout/matrix_1\"", ",", "\n", "\"rnnsearch/maxout/matrix_2\"", ",", "\n", "\"rnnsearch/softmax/bias\"", ",", "\n", "\"rnnsearch/softmax/matrix_0\"", ",", "\n", "\"rnnsearch/source_embedding/bias\"", ",", "\n", "\"rnnsearch/source_embedding/embedding\"", ",", "\n", "\"rnnsearch/target_embedding/bias\"", ",", "\n", "\"rnnsearch/target_embedding/embedding\"", ",", "\n", "]", "\n", "\n", "return", "keys", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.scripts.convert_old_model.main": [[120, 144], ["dict", "convert_old_model.old_keys", "convert_old_model.new_keys", "enumerate", "numpy.load", "tensorflow.Graph().as_default", "tensorflow.train.Saver", "tensorflow.device", "tensorflow.Variable", "tensorflow.Session", "sess.run", "tf.train.Saver.save", "tensorflow.Graph", "tensorflow.get_variable", "tensorflow.global_variables_initializer"], "function", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.scripts.convert_old_model.old_keys", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.scripts.convert_old_model.new_keys", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.bert_dictionary.BertDictionary.save"], ["", "def", "main", "(", "args", ")", ":", "\n", "    ", "values", "=", "dict", "(", "np", ".", "load", "(", "args", ".", "input", ")", ")", "\n", "variables", "=", "{", "}", "\n", "o_keys", "=", "old_keys", "(", ")", "\n", "n_keys", "=", "new_keys", "(", ")", "\n", "\n", "for", "i", ",", "key", "in", "enumerate", "(", "o_keys", ")", ":", "\n", "        ", "v", "=", "values", "[", "key", "]", "\n", "variables", "[", "n_keys", "[", "i", "]", "]", "=", "v", "\n", "\n", "", "with", "tf", ".", "Graph", "(", ")", ".", "as_default", "(", ")", ":", "\n", "        ", "with", "tf", ".", "device", "(", "\"/cpu:0\"", ")", ":", "\n", "            ", "tf_vars", "=", "[", "\n", "tf", ".", "get_variable", "(", "v", ",", "initializer", "=", "variables", "[", "v", "]", ",", "dtype", "=", "tf", ".", "float32", ")", "\n", "for", "v", "in", "variables", "\n", "]", "\n", "global_step", "=", "tf", ".", "Variable", "(", "0", ",", "name", "=", "\"global_step\"", ",", "trainable", "=", "False", ",", "\n", "dtype", "=", "tf", ".", "int64", ")", "\n", "\n", "", "saver", "=", "tf", ".", "train", ".", "Saver", "(", "tf_vars", ")", "\n", "\n", "with", "tf", ".", "Session", "(", ")", "as", "sess", ":", "\n", "            ", "sess", ".", "run", "(", "tf", ".", "global_variables_initializer", "(", ")", ")", "\n", "saver", ".", "save", "(", "sess", ",", "args", ".", "output", ",", "global_step", "=", "global_step", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.scripts.shuffle_corpus.parseargs": [[12, 24], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args"], "function", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.scripts.build_vocab.parse_args"], ["def", "parseargs", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "description", "=", "\"Shuffle corpus\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--corpus\"", ",", "nargs", "=", "\"+\"", ",", "required", "=", "True", ",", "\n", "help", "=", "\"input corpora\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--suffix\"", ",", "type", "=", "str", ",", "default", "=", "\"shuf\"", ",", "\n", "help", "=", "\"Suffix of output files\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--seed\"", ",", "type", "=", "int", ",", "help", "=", "\"Random seed\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--num_shards\"", ",", "type", "=", "int", ",", "default", "=", "1", ",", "\n", "help", "=", "\"shard number\"", ")", "\n", "\n", "return", "parser", ".", "parse_args", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.scripts.shuffle_corpus.main": [[26, 60], ["min", "numpy.arange", "numpy.random.shuffle", "numpy.arange.tolist", "open", "fd.readlines", "numpy.random.seed", "zip", "fdr.close", "len", "fd.write", "fd.close", "open", "open", "range"], "function", ["None"], ["", "def", "main", "(", "args", ")", ":", "\n", "    ", "name", "=", "args", ".", "corpus", "\n", "suffix", "=", "\".\"", "+", "args", ".", "suffix", "\n", "stream", "=", "[", "open", "(", "item", ",", "\"r\"", ")", "for", "item", "in", "name", "]", "\n", "data", "=", "[", "fd", ".", "readlines", "(", ")", "for", "fd", "in", "stream", "]", "\n", "minlen", "=", "min", "(", "[", "len", "(", "lines", ")", "for", "lines", "in", "data", "]", ")", "\n", "count", "=", "0", "\n", "\n", "if", "args", ".", "seed", ":", "\n", "        ", "numpy", ".", "random", ".", "seed", "(", "args", ".", "seed", ")", "\n", "\n", "", "indices", "=", "numpy", ".", "arange", "(", "minlen", ")", "\n", "numpy", ".", "random", ".", "shuffle", "(", "indices", ")", "\n", "\n", "if", "args", ".", "num_shards", "==", "1", ":", "\n", "        ", "newstream", "=", "[", "[", "open", "(", "item", "+", "suffix", ",", "\"w\"", ")", "for", "item", "in", "name", "]", "]", "\n", "", "else", ":", "\n", "        ", "newstream", "=", "[", "[", "open", "(", "item", "+", "\"-%s-of-%s\"", "%", "(", "i", ",", "args", ".", "num_shards", ")", ",", "\"w\"", ")", "\n", "for", "item", "in", "name", "]", "for", "i", "in", "range", "(", "args", ".", "num_shards", ")", "]", "\n", "\n", "", "for", "idx", "in", "indices", ".", "tolist", "(", ")", ":", "\n", "        ", "lines", "=", "[", "item", "[", "idx", "]", "for", "item", "in", "data", "]", "\n", "\n", "for", "line", ",", "fd", "in", "zip", "(", "lines", ",", "newstream", "[", "count", "%", "args", ".", "num_shards", "]", ")", ":", "\n", "            ", "fd", ".", "write", "(", "line", ")", "\n", "\n", "", "count", "+=", "1", "\n", "\n", "", "for", "fdr", "in", "stream", ":", "\n", "        ", "fdr", ".", "close", "(", ")", "\n", "\n", "", "for", "fds", "in", "newstream", ":", "\n", "        ", "for", "fd", "in", "fds", ":", "\n", "            ", "fd", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.scripts.checkpoint_averaging.parseargs": [[17, 29], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args"], "function", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.scripts.build_vocab.parse_args"], ["def", "parseargs", "(", ")", ":", "\n", "    ", "msg", "=", "\"Average checkpoints\"", "\n", "usage", "=", "\"average.py [<args>] [-h | --help]\"", "\n", "parser", "=", "argparse", ".", "ArgumentParser", "(", "description", "=", "msg", ",", "usage", "=", "usage", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--path\"", ",", "type", "=", "str", ",", "required", "=", "True", ",", "\n", "help", "=", "\"checkpoint dir\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--checkpoints\"", ",", "type", "=", "int", ",", "required", "=", "True", ",", "\n", "help", "=", "\"number of checkpoints to use\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--output\"", ",", "type", "=", "str", ",", "help", "=", "\"output path\"", ")", "\n", "\n", "return", "parser", ".", "parse_args", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.scripts.checkpoint_averaging.get_checkpoints": [[31, 49], ["sorted", "tensorflow.gfile.Exists", "ValueError", "tensorflow.gfile.GFile", "fd.readline", "os.path.join", "os.path.join", "int", "checkpoint_names.append", "operator.itemgetter", "[].strip", "name.split", "os.path.join", "line.strip().split", "line.strip"], "function", ["None"], ["", "def", "get_checkpoints", "(", "path", ")", ":", "\n", "    ", "if", "not", "tf", ".", "gfile", ".", "Exists", "(", "os", ".", "path", ".", "join", "(", "path", ",", "\"checkpoint\"", ")", ")", ":", "\n", "        ", "raise", "ValueError", "(", "\"Cannot find checkpoints in %s\"", "%", "path", ")", "\n", "\n", "", "checkpoint_names", "=", "[", "]", "\n", "\n", "with", "tf", ".", "gfile", ".", "GFile", "(", "os", ".", "path", ".", "join", "(", "path", ",", "\"checkpoint\"", ")", ")", "as", "fd", ":", "\n", "# Skip the first line", "\n", "        ", "fd", ".", "readline", "(", ")", "\n", "for", "line", "in", "fd", ":", "\n", "            ", "name", "=", "line", ".", "strip", "(", ")", ".", "split", "(", "\":\"", ")", "[", "-", "1", "]", ".", "strip", "(", ")", "[", "1", ":", "-", "1", "]", "\n", "key", "=", "int", "(", "name", ".", "split", "(", "\"-\"", ")", "[", "-", "1", "]", ")", "\n", "checkpoint_names", ".", "append", "(", "(", "key", ",", "os", ".", "path", ".", "join", "(", "path", ",", "name", ")", ")", ")", "\n", "\n", "", "", "sorted_names", "=", "sorted", "(", "checkpoint_names", ",", "key", "=", "operator", ".", "itemgetter", "(", "0", ")", ",", "\n", "reverse", "=", "True", ")", "\n", "\n", "return", "[", "item", "[", "-", "1", "]", "for", "item", "in", "sorted_names", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.scripts.checkpoint_averaging.checkpoint_exists": [[51, 54], ["tensorflow.gfile.Exists", "tensorflow.gfile.Exists", "tensorflow.gfile.Exists"], "function", ["None"], ["", "def", "checkpoint_exists", "(", "path", ")", ":", "\n", "    ", "return", "(", "tf", ".", "gfile", ".", "Exists", "(", "path", ")", "or", "tf", ".", "gfile", ".", "Exists", "(", "path", "+", "\".meta\"", ")", "or", "\n", "tf", ".", "gfile", ".", "Exists", "(", "path", "+", "\".index\"", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.scripts.checkpoint_averaging.main": [[56, 117], ["tensorflow.logging.set_verbosity", "checkpoint_averaging.get_checkpoints", "tensorflow.contrib.framework.list_variables", "tensorflow.Variable", "tensorflow.train.Saver", "tensorflow.logging.info", "os.path.join", "tensorflow.gfile.Glob", "ValueError", "ValueError", "tensorflow.contrib.framework.load_checkpoint", "tensorflow.logging.info", "len", "tensorflow.get_variable", "tensorflow.placeholder", "tensorflow.assign", "tensorflow.global_variables", "tensorflow.Session", "sess.run", "zip", "os.path.join", "tf.train.Saver.save", "name.replace", "tensorflow.gfile.Copy", "checkpoint_averaging.checkpoint_exists", "name.startswith", "numpy.zeros", "tf.contrib.framework.load_checkpoint.get_tensor", "zip", "tensorflow.global_variables_initializer", "var_values.iteritems", "sess.run", "FLAGS.path.rstrip", "FLAGS.output.rstrip"], "function", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.scripts.checkpoint_averaging.get_checkpoints", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.bert_dictionary.BertDictionary.save", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.scripts.checkpoint_averaging.checkpoint_exists"], ["", "def", "main", "(", "_", ")", ":", "\n", "    ", "tf", ".", "logging", ".", "set_verbosity", "(", "tf", ".", "logging", ".", "INFO", ")", "\n", "checkpoints", "=", "get_checkpoints", "(", "FLAGS", ".", "path", ")", "\n", "checkpoints", "=", "checkpoints", "[", ":", "FLAGS", ".", "checkpoints", "]", "\n", "\n", "if", "not", "checkpoints", ":", "\n", "        ", "raise", "ValueError", "(", "\"No checkpoints provided for averaging.\"", ")", "\n", "\n", "", "checkpoints", "=", "[", "c", "for", "c", "in", "checkpoints", "if", "checkpoint_exists", "(", "c", ")", "]", "\n", "\n", "if", "not", "checkpoints", ":", "\n", "        ", "raise", "ValueError", "(", "\n", "\"None of the provided checkpoints exist. %s\"", "%", "FLAGS", ".", "checkpoints", "\n", ")", "\n", "\n", "", "var_list", "=", "tf", ".", "contrib", ".", "framework", ".", "list_variables", "(", "checkpoints", "[", "0", "]", ")", "\n", "var_values", ",", "var_dtypes", "=", "{", "}", ",", "{", "}", "\n", "\n", "for", "(", "name", ",", "shape", ")", "in", "var_list", ":", "\n", "        ", "if", "not", "name", ".", "startswith", "(", "\"global_step\"", ")", ":", "\n", "            ", "var_values", "[", "name", "]", "=", "np", ".", "zeros", "(", "shape", ")", "\n", "\n", "", "", "for", "checkpoint", "in", "checkpoints", ":", "\n", "        ", "reader", "=", "tf", ".", "contrib", ".", "framework", ".", "load_checkpoint", "(", "checkpoint", ")", "\n", "for", "name", "in", "var_values", ":", "\n", "            ", "tensor", "=", "reader", ".", "get_tensor", "(", "name", ")", "\n", "var_dtypes", "[", "name", "]", "=", "tensor", ".", "dtype", "\n", "var_values", "[", "name", "]", "+=", "tensor", "\n", "", "tf", ".", "logging", ".", "info", "(", "\"Read from checkpoint %s\"", ",", "checkpoint", ")", "\n", "\n", "# Average checkpoints", "\n", "", "for", "name", "in", "var_values", ":", "\n", "        ", "var_values", "[", "name", "]", "/=", "len", "(", "checkpoints", ")", "\n", "\n", "", "tf_vars", "=", "[", "\n", "tf", ".", "get_variable", "(", "name", ",", "shape", "=", "var_values", "[", "name", "]", ".", "shape", ",", "\n", "dtype", "=", "var_dtypes", "[", "name", "]", ")", "for", "name", "in", "var_values", "\n", "]", "\n", "placeholders", "=", "[", "tf", ".", "placeholder", "(", "v", ".", "dtype", ",", "shape", "=", "v", ".", "shape", ")", "for", "v", "in", "tf_vars", "]", "\n", "assign_ops", "=", "[", "tf", ".", "assign", "(", "v", ",", "p", ")", "for", "(", "v", ",", "p", ")", "in", "zip", "(", "tf_vars", ",", "placeholders", ")", "]", "\n", "global_step", "=", "tf", ".", "Variable", "(", "0", ",", "name", "=", "\"global_step\"", ",", "trainable", "=", "False", ",", "\n", "dtype", "=", "tf", ".", "int64", ")", "\n", "saver", "=", "tf", ".", "train", ".", "Saver", "(", "tf", ".", "global_variables", "(", ")", ")", "\n", "\n", "with", "tf", ".", "Session", "(", ")", "as", "sess", ":", "\n", "        ", "sess", ".", "run", "(", "tf", ".", "global_variables_initializer", "(", ")", ")", "\n", "for", "p", ",", "assign_op", ",", "(", "name", ",", "value", ")", "in", "zip", "(", "placeholders", ",", "assign_ops", ",", "\n", "var_values", ".", "iteritems", "(", ")", ")", ":", "\n", "            ", "sess", ".", "run", "(", "assign_op", ",", "{", "p", ":", "value", "}", ")", "\n", "", "saved_name", "=", "os", ".", "path", ".", "join", "(", "FLAGS", ".", "output", ",", "\"average\"", ")", "\n", "saver", ".", "save", "(", "sess", ",", "saved_name", ",", "global_step", "=", "global_step", ")", "\n", "\n", "", "tf", ".", "logging", ".", "info", "(", "\"Averaged checkpoints saved in %s\"", ",", "saved_name", ")", "\n", "\n", "params_pattern", "=", "os", ".", "path", ".", "join", "(", "FLAGS", ".", "path", ",", "\"*.json\"", ")", "\n", "params_files", "=", "tf", ".", "gfile", ".", "Glob", "(", "params_pattern", ")", "\n", "\n", "for", "name", "in", "params_files", ":", "\n", "        ", "new_name", "=", "name", ".", "replace", "(", "FLAGS", ".", "path", ".", "rstrip", "(", "\"/\"", ")", ",", "\n", "FLAGS", ".", "output", ".", "rstrip", "(", "\"/\"", ")", ")", "\n", "tf", ".", "gfile", ".", "Copy", "(", "name", ",", "new_name", ",", "overwrite", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.scripts.combine_add.parseargs": [[18, 30], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args"], "function", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.scripts.build_vocab.parse_args"], ["def", "parseargs", "(", ")", ":", "\n", "    ", "msg", "=", "\"Average checkpoints\"", "\n", "usage", "=", "\"average.py [<args>] [-h | --help]\"", "\n", "parser", "=", "argparse", ".", "ArgumentParser", "(", "description", "=", "msg", ",", "usage", "=", "usage", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--model\"", ",", "type", "=", "str", ",", "required", "=", "True", ",", "\n", "help", "=", "\"checkpoint dir\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--part\"", ",", "type", "=", "str", ",", "required", "=", "True", ",", "\n", "help", "=", "\"partial model dir\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--output\"", ",", "type", "=", "str", ",", "help", "=", "\"output path\"", ")", "\n", "\n", "return", "parser", ".", "parse_args", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.scripts.combine_add.main": [[31, 93], ["tensorflow.logging.set_verbosity", "tensorflow.contrib.framework.list_variables", "tensorflow.contrib.framework.list_variables", "tensorflow.contrib.framework.load_checkpoint", "tensorflow.contrib.framework.load_checkpoint", "tensorflow.logging.info", "tensorflow.Variable", "tensorflow.train.Saver", "tensorflow.logging.info", "name.endswith", "name.endswith", "tensorflow.get_variable", "tensorflow.placeholder", "tensorflow.assign", "tensorflow.global_variables", "tensorflow.Session", "sess.run", "zip", "os.path.join", "tf.train.Saver.save", "numpy.zeros", "re.search().group", "tf.contrib.framework.load_checkpoint.get_tensor", "numpy.zeros", "numpy.zeros", "re.search().group", "print", "tf.contrib.framework.load_checkpoint.get_tensor", "print", "tf.contrib.framework.load_checkpoint.get_tensor", "print", "zip", "tensorflow.global_variables_initializer", "var_values.items", "sess.run", "re.search", "re.search"], "function", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.prophetnet.bert_dictionary.BertDictionary.save"], ["", "def", "main", "(", "_", ")", ":", "\n", "    ", "tf", ".", "logging", ".", "set_verbosity", "(", "tf", ".", "logging", ".", "INFO", ")", "\n", "\n", "var_list", "=", "tf", ".", "contrib", ".", "framework", ".", "list_variables", "(", "FLAGS", ".", "model", ")", "\n", "var_part", "=", "tf", ".", "contrib", ".", "framework", ".", "list_variables", "(", "FLAGS", ".", "part", ")", "\n", "var_values", ",", "var_dtypes", "=", "{", "}", ",", "{", "}", "\n", "var_values_part", "=", "{", "}", "\n", "var_values_part_for_beta", "=", "{", "}", "\n", "reader", "=", "tf", ".", "contrib", ".", "framework", ".", "load_checkpoint", "(", "FLAGS", ".", "model", ")", "\n", "reader_part", "=", "tf", ".", "contrib", ".", "framework", ".", "load_checkpoint", "(", "FLAGS", ".", "part", ")", "\n", "\n", "for", "(", "name", ",", "shape", ")", "in", "var_list", ":", "\n", "        ", "if", "True", ":", "#not name.startswith(\"global_step\") and not 'Adam' in name:", "\n", "            ", "var_values", "[", "name", "]", "=", "np", ".", "zeros", "(", "shape", ")", "\n", "", "", "for", "(", "name", ",", "shape", ")", "in", "var_part", ":", "\n", "        ", "if", "name", ".", "endswith", "(", "\"power\"", ")", ":", "\n", "            ", "beta_power", "=", "re", ".", "search", "(", "r'(beta[1-2]_power)'", ",", "name", ")", ".", "group", "(", ")", "\n", "tensor", "=", "reader_part", ".", "get_tensor", "(", "name", ")", "\n", "var_values_part_for_beta", "[", "beta_power", "]", "=", "tensor", "\n", "", "else", ":", "\n", "            ", "var_values", "[", "name", "]", "=", "np", ".", "zeros", "(", "shape", ")", "\n", "var_values_part", "[", "name", "]", "=", "np", ".", "zeros", "(", "shape", ")", "\n", "\n", "", "", "for", "name", "in", "var_values", ":", "\n", "        ", "if", "name", ".", "endswith", "(", "\"power\"", ")", ":", "\n", "            ", "beta_power", "=", "re", ".", "search", "(", "r'(beta[1-2]_power)'", ",", "name", ")", ".", "group", "(", ")", "\n", "tensor", "=", "var_values_part_for_beta", "[", "beta_power", "]", "\n", "var_dtypes", "[", "name", "]", "=", "tensor", ".", "dtype", "\n", "var_values", "[", "name", "]", "+=", "tensor", "\n", "print", "(", "name", ",", "\"is new power\"", ")", "\n", "# var_values_part has not power now ", "\n", "", "if", "name", "in", "var_values_part", ":", "\n", "            ", "tensor", "=", "reader_part", ".", "get_tensor", "(", "name", ")", "\n", "var_dtypes", "[", "name", "]", "=", "tensor", ".", "dtype", "\n", "var_values", "[", "name", "]", "+=", "tensor", "\n", "print", "(", "name", "+", "' in part'", ")", "\n", "", "else", ":", "\n", "            ", "tensor", "=", "reader", ".", "get_tensor", "(", "name", ")", "\n", "var_dtypes", "[", "name", "]", "=", "tensor", ".", "dtype", "\n", "var_values", "[", "name", "]", "+=", "tensor", "\n", "print", "(", "name", "+", "' is new'", ")", "\n", "", "", "tf", ".", "logging", ".", "info", "(", "\"Read from %s and %s\"", ",", "FLAGS", ".", "model", ",", "FLAGS", ".", "part", ")", "\n", "\n", "tf_vars", "=", "[", "\n", "tf", ".", "get_variable", "(", "name", ",", "shape", "=", "var_values", "[", "name", "]", ".", "shape", ",", "\n", "dtype", "=", "var_dtypes", "[", "name", "]", ")", "for", "name", "in", "var_values", "\n", "]", "\n", "placeholders", "=", "[", "tf", ".", "placeholder", "(", "v", ".", "dtype", ",", "shape", "=", "v", ".", "shape", ")", "for", "v", "in", "tf_vars", "]", "\n", "assign_ops", "=", "[", "tf", ".", "assign", "(", "v", ",", "p", ")", "for", "(", "v", ",", "p", ")", "in", "zip", "(", "tf_vars", ",", "placeholders", ")", "]", "\n", "global_step", "=", "tf", ".", "Variable", "(", "0", ",", "name", "=", "\"global_step\"", ",", "trainable", "=", "False", ",", "\n", "dtype", "=", "tf", ".", "int64", ")", "\n", "saver", "=", "tf", ".", "train", ".", "Saver", "(", "tf", ".", "global_variables", "(", ")", ")", "\n", "\n", "with", "tf", ".", "Session", "(", ")", "as", "sess", ":", "\n", "        ", "sess", ".", "run", "(", "tf", ".", "global_variables_initializer", "(", ")", ")", "\n", "for", "p", ",", "assign_op", ",", "(", "name", ",", "value", ")", "in", "zip", "(", "placeholders", ",", "assign_ops", ",", "\n", "var_values", ".", "items", "(", ")", ")", ":", "\n", "            ", "sess", ".", "run", "(", "assign_op", ",", "{", "p", ":", "value", "}", ")", "\n", "", "saved_name", "=", "os", ".", "path", ".", "join", "(", "FLAGS", ".", "output", ",", "\"new\"", ")", "\n", "saver", ".", "save", "(", "sess", ",", "saved_name", ",", "global_step", "=", "global_step", ")", "\n", "\n", "", "tf", ".", "logging", ".", "info", "(", "\"Averaged checkpoints saved in %s\"", ",", "saved_name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.scripts.input_converter.load_vocab": [[16, 26], ["tensorflow.gfile.Open", "line.strip"], "function", ["None"], ["def", "load_vocab", "(", "filename", ")", ":", "\n", "    ", "with", "tf", ".", "gfile", ".", "Open", "(", "filename", ")", "as", "fd", ":", "\n", "        ", "count", "=", "0", "\n", "vocab", "=", "{", "}", "\n", "for", "line", "in", "fd", ":", "\n", "            ", "word", "=", "line", ".", "strip", "(", ")", "\n", "vocab", "[", "word", "]", "=", "count", "\n", "count", "+=", "1", "\n", "\n", "", "", "return", "vocab", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.scripts.input_converter.to_example": [[28, 50], ["six.iteritems", "tensorflow.train.Example", "isinstance", "ValueError", "tensorflow.train.Int64List", "tensorflow.train.Feature", "isinstance", "tensorflow.train.Features", "str", "tensorflow.train.FloatList", "tensorflow.train.Feature", "isinstance", "tensorflow.train.BytesList", "tensorflow.train.Feature", "ValueError", "str", "str", "type"], "function", ["None"], ["", "def", "to_example", "(", "dictionary", ")", ":", "\n", "    ", "\"\"\" Convert python dictionary to tf.train.Example \"\"\"", "\n", "features", "=", "{", "}", "\n", "\n", "for", "(", "k", ",", "v", ")", "in", "six", ".", "iteritems", "(", "dictionary", ")", ":", "\n", "        ", "if", "not", "v", ":", "\n", "            ", "raise", "ValueError", "(", "\"Empty generated field: %s\"", ",", "str", "(", "(", "k", ",", "v", ")", ")", ")", "\n", "\n", "", "if", "isinstance", "(", "v", "[", "0", "]", ",", "six", ".", "integer_types", ")", ":", "\n", "            ", "int64_list", "=", "tf", ".", "train", ".", "Int64List", "(", "value", "=", "v", ")", "\n", "features", "[", "k", "]", "=", "tf", ".", "train", ".", "Feature", "(", "int64_list", "=", "int64_list", ")", "\n", "", "elif", "isinstance", "(", "v", "[", "0", "]", ",", "float", ")", ":", "\n", "            ", "float_list", "=", "tf", ".", "train", ".", "FloatList", "(", "value", "=", "v", ")", "\n", "features", "[", "k", "]", "=", "tf", ".", "train", ".", "Feature", "(", "float_list", "=", "float_list", ")", "\n", "", "elif", "isinstance", "(", "v", "[", "0", "]", ",", "six", ".", "string_types", ")", ":", "\n", "            ", "bytes_list", "=", "tf", ".", "train", ".", "BytesList", "(", "value", "=", "v", ")", "\n", "features", "[", "k", "]", "=", "tf", ".", "train", ".", "Feature", "(", "bytes_list", "=", "bytes_list", ")", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"Value is neither an int nor a float; \"", "\n", "\"v: %s type: %s\"", "%", "(", "str", "(", "v", "[", "0", "]", ")", ",", "str", "(", "type", "(", "v", "[", "0", "]", ")", ")", ")", ")", "\n", "\n", "", "", "return", "tf", ".", "train", ".", "Example", "(", "features", "=", "tf", ".", "train", ".", "Features", "(", "feature", "=", "features", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.scripts.input_converter.write_records": [[52, 62], ["tensorflow.python_io.TFRecordWriter", "enumerate", "tf.python_io.TFRecordWriter.close", "tf.python_io.TFRecordWriter.write", "tensorflow.logging.info"], "function", ["None"], ["", "def", "write_records", "(", "records", ",", "out_filename", ")", ":", "\n", "    ", "\"\"\" Write to TensorFlow record \"\"\"", "\n", "writer", "=", "tf", ".", "python_io", ".", "TFRecordWriter", "(", "out_filename", ")", "\n", "\n", "for", "count", ",", "record", "in", "enumerate", "(", "records", ")", ":", "\n", "        ", "writer", ".", "write", "(", "record", ")", "\n", "if", "count", "%", "10000", "==", "0", ":", "\n", "            ", "tf", ".", "logging", ".", "info", "(", "\"write: %d\"", ",", "count", ")", "\n", "\n", "", "", "writer", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.scripts.input_converter.convert_to_record": [[64, 111], ["xrange", "tensorflow.gfile.Open", "os.path.join", "output_files.append", "writers.append", "random.shuffle", "input_converter.to_example", "writers[].write", "writer.close", "tensorflow.gfile.Open", "zip", "tensorflow.python_io.TFRecordWriter", "to_example.SerializeToString", "sline.strip().split.strip().split", "tline.strip().split.strip().split", "records.append", "sline.strip().split.strip", "tline.strip().split.strip", "len", "len"], "function", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.scripts.input_converter.to_example"], ["", "def", "convert_to_record", "(", "inputs", ",", "vocab", ",", "output_name", ",", "output_dir", ",", "num_shards", ",", "\n", "shuffle", "=", "False", ")", ":", "\n", "    ", "\"\"\" Convert plain parallel text to TensorFlow record \"\"\"", "\n", "source", ",", "target", "=", "inputs", "\n", "svocab", ",", "tvocab", "=", "vocab", "\n", "records", "=", "[", "]", "\n", "\n", "with", "tf", ".", "gfile", ".", "Open", "(", "source", ")", "as", "src", ":", "\n", "        ", "with", "tf", ".", "gfile", ".", "Open", "(", "target", ")", "as", "tgt", ":", "\n", "            ", "for", "sline", ",", "tline", "in", "zip", "(", "src", ",", "tgt", ")", ":", "\n", "                ", "sline", "=", "sline", ".", "strip", "(", ")", ".", "split", "(", ")", "\n", "sline", "=", "[", "svocab", "[", "item", "]", "if", "item", "in", "svocab", "else", "svocab", "[", "FLAGS", ".", "unk", "]", "\n", "for", "item", "in", "sline", "]", "+", "[", "svocab", "[", "FLAGS", ".", "eos", "]", "]", "\n", "tline", "=", "tline", ".", "strip", "(", ")", ".", "split", "(", ")", "\n", "tline", "=", "[", "tvocab", "[", "item", "]", "if", "item", "in", "tvocab", "else", "tvocab", "[", "FLAGS", ".", "unk", "]", "\n", "for", "item", "in", "tline", "]", "+", "[", "tvocab", "[", "FLAGS", ".", "eos", "]", "]", "\n", "\n", "feature", "=", "{", "\n", "\"source\"", ":", "sline", ",", "\n", "\"target\"", ":", "tline", ",", "\n", "\"source_length\"", ":", "[", "len", "(", "sline", ")", "]", ",", "\n", "\"target_length\"", ":", "[", "len", "(", "tline", ")", "]", "\n", "}", "\n", "records", ".", "append", "(", "feature", ")", "\n", "\n", "", "", "", "output_files", "=", "[", "]", "\n", "writers", "=", "[", "]", "\n", "\n", "for", "shard", "in", "xrange", "(", "num_shards", ")", ":", "\n", "        ", "output_filename", "=", "\"%s-%.5d-of-%.5d\"", "%", "(", "output_name", ",", "shard", ",", "num_shards", ")", "\n", "output_file", "=", "os", ".", "path", ".", "join", "(", "output_dir", ",", "output_filename", ")", "\n", "output_files", ".", "append", "(", "output_file", ")", "\n", "writers", ".", "append", "(", "tf", ".", "python_io", ".", "TFRecordWriter", "(", "output_file", ")", ")", "\n", "\n", "", "counter", ",", "shard", "=", "0", ",", "0", "\n", "\n", "if", "shuffle", ":", "\n", "        ", "random", ".", "shuffle", "(", "records", ")", "\n", "\n", "", "for", "record", "in", "records", ":", "\n", "        ", "counter", "+=", "1", "\n", "example", "=", "to_example", "(", "record", ")", "\n", "writers", "[", "shard", "]", ".", "write", "(", "example", ".", "SerializeToString", "(", ")", ")", "\n", "shard", "=", "(", "shard", "+", "1", ")", "%", "num_shards", "\n", "\n", "", "for", "writer", "in", "writers", ":", "\n", "        ", "writer", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.scripts.input_converter.parse_args": [[113, 136], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args"], "function", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.scripts.build_vocab.parse_args"], ["", "", "def", "parse_args", "(", ")", ":", "\n", "    ", "msg", "=", "\"convert inputs to tf.Record format\"", "\n", "usage", "=", "\"input_converter.py [<args>] [-h | --help]\"", "\n", "parser", "=", "argparse", ".", "ArgumentParser", "(", "description", "=", "msg", ",", "usage", "=", "usage", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--input\"", ",", "required", "=", "True", ",", "type", "=", "str", ",", "nargs", "=", "2", ",", "\n", "help", "=", "\"Path of input file\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--output_name\"", ",", "required", "=", "True", ",", "type", "=", "str", ",", "\n", "help", "=", "\"Output name\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--output_dir\"", ",", "required", "=", "True", ",", "type", "=", "str", ",", "\n", "help", "=", "\"Output directory\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--vocab\"", ",", "nargs", "=", "2", ",", "required", "=", "True", ",", "type", "=", "str", ",", "\n", "help", "=", "\"Path of vocabulary\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--num_shards\"", ",", "default", "=", "100", ",", "type", "=", "int", ",", "\n", "help", "=", "\"Number of output shards\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--shuffle\"", ",", "action", "=", "\"store_true\"", ",", "\n", "help", "=", "\"Shuffle inputs\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--unk\"", ",", "default", "=", "\"<unk>\"", ",", "type", "=", "str", ",", "\n", "help", "=", "\"Unknown word symbol\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--eos\"", ",", "default", "=", "\"<eos>\"", ",", "type", "=", "str", ",", "\n", "help", "=", "\"End of sentence symbol\"", ")", "\n", "\n", "return", "parser", ".", "parse_args", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.scripts.input_converter.main": [[138, 145], ["input_converter.load_vocab", "input_converter.load_vocab", "input_converter.convert_to_record"], "function", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.scripts.input_converter.load_vocab", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.scripts.input_converter.load_vocab", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.scripts.input_converter.convert_to_record"], ["", "def", "main", "(", "_", ")", ":", "\n", "    ", "svocab", "=", "load_vocab", "(", "FLAGS", ".", "vocab", "[", "0", "]", ")", "\n", "tvocab", "=", "load_vocab", "(", "FLAGS", ".", "vocab", "[", "1", "]", ")", "\n", "\n", "# convert data", "\n", "convert_to_record", "(", "FLAGS", ".", "input", ",", "[", "svocab", ",", "tvocab", "]", ",", "FLAGS", ".", "output_name", ",", "\n", "FLAGS", ".", "output_dir", ",", "FLAGS", ".", "num_shards", ",", "FLAGS", ".", "shuffle", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.scripts.build_vocab.count_words": [[13, 25], ["collections.Counter", "sorted", "list", "open", "collections.Counter.items", "zip", "line.strip().split", "collections.Counter.update", "line.strip"], "function", ["None"], ["def", "count_words", "(", "filename", ")", ":", "\n", "    ", "counter", "=", "collections", ".", "Counter", "(", ")", "\n", "\n", "with", "open", "(", "filename", ",", "\"r\"", ")", "as", "fd", ":", "\n", "        ", "for", "line", "in", "fd", ":", "\n", "            ", "words", "=", "line", ".", "strip", "(", ")", ".", "split", "(", ")", "\n", "counter", ".", "update", "(", "words", ")", "\n", "\n", "", "", "count_pairs", "=", "sorted", "(", "counter", ".", "items", "(", ")", ",", "key", "=", "lambda", "x", ":", "(", "-", "x", "[", "1", "]", ",", "x", "[", "0", "]", ")", ")", "\n", "words", ",", "counts", "=", "list", "(", "zip", "(", "*", "count_pairs", ")", ")", "\n", "\n", "return", "words", ",", "counts", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.scripts.build_vocab.control_symbols": [[27, 32], ["string.strip().split", "string.strip"], "function", ["None"], ["", "def", "control_symbols", "(", "string", ")", ":", "\n", "    ", "if", "not", "string", ":", "\n", "        ", "return", "[", "]", "\n", "", "else", ":", "\n", "        ", "return", "string", ".", "strip", "(", ")", ".", "split", "(", "\",\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.scripts.build_vocab.save_vocab": [[34, 44], ["sorted", "list", "vocab.items", "zip", "open", "name.split", "f.write"], "function", ["None"], ["", "", "def", "save_vocab", "(", "name", ",", "vocab", ")", ":", "\n", "    ", "if", "name", ".", "split", "(", "\".\"", ")", "[", "-", "1", "]", "!=", "\"txt\"", ":", "\n", "        ", "name", "=", "name", "+", "\".txt\"", "\n", "\n", "", "pairs", "=", "sorted", "(", "vocab", ".", "items", "(", ")", ",", "key", "=", "lambda", "x", ":", "(", "x", "[", "1", "]", ",", "x", "[", "0", "]", ")", ")", "\n", "words", ",", "ids", "=", "list", "(", "zip", "(", "*", "pairs", ")", ")", "\n", "\n", "with", "open", "(", "name", ",", "\"w\"", ")", "as", "f", ":", "\n", "        ", "for", "word", "in", "words", ":", "\n", "            ", "f", ".", "write", "(", "word", "+", "\"\\n\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.scripts.build_vocab.parse_args": [[46, 58], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args"], "function", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.scripts.build_vocab.parse_args"], ["", "", "", "def", "parse_args", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "description", "=", "\"Create vocabulary\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"corpus\"", ",", "help", "=", "\"input corpus\"", ")", "\n", "parser", ".", "add_argument", "(", "\"output\"", ",", "default", "=", "\"vocab.txt\"", ",", "\n", "help", "=", "\"Output vocabulary name\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--limit\"", ",", "default", "=", "0", ",", "type", "=", "int", ",", "help", "=", "\"Vocabulary size\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--control\"", ",", "type", "=", "str", ",", "default", "=", "\"<pad>,<eos>,<unk>\"", ",", "\n", "help", "=", "\"Add control symbols to vocabulary. \"", "\n", "\"Control symbols are separated by comma.\"", ")", "\n", "\n", "return", "parser", ".", "parse_args", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.scripts.build_vocab.main": [[60, 87], ["build_vocab.count_words", "build_vocab.control_symbols", "zip", "build_vocab.save_vocab", "print", "print", "print", "len", "len", "print", "sum", "len", "len", "sum"], "function", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.scripts.build_vocab.count_words", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.scripts.build_vocab.control_symbols", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.scripts.build_vocab.save_vocab"], ["", "def", "main", "(", "args", ")", ":", "\n", "    ", "vocab", "=", "{", "}", "\n", "limit", "=", "args", ".", "limit", "\n", "count", "=", "0", "\n", "\n", "words", ",", "counts", "=", "count_words", "(", "args", ".", "corpus", ")", "\n", "ctrl_symbols", "=", "control_symbols", "(", "args", ".", "control", ")", "\n", "\n", "for", "sym", "in", "ctrl_symbols", ":", "\n", "        ", "vocab", "[", "sym", "]", "=", "len", "(", "vocab", ")", "\n", "\n", "", "for", "word", ",", "freq", "in", "zip", "(", "words", ",", "counts", ")", ":", "\n", "        ", "if", "limit", "and", "len", "(", "vocab", ")", ">=", "limit", ":", "\n", "            ", "break", "\n", "\n", "", "if", "word", "in", "vocab", ":", "\n", "            ", "print", "(", "\"Warning: found duplicate token %s, ignored\"", "%", "word", ")", "\n", "continue", "\n", "\n", "", "vocab", "[", "word", "]", "=", "len", "(", "vocab", ")", "\n", "count", "+=", "freq", "\n", "\n", "", "save_vocab", "(", "args", ".", "output", ",", "vocab", ")", "\n", "\n", "print", "(", "\"Total words: %d\"", "%", "sum", "(", "counts", ")", ")", "\n", "print", "(", "\"Unique words: %d\"", "%", "len", "(", "words", ")", ")", "\n", "print", "(", "\"Vocabulary coverage: %4.2f%%\"", "%", "(", "100.0", "*", "count", "/", "sum", "(", "counts", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.scripts.visualize.parse_numpy": [[10, 15], ["re.sub.replace().replace().replace", "re.sub", "numpy.fromstring", "re.sub.replace().replace", "re.sub.replace"], "function", ["None"], ["def", "parse_numpy", "(", "string", ")", ":", "\n", "    ", "string", "=", "string", ".", "replace", "(", "'['", ",", "' '", ")", ".", "replace", "(", "']'", ",", "' '", ")", ".", "replace", "(", "','", ",", "' '", ")", "\n", "string", "=", "re", ".", "sub", "(", "' +'", ",", "' '", ",", "string", ")", "\n", "result", "=", "numpy", ".", "fromstring", "(", "string", ",", "sep", "=", "' '", ")", "\n", "return", "result", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.rnnsearch.RNNsearch.__init__": [[338, 340], ["thumt.models.model.NMTModel.__init__"], "methods", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.transformer_lrp.TransformerLRP.__init__"], ["    ", "def", "__init__", "(", "self", ",", "params", ",", "scope", "=", "\"rnnsearch\"", ")", ":", "\n", "        ", "super", "(", "RNNsearch", ",", "self", ")", ".", "__init__", "(", "params", "=", "params", ",", "scope", "=", "scope", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.rnnsearch.RNNsearch.get_training_func": [[341, 355], ["tensorflow.variable_scope", "rnnsearch.model_graph"], "methods", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.transformer_lrp.model_graph"], ["", "def", "get_training_func", "(", "self", ",", "initializer", ",", "regularizer", "=", "None", ",", "dtype", "=", "None", ")", ":", "\n", "        ", "def", "training_fn", "(", "features", ",", "params", "=", "None", ",", "reuse", "=", "None", ")", ":", "\n", "            ", "if", "params", "is", "None", ":", "\n", "                ", "params", "=", "self", ".", "parameters", "\n", "\n", "", "custom_getter", "=", "utils", ".", "custom_getter", "if", "dtype", "else", "None", "\n", "\n", "with", "tf", ".", "variable_scope", "(", "self", ".", "_scope", ",", "initializer", "=", "initializer", ",", "\n", "regularizer", "=", "regularizer", ",", "reuse", "=", "reuse", ",", "\n", "custom_getter", "=", "custom_getter", ",", "dtype", "=", "dtype", ")", ":", "\n", "                ", "loss", "=", "model_graph", "(", "features", ",", "\"train\"", ",", "params", ")", "\n", "return", "loss", "\n", "\n", "", "", "return", "training_fn", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.rnnsearch.RNNsearch.get_evaluation_func": [[356, 373], ["copy.copy", "copy.copy", "tensorflow.variable_scope", "rnnsearch.model_graph"], "methods", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.transformer_lrp.model_graph"], ["", "def", "get_evaluation_func", "(", "self", ")", ":", "\n", "        ", "def", "evaluation_fn", "(", "features", ",", "params", "=", "None", ")", ":", "\n", "            ", "if", "params", "is", "None", ":", "\n", "                ", "params", "=", "copy", ".", "copy", "(", "self", ".", "parameters", ")", "\n", "", "else", ":", "\n", "                ", "params", "=", "copy", ".", "copy", "(", "params", ")", "\n", "\n", "", "params", ".", "dropout", "=", "0.0", "\n", "params", ".", "use_variational_dropout", "=", "False", "\n", "params", ".", "label_smoothing", "=", "0.0", "\n", "\n", "with", "tf", ".", "variable_scope", "(", "self", ".", "_scope", ")", ":", "\n", "                ", "score", "=", "model_graph", "(", "features", ",", "\"eval\"", ",", "params", ")", "\n", "\n", "", "return", "score", "\n", "\n", "", "return", "evaluation_fn", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.rnnsearch.RNNsearch.get_inference_func": [[374, 391], ["copy.copy", "copy.copy", "tensorflow.variable_scope", "rnnsearch.model_graph"], "methods", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.transformer_lrp.model_graph"], ["", "def", "get_inference_func", "(", "self", ")", ":", "\n", "        ", "def", "inference_fn", "(", "features", ",", "params", "=", "None", ")", ":", "\n", "            ", "if", "params", "is", "None", ":", "\n", "                ", "params", "=", "copy", ".", "copy", "(", "self", ".", "parameters", ")", "\n", "", "else", ":", "\n", "                ", "params", "=", "copy", ".", "copy", "(", "params", ")", "\n", "\n", "", "params", ".", "dropout", "=", "0.0", "\n", "params", ".", "use_variational_dropout", "=", "False", "\n", "params", ".", "label_smoothing", "=", "0.0", "\n", "\n", "with", "tf", ".", "variable_scope", "(", "self", ".", "_scope", ")", ":", "\n", "                ", "log_prob", "=", "model_graph", "(", "features", ",", "\"infer\"", ",", "params", ")", "\n", "\n", "", "return", "log_prob", "\n", "\n", "", "return", "inference_fn", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.rnnsearch.RNNsearch.get_name": [[392, 395], ["None"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "get_name", "(", ")", ":", "\n", "        ", "return", "\"rnnsearch\"", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.rnnsearch.RNNsearch.get_parameters": [[396, 421], ["tensorflow.contrib.training.HParams"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "get_parameters", "(", ")", ":", "\n", "        ", "params", "=", "tf", ".", "contrib", ".", "training", ".", "HParams", "(", "\n", "# vocabulary", "\n", "pad", "=", "\"<pad>\"", ",", "\n", "unk", "=", "\"<unk>\"", ",", "\n", "eos", "=", "\"<eos>\"", ",", "\n", "bos", "=", "\"<eos>\"", ",", "\n", "append_eos", "=", "False", ",", "\n", "# model", "\n", "rnn_cell", "=", "\"LegacyGRUCell\"", ",", "\n", "embedding_size", "=", "620", ",", "\n", "hidden_size", "=", "1000", ",", "\n", "maxnum", "=", "2", ",", "\n", "# regularization", "\n", "dropout", "=", "0.2", ",", "\n", "use_variational_dropout", "=", "False", ",", "\n", "label_smoothing", "=", "0.1", ",", "\n", "constant_batch_size", "=", "True", ",", "\n", "batch_size", "=", "128", ",", "\n", "max_length", "=", "60", ",", "\n", "clip_grad_norm", "=", "5.0", "\n", ")", "\n", "\n", "return", "params", "\n", "", "", ""]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.rnnsearch._copy_through": [[18, 21], ["tensorflow.where"], "function", ["None"], ["def", "_copy_through", "(", "time", ",", "length", ",", "output", ",", "new_output", ")", ":", "\n", "    ", "copy_cond", "=", "(", "time", ">=", "length", ")", "\n", "return", "tf", ".", "where", "(", "copy_cond", ",", "output", ",", "new_output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.rnnsearch._gru_encoder": [[23, 66], ["tensorflow.zeros", "tensorflow.TensorArray", "tensorflow.TensorArray", "input_ta.unstack.unstack", "tensorflow.constant", "tensorflow.while_loop", "output_final_ta.stack", "tf.transpose.set_shape", "tensorflow.transpose", "tensorflow.shape", "tensorflow.shape", "cell.zero_state", "tensorflow.transpose", "input_ta.unstack.read", "cell", "rnnsearch._copy_through", "rnnsearch._copy_through", "out_ta.write.write"], "function", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.rnnsearch_lrp._copy_through", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.rnnsearch_lrp._copy_through"], ["", "def", "_gru_encoder", "(", "cell", ",", "inputs", ",", "sequence_length", ",", "initial_state", ",", "dtype", "=", "None", ")", ":", "\n", "# Assume that the underlying cell is GRUCell-like", "\n", "    ", "output_size", "=", "cell", ".", "output_size", "\n", "dtype", "=", "dtype", "or", "inputs", ".", "dtype", "\n", "\n", "batch", "=", "tf", ".", "shape", "(", "inputs", ")", "[", "0", "]", "\n", "time_steps", "=", "tf", ".", "shape", "(", "inputs", ")", "[", "1", "]", "\n", "\n", "zero_output", "=", "tf", ".", "zeros", "(", "[", "batch", ",", "output_size", "]", ",", "dtype", ")", "\n", "\n", "if", "initial_state", "is", "None", ":", "\n", "        ", "initial_state", "=", "cell", ".", "zero_state", "(", "batch", ",", "dtype", ")", "\n", "\n", "", "input_ta", "=", "tf", ".", "TensorArray", "(", "dtype", ",", "time_steps", ",", "\n", "tensor_array_name", "=", "\"input_array\"", ")", "\n", "output_ta", "=", "tf", ".", "TensorArray", "(", "dtype", ",", "time_steps", ",", "\n", "tensor_array_name", "=", "\"output_array\"", ")", "\n", "input_ta", "=", "input_ta", ".", "unstack", "(", "tf", ".", "transpose", "(", "inputs", ",", "[", "1", ",", "0", ",", "2", "]", ")", ")", "\n", "\n", "def", "loop_func", "(", "t", ",", "out_ta", ",", "state", ")", ":", "\n", "        ", "inp_t", "=", "input_ta", ".", "read", "(", "t", ")", "\n", "cell_output", ",", "new_state", "=", "cell", "(", "inp_t", ",", "state", ")", "\n", "cell_output", "=", "_copy_through", "(", "t", ",", "sequence_length", ",", "zero_output", ",", "\n", "cell_output", ")", "\n", "new_state", "=", "_copy_through", "(", "t", ",", "sequence_length", ",", "state", ",", "new_state", ")", "\n", "out_ta", "=", "out_ta", ".", "write", "(", "t", ",", "cell_output", ")", "\n", "return", "t", "+", "1", ",", "out_ta", ",", "new_state", "\n", "\n", "", "time", "=", "tf", ".", "constant", "(", "0", ",", "dtype", "=", "tf", ".", "int32", ",", "name", "=", "\"time\"", ")", "\n", "loop_vars", "=", "(", "time", ",", "output_ta", ",", "initial_state", ")", "\n", "\n", "outputs", "=", "tf", ".", "while_loop", "(", "lambda", "t", ",", "*", "_", ":", "t", "<", "time_steps", ",", "loop_func", ",", "\n", "loop_vars", ",", "parallel_iterations", "=", "32", ",", "\n", "swap_memory", "=", "True", ")", "\n", "\n", "output_final_ta", "=", "outputs", "[", "1", "]", "\n", "final_state", "=", "outputs", "[", "2", "]", "\n", "\n", "all_output", "=", "output_final_ta", ".", "stack", "(", ")", "\n", "all_output", ".", "set_shape", "(", "[", "None", ",", "None", ",", "output_size", "]", ")", "\n", "all_output", "=", "tf", ".", "transpose", "(", "all_output", ",", "[", "1", ",", "0", ",", "2", "]", ")", "\n", "\n", "return", "all_output", ",", "final_state", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.rnnsearch._encoder": [[68, 101], ["tensorflow.variable_scope", "tensorflow.reverse_sequence", "tensorflow.variable_scope", "rnnsearch._gru_encoder", "tensorflow.variable_scope", "rnnsearch._gru_encoder", "tensorflow.reverse_sequence", "tensorflow.concat"], "function", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.rnnsearch_lrp._gru_encoder", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.rnnsearch_lrp._gru_encoder"], ["", "def", "_encoder", "(", "cell_fw", ",", "cell_bw", ",", "inputs", ",", "sequence_length", ",", "dtype", "=", "None", ",", "\n", "scope", "=", "None", ")", ":", "\n", "    ", "with", "tf", ".", "variable_scope", "(", "scope", "or", "\"encoder\"", ",", "\n", "values", "=", "[", "inputs", ",", "sequence_length", "]", ")", ":", "\n", "        ", "inputs_fw", "=", "inputs", "\n", "inputs_bw", "=", "tf", ".", "reverse_sequence", "(", "inputs", ",", "sequence_length", ",", "\n", "batch_axis", "=", "0", ",", "seq_axis", "=", "1", ")", "\n", "\n", "with", "tf", ".", "variable_scope", "(", "\"forward\"", ")", ":", "\n", "            ", "output_fw", ",", "state_fw", "=", "_gru_encoder", "(", "cell_fw", ",", "inputs_fw", ",", "\n", "sequence_length", ",", "None", ",", "\n", "dtype", "=", "dtype", ")", "\n", "\n", "", "with", "tf", ".", "variable_scope", "(", "\"backward\"", ")", ":", "\n", "            ", "output_bw", ",", "state_bw", "=", "_gru_encoder", "(", "cell_bw", ",", "inputs_bw", ",", "\n", "sequence_length", ",", "None", ",", "\n", "dtype", "=", "dtype", ")", "\n", "output_bw", "=", "tf", ".", "reverse_sequence", "(", "output_bw", ",", "sequence_length", ",", "\n", "batch_axis", "=", "0", ",", "seq_axis", "=", "1", ")", "\n", "\n", "", "results", "=", "{", "\n", "\"annotation\"", ":", "tf", ".", "concat", "(", "[", "output_fw", ",", "output_bw", "]", ",", "axis", "=", "2", ")", ",", "\n", "\"outputs\"", ":", "{", "\n", "\"forward\"", ":", "output_fw", ",", "\n", "\"backward\"", ":", "output_bw", "\n", "}", ",", "\n", "\"final_states\"", ":", "{", "\n", "\"forward\"", ":", "state_fw", ",", "\n", "\"backward\"", ":", "state_bw", "\n", "}", "\n", "}", "\n", "\n", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.rnnsearch._decoder": [[103, 185], ["tensorflow.zeros", "tensorflow.zeros", "tensorflow.shape", "tensorflow.shape", "tensorflow.variable_scope", "tensorflow.transpose", "tensorflow.sequence_mask", "thumt.attention.attention_bias", "tensorflow.squeeze", "thumt.attention.attention", "tensorflow.TensorArray", "tensorflow.TensorArray", "tensorflow.TensorArray", "tensorflow.TensorArray", "input_ta.unstack.unstack", "thumt.nn.linear", "tensorflow.tanh", "tensorflow.constant", "tensorflow.while_loop", "output_final_ta.stack", "tf.transpose.set_shape", "tensorflow.transpose", "value_final_ta.stack", "tf.transpose.set_shape", "tensorflow.transpose", "input_ta.unstack.read", "thumt.attention.attention", "cell", "rnnsearch._copy_through", "rnnsearch._copy_through", "rnnsearch._copy_through", "out_ta.write.write", "att_ta.write.write", "val_ta.write.write", "tensorflow.identity", "tensorflow.shape"], "function", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.layers.attention.attention_bias", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.layers.attention.attention", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.layers.nn.linear", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.layers.attention.attention", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.rnnsearch_lrp._copy_through", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.rnnsearch_lrp._copy_through", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.rnnsearch_lrp._copy_through"], ["", "", "def", "_decoder", "(", "cell", ",", "inputs", ",", "memory", ",", "sequence_length", ",", "initial_state", ",", "dtype", "=", "None", ",", "\n", "scope", "=", "None", ")", ":", "\n", "# Assume that the underlying cell is GRUCell-like", "\n", "    ", "batch", "=", "tf", ".", "shape", "(", "inputs", ")", "[", "0", "]", "\n", "time_steps", "=", "tf", ".", "shape", "(", "inputs", ")", "[", "1", "]", "\n", "dtype", "=", "dtype", "or", "inputs", ".", "dtype", "\n", "output_size", "=", "cell", ".", "output_size", "\n", "zero_output", "=", "tf", ".", "zeros", "(", "[", "batch", ",", "output_size", "]", ",", "dtype", ")", "\n", "zero_value", "=", "tf", ".", "zeros", "(", "[", "batch", ",", "memory", ".", "shape", "[", "-", "1", "]", ".", "value", "]", ",", "dtype", ")", "\n", "\n", "with", "tf", ".", "variable_scope", "(", "scope", "or", "\"decoder\"", ",", "dtype", "=", "dtype", ")", ":", "\n", "        ", "inputs", "=", "tf", ".", "transpose", "(", "inputs", ",", "[", "1", ",", "0", ",", "2", "]", ")", "\n", "mem_mask", "=", "tf", ".", "sequence_mask", "(", "sequence_length", "[", "\"source\"", "]", ",", "\n", "maxlen", "=", "tf", ".", "shape", "(", "memory", ")", "[", "1", "]", ",", "\n", "dtype", "=", "dtype", ")", "\n", "bias", "=", "layers", ".", "attention", ".", "attention_bias", "(", "mem_mask", ",", "\"masking\"", ",", "\n", "dtype", "=", "dtype", ")", "\n", "bias", "=", "tf", ".", "squeeze", "(", "bias", ",", "axis", "=", "[", "1", ",", "2", "]", ")", "\n", "cache", "=", "layers", ".", "attention", ".", "attention", "(", "None", ",", "memory", ",", "None", ",", "output_size", ")", "\n", "\n", "input_ta", "=", "tf", ".", "TensorArray", "(", "dtype", ",", "time_steps", ",", "\n", "tensor_array_name", "=", "\"input_array\"", ")", "\n", "output_ta", "=", "tf", ".", "TensorArray", "(", "dtype", ",", "time_steps", ",", "\n", "tensor_array_name", "=", "\"output_array\"", ")", "\n", "value_ta", "=", "tf", ".", "TensorArray", "(", "dtype", ",", "time_steps", ",", "\n", "tensor_array_name", "=", "\"value_array\"", ")", "\n", "alpha_ta", "=", "tf", ".", "TensorArray", "(", "dtype", ",", "time_steps", ",", "\n", "tensor_array_name", "=", "\"alpha_array\"", ")", "\n", "input_ta", "=", "input_ta", ".", "unstack", "(", "inputs", ")", "\n", "initial_state", "=", "layers", ".", "nn", ".", "linear", "(", "initial_state", ",", "output_size", ",", "True", ",", "\n", "False", ",", "scope", "=", "\"s_transform\"", ")", "\n", "initial_state", "=", "tf", ".", "tanh", "(", "initial_state", ")", "\n", "\n", "def", "loop_func", "(", "t", ",", "out_ta", ",", "att_ta", ",", "val_ta", ",", "state", ",", "cache_key", ")", ":", "\n", "            ", "inp_t", "=", "input_ta", ".", "read", "(", "t", ")", "\n", "results", "=", "layers", ".", "attention", ".", "attention", "(", "state", ",", "memory", ",", "bias", ",", "\n", "output_size", ",", "\n", "cache", "=", "{", "\"key\"", ":", "cache_key", "}", ")", "\n", "alpha", "=", "results", "[", "\"weight\"", "]", "\n", "context", "=", "results", "[", "\"value\"", "]", "\n", "cell_input", "=", "[", "inp_t", ",", "context", "]", "\n", "cell_output", ",", "new_state", "=", "cell", "(", "cell_input", ",", "state", ")", "\n", "cell_output", "=", "_copy_through", "(", "t", ",", "sequence_length", "[", "\"target\"", "]", ",", "\n", "zero_output", ",", "cell_output", ")", "\n", "new_state", "=", "_copy_through", "(", "t", ",", "sequence_length", "[", "\"target\"", "]", ",", "state", ",", "\n", "new_state", ")", "\n", "new_value", "=", "_copy_through", "(", "t", ",", "sequence_length", "[", "\"target\"", "]", ",", "zero_value", ",", "\n", "context", ")", "\n", "\n", "out_ta", "=", "out_ta", ".", "write", "(", "t", ",", "cell_output", ")", "\n", "att_ta", "=", "att_ta", ".", "write", "(", "t", ",", "alpha", ")", "\n", "val_ta", "=", "val_ta", ".", "write", "(", "t", ",", "new_value", ")", "\n", "cache_key", "=", "tf", ".", "identity", "(", "cache_key", ")", "\n", "return", "t", "+", "1", ",", "out_ta", ",", "att_ta", ",", "val_ta", ",", "new_state", ",", "cache_key", "\n", "\n", "", "time", "=", "tf", ".", "constant", "(", "0", ",", "dtype", "=", "tf", ".", "int32", ",", "name", "=", "\"time\"", ")", "\n", "loop_vars", "=", "(", "time", ",", "output_ta", ",", "alpha_ta", ",", "value_ta", ",", "initial_state", ",", "\n", "cache", "[", "\"key\"", "]", ")", "\n", "\n", "outputs", "=", "tf", ".", "while_loop", "(", "lambda", "t", ",", "*", "_", ":", "t", "<", "time_steps", ",", "\n", "loop_func", ",", "loop_vars", ",", "\n", "parallel_iterations", "=", "32", ",", "\n", "swap_memory", "=", "True", ")", "\n", "\n", "output_final_ta", "=", "outputs", "[", "1", "]", "\n", "value_final_ta", "=", "outputs", "[", "3", "]", "\n", "\n", "final_output", "=", "output_final_ta", ".", "stack", "(", ")", "\n", "final_output", ".", "set_shape", "(", "[", "None", ",", "None", ",", "output_size", "]", ")", "\n", "final_output", "=", "tf", ".", "transpose", "(", "final_output", ",", "[", "1", ",", "0", ",", "2", "]", ")", "\n", "\n", "final_value", "=", "value_final_ta", ".", "stack", "(", ")", "\n", "final_value", ".", "set_shape", "(", "[", "None", ",", "None", ",", "memory", ".", "shape", "[", "-", "1", "]", ".", "value", "]", ")", "\n", "final_value", "=", "tf", ".", "transpose", "(", "final_value", ",", "[", "1", ",", "0", ",", "2", "]", ")", "\n", "\n", "result", "=", "{", "\n", "\"outputs\"", ":", "final_output", ",", "\n", "\"values\"", ":", "final_value", ",", "\n", "\"initial_state\"", ":", "initial_state", "\n", "}", "\n", "\n", "", "return", "result", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.rnnsearch.model_graph": [[187, 334], ["len", "len", "tensorflow.nn.bias_add", "tensorflow.nn.bias_add", "thumt.rnn_cell.LegacyGRUCell", "thumt.rnn_cell.LegacyGRUCell", "rnnsearch._encoder", "thumt.rnn_cell.LegacyGRUCell", "rnnsearch._decoder", "tensorflow.pad", "tensorflow.concat", "thumt.nn.maxout", "thumt.nn.linear", "thumt.nn.linear", "tensorflow.reshape", "thumt.smoothed_softmax_cross_entropy_with_logits", "tensorflow.reshape", "tensorflow.to_float", "tensorflow.get_variable_scope", "tensorflow.variable_scope", "tensorflow.get_variable", "tensorflow.get_variable", "tensorflow.nn.embedding_lookup", "tensorflow.variable_scope", "tensorflow.get_variable", "tensorflow.get_variable", "tensorflow.nn.embedding_lookup", "tensorflow.nn.dropout", "tensorflow.nn.dropout", "tensorflow.nn.rnn_cell.DropoutWrapper", "tensorflow.nn.rnn_cell.DropoutWrapper", "tensorflow.nn.rnn_cell.DropoutWrapper", "thumt.nn.maxout", "thumt.nn.linear", "thumt.nn.linear", "tensorflow.nn.log_softmax", "tensorflow.nn.dropout", "tensorflow.shape", "tensorflow.sequence_mask", "tensorflow.reduce_sum", "tensorflow.reduce_sum", "tensorflow.expand_dims", "tensorflow.reduce_sum", "tensorflow.shape"], "function", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.rnnsearch_lrp._encoder", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.rnnsearch_lrp._decoder", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.layers.nn.maxout", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.layers.nn.linear", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.layers.nn.linear", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.losses.losses.smoothed_softmax_cross_entropy_with_logits", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.layers.nn.maxout", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.layers.nn.linear", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.layers.nn.linear"], ["", "def", "model_graph", "(", "features", ",", "mode", ",", "params", ")", ":", "\n", "    ", "src_vocab_size", "=", "len", "(", "params", ".", "vocabulary", "[", "\"source\"", "]", ")", "\n", "tgt_vocab_size", "=", "len", "(", "params", ".", "vocabulary", "[", "\"target\"", "]", ")", "\n", "dtype", "=", "tf", ".", "get_variable_scope", "(", ")", ".", "dtype", "\n", "\n", "with", "tf", ".", "variable_scope", "(", "\"source_embedding\"", ")", ":", "\n", "        ", "src_emb", "=", "tf", ".", "get_variable", "(", "\"embedding\"", ",", "\n", "[", "src_vocab_size", ",", "params", ".", "embedding_size", "]", ")", "\n", "src_bias", "=", "tf", ".", "get_variable", "(", "\"bias\"", ",", "[", "params", ".", "embedding_size", "]", ")", "\n", "src_inputs", "=", "tf", ".", "nn", ".", "embedding_lookup", "(", "src_emb", ",", "features", "[", "\"source\"", "]", ")", "\n", "\n", "", "with", "tf", ".", "variable_scope", "(", "\"target_embedding\"", ")", ":", "\n", "        ", "tgt_emb", "=", "tf", ".", "get_variable", "(", "\"embedding\"", ",", "\n", "[", "tgt_vocab_size", ",", "params", ".", "embedding_size", "]", ")", "\n", "tgt_bias", "=", "tf", ".", "get_variable", "(", "\"bias\"", ",", "[", "params", ".", "embedding_size", "]", ")", "\n", "tgt_inputs", "=", "tf", ".", "nn", ".", "embedding_lookup", "(", "tgt_emb", ",", "features", "[", "\"target\"", "]", ")", "\n", "\n", "", "src_inputs", "=", "tf", ".", "nn", ".", "bias_add", "(", "src_inputs", ",", "src_bias", ")", "\n", "tgt_inputs", "=", "tf", ".", "nn", ".", "bias_add", "(", "tgt_inputs", ",", "tgt_bias", ")", "\n", "\n", "if", "params", ".", "dropout", "and", "not", "params", ".", "use_variational_dropout", ":", "\n", "        ", "src_inputs", "=", "tf", ".", "nn", ".", "dropout", "(", "src_inputs", ",", "1.0", "-", "params", ".", "dropout", ")", "\n", "tgt_inputs", "=", "tf", ".", "nn", ".", "dropout", "(", "tgt_inputs", ",", "1.0", "-", "params", ".", "dropout", ")", "\n", "\n", "# encoder", "\n", "", "cell_fw", "=", "layers", ".", "rnn_cell", ".", "LegacyGRUCell", "(", "params", ".", "hidden_size", ")", "\n", "cell_bw", "=", "layers", ".", "rnn_cell", ".", "LegacyGRUCell", "(", "params", ".", "hidden_size", ")", "\n", "\n", "if", "params", ".", "use_variational_dropout", ":", "\n", "        ", "cell_fw", "=", "tf", ".", "nn", ".", "rnn_cell", ".", "DropoutWrapper", "(", "\n", "cell_fw", ",", "\n", "input_keep_prob", "=", "1.0", "-", "params", ".", "dropout", ",", "\n", "output_keep_prob", "=", "1.0", "-", "params", ".", "dropout", ",", "\n", "state_keep_prob", "=", "1.0", "-", "params", ".", "dropout", ",", "\n", "variational_recurrent", "=", "True", ",", "\n", "input_size", "=", "params", ".", "embedding_size", ",", "\n", "dtype", "=", "dtype", "\n", ")", "\n", "cell_bw", "=", "tf", ".", "nn", ".", "rnn_cell", ".", "DropoutWrapper", "(", "\n", "cell_bw", ",", "\n", "input_keep_prob", "=", "1.0", "-", "params", ".", "dropout", ",", "\n", "output_keep_prob", "=", "1.0", "-", "params", ".", "dropout", ",", "\n", "state_keep_prob", "=", "1.0", "-", "params", ".", "dropout", ",", "\n", "variational_recurrent", "=", "True", ",", "\n", "input_size", "=", "params", ".", "embedding_size", ",", "\n", "dtype", "=", "dtype", "\n", ")", "\n", "\n", "", "encoder_output", "=", "_encoder", "(", "cell_fw", ",", "cell_bw", ",", "src_inputs", ",", "\n", "features", "[", "\"source_length\"", "]", ",", "dtype", "=", "dtype", ")", "\n", "\n", "# decoder", "\n", "cell", "=", "layers", ".", "rnn_cell", ".", "LegacyGRUCell", "(", "params", ".", "hidden_size", ")", "\n", "\n", "if", "params", ".", "use_variational_dropout", ":", "\n", "        ", "cell", "=", "tf", ".", "nn", ".", "rnn_cell", ".", "DropoutWrapper", "(", "\n", "cell", ",", "\n", "input_keep_prob", "=", "1.0", "-", "params", ".", "dropout", ",", "\n", "output_keep_prob", "=", "1.0", "-", "params", ".", "dropout", ",", "\n", "state_keep_prob", "=", "1.0", "-", "params", ".", "dropout", ",", "\n", "variational_recurrent", "=", "True", ",", "\n", "# input + context", "\n", "input_size", "=", "params", ".", "embedding_size", "+", "2", "*", "params", ".", "hidden_size", ",", "\n", "dtype", "=", "dtype", "\n", ")", "\n", "\n", "", "length", "=", "{", "\n", "\"source\"", ":", "features", "[", "\"source_length\"", "]", ",", "\n", "\"target\"", ":", "features", "[", "\"target_length\"", "]", "\n", "}", "\n", "initial_state", "=", "encoder_output", "[", "\"final_states\"", "]", "[", "\"backward\"", "]", "\n", "decoder_output", "=", "_decoder", "(", "cell", ",", "tgt_inputs", ",", "encoder_output", "[", "\"annotation\"", "]", ",", "\n", "length", ",", "initial_state", ",", "dtype", "=", "dtype", ")", "\n", "\n", "# Shift left", "\n", "shifted_tgt_inputs", "=", "tf", ".", "pad", "(", "tgt_inputs", ",", "[", "[", "0", ",", "0", "]", ",", "[", "1", ",", "0", "]", ",", "[", "0", ",", "0", "]", "]", ")", "\n", "shifted_tgt_inputs", "=", "shifted_tgt_inputs", "[", ":", ",", ":", "-", "1", ",", ":", "]", "\n", "\n", "all_outputs", "=", "tf", ".", "concat", "(", "\n", "[", "\n", "tf", ".", "expand_dims", "(", "decoder_output", "[", "\"initial_state\"", "]", ",", "axis", "=", "1", ")", ",", "\n", "decoder_output", "[", "\"outputs\"", "]", ",", "\n", "]", ",", "\n", "axis", "=", "1", "\n", ")", "\n", "shifted_outputs", "=", "all_outputs", "[", ":", ",", ":", "-", "1", ",", ":", "]", "\n", "\n", "maxout_features", "=", "[", "\n", "shifted_tgt_inputs", ",", "\n", "shifted_outputs", ",", "\n", "decoder_output", "[", "\"values\"", "]", "\n", "]", "\n", "maxout_size", "=", "params", ".", "hidden_size", "//", "params", ".", "maxnum", "\n", "\n", "if", "mode", "==", "\"infer\"", ":", "\n", "# Special case for non-incremental decoding", "\n", "        ", "maxout_features", "=", "[", "\n", "shifted_tgt_inputs", "[", ":", ",", "-", "1", ",", ":", "]", ",", "\n", "shifted_outputs", "[", ":", ",", "-", "1", ",", ":", "]", ",", "\n", "decoder_output", "[", "\"values\"", "]", "[", ":", ",", "-", "1", ",", ":", "]", "\n", "]", "\n", "maxhid", "=", "layers", ".", "nn", ".", "maxout", "(", "maxout_features", ",", "maxout_size", ",", "params", ".", "maxnum", ",", "\n", "concat", "=", "False", ")", "\n", "readout", "=", "layers", ".", "nn", ".", "linear", "(", "maxhid", ",", "params", ".", "embedding_size", ",", "False", ",", "\n", "False", ",", "scope", "=", "\"deepout\"", ")", "\n", "\n", "# Prediction", "\n", "logits", "=", "layers", ".", "nn", ".", "linear", "(", "readout", ",", "tgt_vocab_size", ",", "True", ",", "False", ",", "\n", "scope", "=", "\"softmax\"", ")", "\n", "\n", "return", "tf", ".", "nn", ".", "log_softmax", "(", "logits", ")", "\n", "\n", "", "maxhid", "=", "layers", ".", "nn", ".", "maxout", "(", "maxout_features", ",", "maxout_size", ",", "params", ".", "maxnum", ",", "\n", "concat", "=", "False", ")", "\n", "readout", "=", "layers", ".", "nn", ".", "linear", "(", "maxhid", ",", "params", ".", "embedding_size", ",", "False", ",", "False", ",", "\n", "scope", "=", "\"deepout\"", ")", "\n", "\n", "if", "params", ".", "dropout", "and", "not", "params", ".", "use_variational_dropout", ":", "\n", "        ", "readout", "=", "tf", ".", "nn", ".", "dropout", "(", "readout", ",", "1.0", "-", "params", ".", "dropout", ")", "\n", "\n", "# Prediction", "\n", "", "logits", "=", "layers", ".", "nn", ".", "linear", "(", "readout", ",", "tgt_vocab_size", ",", "True", ",", "False", ",", "\n", "scope", "=", "\"softmax\"", ")", "\n", "logits", "=", "tf", ".", "reshape", "(", "logits", ",", "[", "-", "1", ",", "tgt_vocab_size", "]", ")", "\n", "labels", "=", "features", "[", "\"target\"", "]", "\n", "\n", "ce", "=", "losses", ".", "smoothed_softmax_cross_entropy_with_logits", "(", "\n", "logits", "=", "logits", ",", "\n", "labels", "=", "labels", ",", "\n", "smoothing", "=", "params", ".", "label_smoothing", ",", "\n", "normalize", "=", "True", "\n", ")", "\n", "\n", "ce", "=", "tf", ".", "reshape", "(", "ce", ",", "tf", ".", "shape", "(", "labels", ")", ")", "\n", "tgt_mask", "=", "tf", ".", "to_float", "(", "\n", "tf", ".", "sequence_mask", "(", "\n", "features", "[", "\"target_length\"", "]", ",", "\n", "maxlen", "=", "tf", ".", "shape", "(", "features", "[", "\"target\"", "]", ")", "[", "1", "]", "\n", ")", "\n", ")", "\n", "\n", "if", "mode", "==", "\"eval\"", ":", "\n", "        ", "return", "-", "tf", ".", "reduce_sum", "(", "ce", "*", "tgt_mask", ",", "axis", "=", "1", ")", "\n", "\n", "", "loss", "=", "tf", ".", "reduce_sum", "(", "ce", "*", "tgt_mask", ")", "/", "tf", ".", "reduce_sum", "(", "tgt_mask", ")", "\n", "\n", "return", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.seq2seq.Seq2Seq.__init__": [[141, 143], ["thumt.models.model.NMTModel.__init__"], "methods", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.transformer_lrp.TransformerLRP.__init__"], ["    ", "def", "__init__", "(", "self", ",", "params", ",", "scope", "=", "\"seq2seq\"", ")", ":", "\n", "        ", "super", "(", "Seq2Seq", ",", "self", ")", ".", "__init__", "(", "params", "=", "params", ",", "scope", "=", "scope", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.seq2seq.Seq2Seq.get_training_func": [[144, 158], ["tensorflow.variable_scope", "seq2seq.model_graph"], "methods", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.transformer_lrp.model_graph"], ["", "def", "get_training_func", "(", "self", ",", "initializer", ",", "regularizer", "=", "None", ",", "dtype", "=", "None", ")", ":", "\n", "        ", "def", "training_fn", "(", "features", ",", "params", "=", "None", ",", "reuse", "=", "None", ")", ":", "\n", "            ", "if", "params", "is", "None", ":", "\n", "                ", "params", "=", "self", ".", "parameters", "\n", "\n", "", "custom_getter", "=", "utils", ".", "custom_getter", "if", "dtype", "else", "None", "\n", "\n", "with", "tf", ".", "variable_scope", "(", "self", ".", "_scope", ",", "initializer", "=", "initializer", ",", "\n", "regularizer", "=", "regularizer", ",", "reuse", "=", "reuse", ",", "\n", "custom_getter", "=", "custom_getter", ",", "dtype", "=", "dtype", ")", ":", "\n", "                ", "loss", "=", "model_graph", "(", "features", ",", "\"train\"", ",", "params", ")", "\n", "return", "loss", "\n", "\n", "", "", "return", "training_fn", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.seq2seq.Seq2Seq.get_evaluation_func": [[159, 175], ["copy.copy", "copy.copy", "tensorflow.variable_scope", "seq2seq.model_graph"], "methods", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.transformer_lrp.model_graph"], ["", "def", "get_evaluation_func", "(", "self", ")", ":", "\n", "        ", "def", "evaluation_fn", "(", "features", ",", "params", "=", "None", ")", ":", "\n", "            ", "if", "params", "is", "None", ":", "\n", "                ", "params", "=", "copy", ".", "copy", "(", "self", ".", "parameters", ")", "\n", "", "else", ":", "\n", "                ", "params", "=", "copy", ".", "copy", "(", "params", ")", "\n", "", "params", ".", "dropout", "=", "0.0", "\n", "params", ".", "use_variational_dropout", "=", "False", "\n", "params", ".", "label_smoothing", "=", "0.0", "\n", "\n", "with", "tf", ".", "variable_scope", "(", "self", ".", "_scope", ")", ":", "\n", "                ", "score", "=", "model_graph", "(", "features", ",", "\"eval\"", ",", "params", ")", "\n", "\n", "", "return", "score", "\n", "\n", "", "return", "evaluation_fn", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.seq2seq.Seq2Seq.get_inference_func": [[176, 192], ["copy.copy", "copy.copy", "tensorflow.variable_scope", "seq2seq.model_graph"], "methods", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.transformer_lrp.model_graph"], ["", "def", "get_inference_func", "(", "self", ")", ":", "\n", "        ", "def", "inference_fn", "(", "features", ",", "params", "=", "None", ")", ":", "\n", "            ", "if", "params", "is", "None", ":", "\n", "                ", "params", "=", "copy", ".", "copy", "(", "self", ".", "parameters", ")", "\n", "", "else", ":", "\n", "                ", "params", "=", "copy", ".", "copy", "(", "params", ")", "\n", "", "params", ".", "dropout", "=", "0.0", "\n", "params", ".", "use_variational_dropout", "=", "False", "\n", "params", ".", "label_smoothing", "=", "0.0", "\n", "\n", "with", "tf", ".", "variable_scope", "(", "self", ".", "_scope", ")", ":", "\n", "                ", "logits", "=", "model_graph", "(", "features", ",", "\"infer\"", ",", "params", ")", "\n", "\n", "", "return", "logits", "\n", "\n", "", "return", "inference_fn", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.seq2seq.Seq2Seq.get_name": [[193, 196], ["None"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "get_name", "(", ")", ":", "\n", "        ", "return", "\"seq2seq\"", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.seq2seq.Seq2Seq.get_parameters": [[197, 224], ["tensorflow.contrib.training.HParams"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "get_parameters", "(", ")", ":", "\n", "        ", "params", "=", "tf", ".", "contrib", ".", "training", ".", "HParams", "(", "\n", "# vocabulary", "\n", "pad", "=", "\"<pad>\"", ",", "\n", "bos", "=", "\"<eos>\"", ",", "\n", "eos", "=", "\"<eos>\"", ",", "\n", "unk", "=", "\"<unk>\"", ",", "\n", "append_eos", "=", "False", ",", "\n", "# model", "\n", "rnn_cell", "=", "\"LSTMCell\"", ",", "\n", "embedding_size", "=", "1000", ",", "\n", "hidden_size", "=", "1000", ",", "\n", "num_hidden_layers", "=", "4", ",", "\n", "# regularization", "\n", "dropout", "=", "0.2", ",", "\n", "use_variational_dropout", "=", "False", ",", "\n", "label_smoothing", "=", "0.1", ",", "\n", "constant_batch_size", "=", "True", ",", "\n", "batch_size", "=", "128", ",", "\n", "max_length", "=", "80", ",", "\n", "reverse_source", "=", "True", ",", "\n", "use_residual", "=", "True", ",", "\n", "clip_grad_norm", "=", "5.0", "\n", ")", "\n", "\n", "return", "params", "\n", "", "", ""]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.seq2seq.model_graph": [[18, 137], ["len", "len", "tensorflow.nn.bias_add", "tensorflow.nn.bias_add", "range", "tensorflow.nn.rnn_cell.MultiRNNCell", "tensorflow.nn.rnn_cell.MultiRNNCell", "tensorflow.pad", "thumt.nn.linear", "tensorflow.reshape", "thumt.smoothed_softmax_cross_entropy_with_logits", "tensorflow.reshape", "tensorflow.to_float", "tensorflow.get_variable_scope", "tensorflow.reverse_sequence", "tensorflow.device", "tensorflow.nn.dropout", "tensorflow.nn.dropout", "tensorflow.nn.rnn_cell.DropoutWrapper", "tensorflow.nn.rnn_cell.DropoutWrapper", "tf.nn.rnn_cell.MultiRNNCell.append", "tf.nn.rnn_cell.MultiRNNCell.append", "tensorflow.variable_scope", "tensorflow.nn.dynamic_rnn", "tensorflow.variable_scope", "tensorflow.nn.dynamic_rnn", "tensorflow.nn.dropout", "thumt.nn.linear", "tensorflow.nn.log_softmax", "tensorflow.shape", "tensorflow.sequence_mask", "tensorflow.reduce_sum", "tensorflow.reduce_sum", "tensorflow.variable_scope", "tensorflow.get_variable", "tensorflow.get_variable", "tensorflow.nn.embedding_lookup", "tensorflow.variable_scope", "tensorflow.get_variable", "tensorflow.get_variable", "tensorflow.nn.embedding_lookup", "tensorflow.nn.rnn_cell.BasicLSTMCell", "tensorflow.nn.rnn_cell.BasicLSTMCell", "tensorflow.nn.rnn_cell.ResidualWrapper", "tensorflow.nn.rnn_cell.ResidualWrapper", "tensorflow.reduce_sum", "tensorflow.nn.rnn_cell.GRUCell", "tensorflow.nn.rnn_cell.GRUCell", "ValueError", "tensorflow.shape"], "function", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.layers.nn.linear", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.losses.losses.smoothed_softmax_cross_entropy_with_logits", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.layers.nn.linear"], ["def", "model_graph", "(", "features", ",", "mode", ",", "params", ")", ":", "\n", "    ", "src_vocab_size", "=", "len", "(", "params", ".", "vocabulary", "[", "\"source\"", "]", ")", "\n", "tgt_vocab_size", "=", "len", "(", "params", ".", "vocabulary", "[", "\"target\"", "]", ")", "\n", "dtype", "=", "tf", ".", "get_variable_scope", "(", ")", ".", "dtype", "\n", "\n", "src_seq", "=", "features", "[", "\"source\"", "]", "\n", "tgt_seq", "=", "features", "[", "\"target\"", "]", "\n", "\n", "if", "params", ".", "reverse_source", ":", "\n", "        ", "src_seq", "=", "tf", ".", "reverse_sequence", "(", "src_seq", ",", "seq_dim", "=", "1", ",", "\n", "seq_lengths", "=", "features", "[", "\"source_length\"", "]", ")", "\n", "\n", "", "with", "tf", ".", "device", "(", "\"/cpu:0\"", ")", ":", "\n", "        ", "with", "tf", ".", "variable_scope", "(", "\"source_embedding\"", ")", ":", "\n", "            ", "src_emb", "=", "tf", ".", "get_variable", "(", "\"embedding\"", ",", "\n", "[", "src_vocab_size", ",", "params", ".", "embedding_size", "]", ")", "\n", "src_bias", "=", "tf", ".", "get_variable", "(", "\"bias\"", ",", "[", "params", ".", "embedding_size", "]", ")", "\n", "src_inputs", "=", "tf", ".", "nn", ".", "embedding_lookup", "(", "src_emb", ",", "src_seq", ")", "\n", "\n", "", "with", "tf", ".", "variable_scope", "(", "\"target_embedding\"", ")", ":", "\n", "            ", "tgt_emb", "=", "tf", ".", "get_variable", "(", "\"embedding\"", ",", "\n", "[", "tgt_vocab_size", ",", "params", ".", "embedding_size", "]", ")", "\n", "tgt_bias", "=", "tf", ".", "get_variable", "(", "\"bias\"", ",", "[", "params", ".", "embedding_size", "]", ")", "\n", "tgt_inputs", "=", "tf", ".", "nn", ".", "embedding_lookup", "(", "tgt_emb", ",", "tgt_seq", ")", "\n", "\n", "", "", "src_inputs", "=", "tf", ".", "nn", ".", "bias_add", "(", "src_inputs", ",", "src_bias", ")", "\n", "tgt_inputs", "=", "tf", ".", "nn", ".", "bias_add", "(", "tgt_inputs", ",", "tgt_bias", ")", "\n", "\n", "if", "params", ".", "dropout", "and", "not", "params", ".", "use_variational_dropout", ":", "\n", "        ", "src_inputs", "=", "tf", ".", "nn", ".", "dropout", "(", "src_inputs", ",", "1.0", "-", "params", ".", "dropout", ")", "\n", "tgt_inputs", "=", "tf", ".", "nn", ".", "dropout", "(", "tgt_inputs", ",", "1.0", "-", "params", ".", "dropout", ")", "\n", "\n", "", "cell_enc", "=", "[", "]", "\n", "cell_dec", "=", "[", "]", "\n", "\n", "for", "_", "in", "range", "(", "params", ".", "num_hidden_layers", ")", ":", "\n", "        ", "if", "params", ".", "rnn_cell", "==", "\"LSTMCell\"", ":", "\n", "            ", "cell_e", "=", "tf", ".", "nn", ".", "rnn_cell", ".", "BasicLSTMCell", "(", "params", ".", "hidden_size", ")", "\n", "cell_d", "=", "tf", ".", "nn", ".", "rnn_cell", ".", "BasicLSTMCell", "(", "params", ".", "hidden_size", ")", "\n", "", "elif", "params", ".", "rnn_cell", "==", "\"GRUCell\"", ":", "\n", "            ", "cell_e", "=", "tf", ".", "nn", ".", "rnn_cell", ".", "GRUCell", "(", "params", ".", "hidden_size", ")", "\n", "cell_d", "=", "tf", ".", "nn", ".", "rnn_cell", ".", "GRUCell", "(", "params", ".", "hidden_size", ")", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"%s not supported\"", "%", "params", ".", "rnn_cell", ")", "\n", "\n", "", "cell_e", "=", "tf", ".", "nn", ".", "rnn_cell", ".", "DropoutWrapper", "(", "\n", "cell_e", ",", "\n", "output_keep_prob", "=", "1.0", "-", "params", ".", "dropout", ",", "\n", "variational_recurrent", "=", "params", ".", "use_variational_dropout", ",", "\n", "input_size", "=", "params", ".", "embedding_size", ",", "\n", "dtype", "=", "dtype", "\n", ")", "\n", "cell_d", "=", "tf", ".", "nn", ".", "rnn_cell", ".", "DropoutWrapper", "(", "\n", "cell_d", ",", "\n", "output_keep_prob", "=", "1.0", "-", "params", ".", "dropout", ",", "\n", "variational_recurrent", "=", "params", ".", "use_variational_dropout", ",", "\n", "input_size", "=", "params", ".", "embedding_size", ",", "\n", "dtype", "=", "dtype", "\n", ")", "\n", "\n", "if", "params", ".", "use_residual", ":", "\n", "            ", "cell_e", "=", "tf", ".", "nn", ".", "rnn_cell", ".", "ResidualWrapper", "(", "cell_e", ")", "\n", "cell_d", "=", "tf", ".", "nn", ".", "rnn_cell", ".", "ResidualWrapper", "(", "cell_d", ")", "\n", "\n", "", "cell_enc", ".", "append", "(", "cell_e", ")", "\n", "cell_dec", ".", "append", "(", "cell_d", ")", "\n", "\n", "", "cell_enc", "=", "tf", ".", "nn", ".", "rnn_cell", ".", "MultiRNNCell", "(", "cell_enc", ")", "\n", "cell_dec", "=", "tf", ".", "nn", ".", "rnn_cell", ".", "MultiRNNCell", "(", "cell_dec", ")", "\n", "\n", "with", "tf", ".", "variable_scope", "(", "\"encoder\"", ")", ":", "\n", "        ", "_", ",", "final_state", "=", "tf", ".", "nn", ".", "dynamic_rnn", "(", "cell_enc", ",", "src_inputs", ",", "\n", "features", "[", "\"source_length\"", "]", ",", "\n", "dtype", "=", "dtype", ")", "\n", "# Shift left", "\n", "", "shifted_tgt_inputs", "=", "tf", ".", "pad", "(", "tgt_inputs", ",", "[", "[", "0", ",", "0", "]", ",", "[", "1", ",", "0", "]", ",", "[", "0", ",", "0", "]", "]", ")", "\n", "shifted_tgt_inputs", "=", "shifted_tgt_inputs", "[", ":", ",", ":", "-", "1", ",", ":", "]", "\n", "\n", "with", "tf", ".", "variable_scope", "(", "\"decoder\"", ")", ":", "\n", "        ", "outputs", ",", "_", "=", "tf", ".", "nn", ".", "dynamic_rnn", "(", "cell_dec", ",", "shifted_tgt_inputs", ",", "\n", "features", "[", "\"target_length\"", "]", ",", "\n", "initial_state", "=", "final_state", ")", "\n", "\n", "", "if", "params", ".", "dropout", ":", "\n", "        ", "outputs", "=", "tf", ".", "nn", ".", "dropout", "(", "outputs", ",", "1.0", "-", "params", ".", "dropout", ")", "\n", "\n", "", "if", "mode", "==", "\"infer\"", ":", "\n", "# Prediction", "\n", "        ", "logits", "=", "layers", ".", "nn", ".", "linear", "(", "outputs", "[", ":", ",", "-", "1", ",", ":", "]", ",", "tgt_vocab_size", ",", "True", ",", "\n", "scope", "=", "\"softmax\"", ")", "\n", "\n", "return", "tf", ".", "nn", ".", "log_softmax", "(", "logits", ")", "\n", "\n", "# Prediction", "\n", "", "logits", "=", "layers", ".", "nn", ".", "linear", "(", "outputs", ",", "tgt_vocab_size", ",", "True", ",", "scope", "=", "\"softmax\"", ")", "\n", "logits", "=", "tf", ".", "reshape", "(", "logits", ",", "[", "-", "1", ",", "tgt_vocab_size", "]", ")", "\n", "labels", "=", "features", "[", "\"target\"", "]", "\n", "\n", "ce", "=", "losses", ".", "smoothed_softmax_cross_entropy_with_logits", "(", "\n", "logits", "=", "logits", ",", "\n", "labels", "=", "labels", ",", "\n", "smoothing", "=", "params", ".", "label_smoothing", ",", "\n", "normalize", "=", "True", "\n", ")", "\n", "\n", "ce", "=", "tf", ".", "reshape", "(", "ce", ",", "tf", ".", "shape", "(", "labels", ")", ")", "\n", "tgt_mask", "=", "tf", ".", "to_float", "(", "\n", "tf", ".", "sequence_mask", "(", "\n", "features", "[", "\"target_length\"", "]", ",", "\n", "maxlen", "=", "tf", ".", "shape", "(", "features", "[", "\"target\"", "]", ")", "[", "1", "]", "\n", ")", "\n", ")", "\n", "\n", "if", "mode", "==", "\"eval\"", ":", "\n", "        ", "return", "-", "tf", ".", "reduce_sum", "(", "ce", "*", "tgt_mask", ",", "axis", "=", "1", ")", "\n", "\n", "", "loss", "=", "tf", ".", "reduce_sum", "(", "ce", "*", "tgt_mask", ")", "/", "tf", ".", "reduce_sum", "(", "tgt_mask", ")", "\n", "\n", "return", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.model.NMTModel.__init__": [[12, 15], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "params", ",", "scope", ")", ":", "\n", "        ", "self", ".", "_scope", "=", "scope", "\n", "self", ".", "_params", "=", "params", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.model.NMTModel.get_training_func": [[16, 25], ["NotImplementedError"], "methods", ["None"], ["", "def", "get_training_func", "(", "self", ",", "initializer", ",", "regularizer", "=", "None", ",", "dtype", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        :param initializer: the initializer used to initialize the model\n        :param regularizer: the regularizer used for model regularization\n        :param dtype: an instance of tf.DType\n        :return: a function with the following signature:\n            (features, params, reuse) -> loss\n        \"\"\"", "\n", "raise", "NotImplementedError", "(", "\"Not implemented\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.model.NMTModel.get_evaluation_func": [[26, 32], ["NotImplementedError"], "methods", ["None"], ["", "def", "get_evaluation_func", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        :return: a function with the following signature:\n            (features, params) -> score\n        \"\"\"", "\n", "raise", "NotImplementedError", "(", "\"Not implemented\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.model.NMTModel.get_inference_func": [[33, 52], ["NotImplementedError"], "methods", ["None"], ["", "def", "get_inference_func", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        :returns:\n            If a model implements incremental decoding, this function should\n            returns a tuple of (encoding_fn, decoding_fn), with the following\n            requirements:\n                encoding_fn: (features, params) -> initial_state\n                decoding_fn: (feature, state, params) -> log_prob, next_state\n\n            If a model does not implement the incremental decoding (slower\n            decoding speed but easier to write the code), then this\n            function should returns a single function with the following\n            signature:\n                (features, params) -> log_prob\n\n            See models/transformer.py and models/rnnsearch.py\n            for comparison.\n        \"\"\"", "\n", "raise", "NotImplementedError", "(", "\"Not implemented\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.model.NMTModel.get_name": [[53, 56], ["NotImplementedError"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "get_name", "(", ")", ":", "\n", "        ", "raise", "NotImplementedError", "(", "\"Not implemented\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.model.NMTModel.get_parameters": [[57, 60], ["NotImplementedError"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "get_parameters", "(", ")", ":", "\n", "        ", "raise", "NotImplementedError", "(", "\"Not implemented\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.model.NMTModel.parameters": [[61, 64], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "parameters", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "_params", "\n", "", "", ""]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.__init__.get_model": [[15, 32], ["name.lower.lower", "LookupError"], "function", ["None"], []], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.transformer.Transformer.__init__": [[324, 326], ["thumt.models.model.NMTModel.__init__"], "methods", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.transformer_lrp.TransformerLRP.__init__"], ["        ", "keep_prob", "=", "1.0", "-", "params", ".", "residual_dropout", "\n", "soft_input", "=", "tf", ".", "nn", ".", "dropout", "(", "soft_input", ",", "keep_prob", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.transformer.Transformer.get_training_func": [[327, 343], ["copy.copy", "copy.copy", "tensorflow.variable_scope", "transformer.model_graph"], "methods", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.transformer_lrp.model_graph"], ["# [batch_size, length, hidden_size]", "\n", "", "soft_input", "=", "tf", ".", "reshape", "(", "soft_input", ",", "tf", ".", "concat", "(", "[", "tf", ".", "shape", "(", "tgt_seq", ")", ",", "[", "hidden_size", "]", "]", ",", "0", ")", ")", "\n", "soft_input", "=", "soft_input", "*", "tf", ".", "expand_dims", "(", "tgt_mask", ",", "-", "1", ")", "\n", "soft_input", "=", "tf", ".", "pad", "(", "soft_input", ",", "[", "[", "0", ",", "0", "]", ",", "[", "1", ",", "0", "]", ",", "[", "0", ",", "0", "]", "]", ")", "[", ":", ",", ":", "-", "1", ",", ":", "]", "\n", "soft_input", "=", "layers", ".", "attention", ".", "add_timing_signal", "(", "soft_input", ")", "\n", "\n", "select_seed", "=", "tf", ".", "random", ".", "uniform", "(", "tf", ".", "shape", "(", "tgt_seq", ")", ",", "minval", "=", "0", ",", "maxval", "=", "1", ")", "\n", "select_seed", "=", "select_seed", "[", ":", ",", ":", ",", "tf", ".", "newaxis", "]", "\n", "select_seed", "=", "tf", ".", "tile", "(", "select_seed", ",", "[", "1", ",", "1", ",", "hidden_size", "]", ")", "\n", "\n", "global_step", "=", "tf", ".", "train", ".", "get_global_step", "(", ")", "\n", "step", "=", "tf", ".", "to_float", "(", "global_step", ")", "\n", "if", "params", ".", "trainstep_scheduled_sampling_strategy", "==", "\"linear\"", ":", "\n", "        ", "current_step_threshold", "=", "1", "-", "1", "/", "params", ".", "train_steps", "*", "step", "\n", "", "elif", "params", ".", "trainstep_scheduled_sampling_strategy", "==", "\"exp\"", ":", "\n", "        ", "current_step_threshold", "=", "params", ".", "trainstep_exp_radix", "**", "step", "\n", "", "elif", "params", ".", "trainstep_scheduled_sampling_strategy", "==", "\"sigmoid\"", ":", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.transformer.Transformer.get_evaluation_func": [[344, 357], ["copy.copy", "copy.copy", "tensorflow.variable_scope", "transformer.model_graph"], "methods", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.transformer_lrp.model_graph"], ["        ", "current_step_threshold", "=", "params", ".", "trainstep_sigmoid_k", "/", "(", "params", ".", "trainstep_sigmoid_k", "+", "tf", ".", "exp", "(", "step", "/", "params", ".", "trainstep_sigmoid_k", ")", ")", "\n", "", "elif", "params", ".", "trainstep_scheduled_sampling_strategy", "==", "\"none\"", ":", "\n", "        ", "current_step_threshold", "=", "1.0", "\n", "", "else", ":", "\n", "        ", "raise", "ValueError", "(", "\"Unknown trainstep_scheduled_sampling_strategy  %s\"", "%", "params", ".", "trainstep_scheduled_sampling_strategy", ")", "\n", "\n", "# get token-specific threshold for soft bridge", "\n", "", "max_len", "=", "params", ".", "max_length", "\n", "# t starts from 0", "\n", "t", "=", "tf", ".", "tile", "(", "tf", ".", "expand_dims", "(", "tf", ".", "range", "(", "start", "=", "0", ",", "limit", "=", "max_len", ",", "dtype", "=", "tf", ".", "float64", ")", ",", "0", ")", ",", "[", "max_len", ",", "1", "]", ")", "# n", "\n", "# reverse_t is 1/t, but t start from 1", "\n", "reverse_t", "=", "tf", ".", "transpose", "(", "tf", ".", "expand_dims", "(", "1", "/", "tf", ".", "range", "(", "start", "=", "1", ",", "limit", "=", "max_len", "+", "1", ",", "dtype", "=", "tf", ".", "float64", ")", ",", "axis", "=", "0", ")", ")", "# 1/n", "\n", "current_step_threshold", "=", "tf", ".", "cast", "(", "current_step_threshold", ",", "tf", ".", "float64", ")", "\n", "if", "params", ".", "timestep_scheduled_sampling_strategy", "==", "\"linear\"", ":", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.transformer.Transformer.get_inference_func": [[358, 394], ["copy.copy", "copy.copy", "tensorflow.variable_scope", "transformer.encoding_graph", "copy.copy", "copy.copy", "tensorflow.variable_scope", "transformer.decoding_graph", "tensorflow.shape", "tensorflow.zeros", "tensorflow.zeros", "range"], "methods", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.transformer.encoding_graph", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.transformer.decoding_graph"], ["        ", "threshold_table", "=", "1", "-", "t", "/", "max_len", "*", "(", "1", "-", "current_step_threshold", ")", "\n", "", "elif", "params", ".", "timestep_scheduled_sampling_strategy", "==", "\"inverse_linear\"", ":", "\n", "        ", "threshold_table", "=", "t", "/", "max_len", "*", "(", "1", "-", "current_step_threshold", ")", "\n", "\n", "", "elif", "params", ".", "timestep_scheduled_sampling_strategy", "==", "\"exp\"", ":", "\n", "        ", "threshold_table", "=", "params", ".", "timestep_exp_epsilon", "**", "(", "t", "*", "(", "1", "-", "current_step_threshold", ")", ")", "\n", "", "elif", "params", ".", "timestep_scheduled_sampling_strategy", "==", "\"sigmoid\"", ":", "\n", "        ", "threshold_table", "=", "params", ".", "timestep_sigmoid_k", "/", "(", "params", ".", "timestep_sigmoid_k", "+", "tf", ".", "exp", "(", "t", "/", "params", ".", "timestep_sigmoid_k", "*", "(", "1", "-", "current_step_threshold", ")", ")", ")", "\n", "\n", "", "elif", "params", ".", "timestep_scheduled_sampling_strategy", "==", "\"inverse_exp\"", ":", "\n", "        ", "threshold_table", "=", "1", "-", "params", ".", "timestep_exp_epsilon", "**", "t", "*", "(", "1", "-", "current_step_threshold", ")", "\n", "", "elif", "params", ".", "timestep_scheduled_sampling_strategy", "==", "\"inverse_sigmoid\"", ":", "\n", "        ", "threshold_table", "=", "1", "-", "params", ".", "timestep_sigmoid_k", "/", "(", "params", ".", "timestep_sigmoid_k", "+", "tf", ".", "exp", "(", "t", "/", "params", ".", "timestep_sigmoid_k", "*", "(", "1", "-", "current_step_threshold", ")", ")", ")", "\n", "\n", "", "else", ":", "\n", "        ", "raise", "ValueError", "(", "\"Unknown timestep_scheduled_sampling_strategy  %s\"", "%", "params", ".", "timestep_scheduled_sampling_strategy", ")", "\n", "\n", "", "threshold_table", "=", "tf", ".", "matrix_band_part", "(", "threshold_table", ",", "-", "1", ",", "0", ")", "\n", "tgt_len", "-=", "1", "# for index   # [b]", "\n", "current_threshold", "=", "tf", ".", "gather", "(", "threshold_table", ",", "tgt_len", ")", "# [b, max_len]", "\n", "# cliping max_len to cur_max_len", "\n", "cur_max_trg_len", "=", "tf", ".", "shape", "(", "tgt_seq", ")", "[", "1", "]", "\n", "current_threshold", "=", "current_threshold", "[", ":", ",", ":", "cur_max_trg_len", "]", "\n", "current_threshold", "=", "current_threshold", "[", ":", ",", ":", ",", "tf", ".", "newaxis", "]", "\n", "current_threshold", "=", "tf", ".", "tile", "(", "current_threshold", ",", "[", "1", ",", "1", ",", "hidden_size", "]", ")", "\n", "current_threshold", "=", "tf", ".", "cast", "(", "current_threshold", ",", "tf", ".", "float32", ")", "\n", "\n", "final_threshold", "=", "current_threshold", "\n", "\n", "new_soft_input", "=", "tf", ".", "where", "(", "tf", ".", "less", "(", "select_seed", ",", "final_threshold", ")", ",", "\n", "x", "=", "decoder_input", ",", "y", "=", "soft_input", ",", "name", "=", "'new_soft_input_cur_threshold'", ")", "\n", "\n", "bridge_output", "=", "transformer_decoder", "(", "new_soft_input", ",", "encoder_output", ",", "\n", "dec_attn_bias", ",", "enc_attn_bias", ",", "\n", "params", ",", "scope", "=", "'decoder'", ")", "\n", "bridge_output", "=", "tf", ".", "reshape", "(", "bridge_output", ",", "[", "-", "1", ",", "hidden_size", "]", ")", "\n", "bridge_logits", "=", "tf", ".", "matmul", "(", "bridge_output", ",", "weights", ",", "False", ",", "True", ")", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.transformer.Transformer.get_name": [[395, 398], ["None"], "methods", ["None"], ["bridge_ce", "=", "losses", ".", "smoothed_softmax_cross_entropy_with_logits", "(", "\n", "logits", "=", "bridge_logits", ",", "\n", "labels", "=", "labels", ",", "\n", "smoothing", "=", "params", ".", "label_smoothing", ",", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.transformer.Transformer.get_parameters": [[399, 441], ["tensorflow.contrib.training.HParams"], "methods", ["None"], ["normalize", "=", "True", "\n", ")", "\n", "bridge_ce", "=", "tf", ".", "reshape", "(", "bridge_ce", ",", "tf", ".", "shape", "(", "tgt_seq", ")", ")", "\n", "bridge_loss", "=", "tf", ".", "reduce_sum", "(", "bridge_ce", "*", "tgt_mask", ")", "/", "tf", ".", "reduce_sum", "(", "tgt_mask", ")", "\n", "\n", "return", "(", "loss", ",", "bridge_loss", ")", "\n", "\n", "\n", "", "def", "model_graph", "(", "features", ",", "mode", ",", "params", ")", ":", "\n", "    ", "encoder_output", "=", "encoding_graph", "(", "features", ",", "mode", ",", "params", ")", "\n", "state", "=", "{", "\n", "\"encoder\"", ":", "encoder_output", "\n", "}", "\n", "output", "=", "decoding_graph", "(", "features", ",", "state", ",", "mode", ",", "params", ")", "\n", "\n", "return", "output", "\n", "\n", "\n", "", "class", "Transformer", "(", "NMTModel", ")", ":", "\n", "\n", "    ", "def", "__init__", "(", "self", ",", "params", ",", "scope", "=", "\"transformer\"", ")", ":", "\n", "        ", "super", "(", "Transformer", ",", "self", ")", ".", "__init__", "(", "params", "=", "params", ",", "scope", "=", "scope", ")", "\n", "\n", "", "def", "get_training_func", "(", "self", ",", "initializer", ",", "regularizer", "=", "None", ",", "dtype", "=", "None", ")", ":", "\n", "        ", "def", "training_fn", "(", "features", ",", "params", "=", "None", ",", "reuse", "=", "None", ")", ":", "\n", "            ", "if", "params", "is", "None", ":", "\n", "                ", "params", "=", "copy", ".", "copy", "(", "self", ".", "parameters", ")", "\n", "", "else", ":", "\n", "                ", "params", "=", "copy", ".", "copy", "(", "params", ")", "\n", "\n", "", "custom_getter", "=", "utils", ".", "custom_getter", "if", "dtype", "else", "None", "\n", "\n", "with", "tf", ".", "variable_scope", "(", "self", ".", "_scope", ",", "initializer", "=", "initializer", ",", "\n", "regularizer", "=", "regularizer", ",", "reuse", "=", "reuse", ",", "\n", "custom_getter", "=", "custom_getter", ",", "dtype", "=", "dtype", ")", ":", "\n", "                ", "loss", "=", "model_graph", "(", "features", ",", "\"train\"", ",", "params", ")", "\n", "return", "loss", "\n", "\n", "", "", "return", "training_fn", "\n", "\n", "", "def", "get_evaluation_func", "(", "self", ")", ":", "\n", "        ", "def", "evaluation_fn", "(", "features", ",", "params", "=", "None", ")", ":", "\n", "            ", "if", "params", "is", "None", ":", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.transformer._layer_process": [[18, 25], ["thumt.nn.layer_norm", "ValueError"], "function", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.layers.nn.layer_norm"], ["def", "_layer_process", "(", "x", ",", "mode", ")", ":", "\n", "    ", "if", "not", "mode", "or", "mode", "==", "\"none\"", ":", "\n", "        ", "return", "x", "\n", "", "elif", "mode", "==", "\"layer_norm\"", ":", "\n", "        ", "return", "layers", ".", "nn", ".", "layer_norm", "(", "x", ",", "epsilon", "=", "1e-12", ")", "\n", "", "else", ":", "\n", "        ", "raise", "ValueError", "(", "\"Unknown mode %s\"", "%", "mode", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.transformer._residual_fn": [[27, 31], ["tensorflow.nn.dropout"], "function", ["None"], ["", "", "def", "_residual_fn", "(", "x", ",", "y", ",", "keep_prob", "=", "None", ")", ":", "\n", "    ", "if", "keep_prob", "and", "keep_prob", "<", "1.0", ":", "\n", "        ", "y", "=", "tf", ".", "nn", ".", "dropout", "(", "y", ",", "keep_prob", ")", "\n", "", "return", "x", "+", "y", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.transformer._ffn_layer": [[33, 48], ["tensorflow.variable_scope", "tensorflow.variable_scope", "thumt.nn.linear", "tensorflow.nn.relu", "tensorflow.nn.dropout", "tensorflow.variable_scope", "thumt.nn.linear"], "function", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.layers.nn.linear", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.layers.nn.linear"], ["", "def", "_ffn_layer", "(", "inputs", ",", "hidden_size", ",", "output_size", ",", "keep_prob", "=", "None", ",", "\n", "dtype", "=", "None", ",", "scope", "=", "None", ")", ":", "\n", "    ", "with", "tf", ".", "variable_scope", "(", "scope", ",", "default_name", "=", "\"ffn_layer\"", ",", "values", "=", "[", "inputs", "]", ",", "\n", "dtype", "=", "dtype", ")", ":", "\n", "        ", "with", "tf", ".", "variable_scope", "(", "\"input_layer\"", ")", ":", "\n", "            ", "hidden", "=", "layers", ".", "nn", ".", "linear", "(", "inputs", ",", "hidden_size", ",", "True", ",", "True", ")", "\n", "hidden", "=", "tf", ".", "nn", ".", "relu", "(", "hidden", ")", "\n", "\n", "", "if", "keep_prob", "and", "keep_prob", "<", "1.0", ":", "\n", "            ", "hidden", "=", "tf", ".", "nn", ".", "dropout", "(", "hidden", ",", "keep_prob", ")", "\n", "\n", "", "with", "tf", ".", "variable_scope", "(", "\"output_layer\"", ")", ":", "\n", "            ", "output", "=", "layers", ".", "nn", ".", "linear", "(", "hidden", ",", "output_size", ",", "True", ",", "True", ")", "\n", "\n", "", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.transformer.transformer_encoder": [[50, 88], ["tensorflow.variable_scope", "range", "transformer._layer_process", "tensorflow.variable_scope", "tensorflow.variable_scope", "thumt.attention.multihead_attention", "transformer._residual_fn", "transformer._layer_process", "tensorflow.variable_scope", "transformer._ffn_layer", "transformer._residual_fn", "transformer._layer_process", "transformer._layer_process", "transformer._layer_process"], "function", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.transformer._layer_process", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.layers.attention.multihead_attention", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.transformer._residual_fn", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.transformer._layer_process", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.transformer._ffn_layer", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.transformer._residual_fn", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.transformer._layer_process", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.transformer._layer_process", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.transformer._layer_process"], ["", "", "def", "transformer_encoder", "(", "inputs", ",", "bias", ",", "params", ",", "dtype", "=", "None", ",", "scope", "=", "None", ")", ":", "\n", "    ", "with", "tf", ".", "variable_scope", "(", "scope", ",", "default_name", "=", "\"encoder\"", ",", "dtype", "=", "dtype", ",", "\n", "values", "=", "[", "inputs", ",", "bias", "]", ")", ":", "\n", "        ", "x", "=", "inputs", "\n", "for", "layer", "in", "range", "(", "params", ".", "num_encoder_layers", ")", ":", "\n", "            ", "with", "tf", ".", "variable_scope", "(", "\"layer_%d\"", "%", "layer", ")", ":", "\n", "                ", "with", "tf", ".", "variable_scope", "(", "\"self_attention\"", ")", ":", "\n", "                    ", "max_relative_dis", "=", "params", ".", "max_relative_dis", "if", "params", ".", "position_info_type", "==", "'relative'", "else", "None", "\n", "\n", "y", "=", "layers", ".", "attention", ".", "multihead_attention", "(", "\n", "_layer_process", "(", "x", ",", "params", ".", "layer_preprocess", ")", ",", "\n", "None", ",", "\n", "bias", ",", "\n", "params", ".", "num_heads", ",", "\n", "params", ".", "attention_key_channels", "or", "params", ".", "hidden_size", ",", "\n", "params", ".", "attention_value_channels", "or", "params", ".", "hidden_size", ",", "\n", "params", ".", "hidden_size", ",", "\n", "1.0", "-", "params", ".", "attention_dropout", ",", "\n", "max_relative_dis", "=", "max_relative_dis", ",", "\n", ")", "\n", "y", "=", "y", "[", "\"outputs\"", "]", "\n", "x", "=", "_residual_fn", "(", "x", ",", "y", ",", "1.0", "-", "params", ".", "residual_dropout", ")", "\n", "x", "=", "_layer_process", "(", "x", ",", "params", ".", "layer_postprocess", ")", "\n", "\n", "", "with", "tf", ".", "variable_scope", "(", "\"feed_forward\"", ")", ":", "\n", "                    ", "y", "=", "_ffn_layer", "(", "\n", "_layer_process", "(", "x", ",", "params", ".", "layer_preprocess", ")", ",", "\n", "params", ".", "filter_size", ",", "\n", "params", ".", "hidden_size", ",", "\n", "1.0", "-", "params", ".", "relu_dropout", ",", "\n", ")", "\n", "x", "=", "_residual_fn", "(", "x", ",", "y", ",", "1.0", "-", "params", ".", "residual_dropout", ")", "\n", "x", "=", "_layer_process", "(", "x", ",", "params", ".", "layer_postprocess", ")", "\n", "\n", "", "", "", "outputs", "=", "_layer_process", "(", "x", ",", "params", ".", "layer_preprocess", ")", "\n", "\n", "return", "outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.transformer.transformer_decoder": [[90, 156], ["tensorflow.variable_scope", "range", "transformer._layer_process", "tensorflow.variable_scope", "tensorflow.variable_scope", "thumt.attention.multihead_attention", "transformer._residual_fn", "transformer._layer_process", "tensorflow.variable_scope", "thumt.attention.multihead_attention", "transformer._residual_fn", "transformer._layer_process", "tensorflow.variable_scope", "transformer._ffn_layer", "transformer._residual_fn", "transformer._layer_process", "transformer._layer_process", "transformer._layer_process", "transformer._layer_process"], "function", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.transformer._layer_process", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.layers.attention.multihead_attention", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.transformer._residual_fn", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.transformer._layer_process", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.layers.attention.multihead_attention", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.transformer._residual_fn", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.transformer._layer_process", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.transformer._ffn_layer", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.transformer._residual_fn", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.transformer._layer_process", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.transformer._layer_process", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.transformer._layer_process", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.transformer._layer_process"], ["", "", "def", "transformer_decoder", "(", "inputs", ",", "memory", ",", "bias", ",", "mem_bias", ",", "params", ",", "state", "=", "None", ",", "\n", "dtype", "=", "None", ",", "scope", "=", "None", ",", "reuse", "=", "tf", ".", "AUTO_REUSE", ")", ":", "\n", "    ", "with", "tf", ".", "variable_scope", "(", "scope", ",", "default_name", "=", "\"decoder\"", ",", "dtype", "=", "dtype", ",", "\n", "values", "=", "[", "inputs", ",", "memory", ",", "bias", ",", "mem_bias", "]", ",", "reuse", "=", "reuse", ")", ":", "\n", "        ", "x", "=", "inputs", "\n", "next_state", "=", "{", "}", "\n", "for", "layer", "in", "range", "(", "params", ".", "num_decoder_layers", ")", ":", "\n", "            ", "layer_name", "=", "\"layer_%d\"", "%", "layer", "\n", "with", "tf", ".", "variable_scope", "(", "layer_name", ")", ":", "\n", "                ", "layer_state", "=", "state", "[", "layer_name", "]", "if", "state", "is", "not", "None", "else", "None", "\n", "max_relative_dis", "=", "params", ".", "max_relative_dis", "if", "params", ".", "position_info_type", "==", "'relative'", "else", "None", "\n", "\n", "with", "tf", ".", "variable_scope", "(", "\"self_attention\"", ")", ":", "\n", "                    ", "y", "=", "layers", ".", "attention", ".", "multihead_attention", "(", "\n", "_layer_process", "(", "x", ",", "params", ".", "layer_preprocess", ")", ",", "\n", "None", ",", "\n", "bias", ",", "\n", "params", ".", "num_heads", ",", "\n", "params", ".", "attention_key_channels", "or", "params", ".", "hidden_size", ",", "\n", "params", ".", "attention_value_channels", "or", "params", ".", "hidden_size", ",", "\n", "params", ".", "hidden_size", ",", "\n", "1.0", "-", "params", ".", "attention_dropout", ",", "\n", "state", "=", "layer_state", ",", "\n", "max_relative_dis", "=", "max_relative_dis", ",", "\n", ")", "\n", "\n", "if", "layer_state", "is", "not", "None", ":", "\n", "                        ", "next_state", "[", "layer_name", "]", "=", "y", "[", "\"state\"", "]", "\n", "\n", "", "y", "=", "y", "[", "\"outputs\"", "]", "\n", "x", "=", "_residual_fn", "(", "x", ",", "y", ",", "1.0", "-", "params", ".", "residual_dropout", ")", "\n", "x", "=", "_layer_process", "(", "x", ",", "params", ".", "layer_postprocess", ")", "\n", "\n", "", "with", "tf", ".", "variable_scope", "(", "\"encdec_attention\"", ")", ":", "\n", "                    ", "y", "=", "layers", ".", "attention", ".", "multihead_attention", "(", "\n", "_layer_process", "(", "x", ",", "params", ".", "layer_preprocess", ")", ",", "\n", "memory", ",", "\n", "mem_bias", ",", "\n", "params", ".", "num_heads", ",", "\n", "params", ".", "attention_key_channels", "or", "params", ".", "hidden_size", ",", "\n", "params", ".", "attention_value_channels", "or", "params", ".", "hidden_size", ",", "\n", "params", ".", "hidden_size", ",", "\n", "1.0", "-", "params", ".", "attention_dropout", ",", "\n", "max_relative_dis", "=", "max_relative_dis", ",", "\n", ")", "\n", "y", "=", "y", "[", "\"outputs\"", "]", "\n", "x", "=", "_residual_fn", "(", "x", ",", "y", ",", "1.0", "-", "params", ".", "residual_dropout", ")", "\n", "x", "=", "_layer_process", "(", "x", ",", "params", ".", "layer_postprocess", ")", "\n", "\n", "", "with", "tf", ".", "variable_scope", "(", "\"feed_forward\"", ")", ":", "\n", "                    ", "y", "=", "_ffn_layer", "(", "\n", "_layer_process", "(", "x", ",", "params", ".", "layer_preprocess", ")", ",", "\n", "params", ".", "filter_size", ",", "\n", "params", ".", "hidden_size", ",", "\n", "1.0", "-", "params", ".", "relu_dropout", ",", "\n", ")", "\n", "x", "=", "_residual_fn", "(", "x", ",", "y", ",", "1.0", "-", "params", ".", "residual_dropout", ")", "\n", "x", "=", "_layer_process", "(", "x", ",", "params", ".", "layer_postprocess", ")", "\n", "\n", "", "", "", "outputs", "=", "_layer_process", "(", "x", ",", "params", ".", "layer_preprocess", ")", "\n", "\n", "if", "state", "is", "not", "None", ":", "\n", "            ", "return", "outputs", ",", "next_state", "\n", "\n", "", "return", "outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.transformer.encoding_graph": [[158, 208], ["tensorflow.sequence_mask", "len", "tensorflow.random_normal_initializer", "tensorflow.get_variable", "tensorflow.gather", "tensorflow.nn.bias_add", "thumt.attention.attention_bias", "thumt.attention.add_timing_signal", "transformer.transformer_encoder", "tensorflow.get_variable_scope", "tensorflow.get_variable", "tensorflow.get_variable", "tensorflow.expand_dims", "tensorflow.nn.dropout", "tensorflow.shape"], "function", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.layers.attention.attention_bias", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.layers.attention.add_timing_signal", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.transformer_lrp.transformer_encoder"], ["", "", "def", "encoding_graph", "(", "features", ",", "mode", ",", "params", ")", ":", "\n", "    ", "if", "mode", "!=", "\"train\"", ":", "\n", "        ", "params", ".", "residual_dropout", "=", "0.0", "\n", "params", ".", "attention_dropout", "=", "0.0", "\n", "params", ".", "relu_dropout", "=", "0.0", "\n", "params", ".", "label_smoothing", "=", "0.0", "\n", "\n", "", "dtype", "=", "tf", ".", "get_variable_scope", "(", ")", ".", "dtype", "\n", "hidden_size", "=", "params", ".", "hidden_size", "\n", "src_seq", "=", "features", "[", "\"source\"", "]", "\n", "src_len", "=", "features", "[", "\"source_length\"", "]", "\n", "src_mask", "=", "tf", ".", "sequence_mask", "(", "src_len", ",", "\n", "maxlen", "=", "tf", ".", "shape", "(", "features", "[", "\"source\"", "]", ")", "[", "1", "]", ",", "\n", "dtype", "=", "dtype", "or", "tf", ".", "float32", ")", "\n", "\n", "svocab", "=", "params", ".", "vocabulary", "[", "\"source\"", "]", "\n", "src_vocab_size", "=", "len", "(", "svocab", ")", "\n", "initializer", "=", "tf", ".", "random_normal_initializer", "(", "0.0", ",", "params", ".", "hidden_size", "**", "-", "0.5", ")", "\n", "\n", "if", "params", ".", "shared_source_target_embedding", ":", "\n", "        ", "src_embedding", "=", "tf", ".", "get_variable", "(", "\"weights\"", ",", "\n", "[", "src_vocab_size", ",", "hidden_size", "]", ",", "\n", "initializer", "=", "initializer", ")", "\n", "", "else", ":", "\n", "        ", "src_embedding", "=", "tf", ".", "get_variable", "(", "\"source_embedding\"", ",", "\n", "[", "src_vocab_size", ",", "hidden_size", "]", ",", "\n", "initializer", "=", "initializer", ")", "\n", "\n", "", "bias", "=", "tf", ".", "get_variable", "(", "\"bias\"", ",", "[", "hidden_size", "]", ")", "\n", "\n", "inputs", "=", "tf", ".", "gather", "(", "src_embedding", ",", "src_seq", ")", "\n", "\n", "if", "params", ".", "multiply_embedding_mode", "==", "\"sqrt_depth\"", ":", "\n", "        ", "inputs", "=", "inputs", "*", "(", "hidden_size", "**", "0.5", ")", "\n", "\n", "", "inputs", "=", "inputs", "*", "tf", ".", "expand_dims", "(", "src_mask", ",", "-", "1", ")", "\n", "\n", "encoder_input", "=", "tf", ".", "nn", ".", "bias_add", "(", "inputs", ",", "bias", ")", "\n", "enc_attn_bias", "=", "layers", ".", "attention", ".", "attention_bias", "(", "src_mask", ",", "\"masking\"", ",", "\n", "dtype", "=", "dtype", ")", "\n", "#if params.position_info_type == 'absolute':", "\n", "encoder_input", "=", "layers", ".", "attention", ".", "add_timing_signal", "(", "encoder_input", ")", "\n", "\n", "if", "params", ".", "residual_dropout", ":", "\n", "        ", "keep_prob", "=", "1.0", "-", "params", ".", "residual_dropout", "\n", "encoder_input", "=", "tf", ".", "nn", ".", "dropout", "(", "encoder_input", ",", "keep_prob", ")", "\n", "\n", "", "encoder_output", "=", "transformer_encoder", "(", "encoder_input", ",", "enc_attn_bias", ",", "params", ")", "\n", "\n", "return", "encoder_output", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.transformer.decoding_graph": [[210, 310], ["tensorflow.sequence_mask", "tensorflow.sequence_mask", "len", "tensorflow.random_normal_initializer", "tensorflow.gather", "thumt.attention.attention_bias", "thumt.attention.attention_bias", "thumt.attention.add_timing_signal", "tensorflow.reshape", "tensorflow.matmul", "thumt.smoothed_softmax_cross_entropy_with_logits", "tensorflow.cast", "tensorflow.reshape", "tensorflow.get_variable_scope", "tensorflow.get_variable", "tensorflow.get_variable", "tensorflow.expand_dims", "tensorflow.pad", "tensorflow.nn.dropout", "transformer.transformer_decoder", "transformer.transformer_decoder", "tensorflow.matmul", "tensorflow.nn.log_softmax", "tensorflow.shape", "tensorflow.reduce_sum", "tensorflow.reduce_sum", "tensorflow.variable_scope", "tensorflow.get_variable", "tensorflow.shape", "tensorflow.shape", "tensorflow.shape", "tensorflow.get_variable_scope"], "function", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.layers.attention.attention_bias", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.layers.attention.attention_bias", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.layers.attention.add_timing_signal", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.losses.losses.smoothed_softmax_cross_entropy_with_logits", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.transformer_lrp.transformer_decoder", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.transformer_lrp.transformer_decoder"], ["", "def", "decoding_graph", "(", "features", ",", "state", ",", "mode", ",", "params", ")", ":", "\n", "    ", "if", "mode", "!=", "\"train\"", ":", "\n", "        ", "params", ".", "residual_dropout", "=", "0.0", "\n", "params", ".", "attention_dropout", "=", "0.0", "\n", "params", ".", "relu_dropout", "=", "0.0", "\n", "params", ".", "label_smoothing", "=", "0.0", "\n", "\n", "", "dtype", "=", "tf", ".", "get_variable_scope", "(", ")", ".", "dtype", "\n", "tgt_seq", "=", "features", "[", "\"target\"", "]", "\n", "src_len", "=", "features", "[", "\"source_length\"", "]", "\n", "tgt_len", "=", "features", "[", "\"target_length\"", "]", "\n", "src_mask", "=", "tf", ".", "sequence_mask", "(", "src_len", ",", "\n", "maxlen", "=", "tf", ".", "shape", "(", "features", "[", "\"source\"", "]", ")", "[", "1", "]", ",", "\n", "dtype", "=", "dtype", "or", "tf", ".", "float32", ")", "\n", "tgt_mask", "=", "tf", ".", "sequence_mask", "(", "tgt_len", ",", "\n", "maxlen", "=", "tf", ".", "shape", "(", "features", "[", "\"target\"", "]", ")", "[", "1", "]", ",", "\n", "dtype", "=", "dtype", "or", "tf", ".", "float32", ")", "\n", "\n", "hidden_size", "=", "params", ".", "hidden_size", "\n", "tvocab", "=", "params", ".", "vocabulary", "[", "\"target\"", "]", "\n", "tgt_vocab_size", "=", "len", "(", "tvocab", ")", "\n", "initializer", "=", "tf", ".", "random_normal_initializer", "(", "0.0", ",", "params", ".", "hidden_size", "**", "-", "0.5", ")", "\n", "\n", "if", "params", ".", "shared_source_target_embedding", ":", "\n", "        ", "with", "tf", ".", "variable_scope", "(", "tf", ".", "get_variable_scope", "(", ")", ",", "reuse", "=", "True", ")", ":", "\n", "            ", "tgt_embedding", "=", "tf", ".", "get_variable", "(", "\"weights\"", ",", "\n", "[", "tgt_vocab_size", ",", "hidden_size", "]", ",", "\n", "initializer", "=", "initializer", ")", "\n", "", "", "else", ":", "\n", "        ", "tgt_embedding", "=", "tf", ".", "get_variable", "(", "\"target_embedding\"", ",", "\n", "[", "tgt_vocab_size", ",", "hidden_size", "]", ",", "\n", "initializer", "=", "initializer", ")", "\n", "\n", "", "if", "params", ".", "shared_embedding_and_softmax_weights", ":", "\n", "        ", "weights", "=", "tgt_embedding", "\n", "", "else", ":", "\n", "        ", "weights", "=", "tf", ".", "get_variable", "(", "\"softmax\"", ",", "[", "tgt_vocab_size", ",", "hidden_size", "]", ",", "\n", "initializer", "=", "initializer", ")", "\n", "\n", "", "targets", "=", "tf", ".", "gather", "(", "tgt_embedding", ",", "tgt_seq", ")", "\n", "\n", "if", "params", ".", "multiply_embedding_mode", "==", "\"sqrt_depth\"", ":", "\n", "        ", "targets", "=", "targets", "*", "(", "hidden_size", "**", "0.5", ")", "\n", "\n", "", "targets", "=", "targets", "*", "tf", ".", "expand_dims", "(", "tgt_mask", ",", "-", "1", ")", "\n", "\n", "enc_attn_bias", "=", "layers", ".", "attention", ".", "attention_bias", "(", "src_mask", ",", "\"masking\"", ",", "\n", "dtype", "=", "dtype", ")", "\n", "dec_attn_bias", "=", "layers", ".", "attention", ".", "attention_bias", "(", "tf", ".", "shape", "(", "targets", ")", "[", "1", "]", ",", "\n", "\"causal\"", ",", "dtype", "=", "dtype", ")", "\n", "# Shift left", "\n", "decoder_input", "=", "tf", ".", "pad", "(", "targets", ",", "[", "[", "0", ",", "0", "]", ",", "[", "1", ",", "0", "]", ",", "[", "0", ",", "0", "]", "]", ")", "[", ":", ",", ":", "-", "1", ",", ":", "]", "\n", "#if params.position_info_type == 'absolute':", "\n", "decoder_input", "=", "layers", ".", "attention", ".", "add_timing_signal", "(", "decoder_input", ")", "\n", "\n", "if", "params", ".", "residual_dropout", ":", "\n", "        ", "keep_prob", "=", "1.0", "-", "params", ".", "residual_dropout", "\n", "decoder_input", "=", "tf", ".", "nn", ".", "dropout", "(", "decoder_input", ",", "keep_prob", ")", "\n", "\n", "", "encoder_output", "=", "state", "[", "\"encoder\"", "]", "\n", "\n", "if", "mode", "!=", "\"infer\"", ":", "\n", "        ", "decoder_output", "=", "transformer_decoder", "(", "decoder_input", ",", "encoder_output", ",", "\n", "dec_attn_bias", ",", "enc_attn_bias", ",", "\n", "params", ",", "scope", "=", "'decoder'", ")", "\n", "", "else", ":", "\n", "# When inference decoder_input is one time step", "\n", "        ", "decoder_input", "=", "decoder_input", "[", ":", ",", "-", "1", ":", ",", ":", "]", "\n", "dec_attn_bias", "=", "dec_attn_bias", "[", ":", ",", ":", ",", "-", "1", ":", ",", ":", "]", "\n", "decoder_outputs", "=", "transformer_decoder", "(", "decoder_input", ",", "encoder_output", ",", "\n", "dec_attn_bias", ",", "enc_attn_bias", ",", "\n", "params", ",", "state", "=", "state", "[", "\"decoder\"", "]", ",", "\n", "scope", "=", "'decoder'", ")", "\n", "\n", "decoder_output", ",", "decoder_state", "=", "decoder_outputs", "\n", "# [batch_size, 1, vocab_size] => [batch_size, vocab_size]", "\n", "decoder_output", "=", "decoder_output", "[", ":", ",", "-", "1", ",", ":", "]", "\n", "logits", "=", "tf", ".", "matmul", "(", "decoder_output", ",", "weights", ",", "False", ",", "True", ")", "\n", "log_prob", "=", "tf", ".", "nn", ".", "log_softmax", "(", "logits", ")", "\n", "\n", "return", "log_prob", ",", "{", "\"encoder\"", ":", "encoder_output", ",", "\"decoder\"", ":", "decoder_state", "}", "\n", "\n", "", "decoder_output", "=", "tf", ".", "reshape", "(", "decoder_output", ",", "[", "-", "1", ",", "hidden_size", "]", ")", "\n", "logits", "=", "tf", ".", "matmul", "(", "decoder_output", ",", "weights", ",", "False", ",", "True", ")", "\n", "labels", "=", "features", "[", "\"target\"", "]", "\n", "\n", "# label smoothing", "\n", "ce", "=", "losses", ".", "smoothed_softmax_cross_entropy_with_logits", "(", "\n", "logits", "=", "logits", ",", "\n", "labels", "=", "labels", ",", "\n", "smoothing", "=", "params", ".", "label_smoothing", ",", "\n", "normalize", "=", "True", "\n", ")", "\n", "tgt_mask", "=", "tf", ".", "cast", "(", "tgt_mask", ",", "ce", ".", "dtype", ")", "\n", "\n", "ce", "=", "tf", ".", "reshape", "(", "ce", ",", "tf", ".", "shape", "(", "tgt_seq", ")", ")", "\n", "\n", "if", "mode", "==", "\"eval\"", ":", "\n", "        ", "return", "-", "tf", ".", "reduce_sum", "(", "ce", "*", "tgt_mask", ",", "axis", "=", "1", ")", "\n", "\n", "", "loss", "=", "tf", ".", "reduce_sum", "(", "ce", "*", "tgt_mask", ")", "/", "tf", ".", "reduce_sum", "(", "tgt_mask", ")", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.transformer.model_graph": [[312, 320], ["transformer.encoding_graph", "transformer.decoding_graph"], "function", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.transformer.encoding_graph", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.transformer.decoding_graph"], ["# bridge layer", "\n", "# [batch_size*length, vocab_size]", "\n", "soft_input", "=", "tf", ".", "nn", ".", "softmax", "(", "logits", ")", "\n", "soft_input", "=", "tf", ".", "stop_gradient", "(", "soft_input", ",", "name", "=", "'stop_grad_softinput'", ")", "\n", "tf", ".", "get_variable_scope", "(", ")", ".", "reuse_variables", "(", ")", "\n", "# [batch_size*length, hidden_size] ", "\n", "soft_input", "=", "tf", ".", "matmul", "(", "soft_input", ",", "tgt_embedding", ")", "\n", "\n", "if", "params", ".", "bridge_input_scale", "==", "\"sqrt_depth\"", ":", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.rnnsearch_lrp.RNNsearchLRP.__init__": [[447, 449], ["thumt.models.model.NMTModel.__init__"], "methods", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.transformer_lrp.TransformerLRP.__init__"], ["    ", "def", "__init__", "(", "self", ",", "params", ",", "scope", "=", "\"rnnsearch\"", ")", ":", "\n", "        ", "super", "(", "RNNsearchLRP", ",", "self", ")", ".", "__init__", "(", "params", "=", "params", ",", "scope", "=", "scope", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.rnnsearch_lrp.RNNsearchLRP.get_training_func": [[450, 460], ["tensorflow.variable_scope", "rnnsearch_lrp.model_graph"], "methods", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.transformer_lrp.model_graph"], ["", "def", "get_training_func", "(", "self", ",", "initializer", ")", ":", "\n", "        ", "def", "training_fn", "(", "features", ",", "params", "=", "None", ")", ":", "\n", "            ", "if", "params", "is", "None", ":", "\n", "                ", "params", "=", "self", ".", "parameters", "\n", "", "with", "tf", ".", "variable_scope", "(", "self", ".", "_scope", ",", "initializer", "=", "initializer", ",", "\n", "reuse", "=", "tf", ".", "AUTO_REUSE", ")", ":", "\n", "                ", "loss", "=", "model_graph", "(", "features", ",", "features", "[", "\"target\"", "]", ",", "params", ")", "[", "0", "]", "\n", "return", "loss", "\n", "\n", "", "", "return", "training_fn", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.rnnsearch_lrp.RNNsearchLRP.get_evaluation_func": [[461, 477], ["copy.copy", "copy.copy", "tensorflow.variable_scope", "rnnsearch_lrp.model_graph"], "methods", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.transformer_lrp.model_graph"], ["", "def", "get_evaluation_func", "(", "self", ")", ":", "\n", "        ", "def", "evaluation_fn", "(", "features", ",", "params", "=", "None", ")", ":", "\n", "            ", "if", "params", "is", "None", ":", "\n", "                ", "params", "=", "copy", ".", "copy", "(", "self", ".", "parameters", ")", "\n", "", "else", ":", "\n", "                ", "params", "=", "copy", ".", "copy", "(", "params", ")", "\n", "", "params", ".", "dropout", "=", "0.0", "\n", "params", ".", "use_variational_dropout", "=", "False", "\n", "params", ".", "label_smoothing", "=", "0.0", "\n", "\n", "with", "tf", ".", "variable_scope", "(", "self", ".", "_scope", ")", ":", "\n", "                ", "logits", "=", "model_graph", "(", "features", ",", "None", ",", "params", ")", "\n", "\n", "", "return", "logits", "\n", "\n", "", "return", "evaluation_fn", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.rnnsearch_lrp.RNNsearchLRP.get_relevance_func": [[478, 494], ["copy.copy", "copy.copy", "tensorflow.variable_scope", "rnnsearch_lrp.model_graph"], "methods", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.transformer_lrp.model_graph"], ["", "def", "get_relevance_func", "(", "self", ")", ":", "\n", "        ", "def", "relevance_fn", "(", "features", ",", "params", "=", "None", ")", ":", "\n", "            ", "if", "params", "is", "None", ":", "\n", "                ", "params", "=", "copy", ".", "copy", "(", "self", ".", "parameters", ")", "\n", "", "else", ":", "\n", "                ", "params", "=", "copy", ".", "copy", "(", "params", ")", "\n", "\n", "", "params", ".", "dropout", "=", "0.0", "\n", "params", ".", "use_variational_dropout", "=", "False", "\n", "params", ".", "label_smoothing", "=", "0.0", "\n", "\n", "with", "tf", ".", "variable_scope", "(", "self", ".", "_scope", ")", ":", "\n", "                ", "loss", ",", "rlv", "=", "model_graph", "(", "features", ",", "features", "[", "\"target\"", "]", ",", "\n", "params", ")", "\n", "return", "features", "[", "\"source\"", "]", ",", "features", "[", "\"target\"", "]", ",", "rlv", ",", "loss", "\n", "", "", "return", "relevance_fn", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.rnnsearch_lrp.RNNsearchLRP.get_inference_func": [[495, 511], ["copy.copy", "copy.copy", "tensorflow.variable_scope", "rnnsearch_lrp.model_graph"], "methods", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.transformer_lrp.model_graph"], ["", "def", "get_inference_func", "(", "self", ")", ":", "\n", "        ", "def", "inference_fn", "(", "features", ",", "params", "=", "None", ")", ":", "\n", "            ", "if", "params", "is", "None", ":", "\n", "                ", "params", "=", "copy", ".", "copy", "(", "self", ".", "parameters", ")", "\n", "", "else", ":", "\n", "                ", "params", "=", "copy", ".", "copy", "(", "params", ")", "\n", "", "params", ".", "dropout", "=", "0.0", "\n", "params", ".", "use_variational_dropout", "=", "False", "\n", "params", ".", "label_smoothing", "=", "0.0", "\n", "\n", "with", "tf", ".", "variable_scope", "(", "self", ".", "_scope", ")", ":", "\n", "                ", "logits", "=", "model_graph", "(", "features", ",", "None", ",", "params", ")", "\n", "\n", "", "return", "logits", "\n", "\n", "", "return", "inference_fn", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.rnnsearch_lrp.RNNsearchLRP.get_name": [[512, 515], ["None"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "get_name", "(", ")", ":", "\n", "        ", "return", "\"rnnsearch\"", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.rnnsearch_lrp.RNNsearchLRP.get_parameters": [[516, 543], ["tensorflow.contrib.training.HParams"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "get_parameters", "(", ")", ":", "\n", "        ", "params", "=", "tf", ".", "contrib", ".", "training", ".", "HParams", "(", "\n", "# vocabulary", "\n", "pad", "=", "\"<pad>\"", ",", "\n", "unk", "=", "\"<unk>\"", ",", "\n", "eos", "=", "\"<eos>\"", ",", "\n", "bos", "=", "\"<eos>\"", ",", "\n", "append_eos", "=", "False", ",", "\n", "# model", "\n", "rnn_cell", "=", "\"LegacyGRUCell\"", ",", "\n", "embedding_size", "=", "620", ",", "\n", "hidden_size", "=", "1000", ",", "\n", "maxnum", "=", "2", ",", "\n", "# regularization", "\n", "dropout", "=", "0.2", ",", "\n", "use_variational_dropout", "=", "False", ",", "\n", "label_smoothing", "=", "0.1", ",", "\n", "constant_batch_size", "=", "True", ",", "\n", "batch_size", "=", "128", ",", "\n", "max_length", "=", "60", ",", "\n", "clip_grad_norm", "=", "5.0", ",", "\n", "#lrp", "\n", "stab", "=", "0.05", "\n", ")", "\n", "\n", "return", "params", "\n", "", "", ""]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.rnnsearch_lrp.normalize": [[18, 27], ["tensorflow.abs", "tensorflow.reduce_sum", "tensorflow.abs", "tensorflow.reduce_sum", "tensorflow.expand_dims", "tensorflow.expand_dims"], "function", ["None"], ["def", "normalize", "(", "matrix", ",", "negative", "=", "False", ")", ":", "\n", "    ", "if", "negative", ":", "\n", "        ", "matrix_abs", "=", "tf", ".", "abs", "(", "matrix", ")", "\n", "total", "=", "tf", ".", "reduce_sum", "(", "matrix_abs", ",", "-", "1", ")", "\n", "return", "matrix", "/", "tf", ".", "expand_dims", "(", "total", ",", "-", "1", ")", "\n", "", "else", ":", "\n", "        ", "matrix", "=", "tf", ".", "abs", "(", "matrix", ")", "\n", "total", "=", "tf", ".", "reduce_sum", "(", "matrix", ",", "-", "1", ")", "\n", "return", "matrix", "/", "tf", ".", "expand_dims", "(", "total", ",", "-", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.rnnsearch_lrp.stabilize": [[29, 36], ["tensorflow.sign", "tensorflow.equal", "tensorflow.cast", "tensorflow.zeros", "tensorflow.shape"], "function", ["None"], ["", "", "def", "stabilize", "(", "matrix", ",", "stab", ")", ":", "\n", "    ", "sign", "=", "tf", ".", "sign", "(", "matrix", ")", "\n", "zero_pos", "=", "tf", ".", "equal", "(", "sign", ",", "tf", ".", "zeros", "(", "tf", ".", "shape", "(", "sign", ")", ")", ")", "\n", "zero_pos", "=", "tf", ".", "cast", "(", "zero_pos", ",", "tf", ".", "float32", ")", "\n", "sign", "+=", "zero_pos", "\n", "result", "=", "matrix", "+", "stab", "*", "sign", "\n", "return", "result", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.rnnsearch_lrp._copy_through": [[38, 41], ["tensorflow.where"], "function", ["None"], ["", "def", "_copy_through", "(", "time", ",", "length", ",", "output", ",", "new_output", ")", ":", "\n", "    ", "copy_cond", "=", "(", "time", ">=", "length", ")", "\n", "return", "tf", ".", "where", "(", "copy_cond", ",", "output", ",", "new_output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.rnnsearch_lrp._gru_encoder": [[43, 101], ["tensorflow.zeros", "tensorflow.TensorArray", "tensorflow.TensorArray", "tensorflow.TensorArray", "tensorflow.zeros", "input_ta.unstack.unstack", "tensorflow.constant", "tensorflow.while_loop", "output_final_ta.stack", "tf.transpose.set_shape", "tensorflow.transpose", "w_x_h_final_ta.stack", "tensorflow.shape", "tensorflow.shape", "cell.zero_state", "tensorflow.transpose", "input_ta.unstack.read", "cell", "tensorflow.pad", "rnnsearch_lrp._copy_through", "rnnsearch_lrp._copy_through", "rnnsearch_lrp._copy_through", "out_ta.write.write", "wxh_ta.write.write"], "function", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.rnnsearch_lrp._copy_through", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.rnnsearch_lrp._copy_through", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.rnnsearch_lrp._copy_through"], ["", "def", "_gru_encoder", "(", "cell", ",", "inputs", ",", "sequence_length", ",", "initial_state", ",", "params", ",", "\n", "dtype", "=", "None", ")", ":", "\n", "# Assume that the underlying cell is GRUCell-like", "\n", "    ", "output_size", "=", "cell", ".", "output_size", "\n", "dtype", "=", "dtype", "or", "inputs", ".", "dtype", "\n", "\n", "batch", "=", "tf", ".", "shape", "(", "inputs", ")", "[", "0", "]", "\n", "time_steps", "=", "tf", ".", "shape", "(", "inputs", ")", "[", "1", "]", "\n", "\n", "zero_output", "=", "tf", ".", "zeros", "(", "[", "batch", ",", "output_size", "]", ",", "dtype", ")", "\n", "\n", "if", "initial_state", "is", "None", ":", "\n", "        ", "initial_state", "=", "cell", ".", "zero_state", "(", "batch", ",", "dtype", ")", "\n", "\n", "", "input_ta", "=", "tf", ".", "TensorArray", "(", "dtype", ",", "time_steps", ",", "\n", "tensor_array_name", "=", "\"input_array\"", ")", "\n", "output_ta", "=", "tf", ".", "TensorArray", "(", "dtype", ",", "time_steps", ",", "\n", "tensor_array_name", "=", "\"output_array\"", ")", "\n", "\n", "w_x_h_ta", "=", "tf", ".", "TensorArray", "(", "dtype", ",", "time_steps", ",", "\n", "tensor_array_name", "=", "\"w_x_h_array\"", ")", "\n", "w_x_h_init", "=", "tf", ".", "zeros", "(", "[", "batch", ",", "time_steps", ",", "output_size", "]", ",", "dtype", "=", "tf", ".", "float32", ")", "\n", "\n", "input_ta", "=", "input_ta", ".", "unstack", "(", "tf", ".", "transpose", "(", "inputs", ",", "[", "1", ",", "0", ",", "2", "]", ")", ")", "\n", "\n", "def", "loop_func", "(", "t", ",", "out_ta", ",", "state", ",", "wxh_ta", ",", "w_x_h_last", ")", ":", "\n", "        ", "inp_t", "=", "input_ta", ".", "read", "(", "t", ")", "\n", "cell_output", ",", "new_state", ",", "w_xlast_newh", ",", "w_x_newh", "=", "cell", "(", "inp_t", ",", "state", ",", "\n", "w_x_h_last", ",", "\n", "params", ")", "\n", "w_x_newh", "=", "tf", ".", "pad", "(", "w_x_newh", ",", "[", "[", "0", ",", "0", "]", ",", "[", "t", ",", "time_steps", "-", "t", "-", "1", "]", ",", "[", "0", ",", "0", "]", "]", ")", "\n", "w_x_h_new", "=", "w_xlast_newh", "+", "w_x_newh", "\n", "cell_output", "=", "_copy_through", "(", "t", ",", "sequence_length", ",", "zero_output", ",", "\n", "cell_output", ")", "\n", "new_state", "=", "_copy_through", "(", "t", ",", "sequence_length", ",", "state", ",", "new_state", ")", "\n", "w_x_h_new", "=", "_copy_through", "(", "t", ",", "sequence_length", ",", "w_x_h_last", ",", "w_x_h_new", ")", "\n", "out_ta", "=", "out_ta", ".", "write", "(", "t", ",", "cell_output", ")", "\n", "wxh_ta", "=", "wxh_ta", ".", "write", "(", "t", ",", "w_x_h_new", ")", "\n", "return", "t", "+", "1", ",", "out_ta", ",", "new_state", ",", "wxh_ta", ",", "w_x_h_new", "\n", "\n", "", "time", "=", "tf", ".", "constant", "(", "0", ",", "dtype", "=", "tf", ".", "int32", ",", "name", "=", "\"time\"", ")", "\n", "loop_vars", "=", "(", "time", ",", "output_ta", ",", "initial_state", ",", "w_x_h_ta", ",", "w_x_h_init", ")", "\n", "\n", "outputs", "=", "tf", ".", "while_loop", "(", "lambda", "t", ",", "*", "_", ":", "t", "<", "time_steps", ",", "loop_func", ",", "\n", "loop_vars", ",", "parallel_iterations", "=", "32", ",", "\n", "swap_memory", "=", "True", ")", "\n", "\n", "output_final_ta", "=", "outputs", "[", "1", "]", "\n", "final_state", "=", "outputs", "[", "2", "]", "\n", "\n", "all_output", "=", "output_final_ta", ".", "stack", "(", ")", "\n", "all_output", ".", "set_shape", "(", "[", "None", ",", "None", ",", "output_size", "]", ")", "\n", "all_output", "=", "tf", ".", "transpose", "(", "all_output", ",", "[", "1", ",", "0", ",", "2", "]", ")", "\n", "\n", "w_x_h_final_ta", "=", "outputs", "[", "3", "]", "\n", "w_x_h_final", "=", "w_x_h_final_ta", ".", "stack", "(", ")", "\n", "\n", "return", "all_output", ",", "final_state", ",", "w_x_h_final", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.rnnsearch_lrp._encoder": [[103, 137], ["tensorflow.variable_scope", "tensorflow.reverse_sequence", "tensorflow.variable_scope", "rnnsearch_lrp._gru_encoder", "tensorflow.variable_scope", "rnnsearch_lrp._gru_encoder", "tensorflow.reverse_sequence", "tensorflow.concat"], "function", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.rnnsearch_lrp._gru_encoder", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.rnnsearch_lrp._gru_encoder"], ["", "def", "_encoder", "(", "cell_fw", ",", "cell_bw", ",", "inputs", ",", "sequence_length", ",", "params", ",", "dtype", "=", "None", ",", "\n", "scope", "=", "None", ")", ":", "\n", "    ", "with", "tf", ".", "variable_scope", "(", "scope", "or", "\"encoder\"", ",", "\n", "values", "=", "[", "inputs", ",", "sequence_length", "]", ")", ":", "\n", "        ", "inputs_fw", "=", "inputs", "\n", "inputs_bw", "=", "tf", ".", "reverse_sequence", "(", "inputs", ",", "sequence_length", ",", "\n", "batch_axis", "=", "0", ",", "seq_axis", "=", "1", ")", "\n", "\n", "with", "tf", ".", "variable_scope", "(", "\"forward\"", ")", ":", "\n", "            ", "output_fw", ",", "state_fw", ",", "w_x_h_fw", "=", "_gru_encoder", "(", "cell_fw", ",", "inputs_fw", ",", "\n", "sequence_length", ",", "None", ",", "params", ",", "\n", "dtype", "=", "dtype", ")", "\n", "\n", "", "with", "tf", ".", "variable_scope", "(", "\"backward\"", ")", ":", "\n", "            ", "output_bw", ",", "state_bw", ",", "w_x_h_bw", "=", "_gru_encoder", "(", "cell_bw", ",", "inputs_bw", ",", "\n", "sequence_length", ",", "None", ",", "params", ",", "\n", "dtype", "=", "dtype", ")", "\n", "output_bw", "=", "tf", ".", "reverse_sequence", "(", "output_bw", ",", "sequence_length", ",", "\n", "batch_axis", "=", "0", ",", "seq_axis", "=", "1", ")", "\n", "\n", "", "results", "=", "{", "\n", "\"annotation\"", ":", "tf", ".", "concat", "(", "[", "output_fw", ",", "output_bw", "]", ",", "axis", "=", "2", ")", ",", "\n", "\"outputs\"", ":", "{", "\n", "\"forward\"", ":", "output_fw", ",", "\n", "\"backward\"", ":", "output_bw", "\n", "}", ",", "\n", "\"final_states\"", ":", "{", "\n", "\"forward\"", ":", "state_fw", ",", "\n", "\"backward\"", ":", "state_bw", "\n", "}", ",", "\n", "\"weight_ratios\"", ":", "[", "w_x_h_fw", ",", "w_x_h_bw", "]", "\n", "}", "\n", "\n", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.rnnsearch_lrp._decoder": [[139, 270], ["tensorflow.zeros", "tensorflow.zeros", "tensorflow.shape", "tensorflow.shape", "tensorflow.variable_scope", "tensorflow.transpose", "tensorflow.sequence_mask", "thumt.attention.attention_bias", "tensorflow.squeeze", "thumt.attention.attention", "tensorflow.TensorArray", "tensorflow.TensorArray", "tensorflow.TensorArray", "tensorflow.TensorArray", "input_ta.unstack.unstack", "tensorflow.TensorArray", "w_x_bw_ta.unstack.unstack", "tensorflow.transpose", "tensorflow.reshape", "tensorflow.TensorArray", "tensorflow.TensorArray", "thumt.linear_v2n", "tensorflow.tanh", "tensorflow.constant", "tensorflow.while_loop", "output_final_ta.stack", "tf.transpose.set_shape", "tensorflow.transpose", "value_final_ta.stack", "tf.transpose.set_shape", "tensorflow.transpose", "w_x_h_final_ta.stack", "w_x_c_final_ta.stack", "tensorflow.shape", "tensorflow.shape", "tensorflow.concat", "wxh_ta.write.write", "input_ta.unstack.read", "thumt.attention.attention", "tensorflow.expand_dims", "tensorflow.squeeze", "rnnsearch_lrp.stabilize", "tensorflow.expand_dims", "tensorflow.reduce_sum", "tensorflow.reshape", "wxc_ta.write.write", "cell", "rnnsearch_lrp._copy_through", "rnnsearch_lrp._copy_through", "rnnsearch_lrp._copy_through", "rnnsearch_lrp._copy_through", "out_ta.write.write", "att_ta.write.write", "val_ta.write.write", "tensorflow.identity", "w_x_bw_ta.unstack.read", "tensorflow.expand_dims", "tensorflow.expand_dims", "tensorflow.shape", "tensorflow.reshape", "tensorflow.shape", "tensorflow.shape"], "function", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.layers.attention.attention_bias", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.layers.attention.attention", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.lrp.linear_v2n", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.layers.attention.attention", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.rnnsearch_lrp.stabilize", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.rnnsearch_lrp._copy_through", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.rnnsearch_lrp._copy_through", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.rnnsearch_lrp._copy_through", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.rnnsearch_lrp._copy_through"], ["", "", "def", "_decoder", "(", "cell", ",", "inputs", ",", "memory", ",", "sequence_length", ",", "initial_state", ",", "w_x_enc", ",", "\n", "w_x_bw", ",", "params", ",", "dtype", "=", "None", ",", "scope", "=", "None", ")", ":", "\n", "# Assume that the underlying cell is GRUCell-like", "\n", "    ", "batch", "=", "tf", ".", "shape", "(", "inputs", ")", "[", "0", "]", "\n", "time_steps", "=", "tf", ".", "shape", "(", "inputs", ")", "[", "1", "]", "\n", "dtype", "=", "dtype", "or", "inputs", ".", "dtype", "\n", "output_size", "=", "cell", ".", "output_size", "\n", "zero_output", "=", "tf", ".", "zeros", "(", "[", "batch", ",", "output_size", "]", ",", "dtype", ")", "\n", "zero_value", "=", "tf", ".", "zeros", "(", "[", "batch", ",", "memory", ".", "shape", "[", "-", "1", "]", ".", "value", "]", ",", "dtype", ")", "\n", "\n", "with", "tf", ".", "variable_scope", "(", "scope", "or", "\"decoder\"", ",", "dtype", "=", "dtype", ")", ":", "\n", "        ", "inputs", "=", "tf", ".", "transpose", "(", "inputs", ",", "[", "1", ",", "0", ",", "2", "]", ")", "\n", "mem_mask", "=", "tf", ".", "sequence_mask", "(", "sequence_length", "[", "\"source\"", "]", ",", "\n", "maxlen", "=", "tf", ".", "shape", "(", "memory", ")", "[", "1", "]", ",", "\n", "dtype", "=", "tf", ".", "float32", ")", "\n", "bias", "=", "layers", ".", "attention", ".", "attention_bias", "(", "mem_mask", ",", "\"masking\"", ")", "\n", "bias", "=", "tf", ".", "squeeze", "(", "bias", ",", "axis", "=", "[", "1", ",", "2", "]", ")", "\n", "cache", "=", "layers", ".", "attention", ".", "attention", "(", "None", ",", "memory", ",", "None", ",", "output_size", ")", "\n", "\n", "input_ta", "=", "tf", ".", "TensorArray", "(", "tf", ".", "float32", ",", "time_steps", ",", "\n", "tensor_array_name", "=", "\"input_array\"", ")", "\n", "output_ta", "=", "tf", ".", "TensorArray", "(", "tf", ".", "float32", ",", "time_steps", ",", "\n", "tensor_array_name", "=", "\"output_array\"", ")", "\n", "value_ta", "=", "tf", ".", "TensorArray", "(", "tf", ".", "float32", ",", "time_steps", ",", "\n", "tensor_array_name", "=", "\"value_array\"", ")", "\n", "alpha_ta", "=", "tf", ".", "TensorArray", "(", "tf", ".", "float32", ",", "time_steps", ",", "\n", "tensor_array_name", "=", "\"alpha_array\"", ")", "\n", "input_ta", "=", "input_ta", ".", "unstack", "(", "inputs", ")", "\n", "\n", "len_src", "=", "tf", ".", "shape", "(", "w_x_bw", ")", "[", "0", "]", "\n", "w_x_bw_ta", "=", "tf", ".", "TensorArray", "(", "tf", ".", "float32", ",", "len_src", ",", "\n", "tensor_array_name", "=", "\"w_x_bw_array\"", ")", "\n", "\n", "w_x_bw_ta", "=", "w_x_bw_ta", ".", "unstack", "(", "w_x_bw", ")", "\n", "w_x_c_shape", "=", "tf", ".", "shape", "(", "w_x_enc", ")", "[", "1", ":", "]", "\n", "w_x_enc", "=", "tf", ".", "transpose", "(", "w_x_enc", ",", "[", "1", ",", "0", ",", "2", ",", "3", "]", ")", "\n", "w_x_enc", "=", "tf", ".", "reshape", "(", "w_x_enc", ",", "\n", "tf", ".", "concat", "(", "[", "tf", ".", "shape", "(", "w_x_enc", ")", "[", ":", "2", "]", ",", "[", "-", "1", "]", "]", ",", "-", "1", ")", ")", "\n", "\n", "w_x_h_ta", "=", "tf", ".", "TensorArray", "(", "tf", ".", "float32", ",", "time_steps", ",", "\n", "tensor_array_name", "=", "\"w_x_h_array\"", ")", "\n", "w_x_ctx_ta", "=", "tf", ".", "TensorArray", "(", "tf", ".", "float32", ",", "time_steps", ",", "\n", "tensor_array_name", "=", "\"w_x_ctx_array\"", ")", "\n", "\n", "initial_state_linear", "=", "lrp", ".", "linear_v2n", "(", "initial_state", ",", "output_size", ",", "True", ",", "\n", "[", "w_x_bw_ta", ".", "read", "(", "0", ")", "]", ",", "params", ",", "\n", "False", ",", "scope", "=", "\"s_transform\"", ",", "\n", "d2", "=", "True", ")", "\n", "initial_state", "=", "initial_state_linear", "[", "\"output\"", "]", "\n", "w_initial", "=", "initial_state_linear", "[", "\"weight_ratios\"", "]", "[", "0", "]", "\n", "initial_state", "=", "tf", ".", "tanh", "(", "initial_state", ")", "\n", "\n", "def", "loop_func", "(", "t", ",", "out_ta", ",", "att_ta", ",", "val_ta", ",", "state", ",", "cache_key", ",", "wxh_ta", ",", "\n", "wxc_ta", ",", "w_x_h_last", ")", ":", "\n", "# now state", "\n", "            ", "wxh_ta", "=", "wxh_ta", ".", "write", "(", "t", ",", "w_x_h_last", ")", "\n", "inp_t", "=", "input_ta", ".", "read", "(", "t", ")", "\n", "results", "=", "layers", ".", "attention", ".", "attention", "(", "state", ",", "memory", ",", "bias", ",", "\n", "output_size", ",", "\n", "cache", "=", "{", "\"key\"", ":", "cache_key", "}", ")", "\n", "alpha", "=", "results", "[", "\"weight\"", "]", "\n", "context", "=", "results", "[", "\"value\"", "]", "\n", "\n", "att", "=", "tf", ".", "expand_dims", "(", "alpha", ",", "1", ")", "\n", "\n", "wr_att", "=", "tf", ".", "expand_dims", "(", "att", ",", "-", "1", ")", "*", "tf", ".", "expand_dims", "(", "memory", ",", "1", ")", "\n", "wr_att", "=", "tf", ".", "squeeze", "(", "wr_att", ",", "1", ")", "\n", "result_stab", "=", "stabilize", "(", "context", ",", "params", ".", "stab", ")", "\n", "wr_att", "/=", "tf", ".", "expand_dims", "(", "result_stab", ",", "1", ")", "\n", "len_src", "=", "tf", ".", "shape", "(", "wr_att", ")", "[", "1", "]", "\n", "w_x_c", "=", "tf", ".", "reshape", "(", "w_x_enc", ",", "[", "1", ",", "len_src", ",", "len_src", ",", "-", "1", "]", ")", "*", "wr_att", "\n", "w_x_c", "=", "tf", ".", "reduce_sum", "(", "w_x_c", ",", "2", ")", "\n", "\n", "#w_x_c = tf.matmul(att, w_x_enc)", "\n", "w_x_c", "=", "tf", ".", "reshape", "(", "w_x_c", ",", "w_x_c_shape", ")", "\n", "wxc_ta", "=", "wxc_ta", ".", "write", "(", "t", ",", "w_x_c", ")", "\n", "\n", "# next state", "\n", "cell_input", "=", "[", "inp_t", ",", "context", "]", "\n", "cell_output", ",", "new_state", ",", "w_x_h_new", "=", "cell", "(", "cell_input", ",", "state", ",", "\n", "w_x_h_last", ",", "w_x_c", ",", "params", ")", "\n", "cell_output", "=", "_copy_through", "(", "t", ",", "sequence_length", "[", "\"target\"", "]", ",", "\n", "zero_output", ",", "cell_output", ")", "\n", "new_state", "=", "_copy_through", "(", "t", ",", "sequence_length", "[", "\"target\"", "]", ",", "state", ",", "\n", "new_state", ")", "\n", "new_value", "=", "_copy_through", "(", "t", ",", "sequence_length", "[", "\"target\"", "]", ",", "zero_value", ",", "\n", "context", ")", "\n", "w_x_h_new", "=", "_copy_through", "(", "t", ",", "sequence_length", "[", "\"target\"", "]", ",", "w_x_h_last", ",", "\n", "w_x_h_new", ")", "\n", "\n", "out_ta", "=", "out_ta", ".", "write", "(", "t", ",", "cell_output", ")", "\n", "att_ta", "=", "att_ta", ".", "write", "(", "t", ",", "alpha", ")", "\n", "val_ta", "=", "val_ta", ".", "write", "(", "t", ",", "new_value", ")", "\n", "cache_key", "=", "tf", ".", "identity", "(", "cache_key", ")", "\n", "\n", "return", "t", "+", "1", ",", "out_ta", ",", "att_ta", ",", "val_ta", ",", "new_state", ",", "cache_key", ",", "wxh_ta", ",", "wxc_ta", ",", "w_x_h_new", "\n", "\n", "", "time", "=", "tf", ".", "constant", "(", "0", ",", "dtype", "=", "tf", ".", "int32", ",", "name", "=", "\"time\"", ")", "\n", "loop_vars", "=", "(", "time", ",", "output_ta", ",", "alpha_ta", ",", "value_ta", ",", "initial_state", ",", "\n", "cache", "[", "\"key\"", "]", ",", "w_x_h_ta", ",", "w_x_ctx_ta", ",", "w_initial", ")", "\n", "\n", "outputs", "=", "tf", ".", "while_loop", "(", "lambda", "t", ",", "*", "_", ":", "t", "<", "time_steps", ",", "\n", "loop_func", ",", "loop_vars", ",", "\n", "parallel_iterations", "=", "32", ",", "\n", "swap_memory", "=", "True", ")", "\n", "\n", "output_final_ta", "=", "outputs", "[", "1", "]", "\n", "value_final_ta", "=", "outputs", "[", "3", "]", "\n", "\n", "final_output", "=", "output_final_ta", ".", "stack", "(", ")", "\n", "final_output", ".", "set_shape", "(", "[", "None", ",", "None", ",", "output_size", "]", ")", "\n", "final_output", "=", "tf", ".", "transpose", "(", "final_output", ",", "[", "1", ",", "0", ",", "2", "]", ")", "\n", "\n", "final_value", "=", "value_final_ta", ".", "stack", "(", ")", "\n", "final_value", ".", "set_shape", "(", "[", "None", ",", "None", ",", "memory", ".", "shape", "[", "-", "1", "]", ".", "value", "]", ")", "\n", "final_value", "=", "tf", ".", "transpose", "(", "final_value", ",", "[", "1", ",", "0", ",", "2", "]", ")", "\n", "\n", "w_x_h_final_ta", "=", "outputs", "[", "6", "]", "\n", "w_x_h_final", "=", "w_x_h_final_ta", ".", "stack", "(", ")", "\n", "w_x_c_final_ta", "=", "outputs", "[", "7", "]", "\n", "w_x_c_final", "=", "w_x_c_final_ta", ".", "stack", "(", ")", "\n", "\n", "result", "=", "{", "\n", "\"outputs\"", ":", "final_output", ",", "\n", "\"values\"", ":", "final_value", ",", "\n", "\"initial_state\"", ":", "initial_state", ",", "\n", "\"weight_ratios\"", ":", "[", "w_x_h_final", ",", "w_x_c_final", ",", "w_initial", "]", "\n", "}", "\n", "\n", "", "return", "result", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.rnnsearch_lrp.model_graph": [[272, 444], ["len", "len", "tensorflow.nn.bias_add", "tensorflow.nn.bias_add", "thumt.LegacyGRUCell_encoder_v2n", "thumt.LegacyGRUCell_encoder_v2n", "rnnsearch_lrp._encoder", "tensorflow.concat", "thumt.LegacyGRUCell_decoder_v2n", "rnnsearch_lrp._decoder", "tensorflow.pad", "tensorflow.concat", "thumt.maxout_v2n", "tensorflow.transpose", "thumt.linear_v2n", "thumt.linear_v2n", "tensorflow.reshape", "tensorflow.transpose", "tensorflow.reshape", "tensorflow.transpose", "tensorflow.range", "tensorflow.cast", "tensorflow.reshape", "tensorflow.concat", "tensorflow.transpose", "tensorflow.gather_nd", "tensorflow.reshape", "thumt.smoothed_softmax_cross_entropy_with_logits", "tensorflow.reshape", "tensorflow.to_float", "tensorflow.variable_scope", "tensorflow.get_variable", "tensorflow.get_variable", "tensorflow.nn.embedding_lookup", "tensorflow.variable_scope", "tensorflow.get_variable", "tensorflow.get_variable", "tensorflow.nn.embedding_lookup", "tensorflow.nn.dropout", "tensorflow.nn.dropout", "tensorflow.nn.rnn_cell.DropoutWrapper", "tensorflow.nn.rnn_cell.DropoutWrapper", "tensorflow.nn.rnn_cell.DropoutWrapper", "thumt.nn.maxout", "thumt.nn.linear", "thumt.nn.linear", "tensorflow.nn.dropout", "tensorflow.shape", "tensorflow.shape", "tensorflow.sequence_mask", "tensorflow.reduce_sum", "tensorflow.reduce_sum", "tensorflow.expand_dims", "tensorflow.shape", "tensorflow.shape", "tensorflow.shape", "tensorflow.shape", "tensorflow.shape"], "function", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.rnnsearch_lrp._encoder", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.rnnsearch_lrp._decoder", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.lrp.maxout_v2n", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.lrp.linear_v2n", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.lrp.linear_v2n", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.losses.losses.smoothed_softmax_cross_entropy_with_logits", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.layers.nn.maxout", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.layers.nn.linear", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.layers.nn.linear"], ["", "def", "model_graph", "(", "features", ",", "labels", ",", "params", ")", ":", "\n", "    ", "src_vocab_size", "=", "len", "(", "params", ".", "vocabulary", "[", "\"source\"", "]", ")", "\n", "tgt_vocab_size", "=", "len", "(", "params", ".", "vocabulary", "[", "\"target\"", "]", ")", "\n", "\n", "with", "tf", ".", "variable_scope", "(", "\"source_embedding\"", ")", ":", "\n", "        ", "src_emb", "=", "tf", ".", "get_variable", "(", "\"embedding\"", ",", "\n", "[", "src_vocab_size", ",", "params", ".", "embedding_size", "]", ")", "\n", "src_bias", "=", "tf", ".", "get_variable", "(", "\"bias\"", ",", "[", "params", ".", "embedding_size", "]", ")", "\n", "src_inputs", "=", "tf", ".", "nn", ".", "embedding_lookup", "(", "src_emb", ",", "features", "[", "\"source\"", "]", ")", "\n", "\n", "", "with", "tf", ".", "variable_scope", "(", "\"target_embedding\"", ")", ":", "\n", "        ", "tgt_emb", "=", "tf", ".", "get_variable", "(", "\"embedding\"", ",", "\n", "[", "tgt_vocab_size", ",", "params", ".", "embedding_size", "]", ")", "\n", "tgt_bias", "=", "tf", ".", "get_variable", "(", "\"bias\"", ",", "[", "params", ".", "embedding_size", "]", ")", "\n", "tgt_inputs", "=", "tf", ".", "nn", ".", "embedding_lookup", "(", "tgt_emb", ",", "features", "[", "\"target\"", "]", ")", "\n", "\n", "", "src_inputs", "=", "tf", ".", "nn", ".", "bias_add", "(", "src_inputs", ",", "src_bias", ")", "\n", "tgt_inputs", "=", "tf", ".", "nn", ".", "bias_add", "(", "tgt_inputs", ",", "tgt_bias", ")", "\n", "\n", "if", "params", ".", "dropout", "and", "not", "params", ".", "use_variational_dropout", ":", "\n", "        ", "src_inputs", "=", "tf", ".", "nn", ".", "dropout", "(", "src_inputs", ",", "1.0", "-", "params", ".", "dropout", ")", "\n", "tgt_inputs", "=", "tf", ".", "nn", ".", "dropout", "(", "tgt_inputs", ",", "1.0", "-", "params", ".", "dropout", ")", "\n", "\n", "# encoder", "\n", "", "cell_fw", "=", "lrp", ".", "LegacyGRUCell_encoder_v2n", "(", "params", ".", "hidden_size", ")", "\n", "cell_bw", "=", "lrp", ".", "LegacyGRUCell_encoder_v2n", "(", "params", ".", "hidden_size", ")", "\n", "\n", "if", "params", ".", "use_variational_dropout", ":", "\n", "        ", "cell_fw", "=", "tf", ".", "nn", ".", "rnn_cell", ".", "DropoutWrapper", "(", "\n", "cell_fw", ",", "\n", "input_keep_prob", "=", "1.0", "-", "params", ".", "dropout", ",", "\n", "output_keep_prob", "=", "1.0", "-", "params", ".", "dropout", ",", "\n", "state_keep_prob", "=", "1.0", "-", "params", ".", "dropout", ",", "\n", "variational_recurrent", "=", "True", ",", "\n", "input_size", "=", "params", ".", "embedding_size", ",", "\n", "dtype", "=", "tf", ".", "float32", "\n", ")", "\n", "cell_bw", "=", "tf", ".", "nn", ".", "rnn_cell", ".", "DropoutWrapper", "(", "\n", "cell_bw", ",", "\n", "input_keep_prob", "=", "1.0", "-", "params", ".", "dropout", ",", "\n", "output_keep_prob", "=", "1.0", "-", "params", ".", "dropout", ",", "\n", "state_keep_prob", "=", "1.0", "-", "params", ".", "dropout", ",", "\n", "variational_recurrent", "=", "True", ",", "\n", "input_size", "=", "params", ".", "embedding_size", ",", "\n", "dtype", "=", "tf", ".", "float32", "\n", ")", "\n", "\n", "", "encoder_output", "=", "_encoder", "(", "cell_fw", ",", "cell_bw", ",", "src_inputs", ",", "\n", "features", "[", "\"source_length\"", "]", ",", "params", ")", "\n", "\n", "w_x_h_fw", ",", "w_x_h_bw", "=", "encoder_output", "[", "\"weight_ratios\"", "]", "\n", "w_x_h_bw", "=", "w_x_h_bw", "[", ":", ":", "-", "1", ",", ":", ",", ":", ":", "-", "1", "]", "\n", "w_x_enc", "=", "tf", ".", "concat", "(", "[", "w_x_h_fw", ",", "w_x_h_bw", "]", ",", "-", "1", ")", "\n", "\n", "# decoder", "\n", "cell", "=", "lrp", ".", "LegacyGRUCell_decoder_v2n", "(", "params", ".", "hidden_size", ")", "\n", "\n", "if", "params", ".", "use_variational_dropout", ":", "\n", "        ", "cell", "=", "tf", ".", "nn", ".", "rnn_cell", ".", "DropoutWrapper", "(", "\n", "cell", ",", "\n", "input_keep_prob", "=", "1.0", "-", "params", ".", "dropout", ",", "\n", "output_keep_prob", "=", "1.0", "-", "params", ".", "dropout", ",", "\n", "state_keep_prob", "=", "1.0", "-", "params", ".", "dropout", ",", "\n", "variational_recurrent", "=", "True", ",", "\n", "# input + context", "\n", "input_size", "=", "params", ".", "embedding_size", "+", "2", "*", "params", ".", "hidden_size", ",", "\n", "dtype", "=", "tf", ".", "float32", "\n", ")", "\n", "\n", "", "length", "=", "{", "\n", "\"source\"", ":", "features", "[", "\"source_length\"", "]", ",", "\n", "\"target\"", ":", "features", "[", "\"target_length\"", "]", "\n", "}", "\n", "initial_state", "=", "encoder_output", "[", "\"final_states\"", "]", "[", "\"backward\"", "]", "\n", "decoder_output", "=", "_decoder", "(", "cell", ",", "tgt_inputs", ",", "encoder_output", "[", "\"annotation\"", "]", ",", "\n", "length", ",", "initial_state", ",", "w_x_enc", ",", "w_x_h_bw", ",", "params", ")", "\n", "\n", "w_x_dec", ",", "w_x_ctx", ",", "w_x_init", "=", "decoder_output", "[", "\"weight_ratios\"", "]", "\n", "\n", "# Shift left", "\n", "shifted_tgt_inputs", "=", "tf", ".", "pad", "(", "tgt_inputs", ",", "[", "[", "0", ",", "0", "]", ",", "[", "1", ",", "0", "]", ",", "[", "0", ",", "0", "]", "]", ")", "\n", "shifted_tgt_inputs", "=", "shifted_tgt_inputs", "[", ":", ",", ":", "-", "1", ",", ":", "]", "\n", "\n", "all_outputs", "=", "tf", ".", "concat", "(", "\n", "[", "\n", "tf", ".", "expand_dims", "(", "decoder_output", "[", "\"initial_state\"", "]", ",", "axis", "=", "1", ")", ",", "\n", "decoder_output", "[", "\"outputs\"", "]", ",", "\n", "]", ",", "\n", "axis", "=", "1", "\n", ")", "\n", "shifted_outputs", "=", "all_outputs", "[", ":", ",", ":", "-", "1", ",", ":", "]", "\n", "\n", "maxout_features", "=", "[", "\n", "shifted_tgt_inputs", ",", "\n", "shifted_outputs", ",", "\n", "decoder_output", "[", "\"values\"", "]", "\n", "]", "\n", "maxout_size", "=", "params", ".", "hidden_size", "//", "params", ".", "maxnum", "\n", "\n", "if", "labels", "is", "None", ":", "\n", "# Special case for non-incremental decoding", "\n", "        ", "maxout_features", "=", "[", "\n", "shifted_tgt_inputs", "[", ":", ",", "-", "1", ",", ":", "]", ",", "\n", "shifted_outputs", "[", ":", ",", "-", "1", ",", ":", "]", ",", "\n", "decoder_output", "[", "\"values\"", "]", "[", ":", ",", "-", "1", ",", ":", "]", "\n", "]", "\n", "maxhid", "=", "layers", ".", "nn", ".", "maxout", "(", "maxout_features", ",", "maxout_size", ",", "params", ".", "maxnum", ",", "\n", "params", ",", "concat", "=", "False", ")", "\n", "\n", "readout", "=", "layers", ".", "nn", ".", "linear", "(", "maxhid", ",", "params", ".", "embedding_size", ",", "False", ",", "\n", "False", ",", "scope", "=", "\"deepout\"", ")", "\n", "\n", "# Prediction", "\n", "logits", "=", "layers", ".", "nn", ".", "linear", "(", "readout", ",", "tgt_vocab_size", ",", "True", ",", "False", ",", "\n", "scope", "=", "\"softmax\"", ")", "\n", "\n", "return", "logits", "\n", "\n", "", "maxhid_maxout", "=", "lrp", ".", "maxout_v2n", "(", "maxout_features", ",", "maxout_size", ",", "params", ".", "maxnum", ",", "\n", "[", "w_x_dec", ",", "w_x_ctx", "]", ",", "params", ",", "concat", "=", "False", ")", "\n", "maxhid", "=", "maxhid_maxout", "[", "\"output\"", "]", "\n", "w_x_maxout", "=", "maxhid_maxout", "[", "\"weight_ratios\"", "]", "[", "0", "]", "\n", "w_x_maxout", "=", "tf", ".", "transpose", "(", "w_x_maxout", ",", "[", "0", ",", "2", ",", "1", ",", "3", "]", ")", "\n", "readout", "=", "lrp", ".", "linear_v2n", "(", "maxhid", ",", "params", ".", "embedding_size", ",", "False", ",", "\n", "[", "w_x_maxout", "]", ",", "params", ",", "False", ",", "scope", "=", "\"deepout\"", ")", "\n", "w_x_readout", "=", "readout", "[", "\"weight_ratios\"", "]", "[", "0", "]", "\n", "readout", "=", "readout", "[", "\"output\"", "]", "\n", "\n", "if", "params", ".", "dropout", "and", "not", "params", ".", "use_variational_dropout", ":", "\n", "        ", "readout", "=", "tf", ".", "nn", ".", "dropout", "(", "readout", ",", "1.0", "-", "params", ".", "dropout", ")", "\n", "\n", "# Prediction and final relevance", "\n", "", "logits", "=", "lrp", ".", "linear_v2n", "(", "readout", ",", "tgt_vocab_size", ",", "True", ",", "[", "w_x_readout", "]", ",", "\n", "params", ",", "False", ",", "scope", "=", "\"softmax\"", ")", "\n", "w_x_true", "=", "logits", "[", "\"weight_ratios\"", "]", "[", "0", "]", "\n", "logits", "=", "logits", "[", "\"output\"", "]", "\n", "logits", "=", "tf", ".", "reshape", "(", "logits", ",", "[", "-", "1", ",", "tgt_vocab_size", "]", ")", "\n", "w_x_true", "=", "tf", ".", "transpose", "(", "w_x_true", ",", "[", "0", ",", "2", ",", "1", ",", "3", "]", ")", "\n", "w_x_true", "=", "tf", ".", "reshape", "(", "w_x_true", ",", "[", "-", "1", ",", "tf", ".", "shape", "(", "w_x_true", ")", "[", "-", "2", "]", ",", "\n", "tf", ".", "shape", "(", "w_x_true", ")", "[", "-", "1", "]", "]", ")", "\n", "w_x_true", "=", "tf", ".", "transpose", "(", "w_x_true", ",", "[", "0", ",", "2", ",", "1", "]", ")", "\n", "labels_lrp", "=", "labels", "\n", "bs", "=", "tf", ".", "shape", "(", "labels_lrp", ")", "[", "0", "]", "\n", "idx", "=", "tf", ".", "range", "(", "tf", ".", "shape", "(", "labels_lrp", ")", "[", "-", "1", "]", ")", "\n", "idx", "=", "tf", ".", "cast", "(", "idx", ",", "tf", ".", "int64", ")", "\n", "idx", "=", "tf", ".", "reshape", "(", "idx", ",", "[", "1", ",", "-", "1", "]", ")", "\n", "labels_lrp", "=", "tf", ".", "concat", "(", "[", "idx", ",", "labels_lrp", "]", ",", "axis", "=", "0", ")", "\n", "labels_lrp", "=", "tf", ".", "transpose", "(", "labels_lrp", ",", "[", "1", ",", "0", "]", ")", "\n", "w_x_true", "=", "tf", ".", "gather_nd", "(", "w_x_true", ",", "labels_lrp", ")", "\n", "w_x_true", "=", "tf", ".", "reshape", "(", "w_x_true", ",", "[", "bs", ",", "-", "1", ",", "tf", ".", "shape", "(", "w_x_true", ")", "[", "-", "1", "]", "]", ")", "\n", "\n", "ce", "=", "losses", ".", "smoothed_softmax_cross_entropy_with_logits", "(", "\n", "logits", "=", "logits", ",", "\n", "labels", "=", "labels", ",", "\n", "smoothing", "=", "params", ".", "label_smoothing", ",", "\n", "normalize", "=", "True", "\n", ")", "\n", "\n", "ce", "=", "tf", ".", "reshape", "(", "ce", ",", "tf", ".", "shape", "(", "labels", ")", ")", "\n", "tgt_mask", "=", "tf", ".", "to_float", "(", "\n", "tf", ".", "sequence_mask", "(", "\n", "features", "[", "\"target_length\"", "]", ",", "\n", "maxlen", "=", "tf", ".", "shape", "(", "features", "[", "\"target\"", "]", ")", "[", "1", "]", "\n", ")", "\n", ")", "\n", "\n", "rlv_info", "=", "{", "}", "\n", "rlv_info", "[", "\"result\"", "]", "=", "w_x_true", "\n", "\n", "loss", "=", "tf", ".", "reduce_sum", "(", "ce", "*", "tgt_mask", ")", "/", "tf", ".", "reduce_sum", "(", "tgt_mask", ")", "\n", "\n", "return", "loss", ",", "rlv_info", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.transformer_lrp.TransformerLRP.__init__": [[367, 369], ["thumt.models.model.NMTModel.__init__"], "methods", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.transformer_lrp.TransformerLRP.__init__"], ["    ", "def", "__init__", "(", "self", ",", "params", ",", "scope", "=", "\"transformer\"", ")", ":", "\n", "        ", "super", "(", "TransformerLRP", ",", "self", ")", ".", "__init__", "(", "params", "=", "params", ",", "scope", "=", "scope", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.transformer_lrp.TransformerLRP.get_training_func": [[370, 381], ["tensorflow.variable_scope", "transformer_lrp.model_graph"], "methods", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.transformer_lrp.model_graph"], ["", "def", "get_training_func", "(", "self", ",", "initializer", ")", ":", "\n", "        ", "def", "training_fn", "(", "features", ",", "params", "=", "None", ")", ":", "\n", "            ", "if", "params", "is", "None", ":", "\n", "                ", "params", "=", "self", ".", "parameters", "\n", "", "with", "tf", ".", "variable_scope", "(", "self", ".", "_scope", ",", "initializer", "=", "initializer", ",", "\n", "reuse", "=", "tf", ".", "AUTO_REUSE", ")", ":", "\n", "                ", "loss", "=", "model_graph", "(", "features", ",", "features", "[", "\"target\"", "]", ",", "\n", "\"train\"", ",", "params", ")", "[", "0", "]", "\n", "return", "loss", "\n", "\n", "", "", "return", "training_fn", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.transformer_lrp.TransformerLRP.get_relevance_func": [[382, 399], ["copy.copy", "copy.copy", "tensorflow.variable_scope", "transformer_lrp.model_graph"], "methods", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.transformer_lrp.model_graph"], ["", "def", "get_relevance_func", "(", "self", ")", ":", "\n", "        ", "def", "relevance_fn", "(", "features", ",", "params", "=", "None", ")", ":", "\n", "            ", "if", "params", "is", "None", ":", "\n", "                ", "params", "=", "copy", ".", "copy", "(", "self", ".", "parameters", ")", "\n", "", "else", ":", "\n", "                ", "params", "=", "copy", ".", "copy", "(", "params", ")", "\n", "\n", "", "params", ".", "residual_dropout", "=", "0.0", "\n", "params", ".", "attention_dropout", "=", "0.0", "\n", "params", ".", "relu_dropout", "=", "0.0", "\n", "params", ".", "label_smoothing", "=", "0.0", "\n", "\n", "with", "tf", ".", "variable_scope", "(", "self", ".", "_scope", ",", "reuse", "=", "tf", ".", "AUTO_REUSE", ")", ":", "\n", "                ", "loss", ",", "rlv", "=", "model_graph", "(", "features", ",", "features", "[", "\"target\"", "]", ",", "\n", "\"train\"", ",", "params", ")", "\n", "return", "features", "[", "\"source\"", "]", ",", "features", "[", "\"target\"", "]", ",", "rlv", ",", "loss", "\n", "", "", "return", "relevance_fn", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.transformer_lrp.TransformerLRP.get_evaluation_func": [[400, 418], ["copy.copy", "copy.copy", "tensorflow.variable_scope", "transformer_lrp.model_graph"], "methods", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.transformer_lrp.model_graph"], ["", "def", "get_evaluation_func", "(", "self", ")", ":", "\n", "        ", "def", "evaluation_fn", "(", "features", ",", "params", "=", "None", ")", ":", "\n", "            ", "if", "params", "is", "None", ":", "\n", "                ", "params", "=", "copy", ".", "copy", "(", "self", ".", "parameters", ")", "\n", "", "else", ":", "\n", "                ", "params", "=", "copy", ".", "copy", "(", "params", ")", "\n", "\n", "", "params", ".", "residual_dropout", "=", "0.0", "\n", "params", ".", "attention_dropout", "=", "0.0", "\n", "params", ".", "relu_dropout", "=", "0.0", "\n", "params", ".", "label_smoothing", "=", "0.0", "\n", "\n", "with", "tf", ".", "variable_scope", "(", "self", ".", "_scope", ")", ":", "\n", "                ", "logits", "=", "model_graph", "(", "features", ",", "None", ",", "\"infer\"", ",", "params", ")", "\n", "\n", "", "return", "logits", "\n", "\n", "", "return", "evaluation_fn", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.transformer_lrp.TransformerLRP.get_inference_func": [[419, 437], ["copy.copy", "copy.copy", "tensorflow.variable_scope", "transformer_lrp.model_graph"], "methods", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.transformer_lrp.model_graph"], ["", "def", "get_inference_func", "(", "self", ")", ":", "\n", "        ", "def", "inference_fn", "(", "features", ",", "params", "=", "None", ")", ":", "\n", "            ", "if", "params", "is", "None", ":", "\n", "                ", "params", "=", "copy", ".", "copy", "(", "self", ".", "parameters", ")", "\n", "", "else", ":", "\n", "                ", "params", "=", "copy", ".", "copy", "(", "params", ")", "\n", "\n", "", "params", ".", "residual_dropout", "=", "0.0", "\n", "params", ".", "attention_dropout", "=", "0.0", "\n", "params", ".", "relu_dropout", "=", "0.0", "\n", "params", ".", "label_smoothing", "=", "0.0", "\n", "\n", "with", "tf", ".", "variable_scope", "(", "self", ".", "_scope", ")", ":", "\n", "                ", "logits", "=", "model_graph", "(", "features", ",", "None", ",", "\"infer\"", ",", "params", ")", "\n", "\n", "", "return", "logits", "\n", "\n", "", "return", "inference_fn", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.transformer_lrp.TransformerLRP.get_name": [[438, 441], ["None"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "get_name", "(", ")", ":", "\n", "        ", "return", "\"transformer\"", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.transformer_lrp.TransformerLRP.get_parameters": [[442, 482], ["tensorflow.contrib.training.HParams"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "get_parameters", "(", ")", ":", "\n", "        ", "params", "=", "tf", ".", "contrib", ".", "training", ".", "HParams", "(", "\n", "pad", "=", "\"<pad>\"", ",", "\n", "bos", "=", "\"<eos>\"", ",", "\n", "eos", "=", "\"<eos>\"", ",", "\n", "unk", "=", "\"<unk>\"", ",", "\n", "append_eos", "=", "False", ",", "\n", "hidden_size", "=", "512", ",", "\n", "filter_size", "=", "2048", ",", "\n", "num_heads", "=", "8", ",", "\n", "num_encoder_layers", "=", "6", ",", "\n", "num_decoder_layers", "=", "6", ",", "\n", "attention_dropout", "=", "0.0", ",", "\n", "residual_dropout", "=", "0.1", ",", "\n", "relu_dropout", "=", "0.0", ",", "\n", "label_smoothing", "=", "0.1", ",", "\n", "attention_key_channels", "=", "0", ",", "\n", "attention_value_channels", "=", "0", ",", "\n", "multiply_embedding_mode", "=", "\"sqrt_depth\"", ",", "\n", "shared_embedding_and_softmax_weights", "=", "False", ",", "\n", "shared_source_target_embedding", "=", "False", ",", "\n", "# Override default parameters", "\n", "learning_rate_decay", "=", "\"noam\"", ",", "\n", "initializer", "=", "\"uniform_unit_scaling\"", ",", "\n", "initializer_gain", "=", "1.0", ",", "\n", "learning_rate", "=", "1.0", ",", "\n", "layer_preprocess", "=", "\"none\"", ",", "\n", "layer_postprocess", "=", "\"layer_norm\"", ",", "\n", "batch_size", "=", "4096", ",", "\n", "constant_batch_size", "=", "False", ",", "\n", "adam_beta1", "=", "0.9", ",", "\n", "adam_beta2", "=", "0.98", ",", "\n", "adam_epsilon", "=", "1e-9", ",", "\n", "clip_grad_norm", "=", "0.0", ",", "\n", "# lrp", "\n", "stab", "=", "0.05", ",", "\n", ")", "\n", "\n", "return", "params", "\n", "", "", ""]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.transformer_lrp.normalize": [[19, 28], ["tensorflow.abs", "tensorflow.reduce_sum", "tensorflow.abs", "tensorflow.reduce_sum", "tensorflow.expand_dims", "tensorflow.expand_dims"], "function", ["None"], ["def", "normalize", "(", "matrix", ",", "negative", "=", "False", ")", ":", "\n", "    ", "if", "negative", ":", "\n", "        ", "matrix_abs", "=", "tf", ".", "abs", "(", "matrix", ")", "\n", "total", "=", "tf", ".", "reduce_sum", "(", "matrix_abs", ",", "-", "1", ")", "\n", "return", "matrix", "/", "tf", ".", "expand_dims", "(", "total", ",", "-", "1", ")", "\n", "", "else", ":", "\n", "        ", "matrix", "=", "tf", ".", "abs", "(", "matrix", ")", "\n", "total", "=", "tf", ".", "reduce_sum", "(", "matrix", ",", "-", "1", ")", "\n", "return", "matrix", "/", "tf", ".", "expand_dims", "(", "total", ",", "-", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.transformer_lrp.get_weights": [[30, 62], ["len", "len", "tensorflow.random_normal_initializer", "tensorflow.get_variable", "tensorflow.get_variable", "tensorflow.get_variable", "tensorflow.get_variable", "cmp", "ValueError"], "function", ["None"], ["", "", "def", "get_weights", "(", "params", ")", ":", "\n", "    ", "svocab", "=", "params", ".", "vocabulary", "[", "\"source\"", "]", "\n", "tvocab", "=", "params", ".", "vocabulary", "[", "\"target\"", "]", "\n", "src_vocab_size", "=", "len", "(", "svocab", ")", "\n", "tgt_vocab_size", "=", "len", "(", "tvocab", ")", "\n", "vocab_size", "=", "tgt_vocab_size", "\n", "\n", "hidden_size", "=", "params", ".", "hidden_size", "\n", "initializer", "=", "tf", ".", "random_normal_initializer", "(", "0.0", ",", "params", ".", "hidden_size", "**", "-", "0.5", ")", "\n", "\n", "if", "params", ".", "shared_source_target_embedding", ":", "\n", "        ", "if", "cmp", "(", "svocab", ",", "tvocab", ")", "!=", "0", ":", "\n", "            ", "raise", "ValueError", "(", "\"Source and target vocabularies are not the same\"", ")", "\n", "\n", "", "weights", "=", "tf", ".", "get_variable", "(", "\"weights\"", ",", "[", "src_vocab_size", ",", "hidden_size", "]", ",", "\n", "initializer", "=", "initializer", ")", "\n", "semb", ",", "temb", "=", "weights", ",", "weights", "\n", "", "else", ":", "\n", "        ", "semb", "=", "tf", ".", "get_variable", "(", "\"source_embedding\"", ",", "\n", "[", "src_vocab_size", ",", "hidden_size", "]", ",", "\n", "initializer", "=", "initializer", ")", "\n", "temb", "=", "tf", ".", "get_variable", "(", "\"target_embedding\"", ",", "\n", "[", "tgt_vocab_size", ",", "hidden_size", "]", ",", "\n", "initializer", "=", "initializer", ")", "\n", "\n", "", "if", "params", ".", "shared_embedding_and_softmax_weights", ":", "\n", "        ", "softmax_weights", "=", "temb", "\n", "", "else", ":", "\n", "        ", "softmax_weights", "=", "tf", ".", "get_variable", "(", "\"softmax\"", ",", "[", "vocab_size", ",", "hidden_size", "]", ",", "\n", "initializer", "=", "initializer", ")", "\n", "\n", "", "return", "semb", ",", "temb", ",", "softmax_weights", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.transformer_lrp.layer_process": [[64, 71], ["thumt.nn.layer_norm", "ValueError"], "function", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.layers.nn.layer_norm"], ["", "def", "layer_process", "(", "x", ",", "mode", ")", ":", "\n", "    ", "if", "not", "mode", "or", "mode", "==", "\"none\"", ":", "\n", "        ", "return", "x", "\n", "", "elif", "mode", "==", "\"layer_norm\"", ":", "\n", "        ", "return", "layers", ".", "nn", ".", "layer_norm", "(", "x", ")", "\n", "", "else", ":", "\n", "        ", "raise", "ValueError", "(", "\"Unknown mode %s\"", "%", "mode", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.transformer_lrp.residual_fn": [[73, 97], ["tensorflow.reshape", "tensorflow.reshape", "tensorflow.reshape", "thumt.weight_ratio_weighted_sum", "tensorflow.reshape", "tensorflow.reshape", "tensorflow.nn.dropout", "tensorflow.shape", "tensorflow.shape", "tensorflow.shape", "tensorflow.shape"], "function", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.weight_ratio.weight_ratio_weighted_sum"], ["", "", "def", "residual_fn", "(", "x", ",", "y", ",", "w_x_last", ",", "w_x_inp", ",", "params", ",", "keep_prob", "=", "None", ")", ":", "\n", "    ", "if", "keep_prob", "and", "keep_prob", "<", "1.0", ":", "\n", "        ", "y", "=", "tf", ".", "nn", ".", "dropout", "(", "y", ",", "keep_prob", ")", "\n", "", "batchsize", "=", "tf", ".", "shape", "(", "x", ")", "[", "0", "]", "\n", "len_inp", "=", "tf", ".", "shape", "(", "x", ")", "[", "1", "]", "\n", "len_src", "=", "tf", ".", "shape", "(", "w_x_last", ")", "[", "1", "]", "\n", "dim", "=", "tf", ".", "shape", "(", "x", ")", "[", "2", "]", "\n", "result", "=", "{", "}", "\n", "result", "[", "\"output\"", "]", "=", "x", "+", "y", "\n", "x_down", "=", "tf", ".", "reshape", "(", "x", ",", "[", "batchsize", ",", "-", "1", "]", ")", "\n", "y_down", "=", "tf", ".", "reshape", "(", "y", ",", "[", "batchsize", ",", "-", "1", "]", ")", "\n", "z_down", "=", "tf", ".", "reshape", "(", "result", "[", "\"output\"", "]", ",", "[", "batchsize", ",", "-", "1", "]", ")", "\n", "\n", "w_last_out", ",", "w_inp_out", "=", "wr", ".", "weight_ratio_weighted_sum", "(", "[", "x_down", ",", "y_down", "]", ",", "\n", "[", "1.", ",", "1.", "]", ",", "\n", "z_down", ",", "\n", "stab", "=", "params", ".", "stab", ",", "\n", "flatten", "=", "True", ")", "\n", "# bs, len*d", "\n", "w_last_out", "=", "tf", ".", "reshape", "(", "w_last_out", ",", "[", "batchsize", ",", "1", ",", "len_inp", ",", "dim", "]", ")", "\n", "w_inp_out", "=", "tf", ".", "reshape", "(", "w_inp_out", ",", "[", "batchsize", ",", "1", ",", "len_inp", ",", "dim", "]", ")", "\n", "w_x_out", "=", "w_x_last", "*", "w_last_out", "+", "w_x_inp", "*", "w_inp_out", "\n", "result", "[", "\"weight_ratio\"", "]", "=", "w_x_out", "\n", "return", "result", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.transformer_lrp.ffn_layer": [[99, 120], ["tensorflow.variable_scope", "tensorflow.variable_scope", "thumt.linear_v2n", "tensorflow.nn.relu", "tensorflow.nn.dropout", "tensorflow.variable_scope", "thumt.linear_v2n"], "function", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.lrp.linear_v2n", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.lrp.linear_v2n"], ["", "def", "ffn_layer", "(", "inputs", ",", "w_x_inp", ",", "hidden_size", ",", "output_size", ",", "params", ",", "\n", "keep_prob", "=", "None", ",", "dtype", "=", "None", ",", "scope", "=", "None", ")", ":", "\n", "    ", "with", "tf", ".", "variable_scope", "(", "scope", ",", "default_name", "=", "\"ffn_layer\"", ",", "values", "=", "[", "inputs", "]", ",", "\n", "dtype", "=", "dtype", ")", ":", "\n", "        ", "with", "tf", ".", "variable_scope", "(", "\"input_layer\"", ")", ":", "\n", "            ", "hidden_linear", "=", "lrp", ".", "linear_v2n", "(", "inputs", ",", "hidden_size", ",", "True", ",", "\n", "[", "w_x_inp", "]", ",", "params", ",", "True", ")", "\n", "hidden", "=", "hidden_linear", "[", "\"output\"", "]", "\n", "w_x_hid", "=", "hidden_linear", "[", "\"weight_ratios\"", "]", "[", "0", "]", "\n", "hidden", "=", "tf", ".", "nn", ".", "relu", "(", "hidden", ")", "\n", "\n", "", "if", "keep_prob", "and", "keep_prob", "<", "1.0", ":", "\n", "            ", "hidden", "=", "tf", ".", "nn", ".", "dropout", "(", "hidden", ",", "keep_prob", ")", "\n", "\n", "", "with", "tf", ".", "variable_scope", "(", "\"output_layer\"", ")", ":", "\n", "            ", "output_linear", "=", "lrp", ".", "linear_v2n", "(", "hidden", ",", "output_size", ",", "True", ",", "\n", "[", "w_x_hid", "]", ",", "params", ",", "True", ")", "\n", "output", "=", "output_linear", "[", "\"output\"", "]", "\n", "w_x_outp", "=", "output_linear", "[", "\"weight_ratios\"", "]", "[", "0", "]", "\n", "\n", "", "return", "{", "\"output\"", ":", "output", ",", "\"weight_ratios\"", ":", "w_x_outp", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.transformer_lrp.transformer_encoder": [[122, 184], ["tensorflow.variable_scope", "thumt.create_diagonal_v2n", "range", "transformer_lrp.layer_process", "tensorflow.shape", "tensorflow.shape", "tensorflow.shape", "tensorflow.variable_scope", "tensorflow.variable_scope", "thumt.multihead_attention_v2n", "transformer_lrp.residual_fn", "thumt.layer_process", "tensorflow.variable_scope", "transformer_lrp.ffn_layer", "transformer_lrp.residual_fn", "thumt.layer_process", "transformer_lrp.layer_process", "transformer_lrp.layer_process"], "function", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.lrp.create_diagonal_v2n", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.transformer_lrp.layer_process", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.lrp.multihead_attention_v2n", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.transformer_lrp.residual_fn", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.transformer_lrp.layer_process", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.transformer_lrp.ffn_layer", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.transformer_lrp.residual_fn", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.transformer_lrp.layer_process", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.transformer_lrp.layer_process", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.transformer_lrp.layer_process"], ["", "", "def", "transformer_encoder", "(", "inputs", ",", "bias", ",", "params", ",", "dtype", "=", "None", ",", "scope", "=", "None", ")", ":", "\n", "    ", "with", "tf", ".", "variable_scope", "(", "scope", ",", "default_name", "=", "\"encoder\"", ",", "dtype", "=", "dtype", ",", "\n", "values", "=", "[", "inputs", ",", "bias", "]", ")", ":", "\n", "        ", "len_src", "=", "tf", ".", "shape", "(", "inputs", ")", "[", "1", "]", "\n", "batchsize", "=", "tf", ".", "shape", "(", "inputs", ")", "[", "0", "]", "\n", "dim", "=", "tf", ".", "shape", "(", "inputs", ")", "[", "2", "]", "\n", "x", "=", "inputs", "\n", "w_x_last", "=", "lrp", ".", "create_diagonal_v2n", "(", "batchsize", ",", "len_src", ",", "dim", ")", "\n", "for", "layer", "in", "range", "(", "params", ".", "num_encoder_layers", ")", ":", "\n", "            ", "with", "tf", ".", "variable_scope", "(", "\"layer_%d\"", "%", "layer", ")", ":", "\n", "                ", "with", "tf", ".", "variable_scope", "(", "\"self_attention\"", ")", ":", "\n", "                    ", "y_self", "=", "lrp", ".", "multihead_attention_v2n", "(", "\n", "layer_process", "(", "x", ",", "params", ".", "layer_preprocess", ")", ",", "\n", "None", ",", "\n", "bias", ",", "\n", "w_x_last", ",", "\n", "params", ".", "num_heads", ",", "\n", "params", ".", "attention_key_channels", "or", "params", ".", "hidden_size", ",", "\n", "params", ".", "attention_value_channels", "or", "params", ".", "hidden_size", ",", "\n", "params", ".", "hidden_size", ",", "\n", "params", ",", "\n", "1.0", "-", "params", ".", "attention_dropout", "\n", ")", "\n", "y", "=", "y_self", "[", "\"outputs\"", "]", "\n", "w_x_self", "=", "y_self", "[", "\"weight_ratio\"", "]", "\n", "\n", "x_res", "=", "residual_fn", "(", "x", ",", "y", ",", "w_x_last", ",", "w_x_self", ",", "params", ",", "\n", "1.0", "-", "params", ".", "residual_dropout", ")", "\n", "x", "=", "x_res", "[", "\"output\"", "]", "\n", "w_x_selfres", "=", "x_res", "[", "\"weight_ratio\"", "]", "\n", "\n", "x_norm", "=", "lrp", ".", "layer_process", "(", "x", ",", "params", ".", "layer_postprocess", ",", "\n", "w_x_selfres", ",", "params", ")", "\n", "x", "=", "x_norm", "[", "\"outputs\"", "]", "\n", "w_x_selfres", "=", "x_norm", "[", "\"weight_ratios\"", "]", "\n", "\n", "", "with", "tf", ".", "variable_scope", "(", "\"feed_forward\"", ")", ":", "\n", "                    ", "y_ffn", "=", "ffn_layer", "(", "\n", "layer_process", "(", "x", ",", "params", ".", "layer_preprocess", ")", ",", "\n", "w_x_selfres", ",", "\n", "params", ".", "filter_size", ",", "\n", "params", ".", "hidden_size", ",", "\n", "params", ",", "\n", "1.0", "-", "params", ".", "relu_dropout", ",", "\n", ")", "\n", "y", "=", "y_ffn", "[", "\"output\"", "]", "\n", "w_x_ffn", "=", "y_ffn", "[", "\"weight_ratios\"", "]", "\n", "\n", "x_res", "=", "residual_fn", "(", "x", ",", "y", ",", "w_x_selfres", ",", "w_x_ffn", ",", "params", ",", "\n", "1.0", "-", "params", ".", "residual_dropout", ")", "\n", "x", "=", "x_res", "[", "\"output\"", "]", "\n", "w_x_ffnres", "=", "x_res", "[", "\"weight_ratio\"", "]", "\n", "\n", "x_norm", "=", "lrp", ".", "layer_process", "(", "x", ",", "params", ".", "layer_postprocess", ",", "\n", "w_x_ffnres", ",", "params", ")", "\n", "x", "=", "x_norm", "[", "\"outputs\"", "]", "\n", "w_x_ffnres", "=", "x_norm", "[", "\"weight_ratios\"", "]", "\n", "\n", "w_x_last", "=", "w_x_ffnres", "\n", "\n", "", "", "", "outputs", "=", "layer_process", "(", "x", ",", "params", ".", "layer_preprocess", ")", "\n", "return", "{", "\"outputs\"", ":", "outputs", ",", "\"weight_ratios\"", ":", "w_x_last", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.transformer_lrp.transformer_decoder": [[186, 278], ["tensorflow.variable_scope", "tensorflow.zeros", "range", "transformer_lrp.layer_process", "tensorflow.shape", "tensorflow.shape", "tensorflow.shape", "tensorflow.shape", "tensorflow.variable_scope", "tensorflow.variable_scope", "thumt.multihead_attention_v2n", "transformer_lrp.residual_fn", "thumt.layer_process", "tensorflow.variable_scope", "thumt.multihead_attention_v2n", "transformer_lrp.residual_fn", "thumt.layer_process", "tensorflow.variable_scope", "transformer_lrp.ffn_layer", "transformer_lrp.residual_fn", "thumt.layer_process", "transformer_lrp.layer_process", "transformer_lrp.layer_process", "transformer_lrp.layer_process"], "function", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.transformer_lrp.layer_process", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.lrp.multihead_attention_v2n", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.transformer_lrp.residual_fn", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.transformer_lrp.layer_process", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.lrp.multihead_attention_v2n", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.transformer_lrp.residual_fn", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.transformer_lrp.layer_process", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.transformer_lrp.ffn_layer", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.transformer_lrp.residual_fn", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.transformer_lrp.layer_process", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.transformer_lrp.layer_process", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.transformer_lrp.layer_process", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.transformer_lrp.layer_process"], ["", "", "def", "transformer_decoder", "(", "inputs", ",", "memory", ",", "bias", ",", "mem_bias", ",", "w_x_enc", ",", "params", ",", "\n", "dtype", "=", "None", ",", "scope", "=", "None", ")", ":", "\n", "    ", "with", "tf", ".", "variable_scope", "(", "scope", ",", "default_name", "=", "\"decoder\"", ",", "dtype", "=", "dtype", ",", "\n", "values", "=", "[", "inputs", ",", "memory", ",", "bias", ",", "mem_bias", "]", ")", ":", "\n", "        ", "len_src", "=", "tf", ".", "shape", "(", "memory", ")", "[", "1", "]", "\n", "len_trg", "=", "tf", ".", "shape", "(", "inputs", ")", "[", "1", "]", "\n", "batchsize", "=", "tf", ".", "shape", "(", "inputs", ")", "[", "0", "]", "\n", "dim", "=", "tf", ".", "shape", "(", "inputs", ")", "[", "2", "]", "\n", "x", "=", "inputs", "\n", "w_x_last", "=", "tf", ".", "zeros", "(", "[", "batchsize", ",", "len_src", ",", "len_trg", ",", "dim", "]", ",", "\n", "dtype", "=", "tf", ".", "float32", ")", "\n", "for", "layer", "in", "range", "(", "params", ".", "num_decoder_layers", ")", ":", "\n", "            ", "with", "tf", ".", "variable_scope", "(", "\"layer_%d\"", "%", "layer", ")", ":", "\n", "                ", "with", "tf", ".", "variable_scope", "(", "\"self_attention\"", ")", ":", "\n", "                    ", "y_self", "=", "lrp", ".", "multihead_attention_v2n", "(", "\n", "layer_process", "(", "x", ",", "params", ".", "layer_preprocess", ")", ",", "\n", "None", ",", "\n", "bias", ",", "\n", "w_x_last", ",", "\n", "params", ".", "num_heads", ",", "\n", "params", ".", "attention_key_channels", "or", "params", ".", "hidden_size", ",", "\n", "params", ".", "attention_value_channels", "or", "params", ".", "hidden_size", ",", "\n", "params", ".", "hidden_size", ",", "\n", "params", ",", "\n", "1.0", "-", "params", ".", "attention_dropout", "\n", ")", "\n", "y", "=", "y_self", "[", "\"outputs\"", "]", "\n", "w_x_self", "=", "y_self", "[", "\"weight_ratio\"", "]", "\n", "\n", "x_res", "=", "residual_fn", "(", "x", ",", "y", ",", "w_x_last", ",", "w_x_self", ",", "params", ",", "\n", "1.0", "-", "params", ".", "residual_dropout", ")", "\n", "x", "=", "x_res", "[", "\"output\"", "]", "\n", "w_x_selfres", "=", "x_res", "[", "\"weight_ratio\"", "]", "\n", "\n", "x_norm", "=", "lrp", ".", "layer_process", "(", "x", ",", "params", ".", "layer_postprocess", ",", "\n", "w_x_selfres", ",", "params", ")", "\n", "x", "=", "x_norm", "[", "\"outputs\"", "]", "\n", "w_x_selfres", "=", "x_norm", "[", "\"weight_ratios\"", "]", "\n", "\n", "", "with", "tf", ".", "variable_scope", "(", "\"encdec_attention\"", ")", ":", "\n", "                    ", "y_encdec", "=", "lrp", ".", "multihead_attention_v2n", "(", "\n", "layer_process", "(", "x", ",", "params", ".", "layer_preprocess", ")", ",", "\n", "memory", ",", "\n", "mem_bias", ",", "\n", "w_x_enc", ",", "\n", "params", ".", "num_heads", ",", "\n", "params", ".", "attention_key_channels", "or", "params", ".", "hidden_size", ",", "\n", "params", ".", "attention_value_channels", "or", "params", ".", "hidden_size", ",", "\n", "params", ".", "hidden_size", ",", "\n", "params", ",", "\n", "1.0", "-", "params", ".", "attention_dropout", "\n", ")", "\n", "y", "=", "y_encdec", "[", "\"outputs\"", "]", "\n", "w_x_encdec", "=", "y_encdec", "[", "\"weight_ratio\"", "]", "\n", "\n", "x_res", "=", "residual_fn", "(", "x", ",", "y", ",", "w_x_selfres", ",", "w_x_encdec", ",", "params", ",", "\n", "1.0", "-", "params", ".", "residual_dropout", ")", "\n", "x", "=", "x_res", "[", "\"output\"", "]", "\n", "w_x_encdecres", "=", "x_res", "[", "\"weight_ratio\"", "]", "\n", "\n", "x_norm", "=", "lrp", ".", "layer_process", "(", "x", ",", "params", ".", "layer_postprocess", ",", "\n", "w_x_encdecres", ",", "params", ")", "\n", "x", "=", "x_norm", "[", "\"outputs\"", "]", "\n", "w_x_encdecres", "=", "x_norm", "[", "\"weight_ratios\"", "]", "\n", "\n", "", "with", "tf", ".", "variable_scope", "(", "\"feed_forward\"", ")", ":", "\n", "                    ", "y_ffn", "=", "ffn_layer", "(", "\n", "layer_process", "(", "x", ",", "params", ".", "layer_preprocess", ")", ",", "\n", "w_x_encdecres", ",", "\n", "params", ".", "filter_size", ",", "\n", "params", ".", "hidden_size", ",", "\n", "params", ",", "\n", "1.0", "-", "params", ".", "relu_dropout", ",", "\n", ")", "\n", "y", "=", "y_ffn", "[", "\"output\"", "]", "\n", "w_x_ffn", "=", "y_ffn", "[", "\"weight_ratios\"", "]", "\n", "\n", "x_res", "=", "residual_fn", "(", "x", ",", "y", ",", "w_x_encdecres", ",", "w_x_ffn", ",", "params", ",", "\n", "1.0", "-", "params", ".", "residual_dropout", ")", "\n", "x", "=", "x_res", "[", "\"output\"", "]", "\n", "w_x_ffnres", "=", "x_res", "[", "\"weight_ratio\"", "]", "\n", "\n", "x_norm", "=", "lrp", ".", "layer_process", "(", "x", ",", "params", ".", "layer_postprocess", ",", "\n", "w_x_ffnres", ",", "params", ")", "\n", "x", "=", "x_norm", "[", "\"outputs\"", "]", "\n", "w_x_ffnres", "=", "x_norm", "[", "\"weight_ratios\"", "]", "\n", "\n", "w_x_last", "=", "w_x_ffnres", "\n", "\n", "", "", "", "outputs", "=", "layer_process", "(", "x", ",", "params", ".", "layer_preprocess", ")", "\n", "\n", "return", "{", "\"outputs\"", ":", "outputs", ",", "\"weight_ratios\"", ":", "w_x_last", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.transformer_lrp.model_graph": [[280, 364], ["tensorflow.sequence_mask", "tensorflow.sequence_mask", "transformer_lrp.get_weights", "tensorflow.get_variable", "tensorflow.nn.bias_add", "thumt.attention.add_timing_signal", "thumt.attention.attention_bias", "thumt.attention.attention_bias", "thumt.attention.add_timing_signal", "transformer_lrp.transformer_encoder", "transformer_lrp.transformer_decoder", "tensorflow.gather", "tensorflow.reduce_sum", "thumt.stabilize", "tensorflow.reduce_sum", "tensorflow.reshape", "tensorflow.matmul", "thumt.smoothed_softmax_cross_entropy_with_logits", "tensorflow.reshape", "tensorflow.transpose", "transformer_lrp.normalize", "tensorflow.gather", "tensorflow.gather", "tensorflow.expand_dims", "tensorflow.expand_dims", "tensorflow.pad", "tensorflow.nn.dropout", "tensorflow.nn.dropout", "tensorflow.expand_dims", "tensorflow.expand_dims", "tensorflow.matmul", "tensorflow.shape", "tensorflow.reduce_sum", "tensorflow.reduce_sum", "tensorflow.shape", "tensorflow.shape", "tensorflow.shape"], "function", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.transformer_lrp.get_weights", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.layers.attention.add_timing_signal", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.layers.attention.attention_bias", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.layers.attention.attention_bias", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.layers.attention.add_timing_signal", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.transformer_lrp.transformer_encoder", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.transformer_lrp.transformer_decoder", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.rnnsearch_lrp.stabilize", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.losses.losses.smoothed_softmax_cross_entropy_with_logits", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.models.transformer_lrp.normalize"], ["", "", "def", "model_graph", "(", "features", ",", "labels", ",", "mode", ",", "params", ")", ":", "\n", "    ", "hidden_size", "=", "params", ".", "hidden_size", "\n", "\n", "src_seq", "=", "features", "[", "\"source\"", "]", "\n", "tgt_seq", "=", "features", "[", "\"target\"", "]", "\n", "src_len", "=", "features", "[", "\"source_length\"", "]", "\n", "tgt_len", "=", "features", "[", "\"target_length\"", "]", "\n", "src_mask", "=", "tf", ".", "sequence_mask", "(", "src_len", ",", "\n", "maxlen", "=", "tf", ".", "shape", "(", "features", "[", "\"source\"", "]", ")", "[", "1", "]", ",", "\n", "dtype", "=", "tf", ".", "float32", ")", "\n", "tgt_mask", "=", "tf", ".", "sequence_mask", "(", "tgt_len", ",", "\n", "maxlen", "=", "tf", ".", "shape", "(", "features", "[", "\"target\"", "]", ")", "[", "1", "]", ",", "\n", "dtype", "=", "tf", ".", "float32", ")", "\n", "\n", "src_embedding", ",", "tgt_embedding", ",", "weights", "=", "get_weights", "(", "params", ")", "\n", "bias", "=", "tf", ".", "get_variable", "(", "\"bias\"", ",", "[", "hidden_size", "]", ")", "\n", "\n", "# id => embedding", "\n", "# src_seq: [batch, max_src_length]", "\n", "# tgt_seq: [batch, max_tgt_length]", "\n", "inputs", "=", "tf", ".", "gather", "(", "src_embedding", ",", "src_seq", ")", "*", "(", "hidden_size", "**", "0.5", ")", "\n", "targets", "=", "tf", ".", "gather", "(", "tgt_embedding", ",", "tgt_seq", ")", "*", "(", "hidden_size", "**", "0.5", ")", "\n", "inputs", "=", "inputs", "*", "tf", ".", "expand_dims", "(", "src_mask", ",", "-", "1", ")", "\n", "targets", "=", "targets", "*", "tf", ".", "expand_dims", "(", "tgt_mask", ",", "-", "1", ")", "\n", "\n", "# Preparing encoder & decoder input", "\n", "encoder_input", "=", "tf", ".", "nn", ".", "bias_add", "(", "inputs", ",", "bias", ")", "\n", "encoder_input", "=", "layers", ".", "attention", ".", "add_timing_signal", "(", "encoder_input", ")", "\n", "enc_attn_bias", "=", "layers", ".", "attention", ".", "attention_bias", "(", "src_mask", ",", "\"masking\"", ")", "\n", "dec_attn_bias", "=", "layers", ".", "attention", ".", "attention_bias", "(", "tf", ".", "shape", "(", "targets", ")", "[", "1", "]", ",", "\n", "\"causal\"", ")", "\n", "\n", "# Shift left", "\n", "decoder_input", "=", "tf", ".", "pad", "(", "targets", ",", "[", "[", "0", ",", "0", "]", ",", "[", "1", ",", "0", "]", ",", "[", "0", ",", "0", "]", "]", ")", "[", ":", ",", ":", "-", "1", ",", ":", "]", "\n", "decoder_input", "=", "layers", ".", "attention", ".", "add_timing_signal", "(", "decoder_input", ")", "\n", "\n", "if", "params", ".", "residual_dropout", ":", "\n", "        ", "keep_prob", "=", "1.0", "-", "params", ".", "residual_dropout", "\n", "encoder_input", "=", "tf", ".", "nn", ".", "dropout", "(", "encoder_input", ",", "keep_prob", ")", "\n", "decoder_input", "=", "tf", ".", "nn", ".", "dropout", "(", "decoder_input", ",", "keep_prob", ")", "\n", "\n", "", "encoder_out", "=", "transformer_encoder", "(", "encoder_input", ",", "enc_attn_bias", ",", "params", ")", "\n", "encoder_output", "=", "encoder_out", "[", "\"outputs\"", "]", "\n", "w_x_enc", "=", "encoder_out", "[", "\"weight_ratios\"", "]", "\n", "decoder_out", "=", "transformer_decoder", "(", "decoder_input", ",", "encoder_output", ",", "\n", "dec_attn_bias", ",", "enc_attn_bias", ",", "\n", "w_x_enc", ",", "params", ")", "\n", "decoder_output", "=", "decoder_out", "[", "\"outputs\"", "]", "\n", "w_x_dec", "=", "decoder_out", "[", "\"weight_ratios\"", "]", "\n", "\n", "weights_true", "=", "tf", ".", "gather", "(", "weights", ",", "labels", ")", "\n", "logits_elewise_true", "=", "decoder_output", "*", "weights_true", "\n", "logits_true", "=", "tf", ".", "reduce_sum", "(", "logits_elewise_true", ",", "-", "1", ")", "\n", "logits_stab", "=", "lrp", ".", "stabilize", "(", "tf", ".", "expand_dims", "(", "logits_true", ",", "-", "1", ")", ",", "params", ".", "stab", ")", "\n", "wr_logit_decoder", "=", "logits_elewise_true", "/", "logits_stab", "\n", "w_x_true", "=", "w_x_dec", "*", "tf", ".", "expand_dims", "(", "wr_logit_decoder", ",", "1", ")", "\n", "w_x_true", "=", "tf", ".", "reduce_sum", "(", "w_x_true", ",", "-", "1", ")", "\n", "# inference mode, take the last position", "\n", "if", "mode", "==", "\"infer\"", ":", "\n", "        ", "decoder_output", "=", "decoder_output", "[", ":", ",", "-", "1", ",", ":", "]", "\n", "logits", "=", "tf", ".", "matmul", "(", "decoder_output", ",", "weights", ",", "False", ",", "True", ")", "\n", "\n", "return", "logits", "\n", "\n", "# [batch, length, channel] => [batch * length, vocab_size]", "\n", "", "decoder_output", "=", "tf", ".", "reshape", "(", "decoder_output", ",", "[", "-", "1", ",", "hidden_size", "]", ")", "\n", "logits", "=", "tf", ".", "matmul", "(", "decoder_output", ",", "weights", ",", "False", ",", "True", ")", "\n", "\n", "# label smoothing", "\n", "ce", "=", "losses", ".", "smoothed_softmax_cross_entropy_with_logits", "(", "\n", "logits", "=", "logits", ",", "\n", "labels", "=", "labels", ",", "\n", "smoothing", "=", "params", ".", "label_smoothing", ",", "\n", "normalize", "=", "True", "\n", ")", "\n", "\n", "ce", "=", "tf", ".", "reshape", "(", "ce", ",", "tf", ".", "shape", "(", "tgt_seq", ")", ")", "\n", "loss", "=", "tf", ".", "reduce_sum", "(", "ce", "*", "tgt_mask", ")", "/", "tf", ".", "reduce_sum", "(", "tgt_mask", ")", "\n", "\n", "rlv_info", "=", "{", "}", "\n", "R_x_true", "=", "tf", ".", "transpose", "(", "w_x_true", ",", "[", "0", ",", "2", ",", "1", "]", ")", "\n", "rlv_info", "[", "\"result\"", "]", "=", "normalize", "(", "R_x_true", ",", "True", ")", "\n", "\n", "return", "loss", ",", "rlv_info", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.data.dataset.batch_examples": [[16, 83], ["tensorflow.name_scope", "example.values", "tensorflow.contrib.training.bucket_by_sequence_length", "boundaries.append", "max", "max", "tensorflow.maximum", "tensorflow.shape", "int", "math.log"], "function", ["None"], ["def", "batch_examples", "(", "example", ",", "batch_size", ",", "max_length", ",", "mantissa_bits", ",", "\n", "shard_multiplier", "=", "1", ",", "length_multiplier", "=", "1", ",", "constant", "=", "False", ",", "\n", "num_threads", "=", "4", ",", "drop_long_sequences", "=", "True", ")", ":", "\n", "    ", "\"\"\" Batch examples\n\n    :param example: A dictionary of <feature name, Tensor>.\n    :param batch_size: The number of tokens or sentences in a batch\n    :param max_length: The maximum length of a example to keep\n    :param mantissa_bits: An integer\n    :param shard_multiplier: an integer increasing the batch_size to suit\n        splitting across data shards.\n    :param length_multiplier: an integer multiplier that is used to\n        increase the batch sizes and sequence length tolerance.\n    :param constant: Whether to use constant batch size\n    :param num_threads: Number of threads\n    :param drop_long_sequences: Whether to drop long sequences\n\n    :returns: A dictionary of batched examples\n    \"\"\"", "\n", "\n", "with", "tf", ".", "name_scope", "(", "\"batch_examples\"", ")", ":", "\n", "        ", "max_length", "=", "max_length", "or", "batch_size", "\n", "min_length", "=", "8", "\n", "mantissa_bits", "=", "mantissa_bits", "\n", "\n", "# Compute boundaries", "\n", "x", "=", "min_length", "\n", "boundaries", "=", "[", "]", "\n", "\n", "while", "x", "<", "max_length", ":", "\n", "            ", "boundaries", ".", "append", "(", "x", ")", "\n", "x", "+=", "2", "**", "max", "(", "0", ",", "int", "(", "math", ".", "log", "(", "x", ",", "2", ")", ")", "-", "mantissa_bits", ")", "\n", "\n", "# Whether the batch size is constant", "\n", "", "if", "not", "constant", ":", "\n", "            ", "batch_sizes", "=", "[", "max", "(", "1", ",", "batch_size", "//", "length", ")", "\n", "for", "length", "in", "boundaries", "+", "[", "max_length", "]", "]", "\n", "batch_sizes", "=", "[", "b", "*", "shard_multiplier", "for", "b", "in", "batch_sizes", "]", "\n", "bucket_capacities", "=", "[", "2", "*", "b", "for", "b", "in", "batch_sizes", "]", "\n", "", "else", ":", "\n", "            ", "batch_sizes", "=", "batch_size", "*", "shard_multiplier", "\n", "bucket_capacities", "=", "[", "2", "*", "n", "for", "n", "in", "boundaries", "+", "[", "max_length", "]", "]", "\n", "\n", "", "max_length", "*=", "length_multiplier", "\n", "boundaries", "=", "[", "boundary", "*", "length_multiplier", "for", "boundary", "in", "boundaries", "]", "\n", "max_length", "=", "max_length", "if", "drop_long_sequences", "else", "10", "**", "9", "\n", "\n", "# The queue to bucket on will be chosen based on maximum length", "\n", "max_example_length", "=", "0", "\n", "for", "v", "in", "example", ".", "values", "(", ")", ":", "\n", "            ", "if", "v", ".", "shape", ".", "ndims", ">", "0", ":", "\n", "                ", "seq_length", "=", "tf", ".", "shape", "(", "v", ")", "[", "0", "]", "\n", "max_example_length", "=", "tf", ".", "maximum", "(", "max_example_length", ",", "seq_length", ")", "\n", "\n", "", "", "(", "_", ",", "outputs", ")", "=", "tf", ".", "contrib", ".", "training", ".", "bucket_by_sequence_length", "(", "\n", "max_example_length", ",", "\n", "example", ",", "\n", "batch_sizes", ",", "\n", "[", "b", "+", "1", "for", "b", "in", "boundaries", "]", ",", "\n", "num_threads", "=", "num_threads", ",", "\n", "capacity", "=", "2", ",", "# Number of full batches to store, we don't need many.", "\n", "bucket_capacities", "=", "bucket_capacities", ",", "\n", "dynamic_pad", "=", "True", ",", "\n", "keep_input", "=", "(", "max_example_length", "<=", "max_length", ")", "\n", ")", "\n", "\n", "", "return", "outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.data.dataset.get_training_input": [[85, 170], ["tensorflow.device", "tensorflow.data.TextLineDataset", "tensorflow.data.TextLineDataset", "tensorflow.data.Dataset.zip", "thumt.is_distributed_training_mode", "dataset.shard.shuffle", "dataset.shard.repeat", "dataset.shard.map", "dataset.shard.map", "dataset.shard.map", "dataset.shard.make_one_shot_iterator", "dataset.make_one_shot_iterator.get_next", "tensorflow.contrib.lookup.index_table_from_tensor", "tensorflow.contrib.lookup.index_table_from_tensor", "tf.contrib.lookup.index_table_from_tensor.lookup", "tf.contrib.lookup.index_table_from_tensor.lookup", "dataset.batch_examples", "tensorflow.to_int32", "tensorflow.to_int32", "tensorflow.to_int32", "tensorflow.to_int32", "tensorflow.squeeze", "tensorflow.squeeze", "dataset.shard.shard", "tensorflow.constant", "tensorflow.constant", "thumt.size", "thumt.rank", "len", "tensorflow.concat", "tensorflow.concat", "tensorflow.shape", "tensorflow.shape", "tensorflow.string_split", "tensorflow.string_split", "tensorflow.constant", "tensorflow.constant"], "function", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.distribute.is_distributed_training_mode", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.data.record.batch_examples", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.distribute.size", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.utils.distribute.rank"], ["", "def", "get_training_input", "(", "filenames", ",", "params", ")", ":", "\n", "    ", "\"\"\" Get input for training stage\n\n    :param filenames: A list contains [source_filenames, target_filenames]\n    :param params: Hyper-parameters\n\n    :returns: A dictionary of pair <Key, Tensor>\n    \"\"\"", "\n", "\n", "with", "tf", ".", "device", "(", "\"/cpu:0\"", ")", ":", "\n", "        ", "src_dataset", "=", "tf", ".", "data", ".", "TextLineDataset", "(", "filenames", "[", "0", "]", ")", "\n", "tgt_dataset", "=", "tf", ".", "data", ".", "TextLineDataset", "(", "filenames", "[", "1", "]", ")", "\n", "\n", "dataset", "=", "tf", ".", "data", ".", "Dataset", ".", "zip", "(", "(", "src_dataset", ",", "tgt_dataset", ")", ")", "\n", "\n", "if", "distribute", ".", "is_distributed_training_mode", "(", ")", ":", "\n", "            ", "dataset", "=", "dataset", ".", "shard", "(", "distribute", ".", "size", "(", ")", ",", "distribute", ".", "rank", "(", ")", ")", "\n", "\n", "", "dataset", "=", "dataset", ".", "shuffle", "(", "params", ".", "buffer_size", ")", "\n", "dataset", "=", "dataset", ".", "repeat", "(", ")", "\n", "\n", "# Split string", "\n", "dataset", "=", "dataset", ".", "map", "(", "\n", "lambda", "src", ",", "tgt", ":", "(", "\n", "tf", ".", "string_split", "(", "[", "src", "]", ")", ".", "values", ",", "\n", "tf", ".", "string_split", "(", "[", "tgt", "]", ")", ".", "values", "\n", ")", ",", "\n", "num_parallel_calls", "=", "params", ".", "num_threads", "\n", ")", "\n", "\n", "# Append <eos> symbol", "\n", "dataset", "=", "dataset", ".", "map", "(", "\n", "lambda", "src", ",", "tgt", ":", "(", "\n", "tf", ".", "concat", "(", "[", "src", ",", "[", "tf", ".", "constant", "(", "params", ".", "eos", ")", "]", "]", ",", "axis", "=", "0", ")", ",", "\n", "tf", ".", "concat", "(", "[", "tgt", ",", "[", "tf", ".", "constant", "(", "params", ".", "eos", ")", "]", "]", ",", "axis", "=", "0", ")", "\n", ")", ",", "\n", "num_parallel_calls", "=", "params", ".", "num_threads", "\n", ")", "\n", "\n", "# Convert to dictionary", "\n", "dataset", "=", "dataset", ".", "map", "(", "\n", "lambda", "src", ",", "tgt", ":", "{", "\n", "\"source\"", ":", "src", ",", "\n", "\"target\"", ":", "tgt", ",", "\n", "\"source_length\"", ":", "tf", ".", "shape", "(", "src", ")", ",", "\n", "\"target_length\"", ":", "tf", ".", "shape", "(", "tgt", ")", "\n", "}", ",", "\n", "num_parallel_calls", "=", "params", ".", "num_threads", "\n", ")", "\n", "\n", "# Create iterator", "\n", "iterator", "=", "dataset", ".", "make_one_shot_iterator", "(", ")", "\n", "features", "=", "iterator", ".", "get_next", "(", ")", "\n", "\n", "# Create lookup table", "\n", "src_table", "=", "tf", ".", "contrib", ".", "lookup", ".", "index_table_from_tensor", "(", "\n", "tf", ".", "constant", "(", "params", ".", "vocabulary", "[", "\"source\"", "]", ")", ",", "\n", "default_value", "=", "params", ".", "mapping", "[", "\"source\"", "]", "[", "params", ".", "unk", "]", "\n", ")", "\n", "tgt_table", "=", "tf", ".", "contrib", ".", "lookup", ".", "index_table_from_tensor", "(", "\n", "tf", ".", "constant", "(", "params", ".", "vocabulary", "[", "\"target\"", "]", ")", ",", "\n", "default_value", "=", "params", ".", "mapping", "[", "\"target\"", "]", "[", "params", ".", "unk", "]", "\n", ")", "\n", "\n", "# String to index lookup", "\n", "features", "[", "\"source\"", "]", "=", "src_table", ".", "lookup", "(", "features", "[", "\"source\"", "]", ")", "\n", "features", "[", "\"target\"", "]", "=", "tgt_table", ".", "lookup", "(", "features", "[", "\"target\"", "]", ")", "\n", "\n", "# Batching", "\n", "features", "=", "batch_examples", "(", "features", ",", "params", ".", "batch_size", ",", "\n", "params", ".", "max_length", ",", "params", ".", "mantissa_bits", ",", "\n", "shard_multiplier", "=", "len", "(", "params", ".", "device_list", ")", ",", "\n", "length_multiplier", "=", "params", ".", "length_multiplier", ",", "\n", "constant", "=", "params", ".", "constant_batch_size", ",", "\n", "num_threads", "=", "params", ".", "num_threads", ")", "\n", "\n", "# Convert to int32", "\n", "features", "[", "\"source\"", "]", "=", "tf", ".", "to_int32", "(", "features", "[", "\"source\"", "]", ")", "\n", "features", "[", "\"target\"", "]", "=", "tf", ".", "to_int32", "(", "features", "[", "\"target\"", "]", ")", "\n", "features", "[", "\"source_length\"", "]", "=", "tf", ".", "to_int32", "(", "features", "[", "\"source_length\"", "]", ")", "\n", "features", "[", "\"target_length\"", "]", "=", "tf", ".", "to_int32", "(", "features", "[", "\"target_length\"", "]", ")", "\n", "features", "[", "\"source_length\"", "]", "=", "tf", ".", "squeeze", "(", "features", "[", "\"source_length\"", "]", ",", "1", ")", "\n", "features", "[", "\"target_length\"", "]", "=", "tf", ".", "squeeze", "(", "features", "[", "\"target_length\"", "]", ",", "1", ")", "\n", "\n", "return", "features", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.data.dataset.sort_input_file": [[172, 191], ["sorted", "enumerate", "tensorflow.gfile.Open", "sorted_inputs.append", "line.strip", "len", "enumerate", "operator.itemgetter", "line.strip().split", "line.strip"], "function", ["None"], ["", "", "def", "sort_input_file", "(", "filename", ",", "reverse", "=", "True", ")", ":", "\n", "# Read file", "\n", "    ", "with", "tf", ".", "gfile", ".", "Open", "(", "filename", ")", "as", "fd", ":", "\n", "        ", "inputs", "=", "[", "line", ".", "strip", "(", ")", "for", "line", "in", "fd", "]", "\n", "\n", "", "input_lens", "=", "[", "\n", "(", "i", ",", "len", "(", "line", ".", "strip", "(", ")", ".", "split", "(", ")", ")", ")", "for", "i", ",", "line", "in", "enumerate", "(", "inputs", ")", "\n", "]", "\n", "\n", "sorted_input_lens", "=", "sorted", "(", "input_lens", ",", "key", "=", "operator", ".", "itemgetter", "(", "1", ")", ",", "\n", "reverse", "=", "reverse", ")", "\n", "sorted_keys", "=", "{", "}", "\n", "sorted_inputs", "=", "[", "]", "\n", "\n", "for", "i", ",", "(", "index", ",", "_", ")", "in", "enumerate", "(", "sorted_input_lens", ")", ":", "\n", "        ", "sorted_inputs", ".", "append", "(", "inputs", "[", "index", "]", ")", "\n", "sorted_keys", "[", "index", "]", "=", "i", "\n", "\n", "", "return", "sorted_keys", ",", "sorted_inputs", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.data.dataset.sort_and_zip_files": [[193, 218], ["zip", "sorted", "enumerate", "tensorflow.gfile.GFile", "input_lens.append", "inputs.append", "fd.close", "sorted_inputs.append", "list", "line.strip", "operator.itemgetter", "zip", "len", "lines[].split"], "function", ["None"], ["", "def", "sort_and_zip_files", "(", "names", ")", ":", "\n", "    ", "inputs", "=", "[", "]", "\n", "input_lens", "=", "[", "]", "\n", "files", "=", "[", "tf", ".", "gfile", ".", "GFile", "(", "name", ")", "for", "name", "in", "names", "]", "\n", "\n", "count", "=", "0", "\n", "\n", "for", "lines", "in", "zip", "(", "*", "files", ")", ":", "\n", "        ", "lines", "=", "[", "line", ".", "strip", "(", ")", "for", "line", "in", "lines", "]", "\n", "input_lens", ".", "append", "(", "(", "count", ",", "len", "(", "lines", "[", "0", "]", ".", "split", "(", ")", ")", ")", ")", "\n", "inputs", ".", "append", "(", "lines", ")", "\n", "count", "+=", "1", "\n", "\n", "# Close files", "\n", "", "for", "fd", "in", "files", ":", "\n", "        ", "fd", ".", "close", "(", ")", "\n", "\n", "", "sorted_input_lens", "=", "sorted", "(", "input_lens", ",", "key", "=", "operator", ".", "itemgetter", "(", "1", ")", ",", "\n", "reverse", "=", "True", ")", "\n", "sorted_inputs", "=", "[", "]", "\n", "\n", "for", "i", ",", "(", "index", ",", "_", ")", "in", "enumerate", "(", "sorted_input_lens", ")", ":", "\n", "        ", "sorted_inputs", ".", "append", "(", "inputs", "[", "index", "]", ")", "\n", "\n", "", "return", "[", "list", "(", "x", ")", "for", "x", "in", "zip", "(", "*", "sorted_inputs", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.data.dataset.get_evaluation_input": [[220, 275], ["tensorflow.device", "tensorflow.data.Dataset.zip", "dataset.map.map", "dataset.map.padded_batch", "dataset.map.make_one_shot_iterator", "dataset.make_one_shot_iterator.get_next", "tensorflow.contrib.lookup.index_table_from_tensor", "tf.contrib.lookup.index_table_from_tensor.lookup", "tensorflow.data.Dataset.from_tensor_slices", "dataset.map.map", "dataset.map.map", "datasets.append", "tuple", "tensorflow.constant", "tensorflow.concat", "tensorflow.Dimension", "tensorflow.string_split", "tensorflow.shape", "tensorflow.Dimension", "len", "len", "tensorflow.constant"], "function", ["None"], ["", "def", "get_evaluation_input", "(", "inputs", ",", "params", ")", ":", "\n", "    ", "with", "tf", ".", "device", "(", "\"/cpu:0\"", ")", ":", "\n", "# Create datasets", "\n", "        ", "datasets", "=", "[", "]", "\n", "\n", "for", "data", "in", "inputs", ":", "\n", "            ", "dataset", "=", "tf", ".", "data", ".", "Dataset", ".", "from_tensor_slices", "(", "data", ")", "\n", "# Split string", "\n", "dataset", "=", "dataset", ".", "map", "(", "lambda", "x", ":", "tf", ".", "string_split", "(", "[", "x", "]", ")", ".", "values", ",", "\n", "num_parallel_calls", "=", "params", ".", "num_threads", ")", "\n", "# Append <eos>", "\n", "dataset", "=", "dataset", ".", "map", "(", "\n", "lambda", "x", ":", "tf", ".", "concat", "(", "[", "x", ",", "[", "tf", ".", "constant", "(", "params", ".", "eos", ")", "]", "]", ",", "axis", "=", "0", ")", ",", "\n", "num_parallel_calls", "=", "params", ".", "num_threads", "\n", ")", "\n", "datasets", ".", "append", "(", "dataset", ")", "\n", "\n", "", "dataset", "=", "tf", ".", "data", ".", "Dataset", ".", "zip", "(", "tuple", "(", "datasets", ")", ")", "\n", "\n", "# Convert tuple to dictionary", "\n", "dataset", "=", "dataset", ".", "map", "(", "\n", "lambda", "*", "x", ":", "{", "\n", "\"source\"", ":", "x", "[", "0", "]", ",", "\n", "\"source_length\"", ":", "tf", ".", "shape", "(", "x", "[", "0", "]", ")", "[", "0", "]", ",", "\n", "\"references\"", ":", "x", "[", "1", ":", "]", "\n", "}", ",", "\n", "num_parallel_calls", "=", "params", ".", "num_threads", "\n", ")", "\n", "\n", "dataset", "=", "dataset", ".", "padded_batch", "(", "\n", "params", ".", "eval_batch_size", ",", "\n", "{", "\n", "\"source\"", ":", "[", "tf", ".", "Dimension", "(", "None", ")", "]", ",", "\n", "\"source_length\"", ":", "[", "]", ",", "\n", "\"references\"", ":", "(", "tf", ".", "Dimension", "(", "None", ")", ",", ")", "*", "(", "len", "(", "inputs", ")", "-", "1", ")", "\n", "}", ",", "\n", "{", "\n", "\"source\"", ":", "params", ".", "pad", ",", "\n", "\"source_length\"", ":", "0", ",", "\n", "\"references\"", ":", "(", "params", ".", "pad", ",", ")", "*", "(", "len", "(", "inputs", ")", "-", "1", ")", "\n", "}", "\n", ")", "\n", "\n", "iterator", "=", "dataset", ".", "make_one_shot_iterator", "(", ")", "\n", "features", "=", "iterator", ".", "get_next", "(", ")", "\n", "\n", "# Covert source symbols to ids", "\n", "src_table", "=", "tf", ".", "contrib", ".", "lookup", ".", "index_table_from_tensor", "(", "\n", "tf", ".", "constant", "(", "params", ".", "vocabulary", "[", "\"source\"", "]", ")", ",", "\n", "default_value", "=", "params", ".", "mapping", "[", "\"source\"", "]", "[", "params", ".", "unk", "]", "\n", ")", "\n", "\n", "features", "[", "\"source\"", "]", "=", "src_table", ".", "lookup", "(", "features", "[", "\"source\"", "]", ")", "\n", "\n", "", "return", "features", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.data.dataset.get_inference_input": [[277, 320], ["tensorflow.device", "tensorflow.data.Dataset.from_tensor_slices", "dataset.padded_batch.map", "dataset.padded_batch.map", "dataset.padded_batch.map", "dataset.padded_batch.padded_batch", "dataset.padded_batch.make_one_shot_iterator", "dataset.make_one_shot_iterator.get_next", "tensorflow.contrib.lookup.index_table_from_tensor", "tf.contrib.lookup.index_table_from_tensor.lookup", "tensorflow.constant", "tensorflow.constant", "tensorflow.concat", "len", "tensorflow.string_split", "tensorflow.Dimension", "tensorflow.shape", "tensorflow.constant"], "function", ["None"], ["", "def", "get_inference_input", "(", "inputs", ",", "params", ")", ":", "\n", "    ", "if", "params", ".", "generate_samples", ":", "\n", "        ", "batch_size", "=", "params", ".", "sample_batch_size", "\n", "", "else", ":", "\n", "        ", "batch_size", "=", "params", ".", "decode_batch_size", "\n", "\n", "", "with", "tf", ".", "device", "(", "\"/cpu:0\"", ")", ":", "\n", "        ", "dataset", "=", "tf", ".", "data", ".", "Dataset", ".", "from_tensor_slices", "(", "\n", "tf", ".", "constant", "(", "inputs", ")", "\n", ")", "\n", "\n", "# Split string", "\n", "dataset", "=", "dataset", ".", "map", "(", "lambda", "x", ":", "tf", ".", "string_split", "(", "[", "x", "]", ")", ".", "values", ",", "\n", "num_parallel_calls", "=", "params", ".", "num_threads", ")", "\n", "\n", "# Append <eos>", "\n", "dataset", "=", "dataset", ".", "map", "(", "\n", "lambda", "x", ":", "tf", ".", "concat", "(", "[", "x", ",", "[", "tf", ".", "constant", "(", "params", ".", "eos", ")", "]", "]", ",", "axis", "=", "0", ")", ",", "\n", "num_parallel_calls", "=", "params", ".", "num_threads", "\n", ")", "\n", "\n", "# Convert tuple to dictionary", "\n", "dataset", "=", "dataset", ".", "map", "(", "\n", "lambda", "x", ":", "{", "\"source\"", ":", "x", ",", "\"source_length\"", ":", "tf", ".", "shape", "(", "x", ")", "[", "0", "]", "}", ",", "\n", "num_parallel_calls", "=", "params", ".", "num_threads", "\n", ")", "\n", "\n", "dataset", "=", "dataset", ".", "padded_batch", "(", "\n", "batch_size", "*", "len", "(", "params", ".", "device_list", ")", ",", "\n", "{", "\"source\"", ":", "[", "tf", ".", "Dimension", "(", "None", ")", "]", ",", "\"source_length\"", ":", "[", "]", "}", ",", "\n", "{", "\"source\"", ":", "params", ".", "pad", ",", "\"source_length\"", ":", "0", "}", "\n", ")", "\n", "\n", "iterator", "=", "dataset", ".", "make_one_shot_iterator", "(", ")", "\n", "features", "=", "iterator", ".", "get_next", "(", ")", "\n", "\n", "src_table", "=", "tf", ".", "contrib", ".", "lookup", ".", "index_table_from_tensor", "(", "\n", "tf", ".", "constant", "(", "params", ".", "vocabulary", "[", "\"source\"", "]", ")", ",", "\n", "default_value", "=", "params", ".", "mapping", "[", "\"source\"", "]", "[", "params", ".", "unk", "]", "\n", ")", "\n", "features", "[", "\"source\"", "]", "=", "src_table", ".", "lookup", "(", "features", "[", "\"source\"", "]", ")", "\n", "\n", "return", "features", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.data.dataset.get_relevance_input": [[322, 397], ["tensorflow.data.Dataset.from_tensor_slices", "dataset.padded_batch.map", "dataset.padded_batch.map", "dataset.padded_batch.map", "dataset.padded_batch.padded_batch", "dataset.padded_batch.make_one_shot_iterator", "dataset_o.make_one_shot_iterator.get_next", "tensorflow.contrib.lookup.index_table_from_tensor", "tf.contrib.lookup.index_table_from_tensor.lookup", "tensorflow.data.Dataset.from_tensor_slices", "dataset_o.padded_batch.map", "dataset_o.padded_batch.map", "dataset_o.padded_batch.map", "dataset_o.padded_batch.padded_batch", "dataset_o.padded_batch.make_one_shot_iterator", "dataset_o.make_one_shot_iterator.get_next", "tensorflow.contrib.lookup.index_table_from_tensor", "tf.contrib.lookup.index_table_from_tensor.lookup", "tensorflow.constant", "tensorflow.constant", "tensorflow.constant", "tensorflow.constant", "tensorflow.concat", "tensorflow.concat", "tensorflow.string_split", "tensorflow.Dimension", "tensorflow.string_split", "tensorflow.Dimension", "tensorflow.shape", "tensorflow.shape", "tensorflow.constant", "tensorflow.constant"], "function", ["None"], ["", "", "def", "get_relevance_input", "(", "inputs", ",", "outputs", ",", "params", ")", ":", "\n", "# inputs", "\n", "    ", "dataset", "=", "tf", ".", "data", ".", "Dataset", ".", "from_tensor_slices", "(", "\n", "tf", ".", "constant", "(", "inputs", ")", "\n", ")", "\n", "\n", "# Split string", "\n", "dataset", "=", "dataset", ".", "map", "(", "lambda", "x", ":", "tf", ".", "string_split", "(", "[", "x", "]", ")", ".", "values", ",", "\n", "num_parallel_calls", "=", "params", ".", "num_threads", ")", "\n", "\n", "# Append <eos>", "\n", "dataset", "=", "dataset", ".", "map", "(", "\n", "lambda", "x", ":", "tf", ".", "concat", "(", "[", "x", ",", "[", "tf", ".", "constant", "(", "params", ".", "eos", ")", "]", "]", ",", "axis", "=", "0", ")", ",", "\n", "num_parallel_calls", "=", "params", ".", "num_threads", "\n", ")", "\n", "\n", "# Convert tuple to dictionary", "\n", "dataset", "=", "dataset", ".", "map", "(", "\n", "lambda", "x", ":", "{", "\"source\"", ":", "x", ",", "\"source_length\"", ":", "tf", ".", "shape", "(", "x", ")", "[", "0", "]", "}", ",", "\n", "num_parallel_calls", "=", "params", ".", "num_threads", "\n", ")", "\n", "\n", "dataset", "=", "dataset", ".", "padded_batch", "(", "\n", "params", ".", "decode_batch_size", ",", "\n", "{", "\"source\"", ":", "[", "tf", ".", "Dimension", "(", "None", ")", "]", ",", "\"source_length\"", ":", "[", "]", "}", ",", "\n", "{", "\"source\"", ":", "params", ".", "pad", ",", "\"source_length\"", ":", "0", "}", "\n", ")", "\n", "\n", "iterator", "=", "dataset", ".", "make_one_shot_iterator", "(", ")", "\n", "features", "=", "iterator", ".", "get_next", "(", ")", "\n", "\n", "src_table", "=", "tf", ".", "contrib", ".", "lookup", ".", "index_table_from_tensor", "(", "\n", "tf", ".", "constant", "(", "params", ".", "vocabulary", "[", "\"source\"", "]", ")", ",", "\n", "default_value", "=", "params", ".", "mapping", "[", "\"source\"", "]", "[", "params", ".", "unk", "]", "\n", ")", "\n", "features", "[", "\"source\"", "]", "=", "src_table", ".", "lookup", "(", "features", "[", "\"source\"", "]", ")", "\n", "\n", "# outputs", "\n", "dataset_o", "=", "tf", ".", "data", ".", "Dataset", ".", "from_tensor_slices", "(", "\n", "tf", ".", "constant", "(", "outputs", ")", "\n", ")", "\n", "\n", "# Split string", "\n", "dataset_o", "=", "dataset_o", ".", "map", "(", "lambda", "x", ":", "tf", ".", "string_split", "(", "[", "x", "]", ")", ".", "values", ",", "\n", "num_parallel_calls", "=", "params", ".", "num_threads", ")", "\n", "\n", "# Append <eos>", "\n", "dataset_o", "=", "dataset_o", ".", "map", "(", "\n", "lambda", "x", ":", "tf", ".", "concat", "(", "[", "x", ",", "[", "tf", ".", "constant", "(", "params", ".", "eos", ")", "]", "]", ",", "axis", "=", "0", ")", ",", "\n", "num_parallel_calls", "=", "params", ".", "num_threads", "\n", ")", "\n", "\n", "# Convert tuple to dictionary", "\n", "dataset_o", "=", "dataset_o", ".", "map", "(", "\n", "lambda", "x", ":", "{", "\"target\"", ":", "x", ",", "\"target_length\"", ":", "tf", ".", "shape", "(", "x", ")", "[", "0", "]", "}", ",", "\n", "num_parallel_calls", "=", "params", ".", "num_threads", "\n", ")", "\n", "\n", "dataset_o", "=", "dataset_o", ".", "padded_batch", "(", "\n", "params", ".", "decode_batch_size", ",", "\n", "{", "\"target\"", ":", "[", "tf", ".", "Dimension", "(", "None", ")", "]", ",", "\"target_length\"", ":", "[", "]", "}", ",", "\n", "{", "\"target\"", ":", "params", ".", "pad", ",", "\"target_length\"", ":", "0", "}", "\n", ")", "\n", "\n", "iterator", "=", "dataset_o", ".", "make_one_shot_iterator", "(", ")", "\n", "features_o", "=", "iterator", ".", "get_next", "(", ")", "\n", "\n", "src_table", "=", "tf", ".", "contrib", ".", "lookup", ".", "index_table_from_tensor", "(", "\n", "tf", ".", "constant", "(", "params", ".", "vocabulary", "[", "\"target\"", "]", ")", ",", "\n", "default_value", "=", "params", ".", "mapping", "[", "\"target\"", "]", "[", "params", ".", "unk", "]", "\n", ")", "\n", "features", "[", "\"target\"", "]", "=", "src_table", ".", "lookup", "(", "features_o", "[", "\"target\"", "]", ")", "\n", "features", "[", "\"target_length\"", "]", "=", "features_o", "[", "\"target_length\"", "]", "\n", "\n", "return", "features", "\n", "", ""]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.data.record.input_pipeline": [[17, 58], ["tensorflow.VarLenFeature", "tensorflow.VarLenFeature", "tensorflow.FixedLenFeature", "tensorflow.FixedLenFeature", "tensorflow.contrib.slim.tfexample_decoder.Tensor", "tensorflow.contrib.slim.tfexample_decoder.Tensor", "tensorflow.contrib.slim.tfexample_decoder.Tensor", "tensorflow.contrib.slim.tfexample_decoder.Tensor", "tensorflow.name_scope", "tensorflow.contrib.slim.parallel_reader.get_data_files", "min", "tensorflow.contrib.slim.parallel_reader.parallel_read", "tensorflow.contrib.slim.tfexample_decoder.TFExampleDecoder", "tfexample_decoder.TFExampleDecoder.decode", "zip", "len", "tensorflow.to_int32", "list", "six.iteritems"], "function", ["None"], ["def", "input_pipeline", "(", "file_pattern", ",", "mode", ",", "capacity", "=", "64", ")", ":", "\n", "    ", "keys_to_features", "=", "{", "\n", "\"source\"", ":", "tf", ".", "VarLenFeature", "(", "tf", ".", "int64", ")", ",", "\n", "\"target\"", ":", "tf", ".", "VarLenFeature", "(", "tf", ".", "int64", ")", ",", "\n", "\"source_length\"", ":", "tf", ".", "FixedLenFeature", "(", "[", "1", "]", ",", "tf", ".", "int64", ")", ",", "\n", "\"target_length\"", ":", "tf", ".", "FixedLenFeature", "(", "[", "1", "]", ",", "tf", ".", "int64", ")", "\n", "}", "\n", "\n", "items_to_handlers", "=", "{", "\n", "\"source\"", ":", "tfexample_decoder", ".", "Tensor", "(", "\"source\"", ")", ",", "\n", "\"target\"", ":", "tfexample_decoder", ".", "Tensor", "(", "\"target\"", ")", ",", "\n", "\"source_length\"", ":", "tfexample_decoder", ".", "Tensor", "(", "\"source_length\"", ")", ",", "\n", "\"target_length\"", ":", "tfexample_decoder", ".", "Tensor", "(", "\"target_length\"", ")", "\n", "}", "\n", "\n", "# Now the non-trivial case construction.", "\n", "with", "tf", ".", "name_scope", "(", "\"examples_queue\"", ")", ":", "\n", "        ", "training", "=", "(", "mode", "==", "\"train\"", ")", "\n", "# Read serialized examples using slim parallel_reader.", "\n", "num_epochs", "=", "None", "if", "training", "else", "1", "\n", "data_files", "=", "parallel_reader", ".", "get_data_files", "(", "file_pattern", ")", "\n", "num_readers", "=", "min", "(", "4", "if", "training", "else", "1", ",", "len", "(", "data_files", ")", ")", "\n", "_", ",", "examples", "=", "parallel_reader", ".", "parallel_read", "(", "[", "file_pattern", "]", ",", "\n", "tf", ".", "TFRecordReader", ",", "\n", "num_epochs", "=", "num_epochs", ",", "\n", "shuffle", "=", "training", ",", "\n", "capacity", "=", "2", "*", "capacity", ",", "\n", "min_after_dequeue", "=", "capacity", ",", "\n", "num_readers", "=", "num_readers", ")", "\n", "\n", "decoder", "=", "tfexample_decoder", ".", "TFExampleDecoder", "(", "keys_to_features", ",", "\n", "items_to_handlers", ")", "\n", "\n", "decoded", "=", "decoder", ".", "decode", "(", "examples", ",", "items", "=", "list", "(", "items_to_handlers", ")", ")", "\n", "examples", "=", "{", "}", "\n", "\n", "for", "(", "field", ",", "tensor", ")", "in", "zip", "(", "keys_to_features", ",", "decoded", ")", ":", "\n", "            ", "examples", "[", "field", "]", "=", "tensor", "\n", "\n", "# We do not want int64s as they do are not supported on GPUs.", "\n", "", "return", "{", "k", ":", "tf", ".", "to_int32", "(", "v", ")", "for", "(", "k", ",", "v", ")", "in", "six", ".", "iteritems", "(", "examples", ")", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.data.record.batch_examples": [[60, 107], ["tensorflow.name_scope", "examples.values", "tensorflow.contrib.training.bucket_by_sequence_length", "boundaries.append", "tensorflow.maximum", "max", "max", "tensorflow.shape", "int", "math.log"], "function", ["None"], ["", "", "def", "batch_examples", "(", "examples", ",", "batch_size", ",", "max_length", ",", "mantissa_bits", ",", "\n", "shard_multiplier", "=", "1", ",", "length_multiplier", "=", "1", ",", "scheme", "=", "\"token\"", ",", "\n", "drop_long_sequences", "=", "True", ")", ":", "\n", "    ", "with", "tf", ".", "name_scope", "(", "\"batch_examples\"", ")", ":", "\n", "        ", "max_length", "=", "max_length", "or", "batch_size", "\n", "min_length", "=", "8", "\n", "mantissa_bits", "=", "mantissa_bits", "\n", "\n", "# compute boundaries", "\n", "x", "=", "min_length", "\n", "boundaries", "=", "[", "]", "\n", "\n", "while", "x", "<", "max_length", ":", "\n", "            ", "boundaries", ".", "append", "(", "x", ")", "\n", "x", "+=", "2", "**", "max", "(", "0", ",", "int", "(", "math", ".", "log", "(", "x", ",", "2", ")", ")", "-", "mantissa_bits", ")", "\n", "\n", "", "if", "scheme", "is", "\"token\"", ":", "\n", "            ", "batch_sizes", "=", "[", "max", "(", "1", ",", "batch_size", "//", "length", ")", "\n", "for", "length", "in", "boundaries", "+", "[", "max_length", "]", "]", "\n", "batch_sizes", "=", "[", "b", "*", "shard_multiplier", "for", "b", "in", "batch_sizes", "]", "\n", "bucket_capacities", "=", "[", "2", "*", "b", "for", "b", "in", "batch_sizes", "]", "\n", "", "else", ":", "\n", "            ", "batch_sizes", "=", "batch_size", "*", "shard_multiplier", "\n", "bucket_capacities", "=", "[", "2", "*", "n", "for", "n", "in", "boundaries", "+", "[", "max_length", "]", "]", "\n", "\n", "", "max_length", "*=", "length_multiplier", "\n", "boundaries", "=", "[", "boundary", "*", "length_multiplier", "for", "boundary", "in", "boundaries", "]", "\n", "max_length", "=", "max_length", "if", "drop_long_sequences", "else", "10", "**", "9", "\n", "\n", "# The queue to bucket on will be chosen based on maximum length.", "\n", "max_example_length", "=", "0", "\n", "for", "v", "in", "examples", ".", "values", "(", ")", ":", "\n", "            ", "seq_length", "=", "tf", ".", "shape", "(", "v", ")", "[", "0", "]", "\n", "max_example_length", "=", "tf", ".", "maximum", "(", "max_example_length", ",", "seq_length", ")", "\n", "\n", "", "(", "_", ",", "outputs", ")", "=", "tf", ".", "contrib", ".", "training", ".", "bucket_by_sequence_length", "(", "\n", "max_example_length", ",", "\n", "examples", ",", "\n", "batch_sizes", ",", "\n", "[", "b", "+", "1", "for", "b", "in", "boundaries", "]", ",", "\n", "capacity", "=", "2", ",", "\n", "bucket_capacities", "=", "bucket_capacities", ",", "\n", "dynamic_pad", "=", "True", ",", "\n", "keep_input", "=", "(", "max_example_length", "<=", "max_length", ")", "\n", ")", "\n", "\n", "", "return", "outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.data.record.get_input_features": [[109, 143], ["tensorflow.name_scope", "tensorflow.device", "record.input_pipeline", "record.batch_examples", "tensorflow.squeeze", "tensorflow.squeeze", "len"], "function", ["home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.data.record.input_pipeline", "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.data.record.batch_examples"], ["", "def", "get_input_features", "(", "file_patterns", ",", "mode", ",", "params", ")", ":", "\n", "    ", "with", "tf", ".", "name_scope", "(", "\"input_queues\"", ")", ":", "\n", "        ", "with", "tf", ".", "device", "(", "\"/cpu:0\"", ")", ":", "\n", "            ", "if", "mode", "!=", "\"train\"", ":", "\n", "                ", "num_datashards", "=", "1", "\n", "batch_size", "=", "params", ".", "eval_batch_size", "\n", "", "else", ":", "\n", "                ", "num_datashards", "=", "len", "(", "params", ".", "device_list", ")", "\n", "batch_size", "=", "params", ".", "batch_size", "\n", "\n", "", "batch_size_multiplier", "=", "1", "\n", "capacity", "=", "64", "*", "num_datashards", "\n", "examples", "=", "input_pipeline", "(", "file_patterns", ",", "mode", ",", "capacity", ")", "\n", "drop_long_sequences", "=", "(", "mode", "==", "\"train\"", ")", "\n", "\n", "feature_map", "=", "batch_examples", "(", "\n", "examples", ",", "\n", "batch_size", ",", "\n", "params", ".", "max_length", ",", "\n", "params", ".", "mantissa_bits", ",", "\n", "num_datashards", ",", "\n", "batch_size_multiplier", ",", "\n", "\"token\"", "if", "not", "params", ".", "constant_batch_size", "else", "\"constant\"", ",", "\n", "drop_long_sequences", "\n", ")", "\n", "\n", "", "features", "=", "{", "\n", "\"source\"", ":", "feature_map", "[", "\"source\"", "]", ",", "\n", "\"target\"", ":", "feature_map", "[", "\"target\"", "]", ",", "\n", "\"source_length\"", ":", "tf", ".", "squeeze", "(", "feature_map", "[", "\"source_length\"", "]", ",", "axis", "=", "1", ")", ",", "\n", "\"target_length\"", ":", "tf", ".", "squeeze", "(", "feature_map", "[", "\"target_length\"", "]", ",", "axis", "=", "1", ")", "\n", "}", "\n", "\n", "", "return", "features", "\n", "", ""]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.data.vocab.load_vocabulary": [[11, 19], ["tensorflow.gfile.GFile", "line.strip", "vocab.append"], "function", ["None"], ["def", "load_vocabulary", "(", "filename", ")", ":", "\n", "    ", "vocab", "=", "[", "]", "\n", "with", "tf", ".", "gfile", ".", "GFile", "(", "filename", ")", "as", "fd", ":", "\n", "        ", "for", "line", "in", "fd", ":", "\n", "            ", "word", "=", "line", ".", "strip", "(", ")", "\n", "vocab", ".", "append", "(", "word", ")", "\n", "\n", "", "", "return", "vocab", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.data.vocab.process_vocabulary": [[21, 26], ["vocab.append"], "function", ["None"], ["", "def", "process_vocabulary", "(", "vocab", ",", "params", ")", ":", "\n", "    ", "if", "params", ".", "append_eos", ":", "\n", "        ", "vocab", ".", "append", "(", "params", ".", "eos", ")", "\n", "\n", "", "return", "vocab", "\n", "\n"]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.data.vocab.get_control_mapping": [[28, 37], ["enumerate"], "function", ["None"], ["", "def", "get_control_mapping", "(", "vocab", ",", "symbols", ")", ":", "\n", "    ", "mapping", "=", "{", "}", "\n", "\n", "for", "i", ",", "token", "in", "enumerate", "(", "vocab", ")", ":", "\n", "        ", "for", "symbol", "in", "symbols", ":", "\n", "            ", "if", "symbol", "==", "token", ":", "\n", "                ", "mapping", "[", "symbol", "]", "=", "i", "\n", "\n", "", "", "", "return", "mapping", "\n", "", ""]], "home.repos.pwc.inspect_result.adaxry_ss_on_decoding_steps.losses.losses.smoothed_softmax_cross_entropy_with_logits": [[11, 56], ["kwargs.get", "kwargs.get", "kwargs.get", "kwargs.get", "kwargs.get", "ValueError", "tensorflow.name_scope", "tensorflow.reshape", "tensorflow.to_float", "tensorflow.one_hot", "tensorflow.stop_gradient", "tensorflow.nn.softmax_cross_entropy_with_logits_v2", "tensorflow.nn.sparse_softmax_cross_entropy_with_logits", "tensorflow.shape", "tensorflow.cast", "tensorflow.cast", "tensorflow.cast", "tensorflow.log", "tensorflow.log"], "function", ["None"], ["def", "smoothed_softmax_cross_entropy_with_logits", "(", "**", "kwargs", ")", ":", "\n", "    ", "logits", "=", "kwargs", ".", "get", "(", "\"logits\"", ")", "\n", "labels", "=", "kwargs", ".", "get", "(", "\"labels\"", ")", "\n", "smoothing", "=", "kwargs", ".", "get", "(", "\"smoothing\"", ")", "or", "0.0", "\n", "normalize", "=", "kwargs", ".", "get", "(", "\"normalize\"", ")", "\n", "scope", "=", "kwargs", ".", "get", "(", "\"scope\"", ")", "\n", "\n", "if", "logits", "is", "None", "or", "labels", "is", "None", ":", "\n", "        ", "raise", "ValueError", "(", "\"Both logits and labels must be provided\"", ")", "\n", "\n", "", "with", "tf", ".", "name_scope", "(", "scope", "or", "\"smoothed_softmax_cross_entropy_with_logits\"", ",", "\n", "values", "=", "[", "logits", ",", "labels", "]", ")", ":", "\n", "\n", "        ", "labels", "=", "tf", ".", "reshape", "(", "labels", ",", "[", "-", "1", "]", ")", "\n", "\n", "if", "not", "smoothing", ":", "\n", "            ", "ce", "=", "tf", ".", "nn", ".", "sparse_softmax_cross_entropy_with_logits", "(", "\n", "logits", "=", "tf", ".", "cast", "(", "logits", ",", "tf", ".", "float32", ")", ",", "\n", "labels", "=", "labels", "\n", ")", "\n", "return", "ce", "\n", "\n", "# label smoothing", "\n", "", "vocab_size", "=", "tf", ".", "shape", "(", "logits", ")", "[", "1", "]", "\n", "\n", "n", "=", "tf", ".", "to_float", "(", "vocab_size", "-", "1", ")", "\n", "p", "=", "1.0", "-", "smoothing", "\n", "q", "=", "smoothing", "/", "n", "\n", "\n", "soft_targets", "=", "tf", ".", "one_hot", "(", "tf", ".", "cast", "(", "labels", ",", "tf", ".", "int32", ")", ",", "depth", "=", "vocab_size", ",", "\n", "on_value", "=", "p", ",", "off_value", "=", "q", ")", "\n", "soft_targets", "=", "tf", ".", "stop_gradient", "(", "soft_targets", ")", "\n", "xentropy", "=", "tf", ".", "nn", ".", "softmax_cross_entropy_with_logits_v2", "(", "\n", "logits", "=", "tf", ".", "cast", "(", "logits", ",", "tf", ".", "float32", ")", ",", "\n", "labels", "=", "soft_targets", ")", "\n", "\n", "if", "normalize", "is", "False", ":", "\n", "            ", "return", "xentropy", "\n", "\n", "# Normalizing constant is the best cross-entropy value with soft", "\n", "# targets. We subtract it just for readability, makes no difference on", "\n", "# learning", "\n", "", "normalizing", "=", "-", "(", "p", "*", "tf", ".", "log", "(", "p", ")", "+", "n", "*", "q", "*", "tf", ".", "log", "(", "q", "+", "1e-20", ")", ")", "\n", "\n", "return", "xentropy", "-", "normalizing", "\n", "", "", ""]]}