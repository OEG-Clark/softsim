{"home.repos.pwc.inspect_result.nkolkin13_neuralneighborstyletransfer.None.predict.Predictor.setup": [[16, 20], ["utils.misc.to_device", "pretrained.vgg.Vgg16Pretrained", "utils.misc.to_device.forward"], "methods", ["home.repos.pwc.inspect_result.nkolkin13_neuralneighborstyletransfer.utils.misc.to_device", "home.repos.pwc.inspect_result.nkolkin13_neuralneighborstyletransfer.pretrained.vgg.Vgg16Pretrained.forward"], ["    ", "def", "setup", "(", "self", ")", ":", "\n", "# Define feature extractor", "\n", "        ", "cnn", "=", "misc", ".", "to_device", "(", "Vgg16Pretrained", "(", ")", ")", "\n", "self", ".", "phi", "=", "lambda", "x", ",", "y", ",", "z", ":", "cnn", ".", "forward", "(", "x", ",", "inds", "=", "y", ",", "concat", "=", "z", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nkolkin13_neuralneighborstyletransfer.None.predict.Predictor.predict": [[21, 104], ["cog.Input", "cog.Input", "cog.Input", "cog.Input", "cog.Input", "cog.Input", "utils.misc.to_device().unsqueeze", "utils.misc.to_device().unsqueeze", "torch.cuda.synchronize", "time.time", "utils.stylize.produce_stylization", "torch.cuda.synchronize", "print", "numpy.clip", "imageio.imwrite", "imageio.imwrite", "torch.cuda.is_available", "output[].permute().detach().cpu().numpy", "cog.Path", "str", "torch.cuda.empty_cache", "utils.misc.to_device", "utils.misc.to_device", "tempfile.mkdtemp", "utils.misc.load_path_for_pytorch", "utils.misc.load_path_for_pytorch", "time.time", "output[].permute().detach().cpu", "str", "str", "output[].permute().detach", "output[].permute"], "methods", ["home.repos.pwc.inspect_result.nkolkin13_neuralneighborstyletransfer.utils.stylize.produce_stylization", "home.repos.pwc.inspect_result.nkolkin13_neuralneighborstyletransfer.utils.misc.to_device", "home.repos.pwc.inspect_result.nkolkin13_neuralneighborstyletransfer.utils.misc.to_device", "home.repos.pwc.inspect_result.nkolkin13_neuralneighborstyletransfer.utils.misc.load_path_for_pytorch", "home.repos.pwc.inspect_result.nkolkin13_neuralneighborstyletransfer.utils.misc.load_path_for_pytorch"], ["", "def", "predict", "(", "\n", "self", ",", "\n", "content", ":", "Path", "=", "Input", "(", "description", "=", "\"Content image.\"", ")", ",", "\n", "style", ":", "Path", "=", "Input", "(", "description", "=", "\"Style image.\"", ")", ",", "\n", "colorize", ":", "bool", "=", "Input", "(", "\n", "default", "=", "True", ",", "description", "=", "\"Whether use color correction in the output.\"", "\n", ")", ",", "\n", "high_res", ":", "bool", "=", "Input", "(", "\n", "default", "=", "False", ",", "\n", "description", "=", "\"Whether output high resolution image (1024 instead if 512).\"", ",", "\n", ")", ",", "\n", "alpha", ":", "float", "=", "Input", "(", "\n", "default", "=", "0.75", ",", "\n", "ge", "=", "0.0", ",", "\n", "le", "=", "1.0", ",", "\n", "description", "=", "\"alpha=1.0 corresponds to maximum content preservation, alpha=0.0 is maximum stylization.\"", ",", "\n", ")", ",", "\n", "content_loss", ":", "bool", "=", "Input", "(", "\n", "default", "=", "False", ",", "description", "=", "\"Whether use experimental content loss.\"", "\n", ")", ",", "\n", ")", "->", "Path", ":", "\n", "\n", "        ", "max_scls", "=", "4", "\n", "sz", "=", "512", "\n", "if", "high_res", ":", "\n", "            ", "max_scls", "=", "5", "\n", "sz", "=", "1024", "\n", "\n", "", "flip_aug", "=", "True", "\n", "misc", ".", "USE_GPU", "=", "True", "\n", "content_weight", "=", "1.0", "-", "alpha", "\n", "\n", "# Error checking for arguments", "\n", "# error checking for paths deferred to imageio", "\n", "assert", "(", "0.0", "<=", "content_weight", ")", "and", "(", "\n", "content_weight", "<=", "1.0", "\n", ")", ",", "\"alpha must be between 0 and 1\"", "\n", "assert", "torch", ".", "cuda", ".", "is_available", "(", ")", "or", "(", "\n", "not", "misc", ".", "USE_GPU", "\n", ")", ",", "\"attempted to use gpu when unavailable\"", "\n", "\n", "# Load images", "\n", "content_im_orig", "=", "misc", ".", "to_device", "(", "\n", "load_path_for_pytorch", "(", "str", "(", "content", ")", ",", "target_size", "=", "sz", ")", "\n", ")", ".", "unsqueeze", "(", "0", ")", "\n", "style_im_orig", "=", "misc", ".", "to_device", "(", "\n", "load_path_for_pytorch", "(", "str", "(", "style", ")", ",", "target_size", "=", "sz", ")", "\n", ")", ".", "unsqueeze", "(", "0", ")", "\n", "\n", "# Run Style Transfer", "\n", "torch", ".", "cuda", ".", "synchronize", "(", ")", "\n", "start_time", "=", "time", ".", "time", "(", ")", "\n", "output", "=", "produce_stylization", "(", "\n", "content_im_orig", ",", "\n", "style_im_orig", ",", "\n", "self", ".", "phi", ",", "\n", "max_iter", "=", "200", ",", "\n", "lr", "=", "2e-3", ",", "\n", "content_weight", "=", "content_weight", ",", "\n", "max_scls", "=", "max_scls", ",", "\n", "flip_aug", "=", "flip_aug", ",", "\n", "content_loss", "=", "content_loss", ",", "\n", "dont_colorize", "=", "not", "colorize", ",", "\n", ")", "\n", "torch", ".", "cuda", ".", "synchronize", "(", ")", "\n", "print", "(", "\"Done! total time: {}\"", ".", "format", "(", "time", ".", "time", "(", ")", "-", "start_time", ")", ")", "\n", "\n", "# Convert from pyTorch to numpy, clip to valid range", "\n", "new_im_out", "=", "np", ".", "clip", "(", "\n", "output", "[", "0", "]", ".", "permute", "(", "1", ",", "2", ",", "0", ")", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ",", "0.0", ",", "1.0", "\n", ")", "\n", "\n", "# Save stylized output", "\n", "save_im", "=", "(", "new_im_out", "*", "255", ")", ".", "astype", "(", "np", ".", "uint8", ")", "\n", "out_path", "=", "Path", "(", "tempfile", ".", "mkdtemp", "(", ")", ")", "/", "\"output.png\"", "\n", "imwrite", "(", "\"ooo.png\"", ",", "save_im", ")", "\n", "imwrite", "(", "str", "(", "out_path", ")", ",", "save_im", ")", "\n", "\n", "# Free gpu memory in case something else needs it later", "\n", "if", "misc", ".", "USE_GPU", ":", "\n", "            ", "torch", ".", "cuda", ".", "empty_cache", "(", ")", "\n", "\n", "", "return", "out_path", "\n", "", "", ""]], "home.repos.pwc.inspect_result.nkolkin13_neuralneighborstyletransfer.utils.zca.whiten": [[3, 14], ["tps", "x.transpose", "torch.matmul", "torch.matmul", "tps"], "function", ["None"], ["def", "whiten", "(", "x", ",", "ui", ",", "u", ",", "s", ")", ":", "\n", "    ", "'''\n    Applies whitening as described in:\n    https://openaccess.thecvf.com/content_ICCV_2019/papers/Chiu_Understanding_Generalized_Whitening_and_Coloring_Transform_for_Universal_Style_Transfer_ICCV_2019_paper.pdf\n    x -- N x D pytorch tensor\n    ui -- D x D transposed eigenvectors of whitening covariance\n    u  -- D x D eigenvectors of whitening covariance\n    s  -- D x 1 eigenvalues of whitening covariance\n    '''", "\n", "tps", "=", "lambda", "x", ":", "x", ".", "transpose", "(", "1", ",", "0", ")", "\n", "return", "tps", "(", "torch", ".", "matmul", "(", "u", ",", "torch", ".", "matmul", "(", "ui", ",", "tps", "(", "x", ")", ")", "/", "s", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nkolkin13_neuralneighborstyletransfer.utils.zca.colorize": [[15, 26], ["tps", "x.transpose", "torch.matmul", "torch.matmul", "tps"], "function", ["None"], ["", "def", "colorize", "(", "x", ",", "ui", ",", "u", ",", "s", ")", ":", "\n", "    ", "'''\n    Applies \"coloring transform\" as described in:\n    https://openaccess.thecvf.com/content_ICCV_2019/papers/Chiu_Understanding_Generalized_Whitening_and_Coloring_Transform_for_Universal_Style_Transfer_ICCV_2019_paper.pdf\n    x -- N x D pytorch tensor\n    ui -- D x D transposed eigenvectors of coloring covariance\n    u  -- D x D eigenvectors of coloring covariance\n    s  -- D x 1 eigenvalues of coloring covariance\n    '''", "\n", "tps", "=", "lambda", "x", ":", "x", ".", "transpose", "(", "1", ",", "0", ")", "\n", "return", "tps", "(", "torch", ".", "matmul", "(", "u", ",", "torch", ".", "matmul", "(", "ui", ",", "tps", "(", "x", ")", ")", "*", "s", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nkolkin13_neuralneighborstyletransfer.utils.zca.zca": [[27, 60], ["content.mean", "style.mean", "torch.svd", "torch.svd", "sig_c.unsqueeze.unsqueeze", "sig_s.unsqueeze.unsqueeze", "u_c.transpose", "u_s.transpose", "torch.sqrt", "torch.sqrt", "zca.whiten", "torch.matmul", "float", "torch.matmul", "float", "torch.clamp", "torch.clamp", "zca.colorize", "content.transpose", "content.size", "style.transpose", "style.size", "torch.eye().cuda", "torch.eye().cuda", "torch.eye", "torch.eye", "cov_c.size", "cov_s.size"], "function", ["home.repos.pwc.inspect_result.nkolkin13_neuralneighborstyletransfer.utils.zca.whiten", "home.repos.pwc.inspect_result.nkolkin13_neuralneighborstyletransfer.utils.zca.colorize"], ["", "def", "zca", "(", "content", ",", "style", ")", ":", "\n", "    ", "'''\n    Matches the mean and covariance of 'content' to those of 'style'\n    content -- N x D pytorch tensor of content feature vectors\n    style   -- N x D pytorch tensor of style feature vectors\n    '''", "\n", "mu_c", "=", "content", ".", "mean", "(", "0", ",", "keepdim", "=", "True", ")", "\n", "mu_s", "=", "style", ".", "mean", "(", "0", ",", "keepdim", "=", "True", ")", "\n", "\n", "content", "=", "content", "-", "mu_c", "\n", "style", "=", "style", "-", "mu_s", "\n", "\n", "cov_c", "=", "torch", ".", "matmul", "(", "content", ".", "transpose", "(", "1", ",", "0", ")", ",", "content", ")", "/", "float", "(", "content", ".", "size", "(", "0", ")", ")", "\n", "cov_s", "=", "torch", ".", "matmul", "(", "style", ".", "transpose", "(", "1", ",", "0", ")", ",", "style", ")", "/", "float", "(", "style", ".", "size", "(", "0", ")", ")", "\n", "\n", "u_c", ",", "sig_c", ",", "_", "=", "torch", ".", "svd", "(", "cov_c", "+", "torch", ".", "eye", "(", "cov_c", ".", "size", "(", "0", ")", ")", ".", "cuda", "(", ")", "*", "1e-4", ")", "\n", "u_s", ",", "sig_s", ",", "_", "=", "torch", ".", "svd", "(", "cov_s", "+", "torch", ".", "eye", "(", "cov_s", ".", "size", "(", "0", ")", ")", ".", "cuda", "(", ")", "*", "1e-4", ")", "\n", "\n", "sig_c", "=", "sig_c", ".", "unsqueeze", "(", "1", ")", "\n", "sig_s", "=", "sig_s", ".", "unsqueeze", "(", "1", ")", "\n", "\n", "\n", "u_c_i", "=", "u_c", ".", "transpose", "(", "1", ",", "0", ")", "\n", "u_s_i", "=", "u_s", ".", "transpose", "(", "1", ",", "0", ")", "\n", "\n", "scl_c", "=", "torch", ".", "sqrt", "(", "torch", ".", "clamp", "(", "sig_c", ",", "1e-8", ",", "1e8", ")", ")", "\n", "scl_s", "=", "torch", ".", "sqrt", "(", "torch", ".", "clamp", "(", "sig_s", ",", "1e-8", ",", "1e8", ")", ")", "\n", "\n", "\n", "whiten_c", "=", "whiten", "(", "content", ",", "u_c_i", ",", "u_c", ",", "scl_c", ")", "\n", "color_c", "=", "colorize", "(", "whiten_c", ",", "u_s_i", ",", "u_s", ",", "scl_s", ")", "+", "mu_s", "\n", "\n", "return", "color_c", ",", "cov_s", "\n", "\n"]], "home.repos.pwc.inspect_result.nkolkin13_neuralneighborstyletransfer.utils.zca.zca_tensor": [[61, 74], ["content.permute().contiguous().view", "style.permute().contiguous().view", "zca.zca", "cs.view().permute.view().permute", "content.size", "style.size", "cs.view().permute.contiguous", "content.permute().contiguous", "style.permute().contiguous", "cs.view().permute.view", "content.size", "content.size", "content.size", "content.size", "content.permute", "style.permute"], "function", ["home.repos.pwc.inspect_result.nkolkin13_neuralneighborstyletransfer.utils.zca.zca"], ["", "def", "zca_tensor", "(", "content", ",", "style", ")", ":", "\n", "    ", "'''\n    Matches the mean and covariance of 'content' to those of 'style'\n    content -- B x D x H x W pytorch tensor of content feature vectors\n    style   -- B x D x H x W pytorch tensor of style feature vectors\n    '''", "\n", "content_rs", "=", "content", ".", "permute", "(", "0", ",", "2", ",", "3", ",", "1", ")", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ",", "content", ".", "size", "(", "1", ")", ")", "\n", "style_rs", "=", "style", ".", "permute", "(", "0", ",", "2", ",", "3", ",", "1", ")", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ",", "style", ".", "size", "(", "1", ")", ")", "\n", "\n", "cs", ",", "cov_s", "=", "zca", "(", "content_rs", ",", "style_rs", ")", "\n", "\n", "cs", "=", "cs", ".", "view", "(", "content", ".", "size", "(", "0", ")", ",", "content", ".", "size", "(", "2", ")", ",", "content", ".", "size", "(", "3", ")", ",", "content", ".", "size", "(", "1", ")", ")", ".", "permute", "(", "0", ",", "3", ",", "1", ",", "2", ")", "\n", "return", "cs", ".", "contiguous", "(", ")", ",", "cov_s", "\n", "", ""]], "home.repos.pwc.inspect_result.nkolkin13_neuralneighborstyletransfer.utils.stylize.produce_stylization": [[18, 133], ["max", "utils.imagePyramid.dec_lap_pyr", "utils.imagePyramid.dec_lap_pyr", "utils.imagePyramid.dec_lap_pyr", "stylize.optimize_output_im", "content_im.size", "content_im.size", "content_im.clone", "range", "range", "range", "utils.imagePyramid.syn_lap_pyr", "utils.imagePyramid.syn_lap_pyr", "utils.imagePyramid.syn_lap_pyr", "print", "stylize.optimize_output_im", "torch.no_grad", "torch.no_grad", "utils.imagePyramid.syn_lap_pyr", "utils.colorization.color_match", "len", "torch.cuda.empty_cache", "torch.cuda.empty_cache", "torch.no_grad", "torch.no_grad", "utils.imagePyramid.syn_lap_pyr", "utils.featureExtract.extract_feats().cpu", "utils.featureExtract.extract_feats().cpu", "stylize.replace_features", "torch.no_grad", "torch.no_grad", "utils.imagePyramid.syn_lap_pyr", "utils.featureExtract.extract_feats", "utils.featureExtract.extract_feats", "max", "utils.imagePyramid.syn_lap_pyr.size", "utils.imagePyramid.syn_lap_pyr.size"], "function", ["home.repos.pwc.inspect_result.nkolkin13_neuralneighborstyletransfer.utils.imagePyramid.dec_lap_pyr", "home.repos.pwc.inspect_result.nkolkin13_neuralneighborstyletransfer.utils.imagePyramid.dec_lap_pyr", "home.repos.pwc.inspect_result.nkolkin13_neuralneighborstyletransfer.utils.imagePyramid.dec_lap_pyr", "home.repos.pwc.inspect_result.nkolkin13_neuralneighborstyletransfer.utils.stylize.optimize_output_im", "home.repos.pwc.inspect_result.nkolkin13_neuralneighborstyletransfer.utils.imagePyramid.syn_lap_pyr", "home.repos.pwc.inspect_result.nkolkin13_neuralneighborstyletransfer.utils.imagePyramid.syn_lap_pyr", "home.repos.pwc.inspect_result.nkolkin13_neuralneighborstyletransfer.utils.imagePyramid.syn_lap_pyr", "home.repos.pwc.inspect_result.nkolkin13_neuralneighborstyletransfer.utils.stylize.optimize_output_im", "home.repos.pwc.inspect_result.nkolkin13_neuralneighborstyletransfer.utils.imagePyramid.syn_lap_pyr", "home.repos.pwc.inspect_result.nkolkin13_neuralneighborstyletransfer.utils.colorization.color_match", "home.repos.pwc.inspect_result.nkolkin13_neuralneighborstyletransfer.utils.imagePyramid.syn_lap_pyr", "home.repos.pwc.inspect_result.nkolkin13_neuralneighborstyletransfer.utils.stylize.replace_features", "home.repos.pwc.inspect_result.nkolkin13_neuralneighborstyletransfer.utils.imagePyramid.syn_lap_pyr", "home.repos.pwc.inspect_result.nkolkin13_neuralneighborstyletransfer.utils.featureExtract.extract_feats", "home.repos.pwc.inspect_result.nkolkin13_neuralneighborstyletransfer.utils.featureExtract.extract_feats"], ["def", "produce_stylization", "(", "content_im", ",", "style_im", ",", "phi", ",", "\n", "max_iter", "=", "350", ",", "\n", "lr", "=", "1e-3", ",", "\n", "content_weight", "=", "1.", ",", "\n", "max_scls", "=", "0", ",", "\n", "flip_aug", "=", "False", ",", "\n", "content_loss", "=", "False", ",", "\n", "zero_init", "=", "False", ",", "\n", "dont_colorize", "=", "False", ")", ":", "\n", "    ", "\"\"\" Produce stylization of 'content_im' in the style of 'style_im'\n        Inputs:\n            content_im -- 1x3xHxW pytorch tensor containing rbg content image\n            style_im -- 1x3xH'xW' pytorch tensor containing rgb style image\n            phi -- lambda function to extract features using VGG16Pretrained\n            max_iter -- number of updates to image pyramid per scale\n            lr -- learning rate of optimizer updating pyramid coefficients\n            content_weight -- controls stylization level, between 0 and 1\n            max_scl -- number of scales to stylize (performed coarse to fine)\n            flip_aug -- extract features from rotations of style image too?\n            content_loss -- use self-sim content loss? (compares downsampled\n                            version of output and content image)\n            zero_init -- if true initialize w/ grey image, o.w. initialize w/\n                         downsampled content image\n    \"\"\"", "\n", "# Get max side length of final output and set number of pyramid levels to ", "\n", "# optimize over", "\n", "max_size", "=", "max", "(", "content_im", ".", "size", "(", "2", ")", ",", "content_im", ".", "size", "(", "3", ")", ")", "\n", "pyr_levs", "=", "8", "\n", "\n", "# Decompose style image, content image, and output image into laplacian ", "\n", "# pyramid", "\n", "style_pyr", "=", "dec_pyr", "(", "style_im", ",", "pyr_levs", ")", "\n", "c_pyr", "=", "dec_pyr", "(", "content_im", ",", "pyr_levs", ")", "\n", "s_pyr", "=", "dec_pyr", "(", "content_im", ".", "clone", "(", ")", ",", "pyr_levs", ")", "\n", "\n", "# Initialize output image pyramid", "\n", "if", "zero_init", ":", "\n", "# Initialize with flat grey image (works, but less vivid)", "\n", "        ", "for", "i", "in", "range", "(", "len", "(", "s_pyr", ")", ")", ":", "\n", "            ", "s_pyr", "[", "i", "]", "=", "s_pyr", "[", "i", "]", "*", "0.", "\n", "", "s_pyr", "[", "-", "1", "]", "=", "s_pyr", "[", "-", "1", "]", "*", "0.", "+", "0.5", "\n", "\n", "", "else", ":", "\n", "# Initialize with low-res version of content image (generally better ", "\n", "# results, improves contrast of final output)", "\n", "        ", "z_max", "=", "2", "\n", "if", "max_size", "<", "1024", ":", "\n", "            ", "z_max", "=", "3", "\n", "\n", "", "for", "i", "in", "range", "(", "z_max", ")", ":", "\n", "            ", "s_pyr", "[", "i", "]", "=", "s_pyr", "[", "i", "]", "*", "0.", "\n", "\n", "# Stylize using hypercolumn matching from coarse to fine scale", "\n", "", "", "li", "=", "-", "1", "\n", "for", "scl", "in", "range", "(", "max_scls", ")", "[", ":", ":", "-", "1", "]", ":", "\n", "\n", "# Get content image and style image from pyramid at current resolution", "\n", "        ", "if", "misc", ".", "USE_GPU", ":", "\n", "            ", "torch", ".", "cuda", ".", "empty_cache", "(", ")", "\n", "", "style_im_tmp", "=", "syn_pyr", "(", "style_pyr", "[", "scl", ":", "]", ")", "\n", "content_im_tmp", "=", "syn_pyr", "(", "c_pyr", "[", "scl", ":", "]", ")", "\n", "output_im_tmp", "=", "syn_pyr", "(", "s_pyr", "[", "scl", ":", "]", ")", "\n", "li", "+=", "1", "\n", "print", "(", "f'-{li, max(output_im_tmp.size(2),output_im_tmp.size(3))}-'", ")", "\n", "\n", "\n", "# Construct stylized activations", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "\n", "# Control tradeoff between searching for features that match", "\n", "# current iterate, and features that match content image (at", "\n", "# coarsest scale, only use content image)    ", "\n", "            ", "alpha", "=", "content_weight", "\n", "if", "li", "==", "0", ":", "\n", "                ", "alpha", "=", "0.", "\n", "\n", "# Search for features using high frequencies from content ", "\n", "# (but do not initialize actual output with them)", "\n", "", "output_extract", "=", "syn_pyr", "(", "[", "c_pyr", "[", "scl", "]", "]", "+", "s_pyr", "[", "(", "scl", "+", "1", ")", ":", "]", ")", "\n", "\n", "# Extract style features from rotated copies of style image", "\n", "feats_s", "=", "extract_feats", "(", "style_im_tmp", ",", "phi", ",", "flip_aug", "=", "flip_aug", ")", ".", "cpu", "(", ")", "\n", "\n", "# Extract features from convex combination of content image and", "\n", "# current iterate:", "\n", "c_tmp", "=", "(", "output_extract", "*", "alpha", ")", "+", "(", "content_im_tmp", "*", "(", "1.", "-", "alpha", ")", ")", "\n", "feats_c", "=", "extract_feats", "(", "c_tmp", ",", "phi", ")", ".", "cpu", "(", ")", "\n", "\n", "# Replace content features with style features", "\n", "target_feats", "=", "replace_features", "(", "feats_c", ",", "feats_s", ")", "\n", "\n", "# Synthesize output at current resolution using hypercolumn matching", "\n", "", "s_pyr", "=", "optimize_output_im", "(", "s_pyr", ",", "c_pyr", ",", "content_im", ",", "style_im_tmp", ",", "\n", "target_feats", ",", "lr", ",", "max_iter", ",", "scl", ",", "phi", ",", "\n", "content_loss", "=", "content_loss", ")", "\n", "\n", "# Get output at current resolution from pyramid", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "output_im", "=", "syn_pyr", "(", "s_pyr", ")", "\n", "\n", "# Perform final pass using feature splitting (pass in flip_aug argument", "\n", "# because style features are extracted internally in this regime)", "\n", "", "", "s_pyr", "=", "optimize_output_im", "(", "s_pyr", ",", "c_pyr", ",", "content_im", ",", "style_im_tmp", ",", "\n", "target_feats", ",", "lr", ",", "max_iter", ",", "scl", ",", "phi", ",", "\n", "final_pass", "=", "True", ",", "content_loss", "=", "content_loss", ",", "\n", "flip_aug", "=", "flip_aug", ")", "\n", "\n", "# Get final output from pyramid", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "        ", "output_im", "=", "syn_pyr", "(", "s_pyr", ")", "\n", "\n", "", "if", "dont_colorize", ":", "\n", "        ", "return", "output_im", "\n", "", "else", ":", "\n", "        ", "return", "color_match", "(", "content_im", ",", "style_im", ",", "output_im", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nkolkin13_neuralneighborstyletransfer.utils.stylize.replace_features": [[134, 184], ["utils.misc.to_device", "range", "torch.cat", "torch.cat", "utils.misc.flatten_grid", "src.size", "utils.misc.flatten_grid", "torch.cat", "torch.cat", "out.view.view", "torch.cat.append", "max", "utils.misc.flatten_grid.size", "min", "utils.misc.to_device", "utils.distance.pairwise_distances_cos_center", "torch.min", "torch.min", "nn_inds.unsqueeze().expand.unsqueeze().expand", "torch.gather().transpose().contiguous", "torch.gather().transpose().contiguous", "out.view.append", "ref.size", "src.size", "src.size", "utils.misc.flatten_grid.size", "nn_inds.unsqueeze().expand.size", "utils.misc.to_device.size", "nn_inds.unsqueeze().expand.unsqueeze", "torch.gather().transpose", "torch.gather().transpose", "ref.size", "ref.size", "torch.gather", "torch.gather"], "function", ["home.repos.pwc.inspect_result.nkolkin13_neuralneighborstyletransfer.utils.misc.to_device", "home.repos.pwc.inspect_result.nkolkin13_neuralneighborstyletransfer.utils.misc.flatten_grid", "home.repos.pwc.inspect_result.nkolkin13_neuralneighborstyletransfer.utils.misc.flatten_grid", "home.repos.pwc.inspect_result.nkolkin13_neuralneighborstyletransfer.utils.misc.to_device", "home.repos.pwc.inspect_result.nkolkin13_neuralneighborstyletransfer.utils.distance.pairwise_distances_cos_center"], ["", "", "def", "replace_features", "(", "src", ",", "ref", ")", ":", "\n", "    ", "\"\"\" Replace each feature vector in 'src' with the nearest (under centered \n    cosine distance) feature vector in 'ref'\n    Inputs:\n        src -- 1xCxAxB tensor of content features\n        ref -- 1xCxHxW tensor of style features\n    Outputs:\n        rplc -- 1xCxHxW tensor of features, where rplc[0,:,i,j] is the nearest\n                neighbor feature vector of src[0,:,i,j] in ref\n    \"\"\"", "\n", "# Move style features to gpu (necessary to mostly store on cpu for gpus w/", "\n", "# < 12GB of memory)", "\n", "ref_flat", "=", "to_device", "(", "flatten_grid", "(", "ref", ")", ")", "\n", "rplc", "=", "[", "]", "\n", "for", "j", "in", "range", "(", "src", ".", "size", "(", "0", ")", ")", ":", "\n", "# How many rows of the distance matrix to compute at once, can be", "\n", "# reduced if less memory is available, but this slows method down", "\n", "        ", "stride", "=", "128", "**", "2", "//", "max", "(", "1", ",", "(", "ref", ".", "size", "(", "2", ")", "*", "ref", ".", "size", "(", "3", ")", ")", "//", "(", "128", "**", "2", ")", ")", "\n", "bi", "=", "0", "\n", "ei", "=", "0", "\n", "\n", "# Loop until all content features are replaced by style feature / all", "\n", "# rows of distance matrix are computed", "\n", "out", "=", "[", "]", "\n", "src_flat_all", "=", "flatten_grid", "(", "src", "[", "j", ":", "j", "+", "1", ",", ":", ",", ":", ",", ":", "]", ")", "\n", "while", "bi", "<", "src_flat_all", ".", "size", "(", "0", ")", ":", "\n", "            ", "ei", "=", "min", "(", "bi", "+", "stride", ",", "src_flat_all", ".", "size", "(", "0", ")", ")", "\n", "\n", "# Get chunck of content features, compute corresponding portion", "\n", "# distance matrix, and store nearest style feature to each content", "\n", "# feature", "\n", "src_flat", "=", "to_device", "(", "src_flat_all", "[", "bi", ":", "ei", ",", ":", "]", ")", "\n", "d_mat", "=", "pairwise_distances_cos_center", "(", "ref_flat", ",", "src_flat", ")", "\n", "_", ",", "nn_inds", "=", "torch", ".", "min", "(", "d_mat", ",", "0", ")", "\n", "del", "d_mat", "# distance matrix uses lots of memory, free asap", "\n", "\n", "# Get style feature closest to each content feature and save", "\n", "# in 'out'", "\n", "nn_inds", "=", "nn_inds", ".", "unsqueeze", "(", "1", ")", ".", "expand", "(", "nn_inds", ".", "size", "(", "0", ")", ",", "ref_flat", ".", "size", "(", "1", ")", ")", "\n", "ref_sel", "=", "torch", ".", "gather", "(", "ref_flat", ",", "0", ",", "nn_inds", ")", ".", "transpose", "(", "1", ",", "0", ")", ".", "contiguous", "(", ")", "\n", "out", ".", "append", "(", "ref_sel", ")", "#.view(1, ref.size(1), src.size(2), ei - bi))", "\n", "\n", "bi", "=", "ei", "\n", "\n", "", "out", "=", "torch", ".", "cat", "(", "out", ",", "1", ")", "\n", "out", "=", "out", ".", "view", "(", "1", ",", "ref", ".", "size", "(", "1", ")", ",", "src", ".", "size", "(", "2", ")", ",", "src", ".", "size", "(", "3", ")", ")", "\n", "rplc", ".", "append", "(", "out", ")", "\n", "\n", "", "rplc", "=", "torch", ".", "cat", "(", "rplc", ",", "0", ")", "\n", "return", "rplc", "\n", "\n"]], "home.repos.pwc.inspect_result.nkolkin13_neuralneighborstyletransfer.utils.stylize.optimize_output_im": [[185, 344], ["utils.imagePyramid.syn_lap_pyr", "torch.optim.Adam", "torch.optim.Adam", "range", "utils.imagePyramid.dec_lap_pyr", "torch.autograd.Variable", "phi", "utils.imagePyramid.syn_lap_pyr", "max", "int", "int", "utils.misc.flatten_grid", "utils.distance.pairwise_distances_l2().clone().detach", "torch.optim.Adam.zero_grad", "utils.imagePyramid.syn_lap_pyr", "ell.backward", "torch.optim.Adam.step", "utils.featureExtract.get_feat_norms", "enumerate", "utils.imagePyramid.syn_lap_pyr.size", "utils.imagePyramid.syn_lap_pyr.size", "utils.misc.scl_spatial", "utils.featureExtract.extract_feats", "phi", "range", "max", "int", "int", "utils.misc.flatten_grid", "utils.distance.pairwise_distances_l2", "torch.flip().transpose", "torch.flip().transpose", "torch.flip", "torch.flip", "torch.flip().transpose", "torch.flip().transpose", "phi", "utils.imagePyramid.syn_lap_pyr.size", "utils.imagePyramid.syn_lap_pyr.size", "utils.distance.pairwise_distances_l2().clone", "utils.featureExtract.get_feat_norms", "len", "s_tmp.size", "s_tmp.contiguous().view", "cs_tmp.contiguous().view", "utils.distance.pairwise_distances_cos_center", "torch.min", "torch.min", "utils.imagePyramid.syn_lap_pyr.size", "utils.imagePyramid.syn_lap_pyr.size", "utils.misc.scl_spatial", "torch.mean", "torch.mean", "len", "torch.cat", "torch.cat", "max", "random.randint", "random.randint", "r_col_samp[].transpose", "s_col_samp[].transpose", "d_min.mean", "utils.imagePyramid.syn_lap_pyr.size", "utils.imagePyramid.syn_lap_pyr.size", "torch.abs", "torch.abs", "torch.flip", "torch.flip", "torch.flip", "torch.flip", "s_feat_tmp[].transpose", "range", "utils.distance.pairwise_distances_l2", "cur_tmp.size", "cur_tmp.size", "max", "s_tmp.contiguous", "cs_tmp.contiguous", "range", "len", "cur_tmp.size", "cur_tmp.size", "len"], "function", ["home.repos.pwc.inspect_result.nkolkin13_neuralneighborstyletransfer.utils.imagePyramid.syn_lap_pyr", "home.repos.pwc.inspect_result.nkolkin13_neuralneighborstyletransfer.utils.imagePyramid.dec_lap_pyr", "home.repos.pwc.inspect_result.nkolkin13_neuralneighborstyletransfer.utils.imagePyramid.syn_lap_pyr", "home.repos.pwc.inspect_result.nkolkin13_neuralneighborstyletransfer.utils.misc.flatten_grid", "home.repos.pwc.inspect_result.nkolkin13_neuralneighborstyletransfer.utils.imagePyramid.syn_lap_pyr", "home.repos.pwc.inspect_result.nkolkin13_neuralneighborstyletransfer.utils.featureExtract.get_feat_norms", "home.repos.pwc.inspect_result.nkolkin13_neuralneighborstyletransfer.utils.misc.scl_spatial", "home.repos.pwc.inspect_result.nkolkin13_neuralneighborstyletransfer.utils.featureExtract.extract_feats", "home.repos.pwc.inspect_result.nkolkin13_neuralneighborstyletransfer.utils.misc.flatten_grid", "home.repos.pwc.inspect_result.nkolkin13_neuralneighborstyletransfer.utils.distance.pairwise_distances_l2", "home.repos.pwc.inspect_result.nkolkin13_neuralneighborstyletransfer.utils.featureExtract.get_feat_norms", "home.repos.pwc.inspect_result.nkolkin13_neuralneighborstyletransfer.utils.distance.pairwise_distances_cos_center", "home.repos.pwc.inspect_result.nkolkin13_neuralneighborstyletransfer.utils.misc.scl_spatial", "home.repos.pwc.inspect_result.nkolkin13_neuralneighborstyletransfer.utils.distance.pairwise_distances_l2"], ["", "def", "optimize_output_im", "(", "s_pyr", ",", "c_pyr", ",", "content_im", ",", "style_im", ",", "target_feats", ",", "\n", "lr", ",", "max_iter", ",", "scl", ",", "phi", ",", "final_pass", "=", "False", ",", "\n", "content_loss", "=", "False", ",", "flip_aug", "=", "True", ")", ":", "\n", "    ", "''' Optimize laplacian pyramid coefficients of stylized image at a given\n        resolution, and return stylized pyramid coefficients.\n        Inputs:\n            s_pyr -- laplacian pyramid of style image\n            c_pyr -- laplacian pyramid of content image\n            content_im -- content image\n            style_im -- style image\n            target_feats -- precomputed target features of stylized output\n            lr -- learning rate for optimization\n            max_iter -- maximum number of optimization iterations\n            scl -- integer controls which resolution to optimize (corresponds\n                   to pyramid level of target resolution)\n            phi -- lambda function to compute features using pretrained VGG16\n            final_pass -- if true, ignore 'target_feats' and recompute target\n                          features before every step of gradient descent (and\n                          compute feature matches seperately for each layer\n                          instead of using hypercolumns)\n            content_loss -- if true, also minimize content loss that maintains\n                            self-similarity in color space between 32pixel\n                            downsampled output image and content image\n            flip_aug -- if true, extract style features from rotations of style\n                        image. This increases content preservation by making\n                        more options available when matching style features\n                        to content features\n        Outputs:\n            s_pyr -- pyramid coefficients of stylized output image at target\n                     resolution\n    '''", "\n", "# Initialize optimizer variables and optimizer       ", "\n", "output_im", "=", "syn_pyr", "(", "s_pyr", "[", "scl", ":", "]", ")", "\n", "opt_vars", "=", "[", "Variable", "(", "li", ".", "data", ",", "requires_grad", "=", "True", ")", "for", "li", "in", "s_pyr", "[", "scl", ":", "]", "]", "\n", "optimizer", "=", "torch", ".", "optim", ".", "Adam", "(", "opt_vars", ",", "lr", "=", "lr", ")", "\n", "\n", "# Original features uses all layers, but dropping conv5 block  speeds up ", "\n", "# method without hurting quality", "\n", "feature_list_final", "=", "[", "22", ",", "20", ",", "18", ",", "15", ",", "13", ",", "11", ",", "8", ",", "6", ",", "3", ",", "1", "]", "\n", "\n", "# Precompute features that remain constant", "\n", "if", "not", "final_pass", ":", "\n", "# Precompute normalized features targets during hypercolumn-matching ", "\n", "# regime for cosine distance", "\n", "        ", "target_feats_n", "=", "target_feats", "/", "get_feat_norms", "(", "target_feats", ")", "\n", "\n", "", "else", ":", "\n", "# For feature-splitting regime extract style features for each conv ", "\n", "# layer without downsampling (including from rotations if applicable)", "\n", "        ", "s_feat", "=", "phi", "(", "style_im", ",", "feature_list_final", ",", "False", ")", "\n", "\n", "if", "flip_aug", ":", "\n", "            ", "aug_list", "=", "[", "torch", ".", "flip", "(", "style_im", ",", "[", "2", "]", ")", ".", "transpose", "(", "2", ",", "3", ")", ",", "\n", "torch", ".", "flip", "(", "style_im", ",", "[", "2", ",", "3", "]", ")", ",", "\n", "torch", ".", "flip", "(", "style_im", ",", "[", "3", "]", ")", ".", "transpose", "(", "2", ",", "3", ")", "]", "\n", "\n", "for", "ia", ",", "im_aug", "in", "enumerate", "(", "aug_list", ")", ":", "\n", "                ", "s_feat_tmp", "=", "phi", "(", "im_aug", ",", "feature_list_final", ",", "False", ")", "\n", "\n", "if", "ia", "!=", "1", ":", "\n", "                    ", "s_feat_tmp", "=", "[", "s_feat_tmp", "[", "iii", "]", ".", "transpose", "(", "2", ",", "3", ")", "\n", "for", "iii", "in", "range", "(", "len", "(", "s_feat_tmp", ")", ")", "]", "\n", "\n", "", "s_feat", "=", "[", "torch", ".", "cat", "(", "[", "s_feat", "[", "iii", "]", ",", "s_feat_tmp", "[", "iii", "]", "]", ",", "2", ")", "\n", "for", "iii", "in", "range", "(", "len", "(", "s_feat_tmp", ")", ")", "]", "\n", "\n", "# Precompute content self-similarity matrix if needed for 'content_loss'", "\n", "", "", "", "if", "content_loss", ":", "\n", "        ", "c_full", "=", "syn_pyr", "(", "c_pyr", ")", "\n", "c_scl", "=", "max", "(", "c_full", ".", "size", "(", "2", ")", ",", "c_full", ".", "size", "(", "3", ")", ")", "\n", "c_fac", "=", "c_scl", "//", "32", "\n", "h", "=", "int", "(", "c_full", ".", "size", "(", "2", ")", "/", "c_fac", ")", "\n", "w", "=", "int", "(", "c_full", ".", "size", "(", "3", ")", "/", "c_fac", ")", "\n", "\n", "c_low_flat", "=", "flatten_grid", "(", "scl_spatial", "(", "c_full", ",", "h", ",", "w", ")", ")", "\n", "self_sim_target", "=", "pairwise_distances_l2", "(", "c_low_flat", ",", "c_low_flat", ")", ".", "clone", "(", ")", ".", "detach", "(", ")", "\n", "\n", "\n", "# Optimize pyramid coefficients to find image that produces stylized activations", "\n", "", "for", "i", "in", "range", "(", "max_iter", ")", ":", "\n", "\n", "# Zero out gradient and loss before current iteration", "\n", "        ", "optimizer", ".", "zero_grad", "(", ")", "\n", "ell", "=", "0.", "\n", "\n", "# Synthesize current output from pyramid coefficients", "\n", "output_im", "=", "syn_pyr", "(", "opt_vars", ")", "\n", "\n", "\n", "# Compare current features with stylized activations", "\n", "if", "not", "final_pass", ":", "# hypercolumn matching / 'hm' regime", "\n", "\n", "# Extract features from current output, normalize for cos distance", "\n", "            ", "cur_feats", "=", "extract_feats", "(", "output_im", ",", "phi", ")", "\n", "cur_feats_n", "=", "cur_feats", "/", "get_feat_norms", "(", "cur_feats", ")", "\n", "\n", "# Update overall loss w/ cosine loss w.r.t target features", "\n", "ell", "=", "ell", "+", "(", "1.", "-", "(", "target_feats_n", "*", "cur_feats_n", ")", ".", "sum", "(", "1", ")", ")", ".", "mean", "(", ")", "\n", "\n", "\n", "", "else", ":", "# feature splitting / 'fs' regime", "\n", "# Extract features from current output (keep each layer seperate ", "\n", "# and don't downsample)", "\n", "            ", "cur_feats", "=", "phi", "(", "output_im", ",", "feature_list_final", ",", "False", ")", "\n", "\n", "# Compute matches for each layer. For efficiency don't explicitly ", "\n", "# gather matches, only access through distance matrix.", "\n", "ell_fs", "=", "0.", "\n", "for", "h_i", "in", "range", "(", "len", "(", "s_feat", ")", ")", ":", "\n", "# Get features from a particular layer", "\n", "                ", "s_tmp", "=", "s_feat", "[", "h_i", "]", "\n", "cur_tmp", "=", "cur_feats", "[", "h_i", "]", "\n", "chans", "=", "s_tmp", ".", "size", "(", "1", ")", "\n", "\n", "# Sparsely sample feature tensors if too big, otherwise just ", "\n", "# reshape", "\n", "if", "max", "(", "cur_tmp", ".", "size", "(", "2", ")", ",", "cur_tmp", ".", "size", "(", "3", ")", ")", ">", "64", ":", "\n", "                    ", "stride", "=", "max", "(", "cur_tmp", ".", "size", "(", "2", ")", ",", "cur_tmp", ".", "size", "(", "3", ")", ")", "//", "64", "\n", "offset_a", "=", "random", ".", "randint", "(", "0", ",", "stride", "-", "1", ")", "\n", "offset_b", "=", "random", ".", "randint", "(", "0", ",", "stride", "-", "1", ")", "\n", "s_tmp", "=", "s_tmp", "[", ":", ",", ":", ",", "offset_a", ":", ":", "stride", ",", "offset_b", ":", ":", "stride", "]", "\n", "cs_tmp", "=", "cur_tmp", "[", ":", ",", ":", ",", "offset_a", ":", ":", "stride", ",", "offset_b", ":", ":", "stride", "]", "\n", "\n", "", "r_col_samp", "=", "s_tmp", ".", "contiguous", "(", ")", ".", "view", "(", "1", ",", "chans", ",", "-", "1", ")", "\n", "s_col_samp", "=", "cs_tmp", ".", "contiguous", "(", ")", ".", "view", "(", "1", ",", "chans", ",", "-", "1", ")", "\n", "\n", "# Compute distance matrix and find minimum along each row to ", "\n", "# implicitly get matches (and minimize distance between them)", "\n", "d_mat", "=", "pairwise_distances_cos_center", "(", "r_col_samp", "[", "0", "]", ".", "transpose", "(", "1", ",", "0", ")", ",", "\n", "s_col_samp", "[", "0", "]", ".", "transpose", "(", "1", ",", "0", ")", ")", "\n", "d_min", ",", "_", "=", "torch", ".", "min", "(", "d_mat", ",", "0", ")", "\n", "\n", "# Aggregate loss over layers", "\n", "ell_fs", "=", "ell_fs", "+", "d_min", ".", "mean", "(", ")", "\n", "\n", "# Update overall loss", "\n", "", "ell", "=", "ell", "+", "ell_fs", "\n", "\n", "# Optional self similarity content loss between downsampled output ", "\n", "# and content image. Always turn off at end for best results.", "\n", "", "if", "content_loss", "and", "not", "(", "final_pass", "and", "i", ">", "100", ")", ":", "\n", "            ", "o_scl", "=", "max", "(", "output_im", ".", "size", "(", "2", ")", ",", "output_im", ".", "size", "(", "3", ")", ")", "\n", "o_fac", "=", "o_scl", "/", "32.", "\n", "h", "=", "int", "(", "output_im", ".", "size", "(", "2", ")", "/", "o_fac", ")", "\n", "w", "=", "int", "(", "output_im", ".", "size", "(", "3", ")", "/", "o_fac", ")", "\n", "\n", "o_flat", "=", "flatten_grid", "(", "scl_spatial", "(", "output_im", ",", "h", ",", "w", ")", ")", "\n", "self_sim_out", "=", "pairwise_distances_l2", "(", "o_flat", ",", "o_flat", ")", "\n", "\n", "ell", "=", "ell", "+", "torch", ".", "mean", "(", "torch", ".", "abs", "(", "(", "self_sim_out", "-", "self_sim_target", ")", ")", ")", "\n", "\n", "# Update output's pyramid coefficients", "\n", "", "ell", ".", "backward", "(", ")", "\n", "optimizer", ".", "step", "(", ")", "\n", "\n", "# Update output's pyramid coefficients for current resolution", "\n", "# (and all coarser resolutions)    ", "\n", "", "s_pyr", "[", "scl", ":", "]", "=", "dec_pyr", "(", "output_im", ",", "len", "(", "c_pyr", ")", "-", "1", "-", "scl", ")", "\n", "return", "s_pyr", "\n", "", ""]], "home.repos.pwc.inspect_result.nkolkin13_neuralneighborstyletransfer.utils.distance.center": [[4, 7], ["torch.mean"], "function", ["None"], ["def", "center", "(", "x", ")", ":", "\n", "    ", "\"\"\"Subtract the mean of 'x' over leading dimension\"\"\"", "\n", "return", "x", "-", "torch", ".", "mean", "(", "x", ",", "0", ",", "keepdim", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nkolkin13_neuralneighborstyletransfer.utils.distance.pairwise_distances_cos": [[8, 28], ["torch.sqrt", "torch.sqrt", "torch.transpose", "x.size", "y.size", "torch.mm", "len", "len", "x.size", "y.size"], "function", ["None"], ["", "def", "pairwise_distances_cos", "(", "x", ",", "y", ")", ":", "\n", "    ", "\"\"\" Compute all pairwise cosine distances between rows of matrix 'x' and matrix 'y'\n        Inputs:\n            x -- NxD pytorch tensor\n            y -- MxD pytorch tensor\n        Outputs:\n            d -- NxM pytorch tensor where d[i,j] is the cosine distance between\n                 the vector at row i of matrix 'x' and the vector at row j of\n                 matrix 'y'\n    \"\"\"", "\n", "assert", "x", ".", "size", "(", "1", ")", "==", "y", ".", "size", "(", "1", ")", ",", "\"can only compute distance between vectors of same length\"", "\n", "assert", "(", "len", "(", "x", ".", "size", "(", ")", ")", "==", "2", ")", "and", "(", "len", "(", "y", ".", "size", "(", ")", ")", "==", "2", ")", ",", "\"pairwise distance computation\"", "\" assumes input tensors are matrices\"", "\n", "\n", "x_norm", "=", "torch", ".", "sqrt", "(", "(", "x", "**", "2", ")", ".", "sum", "(", "1", ")", ".", "view", "(", "-", "1", ",", "1", ")", ")", "\n", "y_norm", "=", "torch", ".", "sqrt", "(", "(", "y", "**", "2", ")", ".", "sum", "(", "1", ")", ".", "view", "(", "-", "1", ",", "1", ")", ")", "\n", "y_t", "=", "torch", ".", "transpose", "(", "y", "/", "y_norm", ",", "0", ",", "1", ")", "\n", "\n", "d", "=", "1.", "-", "torch", ".", "mm", "(", "x", "/", "x_norm", ",", "y_t", ")", "\n", "return", "d", "\n", "\n"]], "home.repos.pwc.inspect_result.nkolkin13_neuralneighborstyletransfer.utils.distance.pairwise_distances_sq_l2": [[29, 52], ["torch.transpose", "x.size", "y.size", "torch.mm", "len", "len", "x.size", "y.size"], "function", ["None"], ["", "def", "pairwise_distances_sq_l2", "(", "x", ",", "y", ")", ":", "\n", "    ", "\"\"\" Compute all pairwise squared l2 distances between rows of matrix 'x' and matrix 'y'\n        Inputs:\n            x -- NxD pytorch tensor\n            y -- MxD pytorch tensor\n        Outputs:\n            d -- NxM pytorch tensor where d[i,j] is the squared l2 distance between\n                 the vector at row i of matrix 'x' and the vector at row j of\n                 matrix 'y'\n    \"\"\"", "\n", "assert", "x", ".", "size", "(", "1", ")", "==", "y", ".", "size", "(", "1", ")", ",", "\"can only compute distance between vectors of same length\"", "\n", "assert", "(", "len", "(", "x", ".", "size", "(", ")", ")", "==", "2", ")", "and", "(", "len", "(", "y", ".", "size", "(", ")", ")", "==", "2", ")", ",", "\"pairwise distance computation\"", "\" assumes input tensors are matrices\"", "\n", "\n", "x_norm", "=", "(", "x", "**", "2", ")", ".", "sum", "(", "1", ")", ".", "view", "(", "-", "1", ",", "1", ")", "\n", "y_t", "=", "torch", ".", "transpose", "(", "y", ",", "0", ",", "1", ")", "\n", "y_norm", "=", "(", "y", "**", "2", ")", ".", "sum", "(", "1", ")", ".", "view", "(", "1", ",", "-", "1", ")", "\n", "\n", "d", "=", "-", "2.0", "*", "torch", ".", "mm", "(", "x", ",", "y_t", ")", "\n", "d", "+=", "x_norm", "\n", "d", "+=", "y_norm", "\n", "\n", "return", "d", "\n", "\n"]], "home.repos.pwc.inspect_result.nkolkin13_neuralneighborstyletransfer.utils.distance.pairwise_distances_l2": [[53, 59], ["torch.clamp", "torch.sqrt", "distance.pairwise_distances_sq_l2"], "function", ["home.repos.pwc.inspect_result.nkolkin13_neuralneighborstyletransfer.utils.distance.pairwise_distances_sq_l2"], ["", "def", "pairwise_distances_l2", "(", "x", ",", "y", ")", ":", "\n", "    ", "\"\"\" Compute all pairwise l2 distances between rows of 'x' and 'y',\n        thresholds minimum of squared l2 distance for stability of sqrt\n    \"\"\"", "\n", "d", "=", "torch", ".", "clamp", "(", "pairwise_distances_sq_l2", "(", "x", ",", "y", ")", ",", "min", "=", "1e-8", ")", "\n", "return", "torch", ".", "sqrt", "(", "d", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nkolkin13_neuralneighborstyletransfer.utils.distance.pairwise_distances_l2_center": [[60, 63], ["distance.pairwise_distances_l2", "distance.center", "distance.center"], "function", ["home.repos.pwc.inspect_result.nkolkin13_neuralneighborstyletransfer.utils.distance.pairwise_distances_l2", "home.repos.pwc.inspect_result.nkolkin13_neuralneighborstyletransfer.utils.distance.center", "home.repos.pwc.inspect_result.nkolkin13_neuralneighborstyletransfer.utils.distance.center"], ["", "def", "pairwise_distances_l2_center", "(", "x", ",", "y", ")", ":", "\n", "    ", "\"\"\" subtracts mean row from 'x' and 'y' before computing pairwise l2 distance between all rows\"\"\"", "\n", "return", "pairwise_distances_l2", "(", "center", "(", "x", ")", ",", "center", "(", "y", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nkolkin13_neuralneighborstyletransfer.utils.distance.pairwise_distances_cos_center": [[64, 67], ["distance.pairwise_distances_cos", "distance.center", "distance.center"], "function", ["home.repos.pwc.inspect_result.nkolkin13_neuralneighborstyletransfer.utils.distance.pairwise_distances_cos", "home.repos.pwc.inspect_result.nkolkin13_neuralneighborstyletransfer.utils.distance.center", "home.repos.pwc.inspect_result.nkolkin13_neuralneighborstyletransfer.utils.distance.center"], ["", "def", "pairwise_distances_cos_center", "(", "x", ",", "y", ")", ":", "\n", "    ", "\"\"\" subtracts mean row from 'x' and 'y' before computing pairwise cosine distance between all rows\"\"\"", "\n", "return", "pairwise_distances_cos", "(", "center", "(", "x", ")", ",", "center", "(", "y", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nkolkin13_neuralneighborstyletransfer.utils.imagePyramid.dec_lap_pyr": [[4, 31], ["range", "pyr.append", "cur.size", "cur.size", "torch.interpolate", "torch.interpolate", "pyr.append"], "function", ["None"], ["def", "dec_lap_pyr", "(", "x", ",", "levs", ")", ":", "\n", "    ", "\"\"\" constructs batch of 'levs' level laplacian pyramids from x\n        Inputs:\n            x -- BxCxHxW pytorch tensor\n            levs -- integer number of pyramid levels to construct\n        Outputs:\n            pyr -- a list of pytorch tensors, each representing a pyramid level,\n                   pyr[0] contains the finest level, pyr[-1] the coarsest\n    \"\"\"", "\n", "pyr", "=", "[", "]", "\n", "cur", "=", "x", "# Initialize approx. coefficients with original image", "\n", "for", "i", "in", "range", "(", "levs", ")", ":", "\n", "\n", "# Construct and store detail coefficients from current approx. coefficients", "\n", "        ", "h", "=", "cur", ".", "size", "(", "2", ")", "\n", "w", "=", "cur", ".", "size", "(", "3", ")", "\n", "x_small", "=", "F", ".", "interpolate", "(", "cur", ",", "(", "h", "//", "2", ",", "w", "//", "2", ")", ",", "mode", "=", "'bilinear'", ")", "\n", "x_back", "=", "F", ".", "interpolate", "(", "x_small", ",", "(", "h", ",", "w", ")", ",", "mode", "=", "'bilinear'", ")", "\n", "lap", "=", "cur", "-", "x_back", "\n", "pyr", ".", "append", "(", "lap", ")", "\n", "\n", "# Store new approx. coefficients", "\n", "cur", "=", "x_small", "\n", "\n", "", "pyr", ".", "append", "(", "cur", ")", "\n", "\n", "return", "pyr", "\n", "\n"]], "home.repos.pwc.inspect_result.nkolkin13_neuralneighborstyletransfer.utils.imagePyramid.syn_lap_pyr": [[32, 52], ["len", "range", "pyr[].size", "pyr[].size", "torch.interpolate"], "function", ["None"], ["", "def", "syn_lap_pyr", "(", "pyr", ")", ":", "\n", "    ", "\"\"\" collapse batch of laplacian pyramids stored in list of pytorch tensors\n        'pyr' into a single tensor.\n        Inputs:\n            pyr -- list of pytorch tensors, where pyr[i] has size BxCx(H/(2**i)x(W/(2**i))\n        Outpus:\n            x -- a BxCxHxW pytorch tensor\n    \"\"\"", "\n", "cur", "=", "pyr", "[", "-", "1", "]", "\n", "levs", "=", "len", "(", "pyr", ")", "\n", "\n", "for", "i", "in", "range", "(", "0", ",", "levs", "-", "1", ")", "[", ":", ":", "-", "1", "]", ":", "\n", "# Create new approximation coefficients from current approx. and detail coefficients", "\n", "# at next finest pyramid level", "\n", "        ", "up_x", "=", "pyr", "[", "i", "]", ".", "size", "(", "2", ")", "\n", "up_y", "=", "pyr", "[", "i", "]", ".", "size", "(", "3", ")", "\n", "cur", "=", "pyr", "[", "i", "]", "+", "F", ".", "interpolate", "(", "cur", ",", "(", "up_x", ",", "up_y", ")", ",", "mode", "=", "'bilinear'", ")", "\n", "", "x", "=", "cur", "\n", "\n", "return", "x", "\n", "", ""]], "home.repos.pwc.inspect_result.nkolkin13_neuralneighborstyletransfer.utils.featureExtract.get_feat_norms": [[8, 13], ["torch.clamp().sqrt", "torch.clamp().sqrt", "torch.clamp", "torch.clamp", "x.pow().sum", "x.pow"], "function", ["None"], ["def", "get_feat_norms", "(", "x", ")", ":", "\n", "    ", "\"\"\" Makes l2 norm of x[i,:,j,k] = 1 for all i,j,k. Clamps before sqrt for\n    stability\n    \"\"\"", "\n", "return", "torch", ".", "clamp", "(", "x", ".", "pow", "(", "2", ")", ".", "sum", "(", "1", ",", "keepdim", "=", "True", ")", ",", "1e-8", ",", "1e8", ")", ".", "sqrt", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nkolkin13_neuralneighborstyletransfer.utils.featureExtract.phi_cat": [[15, 39], ["x.size", "x.size", "phi", "torch.cat", "torch.cat", "f.size", "utils.misc.scl_spatial"], "function", ["home.repos.pwc.inspect_result.nkolkin13_neuralneighborstyletransfer.utils.misc.scl_spatial"], ["", "def", "phi_cat", "(", "x", ",", "phi", ",", "layer_l", ")", ":", "\n", "    ", "\"\"\" Extract conv features from 'x' at list of VGG16 layers 'layer_l'. Then\n        normalize features from each conv block based on # of channels, resize,\n        and concatenate into hypercolumns\n        Inputs:\n            x -- Bx3xHxW pytorch tensor, presumed to contain rgb images\n            phi -- lambda function calling a pretrained Vgg16Pretrained model\n            layer_l -- layer indexes to form hypercolumns out of\n        Outputs:\n            feats -- BxCxHxW pytorch tensor of hypercolumns extracted from 'x'\n                     C depends on 'layer_l'\n    \"\"\"", "\n", "h", "=", "x", ".", "size", "(", "2", ")", "\n", "w", "=", "x", ".", "size", "(", "3", ")", "\n", "\n", "feats", "=", "phi", "(", "x", ",", "layer_l", ",", "False", ")", "\n", "# Normalize each layer by # channels so # of channels doesn't dominate ", "\n", "# cosine distance", "\n", "feats", "=", "[", "f", "/", "f", ".", "size", "(", "1", ")", "for", "f", "in", "feats", "]", "\n", "\n", "# Scale layers' features to target size and concatenate", "\n", "feats", "=", "torch", ".", "cat", "(", "[", "scl_spatial", "(", "f", ",", "h", "//", "4", ",", "w", "//", "4", ")", "for", "f", "in", "feats", "]", ",", "1", ")", "\n", "\n", "return", "feats", "\n", "\n"]], "home.repos.pwc.inspect_result.nkolkin13_neuralneighborstyletransfer.utils.featureExtract.extract_feats": [[40, 75], ["featureExtract.phi_cat", "enumerate", "torch.flip().transpose", "torch.flip().transpose", "torch.flip", "torch.flip", "torch.flip().transpose", "torch.flip().transpose", "featureExtract.phi_cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.flip", "torch.flip", "torch.flip", "torch.flip", "phi_cat.transpose"], "function", ["home.repos.pwc.inspect_result.nkolkin13_neuralneighborstyletransfer.utils.featureExtract.phi_cat", "home.repos.pwc.inspect_result.nkolkin13_neuralneighborstyletransfer.utils.featureExtract.phi_cat"], ["", "def", "extract_feats", "(", "im", ",", "phi", ",", "flip_aug", "=", "False", ")", ":", "\n", "    ", "\"\"\" Extract hypercolumns from 'im' using pretrained VGG16 (passed as phi),\n    if speficied, extract hypercolumns from rotations of 'im' as well\n        Inputs:\n            im -- a Bx3xHxW pytorch tensor, presumed to contain rgb images\n            phi -- a lambda function calling a pretrained Vgg16Pretrained model\n            flip_aug -- whether to extract hypercolumns from rotations of 'im'\n                        as well\n        Outputs:\n            feats -- a tensor of hypercolumns extracted from 'im', spatial\n                     index is presumed to no longer matter\n    \"\"\"", "\n", "# In the original paper used all layers, but dropping conv5 block increases", "\n", "# speed without harming quality", "\n", "layer_l", "=", "[", "22", ",", "20", ",", "18", ",", "15", ",", "13", ",", "11", ",", "8", ",", "6", ",", "3", ",", "1", "]", "\n", "feats", "=", "phi_cat", "(", "im", ",", "phi", ",", "layer_l", ")", "\n", "\n", "# If specified, extract features from 90, 180, 270 degree rotations of 'im'", "\n", "if", "flip_aug", ":", "\n", "        ", "aug_list", "=", "[", "torch", ".", "flip", "(", "im", ",", "[", "2", "]", ")", ".", "transpose", "(", "2", ",", "3", ")", ",", "\n", "torch", ".", "flip", "(", "im", ",", "[", "2", ",", "3", "]", ")", ",", "\n", "torch", ".", "flip", "(", "im", ",", "[", "3", "]", ")", ".", "transpose", "(", "2", ",", "3", ")", "]", "\n", "\n", "for", "i", ",", "im_aug", "in", "enumerate", "(", "aug_list", ")", ":", "\n", "            ", "feats_new", "=", "phi_cat", "(", "im_aug", ",", "phi", ",", "layer_l", ")", "\n", "\n", "# Code never looks at patches of features, so fine to just stick", "\n", "# features from rotated images in adjacent spatial indexes, since", "\n", "# they will only be accessed in isolation", "\n", "if", "i", "==", "1", ":", "\n", "                ", "feats", "=", "torch", ".", "cat", "(", "[", "feats", ",", "feats_new", "]", ",", "2", ")", "\n", "", "else", ":", "\n", "                ", "feats", "=", "torch", ".", "cat", "(", "[", "feats", ",", "feats_new", ".", "transpose", "(", "2", ",", "3", ")", "]", ",", "2", ")", "\n", "\n", "", "", "", "return", "feats", "\n", "", ""]], "home.repos.pwc.inspect_result.nkolkin13_neuralneighborstyletransfer.utils.colorization.linear_2_oklab": [[13, 44], ["torch.clamp", "torch.clamp", "torch.pow", "torch.pow", "torch.pow", "torch.pow", "torch.pow", "torch.pow", "torch.cat", "torch.cat", "torch.clamp.size"], "function", ["None"], ["def", "linear_2_oklab", "(", "x", ")", ":", "\n", "    ", "\"\"\"Converts pytorch tensor 'x' from Linear to OkLAB colorspace, described here:\n        https://bottosson.github.io/posts/oklab/\n    Inputs:\n        x -- pytorch tensor of size B x 3 x H x W, assumed to be in linear \n             srgb colorspace, scaled between 0. and 1.\n    Returns:\n        y -- pytorch tensor of size B x 3 x H x W in OkLAB colorspace\n    \"\"\"", "\n", "assert", "x", ".", "size", "(", "1", ")", "==", "3", ",", "\"attempted to convert colorspace of tensor w/ > 3 channels\"", "\n", "\n", "x", "=", "torch", ".", "clamp", "(", "x", ",", "0.", ",", "1.", ")", "\n", "\n", "r", "=", "x", "[", ":", ",", "0", ":", "1", ",", ":", ",", ":", "]", "\n", "g", "=", "x", "[", ":", ",", "1", ":", "2", ",", ":", ",", ":", "]", "\n", "b", "=", "x", "[", ":", ",", "2", ":", "3", ",", ":", ",", ":", "]", "\n", "\n", "li", "=", "0.4121656120", "*", "r", "+", "0.5362752080", "*", "g", "+", "0.0514575653", "*", "b", "\n", "m", "=", "0.2118591070", "*", "r", "+", "0.6807189584", "*", "g", "+", "0.1074065790", "*", "b", "\n", "s", "=", "0.0883097947", "*", "r", "+", "0.2818474174", "*", "g", "+", "0.6302613616", "*", "b", "\n", "\n", "li", "=", "torch", ".", "pow", "(", "li", ",", "1.", "/", "3.", ")", "\n", "m", "=", "torch", ".", "pow", "(", "m", ",", "1.", "/", "3.", ")", "\n", "s", "=", "torch", ".", "pow", "(", "s", ",", "1.", "/", "3.", ")", "\n", "\n", "L", "=", "0.2104542553", "*", "li", "+", "0.7936177850", "*", "m", "-", "0.0040720468", "*", "s", "\n", "A", "=", "1.9779984951", "*", "li", "-", "2.4285922050", "*", "m", "+", "0.4505937099", "*", "s", "\n", "B", "=", "0.0259040371", "*", "li", "+", "0.7827717662", "*", "m", "-", "0.8086757660", "*", "s", "\n", "\n", "y", "=", "torch", ".", "cat", "(", "[", "L", ",", "A", ",", "B", "]", ",", "1", ")", "\n", "return", "y", "\n", "\n"]], "home.repos.pwc.inspect_result.nkolkin13_neuralneighborstyletransfer.utils.colorization.oklab_2_linear": [[45, 73], ["torch.pow", "torch.pow", "torch.pow", "torch.pow", "torch.pow", "torch.pow", "torch.cat", "torch.cat", "torch.clamp", "torch.clamp", "x.size"], "function", ["None"], ["", "def", "oklab_2_linear", "(", "x", ")", ":", "\n", "    ", "\"\"\"Converts pytorch tensor 'x' from OkLAB to Linear colorspace, described here:\n        https://bottosson.github.io/posts/oklab/\n    Inputs:\n        x -- pytorch tensor of size B x 3 x H x W, assumed to be in OkLAB colorspace\n    Returns:\n        y -- pytorch tensor of size B x 3 x H x W in Linear sRGB colorspace\n    \"\"\"", "\n", "assert", "x", ".", "size", "(", "1", ")", "==", "3", ",", "\"attempted to convert colorspace of tensor w/ > 3 channels\"", "\n", "\n", "L", "=", "x", "[", ":", ",", "0", ":", "1", ",", ":", ",", ":", "]", "\n", "A", "=", "x", "[", ":", ",", "1", ":", "2", ",", ":", ",", ":", "]", "\n", "B", "=", "x", "[", ":", ",", "2", ":", "3", ",", ":", ",", ":", "]", "\n", "\n", "li", "=", "L", "+", "0.3963377774", "*", "A", "+", "0.2158037573", "*", "B", "\n", "m", "=", "L", "-", "0.1055613458", "*", "A", "-", "0.0638541728", "*", "B", "\n", "s", "=", "L", "-", "0.0894841775", "*", "A", "-", "1.2914855480", "*", "B", "\n", "\n", "li", "=", "torch", ".", "pow", "(", "li", ",", "3", ")", "\n", "m", "=", "torch", ".", "pow", "(", "m", ",", "3", ")", "\n", "s", "=", "torch", ".", "pow", "(", "s", ",", "3", ")", "\n", "\n", "r", "=", "4.0767245293", "*", "li", "-", "3.3072168827", "*", "m", "+", "0.2307590544", "*", "s", "\n", "g", "=", "-", "1.2681437731", "*", "li", "+", "2.6093323231", "*", "m", "-", "0.3411344290", "*", "s", "\n", "b", "=", "-", "0.0041119885", "*", "li", "-", "0.7034763098", "*", "m", "+", "1.7068625689", "*", "s", "\n", "\n", "y", "=", "torch", ".", "cat", "(", "[", "r", ",", "g", ",", "b", "]", ",", "1", ")", "\n", "return", "torch", ".", "clamp", "(", "y", ",", "0.", ",", "1.", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nkolkin13_neuralneighborstyletransfer.utils.colorization.get_pad": [[74, 80], ["torch.pad"], "function", ["None"], ["", "def", "get_pad", "(", "x", ")", ":", "\n", "    ", "\"\"\"\n    Applies 1 pixel of replication padding to x\n    x -- B x D x H x W pytorch tensor\n    \"\"\"", "\n", "return", "F", ".", "pad", "(", "x", ",", "(", "1", ",", "1", ",", "1", ",", "1", ")", ",", "mode", "=", "'replicate'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nkolkin13_neuralneighborstyletransfer.utils.colorization.filter": [[81, 131], ["x.size", "x.size", "colorization.get_pad", "range", "torch.zeros_like", "torch.zeros_like", "xab.clone", "range", "range", "torch.pow", "torch.pow", "colorization.get_pad", "range", "torch.zeros_like", "torch.zeros_like", "range", "torch.abs", "torch.abs", "torch.exp", "torch.exp", "range", "range", "colorization.filter.comp"], "function", ["home.repos.pwc.inspect_result.nkolkin13_neuralneighborstyletransfer.utils.colorization.get_pad", "home.repos.pwc.inspect_result.nkolkin13_neuralneighborstyletransfer.utils.colorization.get_pad"], ["", "def", "filter", "(", "x", ")", ":", "\n", "    ", "\"\"\"\n    applies modified bilateral filter to AB channels of x, guided by L channel\n    x -- B x 3 x H x W pytorch tensor containing an image in LAB colorspace\n    \"\"\"", "\n", "\n", "h", "=", "x", ".", "size", "(", "2", ")", "\n", "w", "=", "x", ".", "size", "(", "3", ")", "\n", "\n", "# Seperate out luminance channel, don't use AB channels to measure similarity", "\n", "xl", "=", "x", "[", ":", ",", ":", "1", ",", ":", ",", ":", "]", "\n", "xab", "=", "x", "[", ":", ",", "1", ":", ",", ":", ",", ":", "]", "\n", "xl_pad", "=", "get_pad", "(", "xl", ")", "\n", "\n", "xl_w", "=", "{", "}", "\n", "for", "i", "in", "range", "(", "3", ")", ":", "\n", "        ", "for", "j", "in", "range", "(", "3", ")", ":", "\n", "            ", "xl_w", "[", "str", "(", "i", ")", "+", "str", "(", "j", ")", "]", "=", "xl_pad", "[", ":", ",", ":", ",", "i", ":", "(", "i", "+", "h", ")", ",", "j", ":", "(", "j", "+", "w", ")", "]", "\n", "\n", "# Iteratively apply in 3x3 window rather than use spatial kernel", "\n", "", "", "max_iters", "=", "5", "\n", "cur", "=", "torch", ".", "zeros_like", "(", "xab", ")", "\n", "\n", "# comparison function for pixel intensity", "\n", "def", "comp", "(", "x", ",", "y", ")", ":", "\n", "        ", "d", "=", "torch", ".", "abs", "(", "x", "-", "y", ")", "*", "5.", "\n", "return", "torch", ".", "pow", "(", "torch", ".", "exp", "(", "-", "1.", "*", "d", ")", ",", "2", ")", "\n", "\n", "# apply bilateral filtering to AB channels, guideded by L channel ", "\n", "", "cur", "=", "xab", ".", "clone", "(", ")", "\n", "for", "it", "in", "range", "(", "max_iters", ")", ":", "\n", "        ", "cur_pad", "=", "get_pad", "(", "cur", ")", "\n", "xl_v", "=", "{", "}", "\n", "for", "i", "in", "range", "(", "3", ")", ":", "\n", "            ", "for", "j", "in", "range", "(", "3", ")", ":", "\n", "                ", "xl_v", "[", "str", "(", "i", ")", "+", "str", "(", "j", ")", "]", "=", "cur_pad", "[", ":", ",", ":", ",", "i", ":", "(", "i", "+", "h", ")", ",", "j", ":", "(", "j", "+", "w", ")", "]", "\n", "\n", "", "", "denom", "=", "torch", ".", "zeros_like", "(", "xl", ")", "\n", "cur", "=", "cur", "*", "0.", "\n", "\n", "for", "i", "in", "range", "(", "3", ")", ":", "\n", "            ", "for", "j", "in", "range", "(", "3", ")", ":", "\n", "                ", "scl", "=", "comp", "(", "xl", ",", "xl_w", "[", "str", "(", "i", ")", "+", "str", "(", "j", ")", "]", ")", "\n", "cur", "=", "cur", "+", "xl_v", "[", "str", "(", "i", ")", "+", "str", "(", "j", ")", "]", "*", "scl", "\n", "denom", "=", "denom", "+", "scl", "\n", "\n", "", "", "cur", "=", "cur", "/", "denom", "\n", "# store result and return", "\n", "", "x", "[", ":", ",", "1", ":", ",", ":", ",", ":", "]", "=", "cur", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.nkolkin13_neuralneighborstyletransfer.utils.colorization.clamp_range": [[132, 139], ["torch.clamp", "torch.clamp", "y.min", "y.max"], "function", ["None"], ["", "def", "clamp_range", "(", "x", ",", "y", ")", ":", "\n", "    ", "'''\n    clamp the range of x to [min(y), max(y)]\n    x -- pytorch tensor\n    y -- pytorch tensor\n    '''", "\n", "return", "torch", ".", "clamp", "(", "x", ",", "y", ".", "min", "(", ")", ",", "y", ".", "max", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nkolkin13_neuralneighborstyletransfer.utils.colorization.color_match": [[140, 184], ["torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "colorization.linear_2_oklab", "linear_2_oklab.view", "colorization.linear_2_oklab", "colorization.linear_2_oklab", "linear_2_oklab.clone", "range", "utils.zca.zca_tensor", "range", "utils.imagePyramid.dec_lap_pyr", "utils.imagePyramid.dec_lap_pyr", "utils.imagePyramid.dec_lap_pyr", "utils.imagePyramid.dec_lap_pyr", "colorization.oklab_2_linear", "linear_2_oklab.size", "linear_2_oklab.size", "colorization.clamp_range", "utils.zca.zca_tensor", "colorization.filter", "colorization.clamp_range", "cov_s[].abs().max", "utils.zca.zca_tensor", "cov_s[].abs"], "function", ["home.repos.pwc.inspect_result.nkolkin13_neuralneighborstyletransfer.utils.colorization.linear_2_oklab", "home.repos.pwc.inspect_result.nkolkin13_neuralneighborstyletransfer.utils.colorization.linear_2_oklab", "home.repos.pwc.inspect_result.nkolkin13_neuralneighborstyletransfer.utils.colorization.linear_2_oklab", "home.repos.pwc.inspect_result.nkolkin13_neuralneighborstyletransfer.utils.zca.zca_tensor", "home.repos.pwc.inspect_result.nkolkin13_neuralneighborstyletransfer.utils.imagePyramid.dec_lap_pyr", "home.repos.pwc.inspect_result.nkolkin13_neuralneighborstyletransfer.utils.imagePyramid.dec_lap_pyr", "home.repos.pwc.inspect_result.nkolkin13_neuralneighborstyletransfer.utils.imagePyramid.dec_lap_pyr", "home.repos.pwc.inspect_result.nkolkin13_neuralneighborstyletransfer.utils.imagePyramid.dec_lap_pyr", "home.repos.pwc.inspect_result.nkolkin13_neuralneighborstyletransfer.utils.colorization.oklab_2_linear", "home.repos.pwc.inspect_result.nkolkin13_neuralneighborstyletransfer.utils.colorization.clamp_range", "home.repos.pwc.inspect_result.nkolkin13_neuralneighborstyletransfer.utils.zca.zca_tensor", "home.repos.pwc.inspect_result.nkolkin13_neuralneighborstyletransfer.utils.colorization.filter", "home.repos.pwc.inspect_result.nkolkin13_neuralneighborstyletransfer.utils.colorization.clamp_range", "home.repos.pwc.inspect_result.nkolkin13_neuralneighborstyletransfer.utils.zca.zca_tensor"], ["", "def", "color_match", "(", "c", ",", "s", ",", "o", ",", "moment_only", "=", "False", ")", ":", "\n", "    ", "'''\n    Constrain the low frequences of the AB channels of output image 'o' (containing hue and saturation)\n    to be an affine transformation of 'c' matching the mean and covariance of the style image 's'.\n    Compared to the raw output of optimization this is highly constrained, but in practice\n    we find the benefit to robustness to be worth the reduced stylization.\n    c -- B x 3 x H x W pytorch tensor containing content image\n    s -- B x 3 x H x W pytorch tensor containing style image\n    o -- B x 3 x H x W pytorch tensor containing initial output image\n    moment_only -- boolean, prevents applying bilateral filter to AB channels of final output to match luminance's edges\n    '''", "\n", "c", "=", "torch", ".", "clamp", "(", "c", ",", "0.", ",", "1.", ")", "\n", "s", "=", "torch", ".", "clamp", "(", "s", ",", "0.", ",", "1.", ")", "\n", "o", "=", "torch", ".", "clamp", "(", "o", ",", "0.", ",", "1.", ")", "\n", "\n", "x", "=", "linear_2_oklab", "(", "c", ")", "\n", "x_flat", "=", "x", ".", "view", "(", "x", ".", "size", "(", "0", ")", ",", "x", ".", "size", "(", "1", ")", ",", "-", "1", ",", "1", ")", "\n", "y", "=", "linear_2_oklab", "(", "s", ")", "\n", "o", "=", "linear_2_oklab", "(", "o", ")", "\n", "\n", "x_new", "=", "o", ".", "clone", "(", ")", "\n", "for", "i", "in", "range", "(", "3", ")", ":", "\n", "        ", "x_new", "[", ":", ",", "i", ":", "i", "+", "1", ",", ":", ",", ":", "]", "=", "clamp_range", "(", "x_new", "[", ":", ",", "i", ":", "i", "+", "1", ",", ":", ",", ":", "]", ",", "y", "[", ":", ",", "i", ":", "i", "+", "1", ",", ":", ",", ":", "]", ")", "\n", "\n", "", "_", ",", "cov_s", "=", "zca_tensor", "(", "x_new", ",", "y", ")", "\n", "\n", "if", "moment_only", "or", "cov_s", "[", "1", ":", ",", "1", ":", "]", ".", "abs", "(", ")", ".", "max", "(", ")", "<", "6e-5", ":", "\n", "       ", "x_new", "[", ":", ",", "1", ":", ",", ":", ",", ":", "]", "=", "o", "[", ":", ",", "1", ":", ",", ":", ",", ":", "]", "\n", "x_new", ",", "_", "=", "zca_tensor", "(", "x_new", ",", "y", ")", "\n", "", "else", ":", "\n", "        ", "x_new", "[", ":", ",", "1", ":", ",", ":", ",", ":", "]", "=", "x", "[", ":", ",", "1", ":", ",", ":", ",", ":", "]", "\n", "x_new", "[", ":", ",", "1", ":", ",", ":", ",", ":", "]", "=", "zca_tensor", "(", "x_new", "[", ":", ",", "1", ":", ",", ":", ",", ":", "]", ",", "y", "[", ":", ",", "1", ":", ",", ":", ",", ":", "]", ")", "[", "0", "]", "\n", "x_new", "=", "filter", "(", "x_new", ")", "\n", "\n", "", "for", "i", "in", "range", "(", "3", ")", ":", "\n", "        ", "x_new", "[", ":", ",", "i", ":", "i", "+", "1", ",", ":", ",", ":", "]", "=", "clamp_range", "(", "x_new", "[", ":", ",", "i", ":", "i", "+", "1", ",", ":", ",", ":", "]", ",", "y", "[", ":", ",", "i", ":", "i", "+", "1", ",", ":", ",", ":", "]", ")", "\n", "\n", "", "x_pyr", "=", "dec_pyr", "(", "x", ",", "4", ")", "\n", "y_pyr", "=", "dec_pyr", "(", "y", ",", "4", ")", "\n", "x_new_pyr", "=", "dec_pyr", "(", "x_new", ",", "4", ")", "\n", "o_pyr", "=", "dec_pyr", "(", "o", ",", "4", ")", "\n", "x_new_pyr", "[", ":", "-", "1", "]", "=", "o_pyr", "[", ":", "-", "1", "]", "\n", "\n", "return", "oklab_2_linear", "(", "x_new", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.nkolkin13_neuralneighborstyletransfer.utils.misc.to_device": [[13, 20], ["tensor.cuda"], "function", ["None"], ["def", "to_device", "(", "tensor", ")", ":", "\n", "    ", "\"\"\"Ensures torch tensor 'tensor' is moved to gpu\n    if global variable USE_GPU is True\"\"\"", "\n", "if", "USE_GPU", ":", "\n", "        ", "return", "tensor", ".", "cuda", "(", ")", "\n", "", "else", ":", "\n", "        ", "return", "tensor", "\n", "\n"]], "home.repos.pwc.inspect_result.nkolkin13_neuralneighborstyletransfer.utils.misc.match_device": [[21, 30], ["mut.cpu.cuda", "mut.cpu.cpu"], "function", ["None"], ["", "", "def", "match_device", "(", "ref", ",", "mut", ")", ":", "\n", "    ", "\"\"\" Puts torch tensor 'mut' on the same device as torch tensor 'ref'\"\"\"", "\n", "if", "ref", ".", "is_cuda", "and", "not", "mut", ".", "is_cuda", ":", "\n", "        ", "mut", "=", "mut", ".", "cuda", "(", ")", "\n", "\n", "", "if", "not", "ref", ".", "is_cuda", "and", "mut", ".", "is_cuda", ":", "\n", "        ", "mut", "=", "mut", ".", "cpu", "(", ")", "\n", "\n", "", "return", "mut", "\n", "\n"]], "home.repos.pwc.inspect_result.nkolkin13_neuralneighborstyletransfer.utils.misc.get_gpu_memory_map": [[31, 50], ["subprocess.check_output", "dict", "print", "int", "zip", "subprocess.check_output.strip().split", "range", "len", "subprocess.check_output.strip"], "function", ["None"], ["", "def", "get_gpu_memory_map", "(", ")", ":", "\n", "    ", "\"\"\"Get the current gpu usage. Taken from:\n    https://discuss.pytorch.org/t/access-gpu-memory-usage-in-pytorch/3192/4\n\n    Returns\n    -------\n    usage: dict\n        Keys are device ids as integers.\n        Values are memory usage as integers in MB.\n    \"\"\"", "\n", "result", "=", "subprocess", ".", "check_output", "(", "\n", "[", "\n", "'nvidia-smi'", ",", "'--query-gpu=memory.used'", ",", "\n", "'--format=csv,nounits,noheader'", "\n", "]", ",", "encoding", "=", "'utf-8'", ")", "\n", "# Convert lines into a dictionary", "\n", "gpu_memory", "=", "[", "int", "(", "x", ")", "for", "x", "in", "result", ".", "strip", "(", ")", ".", "split", "(", "'\\n'", ")", "]", "\n", "gpu_memory_map", "=", "dict", "(", "zip", "(", "range", "(", "len", "(", "gpu_memory", ")", ")", ",", "gpu_memory", ")", ")", "\n", "print", "(", "gpu_memory_map", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nkolkin13_neuralneighborstyletransfer.utils.misc.flatten_grid": [[51, 61], ["x.contiguous().view().clone().transpose", "x.size", "x.contiguous().view().clone", "x.contiguous().view", "x.size", "x.contiguous"], "function", ["None"], ["", "def", "flatten_grid", "(", "x", ")", ":", "\n", "    ", "\"\"\" collapses spatial dimensions of pytorch tensor 'x' and transposes\n        Inputs:\n            x -- 1xCxHxW pytorch tensor\n        Outputs:\n            y -- (H*W)xC pytorch tensor\n    \"\"\"", "\n", "assert", "x", ".", "size", "(", "0", ")", "==", "1", ",", "\"undefined behavior for batched input\"", "\n", "y", "=", "x", ".", "contiguous", "(", ")", ".", "view", "(", "x", ".", "size", "(", "1", ")", ",", "-", "1", ")", ".", "clone", "(", ")", ".", "transpose", "(", "1", ",", "0", ")", "\n", "return", "y", "\n", "\n"]], "home.repos.pwc.inspect_result.nkolkin13_neuralneighborstyletransfer.utils.misc.scl_spatial": [[62, 67], ["torch.interpolate"], "function", ["None"], ["", "def", "scl_spatial", "(", "x", ",", "h", ",", "w", ")", ":", "\n", "    ", "\"\"\"shorter alias for default way I call F.interpolate (i.e. as bilinear\n    interpolation\n    \"\"\"", "\n", "return", "F", ".", "interpolate", "(", "x", ",", "(", "h", ",", "w", ")", ",", "mode", "=", "'bilinear'", ",", "align_corners", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nkolkin13_neuralneighborstyletransfer.utils.misc.load_path_for_pytorch": [[68, 109], ["imageio.imread().astype", "torch.from_numpy().contiguous().permute().contiguous", "torch.from_numpy().contiguous().permute().contiguous", "int", "int", "len", "numpy.stack", "float", "side_comp", "misc.scl_spatial", "print", "imageio.imread", "torch.from_numpy().contiguous().permute", "torch.from_numpy().contiguous().permute", "np.stack.unsqueeze", "torch.from_numpy().contiguous", "torch.from_numpy().contiguous", "torch.from_numpy", "torch.from_numpy"], "function", ["home.repos.pwc.inspect_result.nkolkin13_neuralneighborstyletransfer.utils.misc.scl_spatial"], ["", "def", "load_path_for_pytorch", "(", "im_path", ",", "target_size", "=", "1000", ",", "side_comp", "=", "max", ",", "verbose", "=", "False", ")", ":", "\n", "    ", "\"\"\"\n    Loads image at 'path', selects height or width with function 'side_comp'\n    then scales the image, setting selected dimension to 'target_size' and\n    maintaining aspect ratio. Will also convert RGBA or greyscale images to\n    RGB\n\n    Returns:\n        x -- a HxWxC pytorch tensor of rgb values scaled between 0. and 1.\n    \"\"\"", "\n", "# Load Image", "\n", "x", "=", "imread", "(", "im_path", ")", ".", "astype", "(", "np", ".", "float32", ")", "\n", "\n", "\n", "# Converts image to rgb if greyscale", "\n", "if", "len", "(", "x", ".", "shape", ")", "<", "3", ":", "\n", "        ", "x", "=", "np", ".", "stack", "(", "[", "x", ",", "x", ",", "x", "]", ",", "2", ")", "\n", "\n", "# Removes alpha channel if present", "\n", "", "if", "x", ".", "shape", "[", "2", "]", ">", "3", ":", "\n", "        ", "x", "=", "x", "[", ":", ",", ":", ",", ":", "3", "]", "\n", "\n", "# Rescale rgb values", "\n", "", "x", "=", "x", "/", "255.", "\n", "\n", "# Convert from numpy", "\n", "x_dims", "=", "x", ".", "shape", "\n", "x", "=", "torch", ".", "from_numpy", "(", "x", ")", ".", "contiguous", "(", ")", ".", "permute", "(", "2", ",", "0", ",", "1", ")", ".", "contiguous", "(", ")", "\n", "\n", "# Rescale to desired size", "\n", "# by default maintains aspect ratio relative to long side", "\n", "# change side_comp to be min for short side", "\n", "fac", "=", "float", "(", "target_size", ")", "/", "side_comp", "(", "x_dims", "[", ":", "2", "]", ")", "\n", "h", "=", "int", "(", "x_dims", "[", "0", "]", "*", "fac", ")", "\n", "w", "=", "int", "(", "x_dims", "[", "1", "]", "*", "fac", ")", "\n", "x", "=", "scl_spatial", "(", "x", ".", "unsqueeze", "(", "0", ")", ",", "h", ",", "w", ")", "[", "0", "]", "\n", "\n", "if", "verbose", ":", "\n", "        ", "print", "(", "f'DEBUG: image from path {im_path} loaded with size {x_dims}'", ")", "\n", "\n", "", "return", "x", "\n", "", ""]], "home.repos.pwc.inspect_result.nkolkin13_neuralneighborstyletransfer.pretrained.vgg.Vgg16Pretrained.__init__": [[13, 43], ["super().__init__", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "range", "range", "range", "range", "vgg.Vgg16Pretrained.slice1.add_module", "vgg.Vgg16Pretrained.slice2.add_module", "vgg.Vgg16Pretrained.slice3.add_module", "vgg.Vgg16Pretrained.slice4.add_module", "vgg.Vgg16Pretrained.parameters", "torchvision.models.vgg16", "str", "str", "str", "str", "torchvision.models.vgg16"], "methods", ["home.repos.pwc.inspect_result.nkolkin13_neuralneighborstyletransfer.pretrained.vgg.Vgg16Pretrained.__init__"], ["    ", "def", "__init__", "(", "self", ",", "requires_grad", "=", "False", ")", ":", "\n", "        ", "super", "(", "Vgg16Pretrained", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "try", ":", "\n", "            ", "vgg_pretrained_features", "=", "models", ".", "vgg16", "(", "pretrained", "=", "True", ")", ".", "features", "\n", "", "except", "ssl", ".", "SSLError", ":", "\n", "# unsafe fix to allow pretrained pytorch model to be downloaded", "\n", "# exposes application to man-in-the-middle attacks while model is", "\n", "# being downloaded", "\n", "            ", "create_default_context", "=", "ssl", ".", "_create_default_https_context", "\n", "ssl", ".", "_create_default_https_context", "=", "ssl", ".", "_create_unverified_context", "\n", "vgg_pretrained_features", "=", "models", ".", "vgg16", "(", "pretrained", "=", "True", ")", ".", "features", "\n", "ssl", ".", "_create_default_https_context", "=", "create_default_context", "\n", "\n", "", "self", ".", "vgg_layers", "=", "vgg_pretrained_features", "\n", "self", ".", "slice1", "=", "torch", ".", "nn", ".", "Sequential", "(", ")", "\n", "self", ".", "slice2", "=", "torch", ".", "nn", ".", "Sequential", "(", ")", "\n", "self", ".", "slice3", "=", "torch", ".", "nn", ".", "Sequential", "(", ")", "\n", "self", ".", "slice4", "=", "torch", ".", "nn", ".", "Sequential", "(", ")", "\n", "for", "x", "in", "range", "(", "1", ")", ":", "\n", "            ", "self", ".", "slice1", ".", "add_module", "(", "str", "(", "x", ")", ",", "vgg_pretrained_features", "[", "x", "]", ")", "\n", "", "for", "x", "in", "range", "(", "1", ",", "9", ")", ":", "\n", "            ", "self", ".", "slice2", ".", "add_module", "(", "str", "(", "x", ")", ",", "vgg_pretrained_features", "[", "x", "]", ")", "\n", "", "for", "x", "in", "range", "(", "9", ",", "16", ")", ":", "\n", "            ", "self", ".", "slice3", ".", "add_module", "(", "str", "(", "x", ")", ",", "vgg_pretrained_features", "[", "x", "]", ")", "\n", "", "for", "x", "in", "range", "(", "16", ",", "23", ")", ":", "\n", "            ", "self", ".", "slice4", ".", "add_module", "(", "str", "(", "x", ")", ",", "vgg_pretrained_features", "[", "x", "]", ")", "\n", "", "if", "not", "requires_grad", ":", "\n", "            ", "for", "param", "in", "self", ".", "parameters", "(", ")", ":", "\n", "                ", "param", ".", "requires_grad", "=", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.nkolkin13_neuralneighborstyletransfer.pretrained.vgg.Vgg16Pretrained.forward": [[44, 83], ["x_in.clone", "range", "range", "l2.append", "vgg.Vgg16Pretrained.vgg_layers[].forward", "max", "l2.append", "len", "l2[].size", "l2[].size", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "len", "zi_list.append", "zi_list.append", "torch.interpolate", "torch.interpolate"], "methods", ["home.repos.pwc.inspect_result.nkolkin13_neuralneighborstyletransfer.pretrained.vgg.Vgg16Pretrained.forward"], ["", "", "", "def", "forward", "(", "self", ",", "x_in", ",", "inds", "=", "[", "1", ",", "3", ",", "6", ",", "8", ",", "11", ",", "13", ",", "15", ",", "22", ",", "29", "]", ",", "concat", "=", "True", ")", ":", "\n", "\n", "        ", "x", "=", "x_in", ".", "clone", "(", ")", "# prevent accidentally modifying input in place", "\n", "# Preprocess input according to original imagenet training", "\n", "mean", "=", "[", "0.485", ",", "0.456", ",", "0.406", "]", "\n", "std", "=", "[", "0.229", ",", "0.224", ",", "0.225", "]", "\n", "for", "i", "in", "range", "(", "3", ")", ":", "\n", "            ", "x", "[", ":", ",", "i", ":", "(", "i", "+", "1", ")", ",", ":", ",", ":", "]", "=", "(", "x", "[", ":", ",", "i", ":", "(", "i", "+", "1", ")", ",", ":", ",", ":", "]", "-", "mean", "[", "i", "]", ")", "/", "std", "[", "i", "]", "\n", "\n", "# Get hidden state at layers specified by 'inds'", "\n", "", "l2", "=", "[", "]", "\n", "if", "-", "1", "in", "inds", ":", "\n", "            ", "l2", ".", "append", "(", "x_in", ")", "\n", "\n", "# Only need to run network until we get to the max depth we want outputs from", "\n", "", "for", "i", "in", "range", "(", "max", "(", "inds", ")", "+", "1", ")", ":", "\n", "            ", "x", "=", "self", ".", "vgg_layers", "[", "i", "]", ".", "forward", "(", "x", ")", "\n", "if", "i", "in", "inds", ":", "\n", "                ", "l2", ".", "append", "(", "x", ")", "\n", "\n", "# Concatenate hidden states if desired (after upsampling to spatial size of largest output)", "\n", "", "", "if", "concat", ":", "\n", "            ", "if", "len", "(", "l2", ")", ">", "1", ":", "\n", "                ", "zi_list", "=", "[", "]", "\n", "max_h", "=", "l2", "[", "0", "]", ".", "size", "(", "2", ")", "\n", "max_w", "=", "l2", "[", "0", "]", ".", "size", "(", "3", ")", "\n", "for", "zi", "in", "l2", ":", "\n", "                    ", "if", "len", "(", "zi_list", ")", "==", "0", ":", "\n", "                        ", "zi_list", ".", "append", "(", "zi", ")", "\n", "", "else", ":", "\n", "                        ", "zi_list", ".", "append", "(", "F", ".", "interpolate", "(", "zi", ",", "(", "max_h", ",", "max_w", ")", ",", "mode", "=", "'bilinear'", ")", ")", "\n", "\n", "", "", "z", "=", "torch", ".", "cat", "(", "zi_list", ",", "1", ")", "\n", "", "else", ":", "# don't bother doing anything if only returning one hidden state", "\n", "                ", "z", "=", "l2", "[", "0", "]", "\n", "", "", "else", ":", "# Otherwise return list of hidden states", "\n", "            ", "z", "=", "l2", "\n", "\n", "", "return", "z", "\n", "\n"]]}