{"home.repos.pwc.inspect_result.Garrafao_TokenChange.SemanticChangeDetection.LSC_SVD.main": [[21, 206], ["docopt.docopt", "float", "float", "int", "int", "logging.basicConfig", "print", "time.time", "logging.critical", "logging.critical", "get_ipython().run_line_magic", "utils_.Space", "utils_.Space.matrix.toarray", "logging.critical", "get_ipython().run_line_magic", "utils_.Space", "utils_.Space.matrix.toarray", "LSC_SVD.getCOS", "LSC_SVD.getAPD", "logging.critical", "numpy.concatenate", "utils_.Space", "utils_.Space.save", "get_ipython().run_line_magic", "len", "range", "range", "set", "clusterScoreBinary.append", "scipy.spatial.distance.jensenshannon", "print", "print", "print", "print", "print", "print", "print", "print", "print", "print", "print", "print", "print", "print", "print", "print", "print", "print", "print", "print", "logging.critical", "logging.critical", "print", "len", "cosineDistanceBinary.append", "cosineDistanceBinary.append", "APDBinary.append", "APDBinary.append", "open", "file.readlines", "len", "labelA_1.append", "labelA_2.append", "len", "len", "os.path.splitext", "os.path.splitext", "open", "csv.writer", "csv.writer.writerows", "get_ipython", "get_ipython", "get_ipython", "len", "labelA_1.count", "labelA_2.count", "numpy.histogram", "numpy.histogram", "os.path.basename", "os.path.basename", "labels.append", "labelA_2.count", "labelA_1.count", "time.time", "int"], "function", ["home.repos.pwc.inspect_result.Garrafao_TokenChange.SemanticChangeDetection.LSC_W2V.getCOS", "home.repos.pwc.inspect_result.Garrafao_TokenChange.SemanticChangeDetection.LSC_W2V.getAPD"], ["def", "main", "(", ")", ":", "\n", "\n", "# Get the arguments", "\n", "    ", "args", "=", "docopt", "(", "\"\"\"\n\n    Usage:\n        LSC_SVD.py <pathToMatrix> <pathW2i> <pathCorpora> <pathSentences1> <pathSentences2> <outPathVectors> <outPathLabels> <outPathResults> <sentenceType> <clusteringInitialization> <clustering> <limitAGL> \n        <limitCOS> <limitCluster> <windowSize>  \n        LSC_SVD.py <pathCorpora> <pathSentences1> <pathSentences2> <sentenceType> <clusteringInitialization> <clustering> <limitAGL> <limitCOS> <limitCluster> <windowSize>  \n        \n    Arguments:\n        <pathToMatrix> = Path to the first order matrix\n        <pathW2i> = Path to W2i   \n        <pathCorpora> = Path to the corpora\n        <pathSentences1> = Path to the test sentences from time1\n        <pathSentences2> = Path to the test sentences from time2\n        <outPathVectors> = Path to store the vectors\n        <outPathLabels> = Path to store the clustering labels\n        <outPathResults> = Path to store the lsc scores\t\n        <sentenceType> = \"lemma\" or \"token\"\n        <clusteringInitialization> = \"gaac\" for precalculated initializations, else random\n        <clustering> = \"kmeans\" or \"hierarchical\"\n        <limitAGL> = Change score limit for AGL to still be consiered as change (Good is about 0.2)\n        <limitCOS> = Change score limit for Cosine to still be consiered as change (Good is about 0.02) \n        <limitCluster> = Minimum number of elements a cluster has to contain from one time and less from the other, to get assigned a change (Good is 5-10)\n        <windowSize> = Window size (Good is 20)\n\n\n\n    \"\"\"", ")", "\n", "\n", "pathSentences1", "=", "args", "[", "'<pathSentences1>'", "]", "\n", "pathSentences2", "=", "args", "[", "'<pathSentences2>'", "]", "\n", "outPathVectors", "=", "args", "[", "'<outPathVectors>'", "]", "\n", "outPathLabels", "=", "args", "[", "'<outPathLabels>'", "]", "\n", "clusteringInitialization", "=", "args", "[", "'<clusteringInitialization>'", "]", "\n", "clustering", "=", "args", "[", "'<clustering>'", "]", "\n", "pathResults", "=", "args", "[", "'<outPathResults>'", "]", "\n", "limitAGL", "=", "float", "(", "args", "[", "'<limitAGL>'", "]", ")", "\n", "limitCOS", "=", "float", "(", "args", "[", "'<limitCOS>'", "]", ")", "\n", "limitCluster", "=", "int", "(", "args", "[", "'<limitCluster>'", "]", ")", "\n", "pathToMatrix", "=", "args", "[", "'<pathToMatrix>'", "]", "\n", "pathW2i", "=", "args", "[", "'<pathW2i>'", "]", "\n", "windowSize", "=", "int", "(", "args", "[", "'<windowSize>'", "]", ")", "\n", "pathCorpora", "=", "args", "[", "'<pathCorpora>'", "]", "\n", "sentenceType", "=", "args", "[", "'<sentenceType>'", "]", "\n", "\n", "\n", "\n", "if", "len", "(", "sys", ".", "argv", ")", "==", "11", ":", "\n", "        ", "outPathVectors", "=", "\"Files/Vectors/SecondOrder/Vectors.npz\"", "\n", "outPathLabels", "=", "\"Files/Clustering/cluster_labels.csv\"", "\n", "pathResults", "=", "\"Files/LSC/lsc_scores.csv\"", "\n", "pathToMatrix", "=", "\"Files/Vectors/FirstOrder/matrix.npz\"", "\n", "pathW2i", "=", "\"Files/Vectors/FirstOrder/w2i.npz.npy\"", "\n", "\n", "\n", "\n", "", "logging", ".", "basicConfig", "(", "format", "=", "'%(asctime)s : %(levelname)s : %(message)s'", ",", "level", "=", "logging", ".", "CRITICAL", ")", "\n", "print", "(", "\"\"", ")", "\n", "start_time", "=", "time", ".", "time", "(", ")", "\n", "logging", ".", "critical", "(", "\"SVD LSC start\"", ")", "\n", "\n", "#Create the vectors of corpora 1", "\n", "logging", ".", "critical", "(", "\"Create the vectors of corpora 1\"", ")", "\n", "\n", "get_ipython", "(", ")", ".", "run_line_magic", "(", "'run'", ",", "'WordSenseClustering/CountBasedVectors.py $pathToMatrix $pathW2i $pathCorpora $pathSentences1 $outPathVectors $sentenceType $windowSize'", ")", "\n", "\n", "\n", "\n", "inSpace", "=", "Space", "(", "path", "=", "outPathVectors", ")", "\n", "vectors1", "=", "inSpace", ".", "matrix", ".", "toarray", "(", ")", "\n", "\n", "#Create the vectors of corpora 2", "\n", "logging", ".", "critical", "(", "\"Create the vectors of corpora 2\"", ")", "\n", "get_ipython", "(", ")", ".", "run_line_magic", "(", "'run'", ",", "'WordSenseClustering/CountBasedVectors.py $pathToMatrix $pathW2i $pathCorpora $pathSentences2 $outPathVectors $sentenceType $windowSize'", ")", "\n", "inSpace", "=", "Space", "(", "path", "=", "outPathVectors", ")", "\n", "vectors2", "=", "inSpace", ".", "matrix", ".", "toarray", "(", ")", "\n", "\n", "#Create the lists to store the binary results in ", "\n", "cosineDistanceBinary", "=", "[", "]", "\n", "APDBinary", "=", "[", "]", "\n", "clusterScoreBinary", "=", "[", "]", "\n", "\n", "#Calculate cosineDistance for the two vectors", "\n", "cosineDistance", "=", "getCOS", "(", "vectors1", ",", "vectors2", ")", "\n", "if", "cosineDistance", ">=", "limitCOS", ":", "\n", "        ", "cosineDistanceBinary", ".", "append", "(", "1", ")", "\n", "", "else", ":", "\n", "        ", "cosineDistanceBinary", ".", "append", "(", "0", ")", "\n", "\n", "#Calculate Average pairwise distance for the two vectors", "\n", "", "APD", "=", "getAPD", "(", "vectors1", ",", "vectors2", ",", "200", ")", "\n", "if", "APD", ">=", "limitAGL", ":", "\n", "        ", "APDBinary", ".", "append", "(", "1", ")", "\n", "", "else", ":", "\n", "        ", "APDBinary", ".", "append", "(", "0", ")", "\n", "\n", "#Create and cluster the combined vectors of both corpora", "\n", "", "logging", ".", "critical", "(", "\"Create and cluster the combined vectors of both corpora\"", ")", "\n", "vectors", "=", "np", ".", "concatenate", "(", "(", "vectors1", ",", "vectors2", ")", ",", "axis", "=", "0", ")", "\n", "outSpace", "=", "Space", "(", "matrix", "=", "vectors", ",", "rows", "=", "\" \"", ",", "columns", "=", "\" \"", ")", "\n", "outSpace", ".", "save", "(", "outPathVectors", ")", "\n", "#Cluster the combined vectors", "\n", "get_ipython", "(", ")", ".", "run_line_magic", "(", "'run'", ",", "'WordSenseClustering/Clustering.py $outPathVectors 0 $outPathLabels 0 $clusteringInitialization 0 $clustering'", ")", "\n", "\n", "#Load list of labels", "\n", "labels", "=", "[", "]", "\n", "with", "open", "(", "outPathLabels", ",", "'r'", ")", "as", "file", ":", "\n", "        ", "data", "=", "file", ".", "readlines", "(", ")", "\n", "", "for", "i", "in", "data", "[", "-", "1", "]", ":", "\n", "        ", "if", "i", "!=", "\",\"", ":", "\n", "            ", "if", "i", "!=", "\"\\n\"", ":", "\n", "                ", "labels", ".", "append", "(", "int", "(", "i", ")", ")", "\n", "\n", "# Calculated cluster LSC score", "\n", "", "", "", "labelA_1", "=", "[", "]", "\n", "labelA_2", "=", "[", "]", "\n", "\n", "maximum", "=", "len", "(", "vectors1", ")", "\n", "for", "i", "in", "range", "(", "0", ",", "len", "(", "vectors1", ")", ")", ":", "\n", "        ", "labelA_1", ".", "append", "(", "labels", "[", "i", "]", ")", "\n", "\n", "", "for", "i", "in", "range", "(", "maximum", ",", "maximum", "+", "len", "(", "vectors2", ")", ")", ":", "\n", "        ", "labelA_2", ".", "append", "(", "labels", "[", "i", "]", ")", "\n", "\n", "", "changeA", "=", "0", "\n", "for", "j", "in", "set", "(", "labels", ")", ":", "\n", "        ", "if", "labelA_1", ".", "count", "(", "j", ")", ">=", "limitCluster", ":", "\n", "            ", "if", "labelA_2", ".", "count", "(", "j", ")", "<", "limitCluster", ":", "\n", "                ", "changeA", "=", "1", "\n", "", "", "if", "labelA_2", ".", "count", "(", "j", ")", ">=", "limitCluster", ":", "\n", "            ", "if", "labelA_1", ".", "count", "(", "j", ")", "<", "limitCluster", ":", "\n", "                ", "changeA", "=", "1", "\n", "\n", "", "", "", "clusterScoreBinary", ".", "append", "(", "changeA", ")", "\n", "\n", "p", "=", "np", ".", "histogram", "(", "labelA_1", ")", "[", "0", "]", "/", "len", "(", "labelA_1", ")", "\n", "q", "=", "np", ".", "histogram", "(", "labelA_2", ")", "[", "0", "]", "/", "len", "(", "labelA_2", ")", "\n", "\n", "dist", "=", "distance", ".", "jensenshannon", "(", "p", ",", "q", ")", "\n", "\n", "\n", "filename1", "=", "os", ".", "path", ".", "splitext", "(", "os", ".", "path", ".", "basename", "(", "pathSentences1", ")", ")", "[", "0", "]", "\n", "filename2", "=", "os", ".", "path", ".", "splitext", "(", "os", ".", "path", ".", "basename", "(", "pathSentences2", ")", ")", "[", "0", "]", "\n", "\n", "cos", "=", "[", "filename1", ",", "filename2", ",", "\"cosineDistance\"", ",", "cosineDistance", "]", "\n", "apd", "=", "[", "filename1", ",", "filename2", ",", "\"APD\"", ",", "APD", "]", "\n", "cluster", "=", "[", "filename1", ",", "filename2", ",", "\"clusterScore\"", ",", "dist", "]", "\n", "cosBin", "=", "[", "filename1", ",", "filename2", ",", "\"cosineDistanceBinary\"", ",", "cosineDistanceBinary", "[", "0", "]", "]", "\n", "APDBin", "=", "[", "filename1", ",", "filename2", ",", "\"APDBinary\"", ",", "APDBinary", "[", "0", "]", "]", "\n", "clusterBin", "=", "[", "filename1", ",", "filename2", ",", "\"clusterScoreBinary\"", ",", "clusterScoreBinary", "[", "0", "]", "]", "\n", "\n", "\n", "\n", "print", "(", "\"Graded LSC:\"", ")", "\n", "print", "(", "\"\"", ")", "\n", "print", "(", "\"cosine distance:\"", ")", "\n", "print", "(", "cosineDistance", ")", "\n", "print", "(", "\"\"", ")", "\n", "print", "(", "\"Average pairwise distance:\"", ")", "\n", "print", "(", "APD", ")", "\n", "print", "(", "\"\"", ")", "\n", "print", "(", "\"JSD:\"", ")", "\n", "print", "(", "dist", ")", "\n", "print", "(", "\"\"", ")", "\n", "print", "(", "\"\"", ")", "\n", "print", "(", "\"Binary LSC:\"", ")", "\n", "print", "(", "\"\"", ")", "\n", "print", "(", "\"cosine distance binary:\"", ")", "\n", "print", "(", "cosineDistanceBinary", "[", "0", "]", ")", "\n", "print", "(", "\"APD distance binary:\"", ")", "\n", "print", "(", "APDBinary", "[", "0", "]", ")", "\n", "print", "(", "\"JSD binary:\"", ")", "\n", "print", "(", "clusterScoreBinary", "[", "0", "]", ")", "\n", "\n", "\n", "\n", "with", "open", "(", "pathResults", ",", "'a'", ",", "newline", "=", "''", ")", "as", "file", ":", "\n", "        ", "writer", "=", "csv", ".", "writer", "(", "file", ")", "\n", "writer", ".", "writerows", "(", "[", "cos", ",", "apd", ",", "cluster", ",", "cosBin", ",", "APDBin", ",", "clusterBin", "]", ")", "\n", "\n", "", "logging", ".", "critical", "(", "\"SVD LSC end\"", ")", "\n", "logging", ".", "critical", "(", "\"--- %s seconds ---\"", "%", "(", "time", ".", "time", "(", ")", "-", "start_time", ")", ")", "\n", "print", "(", "\"\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Garrafao_TokenChange.SemanticChangeDetection.LSC_SVD.getCOS": [[209, 214], ["scipy.spatial.distance.cosine", "numpy.sum", "len", "numpy.sum", "len"], "function", ["None"], ["", "def", "getCOS", "(", "vec1", ",", "vec2", ")", ":", "\n", "    ", "sum1", "=", "np", ".", "sum", "(", "vec1", ",", "axis", "=", "0", ")", "/", "len", "(", "vec1", ")", "\n", "sum2", "=", "np", ".", "sum", "(", "vec2", ",", "axis", "=", "0", ")", "/", "len", "(", "vec2", ")", "\n", "result", "=", "distance", ".", "cosine", "(", "sum1", ",", "sum2", ")", "\n", "return", "result", "\n", "\n"]], "home.repos.pwc.inspect_result.Garrafao_TokenChange.SemanticChangeDetection.LSC_SVD.getAPD": [[217, 235], ["min", "random.sample", "range", "len", "len", "range", "testList1.append", "testList2.append", "range", "min", "len", "len", "scipy.spatial.distance.cosine"], "function", ["None"], ["", "def", "getAPD", "(", "vec1", ",", "vec2", ",", "size", ")", ":", "\n", "\n", "    ", "testList1", "=", "[", "]", "\n", "testList2", "=", "[", "]", "\n", "result", "=", "0", "\n", "size", "=", "min", "(", "len", "(", "vec1", ")", ",", "len", "(", "vec2", ")", ",", "size", ")", "\n", "randoms", "=", "random", ".", "sample", "(", "range", "(", "0", ",", "min", "(", "len", "(", "vec1", ")", ",", "len", "(", "vec2", ")", ")", ")", ",", "size", ")", "\n", "for", "i", "in", "randoms", ":", "\n", "        ", "testList1", ".", "append", "(", "vec1", "[", "i", "]", ")", "\n", "testList2", ".", "append", "(", "vec2", "[", "i", "]", ")", "\n", "\n", "", "for", "i", "in", "range", "(", "0", ",", "size", ")", ":", "\n", "        ", "for", "j", "in", "range", "(", "0", ",", "size", ")", ":", "\n", "            ", "result", "=", "result", "+", "distance", ".", "cosine", "(", "testList1", "[", "i", "]", ",", "testList2", "[", "j", "]", ")", "\n", "", "", "if", "result", "!=", "0", ":", "\n", "        ", "result", "=", "result", "/", "(", "size", "*", "size", ")", "\n", "\n", "", "return", "result", "\n", "\n"]], "home.repos.pwc.inspect_result.Garrafao_TokenChange.SemanticChangeDetection.LSC_SVD.jensen_shannon_distance": [[237, 257], ["numpy.array", "numpy.array", "numpy.sqrt", "scipy.stats.entropy", "scipy.stats.entropy"], "function", ["None"], ["", "def", "jensen_shannon_distance", "(", "p", ",", "q", ")", ":", "\n", "    ", "\"\"\"\n    method to compute the Jenson-Shannon Distance \n    between two probability distributions\n    \"\"\"", "\n", "\n", "# convert the vectors into numpy arrays in case that they aren't", "\n", "p", "=", "np", ".", "array", "(", "p", ")", "\n", "q", "=", "np", ".", "array", "(", "q", ")", "\n", "\n", "# calculate m", "\n", "m", "=", "(", "p", "+", "q", ")", "/", "2", "\n", "\n", "# compute Jensen Shannon Divergence", "\n", "divergence", "=", "(", "scipy", ".", "stats", ".", "entropy", "(", "p", ",", "m", ")", "+", "scipy", ".", "stats", ".", "entropy", "(", "q", ",", "m", ")", ")", "/", "2", "\n", "\n", "# compute the Jensen Shannon Distance", "\n", "distance", "=", "np", ".", "sqrt", "(", "divergence", ")", "\n", "\n", "return", "distance", "\n", "\n"]], "home.repos.pwc.inspect_result.Garrafao_TokenChange.SemanticChangeDetection.LSC_Bert.main": [[23, 187], ["docopt.docopt", "float", "float", "int", "logging.basicConfig", "print", "time.time", "logging.critical", "logging.critical", "get_ipython().run_line_magic", "utils_.Space", "utils_.Space.matrix.toarray", "logging.critical", "get_ipython().run_line_magic", "utils_.Space", "utils_.Space.matrix.toarray", "LSC_Bert.getCOS", "LSC_Bert.getAPD", "logging.critical", "numpy.concatenate", "utils_.Space", "utils_.Space.save", "get_ipython().run_line_magic", "len", "range", "range", "set", "clusterScoreBinary.append", "scipy.spatial.distance.jensenshannon", "print", "print", "print", "print", "print", "print", "print", "print", "print", "print", "print", "print", "print", "print", "print", "print", "print", "print", "print", "print", "logging.critical", "logging.critical", "print", "len", "cosineDistanceBinary.append", "cosineDistanceBinary.append", "APDBinary.append", "APDBinary.append", "open", "file.readlines", "len", "labelA_1.append", "labelA_2.append", "len", "len", "os.path.splitext", "os.path.splitext", "open", "csv.writer", "csv.writer.writerows", "get_ipython", "get_ipython", "get_ipython", "len", "labelA_1.count", "labelA_2.count", "numpy.histogram", "numpy.histogram", "os.path.basename", "os.path.basename", "labels.append", "labelA_2.count", "labelA_1.count", "time.time", "int"], "function", ["home.repos.pwc.inspect_result.Garrafao_TokenChange.SemanticChangeDetection.LSC_W2V.getCOS", "home.repos.pwc.inspect_result.Garrafao_TokenChange.SemanticChangeDetection.LSC_W2V.getAPD"], ["def", "main", "(", ")", ":", "\n", "\n", "# Get the arguments", "\n", "    ", "args", "=", "docopt", "(", "\"\"\"\n\n    Usage:\n        LSC_Bert.py  <pathSentences1> <pathSentences2> <outPathVectors> <outPathLabels> <outPathResults> <vecType> <clusteringInitialization> <clustering> <limitAGL> <limitCOS> <limitCluster>\n        LSC_Bert.py  <pathSentences1> <pathSentences2> <vecType> <clusteringInitialization> <clustering> <limitAGL> <limitCOS> <limitCluster>\n        \n    Arguments:\n       \n        <pathSentences1> = Path to the test sentences from time1\n        <pathSentences2> = Path to the test sentences from time2\n        <outPathVectors> = Path to store the vectors\n        <outPathLabels> = Path to store the clustering labels\n        <outPathResults> = Path to store the lsc scores\n        <vecType> = Type of vector representation: \"token\" or \"lemma\"\n        <clusteringInitialization> = \"gaac\" for precalculated initializations, else random\n        <clustering> = \"kmeans\" or \"hierarchical\"\n        <limitAGL> = Change score limit for AGL to still be consiered as change (Good is about 0.2)\n        <limitCOS> = Change score limit for Cosine to still be consiered as change (Good is about 0.02) \n        <limitCluster> = Minimum number of elements a cluster has to contain from one time and less from the other, to get assigned a change (Good is 5-10)\n\n    \"\"\"", ")", "\n", "\n", "pathSentences1", "=", "args", "[", "'<pathSentences1>'", "]", "\n", "pathSentences2", "=", "args", "[", "'<pathSentences2>'", "]", "\n", "outPathVectors", "=", "args", "[", "'<outPathVectors>'", "]", "\n", "outPathLabels", "=", "args", "[", "'<outPathLabels>'", "]", "\n", "vecType", "=", "args", "[", "'<vecType>'", "]", "\n", "clusteringInitialization", "=", "args", "[", "'<clusteringInitialization>'", "]", "\n", "pathResults", "=", "args", "[", "'<outPathResults>'", "]", "\n", "limitAGL", "=", "float", "(", "args", "[", "'<limitAGL>'", "]", ")", "\n", "limitCOS", "=", "float", "(", "args", "[", "'<limitCOS>'", "]", ")", "\n", "limitCluster", "=", "int", "(", "args", "[", "'<limitCluster>'", "]", ")", "\n", "clustering", "=", "args", "[", "'<clustering>'", "]", "\n", "\n", "if", "len", "(", "sys", ".", "argv", ")", "==", "9", ":", "\n", "        ", "outPathVectors", "=", "\"Files/Vectors/SecondOrder/Vectors.npz\"", "\n", "outPathLabels", "=", "\"Files/Clustering/cluster_labels.csv\"", "\n", "pathResults", "=", "\"Files/LSC/lsc_scores.csv\"", "\n", "\n", "\n", "", "logging", ".", "basicConfig", "(", "format", "=", "'%(asctime)s : %(levelname)s : %(message)s'", ",", "level", "=", "logging", ".", "CRITICAL", ")", "\n", "print", "(", "\"\"", ")", "\n", "start_time", "=", "time", ".", "time", "(", ")", "\n", "logging", ".", "critical", "(", "\"Bert LSC start\"", ")", "\n", "\n", "#Create the vectors of corpora 1", "\n", "logging", ".", "critical", "(", "\"Create and cluster the vectors of corpora 1\"", ")", "\n", "get_ipython", "(", ")", ".", "run_line_magic", "(", "'run'", ",", "'WordSenseClustering/Bert.py $pathSentences1 $outPathVectors $vecType'", ")", "\n", "inSpace", "=", "Space", "(", "path", "=", "outPathVectors", ")", "\n", "vectors1", "=", "inSpace", ".", "matrix", ".", "toarray", "(", ")", "\n", "\n", "#Create the vectors of corpora 2", "\n", "logging", ".", "critical", "(", "\"Create the vectors of corpora 2\"", ")", "\n", "get_ipython", "(", ")", ".", "run_line_magic", "(", "'run'", ",", "'WordSenseClustering/Bert.py $pathSentences2 $outPathVectors $vecType'", ")", "\n", "inSpace", "=", "Space", "(", "path", "=", "outPathVectors", ")", "\n", "vectors2", "=", "inSpace", ".", "matrix", ".", "toarray", "(", ")", "\n", "\n", "#Create the lists to store the binary results in ", "\n", "cosineDistanceBinary", "=", "[", "]", "\n", "APDBinary", "=", "[", "]", "\n", "clusterScoreBinary", "=", "[", "]", "\n", "\n", "#Calculate cosineDistance for the two vectors", "\n", "cosineDistance", "=", "getCOS", "(", "vectors1", ",", "vectors2", ")", "\n", "if", "cosineDistance", ">=", "limitCOS", ":", "\n", "        ", "cosineDistanceBinary", ".", "append", "(", "1", ")", "\n", "", "else", ":", "\n", "        ", "cosineDistanceBinary", ".", "append", "(", "0", ")", "\n", "\n", "#Calculate Average pairwise distance for the two vectors", "\n", "", "APD", "=", "getAPD", "(", "vectors1", ",", "vectors2", ",", "200", ")", "\n", "if", "APD", ">=", "limitAGL", ":", "\n", "        ", "APDBinary", ".", "append", "(", "1", ")", "\n", "", "else", ":", "\n", "        ", "APDBinary", ".", "append", "(", "0", ")", "\n", "\n", "#Create and cluster the combined vectors of both corpora", "\n", "", "logging", ".", "critical", "(", "\"Create the combined vectors of both corpora\"", ")", "\n", "vectors", "=", "np", ".", "concatenate", "(", "(", "vectors1", ",", "vectors2", ")", ",", "axis", "=", "0", ")", "\n", "outSpace", "=", "Space", "(", "matrix", "=", "vectors", ",", "rows", "=", "\" \"", ",", "columns", "=", "\" \"", ")", "\n", "outSpace", ".", "save", "(", "outPathVectors", ")", "\n", "#Cluster the combined vectors", "\n", "get_ipython", "(", ")", ".", "run_line_magic", "(", "'run'", ",", "'WordSenseClustering/Clustering.py $outPathVectors 0 $outPathLabels 0 $clusteringInitialization 0 $clustering'", ")", "\n", "\n", "\n", "#Load list of labels", "\n", "labels", "=", "[", "]", "\n", "with", "open", "(", "outPathLabels", ",", "'r'", ")", "as", "file", ":", "\n", "        ", "data", "=", "file", ".", "readlines", "(", ")", "\n", "", "for", "i", "in", "data", "[", "-", "1", "]", ":", "\n", "        ", "if", "i", "!=", "\",\"", ":", "\n", "            ", "if", "i", "!=", "\"\\n\"", ":", "\n", "                ", "labels", ".", "append", "(", "int", "(", "i", ")", ")", "\n", "\n", "# Calculated cluster LSC score", "\n", "", "", "", "labelA_1", "=", "[", "]", "\n", "labelA_2", "=", "[", "]", "\n", "\n", "maximum", "=", "len", "(", "vectors1", ")", "\n", "for", "i", "in", "range", "(", "0", ",", "len", "(", "vectors1", ")", ")", ":", "\n", "        ", "labelA_1", ".", "append", "(", "labels", "[", "i", "]", ")", "\n", "\n", "", "for", "i", "in", "range", "(", "maximum", ",", "maximum", "+", "len", "(", "vectors2", ")", ")", ":", "\n", "        ", "labelA_2", ".", "append", "(", "labels", "[", "i", "]", ")", "\n", "\n", "", "changeA", "=", "0", "\n", "for", "j", "in", "set", "(", "labels", ")", ":", "\n", "        ", "if", "labelA_1", ".", "count", "(", "j", ")", ">=", "limitCluster", ":", "\n", "            ", "if", "labelA_2", ".", "count", "(", "j", ")", "<", "limitCluster", ":", "\n", "                ", "changeA", "=", "1", "\n", "", "", "if", "labelA_2", ".", "count", "(", "j", ")", ">=", "limitCluster", ":", "\n", "            ", "if", "labelA_1", ".", "count", "(", "j", ")", "<", "limitCluster", ":", "\n", "                ", "changeA", "=", "1", "\n", "\n", "", "", "", "clusterScoreBinary", ".", "append", "(", "changeA", ")", "\n", "\n", "p", "=", "np", ".", "histogram", "(", "labelA_1", ")", "[", "0", "]", "/", "len", "(", "labelA_1", ")", "\n", "q", "=", "np", ".", "histogram", "(", "labelA_2", ")", "[", "0", "]", "/", "len", "(", "labelA_2", ")", "\n", "\n", "dist", "=", "distance", ".", "jensenshannon", "(", "p", ",", "q", ")", "\n", "\n", "filename1", "=", "os", ".", "path", ".", "splitext", "(", "os", ".", "path", ".", "basename", "(", "pathSentences1", ")", ")", "[", "0", "]", "\n", "filename2", "=", "os", ".", "path", ".", "splitext", "(", "os", ".", "path", ".", "basename", "(", "pathSentences2", ")", ")", "[", "0", "]", "\n", "\n", "cos", "=", "[", "filename1", ",", "filename2", ",", "\"cosineDistance\"", ",", "cosineDistance", "]", "\n", "apd", "=", "[", "filename1", ",", "filename2", ",", "\"APD\"", ",", "APD", "]", "\n", "cluster", "=", "[", "filename1", ",", "filename2", ",", "\"clusterScore\"", ",", "dist", "]", "\n", "cosBin", "=", "[", "filename1", ",", "filename2", ",", "\"cosineDistanceBinary\"", ",", "cosineDistanceBinary", "[", "0", "]", "]", "\n", "APDBin", "=", "[", "filename1", ",", "filename2", ",", "\"APDBinary\"", ",", "APDBinary", "[", "0", "]", "]", "\n", "clusterBin", "=", "[", "filename1", ",", "filename2", ",", "\"clusterScoreBinary\"", ",", "clusterScoreBinary", "[", "0", "]", "]", "\n", "\n", "\n", "print", "(", "\"Graded LSC:\"", ")", "\n", "print", "(", "\"\"", ")", "\n", "print", "(", "\"cosine distance:\"", ")", "\n", "print", "(", "cosineDistance", ")", "\n", "print", "(", "\"\"", ")", "\n", "print", "(", "\"Average pairwise distance:\"", ")", "\n", "print", "(", "APD", ")", "\n", "print", "(", "\"\"", ")", "\n", "print", "(", "\"JSD:\"", ")", "\n", "print", "(", "dist", ")", "\n", "print", "(", "\"\"", ")", "\n", "print", "(", "\"\"", ")", "\n", "print", "(", "\"Binary LSC:\"", ")", "\n", "print", "(", "\"\"", ")", "\n", "print", "(", "\"cosine distance binary:\"", ")", "\n", "print", "(", "cosineDistanceBinary", "[", "0", "]", ")", "\n", "print", "(", "\"APD distance binary:\"", ")", "\n", "print", "(", "APDBinary", "[", "0", "]", ")", "\n", "print", "(", "\"JSD binary:\"", ")", "\n", "print", "(", "clusterScoreBinary", "[", "0", "]", ")", "\n", "\n", "\n", "with", "open", "(", "pathResults", ",", "'a'", ",", "newline", "=", "''", ")", "as", "file", ":", "\n", "        ", "writer", "=", "csv", ".", "writer", "(", "file", ")", "\n", "writer", ".", "writerows", "(", "[", "cos", ",", "apd", ",", "cluster", ",", "cosBin", ",", "APDBin", ",", "clusterBin", "]", ")", "\n", "\n", "", "logging", ".", "critical", "(", "\"Bert LSC end\"", ")", "\n", "logging", ".", "critical", "(", "\"--- %s seconds ---\"", "%", "(", "time", ".", "time", "(", ")", "-", "start_time", ")", ")", "\n", "print", "(", "\"\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Garrafao_TokenChange.SemanticChangeDetection.LSC_Bert.getCOS": [[190, 195], ["scipy.spatial.distance.cosine", "numpy.sum", "len", "numpy.sum", "len"], "function", ["None"], ["", "def", "getCOS", "(", "vec1", ",", "vec2", ")", ":", "\n", "    ", "sum1", "=", "np", ".", "sum", "(", "vec1", ",", "axis", "=", "0", ")", "/", "len", "(", "vec1", ")", "\n", "sum2", "=", "np", ".", "sum", "(", "vec2", ",", "axis", "=", "0", ")", "/", "len", "(", "vec2", ")", "\n", "result", "=", "distance", ".", "cosine", "(", "sum1", ",", "sum2", ")", "\n", "return", "result", "\n", "\n"]], "home.repos.pwc.inspect_result.Garrafao_TokenChange.SemanticChangeDetection.LSC_Bert.getAPD": [[198, 216], ["min", "random.sample", "range", "len", "len", "range", "testList1.append", "testList2.append", "range", "min", "len", "len", "scipy.spatial.distance.cosine"], "function", ["None"], ["", "def", "getAPD", "(", "vec1", ",", "vec2", ",", "size", ")", ":", "\n", "\n", "    ", "testList1", "=", "[", "]", "\n", "testList2", "=", "[", "]", "\n", "result", "=", "0", "\n", "size", "=", "min", "(", "len", "(", "vec1", ")", ",", "len", "(", "vec2", ")", ",", "size", ")", "\n", "randoms", "=", "random", ".", "sample", "(", "range", "(", "0", ",", "min", "(", "len", "(", "vec1", ")", ",", "len", "(", "vec2", ")", ")", ")", ",", "size", ")", "\n", "for", "i", "in", "randoms", ":", "\n", "        ", "testList1", ".", "append", "(", "vec1", "[", "i", "]", ")", "\n", "testList2", ".", "append", "(", "vec2", "[", "i", "]", ")", "\n", "\n", "", "for", "i", "in", "range", "(", "0", ",", "size", ")", ":", "\n", "        ", "for", "j", "in", "range", "(", "0", ",", "size", ")", ":", "\n", "            ", "result", "=", "result", "+", "distance", ".", "cosine", "(", "testList1", "[", "i", "]", ",", "testList2", "[", "j", "]", ")", "\n", "", "", "if", "result", "!=", "0", ":", "\n", "        ", "result", "=", "result", "/", "(", "size", "*", "size", ")", "\n", "\n", "", "return", "result", "\n", "\n"]], "home.repos.pwc.inspect_result.Garrafao_TokenChange.SemanticChangeDetection.LSC_Bert.jensen_shannon_distance": [[218, 238], ["numpy.array", "numpy.array", "numpy.sqrt", "scipy.stats.entropy", "scipy.stats.entropy"], "function", ["None"], ["", "def", "jensen_shannon_distance", "(", "p", ",", "q", ")", ":", "\n", "    ", "\"\"\"\n    method to compute the Jenson-Shannon Distance \n    between two probability distributions\n    \"\"\"", "\n", "\n", "# convert the vectors into numpy arrays in case that they aren't", "\n", "p", "=", "np", ".", "array", "(", "p", ")", "\n", "q", "=", "np", ".", "array", "(", "q", ")", "\n", "\n", "# calculate m", "\n", "m", "=", "(", "p", "+", "q", ")", "/", "2", "\n", "\n", "# compute Jensen Shannon Divergence", "\n", "divergence", "=", "(", "scipy", ".", "stats", ".", "entropy", "(", "p", ",", "m", ")", "+", "scipy", ".", "stats", ".", "entropy", "(", "q", ",", "m", ")", ")", "/", "2", "\n", "\n", "# compute the Jensen Shannon Distance", "\n", "distance", "=", "np", ".", "sqrt", "(", "divergence", ")", "\n", "\n", "return", "distance", "\n", "\n"]], "home.repos.pwc.inspect_result.Garrafao_TokenChange.SemanticChangeDetection.LSC_W2V.main": [[19, 189], ["docopt.docopt", "float", "float", "int", "int", "logging.basicConfig", "print", "time.time", "logging.critical", "logging.critical", "get_ipython().run_line_magic", "utils_.Space", "utils_.Space.matrix.toarray", "logging.critical", "get_ipython().run_line_magic", "utils_.Space", "utils_.Space.matrix.toarray", "LSC_W2V.getCOS", "LSC_W2V.getAPD", "logging.critical", "numpy.concatenate", "utils_.Space", "utils_.Space.save", "get_ipython().run_line_magic", "len", "range", "range", "set", "clusterScoreBinary.append", "scipy.spatial.distance.jensenshannon", "print", "print", "print", "print", "print", "print", "print", "print", "print", "print", "print", "print", "print", "print", "print", "print", "print", "print", "print", "print", "logging.critical", "logging.critical", "print", "len", "cosineDistanceBinary.append", "cosineDistanceBinary.append", "APDBinary.append", "APDBinary.append", "open", "file.readlines", "len", "labelA_1.append", "labelA_2.append", "len", "len", "os.path.splitext", "os.path.splitext", "open", "csv.writer", "csv.writer.writerows", "get_ipython", "get_ipython", "get_ipython", "len", "labelA_1.count", "labelA_2.count", "numpy.histogram", "numpy.histogram", "os.path.basename", "os.path.basename", "labels.append", "labelA_2.count", "labelA_1.count", "time.time", "int"], "function", ["home.repos.pwc.inspect_result.Garrafao_TokenChange.SemanticChangeDetection.LSC_W2V.getCOS", "home.repos.pwc.inspect_result.Garrafao_TokenChange.SemanticChangeDetection.LSC_W2V.getAPD"], ["def", "main", "(", ")", ":", "\n", "\n", "# Get the arguments", "\n", "    ", "args", "=", "docopt", "(", "\"\"\"\n\n    Usage:\n        LSC_W2V.py  <pathSentences1> <pathSentences2> <outPathVectors> <outPathLabels> <outPathResults> <sentenceType> <clusteringInitialization> <clustering> <limitAGL> <limitCOS> <limitCluster> <windowSize>\n        LSC_W2V.py  <pathSentences1> <pathSentences2> <sentenceType> <clusteringInitialization> <clustering> <limitAGL> <limitCOS> <limitCluster> <windowSize>  \n    \n    Arguments:\n       \n        <pathSentences1> = Path to the test sentences from time1\n        <pathSentences2> = Path to the test sentences from time2\n        <outPathVectors> = Path to store the vectors\n        <outPathLabels> = Path to store the clustering labels\n        <outPathResults> = Path to store the lsc scores\n        <sentenceType> = \"lemma\" or \"token\"\n        <clusteringInitialization> = \"gaac\" for precalculated initializations, else random\n        <clustering> = \"kmeans\" or \"hierarchical\"\n        <limitAGL> = Change score limit for AGL to still be consiered as change (Good is about 0.2)\n        <limitCOS> = Change score limit for Cosine to still be consiered as change (Good is about 0.02) \n        <limitCluster> = Minimum number of elements a cluster has to contain from one time and less from the other, to get assigned a change (Good is 5-10)\n        <windowSize> = Window size for words to be in context of other words (Good is 20)\n        \n\n\n    \"\"\"", ")", "\n", "\n", "pathSentences1", "=", "args", "[", "'<pathSentences1>'", "]", "\n", "pathSentences2", "=", "args", "[", "'<pathSentences2>'", "]", "\n", "outPathVectors", "=", "args", "[", "'<outPathVectors>'", "]", "\n", "outPathLabels", "=", "args", "[", "'<outPathLabels>'", "]", "\n", "clusteringInitialization", "=", "args", "[", "'<clusteringInitialization>'", "]", "\n", "clustering", "=", "args", "[", "'<clustering>'", "]", "\n", "pathResults", "=", "args", "[", "'<outPathResults>'", "]", "\n", "limitAGL", "=", "float", "(", "args", "[", "'<limitAGL>'", "]", ")", "\n", "limitCOS", "=", "float", "(", "args", "[", "'<limitCOS>'", "]", ")", "\n", "limitCluster", "=", "int", "(", "args", "[", "'<limitCluster>'", "]", ")", "\n", "windowSize", "=", "int", "(", "args", "[", "'<windowSize>'", "]", ")", "\n", "sentenceType", "=", "args", "[", "'<sentenceType>'", "]", "\n", "\n", "if", "len", "(", "sys", ".", "argv", ")", "==", "10", ":", "\n", "        ", "outPathVectors", "=", "\"Files/Vectors/SecondOrder/Vectors.npz\"", "\n", "outPathLabels", "=", "\"Files/Clustering/cluster_labels.csv\"", "\n", "pathResults", "=", "\"Files/LSC/lsc_scores.csv\"", "\n", "\n", "\n", "\n", "\n", "", "logging", ".", "basicConfig", "(", "format", "=", "'%(asctime)s : %(levelname)s : %(message)s'", ",", "level", "=", "logging", ".", "CRITICAL", ")", "\n", "print", "(", "\"\"", ")", "\n", "start_time", "=", "time", ".", "time", "(", ")", "\n", "logging", ".", "critical", "(", "\"W2v LSC start\"", ")", "\n", "\n", "\n", "#Create the vectors of corpora 1", "\n", "logging", ".", "critical", "(", "\"Create the vectors of corpora 1\"", ")", "\n", "get_ipython", "(", ")", ".", "run_line_magic", "(", "'run'", ",", "'WordSenseClustering/W2v.py $pathSentences1 $outPathVectors $windowSize $sentenceType'", ")", "\n", "\n", "inSpace", "=", "Space", "(", "path", "=", "outPathVectors", ")", "\n", "vectors1", "=", "inSpace", ".", "matrix", ".", "toarray", "(", ")", "\n", "\n", "#Createthe vectors of corpora 2", "\n", "logging", ".", "critical", "(", "\"Create the vectors of corpora 2\"", ")", "\n", "get_ipython", "(", ")", ".", "run_line_magic", "(", "'run'", ",", "'WordSenseClustering/W2v.py $pathSentences2 $outPathVectors $windowSize $sentenceType'", ")", "\n", "inSpace", "=", "Space", "(", "path", "=", "outPathVectors", ")", "\n", "vectors2", "=", "inSpace", ".", "matrix", ".", "toarray", "(", ")", "\n", "\n", "#Create the lists to store the binary results in ", "\n", "cosineDistanceBinary", "=", "[", "]", "\n", "APDBinary", "=", "[", "]", "\n", "clusterScoreBinary", "=", "[", "]", "\n", "\n", "#Calculate cosineDistance for the two vectors", "\n", "cosineDistance", "=", "getCOS", "(", "vectors1", ",", "vectors2", ")", "\n", "if", "cosineDistance", ">=", "limitCOS", ":", "\n", "        ", "cosineDistanceBinary", ".", "append", "(", "1", ")", "\n", "", "else", ":", "\n", "        ", "cosineDistanceBinary", ".", "append", "(", "0", ")", "\n", "\n", "#Calculate Average pairwise distance for the two vectors", "\n", "", "APD", "=", "getAPD", "(", "vectors1", ",", "vectors2", ",", "200", ")", "\n", "if", "APD", ">=", "limitAGL", ":", "\n", "        ", "APDBinary", ".", "append", "(", "1", ")", "\n", "", "else", ":", "\n", "        ", "APDBinary", ".", "append", "(", "0", ")", "\n", "\n", "#Create and cluster the combined vectors of both corpora", "\n", "", "logging", ".", "critical", "(", "\"Create and cluster the combined vectors of both corpora\"", ")", "\n", "vectors", "=", "np", ".", "concatenate", "(", "(", "vectors1", ",", "vectors2", ")", ",", "axis", "=", "0", ")", "\n", "outSpace", "=", "Space", "(", "matrix", "=", "vectors", ",", "rows", "=", "\" \"", ",", "columns", "=", "\" \"", ")", "\n", "outSpace", ".", "save", "(", "outPathVectors", ")", "\n", "#Cluster the combined vectors", "\n", "get_ipython", "(", ")", ".", "run_line_magic", "(", "'run'", ",", "'WordSenseClustering/Clustering.py $outPathVectors 0 $outPathLabels 0 $clusteringInitialization 0 $clustering'", ")", "\n", "\n", "\n", "#Load list of labels", "\n", "labels", "=", "[", "]", "\n", "with", "open", "(", "outPathLabels", ",", "'r'", ")", "as", "file", ":", "\n", "        ", "data", "=", "file", ".", "readlines", "(", ")", "\n", "", "for", "i", "in", "data", "[", "-", "1", "]", ":", "\n", "        ", "if", "i", "!=", "\",\"", ":", "\n", "            ", "if", "i", "!=", "\"\\n\"", ":", "\n", "                ", "labels", ".", "append", "(", "int", "(", "i", ")", ")", "\n", "\n", "# Calculated cluster LSC score", "\n", "", "", "", "labelA_1", "=", "[", "]", "\n", "labelA_2", "=", "[", "]", "\n", "\n", "maximum", "=", "len", "(", "vectors1", ")", "\n", "for", "i", "in", "range", "(", "0", ",", "len", "(", "vectors1", ")", ")", ":", "\n", "        ", "labelA_1", ".", "append", "(", "labels", "[", "i", "]", ")", "\n", "\n", "", "for", "i", "in", "range", "(", "maximum", ",", "maximum", "+", "len", "(", "vectors2", ")", ")", ":", "\n", "        ", "labelA_2", ".", "append", "(", "labels", "[", "i", "]", ")", "\n", "\n", "", "changeA", "=", "0", "\n", "for", "j", "in", "set", "(", "labels", ")", ":", "\n", "        ", "if", "labelA_1", ".", "count", "(", "j", ")", ">=", "limitCluster", ":", "\n", "            ", "if", "labelA_2", ".", "count", "(", "j", ")", "<", "limitCluster", ":", "\n", "                ", "changeA", "=", "1", "\n", "", "", "if", "labelA_2", ".", "count", "(", "j", ")", ">=", "limitCluster", ":", "\n", "            ", "if", "labelA_1", ".", "count", "(", "j", ")", "<", "limitCluster", ":", "\n", "                ", "changeA", "=", "1", "\n", "\n", "", "", "", "clusterScoreBinary", ".", "append", "(", "changeA", ")", "\n", "\n", "p", "=", "np", ".", "histogram", "(", "labelA_1", ")", "[", "0", "]", "/", "len", "(", "labelA_1", ")", "\n", "q", "=", "np", ".", "histogram", "(", "labelA_2", ")", "[", "0", "]", "/", "len", "(", "labelA_2", ")", "\n", "\n", "dist", "=", "distance", ".", "jensenshannon", "(", "p", ",", "q", ")", "\n", "\n", "filename1", "=", "os", ".", "path", ".", "splitext", "(", "os", ".", "path", ".", "basename", "(", "pathSentences1", ")", ")", "[", "0", "]", "\n", "filename2", "=", "os", ".", "path", ".", "splitext", "(", "os", ".", "path", ".", "basename", "(", "pathSentences2", ")", ")", "[", "0", "]", "\n", "\n", "cos", "=", "[", "filename1", ",", "filename2", ",", "\"cosineDistance\"", ",", "cosineDistance", "]", "\n", "apd", "=", "[", "filename1", ",", "filename2", ",", "\"APD\"", ",", "APD", "]", "\n", "cluster", "=", "[", "filename1", ",", "filename2", ",", "\"clusterScore\"", ",", "dist", "]", "\n", "cosBin", "=", "[", "filename1", ",", "filename2", ",", "\"cosineDistanceBinary\"", ",", "cosineDistanceBinary", "[", "0", "]", "]", "\n", "APDBin", "=", "[", "filename1", ",", "filename2", ",", "\"APDBinary\"", ",", "APDBinary", "[", "0", "]", "]", "\n", "clusterBin", "=", "[", "filename1", ",", "filename2", ",", "\"clusterScoreBinary\"", ",", "clusterScoreBinary", "[", "0", "]", "]", "\n", "\n", "print", "(", "\"Graded LSC:\"", ")", "\n", "print", "(", "\"\"", ")", "\n", "print", "(", "\"cosine distance:\"", ")", "\n", "print", "(", "cosineDistance", ")", "\n", "print", "(", "\"\"", ")", "\n", "print", "(", "\"Average pairwise distance:\"", ")", "\n", "print", "(", "APD", ")", "\n", "print", "(", "\"\"", ")", "\n", "print", "(", "\"JSD:\"", ")", "\n", "print", "(", "dist", ")", "\n", "print", "(", "\"\"", ")", "\n", "print", "(", "\"\"", ")", "\n", "print", "(", "\"Binary LSC:\"", ")", "\n", "print", "(", "\"\"", ")", "\n", "print", "(", "\"cosine distance binary:\"", ")", "\n", "print", "(", "cosineDistanceBinary", "[", "0", "]", ")", "\n", "print", "(", "\"APD distance binary:\"", ")", "\n", "print", "(", "APDBinary", "[", "0", "]", ")", "\n", "print", "(", "\"JSD binary:\"", ")", "\n", "print", "(", "clusterScoreBinary", "[", "0", "]", ")", "\n", "\n", "with", "open", "(", "pathResults", ",", "'a'", ",", "newline", "=", "''", ")", "as", "file", ":", "\n", "        ", "writer", "=", "csv", ".", "writer", "(", "file", ")", "\n", "writer", ".", "writerows", "(", "[", "cos", ",", "apd", ",", "cluster", ",", "cosBin", ",", "APDBin", ",", "clusterBin", "]", ")", "\n", "\n", "", "logging", ".", "critical", "(", "\"W2v LSC end\"", ")", "\n", "logging", ".", "critical", "(", "\"--- %s seconds ---\"", "%", "(", "time", ".", "time", "(", ")", "-", "start_time", ")", ")", "\n", "print", "(", "\"\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Garrafao_TokenChange.SemanticChangeDetection.LSC_W2V.getCOS": [[192, 197], ["scipy.spatial.distance.cosine", "numpy.sum", "len", "numpy.sum", "len"], "function", ["None"], ["", "def", "getCOS", "(", "vec1", ",", "vec2", ")", ":", "\n", "    ", "sum1", "=", "np", ".", "sum", "(", "vec1", ",", "axis", "=", "0", ")", "/", "len", "(", "vec1", ")", "\n", "sum2", "=", "np", ".", "sum", "(", "vec2", ",", "axis", "=", "0", ")", "/", "len", "(", "vec2", ")", "\n", "result", "=", "distance", ".", "cosine", "(", "sum1", ",", "sum2", ")", "\n", "return", "result", "\n", "\n"]], "home.repos.pwc.inspect_result.Garrafao_TokenChange.SemanticChangeDetection.LSC_W2V.getAPD": [[200, 218], ["min", "random.sample", "range", "len", "len", "range", "testList1.append", "testList2.append", "range", "min", "len", "len", "scipy.spatial.distance.cosine"], "function", ["None"], ["", "def", "getAPD", "(", "vec1", ",", "vec2", ",", "size", ")", ":", "\n", "\n", "    ", "testList1", "=", "[", "]", "\n", "testList2", "=", "[", "]", "\n", "result", "=", "0", "\n", "size", "=", "min", "(", "len", "(", "vec1", ")", ",", "len", "(", "vec2", ")", ",", "size", ")", "\n", "randoms", "=", "random", ".", "sample", "(", "range", "(", "0", ",", "min", "(", "len", "(", "vec1", ")", ",", "len", "(", "vec2", ")", ")", ")", ",", "size", ")", "\n", "for", "i", "in", "randoms", ":", "\n", "        ", "testList1", ".", "append", "(", "vec1", "[", "i", "]", ")", "\n", "testList2", ".", "append", "(", "vec2", "[", "i", "]", ")", "\n", "\n", "", "for", "i", "in", "range", "(", "0", ",", "size", ")", ":", "\n", "        ", "for", "j", "in", "range", "(", "0", ",", "size", ")", ":", "\n", "            ", "result", "=", "result", "+", "distance", ".", "cosine", "(", "testList1", "[", "i", "]", ",", "testList2", "[", "j", "]", ")", "\n", "", "", "if", "result", "!=", "0", ":", "\n", "        ", "result", "=", "result", "/", "(", "size", "*", "size", ")", "\n", "\n", "", "return", "result", "\n", "\n"]], "home.repos.pwc.inspect_result.Garrafao_TokenChange.SemanticChangeDetection.LSC_W2V.jensen_shannon_distance": [[220, 240], ["numpy.array", "numpy.array", "numpy.sqrt", "scipy.stats.entropy", "scipy.stats.entropy"], "function", ["None"], ["", "def", "jensen_shannon_distance", "(", "p", ",", "q", ")", ":", "\n", "    ", "\"\"\"\n    method to compute the Jenson-Shannon Distance \n    between two probability distributions\n    \"\"\"", "\n", "\n", "# convert the vectors into numpy arrays in case that they aren't", "\n", "p", "=", "np", ".", "array", "(", "p", ")", "\n", "q", "=", "np", ".", "array", "(", "q", ")", "\n", "\n", "# calculate m", "\n", "m", "=", "(", "p", "+", "q", ")", "/", "2", "\n", "\n", "# compute Jensen Shannon Divergence", "\n", "divergence", "=", "(", "scipy", ".", "stats", ".", "entropy", "(", "p", ",", "m", ")", "+", "scipy", ".", "stats", ".", "entropy", "(", "q", ",", "m", ")", ")", "/", "2", "\n", "\n", "# compute the Jensen Shannon Distance", "\n", "distance", "=", "np", ".", "sqrt", "(", "divergence", ")", "\n", "\n", "return", "distance", "\n", "\n"]], "home.repos.pwc.inspect_result.Garrafao_TokenChange.WordSenseClustering.WordVectors.main": [[18, 114], ["docopt.docopt", "logging.basicConfig", "print", "time.time", "logging.critical", "logging.info", "range", "logging.critical", "get_ipython().run_line_magic", "numpy.save", "logging.critical", "logging.critical", "print", "len", "gzip.open", "set", "sorted", "len", "gzip.open", "w2i.items", "get_ipython().run_line_magic", "get_ipython().run_line_magic", "get_ipython().run_line_magic", "list", "enumerate", "math.log10", "get_ipython", "sentence.split", "set", "get_ipython", "get_ipython", "get_ipython", "time.time", "set.add", "sentence.split", "max"], "function", ["None"], ["def", "main", "(", ")", ":", "\n", "\n", "# Get the arguments", "\n", "    ", "args", "=", "docopt", "(", "\"\"\"\n\n    Usage:\n        WordVectors.py <pathCorpus> <outPathVectors> <outPathw2i> <representation> \n        WordVectors.py <pathCorpus> <representation>\n        \n        \n    Arguments:\n       \n        \n        <pathCorpus> = Path to the corpus\n        <outPathVectors> = Path for storing the vectors \n        <outPathw2i> = Path for storing w2i\n        <representation> = Either \"count\", \"ppmi\" or \"svd\"\n    \"\"\"", ")", "\n", "\n", "\n", "pathCorpus", "=", "args", "[", "'<pathCorpus>'", "]", "\n", "outPathVectors", "=", "args", "[", "'<outPathVectors>'", "]", "\n", "outPathw2i", "=", "args", "[", "'<outPathw2i>'", "]", "\n", "representation", "=", "args", "[", "'<representation>'", "]", "\n", "\n", "\n", "\n", "if", "len", "(", "sys", ".", "argv", ")", "==", "3", ":", "\n", "        ", "outPathVectors", "=", "\"Files/Vectors/FirstOrder/matrix.npz\"", "\n", "outPathw2i", "=", "\"Files/Vectors/FirstOrder/w2i.npz.npy\"", "\n", "\n", "\n", "", "logging", ".", "basicConfig", "(", "format", "=", "'%(asctime)s : %(levelname)s : %(message)s'", ",", "level", "=", "logging", ".", "CRITICAL", ")", "\n", "print", "(", "\"\"", ")", "\n", "start_time", "=", "time", ".", "time", "(", ")", "\n", "logging", ".", "critical", "(", "\"WordVectors start\"", ")", "\n", "\n", "\n", "#Create w2i", "\n", "logging", ".", "info", "(", "\"Create w2i\"", ")", "\n", "count", "=", "0", "\n", "with", "gzip", ".", "open", "(", "pathCorpus", ",", "'rt'", ",", "encoding", "=", "\"utf-8\"", ")", "as", "sentences", ":", "\n", "        ", "setWV", "=", "set", "(", ")", "\n", "listWV", "=", "[", "]", "\n", "try", ":", "\n", "            ", "for", "sentence", "in", "sentences", ":", "\n", "                ", "count", "+=", "1", "\n", "for", "word", "in", "sentence", ".", "split", "(", ")", ":", "\n", "                    ", "setWV", ".", "add", "(", "word", ")", "\n", "", "", "", "except", ":", "\n", "            ", "pass", "\n", "", "vocabulary", "=", "sorted", "(", "list", "(", "setWV", ")", ")", "\n", "\n", "", "w2i", "=", "{", "w", ":", "i", "for", "i", ",", "w", "in", "enumerate", "(", "vocabulary", ")", "}", "\n", "\n", "\n", "\n", "#Calculate IDF for every word ", "\n", "docFreq", "=", "{", "}", "\n", "for", "i", "in", "range", "(", "0", ",", "len", "(", "w2i", ")", ")", ":", "\n", "        ", "docFreq", "[", "i", "]", "=", "0", "\n", "", "with", "gzip", ".", "open", "(", "pathCorpus", ",", "'rt'", ",", "encoding", "=", "\"utf-8\"", ")", "as", "sentences", ":", "\n", "        ", "count", "=", "0", "\n", "try", ":", "\n", "            ", "for", "sentence", "in", "sentences", ":", "\n", "                ", "count", "=", "count", "+", "1", "\n", "for", "word", "in", "set", "(", "sentence", ".", "split", "(", ")", ")", ":", "\n", "                    ", "if", "word", "in", "w2i", ":", "\n", "                        ", "docFreq", "[", "w2i", "[", "word", "]", "]", "+=", "1", "\n", "", "", "", "", "except", ":", "\n", "            ", "pass", "\n", "", "for", "key", ",", "value", "in", "w2i", ".", "items", "(", ")", ":", "\n", "            ", "docFreq", "[", "value", "]", "=", "math", ".", "log10", "(", "count", "/", "max", "(", "docFreq", "[", "value", "]", ",", "1", ")", ")", "\n", "\n", "\n", "#Create co-occurence matrix", "\n", "", "", "logging", ".", "critical", "(", "\"Create co-occurence matrix\"", ")", "\n", "get_ipython", "(", ")", ".", "run_line_magic", "(", "'run'", ",", "'count.py --len $pathCorpus $outPathVectors 20'", ")", "\n", "\n", "if", "representation", "==", "\"ppmi\"", ":", "\n", "#Apply PPMI        ", "\n", "        ", "get_ipython", "(", ")", ".", "run_line_magic", "(", "'run'", ",", "'ppmi.py --len $outPathVectors $outPathVectors 1 1'", ")", "\n", "\n", "", "if", "representation", "==", "\"svd\"", ":", "\n", "#Apply PPMI ", "\n", "        ", "get_ipython", "(", ")", ".", "run_line_magic", "(", "'run'", ",", "'ppmi.py --len $outPathVectors $outPathVectors 1 1'", ")", "\n", "#Apply SVD", "\n", "get_ipython", "(", ")", ".", "run_line_magic", "(", "'run'", ",", "'svd.py --len $outPathVectors $outPathVectors 100 0'", ")", "\n", "\n", "\n", "#Save w2i", "\n", "", "np", ".", "save", "(", "outPathw2i", ",", "w2i", ")", "\n", "\n", "logging", ".", "critical", "(", "\"WordVectors end\"", ")", "\n", "logging", ".", "critical", "(", "\"--- %s seconds ---\"", "%", "(", "time", ".", "time", "(", ")", "-", "start_time", ")", ")", "\n", "print", "(", "\"\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Garrafao_TokenChange.WordSenseClustering.Bert.main": [[19, 128], ["docopt.docopt", "logging.basicConfig", "print", "time.time", "logging.critical", "transformers.BertTokenizer.from_pretrained", "transformers.BertModel.from_pretrained", "sklearn.preprocessing.normalize", "utils_.Space", "utils_.Space.save", "logging.critical", "logging.critical", "print", "len", "open", "csv.DictReader", "logging.critical", "range", "testSentences.append", "len", "str", "targetWords.append", "BertTokenizer.from_pretrained.tokenize", "range", "BertTokenizer.from_pretrained.convert_tokens_to_ids", "torch.tensor", "torch.tensor", "BertModel.from_pretrained.eval", "torch.stack", "torch.squeeze", "token_embeddings.permute.permute", "preprocessing.normalize.append", "dict", "BertTokenizer.from_pretrained.tokenize", "len", "len", "torch.no_grad", "BertModel.from_pretrained.", "numpy.sum", "vectors.append", "numpy.sum", "time.time", "[].split", "range", "numpy.array", "int", "len", "numpy.array", "numpy.array", "targetWordIndices.append", "len", "len"], "function", ["None"], ["def", "main", "(", ")", ":", "\n", "\n", "# Get the arguments", "\n", "    ", "args", "=", "docopt", "(", "\"\"\"\n\n    Usage:\n        Bert.py  <pathTestSentences> <outPathVectors> <vecType> \n        Bert.py  <pathTestSentences> <vecType>\n        \n    Arguments:\n       \n        <pathTestSentences> = Path to the test sentences\n        <outPathVectors> = Path for storing the vectors\n        <vecType> = \"token\" or \"lemma\"\n\n    \"\"\"", ")", "\n", "\n", "pathTestSentences", "=", "args", "[", "'<pathTestSentences>'", "]", "\n", "outPathVectors", "=", "args", "[", "'<outPathVectors>'", "]", "\n", "vecType", "=", "args", "[", "'<vecType>'", "]", "\n", "\n", "\n", "if", "len", "(", "sys", ".", "argv", ")", "==", "3", ":", "\n", "        ", "outPathVectors", "=", "\"Files/Vectors/SecondOrder/Vectors.npz\"", "\n", "\n", "\n", "", "logging", ".", "basicConfig", "(", "format", "=", "'%(asctime)s : %(levelname)s : %(message)s'", ",", "level", "=", "logging", ".", "CRITICAL", ")", "\n", "print", "(", "\"\"", ")", "\n", "start_time", "=", "time", ".", "time", "(", ")", "\n", "logging", ".", "critical", "(", "\"Bert start\"", ")", "\n", "\n", "\n", "#Load TestSentences ", "\n", "# Load pre-trained model tokenizer (vocabulary)", "\n", "global", "tokenizer", "\n", "tokenizer", "=", "BertTokenizer", ".", "from_pretrained", "(", "'bert-base-uncased'", ")", "\n", "# Load pre-trained model (weights)", "\n", "global", "model", "\n", "model", "=", "BertModel", ".", "from_pretrained", "(", "'bert-base-uncased'", ",", "\n", "output_hidden_states", "=", "True", ")", "\n", "\n", "\n", "contextVectorList", "=", "[", "]", "\n", "testSentences", "=", "[", "]", "\n", "with", "open", "(", "pathTestSentences", ",", "'r'", ")", "as", "csvFile", ":", "\n", "        ", "reader", "=", "csv", ".", "DictReader", "(", "csvFile", ",", "delimiter", "=", "\"\\t\"", ")", "\n", "for", "row", "in", "reader", ":", "\n", "            ", "testSentences", ".", "append", "(", "dict", "(", "row", ")", ")", "\n", "\n", "#Token vs. Lemma", "\n", "", "if", "vecType", "==", "\"token\"", ":", "\n", "            ", "vecTypeString", "=", "\"sentence_token\"", "\n", "", "else", ":", "\n", "            ", "vecTypeString", "=", "\"sentence\"", "\n", "\n", "#Create the vectors  ", "\n", "", "logging", ".", "critical", "(", "\"Create Bert embeddings\"", ")", "\n", "for", "i", "in", "range", "(", "0", ",", "len", "(", "testSentences", ")", ")", ":", "\n", "#Create target word(s)", "\n", "            ", "targetWord", "=", "str", "(", "testSentences", "[", "i", "]", "[", "vecTypeString", "]", ".", "split", "(", ")", "[", "int", "(", "[", "testSentences", "[", "i", "]", "[", "\"target_index\"", "]", "]", "[", "0", "]", ")", "]", ")", "\n", "targetWords", "=", "[", "]", "\n", "targetWords", ".", "append", "(", "tokenizer", ".", "tokenize", "(", "targetWord", ")", ")", "\n", "targetWords", "=", "targetWords", "[", "0", "]", "\n", "\n", "#Tokenize text", "\n", "text", "=", "testSentences", "[", "i", "]", "[", "vecTypeString", "]", "\n", "marked_text", "=", "\"[CLS] \"", "+", "text", "+", "\" [SEP]\"", "\n", "tokenized_text", "=", "tokenizer", ".", "tokenize", "(", "marked_text", ")", "\n", "\n", "#Search the indices of the tokenized target word in the tokenized text", "\n", "targetWordIndices", "=", "[", "]", "\n", "\n", "for", "i", "in", "range", "(", "0", ",", "len", "(", "tokenized_text", ")", ")", ":", "\n", "                ", "if", "tokenized_text", "[", "i", "]", "==", "targetWords", "[", "0", "]", ":", "\n", "                    ", "for", "l", "in", "range", "(", "0", ",", "len", "(", "targetWords", ")", ")", ":", "\n", "                        ", "if", "tokenized_text", "[", "i", "+", "l", "]", "==", "targetWords", "[", "l", "]", ":", "\n", "                            ", "targetWordIndices", ".", "append", "(", "i", "+", "l", ")", "\n", "", "if", "len", "(", "targetWordIndices", ")", "==", "len", "(", "targetWords", ")", ":", "\n", "                            ", "break", "\n", "\n", "#Create BERT Token Embeddings        ", "\n", "", "", "", "", "indexed_tokens", "=", "tokenizer", ".", "convert_tokens_to_ids", "(", "tokenized_text", ")", "\n", "segments_ids", "=", "[", "1", "]", "*", "len", "(", "tokenized_text", ")", "\n", "tokens_tensor", "=", "torch", ".", "tensor", "(", "[", "indexed_tokens", "]", ")", "\n", "segments_tensors", "=", "torch", ".", "tensor", "(", "[", "segments_ids", "]", ")", "\n", "model", ".", "eval", "(", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                ", "outputs", "=", "model", "(", "tokens_tensor", ",", "segments_tensors", ")", "\n", "hidden_states", "=", "outputs", "[", "2", "]", "\n", "", "token_embeddings", "=", "torch", ".", "stack", "(", "hidden_states", ",", "dim", "=", "0", ")", "\n", "token_embeddings", "=", "torch", ".", "squeeze", "(", "token_embeddings", ",", "dim", "=", "1", ")", "\n", "token_embeddings", "=", "token_embeddings", ".", "permute", "(", "1", ",", "0", ",", "2", ")", "\n", "vectors", "=", "[", "]", "\n", "for", "number", "in", "targetWordIndices", ":", "\n", "                ", "token", "=", "token_embeddings", "[", "number", "]", "\n", "sum_vec", "=", "np", ".", "sum", "(", "[", "np", ".", "array", "(", "token", "[", "12", "]", ")", ",", "np", ".", "array", "(", "token", "[", "1", "]", ")", "]", ",", "axis", "=", "0", ")", "\n", "vectors", ".", "append", "(", "np", ".", "array", "(", "sum_vec", ")", ")", "\n", "", "contextVectorList", ".", "append", "(", "np", ".", "sum", "(", "vectors", ",", "axis", "=", "0", ",", "dtype", "=", "float", ")", ")", "\n", "\n", "#Normalize vectors in length", "\n", "", "", "contextVectorList", "=", "preprocessing", ".", "normalize", "(", "contextVectorList", ",", "norm", "=", "'l2'", ")", "\n", "\n", "#Save contextVectorList_sparse matrix", "\n", "outSpace", "=", "Space", "(", "matrix", "=", "contextVectorList", ",", "rows", "=", "\" \"", ",", "columns", "=", "\" \"", ")", "\n", "outSpace", ".", "save", "(", "outPathVectors", ")", "\n", "\n", "logging", ".", "critical", "(", "\"Bert end\"", ")", "\n", "logging", ".", "critical", "(", "\"--- %s seconds ---\"", "%", "(", "time", ".", "time", "(", ")", "-", "start_time", ")", ")", "\n", "print", "(", "\"\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Garrafao_TokenChange.WordSenseClustering.Bert.getContextVector": [[131, 134], ["centroid", "sum", "zip"], "function", ["None"], ["", "def", "getContextVector", "(", "toMeltList", ")", ":", "\n", "    ", "centroid", "=", "lambda", "inp", ":", "[", "[", "sum", "(", "m", ")", "for", "m", "in", "zip", "(", "*", "l", ")", "]", "for", "l", "in", "inp", "]", "\n", "return", "centroid", "(", "[", "toMeltList", "]", ")", "[", "0", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.Garrafao_TokenChange.WordSenseClustering.Clustering.main": [[27, 153], ["docopt.docopt", "int", "logging.basicConfig", "print", "time.time", "logging.critical", "utils_.Space", "logging.critical", "logging.critical", "print", "len", "sklearn.cluster.AgglomerativeClustering().fit", "print", "print", "print", "print", "print", "print", "print", "print", "open", "csv.writer", "csv.writer.writerows", "open", "csv.DictReader", "gold.append", "sklearn.cluster.KMeans", "sklearn.cluster.KMeans.fit_predict", "sklearn.metrics.silhouette_score", "loaded_contextVectorList_sparse.toarray", "min", "random.sample", "sklearn.preprocessing.normalize", "scipy.cluster.vq.kmeans2", "scipy.cluster.vq.kmeans2", "os.path.splitext", "round", "Clustering.cluster_accuracy", "open", "csv.writer", "csv.writer.writerows", "round", "Clustering.cluster_accuracy", "testSentences.append", "int", "loaded_contextVectorList_sparse.toarray", "loaded_contextVectorList_sparse.toarray", "sklearn.cluster.AgglomerativeClustering", "len", "range", "testList.append", "Clustering.gaac", "loaded_contextVectorList_sparse.toarray", "loaded_contextVectorList_sparse.toarray", "os.path.basename", "sklearn.metrics.cluster.adjusted_rand_score", "numpy.array", "numpy.array", "sklearn.metrics.cluster.adjusted_rand_score", "numpy.array", "numpy.array", "time.time", "dict", "loaded_contextVectorList_sparse.toarray", "len", "loaded_contextVectorList_sparse.toarray", "loaded_contextVectorList_sparse[].toarray"], "function", ["home.repos.pwc.inspect_result.Garrafao_TokenChange.WordSenseClustering.Clustering.cluster_accuracy", "home.repos.pwc.inspect_result.Garrafao_TokenChange.WordSenseClustering.Clustering.cluster_accuracy", "home.repos.pwc.inspect_result.Garrafao_TokenChange.WordSenseClustering.Clustering.gaac"], ["def", "main", "(", ")", ":", "\n", "\n", "# Get the arguments", "\n", "    ", "args", "=", "docopt", "(", "\"\"\"\n\n    Usage:\n        Clustering.py  <pathVectors> <pathTestSentences> <outPathLabels> <outPathResults> <initializationType> <numberClusters> <clustering> \n        Clustering.py  <pathTestSentences> <initializationType> <numberClusters> <clustering>\n        \n    Arguments:\n       \n        <pathVectors> = Path to the vectors\n        <pathTestSentences> = Path to the test sentecens that contain the gold clustering, if no performance is needed set to 0\n        <outPathLabels> = Path to store the labels\n        <outPathResults> = path to store the performance in, if no performance is needed set to 0 \n        <initializationType> = \"gaac\" for precalculated initialization, else random. (Only for kmeans used)\n        <numberClusters> = Number of desired clusters, if 0 than its calculated by sillhouette\n        <clustering> = Either \"hierarchical\" or \"kmeans\"\n\n    \n    \"\"\"", ")", "\n", "\n", "pathVectors", "=", "args", "[", "'<pathVectors>'", "]", "\n", "pathTestSentences", "=", "args", "[", "'<pathTestSentences>'", "]", "\n", "initializationType", "=", "args", "[", "'<initializationType>'", "]", "\n", "numberClusters", "=", "int", "(", "args", "[", "'<numberClusters>'", "]", ")", "\n", "outPathLabels", "=", "args", "[", "'<outPathLabels>'", "]", "\n", "outPathResults", "=", "args", "[", "'<outPathResults>'", "]", "\n", "clustering", "=", "args", "[", "'<clustering>'", "]", "\n", "\n", "\n", "if", "len", "(", "sys", ".", "argv", ")", "==", "5", ":", "\n", "        ", "pathVectors", "=", "\"Files/Vectors/SecondOrder/Vectors.npz\"", "\n", "outPathLabels", "=", "\"Files/Clustering/cluster_labels.csv\"", "\n", "outPathResults", "=", "\"Files/Clustering/cluster_scores.csv\"", "\n", "\n", "", "logging", ".", "basicConfig", "(", "format", "=", "'%(asctime)s : %(levelname)s : %(message)s'", ",", "level", "=", "logging", ".", "CRITICAL", ")", "\n", "print", "(", "\"\"", ")", "\n", "start_time", "=", "time", ".", "time", "(", ")", "\n", "logging", ".", "critical", "(", "\"Clustering start\"", ")", "\n", "\n", "#Load vectors", "\n", "inSpace", "=", "Space", "(", "path", "=", "pathVectors", ")", "\n", "loaded_contextVectorList_sparse", "=", "inSpace", ".", "matrix", "\n", "\n", "\n", "if", "pathTestSentences", "!=", "\"0\"", ":", "\n", "#Get gold clustering if exists", "\n", "        ", "testSentences", "=", "[", "]", "\n", "gold", "=", "[", "]", "\n", "with", "open", "(", "pathTestSentences", ",", "'r'", ")", "as", "csvFile", ":", "\n", "            ", "reader", "=", "csv", ".", "DictReader", "(", "csvFile", ",", "delimiter", "=", "\"\\t\"", ")", "\n", "for", "row", "in", "reader", ":", "\n", "                ", "testSentences", ".", "append", "(", "dict", "(", "row", ")", ")", "\n", "", "", "for", "dic", "in", "testSentences", ":", "\n", "                ", "gold", ".", "append", "(", "int", "(", "dic", "[", "'cluster'", "]", ")", ")", "\n", "\n", "", "", "if", "numberClusters", "==", "0", ":", "\n", "#Calculate silhouette score for eaach number of clusters", "\n", "        ", "range_n_clusters", "=", "[", "2", ",", "3", ",", "4", ",", "5", ",", "6", ",", "7", ",", "8", ",", "9", ",", "10", "]", "\n", "maxIndex", "=", "0", "\n", "maxValue", "=", "0", "\n", "for", "n_clusters", "in", "range_n_clusters", ":", "\n", "            ", "clusterer", "=", "KMeans", "(", "n_clusters", "=", "n_clusters", ",", "random_state", "=", "10", ")", "\n", "cluster_labels", "=", "clusterer", ".", "fit_predict", "(", "loaded_contextVectorList_sparse", ".", "toarray", "(", ")", ")", "\n", "silhouette_avg", "=", "silhouette_score", "(", "loaded_contextVectorList_sparse", ".", "toarray", "(", ")", ",", "cluster_labels", ")", "\n", "if", "maxValue", "<=", "silhouette_avg", ":", "\n", "                ", "maxValue", "=", "silhouette_avg", "\n", "maxIndex", "=", "n_clusters", "\n", "", "numberClusters", "=", "maxIndex", "\n", "\n", "\n", "\n", "", "", "if", "clustering", "==", "\"hierarchical\"", ":", "\n", "        ", "clustering", "=", "AgglomerativeClustering", "(", "n_clusters", "=", "numberClusters", ")", ".", "fit", "(", "loaded_contextVectorList_sparse", ".", "toarray", "(", ")", ")", "\n", "label", "=", "clustering", ".", "labels_", "\n", "\n", "", "else", ":", "\n", "\n", "        ", "if", "initializationType", "==", "\"gaac\"", ":", "\n", "\n", "#Calculate GAAC on sample vectors for initial centroids", "\n", "            ", "testList", "=", "[", "]", "\n", "size", "=", "min", "(", "len", "(", "loaded_contextVectorList_sparse", ".", "toarray", "(", ")", ")", ",", "50", ")", "\n", "randoms", "=", "random", ".", "sample", "(", "range", "(", "0", ",", "len", "(", "loaded_contextVectorList_sparse", ".", "toarray", "(", ")", ")", ")", ",", "size", ")", "\n", "for", "i", "in", "randoms", ":", "\n", "                ", "testList", ".", "append", "(", "loaded_contextVectorList_sparse", "[", "i", "]", ".", "toarray", "(", ")", "[", "0", "]", ")", "\n", "", "initialCentroids", "=", "preprocessing", ".", "normalize", "(", "gaac", "(", "testList", ",", "numberClusters", ")", ",", "norm", "=", "'l2'", ")", "\n", "\n", "#Calculate kmeans    ", "\n", "centroid", ",", "label", "=", "kmeans2", "(", "loaded_contextVectorList_sparse", ".", "toarray", "(", ")", ",", "\n", "initialCentroids", ",", "5", ",", "minit", "=", "'matrix'", ")", "\n", "\n", "", "else", ":", "\n", "#Calculate kmeans    ", "\n", "            ", "centroid", ",", "label", "=", "kmeans2", "(", "loaded_contextVectorList_sparse", ".", "toarray", "(", ")", ",", "\n", "numberClusters", ",", "5", ",", "minit", "=", "'points'", ")", "\n", "\n", "", "", "if", "outPathResults", "!=", "\"0\"", ":", "\n", "        ", "filename", "=", "os", ".", "path", ".", "splitext", "(", "os", ".", "path", ".", "basename", "(", "pathTestSentences", ")", ")", "[", "0", "]", "\n", "\n", "ADJ", "=", "[", "filename", ",", "\"ADJ\"", ",", "(", "round", "(", "adjusted_rand_score", "(", "gold", ",", "label", ")", ",", "3", ")", ")", "]", "\n", "ACC", "=", "[", "filename", ",", "\"ACC\"", ",", "cluster_accuracy", "(", "np", ".", "array", "(", "gold", ")", ",", "np", ".", "array", "(", "label", ")", ")", "]", "\n", "\n", "with", "open", "(", "outPathResults", ",", "'a'", ",", "newline", "=", "''", ")", "as", "file", ":", "\n", "            ", "writer", "=", "csv", ".", "writer", "(", "file", ")", "\n", "writer", ".", "writerows", "(", "[", "ADJ", ",", "ACC", "]", ")", "\n", "\n", "#Show results ", "\n", "", "print", "(", "\"\"", ")", "\n", "print", "(", "filename", ")", "\n", "print", "(", "\"\"", ")", "\n", "print", "(", "\"Adjusted rand index:\"", ")", "\n", "print", "(", "round", "(", "adjusted_rand_score", "(", "gold", ",", "label", ")", ",", "3", ")", ")", "\n", "print", "(", "\"Accuracy:\"", ")", "\n", "print", "(", "cluster_accuracy", "(", "np", ".", "array", "(", "gold", ")", ",", "np", ".", "array", "(", "label", ")", ")", ")", "\n", "print", "(", "\"\"", ")", "\n", "#plotClusters(loaded_contextVectorList_sparse.toarray(), gold, label)                                  ", "\n", "\n", "#Save labels", "\n", "", "with", "open", "(", "outPathLabels", ",", "'a'", ",", "newline", "=", "''", ")", "as", "file", ":", "\n", "        ", "writer", "=", "csv", ".", "writer", "(", "file", ")", "\n", "writer", ".", "writerows", "(", "[", "label", "]", ")", "\n", "", "logging", ".", "critical", "(", "\"Clustering end\"", ")", "\n", "logging", ".", "critical", "(", "\"--- %s seconds ---\"", "%", "(", "time", ".", "time", "(", ")", "-", "start_time", ")", ")", "\n", "print", "(", "\"\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Garrafao_TokenChange.WordSenseClustering.Clustering.cluster_accuracy": [[156, 163], ["sklearn.metrics.cluster.contingency_matrix", "scipy.optimize.linear_sum_assignment", "contingency_matrix[].sum", "numpy.sum"], "function", ["None"], ["", "def", "cluster_accuracy", "(", "y_true", ",", "y_pred", ")", ":", "\n", "# compute confusion matrix", "\n", "    ", "contingency_matrix", "=", "metrics", ".", "cluster", ".", "contingency_matrix", "(", "y_true", ",", "y_pred", ")", "\n", "# Find best mapping between cluster labels and gold labels", "\n", "row_ind", ",", "col_ind", "=", "linear_sum_assignment", "(", "-", "contingency_matrix", ")", "\n", "#return result", "\n", "return", "contingency_matrix", "[", "row_ind", ",", "col_ind", "]", ".", "sum", "(", ")", "/", "np", ".", "sum", "(", "contingency_matrix", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Garrafao_TokenChange.WordSenseClustering.Clustering.gaac": [[166, 218], ["range", "numpy.zeros", "range", "numpy.zeros", "numpy.zeros", "len", "clusters.append", "len", "range", "len", "range", "clusters.append", "numpy.add", "numpy.add", "len", "scipy.spatial.distance.cosine", "len", "range", "len", "len", "len", "len", "len", "len", "len"], "function", ["None"], ["", "def", "gaac", "(", "vectors", ",", "limit", ")", ":", "\n", "\n", "#Put each vector(its ID) in an individual cluster", "\n", "    ", "clusters", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "0", ",", "len", "(", "vectors", ")", ")", ":", "\n", "        ", "clusters", ".", "append", "(", "[", "i", "]", ")", "\n", "\n", "#Compute pairwise distance of all pairs of vectors", "\n", "", "distances", "=", "np", ".", "zeros", "(", "shape", "=", "(", "len", "(", "vectors", ")", ",", "len", "(", "vectors", ")", ")", ")", "\n", "for", "i", "in", "range", "(", "0", ",", "len", "(", "vectors", ")", ")", ":", "\n", "        ", "for", "j", "in", "range", "(", "0", ",", "len", "(", "vectors", ")", ")", ":", "\n", "            ", "distances", "[", "i", ",", "j", "]", "=", "distance", ".", "cosine", "(", "vectors", "[", "i", "]", ",", "vectors", "[", "j", "]", ")", "\n", "\n", "#Search the two most similar clusters and melt them until number of desired clusters is reached    ", "\n", "", "", "while", "len", "(", "clusters", ")", ">", "limit", ":", "\n", "        ", "cluser0", "=", "0", "\n", "cluser1", "=", "1", "\n", "minimumCosine", "=", "10000", "\n", "\n", "#Find the two most similar clusters", "\n", "for", "i", "in", "range", "(", "0", ",", "len", "(", "clusters", ")", ")", ":", "\n", "            ", "for", "j", "in", "range", "(", "0", ",", "len", "(", "clusters", ")", ")", ":", "\n", "                ", "if", "j", ">", "i", ":", "\n", "                    ", "comparisons", "=", "0", "\n", "sumCosine", "=", "0", "\n", "newCluster", "=", "[", "]", "\n", "newCluster", "=", "clusters", "[", "i", "]", "+", "clusters", "[", "j", "]", "\n", "for", "elem", "in", "newCluster", ":", "\n", "                        ", "for", "elem2", "in", "newCluster", ":", "\n", "                            ", "if", "elem", "!=", "elem2", ":", "\n", "                                ", "sumCosine", "+=", "distances", "[", "elem", ",", "elem2", "]", "\n", "", "", "", "sim", "=", "sumCosine", "*", "(", "0.5", ")", "*", "(", "1", "/", "(", "len", "(", "newCluster", ")", "*", "(", "len", "(", "newCluster", ")", "-", "1", ")", ")", ")", "\n", "if", "sim", "<", "minimumCosine", ":", "\n", "                        ", "minimumCosine", "=", "sim", "\n", "cluser0", "=", "i", "\n", "cluser1", "=", "j", "\n", "#Melt the two found clusters ", "\n", "", "", "", "", "newClusters", "=", "[", "]", "\n", "newCluster", "=", "clusters", "[", "cluser0", "]", "+", "clusters", "[", "cluser1", "]", "\n", "del", "clusters", "[", "cluser1", "]", "\n", "del", "clusters", "[", "cluser0", "]", "\n", "clusters", ".", "append", "(", "newCluster", ")", "\n", "\n", "#Calculate the centroids", "\n", "", "centroids", "=", "[", "]", "\n", "vector0", "=", "np", ".", "zeros", "(", "shape", "=", "(", "1", ",", "len", "(", "vectors", "[", "0", "]", ")", ")", ")", "\n", "vector1", "=", "np", ".", "zeros", "(", "shape", "=", "(", "1", ",", "len", "(", "vectors", "[", "0", "]", ")", ")", ")", "\n", "for", "i", "in", "clusters", "[", "0", "]", ":", "\n", "        ", "vector0", "=", "np", ".", "add", "(", "vector0", ",", "vectors", "[", "i", "]", ")", "\n", "", "for", "i", "in", "clusters", "[", "1", "]", ":", "\n", "        ", "vector1", "=", "np", ".", "add", "(", "vector1", ",", "vectors", "[", "i", "]", ")", "\n", "", "return", "[", "vector0", "[", "0", "]", ",", "vector1", "[", "0", "]", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.Garrafao_TokenChange.WordSenseClustering.Clustering.plotClusters": [[220, 249], ["sklearn.manifold.MDS", "sklearn.manifold.MDS.fit_transform", "range", "matplotlib.figure", "matplotlib.subplot", "set", "matplotlib.title", "matplotlib.xticks", "matplotlib.yticks", "matplotlib.subplot", "set", "matplotlib.title", "matplotlib.xticks", "matplotlib.yticks", "color.append", "range", "range", "len", "len", "randint", "matplotlib.plot", "matplotlib.plot"], "function", ["None"], ["", "def", "plotClusters", "(", "toCluster", ",", "gold", ",", "actual", ")", ":", "\n", "    ", "embedding", "=", "MDS", "(", "n_components", "=", "2", ",", "metric", "=", "True", ",", "n_init", "=", "10", ",", "max_iter", "=", "500", ",", "random_state", "=", "100", ")", "\n", "X_transformed", "=", "embedding", ".", "fit_transform", "(", "toCluster", ")", "\n", "\n", "#Create list of different colors (Start with known colors)", "\n", "color", "=", "[", "\"red\"", ",", "\"blue\"", ",", "\"green\"", ",", "\"olive\"", ",", "\"orange\"", ",", "\"black\"", ",", "\"lime\"", ",", "\"deeppink\"", "]", "\n", "for", "i", "in", "range", "(", "50", ")", ":", "\n", "        ", "color", ".", "append", "(", "'#%06X'", "%", "randint", "(", "0", ",", "0xFFFFFF", ")", ")", "\n", "\n", "#Plot the expected (gold) clustering", "\n", "", "plt", ".", "figure", "(", "figsize", "=", "(", "10", ",", "20", ")", ")", "\n", "plt", ".", "subplot", "(", "211", ")", "\n", "for", "k", "in", "set", "(", "gold", ")", ":", "\n", "        ", "for", "i", "in", "range", "(", "0", ",", "len", "(", "toCluster", ")", ")", ":", "\n", "            ", "if", "gold", "[", "i", "]", "==", "k", ":", "\n", "                ", "plt", ".", "plot", "(", "X_transformed", "[", "i", ",", "0", "]", ",", "X_transformed", "[", "i", ",", "1", "]", ",", "'o'", ",", "color", "=", "color", "[", "k", "]", ",", "markersize", "=", "4", ")", "\n", "", "", "", "plt", ".", "title", "(", "'Gold Labeling'", ",", "fontsize", "=", "15", ",", "color", "=", "'black'", ")", "\n", "plt", ".", "xticks", "(", "[", "]", ")", "\n", "plt", ".", "yticks", "(", "[", "]", ")", "\n", "\n", "#Plot the actual clustering", "\n", "plt", ".", "subplot", "(", "212", ")", "\n", "for", "k", "in", "set", "(", "actual", ")", ":", "\n", "        ", "for", "i", "in", "range", "(", "0", ",", "len", "(", "toCluster", ")", ")", ":", "\n", "            ", "if", "actual", "[", "i", "]", "==", "k", ":", "\n", "                ", "plt", ".", "plot", "(", "X_transformed", "[", "i", ",", "0", "]", ",", "X_transformed", "[", "i", ",", "1", "]", ",", "'o'", ",", "color", "=", "color", "[", "k", "]", ",", "markersize", "=", "4", ")", "\n", "", "", "", "plt", ".", "title", "(", "'Actual labeling'", ",", "fontsize", "=", "15", ",", "color", "=", "'black'", ")", "\n", "plt", ".", "xticks", "(", "[", "]", ")", "\n", "plt", ".", "yticks", "(", "[", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Garrafao_TokenChange.WordSenseClustering.W2v.main": [[22, 112], ["docopt.docopt", "int", "logging.basicConfig", "print", "time.time", "logging.critical", "gensim.models.KeyedVectors.load_word2vec_format", "logging.critical", "sklearn.preprocessing.normalize", "utils_.Space", "utils_.Space.save", "logging.critical", "logging.critical", "print", "len", "isinstance", "open", "csv.DictReader", "dic[].split", "enumerate", "testSentences.append", "dict", "str", "max", "min", "time.time", "len", "preprocessing.normalize.append", "preprocessing.normalize.append", "W2v.getContextVector", "numpy.zeros", "toMelt.append", "sklearn.preprocessing.normalize"], "function", ["home.repos.pwc.inspect_result.Garrafao_TokenChange.WordSenseClustering.CountBasedVectors.getContextVector"], ["def", "main", "(", ")", ":", "\n", "\n", "# Get the arguments", "\n", "    ", "args", "=", "docopt", "(", "\"\"\"\n\n    Usage:\n        W2v.py  <pathTestSentences> <outPathVectors> <windowSize2> <sentenceType>\n        W2v.py  <pathTestSentences> <windowSize2> <sentenceType>\n        \n    Arguments:\n       \n        <pathTestSentences> = Path to the test sentences\n        <outPathVectors> = Path for storing the vectors \n        <windowSize2> = Window size (20 works good)\n        <sentenceType> = \"lemma\" or \"token\"\n    \n    \"\"\"", ")", "\n", "\n", "\n", "pathTestSentences", "=", "args", "[", "'<pathTestSentences>'", "]", "\n", "outPathVectors", "=", "args", "[", "'<outPathVectors>'", "]", "\n", "windowSize2", "=", "int", "(", "args", "[", "'<windowSize2>'", "]", ")", "\n", "sentenceType", "=", "args", "[", "'<sentenceType>'", "]", "\n", "\n", "if", "len", "(", "sys", ".", "argv", ")", "==", "4", ":", "\n", "        ", "outPathVectors", "=", "\"Files/Vectors/SecondOrder/Vectors.npz\"", "\n", "\n", "\n", "", "logging", ".", "basicConfig", "(", "format", "=", "'%(asctime)s : %(levelname)s : %(message)s'", ",", "level", "=", "logging", ".", "CRITICAL", ")", "\n", "print", "(", "\"\"", ")", "\n", "start_time", "=", "time", ".", "time", "(", ")", "\n", "logging", ".", "critical", "(", "\"W2V start\"", ")", "\n", "\n", "if", "sentenceType", "==", "\"token\"", ":", "\n", "        ", "sentType", "=", "\"sentence_token\"", "\n", "", "else", ":", "\n", "        ", "sentType", "=", "\"sentence\"", "\n", "\n", "\n", "", "if", "not", "isinstance", "(", "windowSize2", ",", "int", ")", ":", "\n", "        ", "windowSize2", "=", "20", "\n", "\n", "#Load Word2Vec", "\n", "", "model", "=", "gensim", ".", "models", ".", "KeyedVectors", ".", "load_word2vec_format", "(", "'Data/GoogleNews-vectors-negative300.bin'", ",", "binary", "=", "True", ")", "\n", "\n", "#Load TestSentences ", "\n", "contextVectorList", "=", "[", "]", "\n", "testSentences", "=", "[", "]", "\n", "with", "open", "(", "pathTestSentences", ",", "'r'", ")", "as", "csvFile", ":", "\n", "        ", "reader", "=", "csv", ".", "DictReader", "(", "csvFile", ",", "delimiter", "=", "\"\\t\"", ")", "\n", "for", "row", "in", "reader", ":", "\n", "            ", "testSentences", ".", "append", "(", "dict", "(", "row", ")", ")", "\n", "\n", "#Calculate contextVectorMatrix", "\n", "", "", "logging", ".", "critical", "(", "\"Calculate contextVectorMatrix\"", ")", "\n", "\n", "nonExisting", "=", "False", "\n", "#self.target=str(testSentences[0][\"original_word\"])        ", "\n", "for", "dic", "in", "testSentences", ":", "\n", "        ", "sentence", "=", "dic", "[", "sentType", "]", ".", "split", "(", ")", "\n", "for", "i", ",", "word", "in", "enumerate", "(", "sentence", ")", ":", "\n", "            ", "if", "str", "(", "i", ")", "==", "dic", "[", "'target_index'", "]", ":", "\n", "\n", "                ", "toMelt", "=", "[", "]", "\n", "toMeltIDF", "=", "[", "]", "\n", "lowerWindowSize", "=", "max", "(", "i", "-", "windowSize2", ",", "0", ")", "\n", "upperWindowSize", "=", "min", "(", "i", "+", "windowSize2", ",", "len", "(", "sentence", ")", ")", "\n", "window", "=", "sentence", "[", "lowerWindowSize", ":", "i", "]", "+", "sentence", "[", "i", "+", "1", ":", "upperWindowSize", "+", "1", "]", "\n", "if", "word", "in", "model", ".", "wv", ".", "vocab", ":", "\n", "                    ", "for", "contextWord", "in", "window", ":", "\n", "                        ", "if", "contextWord", "in", "model", ".", "wv", ".", "vocab", ":", "\n", "                            ", "if", "contextWord", "!=", "\"$\"", ":", "\n", "                                ", "toMelt", ".", "append", "(", "preprocessing", ".", "normalize", "(", "[", "model", ".", "wv", "[", "contextWord", "]", "]", ",", "norm", "=", "'l2'", ")", "[", "0", "]", ")", "\n", "\n", "\n", "", "", "", "contextVectorList", ".", "append", "(", "getContextVector", "(", "toMelt", ")", ")", "\n", "", "else", ":", "\n", "                    ", "contextVectorList", ".", "append", "(", "np", ".", "zeros", "(", "300", ")", ")", "\n", "\n", "\n", "#Normalize vectors in length", "\n", "", "", "", "", "contextVectorList", "=", "preprocessing", ".", "normalize", "(", "contextVectorList", ",", "norm", "=", "'l2'", ")", "\n", "\n", "#Save contextVectorList_sparse matrix", "\n", "outSpace", "=", "Space", "(", "matrix", "=", "contextVectorList", ",", "rows", "=", "\" \"", ",", "columns", "=", "\" \"", ")", "\n", "outSpace", ".", "save", "(", "outPathVectors", ")", "\n", "\n", "logging", ".", "critical", "(", "\"W2V end\"", ")", "\n", "logging", ".", "critical", "(", "\"--- %s seconds ---\"", "%", "(", "time", ".", "time", "(", ")", "-", "start_time", ")", ")", "\n", "print", "(", "\"\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Garrafao_TokenChange.WordSenseClustering.W2v.getContextVector": [[115, 118], ["centroid", "sum", "zip"], "function", ["None"], ["", "def", "getContextVector", "(", "toMeltList", ")", ":", "\n", "    ", "centroid", "=", "lambda", "inp", ":", "[", "[", "sum", "(", "m", ")", "for", "m", "in", "zip", "(", "*", "l", ")", "]", "for", "l", "in", "inp", "]", "\n", "return", "centroid", "(", "[", "toMeltList", "]", ")", "[", "0", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.Garrafao_TokenChange.WordSenseClustering.CountBasedVectors.main": [[22, 147], ["docopt.docopt", "int", "logging.basicConfig", "print", "time.time", "logging.critical", "numpy.load().item", "range", "logging.critical", "str", "sklearn.preprocessing.normalize", "utils_.Space", "utils_.Space.save", "logging.critical", "logging.critical", "print", "len", "utils_.Space", "len", "gzip.open", "np.load().item.items", "open", "csv.DictReader", "dic[].split", "enumerate", "numpy.load", "utils_.Space", "math.log10", "testSentences.append", "set", "dict", "max", "min", "time.time", "dic[].split.split", "max", "str", "len", "preprocessing.normalize.append", "CountBasedVectors.getContextVector", "toMelt.append", "math.pow", "cooc_mat_sparse[].toarray"], "function", ["home.repos.pwc.inspect_result.Garrafao_TokenChange.WordSenseClustering.CountBasedVectors.getContextVector"], ["def", "main", "(", ")", ":", "\n", "\n", "# Get the arguments", "\n", "    ", "args", "=", "docopt", "(", "\"\"\"\n\n    Usage:\n        CountBasedVectors.py  <pathMatrix> <pathw2i> <pathCorpus> <pathTestSentences> <outPathVectors> <sentenceType> <windowSize2> \n        CountBasedVectors.py  <pathCorpus> <pathTestSentences> <sentenceType> <windowSize2>\n        \n    Arguments:\n       \n        <pathMatrix> = Path to the word vector matrix\n        <pathw2i> = Path to the word-to-index\n        <pathCorpus> = path to the corpus \n        <pathTestSentences> = Path to the test sentences\n        <outPathVectors> = Path for storing the vectors\n        <sentenceType> = \"lemma\" or \"token\"\n        <windowSize2> = Window size (20 works fine)\n        \n        \n    \"\"\"", ")", "\n", "\n", "pathMatrix", "=", "args", "[", "'<pathMatrix>'", "]", "\n", "pathTestSentences", "=", "args", "[", "'<pathTestSentences>'", "]", "\n", "pathw2i", "=", "args", "[", "'<pathw2i>'", "]", "\n", "outPathVectors", "=", "args", "[", "'<outPathVectors>'", "]", "\n", "windowSize2", "=", "int", "(", "args", "[", "'<windowSize2>'", "]", ")", "\n", "pathCorpus", "=", "args", "[", "'<pathCorpus>'", "]", "\n", "sentenceType", "=", "args", "[", "'<sentenceType>'", "]", "\n", "\n", "\n", "if", "len", "(", "sys", ".", "argv", ")", "==", "5", ":", "\n", "        ", "pathMatrix", "=", "\"Files/Vectors/FirstOrder/matrix.npz\"", "\n", "pathw2i", "=", "\"Files/Vectors/FirstOrder/w2i.npz.npy\"", "\n", "outPathVectors", "=", "\"Files/Vectors/SecondOrder/Vectors.npz\"", "\n", "\n", "\n", "", "logging", ".", "basicConfig", "(", "format", "=", "'%(asctime)s : %(levelname)s : %(message)s'", ",", "level", "=", "logging", ".", "CRITICAL", ")", "\n", "print", "(", "\"\"", ")", "\n", "start_time", "=", "time", ".", "time", "(", ")", "\n", "logging", ".", "critical", "(", "\"ContextVectors start\"", ")", "\n", "\n", "#Load w2i", "\n", "w2i", "=", "np", ".", "load", "(", "pathw2i", ",", "allow_pickle", "=", "'TRUE'", ")", ".", "item", "(", ")", "\n", "\n", "if", "sentenceType", "==", "\"token\"", ":", "\n", "        ", "sentType", "=", "\"sentence_token\"", "\n", "", "else", ":", "\n", "        ", "sentType", "=", "\"sentence\"", "\n", "\n", "\n", "#Load saved wordVectorMatrix", "\n", "", "try", ":", "\n", "        ", "inSpace", "=", "Space", "(", "path", "=", "pathMatrix", ",", "format", "=", "'w2v'", ")", "\n", "", "except", "UnicodeDecodeError", ":", "\n", "        ", "inSpace", "=", "Space", "(", "path", "=", "pathMatrix", ")", "\n", "\n", "\n", "\n", "#inSpace =  Space(path=pathMatrix, format='w2v')", "\n", "#inSpace = Space(path=pathMatrix)", "\n", "", "cooc_mat_sparse", "=", "inSpace", ".", "matrix", "\n", "\n", "#Calculate IDF for every word ", "\n", "docFreq", "=", "{", "}", "\n", "\n", "for", "i", "in", "range", "(", "0", ",", "len", "(", "w2i", ")", ")", ":", "\n", "        ", "docFreq", "[", "i", "]", "=", "0", "\n", "", "with", "gzip", ".", "open", "(", "pathCorpus", ",", "'rt'", ",", "encoding", "=", "\"utf-8\"", ")", "as", "sentences", ":", "\n", "        ", "count", "=", "0", "\n", "try", ":", "\n", "            ", "for", "sentence", "in", "sentences", ":", "\n", "                ", "count", "=", "count", "+", "1", "\n", "for", "word", "in", "set", "(", "sentence", ".", "split", "(", ")", ")", ":", "\n", "                    ", "if", "word", "in", "w2i", ":", "\n", "                        ", "docFreq", "[", "w2i", "[", "word", "]", "]", "+=", "1", "\n", "", "", "", "", "except", ":", "\n", "            ", "pass", "\n", "", "for", "key", ",", "value", "in", "w2i", ".", "items", "(", ")", ":", "\n", "            ", "docFreq", "[", "value", "]", "=", "math", ".", "log10", "(", "count", "/", "max", "(", "docFreq", "[", "value", "]", ",", "1", ")", ")", "\n", "\n", "#Load TestSentences ", "\n", "", "", "contextVectorList", "=", "[", "]", "\n", "testSentences", "=", "[", "]", "\n", "with", "open", "(", "pathTestSentences", ",", "'r'", ")", "as", "csvFile", ":", "\n", "        ", "reader", "=", "csv", ".", "DictReader", "(", "csvFile", ",", "delimiter", "=", "\"\\t\"", ")", "\n", "for", "row", "in", "reader", ":", "\n", "            ", "testSentences", ".", "append", "(", "dict", "(", "row", ")", ")", "\n", "\n", "#Calculate contextVectorMatrix", "\n", "", "", "logging", ".", "critical", "(", "\"Calculate contextVectorMatrix\"", ")", "\n", "nonExisting", "=", "False", "\n", "target", "=", "str", "(", "testSentences", "[", "0", "]", "[", "\"original_word\"", "]", ")", "\n", "for", "dic", "in", "testSentences", ":", "\n", "        ", "sentence", "=", "dic", "[", "sentType", "]", ".", "split", "(", ")", "\n", "for", "i", ",", "word", "in", "enumerate", "(", "sentence", ")", ":", "\n", "            ", "if", "str", "(", "i", ")", "==", "dic", "[", "'target_index'", "]", "and", "word", "==", "target", ":", "\n", "                ", "toMelt", "=", "[", "]", "\n", "toMeltIDF", "=", "[", "]", "\n", "lowerWindowSize", "=", "max", "(", "i", "-", "windowSize2", ",", "0", ")", "\n", "upperWindowSize", "=", "min", "(", "i", "+", "windowSize2", ",", "len", "(", "sentence", ")", ")", "\n", "window", "=", "sentence", "[", "lowerWindowSize", ":", "i", "]", "+", "sentence", "[", "i", "+", "1", ":", "upperWindowSize", "+", "1", "]", "\n", "if", "word", "in", "w2i", ":", "\n", "                    ", "windex", "=", "w2i", "[", "word", "]", "\n", "for", "contextWord", "in", "window", ":", "\n", "                        ", "if", "contextWord", "!=", "\"$\"", ":", "\n", "                            ", "if", "contextWord", "in", "w2i", ":", "\n", "                                ", "contextWordIndex", "=", "w2i", "[", "contextWord", "]", "\n", "toMelt", ".", "append", "(", "cooc_mat_sparse", "[", "contextWordIndex", "]", ".", "toarray", "(", ")", "[", "0", "]", "\n", "*", "math", ".", "pow", "(", "docFreq", "[", "contextWordIndex", "]", ",", "1", ")", ")", "\n", "", "", "", "contextVectorList", ".", "append", "(", "getContextVector", "(", "toMelt", ")", ")", "\n", "", "else", ":", "\n", "                    ", "nonExisting", "=", "True", "\n", "\n", "\n", "#Normalize vectors in length", "\n", "", "", "", "", "contextVectorList", "=", "preprocessing", ".", "normalize", "(", "contextVectorList", ",", "norm", "=", "'l2'", ")", "\n", "\n", "#Save contextVectorList_sparse matrix", "\n", "outSpace", "=", "Space", "(", "matrix", "=", "contextVectorList", ",", "rows", "=", "\" \"", ",", "columns", "=", "\" \"", ")", "\n", "outSpace", ".", "save", "(", "outPathVectors", ")", "\n", "\n", "logging", ".", "critical", "(", "\"ContextVectors end\"", ")", "\n", "logging", ".", "critical", "(", "\"--- %s seconds ---\"", "%", "(", "time", ".", "time", "(", ")", "-", "start_time", ")", ")", "\n", "print", "(", "\"\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Garrafao_TokenChange.WordSenseClustering.CountBasedVectors.getContextVector": [[150, 153], ["centroid", "sum", "zip"], "function", ["None"], ["", "def", "getContextVector", "(", "toMeltList", ")", ":", "\n", "    ", "centroid", "=", "lambda", "inp", ":", "[", "[", "sum", "(", "m", ")", "for", "m", "in", "zip", "(", "*", "l", ")", "]", "for", "l", "in", "inp", "]", "\n", "return", "centroid", "(", "[", "toMeltList", "]", ")", "[", "0", "]", "\n", "\n"]]}