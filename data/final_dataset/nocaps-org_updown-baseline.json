{"home.repos.pwc.inspect_result.nocaps-org_updown-baseline.updown.config.Config.__init__": [[129, 184], ["yacs.config.CfgNode", "yacs.config.CfgNode", "yacs.config.CfgNode", "yacs.config.CfgNode", "yacs.config.CfgNode", "config.Config._C.merge_from_list", "config.Config._validate", "config.Config._C.freeze", "config.Config._C.merge_from_file"], "methods", ["home.repos.pwc.inspect_result.nocaps-org_updown-baseline.updown.config.Config._validate"], ["def", "__init__", "(", "self", ",", "config_file", ":", "Optional", "[", "str", "]", "=", "None", ",", "config_override", ":", "List", "[", "Any", "]", "=", "[", "]", ")", ":", "\n", "\n", "        ", "_C", "=", "CN", "(", ")", "\n", "_C", ".", "RANDOM_SEED", "=", "0", "\n", "\n", "_C", ".", "DATA", "=", "CN", "(", ")", "\n", "_C", ".", "DATA", ".", "VOCABULARY", "=", "\"data/vocabulary\"", "\n", "\n", "_C", ".", "DATA", ".", "TRAIN_FEATURES", "=", "\"data/coco_train2017_vg_detector_features_adaptive.h5\"", "\n", "_C", ".", "DATA", ".", "INFER_FEATURES", "=", "\"data/nocaps_val_vg_detector_features_adaptive.h5\"", "\n", "\n", "# DATA.INFER_CAPTIONS don't contain the captions, just the image info.", "\n", "_C", ".", "DATA", ".", "TRAIN_CAPTIONS", "=", "\"data/coco/captions_train2017.json\"", "\n", "_C", ".", "DATA", ".", "INFER_CAPTIONS", "=", "\"data/nocaps/nocaps_val_image_info.json\"", "\n", "\n", "_C", ".", "DATA", ".", "MAX_CAPTION_LENGTH", "=", "20", "\n", "\n", "# There's no parameter as DATA.CBS.TRAIN_BOXES because CBS is inference-only.", "\n", "_C", ".", "DATA", ".", "CBS", "=", "CN", "(", ")", "\n", "_C", ".", "DATA", ".", "CBS", ".", "INFER_BOXES", "=", "\"data/nocaps_val_oi_detector_boxes.json\"", "\n", "_C", ".", "DATA", ".", "CBS", ".", "CLASS_HIERARCHY", "=", "\"data/cbs/class_hierarchy.json\"", "\n", "_C", ".", "DATA", ".", "CBS", ".", "WORDFORMS", "=", "\"data/cbs/constraint_wordforms.tsv\"", "\n", "\n", "_C", ".", "DATA", ".", "CBS", ".", "NMS_THRESHOLD", "=", "0.85", "\n", "_C", ".", "DATA", ".", "CBS", ".", "MAX_GIVEN_CONSTRAINTS", "=", "3", "\n", "_C", ".", "DATA", ".", "CBS", ".", "MAX_WORDS_PER_CONSTRAINT", "=", "3", "\n", "\n", "_C", ".", "MODEL", "=", "CN", "(", ")", "\n", "_C", ".", "MODEL", ".", "IMAGE_FEATURE_SIZE", "=", "2048", "\n", "_C", ".", "MODEL", ".", "EMBEDDING_SIZE", "=", "1000", "\n", "_C", ".", "MODEL", ".", "HIDDEN_SIZE", "=", "1200", "\n", "_C", ".", "MODEL", ".", "ATTENTION_PROJECTION_SIZE", "=", "768", "\n", "_C", ".", "MODEL", ".", "BEAM_SIZE", "=", "5", "\n", "_C", ".", "MODEL", ".", "USE_CBS", "=", "False", "\n", "_C", ".", "MODEL", ".", "MIN_CONSTRAINTS_TO_SATISFY", "=", "2", "\n", "\n", "_C", ".", "OPTIM", "=", "CN", "(", ")", "\n", "_C", ".", "OPTIM", ".", "BATCH_SIZE", "=", "150", "\n", "_C", ".", "OPTIM", ".", "NUM_ITERATIONS", "=", "70000", "\n", "_C", ".", "OPTIM", ".", "LR", "=", "0.015", "\n", "_C", ".", "OPTIM", ".", "MOMENTUM", "=", "0.9", "\n", "_C", ".", "OPTIM", ".", "WEIGHT_DECAY", "=", "0.001", "\n", "_C", ".", "OPTIM", ".", "CLIP_GRADIENTS", "=", "12.5", "\n", "\n", "# Override parameter values from YAML file first, then from override list.", "\n", "self", ".", "_C", "=", "_C", "\n", "if", "config_file", "is", "not", "None", ":", "\n", "            ", "self", ".", "_C", ".", "merge_from_file", "(", "config_file", ")", "\n", "", "self", ".", "_C", ".", "merge_from_list", "(", "config_override", ")", "\n", "\n", "# Do any sort of validations required for the config.", "\n", "self", ".", "_validate", "(", ")", "\n", "\n", "# Make an instantiated object of this class immutable.", "\n", "self", ".", "_C", ".", "freeze", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nocaps-org_updown-baseline.updown.config.Config.dump": [[185, 195], ["config.Config._C.dump", "open"], "methods", ["home.repos.pwc.inspect_result.nocaps-org_updown-baseline.updown.config.Config.dump"], ["", "def", "dump", "(", "self", ",", "file_path", ":", "str", ")", ":", "\n", "        ", "r\"\"\"\n        Save config at the specified file path.\n\n        Parameters\n        ----------\n        file_path: str\n            (YAML) path to save config at.\n        \"\"\"", "\n", "self", ".", "_C", ".", "dump", "(", "stream", "=", "open", "(", "file_path", ",", "\"w\"", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nocaps-org_updown-baseline.updown.config.Config._validate": [[196, 208], ["None"], "methods", ["None"], ["", "def", "_validate", "(", "self", ")", ":", "\n", "        ", "r\"\"\"\n        Perform all validations to raise error if there are parameters with conflicting values.\n        \"\"\"", "\n", "if", "self", ".", "_C", ".", "MODEL", ".", "USE_CBS", ":", "\n", "            ", "assert", "self", ".", "_C", ".", "MODEL", ".", "EMBEDDING_SIZE", "==", "300", ",", "\"Word embeddings must be initialized with\"", "\n", "\" fixed GloVe Embeddings (300 dim) for performing CBS decoding during inference. \"", "\n", "f\"Found MODEL.EMBEDDING_SIZE as {self._C.MODEL.EMBEDDING_SIZE} instead.\"", "\n", "\n", "", "assert", "(", "\n", "self", ".", "_C", ".", "MODEL", ".", "MIN_CONSTRAINTS_TO_SATISFY", "<=", "self", ".", "_C", ".", "DATA", ".", "CBS", ".", "MAX_GIVEN_CONSTRAINTS", "\n", ")", ",", "\"Satisfying more constraints than maximum specified is not possible.\"", "\n", "\n"]], "home.repos.pwc.inspect_result.nocaps-org_updown-baseline.updown.config.Config.__getattr__": [[209, 211], ["config.Config._C.__getattr__"], "methods", ["home.repos.pwc.inspect_result.nocaps-org_updown-baseline.updown.config.Config.__getattr__"], ["", "def", "__getattr__", "(", "self", ",", "attr", ":", "str", ")", ":", "\n", "        ", "return", "self", ".", "_C", ".", "__getattr__", "(", "attr", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nocaps-org_updown-baseline.updown.config.Config.__str__": [[212, 219], ["str", "str", "str", "str", "yacs.config.CfgNode", "yacs.config.CfgNode", "yacs.config.CfgNode", "yacs.config.CfgNode"], "methods", ["None"], ["", "def", "__str__", "(", "self", ")", ":", "\n", "        ", "common_string", ":", "str", "=", "str", "(", "CN", "(", "{", "\"RANDOM_SEED\"", ":", "self", ".", "_C", ".", "RANDOM_SEED", "}", ")", ")", "+", "\"\\n\"", "\n", "common_string", "+=", "str", "(", "CN", "(", "{", "\"DATA\"", ":", "self", ".", "_C", ".", "DATA", "}", ")", ")", "+", "\"\\n\"", "\n", "common_string", "+=", "str", "(", "CN", "(", "{", "\"MODEL\"", ":", "self", ".", "_C", ".", "MODEL", "}", ")", ")", "+", "\"\\n\"", "\n", "common_string", "+=", "str", "(", "CN", "(", "{", "\"OPTIM\"", ":", "self", ".", "_C", ".", "OPTIM", "}", ")", ")", "+", "\"\\n\"", "\n", "\n", "return", "common_string", "\n", "\n"]], "home.repos.pwc.inspect_result.nocaps-org_updown-baseline.updown.config.Config.__repr__": [[220, 222], ["config.Config._C.__repr__"], "methods", ["home.repos.pwc.inspect_result.nocaps-org_updown-baseline.updown.config.Config.__repr__"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "_C", ".", "__repr__", "(", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.nocaps-org_updown-baseline.utils.decoding.select_best_beam": [[8, 26], ["None"], "function", ["None"], ["def", "select_best_beam", "(", "\n", "beams", ":", "torch", ".", "Tensor", ",", "\n", "beam_log_probabilities", ":", "torch", ".", "Tensor", ",", "\n", ")", "->", "torch", ".", "Tensor", ":", "\n", "    ", "r\"\"\"\n    Select the best beam out of a set of decoded beams.\n\n    Parameters\n    ----------\n    beams: torch.Tensor\n        A tensor of shape ``(batch_size, num_states, max_decoding_steps)`` containing decoded\n        beams by :class:`~allennlp.nn.beam_search.BeamSearch`. These beams are sorted according to\n        their likelihood (descending) in ``beam_size`` dimension.\n    beam_log_probabilities: torch.Tensor\n        A tensor of shape ``(batch_size, num_states, beam_size)`` containing likelihood of decoded\n        beams.\n    \"\"\"", "\n", "return", "beams", "[", ":", ",", "0", ",", ":", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.nocaps-org_updown-baseline.utils.decoding.select_best_beam_with_constraints": [[28, 89], ["beams.size", "range", "torch.stack().long().to", "torch.argmax", "best_beams.append", "best_beam_log_probabilities.append", "torch.stack().long", "range", "bin().count", "min", "torch.stack", "given_constraints[].item", "bin"], "function", ["None"], ["", "def", "select_best_beam_with_constraints", "(", "\n", "beams", ":", "torch", ".", "Tensor", ",", "\n", "beam_log_probabilities", ":", "torch", ".", "Tensor", ",", "\n", "given_constraints", ":", "torch", ".", "Tensor", ",", "\n", "min_constraints_to_satisfy", ":", "int", "=", "2", ",", "\n", ")", "->", "torch", ".", "Tensor", ":", "\n", "    ", "r\"\"\"\n    Select the best beam which satisfies specified minimum constraints out of a total number of\n    given constraints.\n\n    .. note::\n\n        The implementation of this function goes hand-in-hand with the FSM building implementation\n        in :meth:`~updown.utils.constraints.FiniteStateMachineBuilder.build` - it defines which\n        state satisfies which (basically, how many) constraints. If the \"definition\" of states\n        change, then selection of beams also changes accordingly.\n\n    Parameters\n    ----------\n    beams: torch.Tensor\n        A tensor of shape ``(batch_size, num_states, beam_size, max_decoding_steps)`` containing\n        decoded beams by :class:`~updown.modules.cbs.ConstrainedBeamSearch`. These beams are\n        sorted according to their likelihood (descending) in ``beam_size`` dimension.\n    beam_log_probabilities: torch.Tensor\n        A tensor of shape ``(batch_size, num_states, beam_size)`` containing likelihood of decoded\n        beams.\n    given_constraints: torch.Tensor\n        A tensor of shape ``(batch_size, )`` containing number of constraints given at the start\n        of decoding.\n    min_constraints_to_satisfy: int, optional (default = 2)\n        Minimum number of constraints to satisfy. This is either 2, or ``given_constraints`` if\n        they are less than 2. Beams corresponding to states not satisfying at least these number\n        of constraints will be dropped. Only up to 3 supported.\n\n    Returns\n    -------\n    torch.Tensor\n        Decoded sequence (beam) which has highest likelihood among beams satisfying constraints.\n    \"\"\"", "\n", "batch_size", ",", "num_states", ",", "beam_size", ",", "max_decoding_steps", "=", "beams", ".", "size", "(", ")", "\n", "\n", "best_beams", ":", "List", "[", "torch", ".", "Tensor", "]", "=", "[", "]", "\n", "best_beam_log_probabilities", ":", "List", "[", "torch", ".", "Tensor", "]", "=", "[", "]", "\n", "\n", "for", "i", "in", "range", "(", "batch_size", ")", ":", "\n", "# fmt: off", "\n", "        ", "valid_states", "=", "[", "\n", "s", "for", "s", "in", "range", "(", "2", "**", "given_constraints", "[", "i", "]", ".", "item", "(", ")", ")", "\n", "if", "bin", "(", "s", ")", ".", "count", "(", "\"1\"", ")", ">=", "min", "(", "given_constraints", "[", "i", "]", ",", "min_constraints_to_satisfy", ")", "\n", "]", "\n", "# fmt: on", "\n", "\n", "valid_beams", "=", "beams", "[", "i", ",", "valid_states", ",", "0", ",", ":", "]", "\n", "valid_beam_log_probabilities", "=", "beam_log_probabilities", "[", "i", ",", "valid_states", ",", "0", "]", "\n", "\n", "selected_index", "=", "torch", ".", "argmax", "(", "valid_beam_log_probabilities", ")", "\n", "best_beams", ".", "append", "(", "valid_beams", "[", "selected_index", ",", ":", "]", ")", "\n", "best_beam_log_probabilities", ".", "append", "(", "valid_beam_log_probabilities", "[", "selected_index", "]", ")", "\n", "\n", "# shape: (batch_size, max_decoding_steps)", "\n", "", "return", "torch", ".", "stack", "(", "best_beams", ")", ".", "long", "(", ")", ".", "to", "(", "beams", ".", "device", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.nocaps-org_updown-baseline.utils.common.cycle": [[7, 25], ["batch[].to"], "function", ["None"], ["def", "cycle", "(", "\n", "dataloader", ":", "DataLoader", ",", "device", ":", "torch", ".", "device", "\n", ")", "->", "Generator", "[", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ",", "None", ",", "None", "]", ":", "\n", "    ", "r\"\"\"\n    A generator which yields a random batch from dataloader perpetually. This generator is\n    used in the constructor.\n\n    Extended Summary\n    ----------------\n    This is done so because we train for a fixed number of iterations, and do not have the\n    notion of 'epochs'. Using ``itertools.cycle`` with dataloader is harmful and may cause\n    unexpeced memory leaks.\n    \"\"\"", "\n", "while", "True", ":", "\n", "        ", "for", "batch", "in", "dataloader", ":", "\n", "            ", "for", "key", "in", "batch", ":", "\n", "                ", "batch", "[", "key", "]", "=", "batch", "[", "key", "]", ".", "to", "(", "device", ")", "\n", "", "yield", "batch", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.nocaps-org_updown-baseline.utils.checkpointing.CheckpointManager.__init__": [[50, 80], ["isinstance", "isinstance", "TypeError", "isinstance", "TypeError", "type", "type"], "methods", ["None"], ["def", "__init__", "(", "\n", "self", ",", "\n", "models", ":", "Union", "[", "nn", ".", "Module", ",", "Dict", "[", "str", ",", "nn", ".", "Module", "]", "]", ",", "\n", "optimizer", ":", "Type", "[", "optim", ".", "Optimizer", "]", ",", "\n", "serialization_dir", ":", "str", ",", "\n", "mode", ":", "str", "=", "\"max\"", ",", "\n", "filename_prefix", ":", "str", "=", "\"checkpoint\"", ",", "\n", ")", ":", "\n", "\n", "# Convert single model to a dict.", "\n", "        ", "if", "isinstance", "(", "models", ",", "nn", ".", "Module", ")", ":", "\n", "            ", "models", "=", "{", "\"model\"", ":", "models", "}", "\n", "\n", "", "for", "key", "in", "models", ":", "\n", "            ", "if", "not", "isinstance", "(", "models", "[", "key", "]", ",", "nn", ".", "Module", ")", ":", "\n", "                ", "raise", "TypeError", "(", "\"{} is not a Module\"", ".", "format", "(", "type", "(", "models", ")", ".", "__name__", ")", ")", "\n", "\n", "", "", "if", "not", "isinstance", "(", "optimizer", ",", "optim", ".", "Optimizer", ")", ":", "\n", "            ", "raise", "TypeError", "(", "\"{} is not an Optimizer\"", ".", "format", "(", "type", "(", "optimizer", ")", ".", "__name__", ")", ")", "\n", "\n", "", "self", ".", "_models", "=", "models", "\n", "self", ".", "_optimizer", "=", "optimizer", "\n", "self", ".", "_serialization_dir", "=", "serialization_dir", "\n", "\n", "self", ".", "_mode", "=", "mode", "\n", "self", ".", "_filename_prefix", "=", "filename_prefix", "\n", "\n", "# Initialize members to hold state dict of best checkpoint and its performance.", "\n", "self", ".", "_best_metric", ":", "Optional", "[", "Union", "[", "float", ",", "torch", ".", "Tensor", "]", "]", "=", "None", "\n", "self", ".", "_best_ckpt", ":", "Dict", "[", "str", ",", "Any", "]", "=", "{", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.nocaps-org_updown-baseline.utils.checkpointing.CheckpointManager.step": [[81, 112], ["torch.save", "torch.save", "isinstance", "copy.copy", "os.path.join", "os.path.join", "checkpointing.CheckpointManager._models[].module.state_dict", "checkpointing.CheckpointManager._models[].state_dict", "checkpointing.CheckpointManager._optimizer.state_dict"], "methods", ["None"], ["", "def", "step", "(", "self", ",", "metric", ":", "Union", "[", "float", ",", "torch", ".", "Tensor", "]", ",", "epoch_or_iteration", ":", "int", ")", ":", "\n", "        ", "r\"\"\"Serialize checkpoint and update best checkpoint based on metric and mode.\"\"\"", "\n", "\n", "# Update best checkpoint based on metric and metric mode.", "\n", "if", "not", "self", ".", "_best_metric", ":", "\n", "            ", "self", ".", "_best_metric", "=", "metric", "\n", "\n", "", "models_state_dict", ":", "Dict", "[", "str", ",", "Any", "]", "=", "{", "}", "\n", "for", "key", "in", "self", ".", "_models", ":", "\n", "            ", "if", "isinstance", "(", "self", ".", "_models", "[", "key", "]", ",", "nn", ".", "DataParallel", ")", ":", "\n", "                ", "models_state_dict", "[", "key", "]", "=", "self", ".", "_models", "[", "key", "]", ".", "module", ".", "state_dict", "(", ")", "\n", "", "else", ":", "\n", "                ", "models_state_dict", "[", "key", "]", "=", "self", ".", "_models", "[", "key", "]", ".", "state_dict", "(", ")", "\n", "\n", "", "", "if", "(", "self", ".", "_mode", "==", "\"min\"", "and", "metric", "<", "self", ".", "_best_metric", ")", "or", "(", "\n", "self", ".", "_mode", "==", "\"max\"", "and", "metric", ">", "self", ".", "_best_metric", "\n", ")", ":", "\n", "            ", "self", ".", "_best_metric", "=", "metric", "\n", "self", ".", "_best_ckpt", "=", "copy", ".", "copy", "(", "models_state_dict", ")", "\n", "\n", "# Serialize checkpoint corresponding to current epoch (or iteration).", "\n", "", "torch", ".", "save", "(", "\n", "{", "**", "models_state_dict", ",", "\"optimizer\"", ":", "self", ".", "_optimizer", ".", "state_dict", "(", ")", "}", ",", "\n", "os", ".", "path", ".", "join", "(", "\n", "self", ".", "_serialization_dir", ",", "f\"{self._filename_prefix}_{epoch_or_iteration}.pth\"", "\n", ")", ",", "\n", ")", "\n", "# Serialize best performing checkpoint observed so far.", "\n", "torch", ".", "save", "(", "\n", "self", ".", "_best_ckpt", ",", "\n", "os", ".", "path", ".", "join", "(", "self", ".", "_serialization_dir", ",", "f\"{self._filename_prefix}_best.pth\"", ")", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.nocaps-org_updown-baseline.utils.constraints.ConstraintFilter.__init__": [[103, 121], ["constraints.ConstraintFilter.__init__.__read_hierarchy"], "methods", ["None"], ["def", "__init__", "(", "\n", "self", ",", "hierarchy_jsonpath", ":", "str", ",", "nms_threshold", ":", "float", "=", "0.85", ",", "max_given_constraints", ":", "int", "=", "3", "\n", ")", ":", "\n", "        ", "def", "__read_hierarchy", "(", "node", ":", "anytree", ".", "AnyNode", ",", "parent", ":", "Optional", "[", "anytree", ".", "AnyNode", "]", "=", "None", ")", ":", "\n", "# Cast an ``anytree.AnyNode`` (after first level of recursion) to dict.", "\n", "            ", "attributes", "=", "dict", "(", "node", ")", "\n", "children", "=", "attributes", ".", "pop", "(", "\"Subcategory\"", ",", "[", "]", ")", "\n", "\n", "node", "=", "anytree", ".", "AnyNode", "(", "parent", "=", "parent", ",", "**", "attributes", ")", "\n", "for", "child", "in", "children", ":", "\n", "                ", "__read_hierarchy", "(", "child", ",", "parent", "=", "node", ")", "\n", "", "return", "node", "\n", "\n", "# Read the object class hierarchy as a tree, to make searching easier.", "\n", "", "self", ".", "_hierarchy", "=", "__read_hierarchy", "(", "json", ".", "load", "(", "open", "(", "hierarchy_jsonpath", ")", ")", ")", "\n", "\n", "self", ".", "_nms_threshold", "=", "nms_threshold", "\n", "self", ".", "_max_given_constraints", "=", "max_given_constraints", "\n", "\n"]], "home.repos.pwc.inspect_result.nocaps-org_updown-baseline.utils.constraints.ConstraintFilter.__call__": [[122, 152], ["range", "constraints.ConstraintFilter._nms", "sorted", "list", "len", "list", "constraints.ConstraintFilter.REPLACEMENTS.get", "set", "constraints.ConstraintFilter.append", "zip"], "methods", ["home.repos.pwc.inspect_result.nocaps-org_updown-baseline.utils.constraints.ConstraintFilter._nms"], ["", "def", "__call__", "(", "self", ",", "boxes", ":", "np", ".", "ndarray", ",", "class_names", ":", "List", "[", "str", "]", ",", "scores", ":", "np", ".", "ndarray", ")", "->", "List", "[", "str", "]", ":", "\n", "\n", "# Remove padding boxes (which have prediction confidence score = 0), and remove boxes", "\n", "# corresponding to all blacklisted classes. These will never become CBS constraints.", "\n", "        ", "keep_indices", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "class_names", ")", ")", ":", "\n", "            ", "if", "scores", "[", "i", "]", ">", "0", "and", "class_names", "[", "i", "]", "not", "in", "self", ".", "BLACKLIST", ":", "\n", "                ", "keep_indices", ".", "append", "(", "i", ")", "\n", "\n", "", "", "boxes", "=", "boxes", "[", "keep_indices", "]", "\n", "class_names", "=", "[", "class_names", "[", "i", "]", "for", "i", "in", "keep_indices", "]", "\n", "scores", "=", "scores", "[", "keep_indices", "]", "\n", "\n", "# Perform non-maximum suppression according to category hierarchy. For example, for highly", "\n", "# overlapping boxes on a dog, \"dog\" suppresses \"animal\".", "\n", "keep_indices", "=", "self", ".", "_nms", "(", "boxes", ",", "class_names", ")", "\n", "boxes", "=", "boxes", "[", "keep_indices", "]", "\n", "class_names", "=", "[", "class_names", "[", "i", "]", "for", "i", "in", "keep_indices", "]", "\n", "scores", "=", "scores", "[", "keep_indices", "]", "\n", "\n", "# Retain top-k constraints based on prediction confidence score.", "\n", "class_names_and_scores", "=", "sorted", "(", "list", "(", "zip", "(", "class_names", ",", "scores", ")", ")", ",", "key", "=", "lambda", "t", ":", "-", "t", "[", "1", "]", ")", "\n", "class_names_and_scores", "=", "class_names_and_scores", "[", ":", "self", ".", "_max_given_constraints", "]", "\n", "\n", "# Replace class name according to ``self.REPLACEMENTS``.", "\n", "class_names", "=", "[", "self", ".", "REPLACEMENTS", ".", "get", "(", "t", "[", "0", "]", ",", "t", "[", "0", "]", ")", "for", "t", "in", "class_names_and_scores", "]", "\n", "\n", "# Drop duplicates.", "\n", "class_names", "=", "list", "(", "set", "(", "class_names", ")", ")", "\n", "return", "class_names", "\n", "\n"]], "home.repos.pwc.inspect_result.nocaps-org_updown-baseline.utils.constraints.ConstraintFilter._nms": [[153, 206], ["numpy.array", "numpy.array.argsort", "len", "keep_box_indices.append", "numpy.maximum", "numpy.maximum", "numpy.minimum", "numpy.minimum", "numpy.logical_or", "numpy.maximum", "numpy.maximum", "anytree.search.findall", "numpy.where", "node.LabelName.lower"], "methods", ["None"], ["", "def", "_nms", "(", "self", ",", "boxes", ":", "np", ".", "ndarray", ",", "class_names", ":", "List", "[", "str", "]", ")", ":", "\n", "        ", "if", "len", "(", "class_names", ")", "==", "0", ":", "\n", "            ", "return", "[", "]", "\n", "\n", "# For object class, get the height of its corresponding node in the hierarchy tree.", "\n", "# Less height => finer-grained class name => higher score.", "\n", "", "heights", "=", "np", ".", "array", "(", "\n", "[", "\n", "findall", "(", "self", ".", "_hierarchy", ",", "lambda", "node", ":", "node", ".", "LabelName", ".", "lower", "(", ")", "in", "c", ")", "[", "0", "]", ".", "height", "\n", "for", "c", "in", "class_names", "\n", "]", "\n", ")", "\n", "# Get a sorting of the heights in ascending order, i.e. higher scores first.", "\n", "score_order", "=", "heights", ".", "argsort", "(", ")", "\n", "\n", "# Compute areas for calculating intersection over union. Add 1 to avoid division by zero", "\n", "# for zero area (padding/dummy) boxes.", "\n", "x1", ",", "y1", ",", "x2", ",", "y2", "=", "boxes", "[", ":", ",", "0", "]", ",", "boxes", "[", ":", ",", "1", "]", ",", "boxes", "[", ":", ",", "2", "]", ",", "boxes", "[", ":", ",", "3", "]", "\n", "areas", "=", "(", "x2", "-", "x1", "+", "1", ")", "*", "(", "y2", "-", "y1", "+", "1", ")", "\n", "\n", "# Fill \"keep_boxes\" with indices of boxes to keep, move from left to right in", "\n", "# ``score_order``, keep current box index (score_order[0]) and suppress (discard) other", "\n", "# indices of boxes having lower IoU threshold with current box from ``score_order``.", "\n", "# list. Note the order is a sorting of indices according to scores.", "\n", "keep_box_indices", "=", "[", "]", "\n", "\n", "while", "score_order", ".", "size", ">", "0", ":", "\n", "# Keep the index of box under consideration.", "\n", "            ", "current_index", "=", "score_order", "[", "0", "]", "\n", "keep_box_indices", ".", "append", "(", "current_index", ")", "\n", "\n", "# For the box we just decided to keep (score_order[0]), compute its IoU with other", "\n", "# boxes (score_order[1:]).", "\n", "xx1", "=", "np", ".", "maximum", "(", "x1", "[", "score_order", "[", "0", "]", "]", ",", "x1", "[", "score_order", "[", "1", ":", "]", "]", ")", "\n", "yy1", "=", "np", ".", "maximum", "(", "y1", "[", "score_order", "[", "0", "]", "]", ",", "y1", "[", "score_order", "[", "1", ":", "]", "]", ")", "\n", "xx2", "=", "np", ".", "minimum", "(", "x2", "[", "score_order", "[", "0", "]", "]", ",", "x2", "[", "score_order", "[", "1", ":", "]", "]", ")", "\n", "yy2", "=", "np", ".", "minimum", "(", "y2", "[", "score_order", "[", "0", "]", "]", ",", "y2", "[", "score_order", "[", "1", ":", "]", "]", ")", "\n", "\n", "intersection", "=", "np", ".", "maximum", "(", "0.0", ",", "xx2", "-", "xx1", "+", "1", ")", "*", "np", ".", "maximum", "(", "0.0", ",", "yy2", "-", "yy1", "+", "1", ")", "\n", "union", "=", "areas", "[", "score_order", "[", "0", "]", "]", "+", "areas", "[", "score_order", "[", "1", ":", "]", "]", "-", "intersection", "\n", "\n", "# Perform NMS for IoU >= 0.85. Check score, boxes corresponding to object", "\n", "# classes with smaller/equal height in hierarchy cannot be suppressed.", "\n", "keep_condition", "=", "np", ".", "logical_or", "(", "\n", "heights", "[", "score_order", "[", "1", ":", "]", "]", ">=", "heights", "[", "score_order", "[", "0", "]", "]", ",", "\n", "intersection", "/", "union", "<=", "self", ".", "_nms_threshold", ",", "\n", ")", "\n", "\n", "# Only keep the boxes under consideration for next iteration.", "\n", "score_order", "=", "score_order", "[", "1", ":", "]", "\n", "score_order", "=", "score_order", "[", "np", ".", "where", "(", "keep_condition", ")", "[", "0", "]", "]", "\n", "\n", "", "return", "keep_box_indices", "\n", "\n"]], "home.repos.pwc.inspect_result.nocaps-org_updown-baseline.utils.constraints.FiniteStateMachineBuilder.__init__": [[278, 299], ["open", "csv.DictReader", "row[].split"], "methods", ["None"], ["def", "__init__", "(", "\n", "self", ",", "\n", "vocabulary", ":", "Vocabulary", ",", "\n", "wordforms_tsvpath", ":", "str", ",", "\n", "max_given_constraints", ":", "int", "=", "3", ",", "\n", "max_words_per_constraint", ":", "int", "=", "3", ",", "\n", ")", ":", "\n", "        ", "self", ".", "_vocabulary", "=", "vocabulary", "\n", "self", ".", "_max_given_constraints", "=", "max_given_constraints", "\n", "self", ".", "_max_words_per_constraint", "=", "max_words_per_constraint", "\n", "\n", "self", ".", "_num_main_states", "=", "2", "**", "max_given_constraints", "\n", "self", ".", "_num_total_states", "=", "self", ".", "_num_main_states", "*", "max_words_per_constraint", "\n", "\n", "self", ".", "_wordforms", ":", "Dict", "[", "str", ",", "List", "[", "str", "]", "]", "=", "{", "}", "\n", "with", "open", "(", "wordforms_tsvpath", ",", "\"r\"", ")", "as", "wordforms_file", ":", "\n", "            ", "reader", "=", "csv", ".", "DictReader", "(", "\n", "wordforms_file", ",", "delimiter", "=", "\"\\t\"", ",", "fieldnames", "=", "[", "\"class_name\"", ",", "\"words\"", "]", "\n", ")", "\n", "for", "row", "in", "reader", ":", "\n", "                ", "self", ".", "_wordforms", "[", "row", "[", "\"class_name\"", "]", "]", "=", "row", "[", "\"words\"", "]", ".", "split", "(", "\",\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nocaps-org_updown-baseline.utils.constraints.FiniteStateMachineBuilder.build": [[300, 328], ["torch.zeros", "fsm.unsqueeze().repeat.unsqueeze().repeat.unsqueeze().repeat", "enumerate", "constraints.FiniteStateMachineBuilder._vocabulary.get_vocab_size", "constraints.FiniteStateMachineBuilder._add_nth_constraint", "fsm.unsqueeze().repeat.unsqueeze().repeat.unsqueeze", "range", "range"], "methods", ["home.repos.pwc.inspect_result.nocaps-org_updown-baseline.utils.constraints.FiniteStateMachineBuilder._add_nth_constraint"], ["", "", "", "def", "build", "(", "self", ",", "constraints", ":", "List", "[", "str", "]", ")", ":", "\n", "        ", "r\"\"\"\n        Build a finite state machine given a list of constraints.\n\n        Parameters\n        ----------\n        constraints: List[str]\n            A list of up to three (possibly) multi-word constraints, in our use-case these are\n            Open Images object class names.\n\n        Returns\n        -------\n        Tuple[torch.Tensor, int]\n            A finite state machine as an adjacency matrix, index of the next available unused\n            sub-state. This is later used to trim the unused sub-states from FSM.\n        \"\"\"", "\n", "fsm", "=", "torch", ".", "zeros", "(", "self", ".", "_num_total_states", ",", "self", ".", "_num_total_states", ",", "dtype", "=", "torch", ".", "uint8", ")", "\n", "\n", "# Self loops for all words on main states.", "\n", "fsm", "[", "range", "(", "self", ".", "_num_main_states", ")", ",", "range", "(", "self", ".", "_num_main_states", ")", "]", "=", "1", "\n", "\n", "fsm", "=", "fsm", ".", "unsqueeze", "(", "-", "1", ")", ".", "repeat", "(", "1", ",", "1", ",", "self", ".", "_vocabulary", ".", "get_vocab_size", "(", ")", ")", "\n", "\n", "substate_idx", "=", "self", ".", "_num_main_states", "\n", "for", "i", ",", "constraint", "in", "enumerate", "(", "constraints", ")", ":", "\n", "            ", "fsm", ",", "substate_idx", "=", "self", ".", "_add_nth_constraint", "(", "fsm", ",", "i", "+", "1", ",", "substate_idx", ",", "constraint", ")", "\n", "\n", "", "return", "fsm", ",", "substate_idx", "\n", "\n"]], "home.repos.pwc.inspect_result.nocaps-org_updown-baseline.utils.constraints.FiniteStateMachineBuilder._add_nth_constraint": [[329, 380], ["constraint.split", "range", "enumerate", "constraints.FiniteStateMachineBuilder._connect", "constraints.FiniteStateMachineBuilder._connect", "len"], "methods", ["home.repos.pwc.inspect_result.nocaps-org_updown-baseline.utils.constraints.FiniteStateMachineBuilder._connect", "home.repos.pwc.inspect_result.nocaps-org_updown-baseline.utils.constraints.FiniteStateMachineBuilder._connect"], ["", "def", "_add_nth_constraint", "(", "self", ",", "fsm", ":", "torch", ".", "Tensor", ",", "n", ":", "int", ",", "substate_idx", ":", "int", ",", "constraint", ":", "str", ")", ":", "\n", "        ", "r\"\"\"\n        Given an (incomplete) FSM matrix with transitions for \"(n - 1)\" constraints added, add\n        all transitions for the \"n-th\" constraint.\n\n        Parameters\n        ----------\n        fsm: torch.Tensor\n            A tensor of shape ``(num_total_states, num_total_states, vocab_size)`` representing an\n            FSM under construction.\n        n: int\n            The cardinality of constraint to be added. Goes as 1, 2, 3... (not zero-indexed).\n        substate_idx: int\n            An index which points to the next unused position for a sub-state. It starts with\n            ``(2 ** num_main_states)`` and increases according to the number of multi-word\n            constraints added so far. The calling method, :meth:`build` keeps track of this.\n        constraint: str\n            A (possibly) multi-word constraint, in our use-case it is an Open Images object class\n            name.\n\n        Returns\n        -------\n        Tuple[torch.Tensor, int]\n            FSM with added connections for the constraint and updated ``substate_idx`` pointing to\n            the next unused sub-state.\n        \"\"\"", "\n", "words", "=", "constraint", ".", "split", "(", ")", "\n", "connection_stride", "=", "2", "**", "(", "n", "-", "1", ")", "\n", "\n", "from_state", "=", "0", "\n", "while", "from_state", "<", "self", ".", "_num_main_states", ":", "\n", "            ", "for", "_", "in", "range", "(", "connection_stride", ")", ":", "\n", "                ", "word_from_state", "=", "from_state", "\n", "for", "i", ",", "word", "in", "enumerate", "(", "words", ")", ":", "\n", "# fmt: off", "\n", "# Connect to a sub-state for all words in multi-word constraint except last.", "\n", "                    ", "if", "i", "!=", "len", "(", "words", ")", "-", "1", ":", "\n", "                        ", "fsm", "=", "self", ".", "_connect", "(", "\n", "fsm", ",", "word_from_state", ",", "substate_idx", ",", "word", ",", "reset_state", "=", "from_state", "\n", ")", "\n", "word_from_state", "=", "substate_idx", "\n", "substate_idx", "+=", "1", "\n", "", "else", ":", "\n", "                        ", "fsm", "=", "self", ".", "_connect", "(", "\n", "fsm", ",", "word_from_state", ",", "from_state", "+", "connection_stride", ",", "word", ",", "\n", "reset_state", "=", "from_state", ",", "\n", ")", "\n", "# fmt: on", "\n", "", "", "from_state", "+=", "1", "\n", "", "from_state", "+=", "connection_stride", "\n", "", "return", "fsm", ",", "substate_idx", "\n", "\n"]], "home.repos.pwc.inspect_result.nocaps-org_updown-baseline.utils.constraints.FiniteStateMachineBuilder._connect": [[381, 430], ["constraints.FiniteStateMachineBuilder._vocabulary.get_token_index"], "methods", ["None"], ["", "def", "_connect", "(", "\n", "self", ",", "fsm", ":", "torch", ".", "Tensor", ",", "from_state", ":", "int", ",", "to_state", ":", "int", ",", "word", ":", "str", ",", "reset_state", ":", "int", "=", "None", "\n", ")", ":", "\n", "        ", "r\"\"\"\n        Add a connection between two states for a particular word (and all its word-forms). This\n        means removing self-loop from ``from_state`` for all word-forms of ``word`` and connecting\n        them to ``to_state``.\n        \n        Extended Summary\n        ----------------\n        In case of multi-word constraints, we return back to the ``reset_state`` for any utterance\n        other than ``word``, to satisfy a multi-word constraint if all words are decoded\n        consecutively. For example: for \"fire hydrant\" as a constraint between Q0 and Q1, we reach\n        a sub-state \"Q8\" on decoding \"fire\". Go back to main state \"Q1\" on decoding \"hydrant\"\n        immediately after, else we reset back to main state \"Q0\".\n\n        Parameters\n        ----------\n        fsm: torch.Tensor\n            A tensor of shape ``(num_total_states, num_total_states, vocab_size)`` representing an\n            FSM under construction.\n        from_state: int\n            Origin state to make a state transition.\n        to_state: int\n            Destination state to make a state transition.\n        word: str\n            The word which serves as a constraint for transition between given two states.\n        reset_state: int, optional (default = None)\n           State to reset otherwise. This is only valid if ``from_state`` is a sub-state.\n\n        Returns\n        -------\n        torch.Tensor\n            FSM with the added connection.\n        \"\"\"", "\n", "wordforms", "=", "self", ".", "_wordforms", "[", "word", "]", "\n", "wordform_indices", "=", "[", "self", ".", "_vocabulary", ".", "get_token_index", "(", "w", ")", "for", "w", "in", "wordforms", "]", "\n", "\n", "for", "wordform_index", "in", "wordform_indices", ":", "\n", "            ", "fsm", "[", "from_state", ",", "to_state", ",", "wordform_index", "]", "=", "1", "\n", "fsm", "[", "from_state", ",", "from_state", ",", "wordform_index", "]", "=", "0", "\n", "\n", "", "if", "reset_state", "is", "not", "None", ":", "\n", "            ", "fsm", "[", "from_state", ",", "from_state", ",", ":", "]", "=", "0", "\n", "fsm", "[", "from_state", ",", "reset_state", ",", ":", "]", "=", "1", "\n", "for", "wordform_index", "in", "wordform_indices", ":", "\n", "                ", "fsm", "[", "from_state", ",", "reset_state", ",", "wordform_index", "]", "=", "0", "\n", "\n", "", "", "return", "fsm", "\n", "", "", ""]], "home.repos.pwc.inspect_result.nocaps-org_updown-baseline.utils.constraints.add_constraint_words_to_vocabulary": [[17, 52], ["open", "csv.DictReader", "row[].split", "word.split", "vocabulary.add_token_to_namespace"], "function", ["None"], ["def", "add_constraint_words_to_vocabulary", "(", "\n", "vocabulary", ":", "Vocabulary", ",", "wordforms_tsvpath", ":", "str", ",", "namespace", ":", "str", "=", "\"tokens\"", "\n", ")", "->", "Vocabulary", ":", "\n", "    ", "r\"\"\"\n    Expand the :class:`~allennlp.data.vocabulary.Vocabulary` with CBS constraint words. We do not\n    need to worry about duplicate words in constraints and caption vocabulary. AllenNLP avoids\n    duplicates automatically.\n\n    Parameters\n    ----------\n    vocabulary: allennlp.data.vocabulary.Vocabulary\n        The vocabulary to be expanded with provided words.\n    wordforms_tsvpath: str\n        Path to a TSV file containing two fields: first is the name of Open Images object class\n        and second field is a comma separated list of words (possibly singular and plural forms\n        of the word etc.) which could be CBS constraints.\n    namespace: str, optional (default=\"tokens\")\n        The namespace of :class:`~allennlp.data.vocabulary.Vocabulary` to add these words.\n\n    Returns\n    -------\n    allennlp.data.vocabulary.Vocabulary\n        The expanded :class:`~allennlp.data.vocabulary.Vocabulary` with all the words added.\n    \"\"\"", "\n", "\n", "with", "open", "(", "wordforms_tsvpath", ",", "\"r\"", ")", "as", "wordforms_file", ":", "\n", "        ", "reader", "=", "csv", ".", "DictReader", "(", "wordforms_file", ",", "delimiter", "=", "\"\\t\"", ",", "fieldnames", "=", "[", "\"class_name\"", ",", "\"words\"", "]", ")", "\n", "for", "row", "in", "reader", ":", "\n", "            ", "for", "word", "in", "row", "[", "\"words\"", "]", ".", "split", "(", "\",\"", ")", ":", "\n", "# Constraint words can be \"multi-word\" (may have more than one tokens).", "\n", "# Add all tokens to the vocabulary separately.", "\n", "                ", "for", "w", "in", "word", ".", "split", "(", ")", ":", "\n", "                    ", "vocabulary", ".", "add_token_to_namespace", "(", "w", ",", "namespace", ")", "\n", "\n", "", "", "", "", "return", "vocabulary", "\n", "\n"]], "home.repos.pwc.inspect_result.nocaps-org_updown-baseline.utils.evalai.NocapsEvaluator.__init__": [[38, 43], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "phase", ":", "str", "=", "\"val\"", ")", ":", "\n", "\n", "# Constants specific to EvalAI.", "\n", "        ", "self", ".", "_challenge_id", "=", "355", "\n", "self", ".", "_phase_id", "=", "742", "if", "phase", "==", "\"val\"", "else", "743", "\n", "\n"]], "home.repos.pwc.inspect_result.nocaps-org_updown-baseline.utils.evalai.NocapsEvaluator.evaluate": [[44, 149], ["tempfile.mkstemp", "subprocess.Popen", "[].decode", "re.search", "json.loads", "collections.defaultdict", "json.loads.items", "open", "json.dump", "submission_command.split", "print", "print", "time.sleep", "subprocess.check_output().decode", "val.items", "re.search.group().split", "evalai.NocapsEvaluator.evaluate", "ConnectionError", "subprocess.Popen.communicate", "subprocess.check_output", "re.search.group"], "methods", ["home.repos.pwc.inspect_result.nocaps-org_updown-baseline.modules.cbs.ConstrainedBeamSearch.search", "home.repos.pwc.inspect_result.nocaps-org_updown-baseline.updown.config.Config.dump", "home.repos.pwc.inspect_result.nocaps-org_updown-baseline.utils.evalai.NocapsEvaluator.evaluate"], ["", "def", "evaluate", "(", "\n", "self", ",", "predictions", ":", "List", "[", "Prediction", "]", ",", "iteration", ":", "Optional", "[", "int", "]", "=", "None", "\n", ")", "->", "Dict", "[", "str", ",", "Dict", "[", "str", ",", "float", "]", "]", ":", "\n", "        ", "r\"\"\"\n        Take the model predictions (in COCO format), submit them to EvalAI, and retrieve model\n        performance based on captioning metrics.\n\n        Parameters\n        ----------\n        predictions: List[Prediction]\n            Model predictions in COCO format. They are a list of dicts with keys\n            ``{\"image_id\": int, \"caption\": str}``.\n        iteration: int, optional (default = None)\n            Training iteration where the checkpoint was evaluated.\n\n        Returns\n        -------\n        Dict[str, Dict[str, float]]\n            Model performance based on all captioning metrics. Nested dict structure::\n\n                {\n                    \"B1\": {\"in-domain\", \"near-domain\", \"out-domain\", \"entire\"},  # BLEU-1\n                    \"B2\": {\"in-domain\", \"near-domain\", \"out-domain\", \"entire\"},  # BLEU-2\n                    \"B3\": {\"in-domain\", \"near-domain\", \"out-domain\", \"entire\"},  # BLEU-3\n                    \"B4\": {\"in-domain\", \"near-domain\", \"out-domain\", \"entire\"},  # BLEU-4\n                    \"METEOR\": {\"in-domain\", \"near-domain\", \"out-domain\", \"entire\"},\n                    \"ROUGE-L\": {\"in-domain\", \"near-domain\", \"out-domain\", \"entire\"},\n                    \"CIDEr\": {\"in-domain\", \"near-domain\", \"out-domain\", \"entire\"},\n                    \"SPICE\": {\"in-domain\", \"near-domain\", \"out-domain\", \"entire\"},\n                }\n\n        \"\"\"", "\n", "# Save predictions as a json file first.", "\n", "_", ",", "predictions_filename", "=", "tempfile", ".", "mkstemp", "(", "suffix", "=", "\".json\"", ",", "text", "=", "True", ")", "\n", "with", "open", "(", "predictions_filename", ",", "\"w\"", ")", "as", "f", ":", "\n", "            ", "json", ".", "dump", "(", "predictions", ",", "f", ")", "\n", "\n", "", "submission_command", "=", "(", "\n", "f\"evalai challenge {self._challenge_id} phase {self._phase_id} \"", "\n", "f\"submit --file {predictions_filename}\"", "\n", ")", "\n", "\n", "submission_command_subprocess", "=", "subprocess", ".", "Popen", "(", "\n", "submission_command", ".", "split", "(", ")", ",", "\n", "stdout", "=", "subprocess", ".", "PIPE", ",", "\n", "stdin", "=", "subprocess", ".", "PIPE", ",", "\n", "stderr", "=", "subprocess", ".", "STDOUT", ",", "\n", ")", "\n", "\n", "# This terminal output will have submission ID we need to check.", "\n", "submission_command_stdout", "=", "submission_command_subprocess", ".", "communicate", "(", "input", "=", "b\"N\\n\"", ")", "[", "\n", "0", "\n", "]", ".", "decode", "(", "\"utf-8\"", ")", "\n", "\n", "submission_id_regex", "=", "re", ".", "search", "(", "\"evalai submission ([0-9]+)\"", ",", "submission_command_stdout", ")", "\n", "try", ":", "\n", "# Get an integer submission ID (as a string).", "\n", "            ", "submission_id", "=", "submission_id_regex", ".", "group", "(", "0", ")", ".", "split", "(", ")", "[", "-", "1", "]", "# type: ignore", "\n", "", "except", ":", "\n", "# Very unlikely, but submission may fail because of some glitch. Retry for that.", "\n", "            ", "return", "self", ".", "evaluate", "(", "predictions", ")", "\n", "\n", "", "if", "iteration", "is", "not", "None", ":", "\n", "            ", "print", "(", "f\"Submitted predictions for iteration {iteration}, submission id: {submission_id}.\"", ")", "\n", "", "else", ":", "\n", "            ", "print", "(", "f\"Submitted predictions, submission_id: {submission_id}\"", ")", "\n", "\n", "# Placeholder stdout for a pending submission.", "\n", "", "result_stdout", ":", "str", "=", "\"The Submission is yet to be evaluated.\"", "\n", "num_tries", ":", "int", "=", "0", "\n", "\n", "# Query every 10 seconds for result until it appears.", "\n", "while", "\"CIDEr\"", "not", "in", "result_stdout", ":", "\n", "\n", "            ", "time", ".", "sleep", "(", "10", ")", "\n", "result_stdout", "=", "subprocess", ".", "check_output", "(", "\n", "[", "\"evalai\"", ",", "\"submission\"", ",", "submission_id", ",", "\"result\"", "]", "\n", ")", ".", "decode", "(", "\"utf-8\"", ")", "\n", "num_tries", "+=", "1", "\n", "\n", "# Raise error if it takes more than 5 minutes.", "\n", "if", "num_tries", "==", "30", ":", "\n", "                ", "raise", "ConnectionError", "(", "\"Unable to get results from EvalAI within 5 minutes!\"", ")", "\n", "\n", "# Convert result to json.", "\n", "", "", "metrics", "=", "json", ".", "loads", "(", "result_stdout", ",", "encoding", "=", "\"utf-8\"", ")", "\n", "\n", "# keys: {\"in-domain\", \"near-domain\", \"out-domain\", \"entire\"}", "\n", "# In each of these, keys: {\"B1\", \"B2\", \"B3\", \"B4\", \"METEOR\", \"ROUGE-L\", \"CIDEr\", \"SPICE\"}", "\n", "metrics", "=", "{", "\n", "\"in-domain\"", ":", "metrics", "[", "0", "]", "[", "\"in-domain\"", "]", ",", "\n", "\"near-domain\"", ":", "metrics", "[", "1", "]", "[", "\"near-domain\"", "]", ",", "\n", "\"out-domain\"", ":", "metrics", "[", "2", "]", "[", "\"out-domain\"", "]", ",", "\n", "\"entire\"", ":", "metrics", "[", "3", "]", "[", "\"entire\"", "]", ",", "\n", "}", "\n", "\n", "# Restructure the metrics dict for better tensorboard logging.", "\n", "# keys: {\"B1\", \"B2\", \"B3\", \"B4\", \"METEOR\", \"ROUGE-L\", \"CIDEr\", \"SPICE\"}", "\n", "# In each of these, keys: keys: {\"in-domain\", \"near-domain\", \"out-domain\", \"entire\"}", "\n", "flipped_metrics", ":", "Dict", "[", "str", ",", "Dict", "[", "str", ",", "float", "]", "]", "=", "defaultdict", "(", "dict", ")", "\n", "for", "key", ",", "val", "in", "metrics", ".", "items", "(", ")", ":", "\n", "            ", "for", "subkey", ",", "subval", "in", "val", ".", "items", "(", ")", ":", "\n", "                ", "flipped_metrics", "[", "subkey", "]", "[", "key", "]", "=", "subval", "\n", "\n", "", "", "return", "flipped_metrics", "\n", "", "", ""]], "home.repos.pwc.inspect_result.nocaps-org_updown-baseline.modules.cbs.ConstrainedBeamSearch.__init__": [[47, 58], ["None"], "methods", ["None"], ["def", "__init__", "(", "\n", "self", ",", "\n", "end_index", ":", "int", ",", "\n", "max_steps", ":", "int", "=", "20", ",", "\n", "beam_size", ":", "int", "=", "5", ",", "\n", "per_node_beam_size", ":", "Optional", "[", "int", "]", "=", "None", ",", "\n", ")", ":", "\n", "        ", "self", ".", "_end_index", "=", "end_index", "\n", "self", ".", "max_steps", "=", "max_steps", "\n", "self", ".", "beam_size", "=", "beam_size", "\n", "self", ".", "per_node_beam_size", "=", "per_node_beam_size", "or", "self", ".", "beam_size", "\n", "\n"]], "home.repos.pwc.inspect_result.nocaps-org_updown-baseline.modules.cbs.ConstrainedBeamSearch.search": [[59, 278], ["fsm.size", "step", "start_class_log_probabilities.size", "start_class_log_probabilities.view().expand", "start_state_predictions.masked_fill.masked_fill.masked_fill", "start_state_predictions.masked_fill.masked_fill.topk", "predictions.append", "torch.full().to", "fsm.view().expand", "range", "range", "predictions[].gather().unsqueeze", "reconstructed_predictions.append", "torch.cat", "all_predictions.view.view.view", "float", "start_predicted_classes.view", "cbs._enlarge_single_tensor", "predictions[].reshape", "step", "predictions[].reshape.view().unsqueeze().expand", "torch.where", "cleaned_log_probabilities.view.view.view", "torch.LongTensor().to", "torch.FloatTensor().to", "torch.LongTensor().to", "torch.FloatTensor().to.view.view().expand", "range", "restricted_predicted_classes.view.view.view", "predictions.append", "backpointers.append", "torch.FloatTensor().to.view", "predictions[].unsqueeze", "predictions[].gather().unsqueeze", "reconstructed_predictions.append", "backpointers[].gather", "list", "start_class_log_probabilities.view", "torch.full", "state.items", "fsm.view", "state_log_probabilities.masked_fill.masked_fill.masked_fill", "state_log_probabilities.masked_fill.masked_fill.topk", "summed_top_log_probabilities.reshape", "predicted_classes.reshape", "summed_top_log_probabilities.reshape.topk", "predicted_classes.reshape.gather", "backpointer.view", "state_tensor.size", "backpointer.view().expand", "state_tensor.reshape().gather().reshape", "cbs.ConstrainedBeamSearch.search.track_back_state"], "methods", ["home.repos.pwc.inspect_result.nocaps-org_updown-baseline.utils.checkpointing.CheckpointManager.step", "home.repos.pwc.inspect_result.nocaps-org_updown-baseline.modules.cbs._enlarge_single_tensor", "home.repos.pwc.inspect_result.nocaps-org_updown-baseline.utils.checkpointing.CheckpointManager.step"], ["", "def", "search", "(", "\n", "self", ",", "\n", "start_predictions", ":", "torch", ".", "Tensor", ",", "\n", "start_state", ":", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ",", "\n", "step", ":", "StepFunctionType", ",", "\n", "fsm", ":", "torch", ".", "Tensor", ",", "\n", ")", "->", "Tuple", "[", "torch", ".", "Tensor", ",", "torch", ".", "Tensor", "]", ":", "\n", "        ", "r\"\"\"\n        Given a starting state, a step function, and an FSM adjacency matrix, apply Constrained\n        Beam Search to find most likely target sequences satisfying specified constraints in FSM.\n\n        .. note::\n\n            If your step function returns ``-inf`` for some log probabilities\n            (like if you're using a masked log-softmax) then some of the \"best\"\n            sequences returned may also have ``-inf`` log probability. Specifically\n            this happens when the beam size is smaller than the number of actions\n            with finite log probability (non-zero probability) returned by the step function.\n            Therefore if you're using a mask you may want to check the results from ``search``\n            and potentially discard sequences with non-finite log probability.\n\n        Parameters\n        ----------\n        start_predictions : torch.Tensor\n            A tensor containing the initial predictions with shape ``(batch_size, )``. These are\n            usually just ``@@BOUNDARY@@`` token indices.\n        start_state : ``Dict[str, torch.Tensor]``\n            The initial state passed to the ``step`` function. Each value of the state dict\n            should be a tensor of shape ``(batch_size, *)``, where ``*`` means any other\n            number of dimensions.\n        step : ``StepFunctionType``\n            A function that is responsible for computing the next most likely tokens, given the\n            current state and the predictions from the last time step. The function should accept\n            two arguments. The first being a tensor of shape ``(group_size,)``, representing the\n            index of the predicted tokens from the last time step, and the second being the\n            current state. The ``group_size`` will be ``batch_size * beam_size * num_fsm_states``\n            except in the initial step, for which it will just be ``batch_size``. The function is\n            expected to return a tuple, where the first element is a tensor of shape\n            ``(group_size, vocab_size)`` containing the log probabilities of the tokens for the\n            next step, and the second element is the updated state. The tensor in the state should\n            have shape ``(group_size, *)``, where ``*`` means any other number of dimensions.\n\n        Returns\n        -------\n        Tuple[torch.Tensor, torch.Tensor]\n            Tuple of ``(predictions, log_probabilities)``, where ``predictions``\n            has shape ``(batch_size, num_fsm_states, beam_size, max_steps)``\n            and ``log_probabilities`` has shape ``(batch_size, num_fsm_states, beam_size)``.\n        \"\"\"", "\n", "# shape: (batch_size, num_fsm_states, num_fsm_states, vocab_size)", "\n", "batch_size", ",", "num_fsm_states", ",", "_", ",", "vocab_size", "=", "fsm", ".", "size", "(", ")", "\n", "\n", "# List of (batch_size, num_fsm_states, beam_size) tensors. One for each time step. Does not", "\n", "# include the start symbols, which are implicit.", "\n", "predictions", ":", "List", "[", "torch", ".", "Tensor", "]", "=", "[", "]", "\n", "\n", "# List of (batch_size, num_fsm_states, beam_size) tensors. One for each time step. None for", "\n", "# the first. Stores the index n for the parent prediction.", "\n", "backpointers", ":", "List", "[", "torch", ".", "Tensor", "]", "=", "[", "]", "\n", "\n", "# Calculate the first timestep. This is done outside the main loop because we are going", "\n", "# from a single decoder input (the output from the encoder) to the top `beam_size`", "\n", "# decoder outputs per FSM state. On the other hand, within the main loop we are going", "\n", "# from the `beam_size` elements of the beam (per FSM state) to `beam_size`^2 candidates", "\n", "# from which we will select the top `beam_size` elements for the next iteration.", "\n", "\n", "# shape: start_class_log_probabilities (batch_size, vocab_size)", "\n", "# shape: state[\"h1\"], state[\"c1\"]... etc. (batch_size, hidden_size)", "\n", "start_class_log_probabilities", ",", "state", "=", "step", "(", "start_predictions", ",", "start_state", ")", "\n", "vocab_size", "=", "start_class_log_probabilities", ".", "size", "(", "-", "1", ")", "\n", "\n", "start_state_predictions", "=", "start_class_log_probabilities", ".", "view", "(", "\n", "batch_size", ",", "1", ",", "vocab_size", "\n", ")", ".", "expand", "(", "batch_size", ",", "num_fsm_states", ",", "vocab_size", ")", "\n", "\n", "start_state_predictions", "=", "start_state_predictions", ".", "masked_fill", "(", "\n", "1", "-", "fsm", "[", ":", ",", "0", ",", ":", ",", ":", "]", ",", "float", "(", "\"-inf\"", ")", "\n", ")", "\n", "\n", "# (batch_size, num_fsm_states, beam_size)", "\n", "start_top_log_probabilities", ",", "start_predicted_classes", "=", "start_state_predictions", ".", "topk", "(", "\n", "self", ".", "beam_size", "\n", ")", "\n", "# shape: (batch_size, num_fsm_states, beam_size)", "\n", "last_log_probabilities", "=", "start_top_log_probabilities", "\n", "\n", "predictions", ".", "append", "(", "start_predicted_classes", ".", "view", "(", "batch_size", ",", "-", "1", ")", ")", "\n", "\n", "log_probs_after_end", "=", "torch", ".", "full", "(", "(", "1", ",", "vocab_size", ")", ",", "float", "(", "\"-inf\"", ")", ")", ".", "to", "(", "\n", "start_predictions", ".", "device", "\n", ")", "\n", "log_probs_after_end", "[", ":", ",", "self", ".", "_end_index", "]", "=", "0.0", "\n", "\n", "state", "=", "{", "\n", "key", ":", "_enlarge_single_tensor", "(", "value", ",", "batch_size", ",", "num_fsm_states", ",", "self", ".", "beam_size", ")", "\n", "for", "(", "key", ",", "value", ")", "in", "state", ".", "items", "(", ")", "\n", "}", "\n", "\n", "step_state_mask", "=", "fsm", ".", "view", "(", "\n", "batch_size", ",", "num_fsm_states", ",", "num_fsm_states", ",", "1", ",", "vocab_size", "\n", ")", ".", "expand", "(", "batch_size", ",", "num_fsm_states", ",", "num_fsm_states", ",", "self", ".", "beam_size", ",", "vocab_size", ")", "\n", "\n", "for", "timestep", "in", "range", "(", "self", ".", "max_steps", "-", "1", ")", ":", "\n", "# shape: (batch_size * beam_size * num_fsm_states, )", "\n", "            ", "last_predictions", "=", "predictions", "[", "-", "1", "]", ".", "reshape", "(", "\n", "batch_size", "*", "self", ".", "beam_size", "*", "num_fsm_states", "\n", ")", "\n", "\n", "if", "(", "last_predictions", "==", "self", ".", "_end_index", ")", ".", "all", "(", ")", ":", "\n", "                ", "break", "\n", "\n", "", "class_log_probabilities", ",", "state", "=", "step", "(", "last_predictions", ",", "state", ")", "\n", "last_predictions_expanded", "=", "(", "\n", "last_predictions", ".", "view", "(", "-", "1", ")", "\n", ".", "unsqueeze", "(", "-", "1", ")", "\n", ".", "expand", "(", "batch_size", "*", "num_fsm_states", "*", "self", ".", "beam_size", ",", "vocab_size", ")", "\n", ")", "\n", "\n", "cleaned_log_probabilities", "=", "torch", ".", "where", "(", "\n", "last_predictions_expanded", "==", "self", ".", "_end_index", ",", "\n", "log_probs_after_end", ",", "\n", "class_log_probabilities", ",", "\n", ")", "\n", "cleaned_log_probabilities", "=", "cleaned_log_probabilities", ".", "view", "(", "\n", "batch_size", ",", "num_fsm_states", ",", "self", ".", "beam_size", ",", "vocab_size", "\n", ")", "\n", "\n", "restricted_predicted_classes", "=", "torch", ".", "LongTensor", "(", "\n", "batch_size", ",", "num_fsm_states", ",", "self", ".", "beam_size", "\n", ")", ".", "to", "(", "start_predictions", ".", "device", ")", "\n", "restricted_beam_log_probs", "=", "torch", ".", "FloatTensor", "(", "\n", "batch_size", ",", "num_fsm_states", ",", "self", ".", "beam_size", "\n", ")", ".", "to", "(", "start_predictions", ".", "device", ")", "\n", "restricted_beam_indices", "=", "torch", ".", "LongTensor", "(", "\n", "batch_size", ",", "num_fsm_states", ",", "self", ".", "beam_size", "\n", ")", ".", "to", "(", "start_predictions", ".", "device", ")", "\n", "\n", "expanded_last_log_probabilities", "=", "last_log_probabilities", ".", "view", "(", "\n", "batch_size", ",", "num_fsm_states", ",", "self", ".", "beam_size", ",", "1", "\n", ")", ".", "expand", "(", "batch_size", ",", "num_fsm_states", ",", "self", ".", "beam_size", ",", "self", ".", "per_node_beam_size", ")", "\n", "\n", "for", "i", "in", "range", "(", "num_fsm_states", ")", ":", "\n", "# shape (batch_size, num_fsm_states, self.beam_size, vocab_size)", "\n", "                ", "state_log_probabilities", "=", "cleaned_log_probabilities", "\n", "\n", "state_log_probabilities", "=", "state_log_probabilities", ".", "masked_fill", "(", "\n", "1", "-", "step_state_mask", "[", ":", ",", ":", ",", "i", ",", ":", ",", ":", "]", ",", "-", "1e20", "\n", ")", "\n", "top_log_probabilities", ",", "predicted_classes", "=", "state_log_probabilities", ".", "topk", "(", "\n", "self", ".", "per_node_beam_size", "\n", ")", "\n", "summed_top_log_probabilities", "=", "(", "\n", "top_log_probabilities", "+", "expanded_last_log_probabilities", "\n", ")", "\n", "# shape: (batch_size, old_num_fsm_states * beam_size * per_node_beam_size)", "\n", "reshaped_summed", "=", "summed_top_log_probabilities", ".", "reshape", "(", "batch_size", ",", "-", "1", ")", "\n", "\n", "# shape: (batch_size, old_num_fsm_states * beam_size * per_node_beam_size)", "\n", "reshaped_predicted_classes", "=", "predicted_classes", ".", "reshape", "(", "batch_size", ",", "-", "1", ")", "\n", "\n", "# shape (batch_size, beam_size)", "\n", "state_beam_log_probs", ",", "state_beam_indices", "=", "reshaped_summed", ".", "topk", "(", "self", ".", "beam_size", ")", "\n", "# shape (batch_size, beam_size)", "\n", "state_predicted_classes", "=", "reshaped_predicted_classes", ".", "gather", "(", "1", ",", "state_beam_indices", ")", "\n", "\n", "restricted_predicted_classes", "[", ":", ",", "i", ",", ":", "]", "=", "state_predicted_classes", "\n", "restricted_beam_indices", "[", ":", ",", "i", ",", ":", "]", "=", "state_beam_indices", "\n", "restricted_beam_log_probs", "[", ":", ",", "i", ",", ":", "]", "=", "state_beam_log_probs", "\n", "\n", "", "restricted_predicted_classes", "=", "restricted_predicted_classes", ".", "view", "(", "batch_size", ",", "-", "1", ")", "\n", "predictions", ".", "append", "(", "restricted_predicted_classes", ")", "\n", "\n", "backpointer", "=", "restricted_beam_indices", "/", "self", ".", "per_node_beam_size", "\n", "backpointers", ".", "append", "(", "backpointer", ".", "view", "(", "batch_size", ",", "-", "1", ")", ")", "\n", "\n", "last_log_probabilities", "=", "restricted_beam_log_probs", ".", "view", "(", "batch_size", ",", "num_fsm_states", ",", "-", "1", ")", "\n", "\n", "def", "track_back_state", "(", "state_tensor", ")", ":", "\n", "                ", "_", ",", "*", "last_dims", "=", "state_tensor", ".", "size", "(", ")", "\n", "# shape: (batch_size, beam_size, *)", "\n", "expanded_backpointer", "=", "backpointer", ".", "view", "(", "\n", "batch_size", ",", "num_fsm_states", "*", "self", ".", "beam_size", ",", "*", "(", "[", "1", "]", "*", "len", "(", "last_dims", ")", ")", "\n", ")", ".", "expand", "(", "batch_size", ",", "num_fsm_states", "*", "self", ".", "beam_size", ",", "*", "last_dims", ")", "\n", "\n", "# shape: (batch_size * beam_size, *)", "\n", "return", "(", "\n", "state_tensor", ".", "reshape", "(", "batch_size", ",", "num_fsm_states", "*", "self", ".", "beam_size", ",", "*", "last_dims", ")", "\n", ".", "gather", "(", "1", ",", "expanded_backpointer", ")", "\n", ".", "reshape", "(", "batch_size", "*", "num_fsm_states", "*", "self", ".", "beam_size", ",", "*", "last_dims", ")", "\n", ")", "\n", "\n", "", "state", "=", "{", "key", ":", "track_back_state", "(", "value", ")", "for", "(", "key", ",", "value", ")", "in", "state", ".", "items", "(", ")", "}", "\n", "\n", "# Reconstruct the sequences.", "\n", "# shape: [(batch_size, beam_size, 1)]", "\n", "", "reconstructed_predictions", "=", "[", "predictions", "[", "-", "1", "]", ".", "unsqueeze", "(", "2", ")", "]", "\n", "\n", "# shape: (batch_size, beam_size)", "\n", "cur_backpointers", "=", "backpointers", "[", "-", "1", "]", "\n", "\n", "for", "timestep", "in", "range", "(", "len", "(", "predictions", ")", "-", "2", ",", "0", ",", "-", "1", ")", ":", "\n", "# shape: (batch_size, beam_size, 1)", "\n", "            ", "cur_preds", "=", "predictions", "[", "timestep", "]", ".", "gather", "(", "1", ",", "cur_backpointers", ")", ".", "unsqueeze", "(", "2", ")", "\n", "\n", "reconstructed_predictions", ".", "append", "(", "cur_preds", ")", "\n", "\n", "# shape: (batch_size, beam_size)", "\n", "cur_backpointers", "=", "backpointers", "[", "timestep", "-", "1", "]", ".", "gather", "(", "1", ",", "cur_backpointers", ")", "\n", "\n", "# shape: (batch_size, beam_size, 1)", "\n", "", "final_preds", "=", "predictions", "[", "0", "]", ".", "gather", "(", "1", ",", "cur_backpointers", ")", ".", "unsqueeze", "(", "2", ")", "\n", "\n", "reconstructed_predictions", ".", "append", "(", "final_preds", ")", "\n", "\n", "# shape: (batch_size, beam_size, max_steps)", "\n", "all_predictions", "=", "torch", ".", "cat", "(", "list", "(", "reversed", "(", "reconstructed_predictions", ")", ")", ",", "2", ")", "\n", "all_predictions", "=", "all_predictions", ".", "view", "(", "batch_size", ",", "num_fsm_states", ",", "self", ".", "beam_size", ",", "-", "1", ")", "\n", "\n", "return", "all_predictions", ",", "last_log_probabilities", "\n", "", "", ""]], "home.repos.pwc.inspect_result.nocaps-org_updown-baseline.modules.cbs._enlarge_single_tensor": [[10, 17], ["t.size", "t.view().expand().reshape", "t.view().expand", "t.view"], "function", ["None"], ["def", "_enlarge_single_tensor", "(", "t", ",", "batch_size", ",", "num_fsm_states", ",", "beam_size", ")", ":", "\n", "# shape: (batch_size * beam_size, *)", "\n", "    ", "_", ",", "*", "last_dims", "=", "t", ".", "size", "(", ")", "\n", "return", "(", "\n", "t", ".", "view", "(", "batch_size", ",", "1", ",", "1", ",", "*", "last_dims", ")", "\n", ".", "expand", "(", "batch_size", ",", "num_fsm_states", ",", "beam_size", ",", "*", "last_dims", ")", "\n", ".", "reshape", "(", "-", "1", ",", "*", "last_dims", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.nocaps-org_updown-baseline.modules.attention.BottomUpTopDownAttention.__init__": [[27, 35], ["torch.nn.Module.__init__", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear"], "methods", ["home.repos.pwc.inspect_result.nocaps-org_updown-baseline.data.datasets.EvaluationDatasetWithConstraints.__init__"], ["def", "__init__", "(", "self", ",", "query_size", ":", "int", ",", "image_feature_size", ":", "int", ",", "projection_size", ":", "int", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "_query_vector_projection_layer", "=", "nn", ".", "Linear", "(", "query_size", ",", "projection_size", ",", "bias", "=", "False", ")", "\n", "self", ".", "_image_features_projection_layer", "=", "nn", ".", "Linear", "(", "\n", "image_feature_size", ",", "projection_size", ",", "bias", "=", "False", "\n", ")", "\n", "self", ".", "_attention_layer", "=", "nn", ".", "Linear", "(", "projection_size", ",", "1", ",", "bias", "=", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nocaps-org_updown-baseline.modules.attention.BottomUpTopDownAttention.forward": [[36, 98], ["attention.BottomUpTopDownAttention._query_vector_projection_layer", "attention.BottomUpTopDownAttention._project_image_features", "projected_query_vector.unsqueeze().repeat.unsqueeze().repeat.unsqueeze().repeat", "attention.BottomUpTopDownAttention._attention_layer", "attention_logits.squeeze.squeeze.squeeze", "attention.BottomUpTopDownAttention.size", "torch.tanh", "allennlp.nn.util.masked_softmax", "torch.softmax", "projected_query_vector.unsqueeze().repeat.unsqueeze().repeat.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.nocaps-org_updown-baseline.modules.attention.BottomUpTopDownAttention._project_image_features"], ["", "def", "forward", "(", "\n", "self", ",", "\n", "query_vector", ":", "torch", ".", "Tensor", ",", "\n", "image_features", ":", "torch", ".", "Tensor", ",", "\n", "image_features_mask", ":", "Optional", "[", "torch", ".", "Tensor", "]", "=", "None", ",", "\n", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "r\"\"\"\n        Compute attention weights over image features by applying bottom-up top-down attention\n        over image features, using the query vector. Query vector is typically the output of\n        attention LSTM in :class:`~updown.modules.updown_cell.UpDownCell`. Both image features\n        and query vectors are first projected to a common dimension, that is ``projection_size``.\n\n        Parameters\n        ----------\n        query_vector: torch.Tensor\n            A tensor of shape ``(batch_size, query_size)`` used for attending the image features.\n        image_features: torch.Tensor\n            A tensor of shape ``(batch_size, num_boxes, image_feature_size)``. ``num_boxes`` for\n            each instance in a batch might be different. Instances with lesser boxes are padded\n            with zeros up to ``num_boxes``.\n        image_features_mask: torch.Tensor\n            A mask over image features if ``num_boxes`` are different for each instance. Elements\n            where mask is zero are not attended over.\n\n        Returns\n        -------\n        torch.Tensor\n            A tensor of shape ``(batch_size, num_boxes)`` containing attention weights for each\n            image features of each instance in the batch. If ``image_features_mask`` is provided\n            (for adaptive features), then weights where the mask is zero, would be zero.\n        \"\"\"", "\n", "\n", "# shape: (batch_size, projection_size)", "\n", "projected_query_vector", "=", "self", ".", "_query_vector_projection_layer", "(", "query_vector", ")", "\n", "\n", "# Image features are projected by a method call, which is decorated using LRU cache, to", "\n", "# save some computation. Refer method docstring.", "\n", "# shape: (batch_size, num_boxes, projection_size)", "\n", "projected_image_features", "=", "self", ".", "_project_image_features", "(", "image_features", ")", "\n", "\n", "# Broadcast query_vector as image_features for addition.", "\n", "# shape: (batch_size, num_boxes, projection_size)", "\n", "projected_query_vector", "=", "projected_query_vector", ".", "unsqueeze", "(", "1", ")", ".", "repeat", "(", "\n", "1", ",", "projected_image_features", ".", "size", "(", "1", ")", ",", "1", "\n", ")", "\n", "\n", "# shape: (batch_size, num_boxes, 1)", "\n", "attention_logits", "=", "self", ".", "_attention_layer", "(", "\n", "torch", ".", "tanh", "(", "projected_query_vector", "+", "projected_image_features", ")", "\n", ")", "\n", "\n", "# shape: (batch_size, num_boxes)", "\n", "attention_logits", "=", "attention_logits", ".", "squeeze", "(", "-", "1", ")", "\n", "\n", "# `\\alpha`s as importance weights for boxes (rows) in the `image_features`.", "\n", "# shape: (batch_size, num_boxes)", "\n", "if", "image_features_mask", "is", "not", "None", ":", "\n", "            ", "attention_weights", "=", "masked_softmax", "(", "attention_logits", ",", "image_features_mask", ",", "dim", "=", "-", "1", ")", "\n", "", "else", ":", "\n", "            ", "attention_weights", "=", "torch", ".", "softmax", "(", "attention_logits", ",", "dim", "=", "-", "1", ")", "\n", "\n", "", "return", "attention_weights", "\n", "\n"]], "home.repos.pwc.inspect_result.nocaps-org_updown-baseline.modules.attention.BottomUpTopDownAttention._project_image_features": [[99, 126], ["functools.lru_cache", "attention.BottomUpTopDownAttention._image_features_projection_layer"], "methods", ["None"], ["", "@", "lru_cache", "(", "maxsize", "=", "10", ")", "\n", "def", "_project_image_features", "(", "self", ",", "image_features", ":", "torch", ".", "Tensor", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "r\"\"\"\n        Project image features to a common dimension for applying attention.\n\n        Extended Summary\n        ----------------\n        For a single training/evaluation instance, the image features remain the same from first\n        time-step to maximum decoding steps. To keep a clean API, we use LRU cache -- which would\n        maintain a cache of last 10 return values because on call signature, and not actually\n        execute itself if it is called with the same image features seen at least once in last\n        10 calls. This saves some computation.\n\n        Parameters\n        ----------\n        image_features: torch.Tensor\n            A tensor of shape ``(batch_size, num_boxes, image_feature_size)``. ``num_boxes`` for\n            each instance in a batch might be different. Instances with lesser boxes are padded\n            with zeros up to ``num_boxes``.\n\n        Returns\n        -------\n        torch.Tensor\n            Projected image features of shape ``(batch_size, num_boxes, image_feature_size)``.\n        \"\"\"", "\n", "\n", "return", "self", ".", "_image_features_projection_layer", "(", "image_features", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.nocaps-org_updown-baseline.modules.updown_cell.UpDownCell.__init__": [[61, 83], ["torch.nn.Module.__init__", "torch.nn.LSTMCell", "updown.modules.attention.BottomUpTopDownAttention", "torch.nn.LSTMCell"], "methods", ["home.repos.pwc.inspect_result.nocaps-org_updown-baseline.data.datasets.EvaluationDatasetWithConstraints.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "image_feature_size", ":", "int", ",", "\n", "embedding_size", ":", "int", ",", "\n", "hidden_size", ":", "int", ",", "\n", "attention_projection_size", ":", "int", ",", "\n", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "image_feature_size", "=", "image_feature_size", "\n", "self", ".", "embedding_size", "=", "embedding_size", "\n", "self", ".", "hidden_size", "=", "hidden_size", "\n", "self", ".", "attention_projection_size", "=", "attention_projection_size", "\n", "\n", "self", ".", "_attention_lstm_cell", "=", "nn", ".", "LSTMCell", "(", "\n", "self", ".", "embedding_size", "+", "self", ".", "image_feature_size", "+", "2", "*", "self", ".", "hidden_size", ",", "self", ".", "hidden_size", "\n", ")", "\n", "self", ".", "_butd_attention", "=", "BottomUpTopDownAttention", "(", "\n", "self", ".", "hidden_size", ",", "self", ".", "image_feature_size", ",", "self", ".", "attention_projection_size", "\n", ")", "\n", "self", ".", "_language_lstm_cell", "=", "nn", ".", "LSTMCell", "(", "\n", "self", ".", "image_feature_size", "+", "2", "*", "self", ".", "hidden_size", ",", "self", ".", "hidden_size", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.nocaps-org_updown-baseline.modules.updown_cell.UpDownCell.forward": [[85, 160], ["image_features.size", "updown_cell.UpDownCell._average_image_features", "torch.cat", "updown_cell.UpDownCell._attention_lstm_cell", "updown_cell.UpDownCell._butd_attention", "torch.sum", "torch.cat", "updown_cell.UpDownCell._language_lstm_cell", "image_features.new_zeros", "image_features.new_zeros.clone", "image_features.new_zeros.clone", "image_features.new_zeros.clone", "image_features.new_zeros.clone", "updown_cell.UpDownCell.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.nocaps-org_updown-baseline.modules.updown_cell.UpDownCell._average_image_features"], ["", "def", "forward", "(", "\n", "self", ",", "\n", "image_features", ":", "torch", ".", "Tensor", ",", "\n", "token_embedding", ":", "torch", ".", "Tensor", ",", "\n", "states", ":", "Optional", "[", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", "]", "=", "None", ",", "\n", ")", "->", "Tuple", "[", "torch", ".", "Tensor", ",", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", "]", ":", "\n", "        ", "r\"\"\"\n        Given image features, input token embeddings of current time-step and LSTM states,\n        predict output token embeddings for next time-step and update states. This behaves\n        very similar to :class:`~torch.nn.LSTMCell`.\n\n        Parameters\n        ----------\n        image_features: torch.Tensor\n            A tensor of shape ``(batch_size, num_boxes, image_feature_size)``. ``num_boxes`` for\n            each instance in a batch might be different. Instances with lesser boxes are padded\n            with zeros up to ``num_boxes``.\n        token_embedding: torch.Tensor\n            A tensor of shape ``(batch_size, embedding_size)`` containing token embeddings for a\n            particular time-step.\n        states: Dict[str, torch.Tensor], optional (default = None)\n            A dict with keys ``{\"h1\", \"c1\", \"h2\", \"c2\"}`` of LSTM states: (h1, c1) for Attention\n            LSTM and (h2, c2) for Language LSTM. If not provided (at first time-step), these are\n            initialized as zeros.\n\n        Returns\n        -------\n        Tuple[torch.Tensor, Dict[str, torch.Tensor]]\n            A tensor of shape ``(batch_size, hidden_state)`` with output token embedding, which\n            is the updated state \"h2\", and updated states (h1, c1), (h2, c2).\n        \"\"\"", "\n", "batch_size", "=", "image_features", ".", "size", "(", "0", ")", "\n", "\n", "# Average pooling of image features happens only at the first timestep. LRU cache", "\n", "# saves compute by not executing the function call in subsequent timesteps.", "\n", "# shape: (batch_size, image_feature_size), (batch_size, num_boxes)", "\n", "averaged_image_features", ",", "image_features_mask", "=", "self", ".", "_average_image_features", "(", "image_features", ")", "\n", "\n", "# Initialize (h1, c1), (h2, c2) if not passed.", "\n", "if", "states", "is", "None", ":", "\n", "            ", "state", "=", "image_features", ".", "new_zeros", "(", "(", "batch_size", ",", "self", ".", "hidden_size", ")", ")", "\n", "states", "=", "{", "\n", "\"h1\"", ":", "state", ".", "clone", "(", ")", ",", "\n", "\"c1\"", ":", "state", ".", "clone", "(", ")", ",", "\n", "\"h2\"", ":", "state", ".", "clone", "(", ")", ",", "\n", "\"c2\"", ":", "state", ".", "clone", "(", ")", ",", "\n", "}", "\n", "\n", "# shape: (batch_size, embedding_size + image_feature_size + 2 * hidden_size)", "\n", "", "attention_lstm_cell_input", "=", "torch", ".", "cat", "(", "\n", "[", "token_embedding", ",", "averaged_image_features", ",", "states", "[", "\"h1\"", "]", ",", "states", "[", "\"h2\"", "]", "]", ",", "dim", "=", "1", "\n", ")", "\n", "states", "[", "\"h1\"", "]", ",", "states", "[", "\"c1\"", "]", "=", "self", ".", "_attention_lstm_cell", "(", "\n", "attention_lstm_cell_input", ",", "(", "states", "[", "\"h1\"", "]", ",", "states", "[", "\"c1\"", "]", ")", "\n", ")", "\n", "\n", "# shape: (batch_size, num_boxes)", "\n", "attention_weights", "=", "self", ".", "_butd_attention", "(", "\n", "states", "[", "\"h1\"", "]", ",", "image_features", ",", "image_features_mask", "=", "image_features_mask", "\n", ")", "\n", "\n", "# shape: (batch_size, image_feature_size)", "\n", "attended_image_features", "=", "torch", ".", "sum", "(", "\n", "attention_weights", ".", "unsqueeze", "(", "-", "1", ")", "*", "image_features", ",", "dim", "=", "1", "\n", ")", "\n", "\n", "# shape: (batch_size, image_feature_size + 2 * hidden_size)", "\n", "language_lstm_cell_input", "=", "torch", ".", "cat", "(", "\n", "[", "attended_image_features", ",", "states", "[", "\"h1\"", "]", ",", "states", "[", "\"h2\"", "]", "]", ",", "dim", "=", "1", "\n", ")", "\n", "states", "[", "\"h2\"", "]", ",", "states", "[", "\"c2\"", "]", "=", "self", ".", "_language_lstm_cell", "(", "\n", "language_lstm_cell_input", ",", "(", "states", "[", "\"h2\"", "]", ",", "states", "[", "\"c2\"", "]", ")", "\n", ")", "\n", "\n", "return", "states", "[", "\"h2\"", "]", ",", "states", "\n", "\n"]], "home.repos.pwc.inspect_result.nocaps-org_updown-baseline.modules.updown_cell.UpDownCell._average_image_features": [[161, 199], ["functools.lru_cache", "allennlp.nn.util.masked_mean", "torch.sum", "image_features_mask.unsqueeze", "torch.abs"], "methods", ["None"], ["", "@", "lru_cache", "(", "maxsize", "=", "10", ")", "\n", "def", "_average_image_features", "(", "\n", "self", ",", "image_features", ":", "torch", ".", "Tensor", "\n", ")", "->", "Tuple", "[", "torch", ".", "Tensor", ",", "torch", ".", "Tensor", "]", ":", "\n", "        ", "r\"\"\"\n        Perform mean pooling of bottom-up image features, while taking care of variable\n        ``num_boxes`` in case of adaptive features.\n\n        Extended Summary\n        ----------------\n        For a single training/evaluation instance, the image features remain the same from first\n        time-step to maximum decoding steps. To keep a clean API, we use LRU cache -- which would\n        maintain a cache of last 10 return values because on call signature, and not actually\n        execute itself if it is called with the same image features seen at least once in last\n        10 calls. This saves some computation.\n\n        Parameters\n        ----------\n        image_features: torch.Tensor\n            A tensor of shape ``(batch_size, num_boxes, image_feature_size)``. ``num_boxes`` for\n            each instance in a batch might be different. Instances with lesser boxes are padded\n            with zeros up to ``num_boxes``.\n\n        Returns\n        -------\n        Tuple[torch.Tensor, torch.Tensor]\n            Averaged image features of shape ``(batch_size, image_feature_size)`` and a binary\n            mask of shape ``(batch_size, num_boxes)`` which is zero for padded features.\n        \"\"\"", "\n", "# shape: (batch_size, num_boxes)", "\n", "image_features_mask", "=", "torch", ".", "sum", "(", "torch", ".", "abs", "(", "image_features", ")", ",", "dim", "=", "-", "1", ")", ">", "0", "\n", "\n", "# shape: (batch_size, image_feature_size)", "\n", "averaged_image_features", "=", "masked_mean", "(", "\n", "image_features", ",", "image_features_mask", ".", "unsqueeze", "(", "-", "1", ")", ",", "dim", "=", "1", "\n", ")", "\n", "\n", "return", "averaged_image_features", ",", "image_features_mask", "\n", "", "", ""]], "home.repos.pwc.inspect_result.nocaps-org_updown-baseline.models.updown_captioner.UpDownCaptioner.__init__": [[57, 128], ["torch.nn.Module.__init__", "vocabulary.get_vocab_size", "vocabulary.get_token_index", "vocabulary.get_token_index", "updown.modules.UpDownCell", "torch.nn.LogSoftmax", "BeamSearchClass", "updown_captioner.UpDownCaptioner._initialize_glove", "torch.nn.Embedding.from_pretrained", "torch.nn.Embedding", "torch.nn.Sequential", "torch.nn.Linear", "torch.nn.Identity", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Tanh"], "methods", ["home.repos.pwc.inspect_result.nocaps-org_updown-baseline.data.datasets.EvaluationDatasetWithConstraints.__init__", "home.repos.pwc.inspect_result.nocaps-org_updown-baseline.models.updown_captioner.UpDownCaptioner._initialize_glove"], ["def", "__init__", "(", "\n", "self", ",", "\n", "vocabulary", ":", "Vocabulary", ",", "\n", "image_feature_size", ":", "int", ",", "\n", "embedding_size", ":", "int", ",", "\n", "hidden_size", ":", "int", ",", "\n", "attention_projection_size", ":", "int", ",", "\n", "max_caption_length", ":", "int", "=", "20", ",", "\n", "beam_size", ":", "int", "=", "1", ",", "\n", "use_cbs", ":", "bool", "=", "False", ",", "\n", "min_constraints_to_satisfy", ":", "int", "=", "2", ",", "\n", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "_vocabulary", "=", "vocabulary", "\n", "\n", "self", ".", "image_feature_size", "=", "image_feature_size", "\n", "self", ".", "embedding_size", "=", "embedding_size", "\n", "self", ".", "hidden_size", "=", "hidden_size", "\n", "self", ".", "attention_projection_size", "=", "attention_projection_size", "\n", "\n", "self", ".", "_max_caption_length", "=", "max_caption_length", "\n", "self", ".", "_use_cbs", "=", "use_cbs", "\n", "self", ".", "_min_constraints_to_satisfy", "=", "min_constraints_to_satisfy", "\n", "\n", "# Short hand variable names for convenience", "\n", "_vocab_size", "=", "vocabulary", ".", "get_vocab_size", "(", ")", "\n", "self", ".", "_pad_index", "=", "vocabulary", ".", "get_token_index", "(", "\"@@UNKNOWN@@\"", ")", "\n", "self", ".", "_boundary_index", "=", "vocabulary", ".", "get_token_index", "(", "\"@@BOUNDARY@@\"", ")", "\n", "\n", "# Initialize embedding layer with GloVe embeddings and freeze it if the specified size", "\n", "# is 300. CBS cannot be supported for any other embedding size, using CBS is optional", "\n", "# with embedding size 300. So in either cases, embeddig size is the deciding factor.", "\n", "if", "self", ".", "embedding_size", "==", "300", ":", "\n", "            ", "glove_vectors", "=", "self", ".", "_initialize_glove", "(", ")", "\n", "self", ".", "_embedding_layer", "=", "nn", ".", "Embedding", ".", "from_pretrained", "(", "\n", "glove_vectors", ",", "freeze", "=", "True", ",", "padding_idx", "=", "self", ".", "_pad_index", "\n", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "_embedding_layer", "=", "nn", ".", "Embedding", "(", "\n", "_vocab_size", ",", "embedding_size", ",", "padding_idx", "=", "self", ".", "_pad_index", "\n", ")", "\n", "assert", "not", "use_cbs", ",", "\"CBS is not supported without Frozen GloVe embeddings (300d), \"", "\n", "f\"found embedding size to be {self.embedding_size}.\"", "\n", "\n", "", "self", ".", "_updown_cell", "=", "UpDownCell", "(", "\n", "image_feature_size", ",", "embedding_size", ",", "hidden_size", ",", "attention_projection_size", "\n", ")", "\n", "\n", "if", "self", ".", "embedding_size", "==", "300", ":", "\n", "# Tie the input and output word embeddings when using frozen GloVe embeddings.", "\n", "# In this case, project hidden states to GloVe dimension (with a non-linearity).", "\n", "            ", "self", ".", "_output_projection", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Linear", "(", "hidden_size", ",", "self", ".", "embedding_size", ")", ",", "nn", ".", "Tanh", "(", ")", "\n", ")", "\n", "self", ".", "_output_layer", "=", "nn", ".", "Linear", "(", "self", ".", "embedding_size", ",", "_vocab_size", ",", "bias", "=", "False", ")", "\n", "self", ".", "_output_layer", ".", "weight", "=", "self", ".", "_embedding_layer", ".", "weight", "\n", "", "else", ":", "\n", "# Else don't tie them when learning embeddings during training.", "\n", "# In this case, project hidden states directly to output vocab space.", "\n", "            ", "self", ".", "_output_projection", "=", "nn", ".", "Identity", "(", ")", "# type: ignore", "\n", "self", ".", "_output_layer", "=", "nn", ".", "Linear", "(", "hidden_size", ",", "_vocab_size", ")", "\n", "\n", "", "self", ".", "_log_softmax", "=", "nn", ".", "LogSoftmax", "(", "dim", "=", "1", ")", "\n", "\n", "# We use beam search to find the most likely caption during inference.", "\n", "BeamSearchClass", "=", "ConstrainedBeamSearch", "if", "use_cbs", "else", "BeamSearch", "\n", "self", ".", "_beam_search", "=", "BeamSearchClass", "(", "\n", "self", ".", "_boundary_index", ",", "\n", "max_steps", "=", "max_caption_length", ",", "\n", "beam_size", "=", "beam_size", ",", "\n", "per_node_beam_size", "=", "beam_size", "//", "2", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.nocaps-org_updown-baseline.models.updown_captioner.UpDownCaptioner.from_config": [[130, 146], ["kwargs.pop", "cls"], "methods", ["None"], ["", "@", "classmethod", "\n", "def", "from_config", "(", "cls", ",", "config", ":", "Config", ",", "**", "kwargs", ")", ":", "\n", "        ", "r\"\"\"Instantiate this class directly from a :class:`~updown.config.Config`.\"\"\"", "\n", "_C", "=", "config", "\n", "\n", "vocabulary", "=", "kwargs", ".", "pop", "(", "\"vocabulary\"", ")", "\n", "return", "cls", "(", "\n", "vocabulary", "=", "vocabulary", ",", "\n", "image_feature_size", "=", "_C", ".", "MODEL", ".", "IMAGE_FEATURE_SIZE", ",", "\n", "embedding_size", "=", "_C", ".", "MODEL", ".", "EMBEDDING_SIZE", ",", "\n", "hidden_size", "=", "_C", ".", "MODEL", ".", "HIDDEN_SIZE", ",", "\n", "attention_projection_size", "=", "_C", ".", "MODEL", ".", "ATTENTION_PROJECTION_SIZE", ",", "\n", "beam_size", "=", "_C", ".", "MODEL", ".", "BEAM_SIZE", ",", "\n", "max_caption_length", "=", "_C", ".", "DATA", ".", "MAX_CAPTION_LENGTH", ",", "\n", "use_cbs", "=", "_C", ".", "MODEL", ".", "USE_CBS", ",", "\n", "min_constraints_to_satisfy", "=", "_C", ".", "MODEL", ".", "MIN_CONSTRAINTS_TO_SATISFY", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.nocaps-org_updown-baseline.models.updown_captioner.UpDownCaptioner._initialize_glove": [[148, 178], ["torchtext.vocab.GloVe", "torch.zeros", "updown_captioner.UpDownCaptioner._vocabulary.get_token_to_index_vocabulary().items", "updown_captioner.UpDownCaptioner._vocabulary.get_vocab_size", "updown_captioner.UpDownCaptioner._vocabulary.get_token_to_index_vocabulary", "torch.randn"], "methods", ["None"], ["", "def", "_initialize_glove", "(", "self", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "r\"\"\"\n        Initialize embeddings of all the tokens in a given\n        :class:`~allennlp.data.vocabulary.Vocabulary` by their GloVe vectors.\n\n        Extended Summary\n        ----------------\n        It is recommended to train an :class:`~updown.models.updown_captioner.UpDownCaptioner` with\n        frozen word embeddings when one wishes to perform Constrained Beam Search decoding during\n        inference. This is because the constraint words may not appear in caption vocabulary (out of\n        domain), and their embeddings will never be updated during training. Initializing with frozen\n        GloVe embeddings is helpful, because they capture more meaningful semantics than randomly\n        initialized embeddings.\n\n        Returns\n        -------\n        torch.Tensor\n            GloVe Embeddings corresponding to tokens.\n        \"\"\"", "\n", "glove", "=", "GloVe", "(", "name", "=", "\"42B\"", ",", "dim", "=", "300", ")", "\n", "glove_vectors", "=", "torch", ".", "zeros", "(", "self", ".", "_vocabulary", ".", "get_vocab_size", "(", ")", ",", "300", ")", "\n", "\n", "for", "word", ",", "i", "in", "self", ".", "_vocabulary", ".", "get_token_to_index_vocabulary", "(", ")", ".", "items", "(", ")", ":", "\n", "            ", "if", "word", "in", "glove", ".", "stoi", ":", "\n", "                ", "glove_vectors", "[", "i", "]", "=", "glove", ".", "vectors", "[", "glove", ".", "stoi", "[", "word", "]", "]", "\n", "", "elif", "word", "!=", "self", ".", "_pad_index", ":", "\n", "# Initialize by random vector.", "\n", "                ", "glove_vectors", "[", "i", "]", "=", "2", "*", "torch", ".", "randn", "(", "300", ")", "-", "1", "\n", "\n", "", "", "return", "glove_vectors", "\n", "\n"]], "home.repos.pwc.inspect_result.nocaps-org_updown-baseline.models.updown_captioner.UpDownCaptioner.forward": [[179, 287], ["image_features.size", "allennlp.nn.util.add_sentence_boundary_token_ids", "caption_tokens.size", "range", "torch.cat", "image_features.new_full().long", "functools.partial", "updown_captioner.UpDownCaptioner._decode_step", "step_logits.append", "updown_captioner.UpDownCaptioner._get_loss", "updown_captioner.UpDownCaptioner._beam_search.search", "updown.utils.decoding.select_best_beam_with_constraints", "updown_captioner.UpDownCaptioner._beam_search.search", "updown.utils.decoding.select_best_beam", "output_logits.unsqueeze", "caption_tokens[].contiguous", "tokens_mask[].contiguous", "image_features.new_full"], "methods", ["home.repos.pwc.inspect_result.nocaps-org_updown-baseline.models.updown_captioner.UpDownCaptioner._decode_step", "home.repos.pwc.inspect_result.nocaps-org_updown-baseline.models.updown_captioner.UpDownCaptioner._get_loss", "home.repos.pwc.inspect_result.nocaps-org_updown-baseline.modules.cbs.ConstrainedBeamSearch.search", "home.repos.pwc.inspect_result.nocaps-org_updown-baseline.utils.decoding.select_best_beam_with_constraints", "home.repos.pwc.inspect_result.nocaps-org_updown-baseline.modules.cbs.ConstrainedBeamSearch.search", "home.repos.pwc.inspect_result.nocaps-org_updown-baseline.utils.decoding.select_best_beam"], ["", "def", "forward", "(", "# type: ignore", "\n", "self", ",", "\n", "image_features", ":", "torch", ".", "Tensor", ",", "\n", "caption_tokens", ":", "Optional", "[", "torch", ".", "Tensor", "]", "=", "None", ",", "\n", "fsm", ":", "torch", ".", "Tensor", "=", "None", ",", "\n", "num_constraints", ":", "torch", ".", "Tensor", "=", "None", ",", "\n", ")", "->", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ":", "\n", "        ", "r\"\"\"\n        Given bottom-up image features, maximize the likelihood of paired captions during\n        training. During evaluation, decode captions given image features using beam search.\n\n        Parameters\n        ----------\n        image_features: torch.Tensor\n            A tensor of shape ``(batch_size, num_boxes * image_feature_size)``. ``num_boxes`` for\n            each instance in a batch might be different. Instances with lesser boxes are padded\n            with zeros up to ``num_boxes``.\n        caption_tokens: torch.Tensor, optional (default = None)\n            A tensor of shape ``(batch_size, max_caption_length)`` of tokenized captions. This\n            tensor does not contain ``@@BOUNDARY@@`` tokens yet. Captions are not provided\n            during evaluation.\n        fsm: torch.Tensor, optional (default = None)\n            A tensor of shape ``(batch_size, num_states, num_states, vocab_size)``: finite state\n            machines per instance, represented as adjacency matrix. For a particular instance\n            ``[_, s1, s2, v] = 1`` shows a transition from state ``s1`` to ``s2`` on decoding\n            ``v`` token (constraint). Would be ``None`` for regular beam search decoding.\n        num_constraints: torch.Tensor, optional (default = None)\n            A tensor of shape ``(batch_size, )`` containing the total number of given constraints\n            for CBS. Would be ``None`` for regular beam search decoding.\n\n        Returns\n        -------\n        Dict[str, torch.Tensor]\n            Decoded captions and/or per-instance cross entropy loss, dict with keys either\n            ``{\"predictions\"}`` or ``{\"loss\"}``.\n        \"\"\"", "\n", "batch_size", ",", "num_boxes", ",", "image_feature_size", "=", "image_features", ".", "size", "(", ")", "\n", "\n", "# Initialize states at zero-th timestep.", "\n", "states", "=", "None", "\n", "\n", "if", "self", ".", "training", "and", "caption_tokens", "is", "not", "None", ":", "\n", "# Add \"@@BOUNDARY@@\" tokens to caption sequences.", "\n", "            ", "caption_tokens", ",", "_", "=", "add_sentence_boundary_token_ids", "(", "\n", "caption_tokens", ",", "\n", "(", "caption_tokens", "!=", "self", ".", "_pad_index", ")", ",", "\n", "self", ".", "_boundary_index", ",", "\n", "self", ".", "_boundary_index", ",", "\n", ")", "\n", "batch_size", ",", "max_caption_length", "=", "caption_tokens", ".", "size", "(", ")", "\n", "\n", "# shape: (batch_size, max_caption_length)", "\n", "tokens_mask", "=", "caption_tokens", "!=", "self", ".", "_pad_index", "\n", "\n", "# The last input from the target is either padding or the boundary token.", "\n", "# Either way, we don't have to process it.", "\n", "num_decoding_steps", "=", "max_caption_length", "-", "1", "\n", "\n", "step_logits", ":", "List", "[", "torch", ".", "Tensor", "]", "=", "[", "]", "\n", "for", "timestep", "in", "range", "(", "num_decoding_steps", ")", ":", "\n", "# shape: (batch_size,)", "\n", "                ", "input_tokens", "=", "caption_tokens", "[", ":", ",", "timestep", "]", "\n", "\n", "# shape: (batch_size, num_classes)", "\n", "output_logits", ",", "states", "=", "self", ".", "_decode_step", "(", "image_features", ",", "input_tokens", ",", "states", ")", "\n", "\n", "# list of tensors, shape: (batch_size, 1, vocab_size)", "\n", "step_logits", ".", "append", "(", "output_logits", ".", "unsqueeze", "(", "1", ")", ")", "\n", "\n", "# shape: (batch_size, num_decoding_steps)", "\n", "", "logits", "=", "torch", ".", "cat", "(", "step_logits", ",", "1", ")", "\n", "\n", "# Skip first time-step from targets for calculating loss.", "\n", "output_dict", "=", "{", "\n", "\"loss\"", ":", "self", ".", "_get_loss", "(", "\n", "logits", ",", "caption_tokens", "[", ":", ",", "1", ":", "]", ".", "contiguous", "(", ")", ",", "tokens_mask", "[", ":", ",", "1", ":", "]", ".", "contiguous", "(", ")", "\n", ")", "\n", "}", "\n", "", "else", ":", "\n", "            ", "num_decoding_steps", "=", "self", ".", "_max_caption_length", "\n", "start_predictions", "=", "image_features", ".", "new_full", "(", "(", "batch_size", ",", ")", ",", "self", ".", "_boundary_index", ")", ".", "long", "(", ")", "\n", "\n", "# Add image features as a default argument to match callable signature acceptable by", "\n", "# beam search class (previous predictions and states only).", "\n", "beam_decode_step", "=", "functools", ".", "partial", "(", "self", ".", "_decode_step", ",", "image_features", ")", "\n", "\n", "# shape (all_top_k_predictions): (batch_size, net_beam_size, num_decoding_steps)", "\n", "# shape (log_probabilities): (batch_size, net_beam_size)", "\n", "if", "self", ".", "_use_cbs", ":", "\n", "                ", "all_top_k_predictions", ",", "log_probabilities", "=", "self", ".", "_beam_search", ".", "search", "(", "\n", "start_predictions", ",", "states", ",", "beam_decode_step", ",", "fsm", "\n", ")", "\n", "best_beam", "=", "select_best_beam_with_constraints", "(", "\n", "all_top_k_predictions", ",", "\n", "log_probabilities", ",", "\n", "num_constraints", ",", "\n", "self", ".", "_min_constraints_to_satisfy", ",", "\n", ")", "\n", "", "else", ":", "\n", "                ", "all_top_k_predictions", ",", "log_probabilities", "=", "self", ".", "_beam_search", ".", "search", "(", "\n", "start_predictions", ",", "states", ",", "beam_decode_step", "\n", ")", "\n", "best_beam", "=", "select_best_beam", "(", "all_top_k_predictions", ",", "log_probabilities", ")", "\n", "\n", "# shape: (batch_size, num_decoding_steps)", "\n", "", "output_dict", "=", "{", "\"predictions\"", ":", "best_beam", "}", "\n", "\n", "", "return", "output_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.nocaps-org_updown-baseline.models.updown_captioner.UpDownCaptioner._decode_step": [[288, 349], ["updown_captioner.UpDownCaptioner._embedding_layer", "updown_captioner.UpDownCaptioner._updown_cell", "updown_captioner.UpDownCaptioner._output_projection", "updown_captioner.UpDownCaptioner._output_layer", "image_features.view.view.size", "int", "image_features.view.view.unsqueeze().repeat", "image_features.view.view.view", "updown_captioner.UpDownCaptioner._log_softmax", "image_features.view.view.size", "previous_predictions.size", "previous_predictions.size", "image_features.view.view.unsqueeze"], "methods", ["None"], ["", "def", "_decode_step", "(", "\n", "self", ",", "\n", "image_features", ":", "torch", ".", "Tensor", ",", "\n", "previous_predictions", ":", "torch", ".", "Tensor", ",", "\n", "states", ":", "Optional", "[", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", "]", "=", "None", ",", "\n", ")", "->", "Tuple", "[", "torch", ".", "Tensor", ",", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", "]", ":", "\n", "        ", "r\"\"\"\n        Given image features, tokens predicted at previous time-step and LSTM states of the\n        :class:`~updown.modules.updown_cell.UpDownCell`, take a decoding step. This is also\n        called by the beam search class.\n\n        Parameters\n        ----------\n        image_features: torch.Tensor\n            A tensor of shape ``(batch_size, num_boxes, image_feature_size)``.\n        previous_predictions: torch.Tensor\n            A tensor of shape ``(batch_size * net_beam_size, )`` containing tokens predicted at\n            previous time-step -- one for each beam, for each instances in a batch.\n            ``net_beam_size`` is 1 during teacher forcing (training), ``beam_size`` for regular\n            :class:`allennlp.nn.beam_search.BeamSearch` and ``beam_size * num_states`` for\n            :class:`updown.modules.cbs.ConstrainedBeamSearch`\n\n        states: [Dict[str, torch.Tensor], optional (default = None)\n            LSTM states of the :class:`~updown.modules.updown_cell.UpDownCell`. These are\n            initialized as zero tensors if not provided (at first time-step).\n        \"\"\"", "\n", "net_beam_size", "=", "1", "\n", "\n", "# Expand and repeat image features while doing beam search (during inference).", "\n", "if", "not", "self", ".", "training", "and", "image_features", ".", "size", "(", "0", ")", "!=", "previous_predictions", ".", "size", "(", "0", ")", ":", "\n", "\n", "            ", "batch_size", ",", "num_boxes", ",", "image_feature_size", "=", "image_features", ".", "size", "(", ")", "\n", "net_beam_size", "=", "int", "(", "previous_predictions", ".", "size", "(", "0", ")", "/", "batch_size", ")", "\n", "\n", "# Add (net) beam dimension and repeat image features.", "\n", "image_features", "=", "image_features", ".", "unsqueeze", "(", "1", ")", ".", "repeat", "(", "1", ",", "net_beam_size", ",", "1", ",", "1", ")", "\n", "\n", "# shape: (batch_size * net_beam_size, num_boxes, image_feature_size)", "\n", "image_features", "=", "image_features", ".", "view", "(", "\n", "batch_size", "*", "net_beam_size", ",", "num_boxes", ",", "image_feature_size", "\n", ")", "\n", "\n", "# shape: (batch_size * net_beam_size, )", "\n", "", "current_input", "=", "previous_predictions", "\n", "\n", "# shape: (batch_size * net_beam_size, embedding_size)", "\n", "token_embeddings", "=", "self", ".", "_embedding_layer", "(", "current_input", ")", "\n", "\n", "# shape: (batch_size * net_beam_size, hidden_size)", "\n", "updown_output", ",", "states", "=", "self", ".", "_updown_cell", "(", "image_features", ",", "token_embeddings", ",", "states", ")", "\n", "\n", "# shape: (batch_size * net_beam_size, vocab_size)", "\n", "updown_output", "=", "self", ".", "_output_projection", "(", "updown_output", ")", "\n", "output_logits", "=", "self", ".", "_output_layer", "(", "updown_output", ")", "\n", "\n", "# Return logits while training, to further calculate cross entropy loss.", "\n", "# Return logprobs during inference, because beam search needs them.", "\n", "# Note:: This means NO BEAM SEARCH DURING TRAINING.", "\n", "outputs", "=", "output_logits", "if", "self", ".", "training", "else", "self", ".", "_log_softmax", "(", "output_logits", ")", "\n", "\n", "return", "outputs", ",", "states", "# type: ignore", "\n", "\n"]], "home.repos.pwc.inspect_result.nocaps-org_updown-baseline.models.updown_captioner.UpDownCaptioner._get_loss": [[350, 383], ["torch.sum().float", "allennlp.nn.util.sequence_cross_entropy_with_logits", "torch.sum"], "methods", ["None"], ["", "def", "_get_loss", "(", "\n", "self", ",", "logits", ":", "torch", ".", "Tensor", ",", "targets", ":", "torch", ".", "Tensor", ",", "target_mask", ":", "torch", ".", "Tensor", "\n", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "r\"\"\"\n        Compute cross entropy loss of predicted caption (logits) w.r.t. target caption. The cross\n        entropy loss of caption is cross entropy loss at each time-step, summed.\n\n        Parameters\n        ----------\n        logits: torch.Tensor\n            A tensor of shape ``(batch_size, max_caption_length - 1, vocab_size)`` containing\n            unnormalized log-probabilities of predicted captions.\n        targets: torch.Tensor\n            A tensor of shape ``(batch_size, max_caption_length - 1)`` of tokenized target\n            captions.\n        target_mask: torch.Tensor\n            A mask over target captions, elements where mask is zero are ignored from loss\n            computation. Here, we ignore ``@@UNKNOWN@@`` token (and hence padding tokens too\n            because they are basically the same).\n\n        Returns\n        -------\n        torch.Tensor\n            A tensor of shape ``(batch_size, )`` containing cross entropy loss of captions, summed\n            across time-steps.\n        \"\"\"", "\n", "\n", "# shape: (batch_size, )", "\n", "target_lengths", "=", "torch", ".", "sum", "(", "target_mask", ",", "dim", "=", "-", "1", ")", ".", "float", "(", ")", "\n", "\n", "# shape: (batch_size, )", "\n", "return", "target_lengths", "*", "sequence_cross_entropy_with_logits", "(", "\n", "logits", ",", "targets", ",", "target_mask", ",", "average", "=", "None", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.nocaps-org_updown-baseline.data.readers.ImageFeaturesReader.__init__": [[46, 78], ["print", "h5py.File", "tqdm.tqdm.tqdm", "h5py.File.close", "h5py.File", "numpy.array", "range", "range", "range"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "features_h5path", ":", "str", ",", "in_memory", ":", "bool", "=", "False", ")", "->", "None", ":", "\n", "        ", "self", ".", "features_h5path", "=", "features_h5path", "\n", "self", ".", "_in_memory", "=", "in_memory", "\n", "\n", "# Keys are all the image ids, values depend on ``self._in_memory``.", "\n", "# If ``self._in_memory`` is True, values are image features corresponding to the image id.", "\n", "# Else values will be integers; indices in the files to read features from.", "\n", "self", ".", "_map", ":", "Dict", "[", "int", ",", "Union", "[", "int", ",", "np", ".", "ndarray", "]", "]", "=", "{", "}", "\n", "self", ".", "_num_boxes", ":", "Dict", "[", "int", ",", "int", "]", "=", "{", "}", "\n", "\n", "if", "self", ".", "_in_memory", ":", "\n", "            ", "print", "(", "f\"Loading image features from {self.features_h5path}...\"", ")", "\n", "features_h5", "=", "h5py", ".", "File", "(", "self", ".", "features_h5path", ",", "\"r\"", ")", "\n", "\n", "# If loading all features in memory at once, keep a mapping of image id to features.", "\n", "for", "index", "in", "tqdm", "(", "range", "(", "features_h5", "[", "\"image_id\"", "]", ".", "shape", "[", "0", "]", ")", ")", ":", "\n", "                ", "self", ".", "_map", "[", "features_h5", "[", "\"image_id\"", "]", "[", "index", "]", "]", "=", "features_h5", "[", "\"features\"", "]", "[", "index", "]", "\n", "self", ".", "_num_boxes", "[", "features_h5", "[", "\"image_id\"", "]", "[", "index", "]", "]", "=", "features_h5", "[", "\"num_boxes\"", "]", "[", "index", "]", "\n", "\n", "", "features_h5", ".", "close", "(", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "features_h5", "=", "h5py", ".", "File", "(", "self", ".", "features_h5path", ",", "\"r\"", ")", "\n", "image_id_np", "=", "np", ".", "array", "(", "self", ".", "features_h5", "[", "\"image_id\"", "]", ")", "\n", "\n", "# If not loading all features in memory at once, just keep a mapping of image id to", "\n", "# index of features in H5 file.", "\n", "self", ".", "_map", "=", "{", "image_id_np", "[", "index", "]", ":", "index", "for", "index", "in", "range", "(", "image_id_np", ".", "shape", "[", "0", "]", ")", "}", "\n", "\n", "# Load the number of boxes for each image anyway, there's not bulky.", "\n", "self", ".", "_num_boxes", "=", "{", "\n", "image_id_np", "[", "index", "]", ":", "self", ".", "features_h5", "[", "\"num_boxes\"", "]", "[", "index", "]", "\n", "for", "index", "in", "range", "(", "image_id_np", ".", "shape", "[", "0", "]", ")", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.nocaps-org_updown-baseline.data.readers.ImageFeaturesReader.__len__": [[80, 82], ["len"], "methods", ["None"], ["", "", "def", "__len__", "(", "self", ")", "->", "int", ":", "\n", "        ", "return", "len", "(", "self", ".", "_map", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nocaps-org_updown-baseline.data.readers.ImageFeaturesReader.__getitem__": [[83, 92], ["image_id_features.reshape"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "image_id", ":", "int", ")", "->", "np", ".", "ndarray", ":", "\n", "        ", "if", "self", ".", "_in_memory", ":", "\n", "            ", "image_id_features", "=", "self", ".", "_map", "[", "image_id", "]", "\n", "", "else", ":", "\n", "            ", "index", "=", "self", ".", "_map", "[", "image_id", "]", "\n", "image_id_features", "=", "self", ".", "features_h5", "[", "\"features\"", "]", "[", "index", "]", "\n", "\n", "", "num_boxes", "=", "self", ".", "_num_boxes", "[", "image_id", "]", "\n", "return", "image_id_features", ".", "reshape", "(", "(", "num_boxes", ",", "-", "1", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nocaps-org_updown-baseline.data.readers.CocoCaptionsReader.__init__": [[105, 129], ["print", "tqdm.tqdm.tqdm", "open", "json.load", "caption_item[].lower().strip", "nltk.tokenize.word_tokenize", "readers.CocoCaptionsReader._captions.append", "caption_item[].lower"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "captions_jsonpath", ":", "str", ")", "->", "None", ":", "\n", "        ", "self", ".", "_captions_jsonpath", "=", "captions_jsonpath", "\n", "\n", "with", "open", "(", "self", ".", "_captions_jsonpath", ")", "as", "cap", ":", "\n", "            ", "captions_json", ":", "Dict", "[", "str", ",", "Any", "]", "=", "json", ".", "load", "(", "cap", ")", "\n", "# fmt: off", "\n", "# List of punctuations taken from pycocoevalcap - these are ignored during evaluation.", "\n", "", "PUNCTUATIONS", ":", "List", "[", "str", "]", "=", "[", "\n", "\"''\"", ",", "\"'\"", ",", "\"``\"", ",", "\"`\"", ",", "\"(\"", ",", "\")\"", ",", "\"{\"", ",", "\"}\"", ",", "\n", "\".\"", ",", "\"?\"", ",", "\"!\"", ",", "\",\"", ",", "\":\"", ",", "\"-\"", ",", "\"--\"", ",", "\"...\"", ",", "\";\"", "\n", "]", "\n", "# fmt: on", "\n", "\n", "# List of (image id, caption) tuples.", "\n", "self", ".", "_captions", ":", "List", "[", "Tuple", "[", "int", ",", "List", "[", "str", "]", "]", "]", "=", "[", "]", "\n", "\n", "print", "(", "f\"Tokenizing captions from {captions_jsonpath}...\"", ")", "\n", "for", "caption_item", "in", "tqdm", "(", "captions_json", "[", "\"annotations\"", "]", ")", ":", "\n", "\n", "            ", "caption", ":", "str", "=", "caption_item", "[", "\"caption\"", "]", ".", "lower", "(", ")", ".", "strip", "(", ")", "\n", "caption_tokens", ":", "List", "[", "str", "]", "=", "word_tokenize", "(", "caption", ")", "\n", "caption_tokens", "=", "[", "ct", "for", "ct", "in", "caption_tokens", "if", "ct", "not", "in", "PUNCTUATIONS", "]", "\n", "\n", "self", ".", "_captions", ".", "append", "(", "(", "caption_item", "[", "\"image_id\"", "]", ",", "caption_tokens", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nocaps-org_updown-baseline.data.readers.CocoCaptionsReader.__len__": [[130, 132], ["len"], "methods", ["None"], ["", "", "def", "__len__", "(", "self", ")", "->", "int", ":", "\n", "        ", "return", "len", "(", "self", ".", "_captions", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nocaps-org_updown-baseline.data.readers.CocoCaptionsReader.__getitem__": [[133, 135], ["None"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "index", ")", "->", "Tuple", "[", "int", ",", "List", "[", "str", "]", "]", ":", "\n", "        ", "return", "self", ".", "_captions", "[", "index", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.nocaps-org_updown-baseline.data.readers.ConstraintBoxesReader.__init__": [[156, 172], ["json.load", "open", "readers.ConstraintBoxesReader._image_id_to_boxes[].append"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "boxes_jsonpath", ":", "str", ")", ":", "\n", "\n", "        ", "_boxes", "=", "json", ".", "load", "(", "open", "(", "boxes_jsonpath", ")", ")", "\n", "\n", "# Form a mapping between Image ID and corresponding boxes from OI Detector.", "\n", "self", ".", "_image_id_to_boxes", ":", "Dict", "[", "int", ",", "Any", "]", "=", "{", "}", "\n", "\n", "for", "ann", "in", "_boxes", "[", "\"annotations\"", "]", ":", "\n", "            ", "if", "ann", "[", "\"image_id\"", "]", "not", "in", "self", ".", "_image_id_to_boxes", ":", "\n", "                ", "self", ".", "_image_id_to_boxes", "[", "ann", "[", "\"image_id\"", "]", "]", "=", "[", "]", "\n", "\n", "", "self", ".", "_image_id_to_boxes", "[", "ann", "[", "\"image_id\"", "]", "]", ".", "append", "(", "ann", ")", "\n", "\n", "# A list of Open Image object classes. Index of a class in this list is its Open Images", "\n", "# class ID. Open Images class IDs start from 1, so zero-th element is \"__background__\".", "\n", "", "self", ".", "_class_names", "=", "[", "c", "[", "\"name\"", "]", "for", "c", "in", "_boxes", "[", "\"categories\"", "]", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.nocaps-org_updown-baseline.data.readers.ConstraintBoxesReader.__len__": [[173, 175], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", "->", "int", ":", "\n", "        ", "return", "len", "(", "self", ".", "_image_id_to_boxes", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nocaps-org_updown-baseline.data.readers.ConstraintBoxesReader.__getitem__": [[176, 189], ["readers.ConstraintBoxesReader._image_id_to_boxes.get", "numpy.array", "numpy.array", "int", "ann.get"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "image_id", ":", "int", ")", "->", "ConstraintBoxes", ":", "\n", "\n", "# List of bounding box detections from OI detector in COCO format.", "\n", "# Some images may not have any boxes, handle that case too.", "\n", "        ", "bbox_anns", "=", "self", ".", "_image_id_to_boxes", ".", "get", "(", "int", "(", "image_id", ")", ",", "[", "]", ")", "\n", "\n", "boxes", "=", "np", ".", "array", "(", "[", "ann", "[", "\"bbox\"", "]", "for", "ann", "in", "bbox_anns", "]", ")", "\n", "scores", "=", "np", ".", "array", "(", "[", "ann", ".", "get", "(", "\"score\"", ",", "1", ")", "for", "ann", "in", "bbox_anns", "]", ")", "\n", "\n", "# Convert object class IDs to their names.", "\n", "class_names", "=", "[", "self", ".", "_class_names", "[", "ann", "[", "\"category_id\"", "]", "]", "for", "ann", "in", "bbox_anns", "]", "\n", "\n", "return", "{", "\"boxes\"", ":", "boxes", ",", "\"class_names\"", ":", "class_names", ",", "\"scores\"", ":", "scores", "}", "\n", "", "", ""]], "home.repos.pwc.inspect_result.nocaps-org_updown-baseline.data.datasets.TrainingDataset.__init__": [[47, 60], ["updown.data.readers.ImageFeaturesReader", "updown.data.readers.CocoCaptionsReader"], "methods", ["None"], ["def", "__init__", "(", "\n", "self", ",", "\n", "vocabulary", ":", "Vocabulary", ",", "\n", "captions_jsonpath", ":", "str", ",", "\n", "image_features_h5path", ":", "str", ",", "\n", "max_caption_length", ":", "int", "=", "20", ",", "\n", "in_memory", ":", "bool", "=", "True", ",", "\n", ")", "->", "None", ":", "\n", "        ", "self", ".", "_vocabulary", "=", "vocabulary", "\n", "self", ".", "_image_features_reader", "=", "ImageFeaturesReader", "(", "image_features_h5path", ",", "in_memory", ")", "\n", "self", ".", "_captions_reader", "=", "CocoCaptionsReader", "(", "captions_jsonpath", ")", "\n", "\n", "self", ".", "_max_caption_length", "=", "max_caption_length", "\n", "\n"]], "home.repos.pwc.inspect_result.nocaps-org_updown-baseline.data.datasets.TrainingDataset.from_config": [[61, 72], ["kwargs.pop", "cls", "kwargs.pop"], "methods", ["None"], ["", "@", "classmethod", "\n", "def", "from_config", "(", "cls", ",", "config", ":", "Config", ",", "**", "kwargs", ")", ":", "\n", "        ", "r\"\"\"Instantiate this class directly from a :class:`~updown.config.Config`.\"\"\"", "\n", "_C", "=", "config", "\n", "vocabulary", "=", "kwargs", ".", "pop", "(", "\"vocabulary\"", ")", "\n", "return", "cls", "(", "\n", "vocabulary", "=", "vocabulary", ",", "\n", "image_features_h5path", "=", "_C", ".", "DATA", ".", "TRAIN_FEATURES", ",", "\n", "captions_jsonpath", "=", "_C", ".", "DATA", ".", "TRAIN_CAPTIONS", ",", "\n", "max_caption_length", "=", "_C", ".", "DATA", ".", "MAX_CAPTION_LENGTH", ",", "\n", "in_memory", "=", "kwargs", ".", "pop", "(", "\"in_memory\"", ")", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.nocaps-org_updown-baseline.data.datasets.TrainingDataset.__len__": [[74, 77], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", "->", "int", ":", "\n", "# Number of training examples are number of captions, not number of images.", "\n", "        ", "return", "len", "(", "self", ".", "_captions_reader", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nocaps-org_updown-baseline.data.datasets.TrainingDataset.__getitem__": [[78, 98], ["caption_tokens.extend", "datasets.TrainingDataset._vocabulary.get_token_index", "datasets.TrainingDataset._vocabulary.get_token_index", "len"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "index", ":", "int", ")", "->", "TrainingInstance", ":", "\n", "        ", "image_id", ",", "caption", "=", "self", ".", "_captions_reader", "[", "index", "]", "\n", "image_features", "=", "self", ".", "_image_features_reader", "[", "image_id", "]", "\n", "\n", "# Tokenize caption.", "\n", "caption_tokens", ":", "List", "[", "int", "]", "=", "[", "self", ".", "_vocabulary", ".", "get_token_index", "(", "c", ")", "for", "c", "in", "caption", "]", "\n", "\n", "# Pad upto max_caption_length.", "\n", "caption_tokens", "=", "caption_tokens", "[", ":", "self", ".", "_max_caption_length", "]", "\n", "caption_tokens", ".", "extend", "(", "\n", "[", "self", ".", "_vocabulary", ".", "get_token_index", "(", "\"@@UNKNOWN@@\"", ")", "]", "\n", "*", "(", "self", ".", "_max_caption_length", "-", "len", "(", "caption_tokens", ")", ")", "\n", ")", "\n", "\n", "item", ":", "TrainingInstance", "=", "{", "\n", "\"image_id\"", ":", "image_id", ",", "\n", "\"image_features\"", ":", "image_features", ",", "\n", "\"caption_tokens\"", ":", "caption_tokens", ",", "\n", "}", "\n", "return", "item", "\n", "\n"]], "home.repos.pwc.inspect_result.nocaps-org_updown-baseline.data.datasets.TrainingDataset.collate_fn": [[99, 117], ["torch.tensor().long", "torch.tensor().long", "torch.from_numpy", "datasets._collate_image_features", "torch.tensor", "torch.tensor"], "methods", ["home.repos.pwc.inspect_result.nocaps-org_updown-baseline.data.datasets._collate_image_features"], ["", "def", "collate_fn", "(", "self", ",", "batch_list", ":", "List", "[", "TrainingInstance", "]", ")", "->", "TrainingBatch", ":", "\n", "# Convert lists of ``image_id``s and ``caption_tokens``s as tensors.", "\n", "        ", "image_id", "=", "torch", ".", "tensor", "(", "[", "instance", "[", "\"image_id\"", "]", "for", "instance", "in", "batch_list", "]", ")", ".", "long", "(", ")", "\n", "caption_tokens", "=", "torch", ".", "tensor", "(", "\n", "[", "instance", "[", "\"caption_tokens\"", "]", "for", "instance", "in", "batch_list", "]", "\n", ")", ".", "long", "(", ")", "\n", "\n", "# Pad adaptive image features in the batch.", "\n", "image_features", "=", "torch", ".", "from_numpy", "(", "\n", "_collate_image_features", "(", "[", "instance", "[", "\"image_features\"", "]", "for", "instance", "in", "batch_list", "]", ")", "\n", ")", "\n", "\n", "batch", ":", "TrainingBatch", "=", "{", "\n", "\"image_id\"", ":", "image_id", ",", "\n", "\"image_features\"", ":", "image_features", ",", "\n", "\"caption_tokens\"", ":", "caption_tokens", ",", "\n", "}", "\n", "return", "batch", "\n", "\n"]], "home.repos.pwc.inspect_result.nocaps-org_updown-baseline.data.datasets.EvaluationDataset.__init__": [[138, 141], ["updown.data.readers.ImageFeaturesReader", "sorted", "list", "datasets.EvaluationDataset._image_features_reader._map.keys"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "image_features_h5path", ":", "str", ",", "in_memory", ":", "bool", "=", "True", ")", "->", "None", ":", "\n", "        ", "self", ".", "_image_features_reader", "=", "ImageFeaturesReader", "(", "image_features_h5path", ",", "in_memory", ")", "\n", "self", ".", "_image_ids", "=", "sorted", "(", "list", "(", "self", ".", "_image_features_reader", ".", "_map", ".", "keys", "(", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nocaps-org_updown-baseline.data.datasets.EvaluationDataset.from_config": [[142, 147], ["cls", "kwargs.pop"], "methods", ["None"], ["", "@", "classmethod", "\n", "def", "from_config", "(", "cls", ",", "config", ":", "Config", ",", "**", "kwargs", ")", ":", "\n", "        ", "r\"\"\"Instantiate this class directly from a :class:`~updown.config.Config`.\"\"\"", "\n", "_C", "=", "config", "\n", "return", "cls", "(", "image_features_h5path", "=", "_C", ".", "DATA", ".", "INFER_FEATURES", ",", "in_memory", "=", "kwargs", ".", "pop", "(", "\"in_memory\"", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nocaps-org_updown-baseline.data.datasets.EvaluationDataset.__len__": [[148, 150], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", "->", "int", ":", "\n", "        ", "return", "len", "(", "self", ".", "_image_ids", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nocaps-org_updown-baseline.data.datasets.EvaluationDataset.__getitem__": [[151, 157], ["None"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "index", ":", "int", ")", "->", "EvaluationInstance", ":", "\n", "        ", "image_id", "=", "self", ".", "_image_ids", "[", "index", "]", "\n", "image_features", "=", "self", ".", "_image_features_reader", "[", "image_id", "]", "\n", "\n", "item", ":", "EvaluationInstance", "=", "{", "\"image_id\"", ":", "image_id", ",", "\"image_features\"", ":", "image_features", "}", "\n", "return", "item", "\n", "\n"]], "home.repos.pwc.inspect_result.nocaps-org_updown-baseline.data.datasets.EvaluationDataset.collate_fn": [[158, 169], ["torch.tensor().long", "torch.from_numpy", "datasets._collate_image_features", "torch.tensor"], "methods", ["home.repos.pwc.inspect_result.nocaps-org_updown-baseline.data.datasets._collate_image_features"], ["", "def", "collate_fn", "(", "self", ",", "batch_list", ":", "List", "[", "EvaluationInstance", "]", ")", "->", "EvaluationBatch", ":", "\n", "# Convert lists of ``image_id``s and ``caption_tokens``s as tensors.", "\n", "        ", "image_id", "=", "torch", ".", "tensor", "(", "[", "instance", "[", "\"image_id\"", "]", "for", "instance", "in", "batch_list", "]", ")", ".", "long", "(", ")", "\n", "\n", "# Pad adaptive image features in the batch.", "\n", "image_features", "=", "torch", ".", "from_numpy", "(", "\n", "_collate_image_features", "(", "[", "instance", "[", "\"image_features\"", "]", "for", "instance", "in", "batch_list", "]", ")", "\n", ")", "\n", "\n", "batch", ":", "EvaluationBatch", "=", "{", "\"image_id\"", ":", "image_id", ",", "\"image_features\"", ":", "image_features", "}", "\n", "return", "batch", "\n", "\n"]], "home.repos.pwc.inspect_result.nocaps-org_updown-baseline.data.datasets.EvaluationDatasetWithConstraints.__init__": [[216, 239], ["datasets.EvaluationDataset.__init__", "vocabulary.get_token_index", "updown.data.readers.ConstraintBoxesReader", "updown.utils.constraints.ConstraintFilter", "updown.utils.constraints.FiniteStateMachineBuilder"], "methods", ["home.repos.pwc.inspect_result.nocaps-org_updown-baseline.data.datasets.EvaluationDatasetWithConstraints.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "vocabulary", ":", "Vocabulary", ",", "\n", "image_features_h5path", ":", "str", ",", "\n", "boxes_jsonpath", ":", "str", ",", "\n", "wordforms_tsvpath", ":", "str", ",", "\n", "hierarchy_jsonpath", ":", "str", ",", "\n", "nms_threshold", ":", "float", "=", "0.85", ",", "\n", "max_given_constraints", ":", "int", "=", "3", ",", "\n", "max_words_per_constraint", ":", "int", "=", "3", ",", "\n", "in_memory", ":", "bool", "=", "True", ",", "\n", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "image_features_h5path", ",", "in_memory", "=", "in_memory", ")", "\n", "\n", "self", ".", "_vocabulary", "=", "vocabulary", "\n", "self", ".", "_pad_index", "=", "vocabulary", ".", "get_token_index", "(", "\"@@UNKNOWN@@\"", ")", "\n", "\n", "self", ".", "_boxes_reader", "=", "ConstraintBoxesReader", "(", "boxes_jsonpath", ")", "\n", "\n", "self", ".", "_constraint_filter", "=", "ConstraintFilter", "(", "\n", "hierarchy_jsonpath", ",", "nms_threshold", ",", "max_given_constraints", "\n", ")", "\n", "self", ".", "_fsm_builder", "=", "FiniteStateMachineBuilder", "(", "vocabulary", ",", "wordforms_tsvpath", ",", "max_given_constraints", ",", "max_words_per_constraint", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nocaps-org_updown-baseline.data.datasets.EvaluationDatasetWithConstraints.from_config": [[240, 254], ["kwargs.pop", "cls", "kwargs.pop"], "methods", ["None"], ["", "@", "classmethod", "\n", "def", "from_config", "(", "cls", ",", "config", ":", "Config", ",", "**", "kwargs", ")", ":", "\n", "        ", "r\"\"\"Instantiate this class directly from a :class:`~updown.config.Config`.\"\"\"", "\n", "_C", "=", "config", "\n", "vocabulary", "=", "kwargs", ".", "pop", "(", "\"vocabulary\"", ")", "\n", "return", "cls", "(", "\n", "vocabulary", "=", "vocabulary", ",", "\n", "image_features_h5path", "=", "_C", ".", "DATA", ".", "INFER_FEATURES", ",", "\n", "boxes_jsonpath", "=", "_C", ".", "DATA", ".", "CBS", ".", "INFER_BOXES", ",", "\n", "wordforms_tsvpath", "=", "_C", ".", "DATA", ".", "CBS", ".", "WORDFORMS", ",", "\n", "hierarchy_jsonpath", "=", "_C", ".", "DATA", ".", "CBS", ".", "CLASS_HIERARCHY", ",", "\n", "max_given_constraints", "=", "_C", ".", "DATA", ".", "CBS", ".", "MAX_GIVEN_CONSTRAINTS", ",", "\n", "max_words_per_constraint", "=", "_C", ".", "DATA", ".", "CBS", ".", "MAX_WORDS_PER_CONSTRAINT", ",", "\n", "in_memory", "=", "kwargs", ".", "pop", "(", "\"in_memory\"", ")", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.nocaps-org_updown-baseline.data.datasets.EvaluationDatasetWithConstraints.__getitem__": [[256, 268], ["datasets.EvaluationDataset.__getitem__", "datasets.EvaluationDatasetWithConstraints._constraint_filter", "datasets.EvaluationDatasetWithConstraints._fsm_builder.build", "len"], "methods", ["home.repos.pwc.inspect_result.nocaps-org_updown-baseline.data.datasets.EvaluationDatasetWithConstraints.__getitem__", "home.repos.pwc.inspect_result.nocaps-org_updown-baseline.utils.constraints.FiniteStateMachineBuilder.build"], ["", "def", "__getitem__", "(", "self", ",", "index", ":", "int", ")", "->", "EvaluationInstanceWithConstraints", ":", "\n", "        ", "item", ":", "EvaluationInstance", "=", "super", "(", ")", ".", "__getitem__", "(", "index", ")", "\n", "\n", "# Apply constraint filtering to object class names.", "\n", "constraint_boxes", "=", "self", ".", "_boxes_reader", "[", "item", "[", "\"image_id\"", "]", "]", "\n", "\n", "candidates", ":", "List", "[", "str", "]", "=", "self", ".", "_constraint_filter", "(", "\n", "constraint_boxes", "[", "\"boxes\"", "]", ",", "constraint_boxes", "[", "\"class_names\"", "]", ",", "constraint_boxes", "[", "\"scores\"", "]", "\n", ")", "\n", "fsm", ",", "nstates", "=", "self", ".", "_fsm_builder", ".", "build", "(", "candidates", ")", "\n", "\n", "return", "{", "\"fsm\"", ":", "fsm", ",", "\"num_states\"", ":", "nstates", ",", "\"num_constraints\"", ":", "len", "(", "candidates", ")", ",", "**", "item", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.nocaps-org_updown-baseline.data.datasets.EvaluationDatasetWithConstraints.collate_fn": [[269, 281], ["datasets.EvaluationDataset.collate_fn", "max", "torch.stack", "torch.tensor().long", "super().collate_fn.update", "torch.tensor"], "methods", ["home.repos.pwc.inspect_result.nocaps-org_updown-baseline.data.datasets.EvaluationDatasetWithConstraints.collate_fn"], ["", "def", "collate_fn", "(", "\n", "self", ",", "batch_list", ":", "List", "[", "EvaluationInstanceWithConstraints", "]", "\n", ")", "->", "EvaluationBatchWithConstraints", ":", "\n", "\n", "        ", "batch", "=", "super", "(", ")", ".", "collate_fn", "(", "batch_list", ")", "\n", "\n", "max_state", "=", "max", "(", "[", "s", "[", "\"num_states\"", "]", "for", "s", "in", "batch_list", "]", ")", "\n", "fsm", "=", "torch", ".", "stack", "(", "[", "s", "[", "\"fsm\"", "]", "[", ":", "max_state", ",", ":", "max_state", ",", ":", "]", "for", "s", "in", "batch_list", "]", ")", "\n", "num_candidates", "=", "torch", ".", "tensor", "(", "[", "s", "[", "\"num_constraints\"", "]", "for", "s", "in", "batch_list", "]", ")", ".", "long", "(", ")", "\n", "\n", "batch", ".", "update", "(", "{", "\"fsm\"", ":", "fsm", ",", "\"num_constraints\"", ":", "num_candidates", "}", ")", "\n", "return", "batch", "\n", "\n"]], "home.repos.pwc.inspect_result.nocaps-org_updown-baseline.data.datasets._collate_image_features": [[283, 293], ["numpy.zeros", "enumerate", "zip", "len", "max"], "function", ["None"], ["", "", "def", "_collate_image_features", "(", "image_features_list", ":", "List", "[", "np", ".", "ndarray", "]", ")", "->", "np", ".", "ndarray", ":", "\n", "    ", "num_boxes", "=", "[", "instance", ".", "shape", "[", "0", "]", "for", "instance", "in", "image_features_list", "]", "\n", "image_feature_size", "=", "image_features_list", "[", "0", "]", ".", "shape", "[", "-", "1", "]", "\n", "\n", "image_features", "=", "np", ".", "zeros", "(", "\n", "(", "len", "(", "image_features_list", ")", ",", "max", "(", "num_boxes", ")", ",", "image_feature_size", ")", ",", "dtype", "=", "np", ".", "float32", "\n", ")", "\n", "for", "i", ",", "(", "instance", ",", "dim", ")", "in", "enumerate", "(", "zip", "(", "image_features_list", ",", "num_boxes", ")", ")", ":", "\n", "        ", "image_features", "[", "i", ",", ":", "dim", "]", "=", "instance", "\n", "", "return", "image_features", "\n", "", ""]], "home.repos.pwc.inspect_result.nocaps-org_updown-baseline.docs.conf.linkcode_resolve": [[127, 170], ["sys.modules.get", "fullname.split", "info[].replace", "inspect.getsourcefile", "inspect.getsourcelines", "getattr", "len"], "function", ["None"], ["def", "linkcode_resolve", "(", "domain", ",", "info", ")", ":", "\n", "    ", "\"\"\"\n    Determine the URL corresponding to Python object\n    This code is from\n    https://github.com/numpy/numpy/blob/master/doc/source/conf.py#L290\n    and https://github.com/Lasagne/Lasagne/pull/262\n    \"\"\"", "\n", "if", "domain", "!=", "\"py\"", ":", "\n", "        ", "return", "None", "\n", "\n", "", "modname", "=", "info", "[", "\"module\"", "]", "\n", "fullname", "=", "info", "[", "\"fullname\"", "]", "\n", "\n", "submod", "=", "sys", ".", "modules", ".", "get", "(", "modname", ")", "\n", "if", "submod", "is", "None", ":", "\n", "        ", "return", "None", "\n", "\n", "", "obj", "=", "submod", "\n", "for", "part", "in", "fullname", ".", "split", "(", "\".\"", ")", ":", "\n", "        ", "try", ":", "\n", "            ", "obj", "=", "getattr", "(", "obj", ",", "part", ")", "\n", "", "except", ":", "# noqa: E722", "\n", "            ", "return", "None", "\n", "\n", "", "", "try", ":", "\n", "        ", "fn", "=", "inspect", ".", "getsourcefile", "(", "obj", ")", "\n", "", "except", ":", "# noqa: E722", "\n", "        ", "fn", "=", "None", "\n", "", "if", "not", "fn", ":", "\n", "        ", "return", "None", "\n", "\n", "", "try", ":", "\n", "        ", "source", ",", "lineno", "=", "inspect", ".", "getsourcelines", "(", "obj", ")", "\n", "", "except", ":", "# noqa: E722", "\n", "        ", "lineno", "=", "None", "\n", "\n", "", "if", "lineno", ":", "\n", "        ", "linespec", "=", "\"#L%d-L%d\"", "%", "(", "lineno", ",", "lineno", "+", "len", "(", "source", ")", "-", "1", ")", "\n", "", "else", ":", "\n", "        ", "linespec", "=", "\"\"", "\n", "\n", "", "filename", "=", "info", "[", "\"module\"", "]", ".", "replace", "(", "\".\"", ",", "\"/\"", ")", "\n", "return", "f\"https://github.com/nocaps-org/updown-baseline/blob/master/{filename}.py{linespec}\"", "\n", "", ""]], "home.repos.pwc.inspect_result.nocaps-org_updown-baseline.scripts.build_vocabulary.build_caption_vocabulary": [[46, 73], ["tqdm.tqdm", "sorted", "sorted", "item[].lower().strip", "nltk.tokenize.word_tokenize", "list", "item[].lower"], "function", ["None"], ["def", "build_caption_vocabulary", "(", "\n", "caption_json", ":", "List", "[", "CocoCaptionExample", "]", ",", "word_count_threshold", ":", "int", "=", "5", "\n", ")", "->", "List", "[", "str", "]", ":", "\n", "    ", "r\"\"\"\n    Given a list of COCO caption examples, return a list of unique captions tokens thresholded\n    by minimum occurence.\n    \"\"\"", "\n", "\n", "word_counts", ":", "Dict", "[", "str", ",", "int", "]", "=", "{", "}", "\n", "\n", "# Accumulate unique caption tokens from all caption sequences.", "\n", "for", "item", "in", "tqdm", "(", "caption_json", ")", ":", "\n", "        ", "caption", ":", "str", "=", "item", "[", "\"caption\"", "]", ".", "lower", "(", ")", ".", "strip", "(", ")", "\n", "caption_tokens", ":", "List", "[", "str", "]", "=", "word_tokenize", "(", "caption", ")", "\n", "caption_tokens", "=", "[", "ct", "for", "ct", "in", "caption_tokens", "if", "ct", "not", "in", "PUNCTUATIONS", "]", "\n", "\n", "for", "token", "in", "caption_tokens", ":", "\n", "            ", "if", "token", "in", "word_counts", ":", "\n", "                ", "word_counts", "[", "token", "]", "+=", "1", "\n", "", "else", ":", "\n", "                ", "word_counts", "[", "token", "]", "=", "1", "\n", "\n", "", "", "", "all_caption_tokens", "=", "sorted", "(", "\n", "[", "key", "for", "key", "in", "word_counts", "if", "word_counts", "[", "key", "]", ">=", "word_count_threshold", "]", "\n", ")", "\n", "caption_vocabulary", ":", "List", "[", "str", "]", "=", "sorted", "(", "list", "(", "all_caption_tokens", ")", ")", "\n", "return", "caption_vocabulary", "\n", "\n"]]}