{"home.repos.pwc.inspect_result.Jivnesh_LCM.utils.load_word_embeddings.calc_mean_vec_for_lower_mapping": [[8, 20], ["word.lower", "numpy.mean"], "function", ["None"], ["def", "calc_mean_vec_for_lower_mapping", "(", "embedd_dict", ")", ":", "\n", "    ", "lower_counts", "=", "{", "}", "\n", "for", "word", "in", "embedd_dict", ":", "\n", "        ", "word_lower", "=", "word", ".", "lower", "(", ")", "\n", "if", "word_lower", "not", "in", "lower_counts", ":", "\n", "            ", "lower_counts", "[", "word_lower", "]", "=", "[", "word", "]", "\n", "", "else", ":", "\n", "            ", "lower_counts", "[", "word_lower", "]", "=", "lower_counts", "[", "word_lower", "]", "+", "[", "word", "]", "\n", "# calculating mean vector for all words that have the same mapping after performing lower()", "\n", "", "", "for", "word", "in", "lower_counts", ":", "\n", "        ", "embedd_dict", "[", "word", "]", "=", "np", ".", "mean", "(", "[", "embedd_dict", "[", "word_", "]", "for", "word_", "in", "lower_counts", "[", "word", "]", "]", ")", "\n", "", "return", "embedd_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.utils.load_word_embeddings.load_embedding_dict": [[21, 124], ["print", "os.path.isfile", "print", "print", "len", "calc_mean_vec_for_lower_mapping.items", "open", "pickle.dump", "open", "pickle.load", "io.open", "load_word_embeddings.calc_mean_vec_for_lower_mapping", "len", "calc_mean_vec_for_lower_mapping.items", "line.split", "numpy.fromstring", "len", "print", "io.open", "enumerate", "load_word_embeddings.calc_mean_vec_for_lower_mapping", "len", "calc_mean_vec_for_lower_mapping.items", "len", "line.split", "numpy.fromstring", "len", "print", "io.open", "enumerate", "load_word_embeddings.calc_mean_vec_for_lower_mapping", "len", "calc_mean_vec_for_lower_mapping.items", "len", "line.split", "numpy.fromstring", "len", "print", "io.open", "enumerate", "load_word_embeddings.calc_mean_vec_for_lower_mapping", "gensim.models.KeyedVectors.load_word2vec_format", "ValueError", "len", "line.split", "numpy.fromstring", "len", "print", "load_word_embeddings.calc_mean_vec_for_lower_mapping", "len"], "function", ["home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.items", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.open", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.open", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.load", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.open", "home.repos.pwc.inspect_result.Jivnesh_LCM.utils.load_word_embeddings.calc_mean_vec_for_lower_mapping", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.items", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.open", "home.repos.pwc.inspect_result.Jivnesh_LCM.utils.load_word_embeddings.calc_mean_vec_for_lower_mapping", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.items", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.open", "home.repos.pwc.inspect_result.Jivnesh_LCM.utils.load_word_embeddings.calc_mean_vec_for_lower_mapping", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.items", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.open", "home.repos.pwc.inspect_result.Jivnesh_LCM.utils.load_word_embeddings.calc_mean_vec_for_lower_mapping", "home.repos.pwc.inspect_result.Jivnesh_LCM.utils.load_word_embeddings.calc_mean_vec_for_lower_mapping"], ["", "def", "load_embedding_dict", "(", "embedding", ",", "embedding_path", ",", "lower_case", "=", "False", ")", ":", "\n", "    ", "\"\"\"\n    load word embeddings from file\n    :param embedding:\n    :param embedding_path:\n    :return: embedding dict, embedding dimention, caseless\n    \"\"\"", "\n", "print", "(", "\"loading embedding: %s from %s\"", "%", "(", "embedding", ",", "embedding_path", ")", ")", "\n", "if", "lower_case", ":", "\n", "        ", "pkl_path", "=", "embedding_path", "+", "'_lower'", "+", "'.pkl'", "\n", "", "else", ":", "\n", "        ", "pkl_path", "=", "embedding_path", "+", "'.pkl'", "\n", "", "if", "os", ".", "path", ".", "isfile", "(", "pkl_path", ")", ":", "\n", "# load dict and dim from a pickle file", "\n", "        ", "with", "open", "(", "pkl_path", ",", "'rb'", ")", "as", "f", ":", "\n", "            ", "embedd_dict", ",", "embedd_dim", "=", "pickle", ".", "load", "(", "f", ")", "\n", "", "print", "(", "\"num dimensions of word embeddings:\"", ",", "embedd_dim", ")", "\n", "return", "embedd_dict", ",", "embedd_dim", "\n", "\n", "", "if", "embedding", "==", "'glove'", ":", "\n", "# loading GloVe", "\n", "        ", "embedd_dict", "=", "{", "}", "\n", "word", "=", "None", "\n", "with", "io", ".", "open", "(", "embedding_path", ",", "'r'", ",", "encoding", "=", "'utf-8'", ")", "as", "f", ":", "\n", "            ", "for", "line", "in", "f", ":", "\n", "                ", "word", ",", "vec", "=", "line", ".", "split", "(", "' '", ",", "1", ")", "\n", "embedd_dict", "[", "word", "]", "=", "np", ".", "fromstring", "(", "vec", ",", "sep", "=", "' '", ")", "\n", "", "", "embedd_dim", "=", "len", "(", "embedd_dict", "[", "word", "]", ")", "\n", "if", "lower_case", ":", "\n", "            ", "embedd_dict", "=", "calc_mean_vec_for_lower_mapping", "(", "embedd_dict", ")", "\n", "", "for", "k", ",", "v", "in", "embedd_dict", ".", "items", "(", ")", ":", "\n", "            ", "if", "len", "(", "v", ")", "!=", "embedd_dim", ":", "\n", "                ", "print", "(", "len", "(", "v", ")", ",", "embedd_dim", ")", "\n", "\n", "", "", "", "elif", "embedding", "==", "'fasttext'", ":", "\n", "# loading GloVe", "\n", "        ", "embedd_dict", "=", "{", "}", "\n", "word", "=", "None", "\n", "with", "io", ".", "open", "(", "embedding_path", ",", "'r'", ",", "encoding", "=", "'utf-8'", ")", "as", "f", ":", "\n", "# skip first line", "\n", "            ", "for", "i", ",", "line", "in", "enumerate", "(", "f", ")", ":", "\n", "                ", "if", "i", "==", "0", ":", "\n", "                    ", "continue", "\n", "", "word", ",", "vec", "=", "line", ".", "split", "(", "' '", ",", "1", ")", "\n", "embedd_dict", "[", "word", "]", "=", "np", ".", "fromstring", "(", "vec", ",", "sep", "=", "' '", ")", "\n", "", "", "embedd_dim", "=", "len", "(", "embedd_dict", "[", "word", "]", ")", "\n", "if", "lower_case", ":", "\n", "            ", "embedd_dict", "=", "calc_mean_vec_for_lower_mapping", "(", "embedd_dict", ")", "\n", "", "for", "k", ",", "v", "in", "embedd_dict", ".", "items", "(", ")", ":", "\n", "            ", "if", "len", "(", "v", ")", "!=", "embedd_dim", ":", "\n", "                ", "print", "(", "len", "(", "v", ")", ",", "embedd_dim", ")", "\n", "\n", "", "", "", "elif", "embedding", "==", "'hellwig'", ":", "\n", "# loading hellwig", "\n", "        ", "embedd_dict", "=", "{", "}", "\n", "word", "=", "None", "\n", "with", "io", ".", "open", "(", "embedding_path", ",", "'r'", ",", "encoding", "=", "'utf-8'", ")", "as", "f", ":", "\n", "# skip first line", "\n", "            ", "for", "i", ",", "line", "in", "enumerate", "(", "f", ")", ":", "\n", "                ", "if", "i", "==", "0", ":", "\n", "                    ", "continue", "\n", "", "word", ",", "vec", "=", "line", ".", "split", "(", "' '", ",", "1", ")", "\n", "embedd_dict", "[", "word", "]", "=", "np", ".", "fromstring", "(", "vec", ",", "sep", "=", "' '", ")", "\n", "", "", "embedd_dim", "=", "len", "(", "embedd_dict", "[", "word", "]", ")", "\n", "if", "lower_case", ":", "\n", "            ", "embedd_dict", "=", "calc_mean_vec_for_lower_mapping", "(", "embedd_dict", ")", "\n", "", "for", "k", ",", "v", "in", "embedd_dict", ".", "items", "(", ")", ":", "\n", "            ", "if", "len", "(", "v", ")", "!=", "embedd_dim", ":", "\n", "                ", "print", "(", "len", "(", "v", ")", ",", "embedd_dim", ")", "\n", "\n", "", "", "", "elif", "embedding", "==", "'one_hot'", ":", "\n", "# loading hellwig", "\n", "        ", "embedd_dict", "=", "{", "}", "\n", "word", "=", "None", "\n", "with", "io", ".", "open", "(", "embedding_path", ",", "'r'", ",", "encoding", "=", "'utf-8'", ")", "as", "f", ":", "\n", "# skip first line", "\n", "            ", "for", "i", ",", "line", "in", "enumerate", "(", "f", ")", ":", "\n", "                ", "if", "i", "==", "0", ":", "\n", "                    ", "continue", "\n", "", "word", ",", "vec", "=", "line", ".", "split", "(", "'@'", ",", "1", ")", "\n", "embedd_dict", "[", "word", "]", "=", "np", ".", "fromstring", "(", "vec", ",", "sep", "=", "' '", ")", "\n", "", "", "embedd_dim", "=", "len", "(", "embedd_dict", "[", "word", "]", ")", "\n", "if", "lower_case", ":", "\n", "            ", "embedd_dict", "=", "calc_mean_vec_for_lower_mapping", "(", "embedd_dict", ")", "\n", "", "for", "k", ",", "v", "in", "embedd_dict", ".", "items", "(", ")", ":", "\n", "            ", "if", "len", "(", "v", ")", "!=", "embedd_dim", ":", "\n", "                ", "print", "(", "len", "(", "v", ")", ",", "embedd_dim", ")", "\n", "\n", "", "", "", "elif", "embedding", "==", "'word2vec'", ":", "\n", "# loading word2vec", "\n", "        ", "embedd_dict", "=", "KeyedVectors", ".", "load_word2vec_format", "(", "embedding_path", ",", "binary", "=", "True", ")", "\n", "if", "lower_case", ":", "\n", "            ", "embedd_dict", "=", "calc_mean_vec_for_lower_mapping", "(", "embedd_dict", ")", "\n", "", "embedd_dim", "=", "embedd_dict", ".", "vector_size", "\n", "\n", "", "else", ":", "\n", "        ", "raise", "ValueError", "(", "\"embedding should choose from [fasttext, glove, word2vec]\"", ")", "\n", "\n", "", "print", "(", "\"num dimensions of word embeddings:\"", ",", "embedd_dim", ")", "\n", "# save dict and dim to a pickle file", "\n", "with", "open", "(", "pkl_path", ",", "'wb'", ")", "as", "f", ":", "\n", "        ", "pickle", ".", "dump", "(", "[", "embedd_dict", ",", "embedd_dim", "]", ",", "f", ",", "pickle", ".", "HIGHEST_PROTOCOL", ")", "\n", "", "return", "embedd_dict", ",", "embedd_dim", "", "", ""]], "home.repos.pwc.inspect_result.Jivnesh_LCM.nlinalg.nlinalg.logdet": [[4, 18], ["print", "print", "x.potrf", "torch.log", "torch.sum", "torch.log", "torch.eig", "x.potrf.diag"], "function", ["None"], ["def", "logdet", "(", "x", ")", ":", "\n", "    ", "\"\"\"\n\n    Args:\n        x: 2D positive semidefinite matrix.\n\n    Returns: log determinant of x\n\n    \"\"\"", "\n", "# TODO for pytorch 2.0.4, use inside potrf for variable.", "\n", "print", "(", "torch", ".", "log", "(", "torch", ".", "eig", "(", "x", ".", "data", ")", "[", "0", "]", ")", ")", "\n", "print", "(", "x", ")", "\n", "u_chol", "=", "x", ".", "potrf", "(", ")", "\n", "return", "torch", ".", "sum", "(", "torch", ".", "log", "(", "u_chol", ".", "diag", "(", ")", ")", ")", "*", "2", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.nlinalg.nlinalg.logsumexp": [[20, 39], ["x.max", "x.max", "x.max", "x.max", "torch.log", "torch.log", "torch.exp().sum", "torch.exp().sum", "torch.exp", "torch.exp"], "function", ["None"], ["", "def", "logsumexp", "(", "x", ",", "dim", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n\n    Args:\n        x: A pytorch tensor (any dimension will do)\n        dim: int or None, over which to perform the summation. `None`, the\n             default, performs over all axes.\n\n    Returns: The result of the log(sum(exp(...))) operation.\n\n    \"\"\"", "\n", "if", "dim", "is", "None", ":", "\n", "        ", "xmax", "=", "x", ".", "max", "(", ")", "\n", "xmax_", "=", "x", ".", "max", "(", ")", "\n", "return", "xmax_", "+", "torch", ".", "log", "(", "torch", ".", "exp", "(", "x", "-", "xmax", ")", ".", "sum", "(", ")", ")", "\n", "", "else", ":", "\n", "        ", "xmax", ",", "_", "=", "x", ".", "max", "(", "dim", ",", "keepdim", "=", "True", ")", "\n", "xmax_", ",", "_", "=", "x", ".", "max", "(", "dim", ")", "\n", "return", "xmax_", "+", "torch", ".", "log", "(", "torch", ".", "exp", "(", "x", "-", "xmax", ")", ".", "sum", "(", "dim", ")", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.Jivnesh_LCM.nn.init.assign_tensor": [[1, 15], ["tensor.data.copy_"], "function", ["None"], ["def", "assign_tensor", "(", "tensor", ",", "val", ")", ":", "\n", "    ", "\"\"\"\n    copy val to tensor\n    Args:\n        tensor: an n-dimensional torch.Tensor\n        val: an n-dimensional torch.Tensor to fill the tensor with\n\n    Returns:\n\n    \"\"\"", "\n", "#if isinstance(tensor, Variable):", "\n", "#    assign_tensor(tensor.data, val)", "\n", "#    return tensor", "\n", "return", "tensor", ".", "data", ".", "copy_", "(", "val", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.Jivnesh_LCM.nn.utils._ntuple": [[7, 13], ["isinstance", "tuple", "itertools.repeat"], "function", ["None"], ["def", "_ntuple", "(", "n", ")", ":", "\n", "    ", "def", "parse", "(", "x", ")", ":", "\n", "        ", "if", "isinstance", "(", "x", ",", "collections", ".", "Iterable", ")", ":", "\n", "            ", "return", "x", "\n", "", "return", "tuple", "(", "repeat", "(", "x", ",", "n", ")", ")", "\n", "", "return", "parse", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.nn.utils.prepare_rnn_seq": [[20, 68], ["utils.prepare_rnn_seq.check_decreasing"], "function", ["None"], ["def", "prepare_rnn_seq", "(", "rnn_input", ",", "lengths", ",", "hx", "=", "None", ",", "masks", "=", "None", ",", "batch_first", "=", "False", ")", ":", "\n", "    ", "'''\n\n    Args:\n        rnn_input: [seq_len, batch_size, input_size]: tensor containing the features of the input sequence.\n        lengths: [batch_size]: tensor containing the lengthes of the input sequence\n        hx: [num_layers * num_directions, batch_size, hidden_size]: tensor containing the initial hidden state for each element in the batch.\n        masks: [seq_len, batch_size]: tensor containing the mask for each element in the batch.\n        batch_first: If True, then the input and output tensors are provided as [batch_size, seq_len, feature].\n\n    Returns:\n\n    '''", "\n", "def", "check_decreasing", "(", "lengths", ")", ":", "\n", "        ", "lens", ",", "order", "=", "torch", ".", "sort", "(", "lengths", ",", "dim", "=", "0", ",", "descending", "=", "True", ")", "\n", "if", "torch", ".", "ne", "(", "lens", ",", "lengths", ")", ".", "sum", "(", ")", "==", "0", ":", "\n", "            ", "return", "None", "\n", "", "else", ":", "\n", "            ", "_", ",", "rev_order", "=", "torch", ".", "sort", "(", "order", ")", "\n", "return", "lens", ",", "order", ",", "rev_order", "\n", "\n", "", "", "check_res", "=", "check_decreasing", "(", "lengths", ")", "\n", "\n", "if", "check_res", "is", "None", ":", "\n", "        ", "lens", "=", "lengths", "\n", "rev_order", "=", "None", "\n", "", "else", ":", "\n", "        ", "lens", ",", "order", ",", "rev_order", "=", "check_res", "\n", "batch_dim", "=", "0", "if", "batch_first", "else", "1", "\n", "rnn_input", "=", "rnn_input", ".", "index_select", "(", "batch_dim", ",", "order", ")", "\n", "if", "hx", "is", "not", "None", ":", "\n", "# hack lstm", "\n", "            ", "if", "isinstance", "(", "hx", ",", "tuple", ")", ":", "\n", "                ", "hx", ",", "cx", "=", "hx", "\n", "hx", "=", "hx", ".", "index_select", "(", "1", ",", "order", ")", "\n", "cx", "=", "cx", ".", "index_select", "(", "1", ",", "order", ")", "\n", "hx", "=", "(", "hx", ",", "cx", ")", "\n", "", "else", ":", "\n", "                ", "hx", "=", "hx", ".", "index_select", "(", "1", ",", "order", ")", "\n", "\n", "", "", "", "lens", "=", "lens", ".", "tolist", "(", ")", "\n", "seq", "=", "rnn_utils", ".", "pack_padded_sequence", "(", "rnn_input", ",", "lens", ",", "batch_first", "=", "batch_first", ")", "\n", "if", "masks", "is", "not", "None", ":", "\n", "        ", "if", "batch_first", ":", "\n", "            ", "masks", "=", "masks", "[", ":", ",", ":", "lens", "[", "0", "]", "]", "\n", "", "else", ":", "\n", "            ", "masks", "=", "masks", "[", ":", "lens", "[", "0", "]", "]", "\n", "", "", "return", "seq", ",", "hx", ",", "rev_order", ",", "masks", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.nn.utils.recover_rnn_seq": [[70, 85], ["torch.pad_packed_sequence", "output.index_select.index_select", "isinstance", "hx.index_select.index_select", "cx.index_select.index_select", "hx.index_select.index_select"], "function", ["None"], ["", "def", "recover_rnn_seq", "(", "seq", ",", "rev_order", ",", "hx", "=", "None", ",", "batch_first", "=", "False", ")", ":", "\n", "    ", "output", ",", "_", "=", "rnn_utils", ".", "pad_packed_sequence", "(", "seq", ",", "batch_first", "=", "batch_first", ")", "\n", "if", "rev_order", "is", "not", "None", ":", "\n", "        ", "batch_dim", "=", "0", "if", "batch_first", "else", "1", "\n", "output", "=", "output", ".", "index_select", "(", "batch_dim", ",", "rev_order", ")", "\n", "if", "hx", "is", "not", "None", ":", "\n", "# hack lstm", "\n", "            ", "if", "isinstance", "(", "hx", ",", "tuple", ")", ":", "\n", "                ", "hx", ",", "cx", "=", "hx", "\n", "hx", "=", "hx", ".", "index_select", "(", "1", ",", "rev_order", ")", "\n", "cx", "=", "cx", ".", "index_select", "(", "1", ",", "rev_order", ")", "\n", "hx", "=", "(", "hx", ",", "cx", ")", "\n", "", "else", ":", "\n", "                ", "hx", "=", "hx", ".", "index_select", "(", "1", ",", "rev_order", ")", "\n", "", "", "", "return", "output", ",", "hx", "\n", "", ""]], "home.repos.pwc.inspect_result.Jivnesh_LCM.modules.crf.ChainCRF.__init__": [[12, 41], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "crf.ChainCRF.reset_parameters", "torch.Linear", "torch.Linear", "crf.ChainCRF.register_parameter", "torch.nn.parameter.Parameter", "torch.nn.parameter.Parameter", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor"], "methods", ["home.repos.pwc.inspect_result.Jivnesh_LCM.io_.reader.Reader.__init__", "home.repos.pwc.inspect_result.Jivnesh_LCM.models.parsing_gating.BiRecurrentConv_Encoder.reset_parameters"], ["    ", "def", "__init__", "(", "self", ",", "input_size", ",", "num_labels", ",", "bigram", "=", "True", ")", ":", "\n", "        ", "'''\n\n        Args:\n            input_size: int\n                the dimension of the input.\n            num_labels: int\n                the number of labels of the crf layer\n            bigram: bool\n                if apply bi-gram parameter.\n        '''", "\n", "super", "(", "ChainCRF", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "input_size", "=", "input_size", "\n", "self", ".", "num_labels", "=", "num_labels", "+", "1", "\n", "self", ".", "pad_label_id", "=", "num_labels", "\n", "self", ".", "bigram", "=", "bigram", "\n", "\n", "\n", "# state weight tensor", "\n", "self", ".", "state_nn", "=", "nn", ".", "Linear", "(", "input_size", ",", "self", ".", "num_labels", ")", "\n", "if", "bigram", ":", "\n", "# transition weight tensor", "\n", "            ", "self", ".", "trans_nn", "=", "nn", ".", "Linear", "(", "input_size", ",", "self", ".", "num_labels", "*", "self", ".", "num_labels", ")", "\n", "self", ".", "register_parameter", "(", "'trans_matrix'", ",", "None", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "trans_nn", "=", "None", "\n", "self", ".", "trans_matrix", "=", "Parameter", "(", "torch", ".", "Tensor", "(", "self", ".", "num_labels", ",", "self", ".", "num_labels", ")", ")", "\n", "\n", "", "self", ".", "reset_parameters", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.modules.crf.ChainCRF.reset_parameters": [[42, 49], ["torch.init.constant_", "torch.init.constant_", "torch.init.xavier_uniform_", "torch.init.xavier_uniform_", "torch.init.constant_", "torch.init.constant_", "torch.init.normal_", "torch.init.normal_"], "methods", ["None"], ["", "def", "reset_parameters", "(", "self", ")", ":", "\n", "        ", "nn", ".", "init", ".", "constant_", "(", "self", ".", "state_nn", ".", "bias", ",", "0.", ")", "\n", "if", "self", ".", "bigram", ":", "\n", "            ", "nn", ".", "init", ".", "xavier_uniform_", "(", "self", ".", "trans_nn", ".", "weight", ")", "\n", "nn", ".", "init", ".", "constant_", "(", "self", ".", "trans_nn", ".", "bias", ",", "0.", ")", "\n", "", "else", ":", "\n", "            ", "nn", ".", "init", ".", "normal_", "(", "self", ".", "trans_matrix", ")", "\n", "# if not self.bigram:", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.modules.crf.ChainCRF.forward": [[52, 81], ["input.size", "crf.ChainCRF.state_nn().unsqueeze", "crf.ChainCRF.trans_nn().view", "crf.ChainCRF.state_nn", "crf.ChainCRF.trans_nn"], "methods", ["home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.size"], ["", "", "def", "forward", "(", "self", ",", "input", ",", "mask", "=", "None", ")", ":", "\n", "        ", "'''\n\n        Args:\n            input: Tensor\n                the input tensor with shape = [batch_size, length, input_size]\n            mask: Tensor or None\n                the mask tensor with shape = [batch_size, length]\n\n        Returns: Tensor\n            the energy tensor with shape = [batch_size, length, num_label, num_label]\n\n        '''", "\n", "batch_size", ",", "length", ",", "_", "=", "input", ".", "size", "(", ")", "\n", "\n", "# compute out_s by tensor dot [batch_size, length, input_size] * [input_size, num_label]", "\n", "# thus out_s should be [batch_size, length, num_label] --> [batch_size, length, num_label, 1]", "\n", "out_s", "=", "self", ".", "state_nn", "(", "input", ")", ".", "unsqueeze", "(", "-", "1", ")", "\n", "\n", "if", "self", ".", "bigram", ":", "\n", "# compute out_s by tensor dot: [batch_size, length, input_size] * [input_size, num_label * num_label]", "\n", "            ", "out_t", "=", "self", ".", "trans_nn", "(", "input", ")", ".", "view", "(", "batch_size", ",", "length", ",", "self", ".", "num_labels", ",", "self", ".", "num_labels", ")", "\n", "", "else", ":", "\n", "            ", "out_t", "=", "self", ".", "trans_matrix", "\n", "# the output should be [batch_size, length, num_label, num_label]", "\n", "#output = out_t + out_s", "\n", "#if mask is not None:", "\n", "#    output = output * mask.unsqueeze(2).unsqueeze(3)", "\n", "", "return", "(", "out_s", ",", "out_t", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.modules.crf.ChainCRF.loss": [[82, 141], ["energy.size", "energy.transpose", "target.transpose", "range", "length.max", "mask.unsqueeze().transpose", "torch.arange().long().cuda", "torch.arange().long().cuda", "torch.arange().long().cuda", "torch.arange().long().cuda", "torch.cuda.LongTensor().fill_", "torch.cuda.LongTensor().fill_", "torch.cuda.LongTensor().fill_", "torch.cuda.LongTensor().fill_", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.LongTensor().fill_", "torch.LongTensor().fill_", "torch.LongTensor().fill_", "torch.LongTensor().fill_", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "utils.nlinalg.logsumexp", "energy.size", "mask.unsqueeze().unsqueeze", "utils.nlinalg.logsumexp", "mask.unsqueeze", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.cuda.LongTensor", "torch.cuda.LongTensor", "torch.cuda.LongTensor", "torch.cuda.LongTensor", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "mask.unsqueeze", "partition.unsqueeze", "torch.arange", "torch.arange", "torch.arange", "torch.arange"], "methods", ["home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.size", "home.repos.pwc.inspect_result.Jivnesh_LCM.nlinalg.nlinalg.logsumexp", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.size", "home.repos.pwc.inspect_result.Jivnesh_LCM.nlinalg.nlinalg.logsumexp"], ["", "def", "loss", "(", "self", ",", "energy", ",", "target", ",", "mask", "=", "None", ",", "length", "=", "None", ")", ":", "\n", "        ", "'''\n\n        Args:\n            energy: Tensor\n                the input tensor with shape = [batch_size, length, num_label, num_label]\n            target: Tensor\n                the tensor of target labels with shape [batch_size, length]\n            mask:Tensor or None\n                the mask tensor with shape = [batch_size, length]\n\n        Returns: Tensor\n                A 1D tensor for minus log likelihood loss\n        '''", "\n", "if", "length", "is", "not", "None", ":", "\n", "            ", "max_len", "=", "length", ".", "max", "(", ")", "\n", "if", "energy", ".", "size", "(", "1", ")", "!=", "max_len", ":", "\n", "                ", "target", "=", "target", "[", ":", ",", ":", "max_len", "]", "\n", "", "", "mask_transpose", "=", "None", "\n", "if", "mask", "is", "not", "None", ":", "\n", "            ", "energy", "=", "energy", "*", "mask", ".", "unsqueeze", "(", "2", ")", ".", "unsqueeze", "(", "3", ")", "\n", "mask_transpose", "=", "mask", ".", "unsqueeze", "(", "2", ")", ".", "transpose", "(", "0", ",", "1", ")", "\n", "\n", "", "batch_size", ",", "len", ",", "_", ",", "_", "=", "energy", ".", "size", "(", ")", "\n", "# shape = [length, batch_size, num_label, num_label]", "\n", "energy_transpose", "=", "energy", ".", "transpose", "(", "0", ",", "1", ")", "\n", "# shape = [length, batch_size]", "\n", "target_transpose", "=", "target", ".", "transpose", "(", "0", ",", "1", ")", "\n", "# shape = [length, batch_size, 1]", "\n", "\n", "# shape = [batch_size, num_label]", "\n", "partition", "=", "None", "\n", "if", "energy", ".", "is_cuda", ":", "\n", "# shape = [batch_size]", "\n", "            ", "batch_index", "=", "torch", ".", "arange", "(", "0", ",", "batch_size", ")", ".", "long", "(", ")", ".", "cuda", "(", ")", "\n", "prev_label", "=", "torch", ".", "cuda", ".", "LongTensor", "(", "batch_size", ")", ".", "fill_", "(", "self", ".", "num_labels", "-", "1", ")", "\n", "tgt_energy", "=", "torch", ".", "zeros", "(", "batch_size", ")", ".", "cuda", "(", ")", "\n", "", "else", ":", "\n", "# shape = [batch_size]", "\n", "            ", "batch_index", "=", "torch", ".", "arange", "(", "0", ",", "batch_size", ")", ".", "long", "(", ")", "\n", "prev_label", "=", "torch", ".", "LongTensor", "(", "batch_size", ")", ".", "fill_", "(", "self", ".", "num_labels", "-", "1", ")", "\n", "tgt_energy", "=", "torch", ".", "zeros", "(", "batch_size", ")", "\n", "\n", "", "for", "t", "in", "range", "(", "len", ")", ":", "\n", "# shape = [batch_size, num_label, num_label]", "\n", "            ", "curr_energy", "=", "energy_transpose", "[", "t", "]", "\n", "if", "t", "==", "0", ":", "\n", "                ", "partition", "=", "curr_energy", "[", ":", ",", "-", "1", ",", ":", "]", "\n", "", "else", ":", "\n", "# shape = [batch_size, num_label]", "\n", "                ", "partition_new", "=", "logsumexp", "(", "curr_energy", "+", "partition", ".", "unsqueeze", "(", "2", ")", ",", "dim", "=", "1", ")", "\n", "if", "mask_transpose", "is", "None", ":", "\n", "                    ", "partition", "=", "partition_new", "\n", "", "else", ":", "\n", "                    ", "mask_t", "=", "mask_transpose", "[", "t", "]", "\n", "partition", "=", "partition", "+", "(", "partition_new", "-", "partition", ")", "*", "mask_t", "\n", "", "", "tgt_energy", "+=", "curr_energy", "[", "batch_index", ",", "prev_label", ",", "target_transpose", "[", "t", "]", ".", "data", "]", "\n", "prev_label", "=", "target_transpose", "[", "t", "]", ".", "data", "\n", "", "return", "logsumexp", "(", "partition", ",", "dim", "=", "1", ")", "-", "tgt_energy", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.modules.crf.ChainCRF.decode": [[142, 191], ["energy.transpose", "energy.transpose.size", "energy[].unsqueeze", "range", "torch.max", "torch.max", "torch.max", "torch.max", "torch.LongTensor().zero_.squeeze", "torch.LongTensor().zero_.squeeze", "reversed", "torch.arange().long().cuda", "torch.arange().long().cuda", "torch.arange().long().cuda", "torch.arange().long().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.cuda.LongTensor().zero_", "torch.cuda.LongTensor().zero_", "torch.cuda.LongTensor().zero_", "torch.cuda.LongTensor().zero_", "torch.cuda.LongTensor().zero_", "torch.cuda.LongTensor().zero_", "torch.cuda.LongTensor().zero_", "torch.cuda.LongTensor().zero_", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.LongTensor().zero_", "torch.LongTensor().zero_", "torch.LongTensor().zero_", "torch.LongTensor().zero_", "torch.LongTensor().zero_", "torch.LongTensor().zero_", "torch.LongTensor().zero_", "torch.LongTensor().zero_", "torch.max", "torch.max", "torch.max", "torch.max", "x.unsqueeze", "range", "torch.LongTensor().zero_.transpose", "torch.LongTensor().zero_.transpose", "mask.unsqueeze().unsqueeze", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.cuda.LongTensor", "torch.cuda.LongTensor", "torch.cuda.LongTensor", "torch.cuda.LongTensor", "torch.cuda.LongTensor", "torch.cuda.LongTensor", "torch.cuda.LongTensor", "torch.cuda.LongTensor", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "mask.unsqueeze", "torch.arange", "torch.arange", "torch.arange", "torch.arange"], "methods", ["home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.size"], ["", "def", "decode", "(", "self", ",", "energy", ",", "mask", "=", "None", ",", "leading_symbolic", "=", "0", ")", ":", "\n", "        ", "\"\"\"\n\n        Args:\n            energy: Tensor\n                the input tensor with shape = [length, batch_size, num_label, num_label]\n            leading_symbolic: nt\n                number of symbolic labels leading in type alphabets (set it to 0 if you are not sure)\n\n        Returns: Tensor\n            decoding results in shape [batch_size, length]\n\n        \"\"\"", "\n", "if", "mask", "is", "not", "None", ":", "\n", "            ", "energy", "=", "energy", "*", "mask", ".", "unsqueeze", "(", "2", ")", ".", "unsqueeze", "(", "3", ")", "\n", "# Input should be provided as (batch_size, n_time_steps, num_labels, num_labels)", "\n", "# For convenience, we need to dimshuffle to (n_time_steps, batch_size, num_labels, num_labels)", "\n", "", "energy_transpose", "=", "energy", ".", "transpose", "(", "0", ",", "1", ")", "\n", "\n", "# the last row and column is the tag for pad symbol. reduce these two dimensions by 1 to remove that.", "\n", "# also remove the first #symbolic rows and columns.", "\n", "# now the shape of energies_shuffled is [n_time_steps, batch_size, t, t] where t = num_labels - #symbolic - 1.", "\n", "energy_transpose", "=", "energy_transpose", "[", ":", ",", ":", ",", "leading_symbolic", ":", "-", "1", ",", "leading_symbolic", ":", "-", "1", "]", "\n", "\n", "length", ",", "batch_size", ",", "num_label", ",", "_", "=", "energy_transpose", ".", "size", "(", ")", "\n", "if", "energy", ".", "is_cuda", ":", "\n", "            ", "batch_index", "=", "torch", ".", "arange", "(", "0", ",", "batch_size", ")", ".", "long", "(", ")", ".", "cuda", "(", ")", "\n", "pi", "=", "torch", ".", "zeros", "(", "[", "length", ",", "batch_size", ",", "num_label", ",", "1", "]", ")", ".", "cuda", "(", ")", "\n", "pointer", "=", "torch", ".", "cuda", ".", "LongTensor", "(", "length", ",", "batch_size", ",", "num_label", ")", ".", "zero_", "(", ")", "\n", "back_pointer", "=", "torch", ".", "cuda", ".", "LongTensor", "(", "length", ",", "batch_size", ",", "1", ")", ".", "zero_", "(", ")", "\n", "", "else", ":", "\n", "            ", "batch_index", "=", "torch", ".", "arange", "(", "0", ",", "batch_size", ")", ".", "long", "(", ")", "\n", "pi", "=", "torch", ".", "zeros", "(", "[", "length", ",", "batch_size", ",", "num_label", ",", "1", "]", ")", "\n", "pointer", "=", "torch", ".", "LongTensor", "(", "length", ",", "batch_size", ",", "num_label", ")", ".", "zero_", "(", ")", "\n", "back_pointer", "=", "torch", ".", "LongTensor", "(", "length", ",", "batch_size", ",", "1", ")", ".", "zero_", "(", ")", "\n", "\n", "", "pi", "[", "0", "]", "=", "energy", "[", ":", ",", "0", ",", "-", "1", ",", "leading_symbolic", ":", "-", "1", "]", ".", "unsqueeze", "(", "-", "1", ")", "\n", "pointer", "[", "0", "]", "=", "-", "1", "\n", "for", "t", "in", "range", "(", "1", ",", "length", ")", ":", "\n", "            ", "pi_prev", "=", "pi", "[", "t", "-", "1", "]", "\n", "x", ",", "y", "=", "torch", ".", "max", "(", "energy_transpose", "[", "t", "]", "+", "pi_prev", ",", "dim", "=", "1", ")", "\n", "pi", "[", "t", "]", "=", "x", ".", "unsqueeze", "(", "-", "1", ")", "\n", "pointer", "[", "t", "]", "=", "y", "\n", "", "_", ",", "back_pointer", "[", "-", "1", "]", "=", "torch", ".", "max", "(", "pi", "[", "-", "1", "]", ",", "dim", "=", "1", ")", "\n", "back_pointer", "=", "back_pointer", ".", "squeeze", "(", "-", "1", ")", "\n", "for", "t", "in", "reversed", "(", "range", "(", "length", "-", "1", ")", ")", ":", "\n", "            ", "pointer_last", "=", "pointer", "[", "t", "+", "1", "]", "\n", "back_pointer", "[", "t", "]", "=", "pointer_last", "[", "batch_index", ",", "back_pointer", "[", "t", "+", "1", "]", "]", "\n", "", "return", "back_pointer", ".", "transpose", "(", "0", ",", "1", ")", "+", "leading_symbolic", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.modules.crf.TreeCRF.__init__": [[196, 212], ["torch.Module.__init__", "attention.BiAAttention"], "methods", ["home.repos.pwc.inspect_result.Jivnesh_LCM.io_.reader.Reader.__init__"], ["def", "__init__", "(", "self", ",", "input_size", ",", "num_labels", ",", "biaffine", "=", "True", ")", ":", "\n", "        ", "'''\n\n        Args:\n            input_size: int\n                the dimension of the input.\n            num_labels: int\n                the number of labels of the crf layer\n            biaffine: bool\n                if apply bi-affine parameter.\n            **kwargs:\n        '''", "\n", "super", "(", "TreeCRF", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "input_size", "=", "input_size", "\n", "self", ".", "num_labels", "=", "num_labels", "\n", "self", ".", "attention", "=", "BiAAttention", "(", "input_size", ",", "input_size", ",", "num_labels", ",", "biaffine", "=", "biaffine", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.modules.crf.TreeCRF.forward": [[213, 236], ["input_h.size", "crf.TreeCRF.attention", "torch.diag", "torch.diag", "torch.diag", "torch.diag", "crf.TreeCRF.data.new().fill_", "crf.TreeCRF.data.new"], "methods", ["home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.size"], ["", "def", "forward", "(", "self", ",", "input_h", ",", "input_c", ",", "mask", "=", "None", ")", ":", "\n", "        ", "'''\n\n        Args:\n            input_h: Tensor\n                the head input tensor with shape = [batch_size, length, input_size]\n            input_c: Tensor\n                the child input tensor with shape = [batch_size, length, input_size]\n            mask: Tensor or None\n                the mask tensor with shape = [batch_size, length]\n            lengths: Tensor or None\n                the length tensor with shape = [batch_size]\n\n        Returns: Tensor\n            the energy tensor with shape = [batch_size, num_label, length, length]\n\n        '''", "\n", "_", ",", "length", ",", "_", "=", "input_h", ".", "size", "(", ")", "\n", "# [batch_size, num_labels, length, length]", "\n", "output", "=", "self", ".", "attention", "(", "input_h", ",", "input_c", ",", "mask_d", "=", "mask", ",", "mask_e", "=", "mask", ")", "\n", "# set diagonal elements to -inf", "\n", "output", "=", "output", "+", "torch", ".", "diag", "(", "output", ".", "data", ".", "new", "(", "length", ")", ".", "fill_", "(", "-", "np", ".", "inf", ")", ")", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.modules.crf.TreeCRF.loss": [[237, 307], ["input_h.size", "crf.TreeCRF.forward", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "A.sum.sum.sum", "A.sum.sum.sum", "crf.TreeCRF.data.new", "range", "torch.arange().view().expand", "torch.arange().view().expand", "torch.arange().view().expand", "torch.arange().view().expand", "index.type_as().long.type_as().long.type_as().long", "torch.arange().type_as().long", "torch.arange().type_as().long", "torch.arange().type_as().long", "torch.arange().type_as().long", "tgt_energy.sum.sum.sum", "A.sum.sum.data.new().zero_", "torch.eye().type_as", "torch.eye().type_as", "torch.eye().type_as", "torch.eye().type_as", "utils.nlinalg.logdet", "mask.unsqueeze().unsqueeze", "mask.data.sum().long", "torch.arange().view", "torch.arange().view", "torch.arange().view", "torch.arange().view", "index.type_as().long.type_as().long.type_as", "torch.arange().type_as", "torch.arange().type_as", "torch.arange().type_as", "torch.arange().type_as", "mask.unsqueeze().unsqueeze", "A.sum.sum.data.new", "torch.eye", "torch.eye", "torch.eye", "torch.eye", "mask.unsqueeze", "A.sum.sum.size", "range", "mask.data.sum", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "arc_tags.data.t", "heads.data.t", "mask.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.size", "home.repos.pwc.inspect_result.Jivnesh_LCM.models.parsing_gating.Gating.forward", "home.repos.pwc.inspect_result.Jivnesh_LCM.nlinalg.nlinalg.logdet", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.size"], ["", "def", "loss", "(", "self", ",", "input_h", ",", "input_c", ",", "heads", ",", "arc_tags", ",", "mask", "=", "None", ",", "lengths", "=", "None", ")", ":", "\n", "        ", "'''\n\n        Args:\n            input_h: Tensor\n                the head input tensor with shape = [batch_size, length, input_size]\n            input_c: Tensor\n                the child input tensor with shape = [batch_size, length, input_size]\n            target: Tensor\n                the tensor of target labels with shape [batch_size, length]\n            mask:Tensor or None\n                the mask tensor with shape = [batch_size, length]\n            lengths: tensor or list of int\n                the length of each input shape = [batch_size]\n\n        Returns: Tensor\n                A 1D tensor for minus log likelihood loss\n        '''", "\n", "batch_size", ",", "length", ",", "_", "=", "input_h", ".", "size", "(", ")", "\n", "energy", "=", "self", ".", "forward", "(", "input_h", ",", "input_c", ",", "mask", "=", "mask", ")", "\n", "# [batch_size, num_labels, length, length]", "\n", "A", "=", "torch", ".", "exp", "(", "energy", ")", "\n", "# mask out invalid positions", "\n", "if", "mask", "is", "not", "None", ":", "\n", "            ", "A", "=", "A", "*", "mask", ".", "unsqueeze", "(", "1", ")", ".", "unsqueeze", "(", "3", ")", "*", "mask", ".", "unsqueeze", "(", "1", ")", ".", "unsqueeze", "(", "2", ")", "\n", "\n", "# sum along the label axis [batch_size, length, length]", "\n", "", "A", "=", "A", ".", "sum", "(", "dim", "=", "1", ")", "\n", "# get D [batch_size, 1, length]", "\n", "D", "=", "A", ".", "sum", "(", "dim", "=", "1", ",", "keepdim", "=", "True", ")", "\n", "\n", "# make sure L is positive-defined", "\n", "rtol", "=", "1e-4", "\n", "atol", "=", "1e-6", "\n", "D", "+=", "D", "*", "rtol", "+", "atol", "\n", "\n", "# [batch_size, length, length]", "\n", "D", "=", "A", ".", "data", ".", "new", "(", "A", ".", "size", "(", ")", ")", ".", "zero_", "(", ")", "+", "D", "\n", "# zeros out all elements except diagonal.", "\n", "D", "=", "D", "*", "torch", ".", "eye", "(", "length", ")", ".", "type_as", "(", "D", ")", "\n", "\n", "# compute laplacian matrix", "\n", "# [batch_size, length, length]", "\n", "L", "=", "D", "-", "A", "\n", "\n", "# compute lengths", "\n", "if", "lengths", "is", "None", ":", "\n", "            ", "if", "mask", "is", "None", ":", "\n", "                ", "lengths", "=", "[", "length", "for", "_", "in", "range", "(", "batch_size", ")", "]", "\n", "", "else", ":", "\n", "                ", "lengths", "=", "mask", ".", "data", ".", "sum", "(", "dim", "=", "1", ")", ".", "long", "(", ")", "\n", "\n", "# compute partition Z(x) [batch_size]", "\n", "", "", "z", "=", "energy", ".", "data", ".", "new", "(", "batch_size", ")", "\n", "for", "b", "in", "range", "(", "batch_size", ")", ":", "\n", "            ", "Lx", "=", "L", "[", "b", ",", "1", ":", "lengths", "[", "b", "]", ",", "1", ":", "lengths", "[", "b", "]", "]", "\n", "# print(torch.log(torch.eig(Lx.data)[0]))", "\n", "z", "[", "b", "]", "=", "logdet", "(", "Lx", ")", "\n", "\n", "# first create index matrix [length, batch_size]", "\n", "# index = torch.zeros(length, batch_size) + torch.arange(0, length).view(length, 1)", "\n", "", "index", "=", "torch", ".", "arange", "(", "0", ",", "length", ")", ".", "view", "(", "length", ",", "1", ")", ".", "expand", "(", "length", ",", "batch_size", ")", "\n", "index", "=", "index", ".", "type_as", "(", "energy", ".", "data", ")", ".", "long", "(", ")", "\n", "batch_index", "=", "torch", ".", "arange", "(", "0", ",", "batch_size", ")", ".", "type_as", "(", "energy", ".", "data", ")", ".", "long", "(", ")", "\n", "# compute target energy [length-1, batch_size]", "\n", "tgt_energy", "=", "energy", "[", "batch_index", ",", "arc_tags", ".", "data", ".", "t", "(", ")", ",", "heads", ".", "data", ".", "t", "(", ")", ",", "index", "]", "[", "1", ":", "]", "\n", "# sum over dim=0 shape = [batch_size]", "\n", "tgt_energy", "=", "tgt_energy", ".", "sum", "(", "dim", "=", "0", ")", "\n", "\n", "return", "z", "-", "tgt_energy", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.modules.crf.ChainCRF_with_LE.__init__": [[309, 330], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "crf.ChainCRF_with_LE.reset_parameters"], "methods", ["home.repos.pwc.inspect_result.Jivnesh_LCM.io_.reader.Reader.__init__", "home.repos.pwc.inspect_result.Jivnesh_LCM.models.parsing_gating.BiRecurrentConv_Encoder.reset_parameters"], ["    ", "def", "__init__", "(", "self", ",", "input_size", ",", "num_labels", ",", "bigram", "=", "True", ")", ":", "\n", "        ", "'''\n\n        Args:\n            input_size: int\n                the dimension of the input.\n            num_labels: int\n                the number of labels of the crf layer\n            bigram: bool\n                if apply bi-gram parameter.\n        '''", "\n", "super", "(", "ChainCRF_with_LE", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "input_size", "=", "input_size", "\n", "self", ".", "num_labels", "=", "num_labels", "+", "1", "\n", "self", ".", "pad_label_id", "=", "num_labels", "\n", "self", ".", "bigram", "=", "bigram", "\n", "\n", "# state weight tensor", "\n", "self", ".", "state_nn", "=", "nn", ".", "Linear", "(", "input_size", ",", "self", ".", "num_labels", ")", "\n", "\n", "self", ".", "reset_parameters", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.modules.crf.ChainCRF_with_LE.reset_parameters": [[331, 333], ["torch.init.constant_", "torch.init.constant_"], "methods", ["None"], ["", "def", "reset_parameters", "(", "self", ")", ":", "\n", "        ", "nn", ".", "init", ".", "constant_", "(", "self", ".", "state_nn", ".", "bias", ",", "0.", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.modules.crf.ChainCRF_with_LE.forward": [[334, 364], ["input.size", "crf.ChainCRF_with_LE.state_nn().unsqueeze", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.t", "torch.t", "torch.t", "torch.t", "col_zeros.cuda.cuda.cuda", "row_zeros.cuda.cuda.cuda", "crf.ChainCRF_with_LE.state_nn"], "methods", ["home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.size"], ["", "def", "forward", "(", "self", ",", "input", ",", "LE", ",", "mask", "=", "None", ")", ":", "\n", "        ", "'''\n\n        Args:\n            input: Tensor\n                the input tensor with shape = [batch_size, length, input_size]\n            mask: Tensor or None\n                the mask tensor with shape = [batch_size, length]\n\n        Returns: Tensor\n            the energy tensor with shape = [batch_size, length, num_label, num_label]\n\n        '''", "\n", "batch_size", ",", "length", ",", "_", "=", "input", ".", "size", "(", ")", "\n", "\n", "# compute out_s by tensor dot [batch_size, length, input_size] * [input_size, num_label]", "\n", "# thus out_s should be [batch_size, length, num_label] --> [batch_size, length, num_label, 1]", "\n", "out_s", "=", "self", ".", "state_nn", "(", "input", ")", ".", "unsqueeze", "(", "-", "1", ")", "\n", "out_t", "=", "torch", ".", "matmul", "(", "LE", ",", "torch", ".", "t", "(", "LE", ")", ")", "\n", "# add column of zeros (for END token)", "\n", "col_zeros", "=", "torch", ".", "zeros", "(", "(", "out_t", ".", "shape", "[", "0", "]", ",", "1", ")", ")", "\n", "row_zeros", "=", "torch", ".", "zeros", "(", "(", "1", ",", "out_t", ".", "shape", "[", "1", "]", "+", "1", ")", ")", "\n", "if", "out_t", ".", "is_cuda", ":", "\n", "            ", "col_zeros", "=", "col_zeros", ".", "cuda", "(", ")", "\n", "row_zeros", "=", "row_zeros", ".", "cuda", "(", ")", "\n", "", "out_t", "=", "torch", ".", "cat", "(", "(", "out_t", ",", "col_zeros", ")", ",", "dim", "=", "1", ")", "\n", "# add row of zeros (for END token)", "\n", "out_t", "=", "torch", ".", "cat", "(", "(", "out_t", ",", "row_zeros", ")", ")", "\n", "# the output should be [batch_size, length, num_label, num_label]", "\n", "return", "(", "out_s", ",", "out_t", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.modules.crf.ChainCRF_with_LE.loss": [[365, 424], ["energy.size", "energy.transpose", "target.transpose", "range", "length.max", "mask.unsqueeze().transpose", "torch.arange().long().cuda", "torch.arange().long().cuda", "torch.arange().long().cuda", "torch.arange().long().cuda", "torch.cuda.LongTensor().fill_", "torch.cuda.LongTensor().fill_", "torch.cuda.LongTensor().fill_", "torch.cuda.LongTensor().fill_", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.LongTensor().fill_", "torch.LongTensor().fill_", "torch.LongTensor().fill_", "torch.LongTensor().fill_", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "utils.nlinalg.logsumexp", "energy.size", "mask.unsqueeze().unsqueeze", "utils.nlinalg.logsumexp", "mask.unsqueeze", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.cuda.LongTensor", "torch.cuda.LongTensor", "torch.cuda.LongTensor", "torch.cuda.LongTensor", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "mask.unsqueeze", "partition.unsqueeze", "torch.arange", "torch.arange", "torch.arange", "torch.arange"], "methods", ["home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.size", "home.repos.pwc.inspect_result.Jivnesh_LCM.nlinalg.nlinalg.logsumexp", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.size", "home.repos.pwc.inspect_result.Jivnesh_LCM.nlinalg.nlinalg.logsumexp"], ["", "def", "loss", "(", "self", ",", "energy", ",", "target", ",", "mask", "=", "None", ",", "length", "=", "None", ")", ":", "\n", "        ", "'''\n\n        Args:\n            energy: Tensor\n                the input tensor with shape = [batch_size, length, num_label, num_label]\n            target: Tensor\n                the tensor of target labels with shape [batch_size, length]\n            mask:Tensor or None\n                the mask tensor with shape = [batch_size, length]\n\n        Returns: Tensor\n                A 1D tensor for minus log likelihood loss\n        '''", "\n", "if", "length", "is", "not", "None", ":", "\n", "            ", "max_len", "=", "length", ".", "max", "(", ")", "\n", "if", "energy", ".", "size", "(", "1", ")", "!=", "max_len", ":", "\n", "                ", "target", "=", "target", "[", ":", ",", ":", "max_len", "]", "\n", "", "", "mask_transpose", "=", "None", "\n", "if", "mask", "is", "not", "None", ":", "\n", "            ", "energy", "=", "energy", "*", "mask", ".", "unsqueeze", "(", "2", ")", ".", "unsqueeze", "(", "3", ")", "\n", "mask_transpose", "=", "mask", ".", "unsqueeze", "(", "2", ")", ".", "transpose", "(", "0", ",", "1", ")", "\n", "\n", "", "batch_size", ",", "len", ",", "_", ",", "_", "=", "energy", ".", "size", "(", ")", "\n", "# shape = [length, batch_size, num_label, num_label]", "\n", "energy_transpose", "=", "energy", ".", "transpose", "(", "0", ",", "1", ")", "\n", "# shape = [length, batch_size]", "\n", "target_transpose", "=", "target", ".", "transpose", "(", "0", ",", "1", ")", "\n", "# shape = [length, batch_size, 1]", "\n", "\n", "# shape = [batch_size, num_label]", "\n", "partition", "=", "None", "\n", "if", "energy", ".", "is_cuda", ":", "\n", "# shape = [batch_size]", "\n", "            ", "batch_index", "=", "torch", ".", "arange", "(", "0", ",", "batch_size", ")", ".", "long", "(", ")", ".", "cuda", "(", ")", "\n", "prev_label", "=", "torch", ".", "cuda", ".", "LongTensor", "(", "batch_size", ")", ".", "fill_", "(", "self", ".", "num_labels", "-", "1", ")", "\n", "tgt_energy", "=", "torch", ".", "zeros", "(", "batch_size", ")", ".", "cuda", "(", ")", "\n", "", "else", ":", "\n", "# shape = [batch_size]", "\n", "            ", "batch_index", "=", "torch", ".", "arange", "(", "0", ",", "batch_size", ")", ".", "long", "(", ")", "\n", "prev_label", "=", "torch", ".", "LongTensor", "(", "batch_size", ")", ".", "fill_", "(", "self", ".", "num_labels", "-", "1", ")", "\n", "tgt_energy", "=", "torch", ".", "zeros", "(", "batch_size", ")", "\n", "\n", "", "for", "t", "in", "range", "(", "len", ")", ":", "\n", "# shape = [batch_size, num_label, num_label]", "\n", "            ", "curr_energy", "=", "energy_transpose", "[", "t", "]", "\n", "if", "t", "==", "0", ":", "\n", "                ", "partition", "=", "curr_energy", "[", ":", ",", "-", "1", ",", ":", "]", "\n", "", "else", ":", "\n", "# shape = [batch_size, num_label]", "\n", "                ", "partition_new", "=", "logsumexp", "(", "curr_energy", "+", "partition", ".", "unsqueeze", "(", "2", ")", ",", "dim", "=", "1", ")", "\n", "if", "mask_transpose", "is", "None", ":", "\n", "                    ", "partition", "=", "partition_new", "\n", "", "else", ":", "\n", "                    ", "mask_t", "=", "mask_transpose", "[", "t", "]", "\n", "partition", "=", "partition", "+", "(", "partition_new", "-", "partition", ")", "*", "mask_t", "\n", "", "", "tgt_energy", "+=", "curr_energy", "[", "batch_index", ",", "prev_label", ",", "target_transpose", "[", "t", "]", ".", "data", "]", "\n", "prev_label", "=", "target_transpose", "[", "t", "]", ".", "data", "\n", "", "return", "logsumexp", "(", "partition", ",", "dim", "=", "1", ")", "-", "tgt_energy", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.modules.crf.ChainCRF_with_LE.decode": [[425, 474], ["energy.transpose", "energy.transpose.size", "energy[].unsqueeze", "range", "torch.max", "torch.max", "torch.max", "torch.max", "torch.LongTensor().zero_.squeeze", "torch.LongTensor().zero_.squeeze", "reversed", "torch.arange().long().cuda", "torch.arange().long().cuda", "torch.arange().long().cuda", "torch.arange().long().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.cuda.LongTensor().zero_", "torch.cuda.LongTensor().zero_", "torch.cuda.LongTensor().zero_", "torch.cuda.LongTensor().zero_", "torch.cuda.LongTensor().zero_", "torch.cuda.LongTensor().zero_", "torch.cuda.LongTensor().zero_", "torch.cuda.LongTensor().zero_", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.LongTensor().zero_", "torch.LongTensor().zero_", "torch.LongTensor().zero_", "torch.LongTensor().zero_", "torch.LongTensor().zero_", "torch.LongTensor().zero_", "torch.LongTensor().zero_", "torch.LongTensor().zero_", "torch.max", "torch.max", "torch.max", "torch.max", "x.unsqueeze", "range", "torch.LongTensor().zero_.transpose", "torch.LongTensor().zero_.transpose", "mask.unsqueeze().unsqueeze", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.cuda.LongTensor", "torch.cuda.LongTensor", "torch.cuda.LongTensor", "torch.cuda.LongTensor", "torch.cuda.LongTensor", "torch.cuda.LongTensor", "torch.cuda.LongTensor", "torch.cuda.LongTensor", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "mask.unsqueeze", "torch.arange", "torch.arange", "torch.arange", "torch.arange"], "methods", ["home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.size"], ["", "def", "decode", "(", "self", ",", "energy", ",", "mask", "=", "None", ",", "leading_symbolic", "=", "0", ")", ":", "\n", "        ", "\"\"\"\n\n        Args:\n            energy: Tensor\n                the input tensor with shape = [length, batch_size, num_label, num_label]\n            leading_symbolic: nt\n                number of symbolic labels leading in type alphabets (set it to 0 if you are not sure)\n\n        Returns: Tensor\n            decoding results in shape [batch_size, length]\n\n        \"\"\"", "\n", "if", "mask", "is", "not", "None", ":", "\n", "            ", "energy", "=", "energy", "*", "mask", ".", "unsqueeze", "(", "2", ")", ".", "unsqueeze", "(", "3", ")", "\n", "# Input should be provided as (batch_size, n_time_steps, num_labels, num_labels)", "\n", "# For convenience, we need to dimshuffle to (n_time_steps, batch_size, num_labels, num_labels)", "\n", "", "energy_transpose", "=", "energy", ".", "transpose", "(", "0", ",", "1", ")", "\n", "\n", "# the last row and column is the tag for pad symbol. reduce these two dimensions by 1 to remove that.", "\n", "# also remove the first #symbolic rows and columns.", "\n", "# now the shape of energies_shuffled is [n_time_steps, batch_size, t, t] where t = num_labels - #symbolic - 1.", "\n", "energy_transpose", "=", "energy_transpose", "[", ":", ",", ":", ",", "leading_symbolic", ":", "-", "1", ",", "leading_symbolic", ":", "-", "1", "]", "\n", "\n", "length", ",", "batch_size", ",", "num_label", ",", "_", "=", "energy_transpose", ".", "size", "(", ")", "\n", "if", "energy", ".", "is_cuda", ":", "\n", "            ", "batch_index", "=", "torch", ".", "arange", "(", "0", ",", "batch_size", ")", ".", "long", "(", ")", ".", "cuda", "(", ")", "\n", "pi", "=", "torch", ".", "zeros", "(", "[", "length", ",", "batch_size", ",", "num_label", ",", "1", "]", ")", ".", "cuda", "(", ")", "\n", "pointer", "=", "torch", ".", "cuda", ".", "LongTensor", "(", "length", ",", "batch_size", ",", "num_label", ")", ".", "zero_", "(", ")", "\n", "back_pointer", "=", "torch", ".", "cuda", ".", "LongTensor", "(", "length", ",", "batch_size", ",", "1", ")", ".", "zero_", "(", ")", "\n", "", "else", ":", "\n", "            ", "batch_index", "=", "torch", ".", "arange", "(", "0", ",", "batch_size", ")", ".", "long", "(", ")", "\n", "pi", "=", "torch", ".", "zeros", "(", "[", "length", ",", "batch_size", ",", "num_label", ",", "1", "]", ")", "\n", "pointer", "=", "torch", ".", "LongTensor", "(", "length", ",", "batch_size", ",", "num_label", ")", ".", "zero_", "(", ")", "\n", "back_pointer", "=", "torch", ".", "LongTensor", "(", "length", ",", "batch_size", ",", "1", ")", ".", "zero_", "(", ")", "\n", "\n", "", "pi", "[", "0", "]", "=", "energy", "[", ":", ",", "0", ",", "-", "1", ",", "leading_symbolic", ":", "-", "1", "]", ".", "unsqueeze", "(", "-", "1", ")", "\n", "pointer", "[", "0", "]", "=", "-", "1", "\n", "for", "t", "in", "range", "(", "1", ",", "length", ")", ":", "\n", "            ", "pi_prev", "=", "pi", "[", "t", "-", "1", "]", "\n", "x", ",", "y", "=", "torch", ".", "max", "(", "energy_transpose", "[", "t", "]", "+", "pi_prev", ",", "dim", "=", "1", ")", "\n", "pi", "[", "t", "]", "=", "x", ".", "unsqueeze", "(", "-", "1", ")", "\n", "pointer", "[", "t", "]", "=", "y", "\n", "", "_", ",", "back_pointer", "[", "-", "1", "]", "=", "torch", ".", "max", "(", "pi", "[", "-", "1", "]", ",", "dim", "=", "1", ")", "\n", "back_pointer", "=", "back_pointer", ".", "squeeze", "(", "-", "1", ")", "\n", "for", "t", "in", "reversed", "(", "range", "(", "length", "-", "1", ")", ")", ":", "\n", "            ", "pointer_last", "=", "pointer", "[", "t", "+", "1", "]", "\n", "back_pointer", "[", "t", "]", "=", "pointer_last", "[", "batch_index", ",", "back_pointer", "[", "t", "+", "1", "]", "]", "\n", "", "return", "back_pointer", ".", "transpose", "(", "0", ",", "1", ")", "+", "leading_symbolic", "\n", "", "", ""]], "home.repos.pwc.inspect_result.Jivnesh_LCM.modules.sparse.Embedding.__init__": [[38, 52], ["torch.Module.__init__", "torch.nn.parameter.Parameter", "torch.nn.parameter.Parameter", "sparse.Embedding.reset_parameters", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor"], "methods", ["home.repos.pwc.inspect_result.Jivnesh_LCM.io_.reader.Reader.__init__", "home.repos.pwc.inspect_result.Jivnesh_LCM.models.parsing_gating.BiRecurrentConv_Encoder.reset_parameters"], ["def", "__init__", "(", "self", ",", "num_embeddings", ",", "embedding_dim", ",", "init_embedding", "=", "None", ",", "freeze", "=", "False", ",", "padding_idx", "=", "None", ",", "\n", "max_norm", "=", "None", ",", "norm_type", "=", "2", ",", "scale_grad_by_freq", "=", "False", ",", "sparse", "=", "False", ")", ":", "\n", "        ", "super", "(", "Embedding", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "num_embeddings", "=", "num_embeddings", "\n", "self", ".", "embedding_dim", "=", "embedding_dim", "\n", "self", ".", "padding_idx", "=", "padding_idx", "\n", "self", ".", "max_norm", "=", "max_norm", "\n", "self", ".", "norm_type", "=", "norm_type", "\n", "self", ".", "scale_grad_by_freq", "=", "scale_grad_by_freq", "\n", "self", ".", "weight", "=", "Parameter", "(", "torch", ".", "Tensor", "(", "num_embeddings", ",", "embedding_dim", ")", ")", "\n", "self", ".", "frozen", "=", "freeze", "\n", "self", ".", "sparse", "=", "sparse", "\n", "\n", "self", ".", "reset_parameters", "(", "init_embedding", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.modules.sparse.Embedding.reset_parameters": [[53, 66], ["numpy.sqrt", "sparse.Embedding.weight.data.uniform_", "init.assign_tensor", "sparse.Embedding.weight.data[].fill_", "Warning"], "methods", ["home.repos.pwc.inspect_result.Jivnesh_LCM.nn.init.assign_tensor"], ["", "def", "reset_parameters", "(", "self", ",", "init_embedding", ")", ":", "\n", "        ", "if", "init_embedding", "is", "None", ":", "\n", "            ", "scale", "=", "np", ".", "sqrt", "(", "3.0", "/", "self", ".", "embedding_dim", ")", "\n", "self", ".", "weight", ".", "data", ".", "uniform_", "(", "-", "scale", ",", "scale", ")", "\n", "", "else", ":", "\n", "            ", "assign_tensor", "(", "self", ".", "weight", ",", "init_embedding", ")", "\n", "", "if", "self", ".", "padding_idx", "is", "not", "None", ":", "\n", "            ", "self", ".", "weight", ".", "data", "[", "self", ".", "padding_idx", "]", ".", "fill_", "(", "0", ")", "\n", "\n", "", "if", "self", ".", "frozen", ":", "\n", "            ", "if", "init_embedding", "is", "None", ":", "\n", "                ", "raise", "Warning", "(", "'Freeze embeddings which are randomly initialized.'", ")", "\n", "", "self", ".", "weight", ".", "requires_grad", "=", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.modules.sparse.Embedding.freeze": [[67, 70], ["None"], "methods", ["None"], ["", "", "def", "freeze", "(", "self", ")", ":", "\n", "        ", "self", ".", "weight", ".", "requires_grad", "=", "False", "\n", "self", ".", "frozen", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.modules.sparse.Embedding.forward": [[71, 85], ["input.view.view.size", "torch.nn.functional.embedding().view", "torch.nn.functional.embedding().view", "input.view.view.dim", "int", "input.view.view.view", "numpy.prod", "torch.nn.functional.embedding", "torch.nn.functional.embedding"], "methods", ["home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.size"], ["", "def", "forward", "(", "self", ",", "input", ")", ":", "\n", "        ", "padding_idx", "=", "self", ".", "padding_idx", "\n", "if", "padding_idx", "is", "None", ":", "\n", "            ", "padding_idx", "=", "-", "1", "\n", "\n", "", "input_size", "=", "input", ".", "size", "(", ")", "\n", "if", "input", ".", "dim", "(", ")", ">", "2", ":", "\n", "            ", "num_inputs", "=", "int", "(", "np", ".", "prod", "(", "input_size", "[", ":", "-", "1", "]", ")", ")", "\n", "input", "=", "input", ".", "view", "(", "num_inputs", ",", "input_size", "[", "-", "1", "]", ")", "\n", "\n", "", "output_size", "=", "input_size", "+", "(", "self", ".", "embedding_dim", ",", ")", "\n", "return", "embedding", "(", "input", ",", "self", ".", "weight", ",", "padding_idx", ",", "self", ".", "max_norm", ",", "\n", "self", ".", "norm_type", ",", "self", ".", "scale_grad_by_freq", ",", "\n", "self", ".", "sparse", ")", ".", "view", "(", "output_size", ")", "\n", "#return self._backend.Embedding.apply(", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.modules.sparse.Embedding.__repr__": [[90, 104], ["s.format"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "s", "=", "'{name}({num_embeddings}, {embedding_dim}'", "\n", "if", "self", ".", "padding_idx", "is", "not", "None", ":", "\n", "            ", "s", "+=", "', padding_idx={padding_idx}'", "\n", "", "if", "self", ".", "max_norm", "is", "not", "None", ":", "\n", "            ", "s", "+=", "', max_norm={max_norm}'", "\n", "", "if", "self", ".", "norm_type", "!=", "2", ":", "\n", "            ", "s", "+=", "', norm_type={norm_type}'", "\n", "", "if", "self", ".", "scale_grad_by_freq", "is", "not", "False", ":", "\n", "            ", "s", "+=", "', scale_grad_by_freq={scale_grad_by_freq}'", "\n", "", "if", "self", ".", "sparse", "is", "not", "False", ":", "\n", "            ", "s", "+=", "', sparse=True'", "\n", "", "s", "+=", "')'", "\n", "return", "s", ".", "format", "(", "name", "=", "self", ".", "__class__", ".", "__name__", ",", "**", "self", ".", "__dict__", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.Jivnesh_LCM.modules.attention.BiAAttention.__init__": [[12, 41], ["torch.Module.__init__", "torch.nn.parameter.Parameter", "torch.nn.parameter.Parameter", "torch.nn.parameter.Parameter", "torch.nn.parameter.Parameter", "torch.nn.parameter.Parameter", "torch.nn.parameter.Parameter", "torch.nn.parameter.Parameter", "torch.nn.parameter.Parameter", "torch.nn.parameter.Parameter", "attention.BiAAttention.reset_parameters", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.nn.parameter.Parameter", "torch.nn.parameter.Parameter", "torch.nn.parameter.Parameter", "attention.BiAAttention.register_parameter", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor"], "methods", ["home.repos.pwc.inspect_result.Jivnesh_LCM.io_.reader.Reader.__init__", "home.repos.pwc.inspect_result.Jivnesh_LCM.models.parsing_gating.BiRecurrentConv_Encoder.reset_parameters"], ["def", "__init__", "(", "self", ",", "input_size_encoder", ",", "input_size_decoder", ",", "num_labels", ",", "biaffine", "=", "True", ",", "**", "kwargs", ")", ":", "\n", "        ", "'''\n\n        Args:\n            input_size_encoder: int\n                the dimension of the encoder input.\n            input_size_decoder: int\n                the dimension of the decoder input.\n            num_labels: int\n                the number of labels of the crf layer\n            biaffine: bool\n                if apply bi-affine parameter.\n            **kwargs:\n        '''", "\n", "super", "(", "BiAAttention", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "input_size_encoder", "=", "input_size_encoder", "\n", "self", ".", "input_size_decoder", "=", "input_size_decoder", "\n", "self", ".", "num_labels", "=", "num_labels", "\n", "self", ".", "biaffine", "=", "biaffine", "\n", "\n", "self", ".", "W_d", "=", "Parameter", "(", "torch", ".", "Tensor", "(", "self", ".", "num_labels", ",", "self", ".", "input_size_decoder", ")", ")", "\n", "self", ".", "W_e", "=", "Parameter", "(", "torch", ".", "Tensor", "(", "self", ".", "num_labels", ",", "self", ".", "input_size_encoder", ")", ")", "\n", "self", ".", "b", "=", "Parameter", "(", "torch", ".", "Tensor", "(", "self", ".", "num_labels", ",", "1", ",", "1", ")", ")", "\n", "if", "self", ".", "biaffine", ":", "\n", "            ", "self", ".", "U", "=", "Parameter", "(", "torch", ".", "Tensor", "(", "self", ".", "num_labels", ",", "self", ".", "input_size_decoder", ",", "self", ".", "input_size_encoder", ")", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "register_parameter", "(", "'U'", ",", "None", ")", "\n", "\n", "", "self", ".", "reset_parameters", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.modules.attention.BiAAttention.reset_parameters": [[42, 48], ["torch.init.xavier_uniform_", "torch.init.xavier_uniform_", "torch.init.xavier_uniform_", "torch.init.xavier_uniform_", "torch.init.xavier_uniform_", "torch.init.xavier_uniform_", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_", "torch.init.xavier_uniform_", "torch.init.xavier_uniform_", "torch.init.xavier_uniform_"], "methods", ["None"], ["", "def", "reset_parameters", "(", "self", ")", ":", "\n", "        ", "nn", ".", "init", ".", "xavier_uniform_", "(", "self", ".", "W_d", ")", "\n", "nn", ".", "init", ".", "xavier_uniform_", "(", "self", ".", "W_e", ")", "\n", "nn", ".", "init", ".", "constant_", "(", "self", ".", "b", ",", "0.", ")", "\n", "if", "self", ".", "biaffine", ":", "\n", "            ", "nn", ".", "init", ".", "xavier_uniform_", "(", "self", ".", "U", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.modules.attention.BiAAttention.forward": [[49, 95], ["input_d.size", "input_e.size", "torch.matmul().unsqueeze", "torch.matmul().unsqueeze", "torch.matmul().unsqueeze", "torch.matmul().unsqueeze", "torch.matmul().unsqueeze", "torch.matmul().unsqueeze", "torch.matmul().unsqueeze", "torch.matmul().unsqueeze", "torch.matmul().unsqueeze", "torch.matmul().unsqueeze", "torch.matmul().unsqueeze", "torch.matmul().unsqueeze", "torch.matmul().unsqueeze", "torch.matmul().unsqueeze", "torch.matmul().unsqueeze", "torch.matmul().unsqueeze", "torch.matmul().unsqueeze", "torch.matmul().unsqueeze", "input_d.size", "input_e.size", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "input_d.unsqueeze", "input_e.unsqueeze().transpose", "mask_e.unsqueeze().unsqueeze", "input_d.transpose", "input_e.transpose", "mask_d.unsqueeze().unsqueeze", "input_e.unsqueeze", "mask_e.unsqueeze", "mask_d.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.size", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.size", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.size", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.size"], ["", "", "def", "forward", "(", "self", ",", "input_d", ",", "input_e", ",", "mask_d", "=", "None", ",", "mask_e", "=", "None", ")", ":", "\n", "        ", "'''\n\n        Args:\n            input_d: Tensor\n                the decoder input tensor with shape = [batch_size, length_decoder, input_size]\n            input_e: Tensor\n                the child input tensor with shape = [batch_size, length_encoder, input_size]\n            mask_d: Tensor or None\n                the mask tensor for decoder with shape = [batch_size, length_decoder]\n            mask_e: Tensor or None\n                the mask tensor for encoder with shape = [batch_size, length_encoder]\n\n        Returns: Tensor\n            the energy tensor with shape = [batch_size, num_label, length, length]\n\n        '''", "\n", "assert", "input_d", ".", "size", "(", "0", ")", "==", "input_e", ".", "size", "(", "0", ")", ",", "'batch sizes of encoder and decoder are requires to be equal.'", "\n", "batch_size", ",", "length_decoder", ",", "_", "=", "input_d", ".", "size", "(", ")", "\n", "_", ",", "length_encoder", ",", "_", "=", "input_e", ".", "size", "(", ")", "\n", "\n", "# compute decoder part: [num_label, input_size_decoder] * [batch_size, input_size_decoder, length_decoder]", "\n", "# the output shape is [batch_size, num_label, length_decoder]", "\n", "out_d", "=", "torch", ".", "matmul", "(", "self", ".", "W_d", ",", "input_d", ".", "transpose", "(", "1", ",", "2", ")", ")", ".", "unsqueeze", "(", "3", ")", "\n", "# compute decoder part: [num_label, input_size_encoder] * [batch_size, input_size_encoder, length_encoder]", "\n", "# the output shape is [batch_size, num_label, length_encoder]", "\n", "out_e", "=", "torch", ".", "matmul", "(", "self", ".", "W_e", ",", "input_e", ".", "transpose", "(", "1", ",", "2", ")", ")", ".", "unsqueeze", "(", "2", ")", "\n", "\n", "# output shape [batch_size, num_label, length_decoder, length_encoder]", "\n", "if", "self", ".", "biaffine", ":", "\n", "# compute bi-affine part", "\n", "# [batch_size, 1, length_decoder, input_size_decoder] * [num_labels, input_size_decoder, input_size_encoder]", "\n", "# output shape [batch_size, num_label, length_decoder, input_size_encoder]", "\n", "            ", "output", "=", "torch", ".", "matmul", "(", "input_d", ".", "unsqueeze", "(", "1", ")", ",", "self", ".", "U", ")", "\n", "# [batch_size, num_label, length_decoder, input_size_encoder] * [batch_size, 1, input_size_encoder, length_encoder]", "\n", "# output shape [batch_size, num_label, length_decoder, length_encoder]", "\n", "output", "=", "torch", ".", "matmul", "(", "output", ",", "input_e", ".", "unsqueeze", "(", "1", ")", ".", "transpose", "(", "2", ",", "3", ")", ")", "\n", "\n", "output", "=", "output", "+", "out_d", "+", "out_e", "+", "self", ".", "b", "\n", "", "else", ":", "\n", "            ", "output", "=", "out_d", "+", "out_d", "+", "self", ".", "b", "\n", "\n", "", "if", "mask_d", "is", "not", "None", ":", "\n", "            ", "output", "=", "output", "*", "mask_d", ".", "unsqueeze", "(", "1", ")", ".", "unsqueeze", "(", "3", ")", "*", "mask_e", ".", "unsqueeze", "(", "1", ")", ".", "unsqueeze", "(", "2", ")", "\n", "\n", "", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.modules.attention.ConcatAttention.__init__": [[103, 131], ["torch.Module.__init__", "torch.nn.parameter.Parameter", "torch.nn.parameter.Parameter", "torch.nn.parameter.Parameter", "torch.nn.parameter.Parameter", "torch.nn.parameter.Parameter", "torch.nn.parameter.Parameter", "torch.nn.parameter.Parameter", "torch.nn.parameter.Parameter", "torch.nn.parameter.Parameter", "torch.nn.parameter.Parameter", "torch.nn.parameter.Parameter", "torch.nn.parameter.Parameter", "attention.ConcatAttention.reset_parameters", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor"], "methods", ["home.repos.pwc.inspect_result.Jivnesh_LCM.io_.reader.Reader.__init__", "home.repos.pwc.inspect_result.Jivnesh_LCM.models.parsing_gating.BiRecurrentConv_Encoder.reset_parameters"], ["def", "__init__", "(", "self", ",", "input_size_encoder", ",", "input_size_decoder", ",", "hidden_size", ",", "num_labels", ",", "**", "kwargs", ")", ":", "\n", "        ", "'''\n\n        Args:\n            input_size_encoder: int\n                the dimension of the encoder input.\n            input_size_decoder: int\n                the dimension of the decoder input.\n            hidden_size: int\n                the dimension of the hidden.\n            num_labels: int\n                the number of labels of the crf layer\n            biaffine: bool\n                if apply bi-affine parameter.\n            **kwargs:\n        '''", "\n", "super", "(", "ConcatAttention", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "input_size_encoder", "=", "input_size_encoder", "\n", "self", ".", "input_size_decoder", "=", "input_size_decoder", "\n", "self", ".", "hidden_size", "=", "hidden_size", "\n", "self", ".", "num_labels", "=", "num_labels", "\n", "\n", "self", ".", "W_d", "=", "Parameter", "(", "torch", ".", "Tensor", "(", "self", ".", "input_size_decoder", ",", "self", ".", "hidden_size", ")", ")", "\n", "self", ".", "W_e", "=", "Parameter", "(", "torch", ".", "Tensor", "(", "self", ".", "input_size_encoder", ",", "self", ".", "hidden_size", ")", ")", "\n", "self", ".", "b", "=", "Parameter", "(", "torch", ".", "Tensor", "(", "self", ".", "hidden_size", ")", ")", "\n", "self", ".", "v", "=", "Parameter", "(", "torch", ".", "Tensor", "(", "self", ".", "hidden_size", ",", "self", ".", "num_labels", ")", ")", "\n", "\n", "self", ".", "reset_parameters", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.modules.attention.ConcatAttention.reset_parameters": [[132, 137], ["torch.init.xavier_uniform", "torch.init.xavier_uniform", "torch.init.xavier_uniform", "torch.init.xavier_uniform", "torch.init.xavier_uniform", "torch.init.xavier_uniform", "torch.init.xavier_uniform", "torch.init.xavier_uniform", "torch.init.xavier_uniform", "torch.init.constant", "torch.init.constant", "torch.init.constant"], "methods", ["None"], ["", "def", "reset_parameters", "(", "self", ")", ":", "\n", "        ", "nn", ".", "init", ".", "xavier_uniform", "(", "self", ".", "W_d", ")", "\n", "nn", ".", "init", ".", "xavier_uniform", "(", "self", ".", "W_e", ")", "\n", "nn", ".", "init", ".", "xavier_uniform", "(", "self", ".", "v", ")", "\n", "nn", ".", "init", ".", "constant", "(", "self", ".", "b", ",", "0.", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.modules.attention.ConcatAttention.forward": [[138, 176], ["input_d.size", "input_e.size", "torch.matmul().unsqueeze", "torch.matmul().unsqueeze", "torch.matmul().unsqueeze", "torch.matmul().unsqueeze", "torch.matmul().unsqueeze", "torch.matmul().unsqueeze", "torch.matmul().unsqueeze", "torch.matmul().unsqueeze", "torch.matmul().unsqueeze", "torch.matmul().unsqueeze", "torch.matmul().unsqueeze", "torch.matmul().unsqueeze", "torch.matmul().unsqueeze", "torch.matmul().unsqueeze", "torch.matmul().unsqueeze", "torch.matmul().unsqueeze", "torch.matmul().unsqueeze", "torch.matmul().unsqueeze", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.matmul().transpose", "torch.matmul().transpose", "torch.matmul().transpose", "torch.matmul().transpose", "torch.matmul().transpose", "torch.matmul().transpose", "torch.matmul().transpose", "torch.matmul().transpose", "torch.matmul().transpose", "input_d.size", "input_e.size", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul"], "methods", ["home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.size", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.size", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.size", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.size"], ["", "def", "forward", "(", "self", ",", "input_d", ",", "input_e", ",", "mask_d", "=", "None", ",", "mask_e", "=", "None", ")", ":", "\n", "        ", "'''\n\n        Args:\n            input_d: Tensor\n                the decoder input tensor with shape = [batch_size, length_decoder, input_size]\n            input_e: Tensor\n                the child input tensor with shape = [batch_size, length_encoder, input_size]\n            mask_d: Tensor or None\n                the mask tensor for decoder with shape = [batch_size, length_decoder]\n            mask_e: Tensor or None\n                the mask tensor for encoder with shape = [batch_size, length_encoder]\n\n        Returns: Tensor\n            the energy tensor with shape = [batch_size, num_label, length, length]\n\n        '''", "\n", "assert", "input_d", ".", "size", "(", "0", ")", "==", "input_e", ".", "size", "(", "0", ")", ",", "'batch sizes of encoder and decoder are requires to be equal.'", "\n", "batch_size", ",", "length_decoder", ",", "_", "=", "input_d", ".", "size", "(", ")", "\n", "_", ",", "length_encoder", ",", "_", "=", "input_e", ".", "size", "(", ")", "\n", "\n", "# compute decoder part: [batch_size, length_decoder, input_size_decoder] * [input_size_decoder, hidden_size]", "\n", "# the output shape is [batch_size, length_decoder, hidden_size]", "\n", "# then --> [batch_size, 1, length_decoder, hidden_size]", "\n", "out_d", "=", "torch", ".", "matmul", "(", "input_d", ",", "self", ".", "W_d", ")", ".", "unsqueeze", "(", "1", ")", "\n", "# compute decoder part: [batch_size, length_encoder, input_size_encoder] * [input_size_encoder, hidden_size]", "\n", "# the output shape is [batch_size, length_encoder, hidden_size]", "\n", "# then --> [batch_size, length_encoder, 1, hidden_size]", "\n", "out_e", "=", "torch", ".", "matmul", "(", "input_e", ",", "self", ".", "W_e", ")", ".", "unsqueeze", "(", "2", ")", "\n", "\n", "# add them together [batch_size, length_encoder, length_decoder, hidden_size]", "\n", "out", "=", "torch", ".", "tanh", "(", "out_d", "+", "out_e", "+", "self", ".", "b", ")", "\n", "\n", "# product with v", "\n", "# [batch_size, length_encoder, length_decoder, hidden_size] * [hidden, num_label]", "\n", "# [batch_size, length_encoder, length_decoder, num_labels]", "\n", "# then --> [batch_size, num_labels, length_decoder, length_encoder]", "\n", "return", "torch", ".", "matmul", "(", "out", ",", "self", ".", "v", ")", ".", "transpose", "(", "1", ",", "3", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.Jivnesh_LCM.modules.linear.BiLinear.__init__": [[13, 38], ["torch.Module.__init__", "torch.nn.parameter.Parameter", "torch.nn.parameter.Parameter", "torch.nn.parameter.Parameter", "torch.nn.parameter.Parameter", "torch.nn.parameter.Parameter", "torch.nn.parameter.Parameter", "torch.nn.parameter.Parameter", "torch.nn.parameter.Parameter", "torch.nn.parameter.Parameter", "linear.BiLinear.reset_parameters", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.nn.parameter.Parameter", "torch.nn.parameter.Parameter", "torch.nn.parameter.Parameter", "linear.BiLinear.register_parameter", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor"], "methods", ["home.repos.pwc.inspect_result.Jivnesh_LCM.io_.reader.Reader.__init__", "home.repos.pwc.inspect_result.Jivnesh_LCM.models.parsing_gating.BiRecurrentConv_Encoder.reset_parameters"], ["def", "__init__", "(", "self", ",", "left_features", ",", "right_features", ",", "out_features", ",", "bias", "=", "True", ")", ":", "\n", "        ", "'''\n\n        Args:\n            left_features: size of left input\n            right_features: size of right input\n            out_features: size of output\n            bias: If set to False, the layer will not learn an additive bias.\n                Default: True\n        '''", "\n", "super", "(", "BiLinear", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "left_features", "=", "left_features", "\n", "self", ".", "right_features", "=", "right_features", "\n", "self", ".", "out_features", "=", "out_features", "\n", "\n", "self", ".", "U", "=", "Parameter", "(", "torch", ".", "Tensor", "(", "self", ".", "out_features", ",", "self", ".", "left_features", ",", "self", ".", "right_features", ")", ")", "\n", "self", ".", "W_l", "=", "Parameter", "(", "torch", ".", "Tensor", "(", "self", ".", "out_features", ",", "self", ".", "left_features", ")", ")", "\n", "self", ".", "W_r", "=", "Parameter", "(", "torch", ".", "Tensor", "(", "self", ".", "out_features", ",", "self", ".", "left_features", ")", ")", "\n", "\n", "if", "bias", ":", "\n", "            ", "self", ".", "bias", "=", "Parameter", "(", "torch", ".", "Tensor", "(", "out_features", ")", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "register_parameter", "(", "'bias'", ",", "None", ")", "\n", "\n", "", "self", ".", "reset_parameters", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.modules.linear.BiLinear.reset_parameters": [[39, 44], ["torch.init.xavier_uniform_", "torch.init.xavier_uniform_", "torch.init.xavier_uniform_", "torch.init.xavier_uniform_", "torch.init.xavier_uniform_", "torch.init.xavier_uniform_", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_", "torch.init.xavier_uniform_", "torch.init.xavier_uniform_", "torch.init.xavier_uniform_"], "methods", ["None"], ["", "def", "reset_parameters", "(", "self", ")", ":", "\n", "        ", "nn", ".", "init", ".", "xavier_uniform_", "(", "self", ".", "W_l", ")", "\n", "nn", ".", "init", ".", "xavier_uniform_", "(", "self", ".", "W_r", ")", "\n", "nn", ".", "init", ".", "constant_", "(", "self", ".", "bias", ",", "0.", ")", "\n", "nn", ".", "init", ".", "xavier_uniform_", "(", "self", ".", "U", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.modules.linear.BiLinear.forward": [[45, 75], ["input_left.view.view.size", "input_right.view.view.size", "int", "input_left.view.view.view", "input_right.view.view.view", "torch.bilinear", "torch.bilinear", "torch.bilinear", "torch.bilinear.view", "numpy.prod", "torch.linear", "torch.linear", "torch.linear", "torch.linear", "torch.linear", "torch.linear"], "methods", ["home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.size", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.size"], ["", "def", "forward", "(", "self", ",", "input_left", ",", "input_right", ")", ":", "\n", "        ", "'''\n\n        Args:\n            input_left: Tensor\n                the left input tensor with shape = [batch1, batch2, ..., left_features]\n            input_right: Tensor\n                the right input tensor with shape = [batch1, batch2, ..., right_features]\n\n        Returns:\n\n        '''", "\n", "\n", "left_size", "=", "input_left", ".", "size", "(", ")", "\n", "right_size", "=", "input_right", ".", "size", "(", ")", "\n", "assert", "left_size", "[", ":", "-", "1", "]", "==", "right_size", "[", ":", "-", "1", "]", ",", "\"batch size of left and right inputs mis-match: (%s, %s)\"", "%", "(", "left_size", "[", ":", "-", "1", "]", ",", "right_size", "[", ":", "-", "1", "]", ")", "\n", "batch_size", "=", "int", "(", "np", ".", "prod", "(", "left_size", "[", ":", "-", "1", "]", ")", ")", "\n", "\n", "# convert left and right input to matrices [batch_size, left_features], [batch_size, right_features]", "\n", "input_left", "=", "input_left", ".", "view", "(", "batch_size", ",", "self", ".", "left_features", ")", "\n", "input_right", "=", "input_right", ".", "view", "(", "batch_size", ",", "self", ".", "right_features", ")", "\n", "\n", "# output [batch_size, out_features]", "\n", "output", "=", "F", ".", "bilinear", "(", "input_left", ",", "input_right", ",", "self", ".", "U", ",", "self", ".", "bias", ")", "\n", "output", "=", "output", "+", "F", ".", "linear", "(", "input_left", ",", "self", ".", "W_l", ",", "None", ")", "+", "F", ".", "linear", "(", "input_right", ",", "self", ".", "W_r", ",", "None", ")", "\n", "# convert back to [batch1, batch2, ..., out_features]", "\n", "\n", "# output = F.linear(input_left, self.W_l, None) + F.linear(input_right, self.W_r, None)", "\n", "return", "output", ".", "view", "(", "left_size", "[", ":", "-", "1", "]", "+", "(", "self", ".", "out_features", ",", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.modules.linear.BiLinear.__repr__": [[76, 81], ["str", "str", "str"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "__class__", ".", "__name__", "+", "' ('", "+", "'in1_features='", "+", "str", "(", "self", ".", "left_features", ")", "+", "', in2_features='", "+", "str", "(", "self", ".", "right_features", ")", "+", "', out_features='", "+", "str", "(", "self", ".", "out_features", ")", "+", "')'", "\n", "", "", ""]], "home.repos.pwc.inspect_result.Jivnesh_LCM.tasks.seqeval.get_entities": [[16, 52], ["any", "enumerate", "seqeval.end_of_chunk", "seqeval.start_of_chunk", "isinstance", "chunks.append", "chunk.split", "chunk.split"], "function", ["home.repos.pwc.inspect_result.Jivnesh_LCM.tasks.seqeval.end_of_chunk", "home.repos.pwc.inspect_result.Jivnesh_LCM.tasks.seqeval.start_of_chunk"], ["def", "get_entities", "(", "seq", ",", "suffix", "=", "False", ")", ":", "\n", "    ", "\"\"\"Gets entities from sequence.\n    Args:\n        seq (list): sequence of labels.\n    Returns:\n        list: list of (chunk_type, chunk_start, chunk_end).\n    Example:\n        >>> from seqeval.metrics.sequence_labeling import get_entities\n        >>> seq = ['B-PER', 'I-PER', 'O', 'B-LOC']\n        >>> get_entities(seq)\n        [('PER', 0, 1), ('LOC', 3, 3)]\n    \"\"\"", "\n", "# for nested list", "\n", "if", "any", "(", "isinstance", "(", "s", ",", "list", ")", "for", "s", "in", "seq", ")", ":", "\n", "        ", "seq", "=", "[", "item", "for", "sublist", "in", "seq", "for", "item", "in", "sublist", "+", "[", "'O'", "]", "]", "\n", "\n", "", "prev_tag", "=", "'O'", "\n", "prev_type", "=", "''", "\n", "begin_offset", "=", "0", "\n", "chunks", "=", "[", "]", "\n", "for", "i", ",", "chunk", "in", "enumerate", "(", "seq", "+", "[", "'O'", "]", ")", ":", "\n", "        ", "if", "suffix", ":", "\n", "            ", "tag", "=", "chunk", "[", "-", "1", "]", "\n", "type_", "=", "chunk", ".", "split", "(", "'-'", ")", "[", "0", "]", "\n", "", "else", ":", "\n", "            ", "tag", "=", "chunk", "[", "0", "]", "\n", "type_", "=", "chunk", ".", "split", "(", "'-'", ")", "[", "-", "1", "]", "\n", "\n", "", "if", "end_of_chunk", "(", "prev_tag", ",", "tag", ",", "prev_type", ",", "type_", ")", ":", "\n", "            ", "chunks", ".", "append", "(", "(", "prev_type", ",", "begin_offset", ",", "i", "-", "1", ")", ")", "\n", "", "if", "start_of_chunk", "(", "prev_tag", ",", "tag", ",", "prev_type", ",", "type_", ")", ":", "\n", "            ", "begin_offset", "=", "i", "\n", "", "prev_tag", "=", "tag", "\n", "prev_type", "=", "type_", "\n", "\n", "", "return", "chunks", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.tasks.seqeval.end_of_chunk": [[54, 80], ["None"], "function", ["None"], ["", "def", "end_of_chunk", "(", "prev_tag", ",", "tag", ",", "prev_type", ",", "type_", ")", ":", "\n", "    ", "\"\"\"Checks if a chunk ended between the previous and current word.\n    Args:\n        prev_tag: previous chunk tag.\n        tag: current chunk tag.\n        prev_type: previous type.\n        type_: current type.\n    Returns:\n        chunk_end: boolean.\n    \"\"\"", "\n", "chunk_end", "=", "False", "\n", "\n", "if", "prev_tag", "==", "'E'", ":", "chunk_end", "=", "True", "\n", "if", "prev_tag", "==", "'S'", ":", "chunk_end", "=", "True", "\n", "\n", "if", "prev_tag", "==", "'B'", "and", "tag", "==", "'B'", ":", "chunk_end", "=", "True", "\n", "if", "prev_tag", "==", "'B'", "and", "tag", "==", "'S'", ":", "chunk_end", "=", "True", "\n", "if", "prev_tag", "==", "'B'", "and", "tag", "==", "'O'", ":", "chunk_end", "=", "True", "\n", "if", "prev_tag", "==", "'I'", "and", "tag", "==", "'B'", ":", "chunk_end", "=", "True", "\n", "if", "prev_tag", "==", "'I'", "and", "tag", "==", "'S'", ":", "chunk_end", "=", "True", "\n", "if", "prev_tag", "==", "'I'", "and", "tag", "==", "'O'", ":", "chunk_end", "=", "True", "\n", "\n", "if", "prev_tag", "!=", "'O'", "and", "prev_tag", "!=", "'.'", "and", "prev_type", "!=", "type_", ":", "\n", "        ", "chunk_end", "=", "True", "\n", "\n", "", "return", "chunk_end", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.tasks.seqeval.start_of_chunk": [[82, 108], ["None"], "function", ["None"], ["", "def", "start_of_chunk", "(", "prev_tag", ",", "tag", ",", "prev_type", ",", "type_", ")", ":", "\n", "    ", "\"\"\"Checks if a chunk started between the previous and current word.\n    Args:\n        prev_tag: previous chunk tag.\n        tag: current chunk tag.\n        prev_type: previous type.\n        type_: current type.\n    Returns:\n        chunk_start: boolean.\n    \"\"\"", "\n", "chunk_start", "=", "False", "\n", "\n", "if", "tag", "==", "'B'", ":", "chunk_start", "=", "True", "\n", "if", "tag", "==", "'S'", ":", "chunk_start", "=", "True", "\n", "\n", "if", "prev_tag", "==", "'E'", "and", "tag", "==", "'E'", ":", "chunk_start", "=", "True", "\n", "if", "prev_tag", "==", "'E'", "and", "tag", "==", "'I'", ":", "chunk_start", "=", "True", "\n", "if", "prev_tag", "==", "'S'", "and", "tag", "==", "'E'", ":", "chunk_start", "=", "True", "\n", "if", "prev_tag", "==", "'S'", "and", "tag", "==", "'I'", ":", "chunk_start", "=", "True", "\n", "if", "prev_tag", "==", "'O'", "and", "tag", "==", "'E'", ":", "chunk_start", "=", "True", "\n", "if", "prev_tag", "==", "'O'", "and", "tag", "==", "'I'", ":", "chunk_start", "=", "True", "\n", "\n", "if", "tag", "!=", "'O'", "and", "tag", "!=", "'.'", "and", "prev_type", "!=", "type_", ":", "\n", "        ", "chunk_start", "=", "True", "\n", "\n", "", "return", "chunk_start", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.tasks.seqeval.f1_score": [[110, 143], ["set", "set", "len", "len", "len", "seqeval.get_entities", "seqeval.get_entities"], "function", ["home.repos.pwc.inspect_result.Jivnesh_LCM.tasks.seqeval.get_entities", "home.repos.pwc.inspect_result.Jivnesh_LCM.tasks.seqeval.get_entities"], ["", "def", "f1_score", "(", "y_true", ",", "y_pred", ",", "average", "=", "'micro'", ",", "suffix", "=", "False", ")", ":", "\n", "    ", "\"\"\"Compute the F1 score.\n    The F1 score can be interpreted as a weighted average of the precision and\n    recall, where an F1 score reaches its best value at 1 and worst score at 0.\n    The relative contribution of precision and recall to the F1 score are\n    equal. The formula for the F1 score is::\n        F1 = 2 * (precision * recall) / (precision + recall)\n    Args:\n        y_true : 2d array. Ground truth (correct) target values.\n        y_pred : 2d array. Estimated targets as returned by a tagger.\n    Returns:\n        score : float.\n    Example:\n        >>> from seqeval.metrics import f1_score\n        >>> y_true = [['O', 'O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]\n        >>> y_pred = [['O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]\n        >>> f1_score(y_true, y_pred)\n        0.50\n    # \"\"\"", "\n", "# from sklearn.metrics import f1_score", "\n", "# score = f1_score(y_true, y_pred, average='macro')", "\n", "true_entities", "=", "set", "(", "get_entities", "(", "y_true", ",", "suffix", ")", ")", "\n", "pred_entities", "=", "set", "(", "get_entities", "(", "y_pred", ",", "suffix", ")", ")", "\n", "\n", "nb_correct", "=", "len", "(", "true_entities", "&", "pred_entities", ")", "\n", "nb_pred", "=", "len", "(", "pred_entities", ")", "\n", "nb_true", "=", "len", "(", "true_entities", ")", "\n", "\n", "p", "=", "nb_correct", "/", "nb_pred", "if", "nb_pred", ">", "0", "else", "0", "\n", "r", "=", "nb_correct", "/", "nb_true", "if", "nb_true", ">", "0", "else", "0", "\n", "score", "=", "2", "*", "p", "*", "r", "/", "(", "p", "+", "r", ")", "if", "p", "+", "r", ">", "0", "else", "0", "\n", "\n", "return", "score", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.tasks.seqeval.accuracy_score": [[145, 172], ["any", "sum", "len", "isinstance", "zip"], "function", ["None"], ["", "def", "accuracy_score", "(", "y_true", ",", "y_pred", ")", ":", "\n", "    ", "\"\"\"Accuracy classification score.\n    In multilabel classification, this function computes subset accuracy:\n    the set of labels predicted for a sample must *exactly* match the\n    corresponding set of labels in y_true.\n    Args:\n        y_true : 2d array. Ground truth (correct) target values.\n        y_pred : 2d array. Estimated targets as returned by a tagger.\n    Returns:\n        score : float.\n    Example:\n        >>> from seqeval.metrics import accuracy_score\n        >>> y_true = [['O', 'O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]\n        >>> y_pred = [['O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]\n        >>> accuracy_score(y_true, y_pred)\n        0.80\n    \"\"\"", "\n", "if", "any", "(", "isinstance", "(", "s", ",", "list", ")", "for", "s", "in", "y_true", ")", ":", "\n", "        ", "y_true", "=", "[", "item", "for", "sublist", "in", "y_true", "for", "item", "in", "sublist", "]", "\n", "y_pred", "=", "[", "item", "for", "sublist", "in", "y_pred", "for", "item", "in", "sublist", "]", "\n", "\n", "", "nb_correct", "=", "sum", "(", "y_t", "==", "y_p", "for", "y_t", ",", "y_p", "in", "zip", "(", "y_true", ",", "y_pred", ")", ")", "\n", "nb_true", "=", "len", "(", "y_true", ")", "\n", "\n", "score", "=", "nb_correct", "/", "nb_true", "\n", "\n", "return", "score", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.tasks.seqeval.precision_score": [[174, 201], ["set", "set", "len", "len", "seqeval.get_entities", "seqeval.get_entities"], "function", ["home.repos.pwc.inspect_result.Jivnesh_LCM.tasks.seqeval.get_entities", "home.repos.pwc.inspect_result.Jivnesh_LCM.tasks.seqeval.get_entities"], ["", "def", "precision_score", "(", "y_true", ",", "y_pred", ",", "average", "=", "'micro'", ",", "suffix", "=", "False", ")", ":", "\n", "    ", "\"\"\"Compute the precision.\n    The precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of\n    true positives and ``fp`` the number of false positives. The precision is\n    intuitively the ability of the sequence_tagger not to label as positive a sample.\n    The best value is 1 and the worst value is 0.\n    Args:\n        y_true : 2d array. Ground truth (correct) target values.\n        y_pred : 2d array. Estimated targets as returned by a tagger.\n    Returns:\n        score : float.\n    Example:\n        >>> from seqeval.metrics import precision_score\n        >>> y_true = [['O', 'O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]\n        >>> y_pred = [['O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]\n        >>> precision_score(y_true, y_pred)\n        0.50\n    \"\"\"", "\n", "true_entities", "=", "set", "(", "get_entities", "(", "y_true", ",", "suffix", ")", ")", "\n", "pred_entities", "=", "set", "(", "get_entities", "(", "y_pred", ",", "suffix", ")", ")", "\n", "\n", "nb_correct", "=", "len", "(", "true_entities", "&", "pred_entities", ")", "\n", "nb_pred", "=", "len", "(", "pred_entities", ")", "\n", "\n", "score", "=", "nb_correct", "/", "nb_pred", "if", "nb_pred", ">", "0", "else", "0", "\n", "\n", "return", "score", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.tasks.seqeval.recall_score": [[203, 230], ["set", "set", "len", "len", "seqeval.get_entities", "seqeval.get_entities"], "function", ["home.repos.pwc.inspect_result.Jivnesh_LCM.tasks.seqeval.get_entities", "home.repos.pwc.inspect_result.Jivnesh_LCM.tasks.seqeval.get_entities"], ["", "def", "recall_score", "(", "y_true", ",", "y_pred", ",", "average", "=", "'micro'", ",", "suffix", "=", "False", ")", ":", "\n", "    ", "\"\"\"Compute the recall.\n    The recall is the ratio ``tp / (tp + fn)`` where ``tp`` is the number of\n    true positives and ``fn`` the number of false negatives. The recall is\n    intuitively the ability of the sequence_tagger to find all the positive samples.\n    The best value is 1 and the worst value is 0.\n    Args:\n        y_true : 2d array. Ground truth (correct) target values.\n        y_pred : 2d array. Estimated targets as returned by a tagger.\n    Returns:\n        score : float.\n    Example:\n        >>> from seqeval.metrics import recall_score\n        >>> y_true = [['O', 'O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]\n        >>> y_pred = [['O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]\n        >>> recall_score(y_true, y_pred)\n        0.50\n    \"\"\"", "\n", "true_entities", "=", "set", "(", "get_entities", "(", "y_true", ",", "suffix", ")", ")", "\n", "pred_entities", "=", "set", "(", "get_entities", "(", "y_pred", ",", "suffix", ")", ")", "\n", "\n", "nb_correct", "=", "len", "(", "true_entities", "&", "pred_entities", ")", "\n", "nb_true", "=", "len", "(", "true_entities", ")", "\n", "\n", "score", "=", "nb_correct", "/", "nb_true", "if", "nb_true", ">", "0", "else", "0", "\n", "\n", "return", "score", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.tasks.seqeval.performance_measure": [[232, 260], ["dict", "any", "sum", "sum", "sum", "sum", "isinstance", "zip", "zip", "zip", "zip"], "function", ["None"], ["", "def", "performance_measure", "(", "y_true", ",", "y_pred", ")", ":", "\n", "    ", "\"\"\"\n    Compute the performance metrics: TP, FP, FN, TN\n    Args:\n        y_true : 2d array. Ground truth (correct) target values.\n        y_pred : 2d array. Estimated targets as returned by a tagger.\n    Returns:\n        performance_dict : dict\n    Example:\n        >>> from seqeval.metrics import performance_measure\n        >>> y_true = [['O', 'O', 'O', 'B-MISC', 'I-MISC', 'O', 'B-ORG'], ['B-PER', 'I-PER', 'O']]\n        >>> y_pred = [['O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'O', 'O'], ['B-PER', 'I-PER', 'O']]\n        >>> performance_measure(y_true, y_pred)\n        (3, 3, 1, 4)\n    \"\"\"", "\n", "performace_dict", "=", "dict", "(", ")", "\n", "if", "any", "(", "isinstance", "(", "s", ",", "list", ")", "for", "s", "in", "y_true", ")", ":", "\n", "        ", "y_true", "=", "[", "item", "for", "sublist", "in", "y_true", "for", "item", "in", "sublist", "]", "\n", "y_pred", "=", "[", "item", "for", "sublist", "in", "y_pred", "for", "item", "in", "sublist", "]", "\n", "", "performace_dict", "[", "'TP'", "]", "=", "sum", "(", "y_t", "==", "y_p", "for", "y_t", ",", "y_p", "in", "zip", "(", "y_true", ",", "y_pred", ")", "\n", "if", "(", "(", "y_t", "!=", "'O'", ")", "or", "(", "y_p", "!=", "'O'", ")", ")", ")", "\n", "performace_dict", "[", "'FP'", "]", "=", "sum", "(", "y_t", "!=", "y_p", "for", "y_t", ",", "y_p", "in", "zip", "(", "y_true", ",", "y_pred", ")", ")", "\n", "performace_dict", "[", "'FN'", "]", "=", "sum", "(", "(", "(", "y_t", "!=", "'O'", ")", "and", "(", "y_p", "==", "'O'", ")", ")", "\n", "for", "y_t", ",", "y_p", "in", "zip", "(", "y_true", ",", "y_pred", ")", ")", "\n", "performace_dict", "[", "'TN'", "]", "=", "sum", "(", "(", "y_t", "==", "y_p", "==", "'O'", ")", "\n", "for", "y_t", ",", "y_p", "in", "zip", "(", "y_true", ",", "y_pred", ")", ")", "\n", "\n", "return", "performace_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.tasks.seqeval.classification_report": [[262, 334], ["set", "set", "collections.defaultdict", "collections.defaultdict", "max", "head_fmt.format", "collections.defaultdict.items", "row_fmt.format", "seqeval.get_entities", "seqeval.get_entities", "d1[].add", "max", "d2[].add", "len", "len", "len", "len", "row_fmt.format", "ps.append", "rs.append", "f1s.append", "s.append", "numpy.average", "numpy.average", "numpy.average", "numpy.sum", "len", "len"], "function", ["home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.items", "home.repos.pwc.inspect_result.Jivnesh_LCM.tasks.seqeval.get_entities", "home.repos.pwc.inspect_result.Jivnesh_LCM.tasks.seqeval.get_entities", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.add", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.add"], ["", "def", "classification_report", "(", "y_true", ",", "y_pred", ",", "digits", "=", "2", ",", "suffix", "=", "False", ")", ":", "\n", "    ", "\"\"\"Build a text report showing the main classification metrics.\n    Args:\n        y_true : 2d array. Ground truth (correct) target values.\n        y_pred : 2d array. Estimated targets as returned by a sequence_tagger.\n        digits : int. Number of digits for formatting output floating point values.\n    Returns:\n        report : string. Text summary of the precision, recall, F1 score for each class.\n    Examples:\n        >>> from seqeval.metrics import classification_report\n        >>> y_true = [['O', 'O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]\n        >>> y_pred = [['O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]\n        >>> print(classification_report(y_true, y_pred))\n                     precision    recall  f1-score   support\n        <BLANKLINE>\n               MISC       0.00      0.00      0.00         1\n                PER       1.00      1.00      1.00         1\n        <BLANKLINE>\n        avg / total       0.50      0.50      0.50         2\n        <BLANKLINE>\n    \"\"\"", "\n", "true_entities", "=", "set", "(", "get_entities", "(", "y_true", ",", "suffix", ")", ")", "\n", "pred_entities", "=", "set", "(", "get_entities", "(", "y_pred", ",", "suffix", ")", ")", "\n", "\n", "name_width", "=", "0", "\n", "d1", "=", "defaultdict", "(", "set", ")", "\n", "d2", "=", "defaultdict", "(", "set", ")", "\n", "for", "e", "in", "true_entities", ":", "\n", "        ", "d1", "[", "e", "[", "0", "]", "]", ".", "add", "(", "(", "e", "[", "1", "]", ",", "e", "[", "2", "]", ")", ")", "\n", "name_width", "=", "max", "(", "name_width", ",", "len", "(", "e", "[", "0", "]", ")", ")", "\n", "", "for", "e", "in", "pred_entities", ":", "\n", "        ", "d2", "[", "e", "[", "0", "]", "]", ".", "add", "(", "(", "e", "[", "1", "]", ",", "e", "[", "2", "]", ")", ")", "\n", "\n", "", "last_line_heading", "=", "'avg / total'", "\n", "width", "=", "max", "(", "name_width", ",", "len", "(", "last_line_heading", ")", ",", "digits", ")", "\n", "\n", "headers", "=", "[", "\"precision\"", ",", "\"recall\"", ",", "\"f1-score\"", ",", "\"support\"", "]", "\n", "head_fmt", "=", "u'{:>{width}s} '", "+", "u' {:>9}'", "*", "len", "(", "headers", ")", "\n", "report", "=", "head_fmt", ".", "format", "(", "u''", ",", "*", "headers", ",", "width", "=", "width", ")", "\n", "report", "+=", "u'\\n\\n'", "\n", "\n", "row_fmt", "=", "u'{:>{width}s} '", "+", "u' {:>9.{digits}f}'", "*", "3", "+", "u' {:>9}\\n'", "\n", "\n", "ps", ",", "rs", ",", "f1s", ",", "s", "=", "[", "]", ",", "[", "]", ",", "[", "]", ",", "[", "]", "\n", "for", "type_name", ",", "true_entities", "in", "d1", ".", "items", "(", ")", ":", "\n", "        ", "pred_entities", "=", "d2", "[", "type_name", "]", "\n", "nb_correct", "=", "len", "(", "true_entities", "&", "pred_entities", ")", "\n", "nb_pred", "=", "len", "(", "pred_entities", ")", "\n", "nb_true", "=", "len", "(", "true_entities", ")", "\n", "\n", "p", "=", "nb_correct", "/", "nb_pred", "if", "nb_pred", ">", "0", "else", "0", "\n", "r", "=", "nb_correct", "/", "nb_true", "if", "nb_true", ">", "0", "else", "0", "\n", "f1", "=", "2", "*", "p", "*", "r", "/", "(", "p", "+", "r", ")", "if", "p", "+", "r", ">", "0", "else", "0", "\n", "\n", "report", "+=", "row_fmt", ".", "format", "(", "*", "[", "type_name", ",", "p", ",", "r", ",", "f1", ",", "nb_true", "]", ",", "width", "=", "width", ",", "digits", "=", "digits", ")", "\n", "\n", "ps", ".", "append", "(", "p", ")", "\n", "rs", ".", "append", "(", "r", ")", "\n", "f1s", ".", "append", "(", "f1", ")", "\n", "s", ".", "append", "(", "nb_true", ")", "\n", "\n", "", "report", "+=", "u'\\n'", "\n", "\n", "# compute averages", "\n", "report", "+=", "row_fmt", ".", "format", "(", "last_line_heading", ",", "\n", "np", ".", "average", "(", "ps", ",", "weights", "=", "s", ")", ",", "\n", "np", ".", "average", "(", "rs", ",", "weights", "=", "s", ")", ",", "\n", "np", ".", "average", "(", "f1s", ",", "weights", "=", "s", ")", ",", "\n", "np", ".", "sum", "(", "s", ")", ",", "\n", "width", "=", "width", ",", "digits", "=", "digits", ")", "\n", "\n", "return", "report", "", "", ""]], "home.repos.pwc.inspect_result.Jivnesh_LCM.tasks.parse.is_uni_punctuation": [[4, 7], ["re.match"], "function", ["None"], ["def", "is_uni_punctuation", "(", "word", ")", ":", "\n", "    ", "match", "=", "re", ".", "match", "(", "\"^[^\\w\\s]+$]\"", ",", "word", ",", "flags", "=", "re", ".", "UNICODE", ")", "\n", "return", "match", "is", "not", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.tasks.parse.is_punctuation": [[9, 14], ["parse.is_uni_punctuation"], "function", ["home.repos.pwc.inspect_result.Jivnesh_LCM.tasks.parse.is_uni_punctuation"], ["", "def", "is_punctuation", "(", "word", ",", "pos", ",", "punct_set", "=", "None", ")", ":", "\n", "    ", "if", "punct_set", "is", "None", ":", "\n", "        ", "return", "is_uni_punctuation", "(", "word", ")", "\n", "", "else", ":", "\n", "        ", "return", "pos", "in", "punct_set", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.tasks.parse.eval_": [[16, 82], ["range", "range", "word_alphabet.get_instance", "word.encode.encode", "pos_alphabet.get_instance", "pos.encode.encode", "parse.is_punctuation"], "function", ["home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.get_instance", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.get_instance", "home.repos.pwc.inspect_result.Jivnesh_LCM.tasks.parse.is_punctuation"], ["", "", "def", "eval_", "(", "words", ",", "postags", ",", "heads_pred", ",", "arc_tag_pred", ",", "heads", ",", "arc_tag", ",", "word_alphabet", ",", "pos_alphabet", ",", "lengths", ",", "\n", "punct_set", "=", "None", ",", "symbolic_root", "=", "False", ",", "symbolic_end", "=", "False", ")", ":", "\n", "    ", "batch_size", ",", "_", "=", "words", ".", "shape", "\n", "ucorr", "=", "0.", "\n", "lcorr", "=", "0.", "\n", "total", "=", "0.", "\n", "ucomplete_match", "=", "0.", "\n", "lcomplete_match", "=", "0.", "\n", "\n", "ucorr_nopunc", "=", "0.", "\n", "lcorr_nopunc", "=", "0.", "\n", "total_nopunc", "=", "0.", "\n", "ucomplete_match_nopunc", "=", "0.", "\n", "lcomplete_match_nopunc", "=", "0.", "\n", "\n", "corr_root", "=", "0.", "\n", "total_root", "=", "0.", "\n", "start", "=", "1", "if", "symbolic_root", "else", "0", "\n", "end", "=", "1", "if", "symbolic_end", "else", "0", "\n", "for", "i", "in", "range", "(", "batch_size", ")", ":", "\n", "        ", "ucm", "=", "1.", "\n", "lcm", "=", "1.", "\n", "ucm_nopunc", "=", "1.", "\n", "lcm_nopunc", "=", "1.", "\n", "for", "j", "in", "range", "(", "start", ",", "lengths", "[", "i", "]", "-", "end", ")", ":", "\n", "            ", "word", "=", "word_alphabet", ".", "get_instance", "(", "words", "[", "i", ",", "j", "]", ")", "\n", "word", "=", "word", ".", "encode", "(", "'utf8'", ")", "\n", "\n", "pos", "=", "pos_alphabet", ".", "get_instance", "(", "postags", "[", "i", ",", "j", "]", ")", "\n", "pos", "=", "pos", ".", "encode", "(", "'utf8'", ")", "\n", "\n", "total", "+=", "1", "\n", "if", "heads", "[", "i", ",", "j", "]", "==", "heads_pred", "[", "i", ",", "j", "]", ":", "\n", "                ", "ucorr", "+=", "1", "\n", "if", "arc_tag", "[", "i", ",", "j", "]", "==", "arc_tag_pred", "[", "i", ",", "j", "]", ":", "\n", "                    ", "lcorr", "+=", "1", "\n", "", "else", ":", "\n", "                    ", "lcm", "=", "0", "\n", "", "", "else", ":", "\n", "                ", "ucm", "=", "0", "\n", "lcm", "=", "0", "\n", "\n", "", "if", "not", "is_punctuation", "(", "word", ",", "pos", ",", "punct_set", ")", ":", "\n", "                ", "total_nopunc", "+=", "1", "\n", "if", "heads", "[", "i", ",", "j", "]", "==", "heads_pred", "[", "i", ",", "j", "]", ":", "\n", "                    ", "ucorr_nopunc", "+=", "1", "\n", "if", "arc_tag", "[", "i", ",", "j", "]", "==", "arc_tag_pred", "[", "i", ",", "j", "]", ":", "\n", "                        ", "lcorr_nopunc", "+=", "1", "\n", "", "else", ":", "\n", "                        ", "lcm_nopunc", "=", "0", "\n", "", "", "else", ":", "\n", "                    ", "ucm_nopunc", "=", "0", "\n", "lcm_nopunc", "=", "0", "\n", "\n", "", "", "if", "heads", "[", "i", ",", "j", "]", "==", "0", ":", "\n", "                ", "total_root", "+=", "1", "\n", "corr_root", "+=", "1", "if", "heads_pred", "[", "i", ",", "j", "]", "==", "0", "else", "0", "\n", "\n", "", "", "ucomplete_match", "+=", "ucm", "\n", "lcomplete_match", "+=", "lcm", "\n", "ucomplete_match_nopunc", "+=", "ucm_nopunc", "\n", "lcomplete_match_nopunc", "+=", "lcm_nopunc", "\n", "\n", "", "return", "(", "ucorr", ",", "lcorr", ",", "total", ",", "ucomplete_match", ",", "lcomplete_match", ")", ",", "(", "ucorr_nopunc", ",", "lcorr_nopunc", ",", "total_nopunc", ",", "ucomplete_match_nopunc", ",", "lcomplete_match_nopunc", ")", ",", "(", "corr_root", ",", "total_root", ")", ",", "batch_size", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.tasks.parse.decode_MST": [[84, 307], ["numpy.zeros", "range", "numpy.zeros", "set", "range", "numpy.zeros", "range", "parse.decode_MST.find_cycle"], "function", ["None"], ["", "def", "decode_MST", "(", "energies", ",", "lengths", ",", "leading_symbolic", "=", "0", ",", "labeled", "=", "True", ")", ":", "\n", "    ", "\"\"\"\n    decode best parsing tree with MST algorithm.\n    :param energies: energies: numpy 4D tensor\n        energies of each edge. the shape is [batch_size, num_labels, n_steps, n_steps],\n        where the summy root is at index 0.\n    :param masks: numpy 2D tensor\n        masks in the shape [batch_size, n_steps].\n    :param leading_symbolic: int\n        number of symbolic dependency arcs leading in arc alphabets)\n    :return:\n    \"\"\"", "\n", "\n", "def", "find_cycle", "(", "par", ")", ":", "\n", "        ", "added", "=", "np", ".", "zeros", "(", "[", "length", "]", ",", "np", ".", "bool", ")", "\n", "added", "[", "0", "]", "=", "True", "\n", "cycle", "=", "set", "(", ")", "\n", "findcycle", "=", "False", "\n", "for", "i", "in", "range", "(", "1", ",", "length", ")", ":", "\n", "            ", "if", "findcycle", ":", "\n", "                ", "break", "\n", "\n", "", "if", "added", "[", "i", "]", "or", "not", "curr_nodes", "[", "i", "]", ":", "\n", "                ", "continue", "\n", "\n", "# init cycle", "\n", "", "tmp_cycle", "=", "set", "(", ")", "\n", "tmp_cycle", ".", "add", "(", "i", ")", "\n", "added", "[", "i", "]", "=", "True", "\n", "findcycle", "=", "True", "\n", "l", "=", "i", "\n", "\n", "while", "par", "[", "l", "]", "not", "in", "tmp_cycle", ":", "\n", "                ", "l", "=", "par", "[", "l", "]", "\n", "if", "added", "[", "l", "]", ":", "\n", "                    ", "findcycle", "=", "False", "\n", "break", "\n", "", "added", "[", "l", "]", "=", "True", "\n", "tmp_cycle", ".", "add", "(", "l", ")", "\n", "\n", "", "if", "findcycle", ":", "\n", "                ", "lorg", "=", "l", "\n", "cycle", ".", "add", "(", "lorg", ")", "\n", "l", "=", "par", "[", "lorg", "]", "\n", "while", "l", "!=", "lorg", ":", "\n", "                    ", "cycle", ".", "add", "(", "l", ")", "\n", "l", "=", "par", "[", "l", "]", "\n", "", "break", "\n", "\n", "", "", "return", "findcycle", ",", "cycle", "\n", "\n", "", "def", "chuLiuEdmonds", "(", ")", ":", "\n", "        ", "par", "=", "np", ".", "zeros", "(", "[", "length", "]", ",", "dtype", "=", "np", ".", "int32", ")", "\n", "# create best graph", "\n", "par", "[", "0", "]", "=", "-", "1", "\n", "for", "i", "in", "range", "(", "1", ",", "length", ")", ":", "\n", "# only interested at current nodes", "\n", "            ", "if", "curr_nodes", "[", "i", "]", ":", "\n", "                ", "max_score", "=", "score_matrix", "[", "0", ",", "i", "]", "\n", "par", "[", "i", "]", "=", "0", "\n", "for", "j", "in", "range", "(", "1", ",", "length", ")", ":", "\n", "                    ", "if", "j", "==", "i", "or", "not", "curr_nodes", "[", "j", "]", ":", "\n", "                        ", "continue", "\n", "\n", "", "new_score", "=", "score_matrix", "[", "j", ",", "i", "]", "\n", "if", "new_score", ">", "max_score", ":", "\n", "                        ", "max_score", "=", "new_score", "\n", "par", "[", "i", "]", "=", "j", "\n", "\n", "# find a cycle", "\n", "", "", "", "", "findcycle", ",", "cycle", "=", "find_cycle", "(", "par", ")", "\n", "# no cycles, get all edges and return them.", "\n", "if", "not", "findcycle", ":", "\n", "            ", "final_edges", "[", "0", "]", "=", "-", "1", "\n", "for", "i", "in", "range", "(", "1", ",", "length", ")", ":", "\n", "                ", "if", "not", "curr_nodes", "[", "i", "]", ":", "\n", "                    ", "continue", "\n", "\n", "", "pr", "=", "oldI", "[", "par", "[", "i", "]", ",", "i", "]", "\n", "ch", "=", "oldO", "[", "par", "[", "i", "]", ",", "i", "]", "\n", "final_edges", "[", "ch", "]", "=", "pr", "\n", "", "return", "\n", "\n", "", "cyc_len", "=", "len", "(", "cycle", ")", "\n", "cyc_weight", "=", "0.0", "\n", "cyc_nodes", "=", "np", ".", "zeros", "(", "[", "cyc_len", "]", ",", "dtype", "=", "np", ".", "int32", ")", "\n", "id", "=", "0", "\n", "for", "cyc_node", "in", "cycle", ":", "\n", "            ", "cyc_nodes", "[", "id", "]", "=", "cyc_node", "\n", "id", "+=", "1", "\n", "cyc_weight", "+=", "score_matrix", "[", "par", "[", "cyc_node", "]", ",", "cyc_node", "]", "\n", "\n", "", "rep", "=", "cyc_nodes", "[", "0", "]", "\n", "for", "i", "in", "range", "(", "length", ")", ":", "\n", "            ", "if", "not", "curr_nodes", "[", "i", "]", "or", "i", "in", "cycle", ":", "\n", "                ", "continue", "\n", "\n", "", "max1", "=", "float", "(", "\"-inf\"", ")", "\n", "wh1", "=", "-", "1", "\n", "max2", "=", "float", "(", "\"-inf\"", ")", "\n", "wh2", "=", "-", "1", "\n", "\n", "for", "j", "in", "range", "(", "cyc_len", ")", ":", "\n", "                ", "j1", "=", "cyc_nodes", "[", "j", "]", "\n", "if", "score_matrix", "[", "j1", ",", "i", "]", ">", "max1", ":", "\n", "                    ", "max1", "=", "score_matrix", "[", "j1", ",", "i", "]", "\n", "wh1", "=", "j1", "\n", "\n", "", "scr", "=", "cyc_weight", "+", "score_matrix", "[", "i", ",", "j1", "]", "-", "score_matrix", "[", "par", "[", "j1", "]", ",", "j1", "]", "\n", "\n", "if", "scr", ">", "max2", ":", "\n", "                    ", "max2", "=", "scr", "\n", "wh2", "=", "j1", "\n", "\n", "", "", "score_matrix", "[", "rep", ",", "i", "]", "=", "max1", "\n", "oldI", "[", "rep", ",", "i", "]", "=", "oldI", "[", "wh1", ",", "i", "]", "\n", "oldO", "[", "rep", ",", "i", "]", "=", "oldO", "[", "wh1", ",", "i", "]", "\n", "score_matrix", "[", "i", ",", "rep", "]", "=", "max2", "\n", "oldO", "[", "i", ",", "rep", "]", "=", "oldO", "[", "i", ",", "wh2", "]", "\n", "oldI", "[", "i", ",", "rep", "]", "=", "oldI", "[", "i", ",", "wh2", "]", "\n", "\n", "", "rep_cons", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "cyc_len", ")", ":", "\n", "            ", "rep_cons", ".", "append", "(", "set", "(", ")", ")", "\n", "cyc_node", "=", "cyc_nodes", "[", "i", "]", "\n", "for", "cc", "in", "reps", "[", "cyc_node", "]", ":", "\n", "                ", "rep_cons", "[", "i", "]", ".", "add", "(", "cc", ")", "\n", "\n", "", "", "for", "i", "in", "range", "(", "1", ",", "cyc_len", ")", ":", "\n", "            ", "cyc_node", "=", "cyc_nodes", "[", "i", "]", "\n", "curr_nodes", "[", "cyc_node", "]", "=", "False", "\n", "for", "cc", "in", "reps", "[", "cyc_node", "]", ":", "\n", "                ", "reps", "[", "rep", "]", ".", "add", "(", "cc", ")", "\n", "\n", "", "", "chuLiuEdmonds", "(", ")", "\n", "\n", "# check each node in cycle, if one of its representatives is a key in the final_edges, it is the one.", "\n", "found", "=", "False", "\n", "wh", "=", "-", "1", "\n", "for", "i", "in", "range", "(", "cyc_len", ")", ":", "\n", "            ", "for", "repc", "in", "rep_cons", "[", "i", "]", ":", "\n", "                ", "if", "repc", "in", "final_edges", ":", "\n", "                    ", "wh", "=", "cyc_nodes", "[", "i", "]", "\n", "found", "=", "True", "\n", "break", "\n", "", "", "if", "found", ":", "\n", "                ", "break", "\n", "\n", "", "", "l", "=", "par", "[", "wh", "]", "\n", "while", "l", "!=", "wh", ":", "\n", "            ", "ch", "=", "oldO", "[", "par", "[", "l", "]", ",", "l", "]", "\n", "pr", "=", "oldI", "[", "par", "[", "l", "]", ",", "l", "]", "\n", "final_edges", "[", "ch", "]", "=", "pr", "\n", "l", "=", "par", "[", "l", "]", "\n", "\n", "", "", "if", "labeled", ":", "\n", "        ", "assert", "energies", ".", "ndim", "==", "4", ",", "'dimension of energies is not equal to 4'", "\n", "", "else", ":", "\n", "        ", "assert", "energies", ".", "ndim", "==", "3", ",", "'dimension of energies is not equal to 3'", "\n", "", "input_shape", "=", "energies", ".", "shape", "\n", "batch_size", "=", "input_shape", "[", "0", "]", "\n", "max_length", "=", "input_shape", "[", "2", "]", "\n", "\n", "pars", "=", "np", ".", "zeros", "(", "[", "batch_size", ",", "max_length", "]", ",", "dtype", "=", "np", ".", "int32", ")", "\n", "arc_tags", "=", "np", ".", "zeros", "(", "[", "batch_size", ",", "max_length", "]", ",", "dtype", "=", "np", ".", "int32", ")", "if", "labeled", "else", "None", "\n", "for", "i", "in", "range", "(", "batch_size", ")", ":", "\n", "        ", "energy", "=", "energies", "[", "i", "]", "\n", "\n", "# calc the real length of this instance", "\n", "length", "=", "lengths", "[", "i", "]", "\n", "\n", "# calc real energy matrix shape = [length, length, num_labels - #symbolic] (remove the label for symbolic arcs).", "\n", "if", "labeled", ":", "\n", "            ", "energy", "=", "energy", "[", "leading_symbolic", ":", ",", ":", "length", ",", ":", "length", "]", "\n", "# get best label for each edge.", "\n", "label_id_matrix", "=", "energy", ".", "argmax", "(", "axis", "=", "0", ")", "+", "leading_symbolic", "\n", "energy", "=", "energy", ".", "max", "(", "axis", "=", "0", ")", "\n", "", "else", ":", "\n", "            ", "energy", "=", "energy", "[", ":", "length", ",", ":", "length", "]", "\n", "label_id_matrix", "=", "None", "\n", "# get original score matrix", "\n", "", "orig_score_matrix", "=", "energy", "\n", "# initialize score matrix to original score matrix", "\n", "score_matrix", "=", "np", ".", "array", "(", "orig_score_matrix", ",", "copy", "=", "True", ")", "\n", "\n", "oldI", "=", "np", ".", "zeros", "(", "[", "length", ",", "length", "]", ",", "dtype", "=", "np", ".", "int32", ")", "\n", "oldO", "=", "np", ".", "zeros", "(", "[", "length", ",", "length", "]", ",", "dtype", "=", "np", ".", "int32", ")", "\n", "curr_nodes", "=", "np", ".", "zeros", "(", "[", "length", "]", ",", "dtype", "=", "np", ".", "bool", ")", "\n", "reps", "=", "[", "]", "\n", "\n", "for", "s", "in", "range", "(", "length", ")", ":", "\n", "            ", "orig_score_matrix", "[", "s", ",", "s", "]", "=", "0.0", "\n", "score_matrix", "[", "s", ",", "s", "]", "=", "0.0", "\n", "curr_nodes", "[", "s", "]", "=", "True", "\n", "reps", ".", "append", "(", "set", "(", ")", ")", "\n", "reps", "[", "s", "]", ".", "add", "(", "s", ")", "\n", "for", "t", "in", "range", "(", "s", "+", "1", ",", "length", ")", ":", "\n", "                ", "oldI", "[", "s", ",", "t", "]", "=", "s", "\n", "oldO", "[", "s", ",", "t", "]", "=", "t", "\n", "\n", "oldI", "[", "t", ",", "s", "]", "=", "t", "\n", "oldO", "[", "t", ",", "s", "]", "=", "s", "\n", "\n", "", "", "final_edges", "=", "dict", "(", ")", "\n", "chuLiuEdmonds", "(", ")", "\n", "par", "=", "np", ".", "zeros", "(", "[", "max_length", "]", ",", "np", ".", "int32", ")", "\n", "if", "labeled", ":", "\n", "            ", "arc_tag", "=", "np", ".", "ones", "(", "[", "max_length", "]", ",", "np", ".", "int32", ")", "\n", "arc_tag", "[", "0", "]", "=", "0", "\n", "", "else", ":", "\n", "            ", "arc_tag", "=", "None", "\n", "\n", "", "for", "ch", ",", "pr", "in", "final_edges", ".", "items", "(", ")", ":", "\n", "            ", "par", "[", "ch", "]", "=", "pr", "\n", "if", "labeled", "and", "ch", "!=", "0", ":", "\n", "                ", "arc_tag", "[", "ch", "]", "=", "label_id_matrix", "[", "pr", ",", "ch", "]", "\n", "\n", "", "", "par", "[", "0", "]", "=", "0", "\n", "pars", "[", "i", "]", "=", "par", "\n", "if", "labeled", ":", "\n", "            ", "arc_tags", "[", "i", "]", "=", "arc_tag", "\n", "\n", "", "", "return", "pars", ",", "arc_tags", "\n", "", ""]], "home.repos.pwc.inspect_result.Jivnesh_LCM.models.parsing.BiAffine_Parser.__init__": [[12, 25], ["torch.Module.__init__", "parsing.BiRecurrentConv_Encoder", "parsing.BiAffine_Parser_Decoder"], "methods", ["home.repos.pwc.inspect_result.Jivnesh_LCM.io_.reader.Reader.__init__"], ["    ", "def", "__init__", "(", "self", ",", "word_dim", ",", "num_words", ",", "char_dim", ",", "num_chars", ",", "use_pos", ",", "use_char", ",", "pos_dim", ",", "num_pos", ",", "num_filters", ",", "\n", "kernel_size", ",", "rnn_mode", ",", "hidden_size", ",", "num_layers", ",", "num_arcs", ",", "\n", "arc_space", ",", "arc_tag_space", ",", "embedd_word", "=", "None", ",", "embedd_char", "=", "None", ",", "embedd_pos", "=", "None", ",", "\n", "p_in", "=", "0.33", ",", "p_out", "=", "0.33", ",", "p_rnn", "=", "(", "0.33", ",", "0.33", ")", ",", "biaffine", "=", "True", ",", "arc_decode", "=", "'mst'", ",", "\n", "initializer", "=", "None", ")", ":", "\n", "        ", "super", "(", "BiAffine_Parser", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "rnn_encoder", "=", "BiRecurrentConv_Encoder", "(", "word_dim", ",", "num_words", ",", "char_dim", ",", "num_chars", ",", "use_pos", ",", "use_char", ",", "\n", "pos_dim", ",", "num_pos", ",", "num_filters", ",", "\n", "kernel_size", ",", "rnn_mode", ",", "hidden_size", ",", "\n", "num_layers", ",", "embedd_word", "=", "embedd_word", ",", "\n", "embedd_char", "=", "embedd_char", ",", "embedd_pos", "=", "embedd_pos", ",", "\n", "p_in", "=", "p_in", ",", "p_out", "=", "p_out", ",", "p_rnn", "=", "p_rnn", ",", "initializer", "=", "initializer", ")", "\n", "self", ".", "parser", "=", "BiAffine_Parser_Decoder", "(", "hidden_size", ",", "num_arcs", ",", "arc_space", ",", "arc_tag_space", ",", "biaffine", ",", "p_out", ",", "arc_decode", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.models.parsing.BiAffine_Parser.forward": [[26, 30], ["parsing.BiAffine_Parser.rnn_encoder", "parsing.BiAffine_Parser.parser"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_word", ",", "input_char", ",", "input_pos", ",", "mask", "=", "None", ",", "length", "=", "None", ",", "hx", "=", "None", ")", ":", "\n", "        ", "encoder_output", ",", "hn", ",", "mask", ",", "length", "=", "self", ".", "rnn_encoder", "(", "input_word", ",", "input_char", ",", "input_pos", ",", "mask", ",", "length", ",", "hx", ")", "\n", "out_arc", ",", "out_arc_tag", "=", "self", ".", "parser", "(", "encoder_output", ",", "mask", ")", "\n", "return", "out_arc", ",", "out_arc_tag", ",", "mask", ",", "length", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.models.parsing.BiAffine_Parser.loss": [[31, 36], ["parsing.BiAffine_Parser.parser.loss"], "methods", ["home.repos.pwc.inspect_result.Jivnesh_LCM.models.parsing_gating.BiAffine_Parser_Decoder.loss"], ["", "def", "loss", "(", "self", ",", "out_arc", ",", "out_arc_tag", ",", "heads", ",", "arc_tags", ",", "mask", "=", "None", ",", "length", "=", "None", ")", ":", "\n", "# out_arc shape [batch_size, length, length]", "\n", "# out_arc_tag shape [batch_size, length, arc_tag_space]", "\n", "        ", "loss_arc", ",", "loss_arc_tag", "=", "self", ".", "parser", ".", "loss", "(", "out_arc", ",", "out_arc_tag", ",", "heads", ",", "arc_tags", ",", "mask", ",", "length", ")", "\n", "return", "loss_arc", ",", "loss_arc_tag", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.models.parsing.BiAffine_Parser.loss_per_sample": [[37, 42], ["parsing.BiAffine_Parser.parser.loss_per_sample"], "methods", ["home.repos.pwc.inspect_result.Jivnesh_LCM.models.parsing.BiAffine_Parser.loss_per_sample"], ["", "def", "loss_per_sample", "(", "self", ",", "out_arc", ",", "out_arc_tag", ",", "heads", ",", "arc_tags", ",", "mask", "=", "None", ",", "length", "=", "None", ")", ":", "\n", "# out_arc shape [batch_size, length, length]", "\n", "# out_arc_tag shape [batch_size, length, arc_tag_space]", "\n", "        ", "loss_arc", ",", "loss_arc_tag", "=", "self", ".", "parser", ".", "loss_per_sample", "(", "out_arc", ",", "out_arc_tag", ",", "heads", ",", "arc_tags", ",", "mask", ",", "length", ")", "\n", "return", "loss_arc", ",", "loss_arc_tag", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.models.parsing.BiAffine_Parser.decode": [[43, 46], ["parsing.BiAffine_Parser.parser.decode"], "methods", ["home.repos.pwc.inspect_result.Jivnesh_LCM.models.parsing_gating.BiAffine_Parser_Decoder.decode"], ["", "def", "decode", "(", "self", ",", "out_arc", ",", "out_arc_tag", ",", "mask", "=", "None", ",", "length", "=", "None", ",", "leading_symbolic", "=", "0", ")", ":", "\n", "        ", "heads_pred", ",", "arc_tags_pred", ",", "scores", "=", "self", ".", "parser", ".", "decode", "(", "out_arc", ",", "out_arc_tag", ",", "mask", ",", "length", ",", "leading_symbolic", ")", "\n", "return", "heads_pred", ",", "arc_tags_pred", ",", "scores", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.models.parsing.BiAffine_Parser.pre_loss": [[47, 50], ["parsing.BiAffine_Parser.parser.pre_loss"], "methods", ["home.repos.pwc.inspect_result.Jivnesh_LCM.models.parsing_gating.BiAffine_Parser_Decoder.pre_loss"], ["", "def", "pre_loss", "(", "self", ",", "out_arc", ",", "out_arc_tag", ",", "heads", ",", "arc_tags", ",", "mask", "=", "None", ",", "length", "=", "None", ",", "use_log", "=", "True", ",", "temperature", "=", "1.0", ")", ":", "\n", "        ", "out_arc", ",", "out_arc_tag", "=", "self", ".", "parser", ".", "pre_loss", "(", "out_arc", ",", "out_arc_tag", ",", "heads", ",", "arc_tags", ",", "mask", ",", "length", ",", "use_log", ",", "temperature", ")", "\n", "return", "out_arc", ",", "out_arc_tag", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.models.parsing.BiAffine_Parser_Decoder.__init__": [[52, 68], ["torch.Module.__init__", "torch.Dropout", "torch.Dropout", "torch.Dropout", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.BiAAttention", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.BiLinear"], "methods", ["home.repos.pwc.inspect_result.Jivnesh_LCM.io_.reader.Reader.__init__"], ["    ", "def", "__init__", "(", "self", ",", "hidden_size", ",", "num_arcs", ",", "arc_space", ",", "arc_tag_space", ",", "biaffine", ",", "p_out", ",", "arc_decode", ")", ":", "\n", "        ", "super", "(", "BiAffine_Parser_Decoder", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "num_arcs", "=", "num_arcs", "\n", "self", ".", "arc_space", "=", "arc_space", "\n", "self", ".", "arc_tag_space", "=", "arc_tag_space", "\n", "self", ".", "out_dim", "=", "hidden_size", "*", "2", "\n", "self", ".", "biaffine", "=", "biaffine", "\n", "self", ".", "p_out", "=", "p_out", "\n", "self", ".", "arc_decode", "=", "arc_decode", "\n", "self", ".", "dropout_out", "=", "nn", ".", "Dropout", "(", "self", ".", "p_out", ")", "\n", "self", ".", "arc_h", "=", "nn", ".", "Linear", "(", "self", ".", "out_dim", ",", "self", ".", "arc_space", ")", "\n", "self", ".", "arc_c", "=", "nn", ".", "Linear", "(", "self", ".", "out_dim", ",", "self", ".", "arc_space", ")", "\n", "self", ".", "attention", "=", "BiAAttention", "(", "self", ".", "arc_space", ",", "self", ".", "arc_space", ",", "1", ",", "biaffine", "=", "biaffine", ")", "\n", "self", ".", "arc_tag_h", "=", "nn", ".", "Linear", "(", "self", ".", "out_dim", ",", "arc_tag_space", ")", "\n", "self", ".", "arc_tag_c", "=", "nn", ".", "Linear", "(", "self", ".", "out_dim", ",", "arc_tag_space", ")", "\n", "self", ".", "bilinear", "=", "BiLinear", "(", "arc_tag_space", ",", "arc_tag_space", ",", "num_arcs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.models.parsing.BiAffine_Parser_Decoder.forward": [[69, 102], ["parsing.BiAffine_Parser_Decoder.dropout_out().transpose", "torch.elu", "torch.elu", "torch.elu", "torch.elu", "torch.elu", "torch.elu", "torch.elu", "torch.elu", "torch.elu", "torch.elu", "torch.elu", "torch.elu", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "parsing.BiAffine_Parser_Decoder.dropout_out().transpose", "parsing.BiAffine_Parser_Decoder.chunk", "parsing.BiAffine_Parser_Decoder.dropout_out().transpose", "parsing.BiAffine_Parser_Decoder.chunk", "arc_tag_h.contiguous.contiguous.contiguous", "arc_tag_c.contiguous.contiguous.contiguous", "parsing.BiAffine_Parser_Decoder.attention().squeeze", "parsing.BiAffine_Parser_Decoder.arc_h", "parsing.BiAffine_Parser_Decoder.arc_c", "parsing.BiAffine_Parser_Decoder.arc_tag_h", "parsing.BiAffine_Parser_Decoder.arc_tag_c", "parsing.BiAffine_Parser_Decoder.dropout_out", "parsing.BiAffine_Parser_Decoder.dropout_out", "parsing.BiAffine_Parser_Decoder.dropout_out", "parsing.BiAffine_Parser_Decoder.attention", "parsing.BiAffine_Parser_Decoder.transpose", "parsing.BiAffine_Parser_Decoder.transpose", "parsing.BiAffine_Parser_Decoder.transpose"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input", ",", "mask", ")", ":", "\n", "# apply dropout for output", "\n", "# [batch_size, length, hidden_size] --> [batch_size, hidden_size, length] --> [batch_size, length, hidden_size]", "\n", "        ", "input", "=", "self", ".", "dropout_out", "(", "input", ".", "transpose", "(", "1", ",", "2", ")", ")", ".", "transpose", "(", "1", ",", "2", ")", "\n", "\n", "# output size [batch_size, length, arc_space]", "\n", "arc_h", "=", "F", ".", "elu", "(", "self", ".", "arc_h", "(", "input", ")", ")", "\n", "arc_c", "=", "F", ".", "elu", "(", "self", ".", "arc_c", "(", "input", ")", ")", "\n", "\n", "# output size [batch_size, length, arc_tag_space]", "\n", "arc_tag_h", "=", "F", ".", "elu", "(", "self", ".", "arc_tag_h", "(", "input", ")", ")", "\n", "arc_tag_c", "=", "F", ".", "elu", "(", "self", ".", "arc_tag_c", "(", "input", ")", ")", "\n", "\n", "# apply dropout", "\n", "# [batch_size, length, dim] --> [batch_size, 2 * length, dim]", "\n", "arc", "=", "torch", ".", "cat", "(", "[", "arc_h", ",", "arc_c", "]", ",", "dim", "=", "1", ")", "\n", "arc_tag", "=", "torch", ".", "cat", "(", "[", "arc_tag_h", ",", "arc_tag_c", "]", ",", "dim", "=", "1", ")", "\n", "\n", "arc", "=", "self", ".", "dropout_out", "(", "arc", ".", "transpose", "(", "1", ",", "2", ")", ")", ".", "transpose", "(", "1", ",", "2", ")", "\n", "arc_h", ",", "arc_c", "=", "arc", ".", "chunk", "(", "2", ",", "1", ")", "\n", "arc_tag", "=", "self", ".", "dropout_out", "(", "arc_tag", ".", "transpose", "(", "1", ",", "2", ")", ")", ".", "transpose", "(", "1", ",", "2", ")", "\n", "\n", "# output from rnn [batch_size, length, tag_space]", "\n", "arc_tag_h", ",", "arc_tag_c", "=", "arc_tag", ".", "chunk", "(", "2", ",", "1", ")", "\n", "# head shape [batch_size, length, arc_tag_space]", "\n", "arc_tag_h", "=", "arc_tag_h", ".", "contiguous", "(", ")", "\n", "# child shape [batch_size, length, arc_tag_space]", "\n", "arc_tag_c", "=", "arc_tag_c", ".", "contiguous", "(", ")", "\n", "arc", "=", "(", "arc_h", ",", "arc_c", ")", "\n", "# [batch_size, length, length]", "\n", "out_arc", "=", "self", ".", "attention", "(", "arc", "[", "0", "]", ",", "arc", "[", "1", "]", ",", "mask_d", "=", "mask", ",", "mask_e", "=", "mask", ")", ".", "squeeze", "(", "dim", "=", "1", ")", "\n", "out_arc_tag", "=", "(", "arc_tag_h", ",", "arc_tag_c", ")", "\n", "return", "out_arc", ",", "out_arc_tag", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.models.parsing.BiAffine_Parser_Decoder.loss": [[103, 114], ["parsing.BiAffine_Parser_Decoder.pre_loss", "out_arc.t.t.size", "out_arc.t.t.t", "out_arc_tag.t.t.t", "mask.sum", "float", "out_arc.t.t.sum", "out_arc_tag.t.t.sum"], "methods", ["home.repos.pwc.inspect_result.Jivnesh_LCM.models.parsing_gating.BiAffine_Parser_Decoder.pre_loss", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.size"], ["", "def", "loss", "(", "self", ",", "out_arc", ",", "out_arc_tag", ",", "heads", ",", "arc_tags", ",", "mask", "=", "None", ",", "length", "=", "None", ")", ":", "\n", "        ", "out_arc", ",", "out_arc_tag", "=", "self", ".", "pre_loss", "(", "out_arc", ",", "out_arc_tag", ",", "heads", "=", "heads", ",", "arc_tags", "=", "arc_tags", ",", "mask", "=", "mask", ",", "length", "=", "length", ",", "use_log", "=", "True", ",", "temperature", "=", "1.0", ")", "\n", "batch_size", ",", "max_len", "=", "out_arc", ".", "size", "(", ")", "\n", "# loss_arc shape [length-1, batch_size]", "\n", "out_arc", "=", "out_arc", ".", "t", "(", ")", "\n", "# loss_arc_tag shape [length-1, batch_size]", "\n", "out_arc_tag", "=", "out_arc_tag", ".", "t", "(", ")", "\n", "# number of valid positions which contribute to loss (remove the symbolic head for each sentence).", "\n", "num", "=", "mask", ".", "sum", "(", ")", "-", "batch_size", "if", "mask", "is", "not", "None", "else", "float", "(", "max_len", ")", "*", "batch_size", "\n", "dp_loss", "=", "-", "out_arc", ".", "sum", "(", ")", "/", "num", ",", "-", "out_arc_tag", ".", "sum", "(", ")", "/", "num", "\n", "return", "dp_loss", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.models.parsing.BiAffine_Parser_Decoder.decode": [[115, 121], ["parsing.BiAffine_Parser_Decoder.decode_mst", "parsing.BiAffine_Parser_Decoder.decode_greedy"], "methods", ["home.repos.pwc.inspect_result.Jivnesh_LCM.models.parsing_gating.BiAffine_Parser_Decoder.decode_mst", "home.repos.pwc.inspect_result.Jivnesh_LCM.models.parsing_gating.BiAffine_Parser_Decoder.decode_greedy"], ["", "def", "decode", "(", "self", ",", "out_arc", ",", "out_arc_tag", ",", "mask", ",", "length", ",", "leading_symbolic", ")", ":", "\n", "        ", "if", "self", ".", "arc_decode", "==", "'mst'", ":", "\n", "            ", "heads", ",", "arc_tags", ",", "scores", "=", "self", ".", "decode_mst", "(", "out_arc", ",", "out_arc_tag", ",", "mask", ",", "length", ",", "leading_symbolic", ")", "\n", "", "else", ":", "#self.arc_decode == 'greedy'", "\n", "            ", "heads", ",", "arc_tags", ",", "scores", "=", "self", ".", "decode_greedy", "(", "out_arc", ",", "out_arc_tag", ",", "mask", ",", "leading_symbolic", ")", "\n", "", "return", "heads", ",", "arc_tags", ",", "scores", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.models.parsing.BiAffine_Parser_Decoder.decode_mst": [[122, 150], ["parsing.BiAffine_Parser_Decoder.pre_loss", "loss_arc.size", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "nn.utils.tasks.parse.decode_MST", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy.size", "torch.from_numpy.size", "torch.from_numpy.size", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "range", "scores.detach.detach.detach", "torch.exp.data.cpu().numpy", "torch.exp.data.cpu().numpy", "torch.exp.data.cpu().numpy", "range", "mask.data.sum().long().cpu().numpy", "loss_arc.unsqueeze", "scores.detach.detach.sum", "mask.sum", "scores.detach.detach.sum", "torch.exp.data.cpu", "torch.exp.data.cpu", "torch.exp.data.cpu", "range", "mask.data.sum().long().cpu", "mask.data.sum().long", "mask.data.sum"], "methods", ["home.repos.pwc.inspect_result.Jivnesh_LCM.models.parsing_gating.BiAffine_Parser_Decoder.pre_loss", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.size", "home.repos.pwc.inspect_result.Jivnesh_LCM.tasks.parse.decode_MST", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.size", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.size", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.size"], ["", "def", "decode_mst", "(", "self", ",", "out_arc", ",", "out_arc_tag", ",", "mask", ",", "length", ",", "leading_symbolic", ")", ":", "\n", "        ", "loss_arc", ",", "loss_arc_tag", "=", "self", ".", "pre_loss", "(", "out_arc", ",", "out_arc_tag", ",", "heads", "=", "None", ",", "arc_tags", "=", "None", ",", "mask", "=", "mask", ",", "length", "=", "length", ",", "use_log", "=", "True", ",", "temperature", "=", "1.0", ")", "\n", "batch_size", ",", "max_len", ",", "_", "=", "loss_arc", ".", "size", "(", ")", "\n", "# compute lengths", "\n", "if", "length", "is", "None", ":", "\n", "            ", "if", "mask", "is", "None", ":", "\n", "                ", "length", "=", "[", "max_len", "for", "_", "in", "range", "(", "batch_size", ")", "]", "\n", "", "else", ":", "\n", "                ", "length", "=", "mask", ".", "data", ".", "sum", "(", "dim", "=", "1", ")", ".", "long", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "# energy shape [batch_size, num_arcs, length, length]", "\n", "", "", "energy", "=", "torch", ".", "exp", "(", "loss_arc", ".", "unsqueeze", "(", "1", ")", "+", "loss_arc_tag", ")", "\n", "heads", ",", "arc_tags", "=", "parse", ".", "decode_MST", "(", "energy", ".", "data", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ",", "length", ",", "leading_symbolic", "=", "leading_symbolic", ",", "\n", "labeled", "=", "True", ")", "\n", "heads", "=", "from_numpy", "(", "heads", ")", "\n", "arc_tags", "=", "from_numpy", "(", "arc_tags", ")", "\n", "\n", "# compute the average score for each tree", "\n", "batch_size", ",", "max_len", "=", "heads", ".", "size", "(", ")", "\n", "scores", "=", "torch", ".", "zeros_like", "(", "heads", ",", "dtype", "=", "energy", ".", "dtype", ",", "device", "=", "energy", ".", "device", ")", "\n", "for", "b_idx", "in", "range", "(", "batch_size", ")", ":", "\n", "            ", "for", "len_idx", "in", "range", "(", "max_len", ")", ":", "\n", "                ", "scores", "[", "b_idx", ",", "len_idx", "]", "=", "energy", "[", "b_idx", ",", "arc_tags", "[", "b_idx", ",", "len_idx", "]", ",", "heads", "[", "b_idx", ",", "len_idx", "]", ",", "len_idx", "]", "\n", "", "", "if", "mask", "is", "not", "None", ":", "\n", "            ", "scores", "=", "scores", ".", "sum", "(", "1", ")", "/", "mask", ".", "sum", "(", "1", ")", "\n", "", "else", ":", "\n", "            ", "scores", "=", "scores", ".", "sum", "(", "1", ")", "/", "max_len", "\n", "", "scores", "=", "scores", ".", "detach", "(", ")", "\n", "return", "heads", ",", "arc_tags", ",", "scores", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.models.parsing.BiAffine_Parser_Decoder.decode_greedy": [[151, 206], ["out_arc.size", "out_arc.max", "parsing.BiAffine_Parser_Decoder.decode_greedy._decode_arc_tags"], "methods", ["home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.size"], ["", "def", "decode_greedy", "(", "self", ",", "out_arc", ",", "out_arc_tag", ",", "mask", ",", "leading_symbolic", ")", ":", "\n", "        ", "'''\n        Args:\n            out_arc: Tensor\n                the arc scores with shape [batch_size, length, length]\n            out_arc_tag: Tensor\n                the labeled arc scores with shape [batch_size, length, arc_tag_space]\n            mask: Tensor or None\n                the mask tensor with shape = [batch_size, length]\n            length: Tensor or None\n                the length tensor with shape = [batch_size]\n            leading_symbolic: int\n                number of symbolic labels leading in arc_tag alphabets (set it to 0 if you are not sure)\n\n        Returns: (Tensor, Tensor)\n                predicted heads and arc_tags.\n        '''", "\n", "def", "_decode_arc_tags", "(", "out_arc_tag", ",", "heads", ",", "leading_symbolic", ")", ":", "\n", "# out_arc_tag shape [batch_size, length, arc_tag_space]", "\n", "            ", "arc_tag_h", ",", "arc_tag_c", "=", "out_arc_tag", "\n", "batch_size", ",", "max_len", ",", "_", "=", "arc_tag_h", ".", "size", "(", ")", "\n", "# create batch index [batch_size]", "\n", "batch_index", "=", "torch", ".", "arange", "(", "0", ",", "batch_size", ")", ".", "type_as", "(", "arc_tag_h", ".", "data", ")", ".", "long", "(", ")", "\n", "# get vector for heads [batch_size, length, arc_tag_space],", "\n", "arc_tag_h", "=", "arc_tag_h", "[", "batch_index", ",", "heads", ".", "t", "(", ")", "]", ".", "transpose", "(", "0", ",", "1", ")", ".", "contiguous", "(", ")", "\n", "# compute output for arc_tag [batch_size, length, num_arcs]", "\n", "out_arc_tag", "=", "self", ".", "bilinear", "(", "arc_tag_h", ",", "arc_tag_c", ")", "\n", "# remove the first #leading_symbolic arc_tags.", "\n", "out_arc_tag", "=", "out_arc_tag", "[", ":", ",", ":", ",", "leading_symbolic", ":", "]", "\n", "# compute the prediction of arc_tags [batch_size, length]", "\n", "_", ",", "arc_tags", "=", "out_arc_tag", ".", "max", "(", "dim", "=", "2", ")", "\n", "return", "arc_tags", "+", "leading_symbolic", "\n", "\n", "# out_arc shape [batch_size, length, length]", "\n", "", "out_arc", "=", "out_arc", ".", "data", "\n", "_", ",", "max_len", ",", "_", "=", "out_arc", ".", "size", "(", ")", "\n", "# set diagonal elements to -inf", "\n", "out_arc", "=", "out_arc", "+", "torch", ".", "diag", "(", "out_arc", ".", "new", "(", "max_len", ")", ".", "fill_", "(", "-", "np", ".", "inf", ")", ")", "\n", "# set invalid positions to -inf", "\n", "if", "mask", "is", "not", "None", ":", "\n", "# minus_mask = (1 - mask.data).byte().view(batch_size, max_len, 1)", "\n", "            ", "minus_mask", "=", "(", "1", "-", "mask", ".", "data", ")", ".", "byte", "(", ")", ".", "unsqueeze", "(", "2", ")", "\n", "out_arc", ".", "masked_fill_", "(", "minus_mask", ",", "-", "np", ".", "inf", ")", "\n", "\n", "# compute naive predictions.", "\n", "# prediction shape = [batch_size, length]", "\n", "", "scores", ",", "heads", "=", "out_arc", ".", "max", "(", "dim", "=", "1", ")", "\n", "\n", "arc_tags", "=", "_decode_arc_tags", "(", "out_arc_tag", ",", "heads", ",", "leading_symbolic", ")", "\n", "# compute the average score for each tree", "\n", "if", "mask", "is", "not", "None", ":", "\n", "            ", "scores", "=", "scores", ".", "sum", "(", "1", ")", "/", "mask", ".", "sum", "(", "1", ")", "\n", "", "else", ":", "\n", "            ", "scores", "=", "scores", ".", "sum", "(", "1", ")", "/", "max_len", "\n", "", "return", "heads", ",", "arc_tags", ",", "scores", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.models.parsing.BiAffine_Parser_Decoder.pre_loss": [[207, 261], ["arc_tag_h.unsqueeze().expand().contiguous.unsqueeze().expand().contiguous.size", "parsing.BiAffine_Parser_Decoder.bilinear", "ValueError", "torch.arange().type_as().long", "torch.arange().type_as().long", "torch.arange().type_as().long", "torch.arange().type_as().long", "torch.arange().type_as().long", "torch.arange().type_as().long", "torch.arange().type_as().long", "torch.arange().type_as().long", "torch.arange().type_as().long", "arc_tag_h[].transpose().contiguous", "arc_tag_h.unsqueeze().expand().contiguous.unsqueeze().expand().contiguous.unsqueeze().expand().contiguous", "arc_tag_c.unsqueeze().expand().contiguous.unsqueeze().expand().contiguous.unsqueeze().expand().contiguous", "softmax_func", "softmax_func", "torch.arange().view().expand", "torch.arange().view().expand", "torch.arange().view().expand", "torch.arange().view().expand", "torch.arange().view().expand", "torch.arange().view().expand", "torch.arange().view().expand", "torch.arange().view().expand", "torch.arange().view().expand", "child_index.type_as().long.type_as().long.type_as().long", "[].t", "[].t", "softmax_func", "softmax_func().permute", "minus_mask.unsqueeze", "heads.size", "torch.arange().type_as", "torch.arange().type_as", "torch.arange().type_as", "torch.arange().type_as", "torch.arange().type_as", "torch.arange().type_as", "torch.arange().type_as", "torch.arange().type_as", "torch.arange().type_as", "arc_tag_h[].transpose", "arc_tag_h.unsqueeze().expand().contiguous.unsqueeze().expand().contiguous.unsqueeze().expand", "arc_tag_c.unsqueeze().expand().contiguous.unsqueeze().expand().contiguous.unsqueeze().expand", "minus_mask.unsqueeze", "mask.unsqueeze", "mask.unsqueeze", "torch.arange().view", "torch.arange().view", "torch.arange().view", "torch.arange().view", "torch.arange().view", "torch.arange().view", "torch.arange().view", "torch.arange().view", "torch.arange().view", "child_index.type_as().long.type_as().long.type_as", "softmax_func", "mask.unsqueeze", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "arc_tag_h.unsqueeze().expand().contiguous.unsqueeze().expand().contiguous.unsqueeze", "arc_tag_c.unsqueeze().expand().contiguous.unsqueeze().expand().contiguous.unsqueeze", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "heads.data.t", "arc_tags.data.t", "heads.data.t"], "methods", ["home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.size", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.size"], ["", "def", "pre_loss", "(", "self", ",", "out_arc", ",", "out_arc_tag", ",", "heads", "=", "None", ",", "arc_tags", "=", "None", ",", "mask", "=", "None", ",", "length", "=", "None", ",", "use_log", "=", "True", ",", "temperature", "=", "1.0", ")", ":", "\n", "        ", "if", "(", "heads", "is", "not", "None", "and", "arc_tags", "is", "None", ")", "or", "(", "heads", "is", "None", "and", "arc_tags", "is", "not", "None", ")", ":", "\n", "            ", "raise", "ValueError", "(", "'heads and arc_tags should be both Nones or both not Nones'", ")", "\n", "", "decode", "=", "True", "if", "(", "heads", "is", "None", "and", "arc_tags", "is", "None", ")", "else", "False", "\n", "softmax_func", "=", "F", ".", "log_softmax", "if", "use_log", "else", "F", ".", "softmax", "\n", "# out_arc shape [batch_size, length, length]", "\n", "# out_arc_tag shape [batch_size, length, arc_tag_space]", "\n", "arc_tag_h", ",", "arc_tag_c", "=", "out_arc_tag", "\n", "batch_size", ",", "max_len", ",", "arc_tag_space", "=", "arc_tag_h", ".", "size", "(", ")", "\n", "batch_index", "=", "None", "\n", "if", "not", "decode", ":", "\n", "            ", "if", "length", "is", "not", "None", "and", "heads", ".", "size", "(", "1", ")", "!=", "max_len", ":", "\n", "                ", "heads", "=", "heads", "[", ":", ",", ":", "max_len", "]", "\n", "arc_tags", "=", "arc_tags", "[", ":", ",", ":", "max_len", "]", "\n", "# create batch index [batch_size]", "\n", "", "batch_index", "=", "torch", ".", "arange", "(", "0", ",", "batch_size", ")", ".", "type_as", "(", "out_arc", ".", "data", ")", ".", "long", "(", ")", "\n", "# get vector for heads [batch_size, length, arc_tag_space],", "\n", "arc_tag_h", "=", "arc_tag_h", "[", "batch_index", ",", "heads", ".", "data", ".", "t", "(", ")", "]", ".", "transpose", "(", "0", ",", "1", ")", ".", "contiguous", "(", ")", "\n", "", "else", ":", "\n", "            ", "arc_tag_h", "=", "arc_tag_h", ".", "unsqueeze", "(", "2", ")", ".", "expand", "(", "batch_size", ",", "max_len", ",", "max_len", ",", "arc_tag_space", ")", ".", "contiguous", "(", ")", "\n", "arc_tag_c", "=", "arc_tag_c", ".", "unsqueeze", "(", "1", ")", ".", "expand", "(", "batch_size", ",", "max_len", ",", "max_len", ",", "arc_tag_space", ")", ".", "contiguous", "(", ")", "\n", "\n", "# compute output for arc_tag [batch_size, length, num_arcs]", "\n", "", "out_arc_tag", "=", "self", ".", "bilinear", "(", "arc_tag_h", ",", "arc_tag_c", ")", "\n", "\n", "# mask invalid position to -inf for softmax_func", "\n", "if", "mask", "is", "not", "None", ":", "\n", "            ", "minus_inf", "=", "-", "1e8", "\n", "minus_mask", "=", "(", "1", "-", "mask", ")", "*", "minus_inf", "\n", "out_arc", "=", "out_arc", "+", "minus_mask", ".", "unsqueeze", "(", "2", ")", "+", "minus_mask", ".", "unsqueeze", "(", "1", ")", "\n", "\n", "", "if", "not", "decode", ":", "\n", "# loss_arc shape [batch_size, length, length]", "\n", "            ", "out_arc", "=", "softmax_func", "(", "out_arc", "/", "temperature", ",", "dim", "=", "1", ")", "\n", "# loss_arc_tag shape [batch_size, length, num_arcs]", "\n", "out_arc_tag", "=", "softmax_func", "(", "out_arc_tag", "/", "temperature", ",", "dim", "=", "2", ")", "\n", "# mask invalid position to 0 for sum loss", "\n", "if", "mask", "is", "not", "None", ":", "\n", "                ", "out_arc", "=", "out_arc", "*", "mask", ".", "unsqueeze", "(", "2", ")", "*", "mask", ".", "unsqueeze", "(", "1", ")", "\n", "out_arc_tag", "=", "out_arc_tag", "*", "mask", ".", "unsqueeze", "(", "2", ")", "\n", "\n", "# first create index matrix [length, batch_size]", "\n", "", "child_index", "=", "torch", ".", "arange", "(", "0", ",", "max_len", ")", ".", "view", "(", "max_len", ",", "1", ")", ".", "expand", "(", "max_len", ",", "batch_size", ")", "\n", "child_index", "=", "child_index", ".", "type_as", "(", "out_arc", ".", "data", ")", ".", "long", "(", ")", "\n", "# loss_arc shape [batch_size, length-1]", "\n", "out_arc", "=", "out_arc", "[", "batch_index", ",", "heads", ".", "data", ".", "t", "(", ")", ",", "child_index", "]", "[", "1", ":", "]", ".", "t", "(", ")", "\n", "# loss_arc_tag shape [batch_size, length-1]", "\n", "out_arc_tag", "=", "out_arc_tag", "[", "batch_index", ",", "child_index", ",", "arc_tags", ".", "data", ".", "t", "(", ")", "]", "[", "1", ":", "]", ".", "t", "(", ")", "\n", "", "else", ":", "\n", "# loss_arc shape [batch_size, length, length]", "\n", "            ", "out_arc", "=", "softmax_func", "(", "out_arc", "/", "temperature", ",", "dim", "=", "1", ")", "\n", "# loss_arc_tag shape [batch_size, length, length, num_arcs]", "\n", "out_arc_tag", "=", "softmax_func", "(", "out_arc_tag", "/", "temperature", ",", "dim", "=", "3", ")", ".", "permute", "(", "0", ",", "3", ",", "1", ",", "2", ")", "\n", "", "return", "out_arc", ",", "out_arc_tag", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.models.parsing.BiRecurrentConv_Encoder.__init__": [[264, 301], ["torch.Module.__init__", "torch.Embedding", "torch.Dropout2d", "torch.Dropout2d", "torch.Dropout2d", "torch.Dropout2d", "torch.Dropout2d", "torch.Dropout2d", "torch.Dropout", "torch.Dropout", "torch.Dropout", "RNN", "parsing.BiRecurrentConv_Encoder.reset_parameters", "torch.Embedding", "torch.Embedding", "torch.Conv1d", "torch.Conv1d", "torch.Conv1d", "ValueError"], "methods", ["home.repos.pwc.inspect_result.Jivnesh_LCM.io_.reader.Reader.__init__", "home.repos.pwc.inspect_result.Jivnesh_LCM.models.parsing_gating.BiRecurrentConv_Encoder.reset_parameters"], ["    ", "def", "__init__", "(", "self", ",", "word_dim", ",", "num_words", ",", "char_dim", ",", "num_chars", ",", "use_pos", ",", "use_char", ",", "pos_dim", ",", "num_pos", ",", "num_filters", ",", "\n", "kernel_size", ",", "rnn_mode", ",", "hidden_size", ",", "num_layers", ",", "embedd_word", "=", "None", ",", "embedd_char", "=", "None", ",", "embedd_pos", "=", "None", ",", "\n", "p_in", "=", "0.33", ",", "p_out", "=", "0.33", ",", "p_rnn", "=", "(", "0.33", ",", "0.33", ")", ",", "initializer", "=", "None", ")", ":", "\n", "        ", "super", "(", "BiRecurrentConv_Encoder", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "word_embedd", "=", "Embedding", "(", "num_words", ",", "word_dim", ",", "init_embedding", "=", "embedd_word", ")", "\n", "self", ".", "char_embedd", "=", "Embedding", "(", "num_chars", ",", "char_dim", ",", "init_embedding", "=", "embedd_char", ")", "if", "use_char", "else", "None", "\n", "self", ".", "pos_embedd", "=", "Embedding", "(", "num_pos", ",", "pos_dim", ",", "init_embedding", "=", "embedd_pos", ")", "if", "use_pos", "else", "None", "\n", "self", ".", "conv1d", "=", "nn", ".", "Conv1d", "(", "char_dim", ",", "num_filters", ",", "kernel_size", ",", "padding", "=", "kernel_size", "-", "1", ")", "if", "use_char", "else", "None", "\n", "# dropout word", "\n", "self", ".", "dropout_in", "=", "nn", ".", "Dropout2d", "(", "p_in", ")", "\n", "# standard dropout", "\n", "self", ".", "dropout_out", "=", "nn", ".", "Dropout2d", "(", "p_out", ")", "\n", "self", ".", "dropout_rnn_in", "=", "nn", ".", "Dropout", "(", "p_rnn", "[", "0", "]", ")", "\n", "self", ".", "use_pos", "=", "use_pos", "\n", "self", ".", "use_char", "=", "use_char", "\n", "self", ".", "rnn_mode", "=", "rnn_mode", "\n", "self", ".", "dim_enc", "=", "word_dim", "\n", "if", "use_pos", ":", "\n", "            ", "self", ".", "dim_enc", "+=", "pos_dim", "\n", "", "if", "use_char", ":", "\n", "            ", "self", ".", "dim_enc", "+=", "num_filters", "\n", "\n", "", "if", "rnn_mode", "==", "'RNN'", ":", "\n", "            ", "RNN", "=", "nn", ".", "RNN", "\n", "drop_p_rnn", "=", "p_rnn", "[", "1", "]", "\n", "", "elif", "rnn_mode", "==", "'LSTM'", ":", "\n", "            ", "RNN", "=", "nn", ".", "LSTM", "\n", "drop_p_rnn", "=", "p_rnn", "[", "1", "]", "\n", "", "elif", "rnn_mode", "==", "'GRU'", ":", "\n", "            ", "RNN", "=", "nn", ".", "GRU", "\n", "drop_p_rnn", "=", "p_rnn", "[", "1", "]", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "'Unknown RNN mode: %s'", "%", "rnn_mode", ")", "\n", "", "self", ".", "rnn", "=", "RNN", "(", "self", ".", "dim_enc", ",", "hidden_size", ",", "num_layers", "=", "num_layers", ",", "batch_first", "=", "True", ",", "bidirectional", "=", "True", ",", "\n", "dropout", "=", "drop_p_rnn", ")", "\n", "self", ".", "initializer", "=", "initializer", "\n", "self", ".", "reset_parameters", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.models.parsing.BiRecurrentConv_Encoder.reset_parameters": [[302, 312], ["parsing.BiRecurrentConv_Encoder.named_parameters", "name.find", "parameter.dim", "parameter.data.zero_", "parsing.BiRecurrentConv_Encoder.initializer"], "methods", ["None"], ["", "def", "reset_parameters", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "initializer", "is", "None", ":", "\n", "            ", "return", "\n", "\n", "", "for", "name", ",", "parameter", "in", "self", ".", "named_parameters", "(", ")", ":", "\n", "            ", "if", "name", ".", "find", "(", "'embedd'", ")", "==", "-", "1", ":", "\n", "                ", "if", "parameter", ".", "dim", "(", ")", "==", "1", ":", "\n", "                    ", "parameter", ".", "data", ".", "zero_", "(", ")", "\n", "", "else", ":", "\n", "                    ", "self", ".", "initializer", "(", "parameter", ".", "data", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.models.parsing.BiRecurrentConv_Encoder.forward": [[313, 365], ["parsing.BiRecurrentConv_Encoder.word_embedd", "parsing.BiRecurrentConv_Encoder.dropout_in", "parsing.BiRecurrentConv_Encoder.dropout_rnn_in", "parsing.BiRecurrentConv_Encoder.dropout_out", "mask.data.sum().long", "parsing.BiRecurrentConv_Encoder.char_embedd", "parsing.BiRecurrentConv_Encoder.size", "parsing.BiRecurrentConv_Encoder.view().transpose", "parsing.BiRecurrentConv_Encoder.conv1d().max", "torch.tanh().view", "torch.tanh().view", "torch.tanh().view", "torch.tanh().view", "torch.tanh().view", "torch.tanh().view", "torch.tanh().view", "torch.tanh().view", "torch.tanh().view", "parsing.BiRecurrentConv_Encoder.dropout_in", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "parsing.BiRecurrentConv_Encoder.pos_embedd", "parsing.BiRecurrentConv_Encoder.dropout_in", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.utils.prepare_rnn_seq", "parsing.BiRecurrentConv_Encoder.rnn.flatten_parameters", "parsing.BiRecurrentConv_Encoder.rnn", "torch.utils.recover_rnn_seq", "parsing.BiRecurrentConv_Encoder.rnn.flatten_parameters", "parsing.BiRecurrentConv_Encoder.rnn", "mask.data.sum", "parsing.BiRecurrentConv_Encoder.view", "parsing.BiRecurrentConv_Encoder.conv1d", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh"], "methods", ["home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.size", "home.repos.pwc.inspect_result.Jivnesh_LCM.nn.utils.prepare_rnn_seq", "home.repos.pwc.inspect_result.Jivnesh_LCM.nn.utils.recover_rnn_seq"], ["", "", "", "", "def", "forward", "(", "self", ",", "input_word", ",", "input_char", ",", "input_pos", ",", "mask", "=", "None", ",", "length", "=", "None", ",", "hx", "=", "None", ")", ":", "\n", "# hack length from mask", "\n", "# we do not hack mask from length for special reasons.", "\n", "# Thus, always provide mask if it is necessary.", "\n", "        ", "if", "length", "is", "None", "and", "mask", "is", "not", "None", ":", "\n", "            ", "length", "=", "mask", ".", "data", ".", "sum", "(", "dim", "=", "1", ")", ".", "long", "(", ")", "\n", "\n", "# [batch_size, length, word_dim]", "\n", "", "word", "=", "self", ".", "word_embedd", "(", "input_word", ")", "\n", "# apply dropout on input", "\n", "word", "=", "self", ".", "dropout_in", "(", "word", ")", "\n", "\n", "input", "=", "word", "\n", "if", "self", ".", "use_char", ":", "\n", "# [batch_size, length, char_length, char_dim]", "\n", "            ", "char", "=", "self", ".", "char_embedd", "(", "input_char", ")", "\n", "char_size", "=", "char", ".", "size", "(", ")", "\n", "# first transform to [batch *length, char_length, char_dim]", "\n", "# then transpose to [batch * length, char_dim, char_length]", "\n", "char", "=", "char", ".", "view", "(", "char_size", "[", "0", "]", "*", "char_size", "[", "1", "]", ",", "char_size", "[", "2", "]", ",", "char_size", "[", "3", "]", ")", ".", "transpose", "(", "1", ",", "2", ")", "\n", "# put into cnn [batch*length, char_filters, char_length]", "\n", "# then put into maxpooling [batch * length, char_filters]", "\n", "char", ",", "_", "=", "self", ".", "conv1d", "(", "char", ")", ".", "max", "(", "dim", "=", "2", ")", "\n", "# reshape to [batch_size, length, char_filters]", "\n", "char", "=", "torch", ".", "tanh", "(", "char", ")", ".", "view", "(", "char_size", "[", "0", "]", ",", "char_size", "[", "1", "]", ",", "-", "1", ")", "\n", "# apply dropout on input", "\n", "char", "=", "self", ".", "dropout_in", "(", "char", ")", "\n", "# concatenate word and char [batch_size, length, word_dim+char_filter]", "\n", "input", "=", "torch", ".", "cat", "(", "[", "input", ",", "char", "]", ",", "dim", "=", "2", ")", "\n", "\n", "", "if", "self", ".", "use_pos", ":", "\n", "# [batch_size, length, pos_dim]", "\n", "            ", "pos", "=", "self", ".", "pos_embedd", "(", "input_pos", ")", "\n", "# apply dropout on input", "\n", "pos", "=", "self", ".", "dropout_in", "(", "pos", ")", "\n", "input", "=", "torch", ".", "cat", "(", "[", "input", ",", "pos", "]", ",", "dim", "=", "2", ")", "\n", "\n", "# apply dropout rnn input", "\n", "", "input", "=", "self", ".", "dropout_rnn_in", "(", "input", ")", "\n", "# prepare packed_sequence", "\n", "if", "length", "is", "not", "None", ":", "\n", "            ", "seq_input", ",", "hx", ",", "rev_order", ",", "mask", "=", "utils", ".", "prepare_rnn_seq", "(", "input", ",", "length", ",", "hx", "=", "hx", ",", "masks", "=", "mask", ",", "batch_first", "=", "True", ")", "\n", "self", ".", "rnn", ".", "flatten_parameters", "(", ")", "\n", "seq_output", ",", "hn", "=", "self", ".", "rnn", "(", "seq_input", ",", "hx", "=", "hx", ")", "\n", "output", ",", "hn", "=", "utils", ".", "recover_rnn_seq", "(", "seq_output", ",", "rev_order", ",", "hx", "=", "hn", ",", "batch_first", "=", "True", ")", "\n", "", "else", ":", "\n", "# output from rnn [batch_size, length, hidden_size]", "\n", "            ", "self", ".", "rnn", ".", "flatten_parameters", "(", ")", "\n", "output", ",", "hn", "=", "self", ".", "rnn", "(", "input", ",", "hx", "=", "hx", ")", "\n", "# apply dropout for the output of rnn", "\n", "", "output", "=", "self", ".", "dropout_out", "(", "output", ")", "\n", "return", "output", ",", "hn", ",", "mask", ",", "length", "", "", "", ""]], "home.repos.pwc.inspect_result.Jivnesh_LCM.models.sequence_tagger.Sequence_Tagger.__init__": [[8, 21], ["torch.Module.__init__", "sequence_tagger.BiRecurrentConv_Encoder", "sequence_tagger.Tagger_Decoder"], "methods", ["home.repos.pwc.inspect_result.Jivnesh_LCM.io_.reader.Reader.__init__"], ["    ", "def", "__init__", "(", "self", ",", "word_dim", ",", "num_words", ",", "char_dim", ",", "num_chars", ",", "use_pos", ",", "use_char", ",", "pos_dim", ",", "num_pos", ",", "\n", "num_filters", ",", "kernel_size", ",", "rnn_mode", ",", "hidden_size", ",", "num_layers", ",", "tag_space", ",", "num_tags", ",", "\n", "embedd_word", "=", "None", ",", "embedd_char", "=", "None", ",", "embedd_pos", "=", "None", ",", "\n", "p_in", "=", "0.33", ",", "p_out", "=", "0.33", ",", "p_rnn", "=", "(", "0.33", ",", "0.33", ")", ",", "\n", "initializer", "=", "None", ")", ":", "\n", "        ", "super", "(", "Sequence_Tagger", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "rnn_encoder", "=", "BiRecurrentConv_Encoder", "(", "word_dim", ",", "num_words", ",", "char_dim", ",", "num_chars", ",", "use_pos", ",", "use_char", ",", "\n", "pos_dim", ",", "num_pos", ",", "num_filters", ",", "\n", "kernel_size", ",", "rnn_mode", ",", "hidden_size", ",", "\n", "num_layers", ",", "embedd_word", "=", "embedd_word", ",", "\n", "embedd_char", "=", "embedd_char", ",", "embedd_pos", "=", "embedd_pos", ",", "\n", "p_in", "=", "p_in", ",", "p_out", "=", "p_out", ",", "p_rnn", "=", "p_rnn", ",", "initializer", "=", "initializer", ")", "\n", "self", ".", "sequence_tagger_decoder", "=", "Tagger_Decoder", "(", "hidden_size", ",", "tag_space", ",", "num_tags", ",", "p_out", ",", "initializer", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.models.sequence_tagger.Sequence_Tagger.forward": [[22, 26], ["sequence_tagger.Sequence_Tagger.rnn_encoder", "sequence_tagger.Sequence_Tagger.sequence_tagger_decoder"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_word", ",", "input_char", ",", "input_pos", ",", "mask", "=", "None", ",", "length", "=", "None", ",", "hx", "=", "None", ")", ":", "\n", "        ", "encoder_output", ",", "hn", ",", "mask", ",", "length", "=", "self", ".", "rnn_encoder", "(", "input_word", ",", "input_char", ",", "input_pos", ",", "mask", ",", "length", ",", "hx", ")", "\n", "out_counter", "=", "self", ".", "sequence_tagger_decoder", "(", "encoder_output", ",", "mask", ")", "\n", "return", "out_counter", ",", "mask", ",", "length", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.models.sequence_tagger.Sequence_Tagger.loss": [[27, 30], ["sequence_tagger.Sequence_Tagger.sequence_tagger_decoder.loss"], "methods", ["home.repos.pwc.inspect_result.Jivnesh_LCM.models.parsing_gating.BiAffine_Parser_Decoder.loss"], ["", "def", "loss", "(", "self", ",", "input", ",", "target", ",", "mask", "=", "None", ",", "length", "=", "None", ")", ":", "\n", "        ", "loss_", "=", "self", ".", "sequence_tagger_decoder", ".", "loss", "(", "input", ",", "target", ",", "mask", ",", "length", ")", "\n", "return", "loss_", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.models.sequence_tagger.Sequence_Tagger.decode": [[31, 34], ["sequence_tagger.Sequence_Tagger.sequence_tagger_decoder.decode"], "methods", ["home.repos.pwc.inspect_result.Jivnesh_LCM.models.parsing_gating.BiAffine_Parser_Decoder.decode"], ["", "def", "decode", "(", "self", ",", "input", ",", "mask", "=", "None", ",", "length", "=", "None", ",", "leading_symbolic", "=", "0", ")", ":", "\n", "        ", "out_pred", "=", "self", ".", "sequence_tagger_decoder", ".", "decode", "(", "input", ",", "mask", ",", "leading_symbolic", ")", "\n", "return", "out_pred", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.models.sequence_tagger.Tagger_Decoder.__init__": [[36, 50], ["torch.Module.__init__", "torch.CrossEntropyLoss", "torch.CrossEntropyLoss", "torch.CrossEntropyLoss", "torch.Dropout", "torch.Dropout", "torch.Dropout", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "sequence_tagger.Tagger_Decoder.reset_parameters"], "methods", ["home.repos.pwc.inspect_result.Jivnesh_LCM.io_.reader.Reader.__init__", "home.repos.pwc.inspect_result.Jivnesh_LCM.models.parsing_gating.BiRecurrentConv_Encoder.reset_parameters"], ["    ", "def", "__init__", "(", "self", ",", "hidden_size", ",", "tag_space", ",", "num_tags", ",", "p_out", ",", "initializer", ")", ":", "\n", "        ", "super", "(", "Tagger_Decoder", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "criterion_obj", "=", "nn", ".", "CrossEntropyLoss", "(", ")", "\n", "self", ".", "tag_space", "=", "tag_space", "\n", "self", ".", "num_tags", "=", "num_tags", "\n", "self", ".", "p_out", "=", "p_out", "\n", "self", ".", "initializer", "=", "initializer", "\n", "self", ".", "dropout_out", "=", "nn", ".", "Dropout", "(", "p_out", ")", "\n", "self", ".", "out_dim", "=", "2", "*", "hidden_size", "\n", "self", ".", "num_tags", "=", "num_tags", "\n", "self", ".", "fc_1", "=", "nn", ".", "Linear", "(", "self", ".", "out_dim", ",", "tag_space", ")", "\n", "self", ".", "fc_2", "=", "nn", ".", "Linear", "(", "tag_space", ",", "tag_space", "//", "2", ")", "\n", "self", ".", "fc_3", "=", "nn", ".", "Linear", "(", "tag_space", "//", "2", ",", "num_tags", ")", "\n", "self", ".", "reset_parameters", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.models.sequence_tagger.Tagger_Decoder.reset_parameters": [[51, 59], ["sequence_tagger.Tagger_Decoder.named_parameters", "parameter.dim", "parameter.data.zero_", "sequence_tagger.Tagger_Decoder.initializer"], "methods", ["None"], ["", "def", "reset_parameters", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "initializer", "is", "None", ":", "\n", "            ", "return", "\n", "", "for", "name", ",", "parameter", "in", "self", ".", "named_parameters", "(", ")", ":", "\n", "            ", "if", "parameter", ".", "dim", "(", ")", "==", "1", ":", "\n", "                ", "parameter", ".", "data", ".", "zero_", "(", ")", "\n", "", "else", ":", "\n", "                ", "self", ".", "initializer", "(", "parameter", ".", "data", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.models.sequence_tagger.Tagger_Decoder.forward": [[60, 68], ["sequence_tagger.Tagger_Decoder.dropout_out", "sequence_tagger.Tagger_Decoder.dropout_out", "sequence_tagger.Tagger_Decoder.fc_3", "torch.elu", "torch.elu", "torch.elu", "torch.elu", "torch.elu", "torch.elu", "sequence_tagger.Tagger_Decoder.fc_1", "sequence_tagger.Tagger_Decoder.fc_2"], "methods", ["None"], ["", "", "", "def", "forward", "(", "self", ",", "input", ",", "mask", ")", ":", "\n", "# input from rnn [batch_size, length, hidden_size]", "\n", "# [batch_size, length, tag_space]", "\n", "        ", "output", "=", "self", ".", "dropout_out", "(", "F", ".", "elu", "(", "self", ".", "fc_1", "(", "input", ")", ")", ")", "\n", "#output = self.fc_2(output)", "\n", "output", "=", "self", ".", "dropout_out", "(", "F", ".", "elu", "(", "self", ".", "fc_2", "(", "output", ")", ")", ")", "\n", "output", "=", "self", ".", "fc_3", "(", "output", ")", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.models.sequence_tagger.Tagger_Decoder.loss": [[69, 78], ["input.view.view.view", "target.contiguous().view.contiguous().view.contiguous().view", "sequence_tagger.Tagger_Decoder.criterion_obj", "length.max", "target.contiguous().view.contiguous().view.size", "target.contiguous().view.contiguous().view.contiguous"], "methods", ["home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.size"], ["", "def", "loss", "(", "self", ",", "input", ",", "target", ",", "mask", "=", "None", ",", "length", "=", "None", ")", ":", "\n", "        ", "if", "length", "is", "not", "None", ":", "\n", "            ", "max_len", "=", "length", ".", "max", "(", ")", "\n", "if", "target", ".", "size", "(", "1", ")", "!=", "max_len", ":", "\n", "                ", "target", "=", "target", "[", ":", ",", ":", "max_len", "]", "\n", "", "", "input", "=", "input", ".", "view", "(", "-", "1", ",", "self", ".", "num_tags", ")", "\n", "target", "=", "target", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ")", "\n", "loss_", "=", "self", ".", "criterion_obj", "(", "input", ",", "target", ")", "\n", "return", "loss_", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.models.sequence_tagger.Tagger_Decoder.decode": [[79, 87], ["torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "mask.unsqueeze"], "methods", ["None"], ["", "def", "decode", "(", "self", ",", "input", ",", "mask", "=", "None", ",", "leading_symbolic", "=", "0", ")", ":", "\n", "        ", "if", "mask", "is", "not", "None", ":", "\n", "            ", "input", "=", "input", "*", "mask", ".", "unsqueeze", "(", "2", ")", "\n", "# remove the first #symbolic rows and columns.", "\n", "# now the shape of the input is [n_time_steps, batch_size, t] where t = num_labels - #symbolic.", "\n", "", "input", "=", "input", "[", ":", ",", ":", ",", ":", "-", "leading_symbolic", "]", "\n", "preds", "=", "torch", ".", "argmax", "(", "input", ",", "-", "1", ")", "\n", "return", "preds", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.models.sequence_tagger.BiRecurrentConv_Encoder.__init__": [[89, 126], ["torch.Module.__init__", "torch.Embedding", "torch.Dropout2d", "torch.Dropout2d", "torch.Dropout2d", "torch.Dropout2d", "torch.Dropout2d", "torch.Dropout2d", "torch.Dropout", "torch.Dropout", "torch.Dropout", "RNN", "sequence_tagger.BiRecurrentConv_Encoder.reset_parameters", "torch.Embedding", "torch.Embedding", "torch.Conv1d", "torch.Conv1d", "torch.Conv1d", "ValueError"], "methods", ["home.repos.pwc.inspect_result.Jivnesh_LCM.io_.reader.Reader.__init__", "home.repos.pwc.inspect_result.Jivnesh_LCM.models.parsing_gating.BiRecurrentConv_Encoder.reset_parameters"], ["    ", "def", "__init__", "(", "self", ",", "word_dim", ",", "num_words", ",", "char_dim", ",", "num_chars", ",", "use_pos", ",", "use_char", ",", "pos_dim", ",", "num_pos", ",", "num_filters", ",", "\n", "kernel_size", ",", "rnn_mode", ",", "hidden_size", ",", "num_layers", ",", "embedd_word", "=", "None", ",", "embedd_char", "=", "None", ",", "embedd_pos", "=", "None", ",", "\n", "p_in", "=", "0.33", ",", "p_out", "=", "0.33", ",", "p_rnn", "=", "(", "0.33", ",", "0.33", ")", ",", "initializer", "=", "None", ")", ":", "\n", "        ", "super", "(", "BiRecurrentConv_Encoder", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "word_embedd", "=", "Embedding", "(", "num_words", ",", "word_dim", ",", "init_embedding", "=", "embedd_word", ")", "\n", "self", ".", "char_embedd", "=", "Embedding", "(", "num_chars", ",", "char_dim", ",", "init_embedding", "=", "embedd_char", ")", "if", "use_char", "else", "None", "\n", "self", ".", "pos_embedd", "=", "Embedding", "(", "num_pos", ",", "pos_dim", ",", "init_embedding", "=", "embedd_pos", ")", "if", "use_pos", "else", "None", "\n", "self", ".", "conv1d", "=", "nn", ".", "Conv1d", "(", "char_dim", ",", "num_filters", ",", "kernel_size", ",", "padding", "=", "kernel_size", "-", "1", ")", "if", "use_char", "else", "None", "\n", "# dropout word", "\n", "self", ".", "dropout_in", "=", "nn", ".", "Dropout2d", "(", "p_in", ")", "\n", "# standard dropout", "\n", "self", ".", "dropout_out", "=", "nn", ".", "Dropout2d", "(", "p_out", ")", "\n", "self", ".", "dropout_rnn_in", "=", "nn", ".", "Dropout", "(", "p_rnn", "[", "0", "]", ")", "\n", "self", ".", "use_pos", "=", "use_pos", "\n", "self", ".", "use_char", "=", "use_char", "\n", "self", ".", "rnn_mode", "=", "rnn_mode", "\n", "self", ".", "dim_enc", "=", "word_dim", "\n", "if", "use_pos", ":", "\n", "            ", "self", ".", "dim_enc", "+=", "pos_dim", "\n", "", "if", "use_char", ":", "\n", "            ", "self", ".", "dim_enc", "+=", "num_filters", "\n", "\n", "", "if", "rnn_mode", "==", "'RNN'", ":", "\n", "            ", "RNN", "=", "nn", ".", "RNN", "\n", "drop_p_rnn", "=", "p_rnn", "[", "1", "]", "\n", "", "elif", "rnn_mode", "==", "'LSTM'", ":", "\n", "            ", "RNN", "=", "nn", ".", "LSTM", "\n", "drop_p_rnn", "=", "p_rnn", "[", "1", "]", "\n", "", "elif", "rnn_mode", "==", "'GRU'", ":", "\n", "            ", "RNN", "=", "nn", ".", "GRU", "\n", "drop_p_rnn", "=", "p_rnn", "[", "1", "]", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "'Unknown RNN mode: %s'", "%", "rnn_mode", ")", "\n", "", "self", ".", "rnn", "=", "RNN", "(", "self", ".", "dim_enc", ",", "hidden_size", ",", "num_layers", "=", "num_layers", ",", "batch_first", "=", "True", ",", "bidirectional", "=", "True", ",", "\n", "dropout", "=", "drop_p_rnn", ")", "\n", "self", ".", "initializer", "=", "initializer", "\n", "self", ".", "reset_parameters", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.models.sequence_tagger.BiRecurrentConv_Encoder.reset_parameters": [[127, 137], ["sequence_tagger.BiRecurrentConv_Encoder.named_parameters", "name.find", "parameter.dim", "parameter.data.zero_", "sequence_tagger.BiRecurrentConv_Encoder.initializer"], "methods", ["None"], ["", "def", "reset_parameters", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "initializer", "is", "None", ":", "\n", "            ", "return", "\n", "\n", "", "for", "name", ",", "parameter", "in", "self", ".", "named_parameters", "(", ")", ":", "\n", "            ", "if", "name", ".", "find", "(", "'embedd'", ")", "==", "-", "1", ":", "\n", "                ", "if", "parameter", ".", "dim", "(", ")", "==", "1", ":", "\n", "                    ", "parameter", ".", "data", ".", "zero_", "(", ")", "\n", "", "else", ":", "\n", "                    ", "self", ".", "initializer", "(", "parameter", ".", "data", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.models.sequence_tagger.BiRecurrentConv_Encoder.forward": [[138, 190], ["sequence_tagger.BiRecurrentConv_Encoder.word_embedd", "sequence_tagger.BiRecurrentConv_Encoder.dropout_in", "sequence_tagger.BiRecurrentConv_Encoder.dropout_rnn_in", "sequence_tagger.BiRecurrentConv_Encoder.dropout_out", "mask.data.sum().long", "sequence_tagger.BiRecurrentConv_Encoder.char_embedd", "sequence_tagger.BiRecurrentConv_Encoder.size", "sequence_tagger.BiRecurrentConv_Encoder.view().transpose", "sequence_tagger.BiRecurrentConv_Encoder.conv1d().max", "torch.tanh().view", "torch.tanh().view", "torch.tanh().view", "torch.tanh().view", "torch.tanh().view", "torch.tanh().view", "torch.tanh().view", "torch.tanh().view", "torch.tanh().view", "sequence_tagger.BiRecurrentConv_Encoder.dropout_in", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "sequence_tagger.BiRecurrentConv_Encoder.pos_embedd", "sequence_tagger.BiRecurrentConv_Encoder.dropout_in", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.utils.prepare_rnn_seq", "sequence_tagger.BiRecurrentConv_Encoder.rnn.flatten_parameters", "sequence_tagger.BiRecurrentConv_Encoder.rnn", "torch.utils.recover_rnn_seq", "sequence_tagger.BiRecurrentConv_Encoder.rnn.flatten_parameters", "sequence_tagger.BiRecurrentConv_Encoder.rnn", "mask.data.sum", "sequence_tagger.BiRecurrentConv_Encoder.view", "sequence_tagger.BiRecurrentConv_Encoder.conv1d", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh"], "methods", ["home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.size", "home.repos.pwc.inspect_result.Jivnesh_LCM.nn.utils.prepare_rnn_seq", "home.repos.pwc.inspect_result.Jivnesh_LCM.nn.utils.recover_rnn_seq"], ["", "", "", "", "def", "forward", "(", "self", ",", "input_word", ",", "input_char", ",", "input_pos", ",", "mask", "=", "None", ",", "length", "=", "None", ",", "hx", "=", "None", ")", ":", "\n", "# hack length from mask", "\n", "# we do not hack mask from length for special reasons.", "\n", "# Thus, always provide mask if it is necessary.", "\n", "        ", "if", "length", "is", "None", "and", "mask", "is", "not", "None", ":", "\n", "            ", "length", "=", "mask", ".", "data", ".", "sum", "(", "dim", "=", "1", ")", ".", "long", "(", ")", "\n", "\n", "# [batch_size, length, word_dim]", "\n", "", "word", "=", "self", ".", "word_embedd", "(", "input_word", ")", "\n", "# apply dropout on input", "\n", "word", "=", "self", ".", "dropout_in", "(", "word", ")", "\n", "\n", "input", "=", "word", "\n", "if", "self", ".", "use_char", ":", "\n", "# [batch_size, length, char_length, char_dim]", "\n", "            ", "char", "=", "self", ".", "char_embedd", "(", "input_char", ")", "\n", "char_size", "=", "char", ".", "size", "(", ")", "\n", "# first transform to [batch *length, char_length, char_dim]", "\n", "# then transpose to [batch * length, char_dim, char_length]", "\n", "char", "=", "char", ".", "view", "(", "char_size", "[", "0", "]", "*", "char_size", "[", "1", "]", ",", "char_size", "[", "2", "]", ",", "char_size", "[", "3", "]", ")", ".", "transpose", "(", "1", ",", "2", ")", "\n", "# put into cnn [batch*length, char_filters, char_length]", "\n", "# then put into maxpooling [batch * length, char_filters]", "\n", "char", ",", "_", "=", "self", ".", "conv1d", "(", "char", ")", ".", "max", "(", "dim", "=", "2", ")", "\n", "# reshape to [batch_size, length, char_filters]", "\n", "char", "=", "torch", ".", "tanh", "(", "char", ")", ".", "view", "(", "char_size", "[", "0", "]", ",", "char_size", "[", "1", "]", ",", "-", "1", ")", "\n", "# apply dropout on input", "\n", "char", "=", "self", ".", "dropout_in", "(", "char", ")", "\n", "# concatenate word and char [batch_size, length, word_dim+char_filter]", "\n", "input", "=", "torch", ".", "cat", "(", "[", "input", ",", "char", "]", ",", "dim", "=", "2", ")", "\n", "\n", "", "if", "self", ".", "use_pos", ":", "\n", "# [batch_size, length, pos_dim]", "\n", "            ", "pos", "=", "self", ".", "pos_embedd", "(", "input_pos", ")", "\n", "# apply dropout on input", "\n", "pos", "=", "self", ".", "dropout_in", "(", "pos", ")", "\n", "input", "=", "torch", ".", "cat", "(", "[", "input", ",", "pos", "]", ",", "dim", "=", "2", ")", "\n", "\n", "# apply dropout rnn input", "\n", "", "input", "=", "self", ".", "dropout_rnn_in", "(", "input", ")", "\n", "# prepare packed_sequence", "\n", "if", "length", "is", "not", "None", ":", "\n", "            ", "seq_input", ",", "hx", ",", "rev_order", ",", "mask", "=", "utils", ".", "prepare_rnn_seq", "(", "input", ",", "length", ",", "hx", "=", "hx", ",", "masks", "=", "mask", ",", "batch_first", "=", "True", ")", "\n", "self", ".", "rnn", ".", "flatten_parameters", "(", ")", "\n", "seq_output", ",", "hn", "=", "self", ".", "rnn", "(", "seq_input", ",", "hx", "=", "hx", ")", "\n", "output", ",", "hn", "=", "utils", ".", "recover_rnn_seq", "(", "seq_output", ",", "rev_order", ",", "hx", "=", "hn", ",", "batch_first", "=", "True", ")", "\n", "", "else", ":", "\n", "# output from rnn [batch_size, length, hidden_size]", "\n", "            ", "self", ".", "rnn", ".", "flatten_parameters", "(", ")", "\n", "output", ",", "hn", "=", "self", ".", "rnn", "(", "input", ",", "hx", "=", "hx", ")", "\n", "# apply dropout for the output of rnn", "\n", "", "output", "=", "self", ".", "dropout_out", "(", "output", ")", "\n", "return", "output", ",", "hn", ",", "mask", ",", "length", "", "", "", ""]], "home.repos.pwc.inspect_result.Jivnesh_LCM.models.parsing_gating.BiAffine_Parser_Gated.__init__": [[14, 38], ["torch.Module.__init__", "parsing_gating.BiRecurrentConv_Encoder", "parsing_gating.BiAffine_Parser_Decoder", "torch.ModuleDict", "torch.ModuleDict", "torch.ModuleDict", "parsing_gating.Gating", "str", "parsing_gating.BiRecurrentConv_Encoder", "range"], "methods", ["home.repos.pwc.inspect_result.Jivnesh_LCM.io_.reader.Reader.__init__"], ["    ", "def", "__init__", "(", "self", ",", "word_dim", ",", "num_words", ",", "char_dim", ",", "num_chars", ",", "use_pos", ",", "use_char", ",", "pos_dim", ",", "num_pos", ",", "num_filters", ",", "\n", "kernel_size", ",", "rnn_mode", ",", "hidden_size", ",", "num_layers", ",", "num_arcs", ",", "arc_space", ",", "arc_tag_space", ",", "num_gates", ",", "\n", "embedd_word", "=", "None", ",", "embedd_char", "=", "None", ",", "embedd_pos", "=", "None", ",", "p_in", "=", "0.33", ",", "p_out", "=", "0.33", ",", "p_rnn", "=", "(", "0.33", ",", "0.33", ")", ",", "\n", "biaffine", "=", "True", ",", "arc_decode", "=", "'mst'", ",", "initializer", "=", "None", ")", ":", "\n", "        ", "super", "(", "BiAffine_Parser_Gated", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "num_gates", "=", "num_gates", "\n", "self", ".", "rnn_encoder", "=", "BiRecurrentConv_Encoder", "(", "word_dim", ",", "num_words", ",", "char_dim", ",", "num_chars", ",", "use_pos", ",", "use_char", ",", "\n", "pos_dim", ",", "num_pos", ",", "num_filters", ",", "\n", "kernel_size", ",", "rnn_mode", ",", "hidden_size", ",", "\n", "num_layers", ",", "embedd_word", "=", "embedd_word", ",", "\n", "embedd_char", "=", "embedd_char", ",", "embedd_pos", "=", "embedd_pos", ",", "\n", "p_in", "=", "p_in", ",", "p_out", "=", "p_out", ",", "p_rnn", "=", "p_rnn", ",", "initializer", "=", "initializer", ")", "\n", "if", "self", ".", "num_gates", ">=", "2", ":", "\n", "            ", "self", ".", "extra_rnn_encoders", "=", "nn", ".", "ModuleDict", "(", "[", "[", "str", "(", "i", ")", ",", "BiRecurrentConv_Encoder", "(", "word_dim", ",", "num_words", ",", "char_dim", ",", "num_chars", ",", "use_pos", ",", "use_char", ",", "\n", "pos_dim", ",", "num_pos", ",", "num_filters", ",", "\n", "kernel_size", ",", "rnn_mode", ",", "hidden_size", ",", "\n", "num_layers", ",", "embedd_word", "=", "embedd_word", ",", "\n", "embedd_char", "=", "embedd_char", ",", "embedd_pos", "=", "embedd_pos", ",", "\n", "p_in", "=", "p_in", ",", "p_out", "=", "p_out", ",", "p_rnn", "=", "p_rnn", ",", "initializer", "=", "initializer", ")", "]", "for", "i", "in", "range", "(", "num_gates", "-", "1", ")", "]", ")", "\n", "self", ".", "gate", "=", "Gating", "(", "num_gates", ",", "2", "*", "hidden_size", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "extra_rnn_encoders", "=", "None", "\n", "self", ".", "gate", "=", "None", "\n", "", "self", ".", "parser", "=", "BiAffine_Parser_Decoder", "(", "hidden_size", ",", "num_arcs", ",", "arc_space", ",", "arc_tag_space", ",", "biaffine", ",", "p_out", ",", "arc_decode", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.models.parsing_gating.BiAffine_Parser_Gated.forward": [[39, 49], ["parsing_gating.BiAffine_Parser_Gated.rnn_encoder", "parsing_gating.BiAffine_Parser_Gated.parser", "len", "parsing_gating.BiAffine_Parser_Gated.gate", "parsing_gating.BiAffine_Parser_Gated.extra_rnn_encoders.keys", "tuple", "range", "str"], "methods", ["home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.keys"], ["", "def", "forward", "(", "self", ",", "input_word", ",", "input_char", ",", "input_pos", ",", "mask", "=", "None", ",", "length", "=", "None", ",", "hx", "=", "None", ")", ":", "\n", "        ", "encoder_output", ",", "hn", ",", "mask", ",", "length", "=", "self", ".", "rnn_encoder", "(", "input_word", ",", "input_char", ",", "input_pos", ",", "mask", ",", "length", ",", "hx", ")", "\n", "if", "self", ".", "num_gates", ">=", "2", ":", "\n", "            ", "len_extra_encoders", "=", "len", "(", "self", ".", "extra_rnn_encoders", ".", "keys", "(", ")", ")", "\n", "extra_enconder_outputs", "=", "[", "self", ".", "extra_rnn_encoders", "[", "str", "(", "i", ")", "]", "(", "input_word", ",", "input_char", ",", "input_pos", ",", "mask", ",", "length", ",", "hx", ")", "[", "0", "]", "for", "i", "in", "range", "(", "len_extra_encoders", ")", "]", "\n", "rnns_output", "=", "self", ".", "gate", "(", "tuple", "(", "[", "encoder_output", "]", "+", "extra_enconder_outputs", ")", ")", "\n", "", "else", ":", "\n", "            ", "rnns_output", "=", "encoder_output", "\n", "", "out_arc", ",", "out_arc_tag", "=", "self", ".", "parser", "(", "rnns_output", ",", "mask", ")", "\n", "return", "out_arc", ",", "out_arc_tag", ",", "mask", ",", "length", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.models.parsing_gating.BiAffine_Parser_Gated.loss": [[50, 55], ["parsing_gating.BiAffine_Parser_Gated.parser.loss"], "methods", ["home.repos.pwc.inspect_result.Jivnesh_LCM.models.parsing_gating.BiAffine_Parser_Decoder.loss"], ["", "def", "loss", "(", "self", ",", "out_arc", ",", "out_arc_tag", ",", "heads", ",", "arc_tags", ",", "mask", "=", "None", ",", "length", "=", "None", ")", ":", "\n", "# out_arc shape [batch_size, length, length]", "\n", "# out_arc_tag shape [batch_size, length, arc_tag_space]", "\n", "        ", "loss_arc", ",", "loss_arc_tag", "=", "self", ".", "parser", ".", "loss", "(", "out_arc", ",", "out_arc_tag", ",", "heads", ",", "arc_tags", ",", "mask", ",", "length", ")", "\n", "return", "loss_arc", ",", "loss_arc_tag", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.models.parsing_gating.BiAffine_Parser_Gated.decode": [[56, 61], ["parsing_gating.BiAffine_Parser_Gated.parser.unconstrained_decode_mst"], "methods", ["home.repos.pwc.inspect_result.Jivnesh_LCM.models.parsing_gating.BiAffine_Parser_Decoder.unconstrained_decode_mst"], ["", "def", "decode", "(", "self", ",", "model_path", ",", "input_word", ",", "input_pos", ",", "input_lemma", ",", "out_arc", ",", "out_arc_tag", ",", "mask", "=", "None", ",", "length", "=", "None", ",", "leading_symbolic", "=", "0", ")", ":", "\n", "\n", "        ", "heads_pred", ",", "arc_tags_pred", ",", "scores", "=", "self", ".", "parser", ".", "unconstrained_decode_mst", "(", "model_path", ",", "input_word", ",", "input_pos", ",", "input_lemma", ",", "out_arc", ",", "out_arc_tag", ",", "mask", ",", "length", ",", "leading_symbolic", ")", "\n", "# heads_pred, arc_tags_pred, scores = self.parser.decode(out_arc, out_arc_tag, mask, length, leading_symbolic)", "\n", "return", "heads_pred", ",", "arc_tags_pred", ",", "scores", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.models.parsing_gating.BiAffine_Parser_Gated.constrained_decode": [[62, 66], ["parsing_gating.BiAffine_Parser_Gated.parser.constrained_decode_mst"], "methods", ["None"], ["", "def", "constrained_decode", "(", "self", ",", "model_path", ",", "input_word", ",", "input_pos", ",", "input_lemma", ",", "out_arc", ",", "out_arc_tag", ",", "mask", "=", "None", ",", "length", "=", "None", ",", "leading_symbolic", "=", "0", ")", ":", "\n", "        ", "heads_pred", ",", "arc_tags_pred", ",", "scores", "=", "self", ".", "parser", ".", "constrained_decode_mst", "(", "model_path", ",", "input_word", ",", "input_pos", ",", "input_lemma", ",", "out_arc", ",", "out_arc_tag", ",", "mask", ",", "length", ",", "leading_symbolic", ")", "\n", "# heads_pred, arc_tags_pred, scores = self.parser.decode(out_arc, out_arc_tag, mask, length, leading_symbolic)", "\n", "return", "heads_pred", ",", "arc_tags_pred", ",", "scores", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.models.parsing_gating.BiAffine_Parser_Gated.pre_loss": [[67, 70], ["parsing_gating.BiAffine_Parser_Gated.parser.pre_loss"], "methods", ["home.repos.pwc.inspect_result.Jivnesh_LCM.models.parsing_gating.BiAffine_Parser_Decoder.pre_loss"], ["", "def", "pre_loss", "(", "self", ",", "out_arc", ",", "out_arc_tag", ",", "heads", ",", "arc_tags", ",", "mask", "=", "None", ",", "length", "=", "None", ",", "use_log", "=", "True", ",", "temperature", "=", "1.0", ")", ":", "\n", "        ", "out_arc", ",", "out_arc_tag", "=", "self", ".", "parser", ".", "pre_loss", "(", "out_arc", ",", "out_arc_tag", ",", "heads", ",", "arc_tags", ",", "mask", ",", "length", ",", "use_log", ",", "temperature", ")", "\n", "return", "out_arc", ",", "out_arc_tag", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.models.parsing_gating.BiAffine_Parser_Decoder.__init__": [[72, 88], ["torch.Module.__init__", "torch.Dropout", "torch.Dropout", "torch.Dropout", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.BiAAttention", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.BiLinear"], "methods", ["home.repos.pwc.inspect_result.Jivnesh_LCM.io_.reader.Reader.__init__"], ["    ", "def", "__init__", "(", "self", ",", "hidden_size", ",", "num_arcs", ",", "arc_space", ",", "arc_tag_space", ",", "biaffine", ",", "p_out", ",", "arc_decode", ")", ":", "\n", "        ", "super", "(", "BiAffine_Parser_Decoder", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "num_arcs", "=", "num_arcs", "\n", "self", ".", "arc_space", "=", "arc_space", "\n", "self", ".", "arc_tag_space", "=", "arc_tag_space", "\n", "self", ".", "out_dim", "=", "hidden_size", "*", "2", "\n", "self", ".", "biaffine", "=", "biaffine", "\n", "self", ".", "p_out", "=", "p_out", "\n", "self", ".", "arc_decode", "=", "arc_decode", "\n", "self", ".", "dropout_out", "=", "nn", ".", "Dropout", "(", "self", ".", "p_out", ")", "\n", "self", ".", "arc_h", "=", "nn", ".", "Linear", "(", "self", ".", "out_dim", ",", "self", ".", "arc_space", ")", "\n", "self", ".", "arc_c", "=", "nn", ".", "Linear", "(", "self", ".", "out_dim", ",", "self", ".", "arc_space", ")", "\n", "self", ".", "attention", "=", "BiAAttention", "(", "self", ".", "arc_space", ",", "self", ".", "arc_space", ",", "1", ",", "biaffine", "=", "biaffine", ")", "\n", "self", ".", "arc_tag_h", "=", "nn", ".", "Linear", "(", "self", ".", "out_dim", ",", "arc_tag_space", ")", "\n", "self", ".", "arc_tag_c", "=", "nn", ".", "Linear", "(", "self", ".", "out_dim", ",", "arc_tag_space", ")", "\n", "self", ".", "bilinear", "=", "BiLinear", "(", "arc_tag_space", ",", "arc_tag_space", ",", "num_arcs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.models.parsing_gating.BiAffine_Parser_Decoder.forward": [[89, 122], ["parsing_gating.BiAffine_Parser_Decoder.dropout_out().transpose", "torch.elu", "torch.elu", "torch.elu", "torch.elu", "torch.elu", "torch.elu", "torch.elu", "torch.elu", "torch.elu", "torch.elu", "torch.elu", "torch.elu", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "parsing_gating.BiAffine_Parser_Decoder.dropout_out().transpose", "parsing_gating.BiAffine_Parser_Decoder.chunk", "parsing_gating.BiAffine_Parser_Decoder.dropout_out().transpose", "parsing_gating.BiAffine_Parser_Decoder.chunk", "arc_tag_h.contiguous.contiguous.contiguous", "arc_tag_c.contiguous.contiguous.contiguous", "parsing_gating.BiAffine_Parser_Decoder.attention().squeeze", "parsing_gating.BiAffine_Parser_Decoder.arc_h", "parsing_gating.BiAffine_Parser_Decoder.arc_c", "parsing_gating.BiAffine_Parser_Decoder.arc_tag_h", "parsing_gating.BiAffine_Parser_Decoder.arc_tag_c", "parsing_gating.BiAffine_Parser_Decoder.dropout_out", "parsing_gating.BiAffine_Parser_Decoder.dropout_out", "parsing_gating.BiAffine_Parser_Decoder.dropout_out", "parsing_gating.BiAffine_Parser_Decoder.attention", "parsing_gating.BiAffine_Parser_Decoder.transpose", "parsing_gating.BiAffine_Parser_Decoder.transpose", "parsing_gating.BiAffine_Parser_Decoder.transpose"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input", ",", "mask", ")", ":", "\n", "# apply dropout for output", "\n", "# [batch_size, length, hidden_size] --> [batch_size, hidden_size, length] --> [batch_size, length, hidden_size]", "\n", "        ", "input", "=", "self", ".", "dropout_out", "(", "input", ".", "transpose", "(", "1", ",", "2", ")", ")", ".", "transpose", "(", "1", ",", "2", ")", "\n", "\n", "# output size [batch_size, length, arc_space]", "\n", "arc_h", "=", "F", ".", "elu", "(", "self", ".", "arc_h", "(", "input", ")", ")", "\n", "arc_c", "=", "F", ".", "elu", "(", "self", ".", "arc_c", "(", "input", ")", ")", "\n", "\n", "# output size [batch_size, length, arc_tag_space]", "\n", "arc_tag_h", "=", "F", ".", "elu", "(", "self", ".", "arc_tag_h", "(", "input", ")", ")", "\n", "arc_tag_c", "=", "F", ".", "elu", "(", "self", ".", "arc_tag_c", "(", "input", ")", ")", "\n", "\n", "# apply dropout", "\n", "# [batch_size, length, dim] --> [batch_size, 2 * length, dim]", "\n", "arc", "=", "torch", ".", "cat", "(", "[", "arc_h", ",", "arc_c", "]", ",", "dim", "=", "1", ")", "\n", "arc_tag", "=", "torch", ".", "cat", "(", "[", "arc_tag_h", ",", "arc_tag_c", "]", ",", "dim", "=", "1", ")", "\n", "\n", "arc", "=", "self", ".", "dropout_out", "(", "arc", ".", "transpose", "(", "1", ",", "2", ")", ")", ".", "transpose", "(", "1", ",", "2", ")", "\n", "arc_h", ",", "arc_c", "=", "arc", ".", "chunk", "(", "2", ",", "1", ")", "\n", "arc_tag", "=", "self", ".", "dropout_out", "(", "arc_tag", ".", "transpose", "(", "1", ",", "2", ")", ")", ".", "transpose", "(", "1", ",", "2", ")", "\n", "\n", "# output from rnn [batch_size, length, tag_space]", "\n", "arc_tag_h", ",", "arc_tag_c", "=", "arc_tag", ".", "chunk", "(", "2", ",", "1", ")", "\n", "# head shape [batch_size, length, arc_tag_space]", "\n", "arc_tag_h", "=", "arc_tag_h", ".", "contiguous", "(", ")", "\n", "# child shape [batch_size, length, arc_tag_space]", "\n", "arc_tag_c", "=", "arc_tag_c", ".", "contiguous", "(", ")", "\n", "arc", "=", "(", "arc_h", ",", "arc_c", ")", "\n", "# [batch_size, length, length]", "\n", "out_arc", "=", "self", ".", "attention", "(", "arc", "[", "0", "]", ",", "arc", "[", "1", "]", ",", "mask_d", "=", "mask", ",", "mask_e", "=", "mask", ")", ".", "squeeze", "(", "dim", "=", "1", ")", "\n", "out_arc_tag", "=", "(", "arc_tag_h", ",", "arc_tag_c", ")", "\n", "return", "out_arc", ",", "out_arc_tag", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.models.parsing_gating.BiAffine_Parser_Decoder.loss": [[123, 138], ["parsing_gating.BiAffine_Parser_Decoder.pre_loss", "out_arc.t.t.size", "out_arc.t.t.t", "out_arc_tag.t.t.t", "mask.sum", "float", "out_arc.t.t.sum", "out_arc_tag.t.t.sum"], "methods", ["home.repos.pwc.inspect_result.Jivnesh_LCM.models.parsing_gating.BiAffine_Parser_Decoder.pre_loss", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.size"], ["", "def", "loss", "(", "self", ",", "out_arc", ",", "out_arc_tag", ",", "heads", ",", "arc_tags", ",", "mask", "=", "None", ",", "length", "=", "None", ")", ":", "\n", "        ", "out_arc", ",", "out_arc_tag", "=", "self", ".", "pre_loss", "(", "out_arc", ",", "out_arc_tag", ",", "heads", "=", "heads", ",", "arc_tags", "=", "arc_tags", ",", "mask", "=", "mask", ",", "length", "=", "length", ",", "use_log", "=", "True", ",", "temperature", "=", "1.0", ")", "\n", "batch_size", ",", "max_len", "=", "out_arc", ".", "size", "(", ")", "\n", "# pdb.set_trace()", "\n", "# loss_arc shape [length-1, batch_size]", "\n", "## out_arc.size() = [16,8]", "\n", "out_arc", "=", "out_arc", ".", "t", "(", ")", "\n", "# loss_arc_tag shape [length-1, batch_size]", "\n", "## out_arc_tag.size() = [16,8]", "\n", "out_arc_tag", "=", "out_arc_tag", ".", "t", "(", ")", "\n", "# number of valid positions which contribute to loss (remove the symbolic head for each sentence).", "\n", "## In mask all valid positions are 1's and others are 0.", "\n", "num", "=", "mask", ".", "sum", "(", ")", "-", "batch_size", "if", "mask", "is", "not", "None", "else", "float", "(", "max_len", ")", "*", "batch_size", "\n", "dp_loss", "=", "-", "out_arc", ".", "sum", "(", ")", "/", "num", ",", "-", "out_arc_tag", ".", "sum", "(", ")", "/", "num", "\n", "return", "dp_loss", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.models.parsing_gating.BiAffine_Parser_Decoder.decode": [[139, 145], ["parsing_gating.BiAffine_Parser_Decoder.decode_mst", "parsing_gating.BiAffine_Parser_Decoder.decode_greedy"], "methods", ["home.repos.pwc.inspect_result.Jivnesh_LCM.models.parsing_gating.BiAffine_Parser_Decoder.decode_mst", "home.repos.pwc.inspect_result.Jivnesh_LCM.models.parsing_gating.BiAffine_Parser_Decoder.decode_greedy"], ["", "def", "decode", "(", "self", ",", "out_arc", ",", "out_arc_tag", ",", "mask", ",", "length", ",", "leading_symbolic", ")", ":", "\n", "        ", "if", "self", ".", "arc_decode", "==", "'mst'", ":", "\n", "            ", "heads", ",", "arc_tags", ",", "scores", "=", "self", ".", "decode_mst", "(", "out_arc", ",", "out_arc_tag", ",", "mask", ",", "length", ",", "leading_symbolic", ")", "\n", "", "else", ":", "#self.arc_decode == 'greedy'", "\n", "            ", "heads", ",", "arc_tags", ",", "scores", "=", "self", ".", "decode_greedy", "(", "out_arc", ",", "out_arc_tag", ",", "mask", ",", "leading_symbolic", ")", "\n", "", "return", "heads", ",", "arc_tags", ",", "scores", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.models.parsing_gating.BiAffine_Parser_Decoder.unconstrained_decode_mst": [[147, 179], ["parsing_gating.BiAffine_Parser_Decoder.pre_loss", "loss_arc.size", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp.data.cpu().numpy", "torch.exp.data.cpu().numpy", "torch.exp.data.cpu().numpy", "nn.utils.tasks.parse.decode_MST", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy.size", "torch.from_numpy.size", "torch.from_numpy.size", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "range", "loss_arc.unsqueeze", "range", "mask.data.sum().long().cpu().numpy", "torch.exp.data.cpu", "torch.exp.data.cpu", "torch.exp.data.cpu", "torch.zeros_like.sum", "torch.zeros_like.sum", "torch.zeros_like.sum", "mask.sum", "torch.zeros_like.sum", "torch.zeros_like.sum", "torch.zeros_like.sum", "range", "mask.data.sum().long().cpu", "mask.data.sum().long", "mask.data.sum"], "methods", ["home.repos.pwc.inspect_result.Jivnesh_LCM.models.parsing_gating.BiAffine_Parser_Decoder.pre_loss", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.size", "home.repos.pwc.inspect_result.Jivnesh_LCM.tasks.parse.decode_MST", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.size", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.size", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.size"], ["", "def", "unconstrained_decode_mst", "(", "self", ",", "model_path", ",", "input_word", ",", "input_pos", ",", "input_lemma", ",", "out_arc", ",", "out_arc_tag", ",", "mask", ",", "length", ",", "leading_symbolic", ")", ":", "\n", "        ", "loss_arc", ",", "loss_arc_tag", "=", "self", ".", "pre_loss", "(", "out_arc", ",", "out_arc_tag", ",", "heads", "=", "None", ",", "arc_tags", "=", "None", ",", "mask", "=", "mask", ",", "length", "=", "length", ",", "use_log", "=", "True", ",", "temperature", "=", "1.0", ")", "\n", "batch_size", ",", "max_len", ",", "_", "=", "loss_arc", ".", "size", "(", ")", "\n", "# compute lengths", "\n", "if", "length", "is", "None", ":", "\n", "            ", "if", "mask", "is", "None", ":", "\n", "                ", "length", "=", "[", "max_len", "for", "_", "in", "range", "(", "batch_size", ")", "]", "\n", "", "else", ":", "\n", "                ", "length", "=", "mask", ".", "data", ".", "sum", "(", "dim", "=", "1", ")", ".", "long", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "# energy shape [batch_size, num_arcs, length, length]", "\n", "", "", "raw_energy", "=", "loss_arc", ".", "unsqueeze", "(", "1", ")", "+", "loss_arc_tag", "\n", "# pdb.set_trace()", "\n", "energy", "=", "torch", ".", "exp", "(", "raw_energy", ")", "\n", "# with open('/home/jivnesh/Documents/DCST/energy.npy', 'wb') as f:", "\n", "#     np.save(f, energy.data.cpu().numpy())", "\n", "constrained_energy", "=", "energy", ".", "data", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "#", "\n", "heads", ",", "arc_tags", ",", "=", "parse", ".", "decode_MST", "(", "constrained_energy", ",", "length", ",", "leading_symbolic", "=", "leading_symbolic", ",", "\n", "labeled", "=", "True", ")", "\n", "heads", "=", "from_numpy", "(", "heads", ")", "\n", "arc_tags", "=", "from_numpy", "(", "arc_tags", ")", "\n", "# compute the average score for each tree", "\n", "batch_size", ",", "max_len", "=", "heads", ".", "size", "(", ")", "\n", "scores", "=", "torch", ".", "zeros_like", "(", "heads", ",", "dtype", "=", "energy", ".", "dtype", ",", "device", "=", "energy", ".", "device", ")", "\n", "for", "b_idx", "in", "range", "(", "batch_size", ")", ":", "\n", "            ", "for", "len_idx", "in", "range", "(", "max_len", ")", ":", "\n", "                ", "scores", "[", "b_idx", ",", "len_idx", "]", "=", "energy", "[", "b_idx", ",", "arc_tags", "[", "b_idx", ",", "len_idx", "]", ",", "heads", "[", "b_idx", ",", "len_idx", "]", ",", "len_idx", "]", "\n", "", "", "if", "mask", "is", "not", "None", ":", "\n", "            ", "scores", "=", "scores", ".", "sum", "(", "1", ")", "/", "mask", ".", "sum", "(", "1", ")", "\n", "", "else", ":", "\n", "            ", "scores", "=", "scores", ".", "sum", "(", "1", ")", "/", "max_len", "\n", "", "return", "heads", ",", "arc_tags", ",", "scores", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.models.parsing_gating.BiAffine_Parser_Decoder.decode_mst": [[180, 212], ["parsing_gating.BiAffine_Parser_Decoder.pre_loss", "loss_arc.size", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "nn.utils.tasks.parse.decode_MST", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy.size", "torch.from_numpy.size", "torch.from_numpy.size", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "range", "loss_arc.unsqueeze", "torch.exp.data.cpu().numpy", "torch.exp.data.cpu().numpy", "torch.exp.data.cpu().numpy", "range", "mask.data.sum().long().cpu().numpy", "torch.zeros_like.sum", "torch.zeros_like.sum", "torch.zeros_like.sum", "mask.sum", "torch.zeros_like.sum", "torch.zeros_like.sum", "torch.zeros_like.sum", "torch.exp.data.cpu", "torch.exp.data.cpu", "torch.exp.data.cpu", "range", "mask.data.sum().long().cpu", "mask.data.sum().long", "mask.data.sum"], "methods", ["home.repos.pwc.inspect_result.Jivnesh_LCM.models.parsing_gating.BiAffine_Parser_Decoder.pre_loss", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.size", "home.repos.pwc.inspect_result.Jivnesh_LCM.tasks.parse.decode_MST", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.size", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.size", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.size"], ["", "def", "decode_mst", "(", "self", ",", "out_arc", ",", "out_arc_tag", ",", "mask", ",", "length", ",", "leading_symbolic", ")", ":", "\n", "        ", "loss_arc", ",", "loss_arc_tag", "=", "self", ".", "pre_loss", "(", "out_arc", ",", "out_arc_tag", ",", "heads", "=", "None", ",", "arc_tags", "=", "None", ",", "mask", "=", "mask", ",", "length", "=", "length", ",", "use_log", "=", "True", ",", "temperature", "=", "1.0", ")", "\n", "batch_size", ",", "max_len", ",", "_", "=", "loss_arc", ".", "size", "(", ")", "\n", "# compute lengths", "\n", "if", "length", "is", "None", ":", "\n", "            ", "if", "mask", "is", "None", ":", "\n", "                ", "length", "=", "[", "max_len", "for", "_", "in", "range", "(", "batch_size", ")", "]", "\n", "", "else", ":", "\n", "                ", "length", "=", "mask", ".", "data", ".", "sum", "(", "dim", "=", "1", ")", ".", "long", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "# energy shape [batch_size, num_arcs, length, length]", "\n", "", "", "raw_energy", "=", "loss_arc", ".", "unsqueeze", "(", "1", ")", "+", "loss_arc_tag", "\n", "# pdb.set_trace()", "\n", "energy", "=", "torch", ".", "exp", "(", "raw_energy", ")", "\n", "# pdb.set_trace()", "\n", "\n", "\n", "heads", ",", "arc_tags", "=", "parse", ".", "decode_MST", "(", "energy", ".", "data", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ",", "length", ",", "leading_symbolic", "=", "leading_symbolic", ",", "\n", "labeled", "=", "True", ")", "\n", "heads", "=", "from_numpy", "(", "heads", ")", "\n", "arc_tags", "=", "from_numpy", "(", "arc_tags", ")", "\n", "\n", "# compute the average score for each tree", "\n", "batch_size", ",", "max_len", "=", "heads", ".", "size", "(", ")", "\n", "scores", "=", "torch", ".", "zeros_like", "(", "heads", ",", "dtype", "=", "energy", ".", "dtype", ",", "device", "=", "energy", ".", "device", ")", "\n", "for", "b_idx", "in", "range", "(", "batch_size", ")", ":", "\n", "            ", "for", "len_idx", "in", "range", "(", "max_len", ")", ":", "\n", "                ", "scores", "[", "b_idx", ",", "len_idx", "]", "=", "energy", "[", "b_idx", ",", "arc_tags", "[", "b_idx", ",", "len_idx", "]", ",", "heads", "[", "b_idx", ",", "len_idx", "]", ",", "len_idx", "]", "\n", "", "", "if", "mask", "is", "not", "None", ":", "\n", "            ", "scores", "=", "scores", ".", "sum", "(", "1", ")", "/", "mask", ".", "sum", "(", "1", ")", "\n", "", "else", ":", "\n", "            ", "scores", "=", "scores", ".", "sum", "(", "1", ")", "/", "max_len", "\n", "", "return", "heads", ",", "arc_tags", ",", "scores", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.models.parsing_gating.BiAffine_Parser_Decoder.decode_greedy": [[213, 269], ["out_arc.size", "out_arc.max", "parsing_gating.BiAffine_Parser_Decoder.decode_greedy._decode_arc_tags"], "methods", ["home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.size"], ["", "def", "decode_greedy", "(", "self", ",", "out_arc", ",", "out_arc_tag", ",", "mask", ",", "leading_symbolic", ")", ":", "\n", "        ", "'''\n        Args:\n            out_arc: Tensor\n                the arc scores with shape [batch_size, length, length]\n            out_arc_tag: Tensor\n                the labeled arc scores with shape [batch_size, length, arc_tag_space]\n            mask: Tensor or None\n                the mask tensor with shape = [batch_size, length]\n            length: Tensor or None\n                the length tensor with shape = [batch_size]\n            leading_symbolic: int\n                number of symbolic labels leading in arc_tag alphabets (set it to 0 if you are not sure)\n\n        Returns: (Tensor, Tensor)\n                predicted heads and arc_tags.\n        '''", "\n", "def", "_decode_arc_tags", "(", "out_arc_tag", ",", "heads", ",", "leading_symbolic", ")", ":", "\n", "# out_arc_tag shape [batch_size, length, arc_tag_space]", "\n", "            ", "arc_tag_h", ",", "arc_tag_c", "=", "out_arc_tag", "\n", "batch_size", ",", "max_len", ",", "_", "=", "arc_tag_h", ".", "size", "(", ")", "\n", "# create batch index [batch_size]", "\n", "batch_index", "=", "torch", ".", "arange", "(", "0", ",", "batch_size", ")", ".", "type_as", "(", "arc_tag_h", ".", "data", ")", ".", "long", "(", ")", "\n", "# get vector for heads [batch_size, length, arc_tag_space],", "\n", "arc_tag_h", "=", "arc_tag_h", "[", "batch_index", ",", "heads", ".", "t", "(", ")", "]", ".", "transpose", "(", "0", ",", "1", ")", ".", "contiguous", "(", ")", "\n", "# compute output for arc_tag [batch_size, length, num_arcs]", "\n", "out_arc_tag", "=", "self", ".", "bilinear", "(", "arc_tag_h", ",", "arc_tag_c", ")", "\n", "# remove the first #leading_symbolic arc_tags.", "\n", "out_arc_tag", "=", "out_arc_tag", "[", ":", ",", ":", ",", "leading_symbolic", ":", "]", "\n", "# compute the prediction of arc_tags [batch_size, length]", "\n", "_", ",", "arc_tags", "=", "out_arc_tag", ".", "max", "(", "dim", "=", "2", ")", "\n", "return", "arc_tags", "+", "leading_symbolic", "\n", "\n", "# out_arc shape [batch_size, length, length]", "\n", "", "out_arc", "=", "out_arc", ".", "data", "\n", "_", ",", "max_len", ",", "_", "=", "out_arc", ".", "size", "(", ")", "\n", "# set diagonal elements to -inf", "\n", "out_arc", "=", "out_arc", "+", "torch", ".", "diag", "(", "out_arc", ".", "new", "(", "max_len", ")", ".", "fill_", "(", "-", "np", ".", "inf", ")", ")", "\n", "# set invalid positions to -inf", "\n", "if", "mask", "is", "not", "None", ":", "\n", "# minus_mask = (1 - mask.data).byte().view(batch_size, max_len, 1)", "\n", "            ", "minus_mask", "=", "(", "1", "-", "mask", ".", "data", ")", ".", "byte", "(", ")", ".", "unsqueeze", "(", "2", ")", "\n", "out_arc", ".", "masked_fill_", "(", "minus_mask", ",", "-", "np", ".", "inf", ")", "\n", "\n", "# compute naive predictions.", "\n", "# predition shape = [batch_size, length]", "\n", "", "scores", ",", "heads", "=", "out_arc", ".", "max", "(", "dim", "=", "1", ")", "\n", "\n", "arc_tags", "=", "_decode_arc_tags", "(", "out_arc_tag", ",", "heads", ",", "leading_symbolic", ")", "\n", "\n", "# compute the average score for each tree", "\n", "if", "mask", "is", "not", "None", ":", "\n", "            ", "scores", "=", "scores", ".", "sum", "(", "1", ")", "/", "mask", ".", "sum", "(", "1", ")", "\n", "", "else", ":", "\n", "            ", "scores", "=", "scores", ".", "sum", "(", "1", ")", "/", "max_len", "\n", "", "return", "heads", ",", "arc_tags", ",", "scores", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.models.parsing_gating.BiAffine_Parser_Decoder.pre_loss": [[270, 334], ["arc_tag_h.unsqueeze().expand().contiguous.unsqueeze().expand().contiguous.size", "parsing_gating.BiAffine_Parser_Decoder.bilinear", "ValueError", "torch.arange().type_as().long", "torch.arange().type_as().long", "torch.arange().type_as().long", "torch.arange().type_as().long", "torch.arange().type_as().long", "torch.arange().type_as().long", "torch.arange().type_as().long", "torch.arange().type_as().long", "torch.arange().type_as().long", "arc_tag_h[].transpose().contiguous", "arc_tag_h.unsqueeze().expand().contiguous.unsqueeze().expand().contiguous.unsqueeze().expand().contiguous", "arc_tag_c.unsqueeze().expand().contiguous.unsqueeze().expand().contiguous.unsqueeze().expand().contiguous", "softmax_func", "softmax_func", "torch.arange().view().expand", "torch.arange().view().expand", "torch.arange().view().expand", "torch.arange().view().expand", "torch.arange().view().expand", "torch.arange().view().expand", "torch.arange().view().expand", "torch.arange().view().expand", "torch.arange().view().expand", "child_index.type_as().long.type_as().long.type_as().long", "[].t", "[].t", "softmax_func", "softmax_func().permute", "minus_mask.unsqueeze", "heads.size", "torch.arange().type_as", "torch.arange().type_as", "torch.arange().type_as", "torch.arange().type_as", "torch.arange().type_as", "torch.arange().type_as", "torch.arange().type_as", "torch.arange().type_as", "torch.arange().type_as", "arc_tag_h[].transpose", "arc_tag_h.unsqueeze().expand().contiguous.unsqueeze().expand().contiguous.unsqueeze().expand", "arc_tag_c.unsqueeze().expand().contiguous.unsqueeze().expand().contiguous.unsqueeze().expand", "minus_mask.unsqueeze", "mask.unsqueeze", "mask.unsqueeze", "torch.arange().view", "torch.arange().view", "torch.arange().view", "torch.arange().view", "torch.arange().view", "torch.arange().view", "torch.arange().view", "torch.arange().view", "torch.arange().view", "child_index.type_as().long.type_as().long.type_as", "softmax_func", "mask.unsqueeze", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "arc_tag_h.unsqueeze().expand().contiguous.unsqueeze().expand().contiguous.unsqueeze", "arc_tag_c.unsqueeze().expand().contiguous.unsqueeze().expand().contiguous.unsqueeze", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "heads.data.t", "arc_tags.data.t", "heads.data.t"], "methods", ["home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.size", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.size"], ["", "def", "pre_loss", "(", "self", ",", "out_arc", ",", "out_arc_tag", ",", "heads", "=", "None", ",", "arc_tags", "=", "None", ",", "mask", "=", "None", ",", "length", "=", "None", ",", "use_log", "=", "True", ",", "temperature", "=", "1.0", ")", ":", "\n", "        ", "if", "(", "heads", "is", "not", "None", "and", "arc_tags", "is", "None", ")", "or", "(", "heads", "is", "None", "and", "arc_tags", "is", "not", "None", ")", ":", "\n", "            ", "raise", "ValueError", "(", "'heads and arc_tags should be both Nones or both not Nones'", ")", "\n", "", "decode", "=", "True", "if", "(", "heads", "is", "None", "and", "arc_tags", "is", "None", ")", "else", "False", "\n", "softmax_func", "=", "F", ".", "log_softmax", "if", "use_log", "else", "F", ".", "softmax", "\n", "# out_arc shape [batch_size, length, length]", "\n", "# out_arc_tag shape [batch_size, length, arc_tag_space]", "\n", "## Out_arc_tag [16,9,128]", "\n", "arc_tag_h", ",", "arc_tag_c", "=", "out_arc_tag", "\n", "# pdb.set_trace()", "\n", "# pdb.set_trace()", "\n", "batch_size", ",", "max_len", ",", "arc_tag_space", "=", "arc_tag_h", ".", "size", "(", ")", "\n", "batch_index", "=", "None", "\n", "if", "not", "decode", ":", "\n", "            ", "if", "length", "is", "not", "None", "and", "heads", ".", "size", "(", "1", ")", "!=", "max_len", ":", "\n", "                ", "heads", "=", "heads", "[", ":", ",", ":", "max_len", "]", "\n", "arc_tags", "=", "arc_tags", "[", ":", ",", ":", "max_len", "]", "\n", "# create batch index [batch_size]", "\n", "", "batch_index", "=", "torch", ".", "arange", "(", "0", ",", "batch_size", ")", ".", "type_as", "(", "out_arc", ".", "data", ")", ".", "long", "(", ")", "\n", "# get vector for heads [batch_size, length, arc_tag_space],", "\n", "arc_tag_h", "=", "arc_tag_h", "[", "batch_index", ",", "heads", ".", "data", ".", "t", "(", ")", "]", ".", "transpose", "(", "0", ",", "1", ")", ".", "contiguous", "(", ")", "\n", "", "else", ":", "\n", "            ", "arc_tag_h", "=", "arc_tag_h", ".", "unsqueeze", "(", "2", ")", ".", "expand", "(", "batch_size", ",", "max_len", ",", "max_len", ",", "arc_tag_space", ")", ".", "contiguous", "(", ")", "\n", "arc_tag_c", "=", "arc_tag_c", ".", "unsqueeze", "(", "1", ")", ".", "expand", "(", "batch_size", ",", "max_len", ",", "max_len", ",", "arc_tag_space", ")", ".", "contiguous", "(", ")", "\n", "\n", "# compute output for arc_tag [batch_size, length, num_arcs]", "\n", "## out_arc_tag.size() [16,9,27]", "\n", "## We used gold head to calculate these scores", "\n", "", "out_arc_tag", "=", "self", ".", "bilinear", "(", "arc_tag_h", ",", "arc_tag_c", ")", "\n", "\n", "# mask invalid position to -inf for softmax_func", "\n", "if", "mask", "is", "not", "None", ":", "\n", "            ", "minus_inf", "=", "-", "1e8", "\n", "minus_mask", "=", "(", "1", "-", "mask", ")", "*", "minus_inf", "\n", "## Out_arc = [16,9,9]", "\n", "## Removed the right and bottom invalid columns and rows from out_arc", "\n", "out_arc", "=", "out_arc", "+", "minus_mask", ".", "unsqueeze", "(", "2", ")", "+", "minus_mask", ".", "unsqueeze", "(", "1", ")", "\n", "\n", "", "if", "not", "decode", ":", "\n", "# loss_arc shape [batch_size, length, length]", "\n", "## Please note that this is log softmax function", "\n", "            ", "out_arc", "=", "softmax_func", "(", "out_arc", "/", "temperature", ",", "dim", "=", "1", ")", "\n", "# loss_arc_tag shape [batch_size, length, num_arcs]", "\n", "out_arc_tag", "=", "softmax_func", "(", "out_arc_tag", "/", "temperature", ",", "dim", "=", "2", ")", "\n", "# mask invalid position to 0 for sum loss", "\n", "if", "mask", "is", "not", "None", ":", "\n", "                ", "out_arc", "=", "out_arc", "*", "mask", ".", "unsqueeze", "(", "2", ")", "*", "mask", ".", "unsqueeze", "(", "1", ")", "\n", "out_arc_tag", "=", "out_arc_tag", "*", "mask", ".", "unsqueeze", "(", "2", ")", "\n", "\n", "# first create index matrix [length, batch_size]", "\n", "", "child_index", "=", "torch", ".", "arange", "(", "0", ",", "max_len", ")", ".", "view", "(", "max_len", ",", "1", ")", ".", "expand", "(", "max_len", ",", "batch_size", ")", "\n", "child_index", "=", "child_index", ".", "type_as", "(", "out_arc", ".", "data", ")", ".", "long", "(", ")", "\n", "# loss_arc shape [batch_size, length-1]", "\n", "## This is the position we need to integrate constraints during training.", "\n", "\n", "out_arc", "=", "out_arc", "[", "batch_index", ",", "heads", ".", "data", ".", "t", "(", ")", ",", "child_index", "]", "[", "1", ":", "]", ".", "t", "(", ")", "\n", "# loss_arc_tag shape [batch_size, length-1]", "\n", "out_arc_tag", "=", "out_arc_tag", "[", "batch_index", ",", "child_index", ",", "arc_tags", ".", "data", ".", "t", "(", ")", "]", "[", "1", ":", "]", ".", "t", "(", ")", "\n", "", "else", ":", "\n", "# loss_arc shape [batch_size, length, length]", "\n", "            ", "out_arc", "=", "softmax_func", "(", "out_arc", "/", "temperature", ",", "dim", "=", "1", ")", "\n", "# loss_arc_tag shape [batch_size, length, length, num_arcs]", "\n", "out_arc_tag", "=", "softmax_func", "(", "out_arc_tag", "/", "temperature", ",", "dim", "=", "3", ")", ".", "permute", "(", "0", ",", "3", ",", "1", ",", "2", ")", "\n", "", "return", "out_arc", ",", "out_arc_tag", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.models.parsing_gating.BiRecurrentConv_Encoder.__init__": [[336, 374], ["torch.Module.__init__", "torch.Embedding", "torch.Dropout2d", "torch.Dropout2d", "torch.Dropout2d", "torch.Dropout2d", "torch.Dropout2d", "torch.Dropout2d", "torch.Dropout", "torch.Dropout", "torch.Dropout", "RNN", "parsing_gating.BiRecurrentConv_Encoder.reset_parameters", "torch.Embedding", "torch.Embedding", "torch.Conv1d", "torch.Conv1d", "torch.Conv1d", "ValueError"], "methods", ["home.repos.pwc.inspect_result.Jivnesh_LCM.io_.reader.Reader.__init__", "home.repos.pwc.inspect_result.Jivnesh_LCM.models.parsing_gating.BiRecurrentConv_Encoder.reset_parameters"], ["    ", "def", "__init__", "(", "self", ",", "word_dim", ",", "num_words", ",", "char_dim", ",", "num_chars", ",", "use_pos", ",", "use_char", ",", "pos_dim", ",", "num_pos", ",", "num_filters", ",", "\n", "kernel_size", ",", "rnn_mode", ",", "hidden_size", ",", "num_layers", ",", "embedd_word", "=", "None", ",", "embedd_char", "=", "None", ",", "embedd_pos", "=", "None", ",", "\n", "p_in", "=", "0.33", ",", "p_out", "=", "0.33", ",", "p_rnn", "=", "(", "0.33", ",", "0.33", ")", ",", "initializer", "=", "None", ")", ":", "\n", "        ", "super", "(", "BiRecurrentConv_Encoder", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "word_embedd", "=", "Embedding", "(", "num_words", ",", "word_dim", ",", "init_embedding", "=", "embedd_word", ")", "\n", "self", ".", "char_embedd", "=", "Embedding", "(", "num_chars", ",", "char_dim", ",", "init_embedding", "=", "embedd_char", ")", "if", "use_char", "else", "None", "\n", "self", ".", "pos_embedd", "=", "Embedding", "(", "num_pos", ",", "pos_dim", ",", "init_embedding", "=", "embedd_pos", ")", "if", "use_pos", "else", "None", "\n", "self", ".", "conv1d", "=", "nn", ".", "Conv1d", "(", "char_dim", ",", "num_filters", ",", "kernel_size", ",", "padding", "=", "kernel_size", "-", "1", ")", "if", "use_char", "else", "None", "\n", "# self.OOV_layer = nn.Linear(word_dim, word_dim)", "\n", "# dropout word", "\n", "self", ".", "dropout_in", "=", "nn", ".", "Dropout2d", "(", "p_in", ")", "\n", "# standard dropout", "\n", "self", ".", "dropout_out", "=", "nn", ".", "Dropout2d", "(", "p_out", ")", "\n", "self", ".", "dropout_rnn_in", "=", "nn", ".", "Dropout", "(", "p_rnn", "[", "0", "]", ")", "\n", "self", ".", "use_pos", "=", "use_pos", "\n", "self", ".", "use_char", "=", "use_char", "\n", "self", ".", "rnn_mode", "=", "rnn_mode", "\n", "self", ".", "dim_enc", "=", "word_dim", "\n", "if", "use_pos", ":", "\n", "            ", "self", ".", "dim_enc", "+=", "pos_dim", "\n", "", "if", "use_char", ":", "\n", "            ", "self", ".", "dim_enc", "+=", "num_filters", "\n", "\n", "", "if", "rnn_mode", "==", "'RNN'", ":", "\n", "            ", "RNN", "=", "nn", ".", "RNN", "\n", "drop_p_rnn", "=", "p_rnn", "[", "1", "]", "\n", "", "elif", "rnn_mode", "==", "'LSTM'", ":", "\n", "            ", "RNN", "=", "nn", ".", "LSTM", "\n", "drop_p_rnn", "=", "p_rnn", "[", "1", "]", "\n", "", "elif", "rnn_mode", "==", "'GRU'", ":", "\n", "            ", "RNN", "=", "nn", ".", "GRU", "\n", "drop_p_rnn", "=", "p_rnn", "[", "1", "]", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "'Unknown RNN mode: %s'", "%", "rnn_mode", ")", "\n", "", "self", ".", "rnn", "=", "RNN", "(", "self", ".", "dim_enc", ",", "hidden_size", ",", "num_layers", "=", "num_layers", ",", "batch_first", "=", "True", ",", "bidirectional", "=", "True", ",", "\n", "dropout", "=", "drop_p_rnn", ")", "\n", "self", ".", "initializer", "=", "initializer", "\n", "self", ".", "reset_parameters", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.models.parsing_gating.BiRecurrentConv_Encoder.reset_parameters": [[375, 385], ["parsing_gating.BiRecurrentConv_Encoder.named_parameters", "name.find", "parameter.dim", "parameter.data.zero_", "parsing_gating.BiRecurrentConv_Encoder.initializer"], "methods", ["None"], ["", "def", "reset_parameters", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "initializer", "is", "None", ":", "\n", "            ", "return", "\n", "\n", "", "for", "name", ",", "parameter", "in", "self", ".", "named_parameters", "(", ")", ":", "\n", "            ", "if", "name", ".", "find", "(", "'embedd'", ")", "==", "-", "1", ":", "\n", "                ", "if", "parameter", ".", "dim", "(", ")", "==", "1", ":", "\n", "                    ", "parameter", ".", "data", ".", "zero_", "(", ")", "\n", "", "else", ":", "\n", "                    ", "self", ".", "initializer", "(", "parameter", ".", "data", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.models.parsing_gating.BiRecurrentConv_Encoder.forward": [[386, 441], ["parsing_gating.BiRecurrentConv_Encoder.word_embedd", "parsing_gating.BiRecurrentConv_Encoder.dropout_in", "parsing_gating.BiRecurrentConv_Encoder.dropout_rnn_in", "parsing_gating.BiRecurrentConv_Encoder.dropout_out", "mask.data.sum().long", "parsing_gating.BiRecurrentConv_Encoder.char_embedd", "parsing_gating.BiRecurrentConv_Encoder.size", "parsing_gating.BiRecurrentConv_Encoder.view().transpose", "parsing_gating.BiRecurrentConv_Encoder.conv1d().max", "torch.tanh().view", "torch.tanh().view", "torch.tanh().view", "torch.tanh().view", "torch.tanh().view", "torch.tanh().view", "torch.tanh().view", "torch.tanh().view", "torch.tanh().view", "parsing_gating.BiRecurrentConv_Encoder.dropout_in", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "parsing_gating.BiRecurrentConv_Encoder.pos_embedd", "parsing_gating.BiRecurrentConv_Encoder.dropout_in", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.utils.prepare_rnn_seq", "parsing_gating.BiRecurrentConv_Encoder.rnn.flatten_parameters", "parsing_gating.BiRecurrentConv_Encoder.rnn", "torch.utils.recover_rnn_seq", "parsing_gating.BiRecurrentConv_Encoder.rnn.flatten_parameters", "parsing_gating.BiRecurrentConv_Encoder.rnn", "mask.data.sum", "parsing_gating.BiRecurrentConv_Encoder.view", "parsing_gating.BiRecurrentConv_Encoder.conv1d", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh"], "methods", ["home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.size", "home.repos.pwc.inspect_result.Jivnesh_LCM.nn.utils.prepare_rnn_seq", "home.repos.pwc.inspect_result.Jivnesh_LCM.nn.utils.recover_rnn_seq"], ["", "", "", "", "def", "forward", "(", "self", ",", "input_word", ",", "input_char", ",", "input_pos", ",", "mask", "=", "None", ",", "length", "=", "None", ",", "hx", "=", "None", ")", ":", "\n", "# hack length from mask", "\n", "# we do not hack mask from length for special reasons.", "\n", "# Thus, always provide mask if it is necessary.", "\n", "        ", "if", "length", "is", "None", "and", "mask", "is", "not", "None", ":", "\n", "            ", "length", "=", "mask", ".", "data", ".", "sum", "(", "dim", "=", "1", ")", ".", "long", "(", ")", "\n", "\n", "# [batch_size, length, word_dim]", "\n", "", "word", "=", "self", ".", "word_embedd", "(", "input_word", ")", "\n", "# apply dropout on input", "\n", "word", "=", "self", ".", "dropout_in", "(", "word", ")", "\n", "###########################################", "\n", "## To handle very less overlap of training and testing dataset", "\n", "# word = self.OOV_layer(word)", "\n", "\n", "input", "=", "word", "\n", "if", "self", ".", "use_char", ":", "\n", "# [batch_size, length, char_length, char_dim]", "\n", "            ", "char", "=", "self", ".", "char_embedd", "(", "input_char", ")", "\n", "char_size", "=", "char", ".", "size", "(", ")", "\n", "# first transform to [batch *length, char_length, char_dim]", "\n", "# then transpose to [batch * length, char_dim, char_length]", "\n", "char", "=", "char", ".", "view", "(", "char_size", "[", "0", "]", "*", "char_size", "[", "1", "]", ",", "char_size", "[", "2", "]", ",", "char_size", "[", "3", "]", ")", ".", "transpose", "(", "1", ",", "2", ")", "\n", "# put into cnn [batch*length, char_filters, char_length]", "\n", "# then put into maxpooling [batch * length, char_filters]", "\n", "char", ",", "_", "=", "self", ".", "conv1d", "(", "char", ")", ".", "max", "(", "dim", "=", "2", ")", "\n", "# reshape to [batch_size, length, char_filters]", "\n", "char", "=", "torch", ".", "tanh", "(", "char", ")", ".", "view", "(", "char_size", "[", "0", "]", ",", "char_size", "[", "1", "]", ",", "-", "1", ")", "\n", "# apply dropout on input", "\n", "char", "=", "self", ".", "dropout_in", "(", "char", ")", "\n", "# concatenate word and char [batch_size, length, word_dim+char_filter]", "\n", "input", "=", "torch", ".", "cat", "(", "[", "input", ",", "char", "]", ",", "dim", "=", "2", ")", "\n", "\n", "", "if", "self", ".", "use_pos", ":", "\n", "# [batch_size, length, pos_dim]", "\n", "            ", "pos", "=", "self", ".", "pos_embedd", "(", "input_pos", ")", "\n", "# apply dropout on input", "\n", "pos", "=", "self", ".", "dropout_in", "(", "pos", ")", "\n", "input", "=", "torch", ".", "cat", "(", "[", "input", ",", "pos", "]", ",", "dim", "=", "2", ")", "\n", "\n", "# apply dropout rnn input", "\n", "", "input", "=", "self", ".", "dropout_rnn_in", "(", "input", ")", "\n", "# prepare packed_sequence", "\n", "if", "length", "is", "not", "None", ":", "\n", "            ", "seq_input", ",", "hx", ",", "rev_order", ",", "mask", "=", "utils", ".", "prepare_rnn_seq", "(", "input", ",", "length", ",", "hx", "=", "hx", ",", "masks", "=", "mask", ",", "batch_first", "=", "True", ")", "\n", "self", ".", "rnn", ".", "flatten_parameters", "(", ")", "\n", "seq_output", ",", "hn", "=", "self", ".", "rnn", "(", "seq_input", ",", "hx", "=", "hx", ")", "\n", "output", ",", "hn", "=", "utils", ".", "recover_rnn_seq", "(", "seq_output", ",", "rev_order", ",", "hx", "=", "hn", ",", "batch_first", "=", "True", ")", "\n", "", "else", ":", "\n", "# output from rnn [batch_size, length, hidden_size]", "\n", "            ", "self", ".", "rnn", ".", "flatten_parameters", "(", ")", "\n", "output", ",", "hn", "=", "self", ".", "rnn", "(", "input", ",", "hx", "=", "hx", ")", "\n", "# apply dropout for the output of rnn", "\n", "", "output", "=", "self", ".", "dropout_out", "(", "output", ")", "\n", "return", "output", ",", "hn", ",", "mask", ",", "length", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.models.parsing_gating.Gating.__init__": [[447, 458], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Softmax", "torch.Softmax", "torch.Softmax", "ValueError"], "methods", ["home.repos.pwc.inspect_result.Jivnesh_LCM.io_.reader.Reader.__init__"], ["    ", "def", "__init__", "(", "self", ",", "num_gates", ",", "input_dim", ")", ":", "\n", "        ", "super", "(", "Gating", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "num_gates", "=", "num_gates", "\n", "self", ".", "input_dim", "=", "input_dim", "\n", "if", "self", ".", "num_gates", "==", "2", ":", "\n", "            ", "self", ".", "linear", "=", "nn", ".", "Linear", "(", "self", ".", "num_gates", "*", "self", ".", "input_dim", ",", "self", ".", "input_dim", ")", "\n", "", "elif", "self", ".", "num_gates", ">", "2", ":", "\n", "            ", "self", ".", "linear", "=", "nn", ".", "Linear", "(", "self", ".", "num_gates", "*", "self", ".", "input_dim", ",", "self", ".", "num_gates", "*", "self", ".", "input_dim", ")", "\n", "self", ".", "softmax", "=", "nn", ".", "Softmax", "(", "-", "1", ")", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "'num_gates should be greater or equal to 2'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.models.parsing_gating.Gating.forward": [[459, 471], ["torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "tuple_of_inputs[].size", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "parsing_gating.Gating.softmax", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "parsing_gating.Gating.linear", "torch.mul", "torch.mul", "torch.mul", "torch.mul", "torch.mul", "torch.mul", "torch.mul", "torch.mul", "torch.mul", "torch.mul", "torch.mul", "torch.mul", "torch.mul", "torch.mul", "torch.mul", "torch.mul", "torch.mul", "torch.mul", "parsing_gating.Gating.linear", "parsing_gating.Gating.view", "torch.mul", "torch.mul", "torch.mul", "torch.mul", "torch.mul", "torch.mul", "torch.mul", "torch.mul", "torch.mul", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack"], "methods", ["home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.size"], ["", "", "def", "forward", "(", "self", ",", "tuple_of_inputs", ")", ":", "\n", "# output size should be equal to the input sizes", "\n", "        ", "if", "self", ".", "num_gates", "==", "2", ":", "\n", "            ", "alpha", "=", "torch", ".", "sigmoid", "(", "self", ".", "linear", "(", "torch", ".", "cat", "(", "tuple_of_inputs", ",", "dim", "=", "-", "1", ")", ")", ")", "\n", "output", "=", "torch", ".", "mul", "(", "alpha", ",", "tuple_of_inputs", "[", "0", "]", ")", "+", "torch", ".", "mul", "(", "1", "-", "alpha", ",", "tuple_of_inputs", "[", "1", "]", ")", "\n", "", "else", ":", "# elif self.num_gates > 2:", "\n", "# extend the gating mechanism to more than 2 encoders", "\n", "            ", "batch_size", ",", "len_size", ",", "dim_size", "=", "tuple_of_inputs", "[", "0", "]", ".", "size", "(", ")", "\n", "alpha", "=", "torch", ".", "sigmoid", "(", "self", ".", "linear", "(", "torch", ".", "cat", "(", "tuple_of_inputs", ",", "dim", "=", "-", "1", ")", ")", ")", "\n", "alpha", "=", "self", ".", "softmax", "(", "alpha", ".", "view", "(", "batch_size", ",", "len_size", ",", "dim_size", ",", "self", ".", "num_gates", ")", ")", "\n", "output", "=", "torch", ".", "sum", "(", "torch", ".", "mul", "(", "alpha", ",", "torch", ".", "stack", "(", "tuple_of_inputs", ",", "dim", "=", "-", "1", ")", ")", ",", "dim", "=", "-", "1", ")", "\n", "", "return", "output", "", "", "", ""]], "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.convert_ud_to_onto_format.write_ud_files": [[6, 64], ["sorted", "dict", "dict.items", "os.listdir", "dict.items", "list", "os.path.join", "os.listdir", "split_dict.items", "set", "print", "print", "print", "os.path.exists", "os.makedirs", "open", "file.startswith", "file.endswith", "len", "open", "f.write", "line.strip.strip", "line.strip.split", "sentences.append", "os.path.join", "[].append", "len", "sentences.append", "tokens[].isdigit"], "function", ["home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.items", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.items", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.items", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.open", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.open", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.writer.Writer.write"], ["def", "write_ud_files", "(", "args", ")", ":", "\n", "    ", "languages_for_low_resource", "=", "[", "'el'", "]", "\n", "\n", "languages", "=", "sorted", "(", "list", "(", "set", "(", "languages_for_low_resource", ")", ")", ")", "\n", "splits", "=", "[", "'train'", ",", "'dev'", ",", "'test'", "]", "\n", "lng_to_files", "=", "dict", "(", "(", "language", ",", "{", "}", ")", "for", "language", "in", "languages", ")", "\n", "for", "language", ",", "d", "in", "lng_to_files", ".", "items", "(", ")", ":", "\n", "        ", "for", "split", "in", "splits", ":", "\n", "            ", "d", "[", "split", "]", "=", "[", "]", "\n", "", "lng_to_files", "[", "language", "]", "=", "d", "\n", "", "sub_folders", "=", "os", ".", "listdir", "(", "args", ".", "ud_data_path", ")", "\n", "for", "sub_folder", "in", "sub_folders", ":", "\n", "        ", "folder", "=", "os", ".", "path", ".", "join", "(", "args", ".", "ud_data_path", ",", "sub_folder", ")", "\n", "files", "=", "os", ".", "listdir", "(", "folder", ")", "\n", "for", "file", "in", "files", ":", "\n", "            ", "for", "language", "in", "languages", ":", "\n", "                ", "if", "file", ".", "startswith", "(", "language", ")", "and", "file", ".", "endswith", "(", "'conllu'", ")", ":", "\n", "                    ", "for", "split", "in", "splits", ":", "\n", "                        ", "if", "split", "in", "file", ":", "\n", "                            ", "full_path", "=", "os", ".", "path", ".", "join", "(", "folder", ",", "file", ")", "\n", "lng_to_files", "[", "language", "]", "[", "split", "]", ".", "append", "(", "full_path", ")", "\n", "break", "\n", "\n", "", "", "", "", "", "", "for", "language", ",", "split_dict", "in", "lng_to_files", ".", "items", "(", ")", ":", "\n", "        ", "for", "split", ",", "files", "in", "split_dict", ".", "items", "(", ")", ":", "\n", "            ", "if", "split", "==", "'dev'", "and", "len", "(", "files", ")", "==", "0", ":", "\n", "                ", "files", "=", "split_dict", "[", "'train'", "]", "\n", "print", "(", "'No dev files were found, copying train files instead'", ")", "\n", "", "sentences", "=", "[", "]", "\n", "num_sentences", "=", "0", "\n", "for", "file", "in", "files", ":", "\n", "                ", "with", "open", "(", "file", ",", "'r'", ")", "as", "file", ":", "\n", "                    ", "for", "line", "in", "file", ":", "\n", "                        ", "new_line", "=", "[", "]", "\n", "line", "=", "line", ".", "strip", "(", ")", "\n", "if", "len", "(", "line", ")", "==", "0", ":", "\n", "                            ", "sentences", ".", "append", "(", "new_line", ")", "\n", "num_sentences", "+=", "1", "\n", "continue", "\n", "", "tokens", "=", "line", ".", "split", "(", "'\\t'", ")", "\n", "if", "not", "tokens", "[", "0", "]", ".", "isdigit", "(", ")", ":", "\n", "                            ", "continue", "\n", "", "id", "=", "tokens", "[", "0", "]", "\n", "word", "=", "tokens", "[", "1", "]", "\n", "pos", "=", "tokens", "[", "3", "]", "\n", "ner", "=", "tokens", "[", "5", "]", "\n", "head", "=", "tokens", "[", "6", "]", "\n", "arc_tag", "=", "tokens", "[", "7", "]", "\n", "new_line", "=", "[", "id", ",", "word", ",", "pos", ",", "ner", ",", "head", ",", "arc_tag", "]", "\n", "sentences", ".", "append", "(", "new_line", ")", "\n", "", "", "", "print", "(", "'Language: %s Split: %s Num. Sentences: %s '", "%", "(", "language", ",", "split", ",", "num_sentences", ")", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "'data'", ")", ":", "\n", "                ", "os", ".", "makedirs", "(", "'data'", ")", "\n", "", "write_data_path", "=", "'data/MRL/ud_pos_ner_dp_'", "+", "split", "+", "'_'", "+", "language", "\n", "print", "(", "'creating %s'", "%", "write_data_path", ")", "\n", "with", "open", "(", "write_data_path", ",", "'w'", ")", "as", "f", ":", "\n", "                ", "for", "line", "in", "sentences", ":", "\n", "                    ", "f", ".", "write", "(", "'\\t'", ".", "join", "(", "line", ")", "+", "'\\n'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.convert_ud_to_onto_format.main": [[65, 72], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "convert_ud_to_onto_format.write_ud_files"], "function", ["home.repos.pwc.inspect_result.Jivnesh_LCM.io_.convert_ud_to_onto_format.write_ud_files"], ["", "", "", "", "", "def", "main", "(", ")", ":", "\n", "# Parse arguments", "\n", "    ", "args_", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "args_", ".", "add_argument", "(", "'--ud_data_path'", ",", "help", "=", "'Directory path of the UD treebanks.'", ",", "required", "=", "True", ")", "\n", "\n", "args", "=", "args_", ".", "parse_args", "(", ")", "\n", "write_ud_files", "(", "args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.writer.Writer.__init__": [[3, 6], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "alphabets", ")", ":", "\n", "        ", "self", ".", "__source_file", "=", "None", "\n", "self", ".", "alphabets", "=", "alphabets", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.writer.Writer.start": [[7, 9], ["open"], "methods", ["home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.open"], ["", "def", "start", "(", "self", ",", "file_path", ")", ":", "\n", "        ", "self", ".", "__source_file", "=", "open", "(", "file_path", ",", "'w'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.writer.Writer.close": [[10, 12], ["writer.Writer.__source_file.close"], "methods", ["home.repos.pwc.inspect_result.Jivnesh_LCM.io_.reader.Reader.close"], ["", "def", "close", "(", "self", ")", ":", "\n", "        ", "self", ".", "__source_file", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.writer.Writer.write": [[13, 30], ["range", "range", "writer.Writer.__source_file.write", "writer.Writer.alphabets[].get_instance", "writer.Writer.alphabets[].get_instance", "writer.Writer.alphabets[].get_instance", "writer.Writer.alphabets[].get_instance", "writer.Writer.alphabets[].get_instance", "writer.Writer.__source_file.write", "writer.Writer.__source_file.write"], "methods", ["home.repos.pwc.inspect_result.Jivnesh_LCM.io_.writer.Writer.write", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.get_instance", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.get_instance", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.get_instance", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.get_instance", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.get_instance", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.writer.Writer.write", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.writer.Writer.write"], ["", "def", "write", "(", "self", ",", "word", ",", "pos", ",", "ner", ",", "head", ",", "arc", ",", "lengths", ",", "auto_label", "=", "None", ",", "symbolic_root", "=", "False", ",", "symbolic_end", "=", "False", ")", ":", "\n", "        ", "batch_size", ",", "_", "=", "word", ".", "shape", "\n", "start", "=", "1", "if", "symbolic_root", "else", "0", "\n", "end", "=", "1", "if", "symbolic_end", "else", "0", "\n", "for", "i", "in", "range", "(", "batch_size", ")", ":", "\n", "            ", "for", "j", "in", "range", "(", "start", ",", "lengths", "[", "i", "]", "-", "end", ")", ":", "\n", "                ", "w", "=", "self", ".", "alphabets", "[", "'word_alphabet'", "]", ".", "get_instance", "(", "word", "[", "i", ",", "j", "]", ")", "\n", "p", "=", "self", ".", "alphabets", "[", "'pos_alphabet'", "]", ".", "get_instance", "(", "pos", "[", "i", ",", "j", "]", ")", "\n", "n", "=", "self", ".", "alphabets", "[", "'ner_alphabet'", "]", ".", "get_instance", "(", "ner", "[", "i", ",", "j", "]", ")", "\n", "t", "=", "self", ".", "alphabets", "[", "'arc_alphabet'", "]", ".", "get_instance", "(", "arc", "[", "i", ",", "j", "]", ")", "\n", "h", "=", "head", "[", "i", ",", "j", "]", "\n", "if", "auto_label", "is", "not", "None", ":", "\n", "                    ", "m", "=", "self", ".", "alphabets", "[", "'auto_label_alphabet'", "]", ".", "get_instance", "(", "auto_label", "[", "i", ",", "j", "]", ")", "\n", "self", ".", "__source_file", ".", "write", "(", "'%d\\t%s\\t%s\\t%s\\t%d\\t%s\\t%s\\n'", "%", "(", "j", ",", "w", ",", "p", ",", "n", ",", "h", ",", "t", ",", "m", ")", ")", "\n", "", "else", ":", "\n", "                    ", "self", ".", "__source_file", ".", "write", "(", "'%d\\t%s\\t%s\\t%s\\t%d\\t%s\\n'", "%", "(", "j", ",", "w", ",", "p", ",", "n", ",", "h", ",", "t", ")", ")", "\n", "", "", "self", ".", "__source_file", ".", "write", "(", "'\\n'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.writer.Index2Instance.__init__": [[32, 34], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "alphabet", ")", ":", "\n", "        ", "self", ".", "__alphabet", "=", "alphabet", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.writer.Index2Instance.index2instance": [[35, 47], ["range", "range", "instnaces.append", "writer.Index2Instance.__alphabet.get_instance", "tmp_instances.append"], "methods", ["home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.get_instance"], ["", "def", "index2instance", "(", "self", ",", "indices", ",", "lengths", ",", "symbolic_root", "=", "False", ",", "symbolic_end", "=", "False", ")", ":", "\n", "        ", "batch_size", ",", "_", "=", "indices", ".", "shape", "\n", "start", "=", "1", "if", "symbolic_root", "else", "0", "\n", "end", "=", "1", "if", "symbolic_end", "else", "0", "\n", "instnaces", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "batch_size", ")", ":", "\n", "            ", "tmp_instances", "=", "[", "]", "\n", "for", "j", "in", "range", "(", "start", ",", "lengths", "[", "i", "]", "-", "end", ")", ":", "\n", "                ", "instamce", "=", "self", ".", "__alphabet", ".", "get_instance", "(", "indices", "[", "i", ",", "j", "]", ")", "\n", "tmp_instances", ".", "append", "(", "instamce", ")", "\n", "", "instnaces", ".", "append", "(", "tmp_instances", ")", "\n", "", "return", "instnaces", "", "", "", ""]], "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.write_extra_labels.get_split": [[4, 18], ["None"], "function", ["None"], ["def", "get_split", "(", "path", ")", ":", "\n", "    ", "if", "'train'", "in", "path", ":", "\n", "        ", "if", "'extra_train'", "in", "path", ":", "\n", "            ", "split", "=", "'extra_train'", "\n", "", "else", ":", "\n", "            ", "split", "=", "'train'", "\n", "", "", "elif", "'dev'", "in", "path", ":", "\n", "        ", "if", "'extra_dev'", "in", "path", ":", "\n", "            ", "split", "=", "'extra_dev'", "\n", "", "else", ":", "\n", "            ", "split", "=", "'dev'", "\n", "", "", "else", ":", "\n", "        ", "split", "=", "'test'", "\n", "", "return", "split", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.write_extra_labels.add_number_of_children": [[19, 118], ["print", "writing_paths[].replace", "open", "write_extra_labels.get_split", "open", "f.write", "os.listdir", "os.listdir", "line.strip.strip", "line.strip.split", "lines.append", "open", "os.listdir", "file.endswith", "os.listdir", "file.endswith", "len", "range", "sentences_list.append", "f.write", "file.endswith", "file.endswith", "str", "len", "sentences_list.append", "sentences_list.append", "lines[].append", "lines[].append", "str", "str"], "function", ["home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.open", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.write_extra_labels.get_split", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.open", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.writer.Writer.write", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.open", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.writer.Writer.write"], ["", "def", "add_number_of_children", "(", "model_path", ",", "parser_path", ",", "src_domain", ",", "tgt_domain", ",", "use_unlabeled_data", "=", "True", ",", "use_labeled_data", "=", "True", ")", ":", "\n", "    ", "if", "src_domain", "==", "tgt_domain", ":", "\n", "        ", "pred_paths", "=", "[", "]", "\n", "if", "use_unlabeled_data", ":", "\n", "            ", "pred_paths", "=", "[", "file", "for", "file", "in", "os", ".", "listdir", "(", "parser_path", ")", "if", "file", ".", "endswith", "(", "\"pred.txt\"", ")", "and", "'poetry'", "not", "in", "file", "and", "'prose'", "not", "in", "file", "and", "'extra'", "in", "file", "and", "tgt_domain", "in", "file", "]", "\n", "\n", "", "gold_paths", "=", "[", "file", "for", "file", "in", "os", ".", "listdir", "(", "parser_path", ")", "if", "file", ".", "endswith", "(", "\"gold.txt\"", ")", "and", "'poetry'", "not", "in", "file", "and", "'prose'", "not", "in", "file", "and", "'extra'", "not", "in", "file", "and", "tgt_domain", "in", "file", "and", "'train'", "not", "in", "file", "]", "\n", "if", "use_labeled_data", ":", "\n", "            ", "gold_paths", "+=", "[", "file", "for", "file", "in", "os", ".", "listdir", "(", "parser_path", ")", "if", "file", ".", "endswith", "(", "\"gold.txt\"", ")", "and", "'poetry'", "not", "in", "file", "and", "'prose'", "not", "in", "file", "and", "'extra'", "not", "in", "file", "and", "tgt_domain", "in", "file", "and", "'train'", "in", "file", "]", "\n", "\n", "", "if", "not", "use_unlabeled_data", "and", "not", "use_labeled_data", ":", "\n", "            ", "raise", "ValueError", "\n", "", "", "else", ":", "\n", "        ", "pred_paths", "=", "[", "file", "for", "file", "in", "os", ".", "listdir", "(", "parser_path", ")", "if", "file", ".", "endswith", "(", "\"pred.txt\"", ")", "and", "tgt_domain", "in", "file", "]", "\n", "\n", "gold_paths", "=", "[", "]", "\n", "if", "use_labeled_data", ":", "\n", "            ", "gold_paths", "=", "[", "'data/onto_pos_ner_dp_train_'", "+", "src_domain", "]", "\n", "\n", "", "if", "not", "use_unlabeled_data", "and", "not", "use_labeled_data", ":", "\n", "            ", "raise", "ValueError", "\n", "\n", "", "", "paths", "=", "pred_paths", "+", "gold_paths", "\n", "print", "(", "\"Adding labels to paths: %s\"", "%", "', '", ".", "join", "(", "paths", ")", ")", "\n", "root_line", "=", "[", "'0'", ",", "ROOT", ",", "'XX'", ",", "'O'", ",", "'0'", ",", "'root'", "]", "\n", "writing_paths", "=", "{", "}", "\n", "sentences", "=", "{", "}", "\n", "for", "path", "in", "paths", ":", "\n", "        ", "if", "tgt_domain", "in", "path", ":", "\n", "            ", "reading_path", "=", "parser_path", "+", "path", "\n", "writing_path", "=", "model_path", "+", "'parser_'", "+", "path", "\n", "split", "=", "get_split", "(", "writing_path", ")", "\n", "", "else", ":", "\n", "            ", "reading_path", "=", "path", "\n", "writing_path", "=", "model_path", "+", "'parser_'", "+", "'domain_'", "+", "src_domain", "+", "'_train_model_domain_'", "+", "src_domain", "+", "'_data_domain_'", "+", "src_domain", "+", "'_gold.txt'", "\n", "split", "=", "'extra_train'", "\n", "", "writing_paths", "[", "split", "]", "=", "writing_path", "\n", "len_sent", "=", "0", "\n", "number_of_children", "=", "{", "}", "\n", "lines", "=", "[", "]", "\n", "sentences_list", "=", "[", "]", "\n", "with", "open", "(", "reading_path", ",", "'r'", ")", "as", "file", ":", "\n", "            ", "for", "line", "in", "file", ":", "\n", "# line = line.decode('utf-8')", "\n", "                ", "line", "=", "line", ".", "strip", "(", ")", "\n", "# print(line)", "\n", "if", "len", "(", "line", ")", "==", "0", ":", "\n", "                    ", "for", "idx", "in", "range", "(", "len_sent", ")", ":", "\n", "                        ", "node", "=", "str", "(", "idx", "+", "1", ")", "\n", "if", "node", "not", "in", "number_of_children", ":", "\n", "                            ", "lines", "[", "idx", "]", ".", "append", "(", "'0'", ")", "\n", "", "else", ":", "\n", "                            ", "lines", "[", "idx", "]", ".", "append", "(", "str", "(", "number_of_children", "[", "node", "]", ")", ")", "\n", "", "", "if", "len", "(", "lines", ")", ">", "0", ":", "\n", "                        ", "tmp_root_line", "=", "root_line", "+", "[", "str", "(", "number_of_children", "[", "'0'", "]", ")", "]", "\n", "sentences_list", ".", "append", "(", "tmp_root_line", ")", "\n", "", "for", "line_", "in", "lines", ":", "\n", "                        ", "sentences_list", ".", "append", "(", "line_", ")", "\n", "", "sentences_list", ".", "append", "(", "[", "]", ")", "\n", "lines", "=", "[", "]", "\n", "number_of_children", "=", "{", "}", "\n", "len_sent", "=", "0", "\n", "continue", "\n", "", "tokens", "=", "line", ".", "split", "(", "'\\t'", ")", "\n", "idx", "=", "tokens", "[", "0", "]", "\n", "word", "=", "tokens", "[", "1", "]", "\n", "pos", "=", "tokens", "[", "2", "]", "\n", "ner", "=", "tokens", "[", "3", "]", "\n", "head", "=", "tokens", "[", "4", "]", "\n", "arc_tag", "=", "tokens", "[", "5", "]", "\n", "if", "head", "not", "in", "number_of_children", ":", "\n", "                    ", "number_of_children", "[", "head", "]", "=", "1", "\n", "", "else", ":", "\n", "                    ", "number_of_children", "[", "head", "]", "+=", "1", "\n", "", "lines", ".", "append", "(", "[", "idx", ",", "word", ",", "pos", ",", "ner", ",", "head", ",", "arc_tag", "]", ")", "\n", "len_sent", "+=", "1", "\n", "", "", "sentences", "[", "split", "]", "=", "sentences_list", "\n", "\n", "", "train_sentences", "=", "[", "]", "\n", "if", "'train'", "in", "sentences", ":", "\n", "        ", "train_sentences", "=", "sentences", "[", "'train'", "]", "\n", "", "else", ":", "\n", "        ", "writing_paths", "[", "'train'", "]", "=", "writing_paths", "[", "'extra_train'", "]", ".", "replace", "(", "'extra_train'", ",", "'train'", ")", "\n", "", "if", "'extra_train'", "in", "sentences", ":", "\n", "        ", "train_sentences", "+=", "sentences", "[", "'extra_train'", "]", "\n", "del", "writing_paths", "[", "'extra_train'", "]", "\n", "", "if", "'extra_dev'", "in", "sentences", ":", "\n", "        ", "train_sentences", "+=", "sentences", "[", "'extra_dev'", "]", "\n", "del", "writing_paths", "[", "'extra_dev'", "]", "\n", "", "with", "open", "(", "writing_paths", "[", "'train'", "]", ",", "'w'", ")", "as", "f", ":", "\n", "        ", "for", "sent", "in", "train_sentences", ":", "\n", "            ", "f", ".", "write", "(", "'\\t'", ".", "join", "(", "sent", ")", "+", "'\\n'", ")", "\n", "", "", "for", "split", "in", "[", "'dev'", ",", "'test'", "]", ":", "\n", "        ", "if", "split", "in", "sentences", ":", "\n", "            ", "split_sentences", "=", "sentences", "[", "split", "]", "\n", "with", "open", "(", "writing_paths", "[", "split", "]", ",", "'w'", ")", "as", "f", ":", "\n", "                ", "for", "sent", "in", "split_sentences", ":", "\n", "                    ", "f", ".", "write", "(", "'\\t'", ".", "join", "(", "sent", ")", "+", "'\\n'", ")", "\n", "", "", "", "", "return", "writing_paths", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.write_extra_labels.add_distance_from_the_root": [[120, 218], ["print", "writing_paths[].replace", "open", "write_extra_labels.get_split", "open", "f.write", "os.listdir", "os.listdir", "line.strip.strip", "line.strip.split", "lines.append", "open", "os.listdir", "file.endswith", "os.listdir", "file.endswith", "len", "range", "sentences_list.append", "f.write", "file.endswith", "file.endswith", "str", "lines[].append", "len", "sentences_list.append", "sentences_list.append", "str"], "function", ["home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.open", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.write_extra_labels.get_split", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.open", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.writer.Writer.write", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.open", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.writer.Writer.write"], ["", "def", "add_distance_from_the_root", "(", "model_path", ",", "parser_path", ",", "src_domain", ",", "tgt_domain", ",", "use_unlabeled_data", "=", "True", ",", "use_labeled_data", "=", "True", ")", ":", "\n", "    ", "if", "src_domain", "==", "tgt_domain", ":", "\n", "        ", "pred_paths", "=", "[", "]", "\n", "if", "use_unlabeled_data", ":", "\n", "            ", "pred_paths", "=", "[", "file", "for", "file", "in", "os", ".", "listdir", "(", "parser_path", ")", "if", "\n", "file", ".", "endswith", "(", "\"pred.txt\"", ")", "and", "'poetry'", "not", "in", "file", "and", "'prose'", "not", "in", "file", "and", "'extra'", "in", "file", "and", "tgt_domain", "in", "file", "]", "\n", "\n", "", "gold_paths", "=", "[", "file", "for", "file", "in", "os", ".", "listdir", "(", "parser_path", ")", "if", "\n", "file", ".", "endswith", "(", "\"gold.txt\"", ")", "and", "'poetry'", "not", "in", "file", "and", "'prose'", "not", "in", "file", "and", "'extra'", "not", "in", "file", "and", "tgt_domain", "in", "file", "and", "'train'", "not", "in", "file", "]", "\n", "if", "use_labeled_data", ":", "\n", "            ", "gold_paths", "+=", "[", "file", "for", "file", "in", "os", ".", "listdir", "(", "parser_path", ")", "if", "\n", "file", ".", "endswith", "(", "\"gold.txt\"", ")", "and", "'extra'", "not", "in", "file", "and", "'poetry'", "not", "in", "file", "and", "'prose'", "not", "in", "file", "and", "tgt_domain", "in", "file", "and", "'train'", "in", "file", "]", "\n", "\n", "", "if", "not", "use_unlabeled_data", "and", "not", "use_labeled_data", ":", "\n", "            ", "raise", "ValueError", "\n", "", "", "else", ":", "\n", "        ", "pred_paths", "=", "[", "file", "for", "file", "in", "os", ".", "listdir", "(", "parser_path", ")", "if", "file", ".", "endswith", "(", "\"pred.txt\"", ")", "and", "tgt_domain", "in", "file", "]", "\n", "\n", "gold_paths", "=", "[", "]", "\n", "if", "use_labeled_data", ":", "\n", "            ", "gold_paths", "=", "[", "'data/onto_pos_ner_dp_train_'", "+", "src_domain", "]", "\n", "\n", "", "if", "not", "use_unlabeled_data", "and", "not", "use_labeled_data", ":", "\n", "            ", "raise", "ValueError", "\n", "\n", "", "", "paths", "=", "pred_paths", "+", "gold_paths", "\n", "print", "(", "\"Adding labels to paths: %s\"", "%", "', '", ".", "join", "(", "paths", ")", ")", "\n", "root_line", "=", "[", "'0'", ",", "ROOT", ",", "'XX'", ",", "'O'", ",", "'0'", ",", "'root'", ",", "'0'", "]", "\n", "writing_paths", "=", "{", "}", "\n", "sentences", "=", "{", "}", "\n", "for", "path", "in", "paths", ":", "\n", "        ", "if", "tgt_domain", "in", "path", ":", "\n", "            ", "reading_path", "=", "parser_path", "+", "path", "\n", "writing_path", "=", "model_path", "+", "'parser_'", "+", "path", "\n", "split", "=", "get_split", "(", "writing_path", ")", "\n", "", "else", ":", "\n", "            ", "reading_path", "=", "path", "\n", "writing_path", "=", "model_path", "+", "'parser_'", "+", "'domain_'", "+", "src_domain", "+", "'_train_model_domain_'", "+", "src_domain", "+", "'_data_domain_'", "+", "src_domain", "+", "'_gold.txt'", "\n", "split", "=", "'extra_train'", "\n", "", "writing_paths", "[", "split", "]", "=", "writing_path", "\n", "len_sent", "=", "0", "\n", "tree_dict", "=", "{", "'0'", ":", "'0'", "}", "\n", "lines", "=", "[", "]", "\n", "sentences_list", "=", "[", "]", "\n", "with", "open", "(", "reading_path", ",", "'r'", ")", "as", "file", ":", "\n", "            ", "for", "line", "in", "file", ":", "\n", "# line = line.decode('utf-8')", "\n", "                ", "line", "=", "line", ".", "strip", "(", ")", "\n", "if", "len", "(", "line", ")", "==", "0", ":", "\n", "                    ", "for", "idx", "in", "range", "(", "len_sent", ")", ":", "\n", "                        ", "depth", "=", "1", "\n", "node", "=", "str", "(", "idx", "+", "1", ")", "\n", "while", "tree_dict", "[", "node", "]", "!=", "'0'", ":", "\n", "                            ", "node", "=", "tree_dict", "[", "node", "]", "\n", "depth", "+=", "1", "\n", "", "lines", "[", "idx", "]", ".", "append", "(", "str", "(", "depth", ")", ")", "\n", "", "if", "len", "(", "lines", ")", ">", "0", ":", "\n", "                        ", "sentences_list", ".", "append", "(", "root_line", ")", "\n", "", "for", "line_", "in", "lines", ":", "\n", "                        ", "sentences_list", ".", "append", "(", "line_", ")", "\n", "", "sentences_list", ".", "append", "(", "[", "]", ")", "\n", "lines", "=", "[", "]", "\n", "tree_dict", "=", "{", "'0'", ":", "'0'", "}", "\n", "len_sent", "=", "0", "\n", "continue", "\n", "", "tokens", "=", "line", ".", "split", "(", "'\\t'", ")", "\n", "idx", "=", "tokens", "[", "0", "]", "\n", "word", "=", "tokens", "[", "1", "]", "\n", "pos", "=", "tokens", "[", "2", "]", "\n", "ner", "=", "tokens", "[", "3", "]", "\n", "head", "=", "tokens", "[", "4", "]", "\n", "arc_tag", "=", "tokens", "[", "5", "]", "\n", "tree_dict", "[", "idx", "]", "=", "head", "\n", "lines", ".", "append", "(", "[", "idx", ",", "word", ",", "pos", ",", "ner", ",", "head", ",", "arc_tag", "]", ")", "\n", "len_sent", "+=", "1", "\n", "", "", "sentences", "[", "split", "]", "=", "sentences_list", "\n", "\n", "", "train_sentences", "=", "[", "]", "\n", "if", "'train'", "in", "sentences", ":", "\n", "        ", "train_sentences", "=", "sentences", "[", "'train'", "]", "\n", "", "else", ":", "\n", "        ", "writing_paths", "[", "'train'", "]", "=", "writing_paths", "[", "'extra_train'", "]", ".", "replace", "(", "'extra_train'", ",", "'train'", ")", "\n", "", "if", "'extra_train'", "in", "sentences", ":", "\n", "        ", "train_sentences", "+=", "sentences", "[", "'extra_train'", "]", "\n", "del", "writing_paths", "[", "'extra_train'", "]", "\n", "", "if", "'extra_dev'", "in", "sentences", ":", "\n", "        ", "train_sentences", "+=", "sentences", "[", "'extra_dev'", "]", "\n", "del", "writing_paths", "[", "'extra_dev'", "]", "\n", "", "with", "open", "(", "writing_paths", "[", "'train'", "]", ",", "'w'", ")", "as", "f", ":", "\n", "        ", "for", "sent", "in", "train_sentences", ":", "\n", "            ", "f", ".", "write", "(", "'\\t'", ".", "join", "(", "sent", ")", "+", "'\\n'", ")", "\n", "", "", "for", "split", "in", "[", "'dev'", ",", "'test'", "]", ":", "\n", "        ", "if", "split", "in", "sentences", ":", "\n", "            ", "split_sentences", "=", "sentences", "[", "split", "]", "\n", "with", "open", "(", "writing_paths", "[", "split", "]", ",", "'w'", ")", "as", "f", ":", "\n", "                ", "for", "sent", "in", "split_sentences", ":", "\n", "                    ", "f", ".", "write", "(", "'\\t'", ".", "join", "(", "sent", ")", "+", "'\\n'", ")", "\n", "", "", "", "", "return", "writing_paths", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.write_extra_labels.add_relative_pos_based": [[219, 372], ["print", "writing_paths[].replace", "open", "write_extra_labels.get_split", "open", "f.write", "os.listdir", "os.listdir", "line.strip.strip", "line.strip.split", "lines.append", "open", "os.listdir", "file.endswith", "os.listdir", "file.endswith", "len", "range", "sentences_list.append", "f.write", "file.endswith", "file.endswith", "len", "sentences_list.append", "sentences_list.append", "int", "write_extra_labels.add_relative_pos_based.pos_cluster"], "function", ["home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.open", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.write_extra_labels.get_split", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.open", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.writer.Writer.write", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.open", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.writer.Writer.write"], ["", "def", "add_relative_pos_based", "(", "model_path", ",", "parser_path", ",", "src_domain", ",", "tgt_domain", ",", "use_unlabeled_data", "=", "True", ",", "use_labeled_data", "=", "True", ")", ":", "\n", "# most of the code for this function is taken from:", "\n", "# https://github.com/mstrise/dep2label/blob/master/encoding.py", "\n", "    ", "def", "pos_cluster", "(", "pos", ")", ":", "\n", "# clustering the parts of speech", "\n", "        ", "if", "pos", "[", "0", "]", "==", "'V'", ":", "\n", "            ", "pos", "=", "'VB'", "\n", "", "elif", "pos", "==", "'NNS'", ":", "\n", "            ", "pos", "=", "'NN'", "\n", "", "elif", "pos", "==", "'NNPS'", ":", "\n", "            ", "pos", "=", "'NNP'", "\n", "", "elif", "'JJ'", "in", "pos", ":", "\n", "            ", "pos", "=", "'JJ'", "\n", "", "elif", "pos", "[", ":", "2", "]", "==", "'RB'", "or", "pos", "==", "'WRB'", "or", "pos", "==", "'RP'", ":", "\n", "            ", "pos", "=", "'RB'", "\n", "", "elif", "pos", "[", ":", "3", "]", "==", "'PRP'", ":", "\n", "            ", "pos", "=", "'PRP'", "\n", "", "elif", "pos", "in", "[", "'.'", ",", "':'", ",", "','", ",", "\"''\"", ",", "'``'", "]", ":", "\n", "            ", "pos", "=", "'.'", "\n", "", "elif", "pos", "[", "0", "]", "==", "'-'", ":", "\n", "            ", "pos", "=", "'-RB-'", "\n", "", "elif", "pos", "[", ":", "2", "]", "==", "'WP'", ":", "\n", "            ", "pos", "=", "'WP'", "\n", "", "return", "pos", "\n", "\n", "", "if", "src_domain", "==", "tgt_domain", ":", "\n", "        ", "pred_paths", "=", "[", "]", "\n", "if", "use_unlabeled_data", ":", "\n", "            ", "pred_paths", "=", "[", "file", "for", "file", "in", "os", ".", "listdir", "(", "parser_path", ")", "if", "\n", "file", ".", "endswith", "(", "\"pred.txt\"", ")", "and", "'poetry'", "not", "in", "file", "and", "'prose'", "not", "in", "file", "and", "'extra'", "in", "file", "and", "tgt_domain", "in", "file", "]", "\n", "\n", "", "gold_paths", "=", "[", "file", "for", "file", "in", "os", ".", "listdir", "(", "parser_path", ")", "if", "\n", "file", ".", "endswith", "(", "\"gold.txt\"", ")", "and", "'poetry'", "not", "in", "file", "and", "'prose'", "not", "in", "file", "and", "'extra'", "not", "in", "file", "and", "tgt_domain", "in", "file", "and", "'train'", "not", "in", "file", "]", "\n", "if", "use_labeled_data", ":", "\n", "            ", "gold_paths", "+=", "[", "file", "for", "file", "in", "os", ".", "listdir", "(", "parser_path", ")", "if", "\n", "file", ".", "endswith", "(", "\"gold.txt\"", ")", "and", "'extra'", "not", "in", "file", "and", "'poetry'", "not", "in", "file", "and", "'prose'", "not", "in", "file", "and", "tgt_domain", "in", "file", "and", "'train'", "in", "file", "]", "\n", "\n", "", "if", "not", "use_unlabeled_data", "and", "not", "use_labeled_data", ":", "\n", "            ", "raise", "ValueError", "\n", "", "", "else", ":", "\n", "        ", "pred_paths", "=", "[", "file", "for", "file", "in", "os", ".", "listdir", "(", "parser_path", ")", "if", "file", ".", "endswith", "(", "\"pred.txt\"", ")", "and", "tgt_domain", "in", "file", "]", "\n", "gold_paths", "=", "[", "]", "\n", "if", "use_labeled_data", ":", "\n", "            ", "gold_paths", "=", "[", "'data/onto_pos_ner_dp_train_'", "+", "src_domain", "]", "\n", "\n", "", "if", "not", "use_unlabeled_data", ":", "\n", "            ", "raise", "ValueError", "\n", "\n", "", "", "paths", "=", "pred_paths", "+", "gold_paths", "\n", "print", "(", "\"Adding labels to paths: %s\"", "%", "', '", ".", "join", "(", "paths", ")", ")", "\n", "root_line", "=", "[", "'0'", ",", "ROOT", ",", "'XX'", ",", "'O'", ",", "'0'", ",", "'root'", ",", "'+0_XX'", "]", "\n", "writing_paths", "=", "{", "}", "\n", "sentences", "=", "{", "}", "\n", "for", "path", "in", "paths", ":", "\n", "        ", "if", "tgt_domain", "in", "path", ":", "\n", "            ", "reading_path", "=", "parser_path", "+", "path", "\n", "writing_path", "=", "model_path", "+", "'parser_'", "+", "path", "\n", "split", "=", "get_split", "(", "writing_path", ")", "\n", "", "else", ":", "\n", "            ", "reading_path", "=", "path", "\n", "writing_path", "=", "model_path", "+", "'parser_'", "+", "'domain_'", "+", "src_domain", "+", "'_train_model_domain_'", "+", "src_domain", "+", "'_data_domain_'", "+", "src_domain", "+", "'_gold.txt'", "\n", "split", "=", "'extra_train'", "\n", "", "writing_paths", "[", "split", "]", "=", "writing_path", "\n", "len_sent", "=", "0", "\n", "tree_dict", "=", "{", "'0'", ":", "'0'", "}", "\n", "lines", "=", "[", "]", "\n", "sentences_list", "=", "[", "]", "\n", "with", "open", "(", "reading_path", ",", "'r'", ")", "as", "file", ":", "\n", "            ", "for", "line", "in", "file", ":", "\n", "# line = line.decode('utf-8')", "\n", "                ", "line", "=", "line", ".", "strip", "(", ")", "\n", "if", "len", "(", "line", ")", "==", "0", ":", "\n", "                    ", "for", "idx", "in", "range", "(", "len_sent", ")", ":", "\n", "                        ", "info_of_a_word", "=", "lines", "[", "idx", "]", "\n", "# head is on the right side from the word", "\n", "head", "=", "int", "(", "info_of_a_word", "[", "4", "]", ")", "-", "1", "\n", "if", "head", "==", "-", "1", ":", "\n", "                            ", "info_about_head", "=", "root_line", "\n", "", "else", ":", "\n", "                            ", "info_about_head", "=", "lines", "[", "head", "]", "\n", "", "if", "idx", "<", "head", ":", "\n", "                            ", "relative_position_head", "=", "1", "\n", "postag_head", "=", "pos_cluster", "(", "info_about_head", "[", "2", "]", ")", "\n", "\n", "for", "x", "in", "range", "(", "idx", "+", "1", ",", "head", ")", ":", "\n", "                                ", "another_word", "=", "lines", "[", "x", "]", "\n", "postag_word_before_head", "=", "pos_cluster", "(", "another_word", "[", "2", "]", ")", "\n", "if", "postag_word_before_head", "==", "postag_head", ":", "\n", "                                    ", "relative_position_head", "+=", "1", "\n", "", "", "label", "=", "str", "(", "\n", "\"+\"", "+", "\n", "repr", "(", "relative_position_head", ")", "+", "\n", "\"_\"", "+", "\n", "postag_head", ")", "\n", "lines", "[", "idx", "]", ".", "append", "(", "label", ")", "\n", "\n", "# head is on the left side from the word", "\n", "", "elif", "idx", ">", "head", ":", "\n", "                            ", "relative_position_head", "=", "1", "\n", "postag_head", "=", "pos_cluster", "(", "info_about_head", "[", "2", "]", ")", "\n", "for", "x", "in", "range", "(", "head", "+", "1", ",", "idx", ")", ":", "\n", "                                ", "another_word", "=", "lines", "[", "x", "]", "\n", "postag_word_before_head", "=", "pos_cluster", "(", "another_word", "[", "2", "]", ")", "\n", "if", "postag_word_before_head", "==", "postag_head", ":", "\n", "                                    ", "relative_position_head", "+=", "1", "\n", "", "", "label", "=", "str", "(", "\n", "\"-\"", "+", "\n", "repr", "(", "relative_position_head", ")", "+", "\n", "\"_\"", "+", "\n", "postag_head", ")", "\n", "lines", "[", "idx", "]", ".", "append", "(", "label", ")", "\n", "", "", "if", "len", "(", "lines", ")", ">", "0", ":", "\n", "                        ", "sentences_list", ".", "append", "(", "root_line", ")", "\n", "", "for", "line_", "in", "lines", ":", "\n", "                        ", "sentences_list", ".", "append", "(", "line_", ")", "\n", "", "sentences_list", ".", "append", "(", "[", "]", ")", "\n", "lines", "=", "[", "]", "\n", "tree_dict", "=", "{", "'0'", ":", "'0'", "}", "\n", "len_sent", "=", "0", "\n", "continue", "\n", "", "tokens", "=", "line", ".", "split", "(", "'\\t'", ")", "\n", "idx", "=", "tokens", "[", "0", "]", "\n", "word", "=", "tokens", "[", "1", "]", "\n", "pos", "=", "tokens", "[", "2", "]", "\n", "ner", "=", "tokens", "[", "3", "]", "\n", "head", "=", "tokens", "[", "4", "]", "\n", "arc_tag", "=", "tokens", "[", "5", "]", "\n", "tree_dict", "[", "idx", "]", "=", "head", "\n", "lines", ".", "append", "(", "[", "idx", ",", "word", ",", "pos", ",", "ner", ",", "head", ",", "arc_tag", "]", ")", "\n", "len_sent", "+=", "1", "\n", "", "", "sentences", "[", "split", "]", "=", "sentences_list", "\n", "\n", "", "train_sentences", "=", "[", "]", "\n", "if", "'train'", "in", "sentences", ":", "\n", "        ", "train_sentences", "=", "sentences", "[", "'train'", "]", "\n", "", "else", ":", "\n", "        ", "writing_paths", "[", "'train'", "]", "=", "writing_paths", "[", "'extra_train'", "]", ".", "replace", "(", "'extra_train'", ",", "'train'", ")", "\n", "", "if", "'extra_train'", "in", "sentences", ":", "\n", "        ", "train_sentences", "+=", "sentences", "[", "'extra_train'", "]", "\n", "del", "writing_paths", "[", "'extra_train'", "]", "\n", "", "if", "'extra_dev'", "in", "sentences", ":", "\n", "        ", "train_sentences", "+=", "sentences", "[", "'extra_dev'", "]", "\n", "del", "writing_paths", "[", "'extra_dev'", "]", "\n", "", "with", "open", "(", "writing_paths", "[", "'train'", "]", ",", "'w'", ")", "as", "f", ":", "\n", "        ", "for", "sent", "in", "train_sentences", ":", "\n", "            ", "f", ".", "write", "(", "'\\t'", ".", "join", "(", "sent", ")", "+", "'\\n'", ")", "\n", "", "", "for", "split", "in", "[", "'dev'", ",", "'test'", "]", ":", "\n", "        ", "if", "split", "in", "sentences", ":", "\n", "            ", "split_sentences", "=", "sentences", "[", "split", "]", "\n", "with", "open", "(", "writing_paths", "[", "split", "]", ",", "'w'", ")", "as", "f", ":", "\n", "                ", "for", "sent", "in", "split_sentences", ":", "\n", "                    ", "f", ".", "write", "(", "'\\t'", ".", "join", "(", "sent", ")", "+", "'\\n'", ")", "\n", "", "", "", "", "return", "writing_paths", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.write_extra_labels.add_language_model": [[373, 467], ["print", "writing_paths[].replace", "open", "write_extra_labels.get_split", "open", "f.write", "os.listdir", "os.listdir", "line.strip.strip", "line.strip.split", "lines.append", "open", "os.listdir", "file.endswith", "os.listdir", "file.endswith", "len", "range", "sentences_list.append", "f.write", "file.endswith", "file.endswith", "len", "sentences_list.append", "sentences_list.append", "lines[].append", "lines[].append"], "function", ["home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.open", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.write_extra_labels.get_split", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.open", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.writer.Writer.write", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.open", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.writer.Writer.write"], ["", "def", "add_language_model", "(", "model_path", ",", "parser_path", ",", "src_domain", ",", "tgt_domain", ",", "use_unlabeled_data", "=", "True", ",", "use_labeled_data", "=", "True", ")", ":", "\n", "    ", "if", "src_domain", "==", "tgt_domain", ":", "\n", "        ", "pred_paths", "=", "[", "]", "\n", "if", "use_unlabeled_data", ":", "\n", "            ", "pred_paths", "=", "[", "file", "for", "file", "in", "os", ".", "listdir", "(", "parser_path", ")", "if", "\n", "file", ".", "endswith", "(", "\"pred.txt\"", ")", "and", "'extra'", "in", "file", "and", "tgt_domain", "in", "file", "]", "\n", "\n", "", "gold_paths", "=", "[", "file", "for", "file", "in", "os", ".", "listdir", "(", "parser_path", ")", "if", "\n", "file", ".", "endswith", "(", "\"gold.txt\"", ")", "and", "'extra'", "not", "in", "file", "and", "tgt_domain", "in", "file", "and", "'train'", "not", "in", "file", "]", "\n", "if", "use_labeled_data", ":", "\n", "            ", "gold_paths", "+=", "[", "file", "for", "file", "in", "os", ".", "listdir", "(", "parser_path", ")", "if", "\n", "file", ".", "endswith", "(", "\"gold.txt\"", ")", "and", "'extra'", "not", "in", "file", "and", "tgt_domain", "in", "file", "and", "'train'", "in", "file", "]", "\n", "\n", "", "if", "not", "use_unlabeled_data", "and", "not", "use_labeled_data", ":", "\n", "            ", "raise", "ValueError", "\n", "", "", "else", ":", "\n", "        ", "pred_paths", "=", "[", "file", "for", "file", "in", "os", ".", "listdir", "(", "parser_path", ")", "if", "file", ".", "endswith", "(", "\"pred.txt\"", ")", "and", "tgt_domain", "in", "file", "]", "\n", "\n", "gold_paths", "=", "[", "]", "\n", "if", "use_labeled_data", ":", "\n", "            ", "gold_paths", "=", "[", "'data/onto_pos_ner_dp_train_'", "+", "src_domain", "]", "\n", "\n", "", "if", "not", "use_unlabeled_data", "and", "not", "use_labeled_data", ":", "\n", "            ", "raise", "ValueError", "\n", "\n", "", "", "paths", "=", "pred_paths", "+", "gold_paths", "\n", "print", "(", "\"Adding labels to paths: %s\"", "%", "', '", ".", "join", "(", "paths", ")", ")", "\n", "root_line", "=", "[", "'0'", ",", "ROOT", ",", "'XX'", ",", "'O'", ",", "'0'", ",", "'root'", "]", "\n", "writing_paths", "=", "{", "}", "\n", "sentences", "=", "{", "}", "\n", "for", "path", "in", "paths", ":", "\n", "        ", "if", "tgt_domain", "in", "path", ":", "\n", "            ", "reading_path", "=", "parser_path", "+", "path", "\n", "writing_path", "=", "model_path", "+", "'parser_'", "+", "path", "\n", "split", "=", "get_split", "(", "writing_path", ")", "\n", "", "else", ":", "\n", "            ", "reading_path", "=", "path", "\n", "writing_path", "=", "model_path", "+", "'parser_'", "+", "'domain_'", "+", "src_domain", "+", "'_train_model_domain_'", "+", "src_domain", "+", "'_data_domain_'", "+", "src_domain", "+", "'_gold.txt'", "\n", "split", "=", "'extra_train'", "\n", "", "writing_paths", "[", "split", "]", "=", "writing_path", "\n", "len_sent", "=", "0", "\n", "lines", "=", "[", "]", "\n", "sentences_list", "=", "[", "]", "\n", "with", "open", "(", "reading_path", ",", "'r'", ")", "as", "file", ":", "\n", "            ", "for", "line", "in", "file", ":", "\n", "# line = line.decode('utf-8')", "\n", "                ", "line", "=", "line", ".", "strip", "(", ")", "\n", "if", "len", "(", "line", ")", "==", "0", ":", "\n", "                    ", "for", "idx", "in", "range", "(", "len_sent", ")", ":", "\n", "                        ", "if", "idx", "<", "len_sent", "-", "1", ":", "\n", "                            ", "lines", "[", "idx", "]", ".", "append", "(", "lines", "[", "idx", "+", "1", "]", "[", "1", "]", ")", "\n", "", "else", ":", "\n", "                            ", "lines", "[", "idx", "]", ".", "append", "(", "END", ")", "\n", "", "", "if", "len", "(", "lines", ")", ">", "0", ":", "\n", "                        ", "tmp_root_line", "=", "root_line", "+", "[", "lines", "[", "0", "]", "[", "1", "]", "]", "\n", "sentences_list", ".", "append", "(", "tmp_root_line", ")", "\n", "", "for", "line_", "in", "lines", ":", "\n", "                        ", "sentences_list", ".", "append", "(", "line_", ")", "\n", "", "sentences_list", ".", "append", "(", "[", "]", ")", "\n", "lines", "=", "[", "]", "\n", "len_sent", "=", "0", "\n", "continue", "\n", "", "tokens", "=", "line", ".", "split", "(", "'\\t'", ")", "\n", "idx", "=", "tokens", "[", "0", "]", "\n", "word", "=", "tokens", "[", "1", "]", "\n", "pos", "=", "tokens", "[", "2", "]", "\n", "ner", "=", "tokens", "[", "3", "]", "\n", "head", "=", "tokens", "[", "4", "]", "\n", "arc_tag", "=", "tokens", "[", "5", "]", "\n", "lines", ".", "append", "(", "[", "idx", ",", "word", ",", "pos", ",", "ner", ",", "head", ",", "arc_tag", "]", ")", "\n", "len_sent", "+=", "1", "\n", "", "", "sentences", "[", "split", "]", "=", "sentences_list", "\n", "\n", "", "train_sentences", "=", "[", "]", "\n", "if", "'train'", "in", "sentences", ":", "\n", "        ", "train_sentences", "=", "sentences", "[", "'train'", "]", "\n", "", "else", ":", "\n", "        ", "writing_paths", "[", "'train'", "]", "=", "writing_paths", "[", "'extra_train'", "]", ".", "replace", "(", "'extra_train'", ",", "'train'", ")", "\n", "", "if", "'extra_train'", "in", "sentences", ":", "\n", "        ", "train_sentences", "+=", "sentences", "[", "'extra_train'", "]", "\n", "del", "writing_paths", "[", "'extra_train'", "]", "\n", "", "if", "'extra_dev'", "in", "sentences", ":", "\n", "        ", "train_sentences", "+=", "sentences", "[", "'extra_dev'", "]", "\n", "del", "writing_paths", "[", "'extra_dev'", "]", "\n", "", "with", "open", "(", "writing_paths", "[", "'train'", "]", ",", "'w'", ")", "as", "f", ":", "\n", "        ", "for", "sent", "in", "train_sentences", ":", "\n", "            ", "f", ".", "write", "(", "'\\t'", ".", "join", "(", "sent", ")", "+", "'\\n'", ")", "\n", "", "", "for", "split", "in", "[", "'dev'", ",", "'test'", "]", ":", "\n", "        ", "if", "split", "in", "sentences", ":", "\n", "            ", "split_sentences", "=", "sentences", "[", "split", "]", "\n", "with", "open", "(", "writing_paths", "[", "split", "]", ",", "'w'", ")", "as", "f", ":", "\n", "                ", "for", "sent", "in", "split_sentences", ":", "\n", "                    ", "f", ".", "write", "(", "'\\t'", ".", "join", "(", "sent", ")", "+", "'\\n'", ")", "\n", "", "", "", "", "return", "writing_paths", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.write_extra_labels.add_relative_TAG": [[468, 626], ["print", "writing_paths[].replace", "open", "write_extra_labels.get_split", "open", "f.write", "os.listdir", "os.listdir", "line.strip.strip", "line.strip.split", "lines.append", "open", "os.listdir", "file.endswith", "os.listdir", "file.endswith", "len", "range", "sentences_list.append", "f.write", "file.endswith", "file.endswith", "len", "sentences_list.append", "sentences_list.append", "int", "range", "str", "lines[].append", "range", "str", "lines[].append", "repr", "repr"], "function", ["home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.open", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.write_extra_labels.get_split", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.open", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.writer.Writer.write", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.open", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.writer.Writer.write"], ["", "def", "add_relative_TAG", "(", "model_path", ",", "parser_path", ",", "src_domain", ",", "tgt_domain", ",", "use_unlabeled_data", "=", "True", ",", "use_labeled_data", "=", "True", ")", ":", "\n", "# most of the code for this function is taken from:", "\n", "# https://github.com/mstrise/dep2label/blob/master/encoding.py", "\n", "    ", "def", "pos_cluster", "(", "pos", ")", ":", "\n", "# clustering the parts of speech", "\n", "        ", "if", "pos", "[", "0", "]", "==", "'V'", ":", "\n", "            ", "pos", "=", "'VB'", "\n", "", "elif", "pos", "==", "'NNS'", ":", "\n", "            ", "pos", "=", "'NN'", "\n", "", "elif", "pos", "==", "'NNPS'", ":", "\n", "            ", "pos", "=", "'NNP'", "\n", "", "elif", "'JJ'", "in", "pos", ":", "\n", "            ", "pos", "=", "'JJ'", "\n", "", "elif", "pos", "[", ":", "2", "]", "==", "'RB'", "or", "pos", "==", "'WRB'", "or", "pos", "==", "'RP'", ":", "\n", "            ", "pos", "=", "'RB'", "\n", "", "elif", "pos", "[", ":", "3", "]", "==", "'PRP'", ":", "\n", "            ", "pos", "=", "'PRP'", "\n", "", "elif", "pos", "in", "[", "'.'", ",", "':'", ",", "','", ",", "\"''\"", ",", "'``'", "]", ":", "\n", "            ", "pos", "=", "'.'", "\n", "", "elif", "pos", "[", "0", "]", "==", "'-'", ":", "\n", "            ", "pos", "=", "'-RB-'", "\n", "", "elif", "pos", "[", ":", "2", "]", "==", "'WP'", ":", "\n", "            ", "pos", "=", "'WP'", "\n", "", "return", "pos", "\n", "\n", "", "if", "src_domain", "==", "tgt_domain", ":", "\n", "        ", "pred_paths", "=", "[", "]", "\n", "if", "use_unlabeled_data", ":", "\n", "            ", "pred_paths", "=", "[", "file", "for", "file", "in", "os", ".", "listdir", "(", "parser_path", ")", "if", "\n", "file", ".", "endswith", "(", "\"pred.txt\"", ")", "and", "'extra'", "in", "file", "and", "tgt_domain", "in", "file", "]", "\n", "\n", "", "gold_paths", "=", "[", "file", "for", "file", "in", "os", ".", "listdir", "(", "parser_path", ")", "if", "\n", "file", ".", "endswith", "(", "\"gold.txt\"", ")", "and", "'extra'", "not", "in", "file", "and", "tgt_domain", "in", "file", "and", "'train'", "not", "in", "file", "]", "\n", "if", "use_labeled_data", ":", "\n", "            ", "gold_paths", "+=", "[", "file", "for", "file", "in", "os", ".", "listdir", "(", "parser_path", ")", "if", "\n", "file", ".", "endswith", "(", "\"gold.txt\"", ")", "and", "'extra'", "not", "in", "file", "and", "tgt_domain", "in", "file", "and", "'train'", "in", "file", "]", "\n", "\n", "", "if", "not", "use_unlabeled_data", "and", "not", "use_labeled_data", ":", "\n", "            ", "raise", "ValueError", "\n", "", "", "else", ":", "\n", "        ", "pred_paths", "=", "[", "file", "for", "file", "in", "os", ".", "listdir", "(", "parser_path", ")", "if", "file", ".", "endswith", "(", "\"pred.txt\"", ")", "and", "tgt_domain", "in", "file", "]", "\n", "gold_paths", "=", "[", "]", "\n", "if", "use_labeled_data", ":", "\n", "            ", "gold_paths", "=", "[", "'data/onto_pos_ner_dp_train_'", "+", "src_domain", "]", "\n", "\n", "", "if", "not", "use_unlabeled_data", ":", "\n", "            ", "raise", "ValueError", "\n", "\n", "", "", "paths", "=", "pred_paths", "+", "gold_paths", "\n", "print", "(", "\"Adding labels to paths: %s\"", "%", "', '", ".", "join", "(", "paths", ")", ")", "\n", "root_line", "=", "[", "'0'", ",", "ROOT", ",", "'XX'", ",", "'O'", ",", "'0'", ",", "'root'", ",", "'+0_XX'", "]", "\n", "writing_paths", "=", "{", "}", "\n", "sentences", "=", "{", "}", "\n", "for", "path", "in", "paths", ":", "\n", "        ", "if", "tgt_domain", "in", "path", ":", "\n", "            ", "reading_path", "=", "parser_path", "+", "path", "\n", "writing_path", "=", "model_path", "+", "'parser_'", "+", "path", "\n", "split", "=", "get_split", "(", "writing_path", ")", "\n", "", "else", ":", "\n", "            ", "reading_path", "=", "path", "\n", "writing_path", "=", "model_path", "+", "'parser_'", "+", "'domain_'", "+", "src_domain", "+", "'_train_model_domain_'", "+", "src_domain", "+", "'_data_domain_'", "+", "src_domain", "+", "'_gold.txt'", "\n", "split", "=", "'extra_train'", "\n", "", "writing_paths", "[", "split", "]", "=", "writing_path", "\n", "len_sent", "=", "0", "\n", "tree_dict", "=", "{", "'0'", ":", "'0'", "}", "\n", "lines", "=", "[", "]", "\n", "sentences_list", "=", "[", "]", "\n", "with", "open", "(", "reading_path", ",", "'r'", ")", "as", "file", ":", "\n", "\n", "            ", "for", "line", "in", "file", ":", "\n", "# print(line)", "\n", "# print(reading_path)", "\n", "# line = line.decode('utf-8')", "\n", "                ", "line", "=", "line", ".", "strip", "(", ")", "\n", "if", "len", "(", "line", ")", "==", "0", ":", "\n", "                    ", "for", "idx", "in", "range", "(", "len_sent", ")", ":", "\n", "                        ", "info_of_a_word", "=", "lines", "[", "idx", "]", "\n", "# head is on the right side from the word", "\n", "head", "=", "int", "(", "info_of_a_word", "[", "4", "]", ")", "-", "1", "\n", "if", "head", "==", "-", "1", ":", "\n", "                            ", "info_about_head", "=", "root_line", "\n", "", "else", ":", "\n", "# print(len(lines), head)", "\n", "                            ", "info_about_head", "=", "lines", "[", "head", "]", "\n", "\n", "", "if", "idx", "<", "head", ":", "\n", "                            ", "relative_position_head", "=", "1", "\n", "tag_head", "=", "info_about_head", "[", "5", "]", "\n", "\n", "for", "x", "in", "range", "(", "idx", "+", "1", ",", "head", ")", ":", "\n", "                                ", "another_word", "=", "lines", "[", "x", "]", "\n", "tag_word_before_head", "=", "another_word", "[", "5", "]", "\n", "if", "tag_word_before_head", "==", "tag_head", ":", "\n", "                                    ", "relative_position_head", "+=", "1", "\n", "", "", "label", "=", "str", "(", "\n", "\"+\"", "+", "\n", "repr", "(", "relative_position_head", ")", "+", "\n", "\"_\"", "+", "\n", "tag_head", ")", "\n", "lines", "[", "idx", "]", ".", "append", "(", "label", ")", "\n", "\n", "# head is on the left side from the word", "\n", "", "elif", "idx", ">", "head", ":", "\n", "                            ", "relative_position_head", "=", "1", "\n", "tag_head", "=", "info_about_head", "[", "5", "]", "\n", "for", "x", "in", "range", "(", "head", "+", "1", ",", "idx", ")", ":", "\n", "                                ", "another_word", "=", "lines", "[", "x", "]", "\n", "tag_word_before_head", "=", "another_word", "[", "5", "]", "\n", "if", "tag_word_before_head", "==", "tag_head", ":", "\n", "                                    ", "relative_position_head", "+=", "1", "\n", "", "", "label", "=", "str", "(", "\n", "\"-\"", "+", "\n", "repr", "(", "relative_position_head", ")", "+", "\n", "\"_\"", "+", "\n", "tag_head", ")", "\n", "lines", "[", "idx", "]", ".", "append", "(", "label", ")", "\n", "", "", "if", "len", "(", "lines", ")", ">", "0", ":", "\n", "                        ", "sentences_list", ".", "append", "(", "root_line", ")", "\n", "", "for", "line_", "in", "lines", ":", "\n", "                        ", "sentences_list", ".", "append", "(", "line_", ")", "\n", "", "sentences_list", ".", "append", "(", "[", "]", ")", "\n", "lines", "=", "[", "]", "\n", "tree_dict", "=", "{", "'0'", ":", "'0'", "}", "\n", "len_sent", "=", "0", "\n", "continue", "\n", "", "tokens", "=", "line", ".", "split", "(", "'\\t'", ")", "\n", "idx", "=", "tokens", "[", "0", "]", "\n", "word", "=", "tokens", "[", "1", "]", "\n", "pos", "=", "tokens", "[", "2", "]", "\n", "ner", "=", "tokens", "[", "3", "]", "\n", "head", "=", "tokens", "[", "4", "]", "\n", "arc_tag", "=", "tokens", "[", "5", "]", "\n", "tree_dict", "[", "idx", "]", "=", "head", "\n", "lines", ".", "append", "(", "[", "idx", ",", "word", ",", "pos", ",", "ner", ",", "head", ",", "arc_tag", "]", ")", "\n", "len_sent", "+=", "1", "\n", "", "", "sentences", "[", "split", "]", "=", "sentences_list", "\n", "\n", "", "train_sentences", "=", "[", "]", "\n", "if", "'train'", "in", "sentences", ":", "\n", "        ", "train_sentences", "=", "sentences", "[", "'train'", "]", "\n", "", "else", ":", "\n", "        ", "writing_paths", "[", "'train'", "]", "=", "writing_paths", "[", "'extra_train'", "]", ".", "replace", "(", "'extra_train'", ",", "'train'", ")", "\n", "", "if", "'extra_train'", "in", "sentences", ":", "\n", "        ", "train_sentences", "+=", "sentences", "[", "'extra_train'", "]", "\n", "del", "writing_paths", "[", "'extra_train'", "]", "\n", "", "if", "'extra_dev'", "in", "sentences", ":", "\n", "        ", "train_sentences", "+=", "sentences", "[", "'extra_dev'", "]", "\n", "del", "writing_paths", "[", "'extra_dev'", "]", "\n", "", "with", "open", "(", "writing_paths", "[", "'train'", "]", ",", "'w'", ")", "as", "f", ":", "\n", "        ", "for", "sent", "in", "train_sentences", ":", "\n", "            ", "f", ".", "write", "(", "'\\t'", ".", "join", "(", "sent", ")", "+", "'\\n'", ")", "\n", "", "", "for", "split", "in", "[", "'dev'", ",", "'test'", "]", ":", "\n", "        ", "if", "split", "in", "sentences", ":", "\n", "            ", "split_sentences", "=", "sentences", "[", "split", "]", "\n", "with", "open", "(", "writing_paths", "[", "split", "]", ",", "'w'", ")", "as", "f", ":", "\n", "                ", "for", "sent", "in", "split_sentences", ":", "\n", "                    ", "f", ".", "write", "(", "'\\t'", ".", "join", "(", "sent", ")", "+", "'\\n'", ")", "\n", "", "", "", "", "return", "writing_paths", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.write_extra_labels.add_head": [[628, 787], ["print", "writing_paths[].replace", "open", "write_extra_labels.get_split", "open", "f.write", "os.listdir", "os.listdir", "line.strip.strip", "line.strip.split", "lines.append", "open", "os.listdir", "file.endswith", "os.listdir", "file.endswith", "len", "range", "sentences_list.append", "f.write", "file.endswith", "file.endswith", "lines[].append", "len", "sentences_list.append", "sentences_list.append", "int"], "function", ["home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.open", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.write_extra_labels.get_split", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.open", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.writer.Writer.write", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.open", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.writer.Writer.write"], ["", "def", "add_head", "(", "model_path", ",", "parser_path", ",", "src_domain", ",", "tgt_domain", ",", "use_unlabeled_data", "=", "True", ",", "use_labeled_data", "=", "True", ")", ":", "\n", "# most of the code for this function is taken from:", "\n", "# https://github.com/mstrise/dep2label/blob/master/encoding.py", "\n", "    ", "def", "pos_cluster", "(", "pos", ")", ":", "\n", "# clustering the parts of speech", "\n", "        ", "if", "pos", "[", "0", "]", "==", "'V'", ":", "\n", "            ", "pos", "=", "'VB'", "\n", "", "elif", "pos", "==", "'NNS'", ":", "\n", "            ", "pos", "=", "'NN'", "\n", "", "elif", "pos", "==", "'NNPS'", ":", "\n", "            ", "pos", "=", "'NNP'", "\n", "", "elif", "'JJ'", "in", "pos", ":", "\n", "            ", "pos", "=", "'JJ'", "\n", "", "elif", "pos", "[", ":", "2", "]", "==", "'RB'", "or", "pos", "==", "'WRB'", "or", "pos", "==", "'RP'", ":", "\n", "            ", "pos", "=", "'RB'", "\n", "", "elif", "pos", "[", ":", "3", "]", "==", "'PRP'", ":", "\n", "            ", "pos", "=", "'PRP'", "\n", "", "elif", "pos", "in", "[", "'.'", ",", "':'", ",", "','", ",", "\"''\"", ",", "'``'", "]", ":", "\n", "            ", "pos", "=", "'.'", "\n", "", "elif", "pos", "[", "0", "]", "==", "'-'", ":", "\n", "            ", "pos", "=", "'-RB-'", "\n", "", "elif", "pos", "[", ":", "2", "]", "==", "'WP'", ":", "\n", "            ", "pos", "=", "'WP'", "\n", "", "return", "pos", "\n", "\n", "", "if", "src_domain", "==", "tgt_domain", ":", "\n", "        ", "pred_paths", "=", "[", "]", "\n", "if", "use_unlabeled_data", ":", "\n", "            ", "pred_paths", "=", "[", "file", "for", "file", "in", "os", ".", "listdir", "(", "parser_path", ")", "if", "\n", "file", ".", "endswith", "(", "\"pred.txt\"", ")", "and", "'extra'", "in", "file", "and", "tgt_domain", "in", "file", "]", "\n", "\n", "", "gold_paths", "=", "[", "file", "for", "file", "in", "os", ".", "listdir", "(", "parser_path", ")", "if", "\n", "file", ".", "endswith", "(", "\"gold.txt\"", ")", "and", "'extra'", "not", "in", "file", "and", "tgt_domain", "in", "file", "and", "'train'", "not", "in", "file", "]", "\n", "if", "use_labeled_data", ":", "\n", "            ", "gold_paths", "+=", "[", "file", "for", "file", "in", "os", ".", "listdir", "(", "parser_path", ")", "if", "\n", "file", ".", "endswith", "(", "\"gold.txt\"", ")", "and", "'extra'", "not", "in", "file", "and", "tgt_domain", "in", "file", "and", "'train'", "in", "file", "]", "\n", "\n", "", "if", "not", "use_unlabeled_data", "and", "not", "use_labeled_data", ":", "\n", "            ", "raise", "ValueError", "\n", "", "", "else", ":", "\n", "        ", "pred_paths", "=", "[", "file", "for", "file", "in", "os", ".", "listdir", "(", "parser_path", ")", "if", "file", ".", "endswith", "(", "\"pred.txt\"", ")", "and", "tgt_domain", "in", "file", "]", "\n", "gold_paths", "=", "[", "]", "\n", "if", "use_labeled_data", ":", "\n", "            ", "gold_paths", "=", "[", "'data/onto_pos_ner_dp_train_'", "+", "src_domain", "]", "\n", "\n", "", "if", "not", "use_unlabeled_data", ":", "\n", "            ", "raise", "ValueError", "\n", "\n", "", "", "paths", "=", "pred_paths", "+", "gold_paths", "\n", "print", "(", "\"Adding labels to paths: %s\"", "%", "', '", ".", "join", "(", "paths", ")", ")", "\n", "root_line", "=", "[", "'0'", ",", "ROOT", ",", "'XX'", ",", "'O'", ",", "'0'", ",", "'root'", ",", "'+0_XX'", "]", "\n", "writing_paths", "=", "{", "}", "\n", "sentences", "=", "{", "}", "\n", "for", "path", "in", "paths", ":", "\n", "        ", "if", "tgt_domain", "in", "path", ":", "\n", "            ", "reading_path", "=", "parser_path", "+", "path", "\n", "writing_path", "=", "model_path", "+", "'parser_'", "+", "path", "\n", "split", "=", "get_split", "(", "writing_path", ")", "\n", "", "else", ":", "\n", "            ", "reading_path", "=", "path", "\n", "writing_path", "=", "model_path", "+", "'parser_'", "+", "'domain_'", "+", "src_domain", "+", "'_train_model_domain_'", "+", "src_domain", "+", "'_data_domain_'", "+", "src_domain", "+", "'_gold.txt'", "\n", "split", "=", "'extra_train'", "\n", "", "writing_paths", "[", "split", "]", "=", "writing_path", "\n", "len_sent", "=", "0", "\n", "tree_dict", "=", "{", "'0'", ":", "'0'", "}", "\n", "lines", "=", "[", "]", "\n", "sentences_list", "=", "[", "]", "\n", "with", "open", "(", "reading_path", ",", "'r'", ")", "as", "file", ":", "\n", "\n", "            ", "for", "line", "in", "file", ":", "\n", "# print(line)", "\n", "# print(reading_path)", "\n", "# line = line.decode('utf-8')", "\n", "                ", "line", "=", "line", ".", "strip", "(", ")", "\n", "if", "len", "(", "line", ")", "==", "0", ":", "\n", "                    ", "for", "idx", "in", "range", "(", "len_sent", ")", ":", "\n", "                        ", "info_of_a_word", "=", "lines", "[", "idx", "]", "\n", "# head is on the right side from the word", "\n", "head", "=", "int", "(", "info_of_a_word", "[", "4", "]", ")", "-", "1", "\n", "if", "head", "==", "-", "1", ":", "\n", "                            ", "info_about_head", "=", "root_line", "\n", "", "else", ":", "\n", "# print(len(lines), head)", "\n", "                            ", "info_about_head", "=", "lines", "[", "head", "]", "\n", "", "head_word", "=", "info_about_head", "[", "1", "]", "\n", "lines", "[", "idx", "]", ".", "append", "(", "head_word", ")", "\n", "# if idx < head:", "\n", "#     relative_position_head = 1", "\n", "\n", "\n", "#     for x in range(idx + 1, head):", "\n", "#         another_word = lines[x]", "\n", "#         postag_word_before_head = pos_cluster(another_word[2])", "\n", "#         if postag_word_before_head == postag_head:", "\n", "#             relative_position_head += 1", "\n", "#     label = str(", "\n", "#         \"+\" +", "\n", "#         repr(relative_position_head) +", "\n", "#         \"_\" +", "\n", "#         postag_head)", "\n", "\n", "\n", "# # head is on the left side from the word", "\n", "# elif idx > head:", "\n", "#     relative_position_head = 1", "\n", "#     postag_head = pos_cluster(info_about_head[2])", "\n", "#     for x in range(head + 1, idx):", "\n", "#         another_word = lines[x]", "\n", "#         postag_word_before_head = pos_cluster(another_word[2])", "\n", "#         if postag_word_before_head == postag_head:", "\n", "#             relative_position_head += 1", "\n", "#     label = str(", "\n", "#         \"-\" +", "\n", "#         repr(relative_position_head) +", "\n", "#         \"_\" +", "\n", "#         postag_head)", "\n", "#     lines[idx].append(label)", "\n", "", "if", "len", "(", "lines", ")", ">", "0", ":", "\n", "                        ", "sentences_list", ".", "append", "(", "root_line", ")", "\n", "", "for", "line_", "in", "lines", ":", "\n", "                        ", "sentences_list", ".", "append", "(", "line_", ")", "\n", "", "sentences_list", ".", "append", "(", "[", "]", ")", "\n", "lines", "=", "[", "]", "\n", "tree_dict", "=", "{", "'0'", ":", "'0'", "}", "\n", "len_sent", "=", "0", "\n", "continue", "\n", "", "tokens", "=", "line", ".", "split", "(", "'\\t'", ")", "\n", "idx", "=", "tokens", "[", "0", "]", "\n", "word", "=", "tokens", "[", "1", "]", "\n", "pos", "=", "tokens", "[", "2", "]", "\n", "ner", "=", "tokens", "[", "3", "]", "\n", "head", "=", "tokens", "[", "4", "]", "\n", "arc_tag", "=", "tokens", "[", "5", "]", "\n", "tree_dict", "[", "idx", "]", "=", "head", "\n", "lines", ".", "append", "(", "[", "idx", ",", "word", ",", "pos", ",", "ner", ",", "head", ",", "arc_tag", "]", ")", "\n", "len_sent", "+=", "1", "\n", "", "", "sentences", "[", "split", "]", "=", "sentences_list", "\n", "\n", "", "train_sentences", "=", "[", "]", "\n", "if", "'train'", "in", "sentences", ":", "\n", "        ", "train_sentences", "=", "sentences", "[", "'train'", "]", "\n", "", "else", ":", "\n", "        ", "writing_paths", "[", "'train'", "]", "=", "writing_paths", "[", "'extra_train'", "]", ".", "replace", "(", "'extra_train'", ",", "'train'", ")", "\n", "", "if", "'extra_train'", "in", "sentences", ":", "\n", "        ", "train_sentences", "+=", "sentences", "[", "'extra_train'", "]", "\n", "del", "writing_paths", "[", "'extra_train'", "]", "\n", "", "if", "'extra_dev'", "in", "sentences", ":", "\n", "        ", "train_sentences", "+=", "sentences", "[", "'extra_dev'", "]", "\n", "del", "writing_paths", "[", "'extra_dev'", "]", "\n", "", "with", "open", "(", "writing_paths", "[", "'train'", "]", ",", "'w'", ")", "as", "f", ":", "\n", "        ", "for", "sent", "in", "train_sentences", ":", "\n", "            ", "f", ".", "write", "(", "'\\t'", ".", "join", "(", "sent", ")", "+", "'\\n'", ")", "\n", "", "", "for", "split", "in", "[", "'dev'", ",", "'test'", "]", ":", "\n", "        ", "if", "split", "in", "sentences", ":", "\n", "            ", "split_sentences", "=", "sentences", "[", "split", "]", "\n", "with", "open", "(", "writing_paths", "[", "split", "]", ",", "'w'", ")", "as", "f", ":", "\n", "                ", "for", "sent", "in", "split_sentences", ":", "\n", "                    ", "f", ".", "write", "(", "'\\t'", ".", "join", "(", "sent", ")", "+", "'\\n'", ")", "\n", "", "", "", "", "return", "writing_paths", "\n", "", "import", "json", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.write_extra_labels.get_modified_coarse": [[788, 795], ["ma.replace().replace.replace().replace", "json.load.keys", "open", "json.load", "ma.replace().replace.replace"], "function", ["home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.keys", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.open", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.load"], ["def", "get_modified_coarse", "(", "ma", ")", ":", "\n", "    ", "ma", "=", "ma", ".", "replace", "(", "'sgpl'", ",", "'sg'", ")", ".", "replace", "(", "'sgdu'", ",", "'sg'", ")", "\n", "with", "open", "(", "'/home/jivnesh/DCST_scratch/utils/io_/coarse_to_ma_dict.json'", ",", "'r'", ")", "as", "fh", ":", "\n", "        ", "coarse_dict", "=", "json", ".", "load", "(", "fh", ")", "\n", "", "for", "key", "in", "coarse_dict", ".", "keys", "(", ")", ":", "\n", "        ", "if", "ma", "in", "coarse_dict", "[", "key", "]", ":", "\n", "            ", "return", "key", "\n", "", "", "", "def", "add_head_coarse_pos", "(", "model_path", ",", "parser_path", ",", "src_domain", ",", "tgt_domain", ",", "use_unlabeled_data", "=", "True", ",", "use_labeled_data", "=", "True", ")", ":", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.write_extra_labels.add_head_coarse_pos": [[795, 923], ["print", "writing_paths[].replace", "open", "write_extra_labels.get_split", "open", "f.write", "os.listdir", "os.listdir", "line.strip.strip", "line.strip.split", "lines.append", "open", "os.listdir", "file.endswith", "os.listdir", "file.endswith", "len", "range", "sentences_list.append", "f.write", "file.endswith", "file.endswith", "lines[].append", "len", "sentences_list.append", "sentences_list.append", "int"], "function", ["home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.open", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.write_extra_labels.get_split", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.open", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.writer.Writer.write", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.open", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.writer.Writer.write"], ["", "", "", "def", "add_head_coarse_pos", "(", "model_path", ",", "parser_path", ",", "src_domain", ",", "tgt_domain", ",", "use_unlabeled_data", "=", "True", ",", "use_labeled_data", "=", "True", ")", ":", "\n", "# most of the code for this function is taken from:", "\n", "# https://github.com/mstrise/dep2label/blob/master/encoding.py", "\n", "    ", "def", "pos_cluster", "(", "pos", ")", ":", "\n", "# clustering the parts of speech", "\n", "        ", "if", "pos", "[", "0", "]", "==", "'V'", ":", "\n", "            ", "pos", "=", "'VB'", "\n", "", "elif", "pos", "==", "'NNS'", ":", "\n", "            ", "pos", "=", "'NN'", "\n", "", "elif", "pos", "==", "'NNPS'", ":", "\n", "            ", "pos", "=", "'NNP'", "\n", "", "elif", "'JJ'", "in", "pos", ":", "\n", "            ", "pos", "=", "'JJ'", "\n", "", "elif", "pos", "[", ":", "2", "]", "==", "'RB'", "or", "pos", "==", "'WRB'", "or", "pos", "==", "'RP'", ":", "\n", "            ", "pos", "=", "'RB'", "\n", "", "elif", "pos", "[", ":", "3", "]", "==", "'PRP'", ":", "\n", "            ", "pos", "=", "'PRP'", "\n", "", "elif", "pos", "in", "[", "'.'", ",", "':'", ",", "','", ",", "\"''\"", ",", "'``'", "]", ":", "\n", "            ", "pos", "=", "'.'", "\n", "", "elif", "pos", "[", "0", "]", "==", "'-'", ":", "\n", "            ", "pos", "=", "'-RB-'", "\n", "", "elif", "pos", "[", ":", "2", "]", "==", "'WP'", ":", "\n", "            ", "pos", "=", "'WP'", "\n", "", "return", "pos", "\n", "\n", "", "if", "src_domain", "==", "tgt_domain", ":", "\n", "        ", "pred_paths", "=", "[", "]", "\n", "if", "use_unlabeled_data", ":", "\n", "            ", "pred_paths", "=", "[", "file", "for", "file", "in", "os", ".", "listdir", "(", "parser_path", ")", "if", "\n", "file", ".", "endswith", "(", "\"pred.txt\"", ")", "and", "'extra'", "in", "file", "and", "tgt_domain", "in", "file", "]", "\n", "\n", "", "gold_paths", "=", "[", "file", "for", "file", "in", "os", ".", "listdir", "(", "parser_path", ")", "if", "\n", "file", ".", "endswith", "(", "\"gold.txt\"", ")", "and", "'extra'", "not", "in", "file", "and", "tgt_domain", "in", "file", "and", "'train'", "not", "in", "file", "]", "\n", "if", "use_labeled_data", ":", "\n", "            ", "gold_paths", "+=", "[", "file", "for", "file", "in", "os", ".", "listdir", "(", "parser_path", ")", "if", "\n", "file", ".", "endswith", "(", "\"gold.txt\"", ")", "and", "'extra'", "not", "in", "file", "and", "tgt_domain", "in", "file", "and", "'train'", "in", "file", "]", "\n", "\n", "", "if", "not", "use_unlabeled_data", "and", "not", "use_labeled_data", ":", "\n", "            ", "raise", "ValueError", "\n", "", "", "else", ":", "\n", "        ", "pred_paths", "=", "[", "file", "for", "file", "in", "os", ".", "listdir", "(", "parser_path", ")", "if", "file", ".", "endswith", "(", "\"pred.txt\"", ")", "and", "tgt_domain", "in", "file", "]", "\n", "gold_paths", "=", "[", "]", "\n", "if", "use_labeled_data", ":", "\n", "            ", "gold_paths", "=", "[", "'data/onto_pos_ner_dp_train_'", "+", "src_domain", "]", "\n", "\n", "", "if", "not", "use_unlabeled_data", ":", "\n", "            ", "raise", "ValueError", "\n", "\n", "", "", "paths", "=", "pred_paths", "+", "gold_paths", "\n", "print", "(", "\"Adding labels to paths: %s\"", "%", "', '", ".", "join", "(", "paths", ")", ")", "\n", "root_line", "=", "[", "'0'", ",", "ROOT", ",", "'XX'", ",", "'O'", ",", "'0'", ",", "'root'", ",", "'O'", "]", "\n", "writing_paths", "=", "{", "}", "\n", "sentences", "=", "{", "}", "\n", "for", "path", "in", "paths", ":", "\n", "        ", "if", "tgt_domain", "in", "path", ":", "\n", "            ", "reading_path", "=", "parser_path", "+", "path", "\n", "writing_path", "=", "model_path", "+", "'parser_'", "+", "path", "\n", "split", "=", "get_split", "(", "writing_path", ")", "\n", "", "else", ":", "\n", "            ", "reading_path", "=", "path", "\n", "writing_path", "=", "model_path", "+", "'parser_'", "+", "'domain_'", "+", "src_domain", "+", "'_train_model_domain_'", "+", "src_domain", "+", "'_data_domain_'", "+", "src_domain", "+", "'_gold.txt'", "\n", "split", "=", "'extra_train'", "\n", "", "writing_paths", "[", "split", "]", "=", "writing_path", "\n", "len_sent", "=", "0", "\n", "tree_dict", "=", "{", "'0'", ":", "'0'", "}", "\n", "lines", "=", "[", "]", "\n", "sentences_list", "=", "[", "]", "\n", "with", "open", "(", "reading_path", ",", "'r'", ")", "as", "file", ":", "\n", "\n", "            ", "for", "line", "in", "file", ":", "\n", "# print(line)", "\n", "# print(reading_path)", "\n", "# line = line.decode('utf-8')", "\n", "                ", "line", "=", "line", ".", "strip", "(", ")", "\n", "if", "len", "(", "line", ")", "==", "0", ":", "\n", "                    ", "for", "idx", "in", "range", "(", "len_sent", ")", ":", "\n", "                        ", "info_of_a_word", "=", "lines", "[", "idx", "]", "\n", "# head is on the right side from the word", "\n", "head", "=", "int", "(", "info_of_a_word", "[", "4", "]", ")", "-", "1", "\n", "if", "head", "==", "-", "1", ":", "\n", "                            ", "info_about_head", "=", "root_line", "\n", "", "else", ":", "\n", "# print(len(lines), head)", "\n", "                            ", "info_about_head", "=", "lines", "[", "head", "]", "\n", "", "postag_head", "=", "info_about_head", "[", "2", "]", "\n", "lines", "[", "idx", "]", ".", "append", "(", "postag_head", ")", "\n", "", "if", "len", "(", "lines", ")", ">", "0", ":", "\n", "                        ", "sentences_list", ".", "append", "(", "root_line", ")", "\n", "", "for", "line_", "in", "lines", ":", "\n", "                        ", "sentences_list", ".", "append", "(", "line_", ")", "\n", "", "sentences_list", ".", "append", "(", "[", "]", ")", "\n", "lines", "=", "[", "]", "\n", "tree_dict", "=", "{", "'0'", ":", "'0'", "}", "\n", "len_sent", "=", "0", "\n", "continue", "\n", "", "tokens", "=", "line", ".", "split", "(", "'\\t'", ")", "\n", "idx", "=", "tokens", "[", "0", "]", "\n", "word", "=", "tokens", "[", "1", "]", "\n", "pos", "=", "tokens", "[", "2", "]", "\n", "ner", "=", "tokens", "[", "3", "]", "\n", "head", "=", "tokens", "[", "4", "]", "\n", "arc_tag", "=", "tokens", "[", "5", "]", "\n", "tree_dict", "[", "idx", "]", "=", "head", "\n", "lines", ".", "append", "(", "[", "idx", ",", "word", ",", "pos", ",", "ner", ",", "head", ",", "arc_tag", "]", ")", "\n", "len_sent", "+=", "1", "\n", "", "", "sentences", "[", "split", "]", "=", "sentences_list", "\n", "\n", "", "train_sentences", "=", "[", "]", "\n", "if", "'train'", "in", "sentences", ":", "\n", "        ", "train_sentences", "=", "sentences", "[", "'train'", "]", "\n", "", "else", ":", "\n", "        ", "writing_paths", "[", "'train'", "]", "=", "writing_paths", "[", "'extra_train'", "]", ".", "replace", "(", "'extra_train'", ",", "'train'", ")", "\n", "", "if", "'extra_train'", "in", "sentences", ":", "\n", "        ", "train_sentences", "+=", "sentences", "[", "'extra_train'", "]", "\n", "del", "writing_paths", "[", "'extra_train'", "]", "\n", "", "if", "'extra_dev'", "in", "sentences", ":", "\n", "        ", "train_sentences", "+=", "sentences", "[", "'extra_dev'", "]", "\n", "del", "writing_paths", "[", "'extra_dev'", "]", "\n", "", "with", "open", "(", "writing_paths", "[", "'train'", "]", ",", "'w'", ")", "as", "f", ":", "\n", "        ", "for", "sent", "in", "train_sentences", ":", "\n", "            ", "f", ".", "write", "(", "'\\t'", ".", "join", "(", "sent", ")", "+", "'\\n'", ")", "\n", "", "", "for", "split", "in", "[", "'dev'", ",", "'test'", "]", ":", "\n", "        ", "if", "split", "in", "sentences", ":", "\n", "            ", "split_sentences", "=", "sentences", "[", "split", "]", "\n", "with", "open", "(", "writing_paths", "[", "split", "]", ",", "'w'", ")", "as", "f", ":", "\n", "                ", "for", "sent", "in", "split_sentences", ":", "\n", "                    ", "f", ".", "write", "(", "'\\t'", ".", "join", "(", "sent", ")", "+", "'\\n'", ")", "\n", "", "", "", "", "return", "writing_paths", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.write_extra_labels.add_head_ma": [[924, 1052], ["print", "writing_paths[].replace", "open", "write_extra_labels.get_split", "open", "f.write", "os.listdir", "os.listdir", "line.strip.strip", "line.strip.split", "lines.append", "open", "os.listdir", "file.endswith", "os.listdir", "file.endswith", "len", "range", "sentences_list.append", "f.write", "file.endswith", "file.endswith", "write_extra_labels.add_relative_pos_based.pos_cluster"], "function", ["home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.open", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.write_extra_labels.get_split", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.open", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.writer.Writer.write", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.open", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.writer.Writer.write"], ["", "def", "add_head_ma", "(", "model_path", ",", "parser_path", ",", "src_domain", ",", "tgt_domain", ",", "use_unlabeled_data", "=", "True", ",", "use_labeled_data", "=", "True", ")", ":", "\n", "# most of the code for this function is taken from:", "\n", "# https://github.com/mstrise/dep2label/blob/master/encoding.py", "\n", "    ", "def", "pos_cluster", "(", "pos", ")", ":", "\n", "# clustering the parts of speech", "\n", "        ", "if", "pos", "[", "0", "]", "==", "'V'", ":", "\n", "            ", "pos", "=", "'VB'", "\n", "", "elif", "pos", "==", "'NNS'", ":", "\n", "            ", "pos", "=", "'NN'", "\n", "", "elif", "pos", "==", "'NNPS'", ":", "\n", "            ", "pos", "=", "'NNP'", "\n", "", "elif", "'JJ'", "in", "pos", ":", "\n", "            ", "pos", "=", "'JJ'", "\n", "", "elif", "pos", "[", ":", "2", "]", "==", "'RB'", "or", "pos", "==", "'WRB'", "or", "pos", "==", "'RP'", ":", "\n", "            ", "pos", "=", "'RB'", "\n", "", "elif", "pos", "[", ":", "3", "]", "==", "'PRP'", ":", "\n", "            ", "pos", "=", "'PRP'", "\n", "", "elif", "pos", "in", "[", "'.'", ",", "':'", ",", "','", ",", "\"''\"", ",", "'``'", "]", ":", "\n", "            ", "pos", "=", "'.'", "\n", "", "elif", "pos", "[", "0", "]", "==", "'-'", ":", "\n", "            ", "pos", "=", "'-RB-'", "\n", "", "elif", "pos", "[", ":", "2", "]", "==", "'WP'", ":", "\n", "            ", "pos", "=", "'WP'", "\n", "", "return", "pos", "\n", "\n", "", "if", "src_domain", "==", "tgt_domain", ":", "\n", "        ", "pred_paths", "=", "[", "]", "\n", "if", "use_unlabeled_data", ":", "\n", "            ", "pred_paths", "=", "[", "file", "for", "file", "in", "os", ".", "listdir", "(", "parser_path", ")", "if", "\n", "file", ".", "endswith", "(", "\"pred.txt\"", ")", "and", "'extra'", "in", "file", "and", "tgt_domain", "in", "file", "]", "\n", "\n", "", "gold_paths", "=", "[", "file", "for", "file", "in", "os", ".", "listdir", "(", "parser_path", ")", "if", "\n", "file", ".", "endswith", "(", "\"gold.txt\"", ")", "and", "'extra'", "not", "in", "file", "and", "tgt_domain", "in", "file", "and", "'train'", "not", "in", "file", "]", "\n", "if", "use_labeled_data", ":", "\n", "            ", "gold_paths", "+=", "[", "file", "for", "file", "in", "os", ".", "listdir", "(", "parser_path", ")", "if", "\n", "file", ".", "endswith", "(", "\"gold.txt\"", ")", "and", "'extra'", "not", "in", "file", "and", "tgt_domain", "in", "file", "and", "'train'", "in", "file", "]", "\n", "\n", "", "if", "not", "use_unlabeled_data", "and", "not", "use_labeled_data", ":", "\n", "            ", "raise", "ValueError", "\n", "", "", "else", ":", "\n", "        ", "pred_paths", "=", "[", "file", "for", "file", "in", "os", ".", "listdir", "(", "parser_path", ")", "if", "file", ".", "endswith", "(", "\"pred.txt\"", ")", "and", "tgt_domain", "in", "file", "]", "\n", "gold_paths", "=", "[", "]", "\n", "if", "use_labeled_data", ":", "\n", "            ", "gold_paths", "=", "[", "'data/onto_pos_ner_dp_train_'", "+", "src_domain", "]", "\n", "\n", "", "if", "not", "use_unlabeled_data", ":", "\n", "            ", "raise", "ValueError", "\n", "\n", "", "", "paths", "=", "pred_paths", "+", "gold_paths", "\n", "print", "(", "\"Adding labels to paths: %s\"", "%", "', '", ".", "join", "(", "paths", ")", ")", "\n", "root_line", "=", "[", "'0'", ",", "ROOT", ",", "'XX'", ",", "'O'", ",", "'0'", ",", "'root'", ",", "'XX'", "]", "\n", "writing_paths", "=", "{", "}", "\n", "sentences", "=", "{", "}", "\n", "for", "path", "in", "paths", ":", "\n", "        ", "if", "tgt_domain", "in", "path", ":", "\n", "            ", "reading_path", "=", "parser_path", "+", "path", "\n", "writing_path", "=", "model_path", "+", "'parser_'", "+", "path", "\n", "split", "=", "get_split", "(", "writing_path", ")", "\n", "", "else", ":", "\n", "            ", "reading_path", "=", "path", "\n", "writing_path", "=", "model_path", "+", "'parser_'", "+", "'domain_'", "+", "src_domain", "+", "'_train_model_domain_'", "+", "src_domain", "+", "'_data_domain_'", "+", "src_domain", "+", "'_gold.txt'", "\n", "split", "=", "'extra_train'", "\n", "", "writing_paths", "[", "split", "]", "=", "writing_path", "\n", "len_sent", "=", "0", "\n", "tree_dict", "=", "{", "'0'", ":", "'0'", "}", "\n", "lines", "=", "[", "]", "\n", "sentences_list", "=", "[", "]", "\n", "with", "open", "(", "reading_path", ",", "'r'", ")", "as", "file", ":", "\n", "\n", "            ", "for", "line", "in", "file", ":", "\n", "# print(line)", "\n", "# print(reading_path)", "\n", "# line = line.decode('utf-8')", "\n", "                ", "line", "=", "line", ".", "strip", "(", ")", "\n", "if", "len", "(", "line", ")", "==", "0", ":", "\n", "                    ", "for", "idx", "in", "range", "(", "len_sent", ")", ":", "\n", "                        ", "info_of_a_word", "=", "lines", "[", "idx", "]", "\n", "# head is on the right side from the word", "\n", "head", "=", "int", "(", "info_of_a_word", "[", "4", "]", ")", "-", "1", "\n", "if", "head", "==", "-", "1", ":", "\n", "                            ", "info_about_head", "=", "root_line", "\n", "", "else", ":", "\n", "# print(len(lines), head)", "\n", "                            ", "info_about_head", "=", "lines", "[", "head", "]", "\n", "", "postag_head", "=", "pos_cluster", "(", "info_about_head", "[", "2", "]", ")", "\n", "lines", "[", "idx", "]", ".", "append", "(", "postag_head", ")", "\n", "", "if", "len", "(", "lines", ")", ">", "0", ":", "\n", "                        ", "sentences_list", ".", "append", "(", "root_line", ")", "\n", "", "for", "line_", "in", "lines", ":", "\n", "                        ", "sentences_list", ".", "append", "(", "line_", ")", "\n", "", "sentences_list", ".", "append", "(", "[", "]", ")", "\n", "lines", "=", "[", "]", "\n", "tree_dict", "=", "{", "'0'", ":", "'0'", "}", "\n", "len_sent", "=", "0", "\n", "continue", "\n", "", "tokens", "=", "line", ".", "split", "(", "'\\t'", ")", "\n", "idx", "=", "tokens", "[", "0", "]", "\n", "word", "=", "tokens", "[", "1", "]", "\n", "pos", "=", "tokens", "[", "2", "]", "\n", "ner", "=", "tokens", "[", "3", "]", "\n", "head", "=", "tokens", "[", "4", "]", "\n", "arc_tag", "=", "tokens", "[", "5", "]", "\n", "tree_dict", "[", "idx", "]", "=", "head", "\n", "lines", ".", "append", "(", "[", "idx", ",", "word", ",", "pos", ",", "ner", ",", "head", ",", "arc_tag", "]", ")", "\n", "len_sent", "+=", "1", "\n", "", "", "sentences", "[", "split", "]", "=", "sentences_list", "\n", "\n", "", "train_sentences", "=", "[", "]", "\n", "if", "'train'", "in", "sentences", ":", "\n", "        ", "train_sentences", "=", "sentences", "[", "'train'", "]", "\n", "", "else", ":", "\n", "        ", "writing_paths", "[", "'train'", "]", "=", "writing_paths", "[", "'extra_train'", "]", ".", "replace", "(", "'extra_train'", ",", "'train'", ")", "\n", "", "if", "'extra_train'", "in", "sentences", ":", "\n", "        ", "train_sentences", "+=", "sentences", "[", "'extra_train'", "]", "\n", "del", "writing_paths", "[", "'extra_train'", "]", "\n", "", "if", "'extra_dev'", "in", "sentences", ":", "\n", "        ", "train_sentences", "+=", "sentences", "[", "'extra_dev'", "]", "\n", "del", "writing_paths", "[", "'extra_dev'", "]", "\n", "", "with", "open", "(", "writing_paths", "[", "'train'", "]", ",", "'w'", ")", "as", "f", ":", "\n", "        ", "for", "sent", "in", "train_sentences", ":", "\n", "            ", "f", ".", "write", "(", "'\\t'", ".", "join", "(", "sent", ")", "+", "'\\n'", ")", "\n", "", "", "for", "split", "in", "[", "'dev'", ",", "'test'", "]", ":", "\n", "        ", "if", "split", "in", "sentences", ":", "\n", "            ", "split_sentences", "=", "sentences", "[", "split", "]", "\n", "with", "open", "(", "writing_paths", "[", "split", "]", ",", "'w'", ")", "as", "f", ":", "\n", "                ", "for", "sent", "in", "split_sentences", ":", "\n", "                    ", "f", ".", "write", "(", "'\\t'", ".", "join", "(", "sent", ")", "+", "'\\n'", ")", "\n", "", "", "", "", "return", "writing_paths", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.write_extra_labels.add_label": [[1053, 1149], ["print", "print", "writing_paths[].replace", "open", "write_extra_labels.get_split", "open", "f.write", "os.listdir", "os.listdir", "line.strip.strip", "line.strip.split", "lines.append", "open", "os.listdir", "file.endswith", "os.listdir", "file.endswith", "len", "range", "sentences_list.append", "f.write", "file.endswith", "file.endswith", "lines[].append", "len", "sentences_list.append", "sentences_list.append"], "function", ["home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.open", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.write_extra_labels.get_split", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.open", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.writer.Writer.write", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.open", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.writer.Writer.write"], ["", "def", "add_label", "(", "model_path", ",", "parser_path", ",", "src_domain", ",", "tgt_domain", ",", "use_unlabeled_data", "=", "True", ",", "use_labeled_data", "=", "True", ")", ":", "\n", "    ", "if", "src_domain", "==", "tgt_domain", ":", "\n", "        ", "pred_paths", "=", "[", "]", "\n", "if", "use_unlabeled_data", ":", "\n", "            ", "pred_paths", "=", "[", "file", "for", "file", "in", "os", ".", "listdir", "(", "parser_path", ")", "if", "\n", "file", ".", "endswith", "(", "\"pred.txt\"", ")", "and", "'extra'", "in", "file", "and", "tgt_domain", "in", "file", "]", "\n", "\n", "", "gold_paths", "=", "[", "file", "for", "file", "in", "os", ".", "listdir", "(", "parser_path", ")", "if", "\n", "file", ".", "endswith", "(", "\"gold.txt\"", ")", "and", "'extra'", "not", "in", "file", "and", "tgt_domain", "in", "file", "and", "'train'", "not", "in", "file", "]", "\n", "if", "use_labeled_data", ":", "\n", "            ", "gold_paths", "+=", "[", "file", "for", "file", "in", "os", ".", "listdir", "(", "parser_path", ")", "if", "\n", "file", ".", "endswith", "(", "\"gold.txt\"", ")", "and", "'extra'", "not", "in", "file", "and", "tgt_domain", "in", "file", "and", "'train'", "in", "file", "]", "\n", "\n", "", "if", "not", "use_unlabeled_data", "and", "not", "use_labeled_data", ":", "\n", "            ", "raise", "ValueError", "\n", "", "", "else", ":", "\n", "        ", "pred_paths", "=", "[", "file", "for", "file", "in", "os", ".", "listdir", "(", "parser_path", ")", "if", "file", ".", "endswith", "(", "\"pred.txt\"", ")", "and", "tgt_domain", "in", "file", "]", "\n", "\n", "gold_paths", "=", "[", "]", "\n", "if", "use_labeled_data", ":", "\n", "            ", "gold_paths", "=", "[", "'data/onto_pos_ner_dp_train_'", "+", "src_domain", "]", "\n", "\n", "", "if", "not", "use_unlabeled_data", "and", "not", "use_labeled_data", ":", "\n", "            ", "raise", "ValueError", "\n", "\n", "", "", "paths", "=", "pred_paths", "+", "gold_paths", "\n", "print", "(", "'############ Add Label Task #################'", ")", "\n", "print", "(", "\"Adding labels to paths: %s\"", "%", "', '", ".", "join", "(", "paths", ")", ")", "\n", "root_line", "=", "[", "'0'", ",", "ROOT", ",", "'XX'", ",", "'O'", ",", "'0'", ",", "'root'", "]", "\n", "writing_paths", "=", "{", "}", "\n", "sentences", "=", "{", "}", "\n", "for", "path", "in", "paths", ":", "\n", "        ", "if", "tgt_domain", "in", "path", ":", "\n", "            ", "reading_path", "=", "parser_path", "+", "path", "\n", "writing_path", "=", "model_path", "+", "'parser_'", "+", "path", "\n", "split", "=", "get_split", "(", "writing_path", ")", "\n", "", "else", ":", "\n", "            ", "reading_path", "=", "path", "\n", "writing_path", "=", "model_path", "+", "'parser_'", "+", "'domain_'", "+", "src_domain", "+", "'_train_model_domain_'", "+", "src_domain", "+", "'_data_domain_'", "+", "src_domain", "+", "'_gold.txt'", "\n", "split", "=", "'extra_train'", "\n", "", "writing_paths", "[", "split", "]", "=", "writing_path", "\n", "len_sent", "=", "0", "\n", "lines", "=", "[", "]", "\n", "sentences_list", "=", "[", "]", "\n", "with", "open", "(", "reading_path", ",", "'r'", ")", "as", "file", ":", "\n", "            ", "for", "line", "in", "file", ":", "\n", "# line = line.decode('utf-8')", "\n", "                ", "line", "=", "line", ".", "strip", "(", ")", "\n", "# Now blank space got detected", "\n", "if", "len", "(", "line", ")", "==", "0", ":", "\n", "# Append next word to last column", "\n", "                    ", "for", "idx", "in", "range", "(", "len_sent", ")", ":", "\n", "                            ", "lines", "[", "idx", "]", ".", "append", "(", "lines", "[", "idx", "]", "[", "5", "]", ")", "\n", "# Add root line first", "\n", "", "if", "len", "(", "lines", ")", ">", "0", ":", "\n", "                        ", "tmp_root_line", "=", "root_line", "+", "[", "root_line", "[", "5", "]", "]", "\n", "sentences_list", ".", "append", "(", "tmp_root_line", ")", "\n", "", "for", "line_", "in", "lines", ":", "\n", "                        ", "sentences_list", ".", "append", "(", "line_", ")", "\n", "", "sentences_list", ".", "append", "(", "[", "]", ")", "\n", "lines", "=", "[", "]", "\n", "len_sent", "=", "0", "\n", "continue", "\n", "", "tokens", "=", "line", ".", "split", "(", "'\\t'", ")", "\n", "idx", "=", "tokens", "[", "0", "]", "\n", "word", "=", "tokens", "[", "1", "]", "\n", "pos", "=", "tokens", "[", "2", "]", "\n", "ner", "=", "tokens", "[", "3", "]", "\n", "head", "=", "tokens", "[", "4", "]", "\n", "arc_tag", "=", "tokens", "[", "5", "]", "\n", "lines", ".", "append", "(", "[", "idx", ",", "word", ",", "pos", ",", "ner", ",", "head", ",", "arc_tag", "]", ")", "\n", "len_sent", "+=", "1", "\n", "", "", "sentences", "[", "split", "]", "=", "sentences_list", "\n", "\n", "", "train_sentences", "=", "[", "]", "\n", "if", "'train'", "in", "sentences", ":", "\n", "        ", "train_sentences", "=", "sentences", "[", "'train'", "]", "\n", "", "else", ":", "\n", "# pdb.set_trace()", "\n", "        ", "writing_paths", "[", "'train'", "]", "=", "writing_paths", "[", "'extra_train'", "]", ".", "replace", "(", "'extra_train'", ",", "'train'", ")", "\n", "", "if", "'extra_train'", "in", "sentences", ":", "\n", "        ", "train_sentences", "+=", "sentences", "[", "'extra_train'", "]", "\n", "del", "writing_paths", "[", "'extra_train'", "]", "\n", "", "if", "'extra_dev'", "in", "sentences", ":", "\n", "        ", "train_sentences", "+=", "sentences", "[", "'extra_dev'", "]", "\n", "del", "writing_paths", "[", "'extra_dev'", "]", "\n", "", "with", "open", "(", "writing_paths", "[", "'train'", "]", ",", "'w'", ")", "as", "f", ":", "\n", "        ", "for", "sent", "in", "train_sentences", ":", "\n", "            ", "f", ".", "write", "(", "'\\t'", ".", "join", "(", "sent", ")", "+", "'\\n'", ")", "\n", "", "", "for", "split", "in", "[", "'dev'", ",", "'test'", "]", ":", "\n", "        ", "if", "split", "in", "sentences", ":", "\n", "            ", "split_sentences", "=", "sentences", "[", "split", "]", "\n", "with", "open", "(", "writing_paths", "[", "split", "]", ",", "'w'", ")", "as", "f", ":", "\n", "                ", "for", "sent", "in", "split_sentences", ":", "\n", "                    ", "f", ".", "write", "(", "'\\t'", ".", "join", "(", "sent", ")", "+", "'\\n'", ")", "\n", "", "", "", "", "return", "writing_paths", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.write_extra_labels.predict_ma_tag_of_modifier": [[1150, 1245], ["print", "print", "writing_paths[].replace", "open", "write_extra_labels.get_split", "open", "f.write", "os.listdir", "os.listdir", "line.strip.strip", "line.strip.split", "lines.append", "open", "os.listdir", "file.endswith", "os.listdir", "file.endswith", "len", "range", "sentences_list.append", "f.write", "file.endswith", "file.endswith", "lines[].append", "len", "sentences_list.append", "sentences_list.append", "write_extra_labels.clean_ma"], "function", ["home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.open", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.write_extra_labels.get_split", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.open", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.writer.Writer.write", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.open", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.writer.Writer.write", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.write_extra_labels.clean_ma"], ["", "def", "predict_ma_tag_of_modifier", "(", "model_path", ",", "parser_path", ",", "src_domain", ",", "tgt_domain", ",", "use_unlabeled_data", "=", "True", ",", "use_labeled_data", "=", "True", ")", ":", "\n", "    ", "if", "src_domain", "==", "tgt_domain", ":", "\n", "        ", "pred_paths", "=", "[", "]", "\n", "if", "use_unlabeled_data", ":", "\n", "            ", "pred_paths", "=", "[", "file", "for", "file", "in", "os", ".", "listdir", "(", "parser_path", ")", "if", "\n", "file", ".", "endswith", "(", "\"pred.txt\"", ")", "and", "'extra'", "in", "file", "and", "tgt_domain", "in", "file", "]", "\n", "\n", "", "gold_paths", "=", "[", "file", "for", "file", "in", "os", ".", "listdir", "(", "parser_path", ")", "if", "\n", "file", ".", "endswith", "(", "\"gold.txt\"", ")", "and", "'extra'", "not", "in", "file", "and", "tgt_domain", "in", "file", "and", "'train'", "not", "in", "file", "]", "\n", "if", "use_labeled_data", ":", "\n", "            ", "gold_paths", "+=", "[", "file", "for", "file", "in", "os", ".", "listdir", "(", "parser_path", ")", "if", "\n", "file", ".", "endswith", "(", "\"gold.txt\"", ")", "and", "'extra'", "not", "in", "file", "and", "tgt_domain", "in", "file", "and", "'train'", "in", "file", "]", "\n", "\n", "", "if", "not", "use_unlabeled_data", "and", "not", "use_labeled_data", ":", "\n", "            ", "raise", "ValueError", "\n", "", "", "else", ":", "\n", "        ", "pred_paths", "=", "[", "file", "for", "file", "in", "os", ".", "listdir", "(", "parser_path", ")", "if", "file", ".", "endswith", "(", "\"pred.txt\"", ")", "and", "tgt_domain", "in", "file", "]", "\n", "\n", "gold_paths", "=", "[", "]", "\n", "if", "use_labeled_data", ":", "\n", "            ", "gold_paths", "=", "[", "'data/onto_pos_ner_dp_train_'", "+", "src_domain", "]", "\n", "\n", "", "if", "not", "use_unlabeled_data", "and", "not", "use_labeled_data", ":", "\n", "            ", "raise", "ValueError", "\n", "\n", "", "", "paths", "=", "pred_paths", "+", "gold_paths", "\n", "print", "(", "'############ Add Label Task #################'", ")", "\n", "print", "(", "\"Adding labels to paths: %s\"", "%", "', '", ".", "join", "(", "paths", ")", ")", "\n", "root_line", "=", "[", "'0'", ",", "ROOT", ",", "'XX'", ",", "'O'", ",", "'0'", ",", "'root'", "]", "\n", "writing_paths", "=", "{", "}", "\n", "sentences", "=", "{", "}", "\n", "for", "path", "in", "paths", ":", "\n", "        ", "if", "tgt_domain", "in", "path", ":", "\n", "            ", "reading_path", "=", "parser_path", "+", "path", "\n", "writing_path", "=", "model_path", "+", "'parser_'", "+", "path", "\n", "split", "=", "get_split", "(", "writing_path", ")", "\n", "", "else", ":", "\n", "            ", "reading_path", "=", "path", "\n", "writing_path", "=", "model_path", "+", "'parser_'", "+", "'domain_'", "+", "src_domain", "+", "'_train_model_domain_'", "+", "src_domain", "+", "'_data_domain_'", "+", "src_domain", "+", "'_gold.txt'", "\n", "split", "=", "'extra_train'", "\n", "", "writing_paths", "[", "split", "]", "=", "writing_path", "\n", "len_sent", "=", "0", "\n", "lines", "=", "[", "]", "\n", "sentences_list", "=", "[", "]", "\n", "with", "open", "(", "reading_path", ",", "'r'", ")", "as", "file", ":", "\n", "            ", "for", "line", "in", "file", ":", "\n", "# line = line.decode('utf-8')", "\n", "                ", "line", "=", "line", ".", "strip", "(", ")", "\n", "# Now blank space got detected", "\n", "if", "len", "(", "line", ")", "==", "0", ":", "\n", "# Append next word to last column", "\n", "                    ", "for", "idx", "in", "range", "(", "len_sent", ")", ":", "\n", "                            ", "lines", "[", "idx", "]", ".", "append", "(", "clean_ma", "(", "lines", "[", "idx", "]", "[", "3", "]", ")", ")", "\n", "# Add root line first", "\n", "", "if", "len", "(", "lines", ")", ">", "0", ":", "\n", "                        ", "tmp_root_line", "=", "root_line", "+", "[", "root_line", "[", "3", "]", "]", "\n", "sentences_list", ".", "append", "(", "tmp_root_line", ")", "\n", "", "for", "line_", "in", "lines", ":", "\n", "                        ", "sentences_list", ".", "append", "(", "line_", ")", "\n", "", "sentences_list", ".", "append", "(", "[", "]", ")", "\n", "lines", "=", "[", "]", "\n", "len_sent", "=", "0", "\n", "continue", "\n", "", "tokens", "=", "line", ".", "split", "(", "'\\t'", ")", "\n", "idx", "=", "tokens", "[", "0", "]", "\n", "word", "=", "tokens", "[", "1", "]", "\n", "pos", "=", "tokens", "[", "2", "]", "\n", "ner", "=", "tokens", "[", "3", "]", "\n", "head", "=", "tokens", "[", "4", "]", "\n", "arc_tag", "=", "tokens", "[", "5", "]", "\n", "lines", ".", "append", "(", "[", "idx", ",", "word", ",", "pos", ",", "ner", ",", "head", ",", "arc_tag", "]", ")", "\n", "len_sent", "+=", "1", "\n", "", "", "sentences", "[", "split", "]", "=", "sentences_list", "\n", "\n", "", "train_sentences", "=", "[", "]", "\n", "if", "'train'", "in", "sentences", ":", "\n", "        ", "train_sentences", "=", "sentences", "[", "'train'", "]", "\n", "", "else", ":", "\n", "        ", "writing_paths", "[", "'train'", "]", "=", "writing_paths", "[", "'extra_train'", "]", ".", "replace", "(", "'extra_train'", ",", "'train'", ")", "\n", "", "if", "'extra_train'", "in", "sentences", ":", "\n", "        ", "train_sentences", "+=", "sentences", "[", "'extra_train'", "]", "\n", "del", "writing_paths", "[", "'extra_train'", "]", "\n", "", "if", "'extra_dev'", "in", "sentences", ":", "\n", "        ", "train_sentences", "+=", "sentences", "[", "'extra_dev'", "]", "\n", "del", "writing_paths", "[", "'extra_dev'", "]", "\n", "", "with", "open", "(", "writing_paths", "[", "'train'", "]", ",", "'w'", ")", "as", "f", ":", "\n", "        ", "for", "sent", "in", "train_sentences", ":", "\n", "            ", "f", ".", "write", "(", "'\\t'", ".", "join", "(", "sent", ")", "+", "'\\n'", ")", "\n", "", "", "for", "split", "in", "[", "'dev'", ",", "'test'", "]", ":", "\n", "        ", "if", "split", "in", "sentences", ":", "\n", "            ", "split_sentences", "=", "sentences", "[", "split", "]", "\n", "with", "open", "(", "writing_paths", "[", "split", "]", ",", "'w'", ")", "as", "f", ":", "\n", "                ", "for", "sent", "in", "split_sentences", ":", "\n", "                    ", "f", ".", "write", "(", "'\\t'", ".", "join", "(", "sent", ")", "+", "'\\n'", ")", "\n", "", "", "", "", "return", "writing_paths", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.write_extra_labels.predict_coarse_of_modifier": [[1246, 1341], ["print", "print", "writing_paths[].replace", "open", "write_extra_labels.get_split", "open", "f.write", "os.listdir", "os.listdir", "line.strip.strip", "line.strip.split", "lines.append", "open", "os.listdir", "file.endswith", "os.listdir", "file.endswith", "len", "range", "sentences_list.append", "f.write", "file.endswith", "file.endswith", "lines[].append", "len", "sentences_list.append", "sentences_list.append"], "function", ["home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.open", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.write_extra_labels.get_split", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.open", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.writer.Writer.write", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.open", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.writer.Writer.write"], ["", "def", "predict_coarse_of_modifier", "(", "model_path", ",", "parser_path", ",", "src_domain", ",", "tgt_domain", ",", "use_unlabeled_data", "=", "True", ",", "use_labeled_data", "=", "True", ")", ":", "\n", "    ", "if", "src_domain", "==", "tgt_domain", ":", "\n", "        ", "pred_paths", "=", "[", "]", "\n", "if", "use_unlabeled_data", ":", "\n", "            ", "pred_paths", "=", "[", "file", "for", "file", "in", "os", ".", "listdir", "(", "parser_path", ")", "if", "\n", "file", ".", "endswith", "(", "\"pred.txt\"", ")", "and", "'extra'", "in", "file", "and", "tgt_domain", "in", "file", "]", "\n", "\n", "", "gold_paths", "=", "[", "file", "for", "file", "in", "os", ".", "listdir", "(", "parser_path", ")", "if", "\n", "file", ".", "endswith", "(", "\"gold.txt\"", ")", "and", "'extra'", "not", "in", "file", "and", "tgt_domain", "in", "file", "and", "'train'", "not", "in", "file", "]", "\n", "if", "use_labeled_data", ":", "\n", "            ", "gold_paths", "+=", "[", "file", "for", "file", "in", "os", ".", "listdir", "(", "parser_path", ")", "if", "\n", "file", ".", "endswith", "(", "\"gold.txt\"", ")", "and", "'extra'", "not", "in", "file", "and", "tgt_domain", "in", "file", "and", "'train'", "in", "file", "]", "\n", "\n", "", "if", "not", "use_unlabeled_data", "and", "not", "use_labeled_data", ":", "\n", "            ", "raise", "ValueError", "\n", "", "", "else", ":", "\n", "        ", "pred_paths", "=", "[", "file", "for", "file", "in", "os", ".", "listdir", "(", "parser_path", ")", "if", "file", ".", "endswith", "(", "\"pred.txt\"", ")", "and", "tgt_domain", "in", "file", "]", "\n", "\n", "gold_paths", "=", "[", "]", "\n", "if", "use_labeled_data", ":", "\n", "            ", "gold_paths", "=", "[", "'data/onto_pos_ner_dp_train_'", "+", "src_domain", "]", "\n", "\n", "", "if", "not", "use_unlabeled_data", "and", "not", "use_labeled_data", ":", "\n", "            ", "raise", "ValueError", "\n", "\n", "", "", "paths", "=", "pred_paths", "+", "gold_paths", "\n", "print", "(", "'############ Add Label Task #################'", ")", "\n", "print", "(", "\"Adding labels to paths: %s\"", "%", "', '", ".", "join", "(", "paths", ")", ")", "\n", "root_line", "=", "[", "'0'", ",", "ROOT", ",", "'XX'", ",", "'O'", ",", "'0'", ",", "'root'", "]", "\n", "writing_paths", "=", "{", "}", "\n", "sentences", "=", "{", "}", "\n", "for", "path", "in", "paths", ":", "\n", "        ", "if", "tgt_domain", "in", "path", ":", "\n", "            ", "reading_path", "=", "parser_path", "+", "path", "\n", "writing_path", "=", "model_path", "+", "'parser_'", "+", "path", "\n", "split", "=", "get_split", "(", "writing_path", ")", "\n", "", "else", ":", "\n", "            ", "reading_path", "=", "path", "\n", "writing_path", "=", "model_path", "+", "'parser_'", "+", "'domain_'", "+", "src_domain", "+", "'_train_model_domain_'", "+", "src_domain", "+", "'_data_domain_'", "+", "src_domain", "+", "'_gold.txt'", "\n", "split", "=", "'extra_train'", "\n", "", "writing_paths", "[", "split", "]", "=", "writing_path", "\n", "len_sent", "=", "0", "\n", "lines", "=", "[", "]", "\n", "sentences_list", "=", "[", "]", "\n", "with", "open", "(", "reading_path", ",", "'r'", ")", "as", "file", ":", "\n", "            ", "for", "line", "in", "file", ":", "\n", "# line = line.decode('utf-8')", "\n", "                ", "line", "=", "line", ".", "strip", "(", ")", "\n", "# Now blank space got detected", "\n", "if", "len", "(", "line", ")", "==", "0", ":", "\n", "# Append next word to last column", "\n", "                    ", "for", "idx", "in", "range", "(", "len_sent", ")", ":", "\n", "                            ", "lines", "[", "idx", "]", ".", "append", "(", "lines", "[", "idx", "]", "[", "3", "]", ")", "\n", "# Add root line first", "\n", "", "if", "len", "(", "lines", ")", ">", "0", ":", "\n", "                        ", "tmp_root_line", "=", "root_line", "+", "[", "root_line", "[", "3", "]", "]", "\n", "sentences_list", ".", "append", "(", "tmp_root_line", ")", "\n", "", "for", "line_", "in", "lines", ":", "\n", "                        ", "sentences_list", ".", "append", "(", "line_", ")", "\n", "", "sentences_list", ".", "append", "(", "[", "]", ")", "\n", "lines", "=", "[", "]", "\n", "len_sent", "=", "0", "\n", "continue", "\n", "", "tokens", "=", "line", ".", "split", "(", "'\\t'", ")", "\n", "idx", "=", "tokens", "[", "0", "]", "\n", "word", "=", "tokens", "[", "1", "]", "\n", "pos", "=", "tokens", "[", "2", "]", "\n", "ner", "=", "tokens", "[", "3", "]", "\n", "head", "=", "tokens", "[", "4", "]", "\n", "arc_tag", "=", "tokens", "[", "5", "]", "\n", "lines", ".", "append", "(", "[", "idx", ",", "word", ",", "pos", ",", "ner", ",", "head", ",", "arc_tag", "]", ")", "\n", "len_sent", "+=", "1", "\n", "", "", "sentences", "[", "split", "]", "=", "sentences_list", "\n", "\n", "", "train_sentences", "=", "[", "]", "\n", "if", "'train'", "in", "sentences", ":", "\n", "        ", "train_sentences", "=", "sentences", "[", "'train'", "]", "\n", "", "else", ":", "\n", "        ", "writing_paths", "[", "'train'", "]", "=", "writing_paths", "[", "'extra_train'", "]", ".", "replace", "(", "'extra_train'", ",", "'train'", ")", "\n", "", "if", "'extra_train'", "in", "sentences", ":", "\n", "        ", "train_sentences", "+=", "sentences", "[", "'extra_train'", "]", "\n", "del", "writing_paths", "[", "'extra_train'", "]", "\n", "", "if", "'extra_dev'", "in", "sentences", ":", "\n", "        ", "train_sentences", "+=", "sentences", "[", "'extra_dev'", "]", "\n", "del", "writing_paths", "[", "'extra_dev'", "]", "\n", "", "with", "open", "(", "writing_paths", "[", "'train'", "]", ",", "'w'", ")", "as", "f", ":", "\n", "        ", "for", "sent", "in", "train_sentences", ":", "\n", "            ", "f", ".", "write", "(", "'\\t'", ".", "join", "(", "sent", ")", "+", "'\\n'", ")", "\n", "", "", "for", "split", "in", "[", "'dev'", ",", "'test'", "]", ":", "\n", "        ", "if", "split", "in", "sentences", ":", "\n", "            ", "split_sentences", "=", "sentences", "[", "split", "]", "\n", "with", "open", "(", "writing_paths", "[", "split", "]", ",", "'w'", ")", "as", "f", ":", "\n", "                ", "for", "sent", "in", "split_sentences", ":", "\n", "                    ", "f", ".", "write", "(", "'\\t'", ".", "join", "(", "sent", ")", "+", "'\\n'", ")", "\n", "", "", "", "", "return", "writing_paths", "\n", "", "import", "re", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.write_extra_labels.get_case": [[1342, 1411], ["ma.replace().replace.replace().replace", "re.sub().replace().strip", "temp.split.split", "enumerate", "enumerate", "enumerate", "enumerate", "enumerate", "tense.strip.strip", "temp.split.pop", "ma.replace().replace.replace", "re.sub().replace", "temp.split.pop", "b.strip", "b.strip", "temp.split.pop", "b.strip", "b.strip", "temp.split.pop", "b.strip", "b.strip", "temp.split.pop", "b.strip", "b.strip", "temp.split.pop", "b.strip", "re.sub"], "function", ["None"], ["def", "get_case", "(", "ma", ")", ":", "\n", "        ", "indeclinable", "=", "[", "'ind'", ",", "'prep'", ",", "'interj'", ",", "'prep'", ",", "'conj'", ",", "'part'", "]", "\n", "case_list", "=", "[", "'nom'", ",", "'voc'", ",", "'acc'", ",", "'i'", ",", "'inst'", ",", "'dat'", ",", "'abl'", ",", "'g'", ",", "'loc'", "]", "\n", "gender_list", "=", "[", "'n'", ",", "'f'", ",", "'m'", ",", "'*'", "]", "\n", "person_list", "=", "[", "'1'", ",", "'2'", ",", "'3'", "]", "\n", "no_list", "=", "[", "'du'", ",", "'sg'", ",", "'pl'", "]", "\n", "pops", "=", "[", "' ac'", ",", "' ps'", "]", "\n", "ma", "=", "ma", ".", "replace", "(", "'sgpl'", ",", "'sg'", ")", ".", "replace", "(", "'sgdu'", ",", "'sg'", ")", "\n", "temp", "=", "re", ".", "sub", "(", "\"([\\(\\[]).*?([\\)\\]])\"", ",", "\"\\g<1>\\g<2>\"", ",", "ma", ")", ".", "replace", "(", "'[] '", ",", "''", ")", ".", "strip", "(", "' []'", ")", "\n", "temp", "=", "temp", ".", "split", "(", "'.'", ")", "\n", "if", "temp", "[", "-", "1", "]", "==", "''", ":", "\n", "            ", "temp", ".", "pop", "(", "-", "1", ")", "\n", "# Remove active passive", "\n", "", "case", "=", "''", "\n", "no", "=", "''", "\n", "person", "=", "''", "\n", "gender", "=", "''", "\n", "tense", "=", "''", "\n", "coarse", "=", "''", "\n", "for", "a", ",", "b", "in", "enumerate", "(", "temp", ")", ":", "\n", "            ", "if", "b", "in", "pops", ":", "\n", "                ", "temp", ".", "pop", "(", "a", ")", "\n", "# Get gender", "\n", "", "", "for", "a", ",", "b", "in", "enumerate", "(", "temp", ")", ":", "\n", "            ", "if", "b", ".", "strip", "(", ")", "in", "gender_list", ":", "\n", "                ", "gender", "=", "b", ".", "strip", "(", ")", "\n", "temp", ".", "pop", "(", "a", ")", "\n", "# Get case", "\n", "", "", "for", "a", ",", "b", "in", "enumerate", "(", "temp", ")", ":", "\n", "            ", "if", "b", ".", "strip", "(", ")", "in", "case_list", ":", "\n", "                ", "case", "=", "b", ".", "strip", "(", ")", "\n", "temp", ".", "pop", "(", "a", ")", "\n", "", "", "if", "case", "!=", "''", ":", "\n", "            ", "coarse", "=", "'Noun'", "\n", "# Get person", "\n", "", "for", "a", ",", "b", "in", "enumerate", "(", "temp", ")", ":", "\n", "            ", "if", "b", ".", "strip", "(", ")", "in", "person_list", ":", "\n", "                ", "person", "=", "b", ".", "strip", "(", ")", "\n", "temp", ".", "pop", "(", "a", ")", "\n", "# Get no", "\n", "", "", "for", "a", ",", "b", "in", "enumerate", "(", "temp", ")", ":", "\n", "            ", "if", "b", ".", "strip", "(", ")", "in", "no_list", ":", "\n", "                ", "no", "=", "b", ".", "strip", "(", ")", "\n", "temp", ".", "pop", "(", "a", ")", "\n", "# Get Tense", "\n", "", "", "for", "b", "in", "temp", ":", "\n", "            ", "tense", "=", "tense", "+", "' '", "+", "b", ".", "strip", "(", ")", "\n", "", "tense", "=", "tense", ".", "strip", "(", ")", "\n", "\n", "#         print(tense)", "\n", "if", "tense", "==", "'adv'", ":", "\n", "            ", "coarse", "=", "'adv'", "\n", "", "for", "ind", "in", "indeclinable", ":", "\n", "            ", "if", "tense", "==", "ind", ":", "\n", "                ", "coarse", "=", "'Ind'", "\n", "", "", "if", "tense", "==", "'abs'", "or", "tense", "==", "'ca abs'", ":", "\n", "            ", "coarse", "=", "'IV'", "\n", "", "if", "tense", "!=", "''", "and", "coarse", "==", "''", ":", "\n", "            ", "if", "person", "!=", "''", "or", "no", "!=", "''", ":", "\n", "                ", "coarse", "=", "'FV'", "\n", "", "else", ":", "\n", "                ", "coarse", "=", "'IV'", "\n", "", "", "if", "case", "==", "'i'", ":", "\n", "            ", "return", "'inst'", "\n", "\n", "", "if", "case", "!=", "''", ":", "\n", "            ", "return", "case", "\n", "", "else", ":", "\n", "            ", "return", "coarse", "\n", "", "", "def", "clean_ma", "(", "ma", ")", ":", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.write_extra_labels.clean_ma": [[1411, 1415], ["re.sub().replace().strip().replace().replace().replace().replace", "ma.replace().replace().replace.replace().replace().replace", "re.sub().replace().strip().replace().replace().replace", "ma.replace().replace().replace.replace().replace", "re.sub().replace().strip().replace().replace", "ma.replace().replace().replace.replace", "re.sub().replace().strip().replace", "re.sub().replace().strip", "re.sub().replace", "re.sub"], "function", ["None"], ["", "", "def", "clean_ma", "(", "ma", ")", ":", "\n", "    ", "ma", "=", "re", ".", "sub", "(", "\"([\\(\\[]).*?([\\)\\]])\"", ",", "\"\\g<1>\\g<2>\"", ",", "ma", ")", ".", "replace", "(", "'[] '", ",", "''", ")", ".", "strip", "(", "' []'", ")", ".", "replace", "(", "' ac'", ",", "''", ")", ".", "replace", "(", "' ps'", ",", "''", ")", ".", "replace", "(", "'sgpl'", ",", "'sg'", ")", ".", "replace", "(", "'sgdu'", ",", "'sg'", ")", "\n", "ma", "=", "ma", ".", "replace", "(", "'i.'", ",", "'inst.'", ")", ".", "replace", "(", "'.'", ",", "''", ")", ".", "replace", "(", "' '", ",", "''", ")", "\n", "return", "ma", "\n", "", "def", "predict_case_of_modifier", "(", "model_path", ",", "parser_path", ",", "src_domain", ",", "tgt_domain", ",", "use_unlabeled_data", "=", "True", ",", "use_labeled_data", "=", "True", ")", ":", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.write_extra_labels.predict_case_of_modifier": [[1415, 1510], ["print", "print", "writing_paths[].replace", "open", "write_extra_labels.get_split", "open", "f.write", "os.listdir", "os.listdir", "line.strip.strip", "line.strip.split", "lines.append", "open", "os.listdir", "file.endswith", "os.listdir", "file.endswith", "len", "range", "sentences_list.append", "f.write", "file.endswith", "file.endswith", "lines[].append", "len", "sentences_list.append", "sentences_list.append", "write_extra_labels.get_case"], "function", ["home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.open", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.write_extra_labels.get_split", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.open", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.writer.Writer.write", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.open", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.writer.Writer.write", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.write_extra_labels.get_case"], ["", "def", "predict_case_of_modifier", "(", "model_path", ",", "parser_path", ",", "src_domain", ",", "tgt_domain", ",", "use_unlabeled_data", "=", "True", ",", "use_labeled_data", "=", "True", ")", ":", "\n", "    ", "if", "src_domain", "==", "tgt_domain", ":", "\n", "        ", "pred_paths", "=", "[", "]", "\n", "if", "use_unlabeled_data", ":", "\n", "            ", "pred_paths", "=", "[", "file", "for", "file", "in", "os", ".", "listdir", "(", "parser_path", ")", "if", "\n", "file", ".", "endswith", "(", "\"pred.txt\"", ")", "and", "'extra'", "in", "file", "and", "tgt_domain", "in", "file", "]", "\n", "\n", "", "gold_paths", "=", "[", "file", "for", "file", "in", "os", ".", "listdir", "(", "parser_path", ")", "if", "\n", "file", ".", "endswith", "(", "\"gold.txt\"", ")", "and", "'extra'", "not", "in", "file", "and", "tgt_domain", "in", "file", "and", "'train'", "not", "in", "file", "]", "\n", "if", "use_labeled_data", ":", "\n", "            ", "gold_paths", "+=", "[", "file", "for", "file", "in", "os", ".", "listdir", "(", "parser_path", ")", "if", "\n", "file", ".", "endswith", "(", "\"gold.txt\"", ")", "and", "'extra'", "not", "in", "file", "and", "tgt_domain", "in", "file", "and", "'train'", "in", "file", "]", "\n", "\n", "", "if", "not", "use_unlabeled_data", "and", "not", "use_labeled_data", ":", "\n", "            ", "raise", "ValueError", "\n", "", "", "else", ":", "\n", "        ", "pred_paths", "=", "[", "file", "for", "file", "in", "os", ".", "listdir", "(", "parser_path", ")", "if", "file", ".", "endswith", "(", "\"pred.txt\"", ")", "and", "tgt_domain", "in", "file", "]", "\n", "\n", "gold_paths", "=", "[", "]", "\n", "if", "use_labeled_data", ":", "\n", "            ", "gold_paths", "=", "[", "'data/onto_pos_ner_dp_train_'", "+", "src_domain", "]", "\n", "\n", "", "if", "not", "use_unlabeled_data", "and", "not", "use_labeled_data", ":", "\n", "            ", "raise", "ValueError", "\n", "\n", "", "", "paths", "=", "pred_paths", "+", "gold_paths", "\n", "print", "(", "'############ Add Label Task #################'", ")", "\n", "print", "(", "\"Adding labels to paths: %s\"", "%", "', '", ".", "join", "(", "paths", ")", ")", "\n", "root_line", "=", "[", "'0'", ",", "ROOT", ",", "'XX'", ",", "'O'", ",", "'0'", ",", "'root'", "]", "\n", "writing_paths", "=", "{", "}", "\n", "sentences", "=", "{", "}", "\n", "for", "path", "in", "paths", ":", "\n", "        ", "if", "tgt_domain", "in", "path", ":", "\n", "            ", "reading_path", "=", "parser_path", "+", "path", "\n", "writing_path", "=", "model_path", "+", "'parser_'", "+", "path", "\n", "split", "=", "get_split", "(", "writing_path", ")", "\n", "", "else", ":", "\n", "            ", "reading_path", "=", "path", "\n", "writing_path", "=", "model_path", "+", "'parser_'", "+", "'domain_'", "+", "src_domain", "+", "'_train_model_domain_'", "+", "src_domain", "+", "'_data_domain_'", "+", "src_domain", "+", "'_gold.txt'", "\n", "split", "=", "'extra_train'", "\n", "", "writing_paths", "[", "split", "]", "=", "writing_path", "\n", "len_sent", "=", "0", "\n", "lines", "=", "[", "]", "\n", "sentences_list", "=", "[", "]", "\n", "with", "open", "(", "reading_path", ",", "'r'", ")", "as", "file", ":", "\n", "            ", "for", "line", "in", "file", ":", "\n", "# line = line.decode('utf-8')", "\n", "                ", "line", "=", "line", ".", "strip", "(", ")", "\n", "# Now blank space got detected", "\n", "if", "len", "(", "line", ")", "==", "0", ":", "\n", "# Append next word to last column", "\n", "                    ", "for", "idx", "in", "range", "(", "len_sent", ")", ":", "\n", "                            ", "lines", "[", "idx", "]", ".", "append", "(", "get_case", "(", "lines", "[", "idx", "]", "[", "3", "]", ")", ")", "\n", "# Add root line first", "\n", "", "if", "len", "(", "lines", ")", ">", "0", ":", "\n", "                        ", "tmp_root_line", "=", "root_line", "+", "[", "root_line", "[", "3", "]", "]", "\n", "sentences_list", ".", "append", "(", "tmp_root_line", ")", "\n", "", "for", "line_", "in", "lines", ":", "\n", "                        ", "sentences_list", ".", "append", "(", "line_", ")", "\n", "", "sentences_list", ".", "append", "(", "[", "]", ")", "\n", "lines", "=", "[", "]", "\n", "len_sent", "=", "0", "\n", "continue", "\n", "", "tokens", "=", "line", ".", "split", "(", "'\\t'", ")", "\n", "idx", "=", "tokens", "[", "0", "]", "\n", "word", "=", "tokens", "[", "1", "]", "\n", "pos", "=", "tokens", "[", "2", "]", "\n", "ner", "=", "tokens", "[", "3", "]", "\n", "head", "=", "tokens", "[", "4", "]", "\n", "arc_tag", "=", "tokens", "[", "5", "]", "\n", "lines", ".", "append", "(", "[", "idx", ",", "word", ",", "pos", ",", "ner", ",", "head", ",", "arc_tag", "]", ")", "\n", "len_sent", "+=", "1", "\n", "", "", "sentences", "[", "split", "]", "=", "sentences_list", "\n", "\n", "", "train_sentences", "=", "[", "]", "\n", "if", "'train'", "in", "sentences", ":", "\n", "        ", "train_sentences", "=", "sentences", "[", "'train'", "]", "\n", "", "else", ":", "\n", "        ", "writing_paths", "[", "'train'", "]", "=", "writing_paths", "[", "'extra_train'", "]", ".", "replace", "(", "'extra_train'", ",", "'train'", ")", "\n", "", "if", "'extra_train'", "in", "sentences", ":", "\n", "        ", "train_sentences", "+=", "sentences", "[", "'extra_train'", "]", "\n", "del", "writing_paths", "[", "'extra_train'", "]", "\n", "", "if", "'extra_dev'", "in", "sentences", ":", "\n", "        ", "train_sentences", "+=", "sentences", "[", "'extra_dev'", "]", "\n", "del", "writing_paths", "[", "'extra_dev'", "]", "\n", "", "with", "open", "(", "writing_paths", "[", "'train'", "]", ",", "'w'", ")", "as", "f", ":", "\n", "        ", "for", "sent", "in", "train_sentences", ":", "\n", "            ", "f", ".", "write", "(", "'\\t'", ".", "join", "(", "sent", ")", "+", "'\\n'", ")", "\n", "", "", "for", "split", "in", "[", "'dev'", ",", "'test'", "]", ":", "\n", "        ", "if", "split", "in", "sentences", ":", "\n", "            ", "split_sentences", "=", "sentences", "[", "split", "]", "\n", "with", "open", "(", "writing_paths", "[", "split", "]", ",", "'w'", ")", "as", "f", ":", "\n", "                ", "for", "sent", "in", "split_sentences", ":", "\n", "                    ", "f", ".", "write", "(", "'\\t'", ".", "join", "(", "sent", ")", "+", "'\\n'", ")", "\n", "", "", "", "", "return", "writing_paths", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.write_extra_labels.Multitask_case_predict": [[1511, 1518], ["None"], "function", ["None"], ["", "def", "Multitask_case_predict", "(", "model_path", ",", "parser_path", ",", "src_domain", ",", "tgt_domain", ",", "use_unlabeled_data", "=", "True", ",", "use_labeled_data", "=", "True", ")", ":", "\n", "    ", "writing_paths", "=", "{", "}", "\n", "# multitask_silver_20ktrain_san ", "\n", "writing_paths", "[", "'train'", "]", "=", "'data/ud_pos_ner_dp_train_san_case'", "\n", "writing_paths", "[", "'dev'", "]", "=", "'data/ud_pos_ner_dp_dev_san_case'", "\n", "writing_paths", "[", "'test'", "]", "=", "'data/ud_pos_ner_dp_test_san_case'", "\n", "return", "writing_paths", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.write_extra_labels.Multitask_POS_predict": [[1519, 1529], ["None"], "function", ["None"], ["", "def", "Multitask_POS_predict", "(", "model_path", ",", "parser_path", ",", "src_domain", ",", "tgt_domain", ",", "use_unlabeled_data", "=", "True", ",", "use_labeled_data", "=", "True", ")", ":", "\n", "    ", "writing_paths", "=", "{", "}", "\n", "# multitask_silver_20ktrain_san", "\n", "# writing_paths['train'] = 'data/Multitask_POS_predict_train_san'", "\n", "# writing_paths['dev'] = 'data/Multitask_POS_predict_dev_san'", "\n", "# writing_paths['test'] = 'data/Multitask_POS_predict_test_san'", "\n", "writing_paths", "[", "'train'", "]", "=", "'data/ud_pos_ner_dp_train_san_POS'", "\n", "writing_paths", "[", "'dev'", "]", "=", "'data/ud_pos_ner_dp_dev_san_POS'", "\n", "writing_paths", "[", "'test'", "]", "=", "'data/ud_pos_ner_dp_test_san_POS'", "\n", "return", "writing_paths", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.write_extra_labels.Multitask_coarse_predict": [[1530, 1537], ["None"], "function", ["None"], ["", "def", "Multitask_coarse_predict", "(", "model_path", ",", "parser_path", ",", "src_domain", ",", "tgt_domain", ",", "use_unlabeled_data", "=", "True", ",", "use_labeled_data", "=", "True", ")", ":", "\n", "    ", "writing_paths", "=", "{", "}", "\n", "# multitask_silver_20ktrain_san", "\n", "writing_paths", "[", "'train'", "]", "=", "'data/Multitask_coarse_predict_train_san'", "\n", "writing_paths", "[", "'dev'", "]", "=", "'data/Multitask_coarse_predict_dev_san'", "\n", "writing_paths", "[", "'test'", "]", "=", "'data/Multitask_coarse_predict_test_san'", "\n", "return", "writing_paths", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.write_extra_labels.Multitask_label_predict": [[1538, 1545], ["None"], "function", ["None"], ["", "def", "Multitask_label_predict", "(", "model_path", ",", "parser_path", ",", "src_domain", ",", "tgt_domain", ",", "use_unlabeled_data", "=", "True", ",", "use_labeled_data", "=", "True", ")", ":", "\n", "    ", "writing_paths", "=", "{", "}", "\n", "# multitask_silver_20ktrain_san", "\n", "writing_paths", "[", "'train'", "]", "=", "'data/Multitask_label_predict_train_san'", "\n", "writing_paths", "[", "'dev'", "]", "=", "'data/Multitask_label_predict_dev_san'", "\n", "writing_paths", "[", "'test'", "]", "=", "'data/Multitask_label_predict_test_san'", "\n", "return", "writing_paths", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.write_extra_labels.MRL_case": [[1547, 1554], ["None"], "function", ["None"], ["", "def", "MRL_case", "(", "model_path", ",", "parser_path", ",", "src_domain", ",", "tgt_domain", ",", "use_unlabeled_data", "=", "True", ",", "use_labeled_data", "=", "True", ")", ":", "\n", "    ", "writing_paths", "=", "{", "}", "\n", "# multitask_silver_20ktrain_san", "\n", "writing_paths", "[", "'train'", "]", "=", "'data/Prep_MRL/ud_pos_ner_dp_train_'", "+", "src_domain", "+", "'_case'", "\n", "writing_paths", "[", "'dev'", "]", "=", "'data/Prep_MRL/ud_pos_ner_dp_dev_'", "+", "src_domain", "+", "'_case'", "\n", "writing_paths", "[", "'test'", "]", "=", "'data/Prep_MRL/ud_pos_ner_dp_test_'", "+", "src_domain", "+", "'_case'", "\n", "return", "writing_paths", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.write_extra_labels.MRL_POS": [[1555, 1562], ["None"], "function", ["None"], ["", "def", "MRL_POS", "(", "model_path", ",", "parser_path", ",", "src_domain", ",", "tgt_domain", ",", "use_unlabeled_data", "=", "True", ",", "use_labeled_data", "=", "True", ")", ":", "\n", "    ", "writing_paths", "=", "{", "}", "\n", "# multitask_silver_20ktrain_san", "\n", "writing_paths", "[", "'train'", "]", "=", "'data/Prep_MRL/ud_pos_ner_dp_train_'", "+", "src_domain", "+", "'_POS'", "\n", "writing_paths", "[", "'dev'", "]", "=", "'data/Prep_MRL/ud_pos_ner_dp_dev_'", "+", "src_domain", "+", "'_POS'", "\n", "writing_paths", "[", "'test'", "]", "=", "'data/Prep_MRL/ud_pos_ner_dp_test_'", "+", "src_domain", "+", "'_POS'", "\n", "return", "writing_paths", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.write_extra_labels.MRL_label": [[1563, 1570], ["None"], "function", ["None"], ["", "def", "MRL_label", "(", "model_path", ",", "parser_path", ",", "src_domain", ",", "tgt_domain", ",", "use_unlabeled_data", "=", "True", ",", "use_labeled_data", "=", "True", ")", ":", "\n", "    ", "writing_paths", "=", "{", "}", "\n", "# multitask_silver_20ktrain_san", "\n", "writing_paths", "[", "'train'", "]", "=", "'data/Prep_MRL/ud_pos_ner_dp_train_'", "+", "src_domain", "+", "'_dep'", "\n", "writing_paths", "[", "'dev'", "]", "=", "'data/Prep_MRL/ud_pos_ner_dp_dev_'", "+", "src_domain", "+", "'_dep'", "\n", "writing_paths", "[", "'test'", "]", "=", "'data/Prep_MRL/ud_pos_ner_dp_test_'", "+", "src_domain", "+", "'_dep'", "\n", "return", "writing_paths", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.write_extra_labels.MRL_no": [[1571, 1578], ["None"], "function", ["None"], ["", "def", "MRL_no", "(", "model_path", ",", "parser_path", ",", "src_domain", ",", "tgt_domain", ",", "use_unlabeled_data", "=", "True", ",", "use_labeled_data", "=", "True", ")", ":", "\n", "    ", "writing_paths", "=", "{", "}", "\n", "# multitask_silver_20ktrain_san", "\n", "writing_paths", "[", "'train'", "]", "=", "'data/Prep_MRL/ud_pos_ner_dp_train_'", "+", "src_domain", "+", "'_no'", "\n", "writing_paths", "[", "'dev'", "]", "=", "'data/Prep_MRL/ud_pos_ner_dp_dev_'", "+", "src_domain", "+", "'_no'", "\n", "writing_paths", "[", "'test'", "]", "=", "'data/Prep_MRL/ud_pos_ner_dp_test_'", "+", "src_domain", "+", "'_no'", "\n", "return", "writing_paths", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.write_extra_labels.MRL_Person": [[1579, 1586], ["None"], "function", ["None"], ["", "def", "MRL_Person", "(", "model_path", ",", "parser_path", ",", "src_domain", ",", "tgt_domain", ",", "use_unlabeled_data", "=", "True", ",", "use_labeled_data", "=", "True", ")", ":", "\n", "    ", "writing_paths", "=", "{", "}", "\n", "# multitask_silver_20ktrain_san", "\n", "writing_paths", "[", "'train'", "]", "=", "'data/Prep_MRL/ud_pos_ner_dp_train_'", "+", "src_domain", "+", "'_per'", "\n", "writing_paths", "[", "'dev'", "]", "=", "'data/Prep_MRL/ud_pos_ner_dp_dev_'", "+", "src_domain", "+", "'_per'", "\n", "writing_paths", "[", "'test'", "]", "=", "'data/Prep_MRL/ud_pos_ner_dp_test_'", "+", "src_domain", "+", "'_per'", "\n", "return", "writing_paths", "\n", "", "def", "MRL_Gender", "(", "model_path", ",", "parser_path", ",", "src_domain", ",", "tgt_domain", ",", "use_unlabeled_data", "=", "True", ",", "use_labeled_data", "=", "True", ")", ":", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.write_extra_labels.MRL_Gender": [[1586, 1593], ["None"], "function", ["None"], ["", "def", "MRL_Gender", "(", "model_path", ",", "parser_path", ",", "src_domain", ",", "tgt_domain", ",", "use_unlabeled_data", "=", "True", ",", "use_labeled_data", "=", "True", ")", ":", "\n", "    ", "writing_paths", "=", "{", "}", "\n", "# multitask_silver_20ktrain_san", "\n", "writing_paths", "[", "'train'", "]", "=", "'data/Prep_MRL/ud_pos_ner_dp_train_'", "+", "src_domain", "+", "'_gen'", "\n", "writing_paths", "[", "'dev'", "]", "=", "'data/Prep_MRL/ud_pos_ner_dp_dev_'", "+", "src_domain", "+", "'_gen'", "\n", "writing_paths", "[", "'test'", "]", "=", "'data/Prep_MRL/ud_pos_ner_dp_test_'", "+", "src_domain", "+", "'_gen'", "\n", "return", "writing_paths", "", "", ""]], "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.logger.get_logger": [[5, 16], ["logging.getLogger", "logging.getLogger.setLevel", "logging.Formatter", "logging.StreamHandler", "logging.StreamHandler.setLevel", "logging.StreamHandler.setFormatter", "logging.getLogger.addHandler"], "function", ["None"], ["def", "get_logger", "(", "name", ",", "level", "=", "logging", ".", "INFO", ",", "handler", "=", "sys", ".", "stdout", ",", "\n", "formatter", "=", "'%(asctime)s - %(name)s - %(levelname)s - %(message)s'", ")", ":", "\n", "    ", "logger", "=", "logging", ".", "getLogger", "(", "name", ")", "\n", "logger", ".", "setLevel", "(", "logging", ".", "INFO", ")", "\n", "formatter", "=", "logging", ".", "Formatter", "(", "formatter", ")", "\n", "stream_handler", "=", "logging", ".", "StreamHandler", "(", "handler", ")", "\n", "stream_handler", ".", "setLevel", "(", "level", ")", "\n", "stream_handler", ".", "setFormatter", "(", "formatter", ")", "\n", "logger", ".", "addHandler", "(", "stream_handler", ")", "\n", "\n", "return", "logger", "\n", "", ""]], "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.rearrange_splits.rearranging_splits": [[4, 69], ["datasets.keys", "range", "len", "len", "sum", "len", "sum", "enumerate", "numpy.arange", "numpy.random.permutation", "sample_indices.items", "range", "print", "range", "len", "[].append", "[].append", "len", "sample_indices[].append", "sample_indices[].append", "enumerate", "len", "torch.stack", "len", "range", "range", "sample_indices[].pop", "[].append", "range", "range"], "function", ["home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.keys", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.items"], ["def", "rearranging_splits", "(", "datasets", ",", "num_training_samples", ")", ":", "\n", "    ", "new_datasets", "=", "{", "}", "\n", "data_splits", "=", "datasets", ".", "keys", "(", ")", "\n", "for", "split", "in", "data_splits", ":", "\n", "        ", "if", "split", "==", "'test'", ":", "\n", "            ", "new_datasets", "[", "'test'", "]", "=", "datasets", "[", "'test'", "]", "\n", "\n", "", "else", ":", "\n", "            ", "num_buckets", "=", "len", "(", "datasets", "[", "split", "]", "[", "1", "]", ")", "\n", "num_tensors", "=", "len", "(", "datasets", "[", "split", "]", "[", "0", "]", "[", "0", "]", ")", "\n", "num_samples", "=", "sum", "(", "datasets", "[", "split", "]", "[", "1", "]", ")", "\n", "if", "num_samples", "<", "num_training_samples", ":", "\n", "                ", "print", "(", "\"set_num_training_samples (%d) should be smaller than the actual %s size (%d)\"", "\n", "%", "(", "num_training_samples", ",", "split", ",", "num_samples", ")", ")", "\n", "", "new_datasets", "[", "split", "]", "=", "[", "[", "[", "[", "]", "for", "_", "in", "range", "(", "num_tensors", ")", "]", "for", "_", "in", "range", "(", "num_buckets", ")", "]", ",", "[", "]", "]", "\n", "new_datasets", "[", "'extra_'", "+", "split", "]", "=", "[", "[", "[", "[", "]", "for", "_", "in", "range", "(", "num_tensors", ")", "]", "for", "_", "in", "range", "(", "num_buckets", ")", "]", ",", "[", "]", "]", "\n", "", "", "for", "split", "in", "data_splits", ":", "\n", "        ", "if", "split", "==", "'test'", ":", "\n", "            ", "continue", "\n", "", "else", ":", "\n", "            ", "curr_bucket_sizes", "=", "datasets", "[", "split", "]", "[", "1", "]", "\n", "curr_samples", "=", "datasets", "[", "split", "]", "[", "0", "]", "\n", "num_tensors", "=", "len", "(", "datasets", "[", "split", "]", "[", "0", "]", "[", "0", "]", ")", "\n", "curr_num_samples", "=", "sum", "(", "curr_bucket_sizes", ")", "\n", "sample_indices_in_buckets", "=", "{", "}", "\n", "i", "=", "0", "\n", "for", "bucket_idx", ",", "bucket_size", "in", "enumerate", "(", "curr_bucket_sizes", ")", ":", "\n", "                ", "for", "sample_idx", "in", "range", "(", "bucket_size", ")", ":", "\n", "                    ", "sample_indices_in_buckets", "[", "i", "]", "=", "(", "bucket_idx", ",", "sample_idx", ")", "\n", "i", "+=", "1", "\n", "", "", "rng", "=", "np", ".", "arange", "(", "curr_num_samples", ")", "\n", "rng", "=", "np", ".", "random", ".", "permutation", "(", "rng", ")", "\n", "sample_indices", "=", "{", "}", "\n", "sample_indices", "[", "split", "]", "=", "[", "sample_indices_in_buckets", "[", "key", "]", "for", "key", "in", "rng", "[", ":", "num_training_samples", "]", "]", "\n", "sample_indices", "[", "'extra_'", "+", "split", "]", "=", "[", "sample_indices_in_buckets", "[", "key", "]", "for", "key", "in", "rng", "[", "num_training_samples", ":", "]", "]", "\n", "if", "len", "(", "sample_indices", "[", "'extra_'", "+", "split", "]", ")", "==", "0", ":", "\n", "                ", "if", "len", "(", "sample_indices", "[", "split", "]", ")", ">", "1", ":", "\n", "                    ", "sample_indices", "[", "'extra_'", "+", "split", "]", ".", "append", "(", "sample_indices", "[", "split", "]", ".", "pop", "(", "-", "1", ")", ")", "\n", "", "else", ":", "\n", "                    ", "sample_indices", "[", "'extra_'", "+", "split", "]", ".", "append", "(", "sample_indices", "[", "split", "]", "[", "0", "]", ")", "\n", "\n", "", "", "for", "key", ",", "indices", "in", "sample_indices", ".", "items", "(", ")", ":", "\n", "                ", "for", "bucket_idx", ",", "sample_idx", "in", "indices", ":", "\n", "                    ", "curr_bucket", "=", "curr_samples", "[", "bucket_idx", "]", "\n", "for", "tensor_idx", ",", "tensor", "in", "enumerate", "(", "curr_bucket", ")", ":", "\n", "                        ", "new_datasets", "[", "key", "]", "[", "0", "]", "[", "bucket_idx", "]", "[", "tensor_idx", "]", ".", "append", "(", "tensor", "[", "sample_idx", "]", ")", "\n", "", "", "", "", "", "del", "datasets", "\n", "new_splits", "=", "[", "]", "\n", "new_splits", "+=", "[", "split", "for", "split", "in", "data_splits", "if", "split", "!=", "'test'", "]", "\n", "new_splits", "+=", "[", "'extra_'", "+", "split", "for", "split", "in", "data_splits", "if", "split", "!=", "'test'", "]", "\n", "\n", "for", "split", "in", "new_splits", ":", "\n", "        ", "for", "bucket_idx", "in", "range", "(", "num_buckets", ")", ":", "\n", "            ", "for", "tensor_idx", "in", "range", "(", "num_tensors", ")", ":", "\n", "                ", "if", "len", "(", "new_datasets", "[", "split", "]", "[", "0", "]", "[", "bucket_idx", "]", "[", "tensor_idx", "]", ")", ">", "0", ":", "\n", "                    ", "new_datasets", "[", "split", "]", "[", "0", "]", "[", "bucket_idx", "]", "[", "tensor_idx", "]", "=", "stack", "(", "new_datasets", "[", "split", "]", "[", "0", "]", "[", "bucket_idx", "]", "[", "tensor_idx", "]", ")", "\n", "", "else", ":", "\n", "                    ", "new_datasets", "[", "split", "]", "[", "0", "]", "[", "bucket_idx", "]", "=", "(", "1", ",", "1", ")", "\n", "break", "\n", "# set lengths of buckets", "\n", "", "", "if", "new_datasets", "[", "split", "]", "[", "0", "]", "[", "bucket_idx", "]", "==", "(", "1", ",", "1", ")", ":", "\n", "                ", "new_datasets", "[", "split", "]", "[", "1", "]", ".", "append", "(", "0", ")", "\n", "", "else", ":", "\n", "                ", "new_datasets", "[", "split", "]", "[", "1", "]", ".", "append", "(", "len", "(", "new_datasets", "[", "split", "]", "[", "0", "]", "[", "bucket_idx", "]", "[", "tensor_idx", "]", ")", ")", "\n", "", "", "", "return", "new_datasets", "", "", ""]], "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.__init__": [[9, 25], ["logger.get_logger", "set"], "methods", ["home.repos.pwc.inspect_result.Jivnesh_LCM.io_.logger.get_logger"], ["    ", "def", "__init__", "(", "self", ",", "name", ",", "defualt_value", "=", "False", ",", "keep_growing", "=", "True", ",", "singleton", "=", "False", ")", ":", "\n", "        ", "self", ".", "__name", "=", "name", "\n", "\n", "self", ".", "instance2index", "=", "{", "}", "\n", "self", ".", "instances", "=", "[", "]", "\n", "self", ".", "default_value", "=", "defualt_value", "\n", "self", ".", "offset", "=", "1", "if", "self", ".", "default_value", "else", "0", "\n", "self", ".", "keep_growing", "=", "keep_growing", "\n", "self", ".", "singletons", "=", "set", "(", ")", "if", "singleton", "else", "None", "\n", "\n", "# Index 0 is occupied by default, all else following.", "\n", "self", ".", "default_index", "=", "0", "if", "self", ".", "default_value", "else", "None", "\n", "\n", "self", ".", "next_index", "=", "self", ".", "offset", "\n", "\n", "self", ".", "logger", "=", "get_logger", "(", "\"Alphabet\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.add": [[26, 31], ["alphabet.Alphabet.instances.append"], "methods", ["None"], ["", "def", "add", "(", "self", ",", "instance", ")", ":", "\n", "        ", "if", "instance", "not", "in", "self", ".", "instance2index", "and", "instance", "!=", "'<_UNK>'", ":", "\n", "            ", "self", ".", "instances", ".", "append", "(", "instance", ")", "\n", "self", ".", "instance2index", "[", "instance", "]", "=", "self", ".", "next_index", "\n", "self", ".", "next_index", "+=", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.add_singleton": [[32, 37], ["RuntimeError", "alphabet.Alphabet.singletons.add"], "methods", ["home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.add"], ["", "", "def", "add_singleton", "(", "self", ",", "id", ")", ":", "\n", "        ", "if", "self", ".", "singletons", "is", "None", ":", "\n", "            ", "raise", "RuntimeError", "(", "\"Alphabet %s does not have singleton.\"", "%", "self", ".", "__name", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "singletons", ".", "add", "(", "id", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.add_singletons": [[38, 43], ["RuntimeError", "alphabet.Alphabet.singletons.update"], "methods", ["None"], ["", "", "def", "add_singletons", "(", "self", ",", "ids", ")", ":", "\n", "        ", "if", "self", ".", "singletons", "is", "None", ":", "\n", "            ", "raise", "RuntimeError", "(", "\"Alphabet %s does not have singleton.\"", "%", "self", ".", "__name", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "singletons", ".", "update", "(", "ids", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.is_singleton": [[44, 49], ["RuntimeError"], "methods", ["None"], ["", "", "def", "is_singleton", "(", "self", ",", "id", ")", ":", "\n", "        ", "if", "self", ".", "singletons", "is", "None", ":", "\n", "            ", "raise", "RuntimeError", "(", "\"Alphabet %s does not have singleton.\"", "%", "self", ".", "__name", ")", "\n", "", "else", ":", "\n", "            ", "return", "id", "in", "self", ".", "singletons", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.get_index": [[50, 63], ["alphabet.Alphabet.add", "KeyError"], "methods", ["home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.add"], ["", "", "def", "get_index", "(", "self", ",", "instance", ")", ":", "\n", "        ", "try", ":", "\n", "            ", "return", "self", ".", "instance2index", "[", "instance", "]", "\n", "", "except", "KeyError", ":", "\n", "            ", "if", "self", ".", "keep_growing", ":", "\n", "                ", "index", "=", "self", ".", "next_index", "\n", "self", ".", "add", "(", "instance", ")", "\n", "return", "index", "\n", "", "else", ":", "\n", "                ", "if", "self", ".", "default_value", ":", "\n", "                    ", "return", "self", ".", "default_index", "\n", "", "else", ":", "\n", "                    ", "raise", "KeyError", "(", "\"instance not found: %s\"", "%", "instance", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.get_instance": [[64, 73], ["IndexError"], "methods", ["None"], ["", "", "", "", "def", "get_instance", "(", "self", ",", "index", ")", ":", "\n", "        ", "if", "self", ".", "default_value", "and", "index", "==", "self", ".", "default_index", ":", "\n", "# First index is occupied by the wildcard element.", "\n", "            ", "return", "\"<_UNK>\"", "\n", "", "else", ":", "\n", "            ", "try", ":", "\n", "                ", "return", "self", ".", "instances", "[", "index", "-", "self", ".", "offset", "]", "\n", "", "except", "IndexError", ":", "\n", "                ", "raise", "IndexError", "(", "\"unknown index: %d\"", "%", "index", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.size": [[74, 76], ["len"], "methods", ["None"], ["", "", "", "def", "size", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "instances", ")", "+", "self", ".", "offset", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.singleton_size": [[77, 79], ["len"], "methods", ["None"], ["", "def", "singleton_size", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "singletons", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.items": [[80, 82], ["alphabet.Alphabet.instance2index.items"], "methods", ["home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.items"], ["", "def", "items", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "instance2index", ".", "items", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.keys": [[83, 85], ["alphabet.Alphabet.instance2index.keys"], "methods", ["home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.keys"], ["", "def", "keys", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "instance2index", ".", "keys", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.values": [[86, 88], ["alphabet.Alphabet.instance2index.values"], "methods", ["home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.values"], ["", "def", "values", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "instance2index", ".", "values", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.token_in_alphabet": [[89, 91], ["set", "alphabet.Alphabet.instance2index.keys"], "methods", ["home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.keys"], ["", "def", "token_in_alphabet", "(", "self", ",", "token", ")", ":", "\n", "        ", "return", "token", "in", "set", "(", "self", ".", "instance2index", ".", "keys", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.enumerate_items": [[92, 96], ["zip", "IndexError", "range", "alphabet.Alphabet.size", "len"], "methods", ["home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.size"], ["", "def", "enumerate_items", "(", "self", ",", "start", ")", ":", "\n", "        ", "if", "start", "<", "self", ".", "offset", "or", "start", ">=", "self", ".", "size", "(", ")", ":", "\n", "            ", "raise", "IndexError", "(", "\"Enumerate is allowed between [%d : size of the alphabet)\"", "%", "self", ".", "offset", ")", "\n", "", "return", "zip", "(", "range", "(", "start", ",", "len", "(", "self", ".", "instances", ")", "+", "self", ".", "offset", ")", ",", "self", ".", "instances", "[", "start", "-", "self", ".", "offset", ":", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.close": [[97, 99], ["None"], "methods", ["None"], ["", "def", "close", "(", "self", ")", ":", "\n", "        ", "self", ".", "keep_growing", "=", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.open": [[100, 102], ["None"], "methods", ["None"], ["", "def", "open", "(", "self", ")", ":", "\n", "        ", "self", ".", "keep_growing", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.get_content": [[103, 109], ["list"], "methods", ["None"], ["", "def", "get_content", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "singletons", "is", "None", ":", "\n", "            ", "return", "{", "\"instance2index\"", ":", "self", ".", "instance2index", ",", "\"instances\"", ":", "self", ".", "instances", "}", "\n", "", "else", ":", "\n", "            ", "return", "{", "\"instance2index\"", ":", "self", ".", "instance2index", ",", "\"instances\"", ":", "self", ".", "instances", ",", "\n", "\"singletions\"", ":", "list", "(", "self", ".", "singletons", ")", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.__from_json": [[110, 117], ["set"], "methods", ["None"], ["", "", "def", "__from_json", "(", "self", ",", "data", ")", ":", "\n", "        ", "self", ".", "instances", "=", "data", "[", "\"instances\"", "]", "\n", "self", ".", "instance2index", "=", "data", "[", "\"instance2index\"", "]", "\n", "if", "\"singletions\"", "in", "data", ":", "\n", "            ", "self", ".", "singletons", "=", "set", "(", "data", "[", "\"singletions\"", "]", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "singletons", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.save": [[118, 134], ["json.dump", "os.path.exists", "os.makedirs", "alphabet.Alphabet.get_content", "alphabet.Alphabet.open"], "methods", ["home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.get_content", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.open"], ["", "", "def", "save", "(", "self", ",", "output_directory", ",", "name", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Save both alhpabet records to the given directory.\n        :param output_directory: Directory to save model and weights.\n        :param name: The alphabet saving name, optional.\n        :return:\n        \"\"\"", "\n", "saving_name", "=", "name", "if", "name", "else", "self", ".", "__name", "\n", "try", ":", "\n", "            ", "if", "not", "os", ".", "path", ".", "exists", "(", "output_directory", ")", ":", "\n", "                ", "os", ".", "makedirs", "(", "output_directory", ")", "\n", "\n", "", "json", ".", "dump", "(", "self", ".", "get_content", "(", ")", ",", "\n", "open", "(", "os", ".", "path", ".", "join", "(", "output_directory", ",", "saving_name", "+", "\".json\"", ")", ",", "'w'", ")", ",", "indent", "=", "4", ")", "\n", "", "except", "Exception", "as", "e", ":", "\n", "            ", "self", ".", "logger", ".", "warn", "(", "\"Alphabet is not saved: %s\"", "%", "repr", "(", "e", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.load": [[135, 148], ["os.path.join", "json.load", "alphabet.Alphabet.__from_json", "alphabet.Alphabet.open"], "methods", ["home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.load", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.__from_json", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.open"], ["", "", "def", "load", "(", "self", ",", "input_directory", ",", "name", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Load model architecture and weights from the give directory. This allow we use old saved_models even the structure\n        changes.\n        :param input_directory: Directory to save model and weights\n        :return:\n        \"\"\"", "\n", "loading_name", "=", "name", "if", "name", "else", "self", ".", "__name", "\n", "filename", "=", "os", ".", "path", ".", "join", "(", "input_directory", ",", "loading_name", "+", "\".json\"", ")", "\n", "f", "=", "json", ".", "load", "(", "open", "(", "filename", ")", ")", "\n", "self", ".", "__from_json", "(", "f", ")", "\n", "self", ".", "next_index", "=", "len", "(", "self", ".", "instances", ")", "+", "self", ".", "offset", "\n", "self", ".", "keep_growing", "=", "False", "\n", "", "", ""]], "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.remove_xx.read_file": [[3, 40], ["print", "print", "open", "line.strip.strip", "line.strip.split", "sentence.append", "len", "len", "len", "sentences.append", "lengths.append", "len", "len"], "function", ["home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.open"], ["def", "read_file", "(", "filename", ")", ":", "\n", "    ", "sentences", "=", "[", "]", "\n", "sentence", "=", "[", "]", "\n", "lengths", "=", "[", "]", "\n", "num_sentneces_to_remove", "=", "0", "\n", "num_sentences", "=", "0", "\n", "num_tokens_to_remove", "=", "0", "\n", "num_tokens", "=", "0", "\n", "with", "open", "(", "filename", ",", "'r'", ")", "as", "file", ":", "\n", "        ", "for", "line", "in", "file", ":", "\n", "            ", "line", "=", "line", ".", "strip", "(", ")", "\n", "if", "len", "(", "line", ")", "==", "0", ":", "\n", "                ", "xx_count", "=", "0", "\n", "for", "row", "in", "sentence", ":", "\n", "                    ", "if", "row", "[", "2", "]", "==", "'XX'", ":", "\n", "                        ", "xx_count", "+=", "1", "\n", "", "", "if", "xx_count", "/", "len", "(", "sentence", ")", ">=", "0.5", ":", "\n", "                    ", "num_sentneces_to_remove", "+=", "1", "\n", "num_tokens_to_remove", "+=", "len", "(", "sentence", ")", "\n", "", "else", ":", "\n", "                    ", "sentences", ".", "append", "(", "sentence", ")", "\n", "lengths", ".", "append", "(", "len", "(", "sentence", ")", ")", "\n", "", "num_sentences", "+=", "1", "\n", "num_tokens", "+=", "len", "(", "sentence", ")", "\n", "sentence", "=", "[", "]", "\n", "continue", "\n", "", "tokens", "=", "line", ".", "split", "(", "'\\t'", ")", "\n", "idx", "=", "tokens", "[", "0", "]", "\n", "word", "=", "tokens", "[", "1", "]", "\n", "pos", "=", "tokens", "[", "2", "]", "\n", "ner", "=", "tokens", "[", "3", "]", "\n", "arc", "=", "tokens", "[", "4", "]", "\n", "arc_tag", "=", "tokens", "[", "5", "]", "\n", "sentence", ".", "append", "(", "(", "idx", ",", "word", ",", "pos", ",", "ner", ",", "arc", ",", "arc_tag", ")", ")", "\n", "", "", "print", "(", "\"removed %d sentences out of %d sentences\"", "%", "(", "num_sentneces_to_remove", ",", "num_sentences", ")", ")", "\n", "print", "(", "\"removed %d tokens out of %d tokens\"", "%", "(", "num_tokens_to_remove", ",", "num_tokens", ")", ")", "\n", "return", "sentences", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.remove_xx.write_file": [[41, 47], ["open", "file.write", "file.write"], "function", ["home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.open", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.writer.Writer.write", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.writer.Writer.write"], ["", "def", "write_file", "(", "filename", ",", "sentences", ")", ":", "\n", "    ", "with", "open", "(", "filename", ",", "'w'", ")", "as", "file", ":", "\n", "        ", "for", "sentence", "in", "sentences", ":", "\n", "            ", "for", "row", "in", "sentence", ":", "\n", "                ", "file", ".", "write", "(", "'\\t'", ".", "join", "(", "[", "token", "for", "token", "in", "row", "]", ")", "+", "'\\n'", ")", "\n", "", "file", ".", "write", "(", "'\\n'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.prepare_data.create_alphabets": [[27, 172], ["logger.get_logger", "alphabet.Alphabet", "alphabet.Alphabet", "alphabet.Alphabet", "alphabet.Alphabet", "alphabet.Alphabet", "alphabet.Alphabet", "alphabet.Alphabet.close", "alphabet.Alphabet.close", "alphabet.Alphabet.close", "alphabet.Alphabet.close", "alphabet.Alphabet.close", "alphabet.Alphabet.close", "set", "os.path.isdir", "logger.get_logger.info", "alphabet.Alphabet.add", "alphabet.Alphabet.add", "alphabet.Alphabet.add", "alphabet.Alphabet.add", "alphabet.Alphabet.add", "alphabet.Alphabet.add", "alphabet.Alphabet.add", "alphabet.Alphabet.add", "alphabet.Alphabet.add", "alphabet.Alphabet.add", "alphabet.Alphabet.add", "alphabet.Alphabet.add", "alphabet.Alphabet.add", "alphabet.Alphabet.add", "alphabet.Alphabet.add", "dict", "isinstance", "set", "sorted", "alphabet.Alphabet.save", "alphabet.Alphabet.save", "alphabet.Alphabet.save", "alphabet.Alphabet.save", "alphabet.Alphabet.save", "alphabet.Alphabet.save", "print", "alphabet.Alphabet.load", "alphabet.Alphabet.load", "alphabet.Alphabet.load", "alphabet.Alphabet.load", "alphabet.Alphabet.load", "alphabet.Alphabet.load", "dict.keys", "prepare_data.create_alphabets.expand_vocab"], "function", ["home.repos.pwc.inspect_result.Jivnesh_LCM.io_.logger.get_logger", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.reader.Reader.close", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.reader.Reader.close", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.reader.Reader.close", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.reader.Reader.close", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.reader.Reader.close", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.reader.Reader.close", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.add", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.add", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.add", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.add", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.add", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.add", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.add", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.add", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.add", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.add", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.add", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.add", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.add", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.add", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.add", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.save", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.save", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.save", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.save", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.save", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.save", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.load", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.load", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.load", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.load", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.load", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.load", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.keys"], ["def", "create_alphabets", "(", "alphabet_directory", ",", "train_paths", ",", "extra_paths", "=", "None", ",", "max_vocabulary_size", "=", "100000", ",", "embedd_dict", "=", "None", ",", "\n", "min_occurence", "=", "1", ",", "lower_case", "=", "False", ")", ":", "\n", "    ", "def", "expand_vocab", "(", "vocab_list", ",", "char_alphabet", ",", "pos_alphabet", ",", "ner_alphabet", ",", "arc_alphabet", ")", ":", "\n", "        ", "vocab_set", "=", "set", "(", "vocab_list", ")", "\n", "for", "data_path", "in", "extra_paths", ":", "\n", "            ", "with", "open", "(", "data_path", ",", "'r'", ")", "as", "file", ":", "\n", "                ", "for", "line", "in", "file", ":", "\n", "                    ", "line", "=", "line", ".", "strip", "(", ")", "\n", "if", "len", "(", "line", ")", "==", "0", ":", "\n", "                        ", "continue", "\n", "\n", "", "tokens", "=", "line", ".", "split", "(", "'\\t'", ")", "\n", "if", "lower_case", ":", "\n", "                        ", "tokens", "[", "1", "]", "=", "tokens", "[", "1", "]", ".", "lower", "(", ")", "\n", "", "for", "char", "in", "tokens", "[", "1", "]", ":", "\n", "                        ", "char_alphabet", ".", "add", "(", "char", ")", "\n", "\n", "", "word", "=", "tokens", "[", "1", "]", "\n", "pos", "=", "tokens", "[", "2", "]", "\n", "ner", "=", "tokens", "[", "3", "]", "\n", "arc_tag", "=", "tokens", "[", "5", "]", "\n", "\n", "pos_alphabet", ".", "add", "(", "pos", ")", "\n", "ner_alphabet", ".", "add", "(", "ner", ")", "\n", "arc_alphabet", ".", "add", "(", "arc_tag", ")", "\n", "if", "embedd_dict", "is", "not", "None", ":", "\n", "                        ", "if", "word", "not", "in", "vocab_set", "and", "(", "word", "in", "embedd_dict", "or", "word", ".", "lower", "(", ")", "in", "embedd_dict", ")", ":", "\n", "                            ", "vocab_set", ".", "add", "(", "word", ")", "\n", "vocab_list", ".", "append", "(", "word", ")", "\n", "", "", "else", ":", "\n", "                        ", "if", "word", "not", "in", "vocab_set", ":", "\n", "                            ", "vocab_set", ".", "add", "(", "word", ")", "\n", "vocab_list", ".", "append", "(", "word", ")", "\n", "", "", "", "", "", "return", "vocab_list", ",", "char_alphabet", ",", "pos_alphabet", ",", "ner_alphabet", ",", "arc_alphabet", "\n", "\n", "", "logger", "=", "get_logger", "(", "\"Create Alphabets\"", ")", "\n", "word_alphabet", "=", "Alphabet", "(", "'word'", ",", "defualt_value", "=", "True", ",", "singleton", "=", "True", ")", "\n", "char_alphabet", "=", "Alphabet", "(", "'character'", ",", "defualt_value", "=", "True", ")", "\n", "pos_alphabet", "=", "Alphabet", "(", "'pos'", ",", "defualt_value", "=", "True", ")", "\n", "ner_alphabet", "=", "Alphabet", "(", "'ner'", ",", "defualt_value", "=", "True", ")", "\n", "arc_alphabet", "=", "Alphabet", "(", "'arc'", ",", "defualt_value", "=", "True", ")", "\n", "auto_label_alphabet", "=", "Alphabet", "(", "'auto_labeler'", ",", "defualt_value", "=", "True", ")", "\n", "if", "not", "os", ".", "path", ".", "isdir", "(", "alphabet_directory", ")", ":", "\n", "        ", "logger", ".", "info", "(", "\"Creating Alphabets: %s\"", "%", "alphabet_directory", ")", "\n", "\n", "char_alphabet", ".", "add", "(", "PAD", ")", "\n", "pos_alphabet", ".", "add", "(", "PAD", ")", "\n", "ner_alphabet", ".", "add", "(", "PAD", ")", "\n", "arc_alphabet", ".", "add", "(", "PAD", ")", "\n", "auto_label_alphabet", ".", "add", "(", "PAD", ")", "\n", "\n", "char_alphabet", ".", "add", "(", "ROOT", ")", "\n", "pos_alphabet", ".", "add", "(", "ROOT", ")", "\n", "ner_alphabet", ".", "add", "(", "ROOT", ")", "\n", "arc_alphabet", ".", "add", "(", "ROOT", ")", "\n", "auto_label_alphabet", ".", "add", "(", "ROOT", ")", "\n", "\n", "char_alphabet", ".", "add", "(", "END", ")", "\n", "pos_alphabet", ".", "add", "(", "END", ")", "\n", "ner_alphabet", ".", "add", "(", "END", ")", "\n", "arc_alphabet", ".", "add", "(", "END", ")", "\n", "auto_label_alphabet", ".", "add", "(", "END", ")", "\n", "\n", "vocab", "=", "dict", "(", ")", "\n", "if", "isinstance", "(", "train_paths", ",", "str", ")", ":", "\n", "            ", "train_paths", "=", "[", "train_paths", "]", "\n", "", "for", "train_path", "in", "train_paths", ":", "\n", "            ", "with", "open", "(", "train_path", ",", "'r'", ")", "as", "file", ":", "\n", "                ", "for", "line", "in", "file", ":", "\n", "                    ", "line", "=", "line", ".", "strip", "(", ")", "\n", "if", "len", "(", "line", ")", "==", "0", ":", "\n", "                        ", "continue", "\n", "\n", "", "tokens", "=", "line", ".", "split", "(", "'\\t'", ")", "\n", "if", "lower_case", ":", "\n", "                        ", "tokens", "[", "1", "]", "=", "tokens", "[", "1", "]", ".", "lower", "(", ")", "\n", "", "for", "char", "in", "tokens", "[", "1", "]", ":", "\n", "                        ", "char_alphabet", ".", "add", "(", "char", ")", "\n", "\n", "", "word", "=", "tokens", "[", "1", "]", "\n", "# print(word)", "\n", "pos", "=", "tokens", "[", "2", "]", "\n", "ner", "=", "tokens", "[", "3", "]", "\n", "arc_tag", "=", "tokens", "[", "5", "]", "\n", "\n", "pos_alphabet", ".", "add", "(", "pos", ")", "\n", "ner_alphabet", ".", "add", "(", "ner", ")", "\n", "arc_alphabet", ".", "add", "(", "arc_tag", ")", "\n", "\n", "if", "word", "in", "vocab", ":", "\n", "                        ", "vocab", "[", "word", "]", "+=", "1", "\n", "", "else", ":", "\n", "                        ", "vocab", "[", "word", "]", "=", "1", "\n", "\n", "# collect singletons", "\n", "", "", "", "", "singletons", "=", "set", "(", "[", "word", "for", "word", ",", "count", "in", "vocab", ".", "items", "(", ")", "if", "count", "<=", "min_occurence", "]", ")", "\n", "\n", "# if a singleton is in pretrained embedding dict, set the count to min_occur + c", "\n", "if", "embedd_dict", "is", "not", "None", ":", "\n", "            ", "for", "word", "in", "vocab", ".", "keys", "(", ")", ":", "\n", "                ", "if", "word", "in", "embedd_dict", "or", "word", ".", "lower", "(", ")", "in", "embedd_dict", ":", "\n", "                    ", "vocab", "[", "word", "]", "+=", "min_occurence", "\n", "\n", "", "", "", "vocab_list", "=", "sorted", "(", "vocab", ",", "key", "=", "vocab", ".", "get", ",", "reverse", "=", "True", ")", "\n", "vocab_list", "=", "[", "word", "for", "word", "in", "vocab_list", "if", "vocab", "[", "word", "]", ">", "min_occurence", "]", "\n", "vocab_list", "=", "_START_VOCAB", "+", "vocab_list", "\n", "\n", "if", "extra_paths", "is", "not", "None", ":", "\n", "            ", "vocab_list", ",", "char_alphabet", ",", "pos_alphabet", ",", "ner_alphabet", ",", "arc_alphabet", "=", "expand_vocab", "(", "vocab_list", ",", "char_alphabet", ",", "pos_alphabet", ",", "ner_alphabet", ",", "arc_alphabet", ")", "\n", "\n", "", "if", "len", "(", "vocab_list", ")", ">", "max_vocabulary_size", ":", "\n", "            ", "vocab_list", "=", "vocab_list", "[", ":", "max_vocabulary_size", "]", "\n", "\n", "", "for", "word", "in", "vocab_list", ":", "\n", "            ", "word_alphabet", ".", "add", "(", "word", ")", "\n", "if", "word", "in", "singletons", ":", "\n", "                ", "word_alphabet", ".", "add_singleton", "(", "word_alphabet", ".", "get_index", "(", "word", ")", ")", "\n", "\n", "", "", "word_alphabet", ".", "save", "(", "alphabet_directory", ")", "\n", "char_alphabet", ".", "save", "(", "alphabet_directory", ")", "\n", "pos_alphabet", ".", "save", "(", "alphabet_directory", ")", "\n", "ner_alphabet", ".", "save", "(", "alphabet_directory", ")", "\n", "arc_alphabet", ".", "save", "(", "alphabet_directory", ")", "\n", "auto_label_alphabet", ".", "save", "(", "alphabet_directory", ")", "\n", "\n", "", "else", ":", "\n", "        ", "print", "(", "'loading saved alphabet from %s'", "%", "alphabet_directory", ")", "\n", "word_alphabet", ".", "load", "(", "alphabet_directory", ")", "\n", "char_alphabet", ".", "load", "(", "alphabet_directory", ")", "\n", "pos_alphabet", ".", "load", "(", "alphabet_directory", ")", "\n", "ner_alphabet", ".", "load", "(", "alphabet_directory", ")", "\n", "arc_alphabet", ".", "load", "(", "alphabet_directory", ")", "\n", "auto_label_alphabet", ".", "load", "(", "alphabet_directory", ")", "\n", "\n", "", "word_alphabet", ".", "close", "(", ")", "\n", "char_alphabet", ".", "close", "(", ")", "\n", "pos_alphabet", ".", "close", "(", ")", "\n", "ner_alphabet", ".", "close", "(", ")", "\n", "arc_alphabet", ".", "close", "(", ")", "\n", "auto_label_alphabet", ".", "close", "(", ")", "\n", "\n", "alphabet_dict", "=", "{", "'word_alphabet'", ":", "word_alphabet", ",", "'char_alphabet'", ":", "char_alphabet", ",", "'pos_alphabet'", ":", "pos_alphabet", ",", "\n", "'ner_alphabet'", ":", "ner_alphabet", ",", "'arc_alphabet'", ":", "arc_alphabet", ",", "'auto_label_alphabet'", ":", "auto_label_alphabet", "}", "\n", "return", "alphabet_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.prepare_data.create_alphabets_for_sequence_tagger": [[173, 221], ["logger.get_logger", "print", "alphabet.Alphabet", "alphabet.Alphabet", "alphabet.Alphabet", "alphabet.Alphabet", "alphabet.Alphabet", "alphabet.Alphabet", "alphabet.Alphabet.load", "alphabet.Alphabet.load", "alphabet.Alphabet.load", "alphabet.Alphabet.load", "alphabet.Alphabet.load", "alphabet.Alphabet.save", "alphabet.Alphabet.save", "alphabet.Alphabet.save", "alphabet.Alphabet.save", "alphabet.Alphabet.save", "alphabet.Alphabet.save", "alphabet.Alphabet.close", "alphabet.Alphabet.close", "alphabet.Alphabet.close", "alphabet.Alphabet.close", "alphabet.Alphabet.close", "alphabet.Alphabet.close", "alphabet.Alphabet.load", "print", "alphabet.Alphabet.add", "alphabet.Alphabet.add", "alphabet.Alphabet.add", "open", "line.strip.strip", "line.strip.split", "len", "len", "alphabet.Alphabet.add"], "function", ["home.repos.pwc.inspect_result.Jivnesh_LCM.io_.logger.get_logger", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.load", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.load", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.load", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.load", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.load", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.save", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.save", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.save", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.save", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.save", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.save", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.reader.Reader.close", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.reader.Reader.close", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.reader.Reader.close", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.reader.Reader.close", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.reader.Reader.close", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.reader.Reader.close", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.load", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.add", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.add", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.add", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.open", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.add"], ["", "def", "create_alphabets_for_sequence_tagger", "(", "alphabet_directory", ",", "parser_alphabet_directory", ",", "paths", ")", ":", "\n", "    ", "logger", "=", "get_logger", "(", "\"Create Alphabets\"", ")", "\n", "print", "(", "'loading saved alphabet from %s'", "%", "parser_alphabet_directory", ")", "\n", "word_alphabet", "=", "Alphabet", "(", "'word'", ",", "defualt_value", "=", "True", ",", "singleton", "=", "True", ")", "\n", "char_alphabet", "=", "Alphabet", "(", "'character'", ",", "defualt_value", "=", "True", ")", "\n", "pos_alphabet", "=", "Alphabet", "(", "'pos'", ",", "defualt_value", "=", "True", ")", "\n", "ner_alphabet", "=", "Alphabet", "(", "'ner'", ",", "defualt_value", "=", "True", ")", "\n", "arc_alphabet", "=", "Alphabet", "(", "'arc'", ",", "defualt_value", "=", "True", ")", "\n", "auto_label_alphabet", "=", "Alphabet", "(", "'auto_labeler'", ",", "defualt_value", "=", "True", ")", "\n", "\n", "word_alphabet", ".", "load", "(", "parser_alphabet_directory", ")", "\n", "char_alphabet", ".", "load", "(", "parser_alphabet_directory", ")", "\n", "pos_alphabet", ".", "load", "(", "parser_alphabet_directory", ")", "\n", "ner_alphabet", ".", "load", "(", "parser_alphabet_directory", ")", "\n", "arc_alphabet", ".", "load", "(", "parser_alphabet_directory", ")", "\n", "try", ":", "\n", "        ", "auto_label_alphabet", ".", "load", "(", "alphabet_directory", ")", "\n", "", "except", ":", "\n", "        ", "print", "(", "'Creating auto labeler alphabet'", ")", "\n", "auto_label_alphabet", ".", "add", "(", "PAD", ")", "\n", "auto_label_alphabet", ".", "add", "(", "ROOT", ")", "\n", "auto_label_alphabet", ".", "add", "(", "END", ")", "\n", "for", "path", "in", "paths", ":", "\n", "            ", "with", "open", "(", "path", ",", "'r'", ")", "as", "file", ":", "\n", "                ", "for", "line", "in", "file", ":", "\n", "                    ", "line", "=", "line", ".", "strip", "(", ")", "\n", "if", "len", "(", "line", ")", "==", "0", ":", "\n", "                        ", "continue", "\n", "", "tokens", "=", "line", ".", "split", "(", "'\\t'", ")", "\n", "if", "len", "(", "tokens", ")", ">", "6", ":", "\n", "                        ", "auto_label", "=", "tokens", "[", "6", "]", "\n", "auto_label_alphabet", ".", "add", "(", "auto_label", ")", "\n", "\n", "", "", "", "", "", "word_alphabet", ".", "save", "(", "alphabet_directory", ")", "\n", "char_alphabet", ".", "save", "(", "alphabet_directory", ")", "\n", "pos_alphabet", ".", "save", "(", "alphabet_directory", ")", "\n", "ner_alphabet", ".", "save", "(", "alphabet_directory", ")", "\n", "arc_alphabet", ".", "save", "(", "alphabet_directory", ")", "\n", "auto_label_alphabet", ".", "save", "(", "alphabet_directory", ")", "\n", "word_alphabet", ".", "close", "(", ")", "\n", "char_alphabet", ".", "close", "(", ")", "\n", "pos_alphabet", ".", "close", "(", ")", "\n", "ner_alphabet", ".", "close", "(", ")", "\n", "arc_alphabet", ".", "close", "(", ")", "\n", "auto_label_alphabet", ".", "close", "(", ")", "\n", "alphabet_dict", "=", "{", "'word_alphabet'", ":", "word_alphabet", ",", "'char_alphabet'", ":", "char_alphabet", ",", "'pos_alphabet'", ":", "pos_alphabet", ",", "\n", "'ner_alphabet'", ":", "ner_alphabet", ",", "'arc_alphabet'", ":", "arc_alphabet", ",", "'auto_label_alphabet'", ":", "auto_label_alphabet", "}", "\n", "return", "alphabet_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.prepare_data.read_data": [[222, 250], ["print", "print", "type", "reader.Reader", "reader.Reader.getNext", "reader.Reader.close", "reader.getNext.length", "enumerate", "reader.Reader.getNext", "type", "data[].append", "max", "len"], "function", ["home.repos.pwc.inspect_result.Jivnesh_LCM.io_.reader.Reader.getNext", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.reader.Reader.close", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.instance.NER_DependencyInstance.length", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.reader.Reader.getNext"], ["", "def", "read_data", "(", "source_path", ",", "alphabets", ",", "max_size", "=", "None", ",", "\n", "lower_case", "=", "False", ",", "symbolic_root", "=", "False", ",", "symbolic_end", "=", "False", ")", ":", "\n", "    ", "data", "=", "[", "[", "]", "for", "_", "in", "_buckets", "]", "\n", "max_char_length", "=", "[", "0", "for", "_", "in", "_buckets", "]", "\n", "print", "(", "'Reading data from %s'", "%", "', '", ".", "join", "(", "source_path", ")", "if", "type", "(", "source_path", ")", "is", "list", "else", "source_path", ")", "\n", "counter", "=", "0", "\n", "if", "type", "(", "source_path", ")", "is", "not", "list", ":", "\n", "        ", "source_path", "=", "[", "source_path", "]", "\n", "", "for", "path", "in", "source_path", ":", "\n", "        ", "reader", "=", "Reader", "(", "path", ",", "alphabets", ")", "\n", "inst", "=", "reader", ".", "getNext", "(", "lower_case", "=", "lower_case", ",", "symbolic_root", "=", "symbolic_root", ",", "symbolic_end", "=", "symbolic_end", ")", "\n", "while", "inst", "is", "not", "None", "and", "(", "not", "max_size", "or", "counter", "<", "max_size", ")", ":", "\n", "            ", "counter", "+=", "1", "\n", "inst_size", "=", "inst", ".", "length", "(", ")", "\n", "sent", "=", "inst", ".", "sentence", "\n", "for", "bucket_id", ",", "bucket_size", "in", "enumerate", "(", "_buckets", ")", ":", "\n", "                ", "if", "inst_size", "<", "bucket_size", ":", "\n", "                    ", "data", "[", "bucket_id", "]", ".", "append", "(", "[", "sent", ".", "word_ids", ",", "sent", ".", "char_id_seqs", ",", "inst", ".", "ids", "[", "'pos_alphabet'", "]", ",", "inst", ".", "ids", "[", "'ner_alphabet'", "]", ",", "\n", "inst", ".", "heads", ",", "inst", ".", "ids", "[", "'arc_alphabet'", "]", ",", "inst", ".", "ids", "[", "'auto_label_alphabet'", "]", "]", ")", "\n", "max_len", "=", "max", "(", "[", "len", "(", "char_seq", ")", "for", "char_seq", "in", "sent", ".", "char_seqs", "]", ")", "\n", "if", "max_char_length", "[", "bucket_id", "]", "<", "max_len", ":", "\n", "                        ", "max_char_length", "[", "bucket_id", "]", "=", "max_len", "\n", "", "break", "\n", "\n", "", "", "inst", "=", "reader", ".", "getNext", "(", "lower_case", "=", "lower_case", ",", "symbolic_root", "=", "symbolic_root", ",", "symbolic_end", "=", "symbolic_end", ")", "\n", "", "reader", ".", "close", "(", ")", "\n", "", "print", "(", "\"Total number of data: %d\"", "%", "counter", ")", "\n", "return", "data", ",", "max_char_length", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.prepare_data.read_data_to_variable": [[251, 337], ["prepare_data.read_data", "range", "len", "len", "min", "numpy.empty", "numpy.empty", "numpy.empty", "numpy.empty", "numpy.empty", "numpy.empty", "numpy.empty", "numpy.zeros", "numpy.zeros", "numpy.empty", "enumerate", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.FloatTensor", "torch.LongTensor", "torch.LongTensor", "words.to.to", "chars.to.to", "pos.to.to", "ner.to.to", "heads.to.to", "arc.to.to", "auto_label.to.to", "masks.to.to", "single.to.to", "lengths.to.to", "data_variable.append", "range", "data_variable.append", "len", "enumerate", "enumerate", "len", "alphabets[].is_singleton", "len", "len"], "function", ["home.repos.pwc.inspect_result.Jivnesh_LCM.io_.prepare_data.read_data", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.is_singleton"], ["", "def", "read_data_to_variable", "(", "source_path", ",", "alphabets", ",", "device", ",", "max_size", "=", "None", ",", "\n", "lower_case", "=", "False", ",", "symbolic_root", "=", "False", ",", "symbolic_end", "=", "False", ")", ":", "\n", "    ", "data", ",", "max_char_length", "=", "read_data", "(", "source_path", ",", "alphabets", ",", "\n", "max_size", "=", "max_size", ",", "lower_case", "=", "lower_case", ",", "\n", "symbolic_root", "=", "symbolic_root", ",", "symbolic_end", "=", "symbolic_end", ")", "\n", "bucket_sizes", "=", "[", "len", "(", "data", "[", "b", "]", ")", "for", "b", "in", "range", "(", "len", "(", "_buckets", ")", ")", "]", "\n", "\n", "data_variable", "=", "[", "]", "\n", "\n", "for", "bucket_id", "in", "range", "(", "len", "(", "_buckets", ")", ")", ":", "\n", "        ", "bucket_size", "=", "bucket_sizes", "[", "bucket_id", "]", "\n", "if", "bucket_size", "<=", "0", ":", "\n", "            ", "data_variable", ".", "append", "(", "(", "1", ",", "1", ")", ")", "\n", "continue", "\n", "\n", "", "bucket_length", "=", "_buckets", "[", "bucket_id", "]", "\n", "char_length", "=", "min", "(", "MAX_CHAR_LENGTH", ",", "max_char_length", "[", "bucket_id", "]", "+", "NUM_CHAR_PAD", ")", "\n", "wid_inputs", "=", "np", ".", "empty", "(", "[", "bucket_size", ",", "bucket_length", "]", ",", "dtype", "=", "np", ".", "int64", ")", "\n", "cid_inputs", "=", "np", ".", "empty", "(", "[", "bucket_size", ",", "bucket_length", ",", "char_length", "]", ",", "dtype", "=", "np", ".", "int64", ")", "\n", "pid_inputs", "=", "np", ".", "empty", "(", "[", "bucket_size", ",", "bucket_length", "]", ",", "dtype", "=", "np", ".", "int64", ")", "\n", "nid_inputs", "=", "np", ".", "empty", "(", "[", "bucket_size", ",", "bucket_length", "]", ",", "dtype", "=", "np", ".", "int64", ")", "\n", "hid_inputs", "=", "np", ".", "empty", "(", "[", "bucket_size", ",", "bucket_length", "]", ",", "dtype", "=", "np", ".", "int64", ")", "\n", "aid_inputs", "=", "np", ".", "empty", "(", "[", "bucket_size", ",", "bucket_length", "]", ",", "dtype", "=", "np", ".", "int64", ")", "\n", "mid_inputs", "=", "np", ".", "empty", "(", "[", "bucket_size", ",", "bucket_length", "]", ",", "dtype", "=", "np", ".", "int64", ")", "\n", "\n", "masks", "=", "np", ".", "zeros", "(", "[", "bucket_size", ",", "bucket_length", "]", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "single", "=", "np", ".", "zeros", "(", "[", "bucket_size", ",", "bucket_length", "]", ",", "dtype", "=", "np", ".", "int64", ")", "\n", "\n", "lengths", "=", "np", ".", "empty", "(", "bucket_size", ",", "dtype", "=", "np", ".", "int64", ")", "\n", "\n", "for", "i", ",", "inst", "in", "enumerate", "(", "data", "[", "bucket_id", "]", ")", ":", "\n", "            ", "wids", ",", "cid_seqs", ",", "pids", ",", "nids", ",", "hids", ",", "aids", ",", "mids", "=", "inst", "\n", "inst_size", "=", "len", "(", "wids", ")", "\n", "lengths", "[", "i", "]", "=", "inst_size", "\n", "# word ids", "\n", "wid_inputs", "[", "i", ",", ":", "inst_size", "]", "=", "wids", "\n", "wid_inputs", "[", "i", ",", "inst_size", ":", "]", "=", "PAD_ID_WORD", "\n", "for", "c", ",", "cids", "in", "enumerate", "(", "cid_seqs", ")", ":", "\n", "                ", "cid_inputs", "[", "i", ",", "c", ",", ":", "len", "(", "cids", ")", "]", "=", "cids", "\n", "cid_inputs", "[", "i", ",", "c", ",", "len", "(", "cids", ")", ":", "]", "=", "PAD_ID_CHAR", "\n", "", "cid_inputs", "[", "i", ",", "inst_size", ":", ",", ":", "]", "=", "PAD_ID_CHAR", "\n", "# pos ids", "\n", "pid_inputs", "[", "i", ",", ":", "inst_size", "]", "=", "pids", "\n", "pid_inputs", "[", "i", ",", "inst_size", ":", "]", "=", "PAD_ID_TAG", "\n", "# ner ids", "\n", "nid_inputs", "[", "i", ",", ":", "inst_size", "]", "=", "nids", "\n", "nid_inputs", "[", "i", ",", "inst_size", ":", "]", "=", "PAD_ID_TAG", "\n", "# arc ids", "\n", "aid_inputs", "[", "i", ",", ":", "inst_size", "]", "=", "aids", "\n", "aid_inputs", "[", "i", ",", "inst_size", ":", "]", "=", "PAD_ID_TAG", "\n", "# auto_label ids", "\n", "mid_inputs", "[", "i", ",", ":", "inst_size", "]", "=", "mids", "\n", "mid_inputs", "[", "i", ",", "inst_size", ":", "]", "=", "PAD_ID_TAG", "\n", "# heads", "\n", "hid_inputs", "[", "i", ",", ":", "inst_size", "]", "=", "hids", "\n", "hid_inputs", "[", "i", ",", "inst_size", ":", "]", "=", "PAD_ID_TAG", "\n", "# masks", "\n", "masks", "[", "i", ",", ":", "inst_size", "]", "=", "1.0", "\n", "for", "j", ",", "wid", "in", "enumerate", "(", "wids", ")", ":", "\n", "                ", "if", "alphabets", "[", "'word_alphabet'", "]", ".", "is_singleton", "(", "wid", ")", ":", "\n", "                    ", "single", "[", "i", ",", "j", "]", "=", "1", "\n", "\n", "", "", "", "words", "=", "torch", ".", "LongTensor", "(", "wid_inputs", ")", "\n", "chars", "=", "torch", ".", "LongTensor", "(", "cid_inputs", ")", "\n", "pos", "=", "torch", ".", "LongTensor", "(", "pid_inputs", ")", "\n", "ner", "=", "torch", ".", "LongTensor", "(", "nid_inputs", ")", "\n", "heads", "=", "torch", ".", "LongTensor", "(", "hid_inputs", ")", "\n", "arc", "=", "torch", ".", "LongTensor", "(", "aid_inputs", ")", "\n", "auto_label", "=", "torch", ".", "LongTensor", "(", "mid_inputs", ")", "\n", "masks", "=", "torch", ".", "FloatTensor", "(", "masks", ")", "\n", "single", "=", "torch", ".", "LongTensor", "(", "single", ")", "\n", "lengths", "=", "torch", ".", "LongTensor", "(", "lengths", ")", "\n", "words", "=", "words", ".", "to", "(", "device", ")", "\n", "chars", "=", "chars", ".", "to", "(", "device", ")", "\n", "pos", "=", "pos", ".", "to", "(", "device", ")", "\n", "ner", "=", "ner", ".", "to", "(", "device", ")", "\n", "heads", "=", "heads", ".", "to", "(", "device", ")", "\n", "arc", "=", "arc", ".", "to", "(", "device", ")", "\n", "auto_label", "=", "auto_label", ".", "to", "(", "device", ")", "\n", "masks", "=", "masks", ".", "to", "(", "device", ")", "\n", "single", "=", "single", ".", "to", "(", "device", ")", "\n", "lengths", "=", "lengths", ".", "to", "(", "device", ")", "\n", "\n", "data_variable", ".", "append", "(", "(", "words", ",", "chars", ",", "pos", ",", "ner", ",", "heads", ",", "arc", ",", "auto_label", ",", "masks", ",", "single", ",", "lengths", ")", ")", "\n", "\n", "", "return", "data_variable", ",", "bucket_sizes", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.prepare_data.iterate_batch": [[338, 368], ["numpy.arange", "len", "numpy.random.shuffle", "range", "single.data.new().fill_", "masks.data.new().bernoulli_().long", "torch.randperm().long", "indices.to.to", "slice", "single.data.new", "masks.data.new().bernoulli_", "torch.randperm", "masks.data.new"], "function", ["None"], ["", "def", "iterate_batch", "(", "data", ",", "batch_size", ",", "device", ",", "unk_replace", "=", "0.0", ",", "shuffle", "=", "False", ")", ":", "\n", "    ", "data_variable", ",", "bucket_sizes", "=", "data", "\n", "\n", "bucket_indices", "=", "np", ".", "arange", "(", "len", "(", "_buckets", ")", ")", "\n", "if", "shuffle", ":", "\n", "        ", "np", ".", "random", ".", "shuffle", "(", "(", "bucket_indices", ")", ")", "\n", "\n", "", "for", "bucket_id", "in", "bucket_indices", ":", "\n", "        ", "bucket_size", "=", "bucket_sizes", "[", "bucket_id", "]", "\n", "bucket_length", "=", "_buckets", "[", "bucket_id", "]", "\n", "if", "bucket_size", "<=", "0", ":", "\n", "            ", "continue", "\n", "\n", "", "words", ",", "chars", ",", "pos", ",", "ner", ",", "heads", ",", "arc", ",", "auto_label", ",", "masks", ",", "single", ",", "lengths", "=", "data_variable", "[", "bucket_id", "]", "\n", "if", "unk_replace", ":", "\n", "            ", "ones", "=", "single", ".", "data", ".", "new", "(", "bucket_size", ",", "bucket_length", ")", ".", "fill_", "(", "1", ")", "\n", "noise", "=", "masks", ".", "data", ".", "new", "(", "bucket_size", ",", "bucket_length", ")", ".", "bernoulli_", "(", "unk_replace", ")", ".", "long", "(", ")", "\n", "words", "=", "words", "*", "(", "ones", "-", "single", "*", "noise", ")", "\n", "\n", "", "indices", "=", "None", "\n", "if", "shuffle", ":", "\n", "            ", "indices", "=", "torch", ".", "randperm", "(", "bucket_size", ")", ".", "long", "(", ")", "\n", "indices", "=", "indices", ".", "to", "(", "device", ")", "\n", "", "for", "start_idx", "in", "range", "(", "0", ",", "bucket_size", ",", "batch_size", ")", ":", "\n", "            ", "if", "shuffle", ":", "\n", "                ", "excerpt", "=", "indices", "[", "start_idx", ":", "start_idx", "+", "batch_size", "]", "\n", "", "else", ":", "\n", "                ", "excerpt", "=", "slice", "(", "start_idx", ",", "start_idx", "+", "batch_size", ")", "\n", "", "yield", "words", "[", "excerpt", "]", ",", "chars", "[", "excerpt", "]", ",", "pos", "[", "excerpt", "]", ",", "ner", "[", "excerpt", "]", ",", "heads", "[", "excerpt", "]", ",", "arc", "[", "excerpt", "]", ",", "auto_label", "[", "excerpt", "]", ",", "masks", "[", "excerpt", "]", ",", "lengths", "[", "excerpt", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.prepare_data.iterate_batch_rand_bucket_choosing": [[369, 391], ["set", "sum", "numpy.random.choice", "min", "torch.LongTensor", "set", "indices_left[].difference", "indices.to.to", "numpy.arange", "numpy.random.choice", "indices.to.numpy", "single.data.new().fill_", "masks.data.new().bernoulli_().long", "len", "enumerate", "list", "single.data.new", "masks.data.new().bernoulli_", "masks.data.new"], "function", ["None"], ["", "", "", "def", "iterate_batch_rand_bucket_choosing", "(", "data", ",", "batch_size", ",", "device", ",", "unk_replace", "=", "0.0", ")", ":", "\n", "    ", "data_variable", ",", "bucket_sizes", "=", "data", "\n", "indices_left", "=", "[", "set", "(", "np", ".", "arange", "(", "bucket_size", ")", ")", "for", "bucket_size", "in", "bucket_sizes", "]", "\n", "while", "sum", "(", "bucket_sizes", ")", ">", "0", ":", "\n", "        ", "non_empty_buckets", "=", "[", "i", "for", "i", ",", "bucket_size", "in", "enumerate", "(", "bucket_sizes", ")", "if", "bucket_size", ">", "0", "]", "\n", "bucket_id", "=", "np", ".", "random", ".", "choice", "(", "non_empty_buckets", ")", "\n", "bucket_size", "=", "bucket_sizes", "[", "bucket_id", "]", "\n", "bucket_length", "=", "_buckets", "[", "bucket_id", "]", "\n", "\n", "words", ",", "chars", ",", "pos", ",", "ner", ",", "heads", ",", "arc", ",", "auto_label", ",", "masks", ",", "single", ",", "lengths", "=", "data_variable", "[", "bucket_id", "]", "\n", "min_batch_size", "=", "min", "(", "bucket_size", ",", "batch_size", ")", "\n", "indices", "=", "torch", ".", "LongTensor", "(", "np", ".", "random", ".", "choice", "(", "list", "(", "indices_left", "[", "bucket_id", "]", ")", ",", "min_batch_size", ",", "replace", "=", "False", ")", ")", "\n", "set_indices", "=", "set", "(", "indices", ".", "numpy", "(", ")", ")", "\n", "indices_left", "[", "bucket_id", "]", "=", "indices_left", "[", "bucket_id", "]", ".", "difference", "(", "set_indices", ")", "\n", "indices", "=", "indices", ".", "to", "(", "device", ")", "\n", "words", "=", "words", "[", "indices", "]", "\n", "if", "unk_replace", ":", "\n", "            ", "ones", "=", "single", ".", "data", ".", "new", "(", "min_batch_size", ",", "bucket_length", ")", ".", "fill_", "(", "1", ")", "\n", "noise", "=", "masks", ".", "data", ".", "new", "(", "min_batch_size", ",", "bucket_length", ")", ".", "bernoulli_", "(", "unk_replace", ")", ".", "long", "(", ")", "\n", "words", "=", "words", "*", "(", "ones", "-", "single", "[", "indices", "]", "*", "noise", ")", "\n", "", "bucket_sizes", "=", "[", "len", "(", "s", ")", "for", "s", "in", "indices_left", "]", "\n", "yield", "words", ",", "chars", "[", "indices", "]", ",", "pos", "[", "indices", "]", ",", "ner", "[", "indices", "]", ",", "heads", "[", "indices", "]", ",", "arc", "[", "indices", "]", ",", "auto_label", "[", "indices", "]", ",", "masks", "[", "indices", "]", ",", "lengths", "[", "indices", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.prepare_data.calc_num_batches": [[393, 398], ["sum", "int"], "function", ["None"], ["", "", "def", "calc_num_batches", "(", "data", ",", "batch_size", ")", ":", "\n", "    ", "_", ",", "bucket_sizes", "=", "data", "\n", "bucket_sizes_mod_batch_size", "=", "[", "int", "(", "bucket_size", "/", "batch_size", ")", "+", "1", "if", "bucket_size", ">", "0", "else", "0", "for", "bucket_size", "in", "bucket_sizes", "]", "\n", "num_batches", "=", "sum", "(", "bucket_sizes_mod_batch_size", ")", "\n", "return", "num_batches", "", "", ""]], "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.instance.Sentence.__init__": [[2, 7], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "words", ",", "word_ids", ",", "char_seqs", ",", "char_id_seqs", ")", ":", "\n", "        ", "self", ".", "words", "=", "words", "\n", "self", ".", "word_ids", "=", "word_ids", "\n", "self", ".", "char_seqs", "=", "char_seqs", "\n", "self", ".", "char_id_seqs", "=", "char_id_seqs", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.instance.Sentence.length": [[8, 10], ["len"], "methods", ["None"], ["", "def", "length", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "words", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.instance.NER_DependencyInstance.__init__": [[12, 17], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "sentence", ",", "tokens_dict", ",", "ids_dict", ",", "heads", ")", ":", "\n", "        ", "self", ".", "sentence", "=", "sentence", "\n", "self", ".", "tokens", "=", "tokens_dict", "\n", "self", ".", "ids", "=", "ids_dict", "\n", "self", ".", "heads", "=", "heads", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.instance.NER_DependencyInstance.length": [[18, 20], ["instance.NER_DependencyInstance.sentence.length"], "methods", ["home.repos.pwc.inspect_result.Jivnesh_LCM.io_.instance.NER_DependencyInstance.length"], ["", "def", "length", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "sentence", ".", "length", "(", ")", "", "", "", ""]], "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.reader.Reader.__init__": [[6, 9], ["open"], "methods", ["home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.open"], ["    ", "def", "__init__", "(", "self", ",", "file_path", ",", "alphabets", ")", ":", "\n", "        ", "self", ".", "__source_file", "=", "open", "(", "file_path", ",", "'r'", ")", "\n", "self", ".", "alphabets", "=", "alphabets", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.reader.Reader.close": [[10, 12], ["reader.Reader.__source_file.close"], "methods", ["home.repos.pwc.inspect_result.Jivnesh_LCM.io_.reader.Reader.close"], ["", "def", "close", "(", "self", ")", ":", "\n", "        ", "self", ".", "__source_file", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.reader.Reader.getNext": [[13, 94], ["reader.Reader.__source_file.readline", "len", "reader.Reader.alphabets.keys", "instance.NER_DependencyInstance", "reader.Reader.__source_file.readline", "len", "len", "reader.Reader.strip", "lines.append", "reader.Reader.__source_file.readline", "reader.Reader.alphabets.items", "heads.append", "tokens_dict[].append", "ids_dict[].append", "int", "tokens_dict[].append", "ids_dict[].append", "tokens_dict[].append", "ids_dict[].append", "tokens_dict[].append", "ids_dict[].append", "tokens_dict[].append", "ids_dict[].append", "heads.append", "reader.Reader.alphabets.items", "heads.append", "instance.Sentence", "len", "len", "reader.Reader.strip", "reader.Reader.split", "alphabet_name.startswith", "tokens[].lower", "chars.append", "char_ids.append", "len", "len", "tokens_dict[].append", "ids_dict[].append", "reader.Reader.alphabets[].get_index", "reader.Reader.alphabets[].get_index", "reader.Reader.alphabets[].get_index", "reader.Reader.alphabets[].get_index", "alphabet_name.startswith", "reader.Reader.strip", "tokens_dict[].append", "ids_dict[].append", "tokens_dict[].append", "ids_dict[].append", "reader.Reader.alphabets[].get_index", "reader.Reader.alphabets[].get_index", "tokens_dict[].append", "ids_dict[].append", "alphabet.get_index", "alphabet.get_index", "alphabet.get_index", "alphabet.get_index"], "methods", ["home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.keys", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.items", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.items", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.get_index", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.get_index", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.get_index", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.get_index", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.get_index", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.get_index", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.get_index", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.get_index", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.get_index", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.get_index"], ["", "def", "getNext", "(", "self", ",", "lower_case", "=", "False", ",", "symbolic_root", "=", "False", ",", "symbolic_end", "=", "False", ")", ":", "\n", "        ", "line", "=", "self", ".", "__source_file", ".", "readline", "(", ")", "\n", "# skip multiple blank lines.", "\n", "while", "len", "(", "line", ")", ">", "0", "and", "len", "(", "line", ".", "strip", "(", ")", ")", "==", "0", ":", "\n", "            ", "line", "=", "self", ".", "__source_file", ".", "readline", "(", ")", "\n", "", "if", "len", "(", "line", ")", "==", "0", ":", "\n", "            ", "return", "None", "\n", "\n", "", "lines", "=", "[", "]", "\n", "while", "len", "(", "line", ".", "strip", "(", ")", ")", ">", "0", ":", "\n", "            ", "line", "=", "line", ".", "strip", "(", ")", "\n", "lines", ".", "append", "(", "line", ".", "split", "(", "'\\t'", ")", ")", "\n", "line", "=", "self", ".", "__source_file", ".", "readline", "(", ")", "\n", "\n", "", "length", "=", "len", "(", "lines", ")", "\n", "if", "length", "==", "0", ":", "\n", "            ", "return", "None", "\n", "\n", "", "heads", "=", "[", "]", "\n", "tokens_dict", "=", "{", "}", "\n", "ids_dict", "=", "{", "}", "\n", "for", "alphabet_name", "in", "self", ".", "alphabets", ".", "keys", "(", ")", ":", "\n", "            ", "tokens_dict", "[", "alphabet_name", "]", "=", "[", "]", "\n", "ids_dict", "[", "alphabet_name", "]", "=", "[", "]", "\n", "", "if", "symbolic_root", ":", "\n", "            ", "for", "alphabet_name", ",", "alphabet", "in", "self", ".", "alphabets", ".", "items", "(", ")", ":", "\n", "                ", "if", "alphabet_name", ".", "startswith", "(", "'char'", ")", ":", "\n", "                    ", "tokens_dict", "[", "alphabet_name", "]", ".", "append", "(", "[", "ROOT", ",", "]", ")", "\n", "ids_dict", "[", "alphabet_name", "]", ".", "append", "(", "[", "alphabet", ".", "get_index", "(", "ROOT", ")", ",", "]", ")", "\n", "", "else", ":", "\n", "                    ", "tokens_dict", "[", "alphabet_name", "]", ".", "append", "(", "ROOT", ")", "\n", "ids_dict", "[", "alphabet_name", "]", ".", "append", "(", "alphabet", ".", "get_index", "(", "ROOT", ")", ")", "\n", "", "", "heads", ".", "append", "(", "0", ")", "\n", "\n", "", "for", "tokens", "in", "lines", ":", "\n", "            ", "chars", "=", "[", "]", "\n", "char_ids", "=", "[", "]", "\n", "if", "lower_case", ":", "\n", "                ", "tokens", "[", "1", "]", "=", "tokens", "[", "1", "]", ".", "lower", "(", ")", "\n", "", "for", "char", "in", "tokens", "[", "1", "]", ":", "\n", "                ", "chars", ".", "append", "(", "char", ")", "\n", "char_ids", ".", "append", "(", "self", ".", "alphabets", "[", "'char_alphabet'", "]", ".", "get_index", "(", "char", ")", ")", "\n", "", "if", "len", "(", "chars", ")", ">", "MAX_CHAR_LENGTH", ":", "\n", "                ", "chars", "=", "chars", "[", ":", "MAX_CHAR_LENGTH", "]", "\n", "char_ids", "=", "char_ids", "[", ":", "MAX_CHAR_LENGTH", "]", "\n", "", "tokens_dict", "[", "'char_alphabet'", "]", ".", "append", "(", "chars", ")", "\n", "ids_dict", "[", "'char_alphabet'", "]", ".", "append", "(", "char_ids", ")", "\n", "\n", "word", "=", "tokens", "[", "1", "]", "\n", "# print(word+ ' ')", "\n", "pos", "=", "tokens", "[", "2", "]", "\n", "ner", "=", "tokens", "[", "3", "]", "\n", "head", "=", "int", "(", "tokens", "[", "4", "]", ")", "\n", "arc_tag", "=", "tokens", "[", "5", "]", "\n", "if", "len", "(", "tokens", ")", ">", "6", ":", "\n", "                ", "auto_label", "=", "tokens", "[", "6", "]", "\n", "tokens_dict", "[", "'auto_label_alphabet'", "]", ".", "append", "(", "auto_label", ")", "\n", "ids_dict", "[", "'auto_label_alphabet'", "]", ".", "append", "(", "self", ".", "alphabets", "[", "'auto_label_alphabet'", "]", ".", "get_index", "(", "auto_label", ")", ")", "\n", "", "tokens_dict", "[", "'word_alphabet'", "]", ".", "append", "(", "word", ")", "\n", "ids_dict", "[", "'word_alphabet'", "]", ".", "append", "(", "self", ".", "alphabets", "[", "'word_alphabet'", "]", ".", "get_index", "(", "word", ")", ")", "\n", "tokens_dict", "[", "'pos_alphabet'", "]", ".", "append", "(", "pos", ")", "\n", "ids_dict", "[", "'pos_alphabet'", "]", ".", "append", "(", "self", ".", "alphabets", "[", "'pos_alphabet'", "]", ".", "get_index", "(", "pos", ")", ")", "\n", "tokens_dict", "[", "'ner_alphabet'", "]", ".", "append", "(", "ner", ")", "\n", "ids_dict", "[", "'ner_alphabet'", "]", ".", "append", "(", "self", ".", "alphabets", "[", "'ner_alphabet'", "]", ".", "get_index", "(", "ner", ")", ")", "\n", "tokens_dict", "[", "'arc_alphabet'", "]", ".", "append", "(", "arc_tag", ")", "\n", "ids_dict", "[", "'arc_alphabet'", "]", ".", "append", "(", "self", ".", "alphabets", "[", "'arc_alphabet'", "]", ".", "get_index", "(", "arc_tag", ")", ")", "\n", "heads", ".", "append", "(", "head", ")", "\n", "\n", "", "if", "symbolic_end", ":", "\n", "            ", "for", "alphabet_name", ",", "alphabet", "in", "self", ".", "alphabets", ".", "items", "(", ")", ":", "\n", "                ", "if", "alphabet_name", ".", "startswith", "(", "'char'", ")", ":", "\n", "                    ", "tokens_dict", "[", "alphabet_name", "]", ".", "append", "(", "[", "END", ",", "]", ")", "\n", "ids_dict", "[", "alphabet_name", "]", ".", "append", "(", "[", "alphabet", ".", "get_index", "(", "END", ")", ",", "]", ")", "\n", "", "else", ":", "\n", "                    ", "tokens_dict", "[", "alphabet_name", "]", "=", "[", "END", "]", "\n", "ids_dict", "[", "alphabet_name", "]", "=", "[", "alphabet", ".", "get_index", "(", "END", ")", "]", "\n", "", "", "heads", ".", "append", "(", "0", ")", "\n", "\n", "", "return", "NER_DependencyInstance", "(", "Sentence", "(", "tokens_dict", "[", "'word_alphabet'", "]", ",", "ids_dict", "[", "'word_alphabet'", "]", ",", "\n", "tokens_dict", "[", "'char_alphabet'", "]", ",", "ids_dict", "[", "'char_alphabet'", "]", ")", ",", "\n", "tokens_dict", ",", "ids_dict", ",", "heads", ")", "", "", "", ""]], "home.repos.pwc.inspect_result.Jivnesh_LCM.examples.GraphParser_MRL.read_arguments": [[28, 195], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "os.path.join", "tuple", "tuple", "os.path.join", "os.path.join", "torch.device", "torch.device", "logger.info", "GraphParser_MRL.save_args", "logger.info", "GraphParser_MRL.creating_alphabets", "collections.namedtuple", "collections.namedtuple.", "os.path.exists", "os.makedirs", "set", "logger.info", "utils.load_word_embeddings.load_embedding_dict", "utils.load_word_embeddings.load_embedding_dict", "utils.load_word_embeddings.load_embedding_dict", "args_dict.keys", "len", "torch.cuda.is_available", "torch.cuda.is_available", "len", "args_dict[].split"], "function", ["home.repos.pwc.inspect_result.Jivnesh_LCM.examples.GraphParser.save_args", "home.repos.pwc.inspect_result.Jivnesh_LCM.examples.GraphParser.creating_alphabets", "home.repos.pwc.inspect_result.Jivnesh_LCM.utils.load_word_embeddings.load_embedding_dict", "home.repos.pwc.inspect_result.Jivnesh_LCM.utils.load_word_embeddings.load_embedding_dict", "home.repos.pwc.inspect_result.Jivnesh_LCM.utils.load_word_embeddings.load_embedding_dict", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.keys"], ["def", "read_arguments", "(", ")", ":", "\n", "    ", "args_", "=", "argparse", ".", "ArgumentParser", "(", "description", "=", "'Sovling GraphParser'", ")", "\n", "args_", ".", "add_argument", "(", "'--dataset'", ",", "choices", "=", "[", "'ontonotes'", ",", "'ud'", "]", ",", "help", "=", "'Dataset'", ",", "required", "=", "True", ")", "\n", "args_", ".", "add_argument", "(", "'--domain'", ",", "help", "=", "'domain/language'", ",", "required", "=", "True", ")", "\n", "args_", ".", "add_argument", "(", "'--rnn_mode'", ",", "choices", "=", "[", "'RNN'", ",", "'LSTM'", ",", "'GRU'", "]", ",", "help", "=", "'architecture of rnn'", ",", "\n", "required", "=", "True", ")", "\n", "args_", ".", "add_argument", "(", "'--gating'", ",", "action", "=", "'store_true'", ",", "help", "=", "'use gated mechanism'", ")", "\n", "args_", ".", "add_argument", "(", "'--num_gates'", ",", "type", "=", "int", ",", "default", "=", "0", ",", "help", "=", "'number of gates for gating mechanism'", ")", "\n", "args_", ".", "add_argument", "(", "'--num_epochs'", ",", "type", "=", "int", ",", "default", "=", "200", ",", "help", "=", "'Number of training epochs'", ")", "\n", "args_", ".", "add_argument", "(", "'--batch_size'", ",", "type", "=", "int", ",", "default", "=", "64", ",", "help", "=", "'Number of sentences in each batch'", ")", "\n", "args_", ".", "add_argument", "(", "'--hidden_size'", ",", "type", "=", "int", ",", "default", "=", "256", ",", "help", "=", "'Number of hidden units in RNN'", ")", "\n", "args_", ".", "add_argument", "(", "'--arc_space'", ",", "type", "=", "int", ",", "default", "=", "128", ",", "help", "=", "'Dimension of tag space'", ")", "\n", "args_", ".", "add_argument", "(", "'--arc_tag_space'", ",", "type", "=", "int", ",", "default", "=", "128", ",", "help", "=", "'Dimension of tag space'", ")", "\n", "args_", ".", "add_argument", "(", "'--num_layers'", ",", "type", "=", "int", ",", "default", "=", "1", ",", "help", "=", "'Number of layers of RNN'", ")", "\n", "args_", ".", "add_argument", "(", "'--num_filters'", ",", "type", "=", "int", ",", "default", "=", "50", ",", "help", "=", "'Number of filters in CNN'", ")", "\n", "args_", ".", "add_argument", "(", "'--kernel_size'", ",", "type", "=", "int", ",", "default", "=", "3", ",", "help", "=", "'Size of Kernel for CNN'", ")", "\n", "args_", ".", "add_argument", "(", "'--use_pos'", ",", "action", "=", "'store_true'", ",", "help", "=", "'use part-of-speech embedding.'", ")", "\n", "args_", ".", "add_argument", "(", "'--use_char'", ",", "action", "=", "'store_true'", ",", "help", "=", "'use character embedding and CNN.'", ")", "\n", "args_", ".", "add_argument", "(", "'--word_dim'", ",", "type", "=", "int", ",", "default", "=", "300", ",", "help", "=", "'Dimension of word embeddings'", ")", "\n", "args_", ".", "add_argument", "(", "'--pos_dim'", ",", "type", "=", "int", ",", "default", "=", "50", ",", "help", "=", "'Dimension of POS embeddings'", ")", "\n", "args_", ".", "add_argument", "(", "'--char_dim'", ",", "type", "=", "int", ",", "default", "=", "50", ",", "help", "=", "'Dimension of Character embeddings'", ")", "\n", "args_", ".", "add_argument", "(", "'--initializer'", ",", "choices", "=", "[", "'xavier'", "]", ",", "help", "=", "'initialize model parameters'", ")", "\n", "args_", ".", "add_argument", "(", "'--opt'", ",", "choices", "=", "[", "'adam'", ",", "'sgd'", "]", ",", "help", "=", "'optimization algorithm'", ")", "\n", "args_", ".", "add_argument", "(", "'--momentum'", ",", "type", "=", "float", ",", "default", "=", "0.9", ",", "help", "=", "'momentum of optimizer'", ")", "\n", "args_", ".", "add_argument", "(", "'--betas'", ",", "nargs", "=", "2", ",", "type", "=", "float", ",", "default", "=", "[", "0.9", ",", "0.9", "]", ",", "help", "=", "'betas of optimizer'", ")", "\n", "args_", ".", "add_argument", "(", "'--learning_rate'", ",", "type", "=", "float", ",", "default", "=", "0.01", ",", "help", "=", "'Learning rate'", ")", "\n", "args_", ".", "add_argument", "(", "'--decay_rate'", ",", "type", "=", "float", ",", "default", "=", "0.05", ",", "help", "=", "'Decay rate of learning rate'", ")", "\n", "args_", ".", "add_argument", "(", "'--schedule'", ",", "type", "=", "int", ",", "help", "=", "'schedule for learning rate decay'", ")", "\n", "args_", ".", "add_argument", "(", "'--clip'", ",", "type", "=", "float", ",", "default", "=", "5.0", ",", "help", "=", "'gradient clipping'", ")", "\n", "args_", ".", "add_argument", "(", "'--gamma'", ",", "type", "=", "float", ",", "default", "=", "0.0", ",", "help", "=", "'weight for regularization'", ")", "\n", "args_", ".", "add_argument", "(", "'--epsilon'", ",", "type", "=", "float", ",", "default", "=", "1e-8", ",", "help", "=", "'epsilon for adam'", ")", "\n", "args_", ".", "add_argument", "(", "'--p_rnn'", ",", "nargs", "=", "2", ",", "type", "=", "float", ",", "required", "=", "True", ",", "help", "=", "'dropout rate for RNN'", ")", "\n", "args_", ".", "add_argument", "(", "'--p_in'", ",", "type", "=", "float", ",", "default", "=", "0.33", ",", "help", "=", "'dropout rate for input embeddings'", ")", "\n", "args_", ".", "add_argument", "(", "'--p_out'", ",", "type", "=", "float", ",", "default", "=", "0.33", ",", "help", "=", "'dropout rate for output layer'", ")", "\n", "args_", ".", "add_argument", "(", "'--arc_decode'", ",", "choices", "=", "[", "'mst'", ",", "'greedy'", "]", ",", "help", "=", "'arc decoding algorithm'", ",", "required", "=", "True", ")", "\n", "args_", ".", "add_argument", "(", "'--unk_replace'", ",", "type", "=", "float", ",", "default", "=", "0.", ",", "\n", "help", "=", "'The rate to replace a singleton word with UNK'", ")", "\n", "args_", ".", "add_argument", "(", "'--punct_set'", ",", "nargs", "=", "'+'", ",", "type", "=", "str", ",", "help", "=", "'List of punctuations'", ")", "\n", "args_", ".", "add_argument", "(", "'--word_embedding'", ",", "choices", "=", "[", "'random'", ",", "'glove'", ",", "'fasttext'", ",", "'word2vec'", "]", ",", "\n", "help", "=", "'Embedding for words'", ")", "\n", "args_", ".", "add_argument", "(", "'--word_path'", ",", "help", "=", "'path for word embedding dict - in case word_embedding is not random'", ")", "\n", "args_", ".", "add_argument", "(", "'--freeze_word_embeddings'", ",", "action", "=", "'store_true'", ",", "help", "=", "'frozen the word embedding (disable fine-tuning).'", ")", "\n", "args_", ".", "add_argument", "(", "'--freeze_sequence_taggers'", ",", "action", "=", "'store_true'", ",", "help", "=", "'frozen the BiLSTMs of the pre-trained taggers.'", ")", "\n", "args_", ".", "add_argument", "(", "'--char_embedding'", ",", "choices", "=", "[", "'random'", ",", "'hellwig'", "]", ",", "help", "=", "'Embedding for characters'", ",", "\n", "required", "=", "True", ")", "\n", "args_", ".", "add_argument", "(", "'--pos_embedding'", ",", "choices", "=", "[", "'random'", ",", "'one_hot'", "]", ",", "help", "=", "'Embedding for pos'", ",", "\n", "required", "=", "True", ")", "\n", "args_", ".", "add_argument", "(", "'--char_path'", ",", "help", "=", "'path for character embedding dict'", ")", "\n", "args_", ".", "add_argument", "(", "'--pos_path'", ",", "help", "=", "'path for pos embedding dict'", ")", "\n", "args_", ".", "add_argument", "(", "'--set_num_training_samples'", ",", "type", "=", "int", ",", "help", "=", "'downsampling training set to a fixed number of samples'", ")", "\n", "args_", ".", "add_argument", "(", "'--model_path'", ",", "help", "=", "'path for saving model file.'", ",", "required", "=", "True", ")", "\n", "args_", ".", "add_argument", "(", "'--load_path'", ",", "help", "=", "'path for loading saved source model file.'", ",", "default", "=", "None", ")", "\n", "args_", ".", "add_argument", "(", "'--load_sequence_taggers_paths'", ",", "nargs", "=", "'+'", ",", "help", "=", "'path for loading saved sequence_tagger saved_models files.'", ",", "default", "=", "None", ")", "\n", "args_", ".", "add_argument", "(", "'--strict'", ",", "action", "=", "'store_true'", ",", "help", "=", "'if True loaded model state should contin '", "\n", "'exactly the same keys as current model'", ")", "\n", "args_", ".", "add_argument", "(", "'--eval_mode'", ",", "action", "=", "'store_true'", ",", "help", "=", "'evaluating model without training it'", ")", "\n", "args", "=", "args_", ".", "parse_args", "(", ")", "\n", "args_dict", "=", "{", "}", "\n", "args_dict", "[", "'dataset'", "]", "=", "args", ".", "dataset", "\n", "args_dict", "[", "'domain'", "]", "=", "args", ".", "domain", "\n", "args_dict", "[", "'rnn_mode'", "]", "=", "args", ".", "rnn_mode", "\n", "args_dict", "[", "'gating'", "]", "=", "args", ".", "gating", "\n", "args_dict", "[", "'num_gates'", "]", "=", "args", ".", "num_gates", "\n", "args_dict", "[", "'arc_decode'", "]", "=", "args", ".", "arc_decode", "\n", "# args_dict['splits'] = ['train', 'dev', 'test']", "\n", "args_dict", "[", "'splits'", "]", "=", "[", "'train'", ",", "'dev'", ",", "'test'", ",", "'poetry'", ",", "'prose'", "]", "\n", "args_dict", "[", "'model_path'", "]", "=", "args", ".", "model_path", "\n", "if", "not", "path", ".", "exists", "(", "args_dict", "[", "'model_path'", "]", ")", ":", "\n", "        ", "makedirs", "(", "args_dict", "[", "'model_path'", "]", ")", "\n", "", "args_dict", "[", "'data_paths'", "]", "=", "{", "}", "\n", "if", "args_dict", "[", "'dataset'", "]", "==", "'ontonotes'", ":", "\n", "        ", "data_path", "=", "'data/Pre_MRL/onto_pos_ner_dp'", "\n", "", "else", ":", "\n", "        ", "data_path", "=", "'data/Prep_MRL/ud_pos_ner_dp'", "\n", "", "for", "split", "in", "args_dict", "[", "'splits'", "]", ":", "\n", "        ", "args_dict", "[", "'data_paths'", "]", "[", "split", "]", "=", "data_path", "+", "'_'", "+", "split", "+", "'_'", "+", "args_dict", "[", "'domain'", "]", "\n", "###################################    ", "\n", "", "args_dict", "[", "'data_paths'", "]", "[", "'poetry'", "]", "=", "data_path", "+", "'_'", "+", "'test'", "+", "'_'", "+", "args_dict", "[", "'domain'", "]", "\n", "args_dict", "[", "'data_paths'", "]", "[", "'prose'", "]", "=", "data_path", "+", "'_'", "+", "'test'", "+", "'_'", "+", "args_dict", "[", "'domain'", "]", "\n", "###################################", "\n", "args_dict", "[", "'alphabet_data_paths'", "]", "=", "{", "}", "\n", "for", "split", "in", "args_dict", "[", "'splits'", "]", ":", "\n", "        ", "if", "args_dict", "[", "'dataset'", "]", "==", "'ontonotes'", ":", "\n", "            ", "args_dict", "[", "'alphabet_data_paths'", "]", "[", "split", "]", "=", "data_path", "+", "'_'", "+", "split", "+", "'_'", "+", "'all'", "\n", "", "else", ":", "\n", "            ", "if", "'_'", "in", "args_dict", "[", "'domain'", "]", ":", "\n", "                ", "args_dict", "[", "'alphabet_data_paths'", "]", "[", "split", "]", "=", "data_path", "+", "'_'", "+", "split", "+", "'_'", "+", "args_dict", "[", "'domain'", "]", ".", "split", "(", "'_'", ")", "[", "0", "]", "\n", "", "else", ":", "\n", "                ", "args_dict", "[", "'alphabet_data_paths'", "]", "[", "split", "]", "=", "args_dict", "[", "'data_paths'", "]", "[", "split", "]", "\n", "", "", "", "args_dict", "[", "'model_name'", "]", "=", "'domain_'", "+", "args_dict", "[", "'domain'", "]", "\n", "args_dict", "[", "'full_model_name'", "]", "=", "path", ".", "join", "(", "args_dict", "[", "'model_path'", "]", ",", "args_dict", "[", "'model_name'", "]", ")", "\n", "args_dict", "[", "'load_path'", "]", "=", "args", ".", "load_path", "\n", "args_dict", "[", "'load_sequence_taggers_paths'", "]", "=", "args", ".", "load_sequence_taggers_paths", "\n", "if", "args_dict", "[", "'load_sequence_taggers_paths'", "]", "is", "not", "None", ":", "\n", "        ", "args_dict", "[", "'gating'", "]", "=", "True", "\n", "args_dict", "[", "'num_gates'", "]", "=", "len", "(", "args_dict", "[", "'load_sequence_taggers_paths'", "]", ")", "+", "1", "\n", "", "else", ":", "\n", "        ", "if", "not", "args_dict", "[", "'gating'", "]", ":", "\n", "            ", "args_dict", "[", "'num_gates'", "]", "=", "0", "\n", "", "", "args_dict", "[", "'strict'", "]", "=", "args", ".", "strict", "\n", "args_dict", "[", "'num_epochs'", "]", "=", "args", ".", "num_epochs", "\n", "args_dict", "[", "'batch_size'", "]", "=", "args", ".", "batch_size", "\n", "args_dict", "[", "'hidden_size'", "]", "=", "args", ".", "hidden_size", "\n", "args_dict", "[", "'arc_space'", "]", "=", "args", ".", "arc_space", "\n", "args_dict", "[", "'arc_tag_space'", "]", "=", "args", ".", "arc_tag_space", "\n", "args_dict", "[", "'num_layers'", "]", "=", "args", ".", "num_layers", "\n", "args_dict", "[", "'num_filters'", "]", "=", "args", ".", "num_filters", "\n", "args_dict", "[", "'kernel_size'", "]", "=", "args", ".", "kernel_size", "\n", "args_dict", "[", "'learning_rate'", "]", "=", "args", ".", "learning_rate", "\n", "args_dict", "[", "'initializer'", "]", "=", "nn", ".", "init", ".", "xavier_uniform_", "if", "args", ".", "initializer", "==", "'xavier'", "else", "None", "\n", "args_dict", "[", "'opt'", "]", "=", "args", ".", "opt", "\n", "args_dict", "[", "'momentum'", "]", "=", "args", ".", "momentum", "\n", "args_dict", "[", "'betas'", "]", "=", "tuple", "(", "args", ".", "betas", ")", "\n", "args_dict", "[", "'epsilon'", "]", "=", "args", ".", "epsilon", "\n", "args_dict", "[", "'decay_rate'", "]", "=", "args", ".", "decay_rate", "\n", "args_dict", "[", "'clip'", "]", "=", "args", ".", "clip", "\n", "args_dict", "[", "'gamma'", "]", "=", "args", ".", "gamma", "\n", "args_dict", "[", "'schedule'", "]", "=", "args", ".", "schedule", "\n", "args_dict", "[", "'p_rnn'", "]", "=", "tuple", "(", "args", ".", "p_rnn", ")", "\n", "args_dict", "[", "'p_in'", "]", "=", "args", ".", "p_in", "\n", "args_dict", "[", "'p_out'", "]", "=", "args", ".", "p_out", "\n", "args_dict", "[", "'unk_replace'", "]", "=", "args", ".", "unk_replace", "\n", "args_dict", "[", "'set_num_training_samples'", "]", "=", "args", ".", "set_num_training_samples", "\n", "args_dict", "[", "'punct_set'", "]", "=", "None", "\n", "if", "args", ".", "punct_set", "is", "not", "None", ":", "\n", "        ", "args_dict", "[", "'punct_set'", "]", "=", "set", "(", "args", ".", "punct_set", ")", "\n", "logger", ".", "info", "(", "\"punctuations(%d): %s\"", "%", "(", "len", "(", "args_dict", "[", "'punct_set'", "]", ")", ",", "' '", ".", "join", "(", "args_dict", "[", "'punct_set'", "]", ")", ")", ")", "\n", "", "args_dict", "[", "'freeze_word_embeddings'", "]", "=", "args", ".", "freeze_word_embeddings", "\n", "args_dict", "[", "'freeze_sequence_taggers'", "]", "=", "args", ".", "freeze_sequence_taggers", "\n", "args_dict", "[", "'word_embedding'", "]", "=", "args", ".", "word_embedding", "\n", "args_dict", "[", "'word_path'", "]", "=", "args", ".", "word_path", "\n", "args_dict", "[", "'use_char'", "]", "=", "args", ".", "use_char", "\n", "args_dict", "[", "'char_embedding'", "]", "=", "args", ".", "char_embedding", "\n", "args_dict", "[", "'char_path'", "]", "=", "args", ".", "char_path", "\n", "args_dict", "[", "'pos_embedding'", "]", "=", "args", ".", "pos_embedding", "\n", "args_dict", "[", "'pos_path'", "]", "=", "args", ".", "pos_path", "\n", "args_dict", "[", "'use_pos'", "]", "=", "args", ".", "use_pos", "\n", "args_dict", "[", "'pos_dim'", "]", "=", "args", ".", "pos_dim", "\n", "args_dict", "[", "'word_dict'", "]", "=", "None", "\n", "args_dict", "[", "'word_dim'", "]", "=", "args", ".", "word_dim", "\n", "if", "args_dict", "[", "'word_embedding'", "]", "!=", "'random'", "and", "args_dict", "[", "'word_path'", "]", ":", "\n", "        ", "args_dict", "[", "'word_dict'", "]", ",", "args_dict", "[", "'word_dim'", "]", "=", "load_word_embeddings", ".", "load_embedding_dict", "(", "args_dict", "[", "'word_embedding'", "]", ",", "\n", "args_dict", "[", "'word_path'", "]", ")", "\n", "", "args_dict", "[", "'char_dict'", "]", "=", "None", "\n", "args_dict", "[", "'char_dim'", "]", "=", "args", ".", "char_dim", "\n", "if", "args_dict", "[", "'char_embedding'", "]", "!=", "'random'", ":", "\n", "        ", "args_dict", "[", "'char_dict'", "]", ",", "args_dict", "[", "'char_dim'", "]", "=", "load_word_embeddings", ".", "load_embedding_dict", "(", "args_dict", "[", "'char_embedding'", "]", ",", "\n", "args_dict", "[", "'char_path'", "]", ")", "\n", "", "args_dict", "[", "'pos_dict'", "]", "=", "None", "\n", "if", "args_dict", "[", "'pos_embedding'", "]", "!=", "'random'", ":", "\n", "        ", "args_dict", "[", "'pos_dict'", "]", ",", "args_dict", "[", "'pos_dim'", "]", "=", "load_word_embeddings", ".", "load_embedding_dict", "(", "args_dict", "[", "'pos_embedding'", "]", ",", "\n", "args_dict", "[", "'pos_path'", "]", ")", "\n", "", "args_dict", "[", "'alphabet_path'", "]", "=", "path", ".", "join", "(", "args_dict", "[", "'model_path'", "]", ",", "'alphabets'", "+", "'_src_domain_'", "+", "args_dict", "[", "'domain'", "]", "+", "'/'", ")", "\n", "args_dict", "[", "'model_name'", "]", "=", "path", ".", "join", "(", "args_dict", "[", "'model_path'", "]", ",", "args_dict", "[", "'model_name'", "]", ")", "\n", "args_dict", "[", "'eval_mode'", "]", "=", "args", ".", "eval_mode", "\n", "args_dict", "[", "'device'", "]", "=", "torch", ".", "device", "(", "'cuda'", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "else", "'cpu'", ")", "\n", "args_dict", "[", "'word_status'", "]", "=", "'frozen'", "if", "args", ".", "freeze_word_embeddings", "else", "'fine tune'", "\n", "args_dict", "[", "'char_status'", "]", "=", "'enabled'", "if", "args", ".", "use_char", "else", "'disabled'", "\n", "args_dict", "[", "'pos_status'", "]", "=", "'enabled'", "if", "args", ".", "use_pos", "else", "'disabled'", "\n", "logger", ".", "info", "(", "\"Saving arguments to file\"", ")", "\n", "save_args", "(", "args", ",", "args_dict", "[", "'full_model_name'", "]", ")", "\n", "logger", ".", "info", "(", "\"Creating Alphabets\"", ")", "\n", "alphabet_dict", "=", "creating_alphabets", "(", "args_dict", "[", "'alphabet_path'", "]", ",", "args_dict", "[", "'alphabet_data_paths'", "]", ",", "args_dict", "[", "'word_dict'", "]", ")", "\n", "args_dict", "=", "{", "**", "args_dict", ",", "**", "alphabet_dict", "}", "\n", "ARGS", "=", "namedtuple", "(", "'ARGS'", ",", "args_dict", ".", "keys", "(", ")", ")", "\n", "my_args", "=", "ARGS", "(", "**", "args_dict", ")", "\n", "return", "my_args", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.examples.GraphParser_MRL.creating_alphabets": [[197, 211], ["utils.io_.prepare_data.create_alphabets", "alphabet_dict[].items", "v.size", "logger.info", "alphabet_data_paths.items", "k.split"], "function", ["home.repos.pwc.inspect_result.Jivnesh_LCM.io_.prepare_data.create_alphabets", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.items", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.size", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.items"], ["", "def", "creating_alphabets", "(", "alphabet_path", ",", "alphabet_data_paths", ",", "word_dict", ")", ":", "\n", "    ", "train_paths", "=", "alphabet_data_paths", "[", "'train'", "]", "\n", "extra_paths", "=", "[", "v", "for", "k", ",", "v", "in", "alphabet_data_paths", ".", "items", "(", ")", "if", "k", "!=", "'train'", "]", "\n", "alphabet_dict", "=", "{", "}", "\n", "alphabet_dict", "[", "'alphabets'", "]", "=", "prepare_data", ".", "create_alphabets", "(", "alphabet_path", ",", "\n", "train_paths", ",", "\n", "extra_paths", "=", "extra_paths", ",", "\n", "max_vocabulary_size", "=", "100000", ",", "\n", "embedd_dict", "=", "word_dict", ")", "\n", "for", "k", ",", "v", "in", "alphabet_dict", "[", "'alphabets'", "]", ".", "items", "(", ")", ":", "\n", "        ", "num_key", "=", "'num_'", "+", "k", ".", "split", "(", "'_'", ")", "[", "0", "]", "\n", "alphabet_dict", "[", "num_key", "]", "=", "v", ".", "size", "(", ")", "\n", "logger", ".", "info", "(", "\"%s : %d\"", "%", "(", "num_key", ",", "alphabet_dict", "[", "num_key", "]", ")", ")", "\n", "", "return", "alphabet_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.examples.GraphParser_MRL.construct_embedding_table": [[212, 235], ["numpy.sqrt", "numpy.empty", "numpy.random.uniform().astype", "alphabet.items", "print", "torch.from_numpy", "torch.from_numpy", "alphabet.size", "numpy.random.uniform", "numpy.random.uniform().astype", "numpy.random.uniform", "token.lower", "numpy.random.uniform().astype", "token.lower", "numpy.random.uniform"], "function", ["home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.items", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.size"], ["", "def", "construct_embedding_table", "(", "alphabet", ",", "tokens_dict", ",", "dim", ",", "token_type", "=", "'word'", ")", ":", "\n", "    ", "if", "tokens_dict", "is", "None", ":", "\n", "        ", "return", "None", "\n", "", "scale", "=", "np", ".", "sqrt", "(", "3.0", "/", "dim", ")", "\n", "table", "=", "np", ".", "empty", "(", "[", "alphabet", ".", "size", "(", ")", ",", "dim", "]", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "table", "[", "prepare_data", ".", "UNK_ID", ",", ":", "]", "=", "np", ".", "random", ".", "uniform", "(", "-", "scale", ",", "scale", ",", "[", "1", ",", "dim", "]", ")", ".", "astype", "(", "np", ".", "float32", ")", "\n", "oov_tokens", "=", "0", "\n", "for", "token", ",", "index", "in", "alphabet", ".", "items", "(", ")", ":", "\n", "        ", "if", "token", "in", "[", "'aTA'", "]", ":", "\n", "            ", "embedding", "=", "np", ".", "random", ".", "uniform", "(", "-", "scale", ",", "scale", ",", "[", "1", ",", "dim", "]", ")", ".", "astype", "(", "np", ".", "float32", ")", "\n", "oov_tokens", "+=", "1", "\n", "", "elif", "token", "in", "tokens_dict", ":", "\n", "            ", "embedding", "=", "tokens_dict", "[", "token", "]", "\n", "", "elif", "token", ".", "lower", "(", ")", "in", "tokens_dict", ":", "\n", "            ", "embedding", "=", "tokens_dict", "[", "token", ".", "lower", "(", ")", "]", "\n", "", "else", ":", "\n", "            ", "embedding", "=", "np", ".", "random", ".", "uniform", "(", "-", "scale", ",", "scale", ",", "[", "1", ",", "dim", "]", ")", ".", "astype", "(", "np", ".", "float32", ")", "\n", "oov_tokens", "+=", "1", "\n", "# print(token)", "\n", "", "table", "[", "index", ",", ":", "]", "=", "embedding", "\n", "", "print", "(", "'token type : %s, number of oov: %d'", "%", "(", "token_type", ",", "oov_tokens", ")", ")", "\n", "table", "=", "torch", ".", "from_numpy", "(", "table", ")", "\n", "return", "table", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.examples.GraphParser_MRL.save_args": [[236, 241], ["vars", "open", "json.dump"], "function", ["home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.open"], ["", "def", "save_args", "(", "args", ",", "full_model_name", ")", ":", "\n", "    ", "arg_path", "=", "full_model_name", "+", "'.arg.json'", "\n", "argparse_dict", "=", "vars", "(", "args", ")", "\n", "with", "open", "(", "arg_path", ",", "'w'", ")", "as", "f", ":", "\n", "        ", "json", ".", "dump", "(", "argparse_dict", ",", "f", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.examples.GraphParser_MRL.generate_optimizer": [[242, 250], ["filter", "torch.optim.Adam", "torch.optim.SGD", "ValueError"], "function", ["None"], ["", "", "def", "generate_optimizer", "(", "args", ",", "lr", ",", "params", ")", ":", "\n", "    ", "params", "=", "filter", "(", "lambda", "param", ":", "param", ".", "requires_grad", ",", "params", ")", "\n", "if", "args", ".", "opt", "==", "'adam'", ":", "\n", "        ", "return", "Adam", "(", "params", ",", "lr", "=", "lr", ",", "betas", "=", "args", ".", "betas", ",", "weight_decay", "=", "args", ".", "gamma", ",", "eps", "=", "args", ".", "epsilon", ")", "\n", "", "elif", "args", ".", "opt", "==", "'sgd'", ":", "\n", "        ", "return", "SGD", "(", "params", ",", "lr", "=", "lr", ",", "momentum", "=", "args", ".", "momentum", ",", "weight_decay", "=", "args", ".", "gamma", ",", "nesterov", "=", "True", ")", "\n", "", "else", ":", "\n", "        ", "raise", "ValueError", "(", "'Unknown optimization algorithm: %s'", "%", "args", ".", "opt", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.examples.GraphParser_MRL.save_checkpoint": [[252, 261], ["print", "torch.save", "torch.save", "model.state_dict", "optimizer.state_dict"], "function", ["home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.save", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.save"], ["", "", "def", "save_checkpoint", "(", "model", ",", "optimizer", ",", "opt", ",", "dev_eval_dict", ",", "test_eval_dict", ",", "full_model_name", ")", ":", "\n", "    ", "path_name", "=", "full_model_name", "+", "'.pt'", "\n", "print", "(", "'Saving model to: %s'", "%", "path_name", ")", "\n", "state", "=", "{", "'model_state_dict'", ":", "model", ".", "state_dict", "(", ")", ",", "\n", "'optimizer_state_dict'", ":", "optimizer", ".", "state_dict", "(", ")", ",", "\n", "'opt'", ":", "opt", ",", "\n", "'dev_eval_dict'", ":", "dev_eval_dict", ",", "\n", "'test_eval_dict'", ":", "test_eval_dict", "}", "\n", "torch", ".", "save", "(", "state", ",", "path_name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.examples.GraphParser_MRL.load_checkpoint": [[263, 281], ["print", "torch.load", "torch.load", "model.load_state_dict", "ValueError", "GraphParser_MRL.generate_optimizer", "optimizer.load_state_dict", "optimizer.state.values", "model.parameters", "state.items", "isinstance", "v.to"], "function", ["home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.load", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.load", "home.repos.pwc.inspect_result.Jivnesh_LCM.examples.GraphParser.generate_optimizer", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.values", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.items"], ["", "def", "load_checkpoint", "(", "args", ",", "model", ",", "optimizer", ",", "dev_eval_dict", ",", "test_eval_dict", ",", "start_epoch", ",", "load_path", ",", "strict", "=", "True", ")", ":", "\n", "    ", "print", "(", "'Loading saved model from: %s'", "%", "load_path", ")", "\n", "checkpoint", "=", "torch", ".", "load", "(", "load_path", ",", "map_location", "=", "args", ".", "device", ")", "\n", "if", "checkpoint", "[", "'opt'", "]", "!=", "args", ".", "opt", ":", "\n", "        ", "raise", "ValueError", "(", "'loaded optimizer type is: %s instead of: %s'", "%", "(", "checkpoint", "[", "'opt'", "]", ",", "args", ".", "opt", ")", ")", "\n", "", "model", ".", "load_state_dict", "(", "checkpoint", "[", "'model_state_dict'", "]", ",", "strict", "=", "strict", ")", "\n", "\n", "if", "strict", ":", "\n", "        ", "generate_optimizer", "(", "args", ",", "args", ".", "learning_rate", ",", "model", ".", "parameters", "(", ")", ")", "\n", "optimizer", ".", "load_state_dict", "(", "checkpoint", "[", "'optimizer_state_dict'", "]", ")", "\n", "for", "state", "in", "optimizer", ".", "state", ".", "values", "(", ")", ":", "\n", "            ", "for", "k", ",", "v", "in", "state", ".", "items", "(", ")", ":", "\n", "                ", "if", "isinstance", "(", "v", ",", "torch", ".", "Tensor", ")", ":", "\n", "                    ", "state", "[", "k", "]", "=", "v", ".", "to", "(", "args", ".", "device", ")", "\n", "", "", "", "dev_eval_dict", "=", "checkpoint", "[", "'dev_eval_dict'", "]", "\n", "test_eval_dict", "=", "checkpoint", "[", "'test_eval_dict'", "]", "\n", "start_epoch", "=", "dev_eval_dict", "[", "'in_domain'", "]", "[", "'epoch'", "]", "\n", "", "return", "model", ",", "optimizer", ",", "dev_eval_dict", ",", "test_eval_dict", ",", "start_epoch", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.examples.GraphParser_MRL.build_model_and_optimizer": [[283, 328], ["GraphParser_MRL.construct_embedding_table", "GraphParser_MRL.construct_embedding_table", "GraphParser_MRL.construct_embedding_table", "utils.models.parsing_gating.BiAffine_Parser_Gated", "print", "GraphParser_MRL.generate_optimizer", "utils.models.parsing_gating.BiAffine_Parser_Gated.to", "utils.models.parsing_gating.BiAffine_Parser_Gated.parameters", "GraphParser_MRL.initialize_eval_dict", "GraphParser_MRL.initialize_eval_dict", "GraphParser_MRL.load_checkpoint", "utils.models.parsing_gating.BiAffine_Parser_Gated.state_dict", "enumerate", "model.state_dict.update", "utils.models.parsing_gating.BiAffine_Parser_Gated.load_state_dict", "print", "utils.models.parsing_gating.BiAffine_Parser_Gated.named_parameters", "print", "torch.load", "torch.load", "checkpoint[].items", "k.replace", "str"], "function", ["home.repos.pwc.inspect_result.Jivnesh_LCM.examples.GraphParser.construct_embedding_table", "home.repos.pwc.inspect_result.Jivnesh_LCM.examples.GraphParser.construct_embedding_table", "home.repos.pwc.inspect_result.Jivnesh_LCM.examples.GraphParser.construct_embedding_table", "home.repos.pwc.inspect_result.Jivnesh_LCM.examples.GraphParser.generate_optimizer", "home.repos.pwc.inspect_result.Jivnesh_LCM.examples.GraphParser.initialize_eval_dict", "home.repos.pwc.inspect_result.Jivnesh_LCM.examples.GraphParser.initialize_eval_dict", "home.repos.pwc.inspect_result.Jivnesh_LCM.examples.GraphParser.load_checkpoint", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.load", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.load", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.items"], ["", "def", "build_model_and_optimizer", "(", "args", ")", ":", "\n", "    ", "word_table", "=", "construct_embedding_table", "(", "args", ".", "alphabets", "[", "'word_alphabet'", "]", ",", "args", ".", "word_dict", ",", "args", ".", "word_dim", ",", "token_type", "=", "'word'", ")", "\n", "char_table", "=", "construct_embedding_table", "(", "args", ".", "alphabets", "[", "'char_alphabet'", "]", ",", "args", ".", "char_dict", ",", "args", ".", "char_dim", ",", "token_type", "=", "'char'", ")", "\n", "pos_table", "=", "construct_embedding_table", "(", "args", ".", "alphabets", "[", "'pos_alphabet'", "]", ",", "args", ".", "pos_dict", ",", "args", ".", "pos_dim", ",", "token_type", "=", "'pos'", ")", "\n", "model", "=", "BiAffine_Parser_Gated", "(", "args", ".", "word_dim", ",", "args", ".", "num_word", ",", "args", ".", "char_dim", ",", "args", ".", "num_char", ",", "\n", "args", ".", "use_pos", ",", "args", ".", "use_char", ",", "args", ".", "pos_dim", ",", "args", ".", "num_pos", ",", "\n", "args", ".", "num_filters", ",", "args", ".", "kernel_size", ",", "args", ".", "rnn_mode", ",", "\n", "args", ".", "hidden_size", ",", "args", ".", "num_layers", ",", "args", ".", "num_arc", ",", "\n", "args", ".", "arc_space", ",", "args", ".", "arc_tag_space", ",", "args", ".", "num_gates", ",", "\n", "embedd_word", "=", "word_table", ",", "embedd_char", "=", "char_table", ",", "embedd_pos", "=", "pos_table", ",", "\n", "p_in", "=", "args", ".", "p_in", ",", "p_out", "=", "args", ".", "p_out", ",", "p_rnn", "=", "args", ".", "p_rnn", ",", "\n", "biaffine", "=", "True", ",", "arc_decode", "=", "args", ".", "arc_decode", ",", "initializer", "=", "args", ".", "initializer", ")", "\n", "print", "(", "model", ")", "\n", "optimizer", "=", "generate_optimizer", "(", "args", ",", "args", ".", "learning_rate", ",", "model", ".", "parameters", "(", ")", ")", "\n", "start_epoch", "=", "0", "\n", "dev_eval_dict", "=", "{", "'in_domain'", ":", "initialize_eval_dict", "(", ")", "}", "\n", "test_eval_dict", "=", "{", "'in_domain'", ":", "initialize_eval_dict", "(", ")", "}", "\n", "if", "args", ".", "load_path", ":", "\n", "        ", "model", ",", "optimizer", ",", "dev_eval_dict", ",", "test_eval_dict", ",", "start_epoch", "=", "load_checkpoint", "(", "args", ",", "model", ",", "optimizer", ",", "\n", "dev_eval_dict", ",", "test_eval_dict", ",", "\n", "start_epoch", ",", "args", ".", "load_path", ",", "strict", "=", "args", ".", "strict", ")", "\n", "", "if", "args", ".", "load_sequence_taggers_paths", ":", "\n", "        ", "pretrained_dict", "=", "{", "}", "\n", "model_dict", "=", "model", ".", "state_dict", "(", ")", "\n", "for", "idx", ",", "path", "in", "enumerate", "(", "args", ".", "load_sequence_taggers_paths", ")", ":", "\n", "            ", "print", "(", "'Loading saved sequence_tagger from: %s'", "%", "path", ")", "\n", "checkpoint", "=", "torch", ".", "load", "(", "path", ",", "map_location", "=", "args", ".", "device", ")", "\n", "for", "k", ",", "v", "in", "checkpoint", "[", "'model_state_dict'", "]", ".", "items", "(", ")", ":", "\n", "                ", "if", "'rnn_encoder.'", "in", "k", ":", "\n", "                    ", "pretrained_dict", "[", "'extra_rnn_encoders.'", "+", "str", "(", "idx", ")", "+", "'.'", "+", "k", ".", "replace", "(", "'rnn_encoder.'", ",", "''", ")", "]", "=", "v", "\n", "", "", "", "model_dict", ".", "update", "(", "pretrained_dict", ")", "\n", "model", ".", "load_state_dict", "(", "model_dict", ")", "\n", "", "if", "args", ".", "freeze_sequence_taggers", ":", "\n", "        ", "print", "(", "'Freezing Classifiers'", ")", "\n", "for", "name", ",", "parameter", "in", "model", ".", "named_parameters", "(", ")", ":", "\n", "            ", "if", "'extra_rnn_encoders'", "in", "name", ":", "\n", "                ", "parameter", ".", "requires_grad", "=", "False", "\n", "", "", "", "if", "args", ".", "freeze_word_embeddings", ":", "\n", "        ", "model", ".", "rnn_encoder", ".", "word_embedd", ".", "weight", ".", "requires_grad", "=", "False", "\n", "# model.rnn_encoder.char_embedd.weight.requires_grad = False", "\n", "# model.rnn_encoder.pos_embedd.weight.requires_grad = False", "\n", "", "device", "=", "args", ".", "device", "\n", "model", ".", "to", "(", "device", ")", "\n", "return", "model", ",", "optimizer", ",", "dev_eval_dict", ",", "test_eval_dict", ",", "start_epoch", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.examples.GraphParser_MRL.initialize_eval_dict": [[330, 353], ["None"], "function", ["None"], ["", "def", "initialize_eval_dict", "(", ")", ":", "\n", "    ", "eval_dict", "=", "{", "}", "\n", "eval_dict", "[", "'dp_uas'", "]", "=", "0.0", "\n", "eval_dict", "[", "'dp_las'", "]", "=", "0.0", "\n", "eval_dict", "[", "'epoch'", "]", "=", "0", "\n", "eval_dict", "[", "'dp_ucorrect'", "]", "=", "0.0", "\n", "eval_dict", "[", "'dp_lcorrect'", "]", "=", "0.0", "\n", "eval_dict", "[", "'dp_total'", "]", "=", "0.0", "\n", "eval_dict", "[", "'dp_ucomplete_match'", "]", "=", "0.0", "\n", "eval_dict", "[", "'dp_lcomplete_match'", "]", "=", "0.0", "\n", "eval_dict", "[", "'dp_ucorrect_nopunc'", "]", "=", "0.0", "\n", "eval_dict", "[", "'dp_lcorrect_nopunc'", "]", "=", "0.0", "\n", "eval_dict", "[", "'dp_total_nopunc'", "]", "=", "0.0", "\n", "eval_dict", "[", "'dp_ucomplete_match_nopunc'", "]", "=", "0.0", "\n", "eval_dict", "[", "'dp_lcomplete_match_nopunc'", "]", "=", "0.0", "\n", "eval_dict", "[", "'dp_root_correct'", "]", "=", "0.0", "\n", "eval_dict", "[", "'dp_total_root'", "]", "=", "0.0", "\n", "eval_dict", "[", "'dp_total_inst'", "]", "=", "0.0", "\n", "eval_dict", "[", "'dp_total'", "]", "=", "0.0", "\n", "eval_dict", "[", "'dp_total_inst'", "]", "=", "0.0", "\n", "eval_dict", "[", "'dp_total_nopunc'", "]", "=", "0.0", "\n", "eval_dict", "[", "'dp_total_root'", "]", "=", "0.0", "\n", "return", "eval_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.examples.GraphParser_MRL.in_domain_evaluation": [[354, 392], ["GraphParser_MRL.evaluation", "print", "evaluation.items", "GraphParser_MRL.evaluation", "evaluation.items", "copy.deepcopy", "copy.deepcopy", "print", "GraphParser_MRL.save_checkpoint", "datasets.keys", "GraphParser_MRL.write_results"], "function", ["home.repos.pwc.inspect_result.Jivnesh_LCM.examples.GraphParser.evaluation", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.items", "home.repos.pwc.inspect_result.Jivnesh_LCM.examples.GraphParser.evaluation", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.items", "home.repos.pwc.inspect_result.Jivnesh_LCM.examples.GraphParser.save_checkpoint", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.keys", "home.repos.pwc.inspect_result.Jivnesh_LCM.examples.GraphParser.write_results"], ["", "def", "in_domain_evaluation", "(", "args", ",", "datasets", ",", "model", ",", "optimizer", ",", "dev_eval_dict", ",", "test_eval_dict", ",", "epoch", ",", "\n", "best_model", ",", "best_optimizer", ",", "patient", ")", ":", "\n", "# In-domain evaluation", "\n", "    ", "curr_dev_eval_dict", "=", "evaluation", "(", "args", ",", "datasets", "[", "'dev'", "]", ",", "'dev'", ",", "model", ",", "args", ".", "domain", ",", "epoch", ",", "'current_results'", ")", "\n", "is_best_in_domain", "=", "dev_eval_dict", "[", "'in_domain'", "]", "[", "'dp_lcorrect_nopunc'", "]", "<=", "curr_dev_eval_dict", "[", "'dp_lcorrect_nopunc'", "]", "or", "(", "dev_eval_dict", "[", "'in_domain'", "]", "[", "'dp_lcorrect_nopunc'", "]", "==", "curr_dev_eval_dict", "[", "'dp_lcorrect_nopunc'", "]", "and", "\n", "dev_eval_dict", "[", "'in_domain'", "]", "[", "'dp_ucorrect_nopunc'", "]", "<=", "curr_dev_eval_dict", "[", "'dp_ucorrect_nopunc'", "]", ")", "\n", "\n", "if", "is_best_in_domain", ":", "\n", "        ", "for", "key", ",", "value", "in", "curr_dev_eval_dict", ".", "items", "(", ")", ":", "\n", "            ", "dev_eval_dict", "[", "'in_domain'", "]", "[", "key", "]", "=", "value", "\n", "", "curr_test_eval_dict", "=", "evaluation", "(", "args", ",", "datasets", "[", "'test'", "]", ",", "'test'", ",", "model", ",", "args", ".", "domain", ",", "epoch", ",", "'current_results'", ")", "\n", "for", "key", ",", "value", "in", "curr_test_eval_dict", ".", "items", "(", ")", ":", "\n", "            ", "test_eval_dict", "[", "'in_domain'", "]", "[", "key", "]", "=", "value", "\n", "", "best_model", "=", "deepcopy", "(", "model", ")", "\n", "best_optimizer", "=", "deepcopy", "(", "optimizer", ")", "\n", "patient", "=", "0", "\n", "", "else", ":", "\n", "        ", "patient", "+=", "1", "\n", "", "if", "epoch", "==", "args", ".", "num_epochs", ":", "\n", "# save in-domain checkpoint", "\n", "        ", "if", "args", ".", "set_num_training_samples", "is", "not", "None", ":", "\n", "            ", "splits_to_write", "=", "datasets", ".", "keys", "(", ")", "\n", "", "else", ":", "\n", "            ", "splits_to_write", "=", "[", "'dev'", ",", "'test'", "]", "\n", "", "for", "split", "in", "splits_to_write", ":", "\n", "            ", "if", "split", "==", "'dev'", ":", "\n", "                ", "eval_dict", "=", "dev_eval_dict", "[", "'in_domain'", "]", "\n", "", "elif", "split", "==", "'test'", ":", "\n", "                ", "eval_dict", "=", "test_eval_dict", "[", "'in_domain'", "]", "\n", "", "else", ":", "\n", "                ", "eval_dict", "=", "None", "\n", "", "write_results", "(", "args", ",", "datasets", "[", "split", "]", ",", "args", ".", "domain", ",", "split", ",", "best_model", ",", "args", ".", "domain", ",", "eval_dict", ")", "\n", "", "print", "(", "\"Saving best model\"", ")", "\n", "save_checkpoint", "(", "best_model", ",", "best_optimizer", ",", "args", ".", "opt", ",", "dev_eval_dict", ",", "test_eval_dict", ",", "args", ".", "full_model_name", ")", "\n", "\n", "", "print", "(", "'\\n'", ")", "\n", "return", "dev_eval_dict", ",", "test_eval_dict", ",", "best_model", ",", "best_optimizer", ",", "patient", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.examples.GraphParser_MRL.evaluation": [[394, 437], ["model.eval", "GraphParser_MRL.initialize_eval_dict", "utils.io_.prepare_data.iterate_batch", "GraphParser_MRL.print_results", "model.forward", "model.decode", "lengths.cpu().numpy.cpu().numpy", "word.data.cpu().numpy.data.cpu().numpy", "pos.data.cpu().numpy.data.cpu().numpy", "ner.data.cpu().numpy.data.cpu().numpy", "heads.data.cpu().numpy.data.cpu().numpy", "arc_tags.data.cpu().numpy.data.cpu().numpy", "heads_pred.data.cpu().numpy.data.cpu().numpy", "arc_tags_pred.data.cpu().numpy.data.cpu().numpy", "utils.tasks.parse.eval_", "lengths.cpu().numpy.cpu", "word.data.cpu().numpy.data.cpu", "pos.data.cpu().numpy.data.cpu", "ner.data.cpu().numpy.data.cpu", "heads.data.cpu().numpy.data.cpu", "arc_tags.data.cpu().numpy.data.cpu", "heads_pred.data.cpu().numpy.data.cpu", "arc_tags_pred.data.cpu().numpy.data.cpu"], "function", ["home.repos.pwc.inspect_result.Jivnesh_LCM.examples.GraphParser.initialize_eval_dict", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.prepare_data.iterate_batch", "home.repos.pwc.inspect_result.Jivnesh_LCM.examples.GraphParser.print_results", "home.repos.pwc.inspect_result.Jivnesh_LCM.models.parsing_gating.Gating.forward", "home.repos.pwc.inspect_result.Jivnesh_LCM.models.parsing_gating.BiAffine_Parser_Decoder.decode", "home.repos.pwc.inspect_result.Jivnesh_LCM.tasks.parse.eval_"], ["", "def", "evaluation", "(", "args", ",", "data", ",", "split", ",", "model", ",", "domain", ",", "epoch", ",", "str_res", "=", "'results'", ")", ":", "\n", "# evaluate performance on data", "\n", "    ", "model", ".", "eval", "(", ")", "\n", "\n", "eval_dict", "=", "initialize_eval_dict", "(", ")", "\n", "eval_dict", "[", "'epoch'", "]", "=", "epoch", "\n", "for", "batch", "in", "prepare_data", ".", "iterate_batch", "(", "data", ",", "args", ".", "batch_size", ",", "args", ".", "device", ")", ":", "\n", "        ", "word", ",", "char", ",", "pos", ",", "ner", ",", "heads", ",", "arc_tags", ",", "auto_label", ",", "masks", ",", "lengths", "=", "batch", "\n", "out_arc", ",", "out_arc_tag", ",", "masks", ",", "lengths", "=", "model", ".", "forward", "(", "word", ",", "char", ",", "pos", ",", "mask", "=", "masks", ",", "length", "=", "lengths", ")", "\n", "heads_pred", ",", "arc_tags_pred", ",", "_", "=", "model", ".", "decode", "(", "args", ".", "model_path", ",", "word", ",", "pos", ",", "ner", ",", "out_arc", ",", "out_arc_tag", ",", "mask", "=", "masks", ",", "length", "=", "lengths", ",", "\n", "leading_symbolic", "=", "prepare_data", ".", "NUM_SYMBOLIC_TAGS", ")", "\n", "lengths", "=", "lengths", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "word", "=", "word", ".", "data", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "pos", "=", "pos", ".", "data", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "ner", "=", "ner", ".", "data", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "heads", "=", "heads", ".", "data", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "arc_tags", "=", "arc_tags", ".", "data", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "heads_pred", "=", "heads_pred", ".", "data", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "arc_tags_pred", "=", "arc_tags_pred", ".", "data", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "stats", ",", "stats_nopunc", ",", "stats_root", ",", "num_inst", "=", "parse", ".", "eval_", "(", "word", ",", "pos", ",", "heads_pred", ",", "arc_tags_pred", ",", "heads", ",", "\n", "arc_tags", ",", "args", ".", "alphabets", "[", "'word_alphabet'", "]", ",", "args", ".", "alphabets", "[", "'pos_alphabet'", "]", ",", "\n", "lengths", ",", "punct_set", "=", "args", ".", "punct_set", ",", "symbolic_root", "=", "True", ")", "\n", "ucorr", ",", "lcorr", ",", "total", ",", "ucm", ",", "lcm", "=", "stats", "\n", "ucorr_nopunc", ",", "lcorr_nopunc", ",", "total_nopunc", ",", "ucm_nopunc", ",", "lcm_nopunc", "=", "stats_nopunc", "\n", "corr_root", ",", "total_root", "=", "stats_root", "\n", "eval_dict", "[", "'dp_ucorrect'", "]", "+=", "ucorr", "\n", "eval_dict", "[", "'dp_lcorrect'", "]", "+=", "lcorr", "\n", "eval_dict", "[", "'dp_total'", "]", "+=", "total", "\n", "eval_dict", "[", "'dp_ucomplete_match'", "]", "+=", "ucm", "\n", "eval_dict", "[", "'dp_lcomplete_match'", "]", "+=", "lcm", "\n", "eval_dict", "[", "'dp_ucorrect_nopunc'", "]", "+=", "ucorr_nopunc", "\n", "eval_dict", "[", "'dp_lcorrect_nopunc'", "]", "+=", "lcorr_nopunc", "\n", "eval_dict", "[", "'dp_total_nopunc'", "]", "+=", "total_nopunc", "\n", "eval_dict", "[", "'dp_ucomplete_match_nopunc'", "]", "+=", "ucm_nopunc", "\n", "eval_dict", "[", "'dp_lcomplete_match_nopunc'", "]", "+=", "lcm_nopunc", "\n", "eval_dict", "[", "'dp_root_correct'", "]", "+=", "corr_root", "\n", "eval_dict", "[", "'dp_total_root'", "]", "+=", "total_root", "\n", "eval_dict", "[", "'dp_total_inst'", "]", "+=", "num_inst", "\n", "\n", "", "eval_dict", "[", "'dp_uas'", "]", "=", "eval_dict", "[", "'dp_ucorrect'", "]", "*", "100", "/", "eval_dict", "[", "'dp_total'", "]", "# considering w. punctuation", "\n", "eval_dict", "[", "'dp_las'", "]", "=", "eval_dict", "[", "'dp_lcorrect'", "]", "*", "100", "/", "eval_dict", "[", "'dp_total'", "]", "# considering w. punctuation", "\n", "print_results", "(", "eval_dict", ",", "split", ",", "domain", ",", "str_res", ")", "\n", "return", "eval_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.examples.GraphParser_MRL.print_results": [[439, 463], ["print", "print", "print", "print", "print", "print", "print"], "function", ["None"], ["", "def", "print_results", "(", "eval_dict", ",", "split", ",", "domain", ",", "str_res", "=", "'results'", ")", ":", "\n", "    ", "print", "(", "'----------------------------------------------------------------------------------------------------------------------------'", ")", "\n", "print", "(", "'Testing model on domain %s'", "%", "domain", ")", "\n", "print", "(", "'--------------- Dependency Parsing - %s ---------------'", "%", "split", ")", "\n", "print", "(", "\n", "str_res", "+", "' on '", "+", "split", "+", "'  W. Punct: ucorr: %d, lcorr: %d, total: %d, uas: %.2f%%, las: %.2f%%, ucm: %.2f%%, lcm: %.2f%% (epoch: %d)'", "%", "(", "\n", "eval_dict", "[", "'dp_ucorrect'", "]", ",", "eval_dict", "[", "'dp_lcorrect'", "]", ",", "eval_dict", "[", "'dp_total'", "]", ",", "\n", "eval_dict", "[", "'dp_ucorrect'", "]", "*", "100", "/", "eval_dict", "[", "'dp_total'", "]", ",", "\n", "eval_dict", "[", "'dp_lcorrect'", "]", "*", "100", "/", "eval_dict", "[", "'dp_total'", "]", ",", "\n", "eval_dict", "[", "'dp_ucomplete_match'", "]", "*", "100", "/", "eval_dict", "[", "'dp_total_inst'", "]", ",", "\n", "eval_dict", "[", "'dp_lcomplete_match'", "]", "*", "100", "/", "eval_dict", "[", "'dp_total_inst'", "]", ",", "\n", "eval_dict", "[", "'epoch'", "]", ")", ")", "\n", "print", "(", "\n", "str_res", "+", "' on '", "+", "split", "+", "'  Wo Punct: ucorr: %d, lcorr: %d, total: %d, uas: %.2f%%, las: %.2f%%, ucm: %.2f%%, lcm: %.2f%% (epoch: %d)'", "%", "(", "\n", "eval_dict", "[", "'dp_ucorrect_nopunc'", "]", ",", "eval_dict", "[", "'dp_lcorrect_nopunc'", "]", ",", "eval_dict", "[", "'dp_total_nopunc'", "]", ",", "\n", "eval_dict", "[", "'dp_ucorrect_nopunc'", "]", "*", "100", "/", "eval_dict", "[", "'dp_total_nopunc'", "]", ",", "\n", "eval_dict", "[", "'dp_lcorrect_nopunc'", "]", "*", "100", "/", "eval_dict", "[", "'dp_total_nopunc'", "]", ",", "\n", "eval_dict", "[", "'dp_ucomplete_match_nopunc'", "]", "*", "100", "/", "eval_dict", "[", "'dp_total_inst'", "]", ",", "\n", "eval_dict", "[", "'dp_lcomplete_match_nopunc'", "]", "*", "100", "/", "eval_dict", "[", "'dp_total_inst'", "]", ",", "\n", "eval_dict", "[", "'epoch'", "]", ")", ")", "\n", "print", "(", "str_res", "+", "' on '", "+", "split", "+", "'  Root: corr: %d, total: %d, acc: %.2f%% (epoch: %d)'", "%", "(", "\n", "eval_dict", "[", "'dp_root_correct'", "]", ",", "eval_dict", "[", "'dp_total_root'", "]", ",", "\n", "eval_dict", "[", "'dp_root_correct'", "]", "*", "100", "/", "eval_dict", "[", "'dp_total_root'", "]", ",", "eval_dict", "[", "'epoch'", "]", ")", ")", "\n", "print", "(", "'\\n'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.examples.GraphParser_MRL.write_results": [[464, 499], ["utils.io_.Writer", "utils.io_.Writer", "utils.io_.Writer.start", "utils.io_.Writer.start", "utils.io_.prepare_data.iterate_batch", "utils.io_.Writer.close", "utils.io_.Writer.close", "model.forward", "model.decode", "lengths.cpu().numpy.cpu().numpy", "word.data.cpu().numpy.data.cpu().numpy", "pos.data.cpu().numpy.data.cpu().numpy", "ner.data.cpu().numpy.data.cpu().numpy", "heads.data.cpu().numpy.data.cpu().numpy", "arc_tags.data.cpu().numpy.data.cpu().numpy", "heads_pred.data.cpu().numpy.data.cpu().numpy", "arc_tags_pred.data.cpu().numpy.data.cpu().numpy", "utils.io_.Writer.write", "utils.io_.Writer.write", "open", "json.dump", "lengths.cpu().numpy.cpu", "word.data.cpu().numpy.data.cpu", "pos.data.cpu().numpy.data.cpu", "ner.data.cpu().numpy.data.cpu", "heads.data.cpu().numpy.data.cpu", "arc_tags.data.cpu().numpy.data.cpu", "heads_pred.data.cpu().numpy.data.cpu", "arc_tags_pred.data.cpu().numpy.data.cpu"], "function", ["home.repos.pwc.inspect_result.Jivnesh_LCM.io_.writer.Writer.start", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.writer.Writer.start", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.prepare_data.iterate_batch", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.reader.Reader.close", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.reader.Reader.close", "home.repos.pwc.inspect_result.Jivnesh_LCM.models.parsing_gating.Gating.forward", "home.repos.pwc.inspect_result.Jivnesh_LCM.models.parsing_gating.BiAffine_Parser_Decoder.decode", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.writer.Writer.write", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.writer.Writer.write", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.open"], ["", "def", "write_results", "(", "args", ",", "data", ",", "data_domain", ",", "split", ",", "model", ",", "model_domain", ",", "eval_dict", ")", ":", "\n", "    ", "str_file", "=", "args", ".", "full_model_name", "+", "'_'", "+", "split", "+", "'_model_domain_'", "+", "model_domain", "+", "'_data_domain_'", "+", "data_domain", "\n", "res_filename", "=", "str_file", "+", "'_res.txt'", "\n", "pred_filename", "=", "str_file", "+", "'_pred.txt'", "\n", "gold_filename", "=", "str_file", "+", "'_gold.txt'", "\n", "if", "eval_dict", "is", "not", "None", ":", "\n", "# save results dictionary into a file", "\n", "        ", "with", "open", "(", "res_filename", ",", "'w'", ")", "as", "f", ":", "\n", "            ", "json", ".", "dump", "(", "eval_dict", ",", "f", ")", "\n", "\n", "# save predictions and gold labels into files", "\n", "", "", "pred_writer", "=", "Writer", "(", "args", ".", "alphabets", ")", "\n", "gold_writer", "=", "Writer", "(", "args", ".", "alphabets", ")", "\n", "pred_writer", ".", "start", "(", "pred_filename", ")", "\n", "gold_writer", ".", "start", "(", "gold_filename", ")", "\n", "for", "batch", "in", "prepare_data", ".", "iterate_batch", "(", "data", ",", "args", ".", "batch_size", ",", "args", ".", "device", ")", ":", "\n", "        ", "word", ",", "char", ",", "pos", ",", "ner", ",", "heads", ",", "arc_tags", ",", "auto_label", ",", "masks", ",", "lengths", "=", "batch", "\n", "out_arc", ",", "out_arc_tag", ",", "masks", ",", "lengths", "=", "model", ".", "forward", "(", "word", ",", "char", ",", "pos", ",", "mask", "=", "masks", ",", "length", "=", "lengths", ")", "\n", "heads_pred", ",", "arc_tags_pred", ",", "_", "=", "model", ".", "decode", "(", "args", ".", "model_path", ",", "word", ",", "pos", ",", "ner", ",", "out_arc", ",", "out_arc_tag", ",", "mask", "=", "masks", ",", "length", "=", "lengths", ",", "\n", "leading_symbolic", "=", "prepare_data", ".", "NUM_SYMBOLIC_TAGS", ")", "\n", "lengths", "=", "lengths", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "word", "=", "word", ".", "data", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "pos", "=", "pos", ".", "data", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "ner", "=", "ner", ".", "data", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "heads", "=", "heads", ".", "data", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "arc_tags", "=", "arc_tags", ".", "data", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "heads_pred", "=", "heads_pred", ".", "data", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "arc_tags_pred", "=", "arc_tags_pred", ".", "data", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "# writing predictions", "\n", "pred_writer", ".", "write", "(", "word", ",", "pos", ",", "ner", ",", "heads_pred", ",", "arc_tags_pred", ",", "lengths", ",", "symbolic_root", "=", "True", ")", "\n", "# writing gold labels", "\n", "gold_writer", ".", "write", "(", "word", ",", "pos", ",", "ner", ",", "heads", ",", "arc_tags", ",", "lengths", ",", "symbolic_root", "=", "True", ")", "\n", "\n", "", "pred_writer", ".", "close", "(", ")", "\n", "gold_writer", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.examples.GraphParser_MRL.main": [[500, 600], ["logger.info", "GraphParser_MRL.read_arguments", "logger.info", "logger.info", "sum", "GraphParser_MRL.build_model_and_optimizer", "copy.deepcopy", "copy.deepcopy", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "print", "print", "utils.io_.prepare_data.read_data_to_variable", "print", "utils.io_.rearrange_splits.rearranging_splits", "logger.info", "utils.io_.prepare_data.calc_num_batches", "range", "rearrange_splits.rearranging_splits.keys", "logger.info", "print", "model.train", "utils.io_.prepare_data.iterate_batch_rand_bucket_choosing", "time.time", "enumerate", "print", "print", "GraphParser_MRL.in_domain_evaluation", "GraphParser_MRL.print_results", "print", "GraphParser_MRL.evaluation", "GraphParser_MRL.write_results", "GraphParser_MRL.evaluation", "GraphParser_MRL.write_results", "generate_optimizer.zero_grad", "model.forward", "model.loss", "loss.backward", "torch.nn.utils.clip_grad_norm_", "generate_optimizer.step", "GraphParser_MRL.generate_optimizer", "print", "masks.data.sum", "word.size", "loss_arc.item", "loss_arc_tag.item", "loss.item", "model.parameters", "sys.stdout.write", "sys.stdout.write", "sys.stdout.flush", "model.parameters", "time.time", "time.time"], "function", ["home.repos.pwc.inspect_result.Jivnesh_LCM.examples.GraphParser.read_arguments", "home.repos.pwc.inspect_result.Jivnesh_LCM.examples.GraphParser.build_model_and_optimizer", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.prepare_data.read_data_to_variable", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.rearrange_splits.rearranging_splits", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.prepare_data.calc_num_batches", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.keys", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.prepare_data.iterate_batch_rand_bucket_choosing", "home.repos.pwc.inspect_result.Jivnesh_LCM.examples.GraphParser.in_domain_evaluation", "home.repos.pwc.inspect_result.Jivnesh_LCM.examples.GraphParser.print_results", "home.repos.pwc.inspect_result.Jivnesh_LCM.examples.GraphParser.evaluation", "home.repos.pwc.inspect_result.Jivnesh_LCM.examples.GraphParser.write_results", "home.repos.pwc.inspect_result.Jivnesh_LCM.examples.GraphParser.evaluation", "home.repos.pwc.inspect_result.Jivnesh_LCM.examples.GraphParser.write_results", "home.repos.pwc.inspect_result.Jivnesh_LCM.models.parsing_gating.Gating.forward", "home.repos.pwc.inspect_result.Jivnesh_LCM.models.parsing_gating.BiAffine_Parser_Decoder.loss", "home.repos.pwc.inspect_result.Jivnesh_LCM.examples.GraphParser.generate_optimizer", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.size", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.writer.Writer.write", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.writer.Writer.write"], ["", "def", "main", "(", ")", ":", "\n", "    ", "logger", ".", "info", "(", "\"Reading and creating arguments\"", ")", "\n", "args", "=", "read_arguments", "(", ")", "\n", "logger", ".", "info", "(", "\"Reading Data\"", ")", "\n", "datasets", "=", "{", "}", "\n", "for", "split", "in", "args", ".", "splits", ":", "\n", "        ", "print", "(", "\"Splits are:\"", ",", "split", ")", "\n", "dataset", "=", "prepare_data", ".", "read_data_to_variable", "(", "args", ".", "data_paths", "[", "split", "]", ",", "args", ".", "alphabets", ",", "args", ".", "device", ",", "\n", "symbolic_root", "=", "True", ")", "\n", "datasets", "[", "split", "]", "=", "dataset", "\n", "", "if", "args", ".", "set_num_training_samples", "is", "not", "None", ":", "\n", "        ", "print", "(", "'Setting train and dev to %d samples'", "%", "args", ".", "set_num_training_samples", ")", "\n", "datasets", "=", "rearrange_splits", ".", "rearranging_splits", "(", "datasets", ",", "args", ".", "set_num_training_samples", ")", "\n", "", "logger", ".", "info", "(", "\"Creating Networks\"", ")", "\n", "num_data", "=", "sum", "(", "datasets", "[", "'train'", "]", "[", "1", "]", ")", "\n", "model", ",", "optimizer", ",", "dev_eval_dict", ",", "test_eval_dict", ",", "start_epoch", "=", "build_model_and_optimizer", "(", "args", ")", "\n", "best_model", "=", "deepcopy", "(", "model", ")", "\n", "best_optimizer", "=", "deepcopy", "(", "optimizer", ")", "\n", "\n", "logger", ".", "info", "(", "'Training INFO of in domain %s'", "%", "args", ".", "domain", ")", "\n", "logger", ".", "info", "(", "'Training on Dependecy Parsing'", ")", "\n", "logger", ".", "info", "(", "\"train: gamma: %f, batch: %d, clip: %.2f, unk replace: %.2f\"", "%", "(", "args", ".", "gamma", ",", "args", ".", "batch_size", ",", "args", ".", "clip", ",", "args", ".", "unk_replace", ")", ")", "\n", "logger", ".", "info", "(", "'number of training samples for %s is: %d'", "%", "(", "args", ".", "domain", ",", "num_data", ")", ")", "\n", "logger", ".", "info", "(", "\"dropout(in, out, rnn): (%.2f, %.2f, %s)\"", "%", "(", "args", ".", "p_in", ",", "args", ".", "p_out", ",", "args", ".", "p_rnn", ")", ")", "\n", "logger", ".", "info", "(", "\"num_epochs: %d\"", "%", "(", "args", ".", "num_epochs", ")", ")", "\n", "print", "(", "'\\n'", ")", "\n", "\n", "if", "not", "args", ".", "eval_mode", ":", "\n", "        ", "logger", ".", "info", "(", "\"Training\"", ")", "\n", "num_batches", "=", "prepare_data", ".", "calc_num_batches", "(", "datasets", "[", "'train'", "]", ",", "args", ".", "batch_size", ")", "\n", "lr", "=", "args", ".", "learning_rate", "\n", "patient", "=", "0", "\n", "decay", "=", "0", "\n", "for", "epoch", "in", "range", "(", "start_epoch", "+", "1", ",", "args", ".", "num_epochs", "+", "1", ")", ":", "\n", "            ", "print", "(", "'Epoch %d (Training: rnn mode: %s, optimizer: %s, learning rate=%.6f, eps=%.1e, decay rate=%.2f (schedule=%d, decay=%d)): '", "%", "(", "\n", "epoch", ",", "args", ".", "rnn_mode", ",", "args", ".", "opt", ",", "lr", ",", "args", ".", "epsilon", ",", "args", ".", "decay_rate", ",", "args", ".", "schedule", ",", "decay", ")", ")", "\n", "model", ".", "train", "(", ")", "\n", "total_loss", "=", "0.0", "\n", "total_arc_loss", "=", "0.0", "\n", "total_arc_tag_loss", "=", "0.0", "\n", "total_train_inst", "=", "0.0", "\n", "\n", "train_iter", "=", "prepare_data", ".", "iterate_batch_rand_bucket_choosing", "(", "\n", "datasets", "[", "'train'", "]", ",", "args", ".", "batch_size", ",", "args", ".", "device", ",", "unk_replace", "=", "args", ".", "unk_replace", ")", "\n", "start_time", "=", "time", ".", "time", "(", ")", "\n", "batch_num", "=", "0", "\n", "for", "batch_num", ",", "batch", "in", "enumerate", "(", "train_iter", ")", ":", "\n", "                ", "batch_num", "=", "batch_num", "+", "1", "\n", "optimizer", ".", "zero_grad", "(", ")", "\n", "# compute loss of main task", "\n", "word", ",", "char", ",", "pos", ",", "ner_tags", ",", "heads", ",", "arc_tags", ",", "auto_label", ",", "masks", ",", "lengths", "=", "batch", "\n", "out_arc", ",", "out_arc_tag", ",", "masks", ",", "lengths", "=", "model", ".", "forward", "(", "word", ",", "char", ",", "pos", ",", "mask", "=", "masks", ",", "length", "=", "lengths", ")", "\n", "loss_arc", ",", "loss_arc_tag", "=", "model", ".", "loss", "(", "out_arc", ",", "out_arc_tag", ",", "heads", ",", "arc_tags", ",", "mask", "=", "masks", ",", "length", "=", "lengths", ")", "\n", "loss", "=", "loss_arc", "+", "loss_arc_tag", "\n", "\n", "# update losses", "\n", "num_insts", "=", "masks", ".", "data", ".", "sum", "(", ")", "-", "word", ".", "size", "(", "0", ")", "\n", "total_arc_loss", "+=", "loss_arc", ".", "item", "(", ")", "*", "num_insts", "\n", "total_arc_tag_loss", "+=", "loss_arc_tag", ".", "item", "(", ")", "*", "num_insts", "\n", "total_loss", "+=", "loss", ".", "item", "(", ")", "*", "num_insts", "\n", "total_train_inst", "+=", "num_insts", "\n", "# optimize parameters", "\n", "loss", ".", "backward", "(", ")", "\n", "clip_grad_norm_", "(", "model", ".", "parameters", "(", ")", ",", "args", ".", "clip", ")", "\n", "optimizer", ".", "step", "(", ")", "\n", "\n", "time_ave", "=", "(", "time", ".", "time", "(", ")", "-", "start_time", ")", "/", "batch_num", "\n", "time_left", "=", "(", "num_batches", "-", "batch_num", ")", "*", "time_ave", "\n", "\n", "# update log", "\n", "if", "batch_num", "%", "50", "==", "0", ":", "\n", "                    ", "log_info", "=", "'train: %d/%d, domain: %s, total loss: %.2f, arc_loss: %.2f, arc_tag_loss: %.2f, time left: %.2fs'", "%", "(", "batch_num", ",", "num_batches", ",", "args", ".", "domain", ",", "total_loss", "/", "total_train_inst", ",", "total_arc_loss", "/", "total_train_inst", ",", "\n", "total_arc_tag_loss", "/", "total_train_inst", ",", "time_left", ")", "\n", "sys", ".", "stdout", ".", "write", "(", "log_info", ")", "\n", "sys", ".", "stdout", ".", "write", "(", "'\\n'", ")", "\n", "sys", ".", "stdout", ".", "flush", "(", ")", "\n", "", "", "print", "(", "'\\n'", ")", "\n", "print", "(", "'train: %d/%d, domain: %s, total_loss: %.2f, arc_loss: %.2f, arc_tag_loss: %.2f, time: %.2fs'", "%", "\n", "(", "batch_num", ",", "num_batches", ",", "args", ".", "domain", ",", "total_loss", "/", "total_train_inst", ",", "total_arc_loss", "/", "total_train_inst", ",", "\n", "total_arc_tag_loss", "/", "total_train_inst", ",", "time", ".", "time", "(", ")", "-", "start_time", ")", ")", "\n", "\n", "dev_eval_dict", ",", "test_eval_dict", ",", "best_model", ",", "best_optimizer", ",", "patient", "=", "in_domain_evaluation", "(", "args", ",", "datasets", ",", "model", ",", "optimizer", ",", "dev_eval_dict", ",", "test_eval_dict", ",", "epoch", ",", "best_model", ",", "best_optimizer", ",", "patient", ")", "\n", "if", "patient", ">=", "args", ".", "schedule", ":", "\n", "                ", "lr", "=", "args", ".", "learning_rate", "/", "(", "1.0", "+", "epoch", "*", "args", ".", "decay_rate", ")", "\n", "optimizer", "=", "generate_optimizer", "(", "args", ",", "lr", ",", "model", ".", "parameters", "(", ")", ")", "\n", "print", "(", "'updated learning rate to %.6f'", "%", "lr", ")", "\n", "patient", "=", "0", "\n", "", "print_results", "(", "test_eval_dict", "[", "'in_domain'", "]", ",", "'test'", ",", "args", ".", "domain", ",", "'best_results'", ")", "\n", "print", "(", "'\\n'", ")", "\n", "", "for", "split", "in", "datasets", ".", "keys", "(", ")", ":", "\n", "            ", "eval_dict", "=", "evaluation", "(", "args", ",", "datasets", "[", "split", "]", ",", "split", ",", "best_model", ",", "args", ".", "domain", ",", "epoch", ",", "'best_results'", ")", "\n", "write_results", "(", "args", ",", "datasets", "[", "split", "]", ",", "args", ".", "domain", ",", "split", ",", "model", ",", "args", ".", "domain", ",", "eval_dict", ")", "\n", "\n", "", "", "else", ":", "\n", "        ", "logger", ".", "info", "(", "\"Evaluating\"", ")", "\n", "epoch", "=", "start_epoch", "\n", "for", "split", "in", "[", "'train'", ",", "'dev'", ",", "'test'", ",", "'poetry'", ",", "'prose'", "]", ":", "\n", "            ", "eval_dict", "=", "evaluation", "(", "args", ",", "datasets", "[", "split", "]", ",", "split", ",", "model", ",", "args", ".", "domain", ",", "epoch", ",", "'best_results'", ")", "\n", "write_results", "(", "args", ",", "datasets", "[", "split", "]", ",", "args", ".", "domain", ",", "split", ",", "model", ",", "args", ".", "domain", ",", "eval_dict", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.examples.SequenceTagger.read_arguments": [[28, 266], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "os.path.join", "print", "args_dict[].keys", "copy.deepcopy", "tuple", "tuple", "os.path.join", "os.path.join", "os.path.join", "torch.device", "torch.device", "logger.info", "SequenceTagger.save_args", "logger.info", "SequenceTagger.creating_alphabets", "collections.namedtuple", "collections.namedtuple.", "os.path.exists", "os.makedirs", "utils.io_.write_extra_labels.add_number_of_children", "set", "logger.info", "utils.load_word_embeddings.load_embedding_dict", "utils.load_word_embeddings.load_embedding_dict", "utils.load_word_embeddings.load_embedding_dict", "args_dict.keys", "utils.io_.write_extra_labels.add_distance_from_the_root", "torch.cuda.is_available", "torch.cuda.is_available", "utils.io_.write_extra_labels.Multitask_label_predict", "utils.io_.write_extra_labels.Multitask_coarse_predict", "len", "utils.io_.write_extra_labels.Multitask_POS_predict", "args_dict[].split", "utils.io_.write_extra_labels.add_relative_pos_based", "utils.io_.write_extra_labels.add_label", "utils.io_.write_extra_labels.add_relative_TAG", "utils.io_.write_extra_labels.add_head_coarse_pos", "utils.io_.write_extra_labels.predict_ma_tag_of_modifier", "utils.io_.write_extra_labels.Multitask_case_predict", "utils.io_.write_extra_labels.predict_coarse_of_modifier", "utils.io_.write_extra_labels.predict_case_of_modifier", "utils.io_.write_extra_labels.add_head_ma", "utils.io_.write_extra_labels.MRL_case", "utils.io_.write_extra_labels.MRL_POS", "utils.io_.write_extra_labels.MRL_no", "utils.io_.write_extra_labels.MRL_label", "utils.io_.write_extra_labels.MRL_Person", "utils.io_.write_extra_labels.MRL_Gender", "utils.io_.write_extra_labels.add_language_model"], "function", ["home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.keys", "home.repos.pwc.inspect_result.Jivnesh_LCM.examples.GraphParser.save_args", "home.repos.pwc.inspect_result.Jivnesh_LCM.examples.GraphParser.creating_alphabets", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.write_extra_labels.add_number_of_children", "home.repos.pwc.inspect_result.Jivnesh_LCM.utils.load_word_embeddings.load_embedding_dict", "home.repos.pwc.inspect_result.Jivnesh_LCM.utils.load_word_embeddings.load_embedding_dict", "home.repos.pwc.inspect_result.Jivnesh_LCM.utils.load_word_embeddings.load_embedding_dict", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.keys", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.write_extra_labels.add_distance_from_the_root", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.write_extra_labels.Multitask_label_predict", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.write_extra_labels.Multitask_coarse_predict", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.write_extra_labels.Multitask_POS_predict", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.write_extra_labels.add_relative_pos_based", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.write_extra_labels.add_label", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.write_extra_labels.add_relative_TAG", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.write_extra_labels.add_head_coarse_pos", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.write_extra_labels.predict_ma_tag_of_modifier", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.write_extra_labels.Multitask_case_predict", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.write_extra_labels.predict_coarse_of_modifier", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.write_extra_labels.predict_case_of_modifier", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.write_extra_labels.add_head_ma", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.write_extra_labels.MRL_case", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.write_extra_labels.MRL_POS", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.write_extra_labels.MRL_no", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.write_extra_labels.MRL_label", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.write_extra_labels.MRL_Person", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.write_extra_labels.MRL_Gender", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.write_extra_labels.add_language_model"], ["def", "read_arguments", "(", ")", ":", "\n", "    ", "args_", "=", "argparse", ".", "ArgumentParser", "(", "description", "=", "'Sovling SequenceTagger'", ")", "\n", "args_", ".", "add_argument", "(", "'--dataset'", ",", "choices", "=", "[", "'ontonotes'", ",", "'ud'", "]", ",", "help", "=", "'Dataset'", ",", "required", "=", "True", ")", "\n", "args_", ".", "add_argument", "(", "'--domain'", ",", "help", "=", "'domain'", ",", "required", "=", "True", ")", "\n", "args_", ".", "add_argument", "(", "'--rnn_mode'", ",", "choices", "=", "[", "'RNN'", ",", "'LSTM'", ",", "'GRU'", "]", ",", "help", "=", "'architecture of rnn'", ",", "\n", "required", "=", "True", ")", "\n", "args_", ".", "add_argument", "(", "'--task'", ",", "default", "=", "'distance_from_the_root'", ",", "choices", "=", "[", "'distance_from_the_root'", ",", "'number_of_children'", ",", "'relative_pos_based'", ",", "'language_model'", ",", "'add_label'", ",", "'add_head_coarse_pos'", ",", "'Multitask_POS_predict'", ",", "'Multitask_case_predict'", ",", "'Multitask_label_predict'", ",", "'Multitask_coarse_predict'", ",", "'MRL_Person'", ",", "'MRL_Gender'", ",", "'MRL_case'", ",", "'MRL_POS'", ",", "'MRL_no'", ",", "'MRL_label'", ",", "'predict_coarse_of_modifier'", ",", "'predict_ma_tag_of_modifier'", ",", "'add_head_ma'", ",", "'predict_case_of_modifier'", "]", ",", "help", "=", "'sequence_tagger task'", ")", "\n", "args_", ".", "add_argument", "(", "'--num_epochs'", ",", "type", "=", "int", ",", "default", "=", "200", ",", "help", "=", "'Number of training epochs'", ")", "\n", "args_", ".", "add_argument", "(", "'--batch_size'", ",", "type", "=", "int", ",", "default", "=", "64", ",", "help", "=", "'Number of sentences in each batch'", ")", "\n", "args_", ".", "add_argument", "(", "'--hidden_size'", ",", "type", "=", "int", ",", "default", "=", "256", ",", "help", "=", "'Number of hidden units in RNN'", ")", "\n", "args_", ".", "add_argument", "(", "'--tag_space'", ",", "type", "=", "int", ",", "default", "=", "128", ",", "help", "=", "'Dimension of tag space'", ")", "\n", "args_", ".", "add_argument", "(", "'--num_layers'", ",", "type", "=", "int", ",", "default", "=", "1", ",", "help", "=", "'Number of layers of RNN'", ")", "\n", "args_", ".", "add_argument", "(", "'--num_filters'", ",", "type", "=", "int", ",", "default", "=", "50", ",", "help", "=", "'Number of filters in CNN'", ")", "\n", "args_", ".", "add_argument", "(", "'--kernel_size'", ",", "type", "=", "int", ",", "default", "=", "3", ",", "help", "=", "'Size of Kernel for CNN'", ")", "\n", "args_", ".", "add_argument", "(", "'--use_pos'", ",", "action", "=", "'store_true'", ",", "help", "=", "'use part-of-speech embedding.'", ")", "\n", "args_", ".", "add_argument", "(", "'--use_char'", ",", "action", "=", "'store_true'", ",", "help", "=", "'use character embedding and CNN.'", ")", "\n", "args_", ".", "add_argument", "(", "'--word_dim'", ",", "type", "=", "int", ",", "default", "=", "300", ",", "help", "=", "'Dimension of word embeddings'", ")", "\n", "args_", ".", "add_argument", "(", "'--pos_dim'", ",", "type", "=", "int", ",", "default", "=", "50", ",", "help", "=", "'Dimension of POS embeddings'", ")", "\n", "args_", ".", "add_argument", "(", "'--char_dim'", ",", "type", "=", "int", ",", "default", "=", "50", ",", "help", "=", "'Dimension of Character embeddings'", ")", "\n", "args_", ".", "add_argument", "(", "'--initializer'", ",", "choices", "=", "[", "'xavier'", "]", ",", "help", "=", "'initialize model parameters'", ")", "\n", "args_", ".", "add_argument", "(", "'--opt'", ",", "choices", "=", "[", "'adam'", ",", "'sgd'", "]", ",", "help", "=", "'optimization algorithm'", ")", "\n", "args_", ".", "add_argument", "(", "'--momentum'", ",", "type", "=", "float", ",", "default", "=", "0.9", ",", "help", "=", "'momentum of optimizer'", ")", "\n", "args_", ".", "add_argument", "(", "'--betas'", ",", "nargs", "=", "2", ",", "type", "=", "float", ",", "default", "=", "[", "0.9", ",", "0.9", "]", ",", "help", "=", "'betas of optimizer'", ")", "\n", "args_", ".", "add_argument", "(", "'--learning_rate'", ",", "type", "=", "float", ",", "default", "=", "0.01", ",", "help", "=", "'Learning rate'", ")", "\n", "args_", ".", "add_argument", "(", "'--decay_rate'", ",", "type", "=", "float", ",", "default", "=", "0.05", ",", "help", "=", "'Decay rate of learning rate'", ")", "\n", "args_", ".", "add_argument", "(", "'--schedule'", ",", "type", "=", "int", ",", "help", "=", "'schedule for learning rate decay'", ")", "\n", "args_", ".", "add_argument", "(", "'--clip'", ",", "type", "=", "float", ",", "default", "=", "5.0", ",", "help", "=", "'gradient clipping'", ")", "\n", "args_", ".", "add_argument", "(", "'--gamma'", ",", "type", "=", "float", ",", "default", "=", "0.0", ",", "help", "=", "'weight for regularization'", ")", "\n", "args_", ".", "add_argument", "(", "'--epsilon'", ",", "type", "=", "float", ",", "default", "=", "1e-8", ",", "help", "=", "'epsilon for adam'", ")", "\n", "args_", ".", "add_argument", "(", "'--p_rnn'", ",", "nargs", "=", "2", ",", "type", "=", "float", ",", "required", "=", "True", ",", "help", "=", "'dropout rate for RNN'", ")", "\n", "args_", ".", "add_argument", "(", "'--p_in'", ",", "type", "=", "float", ",", "default", "=", "0.33", ",", "help", "=", "'dropout rate for input embeddings'", ")", "\n", "args_", ".", "add_argument", "(", "'--p_out'", ",", "type", "=", "float", ",", "default", "=", "0.33", ",", "help", "=", "'dropout rate for output layer'", ")", "\n", "args_", ".", "add_argument", "(", "'--unk_replace'", ",", "type", "=", "float", ",", "default", "=", "0.", ",", "\n", "help", "=", "'The rate to replace a singleton word with UNK'", ")", "\n", "args_", ".", "add_argument", "(", "'--punct_set'", ",", "nargs", "=", "'+'", ",", "type", "=", "str", ",", "help", "=", "'List of punctuations'", ")", "\n", "args_", ".", "add_argument", "(", "'--word_embedding'", ",", "choices", "=", "[", "'random'", ",", "'glove'", ",", "'fasttext'", ",", "'word2vec'", "]", ",", "\n", "help", "=", "'Embedding for words'", ")", "\n", "args_", ".", "add_argument", "(", "'--word_path'", ",", "help", "=", "'path for word embedding dict - in case word_embedding is not random'", ")", "\n", "args_", ".", "add_argument", "(", "'--freeze_word_embeddings'", ",", "action", "=", "'store_true'", ",", "help", "=", "'frozen the word embedding (disable fine-tuning).'", ")", "\n", "args_", ".", "add_argument", "(", "'--char_embedding'", ",", "choices", "=", "[", "'random'", ",", "'hellwig'", "]", ",", "help", "=", "'Embedding for characters'", ",", "\n", "required", "=", "True", ")", "\n", "args_", ".", "add_argument", "(", "'--pos_embedding'", ",", "choices", "=", "[", "'random'", ",", "'one_hot'", "]", ",", "help", "=", "'Embedding for pos'", ",", "\n", "required", "=", "True", ")", "\n", "args_", ".", "add_argument", "(", "'--char_path'", ",", "help", "=", "'path for character embedding dict'", ")", "\n", "args_", ".", "add_argument", "(", "'--pos_path'", ",", "help", "=", "'path for pos embedding dict'", ")", "\n", "args_", ".", "add_argument", "(", "'--use_unlabeled_data'", ",", "action", "=", "'store_true'", ",", "help", "=", "'flag to use unlabeled data.'", ")", "\n", "args_", ".", "add_argument", "(", "'--use_labeled_data'", ",", "action", "=", "'store_true'", ",", "help", "=", "'flag to use labeled data.'", ")", "\n", "args_", ".", "add_argument", "(", "'--model_path'", ",", "help", "=", "'path for saving model file.'", ",", "required", "=", "True", ")", "\n", "args_", ".", "add_argument", "(", "'--parser_path'", ",", "help", "=", "'path for loading parser files.'", ",", "default", "=", "None", ")", "\n", "args_", ".", "add_argument", "(", "'--load_path'", ",", "help", "=", "'path for loading saved source model file.'", ",", "default", "=", "None", ")", "\n", "args_", ".", "add_argument", "(", "'--strict'", ",", "action", "=", "'store_true'", ",", "help", "=", "'if True loaded model state should contain '", "\n", "'exactly the same keys as current model'", ")", "\n", "args_", ".", "add_argument", "(", "'--eval_mode'", ",", "action", "=", "'store_true'", ",", "help", "=", "'evaluating model without training it'", ")", "\n", "args", "=", "args_", ".", "parse_args", "(", ")", "\n", "args_dict", "=", "{", "}", "\n", "args_dict", "[", "'dataset'", "]", "=", "args", ".", "dataset", "\n", "args_dict", "[", "'domain'", "]", "=", "args", ".", "domain", "\n", "args_dict", "[", "'task'", "]", "=", "args", ".", "task", "\n", "args_dict", "[", "'rnn_mode'", "]", "=", "args", ".", "rnn_mode", "\n", "args_dict", "[", "'load_path'", "]", "=", "args", ".", "load_path", "\n", "args_dict", "[", "'strict'", "]", "=", "args", ".", "strict", "\n", "args_dict", "[", "'model_path'", "]", "=", "args", ".", "model_path", "\n", "if", "not", "path", ".", "exists", "(", "args_dict", "[", "'model_path'", "]", ")", ":", "\n", "        ", "makedirs", "(", "args_dict", "[", "'model_path'", "]", ")", "\n", "", "args_dict", "[", "'parser_path'", "]", "=", "args", ".", "parser_path", "\n", "args_dict", "[", "'model_name'", "]", "=", "'domain_'", "+", "args_dict", "[", "'domain'", "]", "\n", "args_dict", "[", "'full_model_name'", "]", "=", "path", ".", "join", "(", "args_dict", "[", "'model_path'", "]", ",", "args_dict", "[", "'model_name'", "]", ")", "\n", "args_dict", "[", "'use_unlabeled_data'", "]", "=", "args", ".", "use_unlabeled_data", "\n", "args_dict", "[", "'use_labeled_data'", "]", "=", "args", ".", "use_labeled_data", "\n", "print", "(", "args_dict", "[", "'parser_path'", "]", ")", "\n", "if", "args_dict", "[", "'task'", "]", "==", "'number_of_children'", ":", "\n", "        ", "args_dict", "[", "'data_paths'", "]", "=", "write_extra_labels", ".", "add_number_of_children", "(", "args_dict", "[", "'model_path'", "]", ",", "args_dict", "[", "'parser_path'", "]", ",", "args_dict", "[", "'domain'", "]", ",", "args_dict", "[", "'domain'", "]", ",", "\n", "use_unlabeled_data", "=", "args_dict", "[", "'use_unlabeled_data'", "]", ",", "\n", "use_labeled_data", "=", "args_dict", "[", "'use_labeled_data'", "]", ")", "\n", "", "elif", "args_dict", "[", "'task'", "]", "==", "'distance_from_the_root'", ":", "\n", "        ", "args_dict", "[", "'data_paths'", "]", "=", "write_extra_labels", ".", "add_distance_from_the_root", "(", "args_dict", "[", "'model_path'", "]", ",", "args_dict", "[", "'parser_path'", "]", ",", "args_dict", "[", "'domain'", "]", ",", "args_dict", "[", "'domain'", "]", ",", "\n", "use_unlabeled_data", "=", "args_dict", "[", "'use_unlabeled_data'", "]", ",", "\n", "use_labeled_data", "=", "args_dict", "[", "'use_labeled_data'", "]", ")", "\n", "", "elif", "args_dict", "[", "'task'", "]", "==", "'Multitask_label_predict'", ":", "\n", "        ", "args_dict", "[", "'data_paths'", "]", "=", "write_extra_labels", ".", "Multitask_label_predict", "(", "args_dict", "[", "'model_path'", "]", ",", "args_dict", "[", "'parser_path'", "]", ",", "args_dict", "[", "'domain'", "]", ",", "args_dict", "[", "'domain'", "]", ",", "\n", "use_unlabeled_data", "=", "args_dict", "[", "'use_unlabeled_data'", "]", ",", "\n", "use_labeled_data", "=", "args_dict", "[", "'use_labeled_data'", "]", ")", "\n", "", "elif", "args_dict", "[", "'task'", "]", "==", "'Multitask_coarse_predict'", ":", "\n", "        ", "args_dict", "[", "'data_paths'", "]", "=", "write_extra_labels", ".", "Multitask_coarse_predict", "(", "args_dict", "[", "'model_path'", "]", ",", "args_dict", "[", "'parser_path'", "]", ",", "args_dict", "[", "'domain'", "]", ",", "args_dict", "[", "'domain'", "]", ",", "\n", "use_unlabeled_data", "=", "args_dict", "[", "'use_unlabeled_data'", "]", ",", "\n", "use_labeled_data", "=", "args_dict", "[", "'use_labeled_data'", "]", ")", "\n", "", "elif", "args_dict", "[", "'task'", "]", "==", "'Multitask_POS_predict'", ":", "\n", "        ", "args_dict", "[", "'data_paths'", "]", "=", "write_extra_labels", ".", "Multitask_POS_predict", "(", "args_dict", "[", "'model_path'", "]", ",", "args_dict", "[", "'parser_path'", "]", ",", "args_dict", "[", "'domain'", "]", ",", "args_dict", "[", "'domain'", "]", ",", "\n", "use_unlabeled_data", "=", "args_dict", "[", "'use_unlabeled_data'", "]", ",", "\n", "use_labeled_data", "=", "args_dict", "[", "'use_labeled_data'", "]", ")", "\n", "", "elif", "args_dict", "[", "'task'", "]", "==", "'relative_pos_based'", ":", "\n", "        ", "args_dict", "[", "'data_paths'", "]", "=", "write_extra_labels", ".", "add_relative_pos_based", "(", "args_dict", "[", "'model_path'", "]", ",", "args_dict", "[", "'parser_path'", "]", ",", "args_dict", "[", "'domain'", "]", ",", "args_dict", "[", "'domain'", "]", ",", "\n", "use_unlabeled_data", "=", "args_dict", "[", "'use_unlabeled_data'", "]", ",", "\n", "use_labeled_data", "=", "args_dict", "[", "'use_labeled_data'", "]", ")", "\n", "", "elif", "args_dict", "[", "'task'", "]", "==", "'add_label'", ":", "\n", "        ", "args_dict", "[", "'data_paths'", "]", "=", "write_extra_labels", ".", "add_label", "(", "args_dict", "[", "'model_path'", "]", ",", "args_dict", "[", "'parser_path'", "]", ",", "args_dict", "[", "'domain'", "]", ",", "args_dict", "[", "'domain'", "]", ",", "\n", "use_unlabeled_data", "=", "args_dict", "[", "'use_unlabeled_data'", "]", ",", "\n", "use_labeled_data", "=", "args_dict", "[", "'use_labeled_data'", "]", ")", "\n", "", "elif", "args_dict", "[", "'task'", "]", "==", "'add_relative_TAG'", ":", "\n", "        ", "args_dict", "[", "'data_paths'", "]", "=", "write_extra_labels", ".", "add_relative_TAG", "(", "args_dict", "[", "'model_path'", "]", ",", "args_dict", "[", "'parser_path'", "]", ",", "args_dict", "[", "'domain'", "]", ",", "args_dict", "[", "'domain'", "]", ",", "\n", "use_unlabeled_data", "=", "args_dict", "[", "'use_unlabeled_data'", "]", ",", "\n", "use_labeled_data", "=", "args_dict", "[", "'use_labeled_data'", "]", ")", "\n", "", "elif", "args_dict", "[", "'task'", "]", "==", "'add_head_coarse_pos'", ":", "\n", "        ", "args_dict", "[", "'data_paths'", "]", "=", "write_extra_labels", ".", "add_head_coarse_pos", "(", "args_dict", "[", "'model_path'", "]", ",", "args_dict", "[", "'parser_path'", "]", ",", "args_dict", "[", "'domain'", "]", ",", "args_dict", "[", "'domain'", "]", ",", "\n", "use_unlabeled_data", "=", "args_dict", "[", "'use_unlabeled_data'", "]", ",", "\n", "use_labeled_data", "=", "args_dict", "[", "'use_labeled_data'", "]", ")", "\n", "", "elif", "args_dict", "[", "'task'", "]", "==", "'predict_ma_tag_of_modifier'", ":", "\n", "        ", "args_dict", "[", "'data_paths'", "]", "=", "write_extra_labels", ".", "predict_ma_tag_of_modifier", "(", "args_dict", "[", "'model_path'", "]", ",", "args_dict", "[", "'parser_path'", "]", ",", "args_dict", "[", "'domain'", "]", ",", "args_dict", "[", "'domain'", "]", ",", "\n", "use_unlabeled_data", "=", "args_dict", "[", "'use_unlabeled_data'", "]", ",", "\n", "use_labeled_data", "=", "args_dict", "[", "'use_labeled_data'", "]", ")", "\n", "", "elif", "args_dict", "[", "'task'", "]", "==", "'Multitask_case_predict'", ":", "\n", "        ", "args_dict", "[", "'data_paths'", "]", "=", "write_extra_labels", ".", "Multitask_case_predict", "(", "args_dict", "[", "'model_path'", "]", ",", "args_dict", "[", "'parser_path'", "]", ",", "args_dict", "[", "'domain'", "]", ",", "args_dict", "[", "'domain'", "]", ",", "\n", "use_unlabeled_data", "=", "args_dict", "[", "'use_unlabeled_data'", "]", ",", "\n", "use_labeled_data", "=", "args_dict", "[", "'use_labeled_data'", "]", ")", "\n", "", "elif", "args_dict", "[", "'task'", "]", "==", "'predict_coarse_of_modifier'", ":", "\n", "        ", "args_dict", "[", "'data_paths'", "]", "=", "write_extra_labels", ".", "predict_coarse_of_modifier", "(", "args_dict", "[", "'model_path'", "]", ",", "args_dict", "[", "'parser_path'", "]", ",", "args_dict", "[", "'domain'", "]", ",", "args_dict", "[", "'domain'", "]", ",", "\n", "use_unlabeled_data", "=", "args_dict", "[", "'use_unlabeled_data'", "]", ",", "\n", "use_labeled_data", "=", "args_dict", "[", "'use_labeled_data'", "]", ")", "\n", "", "elif", "args_dict", "[", "'task'", "]", "==", "'predict_case_of_modifier'", ":", "\n", "        ", "args_dict", "[", "'data_paths'", "]", "=", "write_extra_labels", ".", "predict_case_of_modifier", "(", "args_dict", "[", "'model_path'", "]", ",", "args_dict", "[", "'parser_path'", "]", ",", "args_dict", "[", "'domain'", "]", ",", "args_dict", "[", "'domain'", "]", ",", "\n", "use_unlabeled_data", "=", "args_dict", "[", "'use_unlabeled_data'", "]", ",", "\n", "use_labeled_data", "=", "args_dict", "[", "'use_labeled_data'", "]", ")", "\n", "", "elif", "args_dict", "[", "'task'", "]", "==", "'add_head_ma'", ":", "\n", "        ", "args_dict", "[", "'data_paths'", "]", "=", "write_extra_labels", ".", "add_head_ma", "(", "args_dict", "[", "'model_path'", "]", ",", "args_dict", "[", "'parser_path'", "]", ",", "args_dict", "[", "'domain'", "]", ",", "args_dict", "[", "'domain'", "]", ",", "\n", "use_unlabeled_data", "=", "args_dict", "[", "'use_unlabeled_data'", "]", ",", "\n", "use_labeled_data", "=", "args_dict", "[", "'use_labeled_data'", "]", ")", "\n", "", "elif", "args_dict", "[", "'task'", "]", "==", "'MRL_case'", ":", "\n", "        ", "args_dict", "[", "'data_paths'", "]", "=", "write_extra_labels", ".", "MRL_case", "(", "args_dict", "[", "'model_path'", "]", ",", "args_dict", "[", "'parser_path'", "]", ",", "args_dict", "[", "'domain'", "]", ",", "args_dict", "[", "'domain'", "]", ",", "\n", "use_unlabeled_data", "=", "args_dict", "[", "'use_unlabeled_data'", "]", ",", "\n", "use_labeled_data", "=", "args_dict", "[", "'use_labeled_data'", "]", ")", "\n", "", "elif", "args_dict", "[", "'task'", "]", "==", "'MRL_POS'", ":", "\n", "        ", "args_dict", "[", "'data_paths'", "]", "=", "write_extra_labels", ".", "MRL_POS", "(", "args_dict", "[", "'model_path'", "]", ",", "args_dict", "[", "'parser_path'", "]", ",", "args_dict", "[", "'domain'", "]", ",", "args_dict", "[", "'domain'", "]", ",", "\n", "use_unlabeled_data", "=", "args_dict", "[", "'use_unlabeled_data'", "]", ",", "\n", "use_labeled_data", "=", "args_dict", "[", "'use_labeled_data'", "]", ")", "\n", "", "elif", "args_dict", "[", "'task'", "]", "==", "'MRL_no'", ":", "\n", "        ", "args_dict", "[", "'data_paths'", "]", "=", "write_extra_labels", ".", "MRL_no", "(", "args_dict", "[", "'model_path'", "]", ",", "args_dict", "[", "'parser_path'", "]", ",", "args_dict", "[", "'domain'", "]", ",", "args_dict", "[", "'domain'", "]", ",", "\n", "use_unlabeled_data", "=", "args_dict", "[", "'use_unlabeled_data'", "]", ",", "\n", "use_labeled_data", "=", "args_dict", "[", "'use_labeled_data'", "]", ")", "\n", "", "elif", "args_dict", "[", "'task'", "]", "==", "'MRL_label'", ":", "\n", "        ", "args_dict", "[", "'data_paths'", "]", "=", "write_extra_labels", ".", "MRL_label", "(", "args_dict", "[", "'model_path'", "]", ",", "args_dict", "[", "'parser_path'", "]", ",", "args_dict", "[", "'domain'", "]", ",", "args_dict", "[", "'domain'", "]", ",", "\n", "use_unlabeled_data", "=", "args_dict", "[", "'use_unlabeled_data'", "]", ",", "\n", "use_labeled_data", "=", "args_dict", "[", "'use_labeled_data'", "]", ")", "\n", "", "elif", "args_dict", "[", "'task'", "]", "==", "'MRL_Person'", ":", "\n", "        ", "args_dict", "[", "'data_paths'", "]", "=", "write_extra_labels", ".", "MRL_Person", "(", "args_dict", "[", "'model_path'", "]", ",", "args_dict", "[", "'parser_path'", "]", ",", "args_dict", "[", "'domain'", "]", ",", "args_dict", "[", "'domain'", "]", ",", "\n", "use_unlabeled_data", "=", "args_dict", "[", "'use_unlabeled_data'", "]", ",", "\n", "use_labeled_data", "=", "args_dict", "[", "'use_labeled_data'", "]", ")", "\n", "", "elif", "args_dict", "[", "'task'", "]", "==", "'MRL_Gender'", ":", "\n", "        ", "args_dict", "[", "'data_paths'", "]", "=", "write_extra_labels", ".", "MRL_Gender", "(", "args_dict", "[", "'model_path'", "]", ",", "args_dict", "[", "'parser_path'", "]", ",", "args_dict", "[", "'domain'", "]", ",", "args_dict", "[", "'domain'", "]", ",", "\n", "use_unlabeled_data", "=", "args_dict", "[", "'use_unlabeled_data'", "]", ",", "\n", "use_labeled_data", "=", "args_dict", "[", "'use_labeled_data'", "]", ")", "\n", "", "else", ":", "#args_dict['task'] == 'language_model':", "\n", "        ", "args_dict", "[", "'data_paths'", "]", "=", "write_extra_labels", ".", "add_language_model", "(", "args_dict", "[", "'model_path'", "]", ",", "args_dict", "[", "'parser_path'", "]", ",", "args_dict", "[", "'domain'", "]", ",", "args_dict", "[", "'domain'", "]", ",", "\n", "use_unlabeled_data", "=", "args_dict", "[", "'use_unlabeled_data'", "]", ",", "\n", "use_labeled_data", "=", "args_dict", "[", "'use_labeled_data'", "]", ")", "\n", "", "args_dict", "[", "'splits'", "]", "=", "args_dict", "[", "'data_paths'", "]", ".", "keys", "(", ")", "\n", "alphabet_data_paths", "=", "deepcopy", "(", "args_dict", "[", "'data_paths'", "]", ")", "\n", "if", "args_dict", "[", "'dataset'", "]", "==", "'ontonotes'", ":", "\n", "        ", "data_path", "=", "'data/onto_pos_ner_dp'", "\n", "", "else", ":", "\n", "        ", "data_path", "=", "'data/ud_pos_ner_dp'", "\n", "# Adding more resources to make sure equal alphabet size for all domains", "\n", "", "for", "split", "in", "args_dict", "[", "'splits'", "]", ":", "\n", "        ", "if", "args_dict", "[", "'dataset'", "]", "==", "'ontonotes'", ":", "\n", "            ", "alphabet_data_paths", "[", "'additional_'", "+", "split", "]", "=", "data_path", "+", "'_'", "+", "split", "+", "'_'", "+", "'all'", "\n", "", "else", ":", "\n", "            ", "if", "'_'", "in", "args_dict", "[", "'domain'", "]", ":", "\n", "                ", "alphabet_data_paths", "[", "split", "]", "=", "data_path", "+", "'_'", "+", "split", "+", "'_'", "+", "args_dict", "[", "'domain'", "]", ".", "split", "(", "'_'", ")", "[", "0", "]", "\n", "", "else", ":", "\n", "                ", "alphabet_data_paths", "[", "split", "]", "=", "args_dict", "[", "'data_paths'", "]", "[", "split", "]", "\n", "", "", "", "args_dict", "[", "'alphabet_data_paths'", "]", "=", "alphabet_data_paths", "\n", "args_dict", "[", "'num_epochs'", "]", "=", "args", ".", "num_epochs", "\n", "args_dict", "[", "'batch_size'", "]", "=", "args", ".", "batch_size", "\n", "args_dict", "[", "'hidden_size'", "]", "=", "args", ".", "hidden_size", "\n", "args_dict", "[", "'tag_space'", "]", "=", "args", ".", "tag_space", "\n", "args_dict", "[", "'num_layers'", "]", "=", "args", ".", "num_layers", "\n", "args_dict", "[", "'num_filters'", "]", "=", "args", ".", "num_filters", "\n", "args_dict", "[", "'kernel_size'", "]", "=", "args", ".", "kernel_size", "\n", "args_dict", "[", "'learning_rate'", "]", "=", "args", ".", "learning_rate", "\n", "args_dict", "[", "'initializer'", "]", "=", "nn", ".", "init", ".", "xavier_uniform_", "if", "args", ".", "initializer", "==", "'xavier'", "else", "None", "\n", "args_dict", "[", "'opt'", "]", "=", "args", ".", "opt", "\n", "args_dict", "[", "'momentum'", "]", "=", "args", ".", "momentum", "\n", "args_dict", "[", "'betas'", "]", "=", "tuple", "(", "args", ".", "betas", ")", "\n", "args_dict", "[", "'epsilon'", "]", "=", "args", ".", "epsilon", "\n", "args_dict", "[", "'decay_rate'", "]", "=", "args", ".", "decay_rate", "\n", "args_dict", "[", "'clip'", "]", "=", "args", ".", "clip", "\n", "args_dict", "[", "'gamma'", "]", "=", "args", ".", "gamma", "\n", "args_dict", "[", "'schedule'", "]", "=", "args", ".", "schedule", "\n", "args_dict", "[", "'p_rnn'", "]", "=", "tuple", "(", "args", ".", "p_rnn", ")", "\n", "args_dict", "[", "'p_in'", "]", "=", "args", ".", "p_in", "\n", "args_dict", "[", "'p_out'", "]", "=", "args", ".", "p_out", "\n", "args_dict", "[", "'unk_replace'", "]", "=", "args", ".", "unk_replace", "\n", "args_dict", "[", "'punct_set'", "]", "=", "None", "\n", "if", "args", ".", "punct_set", "is", "not", "None", ":", "\n", "        ", "args_dict", "[", "'punct_set'", "]", "=", "set", "(", "args", ".", "punct_set", ")", "\n", "logger", ".", "info", "(", "\"punctuations(%d): %s\"", "%", "(", "len", "(", "args_dict", "[", "'punct_set'", "]", ")", ",", "' '", ".", "join", "(", "args_dict", "[", "'punct_set'", "]", ")", ")", ")", "\n", "", "args_dict", "[", "'freeze_word_embeddings'", "]", "=", "args", ".", "freeze_word_embeddings", "\n", "args_dict", "[", "'word_embedding'", "]", "=", "args", ".", "word_embedding", "\n", "args_dict", "[", "'word_path'", "]", "=", "args", ".", "word_path", "\n", "args_dict", "[", "'use_char'", "]", "=", "args", ".", "use_char", "\n", "args_dict", "[", "'char_embedding'", "]", "=", "args", ".", "char_embedding", "\n", "args_dict", "[", "'pos_embedding'", "]", "=", "args", ".", "pos_embedding", "\n", "args_dict", "[", "'char_path'", "]", "=", "args", ".", "char_path", "\n", "args_dict", "[", "'pos_path'", "]", "=", "args", ".", "pos_path", "\n", "args_dict", "[", "'use_pos'", "]", "=", "args", ".", "use_pos", "\n", "args_dict", "[", "'pos_dim'", "]", "=", "args", ".", "pos_dim", "\n", "args_dict", "[", "'word_dict'", "]", "=", "None", "\n", "args_dict", "[", "'word_dim'", "]", "=", "args", ".", "word_dim", "\n", "if", "args_dict", "[", "'word_embedding'", "]", "!=", "'random'", "and", "args_dict", "[", "'word_path'", "]", ":", "\n", "        ", "args_dict", "[", "'word_dict'", "]", ",", "args_dict", "[", "'word_dim'", "]", "=", "load_word_embeddings", ".", "load_embedding_dict", "(", "args_dict", "[", "'word_embedding'", "]", ",", "\n", "args_dict", "[", "'word_path'", "]", ")", "\n", "", "args_dict", "[", "'char_dict'", "]", "=", "None", "\n", "args_dict", "[", "'char_dim'", "]", "=", "args", ".", "char_dim", "\n", "if", "args_dict", "[", "'char_embedding'", "]", "!=", "'random'", ":", "\n", "        ", "args_dict", "[", "'char_dict'", "]", ",", "args_dict", "[", "'char_dim'", "]", "=", "load_word_embeddings", ".", "load_embedding_dict", "(", "args_dict", "[", "'char_embedding'", "]", ",", "\n", "args_dict", "[", "'char_path'", "]", ")", "\n", "", "args_dict", "[", "'pos_dict'", "]", "=", "None", "\n", "if", "args_dict", "[", "'pos_embedding'", "]", "!=", "'random'", ":", "\n", "        ", "args_dict", "[", "'pos_dict'", "]", ",", "args_dict", "[", "'pos_dim'", "]", "=", "load_word_embeddings", ".", "load_embedding_dict", "(", "args_dict", "[", "'pos_embedding'", "]", ",", "\n", "args_dict", "[", "'pos_path'", "]", ")", "\n", "", "args_dict", "[", "'alphabet_path'", "]", "=", "path", ".", "join", "(", "args_dict", "[", "'model_path'", "]", ",", "'alphabets'", "+", "'_src_domain_'", "+", "args_dict", "[", "'domain'", "]", "+", "'/'", ")", "\n", "args_dict", "[", "'alphabet_parser_path'", "]", "=", "path", ".", "join", "(", "args_dict", "[", "'parser_path'", "]", ",", "'alphabets'", "+", "'_src_domain_'", "+", "args_dict", "[", "'domain'", "]", "+", "'/'", ")", "\n", "args_dict", "[", "'model_name'", "]", "=", "path", ".", "join", "(", "args_dict", "[", "'model_path'", "]", ",", "args_dict", "[", "'model_name'", "]", ")", "\n", "args_dict", "[", "'eval_mode'", "]", "=", "args", ".", "eval_mode", "\n", "args_dict", "[", "'device'", "]", "=", "torch", ".", "device", "(", "'cuda'", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "else", "'cpu'", ")", "\n", "args_dict", "[", "'word_status'", "]", "=", "'frozen'", "if", "args", ".", "freeze_word_embeddings", "else", "'fine tune'", "\n", "args_dict", "[", "'char_status'", "]", "=", "'enabled'", "if", "args", ".", "use_char", "else", "'disabled'", "\n", "args_dict", "[", "'pos_status'", "]", "=", "'enabled'", "if", "args", ".", "use_pos", "else", "'disabled'", "\n", "logger", ".", "info", "(", "\"Saving arguments to file\"", ")", "\n", "save_args", "(", "args", ",", "args_dict", "[", "'full_model_name'", "]", ")", "\n", "logger", ".", "info", "(", "\"Creating Alphabets\"", ")", "\n", "alphabet_dict", "=", "creating_alphabets", "(", "args_dict", "[", "'alphabet_path'", "]", ",", "args_dict", "[", "'alphabet_parser_path'", "]", ",", "args_dict", "[", "'alphabet_data_paths'", "]", ")", "\n", "args_dict", "=", "{", "**", "args_dict", ",", "**", "alphabet_dict", "}", "\n", "ARGS", "=", "namedtuple", "(", "'ARGS'", ",", "args_dict", ".", "keys", "(", ")", ")", "\n", "my_args", "=", "ARGS", "(", "**", "args_dict", ")", "\n", "return", "my_args", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.examples.SequenceTagger.creating_alphabets": [[268, 277], ["alphabet_data_paths.values", "utils.io_.prepare_data.create_alphabets_for_sequence_tagger", "alphabet_dict[].items", "v.size", "logger.info", "k.split"], "function", ["home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.values", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.prepare_data.create_alphabets_for_sequence_tagger", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.items", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.size"], ["", "def", "creating_alphabets", "(", "alphabet_path", ",", "alphabet_parser_path", ",", "alphabet_data_paths", ")", ":", "\n", "    ", "data_paths_list", "=", "alphabet_data_paths", ".", "values", "(", ")", "\n", "alphabet_dict", "=", "{", "}", "\n", "alphabet_dict", "[", "'alphabets'", "]", "=", "prepare_data", ".", "create_alphabets_for_sequence_tagger", "(", "alphabet_path", ",", "alphabet_parser_path", ",", "data_paths_list", ")", "\n", "for", "k", ",", "v", "in", "alphabet_dict", "[", "'alphabets'", "]", ".", "items", "(", ")", ":", "\n", "        ", "num_key", "=", "'num_'", "+", "k", ".", "split", "(", "'_alphabet'", ")", "[", "0", "]", "\n", "alphabet_dict", "[", "num_key", "]", "=", "v", ".", "size", "(", ")", "\n", "logger", ".", "info", "(", "\"%s : %d\"", "%", "(", "num_key", ",", "alphabet_dict", "[", "num_key", "]", ")", ")", "\n", "", "return", "alphabet_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.examples.SequenceTagger.construct_embedding_table": [[278, 297], ["numpy.sqrt", "numpy.empty", "numpy.random.uniform().astype", "alphabet.items", "print", "torch.from_numpy", "torch.from_numpy", "alphabet.size", "numpy.random.uniform", "numpy.random.uniform().astype", "numpy.random.uniform().astype", "numpy.random.uniform", "numpy.random.uniform"], "function", ["home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.items", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.size"], ["", "def", "construct_embedding_table", "(", "alphabet", ",", "tokens_dict", ",", "dim", ",", "token_type", "=", "'word'", ")", ":", "\n", "    ", "if", "tokens_dict", "is", "None", ":", "\n", "        ", "return", "None", "\n", "", "scale", "=", "np", ".", "sqrt", "(", "3.0", "/", "dim", ")", "\n", "table", "=", "np", ".", "empty", "(", "[", "alphabet", ".", "size", "(", ")", ",", "dim", "]", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "table", "[", "prepare_data", ".", "UNK_ID", ",", ":", "]", "=", "np", ".", "random", ".", "uniform", "(", "-", "scale", ",", "scale", ",", "[", "1", ",", "dim", "]", ")", ".", "astype", "(", "np", ".", "float32", ")", "\n", "oov_tokens", "=", "0", "\n", "for", "token", ",", "index", "in", "alphabet", ".", "items", "(", ")", ":", "\n", "        ", "if", "token", "in", "tokens_dict", ":", "\n", "            ", "embedding", "=", "tokens_dict", "[", "token", "]", "\n", "if", "token", "==", "'ata'", ":", "\n", "                ", "embedding", "=", "np", ".", "random", ".", "uniform", "(", "-", "scale", ",", "scale", ",", "[", "1", ",", "dim", "]", ")", ".", "astype", "(", "np", ".", "float32", ")", "\n", "", "", "else", ":", "\n", "            ", "embedding", "=", "np", ".", "random", ".", "uniform", "(", "-", "scale", ",", "scale", ",", "[", "1", ",", "dim", "]", ")", ".", "astype", "(", "np", ".", "float32", ")", "\n", "oov_tokens", "+=", "1", "\n", "", "table", "[", "index", ",", ":", "]", "=", "embedding", "\n", "", "print", "(", "'token type : %s, number of oov: %d'", "%", "(", "token_type", ",", "oov_tokens", ")", ")", "\n", "table", "=", "torch", ".", "from_numpy", "(", "table", ")", "\n", "return", "table", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.examples.SequenceTagger.get_free_gpu": [[298, 304], ["os.system", "os.remove", "int", "str", "open().readlines", "numpy.argmax", "x.split", "open"], "function", ["home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.open"], ["", "def", "get_free_gpu", "(", ")", ":", "\n", "    ", "system", "(", "'nvidia-smi -q -d Memory |grep -A4 GPU|grep Free > tmp.txt'", ")", "\n", "memory_available", "=", "[", "int", "(", "x", ".", "split", "(", ")", "[", "2", "]", ")", "for", "x", "in", "open", "(", "'tmp.txt'", ",", "'r'", ")", ".", "readlines", "(", ")", "]", "\n", "remove", "(", "\"tmp.txt\"", ")", "\n", "free_device", "=", "'cuda:'", "+", "str", "(", "np", ".", "argmax", "(", "memory_available", ")", ")", "\n", "return", "free_device", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.examples.SequenceTagger.save_args": [[305, 310], ["vars", "open", "json.dump"], "function", ["home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.open"], ["", "def", "save_args", "(", "args", ",", "full_model_name", ")", ":", "\n", "    ", "arg_path", "=", "full_model_name", "+", "'.arg.json'", "\n", "argparse_dict", "=", "vars", "(", "args", ")", "\n", "with", "open", "(", "arg_path", ",", "'w'", ")", "as", "f", ":", "\n", "        ", "json", ".", "dump", "(", "argparse_dict", ",", "f", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.examples.SequenceTagger.generate_optimizer": [[311, 319], ["filter", "torch.optim.Adam", "torch.optim.SGD", "ValueError"], "function", ["None"], ["", "", "def", "generate_optimizer", "(", "args", ",", "lr", ",", "params", ")", ":", "\n", "    ", "params", "=", "filter", "(", "lambda", "param", ":", "param", ".", "requires_grad", ",", "params", ")", "\n", "if", "args", ".", "opt", "==", "'adam'", ":", "\n", "        ", "return", "Adam", "(", "params", ",", "lr", "=", "lr", ",", "betas", "=", "args", ".", "betas", ",", "weight_decay", "=", "args", ".", "gamma", ",", "eps", "=", "args", ".", "epsilon", ")", "\n", "", "elif", "args", ".", "opt", "==", "'sgd'", ":", "\n", "        ", "return", "SGD", "(", "params", ",", "lr", "=", "lr", ",", "momentum", "=", "args", ".", "momentum", ",", "weight_decay", "=", "args", ".", "gamma", ",", "nesterov", "=", "True", ")", "\n", "", "else", ":", "\n", "        ", "raise", "ValueError", "(", "'Unknown optimization algorithm: %s'", "%", "args", ".", "opt", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.examples.SequenceTagger.save_checkpoint": [[321, 328], ["print", "torch.save", "torch.save", "model.state_dict", "optimizer.state_dict"], "function", ["home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.save", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.save"], ["", "", "def", "save_checkpoint", "(", "model", ",", "optimizer", ",", "opt", ",", "dev_eval_dict", ",", "test_eval_dict", ",", "full_model_name", ")", ":", "\n", "    ", "path_name", "=", "full_model_name", "+", "'.pt'", "\n", "print", "(", "'Saving model to: %s'", "%", "path_name", ")", "\n", "state", "=", "{", "'model_state_dict'", ":", "model", ".", "state_dict", "(", ")", ",", "\n", "'optimizer_state_dict'", ":", "optimizer", ".", "state_dict", "(", ")", ",", "\n", "'opt'", ":", "opt", ",", "'dev_eval_dict'", ":", "dev_eval_dict", ",", "'test_eval_dict'", ":", "test_eval_dict", "}", "\n", "torch", ".", "save", "(", "state", ",", "path_name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.examples.SequenceTagger.load_checkpoint": [[330, 342], ["print", "torch.load", "torch.load", "model.load_state_dict", "ValueError", "optimizer.load_state_dict"], "function", ["home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.load", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.load"], ["", "def", "load_checkpoint", "(", "args", ",", "model", ",", "optimizer", ",", "dev_eval_dict", ",", "test_eval_dict", ",", "start_epoch", ",", "load_path", ",", "strict", "=", "True", ")", ":", "\n", "    ", "print", "(", "'Loading saved model from: %s'", "%", "load_path", ")", "\n", "checkpoint", "=", "torch", ".", "load", "(", "load_path", ",", "map_location", "=", "args", ".", "device", ")", "\n", "if", "checkpoint", "[", "'opt'", "]", "!=", "args", ".", "opt", ":", "\n", "        ", "raise", "ValueError", "(", "'loaded optimizer type is: %s instead of: %s'", "%", "(", "checkpoint", "[", "'opt'", "]", ",", "args", ".", "opt", ")", ")", "\n", "", "model", ".", "load_state_dict", "(", "checkpoint", "[", "'model_state_dict'", "]", ",", "strict", "=", "strict", ")", "\n", "if", "strict", ":", "\n", "        ", "optimizer", ".", "load_state_dict", "(", "checkpoint", "[", "'optimizer_state_dict'", "]", ")", "\n", "dev_eval_dict", "=", "checkpoint", "[", "'dev_eval_dict'", "]", "\n", "test_eval_dict", "=", "checkpoint", "[", "'test_eval_dict'", "]", "\n", "start_epoch", "=", "dev_eval_dict", "[", "'in_domain'", "]", "[", "'epoch'", "]", "\n", "", "return", "model", ",", "optimizer", ",", "dev_eval_dict", ",", "test_eval_dict", ",", "start_epoch", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.examples.SequenceTagger.build_model_and_optimizer": [[344, 371], ["SequenceTagger.construct_embedding_table", "SequenceTagger.construct_embedding_table", "SequenceTagger.construct_embedding_table", "utils.models.sequence_tagger.Sequence_Tagger", "SequenceTagger.generate_optimizer", "utils.models.sequence_tagger.Sequence_Tagger.to", "utils.models.sequence_tagger.Sequence_Tagger.parameters", "SequenceTagger.initialize_eval_dict", "SequenceTagger.initialize_eval_dict", "SequenceTagger.load_checkpoint"], "function", ["home.repos.pwc.inspect_result.Jivnesh_LCM.examples.GraphParser.construct_embedding_table", "home.repos.pwc.inspect_result.Jivnesh_LCM.examples.GraphParser.construct_embedding_table", "home.repos.pwc.inspect_result.Jivnesh_LCM.examples.GraphParser.construct_embedding_table", "home.repos.pwc.inspect_result.Jivnesh_LCM.examples.GraphParser.generate_optimizer", "home.repos.pwc.inspect_result.Jivnesh_LCM.examples.GraphParser.initialize_eval_dict", "home.repos.pwc.inspect_result.Jivnesh_LCM.examples.GraphParser.initialize_eval_dict", "home.repos.pwc.inspect_result.Jivnesh_LCM.examples.GraphParser.load_checkpoint"], ["", "def", "build_model_and_optimizer", "(", "args", ")", ":", "\n", "    ", "word_table", "=", "construct_embedding_table", "(", "args", ".", "alphabets", "[", "'word_alphabet'", "]", ",", "args", ".", "word_dict", ",", "args", ".", "word_dim", ",", "token_type", "=", "'word'", ")", "\n", "char_table", "=", "construct_embedding_table", "(", "args", ".", "alphabets", "[", "'char_alphabet'", "]", ",", "args", ".", "char_dict", ",", "args", ".", "char_dim", ",", "token_type", "=", "'char'", ")", "\n", "pos_table", "=", "construct_embedding_table", "(", "args", ".", "alphabets", "[", "'pos_alphabet'", "]", ",", "args", ".", "pos_dict", ",", "args", ".", "pos_dim", ",", "token_type", "=", "'pos'", ")", "\n", "model", "=", "Sequence_Tagger", "(", "args", ".", "word_dim", ",", "args", ".", "num_word", ",", "args", ".", "char_dim", ",", "args", ".", "num_char", ",", "\n", "args", ".", "use_pos", ",", "args", ".", "use_char", ",", "args", ".", "pos_dim", ",", "args", ".", "num_pos", ",", "\n", "args", ".", "num_filters", ",", "args", ".", "kernel_size", ",", "args", ".", "rnn_mode", ",", "\n", "args", ".", "hidden_size", ",", "args", ".", "num_layers", ",", "args", ".", "tag_space", ",", "args", ".", "num_auto_label", ",", "\n", "embedd_word", "=", "word_table", ",", "embedd_char", "=", "char_table", ",", "embedd_pos", "=", "pos_table", ",", "\n", "p_in", "=", "args", ".", "p_in", ",", "p_out", "=", "args", ".", "p_out", ",", "p_rnn", "=", "args", ".", "p_rnn", ",", "\n", "initializer", "=", "args", ".", "initializer", ")", "\n", "optimizer", "=", "generate_optimizer", "(", "args", ",", "args", ".", "learning_rate", ",", "model", ".", "parameters", "(", ")", ")", "\n", "start_epoch", "=", "0", "\n", "dev_eval_dict", "=", "{", "'in_domain'", ":", "initialize_eval_dict", "(", ")", "}", "\n", "test_eval_dict", "=", "{", "'in_domain'", ":", "initialize_eval_dict", "(", ")", "}", "\n", "if", "args", ".", "load_path", ":", "\n", "        ", "model", ",", "optimizer", ",", "dev_eval_dict", ",", "test_eval_dict", ",", "start_epoch", "=", "load_checkpoint", "(", "args", ",", "model", ",", "optimizer", ",", "\n", "dev_eval_dict", ",", "test_eval_dict", ",", "\n", "start_epoch", ",", "args", ".", "load_path", ",", "strict", "=", "args", ".", "strict", ")", "\n", "", "if", "args", ".", "freeze_word_embeddings", ":", "\n", "        ", "model", ".", "rnn_encoder", ".", "word_embedd", ".", "weight", ".", "requires_grad", "=", "False", "\n", "# model.rnn_encoder.char_embedd.weight.requires_grad = False", "\n", "# model.rnn_encoder.pos_embedd.weight.requires_grad = False", "\n", "", "device", "=", "args", ".", "device", "\n", "model", ".", "to", "(", "device", ")", "\n", "return", "model", ",", "optimizer", ",", "dev_eval_dict", ",", "test_eval_dict", ",", "start_epoch", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.examples.SequenceTagger.initialize_eval_dict": [[373, 380], ["None"], "function", ["None"], ["", "def", "initialize_eval_dict", "(", ")", ":", "\n", "    ", "eval_dict", "=", "{", "}", "\n", "eval_dict", "[", "'auto_label_accuracy'", "]", "=", "0.0", "\n", "eval_dict", "[", "'auto_label_precision'", "]", "=", "0.0", "\n", "eval_dict", "[", "'auto_label_recall'", "]", "=", "0.0", "\n", "eval_dict", "[", "'auto_label_f1'", "]", "=", "0.0", "\n", "return", "eval_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.examples.SequenceTagger.in_domain_evaluation": [[381, 407], ["SequenceTagger.evaluation", "print", "evaluation.items", "SequenceTagger.evaluation", "evaluation.items", "copy.deepcopy", "copy.deepcopy", "SequenceTagger.save_checkpoint", "SequenceTagger.write_results"], "function", ["home.repos.pwc.inspect_result.Jivnesh_LCM.examples.GraphParser.evaluation", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.items", "home.repos.pwc.inspect_result.Jivnesh_LCM.examples.GraphParser.evaluation", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.items", "home.repos.pwc.inspect_result.Jivnesh_LCM.examples.GraphParser.save_checkpoint", "home.repos.pwc.inspect_result.Jivnesh_LCM.examples.GraphParser.write_results"], ["", "def", "in_domain_evaluation", "(", "args", ",", "datasets", ",", "model", ",", "optimizer", ",", "dev_eval_dict", ",", "test_eval_dict", ",", "epoch", ",", "\n", "best_model", ",", "best_optimizer", ",", "patient", ")", ":", "\n", "# In-domain evaluation", "\n", "    ", "curr_dev_eval_dict", "=", "evaluation", "(", "args", ",", "datasets", "[", "'dev'", "]", ",", "'dev'", ",", "model", ",", "args", ".", "domain", ",", "epoch", ",", "'current_results'", ")", "\n", "is_best_in_domain", "=", "dev_eval_dict", "[", "'in_domain'", "]", "[", "'auto_label_f1'", "]", "<=", "curr_dev_eval_dict", "[", "'auto_label_f1'", "]", "\n", "\n", "if", "is_best_in_domain", ":", "\n", "        ", "for", "key", ",", "value", "in", "curr_dev_eval_dict", ".", "items", "(", ")", ":", "\n", "            ", "dev_eval_dict", "[", "'in_domain'", "]", "[", "key", "]", "=", "value", "\n", "", "curr_test_eval_dict", "=", "evaluation", "(", "args", ",", "datasets", "[", "'test'", "]", ",", "'test'", ",", "model", ",", "args", ".", "domain", ",", "epoch", ",", "'current_results'", ")", "\n", "for", "key", ",", "value", "in", "curr_test_eval_dict", ".", "items", "(", ")", ":", "\n", "            ", "test_eval_dict", "[", "'in_domain'", "]", "[", "key", "]", "=", "value", "\n", "", "best_model", "=", "deepcopy", "(", "model", ")", "\n", "best_optimizer", "=", "deepcopy", "(", "optimizer", ")", "\n", "patient", "=", "0", "\n", "", "else", ":", "\n", "        ", "patient", "+=", "1", "\n", "", "if", "epoch", "==", "args", ".", "num_epochs", ":", "\n", "# save in-domain checkpoint", "\n", "        ", "for", "split", "in", "[", "'dev'", ",", "'test'", "]", ":", "\n", "            ", "eval_dict", "=", "dev_eval_dict", "[", "'in_domain'", "]", "if", "split", "==", "'dev'", "else", "test_eval_dict", "[", "'in_domain'", "]", "\n", "write_results", "(", "args", ",", "datasets", "[", "split", "]", ",", "args", ".", "domain", ",", "split", ",", "best_model", ",", "args", ".", "domain", ",", "eval_dict", ")", "\n", "", "save_checkpoint", "(", "best_model", ",", "best_optimizer", ",", "args", ".", "opt", ",", "dev_eval_dict", ",", "test_eval_dict", ",", "args", ".", "full_model_name", ")", "\n", "\n", "", "print", "(", "'\\n'", ")", "\n", "return", "dev_eval_dict", ",", "test_eval_dict", ",", "best_model", ",", "best_optimizer", ",", "patient", ",", "curr_dev_eval_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.examples.SequenceTagger.evaluation": [[409, 439], ["model.eval", "utils.io_.Index2Instance", "SequenceTagger.initialize_eval_dict", "utils.io_.prepare_data.iterate_batch", "utils.tasks.seqeval.classification_report", "SequenceTagger.print_results", "model.forward", "model.decode", "lengths.cpu().numpy.cpu().numpy", "word.data.cpu().numpy.data.cpu().numpy", "pos.data.cpu().numpy.data.cpu().numpy", "ner.data.cpu().numpy.data.cpu().numpy", "heads.data.cpu().numpy.data.cpu().numpy", "arc_tags.data.cpu().numpy.data.cpu().numpy", "auto_label.data.cpu().numpy.data.cpu().numpy", "auto_label_preds.data.cpu().numpy.data.cpu().numpy", "utils.io_.Index2Instance.index2instance", "utils.io_.Index2Instance.index2instance", "utils.tasks.seqeval.accuracy_score", "utils.tasks.seqeval.precision_score", "utils.tasks.seqeval.recall_score", "utils.tasks.seqeval.f1_score", "lengths.cpu().numpy.cpu", "word.data.cpu().numpy.data.cpu", "pos.data.cpu().numpy.data.cpu", "ner.data.cpu().numpy.data.cpu", "heads.data.cpu().numpy.data.cpu", "arc_tags.data.cpu().numpy.data.cpu", "auto_label.data.cpu().numpy.data.cpu", "auto_label_preds.data.cpu().numpy.data.cpu"], "function", ["home.repos.pwc.inspect_result.Jivnesh_LCM.examples.GraphParser.initialize_eval_dict", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.prepare_data.iterate_batch", "home.repos.pwc.inspect_result.Jivnesh_LCM.tasks.seqeval.classification_report", "home.repos.pwc.inspect_result.Jivnesh_LCM.examples.GraphParser.print_results", "home.repos.pwc.inspect_result.Jivnesh_LCM.models.parsing_gating.Gating.forward", "home.repos.pwc.inspect_result.Jivnesh_LCM.models.parsing_gating.BiAffine_Parser_Decoder.decode", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.writer.Index2Instance.index2instance", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.writer.Index2Instance.index2instance", "home.repos.pwc.inspect_result.Jivnesh_LCM.tasks.seqeval.accuracy_score", "home.repos.pwc.inspect_result.Jivnesh_LCM.tasks.seqeval.precision_score", "home.repos.pwc.inspect_result.Jivnesh_LCM.tasks.seqeval.recall_score", "home.repos.pwc.inspect_result.Jivnesh_LCM.tasks.seqeval.f1_score"], ["", "def", "evaluation", "(", "args", ",", "data", ",", "split", ",", "model", ",", "domain", ",", "epoch", ",", "str_res", "=", "'results'", ")", ":", "\n", "# evaluate performance on data", "\n", "    ", "model", ".", "eval", "(", ")", "\n", "auto_label_idx2inst", "=", "Index2Instance", "(", "args", ".", "alphabets", "[", "'auto_label_alphabet'", "]", ")", "\n", "eval_dict", "=", "initialize_eval_dict", "(", ")", "\n", "eval_dict", "[", "'epoch'", "]", "=", "epoch", "\n", "pred_labels", "=", "[", "]", "\n", "gold_labels", "=", "[", "]", "\n", "for", "batch", "in", "prepare_data", ".", "iterate_batch", "(", "data", ",", "args", ".", "batch_size", ",", "args", ".", "device", ")", ":", "\n", "        ", "word", ",", "char", ",", "pos", ",", "ner", ",", "heads", ",", "arc_tags", ",", "auto_label", ",", "masks", ",", "lengths", "=", "batch", "\n", "output", ",", "masks", ",", "lengths", "=", "model", ".", "forward", "(", "word", ",", "char", ",", "pos", ",", "mask", "=", "masks", ",", "length", "=", "lengths", ")", "\n", "auto_label_preds", "=", "model", ".", "decode", "(", "output", ",", "mask", "=", "masks", ",", "length", "=", "lengths", ",", "leading_symbolic", "=", "prepare_data", ".", "NUM_SYMBOLIC_TAGS", ")", "\n", "lengths", "=", "lengths", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "word", "=", "word", ".", "data", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "pos", "=", "pos", ".", "data", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "ner", "=", "ner", ".", "data", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "heads", "=", "heads", ".", "data", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "arc_tags", "=", "arc_tags", ".", "data", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "auto_label", "=", "auto_label", ".", "data", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "auto_label_preds", "=", "auto_label_preds", ".", "data", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "gold_labels", "+=", "auto_label_idx2inst", ".", "index2instance", "(", "auto_label", ",", "lengths", ",", "symbolic_root", "=", "True", ")", "\n", "pred_labels", "+=", "auto_label_idx2inst", ".", "index2instance", "(", "auto_label_preds", ",", "lengths", ",", "symbolic_root", "=", "True", ")", "\n", "\n", "", "eval_dict", "[", "'auto_label_accuracy'", "]", "=", "accuracy_score", "(", "gold_labels", ",", "pred_labels", ")", "*", "100", "\n", "eval_dict", "[", "'auto_label_precision'", "]", "=", "precision_score", "(", "gold_labels", ",", "pred_labels", ")", "*", "100", "\n", "eval_dict", "[", "'auto_label_recall'", "]", "=", "recall_score", "(", "gold_labels", ",", "pred_labels", ")", "*", "100", "\n", "eval_dict", "[", "'auto_label_f1'", "]", "=", "f1_score", "(", "gold_labels", ",", "pred_labels", ")", "*", "100", "\n", "eval_dict", "[", "'classification_report'", "]", "=", "classification_report", "(", "gold_labels", ",", "pred_labels", ")", "\n", "print_results", "(", "eval_dict", ",", "split", ",", "domain", ",", "str_res", ")", "\n", "return", "eval_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.examples.SequenceTagger.print_results": [[441, 450], ["print", "print", "print", "print", "print"], "function", ["None"], ["", "def", "print_results", "(", "eval_dict", ",", "split", ",", "domain", ",", "str_res", "=", "'results'", ")", ":", "\n", "    ", "print", "(", "'----------------------------------------------------------------------------------------------------------------------------'", ")", "\n", "print", "(", "'Testing model on domain %s'", "%", "domain", ")", "\n", "print", "(", "'--------------- sequence_tagger - %s ---------------'", "%", "split", ")", "\n", "print", "(", "\n", "str_res", "+", "' on '", "+", "split", "+", "' accuracy: %.2f%%, precision: %.2f%%, recall: %.2f%%, F1: %.2f%% (epoch: %d)'", "\n", "%", "(", "eval_dict", "[", "'auto_label_accuracy'", "]", ",", "eval_dict", "[", "'auto_label_precision'", "]", ",", "eval_dict", "[", "'auto_label_recall'", "]", ",", "eval_dict", "[", "'auto_label_f1'", "]", ",", "\n", "eval_dict", "[", "'epoch'", "]", ")", ")", "\n", "print", "(", "eval_dict", "[", "'classification_report'", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.examples.SequenceTagger.write_results": [[452, 486], ["utils.io_.Writer", "utils.io_.Writer", "utils.io_.Writer.start", "utils.io_.Writer.start", "utils.io_.prepare_data.iterate_batch", "utils.io_.Writer.close", "utils.io_.Writer.close", "open", "json.dump", "model.forward", "model.decode", "lengths.cpu().numpy.cpu().numpy", "word.data.cpu().numpy.data.cpu().numpy", "pos.data.cpu().numpy.data.cpu().numpy", "ner.data.cpu().numpy.data.cpu().numpy", "heads.data.cpu().numpy.data.cpu().numpy", "arc_tags.data.cpu().numpy.data.cpu().numpy", "auto_label_preds.data.cpu().numpy.data.cpu().numpy", "utils.io_.Writer.write", "utils.io_.Writer.write", "lengths.cpu().numpy.cpu", "word.data.cpu().numpy.data.cpu", "pos.data.cpu().numpy.data.cpu", "ner.data.cpu().numpy.data.cpu", "heads.data.cpu().numpy.data.cpu", "arc_tags.data.cpu().numpy.data.cpu", "auto_label_preds.data.cpu().numpy.data.cpu"], "function", ["home.repos.pwc.inspect_result.Jivnesh_LCM.io_.writer.Writer.start", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.writer.Writer.start", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.prepare_data.iterate_batch", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.reader.Reader.close", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.reader.Reader.close", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.open", "home.repos.pwc.inspect_result.Jivnesh_LCM.models.parsing_gating.Gating.forward", "home.repos.pwc.inspect_result.Jivnesh_LCM.models.parsing_gating.BiAffine_Parser_Decoder.decode", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.writer.Writer.write", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.writer.Writer.write"], ["", "def", "write_results", "(", "args", ",", "data", ",", "data_domain", ",", "split", ",", "model", ",", "model_domain", ",", "eval_dict", ")", ":", "\n", "    ", "str_file", "=", "args", ".", "full_model_name", "+", "'_'", "+", "split", "+", "'_model_domain_'", "+", "model_domain", "+", "'_data_domain_'", "+", "data_domain", "\n", "res_filename", "=", "str_file", "+", "'_res.txt'", "\n", "pred_filename", "=", "str_file", "+", "'_pred.txt'", "\n", "gold_filename", "=", "str_file", "+", "'_gold.txt'", "\n", "\n", "# save results dictionary into a file", "\n", "with", "open", "(", "res_filename", ",", "'w'", ")", "as", "f", ":", "\n", "        ", "json", ".", "dump", "(", "eval_dict", ",", "f", ")", "\n", "\n", "# save predictions and gold labels into files", "\n", "", "pred_writer", "=", "Writer", "(", "args", ".", "alphabets", ")", "\n", "gold_writer", "=", "Writer", "(", "args", ".", "alphabets", ")", "\n", "pred_writer", ".", "start", "(", "pred_filename", ")", "\n", "gold_writer", ".", "start", "(", "gold_filename", ")", "\n", "for", "batch", "in", "prepare_data", ".", "iterate_batch", "(", "data", ",", "args", ".", "batch_size", ",", "args", ".", "device", ")", ":", "\n", "        ", "word", ",", "char", ",", "pos", ",", "ner", ",", "heads", ",", "arc_tags", ",", "auto_label", ",", "masks", ",", "lengths", "=", "batch", "\n", "output", ",", "masks", ",", "lengths", "=", "model", ".", "forward", "(", "word", ",", "char", ",", "pos", ",", "mask", "=", "masks", ",", "length", "=", "lengths", ")", "\n", "auto_label_preds", "=", "model", ".", "decode", "(", "output", ",", "mask", "=", "masks", ",", "length", "=", "lengths", ",", "\n", "leading_symbolic", "=", "prepare_data", ".", "NUM_SYMBOLIC_TAGS", ")", "\n", "lengths", "=", "lengths", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "word", "=", "word", ".", "data", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "pos", "=", "pos", ".", "data", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "ner", "=", "ner", ".", "data", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "heads", "=", "heads", ".", "data", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "arc_tags", "=", "arc_tags", ".", "data", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "auto_label_preds", "=", "auto_label_preds", ".", "data", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "# writing predictions", "\n", "pred_writer", ".", "write", "(", "word", ",", "pos", ",", "ner", ",", "heads", ",", "arc_tags", ",", "lengths", ",", "auto_label", "=", "auto_label_preds", ",", "symbolic_root", "=", "True", ")", "\n", "# writing gold labels", "\n", "gold_writer", ".", "write", "(", "word", ",", "pos", ",", "ner", ",", "heads", ",", "arc_tags", ",", "lengths", ",", "auto_label", "=", "auto_label", ",", "symbolic_root", "=", "True", ")", "\n", "\n", "", "pred_writer", ".", "close", "(", ")", "\n", "gold_writer", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.examples.SequenceTagger.main": [[487, 594], ["logger.info", "SequenceTagger.read_arguments", "logger.info", "logger.info", "sum", "SequenceTagger.build_model_and_optimizer", "copy.deepcopy", "copy.deepcopy", "logger.info", "logger.info", "print", "logger.info", "logger.info", "logger.info", "logger.info", "print", "utils.io_.prepare_data.read_data_to_variable", "logger.info", "utils.io_.prepare_data.calc_num_batches", "range", "logger.info", "print", "model.train", "utils.io_.prepare_data.iterate_batch_rand_bucket_choosing", "time.time", "enumerate", "print", "print", "SequenceTagger.in_domain_evaluation", "SequenceTagger.print_results", "print", "SequenceTagger.evaluation", "generate_optimizer.zero_grad", "model.forward", "model.loss", "model.loss.backward", "torch.nn.utils.clip_grad_norm_", "generate_optimizer.step", "open", "f.write", "SequenceTagger.in_domain_evaluation", "sys.stdout.write", "sys.stdout.write", "sys.stdout.flush", "SequenceTagger.generate_optimizer", "print", "masks.data.sum", "word.size", "model.loss.item", "model.parameters", "sys.stdout.write", "sys.stdout.write", "sys.stdout.flush", "model.parameters", "time.time", "str", "time.time"], "function", ["home.repos.pwc.inspect_result.Jivnesh_LCM.examples.GraphParser.read_arguments", "home.repos.pwc.inspect_result.Jivnesh_LCM.examples.GraphParser.build_model_and_optimizer", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.prepare_data.read_data_to_variable", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.prepare_data.calc_num_batches", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.prepare_data.iterate_batch_rand_bucket_choosing", "home.repos.pwc.inspect_result.Jivnesh_LCM.examples.GraphParser.in_domain_evaluation", "home.repos.pwc.inspect_result.Jivnesh_LCM.examples.GraphParser.print_results", "home.repos.pwc.inspect_result.Jivnesh_LCM.examples.GraphParser.evaluation", "home.repos.pwc.inspect_result.Jivnesh_LCM.models.parsing_gating.Gating.forward", "home.repos.pwc.inspect_result.Jivnesh_LCM.models.parsing_gating.BiAffine_Parser_Decoder.loss", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.open", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.writer.Writer.write", "home.repos.pwc.inspect_result.Jivnesh_LCM.examples.GraphParser.in_domain_evaluation", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.writer.Writer.write", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.writer.Writer.write", "home.repos.pwc.inspect_result.Jivnesh_LCM.examples.GraphParser.generate_optimizer", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.size", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.writer.Writer.write", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.writer.Writer.write"], ["", "def", "main", "(", ")", ":", "\n", "    ", "logger", ".", "info", "(", "\"Reading and creating arguments\"", ")", "\n", "args", "=", "read_arguments", "(", ")", "\n", "logger", ".", "info", "(", "\"Reading Data\"", ")", "\n", "datasets", "=", "{", "}", "\n", "for", "split", "in", "args", ".", "splits", ":", "\n", "        ", "dataset", "=", "prepare_data", ".", "read_data_to_variable", "(", "args", ".", "data_paths", "[", "split", "]", ",", "args", ".", "alphabets", ",", "args", ".", "device", ",", "\n", "symbolic_root", "=", "True", ")", "\n", "datasets", "[", "split", "]", "=", "dataset", "\n", "\n", "", "logger", ".", "info", "(", "\"Creating Networks\"", ")", "\n", "num_data", "=", "sum", "(", "datasets", "[", "'train'", "]", "[", "1", "]", ")", "\n", "model", ",", "optimizer", ",", "dev_eval_dict", ",", "test_eval_dict", ",", "start_epoch", "=", "build_model_and_optimizer", "(", "args", ")", "\n", "best_model", "=", "deepcopy", "(", "model", ")", "\n", "best_optimizer", "=", "deepcopy", "(", "optimizer", ")", "\n", "logger", ".", "info", "(", "'Training INFO of in domain %s'", "%", "args", ".", "domain", ")", "\n", "logger", ".", "info", "(", "'Training on Dependecy Parsing'", ")", "\n", "print", "(", "model", ")", "\n", "logger", ".", "info", "(", "\"train: gamma: %f, batch: %d, clip: %.2f, unk replace: %.2f\"", "%", "(", "args", ".", "gamma", ",", "args", ".", "batch_size", ",", "args", ".", "clip", ",", "args", ".", "unk_replace", ")", ")", "\n", "logger", ".", "info", "(", "'number of training samples for %s is: %d'", "%", "(", "args", ".", "domain", ",", "num_data", ")", ")", "\n", "logger", ".", "info", "(", "\"dropout(in, out, rnn): (%.2f, %.2f, %s)\"", "%", "(", "args", ".", "p_in", ",", "args", ".", "p_out", ",", "args", ".", "p_rnn", ")", ")", "\n", "logger", ".", "info", "(", "\"num_epochs: %d\"", "%", "(", "args", ".", "num_epochs", ")", ")", "\n", "print", "(", "'\\n'", ")", "\n", "\n", "if", "not", "args", ".", "eval_mode", ":", "\n", "        ", "logger", ".", "info", "(", "\"Training\"", ")", "\n", "num_batches", "=", "prepare_data", ".", "calc_num_batches", "(", "datasets", "[", "'train'", "]", ",", "args", ".", "batch_size", ")", "\n", "lr", "=", "args", ".", "learning_rate", "\n", "patient", "=", "0", "\n", "terminal_patient", "=", "0", "\n", "decay", "=", "0", "\n", "for", "epoch", "in", "range", "(", "start_epoch", "+", "1", ",", "args", ".", "num_epochs", "+", "1", ")", ":", "\n", "            ", "print", "(", "'Epoch %d (Training: rnn mode: %s, optimizer: %s, learning rate=%.6f, eps=%.1e, decay rate=%.2f (schedule=%d, decay=%d)): '", "%", "(", "\n", "epoch", ",", "args", ".", "rnn_mode", ",", "args", ".", "opt", ",", "lr", ",", "args", ".", "epsilon", ",", "args", ".", "decay_rate", ",", "args", ".", "schedule", ",", "decay", ")", ")", "\n", "model", ".", "train", "(", ")", "\n", "total_loss", "=", "0.0", "\n", "total_train_inst", "=", "0.0", "\n", "\n", "iter", "=", "prepare_data", ".", "iterate_batch_rand_bucket_choosing", "(", "\n", "datasets", "[", "'train'", "]", ",", "args", ".", "batch_size", ",", "args", ".", "device", ",", "unk_replace", "=", "args", ".", "unk_replace", ")", "\n", "start_time", "=", "time", ".", "time", "(", ")", "\n", "batch_num", "=", "0", "\n", "for", "batch_num", ",", "batch", "in", "enumerate", "(", "iter", ")", ":", "\n", "                ", "batch_num", "=", "batch_num", "+", "1", "\n", "optimizer", ".", "zero_grad", "(", ")", "\n", "# compute loss of main task", "\n", "word", ",", "char", ",", "pos", ",", "ner_tags", ",", "heads", ",", "arc_tags", ",", "auto_label", ",", "masks", ",", "lengths", "=", "batch", "\n", "output", ",", "masks", ",", "lengths", "=", "model", ".", "forward", "(", "word", ",", "char", ",", "pos", ",", "mask", "=", "masks", ",", "length", "=", "lengths", ")", "\n", "loss", "=", "model", ".", "loss", "(", "output", ",", "auto_label", ",", "mask", "=", "masks", ",", "length", "=", "lengths", ")", "\n", "\n", "# update losses", "\n", "num_insts", "=", "masks", ".", "data", ".", "sum", "(", ")", "-", "word", ".", "size", "(", "0", ")", "\n", "total_loss", "+=", "loss", ".", "item", "(", ")", "*", "num_insts", "\n", "total_train_inst", "+=", "num_insts", "\n", "# optimize parameters", "\n", "loss", ".", "backward", "(", ")", "\n", "clip_grad_norm_", "(", "model", ".", "parameters", "(", ")", ",", "args", ".", "clip", ")", "\n", "optimizer", ".", "step", "(", ")", "\n", "\n", "time_ave", "=", "(", "time", ".", "time", "(", ")", "-", "start_time", ")", "/", "batch_num", "\n", "time_left", "=", "(", "num_batches", "-", "batch_num", ")", "*", "time_ave", "\n", "\n", "# update log", "\n", "if", "batch_num", "%", "50", "==", "0", ":", "\n", "                    ", "log_info", "=", "'train: %d/%d, domain: %s, total loss: %.2f, time left: %.2fs'", "%", "(", "batch_num", ",", "num_batches", ",", "args", ".", "domain", ",", "total_loss", "/", "total_train_inst", ",", "time_left", ")", "\n", "sys", ".", "stdout", ".", "write", "(", "log_info", ")", "\n", "sys", ".", "stdout", ".", "write", "(", "'\\n'", ")", "\n", "sys", ".", "stdout", ".", "flush", "(", ")", "\n", "", "", "print", "(", "'\\n'", ")", "\n", "print", "(", "'train: %d/%d, domain: %s, total_loss: %.2f, time: %.2fs'", "%", "\n", "(", "batch_num", ",", "num_batches", ",", "args", ".", "domain", ",", "total_loss", "/", "total_train_inst", ",", "time", ".", "time", "(", ")", "-", "start_time", ")", ")", "\n", "\n", "dev_eval_dict", ",", "test_eval_dict", ",", "best_model", ",", "best_optimizer", ",", "patient", ",", "curr_dev_eval_dict", "=", "in_domain_evaluation", "(", "args", ",", "datasets", ",", "model", ",", "optimizer", ",", "dev_eval_dict", ",", "test_eval_dict", ",", "epoch", ",", "best_model", ",", "best_optimizer", ",", "patient", ")", "\n", "store", "=", "{", "'dev_eval_dict'", ":", "curr_dev_eval_dict", "}", "\n", "############################################# ", "\n", "str_file", "=", "args", ".", "full_model_name", "+", "'_'", "+", "'all_epochs'", "\n", "with", "open", "(", "str_file", ",", "'a'", ")", "as", "f", ":", "\n", "                ", "f", ".", "write", "(", "str", "(", "store", ")", "+", "'\\n'", ")", "\n", "", "if", "patient", "==", "0", ":", "\n", "                ", "terminal_patient", "=", "0", "\n", "", "else", ":", "\n", "                ", "terminal_patient", "+=", "1", "\n", "", "if", "terminal_patient", ">=", "4", "*", "args", ".", "schedule", ":", "\n", "# Save best model and terminate learning", "\n", "                ", "cur_epoch", "=", "epoch", "\n", "epoch", "=", "args", ".", "num_epochs", "\n", "in_domain_evaluation", "(", "args", ",", "datasets", ",", "model", ",", "optimizer", ",", "dev_eval_dict", ",", "test_eval_dict", ",", "epoch", ",", "best_model", ",", "\n", "best_optimizer", ",", "patient", ")", "\n", "log_info", "=", "'Terminating training in epoch %d'", "%", "(", "cur_epoch", ")", "\n", "sys", ".", "stdout", ".", "write", "(", "log_info", ")", "\n", "sys", ".", "stdout", ".", "write", "(", "'\\n'", ")", "\n", "sys", ".", "stdout", ".", "flush", "(", ")", "\n", "return", "\n", "", "if", "patient", ">=", "args", ".", "schedule", ":", "\n", "                ", "lr", "=", "args", ".", "learning_rate", "/", "(", "1.0", "+", "epoch", "*", "args", ".", "decay_rate", ")", "\n", "optimizer", "=", "generate_optimizer", "(", "args", ",", "lr", ",", "model", ".", "parameters", "(", ")", ")", "\n", "print", "(", "'updated learning rate to %.6f'", "%", "lr", ")", "\n", "patient", "=", "0", "\n", "", "print_results", "(", "test_eval_dict", "[", "'in_domain'", "]", ",", "'test'", ",", "args", ".", "domain", ",", "'best_results'", ")", "\n", "print", "(", "'\\n'", ")", "\n", "\n", "", "", "else", ":", "\n", "        ", "logger", ".", "info", "(", "\"Evaluating\"", ")", "\n", "epoch", "=", "start_epoch", "\n", "for", "split", "in", "[", "'train'", ",", "'dev'", ",", "'test'", "]", ":", "\n", "            ", "evaluation", "(", "args", ",", "datasets", "[", "split", "]", ",", "split", ",", "model", ",", "args", ".", "domain", ",", "epoch", ",", "'best_results'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.examples.GraphParser.read_arguments": [[28, 198], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "os.path.join", "tuple", "tuple", "os.path.join", "os.path.join", "torch.device", "torch.device", "logger.info", "GraphParser.save_args", "logger.info", "GraphParser.creating_alphabets", "collections.namedtuple", "collections.namedtuple.", "os.path.exists", "os.makedirs", "set", "logger.info", "utils.load_word_embeddings.load_embedding_dict", "utils.load_word_embeddings.load_embedding_dict", "utils.load_word_embeddings.load_embedding_dict", "args_dict.keys", "len", "torch.cuda.is_available", "torch.cuda.is_available", "len", "args_dict[].split"], "function", ["home.repos.pwc.inspect_result.Jivnesh_LCM.examples.GraphParser.save_args", "home.repos.pwc.inspect_result.Jivnesh_LCM.examples.GraphParser.creating_alphabets", "home.repos.pwc.inspect_result.Jivnesh_LCM.utils.load_word_embeddings.load_embedding_dict", "home.repos.pwc.inspect_result.Jivnesh_LCM.utils.load_word_embeddings.load_embedding_dict", "home.repos.pwc.inspect_result.Jivnesh_LCM.utils.load_word_embeddings.load_embedding_dict", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.keys"], ["def", "read_arguments", "(", ")", ":", "\n", "    ", "args_", "=", "argparse", ".", "ArgumentParser", "(", "description", "=", "'Sovling GraphParser'", ")", "\n", "args_", ".", "add_argument", "(", "'--dataset'", ",", "choices", "=", "[", "'ontonotes'", ",", "'ud'", "]", ",", "help", "=", "'Dataset'", ",", "required", "=", "True", ")", "\n", "args_", ".", "add_argument", "(", "'--domain'", ",", "help", "=", "'domain/language'", ",", "required", "=", "True", ")", "\n", "args_", ".", "add_argument", "(", "'--rnn_mode'", ",", "choices", "=", "[", "'RNN'", ",", "'LSTM'", ",", "'GRU'", "]", ",", "help", "=", "'architecture of rnn'", ",", "\n", "required", "=", "True", ")", "\n", "args_", ".", "add_argument", "(", "'--gating'", ",", "action", "=", "'store_true'", ",", "help", "=", "'use gated mechanism'", ")", "\n", "args_", ".", "add_argument", "(", "'--num_gates'", ",", "type", "=", "int", ",", "default", "=", "0", ",", "help", "=", "'number of gates for gating mechanism'", ")", "\n", "args_", ".", "add_argument", "(", "'--num_epochs'", ",", "type", "=", "int", ",", "default", "=", "200", ",", "help", "=", "'Number of training epochs'", ")", "\n", "args_", ".", "add_argument", "(", "'--batch_size'", ",", "type", "=", "int", ",", "default", "=", "64", ",", "help", "=", "'Number of sentences in each batch'", ")", "\n", "args_", ".", "add_argument", "(", "'--hidden_size'", ",", "type", "=", "int", ",", "default", "=", "256", ",", "help", "=", "'Number of hidden units in RNN'", ")", "\n", "args_", ".", "add_argument", "(", "'--arc_space'", ",", "type", "=", "int", ",", "default", "=", "128", ",", "help", "=", "'Dimension of tag space'", ")", "\n", "args_", ".", "add_argument", "(", "'--arc_tag_space'", ",", "type", "=", "int", ",", "default", "=", "128", ",", "help", "=", "'Dimension of tag space'", ")", "\n", "args_", ".", "add_argument", "(", "'--num_layers'", ",", "type", "=", "int", ",", "default", "=", "1", ",", "help", "=", "'Number of layers of RNN'", ")", "\n", "args_", ".", "add_argument", "(", "'--num_filters'", ",", "type", "=", "int", ",", "default", "=", "50", ",", "help", "=", "'Number of filters in CNN'", ")", "\n", "args_", ".", "add_argument", "(", "'--kernel_size'", ",", "type", "=", "int", ",", "default", "=", "3", ",", "help", "=", "'Size of Kernel for CNN'", ")", "\n", "args_", ".", "add_argument", "(", "'--use_pos'", ",", "action", "=", "'store_true'", ",", "help", "=", "'use part-of-speech embedding.'", ")", "\n", "args_", ".", "add_argument", "(", "'--use_char'", ",", "action", "=", "'store_true'", ",", "help", "=", "'use character embedding and CNN.'", ")", "\n", "args_", ".", "add_argument", "(", "'--word_dim'", ",", "type", "=", "int", ",", "default", "=", "300", ",", "help", "=", "'Dimension of word embeddings'", ")", "\n", "args_", ".", "add_argument", "(", "'--pos_dim'", ",", "type", "=", "int", ",", "default", "=", "50", ",", "help", "=", "'Dimension of POS embeddings'", ")", "\n", "args_", ".", "add_argument", "(", "'--char_dim'", ",", "type", "=", "int", ",", "default", "=", "50", ",", "help", "=", "'Dimension of Character embeddings'", ")", "\n", "args_", ".", "add_argument", "(", "'--initializer'", ",", "choices", "=", "[", "'xavier'", "]", ",", "help", "=", "'initialize model parameters'", ")", "\n", "args_", ".", "add_argument", "(", "'--opt'", ",", "choices", "=", "[", "'adam'", ",", "'sgd'", "]", ",", "help", "=", "'optimization algorithm'", ")", "\n", "args_", ".", "add_argument", "(", "'--momentum'", ",", "type", "=", "float", ",", "default", "=", "0.9", ",", "help", "=", "'momentum of optimizer'", ")", "\n", "args_", ".", "add_argument", "(", "'--betas'", ",", "nargs", "=", "2", ",", "type", "=", "float", ",", "default", "=", "[", "0.9", ",", "0.9", "]", ",", "help", "=", "'betas of optimizer'", ")", "\n", "args_", ".", "add_argument", "(", "'--learning_rate'", ",", "type", "=", "float", ",", "default", "=", "0.01", ",", "help", "=", "'Learning rate'", ")", "\n", "args_", ".", "add_argument", "(", "'--decay_rate'", ",", "type", "=", "float", ",", "default", "=", "0.05", ",", "help", "=", "'Decay rate of learning rate'", ")", "\n", "args_", ".", "add_argument", "(", "'--schedule'", ",", "type", "=", "int", ",", "help", "=", "'schedule for learning rate decay'", ")", "\n", "args_", ".", "add_argument", "(", "'--clip'", ",", "type", "=", "float", ",", "default", "=", "5.0", ",", "help", "=", "'gradient clipping'", ")", "\n", "args_", ".", "add_argument", "(", "'--gamma'", ",", "type", "=", "float", ",", "default", "=", "0.0", ",", "help", "=", "'weight for regularization'", ")", "\n", "args_", ".", "add_argument", "(", "'--epsilon'", ",", "type", "=", "float", ",", "default", "=", "1e-8", ",", "help", "=", "'epsilon for adam'", ")", "\n", "args_", ".", "add_argument", "(", "'--p_rnn'", ",", "nargs", "=", "2", ",", "type", "=", "float", ",", "required", "=", "True", ",", "help", "=", "'dropout rate for RNN'", ")", "\n", "args_", ".", "add_argument", "(", "'--p_in'", ",", "type", "=", "float", ",", "default", "=", "0.33", ",", "help", "=", "'dropout rate for input embeddings'", ")", "\n", "args_", ".", "add_argument", "(", "'--p_out'", ",", "type", "=", "float", ",", "default", "=", "0.33", ",", "help", "=", "'dropout rate for output layer'", ")", "\n", "args_", ".", "add_argument", "(", "'--arc_decode'", ",", "choices", "=", "[", "'mst'", ",", "'greedy'", "]", ",", "help", "=", "'arc decoding algorithm'", ",", "required", "=", "True", ")", "\n", "args_", ".", "add_argument", "(", "'--unk_replace'", ",", "type", "=", "float", ",", "default", "=", "0.", ",", "\n", "help", "=", "'The rate to replace a singleton word with UNK'", ")", "\n", "args_", ".", "add_argument", "(", "'--punct_set'", ",", "nargs", "=", "'+'", ",", "type", "=", "str", ",", "help", "=", "'List of punctuations'", ")", "\n", "args_", ".", "add_argument", "(", "'--word_embedding'", ",", "choices", "=", "[", "'random'", ",", "'glove'", ",", "'fasttext'", ",", "'word2vec'", "]", ",", "\n", "help", "=", "'Embedding for words'", ")", "\n", "args_", ".", "add_argument", "(", "'--word_path'", ",", "help", "=", "'path for word embedding dict - in case word_embedding is not random'", ")", "\n", "args_", ".", "add_argument", "(", "'--freeze_word_embeddings'", ",", "action", "=", "'store_true'", ",", "help", "=", "'frozen the word embedding (disable fine-tuning).'", ")", "\n", "args_", ".", "add_argument", "(", "'--freeze_sequence_taggers'", ",", "action", "=", "'store_true'", ",", "help", "=", "'frozen the BiLSTMs of the pre-trained taggers.'", ")", "\n", "args_", ".", "add_argument", "(", "'--char_embedding'", ",", "choices", "=", "[", "'random'", ",", "'hellwig'", "]", ",", "help", "=", "'Embedding for characters'", ",", "\n", "required", "=", "True", ")", "\n", "args_", ".", "add_argument", "(", "'--pos_embedding'", ",", "choices", "=", "[", "'random'", ",", "'one_hot'", "]", ",", "help", "=", "'Embedding for pos'", ",", "\n", "required", "=", "True", ")", "\n", "args_", ".", "add_argument", "(", "'--char_path'", ",", "help", "=", "'path for character embedding dict'", ")", "\n", "args_", ".", "add_argument", "(", "'--pos_path'", ",", "help", "=", "'path for pos embedding dict'", ")", "\n", "args_", ".", "add_argument", "(", "'--set_num_training_samples'", ",", "type", "=", "int", ",", "help", "=", "'downsampling training set to a fixed number of samples'", ")", "\n", "args_", ".", "add_argument", "(", "'--model_path'", ",", "help", "=", "'path for saving model file.'", ",", "required", "=", "True", ")", "\n", "args_", ".", "add_argument", "(", "'--load_path'", ",", "help", "=", "'path for loading saved source model file.'", ",", "default", "=", "None", ")", "\n", "args_", ".", "add_argument", "(", "'--load_sequence_taggers_paths'", ",", "nargs", "=", "'+'", ",", "help", "=", "'path for loading saved sequence_tagger saved_models files.'", ",", "default", "=", "None", ")", "\n", "args_", ".", "add_argument", "(", "'--strict'", ",", "action", "=", "'store_true'", ",", "help", "=", "'if True loaded model state should contin '", "\n", "'exactly the same keys as current model'", ")", "\n", "args_", ".", "add_argument", "(", "'--eval_mode'", ",", "action", "=", "'store_true'", ",", "help", "=", "'evaluating model without training it'", ")", "\n", "args_", ".", "add_argument", "(", "'--eval_with_CI'", ",", "action", "=", "'store_true'", ",", "help", "=", "'evaluating model in constrained inference mode'", ")", "\n", "args_", ".", "add_argument", "(", "'--LCM_Path_flag'", ",", "action", "=", "'store_true'", ",", "help", "=", "'for constrained inference with LCM, flag is used to change path'", ")", "\n", "args", "=", "args_", ".", "parse_args", "(", ")", "\n", "args_dict", "=", "{", "}", "\n", "args_dict", "[", "'dataset'", "]", "=", "args", ".", "dataset", "\n", "args_dict", "[", "'domain'", "]", "=", "args", ".", "domain", "\n", "args_dict", "[", "'rnn_mode'", "]", "=", "args", ".", "rnn_mode", "\n", "args_dict", "[", "'gating'", "]", "=", "args", ".", "gating", "\n", "args_dict", "[", "'num_gates'", "]", "=", "args", ".", "num_gates", "\n", "args_dict", "[", "'arc_decode'", "]", "=", "args", ".", "arc_decode", "\n", "# args_dict['splits'] = ['train', 'dev', 'test']", "\n", "args_dict", "[", "'splits'", "]", "=", "[", "'train'", ",", "'dev'", ",", "'test'", "]", "\n", "args_dict", "[", "'model_path'", "]", "=", "args", ".", "model_path", "\n", "if", "not", "path", ".", "exists", "(", "args_dict", "[", "'model_path'", "]", ")", ":", "\n", "        ", "makedirs", "(", "args_dict", "[", "'model_path'", "]", ")", "\n", "", "args_dict", "[", "'data_paths'", "]", "=", "{", "}", "\n", "if", "args_dict", "[", "'dataset'", "]", "==", "'ontonotes'", ":", "\n", "        ", "data_path", "=", "'data/onto_pos_ner_dp'", "\n", "", "else", ":", "\n", "        ", "data_path", "=", "'data/ud_pos_ner_dp'", "\n", "", "for", "split", "in", "args_dict", "[", "'splits'", "]", ":", "\n", "        ", "args_dict", "[", "'data_paths'", "]", "[", "split", "]", "=", "data_path", "+", "'_'", "+", "split", "+", "'_'", "+", "args_dict", "[", "'domain'", "]", "\n", "###################################    ", "\n", "\n", "###################################", "\n", "", "args_dict", "[", "'alphabet_data_paths'", "]", "=", "{", "}", "\n", "for", "split", "in", "args_dict", "[", "'splits'", "]", ":", "\n", "        ", "if", "args_dict", "[", "'dataset'", "]", "==", "'ontonotes'", ":", "\n", "            ", "args_dict", "[", "'alphabet_data_paths'", "]", "[", "split", "]", "=", "data_path", "+", "'_'", "+", "split", "+", "'_'", "+", "'all'", "\n", "", "else", ":", "\n", "            ", "if", "'_'", "in", "args_dict", "[", "'domain'", "]", ":", "\n", "                ", "args_dict", "[", "'alphabet_data_paths'", "]", "[", "split", "]", "=", "data_path", "+", "'_'", "+", "split", "+", "'_'", "+", "args_dict", "[", "'domain'", "]", ".", "split", "(", "'_'", ")", "[", "0", "]", "\n", "", "else", ":", "\n", "                ", "args_dict", "[", "'alphabet_data_paths'", "]", "[", "split", "]", "=", "args_dict", "[", "'data_paths'", "]", "[", "split", "]", "\n", "", "", "", "args_dict", "[", "'model_name'", "]", "=", "'domain_'", "+", "args_dict", "[", "'domain'", "]", "\n", "args_dict", "[", "'full_model_name'", "]", "=", "path", ".", "join", "(", "args_dict", "[", "'model_path'", "]", ",", "args_dict", "[", "'model_name'", "]", ")", "\n", "args_dict", "[", "'load_path'", "]", "=", "args", ".", "load_path", "\n", "args_dict", "[", "'load_sequence_taggers_paths'", "]", "=", "args", ".", "load_sequence_taggers_paths", "\n", "if", "args_dict", "[", "'load_sequence_taggers_paths'", "]", "is", "not", "None", ":", "\n", "        ", "args_dict", "[", "'gating'", "]", "=", "True", "\n", "args_dict", "[", "'num_gates'", "]", "=", "len", "(", "args_dict", "[", "'load_sequence_taggers_paths'", "]", ")", "+", "1", "\n", "", "else", ":", "\n", "        ", "if", "not", "args_dict", "[", "'gating'", "]", ":", "\n", "            ", "args_dict", "[", "'num_gates'", "]", "=", "0", "\n", "", "", "args_dict", "[", "'strict'", "]", "=", "args", ".", "strict", "\n", "args_dict", "[", "'num_epochs'", "]", "=", "args", ".", "num_epochs", "\n", "args_dict", "[", "'batch_size'", "]", "=", "args", ".", "batch_size", "\n", "args_dict", "[", "'hidden_size'", "]", "=", "args", ".", "hidden_size", "\n", "args_dict", "[", "'arc_space'", "]", "=", "args", ".", "arc_space", "\n", "args_dict", "[", "'arc_tag_space'", "]", "=", "args", ".", "arc_tag_space", "\n", "args_dict", "[", "'num_layers'", "]", "=", "args", ".", "num_layers", "\n", "args_dict", "[", "'num_filters'", "]", "=", "args", ".", "num_filters", "\n", "args_dict", "[", "'kernel_size'", "]", "=", "args", ".", "kernel_size", "\n", "args_dict", "[", "'learning_rate'", "]", "=", "args", ".", "learning_rate", "\n", "args_dict", "[", "'initializer'", "]", "=", "nn", ".", "init", ".", "xavier_uniform_", "if", "args", ".", "initializer", "==", "'xavier'", "else", "None", "\n", "args_dict", "[", "'opt'", "]", "=", "args", ".", "opt", "\n", "args_dict", "[", "'momentum'", "]", "=", "args", ".", "momentum", "\n", "args_dict", "[", "'betas'", "]", "=", "tuple", "(", "args", ".", "betas", ")", "\n", "args_dict", "[", "'epsilon'", "]", "=", "args", ".", "epsilon", "\n", "args_dict", "[", "'decay_rate'", "]", "=", "args", ".", "decay_rate", "\n", "args_dict", "[", "'clip'", "]", "=", "args", ".", "clip", "\n", "args_dict", "[", "'gamma'", "]", "=", "args", ".", "gamma", "\n", "args_dict", "[", "'schedule'", "]", "=", "args", ".", "schedule", "\n", "args_dict", "[", "'p_rnn'", "]", "=", "tuple", "(", "args", ".", "p_rnn", ")", "\n", "args_dict", "[", "'p_in'", "]", "=", "args", ".", "p_in", "\n", "args_dict", "[", "'p_out'", "]", "=", "args", ".", "p_out", "\n", "args_dict", "[", "'unk_replace'", "]", "=", "args", ".", "unk_replace", "\n", "args_dict", "[", "'set_num_training_samples'", "]", "=", "args", ".", "set_num_training_samples", "\n", "args_dict", "[", "'punct_set'", "]", "=", "None", "\n", "if", "args", ".", "punct_set", "is", "not", "None", ":", "\n", "        ", "args_dict", "[", "'punct_set'", "]", "=", "set", "(", "args", ".", "punct_set", ")", "\n", "logger", ".", "info", "(", "\"punctuations(%d): %s\"", "%", "(", "len", "(", "args_dict", "[", "'punct_set'", "]", ")", ",", "' '", ".", "join", "(", "args_dict", "[", "'punct_set'", "]", ")", ")", ")", "\n", "", "args_dict", "[", "'freeze_word_embeddings'", "]", "=", "args", ".", "freeze_word_embeddings", "\n", "args_dict", "[", "'freeze_sequence_taggers'", "]", "=", "args", ".", "freeze_sequence_taggers", "\n", "args_dict", "[", "'word_embedding'", "]", "=", "args", ".", "word_embedding", "\n", "args_dict", "[", "'word_path'", "]", "=", "args", ".", "word_path", "\n", "args_dict", "[", "'use_char'", "]", "=", "args", ".", "use_char", "\n", "args_dict", "[", "'char_embedding'", "]", "=", "args", ".", "char_embedding", "\n", "args_dict", "[", "'char_path'", "]", "=", "args", ".", "char_path", "\n", "args_dict", "[", "'pos_embedding'", "]", "=", "args", ".", "pos_embedding", "\n", "args_dict", "[", "'pos_path'", "]", "=", "args", ".", "pos_path", "\n", "args_dict", "[", "'use_pos'", "]", "=", "args", ".", "use_pos", "\n", "args_dict", "[", "'pos_dim'", "]", "=", "args", ".", "pos_dim", "\n", "args_dict", "[", "'word_dict'", "]", "=", "None", "\n", "args_dict", "[", "'word_dim'", "]", "=", "args", ".", "word_dim", "\n", "if", "args_dict", "[", "'word_embedding'", "]", "!=", "'random'", "and", "args_dict", "[", "'word_path'", "]", ":", "\n", "        ", "args_dict", "[", "'word_dict'", "]", ",", "args_dict", "[", "'word_dim'", "]", "=", "load_word_embeddings", ".", "load_embedding_dict", "(", "args_dict", "[", "'word_embedding'", "]", ",", "\n", "args_dict", "[", "'word_path'", "]", ")", "\n", "", "args_dict", "[", "'char_dict'", "]", "=", "None", "\n", "args_dict", "[", "'char_dim'", "]", "=", "args", ".", "char_dim", "\n", "if", "args_dict", "[", "'char_embedding'", "]", "!=", "'random'", ":", "\n", "        ", "args_dict", "[", "'char_dict'", "]", ",", "args_dict", "[", "'char_dim'", "]", "=", "load_word_embeddings", ".", "load_embedding_dict", "(", "args_dict", "[", "'char_embedding'", "]", ",", "\n", "args_dict", "[", "'char_path'", "]", ")", "\n", "", "args_dict", "[", "'pos_dict'", "]", "=", "None", "\n", "if", "args_dict", "[", "'pos_embedding'", "]", "!=", "'random'", ":", "\n", "        ", "args_dict", "[", "'pos_dict'", "]", ",", "args_dict", "[", "'pos_dim'", "]", "=", "load_word_embeddings", ".", "load_embedding_dict", "(", "args_dict", "[", "'pos_embedding'", "]", ",", "\n", "args_dict", "[", "'pos_path'", "]", ")", "\n", "", "args_dict", "[", "'alphabet_path'", "]", "=", "path", ".", "join", "(", "args_dict", "[", "'model_path'", "]", ",", "'alphabets'", "+", "'_src_domain_'", "+", "args_dict", "[", "'domain'", "]", "+", "'/'", ")", "\n", "args_dict", "[", "'model_name'", "]", "=", "path", ".", "join", "(", "args_dict", "[", "'model_path'", "]", ",", "args_dict", "[", "'model_name'", "]", ")", "\n", "args_dict", "[", "'eval_mode'", "]", "=", "args", ".", "eval_mode", "\n", "args_dict", "[", "'eval_with_CI'", "]", "=", "args", ".", "eval_with_CI", "\n", "args_dict", "[", "'LCM_Path_flag'", "]", "=", "args", ".", "LCM_Path_flag", "\n", "args_dict", "[", "'device'", "]", "=", "torch", ".", "device", "(", "'cuda'", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "else", "'cpu'", ")", "\n", "args_dict", "[", "'word_status'", "]", "=", "'frozen'", "if", "args", ".", "freeze_word_embeddings", "else", "'fine tune'", "\n", "args_dict", "[", "'char_status'", "]", "=", "'enabled'", "if", "args", ".", "use_char", "else", "'disabled'", "\n", "args_dict", "[", "'pos_status'", "]", "=", "'enabled'", "if", "args", ".", "use_pos", "else", "'disabled'", "\n", "logger", ".", "info", "(", "\"Saving arguments to file\"", ")", "\n", "save_args", "(", "args", ",", "args_dict", "[", "'full_model_name'", "]", ")", "\n", "logger", ".", "info", "(", "\"Creating Alphabets\"", ")", "\n", "alphabet_dict", "=", "creating_alphabets", "(", "args_dict", "[", "'alphabet_path'", "]", ",", "args_dict", "[", "'alphabet_data_paths'", "]", ",", "args_dict", "[", "'word_dict'", "]", ")", "\n", "args_dict", "=", "{", "**", "args_dict", ",", "**", "alphabet_dict", "}", "\n", "ARGS", "=", "namedtuple", "(", "'ARGS'", ",", "args_dict", ".", "keys", "(", ")", ")", "\n", "my_args", "=", "ARGS", "(", "**", "args_dict", ")", "\n", "return", "my_args", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.examples.GraphParser.creating_alphabets": [[200, 214], ["utils.io_.prepare_data.create_alphabets", "alphabet_dict[].items", "v.size", "logger.info", "alphabet_data_paths.items", "k.split"], "function", ["home.repos.pwc.inspect_result.Jivnesh_LCM.io_.prepare_data.create_alphabets", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.items", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.size", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.items"], ["", "def", "creating_alphabets", "(", "alphabet_path", ",", "alphabet_data_paths", ",", "word_dict", ")", ":", "\n", "    ", "train_paths", "=", "alphabet_data_paths", "[", "'train'", "]", "\n", "extra_paths", "=", "[", "v", "for", "k", ",", "v", "in", "alphabet_data_paths", ".", "items", "(", ")", "if", "k", "!=", "'train'", "]", "\n", "alphabet_dict", "=", "{", "}", "\n", "alphabet_dict", "[", "'alphabets'", "]", "=", "prepare_data", ".", "create_alphabets", "(", "alphabet_path", ",", "\n", "train_paths", ",", "\n", "extra_paths", "=", "extra_paths", ",", "\n", "max_vocabulary_size", "=", "100000", ",", "\n", "embedd_dict", "=", "word_dict", ")", "\n", "for", "k", ",", "v", "in", "alphabet_dict", "[", "'alphabets'", "]", ".", "items", "(", ")", ":", "\n", "        ", "num_key", "=", "'num_'", "+", "k", ".", "split", "(", "'_'", ")", "[", "0", "]", "\n", "alphabet_dict", "[", "num_key", "]", "=", "v", ".", "size", "(", ")", "\n", "logger", ".", "info", "(", "\"%s : %d\"", "%", "(", "num_key", ",", "alphabet_dict", "[", "num_key", "]", ")", ")", "\n", "", "return", "alphabet_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.examples.GraphParser.construct_embedding_table": [[215, 234], ["numpy.sqrt", "numpy.empty", "numpy.random.uniform().astype", "alphabet.items", "print", "torch.from_numpy", "torch.from_numpy", "alphabet.size", "numpy.random.uniform", "token.lower", "numpy.random.uniform().astype", "token.lower", "numpy.random.uniform"], "function", ["home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.items", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.size"], ["", "def", "construct_embedding_table", "(", "alphabet", ",", "tokens_dict", ",", "dim", ",", "token_type", "=", "'word'", ")", ":", "\n", "    ", "if", "tokens_dict", "is", "None", ":", "\n", "        ", "return", "None", "\n", "", "scale", "=", "np", ".", "sqrt", "(", "3.0", "/", "dim", ")", "\n", "table", "=", "np", ".", "empty", "(", "[", "alphabet", ".", "size", "(", ")", ",", "dim", "]", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "table", "[", "prepare_data", ".", "UNK_ID", ",", ":", "]", "=", "np", ".", "random", ".", "uniform", "(", "-", "scale", ",", "scale", ",", "[", "1", ",", "dim", "]", ")", ".", "astype", "(", "np", ".", "float32", ")", "\n", "oov_tokens", "=", "0", "\n", "for", "token", ",", "index", "in", "alphabet", ".", "items", "(", ")", ":", "\n", "        ", "if", "token", "in", "tokens_dict", ":", "\n", "            ", "embedding", "=", "tokens_dict", "[", "token", "]", "\n", "", "elif", "token", ".", "lower", "(", ")", "in", "tokens_dict", ":", "\n", "            ", "embedding", "=", "tokens_dict", "[", "token", ".", "lower", "(", ")", "]", "\n", "", "else", ":", "\n", "            ", "embedding", "=", "np", ".", "random", ".", "uniform", "(", "-", "scale", ",", "scale", ",", "[", "1", ",", "dim", "]", ")", ".", "astype", "(", "np", ".", "float32", ")", "\n", "oov_tokens", "+=", "1", "\n", "", "table", "[", "index", ",", ":", "]", "=", "embedding", "\n", "", "print", "(", "'token type : %s, number of oov: %d'", "%", "(", "token_type", ",", "oov_tokens", ")", ")", "\n", "table", "=", "torch", ".", "from_numpy", "(", "table", ")", "\n", "return", "table", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.examples.GraphParser.save_args": [[235, 240], ["vars", "open", "json.dump"], "function", ["home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.open"], ["", "def", "save_args", "(", "args", ",", "full_model_name", ")", ":", "\n", "    ", "arg_path", "=", "full_model_name", "+", "'.arg.json'", "\n", "argparse_dict", "=", "vars", "(", "args", ")", "\n", "with", "open", "(", "arg_path", ",", "'w'", ")", "as", "f", ":", "\n", "        ", "json", ".", "dump", "(", "argparse_dict", ",", "f", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.examples.GraphParser.generate_optimizer": [[241, 249], ["filter", "torch.optim.Adam", "torch.optim.SGD", "ValueError"], "function", ["None"], ["", "", "def", "generate_optimizer", "(", "args", ",", "lr", ",", "params", ")", ":", "\n", "    ", "params", "=", "filter", "(", "lambda", "param", ":", "param", ".", "requires_grad", ",", "params", ")", "\n", "if", "args", ".", "opt", "==", "'adam'", ":", "\n", "        ", "return", "Adam", "(", "params", ",", "lr", "=", "lr", ",", "betas", "=", "args", ".", "betas", ",", "weight_decay", "=", "args", ".", "gamma", ",", "eps", "=", "args", ".", "epsilon", ")", "\n", "", "elif", "args", ".", "opt", "==", "'sgd'", ":", "\n", "        ", "return", "SGD", "(", "params", ",", "lr", "=", "lr", ",", "momentum", "=", "args", ".", "momentum", ",", "weight_decay", "=", "args", ".", "gamma", ",", "nesterov", "=", "True", ")", "\n", "", "else", ":", "\n", "        ", "raise", "ValueError", "(", "'Unknown optimization algorithm: %s'", "%", "args", ".", "opt", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.examples.GraphParser.save_checkpoint": [[251, 260], ["print", "torch.save", "torch.save", "model.state_dict", "optimizer.state_dict"], "function", ["home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.save", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.save"], ["", "", "def", "save_checkpoint", "(", "args", ",", "model", ",", "optimizer", ",", "opt", ",", "dev_eval_dict", ",", "test_eval_dict", ",", "full_model_name", ")", ":", "\n", "    ", "path_name", "=", "full_model_name", "+", "'.pt'", "\n", "print", "(", "'Saving model to: %s'", "%", "path_name", ")", "\n", "state", "=", "{", "'model_state_dict'", ":", "model", ".", "state_dict", "(", ")", ",", "\n", "'optimizer_state_dict'", ":", "optimizer", ".", "state_dict", "(", ")", ",", "\n", "'opt'", ":", "opt", ",", "\n", "'dev_eval_dict'", ":", "dev_eval_dict", ",", "\n", "'test_eval_dict'", ":", "test_eval_dict", "}", "\n", "torch", ".", "save", "(", "state", ",", "path_name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.examples.GraphParser.load_checkpoint": [[261, 280], ["print", "torch.load", "torch.load", "model.load_state_dict", "ValueError", "GraphParser.generate_optimizer", "optimizer.load_state_dict", "optimizer.state.values", "model.parameters", "state.items", "isinstance", "v.to"], "function", ["home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.load", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.load", "home.repos.pwc.inspect_result.Jivnesh_LCM.examples.GraphParser.generate_optimizer", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.values", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.items"], ["", "def", "load_checkpoint", "(", "args", ",", "model", ",", "optimizer", ",", "dev_eval_dict", ",", "test_eval_dict", ",", "start_epoch", ",", "load_path", ",", "strict", "=", "True", ")", ":", "\n", "    ", "print", "(", "'Loading saved model from: %s'", "%", "load_path", ")", "\n", "checkpoint", "=", "torch", ".", "load", "(", "load_path", ",", "map_location", "=", "args", ".", "device", ")", "\n", "if", "checkpoint", "[", "'opt'", "]", "!=", "args", ".", "opt", ":", "\n", "        ", "raise", "ValueError", "(", "'loaded optimizer type is: %s instead of: %s'", "%", "(", "checkpoint", "[", "'opt'", "]", ",", "args", ".", "opt", ")", ")", "\n", "", "model", ".", "load_state_dict", "(", "checkpoint", "[", "'model_state_dict'", "]", ",", "strict", "=", "strict", ")", "\n", "\n", "\n", "if", "strict", ":", "\n", "        ", "generate_optimizer", "(", "args", ",", "args", ".", "learning_rate", ",", "model", ".", "parameters", "(", ")", ")", "\n", "optimizer", ".", "load_state_dict", "(", "checkpoint", "[", "'optimizer_state_dict'", "]", ")", "\n", "for", "state", "in", "optimizer", ".", "state", ".", "values", "(", ")", ":", "\n", "            ", "for", "k", ",", "v", "in", "state", ".", "items", "(", ")", ":", "\n", "                ", "if", "isinstance", "(", "v", ",", "torch", ".", "Tensor", ")", ":", "\n", "                    ", "state", "[", "k", "]", "=", "v", ".", "to", "(", "args", ".", "device", ")", "\n", "", "", "", "dev_eval_dict", "=", "checkpoint", "[", "'dev_eval_dict'", "]", "\n", "test_eval_dict", "=", "checkpoint", "[", "'test_eval_dict'", "]", "\n", "start_epoch", "=", "dev_eval_dict", "[", "'in_domain'", "]", "[", "'epoch'", "]", "\n", "", "return", "model", ",", "optimizer", ",", "dev_eval_dict", ",", "test_eval_dict", ",", "start_epoch", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.examples.GraphParser.build_model_and_optimizer": [[282, 327], ["GraphParser.construct_embedding_table", "GraphParser.construct_embedding_table", "GraphParser.construct_embedding_table", "utils.models.parsing_gating.BiAffine_Parser_Gated", "print", "GraphParser.generate_optimizer", "utils.models.parsing_gating.BiAffine_Parser_Gated.to", "utils.models.parsing_gating.BiAffine_Parser_Gated.parameters", "GraphParser.initialize_eval_dict", "GraphParser.initialize_eval_dict", "GraphParser.load_checkpoint", "utils.models.parsing_gating.BiAffine_Parser_Gated.state_dict", "enumerate", "model.state_dict.update", "utils.models.parsing_gating.BiAffine_Parser_Gated.load_state_dict", "print", "utils.models.parsing_gating.BiAffine_Parser_Gated.named_parameters", "print", "torch.load", "torch.load", "checkpoint[].items", "k.replace", "str"], "function", ["home.repos.pwc.inspect_result.Jivnesh_LCM.examples.GraphParser.construct_embedding_table", "home.repos.pwc.inspect_result.Jivnesh_LCM.examples.GraphParser.construct_embedding_table", "home.repos.pwc.inspect_result.Jivnesh_LCM.examples.GraphParser.construct_embedding_table", "home.repos.pwc.inspect_result.Jivnesh_LCM.examples.GraphParser.generate_optimizer", "home.repos.pwc.inspect_result.Jivnesh_LCM.examples.GraphParser.initialize_eval_dict", "home.repos.pwc.inspect_result.Jivnesh_LCM.examples.GraphParser.initialize_eval_dict", "home.repos.pwc.inspect_result.Jivnesh_LCM.examples.GraphParser.load_checkpoint", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.load", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.load", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.items"], ["", "def", "build_model_and_optimizer", "(", "args", ")", ":", "\n", "    ", "word_table", "=", "construct_embedding_table", "(", "args", ".", "alphabets", "[", "'word_alphabet'", "]", ",", "args", ".", "word_dict", ",", "args", ".", "word_dim", ",", "token_type", "=", "'word'", ")", "\n", "char_table", "=", "construct_embedding_table", "(", "args", ".", "alphabets", "[", "'char_alphabet'", "]", ",", "args", ".", "char_dict", ",", "args", ".", "char_dim", ",", "token_type", "=", "'char'", ")", "\n", "pos_table", "=", "construct_embedding_table", "(", "args", ".", "alphabets", "[", "'pos_alphabet'", "]", ",", "args", ".", "pos_dict", ",", "args", ".", "pos_dim", ",", "token_type", "=", "'pos'", ")", "\n", "model", "=", "BiAffine_Parser_Gated", "(", "args", ".", "word_dim", ",", "args", ".", "num_word", ",", "args", ".", "char_dim", ",", "args", ".", "num_char", ",", "\n", "args", ".", "use_pos", ",", "args", ".", "use_char", ",", "args", ".", "pos_dim", ",", "args", ".", "num_pos", ",", "\n", "args", ".", "num_filters", ",", "args", ".", "kernel_size", ",", "args", ".", "rnn_mode", ",", "\n", "args", ".", "hidden_size", ",", "args", ".", "num_layers", ",", "args", ".", "num_arc", ",", "\n", "args", ".", "arc_space", ",", "args", ".", "arc_tag_space", ",", "args", ".", "num_gates", ",", "\n", "embedd_word", "=", "word_table", ",", "embedd_char", "=", "char_table", ",", "embedd_pos", "=", "pos_table", ",", "\n", "p_in", "=", "args", ".", "p_in", ",", "p_out", "=", "args", ".", "p_out", ",", "p_rnn", "=", "args", ".", "p_rnn", ",", "\n", "biaffine", "=", "True", ",", "arc_decode", "=", "args", ".", "arc_decode", ",", "initializer", "=", "args", ".", "initializer", ")", "\n", "print", "(", "model", ")", "\n", "optimizer", "=", "generate_optimizer", "(", "args", ",", "args", ".", "learning_rate", ",", "model", ".", "parameters", "(", ")", ")", "\n", "start_epoch", "=", "0", "\n", "dev_eval_dict", "=", "{", "'in_domain'", ":", "initialize_eval_dict", "(", ")", "}", "\n", "test_eval_dict", "=", "{", "'in_domain'", ":", "initialize_eval_dict", "(", ")", "}", "\n", "if", "args", ".", "load_path", ":", "\n", "        ", "model", ",", "optimizer", ",", "dev_eval_dict", ",", "test_eval_dict", ",", "start_epoch", "=", "load_checkpoint", "(", "args", ",", "model", ",", "optimizer", ",", "\n", "dev_eval_dict", ",", "test_eval_dict", ",", "\n", "start_epoch", ",", "args", ".", "load_path", ",", "strict", "=", "args", ".", "strict", ")", "\n", "", "if", "args", ".", "load_sequence_taggers_paths", ":", "\n", "        ", "pretrained_dict", "=", "{", "}", "\n", "model_dict", "=", "model", ".", "state_dict", "(", ")", "\n", "for", "idx", ",", "path", "in", "enumerate", "(", "args", ".", "load_sequence_taggers_paths", ")", ":", "\n", "            ", "print", "(", "'Loading saved sequence_tagger from: %s'", "%", "path", ")", "\n", "checkpoint", "=", "torch", ".", "load", "(", "path", ",", "map_location", "=", "args", ".", "device", ")", "\n", "for", "k", ",", "v", "in", "checkpoint", "[", "'model_state_dict'", "]", ".", "items", "(", ")", ":", "\n", "                ", "if", "'rnn_encoder.'", "in", "k", ":", "\n", "                    ", "pretrained_dict", "[", "'extra_rnn_encoders.'", "+", "str", "(", "idx", ")", "+", "'.'", "+", "k", ".", "replace", "(", "'rnn_encoder.'", ",", "''", ")", "]", "=", "v", "\n", "", "", "", "model_dict", ".", "update", "(", "pretrained_dict", ")", "\n", "model", ".", "load_state_dict", "(", "model_dict", ")", "\n", "", "if", "args", ".", "freeze_sequence_taggers", ":", "\n", "        ", "print", "(", "'Freezing Classifiers'", ")", "\n", "for", "name", ",", "parameter", "in", "model", ".", "named_parameters", "(", ")", ":", "\n", "            ", "if", "'extra_rnn_encoders'", "in", "name", ":", "\n", "                ", "parameter", ".", "requires_grad", "=", "False", "\n", "", "", "", "if", "args", ".", "freeze_word_embeddings", ":", "\n", "        ", "model", ".", "rnn_encoder", ".", "word_embedd", ".", "weight", ".", "requires_grad", "=", "False", "\n", "# model.rnn_encoder.char_embedd.weight.requires_grad = False", "\n", "# model.rnn_encoder.pos_embedd.weight.requires_grad = False", "\n", "", "device", "=", "args", ".", "device", "\n", "model", ".", "to", "(", "device", ")", "\n", "return", "model", ",", "optimizer", ",", "dev_eval_dict", ",", "test_eval_dict", ",", "start_epoch", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.examples.GraphParser.initialize_eval_dict": [[329, 352], ["None"], "function", ["None"], ["", "def", "initialize_eval_dict", "(", ")", ":", "\n", "    ", "eval_dict", "=", "{", "}", "\n", "eval_dict", "[", "'dp_uas'", "]", "=", "0.0", "\n", "eval_dict", "[", "'dp_las'", "]", "=", "0.0", "\n", "eval_dict", "[", "'epoch'", "]", "=", "0", "\n", "eval_dict", "[", "'dp_ucorrect'", "]", "=", "0.0", "\n", "eval_dict", "[", "'dp_lcorrect'", "]", "=", "0.0", "\n", "eval_dict", "[", "'dp_total'", "]", "=", "0.0", "\n", "eval_dict", "[", "'dp_ucomplete_match'", "]", "=", "0.0", "\n", "eval_dict", "[", "'dp_lcomplete_match'", "]", "=", "0.0", "\n", "eval_dict", "[", "'dp_ucorrect_nopunc'", "]", "=", "0.0", "\n", "eval_dict", "[", "'dp_lcorrect_nopunc'", "]", "=", "0.0", "\n", "eval_dict", "[", "'dp_total_nopunc'", "]", "=", "0.0", "\n", "eval_dict", "[", "'dp_ucomplete_match_nopunc'", "]", "=", "0.0", "\n", "eval_dict", "[", "'dp_lcomplete_match_nopunc'", "]", "=", "0.0", "\n", "eval_dict", "[", "'dp_root_correct'", "]", "=", "0.0", "\n", "eval_dict", "[", "'dp_total_root'", "]", "=", "0.0", "\n", "eval_dict", "[", "'dp_total_inst'", "]", "=", "0.0", "\n", "eval_dict", "[", "'dp_total'", "]", "=", "0.0", "\n", "eval_dict", "[", "'dp_total_inst'", "]", "=", "0.0", "\n", "eval_dict", "[", "'dp_total_nopunc'", "]", "=", "0.0", "\n", "eval_dict", "[", "'dp_total_root'", "]", "=", "0.0", "\n", "return", "eval_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.examples.GraphParser.in_domain_evaluation": [[353, 391], ["GraphParser.evaluation", "print", "evaluation.items", "GraphParser.evaluation", "evaluation.items", "copy.deepcopy", "copy.deepcopy", "print", "GraphParser.save_checkpoint", "datasets.keys", "GraphParser.write_results"], "function", ["home.repos.pwc.inspect_result.Jivnesh_LCM.examples.GraphParser.evaluation", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.items", "home.repos.pwc.inspect_result.Jivnesh_LCM.examples.GraphParser.evaluation", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.items", "home.repos.pwc.inspect_result.Jivnesh_LCM.examples.GraphParser.save_checkpoint", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.keys", "home.repos.pwc.inspect_result.Jivnesh_LCM.examples.GraphParser.write_results"], ["", "def", "in_domain_evaluation", "(", "args", ",", "datasets", ",", "model", ",", "optimizer", ",", "dev_eval_dict", ",", "test_eval_dict", ",", "epoch", ",", "\n", "best_model", ",", "best_optimizer", ",", "patient", ")", ":", "\n", "# In-domain evaluation", "\n", "    ", "curr_dev_eval_dict", "=", "evaluation", "(", "args", ",", "datasets", "[", "'dev'", "]", ",", "'dev'", ",", "model", ",", "args", ".", "domain", ",", "epoch", ",", "'current_results'", ")", "\n", "is_best_in_domain", "=", "dev_eval_dict", "[", "'in_domain'", "]", "[", "'dp_lcorrect_nopunc'", "]", "<=", "curr_dev_eval_dict", "[", "'dp_lcorrect_nopunc'", "]", "or", "(", "dev_eval_dict", "[", "'in_domain'", "]", "[", "'dp_lcorrect_nopunc'", "]", "==", "curr_dev_eval_dict", "[", "'dp_lcorrect_nopunc'", "]", "and", "\n", "dev_eval_dict", "[", "'in_domain'", "]", "[", "'dp_ucorrect_nopunc'", "]", "<=", "curr_dev_eval_dict", "[", "'dp_ucorrect_nopunc'", "]", ")", "\n", "\n", "if", "is_best_in_domain", ":", "\n", "        ", "for", "key", ",", "value", "in", "curr_dev_eval_dict", ".", "items", "(", ")", ":", "\n", "            ", "dev_eval_dict", "[", "'in_domain'", "]", "[", "key", "]", "=", "value", "\n", "", "curr_test_eval_dict", "=", "evaluation", "(", "args", ",", "datasets", "[", "'test'", "]", ",", "'test'", ",", "model", ",", "args", ".", "domain", ",", "epoch", ",", "'current_results'", ")", "\n", "for", "key", ",", "value", "in", "curr_test_eval_dict", ".", "items", "(", ")", ":", "\n", "            ", "test_eval_dict", "[", "'in_domain'", "]", "[", "key", "]", "=", "value", "\n", "", "best_model", "=", "deepcopy", "(", "model", ")", "\n", "best_optimizer", "=", "deepcopy", "(", "optimizer", ")", "\n", "patient", "=", "0", "\n", "", "else", ":", "\n", "        ", "patient", "+=", "1", "\n", "", "if", "epoch", "==", "args", ".", "num_epochs", ":", "\n", "# save in-domain checkpoint", "\n", "        ", "if", "args", ".", "set_num_training_samples", "is", "not", "None", ":", "\n", "            ", "splits_to_write", "=", "datasets", ".", "keys", "(", ")", "\n", "", "else", ":", "\n", "            ", "splits_to_write", "=", "[", "'dev'", ",", "'test'", "]", "\n", "", "for", "split", "in", "splits_to_write", ":", "\n", "            ", "if", "split", "==", "'dev'", ":", "\n", "                ", "eval_dict", "=", "dev_eval_dict", "[", "'in_domain'", "]", "\n", "", "elif", "split", "==", "'test'", ":", "\n", "                ", "eval_dict", "=", "test_eval_dict", "[", "'in_domain'", "]", "\n", "", "else", ":", "\n", "                ", "eval_dict", "=", "None", "\n", "", "write_results", "(", "args", ",", "datasets", "[", "split", "]", ",", "args", ".", "domain", ",", "split", ",", "best_model", ",", "args", ".", "domain", ",", "eval_dict", ")", "\n", "", "print", "(", "\"Saving best model\"", ")", "\n", "save_checkpoint", "(", "args", ",", "best_model", ",", "best_optimizer", ",", "args", ".", "opt", ",", "dev_eval_dict", ",", "test_eval_dict", ",", "args", ".", "full_model_name", ")", "\n", "\n", "", "print", "(", "'\\n'", ")", "\n", "return", "dev_eval_dict", ",", "test_eval_dict", ",", "best_model", ",", "best_optimizer", ",", "patient", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.examples.GraphParser.evaluation": [[393, 436], ["model.eval", "GraphParser.initialize_eval_dict", "utils.io_.prepare_data.iterate_batch", "GraphParser.print_results", "model.forward", "model.decode", "lengths.cpu().numpy.cpu().numpy", "word.data.cpu().numpy.data.cpu().numpy", "pos.data.cpu().numpy.data.cpu().numpy", "ner.data.cpu().numpy.data.cpu().numpy", "heads.data.cpu().numpy.data.cpu().numpy", "arc_tags.data.cpu().numpy.data.cpu().numpy", "heads_pred.data.cpu().numpy.data.cpu().numpy", "arc_tags_pred.data.cpu().numpy.data.cpu().numpy", "utils.tasks.parse.eval_", "lengths.cpu().numpy.cpu", "word.data.cpu().numpy.data.cpu", "pos.data.cpu().numpy.data.cpu", "ner.data.cpu().numpy.data.cpu", "heads.data.cpu().numpy.data.cpu", "arc_tags.data.cpu().numpy.data.cpu", "heads_pred.data.cpu().numpy.data.cpu", "arc_tags_pred.data.cpu().numpy.data.cpu"], "function", ["home.repos.pwc.inspect_result.Jivnesh_LCM.examples.GraphParser.initialize_eval_dict", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.prepare_data.iterate_batch", "home.repos.pwc.inspect_result.Jivnesh_LCM.examples.GraphParser.print_results", "home.repos.pwc.inspect_result.Jivnesh_LCM.models.parsing_gating.Gating.forward", "home.repos.pwc.inspect_result.Jivnesh_LCM.models.parsing_gating.BiAffine_Parser_Decoder.decode", "home.repos.pwc.inspect_result.Jivnesh_LCM.tasks.parse.eval_"], ["", "def", "evaluation", "(", "args", ",", "data", ",", "split", ",", "model", ",", "domain", ",", "epoch", ",", "str_res", "=", "'results'", ")", ":", "\n", "# evaluate performance on data", "\n", "    ", "model", ".", "eval", "(", ")", "\n", "\n", "eval_dict", "=", "initialize_eval_dict", "(", ")", "\n", "eval_dict", "[", "'epoch'", "]", "=", "epoch", "\n", "for", "batch", "in", "prepare_data", ".", "iterate_batch", "(", "data", ",", "args", ".", "batch_size", ",", "args", ".", "device", ")", ":", "\n", "        ", "word", ",", "char", ",", "pos", ",", "ner", ",", "heads", ",", "arc_tags", ",", "auto_label", ",", "masks", ",", "lengths", "=", "batch", "\n", "out_arc", ",", "out_arc_tag", ",", "masks", ",", "lengths", "=", "model", ".", "forward", "(", "word", ",", "char", ",", "pos", ",", "mask", "=", "masks", ",", "length", "=", "lengths", ")", "\n", "heads_pred", ",", "arc_tags_pred", ",", "_", "=", "model", ".", "decode", "(", "args", ".", "model_path", ",", "word", ",", "pos", ",", "ner", ",", "out_arc", ",", "out_arc_tag", ",", "mask", "=", "masks", ",", "length", "=", "lengths", ",", "\n", "leading_symbolic", "=", "prepare_data", ".", "NUM_SYMBOLIC_TAGS", ")", "\n", "lengths", "=", "lengths", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "word", "=", "word", ".", "data", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "pos", "=", "pos", ".", "data", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "ner", "=", "ner", ".", "data", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "heads", "=", "heads", ".", "data", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "arc_tags", "=", "arc_tags", ".", "data", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "heads_pred", "=", "heads_pred", ".", "data", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "arc_tags_pred", "=", "arc_tags_pred", ".", "data", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "stats", ",", "stats_nopunc", ",", "stats_root", ",", "num_inst", "=", "parse", ".", "eval_", "(", "word", ",", "pos", ",", "heads_pred", ",", "arc_tags_pred", ",", "heads", ",", "\n", "arc_tags", ",", "args", ".", "alphabets", "[", "'word_alphabet'", "]", ",", "args", ".", "alphabets", "[", "'pos_alphabet'", "]", ",", "\n", "lengths", ",", "punct_set", "=", "args", ".", "punct_set", ",", "symbolic_root", "=", "True", ")", "\n", "ucorr", ",", "lcorr", ",", "total", ",", "ucm", ",", "lcm", "=", "stats", "\n", "ucorr_nopunc", ",", "lcorr_nopunc", ",", "total_nopunc", ",", "ucm_nopunc", ",", "lcm_nopunc", "=", "stats_nopunc", "\n", "corr_root", ",", "total_root", "=", "stats_root", "\n", "eval_dict", "[", "'dp_ucorrect'", "]", "+=", "ucorr", "\n", "eval_dict", "[", "'dp_lcorrect'", "]", "+=", "lcorr", "\n", "eval_dict", "[", "'dp_total'", "]", "+=", "total", "\n", "eval_dict", "[", "'dp_ucomplete_match'", "]", "+=", "ucm", "\n", "eval_dict", "[", "'dp_lcomplete_match'", "]", "+=", "lcm", "\n", "eval_dict", "[", "'dp_ucorrect_nopunc'", "]", "+=", "ucorr_nopunc", "\n", "eval_dict", "[", "'dp_lcorrect_nopunc'", "]", "+=", "lcorr_nopunc", "\n", "eval_dict", "[", "'dp_total_nopunc'", "]", "+=", "total_nopunc", "\n", "eval_dict", "[", "'dp_ucomplete_match_nopunc'", "]", "+=", "ucm_nopunc", "\n", "eval_dict", "[", "'dp_lcomplete_match_nopunc'", "]", "+=", "lcm_nopunc", "\n", "eval_dict", "[", "'dp_root_correct'", "]", "+=", "corr_root", "\n", "eval_dict", "[", "'dp_total_root'", "]", "+=", "total_root", "\n", "eval_dict", "[", "'dp_total_inst'", "]", "+=", "num_inst", "\n", "\n", "", "eval_dict", "[", "'dp_uas'", "]", "=", "eval_dict", "[", "'dp_ucorrect'", "]", "*", "100", "/", "eval_dict", "[", "'dp_total'", "]", "# considering w. punctuation", "\n", "eval_dict", "[", "'dp_las'", "]", "=", "eval_dict", "[", "'dp_lcorrect'", "]", "*", "100", "/", "eval_dict", "[", "'dp_total'", "]", "# considering w. punctuation", "\n", "print_results", "(", "eval_dict", ",", "split", ",", "domain", ",", "str_res", ")", "\n", "return", "eval_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.examples.GraphParser.constrained_evaluation": [[437, 480], ["model.eval", "GraphParser.initialize_eval_dict", "utils.io_.prepare_data.iterate_batch", "GraphParser.print_results", "model.forward", "model.constrained_decode", "lengths.cpu().numpy.cpu().numpy", "word.data.cpu().numpy.data.cpu().numpy", "pos.data.cpu().numpy.data.cpu().numpy", "ner.data.cpu().numpy.data.cpu().numpy", "heads.data.cpu().numpy.data.cpu().numpy", "arc_tags.data.cpu().numpy.data.cpu().numpy", "heads_pred.data.cpu().numpy.data.cpu().numpy", "arc_tags_pred.data.cpu().numpy.data.cpu().numpy", "utils.tasks.parse.eval_", "lengths.cpu().numpy.cpu", "word.data.cpu().numpy.data.cpu", "pos.data.cpu().numpy.data.cpu", "ner.data.cpu().numpy.data.cpu", "heads.data.cpu().numpy.data.cpu", "arc_tags.data.cpu().numpy.data.cpu", "heads_pred.data.cpu().numpy.data.cpu", "arc_tags_pred.data.cpu().numpy.data.cpu"], "function", ["home.repos.pwc.inspect_result.Jivnesh_LCM.examples.GraphParser.initialize_eval_dict", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.prepare_data.iterate_batch", "home.repos.pwc.inspect_result.Jivnesh_LCM.examples.GraphParser.print_results", "home.repos.pwc.inspect_result.Jivnesh_LCM.models.parsing_gating.Gating.forward", "home.repos.pwc.inspect_result.Jivnesh_LCM.models.parsing_gating.BiAffine_Parser_Gated.constrained_decode", "home.repos.pwc.inspect_result.Jivnesh_LCM.tasks.parse.eval_"], ["", "def", "constrained_evaluation", "(", "args", ",", "data", ",", "split", ",", "model", ",", "domain", ",", "epoch", ",", "str_res", "=", "'results'", ")", ":", "\n", "# evaluate performance on data", "\n", "    ", "model", ".", "eval", "(", ")", "\n", "\n", "eval_dict", "=", "initialize_eval_dict", "(", ")", "\n", "eval_dict", "[", "'epoch'", "]", "=", "epoch", "\n", "for", "batch", "in", "prepare_data", ".", "iterate_batch", "(", "data", ",", "args", ".", "batch_size", ",", "args", ".", "device", ")", ":", "\n", "        ", "word", ",", "char", ",", "pos", ",", "ner", ",", "heads", ",", "arc_tags", ",", "auto_label", ",", "masks", ",", "lengths", "=", "batch", "\n", "out_arc", ",", "out_arc_tag", ",", "masks", ",", "lengths", "=", "model", ".", "forward", "(", "word", ",", "char", ",", "pos", ",", "mask", "=", "masks", ",", "length", "=", "lengths", ")", "\n", "heads_pred", ",", "arc_tags_pred", ",", "_", "=", "model", ".", "constrained_decode", "(", "args", ",", "word", ",", "pos", ",", "ner", ",", "out_arc", ",", "out_arc_tag", ",", "mask", "=", "masks", ",", "length", "=", "lengths", ",", "\n", "leading_symbolic", "=", "prepare_data", ".", "NUM_SYMBOLIC_TAGS", ")", "\n", "lengths", "=", "lengths", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "word", "=", "word", ".", "data", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "pos", "=", "pos", ".", "data", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "ner", "=", "ner", ".", "data", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "heads", "=", "heads", ".", "data", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "arc_tags", "=", "arc_tags", ".", "data", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "heads_pred", "=", "heads_pred", ".", "data", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "arc_tags_pred", "=", "arc_tags_pred", ".", "data", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "stats", ",", "stats_nopunc", ",", "stats_root", ",", "num_inst", "=", "parse", ".", "eval_", "(", "word", ",", "pos", ",", "heads_pred", ",", "arc_tags_pred", ",", "heads", ",", "\n", "arc_tags", ",", "args", ".", "alphabets", "[", "'word_alphabet'", "]", ",", "args", ".", "alphabets", "[", "'pos_alphabet'", "]", ",", "\n", "lengths", ",", "punct_set", "=", "args", ".", "punct_set", ",", "symbolic_root", "=", "True", ")", "\n", "ucorr", ",", "lcorr", ",", "total", ",", "ucm", ",", "lcm", "=", "stats", "\n", "ucorr_nopunc", ",", "lcorr_nopunc", ",", "total_nopunc", ",", "ucm_nopunc", ",", "lcm_nopunc", "=", "stats_nopunc", "\n", "corr_root", ",", "total_root", "=", "stats_root", "\n", "eval_dict", "[", "'dp_ucorrect'", "]", "+=", "ucorr", "\n", "eval_dict", "[", "'dp_lcorrect'", "]", "+=", "lcorr", "\n", "eval_dict", "[", "'dp_total'", "]", "+=", "total", "\n", "eval_dict", "[", "'dp_ucomplete_match'", "]", "+=", "ucm", "\n", "eval_dict", "[", "'dp_lcomplete_match'", "]", "+=", "lcm", "\n", "eval_dict", "[", "'dp_ucorrect_nopunc'", "]", "+=", "ucorr_nopunc", "\n", "eval_dict", "[", "'dp_lcorrect_nopunc'", "]", "+=", "lcorr_nopunc", "\n", "eval_dict", "[", "'dp_total_nopunc'", "]", "+=", "total_nopunc", "\n", "eval_dict", "[", "'dp_ucomplete_match_nopunc'", "]", "+=", "ucm_nopunc", "\n", "eval_dict", "[", "'dp_lcomplete_match_nopunc'", "]", "+=", "lcm_nopunc", "\n", "eval_dict", "[", "'dp_root_correct'", "]", "+=", "corr_root", "\n", "eval_dict", "[", "'dp_total_root'", "]", "+=", "total_root", "\n", "eval_dict", "[", "'dp_total_inst'", "]", "+=", "num_inst", "\n", "\n", "", "eval_dict", "[", "'dp_uas'", "]", "=", "eval_dict", "[", "'dp_ucorrect'", "]", "*", "100", "/", "eval_dict", "[", "'dp_total'", "]", "# considering w. punctuation", "\n", "eval_dict", "[", "'dp_las'", "]", "=", "eval_dict", "[", "'dp_lcorrect'", "]", "*", "100", "/", "eval_dict", "[", "'dp_total'", "]", "# considering w. punctuation", "\n", "print_results", "(", "eval_dict", ",", "split", ",", "domain", ",", "str_res", ")", "\n", "return", "eval_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.examples.GraphParser.print_results": [[481, 505], ["print", "print", "print", "print", "print", "print", "print"], "function", ["None"], ["", "def", "print_results", "(", "eval_dict", ",", "split", ",", "domain", ",", "str_res", "=", "'results'", ")", ":", "\n", "    ", "print", "(", "'----------------------------------------------------------------------------------------------------------------------------'", ")", "\n", "print", "(", "'Testing model on domain %s'", "%", "domain", ")", "\n", "print", "(", "'--------------- Dependency Parsing - %s ---------------'", "%", "split", ")", "\n", "print", "(", "\n", "str_res", "+", "' on '", "+", "split", "+", "'  W. Punct: ucorr: %d, lcorr: %d, total: %d, uas: %.2f%%, las: %.2f%%, ucm: %.2f%%, lcm: %.2f%% (epoch: %d)'", "%", "(", "\n", "eval_dict", "[", "'dp_ucorrect'", "]", ",", "eval_dict", "[", "'dp_lcorrect'", "]", ",", "eval_dict", "[", "'dp_total'", "]", ",", "\n", "eval_dict", "[", "'dp_ucorrect'", "]", "*", "100", "/", "eval_dict", "[", "'dp_total'", "]", ",", "\n", "eval_dict", "[", "'dp_lcorrect'", "]", "*", "100", "/", "eval_dict", "[", "'dp_total'", "]", ",", "\n", "eval_dict", "[", "'dp_ucomplete_match'", "]", "*", "100", "/", "eval_dict", "[", "'dp_total_inst'", "]", ",", "\n", "eval_dict", "[", "'dp_lcomplete_match'", "]", "*", "100", "/", "eval_dict", "[", "'dp_total_inst'", "]", ",", "\n", "eval_dict", "[", "'epoch'", "]", ")", ")", "\n", "print", "(", "\n", "str_res", "+", "' on '", "+", "split", "+", "'  Wo Punct: ucorr: %d, lcorr: %d, total: %d, uas: %.2f%%, las: %.2f%%, ucm: %.2f%%, lcm: %.2f%% (epoch: %d)'", "%", "(", "\n", "eval_dict", "[", "'dp_ucorrect_nopunc'", "]", ",", "eval_dict", "[", "'dp_lcorrect_nopunc'", "]", ",", "eval_dict", "[", "'dp_total_nopunc'", "]", ",", "\n", "eval_dict", "[", "'dp_ucorrect_nopunc'", "]", "*", "100", "/", "eval_dict", "[", "'dp_total_nopunc'", "]", ",", "\n", "eval_dict", "[", "'dp_lcorrect_nopunc'", "]", "*", "100", "/", "eval_dict", "[", "'dp_total_nopunc'", "]", ",", "\n", "eval_dict", "[", "'dp_ucomplete_match_nopunc'", "]", "*", "100", "/", "eval_dict", "[", "'dp_total_inst'", "]", ",", "\n", "eval_dict", "[", "'dp_lcomplete_match_nopunc'", "]", "*", "100", "/", "eval_dict", "[", "'dp_total_inst'", "]", ",", "\n", "eval_dict", "[", "'epoch'", "]", ")", ")", "\n", "print", "(", "str_res", "+", "' on '", "+", "split", "+", "'  Root: corr: %d, total: %d, acc: %.2f%% (epoch: %d)'", "%", "(", "\n", "eval_dict", "[", "'dp_root_correct'", "]", ",", "eval_dict", "[", "'dp_total_root'", "]", ",", "\n", "eval_dict", "[", "'dp_root_correct'", "]", "*", "100", "/", "eval_dict", "[", "'dp_total_root'", "]", ",", "eval_dict", "[", "'epoch'", "]", ")", ")", "\n", "print", "(", "'\\n'", ")", "\n", "", "def", "constrained_write_results", "(", "args", ",", "data", ",", "data_domain", ",", "split", ",", "model", ",", "model_domain", ",", "eval_dict", ")", ":", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.examples.GraphParser.constrained_write_results": [[505, 547], ["utils.io_.Writer", "utils.io_.Writer", "utils.io_.Writer.start", "utils.io_.Writer.start", "utils.io_.prepare_data.iterate_batch", "utils.io_.Writer.close", "utils.io_.Writer.close", "model.forward", "model.constrained_decode", "lengths.cpu().numpy.cpu().numpy", "word.data.cpu().numpy.data.cpu().numpy", "pos.data.cpu().numpy.data.cpu().numpy", "ner.data.cpu().numpy.data.cpu().numpy", "heads.data.cpu().numpy.data.cpu().numpy", "arc_tags.data.cpu().numpy.data.cpu().numpy", "heads_pred.data.cpu().numpy.data.cpu().numpy", "arc_tags_pred.data.cpu().numpy.data.cpu().numpy", "utils.io_.Writer.write", "utils.io_.Writer.write", "open", "json.dump", "lengths.cpu().numpy.cpu", "word.data.cpu().numpy.data.cpu", "pos.data.cpu().numpy.data.cpu", "ner.data.cpu().numpy.data.cpu", "heads.data.cpu().numpy.data.cpu", "arc_tags.data.cpu().numpy.data.cpu", "heads_pred.data.cpu().numpy.data.cpu", "arc_tags_pred.data.cpu().numpy.data.cpu"], "function", ["home.repos.pwc.inspect_result.Jivnesh_LCM.io_.writer.Writer.start", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.writer.Writer.start", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.prepare_data.iterate_batch", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.reader.Reader.close", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.reader.Reader.close", "home.repos.pwc.inspect_result.Jivnesh_LCM.models.parsing_gating.Gating.forward", "home.repos.pwc.inspect_result.Jivnesh_LCM.models.parsing_gating.BiAffine_Parser_Gated.constrained_decode", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.writer.Writer.write", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.writer.Writer.write", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.open"], ["", "def", "constrained_write_results", "(", "args", ",", "data", ",", "data_domain", ",", "split", ",", "model", ",", "model_domain", ",", "eval_dict", ")", ":", "\n", "    ", "str_file", "=", "args", ".", "full_model_name", "+", "'_'", "+", "split", "+", "'_model_domain_'", "+", "model_domain", "+", "'_data_domain_'", "+", "data_domain", "\n", "res_filename", "=", "str_file", "+", "'_res.txt'", "\n", "pred_filename", "=", "str_file", "+", "'_pred.txt'", "\n", "gold_filename", "=", "str_file", "+", "'_gold.txt'", "\n", "if", "eval_dict", "is", "not", "None", ":", "\n", "# save results dictionary into a file", "\n", "        ", "with", "open", "(", "res_filename", ",", "'w'", ")", "as", "f", ":", "\n", "            ", "json", ".", "dump", "(", "eval_dict", ",", "f", ")", "\n", "\n", "# save predictions and gold labels into files", "\n", "", "", "pred_writer", "=", "Writer", "(", "args", ".", "alphabets", ")", "\n", "gold_writer", "=", "Writer", "(", "args", ".", "alphabets", ")", "\n", "pred_writer", ".", "start", "(", "pred_filename", ")", "\n", "gold_writer", ".", "start", "(", "gold_filename", ")", "\n", "for", "batch", "in", "prepare_data", ".", "iterate_batch", "(", "data", ",", "args", ".", "batch_size", ",", "args", ".", "device", ")", ":", "\n", "        ", "word", ",", "char", ",", "pos", ",", "ner", ",", "heads", ",", "arc_tags", ",", "auto_label", ",", "masks", ",", "lengths", "=", "batch", "\n", "# pdb.set_trace()", "\n", "out_arc", ",", "out_arc_tag", ",", "masks", ",", "lengths", "=", "model", ".", "forward", "(", "word", ",", "char", ",", "pos", ",", "mask", "=", "masks", ",", "length", "=", "lengths", ")", "\n", "heads_pred", ",", "arc_tags_pred", ",", "_", "=", "model", ".", "constrained_decode", "(", "args", ",", "word", ",", "pos", ",", "ner", ",", "out_arc", ",", "out_arc_tag", ",", "mask", "=", "masks", ",", "length", "=", "lengths", ",", "\n", "leading_symbolic", "=", "prepare_data", ".", "NUM_SYMBOLIC_TAGS", ")", "\n", "lengths", "=", "lengths", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "word", "=", "word", ".", "data", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "pos", "=", "pos", ".", "data", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "ner", "=", "ner", ".", "data", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "heads", "=", "heads", ".", "data", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "arc_tags", "=", "arc_tags", ".", "data", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "heads_pred", "=", "heads_pred", ".", "data", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "arc_tags_pred", "=", "arc_tags_pred", ".", "data", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "# print('words',word)", "\n", "# print('Pos',pos)", "\n", "\n", "# print('heads_pred',heads_pred)", "\n", "# print('arc_tags_pred',arc_tags_pred)", "\n", "# pdb.set_trace()", "\n", "# writing predictions", "\n", "pred_writer", ".", "write", "(", "word", ",", "pos", ",", "ner", ",", "heads_pred", ",", "arc_tags_pred", ",", "lengths", ",", "symbolic_root", "=", "True", ")", "\n", "# writing gold labels", "\n", "gold_writer", ".", "write", "(", "word", ",", "pos", ",", "ner", ",", "heads", ",", "arc_tags", ",", "lengths", ",", "symbolic_root", "=", "True", ")", "\n", "\n", "", "pred_writer", ".", "close", "(", ")", "\n", "gold_writer", ".", "close", "(", ")", "\n", "", "def", "write_results", "(", "args", ",", "data", ",", "data_domain", ",", "split", ",", "model", ",", "model_domain", ",", "eval_dict", ")", ":", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.examples.GraphParser.write_results": [[547, 589], ["utils.io_.Writer", "utils.io_.Writer", "utils.io_.Writer.start", "utils.io_.Writer.start", "utils.io_.prepare_data.iterate_batch", "utils.io_.Writer.close", "utils.io_.Writer.close", "model.forward", "model.decode", "lengths.cpu().numpy.cpu().numpy", "word.data.cpu().numpy.data.cpu().numpy", "pos.data.cpu().numpy.data.cpu().numpy", "ner.data.cpu().numpy.data.cpu().numpy", "heads.data.cpu().numpy.data.cpu().numpy", "arc_tags.data.cpu().numpy.data.cpu().numpy", "heads_pred.data.cpu().numpy.data.cpu().numpy", "arc_tags_pred.data.cpu().numpy.data.cpu().numpy", "utils.io_.Writer.write", "utils.io_.Writer.write", "open", "json.dump", "lengths.cpu().numpy.cpu", "word.data.cpu().numpy.data.cpu", "pos.data.cpu().numpy.data.cpu", "ner.data.cpu().numpy.data.cpu", "heads.data.cpu().numpy.data.cpu", "arc_tags.data.cpu().numpy.data.cpu", "heads_pred.data.cpu().numpy.data.cpu", "arc_tags_pred.data.cpu().numpy.data.cpu"], "function", ["home.repos.pwc.inspect_result.Jivnesh_LCM.io_.writer.Writer.start", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.writer.Writer.start", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.prepare_data.iterate_batch", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.reader.Reader.close", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.reader.Reader.close", "home.repos.pwc.inspect_result.Jivnesh_LCM.models.parsing_gating.Gating.forward", "home.repos.pwc.inspect_result.Jivnesh_LCM.models.parsing_gating.BiAffine_Parser_Decoder.decode", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.writer.Writer.write", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.writer.Writer.write", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.open"], ["", "def", "write_results", "(", "args", ",", "data", ",", "data_domain", ",", "split", ",", "model", ",", "model_domain", ",", "eval_dict", ")", ":", "\n", "    ", "str_file", "=", "args", ".", "full_model_name", "+", "'_'", "+", "split", "+", "'_model_domain_'", "+", "model_domain", "+", "'_data_domain_'", "+", "data_domain", "\n", "res_filename", "=", "str_file", "+", "'_res.txt'", "\n", "pred_filename", "=", "str_file", "+", "'_pred.txt'", "\n", "gold_filename", "=", "str_file", "+", "'_gold.txt'", "\n", "if", "eval_dict", "is", "not", "None", ":", "\n", "# save results dictionary into a file", "\n", "        ", "with", "open", "(", "res_filename", ",", "'w'", ")", "as", "f", ":", "\n", "            ", "json", ".", "dump", "(", "eval_dict", ",", "f", ")", "\n", "\n", "# save predictions and gold labels into files", "\n", "", "", "pred_writer", "=", "Writer", "(", "args", ".", "alphabets", ")", "\n", "gold_writer", "=", "Writer", "(", "args", ".", "alphabets", ")", "\n", "pred_writer", ".", "start", "(", "pred_filename", ")", "\n", "gold_writer", ".", "start", "(", "gold_filename", ")", "\n", "for", "batch", "in", "prepare_data", ".", "iterate_batch", "(", "data", ",", "args", ".", "batch_size", ",", "args", ".", "device", ")", ":", "\n", "        ", "word", ",", "char", ",", "pos", ",", "ner", ",", "heads", ",", "arc_tags", ",", "auto_label", ",", "masks", ",", "lengths", "=", "batch", "\n", "# pdb.set_trace()", "\n", "out_arc", ",", "out_arc_tag", ",", "masks", ",", "lengths", "=", "model", ".", "forward", "(", "word", ",", "char", ",", "pos", ",", "mask", "=", "masks", ",", "length", "=", "lengths", ")", "\n", "heads_pred", ",", "arc_tags_pred", ",", "_", "=", "model", ".", "decode", "(", "args", ".", "model_path", ",", "word", ",", "pos", ",", "ner", ",", "out_arc", ",", "out_arc_tag", ",", "mask", "=", "masks", ",", "length", "=", "lengths", ",", "\n", "leading_symbolic", "=", "prepare_data", ".", "NUM_SYMBOLIC_TAGS", ")", "\n", "lengths", "=", "lengths", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "word", "=", "word", ".", "data", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "pos", "=", "pos", ".", "data", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "ner", "=", "ner", ".", "data", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "heads", "=", "heads", ".", "data", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "arc_tags", "=", "arc_tags", ".", "data", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "heads_pred", "=", "heads_pred", ".", "data", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "arc_tags_pred", "=", "arc_tags_pred", ".", "data", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "# print('words',word)", "\n", "# print('Pos',pos)", "\n", "\n", "# print('heads_pred',heads_pred)", "\n", "# print('arc_tags_pred',arc_tags_pred)", "\n", "# pdb.set_trace()", "\n", "# writing predictions", "\n", "pred_writer", ".", "write", "(", "word", ",", "pos", ",", "ner", ",", "heads_pred", ",", "arc_tags_pred", ",", "lengths", ",", "symbolic_root", "=", "True", ")", "\n", "# writing gold labels", "\n", "gold_writer", ".", "write", "(", "word", ",", "pos", ",", "ner", ",", "heads", ",", "arc_tags", ",", "lengths", ",", "symbolic_root", "=", "True", ")", "\n", "\n", "", "pred_writer", ".", "close", "(", ")", "\n", "gold_writer", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Jivnesh_LCM.examples.GraphParser.main": [[590, 700], ["logger.info", "GraphParser.read_arguments", "logger.info", "logger.info", "sum", "GraphParser.build_model_and_optimizer", "copy.deepcopy", "copy.deepcopy", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "print", "print", "utils.io_.prepare_data.read_data_to_variable", "print", "utils.io_.rearrange_splits.rearranging_splits", "logger.info", "utils.io_.prepare_data.calc_num_batches", "range", "rearrange_splits.rearranging_splits.keys", "logger.info", "print", "model.train", "utils.io_.prepare_data.iterate_batch_rand_bucket_choosing", "time.time", "enumerate", "print", "print", "GraphParser.in_domain_evaluation", "GraphParser.print_results", "print", "generate_optimizer.zero_grad", "model.forward", "model.loss", "loss.backward", "torch.nn.utils.clip_grad_norm_", "generate_optimizer.step", "GraphParser.generate_optimizer", "print", "print", "GraphParser.constrained_evaluation", "GraphParser.constrained_write_results", "GraphParser.evaluation", "GraphParser.write_results", "GraphParser.constrained_evaluation", "GraphParser.constrained_write_results", "GraphParser.evaluation", "GraphParser.write_results", "masks.data.sum", "word.size", "loss_arc.item", "loss_arc_tag.item", "loss.item", "model.parameters", "sys.stdout.write", "sys.stdout.write", "sys.stdout.flush", "model.parameters", "time.time", "time.time"], "function", ["home.repos.pwc.inspect_result.Jivnesh_LCM.examples.GraphParser.read_arguments", "home.repos.pwc.inspect_result.Jivnesh_LCM.examples.GraphParser.build_model_and_optimizer", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.prepare_data.read_data_to_variable", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.rearrange_splits.rearranging_splits", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.prepare_data.calc_num_batches", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.keys", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.prepare_data.iterate_batch_rand_bucket_choosing", "home.repos.pwc.inspect_result.Jivnesh_LCM.examples.GraphParser.in_domain_evaluation", "home.repos.pwc.inspect_result.Jivnesh_LCM.examples.GraphParser.print_results", "home.repos.pwc.inspect_result.Jivnesh_LCM.models.parsing_gating.Gating.forward", "home.repos.pwc.inspect_result.Jivnesh_LCM.models.parsing_gating.BiAffine_Parser_Decoder.loss", "home.repos.pwc.inspect_result.Jivnesh_LCM.examples.GraphParser.generate_optimizer", "home.repos.pwc.inspect_result.Jivnesh_LCM.examples.GraphParser.constrained_evaluation", "home.repos.pwc.inspect_result.Jivnesh_LCM.examples.GraphParser.constrained_write_results", "home.repos.pwc.inspect_result.Jivnesh_LCM.examples.GraphParser.evaluation", "home.repos.pwc.inspect_result.Jivnesh_LCM.examples.GraphParser.write_results", "home.repos.pwc.inspect_result.Jivnesh_LCM.examples.GraphParser.constrained_evaluation", "home.repos.pwc.inspect_result.Jivnesh_LCM.examples.GraphParser.constrained_write_results", "home.repos.pwc.inspect_result.Jivnesh_LCM.examples.GraphParser.evaluation", "home.repos.pwc.inspect_result.Jivnesh_LCM.examples.GraphParser.write_results", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.alphabet.Alphabet.size", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.writer.Writer.write", "home.repos.pwc.inspect_result.Jivnesh_LCM.io_.writer.Writer.write"], ["", "def", "main", "(", ")", ":", "\n", "    ", "logger", ".", "info", "(", "\"Reading and creating arguments\"", ")", "\n", "args", "=", "read_arguments", "(", ")", "\n", "logger", ".", "info", "(", "\"Reading Data\"", ")", "\n", "datasets", "=", "{", "}", "\n", "for", "split", "in", "args", ".", "splits", ":", "\n", "        ", "print", "(", "\"Splits are:\"", ",", "split", ")", "\n", "dataset", "=", "prepare_data", ".", "read_data_to_variable", "(", "args", ".", "data_paths", "[", "split", "]", ",", "args", ".", "alphabets", ",", "args", ".", "device", ",", "\n", "symbolic_root", "=", "True", ")", "\n", "datasets", "[", "split", "]", "=", "dataset", "\n", "", "if", "args", ".", "set_num_training_samples", "is", "not", "None", ":", "\n", "        ", "print", "(", "'Setting train and dev to %d samples'", "%", "args", ".", "set_num_training_samples", ")", "\n", "datasets", "=", "rearrange_splits", ".", "rearranging_splits", "(", "datasets", ",", "args", ".", "set_num_training_samples", ")", "\n", "", "logger", ".", "info", "(", "\"Creating Networks\"", ")", "\n", "num_data", "=", "sum", "(", "datasets", "[", "'train'", "]", "[", "1", "]", ")", "\n", "model", ",", "optimizer", ",", "dev_eval_dict", ",", "test_eval_dict", ",", "start_epoch", "=", "build_model_and_optimizer", "(", "args", ")", "\n", "best_model", "=", "deepcopy", "(", "model", ")", "\n", "best_optimizer", "=", "deepcopy", "(", "optimizer", ")", "\n", "\n", "logger", ".", "info", "(", "'Training INFO of in domain %s'", "%", "args", ".", "domain", ")", "\n", "logger", ".", "info", "(", "'Training on Dependecy Parsing'", ")", "\n", "logger", ".", "info", "(", "\"train: gamma: %f, batch: %d, clip: %.2f, unk replace: %.2f\"", "%", "(", "args", ".", "gamma", ",", "args", ".", "batch_size", ",", "args", ".", "clip", ",", "args", ".", "unk_replace", ")", ")", "\n", "logger", ".", "info", "(", "'number of training samples for %s is: %d'", "%", "(", "args", ".", "domain", ",", "num_data", ")", ")", "\n", "logger", ".", "info", "(", "\"dropout(in, out, rnn): (%.2f, %.2f, %s)\"", "%", "(", "args", ".", "p_in", ",", "args", ".", "p_out", ",", "args", ".", "p_rnn", ")", ")", "\n", "logger", ".", "info", "(", "\"num_epochs: %d\"", "%", "(", "args", ".", "num_epochs", ")", ")", "\n", "print", "(", "'\\n'", ")", "\n", "\n", "if", "not", "args", ".", "eval_mode", ":", "\n", "        ", "logger", ".", "info", "(", "\"Training\"", ")", "\n", "num_batches", "=", "prepare_data", ".", "calc_num_batches", "(", "datasets", "[", "'train'", "]", ",", "args", ".", "batch_size", ")", "\n", "lr", "=", "args", ".", "learning_rate", "\n", "patient", "=", "0", "\n", "decay", "=", "0", "\n", "for", "epoch", "in", "range", "(", "start_epoch", "+", "1", ",", "args", ".", "num_epochs", "+", "1", ")", ":", "\n", "            ", "print", "(", "'Epoch %d (Training: rnn mode: %s, optimizer: %s, learning rate=%.6f, eps=%.1e, decay rate=%.2f (schedule=%d, decay=%d)): '", "%", "(", "\n", "epoch", ",", "args", ".", "rnn_mode", ",", "args", ".", "opt", ",", "lr", ",", "args", ".", "epsilon", ",", "args", ".", "decay_rate", ",", "args", ".", "schedule", ",", "decay", ")", ")", "\n", "model", ".", "train", "(", ")", "\n", "total_loss", "=", "0.0", "\n", "total_arc_loss", "=", "0.0", "\n", "total_arc_tag_loss", "=", "0.0", "\n", "total_train_inst", "=", "0.0", "\n", "\n", "train_iter", "=", "prepare_data", ".", "iterate_batch_rand_bucket_choosing", "(", "\n", "datasets", "[", "'train'", "]", ",", "args", ".", "batch_size", ",", "args", ".", "device", ",", "unk_replace", "=", "args", ".", "unk_replace", ")", "\n", "start_time", "=", "time", ".", "time", "(", ")", "\n", "batch_num", "=", "0", "\n", "for", "batch_num", ",", "batch", "in", "enumerate", "(", "train_iter", ")", ":", "\n", "                ", "batch_num", "=", "batch_num", "+", "1", "\n", "optimizer", ".", "zero_grad", "(", ")", "\n", "# compute loss of main task", "\n", "word", ",", "char", ",", "pos", ",", "ner_tags", ",", "heads", ",", "arc_tags", ",", "auto_label", ",", "masks", ",", "lengths", "=", "batch", "\n", "out_arc", ",", "out_arc_tag", ",", "masks", ",", "lengths", "=", "model", ".", "forward", "(", "word", ",", "char", ",", "pos", ",", "mask", "=", "masks", ",", "length", "=", "lengths", ")", "\n", "loss_arc", ",", "loss_arc_tag", "=", "model", ".", "loss", "(", "out_arc", ",", "out_arc_tag", ",", "heads", ",", "arc_tags", ",", "mask", "=", "masks", ",", "length", "=", "lengths", ")", "\n", "loss", "=", "loss_arc", "+", "loss_arc_tag", "\n", "# pdb.set_trace()", "\n", "\n", "# update losses", "\n", "num_insts", "=", "masks", ".", "data", ".", "sum", "(", ")", "-", "word", ".", "size", "(", "0", ")", "\n", "total_arc_loss", "+=", "loss_arc", ".", "item", "(", ")", "*", "num_insts", "\n", "total_arc_tag_loss", "+=", "loss_arc_tag", ".", "item", "(", ")", "*", "num_insts", "\n", "total_loss", "+=", "loss", ".", "item", "(", ")", "*", "num_insts", "\n", "total_train_inst", "+=", "num_insts", "\n", "# optimize parameters", "\n", "loss", ".", "backward", "(", ")", "\n", "clip_grad_norm_", "(", "model", ".", "parameters", "(", ")", ",", "args", ".", "clip", ")", "\n", "optimizer", ".", "step", "(", ")", "\n", "\n", "time_ave", "=", "(", "time", ".", "time", "(", ")", "-", "start_time", ")", "/", "batch_num", "\n", "time_left", "=", "(", "num_batches", "-", "batch_num", ")", "*", "time_ave", "\n", "\n", "# update log", "\n", "if", "batch_num", "%", "50", "==", "0", ":", "\n", "                    ", "log_info", "=", "'train: %d/%d, domain: %s, total loss: %.2f, arc_loss: %.2f, arc_tag_loss: %.2f, time left: %.2fs'", "%", "(", "batch_num", ",", "num_batches", ",", "args", ".", "domain", ",", "total_loss", "/", "total_train_inst", ",", "total_arc_loss", "/", "total_train_inst", ",", "\n", "total_arc_tag_loss", "/", "total_train_inst", ",", "time_left", ")", "\n", "sys", ".", "stdout", ".", "write", "(", "log_info", ")", "\n", "sys", ".", "stdout", ".", "write", "(", "'\\n'", ")", "\n", "sys", ".", "stdout", ".", "flush", "(", ")", "\n", "", "", "print", "(", "'\\n'", ")", "\n", "print", "(", "'train: %d/%d, domain: %s, total_loss: %.2f, arc_loss: %.2f, arc_tag_loss: %.2f, time: %.2fs'", "%", "\n", "(", "batch_num", ",", "num_batches", ",", "args", ".", "domain", ",", "total_loss", "/", "total_train_inst", ",", "total_arc_loss", "/", "total_train_inst", ",", "\n", "total_arc_tag_loss", "/", "total_train_inst", ",", "time", ".", "time", "(", ")", "-", "start_time", ")", ")", "\n", "\n", "dev_eval_dict", ",", "test_eval_dict", ",", "best_model", ",", "best_optimizer", ",", "patient", "=", "in_domain_evaluation", "(", "args", ",", "datasets", ",", "model", ",", "optimizer", ",", "dev_eval_dict", ",", "test_eval_dict", ",", "epoch", ",", "best_model", ",", "best_optimizer", ",", "patient", ")", "\n", "if", "patient", ">=", "args", ".", "schedule", ":", "\n", "                ", "lr", "=", "args", ".", "learning_rate", "/", "(", "1.0", "+", "epoch", "*", "args", ".", "decay_rate", ")", "\n", "optimizer", "=", "generate_optimizer", "(", "args", ",", "lr", ",", "model", ".", "parameters", "(", ")", ")", "\n", "print", "(", "'updated learning rate to %.6f'", "%", "lr", ")", "\n", "patient", "=", "0", "\n", "", "print_results", "(", "test_eval_dict", "[", "'in_domain'", "]", ",", "'test'", ",", "args", ".", "domain", ",", "'best_results'", ")", "\n", "print", "(", "'\\n'", ")", "\n", "", "for", "split", "in", "datasets", ".", "keys", "(", ")", ":", "\n", "            ", "if", "args", ".", "eval_with_CI", "and", "split", "not", "in", "[", "'train'", ",", "'extra_train'", ",", "'extra_dev'", "]", ":", "\n", "                ", "print", "(", "'Currently going on ... '", ",", "split", ")", "\n", "eval_dict", "=", "constrained_evaluation", "(", "args", ",", "datasets", "[", "split", "]", ",", "split", ",", "best_model", ",", "args", ".", "domain", ",", "epoch", ",", "'best_results'", ")", "\n", "constrained_write_results", "(", "args", ",", "datasets", "[", "split", "]", ",", "args", ".", "domain", ",", "split", ",", "model", ",", "args", ".", "domain", ",", "eval_dict", ")", "\n", "", "else", ":", "\n", "                ", "eval_dict", "=", "evaluation", "(", "args", ",", "datasets", "[", "split", "]", ",", "split", ",", "best_model", ",", "args", ".", "domain", ",", "epoch", ",", "'best_results'", ")", "\n", "write_results", "(", "args", ",", "datasets", "[", "split", "]", ",", "args", ".", "domain", ",", "split", ",", "model", ",", "args", ".", "domain", ",", "eval_dict", ")", "\n", "\n", "", "", "", "else", ":", "\n", "        ", "logger", ".", "info", "(", "\"Evaluating\"", ")", "\n", "epoch", "=", "start_epoch", "\n", "for", "split", "in", "[", "'train'", ",", "'dev'", ",", "'test'", "]", ":", "\n", "            ", "if", "args", ".", "eval_with_CI", "and", "split", "!=", "'train'", ":", "\n", "                ", "eval_dict", "=", "constrained_evaluation", "(", "args", ",", "datasets", "[", "split", "]", ",", "split", ",", "best_model", ",", "args", ".", "domain", ",", "epoch", ",", "'best_results'", ")", "\n", "constrained_write_results", "(", "args", ",", "datasets", "[", "split", "]", ",", "args", ".", "domain", ",", "split", ",", "model", ",", "args", ".", "domain", ",", "eval_dict", ")", "\n", "", "else", ":", "\n", "                ", "eval_dict", "=", "evaluation", "(", "args", ",", "datasets", "[", "split", "]", ",", "split", ",", "best_model", ",", "args", ".", "domain", ",", "epoch", ",", "'best_results'", ")", "\n", "write_results", "(", "args", ",", "datasets", "[", "split", "]", ",", "args", ".", "domain", ",", "split", ",", "model", ",", "args", ".", "domain", ",", "eval_dict", ")", "\n", "\n"]]}