{"home.repos.pwc.inspect_result.facebookresearch_ppuda.experiments.property_prediction.main": [[30, 133], ["len", "ckpt.replace", "os.path.exists", "properties.items", "print", "open", "json.load", "len", "len", "print", "numpy.load", "ppuda.ghn.nn.GHN.load", "print", "property_prediction.main.extract_graph_embeddings"], "function", ["home.repos.pwc.inspect_result.facebookresearch_ppuda.ghn.nn.GHN.load", "home.repos.pwc.inspect_result.facebookresearch_ppuda.ghn.nn.GHN.load", "home.repos.pwc.inspect_result.facebookresearch_ppuda.ghn.nn.GHN.load"], ["def", "main", "(", ")", ":", "\n", "    ", "dataset", "=", "sys", ".", "argv", "[", "1", "]", "# cifar10, imagenet", "\n", "ckpt", "=", "sys", ".", "argv", "[", "2", "]", "# GHN checkpoint path", "\n", "ghn_device", "=", "'cpu'", "# little benefit of cuda in this experiment", "\n", "\n", "is_imagenet", "=", "dataset", "==", "'imagenet'", "\n", "\n", "with", "open", "(", "'./data/results_%s.json'", "%", "dataset", ",", "'r'", ")", "as", "f", ":", "\n", "        ", "results", "=", "json", ".", "load", "(", "f", ")", "\n", "\n", "", "properties", "=", "{", "}", "\n", "for", "prop", "in", "[", "'val_acc'", ",", "'val_acc_noise'", ",", "'time'", ",", "'converge_time'", "]", ":", "\n", "        ", "properties", "[", "prop", "]", "=", "{", "}", "\n", "for", "split", "in", "[", "'val'", ",", "'test'", "]", ":", "\n", "            ", "properties", "[", "prop", "]", "[", "split", "]", "=", "np", ".", "array", "(", "[", "r", "[", "prop", "]", "for", "r", "in", "results", "[", "split", "]", ".", "values", "(", ")", "]", ")", "\n", "", "", "n_train", "=", "len", "(", "properties", "[", "'val_acc'", "]", "[", "'val'", "]", ")", "\n", "assert", "n_train", "==", "len", "(", "properties", "[", "'val_acc'", "]", "[", "'test'", "]", ")", "==", "500", ",", "(", "'val ({}) and test ({}) splits are expected to be 500 each'", ".", "format", "(", "n_train", ",", "len", "(", "properties", "[", "'val_acc'", "]", "[", "'test'", "]", ")", ")", ")", "\n", "\n", "cache_file", "=", "ckpt", ".", "replace", "(", "'.pt'", ",", "'_embed.npy'", ")", "# file with graph embeddings", "\n", "if", "os", ".", "path", ".", "exists", "(", "cache_file", ")", ":", "\n", "        ", "print", "(", "'\\nloading graph embeddings from the cache: %s'", "%", "cache_file", ")", "\n", "x", "=", "np", ".", "load", "(", "cache_file", ")", "\n", "x_train", ",", "x_test", ",", "x_search", "=", "x", "[", ":", "n_train", "]", ",", "x", "[", "n_train", ":", "n_train", "*", "2", "]", ",", "x", "[", "n_train", "*", "2", ":", "]", "\n", "", "else", ":", "\n", "        ", "ghn", "=", "GHN", ".", "load", "(", "ckpt", ",", "debug_level", "=", "0", ",", "device", "=", "ghn_device", ",", "verbose", "=", "True", ")", "\n", "virtual_edges", "=", "50", "if", "ghn", ".", "ve", "else", "1", "# default values", "\n", "\n", "def", "extract_graph_embeddings", "(", "graphs_queue", ")", ":", "\n", "            ", "x", "=", "[", "]", "\n", "for", "graphs", "in", "tqdm", "(", "graphs_queue", ")", ":", "\n", "                ", "assert", "len", "(", "graphs", ")", "==", "1", ",", "(", "'only one architecture per batch is supported in the evaluation mode'", ",", "len", "(", "graphs", ")", ")", "\n", "net_args", ",", "net_idx", "=", "graphs", ".", "net_args", "[", "0", "]", ",", "graphs", ".", "net_inds", "[", "0", "]", "\n", "\n", "model", "=", "Network", "(", "num_classes", "=", "1000", "if", "is_imagenet", "else", "10", ",", "\n", "is_imagenet_input", "=", "is_imagenet", ",", "\n", "**", "net_args", ")", ".", "eval", "(", ")", "\n", "x", ".", "append", "(", "ghn", "(", "model", ",", "graphs", ".", "to_device", "(", "ghn_device", ")", ",", "return_embeddings", "=", "True", ")", "[", "1", "]", ".", "mean", "(", "0", ",", "keepdim", "=", "True", ")", ".", "data", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ")", "\n", "", "x", "=", "np", ".", "concatenate", "(", "x", ")", "\n", "return", "x", "\n", "\n", "", "print", "(", "'\\nextracting graph embeddings'", ")", "\n", "\n", "x_train", "=", "extract_graph_embeddings", "(", "DeepNets1M", ".", "loader", "(", "split", "=", "'val'", ",", "virtual_edges", "=", "virtual_edges", ",", "large_images", "=", "is_imagenet", ")", ")", "\n", "x_test", "=", "extract_graph_embeddings", "(", "DeepNets1M", ".", "loader", "(", "split", "=", "'test'", ",", "virtual_edges", "=", "virtual_edges", ",", "large_images", "=", "is_imagenet", ")", ")", "\n", "assert", "len", "(", "x_train", ")", "==", "len", "(", "x_test", ")", "==", "n_train", ",", "(", "x_train", ".", "shape", ",", "x_test", ".", "shape", ",", "n_train", ")", "\n", "x_search", "=", "extract_graph_embeddings", "(", "DeepNets1M", ".", "loader", "(", "split", "=", "'search'", ",", "virtual_edges", "=", "virtual_edges", ",", "large_images", "=", "is_imagenet", ")", ")", "\n", "np", ".", "save", "(", "ckpt", ".", "replace", "(", "'.pt'", ",", "'_embed.npy'", ")", ",", "np", ".", "concatenate", "(", "(", "x_train", ",", "x_test", ",", "x_search", ")", ")", ")", "\n", "\n", "\n", "", "grid_search_params", "=", "{", "\n", "'kernel'", ":", "[", "'rbf'", "]", ",", "\n", "'C'", ":", "[", "1", ",", "10", ",", "50", ",", "10", "**", "2", ",", "2", "*", "10", "**", "2", ",", "5", "*", "10", "**", "2", ",", "10", "**", "3", "]", ",", "\n", "'gamma'", ":", "[", "'auto'", ",", "0.05", ",", "0.1", ",", "0.2", ",", "0.5", "]", ",", "\n", "'epsilon'", ":", "[", "0.05", ",", "0.1", ",", "0.2", "]", "\n", "}", "\n", "\n", "for", "prop", ",", "splits", "in", "properties", ".", "items", "(", ")", ":", "\n", "        ", "y_train", ",", "y_test", "=", "splits", "[", "'val'", "]", ",", "splits", "[", "'test'", "]", "\n", "\n", "seeds", "=", "[", "0", ",", "1", ",", "2", ",", "3", ",", "None", "]", "\n", "print", "(", "'\\n{}: running the experiment for {} seeds'", ".", "format", "(", "prop", ".", "upper", "(", ")", ",", "len", "(", "seeds", ")", ")", ")", "\n", "\n", "scores", "=", "[", "]", "\n", "for", "seed", "in", "seeds", ":", "\n", "            ", "if", "seed", "is", "not", "None", ":", "\n", "                ", "np", ".", "random", ".", "seed", "(", "seed", ")", "\n", "ind_rand", "=", "np", ".", "random", ".", "permutation", "(", "n_train", ")", "\n", "", "else", ":", "\n", "                ", "ind_rand", "=", "np", ".", "arange", "(", "n_train", ")", "\n", "\n", "# Find the best hyperparameters of SVR on the training set using cross-validation", "\n", "", "clf", "=", "GridSearchCV", "(", "SVR", "(", ")", ",", "grid_search_params", ",", "cv", "=", "5", ",", "n_jobs", "=", "4", ")", "\n", "clf", ".", "fit", "(", "x_train", "[", "ind_rand", "]", ",", "y_train", "[", "ind_rand", "]", ")", "\n", "if", "seed", "is", "None", ":", "\n", "                ", "print", "(", "'best params'", ",", "clf", ".", "best_params_", ")", "\n", "\n", "", "model", "=", "SVR", "(", "**", "clf", ".", "best_params_", ")", ".", "fit", "(", "x_train", ",", "y_train", ")", "\n", "y_pred", "=", "model", ".", "predict", "(", "x_test", ")", "\n", "if", "prop", "!=", "'converge_time'", ":", "\n", "                ", "y_pred", "=", "np", ".", "round", "(", "y_pred", ")", "# rounding slightly improves results", "\n", "# in the paper we also round the ground truth values, so the results here can be slightly different", "\n", "\n", "", "score", "=", "kendalltau", "(", "y_test", ",", "y_pred", ")", "[", "0", "]", "# rank correlation between prediction and test", "\n", "print", "(", "'Result for seed={}: {:.3f} ({} test samples)'", ".", "format", "(", "seed", ",", "score", ",", "len", "(", "y_test", ")", ")", ")", "\n", "scores", ".", "append", "(", "score", ")", "\n", "\n", "", "print", "(", "'\\nResults for all seeds: {:.3f} +- {:.3f}'", ".", "format", "(", "np", ".", "mean", "(", "scores", ")", ",", "np", ".", "std", "(", "scores", ")", ")", ")", "\n", "\n", "x", "=", "np", ".", "concatenate", "(", "(", "x_train", ",", "x_test", ")", ")", "\n", "print", "(", "'Retrain the regression model on {} examples'", ".", "format", "(", "len", "(", "x", ")", ")", ")", "\n", "model", "=", "SVR", "(", "**", "clf", ".", "best_params_", ")", ".", "fit", "(", "x", ",", "np", ".", "concatenate", "(", "(", "y_train", ",", "y_test", ")", ")", ")", "# using the best params found with seed=None", "\n", "\n", "# Find the best (in the sense of a given property) architecture in the Search split with 100k architectures", "\n", "y_pred", "=", "model", ".", "predict", "(", "x_search", ")", "\n", "best_arch", "=", "np", ".", "argmax", "(", "y_pred", ")", "\n", "print", "(", "'Architecture with the best {} (prediction={:.3f}) in the SEARCH split is {} ({} test samples)'", ".", "format", "(", "\n", "prop", ".", "upper", "(", ")", ",", "y_pred", "[", "best_arch", "]", ",", "best_arch", ",", "len", "(", "y_pred", ")", ")", ")", "\n", "\n", "# the best (in the sense of a given property) architecture can be trained by running (see vision/train_net.py for more examples):", "\n", "# python vision/train_net.py --split search --arch $best_arch", "\n", "\n", "", "print", "(", "'\\ndone'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.experiments.net_generator.main": [[37, 303], ["print", "set_seed", "np.arange", "time.time", "os.path.join", "os.path.join", "print", "print", "sys.argv[].lower", "int", "os.path.exists", "os.mkdir", "subprocess.check_output().decode().strip", "print", "os.path.exists", "h5py.File", "h5_data.create_group", "open", "json.dump", "net_generator.merge_eval", "print", "print", "ValueError", "len", "int", "np.random.choice", "int", "sample_genotype", "int", "int", "bool", "int", "range", "np.array().astype", "ppuda.deepnets1m.graph.Graph._Adj.cpu().numpy().astype", "len", "h5_data.create_group.create_dataset", "h5_data.create_group.create_dataset", "int", "to_dict", "[].append", "subprocess.check_output().decode", "np.random.randint", "np.random.randint", "np.random.choice", "sum", "sum", "sum", "sum", "np.random.choice", "np.random.choice", "bool", "int", "ppuda.deepnets1m.net.Network().to", "capacity", "len", "len", "len", "np.array", "print", "bool", "sum", "sum", "sum", "sum", "bool", "bool", "np.random.randint", "print", "ppuda.deepnets1m.net.get_cell_ind", "param_name.startswith", "np.array().astype.append", "np.array", "ppuda.deepnets1m.graph.Graph._Adj.cpu().numpy", "str", "str", "np.array", "subprocess.check_output", "bool", "np.random.rand", "np.random.choice", "ppuda.deepnets1m.net.Network", "ppuda.deepnets1m.graph.Graph", "len", "param_name.find", "len", "np.array.min", "np.array.max", "np.array.mean", "np.array.std", "all_n_params.min", "all_n_params.max", "all_n_params.mean", "all_n_params.std", "len", "len", "np.random.rand", "n[].find", "np.random.rand", "print", "print", "param_name[].find", "ppuda.deepnets1m.graph.Graph._Adj.cpu", "time.time", "np.random.rand", "np.random.rand", "float", "len", "param_name[].find", "np.random.rand", "np.random.rand", "len"], "function", ["home.repos.pwc.inspect_result.facebookresearch_ppuda.utils.utils.set_seed", "home.repos.pwc.inspect_result.facebookresearch_ppuda.detector.utils.mkdir", "home.repos.pwc.inspect_result.facebookresearch_ppuda.experiments.net_generator.merge_eval", "home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.genotypes.sample_genotype", "home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.genotypes.to_dict", "home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.graph.GraphBatch.append", "home.repos.pwc.inspect_result.facebookresearch_ppuda.utils.utils.capacity", "home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.net.get_cell_ind", "home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.graph.GraphBatch.append", "home.repos.pwc.inspect_result.facebookresearch_ppuda.detector.utils.SmoothedValue.max", "home.repos.pwc.inspect_result.facebookresearch_ppuda.detector.utils.SmoothedValue.max"], ["def", "main", "(", ")", ":", "\n", "\n", "    ", "try", ":", "\n", "        ", "split", "=", "sys", ".", "argv", "[", "1", "]", ".", "lower", "(", ")", "\n", "N", "=", "int", "(", "sys", ".", "argv", "[", "2", "]", ")", "\n", "data_dir", "=", "sys", ".", "argv", "[", "3", "]", "\n", "", "except", ":", "\n", "        ", "print", "(", "'\\nExample of usage: python deepnets1m/net_generator.py train 1000000 ./data\\n'", ")", "\n", "raise", "\n", "\n", "", "device", "=", "'cpu'", "# no much benefit of using cuda", "\n", "\n", "print", "(", "split", ",", "N", ",", "data_dir", ",", "device", ",", "flush", "=", "True", ")", "\n", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "data_dir", ")", ":", "\n", "        ", "os", ".", "mkdir", "(", "data_dir", ")", "\n", "\n", "", "set_seed", "(", "0", "if", "split", "==", "'val'", "else", "1", ")", "\n", "\n", "min_steps", "=", "1", "\n", "medium_steps", "=", "2", "\n", "max_steps", "=", "4", "\n", "min_layers", "=", "4", "\n", "deep_layers_all", "=", "np", ".", "arange", "(", "7", ",", "11", ")", "\n", "max_layers", "=", "18", "\n", "max_params", "=", "10", "**", "7", "\n", "\n", "# for 'train', 'val', 'test' we have the same network generator", "\n", "# for 'wide' we re-use the 'test' split and increase the number of channels when evaluate the model", "\n", "# for 'bnfree' the generator is the same except that all nets have no BN", "\n", "# 'predefined' is created on the fly in the deepnets1m.loader", "\n", "\n", "if", "split", "==", "'deep'", ":", "\n", "        ", "min_layers", "=", "10", "\n", "deep_layers_all", "=", "[", "18", "]", "\n", "max_layers", "=", "36", "\n", "max_params", "=", "10", "**", "8", "\n", "", "elif", "split", "==", "'dense'", ":", "\n", "        ", "min_steps", "=", "2", "\n", "medium_steps", "=", "6", "\n", "max_steps", "=", "10", "\n", "max_params", "=", "10", "**", "8", "\n", "", "elif", "split", "==", "'search'", ":", "\n", "# allow a bit larger networks for search, since larger networks are more likely to have better final results", "\n", "        ", "medium_steps", "=", "3", "\n", "max_steps", "=", "6", "\n", "min_layers", "=", "6", "\n", "deep_layers_all", "=", "[", "10", "]", "\n", "max_layers", "=", "20", "\n", "", "else", ":", "\n", "        ", "assert", "split", "in", "[", "'train'", ",", "'val'", ",", "'test'", ",", "'wide'", ",", "'bnfree'", "]", ",", "(", "'unsupported split: %s'", "%", "split", ")", "\n", "\n", "\n", "", "try", ":", "\n", "        ", "gitcommit", "=", "subprocess", ".", "check_output", "(", "[", "'git'", ",", "'rev-parse'", ",", "'--short'", ",", "'HEAD'", "]", ")", ".", "decode", "(", "'ascii'", ")", ".", "strip", "(", ")", "\n", "print", "(", "'gitcommit:'", ",", "gitcommit", ",", "flush", "=", "True", ")", "\n", "", "except", "Exception", "as", "e", ":", "\n", "        ", "print", "(", "e", ",", "flush", "=", "True", ")", "\n", "\n", "", "start", "=", "time", ".", "time", "(", ")", "\n", "\n", "meta_data", "=", "{", "}", "\n", "meta_data", "[", "split", "]", "=", "{", "'nets'", ":", "[", "]", ",", "'meta'", ":", "{", "}", "}", "\n", "op_types", ",", "op_types_back", ",", "primitives", ",", "primitives_back", "=", "{", "}", ",", "{", "}", ",", "{", "}", ",", "{", "}", "\n", "\n", "h5_file", "=", "join", "(", "data_dir", ",", "'deepnets1m_%s.hdf5'", "%", "split", ")", "\n", "meta_file", "=", "join", "(", "data_dir", ",", "'deepnets1m_%s_meta.json'", "%", "split", ")", "\n", "\n", "for", "f", "in", "[", "h5_file", ",", "meta_file", "]", ":", "\n", "        ", "if", "os", ".", "path", ".", "exists", "(", "h5_file", ")", ":", "\n", "            ", "raise", "ValueError", "(", "'File %s already exists. The script will exit now to avoid accidental overwriting of the file.'", "%", "f", ")", "\n", "\n", "\n", "", "", "with", "h5py", ".", "File", "(", "h5_file", ",", "'w'", ")", "as", "h5_data", ":", "\n", "\n", "        ", "h5_data", ".", "attrs", "[", "'title'", "]", "=", "'DeepNets-1M'", "\n", "group", "=", "h5_data", ".", "create_group", "(", "split", ")", "\n", "\n", "while", "len", "(", "meta_data", "[", "split", "]", "[", "'nets'", "]", ")", "<", "N", ":", "\n", "\n", "            ", "layers", "=", "int", "(", "np", ".", "random", ".", "randint", "(", "min_layers", ",", "max_layers", "+", "1", ")", ")", "# number of cells in total (convert to int to make it serializable)", "\n", "deep_layers", "=", "np", ".", "random", ".", "choice", "(", "deep_layers_all", ")", "# a threshold to consider a deep network", "\n", "\n", "steps", "=", "int", "(", "np", ".", "random", ".", "randint", "(", "min_steps", ",", "(", "max_steps", "if", "layers", "<=", "deep_layers", "else", "medium_steps", ")", "+", "1", ")", ")", "# number of summation nodes in a cell", "\n", "genotype", "=", "sample_genotype", "(", "steps", "=", "steps", ",", "\n", "only_pool", "=", "bool", "(", "np", ".", "random", ".", "rand", "(", ")", ">", "0.5", ")", ",", "# True means no trainable layers in the reduction cell", "\n", "drop_concat", "=", "bool", "(", "np", ".", "random", ".", "rand", "(", ")", ">", "0.5", ")", "if", "steps", ">", "1", "else", "False", ",", "# drop some edges from the sum node to the final concat", "\n", "allow_none", "=", "steps", ">", "1", ",", "# none is the zero operation to allow sparse connections", "\n", "allow_transformer", "=", "True", ")", "# allow to sample msa", "\n", "\n", "ks", "=", "int", "(", "np", ".", "random", ".", "choice", "(", "[", "3", ",", "5", ",", "7", "]", ")", ")", "# kernel size of the first convolutional layer", "\n", "is_vit", "=", "sum", "(", "[", "n", "[", "0", "]", "==", "'msa'", "for", "n", "in", "genotype", ".", "normal", "+", "genotype", ".", "reduce", "]", ")", ">", "0", "# Visual Transformer", "\n", "is_cse", "=", "sum", "(", "[", "n", "[", "0", "]", "==", "'cse'", "for", "n", "in", "genotype", ".", "normal", "+", "genotype", ".", "reduce", "]", ")", ">", "0", "# Model with CSE", "\n", "has_none", "=", "sum", "(", "[", "n", "[", "0", "]", "==", "'none'", "for", "n", "in", "genotype", ".", "normal", "+", "genotype", ".", "reduce", "]", ")", ">", "0", "\n", "\n", "is_cse2", "=", "(", "sum", "(", "[", "n", "[", "0", "]", "==", "'cse'", "for", "n", "in", "genotype", ".", "normal", "]", ")", ">", "1", ")", "or", "(", "\n", "sum", "(", "[", "n", "[", "0", "]", "==", "'cse'", "for", "n", "in", "genotype", ".", "reduce", "]", ")", ">", "1", ")", "# training GHNs on networks with CSE often leads to NaN losses, so we will avoid them", "\n", "\n", "is_conv", "=", "sum", "(", "[", "n", "[", "0", "]", ".", "find", "(", "'conv'", ")", ">=", "0", "for", "n", "in", "genotype", ".", "normal", "+", "genotype", ".", "reduce", "]", ")", ">", "0", "# at least one simple conv op", "\n", "\n", "is_conv_large", "=", "(", "sum", "(", "[", "n", "[", "0", "]", "in", "[", "'conv_5x5'", ",", "'conv_7x7'", "]", "for", "n", "in", "genotype", ".", "normal", "]", ")", ">", "1", ")", "or", "(", "\n", "sum", "(", "[", "n", "[", "0", "]", "in", "[", "'conv_5x5'", ",", "'conv_7x7'", "]", "for", "n", "in", "genotype", ".", "reduce", "]", ")", ">", "1", ")", "# dense convolutions are memory consuming, so we will avoid them", "\n", "\n", "if", "(", "is_cse", "and", "not", "is_conv", ")", "or", "is_cse2", "or", "is_conv_large", ":", "\n", "                ", "continue", "# avoid some networks that are difficult to train or too memory consuming", "\n", "\n", "", "if", "not", "(", "is_cse", "or", "is_vit", "or", "is_conv", ")", ":", "\n", "# print('no lear layers', genotype, flush=True)", "\n", "                ", "continue", "\n", "\n", "", "C_mult", "=", "int", "(", "np", ".", "random", ".", "choice", "(", "[", "1", ",", "2", "]", ")", ")", "\n", "\n", "# Use 1x1 convolutional layers to match the channel dimensionality at the input of each cell", "\n", "if", "steps", ">", "1", "or", "C_mult", ">", "1", ":", "\n", "                ", "preproc", "=", "True", "\n", "", "else", ":", "\n", "# allow some networks without those 1x1 conv layers for diversity", "\n", "                ", "if", "split", "==", "'search'", ":", "\n", "# not sure what's the logic was here, but keep for consistency", "\n", "                    ", "preproc", "=", "bool", "(", "(", "not", "is_vit", "and", "np", ".", "random", ".", "rand", "(", ")", ">", "0.2", ")", "or", "(", "is_vit", "and", "np", ".", "random", ".", "rand", "(", ")", ">", "0.8", ")", ")", "\n", "", "else", ":", "\n", "                    ", "preproc", "=", "bool", "(", "not", "is_vit", "or", "np", ".", "random", ".", "rand", "(", ")", ">", "0.8", ")", "\n", "\n", "# Use global pooling most of the time instead of VGG-style head", "\n", "", "", "glob_avg", "=", "bool", "(", "is_vit", "or", "layers", ">", "deep_layers", "or", "np", ".", "random", ".", "rand", "(", ")", ">", "0.1", ")", "\n", "\n", "if", "split", "==", "'bnfree'", ":", "\n", "                ", "norm", "=", "None", "\n", "", "elif", "split", "==", "'search'", ":", "\n", "                ", "norm", "=", "'bnorm'", "\n", "", "else", ":", "\n", "# Allow no BN in case of shallow networks and few ops", "\n", "                ", "norm", "=", "np", ".", "random", ".", "choice", "(", "[", "'bnorm'", ",", "None", "]", ")", "if", "layers", "<=", "(", "min_layers", "+", "1", ")", "and", "steps", "<=", "medium_steps", "else", "'bnorm'", "\n", "", "stem_type", "=", "int", "(", "np", ".", "random", ".", "choice", "(", "[", "0", ",", "1", "]", ")", ")", "# style of the stem: simple or ImageNet-style from DARTS", "\n", "net_args", "=", "{", "'stem_type'", ":", "stem_type", ",", "\n", "'stem_pool'", ":", "bool", "(", "stem_type", "==", "0", "and", "np", ".", "random", ".", "rand", "(", ")", ">", "0.5", ")", ",", "# add extra pooling layer in case of a simple cell", "\n", "'norm'", ":", "norm", ",", "\n", "'preproc'", ":", "preproc", ",", "\n", "'fc_layers'", ":", "int", "(", "np", ".", "random", ".", "randint", "(", "1", ",", "3", ")", ")", ",", "# number of fully connected layers before classification", "\n", "'glob_avg'", ":", "glob_avg", ",", "\n", "'genotype'", ":", "genotype", ",", "\n", "'n_cells'", ":", "layers", ",", "\n", "'ks'", ":", "ks", ",", "\n", "'C_mult'", ":", "C_mult", ",", "\n", "'fc_dim'", ":", "256", "\n", "}", "\n", "\n", "skip", "=", "False", "\n", "graph", "=", "None", "\n", "num_params", "=", "{", "}", "\n", "\n", "for", "dset_name", "in", "[", "'cifar10'", ",", "'imagenet'", "]", ":", "\n", "\n", "                ", "model", "=", "Network", "(", "C", "=", "32", ",", "# default number of channels", "\n", "num_classes", "=", "10", ",", "# does not matter at this stage", "\n", "is_imagenet_input", "=", "dset_name", "==", "'imagenet'", ",", "\n", "**", "net_args", ")", ".", "to", "(", "device", ")", "\n", "\n", "c", ",", "n", "=", "capacity", "(", "model", ")", "\n", "num_params", "[", "dset_name", "]", "=", "n", "\n", "\n", "if", "n", ">", "max_params", ":", "\n", "                    ", "print", "(", "'too large architecture: %.2f M params \\n'", "%", "(", "float", "(", "n", ")", "/", "10", "**", "6", ")", ",", "flush", "=", "True", ")", "\n", "skip", "=", "True", "\n", "break", "\n", "\n", "", "if", "dset_name", "==", "'cifar10'", ":", "\n", "                    ", "try", ":", "\n", "                        ", "graph", "=", "Graph", "(", "model", ",", "ve_cutoff", "=", "250", ",", "list_all_nodes", "=", "True", ")", "\n", "", "except", "Exception", "as", "e", ":", "\n", "                        ", "print", "(", "'\\n%d: unable to construct the graph: it is likely to be disconnected'", "%", "len", "(", "meta_data", "[", "split", "]", "[", "'nets'", "]", ")", ",", "\n", "'has_none={}, genotype={}'", ".", "\n", "format", "(", "has_none", ",", "net_args", "[", "'genotype'", "]", ")", ",", "flush", "=", "True", ")", "\n", "print", "(", "e", ",", "'\\n'", ")", "\n", "assert", "has_none", "# to be disconnected it has to have none nodes", "\n", "skip", "=", "True", "\n", "break", "\n", "\n", "", "", "", "if", "skip", ":", "\n", "                ", "continue", "\n", "\n", "\n", "", "assert", "layers", "==", "len", "(", "graph", ".", "node_info", ")", ",", "(", "layers", ",", "len", "(", "graph", ".", "node_info", ")", ")", "\n", "cell_ind", ",", "n_nodes", ",", "nodes_array", "=", "0", ",", "0", ",", "[", "]", "\n", "for", "j", "in", "range", "(", "layers", ")", ":", "\n", "\n", "                ", "n_nodes", "+=", "len", "(", "graph", ".", "node_info", "[", "j", "]", ")", "\n", "\n", "for", "node", "in", "graph", ".", "node_info", "[", "j", "]", ":", "\n", "\n", "                    ", "param_name", ",", "name", ",", "sz", "=", "node", "[", "1", ":", "4", "]", "\n", "cell_ind_", "=", "get_cell_ind", "(", "param_name", ",", "layers", ")", "\n", "if", "cell_ind_", "is", "not", "None", ":", "\n", "                        ", "cell_ind", "=", "cell_ind_", "\n", "\n", "", "assert", "cell_ind", "==", "j", ",", "(", "cell_ind", ",", "j", ",", "node", ")", "\n", "\n", "if", "name", "==", "'conv'", "and", "(", "len", "(", "sz", ")", "==", "2", "or", "sz", "[", "2", "]", "==", "sz", "[", "3", "]", "==", "1", ")", ":", "\n", "                        ", "name", "=", "'conv_1x1'", "\n", "\n", "", "if", "name", "not", "in", "primitives", ":", "\n", "                        ", "ind", "=", "len", "(", "primitives", ")", "\n", "primitives", "[", "name", "]", "=", "ind", "\n", "primitives_back", "[", "ind", "]", "=", "name", "\n", "\n", "", "if", "param_name", ".", "startswith", "(", "'cells.'", ")", ":", "\n", "# remove cells.x. prefix", "\n", "                        ", "pos1", "=", "param_name", ".", "find", "(", "'.'", ")", "\n", "assert", "param_name", "[", "pos1", "+", "1", ":", "]", ".", "find", "(", "'.'", ")", ">=", "0", ",", "node", "\n", "pos2", "=", "pos1", "+", "param_name", "[", "pos1", "+", "1", ":", "]", ".", "find", "(", "'.'", ")", "+", "2", "\n", "param_name", "=", "param_name", "[", "pos2", ":", "]", "\n", "\n", "", "if", "param_name", "not", "in", "op_types", ":", "\n", "                        ", "ind", "=", "len", "(", "op_types", ")", "\n", "op_types", "[", "param_name", "]", "=", "ind", "\n", "op_types_back", "[", "ind", "]", "=", "param_name", "\n", "\n", "", "nodes_array", ".", "append", "(", "[", "primitives", "[", "name", "]", ",", "cell_ind", ",", "op_types", "[", "param_name", "]", "]", ")", "\n", "\n", "", "", "nodes_array", "=", "np", ".", "array", "(", "nodes_array", ")", ".", "astype", "(", "np", ".", "uint16", ")", "\n", "\n", "A", "=", "graph", ".", "_Adj", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ".", "astype", "(", "np", ".", "uint8", ")", "\n", "assert", "nodes_array", ".", "shape", "[", "0", "]", "==", "n_nodes", "==", "A", ".", "shape", "[", "0", "]", "==", "graph", ".", "n_nodes", ",", "(", "nodes_array", ".", "shape", ",", "n_nodes", ",", "A", ".", "shape", ",", "graph", ".", "n_nodes", ")", "\n", "\n", "idx", "=", "len", "(", "meta_data", "[", "split", "]", "[", "'nets'", "]", ")", "\n", "group", ".", "create_dataset", "(", "str", "(", "idx", ")", "+", "'/adj'", ",", "data", "=", "A", ")", "\n", "group", ".", "create_dataset", "(", "str", "(", "idx", ")", "+", "'/nodes'", ",", "data", "=", "nodes_array", ")", "\n", "\n", "net_args", "[", "'num_nodes'", "]", "=", "int", "(", "A", ".", "shape", "[", "0", "]", ")", "\n", "net_args", "[", "'num_params'", "]", "=", "num_params", "\n", "\n", "net_args", "[", "'genotype'", "]", "=", "to_dict", "(", "net_args", "[", "'genotype'", "]", ")", "\n", "meta_data", "[", "split", "]", "[", "'nets'", "]", ".", "append", "(", "net_args", ")", "\n", "meta_data", "[", "split", "]", "[", "'meta'", "]", "[", "'primitives_ext'", "]", "=", "primitives_back", "\n", "meta_data", "[", "split", "]", "[", "'meta'", "]", "[", "'unique_op_names'", "]", "=", "op_types_back", "\n", "\n", "if", "(", "idx", "+", "1", ")", "%", "100", "==", "0", "or", "idx", ">=", "N", "-", "1", ":", "\n", "                ", "all_n_nodes", "=", "np", ".", "array", "(", "[", "net", "[", "'num_nodes'", "]", "for", "net", "in", "meta_data", "[", "split", "]", "[", "'nets'", "]", "]", ")", "\n", "all_n_params", "=", "np", ".", "array", "(", "[", "net", "[", "'num_params'", "]", "[", "'cifar10'", "]", "for", "net", "in", "meta_data", "[", "split", "]", "[", "'nets'", "]", "]", ")", "/", "10", "**", "6", "\n", "print", "(", "'N={} nets created: \\t {}-{} nodes (mean\\u00B1std: {:.1f}\\u00B1{:.1f}) '", "\n", "'\\t {:.2f}-{:.2f} params (M) (mean\\u00B1std: {:.2f}\\u00B1{:.2f}) '", "\n", "'\\t {} unique primitives, {} unique param names '", "\n", "'\\t total time={:.2f} sec'", ".", "format", "(", "\n", "idx", "+", "1", ",", "\n", "all_n_nodes", ".", "min", "(", ")", ",", "\n", "all_n_nodes", ".", "max", "(", ")", ",", "\n", "all_n_nodes", ".", "mean", "(", ")", ",", "\n", "all_n_nodes", ".", "std", "(", ")", ",", "\n", "all_n_params", ".", "min", "(", ")", ",", "\n", "all_n_params", ".", "max", "(", ")", ",", "\n", "all_n_params", ".", "mean", "(", ")", ",", "\n", "all_n_params", ".", "std", "(", ")", ",", "\n", "len", "(", "primitives_back", ")", ",", "\n", "len", "(", "op_types_back", ")", ",", "\n", "time", ".", "time", "(", ")", "-", "start", ")", ",", "\n", "flush", "=", "True", ")", "\n", "\n", "", "", "", "with", "open", "(", "meta_file", ",", "'w'", ")", "as", "f", ":", "\n", "        ", "json", ".", "dump", "(", "meta_data", ",", "f", ")", "\n", "\n", "", "print", "(", "'saved to %s and %s'", "%", "(", "h5_file", ",", "meta_file", ")", ")", "\n", "\n", "print", "(", "'\\ndone'", ")", "\n", "\n", "if", "split", "==", "'bnfree'", ":", "\n", "        ", "merge_eval", "(", "data_dir", ")", "# assume bnfree was generated the last", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.experiments.net_generator.merge_eval": [[306, 332], ["print", "print", "print", "list", "open", "json.dump", "h5py.File", "open", "print", "meta_new.keys", "os.path.join", "os.path.join", "os.path.join", "json.load", "len", "len", "len", "h5py.File", "h5_data.create_group", "range", "os.path.join", "len", "h5_data.create_group.create_dataset", "h5_data.create_group.create_dataset", "print", "str", "str", "len", "str", "str"], "function", ["home.repos.pwc.inspect_result.facebookresearch_ppuda.ghn.nn.GHN.load"], ["", "", "def", "merge_eval", "(", "data_dir", ")", ":", "\n", "\n", "    ", "print", "(", "'merging the evaluation splits into one file'", ")", "\n", "\n", "meta_new", "=", "{", "}", "\n", "for", "split", "in", "[", "'val'", ",", "'test'", ",", "'wide'", ",", "'deep'", ",", "'dense'", ",", "'bnfree'", "]", ":", "\n", "        ", "with", "open", "(", "join", "(", "data_dir", ",", "'deepnets1m_%s_meta.json'", "%", "split", ")", ",", "'r'", ")", "as", "f", ":", "\n", "            ", "meta_new", "[", "split", "]", "=", "json", ".", "load", "(", "f", ")", "[", "split", "]", "\n", "print", "(", "split", ",", "len", "(", "meta_new", "[", "split", "]", ")", ",", "len", "(", "meta_new", "[", "split", "]", "[", "'meta'", "]", ")", ",", "len", "(", "meta_new", "[", "split", "]", "[", "'nets'", "]", ")", ")", "\n", "", "", "print", "(", "list", "(", "meta_new", ".", "keys", "(", ")", ")", ")", "\n", "with", "open", "(", "join", "(", "data_dir", ",", "'deepnets1m_eval_meta.json'", ")", ",", "'w'", ")", "as", "f", ":", "\n", "        ", "json", ".", "dump", "(", "meta_new", ",", "f", ")", "\n", "\n", "\n", "", "with", "h5py", ".", "File", "(", "join", "(", "data_dir", ",", "'deepnets1m_eval.hdf5'", ")", ",", "\"w\"", ")", "as", "h5_data", ":", "\n", "        ", "for", "split", "in", "[", "'val'", ",", "'test'", ",", "'wide'", ",", "'deep'", ",", "'dense'", ",", "'bnfree'", "]", ":", "\n", "            ", "with", "h5py", ".", "File", "(", "join", "(", "data_dir", ",", "'deepnets1m_%s.hdf5'", "%", "split", ")", ",", "\"r\"", ")", "as", "data_file", ":", "\n", "                ", "h5_data", ".", "attrs", "[", "'title'", "]", "=", "'DeepNets-1M'", "\n", "group", "=", "h5_data", ".", "create_group", "(", "split", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "data_file", "[", "split", "]", ")", ")", ":", "\n", "                    ", "A", ",", "nodes", "=", "data_file", "[", "split", "]", "[", "str", "(", "i", ")", "]", "[", "'adj'", "]", "[", "(", ")", "]", ",", "data_file", "[", "split", "]", "[", "str", "(", "i", ")", "]", "[", "'nodes'", "]", "[", "(", ")", "]", "\n", "group", ".", "create_dataset", "(", "str", "(", "i", ")", "+", "'/adj'", ",", "data", "=", "A", ")", "\n", "group", ".", "create_dataset", "(", "str", "(", "i", ")", "+", "'/nodes'", ",", "data", "=", "nodes", ")", "\n", "if", "i", "==", "0", ":", "\n", "                        ", "print", "(", "split", ",", "len", "(", "data_file", "[", "split", "]", ")", ",", "A", ".", "dtype", ",", "nodes", ".", "dtype", ")", "\n", "", "", "", "", "", "print", "(", "'\\ndone'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.experiments.train_ghn.main": [[29, 184], ["ppuda.config.init_config", "ppuda.vision.loader.image_loader", "ppuda.deepnets1m.loader.DeepNets1M.loader", "ppuda.ghn.nn.GHN().to", "torch.optim.Adam", "torch.optim.lr_scheduler.MultiStepLR", "ppuda.utils.Trainer", "set", "print", "range", "print", "torch.load", "ppuda.ghn.nn.ghn_parallel.load_state_dict", "ppuda.ghn.nn.ghn_parallel", "ppuda.ghn.nn.ghn_parallel.parameters", "print", "ppuda.utils.Trainer.reset", "ppuda.ghn.nn.ghn_parallel.train", "enumerate", "print", "torch.optim.lr_scheduler.MultiStepLR.step", "ppuda.ghn.nn.GHN", "print", "torch.optim.Adam.load_state_dict", "torch.optim.lr_scheduler.MultiStepLR.step", "len", "os.path.join", "torch.save", "print", "print", "ppuda.utils.capacity", "torch.zeros", "torch.cuda.empty_cache", "len", "torch.load.keys", "torch.optim.lr_scheduler.MultiStepLR.get_last_lr", "next", "ppuda.utils.Trainer.update", "ppuda.utils.Trainer.log", "torch.optim.Adam.state_dict", "ppuda.utils.capacity", "ppuda.deepnets1m.net.Network", "nets_torch.append", "set.add", "print", "type", "str().find", "torch.isnan", "str().find", "print", "print", "ppuda.ghn.nn.ghn_parallel.to", "torch.cuda.empty_cache", "ppuda.ghn.nn.ghn_parallel.to", "str", "len", "ppuda.ghn.nn.ghn_parallel", "str"], "function", ["home.repos.pwc.inspect_result.facebookresearch_ppuda.ppuda.config.init_config", "home.repos.pwc.inspect_result.facebookresearch_ppuda.vision.loader.image_loader", "home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.loader.DeepNets1M.loader", "home.repos.pwc.inspect_result.facebookresearch_ppuda.ghn.nn.GHN.load", "home.repos.pwc.inspect_result.facebookresearch_ppuda.ghn.nn.ghn_parallel", "home.repos.pwc.inspect_result.facebookresearch_ppuda.utils.trainer.Trainer.reset", "home.repos.pwc.inspect_result.facebookresearch_ppuda.utils.utils.capacity", "home.repos.pwc.inspect_result.facebookresearch_ppuda.utils.trainer.Trainer.update", "home.repos.pwc.inspect_result.facebookresearch_ppuda.utils.trainer.Trainer.log", "home.repos.pwc.inspect_result.facebookresearch_ppuda.utils.utils.capacity", "home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.graph.GraphBatch.append", "home.repos.pwc.inspect_result.facebookresearch_ppuda.ghn.nn.ghn_parallel"], ["def", "main", "(", ")", ":", "\n", "\n", "    ", "args", "=", "init_config", "(", "mode", "=", "'train_ghn'", ")", "\n", "\n", "train_queue", ",", "val_queue", ",", "num_classes", "=", "image_loader", "(", "args", ".", "dataset", ",", "\n", "args", ".", "data_dir", ",", "\n", "test", "=", "False", ",", "\n", "batch_size", "=", "args", ".", "batch_size", ",", "\n", "test_batch_size", "=", "args", ".", "test_batch_size", ",", "\n", "num_workers", "=", "args", ".", "num_workers", ",", "\n", "seed", "=", "args", ".", "seed", ")", "\n", "\n", "is_imagenet", "=", "args", ".", "dataset", "==", "'imagenet'", "\n", "graphs_queue", "=", "DeepNets1M", ".", "loader", "(", "args", ".", "meta_batch_size", ",", "\n", "split", "=", "args", ".", "split", ",", "\n", "nets_dir", "=", "args", ".", "data_dir", ",", "\n", "virtual_edges", "=", "args", ".", "virtual_edges", ",", "\n", "num_nets", "=", "args", ".", "num_nets", ",", "\n", "large_images", "=", "is_imagenet", ")", "\n", "\n", "\n", "start_epoch", "=", "0", "\n", "state_dict", "=", "None", "\n", "if", "args", ".", "ckpt", "is", "not", "None", ":", "\n", "# Load config and GHN parameters from existing checkpoint", "\n", "        ", "state_dict", "=", "torch", ".", "load", "(", "args", ".", "ckpt", ",", "map_location", "=", "args", ".", "device", ")", "\n", "config", "=", "state_dict", "[", "'config'", "]", "\n", "", "else", ":", "\n", "        ", "config", "=", "{", "}", "\n", "config", "[", "'max_shape'", "]", "=", "args", ".", "max_shape", "\n", "config", "[", "'num_classes'", "]", "=", "num_classes", "\n", "config", "[", "'hypernet'", "]", "=", "args", ".", "hypernet", "\n", "config", "[", "'decoder'", "]", "=", "args", ".", "decoder", "\n", "config", "[", "'weight_norm'", "]", "=", "args", ".", "weight_norm", "\n", "config", "[", "'ve'", "]", "=", "args", ".", "virtual_edges", ">", "1", "\n", "config", "[", "'layernorm'", "]", "=", "args", ".", "ln", "\n", "config", "[", "'hid'", "]", "=", "args", ".", "hid", "\n", "\n", "\n", "", "ghn", "=", "GHN", "(", "**", "config", ",", "\n", "debug_level", "=", "args", ".", "debug", ")", ".", "to", "(", "args", ".", "device", ")", "\n", "\n", "if", "state_dict", "is", "not", "None", ":", "\n", "        ", "ghn", ".", "load_state_dict", "(", "state_dict", "[", "'state_dict'", "]", ")", "\n", "if", "args", ".", "debug", ":", "\n", "            ", "print", "(", "'GHN with {} parameters loaded from epoch {}.'", ".", "format", "(", "capacity", "(", "ghn", ")", "[", "1", "]", ",", "state_dict", "[", "'epoch'", "]", ")", ")", "\n", "", "start_epoch", "=", "state_dict", "[", "'epoch'", "]", "+", "1", "# resume from the next epoch", "\n", "\n", "\n", "", "if", "args", ".", "multigpu", ":", "\n", "        ", "ghn", "=", "ghn_parallel", "(", "ghn", ")", "\n", "\n", "\n", "", "optimizer", "=", "torch", ".", "optim", ".", "Adam", "(", "ghn", ".", "parameters", "(", ")", ",", "args", ".", "lr", ",", "weight_decay", "=", "args", ".", "wd", ")", "\n", "scheduler", "=", "MultiStepLR", "(", "optimizer", ",", "milestones", "=", "args", ".", "lr_steps", ",", "gamma", "=", "args", ".", "gamma", ")", "\n", "\n", "if", "state_dict", "is", "not", "None", ":", "\n", "        ", "try", ":", "\n", "            ", "optimizer", ".", "load_state_dict", "(", "state_dict", "[", "'optimzer'", "]", ")", "\n", "", "except", "Exception", "as", "e", ":", "\n", "            ", "print", "(", "'WARNING: optimizer is not available in the checkpoint, available keys: '", ",", "state_dict", ".", "keys", "(", ")", ",", "'error:'", ",", "e", ",", "'\\n'", ")", "\n", "\n", "", "if", "start_epoch", ">", "0", ":", "\n", "            ", "scheduler", ".", "step", "(", "start_epoch", ")", "\n", "\n", "", "", "trainer", "=", "Trainer", "(", "optimizer", ",", "\n", "num_classes", ",", "\n", "is_imagenet", ",", "\n", "n_batches", "=", "len", "(", "train_queue", ")", ",", "\n", "grad_clip", "=", "args", ".", "grad_clip", ",", "\n", "device", "=", "ghn", ".", "device_ids", "if", "args", ".", "multigpu", "else", "args", ".", "device", ",", "\n", "log_interval", "=", "args", ".", "log_interval", ",", "\n", "amp", "=", "args", ".", "amp", ")", "\n", "\n", "\n", "seen_nets", "=", "set", "(", ")", "\n", "\n", "print", "(", "'\\nStarting training GHN with {} parameters!'", ".", "format", "(", "capacity", "(", "ghn", ")", "[", "1", "]", ")", ")", "\n", "\n", "for", "epoch", "in", "range", "(", "start_epoch", ",", "args", ".", "epochs", ")", ":", "\n", "\n", "        ", "print", "(", "'\\nepoch={:03d}/{:03d}, lr={:e}'", ".", "format", "(", "epoch", "+", "1", ",", "args", ".", "epochs", ",", "scheduler", ".", "get_last_lr", "(", ")", "[", "0", "]", ")", ")", "\n", "\n", "trainer", ".", "reset", "(", ")", "\n", "ghn", ".", "train", "(", ")", "\n", "failed_batches", "=", "0", "\n", "\n", "for", "step", ",", "(", "images", ",", "targets", ")", "in", "enumerate", "(", "train_queue", ")", ":", "\n", "\n", "            ", "upd", ",", "loss", "=", "False", ",", "torch", ".", "zeros", "(", "1", ",", "device", "=", "args", ".", "device", ")", "\n", "while", "not", "upd", ":", "\n", "                ", "try", ":", "\n", "                    ", "graphs", "=", "next", "(", "graphs_queue", ")", "\n", "\n", "nets_torch", "=", "[", "]", "\n", "\n", "for", "nets_args", "in", "graphs", ".", "net_args", ":", "\n", "                        ", "net", "=", "Network", "(", "is_imagenet_input", "=", "is_imagenet", ",", "\n", "num_classes", "=", "num_classes", ",", "\n", "compress_params", "=", "True", ",", "\n", "**", "nets_args", ")", "\n", "nets_torch", ".", "append", "(", "net", ")", "\n", "\n", "", "loss", "=", "trainer", ".", "update", "(", "nets_torch", ",", "images", ",", "targets", ",", "ghn", "=", "ghn", ",", "graphs", "=", "graphs", ")", "\n", "trainer", ".", "log", "(", ")", "\n", "\n", "for", "ind", "in", "graphs", ".", "net_inds", ":", "\n", "                        ", "seen_nets", ".", "add", "(", "ind", ")", "\n", "", "upd", "=", "True", "\n", "\n", "", "except", "RuntimeError", "as", "e", ":", "\n", "                    ", "print", "(", "'error'", ",", "type", "(", "e", ")", ",", "e", ")", "\n", "oom", "=", "str", "(", "e", ")", ".", "find", "(", "'out of memory'", ")", ">=", "0", "\n", "is_nan", "=", "torch", ".", "isnan", "(", "loss", ")", "or", "str", "(", "e", ")", ".", "find", "(", "'the loss is'", ")", ">=", "0", "\n", "if", "oom", "or", "is_nan", ":", "\n", "                        ", "if", "failed_batches", ">", "len", "(", "train_queue", ")", "//", "50", ":", "\n", "                            ", "print", "(", "'Out of patience (after %d attempts to continue), '", "\n", "'please restart the job with another seed !!!'", "%", "failed_batches", ")", "\n", "raise", "\n", "\n", "", "if", "oom", ":", "\n", "                            ", "print", "(", "'CUDA out of memory, attempt to clean memory #%d'", "%", "failed_batches", ")", "\n", "if", "args", ".", "multigpu", ":", "\n", "                                ", "ghn", "=", "ghn", ".", "module", "\n", "", "ghn", ".", "to", "(", "'cpu'", ")", "\n", "torch", ".", "cuda", ".", "empty_cache", "(", ")", "\n", "ghn", ".", "to", "(", "args", ".", "device", ")", "\n", "if", "args", ".", "multigpu", ":", "\n", "                                ", "ghn", "=", "ghn_parallel", "(", "ghn", ")", "\n", "\n", "", "", "failed_batches", "+=", "1", "\n", "\n", "", "else", ":", "\n", "                        ", "raise", "\n", "\n", "", "", "", "del", "images", ",", "targets", ",", "graphs", ",", "nets_torch", ",", "loss", "\n", "if", "step", "%", "10", "==", "0", ":", "\n", "                ", "torch", ".", "cuda", ".", "empty_cache", "(", ")", "\n", "\n", "", "", "if", "args", ".", "save", ":", "\n", "# Save config necessary to restore GHN configuration when evaluating it", "\n", "            ", "checkpoint_path", "=", "os", ".", "path", ".", "join", "(", "args", ".", "save", ",", "'ghn.pt'", ")", "\n", "torch", ".", "save", "(", "{", "'state_dict'", ":", "(", "ghn", ".", "module", "if", "args", ".", "multigpu", "else", "ghn", ")", ".", "state_dict", "(", ")", ",", "\n", "'optimzer'", ":", "optimizer", ".", "state_dict", "(", ")", ",", "\n", "'epoch'", ":", "epoch", ",", "\n", "'config'", ":", "config", "}", ",", "checkpoint_path", ")", "\n", "print", "(", "'\\nsaved the checkpoint to {}'", ".", "format", "(", "checkpoint_path", ")", ")", "\n", "\n", "", "print", "(", "'{} unique architectures seen'", ".", "format", "(", "len", "(", "seen_nets", ")", ")", ")", "\n", "\n", "scheduler", ".", "step", "(", ")", "\n", "\n", "# Evaluation is done in a separate script: eval_ghn.py", "\n", "\n", "", "print", "(", "'done!'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.sgd.train_net.main": [[41, 139], ["ppuda.config.init_config", "ppuda.vision.loader.image_loader", "isinstance", "ppuda.utils.pretrained_model.train().to", "print", "torch.optim.SGD", "torch.optim.SGD", "ppuda.utils.Trainer", "range", "eval", "ppuda.utils.adjust_net", "ppuda.deepnets1m.net.Network", "isinstance", "ppuda.utils.pretrained_model", "ppuda.utils.pretrained_model.parameters", "torch.optim.lr_scheduler.StepLR", "torch.optim.lr_scheduler.StepLR", "torch.optim.lr_scheduler.CosineAnnealingLR", "torch.optim.lr_scheduler.CosineAnnealingLR", "max", "ppuda.utils.infer", "torch.optim.lr_scheduler.CosineAnnealingLR.step", "ppuda.deepnets1m.loader.DeepNets1M", "eval", "ppuda.utils.pretrained_model.train", "len", "print", "ppuda.utils.Trainer.reset", "ppuda.utils.pretrained_model.train", "ppuda.utils.pretrained_model.eval", "int", "len", "ppuda.utils.capacity", "ppuda.utils.Trainer.update", "ppuda.utils.Trainer.log", "os.path.join", "torch.save", "torch.save", "print", "torch.optim.lr_scheduler.CosineAnnealingLR.get_last_lr", "ppuda.utils.pretrained_model.state_dict"], "function", ["home.repos.pwc.inspect_result.facebookresearch_ppuda.ppuda.config.init_config", "home.repos.pwc.inspect_result.facebookresearch_ppuda.vision.loader.image_loader", "home.repos.pwc.inspect_result.facebookresearch_ppuda.utils.utils.adjust_net", "home.repos.pwc.inspect_result.facebookresearch_ppuda.utils.utils.pretrained_model", "home.repos.pwc.inspect_result.facebookresearch_ppuda.detector.utils.SmoothedValue.max", "home.repos.pwc.inspect_result.facebookresearch_ppuda.utils.utils.infer", "home.repos.pwc.inspect_result.facebookresearch_ppuda.utils.trainer.Trainer.reset", "home.repos.pwc.inspect_result.facebookresearch_ppuda.utils.utils.capacity", "home.repos.pwc.inspect_result.facebookresearch_ppuda.utils.trainer.Trainer.update", "home.repos.pwc.inspect_result.facebookresearch_ppuda.utils.trainer.Trainer.log"], ["def", "main", "(", ")", ":", "\n", "\n", "    ", "args", "=", "init_config", "(", "mode", "=", "'train_net'", ")", "\n", "\n", "is_imagenet", "=", "args", ".", "dataset", "==", "'imagenet'", "\n", "train_queue", ",", "valid_queue", ",", "num_classes", "=", "image_loader", "(", "dataset", "=", "args", ".", "dataset", ",", "\n", "data_dir", "=", "args", ".", "data_dir", ",", "\n", "test", "=", "True", ",", "\n", "load_train_anyway", "=", "True", ",", "\n", "batch_size", "=", "args", ".", "batch_size", ",", "\n", "test_batch_size", "=", "args", ".", "test_batch_size", ",", "\n", "num_workers", "=", "args", ".", "num_workers", ",", "\n", "cutout", "=", "args", ".", "cutout", ",", "\n", "cutout_length", "=", "args", ".", "cutout_length", ",", "\n", "seed", "=", "args", ".", "seed", ",", "\n", "noise", "=", "args", ".", "noise", ",", "\n", "n_shots", "=", "args", ".", "n_shots", ")", "\n", "\n", "\n", "assert", "args", ".", "arch", "is", "not", "None", ",", "'architecture genotype/index must be specified'", "\n", "\n", "try", ":", "\n", "        ", "genotype", "=", "eval", "(", "'genotypes.%s'", "%", "args", ".", "arch", ")", "\n", "net_args", "=", "{", "'C'", ":", "args", ".", "init_channels", ",", "\n", "'genotype'", ":", "genotype", ",", "\n", "'n_cells'", ":", "args", ".", "layers", ",", "\n", "'C_mult'", ":", "int", "(", "genotype", "!=", "ViT", ")", "+", "1", ",", "# assume either ViT or DARTS-style architecture", "\n", "'preproc'", ":", "genotype", "!=", "ViT", ",", "\n", "'stem_type'", ":", "1", "}", "# assume that the ImageNet-style stem is used by default", "\n", "", "except", ":", "\n", "        ", "deepnets", "=", "DeepNets1M", "(", "split", "=", "args", ".", "split", ",", "\n", "nets_dir", "=", "args", ".", "data_dir", ",", "\n", "large_images", "=", "is_imagenet", ",", "\n", "arch", "=", "args", ".", "arch", ")", "\n", "assert", "len", "(", "deepnets", ")", "==", "1", ",", "'one architecture must be chosen to train'", "\n", "graph", "=", "deepnets", "[", "0", "]", "\n", "net_args", ",", "idx", "=", "graph", ".", "net_args", ",", "graph", ".", "net_idx", "\n", "if", "'norm'", "in", "net_args", "and", "net_args", "[", "'norm'", "]", "==", "'bn'", ":", "\n", "            ", "net_args", "[", "'norm'", "]", "=", "'bn-track'", "\n", "", "", "if", "isinstance", "(", "net_args", "[", "'genotype'", "]", ",", "str", ")", ":", "\n", "        ", "model", "=", "adjust_net", "(", "eval", "(", "'torchvision.models.%s(pretrained=%d)'", "%", "(", "net_args", "[", "'genotype'", "]", ",", "args", ".", "pretrained", ")", ")", ",", "is_imagenet", ")", "\n", "", "else", ":", "\n", "        ", "model", "=", "Network", "(", "num_classes", "=", "num_classes", ",", "\n", "is_imagenet_input", "=", "is_imagenet", ",", "\n", "auxiliary", "=", "args", ".", "auxiliary", ",", "\n", "**", "net_args", ")", "\n", "\n", "", "if", "args", ".", "ckpt", "is", "not", "None", "or", "isinstance", "(", "model", ",", "torchvision", ".", "models", ".", "ResNet", ")", ":", "\n", "        ", "model", "=", "pretrained_model", "(", "model", ",", "args", ".", "ckpt", ",", "num_classes", ",", "args", ".", "debug", ",", "GHN", ")", "\n", "\n", "", "model", "=", "model", ".", "train", "(", ")", ".", "to", "(", "args", ".", "device", ")", "\n", "\n", "print", "(", "'\\nTraining arch={} with {} parameters'", ".", "format", "(", "args", ".", "arch", ",", "capacity", "(", "model", ")", "[", "1", "]", ")", ")", "\n", "\n", "optimizer", "=", "torch", ".", "optim", ".", "SGD", "(", "\n", "model", ".", "parameters", "(", ")", ",", "\n", "args", ".", "lr", ",", "\n", "momentum", "=", "args", ".", "momentum", ",", "\n", "weight_decay", "=", "args", ".", "wd", "\n", ")", "\n", "\n", "if", "is_imagenet", ":", "\n", "        ", "scheduler", "=", "torch", ".", "optim", ".", "lr_scheduler", ".", "StepLR", "(", "optimizer", ",", "1", ",", "0.97", ")", "\n", "", "else", ":", "\n", "        ", "scheduler", "=", "torch", ".", "optim", ".", "lr_scheduler", ".", "CosineAnnealingLR", "(", "optimizer", ",", "args", ".", "epochs", ")", "\n", "\n", "", "trainer", "=", "Trainer", "(", "optimizer", ",", "\n", "num_classes", ",", "\n", "is_imagenet", ",", "\n", "n_batches", "=", "len", "(", "train_queue", ")", ",", "\n", "grad_clip", "=", "args", ".", "grad_clip", ",", "\n", "auxiliary", "=", "args", ".", "auxiliary", ",", "\n", "auxiliary_weight", "=", "args", ".", "auxiliary_weight", ",", "\n", "device", "=", "args", ".", "device", ",", "\n", "log_interval", "=", "args", ".", "log_interval", ",", "\n", "amp", "=", "args", ".", "amp", ")", "\n", "\n", "for", "epoch", "in", "range", "(", "max", "(", "1", ",", "args", ".", "epochs", ")", ")", ":", "# if args.epochs=0, then just evaluate the model", "\n", "\n", "        ", "if", "args", ".", "epochs", ">", "0", ":", "\n", "            ", "print", "(", "'\\nepoch={:03d}/{:03d}, lr={:e}'", ".", "format", "(", "epoch", "+", "1", ",", "args", ".", "epochs", ",", "scheduler", ".", "get_last_lr", "(", ")", "[", "0", "]", ")", ")", "\n", "model", ".", "drop_path_prob", "=", "args", ".", "drop_path_prob", "*", "epoch", "/", "args", ".", "epochs", "\n", "\n", "trainer", ".", "reset", "(", ")", "\n", "model", ".", "train", "(", ")", "\n", "for", "images", ",", "targets", "in", "train_queue", ":", "\n", "                ", "trainer", ".", "update", "(", "model", ",", "images", ",", "targets", ")", "\n", "trainer", ".", "log", "(", ")", "\n", "\n", "", "if", "args", ".", "save", ":", "\n", "                ", "checkpoint_path", "=", "os", ".", "path", ".", "join", "(", "args", ".", "save", ",", "'checkpoint.pt'", ")", "\n", "torch", ".", "save", "(", "{", "'state_dict'", ":", "model", ".", "state_dict", "(", ")", ",", "'epoch'", ":", "epoch", "}", ",", "checkpoint_path", ")", "\n", "print", "(", "'\\nsaved the checkpoint to {}'", ".", "format", "(", "checkpoint_path", ")", ")", "\n", "\n", "\n", "", "", "infer", "(", "model", ".", "eval", "(", ")", ",", "valid_queue", ",", "verbose", "=", "True", ")", "\n", "\n", "scheduler", ".", "step", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.detector.coco_eval.CocoEvaluator.__init__": [[26, 38], ["isinstance", "copy.deepcopy", "pycocotools.cocoeval.COCOeval"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "coco_gt", ",", "iou_types", ")", ":", "\n", "        ", "assert", "isinstance", "(", "iou_types", ",", "(", "list", ",", "tuple", ")", ")", "\n", "coco_gt", "=", "copy", ".", "deepcopy", "(", "coco_gt", ")", "\n", "self", ".", "coco_gt", "=", "coco_gt", "\n", "\n", "self", ".", "iou_types", "=", "iou_types", "\n", "self", ".", "coco_eval", "=", "{", "}", "\n", "for", "iou_type", "in", "iou_types", ":", "\n", "            ", "self", ".", "coco_eval", "[", "iou_type", "]", "=", "COCOeval", "(", "coco_gt", ",", "iouType", "=", "iou_type", ")", "\n", "\n", "", "self", ".", "img_ids", "=", "[", "]", "\n", "self", ".", "eval_imgs", "=", "{", "k", ":", "[", "]", "for", "k", "in", "iou_types", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.detector.coco_eval.CocoEvaluator.update": [[39, 53], ["list", "coco_eval.CocoEvaluator.img_ids.extend", "numpy.unique", "coco_eval.CocoEvaluator.prepare", "list", "coco_eval.evaluate", "coco_eval.CocoEvaluator.eval_imgs[].append", "list", "coco_eval.loadRes", "pycocotools.coco.COCO", "predictions.keys"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_ppuda.detector.coco_eval.CocoEvaluator.prepare", "home.repos.pwc.inspect_result.facebookresearch_ppuda.detector.engine.evaluate", "home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.graph.GraphBatch.append", "home.repos.pwc.inspect_result.facebookresearch_ppuda.detector.coco_eval.loadRes"], ["", "def", "update", "(", "self", ",", "predictions", ")", ":", "\n", "        ", "img_ids", "=", "list", "(", "np", ".", "unique", "(", "list", "(", "predictions", ".", "keys", "(", ")", ")", ")", ")", "\n", "self", ".", "img_ids", ".", "extend", "(", "img_ids", ")", "\n", "\n", "for", "iou_type", "in", "self", ".", "iou_types", ":", "\n", "            ", "results", "=", "self", ".", "prepare", "(", "predictions", ",", "iou_type", ")", "\n", "coco_dt", "=", "loadRes", "(", "self", ".", "coco_gt", ",", "results", ")", "if", "results", "else", "COCO", "(", ")", "\n", "coco_eval", "=", "self", ".", "coco_eval", "[", "iou_type", "]", "\n", "\n", "coco_eval", ".", "cocoDt", "=", "coco_dt", "\n", "coco_eval", ".", "params", ".", "imgIds", "=", "list", "(", "img_ids", ")", "\n", "img_ids", ",", "eval_imgs", "=", "evaluate", "(", "coco_eval", ")", "\n", "\n", "self", ".", "eval_imgs", "[", "iou_type", "]", ".", "append", "(", "eval_imgs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.detector.coco_eval.CocoEvaluator.synchronize_between_processes": [[54, 58], ["numpy.concatenate", "coco_eval.create_common_coco_eval"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_ppuda.detector.coco_eval.create_common_coco_eval"], ["", "", "def", "synchronize_between_processes", "(", "self", ")", ":", "\n", "        ", "for", "iou_type", "in", "self", ".", "iou_types", ":", "\n", "            ", "self", ".", "eval_imgs", "[", "iou_type", "]", "=", "np", ".", "concatenate", "(", "self", ".", "eval_imgs", "[", "iou_type", "]", ",", "2", ")", "\n", "create_common_coco_eval", "(", "self", ".", "coco_eval", "[", "iou_type", "]", ",", "self", ".", "img_ids", ",", "self", ".", "eval_imgs", "[", "iou_type", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.detector.coco_eval.CocoEvaluator.accumulate": [[59, 62], ["coco_eval.CocoEvaluator.coco_eval.values", "coco_eval.accumulate"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_ppuda.detector.coco_eval.CocoEvaluator.accumulate"], ["", "", "def", "accumulate", "(", "self", ")", ":", "\n", "        ", "for", "coco_eval", "in", "self", ".", "coco_eval", ".", "values", "(", ")", ":", "\n", "            ", "coco_eval", ".", "accumulate", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.detector.coco_eval.CocoEvaluator.summarize": [[63, 67], ["coco_eval.CocoEvaluator.coco_eval.items", "print", "coco_eval.summarize"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_ppuda.detector.coco_eval.CocoEvaluator.summarize"], ["", "", "def", "summarize", "(", "self", ")", ":", "\n", "        ", "for", "iou_type", ",", "coco_eval", "in", "self", ".", "coco_eval", ".", "items", "(", ")", ":", "\n", "            ", "print", "(", "\"IoU metric: {}\"", ".", "format", "(", "iou_type", ")", ")", "\n", "coco_eval", ".", "summarize", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.detector.coco_eval.CocoEvaluator.prepare": [[68, 77], ["coco_eval.CocoEvaluator.prepare_for_coco_detection", "coco_eval.CocoEvaluator.prepare_for_coco_segmentation", "coco_eval.CocoEvaluator.prepare_for_coco_keypoint", "ValueError"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_ppuda.detector.coco_eval.CocoEvaluator.prepare_for_coco_detection", "home.repos.pwc.inspect_result.facebookresearch_ppuda.detector.coco_eval.CocoEvaluator.prepare_for_coco_segmentation", "home.repos.pwc.inspect_result.facebookresearch_ppuda.detector.coco_eval.CocoEvaluator.prepare_for_coco_keypoint"], ["", "", "def", "prepare", "(", "self", ",", "predictions", ",", "iou_type", ")", ":", "\n", "        ", "if", "iou_type", "==", "\"bbox\"", ":", "\n", "            ", "return", "self", ".", "prepare_for_coco_detection", "(", "predictions", ")", "\n", "", "elif", "iou_type", "==", "\"segm\"", ":", "\n", "            ", "return", "self", ".", "prepare_for_coco_segmentation", "(", "predictions", ")", "\n", "", "elif", "iou_type", "==", "\"keypoints\"", ":", "\n", "            ", "return", "self", ".", "prepare_for_coco_keypoint", "(", "predictions", ")", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"Unknown iou type {}\"", ".", "format", "(", "iou_type", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.detector.coco_eval.CocoEvaluator.prepare_for_coco_detection": [[78, 101], ["predictions.items", "convert_to_xywh().tolist", "prediction[].tolist", "prediction[].tolist", "coco_results.extend", "len", "coco_eval.convert_to_xywh", "enumerate"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_ppuda.detector.coco_eval.convert_to_xywh"], ["", "", "def", "prepare_for_coco_detection", "(", "self", ",", "predictions", ")", ":", "\n", "        ", "coco_results", "=", "[", "]", "\n", "for", "original_id", ",", "prediction", "in", "predictions", ".", "items", "(", ")", ":", "\n", "            ", "if", "len", "(", "prediction", ")", "==", "0", ":", "\n", "                ", "continue", "\n", "\n", "", "boxes", "=", "prediction", "[", "\"boxes\"", "]", "\n", "boxes", "=", "convert_to_xywh", "(", "boxes", ")", ".", "tolist", "(", ")", "\n", "scores", "=", "prediction", "[", "\"scores\"", "]", ".", "tolist", "(", ")", "\n", "labels", "=", "prediction", "[", "\"labels\"", "]", ".", "tolist", "(", ")", "\n", "\n", "coco_results", ".", "extend", "(", "\n", "[", "\n", "{", "\n", "\"image_id\"", ":", "original_id", ",", "\n", "\"category_id\"", ":", "labels", "[", "k", "]", ",", "\n", "\"bbox\"", ":", "box", ",", "\n", "\"score\"", ":", "scores", "[", "k", "]", ",", "\n", "}", "\n", "for", "k", ",", "box", "in", "enumerate", "(", "boxes", ")", "\n", "]", "\n", ")", "\n", "", "return", "coco_results", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.detector.coco_eval.CocoEvaluator.prepare_for_coco_segmentation": [[102, 136], ["predictions.items", "prediction[].tolist", "prediction[].tolist", "coco_results.extend", "len", "rle[].decode", "pycocotools.encode", "numpy.array", "enumerate"], "methods", ["None"], ["", "def", "prepare_for_coco_segmentation", "(", "self", ",", "predictions", ")", ":", "\n", "        ", "coco_results", "=", "[", "]", "\n", "for", "original_id", ",", "prediction", "in", "predictions", ".", "items", "(", ")", ":", "\n", "            ", "if", "len", "(", "prediction", ")", "==", "0", ":", "\n", "                ", "continue", "\n", "\n", "", "scores", "=", "prediction", "[", "\"scores\"", "]", "\n", "labels", "=", "prediction", "[", "\"labels\"", "]", "\n", "masks", "=", "prediction", "[", "\"masks\"", "]", "\n", "\n", "masks", "=", "masks", ">", "0.5", "\n", "\n", "scores", "=", "prediction", "[", "\"scores\"", "]", ".", "tolist", "(", ")", "\n", "labels", "=", "prediction", "[", "\"labels\"", "]", ".", "tolist", "(", ")", "\n", "\n", "rles", "=", "[", "\n", "mask_util", ".", "encode", "(", "np", ".", "array", "(", "mask", "[", "0", ",", ":", ",", ":", ",", "np", ".", "newaxis", "]", ",", "dtype", "=", "np", ".", "uint8", ",", "order", "=", "\"F\"", ")", ")", "[", "0", "]", "\n", "for", "mask", "in", "masks", "\n", "]", "\n", "for", "rle", "in", "rles", ":", "\n", "                ", "rle", "[", "\"counts\"", "]", "=", "rle", "[", "\"counts\"", "]", ".", "decode", "(", "\"utf-8\"", ")", "\n", "\n", "", "coco_results", ".", "extend", "(", "\n", "[", "\n", "{", "\n", "\"image_id\"", ":", "original_id", ",", "\n", "\"category_id\"", ":", "labels", "[", "k", "]", ",", "\n", "\"segmentation\"", ":", "rle", ",", "\n", "\"score\"", ":", "scores", "[", "k", "]", ",", "\n", "}", "\n", "for", "k", ",", "rle", "in", "enumerate", "(", "rles", ")", "\n", "]", "\n", ")", "\n", "", "return", "coco_results", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.detector.coco_eval.CocoEvaluator.prepare_for_coco_keypoint": [[137, 162], ["predictions.items", "convert_to_xywh().tolist", "prediction[].tolist", "prediction[].tolist", "keypoints.flatten().tolist.flatten().tolist.flatten().tolist", "coco_results.extend", "len", "coco_eval.convert_to_xywh", "keypoints.flatten().tolist.flatten().tolist.flatten", "enumerate"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_ppuda.detector.coco_eval.convert_to_xywh"], ["", "def", "prepare_for_coco_keypoint", "(", "self", ",", "predictions", ")", ":", "\n", "        ", "coco_results", "=", "[", "]", "\n", "for", "original_id", ",", "prediction", "in", "predictions", ".", "items", "(", ")", ":", "\n", "            ", "if", "len", "(", "prediction", ")", "==", "0", ":", "\n", "                ", "continue", "\n", "\n", "", "boxes", "=", "prediction", "[", "\"boxes\"", "]", "\n", "boxes", "=", "convert_to_xywh", "(", "boxes", ")", ".", "tolist", "(", ")", "\n", "scores", "=", "prediction", "[", "\"scores\"", "]", ".", "tolist", "(", ")", "\n", "labels", "=", "prediction", "[", "\"labels\"", "]", ".", "tolist", "(", ")", "\n", "keypoints", "=", "prediction", "[", "\"keypoints\"", "]", "\n", "keypoints", "=", "keypoints", ".", "flatten", "(", "start_dim", "=", "1", ")", ".", "tolist", "(", ")", "\n", "\n", "coco_results", ".", "extend", "(", "\n", "[", "\n", "{", "\n", "\"image_id\"", ":", "original_id", ",", "\n", "\"category_id\"", ":", "labels", "[", "k", "]", ",", "\n", "'keypoints'", ":", "keypoint", ",", "\n", "\"score\"", ":", "scores", "[", "k", "]", ",", "\n", "}", "\n", "for", "k", ",", "keypoint", "in", "enumerate", "(", "keypoints", ")", "\n", "]", "\n", ")", "\n", "", "return", "coco_results", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.detector.coco_eval.convert_to_xywh": [[164, 167], ["boxes.unbind", "torch.stack", "torch.stack"], "function", ["None"], ["", "", "def", "convert_to_xywh", "(", "boxes", ")", ":", "\n", "    ", "xmin", ",", "ymin", ",", "xmax", ",", "ymax", "=", "boxes", ".", "unbind", "(", "1", ")", "\n", "return", "torch", ".", "stack", "(", "(", "xmin", ",", "ymin", ",", "xmax", "-", "xmin", ",", "ymax", "-", "ymin", ")", ",", "dim", "=", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.detector.coco_eval.merge": [[169, 189], ["utils.all_gather", "utils.all_gather", "numpy.array", "numpy.concatenate", "numpy.unique", "np.array.extend", "np.concatenate.append"], "function", ["home.repos.pwc.inspect_result.facebookresearch_ppuda.detector.utils.all_gather", "home.repos.pwc.inspect_result.facebookresearch_ppuda.detector.utils.all_gather", "home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.graph.GraphBatch.append"], ["", "def", "merge", "(", "img_ids", ",", "eval_imgs", ")", ":", "\n", "    ", "all_img_ids", "=", "utils", ".", "all_gather", "(", "img_ids", ")", "\n", "all_eval_imgs", "=", "utils", ".", "all_gather", "(", "eval_imgs", ")", "\n", "\n", "merged_img_ids", "=", "[", "]", "\n", "for", "p", "in", "all_img_ids", ":", "\n", "        ", "merged_img_ids", ".", "extend", "(", "p", ")", "\n", "\n", "", "merged_eval_imgs", "=", "[", "]", "\n", "for", "p", "in", "all_eval_imgs", ":", "\n", "        ", "merged_eval_imgs", ".", "append", "(", "p", ")", "\n", "\n", "", "merged_img_ids", "=", "np", ".", "array", "(", "merged_img_ids", ")", "\n", "merged_eval_imgs", "=", "np", ".", "concatenate", "(", "merged_eval_imgs", ",", "2", ")", "\n", "\n", "# keep only unique (and in sorted order) images", "\n", "merged_img_ids", ",", "idx", "=", "np", ".", "unique", "(", "merged_img_ids", ",", "return_index", "=", "True", ")", "\n", "merged_eval_imgs", "=", "merged_eval_imgs", "[", "...", ",", "idx", "]", "\n", "\n", "return", "merged_img_ids", ",", "merged_eval_imgs", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.detector.coco_eval.create_common_coco_eval": [[191, 199], ["coco_eval.merge", "list", "list", "copy.deepcopy", "list.flatten"], "function", ["home.repos.pwc.inspect_result.facebookresearch_ppuda.detector.coco_eval.merge"], ["", "def", "create_common_coco_eval", "(", "coco_eval", ",", "img_ids", ",", "eval_imgs", ")", ":", "\n", "    ", "img_ids", ",", "eval_imgs", "=", "merge", "(", "img_ids", ",", "eval_imgs", ")", "\n", "img_ids", "=", "list", "(", "img_ids", ")", "\n", "eval_imgs", "=", "list", "(", "eval_imgs", ".", "flatten", "(", ")", ")", "\n", "\n", "coco_eval", ".", "evalImgs", "=", "eval_imgs", "\n", "coco_eval", ".", "params", ".", "imgIds", "=", "img_ids", "\n", "coco_eval", ".", "_paramsEval", "=", "copy", ".", "deepcopy", "(", "coco_eval", ".", "params", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.detector.coco_eval.createIndex": [[209, 239], ["collections.defaultdict", "collections.defaultdict", "imgToAnns[].append", "catToImgs[].append"], "function", ["home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.graph.GraphBatch.append", "home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.graph.GraphBatch.append"], ["", "def", "createIndex", "(", "self", ")", ":", "\n", "# create index", "\n", "# print('creating index...')", "\n", "    ", "anns", ",", "cats", ",", "imgs", "=", "{", "}", ",", "{", "}", ",", "{", "}", "\n", "imgToAnns", ",", "catToImgs", "=", "defaultdict", "(", "list", ")", ",", "defaultdict", "(", "list", ")", "\n", "if", "'annotations'", "in", "self", ".", "dataset", ":", "\n", "        ", "for", "ann", "in", "self", ".", "dataset", "[", "'annotations'", "]", ":", "\n", "            ", "imgToAnns", "[", "ann", "[", "'image_id'", "]", "]", ".", "append", "(", "ann", ")", "\n", "anns", "[", "ann", "[", "'id'", "]", "]", "=", "ann", "\n", "\n", "", "", "if", "'images'", "in", "self", ".", "dataset", ":", "\n", "        ", "for", "img", "in", "self", ".", "dataset", "[", "'images'", "]", ":", "\n", "            ", "imgs", "[", "img", "[", "'id'", "]", "]", "=", "img", "\n", "\n", "", "", "if", "'categories'", "in", "self", ".", "dataset", ":", "\n", "        ", "for", "cat", "in", "self", ".", "dataset", "[", "'categories'", "]", ":", "\n", "            ", "cats", "[", "cat", "[", "'id'", "]", "]", "=", "cat", "\n", "\n", "", "", "if", "'annotations'", "in", "self", ".", "dataset", "and", "'categories'", "in", "self", ".", "dataset", ":", "\n", "        ", "for", "ann", "in", "self", ".", "dataset", "[", "'annotations'", "]", ":", "\n", "            ", "catToImgs", "[", "ann", "[", "'category_id'", "]", "]", ".", "append", "(", "ann", "[", "'image_id'", "]", ")", "\n", "\n", "# print('index created!')", "\n", "\n", "# create class members", "\n", "", "", "self", ".", "anns", "=", "anns", "\n", "self", ".", "imgToAnns", "=", "imgToAnns", "\n", "self", ".", "catToImgs", "=", "catToImgs", "\n", "self", ".", "imgs", "=", "imgs", "\n", "self", ".", "cats", "=", "cats", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.detector.coco_eval.loadRes": [[244, 304], ["pycocotools.coco.COCO", "isinstance", "coco_eval.createIndex", "json.load", "type", "set", "enumerate", "open", "type", "coco_eval..loadNumpyAnnotations", "set", "set", "set", "set", "copy.deepcopy", "enumerate", "coco_eval..getImgIds", "copy.deepcopy", "enumerate", "maskUtils.area", "copy.deepcopy", "enumerate", "maskUtils.toBbox", "numpy.min", "numpy.max", "numpy.min", "numpy.max"], "function", ["home.repos.pwc.inspect_result.facebookresearch_ppuda.detector.coco_eval.createIndex", "home.repos.pwc.inspect_result.facebookresearch_ppuda.ghn.nn.GHN.load", "home.repos.pwc.inspect_result.facebookresearch_ppuda.detector.utils.SmoothedValue.max", "home.repos.pwc.inspect_result.facebookresearch_ppuda.detector.utils.SmoothedValue.max"], ["def", "loadRes", "(", "self", ",", "resFile", ")", ":", "\n", "    ", "\"\"\"\n    Load result file and return a result api object.\n    :param   resFile (str)     : file name of result file\n    :return: res (obj)         : result api object\n    \"\"\"", "\n", "res", "=", "COCO", "(", ")", "\n", "res", ".", "dataset", "[", "'images'", "]", "=", "[", "img", "for", "img", "in", "self", ".", "dataset", "[", "'images'", "]", "]", "\n", "\n", "# print('Loading and preparing results...')", "\n", "# tic = time.time()", "\n", "if", "isinstance", "(", "resFile", ",", "torch", ".", "_six", ".", "string_classes", ")", ":", "\n", "        ", "anns", "=", "json", ".", "load", "(", "open", "(", "resFile", ")", ")", "\n", "", "elif", "type", "(", "resFile", ")", "==", "np", ".", "ndarray", ":", "\n", "        ", "anns", "=", "self", ".", "loadNumpyAnnotations", "(", "resFile", ")", "\n", "", "else", ":", "\n", "        ", "anns", "=", "resFile", "\n", "", "assert", "type", "(", "anns", ")", "==", "list", ",", "'results in not an array of objects'", "\n", "annsImgIds", "=", "[", "ann", "[", "'image_id'", "]", "for", "ann", "in", "anns", "]", "\n", "assert", "set", "(", "annsImgIds", ")", "==", "(", "set", "(", "annsImgIds", ")", "&", "set", "(", "self", ".", "getImgIds", "(", ")", ")", ")", ",", "'Results do not correspond to current coco set'", "\n", "if", "'caption'", "in", "anns", "[", "0", "]", ":", "\n", "        ", "imgIds", "=", "set", "(", "[", "img", "[", "'id'", "]", "for", "img", "in", "res", ".", "dataset", "[", "'images'", "]", "]", ")", "&", "set", "(", "[", "ann", "[", "'image_id'", "]", "for", "ann", "in", "anns", "]", ")", "\n", "res", ".", "dataset", "[", "'images'", "]", "=", "[", "img", "for", "img", "in", "res", ".", "dataset", "[", "'images'", "]", "if", "img", "[", "'id'", "]", "in", "imgIds", "]", "\n", "for", "id", ",", "ann", "in", "enumerate", "(", "anns", ")", ":", "\n", "            ", "ann", "[", "'id'", "]", "=", "id", "+", "1", "\n", "", "", "elif", "'bbox'", "in", "anns", "[", "0", "]", "and", "not", "anns", "[", "0", "]", "[", "'bbox'", "]", "==", "[", "]", ":", "\n", "        ", "res", ".", "dataset", "[", "'categories'", "]", "=", "copy", ".", "deepcopy", "(", "self", ".", "dataset", "[", "'categories'", "]", ")", "\n", "for", "id", ",", "ann", "in", "enumerate", "(", "anns", ")", ":", "\n", "            ", "bb", "=", "ann", "[", "'bbox'", "]", "\n", "x1", ",", "x2", ",", "y1", ",", "y2", "=", "[", "bb", "[", "0", "]", ",", "bb", "[", "0", "]", "+", "bb", "[", "2", "]", ",", "bb", "[", "1", "]", ",", "bb", "[", "1", "]", "+", "bb", "[", "3", "]", "]", "\n", "if", "'segmentation'", "not", "in", "ann", ":", "\n", "                ", "ann", "[", "'segmentation'", "]", "=", "[", "[", "x1", ",", "y1", ",", "x1", ",", "y2", ",", "x2", ",", "y2", ",", "x2", ",", "y1", "]", "]", "\n", "", "ann", "[", "'area'", "]", "=", "bb", "[", "2", "]", "*", "bb", "[", "3", "]", "\n", "ann", "[", "'id'", "]", "=", "id", "+", "1", "\n", "ann", "[", "'iscrowd'", "]", "=", "0", "\n", "", "", "elif", "'segmentation'", "in", "anns", "[", "0", "]", ":", "\n", "        ", "res", ".", "dataset", "[", "'categories'", "]", "=", "copy", ".", "deepcopy", "(", "self", ".", "dataset", "[", "'categories'", "]", ")", "\n", "for", "id", ",", "ann", "in", "enumerate", "(", "anns", ")", ":", "\n", "# now only support compressed RLE format as segmentation results", "\n", "            ", "ann", "[", "'area'", "]", "=", "maskUtils", ".", "area", "(", "ann", "[", "'segmentation'", "]", ")", "\n", "if", "'bbox'", "not", "in", "ann", ":", "\n", "                ", "ann", "[", "'bbox'", "]", "=", "maskUtils", ".", "toBbox", "(", "ann", "[", "'segmentation'", "]", ")", "\n", "", "ann", "[", "'id'", "]", "=", "id", "+", "1", "\n", "ann", "[", "'iscrowd'", "]", "=", "0", "\n", "", "", "elif", "'keypoints'", "in", "anns", "[", "0", "]", ":", "\n", "        ", "res", ".", "dataset", "[", "'categories'", "]", "=", "copy", ".", "deepcopy", "(", "self", ".", "dataset", "[", "'categories'", "]", ")", "\n", "for", "id", ",", "ann", "in", "enumerate", "(", "anns", ")", ":", "\n", "            ", "s", "=", "ann", "[", "'keypoints'", "]", "\n", "x", "=", "s", "[", "0", ":", ":", "3", "]", "\n", "y", "=", "s", "[", "1", ":", ":", "3", "]", "\n", "x1", ",", "x2", ",", "y1", ",", "y2", "=", "np", ".", "min", "(", "x", ")", ",", "np", ".", "max", "(", "x", ")", ",", "np", ".", "min", "(", "y", ")", ",", "np", ".", "max", "(", "y", ")", "\n", "ann", "[", "'area'", "]", "=", "(", "x2", "-", "x1", ")", "*", "(", "y2", "-", "y1", ")", "\n", "ann", "[", "'id'", "]", "=", "id", "+", "1", "\n", "ann", "[", "'bbox'", "]", "=", "[", "x1", ",", "y1", ",", "x2", "-", "x1", ",", "y2", "-", "y1", "]", "\n", "# print('DONE (t={:0.2f}s)'.format(time.time()- tic))", "\n", "\n", "", "", "res", ".", "dataset", "[", "'annotations'", "]", "=", "anns", "\n", "createIndex", "(", "res", ")", "\n", "return", "res", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.detector.coco_eval.evaluate": [[306, 352], ["list", "sorted", "coco_eval.._prepare", "numpy.asarray().reshape", "copy.deepcopy", "print", "numpy.unique", "list", "computeIoU", "evaluateImg", "len", "len", "len", "numpy.unique", "numpy.asarray"], "function", ["None"], ["", "def", "evaluate", "(", "self", ")", ":", "\n", "    ", "'''\n    Run per image evaluation on given images and store results (a list of dict) in self.evalImgs\n    :return: None\n    '''", "\n", "# tic = time.time()", "\n", "# print('Running per image evaluation...')", "\n", "p", "=", "self", ".", "params", "\n", "# add backward compatibility if useSegm is specified in params", "\n", "if", "p", ".", "useSegm", "is", "not", "None", ":", "\n", "        ", "p", ".", "iouType", "=", "'segm'", "if", "p", ".", "useSegm", "==", "1", "else", "'bbox'", "\n", "print", "(", "'useSegm (deprecated) is not None. Running {} evaluation'", ".", "format", "(", "p", ".", "iouType", ")", ")", "\n", "# print('Evaluate annotation type *{}*'.format(p.iouType))", "\n", "", "p", ".", "imgIds", "=", "list", "(", "np", ".", "unique", "(", "p", ".", "imgIds", ")", ")", "\n", "if", "p", ".", "useCats", ":", "\n", "        ", "p", ".", "catIds", "=", "list", "(", "np", ".", "unique", "(", "p", ".", "catIds", ")", ")", "\n", "", "p", ".", "maxDets", "=", "sorted", "(", "p", ".", "maxDets", ")", "\n", "self", ".", "params", "=", "p", "\n", "\n", "self", ".", "_prepare", "(", ")", "\n", "# loop through images, area range, max detection number", "\n", "catIds", "=", "p", ".", "catIds", "if", "p", ".", "useCats", "else", "[", "-", "1", "]", "\n", "\n", "if", "p", ".", "iouType", "==", "'segm'", "or", "p", ".", "iouType", "==", "'bbox'", ":", "\n", "        ", "computeIoU", "=", "self", ".", "computeIoU", "\n", "", "elif", "p", ".", "iouType", "==", "'keypoints'", ":", "\n", "        ", "computeIoU", "=", "self", ".", "computeOks", "\n", "", "self", ".", "ious", "=", "{", "\n", "(", "imgId", ",", "catId", ")", ":", "computeIoU", "(", "imgId", ",", "catId", ")", "\n", "for", "imgId", "in", "p", ".", "imgIds", "\n", "for", "catId", "in", "catIds", "}", "\n", "\n", "evaluateImg", "=", "self", ".", "evaluateImg", "\n", "maxDet", "=", "p", ".", "maxDets", "[", "-", "1", "]", "\n", "evalImgs", "=", "[", "\n", "evaluateImg", "(", "imgId", ",", "catId", ",", "areaRng", ",", "maxDet", ")", "\n", "for", "catId", "in", "catIds", "\n", "for", "areaRng", "in", "p", ".", "areaRng", "\n", "for", "imgId", "in", "p", ".", "imgIds", "\n", "]", "\n", "# this is NOT in the pycocotools code, but could be done outside", "\n", "evalImgs", "=", "np", ".", "asarray", "(", "evalImgs", ")", ".", "reshape", "(", "len", "(", "catIds", ")", ",", "len", "(", "p", ".", "areaRng", ")", ",", "len", "(", "p", ".", "imgIds", ")", ")", "\n", "self", ".", "_paramsEval", "=", "copy", ".", "deepcopy", "(", "self", ".", "params", ")", "\n", "# toc = time.time()", "\n", "# print('DONE (t={:0.2f}s).'.format(toc-tic))", "\n", "return", "p", ".", "imgIds", ",", "evalImgs", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.detector.transforms.Compose.__init__": [[24, 26], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "transforms", ")", ":", "\n", "        ", "self", ".", "transforms", "=", "transforms", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.detector.transforms.Compose.__call__": [[27, 31], ["t"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "image", ",", "target", ")", ":", "\n", "        ", "for", "t", "in", "self", ".", "transforms", ":", "\n", "            ", "image", ",", "target", "=", "t", "(", "image", ",", "target", ")", "\n", "", "return", "image", ",", "target", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.detector.transforms.RandomHorizontalFlip.__init__": [[34, 36], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "prob", ")", ":", "\n", "        ", "self", ".", "prob", "=", "prob", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.detector.transforms.RandomHorizontalFlip.__call__": [[37, 51], ["random.random", "image.flip.flip.flip", "target[].flip", "transforms._flip_coco_person_keypoints"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_ppuda.detector.transforms._flip_coco_person_keypoints"], ["", "def", "__call__", "(", "self", ",", "image", ",", "target", ")", ":", "\n", "        ", "if", "random", ".", "random", "(", ")", "<", "self", ".", "prob", ":", "\n", "            ", "height", ",", "width", "=", "image", ".", "shape", "[", "-", "2", ":", "]", "\n", "image", "=", "image", ".", "flip", "(", "-", "1", ")", "\n", "bbox", "=", "target", "[", "\"boxes\"", "]", "\n", "bbox", "[", ":", ",", "[", "0", ",", "2", "]", "]", "=", "width", "-", "bbox", "[", ":", ",", "[", "2", ",", "0", "]", "]", "\n", "target", "[", "\"boxes\"", "]", "=", "bbox", "\n", "if", "\"masks\"", "in", "target", ":", "\n", "                ", "target", "[", "\"masks\"", "]", "=", "target", "[", "\"masks\"", "]", ".", "flip", "(", "-", "1", ")", "\n", "", "if", "\"keypoints\"", "in", "target", ":", "\n", "                ", "keypoints", "=", "target", "[", "\"keypoints\"", "]", "\n", "keypoints", "=", "_flip_coco_person_keypoints", "(", "keypoints", ",", "width", ")", "\n", "target", "[", "\"keypoints\"", "]", "=", "keypoints", "\n", "", "", "return", "image", ",", "target", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.detector.transforms.ToTensor.__call__": [[54, 57], ["torchvision.transforms.functional.to_tensor"], "methods", ["None"], ["    ", "def", "__call__", "(", "self", ",", "image", ",", "target", ")", ":", "\n", "        ", "image", "=", "F", ".", "to_tensor", "(", "image", ")", "\n", "return", "image", ",", "target", "\n", "", "", ""]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.detector.transforms._flip_coco_person_keypoints": [[13, 21], ["None"], "function", ["None"], ["def", "_flip_coco_person_keypoints", "(", "kps", ",", "width", ")", ":", "\n", "    ", "flip_inds", "=", "[", "0", ",", "2", ",", "1", ",", "4", ",", "3", ",", "6", ",", "5", ",", "8", ",", "7", ",", "10", ",", "9", ",", "12", ",", "11", ",", "14", ",", "13", ",", "16", ",", "15", "]", "\n", "flipped_data", "=", "kps", "[", ":", ",", "flip_inds", "]", "\n", "flipped_data", "[", "...", ",", "0", "]", "=", "width", "-", "flipped_data", "[", "...", ",", "0", "]", "\n", "# Maintain COCO convention that if visibility == 0, then x, y = 0", "\n", "inds", "=", "flipped_data", "[", "...", ",", "2", "]", "==", "0", "\n", "flipped_data", "[", "inds", "]", "=", "0", "\n", "return", "flipped_data", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.detector.engine.train_one_epoch": [[19, 63], ["model.train", "utils.MetricLogger", "utils.MetricLogger.add_meter", "utils.MetricLogger.log_every", "utils.SmoothedValue", "min", "utils.warmup_lr_scheduler", "list", "model", "sum", "utils.reduce_dict", "sum", "sum.item", "optimizer.zero_grad", "sum.backward", "optimizer.step", "utils.MetricLogger.update", "utils.MetricLogger.update", "math.isfinite", "print", "print", "sys.exit", "utils.warmup_lr_scheduler.step", "len", "image.to", "v.to", "t.items", "model.values", "utils.reduce_dict.values"], "function", ["home.repos.pwc.inspect_result.facebookresearch_ppuda.detector.utils.MetricLogger.add_meter", "home.repos.pwc.inspect_result.facebookresearch_ppuda.detector.utils.MetricLogger.log_every", "home.repos.pwc.inspect_result.facebookresearch_ppuda.detector.utils.warmup_lr_scheduler", "home.repos.pwc.inspect_result.facebookresearch_ppuda.detector.utils.reduce_dict", "home.repos.pwc.inspect_result.facebookresearch_ppuda.utils.trainer.Trainer.update", "home.repos.pwc.inspect_result.facebookresearch_ppuda.utils.trainer.Trainer.update"], ["def", "train_one_epoch", "(", "model", ",", "optimizer", ",", "data_loader", ",", "device", ",", "epoch", ",", "print_freq", ")", ":", "\n", "    ", "model", ".", "train", "(", ")", "\n", "metric_logger", "=", "utils", ".", "MetricLogger", "(", "delimiter", "=", "\"  \"", ")", "\n", "metric_logger", ".", "add_meter", "(", "'lr'", ",", "utils", ".", "SmoothedValue", "(", "window_size", "=", "1", ",", "fmt", "=", "'{value:.6f}'", ")", ")", "\n", "header", "=", "'Epoch: [{}]'", ".", "format", "(", "epoch", ")", "\n", "\n", "lr_scheduler", "=", "None", "\n", "if", "epoch", "==", "0", ":", "\n", "        ", "warmup_factor", "=", "1.", "/", "1000", "\n", "warmup_iters", "=", "min", "(", "1000", ",", "len", "(", "data_loader", ")", "-", "1", ")", "\n", "\n", "lr_scheduler", "=", "utils", ".", "warmup_lr_scheduler", "(", "optimizer", ",", "warmup_iters", ",", "warmup_factor", ")", "\n", "\n", "", "for", "images", ",", "targets", "in", "metric_logger", ".", "log_every", "(", "data_loader", ",", "print_freq", ",", "header", ")", ":", "\n", "        ", "images", "=", "list", "(", "image", ".", "to", "(", "device", ")", "for", "image", "in", "images", ")", "\n", "targets", "=", "[", "{", "k", ":", "v", ".", "to", "(", "device", ")", "for", "k", ",", "v", "in", "t", ".", "items", "(", ")", "}", "for", "t", "in", "targets", "]", "\n", "\n", "# print(images[0].shape, images[0].min(), images[0].max())", "\n", "loss_dict", "=", "model", "(", "images", ",", "targets", ")", "\n", "\n", "losses", "=", "sum", "(", "loss", "for", "loss", "in", "loss_dict", ".", "values", "(", ")", ")", "\n", "\n", "# reduce losses over all GPUs for logging purposes", "\n", "loss_dict_reduced", "=", "utils", ".", "reduce_dict", "(", "loss_dict", ")", "\n", "losses_reduced", "=", "sum", "(", "loss", "for", "loss", "in", "loss_dict_reduced", ".", "values", "(", ")", ")", "\n", "\n", "loss_value", "=", "losses_reduced", ".", "item", "(", ")", "\n", "\n", "if", "not", "math", ".", "isfinite", "(", "loss_value", ")", ":", "\n", "            ", "print", "(", "\"Loss is {}, stopping training\"", ".", "format", "(", "loss_value", ")", ")", "\n", "print", "(", "loss_dict_reduced", ")", "\n", "sys", ".", "exit", "(", "1", ")", "\n", "\n", "", "optimizer", ".", "zero_grad", "(", ")", "\n", "losses", ".", "backward", "(", ")", "\n", "optimizer", ".", "step", "(", ")", "\n", "\n", "if", "lr_scheduler", "is", "not", "None", ":", "\n", "            ", "lr_scheduler", ".", "step", "(", ")", "\n", "\n", "", "metric_logger", ".", "update", "(", "loss", "=", "losses_reduced", ",", "**", "loss_dict_reduced", ")", "\n", "metric_logger", ".", "update", "(", "lr", "=", "optimizer", ".", "param_groups", "[", "0", "]", "[", "\"lr\"", "]", ")", "\n", "\n", "", "return", "metric_logger", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.detector.engine._get_iou_types": [[65, 75], ["isinstance", "isinstance", "isinstance", "iou_types.append", "iou_types.append"], "function", ["home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.graph.GraphBatch.append", "home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.graph.GraphBatch.append"], ["", "def", "_get_iou_types", "(", "model", ")", ":", "\n", "    ", "model_without_ddp", "=", "model", "\n", "if", "isinstance", "(", "model", ",", "torch", ".", "nn", ".", "parallel", ".", "DistributedDataParallel", ")", ":", "\n", "        ", "model_without_ddp", "=", "model", ".", "module", "\n", "", "iou_types", "=", "[", "\"bbox\"", "]", "\n", "if", "isinstance", "(", "model_without_ddp", ",", "torchvision", ".", "models", ".", "detection", ".", "MaskRCNN", ")", ":", "\n", "        ", "iou_types", ".", "append", "(", "\"segm\"", ")", "\n", "", "if", "isinstance", "(", "model_without_ddp", ",", "torchvision", ".", "models", ".", "detection", ".", "KeypointRCNN", ")", ":", "\n", "        ", "iou_types", ".", "append", "(", "\"keypoints\"", ")", "\n", "", "return", "iou_types", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.detector.engine.evaluate": [[77, 118], ["torch.no_grad", "torch.get_num_threads", "torch.set_num_threads", "torch.device", "model.eval", "utils.MetricLogger", "coco_utils.get_coco_api_from_dataset", "engine._get_iou_types", "coco_eval.CocoEvaluator", "utils.MetricLogger.log_every", "utils.MetricLogger.synchronize_between_processes", "print", "coco_eval.CocoEvaluator.synchronize_between_processes", "coco_eval.CocoEvaluator.accumulate", "coco_eval.CocoEvaluator.summarize", "torch.set_num_threads", "list", "torch.cuda.synchronize", "time.time", "model", "time.time", "coco_eval.CocoEvaluator.update", "utils.MetricLogger.update", "time.time", "target[].item", "time.time", "img.to", "v.to", "v.to", "zip", "t.items", "t.items"], "function", ["home.repos.pwc.inspect_result.facebookresearch_ppuda.detector.coco_utils.get_coco_api_from_dataset", "home.repos.pwc.inspect_result.facebookresearch_ppuda.detector.engine._get_iou_types", "home.repos.pwc.inspect_result.facebookresearch_ppuda.detector.utils.MetricLogger.log_every", "home.repos.pwc.inspect_result.facebookresearch_ppuda.detector.utils.MetricLogger.synchronize_between_processes", "home.repos.pwc.inspect_result.facebookresearch_ppuda.detector.utils.MetricLogger.synchronize_between_processes", "home.repos.pwc.inspect_result.facebookresearch_ppuda.detector.coco_eval.CocoEvaluator.accumulate", "home.repos.pwc.inspect_result.facebookresearch_ppuda.detector.coco_eval.CocoEvaluator.summarize", "home.repos.pwc.inspect_result.facebookresearch_ppuda.utils.trainer.Trainer.update", "home.repos.pwc.inspect_result.facebookresearch_ppuda.utils.trainer.Trainer.update"], ["", "@", "torch", ".", "no_grad", "(", ")", "\n", "def", "evaluate", "(", "model", ",", "data_loader", ",", "device", ")", ":", "\n", "    ", "n_threads", "=", "torch", ".", "get_num_threads", "(", ")", "\n", "# FIXME remove this and make paste_masks_in_image run on the GPU", "\n", "torch", ".", "set_num_threads", "(", "1", ")", "\n", "cpu_device", "=", "torch", ".", "device", "(", "\"cpu\"", ")", "\n", "model", ".", "eval", "(", ")", "\n", "metric_logger", "=", "utils", ".", "MetricLogger", "(", "delimiter", "=", "\"  \"", ")", "\n", "header", "=", "'Test:'", "\n", "\n", "coco", "=", "get_coco_api_from_dataset", "(", "data_loader", ".", "dataset", ")", "\n", "iou_types", "=", "_get_iou_types", "(", "model", ")", "\n", "coco_evaluator", "=", "CocoEvaluator", "(", "coco", ",", "iou_types", ")", "\n", "\n", "for", "image", ",", "targets", "in", "metric_logger", ".", "log_every", "(", "data_loader", ",", "100", ",", "header", ")", ":", "\n", "        ", "image", "=", "list", "(", "img", ".", "to", "(", "device", ")", "for", "img", "in", "image", ")", "\n", "targets", "=", "[", "{", "k", ":", "v", ".", "to", "(", "device", ")", "for", "k", ",", "v", "in", "t", ".", "items", "(", ")", "}", "for", "t", "in", "targets", "]", "\n", "\n", "torch", ".", "cuda", ".", "synchronize", "(", ")", "\n", "model_time", "=", "time", ".", "time", "(", ")", "\n", "outputs", "=", "model", "(", "image", ")", "\n", "\n", "outputs", "=", "[", "{", "k", ":", "v", ".", "to", "(", "cpu_device", ")", "for", "k", ",", "v", "in", "t", ".", "items", "(", ")", "}", "for", "t", "in", "outputs", "]", "\n", "model_time", "=", "time", ".", "time", "(", ")", "-", "model_time", "\n", "\n", "res", "=", "{", "target", "[", "\"image_id\"", "]", ".", "item", "(", ")", ":", "output", "for", "target", ",", "output", "in", "zip", "(", "targets", ",", "outputs", ")", "}", "\n", "evaluator_time", "=", "time", ".", "time", "(", ")", "\n", "coco_evaluator", ".", "update", "(", "res", ")", "\n", "evaluator_time", "=", "time", ".", "time", "(", ")", "-", "evaluator_time", "\n", "metric_logger", ".", "update", "(", "model_time", "=", "model_time", ",", "evaluator_time", "=", "evaluator_time", ")", "\n", "\n", "# gather the stats from all processes", "\n", "", "metric_logger", ".", "synchronize_between_processes", "(", ")", "\n", "print", "(", "\"Averaged stats:\"", ",", "metric_logger", ")", "\n", "coco_evaluator", ".", "synchronize_between_processes", "(", ")", "\n", "\n", "# accumulate predictions from all images", "\n", "coco_evaluator", ".", "accumulate", "(", ")", "\n", "coco_evaluator", ".", "summarize", "(", ")", "\n", "torch", ".", "set_num_threads", "(", "n_threads", ")", "\n", "return", "coco_evaluator", "\n", "", ""]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.detector.coco_utils.FilterAndRemapCocoCategories.__init__": [[22, 25], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "categories", ",", "remap", "=", "True", ")", ":", "\n", "        ", "self", ".", "categories", "=", "categories", "\n", "self", ".", "remap", "=", "remap", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.detector.coco_utils.FilterAndRemapCocoCategories.__call__": [[26, 37], ["copy.deepcopy", "coco_utils.FilterAndRemapCocoCategories.categories.index"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "image", ",", "target", ")", ":", "\n", "        ", "anno", "=", "target", "[", "\"annotations\"", "]", "\n", "anno", "=", "[", "obj", "for", "obj", "in", "anno", "if", "obj", "[", "\"category_id\"", "]", "in", "self", ".", "categories", "]", "\n", "if", "not", "self", ".", "remap", ":", "\n", "            ", "target", "[", "\"annotations\"", "]", "=", "anno", "\n", "return", "image", ",", "target", "\n", "", "anno", "=", "copy", ".", "deepcopy", "(", "anno", ")", "\n", "for", "obj", "in", "anno", ":", "\n", "            ", "obj", "[", "\"category_id\"", "]", "=", "self", ".", "categories", ".", "index", "(", "obj", "[", "\"category_id\"", "]", ")", "\n", "", "target", "[", "\"annotations\"", "]", "=", "anno", "\n", "return", "image", ",", "target", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.detector.coco_utils.ConvertCocoPolysToMask.__call__": [[57, 110], ["torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.as_tensor().reshape", "torch.as_tensor().reshape", "torch.as_tensor().reshape", "torch.as_tensor().reshape", "boxes[].clamp_", "boxes[].clamp_", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "coco_utils.convert_coco_poly_to_mask", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.as_tensor", "torch.as_tensor", "torch.as_tensor", "torch.as_tensor", "torch.as_tensor", "torch.as_tensor", "torch.as_tensor", "torch.as_tensor", "keypoints.view.view.view"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_ppuda.detector.coco_utils.convert_coco_poly_to_mask"], ["    ", "def", "__call__", "(", "self", ",", "image", ",", "target", ")", ":", "\n", "        ", "w", ",", "h", "=", "image", ".", "size", "\n", "\n", "image_id", "=", "target", "[", "\"image_id\"", "]", "\n", "image_id", "=", "torch", ".", "tensor", "(", "[", "image_id", "]", ")", "\n", "\n", "anno", "=", "target", "[", "\"annotations\"", "]", "\n", "\n", "anno", "=", "[", "obj", "for", "obj", "in", "anno", "if", "obj", "[", "'iscrowd'", "]", "==", "0", "]", "\n", "\n", "boxes", "=", "[", "obj", "[", "\"bbox\"", "]", "for", "obj", "in", "anno", "]", "\n", "# guard against no boxes via resizing", "\n", "boxes", "=", "torch", ".", "as_tensor", "(", "boxes", ",", "dtype", "=", "torch", ".", "float32", ")", ".", "reshape", "(", "-", "1", ",", "4", ")", "\n", "boxes", "[", ":", ",", "2", ":", "]", "+=", "boxes", "[", ":", ",", ":", "2", "]", "\n", "boxes", "[", ":", ",", "0", ":", ":", "2", "]", ".", "clamp_", "(", "min", "=", "0", ",", "max", "=", "w", ")", "\n", "boxes", "[", ":", ",", "1", ":", ":", "2", "]", ".", "clamp_", "(", "min", "=", "0", ",", "max", "=", "h", ")", "\n", "\n", "classes", "=", "[", "obj", "[", "\"category_id\"", "]", "for", "obj", "in", "anno", "]", "\n", "classes", "=", "torch", ".", "tensor", "(", "classes", ",", "dtype", "=", "torch", ".", "int64", ")", "\n", "\n", "segmentations", "=", "[", "obj", "[", "\"segmentation\"", "]", "for", "obj", "in", "anno", "]", "\n", "masks", "=", "convert_coco_poly_to_mask", "(", "segmentations", ",", "h", ",", "w", ")", "\n", "\n", "keypoints", "=", "None", "\n", "if", "anno", "and", "\"keypoints\"", "in", "anno", "[", "0", "]", ":", "\n", "            ", "keypoints", "=", "[", "obj", "[", "\"keypoints\"", "]", "for", "obj", "in", "anno", "]", "\n", "keypoints", "=", "torch", ".", "as_tensor", "(", "keypoints", ",", "dtype", "=", "torch", ".", "float32", ")", "\n", "num_keypoints", "=", "keypoints", ".", "shape", "[", "0", "]", "\n", "if", "num_keypoints", ":", "\n", "                ", "keypoints", "=", "keypoints", ".", "view", "(", "num_keypoints", ",", "-", "1", ",", "3", ")", "\n", "\n", "", "", "keep", "=", "(", "boxes", "[", ":", ",", "3", "]", ">", "boxes", "[", ":", ",", "1", "]", ")", "&", "(", "boxes", "[", ":", ",", "2", "]", ">", "boxes", "[", ":", ",", "0", "]", ")", "\n", "boxes", "=", "boxes", "[", "keep", "]", "\n", "classes", "=", "classes", "[", "keep", "]", "\n", "masks", "=", "masks", "[", "keep", "]", "\n", "if", "keypoints", "is", "not", "None", ":", "\n", "            ", "keypoints", "=", "keypoints", "[", "keep", "]", "\n", "\n", "", "target", "=", "{", "}", "\n", "target", "[", "\"boxes\"", "]", "=", "boxes", "\n", "target", "[", "\"labels\"", "]", "=", "classes", "\n", "target", "[", "\"masks\"", "]", "=", "masks", "\n", "target", "[", "\"image_id\"", "]", "=", "image_id", "\n", "if", "keypoints", "is", "not", "None", ":", "\n", "            ", "target", "[", "\"keypoints\"", "]", "=", "keypoints", "\n", "\n", "# for conversion to coco api", "\n", "", "area", "=", "torch", ".", "tensor", "(", "[", "obj", "[", "\"area\"", "]", "for", "obj", "in", "anno", "]", ")", "\n", "iscrowd", "=", "torch", ".", "tensor", "(", "[", "obj", "[", "\"iscrowd\"", "]", "for", "obj", "in", "anno", "]", ")", "\n", "target", "[", "\"area\"", "]", "=", "area", "\n", "target", "[", "\"iscrowd\"", "]", "=", "iscrowd", "\n", "\n", "return", "image", ",", "target", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.detector.coco_utils.CocoDetection.__init__": [[216, 219], ["super().__init__"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_ppuda.examples.custom_cnn.CNN.__init__"], ["    ", "def", "__init__", "(", "self", ",", "img_folder", ",", "ann_file", ",", "transforms", ")", ":", "\n", "        ", "super", "(", "CocoDetection", ",", "self", ")", ".", "__init__", "(", "img_folder", ",", "ann_file", ")", "\n", "self", ".", "_transforms", "=", "transforms", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.detector.coco_utils.CocoDetection.__getitem__": [[220, 227], ["super().__getitem__", "dict", "coco_utils.CocoDetection._transforms"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.loader.DeepNets1M.__getitem__"], ["", "def", "__getitem__", "(", "self", ",", "idx", ")", ":", "\n", "        ", "img", ",", "target", "=", "super", "(", "CocoDetection", ",", "self", ")", ".", "__getitem__", "(", "idx", ")", "\n", "image_id", "=", "self", ".", "ids", "[", "idx", "]", "\n", "target", "=", "dict", "(", "image_id", "=", "image_id", ",", "annotations", "=", "target", ")", "\n", "if", "self", ".", "_transforms", "is", "not", "None", ":", "\n", "            ", "img", ",", "target", "=", "self", ".", "_transforms", "(", "img", ",", "target", ")", "\n", "", "return", "img", ",", "target", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.detector.coco_utils.convert_coco_poly_to_mask": [[39, 54], ["pycocotools.mask.frPyObjects", "pycocotools.mask.decode", "torch.as_tensor", "torch.as_tensor", "mask.any.any", "torch.zeros.append", "torch.stack", "torch.stack", "torch.zeros", "torch.zeros", "len"], "function", ["home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.graph.GraphBatch.append"], ["", "", "def", "convert_coco_poly_to_mask", "(", "segmentations", ",", "height", ",", "width", ")", ":", "\n", "    ", "masks", "=", "[", "]", "\n", "for", "polygons", "in", "segmentations", ":", "\n", "        ", "rles", "=", "coco_mask", ".", "frPyObjects", "(", "polygons", ",", "height", ",", "width", ")", "\n", "mask", "=", "coco_mask", ".", "decode", "(", "rles", ")", "\n", "if", "len", "(", "mask", ".", "shape", ")", "<", "3", ":", "\n", "            ", "mask", "=", "mask", "[", "...", ",", "None", "]", "\n", "", "mask", "=", "torch", ".", "as_tensor", "(", "mask", ",", "dtype", "=", "torch", ".", "uint8", ")", "\n", "mask", "=", "mask", ".", "any", "(", "dim", "=", "2", ")", "\n", "masks", ".", "append", "(", "mask", ")", "\n", "", "if", "masks", ":", "\n", "        ", "masks", "=", "torch", ".", "stack", "(", "masks", ",", "dim", "=", "0", ")", "\n", "", "else", ":", "\n", "        ", "masks", "=", "torch", ".", "zeros", "(", "(", "0", ",", "height", ",", "width", ")", ",", "dtype", "=", "torch", ".", "uint8", ")", "\n", "", "return", "masks", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.detector.coco_utils._coco_remove_images_without_annotations": [[112, 150], ["isinstance", "enumerate", "torch.utils.data.Subset", "torch.utils.data.Subset", "all", "sum", "coco_utils._coco_remove_images_without_annotations._has_only_empty_bbox"], "function", ["None"], ["", "", "def", "_coco_remove_images_without_annotations", "(", "dataset", ",", "cat_list", "=", "None", ")", ":", "\n", "    ", "def", "_has_only_empty_bbox", "(", "anno", ")", ":", "\n", "        ", "return", "all", "(", "any", "(", "o", "<=", "1", "for", "o", "in", "obj", "[", "\"bbox\"", "]", "[", "2", ":", "]", ")", "for", "obj", "in", "anno", ")", "\n", "\n", "", "def", "_count_visible_keypoints", "(", "anno", ")", ":", "\n", "        ", "return", "sum", "(", "sum", "(", "1", "for", "v", "in", "ann", "[", "\"keypoints\"", "]", "[", "2", ":", ":", "3", "]", "if", "v", ">", "0", ")", "for", "ann", "in", "anno", ")", "\n", "\n", "", "min_keypoints_per_image", "=", "10", "\n", "\n", "def", "_has_valid_annotation", "(", "anno", ")", ":", "\n", "# if it's empty, there is no annotation", "\n", "        ", "if", "len", "(", "anno", ")", "==", "0", ":", "\n", "            ", "return", "False", "\n", "# if all boxes have close to zero area, there is no annotation", "\n", "", "if", "_has_only_empty_bbox", "(", "anno", ")", ":", "\n", "            ", "return", "False", "\n", "# keypoints task have a slight different critera for considering", "\n", "# if an annotation is valid", "\n", "", "if", "\"keypoints\"", "not", "in", "anno", "[", "0", "]", ":", "\n", "            ", "return", "True", "\n", "# for keypoint detection tasks, only consider valid images those", "\n", "# containing at least min_keypoints_per_image", "\n", "", "if", "_count_visible_keypoints", "(", "anno", ")", ">=", "min_keypoints_per_image", ":", "\n", "            ", "return", "True", "\n", "", "return", "False", "\n", "\n", "", "assert", "isinstance", "(", "dataset", ",", "torchvision", ".", "datasets", ".", "CocoDetection", ")", "\n", "ids", "=", "[", "]", "\n", "for", "ds_idx", ",", "img_id", "in", "enumerate", "(", "dataset", ".", "ids", ")", ":", "\n", "        ", "ann_ids", "=", "dataset", ".", "coco", ".", "getAnnIds", "(", "imgIds", "=", "img_id", ",", "iscrowd", "=", "None", ")", "\n", "anno", "=", "dataset", ".", "coco", ".", "loadAnns", "(", "ann_ids", ")", "\n", "if", "cat_list", ":", "\n", "            ", "anno", "=", "[", "obj", "for", "obj", "in", "anno", "if", "obj", "[", "\"category_id\"", "]", "in", "cat_list", "]", "\n", "", "if", "_has_valid_annotation", "(", "anno", ")", ":", "\n", "            ", "ids", ".", "append", "(", "ds_idx", ")", "\n", "\n", "", "", "dataset", "=", "torch", ".", "utils", ".", "data", ".", "Subset", "(", "dataset", ",", "ids", ")", "\n", "return", "dataset", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.detector.coco_utils.convert_to_coco_api": [[152, 202], ["pycocotools.coco.COCO", "set", "range", "pycocotools.coco.COCO.createIndex", "len", "targets[].item", "dataset[].append", "bboxes.tolist.tolist", "targets[].tolist", "targets[].tolist", "targets[].tolist", "len", "range", "masks.permute().contiguous().permute.permute().contiguous().permute", "keypoints.reshape().tolist.reshape().tolist", "set.add", "dataset[].append", "sorted", "pycocotools.mask.encode", "sum", "masks.permute().contiguous().permute.permute().contiguous", "keypoints.reshape().tolist.reshape", "masks[].numpy", "masks.permute().contiguous().permute.permute"], "function", ["home.repos.pwc.inspect_result.facebookresearch_ppuda.detector.coco_eval.createIndex", "home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.graph.GraphBatch.append", "home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.graph.GraphBatch.append"], ["", "def", "convert_to_coco_api", "(", "ds", ")", ":", "\n", "    ", "coco_ds", "=", "COCO", "(", ")", "\n", "# annotation IDs need to start at 1, not 0, see torchvision issue #1530", "\n", "ann_id", "=", "1", "\n", "dataset", "=", "{", "'images'", ":", "[", "]", ",", "'categories'", ":", "[", "]", ",", "'annotations'", ":", "[", "]", "}", "\n", "categories", "=", "set", "(", ")", "\n", "for", "img_idx", "in", "range", "(", "len", "(", "ds", ")", ")", ":", "\n", "# find better way to get target", "\n", "# targets = ds.get_annotations(img_idx)", "\n", "        ", "img", ",", "targets", "=", "ds", "[", "img_idx", "]", "\n", "image_id", "=", "targets", "[", "\"image_id\"", "]", ".", "item", "(", ")", "\n", "img_dict", "=", "{", "}", "\n", "img_dict", "[", "'id'", "]", "=", "image_id", "\n", "img_dict", "[", "'height'", "]", "=", "img", ".", "shape", "[", "-", "2", "]", "\n", "img_dict", "[", "'width'", "]", "=", "img", ".", "shape", "[", "-", "1", "]", "\n", "dataset", "[", "'images'", "]", ".", "append", "(", "img_dict", ")", "\n", "bboxes", "=", "targets", "[", "\"boxes\"", "]", "\n", "bboxes", "[", ":", ",", "2", ":", "]", "-=", "bboxes", "[", ":", ",", ":", "2", "]", "\n", "bboxes", "=", "bboxes", ".", "tolist", "(", ")", "\n", "labels", "=", "targets", "[", "'labels'", "]", ".", "tolist", "(", ")", "\n", "areas", "=", "targets", "[", "'area'", "]", ".", "tolist", "(", ")", "\n", "iscrowd", "=", "targets", "[", "'iscrowd'", "]", ".", "tolist", "(", ")", "\n", "if", "'masks'", "in", "targets", ":", "\n", "            ", "masks", "=", "targets", "[", "'masks'", "]", "\n", "# make masks Fortran contiguous for coco_mask", "\n", "masks", "=", "masks", ".", "permute", "(", "0", ",", "2", ",", "1", ")", ".", "contiguous", "(", ")", ".", "permute", "(", "0", ",", "2", ",", "1", ")", "\n", "", "if", "'keypoints'", "in", "targets", ":", "\n", "            ", "keypoints", "=", "targets", "[", "'keypoints'", "]", "\n", "keypoints", "=", "keypoints", ".", "reshape", "(", "keypoints", ".", "shape", "[", "0", "]", ",", "-", "1", ")", ".", "tolist", "(", ")", "\n", "", "num_objs", "=", "len", "(", "bboxes", ")", "\n", "for", "i", "in", "range", "(", "num_objs", ")", ":", "\n", "            ", "ann", "=", "{", "}", "\n", "ann", "[", "'image_id'", "]", "=", "image_id", "\n", "ann", "[", "'bbox'", "]", "=", "bboxes", "[", "i", "]", "\n", "ann", "[", "'category_id'", "]", "=", "labels", "[", "i", "]", "\n", "categories", ".", "add", "(", "labels", "[", "i", "]", ")", "\n", "ann", "[", "'area'", "]", "=", "areas", "[", "i", "]", "\n", "ann", "[", "'iscrowd'", "]", "=", "iscrowd", "[", "i", "]", "\n", "ann", "[", "'id'", "]", "=", "ann_id", "\n", "if", "'masks'", "in", "targets", ":", "\n", "                ", "ann", "[", "\"segmentation\"", "]", "=", "coco_mask", ".", "encode", "(", "masks", "[", "i", "]", ".", "numpy", "(", ")", ")", "\n", "", "if", "'keypoints'", "in", "targets", ":", "\n", "                ", "ann", "[", "'keypoints'", "]", "=", "keypoints", "[", "i", "]", "\n", "ann", "[", "'num_keypoints'", "]", "=", "sum", "(", "k", "!=", "0", "for", "k", "in", "keypoints", "[", "i", "]", "[", "2", ":", ":", "3", "]", ")", "\n", "", "dataset", "[", "'annotations'", "]", ".", "append", "(", "ann", ")", "\n", "ann_id", "+=", "1", "\n", "", "", "dataset", "[", "'categories'", "]", "=", "[", "{", "'id'", ":", "i", "}", "for", "i", "in", "sorted", "(", "categories", ")", "]", "\n", "coco_ds", ".", "dataset", "=", "dataset", "\n", "coco_ds", ".", "createIndex", "(", ")", "\n", "return", "coco_ds", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.detector.coco_utils.get_coco_api_from_dataset": [[204, 213], ["range", "isinstance", "coco_utils.convert_to_coco_api", "isinstance", "isinstance"], "function", ["home.repos.pwc.inspect_result.facebookresearch_ppuda.detector.coco_utils.convert_to_coco_api"], ["", "def", "get_coco_api_from_dataset", "(", "dataset", ")", ":", "\n", "    ", "for", "_", "in", "range", "(", "10", ")", ":", "\n", "        ", "if", "isinstance", "(", "dataset", ",", "torchvision", ".", "datasets", ".", "CocoDetection", ")", ":", "\n", "            ", "break", "\n", "", "if", "isinstance", "(", "dataset", ",", "torch", ".", "utils", ".", "data", ".", "Subset", ")", ":", "\n", "            ", "dataset", "=", "dataset", ".", "dataset", "\n", "", "", "if", "isinstance", "(", "dataset", ",", "torchvision", ".", "datasets", ".", "CocoDetection", ")", ":", "\n", "        ", "return", "dataset", ".", "coco", "\n", "", "return", "convert_to_coco_api", "(", "dataset", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.detector.coco_utils.get_coco": [[229, 255], ["transforms.Compose", "os.path.join", "os.path.join", "coco_utils.CocoDetection", "coco_utils.ConvertCocoPolysToMask", "t.append", "coco_utils._coco_remove_images_without_annotations", "os.path.join", "os.path.join", "anno_file_template.format", "anno_file_template.format"], "function", ["home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.graph.GraphBatch.append", "home.repos.pwc.inspect_result.facebookresearch_ppuda.detector.coco_utils._coco_remove_images_without_annotations"], ["", "", "def", "get_coco", "(", "root", ",", "image_set", ",", "transforms", ",", "mode", "=", "'instances'", ")", ":", "\n", "    ", "anno_file_template", "=", "\"{}_{}2017.json\"", "\n", "PATHS", "=", "{", "\n", "\"train\"", ":", "(", "\"train2017\"", ",", "os", ".", "path", ".", "join", "(", "\"annotations\"", ",", "anno_file_template", ".", "format", "(", "mode", ",", "\"train\"", ")", ")", ")", ",", "\n", "\"val\"", ":", "(", "\"val2017\"", ",", "os", ".", "path", ".", "join", "(", "\"annotations\"", ",", "anno_file_template", ".", "format", "(", "mode", ",", "\"val\"", ")", ")", ")", ",", "\n", "# \"train\": (\"val2017\", os.path.join(\"annotations\", anno_file_template.format(mode, \"val\")))", "\n", "}", "\n", "\n", "t", "=", "[", "ConvertCocoPolysToMask", "(", ")", "]", "\n", "\n", "if", "transforms", "is", "not", "None", ":", "\n", "        ", "t", ".", "append", "(", "transforms", ")", "\n", "", "transforms", "=", "T", ".", "Compose", "(", "t", ")", "\n", "\n", "img_folder", ",", "ann_file", "=", "PATHS", "[", "image_set", "]", "\n", "img_folder", "=", "os", ".", "path", ".", "join", "(", "root", ",", "img_folder", ")", "\n", "ann_file", "=", "os", ".", "path", ".", "join", "(", "root", ",", "ann_file", ")", "\n", "\n", "dataset", "=", "CocoDetection", "(", "img_folder", ",", "ann_file", ",", "transforms", "=", "transforms", ")", "\n", "\n", "if", "image_set", "==", "\"train\"", ":", "\n", "        ", "dataset", "=", "_coco_remove_images_without_annotations", "(", "dataset", ")", "\n", "\n", "# dataset = torch.utils.data.Subset(dataset, [i for i in range(500)])", "\n", "\n", "", "return", "dataset", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.detector.coco_utils.get_coco_kp": [[257, 259], ["coco_utils.get_coco"], "function", ["home.repos.pwc.inspect_result.facebookresearch_ppuda.detector.coco_utils.get_coco"], ["", "def", "get_coco_kp", "(", "root", ",", "image_set", ",", "transforms", ")", ":", "\n", "    ", "return", "get_coco", "(", "root", ",", "image_set", ",", "transforms", ",", "mode", "=", "\"person_keypoints\"", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.detector.train_detector.main": [[41, 92], ["ppuda.config.init_config", "penn.PennFudanDataset", "penn.PennFudanDataset", "torch.randperm().tolist", "torch.randperm().tolist", "torch.utils.data.Subset", "torch.utils.data.Subset", "torch.utils.data.Subset", "torch.utils.data.Subset", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "get_model_detection().to", "torch.optim.SGD", "torch.optim.SGD", "torch.optim.lr_scheduler.StepLR", "torch.optim.lr_scheduler.StepLR", "range", "print", "os.path.join", "os.path.join", "engine.train_one_epoch", "torch.optim.lr_scheduler.StepLR.step", "engine.evaluate", "torch.cuda.empty_cache", "torch.cuda.empty_cache", "transforms.Compose", "transforms.ToTensor", "torch.randperm", "torch.randperm", "train_detector.get_model_detection", "os.path.join", "torch.save", "torch.save", "print", "len", "get_model_detection().to.parameters", "transforms.ToTensor", "transforms.RandomHorizontalFlip", "get_model_detection().to.state_dict"], "function", ["home.repos.pwc.inspect_result.facebookresearch_ppuda.ppuda.config.init_config", "home.repos.pwc.inspect_result.facebookresearch_ppuda.detector.engine.train_one_epoch", "home.repos.pwc.inspect_result.facebookresearch_ppuda.detector.engine.evaluate", "home.repos.pwc.inspect_result.facebookresearch_ppuda.detector.train_detector.get_model_detection"], ["def", "main", "(", ")", ":", "\n", "\n", "    ", "args", "=", "init_config", "(", "mode", "=", "'train_net'", ")", "\n", "\n", "# Penn-Fudan dataset has two classes only - background and person", "\n", "num_classes", "=", "2", "\n", "# use our dataset and defined transformations", "\n", "dataset", "=", "PennFudanDataset", "(", "os", ".", "path", ".", "join", "(", "args", ".", "data_dir", ",", "args", ".", "dataset", ")", ",", "\n", "transforms", "=", "T", ".", "Compose", "(", "[", "T", ".", "ToTensor", "(", ")", ",", "T", ".", "RandomHorizontalFlip", "(", "0.5", ")", "]", ")", ")", "\n", "dataset_test", "=", "PennFudanDataset", "(", "os", ".", "path", ".", "join", "(", "args", ".", "data_dir", ",", "args", ".", "dataset", ")", ",", "transforms", "=", "T", ".", "ToTensor", "(", ")", ")", "\n", "\n", "# split the dataset in train and test set", "\n", "indices", "=", "torch", ".", "randperm", "(", "len", "(", "dataset", ")", ")", ".", "tolist", "(", ")", "\n", "dataset", "=", "torch", ".", "utils", ".", "data", ".", "Subset", "(", "dataset", ",", "indices", "[", ":", "-", "50", "]", ")", "\n", "dataset_test", "=", "torch", ".", "utils", ".", "data", ".", "Subset", "(", "dataset_test", ",", "indices", "[", "-", "50", ":", "]", ")", "\n", "\n", "# define training and validation data loaders", "\n", "data_loader", "=", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "\n", "dataset", ",", "batch_size", "=", "args", ".", "batch_size", ",", "shuffle", "=", "True", ",", "num_workers", "=", "args", ".", "num_workers", ",", "\n", "collate_fn", "=", "utils", ".", "collate_fn", ")", "\n", "\n", "data_loader_test", "=", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "\n", "dataset_test", ",", "batch_size", "=", "args", ".", "test_batch_size", ",", "shuffle", "=", "False", ",", "num_workers", "=", "args", ".", "num_workers", ",", "\n", "collate_fn", "=", "utils", ".", "collate_fn", ")", "\n", "\n", "# get the model using our helper function", "\n", "model", "=", "get_model_detection", "(", "args", ",", "num_classes", ")", ".", "to", "(", "args", ".", "device", ")", "\n", "\n", "# construct an optimizer", "\n", "optimizer", "=", "torch", ".", "optim", ".", "SGD", "(", "[", "p", "for", "p", "in", "model", ".", "parameters", "(", ")", "if", "p", ".", "requires_grad", "]", ",", "\n", "lr", "=", "args", ".", "lr", ",", "momentum", "=", "args", ".", "momentum", ",", "weight_decay", "=", "args", ".", "wd", ")", "\n", "\n", "# and a learning rate scheduler", "\n", "lr_scheduler", "=", "torch", ".", "optim", ".", "lr_scheduler", ".", "StepLR", "(", "optimizer", ",", "step_size", "=", "3", ",", "gamma", "=", "0.1", ")", "\n", "\n", "for", "epoch", "in", "range", "(", "args", ".", "epochs", ")", ":", "\n", "# train for one epoch, printing every 10 iterations", "\n", "        ", "train_one_epoch", "(", "model", ",", "optimizer", ",", "data_loader", ",", "args", ".", "device", ",", "epoch", ",", "print_freq", "=", "args", ".", "log_interval", ")", "\n", "# update the learning rate", "\n", "lr_scheduler", ".", "step", "(", ")", "\n", "\n", "if", "args", ".", "save", ":", "\n", "            ", "checkpoint_path", "=", "os", ".", "path", ".", "join", "(", "args", ".", "save", ",", "'checkpoint.pt'", ")", "\n", "torch", ".", "save", "(", "{", "'state_dict'", ":", "model", ".", "state_dict", "(", ")", ",", "'epoch'", ":", "epoch", "}", ",", "checkpoint_path", ")", "\n", "print", "(", "'\\nsaved the checkpoint to {}'", ".", "format", "(", "checkpoint_path", ")", ")", "\n", "\n", "# evaluate on the test dataset", "\n", "", "evaluate", "(", "model", ",", "data_loader_test", ",", "device", "=", "args", ".", "device", ")", "\n", "torch", ".", "cuda", ".", "empty_cache", "(", ")", "# trying to avoid occasional issues with GPU memory", "\n", "\n", "", "print", "(", "\"That's it!\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.detector.train_detector.get_backbone": [[94, 157], ["isinstance", "isinstance", "ppuda.utils.pretrained_model.apply", "eval", "eval", "ppuda.deepnets1m.net.Network", "isinstance", "ppuda.utils.pretrained_model", "isinstance", "isinstance", "m.register_forward_hook", "torch.Identity", "torch.Identity", "torch.Identity", "torch.Identity", "ppuda.deepnets1m.loader.DeepNets1M", "hasattr", "hasattr", "output.view.view", "int", "len", "len"], "function", ["home.repos.pwc.inspect_result.facebookresearch_ppuda.utils.utils.pretrained_model"], ["", "def", "get_backbone", "(", "args", ")", ":", "\n", "\n", "    ", "try", ":", "\n", "        ", "genotype", "=", "eval", "(", "'genotypes.%s'", "%", "args", ".", "arch", ")", "\n", "net_args", "=", "{", "'C'", ":", "args", ".", "init_channels", ",", "# 48 if genotype == DARTS else 128", "\n", "'genotype'", ":", "genotype", ",", "\n", "'n_cells'", ":", "args", ".", "layers", ",", "# 14 if genotype == DARTS else 12", "\n", "'C_mult'", ":", "int", "(", "genotype", "!=", "ViT", ")", "+", "1", ",", "# assume either ViT or DARTS-style architecture", "\n", "'preproc'", ":", "genotype", "!=", "ViT", ",", "\n", "'stem_type'", ":", "1", "}", "# assume that the ImageNet-style stem is used by default", "\n", "", "except", ":", "\n", "        ", "deepnets", "=", "DeepNets1M", "(", "split", "=", "args", ".", "split", ",", "\n", "nets_dir", "=", "args", ".", "data_dir", ",", "\n", "large_images", "=", "True", ",", "\n", "arch", "=", "args", ".", "arch", ")", "\n", "assert", "len", "(", "deepnets", ")", "==", "1", ",", "'one architecture must be chosen to train'", "\n", "graph", "=", "deepnets", "[", "0", "]", "\n", "net_args", ",", "idx", "=", "graph", ".", "net_args", ",", "graph", ".", "net_idx", "\n", "if", "'norm'", "in", "net_args", "and", "net_args", "[", "'norm'", "]", "==", "'bn'", ":", "\n", "            ", "net_args", "[", "'norm'", "]", "=", "'bn-track'", "\n", "", "if", "net_args", "[", "'genotype'", "]", "==", "ViT", ":", "\n", "            ", "net_args", "[", "'stem_type'", "]", "=", "1", "# using ImageNet style stem even for ViT", "\n", "\n", "", "", "num_classes", "=", "1000", "\n", "if", "isinstance", "(", "net_args", "[", "'genotype'", "]", ",", "str", ")", ":", "\n", "        ", "model", "=", "eval", "(", "'torchvision.models.%s(pretrained=%d)'", "%", "(", "net_args", "[", "'genotype'", "]", ",", "args", ".", "pretrained", ")", ")", "\n", "model", ".", "out_channels", "=", "model", ".", "fc", ".", "in_features", "\n", "", "else", ":", "\n", "        ", "model", "=", "Network", "(", "num_classes", "=", "num_classes", ",", "\n", "is_imagenet_input", "=", "True", ",", "\n", "is_vit", "=", "False", ",", "\n", "**", "net_args", ")", "\n", "model", ".", "out_channels", "=", "net_args", "[", "'C'", "]", "*", "len", "(", "net_args", "[", "'genotype'", "]", ".", "normal_concat", ")", "*", "(", "net_args", "[", "'C_mult'", "]", "**", "2", ")", "\n", "\n", "", "if", "args", ".", "ckpt", "is", "not", "None", "or", "isinstance", "(", "model", ",", "torchvision", ".", "models", ".", "ResNet", ")", ":", "\n", "        ", "model", "=", "pretrained_model", "(", "model", ",", "args", ".", "ckpt", ",", "num_classes", ",", "1", ",", "GHN", ")", "\n", "\n", "# Allow the detector to use this backbone just as a feature extractor without modifying backbone's code", "\n", "", "def", "fw_hook", "(", "module", ",", "input", ",", "output", ")", ":", "\n", "        ", "if", "isinstance", "(", "input", ",", "tuple", ")", ":", "\n", "            ", "input", "=", "input", "[", "0", "]", "\n", "", "if", "isinstance", "(", "output", ",", "tuple", ")", ":", "\n", "            ", "output", "=", "output", "[", "0", "]", "\n", "", "module", ".", "input_sz", "=", "input", ".", "shape", "\n", "if", "hasattr", "(", "module", ",", "'prev_mod'", ")", "and", "hasattr", "(", "module", ".", "prev_mod", ",", "'input_sz'", ")", ":", "\n", "            ", "output", "=", "output", ".", "view", "(", "module", ".", "prev_mod", ".", "input_sz", ")", "\n", "", "return", "output", "\n", "\n", "", "def", "add_fw_hooks", "(", "m", ")", ":", "\n", "        ", "m", ".", "register_forward_hook", "(", "fw_hook", ")", "\n", "\n", "", "if", "isinstance", "(", "net_args", "[", "'genotype'", "]", ",", "str", ")", ":", "\n", "        ", "model", ".", "fc", "=", "nn", ".", "Identity", "(", ")", "\n", "model", ".", "avgpool", "=", "nn", ".", "Identity", "(", ")", "\n", "model", ".", "fc", ".", "prev_mod", "=", "model", ".", "avgpool", "\n", "", "else", ":", "\n", "        ", "model", ".", "classifier", "=", "nn", ".", "Identity", "(", ")", "\n", "model", ".", "global_pooling", "=", "nn", ".", "Identity", "(", ")", "\n", "model", ".", "classifier", ".", "prev_mod", "=", "model", ".", "global_pooling", "\n", "\n", "", "model", ".", "apply", "(", "add_fw_hooks", ")", "\n", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.detector.train_detector.get_model_detection": [[159, 208], ["print", "train_detector.get_backbone", "torchvision.models.detection.rpn.AnchorGenerator", "torchvision.ops.MultiScaleRoIAlign", "torchvision.models.detection.FasterRCNN", "torchvision.models.detection.fasterrcnn_resnet50_fpn", "torchvision.models.resnet50", "torchvision.models.detection.faster_rcnn.FastRCNNPredictor", "hasattr", "torchvision.models.detection.transform.GeneralizedRCNNTransform", "int", "int", "ppuda.utils.capacity"], "function", ["home.repos.pwc.inspect_result.facebookresearch_ppuda.detector.train_detector.get_backbone", "home.repos.pwc.inspect_result.facebookresearch_ppuda.utils.utils.capacity"], ["", "def", "get_model_detection", "(", "args", ",", "num_classes", ")", ":", "\n", "\n", "# load a pre-trained model for classification and return", "\n", "# only the features", "\n", "    ", "if", "args", ".", "arch", "not", "in", "[", "''", ",", "None", "]", ":", "\n", "        ", "backbone", "=", "get_backbone", "(", "args", ")", "\n", "\n", "# let's make the RPN generate 5 x 3 anchors per spatial", "\n", "# location, with 5 different sizes and 3 different aspect", "\n", "# ratios. We have a Tuple[Tuple[int]] because each feature", "\n", "# map could potentially have different sizes and", "\n", "# aspect ratios", "\n", "anchor_generator", "=", "AnchorGenerator", "(", "sizes", "=", "(", "(", "32", ",", "64", ",", "128", ",", "256", ",", "512", ")", ",", ")", ",", "\n", "aspect_ratios", "=", "(", "(", "0.5", ",", "1.0", ",", "2.0", ")", ",", ")", ")", "\n", "\n", "# let's define what are the feature maps that we will", "\n", "# use to perform the region of interest cropping, as well as", "\n", "# the size of the crop after rescaling.", "\n", "# if your backbone returns a Tensor, featmap_names is expected to", "\n", "# be [0]. More generally, the backbone should return an", "\n", "# OrderedDict[Tensor], and in featmap_names you can choose which", "\n", "# feature maps to use.", "\n", "roi_pooler", "=", "torchvision", ".", "ops", ".", "MultiScaleRoIAlign", "(", "featmap_names", "=", "[", "'0'", "]", ",", "\n", "output_size", "=", "7", ",", "#14 if im_size is not None else 7,", "\n", "sampling_ratio", "=", "2", ")", "\n", "\n", "# put the pieces together inside a FasterRCNN model", "\n", "model", "=", "FasterRCNN", "(", "backbone", ",", "\n", "num_classes", "=", "num_classes", ",", "\n", "rpn_anchor_generator", "=", "anchor_generator", ",", "\n", "box_roi_pool", "=", "roi_pooler", ")", "\n", "\n", "if", "hasattr", "(", "model", ".", "backbone", ",", "'genotype'", ")", "and", "model", ".", "backbone", ".", "genotype", "==", "ViT", ":", "\n", "            ", "model", ".", "transform", "=", "GeneralizedRCNNTransform", "(", "int", "(", "800", "/", "2.5", ")", ",", "int", "(", "1333", "/", "2.5", ")", ",", "\n", "[", "0.485", ",", "0.456", ",", "0.406", "]", ",", "\n", "[", "0.229", ",", "0.224", ",", "0.225", "]", ")", "\n", "\n", "", "", "else", ":", "\n", "# load an instance segmentation model pre-trained pre-trained on COCO", "\n", "        ", "model", "=", "torchvision", ".", "models", ".", "detection", ".", "fasterrcnn_resnet50_fpn", "(", "pretrained", "=", "False", ")", "\n", "torchvision", ".", "models", ".", "resnet50", "(", ")", "\n", "# get number of input features for the classifier", "\n", "in_features", "=", "model", ".", "roi_heads", ".", "box_predictor", ".", "cls_score", ".", "in_features", "\n", "# replace the pre-trained head with a new one", "\n", "model", ".", "roi_heads", ".", "box_predictor", "=", "FastRCNNPredictor", "(", "in_features", ",", "num_classes", ")", "\n", "\n", "", "print", "(", "'Training the object detection model with %d parameters'", "%", "(", "capacity", "(", "model", ")", "[", "1", "]", ")", ",", "flush", "=", "True", ")", "\n", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.detector.penn.PennFudanDataset.__init__": [[14, 22], ["list", "list", "sorted", "sorted", "os.listdir", "os.listdir", "os.path.join", "os.path.join"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "root", ",", "transforms", ",", "box_resize", "=", "None", ")", ":", "\n", "        ", "self", ".", "root", "=", "root", "\n", "self", ".", "transforms", "=", "transforms", "\n", "self", ".", "box_resize", "=", "box_resize", "\n", "# load all image files, sorting them to", "\n", "# ensure that they are aligned", "\n", "self", ".", "imgs", "=", "list", "(", "sorted", "(", "os", ".", "listdir", "(", "os", ".", "path", ".", "join", "(", "root", ",", "\"PNGImages\"", ")", ")", ")", ")", "\n", "self", ".", "masks", "=", "list", "(", "sorted", "(", "os", ".", "listdir", "(", "os", ".", "path", ".", "join", "(", "root", ",", "\"PedMasks\"", ")", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.detector.penn.PennFudanDataset.__getitem__": [[23, 85], ["os.path.join", "os.path.join", "PIL.Image.open().convert", "PIL.Image.open", "numpy.array", "numpy.unique", "len", "range", "torch.as_tensor", "torch.ones", "torch.as_tensor", "torch.tensor", "torch.zeros", "numpy.where", "numpy.min", "numpy.max", "numpy.min", "numpy.max", "torch.as_tensor.append", "img.resize.resize.resize", "penn.PennFudanDataset.transforms", "PIL.Image.open"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_ppuda.detector.utils.SmoothedValue.max", "home.repos.pwc.inspect_result.facebookresearch_ppuda.detector.utils.SmoothedValue.max", "home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.graph.GraphBatch.append"], ["", "def", "__getitem__", "(", "self", ",", "idx", ")", ":", "\n", "# load images and masks", "\n", "        ", "img_path", "=", "os", ".", "path", ".", "join", "(", "self", ".", "root", ",", "\"PNGImages\"", ",", "self", ".", "imgs", "[", "idx", "]", ")", "\n", "mask_path", "=", "os", ".", "path", ".", "join", "(", "self", ".", "root", ",", "\"PedMasks\"", ",", "self", ".", "masks", "[", "idx", "]", ")", "\n", "img", "=", "Image", ".", "open", "(", "img_path", ")", ".", "convert", "(", "\"RGB\"", ")", "\n", "# note that we haven't converted the mask to RGB,", "\n", "# because each color corresponds to a different instance", "\n", "# with 0 being background", "\n", "mask", "=", "Image", ".", "open", "(", "mask_path", ")", "\n", "# convert the PIL Image into a numpy array", "\n", "mask", "=", "np", ".", "array", "(", "mask", ")", "\n", "# instances are encoded as different colors", "\n", "obj_ids", "=", "np", ".", "unique", "(", "mask", ")", "\n", "# first id is the background, so remove it", "\n", "obj_ids", "=", "obj_ids", "[", "1", ":", "]", "\n", "\n", "# split the color-encoded mask into a set", "\n", "# of binary masks", "\n", "masks", "=", "mask", "==", "obj_ids", "[", ":", ",", "None", ",", "None", "]", "\n", "\n", "# get bounding box coordinates for each mask", "\n", "num_objs", "=", "len", "(", "obj_ids", ")", "\n", "boxes", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "num_objs", ")", ":", "\n", "            ", "pos", "=", "np", ".", "where", "(", "masks", "[", "i", "]", ")", "\n", "xmin", "=", "np", ".", "min", "(", "pos", "[", "1", "]", ")", "\n", "xmax", "=", "np", ".", "max", "(", "pos", "[", "1", "]", ")", "\n", "ymin", "=", "np", ".", "min", "(", "pos", "[", "0", "]", ")", "\n", "ymax", "=", "np", ".", "max", "(", "pos", "[", "0", "]", ")", "\n", "boxes", ".", "append", "(", "[", "xmin", ",", "ymin", ",", "xmax", ",", "ymax", "]", ")", "\n", "\n", "# convert everything into a torch.Tensor", "\n", "", "boxes", "=", "torch", ".", "as_tensor", "(", "boxes", ",", "dtype", "=", "torch", ".", "float32", ")", "\n", "if", "self", ".", "box_resize", "is", "not", "None", ":", "\n", "            ", "width", ",", "height", "=", "img", ".", "size", "\n", "boxes", "[", ":", ",", "[", "0", ",", "2", "]", "]", "=", "boxes", "[", ":", ",", "[", "0", ",", "2", "]", "]", "/", "width", "*", "self", ".", "box_resize", "\n", "boxes", "[", ":", ",", "[", "1", ",", "3", "]", "]", "=", "boxes", "[", ":", ",", "[", "1", ",", "3", "]", "]", "/", "height", "*", "self", ".", "box_resize", "\n", "img", "=", "img", ".", "resize", "(", "(", "self", ".", "box_resize", ",", "self", ".", "box_resize", ")", ",", "Image", ".", "ANTIALIAS", ")", "\n", "# print(img.size, boxes.min().item(), boxes.max().item())", "\n", "\n", "# box_resize = box_resize / img.shape[0]", "\n", "# there is only one class", "\n", "", "labels", "=", "torch", ".", "ones", "(", "(", "num_objs", ",", ")", ",", "dtype", "=", "torch", ".", "int64", ")", "\n", "masks", "=", "torch", ".", "as_tensor", "(", "masks", ",", "dtype", "=", "torch", ".", "uint8", ")", "\n", "\n", "image_id", "=", "torch", ".", "tensor", "(", "[", "idx", "]", ")", "\n", "area", "=", "(", "boxes", "[", ":", ",", "3", "]", "-", "boxes", "[", ":", ",", "1", "]", ")", "*", "(", "boxes", "[", ":", ",", "2", "]", "-", "boxes", "[", ":", ",", "0", "]", ")", "\n", "# suppose all instances are not crowd", "\n", "iscrowd", "=", "torch", ".", "zeros", "(", "(", "num_objs", ",", ")", ",", "dtype", "=", "torch", ".", "int64", ")", "\n", "\n", "target", "=", "{", "}", "\n", "target", "[", "\"boxes\"", "]", "=", "boxes", "\n", "target", "[", "\"labels\"", "]", "=", "labels", "\n", "target", "[", "\"masks\"", "]", "=", "masks", "\n", "target", "[", "\"image_id\"", "]", "=", "image_id", "\n", "target", "[", "\"area\"", "]", "=", "area", "\n", "target", "[", "\"iscrowd\"", "]", "=", "iscrowd", "\n", "\n", "if", "self", ".", "transforms", "is", "not", "None", ":", "\n", "            ", "img", ",", "target", "=", "self", ".", "transforms", "(", "img", ",", "target", ")", "\n", "\n", "", "return", "img", ",", "target", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.detector.penn.PennFudanDataset.__len__": [[86, 88], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "imgs", ")", "", "", "", ""]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.detector.utils.SmoothedValue.__init__": [[24, 31], ["collections.deque"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "window_size", "=", "20", ",", "fmt", "=", "None", ")", ":", "\n", "        ", "if", "fmt", "is", "None", ":", "\n", "            ", "fmt", "=", "\"{median:.4f} ({global_avg:.4f})\"", "\n", "", "self", ".", "deque", "=", "deque", "(", "maxlen", "=", "window_size", ")", "\n", "self", ".", "total", "=", "0.0", "\n", "self", ".", "count", "=", "0", "\n", "self", ".", "fmt", "=", "fmt", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.detector.utils.SmoothedValue.update": [[32, 36], ["utils.SmoothedValue.deque.append"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.graph.GraphBatch.append"], ["", "def", "update", "(", "self", ",", "value", ",", "n", "=", "1", ")", ":", "\n", "        ", "self", ".", "deque", ".", "append", "(", "value", ")", "\n", "self", ".", "count", "+=", "n", "\n", "self", ".", "total", "+=", "value", "*", "n", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.detector.utils.SmoothedValue.synchronize_between_processes": [[37, 49], ["torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.barrier", "torch.barrier", "torch.all_reduce", "torch.all_reduce", "t.tolist.tolist.tolist", "int", "utils.is_dist_avail_and_initialized"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_ppuda.detector.utils.is_dist_avail_and_initialized"], ["", "def", "synchronize_between_processes", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Warning: does not synchronize the deque!\n        \"\"\"", "\n", "if", "not", "is_dist_avail_and_initialized", "(", ")", ":", "\n", "            ", "return", "\n", "", "t", "=", "torch", ".", "tensor", "(", "[", "self", ".", "count", ",", "self", ".", "total", "]", ",", "dtype", "=", "torch", ".", "float64", ",", "device", "=", "'cuda'", ")", "\n", "dist", ".", "barrier", "(", ")", "\n", "dist", ".", "all_reduce", "(", "t", ")", "\n", "t", "=", "t", ".", "tolist", "(", ")", "\n", "self", ".", "count", "=", "int", "(", "t", "[", "0", "]", ")", "\n", "self", ".", "total", "=", "t", "[", "1", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.detector.utils.SmoothedValue.median": [[50, 54], ["torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor.median().item", "torch.tensor.median().item", "list", "torch.tensor.median", "torch.tensor.median"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_ppuda.detector.utils.SmoothedValue.median", "home.repos.pwc.inspect_result.facebookresearch_ppuda.detector.utils.SmoothedValue.median"], ["", "@", "property", "\n", "def", "median", "(", "self", ")", ":", "\n", "        ", "d", "=", "torch", ".", "tensor", "(", "list", "(", "self", ".", "deque", ")", ")", "\n", "return", "d", ".", "median", "(", ")", ".", "item", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.detector.utils.SmoothedValue.avg": [[55, 59], ["torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor.mean().item", "torch.tensor.mean().item", "list", "torch.tensor.mean", "torch.tensor.mean"], "methods", ["None"], ["", "@", "property", "\n", "def", "avg", "(", "self", ")", ":", "\n", "        ", "d", "=", "torch", ".", "tensor", "(", "list", "(", "self", ".", "deque", ")", ",", "dtype", "=", "torch", ".", "float32", ")", "\n", "return", "d", ".", "mean", "(", ")", ".", "item", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.detector.utils.SmoothedValue.global_avg": [[60, 63], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "global_avg", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "total", "/", "self", ".", "count", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.detector.utils.SmoothedValue.max": [[64, 67], ["utils.SmoothedValue.max"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_ppuda.detector.utils.SmoothedValue.max"], ["", "@", "property", "\n", "def", "max", "(", "self", ")", ":", "\n", "        ", "return", "max", "(", "self", ".", "deque", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.detector.utils.SmoothedValue.value": [[68, 71], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "value", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "deque", "[", "-", "1", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.detector.utils.SmoothedValue.__str__": [[72, 79], ["utils.SmoothedValue.fmt.format"], "methods", ["None"], ["", "def", "__str__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "fmt", ".", "format", "(", "\n", "median", "=", "self", ".", "median", ",", "\n", "avg", "=", "self", ".", "avg", ",", "\n", "global_avg", "=", "self", ".", "global_avg", ",", "\n", "max", "=", "self", ".", "max", ",", "\n", "value", "=", "self", ".", "value", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.detector.utils.MetricLogger.__init__": [[152, 155], ["collections.defaultdict"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "delimiter", "=", "\"\\t\"", ")", ":", "\n", "        ", "self", ".", "meters", "=", "defaultdict", "(", "SmoothedValue", ")", "\n", "self", ".", "delimiter", "=", "delimiter", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.detector.utils.MetricLogger.update": [[156, 162], ["kwargs.items", "isinstance", "isinstance", "utils.MetricLogger.meters[].update", "v.item.item.item"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_ppuda.utils.trainer.Trainer.update"], ["", "def", "update", "(", "self", ",", "**", "kwargs", ")", ":", "\n", "        ", "for", "k", ",", "v", "in", "kwargs", ".", "items", "(", ")", ":", "\n", "            ", "if", "isinstance", "(", "v", ",", "torch", ".", "Tensor", ")", ":", "\n", "                ", "v", "=", "v", ".", "item", "(", ")", "\n", "", "assert", "isinstance", "(", "v", ",", "(", "float", ",", "int", ")", ")", "\n", "self", ".", "meters", "[", "k", "]", ".", "update", "(", "v", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.detector.utils.MetricLogger.__getattr__": [[163, 170], ["AttributeError", "type"], "methods", ["None"], ["", "", "def", "__getattr__", "(", "self", ",", "attr", ")", ":", "\n", "        ", "if", "attr", "in", "self", ".", "meters", ":", "\n", "            ", "return", "self", ".", "meters", "[", "attr", "]", "\n", "", "if", "attr", "in", "self", ".", "__dict__", ":", "\n", "            ", "return", "self", ".", "__dict__", "[", "attr", "]", "\n", "", "raise", "AttributeError", "(", "\"'{}' object has no attribute '{}'\"", ".", "format", "(", "\n", "type", "(", "self", ")", ".", "__name__", ",", "attr", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.detector.utils.MetricLogger.__str__": [[171, 178], ["utils.MetricLogger.meters.items", "utils.MetricLogger.delimiter.join", "loss_str.append", "str"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.graph.GraphBatch.append"], ["", "def", "__str__", "(", "self", ")", ":", "\n", "        ", "loss_str", "=", "[", "]", "\n", "for", "name", ",", "meter", "in", "self", ".", "meters", ".", "items", "(", ")", ":", "\n", "            ", "loss_str", ".", "append", "(", "\n", "\"{}: {}\"", ".", "format", "(", "name", ",", "str", "(", "meter", ")", ")", "\n", ")", "\n", "", "return", "self", ".", "delimiter", ".", "join", "(", "loss_str", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.detector.utils.MetricLogger.synchronize_between_processes": [[179, 182], ["utils.MetricLogger.meters.values", "meter.synchronize_between_processes"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_ppuda.detector.utils.MetricLogger.synchronize_between_processes"], ["", "def", "synchronize_between_processes", "(", "self", ")", ":", "\n", "        ", "for", "meter", "in", "self", ".", "meters", ".", "values", "(", ")", ":", "\n", "            ", "meter", ".", "synchronize_between_processes", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.detector.utils.MetricLogger.add_meter": [[183, 185], ["None"], "methods", ["None"], ["", "", "def", "add_meter", "(", "self", ",", "name", ",", "meter", ")", ":", "\n", "        ", "self", ".", "meters", "[", "name", "]", "=", "meter", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.detector.utils.MetricLogger.log_every": [[186, 239], ["time.time", "time.time", "utils.SmoothedValue", "utils.SmoothedValue", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "str", "print", "utils.MetricLogger.delimiter.join", "utils.MetricLogger.delimiter.join", "utils.SmoothedValue.update", "utils.SmoothedValue.update", "time.time", "time.time", "datetime.timedelta", "str", "str", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "len", "time.time", "time.time", "datetime.timedelta", "print", "print", "int", "len", "str", "len", "len", "utils.MetricLogger.format", "utils.MetricLogger.format", "len", "int", "len", "len", "str", "str", "str", "str", "str", "str", "torch.cuda.max_memory_allocated", "torch.cuda.max_memory_allocated", "torch.cuda.max_memory_allocated", "torch.cuda.max_memory_allocated"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_ppuda.utils.trainer.Trainer.update", "home.repos.pwc.inspect_result.facebookresearch_ppuda.utils.trainer.Trainer.update"], ["", "def", "log_every", "(", "self", ",", "iterable", ",", "print_freq", ",", "header", "=", "None", ")", ":", "\n", "        ", "i", "=", "0", "\n", "if", "not", "header", ":", "\n", "            ", "header", "=", "''", "\n", "", "start_time", "=", "time", ".", "time", "(", ")", "\n", "end", "=", "time", ".", "time", "(", ")", "\n", "iter_time", "=", "SmoothedValue", "(", "fmt", "=", "'{avg:.4f}'", ")", "\n", "data_time", "=", "SmoothedValue", "(", "fmt", "=", "'{avg:.4f}'", ")", "\n", "space_fmt", "=", "':'", "+", "str", "(", "len", "(", "str", "(", "len", "(", "iterable", ")", ")", ")", ")", "+", "'d'", "\n", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", ":", "\n", "            ", "log_msg", "=", "self", ".", "delimiter", ".", "join", "(", "[", "\n", "header", ",", "\n", "'[{0'", "+", "space_fmt", "+", "'}/{1}]'", ",", "\n", "'eta: {eta}'", ",", "\n", "'{meters}'", ",", "\n", "'time: {time}'", ",", "\n", "'data: {data}'", ",", "\n", "'max mem: {memory:.0f}'", "\n", "]", ")", "\n", "", "else", ":", "\n", "            ", "log_msg", "=", "self", ".", "delimiter", ".", "join", "(", "[", "\n", "header", ",", "\n", "'[{0'", "+", "space_fmt", "+", "'}/{1}]'", ",", "\n", "'eta: {eta}'", ",", "\n", "'{meters}'", ",", "\n", "'time: {time}'", ",", "\n", "'data: {data}'", "\n", "]", ")", "\n", "", "MB", "=", "1024.0", "*", "1024.0", "\n", "for", "obj", "in", "iterable", ":", "\n", "            ", "data_time", ".", "update", "(", "time", ".", "time", "(", ")", "-", "end", ")", "\n", "yield", "obj", "\n", "iter_time", ".", "update", "(", "time", ".", "time", "(", ")", "-", "end", ")", "\n", "if", "i", "%", "print_freq", "==", "0", "or", "i", "==", "len", "(", "iterable", ")", "-", "1", ":", "\n", "                ", "eta_seconds", "=", "iter_time", ".", "global_avg", "*", "(", "len", "(", "iterable", ")", "-", "i", ")", "\n", "eta_string", "=", "str", "(", "datetime", ".", "timedelta", "(", "seconds", "=", "int", "(", "eta_seconds", ")", ")", ")", "\n", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", ":", "\n", "                    ", "print", "(", "log_msg", ".", "format", "(", "\n", "i", ",", "len", "(", "iterable", ")", ",", "eta", "=", "eta_string", ",", "\n", "meters", "=", "str", "(", "self", ")", ",", "\n", "time", "=", "str", "(", "iter_time", ")", ",", "data", "=", "str", "(", "data_time", ")", ",", "\n", "memory", "=", "torch", ".", "cuda", ".", "max_memory_allocated", "(", ")", "/", "MB", ")", ")", "\n", "", "else", ":", "\n", "                    ", "print", "(", "log_msg", ".", "format", "(", "\n", "i", ",", "len", "(", "iterable", ")", ",", "eta", "=", "eta_string", ",", "\n", "meters", "=", "str", "(", "self", ")", ",", "\n", "time", "=", "str", "(", "iter_time", ")", ",", "data", "=", "str", "(", "data_time", ")", ")", ")", "\n", "", "", "i", "+=", "1", "\n", "end", "=", "time", ".", "time", "(", ")", "\n", "", "total_time", "=", "time", ".", "time", "(", ")", "-", "start_time", "\n", "total_time_str", "=", "str", "(", "datetime", ".", "timedelta", "(", "seconds", "=", "int", "(", "total_time", ")", ")", ")", "\n", "print", "(", "'{} Total time: {} ({:.4f} s / it)'", ".", "format", "(", "\n", "header", ",", "total_time_str", ",", "total_time", "/", "len", "(", "iterable", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.detector.utils.all_gather": [[81, 122], ["utils.get_world_size", "pickle.dumps", "torch.ByteStorage.from_buffer", "torch.ByteStorage.from_buffer", "torch.ByteTensor().to", "torch.ByteTensor().to", "torch.tensor", "torch.tensor", "torch.all_gather", "max", "torch.all_gather", "zip", "torch.tensor", "torch.tensor", "int", "tensor_list.append", "torch.empty", "torch.empty", "torch.cat", "torch.cat", "data_list.append", "torch.ByteTensor", "torch.ByteTensor", "torch.cat.numel", "range", "size.item", "torch.empty", "torch.empty", "torch.cat.cpu().numpy().tobytes", "pickle.loads", "torch.cat.cpu().numpy", "torch.cat.cpu"], "function", ["home.repos.pwc.inspect_result.facebookresearch_ppuda.detector.utils.get_world_size", "home.repos.pwc.inspect_result.facebookresearch_ppuda.detector.utils.all_gather", "home.repos.pwc.inspect_result.facebookresearch_ppuda.detector.utils.SmoothedValue.max", "home.repos.pwc.inspect_result.facebookresearch_ppuda.detector.utils.all_gather", "home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.graph.GraphBatch.append", "home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.graph.GraphBatch.append"], ["", "", "def", "all_gather", "(", "data", ")", ":", "\n", "    ", "\"\"\"\n    Run all_gather on arbitrary picklable data (not necessarily tensors)\n    Args:\n        data: any picklable object\n    Returns:\n        list[data]: list of data gathered from each rank\n    \"\"\"", "\n", "world_size", "=", "get_world_size", "(", ")", "\n", "if", "world_size", "==", "1", ":", "\n", "        ", "return", "[", "data", "]", "\n", "\n", "# serialized to a Tensor", "\n", "", "buffer", "=", "pickle", ".", "dumps", "(", "data", ")", "\n", "storage", "=", "torch", ".", "ByteStorage", ".", "from_buffer", "(", "buffer", ")", "\n", "tensor", "=", "torch", ".", "ByteTensor", "(", "storage", ")", ".", "to", "(", "\"cuda\"", ")", "\n", "\n", "# obtain Tensor size of each rank", "\n", "local_size", "=", "torch", ".", "tensor", "(", "[", "tensor", ".", "numel", "(", ")", "]", ",", "device", "=", "\"cuda\"", ")", "\n", "size_list", "=", "[", "torch", ".", "tensor", "(", "[", "0", "]", ",", "device", "=", "\"cuda\"", ")", "for", "_", "in", "range", "(", "world_size", ")", "]", "\n", "dist", ".", "all_gather", "(", "size_list", ",", "local_size", ")", "\n", "size_list", "=", "[", "int", "(", "size", ".", "item", "(", ")", ")", "for", "size", "in", "size_list", "]", "\n", "max_size", "=", "max", "(", "size_list", ")", "\n", "\n", "# receiving Tensor from all ranks", "\n", "# we pad the tensor because torch all_gather does not support", "\n", "# gathering tensors of different shapes", "\n", "tensor_list", "=", "[", "]", "\n", "for", "_", "in", "size_list", ":", "\n", "        ", "tensor_list", ".", "append", "(", "torch", ".", "empty", "(", "(", "max_size", ",", ")", ",", "dtype", "=", "torch", ".", "uint8", ",", "device", "=", "\"cuda\"", ")", ")", "\n", "", "if", "local_size", "!=", "max_size", ":", "\n", "        ", "padding", "=", "torch", ".", "empty", "(", "size", "=", "(", "max_size", "-", "local_size", ",", ")", ",", "dtype", "=", "torch", ".", "uint8", ",", "device", "=", "\"cuda\"", ")", "\n", "tensor", "=", "torch", ".", "cat", "(", "(", "tensor", ",", "padding", ")", ",", "dim", "=", "0", ")", "\n", "", "dist", ".", "all_gather", "(", "tensor_list", ",", "tensor", ")", "\n", "\n", "data_list", "=", "[", "]", "\n", "for", "size", ",", "tensor", "in", "zip", "(", "size_list", ",", "tensor_list", ")", ":", "\n", "        ", "buffer", "=", "tensor", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ".", "tobytes", "(", ")", "[", ":", "size", "]", "\n", "data_list", ".", "append", "(", "pickle", ".", "loads", "(", "buffer", ")", ")", "\n", "\n", "", "return", "data_list", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.detector.utils.reduce_dict": [[124, 149], ["utils.get_world_size", "torch.no_grad", "torch.no_grad", "sorted", "torch.stack", "torch.stack", "torch.all_reduce", "input_dict.keys", "names.append", "torch.stack.append", "zip"], "function", ["home.repos.pwc.inspect_result.facebookresearch_ppuda.detector.utils.get_world_size", "home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.graph.GraphBatch.append", "home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.graph.GraphBatch.append"], ["", "def", "reduce_dict", "(", "input_dict", ",", "average", "=", "True", ")", ":", "\n", "    ", "\"\"\"\n    Args:\n        input_dict (dict): all the values will be reduced\n        average (bool): whether to do average or sum\n    Reduce the values in the dictionary from all processes so that all processes\n    have the averaged results. Returns a dict with the same fields as\n    input_dict, after reduction.\n    \"\"\"", "\n", "world_size", "=", "get_world_size", "(", ")", "\n", "if", "world_size", "<", "2", ":", "\n", "        ", "return", "input_dict", "\n", "", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "        ", "names", "=", "[", "]", "\n", "values", "=", "[", "]", "\n", "# sort the keys so that they are consistent across processes", "\n", "for", "k", "in", "sorted", "(", "input_dict", ".", "keys", "(", ")", ")", ":", "\n", "            ", "names", ".", "append", "(", "k", ")", "\n", "values", ".", "append", "(", "input_dict", "[", "k", "]", ")", "\n", "", "values", "=", "torch", ".", "stack", "(", "values", ",", "dim", "=", "0", ")", "\n", "dist", ".", "all_reduce", "(", "values", ")", "\n", "if", "average", ":", "\n", "            ", "values", "/=", "world_size", "\n", "", "reduced_dict", "=", "{", "k", ":", "v", "for", "k", ",", "v", "in", "zip", "(", "names", ",", "values", ")", "}", "\n", "", "return", "reduced_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.detector.utils.collate_fn": [[241, 243], ["tuple", "zip"], "function", ["None"], ["", "", "def", "collate_fn", "(", "batch", ")", ":", "\n", "    ", "return", "tuple", "(", "zip", "(", "*", "batch", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.detector.utils.warmup_lr_scheduler": [[245, 254], ["torch.optim.lr_scheduler.LambdaLR", "torch.optim.lr_scheduler.LambdaLR", "float"], "function", ["None"], ["", "def", "warmup_lr_scheduler", "(", "optimizer", ",", "warmup_iters", ",", "warmup_factor", ")", ":", "\n", "\n", "    ", "def", "f", "(", "x", ")", ":", "\n", "        ", "if", "x", ">=", "warmup_iters", ":", "\n", "            ", "return", "1", "\n", "", "alpha", "=", "float", "(", "x", ")", "/", "warmup_iters", "\n", "return", "warmup_factor", "*", "(", "1", "-", "alpha", ")", "+", "alpha", "\n", "\n", "", "return", "torch", ".", "optim", ".", "lr_scheduler", ".", "LambdaLR", "(", "optimizer", ",", "f", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.detector.utils.mkdir": [[256, 262], ["os.makedirs"], "function", ["None"], ["", "def", "mkdir", "(", "path", ")", ":", "\n", "    ", "try", ":", "\n", "        ", "os", ".", "makedirs", "(", "path", ")", "\n", "", "except", "OSError", "as", "e", ":", "\n", "        ", "if", "e", ".", "errno", "!=", "errno", ".", "EEXIST", ":", "\n", "            ", "raise", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.detector.utils.setup_for_distributed": [[264, 277], ["kwargs.pop", "builtin_print"], "function", ["None"], ["", "", "", "def", "setup_for_distributed", "(", "is_master", ")", ":", "\n", "    ", "\"\"\"\n    This function disables printing when not in master process\n    \"\"\"", "\n", "import", "builtins", "as", "__builtin__", "\n", "builtin_print", "=", "__builtin__", ".", "print", "\n", "\n", "def", "print", "(", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "force", "=", "kwargs", ".", "pop", "(", "'force'", ",", "False", ")", "\n", "if", "is_master", "or", "force", ":", "\n", "            ", "builtin_print", "(", "*", "args", ",", "**", "kwargs", ")", "\n", "\n", "", "", "__builtin__", ".", "print", "=", "print", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.detector.utils.is_dist_avail_and_initialized": [[279, 285], ["torch.is_available", "torch.is_initialized"], "function", ["None"], ["", "def", "is_dist_avail_and_initialized", "(", ")", ":", "\n", "    ", "if", "not", "dist", ".", "is_available", "(", ")", ":", "\n", "        ", "return", "False", "\n", "", "if", "not", "dist", ".", "is_initialized", "(", ")", ":", "\n", "        ", "return", "False", "\n", "", "return", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.detector.utils.get_world_size": [[287, 291], ["torch.get_world_size", "utils.is_dist_avail_and_initialized"], "function", ["home.repos.pwc.inspect_result.facebookresearch_ppuda.detector.utils.get_world_size", "home.repos.pwc.inspect_result.facebookresearch_ppuda.detector.utils.is_dist_avail_and_initialized"], ["", "def", "get_world_size", "(", ")", ":", "\n", "    ", "if", "not", "is_dist_avail_and_initialized", "(", ")", ":", "\n", "        ", "return", "1", "\n", "", "return", "dist", ".", "get_world_size", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.detector.utils.get_rank": [[293, 297], ["torch.get_rank", "utils.is_dist_avail_and_initialized"], "function", ["home.repos.pwc.inspect_result.facebookresearch_ppuda.detector.utils.get_rank", "home.repos.pwc.inspect_result.facebookresearch_ppuda.detector.utils.is_dist_avail_and_initialized"], ["", "def", "get_rank", "(", ")", ":", "\n", "    ", "if", "not", "is_dist_avail_and_initialized", "(", ")", ":", "\n", "        ", "return", "0", "\n", "", "return", "dist", ".", "get_rank", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.detector.utils.is_main_process": [[299, 301], ["utils.get_rank"], "function", ["home.repos.pwc.inspect_result.facebookresearch_ppuda.detector.utils.get_rank"], ["", "def", "is_main_process", "(", ")", ":", "\n", "    ", "return", "get_rank", "(", ")", "==", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.detector.utils.save_on_master": [[303, 306], ["utils.is_main_process", "torch.save", "torch.save"], "function", ["home.repos.pwc.inspect_result.facebookresearch_ppuda.detector.utils.is_main_process"], ["", "def", "save_on_master", "(", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "    ", "if", "is_main_process", "(", ")", ":", "\n", "        ", "torch", ".", "save", "(", "*", "args", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.detector.utils.init_distributed_mode": [[308, 331], ["torch.cuda.set_device", "torch.cuda.set_device", "utils.setup_for_distributed.print", "torch.distributed.init_process_group", "torch.distributed.init_process_group", "torch.distributed.barrier", "torch.distributed.barrier", "utils.setup_for_distributed", "int", "int", "int", "int", "utils.setup_for_distributed.print", "torch.cuda.device_count", "torch.cuda.device_count"], "function", ["home.repos.pwc.inspect_result.facebookresearch_ppuda.detector.utils.setup_for_distributed"], ["", "", "def", "init_distributed_mode", "(", "args", ")", ":", "\n", "    ", "if", "'RANK'", "in", "os", ".", "environ", "and", "'WORLD_SIZE'", "in", "os", ".", "environ", ":", "\n", "        ", "args", ".", "rank", "=", "int", "(", "os", ".", "environ", "[", "\"RANK\"", "]", ")", "\n", "args", ".", "world_size", "=", "int", "(", "os", ".", "environ", "[", "'WORLD_SIZE'", "]", ")", "\n", "args", ".", "gpu", "=", "int", "(", "os", ".", "environ", "[", "'LOCAL_RANK'", "]", ")", "\n", "", "elif", "'SLURM_PROCID'", "in", "os", ".", "environ", ":", "\n", "        ", "args", ".", "rank", "=", "int", "(", "os", ".", "environ", "[", "'SLURM_PROCID'", "]", ")", "\n", "args", ".", "gpu", "=", "args", ".", "rank", "%", "torch", ".", "cuda", ".", "device_count", "(", ")", "\n", "", "else", ":", "\n", "        ", "print", "(", "'Not using distributed mode'", ")", "\n", "args", ".", "distributed", "=", "False", "\n", "return", "\n", "\n", "", "args", ".", "distributed", "=", "True", "\n", "\n", "torch", ".", "cuda", ".", "set_device", "(", "args", ".", "gpu", ")", "\n", "args", ".", "dist_backend", "=", "'nccl'", "\n", "print", "(", "'| distributed init (rank {}): {}'", ".", "format", "(", "\n", "args", ".", "rank", ",", "args", ".", "dist_url", ")", ",", "flush", "=", "True", ")", "\n", "torch", ".", "distributed", ".", "init_process_group", "(", "backend", "=", "args", ".", "dist_backend", ",", "init_method", "=", "args", ".", "dist_url", ",", "\n", "world_size", "=", "args", ".", "world_size", ",", "rank", "=", "args", ".", "rank", ")", "\n", "torch", ".", "distributed", ".", "barrier", "(", ")", "\n", "setup_for_distributed", "(", "args", ".", "rank", "==", "0", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.ppuda.config.init_config": [[48, 198], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "vars.dataset.lower", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "config.init_config.print_args"], "function", ["None"], ["", "def", "init_config", "(", "mode", "=", "'eval'", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "description", "=", "'Parameter Prediction for Unseen Deep Architectures'", ")", "\n", "\n", "# Data args", "\n", "parser", ".", "add_argument", "(", "'-d'", ",", "'--dataset'", ",", "type", "=", "str", ",", "default", "=", "'cifar10'", ",", "help", "=", "'image dataset: cifar10/imagenet/PennFudanPed.'", ")", "\n", "args", "=", "parser", ".", "parse_known_args", "(", ")", "[", "0", "]", "\n", "dataset", "=", "args", ".", "dataset", ".", "lower", "(", ")", "\n", "is_imagenet", "=", "dataset", "==", "'imagenet'", "\n", "is_detection", "=", "dataset", "==", "'pennfudanped'", "\n", "\n", "parser", ".", "add_argument", "(", "'-D'", ",", "'--data_dir'", ",", "type", "=", "str", ",", "default", "=", "'./data'", ",", "\n", "help", "=", "'where image dataset and DeepNets-1M are stored'", ")", "\n", "parser", ".", "add_argument", "(", "'--test_batch_size'", ",", "type", "=", "int", ",", "default", "=", "1", "if", "is_detection", "else", "64", ",", "help", "=", "'image batch size for testing'", ")", "\n", "\n", "# Generic args", "\n", "parser", ".", "add_argument", "(", "'-s'", ",", "'--seed'", ",", "type", "=", "int", ",", "default", "=", "1111", ",", "help", "=", "'random seed'", ")", "\n", "parser", ".", "add_argument", "(", "'-w'", ",", "'--num_workers'", ",", "type", "=", "int", ",", "default", "=", "8", "if", "is_imagenet", "else", "(", "4", "if", "is_detection", "else", "0", ")", ",", "\n", "help", "=", "'number of cpu processes to use'", ")", "\n", "parser", ".", "add_argument", "(", "'--device'", ",", "type", "=", "str", ",", "default", "=", "default_device", "(", ")", ",", "help", "=", "'device: cpu or cuda'", ")", "\n", "parser", ".", "add_argument", "(", "'--debug'", ",", "type", "=", "int", ",", "default", "=", "1", ",", "help", "=", "'the level of details printed out, 0 is the minimal level.'", ")", "\n", "parser", ".", "add_argument", "(", "'--ckpt'", ",", "type", "=", "str", ",", "default", "=", "None", ",", "\n", "help", "=", "'path to load the network/GHN parameters from'", ")", "\n", "\n", "is_train_ghn", "=", "mode", "==", "'train_ghn'", "\n", "is_train_net", "=", "mode", "==", "'train_net'", "\n", "is_eval", "=", "mode", "==", "'eval'", "\n", "\n", "# Generic training args", "\n", "parser", ".", "add_argument", "(", "'--split'", ",", "type", "=", "str", ",", "default", "=", "'train'", "if", "is_train_ghn", "else", "'predefined'", ",", "\n", "help", "=", "'training/testing split of DeepNets-1M'", ")", "\n", "\n", "if", "is_eval", "or", "is_train_net", ":", "\n", "        ", "parser", ".", "add_argument", "(", "'--arch'", ",", "type", "=", "str", ",", "\n", "default", "=", "'DARTS'", "if", "is_train_net", "else", "None", ",", "\n", "help", "=", "'one of the architectures: '", "\n", "'string for the predefined genotypes such as DARTS; '", "\n", "'the architecture index from DeepNets-1M'", ")", "\n", "parser", ".", "add_argument", "(", "'--noise'", ",", "action", "=", "'store_true'", ",", "help", "=", "'add noise to validation/test images'", ")", "\n", "if", "is_train_net", ":", "\n", "            ", "parser", ".", "add_argument", "(", "'--pretrained'", ",", "action", "=", "'store_true'", ",", "help", "=", "'use pretrained torchvision.models'", ")", "\n", "\n", "\n", "", "", "if", "is_train_ghn", "or", "is_train_net", ":", "\n", "\n", "# Predefine default arguments", "\n", "        ", "if", "is_train_ghn", ":", "\n", "            ", "batch_size", "=", "256", "if", "is_imagenet", "else", "64", "\n", "epochs", "=", "300", "\n", "lr", "=", "1e-3", "\n", "wd", "=", "1e-5", "\n", "", "else", ":", "\n", "            ", "if", "is_detection", ":", "\n", "                ", "batch_size", "=", "2", "\n", "epochs", "=", "10", "\n", "lr", "=", "0.005", "\n", "wd", "=", "0.0005", "\n", "", "else", ":", "\n", "                ", "batch_size", "=", "128", "if", "is_imagenet", "else", "96", "\n", "epochs", "=", "250", "if", "is_imagenet", "else", "600", "\n", "lr", "=", "0.1", "if", "is_imagenet", "else", "0.025", "\n", "wd", "=", "3e-5", "if", "is_imagenet", "else", "3e-4", "\n", "\n", "", "", "parser", ".", "add_argument", "(", "'-b'", ",", "'--batch_size'", ",", "type", "=", "int", ",", "default", "=", "batch_size", ",", "help", "=", "'image batch size for training'", ")", "\n", "parser", ".", "add_argument", "(", "'-e'", ",", "'--epochs'", ",", "type", "=", "int", ",", "default", "=", "epochs", ",", "help", "=", "'number of epochs to train'", ")", "\n", "parser", ".", "add_argument", "(", "'--lr'", ",", "type", "=", "float", ",", "default", "=", "lr", ",", "help", "=", "'initial learning rate'", ")", "\n", "parser", ".", "add_argument", "(", "'--grad_clip'", ",", "type", "=", "float", ",", "default", "=", "5", ",", "help", "=", "'grad clip'", ")", "\n", "parser", ".", "add_argument", "(", "'-l'", ",", "'--log_interval'", ",", "type", "=", "int", ",", "default", "=", "10", "if", "is_detection", "else", "100", ",", "\n", "help", "=", "'number of training iterations when print intermediate results'", ")", "\n", "parser", ".", "add_argument", "(", "'-S'", ",", "'--save_dir'", ",", "type", "=", "str", ",", "default", "=", "'./checkpoints'", ",", "\n", "help", "=", "'where to put all trained data and stuff'", ")", "\n", "parser", ".", "add_argument", "(", "'--multigpu'", ",", "action", "=", "'store_true'", ",", "help", "=", "'train on all gpus available'", ")", "\n", "parser", ".", "add_argument", "(", "'--wd'", ",", "type", "=", "float", ",", "default", "=", "wd", ",", "help", "=", "'weight decay'", ")", "\n", "parser", ".", "add_argument", "(", "'--name'", ",", "type", "=", "str", ",", "default", "=", "'EXP'", ",", "help", "=", "'experiment name'", ")", "\n", "parser", ".", "add_argument", "(", "'--amp'", ",", "action", "=", "'store_true'", ",", "help", "=", "'use Automatic Mixed Precision'", ")", "\n", "\n", "if", "is_train_ghn", ":", "\n", "            ", "parser", ".", "add_argument", "(", "'--lr_steps'", ",", "type", "=", "str", ",", "default", "=", "'200,250'", ",", "help", "=", "'epochs when to decrease lr'", ")", "\n", "parser", ".", "add_argument", "(", "'-g'", ",", "'--gamma'", ",", "type", "=", "float", ",", "default", "=", "0.1", ",", "help", "=", "'learning rate decay factor'", ")", "\n", "\n", "# GHN-specific args", "\n", "parser", ".", "add_argument", "(", "'--ln'", ",", "action", "=", "'store_true'", ",", "default", "=", "False", ",", "help", "=", "'apply LayerNorm for node embeddings'", ")", "\n", "parser", ".", "add_argument", "(", "'-M'", ",", "'--num_nets'", ",", "type", "=", "int", ",", "default", "=", "10", "**", "6", ",", "\n", "help", "=", "'number of training architectures'", ")", "\n", "parser", ".", "add_argument", "(", "'-v'", ",", "'--virtual_edges'", ",", "type", "=", "int", ",", "default", "=", "1", ",", "\n", "help", "=", "'the maximum shortest path distance to add in case of virtual edges '", "\n", "'(values <=1 correspond to the baseline and will not add virtual edges)'", ")", "\n", "parser", ".", "add_argument", "(", "'-n'", ",", "'--weight_norm'", ",", "action", "=", "'store_true'", ",", "default", "=", "False", ",", "\n", "help", "=", "'normalize predicted weights'", ")", "\n", "parser", ".", "add_argument", "(", "'-m'", ",", "'--meta_batch_size'", ",", "type", "=", "int", ",", "default", "=", "1", ",", "\n", "help", "=", "'how many nets to sample per batch of images'", ")", "\n", "parser", ".", "add_argument", "(", "'--decoder'", ",", "type", "=", "str", ",", "default", "=", "'conv'", ",", "choices", "=", "[", "'mlp'", ",", "'conv'", "]", ",", "\n", "help", "=", "'decoder to predict final parameters'", ")", "\n", "parser", ".", "add_argument", "(", "'-H'", ",", "'--hypernet'", ",", "type", "=", "str", ",", "default", "=", "'gatedgnn'", ",", "\n", "choices", "=", "[", "'mlp'", ",", "'gatedgnn'", "]", ",", "help", "=", "'message propagation network'", ")", "\n", "parser", ".", "add_argument", "(", "'--hid'", ",", "type", "=", "int", ",", "default", "=", "32", ",", "help", "=", "'number of hidden units'", ")", "\n", "\n", "", "else", ":", "\n", "            ", "parser", ".", "add_argument", "(", "'--momentum'", ",", "type", "=", "float", ",", "default", "=", "0.9", ",", "help", "=", "'momentum'", ")", "\n", "parser", ".", "add_argument", "(", "'--init_channels'", ",", "type", "=", "int", ",", "default", "=", "48", "if", "is_imagenet", "else", "36", ",", "help", "=", "'num of init channels, default is for DARTS'", ")", "\n", "parser", ".", "add_argument", "(", "'--layers'", ",", "type", "=", "int", ",", "default", "=", "14", "if", "is_imagenet", "else", "20", ",", "help", "=", "'total number of layers, default is for DARTS'", ")", "\n", "parser", ".", "add_argument", "(", "'--auxiliary'", ",", "action", "=", "'store_true'", ",", "default", "=", "False", ",", "help", "=", "'use auxiliary tower'", ")", "\n", "parser", ".", "add_argument", "(", "'--auxiliary_weight'", ",", "type", "=", "float", ",", "default", "=", "0.4", ",", "help", "=", "'weight for auxiliary loss'", ")", "\n", "parser", ".", "add_argument", "(", "'--cutout'", ",", "action", "=", "'store_true'", ",", "default", "=", "False", ",", "help", "=", "'use cutout'", ")", "\n", "parser", ".", "add_argument", "(", "'--cutout_length'", ",", "type", "=", "int", ",", "default", "=", "16", ",", "help", "=", "'cutout length'", ")", "\n", "parser", ".", "add_argument", "(", "'--drop_path_prob'", ",", "type", "=", "float", ",", "default", "=", "0", ",", "help", "=", "'drop path probability'", ")", "\n", "parser", ".", "add_argument", "(", "'--n_shots'", ",", "type", "=", "int", ",", "default", "=", "None", ",", "help", "=", "'number of training images per class for fine-tuning experiments'", ")", "\n", "\n", "\n", "", "", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "if", "is_train_ghn", ":", "\n", "        ", "args", ".", "lr_steps", "=", "list", "(", "map", "(", "int", ",", "args", ".", "lr_steps", ".", "split", "(", "','", ")", ")", ")", "\n", "s", "=", "16", "if", "args", ".", "dataset", "==", "'imagenet'", "else", "11", "\n", "args", ".", "max_shape", "=", "(", "64", ",", "64", ",", "s", ",", "s", ")", "\n", "\n", "", "def", "print_args", "(", "args", ",", "name", ")", ":", "\n", "        ", "print", "(", "'\\n%s:'", "%", "name", ")", "\n", "args", "=", "vars", "(", "args", ")", "\n", "for", "x", "in", "sorted", "(", "args", ".", "keys", "(", ")", ")", ":", "\n", "            ", "y", "=", "args", "[", "x", "]", "\n", "print", "(", "'{:20s}: {}'", ".", "format", "(", "x", "[", ":", "20", "]", ",", "y", ")", ")", "\n", "", "print", "(", "'\\n'", ",", "flush", "=", "True", ")", "\n", "\n", "", "print_args", "(", "args", ",", "'Script Arguments ({} mode)'", ".", "format", "(", "mode", ")", ")", "\n", "\n", "if", "hasattr", "(", "args", ",", "'multigpu'", ")", ":", "\n", "        ", "if", "args", ".", "multigpu", ":", "\n", "            ", "n_devices", "=", "torch", ".", "cuda", ".", "device_count", "(", ")", "\n", "if", "n_devices", ">", "1", ":", "\n", "                ", "assert", "args", ".", "device", "==", "'cuda'", ",", "(", "'multigpu can only be used together with device=cuda'", ",", "args", ".", "device", ")", "\n", "assert", "args", ".", "meta_batch_size", ">=", "n_devices", ",", "'multigpu requires meta_batch_size ({}) to be >= number of cuda device ({})'", ".", "format", "(", "args", ".", "meta_batch_size", ",", "n_devices", ")", "\n", "assert", "args", ".", "meta_batch_size", "%", "n_devices", "==", "0", ",", "'meta_batch_size ({}) must be a multiple of the number of cuda device ({})'", ".", "format", "(", "args", ".", "meta_batch_size", ",", "n_devices", ")", "\n", "print", "(", "'{} cuda devices are available for multigpu training\\n'", ".", "format", "(", "n_devices", ")", ")", "\n", "", "else", ":", "\n", "                ", "print", "(", "'multigpu argument is ignored: > 1 cuda devices necessary, while only {} cuda devices are available\\n'", ".", "format", "(", "n_devices", ")", ")", "\n", "args", ".", "multigpu", "=", "False", "\n", "\n", "\n", "", "", "", "set_seed", "(", "args", ".", "seed", ")", "\n", "\n", "args", ".", "save", "=", "None", "\n", "if", "mode", "!=", "'eval'", "and", "args", ".", "save_dir", "not", "in", "[", "'None'", ",", "'none'", ",", "''", "]", ":", "\n", "        ", "args", ".", "save", "=", "os", ".", "path", ".", "join", "(", "args", ".", "save_dir", ",", "'{}-{}-{}'", ".", "format", "(", "args", ".", "name", ",", "env", "[", "'git commit'", "]", ",", "args", ".", "seed", ")", ")", "# time.strftime(\"%Y%m%d-%H%M%S\")", "\n", "print", "(", "'Experiment dir: {}\\n'", ".", "format", "(", "args", ".", "save", ")", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "args", ".", "save", ")", ":", "\n", "            ", "os", ".", "mkdir", "(", "args", ".", "save", ")", "\n", "\n", "", "", "return", "args", "\n", "", ""]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.utils.darts_utils.CrossEntropyLabelSmooth.__init__": [[21, 26], ["torch.Module.__init__", "torch.LogSoftmax", "torch.LogSoftmax", "torch.LogSoftmax"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_ppuda.examples.custom_cnn.CNN.__init__"], ["    ", "def", "__init__", "(", "self", ",", "num_classes", ",", "epsilon", ")", ":", "\n", "        ", "super", "(", "CrossEntropyLabelSmooth", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "num_classes", "=", "num_classes", "\n", "self", ".", "epsilon", "=", "epsilon", "\n", "self", ".", "logsoftmax", "=", "nn", ".", "LogSoftmax", "(", "dim", "=", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.utils.darts_utils.CrossEntropyLabelSmooth.forward": [[27, 33], ["darts_utils.CrossEntropyLabelSmooth.logsoftmax", "torch.zeros_like().scatter_", "torch.zeros_like().scatter_", "torch.zeros_like().scatter_", "torch.zeros_like().scatter_", "torch.zeros_like().scatter_", "torch.zeros_like().scatter_", "torch.zeros_like().scatter_", "torch.zeros_like().scatter_", "torch.zeros_like().scatter_", "torch.zeros_like().scatter_.unsqueeze", "torch.zeros_like().scatter_.unsqueeze", "torch.zeros_like().scatter_.unsqueeze", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "inputs", ",", "targets", ")", ":", "\n", "        ", "log_probs", "=", "self", ".", "logsoftmax", "(", "inputs", ")", "\n", "targets", "=", "torch", ".", "zeros_like", "(", "log_probs", ")", ".", "scatter_", "(", "1", ",", "targets", ".", "unsqueeze", "(", "1", ")", ",", "1", ")", "\n", "targets", "=", "(", "1", "-", "self", ".", "epsilon", ")", "*", "targets", "+", "self", ".", "epsilon", "/", "self", ".", "num_classes", "\n", "loss", "=", "(", "-", "targets", "*", "log_probs", ")", ".", "mean", "(", "0", ")", ".", "sum", "(", ")", "\n", "return", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.utils.darts_utils.AvgrageMeter.__init__": [[37, 42], ["darts_utils.AvgrageMeter.reset"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_ppuda.utils.trainer.Trainer.reset"], ["    ", "def", "__init__", "(", "self", ",", "dispersion_measure", "=", "None", ")", ":", "\n", "        ", "self", ".", "dispersion_measure", "=", "dispersion_measure", "\n", "assert", "dispersion_measure", "in", "[", "None", ",", "'std'", ",", "'se'", "]", ",", "(", "\n", "'must be None, standard deviation (std) or standard error (se) of the mean'", ")", "\n", "self", ".", "reset", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.utils.darts_utils.AvgrageMeter.reset": [[43, 50], ["None"], "methods", ["None"], ["", "def", "reset", "(", "self", ")", ":", "\n", "        ", "self", ".", "avg", "=", "0", "\n", "self", ".", "sum", "=", "0", "\n", "self", ".", "cnt", "=", "0", "\n", "if", "self", ".", "dispersion_measure", "is", "not", "None", ":", "\n", "            ", "self", ".", "values", "=", "[", "]", "\n", "self", ".", "dispersion", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.utils.darts_utils.AvgrageMeter.update": [[51, 61], ["numpy.sum", "numpy.std", "abs", "numpy.mean", "len", "len", "numpy.sqrt", "numpy.mean"], "methods", ["None"], ["", "", "def", "update", "(", "self", ",", "val", ",", "n", "=", "1", ")", ":", "\n", "        ", "self", ".", "sum", "+=", "val", "*", "n", "\n", "self", ".", "cnt", "+=", "n", "\n", "self", ".", "avg", "=", "np", ".", "sum", "(", "self", ".", "sum", ")", "/", "self", ".", "cnt", "\n", "if", "self", ".", "dispersion_measure", "is", "not", "None", ":", "\n", "            ", "self", ".", "values", "+=", "[", "val", "]", "*", "n", "\n", "assert", "abs", "(", "self", ".", "avg", "-", "np", ".", "mean", "(", "self", ".", "values", ")", ")", "<", "1e-3", ",", "(", "self", ".", "avg", ",", "np", ".", "mean", "(", "self", ".", "values", ")", ")", "# sanity check", "\n", "assert", "len", "(", "self", ".", "values", ")", "==", "self", ".", "cnt", ",", "(", "len", "(", "self", ".", "values", ")", ",", "self", ".", "cnt", ")", "# sanity check", "\n", "sd", "=", "np", ".", "std", "(", "self", ".", "values", ")", "\n", "self", ".", "dispersion", "=", "sd", "if", "self", ".", "dispersion_measure", "==", "'std'", "else", "sd", "/", "np", ".", "sqrt", "(", "self", ".", "cnt", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.utils.darts_utils.accuracy": [[63, 76], ["max", "target.size", "output.topk", "pred.t.t", "pred.t.eq", "target.view().expand_as", "correct[].reshape().float().sum", "res.append", "correct[].reshape().float().sum.mul_", "target.view", "correct[].reshape().float", "correct[].reshape"], "function", ["home.repos.pwc.inspect_result.facebookresearch_ppuda.detector.utils.SmoothedValue.max", "home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.graph.GraphBatch.append"], ["", "", "", "def", "accuracy", "(", "output", ",", "target", ",", "topk", "=", "(", "1", ",", ")", ")", ":", "\n", "    ", "maxk", "=", "max", "(", "topk", ")", "\n", "batch_size", "=", "target", ".", "size", "(", "0", ")", "\n", "\n", "_", ",", "pred", "=", "output", ".", "topk", "(", "maxk", ",", "1", ",", "True", ",", "True", ")", "\n", "pred", "=", "pred", ".", "t", "(", ")", "\n", "correct", "=", "pred", ".", "eq", "(", "target", ".", "view", "(", "1", ",", "-", "1", ")", ".", "expand_as", "(", "pred", ")", ")", "\n", "\n", "res", "=", "[", "]", "\n", "for", "k", "in", "topk", ":", "\n", "        ", "correct_k", "=", "correct", "[", ":", "k", "]", ".", "reshape", "(", "-", "1", ")", ".", "float", "(", ")", ".", "sum", "(", "0", ")", "\n", "res", ".", "append", "(", "correct_k", ".", "mul_", "(", "100.0", "/", "batch_size", ")", ")", "\n", "", "return", "res", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.utils.darts_utils.drop_path": [[78, 85], ["torch.autograd.Variable", "x.div_", "x.mul_", "torch.cuda.FloatTensor().bernoulli_", "torch.cuda.FloatTensor().bernoulli_", "torch.cuda.FloatTensor().bernoulli_", "torch.cuda.FloatTensor", "torch.cuda.FloatTensor", "torch.cuda.FloatTensor", "x.size"], "function", ["None"], ["", "def", "drop_path", "(", "x", ",", "drop_prob", ")", ":", "\n", "    ", "if", "drop_prob", ">", "0.", ":", "\n", "        ", "keep_prob", "=", "1.", "-", "drop_prob", "\n", "mask", "=", "Variable", "(", "torch", ".", "cuda", ".", "FloatTensor", "(", "x", ".", "size", "(", "0", ")", ",", "1", ",", "1", ",", "1", ")", ".", "bernoulli_", "(", "keep_prob", ")", ")", "\n", "x", ".", "div_", "(", "keep_prob", ")", "\n", "x", ".", "mul_", "(", "mask", ")", "\n", "", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.utils.darts_utils.load_DARTS_pretrained": [[87, 104], ["state_dict.items", "net.load_state_dict", "torch.load", "torch.load", "torch.load", "name.startswith", "name.startswith", "name.startswith", "name.startswith", "name.replace"], "function", ["home.repos.pwc.inspect_result.facebookresearch_ppuda.ghn.nn.GHN.load", "home.repos.pwc.inspect_result.facebookresearch_ppuda.ghn.nn.GHN.load", "home.repos.pwc.inspect_result.facebookresearch_ppuda.ghn.nn.GHN.load"], ["", "def", "load_DARTS_pretrained", "(", "net", ",", "checkpoint", "=", "'./checkpoints/imagenet_model.pt'", ",", "device", "=", "'cpu'", ",", "load_class_layers", "=", "True", ")", ":", "\n", "    ", "state_dict", "=", "torch", ".", "load", "(", "checkpoint", ",", "map_location", "=", "device", ")", "[", "'state_dict'", "]", "\n", "state_dict_new", "=", "{", "}", "\n", "for", "name", ",", "p", "in", "state_dict", ".", "items", "(", ")", ":", "\n", "        ", "if", "name", ".", "startswith", "(", "'classifier.'", ")", ":", "\n", "            ", "if", "load_class_layers", ":", "\n", "                ", "if", "name", ".", "startswith", "(", "'classifier.w'", ")", "or", "name", ".", "startswith", "(", "'classifier.b'", ")", ":", "\n", "                    ", "state_dict_new", "[", "name", ".", "replace", "(", "'classifier.'", ",", "'classifier.0.'", ")", "]", "=", "p", "\n", "", "else", ":", "\n", "                    ", "state_dict_new", "[", "name", "]", "=", "p", "\n", "", "", "", "elif", "name", ".", "startswith", "(", "'auxiliary'", ")", ":", "\n", "            ", "continue", "\n", "", "else", ":", "\n", "            ", "state_dict_new", "[", "name", "]", "=", "p", "\n", "\n", "", "", "res", "=", "net", ".", "load_state_dict", "(", "state_dict_new", ",", "strict", "=", "False", ")", "\n", "return", "net", ",", "res", "\n", "", ""]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.utils.trainer.Trainer.__init__": [[20, 36], ["criterion.to", "trainer.Trainer.reset", "darts_utils.CrossEntropyLabelSmooth", "nn.CrossEntropyLoss", "torch.cuda.amp.GradScaler", "isinstance"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_ppuda.utils.trainer.Trainer.reset"], ["    ", "def", "__init__", "(", "self", ",", "optimizer", ",", "num_classes", ",", "is_imagenet", ",", "n_batches", ",", "\n", "grad_clip", "=", "5", ",", "auxiliary", "=", "False", ",", "auxiliary_weight", "=", "0.4", ",", "device", "=", "'cuda'", ",", "\n", "log_interval", "=", "100", ",", "amp", "=", "False", ")", ":", "\n", "        ", "self", ".", "optimizer", "=", "optimizer", "\n", "criterion", "=", "CrossEntropyLabelSmooth", "(", "num_classes", ",", "0.1", ")", "if", "is_imagenet", "else", "nn", ".", "CrossEntropyLoss", "(", ")", "\n", "self", ".", "criterion", "=", "criterion", ".", "to", "(", "device", "[", "0", "]", "if", "isinstance", "(", "device", ",", "(", "list", ",", "tuple", ")", ")", "else", "device", ")", "\n", "self", ".", "n_batches", "=", "n_batches", "\n", "self", ".", "grad_clip", "=", "grad_clip", "\n", "self", ".", "auxiliary", "=", "auxiliary", "\n", "self", ".", "auxiliary_weight", "=", "auxiliary_weight", "\n", "self", ".", "device", "=", "device", "\n", "self", ".", "log_interval", "=", "log_interval", "\n", "self", ".", "amp", "=", "amp", "\n", "if", "self", ".", "amp", ":", "\n", "            ", "self", ".", "scaler", "=", "torch", ".", "cuda", ".", "amp", ".", "GradScaler", "(", ")", "\n", "", "self", ".", "reset", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.utils.trainer.Trainer.reset": [[38, 42], ["time.time", "darts_utils.AvgrageMeter", "darts_utils.AvgrageMeter", "darts_utils.AvgrageMeter"], "methods", ["None"], ["", "def", "reset", "(", "self", ")", ":", "\n", "        ", "self", ".", "start", "=", "time", ".", "time", "(", ")", "\n", "self", ".", "step", "=", "0", "\n", "self", ".", "metrics", "=", "{", "'loss'", ":", "AvgrageMeter", "(", ")", ",", "'top1'", ":", "AvgrageMeter", "(", ")", ",", "'top5'", ":", "AvgrageMeter", "(", ")", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.utils.trainer.Trainer.update": [[44, 143], ["trainer.Trainer.optimizer.zero_grad", "torch.isnan", "nn.utils.clip_grad_norm_", "torch.stack", "targets.to.to.reshape().unsqueeze().expand().reshape", "logits.reshape.reshape.reshape", "darts_utils.accuracy", "len", "trainer.Trainer.metrics[].update", "trainer.Trainer.metrics[].update", "trainer.Trainer.metrics[].update", "torch.cuda.amp.autocast", "isinstance", "len", "RuntimeError", "trainer.Trainer.scaler.scale().backward", "trainer.Trainer.scaler.unscale_", "loss.backward", "parameters.extend", "trainer.Trainer.scaler.step", "trainer.Trainer.scaler.update", "trainer.Trainer.optimizer.step", "loss.item", "prec1.item", "prec5.item", "ghn", "targets.to.to.to", "len", "range", "images.to.to.to", "targets.to.to.to", "targets.to.to.reshape().unsqueeze().expand", "isinstance", "isinstance", "images.to.to.to", "parallel_apply", "zip", "isinstance", "model", "trainer.Trainer.criterion", "logits.reshape.reshape.append", "trainer.Trainer.scaler.scale", "isinstance", "graphs.to_device", "trainer.Trainer.criterion", "logits.reshape.reshape.append", "isinstance", "y.detach", "targets.to.to.reshape().unsqueeze", "y.detach", "trainer.Trainer.criterion", "len", "len", "len", "trainer.Trainer.criterion", "targets.to.to.reshape", "isinstance", "out[].to"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_ppuda.utils.darts_utils.accuracy", "home.repos.pwc.inspect_result.facebookresearch_ppuda.utils.trainer.Trainer.update", "home.repos.pwc.inspect_result.facebookresearch_ppuda.utils.trainer.Trainer.update", "home.repos.pwc.inspect_result.facebookresearch_ppuda.utils.trainer.Trainer.update", "home.repos.pwc.inspect_result.facebookresearch_ppuda.utils.trainer.Trainer.update", "home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.graph.GraphBatch.append", "home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.graph.GraphBatch.to_device", "home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.graph.GraphBatch.append"], ["", "def", "update", "(", "self", ",", "models", ",", "images", ",", "targets", ",", "ghn", "=", "None", ",", "graphs", "=", "None", ")", ":", "\n", "\n", "        ", "logits", "=", "[", "]", "\n", "loss", "=", "0", "\n", "\n", "self", ".", "optimizer", ".", "zero_grad", "(", ")", "\n", "\n", "with", "torch", ".", "cuda", ".", "amp", ".", "autocast", "(", "enabled", "=", "self", ".", "amp", ")", ":", "\n", "\n", "            ", "if", "ghn", "is", "not", "None", ":", "\n", "# Predict parameters", "\n", "                ", "models", "=", "ghn", "(", "models", ",", "graphs", "if", "isinstance", "(", "self", ".", "device", ",", "(", "list", ",", "tuple", ")", ")", "else", "graphs", ".", "to_device", "(", "self", ".", "device", ")", ")", "\n", "\n", "", "if", "isinstance", "(", "self", ".", "device", ",", "(", "list", ",", "tuple", ")", ")", ":", "\n", "# Multigpu training", "\n", "                ", "assert", "isinstance", "(", "models", ",", "(", "list", ",", "tuple", ")", ")", "and", "isinstance", "(", "models", "[", "0", "]", ",", "(", "list", ",", "tuple", ")", ")", ",", "'models must be a list of lists'", "\n", "image_replicas", "=", "[", "images", ".", "to", "(", "device", ",", "non_blocking", "=", "True", ")", "for", "device", "in", "self", ".", "device", "]", "\n", "targets", "=", "targets", ".", "to", "(", "self", ".", "device", "[", "0", "]", ",", "non_blocking", "=", "True", ")", "# loss will be computed on the first device", "\n", "\n", "models_per_device", "=", "len", "(", "models", "[", "0", "]", ")", "# assume that on the first device the number of models is >= than on other devices", "\n", "for", "ind", "in", "range", "(", "models_per_device", ")", ":", "# for index withing each device", "\n", "                    ", "model_replicas", "=", "[", "models", "[", "device", "]", "[", "ind", "]", "for", "device", "in", "self", ".", "device", "if", "ind", "<", "len", "(", "models", "[", "device", "]", ")", "]", "\n", "outputs", "=", "parallel_apply", "(", "model_replicas", ",", "\n", "image_replicas", "[", ":", "len", "(", "model_replicas", ")", "]", ",", "\n", "None", ",", "\n", "self", ".", "device", "[", ":", "len", "(", "model_replicas", ")", "]", ")", "# forward pass at each device in parallel", "\n", "\n", "# gather outputs from multiple devices and update the loss on the first device", "\n", "for", "device", ",", "out", "in", "zip", "(", "self", ".", "device", ",", "outputs", ")", ":", "\n", "                        ", "y", "=", "(", "out", "[", "0", "]", "if", "isinstance", "(", "out", ",", "(", "list", ",", "tuple", ")", ")", "else", "out", ")", ".", "to", "(", "self", ".", "device", "[", "0", "]", ")", "\n", "\n", "loss", "+=", "self", ".", "criterion", "(", "y", ",", "targets", ")", "\n", "if", "self", ".", "auxiliary", ":", "\n", "                            ", "loss", "+=", "self", ".", "auxiliary_weight", "*", "self", ".", "criterion", "(", "out", "[", "1", "]", ".", "to", "(", "self", ".", "device", "[", "0", "]", ")", ",", "targets", ")", "\n", "", "logits", ".", "append", "(", "y", ".", "detach", "(", ")", ")", "\n", "\n", "", "", "", "else", ":", "\n", "\n", "                ", "images", "=", "images", ".", "to", "(", "self", ".", "device", ",", "non_blocking", "=", "True", ")", "\n", "targets", "=", "targets", ".", "to", "(", "self", ".", "device", ",", "non_blocking", "=", "True", ")", "\n", "\n", "if", "not", "isinstance", "(", "models", ",", "(", "list", ",", "tuple", ")", ")", ":", "\n", "                    ", "models", "=", "[", "models", "]", "\n", "\n", "", "for", "model", "in", "models", ":", "\n", "                    ", "out", "=", "model", "(", "images", ")", "\n", "y", "=", "out", "[", "0", "]", "if", "isinstance", "(", "out", ",", "tuple", ")", "else", "out", "\n", "\n", "loss", "+=", "self", ".", "criterion", "(", "y", ",", "targets", ")", "\n", "if", "self", ".", "auxiliary", ":", "\n", "                        ", "loss", "+=", "self", ".", "auxiliary_weight", "*", "self", ".", "criterion", "(", "out", "[", "1", "]", ",", "targets", ")", "\n", "\n", "", "logits", ".", "append", "(", "y", ".", "detach", "(", ")", ")", "\n", "\n", "", "", "", "loss", "=", "loss", "/", "len", "(", "logits", ")", "# mean loss across models", "\n", "\n", "if", "torch", ".", "isnan", "(", "loss", ")", ":", "\n", "            ", "raise", "RuntimeError", "(", "'the loss is {}, unable to proceed. This issue can often be fixed by restarting the script and loading the saved checkpoint using the --ckpt argument.'", ".", "format", "(", "loss", ")", ")", "\n", "\n", "", "if", "self", ".", "amp", ":", "\n", "# Scales the loss, and calls backward()", "\n", "# to create scaled gradients", "\n", "            ", "self", ".", "scaler", ".", "scale", "(", "loss", ")", ".", "backward", "(", ")", "\n", "\n", "# Unscales the gradients of optimizer's assigned params in-place", "\n", "self", ".", "scaler", ".", "unscale_", "(", "self", ".", "optimizer", ")", "\n", "", "else", ":", "\n", "            ", "loss", ".", "backward", "(", ")", "\n", "\n", "", "parameters", "=", "[", "]", "\n", "for", "group", "in", "self", ".", "optimizer", ".", "param_groups", ":", "\n", "            ", "parameters", ".", "extend", "(", "group", "[", "'params'", "]", ")", "\n", "\n", "", "nn", ".", "utils", ".", "clip_grad_norm_", "(", "parameters", ",", "self", ".", "grad_clip", ")", "\n", "if", "self", ".", "amp", ":", "\n", "# Unscales gradients and calls", "\n", "# or skips optimizer.step()", "\n", "            ", "self", ".", "scaler", ".", "step", "(", "self", ".", "optimizer", ")", "\n", "\n", "# Updates the scale for next iteration", "\n", "self", ".", "scaler", ".", "update", "(", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "optimizer", ".", "step", "(", ")", "\n", "\n", "# Concatenate logits across models, duplicate targets accordingly", "\n", "", "logits", "=", "torch", ".", "stack", "(", "logits", ",", "dim", "=", "0", ")", "\n", "targets", "=", "targets", ".", "reshape", "(", "-", "1", ",", "1", ")", ".", "unsqueeze", "(", "0", ")", ".", "expand", "(", "logits", ".", "shape", "[", "0", "]", ",", "targets", ".", "shape", "[", "0", "]", ",", "1", ")", ".", "reshape", "(", "-", "1", ")", "\n", "logits", "=", "logits", ".", "reshape", "(", "-", "1", ",", "logits", ".", "shape", "[", "-", "1", "]", ")", "\n", "\n", "# Update training metrics", "\n", "prec1", ",", "prec5", "=", "accuracy", "(", "logits", ",", "targets", ",", "topk", "=", "(", "1", ",", "5", ")", ")", "\n", "n", "=", "len", "(", "targets", ")", "\n", "self", ".", "metrics", "[", "'loss'", "]", ".", "update", "(", "loss", ".", "item", "(", ")", ",", "n", ")", "\n", "self", ".", "metrics", "[", "'top1'", "]", ".", "update", "(", "prec1", ".", "item", "(", ")", ",", "n", ")", "\n", "self", ".", "metrics", "[", "'top5'", "]", ".", "update", "(", "prec5", ".", "item", "(", ")", ",", "n", ")", "\n", "\n", "self", ".", "step", "+=", "1", "\n", "\n", "return", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.utils.trainer.Trainer.log": [[145, 154], ["print", "time.time", "trainer.Trainer.metrics.items"], "methods", ["None"], ["", "def", "log", "(", "self", ",", "step", "=", "None", ")", ":", "\n", "        ", "step_", "=", "self", ".", "step", "if", "step", "is", "None", "else", "step", "\n", "if", "step_", "%", "self", ".", "log_interval", "==", "0", "or", "step_", ">=", "self", ".", "n_batches", "-", "1", ":", "\n", "            ", "speed", "=", "(", "time", ".", "time", "(", ")", "-", "self", ".", "start", ")", "/", "step_", "\n", "metrics", "=", "'\\t'", ".", "join", "(", "[", "'{}={:.2f}'", ".", "format", "(", "metric", ",", "value", ".", "avg", ")", "for", "metric", ",", "value", "in", "self", ".", "metrics", ".", "items", "(", ")", "]", ")", "\n", "print", "(", "'batch={:04d}/{:04d} \\t {} \\t {:.2f} sec/batch ({:.1f} min left) '", ".", "\n", "format", "(", "step_", ",", "self", ".", "n_batches", ",", "\n", "metrics", ",", "\n", "speed", ",", "speed", "*", "(", "self", ".", "n_batches", "-", "step_", ")", "/", "60", ")", ",", "flush", "=", "True", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.utils.utils.infer": [[27, 55], ["time.time", "darts_utils.AvgrageMeter", "darts_utils.AvgrageMeter", "torch.no_grad", "torch.no_grad", "torch.no_grad", "enumerate", "print", "tqdm.tqdm", "model", "darts_utils.accuracy", "top1.update", "top5.update", "list", "images.to", "targets.to", "prec1.item", "prec5.item", "model.parameters", "isinstance", "time.time"], "function", ["home.repos.pwc.inspect_result.facebookresearch_ppuda.utils.darts_utils.accuracy", "home.repos.pwc.inspect_result.facebookresearch_ppuda.utils.trainer.Trainer.update", "home.repos.pwc.inspect_result.facebookresearch_ppuda.utils.trainer.Trainer.update"], ["", "self", ".", "deque", "=", "deque", "(", "maxlen", "=", "window_size", ")", "\n", "self", ".", "total", "=", "0.0", "\n", "self", ".", "count", "=", "0", "\n", "self", ".", "fmt", "=", "fmt", "\n", "\n", "", "def", "update", "(", "self", ",", "value", ",", "n", "=", "1", ")", ":", "\n", "        ", "self", ".", "deque", ".", "append", "(", "value", ")", "\n", "self", ".", "count", "+=", "n", "\n", "self", ".", "total", "+=", "value", "*", "n", "\n", "\n", "", "def", "synchronize_between_processes", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Warning: does not synchronize the deque!\n        \"\"\"", "\n", "if", "not", "is_dist_avail_and_initialized", "(", ")", ":", "\n", "            ", "return", "\n", "", "t", "=", "torch", ".", "tensor", "(", "[", "self", ".", "count", ",", "self", ".", "total", "]", ",", "dtype", "=", "torch", ".", "float64", ",", "device", "=", "'cuda'", ")", "\n", "dist", ".", "barrier", "(", ")", "\n", "dist", ".", "all_reduce", "(", "t", ")", "\n", "t", "=", "t", ".", "tolist", "(", ")", "\n", "self", ".", "count", "=", "int", "(", "t", "[", "0", "]", ")", "\n", "self", ".", "total", "=", "t", "[", "1", "]", "\n", "\n", "", "@", "property", "\n", "def", "median", "(", "self", ")", ":", "\n", "        ", "d", "=", "torch", ".", "tensor", "(", "list", "(", "self", ".", "deque", ")", ")", "\n", "return", "d", ".", "median", "(", ")", ".", "item", "(", ")", "\n", "\n", "", "@", "property", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.utils.utils.pretrained_model": [[57, 99], ["os.path.exists", "torch.load", "torch.load", "torch.load", "isinstance", "ghn_class().to().eval", "ghn_class().to().eval.load_state_dict", "ghn_class().to().eval.", "print", "print", "NotImplementedError", "hasattr", "darts_utils.load_DARTS_pretrained", "dict", "list", "model.load_state_dict", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "type", "ghn_class().to", "torch.load", "torch.load", "torch.load", "model.named_parameters", "torch.load.keys", "name.startswith", "ghn_class"], "function", ["home.repos.pwc.inspect_result.facebookresearch_ppuda.ghn.nn.GHN.load", "home.repos.pwc.inspect_result.facebookresearch_ppuda.ghn.nn.GHN.load", "home.repos.pwc.inspect_result.facebookresearch_ppuda.ghn.nn.GHN.load", "home.repos.pwc.inspect_result.facebookresearch_ppuda.utils.darts_utils.load_DARTS_pretrained", "home.repos.pwc.inspect_result.facebookresearch_ppuda.ghn.nn.GHN.load", "home.repos.pwc.inspect_result.facebookresearch_ppuda.ghn.nn.GHN.load", "home.repos.pwc.inspect_result.facebookresearch_ppuda.ghn.nn.GHN.load"], ["        ", "d", "=", "torch", ".", "tensor", "(", "list", "(", "self", ".", "deque", ")", ",", "dtype", "=", "torch", ".", "float32", ")", "\n", "return", "d", ".", "mean", "(", ")", ".", "item", "(", ")", "\n", "\n", "", "@", "property", "\n", "def", "global_avg", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "total", "/", "self", ".", "count", "\n", "\n", "", "@", "property", "\n", "def", "max", "(", "self", ")", ":", "\n", "        ", "return", "max", "(", "self", ".", "deque", ")", "\n", "\n", "", "@", "property", "\n", "def", "value", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "deque", "[", "-", "1", "]", "\n", "\n", "", "def", "__str__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "fmt", ".", "format", "(", "\n", "median", "=", "self", ".", "median", ",", "\n", "avg", "=", "self", ".", "avg", ",", "\n", "global_avg", "=", "self", ".", "global_avg", ",", "\n", "max", "=", "self", ".", "max", ",", "\n", "value", "=", "self", ".", "value", ")", "\n", "\n", "\n", "", "", "def", "all_gather", "(", "data", ")", ":", "\n", "    ", "\"\"\"\n    Run all_gather on arbitrary picklable data (not necessarily tensors)\n    Args:\n        data: any picklable object\n    Returns:\n        list[data]: list of data gathered from each rank\n    \"\"\"", "\n", "world_size", "=", "get_world_size", "(", ")", "\n", "if", "world_size", "==", "1", ":", "\n", "        ", "return", "[", "data", "]", "\n", "\n", "# serialized to a Tensor", "\n", "", "buffer", "=", "pickle", ".", "dumps", "(", "data", ")", "\n", "storage", "=", "torch", ".", "ByteStorage", ".", "from_buffer", "(", "buffer", ")", "\n", "tensor", "=", "torch", ".", "ByteTensor", "(", "storage", ")", ".", "to", "(", "\"cuda\"", ")", "\n", "\n", "# obtain Tensor size of each rank", "\n", "local_size", "=", "torch", ".", "tensor", "(", "[", "tensor", ".", "numel", "(", ")", "]", ",", "device", "=", "\"cuda\"", ")", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.utils.utils.adjust_net": [[101, 177], ["isinstance", "utils.adjust_net.adjust_first_conv"], "function", ["None"], ["dist", ".", "all_gather", "(", "size_list", ",", "local_size", ")", "\n", "size_list", "=", "[", "int", "(", "size", ".", "item", "(", ")", ")", "for", "size", "in", "size_list", "]", "\n", "max_size", "=", "max", "(", "size_list", ")", "\n", "\n", "# receiving Tensor from all ranks", "\n", "# we pad the tensor because torch all_gather does not support", "\n", "# gathering tensors of different shapes", "\n", "tensor_list", "=", "[", "]", "\n", "for", "_", "in", "size_list", ":", "\n", "        ", "tensor_list", ".", "append", "(", "torch", ".", "empty", "(", "(", "max_size", ",", ")", ",", "dtype", "=", "torch", ".", "uint8", ",", "device", "=", "\"cuda\"", ")", ")", "\n", "", "if", "local_size", "!=", "max_size", ":", "\n", "        ", "padding", "=", "torch", ".", "empty", "(", "size", "=", "(", "max_size", "-", "local_size", ",", ")", ",", "dtype", "=", "torch", ".", "uint8", ",", "device", "=", "\"cuda\"", ")", "\n", "tensor", "=", "torch", ".", "cat", "(", "(", "tensor", ",", "padding", ")", ",", "dim", "=", "0", ")", "\n", "", "dist", ".", "all_gather", "(", "tensor_list", ",", "tensor", ")", "\n", "\n", "data_list", "=", "[", "]", "\n", "for", "size", ",", "tensor", "in", "zip", "(", "size_list", ",", "tensor_list", ")", ":", "\n", "        ", "buffer", "=", "tensor", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ".", "tobytes", "(", ")", "[", ":", "size", "]", "\n", "data_list", ".", "append", "(", "pickle", ".", "loads", "(", "buffer", ")", ")", "\n", "\n", "", "return", "data_list", "\n", "\n", "\n", "", "def", "reduce_dict", "(", "input_dict", ",", "average", "=", "True", ")", ":", "\n", "    ", "\"\"\"\n    Args:\n        input_dict (dict): all the values will be reduced\n        average (bool): whether to do average or sum\n    Reduce the values in the dictionary from all processes so that all processes\n    have the averaged results. Returns a dict with the same fields as\n    input_dict, after reduction.\n    \"\"\"", "\n", "world_size", "=", "get_world_size", "(", ")", "\n", "if", "world_size", "<", "2", ":", "\n", "        ", "return", "input_dict", "\n", "", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "        ", "names", "=", "[", "]", "\n", "values", "=", "[", "]", "\n", "# sort the keys so that they are consistent across processes", "\n", "for", "k", "in", "sorted", "(", "input_dict", ".", "keys", "(", ")", ")", ":", "\n", "            ", "names", ".", "append", "(", "k", ")", "\n", "values", ".", "append", "(", "input_dict", "[", "k", "]", ")", "\n", "", "values", "=", "torch", ".", "stack", "(", "values", ",", "dim", "=", "0", ")", "\n", "dist", ".", "all_reduce", "(", "values", ")", "\n", "if", "average", ":", "\n", "            ", "values", "/=", "world_size", "\n", "", "reduced_dict", "=", "{", "k", ":", "v", "for", "k", ",", "v", "in", "zip", "(", "names", ",", "values", ")", "}", "\n", "", "return", "reduced_dict", "\n", "\n", "\n", "", "class", "MetricLogger", "(", "object", ")", ":", "\n", "    ", "def", "__init__", "(", "self", ",", "delimiter", "=", "\"\\t\"", ")", ":", "\n", "        ", "self", ".", "meters", "=", "defaultdict", "(", "SmoothedValue", ")", "\n", "self", ".", "delimiter", "=", "delimiter", "\n", "\n", "", "def", "update", "(", "self", ",", "**", "kwargs", ")", ":", "\n", "        ", "for", "k", ",", "v", "in", "kwargs", ".", "items", "(", ")", ":", "\n", "            ", "if", "isinstance", "(", "v", ",", "torch", ".", "Tensor", ")", ":", "\n", "                ", "v", "=", "v", ".", "item", "(", ")", "\n", "", "assert", "isinstance", "(", "v", ",", "(", "float", ",", "int", ")", ")", "\n", "self", ".", "meters", "[", "k", "]", ".", "update", "(", "v", ")", "\n", "\n", "", "", "def", "__getattr__", "(", "self", ",", "attr", ")", ":", "\n", "        ", "if", "attr", "in", "self", ".", "meters", ":", "\n", "            ", "return", "self", ".", "meters", "[", "attr", "]", "\n", "", "if", "attr", "in", "self", ".", "__dict__", ":", "\n", "            ", "return", "self", ".", "__dict__", "[", "attr", "]", "\n", "", "raise", "AttributeError", "(", "\"'{}' object has no attribute '{}'\"", ".", "format", "(", "\n", "type", "(", "self", ")", ".", "__name__", ",", "attr", ")", ")", "\n", "\n", "", "def", "__str__", "(", "self", ")", ":", "\n", "        ", "loss_str", "=", "[", "]", "\n", "for", "name", ",", "meter", "in", "self", ".", "meters", ".", "items", "(", ")", ":", "\n", "            ", "loss_str", ".", "append", "(", "\n", "\"{}: {}\"", ".", "format", "(", "name", ",", "str", "(", "meter", ")", ")", "\n", ")", "\n", "", "return", "self", ".", "delimiter", ".", "join", "(", "loss_str", ")", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.utils.utils.set_seed": [[179, 186], ["torch.manual_seed", "torch.manual_seed", "torch.manual_seed", "torch.cuda.manual_seed", "torch.cuda.manual_seed", "torch.cuda.manual_seed", "torch.cuda.manual_seed_all", "torch.cuda.manual_seed_all", "torch.cuda.manual_seed_all", "random.seed", "numpy.random.seed"], "function", ["None"], ["", "def", "synchronize_between_processes", "(", "self", ")", ":", "\n", "        ", "for", "meter", "in", "self", ".", "meters", ".", "values", "(", ")", ":", "\n", "            ", "meter", ".", "synchronize_between_processes", "(", ")", "\n", "\n", "", "", "def", "add_meter", "(", "self", ",", "name", ",", "meter", ")", ":", "\n", "        ", "self", ".", "meters", "[", "name", "]", "=", "meter", "\n", "\n", "", "def", "log_every", "(", "self", ",", "iterable", ",", "print_freq", ",", "header", "=", "None", ")", ":", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.utils.utils.capacity": [[188, 195], ["model.named_parameters", "int", "numpy.prod"], "function", ["None"], ["if", "not", "header", ":", "\n", "            ", "header", "=", "''", "\n", "", "start_time", "=", "time", ".", "time", "(", ")", "\n", "end", "=", "time", ".", "time", "(", ")", "\n", "iter_time", "=", "SmoothedValue", "(", "fmt", "=", "'{avg:.4f}'", ")", "\n", "data_time", "=", "SmoothedValue", "(", "fmt", "=", "'{avg:.4f}'", ")", "\n", "space_fmt", "=", "':'", "+", "str", "(", "len", "(", "str", "(", "len", "(", "iterable", ")", ")", ")", ")", "+", "'d'", "\n", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", ":", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.utils.utils.rand_choice": [[197, 199], ["torch.randint", "torch.randint", "torch.randint", "len", "min", "len"], "function", ["None"], ["header", ",", "\n", "'[{0'", "+", "space_fmt", "+", "'}/{1}]'", ",", "\n", "'eta: {eta}'", ",", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.utils.utils.default_device": [[201, 203], ["torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available"], "function", ["None"], ["'time: {time}'", ",", "\n", "'data: {data}'", ",", "\n", "'max mem: {memory:.0f}'", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.ghn.gatedgnn.GatedGNN.__init__": [[18, 38], ["torch.Module.__init__", "mlp.MLP", "torch.GRUCell", "torch.GRUCell", "torch.GRUCell", "mlp.MLP"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_ppuda.examples.custom_cnn.CNN.__init__"], ["def", "__init__", "(", "self", ",", "\n", "in_features", "=", "32", ",", "\n", "ve", "=", "False", ",", "\n", "T", "=", "1", ")", ":", "\n", "        ", "\"\"\"\n        Initializes Gated Graph Neural Network.\n        :param in_features: how many features in each node.\n        :param ve: use virtual edges defined according to Eq. 4 in the paper.\n        :param T: number of forward+backward graph traversal steps.\n        \"\"\"", "\n", "super", "(", "GatedGNN", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "in_features", "=", "in_features", "\n", "self", ".", "hid", "=", "in_features", "\n", "self", ".", "ve", "=", "ve", "\n", "self", ".", "T", "=", "T", "\n", "self", ".", "mlp", "=", "MLP", "(", "in_features", ",", "hid", "=", "(", "(", "self", ".", "hid", "//", "2", ")", "if", "ve", "else", "self", ".", "hid", ",", "self", ".", "hid", ")", ")", "\n", "if", "ve", ":", "\n", "            ", "self", ".", "mlp_ve", "=", "MLP", "(", "in_features", ",", "hid", "=", "(", "self", ".", "hid", "//", "2", ",", "self", ".", "hid", ")", ")", "\n", "\n", "", "self", ".", "gru", "=", "nn", ".", "GRUCell", "(", "self", ".", "hid", ",", "self", ".", "hid", ")", "# shared across all nodes/cells in a graph", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.ghn.gatedgnn.GatedGNN.forward": [[40, 107], ["torch.cat().to().view", "torch.cat().to().view", "torch.cat().to().view", "torch.cat().to().view", "torch.cat().to().view", "torch.cat().to().view", "torch.cat().to().view", "torch.cat().to().view", "torch.cat().to().view", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "edge_graph_ind.view().expand.view().expand.view().expand", "range", "torch.unique", "torch.unique", "torch.unique", "torch.unique", "torch.unique", "torch.unique", "torch.unique", "torch.unique", "torch.unique", "len", "n_nodes.sum", "len", "n_nodes.sum", "ve.view.view.view", "torch.cumsum", "torch.cumsum", "torch.cumsum", "torch.cumsum", "torch.cumsum", "torch.cumsum", "torch.cumsum", "torch.cumsum", "torch.cumsum", "torch.arange.view", "torch.arange.view", "torch.arange.view", "x.dim", "edges.dim", "torch.pad", "torch.pad", "torch.pad", "torch.cat().to", "torch.cat().to", "torch.cat().to", "torch.cat().to", "torch.cat().to", "torch.cat().to", "torch.cat().to", "torch.cat().to", "torch.cat().to", "edges[].max", "edges[].view", "torch.arange.view", "torch.arange.view", "torch.arange.view", "is_1hop.view", "edge_graph_ind.view().expand.view().expand.view", "torch.flipud", "torch.flipud", "torch.flipud", "torch.flipud", "torch.flipud", "torch.flipud", "torch.flipud", "torch.flipud", "torch.flipud", "torch.nonzero().view", "torch.nonzero().view", "torch.nonzero().view", "torch.nonzero().view", "torch.nonzero().view", "torch.nonzero().view", "torch.nonzero().view", "torch.nonzero().view", "torch.nonzero().view", "gatedgnn.GatedGNN.mlp", "torch.zeros().scatter_add_", "torch.zeros().scatter_add_", "torch.zeros().scatter_add_", "torch.zeros().scatter_add_", "torch.zeros().scatter_add_", "torch.zeros().scatter_add_", "torch.zeros().scatter_add_", "torch.zeros().scatter_add_", "torch.zeros().scatter_add_", "torch.nonzero().view", "torch.nonzero().view", "torch.nonzero().view", "torch.nonzero().view", "torch.nonzero().view", "torch.nonzero().view", "torch.nonzero().view", "torch.nonzero().view", "torch.nonzero().view", "gatedgnn.GatedGNN.gru().to", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.nonzero().view", "torch.nonzero().view", "torch.nonzero().view", "torch.nonzero().view", "torch.nonzero().view", "torch.nonzero().view", "torch.nonzero().view", "torch.nonzero().view", "torch.nonzero().view", "m.scatter_add_.scatter_add_.scatter_add_", "torch.nonzero", "torch.nonzero", "torch.nonzero", "torch.nonzero", "torch.nonzero", "torch.nonzero", "torch.nonzero", "torch.nonzero", "torch.nonzero", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "gatedgnn.GatedGNN.mlp_ve", "ve[].to", "torch.nonzero", "torch.nonzero", "torch.nonzero", "torch.nonzero", "torch.nonzero", "torch.nonzero", "torch.nonzero", "torch.nonzero", "torch.nonzero", "gatedgnn.GatedGNN.gru", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.nonzero", "torch.nonzero", "torch.nonzero", "torch.nonzero", "torch.nonzero", "torch.nonzero", "torch.nonzero", "torch.nonzero", "torch.nonzero"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_ppuda.detector.utils.SmoothedValue.max"], ["", "def", "forward", "(", "self", ",", "x", ",", "edges", ",", "node_graph_ind", ")", ":", "\n", "        ", "r\"\"\"\n        Updates node features by sequentially traversing the graph in the forward and backward directions.\n        :param x: (N, C) node features, where N is the total number of nodes in a batch of B graphs, C is node feature dimensionality.\n        :param edges: (M, 4) tensor of edges, where M is the total number of edges;\n                       first column in edges is the row indices of edges,\n                       second column in edges is the column indices of edges,\n                       third column in edges is the shortest path distance between the nodes,\n                       fourth column in edges is the graph indices (from 0 to B-1) within a batch for each edge.\n        :param node_graph_ind: (N,) tensor of graph indices (from 0 to B-1) within a batch for each node.\n        :return: updated (N, C) node features.\n        \"\"\"", "\n", "\n", "assert", "x", ".", "dim", "(", ")", "==", "2", "and", "edges", ".", "dim", "(", ")", "==", "2", "and", "edges", ".", "shape", "[", "1", "]", "==", "4", ",", "(", "x", ".", "shape", ",", "edges", ".", "shape", ")", "\n", "n_nodes", "=", "torch", ".", "unique", "(", "node_graph_ind", ",", "return_counts", "=", "True", ")", "[", "1", "]", "\n", "\n", "B", ",", "C", "=", "len", "(", "n_nodes", ")", ",", "x", ".", "shape", "[", "1", "]", "# batch size, features", "\n", "\n", "ve", ",", "edge_graph_ind", "=", "edges", "[", ":", ",", "2", "]", ",", "edges", "[", ":", ",", "3", "]", "\n", "\n", "assert", "n_nodes", ".", "sum", "(", ")", "==", "len", "(", "x", ")", ",", "(", "n_nodes", ".", "sum", "(", ")", ",", "x", ".", "shape", ")", "\n", "\n", "is_1hop", "=", "ve", "==", "1", "\n", "if", "self", ".", "ve", ":", "\n", "            ", "ve", "=", "ve", ".", "view", "(", "-", "1", ",", "1", ")", "# according to Eq. 4 in the paper", "\n", "\n", "", "traversal_orders", "=", "[", "1", ",", "0", "]", "# forward, backward", "\n", "\n", "edge_offset", "=", "torch", ".", "cumsum", "(", "F", ".", "pad", "(", "n_nodes", "[", ":", "-", "1", "]", ",", "(", "1", ",", "0", ")", ")", ",", "0", ")", "[", "edge_graph_ind", "]", "\n", "node_inds", "=", "torch", ".", "cat", "(", "[", "torch", ".", "arange", "(", "n", ")", "for", "n", "in", "n_nodes", "]", ")", ".", "to", "(", "x", ".", "device", ")", ".", "view", "(", "-", "1", ",", "1", ")", "\n", "\n", "# Parallelize computation of indices and masks of one/all hops", "\n", "# This will slightly speed up the operations in the main loop", "\n", "# But indexing of the GPU tensors (used in the main loop) for some reason remains slow, see", "\n", "# https://github.com/pytorch/pytorch/issues/29973 for more info", "\n", "all_nodes", "=", "torch", ".", "arange", "(", "edges", "[", ":", ",", "1", "]", ".", "max", "(", ")", "+", "1", ",", "device", "=", "x", ".", "device", ")", "\n", "masks_1hop", ",", "masks_all", "=", "{", "}", ",", "{", "}", "\n", "for", "order", "in", "traversal_orders", ":", "\n", "            ", "masks_all", "[", "order", "]", "=", "edges", "[", ":", ",", "order", "]", ".", "view", "(", "1", ",", "-", "1", ")", "==", "all_nodes", ".", "view", "(", "-", "1", ",", "1", ")", "\n", "masks_1hop", "[", "order", "]", "=", "masks_all", "[", "order", "]", "&", "is_1hop", ".", "view", "(", "1", ",", "-", "1", ")", "\n", "", "mask2d", "=", "node_inds", "==", "all_nodes", ".", "view", "(", "1", ",", "-", "1", ")", "\n", "edge_graph_ind", "=", "edge_graph_ind", ".", "view", "(", "-", "1", ",", "1", ")", ".", "expand", "(", "-", "1", ",", "C", ")", "\n", "\n", "hx", "=", "x", "# initial hidden node features", "\n", "\n", "# Main loop", "\n", "for", "t", "in", "range", "(", "self", ".", "T", ")", ":", "\n", "            ", "for", "order", "in", "traversal_orders", ":", "# forward, backward", "\n", "                ", "start", "=", "edges", "[", ":", ",", "1", "-", "order", "]", "+", "edge_offset", "# node indices from which the message will be passed further", "\n", "for", "node", "in", "(", "all_nodes", "if", "order", "else", "torch", ".", "flipud", "(", "all_nodes", ")", ")", ":", "\n", "\n", "# Compute the message by aggregating features from neighbors", "\n", "                    ", "e_1hop", "=", "torch", ".", "nonzero", "(", "masks_1hop", "[", "order", "]", "[", "node", ",", ":", "]", ")", ".", "view", "(", "-", "1", ")", "\n", "m", "=", "self", ".", "mlp", "(", "hx", "[", "start", "[", "e_1hop", "]", "]", ")", "# transform node features of all 1-hop neighbors", "\n", "m", "=", "torch", ".", "zeros", "(", "B", ",", "C", ",", "dtype", "=", "m", ".", "dtype", ",", "device", "=", "m", ".", "device", ")", ".", "scatter_add_", "(", "0", ",", "edge_graph_ind", "[", "e_1hop", "]", ",", "m", ")", "# sum the transformed features into a (B,C) tensor", "\n", "if", "self", ".", "ve", ":", "\n", "                        ", "e", "=", "torch", ".", "nonzero", "(", "masks_all", "[", "order", "]", "[", "node", ",", ":", "]", ")", ".", "view", "(", "-", "1", ")", "# virtual edges connected to node", "\n", "m_ve", "=", "self", ".", "mlp_ve", "(", "hx", "[", "start", "[", "e", "]", "]", ")", "/", "ve", "[", "e", "]", ".", "to", "(", "m", ")", "# transform node features of all ve-hop neighbors", "\n", "m", "=", "m", ".", "scatter_add_", "(", "0", ",", "edge_graph_ind", "[", "e", "]", ",", "m_ve", ")", "# sum m and m_ve according to Eq. 4 in the paper", "\n", "\n", "# Udpate node hidden states in parallel for a batch of graphs", "\n", "", "ind", "=", "torch", ".", "nonzero", "(", "mask2d", "[", ":", ",", "node", "]", ")", ".", "view", "(", "-", "1", ")", "\n", "if", "B", ">", "1", ":", "\n", "                        ", "m", "=", "m", "[", "node_graph_ind", "[", "ind", "]", "]", "\n", "", "hx", "[", "ind", "]", "=", "self", ".", "gru", "(", "m", ",", "hx", "[", "ind", "]", ")", ".", "to", "(", "hx", ")", "# 'to(hx)' is to make automatic mixed precision work", "\n", "\n", "", "", "", "return", "hx", "\n", "", "", ""]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.ghn.decoder.ConvDecoder.__init__": [[20, 44], ["torch.Module.__init__", "torch.Sequential", "enumerate", "torch.Sequential", "torch.Sequential", "len", "torch.Linear", "torch.ReLU", "conv.extend", "torch.ReLU", "torch.Conv2d", "numpy.prod", "numpy.prod", "torch.Conv2d", "layers.get_activation", "len", "len"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_ppuda.examples.custom_cnn.CNN.__init__", "home.repos.pwc.inspect_result.facebookresearch_ppuda.ghn.layers.get_activation"], ["    ", "def", "__init__", "(", "self", ",", "\n", "in_features", "=", "64", ",", "\n", "hid", "=", "(", "128", ",", "256", ")", ",", "\n", "out_shape", "=", "None", ",", "\n", "num_classes", "=", "None", ")", ":", "\n", "        ", "super", "(", "ConvDecoder", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "assert", "len", "(", "hid", ")", ">", "0", ",", "hid", "\n", "self", ".", "out_shape", "=", "out_shape", "\n", "self", ".", "num_classes", "=", "num_classes", "\n", "self", ".", "fc", "=", "nn", ".", "Sequential", "(", "nn", ".", "Linear", "(", "in_features", ",", "\n", "hid", "[", "0", "]", "*", "np", ".", "prod", "(", "out_shape", "[", "2", ":", "]", ")", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ")", "\n", "\n", "conv", "=", "[", "]", "\n", "for", "j", ",", "n_hid", "in", "enumerate", "(", "hid", ")", ":", "\n", "            ", "n_out", "=", "np", ".", "prod", "(", "out_shape", "[", ":", "2", "]", ")", "if", "j", "==", "len", "(", "hid", ")", "-", "1", "else", "hid", "[", "j", "+", "1", "]", "\n", "conv", ".", "extend", "(", "[", "nn", ".", "Conv2d", "(", "n_hid", ",", "n_out", ",", "1", ")", ",", "\n", "get_activation", "(", "None", "if", "j", "==", "len", "(", "hid", ")", "-", "1", "else", "'relu'", ")", "]", ")", "\n", "\n", "", "self", ".", "conv", "=", "nn", ".", "Sequential", "(", "*", "conv", ")", "\n", "self", ".", "class_layer_predictor", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Conv2d", "(", "out_shape", "[", "0", "]", ",", "num_classes", ",", "1", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.ghn.decoder.ConvDecoder.forward": [[46, 62], ["decoder.ConvDecoder.fc().view", "decoder.ConvDecoder.conv().view", "sum", "decoder.ConvDecoder.class_layer_predictor", "decoder.ConvDecoder.fc", "decoder.ConvDecoder.conv"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ",", "max_shape", "=", "(", "1", ",", "1", ")", ",", "class_pred", "=", "False", ")", ":", "\n", "\n", "        ", "N", "=", "x", ".", "shape", "[", "0", "]", "\n", "x", "=", "self", ".", "fc", "(", "x", ")", ".", "view", "(", "N", ",", "-", "1", ",", "*", "self", ".", "out_shape", "[", "2", ":", "]", ")", "# N,128,11,11", "\n", "out_shape", "=", "self", ".", "out_shape", "\n", "if", "sum", "(", "max_shape", ")", ">", "0", ":", "\n", "            ", "x", "=", "x", "[", ":", ",", ":", ",", ":", "max_shape", "[", "0", "]", ",", ":", "max_shape", "[", "1", "]", "]", "\n", "out_shape", "=", "(", "out_shape", "[", "0", "]", ",", "out_shape", "[", "1", "]", ",", "max_shape", "[", "0", "]", ",", "max_shape", "[", "1", "]", ")", "\n", "\n", "", "x", "=", "self", ".", "conv", "(", "x", ")", ".", "view", "(", "N", ",", "*", "out_shape", ")", "# N, out, in, h, w", "\n", "\n", "if", "class_pred", ":", "\n", "            ", "x", "=", "self", ".", "class_layer_predictor", "(", "x", "[", ":", ",", ":", ",", ":", ",", ":", ",", "0", "]", ")", "# N, num_classes, 64, 1", "\n", "x", "=", "x", "[", ":", ",", ":", ",", ":", ",", "0", "]", "# N, num_classes, 64", "\n", "\n", "", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.ghn.decoder.MLPDecoder.__init__": [[66, 83], ["torch.Module.__init__", "mlp.MLP", "torch.Sequential", "len", "layers.get_activation", "torch.Linear", "numpy.prod"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_ppuda.examples.custom_cnn.CNN.__init__", "home.repos.pwc.inspect_result.facebookresearch_ppuda.ghn.layers.get_activation"], ["    ", "def", "__init__", "(", "self", ",", "\n", "in_features", "=", "32", ",", "\n", "hid", "=", "(", "64", ",", ")", ",", "\n", "out_shape", "=", "None", ",", "\n", "num_classes", "=", "None", ")", ":", "\n", "        ", "super", "(", "MLPDecoder", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "assert", "len", "(", "hid", ")", ">", "0", ",", "hid", "\n", "self", ".", "out_shape", "=", "out_shape", "\n", "self", ".", "num_classes", "=", "num_classes", "\n", "self", ".", "mlp", "=", "MLP", "(", "in_features", "=", "in_features", ",", "\n", "hid", "=", "(", "*", "hid", ",", "np", ".", "prod", "(", "out_shape", ")", ")", ",", "\n", "activation", "=", "'relu'", ",", "\n", "last_activation", "=", "None", ")", "\n", "self", ".", "class_layer_predictor", "=", "nn", ".", "Sequential", "(", "\n", "get_activation", "(", "'relu'", ")", ",", "\n", "nn", ".", "Linear", "(", "hid", "[", "0", "]", ",", "num_classes", "*", "out_shape", "[", "0", "]", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.ghn.decoder.MLPDecoder.forward": [[85, 95], ["decoder.MLPDecoder.class_layer_predictor", "decoder.MLPDecoder.view", "decoder.MLPDecoder.mlp().view", "sum", "list", "decoder.MLPDecoder.mlp", "decoder.MLPDecoder.mlp.fc.children"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ",", "max_shape", "=", "(", "1", ",", "1", ")", ",", "class_pred", "=", "False", ")", ":", "\n", "        ", "if", "class_pred", ":", "\n", "            ", "x", "=", "list", "(", "self", ".", "mlp", ".", "fc", ".", "children", "(", ")", ")", "[", "0", "]", "(", "x", ")", "# shared first layer", "\n", "x", "=", "self", ".", "class_layer_predictor", "(", "x", ")", "# N, 1000, 64, 1", "\n", "x", "=", "x", ".", "view", "(", "x", ".", "shape", "[", "0", "]", ",", "self", ".", "num_classes", ",", "self", ".", "out_shape", "[", "1", "]", ")", "\n", "", "else", ":", "\n", "            ", "x", "=", "self", ".", "mlp", "(", "x", ")", ".", "view", "(", "-", "1", ",", "*", "self", ".", "out_shape", ")", "\n", "if", "sum", "(", "max_shape", ")", ">", "0", ":", "\n", "                ", "x", "=", "x", "[", ":", ",", ":", ",", ":", ",", ":", "max_shape", "[", "0", "]", ",", ":", "max_shape", "[", "1", "]", "]", "\n", "", "", "return", "x", "\n", "", "", ""]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.ghn.layers.ShapeEncoder.__init__": [[42, 79], ["torch.Module.__init__", "numpy.unique", "numpy.unique", "copy.deepcopy", "range", "range", "copy.deepcopy", "range", "torch.nn.Embedding", "torch.nn.Embedding", "torch.nn.Embedding", "torch.nn.Embedding", "torch.nn.Embedding", "torch.nn.Embedding", "torch.nn.Embedding", "torch.nn.Embedding", "layers.ShapeEncoder.register_buffer", "len", "len", "torch.tensor().view", "torch.tensor().view", "torch.tensor().view", "torch.tensor().view", "list", "list", "enumerate", "enumerate", "list", "range", "range", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "list", "range", "max", "range", "numpy.argmin", "numpy.argmin", "abs", "abs"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_ppuda.examples.custom_cnn.CNN.__init__", "home.repos.pwc.inspect_result.facebookresearch_ppuda.detector.utils.SmoothedValue.max"], ["    ", "def", "__init__", "(", "self", ",", "hid", ",", "num_classes", ",", "max_shape", ",", "debug_level", "=", "0", ")", ":", "\n", "        ", "super", "(", "ShapeEncoder", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "assert", "max_shape", "[", "2", "]", "==", "max_shape", "[", "3", "]", ",", "max_shape", "\n", "self", ".", "debug_level", "=", "debug_level", "\n", "self", ".", "num_classes", "=", "num_classes", "\n", "self", ".", "ch_steps", "=", "(", "2", "**", "3", ",", "2", "**", "6", ",", "2", "**", "12", ",", "2", "**", "13", ")", "\n", "self", ".", "channels", "=", "np", ".", "unique", "(", "[", "1", ",", "3", ",", "num_classes", "]", "+", "\n", "list", "(", "range", "(", "self", ".", "ch_steps", "[", "0", "]", ",", "self", ".", "ch_steps", "[", "1", "]", ",", "2", "**", "3", ")", ")", "+", "\n", "list", "(", "range", "(", "self", ".", "ch_steps", "[", "1", "]", ",", "self", ".", "ch_steps", "[", "2", "]", ",", "2", "**", "4", ")", ")", "+", "\n", "list", "(", "range", "(", "self", ".", "ch_steps", "[", "2", "]", ",", "self", ".", "ch_steps", "[", "3", "]", "+", "1", ",", "2", "**", "5", ")", ")", ")", "\n", "\n", "self", ".", "spatial", "=", "np", ".", "unique", "(", "list", "(", "range", "(", "1", ",", "max", "(", "12", ",", "max_shape", "[", "3", "]", ")", ",", "2", ")", ")", "+", "[", "14", ",", "16", "]", ")", "\n", "\n", "# create a look up dictionary for faster determining the channel shape index", "\n", "# include shapes not seen during training by assigning them the the closest seen values", "\n", "self", ".", "channels_lookup", "=", "{", "c", ":", "i", "for", "i", ",", "c", "in", "enumerate", "(", "self", ".", "channels", ")", "}", "\n", "self", ".", "channels_lookup_training", "=", "copy", ".", "deepcopy", "(", "self", ".", "channels_lookup", ")", "\n", "for", "c", "in", "range", "(", "4", ",", "self", ".", "ch_steps", "[", "0", "]", ")", ":", "\n", "            ", "self", ".", "channels_lookup", "[", "c", "]", "=", "self", ".", "channels_lookup", "[", "self", ".", "ch_steps", "[", "0", "]", "]", "# 4-7 channels will be treated as 8 channels", "\n", "", "for", "c", "in", "range", "(", "1", ",", "self", ".", "channels", "[", "-", "1", "]", ")", ":", "\n", "            ", "if", "c", "not", "in", "self", ".", "channels_lookup", ":", "\n", "                ", "self", ".", "channels_lookup", "[", "c", "]", "=", "self", ".", "channels_lookup", "[", "self", ".", "channels", "[", "np", ".", "argmin", "(", "abs", "(", "self", ".", "channels", "-", "c", ")", ")", "]", "]", "\n", "\n", "", "", "self", ".", "spatial_lookup", "=", "{", "c", ":", "i", "for", "i", ",", "c", "in", "enumerate", "(", "self", ".", "spatial", ")", "}", "\n", "self", ".", "spatial_lookup_training", "=", "copy", ".", "deepcopy", "(", "self", ".", "spatial_lookup", ")", "\n", "self", ".", "spatial_lookup", "[", "2", "]", "=", "self", ".", "spatial_lookup", "[", "3", "]", "# 2x2 (not seen during training) will be treated as 3x3", "\n", "for", "c", "in", "range", "(", "1", ",", "self", ".", "spatial", "[", "-", "1", "]", ")", ":", "\n", "            ", "if", "c", "not", "in", "self", ".", "spatial_lookup", ":", "\n", "                ", "self", ".", "spatial_lookup", "[", "c", "]", "=", "self", ".", "spatial_lookup", "[", "self", ".", "spatial", "[", "np", ".", "argmin", "(", "abs", "(", "self", ".", "spatial", "-", "c", ")", ")", "]", "]", "\n", "\n", "", "", "n_ch", ",", "n_s", "=", "len", "(", "self", ".", "channels", ")", ",", "len", "(", "self", ".", "spatial", ")", "\n", "self", ".", "embed_spatial", "=", "torch", ".", "nn", ".", "Embedding", "(", "n_s", "+", "1", ",", "hid", "//", "4", ")", "\n", "self", ".", "embed_channel", "=", "torch", ".", "nn", ".", "Embedding", "(", "n_ch", "+", "1", ",", "hid", "//", "4", ")", "\n", "\n", "self", ".", "register_buffer", "(", "'dummy_ind'", ",", "torch", ".", "tensor", "(", "[", "n_ch", ",", "n_ch", ",", "n_s", ",", "n_s", "]", ",", "dtype", "=", "torch", ".", "long", ")", ".", "view", "(", "1", ",", "4", ")", ",", "\n", "persistent", "=", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.ghn.layers.ShapeEncoder.forward": [[81, 127], ["layers.ShapeEncoder.dummy_ind.repeat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "len", "range", "len", "len", "len", "layers.ShapeEncoder.embed_channel", "layers.ShapeEncoder.embed_channel", "layers.ShapeEncoder.embed_spatial", "layers.ShapeEncoder.embed_spatial", "print", "int", "int", "enumerate", "c.item", "c.item"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ",", "params_map", ",", "predict_class_layers", "=", "True", ")", ":", "\n", "        ", "shape_ind", "=", "self", ".", "dummy_ind", ".", "repeat", "(", "len", "(", "x", ")", ",", "1", ")", "\n", "\n", "self", ".", "printed_warning", "=", "False", "\n", "for", "node_ind", "in", "params_map", ":", "\n", "            ", "sz", "=", "params_map", "[", "node_ind", "]", "[", "0", "]", "[", "'sz'", "]", "\n", "if", "sz", "is", "None", ":", "\n", "                ", "continue", "\n", "\n", "", "sz_org", "=", "sz", "\n", "if", "len", "(", "sz", ")", "==", "1", ":", "\n", "                ", "sz", "=", "(", "sz", "[", "0", "]", ",", "1", ")", "\n", "", "if", "len", "(", "sz", ")", "==", "2", ":", "\n", "                ", "sz", "=", "(", "sz", "[", "0", "]", ",", "sz", "[", "1", "]", ",", "1", ",", "1", ")", "\n", "", "assert", "len", "(", "sz", ")", "==", "4", ",", "sz", "\n", "\n", "if", "not", "predict_class_layers", "and", "params_map", "[", "node_ind", "]", "[", "1", "]", "in", "[", "'cls_w'", ",", "'cls_b'", "]", ":", "\n", "# keep the classification shape as though the GHN is used on the dataset it was trained on", "\n", "                ", "sz", "=", "(", "self", ".", "num_classes", ",", "*", "sz", "[", "1", ":", "]", ")", "\n", "\n", "", "recognized_sz", "=", "0", "\n", "for", "i", "in", "range", "(", "4", ")", ":", "\n", "# if not in the dictionary, then use the maximum shape", "\n", "                ", "if", "i", "<", "2", ":", "# for out/in channel dimensions", "\n", "                    ", "shape_ind", "[", "node_ind", ",", "i", "]", "=", "self", ".", "channels_lookup", "[", "sz", "[", "i", "]", "if", "sz", "[", "i", "]", "in", "self", ".", "channels_lookup", "else", "self", ".", "channels", "[", "-", "1", "]", "]", "\n", "if", "self", ".", "debug_level", "and", "not", "self", ".", "printed_warning", ":", "\n", "                        ", "recognized_sz", "+=", "int", "(", "sz", "[", "i", "]", "in", "self", ".", "channels_lookup_training", ")", "\n", "", "", "else", ":", "# for kernel height/width", "\n", "                    ", "shape_ind", "[", "node_ind", ",", "i", "]", "=", "self", ".", "spatial_lookup", "[", "sz", "[", "i", "]", "if", "sz", "[", "i", "]", "in", "self", ".", "spatial_lookup", "else", "self", ".", "spatial", "[", "-", "1", "]", "]", "\n", "if", "self", ".", "debug_level", "and", "not", "self", ".", "printed_warning", ":", "\n", "                        ", "recognized_sz", "+=", "int", "(", "sz", "[", "i", "]", "in", "self", ".", "spatial_lookup_training", ")", "\n", "\n", "", "", "", "if", "self", ".", "debug_level", "and", "not", "self", ".", "printed_warning", ":", "# print a warning once per architecture", "\n", "                ", "if", "recognized_sz", "!=", "4", ":", "\n", "                    ", "print", "(", "'WARNING: unrecognized shape %s, so the closest shape at index %s will be used instead.'", "%", "(", "\n", "sz_org", ",", "(", "[", "self", ".", "channels", "[", "c", ".", "item", "(", ")", "]", "if", "i", "<", "2", "else", "self", ".", "spatial", "[", "c", ".", "item", "(", ")", "]", "for", "i", ",", "c", "in", "\n", "enumerate", "(", "shape_ind", "[", "node_ind", "]", ")", "]", ")", ")", ")", "\n", "self", ".", "printed_warning", "=", "True", "\n", "\n", "", "", "", "shape_embed", "=", "torch", ".", "cat", "(", "\n", "(", "self", ".", "embed_channel", "(", "shape_ind", "[", ":", ",", "0", "]", ")", ",", "\n", "self", ".", "embed_channel", "(", "shape_ind", "[", ":", ",", "1", "]", ")", ",", "\n", "self", ".", "embed_spatial", "(", "shape_ind", "[", ":", ",", "2", "]", ")", ",", "\n", "self", ".", "embed_spatial", "(", "shape_ind", "[", ":", ",", "3", "]", ")", ")", ",", "dim", "=", "1", ")", "\n", "\n", "return", "x", "+", "shape_embed", "\n", "", "", ""]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.ghn.layers.get_activation": [[19, 39], ["torch.Identity", "torch.ReLU", "torch.LeakyReLU", "torch.SELU", "torch.ELU", "torch.RReLU", "torch.Sigmoid", "NotImplementedError"], "function", ["None"], ["def", "get_activation", "(", "activation", ")", ":", "\n", "    ", "if", "activation", "is", "not", "None", ":", "\n", "        ", "if", "activation", "==", "'relu'", ":", "\n", "            ", "f", "=", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", "\n", "", "elif", "activation", "==", "'lrelu'", ":", "\n", "            ", "f", "=", "nn", ".", "LeakyReLU", "(", ")", "\n", "", "elif", "activation", "==", "'selu'", ":", "\n", "            ", "f", "=", "nn", ".", "SELU", "(", ")", "\n", "", "elif", "activation", "==", "'elu'", ":", "\n", "            ", "f", "=", "nn", ".", "ELU", "(", ")", "\n", "", "elif", "activation", "==", "'rrelu'", ":", "\n", "            ", "f", "=", "nn", ".", "RReLU", "(", ")", "\n", "", "elif", "activation", "==", "'sigmoid'", ":", "\n", "            ", "f", "=", "nn", ".", "Sigmoid", "(", ")", "\n", "", "else", ":", "\n", "            ", "raise", "NotImplementedError", "(", "activation", ")", "\n", "", "", "else", ":", "\n", "        ", "f", "=", "nn", ".", "Identity", "(", ")", "\n", "\n", "", "return", "f", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.ghn.mlp.MLP.__init__": [[13, 29], ["torch.Module.__init__", "enumerate", "torch.Sequential", "torch.Sequential", "len", "fc.extend", "torch.Linear", "torch.Linear", "layers.get_activation", "len"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_ppuda.examples.custom_cnn.CNN.__init__", "home.repos.pwc.inspect_result.facebookresearch_ppuda.ghn.layers.get_activation"], ["    ", "def", "__init__", "(", "self", ",", "\n", "in_features", "=", "32", ",", "\n", "hid", "=", "(", "32", ",", "32", ")", ",", "\n", "activation", "=", "'relu'", ",", "\n", "last_activation", "=", "'same'", ")", ":", "\n", "        ", "super", "(", "MLP", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "assert", "len", "(", "hid", ")", ">", "0", ",", "hid", "\n", "fc", "=", "[", "]", "\n", "for", "j", ",", "n", "in", "enumerate", "(", "hid", ")", ":", "\n", "            ", "fc", ".", "extend", "(", "[", "nn", ".", "Linear", "(", "in_features", "if", "j", "==", "0", "else", "hid", "[", "j", "-", "1", "]", ",", "n", ")", ",", "\n", "get_activation", "(", "last_activation", "if", "\n", "(", "j", "==", "len", "(", "hid", ")", "-", "1", "and", "\n", "last_activation", "!=", "'same'", ")", "\n", "else", "activation", ")", "]", ")", "\n", "", "self", ".", "fc", "=", "nn", ".", "Sequential", "(", "*", "fc", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.ghn.mlp.MLP.forward": [[31, 35], ["isinstance", "mlp.MLP.fc"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "if", "isinstance", "(", "x", ",", "tuple", ")", ":", "\n", "            ", "x", "=", "x", "[", "0", "]", "\n", "", "return", "self", ".", "fc", "(", "x", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.ghn.nn.GHN.__init__": [[80, 130], ["torch.Module.__init__", "torch.nn.Embedding", "torch.nn.Embedding", "torch.nn.Embedding", "torch.nn.Embedding", "layers.ShapeEncoder", "fn_dec", "max", "mlp.MLP", "torch.Sequential", "torch.Sequential", "len", "torch.LayerNorm", "torch.LayerNorm", "len", "gatedgnn.GatedGNN", "torch.ReLU", "torch.ReLU", "torch.Linear", "torch.Linear", "mlp.MLP", "NotImplementedError", "NotImplementedError"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_ppuda.examples.custom_cnn.CNN.__init__", "home.repos.pwc.inspect_result.facebookresearch_ppuda.detector.utils.SmoothedValue.max"], ["def", "__init__", "(", "self", ",", "\n", "max_shape", ",", "\n", "num_classes", ",", "\n", "hypernet", "=", "'gatedgnn'", ",", "\n", "decoder", "=", "'conv'", ",", "\n", "weight_norm", "=", "False", ",", "\n", "ve", "=", "False", ",", "\n", "layernorm", "=", "False", ",", "\n", "hid", "=", "32", ",", "\n", "debug_level", "=", "0", ")", ":", "\n", "        ", "super", "(", "GHN", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "assert", "len", "(", "max_shape", ")", "==", "4", ",", "max_shape", "\n", "self", ".", "layernorm", "=", "layernorm", "\n", "self", ".", "weight_norm", "=", "weight_norm", "\n", "self", ".", "ve", "=", "ve", "\n", "self", ".", "debug_level", "=", "debug_level", "\n", "self", ".", "num_classes", "=", "num_classes", "\n", "\n", "if", "layernorm", ":", "\n", "            ", "self", ".", "ln", "=", "nn", ".", "LayerNorm", "(", "hid", ")", "\n", "\n", "", "self", ".", "embed", "=", "torch", ".", "nn", ".", "Embedding", "(", "len", "(", "PRIMITIVES_DEEPNETS1M", ")", ",", "hid", ")", "\n", "self", ".", "shape_enc", "=", "ShapeEncoder", "(", "hid", "=", "hid", ",", "\n", "num_classes", "=", "num_classes", ",", "\n", "max_shape", "=", "max_shape", ",", "\n", "debug_level", "=", "debug_level", ")", "\n", "if", "hypernet", "==", "'gatedgnn'", ":", "\n", "            ", "self", ".", "gnn", "=", "GatedGNN", "(", "in_features", "=", "hid", ",", "ve", "=", "ve", ")", "\n", "", "elif", "hypernet", "==", "'mlp'", ":", "\n", "            ", "self", ".", "gnn", "=", "MLP", "(", "in_features", "=", "hid", ",", "hid", "=", "(", "hid", ",", "hid", ")", ")", "\n", "", "else", ":", "\n", "            ", "raise", "NotImplementedError", "(", "hypernet", ")", "\n", "\n", "", "if", "decoder", "==", "'conv'", ":", "\n", "            ", "fn_dec", ",", "layers", "=", "ConvDecoder", ",", "(", "hid", "*", "4", ",", "hid", "*", "8", ")", "\n", "", "elif", "decoder", "==", "'mlp'", ":", "\n", "            ", "fn_dec", ",", "layers", "=", "MLPDecoder", ",", "(", "hid", "*", "2", ",", ")", "\n", "", "else", ":", "\n", "            ", "raise", "NotImplementedError", "(", "decoder", ")", "\n", "", "self", ".", "decoder", "=", "fn_dec", "(", "in_features", "=", "hid", ",", "\n", "hid", "=", "layers", ",", "\n", "out_shape", "=", "max_shape", ",", "\n", "num_classes", "=", "num_classes", ")", "\n", "\n", "max_ch", "=", "max", "(", "max_shape", "[", ":", "2", "]", ")", "\n", "self", ".", "decoder_1d", "=", "MLP", "(", "hid", ",", "hid", "=", "(", "hid", "*", "2", ",", "2", "*", "max_ch", ")", ",", "\n", "last_activation", "=", "None", ")", "\n", "self", ".", "bias_class", "=", "nn", ".", "Sequential", "(", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Linear", "(", "max_ch", ",", "num_classes", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.ghn.nn.GHN.load": [[132, 140], ["utils.default_device", "torch.load", "torch.load", "torch.load", "torch.load", "GHN().to().eval", "GHN().to().eval.load_state_dict", "print", "GHN().to", "torch.GHN", "utils.capacity"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_ppuda.utils.utils.default_device", "home.repos.pwc.inspect_result.facebookresearch_ppuda.ghn.nn.GHN.load", "home.repos.pwc.inspect_result.facebookresearch_ppuda.ghn.nn.GHN.load", "home.repos.pwc.inspect_result.facebookresearch_ppuda.ghn.nn.GHN.load", "home.repos.pwc.inspect_result.facebookresearch_ppuda.ghn.nn.GHN.load", "home.repos.pwc.inspect_result.facebookresearch_ppuda.utils.utils.capacity"], ["", "@", "staticmethod", "\n", "def", "load", "(", "checkpoint_path", ",", "debug_level", "=", "1", ",", "device", "=", "default_device", "(", ")", ",", "verbose", "=", "False", ")", ":", "\n", "        ", "state_dict", "=", "torch", ".", "load", "(", "checkpoint_path", ",", "map_location", "=", "device", ")", "\n", "ghn", "=", "GHN", "(", "**", "state_dict", "[", "'config'", "]", ",", "debug_level", "=", "debug_level", ")", ".", "to", "(", "device", ")", ".", "eval", "(", ")", "\n", "ghn", ".", "load_state_dict", "(", "state_dict", "[", "'state_dict'", "]", ")", "\n", "if", "verbose", ":", "\n", "            ", "print", "(", "'GHN with {} parameters loaded from epoch {}.'", ".", "format", "(", "capacity", "(", "ghn", ")", "[", "1", "]", ",", "state_dict", "[", "'epoch'", "]", ")", ")", "\n", "", "return", "ghn", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.ghn.nn.GHN.forward": [[142, 283], ["torch.GHN._map_net_params", "torch.GHN.shape_enc", "torch.GHN.gnn", "param_groups.items", "params_map.values", "isinstance", "sum", "torch.GHN.embed", "torch.GHN.ln", "range", "nets_torch.apply", "print", "isinstance", "time.time", "deepnets1m.graph.GraphBatch", "deepnets1m.graph.GraphBatch.to_device", "print", "len", "torch.GHN.decoder", "key.startswith", "torch.GHN._set_params", "torch.prod", "torch.prod", "torch.prod", "torch.prod", "isinstance", "time.time", "print", "nets_torch.named_parameters", "len", "graphs[].num_valid_nodes", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "tuple", "torch.GHN.decoder", "torch.GHN.decoder_1d().view", "len", "torch.GHN._tile_params", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "str().upper", "print", "deepnets1m.graph.Graph", "utils.capacity", "map", "len", "torch.GHN.bias_class", "isinstance", "torch.GHN.decoder_1d", "len", "type", "str", "p.min().item", "p.max().item", "p.mean().item", "p.std().item", "torch.norm().item", "torch.norm().item", "torch.norm().item", "torch.norm().item", "key.split", "type", "str", "p.min", "p.max", "p.mean", "p.std", "torch.norm", "torch.norm", "torch.norm", "torch.norm"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_ppuda.ghn.nn.GHN._map_net_params", "home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.graph.GraphBatch.to_device", "home.repos.pwc.inspect_result.facebookresearch_ppuda.ghn.nn.GHN._set_params", "home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.graph.Graph.num_valid_nodes", "home.repos.pwc.inspect_result.facebookresearch_ppuda.ghn.nn.GHN._tile_params", "home.repos.pwc.inspect_result.facebookresearch_ppuda.utils.utils.capacity", "home.repos.pwc.inspect_result.facebookresearch_ppuda.detector.utils.SmoothedValue.max"], ["", "def", "forward", "(", "self", ",", "nets_torch", ",", "graphs", "=", "None", ",", "return_embeddings", "=", "False", ",", "predict_class_layers", "=", "True", ",", "bn_train", "=", "True", ")", ":", "\n", "        ", "r\"\"\"\n        Predict parameters for a list of >=1 networks.\n        :param nets_torch: one network or a list of networks, each is based on nn.Module.\n                           In case of evaluation, only one network can be passed.\n        :param graphs: GraphBatch object in case of training.\n                       For evaluation, graphs can be None and will be constructed on the fly given the nets_torch in this case.\n        :param return_embeddings: True to return the node embeddings obtained after the last graph propagation step.\n                                  return_embeddings=True is used for property prediction experiments.\n        :param predict_class_layers: default=True predicts all parameters including the classification layers.\n                                     predict_class_layers=False is used in fine-tuning experiments.\n        :param bn_train: default=True sets BN layers in nets_torch into the training mode (required to evaluate predicted parameters)\n                        bn_train=False is used in fine-tuning experiments\n        :return: nets_torch with predicted parameters and node embeddings if return_embeddings=True\n        \"\"\"", "\n", "\n", "if", "not", "self", ".", "training", ":", "\n", "            ", "assert", "isinstance", "(", "nets_torch", ",", "\n", "nn", ".", "Module", ")", "or", "len", "(", "nets_torch", ")", "==", "1", ",", "'constructing the graph on the fly is only supported for a single network'", "\n", "\n", "if", "isinstance", "(", "nets_torch", ",", "list", ")", ":", "\n", "                ", "nets_torch", "=", "nets_torch", "[", "0", "]", "\n", "\n", "", "if", "self", ".", "debug_level", ":", "\n", "                ", "if", "self", ".", "debug_level", ">", "1", ":", "\n", "                    ", "valid_ops", "=", "graphs", "[", "0", "]", ".", "num_valid_nodes", "(", "nets_torch", ")", "\n", "", "start_time", "=", "time", ".", "time", "(", ")", "# do not count any debugging steps above", "\n", "\n", "", "if", "graphs", "is", "None", ":", "\n", "                ", "graphs", "=", "GraphBatch", "(", "[", "Graph", "(", "nets_torch", ",", "ve_cutoff", "=", "50", "if", "self", ".", "ve", "else", "1", ")", "]", ")", "\n", "graphs", ".", "to_device", "(", "self", ".", "embed", ".", "weight", ".", "device", ")", "\n", "\n", "", "", "else", ":", "\n", "            ", "assert", "graphs", "is", "not", "None", ",", "'constructing the graph on the fly is only supported in the evaluation mode'", "\n", "\n", "# Find mapping between embeddings and network parameters", "\n", "", "param_groups", ",", "params_map", "=", "self", ".", "_map_net_params", "(", "graphs", ",", "nets_torch", ",", "self", ".", "debug_level", ">", "0", ")", "\n", "\n", "if", "self", ".", "debug_level", "or", "not", "self", ".", "training", ":", "\n", "            ", "n_params_true", "=", "sum", "(", "[", "capacity", "(", "net", ")", "[", "1", "]", "for", "net", "in", "(", "nets_torch", "if", "isinstance", "(", "nets_torch", ",", "list", ")", "else", "[", "nets_torch", "]", ")", "]", ")", "\n", "if", "self", ".", "debug_level", ">", "1", ":", "\n", "                ", "print", "(", "'\\nnumber of learnable parameter tensors: {}, total number of parameters: {}'", ".", "format", "(", "\n", "valid_ops", ",", "n_params_true", ")", ")", "\n", "\n", "# Obtain initial embeddings for all nodes", "\n", "", "", "x", "=", "self", ".", "shape_enc", "(", "self", ".", "embed", "(", "graphs", ".", "node_feat", "[", ":", ",", "0", "]", ")", ",", "params_map", ",", "predict_class_layers", "=", "predict_class_layers", ")", "\n", "\n", "# Update node embeddings using a GatedGNN, MLP or another model", "\n", "x", "=", "self", ".", "gnn", "(", "x", ",", "graphs", ".", "edges", ",", "graphs", ".", "node_feat", "[", ":", ",", "1", "]", ")", "\n", "\n", "if", "self", ".", "layernorm", ":", "\n", "            ", "x", "=", "self", ".", "ln", "(", "x", ")", "\n", "\n", "# Predict max-sized parameters for a batch of nets using decoders", "\n", "", "w", "=", "{", "}", "\n", "for", "key", ",", "inds", "in", "param_groups", ".", "items", "(", ")", ":", "\n", "            ", "if", "len", "(", "inds", ")", "==", "0", ":", "\n", "                ", "continue", "\n", "", "x_", "=", "x", "[", "torch", ".", "tensor", "(", "inds", ",", "device", "=", "x", ".", "device", ")", "]", "\n", "if", "key", "==", "'cls_w'", ":", "\n", "                ", "w", "[", "key", "]", "=", "self", ".", "decoder", "(", "x_", ",", "(", "1", ",", "1", ")", ",", "class_pred", "=", "True", ")", "\n", "", "elif", "key", ".", "startswith", "(", "'4d'", ")", ":", "\n", "                ", "sz", "=", "tuple", "(", "map", "(", "int", ",", "key", ".", "split", "(", "'-'", ")", "[", "1", ":", "]", ")", ")", "\n", "w", "[", "key", "]", "=", "self", ".", "decoder", "(", "x_", ",", "sz", ",", "class_pred", "=", "False", ")", "\n", "", "else", ":", "\n", "                ", "w", "[", "key", "]", "=", "self", ".", "decoder_1d", "(", "x_", ")", ".", "view", "(", "len", "(", "inds", ")", ",", "2", ",", "-", "1", ")", "#.clone()", "\n", "if", "key", "==", "'cls_b'", ":", "\n", "                    ", "w", "[", "key", "]", "=", "self", ".", "bias_class", "(", "w", "[", "key", "]", ")", "\n", "\n", "# Transfer predicted parameters (w) to the networks", "\n", "", "", "", "n_tensors", ",", "n_params", "=", "0", ",", "0", "\n", "for", "matched", ",", "key", ",", "w_ind", "in", "params_map", ".", "values", "(", ")", ":", "\n", "\n", "            ", "if", "w_ind", "is", "None", ":", "\n", "                ", "continue", "# e.g. pooling", "\n", "\n", "", "if", "not", "predict_class_layers", "and", "key", "in", "[", "'cls_w'", ",", "'cls_b'", "]", ":", "\n", "                ", "continue", "# do not set the classification parameters when fine-tuning", "\n", "\n", "", "m", ",", "sz", ",", "is_w", "=", "matched", "[", "'module'", "]", ",", "matched", "[", "'sz'", "]", ",", "matched", "[", "'is_w'", "]", "\n", "for", "it", "in", "range", "(", "2", "if", "(", "len", "(", "sz", ")", "==", "1", "and", "is_w", ")", "else", "1", ")", ":", "\n", "\n", "                ", "if", "len", "(", "sz", ")", "==", "1", ":", "\n", "# separately set for BN/LN biases as they are", "\n", "# not represented as separate nodes in graphs", "\n", "                    ", "w_", "=", "w", "[", "key", "]", "[", "w_ind", "]", "[", "1", "-", "is_w", "+", "it", "]", "\n", "if", "it", "==", "1", ":", "\n", "                        ", "assert", "(", "type", "(", "m", ")", "in", "NormLayers", "and", "key", "==", "'1d'", ")", ",", "(", "type", "(", "m", ")", ",", "key", ")", "\n", "", "", "else", ":", "\n", "                    ", "w_", "=", "w", "[", "key", "]", "[", "w_ind", "]", "\n", "\n", "", "sz_set", "=", "self", ".", "_set_params", "(", "m", ",", "self", ".", "_tile_params", "(", "w_", ",", "sz", ")", ",", "is_w", "=", "is_w", "&", "~", "it", ")", "\n", "n_tensors", "+=", "1", "\n", "n_params", "+=", "torch", ".", "prod", "(", "torch", ".", "tensor", "(", "sz_set", ")", ")", "\n", "\n", "", "", "if", "not", "self", ".", "training", "and", "bn_train", ":", "\n", "\n", "            ", "def", "bn_set_train", "(", "module", ")", ":", "\n", "                ", "if", "isinstance", "(", "module", ",", "nn", ".", "BatchNorm2d", ")", ":", "\n", "                    ", "module", ".", "track_running_stats", "=", "False", "\n", "module", ".", "training", "=", "True", "\n", "\n", "", "", "nets_torch", ".", "apply", "(", "bn_set_train", ")", "# set BN layers to the training mode to enable evaluation without having running statistics", "\n", "\n", "", "if", "self", ".", "debug_level", "and", "not", "self", ".", "training", ":", "\n", "\n", "            ", "end_time", "=", "time", ".", "time", "(", ")", "-", "start_time", "\n", "\n", "print", "(", "'number of parameter tensors predicted using GHN: {}, '", "\n", "'total parameters predicted: {} ({}), time to predict (on {}): {:.4f} sec'", ".", "format", "(", "\n", "n_tensors", ",", "\n", "n_params", ",", "\n", "(", "'matched!'", "if", "n_params_true", "==", "n_params", "else", "'error! not matched'", ")", ".", "upper", "(", ")", ",", "\n", "str", "(", "x", ".", "device", ")", ".", "upper", "(", ")", ",", "\n", "end_time", ")", ")", "\n", "\n", "if", "self", ".", "debug_level", ">", "1", ":", "\n", "                ", "assert", "valid_ops", "==", "n_tensors", ",", "(", "\n", "'number of learnable tensors ({}) must be the same as the number of predicted tensors ({})'", ".", "format", "(", "\n", "valid_ops", ",", "n_tensors", ")", ")", "\n", "\n", "\n", "", "if", "self", ".", "debug_level", ">", "2", ":", "\n", "                ", "print", "(", "'predicted parameter stats:'", ")", "\n", "for", "n", ",", "p", "in", "nets_torch", ".", "named_parameters", "(", ")", ":", "\n", "                    ", "print", "(", "'{:30s} ({:30s}): min={:.3f} \\t max={:.3f} \\t mean={:.3f} \\t std={:.3f} \\t norm={:.3f}'", ".", "format", "(", "\n", "n", "[", ":", "30", "]", ",", "\n", "str", "(", "p", ".", "shape", ")", "[", ":", "30", "]", ",", "\n", "p", ".", "min", "(", ")", ".", "item", "(", ")", ",", "\n", "p", ".", "max", "(", ")", ".", "item", "(", ")", ",", "\n", "p", ".", "mean", "(", ")", ".", "item", "(", ")", ",", "\n", "p", ".", "std", "(", ")", ".", "item", "(", ")", ",", "\n", "torch", ".", "norm", "(", "p", ")", ".", "item", "(", ")", ")", ")", "\n", "", "", "", "elif", "self", ".", "debug_level", "or", "not", "self", ".", "training", ":", "\n", "            ", "assert", "n_params", "==", "n_params_true", ",", "(", "'number of predicted ({}) or actual ({}) parameters must match'", ".", "format", "(", "\n", "n_params", ",", "n_params_true", ")", ")", "\n", "\n", "", "return", "(", "nets_torch", ",", "x", ")", "if", "return_embeddings", "else", "nets_torch", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.ghn.nn.GHN._map_net_params": [[285, 358], ["enumerate", "zip", "deepnets1m.net.named_layered_modules", "torch.sum().item", "torch.sum().item", "torch.sum().item", "torch.sum().item", "range", "type", "len", "set", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "len", "len", "len", "len", "m[].startswith", "len", "ValueError", "set", "set", "matched.append", "len", "set.append", "mapping[].append", "hasattr", "len", "len", "name.find", "len"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.net.named_layered_modules", "home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.graph.GraphBatch.append", "home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.graph.GraphBatch.append", "home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.graph.GraphBatch.append"], ["", "def", "_map_net_params", "(", "self", ",", "graphs", ",", "nets_torch", ",", "sanity_check", "=", "False", ")", ":", "\n", "        ", "r\"\"\"\n        Matches the parameters in the models with the nodes in the graph.\n        Performs additional steps.\n        :param graphs: GraphBatch object\n        :param nets_torch: a single neural network of a list\n        :param sanity_check:\n        :return: mapping, params_map\n        \"\"\"", "\n", "mapping", "=", "{", "}", "\n", "params_map", "=", "{", "}", "\n", "\n", "nets_torch", "=", "[", "nets_torch", "]", "if", "type", "(", "nets_torch", ")", "not", "in", "[", "tuple", ",", "list", "]", "else", "nets_torch", "\n", "\n", "for", "b", ",", "(", "node_info", ",", "net", ")", "in", "enumerate", "(", "zip", "(", "graphs", ".", "node_info", ",", "nets_torch", ")", ")", ":", "\n", "            ", "target_modules", "=", "named_layered_modules", "(", "net", ")", "\n", "\n", "param_ind", "=", "torch", ".", "sum", "(", "graphs", ".", "n_nodes", "[", ":", "b", "]", ")", ".", "item", "(", ")", "\n", "\n", "for", "cell_id", "in", "range", "(", "len", "(", "node_info", ")", ")", ":", "\n", "                ", "matched_names", "=", "[", "]", "\n", "for", "(", "node_ind", ",", "param_name", ",", "name", ",", "sz", ",", "last_weight", ",", "last_bias", ")", "in", "node_info", "[", "cell_id", "]", ":", "\n", "\n", "                    ", "matched", "=", "[", "]", "\n", "for", "m", "in", "target_modules", "[", "cell_id", "]", ":", "\n", "                        ", "if", "m", "[", "'param_name'", "]", ".", "startswith", "(", "param_name", ")", ":", "\n", "                            ", "matched", ".", "append", "(", "m", ")", "\n", "if", "not", "sanity_check", ":", "\n", "                                ", "break", "\n", "", "", "", "if", "len", "(", "matched", ")", ">", "1", ":", "\n", "                        ", "raise", "ValueError", "(", "cell_id", ",", "node_ind", ",", "param_name", ",", "name", ",", "[", "\n", "(", "t", ",", "(", "m", ".", "weight", "if", "is_w", "else", "m", ".", "bias", ")", ".", "shape", ")", "for", "\n", "t", ",", "m", ",", "is_w", "in", "matched", "]", ")", "\n", "", "elif", "len", "(", "matched", ")", "==", "0", ":", "\n", "                        ", "if", "sz", "is", "not", "None", ":", "\n", "                            ", "params_map", "[", "param_ind", "+", "node_ind", "]", "=", "(", "{", "'sz'", ":", "sz", "}", ",", "None", ",", "None", ")", "\n", "\n", "", "if", "sanity_check", ":", "\n", "                            ", "for", "pattern", "in", "[", "'input'", ",", "'sum'", ",", "'concat'", ",", "'pool'", ",", "'glob_avg'", ",", "'msa'", ",", "'cse'", "]", ":", "\n", "                                ", "good", "=", "name", ".", "find", "(", "pattern", ")", ">=", "0", "\n", "if", "good", ":", "\n", "                                    ", "break", "\n", "", "", "assert", "good", ",", "(", "cell_id", ",", "param_name", ",", "name", ",", "\n", "node_info", "[", "cell_id", "]", ",", "\n", "target_modules", "[", "cell_id", "]", ")", "\n", "", "", "else", ":", "\n", "                        ", "matched_names", ".", "append", "(", "matched", "[", "0", "]", "[", "'param_name'", "]", ")", "\n", "sz", "=", "matched", "[", "0", "]", "[", "'sz'", "]", "\n", "if", "len", "(", "sz", ")", "==", "1", ":", "\n", "                            ", "key", "=", "'cls_b'", "if", "last_bias", "else", "'1d'", "\n", "", "elif", "last_weight", ":", "\n", "                            ", "key", "=", "'cls_w'", "\n", "", "else", ":", "\n", "                            ", "key", "=", "'4d-%d-%d'", "%", "(", "(", "1", ",", "1", ")", "if", "len", "(", "sz", ")", "==", "2", "else", "sz", "[", "2", ":", "]", ")", "\n", "", "if", "key", "not", "in", "mapping", ":", "\n", "                            ", "mapping", "[", "key", "]", "=", "[", "]", "\n", "", "params_map", "[", "param_ind", "+", "node_ind", "]", "=", "(", "matched", "[", "0", "]", ",", "key", ",", "len", "(", "mapping", "[", "key", "]", ")", ")", "\n", "mapping", "[", "key", "]", ".", "append", "(", "param_ind", "+", "node_ind", ")", "\n", "\n", "", "", "assert", "len", "(", "matched_names", ")", "==", "len", "(", "set", "(", "matched_names", ")", ")", ",", "(", "\n", "'all matched names must be unique to avoid predicting the same paramters for different moduels'", ",", "\n", "len", "(", "matched_names", ")", ",", "len", "(", "set", "(", "matched_names", ")", ")", ")", "\n", "matched_names", "=", "set", "(", "matched_names", ")", "\n", "\n", "# Prune redundant ops in Network by setting their params to None", "\n", "for", "m", "in", "target_modules", "[", "cell_id", "]", ":", "\n", "                    ", "if", "m", "[", "'is_w'", "]", "and", "m", "[", "'param_name'", "]", "not", "in", "matched_names", ":", "\n", "                        ", "m", "[", "'module'", "]", ".", "weight", "=", "None", "\n", "if", "hasattr", "(", "m", "[", "'module'", "]", ",", "'bias'", ")", "and", "m", "[", "'module'", "]", ".", "bias", "is", "not", "None", ":", "\n", "                            ", "m", "[", "'module'", "]", ".", "bias", "=", "None", "\n", "\n", "", "", "", "", "", "return", "mapping", ",", "params_map", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.ghn.nn.GHN._tile_params": [[360, 412], ["len", "len", "len", "len", "len", "len", "len", "len", "len", "len", "len", "w.repeat", "len", "len", "w.repeat", "w.repeat", "w.repeat", "w.repeat", "min", "min", "min", "min", "min", "min", "min", "min"], "methods", ["None"], ["", "def", "_tile_params", "(", "self", ",", "w", ",", "target_shape", ")", ":", "\n", "        ", "r\"\"\"\n        Makes the shape of predicted parameter tensors the same as the target shape by tiling/slicing across channels dimensions.\n        :param w: predicted tensor, for example of shape (64, 64, 11, 11)\n        :param target_shape: tuple, for example (512, 256, 3, 3)\n        :return: tensor of shape target_shape\n        \"\"\"", "\n", "t", ",", "s", "=", "target_shape", ",", "w", ".", "shape", "\n", "\n", "# Slice first to avoid tiling a larger tensor", "\n", "if", "len", "(", "t", ")", "==", "1", ":", "\n", "            ", "if", "len", "(", "s", ")", "==", "2", ":", "\n", "                ", "w", "=", "w", "[", ":", "min", "(", "t", "[", "0", "]", ",", "s", "[", "0", "]", ")", ",", "0", "]", "\n", "", "elif", "len", "(", "s", ")", ">", "2", ":", "\n", "                ", "w", "=", "w", "[", ":", "min", "(", "t", "[", "0", "]", ",", "s", "[", "0", "]", ")", ",", "0", ",", "0", ",", "0", "]", "\n", "", "", "elif", "len", "(", "t", ")", "==", "2", ":", "\n", "            ", "if", "len", "(", "s", ")", ">", "2", ":", "\n", "                ", "w", "=", "w", "[", ":", "min", "(", "t", "[", "0", "]", ",", "s", "[", "0", "]", ")", ",", ":", "min", "(", "t", "[", "1", "]", ",", "s", "[", "1", "]", ")", ",", "0", ",", "0", "]", "\n", "", "", "else", ":", "\n", "            ", "w", "=", "w", "[", ":", "min", "(", "t", "[", "0", "]", ",", "s", "[", "0", "]", ")", ",", ":", "min", "(", "t", "[", "1", "]", ",", "s", "[", "1", "]", ")", ",", ":", "min", "(", "t", "[", "2", "]", ",", "s", "[", "2", "]", ")", ",", ":", "min", "(", "t", "[", "3", "]", ",", "s", "[", "3", "]", ")", "]", "\n", "\n", "", "s", "=", "w", ".", "shape", "\n", "assert", "len", "(", "s", ")", "==", "len", "(", "t", ")", ",", "(", "s", ",", "t", ")", "\n", "\n", "# Tile out_channels", "\n", "if", "t", "[", "0", "]", ">", "s", "[", "0", "]", ":", "\n", "            ", "n_out", "=", "t", "[", "0", "]", "//", "s", "[", "0", "]", "+", "1", "\n", "if", "len", "(", "t", ")", "==", "1", ":", "\n", "                ", "w", "=", "w", ".", "repeat", "(", "n_out", ")", "[", ":", "t", "[", "0", "]", "]", "\n", "", "elif", "len", "(", "t", ")", "==", "2", ":", "\n", "                ", "w", "=", "w", ".", "repeat", "(", "(", "n_out", ",", "1", ")", ")", "[", ":", "t", "[", "0", "]", "]", "\n", "", "else", ":", "\n", "                ", "w", "=", "w", ".", "repeat", "(", "(", "n_out", ",", "1", ",", "1", ",", "1", ")", ")", "[", ":", "t", "[", "0", "]", "]", "\n", "\n", "# Tile in_channels", "\n", "", "", "if", "len", "(", "t", ")", ">", "1", ":", "\n", "            ", "if", "t", "[", "1", "]", ">", "s", "[", "1", "]", ":", "\n", "                ", "n_in", "=", "t", "[", "1", "]", "//", "s", "[", "1", "]", "+", "1", "\n", "if", "len", "(", "t", ")", "==", "2", ":", "\n", "                    ", "w", "=", "w", ".", "repeat", "(", "(", "1", ",", "n_in", ")", ")", "[", ":", ",", ":", "t", "[", "1", "]", "]", "\n", "", "else", ":", "\n", "                    ", "w", "=", "w", ".", "repeat", "(", "(", "1", ",", "n_in", ",", "1", ",", "1", ")", ")", "[", ":", ",", ":", "t", "[", "1", "]", "]", "\n", "\n", "# Chop out any extra bits tiled", "\n", "", "", "", "if", "len", "(", "t", ")", "==", "1", ":", "\n", "            ", "w", "=", "w", "[", ":", "t", "[", "0", "]", "]", "\n", "", "elif", "len", "(", "t", ")", "==", "2", ":", "\n", "            ", "w", "=", "w", "[", ":", "t", "[", "0", "]", ",", ":", "t", "[", "1", "]", "]", "\n", "", "else", ":", "\n", "            ", "w", "=", "w", "[", ":", "t", "[", "0", "]", ",", ":", "t", "[", "1", "]", ",", ":", "t", "[", "2", "]", ",", ":", "t", "[", "3", "]", "]", "\n", "\n", "", "return", "w", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.ghn.nn.GHN._set_params": [[414, 440], ["torch.GHN._normalize", "isinstance", "type", "torch.GHN.clone"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_ppuda.ghn.nn.GHN._normalize"], ["", "def", "_set_params", "(", "self", ",", "module", ",", "tensor", ",", "is_w", ")", ":", "\n", "        ", "r\"\"\"\n        Copies the predicted parameter tensor to the appropriate field of the module object.\n        :param module: nn.Module\n        :param tensor: predicted tensor\n        :param is_w: True if it is a weight, False if it is a bias\n        :return: the shape of the copied tensor\n        \"\"\"", "\n", "if", "self", ".", "weight_norm", ":", "\n", "            ", "tensor", "=", "self", ".", "_normalize", "(", "module", ",", "tensor", ",", "is_w", ")", "\n", "", "key", "=", "'weight'", "if", "is_w", "else", "'bias'", "\n", "target_param", "=", "module", ".", "weight", "if", "is_w", "else", "module", ".", "bias", "\n", "sz_target", "=", "target_param", ".", "shape", "\n", "if", "self", ".", "training", ":", "\n", "            ", "module", ".", "__dict__", "[", "key", "]", "=", "tensor", "# set the value avoiding the internal logic of PyTorch", "\n", "# update parameters, so that named_parameters() will return tensors", "\n", "# with gradients (for multigpu and other cases)", "\n", "module", ".", "_parameters", "[", "key", "]", "=", "tensor", "\n", "", "else", ":", "\n", "            ", "assert", "isinstance", "(", "target_param", ",", "nn", ".", "Parameter", ")", ",", "type", "(", "target_param", ")", "\n", "# copy to make sure there is no sharing of memory", "\n", "target_param", ".", "data", "=", "tensor", ".", "clone", "(", ")", "\n", "\n", "", "set_param", "=", "module", ".", "weight", "if", "is_w", "else", "module", ".", "bias", "\n", "assert", "sz_target", "==", "set_param", ".", "shape", ",", "(", "sz_target", ",", "set_param", ".", "shape", ")", "\n", "return", "set_param", ".", "shape", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.ghn.nn.GHN._normalize": [[442, 480], ["torch.tanh.dim", "torch.tanh.dim", "isinstance", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "len", "len", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "p[].numel"], "methods", ["None"], ["", "def", "_normalize", "(", "self", ",", "module", ",", "p", ",", "is_w", ")", ":", "\n", "        ", "r\"\"\"\n        Normalizes the predicted parameter tensor according to the Fan-In scheme described in the paper.\n        :param module: nn.Module\n        :param p: predicted tensor\n        :param is_w: True if it is a weight, False if it is a bias\n        :return: normalized predicted tensor\n        \"\"\"", "\n", "if", "p", ".", "dim", "(", ")", ">", "1", ":", "\n", "\n", "            ", "sz", "=", "p", ".", "shape", "\n", "\n", "if", "len", "(", "sz", ")", ">", "2", "and", "sz", "[", "2", "]", ">=", "11", "and", "sz", "[", "0", "]", "==", "1", ":", "\n", "                ", "assert", "isinstance", "(", "module", ",", "PosEnc", ")", ",", "(", "sz", ",", "module", ")", "\n", "return", "p", "# do not normalize positional encoding weights", "\n", "\n", "", "no_relu", "=", "len", "(", "sz", ")", ">", "2", "and", "(", "sz", "[", "1", "]", "==", "1", "or", "sz", "[", "2", "]", "<", "sz", "[", "3", "]", ")", "\n", "if", "no_relu", ":", "\n", "# layers not followed by relu", "\n", "                ", "beta", "=", "1.", "\n", "", "else", ":", "\n", "# for layers followed by rely increase the weight scale", "\n", "                ", "beta", "=", "2.", "\n", "\n", "# fan-out:", "\n", "# p = p * (beta / (sz[0] * p[0, 0].numel())) ** 0.5", "\n", "\n", "# fan-in:", "\n", "", "p", "=", "p", "*", "(", "beta", "/", "p", "[", "0", "]", ".", "numel", "(", ")", ")", "**", "0.5", "\n", "\n", "", "else", ":", "\n", "\n", "            ", "if", "is_w", ":", "\n", "                ", "p", "=", "2", "*", "torch", ".", "sigmoid", "(", "0.5", "*", "p", ")", "# BN/LN norm weight is [0,2]", "\n", "", "else", ":", "\n", "                ", "p", "=", "torch", ".", "tanh", "(", "0.2", "*", "p", ")", "# bias is [-1,1]", "\n", "\n", "", "", "return", "p", "\n", "", "", ""]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.ghn.nn.GHN1": [[28, 37], ["os.path.dirname", "nn.GHN.load", "os.path.abspath", "os.path.join"], "function", ["home.repos.pwc.inspect_result.facebookresearch_ppuda.ghn.nn.GHN.load"], ["def", "GHN1", "(", "dataset", "=", "'imagenet'", ")", ":", "\n", "    ", "\"\"\"\n        Loads GHN-1 trained on ImageNet or CIFAR-10.\n        To load a GHN from an arbitrary checkpoint, use GHN.load(checkpoint_path).\n        :param dataset: imagenet or cifar10\n        :return: GHN-1 with trained weights\n        \"\"\"", "\n", "path", "=", "os", ".", "path", ".", "dirname", "(", "os", ".", "path", ".", "abspath", "(", "__file__", ")", ")", "\n", "return", "GHN", ".", "load", "(", "os", ".", "path", ".", "join", "(", "path", ",", "'../../checkpoints/ghn1_%s.pt'", "%", "dataset", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.ghn.nn.GHN2": [[39, 48], ["os.path.dirname", "nn.GHN.load", "os.path.abspath", "os.path.join"], "function", ["home.repos.pwc.inspect_result.facebookresearch_ppuda.ghn.nn.GHN.load"], ["", "def", "GHN2", "(", "dataset", "=", "'imagenet'", ")", ":", "\n", "    ", "\"\"\"\n    Loads GHN-2 trained on ImageNet or CIFAR-10.\n    To load a GHN from an arbitrary checkpoint, use GHN.load(checkpoint_path).\n    :param dataset: imagenet or cifar10\n    :return: GHN-2 with trained weights\n    \"\"\"", "\n", "path", "=", "os", ".", "path", ".", "dirname", "(", "os", ".", "path", ".", "abspath", "(", "__file__", ")", ")", "\n", "return", "GHN", ".", "load", "(", "os", ".", "path", ".", "join", "(", "path", ",", "'../../checkpoints/ghn2_%s.pt'", "%", "dataset", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.ghn.nn.ghn_parallel": [[50, 72], ["isinstance", "torch.nn.DataParallel", "torch.nn.DataParallel", "graphs.scatter"], "function", ["home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.graph.GraphBatch.scatter"], ["", "def", "ghn_parallel", "(", "ghn", ")", ":", "\n", "    ", "\"\"\"\n    For training a GHN on multiple GPUs.\n    :param ghn: GHN instance\n    :return: DataParallel wrapper of GHN\n    \"\"\"", "\n", "if", "isinstance", "(", "ghn", ",", "torch", ".", "nn", ".", "DataParallel", ")", ":", "\n", "        ", "return", "ghn", "\n", "\n", "", "ghn", "=", "torch", ".", "nn", ".", "DataParallel", "(", "ghn", ")", "\n", "\n", "def", "scatter", "(", "inputs", ",", "kwargs", ",", "device_ids", ")", ":", "\n", "        ", "nets_torch", ",", "graphs", "=", "inputs", "\n", "return", "graphs", ".", "scatter", "(", "device_ids", ",", "nets_torch", ")", ",", "None", "\n", "\n", "", "def", "gather", "(", "outputs", ",", "output_device", ")", ":", "\n", "        ", "return", "outputs", "# nets_torch with predicted parameters on multiple devices", "\n", "\n", "", "ghn", ".", "scatter", "=", "scatter", "\n", "ghn", ".", "gather", "=", "gather", "\n", "\n", "return", "ghn", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.genotypes.from_dict": [[52, 57], ["Genotype"], "function", ["None"], ["def", "from_dict", "(", "genotype", ")", ":", "\n", "    ", "return", "Genotype", "(", "normal", "=", "genotype", "[", "'normal'", "]", ",", "\n", "normal_concat", "=", "genotype", "[", "'normal_concat'", "]", ",", "\n", "reduce", "=", "genotype", "[", "'reduce'", "]", ",", "\n", "reduce_concat", "=", "genotype", "[", "'reduce_concat'", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.genotypes.to_dict": [[59, 64], ["list", "list", "list", "list"], "function", ["None"], ["", "def", "to_dict", "(", "genotype", ")", ":", "\n", "    ", "return", "{", "'normal'", ":", "list", "(", "genotype", ".", "normal", ")", ",", "\n", "'normal_concat'", ":", "list", "(", "genotype", ".", "normal_concat", ")", ",", "\n", "'reduce'", ":", "list", "(", "genotype", ".", "reduce", ")", ",", "\n", "'reduce_concat'", ":", "list", "(", "genotype", ".", "reduce_concat", ")", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.genotypes.sample_genotype": [[67, 143], ["sum", "len", "torch.autograd.Variable", "torch.autograd.Variable", "genotypes.sample_genotype._parse"], "function", ["None"], ["", "def", "sample_genotype", "(", "steps", "=", "1", ",", "only_pool", "=", "False", ",", "allow_none", "=", "True", ",", "drop_concat", "=", "True", ",", "allow_transformer", "=", "False", ")", ":", "\n", "\n", "# Extended set of primitives based on https://github.com/quark0/darts/blob/master/cnn/genotypes.py", "\n", "    ", "PRIMITIVES_DARTS_EXT", "=", "[", "\n", "'none'", ",", "\n", "'max_pool_3x3'", ",", "\n", "'avg_pool_3x3'", ",", "\n", "'skip_connect'", ",", "\n", "'sep_conv_3x3'", ",", "\n", "'sep_conv_5x5'", ",", "\n", "'dil_conv_3x3'", ",", "\n", "'dil_conv_5x5'", ",", "\n", "'conv_1x1'", ",", "\n", "'conv_7x1_1x7'", ",", "\n", "'conv_3x3'", ",", "\n", "'conv_5x5'", ",", "\n", "'conv_7x7'", ",", "\n", "'msa'", ",", "\n", "'cse'", "\n", "]", "\n", "\n", "multiplier", "=", "steps", "\n", "k", "=", "sum", "(", "1", "for", "i", "in", "range", "(", "steps", ")", "for", "n", "in", "range", "(", "2", "+", "i", ")", ")", "\n", "num_ops", "=", "len", "(", "PRIMITIVES_DARTS_EXT", ")", "\n", "alphas_normal", "=", "Variable", "(", "1e-3", "*", "torch", ".", "randn", "(", "k", ",", "num_ops", ")", ")", "\n", "alphas_reduce", "=", "Variable", "(", "1e-3", "*", "torch", ".", "randn", "(", "k", ",", "num_ops", ")", ")", "\n", "\n", "if", "only_pool", ":", "\n", "        ", "assert", "PRIMITIVES_DARTS_EXT", "[", "3", "]", "==", "'skip_connect'", ",", "PRIMITIVES_DARTS_EXT", "\n", "assert", "PRIMITIVES_DARTS_EXT", "[", "4", "]", "==", "'sep_conv_3x3'", ",", "PRIMITIVES_DARTS_EXT", "\n", "alphas_reduce", "[", ":", ",", "4", ":", "]", "=", "-", "1000", "# prevent sampling operators with learnable params to sample the architectures similar to the best DARTS cell", "\n", "\n", "", "if", "not", "allow_transformer", ":", "\n", "        ", "ind", "=", "PRIMITIVES_DARTS_EXT", ".", "index", "(", "'msa'", ")", "\n", "assert", "ind", "==", "len", "(", "PRIMITIVES_DARTS_EXT", ")", "-", "2", ",", "(", "ind", ",", "PRIMITIVES_DARTS_EXT", ")", "\n", "alphas_normal", "[", ":", ",", "ind", "]", "=", "-", "1000", "\n", "alphas_reduce", "[", ":", ",", "ind", "]", "=", "-", "1000", "\n", "\n", "", "def", "_parse", "(", "weights", ")", ":", "\n", "# Based on https://github.com/quark0/darts/blob/master/cnn/model_search.py#L135", "\n", "        ", "gene", "=", "[", "]", "\n", "n", "=", "2", "\n", "start", "=", "0", "\n", "for", "i", "in", "range", "(", "steps", ")", ":", "\n", "            ", "end", "=", "start", "+", "n", "\n", "W", "=", "weights", "[", "start", ":", "end", "]", ".", "copy", "(", ")", "\n", "edges", "=", "sorted", "(", "range", "(", "i", "+", "2", ")", ",", "\n", "key", "=", "lambda", "x", ":", "-", "max", "(", "W", "[", "x", "]", "[", "k", "]", "for", "k", "in", "range", "(", "len", "(", "W", "[", "x", "]", ")", ")", "if", "(", "k", "!=", "PRIMITIVES_DARTS_EXT", ".", "index", "(", "'none'", ")", "or", "allow_none", ")", ")", ")", "[", ":", "2", "]", "\n", "for", "j", "in", "edges", ":", "\n", "                ", "k_best", "=", "None", "\n", "for", "k", "in", "range", "(", "len", "(", "W", "[", "j", "]", ")", ")", ":", "\n", "                    ", "if", "k", "!=", "PRIMITIVES_DARTS_EXT", ".", "index", "(", "'none'", ")", "or", "allow_none", ":", "\n", "                        ", "if", "k_best", "is", "None", "or", "W", "[", "j", "]", "[", "k", "]", ">", "W", "[", "j", "]", "[", "k_best", "]", ":", "\n", "                            ", "k_best", "=", "k", "\n", "", "", "", "gene", ".", "append", "(", "(", "PRIMITIVES_DARTS_EXT", "[", "k_best", "]", ",", "j", ")", ")", "\n", "", "start", "=", "end", "\n", "n", "+=", "1", "\n", "", "return", "gene", "\n", "\n", "", "gene_normal", "=", "_parse", "(", "F", ".", "softmax", "(", "alphas_normal", ",", "dim", "=", "-", "1", ")", ".", "data", ".", "numpy", "(", ")", ")", "\n", "gene_reduce", "=", "_parse", "(", "F", ".", "softmax", "(", "alphas_reduce", ",", "dim", "=", "-", "1", ")", ".", "data", ".", "numpy", "(", ")", ")", "\n", "\n", "if", "drop_concat", ":", "\n", "        ", "concat", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "2", "+", "steps", "-", "multiplier", ",", "steps", "+", "2", ")", ":", "\n", "            ", "if", "i", "==", "steps", "+", "1", "or", "torch", ".", "rand", "(", "1", ")", ".", "item", "(", ")", ">", "0.5", ":", "# always add the last otherwise the features from the previous sum nodes will be lost", "\n", "                ", "concat", ".", "append", "(", "i", ")", "\n", "", "", "", "else", ":", "\n", "        ", "concat", "=", "range", "(", "2", "+", "steps", "-", "multiplier", ",", "steps", "+", "2", ")", "\n", "\n", "", "genotype", "=", "Genotype", "(", "\n", "normal", "=", "gene_normal", ",", "normal_concat", "=", "concat", ",", "\n", "reduce", "=", "gene_reduce", ",", "reduce_concat", "=", "concat", "\n", ")", "\n", "\n", "return", "genotype", "\n", "", ""]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.graph.GraphBatch.__init__": [[36, 46], ["graph.GraphBatch.append"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.graph.GraphBatch.append"], ["def", "__init__", "(", "self", ",", "graphs", ")", ":", "\n", "        ", "r\"\"\"\n        :param graphs: iterable, where each item is a Graph object.\n        \"\"\"", "\n", "self", ".", "n_nodes", ",", "self", ".", "node_feat", ",", "self", ".", "node_info", ",", "self", ".", "edges", ",", "self", ".", "net_args", ",", "self", ".", "net_inds", "=", "[", "]", ",", "[", "]", ",", "[", "]", ",", "[", "]", ",", "[", "]", ",", "[", "]", "\n", "self", ".", "_n_edges", "=", "[", "]", "\n", "self", ".", "graphs", "=", "graphs", "\n", "if", "graphs", "is", "not", "None", ":", "\n", "            ", "for", "graph", "in", "graphs", ":", "\n", "                ", "self", ".", "append", "(", "graph", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.graph.GraphBatch.append": [[48, 60], ["len", "graph.GraphBatch.n_nodes.append", "graph.GraphBatch._n_edges.append", "graph.GraphBatch.node_feat.append", "graph.GraphBatch.edges.append", "graph.GraphBatch.node_info.append", "graph.GraphBatch.net_args.append", "graph.GraphBatch.net_inds.append", "len", "len", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "len", "len"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.graph.GraphBatch.append", "home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.graph.GraphBatch.append", "home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.graph.GraphBatch.append", "home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.graph.GraphBatch.append", "home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.graph.GraphBatch.append", "home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.graph.GraphBatch.append", "home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.graph.GraphBatch.append"], ["", "", "", "def", "append", "(", "self", ",", "graph", ")", ":", "\n", "        ", "graph_offset", "=", "len", "(", "self", ".", "n_nodes", ")", "# current index of the graph in a batch", "\n", "self", ".", "n_nodes", ".", "append", "(", "len", "(", "graph", ".", "node_feat", ")", ")", "# number of nodes", "\n", "self", ".", "_n_edges", ".", "append", "(", "len", "(", "graph", ".", "edges", ")", ")", "# number of edges", "\n", "self", ".", "node_feat", ".", "append", "(", "torch", ".", "cat", "(", "(", "graph", ".", "node_feat", ",", "# primitive type", "\n", "graph_offset", "+", "torch", ".", "zeros", "(", "len", "(", "graph", ".", "node_feat", ")", ",", "1", ",", "dtype", "=", "torch", ".", "long", ")", ")", ",", "dim", "=", "1", ")", ")", "# graph index for each node", "\n", "self", ".", "edges", ".", "append", "(", "torch", ".", "cat", "(", "(", "graph", ".", "edges", ",", "\n", "graph_offset", "+", "torch", ".", "zeros", "(", "len", "(", "graph", ".", "edges", ")", ",", "1", ",", "dtype", "=", "torch", ".", "long", ")", ")", ",", "dim", "=", "1", ")", ")", "# graph index for each edge", "\n", "\n", "self", ".", "node_info", ".", "append", "(", "graph", ".", "node_info", ")", "# op names, ids, etc.", "\n", "self", ".", "net_args", ".", "append", "(", "graph", ".", "net_args", ")", "# a dictionary of arguments to construct a Network object", "\n", "self", ".", "net_inds", ".", "append", "(", "graph", ".", "net_idx", ")", "# network integer identifier (optional)", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.graph.GraphBatch.scatter": [[62, 102], ["len", "int", "numpy.arange", "graph.GraphBatch._cat", "torch.Scatter.apply", "torch.Scatter.apply", "torch.Scatter.apply", "torch.Scatter.apply", "torch.Scatter.apply", "torch.Scatter.apply", "torch.Scatter.apply", "torch.Scatter.apply", "torch.Scatter.apply", "enumerate", "numpy.ceil", "len", "graph.GraphBatch._sort_by_nodes", "sum", "sum", "len", "graph.GraphBatch", "batch_lst.append", "len", "len"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.graph.GraphBatch._cat", "home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.graph.GraphBatch._sort_by_nodes", "home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.graph.GraphBatch.append"], ["", "def", "scatter", "(", "self", ",", "device_ids", ",", "nets", ")", ":", "\n", "        ", "\"\"\"\n        Distributes the batch of graphs and networks to multiple CUDA devices.\n        :param device_ids: list of CUDA devices\n        :param nets: list of networks\n        :return: list of tuples of networks and corresponding graphs\n        \"\"\"", "\n", "\n", "n_graphs", "=", "len", "(", "self", ".", "n_nodes", ")", "# number of graphs in a batch", "\n", "graphs_per_device", "=", "int", "(", "np", ".", "ceil", "(", "n_graphs", "/", "len", "(", "device_ids", ")", ")", ")", "\n", "\n", "if", "len", "(", "device_ids", ")", ">", "1", ":", "\n", "            ", "sorted_idx", "=", "self", ".", "_sort_by_nodes", "(", "len", "(", "device_ids", ")", ",", "graphs_per_device", ")", "\n", "nets", "=", "[", "nets", "[", "i", "]", "for", "i", "in", "sorted_idx", "]", "\n", "\n", "", "chunks_iter", "=", "np", ".", "arange", "(", "0", ",", "n_graphs", ",", "graphs_per_device", ")", "\n", "node_chunks", "=", "[", "sum", "(", "self", ".", "n_nodes", "[", "i", ":", "i", "+", "graphs_per_device", "]", ")", "for", "i", "in", "chunks_iter", "]", "\n", "edge_chunks", "=", "[", "sum", "(", "self", ".", "_n_edges", "[", "i", ":", "i", "+", "graphs_per_device", "]", ")", "for", "i", "in", "chunks_iter", "]", "\n", "n_nodes_chunks", "=", "[", "len", "(", "self", ".", "n_nodes", "[", "i", ":", "i", "+", "graphs_per_device", "]", ")", "for", "i", "in", "chunks_iter", "]", "\n", "self", ".", "_cat", "(", ")", "\n", "\n", "self", ".", "node_feat", "=", "scatter_gather", ".", "Scatter", ".", "apply", "(", "device_ids", ",", "node_chunks", ",", "0", ",", "self", ".", "node_feat", ")", "\n", "self", ".", "edges", "=", "scatter_gather", ".", "Scatter", ".", "apply", "(", "device_ids", ",", "edge_chunks", ",", "0", ",", "self", ".", "edges", ")", "\n", "self", ".", "n_nodes", "=", "scatter_gather", ".", "Scatter", ".", "apply", "(", "device_ids", ",", "n_nodes_chunks", ",", "0", ",", "self", ".", "n_nodes", ")", "\n", "\n", "batch_lst", "=", "[", "]", "# each item in the list is a GraphBatch instance", "\n", "for", "device", ",", "i", "in", "enumerate", "(", "chunks_iter", ")", ":", "\n", "# update graph_offset for each device", "\n", "            ", "self", ".", "node_feat", "[", "device", "]", "[", ":", ",", "-", "1", "]", "=", "self", ".", "node_feat", "[", "device", "]", "[", ":", ",", "-", "1", "]", "-", "graphs_per_device", "*", "device", "\n", "self", ".", "edges", "[", "device", "]", "[", ":", ",", "-", "1", "]", "=", "self", ".", "edges", "[", "device", "]", "[", ":", ",", "-", "1", "]", "-", "graphs_per_device", "*", "device", "\n", "graphs", "=", "GraphBatch", "(", "[", "]", ")", "\n", "graphs", ".", "node_feat", "=", "self", ".", "node_feat", "[", "device", "]", "\n", "graphs", ".", "edges", "=", "self", ".", "edges", "[", "device", "]", "\n", "graphs", ".", "n_nodes", "=", "self", ".", "n_nodes", "[", "device", "]", "\n", "graphs", ".", "node_info", "=", "self", ".", "node_info", "[", "i", ":", "i", "+", "graphs_per_device", "]", "\n", "graphs", ".", "net_args", "=", "self", ".", "net_args", "[", "i", ":", "i", "+", "graphs_per_device", "]", "\n", "graphs", ".", "net_inds", "=", "self", ".", "net_inds", "[", "i", ":", "i", "+", "graphs_per_device", "]", "\n", "batch_lst", ".", "append", "(", "(", "nets", "[", "i", ":", "i", "+", "graphs_per_device", "]", ",", "graphs", ")", ")", "# match signature of the GHN forward pass", "\n", "\n", "", "return", "batch_lst", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.graph.GraphBatch.to_device": [[104, 112], ["isinstance", "graph.GraphBatch._cat", "graph.GraphBatch.n_nodes.to", "graph.GraphBatch.node_feat.to", "graph.GraphBatch.edges.to"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.graph.GraphBatch._cat"], ["", "def", "to_device", "(", "self", ",", "device", ")", ":", "\n", "        ", "if", "isinstance", "(", "device", ",", "(", "tuple", ",", "list", ")", ")", ":", "\n", "            ", "device", "=", "device", "[", "0", "]", "\n", "", "self", ".", "_cat", "(", ")", "\n", "self", ".", "n_nodes", "=", "self", ".", "n_nodes", ".", "to", "(", "device", ")", "\n", "self", ".", "node_feat", "=", "self", ".", "node_feat", ".", "to", "(", "device", ")", "\n", "self", ".", "edges", "=", "self", ".", "edges", ".", "to", "(", "device", ")", "\n", "return", "self", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.graph.GraphBatch._sort_by_nodes": [[114, 161], ["numpy.array", "heapq.heapify", "range", "enumerate", "numpy.concatenate", "enumerate", "numpy.argsort", "idx_groups[].append", "heapq.heappush", "node_feat.append", "edges.append", "range", "heapq.heappop", "numpy.array", "len", "idx_groups.values"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.graph.GraphBatch.append", "home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.graph.GraphBatch.append", "home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.graph.GraphBatch.append"], ["", "def", "_sort_by_nodes", "(", "self", ",", "num_devices", ",", "graphs_per_device", ")", ":", "\n", "        ", "\"\"\"\n        Sorts graphs and associated attributes in a batch by the number of nodes such\n        that the memory consumption is more balanced across GPUs.\n        :param num_devices: number of GPU devices (must be more than 1)\n        :param graphs_per_device: number of graphs per GPU\n                                (all GPUs are assumed to receive the same number of graphs)\n        :return: indices of sorted graphs\n        \"\"\"", "\n", "n_nodes", "=", "np", ".", "array", "(", "self", ".", "n_nodes", ")", "\n", "sorted_idx", "=", "np", ".", "argsort", "(", "n_nodes", ")", "[", ":", ":", "-", "1", "]", "\n", "n_nodes", "=", "n_nodes", "[", "sorted_idx", "]", "\n", "\n", "heap", "=", "[", "(", "0", ",", "idx", ")", "for", "idx", "in", "range", "(", "num_devices", ")", "]", "\n", "heapq", ".", "heapify", "(", "heap", ")", "\n", "idx_groups", "=", "{", "}", "\n", "for", "i", "in", "range", "(", "num_devices", ")", ":", "\n", "            ", "idx_groups", "[", "i", "]", "=", "[", "]", "\n", "\n", "", "for", "idx", ",", "n", "in", "enumerate", "(", "n_nodes", ")", ":", "\n", "            ", "while", "True", ":", "\n", "                ", "set_sum", ",", "set_idx", "=", "heapq", ".", "heappop", "(", "heap", ")", "\n", "if", "len", "(", "idx_groups", "[", "set_idx", "]", ")", "<", "graphs_per_device", ":", "\n", "                    ", "break", "\n", "", "", "idx_groups", "[", "set_idx", "]", ".", "append", "(", "sorted_idx", "[", "idx", "]", ")", "\n", "heapq", ".", "heappush", "(", "heap", ",", "(", "set_sum", "+", "n", ",", "set_idx", ")", ")", "\n", "\n", "", "idx", "=", "np", ".", "concatenate", "(", "[", "np", ".", "array", "(", "v", ")", "for", "v", "in", "idx_groups", ".", "values", "(", ")", "]", ")", "\n", "\n", "# Sort everything according to the idx order", "\n", "self", ".", "n_nodes", "=", "[", "self", ".", "n_nodes", "[", "i", "]", "for", "i", "in", "idx", "]", "\n", "self", ".", "_n_edges", "=", "[", "self", ".", "_n_edges", "[", "i", "]", "for", "i", "in", "idx", "]", "\n", "self", ".", "node_info", "=", "[", "self", ".", "node_info", "[", "i", "]", "for", "i", "in", "idx", "]", "\n", "self", ".", "net_args", "=", "[", "self", ".", "net_args", "[", "i", "]", "for", "i", "in", "idx", "]", "\n", "self", ".", "net_inds", "=", "[", "self", ".", "net_inds", "[", "i", "]", "for", "i", "in", "idx", "]", "\n", "node_feat", ",", "edges", "=", "[", "]", ",", "[", "]", "\n", "for", "graph_offset", ",", "i", "in", "enumerate", "(", "idx", ")", ":", "\n", "# update graph_offset for each graph", "\n", "            ", "node_feat_i", "=", "self", ".", "node_feat", "[", "i", "]", "\n", "edges_i", "=", "self", ".", "edges", "[", "i", "]", "\n", "node_feat_i", "[", ":", ",", "-", "1", "]", "=", "graph_offset", "\n", "edges_i", "[", ":", ",", "-", "1", "]", "=", "graph_offset", "\n", "node_feat", ".", "append", "(", "node_feat_i", ")", "\n", "edges", ".", "append", "(", "edges_i", ")", "\n", "", "self", ".", "node_feat", "=", "node_feat", "\n", "self", ".", "edges", "=", "edges", "\n", "return", "idx", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.graph.GraphBatch._cat": [[163, 170], ["isinstance", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "isinstance", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "isinstance", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat"], "methods", ["None"], ["", "def", "_cat", "(", "self", ")", ":", "\n", "        ", "if", "not", "isinstance", "(", "self", ".", "n_nodes", ",", "torch", ".", "Tensor", ")", ":", "\n", "            ", "self", ".", "n_nodes", "=", "torch", ".", "tensor", "(", "self", ".", "n_nodes", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "", "if", "not", "isinstance", "(", "self", ".", "node_feat", ",", "torch", ".", "Tensor", ")", ":", "\n", "            ", "self", ".", "node_feat", "=", "torch", ".", "cat", "(", "self", ".", "node_feat", ")", "\n", "", "if", "not", "isinstance", "(", "self", ".", "edges", ",", "torch", ".", "Tensor", ")", ":", "\n", "            ", "self", ".", "edges", "=", "torch", ".", "cat", "(", "self", ".", "edges", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.graph.GraphBatch.__getitem__": [[172, 174], ["None"], "methods", ["None"], ["", "", "def", "__getitem__", "(", "self", ",", "idx", ")", ":", "\n", "        ", "return", "self", ".", "graphs", "[", "idx", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.graph.GraphBatch.__len__": [[175, 177], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "n_nodes", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.graph.GraphBatch.__iter__": [[178, 181], ["None"], "methods", ["None"], ["", "def", "__iter__", "(", "self", ")", ":", "\n", "        ", "for", "graph", "in", "self", ".", "graphs", ":", "\n", "            ", "yield", "graph", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.graph.Graph.__init__": [[193, 228], ["graph.Graph._build_graph", "graph.Graph._filter_graph", "graph.Graph._add_virtual_edges", "graph.Graph._construct_features", "len", "hasattr", "isinstance", "hasattr", "torch.nonzero", "torch.nonzero", "torch.nonzero", "torch.nonzero", "torch.nonzero", "torch.nonzero", "torch.nonzero", "torch.nonzero", "torch.nonzero", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "isinstance", "torch.from_numpy().long", "torch.from_numpy().long", "torch.from_numpy().long", "torch.from_numpy().long", "torch.from_numpy().long", "torch.from_numpy().long", "torch.from_numpy().long", "torch.from_numpy().long", "torch.from_numpy().long", "A[].view", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.graph.Graph._build_graph", "home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.graph.Graph._filter_graph", "home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.graph.Graph._add_virtual_edges", "home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.graph.Graph._construct_features"], ["def", "__init__", "(", "self", ",", "model", "=", "None", ",", "node_feat", "=", "None", ",", "node_info", "=", "None", ",", "A", "=", "None", ",", "edges", "=", "None", ",", "net_args", "=", "None", ",", "net_idx", "=", "None", ",", "ve_cutoff", "=", "50", ",", "list_all_nodes", "=", "False", ")", ":", "\n", "        ", "r\"\"\"\n        :param model: Neural Network inherited from nn.Module\n        \"\"\"", "\n", "\n", "assert", "node_feat", "is", "None", "or", "model", "is", "None", ",", "'either model or other arguments must be specified'", "\n", "\n", "self", ".", "model", "=", "model", "\n", "self", ".", "_list_all_nodes", "=", "list_all_nodes", "# True in case of dataset generation", "\n", "self", ".", "nx_graph", "=", "None", "# NetworkX DiGraph instance", "\n", "\n", "if", "model", "is", "not", "None", ":", "\n", "            ", "sz", "=", "model", ".", "expected_input_sz", "if", "hasattr", "(", "model", ",", "'expected_input_sz'", ")", "else", "224", "# assume ImageNet image width/heigh by default", "\n", "self", ".", "expected_input_sz", "=", "sz", "if", "isinstance", "(", "sz", ",", "(", "tuple", ",", "list", ")", ")", "else", "(", "3", ",", "sz", ",", "sz", ")", "# assume images by default", "\n", "self", ".", "n_cells", "=", "self", ".", "model", ".", "_n_cells", "if", "hasattr", "(", "self", ".", "model", ",", "'_n_cells'", ")", "else", "1", "\n", "self", ".", "_build_graph", "(", ")", "# automatically construct an initial computational graph", "\n", "self", ".", "_filter_graph", "(", ")", "# remove redundant/unsupported nodes", "\n", "self", ".", "_add_virtual_edges", "(", "ve_cutoff", "=", "ve_cutoff", ")", "# add virtual edges", "\n", "self", ".", "_construct_features", "(", ")", "# initialize torch.Tensor node and edge features", "\n", "", "else", ":", "\n", "            ", "self", ".", "n_nodes", "=", "len", "(", "node_feat", ")", "\n", "self", ".", "node_feat", "=", "node_feat", "\n", "self", ".", "node_info", "=", "node_info", "\n", "\n", "if", "edges", "is", "None", ":", "\n", "                ", "if", "not", "isinstance", "(", "A", ",", "torch", ".", "Tensor", ")", ":", "\n", "                    ", "A", "=", "torch", ".", "from_numpy", "(", "A", ")", ".", "long", "(", ")", "\n", "", "ind", "=", "torch", ".", "nonzero", "(", "A", ")", "\n", "self", ".", "edges", "=", "torch", ".", "cat", "(", "(", "ind", ",", "A", "[", "ind", "[", ":", ",", "0", "]", ",", "ind", "[", ":", ",", "1", "]", "]", ".", "view", "(", "-", "1", ",", "1", ")", ")", ",", "dim", "=", "1", ")", "\n", "", "else", ":", "\n", "                ", "self", ".", "edges", "=", "edges", "\n", "", "self", ".", "_Adj", "=", "A", "\n", "\n", "", "self", ".", "net_args", "=", "net_args", "\n", "self", ".", "net_idx", "=", "net_idx", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.graph.Graph.num_valid_nodes": [[230, 266], ["model", "isinstance", "loss.mean.mean.mean", "torch.isnan", "torch.isnan", "torch.isnan", "torch.isnan", "torch.isnan", "torch.isnan", "torch.isnan", "torch.isnan", "torch.isnan", "print", "loss.mean.mean.backward", "model.named_parameters", "hasattr", "isinstance", "list", "str", "model.parameters", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "p.grad.abs().sum", "p.dim", "p.grad.abs"], "methods", ["None"], ["", "def", "num_valid_nodes", "(", "self", ",", "model", "=", "None", ")", ":", "\n", "        ", "r\"\"\"\n        Counts the total number of learnable parameter tensors.\n        The function aims to find redundant parameter tensors that are disconnected from the computational graph.\n        The function if based on computing gradients and, thus, is not reliable for all architectures.\n        :param model: nn.Module based object\n        :return: total number of learnable parameter tensors\n        \"\"\"", "\n", "if", "model", "is", "None", ":", "\n", "            ", "model", "=", "self", ".", "model", "\n", "expected_input_sz", "=", "self", ".", "expected_input_sz", "\n", "", "else", ":", "\n", "            ", "sz", "=", "model", ".", "expected_input_sz", "if", "hasattr", "(", "model", ",", "'expected_input_sz'", ")", "else", "224", "\n", "expected_input_sz", "=", "sz", "if", "isinstance", "(", "sz", ",", "(", "tuple", ",", "list", ")", ")", "else", "(", "3", ",", "sz", ",", "sz", ")", "\n", "\n", "", "device", "=", "list", "(", "model", ".", "parameters", "(", ")", ")", "[", "0", "]", ".", "device", "# assume all parameters on the same device", "\n", "loss", "=", "model", "(", "(", "torch", ".", "rand", "(", "1", ",", "*", "expected_input_sz", ",", "device", "=", "device", ")", "-", "0.5", ")", "/", "2", ")", "\n", "if", "isinstance", "(", "loss", ",", "tuple", ")", ":", "\n", "            ", "loss", "=", "loss", "[", "0", "]", "\n", "", "loss", "=", "loss", ".", "mean", "(", ")", "\n", "if", "torch", ".", "isnan", "(", "loss", ")", ":", "\n", "            ", "print", "(", "'could not estimate the number of learnable parameter tensors due the %s loss'", ",", "str", "(", "loss", ")", ")", "\n", "return", "-", "1", "\n", "", "else", ":", "\n", "            ", "loss", ".", "backward", "(", ")", "\n", "valid_ops", "=", "0", "\n", "for", "name", ",", "p", "in", "model", ".", "named_parameters", "(", ")", ":", "\n", "                ", "if", "p", ".", "requires_grad", "and", "p", ".", "grad", "is", "not", "None", ":", "\n", "                    ", "assert", "p", ".", "grad", "is", "not", "None", "and", "p", ".", "dim", "(", ")", ">", "0", ",", "(", "name", ",", "p", ".", "grad", ")", "\n", "s", "=", "p", ".", "grad", ".", "abs", "(", ")", ".", "sum", "(", ")", "\n", "if", "s", ">", "1e-20", ":", "\n", "                        ", "valid_ops", "+=", "1", "\n", "# else:", "\n", "#     print(name, p.shape, s)", "\n", "\n", "", "", "", "", "return", "valid_ops", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.graph.Graph._build_graph": [[268, 390], ["graph.Graph.model", "graph.Graph._build_graph.traverse_graph"], "methods", ["None"], ["", "def", "_build_graph", "(", "self", ")", ":", "\n", "        ", "r\"\"\"\n        Constructs a graph of a neural network in the automatic way.\n        This function is written based on Sergey Zagoruyko's https://github.com/szagoruyko/pytorchviz/blob/master/torchviz/dot.py (MIT License)\n        PyTorch 1.9+ is required to run this script correctly for some architectures.\n        Currently, the function is not written very clearly and may be improved.\n        \"\"\"", "\n", "\n", "param_map", "=", "{", "id", "(", "weight", ")", ":", "(", "name", ",", "module", ")", "for", "name", ",", "(", "weight", ",", "module", ")", "in", "self", ".", "_named_modules", "(", ")", ".", "items", "(", ")", "}", "\n", "nodes", ",", "edges", ",", "seen", "=", "[", "]", ",", "[", "]", ",", "{", "}", "\n", "\n", "def", "get_attr", "(", "fn", ")", ":", "\n", "            ", "attrs", "=", "dict", "(", ")", "\n", "for", "attr", "in", "dir", "(", "fn", ")", ":", "\n", "                ", "if", "not", "attr", ".", "startswith", "(", "'_saved_'", ")", ":", "\n", "                    ", "continue", "\n", "", "val", "=", "getattr", "(", "fn", ",", "attr", ")", "\n", "attr", "=", "attr", "[", "len", "(", "'_saved_'", ")", ":", "]", "\n", "if", "torch", ".", "is_tensor", "(", "val", ")", ":", "\n", "                    ", "attrs", "[", "attr", "]", "=", "\"[saved tensor]\"", "\n", "", "elif", "isinstance", "(", "val", ",", "tuple", ")", "and", "any", "(", "torch", ".", "is_tensor", "(", "t", ")", "for", "t", "in", "val", ")", ":", "\n", "                    ", "attrs", "[", "attr", "]", "=", "\"[saved tensors]\"", "\n", "", "else", ":", "\n", "                    ", "attrs", "[", "attr", "]", "=", "str", "(", "val", ")", "\n", "", "", "return", "attrs", "\n", "\n", "", "def", "traverse_graph", "(", "fn", ")", ":", "\n", "            ", "assert", "not", "torch", ".", "is_tensor", "(", "fn", ")", "\n", "if", "fn", "in", "seen", ":", "\n", "                ", "return", "seen", "[", "fn", "]", "\n", "\n", "", "fn_name", "=", "str", "(", "type", "(", "fn", ")", ".", "__name__", ")", "\n", "node_link", ",", "link_start", "=", "None", ",", "None", "\n", "if", "fn_name", ".", "find", "(", "'AccumulateGrad'", ")", "<", "0", ":", "\n", "                ", "leaf_nodes", "=", "[", "]", "\n", "for", "u", "in", "fn", ".", "next_functions", ":", "\n", "                    ", "if", "u", "[", "0", "]", "is", "not", "None", ":", "\n", "                        ", "if", "hasattr", "(", "u", "[", "0", "]", ",", "'variable'", ")", ":", "\n", "                            ", "var", "=", "u", "[", "0", "]", ".", "variable", "\n", "name", ",", "module", "=", "param_map", "[", "id", "(", "var", ")", "]", "\n", "if", "type", "(", "module", ")", "in", "NormLayers", "and", "name", ".", "find", "(", "'.bias'", ")", ">=", "0", ":", "\n", "                                ", "continue", "# do not add biases of NormLayers as nodes", "\n", "", "leaf_nodes", ".", "append", "(", "{", "'id'", ":", "u", "[", "0", "]", ",", "\n", "'param_name'", ":", "name", ",", "\n", "'attrs'", ":", "{", "'size'", ":", "var", ".", "size", "(", ")", "}", ",", "\n", "'module'", ":", "module", "}", ")", "\n", "assert", "len", "(", "u", "[", "0", "]", ".", "next_functions", ")", "==", "0", "\n", "\n", "", "", "", "if", "len", "(", "leaf_nodes", ")", "==", "0", ":", "\n", "                    ", "leaf_nodes", ".", "append", "(", "{", "'id'", ":", "fn", ",", "\n", "'param_name'", ":", "fn_name", ",", "\n", "'attrs'", ":", "get_attr", "(", "fn", ")", ",", "\n", "'module'", ":", "None", "}", ")", "\n", "\n", "", "assert", "not", "hasattr", "(", "fn", ",", "'variable'", ")", ",", "fn", ".", "variable", "\n", "\n", "for", "leaf", "in", "leaf_nodes", ":", "\n", "                    ", "node_link", "=", "str", "(", "id", "(", "leaf", "[", "'id'", "]", ")", ")", "\n", "if", "link_start", "is", "None", ":", "\n", "                        ", "link_start", "=", "node_link", "\n", "\n", "", "seen", "[", "leaf", "[", "'id'", "]", "]", "=", "(", "node_link", ",", "leaf", "[", "'param_name'", "]", ")", "\n", "nodes", ".", "append", "(", "{", "'id'", ":", "node_link", ",", "\n", "'param_name'", ":", "leaf", "[", "'param_name'", "]", ",", "\n", "'attrs'", ":", "leaf", "[", "'attrs'", "]", ",", "\n", "'module'", ":", "leaf", "[", "'module'", "]", "}", ")", "\n", "\n", "", "", "seen", "[", "fn", "]", "=", "(", "node_link", ",", "fn_name", ")", "\n", "\n", "# recurse", "\n", "if", "hasattr", "(", "fn", ",", "'next_functions'", ")", ":", "\n", "                ", "for", "u", "in", "fn", ".", "next_functions", ":", "\n", "                    ", "if", "u", "[", "0", "]", "is", "not", "None", ":", "\n", "                        ", "link_", ",", "name_", "=", "traverse_graph", "(", "u", "[", "0", "]", ")", "\n", "if", "link_", "is", "not", "None", "and", "link_start", "!=", "link_", ":", "\n", "                            ", "edges", ".", "append", "(", "(", "link_start", ",", "link_", ")", "if", "name_", ".", "find", "(", "'bias'", ")", ">=", "0", "else", "(", "link_", ",", "link_start", ")", ")", "\n", "\n", "", "", "", "", "return", "node_link", ",", "fn_name", "\n", "\n", "", "var", "=", "self", ".", "model", "(", "torch", ".", "randn", "(", "1", ",", "*", "self", ".", "expected_input_sz", ")", ")", "\n", "# take only the first output, but can in principle handle multiple outputs, e.g. from auxiliary classifiers", "\n", "traverse_graph", "(", "(", "var", "[", "0", "]", "if", "isinstance", "(", "var", ",", "(", "tuple", ",", "list", ")", ")", "else", "var", ")", ".", "grad_fn", ")", "# populate nodes and edges", "\n", "\n", "nodes_lookup", "=", "{", "node", "[", "'id'", "]", ":", "i", "for", "i", ",", "node", "in", "enumerate", "(", "nodes", ")", "}", "\n", "A", "=", "np", ".", "zeros", "(", "(", "len", "(", "nodes", ")", "+", "1", ",", "len", "(", "nodes", ")", "+", "1", ")", ")", "# +1 for the input node added below", "\n", "for", "out_node_id", ",", "in_node_id", "in", "edges", ":", "\n", "            ", "A", "[", "nodes_lookup", "[", "out_node_id", "]", ",", "nodes_lookup", "[", "in_node_id", "]", "]", "=", "1", "\n", "\n", "# Fix fc layers nodes and edge directions", "\n", "", "for", "i", ",", "node", "in", "enumerate", "(", "nodes", ")", ":", "\n", "            ", "if", "isinstance", "(", "node", "[", "'module'", "]", ",", "nn", ".", "Linear", ")", "and", "node", "[", "'param_name'", "]", ".", "find", "(", "'.weight'", ")", ">=", "0", ":", "\n", "# assert node['module'].bias is not None, ('this rewiring may not work in case of no biases', node)", "\n", "                ", "for", "out_neigh", "in", "np", ".", "where", "(", "A", "[", "i", ",", ":", "]", ")", "[", "0", "]", ":", "# all nodes where there is an edge from i", "\n", "                    ", "A", "[", "np", ".", "where", "(", "A", "[", ":", ",", "out_neigh", "]", ")", "[", "0", "]", ",", "i", "]", "=", "1", "# rewire edges coming to out_neigh (bias) to node i (weight)", "\n", "A", "[", ":", ",", "out_neigh", "]", "=", "0", "# remove all edges to out_neigh except for the edge from i to out_neigh", "\n", "A", "[", "i", ",", "out_neigh", "]", "=", "1", "\n", "A", "[", "i", ",", "i", "]", "=", "0", "# remove loop", "\n", "\n", "# Add input node", "\n", "", "", "", "nodes", ".", "append", "(", "{", "'id'", ":", "'input'", ",", "'param_name'", ":", "'input'", ",", "'attrs'", ":", "None", ",", "'module'", ":", "None", "}", ")", "\n", "ind", "=", "np", ".", "where", "(", "A", "[", ":", ",", ":", "-", "1", "]", ".", "sum", "(", "0", ")", "==", "0", ")", "[", "0", "]", "\n", "assert", "len", "(", "ind", ")", "==", "1", ",", "ind", "\n", "A", "[", "-", "1", ",", "ind", "]", "=", "1", "\n", "\n", "# Sort nodes in a topological order consistent with forward propagation", "\n", "A", "[", "np", ".", "diag_indices_from", "(", "A", ")", "]", "=", "0", "\n", "ind", "=", "np", ".", "array", "(", "list", "(", "nx", ".", "topological_sort", "(", "nx", ".", "DiGraph", "(", "A", ")", ")", ")", ")", "\n", "nodes", "=", "[", "nodes", "[", "i", "]", "for", "i", "in", "ind", "]", "\n", "A", "=", "A", "[", "ind", ",", ":", "]", "[", ":", ",", "ind", "]", "\n", "\n", "# Adjust graph for Transformers to be consistent with our original code", "\n", "for", "i", ",", "node", "in", "enumerate", "(", "nodes", ")", ":", "\n", "            ", "if", "isinstance", "(", "node", "[", "'module'", "]", ",", "PosEnc", ")", ":", "\n", "                ", "nodes", ".", "insert", "(", "i", "+", "1", ",", "{", "'id'", ":", "'sum_pos_enc'", ",", "'param_name'", ":", "'AddBackward0'", ",", "'attrs'", ":", "None", ",", "'module'", ":", "None", "}", ")", "\n", "A", "=", "np", ".", "insert", "(", "A", ",", "i", ",", "0", ",", "axis", "=", "0", ")", "\n", "A", "=", "np", ".", "insert", "(", "A", ",", "i", ",", "0", ",", "axis", "=", "1", ")", "\n", "A", "[", "i", ",", "i", "+", "1", "]", "=", "1", "# pos_enc to sum", "\n", "\n", "", "", "self", ".", "_Adj", "=", "A", "\n", "self", ".", "_nodes", "=", "nodes", "\n", "\n", "return", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.graph.Graph._filter_graph": [[392, 456], ["set", "enumerate", "node[].find", "enumerate", "numpy.array", "set.add", "list", "len", "type", "op_name.find", "op_name.startswith", "numpy.array.append", "len", "op_name.startswith", "isinstance", "[].startswith", "op_name.startswith", "numpy.where", "[].startswith", "numpy.where", "[].startswith", "op_name.startswith", "op_name.startswith", "len", "len", "numpy.where"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.graph.GraphBatch.append"], ["", "def", "_filter_graph", "(", "self", ")", ":", "\n", "        ", "r\"\"\"\n        Remove redundant/unsupported nodes from the automatically constructed graphs.\n        :return:\n        \"\"\"", "\n", "\n", "# These ops will not be added to the graph", "\n", "unsupported_modules", "=", "set", "(", ")", "\n", "for", "i", ",", "node", "in", "enumerate", "(", "self", ".", "_nodes", ")", ":", "\n", "            ", "ind", "=", "node", "[", "'param_name'", "]", ".", "find", "(", "'Backward'", ")", "\n", "name", "=", "node", "[", "'param_name'", "]", "[", ":", "len", "(", "node", "[", "'param_name'", "]", ")", "if", "ind", "==", "-", "1", "else", "ind", "]", "\n", "if", "type", "(", "node", "[", "'module'", "]", ")", "not", "in", "MODULES", "and", "name", "not", "in", "MODULES", ":", "\n", "                ", "unsupported_modules", ".", "add", "(", "node", "[", "'param_name'", "]", ")", "\n", "\n", "# Add ops requiring extra checks before removing", "\n", "", "", "unsupported_modules", "=", "[", "'Mul'", ",", "'Clone'", "]", "+", "list", "(", "unsupported_modules", ")", "+", "[", "'Mean'", ",", "'Add'", ",", "'Cat'", "]", "\n", "\n", "for", "pattern", "in", "unsupported_modules", ":", "\n", "\n", "            ", "ind_keep", "=", "[", "]", "\n", "\n", "for", "i", ",", "node", "in", "enumerate", "(", "self", ".", "_nodes", ")", ":", "\n", "                ", "op_name", ",", "attrs", "=", "node", "[", "'param_name'", "]", ",", "node", "[", "'attrs'", "]", "\n", "\n", "if", "op_name", ".", "find", "(", "pattern", ")", ">=", "0", ":", "\n", "\n", "                    ", "keep", "=", "False", "\n", "if", "op_name", ".", "startswith", "(", "'Mean'", ")", ":", "\n", "# Avoid adding mean operations (in CSE)", "\n", "                        ", "if", "isinstance", "(", "attrs", ",", "dict", ")", "and", "'keepdim'", "in", "attrs", ":", "\n", "                            ", "keep", "=", "attrs", "[", "'keepdim'", "]", "==", "'True'", "\n", "", "else", ":", "\n", "# In pytorch <1.9 the computational graph may be inaccurate", "\n", "                            ", "keep", "=", "i", "<", "len", "(", "self", ".", "_nodes", ")", "-", "1", "and", "not", "self", ".", "_nodes", "[", "i", "+", "1", "]", "[", "'param_name'", "]", ".", "startswith", "(", "'cells.'", ")", "\n", "\n", "", "", "elif", "op_name", ".", "startswith", "(", "'Mul'", ")", ":", "\n", "                        ", "keep", "=", "self", ".", "_nodes", "[", "i", "-", "2", "]", "[", "'param_name'", "]", ".", "startswith", "(", "'Hard'", ")", "# CSE op", "\n", "\n", "", "elif", "op_name", ".", "startswith", "(", "'Clone'", ")", ":", "\n", "                        ", "keep", "=", "self", ".", "_nodes", "[", "i", "-", "11", "]", "[", "'param_name'", "]", ".", "startswith", "(", "'Softmax'", ")", "# MSA op", "\n", "\n", "", "elif", "op_name", ".", "startswith", "(", "'Cat'", ")", "or", "op_name", ".", "startswith", "(", "'Add'", ")", ":", "# Concat and Residual (Sum) ops", "\n", "                        ", "keep", "=", "len", "(", "np", ".", "where", "(", "self", ".", "_Adj", "[", ":", ",", "i", "]", ")", "[", "0", "]", ")", ">", "1", "# keep only if > 1 edges are incoming", "\n", "\n", "", "if", "not", "keep", ":", "\n", "# rewire edges from/to the to-be-removed node to its neighbors", "\n", "                        ", "for", "n1", "in", "np", ".", "where", "(", "self", ".", "_Adj", "[", "i", ",", ":", "]", ")", "[", "0", "]", ":", "\n", "                            ", "for", "n2", "in", "np", ".", "where", "(", "self", ".", "_Adj", "[", ":", ",", "i", "]", ")", "[", "0", "]", ":", "\n", "                                ", "if", "n1", "!=", "n2", ":", "\n", "                                    ", "self", ".", "_Adj", "[", "n2", ",", "n1", "]", "=", "1", "\n", "", "", "", "", "", "else", ":", "\n", "                    ", "keep", "=", "True", "\n", "\n", "", "if", "keep", ":", "\n", "                    ", "ind_keep", ".", "append", "(", "i", ")", "\n", "\n", "", "", "ind_keep", "=", "np", ".", "array", "(", "ind_keep", ")", "\n", "\n", "if", "len", "(", "ind_keep", ")", "<", "self", ".", "_Adj", ".", "shape", "[", "0", "]", ":", "\n", "                ", "self", ".", "_Adj", "=", "self", ".", "_Adj", "[", ":", ",", "ind_keep", "]", "[", "ind_keep", ",", ":", "]", "\n", "self", ".", "_nodes", "=", "[", "self", ".", "_nodes", "[", "i", "]", "for", "i", "in", "ind_keep", "]", "\n", "\n", "", "", "return", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.graph.Graph._add_virtual_edges": [[458, 490], ["len", "graph.Graph._nx_graph_from_adj", "networkx.shortest_path", "range", "networkx.shortest_path", "range", "graph.Graph._Adj[].sum", "graph.Graph._Adj[].sum", "dict", "[].startswith", "networkx.all_pairs_shortest_path_length", "numpy.diag_indices_from", "numpy.diag_indices_from"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.graph.Graph._nx_graph_from_adj"], ["", "def", "_add_virtual_edges", "(", "self", ",", "ve_cutoff", "=", "50", ")", ":", "\n", "        ", "r\"\"\"\n        Add virtual edges with weights equal the shortest path length between the nodes.\n        :param ve_cutoff: maximum shortest path length between the nodes\n        :return:\n        \"\"\"", "\n", "\n", "self", ".", "n_nodes", "=", "len", "(", "self", ".", "_nodes", ")", "\n", "\n", "assert", "self", ".", "_Adj", "[", "np", ".", "diag_indices_from", "(", "self", ".", "_Adj", ")", "]", ".", "sum", "(", ")", "==", "0", ",", "(", "\n", "'no loops should be in the graph'", ",", "self", ".", "_Adj", "[", "np", ".", "diag_indices_from", "(", "self", ".", "_Adj", ")", "]", ".", "sum", "(", ")", ")", "\n", "\n", "# Check that the graph is connected and all nodes reach the final output", "\n", "self", ".", "_nx_graph_from_adj", "(", ")", "\n", "length", "=", "nx", ".", "shortest_path", "(", "self", ".", "nx_graph", ",", "target", "=", "self", ".", "n_nodes", "-", "1", ")", "\n", "for", "node", "in", "range", "(", "self", ".", "n_nodes", ")", ":", "\n", "            ", "assert", "node", "in", "length", ",", "(", "'not all nodes reach the final node'", ",", "node", ",", "self", ".", "_nodes", "[", "node", "]", ")", "\n", "\n", "# Check that all nodes have a path to the input", "\n", "", "length", "=", "nx", ".", "shortest_path", "(", "self", ".", "nx_graph", ",", "source", "=", "0", ")", "\n", "for", "node", "in", "range", "(", "self", ".", "n_nodes", ")", ":", "\n", "            ", "assert", "node", "in", "length", "or", "self", ".", "_nodes", "[", "node", "]", "[", "'param_name'", "]", ".", "startswith", "(", "'pos_enc'", ")", ",", "(", "\n", "'not all nodes have a path to the input'", ",", "node", ",", "self", ".", "_nodes", "[", "node", "]", ")", "\n", "\n", "", "if", "ve_cutoff", ">", "1", ":", "\n", "            ", "length", "=", "dict", "(", "nx", ".", "all_pairs_shortest_path_length", "(", "self", ".", "nx_graph", ",", "cutoff", "=", "ve_cutoff", ")", ")", "\n", "for", "node1", "in", "length", ":", "\n", "                ", "for", "node2", "in", "length", "[", "node1", "]", ":", "\n", "                    ", "if", "length", "[", "node1", "]", "[", "node2", "]", ">", "0", "and", "self", ".", "_Adj", "[", "node1", ",", "node2", "]", "==", "0", ":", "\n", "                        ", "self", ".", "_Adj", "[", "node1", ",", "node2", "]", "=", "length", "[", "node1", "]", "[", "node2", "]", "\n", "", "", "", "assert", "(", "self", ".", "_Adj", ">", "ve_cutoff", ")", ".", "sum", "(", ")", "==", "0", ",", "(", "(", "self", ".", "_Adj", ">", "ve_cutoff", ")", ".", "sum", "(", ")", ",", "ve_cutoff", ")", "\n", "", "return", "self", ".", "_Adj", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.graph.Graph._construct_features": [[492, 581], ["len", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "enumerate", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.nonzero", "torch.nonzero", "torch.nonzero", "torch.nonzero", "torch.nonzero", "torch.nonzero", "torch.nonzero", "torch.nonzero", "torch.nonzero", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "net.get_cell_ind", "param_name.find", "param_name.find", "isinstance", "graph.Graph._param_shapes.append", "print", "range", "enumerate", "param_name.split", "enumerate", "param_name.find", "int", "graph.Graph.node_info[].append", "graph.Graph._Adj[].view", "name.find", "param_name.startswith", "param_name.startswith", "name.find", "int", "param_name.split.insert", "type", "param_name.find", "len", "len", "len", "int", "a.strip().strip().strip", "attrs[].split", "a.strip().strip", "a.strip"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.net.get_cell_ind", "home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.graph.GraphBatch.append", "home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.graph.GraphBatch.append"], ["", "def", "_construct_features", "(", "self", ")", ":", "\n", "        ", "r\"\"\"\n        Construct pytorch tensor features for nodes and edges.\n        :return:\n        \"\"\"", "\n", "\n", "self", ".", "n_nodes", "=", "len", "(", "self", ".", "_nodes", ")", "\n", "self", ".", "node_feat", "=", "torch", ".", "zeros", "(", "self", ".", "n_nodes", ",", "1", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "self", ".", "node_info", "=", "[", "[", "]", "for", "_", "in", "range", "(", "self", ".", "n_cells", ")", "]", "\n", "self", ".", "_param_shapes", "=", "[", "]", "\n", "\n", "primitives_dict", "=", "{", "op", ":", "i", "for", "i", ",", "op", "in", "enumerate", "(", "PRIMITIVES_DEEPNETS1M", ")", "}", "\n", "\n", "n_glob_avg", "=", "0", "\n", "cell_ind", "=", "0", "\n", "for", "node_ind", ",", "node", "in", "enumerate", "(", "self", ".", "_nodes", ")", ":", "\n", "\n", "            ", "param_name", "=", "node", "[", "'param_name'", "]", "\n", "cell_ind_", "=", "get_cell_ind", "(", "param_name", ",", "self", ".", "n_cells", ")", "\n", "if", "cell_ind_", "is", "not", "None", ":", "\n", "                ", "cell_ind", "=", "cell_ind_", "\n", "\n", "", "pos_stem", "=", "param_name", ".", "find", "(", "'stem'", ")", "\n", "pos_pos", "=", "param_name", ".", "find", "(", "'pos_enc'", ")", "\n", "if", "pos_stem", ">=", "0", ":", "\n", "                ", "param_name", "=", "param_name", "[", "pos_stem", ":", "]", "\n", "", "elif", "pos_pos", ">=", "0", ":", "\n", "                ", "param_name", "=", "param_name", "[", "pos_pos", ":", "]", "\n", "\n", "", "if", "node", "[", "'module'", "]", "is", "not", "None", ":", "\n", "\n", "# Preprocess param_name to be consistent with the DeepNets dataset", "\n", "                ", "parts", "=", "param_name", ".", "split", "(", "'.'", ")", "\n", "for", "i", ",", "s", "in", "enumerate", "(", "parts", ")", ":", "\n", "                    ", "if", "s", "==", "'_ops'", "and", "parts", "[", "i", "+", "2", "]", "!=", "'op'", ":", "\n", "                        ", "try", ":", "\n", "                            ", "_", "=", "int", "(", "parts", "[", "i", "+", "2", "]", ")", "\n", "parts", ".", "insert", "(", "i", "+", "2", ",", "'op'", ")", "\n", "param_name", "=", "'.'", ".", "join", "(", "parts", ")", "\n", "break", "\n", "", "except", ":", "\n", "                            ", "continue", "\n", "\n", "", "", "", "name", "=", "MODULES", "[", "type", "(", "node", "[", "'module'", "]", ")", "]", "(", "node", "[", "'module'", "]", ",", "param_name", ")", "\n", "\n", "", "else", ":", "\n", "                ", "ind", "=", "param_name", ".", "find", "(", "'Backward'", ")", "\n", "name", "=", "MODULES", "[", "param_name", "[", ":", "len", "(", "param_name", ")", "if", "ind", "==", "-", "1", "else", "ind", "]", "]", "\n", "n_glob_avg", "+=", "int", "(", "name", "==", "'glob_avg'", ")", "\n", "\n", "if", "self", ".", "n_cells", ">", "1", ":", "\n", "# Add cell id to the names of pooling layers, so that they will be matched with proper modules in Network", "\n", "                    ", "if", "param_name", ".", "startswith", "(", "'MaxPool'", ")", "or", "param_name", ".", "startswith", "(", "'AvgPool'", ")", ":", "\n", "                        ", "param_name", "=", "'cells.{}.'", ".", "format", "(", "cell_ind", ")", "+", "name", "\n", "\n", "", "", "", "sz", "=", "None", "\n", "attrs", "=", "node", "[", "'attrs'", "]", "\n", "if", "isinstance", "(", "attrs", ",", "dict", ")", ":", "\n", "                ", "if", "'size'", "in", "attrs", ":", "\n", "                    ", "sz", "=", "attrs", "[", "'size'", "]", "\n", "", "elif", "name", ".", "find", "(", "'pool'", ")", ">=", "0", ":", "\n", "                    ", "if", "'kernel_size'", "in", "attrs", ":", "\n", "                        ", "sz", "=", "(", "1", ",", "1", ",", "*", "[", "int", "(", "a", ".", "strip", "(", "'('", ")", ".", "strip", "(", "')'", ")", ".", "strip", "(", "' '", ")", ")", "for", "a", "in", "attrs", "[", "'kernel_size'", "]", ".", "split", "(", "','", ")", "]", ")", "\n", "", "else", ":", "\n", "# Pytorch 1.9+ is required to correctly extract pooling attributes, otherwise the default pooling size of 3 is used", "\n", "                        ", "sz", "=", "(", "1", ",", "1", ",", "3", ",", "3", ")", "\n", "", "", "", "elif", "node", "[", "'module'", "]", "is", "not", "None", ":", "\n", "                ", "sz", "=", "(", "node", "[", "'module'", "]", ".", "weight", "if", "param_name", ".", "find", "(", "'weight'", ")", ">=", "0", "else", "node", "[", "'module'", "]", ".", "bias", ")", ".", "shape", "\n", "\n", "", "self", ".", "_param_shapes", ".", "append", "(", "sz", ")", "\n", "self", ".", "node_feat", "[", "node_ind", "]", "=", "primitives_dict", "[", "name", "]", "\n", "if", "node", "[", "'module'", "]", "is", "not", "None", "or", "name", ".", "find", "(", "'pool'", ")", ">=", "0", "or", "self", ".", "_list_all_nodes", ":", "\n", "                ", "self", ".", "node_info", "[", "cell_ind", "]", ".", "append", "(", "\n", "(", "node_ind", ",", "\n", "param_name", "if", "node", "[", "'module'", "]", "is", "not", "None", "else", "name", ",", "\n", "name", ",", "\n", "sz", ",", "\n", "node_ind", "==", "len", "(", "self", ".", "_nodes", ")", "-", "2", ",", "\n", "node_ind", "==", "len", "(", "self", ".", "_nodes", ")", "-", "1", ")", ")", "\n", "\n", "", "", "if", "n_glob_avg", ">", "1", ":", "\n", "            ", "print", "(", "\n", "'\\nWARNING: n_glob_avg should be 0 or 1 in most architectures, but is %d in this architecture\\n'", "%", "n_glob_avg", ")", "\n", "\n", "", "self", ".", "_Adj", "=", "torch", ".", "tensor", "(", "self", ".", "_Adj", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "\n", "ind", "=", "torch", ".", "nonzero", "(", "self", ".", "_Adj", ")", "# rows, cols", "\n", "self", ".", "edges", "=", "torch", ".", "cat", "(", "(", "ind", ",", "self", ".", "_Adj", "[", "ind", "[", ":", ",", "0", "]", ",", "ind", "[", ":", ",", "1", "]", "]", ".", "view", "(", "-", "1", ",", "1", ")", ")", ",", "dim", "=", "1", ")", "\n", "return", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.graph.Graph._named_modules": [[583, 598], ["graph.Graph.model.named_modules", "hasattr", "hasattr"], "methods", ["None"], ["", "def", "_named_modules", "(", "self", ")", ":", "\n", "        ", "r\"\"\"\n        Helper function to automatically build the graphs.\n        :return:\n        \"\"\"", "\n", "\n", "modules", "=", "{", "}", "\n", "for", "n", ",", "m", "in", "self", ".", "model", ".", "named_modules", "(", ")", ":", "\n", "            ", "is_w", "=", "hasattr", "(", "m", ",", "'weight'", ")", "and", "m", ".", "weight", "is", "not", "None", "\n", "is_b", "=", "hasattr", "(", "m", ",", "'bias'", ")", "and", "m", ".", "bias", "is", "not", "None", "\n", "if", "is_w", ":", "\n", "                ", "modules", "[", "n", "+", "'.weight'", "]", "=", "(", "m", ".", "weight", ",", "m", ")", "\n", "", "if", "is_b", ":", "\n", "                ", "modules", "[", "n", "+", "'.bias'", "]", "=", "(", "m", ".", "bias", ",", "m", ")", "\n", "", "", "return", "modules", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.graph.Graph._nx_graph_from_adj": [[600, 610], ["networkx.DiGraph", "isinstance", "graph.Graph._Adj.data.cpu().numpy", "graph.Graph._Adj.data.cpu"], "methods", ["None"], ["", "def", "_nx_graph_from_adj", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Creates NetworkX directed graph instance that is used for visualization, virtual edges and graph statistics.\n        :return: nx.DiGraph\n        \"\"\"", "\n", "if", "self", ".", "nx_graph", "is", "None", ":", "\n", "            ", "A", "=", "self", ".", "_Adj", ".", "data", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "if", "isinstance", "(", "self", ".", "_Adj", ",", "torch", ".", "Tensor", ")", "else", "self", ".", "_Adj", "\n", "A", "[", "A", ">", "1", "]", "=", "0", "# remove any virtual edges for the visualization/statistics", "\n", "self", ".", "nx_graph", "=", "nx", ".", "DiGraph", "(", "A", ")", "\n", "", "return", "self", ".", "nx_graph", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.graph.Graph.properties": [[612, 634], ["graph.Graph._nx_graph_from_adj", "G.to_undirected.to_undirected.to_undirected", "dict", "G.to_undirected.to_undirected.degree", "len", "sum", "networkx.average_shortest_path_length", "NotImplementedError", "dict.values"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.graph.Graph._nx_graph_from_adj"], ["", "def", "properties", "(", "self", ",", "undirected", "=", "True", ",", "key", "=", "(", "'avg_degree'", ",", "'avg_path'", ")", ")", ":", "\n", "        ", "\"\"\"\n        Computes graph properties.\n        :param undirected: ignore edge direction when computing graph properties.\n        :param key: a tuple/list of graph properties to estimate.\n        :return: dictionary with property names and values.\n        \"\"\"", "\n", "G", "=", "self", ".", "_nx_graph_from_adj", "(", ")", "\n", "if", "undirected", ":", "\n", "            ", "G", "=", "G", ".", "to_undirected", "(", ")", "\n", "", "props", "=", "{", "}", "\n", "for", "prop", "in", "key", ":", "\n", "            ", "if", "prop", "==", "'avg_degree'", ":", "\n", "                ", "degrees", "=", "dict", "(", "G", ".", "degree", "(", ")", ")", "\n", "assert", "len", "(", "degrees", ")", "==", "self", ".", "_Adj", ".", "shape", "[", "0", "]", "==", "self", ".", "n_nodes", ",", "'invalid graph'", "\n", "props", "[", "prop", "]", "=", "sum", "(", "degrees", ".", "values", "(", ")", ")", "/", "self", ".", "n_nodes", "\n", "", "elif", "prop", "==", "'avg_path'", ":", "\n", "                ", "props", "[", "prop", "]", "=", "nx", ".", "average_shortest_path_length", "(", "G", ")", "\n", "", "else", ":", "\n", "                ", "raise", "NotImplementedError", "(", "prop", ")", "\n", "\n", "", "", "return", "props", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.graph.Graph.visualize": [[636, 735], ["graph.Graph._nx_graph_from_adj", "len", "enumerate", "node_groups.values", "networkx.draw_networkx_edges", "matplotlib.grid", "matplotlib.axis", "matplotlib.show", "len", "len", "matplotlib.cm.jet", "color", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "zip", "node_colors.append", "matplotlib.figure", "networkx.DiGraph", "matplotlib.figure", "networkx.drawing.nx_pydot.graphviz_layout", "networkx.draw_networkx_nodes", "networkx.draw_networkx_labels", "matplotlib.savefig", "matplotlib.savefig", "int", "enumerate", "torch.cat.view", "torch.cat.view", "torch.cat.view", "[].append", "numpy.diag", "numpy.round", "torch.tensor().view", "torch.tensor().view", "torch.tensor().view", "torch.tensor().view", "torch.tensor().view", "torch.tensor().view", "torch.tensor().view", "torch.tensor().view", "torch.tensor().view", "name.find", "[].append", "[].append", "numpy.ones", "networkx.drawing.nx_pydot.graphviz_layout.items", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "len", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "len", "numpy.prod"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.graph.Graph._nx_graph_from_adj", "home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.graph.GraphBatch.append", "home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.graph.GraphBatch.append", "home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.graph.GraphBatch.append", "home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.graph.GraphBatch.append"], ["", "def", "visualize", "(", "self", ",", "node_size", "=", "50", ",", "figname", "=", "None", ",", "figsize", "=", "None", ",", "with_labels", "=", "False", ",", "vis_legend", "=", "False", ",", "label_offset", "=", "0.001", ",", "font_size", "=", "10", ")", ":", "\n", "        ", "r\"\"\"\n        Shows the graphs/legend as in the paper using matplotlib.\n        :param node_size: node size\n        :param figname: file name to save the figure in the .pdf and .png formats\n        :param figsize: (width, height) for a figure\n        :param with_labels: show node labels (operations)\n        :param vis_legend: True to only visualize the legend (graph will be ignored)\n        :param label_offset: positioning of node labels when vis_legend=True\n        :param font_size: font size for node labels, used only when with_labels=True\n        :return:\n        \"\"\"", "\n", "\n", "self", ".", "_nx_graph_from_adj", "(", ")", "\n", "\n", "# first are conv layers, so that they have a similar color", "\n", "primitives_order", "=", "[", "2", ",", "3", ",", "4", ",", "10", ",", "5", ",", "6", ",", "11", ",", "12", ",", "13", ",", "0", ",", "1", ",", "14", ",", "7", ",", "8", ",", "9", "]", "\n", "assert", "len", "(", "PRIMITIVES_DEEPNETS1M", ")", "==", "len", "(", "primitives_order", ")", ",", "'make sure the lists correspond to each other'", "\n", "\n", "n_primitives", "=", "len", "(", "primitives_order", ")", "\n", "color", "=", "lambda", "i", ":", "cm", ".", "jet", "(", "int", "(", "np", ".", "round", "(", "255", "*", "i", "/", "n_primitives", ")", ")", ")", "\n", "primitive_colors", "=", "{", "PRIMITIVES_DEEPNETS1M", "[", "ind_org", "]", ":", "color", "(", "ind_new", ")", "for", "ind_new", ",", "ind_org", "in", "enumerate", "(", "primitives_order", ")", "}", "\n", "# manually adjust some colors for better visualization", "\n", "primitive_colors", "[", "'bias'", "]", "=", "'#%02x%02x%02x'", "%", "(", "255", ",", "0", ",", "255", ")", "\n", "primitive_colors", "[", "'msa'", "]", "=", "'#%02x%02x%02x'", "%", "(", "10", ",", "10", ",", "10", ")", "\n", "primitive_colors", "[", "'ln'", "]", "=", "'#%02x%02x%02x'", "%", "(", "255", ",", "255", ",", "0", ")", "\n", "\n", "node_groups", "=", "{", "'bn'", ":", "{", "'style'", ":", "{", "'edgecolors'", ":", "'k'", ",", "'linewidths'", ":", "1", ",", "'node_shape'", ":", "'s'", "}", "}", ",", "\n", "'conv1'", ":", "{", "'style'", ":", "{", "'edgecolors'", ":", "'k'", ",", "'linewidths'", ":", "1", ",", "'node_shape'", ":", "'^'", "}", "}", ",", "\n", "'bias'", ":", "{", "'style'", ":", "{", "'edgecolors'", ":", "'gray'", ",", "'linewidths'", ":", "0.5", ",", "'node_shape'", ":", "'d'", "}", "}", ",", "\n", "'pos_enc'", ":", "{", "'style'", ":", "{", "'edgecolors'", ":", "'gray'", ",", "'linewidths'", ":", "0.5", ",", "'node_shape'", ":", "'s'", "}", "}", ",", "\n", "'ln'", ":", "{", "'style'", ":", "{", "'edgecolors'", ":", "'gray'", ",", "'linewidths'", ":", "0.5", ",", "'node_shape'", ":", "'s'", "}", "}", ",", "\n", "'max_pool'", ":", "{", "'style'", ":", "{", "'edgecolors'", ":", "'k'", ",", "'linewidths'", ":", "1", ",", "'node_shape'", ":", "'o'", ",", "'node_size'", ":", "1.75", "*", "node_size", "}", "}", ",", "\n", "'glob_avg'", ":", "{", "'style'", ":", "{", "'edgecolors'", ":", "'gray'", ",", "'linewidths'", ":", "0.5", ",", "'node_shape'", ":", "'o'", ",", "'node_size'", ":", "2", "*", "node_size", "}", "}", ",", "\n", "'concat'", ":", "{", "'style'", ":", "{", "'edgecolors'", ":", "'gray'", ",", "'linewidths'", ":", "0.5", ",", "'node_shape'", ":", "'^'", "}", "}", ",", "\n", "'input'", ":", "{", "'style'", ":", "{", "'edgecolors'", ":", "'k'", ",", "'linewidths'", ":", "1.5", ",", "'node_shape'", ":", "'s'", ",", "'node_size'", ":", "2", "*", "node_size", "}", "}", ",", "\n", "'other'", ":", "{", "'style'", ":", "{", "'edgecolors'", ":", "'gray'", ",", "'linewidths'", ":", "0.5", ",", "'node_shape'", ":", "'o'", "}", "}", "}", "\n", "\n", "for", "group", "in", "node_groups", ":", "\n", "            ", "node_groups", "[", "group", "]", "[", "'node_lst'", "]", "=", "[", "]", "\n", "if", "'node_size'", "not", "in", "node_groups", "[", "group", "]", "[", "'style'", "]", ":", "\n", "                ", "node_groups", "[", "group", "]", "[", "'style'", "]", "[", "'node_size'", "]", "=", "node_size", "\n", "\n", "", "", "labels", ",", "node_colors", "=", "{", "}", ",", "[", "]", "\n", "\n", "if", "vis_legend", ":", "\n", "            ", "node_feat", "=", "torch", ".", "cat", "(", "(", "torch", ".", "tensor", "(", "[", "n_primitives", "]", ")", ".", "view", "(", "-", "1", ",", "1", ")", ",", "\n", "torch", ".", "tensor", "(", "primitives_order", ")", "[", ":", ",", "None", "]", ")", ")", "\n", "param_shapes", "=", "[", "(", "3", ",", "3", ",", "1", ",", "1", ")", "]", "+", "[", "None", "]", "*", "n_primitives", "\n", "", "else", ":", "\n", "            ", "node_feat", "=", "self", ".", "node_feat", "\n", "param_shapes", "=", "self", ".", "_param_shapes", "\n", "\n", "", "for", "i", ",", "(", "x", ",", "sz", ")", "in", "enumerate", "(", "zip", "(", "node_feat", ".", "view", "(", "-", "1", ")", ",", "param_shapes", ")", ")", ":", "\n", "\n", "            ", "name", "=", "PRIMITIVES_DEEPNETS1M", "[", "x", "]", "if", "x", "<", "n_primitives", "else", "'conv'", "\n", "labels", "[", "i", "]", "=", "name", "[", ":", "20", "]", "if", "x", "<", "n_primitives", "else", "'conv_1x1'", "\n", "node_colors", ".", "append", "(", "primitive_colors", "[", "name", "]", ")", "\n", "\n", "if", "name", ".", "find", "(", "'conv'", ")", ">=", "0", "and", "sz", "is", "not", "None", "and", "(", "(", "len", "(", "sz", ")", "==", "4", "and", "np", ".", "prod", "(", "sz", "[", "2", ":", "]", ")", "==", "1", ")", "or", "len", "(", "sz", ")", "==", "2", ")", ":", "\n", "                ", "node_groups", "[", "'conv1'", "]", "[", "'node_lst'", "]", ".", "append", "(", "i", ")", "\n", "", "elif", "name", "in", "node_groups", ":", "\n", "                ", "node_groups", "[", "name", "]", "[", "'node_lst'", "]", ".", "append", "(", "i", ")", "\n", "", "else", ":", "\n", "                ", "node_groups", "[", "'other'", "]", "[", "'node_lst'", "]", ".", "append", "(", "i", ")", "\n", "\n", "", "", "if", "vis_legend", ":", "\n", "            ", "fig", "=", "plt", ".", "figure", "(", "figsize", "=", "(", "20", ",", "3", ")", "if", "figsize", "is", "None", "else", "figsize", ")", "\n", "G", "=", "nx", ".", "DiGraph", "(", "np", ".", "diag", "(", "np", ".", "ones", "(", "n_primitives", ")", ",", "1", ")", ")", "\n", "pos", "=", "{", "i", ":", "(", "3", "*", "i", "*", "node_size", ",", "0", ")", "for", "i", "in", "labels", "}", "\n", "pos_labels", "=", "{", "i", ":", "(", "x", ",", "y", "-", "label_offset", ")", "for", "i", ",", "(", "x", ",", "y", ")", "in", "pos", ".", "items", "(", ")", "}", "\n", "", "else", ":", "\n", "            ", "fig", "=", "plt", ".", "figure", "(", "figsize", "=", "(", "10", ",", "10", ")", "if", "figsize", "is", "None", "else", "figsize", ")", "\n", "G", "=", "self", ".", "nx_graph", "\n", "pos", "=", "nx", ".", "drawing", ".", "nx_pydot", ".", "graphviz_layout", "(", "G", ")", "\n", "pos_labels", "=", "pos", "\n", "\n", "", "for", "node_group", "in", "node_groups", ".", "values", "(", ")", ":", "\n", "            ", "nx", ".", "draw_networkx_nodes", "(", "G", ",", "pos", ",", "\n", "node_color", "=", "[", "node_colors", "[", "i", "]", "for", "i", "in", "node_group", "[", "'node_lst'", "]", "]", ",", "\n", "nodelist", "=", "node_group", "[", "'node_lst'", "]", ",", "\n", "**", "node_group", "[", "'style'", "]", ")", "\n", "", "if", "with_labels", ":", "\n", "            ", "nx", ".", "draw_networkx_labels", "(", "G", ",", "pos_labels", ",", "labels", ",", "font_size", "=", "font_size", ")", "\n", "\n", "", "nx", ".", "draw_networkx_edges", "(", "G", ",", "pos", ",", "node_size", "=", "node_size", ",", "\n", "width", "=", "0", "if", "vis_legend", "else", "1", ",", "\n", "arrowsize", "=", "10", ",", "\n", "alpha", "=", "0", "if", "vis_legend", "else", "1", ",", "\n", "edge_color", "=", "'white'", "if", "vis_legend", "else", "'k'", ",", "\n", "arrowstyle", "=", "'-|>'", ")", "\n", "\n", "plt", ".", "grid", "(", "False", ")", "\n", "plt", ".", "axis", "(", "'off'", ")", "\n", "if", "figname", "is", "not", "None", ":", "\n", "            ", "plt", ".", "savefig", "(", "figname", "+", "'.pdf'", ",", "dpi", "=", "fig", ".", "dpi", ")", "\n", "plt", ".", "savefig", "(", "figname", "+", "'.png'", ",", "dpi", "=", "fig", ".", "dpi", ",", "transparent", "=", "True", ")", "\n", "", "plt", ".", "show", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.graph.get_conv_name": [[737, 743], ["param_name.find", "isinstance", "min"], "function", ["None"], ["", "", "def", "get_conv_name", "(", "module", ",", "param_name", ")", ":", "\n", "    ", "if", "param_name", ".", "find", "(", "'bias'", ")", ">=", "0", ":", "\n", "        ", "return", "'bias'", "\n", "", "elif", "isinstance", "(", "module", ",", "nn", ".", "Conv2d", ")", "and", "module", ".", "groups", ">", "1", ":", "\n", "        ", "return", "(", "'dil_conv'", "if", "min", "(", "module", ".", "dilation", ")", ">", "1", "else", "'sep_conv'", ")", "\n", "", "return", "'conv'", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.ops.ChannelSELayer.__init__": [[107, 122], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.ReLU", "torch.ReLU", "torch.Hardswish", "torch.Hardswish"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_ppuda.examples.custom_cnn.CNN.__init__"], ["def", "__init__", "(", "self", ",", "num_channels", ",", "reduction_ratio", "=", "2", ",", "dim_out", "=", "None", ",", "stride", "=", "1", ")", ":", "\n", "        ", "\"\"\"\n        :param num_channels: No of input channels\n        :param reduction_ratio: By how much should the num_channels should be reduced\n        \"\"\"", "\n", "super", "(", "ChannelSELayer", ",", "self", ")", ".", "__init__", "(", ")", "\n", "if", "dim_out", "is", "not", "None", ":", "\n", "            ", "assert", "dim_out", "==", "num_channels", ",", "(", "dim_out", ",", "num_channels", ",", "'only same dimensionality is supported'", ")", "\n", "", "num_channels_reduced", "=", "num_channels", "//", "reduction_ratio", "\n", "self", ".", "reduction_ratio", "=", "reduction_ratio", "\n", "self", ".", "stride", "=", "stride", "\n", "self", ".", "fc1", "=", "nn", ".", "Linear", "(", "num_channels", ",", "num_channels_reduced", ",", "bias", "=", "True", ")", "\n", "self", ".", "fc2", "=", "nn", ".", "Linear", "(", "num_channels_reduced", ",", "num_channels", ",", "bias", "=", "True", ")", "\n", "self", ".", "relu", "=", "nn", ".", "ReLU", "(", ")", "\n", "self", ".", "sigmoid", "=", "nn", ".", "Hardswish", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.ops.ChannelSELayer.forward": [[123, 141], ["input_tensor.size", "input_tensor.reshape().mean", "ops.ChannelSELayer.relu", "ops.ChannelSELayer.sigmoid", "input_tensor.reshape().mean.size", "torch.mul", "torch.mul", "torch.mul", "torch.mul", "ops.ChannelSELayer.fc1", "ops.ChannelSELayer.fc2", "ops.ChannelSELayer.view", "input_tensor.reshape"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_tensor", ")", ":", "\n", "        ", "\"\"\"\n        :param input_tensor: X, shape = (batch_size, num_channels, H, W)\n        :return: output tensor\n        \"\"\"", "\n", "batch_size", ",", "num_channels", ",", "H", ",", "W", "=", "input_tensor", ".", "size", "(", ")", "\n", "# Average along each channel", "\n", "squeeze_tensor", "=", "input_tensor", ".", "reshape", "(", "batch_size", ",", "num_channels", ",", "-", "1", ")", ".", "mean", "(", "dim", "=", "2", ")", "\n", "\n", "# channel excitation", "\n", "fc_out_1", "=", "self", ".", "relu", "(", "self", ".", "fc1", "(", "squeeze_tensor", ")", ")", "\n", "fc_out_2", "=", "self", ".", "sigmoid", "(", "self", ".", "fc2", "(", "fc_out_1", ")", ")", "\n", "\n", "a", ",", "b", "=", "squeeze_tensor", ".", "size", "(", ")", "\n", "output_tensor", "=", "torch", ".", "mul", "(", "input_tensor", ",", "fc_out_2", ".", "view", "(", "a", ",", "b", ",", "1", ",", "1", ")", ")", "\n", "if", "self", ".", "stride", ">", "1", ":", "\n", "            ", "output_tensor", "=", "output_tensor", "[", ":", ",", ":", ",", ":", ":", "self", ".", "stride", ",", ":", ":", "self", ".", "stride", "]", "\n", "", "return", "output_tensor", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.ops.FeedForward.__init__": [[144, 152], ["torch.Module.__init__", "torch.Sequential", "torch.Sequential", "torch.Linear", "torch.Linear", "torch.GELU", "torch.GELU", "torch.Dropout", "torch.Dropout", "torch.Linear", "torch.Linear", "torch.Dropout", "torch.Dropout"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_ppuda.examples.custom_cnn.CNN.__init__"], ["    ", "def", "__init__", "(", "self", ",", "dim", ",", "hidden_dim", ",", "dropout", "=", "0.", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "net", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Linear", "(", "dim", ",", "hidden_dim", ")", ",", "\n", "nn", ".", "GELU", "(", ")", ",", "\n", "nn", ".", "Dropout", "(", "dropout", ")", ",", "\n", "nn", ".", "Linear", "(", "hidden_dim", ",", "dim", ")", ",", "\n", "nn", ".", "Dropout", "(", "dropout", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.ops.FeedForward.forward": [[153, 155], ["ops.FeedForward.net"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "return", "self", ".", "net", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.ops.PosEnc.__init__": [[158, 161], ["torch.Module.__init__", "torch.Parameter", "torch.Parameter", "torch.randn", "torch.randn", "torch.randn", "torch.randn"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_ppuda.examples.custom_cnn.CNN.__init__"], ["    ", "def", "__init__", "(", "self", ",", "C", ",", "ks", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "weight", "=", "nn", ".", "Parameter", "(", "torch", ".", "randn", "(", "1", ",", "C", ",", "ks", ",", "ks", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.ops.PosEnc.forward": [[162, 164], ["None"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "return", "x", "+", "self", ".", "weight", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.ops.Transformer.__init__": [[193, 212], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "torch.Sequential", "torch.Sequential", "torch.LayerNorm", "torch.LayerNorm", "torch.LayerNorm", "torch.LayerNorm", "ops.FeedForward", "torch.Linear", "torch.Linear", "torch.Dropout", "torch.Dropout"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_ppuda.examples.custom_cnn.CNN.__init__"], ["def", "__init__", "(", "self", ",", "dim", ",", "dim_out", "=", "None", ",", "heads", "=", "8", ",", "dim_head", "=", "None", ",", "dropout", "=", "0.", ",", "stride", "=", "1", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "stride", "=", "stride", "\n", "if", "dim_head", "is", "None", ":", "\n", "            ", "dim_head", "=", "dim", "//", "heads", "\n", "", "inner_dim", "=", "dim_head", "*", "heads", "\n", "self", ".", "heads", "=", "heads", "\n", "self", ".", "scale", "=", "dim_head", "**", "-", "0.5", "\n", "\n", "self", ".", "to_qkv", "=", "nn", ".", "Linear", "(", "dim", ",", "inner_dim", "*", "3", ",", "bias", "=", "False", ")", "\n", "self", ".", "to_out", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Linear", "(", "inner_dim", ",", "dim_out", ")", ",", "\n", "nn", ".", "Dropout", "(", "dropout", ")", "\n", ")", "\n", "\n", "self", ".", "ln1", "=", "nn", ".", "LayerNorm", "(", "dim", ")", "\n", "self", ".", "ln2", "=", "nn", ".", "LayerNorm", "(", "dim_out", ")", "\n", "self", ".", "ff", "=", "FeedForward", "(", "dim", ",", "dim_out", ")", "\n", "self", ".", "stride", "=", "stride", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.ops.Transformer.forward": [[213, 246], ["ops.Transformer.ln1", "ops.Transformer.to_qkv().chunk", "map", "dots.softmax", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "einops.rearrange", "ops.Transformer.to_out", "len", "x.reshape().permute.reshape().permute.reshape().permute", "x.reshape().permute.reshape().permute.dim", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "NotImplementedError", "ops.Transformer.ff", "len", "out.permute().view.permute().view.permute().view", "ops.Transformer.to_qkv", "einops.rearrange", "ops.Transformer.ln2", "x.reshape().permute.reshape().permute.reshape", "out.permute().view.permute().view.permute"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ",", "mask", "=", "None", ")", ":", "\n", "        ", "sz", "=", "x", ".", "shape", "\n", "if", "len", "(", "sz", ")", "==", "4", ":", "\n", "            ", "x", "=", "x", ".", "reshape", "(", "sz", "[", "0", "]", ",", "sz", "[", "1", "]", ",", "-", "1", ")", ".", "permute", "(", "0", ",", "2", ",", "1", ")", "\n", "\n", "", "assert", "x", ".", "dim", "(", ")", "==", "3", ",", "(", "x", ".", "shape", ",", "sz", ")", "\n", "x_in", "=", "x", "\n", "x", "=", "self", ".", "ln1", "(", "x", ")", "\n", "\n", "b", ",", "n", ",", "_", ",", "h", "=", "*", "x", ".", "shape", ",", "self", ".", "heads", "\n", "qkv", "=", "self", ".", "to_qkv", "(", "x", ")", ".", "chunk", "(", "3", ",", "dim", "=", "-", "1", ")", "\n", "\n", "q", ",", "k", ",", "v", "=", "map", "(", "lambda", "t", ":", "rearrange", "(", "t", ",", "'b n (h d) -> b h n d'", ",", "h", "=", "h", ")", ",", "qkv", ")", "\n", "dots", "=", "torch", ".", "einsum", "(", "'bhid,bhjd->bhij'", ",", "q", ",", "k", ")", "*", "self", ".", "scale", "\n", "if", "mask", "is", "not", "None", ":", "\n", "            ", "raise", "NotImplementedError", "(", "'should not be used for images'", ")", "\n", "\n", "", "attn", "=", "dots", ".", "softmax", "(", "dim", "=", "-", "1", ")", "\n", "\n", "out", "=", "torch", ".", "einsum", "(", "'bhij,bhjd->bhid'", ",", "attn", ",", "v", ")", "\n", "out", "=", "rearrange", "(", "out", ",", "'b h n d -> b n (h d)'", ")", "\n", "out", "=", "self", ".", "to_out", "(", "out", ")", "\n", "# end of MSA", "\n", "out", "=", "out", "+", "x_in", "# residual", "\n", "\n", "out", "=", "self", ".", "ff", "(", "self", ".", "ln2", "(", "out", ")", ")", "+", "out", "# mlp + residual", "\n", "\n", "if", "len", "(", "sz", ")", "==", "4", ":", "\n", "            ", "out", "=", "out", ".", "permute", "(", "0", ",", "2", ",", "1", ")", ".", "view", "(", "sz", ")", "# B,C,H,W", "\n", "if", "self", ".", "stride", ">", "1", ":", "\n", "                ", "out", "=", "out", "[", ":", ",", ":", ",", ":", ":", "self", ".", "stride", ",", ":", ":", "self", ".", "stride", "]", "\n", "\n", "", "", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.ops.ReLUConvBN.__init__": [[459, 474], ["torch.Module.__init__", "torch.Sequential", "torch.Sequential", "torch.ReLU", "torch.ReLU", "ops.get_norm_layer", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_ppuda.examples.custom_cnn.CNN.__init__", "home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.ops.get_norm_layer"], ["    ", "def", "__init__", "(", "self", ",", "C_in", ",", "C_out", ",", "ks", "=", "1", ",", "stride", "=", "1", ",", "padding", "=", "0", ",", "norm", "=", "'bn'", ",", "double", "=", "False", ")", ":", "\n", "        ", "super", "(", "ReLUConvBN", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "stride", "=", "stride", "\n", "if", "double", ":", "\n", "            ", "conv", "=", "[", "\n", "nn", ".", "Conv2d", "(", "C_in", ",", "C_in", ",", "(", "1", ",", "ks", ")", ",", "stride", "=", "(", "1", ",", "stride", ")", ",", "\n", "padding", "=", "(", "0", ",", "padding", ")", ",", "bias", "=", "False", ")", ",", "\n", "nn", ".", "Conv2d", "(", "C_in", ",", "C_out", ",", "(", "ks", ",", "1", ")", ",", "stride", "=", "(", "stride", ",", "1", ")", ",", "\n", "padding", "=", "(", "padding", ",", "0", ")", ",", "bias", "=", "False", ")", "]", "\n", "", "else", ":", "\n", "            ", "conv", "=", "[", "nn", ".", "Conv2d", "(", "C_in", ",", "C_out", ",", "ks", ",", "stride", "=", "stride", ",", "padding", "=", "padding", ",", "bias", "=", "False", ")", "]", "\n", "", "self", ".", "op", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "ReLU", "(", "inplace", "=", "False", ")", ",", "\n", "*", "conv", ",", "\n", "get_norm_layer", "(", "norm", ",", "C_out", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.ops.ReLUConvBN.forward": [[475, 477], ["ops.ReLUConvBN.op"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "return", "self", ".", "op", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.ops.DilConv.__init__": [[481, 490], ["torch.Module.__init__", "torch.Sequential", "torch.Sequential", "torch.ReLU", "torch.ReLU", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "ops.get_norm_layer"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_ppuda.examples.custom_cnn.CNN.__init__", "home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.ops.get_norm_layer"], ["    ", "def", "__init__", "(", "self", ",", "C_in", ",", "C_out", ",", "ks", ",", "stride", ",", "padding", ",", "dilation", ",", "norm", "=", "'bn'", ")", ":", "\n", "        ", "super", "(", "DilConv", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "stride", "=", "stride", "\n", "\n", "self", ".", "op", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "ReLU", "(", "inplace", "=", "False", ")", ",", "\n", "nn", ".", "Conv2d", "(", "C_in", ",", "C_in", ",", "kernel_size", "=", "ks", ",", "stride", "=", "stride", ",", "padding", "=", "padding", ",", "dilation", "=", "dilation", ",", "groups", "=", "C_in", ",", "bias", "=", "False", ")", ",", "\n", "nn", ".", "Conv2d", "(", "C_in", ",", "C_out", ",", "kernel_size", "=", "1", ",", "padding", "=", "0", ",", "bias", "=", "False", ")", ",", "\n", "get_norm_layer", "(", "norm", ",", "C_out", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.ops.DilConv.forward": [[492, 494], ["ops.DilConv.op"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "return", "self", ".", "op", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.ops.SepConv.__init__": [[498, 511], ["torch.Module.__init__", "torch.Sequential", "torch.Sequential", "torch.ReLU", "torch.ReLU", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "ops.get_norm_layer", "torch.ReLU", "torch.ReLU", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "ops.get_norm_layer"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_ppuda.examples.custom_cnn.CNN.__init__", "home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.ops.get_norm_layer", "home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.ops.get_norm_layer"], ["    ", "def", "__init__", "(", "self", ",", "C_in", ",", "C_out", ",", "ks", ",", "stride", ",", "padding", ",", "norm", "=", "'bn'", ")", ":", "\n", "        ", "super", "(", "SepConv", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "stride", "=", "stride", "\n", "\n", "self", ".", "op", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "ReLU", "(", "inplace", "=", "False", ")", ",", "\n", "nn", ".", "Conv2d", "(", "C_in", ",", "C_in", ",", "kernel_size", "=", "ks", ",", "stride", "=", "stride", ",", "padding", "=", "padding", ",", "groups", "=", "C_in", ",", "bias", "=", "False", ")", ",", "\n", "nn", ".", "Conv2d", "(", "C_in", ",", "C_in", ",", "kernel_size", "=", "1", ",", "padding", "=", "0", ",", "bias", "=", "False", ")", ",", "\n", "get_norm_layer", "(", "norm", ",", "C_in", ")", ",", "\n", "nn", ".", "ReLU", "(", "inplace", "=", "False", ")", ",", "\n", "nn", ".", "Conv2d", "(", "C_in", ",", "C_in", ",", "kernel_size", "=", "ks", ",", "stride", "=", "1", ",", "padding", "=", "padding", ",", "groups", "=", "C_in", ",", "bias", "=", "False", ")", ",", "\n", "nn", ".", "Conv2d", "(", "C_in", ",", "C_out", ",", "kernel_size", "=", "1", ",", "padding", "=", "0", ",", "bias", "=", "False", ")", ",", "\n", "get_norm_layer", "(", "norm", ",", "C_out", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.ops.SepConv.forward": [[513, 515], ["ops.SepConv.op"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "return", "self", ".", "op", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.ops.Stride.__init__": [[518, 521], ["torch.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_ppuda.examples.custom_cnn.CNN.__init__"], ["    ", "def", "__init__", "(", "self", ",", "stride", ")", ":", "\n", "        ", "super", "(", "Stride", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "stride", "=", "stride", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.ops.Stride.forward": [[522, 526], ["None"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "if", "self", ".", "stride", "==", "1", ":", "\n", "            ", "return", "x", "\n", "", "return", "x", "[", ":", ",", ":", ",", ":", ":", "self", ".", "stride", ",", ":", ":", "self", ".", "stride", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.ops.Zero.__init__": [[529, 532], ["torch.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_ppuda.examples.custom_cnn.CNN.__init__"], ["    ", "def", "__init__", "(", "self", ",", "stride", ")", ":", "\n", "        ", "super", "(", "Zero", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "stride", "=", "stride", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.ops.Zero.forward": [[533, 537], ["x[].mul", "x.mul"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "if", "self", ".", "stride", "==", "1", ":", "\n", "            ", "return", "x", ".", "mul", "(", "0.", ")", "\n", "", "return", "x", "[", ":", ",", ":", ",", ":", ":", "self", ".", "stride", ",", ":", ":", "self", ".", "stride", "]", ".", "mul", "(", "0.", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.ops.FactorizedReduce.__init__": [[540, 548], ["torch.Module.__init__", "torch.ReLU", "torch.ReLU", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "ops.get_norm_layer"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_ppuda.examples.custom_cnn.CNN.__init__", "home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.ops.get_norm_layer"], ["    ", "def", "__init__", "(", "self", ",", "C_in", ",", "C_out", ",", "norm", "=", "'bn'", ",", "stride", "=", "2", ")", ":", "\n", "        ", "super", "(", "FactorizedReduce", ",", "self", ")", ".", "__init__", "(", ")", "\n", "assert", "C_out", "%", "2", "==", "0", "\n", "self", ".", "stride", "=", "stride", "\n", "self", ".", "relu", "=", "nn", ".", "ReLU", "(", "inplace", "=", "False", ")", "\n", "self", ".", "conv_1", "=", "nn", ".", "Conv2d", "(", "C_in", ",", "C_out", "//", "2", ",", "1", ",", "stride", "=", "stride", ",", "padding", "=", "0", ",", "bias", "=", "False", ")", "\n", "self", ".", "conv_2", "=", "nn", ".", "Conv2d", "(", "C_in", ",", "C_out", "//", "2", ",", "1", ",", "stride", "=", "stride", ",", "padding", "=", "0", ",", "bias", "=", "False", ")", "\n", "self", ".", "bn", "=", "get_norm_layer", "(", "norm", ",", "C_out", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.ops.FactorizedReduce.forward": [[549, 554], ["ops.FactorizedReduce.relu", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "ops.FactorizedReduce.bn", "ops.FactorizedReduce.conv_1", "ops.FactorizedReduce.conv_2"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "relu", "(", "x", ")", "\n", "out", "=", "torch", ".", "cat", "(", "[", "self", ".", "conv_1", "(", "x", ")", ",", "self", ".", "conv_2", "(", "x", "[", ":", ",", ":", ",", "1", ":", ",", "1", ":", "]", "if", "self", ".", "stride", ">", "1", "else", "x", ")", "]", ",", "dim", "=", "1", ")", "\n", "out", "=", "self", ".", "bn", "(", "out", ")", "\n", "return", "out", "\n", "", "", ""]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.ops.parse_op_ks": [[21, 45], ["op.split", "s.split", "len", "range", "len", "max", "int"], "function", ["home.repos.pwc.inspect_result.facebookresearch_ppuda.detector.utils.SmoothedValue.max"], ["def", "parse_op_ks", "(", "op", ")", ":", "\n", "    ", "ks", "=", "0", "\n", "op_name", "=", "''", "\n", "is_double", "=", "False", "\n", "for", "s", "in", "op", ".", "split", "(", "'_'", ")", ":", "\n", "        ", "ks_str", "=", "s", ".", "split", "(", "'x'", ")", "\n", "valid_ks_str", "=", "False", "\n", "if", "len", "(", "ks_str", ")", "==", "2", ":", "\n", "            ", "if", "ks", ">", "0", ":", "\n", "                ", "is_double", "=", "True", "\n", "", "for", "i", "in", "range", "(", "2", ")", ":", "\n", "                ", "try", ":", "\n", "                    ", "ks", "=", "max", "(", "ks", ",", "int", "(", "ks_str", "[", "i", "]", ")", ")", "\n", "valid_ks_str", "=", "True", "\n", "", "except", ":", "\n", "                    ", "continue", "\n", "\n", "", "", "", "if", "not", "valid_ks_str", ":", "\n", "            ", "if", "len", "(", "op_name", ")", ">", "0", ":", "\n", "                ", "op_name", "+=", "'_'", "\n", "", "op_name", "+=", "s", "\n", "", "", "if", "is_double", ":", "\n", "        ", "op_name", "+=", "'2'", "\n", "", "return", "op_name", ",", "ks", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.ops.get_norm_layer": [[50, 59], ["torch.Identity", "norm.startswith", "torch.BatchNorm2d", "NotImplementedError", "norm.find"], "function", ["None"], ["def", "get_norm_layer", "(", "norm", ",", "C", ")", ":", "\n", "    ", "if", "norm", "in", "[", "None", ",", "''", ",", "'none'", "]", ":", "\n", "        ", "norm_layer", "=", "nn", ".", "Identity", "(", ")", "\n", "", "elif", "norm", ".", "startswith", "(", "'bn'", ")", ":", "\n", "        ", "norm_layer", "=", "nn", ".", "BatchNorm2d", "(", "C", ",", "\n", "track_running_stats", "=", "norm", ".", "find", "(", "'track'", ")", ">=", "0", ")", "\n", "", "else", ":", "\n", "        ", "raise", "NotImplementedError", "(", "norm", ")", "\n", "", "return", "norm_layer", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.net.Cell.__init__": [[22, 49], ["torch.Module.__init__", "net.Cell._compile", "sum", "ops.ReLUConvBN", "torch.Identity", "torch.Identity", "torch.Identity", "zip", "zip", "ops.FactorizedReduce", "ops.ReLUConvBN", "ops.Stride", "torch.Identity", "torch.Identity", "torch.Identity"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_ppuda.examples.custom_cnn.CNN.__init__", "home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.net.Cell._compile"], ["    ", "def", "__init__", "(", "self", ",", "genotype", ",", "C_prev_prev", ",", "C_prev", ",", "C_in", ",", "C_out", ",", "reduction", ",", "reduction_prev", ",", "\n", "norm", "=", "'bn'", ",", "preproc", "=", "True", ",", "is_vit", "=", "False", ")", ":", "\n", "        ", "super", "(", "Cell", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "_is_vit", "=", "is_vit", "\n", "self", ".", "_has_none", "=", "sum", "(", "[", "n", "[", "0", "]", "==", "'none'", "for", "n", "in", "genotype", ".", "normal", "+", "genotype", ".", "reduce", "]", ")", ">", "0", "\n", "self", ".", "genotype", "=", "genotype", "\n", "\n", "if", "preproc", ":", "\n", "            ", "if", "reduction_prev", "and", "not", "is_vit", ":", "\n", "                ", "self", ".", "preprocess0", "=", "FactorizedReduce", "(", "C_prev_prev", ",", "C_out", ",", "norm", "=", "norm", ")", "\n", "", "else", ":", "\n", "                ", "self", ".", "preprocess0", "=", "ReLUConvBN", "(", "C_prev_prev", ",", "C_out", ",", "norm", "=", "norm", ")", "\n", "", "self", ".", "preprocess1", "=", "ReLUConvBN", "(", "C_prev", ",", "C_out", ",", "norm", "=", "norm", ")", "\n", "", "else", ":", "\n", "            ", "if", "reduction_prev", "and", "not", "is_vit", ":", "\n", "                ", "self", ".", "preprocess0", "=", "Stride", "(", "stride", "=", "2", ")", "\n", "", "else", ":", "\n", "                ", "self", ".", "preprocess0", "=", "nn", ".", "Identity", "(", ")", "\n", "", "self", ".", "preprocess1", "=", "nn", ".", "Identity", "(", ")", "\n", "\n", "", "if", "reduction", ":", "\n", "            ", "op_names", ",", "indices", "=", "zip", "(", "*", "genotype", ".", "reduce", ")", "\n", "concat", "=", "genotype", ".", "reduce_concat", "\n", "", "else", ":", "\n", "            ", "op_names", ",", "indices", "=", "zip", "(", "*", "genotype", ".", "normal", ")", "\n", "concat", "=", "genotype", ".", "normal_concat", "\n", "", "self", ".", "_compile", "(", "C_in", ",", "C_out", ",", "op_names", ",", "indices", ",", "concat", ",", "reduction", ",", "norm", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.net.Cell._compile": [[51, 64], ["len", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "enumerate", "len", "len", "len", "zip", "ops.parse_op_ks", "net.Cell._ops.append"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.ops.parse_op_ks", "home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.graph.GraphBatch.append"], ["", "def", "_compile", "(", "self", ",", "C_in", ",", "C_out", ",", "op_names", ",", "indices", ",", "concat", ",", "reduction", ",", "norm", ")", ":", "\n", "        ", "assert", "len", "(", "op_names", ")", "==", "len", "(", "indices", ")", "\n", "self", ".", "_steps", "=", "len", "(", "op_names", ")", "//", "2", "\n", "self", ".", "_concat", "=", "concat", "\n", "self", ".", "multiplier", "=", "len", "(", "concat", ")", "\n", "\n", "self", ".", "_ops", "=", "nn", ".", "ModuleList", "(", ")", "\n", "for", "i", ",", "(", "name", ",", "index", ")", "in", "enumerate", "(", "zip", "(", "op_names", ",", "indices", ")", ")", ":", "\n", "            ", "stride", "=", "2", "if", "(", "reduction", "and", "index", "<", "2", "and", "not", "self", ".", "_is_vit", ")", "else", "1", "\n", "name", ",", "ks", "=", "parse_op_ks", "(", "name", ")", "\n", "self", ".", "_ops", ".", "append", "(", "OPS", "[", "name", "]", "(", "C_in", "if", "index", "<=", "1", "else", "C_out", ",", "C_out", ",", "ks", ",", "stride", ",", "norm", ")", ")", "\n", "\n", "", "self", ".", "_indices", "=", "indices", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.net.Cell.forward": [[66, 115], ["range", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "net.Cell.preprocess0", "net.Cell.preprocess1", "states.append", "sum", "net._is_none", "net._is_none", "op1", "op2", "isinstance", "net._is_none", "utils.drop_path", "isinstance", "net._is_none", "utils.drop_path", "isinstance", "isinstance", "print"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.graph.GraphBatch.append", "home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.net._is_none", "home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.net._is_none", "home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.net._is_none", "home.repos.pwc.inspect_result.facebookresearch_ppuda.utils.darts_utils.drop_path", "home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.net._is_none", "home.repos.pwc.inspect_result.facebookresearch_ppuda.utils.darts_utils.drop_path"], ["", "def", "forward", "(", "self", ",", "s0", ",", "s1", ",", "drop_path_prob", "=", "0", ")", ":", "\n", "\n", "        ", "s0", "=", "None", "if", "(", "s0", "is", "None", "or", "_is_none", "(", "self", ".", "preprocess0", ")", ")", "else", "self", ".", "preprocess0", "(", "s0", ")", "\n", "s1", "=", "None", "if", "(", "s1", "is", "None", "or", "_is_none", "(", "self", ".", "preprocess1", ")", ")", "else", "self", ".", "preprocess1", "(", "s1", ")", "\n", "\n", "states", "=", "[", "s0", ",", "s1", "]", "\n", "for", "i", "in", "range", "(", "self", ".", "_steps", ")", ":", "\n", "            ", "h1", "=", "states", "[", "self", ".", "_indices", "[", "2", "*", "i", "]", "]", "\n", "h2", "=", "states", "[", "self", ".", "_indices", "[", "2", "*", "i", "+", "1", "]", "]", "\n", "op1", "=", "self", ".", "_ops", "[", "2", "*", "i", "]", "\n", "op2", "=", "self", ".", "_ops", "[", "2", "*", "i", "+", "1", "]", "\n", "s", "=", "None", "\n", "\n", "if", "not", "(", "isinstance", "(", "op1", ",", "Zero", ")", "or", "_is_none", "(", "op1", ")", "or", "h1", "is", "None", ")", ":", "\n", "                ", "h1", "=", "op1", "(", "h1", ")", "\n", "if", "self", ".", "training", "and", "drop_path_prob", ">", "0", "and", "not", "isinstance", "(", "op1", ",", "nn", ".", "Identity", ")", ":", "\n", "                    ", "h1", "=", "drop_path", "(", "h1", ",", "drop_path_prob", ")", "\n", "", "s", "=", "h1", "\n", "\n", "", "if", "not", "(", "isinstance", "(", "op2", ",", "Zero", ")", "or", "_is_none", "(", "op2", ")", "or", "h2", "is", "None", ")", ":", "\n", "                ", "h2", "=", "op2", "(", "h2", ")", "\n", "if", "self", ".", "training", "and", "drop_path_prob", ">", "0", "and", "not", "isinstance", "(", "op2", ",", "nn", ".", "Identity", ")", ":", "\n", "                    ", "h2", "=", "drop_path", "(", "h2", ",", "drop_path_prob", ")", "\n", "", "try", ":", "\n", "                    ", "s", "=", "h2", "if", "s", "is", "None", "else", "(", "h1", "+", "h2", ")", "\n", "", "except", ":", "\n", "                    ", "print", "(", "h1", ".", "shape", ",", "h2", ".", "shape", ",", "self", ".", "genotype", ")", "\n", "raise", "\n", "\n", "", "", "states", ".", "append", "(", "s", ")", "\n", "\n", "\n", "", "if", "sum", "(", "[", "states", "[", "i", "]", "is", "None", "for", "i", "in", "self", ".", "_concat", "]", ")", ">", "0", ":", "\n", "# Replace None states with Zeros to match feature dimensionalities and enable forward pass", "\n", "            ", "assert", "self", ".", "_has_none", ",", "self", ".", "genotype", "\n", "s_dummy", "=", "None", "\n", "for", "i", "in", "self", ".", "_concat", ":", "\n", "                ", "if", "states", "[", "i", "]", "is", "not", "None", ":", "\n", "                    ", "s_dummy", "=", "states", "[", "i", "]", "*", "0", "\n", "break", "\n", "\n", "", "", "if", "s_dummy", "is", "None", ":", "\n", "                ", "return", "None", "\n", "", "else", ":", "\n", "                ", "for", "i", "in", "self", ".", "_concat", ":", "\n", "                    ", "if", "states", "[", "i", "]", "is", "None", ":", "\n", "                        ", "states", "[", "i", "]", "=", "s_dummy", "\n", "\n", "", "", "", "", "return", "torch", ".", "cat", "(", "[", "states", "[", "i", "]", "for", "i", "in", "self", ".", "_concat", "]", ",", "dim", "=", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.net.AuxiliaryHeadCIFAR.__init__": [[119, 133], ["torch.Module.__init__", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Linear", "torch.Linear", "torch.Linear", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.AvgPool2d", "torch.AvgPool2d", "torch.AvgPool2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "ops.get_norm_layer", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "ops.get_norm_layer", "torch.ReLU", "torch.ReLU", "torch.ReLU", "min"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_ppuda.examples.custom_cnn.CNN.__init__", "home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.ops.get_norm_layer", "home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.ops.get_norm_layer"], ["    ", "def", "__init__", "(", "self", ",", "C", ",", "num_classes", ",", "norm", "=", "'bn'", ",", "pool_sz", "=", "5", ")", ":", "\n", "        ", "\"\"\"assuming input size 8x8\"\"\"", "\n", "super", "(", "AuxiliaryHeadCIFAR", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "features", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", ",", "\n", "nn", ".", "AvgPool2d", "(", "pool_sz", ",", "stride", "=", "min", "(", "pool_sz", ",", "3", ")", ",", "padding", "=", "0", ",", "count_include_pad", "=", "False", ")", ",", "# image size = 2 x 2", "\n", "nn", ".", "Conv2d", "(", "C", ",", "128", ",", "1", ",", "bias", "=", "False", ")", ",", "\n", "get_norm_layer", "(", "norm", ",", "128", ")", ",", "\n", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", ",", "\n", "nn", ".", "Conv2d", "(", "128", ",", "768", ",", "2", ",", "bias", "=", "False", ")", ",", "\n", "get_norm_layer", "(", "norm", ",", "768", ")", ",", "\n", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", "\n", ")", "\n", "self", ".", "classifier", "=", "nn", ".", "Linear", "(", "768", ",", "num_classes", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.net.AuxiliaryHeadCIFAR.forward": [[134, 139], ["net.AuxiliaryHeadCIFAR.features", "net.AuxiliaryHeadCIFAR.classifier", "net.AuxiliaryHeadCIFAR.reshape", "net.AuxiliaryHeadCIFAR.size"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "assert", "self", ".", "training", ",", "'this module is assumed to be used only for training'", "\n", "x", "=", "self", ".", "features", "(", "x", ")", "\n", "x", "=", "self", ".", "classifier", "(", "x", ".", "reshape", "(", "x", ".", "size", "(", "0", ")", ",", "-", "1", ")", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.net.AuxiliaryHeadImageNet.__init__": [[143, 159], ["torch.Module.__init__", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Linear", "torch.Linear", "torch.Linear", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.AvgPool2d", "torch.AvgPool2d", "torch.AvgPool2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "ops.get_norm_layer", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.ReLU", "torch.ReLU", "torch.ReLU"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_ppuda.examples.custom_cnn.CNN.__init__", "home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.ops.get_norm_layer"], ["    ", "def", "__init__", "(", "self", ",", "C", ",", "num_classes", ",", "norm", "=", "'bn'", ")", ":", "\n", "        ", "\"\"\"assuming input size 14x14\"\"\"", "\n", "super", "(", "AuxiliaryHeadImageNet", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "features", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", ",", "\n", "nn", ".", "AvgPool2d", "(", "5", ",", "stride", "=", "2", ",", "padding", "=", "0", ",", "count_include_pad", "=", "False", ")", ",", "\n", "nn", ".", "Conv2d", "(", "C", ",", "128", ",", "1", ",", "bias", "=", "False", ")", ",", "\n", "get_norm_layer", "(", "norm", ",", "128", ")", ",", "\n", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", ",", "\n", "nn", ".", "Conv2d", "(", "128", ",", "768", ",", "2", ",", "bias", "=", "False", ")", ",", "\n", "# NOTE: This batchnorm was omitted in my earlier implementation due to a typo.", "\n", "# Commenting it out for consistency with the experiments in the paper.", "\n", "# get_norm_layer(norm, 768),", "\n", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", "\n", ")", "\n", "self", ".", "classifier", "=", "nn", ".", "Linear", "(", "768", ",", "num_classes", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.net.AuxiliaryHeadImageNet.forward": [[160, 165], ["net.AuxiliaryHeadImageNet.features", "net.AuxiliaryHeadImageNet.classifier", "net.AuxiliaryHeadImageNet.reshape", "net.AuxiliaryHeadImageNet.size"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "assert", "self", ".", "training", ",", "'this module is assumed to be used only for training'", "\n", "x", "=", "self", ".", "features", "(", "x", ")", "\n", "x", "=", "self", ".", "classifier", "(", "x", ".", "reshape", "(", "x", ".", "size", "(", "0", ")", ",", "-", "1", ")", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.net.Network.__init__": [[169, 298], ["torch.Module.__init__", "len", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "range", "range", "torch.Sequential", "torch.Sequential", "torch.Sequential", "ops.PosEnc", "is_reduction", "is_reduction", "net.Cell", "net.Network.cells.append", "torch.AdaptiveAvgPool2d", "torch.AdaptiveAvgPool2d", "torch.AdaptiveAvgPool2d", "torch.Linear", "torch.Linear", "torch.Linear", "fc.append", "fc.append", "fc.append", "net.Network.parameters", "sum", "int", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.Dropout", "torch.Dropout", "torch.Dropout", "torch.Linear", "torch.Linear", "torch.Linear", "p.data.bool", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "ops.get_norm_layer", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "ops.get_norm_layer", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "ops.get_norm_layer", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "ops.get_norm_layer", "net.AuxiliaryHeadImageNet", "net.AuxiliaryHeadCIFAR", "torch.MaxPool2d", "torch.MaxPool2d", "torch.MaxPool2d", "torch.Identity", "torch.Identity", "torch.Identity"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_ppuda.examples.custom_cnn.CNN.__init__", "home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.graph.GraphBatch.append", "home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.graph.GraphBatch.append", "home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.graph.GraphBatch.append", "home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.graph.GraphBatch.append", "home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.ops.get_norm_layer", "home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.ops.get_norm_layer", "home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.ops.get_norm_layer", "home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.ops.get_norm_layer"], ["    ", "def", "__init__", "(", "self", ",", "\n", "C", ",", "\n", "num_classes", ",", "\n", "genotype", ",", "\n", "n_cells", ",", "\n", "ks", "=", "3", ",", "\n", "is_imagenet_input", "=", "True", ",", "\n", "stem_pool", "=", "False", ",", "\n", "stem_type", "=", "0", ",", "\n", "is_vit", "=", "None", ",", "\n", "norm", "=", "'bn-track'", ",", "\n", "preproc", "=", "True", ",", "\n", "C_mult", "=", "2", ",", "\n", "fc_layers", "=", "0", ",", "\n", "fc_dim", "=", "0", ",", "\n", "glob_avg", "=", "True", ",", "\n", "auxiliary", "=", "False", ",", "\n", "compress_params", "=", "False", "\n", ")", ":", "\n", "        ", "super", "(", "Network", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "genotype", "=", "genotype", "\n", "self", ".", "_auxiliary", "=", "auxiliary", "\n", "self", ".", "drop_path_prob", "=", "0", "\n", "self", ".", "expected_input_sz", "=", "224", "if", "is_imagenet_input", "else", "32", "\n", "\n", "self", ".", "_is_vit", "=", "sum", "(", "[", "n", "[", "0", "]", "==", "'msa'", "for", "n", "in", "genotype", ".", "normal", "+", "genotype", ".", "reduce", "]", ")", ">", "0", "if", "is_vit", "is", "None", "else", "is_vit", "\n", "\n", "steps", "=", "len", "(", "genotype", ".", "normal_concat", ")", "# number of inputs to the concatenation operation", "\n", "if", "steps", ">", "1", "or", "C_mult", ">", "1", ":", "\n", "            ", "assert", "preproc", ",", "'preprocessing layers must be used in this case'", "\n", "\n", "", "self", ".", "_stem_type", "=", "stem_type", "\n", "assert", "stem_type", "in", "[", "0", ",", "1", "]", ",", "(", "'either 0 (simple) or 1 (imagenet-style) stem must be chosen'", ",", "stem_type", ")", "\n", "\n", "C_prev_prev", "=", "C_prev", "=", "C_curr", "=", "C", "\n", "\n", "# Define the stem", "\n", "if", "self", ".", "_is_vit", ":", "\n", "# Visual Transformer stem", "\n", "            ", "self", ".", "stem0", "=", "OPS", "[", "'conv_stride'", "]", "(", "3", ",", "C", ",", "16", "if", "is_imagenet_input", "else", "3", ",", "None", ",", "norm", ")", "\n", "self", ".", "pos_enc", "=", "PosEnc", "(", "C", ",", "14", "if", "is_imagenet_input", "else", "11", ")", "\n", "\n", "", "elif", "stem_type", "==", "0", ":", "\n", "# Simple stem", "\n", "            ", "C_stem", "=", "int", "(", "C", "*", "(", "3", "if", "(", "preproc", "and", "not", "is_imagenet_input", ")", "else", "1", ")", ")", "\n", "\n", "self", ".", "stem", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Conv2d", "(", "3", ",", "C_stem", ",", "ks", ",", "stride", "=", "4", "if", "is_imagenet_input", "else", "1", ",", "padding", "=", "ks", "//", "2", ",", "bias", "=", "False", ")", ",", "\n", "get_norm_layer", "(", "norm", ",", "C_stem", ")", ",", "\n", "nn", ".", "MaxPool2d", "(", "3", ",", "stride", "=", "2", ",", "padding", "=", "1", ",", "ceil_mode", "=", "False", ")", "if", "stem_pool", "else", "nn", ".", "Identity", "(", ")", ",", "\n", ")", "\n", "C_prev_prev", "=", "C_prev", "=", "C_stem", "\n", "\n", "", "else", ":", "\n", "# ImageNet-style stem", "\n", "            ", "self", ".", "stem0", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Conv2d", "(", "3", ",", "C", "//", "2", ",", "kernel_size", "=", "ks", ",", "stride", "=", "2", "if", "is_imagenet_input", "else", "1", ",", "padding", "=", "ks", "//", "2", ",", "bias", "=", "False", ")", ",", "\n", "get_norm_layer", "(", "norm", ",", "C", "//", "2", ")", ",", "\n", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", ",", "\n", "nn", ".", "Conv2d", "(", "C", "//", "2", ",", "C", ",", "kernel_size", "=", "3", ",", "stride", "=", "2", "if", "is_imagenet_input", "else", "1", ",", "padding", "=", "1", ",", "bias", "=", "False", ")", ",", "\n", "get_norm_layer", "(", "norm", ",", "C", ")", "\n", ")", "\n", "\n", "self", ".", "stem1", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", ",", "\n", "nn", ".", "Conv2d", "(", "C", ",", "C", ",", "3", ",", "stride", "=", "2", ",", "padding", "=", "1", ",", "bias", "=", "False", ")", ",", "\n", "get_norm_layer", "(", "norm", ",", "C", ")", "\n", ")", "\n", "\n", "", "self", ".", "_n_cells", "=", "n_cells", "\n", "self", ".", "cells", "=", "nn", ".", "ModuleList", "(", ")", "\n", "\n", "is_reduction", "=", "lambda", "cell_ind", ":", "cell_ind", "in", "[", "n_cells", "//", "3", ",", "2", "*", "n_cells", "//", "3", "]", "and", "cell_ind", ">", "0", "\n", "self", ".", "_auxiliary_cell_ind", "=", "2", "*", "n_cells", "//", "3", "\n", "\n", "reduction_prev", "=", "stem_type", "==", "1", "\n", "for", "cell_ind", "in", "range", "(", "n_cells", ")", ":", "\n", "            ", "if", "is_reduction", "(", "cell_ind", ")", ":", "\n", "                ", "C_curr", "*=", "C_mult", "\n", "reduction", "=", "True", "\n", "", "else", ":", "\n", "                ", "reduction", "=", "False", "\n", "\n", "", "reduction_next", "=", "is_reduction", "(", "cell_ind", "+", "1", ")", "\n", "\n", "cell", "=", "Cell", "(", "genotype", ",", "\n", "C_prev_prev", ",", "\n", "C_prev", ",", "\n", "C_in", "=", "C_curr", "if", "preproc", "else", "C_prev", ",", "\n", "C_out", "=", "C_curr", "*", "(", "C_mult", "if", "reduction_next", "and", "steps", "==", "1", "and", "not", "preproc", "else", "1", ")", ",", "\n", "reduction", "=", "reduction", ",", "\n", "reduction_prev", "=", "reduction_prev", ",", "\n", "norm", "=", "norm", ",", "\n", "is_vit", "=", "self", ".", "_is_vit", ",", "\n", "preproc", "=", "preproc", ")", "\n", "self", ".", "cells", ".", "append", "(", "cell", ")", "\n", "\n", "reduction_prev", "=", "reduction", "\n", "C_prev_prev", ",", "C_prev", "=", "C_prev", ",", "cell", ".", "multiplier", "*", "C_curr", "\n", "\n", "if", "auxiliary", "and", "cell_ind", "==", "self", ".", "_auxiliary_cell_ind", ":", "\n", "                ", "if", "is_imagenet_input", ":", "\n", "                    ", "self", ".", "auxiliary_head", "=", "AuxiliaryHeadImageNet", "(", "C_prev", ",", "num_classes", ",", "norm", "=", "norm", ")", "\n", "", "else", ":", "\n", "                    ", "self", ".", "auxiliary_head", "=", "AuxiliaryHeadCIFAR", "(", "C_prev", ",", "num_classes", ",", "norm", "=", "norm", ",", "\n", "pool_sz", "=", "2", "if", "(", "stem_type", "==", "1", "or", "stem_pool", ")", "else", "5", ")", "\n", "\n", "", "", "", "self", ".", "_glob_avg", "=", "glob_avg", "\n", "if", "glob_avg", ":", "\n", "            ", "self", ".", "global_pooling", "=", "nn", ".", "AdaptiveAvgPool2d", "(", "1", ")", "\n", "", "else", ":", "\n", "            ", "if", "is_imagenet_input", ":", "\n", "                ", "s", "=", "7", "if", "(", "stem_type", "==", "1", "or", "stem_pool", ")", "else", "14", "\n", "", "else", ":", "\n", "                ", "s", "=", "4", "if", "(", "stem_type", "==", "1", "or", "stem_pool", ")", "else", "8", "\n", "", "C_prev", "*=", "s", "**", "2", "\n", "\n", "", "fc", "=", "[", "nn", ".", "Linear", "(", "C_prev", ",", "fc_dim", "if", "fc_layers", ">", "1", "else", "num_classes", ")", "]", "\n", "for", "i", "in", "range", "(", "fc_layers", "-", "1", ")", ":", "\n", "            ", "assert", "fc_dim", ">", "0", ",", "fc_dim", "\n", "fc", ".", "append", "(", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", ")", "\n", "fc", ".", "append", "(", "nn", ".", "Dropout", "(", "p", "=", "0.5", ",", "inplace", "=", "False", ")", ")", "\n", "fc", ".", "append", "(", "nn", ".", "Linear", "(", "in_features", "=", "fc_dim", ",", "out_features", "=", "fc_dim", "if", "i", "<", "fc_layers", "-", "2", "else", "num_classes", ")", ")", "\n", "", "self", ".", "classifier", "=", "nn", ".", "Sequential", "(", "*", "fc", ")", "\n", "\n", "if", "compress_params", ":", "\n", "            ", "for", "p", "in", "self", ".", "parameters", "(", ")", ":", "\n", "                ", "p", ".", "data", "=", "p", ".", "data", ".", "bool", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.net.Network.forward": [[300, 325], ["enumerate", "net.Network.classifier", "net.Network.stem0", "net.Network.pos_enc", "ValueError", "net.Network.global_pooling", "out.reshape", "net.Network.stem0", "net.Network.stem", "cell", "net.Network.auxiliary_head", "out.size", "net._is_none", "net.Network.stem1", "torch.adaptive_avg_pool2d", "torch.adaptive_avg_pool2d", "torch.adaptive_avg_pool2d"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.net._is_none"], ["", "", "", "def", "forward", "(", "self", ",", "input", ")", ":", "\n", "\n", "        ", "if", "self", ".", "_is_vit", ":", "\n", "            ", "s0", "=", "self", ".", "stem0", "(", "input", ")", "\n", "s0", "=", "s1", "=", "self", ".", "pos_enc", "(", "s0", ")", "\n", "", "else", ":", "\n", "            ", "if", "self", ".", "_stem_type", "==", "1", ":", "\n", "                ", "s0", "=", "self", ".", "stem0", "(", "input", ")", "\n", "s1", "=", "None", "if", "_is_none", "(", "self", ".", "stem1", ")", "else", "self", ".", "stem1", "(", "s0", ")", "\n", "", "else", ":", "\n", "                ", "s0", "=", "s1", "=", "self", ".", "stem", "(", "input", ")", "\n", "\n", "", "", "logits_aux", "=", "None", "\n", "for", "cell_ind", ",", "cell", "in", "enumerate", "(", "self", ".", "cells", ")", ":", "\n", "            ", "s0", ",", "s1", "=", "s1", ",", "cell", "(", "s0", ",", "s1", ",", "self", ".", "drop_path_prob", ")", "\n", "if", "self", ".", "_auxiliary", "and", "cell_ind", "==", "self", ".", "_auxiliary_cell_ind", "and", "self", ".", "training", ":", "\n", "                ", "logits_aux", "=", "self", ".", "auxiliary_head", "(", "F", ".", "adaptive_avg_pool2d", "(", "s1", ",", "8", ")", "\n", "if", "self", ".", "_is_vit", "and", "self", ".", "expected_input_sz", "==", "32", "\n", "else", "s1", ")", "\n", "", "", "if", "s1", "is", "None", ":", "\n", "            ", "raise", "ValueError", "(", "'the network has invalid configuration: the output is None'", ")", "\n", "", "out", "=", "self", ".", "global_pooling", "(", "s1", ")", "if", "self", ".", "_glob_avg", "else", "s1", "\n", "logits", "=", "self", ".", "classifier", "(", "out", ".", "reshape", "(", "out", ".", "size", "(", "0", ")", ",", "-", "1", ")", ")", "\n", "\n", "return", "logits", ",", "logits_aux", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.net._is_none": [[331, 336], ["mod.named_modules", "hasattr"], "function", ["None"], ["def", "_is_none", "(", "mod", ")", ":", "\n", "    ", "for", "n", ",", "p", "in", "mod", ".", "named_modules", "(", ")", ":", "\n", "        ", "if", "hasattr", "(", "p", ",", "'weight'", ")", "and", "p", ".", "weight", "is", "None", ":", "\n", "            ", "return", "True", "\n", "", "", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.net.get_cell_ind": [[338, 351], ["param_name.find", "len", "int", "param_name[].find", "param_name.startswith", "param_name.startswith", "param_name.startswith", "param_name.startswith"], "function", ["None"], ["", "def", "get_cell_ind", "(", "param_name", ",", "layers", "=", "1", ")", ":", "\n", "    ", "if", "param_name", ".", "find", "(", "'cells.'", ")", ">=", "0", ":", "\n", "        ", "pos1", "=", "len", "(", "'cells.'", ")", "\n", "pos2", "=", "pos1", "+", "param_name", "[", "pos1", ":", "]", ".", "find", "(", "'.'", ")", "\n", "cell_ind", "=", "int", "(", "param_name", "[", "pos1", ":", "pos2", "]", ")", "\n", "", "elif", "param_name", ".", "startswith", "(", "'classifier'", ")", "or", "param_name", ".", "startswith", "(", "'auxiliary'", ")", ":", "\n", "        ", "cell_ind", "=", "layers", "-", "1", "\n", "", "elif", "layers", "==", "1", "or", "param_name", ".", "startswith", "(", "'stem'", ")", "or", "param_name", ".", "startswith", "(", "'pos_enc'", ")", ":", "\n", "        ", "cell_ind", "=", "0", "\n", "", "else", ":", "\n", "        ", "cell_ind", "=", "None", "\n", "\n", "", "return", "cell_ind", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.net.named_layered_modules": [[353, 374], ["hasattr", "model.named_modules", "hasattr", "range", "hasattr", "hasattr", "module_name.startswith", "net.get_cell_ind", "layered_modules[].append", "layered_modules[].append", "module_name.find"], "function", ["home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.net.get_cell_ind", "home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.graph.GraphBatch.append", "home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.graph.GraphBatch.append"], ["", "def", "named_layered_modules", "(", "model", ")", ":", "\n", "    ", "if", "hasattr", "(", "model", ",", "'module'", ")", ":", "# in case of multigpu model", "\n", "        ", "model", "=", "model", ".", "module", "\n", "", "layers", "=", "model", ".", "_n_cells", "if", "hasattr", "(", "model", ",", "'_n_cells'", ")", "else", "1", "\n", "layered_modules", "=", "[", "[", "]", "for", "_", "in", "range", "(", "layers", ")", "]", "\n", "for", "module_name", ",", "m", "in", "model", ".", "named_modules", "(", ")", ":", "\n", "        ", "is_w", "=", "hasattr", "(", "m", ",", "'weight'", ")", "and", "m", ".", "weight", "is", "not", "None", "\n", "is_b", "=", "hasattr", "(", "m", ",", "'bias'", ")", "and", "m", ".", "bias", "is", "not", "None", "\n", "\n", "if", "is_w", "or", "is_b", ":", "\n", "            ", "if", "module_name", ".", "startswith", "(", "'module.'", ")", ":", "\n", "                ", "module_name", "=", "module_name", "[", "module_name", ".", "find", "(", "'.'", ")", "+", "1", ":", "]", "\n", "", "cell_ind", "=", "get_cell_ind", "(", "module_name", ",", "layers", ")", "\n", "if", "is_w", ":", "\n", "                ", "layered_modules", "[", "cell_ind", "]", ".", "append", "(", "\n", "{", "'param_name'", ":", "module_name", "+", "'.weight'", ",", "'module'", ":", "m", ",", "'is_w'", ":", "True", ",", "'sz'", ":", "m", ".", "weight", ".", "shape", "}", ")", "\n", "", "if", "is_b", ":", "\n", "                ", "layered_modules", "[", "cell_ind", "]", ".", "append", "(", "\n", "{", "'param_name'", ":", "module_name", "+", "'.bias'", ",", "'module'", ":", "m", ",", "'is_w'", ":", "False", ",", "'sz'", ":", "m", ".", "bias", ".", "shape", "}", ")", "\n", "\n", "", "", "", "return", "layered_modules", "\n", "", ""]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.loader.DeepNets1M.__init__": [[35, 100], ["super().__init__", "print", "print", "torch.arange", "torch.arange", "loader.DeepNets1M._get_predefined", "len", "torch.tensor", "os.path.join", "os.path.exists", "torch.tensor", "int", "torch.tensor", "loader.DeepNets1M.split.upper", "open", "len", "to_int_dict", "to_int_dict", "len", "len", "loader.DeepNets1M.nodes.min().item", "loader.DeepNets1M.nodes.max().item", "loader.DeepNets1M.nodes.float().mean().item", "loader.DeepNets1M.nodes.float().std().item", "enumerate", "int", "loader.DeepNets1M.h5_file.replace", "json.load", "len", "d.items", "loader.DeepNets1M.nodes.min", "loader.DeepNets1M.nodes.max", "loader.DeepNets1M.nodes.float().mean", "loader.DeepNets1M.nodes.float().std", "loader.DeepNets1M.nodes.float", "loader.DeepNets1M.nodes.float"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_ppuda.examples.custom_cnn.CNN.__init__", "home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.loader.DeepNets1M._get_predefined", "home.repos.pwc.inspect_result.facebookresearch_ppuda.ghn.nn.GHN.load", "home.repos.pwc.inspect_result.facebookresearch_ppuda.detector.utils.SmoothedValue.max"], ["def", "__init__", "(", "self", ",", "\n", "split", "=", "'train'", ",", "\n", "nets_dir", "=", "'./data'", ",", "\n", "virtual_edges", "=", "1", ",", "\n", "num_ch", "=", "(", "32", ",", "128", ")", ",", "\n", "fc_dim", "=", "(", "64", ",", "512", ")", ",", "\n", "num_nets", "=", "None", ",", "\n", "arch", "=", "None", ",", "\n", "large_images", "=", "False", ")", ":", "\n", "        ", "super", "(", "DeepNets1M", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "split", "=", "split", "\n", "assert", "self", ".", "split", "in", "[", "'train'", ",", "'val'", ",", "'test'", ",", "'search'", ",", "\n", "'wide'", ",", "'deep'", ",", "'dense'", ",", "'bnfree'", ",", "'predefined'", "]", ",", "(", "'invalid split'", ",", "self", ".", "split", ")", "\n", "self", ".", "is_train", "=", "self", ".", "split", "==", "'train'", "\n", "\n", "self", ".", "virtual_edges", "=", "virtual_edges", "\n", "assert", "self", ".", "virtual_edges", ">=", "1", ",", "virtual_edges", "\n", "\n", "if", "self", ".", "is_train", ":", "\n", "# During training we will randomly sample values from this range", "\n", "            ", "self", ".", "num_ch", "=", "torch", ".", "arange", "(", "num_ch", "[", "0", "]", ",", "num_ch", "[", "1", "]", "+", "1", ",", "16", ")", "\n", "self", ".", "fc_dim", "=", "torch", ".", "arange", "(", "fc_dim", "[", "0", "]", ",", "fc_dim", "[", "1", "]", "+", "1", ",", "64", ")", "\n", "\n", "", "self", ".", "large_images", "=", "large_images", "# this affects some network parameters", "\n", "\n", "# Load one of the splits", "\n", "print", "(", "'\\nloading %s nets...'", "%", "self", ".", "split", ".", "upper", "(", ")", ")", "\n", "\n", "if", "self", ".", "split", "==", "'predefined'", ":", "\n", "            ", "self", ".", "nets", "=", "self", ".", "_get_predefined", "(", ")", "\n", "n_all", "=", "len", "(", "self", ".", "nets", ")", "\n", "self", ".", "nodes", "=", "torch", ".", "tensor", "(", "[", "net", ".", "n_nodes", "for", "net", "in", "self", ".", "nets", "]", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "h5_data", "=", "None", "\n", "self", ".", "h5_file", "=", "os", ".", "path", ".", "join", "(", "nets_dir", ",", "'deepnets1m_%s.hdf5'", "%", "(", "split", "if", "split", "in", "[", "'train'", ",", "'search'", "]", "else", "'eval'", ")", ")", "\n", "\n", "self", ".", "primitives_dict", "=", "{", "op", ":", "i", "for", "i", ",", "op", "in", "enumerate", "(", "PRIMITIVES_DEEPNETS1M", ")", "}", "\n", "assert", "os", ".", "path", ".", "exists", "(", "self", ".", "h5_file", ")", ",", "(", "'%s not found'", "%", "self", ".", "h5_file", ")", "\n", "\n", "# Load meta data to convert dataset files to graphs later in the _init_graph function", "\n", "to_int_dict", "=", "lambda", "d", ":", "{", "int", "(", "k", ")", ":", "v", "for", "k", ",", "v", "in", "d", ".", "items", "(", ")", "}", "\n", "with", "open", "(", "self", ".", "h5_file", ".", "replace", "(", "'.hdf5'", ",", "'_meta.json'", ")", ",", "'r'", ")", "as", "f", ":", "\n", "                ", "meta", "=", "json", ".", "load", "(", "f", ")", "[", "split", "]", "\n", "n_all", "=", "len", "(", "meta", "[", "'nets'", "]", ")", "\n", "self", ".", "nets", "=", "meta", "[", "'nets'", "]", "[", ":", "n_all", "if", "num_nets", "is", "None", "else", "num_nets", "]", "\n", "self", ".", "primitives_ext", "=", "to_int_dict", "(", "meta", "[", "'meta'", "]", "[", "'primitives_ext'", "]", ")", "\n", "self", ".", "op_names_net", "=", "to_int_dict", "(", "meta", "[", "'meta'", "]", "[", "'unique_op_names'", "]", ")", "\n", "", "self", ".", "h5_idx", "=", "[", "arch", "]", "if", "arch", "is", "not", "None", "else", "None", "\n", "self", ".", "nodes", "=", "torch", ".", "tensor", "(", "[", "net", "[", "'num_nodes'", "]", "for", "net", "in", "self", ".", "nets", "]", ")", "\n", "\n", "", "if", "arch", "is", "not", "None", ":", "\n", "            ", "arch", "=", "int", "(", "arch", ")", "\n", "assert", "arch", ">=", "0", "and", "arch", "<", "len", "(", "self", ".", "nets", ")", ",", "'architecture with index={} is not available in the {} split with {} architectures in total'", ".", "format", "(", "arch", ",", "split", ",", "len", "(", "self", ".", "nets", ")", ")", "\n", "self", ".", "nets", "=", "[", "self", ".", "nets", "[", "arch", "]", "]", "\n", "self", ".", "nodes", "=", "torch", ".", "tensor", "(", "[", "self", ".", "nodes", "[", "arch", "]", "]", ")", "\n", "\n", "", "print", "(", "'loaded {}/{} nets with {}-{} nodes (mean\\u00B1std: {:.1f}\\u00B1{:.1f})'", ".", "\n", "format", "(", "len", "(", "self", ".", "nets", ")", ",", "n_all", ",", "\n", "self", ".", "nodes", ".", "min", "(", ")", ".", "item", "(", ")", ",", "\n", "self", ".", "nodes", ".", "max", "(", ")", ".", "item", "(", ")", ",", "\n", "self", ".", "nodes", ".", "float", "(", ")", ".", "mean", "(", ")", ".", "item", "(", ")", ",", "\n", "self", ".", "nodes", ".", "float", "(", ")", ".", "std", "(", ")", ".", "item", "(", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.loader.DeepNets1M.loader": [[102, 112], ["torch.utils.data.DataLoader.DeepNets1M", "torch.utils.data.DataLoader", "iter", "torch.utils.data.DataLoader.NetBatchSampler", "min"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "loader", "(", "meta_batch_size", "=", "1", ",", "**", "kwargs", ")", ":", "\n", "        ", "nets", "=", "DeepNets1M", "(", "**", "kwargs", ")", "\n", "loader", "=", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "nets", ",", "\n", "batch_sampler", "=", "NetBatchSampler", "(", "nets", ",", "meta_batch_size", ")", "if", "nets", ".", "is_train", "else", "None", ",", "\n", "batch_size", "=", "1", ",", "\n", "pin_memory", "=", "False", ",", "\n", "collate_fn", "=", "GraphBatch", ",", "\n", "num_workers", "=", "2", "if", "meta_batch_size", "<=", "1", "else", "min", "(", "8", ",", "meta_batch_size", ")", ")", "\n", "return", "iter", "(", "loader", ")", "if", "nets", ".", "is_train", "else", "loader", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.loader.DeepNets1M.__len__": [[114, 116], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "nets", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.loader.DeepNets1M.__getitem__": [[118, 165], ["loader.DeepNets1M._init_graph", "h5py.File", "deepnets1m.genotypes.from_dict", "utils.rand_choice", "utils.rand_choice.item", "utils.rand_choice.item", "sum", "loader.DeepNets1M.num_ch.min", "utils.rand_choice", "utils.rand_choice", "utils.rand_choice", "str", "str", "utils.rand_choice"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.loader.DeepNets1M._init_graph", "home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.genotypes.from_dict", "home.repos.pwc.inspect_result.facebookresearch_ppuda.utils.utils.rand_choice", "home.repos.pwc.inspect_result.facebookresearch_ppuda.utils.utils.rand_choice", "home.repos.pwc.inspect_result.facebookresearch_ppuda.utils.utils.rand_choice", "home.repos.pwc.inspect_result.facebookresearch_ppuda.utils.utils.rand_choice", "home.repos.pwc.inspect_result.facebookresearch_ppuda.utils.utils.rand_choice"], ["", "def", "__getitem__", "(", "self", ",", "idx", ")", ":", "\n", "\n", "        ", "if", "self", ".", "split", "==", "'predefined'", ":", "\n", "            ", "graph", "=", "self", ".", "nets", "[", "idx", "]", "\n", "", "else", ":", "\n", "\n", "            ", "if", "self", ".", "h5_data", "is", "None", ":", "# A separate fd is opened for each worker process", "\n", "                ", "self", ".", "h5_data", "=", "h5py", ".", "File", "(", "self", ".", "h5_file", ",", "mode", "=", "'r'", ")", "\n", "\n", "", "args", "=", "self", ".", "nets", "[", "idx", "]", "\n", "idx", "=", "self", ".", "h5_idx", "[", "idx", "]", "if", "self", ".", "h5_idx", "is", "not", "None", "else", "idx", "\n", "cell", ",", "n_cells", "=", "from_dict", "(", "args", "[", "'genotype'", "]", ")", ",", "args", "[", "'n_cells'", "]", "\n", "graph", "=", "self", ".", "_init_graph", "(", "self", ".", "h5_data", "[", "self", ".", "split", "]", "[", "str", "(", "idx", ")", "]", "[", "'adj'", "]", "[", "(", ")", "]", ",", "\n", "self", ".", "h5_data", "[", "self", ".", "split", "]", "[", "str", "(", "idx", ")", "]", "[", "'nodes'", "]", "[", "(", ")", "]", ",", "\n", "n_cells", ")", "\n", "\n", "if", "self", ".", "is_train", ":", "\n", "                ", "is_conv_dense", "=", "sum", "(", "[", "n", "[", "0", "]", "in", "[", "'conv_5x5'", ",", "'conv_7x7'", "]", "for", "n", "in", "\n", "cell", ".", "normal", "+", "cell", ".", "reduce", "]", ")", ">", "0", "\n", "num_params", "=", "args", "[", "'num_params'", "]", "[", "'imagenet'", "if", "self", ".", "large_images", "else", "'cifar10'", "]", "/", "10", "**", "6", "\n", "\n", "fc", "=", "rand_choice", "(", "self", ".", "fc_dim", ",", "4", ")", "# 64-256", "\n", "if", "num_params", ">", "0.8", "or", "not", "args", "[", "'glob_avg'", "]", "or", "is_conv_dense", "or", "n_cells", ">", "12", ":", "\n", "                    ", "C", "=", "self", ".", "num_ch", ".", "min", "(", ")", "\n", "", "elif", "num_params", ">", "0.4", "or", "n_cells", ">", "10", ":", "\n", "                    ", "C", "=", "rand_choice", "(", "self", ".", "num_ch", ",", "2", ")", "# 16-32", "\n", "", "elif", "num_params", ">", "0.2", "or", "n_cells", ">", "8", ":", "\n", "                    ", "C", "=", "rand_choice", "(", "self", ".", "num_ch", ",", "3", ")", "# 16-64", "\n", "", "else", ":", "\n", "                    ", "C", "=", "rand_choice", "(", "self", ".", "num_ch", ")", "# 16-128", "\n", "if", "C", "<=", "64", ":", "\n", "                        ", "fc", "=", "rand_choice", "(", "self", ".", "fc_dim", ")", "\n", "", "", "args", "[", "'C'", "]", "=", "C", ".", "item", "(", ")", "\n", "args", "[", "'fc_dim'", "]", "=", "fc", ".", "item", "(", ")", "\n", "\n", "", "net_args", "=", "{", "'genotype'", ":", "cell", "}", "\n", "for", "key", "in", "[", "'norm'", ",", "'ks'", ",", "'preproc'", ",", "'glob_avg'", ",", "'stem_pool'", ",", "'C_mult'", ",", "\n", "'n_cells'", ",", "'fc_layers'", ",", "'C'", ",", "'fc_dim'", ",", "'stem_type'", "]", ":", "\n", "                ", "if", "key", "==", "'C'", "and", "self", ".", "split", "==", "'wide'", ":", "\n", "                    ", "net_args", "[", "key", "]", "=", "args", "[", "key", "]", "*", "(", "2", "if", "self", ".", "large_images", "else", "4", ")", "\n", "", "else", ":", "\n", "                    ", "net_args", "[", "key", "]", "=", "args", "[", "key", "]", "\n", "\n", "", "", "graph", ".", "net_args", "=", "net_args", "\n", "graph", ".", "net_idx", "=", "idx", "\n", "\n", "", "return", "graph", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.loader.DeepNets1M._init_graph": [[167, 235], ["torch.zeros", "enumerate", "torch.from_numpy().long", "graph.Graph.Graph", "len", "len", "name.startswith", "param_shapes.append", "A[].sum", "A[].sum", "range", "name_op_net.startswith", "name_op_net.find", "name_op_net.find", "node_info[].append", "torch.from_numpy", "len", "name_op_net.endswith", "name.find", "name.find", "name_op_net.split", "enumerate", "name.find", "name.find", "name.find", "numpy.diag_indices_from", "numpy.diag_indices_from", "len", "len", "len", "int", "name_op_net.split.insert"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.graph.GraphBatch.append", "home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.graph.GraphBatch.append"], ["", "def", "_init_graph", "(", "self", ",", "A", ",", "nodes", ",", "layers", ")", ":", "\n", "\n", "        ", "N", "=", "A", ".", "shape", "[", "0", "]", "\n", "assert", "N", "==", "len", "(", "nodes", ")", ",", "(", "N", ",", "len", "(", "nodes", ")", ")", "\n", "\n", "node_feat", "=", "torch", ".", "zeros", "(", "N", ",", "1", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "node_info", "=", "[", "[", "]", "for", "_", "in", "range", "(", "layers", ")", "]", "\n", "param_shapes", "=", "[", "]", "\n", "\n", "for", "node_ind", ",", "node", "in", "enumerate", "(", "nodes", ")", ":", "\n", "            ", "name", "=", "self", ".", "primitives_ext", "[", "node", "[", "0", "]", "]", "\n", "cell_ind", "=", "node", "[", "1", "]", "\n", "name_op_net", "=", "self", ".", "op_names_net", "[", "node", "[", "2", "]", "]", "\n", "\n", "sz", "=", "None", "\n", "\n", "if", "not", "name_op_net", ".", "startswith", "(", "'classifier'", ")", ":", "\n", "# fix some inconsistency between names in different versions of our code", "\n", "                ", "if", "len", "(", "name_op_net", ")", "==", "0", ":", "\n", "                    ", "name_op_net", "=", "'input'", "\n", "", "elif", "name_op_net", ".", "endswith", "(", "'to_out.0.'", ")", ":", "\n", "                    ", "name_op_net", "+=", "'weight'", "\n", "", "else", ":", "\n", "                    ", "parts", "=", "name_op_net", ".", "split", "(", "'.'", ")", "\n", "for", "i", ",", "s", "in", "enumerate", "(", "parts", ")", ":", "\n", "                        ", "if", "s", "==", "'_ops'", "and", "parts", "[", "i", "+", "2", "]", "!=", "'op'", ":", "\n", "                            ", "try", ":", "\n", "                                ", "_", "=", "int", "(", "parts", "[", "i", "+", "2", "]", ")", "\n", "parts", ".", "insert", "(", "i", "+", "2", ",", "'op'", ")", "\n", "name_op_net", "=", "'.'", ".", "join", "(", "parts", ")", "\n", "break", "\n", "", "except", ":", "\n", "                                ", "continue", "\n", "\n", "", "", "", "", "name_op_net", "=", "'cells.%d.%s'", "%", "(", "cell_ind", ",", "name_op_net", ")", "\n", "\n", "stem_p", "=", "name_op_net", ".", "find", "(", "'stem'", ")", "\n", "pos_enc_p", "=", "name_op_net", ".", "find", "(", "'pos_enc'", ")", "\n", "if", "stem_p", ">=", "0", ":", "\n", "                    ", "name_op_net", "=", "name_op_net", "[", "stem_p", ":", "]", "\n", "", "elif", "pos_enc_p", ">=", "0", ":", "\n", "                    ", "name_op_net", "=", "name_op_net", "[", "pos_enc_p", ":", "]", "\n", "", "elif", "name", ".", "find", "(", "'pool'", ")", ">=", "0", ":", "\n", "                    ", "sz", "=", "(", "1", ",", "1", ",", "3", ",", "3", ")", "# assume all pooling layers are 3x3 in our DeepNets-1M", "\n", "\n", "", "", "if", "name", ".", "startswith", "(", "'conv_'", ")", ":", "\n", "                ", "if", "name", "==", "'conv_1x1'", ":", "\n", "                    ", "sz", "=", "(", "3", ",", "16", ",", "1", ",", "1", ")", "# just some random shape for visualization purposes", "\n", "", "name", "=", "'conv'", "# remove kernel size info from the name", "\n", "", "elif", "name", ".", "find", "(", "'conv_'", ")", ">", "0", "or", "name", ".", "find", "(", "'pool_'", ")", ">", "0", ":", "\n", "                ", "name", "=", "name", "[", ":", "len", "(", "name", ")", "-", "4", "]", "# remove kernel size info from the name", "\n", "", "elif", "name", "==", "'fc-b'", ":", "\n", "                ", "name", "=", "'bias'", "\n", "\n", "", "param_shapes", ".", "append", "(", "sz", ")", "\n", "node_feat", "[", "node_ind", "]", "=", "self", ".", "primitives_dict", "[", "name", "]", "\n", "if", "name", ".", "find", "(", "'conv'", ")", ">=", "0", "or", "name", ".", "find", "(", "'pool'", ")", ">=", "0", "or", "name", "in", "[", "'bias'", ",", "'bn'", ",", "'ln'", ",", "'pos_enc'", "]", ":", "\n", "                ", "node_info", "[", "cell_ind", "]", ".", "append", "(", "(", "node_ind", ",", "name_op_net", ",", "name", ",", "sz", ",", "node_ind", "==", "len", "(", "nodes", ")", "-", "2", ",", "node_ind", "==", "len", "(", "nodes", ")", "-", "1", ")", ")", "\n", "\n", "", "", "A", "=", "torch", ".", "from_numpy", "(", "A", ")", ".", "long", "(", ")", "\n", "A", "[", "A", ">", "self", ".", "virtual_edges", "]", "=", "0", "\n", "assert", "A", "[", "np", ".", "diag_indices_from", "(", "A", ")", "]", ".", "sum", "(", ")", "==", "0", ",", "(", "\n", "'no loops should be in the graph'", ",", "A", "[", "np", ".", "diag_indices_from", "(", "A", ")", "]", ".", "sum", "(", ")", ")", "\n", "\n", "graph", "=", "Graph", "(", "node_feat", "=", "node_feat", ",", "node_info", "=", "node_info", ",", "A", "=", "A", ")", "\n", "graph", ".", "_param_shapes", "=", "param_shapes", "\n", "\n", "return", "graph", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.loader.DeepNets1M._get_predefined": [[237, 259], ["enumerate", "graphs.append", "utils.adjust_net", "net.Network", "graph.Graph", "eval", "eval"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.graph.GraphBatch.append", "home.repos.pwc.inspect_result.facebookresearch_ppuda.utils.utils.adjust_net"], ["", "def", "_get_predefined", "(", "self", ")", ":", "\n", "\n", "        ", "graphs", "=", "[", "]", "\n", "for", "idx", ",", "arch", "in", "enumerate", "(", "[", "'resnet50'", ",", "'ViT'", "]", ")", ":", "# Here, other torchvision models can be added", "\n", "\n", "            ", "num_classes", "=", "1000", "if", "self", ".", "large_images", "else", "10", "# the exact number should not be important at the graph construction stage", "\n", "if", "arch", "==", "'resnet50'", ":", "\n", "                ", "model", "=", "adjust_net", "(", "eval", "(", "'torchvision.models.%s(num_classes=%d)'", "%", "(", "arch", ",", "num_classes", ")", ")", ",", "\n", "large_input", "=", "self", ".", "large_images", ")", "\n", "args", "=", "{", "'genotype'", ":", "arch", "}", "\n", "", "else", ":", "\n", "                ", "args", "=", "{", "'C'", ":", "128", ",", "\n", "'genotype'", ":", "eval", "(", "'genotypes.%s'", "%", "arch", ")", ",", "\n", "'n_cells'", ":", "12", ",", "\n", "'glob_avg'", ":", "True", ",", "\n", "'preproc'", ":", "False", ",", "\n", "'C_mult'", ":", "1", "}", "\n", "model", "=", "Network", "(", "num_classes", "=", "num_classes", ",", "is_imagenet_input", "=", "self", ".", "large_images", ",", "**", "args", ")", "\n", "\n", "", "graphs", ".", "append", "(", "Graph", "(", "model", ",", "net_args", "=", "args", ",", "net_idx", "=", "idx", ")", ")", "\n", "\n", "", "return", "graphs", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.loader.NetBatchSampler.__init__": [[267, 274], ["super().__init__", "torch.utils.data.RandomSampler", "torch.utils.data.SequentialSampler"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_ppuda.examples.custom_cnn.CNN.__init__"], ["def", "__init__", "(", "self", ",", "deepnets", ",", "meta_batch_size", "=", "1", ")", ":", "\n", "        ", "super", "(", "NetBatchSampler", ",", "self", ")", ".", "__init__", "(", "\n", "torch", ".", "utils", ".", "data", ".", "RandomSampler", "(", "deepnets", ")", "if", "deepnets", ".", "is_train", "\n", "else", "torch", ".", "utils", ".", "data", ".", "SequentialSampler", "(", "deepnets", ")", ",", "\n", "meta_batch_size", ",", "\n", "drop_last", "=", "False", ")", "\n", "self", ".", "max_nodes_batch", "=", "MAX_NODES_BATCH", "if", "deepnets", ".", "is_train", "else", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.loader.NetBatchSampler.check_batch": [[275, 279], ["loader.NetBatchSampler.sampler.data_source.nodes[].sum"], "methods", ["None"], ["", "def", "check_batch", "(", "self", ",", "batch", ")", ":", "\n", "        ", "return", "(", "self", ".", "max_nodes_batch", "is", "None", "or", "\n", "self", ".", "sampler", ".", "data_source", ".", "nodes", "[", "batch", "]", ".", "sum", "(", ")", "<=", "\n", "self", ".", "max_nodes_batch", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.loader.NetBatchSampler.__iter__": [[280, 292], ["batch.append", "loader.NetBatchSampler.check_batch", "len", "loader.NetBatchSampler.check_batch", "len"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.graph.GraphBatch.append", "home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.loader.NetBatchSampler.check_batch", "home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.loader.NetBatchSampler.check_batch"], ["", "def", "__iter__", "(", "self", ")", ":", "\n", "        ", "while", "True", ":", "# infinite sampler", "\n", "            ", "batch", "=", "[", "]", "\n", "for", "idx", "in", "self", ".", "sampler", ":", "\n", "                ", "batch", ".", "append", "(", "idx", ")", "\n", "if", "len", "(", "batch", ")", "==", "self", ".", "batch_size", ":", "\n", "                    ", "if", "self", ".", "check_batch", "(", "batch", ")", ":", "\n", "                        ", "yield", "batch", "\n", "", "batch", "=", "[", "]", "\n", "", "", "if", "len", "(", "batch", ")", ">", "0", "and", "not", "self", ".", "drop_last", ":", "\n", "                ", "if", "self", ".", "check_batch", "(", "batch", ")", ":", "\n", "                    ", "yield", "batch", "\n", "", "", "", "", "", ""]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.vision.transforms.Cutout.__init__": [[20, 22], ["None"], "methods", ["None"], ["return", "flipped_data", "\n", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.vision.transforms.Cutout.__call__": [[23, 39], ["numpy.ones", "numpy.random.randint", "numpy.random.randint", "numpy.clip", "numpy.clip", "numpy.clip", "numpy.clip", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "mask.expand_as.expand_as.expand_as", "img.size", "img.size"], "methods", ["None"], ["", "class", "Compose", "(", "object", ")", ":", "\n", "    ", "def", "__init__", "(", "self", ",", "transforms", ")", ":", "\n", "        ", "self", ".", "transforms", "=", "transforms", "\n", "\n", "", "def", "__call__", "(", "self", ",", "image", ",", "target", ")", ":", "\n", "        ", "for", "t", "in", "self", ".", "transforms", ":", "\n", "            ", "image", ",", "target", "=", "t", "(", "image", ",", "target", ")", "\n", "", "return", "image", ",", "target", "\n", "\n", "\n", "", "", "class", "RandomHorizontalFlip", "(", "object", ")", ":", "\n", "    ", "def", "__init__", "(", "self", ",", "prob", ")", ":", "\n", "        ", "self", ".", "prob", "=", "prob", "\n", "\n", "", "def", "__call__", "(", "self", ",", "image", ",", "target", ")", ":", "\n", "        ", "if", "random", ".", "random", "(", ")", "<", "self", ".", "prob", ":", "\n", "            ", "height", ",", "width", "=", "image", ".", "shape", "[", "-", "2", ":", "]", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.vision.transforms.Noise.__init__": [[42, 51], ["print"], "methods", ["None"], ["bbox", "[", ":", ",", "[", "0", ",", "2", "]", "]", "=", "width", "-", "bbox", "[", ":", ",", "[", "2", ",", "0", "]", "]", "\n", "target", "[", "\"boxes\"", "]", "=", "bbox", "\n", "if", "\"masks\"", "in", "target", ":", "\n", "                ", "target", "[", "\"masks\"", "]", "=", "target", "[", "\"masks\"", "]", ".", "flip", "(", "-", "1", ")", "\n", "", "if", "\"keypoints\"", "in", "target", ":", "\n", "                ", "keypoints", "=", "target", "[", "\"keypoints\"", "]", "\n", "keypoints", "=", "_flip_coco_person_keypoints", "(", "keypoints", ",", "width", ")", "\n", "target", "[", "\"keypoints\"", "]", "=", "keypoints", "\n", "", "", "return", "image", ",", "target", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.vision.transforms.Noise.__call__": [[52, 55], ["torch.clip", "torch.clip", "torch.clip", "torch.clip", "torch.randn", "torch.randn", "torch.randn", "torch.randn"], "methods", ["None"], ["\n", "", "", "class", "ToTensor", "(", "object", ")", ":", "\n", "    ", "def", "__call__", "(", "self", ",", "image", ",", "target", ")", ":", "\n", "        ", "image", "=", "F", ".", "to_tensor", "(", "image", ")", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.vision.transforms.transforms_cifar": [[57, 86], ["transforms.Compose.extend", "torchvision.Compose", "transforms.Compose.append", "transforms.Compose.append", "torchvision.Compose", "torchvision.RandomCrop", "torchvision.Normalize", "torchvision.Normalize", "transforms.Compose.append", "transforms.Compose.append", "transforms.Compose.append", "torchvision.ToTensor", "transforms.Compose.append", "torchvision.Resize", "torchvision.Resize", "torchvision.RandomHorizontalFlip", "torchvision.ToTensor", "transforms.Cutout", "transforms.Noise"], "function", ["home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.graph.GraphBatch.append", "home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.graph.GraphBatch.append", "home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.graph.GraphBatch.append", "home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.graph.GraphBatch.append", "home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.graph.GraphBatch.append", "home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.graph.GraphBatch.append"], ["", "", ""]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.vision.transforms.transforms_imagenet": [[88, 119], ["torchvision.Normalize", "torchvision.Compose", "transforms.Compose.append", "torchvision.Compose", "torchvision.RandomResizedCrop", "torchvision.RandomHorizontalFlip", "torchvision.ColorJitter", "torchvision.ToTensor", "torchvision.Resize", "torchvision.CenterCrop", "torchvision.ToTensor", "transforms.Compose.append", "transforms.Noise"], "function", ["home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.graph.GraphBatch.append", "home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.graph.GraphBatch.append"], []], "home.repos.pwc.inspect_result.facebookresearch_ppuda.vision.imagenet.ImageNetDataset.__init__": [[40, 65], ["super().__init__", "hashlib.sha256", "hashlib.sha256.update", "int", "imagenet.split_train_and_val", "str().encode", "hashlib.sha256.hexdigest", "str"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_ppuda.examples.custom_cnn.CNN.__init__", "home.repos.pwc.inspect_result.facebookresearch_ppuda.utils.trainer.Trainer.update", "home.repos.pwc.inspect_result.facebookresearch_ppuda.vision.imagenet.split_train_and_val"], ["    ", "def", "__init__", "(", "self", ",", "root", ",", "split", ",", "transform", "=", "None", ",", "has_validation", "=", "True", ")", ":", "\n", "        ", "assert", "split", "in", "{", "\"train\"", ",", "\"val\"", "}", "\n", "\n", "# this is to compute the split in case we have validation set, but with", "\n", "# the actual validation set being used as the test set, meaning", "\n", "# we need to split the train set into train and val", "\n", "base_split", "=", "'train'", "if", "(", "split", "==", "'val'", "and", "has_validation", ")", "else", "split", "\n", "\n", "super", "(", ")", ".", "__init__", "(", "root", ",", "base_split", ",", "transform", "=", "transform", ")", "\n", "\n", "# revert the split back to the actual one", "\n", "# since pytorch will set the self.split attribute", "\n", "self", ".", "split", "=", "split", "\n", "\n", "if", "has_validation", ":", "\n", "            ", "train_samples", ",", "val_samples", "=", "split_train_and_val", "(", "\n", "self", ".", "samples", ",", "num_val_per_class", "=", "50", ")", "\n", "\n", "self", ".", "samples", "=", "train_samples", "if", "split", "==", "\"train\"", "else", "val_samples", "\n", "\n", "", "m", "=", "hashlib", ".", "sha256", "(", ")", "\n", "m", ".", "update", "(", "str", "(", "self", ".", "samples", ")", ".", "encode", "(", ")", ")", "\n", "\n", "# convert from hex string to number", "\n", "self", ".", "checksum", "=", "int", "(", "m", ".", "hexdigest", "(", ")", ",", "16", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.vision.imagenet.ImageNetDataset.num_examples": [[66, 69], ["len"], "methods", ["None"], ["", "@", "property", "\n", "def", "num_examples", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.vision.imagenet.split_train_and_val": [[18, 36], ["collections.defaultdict", "sorted", "class_dict[].append", "collections.defaultdict.keys", "train_samples.extend", "val_samples.extend"], "function", ["home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.graph.GraphBatch.append"], ["def", "split_train_and_val", "(", "list_of_tups", ",", "num_val_per_class", "=", "50", ")", ":", "\n", "\n", "    ", "class_dict", "=", "defaultdict", "(", "list", ")", "\n", "\n", "for", "item", "in", "list_of_tups", ":", "\n", "        ", "class_dict", "[", "item", "[", "1", "]", "]", ".", "append", "(", "item", ")", "\n", "\n", "", "train_samples", ",", "val_samples", "=", "[", "]", ",", "[", "]", "\n", "\n", "# fix the class ordering", "\n", "for", "k", "in", "sorted", "(", "class_dict", ".", "keys", "(", ")", ")", ":", "\n", "        ", "v", "=", "class_dict", "[", "k", "]", "\n", "\n", "# last num_val_per_class will be the val samples", "\n", "train_samples", ".", "extend", "(", "v", "[", ":", "-", "num_val_per_class", "]", ")", "\n", "val_samples", ".", "extend", "(", "v", "[", "-", "num_val_per_class", ":", "]", ")", "\n", "\n", "", "return", "train_samples", ",", "val_samples", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.vision.loader.image_loader": [[22, 116], ["print", "torch.utils.data.DataLoader", "dataset.upper.lower", "transforms.transforms_imagenet", "os.path.join", "imagenet.ImageNetDataset", "len", "torch.Generator", "torch.Generator.manual_seed", "dataset.upper.upper", "transforms.transforms_cifar", "len", "eval.data.mean", "len", "torch.utils.data.DataLoader", "imagenet.ImageNetDataset", "eval", "eval", "eval", "len", "torch.split", "to_few_shot.data.mean", "len", "torch.unique", "eval", "print", "torch.arange", "torch.tensor", "loader.to_few_shot"], "function", ["home.repos.pwc.inspect_result.facebookresearch_ppuda.vision.transforms.transforms_imagenet", "home.repos.pwc.inspect_result.facebookresearch_ppuda.vision.transforms.transforms_cifar", "home.repos.pwc.inspect_result.facebookresearch_ppuda.vision.loader.to_few_shot"], ["from", ".", "ops", "import", "*", "\n", "from", ".", "net", "import", "Network", "\n", "from", ".", "graph", "import", "Graph", ",", "GraphBatch", "\n", "\n", "\n", "MAX_NODES_BATCH", "=", "2200", "# to fit larger meta batches into GPU memory (decreasing this number further may create a bias towards smaller architectures)", "\n", "\n", "\n", "class", "DeepNets1M", "(", "torch", ".", "utils", ".", "data", ".", "Dataset", ")", ":", "\n", "    ", "r\"\"\"\n    Default args correspond to training a baseline GHN on CIFAR-10.\n    \"\"\"", "\n", "\n", "def", "__init__", "(", "self", ",", "\n", "split", "=", "'train'", ",", "\n", "nets_dir", "=", "'./data'", ",", "\n", "virtual_edges", "=", "1", ",", "\n", "num_ch", "=", "(", "32", ",", "128", ")", ",", "\n", "fc_dim", "=", "(", "64", ",", "512", ")", ",", "\n", "num_nets", "=", "None", ",", "\n", "arch", "=", "None", ",", "\n", "large_images", "=", "False", ")", ":", "\n", "        ", "super", "(", "DeepNets1M", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "split", "=", "split", "\n", "assert", "self", ".", "split", "in", "[", "'train'", ",", "'val'", ",", "'test'", ",", "'search'", ",", "\n", "'wide'", ",", "'deep'", ",", "'dense'", ",", "'bnfree'", ",", "'predefined'", "]", ",", "(", "'invalid split'", ",", "self", ".", "split", ")", "\n", "self", ".", "is_train", "=", "self", ".", "split", "==", "'train'", "\n", "\n", "self", ".", "virtual_edges", "=", "virtual_edges", "\n", "assert", "self", ".", "virtual_edges", ">=", "1", ",", "virtual_edges", "\n", "\n", "if", "self", ".", "is_train", ":", "\n", "# During training we will randomly sample values from this range", "\n", "            ", "self", ".", "num_ch", "=", "torch", ".", "arange", "(", "num_ch", "[", "0", "]", ",", "num_ch", "[", "1", "]", "+", "1", ",", "16", ")", "\n", "self", ".", "fc_dim", "=", "torch", ".", "arange", "(", "fc_dim", "[", "0", "]", ",", "fc_dim", "[", "1", "]", "+", "1", ",", "64", ")", "\n", "\n", "", "self", ".", "large_images", "=", "large_images", "# this affects some network parameters", "\n", "\n", "# Load one of the splits", "\n", "print", "(", "'\\nloading %s nets...'", "%", "self", ".", "split", ".", "upper", "(", ")", ")", "\n", "\n", "if", "self", ".", "split", "==", "'predefined'", ":", "\n", "            ", "self", ".", "nets", "=", "self", ".", "_get_predefined", "(", ")", "\n", "n_all", "=", "len", "(", "self", ".", "nets", ")", "\n", "self", ".", "nodes", "=", "torch", ".", "tensor", "(", "[", "net", ".", "n_nodes", "for", "net", "in", "self", ".", "nets", "]", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "h5_data", "=", "None", "\n", "self", ".", "h5_file", "=", "os", ".", "path", ".", "join", "(", "nets_dir", ",", "'deepnets1m_%s.hdf5'", "%", "(", "split", "if", "split", "in", "[", "'train'", ",", "'search'", "]", "else", "'eval'", ")", ")", "\n", "\n", "self", ".", "primitives_dict", "=", "{", "op", ":", "i", "for", "i", ",", "op", "in", "enumerate", "(", "PRIMITIVES_DEEPNETS1M", ")", "}", "\n", "assert", "os", ".", "path", ".", "exists", "(", "self", ".", "h5_file", ")", ",", "(", "'%s not found'", "%", "self", ".", "h5_file", ")", "\n", "\n", "# Load meta data to convert dataset files to graphs later in the _init_graph function", "\n", "to_int_dict", "=", "lambda", "d", ":", "{", "int", "(", "k", ")", ":", "v", "for", "k", ",", "v", "in", "d", ".", "items", "(", ")", "}", "\n", "with", "open", "(", "self", ".", "h5_file", ".", "replace", "(", "'.hdf5'", ",", "'_meta.json'", ")", ",", "'r'", ")", "as", "f", ":", "\n", "                ", "meta", "=", "json", ".", "load", "(", "f", ")", "[", "split", "]", "\n", "n_all", "=", "len", "(", "meta", "[", "'nets'", "]", ")", "\n", "self", ".", "nets", "=", "meta", "[", "'nets'", "]", "[", ":", "n_all", "if", "num_nets", "is", "None", "else", "num_nets", "]", "\n", "self", ".", "primitives_ext", "=", "to_int_dict", "(", "meta", "[", "'meta'", "]", "[", "'primitives_ext'", "]", ")", "\n", "self", ".", "op_names_net", "=", "to_int_dict", "(", "meta", "[", "'meta'", "]", "[", "'unique_op_names'", "]", ")", "\n", "", "self", ".", "h5_idx", "=", "[", "arch", "]", "if", "arch", "is", "not", "None", "else", "None", "\n", "self", ".", "nodes", "=", "torch", ".", "tensor", "(", "[", "net", "[", "'num_nodes'", "]", "for", "net", "in", "self", ".", "nets", "]", ")", "\n", "\n", "", "if", "arch", "is", "not", "None", ":", "\n", "            ", "arch", "=", "int", "(", "arch", ")", "\n", "assert", "arch", ">=", "0", "and", "arch", "<", "len", "(", "self", ".", "nets", ")", ",", "'architecture with index={} is not available in the {} split with {} architectures in total'", ".", "format", "(", "arch", ",", "split", ",", "len", "(", "self", ".", "nets", ")", ")", "\n", "self", ".", "nets", "=", "[", "self", ".", "nets", "[", "arch", "]", "]", "\n", "self", ".", "nodes", "=", "torch", ".", "tensor", "(", "[", "self", ".", "nodes", "[", "arch", "]", "]", ")", "\n", "\n", "", "print", "(", "'loaded {}/{} nets with {}-{} nodes (mean\\u00B1std: {:.1f}\\u00B1{:.1f})'", ".", "\n", "format", "(", "len", "(", "self", ".", "nets", ")", ",", "n_all", ",", "\n", "self", ".", "nodes", ".", "min", "(", ")", ".", "item", "(", ")", ",", "\n", "self", ".", "nodes", ".", "max", "(", ")", ".", "item", "(", ")", ",", "\n", "self", ".", "nodes", ".", "float", "(", ")", ".", "mean", "(", ")", ".", "item", "(", ")", ",", "\n", "self", ".", "nodes", ".", "float", "(", ")", ".", "std", "(", ")", ".", "item", "(", ")", ")", ")", "\n", "\n", "\n", "", "@", "staticmethod", "\n", "def", "loader", "(", "meta_batch_size", "=", "1", ",", "**", "kwargs", ")", ":", "\n", "        ", "nets", "=", "DeepNets1M", "(", "**", "kwargs", ")", "\n", "loader", "=", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "nets", ",", "\n", "batch_sampler", "=", "NetBatchSampler", "(", "nets", ",", "meta_batch_size", ")", "if", "nets", ".", "is_train", "else", "None", ",", "\n", "batch_size", "=", "1", ",", "\n", "pin_memory", "=", "False", ",", "\n", "collate_fn", "=", "GraphBatch", ",", "\n", "num_workers", "=", "2", "if", "meta_batch_size", "<=", "1", "else", "min", "(", "8", ",", "meta_batch_size", ")", ")", "\n", "return", "iter", "(", "loader", ")", "if", "nets", ".", "is_train", "else", "loader", "\n", "\n", "\n", "", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "nets", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.vision.loader.to_few_shot": [[118, 153], ["enumerate", "sorted", "min", "min", "torch.cat", "isinstance", "isinstance", "lbl.item", "len", "labels_dict[].append", "torch.tensor", "labels_dict.items"], "function", ["home.repos.pwc.inspect_result.facebookresearch_ppuda.deepnets1m.graph.GraphBatch.append"], ["", "def", "__getitem__", "(", "self", ",", "idx", ")", ":", "\n", "\n", "        ", "if", "self", ".", "split", "==", "'predefined'", ":", "\n", "            ", "graph", "=", "self", ".", "nets", "[", "idx", "]", "\n", "", "else", ":", "\n", "\n", "            ", "if", "self", ".", "h5_data", "is", "None", ":", "# A separate fd is opened for each worker process", "\n", "                ", "self", ".", "h5_data", "=", "h5py", ".", "File", "(", "self", ".", "h5_file", ",", "mode", "=", "'r'", ")", "\n", "\n", "", "args", "=", "self", ".", "nets", "[", "idx", "]", "\n", "idx", "=", "self", ".", "h5_idx", "[", "idx", "]", "if", "self", ".", "h5_idx", "is", "not", "None", "else", "idx", "\n", "cell", ",", "n_cells", "=", "from_dict", "(", "args", "[", "'genotype'", "]", ")", ",", "args", "[", "'n_cells'", "]", "\n", "graph", "=", "self", ".", "_init_graph", "(", "self", ".", "h5_data", "[", "self", ".", "split", "]", "[", "str", "(", "idx", ")", "]", "[", "'adj'", "]", "[", "(", ")", "]", ",", "\n", "self", ".", "h5_data", "[", "self", ".", "split", "]", "[", "str", "(", "idx", ")", "]", "[", "'nodes'", "]", "[", "(", ")", "]", ",", "\n", "n_cells", ")", "\n", "\n", "if", "self", ".", "is_train", ":", "\n", "                ", "is_conv_dense", "=", "sum", "(", "[", "n", "[", "0", "]", "in", "[", "'conv_5x5'", ",", "'conv_7x7'", "]", "for", "n", "in", "\n", "cell", ".", "normal", "+", "cell", ".", "reduce", "]", ")", ">", "0", "\n", "num_params", "=", "args", "[", "'num_params'", "]", "[", "'imagenet'", "if", "self", ".", "large_images", "else", "'cifar10'", "]", "/", "10", "**", "6", "\n", "\n", "fc", "=", "rand_choice", "(", "self", ".", "fc_dim", ",", "4", ")", "# 64-256", "\n", "if", "num_params", ">", "0.8", "or", "not", "args", "[", "'glob_avg'", "]", "or", "is_conv_dense", "or", "n_cells", ">", "12", ":", "\n", "                    ", "C", "=", "self", ".", "num_ch", ".", "min", "(", ")", "\n", "", "elif", "num_params", ">", "0.4", "or", "n_cells", ">", "10", ":", "\n", "                    ", "C", "=", "rand_choice", "(", "self", ".", "num_ch", ",", "2", ")", "# 16-32", "\n", "", "elif", "num_params", ">", "0.2", "or", "n_cells", ">", "8", ":", "\n", "                    ", "C", "=", "rand_choice", "(", "self", ".", "num_ch", ",", "3", ")", "# 16-64", "\n", "", "else", ":", "\n", "                    ", "C", "=", "rand_choice", "(", "self", ".", "num_ch", ")", "# 16-128", "\n", "if", "C", "<=", "64", ":", "\n", "                        ", "fc", "=", "rand_choice", "(", "self", ".", "fc_dim", ")", "\n", "", "", "args", "[", "'C'", "]", "=", "C", ".", "item", "(", ")", "\n", "args", "[", "'fc_dim'", "]", "=", "fc", ".", "item", "(", ")", "\n", "\n", "", "net_args", "=", "{", "'genotype'", ":", "cell", "}", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.examples.custom_cnn.CNN.__init__": [[35, 48], ["torch.Module.__init__", "torch.Sequential", "torch.Conv2d", "torch.AvgPool2d", "torch.ReLU", "torch.Conv2d", "torch.AvgPool2d", "torch.ReLU", "torch.Conv2d", "torch.ReLU", "torch.AdaptiveAvgPool2d", "torch.Flatten", "torch.Linear"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_ppuda.examples.custom_cnn.CNN.__init__"], ["    ", "def", "__init__", "(", "self", ",", "C", "=", "64", ")", ":", "\n", "        ", "super", "(", "CNN", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "layers", "=", "nn", ".", "Sequential", "(", "nn", ".", "Conv2d", "(", "3", ",", "C", ",", "kernel_size", "=", "5", ")", ",", "\n", "nn", ".", "AvgPool2d", "(", "2", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Conv2d", "(", "C", ",", "C", "*", "2", ",", "kernel_size", "=", "3", ")", ",", "\n", "nn", ".", "AvgPool2d", "(", "2", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Conv2d", "(", "C", "*", "2", ",", "C", "*", "4", ",", "kernel_size", "=", "3", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "AdaptiveAvgPool2d", "(", "1", ")", ",", "\n", "nn", ".", "Flatten", "(", ")", ",", "\n", "nn", ".", "Linear", "(", "C", "*", "4", ",", "num_classes", ")", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_ppuda.examples.custom_cnn.CNN.forward": [[50, 52], ["custom_cnn.CNN.layers"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "return", "self", ".", "layers", "(", "x", ")", "\n", "\n"]]}