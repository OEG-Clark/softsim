{"home.repos.pwc.inspect_result.AADeLucia_gpt2-narrative-decoding.None.generate_responses.batchify": [[27, 56], ["enumerate", "len", "minibatches.append", "minibatches_idx.append", "list", "len", "minibatches.append", "minibatches_idx.append", "curr_batch.append", "curr_batch_idx.append", "len", "range", "range", "len", "len"], "function", ["None"], ["def", "batchify", "(", "prompts", ",", "batch_size", ")", ":", "\n", "    ", "\"\"\"\n    Batch prompts\n    \"\"\"", "\n", "# Edge case: 1 prompt per batch", "\n", "if", "len", "(", "prompts", ")", "==", "batch_size", ":", "\n", "        ", "return", "[", "[", "p", "]", "for", "p", "in", "prompts", "]", ",", "[", "[", "i", "]", "for", "i", "in", "range", "(", "len", "(", "prompts", ")", ")", "]", "\n", "# Edge case: batch size of 1 (all prompts at once)", "\n", "", "if", "batch_size", "==", "1", ":", "\n", "        ", "return", "[", "prompts", "]", ",", "list", "(", "range", "(", "len", "(", "prompts", ")", ")", ")", "\n", "\n", "", "minibatches", "=", "[", "]", "\n", "minibatches_idx", "=", "[", "]", "\n", "curr_batch", "=", "[", "]", "\n", "curr_batch_idx", "=", "[", "]", "\n", "for", "i", ",", "prompt", "in", "enumerate", "(", "prompts", ")", ":", "\n", "        ", "if", "len", "(", "curr_batch", ")", "==", "batch_size", ":", "\n", "            ", "minibatches", ".", "append", "(", "curr_batch", ")", "\n", "minibatches_idx", ".", "append", "(", "curr_batch_idx", ")", "\n", "curr_batch", "=", "[", "prompt", "]", "\n", "curr_batch_idx", "=", "[", "i", "]", "\n", "", "else", ":", "\n", "            ", "curr_batch", ".", "append", "(", "prompt", ")", "\n", "curr_batch_idx", ".", "append", "(", "i", ")", "\n", "# Check if extra prompts", "\n", "", "", "if", "len", "(", "prompts", ")", "%", "batch_size", "!=", "0", ":", "\n", "        ", "minibatches", ".", "append", "(", "curr_batch", ")", "\n", "minibatches_idx", ".", "append", "(", "curr_batch_idx", ")", "\n", "", "return", "minibatches", ",", "minibatches_idx", "\n", "\n"]], "home.repos.pwc.inspect_result.AADeLucia_gpt2-narrative-decoding.None.generate_responses.parse_args": [[58, 78], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "torch.device", "torch.cuda.is_available"], "function", ["home.repos.pwc.inspect_result.AADeLucia_gpt2-narrative-decoding.None.evaluation.parse_args"], ["", "def", "parse_args", "(", ")", ":", "\n", "    ", "\"\"\"Process commandline arguments\"\"\"", "\n", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "parser", ".", "add_argument", "(", "\"--prompt-path\"", ",", "type", "=", "str", ",", "default", "=", "\"/home/aadelucia/gpt/writing_prompts/test.wp.src\"", ",", "\n", "help", "=", "\"Path to prompts, delineated by newlines\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--output-path\"", ",", "type", "=", "str", ",", "default", "=", "\"output.csv\"", ",", "\n", "help", "=", "\"Output path for generated repsonses in CSV format with columns <id>,<prompt>,<response>\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--model-name-or-path\"", ",", "type", "=", "str", ",", "default", "=", "None", ",", "help", "=", "\"Path to model\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--top-p\"", ",", "type", "=", "float", ",", "nargs", "=", "\"+\"", ",", "default", "=", "[", "0.0", ",", "0.3", ",", "0.5", ",", "0.7", ",", "0.9", ",", "0.95", ",", "1.0", "]", ",", "\n", "help", "=", "\"Top-p (nucleus sampling) values to test. Can pass more than one value. Default values were used in the paper.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--length\"", ",", "type", "=", "int", ",", "default", "=", "200", ",", "help", "=", "\"Maximum length of response (not including prompt length)\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--bsz\"", ",", "type", "=", "int", ",", "default", "=", "20", ",", "help", "=", "\"Batch size\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--display-progress\"", ",", "type", "=", "int", ",", "default", "=", "100", ",", "help", "=", "\"How often to print the generation progress\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--seed\"", ",", "type", "=", "int", ",", "default", "=", "42", ",", "help", "=", "\"Random seed for initialization\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--no-cuda\"", ",", "action", "=", "\"store_true\"", ",", "help", "=", "\"Avoid using CUDA even if available\"", ")", "\n", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "args", ".", "device", "=", "torch", ".", "device", "(", "\"cuda\"", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "and", "not", "args", ".", "no_cuda", "else", "\"cpu\"", ")", "\n", "return", "args", "\n", "\n"]], "home.repos.pwc.inspect_result.AADeLucia_gpt2-narrative-decoding.None.evaluation.parse_args": [[29, 43], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "set"], "function", ["home.repos.pwc.inspect_result.AADeLucia_gpt2-narrative-decoding.None.evaluation.parse_args"], ["def", "parse_args", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "parser", ".", "add_argument", "(", "\"--input-files\"", ",", "nargs", "=", "\"+\"", ",", "required", "=", "True", ",", "\n", "help", "=", "\"List of model response output files in CSV form with columns <id>,<prompt>,<response>, where ID is <prompt id>_<top-p>. The filename should be gpt2_{model size}_{dataset size}.csv\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--output-file\"", ",", "type", "=", "str", ",", "required", "=", "True", ")", "\n", "parser", ".", "add_argument", "(", "\"--baseline\"", ",", "action", "=", "\"store_true\"", ",", "help", "=", "\"Filename format is baseline_dummy_{dataset size}_{p}.txt and content is assumed to be on the responses.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--antilm\"", ",", "action", "=", "\"store_true\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--metrics\"", ",", "nargs", "=", "\"+\"", ",", "default", "=", "set", "(", "[", "\"dist-n\"", ",", "\"sentBERT\"", "]", ")", ",", "\n", "choices", "=", "[", "\"dist-n\"", ",", "\"sentBERT\"", "]", ",", "help", "=", "\"Evaluation metrics\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--cpu\"", ",", "help", "=", "\"Use CPU even if GPU is available\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--debug\"", ",", "action", "=", "\"store_true\"", ")", "\n", "return", "parser", ".", "parse_args", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.AADeLucia_gpt2-narrative-decoding.None.evaluation.sentBERT": [[45, 55], ["sentbert_model.encode", "scipy.spatial.distance.pdist", "numpy.average"], "function", ["None"], ["", "def", "sentBERT", "(", "responses", ")", ":", "\n", "    ", "\"\"\"\n    Compute average pairwise cosine distance between BERT representations\n\n    Note: Diversity paper uses cosine similarity and then negates it, here\n    we just use cosine distance\n    \"\"\"", "\n", "embeddings", "=", "sentbert_model", ".", "encode", "(", "responses", ")", "\n", "distances", "=", "pdist", "(", "embeddings", ",", "metric", "=", "\"cosine\"", ")", "\n", "return", "np", ".", "average", "(", "distances", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.AADeLucia_gpt2-narrative-decoding.None.evaluation.distinct_1": [[57, 68], ["len", "set", "float", "len"], "function", ["None"], ["", "def", "distinct_1", "(", "lines", ")", ":", "\n", "    ", "'''\n    Computes the number of distinct words divided by the total number of words.\n    Input:\n    lines: List of strings.\n\n    Written by Joao Sedoc\n    '''", "\n", "words", "=", "' '", ".", "join", "(", "lines", ")", ".", "split", "(", "' '", ")", "\n", "num_distinct_words", "=", "len", "(", "set", "(", "words", ")", ")", "\n", "return", "float", "(", "num_distinct_words", ")", "/", "len", "(", "words", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.AADeLucia_gpt2-narrative-decoding.None.evaluation.distinct_2": [[70, 88], ["line.split", "len", "zip", "all_bigrams.extend", "len", "float", "list", "set"], "function", ["None"], ["", "def", "distinct_2", "(", "lines", ")", ":", "\n", "    ", "'''Computes the number of distinct bigrams divided by the total number of words.\n\n    Input:\n    lines: List of strings.\n\n    Written by Joao Sedoc\n    '''", "\n", "all_bigrams", "=", "[", "]", "\n", "num_words", "=", "0", "\n", "\n", "for", "line", "in", "lines", ":", "\n", "        ", "line_list", "=", "line", ".", "split", "(", "' '", ")", "\n", "num_words", "+=", "len", "(", "line_list", ")", "\n", "bigrams", "=", "zip", "(", "line_list", ",", "line_list", "[", "1", ":", "]", ")", "\n", "all_bigrams", ".", "extend", "(", "list", "(", "bigrams", ")", ")", "\n", "\n", "", "return", "len", "(", "set", "(", "all_bigrams", ")", ")", "/", "float", "(", "num_words", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.AADeLucia_gpt2-narrative-decoding.None.evaluation.clean_response": [[90, 108], ["regex.sub.strip", "regex.sub", "regex.sub.lower", "logging.debug", "regex.sub", "regex.sub", "logging.debug", "regex.findall"], "function", ["None"], ["", "def", "clean_response", "(", "response", ")", ":", "\n", "    ", "\"\"\"\n    Remove special tokens from the response\n    \"\"\"", "\n", "og_response", "=", "response", "\n", "# Remove extra characters", "\n", "response", "=", "response", ".", "strip", "(", ")", "\n", "# Remove special tokens", "\n", "response", "=", "regex", ".", "sub", "(", "\"<\\|endoftext\\|>|<newline>|\\[RESPONSE\\]\"", ",", "\"\"", ",", "response", ")", "\n", "# Lowercase", "\n", "response", "=", "response", ".", "lower", "(", ")", "\n", "# Add spaces in front of punctuation", "\n", "logging", ".", "debug", "(", "regex", ".", "findall", "(", "\"(\\p{P})\"", ",", "response", ")", ")", "\n", "response", "=", "regex", ".", "sub", "(", "\"(\\p{P})\"", ",", "\" \\\\1\"", ",", "response", ")", "\n", "# Remove excess spacing", "\n", "response", "=", "regex", ".", "sub", "(", "\"\\s+\"", ",", "\" \"", ",", "response", ")", "\n", "logging", ".", "debug", "(", "f\"OG: {og_response}\\nNew: {response}\"", ")", "\n", "return", "response", "\n", "\n"]], "home.repos.pwc.inspect_result.AADeLucia_gpt2-narrative-decoding.mturk_human_evaluation.format_generated_narratives_csv.format_for_html": [[36, 47], ["NEWLINE_RE.sub.strip", "SPECIAL_TOKEN_RE.sub", "re.sub", "re.sub", "re.sub", "NEWLINE_RE.sub"], "function", ["None"], ["def", "format_for_html", "(", "text", ")", ":", "\n", "    ", "text", "=", "text", ".", "strip", "(", ")", "\n", "# Remove [WP]/[RESPONSE] tokens", "\n", "text", "=", "SPECIAL_TOKEN_RE", ".", "sub", "(", "\"\"", ",", "text", ")", "\n", "# Fix weird quotes", "\n", "text", "=", "re", ".", "sub", "(", "\" \u2019 \"", ",", "\"'\"", ",", "text", ")", "\n", "text", "=", "re", ".", "sub", "(", "\"``\"", ",", "\"\\\"\"", ",", "text", ")", "\n", "text", "=", "re", ".", "sub", "(", "\"''\"", ",", "\"\\\"\"", ",", "text", ")", "\n", "# Replace newline characters with HTML linebreaks", "\n", "text", "=", "NEWLINE_RE", ".", "sub", "(", "\"<br/>\"", ",", "text", ")", "\n", "return", "text", "\n", "\n"]], "home.repos.pwc.inspect_result.AADeLucia_gpt2-narrative-decoding.mturk_human_evaluation.fleiss.fleiss_kappa": [[9, 20], ["float", "numpy.sum", "numpy.sum", "numpy.sum", "numpy.sum", "numpy.multiply", "numpy.sum", "numpy.multiply"], "function", ["None"], ["def", "fleiss_kappa", "(", "M", ")", ":", "\n", "    ", "N", ",", "k", "=", "M", ".", "shape", "\n", "n_annotators", "=", "float", "(", "np", ".", "sum", "(", "M", "[", "0", ",", ":", "]", ")", ")", "\n", "\n", "p", "=", "np", ".", "sum", "(", "M", ",", "axis", "=", "0", ")", "/", "(", "N", "*", "n_annotators", ")", "\n", "P", "=", "(", "np", ".", "sum", "(", "np", ".", "multiply", "(", "M", ",", "M", ")", ",", "axis", "=", "1", ")", "-", "n_annotators", ")", "/", "(", "n_annotators", "*", "(", "n_annotators", "-", "1", ")", ")", "\n", "Pbar", "=", "np", ".", "sum", "(", "P", ")", "/", "N", "\n", "PbarE", "=", "np", ".", "sum", "(", "np", ".", "multiply", "(", "p", ",", "p", ")", ")", "\n", "\n", "kappa", "=", "(", "Pbar", "-", "PbarE", ")", "/", "(", "1", "-", "PbarE", ")", "\n", "return", "kappa", "\n", "\n"]], "home.repos.pwc.inspect_result.AADeLucia_gpt2-narrative-decoding.mturk_human_evaluation.fleiss.get_idxs": [[22, 34], ["set", "set", "set", "set", "enumerate", "header.split", "set.update", "set.update", "set.update", "set.update"], "function", ["None"], ["", "def", "get_idxs", "(", "header_row", ")", ":", "\n", "    ", "int_idx", "=", "set", "(", ")", ";", "coh_idx", "=", "set", "(", ")", ";", "flu_idx", "=", "set", "(", ")", ";", "rel_idx", "=", "set", "(", ")", "\n", "for", "i", ",", "column", "in", "enumerate", "(", "header", ".", "split", "(", "\",\"", ")", ")", ":", "\n", "        ", "if", "\"interesting\"", "in", "column", ":", "\n", "            ", "int_idx", ".", "update", "(", "[", "i", "]", ")", "\n", "", "elif", "\"coheren\"", "in", "column", ":", "\n", "            ", "coh_idx", ".", "update", "(", "[", "i", "]", ")", "\n", "", "elif", "\"fluen\"", "in", "column", ":", "\n", "            ", "flu_idx", ".", "update", "(", "[", "i", "]", ")", "\n", "", "elif", "\"relevan\"", "in", "column", ":", "\n", "            ", "rel_idx", ".", "update", "(", "[", "i", "]", ")", "\n", "", "", "return", "(", "int_idx", ",", "coh_idx", ",", "flu_idx", ",", "rel_idx", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.AADeLucia_gpt2-narrative-decoding.mturk_human_evaluation.create_html_survey.create_option_list": [[155, 182], ["re.sub", "re.sub", "metric_info[].items", "re.sub"], "function", ["None"], ["def", "create_option_list", "(", "item_id", ",", "metric", ",", "answer", "=", "None", ")", ":", "\n", "    ", "metric_info", "=", "metrics_dict", "[", "metric", "]", "\n", "# Question", "\n", "hover_label", "=", "re", ".", "sub", "(", "\"<\\/?\\w{1,}>\"", ",", "\"\"", ",", "metric_info", "[", "\"definition\"", "]", ")", "\n", "hover_label", "=", "re", ".", "sub", "(", "\"\\\"\"", ",", "\"'\"", ",", "hover_label", ")", "\n", "output", "=", "f\"<p><span title=\\\"{hover_label}\\\">{metric_info['question']}</span></p>\\n\"", "\n", "\n", "# Create the radio buttons with labels above the button", "\n", "output", "+=", "'<div class=\"container\">\\n<div class=\"row justify-content-center\">'", "\n", "for", "value", ",", "info", "in", "metric_info", "[", "\"options\"", "]", ".", "items", "(", ")", ":", "\n", "# Hover label is the metric definition", "\n", "# remove <b>/<u> styles", "\n", "        ", "hover_label", "=", "re", ".", "sub", "(", "\"<\\/?\\w{1,}>\"", ",", "\"\"", ",", "info", "[", "\"definition\"", "]", ")", "\n", "output", "+=", "f'<div class=\"col-sm text-center\" title=\"{hover_label}\">\\n'", "\n", "# Row for label", "\n", "output", "+=", "f'<div class=\"row justify-content-center\"><label for=\"{item_id}-{metric}-{value}\">{info[\"label\"]}</label></div>\\n'", "\n", "# Row for button", "\n", "# Fill in answer if given (used for examples)", "\n", "if", "answer", ":", "\n", "            ", "output", "+=", "f'<div class=\"row justify-content-center\"><input id=\"{item_id}-{metric}-{value}\" type=\"radio\" name=\"{item_id}-{metric}\" value={value} {\"checked\" if answer==value else \"disabled\"}></div>\\n'", "\n", "", "else", ":", "\n", "            ", "output", "+=", "f'<div class=\"row justify-content-center\"><input id=\"{item_id}-{metric}-{value}\" type=\"radio\" name=\"{item_id}-{metric}\" value={value} required></div>\\n'", "\n", "# Close column", "\n", "", "output", "+=", "'</div>\\n'", "\n", "# Close row and container", "\n", "", "output", "+=", "'</div></div><br>\\n'", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.AADeLucia_gpt2-narrative-decoding.mturk_human_evaluation.create_html_survey.create_evaluation_display": [[184, 192], ["metrics_dict.keys", "create_html_survey.create_option_list", "answers.get"], "function", ["home.repos.pwc.inspect_result.AADeLucia_gpt2-narrative-decoding.mturk_human_evaluation.create_html_survey.create_option_list"], ["", "def", "create_evaluation_display", "(", "item_id", ",", "answers", "=", "{", "}", ")", ":", "\n", "    ", "\"\"\"\n    Use answers dict for demo examples\n    \"\"\"", "\n", "output", "=", "\"\"", "\n", "for", "metric", "in", "metrics_dict", ".", "keys", "(", ")", ":", "\n", "        ", "output", "+=", "create_option_list", "(", "item_id", ",", "metric", ",", "answers", ".", "get", "(", "metric", ")", ")", "\n", "", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.AADeLucia_gpt2-narrative-decoding.mturk_human_evaluation.create_html_survey.create_narrative_display": [[194, 206], ["create_html_survey.create_evaluation_display"], "function", ["home.repos.pwc.inspect_result.AADeLucia_gpt2-narrative-decoding.mturk_human_evaluation.create_html_survey.create_evaluation_display"], ["", "def", "create_narrative_display", "(", "display_num", ",", "item_id", ",", "story", ")", ":", "\n", "    ", "output", "=", "f\"\"\"<!-- Response {display_num} -->\n    <div>\n    <!-- Display generated response -->\n    <h3>Narrative {display_num}</h3>\n    <p class=\"story\">{story}</p>\n    <br/>\n    <h4>Evaluation</h4>\n    {create_evaluation_display(item_id)}\n    </div>\n    \"\"\"", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.AADeLucia_gpt2-narrative-decoding.mturk_human_evaluation.create_html_survey.create_examples_display": [[208, 223], ["enumerate", "create_html_survey.create_evaluation_display"], "function", ["home.repos.pwc.inspect_result.AADeLucia_gpt2-narrative-decoding.mturk_human_evaluation.create_html_survey.create_evaluation_display"], ["", "def", "create_examples_display", "(", ")", ":", "\n", "    ", "output", "=", "\"\"", "\n", "for", "i", ",", "example", "in", "enumerate", "(", "narrative_examples", ")", ":", "\n", "        ", "title", "=", "f\"demo{i}\"", "\n", "output", "+=", "f\"\"\"<div>\n        <h3>Demo Narrative {i+1}</h3>\n        <p class='story'>[PROMPT] {example[\"prompt\"]}<br><br>[RESPONSE] {example[\"story\"]}</p>\n        <br/>\n        <h4>Evaluation</h4>\n        <p><b>Comment:</b> {example[\"comment\"]}</p>\n        <br/>\n        {create_evaluation_display(title, example[\"answers\"])}\n        </div><hr>\n        \"\"\"", "\n", "", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.AADeLucia_gpt2-narrative-decoding.mturk_human_evaluation.cohen.print_kappas": [[6, 24], ["print", "print", "print", "print", "print", "print"], "function", ["None"], ["def", "print_kappas", "(", "reviewer_kappas", ",", "reviewer_id", ")", ":", "\n", "    ", "\"\"\"\n    Pretty-print method for displaying a reviewer's average cohen kappa with all\n    other reviewers.\n    \"\"\"", "\n", "print", "(", "\"Reviewer {}:\"", ".", "format", "(", "reviewer_id", ")", ")", "\n", "int_sum", ",", "coh_sum", ",", "flu_sum", ",", "rel_sum", ",", "denom", "=", "(", "0", ",", "0", ",", "0", ",", "0", ",", "0", ")", "\n", "for", "kappas", "in", "reviewer_kappas", "[", "reviewer_id", "]", ":", "\n", "        ", "int_sum", "+=", "kappas", "[", "0", "]", "*", "kappas", "[", "4", "]", "\n", "coh_sum", "+=", "kappas", "[", "1", "]", "*", "kappas", "[", "4", "]", "\n", "flu_sum", "+=", "kappas", "[", "2", "]", "*", "kappas", "[", "4", "]", "\n", "rel_sum", "+=", "kappas", "[", "3", "]", "*", "kappas", "[", "4", "]", "\n", "denom", "+=", "kappas", "[", "4", "]", "\n", "", "print", "(", "\"\\tInterestingness: {:.3f}\"", ".", "format", "(", "int_sum", "/", "denom", ")", ")", "\n", "print", "(", "\"\\tCoherence: {:.3f}\"", ".", "format", "(", "coh_sum", "/", "denom", ")", ")", "\n", "print", "(", "\"\\tFluency: {:.3f}\"", ".", "format", "(", "flu_sum", "/", "denom", ")", ")", "\n", "print", "(", "\"\\tRelevance: {:.3f}\"", ".", "format", "(", "rel_sum", "/", "denom", ")", ")", "\n", "print", "(", "\"\\tMean: {:.3f}\"", ".", "format", "(", "(", "int_sum", "+", "coh_sum", "+", "flu_sum", "+", "rel_sum", ")", "/", "(", "4.0", "*", "denom", ")", ")", ")", "\n", "\n"]]}