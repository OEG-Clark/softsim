{"home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.None.HiGraph.HSumGraph.__init__": [[36, 81], ["torch.Module.__init__", "HiGraph.HSumGraph._init_sn_param", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "module.GAT.WSWGAT", "module.GAT.WSWGAT", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear"], "methods", ["home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.module.GATStackLayer.MultiHeadLayer.__init__", "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.None.HiGraph.HSumGraph._init_sn_param"], ["def", "__init__", "(", "self", ",", "hps", ",", "embed", ")", ":", "\n", "        ", "\"\"\"\n\n        :param hps: \n        :param embed: word embedding\n        \"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "_hps", "=", "hps", "\n", "self", ".", "_n_iter", "=", "hps", ".", "n_iter", "\n", "self", ".", "_embed", "=", "embed", "\n", "self", ".", "embed_size", "=", "hps", ".", "word_emb_dim", "\n", "\n", "\n", "# sent node feature", "\n", "self", ".", "_init_sn_param", "(", ")", "\n", "self", ".", "_TFembed", "=", "nn", ".", "Embedding", "(", "10", ",", "hps", ".", "feat_embed_size", ")", "# box=10", "\n", "self", ".", "n_feature_proj", "=", "nn", ".", "Linear", "(", "hps", ".", "n_feature_size", "*", "2", ",", "hps", ".", "hidden_size", ",", "bias", "=", "False", ")", "\n", "\n", "# word -> sent", "\n", "embed_size", "=", "hps", ".", "word_emb_dim", "\n", "self", ".", "word2sent", "=", "WSWGAT", "(", "in_dim", "=", "embed_size", ",", "\n", "out_dim", "=", "hps", ".", "hidden_size", ",", "\n", "num_heads", "=", "hps", ".", "n_head", ",", "\n", "attn_drop_out", "=", "hps", ".", "atten_dropout_prob", ",", "\n", "ffn_inner_hidden_size", "=", "hps", ".", "ffn_inner_hidden_size", ",", "\n", "ffn_drop_out", "=", "hps", ".", "ffn_dropout_prob", ",", "\n", "feat_embed_size", "=", "hps", ".", "feat_embed_size", ",", "\n", "layerType", "=", "\"W2S\"", "\n", ")", "\n", "\n", "# sent -> word", "\n", "self", ".", "sent2word", "=", "WSWGAT", "(", "in_dim", "=", "hps", ".", "hidden_size", ",", "\n", "out_dim", "=", "embed_size", ",", "\n", "num_heads", "=", "6", ",", "\n", "attn_drop_out", "=", "hps", ".", "atten_dropout_prob", ",", "\n", "ffn_inner_hidden_size", "=", "hps", ".", "ffn_inner_hidden_size", ",", "\n", "ffn_drop_out", "=", "hps", ".", "ffn_dropout_prob", ",", "\n", "feat_embed_size", "=", "hps", ".", "feat_embed_size", ",", "\n", "layerType", "=", "\"S2W\"", "\n", ")", "\n", "\n", "# node classification", "\n", "self", ".", "n_feature", "=", "hps", ".", "hidden_size", "\n", "self", ".", "wh", "=", "nn", ".", "Linear", "(", "self", ".", "n_feature", ",", "2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.None.HiGraph.HSumGraph.forward": [[82, 111], ["HiGraph.HSumGraph.set_wnfeature", "HiGraph.HSumGraph.n_feature_proj", "HiGraph.HSumGraph.word2sent", "range", "HiGraph.HSumGraph.wh", "HiGraph.HSumGraph.set_snfeature", "HiGraph.HSumGraph.sent2word", "HiGraph.HSumGraph.word2sent"], "methods", ["home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.None.HiGraph.HSumGraph.set_wnfeature", "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.None.HiGraph.HSumGraph.set_snfeature"], ["", "def", "forward", "(", "self", ",", "graph", ")", ":", "\n", "        ", "\"\"\"\n        :param graph: [batch_size] * DGLGraph\n            node:\n                word: unit=0, dtype=0, id=(int)wordid in vocab\n                sentence: unit=1, dtype=1, words=tensor, position=int, label=tensor\n            edge:\n                word2sent, sent2word:  tffrac=int, type=0\n        :return: result: [sentnum, 2]\n        \"\"\"", "\n", "\n", "# word node init", "\n", "word_feature", "=", "self", ".", "set_wnfeature", "(", "graph", ")", "# [wnode, embed_size]", "\n", "\n", "sent_feature", "=", "self", ".", "n_feature_proj", "(", "self", ".", "set_snfeature", "(", "graph", ")", ")", "# [snode, n_feature_size]", "\n", "\n", "# the start state", "\n", "word_state", "=", "word_feature", "\n", "sent_state", "=", "self", ".", "word2sent", "(", "graph", ",", "word_feature", ",", "sent_feature", ")", "\n", "\n", "for", "i", "in", "range", "(", "self", ".", "_n_iter", ")", ":", "\n", "# sent -> word", "\n", "            ", "word_state", "=", "self", ".", "sent2word", "(", "graph", ",", "word_state", ",", "sent_state", ")", "\n", "# word -> sent", "\n", "sent_state", "=", "self", ".", "word2sent", "(", "graph", ",", "word_state", ",", "sent_state", ")", "\n", "\n", "", "result", "=", "self", ".", "wh", "(", "sent_state", ")", "\n", "\n", "return", "result", "\n", "\n"]], "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.None.HiGraph.HSumGraph._init_sn_param": [[112, 126], ["torch.Embedding.from_pretrained", "torch.Embedding.from_pretrained", "torch.Embedding.from_pretrained", "torch.Embedding.from_pretrained", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.LSTM", "torch.LSTM", "torch.LSTM", "torch.LSTM", "module.Encoder.sentEncoder", "module.PositionEmbedding.get_sinusoid_encoding_table", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear"], "methods", ["home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.module.PositionEmbedding.get_sinusoid_encoding_table"], ["", "def", "_init_sn_param", "(", "self", ")", ":", "\n", "        ", "self", ".", "sent_pos_embed", "=", "nn", ".", "Embedding", ".", "from_pretrained", "(", "\n", "get_sinusoid_encoding_table", "(", "self", ".", "_hps", ".", "doc_max_timesteps", "+", "1", ",", "self", ".", "embed_size", ",", "padding_idx", "=", "0", ")", ",", "\n", "freeze", "=", "True", ")", "\n", "self", ".", "cnn_proj", "=", "nn", ".", "Linear", "(", "self", ".", "embed_size", ",", "self", ".", "_hps", ".", "n_feature_size", ")", "\n", "self", ".", "lstm_hidden_state", "=", "self", ".", "_hps", ".", "lstm_hidden_state", "\n", "self", ".", "lstm", "=", "nn", ".", "LSTM", "(", "self", ".", "embed_size", ",", "self", ".", "lstm_hidden_state", ",", "num_layers", "=", "self", ".", "_hps", ".", "lstm_layers", ",", "dropout", "=", "0.1", ",", "\n", "batch_first", "=", "True", ",", "bidirectional", "=", "self", ".", "_hps", ".", "bidirectional", ")", "\n", "if", "self", ".", "_hps", ".", "bidirectional", ":", "\n", "            ", "self", ".", "lstm_proj", "=", "nn", ".", "Linear", "(", "self", ".", "lstm_hidden_state", "*", "2", ",", "self", ".", "_hps", ".", "n_feature_size", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "lstm_proj", "=", "nn", ".", "Linear", "(", "self", ".", "lstm_hidden_state", ",", "self", ".", "_hps", ".", "n_feature_size", ")", "\n", "\n", "", "self", ".", "ngram_enc", "=", "sentEncoder", "(", "self", ".", "_hps", ",", "self", ".", "_embed", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.None.HiGraph.HSumGraph._sent_cnn_feature": [[127, 134], ["HiGraph.HSumGraph.ngram_enc.forward", "graph.nodes[].data[].view", "HiGraph.HSumGraph.sent_pos_embed", "HiGraph.HSumGraph.cnn_proj"], "methods", ["home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.module.GATStackLayer.MultiHeadLayer.forward"], ["", "def", "_sent_cnn_feature", "(", "self", ",", "graph", ",", "snode_id", ")", ":", "\n", "        ", "ngram_feature", "=", "self", ".", "ngram_enc", ".", "forward", "(", "graph", ".", "nodes", "[", "snode_id", "]", ".", "data", "[", "\"words\"", "]", ")", "# [snode, embed_size]", "\n", "graph", ".", "nodes", "[", "snode_id", "]", ".", "data", "[", "\"sent_embedding\"", "]", "=", "ngram_feature", "\n", "snode_pos", "=", "graph", ".", "nodes", "[", "snode_id", "]", ".", "data", "[", "\"position\"", "]", ".", "view", "(", "-", "1", ")", "# [n_nodes]", "\n", "position_embedding", "=", "self", ".", "sent_pos_embed", "(", "snode_pos", ")", "\n", "cnn_feature", "=", "self", ".", "cnn_proj", "(", "ngram_feature", "+", "position_embedding", ")", "\n", "return", "cnn_feature", "\n", "\n"]], "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.None.HiGraph.HSumGraph._sent_lstm_feature": [[135, 143], ["torch.pad_sequence", "torch.pad_sequence", "torch.pad_sequence", "torch.pad_sequence", "torch.pack_padded_sequence", "torch.pack_padded_sequence", "torch.pack_padded_sequence", "torch.pack_padded_sequence", "HiGraph.HSumGraph.lstm", "torch.pad_packed_sequence", "torch.pad_packed_sequence", "torch.pad_packed_sequence", "torch.pad_packed_sequence", "HiGraph.HSumGraph.lstm_proj", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "range", "len"], "methods", ["None"], ["", "def", "_sent_lstm_feature", "(", "self", ",", "features", ",", "glen", ")", ":", "\n", "        ", "pad_seq", "=", "rnn", ".", "pad_sequence", "(", "features", ",", "batch_first", "=", "True", ")", "\n", "lstm_input", "=", "rnn", ".", "pack_padded_sequence", "(", "pad_seq", ",", "glen", ",", "batch_first", "=", "True", ")", "\n", "lstm_output", ",", "_", "=", "self", ".", "lstm", "(", "lstm_input", ")", "\n", "unpacked", ",", "unpacked_len", "=", "rnn", ".", "pad_packed_sequence", "(", "lstm_output", ",", "batch_first", "=", "True", ")", "\n", "lstm_embedding", "=", "[", "unpacked", "[", "i", "]", "[", ":", "unpacked_len", "[", "i", "]", "]", "for", "i", "in", "range", "(", "len", "(", "unpacked", ")", ")", "]", "\n", "lstm_feature", "=", "self", ".", "lstm_proj", "(", "torch", ".", "cat", "(", "lstm_embedding", ",", "dim", "=", "0", ")", ")", "# [n_nodes, n_feature_size]", "\n", "return", "lstm_feature", "\n", "\n"]], "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.None.HiGraph.HSumGraph.set_wnfeature": [[144, 153], ["graph.filter_nodes", "graph.filter_edges", "HiGraph.HSumGraph._embed", "HiGraph.HSumGraph._TFembed"], "methods", ["None"], ["", "def", "set_wnfeature", "(", "self", ",", "graph", ")", ":", "\n", "        ", "wnode_id", "=", "graph", ".", "filter_nodes", "(", "lambda", "nodes", ":", "nodes", ".", "data", "[", "\"unit\"", "]", "==", "0", ")", "\n", "wsedge_id", "=", "graph", ".", "filter_edges", "(", "lambda", "edges", ":", "edges", ".", "data", "[", "\"dtype\"", "]", "==", "0", ")", "# for word to supernode(sent&doc)", "\n", "wid", "=", "graph", ".", "nodes", "[", "wnode_id", "]", ".", "data", "[", "\"id\"", "]", "# [n_wnodes]", "\n", "w_embed", "=", "self", ".", "_embed", "(", "wid", ")", "# [n_wnodes, D]", "\n", "graph", ".", "nodes", "[", "wnode_id", "]", ".", "data", "[", "\"embed\"", "]", "=", "w_embed", "\n", "etf", "=", "graph", ".", "edges", "[", "wsedge_id", "]", ".", "data", "[", "\"tffrac\"", "]", "\n", "graph", ".", "edges", "[", "wsedge_id", "]", ".", "data", "[", "\"tfidfembed\"", "]", "=", "self", ".", "_TFembed", "(", "etf", ")", "\n", "return", "w_embed", "\n", "\n"]], "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.None.HiGraph.HSumGraph.set_snfeature": [[154, 162], ["graph.filter_nodes", "HiGraph.HSumGraph._sent_cnn_feature", "HiGraph.get_snode_feat", "HiGraph.HSumGraph._sent_lstm_feature", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat"], "methods", ["home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.None.HiGraph.HSumGraph._sent_cnn_feature", "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.None.HiGraph.get_snode_feat", "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.None.HiGraph.HSumGraph._sent_lstm_feature"], ["", "def", "set_snfeature", "(", "self", ",", "graph", ")", ":", "\n", "# node feature", "\n", "        ", "snode_id", "=", "graph", ".", "filter_nodes", "(", "lambda", "nodes", ":", "nodes", ".", "data", "[", "\"dtype\"", "]", "==", "1", ")", "\n", "cnn_feature", "=", "self", ".", "_sent_cnn_feature", "(", "graph", ",", "snode_id", ")", "\n", "features", ",", "glen", "=", "get_snode_feat", "(", "graph", ",", "feat", "=", "\"sent_embedding\"", ")", "\n", "lstm_feature", "=", "self", ".", "_sent_lstm_feature", "(", "features", ",", "glen", ")", "\n", "node_feature", "=", "torch", ".", "cat", "(", "[", "cnn_feature", ",", "lstm_feature", "]", ",", "dim", "=", "1", ")", "# [n_nodes, n_feature_size * 2]", "\n", "return", "node_feature", "\n", "\n"]], "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.None.HiGraph.HSumDocGraph.__init__": [[172, 176], ["HiGraph.HSumGraph.__init__", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear"], "methods", ["home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.module.GATStackLayer.MultiHeadLayer.__init__"], ["def", "__init__", "(", "self", ",", "hps", ",", "embed", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "hps", ",", "embed", ")", "\n", "self", ".", "dn_feature_proj", "=", "nn", ".", "Linear", "(", "hps", ".", "hidden_size", ",", "hps", ".", "hidden_size", ",", "bias", "=", "False", ")", "\n", "self", ".", "wh", "=", "nn", ".", "Linear", "(", "self", ".", "n_feature", "*", "2", ",", "2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.None.HiGraph.HSumDocGraph.forward": [[177, 229], ["graph.filter_nodes", "graph.filter_nodes", "graph.filter_nodes", "HiGraph.HSumDocGraph.set_wnfeature", "HiGraph.HSumDocGraph.n_feature_proj", "HiGraph.HSumDocGraph.set_dnfeature", "HiGraph.HSumDocGraph.dn_feature_proj", "HiGraph.HSumDocGraph.word2sent", "range", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "HiGraph.HSumDocGraph.wh", "HiGraph.HSumDocGraph.set_snfeature", "HiGraph.HSumDocGraph.sent2word", "HiGraph.HSumDocGraph.word2sent", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "s_state_list.append", "int"], "methods", ["home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.None.HiGraph.HSumGraph.set_wnfeature", "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.None.HiGraph.HSumDocGraph.set_dnfeature", "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.None.HiGraph.HSumGraph.set_snfeature"], ["", "def", "forward", "(", "self", ",", "graph", ")", ":", "\n", "        ", "\"\"\"\n        :param graph: [batch_size] * DGLGraph\n            node:\n                word: unit=0, dtype=0, id=(int)wordid in vocab\n                sentence: unit=1, dtype=1, words=tensor, position=int, label=tensor\n                document: unit=1, dtype=2\n            edge:\n                word2sent, sent2word: tffrac=int, type=0\n                word2doc, doc2word: tffrac=int, type=0\n                sent2doc: type=2\n        :return: result: [sentnum, 2]\n        \"\"\"", "\n", "\n", "snode_id", "=", "graph", ".", "filter_nodes", "(", "lambda", "nodes", ":", "nodes", ".", "data", "[", "\"dtype\"", "]", "==", "1", ")", "\n", "dnode_id", "=", "graph", ".", "filter_nodes", "(", "lambda", "nodes", ":", "nodes", ".", "data", "[", "\"dtype\"", "]", "==", "2", ")", "\n", "supernode_id", "=", "graph", ".", "filter_nodes", "(", "lambda", "nodes", ":", "nodes", ".", "data", "[", "\"unit\"", "]", "==", "1", ")", "\n", "\n", "# word node init", "\n", "word_feature", "=", "self", ".", "set_wnfeature", "(", "graph", ")", "# [wnode, embed_size]", "\n", "sent_feature", "=", "self", ".", "n_feature_proj", "(", "self", ".", "set_snfeature", "(", "graph", ")", ")", "# [snode, n_feature_size]", "\n", "\n", "# sent and doc node init", "\n", "graph", ".", "nodes", "[", "snode_id", "]", ".", "data", "[", "\"init_feature\"", "]", "=", "sent_feature", "\n", "doc_feature", ",", "snid2dnid", "=", "self", ".", "set_dnfeature", "(", "graph", ")", "\n", "doc_feature", "=", "self", ".", "dn_feature_proj", "(", "doc_feature", ")", "\n", "graph", ".", "nodes", "[", "dnode_id", "]", ".", "data", "[", "\"init_feature\"", "]", "=", "doc_feature", "\n", "\n", "# the start state", "\n", "word_state", "=", "word_feature", "\n", "sent_state", "=", "graph", ".", "nodes", "[", "supernode_id", "]", ".", "data", "[", "\"init_feature\"", "]", "\n", "sent_state", "=", "self", ".", "word2sent", "(", "graph", ",", "word_state", ",", "sent_state", ")", "\n", "\n", "for", "i", "in", "range", "(", "self", ".", "_n_iter", ")", ":", "\n", "# sent -> word", "\n", "            ", "word_state", "=", "self", ".", "sent2word", "(", "graph", ",", "word_state", ",", "sent_state", ")", "\n", "# word -> sent", "\n", "sent_state", "=", "self", ".", "word2sent", "(", "graph", ",", "word_state", ",", "sent_state", ")", "\n", "\n", "", "graph", ".", "nodes", "[", "supernode_id", "]", ".", "data", "[", "\"hidden_state\"", "]", "=", "sent_state", "\n", "\n", "# extract sentence nodes", "\n", "s_state_list", "=", "[", "]", "\n", "for", "snid", "in", "snode_id", ":", "\n", "            ", "d_state", "=", "graph", ".", "nodes", "[", "snid2dnid", "[", "int", "(", "snid", ")", "]", "]", ".", "data", "[", "\"hidden_state\"", "]", "\n", "s_state", "=", "graph", ".", "nodes", "[", "snid", "]", ".", "data", "[", "\"hidden_state\"", "]", "\n", "s_state", "=", "torch", ".", "cat", "(", "[", "s_state", ",", "d_state", "]", ",", "dim", "=", "-", "1", ")", "\n", "s_state_list", ".", "append", "(", "s_state", ")", "\n", "\n", "", "s_state", "=", "torch", ".", "cat", "(", "s_state_list", ",", "dim", "=", "0", ")", "\n", "result", "=", "self", ".", "wh", "(", "s_state", ")", "\n", "return", "result", "\n", "\n"]], "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.None.HiGraph.HSumDocGraph.set_dnfeature": [[231, 245], ["graph.filter_nodes", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "graph.nodes[].data[].mean", "node_feature_list.append", "torch.any", "torch.any", "torch.any", "torch.any", "torch.any", "torch.any", "torch.any", "torch.any", "torch.any", "torch.any", "torch.any", "torch.any", "torch.any", "torch.any", "torch.any", "torch.any", "graph.predecessors", "torch.isnan", "torch.isnan", "torch.isnan", "torch.isnan", "torch.isnan", "torch.isnan", "torch.isnan", "torch.isnan", "torch.isnan", "torch.isnan", "torch.isnan", "torch.isnan", "torch.isnan", "torch.isnan", "torch.isnan", "torch.isnan", "int"], "methods", ["None"], ["", "def", "set_dnfeature", "(", "self", ",", "graph", ")", ":", "\n", "        ", "\"\"\" init doc node by mean pooling on the its sent node (connected by the edges with type=1) \"\"\"", "\n", "dnode_id", "=", "graph", ".", "filter_nodes", "(", "lambda", "nodes", ":", "nodes", ".", "data", "[", "\"dtype\"", "]", "==", "2", ")", "\n", "node_feature_list", "=", "[", "]", "\n", "snid2dnid", "=", "{", "}", "\n", "for", "dnode", "in", "dnode_id", ":", "\n", "            ", "snodes", "=", "[", "nid", "for", "nid", "in", "graph", ".", "predecessors", "(", "dnode", ")", "if", "graph", ".", "nodes", "[", "nid", "]", ".", "data", "[", "\"dtype\"", "]", "==", "1", "]", "\n", "doc_feature", "=", "graph", ".", "nodes", "[", "snodes", "]", ".", "data", "[", "\"init_feature\"", "]", ".", "mean", "(", "dim", "=", "0", ")", "\n", "assert", "not", "torch", ".", "any", "(", "torch", ".", "isnan", "(", "doc_feature", ")", ")", ",", "\"doc_feature_element\"", "\n", "node_feature_list", ".", "append", "(", "doc_feature", ")", "\n", "for", "s", "in", "snodes", ":", "\n", "                ", "snid2dnid", "[", "int", "(", "s", ")", "]", "=", "dnode", "\n", "", "", "node_feature", "=", "torch", ".", "stack", "(", "node_feature_list", ")", "\n", "return", "node_feature", ",", "snid2dnid", "\n", "\n"]], "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.None.HiGraph.get_snode_feat": [[247, 256], ["dgl.unbatch", "g.filter_nodes", "feature.append", "glen.append", "len"], "function", ["None"], ["", "", "def", "get_snode_feat", "(", "G", ",", "feat", ")", ":", "\n", "    ", "glist", "=", "dgl", ".", "unbatch", "(", "G", ")", "\n", "feature", "=", "[", "]", "\n", "glen", "=", "[", "]", "\n", "for", "g", "in", "glist", ":", "\n", "        ", "snode_id", "=", "g", ".", "filter_nodes", "(", "lambda", "nodes", ":", "nodes", ".", "data", "[", "\"dtype\"", "]", "==", "1", ")", "\n", "feature", ".", "append", "(", "g", ".", "nodes", "[", "snode_id", "]", ".", "data", "[", "feat", "]", ")", "\n", "glen", ".", "append", "(", "len", "(", "snode_id", ")", ")", "\n", "", "return", "feature", ",", "glen", "\n", "", ""]], "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.None.train.save_model": [[43, 47], ["logger.info", "open", "torch.save", "model.state_dict"], "function", ["None"], ["def", "save_model", "(", "model", ",", "save_file", ")", ":", "\n", "    ", "with", "open", "(", "save_file", ",", "'wb'", ")", "as", "f", ":", "\n", "        ", "torch", ".", "save", "(", "model", ".", "state_dict", "(", ")", ",", "f", ")", "\n", "", "logger", ".", "info", "(", "'[INFO] Saving model to %s'", ",", "save_file", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.None.train.setup_training": [[49, 76], ["os.path.join", "os.path.exists", "logger.info", "os.path.join", "model.load_state_dict", "logger.info", "os.path.exists", "os.makedirs", "train.run_training", "torch.load", "shutil.rmtree", "logger.error", "train.save_model", "os.path.join"], "function", ["home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.None.train.run_training", "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.None.train.save_model"], ["", "def", "setup_training", "(", "model", ",", "train_loader", ",", "valid_loader", ",", "valset", ",", "hps", ")", ":", "\n", "    ", "\"\"\" Does setup before starting training (run_training)\n    \n        :param model: the model\n        :param train_loader: train dataset loader\n        :param valid_loader: valid dataset loader\n        :param valset: valid dataset which includes text and summary\n        :param hps: hps for model\n        :return: \n    \"\"\"", "\n", "\n", "train_dir", "=", "os", ".", "path", ".", "join", "(", "hps", ".", "save_root", ",", "\"train\"", ")", "\n", "if", "os", ".", "path", ".", "exists", "(", "train_dir", ")", "and", "hps", ".", "restore_model", "!=", "'None'", ":", "\n", "        ", "logger", ".", "info", "(", "\"[INFO] Restoring %s for training...\"", ",", "hps", ".", "restore_model", ")", "\n", "bestmodel_file", "=", "os", ".", "path", ".", "join", "(", "train_dir", ",", "hps", ".", "restore_model", ")", "\n", "model", ".", "load_state_dict", "(", "torch", ".", "load", "(", "bestmodel_file", ")", ")", "\n", "hps", ".", "save_root", "=", "hps", ".", "save_root", "+", "\"_reload\"", "\n", "", "else", ":", "\n", "        ", "logger", ".", "info", "(", "\"[INFO] Create new model for training...\"", ")", "\n", "if", "os", ".", "path", ".", "exists", "(", "train_dir", ")", ":", "shutil", ".", "rmtree", "(", "train_dir", ")", "\n", "os", ".", "makedirs", "(", "train_dir", ")", "\n", "\n", "", "try", ":", "\n", "        ", "run_training", "(", "model", ",", "train_loader", ",", "valid_loader", ",", "valset", ",", "hps", ",", "train_dir", ")", "\n", "", "except", "KeyboardInterrupt", ":", "\n", "        ", "logger", ".", "error", "(", "\"[Error] Caught keyboard interrupt on worker. Stopping supervisor...\"", ")", "\n", "save_model", "(", "model", ",", "os", ".", "path", ".", "join", "(", "train_dir", ",", "\"earlystop\"", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.None.train.run_training": [[78, 178], ["logger.info", "torch.optim.Adam", "torch.nn.CrossEntropyLoss", "range", "filter", "time.time", "enumerate", "logger.info", "train.run_eval", "model.parameters", "time.time", "model.train", "model.forward", "G.filter_nodes", "[].sum", "torch.nn.CrossEntropyLoss.unsqueeze", "dgl.sum_nodes", "loss.mean.mean", "torch.optim.Adam.zero_grad", "loss.mean.backward", "torch.optim.Adam.step", "float", "float", "max", "list", "logger.info", "len", "os.path.join", "logger.info", "train.save_model", "logger.error", "train.save_model", "G.to", "numpy.isfinite().numpy", "logger.error", "logger.info", "model.named_parameters", "Exception", "torch.nn.utils.clip_grad_norm_", "logger.info", "float", "float", "logger.error", "train.save_model", "sys.exit", "os.path.join", "torch.device", "torch.nn.CrossEntropyLoss.", "model.parameters", "model.named_parameters", "time.time", "os.path.join", "numpy.isfinite", "logger.info", "float", "loss.mean.data.cpu", "logger.debug", "logger.debug", "time.time", "param.grad.data.sum"], "function", ["home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.None.train.run_eval", "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.module.GATStackLayer.MultiHeadLayer.forward", "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.None.train.save_model", "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.None.train.save_model", "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.None.train.save_model"], ["", "", "def", "run_training", "(", "model", ",", "train_loader", ",", "valid_loader", ",", "valset", ",", "hps", ",", "train_dir", ")", ":", "\n", "    ", "'''  Repeatedly runs training iterations, logging loss to screen and log files\n    \n        :param model: the model\n        :param train_loader: train dataset loader\n        :param valid_loader: valid dataset loader\n        :param valset: valid dataset which includes text and summary\n        :param hps: hps for model\n        :param train_dir: where to save checkpoints\n        :return: \n    '''", "\n", "logger", ".", "info", "(", "\"[INFO] Starting run_training\"", ")", "\n", "\n", "optimizer", "=", "torch", ".", "optim", ".", "Adam", "(", "filter", "(", "lambda", "p", ":", "p", ".", "requires_grad", ",", "model", ".", "parameters", "(", ")", ")", ",", "lr", "=", "hps", ".", "lr", ")", "\n", "\n", "\n", "criterion", "=", "torch", ".", "nn", ".", "CrossEntropyLoss", "(", "reduction", "=", "'none'", ")", "\n", "\n", "best_train_loss", "=", "None", "\n", "best_loss", "=", "None", "\n", "best_F", "=", "None", "\n", "non_descent_cnt", "=", "0", "\n", "saveNo", "=", "0", "\n", "\n", "for", "epoch", "in", "range", "(", "1", ",", "hps", ".", "n_epochs", "+", "1", ")", ":", "\n", "        ", "epoch_loss", "=", "0.0", "\n", "train_loss", "=", "0.0", "\n", "epoch_start_time", "=", "time", ".", "time", "(", ")", "\n", "for", "i", ",", "(", "G", ",", "index", ")", "in", "enumerate", "(", "train_loader", ")", ":", "\n", "            ", "iter_start_time", "=", "time", ".", "time", "(", ")", "\n", "# if i > 10:", "\n", "#     break", "\n", "model", ".", "train", "(", ")", "\n", "\n", "if", "hps", ".", "cuda", ":", "\n", "                ", "G", ".", "to", "(", "torch", ".", "device", "(", "\"cuda\"", ")", ")", "\n", "\n", "", "outputs", "=", "model", ".", "forward", "(", "G", ")", "# [n_snodes, 2]", "\n", "snode_id", "=", "G", ".", "filter_nodes", "(", "lambda", "nodes", ":", "nodes", ".", "data", "[", "\"dtype\"", "]", "==", "1", ")", "\n", "label", "=", "G", ".", "ndata", "[", "\"label\"", "]", "[", "snode_id", "]", ".", "sum", "(", "-", "1", ")", "# [n_nodes]", "\n", "G", ".", "nodes", "[", "snode_id", "]", ".", "data", "[", "\"loss\"", "]", "=", "criterion", "(", "outputs", ",", "label", ")", ".", "unsqueeze", "(", "-", "1", ")", "# [n_nodes, 1]", "\n", "loss", "=", "dgl", ".", "sum_nodes", "(", "G", ",", "\"loss\"", ")", "# [batch_size, 1]", "\n", "loss", "=", "loss", ".", "mean", "(", ")", "\n", "\n", "if", "not", "(", "np", ".", "isfinite", "(", "loss", ".", "data", ".", "cpu", "(", ")", ")", ")", ".", "numpy", "(", ")", ":", "\n", "                ", "logger", ".", "error", "(", "\"train Loss is not finite. Stopping.\"", ")", "\n", "logger", ".", "info", "(", "loss", ")", "\n", "for", "name", ",", "param", "in", "model", ".", "named_parameters", "(", ")", ":", "\n", "                    ", "if", "param", ".", "requires_grad", ":", "\n", "                        ", "logger", ".", "info", "(", "name", ")", "\n", "# logger.info(param.grad.data.sum())", "\n", "", "", "raise", "Exception", "(", "\"train Loss is not finite. Stopping.\"", ")", "\n", "\n", "", "optimizer", ".", "zero_grad", "(", ")", "\n", "loss", ".", "backward", "(", ")", "\n", "if", "hps", ".", "grad_clip", ":", "\n", "                ", "torch", ".", "nn", ".", "utils", ".", "clip_grad_norm_", "(", "model", ".", "parameters", "(", ")", ",", "hps", ".", "max_grad_norm", ")", "\n", "\n", "", "optimizer", ".", "step", "(", ")", "\n", "\n", "train_loss", "+=", "float", "(", "loss", ".", "data", ")", "\n", "epoch_loss", "+=", "float", "(", "loss", ".", "data", ")", "\n", "\n", "if", "i", "%", "100", "==", "0", ":", "\n", "                ", "if", "_DEBUG_FLAG_", ":", "\n", "                    ", "for", "name", ",", "param", "in", "model", ".", "named_parameters", "(", ")", ":", "\n", "                        ", "if", "param", ".", "requires_grad", ":", "\n", "                            ", "logger", ".", "debug", "(", "name", ")", "\n", "logger", ".", "debug", "(", "param", ".", "grad", ".", "data", ".", "sum", "(", ")", ")", "\n", "", "", "", "logger", ".", "info", "(", "'       | end of iter {:3d} | time: {:5.2f}s | train loss {:5.4f} | '", "\n", ".", "format", "(", "i", ",", "(", "time", ".", "time", "(", ")", "-", "iter_start_time", ")", ",", "float", "(", "train_loss", "/", "100", ")", ")", ")", "\n", "train_loss", "=", "0.0", "\n", "\n", "", "", "if", "hps", ".", "lr_descent", ":", "\n", "            ", "new_lr", "=", "max", "(", "5e-6", ",", "hps", ".", "lr", "/", "(", "epoch", "+", "1", ")", ")", "\n", "for", "param_group", "in", "list", "(", "optimizer", ".", "param_groups", ")", ":", "\n", "                ", "param_group", "[", "'lr'", "]", "=", "new_lr", "\n", "", "logger", ".", "info", "(", "\"[INFO] The learning rate now is %f\"", ",", "new_lr", ")", "\n", "\n", "", "epoch_avg_loss", "=", "epoch_loss", "/", "len", "(", "train_loader", ")", "\n", "logger", ".", "info", "(", "'   | end of epoch {:3d} | time: {:5.2f}s | epoch train loss {:5.4f} | '", "\n", ".", "format", "(", "epoch", ",", "(", "time", ".", "time", "(", ")", "-", "epoch_start_time", ")", ",", "float", "(", "epoch_avg_loss", ")", ")", ")", "\n", "\n", "if", "not", "best_train_loss", "or", "epoch_avg_loss", "<", "best_train_loss", ":", "\n", "            ", "save_file", "=", "os", ".", "path", ".", "join", "(", "train_dir", ",", "\"bestmodel\"", ")", "\n", "logger", ".", "info", "(", "'[INFO] Found new best model with %.3f running_train_loss. Saving to %s'", ",", "float", "(", "epoch_avg_loss", ")", ",", "\n", "save_file", ")", "\n", "save_model", "(", "model", ",", "save_file", ")", "\n", "best_train_loss", "=", "epoch_avg_loss", "\n", "", "elif", "epoch_avg_loss", ">=", "best_train_loss", ":", "\n", "            ", "logger", ".", "error", "(", "\"[Error] training loss does not descent. Stopping supervisor...\"", ")", "\n", "save_model", "(", "model", ",", "os", ".", "path", ".", "join", "(", "train_dir", ",", "\"earlystop\"", ")", ")", "\n", "sys", ".", "exit", "(", "1", ")", "\n", "\n", "", "best_loss", ",", "best_F", ",", "non_descent_cnt", ",", "saveNo", "=", "run_eval", "(", "model", ",", "valid_loader", ",", "valset", ",", "hps", ",", "best_loss", ",", "best_F", ",", "non_descent_cnt", ",", "saveNo", ")", "\n", "\n", "if", "non_descent_cnt", ">=", "3", ":", "\n", "            ", "logger", ".", "error", "(", "\"[Error] val loss does not descent for three times. Stopping supervisor...\"", ")", "\n", "save_model", "(", "model", ",", "os", ".", "path", ".", "join", "(", "train_dir", ",", "\"earlystop\"", ")", ")", "\n", "return", "\n", "\n"]], "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.None.train.run_eval": [[180, 259], ["logger.info", "os.path.join", "model.eval", "time.time", "rouge.Rouge", "rouge.Rouge.get_scores", "logger.info", "logger.info", "Tester.SLTester.getMetric", "os.path.exists", "os.makedirs", "torch.no_grad", "Tester.SLTester", "enumerate", "logger.error", "os.path.join", "os.path.join", "Tester.SLTester.evaluation", "len", "len", "float", "logger.info", "logger.info", "open", "torch.save", "logger.info", "logger.info", "open", "torch.save", "G.to", "time.time", "float", "float", "float", "model.state_dict", "float", "float", "float", "model.state_dict", "torch.device"], "function", ["home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.None.Tester.SLTester.getMetric", "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.None.Tester.SLTester.evaluation"], ["", "", "", "def", "run_eval", "(", "model", ",", "loader", ",", "valset", ",", "hps", ",", "best_loss", ",", "best_F", ",", "non_descent_cnt", ",", "saveNo", ")", ":", "\n", "    ", "''' \n        Repeatedly runs eval iterations, logging to screen and writing summaries. Saves the model with the best loss seen so far.\n        :param model: the model\n        :param loader: valid dataset loader\n        :param valset: valid dataset which includes text and summary\n        :param hps: hps for model\n        :param best_loss: best valid loss so far\n        :param best_F: best valid F so far\n        :param non_descent_cnt: the number of non descent epoch (for early stop)\n        :param saveNo: the number of saved models (always keep best saveNo checkpoints)\n        :return: \n    '''", "\n", "logger", ".", "info", "(", "\"[INFO] Starting eval for this model ...\"", ")", "\n", "eval_dir", "=", "os", ".", "path", ".", "join", "(", "hps", ".", "save_root", ",", "\"eval\"", ")", "# make a subdir of the root dir for eval data", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "eval_dir", ")", ":", "os", ".", "makedirs", "(", "eval_dir", ")", "\n", "\n", "model", ".", "eval", "(", ")", "\n", "\n", "iter_start_time", "=", "time", ".", "time", "(", ")", "\n", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "        ", "tester", "=", "SLTester", "(", "model", ",", "hps", ".", "m", ")", "\n", "for", "i", ",", "(", "G", ",", "index", ")", "in", "enumerate", "(", "loader", ")", ":", "\n", "            ", "if", "hps", ".", "cuda", ":", "\n", "                ", "G", ".", "to", "(", "torch", ".", "device", "(", "\"cuda\"", ")", ")", "\n", "", "tester", ".", "evaluation", "(", "G", ",", "index", ",", "valset", ")", "\n", "\n", "", "", "running_avg_loss", "=", "tester", ".", "running_avg_loss", "\n", "\n", "if", "len", "(", "tester", ".", "hyps", ")", "==", "0", "or", "len", "(", "tester", ".", "refer", ")", "==", "0", ":", "\n", "        ", "logger", ".", "error", "(", "\"During testing, no hyps is selected!\"", ")", "\n", "return", "\n", "", "rouge", "=", "Rouge", "(", ")", "\n", "scores_all", "=", "rouge", ".", "get_scores", "(", "tester", ".", "hyps", ",", "tester", ".", "refer", ",", "avg", "=", "True", ")", "\n", "logger", ".", "info", "(", "'[INFO] End of valid | time: {:5.2f}s | valid loss {:5.4f} | '", ".", "format", "(", "(", "time", ".", "time", "(", ")", "-", "iter_start_time", ")", ",", "float", "(", "running_avg_loss", ")", ")", ")", "\n", "\n", "res", "=", "\"Rouge1:\\n\\tp:%.6f, r:%.6f, f:%.6f\\n\"", "%", "(", "\n", "scores_all", "[", "'rouge-1'", "]", "[", "'p'", "]", ",", "scores_all", "[", "'rouge-1'", "]", "[", "'r'", "]", ",", "scores_all", "[", "'rouge-1'", "]", "[", "'f'", "]", ")", "+", "\"Rouge2:\\n\\tp:%.6f, r:%.6f, f:%.6f\\n\"", "%", "(", "\n", "scores_all", "[", "'rouge-2'", "]", "[", "'p'", "]", ",", "scores_all", "[", "'rouge-2'", "]", "[", "'r'", "]", ",", "scores_all", "[", "'rouge-2'", "]", "[", "'f'", "]", ")", "+", "\"Rougel:\\n\\tp:%.6f, r:%.6f, f:%.6f\\n\"", "%", "(", "\n", "scores_all", "[", "'rouge-l'", "]", "[", "'p'", "]", ",", "scores_all", "[", "'rouge-l'", "]", "[", "'r'", "]", ",", "scores_all", "[", "'rouge-l'", "]", "[", "'f'", "]", ")", "\n", "logger", ".", "info", "(", "res", ")", "\n", "\n", "tester", ".", "getMetric", "(", ")", "\n", "F", "=", "tester", ".", "labelMetric", "\n", "\n", "if", "best_loss", "is", "None", "or", "running_avg_loss", "<", "best_loss", ":", "\n", "        ", "bestmodel_save_path", "=", "os", ".", "path", ".", "join", "(", "eval_dir", ",", "'bestmodel_%d'", "%", "(", "saveNo", "%", "3", ")", ")", "# this is where checkpoints of best models are saved", "\n", "if", "best_loss", "is", "not", "None", ":", "\n", "            ", "logger", ".", "info", "(", "\n", "'[INFO] Found new best model with %.6f running_avg_loss. The original loss is %.6f, Saving to %s'", ",", "\n", "float", "(", "running_avg_loss", ")", ",", "float", "(", "best_loss", ")", ",", "bestmodel_save_path", ")", "\n", "", "else", ":", "\n", "            ", "logger", ".", "info", "(", "\n", "'[INFO] Found new best model with %.6f running_avg_loss. The original loss is None, Saving to %s'", ",", "\n", "float", "(", "running_avg_loss", ")", ",", "bestmodel_save_path", ")", "\n", "", "with", "open", "(", "bestmodel_save_path", ",", "'wb'", ")", "as", "f", ":", "\n", "            ", "torch", ".", "save", "(", "model", ".", "state_dict", "(", ")", ",", "f", ")", "\n", "", "best_loss", "=", "running_avg_loss", "\n", "non_descent_cnt", "=", "0", "\n", "saveNo", "+=", "1", "\n", "", "else", ":", "\n", "        ", "non_descent_cnt", "+=", "1", "\n", "\n", "", "if", "best_F", "is", "None", "or", "best_F", "<", "F", ":", "\n", "        ", "bestmodel_save_path", "=", "os", ".", "path", ".", "join", "(", "eval_dir", ",", "'bestFmodel'", ")", "# this is where checkpoints of best models are saved", "\n", "if", "best_F", "is", "not", "None", ":", "\n", "            ", "logger", ".", "info", "(", "'[INFO] Found new best model with %.6f F. The original F is %.6f, Saving to %s'", ",", "float", "(", "F", ")", ",", "\n", "float", "(", "best_F", ")", ",", "bestmodel_save_path", ")", "\n", "", "else", ":", "\n", "            ", "logger", ".", "info", "(", "'[INFO] Found new best model with %.6f F. The original F is None, Saving to %s'", ",", "float", "(", "F", ")", ",", "\n", "bestmodel_save_path", ")", "\n", "", "with", "open", "(", "bestmodel_save_path", ",", "'wb'", ")", "as", "f", ":", "\n", "            ", "torch", ".", "save", "(", "model", ".", "state_dict", "(", ")", ",", "f", ")", "\n", "", "best_F", "=", "F", "\n", "\n", "", "return", "best_loss", ",", "best_F", ",", "non_descent_cnt", ",", "saveNo", "\n", "\n"]], "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.None.train.main": [[261, 385], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "random.seed", "numpy.random.seed", "torch.manual_seed", "torch.set_printoptions", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "datetime.datetime.now().strftime", "os.path.join", "logging.FileHandler", "logging.FileHandler.setFormatter", "logger.addHandler", "logger.info", "logger.info", "module.vocabulary.Vocab", "torch.nn.Embedding", "logger.info", "os.path.join", "os.path.join", "train.setup_training", "os.path.exists", "os.makedirs", "module.vocabulary.Vocab.size", "module.embedding.Word_Embedding", "module.embedding.Word_Embedding.load_my_vecs", "module.embedding.Word_Embedding.add_unknown_words_by_avg", "torch.nn.Embedding.weight.data.copy_", "HiGraph.HSumGraph", "logger.info", "module.dataloader.ExampleSet", "torch.utils.data.DataLoader", "module.dataloader.ExampleSet", "torch.utils.data.DataLoader", "HiGraph.HSumDocGraph.to", "logger.info", "datetime.datetime.now", "torch.Tensor", "HiGraph.HSumDocGraph", "logger.info", "os.path.join", "module.dataloader.MultiExampleSet", "torch.utils.data.DataLoader", "os.path.join", "module.dataloader.MultiExampleSet", "torch.utils.data.DataLoader", "logger.error", "NotImplementedError", "torch.device"], "function", ["home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.None.train.setup_training", "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.module.vocabulary.Vocab.size", "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.module.embedding.Word_Embedding.load_my_vecs", "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.module.embedding.Word_Embedding.add_unknown_words_by_avg"], ["", "def", "main", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "description", "=", "'HeterSumGraph Model'", ")", "\n", "\n", "# Where to find data", "\n", "parser", ".", "add_argument", "(", "'--data_dir'", ",", "type", "=", "str", ",", "default", "=", "'data/CNNDM'", ",", "help", "=", "'The dataset directory.'", ")", "\n", "parser", ".", "add_argument", "(", "'--cache_dir'", ",", "type", "=", "str", ",", "default", "=", "'cache/CNNDM'", ",", "help", "=", "'The processed dataset directory'", ")", "\n", "parser", ".", "add_argument", "(", "'--embedding_path'", ",", "type", "=", "str", ",", "default", "=", "'/remote-home/dqwang/Glove/glove.42B.300d.txt'", ",", "help", "=", "'Path expression to external word embedding.'", ")", "\n", "\n", "# Important settings", "\n", "parser", ".", "add_argument", "(", "'--model'", ",", "type", "=", "str", ",", "default", "=", "'HSG'", ",", "help", "=", "'model structure[HSG|HDSG]'", ")", "\n", "parser", ".", "add_argument", "(", "'--restore_model'", ",", "type", "=", "str", ",", "default", "=", "'None'", ",", "help", "=", "'Restore model for further training. [bestmodel/bestFmodel/earlystop/None]'", ")", "\n", "\n", "# Where to save output", "\n", "parser", ".", "add_argument", "(", "'--save_root'", ",", "type", "=", "str", ",", "default", "=", "'save/'", ",", "help", "=", "'Root directory for all model.'", ")", "\n", "parser", ".", "add_argument", "(", "'--log_root'", ",", "type", "=", "str", ",", "default", "=", "'log/'", ",", "help", "=", "'Root directory for all logging.'", ")", "\n", "\n", "# Hyperparameters", "\n", "parser", ".", "add_argument", "(", "'--seed'", ",", "type", "=", "int", ",", "default", "=", "666", ",", "help", "=", "'set the random seed [default: 666]'", ")", "\n", "parser", ".", "add_argument", "(", "'--gpu'", ",", "type", "=", "str", ",", "default", "=", "'0'", ",", "help", "=", "'GPU ID to use. [default: 0]'", ")", "\n", "parser", ".", "add_argument", "(", "'--cuda'", ",", "action", "=", "'store_true'", ",", "default", "=", "False", ",", "help", "=", "'GPU or CPU [default: False]'", ")", "\n", "parser", ".", "add_argument", "(", "'--vocab_size'", ",", "type", "=", "int", ",", "default", "=", "50000", ",", "help", "=", "'Size of vocabulary. [default: 50000]'", ")", "\n", "parser", ".", "add_argument", "(", "'--n_epochs'", ",", "type", "=", "int", ",", "default", "=", "20", ",", "help", "=", "'Number of epochs [default: 20]'", ")", "\n", "parser", ".", "add_argument", "(", "'--batch_size'", ",", "type", "=", "int", ",", "default", "=", "32", ",", "help", "=", "'Mini batch size [default: 32]'", ")", "\n", "parser", ".", "add_argument", "(", "'--n_iter'", ",", "type", "=", "int", ",", "default", "=", "1", ",", "help", "=", "'iteration hop [default: 1]'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--word_embedding'", ",", "action", "=", "'store_true'", ",", "default", "=", "True", ",", "help", "=", "'whether to use Word embedding [default: True]'", ")", "\n", "parser", ".", "add_argument", "(", "'--word_emb_dim'", ",", "type", "=", "int", ",", "default", "=", "300", ",", "help", "=", "'Word embedding size [default: 300]'", ")", "\n", "parser", ".", "add_argument", "(", "'--embed_train'", ",", "action", "=", "'store_true'", ",", "default", "=", "False", ",", "help", "=", "'whether to train Word embedding [default: False]'", ")", "\n", "parser", ".", "add_argument", "(", "'--feat_embed_size'", ",", "type", "=", "int", ",", "default", "=", "50", ",", "help", "=", "'feature embedding size [default: 50]'", ")", "\n", "parser", ".", "add_argument", "(", "'--n_layers'", ",", "type", "=", "int", ",", "default", "=", "1", ",", "help", "=", "'Number of GAT layers [default: 1]'", ")", "\n", "parser", ".", "add_argument", "(", "'--lstm_hidden_state'", ",", "type", "=", "int", ",", "default", "=", "128", ",", "help", "=", "'size of lstm hidden state [default: 128]'", ")", "\n", "parser", ".", "add_argument", "(", "'--lstm_layers'", ",", "type", "=", "int", ",", "default", "=", "2", ",", "help", "=", "'Number of lstm layers [default: 2]'", ")", "\n", "parser", ".", "add_argument", "(", "'--bidirectional'", ",", "action", "=", "'store_true'", ",", "default", "=", "True", ",", "help", "=", "'whether to use bidirectional LSTM [default: True]'", ")", "\n", "parser", ".", "add_argument", "(", "'--n_feature_size'", ",", "type", "=", "int", ",", "default", "=", "128", ",", "help", "=", "'size of node feature [default: 128]'", ")", "\n", "parser", ".", "add_argument", "(", "'--hidden_size'", ",", "type", "=", "int", ",", "default", "=", "64", ",", "help", "=", "'hidden size [default: 64]'", ")", "\n", "parser", ".", "add_argument", "(", "'--ffn_inner_hidden_size'", ",", "type", "=", "int", ",", "default", "=", "512", ",", "help", "=", "'PositionwiseFeedForward inner hidden size [default: 512]'", ")", "\n", "parser", ".", "add_argument", "(", "'--n_head'", ",", "type", "=", "int", ",", "default", "=", "8", ",", "help", "=", "'multihead attention number [default: 8]'", ")", "\n", "parser", ".", "add_argument", "(", "'--recurrent_dropout_prob'", ",", "type", "=", "float", ",", "default", "=", "0.1", ",", "help", "=", "'recurrent dropout prob [default: 0.1]'", ")", "\n", "parser", ".", "add_argument", "(", "'--atten_dropout_prob'", ",", "type", "=", "float", ",", "default", "=", "0.1", ",", "help", "=", "'attention dropout prob [default: 0.1]'", ")", "\n", "parser", ".", "add_argument", "(", "'--ffn_dropout_prob'", ",", "type", "=", "float", ",", "default", "=", "0.1", ",", "help", "=", "'PositionwiseFeedForward dropout prob [default: 0.1]'", ")", "\n", "parser", ".", "add_argument", "(", "'--use_orthnormal_init'", ",", "action", "=", "'store_true'", ",", "default", "=", "True", ",", "help", "=", "'use orthnormal init for lstm [default: True]'", ")", "\n", "parser", ".", "add_argument", "(", "'--sent_max_len'", ",", "type", "=", "int", ",", "default", "=", "100", ",", "help", "=", "'max length of sentences (max source text sentence tokens)'", ")", "\n", "parser", ".", "add_argument", "(", "'--doc_max_timesteps'", ",", "type", "=", "int", ",", "default", "=", "50", ",", "help", "=", "'max length of documents (max timesteps of documents)'", ")", "\n", "\n", "# Training", "\n", "parser", ".", "add_argument", "(", "'--lr'", ",", "type", "=", "float", ",", "default", "=", "0.0005", ",", "help", "=", "'learning rate'", ")", "\n", "parser", ".", "add_argument", "(", "'--lr_descent'", ",", "action", "=", "'store_true'", ",", "default", "=", "False", ",", "help", "=", "'learning rate descent'", ")", "\n", "parser", ".", "add_argument", "(", "'--grad_clip'", ",", "action", "=", "'store_true'", ",", "default", "=", "False", ",", "help", "=", "'for gradient clipping'", ")", "\n", "parser", ".", "add_argument", "(", "'--max_grad_norm'", ",", "type", "=", "float", ",", "default", "=", "1.0", ",", "help", "=", "'for gradient clipping max gradient normalization'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'-m'", ",", "type", "=", "int", ",", "default", "=", "3", ",", "help", "=", "'decode summary length'", ")", "\n", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "# set the seed", "\n", "random", ".", "seed", "(", "args", ".", "seed", ")", "\n", "np", ".", "random", ".", "seed", "(", "args", ".", "seed", ")", "\n", "torch", ".", "manual_seed", "(", "args", ".", "seed", ")", "\n", "\n", "os", ".", "environ", "[", "'CUDA_VISIBLE_DEVICES'", "]", "=", "args", ".", "gpu", "\n", "torch", ".", "set_printoptions", "(", "threshold", "=", "50000", ")", "\n", "\n", "# File paths", "\n", "DATA_FILE", "=", "os", ".", "path", ".", "join", "(", "args", ".", "data_dir", ",", "\"train.label.jsonl\"", ")", "\n", "VALID_FILE", "=", "os", ".", "path", ".", "join", "(", "args", ".", "data_dir", ",", "\"val.label.jsonl\"", ")", "\n", "VOCAL_FILE", "=", "os", ".", "path", ".", "join", "(", "args", ".", "cache_dir", ",", "\"vocab\"", ")", "\n", "FILTER_WORD", "=", "os", ".", "path", ".", "join", "(", "args", ".", "cache_dir", ",", "\"filter_word.txt\"", ")", "\n", "LOG_PATH", "=", "args", ".", "log_root", "\n", "\n", "\n", "# train_log setting", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "LOG_PATH", ")", ":", "\n", "        ", "os", ".", "makedirs", "(", "LOG_PATH", ")", "\n", "", "nowTime", "=", "datetime", ".", "datetime", ".", "now", "(", ")", ".", "strftime", "(", "'%Y%m%d_%H%M%S'", ")", "\n", "log_path", "=", "os", ".", "path", ".", "join", "(", "LOG_PATH", ",", "\"train_\"", "+", "nowTime", ")", "\n", "file_handler", "=", "logging", ".", "FileHandler", "(", "log_path", ")", "\n", "file_handler", ".", "setFormatter", "(", "formatter", ")", "\n", "logger", ".", "addHandler", "(", "file_handler", ")", "\n", "\n", "logger", ".", "info", "(", "\"Pytorch %s\"", ",", "torch", ".", "__version__", ")", "\n", "logger", ".", "info", "(", "\"[INFO] Create Vocab, vocab path is %s\"", ",", "VOCAL_FILE", ")", "\n", "vocab", "=", "Vocab", "(", "VOCAL_FILE", ",", "args", ".", "vocab_size", ")", "\n", "embed", "=", "torch", ".", "nn", ".", "Embedding", "(", "vocab", ".", "size", "(", ")", ",", "args", ".", "word_emb_dim", ",", "padding_idx", "=", "0", ")", "\n", "if", "args", ".", "word_embedding", ":", "\n", "        ", "embed_loader", "=", "Word_Embedding", "(", "args", ".", "embedding_path", ",", "vocab", ")", "\n", "vectors", "=", "embed_loader", ".", "load_my_vecs", "(", "args", ".", "word_emb_dim", ")", "\n", "pretrained_weight", "=", "embed_loader", ".", "add_unknown_words_by_avg", "(", "vectors", ",", "args", ".", "word_emb_dim", ")", "\n", "embed", ".", "weight", ".", "data", ".", "copy_", "(", "torch", ".", "Tensor", "(", "pretrained_weight", ")", ")", "\n", "embed", ".", "weight", ".", "requires_grad", "=", "args", ".", "embed_train", "\n", "\n", "", "hps", "=", "args", "\n", "logger", ".", "info", "(", "hps", ")", "\n", "\n", "train_w2s_path", "=", "os", ".", "path", ".", "join", "(", "args", ".", "cache_dir", ",", "\"train.w2s.tfidf.jsonl\"", ")", "\n", "val_w2s_path", "=", "os", ".", "path", ".", "join", "(", "args", ".", "cache_dir", ",", "\"val.w2s.tfidf.jsonl\"", ")", "\n", "\n", "if", "hps", ".", "model", "==", "\"HSG\"", ":", "\n", "        ", "model", "=", "HSumGraph", "(", "hps", ",", "embed", ")", "\n", "logger", ".", "info", "(", "\"[MODEL] HeterSumGraph \"", ")", "\n", "dataset", "=", "ExampleSet", "(", "DATA_FILE", ",", "vocab", ",", "hps", ".", "doc_max_timesteps", ",", "hps", ".", "sent_max_len", ",", "FILTER_WORD", ",", "train_w2s_path", ")", "\n", "train_loader", "=", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "dataset", ",", "batch_size", "=", "hps", ".", "batch_size", ",", "shuffle", "=", "True", ",", "num_workers", "=", "32", ",", "collate_fn", "=", "graph_collate_fn", ")", "\n", "del", "dataset", "\n", "valid_dataset", "=", "ExampleSet", "(", "VALID_FILE", ",", "vocab", ",", "hps", ".", "doc_max_timesteps", ",", "hps", ".", "sent_max_len", ",", "FILTER_WORD", ",", "val_w2s_path", ")", "\n", "valid_loader", "=", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "valid_dataset", ",", "batch_size", "=", "hps", ".", "batch_size", ",", "shuffle", "=", "False", ",", "collate_fn", "=", "graph_collate_fn", ",", "num_workers", "=", "32", ")", "\n", "", "elif", "hps", ".", "model", "==", "\"HDSG\"", ":", "\n", "        ", "model", "=", "HSumDocGraph", "(", "hps", ",", "embed", ")", "\n", "logger", ".", "info", "(", "\"[MODEL] HeterDocSumGraph \"", ")", "\n", "train_w2d_path", "=", "os", ".", "path", ".", "join", "(", "args", ".", "cache_dir", ",", "\"train.w2d.tfidf.jsonl\"", ")", "\n", "dataset", "=", "MultiExampleSet", "(", "DATA_FILE", ",", "vocab", ",", "hps", ".", "doc_max_timesteps", ",", "hps", ".", "sent_max_len", ",", "FILTER_WORD", ",", "train_w2s_path", ",", "train_w2d_path", ")", "\n", "train_loader", "=", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "dataset", ",", "batch_size", "=", "hps", ".", "batch_size", ",", "shuffle", "=", "True", ",", "num_workers", "=", "32", ",", "collate_fn", "=", "graph_collate_fn", ")", "\n", "del", "dataset", "\n", "val_w2d_path", "=", "os", ".", "path", ".", "join", "(", "args", ".", "cache_dir", ",", "\"val.w2d.tfidf.jsonl\"", ")", "\n", "valid_dataset", "=", "MultiExampleSet", "(", "VALID_FILE", ",", "vocab", ",", "hps", ".", "doc_max_timesteps", ",", "hps", ".", "sent_max_len", ",", "FILTER_WORD", ",", "val_w2s_path", ",", "val_w2d_path", ")", "\n", "valid_loader", "=", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "valid_dataset", ",", "batch_size", "=", "hps", ".", "batch_size", ",", "shuffle", "=", "False", ",", "collate_fn", "=", "graph_collate_fn", ",", "num_workers", "=", "32", ")", "# Shuffle Must be False for ROUGE evaluation", "\n", "", "else", ":", "\n", "        ", "logger", ".", "error", "(", "\"[ERROR] Invalid Model Type!\"", ")", "\n", "raise", "NotImplementedError", "(", "\"Model Type has not been implemented\"", ")", "\n", "\n", "\n", "", "if", "args", ".", "cuda", ":", "\n", "        ", "model", ".", "to", "(", "torch", ".", "device", "(", "\"cuda:0\"", ")", ")", "\n", "logger", ".", "info", "(", "\"[INFO] Use cuda\"", ")", "\n", "\n", "", "setup_training", "(", "model", ",", "train_loader", ",", "valid_loader", ",", "valid_dataset", ",", "hps", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.None.Tester.TestPipLine.__init__": [[9, 29], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "model", ",", "m", ",", "test_dir", ",", "limited", ")", ":", "\n", "        ", "\"\"\"\n            :param model: the model\n            :param m: the number of sentence to select\n            :param test_dir: for saving decode files\n            :param limited: for limited Recall evaluation\n        \"\"\"", "\n", "self", ".", "model", "=", "model", "\n", "self", ".", "limited", "=", "limited", "\n", "self", ".", "m", "=", "m", "\n", "self", ".", "test_dir", "=", "test_dir", "\n", "self", ".", "extracts", "=", "[", "]", "\n", "\n", "self", ".", "batch_number", "=", "0", "\n", "self", ".", "running_loss", "=", "0", "\n", "self", ".", "example_num", "=", "0", "\n", "self", ".", "total_sentence_num", "=", "0", "\n", "\n", "self", ".", "_hyps", "=", "[", "]", "\n", "self", ".", "_refer", "=", "[", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.None.Tester.TestPipLine.evaluation": [[30, 32], ["None"], "methods", ["None"], ["", "def", "evaluation", "(", "self", ",", "G", ",", "index", ",", "valset", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.None.Tester.TestPipLine.getMetric": [[33, 35], ["None"], "methods", ["None"], ["", "def", "getMetric", "(", "self", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.None.Tester.TestPipLine.SaveDecodeFile": [[36, 50], ["datetime.datetime.now().strftime", "os.path.join", "open", "range", "datetime.datetime.now", "resfile.write", "resfile.write", "resfile.write", "resfile.write", "resfile.write", "resfile.write", "resfile.write", "resfile.write", "Tester.TestPipLine._refer[].encode", "Tester.TestPipLine._hyps[].encode"], "methods", ["None"], ["", "def", "SaveDecodeFile", "(", "self", ")", ":", "\n", "        ", "import", "datetime", "\n", "nowTime", "=", "datetime", ".", "datetime", ".", "now", "(", ")", ".", "strftime", "(", "'%Y%m%d_%H%M%S'", ")", "# \u73b0\u5728", "\n", "log_dir", "=", "os", ".", "path", ".", "join", "(", "self", ".", "test_dir", ",", "nowTime", ")", "\n", "with", "open", "(", "log_dir", ",", "\"wb\"", ")", "as", "resfile", ":", "\n", "            ", "for", "i", "in", "range", "(", "self", ".", "rougePairNum", ")", ":", "\n", "                ", "resfile", ".", "write", "(", "b\"[Reference]\\t\"", ")", "\n", "resfile", ".", "write", "(", "self", ".", "_refer", "[", "i", "]", ".", "encode", "(", "'utf-8'", ")", ")", "\n", "resfile", ".", "write", "(", "b\"\\n\"", ")", "\n", "resfile", ".", "write", "(", "b\"[Hypothesis]\\t\"", ")", "\n", "resfile", ".", "write", "(", "self", ".", "_hyps", "[", "i", "]", ".", "encode", "(", "'utf-8'", ")", ")", "\n", "resfile", ".", "write", "(", "b\"\\n\"", ")", "\n", "resfile", ".", "write", "(", "b\"\\n\"", ")", "\n", "resfile", ".", "write", "(", "b\"\\n\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.None.Tester.TestPipLine.running_avg_loss": [[51, 54], ["None"], "methods", ["None"], ["", "", "", "@", "property", "\n", "def", "running_avg_loss", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "running_loss", "/", "self", ".", "batch_number", "\n", "\n"]], "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.None.Tester.TestPipLine.rougePairNum": [[55, 58], ["len"], "methods", ["None"], ["", "@", "property", "\n", "def", "rougePairNum", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "_hyps", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.None.Tester.TestPipLine.hyps": [[59, 70], ["range", "len", "hlist.append", "Tester.TestPipLine._refer[].split", "Tester.TestPipLine._hyps[].split"], "methods", ["None"], ["", "@", "property", "\n", "def", "hyps", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "limited", ":", "\n", "            ", "hlist", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "self", ".", "rougePairNum", ")", ":", "\n", "                ", "k", "=", "len", "(", "self", ".", "_refer", "[", "i", "]", ".", "split", "(", "\" \"", ")", ")", "\n", "lh", "=", "\" \"", ".", "join", "(", "self", ".", "_hyps", "[", "i", "]", ".", "split", "(", "\" \"", ")", "[", ":", "k", "]", ")", "\n", "hlist", ".", "append", "(", "lh", ")", "\n", "", "return", "hlist", "\n", "", "else", ":", "\n", "            ", "return", "self", ".", "_hyps", "\n", "\n"]], "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.None.Tester.TestPipLine.refer": [[71, 74], ["None"], "methods", ["None"], ["", "", "@", "property", "\n", "def", "refer", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "_refer", "\n", "\n"]], "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.None.Tester.TestPipLine.extractLabel": [[75, 78], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "extractLabel", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "extracts", "\n", "\n"]], "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.None.Tester.SLTester.__init__": [[81, 87], ["Tester.TestPipLine.__init__", "torch.nn.CrossEntropyLoss"], "methods", ["home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.module.GATStackLayer.MultiHeadLayer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "model", ",", "m", ",", "test_dir", "=", "None", ",", "limited", "=", "False", ",", "blocking_win", "=", "3", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "model", ",", "m", ",", "test_dir", ",", "limited", ")", "\n", "self", ".", "pred", ",", "self", ".", "true", ",", "self", ".", "match", ",", "self", ".", "match_true", "=", "0", ",", "0", ",", "0", ",", "0", "\n", "self", ".", "_F", "=", "0", "\n", "self", ".", "criterion", "=", "torch", ".", "nn", ".", "CrossEntropyLoss", "(", "reduction", "=", "'none'", ")", "\n", "self", ".", "blocking_win", "=", "blocking_win", "\n", "\n"]], "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.None.Tester.SLTester.evaluation": [[88, 144], ["Tester.SLTester.model.forward", "G.filter_nodes", "[].sum", "Tester.SLTester.criterion().unsqueeze", "dgl.sum_nodes", "loss.mean.mean.mean", "float", "dgl.unbatch", "range", "len", "dataset.get_example", "len", "g.filter_nodes", "len", "p_sent.view.view.view", "[].sum().squeeze().cpu", "Tester.SLTester.extracts.append", "torch.zeros().long.sum", "[].sum().squeeze().cpu.sum", "Tester.SLTester._hyps.append", "Tester.SLTester._refer.append", "Tester.SLTester.criterion", "[].long", "torch.zeros().long", "Tester.SLTester.tolist", "[].sum().squeeze", "p_sent.view.view.max", "Tester.SLTester.ngram_blocking", "torch.topk", "min", "min", "torch.zeros", "[].sum", "torch.arange"], "methods", ["home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.module.GATStackLayer.MultiHeadLayer.forward", "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.module.dataloader.MultiExampleSet.get_example", "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.None.Tester.SLTester.ngram_blocking"], ["", "def", "evaluation", "(", "self", ",", "G", ",", "index", ",", "dataset", ",", "blocking", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n            :param G: the model\n            :param index: list, example id\n            :param dataset: dataset which includes text and summary\n            :param blocking: bool, for n-gram blocking\n        \"\"\"", "\n", "self", ".", "batch_number", "+=", "1", "\n", "outputs", "=", "self", ".", "model", ".", "forward", "(", "G", ")", "\n", "# logger.debug(outputs)", "\n", "snode_id", "=", "G", ".", "filter_nodes", "(", "lambda", "nodes", ":", "nodes", ".", "data", "[", "\"dtype\"", "]", "==", "1", ")", "\n", "label", "=", "G", ".", "ndata", "[", "\"label\"", "]", "[", "snode_id", "]", ".", "sum", "(", "-", "1", ")", "# [n_nodes]", "\n", "G", ".", "nodes", "[", "snode_id", "]", ".", "data", "[", "\"loss\"", "]", "=", "self", ".", "criterion", "(", "outputs", ",", "label", ")", ".", "unsqueeze", "(", "-", "1", ")", "# [n_nodes, 1]", "\n", "loss", "=", "dgl", ".", "sum_nodes", "(", "G", ",", "\"loss\"", ")", "# [batch_size, 1]", "\n", "loss", "=", "loss", ".", "mean", "(", ")", "\n", "self", ".", "running_loss", "+=", "float", "(", "loss", ".", "data", ")", "\n", "\n", "G", ".", "nodes", "[", "snode_id", "]", ".", "data", "[", "\"p\"", "]", "=", "outputs", "\n", "glist", "=", "dgl", ".", "unbatch", "(", "G", ")", "\n", "for", "j", "in", "range", "(", "len", "(", "glist", ")", ")", ":", "\n", "            ", "idx", "=", "index", "[", "j", "]", "\n", "example", "=", "dataset", ".", "get_example", "(", "idx", ")", "\n", "original_article_sents", "=", "example", ".", "original_article_sents", "\n", "sent_max_number", "=", "len", "(", "original_article_sents", ")", "\n", "refer", "=", "example", ".", "original_abstract", "\n", "\n", "g", "=", "glist", "[", "j", "]", "\n", "snode_id", "=", "g", ".", "filter_nodes", "(", "lambda", "nodes", ":", "nodes", ".", "data", "[", "\"dtype\"", "]", "==", "1", ")", "\n", "N", "=", "len", "(", "snode_id", ")", "\n", "p_sent", "=", "g", ".", "ndata", "[", "\"p\"", "]", "[", "snode_id", "]", "\n", "p_sent", "=", "p_sent", ".", "view", "(", "-", "1", ",", "2", ")", "# [node, 2]", "\n", "label", "=", "g", ".", "ndata", "[", "\"label\"", "]", "[", "snode_id", "]", ".", "sum", "(", "-", "1", ")", ".", "squeeze", "(", ")", ".", "cpu", "(", ")", "# [n_node]", "\n", "if", "self", ".", "m", "==", "0", ":", "\n", "                ", "prediction", "=", "p_sent", ".", "max", "(", "1", ")", "[", "1", "]", "# [node]", "\n", "pred_idx", "=", "torch", ".", "arange", "(", "N", ")", "[", "prediction", "!=", "0", "]", ".", "long", "(", ")", "\n", "", "else", ":", "\n", "                ", "if", "blocking", ":", "\n", "                    ", "pred_idx", "=", "self", ".", "ngram_blocking", "(", "original_article_sents", ",", "p_sent", "[", ":", ",", "1", "]", ",", "self", ".", "blocking_win", ",", "min", "(", "self", ".", "m", ",", "N", ")", ")", "\n", "", "else", ":", "\n", "# print(p_sent.size())", "\n", "                    ", "topk", ",", "pred_idx", "=", "torch", ".", "topk", "(", "p_sent", "[", ":", ",", "1", "]", ",", "min", "(", "self", ".", "m", ",", "N", ")", ")", "\n", "", "prediction", "=", "torch", ".", "zeros", "(", "N", ")", ".", "long", "(", ")", "\n", "prediction", "[", "pred_idx", "]", "=", "1", "\n", "", "self", ".", "extracts", ".", "append", "(", "pred_idx", ".", "tolist", "(", ")", ")", "\n", "\n", "self", ".", "pred", "+=", "prediction", ".", "sum", "(", ")", "\n", "self", ".", "true", "+=", "label", ".", "sum", "(", ")", "\n", "\n", "self", ".", "match_true", "+=", "(", "(", "prediction", "==", "label", ")", "&", "(", "prediction", "==", "1", ")", ")", ".", "sum", "(", ")", "\n", "self", ".", "match", "+=", "(", "prediction", "==", "label", ")", ".", "sum", "(", ")", "\n", "self", ".", "total_sentence_num", "+=", "N", "\n", "self", ".", "example_num", "+=", "1", "\n", "hyps", "=", "\"\\n\"", ".", "join", "(", "original_article_sents", "[", "id", "]", "for", "id", "in", "pred_idx", "if", "id", "<", "sent_max_number", ")", "\n", "\n", "self", ".", "_hyps", ".", "append", "(", "hyps", ")", "\n", "self", ".", "_refer", ".", "append", "(", "refer", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.None.Tester.SLTester.getMetric": [[145, 153], ["logger.info", "tools.utils.eval_label", "logger.info"], "methods", ["home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.tools.utils.eval_label"], ["", "", "def", "getMetric", "(", "self", ")", ":", "\n", "        ", "logger", ".", "info", "(", "\"[INFO] Validset match_true %d, pred %d, true %d, total %d, match %d\"", ",", "\n", "self", ".", "match_true", ",", "self", ".", "pred", ",", "self", ".", "true", ",", "self", ".", "total_sentence_num", ",", "self", ".", "match", ")", "\n", "self", ".", "_accu", ",", "self", ".", "_precision", ",", "self", ".", "_recall", ",", "self", ".", "_F", "=", "eval_label", "(", "\n", "self", ".", "match_true", ",", "self", ".", "pred", ",", "self", ".", "true", ",", "self", ".", "total_sentence_num", ",", "self", ".", "match", ")", "\n", "logger", ".", "info", "(", "\n", "\"[INFO] The size of totalset is %d, sent_number is %d, accu is %f, precision is %f, recall is %f, F is %f\"", ",", "\n", "self", ".", "example_num", ",", "self", ".", "total_sentence_num", ",", "self", ".", "_accu", ",", "self", ".", "_precision", ",", "self", ".", "_recall", ",", "self", ".", "_F", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.None.Tester.SLTester.ngram_blocking": [[155, 185], ["p_sent.sort", "torch.LongTensor", "sent.split", "range", "torch.LongTensor.append", "ngram_list.extend", "len", "sent_ngram.append", "len"], "methods", ["None"], ["", "def", "ngram_blocking", "(", "self", ",", "sents", ",", "p_sent", ",", "n_win", ",", "k", ")", ":", "\n", "        ", "\"\"\"\n        \n        :param p_sent: [sent_num, 1]\n        :param n_win: int, n_win=2,3,4...\n        :return: \n        \"\"\"", "\n", "ngram_list", "=", "[", "]", "\n", "_", ",", "sorted_idx", "=", "p_sent", ".", "sort", "(", "descending", "=", "True", ")", "\n", "S", "=", "[", "]", "\n", "for", "idx", "in", "sorted_idx", ":", "\n", "            ", "sent", "=", "sents", "[", "idx", "]", "\n", "pieces", "=", "sent", ".", "split", "(", ")", "\n", "overlap_flag", "=", "0", "\n", "sent_ngram", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "pieces", ")", "-", "n_win", ")", ":", "\n", "                ", "ngram", "=", "\" \"", ".", "join", "(", "pieces", "[", "i", ":", "(", "i", "+", "n_win", ")", "]", ")", "\n", "if", "ngram", "in", "ngram_list", ":", "\n", "                    ", "overlap_flag", "=", "1", "\n", "break", "\n", "", "else", ":", "\n", "                    ", "sent_ngram", ".", "append", "(", "ngram", ")", "\n", "", "", "if", "overlap_flag", "==", "0", ":", "\n", "                ", "S", ".", "append", "(", "idx", ")", "\n", "ngram_list", ".", "extend", "(", "sent_ngram", ")", "\n", "if", "len", "(", "S", ")", ">=", "k", ":", "\n", "                    ", "break", "\n", "", "", "", "S", "=", "torch", ".", "LongTensor", "(", "S", ")", "\n", "# print(sorted_idx, S)", "\n", "return", "S", "\n", "\n"]], "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.None.Tester.SLTester.labelMetric": [[187, 190], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "labelMetric", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "_F", "\n", "\n"]], "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.None.evaluation.load_test_model": [[39, 60], ["model_name.startswith", "logger.info", "model.load_state_dict", "os.path.join", "model_name.startswith", "os.path.exists", "logger.error", "torch.load", "torch.load", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "logger.error", "ValueError"], "function", ["None"], ["def", "load_test_model", "(", "model", ",", "model_name", ",", "eval_dir", ",", "save_root", ")", ":", "\n", "    ", "\"\"\" choose which model will be loaded for evaluation \"\"\"", "\n", "if", "model_name", ".", "startswith", "(", "'eval'", ")", ":", "\n", "        ", "bestmodel_load_path", "=", "os", ".", "path", ".", "join", "(", "eval_dir", ",", "model_name", "[", "4", ":", "]", ")", "\n", "", "elif", "model_name", ".", "startswith", "(", "'train'", ")", ":", "\n", "        ", "train_dir", "=", "os", ".", "path", ".", "join", "(", "save_root", ",", "\"train\"", ")", "\n", "bestmodel_load_path", "=", "os", ".", "path", ".", "join", "(", "train_dir", ",", "model_name", "[", "5", ":", "]", ")", "\n", "", "elif", "model_name", "==", "\"earlystop\"", ":", "\n", "        ", "train_dir", "=", "os", ".", "path", ".", "join", "(", "save_root", ",", "\"train\"", ")", "\n", "bestmodel_load_path", "=", "os", ".", "path", ".", "join", "(", "train_dir", ",", "'earlystop'", ")", "\n", "", "else", ":", "\n", "        ", "logger", ".", "error", "(", "\"None of such model! Must be one of evalbestmodel/trainbestmodel/earlystop\"", ")", "\n", "raise", "ValueError", "(", "\"None of such model! Must be one of evalbestmodel/trainbestmodel/earlystop\"", ")", "\n", "", "if", "not", "os", ".", "path", ".", "exists", "(", "bestmodel_load_path", ")", ":", "\n", "        ", "logger", ".", "error", "(", "\"[ERROR] Restoring %s for testing...The path %s does not exist!\"", ",", "model_name", ",", "bestmodel_load_path", ")", "\n", "return", "None", "\n", "", "logger", ".", "info", "(", "\"[INFO] Restoring %s for testing...The path is %s\"", ",", "model_name", ",", "bestmodel_load_path", ")", "\n", "\n", "model", ".", "load_state_dict", "(", "torch", ".", "load", "(", "bestmodel_load_path", ")", ")", "\n", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.None.evaluation.run_test": [[63, 122], ["os.path.join", "os.path.join", "evaluation.load_test_model", "load_test_model.eval", "time.time", "logger.info", "logger.info", "Tester.SLTester.getMetric", "Tester.SLTester.SaveDecodeFile", "logger.info", "os.path.exists", "os.makedirs", "os.path.exists", "logger.exception", "Exception", "os.path.join", "open", "logger.info", "torch.no_grad", "torch.no_grad", "logger.info", "Tester.SLTester", "enumerate", "json.dump", "Tester.SLTester.SaveDecodeFile", "logger.info", "logger.error", "sys.exit", "isinstance", "rouge.Rouge", "rouge.Rouge.get_scores", "Tester.SLTester.evaluation", "logger.info", "tools.utils.pyrouge_score_all_multi", "tools.utils.pyrouge_score_all", "float", "hps.cache_dir.split", "G.to", "time.time", "torch.device", "torch.device", "time.time"], "function", ["home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.None.evaluation.load_test_model", "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.None.Tester.SLTester.getMetric", "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.None.Tester.TestPipLine.SaveDecodeFile", "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.None.Tester.TestPipLine.SaveDecodeFile", "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.None.Tester.SLTester.evaluation", "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.tools.utils.pyrouge_score_all_multi", "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.tools.utils.pyrouge_score_all"], ["", "def", "run_test", "(", "model", ",", "dataset", ",", "loader", ",", "model_name", ",", "hps", ")", ":", "\n", "    ", "test_dir", "=", "os", ".", "path", ".", "join", "(", "hps", ".", "save_root", ",", "\"test\"", ")", "# make a subdir of the root dir for eval data", "\n", "eval_dir", "=", "os", ".", "path", ".", "join", "(", "hps", ".", "save_root", ",", "\"eval\"", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "test_dir", ")", ":", "os", ".", "makedirs", "(", "test_dir", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "eval_dir", ")", ":", "\n", "        ", "logger", ".", "exception", "(", "\"[Error] eval_dir %s doesn't exist. Run in train mode to create it.\"", ",", "eval_dir", ")", "\n", "raise", "Exception", "(", "\"[Error] eval_dir %s doesn't exist. Run in train mode to create it.\"", "%", "(", "eval_dir", ")", ")", "\n", "\n", "", "resfile", "=", "None", "\n", "if", "hps", ".", "save_label", ":", "\n", "        ", "log_dir", "=", "os", ".", "path", ".", "join", "(", "test_dir", ",", "hps", ".", "cache_dir", ".", "split", "(", "\"/\"", ")", "[", "-", "1", "]", ")", "\n", "resfile", "=", "open", "(", "log_dir", ",", "\"w\"", ")", "\n", "logger", ".", "info", "(", "\"[INFO] Write the Evaluation into %s\"", ",", "log_dir", ")", "\n", "\n", "", "model", "=", "load_test_model", "(", "model", ",", "model_name", ",", "eval_dir", ",", "hps", ".", "save_root", ")", "\n", "model", ".", "eval", "(", ")", "\n", "\n", "iter_start_time", "=", "time", ".", "time", "(", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "        ", "logger", ".", "info", "(", "\"[Model] Sequence Labeling!\"", ")", "\n", "tester", "=", "SLTester", "(", "model", ",", "hps", ".", "m", ",", "limited", "=", "hps", ".", "limited", ",", "test_dir", "=", "test_dir", ")", "\n", "\n", "for", "i", ",", "(", "G", ",", "index", ")", "in", "enumerate", "(", "loader", ")", ":", "\n", "            ", "if", "hps", ".", "cuda", ":", "\n", "                ", "G", ".", "to", "(", "torch", ".", "device", "(", "\"cuda\"", ")", ")", "\n", "", "tester", ".", "evaluation", "(", "G", ",", "index", ",", "dataset", ",", "blocking", "=", "hps", ".", "blocking", ")", "\n", "\n", "", "", "running_avg_loss", "=", "tester", ".", "running_avg_loss", "\n", "\n", "if", "hps", ".", "save_label", ":", "\n", "# save label and do not calculate rouge", "\n", "        ", "json", ".", "dump", "(", "tester", ".", "extractLabel", ",", "resfile", ")", "\n", "tester", ".", "SaveDecodeFile", "(", ")", "\n", "logger", ".", "info", "(", "'   | end of test | time: {:5.2f}s | '", ".", "format", "(", "(", "time", ".", "time", "(", ")", "-", "iter_start_time", ")", ")", ")", "\n", "return", "\n", "\n", "", "logger", ".", "info", "(", "\"The number of pairs is %d\"", ",", "tester", ".", "rougePairNum", ")", "\n", "if", "not", "tester", ".", "rougePairNum", ":", "\n", "        ", "logger", ".", "error", "(", "\"During testing, no hyps is selected!\"", ")", "\n", "sys", ".", "exit", "(", "1", ")", "\n", "\n", "", "if", "hps", ".", "use_pyrouge", ":", "\n", "        ", "if", "isinstance", "(", "tester", ".", "refer", "[", "0", "]", ",", "list", ")", ":", "\n", "            ", "logger", ".", "info", "(", "\"Multi Reference summaries!\"", ")", "\n", "scores_all", "=", "utils", ".", "pyrouge_score_all_multi", "(", "tester", ".", "hyps", ",", "tester", ".", "refer", ")", "\n", "", "else", ":", "\n", "            ", "scores_all", "=", "utils", ".", "pyrouge_score_all", "(", "tester", ".", "hyps", ",", "tester", ".", "refer", ")", "\n", "", "", "else", ":", "\n", "        ", "rouge", "=", "Rouge", "(", ")", "\n", "scores_all", "=", "rouge", ".", "get_scores", "(", "tester", ".", "hyps", ",", "tester", ".", "refer", ",", "avg", "=", "True", ")", "\n", "\n", "", "res", "=", "\"Rouge1:\\n\\tp:%.6f, r:%.6f, f:%.6f\\n\"", "%", "(", "scores_all", "[", "'rouge-1'", "]", "[", "'p'", "]", ",", "scores_all", "[", "'rouge-1'", "]", "[", "'r'", "]", ",", "scores_all", "[", "'rouge-1'", "]", "[", "'f'", "]", ")", "+", "\"Rouge2:\\n\\tp:%.6f, r:%.6f, f:%.6f\\n\"", "%", "(", "scores_all", "[", "'rouge-2'", "]", "[", "'p'", "]", ",", "scores_all", "[", "'rouge-2'", "]", "[", "'r'", "]", ",", "scores_all", "[", "'rouge-2'", "]", "[", "'f'", "]", ")", "+", "\"Rougel:\\n\\tp:%.6f, r:%.6f, f:%.6f\\n\"", "%", "(", "scores_all", "[", "'rouge-l'", "]", "[", "'p'", "]", ",", "scores_all", "[", "'rouge-l'", "]", "[", "'r'", "]", ",", "scores_all", "[", "'rouge-l'", "]", "[", "'f'", "]", ")", "\n", "logger", ".", "info", "(", "res", ")", "\n", "\n", "tester", ".", "getMetric", "(", ")", "\n", "tester", ".", "SaveDecodeFile", "(", ")", "\n", "logger", ".", "info", "(", "'[INFO] End of test | time: {:5.2f}s | test loss {:5.4f} | '", ".", "format", "(", "(", "time", ".", "time", "(", ")", "-", "iter_start_time", ")", ",", "float", "(", "running_avg_loss", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.None.evaluation.main": [[125, 237], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "torch.set_printoptions", "torch.set_printoptions", "os.path.join", "os.path.join", "os.path.join", "datetime.datetime.now().strftime", "os.path.join", "logging.FileHandler", "logging.FileHandler.setFormatter", "logger.addHandler", "logger.info", "logger.info", "module.vocabulary.Vocab", "torch.nn.Embedding", "torch.nn.Embedding", "logger.info", "os.path.join", "logger.info", "os.path.exists", "logger.exception", "Exception", "module.vocabulary.Vocab.size", "module.embedding.Word_Embedding", "module.embedding.Word_Embedding.load_my_vecs", "module.embedding.Word_Embedding.add_unknown_words_by_avg", "torch.nn.Embedding.weight.data.copy_", "HiGraph.HSumGraph", "logger.info", "module.dataloader.ExampleSet", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "HiGraph.HSumDocGraph.to", "logger.info", "range", "evaluation.run_test", "datetime.datetime.now", "torch.Tensor", "torch.Tensor", "HiGraph.HSumDocGraph", "logger.info", "os.path.join", "module.dataloader.MultiExampleSet", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "logger.error", "NotImplementedError", "torch.device", "torch.device", "evaluation.run_test"], "function", ["home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.module.vocabulary.Vocab.size", "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.module.embedding.Word_Embedding.load_my_vecs", "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.module.embedding.Word_Embedding.add_unknown_words_by_avg", "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.None.evaluation.run_test", "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.None.evaluation.run_test"], ["", "def", "main", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "description", "=", "'HeterSumGraph Model'", ")", "\n", "\n", "# Where to find data", "\n", "parser", ".", "add_argument", "(", "'--data_dir'", ",", "type", "=", "str", ",", "default", "=", "'data/CNNDM'", ",", "help", "=", "'The dataset directory.'", ")", "\n", "parser", ".", "add_argument", "(", "'--cache_dir'", ",", "type", "=", "str", ",", "default", "=", "'cache/CNNDM'", ",", "help", "=", "'The processed dataset directory'", ")", "\n", "parser", ".", "add_argument", "(", "'--embedding_path'", ",", "type", "=", "str", ",", "default", "=", "'/remote-home/dqwang/Glove/glove.42B.300d.txt'", ",", "help", "=", "'Path expression to external word embedding.'", ")", "\n", "\n", "# Important settings", "\n", "parser", ".", "add_argument", "(", "'--model'", ",", "type", "=", "str", ",", "default", "=", "\"HSumGraph\"", ",", "help", "=", "\"model structure[HSG|HDSG]\"", ")", "\n", "parser", ".", "add_argument", "(", "'--test_model'", ",", "type", "=", "str", ",", "default", "=", "'evalbestmodel'", ",", "help", "=", "'choose different model to test [multi/evalbestmodel/trainbestmodel/earlystop]'", ")", "\n", "parser", ".", "add_argument", "(", "'--use_pyrouge'", ",", "action", "=", "'store_true'", ",", "default", "=", "False", ",", "help", "=", "'use_pyrouge'", ")", "\n", "\n", "# Where to save output", "\n", "parser", ".", "add_argument", "(", "'--save_root'", ",", "type", "=", "str", ",", "default", "=", "'save/'", ",", "help", "=", "'Root directory for all model.'", ")", "\n", "parser", ".", "add_argument", "(", "'--log_root'", ",", "type", "=", "str", ",", "default", "=", "'log/'", ",", "help", "=", "'Root directory for all logging.'", ")", "\n", "\n", "# Hyperparameters", "\n", "parser", ".", "add_argument", "(", "'--gpu'", ",", "type", "=", "str", ",", "default", "=", "'0'", ",", "help", "=", "'GPU ID to use'", ")", "\n", "parser", ".", "add_argument", "(", "'--cuda'", ",", "action", "=", "'store_true'", ",", "default", "=", "False", ",", "help", "=", "'use cuda'", ")", "\n", "parser", ".", "add_argument", "(", "'--vocab_size'", ",", "type", "=", "int", ",", "default", "=", "50000", ",", "help", "=", "'Size of vocabulary.'", ")", "\n", "parser", ".", "add_argument", "(", "'--batch_size'", ",", "type", "=", "int", ",", "default", "=", "32", ",", "help", "=", "'Mini batch size [default: 32]'", ")", "\n", "parser", ".", "add_argument", "(", "'--n_iter'", ",", "type", "=", "int", ",", "default", "=", "1", ",", "help", "=", "'iteration '", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--word_embedding'", ",", "action", "=", "'store_true'", ",", "default", "=", "True", ",", "help", "=", "'whether to use Word embedding'", ")", "\n", "parser", ".", "add_argument", "(", "'--word_emb_dim'", ",", "type", "=", "int", ",", "default", "=", "300", ",", "help", "=", "'Word embedding size [default: 300]'", ")", "\n", "parser", ".", "add_argument", "(", "'--embed_train'", ",", "action", "=", "'store_true'", ",", "default", "=", "False", ",", "help", "=", "'whether to train Word embedding [default: False]'", ")", "\n", "parser", ".", "add_argument", "(", "'--feat_embed_size'", ",", "type", "=", "int", ",", "default", "=", "50", ",", "help", "=", "'feature embedding size [default: 50]'", ")", "\n", "parser", ".", "add_argument", "(", "'--n_layers'", ",", "type", "=", "int", ",", "default", "=", "1", ",", "help", "=", "'Number of GAT layers [default: 1]'", ")", "\n", "parser", ".", "add_argument", "(", "'--lstm_hidden_state'", ",", "type", "=", "int", ",", "default", "=", "128", ",", "help", "=", "'size of lstm hidden state'", ")", "\n", "parser", ".", "add_argument", "(", "'--lstm_layers'", ",", "type", "=", "int", ",", "default", "=", "2", ",", "help", "=", "'lstm layers'", ")", "\n", "parser", ".", "add_argument", "(", "'--bidirectional'", ",", "action", "=", "'store_true'", ",", "default", "=", "True", ",", "help", "=", "'use bidirectional LSTM'", ")", "\n", "parser", ".", "add_argument", "(", "'--n_feature_size'", ",", "type", "=", "int", ",", "default", "=", "128", ",", "help", "=", "'size of node feature'", ")", "\n", "parser", ".", "add_argument", "(", "'--hidden_size'", ",", "type", "=", "int", ",", "default", "=", "64", ",", "help", "=", "'hidden size [default: 64]'", ")", "\n", "parser", ".", "add_argument", "(", "'--gcn_hidden_size'", ",", "type", "=", "int", ",", "default", "=", "128", ",", "help", "=", "'hidden size [default: 64]'", ")", "\n", "parser", ".", "add_argument", "(", "'--ffn_inner_hidden_size'", ",", "type", "=", "int", ",", "default", "=", "512", ",", "help", "=", "'PositionwiseFeedForward inner hidden size [default: 512]'", ")", "\n", "parser", ".", "add_argument", "(", "'--n_head'", ",", "type", "=", "int", ",", "default", "=", "8", ",", "help", "=", "'multihead attention number [default: 8]'", ")", "\n", "parser", ".", "add_argument", "(", "'--recurrent_dropout_prob'", ",", "type", "=", "float", ",", "default", "=", "0.1", ",", "help", "=", "'recurrent dropout prob [default: 0.1]'", ")", "\n", "parser", ".", "add_argument", "(", "'--atten_dropout_prob'", ",", "type", "=", "float", ",", "default", "=", "0.1", ",", "help", "=", "'attention dropout prob [default: 0.1]'", ")", "\n", "parser", ".", "add_argument", "(", "'--ffn_dropout_prob'", ",", "type", "=", "float", ",", "default", "=", "0.1", ",", "help", "=", "'PositionwiseFeedForward dropout prob [default: 0.1]'", ")", "\n", "parser", ".", "add_argument", "(", "'--use_orthnormal_init'", ",", "action", "=", "'store_true'", ",", "default", "=", "True", ",", "help", "=", "'use orthnormal init for lstm [default: true]'", ")", "\n", "parser", ".", "add_argument", "(", "'--sent_max_len'", ",", "type", "=", "int", ",", "default", "=", "100", ",", "help", "=", "'max length of sentences (max source text sentence tokens)'", ")", "\n", "parser", ".", "add_argument", "(", "'--doc_max_timesteps'", ",", "type", "=", "int", ",", "default", "=", "50", ",", "help", "=", "'max length of documents (max timesteps of documents)'", ")", "\n", "parser", ".", "add_argument", "(", "'--save_label'", ",", "action", "=", "'store_true'", ",", "default", "=", "False", ",", "help", "=", "'require multihead attention'", ")", "\n", "parser", ".", "add_argument", "(", "'--limited'", ",", "action", "=", "'store_true'", ",", "default", "=", "False", ",", "help", "=", "'limited hypo length'", ")", "\n", "parser", ".", "add_argument", "(", "'--blocking'", ",", "action", "=", "'store_true'", ",", "default", "=", "False", ",", "help", "=", "'ngram blocking'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'-m'", ",", "type", "=", "int", ",", "default", "=", "3", ",", "help", "=", "'decode summary length'", ")", "\n", "\n", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "os", ".", "environ", "[", "'CUDA_VISIBLE_DEVICES'", "]", "=", "args", ".", "gpu", "\n", "torch", ".", "set_printoptions", "(", "threshold", "=", "50000", ")", "\n", "\n", "# File paths", "\n", "DATA_FILE", "=", "os", ".", "path", ".", "join", "(", "args", ".", "data_dir", ",", "\"test.label.jsonl\"", ")", "\n", "VOCAL_FILE", "=", "os", ".", "path", ".", "join", "(", "args", ".", "cache_dir", ",", "\"vocab\"", ")", "\n", "FILTER_WORD", "=", "os", ".", "path", ".", "join", "(", "args", ".", "cache_dir", ",", "\"filter_word.txt\"", ")", "\n", "LOG_PATH", "=", "args", ".", "log_root", "\n", "\n", "# train_log setting", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "LOG_PATH", ")", ":", "\n", "        ", "logger", ".", "exception", "(", "\"[Error] Logdir %s doesn't exist. Run in train mode to create it.\"", ",", "LOG_PATH", ")", "\n", "raise", "Exception", "(", "\"[Error] Logdir %s doesn't exist. Run in train mode to create it.\"", "%", "(", "LOG_PATH", ")", ")", "\n", "", "nowTime", "=", "datetime", ".", "datetime", ".", "now", "(", ")", ".", "strftime", "(", "'%Y%m%d_%H%M%S'", ")", "\n", "log_path", "=", "os", ".", "path", ".", "join", "(", "LOG_PATH", ",", "\"test_\"", "+", "nowTime", ")", "\n", "file_handler", "=", "logging", ".", "FileHandler", "(", "log_path", ")", "\n", "file_handler", ".", "setFormatter", "(", "formatter", ")", "\n", "logger", ".", "addHandler", "(", "file_handler", ")", "\n", "\n", "logger", ".", "info", "(", "\"Pytorch %s\"", ",", "torch", ".", "__version__", ")", "\n", "logger", ".", "info", "(", "\"[INFO] Create Vocab, vocab path is %s\"", ",", "VOCAL_FILE", ")", "\n", "vocab", "=", "Vocab", "(", "VOCAL_FILE", ",", "args", ".", "vocab_size", ")", "\n", "embed", "=", "torch", ".", "nn", ".", "Embedding", "(", "vocab", ".", "size", "(", ")", ",", "args", ".", "word_emb_dim", ")", "\n", "if", "args", ".", "word_embedding", ":", "\n", "        ", "embed_loader", "=", "Word_Embedding", "(", "args", ".", "embedding_path", ",", "vocab", ")", "\n", "vectors", "=", "embed_loader", ".", "load_my_vecs", "(", "args", ".", "word_emb_dim", ")", "\n", "pretrained_weight", "=", "embed_loader", ".", "add_unknown_words_by_avg", "(", "vectors", ",", "args", ".", "word_emb_dim", ")", "\n", "embed", ".", "weight", ".", "data", ".", "copy_", "(", "torch", ".", "Tensor", "(", "pretrained_weight", ")", ")", "\n", "embed", ".", "weight", ".", "requires_grad", "=", "args", ".", "embed_train", "\n", "\n", "", "hps", "=", "args", "\n", "logger", ".", "info", "(", "hps", ")", "\n", "\n", "test_w2s_path", "=", "os", ".", "path", ".", "join", "(", "args", ".", "cache_dir", ",", "\"test.w2s.tfidf.jsonl\"", ")", "\n", "if", "hps", ".", "model", "==", "\"HSG\"", ":", "\n", "        ", "model", "=", "HSumGraph", "(", "hps", ",", "embed", ")", "\n", "logger", ".", "info", "(", "\"[MODEL] HeterSumGraph \"", ")", "\n", "dataset", "=", "ExampleSet", "(", "DATA_FILE", ",", "vocab", ",", "hps", ".", "doc_max_timesteps", ",", "hps", ".", "sent_max_len", ",", "FILTER_WORD", ",", "test_w2s_path", ")", "\n", "loader", "=", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "dataset", ",", "batch_size", "=", "hps", ".", "batch_size", ",", "shuffle", "=", "True", ",", "num_workers", "=", "32", ",", "collate_fn", "=", "graph_collate_fn", ")", "\n", "", "elif", "hps", ".", "model", "==", "\"HDSG\"", ":", "\n", "        ", "model", "=", "HSumDocGraph", "(", "hps", ",", "embed", ")", "\n", "logger", ".", "info", "(", "\"[MODEL] HeterDocSumGraph \"", ")", "\n", "test_w2d_path", "=", "os", ".", "path", ".", "join", "(", "args", ".", "cache_dir", ",", "\"test.w2d.tfidf.jsonl\"", ")", "\n", "dataset", "=", "MultiExampleSet", "(", "DATA_FILE", ",", "vocab", ",", "hps", ".", "doc_max_timesteps", ",", "hps", ".", "sent_max_len", ",", "FILTER_WORD", ",", "test_w2s_path", ",", "test_w2d_path", ")", "\n", "loader", "=", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "dataset", ",", "batch_size", "=", "hps", ".", "batch_size", ",", "shuffle", "=", "True", ",", "num_workers", "=", "32", ",", "collate_fn", "=", "graph_collate_fn", ")", "\n", "", "else", ":", "\n", "        ", "logger", ".", "error", "(", "\"[ERROR] Invalid Model Type!\"", ")", "\n", "raise", "NotImplementedError", "(", "\"Model Type has not been implemented\"", ")", "\n", "\n", "", "if", "args", ".", "cuda", ":", "\n", "        ", "model", ".", "to", "(", "torch", ".", "device", "(", "\"cuda:0\"", ")", ")", "\n", "logger", ".", "info", "(", "\"[INFO] Use cuda\"", ")", "\n", "\n", "", "logger", ".", "info", "(", "\"[INFO] Decoding...\"", ")", "\n", "if", "hps", ".", "test_model", "==", "\"multi\"", ":", "\n", "        ", "for", "i", "in", "range", "(", "3", ")", ":", "\n", "            ", "model_name", "=", "\"evalbestmodel_%d\"", "%", "i", "\n", "run_test", "(", "model", ",", "dataset", ",", "loader", ",", "model_name", ",", "hps", ")", "\n", "", "", "else", ":", "\n", "        ", "run_test", "(", "model", ",", "dataset", ",", "loader", ",", "hps", ".", "test_model", ",", "hps", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.module.vocabulary.Vocab.__init__": [[33, 70], ["logger.info", "open", "line.split", "Exception", "logger.error", "logger.info"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "vocab_file", ",", "max_size", ")", ":", "\n", "        ", "\"\"\"\n        Creates a vocab of up to max_size words, reading from the vocab_file. If max_size is 0, reads the entire vocab file.\n        :param vocab_file: string; path to the vocab file, which is assumed to contain \"<word> <frequency>\" on each line, sorted with most frequent word first. This code doesn't actually use the frequencies, though.\n        :param max_size: int; The maximum size of the resulting Vocabulary.\n        \"\"\"", "\n", "self", ".", "_word_to_id", "=", "{", "}", "\n", "self", ".", "_id_to_word", "=", "{", "}", "\n", "self", ".", "_count", "=", "0", "# keeps track of total number of words in the Vocab", "\n", "\n", "# [UNK], [PAD], [START] and [STOP] get the ids 0,1,2,3.", "\n", "for", "w", "in", "[", "PAD_TOKEN", ",", "UNKNOWN_TOKEN", ",", "START_DECODING", ",", "STOP_DECODING", "]", ":", "\n", "            ", "self", ".", "_word_to_id", "[", "w", "]", "=", "self", ".", "_count", "\n", "self", ".", "_id_to_word", "[", "self", ".", "_count", "]", "=", "w", "\n", "self", ".", "_count", "+=", "1", "\n", "\n", "# Read the vocab file and add words up to max_size", "\n", "", "with", "open", "(", "vocab_file", ",", "'r'", ",", "encoding", "=", "'utf8'", ")", "as", "vocab_f", ":", "#New : add the utf8 encoding to prevent error", "\n", "            ", "cnt", "=", "0", "\n", "for", "line", "in", "vocab_f", ":", "\n", "                ", "cnt", "+=", "1", "\n", "pieces", "=", "line", ".", "split", "(", "\"\\t\"", ")", "\n", "# pieces = line.split()", "\n", "w", "=", "pieces", "[", "0", "]", "\n", "# print(w)", "\n", "if", "w", "in", "[", "UNKNOWN_TOKEN", ",", "PAD_TOKEN", ",", "START_DECODING", ",", "STOP_DECODING", "]", ":", "\n", "                    ", "raise", "Exception", "(", "'[UNK], [PAD], [START] and [STOP] shouldn\\'t be in the vocab file, but %s is'", "%", "w", ")", "\n", "", "if", "w", "in", "self", ".", "_word_to_id", ":", "\n", "                    ", "logger", ".", "error", "(", "'Duplicated word in vocabulary file Line %d : %s'", "%", "(", "cnt", ",", "w", ")", ")", "\n", "continue", "\n", "", "self", ".", "_word_to_id", "[", "w", "]", "=", "self", ".", "_count", "\n", "self", ".", "_id_to_word", "[", "self", ".", "_count", "]", "=", "w", "\n", "self", ".", "_count", "+=", "1", "\n", "if", "max_size", "!=", "0", "and", "self", ".", "_count", ">=", "max_size", ":", "\n", "                    ", "logger", ".", "info", "(", "\"[INFO] max_size of vocab was specified as %i; we now have %i words. Stopping reading.\"", "%", "(", "max_size", ",", "self", ".", "_count", ")", ")", "\n", "break", "\n", "", "", "", "logger", ".", "info", "(", "\"[INFO] Finished constructing vocabulary of %i total words. Last word added: %s\"", ",", "self", ".", "_count", ",", "self", ".", "_id_to_word", "[", "self", ".", "_count", "-", "1", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.module.vocabulary.Vocab.word2id": [[71, 76], ["None"], "methods", ["None"], ["", "def", "word2id", "(", "self", ",", "word", ")", ":", "\n", "        ", "\"\"\"Returns the id (integer) of a word (string). Returns [UNK] id if word is OOV.\"\"\"", "\n", "if", "word", "not", "in", "self", ".", "_word_to_id", ":", "\n", "            ", "return", "self", ".", "_word_to_id", "[", "UNKNOWN_TOKEN", "]", "\n", "", "return", "self", ".", "_word_to_id", "[", "word", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.module.vocabulary.Vocab.id2word": [[77, 82], ["ValueError"], "methods", ["None"], ["", "def", "id2word", "(", "self", ",", "word_id", ")", ":", "\n", "        ", "\"\"\"Returns the word (string) corresponding to an id (integer).\"\"\"", "\n", "if", "word_id", "not", "in", "self", ".", "_id_to_word", ":", "\n", "            ", "raise", "ValueError", "(", "'Id not found in vocab: %d'", "%", "word_id", ")", "\n", "", "return", "self", ".", "_id_to_word", "[", "word_id", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.module.vocabulary.Vocab.size": [[83, 86], ["None"], "methods", ["None"], ["", "def", "size", "(", "self", ")", ":", "\n", "        ", "\"\"\"Returns the total size of the vocabulary\"\"\"", "\n", "return", "self", ".", "_count", "\n", "\n"]], "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.module.vocabulary.Vocab.word_list": [[87, 90], ["vocabulary.Vocab._word_to_id.keys"], "methods", ["None"], ["", "def", "word_list", "(", "self", ")", ":", "\n", "        ", "\"\"\"Return the word list of the vocabulary\"\"\"", "\n", "return", "self", ".", "_word_to_id", ".", "keys", "(", ")", "", "", "", ""]], "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.module.GATLayer.PositionwiseFeedForward.__init__": [[28, 34], ["torch.Module.__init__", "torch.Conv1d", "torch.Conv1d", "torch.Conv1d", "torch.Conv1d", "torch.Conv1d", "torch.Conv1d", "torch.LayerNorm", "torch.LayerNorm", "torch.LayerNorm", "torch.Dropout", "torch.Dropout", "torch.Dropout"], "methods", ["home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.module.GATStackLayer.MultiHeadLayer.__init__"], ["def", "__init__", "(", "self", ",", "d_in", ",", "d_hid", ",", "dropout", "=", "0.1", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "w_1", "=", "nn", ".", "Conv1d", "(", "d_in", ",", "d_hid", ",", "1", ")", "# position-wise", "\n", "self", ".", "w_2", "=", "nn", ".", "Conv1d", "(", "d_hid", ",", "d_in", ",", "1", ")", "# position-wise", "\n", "self", ".", "layer_norm", "=", "nn", ".", "LayerNorm", "(", "d_in", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "dropout", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.module.GATLayer.PositionwiseFeedForward.forward": [[35, 45], ["x.transpose", "GATLayer.PositionwiseFeedForward.w_2", "GATLayer.PositionwiseFeedForward.transpose", "GATLayer.PositionwiseFeedForward.dropout", "GATLayer.PositionwiseFeedForward.layer_norm", "torch.any", "torch.any", "torch.any", "torch.any", "torch.any", "torch.any", "torch.any", "torch.any", "torch.any", "torch.relu", "torch.relu", "torch.relu", "torch.any", "torch.any", "torch.any", "torch.any", "torch.any", "torch.any", "torch.any", "torch.any", "torch.any", "torch.isnan", "torch.isnan", "torch.isnan", "torch.isnan", "torch.isnan", "torch.isnan", "torch.isnan", "torch.isnan", "torch.isnan", "GATLayer.PositionwiseFeedForward.w_1", "torch.isnan", "torch.isnan", "torch.isnan", "torch.isnan", "torch.isnan", "torch.isnan", "torch.isnan", "torch.isnan", "torch.isnan"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "assert", "not", "torch", ".", "any", "(", "torch", ".", "isnan", "(", "x", ")", ")", ",", "\"FFN input\"", "\n", "residual", "=", "x", "\n", "output", "=", "x", ".", "transpose", "(", "1", ",", "2", ")", "\n", "output", "=", "self", ".", "w_2", "(", "F", ".", "relu", "(", "self", ".", "w_1", "(", "output", ")", ")", ")", "\n", "output", "=", "output", ".", "transpose", "(", "1", ",", "2", ")", "\n", "output", "=", "self", ".", "dropout", "(", "output", ")", "\n", "output", "=", "self", ".", "layer_norm", "(", "output", "+", "residual", ")", "\n", "assert", "not", "torch", ".", "any", "(", "torch", ".", "isnan", "(", "output", ")", ")", ",", "\"FFN output\"", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.module.GATLayer.SGATLayer.__init__": [[50, 55], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear"], "methods", ["home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.module.GATStackLayer.MultiHeadLayer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "in_dim", ",", "out_dim", ",", "weight", "=", "0", ")", ":", "\n", "        ", "super", "(", "SGATLayer", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "weight", "=", "weight", "\n", "self", ".", "fc", "=", "nn", ".", "Linear", "(", "in_dim", ",", "out_dim", ",", "bias", "=", "False", ")", "\n", "self", ".", "attn_fc", "=", "nn", ".", "Linear", "(", "2", "*", "out_dim", ",", "1", ",", "bias", "=", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.module.GATLayer.SGATLayer.edge_attention": [[56, 60], ["torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.leaky_relu", "torch.leaky_relu", "torch.leaky_relu", "GATLayer.SGATLayer.attn_fc"], "methods", ["None"], ["", "def", "edge_attention", "(", "self", ",", "edges", ")", ":", "\n", "        ", "z2", "=", "torch", ".", "cat", "(", "[", "edges", ".", "src", "[", "'z'", "]", ",", "edges", ".", "dst", "[", "'z'", "]", "]", ",", "dim", "=", "1", ")", "# [edge_num, 2 * out_dim]", "\n", "wa", "=", "F", ".", "leaky_relu", "(", "self", ".", "attn_fc", "(", "z2", ")", ")", "# [edge_num, 1]", "\n", "return", "{", "'e'", ":", "wa", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.module.GATLayer.SGATLayer.message_func": [[61, 63], ["None"], "methods", ["None"], ["", "def", "message_func", "(", "self", ",", "edges", ")", ":", "\n", "        ", "return", "{", "'z'", ":", "edges", ".", "src", "[", "'z'", "]", ",", "'e'", ":", "edges", ".", "data", "[", "'e'", "]", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.module.GATLayer.SGATLayer.reduce_func": [[64, 68], ["torch.softmax", "torch.softmax", "torch.softmax", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum"], "methods", ["None"], ["", "def", "reduce_func", "(", "self", ",", "nodes", ")", ":", "\n", "        ", "alpha", "=", "F", ".", "softmax", "(", "nodes", ".", "mailbox", "[", "'e'", "]", ",", "dim", "=", "1", ")", "\n", "h", "=", "torch", ".", "sum", "(", "alpha", "*", "nodes", ".", "mailbox", "[", "'z'", "]", ",", "dim", "=", "1", ")", "\n", "return", "{", "'sh'", ":", "h", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.module.GATLayer.SGATLayer.forward": [[69, 79], ["g.filter_nodes", "g.filter_edges", "GATLayer.SGATLayer.fc", "g.apply_edges", "g.pull", "g.ndata.pop", "g.ndata.pop"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "g", ",", "h", ")", ":", "\n", "        ", "snode_id", "=", "g", ".", "filter_nodes", "(", "lambda", "nodes", ":", "nodes", ".", "data", "[", "\"unit\"", "]", "==", "1", ")", "\n", "sedge_id", "=", "g", ".", "filter_edges", "(", "lambda", "edges", ":", "edges", ".", "data", "[", "\"dtype\"", "]", "==", "0", ")", "\n", "z", "=", "self", ".", "fc", "(", "h", ")", "\n", "g", ".", "nodes", "[", "snode_id", "]", ".", "data", "[", "'z'", "]", "=", "z", "\n", "g", ".", "apply_edges", "(", "self", ".", "edge_attention", ",", "edges", "=", "sedge_id", ")", "\n", "g", ".", "pull", "(", "snode_id", ",", "self", ".", "message_func", ",", "self", ".", "reduce_func", ")", "\n", "g", ".", "ndata", ".", "pop", "(", "'z'", ")", "\n", "h", "=", "g", ".", "ndata", ".", "pop", "(", "'sh'", ")", "\n", "return", "h", "[", "snode_id", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.module.GATLayer.WSGATLayer.__init__": [[82, 88], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear"], "methods", ["home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.module.GATStackLayer.MultiHeadLayer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "in_dim", ",", "out_dim", ",", "feat_embed_size", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "fc", "=", "nn", ".", "Linear", "(", "in_dim", ",", "out_dim", ",", "bias", "=", "False", ")", "\n", "\n", "self", ".", "feat_fc", "=", "nn", ".", "Linear", "(", "feat_embed_size", ",", "out_dim", ",", "bias", "=", "False", ")", "\n", "self", ".", "attn_fc", "=", "nn", ".", "Linear", "(", "3", "*", "out_dim", ",", "1", ",", "bias", "=", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.module.GATLayer.WSGATLayer.edge_attention": [[89, 94], ["GATLayer.WSGATLayer.feat_fc", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.leaky_relu", "torch.leaky_relu", "torch.leaky_relu", "GATLayer.WSGATLayer.attn_fc"], "methods", ["None"], ["", "def", "edge_attention", "(", "self", ",", "edges", ")", ":", "\n", "        ", "dfeat", "=", "self", ".", "feat_fc", "(", "edges", ".", "data", "[", "\"tfidfembed\"", "]", ")", "# [edge_num, out_dim]", "\n", "z2", "=", "torch", ".", "cat", "(", "[", "edges", ".", "src", "[", "'z'", "]", ",", "edges", ".", "dst", "[", "'z'", "]", ",", "dfeat", "]", ",", "dim", "=", "1", ")", "# [edge_num, 3 * out_dim]", "\n", "wa", "=", "F", ".", "leaky_relu", "(", "self", ".", "attn_fc", "(", "z2", ")", ")", "# [edge_num, 1]", "\n", "return", "{", "'e'", ":", "wa", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.module.GATLayer.WSGATLayer.message_func": [[95, 98], ["None"], "methods", ["None"], ["", "def", "message_func", "(", "self", ",", "edges", ")", ":", "\n", "# print(\"edge e \", edges.data['e'].size())", "\n", "        ", "return", "{", "'z'", ":", "edges", ".", "src", "[", "'z'", "]", ",", "'e'", ":", "edges", ".", "data", "[", "'e'", "]", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.module.GATLayer.WSGATLayer.reduce_func": [[99, 103], ["torch.softmax", "torch.softmax", "torch.softmax", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum"], "methods", ["None"], ["", "def", "reduce_func", "(", "self", ",", "nodes", ")", ":", "\n", "        ", "alpha", "=", "F", ".", "softmax", "(", "nodes", ".", "mailbox", "[", "'e'", "]", ",", "dim", "=", "1", ")", "\n", "h", "=", "torch", ".", "sum", "(", "alpha", "*", "nodes", ".", "mailbox", "[", "'z'", "]", ",", "dim", "=", "1", ")", "\n", "return", "{", "'sh'", ":", "h", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.module.GATLayer.WSGATLayer.forward": [[104, 117], ["g.filter_nodes", "g.filter_nodes", "g.filter_edges", "GATLayer.WSGATLayer.fc", "g.apply_edges", "g.pull", "g.ndata.pop", "g.ndata.pop"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "g", ",", "h", ")", ":", "\n", "        ", "wnode_id", "=", "g", ".", "filter_nodes", "(", "lambda", "nodes", ":", "nodes", ".", "data", "[", "\"unit\"", "]", "==", "0", ")", "\n", "snode_id", "=", "g", ".", "filter_nodes", "(", "lambda", "nodes", ":", "nodes", ".", "data", "[", "\"unit\"", "]", "==", "1", ")", "\n", "wsedge_id", "=", "g", ".", "filter_edges", "(", "lambda", "edges", ":", "(", "edges", ".", "src", "[", "\"unit\"", "]", "==", "0", ")", "&", "(", "edges", ".", "dst", "[", "\"unit\"", "]", "==", "1", ")", ")", "\n", "# print(\"id in WSGATLayer\")", "\n", "# print(wnode_id, snode_id, wsedge_id)", "\n", "z", "=", "self", ".", "fc", "(", "h", ")", "\n", "g", ".", "nodes", "[", "wnode_id", "]", ".", "data", "[", "'z'", "]", "=", "z", "\n", "g", ".", "apply_edges", "(", "self", ".", "edge_attention", ",", "edges", "=", "wsedge_id", ")", "\n", "g", ".", "pull", "(", "snode_id", ",", "self", ".", "message_func", ",", "self", ".", "reduce_func", ")", "\n", "g", ".", "ndata", ".", "pop", "(", "'z'", ")", "\n", "h", "=", "g", ".", "ndata", ".", "pop", "(", "'sh'", ")", "\n", "return", "h", "[", "snode_id", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.module.GATLayer.SWGATLayer.__init__": [[121, 127], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear"], "methods", ["home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.module.GATStackLayer.MultiHeadLayer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "in_dim", ",", "out_dim", ",", "feat_embed_size", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "fc", "=", "nn", ".", "Linear", "(", "in_dim", ",", "out_dim", ",", "bias", "=", "False", ")", "\n", "\n", "self", ".", "feat_fc", "=", "nn", ".", "Linear", "(", "feat_embed_size", ",", "out_dim", ")", "\n", "self", ".", "attn_fc", "=", "nn", ".", "Linear", "(", "3", "*", "out_dim", ",", "1", ",", "bias", "=", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.module.GATLayer.SWGATLayer.edge_attention": [[128, 133], ["GATLayer.SWGATLayer.feat_fc", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.leaky_relu", "torch.leaky_relu", "torch.leaky_relu", "GATLayer.SWGATLayer.attn_fc"], "methods", ["None"], ["", "def", "edge_attention", "(", "self", ",", "edges", ")", ":", "\n", "        ", "dfeat", "=", "self", ".", "feat_fc", "(", "edges", ".", "data", "[", "\"tfidfembed\"", "]", ")", "# [edge_num, out_dim]", "\n", "z2", "=", "torch", ".", "cat", "(", "[", "edges", ".", "src", "[", "'z'", "]", ",", "edges", ".", "dst", "[", "'z'", "]", ",", "dfeat", "]", ",", "dim", "=", "1", ")", "# [edge_num, 3 * out_dim]", "\n", "wa", "=", "F", ".", "leaky_relu", "(", "self", ".", "attn_fc", "(", "z2", ")", ")", "# [edge_num, 1]", "\n", "return", "{", "'e'", ":", "wa", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.module.GATLayer.SWGATLayer.message_func": [[134, 136], ["None"], "methods", ["None"], ["", "def", "message_func", "(", "self", ",", "edges", ")", ":", "\n", "        ", "return", "{", "'z'", ":", "edges", ".", "src", "[", "'z'", "]", ",", "'e'", ":", "edges", ".", "data", "[", "'e'", "]", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.module.GATLayer.SWGATLayer.reduce_func": [[137, 141], ["torch.softmax", "torch.softmax", "torch.softmax", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum"], "methods", ["None"], ["", "def", "reduce_func", "(", "self", ",", "nodes", ")", ":", "\n", "        ", "alpha", "=", "F", ".", "softmax", "(", "nodes", ".", "mailbox", "[", "'e'", "]", ",", "dim", "=", "1", ")", "\n", "h", "=", "torch", ".", "sum", "(", "alpha", "*", "nodes", ".", "mailbox", "[", "'z'", "]", ",", "dim", "=", "1", ")", "\n", "return", "{", "'sh'", ":", "h", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.module.GATLayer.SWGATLayer.forward": [[142, 153], ["g.filter_nodes", "g.filter_nodes", "g.filter_edges", "GATLayer.SWGATLayer.fc", "g.apply_edges", "g.pull", "g.ndata.pop", "g.ndata.pop"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "g", ",", "h", ")", ":", "\n", "        ", "wnode_id", "=", "g", ".", "filter_nodes", "(", "lambda", "nodes", ":", "nodes", ".", "data", "[", "\"unit\"", "]", "==", "0", ")", "\n", "snode_id", "=", "g", ".", "filter_nodes", "(", "lambda", "nodes", ":", "nodes", ".", "data", "[", "\"unit\"", "]", "==", "1", ")", "\n", "swedge_id", "=", "g", ".", "filter_edges", "(", "lambda", "edges", ":", "(", "edges", ".", "src", "[", "\"unit\"", "]", "==", "1", ")", "&", "(", "edges", ".", "dst", "[", "\"unit\"", "]", "==", "0", ")", ")", "\n", "z", "=", "self", ".", "fc", "(", "h", ")", "\n", "g", ".", "nodes", "[", "snode_id", "]", ".", "data", "[", "'z'", "]", "=", "z", "\n", "g", ".", "apply_edges", "(", "self", ".", "edge_attention", ",", "edges", "=", "swedge_id", ")", "\n", "g", ".", "pull", "(", "wnode_id", ",", "self", ".", "message_func", ",", "self", ".", "reduce_func", ")", "\n", "g", ".", "ndata", ".", "pop", "(", "'z'", ")", "\n", "h", "=", "g", ".", "ndata", ".", "pop", "(", "'sh'", ")", "\n", "return", "h", "[", "wnode_id", "]", "\n", "", "", ""]], "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.module.Encoder.sentEncoder.__init__": [[19, 55], ["torch.Module.__init__", "torch.Embedding.from_pretrained", "torch.Embedding.from_pretrained", "torch.Embedding.from_pretrained", "torch.Embedding.from_pretrained", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "module.PositionEmbedding.get_sinusoid_encoding_table", "torch.xavier_normal_", "torch.xavier_normal_", "torch.xavier_normal_", "torch.xavier_normal_", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "range", "numpy.sqrt"], "methods", ["home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.module.GATStackLayer.MultiHeadLayer.__init__", "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.module.PositionEmbedding.get_sinusoid_encoding_table"], ["    ", "def", "__init__", "(", "self", ",", "hps", ",", "embed", ")", ":", "\n", "        ", "\"\"\"\n\n        :param hps: \n                word_emb_dim: word embedding dimension\n                sent_max_len: max token number in the sentence\n                word_embedding: bool, use word embedding or not\n                embed_train: bool, whether to train word embedding\n                cuda: bool, use cuda or not\n        \"\"\"", "\n", "super", "(", "sentEncoder", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "_hps", "=", "hps", "\n", "self", ".", "sent_max_len", "=", "hps", ".", "sent_max_len", "\n", "embed_size", "=", "hps", ".", "word_emb_dim", "\n", "\n", "input_channels", "=", "1", "\n", "out_channels", "=", "50", "\n", "min_kernel_size", "=", "2", "\n", "max_kernel_size", "=", "7", "\n", "width", "=", "embed_size", "\n", "\n", "# word embedding", "\n", "self", ".", "embed", "=", "embed", "\n", "\n", "# position embedding", "\n", "self", ".", "position_embedding", "=", "nn", ".", "Embedding", ".", "from_pretrained", "(", "\n", "get_sinusoid_encoding_table", "(", "self", ".", "sent_max_len", "+", "1", ",", "embed_size", ",", "padding_idx", "=", "0", ")", ",", "freeze", "=", "True", ")", "\n", "\n", "# cnn", "\n", "self", ".", "convs", "=", "nn", ".", "ModuleList", "(", "[", "nn", ".", "Conv2d", "(", "input_channels", ",", "out_channels", ",", "kernel_size", "=", "(", "height", ",", "width", ")", ")", "for", "height", "in", "\n", "range", "(", "min_kernel_size", ",", "max_kernel_size", "+", "1", ")", "]", ")", "\n", "\n", "for", "conv", "in", "self", ".", "convs", ":", "\n", "            ", "init_weight_value", "=", "6.0", "\n", "init", ".", "xavier_normal_", "(", "conv", ".", "weight", ".", "data", ",", "gain", "=", "np", ".", "sqrt", "(", "init_weight_value", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.module.Encoder.sentEncoder.forward": [[56, 77], ["Encoder.sentEncoder.embed", "torch.Tensor().long", "torch.Tensor().long", "torch.Tensor().long", "torch.Tensor().long", "torch.Tensor().long", "torch.Tensor().long", "torch.Tensor().long", "torch.Tensor().long", "torch.Tensor().long", "torch.Tensor().long", "torch.Tensor().long", "torch.Tensor().long", "torch.Tensor().long", "torch.Tensor().long", "torch.Tensor().long", "torch.Tensor().long", "Encoder.sentEncoder.position_embedding", "enc_conv_input.unsqueeze.unsqueeze.unsqueeze", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "list", "list.extend", "sent_pos_list.append", "input_pos.cuda.cuda.cuda", "input_pos.cuda.cuda.long", "torch.relu().squeeze", "torch.relu().squeeze", "torch.relu().squeeze", "torch.relu().squeeze", "torch.max_pool1d().squeeze", "torch.max_pool1d().squeeze", "torch.max_pool1d().squeeze", "torch.max_pool1d().squeeze", "range", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "int", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.max_pool1d", "torch.max_pool1d", "torch.max_pool1d", "torch.max_pool1d", "min", "conv", "x.size"], "methods", ["home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.module.vocabulary.Vocab.size"], ["", "", "def", "forward", "(", "self", ",", "input", ")", ":", "\n", "# input: a batch of Example object [s_nodes, seq_len]", "\n", "        ", "input_sent_len", "=", "(", "(", "input", "!=", "0", ")", ".", "sum", "(", "dim", "=", "1", ")", ")", ".", "int", "(", ")", "# [s_nodes, 1]", "\n", "enc_embed_input", "=", "self", ".", "embed", "(", "input", ")", "# [s_nodes, L, D]", "\n", "\n", "sent_pos_list", "=", "[", "]", "\n", "for", "sentlen", "in", "input_sent_len", ":", "\n", "            ", "sent_pos", "=", "list", "(", "range", "(", "1", ",", "min", "(", "self", ".", "sent_max_len", ",", "sentlen", ")", "+", "1", ")", ")", "\n", "sent_pos", ".", "extend", "(", "[", "0", "]", "*", "int", "(", "self", ".", "sent_max_len", "-", "sentlen", ")", ")", "\n", "sent_pos_list", ".", "append", "(", "sent_pos", ")", "\n", "", "input_pos", "=", "torch", ".", "Tensor", "(", "sent_pos_list", ")", ".", "long", "(", ")", "\n", "\n", "if", "self", ".", "_hps", ".", "cuda", ":", "\n", "            ", "input_pos", "=", "input_pos", ".", "cuda", "(", ")", "\n", "", "enc_pos_embed_input", "=", "self", ".", "position_embedding", "(", "input_pos", ".", "long", "(", ")", ")", "# [s_nodes, D]", "\n", "enc_conv_input", "=", "enc_embed_input", "+", "enc_pos_embed_input", "\n", "enc_conv_input", "=", "enc_conv_input", ".", "unsqueeze", "(", "1", ")", "# [s_nodes, 1, L, D]", "\n", "enc_conv_output", "=", "[", "F", ".", "relu", "(", "conv", "(", "enc_conv_input", ")", ")", ".", "squeeze", "(", "3", ")", "for", "conv", "in", "self", ".", "convs", "]", "# kernel_sizes * [s_nodes, Co=50, W]", "\n", "enc_maxpool_output", "=", "[", "F", ".", "max_pool1d", "(", "x", ",", "x", ".", "size", "(", "2", ")", ")", ".", "squeeze", "(", "2", ")", "for", "x", "in", "enc_conv_output", "]", "# kernel_sizes * [s_nodes, Co=50]", "\n", "sent_embedding", "=", "torch", ".", "cat", "(", "enc_maxpool_output", ",", "1", ")", "# [s_nodes, 50 * 6]", "\n", "return", "sent_embedding", "# [s_nodes, 300]", "", "", "", ""]], "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.module.GAT.WSWGAT.__init__": [[31, 44], ["torch.Module.__init__", "module.GATLayer.PositionwiseFeedForward", "module.GATStackLayer.MultiHeadLayer", "int", "module.GATStackLayer.MultiHeadLayer", "int", "module.GATStackLayer.MultiHeadSGATLayer", "NotImplementedError", "int"], "methods", ["home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.module.GATStackLayer.MultiHeadLayer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "in_dim", ",", "out_dim", ",", "num_heads", ",", "attn_drop_out", ",", "ffn_inner_hidden_size", ",", "ffn_drop_out", ",", "feat_embed_size", ",", "layerType", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "layerType", "=", "layerType", "\n", "if", "layerType", "==", "\"W2S\"", ":", "\n", "            ", "self", ".", "layer", "=", "MultiHeadLayer", "(", "in_dim", ",", "int", "(", "out_dim", "/", "num_heads", ")", ",", "num_heads", ",", "attn_drop_out", ",", "feat_embed_size", ",", "layer", "=", "WSGATLayer", ")", "\n", "", "elif", "layerType", "==", "\"S2W\"", ":", "\n", "            ", "self", ".", "layer", "=", "MultiHeadLayer", "(", "in_dim", ",", "int", "(", "out_dim", "/", "num_heads", ")", ",", "num_heads", ",", "attn_drop_out", ",", "feat_embed_size", ",", "layer", "=", "SWGATLayer", ")", "\n", "", "elif", "layerType", "==", "\"S2S\"", ":", "\n", "            ", "self", ".", "layer", "=", "MultiHeadSGATLayer", "(", "in_dim", ",", "int", "(", "out_dim", "/", "num_heads", ")", ",", "num_heads", ",", "attn_drop_out", ")", "\n", "", "else", ":", "\n", "            ", "raise", "NotImplementedError", "(", "\"GAT Layer has not been implemented!\"", ")", "\n", "\n", "", "self", ".", "ffn", "=", "PositionwiseFeedForward", "(", "out_dim", ",", "ffn_inner_hidden_size", ",", "ffn_drop_out", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.module.GAT.WSWGAT.forward": [[45, 60], ["torch.elu", "torch.elu", "torch.elu", "GAT.WSWGAT.ffn().squeeze", "GAT.WSWGAT.layer", "GAT.WSWGAT.ffn", "torch.equal", "torch.equal", "torch.equal", "torch.equal", "torch.equal", "torch.equal", "torch.equal", "torch.equal", "torch.equal", "GAT.WSWGAT.unsqueeze"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "g", ",", "w", ",", "s", ")", ":", "\n", "        ", "if", "self", ".", "layerType", "==", "\"W2S\"", ":", "\n", "            ", "origin", ",", "neighbor", "=", "s", ",", "w", "\n", "", "elif", "self", ".", "layerType", "==", "\"S2W\"", ":", "\n", "            ", "origin", ",", "neighbor", "=", "w", ",", "s", "\n", "", "elif", "self", ".", "layerType", "==", "\"S2S\"", ":", "\n", "            ", "assert", "torch", ".", "equal", "(", "w", ",", "s", ")", "\n", "origin", ",", "neighbor", "=", "w", ",", "s", "\n", "", "else", ":", "\n", "            ", "origin", ",", "neighbor", "=", "None", ",", "None", "\n", "\n", "", "h", "=", "F", ".", "elu", "(", "self", ".", "layer", "(", "g", ",", "neighbor", ")", ")", "\n", "h", "=", "h", "+", "origin", "\n", "h", "=", "self", ".", "ffn", "(", "h", ".", "unsqueeze", "(", "0", ")", ")", ".", "squeeze", "(", "0", ")", "\n", "return", "h", "\n", "\n"]], "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.module.embedding.Word_Embedding.__init__": [[24, 33], ["logger.info", "vocab.word_list"], "methods", ["home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.module.vocabulary.Vocab.word_list"], ["    ", "def", "__init__", "(", "self", ",", "path", ",", "vocab", ")", ":", "\n", "        ", "\"\"\"\n        :param path: string; the path of word embedding\n        :param vocab: object;\n        \"\"\"", "\n", "logger", ".", "info", "(", "\"[INFO] Loading external word embedding...\"", ")", "\n", "self", ".", "_path", "=", "path", "\n", "self", ".", "_vocablist", "=", "vocab", ".", "word_list", "(", ")", "\n", "self", ".", "_vocab", "=", "vocab", "\n", "\n"]], "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.module.embedding.Word_Embedding.load_my_vecs": [[34, 53], ["open", "f.readlines", "line.split", "enumerate", "vector.append", "float"], "methods", ["None"], ["", "def", "load_my_vecs", "(", "self", ",", "k", "=", "200", ")", ":", "\n", "        ", "\"\"\"Load word embedding\"\"\"", "\n", "word_vecs", "=", "{", "}", "\n", "with", "open", "(", "self", ".", "_path", ",", "encoding", "=", "\"utf-8\"", ")", "as", "f", ":", "\n", "            ", "count", "=", "0", "\n", "lines", "=", "f", ".", "readlines", "(", ")", "[", "1", ":", "]", "\n", "for", "line", "in", "lines", ":", "\n", "                ", "values", "=", "line", ".", "split", "(", "\" \"", ")", "\n", "word", "=", "values", "[", "0", "]", "\n", "count", "+=", "1", "\n", "if", "word", "in", "self", ".", "_vocablist", ":", "# whether to judge if in vocab", "\n", "                    ", "vector", "=", "[", "]", "\n", "for", "count", ",", "val", "in", "enumerate", "(", "values", ")", ":", "\n", "                        ", "if", "count", "==", "0", ":", "\n", "                            ", "continue", "\n", "", "if", "count", "<=", "k", ":", "\n", "                            ", "vector", ".", "append", "(", "float", "(", "val", ")", ")", "\n", "", "", "word_vecs", "[", "word", "]", "=", "vector", "\n", "", "", "", "return", "word_vecs", "\n", "\n"]], "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.module.embedding.Word_Embedding.add_unknown_words_by_zero": [[54, 71], ["range", "logger.info", "embedding.Word_Embedding._vocab.size", "embedding.Word_Embedding._vocab.id2word", "list_word2vec.append", "list_word2vec.append"], "methods", ["home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.module.vocabulary.Vocab.size", "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.module.vocabulary.Vocab.id2word"], ["", "def", "add_unknown_words_by_zero", "(", "self", ",", "word_vecs", ",", "k", "=", "200", ")", ":", "\n", "        ", "\"\"\"Solve unknown by zeros\"\"\"", "\n", "zero", "=", "[", "0.0", "]", "*", "k", "\n", "list_word2vec", "=", "[", "]", "\n", "oov", "=", "0", "\n", "iov", "=", "0", "\n", "for", "i", "in", "range", "(", "self", ".", "_vocab", ".", "size", "(", ")", ")", ":", "\n", "            ", "word", "=", "self", ".", "_vocab", ".", "id2word", "(", "i", ")", "\n", "if", "word", "not", "in", "word_vecs", ":", "\n", "                ", "oov", "+=", "1", "\n", "word_vecs", "[", "word", "]", "=", "zero", "\n", "list_word2vec", ".", "append", "(", "word_vecs", "[", "word", "]", ")", "\n", "", "else", ":", "\n", "                ", "iov", "+=", "1", "\n", "list_word2vec", ".", "append", "(", "word_vecs", "[", "word", "]", ")", "\n", "", "", "logger", ".", "info", "(", "\"[INFO] oov count %d, iov count %d\"", ",", "oov", ",", "iov", ")", "\n", "return", "list_word2vec", "\n", "\n"]], "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.module.embedding.Word_Embedding.add_unknown_words_by_avg": [[72, 106], ["range", "range", "range", "logger.info", "range", "col.append", "round", "zero.append", "embedding.Word_Embedding._vocab.size", "embedding.Word_Embedding._vocab.id2word", "word_vecs_numpy.append", "int", "round", "int", "float", "list_word2vec.append", "list_word2vec.append", "len", "len"], "methods", ["home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.module.vocabulary.Vocab.size", "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.module.vocabulary.Vocab.id2word"], ["", "def", "add_unknown_words_by_avg", "(", "self", ",", "word_vecs", ",", "k", "=", "200", ")", ":", "\n", "        ", "\"\"\"Solve unknown by avg word embedding\"\"\"", "\n", "# solve unknown words inplaced by zero list", "\n", "word_vecs_numpy", "=", "[", "]", "\n", "for", "word", "in", "self", ".", "_vocablist", ":", "\n", "            ", "if", "word", "in", "word_vecs", ":", "\n", "                ", "word_vecs_numpy", ".", "append", "(", "word_vecs", "[", "word", "]", ")", "\n", "", "", "col", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "k", ")", ":", "\n", "            ", "sum", "=", "0.0", "\n", "for", "j", "in", "range", "(", "int", "(", "len", "(", "word_vecs_numpy", ")", ")", ")", ":", "\n", "                ", "sum", "+=", "word_vecs_numpy", "[", "j", "]", "[", "i", "]", "\n", "sum", "=", "round", "(", "sum", ",", "6", ")", "\n", "", "col", ".", "append", "(", "sum", ")", "\n", "", "zero", "=", "[", "]", "\n", "for", "m", "in", "range", "(", "k", ")", ":", "\n", "            ", "avg", "=", "col", "[", "m", "]", "/", "int", "(", "len", "(", "word_vecs_numpy", ")", ")", "\n", "avg", "=", "round", "(", "avg", ",", "6", ")", "\n", "zero", ".", "append", "(", "float", "(", "avg", ")", ")", "\n", "\n", "", "list_word2vec", "=", "[", "]", "\n", "oov", "=", "0", "\n", "iov", "=", "0", "\n", "for", "i", "in", "range", "(", "self", ".", "_vocab", ".", "size", "(", ")", ")", ":", "\n", "            ", "word", "=", "self", ".", "_vocab", ".", "id2word", "(", "i", ")", "\n", "if", "word", "not", "in", "word_vecs", ":", "\n", "                ", "oov", "+=", "1", "\n", "word_vecs", "[", "word", "]", "=", "zero", "\n", "list_word2vec", ".", "append", "(", "word_vecs", "[", "word", "]", ")", "\n", "", "else", ":", "\n", "                ", "iov", "+=", "1", "\n", "list_word2vec", ".", "append", "(", "word_vecs", "[", "word", "]", ")", "\n", "", "", "logger", ".", "info", "(", "\"[INFO] External Word Embedding iov count: %d, oov count: %d\"", ",", "iov", ",", "oov", ")", "\n", "return", "list_word2vec", "\n", "\n"]], "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.module.embedding.Word_Embedding.add_unknown_words_by_uniform": [[107, 123], ["range", "logger.info", "embedding.Word_Embedding._vocab.size", "embedding.Word_Embedding._vocab.id2word", "numpy.random.uniform().round().tolist", "list_word2vec.append", "list_word2vec.append", "numpy.random.uniform().round", "numpy.random.uniform"], "methods", ["home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.module.vocabulary.Vocab.size", "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.module.vocabulary.Vocab.id2word"], ["", "def", "add_unknown_words_by_uniform", "(", "self", ",", "word_vecs", ",", "uniform", "=", "0.25", ",", "k", "=", "200", ")", ":", "\n", "        ", "\"\"\"Solve unknown word by uniform(-0.25,0.25)\"\"\"", "\n", "list_word2vec", "=", "[", "]", "\n", "oov", "=", "0", "\n", "iov", "=", "0", "\n", "for", "i", "in", "range", "(", "self", ".", "_vocab", ".", "size", "(", ")", ")", ":", "\n", "            ", "word", "=", "self", ".", "_vocab", ".", "id2word", "(", "i", ")", "\n", "if", "word", "not", "in", "word_vecs", ":", "\n", "                ", "oov", "+=", "1", "\n", "word_vecs", "[", "word", "]", "=", "np", ".", "random", ".", "uniform", "(", "-", "1", "*", "uniform", ",", "uniform", ",", "k", ")", ".", "round", "(", "6", ")", ".", "tolist", "(", ")", "\n", "list_word2vec", ".", "append", "(", "word_vecs", "[", "word", "]", ")", "\n", "", "else", ":", "\n", "                ", "iov", "+=", "1", "\n", "list_word2vec", ".", "append", "(", "word_vecs", "[", "word", "]", ")", "\n", "", "", "logger", ".", "info", "(", "\"[INFO] oov count %d, iov count %d\"", ",", "oov", ",", "iov", ")", "\n", "return", "list_word2vec", "\n", "\n"]], "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.module.embedding.Word_Embedding.load_my_vecs_freq1": [[125, 145], ["open", "f.readlines", "line.split", "enumerate", "numpy.random.uniform().round", "vector.append", "float", "numpy.random.uniform"], "methods", ["None"], ["", "def", "load_my_vecs_freq1", "(", "self", ",", "freqs", ",", "pro", ")", ":", "\n", "        ", "word_vecs", "=", "{", "}", "\n", "with", "open", "(", "self", ".", "_path", ",", "encoding", "=", "\"utf-8\"", ")", "as", "f", ":", "\n", "            ", "freq", "=", "0", "\n", "lines", "=", "f", ".", "readlines", "(", ")", "[", "1", ":", "]", "\n", "for", "line", "in", "lines", ":", "\n", "                ", "values", "=", "line", ".", "split", "(", "\" \"", ")", "\n", "word", "=", "values", "[", "0", "]", "\n", "if", "word", "in", "self", ".", "_vocablist", ":", "# whehter to judge if in vocab", "\n", "                    ", "if", "freqs", "[", "word", "]", "==", "1", ":", "\n", "                        ", "a", "=", "np", ".", "random", ".", "uniform", "(", "0", ",", "1", ",", "1", ")", ".", "round", "(", "2", ")", "\n", "if", "pro", "<", "a", ":", "\n", "                            ", "continue", "\n", "", "", "vector", "=", "[", "]", "\n", "for", "count", ",", "val", "in", "enumerate", "(", "values", ")", ":", "\n", "                        ", "if", "count", "==", "0", ":", "\n", "                            ", "continue", "\n", "", "vector", ".", "append", "(", "float", "(", "val", ")", ")", "\n", "", "word_vecs", "[", "word", "]", "=", "vector", "\n", "", "", "", "return", "word_vecs", "", "", "", ""]], "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.module.dataloader.Example.__init__": [[59, 96], ["dataloader.Example._pad_encoder_input", "numpy.zeros", "isinstance", "isinstance", "sent.split", "dataloader.Example.enc_sent_len.append", "dataloader.Example.enc_sent_input.append", "vocab.word2id", "len", "len", "dataloader.Example.original_article_sents.extend", "len", "vocab.word2id", "w.lower", "numpy.array", "numpy.arange", "len"], "methods", ["home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.module.dataloader.Example._pad_encoder_input", "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.module.vocabulary.Vocab.word2id", "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.module.vocabulary.Vocab.word2id"], ["def", "__init__", "(", "self", ",", "article_sents", ",", "abstract_sents", ",", "vocab", ",", "sent_max_len", ",", "label", ")", ":", "\n", "        ", "\"\"\" Initializes the Example, performing tokenization and truncation to produce the encoder, decoder and target sequences, which are stored in self.\n\n        :param article_sents: list(strings) for single document or list(list(string)) for multi-document; one per article sentence. each token is separated by a single space.\n        :param abstract_sents: list(strings); one per abstract sentence. In each sentence, each token is separated by a single space.\n        :param vocab: Vocabulary object\n        :param sent_max_len: int, max length of each sentence\n        :param label: list, the No of selected sentence, e.g. [1,3,5]\n        \"\"\"", "\n", "\n", "self", ".", "sent_max_len", "=", "sent_max_len", "\n", "self", ".", "enc_sent_len", "=", "[", "]", "\n", "self", ".", "enc_sent_input", "=", "[", "]", "\n", "self", ".", "enc_sent_input_pad", "=", "[", "]", "\n", "\n", "# Store the original strings", "\n", "self", ".", "original_article_sents", "=", "article_sents", "\n", "self", ".", "original_abstract", "=", "\"\\n\"", ".", "join", "(", "abstract_sents", ")", "\n", "\n", "# Process the article", "\n", "if", "isinstance", "(", "article_sents", ",", "list", ")", "and", "isinstance", "(", "article_sents", "[", "0", "]", ",", "list", ")", ":", "# multi document", "\n", "            ", "self", ".", "original_article_sents", "=", "[", "]", "\n", "for", "doc", "in", "article_sents", ":", "\n", "                ", "self", ".", "original_article_sents", ".", "extend", "(", "doc", ")", "\n", "", "", "for", "sent", "in", "self", ".", "original_article_sents", ":", "\n", "            ", "article_words", "=", "sent", ".", "split", "(", ")", "\n", "self", ".", "enc_sent_len", ".", "append", "(", "len", "(", "article_words", ")", ")", "# store the length before padding", "\n", "self", ".", "enc_sent_input", ".", "append", "(", "[", "vocab", ".", "word2id", "(", "w", ".", "lower", "(", ")", ")", "for", "w", "in", "article_words", "]", ")", "# list of word ids; OOVs are represented by the id for UNK token", "\n", "", "self", ".", "_pad_encoder_input", "(", "vocab", ".", "word2id", "(", "'[PAD]'", ")", ")", "\n", "\n", "# Store the label", "\n", "self", ".", "label", "=", "label", "\n", "label_shape", "=", "(", "len", "(", "self", ".", "original_article_sents", ")", ",", "len", "(", "label", ")", ")", "# [N, len(label)]", "\n", "# label_shape = (len(self.original_article_sents), len(self.original_article_sents))", "\n", "self", ".", "label_matrix", "=", "np", ".", "zeros", "(", "label_shape", ",", "dtype", "=", "int", ")", "\n", "if", "label", "!=", "[", "]", ":", "\n", "            ", "self", ".", "label_matrix", "[", "np", ".", "array", "(", "label", ")", ",", "np", ".", "arange", "(", "len", "(", "label", ")", ")", "]", "=", "1", "# label_matrix[i][j]=1 indicate the i-th sent will be selected in j-th step", "\n", "\n"]], "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.module.dataloader.Example._pad_encoder_input": [[97, 110], ["range", "len", "dataloader.Example.enc_sent_input[].copy", "dataloader.Example.enc_sent_input_pad.append", "len", "len", "dataloader.Example.extend", "len"], "methods", ["None"], ["", "", "def", "_pad_encoder_input", "(", "self", ",", "pad_id", ")", ":", "\n", "        ", "\"\"\"\n        :param pad_id: int; token pad id\n        :return: \n        \"\"\"", "\n", "max_len", "=", "self", ".", "sent_max_len", "\n", "for", "i", "in", "range", "(", "len", "(", "self", ".", "enc_sent_input", ")", ")", ":", "\n", "            ", "article_words", "=", "self", ".", "enc_sent_input", "[", "i", "]", ".", "copy", "(", ")", "\n", "if", "len", "(", "article_words", ")", ">", "max_len", ":", "\n", "                ", "article_words", "=", "article_words", "[", ":", "max_len", "]", "\n", "", "if", "len", "(", "article_words", ")", "<", "max_len", ":", "\n", "                ", "article_words", ".", "extend", "(", "[", "pad_id", "]", "*", "(", "max_len", "-", "len", "(", "article_words", ")", ")", ")", "\n", "", "self", ".", "enc_sent_input_pad", ".", "append", "(", "article_words", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.module.dataloader.Example2.__init__": [[115, 138], ["dataloader.Example.__init__", "len", "dataloader.Example2.original_articles.append", "dataloader.Example2.article_len.append", "dataloader.Example2.enc_doc_input.append", "len", "dataloader.catDoc"], "methods", ["home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.module.GATStackLayer.MultiHeadLayer.__init__", "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.script.createVoc.catDoc"], ["def", "__init__", "(", "self", ",", "article_sents", ",", "abstract_sents", ",", "vocab", ",", "sent_max_len", ",", "label", ")", ":", "\n", "        ", "\"\"\" Initializes the Example, performing tokenization and truncation to produce the encoder, decoder and target sequences, which are stored in self.\n\n        :param article_sents: list(list(string)) for multi-document; one per article sentence. each token is separated by a single space.\n        :param abstract_sents: list(strings); one per abstract sentence. In each sentence, each token is separated by a single space.\n        :param vocab: Vocabulary object\n        :param sent_max_len: int, max length of each sentence\n        :param label: list, the No of selected sentence, e.g. [1,3,5]\n        \"\"\"", "\n", "\n", "super", "(", ")", ".", "__init__", "(", "article_sents", ",", "abstract_sents", ",", "vocab", ",", "sent_max_len", ",", "label", ")", "\n", "cur", "=", "0", "\n", "self", ".", "original_articles", "=", "[", "]", "\n", "self", ".", "article_len", "=", "[", "]", "\n", "self", ".", "enc_doc_input", "=", "[", "]", "\n", "for", "doc", "in", "article_sents", ":", "\n", "            ", "if", "len", "(", "doc", ")", "==", "0", ":", "\n", "                ", "continue", "\n", "", "docLen", "=", "len", "(", "doc", ")", "\n", "self", ".", "original_articles", ".", "append", "(", "\" \"", ".", "join", "(", "doc", ")", ")", "\n", "self", ".", "article_len", ".", "append", "(", "docLen", ")", "\n", "self", ".", "enc_doc_input", ".", "append", "(", "catDoc", "(", "self", ".", "enc_sent_input", "[", "cur", ":", "cur", "+", "docLen", "]", ")", ")", "\n", "cur", "+=", "docLen", "\n", "\n"]], "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.module.dataloader.ExampleSet.__init__": [[145, 186], ["logger.info", "time.time", "dataloader.readJson", "logger.info", "len", "logger.info", "dataloader.readText", "dataloader.ExampleSet.filterids.append", "logger.info", "dataloader.readJson", "len", "vocab.word2id", "vocab.word2id", "time.time", "w.lower", "vocab.word2id", "vocab.word2id", "dataloader.ExampleSet.filterwords.append", "dataloader.ExampleSet.filterids.append", "vocab.word2id"], "methods", ["home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.module.dataloader.readJson", "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.module.dataloader.readText", "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.module.dataloader.readJson", "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.module.vocabulary.Vocab.word2id", "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.module.vocabulary.Vocab.word2id", "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.module.vocabulary.Vocab.word2id", "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.module.vocabulary.Vocab.word2id", "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.module.vocabulary.Vocab.word2id"], ["def", "__init__", "(", "self", ",", "data_path", ",", "vocab", ",", "doc_max_timesteps", ",", "sent_max_len", ",", "filter_word_path", ",", "w2s_path", ")", ":", "\n", "        ", "\"\"\" Initializes the ExampleSet with the path of data\n        \n        :param data_path: string; the path of data\n        :param vocab: object;\n        :param doc_max_timesteps: int; the maximum sentence number of a document, each example should pad sentences to this length\n        :param sent_max_len: int; the maximum token number of a sentence, each sentence should pad tokens to this length\n        :param filter_word_path: str; file path, the file must contain one word for each line and the tfidf value must go from low to high (the format can refer to script/lowTFIDFWords.py) \n        :param w2s_path: str; file path, each line in the file contain a json format data (which can refer to the format can refer to script/calw2sTFIDF.py)\n        \"\"\"", "\n", "\n", "self", ".", "vocab", "=", "vocab", "\n", "self", ".", "sent_max_len", "=", "sent_max_len", "\n", "self", ".", "doc_max_timesteps", "=", "doc_max_timesteps", "\n", "\n", "logger", ".", "info", "(", "\"[INFO] Start reading %s\"", ",", "self", ".", "__class__", ".", "__name__", ")", "\n", "start", "=", "time", ".", "time", "(", ")", "\n", "self", ".", "example_list", "=", "readJson", "(", "data_path", ")", "\n", "logger", ".", "info", "(", "\"[INFO] Finish reading %s. Total time is %f, Total size is %d\"", ",", "self", ".", "__class__", ".", "__name__", ",", "\n", "time", ".", "time", "(", ")", "-", "start", ",", "len", "(", "self", ".", "example_list", ")", ")", "\n", "self", ".", "size", "=", "len", "(", "self", ".", "example_list", ")", "\n", "\n", "logger", ".", "info", "(", "\"[INFO] Loading filter word File %s\"", ",", "filter_word_path", ")", "\n", "tfidf_w", "=", "readText", "(", "filter_word_path", ")", "\n", "self", ".", "filterwords", "=", "FILTERWORD", "\n", "self", ".", "filterids", "=", "[", "vocab", ".", "word2id", "(", "w", ".", "lower", "(", ")", ")", "for", "w", "in", "FILTERWORD", "]", "\n", "self", ".", "filterids", ".", "append", "(", "vocab", ".", "word2id", "(", "\"[PAD]\"", ")", ")", "# keep \"[UNK]\" but remove \"[PAD]\"", "\n", "lowtfidf_num", "=", "0", "\n", "pattern", "=", "r\"^[0-9]+$\"", "\n", "for", "w", "in", "tfidf_w", ":", "\n", "            ", "if", "vocab", ".", "word2id", "(", "w", ")", "!=", "vocab", ".", "word2id", "(", "'[UNK]'", ")", ":", "\n", "                ", "self", ".", "filterwords", ".", "append", "(", "w", ")", "\n", "self", ".", "filterids", ".", "append", "(", "vocab", ".", "word2id", "(", "w", ")", ")", "\n", "# if re.search(pattern, w) == None:  # if w is a number, it will not increase the lowtfidf_num", "\n", "# lowtfidf_num += 1", "\n", "lowtfidf_num", "+=", "1", "\n", "", "if", "lowtfidf_num", ">", "5000", ":", "\n", "                ", "break", "\n", "\n", "", "", "logger", ".", "info", "(", "\"[INFO] Loading word2sent TFIDF file from %s!\"", "%", "w2s_path", ")", "\n", "self", ".", "w2s_tfidf", "=", "readJson", "(", "w2s_path", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.module.dataloader.ExampleSet.get_example": [[187, 192], ["e.setdefault", "dataloader.Example"], "methods", ["None"], ["", "def", "get_example", "(", "self", ",", "index", ")", ":", "\n", "        ", "e", "=", "self", ".", "example_list", "[", "index", "]", "\n", "e", "[", "\"summary\"", "]", "=", "e", ".", "setdefault", "(", "\"summary\"", ",", "[", "]", ")", "\n", "example", "=", "Example", "(", "e", "[", "\"text\"", "]", ",", "e", "[", "\"summary\"", "]", ",", "self", ".", "vocab", ",", "self", ".", "sent_max_len", ",", "e", "[", "\"label\"", "]", ")", "\n", "return", "example", "\n", "\n"]], "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.module.dataloader.ExampleSet.pad_label_m": [[193, 200], ["numpy.zeros", "numpy.hstack"], "methods", ["None"], ["", "def", "pad_label_m", "(", "self", ",", "label_matrix", ")", ":", "\n", "        ", "label_m", "=", "label_matrix", "[", ":", "self", ".", "doc_max_timesteps", ",", ":", "self", ".", "doc_max_timesteps", "]", "\n", "N", ",", "m", "=", "label_m", ".", "shape", "\n", "if", "m", "<", "self", ".", "doc_max_timesteps", ":", "\n", "            ", "pad_m", "=", "np", ".", "zeros", "(", "(", "N", ",", "self", ".", "doc_max_timesteps", "-", "m", ")", ")", "\n", "return", "np", ".", "hstack", "(", "[", "label_m", ",", "pad_m", "]", ")", "\n", "", "return", "label_m", "\n", "\n"]], "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.module.dataloader.ExampleSet.AddWordNode": [[201, 221], ["len", "G.add_nodes", "G.set_n_initializer", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "list", "nid2wid.values", "wid2nid.keys"], "methods", ["None"], ["", "def", "AddWordNode", "(", "self", ",", "G", ",", "inputid", ")", ":", "\n", "        ", "wid2nid", "=", "{", "}", "\n", "nid2wid", "=", "{", "}", "\n", "nid", "=", "0", "\n", "for", "sentid", "in", "inputid", ":", "\n", "            ", "for", "wid", "in", "sentid", ":", "\n", "                ", "if", "wid", "not", "in", "self", ".", "filterids", "and", "wid", "not", "in", "wid2nid", ".", "keys", "(", ")", ":", "\n", "                    ", "wid2nid", "[", "wid", "]", "=", "nid", "\n", "nid2wid", "[", "nid", "]", "=", "wid", "\n", "nid", "+=", "1", "\n", "\n", "", "", "", "w_nodes", "=", "len", "(", "nid2wid", ")", "\n", "\n", "G", ".", "add_nodes", "(", "w_nodes", ")", "\n", "G", ".", "set_n_initializer", "(", "dgl", ".", "init", ".", "zero_initializer", ")", "\n", "G", ".", "ndata", "[", "\"unit\"", "]", "=", "torch", ".", "zeros", "(", "w_nodes", ")", "\n", "G", ".", "ndata", "[", "\"id\"", "]", "=", "torch", ".", "LongTensor", "(", "list", "(", "nid2wid", ".", "values", "(", ")", ")", ")", "\n", "G", ".", "ndata", "[", "\"dtype\"", "]", "=", "torch", ".", "zeros", "(", "w_nodes", ")", "\n", "\n", "return", "wid2nid", ",", "nid2wid", "\n", "\n"]], "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.module.dataloader.ExampleSet.CreateGraph": [[222, 269], ["dgl.DGLGraph", "dgl.DGLGraph", "dgl.DGLGraph", "dgl.DGLGraph", "dataloader.ExampleSet.AddWordNode", "len", "len", "dgl.DGLGraph.add_nodes", "dgl.DGLGraph.add_nodes", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "dgl.DGLGraph.set_e_initializer", "dgl.DGLGraph.set_e_initializer", "range", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.arange().view().long", "torch.arange().view().long", "torch.arange().view().long", "torch.arange().view().long", "torch.arange().view().long", "torch.arange().view().long", "torch.arange().view().long", "torch.arange().view().long", "torch.arange().view().long", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "collections.Counter", "collections.Counter.keys", "dgl.DGLGraph.add_edges", "dgl.DGLGraph.add_edges", "dgl.DGLGraph.add_edges", "dgl.DGLGraph.add_edges", "range", "torch.arange().view", "torch.arange().view", "torch.arange().view", "torch.arange().view", "torch.arange().view", "torch.arange().view", "torch.arange().view", "torch.arange().view", "torch.arange().view", "str", "numpy.round", "dgl.DGLGraph.add_edges", "dgl.DGLGraph.add_edges", "dgl.DGLGraph.add_edges", "dgl.DGLGraph.add_edges", "wid2nid.keys", "dataloader.ExampleSet.vocab.id2word", "sent_tfw.keys", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "dataloader.ExampleSet.vocab.id2word", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor"], "methods", ["home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.module.dataloader.ExampleSet.AddWordNode", "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.module.vocabulary.Vocab.id2word", "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.module.vocabulary.Vocab.id2word"], ["", "def", "CreateGraph", "(", "self", ",", "input_pad", ",", "label", ",", "w2s_w", ")", ":", "\n", "        ", "\"\"\" Create a graph for each document\n        \n        :param input_pad: list(list); [sentnum, wordnum]\n        :param label: list(list); [sentnum, sentnum]\n        :param w2s_w: dict(dict) {str: {str: float}}; for each sentence and each word, the tfidf between them\n        :return: G: dgl.DGLGraph\n            node:\n                word: unit=0, dtype=0, id=(int)wordid in vocab\n                sentence: unit=1, dtype=1, words=tensor, position=int, label=tensor\n            edge:\n                word2sent, sent2word:  tffrac=int, dtype=0\n        \"\"\"", "\n", "G", "=", "dgl", ".", "DGLGraph", "(", ")", "\n", "wid2nid", ",", "nid2wid", "=", "self", ".", "AddWordNode", "(", "G", ",", "input_pad", ")", "\n", "w_nodes", "=", "len", "(", "nid2wid", ")", "\n", "\n", "N", "=", "len", "(", "input_pad", ")", "\n", "G", ".", "add_nodes", "(", "N", ")", "\n", "G", ".", "ndata", "[", "\"unit\"", "]", "[", "w_nodes", ":", "]", "=", "torch", ".", "ones", "(", "N", ")", "\n", "G", ".", "ndata", "[", "\"dtype\"", "]", "[", "w_nodes", ":", "]", "=", "torch", ".", "ones", "(", "N", ")", "\n", "sentid2nid", "=", "[", "i", "+", "w_nodes", "for", "i", "in", "range", "(", "N", ")", "]", "\n", "\n", "G", ".", "set_e_initializer", "(", "dgl", ".", "init", ".", "zero_initializer", ")", "\n", "for", "i", "in", "range", "(", "N", ")", ":", "\n", "            ", "c", "=", "Counter", "(", "input_pad", "[", "i", "]", ")", "\n", "sent_nid", "=", "sentid2nid", "[", "i", "]", "\n", "sent_tfw", "=", "w2s_w", "[", "str", "(", "i", ")", "]", "\n", "for", "wid", "in", "c", ".", "keys", "(", ")", ":", "\n", "                ", "if", "wid", "in", "wid2nid", ".", "keys", "(", ")", "and", "self", ".", "vocab", ".", "id2word", "(", "wid", ")", "in", "sent_tfw", ".", "keys", "(", ")", ":", "\n", "                    ", "tfidf", "=", "sent_tfw", "[", "self", ".", "vocab", ".", "id2word", "(", "wid", ")", "]", "\n", "tfidf_box", "=", "np", ".", "round", "(", "tfidf", "*", "9", ")", "# box = 10", "\n", "G", ".", "add_edges", "(", "wid2nid", "[", "wid", "]", ",", "sent_nid", ",", "\n", "data", "=", "{", "\"tffrac\"", ":", "torch", ".", "LongTensor", "(", "[", "tfidf_box", "]", ")", ",", "\"dtype\"", ":", "torch", ".", "Tensor", "(", "[", "0", "]", ")", "}", ")", "\n", "G", ".", "add_edges", "(", "sent_nid", ",", "wid2nid", "[", "wid", "]", ",", "\n", "data", "=", "{", "\"tffrac\"", ":", "torch", ".", "LongTensor", "(", "[", "tfidf_box", "]", ")", ",", "\"dtype\"", ":", "torch", ".", "Tensor", "(", "[", "0", "]", ")", "}", ")", "\n", "\n", "# The two lines can be commented out if you use the code for your own training, since HSG does not use sent2sent edges. ", "\n", "# However, if you want to use the released checkpoint directly, please leave them here.", "\n", "# Otherwise it may cause some parameter corresponding errors due to the version differences.", "\n", "", "", "G", ".", "add_edges", "(", "sent_nid", ",", "sentid2nid", ",", "data", "=", "{", "\"dtype\"", ":", "torch", ".", "ones", "(", "N", ")", "}", ")", "\n", "G", ".", "add_edges", "(", "sentid2nid", ",", "sent_nid", ",", "data", "=", "{", "\"dtype\"", ":", "torch", ".", "ones", "(", "N", ")", "}", ")", "\n", "", "G", ".", "nodes", "[", "sentid2nid", "]", ".", "data", "[", "\"words\"", "]", "=", "torch", ".", "LongTensor", "(", "input_pad", ")", "# [N, seq_len]", "\n", "G", ".", "nodes", "[", "sentid2nid", "]", ".", "data", "[", "\"position\"", "]", "=", "torch", ".", "arange", "(", "1", ",", "N", "+", "1", ")", ".", "view", "(", "-", "1", ",", "1", ")", ".", "long", "(", ")", "# [N, 1]", "\n", "G", ".", "nodes", "[", "sentid2nid", "]", ".", "data", "[", "\"label\"", "]", "=", "torch", ".", "LongTensor", "(", "label", ")", "# [N, doc_max]", "\n", "\n", "return", "G", "\n", "\n"]], "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.module.dataloader.ExampleSet.__getitem__": [[270, 284], ["dataloader.ExampleSet.get_example", "dataloader.ExampleSet.pad_label_m", "dataloader.ExampleSet.CreateGraph"], "methods", ["home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.module.dataloader.MultiExampleSet.get_example", "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.module.dataloader.ExampleSet.pad_label_m", "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.module.dataloader.MultiExampleSet.CreateGraph"], ["", "def", "__getitem__", "(", "self", ",", "index", ")", ":", "\n", "        ", "\"\"\"\n        :param index: int; the index of the example\n        :return \n            G: graph for the example\n            index: int; the index of the example in the dataset\n        \"\"\"", "\n", "item", "=", "self", ".", "get_example", "(", "index", ")", "\n", "input_pad", "=", "item", ".", "enc_sent_input_pad", "[", ":", "self", ".", "doc_max_timesteps", "]", "\n", "label", "=", "self", ".", "pad_label_m", "(", "item", ".", "label_matrix", ")", "\n", "w2s_w", "=", "self", ".", "w2s_tfidf", "[", "index", "]", "\n", "G", "=", "self", ".", "CreateGraph", "(", "input_pad", ",", "label", ",", "w2s_w", ")", "\n", "\n", "return", "G", ",", "index", "\n", "\n"]], "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.module.dataloader.ExampleSet.__len__": [[285, 287], ["None"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "size", "\n", "\n"]], "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.module.dataloader.MultiExampleSet.__init__": [[291, 307], ["dataloader.ExampleSet.__init__", "logger.info", "dataloader.readJson"], "methods", ["home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.module.GATStackLayer.MultiHeadLayer.__init__", "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.module.dataloader.readJson"], ["def", "__init__", "(", "self", ",", "data_path", ",", "vocab", ",", "doc_max_timesteps", ",", "sent_max_len", ",", "filter_word_path", ",", "w2s_path", ",", "w2d_path", ")", ":", "\n", "        ", "\"\"\" Initializes the ExampleSet with the path of data\n\n        :param data_path: string; the path of data\n        :param vocab: object;\n        :param doc_max_timesteps: int; the maximum sentence number of a document, each example should pad sentences to this length\n        :param sent_max_len: int; the maximum token number of a sentence, each sentence should pad tokens to this length\n        :param filter_word_path: str; file path, the file must contain one word for each line and the tfidf value must go from low to high (the format can refer to script/lowTFIDFWords.py) \n        :param w2s_path: str; file path, each line in the file contain a json format data (which can refer to the format can refer to script/calw2sTFIDF.py)\n        :param w2d_path: str; file path, each line in the file contain a json format data (which can refer to the format can refer to script/calw2dTFIDF.py)\n        \"\"\"", "\n", "\n", "super", "(", ")", ".", "__init__", "(", "data_path", ",", "vocab", ",", "doc_max_timesteps", ",", "sent_max_len", ",", "filter_word_path", ",", "w2s_path", ")", "\n", "\n", "logger", ".", "info", "(", "\"[INFO] Loading word2doc TFIDF file from %s!\"", "%", "w2d_path", ")", "\n", "self", ".", "w2d_tfidf", "=", "readJson", "(", "w2d_path", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.module.dataloader.MultiExampleSet.get_example": [[308, 313], ["e.setdefault", "dataloader.Example2"], "methods", ["None"], ["", "def", "get_example", "(", "self", ",", "index", ")", ":", "\n", "        ", "e", "=", "self", ".", "example_list", "[", "index", "]", "\n", "e", "[", "\"summary\"", "]", "=", "e", ".", "setdefault", "(", "\"summary\"", ",", "[", "]", ")", "\n", "example", "=", "Example2", "(", "e", "[", "\"text\"", "]", ",", "e", "[", "\"summary\"", "]", ",", "self", ".", "vocab", ",", "self", ".", "sent_max_len", ",", "e", "[", "\"label\"", "]", ")", "\n", "return", "example", "\n", "\n"]], "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.module.dataloader.MultiExampleSet.MapSent2Doc": [[314, 327], ["range", "len", "range", "doc2sent[].append"], "methods", ["None"], ["", "def", "MapSent2Doc", "(", "self", ",", "article_len", ",", "sentNum", ")", ":", "\n", "        ", "sent2doc", "=", "{", "}", "\n", "doc2sent", "=", "{", "}", "\n", "sentNo", "=", "0", "\n", "for", "i", "in", "range", "(", "len", "(", "article_len", ")", ")", ":", "\n", "            ", "doc2sent", "[", "i", "]", "=", "[", "]", "\n", "for", "j", "in", "range", "(", "article_len", "[", "i", "]", ")", ":", "\n", "                ", "sent2doc", "[", "sentNo", "]", "=", "i", "\n", "doc2sent", "[", "i", "]", ".", "append", "(", "sentNo", ")", "\n", "sentNo", "+=", "1", "\n", "if", "sentNo", ">=", "sentNum", ":", "\n", "                    ", "return", "sent2doc", "\n", "", "", "", "return", "sent2doc", "\n", "\n"]], "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.module.dataloader.MultiExampleSet.CreateGraph": [[328, 407], ["dgl.DGLGraph", "dgl.DGLGraph", "dgl.DGLGraph", "dgl.DGLGraph", "dataloader.MultiExampleSet.AddWordNode", "len", "len", "dgl.DGLGraph.add_nodes", "dgl.DGLGraph.add_nodes", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "dataloader.MultiExampleSet.MapSent2Doc", "len", "dgl.DGLGraph.add_nodes", "dgl.DGLGraph.add_nodes", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "range", "range", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.arange().view().long", "torch.arange().view().long", "torch.arange().view().long", "torch.arange().view().long", "torch.arange().view().long", "torch.arange().view().long", "torch.arange().view().long", "torch.arange().view().long", "torch.arange().view().long", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "set", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "collections.Counter", "collections.Counter.items", "dgl.DGLGraph.add_edge", "dgl.DGLGraph.add_edge", "collections.Counter", "collections.Counter.items", "range", "dataloader.MultiExampleSet.values", "range", "torch.arange().view", "torch.arange().view", "torch.arange().view", "torch.arange().view", "torch.arange().view", "torch.arange().view", "torch.arange().view", "torch.arange().view", "torch.arange().view", "str", "numpy.round", "dgl.DGLGraph.add_edge", "dgl.DGLGraph.add_edge", "dgl.DGLGraph.add_edge", "dgl.DGLGraph.add_edge", "str", "numpy.round", "dgl.DGLGraph.add_edge", "dgl.DGLGraph.add_edge", "dgl.DGLGraph.add_edge", "dgl.DGLGraph.add_edge", "wid2nid.keys", "dataloader.MultiExampleSet.vocab.id2word", "sent_tfw.keys", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "wid2nid.keys", "dataloader.MultiExampleSet.vocab.id2word", "doc_tfw.keys", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "dataloader.MultiExampleSet.vocab.id2word", "dataloader.MultiExampleSet.vocab.id2word", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor"], "methods", ["home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.module.dataloader.ExampleSet.AddWordNode", "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.module.dataloader.MultiExampleSet.MapSent2Doc", "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.module.vocabulary.Vocab.id2word", "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.module.vocabulary.Vocab.id2word", "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.module.vocabulary.Vocab.id2word", "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.module.vocabulary.Vocab.id2word"], ["", "def", "CreateGraph", "(", "self", ",", "docLen", ",", "sent_pad", ",", "doc_pad", ",", "label", ",", "w2s_w", ",", "w2d_w", ")", ":", "\n", "        ", "\"\"\" Create a graph for each document\n\n        :param docLen: list; the length of each document in this example\n        :param sent_pad: list(list), [sentnum, wordnum]\n        :param doc_pad: list, [document, wordnum]\n        :param label: list(list), [sentnum, sentnum]\n        :param w2s_w: dict(dict) {str: {str: float}}, for each sentence and each word, the tfidf between them\n        :param w2d_w: dict(dict) {str: {str: float}}, for each document and each word, the tfidf between them\n        :return: G: dgl.DGLGraph\n            node:\n                word: unit=0, dtype=0, id=(int)wordid in vocab\n                sentence: unit=1, dtype=1, words=tensor, position=int, label=tensor\n                document: unit=1, dtype=2\n            edge:\n                word2sent, sent2word: tffrac=int, dtype=0\n                word2doc, doc2word: tffrac=int, dtype=0\n                sent2doc: dtype=2\n        \"\"\"", "\n", "# add word nodes", "\n", "G", "=", "dgl", ".", "DGLGraph", "(", ")", "\n", "wid2nid", ",", "nid2wid", "=", "self", ".", "AddWordNode", "(", "G", ",", "sent_pad", ")", "\n", "w_nodes", "=", "len", "(", "nid2wid", ")", "\n", "\n", "# add sent nodes", "\n", "N", "=", "len", "(", "sent_pad", ")", "\n", "G", ".", "add_nodes", "(", "N", ")", "\n", "G", ".", "ndata", "[", "\"unit\"", "]", "[", "w_nodes", ":", "]", "=", "torch", ".", "ones", "(", "N", ")", "\n", "G", ".", "ndata", "[", "\"dtype\"", "]", "[", "w_nodes", ":", "]", "=", "torch", ".", "ones", "(", "N", ")", "\n", "sentid2nid", "=", "[", "i", "+", "w_nodes", "for", "i", "in", "range", "(", "N", ")", "]", "\n", "ws_nodes", "=", "w_nodes", "+", "N", "\n", "\n", "# add doc nodes", "\n", "sent2doc", "=", "self", ".", "MapSent2Doc", "(", "docLen", ",", "N", ")", "\n", "article_num", "=", "len", "(", "set", "(", "sent2doc", ".", "values", "(", ")", ")", ")", "\n", "G", ".", "add_nodes", "(", "article_num", ")", "\n", "G", ".", "ndata", "[", "\"unit\"", "]", "[", "ws_nodes", ":", "]", "=", "torch", ".", "ones", "(", "article_num", ")", "\n", "G", ".", "ndata", "[", "\"dtype\"", "]", "[", "ws_nodes", ":", "]", "=", "torch", ".", "ones", "(", "article_num", ")", "*", "2", "\n", "docid2nid", "=", "[", "i", "+", "ws_nodes", "for", "i", "in", "range", "(", "article_num", ")", "]", "\n", "\n", "# add sent edges", "\n", "for", "i", "in", "range", "(", "N", ")", ":", "\n", "            ", "c", "=", "Counter", "(", "sent_pad", "[", "i", "]", ")", "\n", "sent_nid", "=", "sentid2nid", "[", "i", "]", "\n", "sent_tfw", "=", "w2s_w", "[", "str", "(", "i", ")", "]", "\n", "for", "wid", ",", "cnt", "in", "c", ".", "items", "(", ")", ":", "\n", "                ", "if", "wid", "in", "wid2nid", ".", "keys", "(", ")", "and", "self", ".", "vocab", ".", "id2word", "(", "wid", ")", "in", "sent_tfw", ".", "keys", "(", ")", ":", "\n", "                    ", "tfidf", "=", "sent_tfw", "[", "self", ".", "vocab", ".", "id2word", "(", "wid", ")", "]", "\n", "tfidf_box", "=", "np", ".", "round", "(", "tfidf", "*", "9", ")", "# box = 10", "\n", "# w2s s2w", "\n", "G", ".", "add_edge", "(", "wid2nid", "[", "wid", "]", ",", "sent_nid", ",", "\n", "data", "=", "{", "\"tffrac\"", ":", "torch", ".", "LongTensor", "(", "[", "tfidf_box", "]", ")", ",", "\"dtype\"", ":", "torch", ".", "Tensor", "(", "[", "0", "]", ")", "}", ")", "\n", "G", ".", "add_edge", "(", "sent_nid", ",", "wid2nid", "[", "wid", "]", ",", "\n", "data", "=", "{", "\"tffrac\"", ":", "torch", ".", "LongTensor", "(", "[", "tfidf_box", "]", ")", ",", "\"dtype\"", ":", "torch", ".", "Tensor", "(", "[", "0", "]", ")", "}", ")", "\n", "# s2d", "\n", "", "", "docid", "=", "sent2doc", "[", "i", "]", "\n", "docnid", "=", "docid2nid", "[", "docid", "]", "\n", "G", ".", "add_edge", "(", "sent_nid", ",", "docnid", ",", "data", "=", "{", "\"tffrac\"", ":", "torch", ".", "LongTensor", "(", "[", "0", "]", ")", ",", "\"dtype\"", ":", "torch", ".", "Tensor", "(", "[", "2", "]", ")", "}", ")", "\n", "\n", "# add doc edges", "\n", "", "for", "i", "in", "range", "(", "article_num", ")", ":", "\n", "            ", "c", "=", "Counter", "(", "doc_pad", "[", "i", "]", ")", "\n", "doc_nid", "=", "docid2nid", "[", "i", "]", "\n", "doc_tfw", "=", "w2d_w", "[", "str", "(", "i", ")", "]", "\n", "for", "wid", ",", "cnt", "in", "c", ".", "items", "(", ")", ":", "\n", "                ", "if", "wid", "in", "wid2nid", ".", "keys", "(", ")", "and", "self", ".", "vocab", ".", "id2word", "(", "wid", ")", "in", "doc_tfw", ".", "keys", "(", ")", ":", "\n", "# w2d d2w", "\n", "                    ", "tfidf", "=", "doc_tfw", "[", "self", ".", "vocab", ".", "id2word", "(", "wid", ")", "]", "\n", "tfidf_box", "=", "np", ".", "round", "(", "tfidf", "*", "9", ")", "# box = 10", "\n", "G", ".", "add_edge", "(", "wid2nid", "[", "wid", "]", ",", "doc_nid", ",", "\n", "data", "=", "{", "\"tffrac\"", ":", "torch", ".", "LongTensor", "(", "[", "tfidf_box", "]", ")", ",", "\"dtype\"", ":", "torch", ".", "Tensor", "(", "[", "0", "]", ")", "}", ")", "\n", "G", ".", "add_edge", "(", "doc_nid", ",", "wid2nid", "[", "wid", "]", ",", "\n", "data", "=", "{", "\"tffrac\"", ":", "torch", ".", "LongTensor", "(", "[", "tfidf_box", "]", ")", ",", "\"dtype\"", ":", "torch", ".", "Tensor", "(", "[", "0", "]", ")", "}", ")", "\n", "\n", "", "", "", "G", ".", "nodes", "[", "sentid2nid", "]", ".", "data", "[", "\"words\"", "]", "=", "torch", ".", "LongTensor", "(", "sent_pad", ")", "# [N, seq_len]", "\n", "G", ".", "nodes", "[", "sentid2nid", "]", ".", "data", "[", "\"position\"", "]", "=", "torch", ".", "arange", "(", "1", ",", "N", "+", "1", ")", ".", "view", "(", "-", "1", ",", "1", ")", ".", "long", "(", ")", "# [N, 1]", "\n", "G", ".", "nodes", "[", "sentid2nid", "]", ".", "data", "[", "\"label\"", "]", "=", "torch", ".", "LongTensor", "(", "label", ")", "# [N, doc_max]", "\n", "\n", "return", "G", "\n", "\n"]], "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.module.dataloader.MultiExampleSet.__getitem__": [[408, 424], ["dataloader.MultiExampleSet.get_example", "dataloader.MultiExampleSet.pad_label_m", "dataloader.MultiExampleSet.CreateGraph"], "methods", ["home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.module.dataloader.MultiExampleSet.get_example", "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.module.dataloader.ExampleSet.pad_label_m", "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.module.dataloader.MultiExampleSet.CreateGraph"], ["", "def", "__getitem__", "(", "self", ",", "index", ")", ":", "\n", "        ", "\"\"\"\n        :param index: int; the index of the example\n        :return \n            G: graph for the example\n            index: int; the index of the example in the dataset\n        \"\"\"", "\n", "item", "=", "self", ".", "get_example", "(", "index", ")", "\n", "sent_pad", "=", "item", ".", "enc_sent_input_pad", "[", ":", "self", ".", "doc_max_timesteps", "]", "\n", "enc_doc_input", "=", "item", ".", "enc_doc_input", "\n", "article_len", "=", "item", ".", "article_len", "\n", "label", "=", "self", ".", "pad_label_m", "(", "item", ".", "label_matrix", ")", "\n", "\n", "G", "=", "self", ".", "CreateGraph", "(", "article_len", ",", "sent_pad", ",", "enc_doc_input", ",", "label", ",", "self", ".", "w2s_tfidf", "[", "index", "]", ",", "self", ".", "w2d_tfidf", "[", "index", "]", ")", "\n", "\n", "return", "G", ",", "index", "\n", "\n"]], "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.module.dataloader.LoadHiExampleSet.__init__": [[427, 432], ["super().__init__", "logger.info", "os.listdir", "f.endswith"], "methods", ["home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.module.GATStackLayer.MultiHeadLayer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "data_root", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "data_root", "=", "data_root", "\n", "self", ".", "gfiles", "=", "[", "f", "for", "f", "in", "os", ".", "listdir", "(", "self", ".", "data_root", ")", "if", "f", ".", "endswith", "(", "\"graph.bin\"", ")", "]", "\n", "logger", ".", "info", "(", "\"[INFO] Start loading %s\"", ",", "self", ".", "data_root", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.module.dataloader.LoadHiExampleSet.__getitem__": [[433, 438], ["os.path.join", "dgl.data.utils.load_graphs", "dgl.data.utils.load_graphs"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "index", ")", ":", "\n", "        ", "graph_file", "=", "os", ".", "path", ".", "join", "(", "self", ".", "data_root", ",", "\"%d.graph.bin\"", "%", "index", ")", "\n", "g", ",", "label_dict", "=", "load_graphs", "(", "graph_file", ")", "\n", "# print(graph_file)", "\n", "return", "g", "[", "0", "]", ",", "index", "\n", "\n"]], "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.module.dataloader.LoadHiExampleSet.__len__": [[439, 441], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "gfiles", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.module.dataloader.catDoc": [[449, 454], ["res.extend"], "function", ["None"], ["def", "catDoc", "(", "textlist", ")", ":", "\n", "    ", "res", "=", "[", "]", "\n", "for", "tlist", "in", "textlist", ":", "\n", "        ", "res", ".", "extend", "(", "tlist", ")", "\n", "", "return", "res", "\n", "\n"]], "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.module.dataloader.readJson": [[456, 462], ["open", "data.append", "json.loads"], "function", ["None"], ["", "def", "readJson", "(", "fname", ")", ":", "\n", "    ", "data", "=", "[", "]", "\n", "with", "open", "(", "fname", ",", "encoding", "=", "\"utf-8\"", ")", "as", "f", ":", "\n", "        ", "for", "line", "in", "f", ":", "\n", "            ", "data", ".", "append", "(", "json", ".", "loads", "(", "line", ")", ")", "\n", "", "", "return", "data", "\n", "\n"]], "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.module.dataloader.readText": [[464, 470], ["open", "data.append", "line.strip"], "function", ["None"], ["", "def", "readText", "(", "fname", ")", ":", "\n", "    ", "data", "=", "[", "]", "\n", "with", "open", "(", "fname", ",", "encoding", "=", "\"utf-8\"", ")", "as", "f", ":", "\n", "        ", "for", "line", "in", "f", ":", "\n", "            ", "data", ".", "append", "(", "line", ".", "strip", "(", ")", ")", "\n", "", "", "return", "data", "\n", "\n"]], "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.module.dataloader.graph_collate_fn": [[472, 482], ["map", "torch.sort", "torch.sort", "torch.sort", "dgl.batch", "dgl.batch", "zip", "len", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "g.filter_nodes"], "function", ["None"], ["", "def", "graph_collate_fn", "(", "samples", ")", ":", "\n", "    ", "'''\n    :param batch: (G, input_pad)\n    :return: \n    '''", "\n", "graphs", ",", "index", "=", "map", "(", "list", ",", "zip", "(", "*", "samples", ")", ")", "\n", "graph_len", "=", "[", "len", "(", "g", ".", "filter_nodes", "(", "lambda", "nodes", ":", "nodes", ".", "data", "[", "\"dtype\"", "]", "==", "1", ")", ")", "for", "g", "in", "graphs", "]", "# sent node of graph", "\n", "sorted_len", ",", "sorted_index", "=", "torch", ".", "sort", "(", "torch", ".", "LongTensor", "(", "graph_len", ")", ",", "dim", "=", "0", ",", "descending", "=", "True", ")", "\n", "batched_graph", "=", "dgl", ".", "batch", "(", "[", "graphs", "[", "idx", "]", "for", "idx", "in", "sorted_index", "]", ")", "\n", "return", "batched_graph", ",", "[", "index", "[", "idx", "]", "for", "idx", "in", "sorted_index", "]", "\n", "", ""]], "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.module.GATStackLayer.MultiHeadSGATLayer.__init__": [[28, 36], ["torch.Module.__init__", "torch.ModuleList", "torch.ModuleList", "range", "torch.Dropout", "torch.Dropout", "GATStackLayer.MultiHeadSGATLayer.heads.append", "module.GATLayer.SGATLayer"], "methods", ["home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.module.GATStackLayer.MultiHeadLayer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "in_dim", ",", "out_dim", ",", "num_heads", ",", "attn_drop_out", ",", "merge", "=", "'cat'", ")", ":", "\n", "        ", "super", "(", "MultiHeadSGATLayer", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "heads", "=", "nn", ".", "ModuleList", "(", ")", "\n", "for", "i", "in", "range", "(", "num_heads", ")", ":", "\n", "            ", "self", ".", "heads", ".", "append", "(", "\n", "SGATLayer", "(", "in_dim", ",", "out_dim", ")", ")", "# [n_nodes, hidden_size]", "\n", "", "self", ".", "merge", "=", "merge", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "attn_drop_out", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.module.GATStackLayer.MultiHeadSGATLayer.forward": [[37, 45], ["attn_head", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "GATStackLayer.MultiHeadSGATLayer.dropout", "torch.stack", "torch.stack", "torch.stack", "torch.stack"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "g", ",", "h", ")", ":", "\n", "        ", "head_outs", "=", "[", "attn_head", "(", "g", ",", "self", ".", "dropout", "(", "h", ")", ")", "for", "attn_head", "in", "self", ".", "heads", "]", "# n_head * [n_nodes, hidden_size]", "\n", "if", "self", ".", "merge", "==", "'cat'", ":", "\n", "# concat on the output feature dimension (dim=1)", "\n", "            ", "return", "torch", ".", "cat", "(", "head_outs", ",", "dim", "=", "1", ")", "# [n_nodes, hidden_size * n_head]", "\n", "", "else", ":", "\n", "# merge using average", "\n", "            ", "return", "torch", ".", "mean", "(", "torch", ".", "stack", "(", "head_outs", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.module.GATStackLayer.MultiHeadLayer.__init__": [[47, 54], ["torch.Module.__init__", "torch.ModuleList", "torch.ModuleList", "range", "torch.Dropout", "torch.Dropout", "GATStackLayer.MultiHeadLayer.heads.append", "layer"], "methods", ["home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.module.GATStackLayer.MultiHeadLayer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "in_dim", ",", "out_dim", ",", "num_heads", ",", "attn_drop_out", ",", "feat_embed_size", ",", "layer", ",", "merge", "=", "'cat'", ")", ":", "\n", "        ", "super", "(", "MultiHeadLayer", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "heads", "=", "nn", ".", "ModuleList", "(", ")", "\n", "for", "i", "in", "range", "(", "num_heads", ")", ":", "\n", "            ", "self", ".", "heads", ".", "append", "(", "layer", "(", "in_dim", ",", "out_dim", ",", "feat_embed_size", ")", ")", "# [n_nodes, hidden_size]", "\n", "", "self", ".", "merge", "=", "merge", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "attn_drop_out", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.module.GATStackLayer.MultiHeadLayer.forward": [[55, 64], ["attn_head", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "GATStackLayer.MultiHeadLayer.dropout", "torch.stack", "torch.stack", "torch.stack", "torch.stack"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "g", ",", "h", ")", ":", "\n", "        ", "head_outs", "=", "[", "attn_head", "(", "g", ",", "self", ".", "dropout", "(", "h", ")", ")", "for", "attn_head", "in", "self", ".", "heads", "]", "# n_head * [n_nodes, hidden_size]", "\n", "if", "self", ".", "merge", "==", "'cat'", ":", "\n", "# concat on the output feature dimension (dim=1)", "\n", "            ", "result", "=", "torch", ".", "cat", "(", "head_outs", ",", "dim", "=", "1", ")", "# [n_nodes, hidden_size * n_head]", "\n", "", "else", ":", "\n", "# merge using average", "\n", "            ", "result", "=", "torch", ".", "mean", "(", "torch", ".", "stack", "(", "head_outs", ")", ")", "\n", "", "return", "result", "", "", "", ""]], "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.module.PositionEmbedding.get_sinusoid_encoding_table": [[20, 39], ["numpy.array", "numpy.sin", "numpy.cos", "torch.FloatTensor", "numpy.power", "PositionEmbedding.get_sinusoid_encoding_table.cal_angle"], "function", ["None"], ["def", "get_sinusoid_encoding_table", "(", "n_position", ",", "d_hid", ",", "padding_idx", "=", "None", ")", ":", "\n", "    ", "''' Sinusoid position encoding table '''", "\n", "\n", "def", "cal_angle", "(", "position", ",", "hid_idx", ")", ":", "\n", "        ", "return", "position", "/", "np", ".", "power", "(", "10000", ",", "2", "*", "(", "hid_idx", "//", "2", ")", "/", "d_hid", ")", "\n", "\n", "", "def", "get_posi_angle_vec", "(", "position", ")", ":", "\n", "        ", "return", "[", "cal_angle", "(", "position", ",", "hid_j", ")", "for", "hid_j", "in", "range", "(", "d_hid", ")", "]", "\n", "\n", "", "sinusoid_table", "=", "np", ".", "array", "(", "[", "get_posi_angle_vec", "(", "pos_i", ")", "for", "pos_i", "in", "range", "(", "n_position", ")", "]", ")", "\n", "\n", "sinusoid_table", "[", ":", ",", "0", ":", ":", "2", "]", "=", "np", ".", "sin", "(", "sinusoid_table", "[", ":", ",", "0", ":", ":", "2", "]", ")", "# dim 2i", "\n", "sinusoid_table", "[", ":", ",", "1", ":", ":", "2", "]", "=", "np", ".", "cos", "(", "sinusoid_table", "[", ":", ",", "1", ":", ":", "2", "]", ")", "# dim 2i+1", "\n", "\n", "if", "padding_idx", "is", "not", "None", ":", "\n", "# zero vector for padding dimension", "\n", "        ", "sinusoid_table", "[", "padding_idx", "]", "=", "0.", "\n", "\n", "", "return", "torch", ".", "FloatTensor", "(", "sinusoid_table", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.tools.utils.clean": [[25, 30], ["x.lower.lower", "re.sub", "REMAP.get", "m.group"], "function", ["None"], ["def", "clean", "(", "x", ")", ":", "\n", "    ", "x", "=", "x", ".", "lower", "(", ")", "\n", "return", "re", ".", "sub", "(", "\n", "r\"-lrb-|-rrb-|-lcb-|-rcb-|-lsb-|-rsb-|``|''\"", ",", "\n", "lambda", "m", ":", "REMAP", ".", "get", "(", "m", ".", "group", "(", ")", ")", ",", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.tools.utils.rouge_eval": [[31, 39], ["rouge.Rouge", "numpy.mean", "rouge.Rouge.get_scores"], "function", ["None"], ["", "def", "rouge_eval", "(", "hyps", ",", "refer", ")", ":", "\n", "    ", "rouge", "=", "Rouge", "(", ")", "\n", "try", ":", "\n", "        ", "score", "=", "rouge", ".", "get_scores", "(", "hyps", ",", "refer", ")", "[", "0", "]", "\n", "mean_score", "=", "np", ".", "mean", "(", "[", "score", "[", "\"rouge-1\"", "]", "[", "\"f\"", "]", ",", "score", "[", "\"rouge-2\"", "]", "[", "\"f\"", "]", ",", "score", "[", "\"rouge-l\"", "]", "[", "\"f\"", "]", "]", ")", "\n", "", "except", ":", "\n", "        ", "mean_score", "=", "0.0", "\n", "", "return", "mean_score", "\n", "\n"]], "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.tools.utils.rouge_all": [[40, 44], ["rouge.Rouge", "rouge.Rouge.get_scores"], "function", ["None"], ["", "def", "rouge_all", "(", "hyps", ",", "refer", ")", ":", "\n", "    ", "rouge", "=", "Rouge", "(", ")", "\n", "score", "=", "rouge", ".", "get_scores", "(", "hyps", ",", "refer", ")", "[", "0", "]", "\n", "return", "score", "\n", "\n"]], "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.tools.utils.eval_label": [[45, 56], ["match_true.float", "pred.float", "true.float", "match.float", "logger.error"], "function", ["None"], ["", "def", "eval_label", "(", "match_true", ",", "pred", ",", "true", ",", "total", ",", "match", ")", ":", "\n", "    ", "match_true", ",", "pred", ",", "true", ",", "match", "=", "match_true", ".", "float", "(", ")", ",", "pred", ".", "float", "(", ")", ",", "true", ".", "float", "(", ")", ",", "match", ".", "float", "(", ")", "\n", "try", ":", "\n", "        ", "accu", "=", "match", "/", "total", "\n", "precision", "=", "match_true", "/", "pred", "\n", "recall", "=", "match_true", "/", "true", "\n", "F", "=", "2", "*", "precision", "*", "recall", "/", "(", "precision", "+", "recall", ")", "\n", "", "except", "ZeroDivisionError", ":", "\n", "        ", "accu", ",", "precision", ",", "recall", ",", "F", "=", "0.0", ",", "0.0", ",", "0.0", ",", "0.0", "\n", "logger", ".", "error", "(", "\"[Error] float division by zero\"", ")", "\n", "", "return", "accu", ",", "precision", ",", "recall", ",", "F", "\n", "\n"]], "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.tools.utils.pyrouge_score": [[58, 60], ["utils.pyrouge_score_all"], "function", ["home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.tools.utils.pyrouge_score_all"], ["", "def", "pyrouge_score", "(", "hyps", ",", "refer", ",", "remap", "=", "True", ")", ":", "\n", "    ", "return", "pyrouge_score_all", "(", "[", "hyps", "]", ",", "[", "refer", "]", ",", "remap", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.tools.utils.pyrouge_score_all": [[61, 107], ["datetime.datetime.now().strftime", "os.path.join", "os.path.join", "os.path.join", "os.path.exists", "os.makedirs", "os.path.exists", "os.makedirs", "range", "Rouge155", "shutil.rmtree", "shutil.rmtree", "len", "len", "len", "os.path.join", "os.path.join", "Rouge155.convert_and_evaluate", "Rouge155.output_to_dict", "logger.error", "shutil.rmtree", "datetime.datetime.now", "utils.clean", "utils.clean", "open", "f.write", "open", "f.write", "hyps.encode", "refer.encode", "os.path.join"], "function", ["home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.tools.utils.clean", "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.tools.utils.clean"], ["", "def", "pyrouge_score_all", "(", "hyps_list", ",", "refer_list", ",", "remap", "=", "True", ")", ":", "\n", "    ", "from", "pyrouge", "import", "Rouge155", "\n", "nowTime", "=", "datetime", ".", "datetime", ".", "now", "(", ")", ".", "strftime", "(", "'%Y%m%d_%H%M%S'", ")", "\n", "PYROUGE_ROOT", "=", "os", ".", "path", ".", "join", "(", "_PYROUGE_TEMP_FILE", ",", "nowTime", ")", "\n", "SYSTEM_PATH", "=", "os", ".", "path", ".", "join", "(", "PYROUGE_ROOT", ",", "'result'", ")", "\n", "MODEL_PATH", "=", "os", ".", "path", ".", "join", "(", "PYROUGE_ROOT", ",", "'gold'", ")", "\n", "if", "os", ".", "path", ".", "exists", "(", "SYSTEM_PATH", ")", ":", "\n", "        ", "shutil", ".", "rmtree", "(", "SYSTEM_PATH", ")", "\n", "", "os", ".", "makedirs", "(", "SYSTEM_PATH", ")", "\n", "if", "os", ".", "path", ".", "exists", "(", "MODEL_PATH", ")", ":", "\n", "        ", "shutil", ".", "rmtree", "(", "MODEL_PATH", ")", "\n", "", "os", ".", "makedirs", "(", "MODEL_PATH", ")", "\n", "\n", "assert", "len", "(", "hyps_list", ")", "==", "len", "(", "refer_list", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "hyps_list", ")", ")", ":", "\n", "        ", "system_file", "=", "os", ".", "path", ".", "join", "(", "SYSTEM_PATH", ",", "'Model.%d.txt'", "%", "i", ")", "\n", "model_file", "=", "os", ".", "path", ".", "join", "(", "MODEL_PATH", ",", "'Reference.A.%d.txt'", "%", "i", ")", "\n", "\n", "refer", "=", "clean", "(", "refer_list", "[", "i", "]", ")", "if", "remap", "else", "refer_list", "[", "i", "]", "\n", "hyps", "=", "clean", "(", "hyps_list", "[", "i", "]", ")", "if", "remap", "else", "hyps_list", "[", "i", "]", "\n", "\n", "with", "open", "(", "system_file", ",", "'wb'", ")", "as", "f", ":", "\n", "            ", "f", ".", "write", "(", "hyps", ".", "encode", "(", "'utf-8'", ")", ")", "\n", "", "with", "open", "(", "model_file", ",", "'wb'", ")", "as", "f", ":", "\n", "            ", "f", ".", "write", "(", "refer", ".", "encode", "(", "'utf-8'", ")", ")", "\n", "\n", "", "", "r", "=", "Rouge155", "(", "_ROUGE_PATH", ")", "\n", "\n", "r", ".", "system_dir", "=", "SYSTEM_PATH", "\n", "r", ".", "model_dir", "=", "MODEL_PATH", "\n", "r", ".", "system_filename_pattern", "=", "'Model.(\\d+).txt'", "\n", "r", ".", "model_filename_pattern", "=", "'Reference.[A-Z].#ID#.txt'", "\n", "\n", "try", ":", "\n", "        ", "output", "=", "r", ".", "convert_and_evaluate", "(", "rouge_args", "=", "\"-e %s -a -m -n 2 -d\"", "%", "os", ".", "path", ".", "join", "(", "_ROUGE_PATH", ",", "\"data\"", ")", ")", "\n", "output_dict", "=", "r", ".", "output_to_dict", "(", "output", ")", "\n", "", "finally", ":", "\n", "        ", "logger", ".", "error", "(", "\"[ERROR] Error stop, delete PYROUGE_ROOT...\"", ")", "\n", "shutil", ".", "rmtree", "(", "PYROUGE_ROOT", ")", "\n", "\n", "", "scores", "=", "{", "}", "\n", "scores", "[", "'rouge-1'", "]", ",", "scores", "[", "'rouge-2'", "]", ",", "scores", "[", "'rouge-l'", "]", "=", "{", "}", ",", "{", "}", ",", "{", "}", "\n", "scores", "[", "'rouge-1'", "]", "[", "'p'", "]", ",", "scores", "[", "'rouge-1'", "]", "[", "'r'", "]", ",", "scores", "[", "'rouge-1'", "]", "[", "'f'", "]", "=", "output_dict", "[", "'rouge_1_precision'", "]", ",", "output_dict", "[", "'rouge_1_recall'", "]", ",", "output_dict", "[", "'rouge_1_f_score'", "]", "\n", "scores", "[", "'rouge-2'", "]", "[", "'p'", "]", ",", "scores", "[", "'rouge-2'", "]", "[", "'r'", "]", ",", "scores", "[", "'rouge-2'", "]", "[", "'f'", "]", "=", "output_dict", "[", "'rouge_2_precision'", "]", ",", "output_dict", "[", "'rouge_2_recall'", "]", ",", "output_dict", "[", "'rouge_2_f_score'", "]", "\n", "scores", "[", "'rouge-l'", "]", "[", "'p'", "]", ",", "scores", "[", "'rouge-l'", "]", "[", "'r'", "]", ",", "scores", "[", "'rouge-l'", "]", "[", "'f'", "]", "=", "output_dict", "[", "'rouge_l_precision'", "]", ",", "output_dict", "[", "'rouge_l_recall'", "]", ",", "output_dict", "[", "'rouge_l_f_score'", "]", "\n", "return", "scores", "\n", "\n"]], "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.tools.utils.pyrouge_score_all_multi": [[109, 157], ["datetime.datetime.now().strftime", "os.path.join", "os.path.join", "os.path.join", "os.path.exists", "os.makedirs", "os.path.exists", "os.makedirs", "range", "Rouge155", "Rouge155.convert_and_evaluate", "Rouge155.output_to_dict", "shutil.rmtree", "shutil.rmtree", "shutil.rmtree", "len", "len", "len", "os.path.join", "range", "datetime.datetime.now", "utils.clean", "open", "f.write", "len", "os.path.join", "hyps.encode", "utils.clean", "open", "f.write", "os.path.join", "refer.encode"], "function", ["home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.tools.utils.clean", "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.tools.utils.clean"], ["", "def", "pyrouge_score_all_multi", "(", "hyps_list", ",", "refer_list", ",", "remap", "=", "True", ")", ":", "\n", "    ", "from", "pyrouge", "import", "Rouge155", "\n", "nowTime", "=", "datetime", ".", "datetime", ".", "now", "(", ")", ".", "strftime", "(", "'%Y%m%d_%H%M%S'", ")", "\n", "PYROUGE_ROOT", "=", "os", ".", "path", ".", "join", "(", "_PYROUGE_TEMP_FILE", ",", "nowTime", ")", "\n", "SYSTEM_PATH", "=", "os", ".", "path", ".", "join", "(", "PYROUGE_ROOT", ",", "'result'", ")", "\n", "MODEL_PATH", "=", "os", ".", "path", ".", "join", "(", "PYROUGE_ROOT", ",", "'gold'", ")", "\n", "if", "os", ".", "path", ".", "exists", "(", "SYSTEM_PATH", ")", ":", "\n", "        ", "shutil", ".", "rmtree", "(", "SYSTEM_PATH", ")", "\n", "", "os", ".", "makedirs", "(", "SYSTEM_PATH", ")", "\n", "if", "os", ".", "path", ".", "exists", "(", "MODEL_PATH", ")", ":", "\n", "        ", "shutil", ".", "rmtree", "(", "MODEL_PATH", ")", "\n", "", "os", ".", "makedirs", "(", "MODEL_PATH", ")", "\n", "\n", "assert", "len", "(", "hyps_list", ")", "==", "len", "(", "refer_list", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "hyps_list", ")", ")", ":", "\n", "        ", "system_file", "=", "os", ".", "path", ".", "join", "(", "SYSTEM_PATH", ",", "'Model.%d.txt'", "%", "i", ")", "\n", "# model_file = os.path.join(MODEL_PATH, 'Reference.A.%d.txt' % i)", "\n", "\n", "hyps", "=", "clean", "(", "hyps_list", "[", "i", "]", ")", "if", "remap", "else", "hyps_list", "[", "i", "]", "\n", "\n", "with", "open", "(", "system_file", ",", "'wb'", ")", "as", "f", ":", "\n", "            ", "f", ".", "write", "(", "hyps", ".", "encode", "(", "'utf-8'", ")", ")", "\n", "\n", "", "referType", "=", "[", "\"A\"", ",", "\"B\"", ",", "\"C\"", ",", "\"D\"", ",", "\"E\"", ",", "\"F\"", ",", "\"G\"", "]", "\n", "for", "j", "in", "range", "(", "len", "(", "refer_list", "[", "i", "]", ")", ")", ":", "\n", "            ", "model_file", "=", "os", ".", "path", ".", "join", "(", "MODEL_PATH", ",", "\"Reference.%s.%d.txt\"", "%", "(", "referType", "[", "j", "]", ",", "i", ")", ")", "\n", "refer", "=", "clean", "(", "refer_list", "[", "i", "]", "[", "j", "]", ")", "if", "remap", "else", "refer_list", "[", "i", "]", "[", "j", "]", "\n", "with", "open", "(", "model_file", ",", "'wb'", ")", "as", "f", ":", "\n", "                ", "f", ".", "write", "(", "refer", ".", "encode", "(", "'utf-8'", ")", ")", "\n", "\n", "", "", "", "r", "=", "Rouge155", "(", "_ROUGE_PATH", ")", "\n", "\n", "r", ".", "system_dir", "=", "SYSTEM_PATH", "\n", "r", ".", "model_dir", "=", "MODEL_PATH", "\n", "r", ".", "system_filename_pattern", "=", "'Model.(\\d+).txt'", "\n", "r", ".", "model_filename_pattern", "=", "'Reference.[A-Z].#ID#.txt'", "\n", "\n", "output", "=", "r", ".", "convert_and_evaluate", "(", "rouge_args", "=", "\"-e %s -a -m -n 2 -d\"", "%", "os", ".", "path", ".", "join", "(", "_ROUGE_PATH", ",", "\"data\"", ")", ")", "\n", "output_dict", "=", "r", ".", "output_to_dict", "(", "output", ")", "\n", "\n", "shutil", ".", "rmtree", "(", "PYROUGE_ROOT", ")", "\n", "\n", "scores", "=", "{", "}", "\n", "scores", "[", "'rouge-1'", "]", ",", "scores", "[", "'rouge-2'", "]", ",", "scores", "[", "'rouge-l'", "]", "=", "{", "}", ",", "{", "}", ",", "{", "}", "\n", "scores", "[", "'rouge-1'", "]", "[", "'p'", "]", ",", "scores", "[", "'rouge-1'", "]", "[", "'r'", "]", ",", "scores", "[", "'rouge-1'", "]", "[", "'f'", "]", "=", "output_dict", "[", "'rouge_1_precision'", "]", ",", "output_dict", "[", "'rouge_1_recall'", "]", ",", "output_dict", "[", "'rouge_1_f_score'", "]", "\n", "scores", "[", "'rouge-2'", "]", "[", "'p'", "]", ",", "scores", "[", "'rouge-2'", "]", "[", "'r'", "]", ",", "scores", "[", "'rouge-2'", "]", "[", "'f'", "]", "=", "output_dict", "[", "'rouge_2_precision'", "]", ",", "output_dict", "[", "'rouge_2_recall'", "]", ",", "output_dict", "[", "'rouge_2_f_score'", "]", "\n", "scores", "[", "'rouge-l'", "]", "[", "'p'", "]", ",", "scores", "[", "'rouge-l'", "]", "[", "'r'", "]", ",", "scores", "[", "'rouge-l'", "]", "[", "'f'", "]", "=", "output_dict", "[", "'rouge_l_precision'", "]", ",", "output_dict", "[", "'rouge_l_recall'", "]", ",", "output_dict", "[", "'rouge_l_f_score'", "]", "\n", "return", "scores", "\n", "\n"]], "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.tools.utils.cal_label": [[159, 193], ["selected.append", "numpy.max", "utils.rouge_eval", "scores.append", "int", "len", "range", "numpy.argmax", "len", "selected.append", "copy.deepcopy", "copy.deepcopy.append", "utils.rouge_eval", "numpy.sort"], "function", ["home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.tools.utils.rouge_eval", "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.tools.utils.rouge_eval"], ["", "def", "cal_label", "(", "article", ",", "abstract", ")", ":", "\n", "    ", "hyps_list", "=", "article", "\n", "\n", "refer", "=", "abstract", "\n", "scores", "=", "[", "]", "\n", "for", "hyps", "in", "hyps_list", ":", "\n", "        ", "mean_score", "=", "rouge_eval", "(", "hyps", ",", "refer", ")", "\n", "scores", ".", "append", "(", "mean_score", ")", "\n", "\n", "", "selected", "=", "[", "]", "\n", "selected", ".", "append", "(", "int", "(", "np", ".", "argmax", "(", "scores", ")", ")", ")", "\n", "selected_sent_cnt", "=", "1", "\n", "\n", "best_rouge", "=", "np", ".", "max", "(", "scores", ")", "\n", "while", "selected_sent_cnt", "<", "len", "(", "hyps_list", ")", ":", "\n", "        ", "cur_max_rouge", "=", "0.0", "\n", "cur_max_idx", "=", "-", "1", "\n", "for", "i", "in", "range", "(", "len", "(", "hyps_list", ")", ")", ":", "\n", "            ", "if", "i", "not", "in", "selected", ":", "\n", "                ", "temp", "=", "copy", ".", "deepcopy", "(", "selected", ")", "\n", "temp", ".", "append", "(", "i", ")", "\n", "hyps", "=", "\"\\n\"", ".", "join", "(", "[", "hyps_list", "[", "idx", "]", "for", "idx", "in", "np", ".", "sort", "(", "temp", ")", "]", ")", "\n", "cur_rouge", "=", "rouge_eval", "(", "hyps", ",", "refer", ")", "\n", "if", "cur_rouge", ">", "cur_max_rouge", ":", "\n", "                    ", "cur_max_rouge", "=", "cur_rouge", "\n", "cur_max_idx", "=", "i", "\n", "", "", "", "if", "cur_max_rouge", "!=", "0.0", "and", "cur_max_rouge", ">=", "best_rouge", ":", "\n", "            ", "selected", ".", "append", "(", "cur_max_idx", ")", "\n", "selected_sent_cnt", "+=", "1", "\n", "best_rouge", "=", "cur_max_rouge", "\n", "", "else", ":", "\n", "            ", "break", "\n", "\n", "", "", "return", "selected", "\n", "", ""]], "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.script.calw2dTFIDF.GetType": [[8, 11], ["path.split", "filename.split"], "function", ["None"], ["def", "GetType", "(", "path", ")", ":", "\n", "    ", "filename", "=", "path", ".", "split", "(", "\"/\"", ")", "[", "-", "1", "]", "\n", "return", "filename", ".", "split", "(", "\".\"", ")", "[", "0", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.script.calw2dTFIDF.get_tfidf_embedding": [[12, 28], ["sklearn.feature_extraction.text.CountVectorizer", "sklearn.feature_extraction.text.CountVectorizer.fit_transform", "sklearn.feature_extraction.text.TfidfTransformer", "sklearn.feature_extraction.text.TfidfTransformer.fit_transform", "tfidf_transformer.fit_transform.toarray"], "function", ["None"], ["", "def", "get_tfidf_embedding", "(", "text", ")", ":", "\n", "    ", "\"\"\"\n\n    :param text: list, doc_number * word\n    :return: \n        vectorizer: \n            vocabulary_: word2id\n            get_feature_names(): id2word\n        tfidf: array [sent_number, max_word_number]\n    \"\"\"", "\n", "vectorizer", "=", "CountVectorizer", "(", "lowercase", "=", "True", ")", "\n", "word_count", "=", "vectorizer", ".", "fit_transform", "(", "text", ")", "\n", "tfidf_transformer", "=", "TfidfTransformer", "(", ")", "\n", "tfidf", "=", "tfidf_transformer", ".", "fit_transform", "(", "word_count", ")", "\n", "tfidf_weight", "=", "tfidf", ".", "toarray", "(", ")", "\n", "return", "vectorizer", ",", "tfidf_weight", "\n", "\n"]], "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.script.calw2dTFIDF.compress_array": [[30, 38], ["range", "len", "range", "len"], "function", ["None"], ["", "def", "compress_array", "(", "a", ",", "id2word", ")", ":", "\n", "    ", "d", "=", "{", "}", "\n", "for", "i", "in", "range", "(", "len", "(", "a", ")", ")", ":", "\n", "        ", "d", "[", "i", "]", "=", "{", "}", "\n", "for", "j", "in", "range", "(", "len", "(", "a", "[", "i", "]", ")", ")", ":", "\n", "            ", "if", "a", "[", "i", "]", "[", "j", "]", "!=", "0", ":", "\n", "                ", "d", "[", "i", "]", "[", "id2word", "[", "j", "]", "]", "=", "a", "[", "i", "]", "[", "j", "]", "\n", "", "", "", "return", "d", "\n", "\n"]], "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.script.calw2dTFIDF.main": [[40, 68], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "os.path.join", "os.path.join", "print", "open", "os.path.exists", "os.makedirs", "calw2dTFIDF.GetType", "open", "json.loads", "calw2dTFIDF.get_tfidf_embedding", "cntvector.vocabulary_.items", "calw2dTFIDF.compress_array", "open.write", "isinstance", "isinstance", "json.dumps"], "function", ["home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.script.calw2sTFIDF.GetType", "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.script.calw2sTFIDF.get_tfidf_embedding", "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.script.calw2sTFIDF.compress_array"], ["", "def", "main", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--data_path'", ",", "type", "=", "str", ",", "default", "=", "'data/CNNDM/train.label.jsonl'", ",", "help", "=", "'File to deal with'", ")", "\n", "parser", ".", "add_argument", "(", "'--dataset'", ",", "type", "=", "str", ",", "default", "=", "'CNNDM'", ",", "help", "=", "'dataset name'", ")", "\n", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "save_dir", "=", "os", ".", "path", ".", "join", "(", "\"cache\"", ",", "args", ".", "dataset", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "save_dir", ")", ":", "os", ".", "makedirs", "(", "save_dir", ")", "\n", "fname", "=", "GetType", "(", "args", ".", "data_path", ")", "+", "\".w2d.tfidf.jsonl\"", "\n", "saveFile", "=", "os", ".", "path", ".", "join", "(", "save_dir", ",", "fname", ")", "\n", "print", "(", "\"Save word2sent features of dataset %s to %s\"", "%", "(", "args", ".", "dataset", ",", "saveFile", ")", ")", "\n", "\n", "fout", "=", "open", "(", "saveFile", ",", "\"w\"", ")", "\n", "with", "open", "(", "args", ".", "data_path", ")", "as", "f", ":", "\n", "        ", "for", "line", "in", "f", ":", "\n", "            ", "e", "=", "json", ".", "loads", "(", "line", ")", "\n", "if", "isinstance", "(", "e", "[", "\"text\"", "]", ",", "list", ")", "and", "isinstance", "(", "e", "[", "\"text\"", "]", "[", "0", "]", ",", "list", ")", ":", "\n", "                ", "docs", "=", "[", "\" \"", ".", "join", "(", "doc", ")", "for", "doc", "in", "e", "[", "\"text\"", "]", "]", "\n", "", "else", ":", "\n", "                ", "docs", "=", "[", "e", "[", "\"text\"", "]", "]", "\n", "", "cntvector", ",", "tfidf_weight", "=", "get_tfidf_embedding", "(", "docs", ")", "\n", "id2word", "=", "{", "}", "\n", "for", "w", ",", "tfidf_id", "in", "cntvector", ".", "vocabulary_", ".", "items", "(", ")", ":", "\n", "                ", "id2word", "[", "tfidf_id", "]", "=", "w", "\n", "", "tfidfvector", "=", "compress_array", "(", "tfidf_weight", ",", "id2word", ")", "\n", "fout", ".", "write", "(", "json", ".", "dumps", "(", "tfidfvector", ")", "+", "\"\\n\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.script.calw2sTFIDF.GetType": [[8, 11], ["path.split", "filename.split"], "function", ["None"], ["def", "GetType", "(", "path", ")", ":", "\n", "    ", "filename", "=", "path", ".", "split", "(", "\"/\"", ")", "[", "-", "1", "]", "\n", "return", "filename", ".", "split", "(", "\".\"", ")", "[", "0", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.script.calw2sTFIDF.catDoc": [[12, 17], ["res.extend"], "function", ["None"], ["", "def", "catDoc", "(", "textlist", ")", ":", "\n", "    ", "res", "=", "[", "]", "\n", "for", "tlist", "in", "textlist", ":", "\n", "        ", "res", ".", "extend", "(", "tlist", ")", "\n", "", "return", "res", "\n", "\n"]], "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.script.calw2sTFIDF.get_tfidf_embedding": [[18, 34], ["sklearn.feature_extraction.text.CountVectorizer", "sklearn.feature_extraction.text.CountVectorizer.fit_transform", "sklearn.feature_extraction.text.TfidfTransformer", "sklearn.feature_extraction.text.TfidfTransformer.fit_transform", "tfidf_transformer.fit_transform.toarray"], "function", ["None"], ["", "def", "get_tfidf_embedding", "(", "text", ")", ":", "\n", "    ", "\"\"\"\n    \n    :param text: list, sent_number * word\n    :return: \n        vectorizer: \n            vocabulary_: word2id\n            get_feature_names(): id2word\n        tfidf: array [sent_number, max_word_number]\n    \"\"\"", "\n", "vectorizer", "=", "CountVectorizer", "(", "lowercase", "=", "True", ")", "\n", "word_count", "=", "vectorizer", ".", "fit_transform", "(", "text", ")", "\n", "tfidf_transformer", "=", "TfidfTransformer", "(", ")", "\n", "tfidf", "=", "tfidf_transformer", ".", "fit_transform", "(", "word_count", ")", "\n", "tfidf_weight", "=", "tfidf", ".", "toarray", "(", ")", "\n", "return", "vectorizer", ",", "tfidf_weight", "\n", "\n"]], "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.script.calw2sTFIDF.compress_array": [[35, 49], ["range", "len", "range", "len"], "function", ["None"], ["", "def", "compress_array", "(", "a", ",", "id2word", ")", ":", "\n", "    ", "\"\"\"\n    \n    :param a: matrix, [N, M], N is document number, M is word number\n    :param id2word: word id to word\n    :return: \n    \"\"\"", "\n", "d", "=", "{", "}", "\n", "for", "i", "in", "range", "(", "len", "(", "a", ")", ")", ":", "\n", "        ", "d", "[", "i", "]", "=", "{", "}", "\n", "for", "j", "in", "range", "(", "len", "(", "a", "[", "i", "]", ")", ")", ":", "\n", "            ", "if", "a", "[", "i", "]", "[", "j", "]", "!=", "0", ":", "\n", "                ", "d", "[", "i", "]", "[", "id2word", "[", "j", "]", "]", "=", "a", "[", "i", "]", "[", "j", "]", "\n", "", "", "", "return", "d", "\n", "\n"]], "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.script.calw2sTFIDF.main": [[51, 79], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "os.path.join", "os.path.join", "print", "open", "os.path.exists", "os.makedirs", "calw2sTFIDF.GetType", "open", "json.loads", "calw2sTFIDF.get_tfidf_embedding", "cntvector.vocabulary_.items", "calw2sTFIDF.compress_array", "open.write", "isinstance", "isinstance", "calw2sTFIDF.catDoc", "json.dumps"], "function", ["home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.script.calw2sTFIDF.GetType", "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.script.calw2sTFIDF.get_tfidf_embedding", "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.script.calw2sTFIDF.compress_array", "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.script.createVoc.catDoc"], ["", "def", "main", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--data_path'", ",", "type", "=", "str", ",", "default", "=", "'data/CNNDM/train.label.jsonl'", ",", "help", "=", "'File to deal with'", ")", "\n", "parser", ".", "add_argument", "(", "'--dataset'", ",", "type", "=", "str", ",", "default", "=", "'CNNDM'", ",", "help", "=", "'dataset name'", ")", "\n", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "save_dir", "=", "os", ".", "path", ".", "join", "(", "\"cache\"", ",", "args", ".", "dataset", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "save_dir", ")", ":", "os", ".", "makedirs", "(", "save_dir", ")", "\n", "fname", "=", "GetType", "(", "args", ".", "data_path", ")", "+", "\".w2s.tfidf.jsonl\"", "\n", "saveFile", "=", "os", ".", "path", ".", "join", "(", "save_dir", ",", "fname", ")", "\n", "print", "(", "\"Save word2sent features of dataset %s to %s\"", "%", "(", "args", ".", "dataset", ",", "saveFile", ")", ")", "\n", "\n", "fout", "=", "open", "(", "saveFile", ",", "\"w\"", ")", "\n", "with", "open", "(", "args", ".", "data_path", ")", "as", "f", ":", "\n", "        ", "for", "line", "in", "f", ":", "\n", "            ", "e", "=", "json", ".", "loads", "(", "line", ")", "\n", "if", "isinstance", "(", "e", "[", "\"text\"", "]", ",", "list", ")", "and", "isinstance", "(", "e", "[", "\"text\"", "]", "[", "0", "]", ",", "list", ")", ":", "\n", "                ", "sents", "=", "catDoc", "(", "e", "[", "\"text\"", "]", ")", "\n", "", "else", ":", "\n", "                ", "sents", "=", "e", "[", "\"text\"", "]", "\n", "", "cntvector", ",", "tfidf_weight", "=", "get_tfidf_embedding", "(", "sents", ")", "\n", "id2word", "=", "{", "}", "\n", "for", "w", ",", "tfidf_id", "in", "cntvector", ".", "vocabulary_", ".", "items", "(", ")", ":", "# word -> tfidf matrix row number", "\n", "                ", "id2word", "[", "tfidf_id", "]", "=", "w", "\n", "", "tfidfvector", "=", "compress_array", "(", "tfidf_weight", ",", "id2word", ")", "\n", "fout", ".", "write", "(", "json", ".", "dumps", "(", "tfidfvector", ")", "+", "\"\\n\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.script.lowTFIDFWords.catDoc": [[12, 17], ["res.extend"], "function", ["None"], ["def", "catDoc", "(", "textlist", ")", ":", "\n", "    ", "res", "=", "[", "]", "\n", "for", "tlist", "in", "textlist", ":", "\n", "        ", "res", ".", "extend", "(", "tlist", ")", "\n", "", "return", "res", "\n", "\n"]], "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.script.lowTFIDFWords.calTFidf": [[18, 24], ["sklearn.feature_extraction.text.CountVectorizer", "sklearn.feature_extraction.text.CountVectorizer.fit_transform", "sklearn.feature_extraction.text.TfidfTransformer", "sklearn.feature_extraction.text.TfidfTransformer.fit_transform"], "function", ["None"], ["", "def", "calTFidf", "(", "text", ")", ":", "\n", "    ", "vectorizer", "=", "CountVectorizer", "(", "lowercase", "=", "True", ")", "\n", "wordcount", "=", "vectorizer", ".", "fit_transform", "(", "text", ")", "\n", "tf_idf_transformer", "=", "TfidfTransformer", "(", ")", "\n", "tfidf_matrix", "=", "tf_idf_transformer", ".", "fit_transform", "(", "wordcount", ")", "\n", "return", "vectorizer", ",", "tfidf_matrix", "\n", "\n"]], "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.script.lowTFIDFWords.main": [[26, 64], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "os.path.join", "os.path.join", "print", "lowTFIDFWords.calTFidf", "print", "numpy.array", "numpy.argsort", "vectorizer.get_feature_names", "os.path.exists", "os.makedirs", "open", "tfidf_matrix.mean", "open", "json.loads", "documents.append", "isinstance", "isinstance", "lowTFIDFWords.catDoc", "len", "len", "fout.write"], "function", ["home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.script.lowTFIDFWords.calTFidf", "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.script.createVoc.catDoc"], ["", "def", "main", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--data_path'", ",", "type", "=", "str", ",", "default", "=", "'data/CNNDM/train.label.jsonl'", ",", "help", "=", "'File to deal with'", ")", "\n", "parser", ".", "add_argument", "(", "'--dataset'", ",", "type", "=", "str", ",", "default", "=", "'CNNDM'", ",", "help", "=", "'dataset name'", ")", "\n", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "save_dir", "=", "os", ".", "path", ".", "join", "(", "\"cache\"", ",", "args", ".", "dataset", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "save_dir", ")", ":", "os", ".", "makedirs", "(", "save_dir", ")", "\n", "saveFile", "=", "os", ".", "path", ".", "join", "(", "save_dir", ",", "\"filter_word.txt\"", ")", "\n", "print", "(", "\"Save low tfidf words in dataset %s to %s\"", "%", "(", "args", ".", "dataset", ",", "saveFile", ")", ")", "\n", "\n", "documents", "=", "[", "]", "\n", "with", "open", "(", "args", ".", "data_path", ",", "\"r\"", ",", "encoding", "=", "\"utf-8\"", ")", "as", "f", ":", "\n", "        ", "for", "line", "in", "f", ":", "\n", "            ", "e", "=", "json", ".", "loads", "(", "line", ")", "\n", "if", "isinstance", "(", "e", "[", "\"text\"", "]", ",", "list", ")", "and", "isinstance", "(", "e", "[", "\"text\"", "]", "[", "0", "]", ",", "list", ")", ":", "\n", "                ", "text", "=", "catDoc", "(", "e", "[", "\"text\"", "]", ")", "\n", "", "else", ":", "\n", "                ", "text", "=", "e", "[", "\"text\"", "]", "\n", "", "documents", ".", "append", "(", "\" \"", ".", "join", "(", "text", ")", ")", "\n", "\n", "", "", "vectorizer", ",", "tfidf_matrix", "=", "calTFidf", "(", "documents", ")", "\n", "print", "(", "\"The number of example is %d, and the TFIDF vocabulary size is %d\"", "%", "(", "len", "(", "documents", ")", ",", "len", "(", "vectorizer", ".", "vocabulary_", ")", ")", ")", "\n", "word_tfidf", "=", "np", ".", "array", "(", "tfidf_matrix", ".", "mean", "(", "0", ")", ")", "\n", "del", "tfidf_matrix", "\n", "word_order", "=", "np", ".", "argsort", "(", "word_tfidf", "[", "0", "]", ")", "\n", "\n", "id2word", "=", "vectorizer", ".", "get_feature_names", "(", ")", "\n", "with", "open", "(", "saveFile", ",", "\"w\"", ")", "as", "fout", ":", "\n", "        ", "for", "idx", "in", "word_order", ":", "\n", "            ", "w", "=", "id2word", "[", "idx", "]", "\n", "string", "=", "w", "+", "\"\\n\"", "\n", "try", ":", "\n", "                ", "fout", ".", "write", "(", "string", ")", "\n", "", "except", ":", "\n", "                ", "pass", "\n", "# print(string.encode(\"utf-8\"))", "\n"]], "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.script.createVoc.catDoc": [[8, 13], ["res.extend"], "function", ["None"], ["def", "catDoc", "(", "textlist", ")", ":", "\n", "    ", "res", "=", "[", "]", "\n", "for", "tlist", "in", "textlist", ":", "\n", "        ", "res", ".", "extend", "(", "tlist", ")", "\n", "", "return", "res", "\n", "\n"]], "home.repos.pwc.inspect_result.brxx122_HeterSUMGraph.script.createVoc.PrintInformation": [[14, 44], ["print", "len", "print", "len", "print", "len", "print"], "function", ["None"], ["", "def", "PrintInformation", "(", "keys", ",", "allcnt", ")", ":", "\n", "# Vocab > 10", "\n", "    ", "cnt", "=", "0", "\n", "first", "=", "0.0", "\n", "for", "key", ",", "val", "in", "keys", ":", "\n", "        ", "if", "val", ">=", "10", ":", "\n", "            ", "cnt", "+=", "1", "\n", "first", "+=", "val", "\n", "", "", "print", "(", "\"appearance > 10 cnt: %d, percent: %f\"", "%", "(", "cnt", ",", "first", "/", "allcnt", ")", ")", "# 416,303", "\n", "\n", "# first 30,000, last freq 31", "\n", "if", "len", "(", "keys", ")", ">", "30000", ":", "\n", "        ", "first", "=", "0.0", "\n", "for", "k", ",", "v", "in", "keys", "[", ":", "30000", "]", ":", "\n", "            ", "first", "+=", "v", "\n", "", "print", "(", "\"First 30,000 percent: %f, last freq %d\"", "%", "(", "first", "/", "allcnt", ",", "keys", "[", "30000", "]", "[", "1", "]", ")", ")", "\n", "\n", "# first 50,000, last freq 383", "\n", "", "if", "len", "(", "keys", ")", ">", "50000", ":", "\n", "        ", "first", "=", "0.0", "\n", "for", "k", ",", "v", "in", "keys", "[", ":", "50000", "]", ":", "\n", "            ", "first", "+=", "v", "\n", "", "print", "(", "\"First 50,000 percent: %f, last freq %d\"", "%", "(", "first", "/", "allcnt", ",", "keys", "[", "50000", "]", "[", "1", "]", ")", ")", "\n", "\n", "# first 100,000, last freq 107", "\n", "", "if", "len", "(", "keys", ")", ">", "100000", ":", "\n", "        ", "first", "=", "0.0", "\n", "for", "k", ",", "v", "in", "keys", "[", ":", "100000", "]", ":", "\n", "            ", "first", "+=", "v", "\n", "", "print", "(", "\"First 100,000 percent: %f, last freq %d\"", "%", "(", "first", "/", "allcnt", ",", "keys", "[", "100000", "]", "[", "1", "]", ")", ")", "\n", "\n"]]}