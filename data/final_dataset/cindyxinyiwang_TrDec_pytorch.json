{"home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.models.MlpAttn.__init__": [[12, 21], ["torch.nn.Module.__init__", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "models.MlpAttn.w_trg.cuda", "models.MlpAttn.w_att.cuda"], "methods", ["home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.utils.Logger.__init__"], ["  ", "def", "__init__", "(", "self", ",", "hparams", ")", ":", "\n", "    ", "super", "(", "MlpAttn", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "hparams", "=", "hparams", "\n", "#self.dropout = nn.Dropout(hparams.dropout)", "\n", "self", ".", "w_trg", "=", "nn", ".", "Linear", "(", "self", ".", "hparams", ".", "d_model", ",", "self", ".", "hparams", ".", "d_model", ")", "\n", "self", ".", "w_att", "=", "nn", ".", "Linear", "(", "self", ".", "hparams", ".", "d_model", ",", "1", ")", "\n", "if", "self", ".", "hparams", ".", "cuda", ":", "\n", "      ", "self", ".", "w_trg", "=", "self", ".", "w_trg", ".", "cuda", "(", ")", "\n", "self", ".", "w_att", "=", "self", ".", "w_att", ".", "cuda", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.models.MlpAttn.forward": [[22, 40], ["q.size", "k.size", "v.size", "torch.nn.functional.tanh", "torch.nn.functional.tanh", "torch.nn.functional.tanh", "models.MlpAttn.w_att().squeeze", "torch.softmax", "torch.softmax", "torch.softmax", "torch.bmm().squeeze", "torch.bmm().squeeze", "torch.bmm().squeeze", "torch.bmm().squeeze", "torch.bmm().squeeze", "torch.bmm().squeeze", "torch.bmm().squeeze", "torch.bmm().squeeze", "torch.bmm().squeeze", "torch.softmax.data.masked_fill_", "models.MlpAttn.w_trg().unsqueeze", "models.MlpAttn.w_att", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.softmax.unsqueeze", "models.MlpAttn.w_trg"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "q", ",", "k", ",", "v", ",", "attn_mask", "=", "None", ")", ":", "\n", "    ", "batch_size", ",", "d_q", "=", "q", ".", "size", "(", ")", "\n", "batch_size", ",", "len_k", ",", "d_k", "=", "k", ".", "size", "(", ")", "\n", "batch_size", ",", "len_v", ",", "d_v", "=", "v", ".", "size", "(", ")", "\n", "# v is bi-directional encoding of source", "\n", "assert", "d_k", "==", "d_q", "\n", "#assert 2*d_k == d_v", "\n", "assert", "len_k", "==", "len_v", "\n", "# (batch_size, len_k, d_k)", "\n", "att_src_hidden", "=", "nn", ".", "functional", ".", "tanh", "(", "k", "+", "self", ".", "w_trg", "(", "q", ")", ".", "unsqueeze", "(", "1", ")", ")", "\n", "# (batch_size, len_k)", "\n", "att_src_weights", "=", "self", ".", "w_att", "(", "att_src_hidden", ")", ".", "squeeze", "(", "2", ")", "\n", "if", "not", "attn_mask", "is", "None", ":", "\n", "      ", "att_src_weights", ".", "data", ".", "masked_fill_", "(", "attn_mask", ",", "-", "self", ".", "hparams", ".", "inf", ")", "\n", "", "att_src_weights", "=", "F", ".", "softmax", "(", "att_src_weights", ",", "dim", "=", "-", "1", ")", "\n", "#att_src_weights = self.dropout(att_src_weights)", "\n", "ctx", "=", "torch", ".", "bmm", "(", "att_src_weights", ".", "unsqueeze", "(", "1", ")", ",", "v", ")", ".", "squeeze", "(", "1", ")", "\n", "return", "ctx", "\n", "\n"]], "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.models.LayerNormalization.__init__": [[43, 50], ["torch.nn.Module.__init__", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros"], "methods", ["home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.utils.Logger.__init__"], ["  ", "def", "__init__", "(", "self", ",", "d_hid", ",", "eps", "=", "1e-9", ")", ":", "\n", "    ", "super", "(", "LayerNormalization", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "d_hid", "=", "d_hid", "\n", "self", ".", "eps", "=", "eps", "\n", "self", ".", "scale", "=", "nn", ".", "Parameter", "(", "torch", ".", "ones", "(", "self", ".", "d_hid", ")", ",", "requires_grad", "=", "True", ")", "\n", "self", ".", "offset", "=", "nn", ".", "Parameter", "(", "torch", ".", "zeros", "(", "self", ".", "d_hid", ")", ",", "requires_grad", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.models.LayerNormalization.forward": [[51, 56], ["x.mean", "x.std", "x.dim"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "    ", "assert", "x", ".", "dim", "(", ")", ">=", "2", "\n", "mean", "=", "x", ".", "mean", "(", "dim", "=", "-", "1", ",", "keepdim", "=", "True", ")", "\n", "std", "=", "x", ".", "std", "(", "dim", "=", "-", "1", ",", "keepdim", "=", "True", ")", "\n", "return", "self", ".", "scale", "*", "(", "x", "-", "mean", ")", "/", "(", "std", "+", "self", ".", "eps", ")", "+", "self", ".", "offset", "\n", "\n"]], "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.models.DotProdAttn.__init__": [[58, 65], ["torch.nn.Module.__init__", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Softmax", "torch.nn.Softmax", "torch.nn.Softmax", "numpy.power"], "methods", ["home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.utils.Logger.__init__"], ["  ", "def", "__init__", "(", "self", ",", "hparams", ")", ":", "\n", "    ", "super", "(", "DotProdAttn", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "hparams", ".", "dropout", ")", "\n", "#self.src_enc_linear = nn.Linear(hparams.d_model * 2, hparams.d_model)", "\n", "self", ".", "softmax", "=", "nn", ".", "Softmax", "(", "dim", "=", "-", "1", ")", "\n", "self", ".", "hparams", "=", "hparams", "\n", "self", ".", "temp", "=", "np", ".", "power", "(", "hparams", ".", "d_model", ",", "0.5", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.models.DotProdAttn.forward": [[66, 95], ["q.size", "k.size", "v.size", "models.DotProdAttn.softmax", "models.DotProdAttn.dropout", "torch.bmm().squeeze", "torch.bmm().squeeze", "torch.bmm().squeeze", "torch.bmm().squeeze", "torch.bmm().squeeze", "torch.bmm().squeeze", "torch.bmm().squeeze", "torch.bmm().squeeze", "torch.bmm().squeeze", "torch.bmm().squeeze", "torch.bmm().squeeze", "torch.bmm().squeeze", "torch.bmm().squeeze", "torch.bmm().squeeze", "torch.bmm().squeeze", "torch.bmm().squeeze", "torch.bmm().squeeze", "torch.bmm().squeeze", "models.DotProdAttn.data.masked_fill_", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "models.DotProdAttn.unsqueeze", "q.unsqueeze"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "q", ",", "k", ",", "v", ",", "attn_mask", "=", "None", ")", ":", "\n", "    ", "\"\"\" \n    dot prodct attention: (q * k.T) * v\n    Args:\n      q: [batch_size, d_q] (target state)\n      k: [batch_size, len_k, d_k] (source enc key vectors)\n      v: [batch_size, len_v, d_v] (source encoding vectors)\n      attn_mask: [batch_size, len_k] (source mask)\n    Return:\n      attn: [batch_size, d_v]\n    \"\"\"", "\n", "batch_size", ",", "d_q", "=", "q", ".", "size", "(", ")", "\n", "batch_size", ",", "len_k", ",", "d_k", "=", "k", ".", "size", "(", ")", "\n", "batch_size", ",", "len_v", ",", "d_v", "=", "v", ".", "size", "(", ")", "\n", "# v is bi-directional encoding of source", "\n", "assert", "d_k", "==", "d_q", "\n", "#assert 2*d_k == d_v", "\n", "assert", "len_k", "==", "len_v", "\n", "# [batch_size, len_k, d_model]", "\n", "#k_vec = self.src_enc_linear(k)", "\n", "# [batch_size, len_k]", "\n", "attn_weight", "=", "torch", ".", "bmm", "(", "k", ",", "q", ".", "unsqueeze", "(", "2", ")", ")", ".", "squeeze", "(", "2", ")", "/", "self", ".", "temp", "\n", "if", "not", "attn_mask", "is", "None", ":", "\n", "      ", "attn_weight", ".", "data", ".", "masked_fill_", "(", "attn_mask", ",", "-", "self", ".", "hparams", ".", "inf", ")", "\n", "", "attn_weight", "=", "self", ".", "softmax", "(", "attn_weight", ")", "\n", "attn_weight", "=", "self", ".", "dropout", "(", "attn_weight", ")", "\n", "# [batch_size, d_v]", "\n", "ctx", "=", "torch", ".", "bmm", "(", "attn_weight", ".", "unsqueeze", "(", "1", ")", ",", "v", ")", ".", "squeeze", "(", "1", ")", "\n", "return", "ctx", "\n", "\n"]], "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.models.MultiHeadAttn.__init__": [[97, 135], ["torch.nn.Module.__init__", "models.DotProdAttn", "models.LayerNormalization", "range", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "utils.init_param", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "utils.init_param", "utils.init_param", "utils.init_param", "Q.append", "K.append", "V.append", "models.MultiHeadAttn.Q.cuda", "models.MultiHeadAttn.K.cuda", "models.MultiHeadAttn.V.cuda", "models.MultiHeadAttn.w_proj.cuda"], "methods", ["home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.utils.Logger.__init__", "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.utils.init_param", "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.utils.init_param", "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.utils.init_param", "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.utils.init_param"], ["  ", "def", "__init__", "(", "self", ",", "hparams", ")", ":", "\n", "    ", "super", "(", "MultiHeadAttn", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "hparams", "=", "hparams", "\n", "\n", "self", ".", "attention", "=", "DotProdAttn", "(", "hparams", ")", "\n", "self", ".", "layer_norm", "=", "LayerNormalization", "(", "hparams", ".", "d_model", ")", "\n", "\n", "# projection of concatenated attn", "\n", "n_heads", "=", "self", ".", "hparams", ".", "n_heads", "\n", "d_model", "=", "self", ".", "hparams", ".", "d_model", "\n", "d_q", "=", "self", ".", "hparams", ".", "d_k", "\n", "d_k", "=", "self", ".", "hparams", ".", "d_k", "\n", "d_v", "=", "self", ".", "hparams", ".", "d_v", "\n", "\n", "Q", ",", "K", ",", "V", "=", "[", "]", ",", "[", "]", ",", "[", "]", "\n", "for", "head_id", "in", "range", "(", "n_heads", ")", ":", "\n", "      ", "q", "=", "nn", ".", "Linear", "(", "d_model", ",", "d_q", ",", "bias", "=", "False", ")", "\n", "k", "=", "nn", ".", "Linear", "(", "d_model", ",", "d_k", ",", "bias", "=", "False", ")", "\n", "v", "=", "nn", ".", "Linear", "(", "d_model", ",", "d_v", ",", "bias", "=", "False", ")", "\n", "init_param", "(", "q", ".", "weight", ",", "init_type", "=", "\"uniform\"", ",", "init_range", "=", "hparams", ".", "init_range", ")", "\n", "init_param", "(", "k", ".", "weight", ",", "init_type", "=", "\"uniform\"", ",", "init_range", "=", "hparams", ".", "init_range", ")", "\n", "init_param", "(", "v", ".", "weight", ",", "init_type", "=", "\"uniform\"", ",", "init_range", "=", "hparams", ".", "init_range", ")", "\n", "Q", ".", "append", "(", "q", ")", "\n", "K", ".", "append", "(", "k", ")", "\n", "V", ".", "append", "(", "v", ")", "\n", "", "self", ".", "Q", "=", "nn", ".", "ModuleList", "(", "Q", ")", "\n", "self", ".", "K", "=", "nn", ".", "ModuleList", "(", "K", ")", "\n", "self", ".", "V", "=", "nn", ".", "ModuleList", "(", "V", ")", "\n", "if", "self", ".", "hparams", ".", "cuda", ":", "\n", "      ", "self", ".", "Q", "=", "self", ".", "Q", ".", "cuda", "(", ")", "\n", "self", ".", "K", "=", "self", ".", "K", ".", "cuda", "(", ")", "\n", "self", ".", "V", "=", "self", ".", "V", ".", "cuda", "(", ")", "\n", "\n", "", "self", ".", "w_proj", "=", "nn", ".", "Linear", "(", "n_heads", "*", "d_v", ",", "d_model", ",", "bias", "=", "False", ")", "\n", "init_param", "(", "self", ".", "w_proj", ".", "weight", ",", "init_type", "=", "\"uniform\"", ",", "init_range", "=", "hparams", ".", "init_range", ")", "\n", "if", "self", ".", "hparams", ".", "cuda", ":", "\n", "      ", "self", ".", "w_proj", "=", "self", ".", "w_proj", ".", "cuda", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.models.MultiHeadAttn.forward": [[136, 174], ["q.size", "zip", "torch.cat().contiguous().view", "torch.cat().contiguous().view", "torch.cat().contiguous().view", "torch.cat().contiguous().view", "torch.cat().contiguous().view", "torch.cat().contiguous().view", "torch.cat().contiguous().view", "torch.cat().contiguous().view", "torch.cat().contiguous().view", "models.MultiHeadAttn.w_proj", "models.MultiHeadAttn.attention", "heads.append", "models.MultiHeadAttn.layer_norm", "Q", "K", "V", "torch.cat().contiguous", "torch.cat().contiguous", "torch.cat().contiguous", "torch.cat().contiguous", "torch.cat().contiguous", "torch.cat().contiguous", "torch.cat().contiguous", "torch.cat().contiguous", "torch.cat().contiguous", "hasattr", "hasattr", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "q", ",", "k", ",", "v", ",", "attn_mask", "=", "None", ")", ":", "\n", "    ", "\"\"\"Performs the following computations:\n         head[i] = Attention(q * w_q[i], k * w_k[i], v * w_v[i])\n         outputs = concat(all head[i]) * self.w_proj\n    Args:\n      q: [batch_size, len_q, d_q].\n      k: [batch_size, len_k, d_k].\n      v: [batch_size, len_v, d_v].\n    Must have: len_k == len_v\n    Note: This batch_size is in general NOT the training batch_size, as\n      both sentences and time steps are batched together for efficiency.\n    Returns:\n      outputs: [batch_size, len_q, d_model].\n    \"\"\"", "\n", "\n", "residual", "=", "q", "\n", "\n", "n_heads", "=", "self", ".", "hparams", ".", "n_heads", "\n", "d_model", "=", "self", ".", "hparams", ".", "d_model", "\n", "d_q", "=", "self", ".", "hparams", ".", "d_k", "\n", "d_k", "=", "self", ".", "hparams", ".", "d_k", "\n", "d_v", "=", "self", ".", "hparams", ".", "d_v", "\n", "batch_size", "=", "q", ".", "size", "(", "0", ")", "\n", "\n", "heads", "=", "[", "]", "\n", "for", "Q", ",", "K", ",", "V", "in", "zip", "(", "self", ".", "Q", ",", "self", ".", "K", ",", "self", ".", "V", ")", ":", "\n", "      ", "head_q", ",", "head_k", ",", "head_v", "=", "Q", "(", "q", ")", ",", "K", "(", "k", ")", ",", "V", "(", "v", ")", "\n", "head", "=", "self", ".", "attention", "(", "head_q", ",", "head_k", ",", "head_v", ",", "attn_mask", "=", "attn_mask", ")", "\n", "heads", ".", "append", "(", "head", ")", "\n", "\n", "", "outputs", "=", "torch", ".", "cat", "(", "heads", ",", "dim", "=", "-", "1", ")", ".", "contiguous", "(", ")", ".", "view", "(", "batch_size", ",", "n_heads", "*", "d_v", ")", "\n", "outputs", "=", "self", ".", "w_proj", "(", "outputs", ")", "\n", "if", "not", "hasattr", "(", "self", ".", "hparams", ",", "\"residue\"", ")", "or", "self", ".", "hparams", ".", "residue", "==", "1", ":", "\n", "      ", "outputs", "=", "outputs", "+", "residual", "\n", "", "if", "not", "hasattr", "(", "self", ".", "hparams", ",", "\"layer_norm\"", ")", "or", "self", ".", "hparams", ".", "layer_norm", "==", "1", ":", "\n", "      ", "outputs", "=", "self", ".", "layer_norm", "(", "outputs", ")", "\n", "\n", "", "return", "outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.models.Encoder.__init__": [[176, 199], ["torch.nn.Module.__init__", "torch.nn.Embedding", "torch.nn.Embedding", "torch.nn.Embedding", "torch.nn.LSTM", "torch.nn.LSTM", "torch.nn.LSTM", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "models.Encoder.word_emb.cuda", "models.Encoder.layer.cuda", "models.Encoder.dropout.cuda", "models.Encoder.bridge.cuda"], "methods", ["home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.utils.Logger.__init__"], ["  ", "def", "__init__", "(", "self", ",", "hparams", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "    ", "super", "(", "Encoder", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "hparams", "=", "hparams", "\n", "#print(\"d_word_vec\", self.hparams.d_word_vec)", "\n", "self", ".", "word_emb", "=", "nn", ".", "Embedding", "(", "self", ".", "hparams", ".", "source_vocab_size", ",", "\n", "self", ".", "hparams", ".", "d_word_vec", ",", "\n", "padding_idx", "=", "hparams", ".", "pad_id", ")", "\n", "\n", "self", ".", "layer", "=", "nn", ".", "LSTM", "(", "self", ".", "hparams", ".", "d_word_vec", ",", "\n", "self", ".", "hparams", ".", "d_model", ",", "\n", "bidirectional", "=", "True", ",", "\n", "dropout", "=", "hparams", ".", "dropout", ")", "\n", "\n", "# bridge from encoder state to decoder init state", "\n", "self", ".", "bridge", "=", "nn", ".", "Linear", "(", "hparams", ".", "d_model", "*", "2", ",", "hparams", ".", "d_model", ",", "bias", "=", "False", ")", "\n", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "self", ".", "hparams", ".", "dropout", ")", "\n", "if", "self", ".", "hparams", ".", "cuda", ":", "\n", "      ", "self", ".", "word_emb", "=", "self", ".", "word_emb", ".", "cuda", "(", ")", "\n", "self", ".", "layer", "=", "self", ".", "layer", ".", "cuda", "(", ")", "\n", "self", ".", "dropout", "=", "self", ".", "dropout", ".", "cuda", "(", ")", "\n", "self", ".", "bridge", "=", "self", ".", "bridge", ".", "cuda", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.models.Encoder.forward": [[200, 228], ["x_train.transpose.transpose.size", "x_train.transpose.transpose.transpose", "models.Encoder.word_emb", "models.Encoder.dropout", "torch.nn.utils.rnn.pack_padded_sequence", "torch.nn.utils.rnn.pack_padded_sequence", "torch.nn.utils.rnn.pack_padded_sequence", "models.Encoder.layer", "torch.nn.utils.rnn.pad_packed_sequence", "torch.nn.utils.rnn.pad_packed_sequence", "torch.nn.utils.rnn.pad_packed_sequence", "enc_output.permute.permute.permute", "models.Encoder.bridge", "torch.tanh", "torch.tanh", "torch.tanh", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "x_train", ",", "x_len", ")", ":", "\n", "    ", "\"\"\"Performs a forward pass.\n\n    Args:\n      x_train: Torch Tensor of size [batch_size, max_len]\n      x_mask: Torch Tensor of size [batch_size, max_len]. 1 means to ignore a\n        position.\n      x_len: [batch_size,]\n\n    Returns:\n      enc_output: Tensor of size [batch_size, max_len, d_model].\n    \"\"\"", "\n", "batch_size", ",", "max_len", "=", "x_train", ".", "size", "(", ")", "\n", "#print(\"x_train\", x_train)", "\n", "#print(\"x_len\", x_len)", "\n", "x_train", "=", "x_train", ".", "transpose", "(", "0", ",", "1", ")", "\n", "# [batch_size, max_len, d_word_vec]", "\n", "word_emb", "=", "self", ".", "word_emb", "(", "x_train", ")", "\n", "word_emb", "=", "self", ".", "dropout", "(", "word_emb", ")", "\n", "packed_word_emb", "=", "pack_padded_sequence", "(", "word_emb", ",", "x_len", ")", "\n", "enc_output", ",", "(", "ht", ",", "ct", ")", "=", "self", ".", "layer", "(", "packed_word_emb", ")", "\n", "enc_output", ",", "_", "=", "pad_packed_sequence", "(", "enc_output", ",", "padding_value", "=", "self", ".", "hparams", ".", "pad_id", ")", "\n", "enc_output", "=", "enc_output", ".", "permute", "(", "1", ",", "0", ",", "2", ")", "\n", "\n", "dec_init_cell", "=", "self", ".", "bridge", "(", "torch", ".", "cat", "(", "[", "ct", "[", "0", "]", ",", "ct", "[", "1", "]", "]", ",", "1", ")", ")", "\n", "dec_init_state", "=", "F", ".", "tanh", "(", "dec_init_cell", ")", "\n", "dec_init", "=", "(", "dec_init_state", ",", "dec_init_cell", ")", "\n", "return", "enc_output", ",", "dec_init", "\n", "\n"]], "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.models.Decoder.__init__": [[230, 252], ["torch.nn.Module.__init__", "models.MlpAttn", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Embedding", "torch.nn.Embedding", "torch.nn.Embedding", "torch.nn.LSTMCell", "torch.nn.LSTMCell", "torch.nn.LSTMCell", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "models.Decoder.ctx_to_readout.cuda", "models.Decoder.readout.cuda", "models.Decoder.word_emb.cuda", "models.Decoder.layer.cuda", "models.Decoder.dropout.cuda"], "methods", ["home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.utils.Logger.__init__"], ["  ", "def", "__init__", "(", "self", ",", "hparams", ")", ":", "\n", "    ", "super", "(", "Decoder", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "hparams", "=", "hparams", "\n", "\n", "#self.attention = DotProdAttn(hparams)", "\n", "self", ".", "attention", "=", "MlpAttn", "(", "hparams", ")", "\n", "# transform [ctx, h_t] to readout state vectors before softmax", "\n", "self", ".", "ctx_to_readout", "=", "nn", ".", "Linear", "(", "hparams", ".", "d_model", "*", "2", "+", "hparams", ".", "d_model", ",", "hparams", ".", "d_model", ",", "bias", "=", "False", ")", "\n", "self", ".", "readout", "=", "nn", ".", "Linear", "(", "hparams", ".", "d_model", ",", "hparams", ".", "target_vocab_size", ",", "bias", "=", "False", ")", "\n", "self", ".", "word_emb", "=", "nn", ".", "Embedding", "(", "self", ".", "hparams", ".", "target_vocab_size", ",", "\n", "self", ".", "hparams", ".", "d_word_vec", ",", "\n", "padding_idx", "=", "hparams", ".", "pad_id", ")", "\n", "# input: [y_t-1, input_feed]", "\n", "self", ".", "layer", "=", "nn", ".", "LSTMCell", "(", "hparams", ".", "d_word_vec", "+", "hparams", ".", "d_model", "*", "2", ",", "\n", "hparams", ".", "d_model", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "hparams", ".", "dropout", ")", "\n", "if", "self", ".", "hparams", ".", "cuda", ":", "\n", "      ", "self", ".", "ctx_to_readout", "=", "self", ".", "ctx_to_readout", ".", "cuda", "(", ")", "\n", "self", ".", "readout", "=", "self", ".", "readout", ".", "cuda", "(", ")", "\n", "self", ".", "word_emb", "=", "self", ".", "word_emb", ".", "cuda", "(", ")", "\n", "self", ".", "layer", "=", "self", ".", "layer", ".", "cuda", "(", ")", "\n", "self", ".", "dropout", "=", "self", ".", "dropout", ".", "cuda", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.models.Decoder.forward": [[253, 286], ["y_train.size", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "models.Decoder.word_emb", "range", "models.Decoder.readout().transpose().contiguous", "x_enc.size", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "input_feed.cuda.cuda.cuda", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "models.Decoder.layer", "models.Decoder.attention", "torch.tanh", "torch.tanh", "torch.tanh", "models.Decoder.dropout", "pre_readouts.append", "models.Decoder.ctx_to_readout", "models.Decoder.readout().transpose", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "models.Decoder.readout", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "x_enc", ",", "x_enc_k", ",", "dec_init", ",", "x_mask", ",", "y_train", ",", "y_mask", ")", ":", "\n", "# get decoder init state and cell, use x_ct", "\n", "    ", "\"\"\"\n    x_enc: [batch_size, max_x_len, d_model * 2]\n\n    \"\"\"", "\n", "batch_size_x", "=", "x_enc", ".", "size", "(", ")", "[", "0", "]", "\n", "batch_size", ",", "y_max_len", "=", "y_train", ".", "size", "(", ")", "\n", "assert", "batch_size_x", "==", "batch_size", "\n", "hidden", "=", "dec_init", "\n", "input_feed", "=", "Variable", "(", "torch", ".", "zeros", "(", "batch_size", ",", "self", ".", "hparams", ".", "d_model", "*", "2", ")", ",", "requires_grad", "=", "False", ")", "\n", "#input_feed = Variable(dec_init[1][1].data.new(batch_size, self.hparams.d_model).zero_(), requires_grad=False)", "\n", "if", "self", ".", "hparams", ".", "cuda", ":", "\n", "      ", "input_feed", "=", "input_feed", ".", "cuda", "(", ")", "\n", "# [batch_size, y_len, d_word_vec]", "\n", "", "trg_emb", "=", "self", ".", "word_emb", "(", "y_train", ")", "\n", "pre_readouts", "=", "[", "]", "\n", "logits", "=", "[", "]", "\n", "for", "t", "in", "range", "(", "y_max_len", ")", ":", "\n", "      ", "y_emb_tm1", "=", "trg_emb", "[", ":", ",", "t", ",", ":", "]", "\n", "y_input", "=", "torch", ".", "cat", "(", "[", "y_emb_tm1", ",", "input_feed", "]", ",", "dim", "=", "1", ")", "\n", "\n", "h_t", ",", "c_t", "=", "self", ".", "layer", "(", "y_input", ",", "hidden", ")", "\n", "ctx", "=", "self", ".", "attention", "(", "h_t", ",", "x_enc_k", ",", "x_enc", ",", "attn_mask", "=", "x_mask", ")", "\n", "pre_readout", "=", "F", ".", "tanh", "(", "self", ".", "ctx_to_readout", "(", "torch", ".", "cat", "(", "[", "h_t", ",", "ctx", "]", ",", "dim", "=", "1", ")", ")", ")", "\n", "pre_readout", "=", "self", ".", "dropout", "(", "pre_readout", ")", "\n", "pre_readouts", ".", "append", "(", "pre_readout", ")", "\n", "\n", "input_feed", "=", "ctx", "\n", "hidden", "=", "(", "h_t", ",", "c_t", ")", "\n", "# [len_y, batch_size, trg_vocab_size]", "\n", "", "logits", "=", "self", ".", "readout", "(", "torch", ".", "stack", "(", "pre_readouts", ")", ")", ".", "transpose", "(", "0", ",", "1", ")", ".", "contiguous", "(", ")", "\n", "return", "logits", "\n", "\n"]], "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.models.Decoder.step": [[287, 299], ["models.Decoder.word_emb", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "models.Decoder.layer", "models.Decoder.attention", "torch.tanh", "torch.tanh", "torch.tanh", "models.Decoder.readout", "models.Decoder.ctx_to_readout", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat"], "methods", ["None"], ["", "def", "step", "(", "self", ",", "x_enc", ",", "x_enc_k", ",", "y_tm1", ",", "dec_state", ",", "ctx_t", ")", ":", "\n", "    ", "y_emb_tm1", "=", "self", ".", "word_emb", "(", "y_tm1", ")", "\n", "y_input", "=", "torch", ".", "cat", "(", "[", "y_emb_tm1", ",", "ctx_t", "]", ",", "dim", "=", "1", ")", "\n", "#print (y_input.size())", "\n", "#print (dec_state[0].size())", "\n", "#print (dec_state[1].size())", "\n", "h_t", ",", "c_t", "=", "self", ".", "layer", "(", "y_input", ",", "dec_state", ")", "\n", "ctx", "=", "self", ".", "attention", "(", "h_t", ",", "x_enc_k", ",", "x_enc", ")", "\n", "pre_readout", "=", "F", ".", "tanh", "(", "self", ".", "ctx_to_readout", "(", "torch", ".", "cat", "(", "[", "h_t", ",", "ctx", "]", ",", "dim", "=", "1", ")", ")", ")", "\n", "logits", "=", "self", ".", "readout", "(", "pre_readout", ")", "\n", "\n", "return", "logits", ",", "(", "h_t", ",", "c_t", ")", ",", "ctx", "\n", "\n"]], "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.models.Seq2Seq.__init__": [[302, 311], ["torch.nn.Module.__init__", "models.Encoder", "models.Decoder", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "models.Seq2Seq.enc_to_k.cuda"], "methods", ["home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.utils.Logger.__init__"], ["  ", "def", "__init__", "(", "self", ",", "hparams", ")", ":", "\n", "    ", "super", "(", "Seq2Seq", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "encoder", "=", "Encoder", "(", "hparams", ")", "\n", "self", ".", "decoder", "=", "Decoder", "(", "hparams", ")", "\n", "# transform encoder state vectors into attention key vector", "\n", "self", ".", "enc_to_k", "=", "nn", ".", "Linear", "(", "hparams", ".", "d_model", "*", "2", ",", "hparams", ".", "d_model", ",", "bias", "=", "False", ")", "\n", "self", ".", "hparams", "=", "hparams", "\n", "if", "self", ".", "hparams", ".", "cuda", ":", "\n", "      ", "self", ".", "enc_to_k", "=", "self", ".", "enc_to_k", ".", "cuda", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.models.Seq2Seq.forward": [[312, 322], ["models.Seq2Seq.encoder", "models.Seq2Seq.enc_to_k", "models.Seq2Seq.decoder"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "x_train", ",", "x_mask", ",", "x_len", ",", "y_train", ",", "y_mask", ",", "y_len", ")", ":", "\n", "# [batch_size, x_len, d_model * 2]", "\n", "#print(\"x_train\", x_train)", "\n", "#print(\"x_mask\", x_mask)", "\n", "#print(\"x_len\", x_len)", "\n", "    ", "x_enc", ",", "dec_init", "=", "self", ".", "encoder", "(", "x_train", ",", "x_len", ")", "\n", "x_enc_k", "=", "self", ".", "enc_to_k", "(", "x_enc", ")", "\n", "# [batch_size, y_len-1, trg_vocab_size]", "\n", "logits", "=", "self", ".", "decoder", "(", "x_enc", ",", "x_enc_k", ",", "dec_init", ",", "x_mask", ",", "y_train", ",", "y_mask", ")", "\n", "return", "logits", "\n", "\n"]], "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.models.Seq2Seq.translate": [[323, 332], ["torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "hyps.append", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "x.cuda.cuda.cuda", "models.Seq2Seq.translate_sent"], "methods", ["home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.trdec_single.TrDecSingle.translate_sent"], ["", "def", "translate", "(", "self", ",", "x_train", ",", "max_len", "=", "100", ",", "beam_size", "=", "5", ",", "poly_norm_m", "=", "0", ")", ":", "\n", "    ", "hyps", "=", "[", "]", "\n", "for", "x", "in", "x_train", ":", "\n", "      ", "x", "=", "Variable", "(", "torch", ".", "LongTensor", "(", "x", ")", ",", "volatile", "=", "True", ")", "\n", "if", "self", ".", "hparams", ".", "cuda", ":", "\n", "        ", "x", "=", "x", ".", "cuda", "(", ")", "\n", "", "hyp", "=", "self", ".", "translate_sent", "(", "x", ",", "max_len", "=", "max_len", ",", "beam_size", "=", "beam_size", ",", "poly_norm_m", "=", "poly_norm_m", ")", "[", "0", "]", "\n", "hyps", ".", "append", "(", "hyp", ".", "y", "[", "1", ":", "-", "1", "]", ")", "\n", "", "return", "hyps", "\n", "\n"]], "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.models.Seq2Seq.translate_sent": [[333, 399], ["x_train.unsqueeze.unsqueeze.unsqueeze", "models.Seq2Seq.encoder", "models.Seq2Seq.enc_to_k", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "sorted", "len", "x_train.unsqueeze.unsqueeze.size", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "input_feed.cuda.cuda.cuda", "models.Hyp", "enumerate", "numpy.concatenate().flatten", "zip", "len", "completed_hyp.append", "x_train.unsqueeze.unsqueeze.size", "len", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "models.Seq2Seq.decoder.step", "new_hyp_score_list.append", "len", "models.Hyp", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "y_tm1.cuda.cuda.cuda", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "numpy.concatenate", "completed_hyp.append", "new_hypotheses.append", "pow", "int", "int", "pow"], "methods", ["home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.trdec_single.TreeDecoderSingle.step"], ["", "def", "translate_sent", "(", "self", ",", "x_train", ",", "max_len", "=", "100", ",", "beam_size", "=", "5", ",", "poly_norm_m", "=", "0", ")", ":", "\n", "    ", "assert", "len", "(", "x_train", ".", "size", "(", ")", ")", "==", "1", "\n", "x_len", "=", "[", "x_train", ".", "size", "(", "0", ")", "]", "\n", "x_train", "=", "x_train", ".", "unsqueeze", "(", "0", ")", "\n", "x_enc", ",", "dec_init", "=", "self", ".", "encoder", "(", "x_train", ",", "x_len", ")", "\n", "x_enc_k", "=", "self", ".", "enc_to_k", "(", "x_enc", ")", "\n", "length", "=", "0", "\n", "completed_hyp", "=", "[", "]", "\n", "input_feed", "=", "Variable", "(", "torch", ".", "zeros", "(", "1", ",", "self", ".", "hparams", ".", "d_model", "*", "2", ")", ",", "requires_grad", "=", "False", ")", "\n", "if", "self", ".", "hparams", ".", "cuda", ":", "\n", "      ", "input_feed", "=", "input_feed", ".", "cuda", "(", ")", "\n", "", "active_hyp", "=", "[", "Hyp", "(", "state", "=", "dec_init", ",", "y", "=", "[", "self", ".", "hparams", ".", "bos_id", "]", ",", "ctx_tm1", "=", "input_feed", ",", "score", "=", "0.", ")", "]", "\n", "while", "len", "(", "completed_hyp", ")", "<", "beam_size", "and", "length", "<", "max_len", ":", "\n", "      ", "length", "+=", "1", "\n", "new_hyp_score_list", "=", "[", "]", "\n", "#hyp_num = len(active_hyp)", "\n", "#cur_x_enc = x_enc.repeat(hyp_num, 1, 1)", "\n", "#cur_x_enc_k = x_enc_k.repeat(hyp_num, 1, 1)", "\n", "#y_tm1 = Variable(torch.LongTensor([hyp.y[-1] for hyp in active_hyp]), volatile=True)", "\n", "#if self.hparams.cuda:", "\n", "#  y_tm1 = y_tm1.cuda()", "\n", "#logits = self.decoder.step(cur_x_enc, cur_x_enc_k, y_tm1, )", "\n", "for", "i", ",", "hyp", "in", "enumerate", "(", "active_hyp", ")", ":", "\n", "        ", "y_tm1", "=", "Variable", "(", "torch", ".", "LongTensor", "(", "[", "int", "(", "hyp", ".", "y", "[", "-", "1", "]", ")", "]", ")", ",", "volatile", "=", "True", ")", "\n", "if", "self", ".", "hparams", ".", "cuda", ":", "\n", "          ", "y_tm1", "=", "y_tm1", ".", "cuda", "(", ")", "\n", "", "logits", ",", "dec_state", ",", "ctx", "=", "self", ".", "decoder", ".", "step", "(", "x_enc", ",", "x_enc_k", ",", "y_tm1", ",", "hyp", ".", "state", ",", "hyp", ".", "ctx_tm1", ")", "\n", "hyp", ".", "state", "=", "dec_state", "\n", "hyp", ".", "ctx_tm1", "=", "ctx", "\n", "\n", "p_t", "=", "F", ".", "log_softmax", "(", "logits", ",", "-", "1", ")", ".", "data", "\n", "if", "poly_norm_m", ">", "0", "and", "length", ">", "1", ":", "\n", "          ", "new_hyp_scores", "=", "(", "hyp", ".", "score", "*", "pow", "(", "length", "-", "1", ",", "poly_norm_m", ")", "+", "p_t", ")", "/", "pow", "(", "length", ",", "poly_norm_m", ")", "\n", "", "else", ":", "\n", "          ", "new_hyp_scores", "=", "hyp", ".", "score", "+", "p_t", "\n", "#print(new_hyp_scores)", "\n", "#print(p_t)", "\n", "", "new_hyp_score_list", ".", "append", "(", "new_hyp_scores", ")", "\n", "#print(hyp.y)", "\n", "#print(dec_state)", "\n", "#if len(active_hyp) > i+1:", "\n", "#  print(active_hyp[i+1].state)", "\n", "#print()", "\n", "#exit(0)", "\n", "", "live_hyp_num", "=", "beam_size", "-", "len", "(", "completed_hyp", ")", "\n", "new_hyp_scores", "=", "np", ".", "concatenate", "(", "new_hyp_score_list", ")", ".", "flatten", "(", ")", "\n", "new_hyp_pos", "=", "(", "-", "new_hyp_scores", ")", ".", "argsort", "(", ")", "[", ":", "live_hyp_num", "]", "\n", "prev_hyp_ids", "=", "new_hyp_pos", "/", "self", ".", "hparams", ".", "target_vocab_size", "\n", "word_ids", "=", "new_hyp_pos", "%", "self", ".", "hparams", ".", "target_vocab_size", "\n", "new_hyp_scores", "=", "new_hyp_scores", "[", "new_hyp_pos", "]", "\n", "\n", "new_hypotheses", "=", "[", "]", "\n", "for", "prev_hyp_id", ",", "word_id", ",", "hyp_score", "in", "zip", "(", "prev_hyp_ids", ",", "word_ids", ",", "new_hyp_scores", ")", ":", "\n", "        ", "prev_hyp", "=", "active_hyp", "[", "int", "(", "prev_hyp_id", ")", "]", "\n", "hyp", "=", "Hyp", "(", "state", "=", "prev_hyp", ".", "state", ",", "y", "=", "prev_hyp", ".", "y", "+", "[", "word_id", "]", ",", "ctx_tm1", "=", "prev_hyp", ".", "ctx_tm1", ",", "score", "=", "hyp_score", ")", "\n", "if", "word_id", "==", "self", ".", "hparams", ".", "eos_id", ":", "\n", "          ", "completed_hyp", ".", "append", "(", "hyp", ")", "\n", "", "else", ":", "\n", "          ", "new_hypotheses", ".", "append", "(", "hyp", ")", "\n", "#print(word_id, hyp_score)", "\n", "#exit(0)", "\n", "", "", "active_hyp", "=", "new_hypotheses", "\n", "\n", "", "if", "len", "(", "completed_hyp", ")", "==", "0", ":", "\n", "      ", "completed_hyp", ".", "append", "(", "active_hyp", "[", "0", "]", ")", "\n", "", "return", "sorted", "(", "completed_hyp", ",", "key", "=", "lambda", "x", ":", "x", ".", "score", ",", "reverse", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.models.Hyp.__init__": [[401, 406], ["None"], "methods", ["None"], ["  ", "def", "__init__", "(", "self", ",", "state", ",", "y", ",", "ctx_tm1", ",", "score", ")", ":", "\n", "    ", "self", ".", "state", "=", "state", "\n", "self", ".", "y", "=", "y", "\n", "self", ".", "ctx_tm1", "=", "ctx_tm1", "\n", "self", ".", "score", "=", "score", "\n", "", "", ""]], "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.trdec_attn_v1.TreeDecoderAttn_v1.__init__": [[14, 67], ["torch.nn.Module.__init__", "torch.nn.Embedding", "torch.nn.Embedding", "torch.nn.Embedding", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.LSTMCell", "torch.nn.LSTMCell", "torch.nn.LSTMCell", "torch.nn.LSTMCell", "torch.nn.LSTMCell", "torch.nn.LSTMCell", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros.index_fill_", "torch.zeros.index_fill_", "torch.zeros.index_fill_", "models.MlpAttn", "models.MlpAttn", "models.MlpAttn", "models.MlpAttn", "models.DotProdAttn", "models.DotProdAttn", "models.DotProdAttn", "models.DotProdAttn", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "trdec_attn_v1.TreeDecoderAttn_v1.rule_vocab_mask.cuda", "trdec_attn_v1.TreeDecoderAttn_v1.word_vocab_mask.cuda", "trdec_attn_v1.TreeDecoderAttn_v1.emb.cuda", "trdec_attn_v1.TreeDecoderAttn_v1.rule_attention.cuda", "trdec_attn_v1.TreeDecoderAttn_v1.word_attention.cuda", "trdec_attn_v1.TreeDecoderAttn_v1.rule_to_word_attn.cuda", "trdec_attn_v1.TreeDecoderAttn_v1.word_to_rule_attn.cuda", "trdec_attn_v1.TreeDecoderAttn_v1.rule_ctx_to_readout.cuda", "trdec_attn_v1.TreeDecoderAttn_v1.word_ctx_to_readout.cuda", "trdec_attn_v1.TreeDecoderAttn_v1.readout.cuda", "trdec_attn_v1.TreeDecoderAttn_v1.rule_lstm_cell.cuda", "trdec_attn_v1.TreeDecoderAttn_v1.word_lstm_cell.cuda", "trdec_attn_v1.TreeDecoderAttn_v1.dropout.cuda", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange"], "methods", ["home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.utils.Logger.__init__"], ["  ", "def", "__init__", "(", "self", ",", "hparams", ")", ":", "\n", "    ", "super", "(", "TreeDecoderAttn_v1", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "hparams", "=", "hparams", "\n", "self", ".", "target_vocab_size", "=", "self", ".", "hparams", ".", "target_word_vocab_size", "+", "self", ".", "hparams", ".", "target_rule_vocab_size", "\n", "self", ".", "emb", "=", "nn", ".", "Embedding", "(", "self", ".", "target_vocab_size", ",", "\n", "self", ".", "hparams", ".", "d_word_vec", ",", "\n", "padding_idx", "=", "hparams", ".", "pad_id", ")", "\n", "if", "self", ".", "hparams", ".", "attn", "==", "\"mlp\"", ":", "\n", "      ", "self", ".", "rule_attention", "=", "MlpAttn", "(", "hparams", ")", "\n", "self", ".", "word_attention", "=", "MlpAttn", "(", "hparams", ")", "\n", "self", ".", "rule_to_word_attn", "=", "MlpAttn", "(", "hparams", ")", "\n", "self", ".", "word_to_rule_attn", "=", "MlpAttn", "(", "hparams", ")", "\n", "", "else", ":", "\n", "      ", "self", ".", "rule_attention", "=", "DotProdAttn", "(", "hparams", ")", "\n", "self", ".", "word_attention", "=", "DotProdAttn", "(", "hparams", ")", "\n", "self", ".", "rule_to_word_attn", "=", "DotProdAttn", "(", "hparams", ")", "\n", "self", ".", "word_to_rule_attn", "=", "DotProdAttn", "(", "hparams", ")", "\n", "# transform [word_ctx, word_h_t, rule_ctx, rule_h_t] to readout state vectors before softmax", "\n", "", "self", ".", "rule_ctx_to_readout", "=", "nn", ".", "Linear", "(", "hparams", ".", "d_model", "*", "6", ",", "hparams", ".", "d_model", ",", "bias", "=", "False", ")", "\n", "self", ".", "word_ctx_to_readout", "=", "nn", ".", "Linear", "(", "hparams", ".", "d_model", "*", "6", ",", "hparams", ".", "d_model", ",", "bias", "=", "False", ")", "\n", "\n", "self", ".", "readout", "=", "nn", ".", "Linear", "(", "hparams", ".", "d_model", ",", "\n", "self", ".", "target_vocab_size", ",", "\n", "bias", "=", "False", ")", "\n", "if", "self", ".", "hparams", ".", "share_emb_softmax", ":", "\n", "      ", "self", ".", "emb", ".", "weight", "=", "self", ".", "readout", ".", "weight", "\n", "# input: [y_t-1, input_feed, rule_to_word, word_state_ctx]", "\n", "", "rule_inp", "=", "hparams", ".", "d_word_vec", "+", "hparams", ".", "d_model", "*", "3", "\n", "# input: [y_t-1, input_feed, word_to_rule, rule_state_ctx]", "\n", "word_inp", "=", "hparams", ".", "d_word_vec", "+", "hparams", ".", "d_model", "*", "3", "\n", "\n", "self", ".", "rule_lstm_cell", "=", "nn", ".", "LSTMCell", "(", "rule_inp", ",", "hparams", ".", "d_model", ")", "\n", "self", ".", "word_lstm_cell", "=", "nn", ".", "LSTMCell", "(", "word_inp", ",", "hparams", ".", "d_model", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "hparams", ".", "dropout", ")", "\n", "\n", "vocab_mask", "=", "torch", ".", "zeros", "(", "1", ",", "1", ",", "self", ".", "target_vocab_size", ")", "\n", "self", ".", "word_vocab_mask", "=", "vocab_mask", ".", "index_fill_", "(", "2", ",", "torch", ".", "arange", "(", "self", ".", "hparams", ".", "target_word_vocab_size", ")", ".", "long", "(", ")", ",", "1", ")", "\n", "self", ".", "rule_vocab_mask", "=", "1", "-", "self", ".", "word_vocab_mask", "\n", "if", "self", ".", "hparams", ".", "cuda", ":", "\n", "      ", "self", ".", "rule_vocab_mask", "=", "self", ".", "rule_vocab_mask", ".", "cuda", "(", ")", "\n", "self", ".", "word_vocab_mask", "=", "self", ".", "word_vocab_mask", ".", "cuda", "(", ")", "\n", "self", ".", "emb", "=", "self", ".", "emb", ".", "cuda", "(", ")", "\n", "self", ".", "rule_attention", "=", "self", ".", "rule_attention", ".", "cuda", "(", ")", "\n", "self", ".", "word_attention", "=", "self", ".", "word_attention", ".", "cuda", "(", ")", "\n", "self", ".", "rule_to_word_attn", "=", "self", ".", "rule_to_word_attn", ".", "cuda", "(", ")", "\n", "self", ".", "word_to_rule_attn", "=", "self", ".", "word_to_rule_attn", ".", "cuda", "(", ")", "\n", "\n", "self", ".", "rule_ctx_to_readout", "=", "self", ".", "rule_ctx_to_readout", ".", "cuda", "(", ")", "\n", "self", ".", "word_ctx_to_readout", "=", "self", ".", "word_ctx_to_readout", ".", "cuda", "(", ")", "\n", "self", ".", "readout", "=", "self", ".", "readout", ".", "cuda", "(", ")", "\n", "self", ".", "rule_lstm_cell", "=", "self", ".", "rule_lstm_cell", ".", "cuda", "(", ")", "\n", "self", ".", "word_lstm_cell", "=", "self", ".", "word_lstm_cell", ".", "cuda", "(", ")", "\n", "self", ".", "dropout", "=", "self", ".", "dropout", ".", "cuda", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.trdec_attn_v1.TreeDecoderAttn_v1.forward": [[68, 174], ["x_enc.size", "y_train.size", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.ones().byte", "torch.ones().byte", "torch.ones().byte", "torch.ones().byte", "torch.ones().byte", "torch.ones().byte", "torch.ones().byte", "torch.ones().byte", "torch.ones().byte", "torch.ones().byte", "torch.ones().byte", "torch.ones().byte", "torch.ones().byte", "torch.ones().byte", "torch.ones().byte", "torch.ones().byte", "torch.ones().byte", "torch.ones().byte", "word_states_mask.cuda.cuda.index_fill_", "rule_states_mask.cuda.cuda.index_fill_", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "trdec_attn_v1.TreeDecoderAttn_v1.emb", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "range", "torch.cat().transpose().contiguous", "torch.cat().transpose().contiguous", "torch.cat().transpose().contiguous", "torch.cat().transpose().contiguous", "torch.cat().transpose().contiguous", "torch.cat().transpose().contiguous", "torch.cat().transpose().contiguous", "torch.cat().transpose().contiguous", "torch.cat().transpose().contiguous", "torch.cat().transpose().contiguous.data.masked_fill_", "torch.cat().transpose().contiguous.data.masked_fill_", "torch.cat().transpose().contiguous.data.masked_fill_", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "input_feed_zeros.cuda.cuda.cuda", "rule_states.cuda.cuda.cuda", "word_states.cuda.cuda.cuda", "rule_states_mask.cuda.cuda.cuda", "word_states_mask.cuda.cuda.cuda", "rule_input_feed.cuda.cuda.cuda", "word_input_feed.cuda.cuda.cuda", "rule_to_word_input_feed.cuda.cuda.cuda", "word_to_rule_input_feed.cuda.cuda.cuda", "offset.cuda.cuda.cuda", "y_train[].unsqueeze().float", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "trdec_attn_v1.TreeDecoderAttn_v1.word_lstm_cell", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "trdec_attn_v1.TreeDecoderAttn_v1.rule_lstm_cell", "trdec_attn_v1.TreeDecoderAttn_v1.rule_to_word_attn", "trdec_attn_v1.TreeDecoderAttn_v1.word_to_rule_attn", "trdec_attn_v1.TreeDecoderAttn_v1.rule_attention", "trdec_attn_v1.TreeDecoderAttn_v1.word_attention", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "trdec_attn_v1.TreeDecoderAttn_v1.dropout", "trdec_attn_v1.TreeDecoderAttn_v1.dropout", "rule_pre_readouts.append", "word_pre_readouts.append", "trdec_attn_v1.TreeDecoderAttn_v1.readout", "trdec_attn_v1.TreeDecoderAttn_v1.readout", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "score_mask.unsqueeze().float", "mask_t.byte", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "trdec_attn_v1.TreeDecoderAttn_v1.rule_ctx_to_readout", "trdec_attn_v1.TreeDecoderAttn_v1.word_ctx_to_readout", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.cat().transpose", "torch.cat().transpose", "torch.cat().transpose", "torch.cat().transpose", "torch.cat().transpose", "torch.cat().transpose", "torch.cat().transpose", "torch.cat().transpose", "torch.cat().transpose", "float", "y_train[].unsqueeze", "y_train[].unsqueeze().float.byte", "torch.softmax", "torch.softmax", "torch.softmax", "score_mask.unsqueeze", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "x_enc", ",", "x_enc_k", ",", "dec_init", ",", "x_mask", ",", "y_train", ",", "y_mask", ",", "score_mask", ",", "y_label", "=", "None", ")", ":", "\n", "# get decoder init state and cell, use x_ct", "\n", "    ", "\"\"\"\n    x_enc: [batch_size, max_x_len, d_model * 2]\n    \"\"\"", "\n", "batch_size_x", ",", "x_max_len", ",", "d_x", "=", "x_enc", ".", "size", "(", ")", "\n", "batch_size", ",", "y_max_len", ",", "data_len", "=", "y_train", ".", "size", "(", ")", "\n", "assert", "batch_size_x", "==", "batch_size", "\n", "#print(y_train)", "\n", "input_feed_zeros", "=", "torch", ".", "zeros", "(", "batch_size", ",", "self", ".", "hparams", ".", "d_model", "*", "2", ")", "\n", "input_feed_zeros_d", "=", "torch", ".", "zeros", "(", "batch_size", ",", "self", ".", "hparams", ".", "d_model", ")", "\n", "rule_states", "=", "Variable", "(", "torch", ".", "zeros", "(", "batch_size", ",", "y_max_len", ",", "self", ".", "hparams", ".", "d_model", ")", ")", "\n", "word_states", "=", "Variable", "(", "torch", ".", "zeros", "(", "batch_size", ",", "y_max_len", ",", "self", ".", "hparams", ".", "d_model", ")", ")", "\n", "rule_states_mask", "=", "torch", ".", "ones", "(", "batch_size", ",", "y_max_len", ")", ".", "byte", "(", ")", "\n", "word_states_mask", "=", "torch", ".", "ones", "(", "batch_size", ",", "y_max_len", ")", ".", "byte", "(", ")", "\n", "# avoid attn nan", "\n", "word_states_mask", ".", "index_fill_", "(", "1", ",", "torch", ".", "LongTensor", "(", "[", "0", "]", ")", ",", "0", ")", "\n", "rule_states_mask", ".", "index_fill_", "(", "1", ",", "torch", ".", "LongTensor", "(", "[", "0", "]", ")", ",", "0", ")", "\n", "if", "self", ".", "hparams", ".", "cuda", ":", "\n", "      ", "input_feed_zeros", "=", "input_feed_zeros", ".", "cuda", "(", ")", "\n", "rule_states", "=", "rule_states", ".", "cuda", "(", ")", "\n", "word_states", "=", "word_states", ".", "cuda", "(", ")", "\n", "rule_states_mask", "=", "rule_states_mask", ".", "cuda", "(", ")", "\n", "word_states_mask", "=", "word_states_mask", ".", "cuda", "(", ")", "\n", "\n", "", "rule_hidden", "=", "dec_init", "\n", "word_hidden", "=", "dec_init", "\n", "rule_input_feed", "=", "Variable", "(", "input_feed_zeros", ",", "requires_grad", "=", "False", ")", "\n", "word_input_feed", "=", "Variable", "(", "input_feed_zeros", ",", "requires_grad", "=", "False", ")", "\n", "rule_to_word_input_feed", "=", "Variable", "(", "input_feed_zeros_d", ",", "requires_grad", "=", "False", ")", "\n", "word_to_rule_input_feed", "=", "Variable", "(", "input_feed_zeros_d", ",", "requires_grad", "=", "False", ")", "\n", "if", "self", ".", "hparams", ".", "cuda", ":", "\n", "      ", "rule_input_feed", "=", "rule_input_feed", ".", "cuda", "(", ")", "\n", "word_input_feed", "=", "word_input_feed", ".", "cuda", "(", ")", "\n", "rule_to_word_input_feed", "=", "rule_to_word_input_feed", ".", "cuda", "(", ")", "\n", "word_to_rule_input_feed", "=", "word_to_rule_input_feed", ".", "cuda", "(", ")", "\n", "# [batch_size, y_len, d_word_vec]", "\n", "", "trg_emb", "=", "self", ".", "emb", "(", "y_train", "[", ":", ",", ":", ",", "0", "]", ")", "\n", "logits", "=", "[", "]", "\n", "rule_pre_readouts", "=", "[", "]", "\n", "word_pre_readouts", "=", "[", "]", "\n", "offset", "=", "torch", ".", "arange", "(", "batch_size", ")", ".", "long", "(", ")", "\n", "if", "self", ".", "hparams", ".", "cuda", ":", "\n", "      ", "offset", "=", "offset", ".", "cuda", "(", ")", "\n", "", "for", "t", "in", "range", "(", "y_max_len", ")", ":", "\n", "      ", "y_emb_tm1", "=", "trg_emb", "[", ":", ",", "t", ",", ":", "]", "\n", "word_mask", "=", "y_train", "[", ":", ",", "t", ",", "2", "]", ".", "unsqueeze", "(", "1", ")", ".", "float", "(", ")", "# (1 is word, 0 is rule)", "\n", "word_input", "=", "torch", ".", "cat", "(", "[", "y_emb_tm1", ",", "word_input_feed", ",", "word_to_rule_input_feed", "]", ",", "dim", "=", "1", ")", "\n", "word_h_t", ",", "word_c_t", "=", "self", ".", "word_lstm_cell", "(", "word_input", ",", "word_hidden", ")", "\n", "\n", "word_h_t", "=", "word_h_t", "*", "word_mask", "+", "word_hidden", "[", "0", "]", "*", "(", "1", "-", "word_mask", ")", "\n", "word_c_t", "=", "word_c_t", "*", "word_mask", "+", "word_hidden", "[", "1", "]", "*", "(", "1", "-", "word_mask", ")", "\n", "\n", "rule_input", "=", "torch", ".", "cat", "(", "[", "y_emb_tm1", ",", "rule_input_feed", ",", "rule_to_word_input_feed", "]", ",", "dim", "=", "1", ")", "\n", "rule_h_t", ",", "rule_c_t", "=", "self", ".", "rule_lstm_cell", "(", "rule_input", ",", "rule_hidden", ")", "\n", "\n", "#eos_mask = (y_train[:, t, 0] == self.hparams.eos_id).unsqueeze(1).float()", "\n", "#word_mask = word_mask - eos_mask", "\n", "#rule_h_t = rule_h_t * (1-word_mask) + rule_hidden[0] * word_mask", "\n", "#rule_c_t = rule_c_t * (1-word_mask) + rule_hidden[1] * word_mask", "\n", "\n", "rule_states", ".", "data", "[", ":", ",", "t", ",", ":", "]", "=", "rule_h_t", ".", "data", "\n", "word_states", ".", "data", "[", ":", ",", "t", ",", ":", "]", "=", "word_h_t", ".", "data", "\n", "# word_mask: 1 is word, 0 is rule", "\n", "if", "t", ">", "0", ":", "\n", "        ", "rule_states_mask", "[", ":", ",", "t", "]", "=", "word_mask", ".", "byte", "(", ")", ".", "data", "\n", "word_states_mask", "[", ":", ",", "t", "]", "=", "(", "1", "-", "word_mask", ")", ".", "byte", "(", ")", ".", "data", "\n", "#idx = torch.LongTensor([t])", "\n", "#if self.hparams.cuda:", "\n", "#  idx = idx.cuda()", "\n", "#rule_states_mask.index_fill_(1, idx, 0)", "\n", "", "rule_to_word_ctx", "=", "self", ".", "rule_to_word_attn", "(", "rule_h_t", ",", "word_states", ",", "word_states", ",", "attn_mask", "=", "word_states_mask", ")", "\n", "word_to_rule_ctx", "=", "self", ".", "word_to_rule_attn", "(", "word_h_t", ",", "rule_states", ",", "rule_states", ",", "attn_mask", "=", "rule_states_mask", ")", "\n", "rule_ctx", "=", "self", ".", "rule_attention", "(", "rule_h_t", ",", "x_enc_k", ",", "x_enc", ",", "attn_mask", "=", "x_mask", ")", "\n", "word_ctx", "=", "self", ".", "word_attention", "(", "word_h_t", ",", "x_enc_k", ",", "x_enc", ",", "attn_mask", "=", "x_mask", ")", "\n", "\n", "inp", "=", "torch", ".", "cat", "(", "[", "rule_h_t", ",", "rule_ctx", ",", "word_h_t", ",", "word_ctx", "]", ",", "dim", "=", "1", ")", "\n", "rule_pre_readout", "=", "F", ".", "tanh", "(", "self", ".", "rule_ctx_to_readout", "(", "inp", ")", ")", "\n", "word_pre_readout", "=", "F", ".", "tanh", "(", "self", ".", "word_ctx_to_readout", "(", "inp", ")", ")", "\n", "\n", "rule_pre_readout", "=", "self", ".", "dropout", "(", "rule_pre_readout", ")", "\n", "word_pre_readout", "=", "self", ".", "dropout", "(", "word_pre_readout", ")", "\n", "\n", "rule_pre_readouts", ".", "append", "(", "rule_pre_readout", ")", "\n", "word_pre_readouts", ".", "append", "(", "word_pre_readout", ")", "\n", "\n", "rule_input_feed", "=", "rule_ctx", "\n", "word_input_feed", "=", "word_ctx", "\n", "rule_to_word_input_feed", "=", "rule_to_word_ctx", "\n", "word_to_rule_input_feed", "=", "word_to_rule_ctx", "\n", "\n", "rule_hidden", "=", "(", "rule_h_t", ",", "rule_c_t", ")", "\n", "word_hidden", "=", "(", "word_h_t", ",", "word_c_t", ")", "\n", "# [len_y, batch_size, trg_vocab_size]", "\n", "", "rule_readouts", "=", "self", ".", "readout", "(", "torch", ".", "stack", "(", "rule_pre_readouts", ")", ")", "[", ":", ",", ":", ",", "-", "self", ".", "hparams", ".", "target_rule_vocab_size", ":", "]", "\n", "word_readouts", "=", "self", ".", "readout", "(", "torch", ".", "stack", "(", "word_pre_readouts", ")", ")", "[", ":", ",", ":", ",", ":", "self", ".", "hparams", ".", "target_word_vocab_size", "]", "\n", "if", "self", ".", "hparams", ".", "label_smooth", ">", "0", ":", "\n", "      ", "smooth", "=", "self", ".", "hparams", ".", "label_smooth", "\n", "rule_probs", "=", "(", "1.0", "-", "smooth", ")", "*", "F", ".", "softmax", "(", "rule_readouts", ",", "dim", "=", "2", ")", "+", "smooth", "/", "self", ".", "hparams", ".", "target_rule_vocab_size", "\n", "rule_readouts", "=", "torch", ".", "log", "(", "rule_probs", ")", "\n", "# [batch_size, len_y, trg_vocab_size]", "\n", "", "logits", "=", "torch", ".", "cat", "(", "[", "word_readouts", ",", "rule_readouts", "]", ",", "dim", "=", "2", ")", ".", "transpose", "(", "0", ",", "1", ")", ".", "contiguous", "(", ")", "\n", "score_mask", "=", "score_mask", ".", "unsqueeze", "(", "2", ")", ".", "float", "(", ")", ".", "data", "\n", "mask_t", "=", "self", ".", "word_vocab_mask", "*", "(", "1", "-", "score_mask", ")", "+", "self", ".", "rule_vocab_mask", "*", "score_mask", "\n", "logits", ".", "data", ".", "masked_fill_", "(", "mask_t", ".", "byte", "(", ")", ",", "-", "float", "(", "\"inf\"", ")", ")", "\n", "return", "logits", "\n", "\n"]], "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.trdec_attn_v1.TreeDecoderAttn_v1.step": [[175, 243], ["torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "trdec_attn_v1.TreeDecoderAttn_v1.emb", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "trdec_attn_v1.TreeDecoderAttn_v1.rule_lstm_cell", "trdec_attn_v1.TreeDecoderAttn_v1.rule_attention", "trdec_attn_v1.TreeDecoderAttn_v1.word_attention", "trdec_attn_v1.TreeDecoderAttn_v1.rule_to_word_attn", "trdec_attn_v1.TreeDecoderAttn_v1.word_to_rule_attn", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.ones().byte", "torch.ones().byte", "torch.ones().byte", "torch.ones().byte", "torch.ones().byte", "torch.ones().byte", "torch.ones().byte", "torch.ones().byte", "torch.ones().byte", "trdec_attn_v1.TreeDecoderAttn_v1.data.masked_fill_", "y_tm1.cuda.cuda.cuda", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "trdec_attn_v1.TreeDecoderAttn_v1.word_lstm_cell", "rule_h_t.unsqueeze", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "mask.cuda.cuda.index_fill_", "torch.tanh", "torch.tanh", "torch.tanh", "trdec_attn_v1.TreeDecoderAttn_v1.readout", "target_rule_vocab.rule_index_with_lhs", "len", "mask.cuda.cuda.index_fill_", "torch.tanh", "torch.tanh", "torch.tanh", "trdec_attn_v1.TreeDecoderAttn_v1.readout", "mask.cuda.cuda.cuda", "int", "word_h_t.unsqueeze", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "trdec_attn_v1.TreeDecoderAttn_v1.word_ctx_to_readout", "rule_select_index.append", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "trdec_attn_v1.TreeDecoderAttn_v1.rule_ctx_to_readout", "float", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "word_h_t.unsqueeze", "rule_h_t.unsqueeze", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange"], "methods", ["home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.tree_utils.RuleVocab.rule_index_with_lhs"], ["", "def", "step", "(", "self", ",", "x_enc", ",", "x_enc_k", ",", "hyp", ",", "target_rule_vocab", ")", ":", "\n", "    ", "y_tm1", "=", "torch", ".", "LongTensor", "(", "[", "int", "(", "hyp", ".", "y", "[", "-", "1", "]", ")", "]", ")", "\n", "if", "self", ".", "hparams", ".", "cuda", ":", "\n", "      ", "y_tm1", "=", "y_tm1", ".", "cuda", "(", ")", "\n", "", "y_tm1", "=", "Variable", "(", "y_tm1", ",", "volatile", "=", "True", ")", "\n", "open_nonterms", "=", "hyp", ".", "open_nonterms", "\n", "rule_input_feed", "=", "hyp", ".", "rule_ctx_tm1", "\n", "word_input_feed", "=", "hyp", ".", "word_ctx_tm1", "\n", "rule_to_word_input_feed", "=", "hyp", ".", "rule_to_word_ctx_tm1", "\n", "word_to_rule_input_feed", "=", "hyp", ".", "word_to_rule_ctx_tm1", "\n", "rule_hidden", "=", "hyp", ".", "rule_hidden", "\n", "word_hidden", "=", "hyp", ".", "word_hidden", "\n", "word_h_t", ",", "word_c_t", "=", "word_hidden", "\n", "rule_h_t", ",", "rule_c_t", "=", "rule_hidden", "\n", "\n", "y_emb_tm1", "=", "self", ".", "emb", "(", "y_tm1", ")", "\n", "cur_nonterm", "=", "open_nonterms", "[", "-", "1", "]", "\n", "\n", "if", "hyp", ".", "y", "[", "-", "1", "]", "<", "self", ".", "hparams", ".", "target_word_vocab_size", ":", "\n", "# word", "\n", "      ", "word_input", "=", "torch", ".", "cat", "(", "[", "y_emb_tm1", ",", "word_input_feed", ",", "word_to_rule_input_feed", "]", ",", "dim", "=", "1", ")", "\n", "word_h_t", ",", "word_c_t", "=", "self", ".", "word_lstm_cell", "(", "word_input", ",", "word_hidden", ")", "\n", "if", "hyp", ".", "word_states", "is", "None", ":", "\n", "        ", "hyp", ".", "word_states", "=", "word_h_t", ".", "unsqueeze", "(", "1", ")", "\n", "", "else", ":", "\n", "        ", "hyp", ".", "word_states", "=", "torch", ".", "cat", "(", "[", "hyp", ".", "word_states", ",", "word_h_t", ".", "unsqueeze", "(", "1", ")", "]", ",", "dim", "=", "1", ")", "\n", "", "", "rule_input", "=", "torch", ".", "cat", "(", "[", "y_emb_tm1", ",", "rule_input_feed", ",", "rule_to_word_input_feed", "]", ",", "dim", "=", "1", ")", "\n", "rule_h_t", ",", "rule_c_t", "=", "self", ".", "rule_lstm_cell", "(", "rule_input", ",", "rule_hidden", ")", "\n", "if", "hyp", ".", "rule_states", "is", "None", ":", "\n", "      ", "hyp", ".", "rule_states", "=", "rule_h_t", ".", "unsqueeze", "(", "1", ")", "\n", "", "else", ":", "\n", "      ", "if", "hyp", ".", "y", "[", "-", "1", "]", ">=", "self", ".", "hparams", ".", "target_word_vocab_size", ":", "\n", "        ", "hyp", ".", "rule_states", "=", "torch", ".", "cat", "(", "[", "hyp", ".", "rule_states", ",", "rule_h_t", ".", "unsqueeze", "(", "1", ")", "]", ",", "dim", "=", "1", ")", "\n", "", "", "hyp", ".", "rule_ctx_tm1", "=", "self", ".", "rule_attention", "(", "rule_h_t", ",", "x_enc_k", ",", "x_enc", ")", "\n", "hyp", ".", "word_ctx_tm1", "=", "self", ".", "word_attention", "(", "word_h_t", ",", "x_enc_k", ",", "x_enc", ")", "\n", "hyp", ".", "rule_to_word_ctx_tm1", "=", "self", ".", "rule_to_word_attn", "(", "rule_h_t", ",", "hyp", ".", "word_states", ",", "hyp", ".", "word_states", ")", "\n", "hyp", ".", "word_to_rule_ctx_tm1", "=", "self", ".", "word_to_rule_attn", "(", "word_h_t", ",", "hyp", ".", "rule_states", ",", "hyp", ".", "rule_states", ")", "\n", "\n", "inp", "=", "torch", ".", "cat", "(", "[", "rule_h_t", ",", "hyp", ".", "rule_ctx_tm1", ",", "word_h_t", ",", "hyp", ".", "word_ctx_tm1", "]", ",", "dim", "=", "1", ")", "\n", "mask", "=", "torch", ".", "ones", "(", "1", ",", "self", ".", "target_vocab_size", ")", ".", "byte", "(", ")", "\n", "if", "cur_nonterm", ".", "label", "==", "'*'", ":", "\n", "      ", "word_index", "=", "torch", ".", "arange", "(", "self", ".", "hparams", ".", "target_word_vocab_size", ")", ".", "long", "(", ")", "\n", "mask", ".", "index_fill_", "(", "1", ",", "word_index", ",", "0", ")", "\n", "word_pre_readout", "=", "F", ".", "tanh", "(", "self", ".", "word_ctx_to_readout", "(", "inp", ")", ")", "\n", "#word_pre_readout = self.dropout(word_pre_readout)", "\n", "score_t", "=", "self", ".", "readout", "(", "word_pre_readout", ")", "\n", "num_rule_index", "=", "-", "1", "\n", "rule_select_index", "=", "[", "]", "\n", "", "else", ":", "\n", "      ", "rule_with_lhs", "=", "target_rule_vocab", ".", "rule_index_with_lhs", "(", "cur_nonterm", ".", "label", ")", "\n", "rule_select_index", "=", "[", "]", "\n", "for", "i", "in", "rule_with_lhs", ":", "rule_select_index", ".", "append", "(", "i", "+", "self", ".", "hparams", ".", "target_word_vocab_size", ")", "\n", "num_rule_index", "=", "len", "(", "rule_with_lhs", ")", "\n", "rule_index", "=", "torch", ".", "arange", "(", "self", ".", "hparams", ".", "target_rule_vocab_size", ")", ".", "long", "(", ")", "+", "self", ".", "hparams", ".", "target_word_vocab_size", "\n", "mask", ".", "index_fill_", "(", "1", ",", "rule_index", ",", "0", ")", "\n", "rule_pre_readout", "=", "F", ".", "tanh", "(", "self", ".", "rule_ctx_to_readout", "(", "inp", ")", ")", "\n", "#rule_pre_readout = self.dropout(rule_pre_readout)", "\n", "score_t", "=", "self", ".", "readout", "(", "rule_pre_readout", ")", "\n", "", "if", "self", ".", "hparams", ".", "cuda", ":", "\n", "      ", "mask", "=", "mask", ".", "cuda", "(", ")", "\n", "", "score_t", ".", "data", ".", "masked_fill_", "(", "mask", ",", "-", "float", "(", "\"inf\"", ")", ")", "\n", "rule_hidden", "=", "(", "rule_h_t", ",", "rule_c_t", ")", "\n", "word_hidden", "=", "(", "word_h_t", ",", "word_c_t", ")", "\n", "\n", "hyp", ".", "rule_hidden", "=", "rule_hidden", "\n", "hyp", ".", "word_hidden", "=", "word_hidden", "\n", "\n", "return", "score_t", ",", "num_rule_index", ",", "rule_select_index", "\n", "\n"]], "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.trdec_attn_v1.TrDecAttn_v1.__init__": [[245, 254], ["torch.nn.Module.__init__", "models.Encoder", "trdec_attn_v1.TreeDecoderAttn_v1", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "trdec_attn_v1.TrDecAttn_v1.enc_to_k.cuda"], "methods", ["home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.utils.Logger.__init__"], ["  ", "def", "__init__", "(", "self", ",", "hparams", ")", ":", "\n", "    ", "super", "(", "TrDecAttn_v1", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "encoder", "=", "Encoder", "(", "hparams", ")", "\n", "self", ".", "decoder", "=", "TreeDecoderAttn_v1", "(", "hparams", ")", "\n", "# transform encoder state vectors into attention key vector", "\n", "self", ".", "enc_to_k", "=", "nn", ".", "Linear", "(", "hparams", ".", "d_model", "*", "2", ",", "hparams", ".", "d_model", ",", "bias", "=", "False", ")", "\n", "self", ".", "hparams", "=", "hparams", "\n", "if", "self", ".", "hparams", ".", "cuda", ":", "\n", "      ", "self", ".", "enc_to_k", "=", "self", ".", "enc_to_k", ".", "cuda", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.trdec_attn_v1.TrDecAttn_v1.forward": [[255, 262], ["trdec_attn_v1.TrDecAttn_v1.encoder", "trdec_attn_v1.TrDecAttn_v1.enc_to_k", "trdec_attn_v1.TrDecAttn_v1.decoder"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "x_train", ",", "x_mask", ",", "x_len", ",", "y_train", ",", "y_mask", ",", "y_len", ",", "score_mask", ",", "y_label", "=", "None", ")", ":", "\n", "# [batch_size, x_len, d_model * 2]", "\n", "    ", "x_enc", ",", "dec_init", "=", "self", ".", "encoder", "(", "x_train", ",", "x_len", ")", "\n", "x_enc_k", "=", "self", ".", "enc_to_k", "(", "x_enc", ")", "\n", "# [batch_size, y_len-1, trg_vocab_size]", "\n", "logits", "=", "self", ".", "decoder", "(", "x_enc", ",", "x_enc_k", ",", "dec_init", ",", "x_mask", ",", "y_train", ",", "y_mask", ",", "score_mask", ",", "y_label", ")", "\n", "return", "logits", "\n", "\n"]], "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.trdec_attn_v1.TrDecAttn_v1.translate": [[263, 286], ["torch.nn.LogSoftmax", "torch.nn.LogSoftmax", "torch.nn.LogSoftmax", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "trdec_attn_v1.TrDecAttn_v1.translate_sent", "hyps.append", "scores.append", "gc.collect", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "x.cuda.cuda.cuda", "sum"], "methods", ["home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.trdec_single.TrDecSingle.translate_sent"], ["", "def", "translate", "(", "self", ",", "x_train", ",", "target_rule_vocab", ",", "max_len", "=", "100", ",", "beam_size", "=", "5", ",", "y_label", "=", "None", ",", "poly_norm_m", "=", "0", ")", ":", "\n", "    ", "hyps", "=", "[", "]", "\n", "scores", "=", "[", "]", "\n", "i", "=", "0", "\n", "self", ".", "logsoftmax", "=", "nn", ".", "LogSoftmax", "(", "dim", "=", "1", ")", "\n", "for", "x", "in", "x_train", ":", "\n", "      ", "x", "=", "Variable", "(", "torch", ".", "LongTensor", "(", "x", ")", ",", "volatile", "=", "True", ")", "\n", "if", "y_label", ":", "\n", "        ", "y", "=", "y_label", "[", "i", "]", "[", "1", ":", "]", "\n", "", "else", ":", "\n", "        ", "y", "=", "None", "\n", "", "if", "self", ".", "hparams", ".", "cuda", ":", "\n", "        ", "x", "=", "x", ".", "cuda", "(", ")", "\n", "", "hyp", ",", "nll_score", "=", "self", ".", "translate_sent", "(", "x", ",", "target_rule_vocab", ",", "max_len", "=", "max_len", ",", "beam_size", "=", "beam_size", ",", "y_label", "=", "y", ",", "poly_norm_m", "=", "poly_norm_m", ")", "\n", "hyp", "=", "hyp", "[", "0", "]", "\n", "hyps", ".", "append", "(", "hyp", ".", "y", "[", "1", ":", "]", ")", "\n", "scores", ".", "append", "(", "sum", "(", "nll_score", ")", ")", "\n", "#print(hyp.y)", "\n", "#print(\"trans score:\", nll_score)", "\n", "#print(\"trans label:\", y)", "\n", "i", "+=", "1", "\n", "gc", ".", "collect", "(", ")", "\n", "", "return", "hyps", ",", "scores", "\n", "\n"]], "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.trdec_attn_v1.TrDecAttn_v1.translate_sent": [[287, 384], ["x_train.unsqueeze.unsqueeze.unsqueeze", "trdec_attn_v1.TrDecAttn_v1.encoder", "trdec_attn_v1.TrDecAttn_v1.enc_to_k", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "len", "x_train.unsqueeze.unsqueeze.size", "input_feed_zeros.cuda.cuda.cuda", "to_input_feed_zeros.cuda.cuda.cuda", "trdec_attn_v1.TrAttnHyp", "len", "enumerate", "len", "completed_hyp.append", "sorted", "x_train.unsqueeze.unsqueeze.size", "len", "trdec_attn_v1.TrDecAttn_v1.decoder.step", "set", "logits.view.view.view", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "nll_score.append", "print", "trdec_attn_v1.TrAttnHyp", "new_active_hyp.append", "len", "sorted", "trdec_attn_v1.OpenNonterm", "pow", "min", "open_nonterms.pop", "reversed", "min", "len", "completed_hyp.append", "active_hyp.append", "len", "open_nonterms.append", "print", "print", "print", "open_nonterms.pop", "pow", "trdec_attn_v1.OpenNonterm", "target_rule_vocab.rule_index_with_lhs"], "methods", ["home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.trdec_single.TreeDecoderSingle.step", "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.tree_utils.RuleVocab.rule_index_with_lhs"], ["", "def", "translate_sent", "(", "self", ",", "x_train", ",", "target_rule_vocab", ",", "max_len", "=", "100", ",", "beam_size", "=", "5", ",", "y_label", "=", "None", ",", "poly_norm_m", "=", "1.", ")", ":", "\n", "    ", "assert", "len", "(", "x_train", ".", "size", "(", ")", ")", "==", "1", "\n", "x_len", "=", "[", "x_train", ".", "size", "(", "0", ")", "]", "\n", "x_train", "=", "x_train", ".", "unsqueeze", "(", "0", ")", "\n", "x_enc", ",", "dec_init", "=", "self", ".", "encoder", "(", "x_train", ",", "x_len", ")", "\n", "x_enc_k", "=", "self", ".", "enc_to_k", "(", "x_enc", ")", "\n", "length", "=", "0", "\n", "completed_hyp", "=", "[", "]", "\n", "input_feed_zeros", "=", "torch", ".", "zeros", "(", "1", ",", "self", ".", "hparams", ".", "d_model", "*", "2", ")", "\n", "to_input_feed_zeros", "=", "torch", ".", "zeros", "(", "1", ",", "self", ".", "hparams", ".", "d_model", ")", "\n", "#state_zeros = torch.zeros(1, 1, self.hparams.d_model)", "\n", "if", "self", ".", "hparams", ".", "cuda", ":", "\n", "      ", "input_feed_zeros", "=", "input_feed_zeros", ".", "cuda", "(", ")", "\n", "to_input_feed_zeros", "=", "to_input_feed_zeros", ".", "cuda", "(", ")", "\n", "#state_zeros = state_zeros.cuda()", "\n", "", "active_hyp", "=", "[", "TrAttnHyp", "(", "rule_hidden", "=", "dec_init", ",", "\n", "word_hidden", "=", "dec_init", ",", "\n", "y", "=", "[", "self", ".", "hparams", ".", "bos_id", "]", ",", "\n", "rule_ctx_tm1", "=", "Variable", "(", "input_feed_zeros", ",", "requires_grad", "=", "False", ")", ",", "\n", "word_ctx_tm1", "=", "Variable", "(", "input_feed_zeros", ",", "requires_grad", "=", "False", ")", ",", "\n", "rule_to_word_ctx_tm1", "=", "Variable", "(", "to_input_feed_zeros", ",", "requires_grad", "=", "False", ")", ",", "\n", "word_to_rule_ctx_tm1", "=", "Variable", "(", "to_input_feed_zeros", ",", "requires_grad", "=", "False", ")", ",", "\n", "rule_states", "=", "None", ",", "\n", "word_states", "=", "None", ",", "\n", "open_nonterms", "=", "[", "OpenNonterm", "(", "label", "=", "self", ".", "hparams", ".", "root_label", ")", "]", ",", "\n", "score", "=", "0.", ")", "]", "\n", "nll_score", "=", "[", "]", "\n", "if", "y_label", "is", "not", "None", ":", "\n", "      ", "max_len", "=", "len", "(", "y_label", ")", "\n", "", "while", "len", "(", "completed_hyp", ")", "<", "beam_size", "and", "length", "<", "max_len", ":", "\n", "      ", "length", "+=", "1", "\n", "new_active_hyp", "=", "[", "]", "\n", "for", "i", ",", "hyp", "in", "enumerate", "(", "active_hyp", ")", ":", "\n", "        ", "logits", ",", "num_rule_index", ",", "rule_index", "=", "self", ".", "decoder", ".", "step", "(", "x_enc", ",", "\n", "x_enc_k", ",", "hyp", ",", "target_rule_vocab", ")", "\n", "\n", "rule_index", "=", "set", "(", "rule_index", ")", "\n", "logits", "=", "logits", ".", "view", "(", "-", "1", ")", "\n", "p_t", "=", "F", ".", "log_softmax", "(", "logits", ",", "dim", "=", "0", ")", ".", "data", "\n", "if", "poly_norm_m", ">", "0", "and", "length", ">", "1", ":", "\n", "          ", "new_hyp_scores", "=", "(", "hyp", ".", "score", "*", "pow", "(", "length", "-", "1", ",", "poly_norm_m", ")", "+", "p_t", ")", "/", "pow", "(", "length", ",", "poly_norm_m", ")", "\n", "", "else", ":", "\n", "          ", "new_hyp_scores", "=", "hyp", ".", "score", "+", "p_t", "\n", "", "if", "y_label", "is", "not", "None", ":", "\n", "          ", "top_ids", "=", "[", "y_label", "[", "length", "-", "1", "]", "[", "0", "]", "]", "\n", "nll", "=", "-", "(", "p_t", "[", "top_ids", "[", "0", "]", "]", ")", "\n", "nll_score", ".", "append", "(", "nll", ")", "\n", "print", "(", "\"logit dedcode\"", ",", "logits", "[", "top_ids", "[", "0", "]", "]", ")", "\n", "", "else", ":", "\n", "          ", "num_select", "=", "beam_size", "\n", "if", "num_rule_index", ">=", "0", ":", "num_select", "=", "min", "(", "num_select", ",", "num_rule_index", ")", "\n", "top_ids", "=", "(", "-", "new_hyp_scores", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ".", "argsort", "(", ")", "[", ":", "num_select", "]", "\n", "", "for", "word_id", "in", "top_ids", ":", "\n", "          ", "if", "y_label", "is", "None", "and", "len", "(", "rule_index", ")", ">", "0", "and", "word_id", "not", "in", "rule_index", ":", "continue", "\n", "open_nonterms", "=", "hyp", ".", "open_nonterms", "[", ":", "]", "\n", "if", "word_id", ">=", "self", ".", "hparams", ".", "target_word_vocab_size", ":", "\n", "            ", "rule", "=", "target_rule_vocab", "[", "word_id", "]", "\n", "cur_nonterm", "=", "open_nonterms", ".", "pop", "(", ")", "\n", "for", "c", "in", "reversed", "(", "rule", ".", "rhs", ")", ":", "\n", "              ", "open_nonterms", ".", "append", "(", "OpenNonterm", "(", "label", "=", "c", ")", ")", "\n", "", "", "else", ":", "\n", "            ", "if", "open_nonterms", "[", "-", "1", "]", ".", "label", "!=", "'*'", ":", "\n", "              ", "print", "(", "open_nonterms", "[", "-", "1", "]", ".", "label", ",", "word_id", ",", "new_hyp_scores", "[", "word_id", "]", ")", "\n", "print", "(", "target_rule_vocab", ".", "rule_index_with_lhs", "(", "open_nonterms", "[", "-", "1", "]", ".", "label", ")", ")", "\n", "print", "(", "top_ids", ")", "\n", "", "assert", "open_nonterms", "[", "-", "1", "]", ".", "label", "==", "'*'", "\n", "if", "word_id", "==", "self", ".", "hparams", ".", "eos_id", ":", "\n", "              ", "open_nonterms", ".", "pop", "(", ")", "\n", "", "", "new_hyp", "=", "TrAttnHyp", "(", "rule_hidden", "=", "(", "hyp", ".", "rule_hidden", "[", "0", "]", ",", "hyp", ".", "rule_hidden", "[", "1", "]", ")", ",", "\n", "word_hidden", "=", "(", "hyp", ".", "word_hidden", "[", "0", "]", ",", "hyp", ".", "word_hidden", "[", "1", "]", ")", ",", "\n", "y", "=", "hyp", ".", "y", "+", "[", "word_id", "]", ",", "\n", "rule_ctx_tm1", "=", "hyp", ".", "rule_ctx_tm1", ",", "\n", "word_ctx_tm1", "=", "hyp", ".", "word_ctx_tm1", ",", "\n", "rule_to_word_ctx_tm1", "=", "hyp", ".", "rule_to_word_ctx_tm1", ",", "\n", "word_to_rule_ctx_tm1", "=", "hyp", ".", "word_to_rule_ctx_tm1", ",", "\n", "rule_states", "=", "hyp", ".", "rule_states", ",", "\n", "word_states", "=", "hyp", ".", "word_states", ",", "\n", "open_nonterms", "=", "open_nonterms", ",", "\n", "score", "=", "new_hyp_scores", "[", "word_id", "]", ")", "\n", "new_active_hyp", ".", "append", "(", "new_hyp", ")", "\n", "", "", "if", "y_label", "is", "None", ":", "\n", "        ", "live_hyp_num", "=", "beam_size", "-", "len", "(", "completed_hyp", ")", "\n", "new_active_hyp", "=", "sorted", "(", "new_active_hyp", ",", "key", "=", "lambda", "x", ":", "x", ".", "score", ",", "reverse", "=", "True", ")", "[", ":", "min", "(", "beam_size", ",", "live_hyp_num", ")", "]", "\n", "active_hyp", "=", "[", "]", "\n", "for", "hyp", "in", "new_active_hyp", ":", "\n", "          ", "if", "len", "(", "hyp", ".", "open_nonterms", ")", "==", "0", ":", "\n", "#if poly_norm_m <= 0:", "\n", "#  hyp.score = hyp.score / len(hyp.y)", "\n", "            ", "completed_hyp", ".", "append", "(", "hyp", ")", "\n", "", "else", ":", "\n", "            ", "active_hyp", ".", "append", "(", "hyp", ")", "\n", "", "", "", "else", ":", "\n", "        ", "active_hyp", "=", "new_active_hyp", "\n", "\n", "", "", "if", "len", "(", "completed_hyp", ")", "==", "0", ":", "\n", "      ", "completed_hyp", ".", "append", "(", "active_hyp", "[", "0", "]", ")", "\n", "", "return", "sorted", "(", "completed_hyp", ",", "key", "=", "lambda", "x", ":", "x", ".", "score", ",", "reverse", "=", "True", ")", ",", "nll_score", "\n", "\n"]], "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.trdec_attn_v1.TrAttnHyp.__init__": [[386, 402], ["None"], "methods", ["None"], ["  ", "def", "__init__", "(", "self", ",", "rule_hidden", ",", "word_hidden", ",", "y", ",", "\n", "rule_ctx_tm1", ",", "word_ctx_tm1", ",", "\n", "rule_to_word_ctx_tm1", ",", "word_to_rule_ctx_tm1", ",", "\n", "rule_states", ",", "word_states", ",", "score", ",", "open_nonterms", ")", ":", "\n", "    ", "self", ".", "rule_hidden", "=", "rule_hidden", "\n", "self", ".", "word_hidden", "=", "word_hidden", "\n", "self", ".", "rule_states", "=", "rule_states", "\n", "self", ".", "word_states", "=", "word_states", "\n", "# [length_y, 2], each element (index, is_word)", "\n", "self", ".", "y", "=", "y", "\n", "self", ".", "rule_ctx_tm1", "=", "rule_ctx_tm1", "\n", "self", ".", "word_ctx_tm1", "=", "word_ctx_tm1", "\n", "self", ".", "rule_to_word_ctx_tm1", "=", "rule_to_word_ctx_tm1", "\n", "self", ".", "word_to_rule_ctx_tm1", "=", "word_to_rule_ctx_tm1", "\n", "self", ".", "score", "=", "score", "\n", "self", ".", "open_nonterms", "=", "open_nonterms", "\n", "\n"]], "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.trdec_attn_v1.OpenNonterm.__init__": [[404, 407], ["None"], "methods", ["None"], ["  ", "def", "__init__", "(", "self", ",", "label", ",", "parent_state", "=", "None", ")", ":", "\n", "    ", "self", ".", "label", "=", "label", "\n", "self", ".", "parent_state", "=", "parent_state", "\n", "", "", ""]], "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.main.eval": [[119, 276], ["print", "model.eval", "data.reset_valid", "print", "model.train", "os.path.join", "open", "gc.collect", "data.next_valid", "val_loss.item", "val_acc.item", "data.x_valid.tolist", "numpy.exp", "numpy.exp", "open.close", "subprocess.getoutput", "[].strip", "re.compile", "open", "model.forward", "logits.view.view", "y_valid[].contiguous().view", "utils.get_performance", "word_loss.item", "rule_loss.item", "eos_loss.item", "model.forward", "logits.view.view", "y_valid[].contiguous().view", "utils.get_performance", "model.translate", "model.translate", "numpy.exp", "numpy.exp", "open.close", "os.path.join", "os.path.join", "float", "tree_utils.Tree.from_rule_deriv", "Tree.from_rule_deriv.to_string", "open.write", "open.flush", "open.write", "open.flush", "map", "line.replace.strip", "open.write", "open.flush", "numpy.exp", "re.compile.match().group", "y_valid[].contiguous", "y_valid[].contiguous", "line.replace.replace", "line.replace.replace().strip", "filter", "line.replace.replace", "[].strip.split", "deriv.append", "deriv.append", "Tree.from_rule_deriv.to_parse_string", "re.compile.match", "line.replace.replace"], "function", ["home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.main.eval", "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.data_utils.DataLoader.reset_valid", "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.main.train", "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.data_utils.DataLoader.next_valid", "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.trdec_single.TrDecSingle.forward", "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.utils.get_performance", "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.trdec_single.TrDecSingle.forward", "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.utils.get_performance", "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.trdec_single.TrDecSingle.translate", "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.trdec_single.TrDecSingle.translate", "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.tree_utils.Tree.from_rule_deriv", "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.tree_utils.Tree.to_string", "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.utils.Logger.write", "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.utils.Logger.flush", "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.utils.Logger.write", "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.utils.Logger.flush", "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.utils.Logger.write", "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.utils.Logger.flush", "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.tree_utils.Tree.to_parse_string"], ["def", "eval", "(", "model", ",", "data", ",", "crit", ",", "step", ",", "hparams", ",", "eval_bleu", "=", "False", ",", "\n", "valid_batch_size", "=", "20", ",", "tr_logits", "=", "None", ")", ":", "\n", "  ", "print", "(", "\"Eval at step {0}. valid_batch_size={1}\"", ".", "format", "(", "step", ",", "valid_batch_size", ")", ")", "\n", "\n", "model", ".", "eval", "(", ")", "\n", "data", ".", "reset_valid", "(", ")", "\n", "valid_words", "=", "0", "\n", "valid_loss", "=", "0", "\n", "valid_acc", "=", "0", "\n", "n_batches", "=", "0", "\n", "\n", "valid_total", "=", "valid_rule_count", "=", "valid_word_count", "=", "valid_eos_count", "=", "0", "\n", "valid_word_loss", ",", "valid_rule_loss", ",", "valid_eos_loss", "=", "0", ",", "0", ",", "0", "\n", "valid_bleu", "=", "None", "\n", "if", "eval_bleu", ":", "\n", "    ", "valid_hyp_file", "=", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir", ",", "\"dev.trans_{0}\"", ".", "format", "(", "step", ")", ")", "\n", "out_file", "=", "open", "(", "valid_hyp_file", ",", "'w'", ",", "encoding", "=", "'utf-8'", ")", "\n", "if", "args", ".", "trdec", ":", "\n", "      ", "valid_parse_file", "=", "valid_hyp_file", "+", "\".parse\"", "\n", "out_parse_file", "=", "open", "(", "valid_parse_file", ",", "'w'", ",", "encoding", "=", "'utf-8'", ")", "\n", "", "", "while", "True", ":", "\n", "# clear GPU memory", "\n", "    ", "gc", ".", "collect", "(", ")", "\n", "\n", "# next batch", "\n", "(", "(", "x_valid", ",", "x_mask", ",", "x_len", ",", "x_count", ")", ",", "\n", "(", "y_valid", ",", "y_mask", ",", "y_len", ",", "y_count", ")", ",", "\n", "batch_size", ",", "end_of_epoch", ")", "=", "data", ".", "next_valid", "(", "valid_batch_size", "=", "valid_batch_size", ")", "\n", "#print(x_valid)", "\n", "#print(x_mask)", "\n", "#print(y_valid)", "\n", "#print(y_mask)", "\n", "# do this since you shift y_valid[:, 1:] and y_valid[:, :-1]", "\n", "if", "args", ".", "trdec", ":", "\n", "      ", "y_total_count", ",", "y_rule_count", ",", "y_word_count", ",", "y_eos_count", "=", "y_count", "\n", "valid_total", "+=", "(", "y_total_count", "-", "batch_size", ")", "\n", "valid_rule_count", "+=", "y_rule_count", "\n", "valid_word_count", "+=", "(", "y_word_count", "-", "batch_size", ")", "\n", "valid_eos_count", "+=", "y_eos_count", "\n", "", "else", ":", "\n", "      ", "y_count", "-=", "batch_size", "\n", "# word count", "\n", "valid_words", "+=", "y_count", "\n", "\n", "", "if", "args", ".", "trdec", ":", "\n", "      ", "logits", "=", "model", ".", "forward", "(", "\n", "x_valid", ",", "x_mask", ",", "x_len", ",", "\n", "y_valid", "[", ":", ",", ":", "-", "1", ",", ":", "]", ",", "y_mask", "[", ":", ",", ":", "-", "1", "]", ",", "y_len", ",", "y_valid", "[", ":", ",", "1", ":", ",", "2", "]", ")", "\n", "logits", "=", "logits", ".", "view", "(", "-", "1", ",", "hparams", ".", "target_word_vocab_size", "+", "hparams", ".", "target_rule_vocab_size", ")", "\n", "labels", "=", "y_valid", "[", ":", ",", "1", ":", ",", "0", "]", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ")", "\n", "#print(\"x_valid\", x_valid)", "\n", "#print(\"x_mask\", x_mask)", "\n", "#print(\"x_len\", x_len)", "\n", "#print(\"y_valid\", y_valid)", "\n", "#print(\"y_mask\", y_mask)", "\n", "#print(tr_logits)", "\n", "#print(logits)", "\n", "#diff = (tr_logits - logits).sum()", "\n", "#print('diff: ', diff)", "\n", "#exit(0)", "\n", "val_loss", ",", "val_acc", ",", "rule_loss", ",", "word_loss", ",", "eos_loss", ",", "rule_count", ",", "word_count", ",", "eos_count", "=", "get_performance", "(", "crit", ",", "logits", ",", "labels", ",", "hparams", ")", "\n", "valid_word_loss", "+=", "word_loss", ".", "item", "(", ")", "\n", "valid_rule_loss", "+=", "rule_loss", ".", "item", "(", ")", "\n", "valid_eos_loss", "+=", "eos_loss", ".", "item", "(", ")", "\n", "", "else", ":", "\n", "      ", "logits", "=", "model", ".", "forward", "(", "\n", "x_valid", ",", "x_mask", ",", "x_len", ",", "\n", "y_valid", "[", ":", ",", ":", "-", "1", "]", ",", "y_mask", "[", ":", ",", ":", "-", "1", "]", ",", "y_len", ")", "\n", "logits", "=", "logits", ".", "view", "(", "-", "1", ",", "hparams", ".", "target_vocab_size", ")", "\n", "labels", "=", "y_valid", "[", ":", ",", "1", ":", "]", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ")", "\n", "val_loss", ",", "val_acc", "=", "get_performance", "(", "crit", ",", "logits", ",", "labels", ",", "hparams", ")", "\n", "", "n_batches", "+=", "1", "\n", "valid_loss", "+=", "val_loss", ".", "item", "(", ")", "\n", "valid_acc", "+=", "val_acc", ".", "item", "(", ")", "\n", "# print(\"{0:<5d} / {1:<5d}\".format(val_acc.data[0], y_count))", "\n", "if", "end_of_epoch", ":", "\n", "      ", "break", "\n", "# BLEU eval", "\n", "", "", "if", "eval_bleu", ":", "\n", "    ", "x_valid", "=", "data", ".", "x_valid", ".", "tolist", "(", ")", "\n", "#print(x_valid)", "\n", "#x_valid = Variable(torch.LongTensor(x_valid), volatile=True)", "\n", "if", "args", ".", "trdec", ":", "\n", "      ", "hyps", ",", "scores", "=", "model", ".", "translate", "(", "\n", "x_valid", ",", "target_rule_vocab", "=", "data", ".", "target_tree_vocab", ",", "\n", "beam_size", "=", "args", ".", "beam_size", ",", "max_len", "=", "args", ".", "max_trans_len", ",", "y_label", "=", "None", ",", "poly_norm_m", "=", "args", ".", "poly_norm_m", ")", "\n", "", "else", ":", "\n", "      ", "hyps", "=", "model", ".", "translate", "(", "\n", "x_valid", ",", "beam_size", "=", "args", ".", "beam_size", ",", "max_len", "=", "args", ".", "max_trans_len", ",", "poly_norm_m", "=", "args", ".", "poly_norm_m", ")", "\n", "", "for", "h", "in", "hyps", ":", "\n", "      ", "if", "args", ".", "trdec", ":", "\n", "        ", "deriv", "=", "[", "]", "\n", "for", "w", "in", "h", ":", "\n", "          ", "if", "w", "<", "data", ".", "target_word_vocab_size", ":", "\n", "            ", "deriv", ".", "append", "(", "[", "data", ".", "target_word_vocab", "[", "w", "]", ",", "False", "]", ")", "\n", "", "else", ":", "\n", "            ", "deriv", ".", "append", "(", "[", "data", ".", "target_tree_vocab", "[", "w", "]", ",", "False", "]", ")", "\n", "", "", "tree", "=", "Tree", ".", "from_rule_deriv", "(", "deriv", ")", "\n", "line", "=", "tree", ".", "to_string", "(", ")", "\n", "if", "hparams", ".", "merge_bpe", ":", "\n", "          ", "line", "=", "line", ".", "replace", "(", "' '", ",", "''", ")", "\n", "line", "=", "line", ".", "replace", "(", "'\u2581'", ",", "' '", ")", ".", "strip", "(", ")", "\n", "", "out_file", ".", "write", "(", "line", "+", "'\\n'", ")", "\n", "out_file", ".", "flush", "(", ")", "\n", "out_parse_file", ".", "write", "(", "tree", ".", "to_parse_string", "(", ")", "+", "'\\n'", ")", "\n", "out_parse_file", ".", "flush", "(", ")", "\n", "", "else", ":", "\n", "        ", "h_best_words", "=", "map", "(", "lambda", "wi", ":", "data", ".", "target_index_to_word", "[", "wi", "]", ",", "\n", "filter", "(", "lambda", "wi", ":", "wi", "not", "in", "hparams", ".", "filtered_tokens", ",", "h", ")", ")", "\n", "if", "hparams", ".", "merge_bpe", ":", "\n", "          ", "line", "=", "''", ".", "join", "(", "h_best_words", ")", "\n", "line", "=", "line", ".", "replace", "(", "'\u2581'", ",", "' '", ")", "\n", "", "else", ":", "\n", "          ", "line", "=", "' '", ".", "join", "(", "h_best_words", ")", "\n", "", "line", "=", "line", ".", "strip", "(", ")", "\n", "out_file", ".", "write", "(", "line", "+", "'\\n'", ")", "\n", "out_file", ".", "flush", "(", ")", "\n", "", "", "", "if", "args", ".", "trdec", ":", "\n", "    ", "val_ppl", "=", "np", ".", "exp", "(", "valid_loss", "/", "valid_word_count", ")", "\n", "log_string", "=", "\"val_step={0:<6d}\"", ".", "format", "(", "step", ")", "\n", "log_string", "+=", "\" loss={0:<6.2f}\"", ".", "format", "(", "valid_loss", "/", "valid_word_count", ")", "\n", "log_string", "+=", "\" acc={0:<5.4f}\"", ".", "format", "(", "valid_acc", "/", "valid_total", ")", "\n", "log_string", "+=", "\" val_ppl={0:<.2f}\"", ".", "format", "(", "val_ppl", ")", "\n", "log_string", "+=", "\" num_word={} num_rule={} num_eos={}\"", ".", "format", "(", "valid_word_count", ",", "valid_rule_count", ",", "valid_eos_count", ")", "\n", "log_string", "+=", "\" ppl_word={0:<8.2f}\"", ".", "format", "(", "np", ".", "exp", "(", "valid_word_loss", "/", "valid_word_count", ")", ")", "\n", "log_string", "+=", "\" ppl_rule={0:<8.2f}\"", ".", "format", "(", "np", ".", "exp", "(", "valid_rule_loss", "/", "valid_rule_count", ")", ")", "\n", "if", "not", "args", ".", "no_piece_tree", ":", "\n", "      ", "log_string", "+=", "\" ppl_eos={0:<8.2f}\"", ".", "format", "(", "np", ".", "exp", "(", "valid_eos_loss", "/", "valid_eos_count", ")", ")", "\n", "", "", "else", ":", "\n", "    ", "val_ppl", "=", "np", ".", "exp", "(", "valid_loss", "/", "valid_words", ")", "\n", "log_string", "=", "\"val_step={0:<6d}\"", ".", "format", "(", "step", ")", "\n", "log_string", "+=", "\" loss={0:<6.2f}\"", ".", "format", "(", "valid_loss", "/", "valid_words", ")", "\n", "log_string", "+=", "\" acc={0:<5.4f}\"", ".", "format", "(", "valid_acc", "/", "valid_words", ")", "\n", "log_string", "+=", "\" val_ppl={0:<.2f}\"", ".", "format", "(", "val_ppl", ")", "\n", "", "if", "eval_bleu", ":", "\n", "    ", "out_file", ".", "close", "(", ")", "\n", "if", "args", ".", "trdec", ":", "\n", "      ", "out_parse_file", ".", "close", "(", ")", "\n", "", "if", "args", ".", "target_valid_ref", ":", "\n", "      ", "ref_file", "=", "os", ".", "path", ".", "join", "(", "hparams", ".", "data_path", ",", "args", ".", "target_valid_ref", ")", "\n", "", "else", ":", "\n", "      ", "ref_file", "=", "os", ".", "path", ".", "join", "(", "hparams", ".", "data_path", ",", "args", ".", "target_valid", ")", "\n", "", "bleu_str", "=", "subprocess", ".", "getoutput", "(", "\n", "\"./multi-bleu.perl {0} < {1}\"", ".", "format", "(", "ref_file", ",", "valid_hyp_file", ")", ")", "\n", "log_string", "+=", "\"\\n{}\"", ".", "format", "(", "bleu_str", ")", "\n", "bleu_str", "=", "bleu_str", ".", "split", "(", "'\\n'", ")", "[", "-", "1", "]", ".", "strip", "(", ")", "\n", "reg", "=", "re", ".", "compile", "(", "\"BLEU = ([^,]*).*\"", ")", "\n", "try", ":", "\n", "      ", "valid_bleu", "=", "float", "(", "reg", ".", "match", "(", "bleu_str", ")", ".", "group", "(", "1", ")", ")", "\n", "", "except", ":", "\n", "      ", "valid_bleu", "=", "0.", "\n", "", "log_string", "+=", "\" val_bleu={0:<.2f}\"", ".", "format", "(", "valid_bleu", ")", "\n", "", "print", "(", "log_string", ")", "\n", "model", ".", "train", "(", ")", "\n", "#exit(0)", "\n", "return", "val_ppl", ",", "valid_bleu", "\n", "\n"]], "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.main.train": [[277, 539], ["data_utils.DataLoader", "hparams.HParams.add_param", "hparams.HParams.add_param", "hparams.HParams.add_param", "hparams.HParams.add_param", "hparams.HParams.add_param", "hparams.HParams.add_param", "print", "print", "utils.get_criterion", "utils.count_params", "print", "print", "print", "time.time", "trdec.TrDec.train", "print", "os.path.join", "torch.load", "torch.load", "hparams.HParams", "hparams.HParams.add_param", "hparams.HParams.add_param", "hparams.HParams.add_param", "os.path.join", "print", "torch.load", "torch.load", "os.path.join", "print", "torch.optim.Adam", "torch.optim.Adam", "torch.load", "torch.load", "torch.optim.Adam.load_state_dict", "os.path.join", "torch.load", "torch.load", "torch.optim.Adam", "torch.optim.Adam", "data_utils.DataLoader.next_train", "torch.optim.Adam.zero_grad", "tr_loss.item", "tr_acc.item", "torch.nn.utils.clip_grad_norm", "torch.nn.utils.clip_grad_norm", "torch.optim.Adam.step", "models.Seq2Seq", "print", "trdec.TrDec.parameters", "trdec.TrDec.parameters", "trdec.TrDec.forward", "logits.view.view", "y_train[].contiguous().view", "utils.get_performance", "word_loss.item", "rule_loss.item", "eos_loss.item", "trdec.TrDec.forward", "logits.view.view", "y_train[].contiguous().view", "utils.get_performance", "rule_loss.div_", "rule_loss.backward", "trdec.TrDec.parameters", "gc.collect", "time.time", "print", "main.eval", "time.time", "trdec.TrDec.parameters", "trdec_attn.TrDecAttn", "p.data.uniform_", "trdec.TrDec.parameters", "rule_count.item", "rule_count.item", "eos_count.item", "eos_count.item", "word_count.item", "word_count.item", "word_loss.div_", "word_loss.backward", "tr_loss.div_", "tr_loss.backward", "tr_loss.item", "utils.save_checkpoint", "utils.set_lr", "trdec_attn_v1.TrDecAttn_v1", "y_train[].contiguous", "y_train[].contiguous", "numpy.exp", "numpy.exp", "numpy.exp", "numpy.exp", "trdec_single.TrDecSingle", "trdec.TrDec", "numpy.exp"], "function", ["home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.hparams.HParams.add_param", "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.hparams.HParams.add_param", "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.hparams.HParams.add_param", "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.hparams.HParams.add_param", "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.hparams.HParams.add_param", "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.hparams.HParams.add_param", "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.utils.get_criterion", "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.utils.count_params", "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.main.train", "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.hparams.HParams.add_param", "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.hparams.HParams.add_param", "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.hparams.HParams.add_param", "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.data_utils.DataLoader.next_train", "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.trdec_single.TreeDecoderSingle.step", "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.trdec_single.TrDecSingle.forward", "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.utils.get_performance", "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.trdec_single.TrDecSingle.forward", "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.utils.get_performance", "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.main.eval", "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.utils.save_checkpoint", "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.utils.set_lr"], ["", "def", "train", "(", ")", ":", "\n", "  ", "if", "args", ".", "load_model", "and", "(", "not", "args", ".", "reset_hparams", ")", ":", "\n", "    ", "print", "(", "\"load hparams..\"", ")", "\n", "hparams_file_name", "=", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir", ",", "\"hparams.pt\"", ")", "\n", "hparams", "=", "torch", ".", "load", "(", "hparams_file_name", ")", "\n", "", "else", ":", "\n", "    ", "hparams", "=", "HParams", "(", "\n", "data_path", "=", "args", ".", "data_path", ",", "\n", "source_train", "=", "args", ".", "source_train", ",", "\n", "target_train", "=", "args", ".", "target_train", ",", "\n", "source_valid", "=", "args", ".", "source_valid", ",", "\n", "target_valid", "=", "args", ".", "target_valid", ",", "\n", "source_vocab", "=", "args", ".", "source_vocab", ",", "\n", "target_vocab", "=", "args", ".", "target_vocab", ",", "\n", "source_test", "=", "args", ".", "source_test", ",", "\n", "target_test", "=", "args", ".", "target_test", ",", "\n", "max_len", "=", "args", ".", "max_len", ",", "\n", "max_tree_len", "=", "args", ".", "max_tree_len", ",", "\n", "n_train_sents", "=", "args", ".", "n_train_sents", ",", "\n", "cuda", "=", "args", ".", "cuda", ",", "\n", "d_word_vec", "=", "args", ".", "d_word_vec", ",", "\n", "d_model", "=", "args", ".", "d_model", ",", "\n", "batch_size", "=", "args", ".", "batch_size", ",", "\n", "batcher", "=", "args", ".", "batcher", ",", "\n", "n_train_steps", "=", "args", ".", "n_train_steps", ",", "\n", "dropout", "=", "args", ".", "dropout", ",", "\n", "lr", "=", "args", ".", "lr", ",", "\n", "lr_dec", "=", "args", ".", "lr_dec", ",", "\n", "l2_reg", "=", "args", ".", "l2_reg", ",", "\n", "init_type", "=", "args", ".", "init_type", ",", "\n", "init_range", "=", "args", ".", "init_range", ",", "\n", "trdec", "=", "args", ".", "trdec", ",", "\n", "target_tree_vocab", "=", "args", ".", "target_tree_vocab", ",", "\n", "target_word_vocab", "=", "args", ".", "target_word_vocab", ",", "\n", "target_tree_train", "=", "args", ".", "target_tree_train", ",", "\n", "target_tree_valid", "=", "args", ".", "target_tree_valid", ",", "\n", "target_tree_test", "=", "args", ".", "target_tree_test", ",", "\n", "max_tree_depth", "=", "args", ".", "max_tree_depth", ",", "\n", "parent_feed", "=", "args", ".", "parent_feed", ",", "\n", "rule_parent_feed", "=", "args", ".", "rule_parent_feed", ",", "\n", "label_smooth", "=", "args", ".", "label_smooth", ",", "\n", "raml_rule", "=", "args", ".", "raml_rule", ",", "\n", "raml_tau", "=", "args", ".", "raml_tau", ",", "\n", "no_lhs", "=", "args", ".", "no_lhs", ",", "\n", "root_label", "=", "args", ".", "root_label", ",", "\n", "single_readout", "=", "args", ".", "single_readout", ",", "\n", "single_attn", "=", "args", ".", "single_attn", ",", "\n", "pos", "=", "args", ".", "pos", ",", "\n", "share_emb_softmax", "=", "args", ".", "share_emb_softmax", ",", "\n", "attn", "=", "args", ".", "attn", ",", "\n", "self_attn", "=", "args", ".", "self_attn", ",", "\n", "no_word_to_rule", "=", "args", ".", "no_word_to_rule", ",", "\n", "single_inp_readout", "=", "args", ".", "single_inp_readout", ",", "\n", "rule_tanh", "=", "args", ".", "rule_tanh", ",", "\n", "n_heads", "=", "args", ".", "n_heads", ",", "\n", "d_k", "=", "args", ".", "d_k", ",", "\n", "d_v", "=", "args", ".", "d_v", ",", "\n", "residue", "=", "args", ".", "residue", ",", "\n", "layer_norm", "=", "args", ".", "layer_norm", ",", "\n", "no_piece_tree", "=", "args", ".", "no_piece_tree", ",", "\n", "self_attn_input_feed", "=", "args", ".", "self_attn_input_feed", ",", "\n", "trdec_attn_v1", "=", "args", ".", "trdec_attn_v1", ",", "\n", "merge_bpe", "=", "args", ".", "merge_bpe", ",", "\n", "ignore_rule_len", "=", "False", ",", "\n", "nbest", "=", "False", ",", "\n", "force_rule", "=", "True", ",", "\n", "force_rule_step", "=", "1", ",", "\n", ")", "\n", "", "data", "=", "DataLoader", "(", "hparams", "=", "hparams", ")", "\n", "hparams", ".", "add_param", "(", "\"source_vocab_size\"", ",", "data", ".", "source_vocab_size", ")", "\n", "if", "args", ".", "trdec", ":", "\n", "    ", "hparams", ".", "add_param", "(", "\"target_rule_vocab_size\"", ",", "data", ".", "target_rule_vocab_size", ")", "\n", "hparams", ".", "add_param", "(", "\"target_word_vocab_size\"", ",", "data", ".", "target_word_vocab_size", ")", "\n", "", "else", ":", "\n", "    ", "hparams", ".", "add_param", "(", "\"target_vocab_size\"", ",", "data", ".", "target_vocab_size", ")", "\n", "", "hparams", ".", "add_param", "(", "\"pad_id\"", ",", "data", ".", "pad_id", ")", "\n", "hparams", ".", "add_param", "(", "\"unk_id\"", ",", "data", ".", "unk_id", ")", "\n", "hparams", ".", "add_param", "(", "\"bos_id\"", ",", "data", ".", "bos_id", ")", "\n", "hparams", ".", "add_param", "(", "\"eos_id\"", ",", "data", ".", "eos_id", ")", "\n", "hparams", ".", "add_param", "(", "\"n_train_steps\"", ",", "args", ".", "n_train_steps", ")", "\n", "\n", "# build or load model model", "\n", "print", "(", "\"-\"", "*", "80", ")", "\n", "print", "(", "\"Creating model\"", ")", "\n", "if", "args", ".", "load_model", ":", "\n", "    ", "model_file_name", "=", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir", ",", "\"model.pt\"", ")", "\n", "print", "(", "\"Loading model from '{0}'\"", ".", "format", "(", "model_file_name", ")", ")", "\n", "model", "=", "torch", ".", "load", "(", "model_file_name", ")", "\n", "\n", "optim_file_name", "=", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir", ",", "\"optimizer.pt\"", ")", "\n", "print", "(", "\"Loading optimizer from {}\"", ".", "format", "(", "optim_file_name", ")", ")", "\n", "trainable_params", "=", "[", "\n", "p", "for", "p", "in", "model", ".", "parameters", "(", ")", "if", "p", ".", "requires_grad", "]", "\n", "optim", "=", "torch", ".", "optim", ".", "Adam", "(", "trainable_params", ",", "lr", "=", "hparams", ".", "lr", ",", "weight_decay", "=", "hparams", ".", "l2_reg", ")", "\n", "optimizer_state", "=", "torch", ".", "load", "(", "optim_file_name", ")", "\n", "optim", ".", "load_state_dict", "(", "optimizer_state", ")", "\n", "\n", "extra_file_name", "=", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir", ",", "\"extra.pt\"", ")", "\n", "step", ",", "best_val_ppl", ",", "best_val_bleu", ",", "cur_attempt", ",", "lr", "=", "torch", ".", "load", "(", "extra_file_name", ")", "\n", "", "else", ":", "\n", "    ", "if", "args", ".", "trdec", ":", "\n", "      ", "if", "args", ".", "trdec_attn", ":", "\n", "        ", "model", "=", "TrDecAttn", "(", "hparams", "=", "hparams", ")", "\n", "", "elif", "args", ".", "trdec_attn_v1", ":", "\n", "        ", "model", "=", "TrDecAttn_v1", "(", "hparams", "=", "hparams", ")", "\n", "", "elif", "args", ".", "trdec_single", ":", "\n", "        ", "model", "=", "TrDecSingle", "(", "hparams", "=", "hparams", ")", "\n", "", "else", ":", "\n", "        ", "model", "=", "TrDec", "(", "hparams", "=", "hparams", ")", "\n", "", "", "else", ":", "\n", "      ", "model", "=", "Seq2Seq", "(", "hparams", "=", "hparams", ")", "\n", "", "if", "args", ".", "init_type", "==", "\"uniform\"", ":", "\n", "      ", "print", "(", "\"initialize uniform with range {}\"", ".", "format", "(", "args", ".", "init_range", ")", ")", "\n", "for", "p", "in", "model", ".", "parameters", "(", ")", ":", "\n", "        ", "p", ".", "data", ".", "uniform_", "(", "-", "args", ".", "init_range", ",", "args", ".", "init_range", ")", "\n", "", "", "trainable_params", "=", "[", "\n", "p", "for", "p", "in", "model", ".", "parameters", "(", ")", "if", "p", ".", "requires_grad", "]", "\n", "optim", "=", "torch", ".", "optim", ".", "Adam", "(", "trainable_params", ",", "lr", "=", "hparams", ".", "lr", ",", "weight_decay", "=", "hparams", ".", "l2_reg", ")", "\n", "#optim = torch.optim.Adam(trainable_params)", "\n", "step", "=", "0", "\n", "best_val_ppl", "=", "1e10", "\n", "best_val_bleu", "=", "0", "\n", "cur_attempt", "=", "0", "\n", "lr", "=", "hparams", ".", "lr", "\n", "", "if", "args", ".", "reset_hparams", ":", "\n", "    ", "lr", "=", "args", ".", "lr", "\n", "", "crit", "=", "get_criterion", "(", "hparams", ")", "\n", "trainable_params", "=", "[", "\n", "p", "for", "p", "in", "model", ".", "parameters", "(", ")", "if", "p", ".", "requires_grad", "]", "\n", "num_params", "=", "count_params", "(", "trainable_params", ")", "\n", "print", "(", "\"Model has {0} params\"", ".", "format", "(", "num_params", ")", ")", "\n", "\n", "print", "(", "\"-\"", "*", "80", ")", "\n", "print", "(", "\"start training...\"", ")", "\n", "start_time", "=", "log_start_time", "=", "time", ".", "time", "(", ")", "\n", "target_words", ",", "total_loss", ",", "total_corrects", "=", "0", ",", "0", ",", "0", "\n", "target_rules", ",", "target_total", ",", "target_eos", "=", "0", ",", "0", ",", "0", "\n", "total_word_loss", ",", "total_rule_loss", ",", "total_eos_loss", "=", "0", ",", "0", ",", "0", "\n", "model", ".", "train", "(", ")", "\n", "#i = 0", "\n", "while", "True", ":", "\n", "    ", "(", "(", "x_train", ",", "x_mask", ",", "x_len", ",", "x_count", ")", ",", "\n", "(", "y_train", ",", "y_mask", ",", "y_len", ",", "y_count", ")", ",", "\n", "batch_size", ")", "=", "data", ".", "next_train", "(", ")", "\n", "#print(\"x_train\", x_train.size())", "\n", "#print(\"y_train\", y_train.size())", "\n", "#print(i)", "\n", "#i += 1", "\n", "#print(\"x_train\", x_train)", "\n", "#print(\"x_mask\", x_mask)", "\n", "#print(\"x_len\", x_len)", "\n", "#print(\"y_train\", y_train)", "\n", "#print(\"y_mask\", y_mask)", "\n", "#exit(0)", "\n", "optim", ".", "zero_grad", "(", ")", "\n", "if", "args", ".", "trdec", ":", "\n", "      ", "y_total_count", ",", "y_rule_count", ",", "y_word_count", ",", "y_eos_count", "=", "y_count", "\n", "target_total", "+=", "(", "y_total_count", "-", "batch_size", ")", "\n", "target_rules", "+=", "y_rule_count", "\n", "target_eos", "+=", "y_eos_count", "\n", "target_words", "+=", "(", "y_word_count", "-", "batch_size", ")", "\n", "\n", "logits", "=", "model", ".", "forward", "(", "x_train", ",", "x_mask", ",", "x_len", ",", "y_train", "[", ":", ",", ":", "-", "1", ",", ":", "]", ",", "y_mask", "[", ":", ",", ":", "-", "1", "]", ",", "y_len", ",", "y_train", "[", ":", ",", "1", ":", ",", "2", "]", ",", "y_label", "=", "y_train", "[", ":", ",", "1", ":", ",", "0", "]", ")", "\n", "logits", "=", "logits", ".", "view", "(", "-", "1", ",", "hparams", ".", "target_word_vocab_size", "+", "hparams", ".", "target_rule_vocab_size", ")", "\n", "labels", "=", "y_train", "[", ":", ",", "1", ":", ",", "0", "]", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ")", "\n", "#print(\"x_train_logits\", logits)", "\n", "#print(\"total:\", y_total_count, \"rule_count:\", y_rule_count, \"word_count:\", y_word_count, \"eos_count:\", y_eos_count)", "\n", "tr_loss", ",", "tr_acc", ",", "rule_loss", ",", "word_loss", ",", "eos_loss", ",", "rule_count", ",", "word_count", ",", "eos_count", "=", "get_performance", "(", "crit", ",", "logits", ",", "labels", ",", "hparams", ")", "\n", "#print(\"perform rule_count:\", rule_count, \"word_count:\", word_count, \"eos_count\", eos_count)", "\n", "#print((y_train[:,:,0] >= hparams.target_word_vocab_size).long().sum().data[0])", "\n", "#print(y_rule_count)", "\n", "#print(rule_count.data[0])", "\n", "assert", "y_rule_count", "==", "rule_count", ".", "item", "(", ")", ",", "\"data rule count {}, performance rule count {}\"", ".", "format", "(", "y_rule_count", ",", "rule_count", ".", "item", "(", ")", ")", "\n", "assert", "y_eos_count", "==", "eos_count", ".", "item", "(", ")", ",", "\"data eos count {}, performance eos count {}\"", ".", "format", "(", "y_eos_count", ",", "eos_count", ".", "item", "(", ")", ")", "\n", "assert", "y_word_count", "-", "batch_size", "==", "word_count", ".", "item", "(", ")", ",", "\"data word count {}, performance word count {}\"", ".", "format", "(", "y_word_count", "-", "batch_size", ",", "word_count", ".", "item", "(", ")", ")", "\n", "total_word_loss", "+=", "word_loss", ".", "item", "(", ")", "\n", "total_rule_loss", "+=", "rule_loss", ".", "item", "(", ")", "\n", "total_eos_loss", "+=", "eos_loss", ".", "item", "(", ")", "\n", "", "else", ":", "\n", "      ", "target_words", "+=", "(", "y_count", "-", "batch_size", ")", "\n", "\n", "logits", "=", "model", ".", "forward", "(", "x_train", ",", "x_mask", ",", "x_len", ",", "y_train", "[", ":", ",", ":", "-", "1", "]", ",", "y_mask", "[", ":", ",", ":", "-", "1", "]", ",", "y_len", ")", "\n", "logits", "=", "logits", ".", "view", "(", "-", "1", ",", "hparams", ".", "target_vocab_size", ")", "\n", "labels", "=", "y_train", "[", ":", ",", "1", ":", "]", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ")", "\n", "tr_loss", ",", "tr_acc", "=", "get_performance", "(", "crit", ",", "logits", ",", "labels", ",", "hparams", ")", "\n", "", "total_loss", "+=", "tr_loss", ".", "item", "(", ")", "\n", "total_corrects", "+=", "tr_acc", ".", "item", "(", ")", "\n", "step", "+=", "1", "\n", "if", "args", ".", "trdec", "and", "args", ".", "loss_type", "==", "\"rule\"", ":", "\n", "      ", "rule_loss", ".", "div_", "(", "batch_size", ")", "\n", "rule_loss", ".", "backward", "(", ")", "\n", "", "elif", "args", ".", "trdec", "and", "args", ".", "loss_type", "==", "\"word\"", ":", "\n", "      ", "word_loss", ".", "div_", "(", "batch_size", ")", "\n", "word_loss", ".", "backward", "(", ")", "\n", "", "else", ":", "\n", "      ", "tr_loss", ".", "div_", "(", "batch_size", ")", "\n", "tr_loss", ".", "backward", "(", ")", "\n", "", "grad_norm", "=", "torch", ".", "nn", ".", "utils", ".", "clip_grad_norm", "(", "model", ".", "parameters", "(", ")", ",", "args", ".", "clip_grad", ")", "\n", "optim", ".", "step", "(", ")", "\n", "# clean up GPU memory", "\n", "if", "step", "%", "args", ".", "clean_mem_every", "==", "0", ":", "\n", "      ", "gc", ".", "collect", "(", ")", "\n", "", "if", "step", "%", "args", ".", "log_every", "==", "0", ":", "\n", "      ", "epoch", "=", "step", "//", "data", ".", "n_train_batches", "\n", "curr_time", "=", "time", ".", "time", "(", ")", "\n", "since_start", "=", "(", "curr_time", "-", "start_time", ")", "/", "60.0", "\n", "elapsed", "=", "(", "curr_time", "-", "log_start_time", ")", "/", "60.0", "\n", "log_string", "=", "\"ep={0:<3d}\"", ".", "format", "(", "epoch", ")", "\n", "log_string", "+=", "\" steps={0:<6.2f}\"", ".", "format", "(", "step", "/", "1000", ")", "\n", "log_string", "+=", "\" lr={0:<9.7f}\"", ".", "format", "(", "lr", ")", "\n", "log_string", "+=", "\" loss={0:<7.2f}\"", ".", "format", "(", "tr_loss", ".", "item", "(", ")", ")", "\n", "log_string", "+=", "\" |g|={0:<5.2f}\"", ".", "format", "(", "grad_norm", ")", "\n", "if", "args", ".", "trdec", ":", "\n", "        ", "log_string", "+=", "\" num_word={} num_rule={} num_eos={}\"", ".", "format", "(", "target_words", ",", "target_rules", ",", "target_eos", ")", "\n", "log_string", "+=", "\" ppl={0:<8.2f}\"", ".", "format", "(", "np", ".", "exp", "(", "total_loss", "/", "target_words", ")", ")", "\n", "log_string", "+=", "\" ppl_word={0:<8.2f}\"", ".", "format", "(", "np", ".", "exp", "(", "total_word_loss", "/", "target_words", ")", ")", "\n", "log_string", "+=", "\" ppl_rule={0:<8.2f}\"", ".", "format", "(", "np", ".", "exp", "(", "total_rule_loss", "/", "target_rules", ")", ")", "\n", "if", "not", "args", ".", "no_piece_tree", ":", "\n", "          ", "log_string", "+=", "\" ppl_eos={0:<8.2f}\"", ".", "format", "(", "np", ".", "exp", "(", "total_eos_loss", "/", "target_eos", ")", ")", "\n", "", "log_string", "+=", "\" acc={0:<5.4f}\"", ".", "format", "(", "total_corrects", "/", "target_total", ")", "\n", "", "else", ":", "\n", "        ", "log_string", "+=", "\" ppl={0:<8.2f}\"", ".", "format", "(", "np", ".", "exp", "(", "total_loss", "/", "target_words", ")", ")", "\n", "log_string", "+=", "\" acc={0:<5.4f}\"", ".", "format", "(", "total_corrects", "/", "target_words", ")", "\n", "\n", "", "log_string", "+=", "\" wpm(k)={0:<5.2f}\"", ".", "format", "(", "target_words", "/", "(", "1000", "*", "elapsed", ")", ")", "\n", "log_string", "+=", "\" time(min)={0:<5.2f}\"", ".", "format", "(", "since_start", ")", "\n", "print", "(", "log_string", ")", "\n", "", "if", "step", "%", "args", ".", "eval_every", "==", "0", ":", "\n", "      ", "based_on_bleu", "=", "args", ".", "eval_bleu", "and", "best_val_ppl", "<=", "args", ".", "ppl_thresh", "\n", "val_ppl", ",", "val_bleu", "=", "eval", "(", "model", ",", "data", ",", "crit", ",", "step", ",", "hparams", ",", "eval_bleu", "=", "based_on_bleu", ",", "valid_batch_size", "=", "args", ".", "valid_batch_size", ",", "tr_logits", "=", "logits", ")", "\n", "if", "based_on_bleu", ":", "\n", "        ", "if", "best_val_bleu", "<=", "val_bleu", ":", "\n", "          ", "save", "=", "True", "\n", "best_val_bleu", "=", "val_bleu", "\n", "cur_attempt", "=", "0", "\n", "", "else", ":", "\n", "          ", "save", "=", "False", "\n", "cur_attempt", "+=", "1", "\n", "", "", "else", ":", "\n", "      \t", "if", "best_val_ppl", ">=", "val_ppl", ":", "\n", "          ", "save", "=", "True", "\n", "best_val_ppl", "=", "val_ppl", "\n", "cur_attempt", "=", "0", "\n", "", "else", ":", "\n", "          ", "save", "=", "False", "\n", "cur_attempt", "+=", "1", "\n", "", "", "if", "save", ":", "\n", "      \t", "save_checkpoint", "(", "[", "step", ",", "best_val_ppl", ",", "best_val_bleu", ",", "cur_attempt", ",", "lr", "]", ",", "\n", "model", ",", "optim", ",", "hparams", ",", "args", ".", "output_dir", ")", "\n", "", "else", ":", "\n", "        ", "lr", "=", "lr", "*", "args", ".", "lr_dec", "\n", "set_lr", "(", "optim", ",", "lr", ")", "\n", "# reset counter after eval", "\n", "", "log_start_time", "=", "time", ".", "time", "(", ")", "\n", "target_words", "=", "total_corrects", "=", "total_loss", "=", "0", "\n", "target_rules", "=", "target_total", "=", "target_eos", "=", "0", "\n", "total_word_loss", "=", "total_rule_loss", "=", "total_eos_loss", "=", "0", "\n", "", "if", "args", ".", "patience", ">=", "0", ":", "\n", "      ", "if", "cur_attempt", ">", "args", ".", "patience", ":", "break", "\n", "", "else", ":", "\n", "      ", "if", "step", ">", "args", ".", "n_train_steps", ":", "break", "\n", "\n"]], "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.main.main": [[540, 561], ["random.seed", "numpy.random.seed", "torch.manual_seed", "torch.manual_seed", "torch.cuda.manual_seed_all", "torch.cuda.manual_seed_all", "print", "os.path.join", "print", "utils.Logger", "main.train", "os.path.isdir", "print", "print", "os.makedirs", "print", "print", "shutil.rmtree", "os.makedirs"], "function", ["home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.main.train"], ["", "", "", "def", "main", "(", ")", ":", "\n", "  ", "random", ".", "seed", "(", "args", ".", "seed", ")", "\n", "np", ".", "random", ".", "seed", "(", "args", ".", "seed", ")", "\n", "torch", ".", "manual_seed", "(", "args", ".", "seed", ")", "\n", "torch", ".", "cuda", ".", "manual_seed_all", "(", "args", ".", "seed", ")", "\n", "\n", "if", "not", "os", ".", "path", ".", "isdir", "(", "args", ".", "output_dir", ")", ":", "\n", "    ", "print", "(", "\"-\"", "*", "80", ")", "\n", "print", "(", "\"Path {} does not exist. Creating.\"", ".", "format", "(", "args", ".", "output_dir", ")", ")", "\n", "os", ".", "makedirs", "(", "args", ".", "output_dir", ")", "\n", "", "elif", "args", ".", "reset_output_dir", ":", "\n", "    ", "print", "(", "\"-\"", "*", "80", ")", "\n", "print", "(", "\"Path {} exists. Remove and remake.\"", ".", "format", "(", "args", ".", "output_dir", ")", ")", "\n", "shutil", ".", "rmtree", "(", "args", ".", "output_dir", ")", "\n", "os", ".", "makedirs", "(", "args", ".", "output_dir", ")", "\n", "\n", "", "print", "(", "\"-\"", "*", "80", ")", "\n", "log_file", "=", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir", ",", "\"stdout\"", ")", "\n", "print", "(", "\"Logging to {}\"", ".", "format", "(", "log_file", ")", ")", "\n", "sys", ".", "stdout", "=", "Logger", "(", "log_file", ")", "\n", "train", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.data_utils.DataLoader.__init__": [[21, 94], ["print", "print", "data_utils.DataLoader._build_vocab", "data_utils.DataLoader._build_tree_vocab", "len", "len", "data_utils.DataLoader._build_vocab", "len", "data_utils.DataLoader.reset_test", "len", "data_utils.DataLoader.reset_train", "len", "data_utils.DataLoader.reset_valid", "data_utils.DataLoader._build_tree_parallel", "data_utils.DataLoader._build_parallel", "data_utils.DataLoader._build_tree_parallel", "data_utils.DataLoader._build_parallel", "data_utils.DataLoader._build_tree_parallel", "data_utils.DataLoader._build_parallel"], "methods", ["home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.data_utils.DataLoader._build_vocab", "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.data_utils.DataLoader._build_tree_vocab", "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.data_utils.DataLoader._build_vocab", "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.data_utils.DataLoader.reset_test", "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.data_utils.DataLoader.reset_train", "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.data_utils.DataLoader.reset_valid", "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.data_utils.DataLoader._build_tree_parallel", "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.data_utils.DataLoader._build_parallel", "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.data_utils.DataLoader._build_tree_parallel", "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.data_utils.DataLoader._build_parallel", "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.data_utils.DataLoader._build_tree_parallel", "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.data_utils.DataLoader._build_parallel"], ["  ", "def", "__init__", "(", "self", ",", "hparams", ",", "decode", "=", "False", ")", ":", "\n", "    ", "\"\"\"Encloses both train and valid data.\n\n    Args:\n      hparams: must be ['tiny' 'bpe16' 'bpe32']\n    \"\"\"", "\n", "\n", "self", ".", "hparams", "=", "hparams", "\n", "self", ".", "decode", "=", "decode", "\n", "\n", "print", "(", "\"-\"", "*", "80", ")", "\n", "print", "(", "\"Building data for '{0}' from '{1}'\"", ".", "format", "(", "\n", "self", ".", "hparams", ".", "dataset", ",", "self", ".", "hparams", ".", "data_path", ")", ")", "\n", "\n", "# vocab", "\n", "(", "self", ".", "source_word_to_index", ",", "\n", "self", ".", "source_index_to_word", ",", "self", ".", "source_vocab_size", ")", "=", "self", ".", "_build_vocab", "(", "self", ".", "hparams", ".", "source_vocab", ")", "\n", "#self.source_vocab_size = len(self.source_word_to_index)", "\n", "\n", "if", "self", ".", "hparams", ".", "trdec", ":", "\n", "      ", "self", ".", "target_tree_vocab", ",", "self", ".", "target_word_vocab", "=", "self", ".", "_build_tree_vocab", "(", "self", ".", "hparams", ".", "target_tree_vocab", ",", "\n", "self", ".", "hparams", ".", "target_word_vocab", ")", "\n", "self", ".", "target_rule_vocab_size", "=", "len", "(", "self", ".", "target_tree_vocab", ")", "\n", "self", ".", "target_word_vocab_size", "=", "len", "(", "self", ".", "target_word_vocab", ")", "\n", "", "else", ":", "\n", "      ", "(", "self", ".", "target_word_to_index", ",", "\n", "self", ".", "target_index_to_word", ",", "self", ".", "target_vocab_size", ")", "=", "self", ".", "_build_vocab", "(", "self", ".", "hparams", ".", "target_vocab", ")", "\n", "#self.target_vocab_size = len(self.target_word_to_index)", "\n", "\n", "", "if", "not", "self", ".", "hparams", ".", "trdec", ":", "\n", "      ", "assert", "self", ".", "source_word_to_index", "[", "self", ".", "hparams", ".", "pad", "]", "==", "self", ".", "target_word_to_index", "[", "self", ".", "hparams", ".", "pad", "]", "\n", "assert", "self", ".", "source_word_to_index", "[", "self", ".", "hparams", ".", "unk", "]", "==", "self", ".", "target_word_to_index", "[", "self", ".", "hparams", ".", "unk", "]", "\n", "assert", "self", ".", "source_word_to_index", "[", "self", ".", "hparams", ".", "bos", "]", "==", "self", ".", "target_word_to_index", "[", "self", ".", "hparams", ".", "bos", "]", "\n", "assert", "self", ".", "source_word_to_index", "[", "self", ".", "hparams", ".", "eos", "]", "==", "self", ".", "target_word_to_index", "[", "self", ".", "hparams", ".", "eos", "]", "\n", "\n", "", "if", "self", ".", "decode", ":", "\n", "      ", "if", "self", ".", "hparams", ".", "trdec", ":", "\n", "        ", "self", ".", "x_test", ",", "self", ".", "y_word_test", ",", "self", ".", "y_test", "=", "self", ".", "_build_tree_parallel", "(", "\n", "self", ".", "hparams", ".", "source_test", ",", "self", ".", "hparams", ".", "target_test", ",", "self", ".", "hparams", ".", "target_tree_test", ",", "\n", "is_training", "=", "False", ")", "\n", "", "else", ":", "\n", "        ", "self", ".", "x_test", ",", "self", ".", "y_test", "=", "self", ".", "_build_parallel", "(", "\n", "self", ".", "hparams", ".", "source_test", ",", "self", ".", "hparams", ".", "target_test", ",", "is_training", "=", "False", ")", "\n", "", "self", ".", "test_size", "=", "len", "(", "self", ".", "x_test", ")", "\n", "#print(self.test_size)", "\n", "self", ".", "reset_test", "(", ")", "\n", "return", "\n", "", "else", ":", "\n", "# train data", "\n", "      ", "if", "self", ".", "hparams", ".", "trdec", ":", "\n", "        ", "self", ".", "x_train", ",", "self", ".", "y_word_train", ",", "self", ".", "y_train", "=", "self", ".", "_build_tree_parallel", "(", "\n", "self", ".", "hparams", ".", "source_train", ",", "self", ".", "hparams", ".", "target_train", ",", "self", ".", "hparams", ".", "target_tree_train", ",", "\n", "is_training", "=", "True", ",", "sort", "=", "True", ")", "\n", "", "else", ":", "\n", "        ", "self", ".", "x_train", ",", "self", ".", "y_train", "=", "self", ".", "_build_parallel", "(", "\n", "self", ".", "hparams", ".", "source_train", ",", "self", ".", "hparams", ".", "target_train", ",", "is_training", "=", "True", ",", "\n", "sort", "=", "True", ")", "\n", "\n", "# signifies that x_train, y_train are not ready for batching", "\n", "", "self", ".", "train_size", "=", "len", "(", "self", ".", "x_train", ")", "\n", "self", ".", "n_train_batches", "=", "None", "\n", "self", ".", "reset_train", "(", ")", "\n", "\n", "# valid data", "\n", "if", "self", ".", "hparams", ".", "trdec", ":", "\n", "        ", "self", ".", "x_valid", ",", "self", ".", "y_word_valid", ",", "self", ".", "y_valid", "=", "self", ".", "_build_tree_parallel", "(", "\n", "self", ".", "hparams", ".", "source_valid", ",", "self", ".", "hparams", ".", "target_valid", ",", "self", ".", "hparams", ".", "target_tree_valid", ",", "\n", "is_training", "=", "False", ")", "\n", "", "else", ":", "\n", "        ", "self", ".", "x_valid", ",", "self", ".", "y_valid", "=", "self", ".", "_build_parallel", "(", "\n", "self", ".", "hparams", ".", "source_valid", ",", "self", ".", "hparams", ".", "target_valid", ",", "is_training", "=", "False", ")", "\n", "", "self", ".", "valid_size", "=", "len", "(", "self", ".", "x_valid", ")", "\n", "self", ".", "reset_valid", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.data_utils.DataLoader.reset_train": [[96, 128], ["numpy.random.permutation", "len", "ValueError", "start_indices.append", "end_indices.append", "len", "len", "len", "len", "len", "len"], "methods", ["None"], ["", "", "def", "reset_train", "(", "self", ")", ":", "\n", "    ", "\"\"\"Shuffle training data. Prepare the batching scheme if necessary.\"\"\"", "\n", "\n", "if", "self", ".", "hparams", ".", "batcher", "==", "\"word\"", ":", "\n", "      ", "if", "self", ".", "n_train_batches", "is", "None", ":", "\n", "        ", "start_indices", ",", "end_indices", "=", "[", "]", ",", "[", "]", "\n", "start_index", "=", "0", "\n", "while", "start_index", "<", "self", ".", "train_size", ":", "\n", "          ", "end_index", "=", "start_index", "\n", "word_count", "=", "0", "\n", "while", "(", "end_index", "+", "1", "<", "self", ".", "train_size", "and", "\n", "(", "word_count", "+", "\n", "len", "(", "self", ".", "x_train", "[", "end_index", "+", "1", "]", ")", "+", "\n", "len", "(", "self", ".", "y_train", "[", "end_index", "+", "1", "]", ")", ")", "<=", "self", ".", "hparams", ".", "batch_size", ")", ":", "\n", "            ", "end_index", "+=", "1", "\n", "word_count", "+=", "(", "len", "(", "self", ".", "x_train", "[", "end_index", "]", ")", "+", "\n", "len", "(", "self", ".", "y_train", "[", "end_index", "]", ")", ")", "\n", "", "start_indices", ".", "append", "(", "start_index", ")", "\n", "end_indices", ".", "append", "(", "end_index", "+", "1", ")", "\n", "start_index", "=", "end_index", "+", "1", "\n", "", "assert", "len", "(", "start_indices", ")", "==", "len", "(", "end_indices", ")", "\n", "self", ".", "n_train_batches", "=", "len", "(", "start_indices", ")", "\n", "self", ".", "start_indices", "=", "start_indices", "\n", "self", ".", "end_indices", "=", "end_indices", "\n", "", "", "elif", "self", ".", "hparams", ".", "batcher", "==", "\"sent\"", ":", "\n", "      ", "if", "self", ".", "n_train_batches", "is", "None", ":", "\n", "        ", "self", ".", "n_train_batches", "=", "(", "(", "self", ".", "train_size", "+", "self", ".", "hparams", ".", "batch_size", "-", "1", ")", "\n", "//", "self", ".", "hparams", ".", "batch_size", ")", "\n", "", "", "else", ":", "\n", "      ", "raise", "ValueError", "(", "\"Unknown batcher scheme '{0}'\"", ".", "format", "(", "self", ".", "batcher", ")", ")", "\n", "", "self", ".", "train_queue", "=", "np", ".", "random", ".", "permutation", "(", "self", ".", "n_train_batches", ")", "\n", "self", ".", "train_index", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.data_utils.DataLoader.reset_valid": [[129, 131], ["None"], "methods", ["None"], ["", "def", "reset_valid", "(", "self", ")", ":", "\n", "    ", "self", ".", "valid_index", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.data_utils.DataLoader.reset_test": [[132, 134], ["None"], "methods", ["None"], ["", "def", "reset_test", "(", "self", ")", ":", "\n", "    ", "self", ".", "test_index", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.data_utils.DataLoader.next_test": [[135, 166], ["min", "data_utils.DataLoader._pad", "data_utils.DataLoader.sort_by_xlen", "data_utils.DataLoader._pad_tree", "data_utils.DataLoader._pad"], "methods", ["home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.data_utils.DataLoader._pad", "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.data_utils.DataLoader.sort_by_xlen", "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.data_utils.DataLoader._pad_tree", "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.data_utils.DataLoader._pad"], ["", "def", "next_test", "(", "self", ",", "test_batch_size", "=", "1", ",", "sort_by_x", "=", "False", ")", ":", "\n", "    ", "end_of_epoch", "=", "False", "\n", "start_index", "=", "self", ".", "test_index", "\n", "end_index", "=", "min", "(", "start_index", "+", "test_batch_size", ",", "self", ".", "test_size", ")", "\n", "batch_size", "=", "end_index", "-", "start_index", "\n", "\n", "# pad data", "\n", "x_test", "=", "self", ".", "x_test", "[", "start_index", ":", "end_index", "]", "\n", "y_test", "=", "self", ".", "y_test", "[", "start_index", ":", "end_index", "]", "\n", "if", "sort_by_x", ":", "\n", "      ", "x_test", ",", "y_test", "=", "self", ".", "sort_by_xlen", "(", "x_test", ",", "y_test", ")", "\n", "\n", "", "x_test", ",", "x_mask", ",", "x_len", ",", "x_count", "=", "self", ".", "_pad", "(", "\n", "sentences", "=", "x_test", ",", "pad_id", "=", "self", ".", "pad_id", ",", "volatile", "=", "True", ")", "\n", "if", "self", ".", "hparams", ".", "trdec", ":", "\n", "      ", "y_test", ",", "y_mask", ",", "y_len", ",", "y_count", ",", "y_count_rule", ",", "y_count_word", ",", "y_count_eos", "=", "self", ".", "_pad_tree", "(", "\n", "sentences", "=", "y_test", ",", "pad_id", "=", "self", ".", "pad_id", ",", "volatile", "=", "True", ")", "\n", "y_count", "=", "(", "y_count", ",", "y_count_rule", ",", "y_count_word", ",", "y_count_eos", ")", "\n", "", "else", ":", "\n", "      ", "y_test", ",", "y_mask", ",", "y_len", ",", "y_count", "=", "self", ".", "_pad", "(", "\n", "sentences", "=", "y_test", ",", "pad_id", "=", "self", ".", "pad_id", ",", "volatile", "=", "True", ")", "\n", "\n", "", "if", "end_index", ">=", "self", ".", "test_size", ":", "\n", "      ", "end_of_epoch", "=", "True", "\n", "self", ".", "test_index", "=", "0", "\n", "", "else", ":", "\n", "      ", "self", ".", "test_index", "+=", "batch_size", "\n", "\n", "", "return", "(", "(", "x_test", ",", "x_mask", ",", "x_len", ",", "x_count", ")", ",", "\n", "(", "y_test", ",", "y_mask", ",", "y_len", ",", "y_count", ")", ",", "\n", "batch_size", ",", "end_of_epoch", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.data_utils.DataLoader.next_valid": [[167, 208], ["min", "data_utils.DataLoader.sort_by_xlen", "data_utils.DataLoader._pad", "data_utils.DataLoader._pad_tree", "data_utils.DataLoader._pad"], "methods", ["home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.data_utils.DataLoader.sort_by_xlen", "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.data_utils.DataLoader._pad", "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.data_utils.DataLoader._pad_tree", "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.data_utils.DataLoader._pad"], ["", "def", "next_valid", "(", "self", ",", "valid_batch_size", "=", "20", ")", ":", "\n", "    ", "\"\"\"Retrieves a sentence of testing examples.\n\n    Returns:\n      (x_valid, x_len): a pair of torch Tensors of size [batch, source_length]\n        and [batch_size].\n      (y_valid, y_len): a pair of torch Tensors of size [batch, target_length]\n        and [batch_size].\n      end_of_epoch: whether we reach the end of training examples.\n    \"\"\"", "\n", "\n", "end_of_epoch", "=", "False", "\n", "start_index", "=", "self", ".", "valid_index", "\n", "end_index", "=", "min", "(", "start_index", "+", "valid_batch_size", ",", "self", ".", "valid_size", ")", "\n", "batch_size", "=", "end_index", "-", "start_index", "\n", "\n", "# pad data", "\n", "x_valid", "=", "self", ".", "x_valid", "[", "start_index", ":", "end_index", "]", "\n", "y_valid", "=", "self", ".", "y_valid", "[", "start_index", ":", "end_index", "]", "\n", "x_valid", ",", "y_valid", "=", "self", ".", "sort_by_xlen", "(", "x_valid", ",", "y_valid", ")", "\n", "\n", "x_valid", ",", "x_mask", ",", "x_len", ",", "x_count", "=", "self", ".", "_pad", "(", "\n", "sentences", "=", "x_valid", ",", "pad_id", "=", "self", ".", "pad_id", ",", "volatile", "=", "True", ")", "\n", "if", "self", ".", "hparams", ".", "trdec", ":", "\n", "      ", "y_valid", ",", "y_mask", ",", "y_len", ",", "y_count", ",", "y_count_rule", ",", "y_count_word", ",", "y_count_eos", "=", "self", ".", "_pad_tree", "(", "\n", "sentences", "=", "y_valid", ",", "pad_id", "=", "self", ".", "pad_id", ",", "volatile", "=", "True", ")", "\n", "y_count", "=", "(", "y_count", ",", "y_count_rule", ",", "y_count_word", ",", "y_count_eos", ")", "\n", "", "else", ":", "\n", "      ", "y_valid", ",", "y_mask", ",", "y_len", ",", "y_count", "=", "self", ".", "_pad", "(", "\n", "sentences", "=", "y_valid", ",", "pad_id", "=", "self", ".", "pad_id", ",", "volatile", "=", "True", ")", "\n", "\n", "# shuffle if reaches the end of data", "\n", "", "if", "end_index", ">=", "self", ".", "valid_size", ":", "\n", "      ", "end_of_epoch", "=", "True", "\n", "self", ".", "valid_index", "=", "0", "\n", "", "else", ":", "\n", "      ", "self", ".", "valid_index", "+=", "batch_size", "\n", "\n", "", "return", "(", "(", "x_valid", ",", "x_mask", ",", "x_len", ",", "x_count", ")", ",", "\n", "(", "y_valid", ",", "y_mask", ",", "y_len", ",", "y_count", ")", ",", "\n", "batch_size", ",", "end_of_epoch", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.data_utils.DataLoader.next_train": [[209, 254], ["len", "data_utils.DataLoader.sort_by_xlen", "data_utils.DataLoader._pad", "data_utils.DataLoader._pad_tree", "data_utils.DataLoader._pad", "data_utils.DataLoader.reset_train", "min", "ValueError"], "methods", ["home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.data_utils.DataLoader.sort_by_xlen", "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.data_utils.DataLoader._pad", "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.data_utils.DataLoader._pad_tree", "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.data_utils.DataLoader._pad", "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.data_utils.DataLoader.reset_train"], ["", "def", "next_train", "(", "self", ")", ":", "\n", "    ", "\"\"\"Retrieves a batch of training examples.\n\n    Returns:\n      (x_train, x_len): a pair of torch Tensors of size [batch, source_length]\n        and [batch_size].\n      (y_train, y_len): a pair of torch Tensors of size [batch, target_length]\n        and [batch_size].\n      end_of_epoch: whether we reach the end of training examples.\n    \"\"\"", "\n", "if", "self", ".", "hparams", ".", "batcher", "==", "\"word\"", ":", "\n", "      ", "start_index", "=", "self", ".", "start_indices", "[", "self", ".", "train_queue", "[", "self", ".", "train_index", "]", "]", "\n", "end_index", "=", "self", ".", "end_indices", "[", "self", ".", "train_queue", "[", "self", ".", "train_index", "]", "]", "\n", "", "elif", "self", ".", "hparams", ".", "batcher", "==", "\"sent\"", ":", "\n", "      ", "start_index", "=", "(", "self", ".", "train_queue", "[", "self", ".", "train_index", "]", "*", "\n", "self", ".", "hparams", ".", "batch_size", ")", "\n", "end_index", "=", "min", "(", "start_index", "+", "self", ".", "hparams", ".", "batch_size", ",", "self", ".", "train_size", ")", "\n", "", "else", ":", "\n", "      ", "raise", "ValueError", "(", "\"Unknown batcher '{0}'\"", ".", "format", "(", "self", ".", "hparams", ".", "batcher", ")", ")", "\n", "\n", "", "x_train", "=", "self", ".", "x_train", "[", "start_index", ":", "end_index", "]", "\n", "y_train", "=", "self", ".", "y_train", "[", "start_index", ":", "end_index", "]", "\n", "self", ".", "train_index", "+=", "1", "\n", "batch_size", "=", "len", "(", "x_train", ")", "\n", "#x_train[0] = x_train[0][:3]", "\n", "# sort based on x_len", "\n", "x_train", ",", "y_train", "=", "self", ".", "sort_by_xlen", "(", "x_train", ",", "y_train", ")", "\n", "# pad data", "\n", "x_train", ",", "x_mask", ",", "x_len", ",", "x_count", "=", "self", ".", "_pad", "(", "\n", "sentences", "=", "x_train", ",", "pad_id", "=", "self", ".", "pad_id", ")", "\n", "if", "self", ".", "hparams", ".", "trdec", ":", "\n", "      ", "y_train", ",", "y_mask", ",", "y_len", ",", "y_count", ",", "y_count_rule", ",", "y_count_word", ",", "y_count_eos", "=", "self", ".", "_pad_tree", "(", "\n", "sentences", "=", "y_train", ",", "pad_id", "=", "self", ".", "pad_id", ",", "raml_rule", "=", "self", ".", "hparams", ".", "raml_rule", ")", "\n", "y_count", "=", "(", "y_count", ",", "y_count_rule", ",", "y_count_word", ",", "y_count_eos", ")", "\n", "", "else", ":", "\n", "      ", "y_train", ",", "y_mask", ",", "y_len", ",", "y_count", "=", "self", ".", "_pad", "(", "\n", "sentences", "=", "y_train", ",", "pad_id", "=", "self", ".", "pad_id", ")", "\n", "\n", "# shuffle if reaches the end of data", "\n", "", "if", "self", ".", "train_index", ">", "self", ".", "n_train_batches", "-", "1", ":", "\n", "      ", "self", ".", "reset_train", "(", ")", "\n", "\n", "", "return", "(", "(", "x_train", ",", "x_mask", ",", "x_len", ",", "x_count", ")", ",", "\n", "(", "y_train", ",", "y_mask", ",", "y_len", ",", "y_count", ")", ",", "\n", "batch_size", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.data_utils.DataLoader.sort_by_xlen": [[255, 263], ["len", "numpy.argsort"], "methods", ["None"], ["", "def", "sort_by_xlen", "(", "self", ",", "x", ",", "y", ")", ":", "\n", "    ", "x_len", "=", "[", "len", "(", "i", ")", "for", "i", "in", "x", "]", "\n", "index", "=", "np", ".", "argsort", "(", "x_len", ")", "[", ":", ":", "-", "1", "]", "\n", "#print(x)", "\n", "#print(y)", "\n", "#print(index)", "\n", "#print(x_len)", "\n", "return", "x", "[", "index", "]", ",", "y", "[", "index", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.data_utils.DataLoader._pad": [[264, 324], ["sum", "max", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.ByteTensor", "torch.ByteTensor", "torch.ByteTensor", "torch.ByteTensor", "torch.ByteTensor", "torch.ByteTensor", "torch.ByteTensor", "torch.ByteTensor", "torch.ByteTensor", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "data_utils.DataLoader.softmax", "torch.distributions.Categorical().sample", "torch.distributions.Categorical().sample", "torch.distributions.Categorical().sample", "torch.distributions.Categorical().sample", "torch.distributions.Categorical().sample", "torch.distributions.Categorical().sample", "torch.distributions.Categorical().sample", "torch.distributions.Categorical().sample", "torch.distributions.Categorical().sample", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.distributions.Categorical().sample.data.float().div_().unsqueeze().expand_as().contiguous().masked_fill_", "torch.distributions.Categorical().sample.data.float().div_().unsqueeze().expand_as().contiguous().masked_fill_", "torch.distributions.Categorical().sample.data.float().div_().unsqueeze().expand_as().contiguous().masked_fill_", "torch.bernoulli().byte", "torch.bernoulli().byte", "torch.bernoulli().byte", "torch.bernoulli().byte", "torch.bernoulli().byte", "torch.bernoulli().byte", "torch.bernoulli().byte", "torch.bernoulli().byte", "torch.bernoulli().byte", "int", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.zeros().long", "torch.zeros().long", "torch.zeros().long", "torch.zeros().long", "torch.zeros().long", "torch.zeros().long", "torch.zeros().long", "torch.zeros().long", "torch.zeros().long", "corrupts.cuda.cuda.masked_scatter_", "padded_sentences.cuda.cuda.add().remainder_().masked_fill_", "len", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "padded_sentences.cuda.cuda.cuda", "mask.cuda.cuda.cuda", "logits.cuda.cuda.cuda", "logits.cuda.cuda.mul_", "lengths.cuda.cuda.cuda", "corrupt_pos.cuda.cuda.sum", "corrupt_val.long().cuda.long().cuda.long().cuda", "corrupts.cuda.cuda.cuda", "corrupt_pos.cuda.cuda.cuda", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.distributions.Categorical", "torch.distributions.Categorical", "torch.distributions.Categorical", "torch.distributions.Categorical", "torch.distributions.Categorical", "torch.distributions.Categorical", "torch.distributions.Categorical", "torch.distributions.Categorical", "torch.distributions.Categorical", "torch.distributions.Categorical().sample.data.float().div_().unsqueeze().expand_as().contiguous", "torch.distributions.Categorical().sample.data.float().div_().unsqueeze().expand_as().contiguous", "torch.distributions.Categorical().sample.data.float().div_().unsqueeze().expand_as().contiguous", "torch.bernoulli", "torch.bernoulli", "torch.bernoulli", "torch.bernoulli", "torch.bernoulli", "torch.bernoulli", "torch.bernoulli", "torch.bernoulli", "torch.bernoulli", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "padded_sentences.cuda.cuda.add().remainder_", "len", "corrupt_val.long().cuda.long().cuda.long", "len", "len", "torch.distributions.Categorical().sample.data.float().div_().unsqueeze().expand_as", "torch.distributions.Categorical().sample.data.float().div_().unsqueeze().expand_as", "torch.distributions.Categorical().sample.data.float().div_().unsqueeze().expand_as", "padded_sentences.cuda.cuda.add", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.distributions.Categorical().sample.data.float().div_().unsqueeze", "torch.distributions.Categorical().sample.data.float().div_().unsqueeze", "torch.distributions.Categorical().sample.data.float().div_().unsqueeze", "torch.distributions.Categorical().sample.data.float().div_", "torch.distributions.Categorical().sample.data.float().div_", "torch.distributions.Categorical().sample.data.float().div_", "torch.distributions.Categorical().sample.data.float", "torch.distributions.Categorical().sample.data.float", "torch.distributions.Categorical().sample.data.float"], "methods", ["None"], ["", "def", "_pad", "(", "self", ",", "sentences", ",", "pad_id", ",", "volatile", "=", "False", ",", "\n", "raml", "=", "False", ",", "raml_tau", "=", "1.", ",", "vocab_size", "=", "None", ")", ":", "\n", "    ", "\"\"\"Pad all instances in [data] to the longest length.\n\n    Args:\n      sentences: list of [batch_size] lists.\n\n    Returns:\n      padded_sentences: Variable of size [batch_size, max_len], the sentences.\n      mask: Variable of size [batch_size, max_len]. 1 means to ignore.\n      pos_emb_indices: Variable of size [batch_size, max_len]. indices to use\n        when computing positional embedding.\n      sum_len: total words\n    \"\"\"", "\n", "\n", "lengths", "=", "[", "len", "(", "sentence", ")", "for", "sentence", "in", "sentences", "]", "\n", "sum_len", "=", "sum", "(", "lengths", ")", "\n", "max_len", "=", "max", "(", "lengths", ")", "\n", "padded_sentences", "=", "[", "\n", "sentence", "+", "(", "[", "pad_id", "]", "*", "(", "max_len", "-", "len", "(", "sentence", ")", ")", ")", "\n", "for", "sentence", "in", "sentences", "]", "\n", "mask", "=", "[", "\n", "(", "[", "0", "]", "*", "len", "(", "sentence", ")", ")", "+", "(", "[", "1", "]", "*", "(", "max_len", "-", "len", "(", "sentence", ")", ")", ")", "\n", "for", "sentence", "in", "sentences", "]", "\n", "\n", "padded_sentences", "=", "Variable", "(", "torch", ".", "LongTensor", "(", "padded_sentences", ")", ")", "\n", "mask", "=", "torch", ".", "ByteTensor", "(", "mask", ")", "\n", "#l = Variable(torch.FloatTensor(lengths))", "\n", "\n", "if", "self", ".", "hparams", ".", "cuda", ":", "\n", "      ", "padded_sentences", "=", "padded_sentences", ".", "cuda", "(", ")", "\n", "mask", "=", "mask", ".", "cuda", "(", ")", "\n", "\n", "", "if", "not", "raml", ":", "\n", "      ", "return", "padded_sentences", ",", "mask", ",", "lengths", ",", "sum_len", "\n", "\n", "", "assert", "vocab_size", "is", "not", "None", "\n", "logits", "=", "torch", ".", "arange", "(", "max_len", ")", "\n", "if", "self", ".", "hparams", ".", "cuda", ":", "\n", "      ", "logits", "=", "logits", ".", "cuda", "(", ")", "\n", "", "probs", "=", "self", ".", "softmax", "(", "logits", ".", "mul_", "(", "raml_tau", ")", ")", "\n", "num_words", "=", "torch", ".", "distributions", ".", "Categorical", "(", "probs", ")", ".", "sample", "(", ")", "\n", "\n", "lengths", "=", "torch", ".", "FloatTensor", "(", "lengths", ")", "\n", "if", "self", ".", "hparams", ".", "cuda", ":", "\n", "      ", "lengths", "=", "lengths", ".", "cuda", "(", ")", "\n", "", "corrupt_pos", "=", "num_words", ".", "data", ".", "float", "(", ")", ".", "div_", "(", "lenngths", ")", ".", "unsqueeze", "(", "1", ")", ".", "expand_as", "(", "padded_sentences", ")", ".", "contiguous", "(", ")", ".", "masked_fill_", "(", "mask", ",", "-", "self", ".", "hparams", ".", "inf", ")", "\n", "corrupt_pos", "=", "torch", ".", "bernoulli", "(", "corrupt_pos", ",", "out", "=", "corrupt_pos", ")", ".", "byte", "(", ")", "\n", "total_words", "=", "int", "(", "corrupt_pos", ".", "sum", "(", ")", ")", "\n", "\n", "corrupt_val", "=", "torch", ".", "LongTensor", "(", "total_words", ")", "\n", "corrupts", "=", "torch", ".", "zeros", "(", "batch_size", ",", "max_len", ")", ".", "long", "(", ")", "\n", "\n", "if", "self", ".", "hparams", ".", "cuda", ":", "\n", "      ", "corrupt_val", "=", "corrupt_val", ".", "long", "(", ")", ".", "cuda", "(", ")", "\n", "corrupts", "=", "corrupts", ".", "cuda", "(", ")", "\n", "corrupt_pos", "=", "corrupt_pos", ".", "cuda", "(", ")", "\n", "", "corrupts", "=", "corrupts", ".", "masked_scatter_", "(", "corrupt_pos", ",", "corrupt_val", ")", "\n", "sample_sentences", "=", "padded_sentences", ".", "add", "(", "Variable", "(", "corrupts", ")", ")", ".", "remainder_", "(", "vocab_size", ")", ".", "masked_fill_", "(", "Variable", "(", "mask", ")", ",", "pad_id", ")", "\n", "return", "sample_sentences", ",", "mask", ",", "lengths", ",", "sum_len", "\n", "\n"]], "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.data_utils.DataLoader._pad_tree": [[325, 401], ["sum", "max", "len", "len", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.ByteTensor", "torch.ByteTensor", "torch.ByteTensor", "torch.ByteTensor", "torch.ByteTensor", "torch.ByteTensor", "torch.ByteTensor", "torch.ByteTensor", "torch.ByteTensor", "rule_mask.cuda.cuda.long().sum", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "rule_mask.cuda.cuda.long().sum().unsqueeze", "rule_len.cuda.cuda.data.cpu().numpy().tolist", "torch.ByteTensor", "torch.ByteTensor", "torch.ByteTensor", "torch.ByteTensor", "torch.ByteTensor", "torch.ByteTensor", "torch.ByteTensor", "torch.ByteTensor", "torch.ByteTensor", "logits.cuda.cuda.mul_().unsqueeze().expand_as().contiguous().masked_fill_", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.softmax", "torch.softmax", "torch.softmax", "torch.distributions.Categorical().sample", "torch.distributions.Categorical().sample", "torch.distributions.Categorical().sample", "torch.distributions.Categorical().sample", "torch.distributions.Categorical().sample", "torch.distributions.Categorical().sample", "torch.distributions.Categorical().sample", "torch.distributions.Categorical().sample", "torch.distributions.Categorical().sample", "torch.distributions.Categorical().sample.data.float().div_().unsqueeze().expand_as().contiguous().masked_fill_", "torch.distributions.Categorical().sample.data.float().div_().unsqueeze().expand_as().contiguous().masked_fill_", "torch.distributions.Categorical().sample.data.float().div_().unsqueeze().expand_as().contiguous().masked_fill_", "torch.bernoulli().byte", "torch.bernoulli().byte", "torch.bernoulli().byte", "torch.bernoulli().byte", "torch.bernoulli().byte", "torch.bernoulli().byte", "torch.bernoulli().byte", "torch.bernoulli().byte", "torch.bernoulli().byte", "int", "torch.LongTensor().random_", "torch.LongTensor().random_", "torch.LongTensor().random_", "torch.LongTensor().random_", "torch.LongTensor().random_", "torch.LongTensor().random_", "torch.LongTensor().random_", "torch.LongTensor().random_", "torch.LongTensor().random_", "torch.zeros().long", "torch.zeros().long", "torch.zeros().long", "torch.zeros().long", "torch.zeros().long", "torch.zeros().long", "torch.zeros().long", "torch.zeros().long", "torch.zeros().long", "padded_sentences[].data.masked_scatter_", "len", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "num_eos.item", "padded_sentences.cuda.cuda.cuda", "mask.cuda.cuda.cuda", "logits.cuda.cuda.cuda", "rule_len_mask.cuda.cuda.cuda", "rule_len.cuda.cuda.cuda", "rule_mask.cuda.cuda.cuda", "logits.cuda.cuda.cuda", "logits.cuda.cuda.mul_", "corrupt_pos.cuda.cuda.sum", "corrupt_val.long().cuda.long().cuda.long().cuda", "corrupts.cuda.cuda.cuda", "corrupt_pos.cuda.cuda.cuda", "rule_mask.cuda.long().sum.item", "num_eos.item", "rule_mask.cuda.cuda.long", "rule_mask.cuda.long().sum.item", "num_padding.item", "len", "rule_mask.cuda.long().sum.item", "num_eos.item", "rule_mask.cuda.cuda.long().sum", "rule_len.cuda.cuda.data.cpu().numpy", "range", "logits.cuda.cuda.mul_().unsqueeze().expand_as().contiguous", "float", "torch.distributions.Categorical", "torch.distributions.Categorical", "torch.distributions.Categorical", "torch.distributions.Categorical", "torch.distributions.Categorical", "torch.distributions.Categorical", "torch.distributions.Categorical", "torch.distributions.Categorical", "torch.distributions.Categorical", "torch.distributions.Categorical().sample.data.float().div_().unsqueeze().expand_as().contiguous", "torch.distributions.Categorical().sample.data.float().div_().unsqueeze().expand_as().contiguous", "torch.distributions.Categorical().sample.data.float().div_().unsqueeze().expand_as().contiguous", "torch.bernoulli", "torch.bernoulli", "torch.bernoulli", "torch.bernoulli", "torch.bernoulli", "torch.bernoulli", "torch.bernoulli", "torch.bernoulli", "torch.bernoulli", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "len", "num_eos.item", "len", "corrupt_val.long().cuda.long().cuda.long", "len", "len", "rule_mask.cuda.long().sum.item", "rule_mask.cuda.cuda.long", "rule_len.cuda.cuda.data.cpu", "logits.cuda.cuda.mul_().unsqueeze().expand_as", "torch.distributions.Categorical().sample.data.float().div_().unsqueeze().expand_as", "torch.distributions.Categorical().sample.data.float().div_().unsqueeze().expand_as", "torch.distributions.Categorical().sample.data.float().div_().unsqueeze().expand_as", "logits.cuda.cuda.mul_().unsqueeze", "torch.distributions.Categorical().sample.data.float().div_().unsqueeze", "torch.distributions.Categorical().sample.data.float().div_().unsqueeze", "torch.distributions.Categorical().sample.data.float().div_().unsqueeze", "logits.cuda.cuda.mul_", "torch.distributions.Categorical().sample.data.float().div_", "torch.distributions.Categorical().sample.data.float().div_", "torch.distributions.Categorical().sample.data.float().div_", "torch.distributions.Categorical().sample.data.float", "torch.distributions.Categorical().sample.data.float", "torch.distributions.Categorical().sample.data.float", "rule_len.cuda.cuda.float"], "methods", ["None"], ["", "def", "_pad_tree", "(", "self", ",", "sentences", ",", "pad_id", ",", "volatile", "=", "False", ",", "raml_rule", "=", "False", ")", ":", "\n", "    ", "lengths", "=", "[", "len", "(", "sentence", ")", "for", "sentence", "in", "sentences", "]", "\n", "sum_len", "=", "sum", "(", "lengths", ")", "\n", "max_len", "=", "max", "(", "lengths", ")", "\n", "batch_size", "=", "len", "(", "lengths", ")", "\n", "\n", "item_size", "=", "len", "(", "sentences", "[", "0", "]", "[", "0", "]", ")", "\n", "padded_sentences", "=", "[", "\n", "sentence", "+", "(", "[", "[", "pad_id", "]", "*", "item_size", "]", "*", "(", "max_len", "-", "len", "(", "sentence", ")", ")", ")", "\n", "for", "sentence", "in", "sentences", "]", "\n", "mask", "=", "[", "\n", "(", "[", "0", "]", "*", "len", "(", "sentence", ")", ")", "+", "(", "[", "1", "]", "*", "(", "max_len", "-", "len", "(", "sentence", ")", ")", ")", "\n", "for", "sentence", "in", "sentences", "]", "\n", "\n", "padded_sentences", "=", "Variable", "(", "torch", ".", "LongTensor", "(", "padded_sentences", ")", ")", "\n", "mask", "=", "torch", ".", "ByteTensor", "(", "mask", ")", "\n", "\n", "num_padding", "=", "(", "padded_sentences", "[", ":", ",", ":", ",", "0", "]", "==", "pad_id", ")", ".", "long", "(", ")", ".", "sum", "(", ")", "\n", "rule_mask", "=", "(", "padded_sentences", "[", ":", ",", ":", ",", "0", "]", ">=", "self", ".", "target_word_vocab_size", ")", "\n", "num_rule", "=", "rule_mask", ".", "long", "(", ")", ".", "sum", "(", ")", "\n", "#print(num_rule)", "\n", "num_eos", "=", "(", "padded_sentences", "[", ":", ",", ":", ",", "0", "]", "==", "self", ".", "hparams", ".", "eos_id", ")", ".", "long", "(", ")", ".", "sum", "(", ")", "\n", "num_word", "=", "sum_len", "-", "num_rule", ".", "item", "(", ")", "-", "num_eos", ".", "item", "(", ")", "\n", "#print(num_rule.data[0] + num_word + num_eos.data[0] + num_padding.data[0])", "\n", "#print(max_len * len(sentences))", "\n", "assert", "num_rule", ".", "item", "(", ")", "+", "num_word", "+", "num_eos", ".", "item", "(", ")", "+", "num_padding", ".", "item", "(", ")", "==", "max_len", "*", "len", "(", "sentences", ")", "\n", "\n", "if", "self", ".", "hparams", ".", "cuda", ":", "\n", "      ", "padded_sentences", "=", "padded_sentences", ".", "cuda", "(", ")", "\n", "mask", "=", "mask", ".", "cuda", "(", ")", "\n", "", "if", "not", "raml_rule", ":", "\n", "      ", "return", "padded_sentences", ",", "mask", ",", "lengths", ",", "sum_len", ",", "num_rule", ".", "item", "(", ")", ",", "num_word", ",", "num_eos", ".", "item", "(", ")", "\n", "\n", "# sample the number of words to corrupt", "\n", "", "logits", "=", "torch", ".", "arange", "(", "max_len", ")", "\n", "rule_len", "=", "rule_mask", ".", "long", "(", ")", ".", "sum", "(", "dim", "=", "1", ")", ".", "unsqueeze", "(", "1", ")", "\n", "rule_len_data", "=", "rule_len", ".", "data", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ".", "tolist", "(", ")", "\n", "#print(rule_len_data)", "\n", "rule_len_mask", "=", "[", "(", "[", "0", "]", "*", "rule_len_data", "[", "i", "]", "[", "0", "]", "+", "[", "1", "]", "*", "(", "max_len", "-", "rule_len_data", "[", "i", "]", "[", "0", "]", ")", ")", "for", "i", "in", "range", "(", "len", "(", "rule_len_data", ")", ")", "]", "\n", "#print(rule_len_mask)", "\n", "rule_len_mask", "=", "torch", ".", "ByteTensor", "(", "rule_len_mask", ")", "\n", "if", "self", ".", "hparams", ".", "cuda", ":", "\n", "      ", "logits", "=", "logits", ".", "cuda", "(", ")", "\n", "rule_len_mask", "=", "rule_len_mask", ".", "cuda", "(", ")", "\n", "rule_len", "=", "rule_len", ".", "cuda", "(", ")", "\n", "rule_mask", "=", "rule_mask", ".", "cuda", "(", ")", "\n", "", "logits", "=", "logits", ".", "mul_", "(", "-", "1", ")", ".", "unsqueeze", "(", "0", ")", ".", "expand_as", "(", "padded_sentences", "[", ":", ",", ":", ",", "0", "]", ")", ".", "contiguous", "(", ")", ".", "masked_fill_", "(", "rule_len_mask", ",", "-", "float", "(", "\"inf\"", ")", ")", "\n", "logits", "=", "Variable", "(", "logits", ",", "volatile", "=", "True", ")", "\n", "if", "self", ".", "hparams", ".", "cuda", ":", "\n", "      ", "logits", "=", "logits", ".", "cuda", "(", ")", "\n", "", "probs", "=", "F", ".", "softmax", "(", "logits", ".", "mul_", "(", "self", ".", "hparams", ".", "raml_tau", ")", ",", "dim", "=", "1", ")", "\n", "num_words", "=", "torch", ".", "distributions", ".", "Categorical", "(", "probs", ")", ".", "sample", "(", ")", "\n", "# sample the rule indices", "\n", "corrupt_pos", "=", "num_words", ".", "data", ".", "float", "(", ")", ".", "div_", "(", "rule_len", ".", "float", "(", ")", ".", "data", ")", ".", "unsqueeze", "(", "1", ")", ".", "expand_as", "(", "padded_sentences", "[", ":", ",", ":", ",", "0", "]", ")", ".", "contiguous", "(", ")", ".", "masked_fill_", "(", "~", "rule_mask", ".", "data", ",", "0", ")", "\n", "corrupt_pos", "=", "torch", ".", "bernoulli", "(", "corrupt_pos", ",", "out", "=", "corrupt_pos", ")", ".", "byte", "(", ")", "\n", "total_words", "=", "int", "(", "corrupt_pos", ".", "sum", "(", ")", ")", "\n", "\n", "corrupt_val", "=", "torch", ".", "LongTensor", "(", "total_words", ")", ".", "random_", "(", "self", ".", "target_word_vocab_size", ",", "self", ".", "target_word_vocab_size", "+", "self", ".", "target_rule_vocab_size", ")", "\n", "corrupts", "=", "torch", ".", "zeros", "(", "batch_size", ",", "max_len", ")", ".", "long", "(", ")", "\n", "if", "self", ".", "hparams", ".", "cuda", ":", "\n", "      ", "corrupt_val", "=", "corrupt_val", ".", "long", "(", ")", ".", "cuda", "(", ")", "\n", "corrupts", "=", "corrupts", ".", "cuda", "(", ")", "\n", "corrupt_pos", "=", "corrupt_pos", ".", "cuda", "(", ")", "\n", "#corrupts = corrupts.masked_scatter_(corrupt_pos, corrupt_val)", "\n", "#print(padded_sentences[:,:,0].masked_select(Variable(corrupt_pos)))", "\n", "", "padded_sentences", "[", ":", ",", ":", ",", "0", "]", ".", "data", ".", "masked_scatter_", "(", "corrupt_pos", ",", "corrupt_val", ")", "\n", "#sampled_sentences = padded_sentences[:,:,0] - Variable(corrupt_pos.long())*self.target_word_vocab_size", "\n", "#sampled_sentences.add_(Variable(corrupts)).remainder_(self.target_rule_vocab_size)", "\n", "#sampled_sentences.add_(Variable(corrupt_pos.long())*self.target_word_vocab_size)", "\n", "#sampled_sentences.masked_fill_(Variable(mask), pad_id)", "\n", "#print(sampled_sentences)", "\n", "#print(padded_sentences[:,:,0].masked_select(Variable(corrupt_pos)))", "\n", "#print(corrupt_pos)", "\n", "#print(corrupt_val)", "\n", "#exit(0)", "\n", "return", "padded_sentences", ",", "mask", ",", "lengths", ",", "sum_len", ",", "num_rule", ".", "item", "(", ")", ",", "num_word", ",", "num_eos", ".", "item", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.data_utils.DataLoader._build_parallel": [[402, 480], ["print", "print", "os.path.join", "os.path.join", "enumerate", "print", "open", "finp.read().split", "open", "finp.read().split", "zip", "source_line.strip.strip.strip", "target_line.strip.strip.strip", "source_line.strip.strip.split", "target_line.strip.strip.split", "source_lens.append", "source_data.append", "target_data.append", "len", "len", "numpy.array", "numpy.array", "source_indices.append", "target_indices.append", "len", "print", "finp.read", "finp.read", "len"], "methods", ["None"], ["", "def", "_build_parallel", "(", "self", ",", "source_file", ",", "target_file", ",", "is_training", ",", "sort", "=", "False", ")", ":", "\n", "    ", "\"\"\"Build pair of data.\"\"\"", "\n", "\n", "print", "(", "\"-\"", "*", "80", ")", "\n", "print", "(", "\"Loading parallel data from '{0}' and '{1}'\"", ".", "format", "(", "\n", "source_file", ",", "target_file", ")", ")", "\n", "\n", "source_file", "=", "os", ".", "path", ".", "join", "(", "self", ".", "hparams", ".", "data_path", ",", "source_file", ")", "\n", "with", "open", "(", "source_file", ",", "encoding", "=", "'utf-8'", ")", "as", "finp", ":", "\n", "      ", "source_lines", "=", "finp", ".", "read", "(", ")", ".", "split", "(", "\"\\n\"", ")", "\n", "\n", "", "target_file", "=", "os", ".", "path", ".", "join", "(", "self", ".", "hparams", ".", "data_path", ",", "target_file", ")", "\n", "with", "open", "(", "target_file", ",", "encoding", "=", "'utf-8'", ")", "as", "finp", ":", "\n", "      ", "target_lines", "=", "finp", ".", "read", "(", ")", ".", "split", "(", "\"\\n\"", ")", "\n", "\n", "", "source_data", ",", "target_data", "=", "[", "]", ",", "[", "]", "\n", "source_lens", "=", "[", "]", "\n", "total_sents", "=", "0", "\n", "source_unk_count", ",", "target_unk_count", "=", "0", ",", "0", "\n", "for", "i", ",", "(", "source_line", ",", "target_line", ")", "in", "enumerate", "(", "\n", "zip", "(", "source_lines", ",", "target_lines", ")", ")", ":", "\n", "      ", "source_line", "=", "source_line", ".", "strip", "(", ")", "\n", "target_line", "=", "target_line", ".", "strip", "(", ")", "\n", "if", "not", "source_line", "or", "not", "target_line", ":", "\n", "        ", "continue", "\n", "\n", "", "source_indices", ",", "target_indices", "=", "[", "self", ".", "bos_id", "]", ",", "[", "self", ".", "bos_id", "]", "\n", "source_tokens", "=", "source_line", ".", "split", "(", ")", "\n", "target_tokens", "=", "target_line", ".", "split", "(", ")", "\n", "if", "is_training", "and", "len", "(", "target_line", ")", ">", "self", ".", "hparams", ".", "max_len", ":", "\n", "        ", "continue", "\n", "", "total_sents", "+=", "1", "\n", "\n", "for", "source_token", "in", "source_tokens", ":", "\n", "#source_token = source_token.strip()", "\n", "        ", "if", "source_token", "not", "in", "self", ".", "source_word_to_index", ":", "\n", "          ", "source_token", "=", "self", ".", "hparams", ".", "unk", "\n", "source_unk_count", "+=", "1", "\n", "#print(source_token)", "\n", "", "source_index", "=", "self", ".", "source_word_to_index", "[", "source_token", "]", "\n", "source_indices", ".", "append", "(", "source_index", ")", "\n", "\n", "", "for", "target_token", "in", "target_tokens", ":", "\n", "#target_token = target_token.strip()", "\n", "        ", "if", "target_token", "not", "in", "self", ".", "target_word_to_index", ":", "\n", "          ", "target_token", "=", "self", ".", "hparams", ".", "unk", "\n", "target_unk_count", "+=", "1", "\n", "", "target_index", "=", "self", ".", "target_word_to_index", "[", "target_token", "]", "\n", "target_indices", ".", "append", "(", "target_index", ")", "\n", "\n", "", "source_indices", "+=", "[", "self", ".", "eos_id", "]", "\n", "target_indices", "+=", "[", "self", ".", "eos_id", "]", "\n", "#assert source_indices[-1] == self.eos_id", "\n", "#assert target_indices[-1] == self.eos_id", "\n", "\n", "source_lens", ".", "append", "(", "len", "(", "source_indices", ")", ")", "\n", "source_data", ".", "append", "(", "source_indices", ")", "\n", "target_data", ".", "append", "(", "target_indices", ")", "\n", "\n", "if", "(", "self", ".", "hparams", ".", "n_train_sents", "is", "not", "None", "and", "\n", "self", ".", "hparams", ".", "n_train_sents", "<=", "total_sents", ")", ":", "\n", "        ", "break", "\n", "\n", "", "if", "total_sents", "%", "10000", "==", "0", ":", "\n", "        ", "print", "(", "\"{0:>6d} pairs. src_unk={1}. tgt_unk={2}\"", ".", "format", "(", "\n", "total_sents", ",", "source_unk_count", ",", "target_unk_count", ")", ")", "\n", "\n", "", "", "assert", "len", "(", "source_data", ")", "==", "len", "(", "target_data", ")", "\n", "print", "(", "\"{0:>6d} pairs. src_unk={1}. tgt_unk={2}\"", ".", "format", "(", "\n", "total_sents", ",", "source_unk_count", ",", "target_unk_count", ")", ")", "\n", "\n", "#if sort:", "\n", "#  print(\"Heuristic sort based on source lens\")", "\n", "#  indices = np.argsort(source_lens)", "\n", "#  source_data = [source_data[index] for index in indices]", "\n", "#  target_data = [target_data[index] for index in indices]", "\n", "\n", "return", "np", ".", "array", "(", "source_data", ")", ",", "np", ".", "array", "(", "target_data", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.data_utils.DataLoader._build_vocab": [[481, 519], ["print", "print", "os.path.join", "print", "open", "finp.read().split", "line.strip.strip.strip", "line.strip.strip.split", "int", "len", "len", "len", "len", "len", "print", "len", "finp.read", "len", "len"], "methods", ["None"], ["", "def", "_build_vocab", "(", "self", ",", "file_name", ")", ":", "\n", "    ", "\"\"\"Build word_to_index and index_to word dicts.\"\"\"", "\n", "\n", "print", "(", "\"-\"", "*", "80", ")", "\n", "print", "(", "\"Loading vocab from '{0}'\"", ".", "format", "(", "file_name", ")", ")", "\n", "file_name", "=", "os", ".", "path", ".", "join", "(", "self", ".", "hparams", ".", "data_path", ",", "file_name", ")", "\n", "with", "open", "(", "file_name", ",", "encoding", "=", "'utf-8'", ")", "as", "finp", ":", "\n", "      ", "lines", "=", "finp", ".", "read", "(", ")", ".", "split", "(", "\"\\n\"", ")", "\n", "", "missed_word", "=", "0", "\n", "word_to_index", ",", "index_to_word", "=", "{", "}", ",", "{", "}", "\n", "for", "line", "in", "lines", ":", "\n", "      ", "line", "=", "line", ".", "strip", "(", ")", "\n", "if", "not", "line", ":", "\n", "        ", "continue", "\n", "", "word_index", "=", "line", ".", "split", "(", "\" \"", ")", "\n", "if", "len", "(", "word_index", ")", "!=", "2", ":", "\n", "        ", "missed_word", "+=", "1", "\n", "print", "(", "\"Weird line: '{0}'. split_len={1}\"", ".", "format", "(", "line", ",", "len", "(", "word_index", ")", ")", ")", "\n", "continue", "\n", "", "word", ",", "index", "=", "word_index", "\n", "index", "=", "int", "(", "index", ")", "\n", "word_to_index", "[", "word", "]", "=", "index", "\n", "index_to_word", "[", "index", "]", "=", "word", "\n", "if", "word", "==", "self", ".", "hparams", ".", "unk", ":", "\n", "        ", "self", ".", "unk_id", "=", "index", "\n", "", "elif", "word", "==", "self", ".", "hparams", ".", "bos", ":", "\n", "        ", "self", ".", "bos_id", "=", "index", "\n", "", "elif", "word", "==", "self", ".", "hparams", ".", "eos", ":", "\n", "        ", "self", ".", "eos_id", "=", "index", "\n", "", "elif", "word", "==", "self", ".", "hparams", ".", "pad", ":", "\n", "        ", "self", ".", "pad_id", "=", "index", "\n", "\n", "", "", "assert", "len", "(", "word_to_index", ")", "==", "len", "(", "index_to_word", ")", ",", "(", "\n", "\"|word_to_index|={0} != |index_to_word|={1}\"", ".", "format", "(", "len", "(", "word_to_index", ")", ",", "\n", "len", "(", "index_to_word", ")", ")", ")", "\n", "print", "(", "\"Done. vocab_size = {0}\"", ".", "format", "(", "len", "(", "word_to_index", ")", "+", "missed_word", ")", ")", "\n", "\n", "return", "word_to_index", ",", "index_to_word", ",", "len", "(", "word_to_index", ")", "+", "missed_word", "\n", "\n"]], "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.data_utils.DataLoader._build_tree_vocab": [[520, 538], ["print", "print", "os.path.join", "os.path.join", "tree_utils.Vocab", "tree_utils.RuleVocab", "print", "print", "len", "len", "len"], "methods", ["None"], ["", "def", "_build_tree_vocab", "(", "self", ",", "rule_file_name", ",", "word_file_name", ")", ":", "\n", "    ", "\"\"\"Build word_to_index and index_to word dicts.\"\"\"", "\n", "print", "(", "\"-\"", "*", "80", ")", "\n", "print", "(", "\"Loading rule vocab from '{0}' and '{1}\"", ".", "format", "(", "rule_file_name", ",", "word_file_name", ")", ")", "\n", "rule_file_name", "=", "os", ".", "path", ".", "join", "(", "self", ".", "hparams", ".", "data_path", ",", "rule_file_name", ")", "\n", "word_file_name", "=", "os", ".", "path", ".", "join", "(", "self", ".", "hparams", ".", "data_path", ",", "word_file_name", ")", "\n", "\n", "word_vocab", "=", "Vocab", "(", "hparams", "=", "self", ".", "hparams", ",", "vocab_file", "=", "word_file_name", ")", "\n", "rule_vocab", "=", "RuleVocab", "(", "hparams", "=", "self", ".", "hparams", ",", "vocab_file", "=", "rule_file_name", ",", "offset", "=", "len", "(", "word_vocab", ")", ")", "\n", "\n", "self", ".", "unk_id", "=", "word_vocab", ".", "UNK", "\n", "self", ".", "bos_id", "=", "word_vocab", ".", "BS", "\n", "self", ".", "eos_id", "=", "word_vocab", ".", "ES", "\n", "self", ".", "pad_id", "=", "word_vocab", ".", "PAD", "\n", "\n", "print", "(", "\"Done. rule_vocab_size = {0}\"", ".", "format", "(", "len", "(", "rule_vocab", ")", ")", ")", "\n", "print", "(", "\"Done. word_vocab_size = {0}\"", ".", "format", "(", "len", "(", "word_vocab", ")", ")", ")", "\n", "return", "rule_vocab", ",", "word_vocab", "\n", "\n"]], "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.data_utils.DataLoader._build_tree_parallel": [[539, 661], ["print", "print", "os.path.join", "os.path.join", "os.path.join", "enumerate", "print", "print", "open", "finp.read().split", "open", "finp.read().split", "open", "finp.read().split", "zip", "source_line.strip.strip.strip", "target_line.strip.strip.strip", "trg_tree_line.strip.strip.strip", "source_line.strip.strip.split", "target_line.strip.strip.split", "tree_utils.Tree", "tree_utils.Tree.reset_timestep", "tree_utils.Tree.get_data_root", "numpy.array", "numpy.append", "trg_tree_indices.tolist.tolist.tolist", "source_lens.append", "trg_tree_lens.append", "source_data.append", "target_data.append", "trg_tree_data.append", "len", "len", "print", "numpy.argsort", "numpy.array", "numpy.array", "numpy.array", "source_indices.append", "data_utils.DataLoader.target_word_vocab.convert", "target_indices.append", "tree_utils.parse_root", "tree_utils.remove_preterminal_POS", "tree_utils.merge_depth", "hasattr", "tree_utils.add_preterminal_wordswitch", "tree_utils.sent_piece_segs", "tree_utils.split_sent_piece", "tree_utils.add_preterminal_wordswitch", "tree_utils.remove_lhs", "len", "len", "len", "print", "finp.read", "finp.read", "finp.read", "len", "tree_utils.tokenize"], "methods", ["home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.tree_utils.Tree.reset_timestep", "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.tree_utils.Tree.get_data_root", "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.tree_utils.RuleVocab.convert", "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.tree_utils.parse_root", "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.tree_utils.remove_preterminal_POS", "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.tree_utils.merge_depth", "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.tree_utils.add_preterminal_wordswitch", "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.tree_utils.sent_piece_segs", "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.tree_utils.split_sent_piece", "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.tree_utils.add_preterminal_wordswitch", "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.tree_utils.remove_lhs", "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.tree_utils.tokenize"], ["", "def", "_build_tree_parallel", "(", "self", ",", "source_file", ",", "target_file", ",", "trg_tree_file", ",", "is_training", ",", "sort", "=", "False", ")", ":", "\n", "    ", "\"\"\"Build pair of data.\"\"\"", "\n", "\n", "print", "(", "\"-\"", "*", "80", ")", "\n", "print", "(", "\"Loading parallel tree data from '{0}' and '{1}' and '{2}'\"", ".", "format", "(", "\n", "source_file", ",", "target_file", ",", "trg_tree_file", ")", ")", "\n", "\n", "source_file", "=", "os", ".", "path", ".", "join", "(", "self", ".", "hparams", ".", "data_path", ",", "source_file", ")", "\n", "with", "open", "(", "source_file", ",", "encoding", "=", "'utf-8'", ")", "as", "finp", ":", "\n", "      ", "source_lines", "=", "finp", ".", "read", "(", ")", ".", "split", "(", "\"\\n\"", ")", "\n", "\n", "", "target_file", "=", "os", ".", "path", ".", "join", "(", "self", ".", "hparams", ".", "data_path", ",", "target_file", ")", "\n", "with", "open", "(", "target_file", ",", "encoding", "=", "'utf-8'", ")", "as", "finp", ":", "\n", "      ", "target_lines", "=", "finp", ".", "read", "(", ")", ".", "split", "(", "\"\\n\"", ")", "\n", "\n", "", "trg_tree_file", "=", "os", ".", "path", ".", "join", "(", "self", ".", "hparams", ".", "data_path", ",", "trg_tree_file", ")", "\n", "with", "open", "(", "trg_tree_file", ",", "encoding", "=", "'utf-8'", ")", "as", "finp", ":", "\n", "      ", "trg_tree_lines", "=", "finp", ".", "read", "(", ")", ".", "split", "(", "\"\\n\"", ")", "\n", "\n", "", "source_data", ",", "target_data", ",", "trg_tree_data", "=", "[", "]", ",", "[", "]", ",", "[", "]", "\n", "source_lens", "=", "[", "]", "\n", "trg_tree_lens", "=", "[", "]", "\n", "total_sents", "=", "0", "\n", "truncate_sents", "=", "0", "\n", "source_unk_count", ",", "target_unk_count", ",", "trg_tree_unk_count", "=", "0", ",", "0", ",", "0", "\n", "for", "i", ",", "(", "source_line", ",", "target_line", ",", "trg_tree_line", ")", "in", "enumerate", "(", "\n", "zip", "(", "source_lines", ",", "target_lines", ",", "trg_tree_lines", ")", ")", ":", "\n", "      ", "source_line", "=", "source_line", ".", "strip", "(", ")", "\n", "target_line", "=", "target_line", ".", "strip", "(", ")", "\n", "trg_tree_line", "=", "trg_tree_line", ".", "strip", "(", ")", "\n", "if", "not", "source_line", "or", "not", "target_line", "or", "not", "trg_tree_line", ":", "\n", "        ", "continue", "\n", "\n", "", "source_indices", ",", "target_indices", "=", "[", "self", ".", "bos_id", "]", ",", "[", "self", ".", "bos_id", "]", "\n", "source_tokens", "=", "source_line", ".", "split", "(", ")", "\n", "target_tokens", "=", "target_line", ".", "split", "(", ")", "\n", "if", "is_training", "and", "len", "(", "target_line", ")", ">", "self", ".", "hparams", ".", "max_len", ":", "\n", "        ", "continue", "\n", "\n", "", "total_sents", "+=", "1", "\n", "\n", "for", "source_token", "in", "source_tokens", ":", "\n", "#source_token = source_token.strip()", "\n", "        ", "if", "source_token", "not", "in", "self", ".", "source_word_to_index", ":", "\n", "          ", "source_token", "=", "self", ".", "hparams", ".", "unk", "\n", "source_unk_count", "+=", "1", "\n", "#print(source_token)", "\n", "", "source_index", "=", "self", ".", "source_word_to_index", "[", "source_token", "]", "\n", "source_indices", ".", "append", "(", "source_index", ")", "\n", "\n", "", "for", "target_token", "in", "target_tokens", ":", "\n", "        ", "target_index", "=", "self", ".", "target_word_vocab", ".", "convert", "(", "target_token", ")", "\n", "target_indices", ".", "append", "(", "target_index", ")", "\n", "\n", "# Process tree", "\n", "", "tree", "=", "Tree", "(", "parse_root", "(", "tokenize", "(", "trg_tree_line", ")", ")", ")", "\n", "if", "self", ".", "hparams", ".", "pos", "==", "0", ":", "\n", "        ", "remove_preterminal_POS", "(", "tree", ".", "root", ")", "\n", "", "if", "self", ".", "hparams", ".", "max_tree_depth", ">", "0", ":", "\n", "        ", "merge_depth", "(", "tree", ".", "root", ",", "self", ".", "hparams", ".", "max_tree_depth", ",", "0", ")", "\n", "", "if", "hasattr", "(", "self", ".", "hparams", ",", "\"no_piece_tree\"", ")", "and", "self", ".", "hparams", ".", "no_piece_tree", ":", "\n", "        ", "add_preterminal_wordswitch", "(", "tree", ".", "root", ",", "add_eos", "=", "True", ")", "\n", "", "else", ":", "\n", "        ", "pieces", "=", "sent_piece_segs", "(", "target_line", ")", "\n", "split_sent_piece", "(", "tree", ".", "root", ",", "pieces", ",", "0", ")", "\n", "add_preterminal_wordswitch", "(", "tree", ".", "root", ",", "add_eos", "=", "True", ")", "\n", "", "if", "self", ".", "hparams", ".", "no_lhs", ":", "\n", "        ", "remove_lhs", "(", "tree", ".", "root", ",", "self", ".", "hparams", ".", "root_label", ")", "\n", "tree", ".", "root", ".", "label", "=", "'XXX'", "\n", "", "tree", ".", "reset_timestep", "(", ")", "\n", "trg_tree_indices", "=", "tree", ".", "get_data_root", "(", "self", ".", "target_tree_vocab", ",", "self", ".", "target_word_vocab", ")", "# (len_y, 3)", "\n", "trg_tree_indices", "=", "[", "[", "self", ".", "bos_id", ",", "0", ",", "1", "]", "]", "+", "trg_tree_indices", "\n", "trg_tree_indices", "=", "np", ".", "array", "(", "trg_tree_indices", ")", "\n", "trg_tree_indices", "[", ":", ",", "1", "]", "=", "np", ".", "append", "(", "trg_tree_indices", "[", "1", ":", ",", "1", "]", ",", "0", ")", "# parent timestep, last one not used", "\n", "trg_tree_indices", "=", "trg_tree_indices", ".", "tolist", "(", ")", "\n", "if", "len", "(", "trg_tree_indices", ")", ">", "self", ".", "hparams", ".", "max_tree_len", ":", "\n", "        ", "trg_tree_indices", "=", "trg_tree_indices", "[", ":", "self", ".", "hparams", ".", "max_tree_len", "]", "\n", "truncate_sents", "+=", "1", "\n", "#print(trg_tree_indices)", "\n", "#exit(0)", "\n", "#trg_tree_indices[:, 2] = np.append(trg_tree_indices[1:, 2], 0) # is word, last one not used in training", "\n", "#trg_tree_indices = trg_tree_indices.tolist()", "\n", "#for data in trg_tree_indices:", "\n", "#  idx, paren_t, is_word = data ", "\n", "#  if is_word:", "\n", "#    print(self.target_word_vocab[idx], data)", "\n", "#  else:", "\n", "#    print(self.target_tree_vocab[idx], data)", "\n", "#exit(0)", "\n", "", "source_indices", "+=", "[", "self", ".", "eos_id", "]", "\n", "target_indices", "+=", "[", "self", ".", "eos_id", "]", "\n", "#assert source_indices[-1] == self.eos_id", "\n", "#assert target_indices[-1] == self.eos_id", "\n", "\n", "source_lens", ".", "append", "(", "len", "(", "source_indices", ")", ")", "\n", "trg_tree_lens", ".", "append", "(", "len", "(", "trg_tree_indices", ")", ")", "\n", "source_data", ".", "append", "(", "source_indices", ")", "\n", "target_data", ".", "append", "(", "target_indices", ")", "\n", "trg_tree_data", ".", "append", "(", "trg_tree_indices", ")", "\n", "#print(trg_tree_indices)", "\n", "if", "(", "self", ".", "hparams", ".", "n_train_sents", "is", "not", "None", "and", "\n", "self", ".", "hparams", ".", "n_train_sents", "<=", "total_sents", ")", ":", "\n", "        ", "break", "\n", "\n", "", "if", "total_sents", "%", "10000", "==", "0", ":", "\n", "        ", "print", "(", "\"{0:>6d} pairs. src_unk={1}. tgt_unk={2}\"", ".", "format", "(", "\n", "total_sents", ",", "source_unk_count", ",", "target_unk_count", ")", ")", "\n", "\n", "", "", "assert", "len", "(", "source_data", ")", "==", "len", "(", "target_data", ")", "\n", "print", "(", "\"{0:>6d} pairs. src_unk={1}. tgt_unk={2}\"", ".", "format", "(", "\n", "total_sents", ",", "source_unk_count", ",", "target_unk_count", ")", ")", "\n", "\n", "print", "(", "\"truncated sents={}\"", ".", "format", "(", "truncate_sents", ")", ")", "\n", "if", "sort", ":", "\n", "      ", "print", "(", "\"Heuristic sort based on source lens\"", ")", "\n", "#print(\"Heuristic sort based on tree lens\")", "\n", "indices", "=", "np", ".", "argsort", "(", "source_lens", ")", "\n", "#indices = np.argsort(trg_tree_lens)", "\n", "source_data", "=", "[", "source_data", "[", "index", "]", "for", "index", "in", "indices", "]", "\n", "target_data", "=", "[", "target_data", "[", "index", "]", "for", "index", "in", "indices", "]", "\n", "trg_tree_data", "=", "[", "trg_tree_data", "[", "index", "]", "for", "index", "in", "indices", "]", "\n", "", "return", "np", ".", "array", "(", "source_data", ")", ",", "np", ".", "array", "(", "target_data", ")", ",", "np", ".", "array", "(", "trg_tree_data", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.trdec_attn.TreeDecoderAttn.__init__": [[14, 79], ["torch.nn.Module.__init__", "torch.nn.Embedding", "torch.nn.Embedding", "torch.nn.Embedding", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.LSTMCell", "torch.nn.LSTMCell", "torch.nn.LSTMCell", "torch.nn.LSTMCell", "torch.nn.LSTMCell", "torch.nn.LSTMCell", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros.index_fill_", "torch.zeros.index_fill_", "torch.zeros.index_fill_", "models.MlpAttn", "models.MlpAttn", "models.DotProdAttn", "models.DotProdAttn", "models.MlpAttn", "models.MlpAttn", "hasattr", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "trdec_attn.TreeDecoderAttn.rule_vocab_mask.cuda", "trdec_attn.TreeDecoderAttn.word_vocab_mask.cuda", "trdec_attn.TreeDecoderAttn.emb.cuda", "trdec_attn.TreeDecoderAttn.rule_attention.cuda", "trdec_attn.TreeDecoderAttn.word_attention.cuda", "trdec_attn.TreeDecoderAttn.rule_to_word_attn.cuda", "trdec_attn.TreeDecoderAttn.word_to_rule_attn.cuda", "trdec_attn.TreeDecoderAttn.rule_ctx_to_readout.cuda", "trdec_attn.TreeDecoderAttn.word_ctx_to_readout.cuda", "trdec_attn.TreeDecoderAttn.readout.cuda", "trdec_attn.TreeDecoderAttn.rule_lstm_cell.cuda", "trdec_attn.TreeDecoderAttn.word_lstm_cell.cuda", "trdec_attn.TreeDecoderAttn.dropout.cuda", "models.DotProdAttn", "models.DotProdAttn", "print", "models.MultiHeadAttn", "models.MultiHeadAttn", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange"], "methods", ["home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.utils.Logger.__init__"], ["  ", "def", "__init__", "(", "self", ",", "hparams", ")", ":", "\n", "    ", "super", "(", "TreeDecoderAttn", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "hparams", "=", "hparams", "\n", "self", ".", "target_vocab_size", "=", "self", ".", "hparams", ".", "target_word_vocab_size", "+", "self", ".", "hparams", ".", "target_rule_vocab_size", "\n", "self", ".", "emb", "=", "nn", ".", "Embedding", "(", "self", ".", "target_vocab_size", ",", "\n", "self", ".", "hparams", ".", "d_word_vec", ",", "\n", "padding_idx", "=", "hparams", ".", "pad_id", ")", "\n", "if", "self", ".", "hparams", ".", "attn", "==", "\"mlp\"", ":", "\n", "      ", "self", ".", "rule_attention", "=", "MlpAttn", "(", "hparams", ")", "\n", "self", ".", "word_attention", "=", "MlpAttn", "(", "hparams", ")", "\n", "", "else", ":", "\n", "      ", "self", ".", "rule_attention", "=", "DotProdAttn", "(", "hparams", ")", "\n", "self", ".", "word_attention", "=", "DotProdAttn", "(", "hparams", ")", "\n", "\n", "", "if", "self", ".", "hparams", ".", "self_attn", "==", "\"mlp\"", ":", "\n", "      ", "self", ".", "rule_to_word_attn", "=", "MlpAttn", "(", "hparams", ")", "\n", "self", ".", "word_to_rule_attn", "=", "MlpAttn", "(", "hparams", ")", "\n", "", "elif", "self", ".", "hparams", ".", "self_attn", "==", "\"dot_prod\"", ":", "\n", "      ", "self", ".", "rule_to_word_attn", "=", "DotProdAttn", "(", "hparams", ")", "\n", "self", ".", "word_to_rule_attn", "=", "DotProdAttn", "(", "hparams", ")", "\n", "", "else", ":", "\n", "      ", "print", "(", "\"mult head attn\"", ")", "\n", "self", ".", "rule_to_word_attn", "=", "MultiHeadAttn", "(", "hparams", ")", "\n", "self", ".", "word_to_rule_attn", "=", "MultiHeadAttn", "(", "hparams", ")", "\n", "\n", "", "readout_in_dim", "=", "hparams", ".", "d_model", "*", "6", "\n", "if", "hasattr", "(", "self", ".", "hparams", ",", "\"self_attn_input_feed\"", ")", "and", "self", ".", "hparams", ".", "self_attn_input_feed", ":", "\n", "      ", "readout_in_dim", "+=", "hparams", ".", "d_model", "*", "2", "\n", "# transform [word_ctx, word_h_t, rule_ctx, rule_h_t] to readout state vectors before softmax", "\n", "", "self", ".", "rule_ctx_to_readout", "=", "nn", ".", "Linear", "(", "readout_in_dim", ",", "hparams", ".", "d_model", ",", "bias", "=", "False", ")", "\n", "self", ".", "word_ctx_to_readout", "=", "nn", ".", "Linear", "(", "readout_in_dim", ",", "hparams", ".", "d_model", ",", "bias", "=", "False", ")", "\n", "\n", "self", ".", "readout", "=", "nn", ".", "Linear", "(", "hparams", ".", "d_model", ",", "\n", "self", ".", "target_vocab_size", ",", "\n", "bias", "=", "False", ")", "\n", "if", "self", ".", "hparams", ".", "share_emb_softmax", ":", "\n", "      ", "self", ".", "emb", ".", "weight", "=", "self", ".", "readout", ".", "weight", "\n", "# input: [y_t-1, input_feed, word_state_ctx]", "\n", "", "rule_inp", "=", "hparams", ".", "d_word_vec", "+", "hparams", ".", "d_model", "*", "3", "\n", "# input: [y_t-1, input_feed, rule_state_ctx]", "\n", "word_inp", "=", "hparams", ".", "d_word_vec", "+", "hparams", ".", "d_model", "*", "3", "\n", "if", "self", ".", "hparams", ".", "parent_feed", ":", "\n", "      ", "word_inp", "+=", "hparams", ".", "d_model", "\n", "", "self", ".", "rule_lstm_cell", "=", "nn", ".", "LSTMCell", "(", "rule_inp", ",", "hparams", ".", "d_model", ")", "\n", "self", ".", "word_lstm_cell", "=", "nn", ".", "LSTMCell", "(", "word_inp", ",", "hparams", ".", "d_model", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "hparams", ".", "dropout", ")", "\n", "\n", "vocab_mask", "=", "torch", ".", "zeros", "(", "1", ",", "1", ",", "self", ".", "target_vocab_size", ")", "\n", "self", ".", "word_vocab_mask", "=", "vocab_mask", ".", "index_fill_", "(", "2", ",", "torch", ".", "arange", "(", "self", ".", "hparams", ".", "target_word_vocab_size", ")", ".", "long", "(", ")", ",", "1", ")", "\n", "self", ".", "rule_vocab_mask", "=", "1", "-", "self", ".", "word_vocab_mask", "\n", "if", "self", ".", "hparams", ".", "cuda", ":", "\n", "      ", "self", ".", "rule_vocab_mask", "=", "self", ".", "rule_vocab_mask", ".", "cuda", "(", ")", "\n", "self", ".", "word_vocab_mask", "=", "self", ".", "word_vocab_mask", ".", "cuda", "(", ")", "\n", "self", ".", "emb", "=", "self", ".", "emb", ".", "cuda", "(", ")", "\n", "self", ".", "rule_attention", "=", "self", ".", "rule_attention", ".", "cuda", "(", ")", "\n", "self", ".", "word_attention", "=", "self", ".", "word_attention", ".", "cuda", "(", ")", "\n", "self", ".", "rule_to_word_attn", "=", "self", ".", "rule_to_word_attn", ".", "cuda", "(", ")", "\n", "self", ".", "word_to_rule_attn", "=", "self", ".", "word_to_rule_attn", ".", "cuda", "(", ")", "\n", "\n", "self", ".", "rule_ctx_to_readout", "=", "self", ".", "rule_ctx_to_readout", ".", "cuda", "(", ")", "\n", "self", ".", "word_ctx_to_readout", "=", "self", ".", "word_ctx_to_readout", ".", "cuda", "(", ")", "\n", "self", ".", "readout", "=", "self", ".", "readout", ".", "cuda", "(", ")", "\n", "self", ".", "rule_lstm_cell", "=", "self", ".", "rule_lstm_cell", ".", "cuda", "(", ")", "\n", "self", ".", "word_lstm_cell", "=", "self", ".", "word_lstm_cell", ".", "cuda", "(", ")", "\n", "self", ".", "dropout", "=", "self", ".", "dropout", ".", "cuda", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.trdec_attn.TreeDecoderAttn.forward": [[80, 204], ["x_enc.size", "y_train.size", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.ones().byte", "torch.ones().byte", "torch.ones().byte", "torch.ones().byte", "torch.ones().byte", "torch.ones().byte", "torch.ones().byte", "torch.ones().byte", "torch.ones().byte", "torch.ones().byte", "torch.ones().byte", "torch.ones().byte", "torch.ones().byte", "torch.ones().byte", "torch.ones().byte", "torch.ones().byte", "torch.ones().byte", "torch.ones().byte", "word_states_mask.cuda.cuda.index_fill_", "rule_states_mask.cuda.cuda.index_fill_", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "trdec_attn.TreeDecoderAttn.emb", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "range", "torch.cat().transpose().contiguous", "torch.cat().transpose().contiguous", "torch.cat().transpose().contiguous", "torch.cat().transpose().contiguous", "torch.cat().transpose().contiguous", "torch.cat().transpose().contiguous", "torch.cat().transpose().contiguous", "torch.cat().transpose().contiguous", "torch.cat().transpose().contiguous", "torch.cat().transpose().contiguous.data.masked_fill_", "torch.cat().transpose().contiguous.data.masked_fill_", "torch.cat().transpose().contiguous.data.masked_fill_", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "input_feed_zeros.cuda.cuda.cuda", "rule_states.cuda.cuda.cuda", "word_states.cuda.cuda.cuda", "rule_states_mask.cuda.cuda.cuda", "word_states_mask.cuda.cuda.cuda", "rule_input_feed.cuda.cuda.cuda", "word_input_feed.cuda.cuda.cuda", "rule_to_word_input_feed.cuda.cuda.cuda", "word_to_rule_input_feed.cuda.cuda.cuda", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "offset.cuda.cuda.cuda", "y_train[].unsqueeze().float", "trdec_attn.TreeDecoderAttn.word_lstm_cell", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "trdec_attn.TreeDecoderAttn.rule_lstm_cell", "trdec_attn.TreeDecoderAttn.rule_to_word_attn", "trdec_attn.TreeDecoderAttn.word_to_rule_attn", "trdec_attn.TreeDecoderAttn.rule_attention", "trdec_attn.TreeDecoderAttn.word_attention", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "trdec_attn.TreeDecoderAttn.dropout", "trdec_attn.TreeDecoderAttn.dropout", "rule_pre_readouts.append", "word_pre_readouts.append", "trdec_attn.TreeDecoderAttn.readout", "trdec_attn.TreeDecoderAttn.readout", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "score_mask.unsqueeze().float", "mask_t.byte", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.cat.cuda", "torch.cat.cuda", "torch.cat.cuda", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.index_select", "torch.index_select", "torch.index_select", "torch.index_select", "torch.index_select", "torch.index_select", "torch.index_select", "torch.index_select", "torch.index_select", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "hasattr", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "trdec_attn.TreeDecoderAttn.rule_ctx_to_readout", "trdec_attn.TreeDecoderAttn.word_ctx_to_readout", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.cat().transpose", "torch.cat().transpose", "torch.cat().transpose", "torch.cat().transpose", "torch.cat().transpose", "torch.cat().transpose", "torch.cat().transpose", "torch.cat().transpose", "torch.cat().transpose", "float", "torch.cat.view", "torch.cat.view", "torch.cat.view", "y_train[].unsqueeze", "y_train[].unsqueeze().float.byte", "torch.softmax", "torch.softmax", "torch.softmax", "score_mask.unsqueeze", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "x_enc", ",", "x_enc_k", ",", "dec_init", ",", "x_mask", ",", "y_train", ",", "y_mask", ",", "score_mask", ",", "y_label", "=", "None", ")", ":", "\n", "# get decoder init state and cell, use x_ct", "\n", "    ", "\"\"\"\n    x_enc: [batch_size, max_x_len, d_model * 2]\n    \"\"\"", "\n", "batch_size_x", ",", "x_max_len", ",", "d_x", "=", "x_enc", ".", "size", "(", ")", "\n", "batch_size", ",", "y_max_len", ",", "data_len", "=", "y_train", ".", "size", "(", ")", "\n", "assert", "batch_size_x", "==", "batch_size", "\n", "#print(y_train)", "\n", "input_feed_zeros", "=", "torch", ".", "zeros", "(", "batch_size", ",", "self", ".", "hparams", ".", "d_model", "*", "2", ")", "\n", "input_feed_zeros_d", "=", "torch", ".", "zeros", "(", "batch_size", ",", "self", ".", "hparams", ".", "d_model", ")", "\n", "rule_states", "=", "Variable", "(", "torch", ".", "zeros", "(", "batch_size", ",", "y_max_len", ",", "self", ".", "hparams", ".", "d_model", ")", ")", "\n", "word_states", "=", "Variable", "(", "torch", ".", "zeros", "(", "batch_size", ",", "y_max_len", ",", "self", ".", "hparams", ".", "d_model", ")", ")", "\n", "rule_states_mask", "=", "torch", ".", "ones", "(", "batch_size", ",", "y_max_len", ")", ".", "byte", "(", ")", "\n", "word_states_mask", "=", "torch", ".", "ones", "(", "batch_size", ",", "y_max_len", ")", ".", "byte", "(", ")", "\n", "# avoid attn nan", "\n", "word_states_mask", ".", "index_fill_", "(", "1", ",", "torch", ".", "LongTensor", "(", "[", "0", "]", ")", ",", "0", ")", "\n", "rule_states_mask", ".", "index_fill_", "(", "1", ",", "torch", ".", "LongTensor", "(", "[", "0", "]", ")", ",", "0", ")", "\n", "if", "self", ".", "hparams", ".", "cuda", ":", "\n", "      ", "input_feed_zeros", "=", "input_feed_zeros", ".", "cuda", "(", ")", "\n", "rule_states", "=", "rule_states", ".", "cuda", "(", ")", "\n", "word_states", "=", "word_states", ".", "cuda", "(", ")", "\n", "rule_states_mask", "=", "rule_states_mask", ".", "cuda", "(", ")", "\n", "word_states_mask", "=", "word_states_mask", ".", "cuda", "(", ")", "\n", "\n", "", "rule_hidden", "=", "dec_init", "\n", "word_hidden", "=", "dec_init", "\n", "rule_input_feed", "=", "Variable", "(", "input_feed_zeros", ",", "requires_grad", "=", "False", ")", "\n", "word_input_feed", "=", "Variable", "(", "input_feed_zeros", ",", "requires_grad", "=", "False", ")", "\n", "rule_to_word_input_feed", "=", "Variable", "(", "input_feed_zeros_d", ",", "requires_grad", "=", "False", ")", "\n", "word_to_rule_input_feed", "=", "Variable", "(", "input_feed_zeros_d", ",", "requires_grad", "=", "False", ")", "\n", "if", "self", ".", "hparams", ".", "cuda", ":", "\n", "      ", "rule_input_feed", "=", "rule_input_feed", ".", "cuda", "(", ")", "\n", "word_input_feed", "=", "word_input_feed", ".", "cuda", "(", ")", "\n", "rule_to_word_input_feed", "=", "rule_to_word_input_feed", ".", "cuda", "(", ")", "\n", "word_to_rule_input_feed", "=", "word_to_rule_input_feed", ".", "cuda", "(", ")", "\n", "# [batch_size, y_len, d_word_vec]", "\n", "", "trg_emb", "=", "self", ".", "emb", "(", "y_train", "[", ":", ",", ":", ",", "0", "]", ")", "\n", "logits", "=", "[", "]", "\n", "rule_pre_readouts", "=", "[", "]", "\n", "word_pre_readouts", "=", "[", "]", "\n", "if", "self", ".", "hparams", ".", "parent_feed", ":", "\n", "      ", "all_state", "=", "Variable", "(", "torch", ".", "zeros", "(", "batch_size", ",", "self", ".", "hparams", ".", "d_model", ")", ",", "requires_grad", "=", "False", ")", "\n", "", "offset", "=", "torch", ".", "arange", "(", "batch_size", ")", ".", "long", "(", ")", "\n", "if", "self", ".", "hparams", ".", "cuda", ":", "\n", "      ", "offset", "=", "offset", ".", "cuda", "(", ")", "\n", "if", "self", ".", "hparams", ".", "parent_feed", ":", "\n", "        ", "all_state", "=", "all_state", ".", "cuda", "(", ")", "\n", "", "", "for", "t", "in", "range", "(", "y_max_len", ")", ":", "\n", "      ", "y_emb_tm1", "=", "trg_emb", "[", ":", ",", "t", ",", ":", "]", "\n", "\n", "if", "self", ".", "hparams", ".", "parent_feed", ":", "\n", "        ", "state_idx_t", "=", "t", "\n", "state_idx_t", "+=", "1", "\n", "parent_t", "=", "y_train", ".", "data", "[", ":", ",", "t", ",", "1", "]", "+", "state_idx_t", "*", "offset", "# [batch_size,]", "\n", "parent_t", "=", "Variable", "(", "parent_t", ",", "requires_grad", "=", "False", ")", "\n", "parent_state", "=", "torch", ".", "index_select", "(", "all_state", ".", "view", "(", "state_idx_t", "*", "batch_size", ",", "self", ".", "hparams", ".", "d_model", ")", ",", "dim", "=", "0", ",", "index", "=", "parent_t", ")", "# [batch_size, d_model]", "\n", "\n", "", "word_mask", "=", "y_train", "[", ":", ",", "t", ",", "2", "]", ".", "unsqueeze", "(", "1", ")", ".", "float", "(", ")", "# (1 is word, 0 is rule)", "\n", "\n", "if", "self", ".", "hparams", ".", "parent_feed", ":", "\n", "        ", "word_input", "=", "torch", ".", "cat", "(", "[", "y_emb_tm1", ",", "word_input_feed", ",", "word_to_rule_input_feed", ",", "parent_state", "]", ",", "dim", "=", "1", ")", "\n", "", "else", ":", "\n", "        ", "word_input", "=", "torch", ".", "cat", "(", "[", "y_emb_tm1", ",", "word_input_feed", ",", "word_to_rule_input_feed", "]", ",", "dim", "=", "1", ")", "\n", "", "word_h_t", ",", "word_c_t", "=", "self", ".", "word_lstm_cell", "(", "word_input", ",", "word_hidden", ")", "\n", "\n", "word_h_t", "=", "word_h_t", "*", "word_mask", "+", "word_hidden", "[", "0", "]", "*", "(", "1", "-", "word_mask", ")", "\n", "word_c_t", "=", "word_c_t", "*", "word_mask", "+", "word_hidden", "[", "1", "]", "*", "(", "1", "-", "word_mask", ")", "\n", "\n", "rule_input", "=", "torch", ".", "cat", "(", "[", "y_emb_tm1", ",", "rule_input_feed", ",", "rule_to_word_input_feed", "]", ",", "dim", "=", "1", ")", "\n", "rule_h_t", ",", "rule_c_t", "=", "self", ".", "rule_lstm_cell", "(", "rule_input", ",", "rule_hidden", ")", "\n", "\n", "eos_mask", "=", "(", "y_train", "[", ":", ",", "t", ",", "0", "]", "==", "self", ".", "hparams", ".", "eos_id", ")", ".", "unsqueeze", "(", "1", ")", ".", "float", "(", ")", "\n", "word_mask", "=", "word_mask", "-", "eos_mask", "\n", "rule_h_t", "=", "rule_h_t", "*", "(", "1", "-", "word_mask", ")", "+", "rule_hidden", "[", "0", "]", "*", "word_mask", "\n", "rule_c_t", "=", "rule_c_t", "*", "(", "1", "-", "word_mask", ")", "+", "rule_hidden", "[", "1", "]", "*", "word_mask", "\n", "\n", "rule_states", ".", "data", "[", ":", ",", "t", ",", ":", "]", "=", "rule_h_t", ".", "data", "\n", "word_states", ".", "data", "[", ":", ",", "t", ",", ":", "]", "=", "word_h_t", ".", "data", "\n", "# word_mask: 1 is word, 0 is rule", "\n", "if", "t", ">", "0", ":", "\n", "        ", "rule_states_mask", "[", ":", ",", "t", "]", "=", "word_mask", ".", "byte", "(", ")", ".", "data", "\n", "word_states_mask", "[", ":", ",", "t", "]", "=", "(", "1", "-", "word_mask", ")", ".", "byte", "(", ")", ".", "data", "\n", "\n", "", "rule_to_word_ctx", "=", "self", ".", "rule_to_word_attn", "(", "rule_h_t", ",", "word_states", ",", "word_states", ",", "attn_mask", "=", "word_states_mask", ")", "\n", "word_to_rule_ctx", "=", "self", ".", "word_to_rule_attn", "(", "word_h_t", ",", "rule_states", ",", "rule_states", ",", "attn_mask", "=", "rule_states_mask", ")", "\n", "rule_ctx", "=", "self", ".", "rule_attention", "(", "rule_h_t", ",", "x_enc_k", ",", "x_enc", ",", "attn_mask", "=", "x_mask", ")", "\n", "word_ctx", "=", "self", ".", "word_attention", "(", "word_h_t", ",", "x_enc_k", ",", "x_enc", ",", "attn_mask", "=", "x_mask", ")", "\n", "\n", "if", "hasattr", "(", "self", ".", "hparams", ",", "\"self_attn_input_feed\"", ")", "and", "self", ".", "hparams", ".", "self_attn_input_feed", ":", "\n", "        ", "inp", "=", "torch", ".", "cat", "(", "[", "rule_h_t", ",", "rule_ctx", ",", "word_h_t", ",", "word_ctx", ",", "rule_to_word_input_feed", ",", "word_to_rule_input_feed", "]", ",", "dim", "=", "1", ")", "\n", "", "else", ":", "\n", "        ", "inp", "=", "torch", ".", "cat", "(", "[", "rule_h_t", ",", "rule_ctx", ",", "word_h_t", ",", "word_ctx", "]", ",", "dim", "=", "1", ")", "\n", "", "rule_pre_readout", "=", "F", ".", "tanh", "(", "self", ".", "rule_ctx_to_readout", "(", "inp", ")", ")", "\n", "word_pre_readout", "=", "F", ".", "tanh", "(", "self", ".", "word_ctx_to_readout", "(", "inp", ")", ")", "\n", "\n", "rule_pre_readout", "=", "self", ".", "dropout", "(", "rule_pre_readout", ")", "\n", "word_pre_readout", "=", "self", ".", "dropout", "(", "word_pre_readout", ")", "\n", "\n", "rule_pre_readouts", ".", "append", "(", "rule_pre_readout", ")", "\n", "word_pre_readouts", ".", "append", "(", "word_pre_readout", ")", "\n", "\n", "rule_input_feed", "=", "rule_ctx", "\n", "word_input_feed", "=", "word_ctx", "\n", "rule_to_word_input_feed", "=", "rule_to_word_ctx", "\n", "word_to_rule_input_feed", "=", "word_to_rule_ctx", "\n", "\n", "rule_hidden", "=", "(", "rule_h_t", ",", "rule_c_t", ")", "\n", "word_hidden", "=", "(", "word_h_t", ",", "word_c_t", ")", "\n", "if", "self", ".", "hparams", ".", "parent_feed", ":", "\n", "        ", "all_state", "=", "torch", ".", "cat", "(", "[", "all_state", ",", "rule_h_t", "]", ",", "dim", "=", "1", ")", "\n", "# [len_y, batch_size, trg_vocab_size]", "\n", "", "", "rule_readouts", "=", "self", ".", "readout", "(", "torch", ".", "stack", "(", "rule_pre_readouts", ")", ")", "[", ":", ",", ":", ",", "-", "self", ".", "hparams", ".", "target_rule_vocab_size", ":", "]", "\n", "word_readouts", "=", "self", ".", "readout", "(", "torch", ".", "stack", "(", "word_pre_readouts", ")", ")", "[", ":", ",", ":", ",", ":", "self", ".", "hparams", ".", "target_word_vocab_size", "]", "\n", "if", "self", ".", "hparams", ".", "label_smooth", ">", "0", ":", "\n", "      ", "smooth", "=", "self", ".", "hparams", ".", "label_smooth", "\n", "rule_probs", "=", "(", "1.0", "-", "smooth", ")", "*", "F", ".", "softmax", "(", "rule_readouts", ",", "dim", "=", "2", ")", "+", "smooth", "/", "self", ".", "hparams", ".", "target_rule_vocab_size", "\n", "rule_readouts", "=", "torch", ".", "log", "(", "rule_probs", ")", "\n", "# [batch_size, len_y, trg_vocab_size]", "\n", "", "logits", "=", "torch", ".", "cat", "(", "[", "word_readouts", ",", "rule_readouts", "]", ",", "dim", "=", "2", ")", ".", "transpose", "(", "0", ",", "1", ")", ".", "contiguous", "(", ")", "\n", "score_mask", "=", "score_mask", ".", "unsqueeze", "(", "2", ")", ".", "float", "(", ")", ".", "data", "\n", "mask_t", "=", "self", ".", "word_vocab_mask", "*", "(", "1", "-", "score_mask", ")", "+", "self", ".", "rule_vocab_mask", "*", "score_mask", "\n", "logits", ".", "data", ".", "masked_fill_", "(", "mask_t", ".", "byte", "(", ")", ",", "-", "float", "(", "\"inf\"", ")", ")", "\n", "return", "logits", "\n", "\n"]], "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.trdec_attn.TreeDecoderAttn.step": [[205, 282], ["torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "trdec_attn.TreeDecoderAttn.emb", "trdec_attn.TreeDecoderAttn.rule_attention", "trdec_attn.TreeDecoderAttn.word_attention", "trdec_attn.TreeDecoderAttn.rule_to_word_attn", "trdec_attn.TreeDecoderAttn.word_to_rule_attn", "torch.ones().byte", "torch.ones().byte", "torch.ones().byte", "torch.ones().byte", "torch.ones().byte", "torch.ones().byte", "torch.ones().byte", "torch.ones().byte", "torch.ones().byte", "trdec_attn.TreeDecoderAttn.data.masked_fill_", "y_tm1.cuda.cuda.cuda", "trdec_attn.TreeDecoderAttn.word_lstm_cell", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "trdec_attn.TreeDecoderAttn.rule_lstm_cell", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "hasattr", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "mask.cuda.cuda.index_fill_", "torch.tanh", "torch.tanh", "torch.tanh", "trdec_attn.TreeDecoderAttn.dropout", "trdec_attn.TreeDecoderAttn.readout", "target_rule_vocab.rule_index_with_lhs", "len", "mask.cuda.cuda.index_fill_", "torch.tanh", "torch.tanh", "torch.tanh", "trdec_attn.TreeDecoderAttn.dropout", "trdec_attn.TreeDecoderAttn.readout", "mask.cuda.cuda.cuda", "int", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "rule_h_t.unsqueeze", "word_h_t.unsqueeze", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "trdec_attn.TreeDecoderAttn.word_ctx_to_readout", "rule_select_index.append", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "trdec_attn.TreeDecoderAttn.rule_ctx_to_readout", "float", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "rule_h_t.unsqueeze", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "word_h_t.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.tree_utils.RuleVocab.rule_index_with_lhs"], ["", "def", "step", "(", "self", ",", "x_enc", ",", "x_enc_k", ",", "hyp", ",", "target_rule_vocab", ")", ":", "\n", "    ", "y_tm1", "=", "torch", ".", "LongTensor", "(", "[", "int", "(", "hyp", ".", "y", "[", "-", "1", "]", ")", "]", ")", "\n", "if", "self", ".", "hparams", ".", "cuda", ":", "\n", "      ", "y_tm1", "=", "y_tm1", ".", "cuda", "(", ")", "\n", "", "y_tm1", "=", "Variable", "(", "y_tm1", ",", "volatile", "=", "True", ")", "\n", "open_nonterms", "=", "hyp", ".", "open_nonterms", "\n", "rule_input_feed", "=", "hyp", ".", "rule_ctx_tm1", "\n", "word_input_feed", "=", "hyp", ".", "word_ctx_tm1", "\n", "rule_to_word_input_feed", "=", "hyp", ".", "rule_to_word_ctx_tm1", "\n", "word_to_rule_input_feed", "=", "hyp", ".", "word_to_rule_ctx_tm1", "\n", "rule_hidden", "=", "hyp", ".", "rule_hidden", "\n", "word_hidden", "=", "hyp", ".", "word_hidden", "\n", "word_h_t", ",", "word_c_t", "=", "word_hidden", "\n", "rule_h_t", ",", "rule_c_t", "=", "rule_hidden", "\n", "\n", "y_emb_tm1", "=", "self", ".", "emb", "(", "y_tm1", ")", "\n", "cur_nonterm", "=", "open_nonterms", "[", "-", "1", "]", "\n", "\n", "if", "hyp", ".", "y", "[", "-", "1", "]", "<", "self", ".", "hparams", ".", "target_word_vocab_size", ":", "\n", "# word", "\n", "      ", "if", "self", ".", "hparams", ".", "parent_feed", ":", "\n", "        ", "word_input", "=", "torch", ".", "cat", "(", "[", "y_emb_tm1", ",", "word_input_feed", ",", "word_to_rule_input_feed", ",", "hyp", ".", "parent_state", "]", ",", "dim", "=", "1", ")", "\n", "", "else", ":", "\n", "        ", "word_input", "=", "torch", ".", "cat", "(", "[", "y_emb_tm1", ",", "word_input_feed", ",", "word_to_rule_input_feed", "]", ",", "dim", "=", "1", ")", "\n", "", "word_h_t", ",", "word_c_t", "=", "self", ".", "word_lstm_cell", "(", "word_input", ",", "word_hidden", ")", "\n", "if", "hyp", ".", "rule_states", "is", "None", ":", "\n", "        ", "hyp", ".", "rule_states", "=", "rule_h_t", ".", "unsqueeze", "(", "1", ")", "\n", "", "if", "hyp", ".", "word_states", "is", "None", ":", "\n", "# [1, 1, d_model]", "\n", "        ", "hyp", ".", "word_states", "=", "word_h_t", ".", "unsqueeze", "(", "1", ")", "\n", "", "else", ":", "\n", "        ", "if", "hyp", ".", "y", "[", "-", "1", "]", "!=", "self", ".", "hparams", ".", "eos_id", ":", "\n", "          ", "hyp", ".", "word_states", "=", "torch", ".", "cat", "(", "[", "hyp", ".", "word_states", ",", "word_h_t", ".", "unsqueeze", "(", "1", ")", "]", ",", "dim", "=", "1", ")", "\n", "\n", "", "", "", "if", "hyp", ".", "y", "[", "-", "1", "]", "==", "self", ".", "hparams", ".", "eos_id", "or", "hyp", ".", "y", "[", "-", "1", "]", ">=", "self", ".", "hparams", ".", "target_word_vocab_size", ":", "\n", "      ", "rule_input", "=", "torch", ".", "cat", "(", "[", "y_emb_tm1", ",", "rule_input_feed", ",", "rule_to_word_input_feed", "]", ",", "dim", "=", "1", ")", "\n", "rule_h_t", ",", "rule_c_t", "=", "self", ".", "rule_lstm_cell", "(", "rule_input", ",", "rule_hidden", ")", "\n", "hyp", ".", "rule_states", "=", "torch", ".", "cat", "(", "[", "hyp", ".", "rule_states", ",", "rule_h_t", ".", "unsqueeze", "(", "1", ")", "]", ",", "dim", "=", "1", ")", "\n", "\n", "", "hyp", ".", "rule_ctx_tm1", "=", "self", ".", "rule_attention", "(", "rule_h_t", ",", "x_enc_k", ",", "x_enc", ")", "\n", "hyp", ".", "word_ctx_tm1", "=", "self", ".", "word_attention", "(", "word_h_t", ",", "x_enc_k", ",", "x_enc", ")", "\n", "hyp", ".", "rule_to_word_ctx_tm1", "=", "self", ".", "rule_to_word_attn", "(", "rule_h_t", ",", "hyp", ".", "word_states", ",", "hyp", ".", "word_states", ")", "\n", "hyp", ".", "word_to_rule_ctx_tm1", "=", "self", ".", "word_to_rule_attn", "(", "word_h_t", ",", "hyp", ".", "rule_states", ",", "hyp", ".", "rule_states", ")", "\n", "\n", "if", "hasattr", "(", "self", ".", "hparams", ",", "\"self_attn_input_feed\"", ")", "and", "self", ".", "hparams", ".", "self_attn_input_feed", ":", "\n", "      ", "inp", "=", "torch", ".", "cat", "(", "[", "rule_h_t", ",", "hyp", ".", "rule_ctx_tm1", ",", "word_h_t", ",", "hyp", ".", "word_ctx_tm1", ",", "hyp", ".", "rule_to_word_ctx_tm1", ",", "hyp", ".", "word_to_rule_ctx_tm1", "]", ",", "dim", "=", "1", ")", "\n", "", "else", ":", "\n", "      ", "inp", "=", "torch", ".", "cat", "(", "[", "rule_h_t", ",", "hyp", ".", "rule_ctx_tm1", ",", "word_h_t", ",", "hyp", ".", "word_ctx_tm1", "]", ",", "dim", "=", "1", ")", "\n", "", "mask", "=", "torch", ".", "ones", "(", "1", ",", "self", ".", "target_vocab_size", ")", ".", "byte", "(", ")", "\n", "if", "cur_nonterm", ".", "label", "==", "'*'", ":", "\n", "      ", "word_index", "=", "torch", ".", "arange", "(", "self", ".", "hparams", ".", "target_word_vocab_size", ")", ".", "long", "(", ")", "\n", "mask", ".", "index_fill_", "(", "1", ",", "word_index", ",", "0", ")", "\n", "word_pre_readout", "=", "F", ".", "tanh", "(", "self", ".", "word_ctx_to_readout", "(", "inp", ")", ")", "\n", "word_pre_readout", "=", "self", ".", "dropout", "(", "word_pre_readout", ")", "\n", "score_t", "=", "self", ".", "readout", "(", "word_pre_readout", ")", "\n", "num_rule_index", "=", "-", "1", "\n", "rule_select_index", "=", "[", "]", "\n", "", "else", ":", "\n", "      ", "rule_with_lhs", "=", "target_rule_vocab", ".", "rule_index_with_lhs", "(", "cur_nonterm", ".", "label", ")", "\n", "rule_select_index", "=", "[", "]", "\n", "for", "i", "in", "rule_with_lhs", ":", "rule_select_index", ".", "append", "(", "i", "+", "self", ".", "hparams", ".", "target_word_vocab_size", ")", "\n", "num_rule_index", "=", "len", "(", "rule_with_lhs", ")", "\n", "rule_index", "=", "torch", ".", "arange", "(", "self", ".", "hparams", ".", "target_rule_vocab_size", ")", ".", "long", "(", ")", "+", "self", ".", "hparams", ".", "target_word_vocab_size", "\n", "mask", ".", "index_fill_", "(", "1", ",", "rule_index", ",", "0", ")", "\n", "rule_pre_readout", "=", "F", ".", "tanh", "(", "self", ".", "rule_ctx_to_readout", "(", "inp", ")", ")", "\n", "rule_pre_readout", "=", "self", ".", "dropout", "(", "rule_pre_readout", ")", "\n", "score_t", "=", "self", ".", "readout", "(", "rule_pre_readout", ")", "\n", "", "if", "self", ".", "hparams", ".", "cuda", ":", "\n", "      ", "mask", "=", "mask", ".", "cuda", "(", ")", "\n", "", "score_t", ".", "data", ".", "masked_fill_", "(", "mask", ",", "-", "float", "(", "\"inf\"", ")", ")", "\n", "rule_hidden", "=", "(", "rule_h_t", ",", "rule_c_t", ")", "\n", "word_hidden", "=", "(", "word_h_t", ",", "word_c_t", ")", "\n", "\n", "hyp", ".", "rule_hidden", "=", "rule_hidden", "\n", "hyp", ".", "word_hidden", "=", "word_hidden", "\n", "\n", "return", "score_t", ",", "num_rule_index", ",", "rule_select_index", "\n", "\n"]], "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.trdec_attn.TrDecAttn.__init__": [[284, 293], ["torch.nn.Module.__init__", "models.Encoder", "trdec_attn.TreeDecoderAttn", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "trdec_attn.TrDecAttn.enc_to_k.cuda"], "methods", ["home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.utils.Logger.__init__"], ["  ", "def", "__init__", "(", "self", ",", "hparams", ")", ":", "\n", "    ", "super", "(", "TrDecAttn", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "encoder", "=", "Encoder", "(", "hparams", ")", "\n", "self", ".", "decoder", "=", "TreeDecoderAttn", "(", "hparams", ")", "\n", "# transform encoder state vectors into attention key vector", "\n", "self", ".", "enc_to_k", "=", "nn", ".", "Linear", "(", "hparams", ".", "d_model", "*", "2", ",", "hparams", ".", "d_model", ",", "bias", "=", "False", ")", "\n", "self", ".", "hparams", "=", "hparams", "\n", "if", "self", ".", "hparams", ".", "cuda", ":", "\n", "      ", "self", ".", "enc_to_k", "=", "self", ".", "enc_to_k", ".", "cuda", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.trdec_attn.TrDecAttn.forward": [[294, 301], ["trdec_attn.TrDecAttn.encoder", "trdec_attn.TrDecAttn.enc_to_k", "trdec_attn.TrDecAttn.decoder"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "x_train", ",", "x_mask", ",", "x_len", ",", "y_train", ",", "y_mask", ",", "y_len", ",", "score_mask", ",", "y_label", "=", "None", ")", ":", "\n", "# [batch_size, x_len, d_model * 2]", "\n", "    ", "x_enc", ",", "dec_init", "=", "self", ".", "encoder", "(", "x_train", ",", "x_len", ")", "\n", "x_enc_k", "=", "self", ".", "enc_to_k", "(", "x_enc", ")", "\n", "# [batch_size, y_len-1, trg_vocab_size]", "\n", "logits", "=", "self", ".", "decoder", "(", "x_enc", ",", "x_enc_k", ",", "dec_init", ",", "x_mask", ",", "y_train", ",", "y_mask", ",", "score_mask", ",", "y_label", ")", "\n", "return", "logits", "\n", "\n"]], "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.trdec_attn.TrDecAttn.translate": [[302, 329], ["torch.nn.LogSoftmax", "torch.nn.LogSoftmax", "torch.nn.LogSoftmax", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "trdec_attn.TrDecAttn.translate_sent", "scores.append", "gc.collect", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "x.cuda.cuda.cuda", "hasattr", "hyps.append", "hyps.append", "sum"], "methods", ["home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.trdec_single.TrDecSingle.translate_sent"], ["", "def", "translate", "(", "self", ",", "x_train", ",", "target_rule_vocab", ",", "max_len", "=", "100", ",", "beam_size", "=", "5", ",", "y_label", "=", "None", ",", "poly_norm_m", "=", "0", ")", ":", "\n", "    ", "hyps", "=", "[", "]", "\n", "scores", "=", "[", "]", "\n", "i", "=", "0", "\n", "self", ".", "logsoftmax", "=", "nn", ".", "LogSoftmax", "(", "dim", "=", "1", ")", "\n", "for", "x", "in", "x_train", ":", "\n", "      ", "x", "=", "Variable", "(", "torch", ".", "LongTensor", "(", "x", ")", ",", "volatile", "=", "True", ")", "\n", "if", "y_label", ":", "\n", "        ", "y", "=", "y_label", "[", "i", "]", "[", "1", ":", "]", "\n", "", "else", ":", "\n", "        ", "y", "=", "None", "\n", "", "if", "self", ".", "hparams", ".", "cuda", ":", "\n", "        ", "x", "=", "x", ".", "cuda", "(", ")", "\n", "", "hyp", ",", "nll_score", "=", "self", ".", "translate_sent", "(", "x", ",", "target_rule_vocab", ",", "max_len", "=", "max_len", ",", "beam_size", "=", "beam_size", ",", "y_label", "=", "y", ",", "poly_norm_m", "=", "poly_norm_m", ")", "\n", "if", "hasattr", "(", "self", ".", "hparams", ",", "\"nbest\"", ")", "and", "self", ".", "hparams", ".", "nbest", ":", "\n", "        ", "nbest", "=", "[", "h", ".", "y", "[", "1", ":", "]", "for", "h", "in", "hyp", "]", "\n", "hyps", ".", "append", "(", "nbest", ")", "\n", "", "else", ":", "\n", "        ", "hyp", "=", "hyp", "[", "0", "]", "\n", "hyps", ".", "append", "(", "hyp", ".", "y", "[", "1", ":", "]", ")", "\n", "", "scores", ".", "append", "(", "sum", "(", "nll_score", ")", ")", "\n", "#print(hyp.y)", "\n", "#print(\"trans score:\", nll_score)", "\n", "#print(\"trans label:\", y)", "\n", "i", "+=", "1", "\n", "gc", ".", "collect", "(", ")", "\n", "", "return", "hyps", ",", "scores", "\n", "\n"]], "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.trdec_attn.TrDecAttn.translate_sent": [[330, 440], ["x_train.unsqueeze.unsqueeze.unsqueeze", "trdec_attn.TrDecAttn.encoder", "trdec_attn.TrDecAttn.enc_to_k", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "len", "x_train.unsqueeze.unsqueeze.size", "input_feed_zeros.cuda.cuda.cuda", "to_input_feed_zeros.cuda.cuda.cuda", "state_zeros.cuda.cuda.cuda", "trdec_attn.TrAttnHyp", "len", "enumerate", "len", "completed_hyp.append", "sorted", "x_train.unsqueeze.unsqueeze.size", "len", "trdec_attn.TrDecAttn.decoder.step", "set", "logits.view.view.view", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "hasattr", "nll_score.append", "print", "trdec_attn.TrAttnHyp", "new_active_hyp.append", "len", "sorted", "trdec_attn.OpenNonterm", "min", "open_nonterms.pop", "reversed", "min", "len", "completed_hyp.append", "active_hyp.append", "pow", "pow", "len", "hasattr", "open_nonterms.append", "print", "print", "print", "open_nonterms.pop", "numpy.array", "trdec_attn.OpenNonterm", "target_rule_vocab.rule_index_with_lhs", "pow", "pow"], "methods", ["home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.trdec_single.TreeDecoderSingle.step", "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.tree_utils.RuleVocab.rule_index_with_lhs"], ["", "def", "translate_sent", "(", "self", ",", "x_train", ",", "target_rule_vocab", ",", "max_len", "=", "100", ",", "beam_size", "=", "5", ",", "y_label", "=", "None", ",", "poly_norm_m", "=", "1.", ")", ":", "\n", "    ", "assert", "len", "(", "x_train", ".", "size", "(", ")", ")", "==", "1", "\n", "x_len", "=", "[", "x_train", ".", "size", "(", "0", ")", "]", "\n", "x_train", "=", "x_train", ".", "unsqueeze", "(", "0", ")", "\n", "x_enc", ",", "dec_init", "=", "self", ".", "encoder", "(", "x_train", ",", "x_len", ")", "\n", "x_enc_k", "=", "self", ".", "enc_to_k", "(", "x_enc", ")", "\n", "length", "=", "0", "\n", "completed_hyp", "=", "[", "]", "\n", "input_feed_zeros", "=", "torch", ".", "zeros", "(", "1", ",", "self", ".", "hparams", ".", "d_model", "*", "2", ")", "\n", "to_input_feed_zeros", "=", "torch", ".", "zeros", "(", "1", ",", "self", ".", "hparams", ".", "d_model", ")", "\n", "state_zeros", "=", "torch", ".", "zeros", "(", "1", ",", "1", ",", "self", ".", "hparams", ".", "d_model", ")", "\n", "if", "self", ".", "hparams", ".", "cuda", ":", "\n", "      ", "input_feed_zeros", "=", "input_feed_zeros", ".", "cuda", "(", ")", "\n", "to_input_feed_zeros", "=", "to_input_feed_zeros", ".", "cuda", "(", ")", "\n", "state_zeros", "=", "state_zeros", ".", "cuda", "(", ")", "\n", "", "active_hyp", "=", "[", "TrAttnHyp", "(", "rule_hidden", "=", "dec_init", ",", "\n", "word_hidden", "=", "dec_init", ",", "\n", "y", "=", "[", "self", ".", "hparams", ".", "bos_id", "]", ",", "\n", "rule_ctx_tm1", "=", "Variable", "(", "input_feed_zeros", ",", "requires_grad", "=", "False", ")", ",", "\n", "word_ctx_tm1", "=", "Variable", "(", "input_feed_zeros", ",", "requires_grad", "=", "False", ")", ",", "\n", "rule_to_word_ctx_tm1", "=", "Variable", "(", "to_input_feed_zeros", ",", "requires_grad", "=", "False", ")", ",", "\n", "word_to_rule_ctx_tm1", "=", "Variable", "(", "to_input_feed_zeros", ",", "requires_grad", "=", "False", ")", ",", "\n", "rule_states", "=", "None", ",", "\n", "word_states", "=", "None", ",", "\n", "open_nonterms", "=", "[", "OpenNonterm", "(", "label", "=", "self", ".", "hparams", ".", "root_label", ",", "parent_state", "=", "state_zeros", ")", "]", ",", "\n", "score", "=", "0.", ")", "]", "\n", "nll_score", "=", "[", "]", "\n", "if", "y_label", "is", "not", "None", ":", "\n", "      ", "max_len", "=", "len", "(", "y_label", ")", "\n", "", "while", "len", "(", "completed_hyp", ")", "<", "beam_size", "and", "length", "<", "max_len", ":", "\n", "      ", "length", "+=", "1", "\n", "new_active_hyp", "=", "[", "]", "\n", "for", "i", ",", "hyp", "in", "enumerate", "(", "active_hyp", ")", ":", "\n", "        ", "logits", ",", "num_rule_index", ",", "rule_index", "=", "self", ".", "decoder", ".", "step", "(", "x_enc", ",", "\n", "x_enc_k", ",", "hyp", ",", "target_rule_vocab", ")", "\n", "\n", "rule_index", "=", "set", "(", "rule_index", ")", "\n", "logits", "=", "logits", ".", "view", "(", "-", "1", ")", "\n", "p_t", "=", "F", ".", "log_softmax", "(", "logits", ",", "dim", "=", "0", ")", ".", "data", "\n", "\n", "if", "hasattr", "(", "self", ".", "hparams", ",", "\"ignore_rule_len\"", ")", "and", "self", ".", "hparams", ".", "ignore_rule_len", ":", "\n", "          ", "l", "=", "(", "np", ".", "array", "(", "hyp", ".", "y", ")", "<", "self", ".", "hparams", ".", "target_word_vocab_size", ")", ".", "sum", "(", ")", "\n", "if", "poly_norm_m", ">", "0", "and", "l", ">", "1", ":", "\n", "            ", "new_hyp_scores", "=", "(", "hyp", ".", "score", "*", "pow", "(", "l", "-", "1", ",", "poly_norm_m", ")", "+", "p_t", ")", "/", "pow", "(", "l", ",", "poly_norm_m", ")", "\n", "", "else", ":", "\n", "            ", "new_hyp_scores", "=", "hyp", ".", "score", "+", "p_t", "\n", "", "", "else", ":", "\n", "          ", "if", "poly_norm_m", ">", "0", "and", "length", ">", "1", ":", "\n", "            ", "new_hyp_scores", "=", "(", "hyp", ".", "score", "*", "pow", "(", "length", "-", "1", ",", "poly_norm_m", ")", "+", "p_t", ")", "/", "pow", "(", "length", ",", "poly_norm_m", ")", "\n", "", "else", ":", "\n", "            ", "new_hyp_scores", "=", "hyp", ".", "score", "+", "p_t", "\n", "\n", "", "", "if", "y_label", "is", "not", "None", ":", "\n", "          ", "top_ids", "=", "[", "y_label", "[", "length", "-", "1", "]", "[", "0", "]", "]", "\n", "nll", "=", "-", "(", "p_t", "[", "top_ids", "[", "0", "]", "]", ")", "\n", "nll_score", ".", "append", "(", "nll", ")", "\n", "print", "(", "\"logit dedcode\"", ",", "logits", "[", "top_ids", "[", "0", "]", "]", ")", "\n", "", "else", ":", "\n", "          ", "num_select", "=", "beam_size", "\n", "if", "num_rule_index", ">=", "0", ":", "num_select", "=", "min", "(", "num_select", ",", "num_rule_index", ")", "\n", "top_ids", "=", "(", "-", "new_hyp_scores", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ".", "argsort", "(", ")", "[", ":", "num_select", "]", "\n", "", "for", "word_id", "in", "top_ids", ":", "\n", "          ", "if", "y_label", "is", "None", "and", "len", "(", "rule_index", ")", ">", "0", "and", "word_id", "not", "in", "rule_index", ":", "continue", "\n", "open_nonterms", "=", "hyp", ".", "open_nonterms", "[", ":", "]", "\n", "if", "word_id", ">=", "self", ".", "hparams", ".", "target_word_vocab_size", ":", "\n", "            ", "rule", "=", "target_rule_vocab", "[", "word_id", "]", "\n", "# force the first rule to be not preterminal", "\n", "if", "hasattr", "(", "self", ".", "hparams", ",", "\"force_rule\"", ")", "and", "self", ".", "hparams", ".", "force_rule", ":", "\n", "              ", "if", "length", "<=", "self", ".", "hparams", ".", "force_rule_step", "and", "rule", ".", "rhs", "[", "0", "]", "==", "\"*\"", ":", "\n", "                ", "continue", "\n", "", "", "cur_nonterm", "=", "open_nonterms", ".", "pop", "(", ")", "\n", "for", "c", "in", "reversed", "(", "rule", ".", "rhs", ")", ":", "\n", "              ", "open_nonterms", ".", "append", "(", "OpenNonterm", "(", "label", "=", "c", ",", "parent_state", "=", "hyp", ".", "rule_hidden", "[", "0", "]", ")", ")", "\n", "", "", "else", ":", "\n", "            ", "if", "open_nonterms", "[", "-", "1", "]", ".", "label", "!=", "'*'", ":", "\n", "              ", "print", "(", "open_nonterms", "[", "-", "1", "]", ".", "label", ",", "word_id", ",", "new_hyp_scores", "[", "word_id", "]", ")", "\n", "print", "(", "target_rule_vocab", ".", "rule_index_with_lhs", "(", "open_nonterms", "[", "-", "1", "]", ".", "label", ")", ")", "\n", "print", "(", "top_ids", ")", "\n", "", "assert", "open_nonterms", "[", "-", "1", "]", ".", "label", "==", "'*'", "\n", "if", "word_id", "==", "self", ".", "hparams", ".", "eos_id", ":", "\n", "              ", "open_nonterms", ".", "pop", "(", ")", "\n", "", "", "new_hyp", "=", "TrAttnHyp", "(", "rule_hidden", "=", "(", "hyp", ".", "rule_hidden", "[", "0", "]", ",", "hyp", ".", "rule_hidden", "[", "1", "]", ")", ",", "\n", "word_hidden", "=", "(", "hyp", ".", "word_hidden", "[", "0", "]", ",", "hyp", ".", "word_hidden", "[", "1", "]", ")", ",", "\n", "y", "=", "hyp", ".", "y", "+", "[", "word_id", "]", ",", "\n", "rule_ctx_tm1", "=", "hyp", ".", "rule_ctx_tm1", ",", "\n", "word_ctx_tm1", "=", "hyp", ".", "word_ctx_tm1", ",", "\n", "rule_to_word_ctx_tm1", "=", "hyp", ".", "rule_to_word_ctx_tm1", ",", "\n", "word_to_rule_ctx_tm1", "=", "hyp", ".", "word_to_rule_ctx_tm1", ",", "\n", "rule_states", "=", "hyp", ".", "rule_states", ",", "\n", "word_states", "=", "hyp", ".", "word_states", ",", "\n", "open_nonterms", "=", "open_nonterms", ",", "\n", "score", "=", "new_hyp_scores", "[", "word_id", "]", ")", "\n", "new_active_hyp", ".", "append", "(", "new_hyp", ")", "\n", "", "", "if", "y_label", "is", "None", ":", "\n", "        ", "live_hyp_num", "=", "beam_size", "-", "len", "(", "completed_hyp", ")", "\n", "new_active_hyp", "=", "sorted", "(", "new_active_hyp", ",", "key", "=", "lambda", "x", ":", "x", ".", "score", ",", "reverse", "=", "True", ")", "[", ":", "min", "(", "beam_size", ",", "live_hyp_num", ")", "]", "\n", "active_hyp", "=", "[", "]", "\n", "for", "hyp", "in", "new_active_hyp", ":", "\n", "          ", "if", "len", "(", "hyp", ".", "open_nonterms", ")", "==", "0", ":", "\n", "#if poly_norm_m <= 0:", "\n", "#  hyp.score = hyp.score / len(hyp.y)", "\n", "            ", "completed_hyp", ".", "append", "(", "hyp", ")", "\n", "", "else", ":", "\n", "            ", "active_hyp", ".", "append", "(", "hyp", ")", "\n", "", "", "", "else", ":", "\n", "        ", "active_hyp", "=", "new_active_hyp", "\n", "\n", "", "", "if", "len", "(", "completed_hyp", ")", "==", "0", ":", "\n", "      ", "completed_hyp", ".", "append", "(", "active_hyp", "[", "0", "]", ")", "\n", "", "return", "sorted", "(", "completed_hyp", ",", "key", "=", "lambda", "x", ":", "x", ".", "score", ",", "reverse", "=", "True", ")", ",", "nll_score", "\n", "\n"]], "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.trdec_attn.TrAttnHyp.__init__": [[442, 458], ["None"], "methods", ["None"], ["  ", "def", "__init__", "(", "self", ",", "rule_hidden", ",", "word_hidden", ",", "y", ",", "\n", "rule_ctx_tm1", ",", "word_ctx_tm1", ",", "\n", "rule_to_word_ctx_tm1", ",", "word_to_rule_ctx_tm1", ",", "\n", "rule_states", ",", "word_states", ",", "score", ",", "open_nonterms", ")", ":", "\n", "    ", "self", ".", "rule_hidden", "=", "rule_hidden", "\n", "self", ".", "word_hidden", "=", "word_hidden", "\n", "self", ".", "rule_states", "=", "rule_states", "\n", "self", ".", "word_states", "=", "word_states", "\n", "# [length_y, 2], each element (index, is_word)", "\n", "self", ".", "y", "=", "y", "\n", "self", ".", "rule_ctx_tm1", "=", "rule_ctx_tm1", "\n", "self", ".", "word_ctx_tm1", "=", "word_ctx_tm1", "\n", "self", ".", "rule_to_word_ctx_tm1", "=", "rule_to_word_ctx_tm1", "\n", "self", ".", "word_to_rule_ctx_tm1", "=", "word_to_rule_ctx_tm1", "\n", "self", ".", "score", "=", "score", "\n", "self", ".", "open_nonterms", "=", "open_nonterms", "\n", "\n"]], "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.trdec_attn.OpenNonterm.__init__": [[460, 463], ["None"], "methods", ["None"], ["  ", "def", "__init__", "(", "self", ",", "label", ",", "parent_state", "=", "None", ")", ":", "\n", "    ", "self", ".", "label", "=", "label", "\n", "self", ".", "parent_state", "=", "parent_state", "\n", "", "", ""]], "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.get_vocab.main": [[19, 57], ["zip", "os.path.join", "print", "sys.stdout.flush", "vocab.items", "os.path.join", "print", "sys.stdout.flush", "open", "finp.read().split", "line.strip.strip", "line.strip.split", "open", "fout.write", "print", "sys.stdout.flush", "len", "finp.read", "len"], "function", ["home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.utils.Logger.flush", "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.utils.Logger.flush", "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.utils.Logger.write", "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.utils.Logger.flush"], ["def", "main", "(", ")", ":", "\n", "  ", "for", "inp_name", ",", "out_name", "in", "zip", "(", "INP_NAMES", ",", "OUT_NAMES", ")", ":", "\n", "    ", "inp_file_name", "=", "os", ".", "path", ".", "join", "(", "DATA_PATH", ",", "inp_name", ")", "\n", "with", "open", "(", "inp_file_name", ",", "\"r\"", ",", "encoding", "=", "\"utf-8\"", ")", "as", "finp", ":", "\n", "      ", "lines", "=", "finp", ".", "read", "(", ")", ".", "split", "(", "\"\\n\"", ")", "\n", "\n", "", "vocab", "=", "{", "\n", "\"<pad>\"", ":", "0", ",", "\n", "\"<unk>\"", ":", "1", ",", "\n", "\"<s>\"", ":", "2", ",", "\n", "\"</s>\"", ":", "3", ",", "\n", "}", "\n", "num_lines", "=", "0", "\n", "for", "line", "in", "lines", ":", "\n", "      ", "line", "=", "line", ".", "strip", "(", ")", "\n", "if", "not", "line", ":", "\n", "        ", "continue", "\n", "", "tokens", "=", "line", ".", "split", "(", ")", "\n", "for", "token", "in", "tokens", ":", "\n", "        ", "if", "token", "not", "in", "vocab", ":", "\n", "          ", "index", "=", "len", "(", "vocab", ")", "\n", "vocab", "[", "token", "]", "=", "index", "\n", "", "", "num_lines", "+=", "1", "\n", "if", "num_lines", "%", "50000", "==", "0", ":", "\n", "        ", "print", "(", "\"Read {0:>6d} lines\"", ".", "format", "(", "num_lines", ")", ")", "\n", "sys", ".", "stdout", ".", "flush", "(", ")", "\n", "", "", "print", "(", "\"Read {0:>6d} lines. vocab_size={1}\"", ".", "format", "(", "num_lines", ",", "len", "(", "vocab", ")", ")", ")", "\n", "sys", ".", "stdout", ".", "flush", "(", ")", "\n", "\n", "log_string", "=", "\"\"", "\n", "for", "word", ",", "idx", "in", "vocab", ".", "items", "(", ")", ":", "\n", "      ", "log_string", "+=", "\"{0} {1}\\n\"", ".", "format", "(", "word", ",", "idx", ")", "\n", "\n", "", "out_name", "=", "os", ".", "path", ".", "join", "(", "DATA_PATH", ",", "out_name", ")", "\n", "print", "(", "\"Saving vocab to '{0}'\"", ".", "format", "(", "out_name", ")", ")", "\n", "sys", ".", "stdout", ".", "flush", "(", ")", "\n", "with", "open", "(", "out_name", ",", "\"w\"", ",", "encoding", "=", "\"utf-8\"", ")", "as", "fout", ":", "\n", "      ", "fout", ".", "write", "(", "log_string", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.correct_dep.remove_pre": [[4, 11], ["tree.is_preterminal", "correct_dep.remove_pre", "type"], "function", ["home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.tree_utils.TreeNode.is_preterminal", "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.correct_dep.remove_pre"], ["def", "remove_pre", "(", "tree", ")", ":", "\n", "  ", "if", "tree", ".", "is_preterminal", "(", ")", ":", "\n", "    ", "tree", ".", "label", "=", "'*'", "\n", "return", "\n", "", "for", "c", "in", "tree", ".", "children", ":", "\n", "    ", "if", "not", "type", "(", "c", ")", "==", "str", ":", "\n", "      ", "remove_pre", "(", "c", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.make_tree.bina_list": [[9, 18], ["tree_utils.TreeNode", "int", "make_tree.bina_list", "make_tree.bina_list"], "function", ["home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.make_tree.bina_list", "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.make_tree.bina_list"], ["def", "bina_list", "(", "node_list", ",", "left", ",", "right", ")", ":", "\n", "  ", "if", "left", "==", "right", ":", "\n", "    ", "return", "node_list", "[", "left", "]", "\n", "", "root", "=", "TreeNode", "(", "\"ROOT\"", ",", "[", "]", ")", "\n", "mid", "=", "int", "(", "(", "left", "+", "right", ")", "/", "2", ")", "\n", "left", "=", "bina_list", "(", "node_list", ",", "left", ",", "mid", ")", "\n", "right", "=", "bina_list", "(", "node_list", ",", "mid", "+", "1", ",", "right", ")", "\n", "root", ".", "children", "=", "[", "left", ",", "right", "]", "\n", "return", "root", "\n", "\n"]], "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.make_tree.make_r_binary_tree": [[19, 39], ["len", "int", "range", "preterms.append", "make_tree.make_r_binary_tree._bina_list"], "function", ["None"], ["", "def", "make_r_binary_tree", "(", "word_list", ",", "left", ",", "right", ")", ":", "\n", "## make if fully binary except for the end of tree", "\n", "  ", "def", "_bina_list", "(", "node_list", ",", "left", ",", "right", ")", ":", "\n", "    ", "if", "left", "==", "right", ":", "\n", "      ", "return", "node_list", "[", "left", "]", "\n", "", "root", "=", "TreeNode", "(", "\"ROOT\"", ",", "[", "]", ")", "\n", "mid", "=", "int", "(", "(", "left", "+", "right", ")", "/", "2", ")", "\n", "left", "=", "_bina_list", "(", "node_list", ",", "left", ",", "mid", ")", "\n", "right", "=", "_bina_list", "(", "node_list", ",", "mid", "+", "1", ",", "right", ")", "\n", "root", ".", "children", "=", "[", "left", ",", "right", "]", "\n", "return", "root", "\n", "", "l", "=", "len", "(", "word_list", ")", "\n", "num_preterm", "=", "int", "(", "pow", "(", "2", ",", "int", "(", "np", ".", "log2", "(", "l", ")", ")", ")", "/", "2", ")", "\n", "preterms", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "num_preterm", "-", "1", ")", ":", "\n", "    ", "lc", "=", "TreeNode", "(", "\"ROOT\"", ",", "[", "word_list", "[", "i", "*", "2", "]", "]", ")", "\n", "rc", "=", "TreeNode", "(", "\"ROOT\"", ",", "[", "word_list", "[", "i", "*", "2", "+", "1", "]", "]", ")", "\n", "preterms", ".", "append", "(", "TreeNode", "(", "\"ROOT\"", ",", "[", "lc", ",", "rc", "]", ")", ")", "\n", "", "preterms", ".", "append", "(", "make_binary_tree", "(", "word_list", ",", "(", "num_preterm", "-", "1", ")", "*", "2", ",", "right", ")", ")", "\n", "return", "_bina_list", "(", "preterms", ",", "0", ",", "len", "(", "preterms", ")", "-", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.make_tree.make_w_binary_tree": [[40, 53], ["len", "make_tree.bina_list", "tree_utils.TreeNode", "tree_utils.TreeNode", "nodes.append", "nodes.append", "tree_utils.TreeNode", "tree_utils.TreeNode", "len"], "function", ["home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.make_tree.bina_list"], ["", "def", "make_w_binary_tree", "(", "word_list", ")", ":", "\n", "## first combine words then make trees ", "\n", "  ", "l", "=", "len", "(", "word_list", ")", "\n", "nodes", "=", "[", "]", "\n", "i", "=", "0", "\n", "while", "i", "<", "l", "-", "1", ":", "\n", "    ", "lc", "=", "TreeNode", "(", "\"ROOT\"", ",", "[", "word_list", "[", "i", "]", "]", ")", "\n", "rc", "=", "TreeNode", "(", "\"ROOT\"", ",", "[", "word_list", "[", "i", "+", "1", "]", "]", ")", "\n", "nodes", ".", "append", "(", "TreeNode", "(", "\"ROOT\"", ",", "[", "lc", ",", "rc", "]", ")", ")", "\n", "i", "+=", "2", "\n", "", "if", "l", "%", "2", "==", "1", ":", "\n", "    ", "nodes", ".", "append", "(", "TreeNode", "(", "\"ROOT\"", ",", "[", "word_list", "[", "-", "1", "]", "]", ")", ")", "\n", "", "return", "bina_list", "(", "nodes", ",", "0", ",", "len", "(", "nodes", ")", "-", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.make_tree.make_right_binary_tree": [[54, 64], ["tree_utils.TreeNode", "int", "make_tree.make_right_binary_tree", "make_tree.make_right_binary_tree", "tree_utils.TreeNode"], "function", ["home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.make_tree.make_right_binary_tree", "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.make_tree.make_right_binary_tree"], ["", "def", "make_right_binary_tree", "(", "word_list", ",", "left", ",", "right", ")", ":", "\n", "  ", "if", "left", "==", "right", ":", "\n", "    ", "return", "TreeNode", "(", "\"ROOT\"", ",", "word_list", "[", "left", "]", ")", "\n", "", "root", "=", "TreeNode", "(", "\"ROOT\"", ",", "[", "]", ")", "\n", "mid", "=", "int", "(", "(", "left", "+", "right", ")", "/", "2", ")", "\n", "if", "mid", "==", "left", ":", "mid", "+=", "1", "\n", "left", "=", "make_right_binary_tree", "(", "word_list", ",", "left", ",", "mid", "-", "1", ")", "\n", "right", "=", "make_right_binary_tree", "(", "word_list", ",", "mid", ",", "right", ")", "\n", "root", ".", "children", "=", "[", "left", ",", "right", "]", "\n", "return", "root", "\n", "\n"]], "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.make_tree.make_binary_tree": [[65, 74], ["tree_utils.TreeNode", "int", "make_tree.make_binary_tree", "make_tree.make_binary_tree", "tree_utils.TreeNode"], "function", ["home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.make_tree.make_binary_tree", "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.make_tree.make_binary_tree"], ["", "def", "make_binary_tree", "(", "word_list", ",", "left", ",", "right", ")", ":", "\n", "  ", "if", "left", "==", "right", ":", "\n", "    ", "return", "TreeNode", "(", "\"ROOT\"", ",", "word_list", "[", "left", "]", ")", "\n", "", "root", "=", "TreeNode", "(", "\"ROOT\"", ",", "[", "]", ")", "\n", "mid", "=", "int", "(", "(", "left", "+", "right", ")", "/", "2", ")", "\n", "left", "=", "make_binary_tree", "(", "word_list", ",", "left", ",", "mid", ")", "\n", "right", "=", "make_binary_tree", "(", "word_list", ",", "mid", "+", "1", ",", "right", ")", "\n", "root", ".", "children", "=", "[", "left", ",", "right", "]", "\n", "return", "root", "\n", "\n"]], "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.make_tree.make_tri_tree": [[75, 90], ["tree_utils.TreeNode", "int", "int", "make_tree.make_tri_tree", "make_tree.make_tri_tree", "make_tree.make_tri_tree", "tree_utils.TreeNode", "tree_utils.TreeNode", "tree_utils.TreeNode", "tree_utils.TreeNode"], "function", ["home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.make_tree.make_tri_tree", "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.make_tree.make_tri_tree", "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.make_tree.make_tri_tree"], ["", "def", "make_tri_tree", "(", "word_list", ",", "left", ",", "right", ")", ":", "\n", "  ", "if", "left", "==", "right", ":", "\n", "    ", "return", "TreeNode", "(", "\"ROOT\"", ",", "word_list", "[", "left", "]", ")", "\n", "", "if", "left", "==", "right", "-", "1", ":", "\n", "    ", "c1", "=", "TreeNode", "(", "\"ROOT\"", ",", "word_list", "[", "left", "]", ")", "\n", "c2", "=", "TreeNode", "(", "\"ROOT\"", ",", "word_list", "[", "right", "]", ")", "\n", "return", "TreeNode", "(", "\"ROOT\"", ",", "[", "c1", ",", "c2", "]", ")", "\n", "", "root", "=", "TreeNode", "(", "\"ROOT\"", ",", "[", "]", ")", "\n", "s1", "=", "int", "(", "(", "2", "*", "left", "+", "right", ")", "/", "3", ")", "\n", "s2", "=", "int", "(", "(", "left", "+", "2", "*", "right", ")", "/", "3", ")", "\n", "c1", "=", "make_tri_tree", "(", "word_list", ",", "left", ",", "s1", ")", "\n", "c2", "=", "make_tri_tree", "(", "word_list", ",", "s1", "+", "1", ",", "s2", ")", "\n", "c3", "=", "make_tri_tree", "(", "word_list", ",", "s2", "+", "1", ",", "right", ")", "\n", "root", ".", "children", "=", "[", "c1", ",", "c2", ",", "c3", "]", "\n", "return", "root", "\n", "\n"]], "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.make_tree.make_random_binary_tree": [[91, 100], ["tree_utils.TreeNode", "random.randint", "make_tree.make_binary_tree", "make_tree.make_binary_tree", "tree_utils.TreeNode"], "function", ["home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.make_tree.make_binary_tree", "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.make_tree.make_binary_tree"], ["", "def", "make_random_binary_tree", "(", "word_list", ",", "left", ",", "right", ")", ":", "\n", "  ", "if", "left", "==", "right", ":", "\n", "    ", "return", "TreeNode", "(", "\"ROOT\"", ",", "word_list", "[", "left", "]", ")", "\n", "", "root", "=", "TreeNode", "(", "\"ROOT\"", ",", "[", "]", ")", "\n", "mid", "=", "random", ".", "randint", "(", "left", ",", "right", "-", "1", ")", "\n", "left", "=", "make_binary_tree", "(", "word_list", ",", "left", ",", "mid", ")", "\n", "right", "=", "make_binary_tree", "(", "word_list", ",", "mid", "+", "1", ",", "right", ")", "\n", "root", ".", "children", "=", "[", "left", ",", "right", "]", "\n", "return", "root", "\n", "\n"]], "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.make_tree.make_phrase_tree": [[101, 142], ["re.finditer", "string[].split", "nodes.append", "make_tree.make_r_binary_tree._bina_list"], "function", ["None"], ["", "def", "make_phrase_tree", "(", "string", ")", ":", "\n", "  ", "def", "_bina_list", "(", "node_list", ",", "left", ",", "right", ")", ":", "\n", "    ", "if", "left", "==", "right", ":", "\n", "      ", "return", "node_list", "[", "left", "]", "\n", "", "root", "=", "TreeNode", "(", "\"ROOT\"", ",", "[", "]", ")", "\n", "mid", "=", "int", "(", "(", "left", "+", "right", ")", "/", "2", ")", "\n", "left", "=", "_bina_list", "(", "node_list", ",", "left", ",", "mid", ")", "\n", "right", "=", "_bina_list", "(", "node_list", ",", "mid", "+", "1", ",", "right", ")", "\n", "root", ".", "children", "=", "[", "left", ",", "right", "]", "\n", "return", "root", "\n", "\n", "#words = re.findall(r\"[\\w]+|[^\\s\\w]\", string)", "\n", "#puncs =  re.findall(r\"[^\\s\\w]\", string)", "\n", "#print(string)", "\n", "", "split_points", "=", "[", "]", "\n", "for", "match", "in", "re", ".", "finditer", "(", "r\"[\\s\\w]+\"", ",", "string", ")", ":", "\n", "    ", "if", "match", ".", "group", "(", ")", ".", "strip", "(", ")", ":", "\n", "#print(match.span(), match.group())", "\n", "      ", "split_points", ".", "append", "(", "match", ".", "span", "(", ")", "[", "0", "]", ")", "\n", "", "", "split_points", "=", "split_points", "[", "1", ":", "]", "\n", "nodes", "=", "[", "]", "\n", "start", "=", "0", "\n", "for", "s", "in", "split_points", ":", "\n", "    ", "cur_str", "=", "string", "[", "start", ":", "s", "]", ".", "split", "(", ")", "\n", "if", "string", "[", "s", "]", "!=", "\" \"", ":", "\n", "      ", "if", "cur_str", "[", "-", "1", "]", "==", "\"'\"", ":", "\n", "        ", "if", "string", "[", "s", "]", "==", "\"s\"", ":", "\n", "          ", "continue", "\n", "", "else", ":", "\n", "          ", "cur_str", "[", "-", "1", "]", "=", "cur_str", "[", "-", "1", "]", "[", ":", "-", "1", "]", "\n", "s", "-=", "1", "\n", "", "", "else", ":", "\n", "        ", "continue", "\n", "", "", "nodes", ".", "append", "(", "make_binary_tree", "(", "cur_str", ",", "0", ",", "len", "(", "cur_str", ")", "-", "1", ")", ")", "\n", "#print(string[start:s])", "\n", "start", "=", "s", "\n", "", "cur_str", "=", "string", "[", "start", ":", "]", ".", "split", "(", ")", "\n", "nodes", ".", "append", "(", "make_binary_tree", "(", "cur_str", ",", "0", ",", "len", "(", "cur_str", ")", "-", "1", ")", ")", "\n", "#print(string[start:])", "\n", "root", "=", "_bina_list", "(", "nodes", ",", "0", ",", "len", "(", "nodes", ")", "-", "1", ")", "\n", "return", "root", "\n", "\n"]], "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.trdec.TreeDecoder.__init__": [[14, 82], ["torch.nn.Module.__init__", "torch.nn.Embedding", "torch.nn.Embedding", "torch.nn.Embedding", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.LSTMCell", "torch.nn.LSTMCell", "torch.nn.LSTMCell", "torch.nn.LSTMCell", "torch.nn.LSTMCell", "torch.nn.LSTMCell", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros.index_fill_", "torch.zeros.index_fill_", "torch.zeros.index_fill_", "hasattr", "hasattr", "hasattr", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "trdec.TreeDecoder.rule_vocab_mask.cuda", "trdec.TreeDecoder.word_vocab_mask.cuda", "trdec.TreeDecoder.emb.cuda", "trdec.TreeDecoder.readout.cuda", "trdec.TreeDecoder.rule_lstm_cell.cuda", "trdec.TreeDecoder.word_lstm_cell.cuda", "trdec.TreeDecoder.dropout.cuda", "models.MlpAttn", "models.DotProdAttn", "models.MlpAttn", "models.MlpAttn", "models.DotProdAttn", "models.DotProdAttn", "hasattr", "trdec.TreeDecoder.attention.cuda", "trdec.TreeDecoder.rule_attention.cuda", "trdec.TreeDecoder.word_attention.cuda", "hasattr", "trdec.TreeDecoder.ctx_to_readout.cuda", "trdec.TreeDecoder.rule_ctx_to_readout.cuda", "trdec.TreeDecoder.word_ctx_to_readout.cuda", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange"], "methods", ["home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.utils.Logger.__init__"], ["  ", "def", "__init__", "(", "self", ",", "hparams", ")", ":", "\n", "    ", "super", "(", "TreeDecoder", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "hparams", "=", "hparams", "\n", "self", ".", "target_vocab_size", "=", "self", ".", "hparams", ".", "target_word_vocab_size", "+", "self", ".", "hparams", ".", "target_rule_vocab_size", "\n", "self", ".", "emb", "=", "nn", ".", "Embedding", "(", "self", ".", "target_vocab_size", ",", "\n", "self", ".", "hparams", ".", "d_word_vec", ",", "\n", "padding_idx", "=", "hparams", ".", "pad_id", ")", "\n", "if", "hasattr", "(", "self", ".", "hparams", ",", "\"single_attn\"", ")", "and", "self", ".", "hparams", ".", "single_attn", ":", "\n", "      ", "if", "self", ".", "hparams", ".", "attn", "==", "\"mlp\"", ":", "\n", "        ", "self", ".", "attention", "=", "MlpAttn", "(", "hparams", ")", "\n", "", "else", ":", "\n", "        ", "self", ".", "attention", "=", "DotProdAttn", "(", "hparams", ")", "\n", "", "", "else", ":", "\n", "      ", "if", "self", ".", "hparams", ".", "attn", "==", "\"mlp\"", ":", "\n", "        ", "self", ".", "rule_attention", "=", "MlpAttn", "(", "hparams", ")", "\n", "self", ".", "word_attention", "=", "MlpAttn", "(", "hparams", ")", "\n", "", "else", ":", "\n", "        ", "self", ".", "rule_attention", "=", "DotProdAttn", "(", "hparams", ")", "\n", "self", ".", "word_attention", "=", "DotProdAttn", "(", "hparams", ")", "\n", "# transform [word_ctx, word_h_t, rule_ctx, rule_h_t] to readout state vectors before softmax", "\n", "", "", "if", "hasattr", "(", "self", ".", "hparams", ",", "\"single_inp_readout\"", ")", "and", "self", ".", "hparams", ".", "single_inp_readout", ":", "\n", "      ", "inp_dim", "=", "hparams", ".", "d_model", "*", "3", "\n", "", "else", ":", "\n", "      ", "inp_dim", "=", "hparams", ".", "d_model", "*", "6", "\n", "", "if", "hasattr", "(", "self", ".", "hparams", ",", "\"single_readout\"", ")", "and", "self", ".", "hparams", ".", "single_readout", ":", "\n", "      ", "self", ".", "ctx_to_readout", "=", "nn", ".", "Linear", "(", "inp_dim", ",", "hparams", ".", "d_model", ",", "bias", "=", "False", ")", "\n", "", "else", ":", "\n", "      ", "self", ".", "rule_ctx_to_readout", "=", "nn", ".", "Linear", "(", "inp_dim", ",", "hparams", ".", "d_model", ",", "bias", "=", "False", ")", "\n", "self", ".", "word_ctx_to_readout", "=", "nn", ".", "Linear", "(", "hparams", ".", "d_model", "*", "6", ",", "hparams", ".", "d_model", ",", "bias", "=", "False", ")", "\n", "\n", "", "self", ".", "readout", "=", "nn", ".", "Linear", "(", "hparams", ".", "d_model", ",", "\n", "self", ".", "target_vocab_size", ",", "\n", "bias", "=", "False", ")", "\n", "if", "self", ".", "hparams", ".", "share_emb_softmax", ":", "\n", "      ", "self", ".", "emb", ".", "weight", "=", "self", ".", "readout", ".", "weight", "\n", "# input: [y_t-1, parent_state, input_feed, word_state]", "\n", "", "rule_inp", "=", "hparams", ".", "d_word_vec", "+", "hparams", ".", "d_model", "*", "3", "\n", "word_inp", "=", "hparams", ".", "d_word_vec", "+", "hparams", ".", "d_model", "*", "2", "\n", "#if hparams.parent_feed:", "\n", "word_inp", "+=", "hparams", ".", "d_model", "\n", "if", "hparams", ".", "rule_parent_feed", ":", "\n", "      ", "rule_inp", "+=", "hparams", ".", "d_model", "\n", "", "self", ".", "rule_lstm_cell", "=", "nn", ".", "LSTMCell", "(", "rule_inp", ",", "hparams", ".", "d_model", ")", "\n", "# input: [y_t-1, parent_state, input_feed]", "\n", "self", ".", "word_lstm_cell", "=", "nn", ".", "LSTMCell", "(", "word_inp", ",", "hparams", ".", "d_model", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "hparams", ".", "dropout", ")", "\n", "\n", "vocab_mask", "=", "torch", ".", "zeros", "(", "1", ",", "1", ",", "self", ".", "target_vocab_size", ")", "\n", "self", ".", "word_vocab_mask", "=", "vocab_mask", ".", "index_fill_", "(", "2", ",", "torch", ".", "arange", "(", "self", ".", "hparams", ".", "target_word_vocab_size", ")", ".", "long", "(", ")", ",", "1", ")", "\n", "self", ".", "rule_vocab_mask", "=", "1", "-", "self", ".", "word_vocab_mask", "\n", "if", "self", ".", "hparams", ".", "cuda", ":", "\n", "      ", "self", ".", "rule_vocab_mask", "=", "self", ".", "rule_vocab_mask", ".", "cuda", "(", ")", "\n", "self", ".", "word_vocab_mask", "=", "self", ".", "word_vocab_mask", ".", "cuda", "(", ")", "\n", "self", ".", "emb", "=", "self", ".", "emb", ".", "cuda", "(", ")", "\n", "if", "hasattr", "(", "self", ".", "hparams", ",", "\"single_attn\"", ")", "and", "self", ".", "hparams", ".", "single_attn", ":", "\n", "        ", "self", ".", "attention", "=", "self", ".", "attention", ".", "cuda", "(", ")", "\n", "", "else", ":", "\n", "        ", "self", ".", "rule_attention", "=", "self", ".", "rule_attention", ".", "cuda", "(", ")", "\n", "self", ".", "word_attention", "=", "self", ".", "word_attention", ".", "cuda", "(", ")", "\n", "", "if", "hasattr", "(", "self", ".", "hparams", ",", "\"single_readout\"", ")", "and", "self", ".", "hparams", ".", "single_readout", ":", "\n", "        ", "self", ".", "ctx_to_readout", "=", "self", ".", "ctx_to_readout", ".", "cuda", "(", ")", "\n", "", "else", ":", "\n", "        ", "self", ".", "rule_ctx_to_readout", "=", "self", ".", "rule_ctx_to_readout", ".", "cuda", "(", ")", "\n", "self", ".", "word_ctx_to_readout", "=", "self", ".", "word_ctx_to_readout", ".", "cuda", "(", ")", "\n", "", "self", ".", "readout", "=", "self", ".", "readout", ".", "cuda", "(", ")", "\n", "self", ".", "rule_lstm_cell", "=", "self", ".", "rule_lstm_cell", ".", "cuda", "(", ")", "\n", "self", ".", "word_lstm_cell", "=", "self", ".", "word_lstm_cell", ".", "cuda", "(", ")", "\n", "self", ".", "dropout", "=", "self", ".", "dropout", ".", "cuda", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.trdec.TreeDecoder.forward": [[83, 212], ["y_train.size", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "trdec.TreeDecoder.emb", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "range", "torch.cat().transpose().contiguous.data.masked_fill_", "torch.cat().transpose().contiguous.data.masked_fill_", "torch.cat().transpose().contiguous.data.masked_fill_", "x_enc.size", "input_feed_zeros.cuda.cuda.cuda", "rule_input_feed.cuda.cuda.cuda", "word_input_feed.cuda.cuda.cuda", "hasattr", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "offset.cuda.cuda.cuda", "y_train[].unsqueeze().float", "trdec.TreeDecoder.word_lstm_cell", "trdec.TreeDecoder.rule_lstm_cell", "trdec.TreeDecoder.dropout", "trdec.TreeDecoder.dropout", "rule_pre_readouts.append", "word_pre_readouts.append", "hasattr", "trdec.TreeDecoder.readout", "trdec.TreeDecoder.transpose().contiguous", "torch.cat().transpose().contiguous", "torch.cat().transpose().contiguous", "torch.cat().transpose().contiguous", "torch.cat().transpose().contiguous", "torch.cat().transpose().contiguous", "torch.cat().transpose().contiguous", "torch.cat().transpose().contiguous", "torch.cat().transpose().contiguous", "torch.cat().transpose().contiguous", "score_mask.unsqueeze().float", "mask_t.byte", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.cat.cuda", "torch.cat.cuda", "torch.cat.cuda", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.index_select", "torch.index_select", "torch.index_select", "torch.index_select", "torch.index_select", "torch.index_select", "torch.index_select", "torch.index_select", "torch.index_select", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "hasattr", "hasattr", "trdec.TreeDecoder.attention", "trdec.TreeDecoder.attention", "trdec.TreeDecoder.rule_attention", "trdec.TreeDecoder.word_attention", "hasattr", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "trdec.TreeDecoder.readout", "hasattr", "trdec.TreeDecoder.readout", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "float", "torch.cat.view", "torch.cat.view", "torch.cat.view", "y_train[].unsqueeze", "trdec.TreeDecoder.rule_ctx_to_readout", "trdec.TreeDecoder.word_ctx_to_readout", "trdec.TreeDecoder.rule_ctx_to_readout", "trdec.TreeDecoder.word_ctx_to_readout", "trdec.TreeDecoder.transpose", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.cat().transpose", "torch.cat().transpose", "torch.cat().transpose", "torch.cat().transpose", "torch.cat().transpose", "torch.cat().transpose", "torch.cat().transpose", "torch.cat().transpose", "torch.cat().transpose", "score_mask.unsqueeze", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.softmax", "torch.softmax", "torch.softmax", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "x_enc", ",", "x_enc_k", ",", "dec_init", ",", "x_mask", ",", "y_train", ",", "y_mask", ",", "score_mask", ",", "y_label", "=", "None", ")", ":", "\n", "# get decoder init state and cell, use x_ct", "\n", "    ", "\"\"\"\n    x_enc: [batch_size, max_x_len, d_model * 2]\n    \"\"\"", "\n", "batch_size_x", "=", "x_enc", ".", "size", "(", ")", "[", "0", "]", "\n", "batch_size", ",", "y_max_len", ",", "data_len", "=", "y_train", ".", "size", "(", ")", "\n", "assert", "batch_size_x", "==", "batch_size", "\n", "#print(y_train)", "\n", "input_feed_zeros", "=", "torch", ".", "zeros", "(", "batch_size", ",", "self", ".", "hparams", ".", "d_model", "*", "2", ")", "\n", "if", "self", ".", "hparams", ".", "cuda", ":", "\n", "      ", "input_feed_zeros", "=", "input_feed_zeros", ".", "cuda", "(", ")", "\n", "", "rule_hidden", "=", "dec_init", "\n", "word_hidden", "=", "dec_init", "\n", "rule_input_feed", "=", "Variable", "(", "input_feed_zeros", ",", "requires_grad", "=", "False", ")", "\n", "word_input_feed", "=", "Variable", "(", "input_feed_zeros", ",", "requires_grad", "=", "False", ")", "\n", "if", "self", ".", "hparams", ".", "cuda", ":", "\n", "      ", "rule_input_feed", "=", "rule_input_feed", ".", "cuda", "(", ")", "\n", "word_input_feed", "=", "word_input_feed", ".", "cuda", "(", ")", "\n", "# [batch_size, y_len, d_word_vec]", "\n", "", "trg_emb", "=", "self", ".", "emb", "(", "y_train", "[", ":", ",", ":", ",", "0", "]", ")", "\n", "logits", "=", "[", "]", "\n", "if", "hasattr", "(", "self", ".", "hparams", ",", "\"single_readout\"", ")", "and", "self", ".", "hparams", ".", "single_readout", ":", "\n", "      ", "pre_readouts", "=", "[", "]", "\n", "", "else", ":", "\n", "      ", "rule_pre_readouts", "=", "[", "]", "\n", "word_pre_readouts", "=", "[", "]", "\n", "", "if", "self", ".", "hparams", ".", "parent_feed", ":", "\n", "      ", "all_state", "=", "Variable", "(", "torch", ".", "zeros", "(", "batch_size", ",", "self", ".", "hparams", ".", "d_model", ")", ",", "requires_grad", "=", "False", ")", "\n", "", "offset", "=", "torch", ".", "arange", "(", "batch_size", ")", ".", "long", "(", ")", "\n", "if", "self", ".", "hparams", ".", "cuda", ":", "\n", "      ", "offset", "=", "offset", ".", "cuda", "(", ")", "\n", "if", "self", ".", "hparams", ".", "parent_feed", ":", "\n", "        ", "all_state", "=", "all_state", ".", "cuda", "(", ")", "\n", "", "", "for", "t", "in", "range", "(", "y_max_len", ")", ":", "\n", "      ", "y_emb_tm1", "=", "trg_emb", "[", ":", ",", "t", ",", ":", "]", "\n", "\n", "if", "self", ".", "hparams", ".", "parent_feed", ":", "\n", "        ", "state_idx_t", "=", "t", "\n", "state_idx_t", "+=", "1", "\n", "parent_t", "=", "y_train", ".", "data", "[", ":", ",", "t", ",", "1", "]", "+", "state_idx_t", "*", "offset", "# [batch_size,]", "\n", "parent_t", "=", "Variable", "(", "parent_t", ",", "requires_grad", "=", "False", ")", "\n", "parent_state", "=", "torch", ".", "index_select", "(", "all_state", ".", "view", "(", "state_idx_t", "*", "batch_size", ",", "self", ".", "hparams", ".", "d_model", ")", ",", "dim", "=", "0", ",", "index", "=", "parent_t", ")", "# [batch_size, d_model]", "\n", "\n", "", "word_mask", "=", "y_train", "[", ":", ",", "t", ",", "2", "]", ".", "unsqueeze", "(", "1", ")", ".", "float", "(", ")", "# (1 is word, 0 is rule)", "\n", "\n", "if", "self", ".", "hparams", ".", "parent_feed", ":", "\n", "        ", "word_input", "=", "torch", ".", "cat", "(", "[", "y_emb_tm1", ",", "parent_state", ",", "word_input_feed", "]", ",", "dim", "=", "1", ")", "\n", "", "else", ":", "\n", "        ", "word_input", "=", "torch", ".", "cat", "(", "[", "y_emb_tm1", ",", "rule_hidden", "[", "0", "]", ",", "word_input_feed", "]", ",", "dim", "=", "1", ")", "\n", "", "word_h_t", ",", "word_c_t", "=", "self", ".", "word_lstm_cell", "(", "word_input", ",", "word_hidden", ")", "\n", "\n", "word_h_t", "=", "word_h_t", "*", "word_mask", "+", "word_hidden", "[", "0", "]", "*", "(", "1", "-", "word_mask", ")", "\n", "word_c_t", "=", "word_c_t", "*", "word_mask", "+", "word_hidden", "[", "1", "]", "*", "(", "1", "-", "word_mask", ")", "\n", "\n", "if", "self", ".", "hparams", ".", "rule_parent_feed", ":", "\n", "        ", "rule_input", "=", "torch", ".", "cat", "(", "[", "y_emb_tm1", ",", "parent_state", ",", "rule_input_feed", ",", "word_h_t", "]", ",", "dim", "=", "1", ")", "\n", "", "else", ":", "\n", "        ", "rule_input", "=", "torch", ".", "cat", "(", "[", "y_emb_tm1", ",", "rule_input_feed", ",", "word_h_t", "]", ",", "dim", "=", "1", ")", "\n", "", "rule_h_t", ",", "rule_c_t", "=", "self", ".", "rule_lstm_cell", "(", "rule_input", ",", "rule_hidden", ")", "\n", "\n", "if", "hasattr", "(", "self", ".", "hparams", ",", "\"no_word_to_rule\"", ")", "and", "self", ".", "hparams", ".", "no_word_to_rule", ":", "\n", "        ", "eos_mask", "=", "(", "y_train", "[", ":", ",", "t", ",", "0", "]", "==", "self", ".", "hparams", ".", "eos_id", ")", ".", "unsqueeze", "(", "1", ")", ".", "float", "(", ")", "\n", "word_mask", "=", "word_mask", "-", "eos_mask", "\n", "rule_h_t", "=", "rule_h_t", "*", "(", "1", "-", "word_mask", ")", "+", "rule_hidden", "[", "0", "]", "*", "word_mask", "\n", "rule_c_t", "=", "rule_c_t", "*", "(", "1", "-", "word_mask", ")", "+", "rule_hidden", "[", "1", "]", "*", "word_mask", "\n", "\n", "", "if", "hasattr", "(", "self", ".", "hparams", ",", "\"single_attn\"", ")", "and", "self", ".", "hparams", ".", "single_attn", ":", "\n", "        ", "rule_ctx", "=", "self", ".", "attention", "(", "rule_h_t", ",", "x_enc_k", ",", "x_enc", ",", "attn_mask", "=", "x_mask", ")", "\n", "word_ctx", "=", "self", ".", "attention", "(", "word_h_t", ",", "x_enc_k", ",", "x_enc", ",", "attn_mask", "=", "x_mask", ")", "\n", "", "else", ":", "\n", "        ", "rule_ctx", "=", "self", ".", "rule_attention", "(", "rule_h_t", ",", "x_enc_k", ",", "x_enc", ",", "attn_mask", "=", "x_mask", ")", "\n", "word_ctx", "=", "self", ".", "word_attention", "(", "word_h_t", ",", "x_enc_k", ",", "x_enc", ",", "attn_mask", "=", "x_mask", ")", "\n", "\n", "", "if", "hasattr", "(", "self", ".", "hparams", ",", "\"single_inp_readout\"", ")", "and", "self", ".", "hparams", ".", "single_inp_readout", ":", "\n", "        ", "rule_pre_readout", "=", "F", ".", "tanh", "(", "self", ".", "rule_ctx_to_readout", "(", "torch", ".", "cat", "(", "[", "rule_h_t", ",", "rule_ctx", "]", ",", "dim", "=", "1", ")", ")", ")", "\n", "word_pre_readout", "=", "F", ".", "tanh", "(", "self", ".", "word_ctx_to_readout", "(", "torch", ".", "cat", "(", "[", "rule_h_t", ",", "rule_ctx", ",", "word_h_t", ",", "word_ctx", "]", ",", "dim", "=", "1", ")", ")", ")", "\n", "", "else", ":", "\n", "        ", "inp", "=", "torch", ".", "cat", "(", "[", "rule_h_t", ",", "rule_ctx", ",", "word_h_t", ",", "word_ctx", "]", ",", "dim", "=", "1", ")", "\n", "rule_pre_readout", "=", "F", ".", "tanh", "(", "self", ".", "rule_ctx_to_readout", "(", "inp", ")", ")", "\n", "word_pre_readout", "=", "F", ".", "tanh", "(", "self", ".", "word_ctx_to_readout", "(", "inp", ")", ")", "\n", "\n", "", "rule_pre_readout", "=", "self", ".", "dropout", "(", "rule_pre_readout", ")", "\n", "word_pre_readout", "=", "self", ".", "dropout", "(", "word_pre_readout", ")", "\n", "\n", "rule_pre_readouts", ".", "append", "(", "rule_pre_readout", ")", "\n", "word_pre_readouts", ".", "append", "(", "word_pre_readout", ")", "\n", "\n", "#inp = torch.cat([rule_h_t, rule_ctx, word_h_t, word_ctx], dim=1)", "\n", "#if hasattr(self.hparams, \"single_readout\") and self.hparams.single_readout:", "\n", "#  pre_readout = F.tanh(self.ctx_to_readout(r_inp))", "\n", "#  pre_readout = self.dropout(pre_readout)", "\n", "#  pre_readouts.append(pre_readout)", "\n", "#else:", "\n", "#  rule_pre_readout = F.tanh(self.rule_ctx_to_readout(r_inp))", "\n", "#  word_pre_readout = F.tanh(self.word_ctx_to_readout(w_inp))", "\n", "#  ", "\n", "#  rule_pre_readout = self.dropout(rule_pre_readout)", "\n", "#  word_pre_readout = self.dropout(word_pre_readout)", "\n", "#  ", "\n", "#  rule_pre_readouts.append(rule_pre_readout)", "\n", "#  word_pre_readouts.append(word_pre_readout)", "\n", "\n", "rule_input_feed", "=", "rule_ctx", "\n", "word_input_feed", "=", "word_ctx", "\n", "\n", "rule_hidden", "=", "(", "rule_h_t", ",", "rule_c_t", ")", "\n", "word_hidden", "=", "(", "word_h_t", ",", "word_c_t", ")", "\n", "if", "self", ".", "hparams", ".", "parent_feed", ":", "\n", "        ", "all_state", "=", "torch", ".", "cat", "(", "[", "all_state", ",", "rule_h_t", "]", ",", "dim", "=", "1", ")", "\n", "", "", "if", "hasattr", "(", "self", ".", "hparams", ",", "\"single_readout\"", ")", "and", "self", ".", "hparams", ".", "single_readout", ":", "\n", "      ", "readouts", "=", "self", ".", "readout", "(", "torch", ".", "stack", "(", "pre_readouts", ")", ")", "\n", "logits", "=", "readouts", ".", "transpose", "(", "0", ",", "1", ")", ".", "contiguous", "(", ")", "\n", "", "else", ":", "\n", "# [len_y, batch_size, trg_vocab_size]", "\n", "      ", "rule_readouts", "=", "self", ".", "readout", "(", "torch", ".", "stack", "(", "rule_pre_readouts", ")", ")", "[", ":", ",", ":", ",", "-", "self", ".", "hparams", ".", "target_rule_vocab_size", ":", "]", "\n", "if", "hasattr", "(", "self", ".", "hparams", ",", "\"rule_tanh\"", ")", "and", "self", ".", "hparams", ".", "rule_tanh", ">", "0", ":", "\n", "        ", "rule_readouts", "=", "self", ".", "hparams", ".", "rule_tanh", "*", "torch", ".", "tanh", "(", "rule_readouts", ")", "\n", "", "word_readouts", "=", "self", ".", "readout", "(", "torch", ".", "stack", "(", "word_pre_readouts", ")", ")", "[", ":", ",", ":", ",", ":", "self", ".", "hparams", ".", "target_word_vocab_size", "]", "\n", "if", "self", ".", "hparams", ".", "label_smooth", ">", "0", ":", "\n", "        ", "smooth", "=", "self", ".", "hparams", ".", "label_smooth", "\n", "rule_probs", "=", "(", "1.0", "-", "smooth", ")", "*", "F", ".", "softmax", "(", "rule_readouts", ",", "dim", "=", "2", ")", "+", "smooth", "/", "self", ".", "hparams", ".", "target_rule_vocab_size", "\n", "rule_readouts", "=", "torch", ".", "log", "(", "rule_probs", ")", "\n", "# [batch_size, len_y, trg_vocab_size]", "\n", "", "logits", "=", "torch", ".", "cat", "(", "[", "word_readouts", ",", "rule_readouts", "]", ",", "dim", "=", "2", ")", ".", "transpose", "(", "0", ",", "1", ")", ".", "contiguous", "(", ")", "\n", "", "score_mask", "=", "score_mask", ".", "unsqueeze", "(", "2", ")", ".", "float", "(", ")", ".", "data", "\n", "mask_t", "=", "self", ".", "word_vocab_mask", "*", "(", "1", "-", "score_mask", ")", "+", "self", ".", "rule_vocab_mask", "*", "score_mask", "\n", "logits", ".", "data", ".", "masked_fill_", "(", "mask_t", ".", "byte", "(", ")", ",", "-", "float", "(", "\"inf\"", ")", ")", "\n", "return", "logits", "\n", "\n"]], "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.trdec.TreeDecoder.step": [[213, 306], ["torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "trdec.TreeDecoder.emb", "torch.ones().byte", "torch.ones().byte", "torch.ones().byte", "torch.ones().byte", "torch.ones().byte", "torch.ones().byte", "torch.ones().byte", "torch.ones().byte", "torch.ones().byte", "trdec.TreeDecoder.data.masked_fill_", "y_tm1.cuda.cuda.cuda", "trdec.TreeDecoder.word_lstm_cell", "hasattr", "trdec.TreeDecoder.rule_lstm_cell", "hasattr", "trdec.TreeDecoder.attention", "trdec.TreeDecoder.rule_attention", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "mask.cuda.cuda.index_fill_", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "trdec.TreeDecoder.readout", "target_rule_vocab.rule_index_with_lhs", "len", "mask.cuda.cuda.index_fill_", "trdec.TreeDecoder.readout", "mask.cuda.cuda.cuda", "int", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "hasattr", "trdec.TreeDecoder.attention", "trdec.TreeDecoder.word_attention", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "hasattr", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "rule_select_index.append", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "hasattr", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "hasattr", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "float", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "trdec.TreeDecoder.ctx_to_readout", "trdec.TreeDecoder.word_ctx_to_readout", "trdec.TreeDecoder.ctx_to_readout", "trdec.TreeDecoder.rule_ctx_to_readout", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange"], "methods", ["home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.tree_utils.RuleVocab.rule_index_with_lhs"], ["", "def", "step", "(", "self", ",", "x_enc", ",", "x_enc_k", ",", "hyp", ",", "target_rule_vocab", ")", ":", "\n", "    ", "y_tm1", "=", "torch", ".", "LongTensor", "(", "[", "int", "(", "hyp", ".", "y", "[", "-", "1", "]", ")", "]", ")", "\n", "if", "self", ".", "hparams", ".", "cuda", ":", "\n", "      ", "y_tm1", "=", "y_tm1", ".", "cuda", "(", ")", "\n", "", "y_tm1", "=", "Variable", "(", "y_tm1", ",", "volatile", "=", "True", ")", "\n", "open_nonterms", "=", "hyp", ".", "open_nonterms", "\n", "rule_input_feed", "=", "hyp", ".", "rule_ctx_tm1", "\n", "word_input_feed", "=", "hyp", ".", "word_ctx_tm1", "\n", "rule_hidden", "=", "hyp", ".", "rule_hidden", "\n", "word_hidden", "=", "hyp", ".", "word_hidden", "\n", "word_h_t", ",", "word_c_t", "=", "word_hidden", "\n", "rule_h_t", ",", "rule_c_t", "=", "rule_hidden", "\n", "\n", "y_emb_tm1", "=", "self", ".", "emb", "(", "y_tm1", ")", "\n", "cur_nonterm", "=", "open_nonterms", "[", "-", "1", "]", "\n", "parent_state", "=", "cur_nonterm", ".", "parent_state", "\n", "\n", "if", "hyp", ".", "y", "[", "-", "1", "]", "<", "self", ".", "hparams", ".", "target_word_vocab_size", ":", "\n", "# word", "\n", "      ", "if", "self", ".", "hparams", ".", "parent_feed", ":", "\n", "        ", "word_input", "=", "torch", ".", "cat", "(", "[", "y_emb_tm1", ",", "parent_state", ",", "word_input_feed", "]", ",", "dim", "=", "1", ")", "\n", "", "else", ":", "\n", "        ", "word_input", "=", "torch", ".", "cat", "(", "[", "y_emb_tm1", ",", "rule_h_t", ",", "word_input_feed", "]", ",", "dim", "=", "1", ")", "\n", "", "word_h_t", ",", "word_c_t", "=", "self", ".", "word_lstm_cell", "(", "word_input", ",", "word_hidden", ")", "\n", "if", "hasattr", "(", "self", ".", "hparams", ",", "\"single_attn\"", ")", "and", "self", ".", "hparams", ".", "single_attn", ":", "\n", "        ", "word_ctx", "=", "self", ".", "attention", "(", "word_h_t", ",", "x_enc_k", ",", "x_enc", ")", "\n", "", "else", ":", "\n", "        ", "word_ctx", "=", "self", ".", "word_attention", "(", "word_h_t", ",", "x_enc_k", ",", "x_enc", ")", "\n", "", "", "else", ":", "\n", "      ", "word_ctx", "=", "hyp", ".", "word_ctx_tm1", "\n", "\n", "", "update_rule_rnn", "=", "True", "\n", "if", "hasattr", "(", "self", ".", "hparams", ",", "\"no_word_to_rule\"", ")", "and", "self", ".", "hparams", ".", "no_word_to_rule", "and", "hyp", ".", "y", "[", "-", "1", "]", "<", "self", ".", "hparams", ".", "target_word_vocab_size", ":", "\n", "      ", "if", "hyp", ".", "y", "[", "-", "1", "]", "!=", "self", ".", "hparams", ".", "eos_id", ":", "\n", "        ", "update_rule_rnn", "=", "False", "\n", "\n", "", "", "if", "update_rule_rnn", ":", "\n", "      ", "if", "self", ".", "hparams", ".", "rule_parent_feed", ":", "\n", "        ", "rule_input", "=", "torch", ".", "cat", "(", "[", "y_emb_tm1", ",", "parent_state", ",", "rule_input_feed", ",", "word_h_t", "]", ",", "dim", "=", "1", ")", "\n", "", "else", ":", "\n", "        ", "rule_input", "=", "torch", ".", "cat", "(", "[", "y_emb_tm1", ",", "rule_input_feed", ",", "word_h_t", "]", ",", "dim", "=", "1", ")", "\n", "", "rule_h_t", ",", "rule_c_t", "=", "self", ".", "rule_lstm_cell", "(", "rule_input", ",", "rule_hidden", ")", "\n", "#  if hasattr(self.hparams, \"single_attn\") and self.hparams.single_attn:", "\n", "#    rule_ctx = self.attention(rule_h_t, x_enc_k, x_enc)", "\n", "#  else:", "\n", "#    rule_ctx = self.rule_attention(rule_h_t, x_enc_k, x_enc)", "\n", "#else:", "\n", "#  rule_ctx = hyp.rule_ctx_tm1", "\n", "", "if", "hasattr", "(", "self", ".", "hparams", ",", "\"single_attn\"", ")", "and", "self", ".", "hparams", ".", "single_attn", ":", "\n", "      ", "rule_ctx", "=", "self", ".", "attention", "(", "rule_h_t", ",", "x_enc_k", ",", "x_enc", ")", "\n", "", "else", ":", "\n", "      ", "rule_ctx", "=", "self", ".", "rule_attention", "(", "rule_h_t", ",", "x_enc_k", ",", "x_enc", ")", "\n", "\n", "", "mask", "=", "torch", ".", "ones", "(", "1", ",", "self", ".", "target_vocab_size", ")", ".", "byte", "(", ")", "\n", "if", "cur_nonterm", ".", "label", "==", "'*'", ":", "\n", "      ", "word_index", "=", "torch", ".", "arange", "(", "self", ".", "hparams", ".", "target_word_vocab_size", ")", ".", "long", "(", ")", "\n", "mask", ".", "index_fill_", "(", "1", ",", "word_index", ",", "0", ")", "\n", "#if hasattr(self.hparams, \"single_inp_readout\") and self.hparams.single_inp_readout:", "\n", "#  inp = torch.cat([word_h_t, word_ctx], dim=1)", "\n", "#else:", "\n", "#  inp = torch.cat([rule_h_t, rule_ctx, word_h_t, word_ctx], dim=1)", "\n", "inp", "=", "torch", ".", "cat", "(", "[", "rule_h_t", ",", "rule_ctx", ",", "word_h_t", ",", "word_ctx", "]", ",", "dim", "=", "1", ")", "\n", "if", "hasattr", "(", "self", ".", "hparams", ",", "\"single_readout\"", ")", "and", "self", ".", "hparams", ".", "single_readout", ":", "\n", "        ", "word_pre_readout", "=", "F", ".", "tanh", "(", "self", ".", "ctx_to_readout", "(", "inp", ")", ")", "\n", "", "else", ":", "\n", "        ", "word_pre_readout", "=", "F", ".", "tanh", "(", "self", ".", "word_ctx_to_readout", "(", "inp", ")", ")", "\n", "#word_pre_readout = self.dropout(word_pre_readout)", "\n", "", "score_t", "=", "self", ".", "readout", "(", "word_pre_readout", ")", "\n", "num_rule_index", "=", "-", "1", "\n", "rule_select_index", "=", "[", "]", "\n", "", "else", ":", "\n", "      ", "rule_with_lhs", "=", "target_rule_vocab", ".", "rule_index_with_lhs", "(", "cur_nonterm", ".", "label", ")", "\n", "rule_select_index", "=", "[", "]", "\n", "for", "i", "in", "rule_with_lhs", ":", "rule_select_index", ".", "append", "(", "i", "+", "self", ".", "hparams", ".", "target_word_vocab_size", ")", "\n", "num_rule_index", "=", "len", "(", "rule_with_lhs", ")", "\n", "rule_index", "=", "torch", ".", "arange", "(", "self", ".", "hparams", ".", "target_rule_vocab_size", ")", ".", "long", "(", ")", "+", "self", ".", "hparams", ".", "target_word_vocab_size", "\n", "mask", ".", "index_fill_", "(", "1", ",", "rule_index", ",", "0", ")", "\n", "if", "hasattr", "(", "self", ".", "hparams", ",", "\"single_inp_readout\"", ")", "and", "self", ".", "hparams", ".", "single_inp_readout", ":", "\n", "        ", "inp", "=", "torch", ".", "cat", "(", "[", "rule_h_t", ",", "rule_ctx", "]", ",", "dim", "=", "1", ")", "\n", "", "else", ":", "\n", "        ", "inp", "=", "torch", ".", "cat", "(", "[", "rule_h_t", ",", "rule_ctx", ",", "word_h_t", ",", "word_ctx", "]", ",", "dim", "=", "1", ")", "\n", "", "if", "hasattr", "(", "self", ".", "hparams", ",", "\"single_readout\"", ")", "and", "self", ".", "hparams", ".", "single_readout", ":", "\n", "        ", "rule_pre_readout", "=", "F", ".", "tanh", "(", "self", ".", "ctx_to_readout", "(", "inp", ")", ")", "\n", "", "else", ":", "\n", "        ", "rule_pre_readout", "=", "F", ".", "tanh", "(", "self", ".", "rule_ctx_to_readout", "(", "inp", ")", ")", "\n", "#rule_pre_readout = self.dropout(rule_pre_readout)", "\n", "", "score_t", "=", "self", ".", "readout", "(", "rule_pre_readout", ")", "\n", "", "if", "self", ".", "hparams", ".", "cuda", ":", "\n", "      ", "mask", "=", "mask", ".", "cuda", "(", ")", "\n", "", "score_t", ".", "data", ".", "masked_fill_", "(", "mask", ",", "-", "float", "(", "\"inf\"", ")", ")", "\n", "rule_hidden", "=", "(", "rule_h_t", ",", "rule_c_t", ")", "\n", "word_hidden", "=", "(", "word_h_t", ",", "word_c_t", ")", "\n", "return", "score_t", ",", "rule_hidden", ",", "word_hidden", ",", "rule_ctx", ",", "word_ctx", ",", "num_rule_index", ",", "rule_select_index", "\n", "\n"]], "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.trdec.TrDec.__init__": [[309, 318], ["torch.nn.Module.__init__", "models.Encoder", "trdec.TreeDecoder", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "trdec.TrDec.enc_to_k.cuda"], "methods", ["home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.utils.Logger.__init__"], ["  ", "def", "__init__", "(", "self", ",", "hparams", ")", ":", "\n", "    ", "super", "(", "TrDec", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "encoder", "=", "Encoder", "(", "hparams", ")", "\n", "self", ".", "decoder", "=", "TreeDecoder", "(", "hparams", ")", "\n", "# transform encoder state vectors into attention key vector", "\n", "self", ".", "enc_to_k", "=", "nn", ".", "Linear", "(", "hparams", ".", "d_model", "*", "2", ",", "hparams", ".", "d_model", ",", "bias", "=", "False", ")", "\n", "self", ".", "hparams", "=", "hparams", "\n", "if", "self", ".", "hparams", ".", "cuda", ":", "\n", "      ", "self", ".", "enc_to_k", "=", "self", ".", "enc_to_k", ".", "cuda", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.trdec.TrDec.forward": [[319, 326], ["trdec.TrDec.encoder", "trdec.TrDec.enc_to_k", "trdec.TrDec.decoder"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "x_train", ",", "x_mask", ",", "x_len", ",", "y_train", ",", "y_mask", ",", "y_len", ",", "score_mask", ",", "y_label", "=", "None", ")", ":", "\n", "# [batch_size, x_len, d_model * 2]", "\n", "    ", "x_enc", ",", "dec_init", "=", "self", ".", "encoder", "(", "x_train", ",", "x_len", ")", "\n", "x_enc_k", "=", "self", ".", "enc_to_k", "(", "x_enc", ")", "\n", "# [batch_size, y_len-1, trg_vocab_size]", "\n", "logits", "=", "self", ".", "decoder", "(", "x_enc", ",", "x_enc_k", ",", "dec_init", ",", "x_mask", ",", "y_train", ",", "y_mask", ",", "score_mask", ",", "y_label", ")", "\n", "return", "logits", "\n", "\n"]], "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.trdec.TrDec.translate": [[327, 351], ["torch.nn.LogSoftmax", "torch.nn.LogSoftmax", "torch.nn.LogSoftmax", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "trdec.TrDec.translate_sent", "hyps.append", "scores.append", "gc.collect", "print", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "x.cuda.cuda.cuda", "sum"], "methods", ["home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.trdec_single.TrDecSingle.translate_sent"], ["", "def", "translate", "(", "self", ",", "x_train", ",", "target_rule_vocab", ",", "max_len", "=", "100", ",", "beam_size", "=", "5", ",", "y_label", "=", "None", ",", "poly_norm_m", "=", "0", ")", ":", "\n", "    ", "hyps", "=", "[", "]", "\n", "scores", "=", "[", "]", "\n", "i", "=", "0", "\n", "self", ".", "logsoftmax", "=", "nn", ".", "LogSoftmax", "(", "dim", "=", "1", ")", "\n", "for", "x", "in", "x_train", ":", "\n", "      ", "x", "=", "Variable", "(", "torch", ".", "LongTensor", "(", "x", ")", ",", "volatile", "=", "True", ")", "\n", "if", "y_label", ":", "\n", "        ", "y", "=", "y_label", "[", "i", "]", "[", "1", ":", "]", "\n", "", "else", ":", "\n", "        ", "y", "=", "None", "\n", "", "if", "self", ".", "hparams", ".", "cuda", ":", "\n", "        ", "x", "=", "x", ".", "cuda", "(", ")", "\n", "", "hyp", ",", "nll_score", "=", "self", ".", "translate_sent", "(", "x", ",", "target_rule_vocab", ",", "max_len", "=", "max_len", ",", "beam_size", "=", "beam_size", ",", "y_label", "=", "y", ",", "poly_norm_m", "=", "poly_norm_m", ")", "\n", "hyp", "=", "hyp", "[", "0", "]", "\n", "hyps", ".", "append", "(", "hyp", ".", "y", "[", "1", ":", "]", ")", "\n", "scores", ".", "append", "(", "sum", "(", "nll_score", ")", ")", "\n", "#print(hyp.y)", "\n", "#print(\"trans score:\", nll_score)", "\n", "#print(\"trans label:\", y)", "\n", "i", "+=", "1", "\n", "gc", ".", "collect", "(", ")", "\n", "print", "(", "i", ")", "\n", "", "return", "hyps", ",", "scores", "\n", "\n"]], "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.trdec.TrDec.translate_sent": [[352, 464], ["x_train.unsqueeze.unsqueeze.unsqueeze", "trdec.TrDec.encoder", "trdec.TrDec.enc_to_k", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "len", "x_train.unsqueeze.unsqueeze.size", "input_feed_zeros.cuda.cuda.cuda", "state_zeros.cuda.cuda.cuda", "trdec.TrHyp", "len", "print", "enumerate", "len", "completed_hyp.append", "sorted", "x_train.unsqueeze.unsqueeze.size", "len", "len", "trdec.TrDec.decoder.step", "set", "logits.view.view.view", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "hasattr", "nll_score.append", "print", "trdec.TrHyp", "new_active_hyp.append", "len", "sorted", "trdec.OpenNonterm", "min", "open_nonterms.pop", "reversed", "min", "len", "completed_hyp.append", "active_hyp.append", "pow", "pow", "len", "hasattr", "open_nonterms.append", "print", "print", "print", "open_nonterms.pop", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "numpy.array", "trdec.OpenNonterm", "target_rule_vocab.rule_index_with_lhs", "pow", "pow", "len"], "methods", ["home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.trdec_single.TreeDecoderSingle.step", "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.tree_utils.RuleVocab.rule_index_with_lhs"], ["", "def", "translate_sent", "(", "self", ",", "x_train", ",", "target_rule_vocab", ",", "max_len", "=", "100", ",", "beam_size", "=", "5", ",", "y_label", "=", "None", ",", "poly_norm_m", "=", "1.", ")", ":", "\n", "    ", "assert", "len", "(", "x_train", ".", "size", "(", ")", ")", "==", "1", "\n", "x_len", "=", "[", "x_train", ".", "size", "(", "0", ")", "]", "\n", "x_train", "=", "x_train", ".", "unsqueeze", "(", "0", ")", "\n", "x_enc", ",", "dec_init", "=", "self", ".", "encoder", "(", "x_train", ",", "x_len", ")", "\n", "x_enc_k", "=", "self", ".", "enc_to_k", "(", "x_enc", ")", "\n", "length", "=", "0", "\n", "completed_hyp", "=", "[", "]", "\n", "input_feed_zeros", "=", "torch", ".", "zeros", "(", "1", ",", "self", ".", "hparams", ".", "d_model", "*", "2", ")", "\n", "state_zeros", "=", "torch", ".", "zeros", "(", "1", ",", "self", ".", "hparams", ".", "d_model", ")", "\n", "if", "self", ".", "hparams", ".", "cuda", ":", "\n", "      ", "input_feed_zeros", "=", "input_feed_zeros", ".", "cuda", "(", ")", "\n", "state_zeros", "=", "state_zeros", ".", "cuda", "(", ")", "\n", "", "rule_input_feed", "=", "Variable", "(", "input_feed_zeros", ",", "requires_grad", "=", "False", ")", "\n", "word_input_feed", "=", "Variable", "(", "input_feed_zeros", ",", "requires_grad", "=", "False", ")", "\n", "active_hyp", "=", "[", "TrHyp", "(", "rule_hidden", "=", "dec_init", ",", "\n", "word_hidden", "=", "dec_init", ",", "\n", "y", "=", "[", "self", ".", "hparams", ".", "bos_id", "]", ",", "\n", "rule_ctx_tm1", "=", "rule_input_feed", ",", "\n", "word_ctx_tm1", "=", "word_input_feed", ",", "\n", "open_nonterms", "=", "[", "OpenNonterm", "(", "label", "=", "self", ".", "hparams", ".", "root_label", ",", "\n", "parent_state", "=", "Variable", "(", "state_zeros", ",", "requires_grad", "=", "False", ")", ")", "]", ",", "\n", "score", "=", "0.", ")", "]", "\n", "nll_score", "=", "[", "]", "\n", "if", "y_label", "is", "not", "None", ":", "\n", "      ", "max_len", "=", "len", "(", "y_label", ")", "\n", "", "while", "len", "(", "completed_hyp", ")", "<", "beam_size", "and", "length", "<", "max_len", ":", "\n", "      ", "length", "+=", "1", "\n", "print", "(", "length", ",", "len", "(", "completed_hyp", ")", ")", "\n", "new_active_hyp", "=", "[", "]", "\n", "for", "i", ",", "hyp", "in", "enumerate", "(", "active_hyp", ")", ":", "\n", "        ", "logits", ",", "rule_hidden", ",", "word_hidden", ",", "rule_ctx", ",", "word_ctx", ",", "num_rule_index", ",", "rule_index", "=", "self", ".", "decoder", ".", "step", "(", "x_enc", ",", "\n", "x_enc_k", ",", "hyp", ",", "target_rule_vocab", ")", "\n", "hyp", ".", "rule_hidden", "=", "rule_hidden", "\n", "hyp", ".", "word_hidden", "=", "word_hidden", "\n", "hyp", ".", "rule_ctx_tm1", "=", "rule_ctx", "\n", "hyp", ".", "word_ctx_tm1", "=", "word_ctx", "\n", "\n", "rule_index", "=", "set", "(", "rule_index", ")", "\n", "logits", "=", "logits", ".", "view", "(", "-", "1", ")", "\n", "p_t", "=", "F", ".", "log_softmax", "(", "logits", ",", "dim", "=", "0", ")", ".", "data", "\n", "if", "hasattr", "(", "self", ".", "hparams", ",", "\"ignore_rule_len\"", ")", "and", "self", ".", "hparams", ".", "ignore_rule_len", ":", "\n", "          ", "l", "=", "(", "np", ".", "array", "(", "hyp", ".", "y", ")", "<", "self", ".", "hparams", ".", "target_word_vocab_size", ")", ".", "sum", "(", ")", "\n", "if", "poly_norm_m", ">", "0", "and", "l", ">", "1", ":", "\n", "            ", "new_hyp_scores", "=", "(", "hyp", ".", "score", "*", "pow", "(", "l", "-", "1", ",", "poly_norm_m", ")", "+", "p_t", ")", "/", "pow", "(", "l", ",", "poly_norm_m", ")", "\n", "", "else", ":", "\n", "            ", "new_hyp_scores", "=", "hyp", ".", "score", "+", "p_t", "\n", "", "", "else", ":", "\n", "          ", "if", "poly_norm_m", ">", "0", "and", "length", ">", "1", ":", "\n", "            ", "new_hyp_scores", "=", "(", "hyp", ".", "score", "*", "pow", "(", "length", "-", "1", ",", "poly_norm_m", ")", "+", "p_t", ")", "/", "pow", "(", "length", ",", "poly_norm_m", ")", "\n", "", "else", ":", "\n", "            ", "new_hyp_scores", "=", "hyp", ".", "score", "+", "p_t", "\n", "", "", "if", "y_label", "is", "not", "None", ":", "\n", "          ", "top_ids", "=", "[", "y_label", "[", "length", "-", "1", "]", "[", "0", "]", "]", "\n", "nll", "=", "-", "(", "p_t", "[", "top_ids", "[", "0", "]", "]", ")", "\n", "nll_score", ".", "append", "(", "nll", ")", "\n", "print", "(", "logits", "[", "top_ids", "[", "0", "]", "]", ")", "\n", "", "else", ":", "\n", "          ", "num_select", "=", "beam_size", "\n", "if", "num_rule_index", ">=", "0", ":", "num_select", "=", "min", "(", "num_select", ",", "num_rule_index", ")", "\n", "top_ids", "=", "(", "-", "new_hyp_scores", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ".", "argsort", "(", ")", "[", ":", "num_select", "]", "\n", "#print(\"top ids\" )", "\n", "", "for", "word_id", "in", "top_ids", ":", "\n", "#print(word_id)", "\n", "          ", "if", "y_label", "is", "None", "and", "len", "(", "rule_index", ")", ">", "0", "and", "word_id", "not", "in", "rule_index", ":", "continue", "\n", "#print(\"pass through\")", "\n", "open_nonterms", "=", "hyp", ".", "open_nonterms", "[", ":", "]", "\n", "if", "word_id", ">=", "self", ".", "hparams", ".", "target_word_vocab_size", ":", "\n", "            ", "rule", "=", "target_rule_vocab", "[", "word_id", "]", "\n", "# force the first rule to be not preterminal", "\n", "if", "hasattr", "(", "self", ".", "hparams", ",", "\"force_rule\"", ")", "and", "self", ".", "hparams", ".", "force_rule", ":", "\n", "#if length <= self.hparams.force_rule_step and rule.rhs[0] == \"*\": ", "\n", "              ", "if", "length", "<=", "self", ".", "hparams", ".", "force_rule_step", "and", "len", "(", "rule", ".", "rhs", ")", "==", "1", ":", "\n", "                ", "continue", "\n", "", "", "cur_nonterm", "=", "open_nonterms", ".", "pop", "(", ")", "\n", "parent_state", "=", "hyp", ".", "rule_hidden", "[", "0", "]", "\n", "for", "c", "in", "reversed", "(", "rule", ".", "rhs", ")", ":", "\n", "              ", "open_nonterms", ".", "append", "(", "OpenNonterm", "(", "label", "=", "c", ",", "parent_state", "=", "parent_state", ")", ")", "\n", "", "", "else", ":", "\n", "            ", "if", "open_nonterms", "[", "-", "1", "]", ".", "label", "!=", "'*'", ":", "\n", "              ", "print", "(", "open_nonterms", "[", "-", "1", "]", ".", "label", ",", "word_id", ",", "new_hyp_scores", "[", "word_id", "]", ")", "\n", "print", "(", "target_rule_vocab", ".", "rule_index_with_lhs", "(", "open_nonterms", "[", "-", "1", "]", ".", "label", ")", ")", "\n", "print", "(", "top_ids", ")", "\n", "", "assert", "open_nonterms", "[", "-", "1", "]", ".", "label", "==", "'*'", "\n", "if", "word_id", "==", "self", ".", "hparams", ".", "eos_id", ":", "\n", "              ", "open_nonterms", ".", "pop", "(", ")", "\n", "", "", "new_hyp", "=", "TrHyp", "(", "rule_hidden", "=", "(", "hyp", ".", "rule_hidden", "[", "0", "]", ",", "hyp", ".", "rule_hidden", "[", "1", "]", ")", ",", "\n", "word_hidden", "=", "(", "hyp", ".", "word_hidden", "[", "0", "]", ",", "hyp", ".", "word_hidden", "[", "1", "]", ")", ",", "\n", "y", "=", "hyp", ".", "y", "+", "[", "word_id", "]", ",", "\n", "rule_ctx_tm1", "=", "hyp", ".", "rule_ctx_tm1", ",", "\n", "word_ctx_tm1", "=", "hyp", ".", "word_ctx_tm1", ",", "\n", "open_nonterms", "=", "open_nonterms", ",", "\n", "score", "=", "new_hyp_scores", "[", "word_id", "]", ",", "\n", "c_p", "=", "p_t", "[", "word_id", "]", ")", "\n", "new_active_hyp", ".", "append", "(", "new_hyp", ")", "\n", "", "", "if", "y_label", "is", "None", ":", "\n", "        ", "live_hyp_num", "=", "beam_size", "-", "len", "(", "completed_hyp", ")", "\n", "new_active_hyp", "=", "sorted", "(", "new_active_hyp", ",", "key", "=", "lambda", "x", ":", "x", ".", "score", ",", "reverse", "=", "True", ")", "[", ":", "min", "(", "beam_size", ",", "live_hyp_num", ")", "]", "\n", "active_hyp", "=", "[", "]", "\n", "for", "hyp", "in", "new_active_hyp", ":", "\n", "          ", "if", "len", "(", "hyp", ".", "open_nonterms", ")", "==", "0", ":", "\n", "#if poly_norm_m <= 0:", "\n", "#  hyp.score = hyp.score / len(hyp.y)", "\n", "            ", "completed_hyp", ".", "append", "(", "hyp", ")", "\n", "", "else", ":", "\n", "            ", "active_hyp", ".", "append", "(", "hyp", ")", "\n", "", "", "", "else", ":", "\n", "        ", "active_hyp", "=", "new_active_hyp", "\n", "\n", "", "", "if", "len", "(", "completed_hyp", ")", "==", "0", ":", "\n", "      ", "completed_hyp", ".", "append", "(", "active_hyp", "[", "0", "]", ")", "\n", "", "return", "sorted", "(", "completed_hyp", ",", "key", "=", "lambda", "x", ":", "x", ".", "score", ",", "reverse", "=", "True", ")", ",", "nll_score", "\n", "\n"]], "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.trdec.TrHyp.__init__": [[466, 476], ["None"], "methods", ["None"], ["  ", "def", "__init__", "(", "self", ",", "rule_hidden", ",", "word_hidden", ",", "y", ",", "rule_ctx_tm1", ",", "word_ctx_tm1", ",", "score", ",", "open_nonterms", ",", "c_p", "=", "0", ")", ":", "\n", "    ", "self", ".", "rule_hidden", "=", "rule_hidden", "\n", "self", ".", "word_hidden", "=", "word_hidden", "\n", "# [length_y, 2], each element (index, is_word)", "\n", "self", ".", "y", "=", "y", "\n", "self", ".", "rule_ctx_tm1", "=", "rule_ctx_tm1", "\n", "self", ".", "word_ctx_tm1", "=", "word_ctx_tm1", "\n", "self", ".", "score", "=", "score", "\n", "self", ".", "open_nonterms", "=", "open_nonterms", "\n", "self", ".", "c_p", "=", "c_p", "\n", "\n"]], "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.trdec.OpenNonterm.__init__": [[478, 481], ["None"], "methods", ["None"], ["  ", "def", "__init__", "(", "self", ",", "label", ",", "parent_state", ")", ":", "\n", "    ", "self", ".", "label", "=", "label", "\n", "self", ".", "parent_state", "=", "parent_state", "\n", "", "", ""]], "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.trdec_single.TreeDecoderSingle.__init__": [[14, 75], ["torch.nn.Module.__init__", "torch.nn.Embedding", "torch.nn.Embedding", "torch.nn.Embedding", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.LSTMCell", "torch.nn.LSTMCell", "torch.nn.LSTMCell", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros.index_fill_", "torch.zeros.index_fill_", "torch.zeros.index_fill_", "hasattr", "hasattr", "hasattr", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "trdec_single.TreeDecoderSingle.rule_vocab_mask.cuda", "trdec_single.TreeDecoderSingle.word_vocab_mask.cuda", "trdec_single.TreeDecoderSingle.emb.cuda", "trdec_single.TreeDecoderSingle.readout.cuda", "trdec_single.TreeDecoderSingle.lstm_cell.cuda", "trdec_single.TreeDecoderSingle.dropout.cuda", "models.MlpAttn", "models.DotProdAttn", "models.MlpAttn", "models.MlpAttn", "models.DotProdAttn", "models.DotProdAttn", "hasattr", "trdec_single.TreeDecoderSingle.attention.cuda", "trdec_single.TreeDecoderSingle.rule_attention.cuda", "trdec_single.TreeDecoderSingle.word_attention.cuda", "hasattr", "trdec_single.TreeDecoderSingle.ctx_to_readout.cuda", "trdec_single.TreeDecoderSingle.rule_ctx_to_readout.cuda", "trdec_single.TreeDecoderSingle.word_ctx_to_readout.cuda", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange"], "methods", ["home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.utils.Logger.__init__"], ["  ", "def", "__init__", "(", "self", ",", "hparams", ")", ":", "\n", "    ", "super", "(", "TreeDecoderSingle", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "hparams", "=", "hparams", "\n", "self", ".", "target_vocab_size", "=", "self", ".", "hparams", ".", "target_word_vocab_size", "+", "self", ".", "hparams", ".", "target_rule_vocab_size", "\n", "self", ".", "emb", "=", "nn", ".", "Embedding", "(", "self", ".", "target_vocab_size", ",", "\n", "self", ".", "hparams", ".", "d_word_vec", ",", "\n", "padding_idx", "=", "hparams", ".", "pad_id", ")", "\n", "if", "hasattr", "(", "self", ".", "hparams", ",", "\"single_attn\"", ")", "and", "self", ".", "hparams", ".", "single_attn", ":", "\n", "      ", "if", "self", ".", "hparams", ".", "attn", "==", "\"mlp\"", ":", "\n", "        ", "self", ".", "attention", "=", "MlpAttn", "(", "hparams", ")", "\n", "", "else", ":", "\n", "        ", "self", ".", "attention", "=", "DotProdAttn", "(", "hparams", ")", "\n", "", "", "else", ":", "\n", "      ", "if", "self", ".", "hparams", ".", "attn", "==", "\"mlp\"", ":", "\n", "        ", "self", ".", "rule_attention", "=", "MlpAttn", "(", "hparams", ")", "\n", "self", ".", "word_attention", "=", "MlpAttn", "(", "hparams", ")", "\n", "", "else", ":", "\n", "        ", "self", ".", "rule_attention", "=", "DotProdAttn", "(", "hparams", ")", "\n", "self", ".", "word_attention", "=", "DotProdAttn", "(", "hparams", ")", "\n", "# transform [word_ctx, word_h_t, rule_ctx, rule_h_t] to readout state vectors before softmax", "\n", "", "", "if", "hasattr", "(", "self", ".", "hparams", ",", "\"single_inp_readout\"", ")", "and", "self", ".", "hparams", ".", "single_inp_readout", ":", "\n", "      ", "inp_dim", "=", "hparams", ".", "d_model", "*", "3", "\n", "", "else", ":", "\n", "      ", "inp_dim", "=", "hparams", ".", "d_model", "*", "6", "\n", "", "if", "hasattr", "(", "self", ".", "hparams", ",", "\"single_readout\"", ")", "and", "self", ".", "hparams", ".", "single_readout", ":", "\n", "      ", "self", ".", "ctx_to_readout", "=", "nn", ".", "Linear", "(", "inp_dim", ",", "hparams", ".", "d_model", ",", "bias", "=", "False", ")", "\n", "", "else", ":", "\n", "      ", "self", ".", "rule_ctx_to_readout", "=", "nn", ".", "Linear", "(", "inp_dim", ",", "hparams", ".", "d_model", ",", "bias", "=", "False", ")", "\n", "self", ".", "word_ctx_to_readout", "=", "nn", ".", "Linear", "(", "hparams", ".", "d_model", "*", "6", ",", "hparams", ".", "d_model", ",", "bias", "=", "False", ")", "\n", "\n", "", "self", ".", "readout", "=", "nn", ".", "Linear", "(", "hparams", ".", "d_model", ",", "\n", "self", ".", "target_vocab_size", ",", "\n", "bias", "=", "False", ")", "\n", "if", "self", ".", "hparams", ".", "share_emb_softmax", ":", "\n", "      ", "self", ".", "emb", ".", "weight", "=", "self", ".", "readout", ".", "weight", "\n", "# input: [y_t-1, parent_state, input_feed]", "\n", "", "inp", "=", "hparams", ".", "d_word_vec", "+", "hparams", ".", "d_model", "*", "3", "\n", "# input: [y_t-1, parent_state, input_feed]", "\n", "self", ".", "lstm_cell", "=", "nn", ".", "LSTMCell", "(", "inp", ",", "hparams", ".", "d_model", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "hparams", ".", "dropout", ")", "\n", "\n", "vocab_mask", "=", "torch", ".", "zeros", "(", "1", ",", "1", ",", "self", ".", "target_vocab_size", ")", "\n", "self", ".", "word_vocab_mask", "=", "vocab_mask", ".", "index_fill_", "(", "2", ",", "torch", ".", "arange", "(", "self", ".", "hparams", ".", "target_word_vocab_size", ")", ".", "long", "(", ")", ",", "1", ")", "\n", "self", ".", "rule_vocab_mask", "=", "1", "-", "self", ".", "word_vocab_mask", "\n", "if", "self", ".", "hparams", ".", "cuda", ":", "\n", "      ", "self", ".", "rule_vocab_mask", "=", "self", ".", "rule_vocab_mask", ".", "cuda", "(", ")", "\n", "self", ".", "word_vocab_mask", "=", "self", ".", "word_vocab_mask", ".", "cuda", "(", ")", "\n", "self", ".", "emb", "=", "self", ".", "emb", ".", "cuda", "(", ")", "\n", "if", "hasattr", "(", "self", ".", "hparams", ",", "\"single_attn\"", ")", "and", "self", ".", "hparams", ".", "single_attn", ":", "\n", "        ", "self", ".", "attention", "=", "self", ".", "attention", ".", "cuda", "(", ")", "\n", "", "else", ":", "\n", "        ", "self", ".", "rule_attention", "=", "self", ".", "rule_attention", ".", "cuda", "(", ")", "\n", "self", ".", "word_attention", "=", "self", ".", "word_attention", ".", "cuda", "(", ")", "\n", "", "if", "hasattr", "(", "self", ".", "hparams", ",", "\"single_readout\"", ")", "and", "self", ".", "hparams", ".", "single_readout", ":", "\n", "        ", "self", ".", "ctx_to_readout", "=", "self", ".", "ctx_to_readout", ".", "cuda", "(", ")", "\n", "", "else", ":", "\n", "        ", "self", ".", "rule_ctx_to_readout", "=", "self", ".", "rule_ctx_to_readout", ".", "cuda", "(", ")", "\n", "self", ".", "word_ctx_to_readout", "=", "self", ".", "word_ctx_to_readout", ".", "cuda", "(", ")", "\n", "", "self", ".", "readout", "=", "self", ".", "readout", ".", "cuda", "(", ")", "\n", "self", ".", "lstm_cell", "=", "self", ".", "lstm_cell", ".", "cuda", "(", ")", "\n", "self", ".", "dropout", "=", "self", ".", "dropout", ".", "cuda", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.trdec_single.TreeDecoderSingle.forward": [[76, 198], ["y_train.size", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "trdec_single.TreeDecoderSingle.emb", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "range", "torch.cat().transpose().contiguous", "torch.cat().transpose().contiguous", "torch.cat().transpose().contiguous", "torch.cat().transpose().contiguous", "torch.cat().transpose().contiguous", "torch.cat().transpose().contiguous", "torch.cat().transpose().contiguous", "torch.cat().transpose().contiguous", "torch.cat().transpose().contiguous", "torch.cat().transpose().contiguous.data.masked_fill_", "torch.cat().transpose().contiguous.data.masked_fill_", "torch.cat().transpose().contiguous.data.masked_fill_", "x_enc.size", "input_feed_zeros.cuda.cuda.cuda", "rule_input_feed.cuda.cuda.cuda", "word_input_feed.cuda.cuda.cuda", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "offset.cuda.cuda.cuda", "y_train[].unsqueeze().float", "trdec_single.TreeDecoderSingle.lstm_cell", "trdec_single.TreeDecoderSingle.lstm_cell", "trdec_single.TreeDecoderSingle.dropout", "trdec_single.TreeDecoderSingle.dropout", "rule_pre_readouts.append", "word_pre_readouts.append", "trdec_single.TreeDecoderSingle.readout", "hasattr", "trdec_single.TreeDecoderSingle.readout", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "score_mask.unsqueeze().float", "mask_t.byte", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.cat.cuda", "torch.cat.cuda", "torch.cat.cuda", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.index_select", "torch.index_select", "torch.index_select", "torch.index_select", "torch.index_select", "torch.index_select", "torch.index_select", "torch.index_select", "torch.index_select", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "hasattr", "hasattr", "trdec_single.TreeDecoderSingle.attention", "trdec_single.TreeDecoderSingle.attention", "trdec_single.TreeDecoderSingle.rule_attention", "trdec_single.TreeDecoderSingle.word_attention", "hasattr", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.cat().transpose", "torch.cat().transpose", "torch.cat().transpose", "torch.cat().transpose", "torch.cat().transpose", "torch.cat().transpose", "torch.cat().transpose", "torch.cat().transpose", "torch.cat().transpose", "float", "torch.cat.view", "torch.cat.view", "torch.cat.view", "y_train[].unsqueeze", "trdec_single.TreeDecoderSingle.ctx_to_readout", "trdec_single.TreeDecoderSingle.ctx_to_readout", "trdec_single.TreeDecoderSingle.rule_ctx_to_readout", "trdec_single.TreeDecoderSingle.word_ctx_to_readout", "torch.softmax", "torch.softmax", "torch.softmax", "score_mask.unsqueeze", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "x_enc", ",", "x_enc_k", ",", "dec_init", ",", "x_mask", ",", "y_train", ",", "y_mask", ",", "score_mask", ",", "y_label", "=", "None", ")", ":", "\n", "# get decoder init state and cell, use x_ct", "\n", "    ", "\"\"\"\n    x_enc: [batch_size, max_x_len, d_model * 2]\n    \"\"\"", "\n", "batch_size_x", "=", "x_enc", ".", "size", "(", ")", "[", "0", "]", "\n", "batch_size", ",", "y_max_len", ",", "data_len", "=", "y_train", ".", "size", "(", ")", "\n", "assert", "batch_size_x", "==", "batch_size", "\n", "#print(y_train)", "\n", "input_feed_zeros", "=", "torch", ".", "zeros", "(", "batch_size", ",", "self", ".", "hparams", ".", "d_model", "*", "2", ")", "\n", "if", "self", ".", "hparams", ".", "cuda", ":", "\n", "      ", "input_feed_zeros", "=", "input_feed_zeros", ".", "cuda", "(", ")", "\n", "", "rule_hidden", "=", "dec_init", "\n", "word_hidden", "=", "dec_init", "\n", "rule_input_feed", "=", "Variable", "(", "input_feed_zeros", ",", "requires_grad", "=", "False", ")", "\n", "word_input_feed", "=", "Variable", "(", "input_feed_zeros", ",", "requires_grad", "=", "False", ")", "\n", "if", "self", ".", "hparams", ".", "cuda", ":", "\n", "      ", "rule_input_feed", "=", "rule_input_feed", ".", "cuda", "(", ")", "\n", "word_input_feed", "=", "word_input_feed", ".", "cuda", "(", ")", "\n", "# [batch_size, y_len, d_word_vec]", "\n", "", "trg_emb", "=", "self", ".", "emb", "(", "y_train", "[", ":", ",", ":", ",", "0", "]", ")", "\n", "logits", "=", "[", "]", "\n", "rule_pre_readouts", "=", "[", "]", "\n", "word_pre_readouts", "=", "[", "]", "\n", "if", "self", ".", "hparams", ".", "parent_feed", ":", "\n", "      ", "all_state", "=", "Variable", "(", "torch", ".", "zeros", "(", "batch_size", ",", "self", ".", "hparams", ".", "d_model", ")", ",", "requires_grad", "=", "False", ")", "\n", "", "offset", "=", "torch", ".", "arange", "(", "batch_size", ")", ".", "long", "(", ")", "\n", "if", "self", ".", "hparams", ".", "cuda", ":", "\n", "      ", "offset", "=", "offset", ".", "cuda", "(", ")", "\n", "if", "self", ".", "hparams", ".", "parent_feed", ":", "\n", "        ", "all_state", "=", "all_state", ".", "cuda", "(", ")", "\n", "", "", "for", "t", "in", "range", "(", "y_max_len", ")", ":", "\n", "      ", "y_emb_tm1", "=", "trg_emb", "[", ":", ",", "t", ",", ":", "]", "\n", "\n", "if", "self", ".", "hparams", ".", "parent_feed", ":", "\n", "        ", "state_idx_t", "=", "t", "\n", "state_idx_t", "+=", "1", "\n", "parent_t", "=", "y_train", ".", "data", "[", ":", ",", "t", ",", "1", "]", "+", "state_idx_t", "*", "offset", "# [batch_size,]", "\n", "parent_t", "=", "Variable", "(", "parent_t", ",", "requires_grad", "=", "False", ")", "\n", "parent_state", "=", "torch", ".", "index_select", "(", "all_state", ".", "view", "(", "state_idx_t", "*", "batch_size", ",", "self", ".", "hparams", ".", "d_model", ")", ",", "dim", "=", "0", ",", "index", "=", "parent_t", ")", "# [batch_size, d_model]", "\n", "\n", "", "word_mask", "=", "y_train", "[", ":", ",", "t", ",", "2", "]", ".", "unsqueeze", "(", "1", ")", ".", "float", "(", ")", "# (1 is word, 0 is rule)", "\n", "\n", "if", "self", ".", "hparams", ".", "parent_feed", ":", "\n", "        ", "word_input", "=", "torch", ".", "cat", "(", "[", "y_emb_tm1", ",", "parent_state", ",", "word_input_feed", "]", ",", "dim", "=", "1", ")", "\n", "", "else", ":", "\n", "        ", "word_input", "=", "torch", ".", "cat", "(", "[", "y_emb_tm1", ",", "rule_hidden", "[", "0", "]", ",", "word_input_feed", "]", ",", "dim", "=", "1", ")", "\n", "", "word_h_t", ",", "word_c_t", "=", "self", ".", "lstm_cell", "(", "word_input", ",", "word_hidden", ")", "\n", "\n", "word_h_t", "=", "word_h_t", "*", "word_mask", "+", "word_hidden", "[", "0", "]", "*", "(", "1", "-", "word_mask", ")", "\n", "word_c_t", "=", "word_c_t", "*", "word_mask", "+", "word_hidden", "[", "1", "]", "*", "(", "1", "-", "word_mask", ")", "\n", "\n", "if", "self", ".", "hparams", ".", "rule_parent_feed", ":", "\n", "        ", "rule_input", "=", "torch", ".", "cat", "(", "[", "y_emb_tm1", ",", "parent_state", ",", "rule_input_feed", "]", ",", "dim", "=", "1", ")", "\n", "", "else", ":", "\n", "        ", "rule_input", "=", "torch", ".", "cat", "(", "[", "y_emb_tm1", ",", "rule_input_feed", "]", ",", "dim", "=", "1", ")", "\n", "", "rule_h_t", ",", "rule_c_t", "=", "self", ".", "lstm_cell", "(", "rule_input", ",", "rule_hidden", ")", "\n", "\n", "if", "hasattr", "(", "self", ".", "hparams", ",", "\"no_word_to_rule\"", ")", "and", "self", ".", "hparams", ".", "no_word_to_rule", ":", "\n", "        ", "eos_mask", "=", "(", "y_train", "[", ":", ",", "t", ",", "0", "]", "==", "self", ".", "hparams", ".", "eos_id", ")", ".", "unsqueeze", "(", "1", ")", ".", "float", "(", ")", "\n", "word_mask", "=", "word_mask", "-", "eos_mask", "\n", "rule_h_t", "=", "rule_h_t", "*", "(", "1", "-", "word_mask", ")", "+", "rule_hidden", "[", "0", "]", "*", "word_mask", "\n", "rule_c_t", "=", "rule_c_t", "*", "(", "1", "-", "word_mask", ")", "+", "rule_hidden", "[", "1", "]", "*", "word_mask", "\n", "\n", "", "if", "hasattr", "(", "self", ".", "hparams", ",", "\"single_attn\"", ")", "and", "self", ".", "hparams", ".", "single_attn", ":", "\n", "        ", "rule_ctx", "=", "self", ".", "attention", "(", "rule_h_t", ",", "x_enc_k", ",", "x_enc", ",", "attn_mask", "=", "x_mask", ")", "\n", "word_ctx", "=", "self", ".", "attention", "(", "word_h_t", ",", "x_enc_k", ",", "x_enc", ",", "attn_mask", "=", "x_mask", ")", "\n", "", "else", ":", "\n", "        ", "rule_ctx", "=", "self", ".", "rule_attention", "(", "rule_h_t", ",", "x_enc_k", ",", "x_enc", ",", "attn_mask", "=", "x_mask", ")", "\n", "word_ctx", "=", "self", ".", "word_attention", "(", "word_h_t", ",", "x_enc_k", ",", "x_enc", ",", "attn_mask", "=", "x_mask", ")", "\n", "\n", "", "if", "hasattr", "(", "self", ".", "hparams", ",", "\"single_inp_readout\"", ")", "and", "self", ".", "hparams", ".", "single_inp_readout", ":", "\n", "        ", "rule_pre_readout", "=", "F", ".", "tanh", "(", "self", ".", "ctx_to_readout", "(", "torch", ".", "cat", "(", "[", "rule_h_t", ",", "rule_ctx", "]", ",", "dim", "=", "1", ")", ")", ")", "\n", "word_pre_readout", "=", "F", ".", "tanh", "(", "self", ".", "ctx_to_readout", "(", "torch", ".", "cat", "(", "[", "word_h_t", ",", "word_ctx", "]", ",", "dim", "=", "1", ")", ")", ")", "\n", "", "else", ":", "\n", "        ", "inp", "=", "torch", ".", "cat", "(", "[", "rule_h_t", ",", "rule_ctx", ",", "word_h_t", ",", "word_ctx", "]", ",", "dim", "=", "1", ")", "\n", "rule_pre_readout", "=", "F", ".", "tanh", "(", "self", ".", "rule_ctx_to_readout", "(", "inp", ")", ")", "\n", "word_pre_readout", "=", "F", ".", "tanh", "(", "self", ".", "word_ctx_to_readout", "(", "inp", ")", ")", "\n", "\n", "", "rule_pre_readout", "=", "self", ".", "dropout", "(", "rule_pre_readout", ")", "\n", "word_pre_readout", "=", "self", ".", "dropout", "(", "word_pre_readout", ")", "\n", "\n", "rule_pre_readouts", ".", "append", "(", "rule_pre_readout", ")", "\n", "word_pre_readouts", ".", "append", "(", "word_pre_readout", ")", "\n", "\n", "#inp = torch.cat([rule_h_t, rule_ctx, word_h_t, word_ctx], dim=1)", "\n", "#if hasattr(self.hparams, \"single_readout\") and self.hparams.single_readout:", "\n", "#  pre_readout = F.tanh(self.ctx_to_readout(r_inp))", "\n", "#  pre_readout = self.dropout(pre_readout)", "\n", "#  pre_readouts.append(pre_readout)", "\n", "#else:", "\n", "#  rule_pre_readout = F.tanh(self.rule_ctx_to_readout(r_inp))", "\n", "#  word_pre_readout = F.tanh(self.word_ctx_to_readout(w_inp))", "\n", "#  ", "\n", "#  rule_pre_readout = self.dropout(rule_pre_readout)", "\n", "#  word_pre_readout = self.dropout(word_pre_readout)", "\n", "#  ", "\n", "#  rule_pre_readouts.append(rule_pre_readout)", "\n", "#  word_pre_readouts.append(word_pre_readout)", "\n", "\n", "rule_input_feed", "=", "rule_ctx", "\n", "word_input_feed", "=", "word_ctx", "\n", "\n", "rule_hidden", "=", "(", "rule_h_t", ",", "rule_c_t", ")", "\n", "word_hidden", "=", "(", "word_h_t", ",", "word_c_t", ")", "\n", "if", "self", ".", "hparams", ".", "parent_feed", ":", "\n", "        ", "all_state", "=", "torch", ".", "cat", "(", "[", "all_state", ",", "rule_h_t", "]", ",", "dim", "=", "1", ")", "\n", "# [len_y, batch_size, trg_vocab_size]", "\n", "", "", "rule_readouts", "=", "self", ".", "readout", "(", "torch", ".", "stack", "(", "rule_pre_readouts", ")", ")", "[", ":", ",", ":", ",", "-", "self", ".", "hparams", ".", "target_rule_vocab_size", ":", "]", "\n", "if", "hasattr", "(", "self", ".", "hparams", ",", "\"rule_tanh\"", ")", "and", "self", ".", "hparams", ".", "rule_tanh", ">", "0", ":", "\n", "      ", "rule_readouts", "=", "self", ".", "hparams", ".", "rule_tanh", "*", "torch", ".", "tanh", "(", "rule_readouts", ")", "\n", "", "word_readouts", "=", "self", ".", "readout", "(", "torch", ".", "stack", "(", "word_pre_readouts", ")", ")", "[", ":", ",", ":", ",", ":", "self", ".", "hparams", ".", "target_word_vocab_size", "]", "\n", "if", "self", ".", "hparams", ".", "label_smooth", ">", "0", ":", "\n", "      ", "smooth", "=", "self", ".", "hparams", ".", "label_smooth", "\n", "rule_probs", "=", "(", "1.0", "-", "smooth", ")", "*", "F", ".", "softmax", "(", "rule_readouts", ",", "dim", "=", "2", ")", "+", "smooth", "/", "self", ".", "hparams", ".", "target_rule_vocab_size", "\n", "rule_readouts", "=", "torch", ".", "log", "(", "rule_probs", ")", "\n", "# [batch_size, len_y, trg_vocab_size]", "\n", "", "logits", "=", "torch", ".", "cat", "(", "[", "word_readouts", ",", "rule_readouts", "]", ",", "dim", "=", "2", ")", ".", "transpose", "(", "0", ",", "1", ")", ".", "contiguous", "(", ")", "\n", "score_mask", "=", "score_mask", ".", "unsqueeze", "(", "2", ")", ".", "float", "(", ")", ".", "data", "\n", "mask_t", "=", "self", ".", "word_vocab_mask", "*", "(", "1", "-", "score_mask", ")", "+", "self", ".", "rule_vocab_mask", "*", "score_mask", "\n", "logits", ".", "data", ".", "masked_fill_", "(", "mask_t", ".", "byte", "(", ")", ",", "-", "float", "(", "\"inf\"", ")", ")", "\n", "return", "logits", "\n", "\n"]], "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.trdec_single.TreeDecoderSingle.step": [[199, 292], ["torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "trdec_single.TreeDecoderSingle.emb", "torch.ones().byte", "torch.ones().byte", "torch.ones().byte", "torch.ones().byte", "torch.ones().byte", "torch.ones().byte", "torch.ones().byte", "torch.ones().byte", "torch.ones().byte", "trdec_single.TreeDecoderSingle.data.masked_fill_", "y_tm1.cuda.cuda.cuda", "trdec_single.TreeDecoderSingle.lstm_cell", "hasattr", "trdec_single.TreeDecoderSingle.lstm_cell", "hasattr", "trdec_single.TreeDecoderSingle.attention", "trdec_single.TreeDecoderSingle.rule_attention", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "mask.cuda.cuda.index_fill_", "trdec_single.TreeDecoderSingle.dropout", "trdec_single.TreeDecoderSingle.readout", "target_rule_vocab.rule_index_with_lhs", "len", "mask.cuda.cuda.index_fill_", "trdec_single.TreeDecoderSingle.readout", "mask.cuda.cuda.cuda", "int", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "hasattr", "trdec_single.TreeDecoderSingle.attention", "trdec_single.TreeDecoderSingle.word_attention", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "hasattr", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "hasattr", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "rule_select_index.append", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "hasattr", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "hasattr", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "float", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "trdec_single.TreeDecoderSingle.ctx_to_readout", "trdec_single.TreeDecoderSingle.word_ctx_to_readout", "trdec_single.TreeDecoderSingle.ctx_to_readout", "trdec_single.TreeDecoderSingle.rule_ctx_to_readout", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange"], "methods", ["home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.tree_utils.RuleVocab.rule_index_with_lhs"], ["", "def", "step", "(", "self", ",", "x_enc", ",", "x_enc_k", ",", "hyp", ",", "target_rule_vocab", ")", ":", "\n", "    ", "y_tm1", "=", "torch", ".", "LongTensor", "(", "[", "int", "(", "hyp", ".", "y", "[", "-", "1", "]", ")", "]", ")", "\n", "if", "self", ".", "hparams", ".", "cuda", ":", "\n", "      ", "y_tm1", "=", "y_tm1", ".", "cuda", "(", ")", "\n", "", "y_tm1", "=", "Variable", "(", "y_tm1", ",", "volatile", "=", "True", ")", "\n", "open_nonterms", "=", "hyp", ".", "open_nonterms", "\n", "rule_input_feed", "=", "hyp", ".", "rule_ctx_tm1", "\n", "word_input_feed", "=", "hyp", ".", "word_ctx_tm1", "\n", "rule_hidden", "=", "hyp", ".", "rule_hidden", "\n", "word_hidden", "=", "hyp", ".", "word_hidden", "\n", "word_h_t", ",", "word_c_t", "=", "word_hidden", "\n", "rule_h_t", ",", "rule_c_t", "=", "rule_hidden", "\n", "\n", "y_emb_tm1", "=", "self", ".", "emb", "(", "y_tm1", ")", "\n", "cur_nonterm", "=", "open_nonterms", "[", "-", "1", "]", "\n", "parent_state", "=", "cur_nonterm", ".", "parent_state", "\n", "\n", "if", "hyp", ".", "y", "[", "-", "1", "]", "<", "self", ".", "hparams", ".", "target_word_vocab_size", ":", "\n", "# word", "\n", "      ", "if", "self", ".", "hparams", ".", "parent_feed", ":", "\n", "        ", "word_input", "=", "torch", ".", "cat", "(", "[", "y_emb_tm1", ",", "parent_state", ",", "word_input_feed", "]", ",", "dim", "=", "1", ")", "\n", "", "else", ":", "\n", "        ", "word_input", "=", "torch", ".", "cat", "(", "[", "y_emb_tm1", ",", "rule_h_t", ",", "word_input_feed", "]", ",", "dim", "=", "1", ")", "\n", "", "word_h_t", ",", "word_c_t", "=", "self", ".", "lstm_cell", "(", "word_input", ",", "word_hidden", ")", "\n", "if", "hasattr", "(", "self", ".", "hparams", ",", "\"single_attn\"", ")", "and", "self", ".", "hparams", ".", "single_attn", ":", "\n", "        ", "word_ctx", "=", "self", ".", "attention", "(", "word_h_t", ",", "x_enc_k", ",", "x_enc", ")", "\n", "", "else", ":", "\n", "        ", "word_ctx", "=", "self", ".", "word_attention", "(", "word_h_t", ",", "x_enc_k", ",", "x_enc", ")", "\n", "", "", "else", ":", "\n", "      ", "word_ctx", "=", "hyp", ".", "word_ctx_tm1", "\n", "\n", "", "update_rule_rnn", "=", "True", "\n", "if", "hasattr", "(", "self", ".", "hparams", ",", "\"no_word_to_rule\"", ")", "and", "self", ".", "hparams", ".", "no_word_to_rule", "and", "hyp", ".", "y", "[", "-", "1", "]", "<", "self", ".", "hparams", ".", "target_word_vocab_size", ":", "\n", "      ", "if", "hyp", ".", "y", "[", "-", "1", "]", "!=", "self", ".", "hparams", ".", "eos_id", ":", "\n", "        ", "update_rule_rnn", "=", "False", "\n", "\n", "", "", "if", "update_rule_rnn", ":", "\n", "      ", "if", "self", ".", "hparams", ".", "rule_parent_feed", ":", "\n", "        ", "rule_input", "=", "torch", ".", "cat", "(", "[", "y_emb_tm1", ",", "parent_state", ",", "rule_input_feed", "]", ",", "dim", "=", "1", ")", "\n", "", "else", ":", "\n", "        ", "rule_input", "=", "torch", ".", "cat", "(", "[", "y_emb_tm1", ",", "rule_input_feed", "]", ",", "dim", "=", "1", ")", "\n", "", "rule_h_t", ",", "rule_c_t", "=", "self", ".", "lstm_cell", "(", "rule_input", ",", "rule_hidden", ")", "\n", "#  if hasattr(self.hparams, \"single_attn\") and self.hparams.single_attn:", "\n", "#    rule_ctx = self.attention(rule_h_t, x_enc_k, x_enc)", "\n", "#  else:", "\n", "#    rule_ctx = self.rule_attention(rule_h_t, x_enc_k, x_enc)", "\n", "#else:", "\n", "#  rule_ctx = hyp.rule_ctx_tm1", "\n", "", "if", "hasattr", "(", "self", ".", "hparams", ",", "\"single_attn\"", ")", "and", "self", ".", "hparams", ".", "single_attn", ":", "\n", "      ", "rule_ctx", "=", "self", ".", "attention", "(", "rule_h_t", ",", "x_enc_k", ",", "x_enc", ")", "\n", "", "else", ":", "\n", "      ", "rule_ctx", "=", "self", ".", "rule_attention", "(", "rule_h_t", ",", "x_enc_k", ",", "x_enc", ")", "\n", "\n", "", "mask", "=", "torch", ".", "ones", "(", "1", ",", "self", ".", "target_vocab_size", ")", ".", "byte", "(", ")", "\n", "if", "cur_nonterm", ".", "label", "==", "'*'", ":", "\n", "      ", "word_index", "=", "torch", ".", "arange", "(", "self", ".", "hparams", ".", "target_word_vocab_size", ")", ".", "long", "(", ")", "\n", "mask", ".", "index_fill_", "(", "1", ",", "word_index", ",", "0", ")", "\n", "if", "hasattr", "(", "self", ".", "hparams", ",", "\"single_inp_readout\"", ")", "and", "self", ".", "hparams", ".", "single_inp_readout", ":", "\n", "        ", "inp", "=", "torch", ".", "cat", "(", "[", "word_h_t", ",", "word_ctx", "]", ",", "dim", "=", "1", ")", "\n", "", "else", ":", "\n", "        ", "inp", "=", "torch", ".", "cat", "(", "[", "rule_h_t", ",", "rule_ctx", ",", "word_h_t", ",", "word_ctx", "]", ",", "dim", "=", "1", ")", "\n", "#inp = torch.cat([rule_h_t, rule_ctx, word_h_t, word_ctx], dim=1)", "\n", "", "if", "hasattr", "(", "self", ".", "hparams", ",", "\"single_readout\"", ")", "and", "self", ".", "hparams", ".", "single_readout", ":", "\n", "        ", "word_pre_readout", "=", "F", ".", "tanh", "(", "self", ".", "ctx_to_readout", "(", "inp", ")", ")", "\n", "", "else", ":", "\n", "        ", "word_pre_readout", "=", "F", ".", "tanh", "(", "self", ".", "word_ctx_to_readout", "(", "inp", ")", ")", "\n", "", "word_pre_readout", "=", "self", ".", "dropout", "(", "word_pre_readout", ")", "\n", "score_t", "=", "self", ".", "readout", "(", "word_pre_readout", ")", "\n", "num_rule_index", "=", "-", "1", "\n", "rule_select_index", "=", "[", "]", "\n", "", "else", ":", "\n", "      ", "rule_with_lhs", "=", "target_rule_vocab", ".", "rule_index_with_lhs", "(", "cur_nonterm", ".", "label", ")", "\n", "rule_select_index", "=", "[", "]", "\n", "for", "i", "in", "rule_with_lhs", ":", "rule_select_index", ".", "append", "(", "i", "+", "self", ".", "hparams", ".", "target_word_vocab_size", ")", "\n", "num_rule_index", "=", "len", "(", "rule_with_lhs", ")", "\n", "rule_index", "=", "torch", ".", "arange", "(", "self", ".", "hparams", ".", "target_rule_vocab_size", ")", ".", "long", "(", ")", "+", "self", ".", "hparams", ".", "target_word_vocab_size", "\n", "mask", ".", "index_fill_", "(", "1", ",", "rule_index", ",", "0", ")", "\n", "if", "hasattr", "(", "self", ".", "hparams", ",", "\"single_inp_readout\"", ")", "and", "self", ".", "hparams", ".", "single_inp_readout", ":", "\n", "        ", "inp", "=", "torch", ".", "cat", "(", "[", "rule_h_t", ",", "rule_ctx", "]", ",", "dim", "=", "1", ")", "\n", "", "else", ":", "\n", "        ", "inp", "=", "torch", ".", "cat", "(", "[", "rule_h_t", ",", "rule_ctx", ",", "word_h_t", ",", "word_ctx", "]", ",", "dim", "=", "1", ")", "\n", "", "if", "hasattr", "(", "self", ".", "hparams", ",", "\"single_readout\"", ")", "and", "self", ".", "hparams", ".", "single_readout", ":", "\n", "        ", "rule_pre_readout", "=", "F", ".", "tanh", "(", "self", ".", "ctx_to_readout", "(", "inp", ")", ")", "\n", "", "else", ":", "\n", "        ", "rule_pre_readout", "=", "F", ".", "tanh", "(", "self", ".", "rule_ctx_to_readout", "(", "inp", ")", ")", "\n", "#rule_pre_readout = self.dropout(rule_pre_readout)", "\n", "", "score_t", "=", "self", ".", "readout", "(", "rule_pre_readout", ")", "\n", "", "if", "self", ".", "hparams", ".", "cuda", ":", "\n", "      ", "mask", "=", "mask", ".", "cuda", "(", ")", "\n", "", "score_t", ".", "data", ".", "masked_fill_", "(", "mask", ",", "-", "float", "(", "\"inf\"", ")", ")", "\n", "rule_hidden", "=", "(", "rule_h_t", ",", "rule_c_t", ")", "\n", "word_hidden", "=", "(", "word_h_t", ",", "word_c_t", ")", "\n", "return", "score_t", ",", "rule_hidden", ",", "word_hidden", ",", "rule_ctx", ",", "word_ctx", ",", "num_rule_index", ",", "rule_select_index", "\n", "\n"]], "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.trdec_single.TrDecSingle.__init__": [[295, 304], ["torch.nn.Module.__init__", "models.Encoder", "trdec_single.TreeDecoderSingle", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "trdec_single.TrDecSingle.enc_to_k.cuda"], "methods", ["home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.utils.Logger.__init__"], ["  ", "def", "__init__", "(", "self", ",", "hparams", ")", ":", "\n", "    ", "super", "(", "TrDecSingle", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "encoder", "=", "Encoder", "(", "hparams", ")", "\n", "self", ".", "decoder", "=", "TreeDecoderSingle", "(", "hparams", ")", "\n", "# transform encoder state vectors into attention key vector", "\n", "self", ".", "enc_to_k", "=", "nn", ".", "Linear", "(", "hparams", ".", "d_model", "*", "2", ",", "hparams", ".", "d_model", ",", "bias", "=", "False", ")", "\n", "self", ".", "hparams", "=", "hparams", "\n", "if", "self", ".", "hparams", ".", "cuda", ":", "\n", "      ", "self", ".", "enc_to_k", "=", "self", ".", "enc_to_k", ".", "cuda", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.trdec_single.TrDecSingle.forward": [[305, 312], ["trdec_single.TrDecSingle.encoder", "trdec_single.TrDecSingle.enc_to_k", "trdec_single.TrDecSingle.decoder"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "x_train", ",", "x_mask", ",", "x_len", ",", "y_train", ",", "y_mask", ",", "y_len", ",", "score_mask", ",", "y_label", "=", "None", ")", ":", "\n", "# [batch_size, x_len, d_model * 2]", "\n", "    ", "x_enc", ",", "dec_init", "=", "self", ".", "encoder", "(", "x_train", ",", "x_len", ")", "\n", "x_enc_k", "=", "self", ".", "enc_to_k", "(", "x_enc", ")", "\n", "# [batch_size, y_len-1, trg_vocab_size]", "\n", "logits", "=", "self", ".", "decoder", "(", "x_enc", ",", "x_enc_k", ",", "dec_init", ",", "x_mask", ",", "y_train", ",", "y_mask", ",", "score_mask", ",", "y_label", ")", "\n", "return", "logits", "\n", "\n"]], "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.trdec_single.TrDecSingle.translate": [[313, 336], ["torch.nn.LogSoftmax", "torch.nn.LogSoftmax", "torch.nn.LogSoftmax", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "trdec_single.TrDecSingle.translate_sent", "hyps.append", "scores.append", "gc.collect", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "x.cuda.cuda.cuda", "sum"], "methods", ["home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.trdec_single.TrDecSingle.translate_sent"], ["", "def", "translate", "(", "self", ",", "x_train", ",", "target_rule_vocab", ",", "max_len", "=", "100", ",", "beam_size", "=", "5", ",", "y_label", "=", "None", ",", "poly_norm_m", "=", "0", ")", ":", "\n", "    ", "hyps", "=", "[", "]", "\n", "scores", "=", "[", "]", "\n", "i", "=", "0", "\n", "self", ".", "logsoftmax", "=", "nn", ".", "LogSoftmax", "(", "dim", "=", "1", ")", "\n", "for", "x", "in", "x_train", ":", "\n", "      ", "x", "=", "Variable", "(", "torch", ".", "LongTensor", "(", "x", ")", ",", "volatile", "=", "True", ")", "\n", "if", "y_label", ":", "\n", "        ", "y", "=", "y_label", "[", "i", "]", "[", "1", ":", "]", "\n", "", "else", ":", "\n", "        ", "y", "=", "None", "\n", "", "if", "self", ".", "hparams", ".", "cuda", ":", "\n", "        ", "x", "=", "x", ".", "cuda", "(", ")", "\n", "", "hyp", ",", "nll_score", "=", "self", ".", "translate_sent", "(", "x", ",", "target_rule_vocab", ",", "max_len", "=", "max_len", ",", "beam_size", "=", "beam_size", ",", "y_label", "=", "y", ",", "poly_norm_m", "=", "poly_norm_m", ")", "\n", "hyp", "=", "hyp", "[", "0", "]", "\n", "hyps", ".", "append", "(", "hyp", ".", "y", "[", "1", ":", "]", ")", "\n", "scores", ".", "append", "(", "sum", "(", "nll_score", ")", ")", "\n", "#print(hyp.y)", "\n", "#print(\"trans score:\", nll_score)", "\n", "#print(\"trans label:\", y)", "\n", "i", "+=", "1", "\n", "gc", ".", "collect", "(", ")", "\n", "", "return", "hyps", ",", "scores", "\n", "\n"]], "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.trdec_single.TrDecSingle.translate_sent": [[337, 444], ["x_train.unsqueeze.unsqueeze.unsqueeze", "trdec_single.TrDecSingle.encoder", "trdec_single.TrDecSingle.enc_to_k", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "len", "x_train.unsqueeze.unsqueeze.size", "input_feed_zeros.cuda.cuda.cuda", "state_zeros.cuda.cuda.cuda", "trdec_single.TrHyp", "len", "enumerate", "len", "completed_hyp.append", "sorted", "x_train.unsqueeze.unsqueeze.size", "len", "trdec_single.TrDecSingle.decoder.step", "set", "logits.view.view.view", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "hasattr", "nll_score.append", "print", "trdec_single.TrHyp", "new_active_hyp.append", "len", "sorted", "trdec_single.OpenNonterm", "min", "open_nonterms.pop", "reversed", "min", "len", "completed_hyp.append", "active_hyp.append", "pow", "pow", "len", "hasattr", "open_nonterms.append", "print", "print", "print", "open_nonterms.pop", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "numpy.array", "trdec_single.OpenNonterm", "target_rule_vocab.rule_index_with_lhs", "pow", "pow"], "methods", ["home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.trdec_single.TreeDecoderSingle.step", "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.tree_utils.RuleVocab.rule_index_with_lhs"], ["", "def", "translate_sent", "(", "self", ",", "x_train", ",", "target_rule_vocab", ",", "max_len", "=", "100", ",", "beam_size", "=", "5", ",", "y_label", "=", "None", ",", "poly_norm_m", "=", "1.", ")", ":", "\n", "    ", "assert", "len", "(", "x_train", ".", "size", "(", ")", ")", "==", "1", "\n", "x_len", "=", "[", "x_train", ".", "size", "(", "0", ")", "]", "\n", "x_train", "=", "x_train", ".", "unsqueeze", "(", "0", ")", "\n", "x_enc", ",", "dec_init", "=", "self", ".", "encoder", "(", "x_train", ",", "x_len", ")", "\n", "x_enc_k", "=", "self", ".", "enc_to_k", "(", "x_enc", ")", "\n", "length", "=", "0", "\n", "completed_hyp", "=", "[", "]", "\n", "input_feed_zeros", "=", "torch", ".", "zeros", "(", "1", ",", "self", ".", "hparams", ".", "d_model", "*", "2", ")", "\n", "state_zeros", "=", "torch", ".", "zeros", "(", "1", ",", "self", ".", "hparams", ".", "d_model", ")", "\n", "if", "self", ".", "hparams", ".", "cuda", ":", "\n", "      ", "input_feed_zeros", "=", "input_feed_zeros", ".", "cuda", "(", ")", "\n", "state_zeros", "=", "state_zeros", ".", "cuda", "(", ")", "\n", "", "rule_input_feed", "=", "Variable", "(", "input_feed_zeros", ",", "requires_grad", "=", "False", ")", "\n", "word_input_feed", "=", "Variable", "(", "input_feed_zeros", ",", "requires_grad", "=", "False", ")", "\n", "active_hyp", "=", "[", "TrHyp", "(", "rule_hidden", "=", "dec_init", ",", "\n", "word_hidden", "=", "dec_init", ",", "\n", "y", "=", "[", "self", ".", "hparams", ".", "bos_id", "]", ",", "\n", "rule_ctx_tm1", "=", "rule_input_feed", ",", "\n", "word_ctx_tm1", "=", "word_input_feed", ",", "\n", "open_nonterms", "=", "[", "OpenNonterm", "(", "label", "=", "self", ".", "hparams", ".", "root_label", ",", "\n", "parent_state", "=", "Variable", "(", "state_zeros", ",", "requires_grad", "=", "False", ")", ")", "]", ",", "\n", "score", "=", "0.", ")", "]", "\n", "nll_score", "=", "[", "]", "\n", "if", "y_label", "is", "not", "None", ":", "\n", "      ", "max_len", "=", "len", "(", "y_label", ")", "\n", "", "while", "len", "(", "completed_hyp", ")", "<", "beam_size", "and", "length", "<", "max_len", ":", "\n", "      ", "length", "+=", "1", "\n", "new_active_hyp", "=", "[", "]", "\n", "for", "i", ",", "hyp", "in", "enumerate", "(", "active_hyp", ")", ":", "\n", "        ", "logits", ",", "rule_hidden", ",", "word_hidden", ",", "rule_ctx", ",", "word_ctx", ",", "num_rule_index", ",", "rule_index", "=", "self", ".", "decoder", ".", "step", "(", "x_enc", ",", "\n", "x_enc_k", ",", "hyp", ",", "target_rule_vocab", ")", "\n", "hyp", ".", "rule_hidden", "=", "rule_hidden", "\n", "hyp", ".", "word_hidden", "=", "word_hidden", "\n", "hyp", ".", "rule_ctx_tm1", "=", "rule_ctx", "\n", "hyp", ".", "word_ctx_tm1", "=", "word_ctx", "\n", "\n", "rule_index", "=", "set", "(", "rule_index", ")", "\n", "logits", "=", "logits", ".", "view", "(", "-", "1", ")", "\n", "p_t", "=", "F", ".", "log_softmax", "(", "logits", ",", "dim", "=", "0", ")", ".", "data", "\n", "if", "hasattr", "(", "self", ".", "hparams", ",", "\"ignore_rule_len\"", ")", "and", "self", ".", "hparams", ".", "ignore_rule_len", ":", "\n", "          ", "l", "=", "(", "np", ".", "array", "(", "hyp", ".", "y", ")", "<", "self", ".", "hparams", ".", "target_word_vocab_size", ")", ".", "sum", "(", ")", "\n", "if", "poly_norm_m", ">", "0", "and", "l", ">", "1", ":", "\n", "            ", "new_hyp_scores", "=", "(", "hyp", ".", "score", "*", "pow", "(", "l", "-", "1", ",", "poly_norm_m", ")", "+", "p_t", ")", "/", "pow", "(", "l", ",", "poly_norm_m", ")", "\n", "", "else", ":", "\n", "            ", "new_hyp_scores", "=", "hyp", ".", "score", "+", "p_t", "\n", "", "", "else", ":", "\n", "          ", "if", "poly_norm_m", ">", "0", "and", "length", ">", "1", ":", "\n", "            ", "new_hyp_scores", "=", "(", "hyp", ".", "score", "*", "pow", "(", "length", "-", "1", ",", "poly_norm_m", ")", "+", "p_t", ")", "/", "pow", "(", "length", ",", "poly_norm_m", ")", "\n", "", "else", ":", "\n", "            ", "new_hyp_scores", "=", "hyp", ".", "score", "+", "p_t", "\n", "", "", "if", "y_label", "is", "not", "None", ":", "\n", "          ", "top_ids", "=", "[", "y_label", "[", "length", "-", "1", "]", "[", "0", "]", "]", "\n", "nll", "=", "-", "(", "p_t", "[", "top_ids", "[", "0", "]", "]", ")", "\n", "nll_score", ".", "append", "(", "nll", ")", "\n", "print", "(", "logits", "[", "top_ids", "[", "0", "]", "]", ")", "\n", "", "else", ":", "\n", "          ", "num_select", "=", "beam_size", "\n", "if", "num_rule_index", ">=", "0", ":", "num_select", "=", "min", "(", "num_select", ",", "num_rule_index", ")", "\n", "top_ids", "=", "(", "-", "new_hyp_scores", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ".", "argsort", "(", ")", "[", ":", "num_select", "]", "\n", "", "for", "word_id", "in", "top_ids", ":", "\n", "          ", "if", "y_label", "is", "None", "and", "len", "(", "rule_index", ")", ">", "0", "and", "word_id", "not", "in", "rule_index", ":", "continue", "\n", "open_nonterms", "=", "hyp", ".", "open_nonterms", "[", ":", "]", "\n", "if", "word_id", ">=", "self", ".", "hparams", ".", "target_word_vocab_size", ":", "\n", "            ", "rule", "=", "target_rule_vocab", "[", "word_id", "]", "\n", "# force the first rule to be not preterminal", "\n", "if", "hasattr", "(", "self", ".", "hparams", ",", "\"force_rule\"", ")", "and", "self", ".", "hparams", ".", "force_rule", ":", "\n", "              ", "if", "length", "<=", "self", ".", "hparams", ".", "force_rule_step", "and", "rule", ".", "rhs", "[", "0", "]", "==", "\"*\"", ":", "\n", "                ", "continue", "\n", "", "", "cur_nonterm", "=", "open_nonterms", ".", "pop", "(", ")", "\n", "parent_state", "=", "hyp", ".", "rule_hidden", "[", "0", "]", "\n", "for", "c", "in", "reversed", "(", "rule", ".", "rhs", ")", ":", "\n", "              ", "open_nonterms", ".", "append", "(", "OpenNonterm", "(", "label", "=", "c", ",", "parent_state", "=", "parent_state", ")", ")", "\n", "", "", "else", ":", "\n", "            ", "if", "open_nonterms", "[", "-", "1", "]", ".", "label", "!=", "'*'", ":", "\n", "              ", "print", "(", "open_nonterms", "[", "-", "1", "]", ".", "label", ",", "word_id", ",", "new_hyp_scores", "[", "word_id", "]", ")", "\n", "print", "(", "target_rule_vocab", ".", "rule_index_with_lhs", "(", "open_nonterms", "[", "-", "1", "]", ".", "label", ")", ")", "\n", "print", "(", "top_ids", ")", "\n", "", "assert", "open_nonterms", "[", "-", "1", "]", ".", "label", "==", "'*'", "\n", "if", "word_id", "==", "self", ".", "hparams", ".", "eos_id", ":", "\n", "              ", "open_nonterms", ".", "pop", "(", ")", "\n", "", "", "new_hyp", "=", "TrHyp", "(", "rule_hidden", "=", "(", "hyp", ".", "rule_hidden", "[", "0", "]", ",", "hyp", ".", "rule_hidden", "[", "1", "]", ")", ",", "\n", "word_hidden", "=", "(", "hyp", ".", "word_hidden", "[", "0", "]", ",", "hyp", ".", "word_hidden", "[", "1", "]", ")", ",", "\n", "y", "=", "hyp", ".", "y", "+", "[", "word_id", "]", ",", "\n", "rule_ctx_tm1", "=", "hyp", ".", "rule_ctx_tm1", ",", "\n", "word_ctx_tm1", "=", "hyp", ".", "word_ctx_tm1", ",", "\n", "open_nonterms", "=", "open_nonterms", ",", "\n", "score", "=", "new_hyp_scores", "[", "word_id", "]", ",", "\n", "c_p", "=", "p_t", "[", "word_id", "]", ")", "\n", "new_active_hyp", ".", "append", "(", "new_hyp", ")", "\n", "", "", "if", "y_label", "is", "None", ":", "\n", "        ", "live_hyp_num", "=", "beam_size", "-", "len", "(", "completed_hyp", ")", "\n", "new_active_hyp", "=", "sorted", "(", "new_active_hyp", ",", "key", "=", "lambda", "x", ":", "x", ".", "score", ",", "reverse", "=", "True", ")", "[", ":", "min", "(", "beam_size", ",", "live_hyp_num", ")", "]", "\n", "active_hyp", "=", "[", "]", "\n", "for", "hyp", "in", "new_active_hyp", ":", "\n", "          ", "if", "len", "(", "hyp", ".", "open_nonterms", ")", "==", "0", ":", "\n", "#if poly_norm_m <= 0:", "\n", "#  hyp.score = hyp.score / len(hyp.y)", "\n", "            ", "completed_hyp", ".", "append", "(", "hyp", ")", "\n", "", "else", ":", "\n", "            ", "active_hyp", ".", "append", "(", "hyp", ")", "\n", "", "", "", "else", ":", "\n", "        ", "active_hyp", "=", "new_active_hyp", "\n", "\n", "", "", "if", "len", "(", "completed_hyp", ")", "==", "0", ":", "\n", "      ", "completed_hyp", ".", "append", "(", "active_hyp", "[", "0", "]", ")", "\n", "", "return", "sorted", "(", "completed_hyp", ",", "key", "=", "lambda", "x", ":", "x", ".", "score", ",", "reverse", "=", "True", ")", ",", "nll_score", "\n", "\n"]], "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.trdec_single.TrHyp.__init__": [[446, 456], ["None"], "methods", ["None"], ["  ", "def", "__init__", "(", "self", ",", "rule_hidden", ",", "word_hidden", ",", "y", ",", "rule_ctx_tm1", ",", "word_ctx_tm1", ",", "score", ",", "open_nonterms", ",", "c_p", "=", "0", ")", ":", "\n", "    ", "self", ".", "rule_hidden", "=", "rule_hidden", "\n", "self", ".", "word_hidden", "=", "word_hidden", "\n", "# [length_y, 2], each element (index, is_word)", "\n", "self", ".", "y", "=", "y", "\n", "self", ".", "rule_ctx_tm1", "=", "rule_ctx_tm1", "\n", "self", ".", "word_ctx_tm1", "=", "word_ctx_tm1", "\n", "self", ".", "score", "=", "score", "\n", "self", ".", "open_nonterms", "=", "open_nonterms", "\n", "self", ".", "c_p", "=", "c_p", "\n", "\n"]], "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.trdec_single.OpenNonterm.__init__": [[458, 461], ["None"], "methods", ["None"], ["  ", "def", "__init__", "(", "self", ",", "label", ",", "parent_state", ")", ":", "\n", "    ", "self", ".", "label", "=", "label", "\n", "self", ".", "parent_state", "=", "parent_state", "\n", "", "", ""]], "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.tree_utils.Vocab.__init__": [[21, 33], ["tree_utils.Vocab.i2w_from_vocab_file", "enumerate"], "methods", ["home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.tree_utils.Vocab.i2w_from_vocab_file"], ["def", "__init__", "(", "self", ",", "hparams", ",", "i2w", "=", "None", ",", "vocab_file", "=", "None", ",", "frozen", "=", "True", ")", ":", "\n", "    ", "assert", "i2w", "is", "None", "or", "vocab_file", "is", "None", "\n", "self", ".", "hparams", "=", "hparams", "\n", "self", ".", "frozen", "=", "frozen", "\n", "if", "vocab_file", ":", "\n", "      ", "i2w", "=", "self", ".", "i2w_from_vocab_file", "(", "vocab_file", ")", "\n", "", "if", "(", "i2w", "is", "not", "None", ")", ":", "\n", "      ", "self", ".", "i2w", "=", "i2w", "\n", "self", ".", "w2i", "=", "{", "word", ":", "word_id", "for", "(", "word_id", ",", "word", ")", "in", "enumerate", "(", "self", ".", "i2w", ")", "}", "\n", "", "else", ":", "\n", "      ", "self", ".", "w2i", "=", "{", "\"<pad>\"", ":", "0", ",", "\"<unk>\"", ":", "1", ",", "\"<s>\"", ":", "2", ",", "\"</s>\"", ":", "3", "}", "\n", "self", ".", "i2w", "=", "[", "\"<pad>\"", ",", "\"<unk>\"", ",", "\"<s>\"", ",", "\"</s>\"", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.tree_utils.Vocab.i2w_from_vocab_file": [[34, 45], ["open", "line.strip", "vocab.append"], "methods", ["None"], ["", "", "def", "i2w_from_vocab_file", "(", "self", ",", "vocab_file", ")", ":", "\n", "    ", "\"\"\"\n    Args:\n      vocab_file: file containing one word per line, and not containing <s>, </s>, <unk>\n    \"\"\"", "\n", "vocab", "=", "[", "]", "\n", "with", "open", "(", "vocab_file", ",", "encoding", "=", "'utf-8'", ")", "as", "f", ":", "\n", "      ", "for", "line", "in", "f", ":", "\n", "        ", "word", "=", "line", ".", "strip", "(", ")", "\n", "vocab", ".", "append", "(", "word", ")", "\n", "", "", "return", "vocab", "\n", "\n"]], "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.tree_utils.Vocab.convert": [[46, 53], ["len", "tree_utils.Vocab.i2w.append"], "methods", ["None"], ["", "def", "convert", "(", "self", ",", "w", ")", ":", "\n", "    ", "if", "w", "not", "in", "self", ".", "w2i", ":", "\n", "      ", "if", "self", ".", "frozen", ":", "\n", "       ", "return", "self", ".", "UNK", "\n", "", "self", ".", "w2i", "[", "w", "]", "=", "len", "(", "self", ".", "i2w", ")", "\n", "self", ".", "i2w", ".", "append", "(", "w", ")", "\n", "", "return", "self", ".", "w2i", "[", "w", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.tree_utils.Vocab.__getitem__": [[54, 56], ["None"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "i", ")", ":", "\n", "    ", "return", "self", ".", "i2w", "[", "i", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.tree_utils.Vocab.__len__": [[57, 59], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "    ", "return", "len", "(", "self", ".", "i2w", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.tree_utils.RuleVocab.__init__": [[69, 84], ["tree_utils.RuleVocab.from_vocab_file", "collections.defaultdict"], "methods", ["home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.tree_utils.RuleVocab.from_vocab_file"], ["def", "__init__", "(", "self", ",", "hparams", ",", "vocab_file", "=", "None", ",", "frozen", "=", "True", ",", "offset", "=", "0", ")", ":", "\n", "    ", "\"\"\"\n    :param i2w: list of words, including <s> and </s>\n    :param vocab_file: file containing one word per line, and not containing <s>, </s>, <unk>\n    i2w and vocab_file are mutually exclusive\n    \"\"\"", "\n", "self", ".", "hparams", "=", "hparams", "\n", "self", ".", "frozen", "=", "frozen", "\n", "self", ".", "offset", "=", "offset", "\n", "if", "vocab_file", ":", "\n", "      ", "self", ".", "i2w", ",", "self", ".", "w2i", ",", "self", ".", "lhs_to_index", "=", "self", ".", "from_vocab_file", "(", "vocab_file", ")", "\n", "", "else", ":", "\n", "      ", "self", ".", "w2i", "=", "{", "\"<pad>\"", ":", "0", ",", "\"<unk>\"", ":", "1", ",", "\"<s>\"", ":", "2", ",", "\"</s>\"", ":", "3", "}", "\n", "self", ".", "i2w", "=", "[", "\"<pad>\"", ",", "\"<unk>\"", ",", "\"<s>\"", ",", "\"</s>\"", "]", "\n", "self", ".", "lhs_to_index", "=", "defaultdict", "(", "list", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.tree_utils.RuleVocab.from_vocab_file": [[85, 107], ["collections.defaultdict", "set", "open", "line.strip", "i2w.append", "tree_utils.Rule.from_str", "i2w.append", "lhs_to_index[].append"], "methods", ["home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.tree_utils.Rule.from_str"], ["", "", "def", "from_vocab_file", "(", "self", ",", "vocab_file", ")", ":", "\n", "    ", "\"\"\"\n    :param vocab_file: file containing one word per line, and not containing <s>, </s>, <unk>\n    \"\"\"", "\n", "i2w", "=", "[", "]", "\n", "w2i", "=", "{", "}", "\n", "lhs_to_index", "=", "defaultdict", "(", "list", ")", "\n", "special_toks", "=", "set", "(", "[", "self", ".", "hparams", ".", "bos", ",", "self", ".", "hparams", ".", "eos", ",", "self", ".", "hparams", ".", "pad", ",", "self", ".", "hparams", ".", "unk", "]", ")", "\n", "with", "open", "(", "vocab_file", ",", "encoding", "=", "'utf-8'", ")", "as", "f", ":", "\n", "      ", "i", "=", "0", "\n", "for", "line", "in", "f", ":", "\n", "        ", "word", "=", "line", ".", "strip", "(", ")", "\n", "if", "word", "in", "special_toks", ":", "\n", "          ", "i2w", ".", "append", "(", "word", ")", "\n", "w2i", "[", "word", "]", "=", "i", "\n", "", "else", ":", "\n", "          ", "rule", "=", "Rule", ".", "from_str", "(", "word", ")", "\n", "i2w", ".", "append", "(", "rule", ")", "\n", "w2i", "[", "rule", "]", "=", "i", "\n", "lhs_to_index", "[", "rule", ".", "lhs", "]", ".", "append", "(", "i", ")", "\n", "", "i", "+=", "1", "\n", "", "", "return", "i2w", ",", "w2i", ",", "lhs_to_index", "\n", "\n"]], "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.tree_utils.RuleVocab.convert": [[108, 117], ["len", "tree_utils.RuleVocab.lhs_to_index[].append", "tree_utils.RuleVocab.i2w.append", "len"], "methods", ["None"], ["", "def", "convert", "(", "self", ",", "w", ")", ":", "\n", "    ", "\"\"\" w is a Rule object \"\"\"", "\n", "if", "w", "not", "in", "self", ".", "w2i", ":", "\n", "      ", "if", "self", ".", "frozen", ":", "\n", "        ", "return", "self", ".", "UNK", "+", "self", ".", "offset", "\n", "", "self", ".", "w2i", "[", "w", "]", "=", "len", "(", "self", ".", "i2w", ")", "\n", "self", ".", "lhs_to_index", "[", "w", ".", "lhs", "]", ".", "append", "(", "len", "(", "self", ".", "i2w", ")", ")", "\n", "self", ".", "i2w", ".", "append", "(", "w", ")", "\n", "", "return", "self", ".", "w2i", "[", "w", "]", "+", "self", ".", "offset", "\n", "\n"]], "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.tree_utils.RuleVocab.rule_index_with_lhs": [[118, 120], ["None"], "methods", ["None"], ["", "def", "rule_index_with_lhs", "(", "self", ",", "lhs", ")", ":", "\n", "    ", "return", "self", ".", "lhs_to_index", "[", "lhs", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.tree_utils.RuleVocab.__getitem__": [[121, 124], ["None"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "i", ")", ":", "\n", "    ", "i", "-=", "self", ".", "offset", "\n", "return", "self", ".", "i2w", "[", "i", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.tree_utils.RuleVocab.__len__": [[125, 127], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "    ", "return", "len", "(", "self", ".", "i2w", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.tree_utils.Rule.__init__": [[131, 136], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "lhs", ",", "rhs", "=", "[", "]", ",", "open_nonterms", "=", "[", "]", ")", ":", "\n", "    ", "self", ".", "lhs", "=", "lhs", "\n", "self", ".", "rhs", "=", "rhs", "\n", "self", ".", "open_nonterms", "=", "open_nonterms", "\n", "self", ".", "serialize_params", "=", "{", "'lhs'", ":", "self", ".", "lhs", ",", "'rhs'", ":", "self", ".", "rhs", ",", "'open_nonterms'", ":", "self", ".", "open_nonterms", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.tree_utils.Rule.__str__": [[137, 139], ["None"], "methods", ["None"], ["", "def", "__str__", "(", "self", ")", ":", "\n", "    ", "return", "(", "self", ".", "lhs", "+", "'|||'", "+", "' '", ".", "join", "(", "self", ".", "rhs", ")", "+", "'|||'", "+", "' '", ".", "join", "(", "self", ".", "open_nonterms", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.tree_utils.Rule.from_str": [[140, 148], ["line.split", "segs[].split", "segs[].split", "tree_utils.Rule", "len"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "from_str", "(", "line", ")", ":", "\n", "    ", "segs", "=", "line", ".", "split", "(", "'|||'", ")", "\n", "assert", "len", "(", "segs", ")", "==", "3", "\n", "lhs", "=", "segs", "[", "0", "]", "\n", "rhs", "=", "segs", "[", "1", "]", ".", "split", "(", ")", "\n", "open_nonterms", "=", "segs", "[", "2", "]", ".", "split", "(", ")", "\n", "return", "Rule", "(", "lhs", ",", "rhs", ",", "open_nonterms", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.tree_utils.Rule.__hash__": [[149, 155], ["hasattr", "id", "hash", "str"], "methods", ["None"], ["", "def", "__hash__", "(", "self", ")", ":", "\n", "#return hash(str(self) + \" \".join(open_nonterms))", "\n", "    ", "if", "not", "hasattr", "(", "self", ",", "'lhs'", ")", ":", "\n", "      ", "return", "id", "(", "self", ")", "\n", "", "else", ":", "\n", "      ", "return", "hash", "(", "str", "(", "self", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.tree_utils.Rule.__eq__": [[156, 166], ["hasattr"], "methods", ["None"], ["", "", "def", "__eq__", "(", "self", ",", "other", ")", ":", "\n", "    ", "if", "not", "hasattr", "(", "other", ",", "'lhs'", ")", ":", "\n", "      ", "return", "False", "\n", "", "if", "not", "self", ".", "lhs", "==", "other", ".", "lhs", ":", "\n", "      ", "return", "False", "\n", "", "if", "not", "\" \"", ".", "join", "(", "self", ".", "rhs", ")", "==", "\" \"", ".", "join", "(", "other", ".", "rhs", ")", ":", "\n", "      ", "return", "False", "\n", "", "if", "not", "\" \"", ".", "join", "(", "self", ".", "open_nonterms", ")", "==", "\" \"", ".", "join", "(", "other", ".", "open_nonterms", ")", ":", "\n", "      ", "return", "False", "\n", "", "return", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.tree_utils.TreeNode.__init__": [[169, 180], ["hasattr", "c.set_parent"], "methods", ["home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.tree_utils.TreeNode.set_parent"], ["def", "__init__", "(", "self", ",", "string", ",", "children", ",", "timestep", "=", "-", "1", ",", "id", "=", "-", "1", ",", "last_word_t", "=", "0", ")", ":", "\n", "    ", "self", ".", "label", "=", "string", "\n", "self", ".", "children", "=", "children", "\n", "for", "c", "in", "children", ":", "\n", "      ", "if", "hasattr", "(", "c", ",", "\"set_parent\"", ")", ":", "\n", "        ", "c", ".", "set_parent", "(", "self", ")", "\n", "", "", "self", ".", "_parent", "=", "None", "\n", "self", ".", "timestep", "=", "timestep", "\n", "self", ".", "id", "=", "id", "\n", "self", ".", "last_word_t", "=", "last_word_t", "\n", "self", ".", "frontir_label", "=", "None", "\n", "", "def", "is_preterminal", "(", "self", ")", ":", "\n"]], "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.tree_utils.TreeNode.is_preterminal": [[180, 185], ["hasattr"], "methods", ["None"], ["", "def", "is_preterminal", "(", "self", ")", ":", "\n", "# return len(self.children) == 1 and (not hasattr(self.children[0], 'is_preterminal'))", "\n", "    ", "for", "c", "in", "self", ".", "children", ":", "\n", "      ", "if", "hasattr", "(", "c", ",", "'is_preterminal'", ")", ":", "return", "False", "\n", "", "return", "True", "\n", "", "def", "to_parse_string", "(", "self", ")", ":", "\n"]], "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.tree_utils.TreeNode.to_parse_string": [[185, 200], ["stack.pop", "stack.append", "reversed", "stack.append", "hasattr", "c_str.append", "stack.pop", "hasattr", "stack.append"], "methods", ["None"], ["", "def", "to_parse_string", "(", "self", ")", ":", "\n", "    ", "c_str", "=", "[", "]", "\n", "stack", "=", "[", "self", "]", "\n", "while", "stack", ":", "\n", "      ", "cur", "=", "stack", ".", "pop", "(", ")", "\n", "while", "not", "hasattr", "(", "cur", ",", "'label'", ")", ":", "\n", "        ", "c_str", ".", "append", "(", "cur", ")", "\n", "if", "not", "stack", ":", "break", "\n", "cur", "=", "stack", ".", "pop", "(", ")", "\n", "", "if", "not", "hasattr", "(", "cur", ",", "'children'", ")", ":", "break", "\n", "stack", ".", "append", "(", "u')'", ")", "\n", "for", "c", "in", "reversed", "(", "cur", ".", "children", ")", ":", "\n", "        ", "stack", ".", "append", "(", "c", ")", "\n", "", "stack", ".", "append", "(", "u'({} '", ".", "format", "(", "cur", ".", "label", ")", ")", "\n", "", "return", "u\"\"", ".", "join", "(", "c_str", ")", "\n", "", "def", "to_string", "(", "self", ",", "piece", "=", "True", ")", ":", "\n"]], "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.tree_utils.TreeNode.to_string": [[200, 219], ["stack.pop", "reversed", "hasattr", "toks.append", "stack.pop", "hasattr", "stack.append"], "methods", ["None"], ["", "def", "to_string", "(", "self", ",", "piece", "=", "True", ")", ":", "\n", "    ", "\"\"\"\n    convert subtree into the sentence it represents\n    \"\"\"", "\n", "toks", "=", "[", "]", "\n", "stack", "=", "[", "self", "]", "\n", "while", "stack", ":", "\n", "      ", "cur", "=", "stack", ".", "pop", "(", ")", "\n", "while", "not", "hasattr", "(", "cur", ",", "'label'", ")", ":", "\n", "        ", "toks", ".", "append", "(", "cur", ")", "\n", "if", "not", "stack", ":", "break", "\n", "cur", "=", "stack", ".", "pop", "(", ")", "\n", "", "if", "not", "hasattr", "(", "cur", ",", "'children'", ")", ":", "break", "\n", "for", "c", "in", "reversed", "(", "cur", ".", "children", ")", ":", "\n", "        ", "stack", ".", "append", "(", "c", ")", "\n", "", "", "if", "not", "piece", ":", "\n", "      ", "return", "u\" \"", ".", "join", "(", "toks", ")", "\n", "", "else", ":", "\n", "      ", "return", "u\"\"", ".", "join", "(", "toks", ")", ".", "replace", "(", "u'\\u2581'", ",", "u' '", ")", ".", "strip", "(", ")", "\n", "", "", "def", "parent", "(", "self", ")", ":", "\n"]], "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.tree_utils.TreeNode.parent": [[219, 221], ["None"], "methods", ["None"], ["", "", "def", "parent", "(", "self", ")", ":", "\n", "    ", "return", "self", ".", "_parent", "\n", "", "def", "set_parent", "(", "self", ",", "new_parent", ")", ":", "\n"]], "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.tree_utils.TreeNode.set_parent": [[221, 223], ["None"], "methods", ["None"], ["", "def", "set_parent", "(", "self", ",", "new_parent", ")", ":", "\n", "    ", "self", ".", "_parent", "=", "new_parent", "\n", "", "def", "add_child", "(", "self", ",", "child", ",", "id2n", "=", "None", ",", "last_word_t", "=", "0", ")", ":", "\n"]], "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.tree_utils.TreeNode.add_child": [[223, 233], ["tree_utils.TreeNode.children.append", "hasattr", "len"], "methods", ["None"], ["", "def", "add_child", "(", "self", ",", "child", ",", "id2n", "=", "None", ",", "last_word_t", "=", "0", ")", ":", "\n", "    ", "self", ".", "children", ".", "append", "(", "child", ")", "\n", "if", "hasattr", "(", "child", ",", "\"set_parent\"", ")", ":", "\n", "      ", "child", ".", "_parent", "=", "self", "\n", "child", ".", "last_word_t", "=", "last_word_t", "\n", "if", "id2n", ":", "\n", "        ", "child", ".", "id", "=", "len", "(", "id2n", ")", "\n", "id2n", "[", "child", ".", "id", "]", "=", "child", "\n", "return", "child", ".", "id", "\n", "", "", "return", "-", "1", "\n", "", "def", "copy", "(", "self", ")", ":", "\n"]], "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.tree_utils.TreeNode.copy": [[233, 241], ["tree_utils.TreeNode", "hasattr", "tree_utils.TreeNode.add_child", "tree_utils.TreeNode.add_child", "c.copy"], "methods", ["home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.tree_utils.TreeNode.add_child", "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.tree_utils.TreeNode.add_child", "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.tree_utils.Tree.copy"], ["", "def", "copy", "(", "self", ")", ":", "\n", "    ", "new_node", "=", "TreeNode", "(", "self", ".", "label", ",", "[", "]", ")", "\n", "for", "c", "in", "self", ".", "children", ":", "\n", "      ", "if", "hasattr", "(", "c", ",", "'copy'", ")", ":", "\n", "        ", "new_node", ".", "add_child", "(", "c", ".", "copy", "(", ")", ")", "\n", "", "else", ":", "\n", "        ", "new_node", ".", "add_child", "(", "c", ")", "\n", "", "", "return", "new_node", "\n", "", "def", "frontir_nodes", "(", "self", ")", ":", "\n"]], "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.tree_utils.TreeNode.frontir_nodes": [[241, 250], ["hasattr", "len", "frontir.append", "frontir.extend", "c.frontir_nodes"], "methods", ["home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.tree_utils.TreeNode.frontir_nodes"], ["", "def", "frontir_nodes", "(", "self", ")", ":", "\n", "    ", "frontir", "=", "[", "]", "\n", "for", "c", "in", "self", ".", "children", ":", "\n", "      ", "if", "hasattr", "(", "c", ",", "'children'", ")", ":", "\n", "        ", "if", "len", "(", "c", ".", "children", ")", "==", "0", ":", "\n", "          ", "frontir", ".", "append", "(", "c", ")", "\n", "", "else", ":", "\n", "          ", "frontir", ".", "extend", "(", "c", ".", "frontir_nodes", "(", ")", ")", "\n", "", "", "", "return", "frontir", "\n", "", "def", "leaf_nodes", "(", "self", ")", ":", "\n"]], "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.tree_utils.TreeNode.leaf_nodes": [[250, 258], ["hasattr", "leaves.extend", "leaves.append", "c.leaf_nodes"], "methods", ["home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.tree_utils.TreeNode.leaf_nodes"], ["", "def", "leaf_nodes", "(", "self", ")", ":", "\n", "    ", "leaves", "=", "[", "]", "\n", "for", "c", "in", "self", ".", "children", ":", "\n", "      ", "if", "hasattr", "(", "c", ",", "'children'", ")", ":", "\n", "        ", "leaves", ".", "extend", "(", "c", ".", "leaf_nodes", "(", ")", ")", "\n", "", "else", ":", "\n", "        ", "leaves", ".", "append", "(", "c", ")", "\n", "", "", "return", "leaves", "\n", "", "def", "get_leaf_lens", "(", "self", ",", "len_dict", ")", ":", "\n"]], "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.tree_utils.TreeNode.get_leaf_lens": [[258, 268], ["tree_utils.TreeNode.is_preterminal", "tree_utils.TreeNode.leaf_nodes", "hasattr", "c.get_leaf_lens", "len"], "methods", ["home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.tree_utils.TreeNode.is_preterminal", "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.tree_utils.TreeNode.leaf_nodes", "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.tree_utils.TreeNode.get_leaf_lens"], ["", "def", "get_leaf_lens", "(", "self", ",", "len_dict", ")", ":", "\n", "    ", "if", "self", ".", "is_preterminal", "(", ")", ":", "\n", "      ", "l", "=", "self", ".", "leaf_nodes", "(", ")", "\n", "# if len(l) > 10:", "\n", "#    print l, len(l)", "\n", "len_dict", "[", "len", "(", "l", ")", "]", "+=", "1", "\n", "return", "\n", "", "for", "c", "in", "self", ".", "children", ":", "\n", "      ", "if", "hasattr", "(", "c", ",", "'is_preterminal'", ")", ":", "\n", "        ", "c", ".", "get_leaf_lens", "(", "len_dict", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.tree_utils.TreeNode.set_timestep_old": [[269, 307], ["open_stack.pop", "new_open_label.reverse", "open_stack.extend", "hasattr", "hasattr", "len", "new_open_label.append", "c.set_timestep_old"], "methods", ["home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.tree_utils.TreeNode.set_timestep_old"], ["", "", "", "def", "set_timestep_old", "(", "self", ",", "t", ",", "t2n", "=", "None", ",", "id2n", "=", "None", ",", "last_word_t", "=", "0", ",", "sib_t", "=", "0", ",", "open_stack", "=", "[", "]", ")", ":", "\n", "    ", "\"\"\"\n    initialize timestep for each node\n    \"\"\"", "\n", "self", ".", "timestep", "=", "t", "\n", "self", ".", "last_word_t", "=", "last_word_t", "\n", "self", ".", "sib_t", "=", "sib_t", "\n", "next_word_t", "=", "last_word_t", "\n", "if", "not", "t2n", "is", "None", ":", "\n", "      ", "assert", "self", ".", "timestep", "==", "len", "(", "t2n", ")", "\n", "assert", "t", "not", "in", "t2n", "\n", "t2n", "[", "t", "]", "=", "self", "\n", "", "if", "not", "id2n", "is", "None", ":", "\n", "      ", "self", ".", "id", "=", "t", "\n", "id2n", "[", "t", "]", "=", "self", "\n", "", "sib_t", "=", "0", "\n", "assert", "self", ".", "label", "==", "open_stack", "[", "-", "1", "]", "\n", "open_stack", ".", "pop", "(", ")", "\n", "new_open_label", "=", "[", "]", "\n", "for", "c", "in", "self", ".", "children", ":", "\n", "      ", "if", "hasattr", "(", "c", ",", "'set_timestep'", ")", ":", "\n", "        ", "new_open_label", ".", "append", "(", "c", ".", "label", ")", "\n", "", "", "new_open_label", ".", "reverse", "(", ")", "\n", "open_stack", ".", "extend", "(", "new_open_label", ")", "\n", "if", "open_stack", ":", "\n", "      ", "self", ".", "frontir_label", "=", "open_stack", "[", "-", "1", "]", "\n", "", "else", ":", "\n", "      ", "self", ".", "frontir_label", "=", "Vocab", ".", "ES_STR", "\n", "", "c_t", "=", "t", "\n", "for", "c", "in", "self", ".", "children", ":", "\n", "# c_t = t + 1  # time of current child", "\n", "      ", "if", "hasattr", "(", "c", ",", "'set_timestep_old'", ")", ":", "\n", "        ", "c_t", "=", "t", "+", "1", "\n", "t", ",", "next_word_t", "=", "c", ".", "set_timestep_old", "(", "c_t", ",", "t2n", ",", "id2n", ",", "next_word_t", ",", "sib_t", ",", "open_stack", ")", "\n", "", "else", ":", "\n", "        ", "next_word_t", "=", "t", "\n", "", "sib_t", "=", "c_t", "\n", "", "return", "t", ",", "next_word_t", "\n", "\n"]], "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.tree_utils.TreeNode.set_timestep": [[308, 341], ["open_stack.pop", "new_open_label.reverse", "open_stack.extend", "hasattr", "hasattr", "len", "new_open_label.append", "c.set_timestep", "hasattr"], "methods", ["home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.tree_utils.TreeNode.set_timestep"], ["", "def", "set_timestep", "(", "self", ",", "t", ",", "num_node", ",", "t2n", "=", "None", ",", "id2n", "=", "None", ",", "open_stack", "=", "[", "]", ")", ":", "\n", "    ", "\"\"\"\n    initialize timestep for each node\n    \"\"\"", "\n", "self", ".", "timestep", "=", "t", "\n", "if", "not", "t2n", "is", "None", ":", "\n", "      ", "assert", "num_node", "==", "len", "(", "t2n", ")", "\n", "assert", "num_node", "not", "in", "t2n", "\n", "t2n", "[", "num_node", "]", "=", "self", "\n", "", "if", "not", "id2n", "is", "None", ":", "\n", "      ", "self", ".", "id", "=", "num_node", "\n", "id2n", "[", "num_node", "]", "=", "self", "\n", "", "assert", "self", ".", "label", "==", "open_stack", "[", "-", "1", "]", "\n", "open_stack", ".", "pop", "(", ")", "\n", "new_open_label", "=", "[", "]", "\n", "for", "c", "in", "self", ".", "children", ":", "\n", "      ", "if", "hasattr", "(", "c", ",", "'set_timestep'", ")", ":", "\n", "        ", "new_open_label", ".", "append", "(", "c", ".", "label", ")", "\n", "", "", "new_open_label", ".", "reverse", "(", ")", "\n", "open_stack", ".", "extend", "(", "new_open_label", ")", "\n", "if", "open_stack", ":", "\n", "      ", "self", ".", "frontir_label", "=", "open_stack", "[", "-", "1", "]", "\n", "", "else", ":", "\n", "      ", "self", ".", "frontir_label", "=", "Vocab", ".", "ES_STR", "\n", "", "c_t", "=", "t", "\n", "for", "c", "in", "self", ".", "children", ":", "\n", "      ", "if", "not", "hasattr", "(", "c", ",", "'label'", ")", "or", "c", ".", "label", "!=", "'*'", ":", "\n", "        ", "c_t", "+=", "1", "# time of current child", "\n", "", "if", "hasattr", "(", "c", ",", "'set_timestep'", ")", ":", "\n", "        ", "num_node", "+=", "1", "\n", "#print(c_t, c.label)", "\n", "c_t", ",", "num_node", "=", "c", ".", "set_timestep", "(", "c_t", ",", "num_node", ",", "t2n", ",", "id2n", ",", "open_stack", ")", "\n", "", "", "return", "c_t", ",", "num_node", "\n", "\n"]], "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.tree_utils.Tree.__init__": [[345, 356], ["tree_utils.TreeNode", "tree_utils.TreeNode"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "root", "=", "None", ",", "sent_piece", "=", "None", ",", "binarize", "=", "False", ")", ":", "\n", "    ", "self", ".", "id2n", "=", "{", "}", "\n", "self", ".", "t2n", "=", "{", "}", "\n", "self", ".", "open_nonterm_ids", "=", "[", "]", "\n", "self", ".", "last_word_t", "=", "-", "1", "\n", "if", "root", ":", "\n", "      ", "self", ".", "root", "=", "TreeNode", "(", "'XXX'", ",", "[", "root", "]", ")", "\n", "", "else", ":", "\n", "      ", "self", ".", "last_word_t", "=", "0", "\n", "self", ".", "root", "=", "TreeNode", "(", "'XXX'", ",", "[", "]", ",", "id", "=", "0", ",", "timestep", "=", "0", ")", "\n", "self", ".", "id2n", "[", "0", "]", "=", "self", ".", "root", "\n", "", "", "def", "reset_timestep", "(", "self", ")", ":", "\n"]], "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.tree_utils.Tree.reset_timestep": [[356, 358], ["tree_utils.Tree.root.set_timestep"], "methods", ["home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.tree_utils.TreeNode.set_timestep"], ["", "", "def", "reset_timestep", "(", "self", ")", ":", "\n", "    ", "self", ".", "root", ".", "set_timestep", "(", "0", ",", "0", ",", "self", ".", "t2n", ",", "self", ".", "id2n", ",", "open_stack", "=", "[", "'XXX'", "]", ")", "\n", "", "def", "__str__", "(", "self", ")", ":", "\n"]], "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.tree_utils.Tree.__str__": [[358, 360], ["tree_utils.Tree.root.to_parse_string"], "methods", ["home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.tree_utils.Tree.to_parse_string"], ["", "def", "__str__", "(", "self", ")", ":", "\n", "    ", "return", "self", ".", "root", ".", "to_parse_string", "(", ")", "\n", "", "def", "to_parse_string", "(", "self", ")", ":", "\n"]], "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.tree_utils.Tree.to_parse_string": [[360, 362], ["tree_utils.Tree.root.to_parse_string"], "methods", ["home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.tree_utils.Tree.to_parse_string"], ["", "def", "to_parse_string", "(", "self", ")", ":", "\n", "    ", "return", "self", ".", "root", ".", "to_parse_string", "(", ")", "\n", "", "def", "copy", "(", "self", ")", ":", "\n"]], "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.tree_utils.Tree.copy": [[362, 393], ["tree_utils.Tree", "tree_utils.TreeNode", "stack.pop", "copy_stack.pop", "hasattr", "tree_utils.TreeNode", "copy_stack.pop.add_child", "stack.append", "copy_stack.append", "copy_stack.pop.add_child"], "methods", ["home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.tree_utils.TreeNode.add_child", "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.tree_utils.TreeNode.add_child"], ["", "def", "copy", "(", "self", ")", ":", "\n", "    ", "'''Return a deep copy of the current tree'''", "\n", "copied_tree", "=", "Tree", "(", ")", "\n", "copied_tree", ".", "id2n", "=", "{", "}", "\n", "copied_tree", ".", "t2n", "=", "{", "}", "\n", "copied_tree", ".", "open_nonterm_ids", "=", "self", ".", "open_nonterm_ids", "[", ":", "]", "\n", "copied_tree", ".", "last_word_t", "=", "self", ".", "last_word_t", "\n", "root", "=", "TreeNode", "(", "'trash'", ",", "[", "]", ")", "\n", "stack", "=", "[", "self", ".", "root", "]", "\n", "copy_stack", "=", "[", "root", "]", "\n", "while", "stack", ":", "\n", "      ", "cur", "=", "stack", ".", "pop", "(", ")", "\n", "copy_cur", "=", "copy_stack", ".", "pop", "(", ")", "\n", "copy_cur", ".", "label", "=", "cur", ".", "label", "\n", "copy_cur", ".", "children", "=", "[", "]", "\n", "copy_cur", ".", "id", "=", "cur", ".", "id", "\n", "copy_cur", ".", "timestep", "=", "cur", ".", "timestep", "\n", "copy_cur", ".", "last_word_t", "=", "cur", ".", "last_word_t", "\n", "copied_tree", ".", "id2n", "[", "copy_cur", ".", "id", "]", "=", "copy_cur", "\n", "if", "copy_cur", ".", "timestep", ">=", "0", ":", "\n", "        ", "copied_tree", ".", "t2n", "[", "copy_cur", ".", "timestep", "]", "=", "copy_cur", "\n", "", "for", "c", "in", "cur", ".", "children", ":", "\n", "        ", "if", "hasattr", "(", "c", ",", "'set_parent'", ")", ":", "\n", "          ", "copy_c", "=", "TreeNode", "(", "c", ".", "label", ",", "[", "]", ")", "\n", "copy_cur", ".", "add_child", "(", "copy_c", ")", "\n", "stack", ".", "append", "(", "c", ")", "\n", "copy_stack", ".", "append", "(", "copy_c", ")", "\n", "", "else", ":", "\n", "          ", "copy_cur", ".", "add_child", "(", "c", ")", "\n", "", "", "", "copied_tree", ".", "root", "=", "root", "\n", "return", "copied_tree", "\n", "", "@", "classmethod", "\n"]], "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.tree_utils.Tree.from_rule_deriv": [[393, 444], ["tree_utils.Tree", "stack_tree.pop", "open_nonterms.reverse", "stack_tree.extend", "type", "tree_utils.TreeNode", "stack_tree.pop.add_child", "stack_tree.pop.add_child", "print", "print", "exit", "tree_utils.TreeNode.add_child", "tree_utils.TreeNode", "tree_utils.TreeNode.add_child", "open_nonterms.append", "stack_tree.pop.add_child", "stack_tree.append", "stack_tree.append", "Tree.to_parse_string().encode", "stack_tree.pop.label.encode", "r.lhs.encode", "type", "print", "print", "type", "print", "print", "i[].encode", "i[].encode", "tree_utils.Tree.to_parse_string"], "methods", ["home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.tree_utils.TreeNode.add_child", "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.tree_utils.TreeNode.add_child", "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.tree_utils.TreeNode.add_child", "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.tree_utils.TreeNode.add_child", "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.tree_utils.TreeNode.add_child", "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.tree_utils.Tree.to_parse_string"], ["", "@", "classmethod", "\n", "def", "from_rule_deriv", "(", "cls", ",", "derivs", ",", "wordswitch", "=", "True", ")", ":", "\n", "    ", "tree", "=", "Tree", "(", ")", "\n", "stack_tree", "=", "[", "tree", ".", "root", "]", "\n", "for", "x", "in", "derivs", ":", "\n", "      ", "r", ",", "stop", "=", "x", "\n", "#print(r, stop)", "\n", "p_tree", "=", "stack_tree", ".", "pop", "(", ")", "\n", "if", "type", "(", "r", ")", "!=", "Rule", ":", "\n", "        ", "if", "p_tree", ".", "label", "!=", "'*'", ":", "\n", "          ", "for", "i", "in", "derivs", ":", "\n", "            ", "if", "type", "(", "i", "[", "0", "]", ")", "!=", "Rule", ":", "\n", "              ", "print", "(", "i", "[", "0", "]", ".", "encode", "(", "'utf-8'", ")", ",", "i", "[", "1", "]", ")", "\n", "", "else", ":", "\n", "              ", "print", "(", "i", "[", "0", "]", ",", "i", "[", "1", "]", ")", "\n", "", "", "", "assert", "p_tree", ".", "label", "==", "'*'", ",", "p_tree", ".", "label", "\n", "if", "wordswitch", ":", "\n", "          ", "if", "r", "!=", "Vocab", ".", "ES_STR", ":", "\n", "            ", "p_tree", ".", "add_child", "(", "r", ")", "\n", "stack_tree", ".", "append", "(", "p_tree", ")", "\n", "", "", "else", ":", "\n", "          ", "p_tree", ".", "add_child", "(", "r", ")", "\n", "if", "not", "stop", ":", "\n", "            ", "stack_tree", ".", "append", "(", "p_tree", ")", "\n", "", "", "continue", "\n", "", "if", "p_tree", ".", "label", "==", "'XXX'", ":", "\n", "        ", "new_tree", "=", "TreeNode", "(", "r", ".", "lhs", ",", "[", "]", ")", "\n", "p_tree", ".", "add_child", "(", "new_tree", ")", "\n", "", "else", ":", "\n", "        ", "if", "p_tree", ".", "label", "!=", "r", ".", "lhs", ":", "\n", "          ", "for", "i", "in", "derivs", ":", "\n", "            ", "if", "type", "(", "i", "[", "0", "]", ")", "!=", "Rule", ":", "\n", "              ", "print", "(", "i", "[", "0", "]", ".", "encode", "(", "'utf-8'", ")", ",", "i", "[", "1", "]", ")", "\n", "", "else", ":", "\n", "              ", "print", "(", "i", "[", "0", "]", ",", "i", "[", "1", "]", ")", "\n", "", "", "print", "(", "tree", ".", "to_parse_string", "(", ")", ".", "encode", "(", "'utf-8'", ")", ")", "\n", "print", "(", "p_tree", ".", "label", ".", "encode", "(", "'utf-8'", ")", ",", "r", ".", "lhs", ".", "encode", "(", "'utf-8'", ")", ")", "\n", "exit", "(", "1", ")", "\n", "", "assert", "p_tree", ".", "label", "==", "r", ".", "lhs", ",", "\"%s %s\"", "%", "(", "p_tree", ".", "label", ",", "r", ".", "lhs", ")", "\n", "new_tree", "=", "p_tree", "\n", "", "open_nonterms", "=", "[", "]", "\n", "for", "child", "in", "r", ".", "rhs", ":", "\n", "        ", "if", "child", "not", "in", "r", ".", "open_nonterms", ":", "\n", "          ", "new_tree", ".", "add_child", "(", "child", ")", "\n", "", "else", ":", "\n", "          ", "n", "=", "TreeNode", "(", "child", ",", "[", "]", ")", "\n", "new_tree", ".", "add_child", "(", "n", ")", "\n", "open_nonterms", ".", "append", "(", "n", ")", "\n", "", "", "open_nonterms", ".", "reverse", "(", ")", "\n", "stack_tree", ".", "extend", "(", "open_nonterms", ")", "\n", "", "return", "tree", "\n", "", "def", "to_string", "(", "self", ",", "piece", "=", "False", ")", ":", "\n"]], "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.tree_utils.Tree.to_string": [[444, 449], ["tree_utils.Tree.root.to_string"], "methods", ["home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.tree_utils.Tree.to_string"], ["", "def", "to_string", "(", "self", ",", "piece", "=", "False", ")", ":", "\n", "    ", "\"\"\"\n    convert subtree into the sentence it represents\n    \"\"\"", "\n", "return", "self", ".", "root", ".", "to_string", "(", "piece", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.tree_utils.Tree.add_rule": [[450, 469], ["node.set_timestep", "new_open_ids.reverse", "tree_utils.Tree.open_nonterm_ids.extend", "len", "new_open_ids.append", "tree_utils.Tree.id2n[].add_child", "tree_utils.Tree.id2n[].add_child", "tree_utils.TreeNode"], "methods", ["home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.tree_utils.TreeNode.set_timestep", "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.tree_utils.TreeNode.add_child", "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.tree_utils.TreeNode.add_child"], ["", "def", "add_rule", "(", "self", ",", "id", ",", "rule", ")", ":", "\n", "    ", "''' Add one node to the tree based on current rule; only called on root tree '''", "\n", "node", "=", "self", ".", "id2n", "[", "id", "]", "\n", "node", ".", "set_timestep", "(", "len", "(", "self", ".", "t2n", ")", ",", "self", ".", "t2n", ")", "\n", "node", ".", "last_word_t", "=", "self", ".", "last_word_t", "\n", "assert", "rule", ".", "lhs", "==", "node", ".", "label", ",", "\"Rule lhs %s does not match the node %s to be expanded\"", "%", "(", "rule", ".", "lhs", ",", "node", ".", "label", ")", "\n", "new_open_ids", "=", "[", "]", "\n", "for", "rhs", "in", "rule", ".", "rhs", ":", "\n", "      ", "if", "rhs", "in", "rule", ".", "open_nonterms", ":", "\n", "        ", "new_open_ids", ".", "append", "(", "self", ".", "id2n", "[", "id", "]", ".", "add_child", "(", "TreeNode", "(", "rhs", ",", "[", "]", ")", ",", "self", ".", "id2n", ")", ")", "\n", "", "else", ":", "\n", "        ", "self", ".", "id2n", "[", "id", "]", ".", "add_child", "(", "rhs", ")", "\n", "self", ".", "last_word_t", "=", "node", ".", "timestep", "\n", "", "", "new_open_ids", ".", "reverse", "(", ")", "\n", "self", ".", "open_nonterm_ids", ".", "extend", "(", "new_open_ids", ")", "\n", "if", "self", ".", "open_nonterm_ids", ":", "\n", "      ", "node", ".", "frontir_label", "=", "self", ".", "id2n", "[", "self", ".", "open_nonterm_ids", "[", "-", "1", "]", "]", ".", "label", "\n", "", "else", ":", "\n", "      ", "node", ".", "frontir_label", "=", "None", "\n", "", "", "def", "get_next_open_node", "(", "self", ")", ":", "\n"]], "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.tree_utils.Tree.get_next_open_node": [[469, 474], ["tree_utils.Tree.open_nonterm_ids.pop", "len", "print"], "methods", ["None"], ["", "", "def", "get_next_open_node", "(", "self", ")", ":", "\n", "    ", "if", "len", "(", "self", ".", "open_nonterm_ids", ")", "==", "0", ":", "\n", "      ", "print", "(", "\"stack empty, tree is complete\"", ")", "\n", "return", "-", "1", "\n", "", "return", "self", ".", "open_nonterm_ids", ".", "pop", "(", ")", "\n", "", "def", "get_timestep_data", "(", "self", ",", "id", ")", ":", "\n"]], "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.tree_utils.Tree.get_timestep_data": [[474, 483], ["tree_utils.Tree.id2n[].parent", "data.append", "data.append", "data.append", "tree_utils.Tree.id2n[].parent"], "methods", ["home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.tree_utils.TreeNode.parent", "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.tree_utils.TreeNode.parent"], ["", "def", "get_timestep_data", "(", "self", ",", "id", ")", ":", "\n", "    ", "''' Return a list of timesteps data associated with current tree node; only called on root tree '''", "\n", "data", "=", "[", "]", "\n", "if", "self", ".", "id2n", "[", "id", "]", ".", "parent", "(", ")", ":", "\n", "      ", "data", ".", "append", "(", "self", ".", "id2n", "[", "id", "]", ".", "parent", "(", ")", ".", "timestep", ")", "\n", "", "else", ":", "\n", "      ", "data", ".", "append", "(", "0", ")", "\n", "", "data", ".", "append", "(", "self", ".", "id2n", "[", "id", "]", ".", "last_word_t", ")", "\n", "return", "data", "\n", "\n"]], "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.tree_utils.Tree.get_data_root": [[484, 510], ["range", "len", "tree_utils.Rule", "data.append", "type", "children.append", "children.append", "open_nonterms.append", "node.parent", "node.parent", "len", "data.append", "rule_vocab.convert", "word_vocab.convert"], "methods", ["home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.tree_utils.TreeNode.parent", "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.tree_utils.TreeNode.parent", "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.tree_utils.RuleVocab.convert", "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.tree_utils.RuleVocab.convert"], ["", "def", "get_data_root", "(", "self", ",", "rule_vocab", ",", "word_vocab", "=", "None", ")", ":", "\n", "    ", "data", "=", "[", "]", "\n", "for", "t", "in", "range", "(", "1", ",", "len", "(", "self", ".", "t2n", ")", ")", ":", "\n", "      ", "node", "=", "self", ".", "t2n", "[", "t", "]", "\n", "children", ",", "open_nonterms", "=", "[", "]", ",", "[", "]", "\n", "for", "c", "in", "node", ".", "children", ":", "\n", "        ", "if", "type", "(", "c", ")", "==", "str", ":", "\n", "          ", "children", ".", "append", "(", "c", ")", "\n", "", "else", ":", "\n", "          ", "children", ".", "append", "(", "c", ".", "label", ")", "\n", "open_nonterms", ".", "append", "(", "c", ".", "label", ")", "\n", "", "", "paren_t", "=", "0", "if", "not", "node", ".", "parent", "(", ")", "else", "node", ".", "parent", "(", ")", ".", "timestep", "\n", "is_terminal", "=", "1", "if", "len", "(", "open_nonterms", ")", "==", "0", "else", "0", "\n", "if", "word_vocab", "and", "is_terminal", ":", "\n", "        ", "for", "c", "in", "node", ".", "children", ":", "\n", "          ", "d", "=", "[", "word_vocab", ".", "convert", "(", "c", ")", ",", "paren_t", ",", "is_terminal", "]", "\n", "data", ".", "append", "(", "d", ")", "\n", "#print(c)", "\n", "#print(d)", "\n", "", "", "else", ":", "\n", "        ", "r", "=", "Rule", "(", "node", ".", "label", ",", "children", ",", "open_nonterms", ")", "\n", "d", "=", "[", "rule_vocab", ".", "convert", "(", "r", ")", ",", "paren_t", ",", "is_terminal", "]", "\n", "data", ".", "append", "(", "d", ")", "\n", "#print(str(r))", "\n", "#print(d)", "\n", "", "", "return", "data", "\n", "\n"]], "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.tree_utils.Tree.get_bpe_rule": [[511, 535], ["range", "len", "rule_vocab.convert", "rule_idx.append", "len", "tree_utils.Rule", "type", "open_nonterms.append", "children.append", "children.append", "children.append", "children.append"], "methods", ["home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.tree_utils.RuleVocab.convert"], ["", "def", "get_bpe_rule", "(", "self", ",", "rule_vocab", ")", ":", "\n", "    ", "''' Get the rules for doing bpe. Label left and right child '''", "\n", "rule_idx", "=", "[", "]", "\n", "for", "t", "in", "range", "(", "1", ",", "len", "(", "self", ".", "t2n", ")", ")", ":", "\n", "      ", "node", "=", "self", ".", "t2n", "[", "t", "]", "\n", "children", ",", "open_nonterms", "=", "[", "]", ",", "[", "]", "\n", "child_idx", "=", "1", "\n", "attach_tag", "=", "len", "(", "children", ")", ">", "1", "\n", "for", "c", "in", "node", ".", "children", ":", "\n", "        ", "if", "type", "(", "c", ")", "==", "str", ":", "\n", "          ", "if", "attach_tag", ":", "\n", "            ", "children", ".", "append", "(", "'{}_{}'", ".", "format", "(", "c", ",", "child_idx", ")", ")", "\n", "", "else", ":", "\n", "            ", "children", ".", "append", "(", "c", ")", "\n", "", "", "else", ":", "\n", "          ", "if", "attach_tag", ":", "\n", "            ", "children", ".", "append", "(", "'{}_{}'", ".", "format", "(", "c", ".", "label", ",", "child_idx", ")", ")", "\n", "", "else", ":", "\n", "            ", "children", ".", "append", "(", "c", ".", "label", ")", "\n", "", "open_nonterms", ".", "append", "(", "c", ".", "label", ")", "\n", "", "child_idx", "+=", "1", "\n", "", "r", "=", "rule_vocab", ".", "convert", "(", "Rule", "(", "node", ".", "label", ",", "children", ",", "open_nonterms", ")", ")", "\n", "rule_idx", ".", "append", "(", "r", ")", "\n", "", "return", "rule_idx", "\n", "", "def", "query_open_node_label", "(", "self", ")", ":", "\n"]], "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.tree_utils.Tree.query_open_node_label": [[535, 537], ["None"], "methods", ["None"], ["", "def", "query_open_node_label", "(", "self", ")", ":", "\n", "    ", "return", "self", ".", "id2n", "[", "self", ".", "open_nonterm_ids", "[", "-", "1", "]", "]", ".", "label", "\n", "", "", "def", "sent_piece_segs", "(", "p", ")", ":", "\n"]], "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.tree_utils.sent_piece_segs": [[537, 553], ["re.compile", "re.compile.finditer", "m.start", "ret.append", "ret.append", "len"], "function", ["None"], ["", "", "def", "sent_piece_segs", "(", "p", ")", ":", "\n", "  ", "'''\n  Segment a sentence piece string into list of piece string for each word\n  '''", "\n", "toks", "=", "re", ".", "compile", "(", "r'\\u2581'", ")", "\n", "ret", "=", "[", "]", "\n", "p_start", "=", "0", "\n", "for", "m", "in", "toks", ".", "finditer", "(", "p", ")", ":", "\n", "    ", "pos", "=", "m", ".", "start", "(", ")", "\n", "if", "pos", "==", "0", ":", "\n", "      ", "continue", "\n", "", "ret", ".", "append", "(", "p", "[", "p_start", ":", "pos", "]", ")", "\n", "p_start", "=", "pos", "\n", "", "if", "p_start", "!=", "len", "(", "p", ")", "-", "1", ":", "\n", "    ", "ret", ".", "append", "(", "p", "[", "p_start", ":", "]", ")", "\n", "", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.tree_utils.sent_piece_segs_bpe": [[554, 570], ["p.split", "cur.append", "t.endswith", "ret.append"], "function", ["None"], ["", "def", "sent_piece_segs_bpe", "(", "p", ")", ":", "\n", "  ", "'''\nSegment a sentence piece string into list of piece string for each word\n'''", "\n", "# print p", "\n", "# print p.split()", "\n", "# toks = re.compile(ur'\\xe2\\x96\\x81[^(\\xe2\\x96\\x81)]+')", "\n", "toks", "=", "p", ".", "split", "(", ")", "\n", "ret", "=", "[", "]", "\n", "cur", "=", "[", "]", "\n", "for", "t", "in", "toks", ":", "\n", "    ", "cur", ".", "append", "(", "t", ")", "\n", "if", "not", "t", ".", "endswith", "(", "u'@@'", ")", ":", "\n", "      ", "ret", ".", "append", "(", "u' '", ".", "join", "(", "cur", ")", ")", "\n", "cur", "=", "[", "]", "\n", "", "", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.tree_utils.sent_piece_segs_post": [[571, 590], ["re.compile", "re.compile.finditer", "m.start", "ret.append", "ret.append", "p[].strip", "len"], "function", ["None"], ["", "def", "sent_piece_segs_post", "(", "p", ")", ":", "\n", "  ", "'''\nSegment a sentence piece string into list of piece string for each word\n'''", "\n", "# print p", "\n", "# print p.split()", "\n", "# toks = re.compile(ur'\\xe2\\x96\\x81[^(\\xe2\\x96\\x81)]+')", "\n", "toks", "=", "re", ".", "compile", "(", "r'\\u2581'", ")", "\n", "ret", "=", "[", "]", "\n", "p_start", "=", "0", "\n", "for", "m", "in", "toks", ".", "finditer", "(", "p", ")", ":", "\n", "    ", "pos", "=", "m", ".", "start", "(", ")", "\n", "if", "pos", "==", "0", ":", "\n", "      ", "continue", "\n", "", "ret", ".", "append", "(", "p", "[", "p_start", ":", "pos", "+", "1", "]", ".", "strip", "(", ")", ")", "\n", "p_start", "=", "pos", "+", "1", "\n", "", "if", "p_start", "!=", "len", "(", "p", ")", "-", "1", ":", "\n", "    ", "ret", ".", "append", "(", "p", "[", "p_start", ":", "]", ")", "\n", "", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.tree_utils.split_sent_piece": [[591, 609], ["enumerate", "type", "piece_l[].split", "new_children.extend", "tree_utils.split_sent_piece", "new_children.append"], "function", ["home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.tree_utils.split_sent_piece"], ["", "def", "split_sent_piece", "(", "root", ",", "piece_l", ",", "word_idx", ")", ":", "\n", "  ", "'''\n  Split words into sentence piece\n  '''", "\n", "new_children", "=", "[", "]", "\n", "\n", "for", "i", ",", "c", "in", "enumerate", "(", "root", ".", "children", ")", ":", "\n", "    ", "if", "type", "(", "c", ")", "==", "str", ":", "\n", "#print(root.to_parse_string())", "\n", "#print(piece_l, word_idx)", "\n", "      ", "piece", "=", "piece_l", "[", "word_idx", "]", ".", "split", "(", ")", "\n", "word_idx", "+=", "1", "\n", "new_children", ".", "extend", "(", "piece", ")", "\n", "", "else", ":", "\n", "      ", "word_idx", "=", "split_sent_piece", "(", "c", ",", "piece_l", ",", "word_idx", ")", "\n", "new_children", ".", "append", "(", "c", ")", "\n", "", "", "root", ".", "children", "=", "new_children", "\n", "return", "word_idx", "\n", "\n"]], "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.tree_utils.right_binarize": [[610, 633], ["type", "len", "tree_utils.TreeNode", "new_children.append", "tree_utils.right_binarize", "tree_utils.right_binarize", "tree_utils.right_binarize"], "function", ["home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.tree_utils.right_binarize", "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.tree_utils.right_binarize", "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.tree_utils.right_binarize"], ["", "def", "right_binarize", "(", "root", ",", "read_word", "=", "False", ")", ":", "\n", "  ", "'''\n  Right binarize a CusTree object\n  read_word: if true, do not binarize terminal rules\n  '''", "\n", "if", "type", "(", "root", ")", "==", "str", ":", "\n", "    ", "return", "root", "\n", "", "if", "read_word", "and", "root", ".", "label", "==", "u'*'", ":", "\n", "    ", "return", "root", "\n", "", "if", "len", "(", "root", ".", "children", ")", "<=", "2", ":", "\n", "    ", "new_children", "=", "[", "]", "\n", "for", "c", "in", "root", ".", "children", ":", "\n", "      ", "new_children", ".", "append", "(", "right_binarize", "(", "c", ")", ")", "\n", "", "root", ".", "children", "=", "new_children", "\n", "", "else", ":", "\n", "    ", "if", "\"__\"", "in", "root", ".", "label", ":", "\n", "      ", "new_label", "=", "root", ".", "label", "\n", "", "else", ":", "\n", "      ", "new_label", "=", "root", ".", "label", "+", "\"__\"", "\n", "", "n_left_child", "=", "TreeNode", "(", "new_label", ",", "root", ".", "children", "[", "1", ":", "]", ")", "\n", "n_left_child", ".", "_parent", "=", "root", "\n", "root", ".", "children", "=", "[", "right_binarize", "(", "root", ".", "children", "[", "0", "]", ")", ",", "right_binarize", "(", "n_left_child", ")", "]", "\n", "", "return", "root", "\n", "\n"]], "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.tree_utils.add_preterminal": [[634, 643], ["enumerate", "type", "tree_utils.TreeNode", "tree_utils.TreeNode.set_parent", "tree_utils.add_preterminal"], "function", ["home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.tree_utils.TreeNode.set_parent", "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.tree_utils.add_preterminal"], ["", "def", "add_preterminal", "(", "root", ")", ":", "\n", "  ", "''' Add preterminal X before each terminal symbol '''", "\n", "for", "i", ",", "c", "in", "enumerate", "(", "root", ".", "children", ")", ":", "\n", "    ", "if", "type", "(", "c", ")", "==", "str", ":", "\n", "      ", "n", "=", "TreeNode", "(", "u'*'", ",", "[", "c", "]", ")", "\n", "n", ".", "set_parent", "(", "root", ")", "\n", "root", ".", "children", "[", "i", "]", "=", "n", "\n", "", "else", ":", "\n", "      ", "add_preterminal", "(", "c", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.tree_utils.add_preterminal_wordswitch": [[644, 672], ["enumerate", "tree_utils.TreeNode.add_child", "root.add_child", "type", "tree_utils.TreeNode.add_child", "tree_utils.add_preterminal_wordswitch", "new_children.append", "tree_utils.TreeNode", "tree_utils.TreeNode.set_parent", "new_children.append", "tree_utils.TreeNode.add_child"], "function", ["home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.tree_utils.TreeNode.add_child", "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.tree_utils.TreeNode.add_child", "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.tree_utils.TreeNode.add_child", "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.tree_utils.add_preterminal_wordswitch", "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.tree_utils.TreeNode.set_parent", "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.tree_utils.TreeNode.add_child"], ["", "", "", "def", "add_preterminal_wordswitch", "(", "root", ",", "add_eos", ")", ":", "\n", "  ", "''' Add preterminal X before each terminal symbol '''", "\n", "''' word_switch: one * symbol for each phrase chunk\n      preterm_paren: * preterm parent already created\n  '''", "\n", "preterm_paren", "=", "None", "\n", "new_children", "=", "[", "]", "\n", "if", "root", ".", "label", "==", "u'*'", ":", "\n", "    ", "if", "add_eos", ":", "\n", "      ", "root", ".", "add_child", "(", "Vocab", ".", "ES_STR", ")", "\n", "", "return", "root", "\n", "", "for", "i", ",", "c", "in", "enumerate", "(", "root", ".", "children", ")", ":", "\n", "    ", "if", "type", "(", "c", ")", "==", "str", ":", "\n", "      ", "if", "not", "preterm_paren", ":", "\n", "        ", "preterm_paren", "=", "TreeNode", "(", "'*'", ",", "[", "]", ")", "\n", "preterm_paren", ".", "set_parent", "(", "root", ")", "\n", "new_children", ".", "append", "(", "preterm_paren", ")", "\n", "", "preterm_paren", ".", "add_child", "(", "c", ")", "\n", "", "else", ":", "\n", "      ", "if", "preterm_paren", "and", "add_eos", ":", "\n", "        ", "preterm_paren", ".", "add_child", "(", "Vocab", ".", "ES_STR", ")", "\n", "", "c", "=", "add_preterminal_wordswitch", "(", "c", ",", "add_eos", ")", "\n", "new_children", ".", "append", "(", "c", ")", "\n", "preterm_paren", "=", "None", "\n", "", "", "if", "preterm_paren", "and", "add_eos", ":", "\n", "    ", "preterm_paren", ".", "add_child", "(", "Vocab", ".", "ES_STR", ")", "\n", "", "root", ".", "children", "=", "new_children", "\n", "return", "root", "\n", "\n"]], "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.tree_utils.remove_preterminal_POS": [[673, 680], ["enumerate", "c.is_preterminal", "tree_utils.remove_preterminal_POS"], "function", ["home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.tree_utils.TreeNode.is_preterminal", "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.tree_utils.remove_preterminal_POS"], ["", "def", "remove_preterminal_POS", "(", "root", ")", ":", "\n", "  ", "''' Remove the POS tag before terminal '''", "\n", "for", "i", ",", "c", "in", "enumerate", "(", "root", ".", "children", ")", ":", "\n", "    ", "if", "c", ".", "is_preterminal", "(", ")", ":", "\n", "      ", "root", ".", "children", "[", "i", "]", "=", "c", ".", "children", "[", "0", "]", "\n", "", "else", ":", "\n", "      ", "remove_preterminal_POS", "(", "c", ")", "\n", "", "", "", "def", "remove_lhs", "(", "root", ",", "label", ")", ":", "\n"]], "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.tree_utils.remove_lhs": [[680, 687], ["enumerate", "hasattr", "c.is_preterminal", "tree_utils.remove_lhs"], "function", ["home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.tree_utils.TreeNode.is_preterminal", "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.tree_utils.remove_lhs"], ["", "", "", "def", "remove_lhs", "(", "root", ",", "label", ")", ":", "\n", "  ", "''' replace all lhs with the label '''", "\n", "root", ".", "label", "=", "label", "\n", "for", "i", ",", "c", "in", "enumerate", "(", "root", ".", "children", ")", ":", "\n", "    ", "if", "not", "hasattr", "(", "c", ",", "'is_preterminal'", ")", ":", "continue", "\n", "if", "not", "c", ".", "is_preterminal", "(", ")", ":", "\n", "      ", "remove_lhs", "(", "c", ",", "label", ")", "\n", "", "", "", "def", "replace_POS", "(", "root", ")", ":", "\n"]], "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.tree_utils.replace_POS": [[687, 694], ["enumerate", "c.is_preterminal", "tree_utils.replace_POS"], "function", ["home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.tree_utils.TreeNode.is_preterminal", "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.tree_utils.replace_POS"], ["", "", "", "def", "replace_POS", "(", "root", ")", ":", "\n", "  ", "''' simply replace POS with * '''", "\n", "for", "i", ",", "c", "in", "enumerate", "(", "root", ".", "children", ")", ":", "\n", "    ", "if", "c", ".", "is_preterminal", "(", ")", ":", "\n", "      ", "c", ".", "label", "=", "'*'", "\n", "", "else", ":", "\n", "      ", "replace_POS", "(", "c", ")", "\n", "", "", "", "def", "merge_depth", "(", "root", ",", "max_depth", ",", "cur_depth", ")", ":", "\n"]], "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.tree_utils.merge_depth": [[694, 715], ["enumerate", "root.leaf_nodes", "hasattr", "tree_utils.merge_depth", "new_children.append", "hasattr", "new_children[].is_preterminal", "merge_depth.is_preterminal", "new_children.append", "new_children[].add_child"], "function", ["home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.tree_utils.TreeNode.leaf_nodes", "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.tree_utils.merge_depth", "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.tree_utils.TreeNode.is_preterminal", "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.tree_utils.TreeNode.is_preterminal", "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.tree_utils.TreeNode.add_child"], ["", "", "", "def", "merge_depth", "(", "root", ",", "max_depth", ",", "cur_depth", ")", ":", "\n", "  ", "''' raise up trees whose depth exceed max_depth '''", "\n", "if", "cur_depth", ">=", "max_depth", ":", "\n", "# root.label = u'*'", "\n", "    ", "root", ".", "children", "=", "root", ".", "leaf_nodes", "(", ")", "\n", "return", "root", "\n", "", "new_children", "=", "[", "]", "\n", "for", "i", ",", "c", "in", "enumerate", "(", "root", ".", "children", ")", ":", "\n", "    ", "if", "hasattr", "(", "c", ",", "'children'", ")", ":", "\n", "      ", "c", "=", "merge_depth", "(", "c", ",", "max_depth", ",", "cur_depth", "+", "1", ")", "\n", "# combine consecutive * nodes", "\n", "if", "new_children", "and", "hasattr", "(", "new_children", "[", "-", "1", "]", ",", "'label'", ")", "and", "new_children", "[", "\n", "-", "1", "]", ".", "is_preterminal", "(", ")", "and", "c", ".", "is_preterminal", "(", ")", ":", "\n", "        ", "for", "x", "in", "c", ".", "children", ":", "\n", "          ", "new_children", "[", "-", "1", "]", ".", "add_child", "(", "x", ")", "\n", "", "", "else", ":", "\n", "        ", "new_children", ".", "append", "(", "c", ")", "\n", "", "", "else", ":", "\n", "      ", "new_children", ".", "append", "(", "c", ")", "\n", "", "", "root", ".", "children", "=", "new_children", "\n", "return", "root", "\n", "", "def", "merge_tags", "(", "root", ")", ":", "\n"]], "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.tree_utils.merge_tags": [[715, 727], ["set", "enumerate", "hasattr", "tree_utils.merge_tags"], "function", ["home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.tree_utils.merge_tags"], ["", "def", "merge_tags", "(", "root", ")", ":", "\n", "  ", "''' raise up trees whose label is in a given set '''", "\n", "kept_label", "=", "set", "(", "[", "u'np'", ",", "u'vp'", ",", "u'pp'", ",", "u's'", ",", "u'root'", ",", "u'sbar'", ",", "u'sinv'", ",", "u'XXX'", ",", "u'prn'", ",", "u'adjp'", ",", "u'advp'", ",", "\n", "u'whnp'", ",", "u'whadvp'", ",", "\n", "u'NP'", ",", "u'VP'", ",", "u'PP'", ",", "u'S'", ",", "u'ROOT'", ",", "u'SBAR'", ",", "u'FRAG'", ",", "u'SINV'", ",", "u'PRN'", "]", ")", "\n", "if", "not", "root", ".", "label", "in", "kept_label", ":", "\n", "    ", "root", ".", "label", "=", "u'xx'", "\n", "", "for", "i", ",", "c", "in", "enumerate", "(", "root", ".", "children", ")", ":", "\n", "    ", "if", "hasattr", "(", "c", ",", "'children'", ")", ":", "\n", "      ", "c", "=", "merge_tags", "(", "c", ")", "\n", "", "root", ".", "children", "[", "i", "]", "=", "c", "\n", "", "return", "root", "\n", "", "def", "combine_tags", "(", "root", ")", ":", "\n"]], "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.tree_utils.combine_tags": [[727, 729], ["None"], "function", ["None"], ["", "def", "combine_tags", "(", "root", ")", ":", "\n", "  ", "tag_dict", "=", "{", "'adjp'", ":", "'advp'", ",", "'sq'", ":", "'sbarq'", ",", "'whadjp'", ":", "'whadvp'", "}", "\n", "# Tokenize a string.", "\n"]], "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.tree_utils.tokenize": [[732, 742], ["re.compile", "re.compile.finditer", "match.group"], "function", ["None"], ["", "def", "tokenize", "(", "s", ")", ":", "\n", "  ", "toks", "=", "re", ".", "compile", "(", "r' +|[^() ]+|[()]'", ")", "\n", "for", "match", "in", "toks", ".", "finditer", "(", "s", ")", ":", "\n", "    ", "s", "=", "match", ".", "group", "(", "0", ")", "\n", "if", "s", "[", "0", "]", "==", "' '", ":", "\n", "      ", "continue", "\n", "", "if", "s", "[", "0", "]", "in", "'()'", ":", "\n", "      ", "yield", "(", "s", ",", "s", ")", "\n", "", "else", ":", "\n", "      ", "yield", "(", "'WORD'", ",", "s", ")", "\n", "# Parse once we're inside an opening bracket.", "\n"]], "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.tree_utils.parse_inner": [[743, 756], ["next", "next", "children.append", "tree_utils.parse_inner", "tree_utils.TreeNode", "children.append"], "function", ["home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.tree_utils.parse_inner"], ["", "", "", "def", "parse_inner", "(", "toks", ")", ":", "\n", "  ", "ty", ",", "name", "=", "next", "(", "toks", ")", "\n", "if", "ty", "!=", "'WORD'", ":", "raise", "ParseError", "\n", "children", "=", "[", "]", "\n", "while", "True", ":", "\n", "    ", "ty", ",", "s", "=", "next", "(", "toks", ")", "\n", "# print ty, s", "\n", "if", "ty", "==", "'('", ":", "\n", "      ", "children", ".", "append", "(", "parse_inner", "(", "toks", ")", ")", "\n", "", "elif", "ty", "==", "')'", ":", "\n", "      ", "return", "TreeNode", "(", "name", ",", "children", ")", "\n", "", "else", ":", "\n", "      ", "children", ".", "append", "(", "s", ")", "\n", "", "", "", "class", "ParseError", "(", "Exception", ")", ":", "\n"]], "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.tree_utils.parse_root": [[762, 768], ["next", "tree_utils.parse_inner"], "function", ["home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.tree_utils.parse_inner"], ["", "def", "parse_root", "(", "toks", ")", ":", "\n", "  ", "ty", ",", "s", "=", "next", "(", "toks", ")", "\n", "if", "ty", "!=", "'('", ":", "\n", "# print ty, s", "\n", "    ", "raise", "ParseError", "\n", "", "return", "parse_inner", "(", "toks", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.hparams.HParams.__init__": [[5, 24], ["six.iteritems", "float", "hparams.HParams.add_param"], "methods", ["home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.hparams.HParams.add_param"], ["  ", "def", "__init__", "(", "self", ",", "**", "kwargs", ")", ":", "\n", "#super(Iwslt16EnDeBpe32SharedParams, self).__init__(**kwargs)", "\n", "    ", "for", "name", ",", "value", "in", "six", ".", "iteritems", "(", "kwargs", ")", ":", "\n", "      ", "self", ".", "add_param", "(", "name", ",", "value", ")", "\n", "\n", "", "self", ".", "dataset", "=", "\"Hparams\"", "\n", "\n", "self", ".", "unk", "=", "\"<unk>\"", "\n", "self", ".", "bos", "=", "\"<s>\"", "\n", "self", ".", "eos", "=", "\"</s>\"", "\n", "self", ".", "pad", "=", "\"<pad>\"", "\n", "\n", "self", ".", "unk_id", "=", "None", "\n", "self", ".", "eos_id", "=", "None", "\n", "self", ".", "bos_id", "=", "None", "\n", "self", ".", "pad_id", "=", "None", "\n", "\n", "self", ".", "tiny", "=", "0.", "\n", "self", ".", "inf", "=", "float", "(", "\"inf\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.hparams.HParams.add_param": [[26, 28], ["setattr"], "methods", ["None"], ["", "def", "add_param", "(", "self", ",", "name", ",", "value", ")", ":", "\n", "    ", "setattr", "(", "self", ",", "name", ",", "value", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.utils.Logger.__init__": [[63, 66], ["open"], "methods", ["None"], ["  ", "def", "__init__", "(", "self", ",", "output_file", ")", ":", "\n", "    ", "self", ".", "terminal", "=", "sys", ".", "stdout", "\n", "self", ".", "log", "=", "open", "(", "output_file", ",", "\"a\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.utils.Logger.write": [[67, 70], ["print", "print"], "methods", ["None"], ["", "def", "write", "(", "self", ",", "message", ")", ":", "\n", "    ", "print", "(", "message", ",", "end", "=", "\"\"", ",", "file", "=", "self", ".", "terminal", ",", "flush", "=", "True", ")", "\n", "print", "(", "message", ",", "end", "=", "\"\"", ",", "file", "=", "self", ".", "log", ",", "flush", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.utils.Logger.flush": [[71, 74], ["utils.Logger.terminal.flush", "utils.Logger.log.flush"], "methods", ["home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.utils.Logger.flush", "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.utils.Logger.flush"], ["", "def", "flush", "(", "self", ")", ":", "\n", "    ", "self", ".", "terminal", ".", "flush", "(", ")", "\n", "self", ".", "log", ".", "flush", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.utils.memReport": [[13, 17], ["gc.get_objects", "torch.is_tensor", "torch.is_tensor", "torch.is_tensor", "print", "hasattr", "torch.is_tensor", "torch.is_tensor", "torch.is_tensor", "type", "obj.size"], "function", ["None"], ["def", "memReport", "(", ")", ":", "\n", "  ", "for", "obj", "in", "gc", ".", "get_objects", "(", ")", ":", "\n", "    ", "if", "torch", ".", "is_tensor", "(", "obj", ")", "or", "(", "hasattr", "(", "obj", ",", "'data'", ")", "and", "torch", ".", "is_tensor", "(", "obj", ".", "data", ")", ")", ":", "\n", "      ", "print", "(", "type", "(", "obj", ")", ",", "obj", ".", "size", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.utils.get_criterion": [[18, 26], ["torch.CrossEntropyLoss", "crit.cuda.cuda"], "function", ["None"], ["", "", "", "def", "get_criterion", "(", "hparams", ")", ":", "\n", "  ", "loss_reduce", "=", "True", "\n", "if", "hparams", ".", "trdec", ":", "\n", "    ", "loss_reduce", "=", "False", "\n", "", "crit", "=", "nn", ".", "CrossEntropyLoss", "(", "ignore_index", "=", "hparams", ".", "pad_id", ",", "size_average", "=", "False", ",", "reduce", "=", "loss_reduce", ")", "\n", "if", "hparams", ".", "cuda", ":", "\n", "    ", "crit", "=", "crit", ".", "cuda", "(", ")", "\n", "", "return", "crit", "\n", "\n"]], "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.utils.get_performance": [[27, 50], ["crit", "torch.max", "torch.max", "torch.max", "torch.eq().int().masked_fill_().sum", "torch.eq().int().masked_fill_().sum", "torch.eq().int().masked_fill_().sum", "loss[].sum", "loss[].sum", "loss[].sum", "crit", "torch.max", "torch.max", "torch.max", "torch.eq().int().masked_fill_().sum", "torch.eq().int().masked_fill_().sum", "torch.eq().int().masked_fill_().sum", "loss.sum.sum", "rule_mask.long().sum", "word_mask.long().sum", "eos_mask.long().sum", "loss.sum.sum", "torch.eq().int().masked_fill_", "torch.eq().int().masked_fill_", "torch.eq().int().masked_fill_", "torch.eq().int().masked_fill_", "torch.eq().int().masked_fill_", "torch.eq().int().masked_fill_", "rule_mask.long", "word_mask.long", "eos_mask.long", "torch.eq().int", "torch.eq().int", "torch.eq().int", "torch.eq().int", "torch.eq().int", "torch.eq().int", "torch.eq", "torch.eq", "torch.eq", "torch.eq", "torch.eq", "torch.eq"], "function", ["None"], ["", "def", "get_performance", "(", "crit", ",", "logits", ",", "labels", ",", "hparams", ",", "sum_loss", "=", "True", ")", ":", "\n", "  ", "mask", "=", "(", "labels", "==", "hparams", ".", "pad_id", ")", "\n", "if", "hparams", ".", "trdec", ":", "\n", "    ", "rule_mask", "=", "(", "labels", ">=", "hparams", ".", "target_word_vocab_size", ")", "\n", "eos_mask", "=", "(", "labels", "==", "hparams", ".", "eos_id", ")", "\n", "word_mask", "=", "(", "labels", "<", "hparams", ".", "target_word_vocab_size", ")", "^", "eos_mask", "^", "mask", "\n", "\n", "#labels = labels - rule_mask.long() * hparams.target_word_vocab_size", "\n", "loss", "=", "crit", "(", "logits", ",", "labels", ")", "\n", "_", ",", "preds", "=", "torch", ".", "max", "(", "logits", ",", "dim", "=", "1", ")", "\n", "acc", "=", "torch", ".", "eq", "(", "preds", ",", "labels", ")", ".", "int", "(", ")", ".", "masked_fill_", "(", "mask", ",", "0", ")", ".", "sum", "(", ")", "\n", "\n", "rule_loss", "=", "loss", "[", "rule_mask", "]", ".", "sum", "(", ")", "\n", "eos_loss", "=", "loss", "[", "eos_mask", "]", ".", "sum", "(", ")", "\n", "word_loss", "=", "loss", "[", "word_mask", "]", ".", "sum", "(", ")", "\n", "if", "sum_loss", ":", "loss", "=", "loss", ".", "sum", "(", ")", "\n", "return", "loss", ",", "acc", ",", "rule_loss", ",", "word_loss", ",", "eos_loss", ",", "rule_mask", ".", "long", "(", ")", ".", "sum", "(", ")", ",", "word_mask", ".", "long", "(", ")", ".", "sum", "(", ")", ",", "eos_mask", ".", "long", "(", ")", ".", "sum", "(", ")", "\n", "", "else", ":", "\n", "    ", "loss", "=", "crit", "(", "logits", ",", "labels", ")", "\n", "_", ",", "preds", "=", "torch", ".", "max", "(", "logits", ",", "dim", "=", "1", ")", "\n", "acc", "=", "torch", ".", "eq", "(", "preds", ",", "labels", ")", ".", "int", "(", ")", ".", "masked_fill_", "(", "mask", ",", "0", ")", ".", "sum", "(", ")", "\n", "if", "sum_loss", ":", "loss", "=", "loss", ".", "sum", "(", ")", "\n", "return", "loss", ",", "acc", "\n", "\n"]], "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.utils.count_params": [[51, 54], ["sum", "p.data.nelement"], "function", ["None"], ["", "", "def", "count_params", "(", "params", ")", ":", "\n", "  ", "num_params", "=", "sum", "(", "p", ".", "data", ".", "nelement", "(", ")", "for", "p", "in", "params", ")", "\n", "return", "num_params", "\n", "\n"]], "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.utils.save_checkpoint": [[55, 61], ["print", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "os.path.join", "os.path.join", "optimizer.state_dict", "os.path.join", "os.path.join"], "function", ["None"], ["", "def", "save_checkpoint", "(", "extra", ",", "model", ",", "optimizer", ",", "hparams", ",", "path", ")", ":", "\n", "  ", "print", "(", "\"Saving model to '{0}'\"", ".", "format", "(", "path", ")", ")", "\n", "torch", ".", "save", "(", "extra", ",", "os", ".", "path", ".", "join", "(", "path", ",", "\"extra.pt\"", ")", ")", "\n", "torch", ".", "save", "(", "model", ",", "os", ".", "path", ".", "join", "(", "path", ",", "\"model.pt\"", ")", ")", "\n", "torch", ".", "save", "(", "optimizer", ".", "state_dict", "(", ")", ",", "os", ".", "path", ".", "join", "(", "path", ",", "\"optimizer.pt\"", ")", ")", "\n", "torch", ".", "save", "(", "hparams", ",", "os", ".", "path", ".", "join", "(", "path", ",", "\"hparams.pt\"", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.utils.set_lr": [[75, 78], ["None"], "function", ["None"], ["", "", "def", "set_lr", "(", "optim", ",", "lr", ")", ":", "\n", "  ", "for", "param_group", "in", "optim", ".", "param_groups", ":", "\n", "    ", "param_group", "[", "\"lr\"", "]", "=", "lr", "\n", "\n"]], "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.utils.init_param": [[79, 93], ["torch.xavier_normal", "torch.xavier_uniform", "torch.kaiming_normal", "torch.kaiming_uniform", "torch.uniform", "ValueError"], "function", ["None"], ["", "", "def", "init_param", "(", "p", ",", "init_type", "=", "\"uniform\"", ",", "init_range", "=", "None", ")", ":", "\n", "  ", "if", "init_type", "==", "\"xavier_normal\"", ":", "\n", "    ", "init", ".", "xavier_normal", "(", "p", ")", "\n", "", "elif", "init_type", "==", "\"xavier_uniform\"", ":", "\n", "    ", "init", ".", "xavier_uniform", "(", "p", ")", "\n", "", "elif", "init_type", "==", "\"kaiming_normal\"", ":", "\n", "    ", "init", ".", "kaiming_normal", "(", "p", ")", "\n", "", "elif", "init_type", "==", "\"kaiming_uniform\"", ":", "\n", "    ", "init", ".", "kaiming_uniform", "(", "p", ")", "\n", "", "elif", "init_type", "==", "\"uniform\"", ":", "\n", "    ", "assert", "init_range", "is", "not", "None", "and", "init_range", ">", "0", "\n", "init", ".", "uniform", "(", "p", ",", "-", "init_range", ",", "init_range", ")", "\n", "", "else", ":", "\n", "    ", "raise", "ValueError", "(", "\"Unknown init_type '{0}'\"", ".", "format", "(", "init_type", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.utils.add_argument": [[94, 122], ["parser.add_mutually_exclusive_group", "parser.add_mutually_exclusive_group.add_argument", "parser.add_mutually_exclusive_group.add_argument", "parser.set_defaults", "parser.add_argument", "parser.add_argument", "parser.add_argument", "ValueError"], "function", ["home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.utils.add_argument", "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.utils.add_argument", "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.utils.add_argument", "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.utils.add_argument", "home.repos.pwc.inspect_result.cindyxinyiwang_TrDec_pytorch.src.utils.add_argument"], ["", "", "def", "add_argument", "(", "parser", ",", "name", ",", "type", ",", "default", ",", "help", ")", ":", "\n", "  ", "\"\"\"Add an argument.\n\n  Args:\n    name: arg's name.\n    type: must be [\"bool\", \"int\", \"float\", \"str\"].\n    default: corresponding type of value.\n    help: help message.\n  \"\"\"", "\n", "\n", "if", "type", "==", "\"bool\"", ":", "\n", "    ", "feature_parser", "=", "parser", ".", "add_mutually_exclusive_group", "(", "required", "=", "False", ")", "\n", "feature_parser", ".", "add_argument", "(", "\"--{0}\"", ".", "format", "(", "name", ")", ",", "dest", "=", "name", ",", "\n", "action", "=", "\"store_true\"", ",", "help", "=", "help", ")", "\n", "feature_parser", ".", "add_argument", "(", "\"--no_{0}\"", ".", "format", "(", "name", ")", ",", "dest", "=", "name", ",", "\n", "action", "=", "\"store_false\"", ",", "help", "=", "help", ")", "\n", "parser", ".", "set_defaults", "(", "name", "=", "default", ")", "\n", "", "elif", "type", "==", "\"int\"", ":", "\n", "    ", "parser", ".", "add_argument", "(", "\"--{0}\"", ".", "format", "(", "name", ")", ",", "\n", "type", "=", "int", ",", "default", "=", "default", ",", "help", "=", "help", ")", "\n", "", "elif", "type", "==", "\"float\"", ":", "\n", "    ", "parser", ".", "add_argument", "(", "\"--{0}\"", ".", "format", "(", "name", ")", ",", "\n", "type", "=", "float", ",", "default", "=", "default", ",", "help", "=", "help", ")", "\n", "", "elif", "type", "==", "\"str\"", ":", "\n", "    ", "parser", ".", "add_argument", "(", "\"--{0}\"", ".", "format", "(", "name", ")", ",", "\n", "type", "=", "str", ",", "default", "=", "default", ",", "help", "=", "help", ")", "\n", "", "else", ":", "\n", "    ", "raise", "ValueError", "(", "\"Unknown type '{0}'\"", ".", "format", "(", "type", ")", ")", "\n", "\n"]]}