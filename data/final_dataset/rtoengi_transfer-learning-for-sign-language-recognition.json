{"home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.datasets.tf_record_utils.tf_record_dataset": [[14, 34], ["tensorflow.data.TFRecordDataset", "tensorflow.data.Options", "dataset.with_options.with_options", "datasets.utils._tf_records_dir", "os.listdir"], "function", ["home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.datasets.utils._tf_records_dir"], ["def", "tf_record_dataset", "(", "dataset_name", ":", "DatasetName", ",", "dataset_type", ":", "DatasetType", ",", "ordered", "=", "False", ")", ":", "\n", "    ", "\"\"\"Returns a `TFRecordDataset` of the requested dataset.\n\n    Arguments:\n        dataset_name: The name of the dataset.\n        dataset_type: The type of the dataset.\n        ordered: Whether the examples should be fetched in order.\n\n    Returns:\n        A `TFRecordDataset` of the requested dataset.\n    \"\"\"", "\n", "path", "=", "f'{_tf_records_dir(dataset_name)}/{dataset_type.value}'", "\n", "files", "=", "[", "f'{path}/{file}'", "for", "file", "in", "os", ".", "listdir", "(", "path", ")", "]", "\n", "num_parallel_reads", "=", "1", "if", "ordered", "else", "tf", ".", "data", ".", "experimental", ".", "AUTOTUNE", "\n", "dataset", "=", "tf", ".", "data", ".", "TFRecordDataset", "(", "files", ",", "num_parallel_reads", "=", "num_parallel_reads", ")", "\n", "if", "not", "ordered", ":", "\n", "        ", "options", "=", "tf", ".", "data", ".", "Options", "(", ")", "\n", "options", ".", "experimental_deterministic", "=", "False", "\n", "dataset", "=", "dataset", ".", "with_options", "(", "options", ")", "\n", "", "return", "dataset", "\n", "\n"]], "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.datasets.tf_record_utils._dataset_counts": [[36, 54], ["tf_record_utils.tf_record_dataset", "dataset.prefetch.batch", "dataset.prefetch.prefetch", "len"], "function", ["home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.datasets.tf_record_utils.tf_record_dataset"], ["", "def", "_dataset_counts", "(", "dataset_name", ":", "DatasetName", ")", ":", "\n", "    ", "\"\"\"Returns the sizes of the `dataset_name` train, validation and test datasets.\n\n    Arguments:\n        dataset_name: The name of the dataset.\n\n    Returns:\n        A dictionary with an entry of the size for each of the train, validation and test datasets.\n    \"\"\"", "\n", "counts", "=", "{", "}", "\n", "for", "dataset_type", "in", "DatasetType", ":", "\n", "        ", "dataset", "=", "tf_record_dataset", "(", "dataset_name", ",", "dataset_type", ")", "\n", "dataset", "=", "dataset", ".", "batch", "(", "64", ")", "\n", "dataset", "=", "dataset", ".", "prefetch", "(", "1", ")", "\n", "counts", "[", "dataset_type", ".", "value", "]", "=", "0", "\n", "for", "records", "in", "dataset", ":", "\n", "            ", "counts", "[", "dataset_type", ".", "value", "]", "+=", "len", "(", "records", ")", "\n", "", "", "return", "counts", "\n", "\n"]], "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.datasets.tf_record_utils._bytes_feature": [[56, 63], ["tensorflow.train.Feature", "tensorflow.train.BytesList"], "function", ["None"], ["", "def", "_bytes_feature", "(", "bytes_list", ")", ":", "\n", "    ", "\"\"\"Converts a list of bytestrings into a protocol buffer feature message.\n\n    Returns:\n        A protocol buffer feature message.\n    \"\"\"", "\n", "return", "tf", ".", "train", ".", "Feature", "(", "bytes_list", "=", "tf", ".", "train", ".", "BytesList", "(", "value", "=", "bytes_list", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.datasets.tf_record_utils._float_feature": [[65, 72], ["tensorflow.train.Feature", "tensorflow.train.FloatList"], "function", ["None"], ["", "def", "_float_feature", "(", "float_list", ")", ":", "\n", "    ", "\"\"\"Converts a list of floats into a protocol buffer feature message.\n\n    Returns:\n        A protocol buffer feature message.\n    \"\"\"", "\n", "return", "tf", ".", "train", ".", "Feature", "(", "float_list", "=", "tf", ".", "train", ".", "FloatList", "(", "value", "=", "float_list", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.datasets.tf_record_utils._int64_feature": [[74, 81], ["tensorflow.train.Feature", "tensorflow.train.Int64List"], "function", ["None"], ["", "def", "_int64_feature", "(", "int64_list", ")", ":", "\n", "    ", "\"\"\"Converts a list of ints into a protocol buffer feature message.\n\n    Returns:\n        A protocol buffer feature message.\n    \"\"\"", "\n", "return", "tf", ".", "train", ".", "Feature", "(", "int64_list", "=", "tf", ".", "train", ".", "Int64List", "(", "value", "=", "int64_list", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.datasets.tf_record_utils._decode_jpeg": [[83, 94], ["numpy.frombuffer", "cv2.imdecode"], "function", ["None"], ["", "def", "_decode_jpeg", "(", "bytestring", ")", ":", "\n", "    ", "\"\"\"Decodes a compressed JPEG image into an ndarray.\n\n    Arguments:\n        bytestring: The binary string representation of the compressed JPEG image.\n\n    Returns:\n        The ndarray representation of the image using the uint8 data type for the channel values.\n    \"\"\"", "\n", "image", "=", "np", ".", "frombuffer", "(", "bytestring", ",", "np", ".", "uint8", ")", "\n", "return", "cv2", ".", "imdecode", "(", "image", ",", "cv2", ".", "IMREAD_COLOR", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.datasets.tf_record_utils._transform_frames_for_inspection": [[96, 110], ["numpy.array", "tf_record_utils._decode_jpeg", "examples.numpy"], "function", ["home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.datasets.tf_record_utils._decode_jpeg"], ["", "def", "_transform_frames_for_inspection", "(", "examples", ")", ":", "\n", "    ", "\"\"\"Transforms a batch of example frames to be consumed for inspection.\n\n    An individual frame is a 3D tensor consisting of RGB uint8 values within the range [0, 255] represented in the\n    `channels_last` data format ([height, width, channels]).\n\n    Arguments:\n        examples: A 5D tensor representing a batch of example frames in the [batch, frames, height, width, channels]\n        format.\n\n    Returns:\n        The transformed batch of example frames.\n    \"\"\"", "\n", "return", "np", ".", "array", "(", "[", "[", "_decode_jpeg", "(", "frame", ")", "for", "frame", "in", "example", "]", "for", "example", "in", "examples", ".", "numpy", "(", ")", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.datasets.tf_record_utils._transform_frames_for_model": [[112, 130], ["numpy.array", "frames.astype.astype", "tf_record_utils._decode_jpeg", "examples.numpy"], "function", ["home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.datasets.tf_record_utils._decode_jpeg"], ["", "def", "_transform_frames_for_model", "(", "examples", ")", ":", "\n", "    ", "\"\"\"Transforms a batch of example frames to be consumed by a model.\n\n    An individual frame is a 3D tensor consisting of RGB float32 values within the range [-1.0, 1.0] represented in the\n    `channels_last` data format ([height, width, channels]).\n\n    Arguments:\n        examples: A 5D tensor representing a batch of example frames in the [batch, frames, height, width, channels]\n        format.\n\n    Returns:\n        The transformed batch of example frames.\n    \"\"\"", "\n", "frames", "=", "np", ".", "array", "(", "[", "[", "_decode_jpeg", "(", "frame", ")", "for", "frame", "in", "example", "]", "for", "example", "in", "examples", ".", "numpy", "(", ")", "]", ")", "\n", "frames", "=", "frames", ".", "astype", "(", "np", ".", "float32", ",", "copy", "=", "False", ")", "\n", "frames", "/=", "127.5", "\n", "frames", "-=", "1.0", "\n", "return", "frames", "\n", "\n"]], "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.datasets.tf_record_utils.transform_for_inspection": [[139, 158], ["tensorflow.io.parse_example", "tensorflow.py_function"], "function", ["None"], ["def", "transform_for_inspection", "(", "examples", ")", ":", "\n", "    ", "\"\"\"Transforms a batch of examples to be consumed for inspection.\n\n    The returned frames are represented as RGB uint8 values within the range [0, 255], and the labels and signers are\n    represented as their corresponding indices.\n\n    Arguments:\n        examples: A batch of serialized `TFRecord` examples.\n\n    Returns:\n        A tuple of batches of frames, labels and signers.\n    \"\"\"", "\n", "parsed_examples", "=", "tf", ".", "io", ".", "parse_example", "(", "examples", ",", "_FEATURES", ")", "\n", "\n", "frames", "=", "tf", ".", "py_function", "(", "_transform_frames_for_inspection", ",", "[", "parsed_examples", "[", "'frames'", "]", "]", ",", "tf", ".", "uint8", ")", "\n", "labels", "=", "parsed_examples", "[", "'label'", "]", "\n", "signers", "=", "parsed_examples", "[", "'signer'", "]", "\n", "\n", "return", "frames", ",", "labels", ",", "signers", "\n", "\n"]], "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.datasets.tf_record_utils.transform_for_prediction": [[160, 179], ["tensorflow.io.parse_example", "tensorflow.py_function"], "function", ["None"], ["", "def", "transform_for_prediction", "(", "examples", ")", ":", "\n", "    ", "\"\"\"Transforms a batch of examples to be consumed for prediction.\n\n    The returned frames are represented as RGB float32 values within the range [-1.0, 1.0], and the labels and signers\n    are label encoded.\n\n    Arguments:\n        examples: A batch of serialized `TFRecord` examples.\n\n    Returns:\n        A tuple of batches of frames, labels and signers.\n    \"\"\"", "\n", "parsed_examples", "=", "tf", ".", "io", ".", "parse_example", "(", "examples", ",", "_FEATURES", ")", "\n", "\n", "frames", "=", "tf", ".", "py_function", "(", "_transform_frames_for_model", ",", "[", "parsed_examples", "[", "'frames'", "]", "]", ",", "tf", ".", "float32", ")", "\n", "labels", "=", "parsed_examples", "[", "'label'", "]", "\n", "signers", "=", "parsed_examples", "[", "'signer'", "]", "\n", "\n", "return", "frames", ",", "labels", ",", "signers", "\n", "\n"]], "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.datasets.tf_record_utils.transform_for_msasl_model": [[181, 199], ["tensorflow.io.parse_example", "tensorflow.py_function", "tensorflow.one_hot"], "function", ["None"], ["", "def", "transform_for_msasl_model", "(", "examples", ")", ":", "\n", "    ", "\"\"\"Transforms a batch of `MS-ASL` dataset examples to be consumed for training.\n\n    The returned frames are represented as RGB float32 values within the range [-1.0, 1.0], and the labels are one-hot\n    encoded with a depth of `datasets.msasl.constants.N_CLASSES`.\n\n    Arguments:\n        examples: A batch of serialized `TFRecord` examples.\n\n    Returns:\n        A tuple of batches of frames and labels.\n    \"\"\"", "\n", "parsed_examples", "=", "tf", ".", "io", ".", "parse_example", "(", "examples", ",", "_FEATURES", ")", "\n", "\n", "frames", "=", "tf", ".", "py_function", "(", "_transform_frames_for_model", ",", "[", "parsed_examples", "[", "'frames'", "]", "]", ",", "tf", ".", "float32", ")", "\n", "labels", "=", "tf", ".", "one_hot", "(", "parsed_examples", "[", "'label'", "]", ",", "MSASL_N_CLASSES", ")", "\n", "\n", "return", "frames", ",", "labels", "\n", "\n"]], "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.datasets.tf_record_utils.transform_for_signum_model": [[201, 219], ["tensorflow.io.parse_example", "tensorflow.py_function", "tensorflow.one_hot"], "function", ["None"], ["", "def", "transform_for_signum_model", "(", "examples", ")", ":", "\n", "    ", "\"\"\"Transforms a batch of `SIGNUM` dataset examples to be consumed for training.\n\n    The returned frames are represented as RGB float32 values within the range [-1.0, 1.0], and the labels are one-hot\n    encoded with a depth of `datasets.signum.constants.N_CLASSES`.\n\n    Arguments:\n        examples: A batch of serialized `TFRecord` examples.\n\n    Returns:\n        A tuple of batches of frames and labels.\n    \"\"\"", "\n", "parsed_examples", "=", "tf", ".", "io", ".", "parse_example", "(", "examples", ",", "_FEATURES", ")", "\n", "\n", "frames", "=", "tf", ".", "py_function", "(", "_transform_frames_for_model", ",", "[", "parsed_examples", "[", "'frames'", "]", "]", ",", "tf", ".", "float32", ")", "\n", "labels", "=", "tf", ".", "one_hot", "(", "parsed_examples", "[", "'label'", "]", ",", "SIGNUM_N_CLASSES", ")", "\n", "\n", "return", "frames", ",", "labels", "\n", "", ""]], "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.datasets.utils._frame_positions": [[8, 26], ["positions.sort", "numpy.random.choice"], "function", ["None"], ["def", "_frame_positions", "(", "start", ":", "int", ",", "end", ":", "int", ")", ":", "\n", "    ", "\"\"\"Samples frame numbers in the range between `start` and `end`.\n\n    `_N_TIME_STEPS` number of samples are drawn uniformly in the range between `start` and `end`. If `_N_TIME_STEPS` is\n    greater than the range that is drawn from, then sampling is done with replacement. The samples are sorted in\n    ascending order.\n\n    Arguments:\n        start: The start frame number, inclusive.\n        end: The end frame number, inclusive.\n\n    Returns:\n        The sorted array of frame numbers between `start` and `end`.\n    \"\"\"", "\n", "range_", "=", "end", "-", "start", "+", "1", "\n", "positions", "=", "np", ".", "random", ".", "choice", "(", "range_", ",", "_N_TIME_STEPS", ",", "replace", "=", "range_", "<", "_N_TIME_STEPS", ")", "+", "start", "\n", "positions", ".", "sort", "(", ")", "\n", "return", "positions", "\n", "\n"]], "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.datasets.utils._crop_image_to_square": [[28, 67], ["int", "int", "max", "min", "int", "int", "max", "min"], "function", ["None"], ["", "def", "_crop_image_to_square", "(", "image", ",", "center_x_ratio", "=", "0.5", ",", "center_y_ratio", "=", "0.5", ")", ":", "\n", "    ", "\"\"\"Crops an image along its longer side to get a square image.\n\n    The image is cropped along its longer side so that the resulting image has the same width and height. If the image\n    is wider than high, then the horizontal cropping is centered around `center_x_ratio`. On the other hand, if the\n    image is higher than wide, then the vertical cropping is centered around `center_y_ratio`. Should the cropping\n    around the center ratios exceed the boundaries of the image, then the cropping sticks to the corresponding boundary.\n    A square image is left unchanged.\n\n    Arguments:\n        image: The ndarray image to be cropped.\n        center_x_ratio: The relative abscissa the horizontal cropping is centered around (float between 0 and 1).\n        center_y_ratio: The relative ordinate the vertical cropping is centered around (float between 0 and 1).\n\n    Returns:\n        A square ndarray image.\n    \"\"\"", "\n", "height", "=", "image", ".", "shape", "[", "0", "]", "\n", "width", "=", "image", ".", "shape", "[", "1", "]", "\n", "if", "width", ">", "height", ":", "\n", "        ", "mid_x", "=", "int", "(", "width", "*", "center_x_ratio", ")", "\n", "half_height", "=", "int", "(", "height", "/", "2", ")", "\n", "if", "center_x_ratio", "<=", "0.5", ":", "\n", "            ", "x", "=", "max", "(", "0", ",", "mid_x", "-", "half_height", ")", "\n", "return", "image", "[", ":", ",", "x", ":", "x", "+", "height", "]", "\n", "", "else", ":", "\n", "            ", "x", "=", "min", "(", "width", ",", "mid_x", "+", "half_height", ")", "\n", "return", "image", "[", ":", ",", "x", "-", "height", ":", "x", "]", "\n", "", "", "elif", "height", ">", "width", ":", "\n", "        ", "mid_y", "=", "int", "(", "height", "*", "center_y_ratio", ")", "\n", "half_width", "=", "int", "(", "width", "/", "2", ")", "\n", "if", "center_y_ratio", "<=", "0.5", ":", "\n", "            ", "y", "=", "max", "(", "0", ",", "mid_y", "-", "half_width", ")", "\n", "return", "image", "[", "y", ":", "y", "+", "width", "]", "\n", "", "else", ":", "\n", "            ", "y", "=", "min", "(", "height", ",", "mid_y", "+", "half_width", ")", "\n", "return", "image", "[", "y", "-", "width", ":", "y", "]", "\n", "", "", "else", ":", "\n", "        ", "return", "image", "\n", "\n"]], "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.datasets.utils._tf_records_dir": [[75, 85], ["None"], "function", ["None"], ["def", "_tf_records_dir", "(", "dataset_name", ":", "DatasetName", ")", ":", "\n", "    ", "\"\"\"Returns the path to the directory storing the `TFRecord` files of the requested dataset.\n\n    Arguments:\n        dataset_name: The name of the dataset.\n\n    Returns:\n        The absolute path to the directory storing the `TFRecord` files of the requested dataset.\n    \"\"\"", "\n", "return", "_TF_RECORDS_DIR_DICT", "[", "dataset_name", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.datasets.utils._display_dataset_counts": [[87, 97], ["print", "print", "print", "print", "print", "sum", "counts.values", "sum", "counts.values"], "function", ["None"], ["", "def", "_display_dataset_counts", "(", "dataset_name", ":", "DatasetName", ",", "counts", ")", ":", "\n", "    ", "\"\"\"Displays the sizes of the `dataset_name` train, validation and test datasets.\"\"\"", "\n", "line_width", "=", "27", "\n", "print", "(", "f'{dataset_name.name} dataset counts'", ")", "\n", "print", "(", "'='", "*", "line_width", ")", "\n", "for", "dataset_type", "in", "DatasetType", ":", "\n", "        ", "count", "=", "counts", "[", "dataset_type", ".", "value", "]", "\n", "print", "(", "f\"{dataset_type.name + ':':13s} {count:5d} ({count / sum(counts.values()) * 100:02.1f}%)\"", ")", "\n", "", "print", "(", "'='", "*", "line_width", ")", "\n", "print", "(", "f\"{'Total:':13s} {sum(counts.values()):5d} (100%)\"", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.datasets.test_utils.test__crop_image_to_square": [[6, 38], ["numpy.random.rand", "numpy.array_equal", "numpy.random.rand", "numpy.array_equal", "numpy.array_equal", "numpy.array_equal", "numpy.array_equal", "numpy.random.rand", "numpy.array_equal", "numpy.array_equal", "numpy.array_equal", "numpy.array_equal", "datasets.utils._crop_image_to_square", "datasets.utils._crop_image_to_square", "datasets.utils._crop_image_to_square", "datasets.utils._crop_image_to_square", "datasets.utils._crop_image_to_square", "datasets.utils._crop_image_to_square", "datasets.utils._crop_image_to_square", "datasets.utils._crop_image_to_square", "datasets.utils._crop_image_to_square"], "function", ["home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.datasets.utils._crop_image_to_square", "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.datasets.utils._crop_image_to_square", "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.datasets.utils._crop_image_to_square", "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.datasets.utils._crop_image_to_square", "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.datasets.utils._crop_image_to_square", "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.datasets.utils._crop_image_to_square", "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.datasets.utils._crop_image_to_square", "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.datasets.utils._crop_image_to_square", "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.datasets.utils._crop_image_to_square"], ["def", "test__crop_image_to_square", "(", ")", ":", "\n", "# Don't crop square image", "\n", "    ", "image", "=", "np", ".", "random", ".", "rand", "(", "5", ",", "5", ",", "3", ")", "\n", "assert", "np", ".", "array_equal", "(", "image", ",", "_crop_image_to_square", "(", "image", ")", ")", "\n", "\n", "image", "=", "np", ".", "random", ".", "rand", "(", "5", ",", "10", ",", "3", ")", "\n", "\n", "# Crop horizontally on the left side", "\n", "assert", "np", ".", "array_equal", "(", "image", "[", ":", ",", "2", ":", "7", ",", ":", "]", ",", "_crop_image_to_square", "(", "image", ",", "0.4", ",", "0.5", ")", ")", "\n", "\n", "# Crop horizontally at the left boundary", "\n", "assert", "np", ".", "array_equal", "(", "image", "[", ":", ",", ":", "5", ",", ":", "]", ",", "_crop_image_to_square", "(", "image", ",", "0.1", ",", "0.5", ")", ")", "\n", "\n", "# Crop horizontally on the right side", "\n", "assert", "np", ".", "array_equal", "(", "image", "[", ":", ",", "3", ":", "8", ",", ":", "]", ",", "_crop_image_to_square", "(", "image", ",", "0.6", ",", "0.5", ")", ")", "\n", "\n", "# Crop horizontally at the right boundary", "\n", "assert", "np", ".", "array_equal", "(", "image", "[", ":", ",", "-", "5", ":", ",", ":", "]", ",", "_crop_image_to_square", "(", "image", ",", "0.9", ",", "0.5", ")", ")", "\n", "\n", "image", "=", "np", ".", "random", ".", "rand", "(", "10", ",", "5", ",", "3", ")", "\n", "\n", "# Crop vertically on the upper side", "\n", "assert", "np", ".", "array_equal", "(", "image", "[", "2", ":", "7", ",", ":", ",", ":", "]", ",", "_crop_image_to_square", "(", "image", ",", "0.5", ",", "0.4", ")", ")", "\n", "\n", "# Crop vertically at the upper boundary", "\n", "assert", "np", ".", "array_equal", "(", "image", "[", ":", "5", ",", ":", ",", ":", "]", ",", "_crop_image_to_square", "(", "image", ",", "0.5", ",", "0.1", ")", ")", "\n", "\n", "# Crop vertically on the lower side", "\n", "assert", "np", ".", "array_equal", "(", "image", "[", "3", ":", "8", ",", ":", ",", ":", "]", ",", "_crop_image_to_square", "(", "image", ",", "0.5", ",", "0.6", ")", ")", "\n", "\n", "# Crop vertically at the lower boundary", "\n", "assert", "np", ".", "array_equal", "(", "image", "[", "-", "5", ":", ",", ":", ",", ":", "]", ",", "_crop_image_to_square", "(", "image", ",", "0.5", ",", "0.9", ")", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.datasets.test_video_download.test__extract_video_id": [[4, 6], ["datasets.msasl.video_download._extract_video_id"], "function", ["home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.msasl.video_download._extract_video_id"], ["def", "test__extract_video_id", "(", ")", ":", "\n", "    ", "assert", "_extract_video_id", "(", "'https://www.youtube.com/watch?v=S2cqitZ0qes'", ")", "==", "'S2cqitZ0qes'", "\n", "", ""]], "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.signum.tf_record._dataset_type_dir": [[16, 35], ["None"], "function", ["None"], ["def", "_dataset_type_dir", "(", "signer", ")", ":", "\n", "    ", "\"\"\"Returns the directory name of the corresponding dataset type.\n\n    There is a `TFRecord` file written for each of the 25 signers. The `TFRecord` files of the first 17 signers are\n    assigned to the train dataset, the `TFRecord` files of the next 4 signers are assigned to the validation dataset,\n    and the `TFRecord` files of the last 4 signers are assigned to the test dataset.\n\n    Arguments:\n        signer: The index of the signer.\n\n    Returns:\n        The directory name of the corresponding dataset type.\n    \"\"\"", "\n", "if", "signer", ">", "20", ":", "\n", "        ", "return", "DatasetType", ".", "TEST", ".", "value", "\n", "", "elif", "signer", ">", "16", ":", "\n", "        ", "return", "DatasetType", ".", "VALIDATION", ".", "value", "\n", "", "else", ":", "\n", "        ", "return", "DatasetType", ".", "TRAIN", ".", "value", "\n", "\n"]], "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.signum.tf_record._read_frames": [[37, 58], ["datasets.utils._frame_positions", "operator.itemgetter", "os.listdir", "cv2.imread", "datasets.utils._crop_image_to_square", "cv2.imencode", "frames.append", "cv2.resize", "buffer.tobytes"], "function", ["home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.datasets.utils._frame_positions", "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.datasets.utils._crop_image_to_square"], ["", "", "def", "_read_frames", "(", "example_path", ")", ":", "\n", "    ", "\"\"\"Reads the individual frames out of the list of images of the dataset example.\n\n    A frame is encoded as a compressed JPEG image instead of an ndarray, as the latter consumes up to 10 times as much\n    storage space.\n\n    Arguments:\n        example_path: The path to the dataset example.\n\n    Returns:\n        The list of frames of the dataset example.\n    \"\"\"", "\n", "positions", "=", "_frame_positions", "(", "0", ",", "_N_FRAMES_PER_EXAMPLE", "-", "1", ")", "\n", "file_names", "=", "itemgetter", "(", "*", "positions", ")", "(", "os", ".", "listdir", "(", "example_path", ")", ")", "\n", "frames", "=", "[", "]", "\n", "for", "file_name", "in", "file_names", ":", "\n", "        ", "frame", "=", "cv2", ".", "imread", "(", "f'{example_path}/{file_name}'", ")", "\n", "cropped_frame", "=", "_crop_image_to_square", "(", "frame", ")", "\n", "_", ",", "buffer", "=", "cv2", ".", "imencode", "(", "'.jpg'", ",", "cv2", ".", "resize", "(", "cropped_frame", ",", "_FRAME_SIZE", ")", ")", "\n", "frames", ".", "append", "(", "buffer", ".", "tobytes", "(", ")", ")", "\n", "", "return", "frames", "\n", "\n"]], "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.signum.tf_record._serialize_example": [[60, 79], ["tf_record._read_frames", "tensorflow.train.Example", "tf.train.Example.SerializeToString", "datasets.tf_record_utils._bytes_feature", "datasets.tf_record_utils._int64_feature", "datasets.tf_record_utils._int64_feature", "tensorflow.train.Features"], "function", ["home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.msasl.tf_record._read_frames", "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.datasets.tf_record_utils._bytes_feature", "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.datasets.tf_record_utils._int64_feature", "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.datasets.tf_record_utils._int64_feature"], ["", "def", "_serialize_example", "(", "example_path", ",", "label", ",", "signer", ")", ":", "\n", "    ", "\"\"\"Serializes a dataset example in the `TFRecord` format.\n\n    Arguments:\n        example_path: The path to the dataset example.\n        label: The index of the label.\n        signer: The index of the signer.\n\n    Returns:\n        The binary string representation of the `TFRecord`.\n    \"\"\"", "\n", "frames", "=", "_read_frames", "(", "example_path", ")", "\n", "feature", "=", "{", "\n", "'frames'", ":", "_bytes_feature", "(", "frames", ")", ",", "\n", "'label'", ":", "_int64_feature", "(", "[", "label", "]", ")", ",", "\n", "'signer'", ":", "_int64_feature", "(", "[", "signer", "]", ")", "\n", "}", "\n", "tf_example", "=", "tf", ".", "train", ".", "Example", "(", "features", "=", "tf", ".", "train", ".", "Features", "(", "feature", "=", "feature", ")", ")", "\n", "return", "tf_example", ".", "SerializeToString", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.signum.tf_record.write_tf_records": [[81, 101], ["os.listdir", "enumerate", "pathlib.Path().mkdir", "tensorflow.io.TFRecordWriter", "os.listdir", "enumerate", "logging.info", "tf_record._dataset_type_dir", "pathlib.Path", "tf_record._serialize_example", "writer.write"], "function", ["home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.signum.tf_record._dataset_type_dir", "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.msasl.tf_record._serialize_example"], ["", "def", "write_tf_records", "(", ")", ":", "\n", "    ", "\"\"\"Creates the `TFRecord` files of the `SIGNUM` train, validation and test datasets.\n\n    The `TFRecord` files of the train, validation and test datasets are saved into corresponding subdirectories inside\n    the `_SIGNUM_TF_RECORDS_DIR` directory. There is one `TFRecord` file for each signer.\n    \"\"\"", "\n", "signer_dirs", "=", "os", ".", "listdir", "(", "_SIGNUM_IMAGES_DIR", ")", "\n", "for", "signer", ",", "signer_dir", "in", "enumerate", "(", "signer_dirs", ")", ":", "\n", "        ", "path", "=", "f'{_SIGNUM_TF_RECORDS_DIR}/{_dataset_type_dir(signer)}'", "\n", "Path", "(", "path", ")", ".", "mkdir", "(", "exist_ok", "=", "True", ")", "\n", "file_name", "=", "f'{path}/signum_{signer + 1:02d}.tfrecord'", "\n", "with", "tf", ".", "io", ".", "TFRecordWriter", "(", "file_name", ")", "as", "writer", ":", "\n", "            ", "running_record_number", "=", "0", "\n", "example_dirs", "=", "os", ".", "listdir", "(", "f'{_SIGNUM_IMAGES_DIR}/{signer_dir}'", ")", "\n", "for", "label", ",", "example_dir", "in", "enumerate", "(", "example_dirs", ")", ":", "\n", "                ", "example_path", "=", "f'{_SIGNUM_IMAGES_DIR}/{signer_dir}/{example_dir}'", "\n", "serialized_example", "=", "_serialize_example", "(", "example_path", ",", "label", ",", "signer", ")", "\n", "writer", ".", "write", "(", "serialized_example", ")", "\n", "running_record_number", "+=", "1", "\n", "", "logging", ".", "info", "(", "f'{running_record_number} records have been written to {file_name}.'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.inspection.display._display_frames": [[6, 21], ["len", "enumerate", "cv2.destroyAllWindows", "cv2.imshow", "cv2.waitKey", "cv2.putText"], "function", ["None"], ["def", "_display_frames", "(", "frames", ",", "show_frame_number", "=", "True", ",", "delay", "=", "0", ")", ":", "\n", "    ", "\"\"\"Displays a sequence of frames.\n\n    Arguments:\n        frames: The frames to display.\n        show_frame_number: Whether to display the frame number of the current frame displayed (defaults to `True`).\n        delay: The number of milliseconds an individual frame is displayed (defaults to 0; waiting for any keypress).\n    \"\"\"", "\n", "count", "=", "len", "(", "frames", ")", "\n", "for", "i", ",", "frame", "in", "enumerate", "(", "frames", ")", ":", "\n", "        ", "if", "show_frame_number", ":", "\n", "            ", "frame", "=", "cv2", ".", "putText", "(", "frame", ",", "f'{i + 1}:{count}'", ",", "(", "5", ",", "20", ")", ",", "*", "_FONT", ")", "\n", "", "cv2", ".", "imshow", "(", "'Display'", ",", "frame", ")", "\n", "cv2", ".", "waitKey", "(", "delay", ")", "\n", "", "cv2", ".", "destroyAllWindows", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.inspection.display._play_frames": [[23, 34], ["display._display_frames"], "function", ["home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.inspection.display._display_frames"], ["", "def", "_play_frames", "(", "frames", ",", "show_frame_number", "=", "True", ",", "delay", "=", "150", ")", ":", "\n", "    ", "\"\"\"Displays a sequence of frames in succession.\n\n    Convenience function with a default play rate for method `_display_frames`.\n\n    Arguments:\n        frames: The frames to display in succession.\n        show_frame_number: Whether to display the frame number of the current frame displayed (defaults to `True`).\n        delay: The number of milliseconds an individual frame is displayed (defaults to 150).\n    \"\"\"", "\n", "_display_frames", "(", "frames", ",", "show_frame_number", ",", "delay", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.inspection.tf_record_inspect.inspect_dataset": [[6, 27], ["datasets.tf_record_utils.tf_record_dataset", "dataset.skip.batch", "dataset.skip.map", "dataset.skip.take", "dataset.skip.skip", "frames_batch.numpy.numpy", "inspect_fn"], "function", ["home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.datasets.tf_record_utils.tf_record_dataset"], ["def", "inspect_dataset", "(", "dataset_name", ":", "DatasetName", ",", "dataset_type", ":", "DatasetType", ",", "\n", "inspect_fn", ",", "batch_size", "=", "1", ",", "skip_count", "=", "0", ",", "take_count", "=", "1", ")", ":", "\n", "    ", "\"\"\"Inspects a given number of dataset examples according to a passed inspection function.\n\n    Arguments:\n        dataset_name: The name of the dataset to inspect (one of `DatasetName`).\n        dataset_type: The type of the dataset to inspect (one of `DatasetType`).\n        inspect_fn: The function to apply to a sequence of frames.\n        batch_size: The number of consecutive dataset examples to combine into a batch.\n        skip_count: The number of dataset examples to skip.\n        take_count: The maximum number of dataset examples to fetch.\n    \"\"\"", "\n", "dataset", "=", "tf_record_dataset", "(", "dataset_name", ",", "dataset_type", ")", "\n", "dataset", "=", "dataset", ".", "batch", "(", "batch_size", ")", "\n", "dataset", "=", "dataset", ".", "map", "(", "transform_for_inspection", ")", "\n", "if", "skip_count", ":", "\n", "        ", "dataset", "=", "dataset", ".", "skip", "(", "skip_count", ")", "\n", "", "for", "frames_batch", ",", "labels", ",", "signers", "in", "dataset", ".", "take", "(", "take_count", ")", ":", "\n", "        ", "frames_batch", "=", "frames_batch", ".", "numpy", "(", ")", "\n", "for", "frames", "in", "frames_batch", ":", "\n", "            ", "inspect_fn", "(", "frames", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.msasl.video_download._video_urls": [[12, 26], ["set", "urls.union.union", "open", "json.load", "video_download._extract_video_id", "video_download._downloaded_video_ids"], "function", ["home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.msasl.video_download._extract_video_id", "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.msasl.video_download._downloaded_video_ids"], ["def", "_video_urls", "(", ")", ":", "\n", "    ", "\"\"\"Returns the URLs of videos that have not yet been downloaded.\n\n    All the URLs in the train, validation and test dataset files of the `MS-ASL` dataset are taken into account.\n\n    Returns:\n        The set of URLs of videos that have not yet been downloaded.\n    \"\"\"", "\n", "urls", "=", "set", "(", ")", "\n", "for", "dataset_type", "in", "DatasetType", ":", "\n", "        ", "with", "open", "(", "f'{_MSASL_SPECS_DIR}/MSASL_{dataset_type.value}.json'", ",", "'r'", ")", "as", "file", ":", "\n", "            ", "dataset", "=", "json", ".", "load", "(", "file", ")", "\n", "", "urls", "=", "urls", ".", "union", "(", "{", "it", "[", "'url'", "]", "for", "it", "in", "dataset", "}", ")", "\n", "", "return", "{", "url", "for", "url", "in", "urls", "if", "_extract_video_id", "(", "url", ")", "not", "in", "_downloaded_video_ids", "(", ")", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.msasl.video_download._extract_video_id": [[28, 39], ["url.index"], "function", ["None"], ["", "def", "_extract_video_id", "(", "url", "=", "''", ")", ":", "\n", "    ", "\"\"\"Extracts the video ID from a YouTube URL.\n\n    Arguments:\n        url: A string representing a YouTube URL (e.g. 'https://www.youtube.com/watch?v=S2cqitZ0qes').\n\n    Returns:\n        The video ID of a YouTube URL (e.g. 'S2cqitZ0qes').\n    \"\"\"", "\n", "index", "=", "url", ".", "index", "(", "'?v='", ")", "+", "3", "\n", "return", "url", "[", "index", ":", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.msasl.video_download._downloaded_video_ids": [[41, 51], ["os.listdir", "set"], "function", ["None"], ["", "def", "_downloaded_video_ids", "(", ")", ":", "\n", "    ", "\"\"\"Returns the IDs of videos that have already been downloaded.\n\n    Can be utilized to prevent the re-downloading of videos in case the download process is executed multiple times.\n\n    Returns:\n        The set of IDs of videos that have already been downloaded.\n    \"\"\"", "\n", "videos", "=", "os", ".", "listdir", "(", "_MSASL_VIDEOS_DIR", ")", "\n", "return", "set", "(", "[", "video", "[", ":", "-", "4", "]", "for", "video", "in", "videos", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.msasl.video_download._download_video": [[53, 67], ["youtube_dl.YoutubeDL", "video.download", "logging.error"], "function", ["None"], ["", "def", "_download_video", "(", "url", ")", ":", "\n", "    ", "\"\"\"Downloads a YouTube video at `url`.\n\n    The video is saved into the `_MSASL_VIDEOS_DIR` directory.\n\n    Arguments:\n        url: A string representing a YouTube URL.\n    \"\"\"", "\n", "ydl_opts", "=", "{", "'outtmpl'", ":", "_MSASL_VIDEOS_DIR", "+", "'/%(id)s.%(ext)s'", "}", "\n", "with", "ydl", ".", "YoutubeDL", "(", "ydl_opts", ")", "as", "video", ":", "\n", "        ", "try", ":", "\n", "            ", "video", ".", "download", "(", "[", "url", "]", ")", "\n", "", "except", "DownloadError", ":", "\n", "            ", "error", "(", "f'Video at {url} could not be downloaded.'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.msasl.video_download.download_videos": [[69, 78], ["video_download._video_urls", "video_download._download_video"], "function", ["home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.msasl.video_download._video_urls", "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.msasl.video_download._download_video"], ["", "", "", "def", "download_videos", "(", ")", ":", "\n", "    ", "\"\"\"Downloads the `MS-ASL` YouTube videos.\n\n    The `MS-ASL` dataset consists of URLs of YouTube videos. Such a video may contain multiple sign gestures, which\n    means that different dataset examples may refer to the same video. This function downloads all the videos of the\n    train, validation and test dataset specification files into the `_MSASL_VIDEOS_DIR` directory.\n    \"\"\"", "\n", "for", "url", "in", "_video_urls", "(", ")", ":", "\n", "        ", "_download_video", "(", "url", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.msasl.info.display_dataset_example_spec": [[9, 16], ["print", "print", "print", "open", "json.load", "json.dumps"], "function", ["None"], []], "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.msasl.specs_filter.create_filtered_specs": [[8, 22], ["datasets.msasl.video_download._downloaded_video_ids", "open", "json.load", "open", "output.write", "datasets.msasl.video_download._extract_video_id", "json.dumps"], "function", ["home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.msasl.video_download._downloaded_video_ids", "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.msasl.video_download._extract_video_id"], ["def", "create_filtered_specs", "(", ")", ":", "\n", "    ", "\"\"\"Filters those rows from the original `MS-ASL` dataset specification files, for which the corresponding videos\n    could not be downloaded.\n\n    Saves the filtered train, validation and test dataset specification files into the `_MSASL_FILTERED_SPECS_DIR`\n    directory.\n    \"\"\"", "\n", "videos", "=", "_downloaded_video_ids", "(", ")", "\n", "for", "dataset_type", "in", "DatasetType", ":", "\n", "        ", "with", "open", "(", "f'{_MSASL_SPECS_DIR}/MSASL_{dataset_type.value}.json'", ",", "'r'", ")", "as", "input", ":", "\n", "            ", "dataset", "=", "json", ".", "load", "(", "input", ")", "\n", "filtered_dataset", "=", "[", "it", "for", "it", "in", "dataset", "if", "_extract_video_id", "(", "it", "[", "'url'", "]", ")", "in", "videos", "]", "\n", "with", "open", "(", "f'{_MSASL_FILTERED_SPECS_DIR}/{dataset_type.value}.json'", ",", "'w'", ",", "encoding", "=", "'utf-8'", ")", "as", "output", ":", "\n", "                ", "output", ".", "write", "(", "'['", "+", "',\\n'", ".", "join", "(", "json", ".", "dumps", "(", "it", ")", "for", "it", "in", "filtered_dataset", ")", "+", "']'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.msasl.tf_record._center_ratios": [[16, 31], ["None"], "function", ["None"], ["def", "_dataset_type_dir", "(", "signer", ")", ":", "\n", "    ", "\"\"\"Returns the directory name of the corresponding dataset type.\n\n    There is a `TFRecord` file written for each of the 25 signers. The `TFRecord` files of the first 17 signers are\n    assigned to the train dataset, the `TFRecord` files of the next 4 signers are assigned to the validation dataset,\n    and the `TFRecord` files of the last 4 signers are assigned to the test dataset.\n\n    Arguments:\n        signer: The index of the signer.\n\n    Returns:\n        The directory name of the corresponding dataset type.\n    \"\"\"", "\n", "if", "signer", ">", "20", ":", "\n", "        ", "return", "DatasetType", ".", "TEST", ".", "value", "\n", "", "elif", "signer", ">", "16", ":", "\n"]], "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.msasl.tf_record._read_frames": [[33, 58], ["cv2.VideoCapture", "datasets.utils._frame_positions", "cv2.VideoCapture.release", "cv2.VideoCapture.set", "cv2.VideoCapture.read", "tf_record._center_ratios", "datasets.utils._crop_image_to_square", "cv2.imencode", "frames.append", "datasets.msasl.video_download._extract_video_id", "cv2.resize", "buffer.tobytes"], "function", ["home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.datasets.utils._frame_positions", "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.msasl.tf_record._center_ratios", "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.datasets.utils._crop_image_to_square", "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.msasl.video_download._extract_video_id"], ["", "else", ":", "\n", "        ", "return", "DatasetType", ".", "TRAIN", ".", "value", "\n", "\n", "\n", "", "", "def", "_read_frames", "(", "example_path", ")", ":", "\n", "    ", "\"\"\"Reads the individual frames out of the list of images of the dataset example.\n\n    A frame is encoded as a compressed JPEG image instead of an ndarray, as the latter consumes up to 10 times as much\n    storage space.\n\n    Arguments:\n        example_path: The path to the dataset example.\n\n    Returns:\n        The list of frames of the dataset example.\n    \"\"\"", "\n", "positions", "=", "_frame_positions", "(", "0", ",", "_N_FRAMES_PER_EXAMPLE", "-", "1", ")", "\n", "file_names", "=", "itemgetter", "(", "*", "positions", ")", "(", "os", ".", "listdir", "(", "example_path", ")", ")", "\n", "frames", "=", "[", "]", "\n", "for", "file_name", "in", "file_names", ":", "\n", "        ", "frame", "=", "cv2", ".", "imread", "(", "f'{example_path}/{file_name}'", ")", "\n", "cropped_frame", "=", "_crop_image_to_square", "(", "frame", ")", "\n", "_", ",", "buffer", "=", "cv2", ".", "imencode", "(", "'.jpg'", ",", "cv2", ".", "resize", "(", "cropped_frame", ",", "_FRAME_SIZE", ")", ")", "\n", "frames", ".", "append", "(", "buffer", ".", "tobytes", "(", ")", ")", "\n", "", "return", "frames", "\n", "\n"]], "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.msasl.tf_record._serialize_example": [[60, 77], ["tf_record._read_frames", "tensorflow.train.Example", "tf.train.Example.SerializeToString", "datasets.tf_record_utils._bytes_feature", "datasets.tf_record_utils._int64_feature", "datasets.tf_record_utils._int64_feature", "tensorflow.train.Features"], "function", ["home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.msasl.tf_record._read_frames", "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.datasets.tf_record_utils._bytes_feature", "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.datasets.tf_record_utils._int64_feature", "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.datasets.tf_record_utils._int64_feature"], ["", "def", "_serialize_example", "(", "example_path", ",", "label", ",", "signer", ")", ":", "\n", "    ", "\"\"\"Serializes a dataset example in the `TFRecord` format.\n\n    Arguments:\n        example_path: The path to the dataset example.\n        label: The index of the label.\n        signer: The index of the signer.\n\n    Returns:\n        The binary string representation of the `TFRecord`.\n    \"\"\"", "\n", "frames", "=", "_read_frames", "(", "example_path", ")", "\n", "feature", "=", "{", "\n", "'frames'", ":", "_bytes_feature", "(", "frames", ")", ",", "\n", "'label'", ":", "_int64_feature", "(", "[", "label", "]", ")", ",", "\n", "'signer'", ":", "_int64_feature", "(", "[", "signer", "]", ")", "\n", "}", "\n", "tf_example", "=", "tf", ".", "train", ".", "Example", "(", "features", "=", "tf", ".", "train", ".", "Features", "(", "feature", "=", "feature", ")", ")", "\n"]], "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.msasl.tf_record.write_tf_records": [[79, 109], ["pathlib.Path().mkdir", "enumerate", "open", "json.load", "tf_record._serialize_example", "tf.io.TFRecordWriter.write", "tf.io.TFRecordWriter.close", "logging.info", "pathlib.Path", "tensorflow.io.TFRecordWriter", "tf.io.TFRecordWriter.close", "logging.info"], "function", ["home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.msasl.tf_record._serialize_example"], ["\n", "\n", "", "def", "write_tf_records", "(", ")", ":", "\n", "    ", "\"\"\"Creates the `TFRecord` files of the `SIGNUM` train, validation and test datasets.\n\n    The `TFRecord` files of the train, validation and test datasets are saved into corresponding subdirectories inside\n    the `_SIGNUM_TF_RECORDS_DIR` directory. There is one `TFRecord` file for each signer.\n    \"\"\"", "\n", "signer_dirs", "=", "os", ".", "listdir", "(", "_SIGNUM_IMAGES_DIR", ")", "\n", "for", "signer", ",", "signer_dir", "in", "enumerate", "(", "signer_dirs", ")", ":", "\n", "        ", "path", "=", "f'{_SIGNUM_TF_RECORDS_DIR}/{_dataset_type_dir(signer)}'", "\n", "Path", "(", "path", ")", ".", "mkdir", "(", "exist_ok", "=", "True", ")", "\n", "file_name", "=", "f'{path}/signum_{signer + 1:02d}.tfrecord'", "\n", "with", "tf", ".", "io", ".", "TFRecordWriter", "(", "file_name", ")", "as", "writer", ":", "\n", "            ", "running_record_number", "=", "0", "\n", "example_dirs", "=", "os", ".", "listdir", "(", "f'{_SIGNUM_IMAGES_DIR}/{signer_dir}'", ")", "\n", "for", "label", ",", "example_dir", "in", "enumerate", "(", "example_dirs", ")", ":", "\n", "                ", "example_path", "=", "f'{_SIGNUM_IMAGES_DIR}/{signer_dir}/{example_dir}'", "\n", "serialized_example", "=", "_serialize_example", "(", "example_path", ",", "label", ",", "signer", ")", "\n", "writer", ".", "write", "(", "serialized_example", ")", "\n", "running_record_number", "+=", "1", "\n", "", "logging", ".", "info", "(", "f'{running_record_number} records have been written to {file_name}.'", ")", "\n", "\n", "\n", "", "", "", "if", "__name__", "==", "'__main__'", ":", "\n", "    ", "logging", ".", "basicConfig", "(", "level", "=", "logging", ".", "INFO", ")", "\n", "write_tf_records", "(", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.testing.utils._create_testing_dir": [[10, 22], ["path.mkdir"], "function", ["None"], ["\n", "range_", "=", "end", "-", "start", "+", "1", "\n"]], "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.testing.utils.scores_file_path": [[24, 34], ["None"], "function", ["None"], ["positions", ".", "sort", "(", ")", "\n", "return", "positions", "\n", "\n", "\n", "", "def", "_crop_image_to_square", "(", "image", ",", "center_x_ratio", "=", "0.5", ",", "center_y_ratio", "=", "0.5", ")", ":", "\n", "    "]], "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.testing.utils.scores_file_exists": [[36, 47], ["utils.scores_file_path", "scores_file_path.is_file"], "function", ["home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.testing.utils.scores_file_path"], ["\n", "height", "=", "image", ".", "shape", "[", "0", "]", "\n", "width", "=", "image", ".", "shape", "[", "1", "]", "\n", "if", "width", ">", "height", ":", "\n"]], "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.testing.utils.save_scores": [[49, 63], ["pandas.DataFrame", "utils._create_testing_dir", "core.utils.save_dataframe"], "function", ["home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.testing.utils._create_testing_dir", "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.core.utils.save_dataframe"], ["half_height", "=", "int", "(", "height", "/", "2", ")", "\n", "if", "center_x_ratio", "<=", "0.5", ":", "\n", "            ", "x", "=", "max", "(", "0", ",", "mid_x", "-", "half_height", ")", "\n", "return", "image", "[", ":", ",", "x", ":", "x", "+", "height", "]", "\n", "", "else", ":", "\n", "            ", "x", "=", "min", "(", "width", ",", "mid_x", "+", "half_height", ")", "\n", "return", "image", "[", ":", ",", "x", "-", "height", ":", "x", "]", "\n", "", "", "elif", "height", ">", "width", ":", "\n", "        ", "mid_y", "=", "int", "(", "height", "*", "center_y_ratio", ")", "\n", "half_width", "=", "int", "(", "width", "/", "2", ")", "\n", "if", "center_y_ratio", "<=", "0.5", ":", "\n", "            ", "y", "=", "max", "(", "0", ",", "mid_y", "-", "half_width", ")", "\n", "return", "image", "[", "y", ":", "y", "+", "width", "]", "\n", "", "else", ":", "\n", "            ", "y", "=", "min", "(", "height", ",", "mid_y", "+", "half_width", ")", "\n"]], "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.testing.utils.display_scores": [[65, 74], ["utils.scores_file_path", "core.utils.load_dataframe", "print"], "function", ["home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.testing.utils.scores_file_path", "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.core.utils.load_dataframe"], ["", "", "else", ":", "\n", "        ", "return", "image", "\n", "\n", "\n", "", "", "_TF_RECORDS_DIR_DICT", "=", "{", "\n", "DatasetName", ".", "MSASL", ":", "_MSASL_TF_RECORDS_DIR", ",", "\n", "DatasetName", ".", "SIGNUM", ":", "_SIGNUM_TF_RECORDS_DIR", "\n", "}", "\n", "\n", "\n"]], "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.plotting.plot.loss_accuracy_plot": [[14, 28], ["plotting.utils._start_index_from_one", "matplotlib.subplots", "range", "fig.show", "seaborn.lineplot", "axes[].set", "axes[].legend"], "function", ["home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.plotting.utils._start_index_from_one"], ["def", "loss_accuracy_plot", "(", "df", ":", "DataFrame", ")", ":", "\n", "    ", "\"\"\"Plots the learning curves of the losses and accuracies for each of the train and validation datasets.\n\n    Arguments:\n        df: The DataFrame of a history file.\n    \"\"\"", "\n", "_start_index_from_one", "(", "df", ")", "\n", "fig", ",", "axes", "=", "plt", ".", "subplots", "(", "nrows", "=", "1", ",", "ncols", "=", "2", ",", "figsize", "=", "(", "12.8", ",", "4.8", ")", ")", "\n", "for", "i", "in", "range", "(", "2", ")", ":", "\n", "        ", "sns", ".", "lineplot", "(", "data", "=", "df", "[", "_LOSS_ACCURACY_COLUMNS", "[", "i", "]", "]", ",", "markers", "=", "[", "'o'", ",", "'o'", "]", ",", "ax", "=", "axes", "[", "i", "]", ")", "\n", "axes", "[", "i", "]", ".", "set", "(", "title", "=", "f\"{_LABELS['loss_accuracy'][i]} learning curves\"", ",", "xlabel", "=", "'Epoch'", ",", "\n", "ylabel", "=", "_LABELS", "[", "'loss_accuracy'", "]", "[", "i", "]", ")", "\n", "axes", "[", "i", "]", ".", "legend", "(", "title", "=", "'Dataset'", ",", "labels", "=", "[", "'Train'", ",", "'Validation'", "]", ")", "\n", "", "fig", ".", "show", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.plotting.plot.loss_accuracy_without_validation_plot": [[30, 44], ["plotting.utils._start_index_from_one", "matplotlib.subplots", "range", "fig.show", "seaborn.lineplot", "axes[].set", "axes[].legend"], "function", ["home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.plotting.utils._start_index_from_one"], ["", "def", "loss_accuracy_without_validation_plot", "(", "df", ":", "DataFrame", ")", ":", "\n", "    ", "\"\"\"Plots the learning curves of the loss and accuracy for the train dataset.\n\n    Arguments:\n        df: The DataFrame of a history file.\n    \"\"\"", "\n", "_start_index_from_one", "(", "df", ")", "\n", "fig", ",", "axes", "=", "plt", ".", "subplots", "(", "nrows", "=", "1", ",", "ncols", "=", "2", ",", "figsize", "=", "(", "12.8", ",", "4.8", ")", ")", "\n", "for", "i", "in", "range", "(", "2", ")", ":", "\n", "        ", "sns", ".", "lineplot", "(", "data", "=", "df", "[", "_LOSS_ACCURACY_COLUMNS", "[", "i", "]", "[", "0", "]", "]", ",", "marker", "=", "'o'", ",", "ax", "=", "axes", "[", "i", "]", ")", "\n", "axes", "[", "i", "]", ".", "set", "(", "xticks", "=", "df", ".", "index", ",", "title", "=", "f\"{_LABELS['loss_accuracy'][i]} learning curve\"", ",", "xlabel", "=", "'Epoch'", ",", "\n", "ylabel", "=", "_LABELS", "[", "'loss_accuracy'", "]", "[", "i", "]", ")", "\n", "axes", "[", "i", "]", ".", "legend", "(", "title", "=", "'Dataset'", ",", "labels", "=", "[", "'Train'", "]", ")", "\n", "", "fig", ".", "show", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.plotting.plot.compare_training_plot": [[46, 59], ["plotting.utils._start_index_from_one", "matplotlib.subplots", "range", "fig.show", "seaborn.lineplot", "axes[].set", "axes[].legend"], "function", ["home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.plotting.utils._start_index_from_one"], ["", "def", "compare_training_plot", "(", "df", ":", "DataFrame", ")", ":", "\n", "    ", "\"\"\"Plots the learning curves of a baseline and a fine-tuned model for each of the train and validation datasets.\n\n    Arguments:\n        df: The DataFrame of the merged history files of a baseline and a fine-tuned model.\n    \"\"\"", "\n", "_start_index_from_one", "(", "df", ")", "\n", "fig", ",", "axes", "=", "plt", ".", "subplots", "(", "nrows", "=", "1", ",", "ncols", "=", "2", ",", "figsize", "=", "(", "12.8", ",", "4.8", ")", ")", "\n", "for", "i", "in", "range", "(", "2", ")", ":", "\n", "        ", "sns", ".", "lineplot", "(", "data", "=", "df", "[", "COMPARE_TRAINING_COLUMNS", "[", "i", "]", "]", ",", "markers", "=", "[", "'o'", ",", "'o'", "]", ",", "ax", "=", "axes", "[", "i", "]", ")", "\n", "axes", "[", "i", "]", ".", "set", "(", "title", "=", "f\"{_LABELS['train_validation'][i]} dataset\"", ",", "xlabel", "=", "'Epoch'", ",", "ylabel", "=", "'Accuracy'", ")", "\n", "axes", "[", "i", "]", ".", "legend", "(", "title", "=", "'Model'", ",", "labels", "=", "[", "'Baseline'", ",", "'Fine-tuned'", "]", ")", "\n", "", "fig", ".", "show", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.plotting.plot.compare_test_scores_plot": [[61, 75], ["matplotlib.subplots", "range", "fig.show", "seaborn.lineplot", "axes[].set", "axes[].legend"], "function", ["None"], ["", "def", "compare_test_scores_plot", "(", "df", ":", "DataFrame", ")", ":", "\n", "    ", "\"\"\"Plots the losses and accuracies of a baseline and a fine-tuned model for each of the different dataset sizes.\n\n    Arguments:\n        df: The DataFrame of the merged test scores files of a baseline and a fine-tuned model.\n    \"\"\"", "\n", "df", ".", "index", "=", "_LABELS", "[", "'dataset_sizes'", "]", "\n", "fig", ",", "axes", "=", "plt", ".", "subplots", "(", "nrows", "=", "1", ",", "ncols", "=", "2", ",", "figsize", "=", "(", "12.8", ",", "4.8", ")", ")", "\n", "for", "i", "in", "range", "(", "2", ")", ":", "\n", "        ", "sns", ".", "lineplot", "(", "data", "=", "df", "[", "COMPARE_TEST_SCORES_COLUMNS", "[", "i", "]", "]", ",", "markers", "=", "[", "'o'", ",", "'o'", "]", ",", "ax", "=", "axes", "[", "i", "]", ")", "\n", "axes", "[", "i", "]", ".", "set", "(", "xticks", "=", "df", ".", "index", ",", "title", "=", "f\"{_LABELS['loss_accuracy'][i]} comparison\"", ",", "xlabel", "=", "'Dataset size'", ",", "\n", "ylabel", "=", "_LABELS", "[", "'loss_accuracy'", "]", "[", "i", "]", ")", "\n", "axes", "[", "i", "]", ".", "legend", "(", "title", "=", "'Model'", ",", "labels", "=", "[", "'Baseline'", ",", "'Fine-tuned'", "]", ")", "\n", "", "fig", ".", "show", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.plotting.plot.test_scores_improvement_plot": [[77, 91], ["matplotlib.subplots", "range", "fig.show", "seaborn.barplot", "axes[].set", "numpy.linspace", "numpy.linspace"], "function", ["None"], ["", "def", "test_scores_improvement_plot", "(", "df", ":", "DataFrame", ")", ":", "\n", "    ", "\"\"\"Plots the improvements of the losses and accuracies for each of the different dataset sizes.\n\n    Arguments:\n        df: The DataFrame of the merged test scores files of a baseline and a fine-tuned model.\n    \"\"\"", "\n", "df", ".", "columns", "=", "_LABELS", "[", "'dataset_sizes'", "]", "\n", "fig", ",", "axes", "=", "plt", ".", "subplots", "(", "nrows", "=", "1", ",", "ncols", "=", "2", ",", "figsize", "=", "(", "12.8", ",", "4.8", ")", ")", "\n", "for", "i", "in", "range", "(", "2", ")", ":", "\n", "        ", "sns", ".", "barplot", "(", "data", "=", "df", ".", "iloc", "[", "[", "i", "]", "]", ",", "ax", "=", "axes", "[", "i", "]", ")", "\n", "yticks", "=", "np", ".", "linspace", "(", "-", "1.2", ",", "0", ",", "7", ")", "if", "i", "==", "0", "else", "np", ".", "linspace", "(", "0", ",", "0.22", ",", "12", ")", "\n", "axes", "[", "i", "]", ".", "set", "(", "yticks", "=", "yticks", ",", "title", "=", "f\"{_LABELS['loss_accuracy'][i]} improvement\"", ",", "xlabel", "=", "'Dataset size'", ",", "\n", "ylabel", "=", "_LABELS", "[", "'loss_accuracy'", "]", "[", "i", "]", ")", "\n", "", "fig", ".", "show", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.plotting.plot.dataset_size_learning_curves_plot": [[93, 106], ["seaborn.lineplot", "reversed", "numpy.linspace", "sns.lineplot.set", "sns.lineplot.legend", "matplotlib.show"], "function", ["None"], ["", "def", "dataset_size_learning_curves_plot", "(", "df", ":", "DataFrame", ")", ":", "\n", "    ", "\"\"\"Plots the train and validation accuracies of the baseline models for each of the different dataset sizes.\n\n    Arguments:\n        df: The DataFrame holding the data of the train and validation accuracies for the different dataset sizes.\n    \"\"\"", "\n", "ax", "=", "sns", ".", "lineplot", "(", "data", "=", "df", ",", "markers", "=", "[", "'o'", ",", "'o'", "]", ")", "\n", "xticklabels", "=", "reversed", "(", "_LABELS", "[", "'dataset_sizes'", "]", ")", "\n", "yticks", "=", "np", ".", "linspace", "(", "0", ",", "1", ",", "11", ")", "\n", "ax", ".", "set", "(", "xticks", "=", "df", ".", "index", ",", "xticklabels", "=", "xticklabels", ",", "yticks", "=", "yticks", ",", "title", "=", "'Dataset size learning curves'", ",", "\n", "xlabel", "=", "'Dataset size'", ",", "ylabel", "=", "'Accuracy'", ")", "\n", "ax", ".", "legend", "(", "title", "=", "'Dataset'", ",", "labels", "=", "[", "'Train'", ",", "'Validation'", "]", ",", "loc", "=", "'lower right'", ")", "\n", "plt", ".", "show", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.plotting.plot.idealised_learning_curves_plot": [[108, 120], ["plotting.utils._start_index_from_one", "seaborn.lineplot", "numpy.linspace", "sns.lineplot.set", "sns.lineplot.legend", "matplotlib.show"], "function", ["home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.plotting.utils._start_index_from_one"], ["", "def", "idealised_learning_curves_plot", "(", "df", ":", "DataFrame", ")", ":", "\n", "    ", "\"\"\"Plots the idealised learning curves of the train and validation losses.\n\n    Arguments:\n        df: The DataFrame holding the data of the train and validation losses of an idealised case.\n    \"\"\"", "\n", "_start_index_from_one", "(", "df", ")", "\n", "ax", "=", "sns", ".", "lineplot", "(", "data", "=", "df", ",", "markers", "=", "[", "'o'", ",", "'o'", "]", ")", "\n", "yticks", "=", "np", ".", "linspace", "(", "0", ",", "5", ",", "6", ")", "\n", "ax", ".", "set", "(", "yticks", "=", "yticks", ",", "xlabel", "=", "'Epoch'", ",", "ylabel", "=", "'Loss'", ")", "\n", "ax", ".", "legend", "(", "title", "=", "'Dataset'", ",", "labels", "=", "[", "'Train'", ",", "'Validation'", "]", ")", "\n", "plt", ".", "show", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.plotting.plot.feature_maps_plot": [[122, 141], ["math.ceil", "range", "range", "matplotlib.savefig", "matplotlib.clf", "matplotlib.subplot", "plt.subplot.set_xticks", "plt.subplot.set_yticks", "matplotlib.imshow"], "function", ["None"], ["", "def", "feature_maps_plot", "(", "feature_maps", ",", "path", ",", "n_cols", "=", "5", ")", ":", "\n", "    ", "\"\"\"Saves the plots of the feature maps to disk.\n\n    Arguments:\n        feature_maps: The list of feature maps to plot and save.\n        path: A Path object pointing to the directory where the plots will be saved.\n        n_cols: The number of frames plotted in one row (defaults to 5).\n    \"\"\"", "\n", "n_time_steps", "=", "feature_maps", ".", "shape", "[", "1", "]", "\n", "n_filters", "=", "feature_maps", ".", "shape", "[", "-", "1", "]", "\n", "n_rows", "=", "math", ".", "ceil", "(", "n_time_steps", "/", "n_cols", ")", "\n", "for", "n", "in", "range", "(", "n_filters", ")", ":", "\n", "        ", "for", "t", "in", "range", "(", "n_time_steps", ")", ":", "\n", "            ", "ax", "=", "plt", ".", "subplot", "(", "n_rows", ",", "n_cols", ",", "t", "+", "1", ")", "\n", "ax", ".", "set_xticks", "(", "[", "]", ")", "\n", "ax", ".", "set_yticks", "(", "[", "]", ")", "\n", "plt", ".", "imshow", "(", "feature_maps", "[", "0", ",", "t", ",", ":", ",", ":", ",", "n", "]", ",", "cmap", "=", "'gray'", ")", "\n", "", "plt", ".", "savefig", "(", "path", "/", "f'feature_map_{n:03d}.png'", ")", "\n", "plt", ".", "clf", "(", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.plotting.utils._start_index_from_one": [[4, 11], ["None"], "function", ["None"], ["from", "datasets", ".", "msasl", ".", "constants", "import", "_MSASL_TF_RECORDS_DIR", "\n", "from", "datasets", ".", "signum", ".", "constants", "import", "_SIGNUM_TF_RECORDS_DIR", "\n", "\n", "\n", "def", "_frame_positions", "(", "start", ":", "int", ",", "end", ":", "int", ")", ":", "\n", "    "]], "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.training.callbacks.ThresholdStopping.__init__": [[18, 28], ["tensorflow.keras.callbacks.Callback.__init__"], "methods", ["home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.training.callbacks.ThresholdStopping.__init__"], ["def", "__init__", "(", "self", ",", "metric", ":", "Metric", ",", "threshold", ")", ":", "\n", "        ", "\"\"\"Initializes this callback.\n\n        Arguments:\n            metric: A Metric value to check for the threshold.\n            threshold: A float value the metric is checked for.\n        \"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "_metric", "=", "metric", "\n", "self", ".", "_threshold", "=", "threshold", "\n", "\n"]], "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.training.callbacks.ThresholdStopping.on_epoch_end": [[29, 45], ["logs.get"], "methods", ["None"], ["", "def", "on_epoch_end", "(", "self", ",", "epoch", ",", "logs", "=", "{", "}", ")", ":", "\n", "        ", "\"\"\"Called at the end of a training epoch.\n\n        Stops training when a metric exceeds or falls below a threshold.\n\n        Arguments:\n            epoch: The index of the training epoch.\n            logs: A dictionary of the metric results for this training epoch.\n        \"\"\"", "\n", "value", "=", "logs", ".", "get", "(", "self", ".", "_metric", ".", "value", ")", "\n", "if", "self", ".", "_metric", "==", "Metric", ".", "ACCURACY", ":", "\n", "            ", "if", "value", ">", "self", ".", "_threshold", ":", "\n", "                ", "self", ".", "model", ".", "stop_training", "=", "True", "\n", "", "", "elif", "self", ".", "_metric", "==", "Metric", ".", "LOSS", ":", "\n", "            ", "if", "value", "<", "self", ".", "_threshold", ":", "\n", "                ", "self", ".", "model", ".", "stop_training", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.training.callbacks.early_stopping": [[47, 58], ["tensorflow.python.keras.callbacks.EarlyStopping"], "function", ["None"], ["", "", "", "", "def", "early_stopping", "(", "metric", ":", "Metric", ",", "patience", ")", ":", "\n", "    ", "\"\"\"Returns an EarlyStopping callback.\n\n    Arguments:\n        metric: A Metric value the model is evaluated against.\n        patience: The number of epochs with no improvement after which the training will be stopped.\n\n    Returns:\n        An EarlyStopping callback.\n    \"\"\"", "\n", "return", "EarlyStopping", "(", "monitor", "=", "metric", ".", "value", ",", "patience", "=", "patience", ",", "verbose", "=", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.training.callbacks.model_checkpoint": [[60, 72], ["str", "tensorflow.python.keras.callbacks.ModelCheckpoint"], "function", ["None"], ["", "def", "model_checkpoint", "(", "metric", ":", "Metric", ",", "path", ":", "Path", ")", ":", "\n", "    ", "\"\"\"Returns a ModelCheckpoint callback.\n\n    Arguments:\n        metric: A Metric value the model is evaluated against.\n        path: A Path object pointing to the directory where the model checkpoint will be saved.\n\n    Returns:\n        A ModelCheckpoint callback.\n    \"\"\"", "\n", "filepath", "=", "str", "(", "path", "/", "_SAVED_MODEL_DIR", ")", "\n", "return", "ModelCheckpoint", "(", "filepath", ",", "monitor", "=", "metric", ".", "value", ",", "verbose", "=", "1", ",", "save_best_only", "=", "True", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.training.utils.create_training_runs_dir": [[13, 26], ["datetime.datetime.now().strftime", "path.mkdir", "datetime.datetime.now"], "function", ["None"], ["\n", "range_", "=", "end", "-", "start", "+", "1", "\n", "positions", "=", "np", ".", "random", ".", "choice", "(", "range_", ",", "_N_TIME_STEPS", ",", "replace", "=", "range_", "<", "_N_TIME_STEPS", ")", "+", "start", "\n", "positions", ".", "sort", "(", ")", "\n", "return", "positions", "\n", "\n"]], "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.training.utils.history_path": [[28, 39], ["None"], "function", ["None"], ["", "def", "_crop_image_to_square", "(", "image", ",", "center_x_ratio", "=", "0.5", ",", "center_y_ratio", "=", "0.5", ")", ":", "\n", "    "]], "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.training.utils.save_history": [[41, 49], ["core.utils.save_dataframe", "pandas.DataFrame"], "function", ["home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.core.utils.save_dataframe"], ["\n", "height", "=", "image", ".", "shape", "[", "0", "]", "\n", "width", "=", "image", ".", "shape", "[", "1", "]", "\n", "if", "width", ">", "height", ":", "\n", "        ", "mid_x", "=", "int", "(", "width", "*", "center_x_ratio", ")", "\n", "half_height", "=", "int", "(", "height", "/", "2", ")", "\n"]], "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.training.utils.model_path": [[51, 62], ["None"], "function", ["None"], ["            ", "x", "=", "max", "(", "0", ",", "mid_x", "-", "half_height", ")", "\n", "return", "image", "[", ":", ",", "x", ":", "x", "+", "height", "]", "\n", "", "else", ":", "\n", "            ", "x", "=", "min", "(", "width", ",", "mid_x", "+", "half_height", ")", "\n", "return", "image", "[", ":", ",", "x", "-", "height", ":", "x", "]", "\n", "", "", "elif", "height", ">", "width", ":", "\n", "        ", "mid_y", "=", "int", "(", "height", "*", "center_y_ratio", ")", "\n", "half_width", "=", "int", "(", "width", "/", "2", ")", "\n", "if", "center_y_ratio", "<=", "0.5", ":", "\n", "            ", "y", "=", "max", "(", "0", ",", "mid_y", "-", "half_width", ")", "\n", "return", "image", "[", "y", ":", "y", "+", "width", "]", "\n", "", "else", ":", "\n"]], "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.training.utils.save_model": [[64, 75], ["str", "logging.basicConfig", "logging.info", "model.save"], "function", ["None"], ["return", "image", "[", "y", "-", "width", ":", "y", "]", "\n", "", "", "else", ":", "\n", "        ", "return", "image", "\n", "\n", "\n", "", "", "_TF_RECORDS_DIR_DICT", "=", "{", "\n", "DatasetName", ".", "MSASL", ":", "_MSASL_TF_RECORDS_DIR", ",", "\n", "DatasetName", ".", "SIGNUM", ":", "_SIGNUM_TF_RECORDS_DIR", "\n", "}", "\n", "\n", "\n", "def", "_tf_records_dir", "(", "dataset_name", ":", "DatasetName", ")", ":", "\n"]], "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.training.utils.load_model": [[77, 87], ["tensorflow.keras.models.load_model"], "function", ["home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.training.utils.load_model"], ["\n", "return", "_TF_RECORDS_DIR_DICT", "[", "dataset_name", "]", "\n", "\n", "\n", "", "def", "_display_dataset_counts", "(", "dataset_name", ":", "DatasetName", ",", "counts", ")", ":", "\n"]], "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.baseline_model.feature_maps._dataset": [[13, 28], ["datasets.tf_record_utils.tf_record_dataset", "dataset.batch.batch", "dataset.batch.map", "dataset.batch.unbatch", "dataset.batch.filter", "dataset.batch.batch", "dataset.batch.take", "tensorflow.math.equal", "tensorflow.math.equal"], "function", ["home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.datasets.tf_record_utils.tf_record_dataset"], ["def", "_dataset", "(", ")", ":", "\n", "    ", "\"\"\"Returns the `SIGNUM` dataset example for the word `dance`.\n\n    The dataset example corresponding to the label with index 420 and the signer with index 1 shall be returned.\n\n    Returns:\n        The dataset example for the word `dance`.\n    \"\"\"", "\n", "dataset", "=", "tf_record_dataset", "(", "DatasetName", ".", "SIGNUM", ",", "DatasetType", ".", "TRAIN", ")", "\n", "dataset", "=", "dataset", ".", "batch", "(", "1", ")", "\n", "dataset", "=", "dataset", ".", "map", "(", "transform_for_prediction", ")", "\n", "dataset", "=", "dataset", ".", "unbatch", "(", ")", "\n", "dataset", "=", "dataset", ".", "filter", "(", "lambda", "frames", ",", "label", ",", "signer", ":", "tf", ".", "math", ".", "equal", "(", "label", ",", "420", ")", "and", "tf", ".", "math", ".", "equal", "(", "signer", ",", "1", ")", ")", "\n", "dataset", "=", "dataset", ".", "batch", "(", "1", ")", "\n", "return", "dataset", ".", "take", "(", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.baseline_model.feature_maps._model": [[30, 34], ["training.utils.model_path", "training.utils.load_model", "tensorflow.keras.models.Model", "pathlib.Path"], "function", ["home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.training.utils.model_path", "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.training.utils.load_model"], ["", "def", "_model", "(", ")", ":", "\n", "    ", "path", "=", "model_path", "(", "Path", "(", ")", ",", "TRAINING_RUNS", "[", "_TRAINING_RUNS_INDEX", "]", ")", "\n", "base_model", "=", "load_model", "(", "path", ")", "\n", "return", "Model", "(", "inputs", "=", "base_model", ".", "inputs", ",", "outputs", "=", "[", "base_model", ".", "layers", "[", "i", "]", ".", "output", "for", "i", "in", "_LAYER_INDICES", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.baseline_model.feature_maps.plot_feature_maps": [[40, 48], ["feature_maps._dataset", "feature_maps._model", "_model.predict", "enumerate", "pathlib.Path().mkdir", "plotting.plot.feature_maps_plot", "pathlib.Path", "pathlib.Path"], "function", ["home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.finetuned_model.feature_maps._dataset", "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.finetuned_model.train._model", "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.plotting.plot.feature_maps_plot"], ["def", "plot_feature_maps", "(", ")", ":", "\n", "    ", "dataset", "=", "_dataset", "(", ")", "\n", "model", "=", "_model", "(", ")", "\n", "feature_maps", "=", "model", ".", "predict", "(", "dataset", ")", "\n", "for", "i", ",", "layer_index", "in", "enumerate", "(", "_LAYER_INDICES", ")", ":", "\n", "        ", "path", "=", "Path", "(", ")", "/", "'feature_maps'", "/", "f'layer_{layer_index:03d}'", "\n", "Path", "(", "path", ")", ".", "mkdir", "(", "parents", "=", "True", ",", "exist_ok", "=", "True", ")", "\n", "feature_maps_plot", "(", "feature_maps", "[", "i", "]", ",", "path", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.baseline_model.train._train_dataset": [[14, 20], ["datasets.tf_record_utils.tf_record_dataset", "train_dataset.map.shuffle", "train_dataset.map.batch", "train_dataset.map.map", "train_dataset.map.prefetch"], "function", ["home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.datasets.tf_record_utils.tf_record_dataset"], ["def", "_train_dataset", "(", ")", ":", "\n", "    ", "train_dataset", "=", "tf_record_dataset", "(", "DatasetName", ".", "SIGNUM", ",", "DatasetType", ".", "TRAIN", ")", "\n", "train_dataset", "=", "train_dataset", ".", "shuffle", "(", "2048", ")", "\n", "train_dataset", "=", "train_dataset", ".", "batch", "(", "32", ")", "\n", "train_dataset", "=", "train_dataset", ".", "map", "(", "transform_for_signum_model", ")", "\n", "return", "train_dataset", ".", "prefetch", "(", "2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.baseline_model.train._validation_dataset": [[22, 27], ["datasets.tf_record_utils.tf_record_dataset", "validation_dataset.map.batch", "validation_dataset.map.map", "validation_dataset.map.prefetch"], "function", ["home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.datasets.tf_record_utils.tf_record_dataset"], ["", "def", "_validation_dataset", "(", ")", ":", "\n", "    ", "validation_dataset", "=", "tf_record_dataset", "(", "DatasetName", ".", "SIGNUM", ",", "DatasetType", ".", "VALIDATION", ")", "\n", "validation_dataset", "=", "validation_dataset", ".", "batch", "(", "32", ")", "\n", "validation_dataset", "=", "validation_dataset", ".", "map", "(", "transform_for_signum_model", ")", "\n", "return", "validation_dataset", ".", "prefetch", "(", "2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.baseline_model.train._model": [[29, 35], ["base_model.inflated_3d_inception_v3.Inflated3DInceptionV3", "base_model.inflated_3d_inception_v3.load_inflated_imagenet_weights", "base_model.inflated_3d_inception_v3.Inflated3DInceptionV3.compile", "tensorflow.keras.optimizers.SGD"], "function", ["home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.base_model.inflated_3d_inception_v3.Inflated3DInceptionV3", "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.base_model.inflated_3d_inception_v3.load_inflated_imagenet_weights"], ["", "def", "_model", "(", ")", ":", "\n", "    ", "model", "=", "Inflated3DInceptionV3", "(", "classes", "=", "N_CLASSES", ")", "\n", "load_inflated_imagenet_weights", "(", "model", ")", "\n", "model", ".", "compile", "(", "optimizer", "=", "SGD", "(", "learning_rate", "=", "0.01", ",", "momentum", "=", "0.9", ")", ",", "loss", "=", "'categorical_crossentropy'", ",", "\n", "metrics", "=", "[", "'accuracy'", "]", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.baseline_model.train.train": [[37, 44], ["train._train_dataset", "train._validation_dataset", "training.callbacks.model_checkpoint", "train._model", "_model.fit"], "function", ["home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.finetuned_model.train._train_dataset", "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.finetuned_model.train._validation_dataset", "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.training.callbacks.model_checkpoint", "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.finetuned_model.train._model"], ["", "def", "train", "(", "path", ":", "Path", ")", ":", "\n", "    ", "train_dataset", "=", "_train_dataset", "(", ")", "\n", "validation_dataset", "=", "_validation_dataset", "(", ")", "\n", "mc", "=", "model_checkpoint", "(", "Metric", ".", "VAL_LOSS", ",", "path", ")", "\n", "model", "=", "_model", "(", ")", "\n", "history", "=", "model", ".", "fit", "(", "train_dataset", ",", "validation_data", "=", "validation_dataset", ",", "epochs", "=", "40", ",", "callbacks", "=", "[", "mc", "]", ")", "\n", "return", "history", ".", "history", "\n", "\n"]], "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.baseline_model.test._test_dataset": [[10, 15], ["datasets.tf_record_utils.tf_record_dataset", "test_dataset.map.batch", "test_dataset.map.map", "test_dataset.map.prefetch"], "function", ["home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.datasets.tf_record_utils.tf_record_dataset"], ["def", "_test_dataset", "(", ")", ":", "\n", "    ", "test_dataset", "=", "tf_record_dataset", "(", "DatasetName", ".", "SIGNUM", ",", "DatasetType", ".", "TEST", ")", "\n", "test_dataset", "=", "test_dataset", ".", "batch", "(", "32", ")", "\n", "test_dataset", "=", "test_dataset", ".", "map", "(", "transform_for_signum_model", ")", "\n", "return", "test_dataset", ".", "prefetch", "(", "2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.baseline_model.test.test": [[17, 27], ["test._test_dataset", "training.utils.model_path", "training.utils.load_model", "training.utils.load_model.evaluate", "losses.append", "accuracies.append", "pathlib.Path"], "function", ["home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.finetuned_model.test._test_dataset", "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.training.utils.model_path", "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.training.utils.load_model"], ["", "def", "test", "(", ")", ":", "\n", "    ", "losses", ",", "accuracies", "=", "[", "]", ",", "[", "]", "\n", "test_dataset", "=", "_test_dataset", "(", ")", "\n", "for", "training_run", "in", "TRAINING_RUNS", ":", "\n", "        ", "path", "=", "model_path", "(", "Path", "(", ")", ",", "training_run", ")", "\n", "model", "=", "load_model", "(", "path", ")", "\n", "loss", ",", "accuracy", "=", "model", ".", "evaluate", "(", "test_dataset", ")", "\n", "losses", ".", "append", "(", "loss", ")", "\n", "accuracies", ".", "append", "(", "accuracy", ")", "\n", "", "return", "losses", ",", "accuracies", "\n", "\n"]], "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.baseline_model.learning_curves.plot_learning_curves": [[9, 14], ["training.utils.history_path", "core.utils.load_dataframe", "plotting.plot.loss_accuracy_plot", "pathlib.Path"], "function", ["home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.training.utils.history_path", "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.core.utils.load_dataframe", "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.plotting.plot.loss_accuracy_plot"], ["def", "plot_learning_curves", "(", ")", ":", "\n", "    ", "for", "training_run", "in", "TRAINING_RUNS", ":", "\n", "        ", "path", "=", "history_path", "(", "Path", "(", ")", ",", "training_run", ")", "\n", "df", "=", "load_dataframe", "(", "path", ")", "\n", "loss_accuracy_plot", "(", "df", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.base_model.feature_maps._dataset": [[14, 29], ["datasets.tf_record_utils.tf_record_dataset", "dataset.batch.batch", "dataset.batch.map", "dataset.batch.unbatch", "dataset.batch.filter", "dataset.batch.batch", "dataset.batch.take", "tensorflow.math.equal", "tensorflow.math.equal"], "function", ["home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.datasets.tf_record_utils.tf_record_dataset"], ["    ", "\"\"\"Returns the `SIGNUM` dataset example for the word `dance`.\n\n    The dataset example corresponding to the label with index 420 and the signer with index 1 shall be returned.\n\n    Returns:\n        The dataset example for the word `dance`.\n    \"\"\"", "\n", "dataset", "=", "tf_record_dataset", "(", "DatasetName", ".", "SIGNUM", ",", "DatasetType", ".", "TRAIN", ")", "\n", "dataset", "=", "dataset", ".", "batch", "(", "1", ")", "\n", "dataset", "=", "dataset", ".", "map", "(", "transform_for_prediction", ")", "\n", "dataset", "=", "dataset", ".", "unbatch", "(", ")", "\n", "dataset", "=", "dataset", ".", "filter", "(", "lambda", "frames", ",", "label", ",", "signer", ":", "tf", ".", "math", ".", "equal", "(", "label", ",", "420", ")", "and", "tf", ".", "math", ".", "equal", "(", "signer", ",", "1", ")", ")", "\n", "dataset", "=", "dataset", ".", "batch", "(", "1", ")", "\n", "return", "dataset", ".", "take", "(", "1", ")", "\n", "\n", "\n"]], "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.base_model.feature_maps._model": [[31, 36], ["base_model.inflated_3d_inception_v3.Inflated3DInceptionV3", "base_model.inflated_3d_inception_v3.load_inflated_imagenet_weights", "base_model.inflated_3d_inception_v3.Inflated3DInceptionV3.compile", "tensorflow.keras.models.Model", "tensorflow.keras.optimizers.SGD"], "function", ["home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.base_model.inflated_3d_inception_v3.Inflated3DInceptionV3", "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.base_model.inflated_3d_inception_v3.load_inflated_imagenet_weights"], ["    ", "path", "=", "model_path", "(", "Path", "(", ")", ",", "TRAINING_RUNS", "[", "_TRAINING_RUNS_INDEX", "]", ")", "\n", "base_model", "=", "load_model", "(", "path", ")", "\n", "return", "Model", "(", "inputs", "=", "base_model", ".", "inputs", ",", "outputs", "=", "[", "base_model", ".", "layers", "[", "i", "]", ".", "output", "for", "i", "in", "_LAYER_INDICES", "]", ")", "\n", "\n", "\n", "", "_TRAINING_RUNS_INDEX", "=", "2", "\n"]], "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.base_model.feature_maps.plot_feature_maps": [[41, 49], ["feature_maps._dataset", "feature_maps._model", "_model.predict", "enumerate", "pathlib.Path().mkdir", "plotting.plot.feature_maps_plot", "pathlib.Path", "pathlib.Path"], "function", ["home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.finetuned_model.feature_maps._dataset", "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.finetuned_model.train._model", "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.plotting.plot.feature_maps_plot"], ["    ", "dataset", "=", "_dataset", "(", ")", "\n", "model", "=", "_model", "(", ")", "\n", "feature_maps", "=", "model", ".", "predict", "(", "dataset", ")", "\n", "for", "i", ",", "layer_index", "in", "enumerate", "(", "_LAYER_INDICES", ")", ":", "\n", "        ", "path", "=", "Path", "(", ")", "/", "'feature_maps'", "/", "f'layer_{layer_index:03d}'", "\n", "Path", "(", "path", ")", ".", "mkdir", "(", "parents", "=", "True", ",", "exist_ok", "=", "True", ")", "\n", "feature_maps_plot", "(", "feature_maps", "[", "i", "]", ",", "path", ")", "\n", "\n", "\n"]], "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.base_model.inflated_3d_inception_v3.Inflated3DInceptionV3": [[24, 180], ["tensorflow.python.keras.layers.Input", "inflated_3d_inception_v3._conv3d_bn", "inflated_3d_inception_v3._conv3d_bn", "inflated_3d_inception_v3._conv3d_bn", "inflated_3d_inception_v3._conv3d_bn", "inflated_3d_inception_v3._conv3d_bn", "inflated_3d_inception_v3._conv3d_bn", "inflated_3d_inception_v3._conv3d_bn", "inflated_3d_inception_v3._conv3d_bn", "inflated_3d_inception_v3._conv3d_bn", "inflated_3d_inception_v3._conv3d_bn", "inflated_3d_inception_v3._conv3d_bn", "inflated_3d_inception_v3._conv3d_bn", "tensorflow.python.keras.layers.concatenate", "inflated_3d_inception_v3._conv3d_bn", "inflated_3d_inception_v3._conv3d_bn", "inflated_3d_inception_v3._conv3d_bn", "inflated_3d_inception_v3._conv3d_bn", "inflated_3d_inception_v3._conv3d_bn", "inflated_3d_inception_v3._conv3d_bn", "inflated_3d_inception_v3._conv3d_bn", "tensorflow.python.keras.layers.concatenate", "inflated_3d_inception_v3._conv3d_bn", "inflated_3d_inception_v3._conv3d_bn", "inflated_3d_inception_v3._conv3d_bn", "inflated_3d_inception_v3._conv3d_bn", "inflated_3d_inception_v3._conv3d_bn", "inflated_3d_inception_v3._conv3d_bn", "inflated_3d_inception_v3._conv3d_bn", "tensorflow.python.keras.layers.concatenate", "inflated_3d_inception_v3._conv3d_bn", "inflated_3d_inception_v3._conv3d_bn", "inflated_3d_inception_v3._conv3d_bn", "inflated_3d_inception_v3._conv3d_bn", "tensorflow.python.keras.layers.concatenate", "inflated_3d_inception_v3._conv3d_bn", "inflated_3d_inception_v3._conv3d_bn", "inflated_3d_inception_v3._conv3d_bn", "inflated_3d_inception_v3._conv3d_bn", "inflated_3d_inception_v3._conv3d_bn", "inflated_3d_inception_v3._conv3d_bn", "inflated_3d_inception_v3._conv3d_bn", "inflated_3d_inception_v3._conv3d_bn", "inflated_3d_inception_v3._conv3d_bn", "inflated_3d_inception_v3._conv3d_bn", "tensorflow.python.keras.layers.concatenate", "range", "inflated_3d_inception_v3._conv3d_bn", "inflated_3d_inception_v3._conv3d_bn", "inflated_3d_inception_v3._conv3d_bn", "inflated_3d_inception_v3._conv3d_bn", "inflated_3d_inception_v3._conv3d_bn", "inflated_3d_inception_v3._conv3d_bn", "inflated_3d_inception_v3._conv3d_bn", "inflated_3d_inception_v3._conv3d_bn", "inflated_3d_inception_v3._conv3d_bn", "inflated_3d_inception_v3._conv3d_bn", "tensorflow.python.keras.layers.concatenate", "inflated_3d_inception_v3._conv3d_bn", "inflated_3d_inception_v3._conv3d_bn", "inflated_3d_inception_v3._conv3d_bn", "inflated_3d_inception_v3._conv3d_bn", "inflated_3d_inception_v3._conv3d_bn", "inflated_3d_inception_v3._conv3d_bn", "tensorflow.python.keras.layers.concatenate", "range", "tensorflow.python.keras.engine.training.Model", "tensorflow.python.keras.layers.MaxPooling3D", "tensorflow.python.keras.layers.MaxPooling3D", "tensorflow.python.keras.layers.AveragePooling3D", "tensorflow.python.keras.layers.AveragePooling3D", "tensorflow.python.keras.layers.AveragePooling3D", "tensorflow.python.keras.layers.MaxPooling3D", "tensorflow.python.keras.layers.AveragePooling3D", "inflated_3d_inception_v3._conv3d_bn", "inflated_3d_inception_v3._conv3d_bn", "inflated_3d_inception_v3._conv3d_bn", "inflated_3d_inception_v3._conv3d_bn", "inflated_3d_inception_v3._conv3d_bn", "inflated_3d_inception_v3._conv3d_bn", "inflated_3d_inception_v3._conv3d_bn", "inflated_3d_inception_v3._conv3d_bn", "inflated_3d_inception_v3._conv3d_bn", "inflated_3d_inception_v3._conv3d_bn", "tensorflow.python.keras.layers.concatenate", "tensorflow.python.keras.layers.AveragePooling3D", "tensorflow.python.keras.layers.MaxPooling3D", "inflated_3d_inception_v3._conv3d_bn", "inflated_3d_inception_v3._conv3d_bn", "inflated_3d_inception_v3._conv3d_bn", "inflated_3d_inception_v3._conv3d_bn", "tensorflow.python.keras.layers.concatenate", "inflated_3d_inception_v3._conv3d_bn", "inflated_3d_inception_v3._conv3d_bn", "inflated_3d_inception_v3._conv3d_bn", "inflated_3d_inception_v3._conv3d_bn", "tensorflow.python.keras.layers.concatenate", "inflated_3d_inception_v3._conv3d_bn", "tensorflow.python.keras.layers.concatenate", "tensorflow.python.keras.layers.GlobalAveragePooling3D", "tensorflow.python.keras.layers.Dense", "tensorflow.python.keras.layers.AveragePooling3D", "tensorflow.python.keras.layers.AveragePooling3D"], "function", ["home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.base_model.inflated_3d_inception_v3._conv3d_bn", "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.base_model.inflated_3d_inception_v3._conv3d_bn", "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.base_model.inflated_3d_inception_v3._conv3d_bn", "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.base_model.inflated_3d_inception_v3._conv3d_bn", "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.base_model.inflated_3d_inception_v3._conv3d_bn", "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.base_model.inflated_3d_inception_v3._conv3d_bn", "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.base_model.inflated_3d_inception_v3._conv3d_bn", "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.base_model.inflated_3d_inception_v3._conv3d_bn", "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.base_model.inflated_3d_inception_v3._conv3d_bn", "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.base_model.inflated_3d_inception_v3._conv3d_bn", "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.base_model.inflated_3d_inception_v3._conv3d_bn", "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.base_model.inflated_3d_inception_v3._conv3d_bn", "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.base_model.inflated_3d_inception_v3._conv3d_bn", "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.base_model.inflated_3d_inception_v3._conv3d_bn", "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.base_model.inflated_3d_inception_v3._conv3d_bn", "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.base_model.inflated_3d_inception_v3._conv3d_bn", "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.base_model.inflated_3d_inception_v3._conv3d_bn", "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.base_model.inflated_3d_inception_v3._conv3d_bn", "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.base_model.inflated_3d_inception_v3._conv3d_bn", "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.base_model.inflated_3d_inception_v3._conv3d_bn", "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.base_model.inflated_3d_inception_v3._conv3d_bn", "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.base_model.inflated_3d_inception_v3._conv3d_bn", "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.base_model.inflated_3d_inception_v3._conv3d_bn", "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.base_model.inflated_3d_inception_v3._conv3d_bn", "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.base_model.inflated_3d_inception_v3._conv3d_bn", "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.base_model.inflated_3d_inception_v3._conv3d_bn", "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.base_model.inflated_3d_inception_v3._conv3d_bn", "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.base_model.inflated_3d_inception_v3._conv3d_bn", "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.base_model.inflated_3d_inception_v3._conv3d_bn", "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.base_model.inflated_3d_inception_v3._conv3d_bn", "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.base_model.inflated_3d_inception_v3._conv3d_bn", "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.base_model.inflated_3d_inception_v3._conv3d_bn", "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.base_model.inflated_3d_inception_v3._conv3d_bn", "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.base_model.inflated_3d_inception_v3._conv3d_bn", "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.base_model.inflated_3d_inception_v3._conv3d_bn", "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.base_model.inflated_3d_inception_v3._conv3d_bn", "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.base_model.inflated_3d_inception_v3._conv3d_bn", "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.base_model.inflated_3d_inception_v3._conv3d_bn", "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.base_model.inflated_3d_inception_v3._conv3d_bn", "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.base_model.inflated_3d_inception_v3._conv3d_bn", "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.base_model.inflated_3d_inception_v3._conv3d_bn", "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.base_model.inflated_3d_inception_v3._conv3d_bn", "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.base_model.inflated_3d_inception_v3._conv3d_bn", "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.base_model.inflated_3d_inception_v3._conv3d_bn", "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.base_model.inflated_3d_inception_v3._conv3d_bn", "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.base_model.inflated_3d_inception_v3._conv3d_bn", "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.base_model.inflated_3d_inception_v3._conv3d_bn", "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.base_model.inflated_3d_inception_v3._conv3d_bn", "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.base_model.inflated_3d_inception_v3._conv3d_bn", "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.base_model.inflated_3d_inception_v3._conv3d_bn", "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.base_model.inflated_3d_inception_v3._conv3d_bn", "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.base_model.inflated_3d_inception_v3._conv3d_bn", "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.base_model.inflated_3d_inception_v3._conv3d_bn", "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.base_model.inflated_3d_inception_v3._conv3d_bn", "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.base_model.inflated_3d_inception_v3._conv3d_bn", "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.base_model.inflated_3d_inception_v3._conv3d_bn", "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.base_model.inflated_3d_inception_v3._conv3d_bn", "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.base_model.inflated_3d_inception_v3._conv3d_bn", "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.base_model.inflated_3d_inception_v3._conv3d_bn", "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.base_model.inflated_3d_inception_v3._conv3d_bn", "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.base_model.inflated_3d_inception_v3._conv3d_bn", "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.base_model.inflated_3d_inception_v3._conv3d_bn", "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.base_model.inflated_3d_inception_v3._conv3d_bn", "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.base_model.inflated_3d_inception_v3._conv3d_bn", "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.base_model.inflated_3d_inception_v3._conv3d_bn", "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.base_model.inflated_3d_inception_v3._conv3d_bn", "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.base_model.inflated_3d_inception_v3._conv3d_bn", "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.base_model.inflated_3d_inception_v3._conv3d_bn", "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.base_model.inflated_3d_inception_v3._conv3d_bn", "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.base_model.inflated_3d_inception_v3._conv3d_bn", "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.base_model.inflated_3d_inception_v3._conv3d_bn", "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.base_model.inflated_3d_inception_v3._conv3d_bn", "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.base_model.inflated_3d_inception_v3._conv3d_bn", "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.base_model.inflated_3d_inception_v3._conv3d_bn", "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.base_model.inflated_3d_inception_v3._conv3d_bn"], ["def", "Inflated3DInceptionV3", "(", "input_shape", "=", "(", "20", ",", "224", ",", "224", ",", "3", ")", ",", "classes", "=", "1000", ")", ":", "\n", "    ", "inputs", "=", "layers", ".", "Input", "(", "shape", "=", "input_shape", ")", "\n", "\n", "x", "=", "_conv3d_bn", "(", "inputs", ",", "32", ",", "3", ",", "3", ",", "3", ",", "strides", "=", "(", "1", ",", "2", ",", "2", ")", ",", "padding", "=", "'valid'", ")", "\n", "x", "=", "_conv3d_bn", "(", "x", ",", "32", ",", "3", ",", "3", ",", "3", ",", "padding", "=", "'valid'", ")", "\n", "x", "=", "_conv3d_bn", "(", "x", ",", "64", ",", "3", ",", "3", ",", "3", ")", "\n", "x", "=", "layers", ".", "MaxPooling3D", "(", "(", "3", ",", "3", ",", "3", ")", ",", "strides", "=", "(", "1", ",", "2", ",", "2", ")", ")", "(", "x", ")", "\n", "\n", "x", "=", "_conv3d_bn", "(", "x", ",", "80", ",", "1", ",", "1", ",", "1", ",", "padding", "=", "'valid'", ")", "\n", "x", "=", "_conv3d_bn", "(", "x", ",", "192", ",", "3", ",", "3", ",", "3", ",", "padding", "=", "'valid'", ")", "\n", "x", "=", "layers", ".", "MaxPooling3D", "(", "(", "3", ",", "3", ",", "3", ")", ",", "strides", "=", "(", "1", ",", "2", ",", "2", ")", ")", "(", "x", ")", "\n", "\n", "# mixed 0: 35 x 35 x 256", "\n", "branch1x1", "=", "_conv3d_bn", "(", "x", ",", "64", ",", "1", ",", "1", ",", "1", ")", "\n", "\n", "branch5x5", "=", "_conv3d_bn", "(", "x", ",", "48", ",", "1", ",", "1", ",", "1", ")", "\n", "branch5x5", "=", "_conv3d_bn", "(", "branch5x5", ",", "64", ",", "5", ",", "5", ",", "5", ")", "\n", "\n", "branch3x3dbl", "=", "_conv3d_bn", "(", "x", ",", "64", ",", "1", ",", "1", ",", "1", ")", "\n", "branch3x3dbl", "=", "_conv3d_bn", "(", "branch3x3dbl", ",", "96", ",", "3", ",", "3", ",", "3", ")", "\n", "branch3x3dbl", "=", "_conv3d_bn", "(", "branch3x3dbl", ",", "96", ",", "3", ",", "3", ",", "3", ")", "\n", "\n", "branch_pool", "=", "layers", ".", "AveragePooling3D", "(", "(", "3", ",", "3", ",", "3", ")", ",", "strides", "=", "(", "1", ",", "1", ",", "1", ")", ",", "padding", "=", "'same'", ")", "(", "x", ")", "\n", "branch_pool", "=", "_conv3d_bn", "(", "branch_pool", ",", "32", ",", "1", ",", "1", ",", "1", ")", "\n", "x", "=", "layers", ".", "concatenate", "(", "[", "branch1x1", ",", "branch5x5", ",", "branch3x3dbl", ",", "branch_pool", "]", ",", "axis", "=", "_CHANNEL_AXIS", ",", "name", "=", "'mixed0'", ")", "\n", "\n", "# mixed 1: 35 x 35 x 288", "\n", "branch1x1", "=", "_conv3d_bn", "(", "x", ",", "64", ",", "1", ",", "1", ",", "1", ")", "\n", "\n", "branch5x5", "=", "_conv3d_bn", "(", "x", ",", "48", ",", "1", ",", "1", ",", "1", ")", "\n", "branch5x5", "=", "_conv3d_bn", "(", "branch5x5", ",", "64", ",", "5", ",", "5", ",", "5", ")", "\n", "\n", "branch3x3dbl", "=", "_conv3d_bn", "(", "x", ",", "64", ",", "1", ",", "1", ",", "1", ")", "\n", "branch3x3dbl", "=", "_conv3d_bn", "(", "branch3x3dbl", ",", "96", ",", "3", ",", "3", ",", "3", ")", "\n", "branch3x3dbl", "=", "_conv3d_bn", "(", "branch3x3dbl", ",", "96", ",", "3", ",", "3", ",", "3", ")", "\n", "\n", "branch_pool", "=", "layers", ".", "AveragePooling3D", "(", "(", "3", ",", "3", ",", "3", ")", ",", "strides", "=", "(", "1", ",", "1", ",", "1", ")", ",", "padding", "=", "'same'", ")", "(", "x", ")", "\n", "branch_pool", "=", "_conv3d_bn", "(", "branch_pool", ",", "64", ",", "1", ",", "1", ",", "1", ")", "\n", "x", "=", "layers", ".", "concatenate", "(", "[", "branch1x1", ",", "branch5x5", ",", "branch3x3dbl", ",", "branch_pool", "]", ",", "axis", "=", "_CHANNEL_AXIS", ",", "name", "=", "'mixed1'", ")", "\n", "\n", "# mixed 2: 35 x 35 x 288", "\n", "branch1x1", "=", "_conv3d_bn", "(", "x", ",", "64", ",", "1", ",", "1", ",", "1", ")", "\n", "\n", "branch5x5", "=", "_conv3d_bn", "(", "x", ",", "48", ",", "1", ",", "1", ",", "1", ")", "\n", "branch5x5", "=", "_conv3d_bn", "(", "branch5x5", ",", "64", ",", "5", ",", "5", ",", "5", ")", "\n", "\n", "branch3x3dbl", "=", "_conv3d_bn", "(", "x", ",", "64", ",", "1", ",", "1", ",", "1", ")", "\n", "branch3x3dbl", "=", "_conv3d_bn", "(", "branch3x3dbl", ",", "96", ",", "3", ",", "3", ",", "3", ")", "\n", "branch3x3dbl", "=", "_conv3d_bn", "(", "branch3x3dbl", ",", "96", ",", "3", ",", "3", ",", "3", ")", "\n", "\n", "branch_pool", "=", "layers", ".", "AveragePooling3D", "(", "(", "3", ",", "3", ",", "3", ")", ",", "strides", "=", "(", "1", ",", "1", ",", "1", ")", ",", "padding", "=", "'same'", ")", "(", "x", ")", "\n", "branch_pool", "=", "_conv3d_bn", "(", "branch_pool", ",", "64", ",", "1", ",", "1", ",", "1", ")", "\n", "x", "=", "layers", ".", "concatenate", "(", "[", "branch1x1", ",", "branch5x5", ",", "branch3x3dbl", ",", "branch_pool", "]", ",", "axis", "=", "_CHANNEL_AXIS", ",", "name", "=", "'mixed2'", ")", "\n", "\n", "# mixed 3: 17 x 17 x 768", "\n", "branch3x3", "=", "_conv3d_bn", "(", "x", ",", "384", ",", "3", ",", "3", ",", "3", ",", "strides", "=", "(", "1", ",", "2", ",", "2", ")", ",", "padding", "=", "'valid'", ")", "\n", "\n", "branch3x3dbl", "=", "_conv3d_bn", "(", "x", ",", "64", ",", "1", ",", "1", ",", "1", ")", "\n", "branch3x3dbl", "=", "_conv3d_bn", "(", "branch3x3dbl", ",", "96", ",", "3", ",", "3", ",", "3", ")", "\n", "branch3x3dbl", "=", "_conv3d_bn", "(", "branch3x3dbl", ",", "96", ",", "3", ",", "3", ",", "3", ",", "strides", "=", "(", "1", ",", "2", ",", "2", ")", ",", "padding", "=", "'valid'", ")", "\n", "\n", "branch_pool", "=", "layers", ".", "MaxPooling3D", "(", "(", "3", ",", "3", ",", "3", ")", ",", "strides", "=", "(", "1", ",", "2", ",", "2", ")", ")", "(", "x", ")", "\n", "x", "=", "layers", ".", "concatenate", "(", "[", "branch3x3", ",", "branch3x3dbl", ",", "branch_pool", "]", ",", "axis", "=", "_CHANNEL_AXIS", ",", "name", "=", "'mixed3'", ")", "\n", "\n", "# mixed 4: 17 x 17 x 768", "\n", "branch1x1", "=", "_conv3d_bn", "(", "x", ",", "192", ",", "1", ",", "1", ",", "1", ")", "\n", "\n", "branch7x7", "=", "_conv3d_bn", "(", "x", ",", "128", ",", "1", ",", "1", ",", "1", ")", "\n", "branch7x7", "=", "_conv3d_bn", "(", "branch7x7", ",", "128", ",", "1", ",", "1", ",", "7", ")", "\n", "branch7x7", "=", "_conv3d_bn", "(", "branch7x7", ",", "192", ",", "1", ",", "7", ",", "1", ")", "\n", "\n", "branch7x7dbl", "=", "_conv3d_bn", "(", "x", ",", "128", ",", "1", ",", "1", ",", "1", ")", "\n", "branch7x7dbl", "=", "_conv3d_bn", "(", "branch7x7dbl", ",", "128", ",", "1", ",", "7", ",", "1", ")", "\n", "branch7x7dbl", "=", "_conv3d_bn", "(", "branch7x7dbl", ",", "128", ",", "1", ",", "1", ",", "7", ")", "\n", "branch7x7dbl", "=", "_conv3d_bn", "(", "branch7x7dbl", ",", "128", ",", "1", ",", "7", ",", "1", ")", "\n", "branch7x7dbl", "=", "_conv3d_bn", "(", "branch7x7dbl", ",", "192", ",", "1", ",", "1", ",", "7", ")", "\n", "\n", "branch_pool", "=", "layers", ".", "AveragePooling3D", "(", "(", "3", ",", "3", ",", "3", ")", ",", "strides", "=", "(", "1", ",", "1", ",", "1", ")", ",", "padding", "=", "'same'", ")", "(", "x", ")", "\n", "branch_pool", "=", "_conv3d_bn", "(", "branch_pool", ",", "192", ",", "1", ",", "1", ",", "1", ")", "\n", "x", "=", "layers", ".", "concatenate", "(", "[", "branch1x1", ",", "branch7x7", ",", "branch7x7dbl", ",", "branch_pool", "]", ",", "axis", "=", "_CHANNEL_AXIS", ",", "name", "=", "'mixed4'", ")", "\n", "\n", "# mixed 5, 6: 17 x 17 x 768", "\n", "for", "i", "in", "range", "(", "2", ")", ":", "\n", "        ", "branch1x1", "=", "_conv3d_bn", "(", "x", ",", "192", ",", "1", ",", "1", ",", "1", ")", "\n", "\n", "branch7x7", "=", "_conv3d_bn", "(", "x", ",", "160", ",", "1", ",", "1", ",", "1", ")", "\n", "branch7x7", "=", "_conv3d_bn", "(", "branch7x7", ",", "160", ",", "1", ",", "1", ",", "7", ")", "\n", "branch7x7", "=", "_conv3d_bn", "(", "branch7x7", ",", "192", ",", "1", ",", "7", ",", "1", ")", "\n", "\n", "branch7x7dbl", "=", "_conv3d_bn", "(", "x", ",", "160", ",", "1", ",", "1", ",", "1", ")", "\n", "branch7x7dbl", "=", "_conv3d_bn", "(", "branch7x7dbl", ",", "160", ",", "1", ",", "7", ",", "1", ")", "\n", "branch7x7dbl", "=", "_conv3d_bn", "(", "branch7x7dbl", ",", "160", ",", "1", ",", "1", ",", "7", ")", "\n", "branch7x7dbl", "=", "_conv3d_bn", "(", "branch7x7dbl", ",", "160", ",", "1", ",", "7", ",", "1", ")", "\n", "branch7x7dbl", "=", "_conv3d_bn", "(", "branch7x7dbl", ",", "192", ",", "1", ",", "1", ",", "7", ")", "\n", "\n", "branch_pool", "=", "layers", ".", "AveragePooling3D", "(", "(", "3", ",", "3", ",", "3", ")", ",", "strides", "=", "(", "1", ",", "1", ",", "1", ")", ",", "padding", "=", "'same'", ")", "(", "x", ")", "\n", "branch_pool", "=", "_conv3d_bn", "(", "branch_pool", ",", "192", ",", "1", ",", "1", ",", "1", ")", "\n", "x", "=", "layers", ".", "concatenate", "(", "[", "branch1x1", ",", "branch7x7", ",", "branch7x7dbl", ",", "branch_pool", "]", ",", "axis", "=", "_CHANNEL_AXIS", ",", "\n", "name", "=", "f'mixed{5 + i}'", ")", "\n", "\n", "# mixed 7: 17 x 17 x 768", "\n", "", "branch1x1", "=", "_conv3d_bn", "(", "x", ",", "192", ",", "1", ",", "1", ",", "1", ")", "\n", "\n", "branch7x7", "=", "_conv3d_bn", "(", "x", ",", "192", ",", "1", ",", "1", ",", "1", ")", "\n", "branch7x7", "=", "_conv3d_bn", "(", "branch7x7", ",", "192", ",", "1", ",", "1", ",", "7", ")", "\n", "branch7x7", "=", "_conv3d_bn", "(", "branch7x7", ",", "192", ",", "1", ",", "7", ",", "1", ")", "\n", "\n", "branch7x7dbl", "=", "_conv3d_bn", "(", "x", ",", "192", ",", "1", ",", "1", ",", "1", ")", "\n", "branch7x7dbl", "=", "_conv3d_bn", "(", "branch7x7dbl", ",", "192", ",", "1", ",", "7", ",", "1", ")", "\n", "branch7x7dbl", "=", "_conv3d_bn", "(", "branch7x7dbl", ",", "192", ",", "1", ",", "1", ",", "7", ")", "\n", "branch7x7dbl", "=", "_conv3d_bn", "(", "branch7x7dbl", ",", "192", ",", "1", ",", "7", ",", "1", ")", "\n", "branch7x7dbl", "=", "_conv3d_bn", "(", "branch7x7dbl", ",", "192", ",", "1", ",", "1", ",", "7", ")", "\n", "\n", "branch_pool", "=", "layers", ".", "AveragePooling3D", "(", "(", "3", ",", "3", ",", "3", ")", ",", "strides", "=", "(", "1", ",", "1", ",", "1", ")", ",", "padding", "=", "'same'", ")", "(", "x", ")", "\n", "branch_pool", "=", "_conv3d_bn", "(", "branch_pool", ",", "192", ",", "1", ",", "1", ",", "1", ")", "\n", "x", "=", "layers", ".", "concatenate", "(", "[", "branch1x1", ",", "branch7x7", ",", "branch7x7dbl", ",", "branch_pool", "]", ",", "axis", "=", "_CHANNEL_AXIS", ",", "name", "=", "'mixed7'", ")", "\n", "\n", "# mixed 8: 8 x 8 x 1280", "\n", "branch3x3", "=", "_conv3d_bn", "(", "x", ",", "192", ",", "1", ",", "1", ",", "1", ")", "\n", "branch3x3", "=", "_conv3d_bn", "(", "branch3x3", ",", "320", ",", "3", ",", "3", ",", "3", ",", "strides", "=", "(", "1", ",", "2", ",", "2", ")", ",", "padding", "=", "'valid'", ")", "\n", "\n", "branch7x7x3", "=", "_conv3d_bn", "(", "x", ",", "192", ",", "1", ",", "1", ",", "1", ")", "\n", "branch7x7x3", "=", "_conv3d_bn", "(", "branch7x7x3", ",", "192", ",", "1", ",", "1", ",", "7", ")", "\n", "branch7x7x3", "=", "_conv3d_bn", "(", "branch7x7x3", ",", "192", ",", "1", ",", "7", ",", "1", ")", "\n", "branch7x7x3", "=", "_conv3d_bn", "(", "branch7x7x3", ",", "192", ",", "3", ",", "3", ",", "3", ",", "strides", "=", "(", "1", ",", "2", ",", "2", ")", ",", "padding", "=", "'valid'", ")", "\n", "\n", "branch_pool", "=", "layers", ".", "MaxPooling3D", "(", "(", "3", ",", "3", ",", "3", ")", ",", "strides", "=", "(", "1", ",", "2", ",", "2", ")", ")", "(", "x", ")", "\n", "x", "=", "layers", ".", "concatenate", "(", "[", "branch3x3", ",", "branch7x7x3", ",", "branch_pool", "]", ",", "axis", "=", "_CHANNEL_AXIS", ",", "name", "=", "'mixed8'", ")", "\n", "\n", "# mixed 9: 8 x 8 x 2048", "\n", "for", "i", "in", "range", "(", "2", ")", ":", "\n", "        ", "branch1x1", "=", "_conv3d_bn", "(", "x", ",", "320", ",", "1", ",", "1", ",", "1", ")", "\n", "\n", "branch3x3", "=", "_conv3d_bn", "(", "x", ",", "384", ",", "1", ",", "1", ",", "1", ")", "\n", "branch3x3_1", "=", "_conv3d_bn", "(", "branch3x3", ",", "384", ",", "1", ",", "1", ",", "3", ")", "\n", "branch3x3_2", "=", "_conv3d_bn", "(", "branch3x3", ",", "384", ",", "1", ",", "3", ",", "1", ")", "\n", "branch3x3", "=", "layers", ".", "concatenate", "(", "[", "branch3x3_1", ",", "branch3x3_2", "]", ",", "axis", "=", "_CHANNEL_AXIS", ",", "name", "=", "f'mixed9_{i}'", ")", "\n", "\n", "branch3x3dbl", "=", "_conv3d_bn", "(", "x", ",", "448", ",", "1", ",", "1", ",", "1", ")", "\n", "branch3x3dbl", "=", "_conv3d_bn", "(", "branch3x3dbl", ",", "384", ",", "3", ",", "3", ",", "3", ")", "\n", "branch3x3dbl_1", "=", "_conv3d_bn", "(", "branch3x3dbl", ",", "384", ",", "1", ",", "1", ",", "3", ")", "\n", "branch3x3dbl_2", "=", "_conv3d_bn", "(", "branch3x3dbl", ",", "384", ",", "1", ",", "3", ",", "1", ")", "\n", "branch3x3dbl", "=", "layers", ".", "concatenate", "(", "[", "branch3x3dbl_1", ",", "branch3x3dbl_2", "]", ",", "axis", "=", "_CHANNEL_AXIS", ")", "\n", "\n", "branch_pool", "=", "layers", ".", "AveragePooling3D", "(", "(", "3", ",", "3", ",", "3", ")", ",", "strides", "=", "(", "1", ",", "1", ",", "1", ")", ",", "padding", "=", "'same'", ")", "(", "x", ")", "\n", "branch_pool", "=", "_conv3d_bn", "(", "branch_pool", ",", "192", ",", "1", ",", "1", ",", "1", ")", "\n", "x", "=", "layers", ".", "concatenate", "(", "[", "branch1x1", ",", "branch3x3", ",", "branch3x3dbl", ",", "branch_pool", "]", ",", "axis", "=", "_CHANNEL_AXIS", ",", "\n", "name", "=", "f'mixed{9 + i}'", ")", "\n", "\n", "# Classification block", "\n", "", "x", "=", "layers", ".", "GlobalAveragePooling3D", "(", "name", "=", "'avg_pool'", ")", "(", "x", ")", "\n", "x", "=", "layers", ".", "Dense", "(", "classes", ",", "activation", "=", "'softmax'", ",", "name", "=", "'predictions'", ")", "(", "x", ")", "\n", "\n", "# Create model", "\n", "model", "=", "training", ".", "Model", "(", "inputs", ",", "x", ",", "name", "=", "'inflated_3d_inception_v3'", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.base_model.inflated_3d_inception_v3._conv3d_bn": [[182, 212], ["tensorflow.python.keras.layers.Conv3D", "tensorflow.python.keras.layers.BatchNormalization", "tensorflow.python.keras.layers.Activation"], "function", ["None"], ["", "def", "_conv3d_bn", "(", "x", ",", "filters", ",", "num_step", ",", "num_row", ",", "num_col", ",", "padding", "=", "'same'", ",", "strides", "=", "(", "1", ",", "1", ",", "1", ")", ",", "name", "=", "None", ")", ":", "\n", "    ", "\"\"\"Utility function to apply convolution and batch normalization.\n\n    Arguments:\n        x: The input tensor.\n        filters: The filters in `Conv3D`.\n        num_step: The depth of the convolution kernel.\n        num_row: The height of the convolution kernel.\n        num_col: The width of the convolution kernel.\n        padding: The padding mode in `Conv3D`.\n        strides: The strides in `Conv3D`.\n        name: The name of the ops; will become `name + '_conv'` for the convolution and `name + '_bn'` for the batch\n              norm layer.\n\n    Returns:\n        The output tensor after applying `Conv3D` and `BatchNormalization`.\n    \"\"\"", "\n", "if", "name", "is", "not", "None", ":", "\n", "        ", "bn_name", "=", "name", "+", "'_bn'", "\n", "conv_name", "=", "name", "+", "'_conv'", "\n", "", "else", ":", "\n", "        ", "bn_name", "=", "None", "\n", "conv_name", "=", "None", "\n", "\n", "", "x", "=", "layers", ".", "Conv3D", "(", "filters", ",", "(", "num_step", ",", "num_row", ",", "num_col", ")", ",", "strides", "=", "strides", ",", "padding", "=", "padding", ",", "use_bias", "=", "False", ",", "\n", "name", "=", "conv_name", ")", "(", "x", ")", "\n", "x", "=", "layers", ".", "BatchNormalization", "(", "axis", "=", "_CHANNEL_AXIS", ",", "scale", "=", "False", ",", "name", "=", "bn_name", ")", "(", "x", ")", "\n", "x", "=", "layers", ".", "Activation", "(", "'relu'", ",", "name", "=", "name", ")", "(", "x", ")", "\n", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.base_model.inflated_3d_inception_v3.load_inflated_imagenet_weights": [[214, 222], ["tensorflow.keras.applications.InceptionV3", "enumerate", "inflated_3d_inception_v3._copy_conv_weights", "inflated_3d_inception_v3._copy_batch_normalization_weights"], "function", ["home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.base_model.inflated_3d_inception_v3._copy_conv_weights", "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.base_model.inflated_3d_inception_v3._copy_batch_normalization_weights"], ["", "def", "load_inflated_imagenet_weights", "(", "target_model", ")", ":", "\n", "    ", "source_model", "=", "InceptionV3", "(", "include_top", "=", "True", ",", "weights", "=", "'imagenet'", ")", "\n", "for", "i", ",", "source_layer", "in", "enumerate", "(", "source_model", ".", "layers", ")", ":", "\n", "        ", "target_layer", "=", "target_model", ".", "layers", "[", "i", "]", "\n", "if", "'conv'", "in", "source_layer", ".", "name", ":", "\n", "            ", "_copy_conv_weights", "(", "source_layer", ",", "target_layer", ")", "\n", "", "elif", "'batch_normalization'", "in", "source_layer", ".", "name", ":", "\n", "            ", "_copy_batch_normalization_weights", "(", "source_layer", ",", "target_layer", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.base_model.inflated_3d_inception_v3._copy_conv_weights": [[224, 232], ["numpy.ones", "numpy.tile", "target_layer.set_weights", "numpy.copy", "len", "target_layer.get_weights", "source_layer.get_weights"], "function", ["None"], ["", "", "", "def", "_copy_conv_weights", "(", "source_layer", ",", "target_layer", ")", ":", "\n", "    ", "target_shape", "=", "target_layer", ".", "get_weights", "(", ")", "[", "0", "]", ".", "shape", "\n", "depth", "=", "target_shape", "[", "0", "]", "\n", "weights", "=", "np", ".", "copy", "(", "source_layer", ".", "get_weights", "(", ")", "[", "0", "]", ")", "/", "depth", "\n", "reps", "=", "np", ".", "ones", "(", "len", "(", "target_shape", ")", ",", "dtype", "=", "int", ")", "\n", "reps", "[", "0", "]", "=", "depth", "\n", "weights", "=", "np", ".", "tile", "(", "weights", ",", "reps", ")", "\n", "target_layer", ".", "set_weights", "(", "[", "weights", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.base_model.inflated_3d_inception_v3._copy_batch_normalization_weights": [[234, 236], ["target_layer.set_weights", "source_layer.get_weights().copy", "source_layer.get_weights"], "function", ["None"], ["", "def", "_copy_batch_normalization_weights", "(", "source_layer", ",", "target_layer", ")", ":", "\n", "    ", "target_layer", ".", "set_weights", "(", "source_layer", ".", "get_weights", "(", ")", ".", "copy", "(", ")", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.base_model.summarise_architecture.summarise_architecture": [[6, 10], ["base_model.inflated_3d_inception_v3.Inflated3DInceptionV3", "base_model.inflated_3d_inception_v3.Inflated3DInceptionV3.summary", "tensorflow.keras.utils.plot_model"], "function", ["home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.base_model.inflated_3d_inception_v3.Inflated3DInceptionV3"], ["def", "summarise_architecture", "(", ")", ":", "\n", "    ", "model", "=", "Inflated3DInceptionV3", "(", ")", "\n", "model", ".", "summary", "(", "line_length", "=", "90", ",", "positions", "=", "[", ".25", ",", ".57", ",", ".7", ",", "1.", "]", ")", "\n", "plot_model", "(", "model", ",", "'model_architecture.png'", ",", "show_shapes", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.base_model.display_conv_layers.display_conv_layers": [[4, 11], ["base_model.inflated_3d_inception_v3.Inflated3DInceptionV3", "enumerate", "print"], "function", ["home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.base_model.inflated_3d_inception_v3.Inflated3DInceptionV3"], ["def", "display_conv_layers", "(", ")", ":", "\n", "    ", "\"\"\"Displays the convolutional layers of the inflated 3D Inception-v3 network.\"\"\"", "\n", "model", "=", "Inflated3DInceptionV3", "(", ")", "\n", "for", "i", ",", "layer", "in", "enumerate", "(", "model", ".", "layers", ")", ":", "\n", "        ", "if", "'conv'", "not", "in", "layer", ".", "name", ":", "\n", "            ", "continue", "\n", "", "print", "(", "'%-5d %-11s %s'", "%", "(", "i", ",", "layer", ".", "name", ",", "layer", ".", "output", ".", "shape", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.pretrained_model.feature_maps._dataset": [[13, 28], ["datasets.tf_record_utils.tf_record_dataset", "dataset.batch.batch", "dataset.batch.map", "dataset.batch.unbatch", "dataset.batch.filter", "dataset.batch.batch", "dataset.batch.take", "tensorflow.math.equal", "tensorflow.math.equal"], "function", ["home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.datasets.tf_record_utils.tf_record_dataset"], ["def", "_dataset", "(", ")", ":", "\n", "    ", "\"\"\"Returns the `SIGNUM` dataset example for the word `dance`.\n\n    The dataset example corresponding to the label with index 420 and the signer with index 1 shall be returned.\n\n    Returns:\n        The dataset example for the word `dance`.\n    \"\"\"", "\n", "dataset", "=", "tf_record_dataset", "(", "DatasetName", ".", "SIGNUM", ",", "DatasetType", ".", "TRAIN", ")", "\n", "dataset", "=", "dataset", ".", "batch", "(", "1", ")", "\n", "dataset", "=", "dataset", ".", "map", "(", "transform_for_prediction", ")", "\n", "dataset", "=", "dataset", ".", "unbatch", "(", ")", "\n", "dataset", "=", "dataset", ".", "filter", "(", "lambda", "frames", ",", "label", ",", "signer", ":", "tf", ".", "math", ".", "equal", "(", "label", ",", "420", ")", "and", "tf", ".", "math", ".", "equal", "(", "signer", ",", "1", ")", ")", "\n", "dataset", "=", "dataset", ".", "batch", "(", "1", ")", "\n", "return", "dataset", ".", "take", "(", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.pretrained_model.feature_maps._model": [[30, 34], ["training.utils.model_path", "training.utils.load_model", "tensorflow.keras.models.Model", "pathlib.Path"], "function", ["home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.training.utils.model_path", "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.training.utils.load_model"], ["", "def", "_model", "(", ")", ":", "\n", "    ", "path", "=", "model_path", "(", "Path", "(", ")", ",", "TRAINING_RUNS", "[", "_TRAINING_RUNS_INDEX", "]", ")", "\n", "base_model", "=", "load_model", "(", "path", ")", "\n", "return", "Model", "(", "inputs", "=", "base_model", ".", "inputs", ",", "outputs", "=", "[", "base_model", ".", "layers", "[", "i", "]", ".", "output", "for", "i", "in", "_LAYER_INDICES", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.pretrained_model.feature_maps.plot_feature_maps": [[39, 47], ["feature_maps._dataset", "feature_maps._model", "_model.predict", "enumerate", "pathlib.Path().mkdir", "plotting.plot.feature_maps_plot", "pathlib.Path", "pathlib.Path"], "function", ["home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.finetuned_model.feature_maps._dataset", "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.finetuned_model.train._model", "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.plotting.plot.feature_maps_plot"], ["\n", "def", "plot_feature_maps", "(", ")", ":", "\n", "    ", "dataset", "=", "_dataset", "(", ")", "\n", "model", "=", "_model", "(", ")", "\n", "feature_maps", "=", "model", ".", "predict", "(", "dataset", ")", "\n", "for", "i", ",", "layer_index", "in", "enumerate", "(", "_LAYER_INDICES", ")", ":", "\n", "        ", "path", "=", "Path", "(", ")", "/", "'feature_maps'", "/", "f'layer_{layer_index:03d}'", "\n", "Path", "(", "path", ")", ".", "mkdir", "(", "parents", "=", "True", ",", "exist_ok", "=", "True", ")", "\n", "feature_maps_plot", "(", "feature_maps", "[", "i", "]", ",", "path", ")", "\n"]], "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.pretrained_model.train._train_dataset": [[14, 20], ["datasets.tf_record_utils.tf_record_dataset", "train_dataset.map.shuffle", "train_dataset.map.batch", "train_dataset.map.map", "train_dataset.map.prefetch"], "function", ["home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.datasets.tf_record_utils.tf_record_dataset"], ["def", "_train_dataset", "(", ")", ":", "\n", "    ", "train_dataset", "=", "tf_record_dataset", "(", "DatasetName", ".", "SIGNUM", ",", "DatasetType", ".", "TRAIN", ")", "\n", "train_dataset", "=", "train_dataset", ".", "shuffle", "(", "2048", ")", "\n", "train_dataset", "=", "train_dataset", ".", "batch", "(", "32", ")", "\n", "train_dataset", "=", "train_dataset", ".", "map", "(", "transform_for_signum_model", ")", "\n", "return", "train_dataset", ".", "prefetch", "(", "2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.pretrained_model.train._model": [[22, 28], ["base_model.inflated_3d_inception_v3.Inflated3DInceptionV3", "base_model.inflated_3d_inception_v3.load_inflated_imagenet_weights", "base_model.inflated_3d_inception_v3.Inflated3DInceptionV3.compile", "tensorflow.keras.optimizers.SGD"], "function", ["home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.base_model.inflated_3d_inception_v3.Inflated3DInceptionV3", "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.base_model.inflated_3d_inception_v3.load_inflated_imagenet_weights"], ["", "def", "_validation_dataset", "(", ")", ":", "\n", "    ", "validation_dataset", "=", "tf_record_dataset", "(", "DatasetName", ".", "SIGNUM", ",", "DatasetType", ".", "VALIDATION", ")", "\n", "validation_dataset", "=", "validation_dataset", ".", "batch", "(", "32", ")", "\n", "validation_dataset", "=", "validation_dataset", ".", "map", "(", "transform_for_signum_model", ")", "\n", "return", "validation_dataset", ".", "prefetch", "(", "2", ")", "\n", "\n", "\n"]], "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.pretrained_model.train.train": [[30, 37], ["train._train_dataset", "training.callbacks.ThresholdStopping", "train._model", "_model.fit", "training.utils.save_model"], "function", ["home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.finetuned_model.train._train_dataset", "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.finetuned_model.train._model", "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.training.utils.save_model"], ["    ", "model", "=", "Inflated3DInceptionV3", "(", "classes", "=", "N_CLASSES", ")", "\n", "load_inflated_imagenet_weights", "(", "model", ")", "\n", "model", ".", "compile", "(", "optimizer", "=", "SGD", "(", "learning_rate", "=", "0.01", ",", "momentum", "=", "0.9", ")", ",", "loss", "=", "'categorical_crossentropy'", ",", "\n", "metrics", "=", "[", "'accuracy'", "]", ")", "\n", "return", "model", "\n", "\n", "\n", "", "def", "train", "(", "path", ":", "Path", ")", ":", "\n"]], "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.pretrained_model.learning_curves.plot_learning_curves": [[9, 13], ["training.utils.history_path", "core.utils.load_dataframe", "plotting.plot.loss_accuracy_without_validation_plot", "pathlib.Path"], "function", ["home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.training.utils.history_path", "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.core.utils.load_dataframe", "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.plotting.plot.loss_accuracy_without_validation_plot"], ["def", "plot_learning_curves", "(", ")", ":", "\n", "    ", "for", "training_run", "in", "TRAINING_RUNS", ":", "\n", "        ", "path", "=", "history_path", "(", "Path", "(", ")", ",", "training_run", ")", "\n", "df", "=", "load_dataframe", "(", "path", ")", "\n", "loss_accuracy_plot", "(", "df", ")", "\n"]], "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.evaluation.idealised_learning_curves._splines": [[9, 14], ["scipy.interpolate.UnivariateSpline", "scipy.interpolate.UnivariateSpline", "numpy.linspace", "scipy.interpolate.UnivariateSpline.", "scipy.interpolate.UnivariateSpline."], "function", ["None"], ["def", "_splines", "(", ")", ":", "\n", "    ", "spline", "=", "UnivariateSpline", "(", "[", "1", ",", "10", ",", "20", ",", "30", ",", "40", "]", ",", "[", "4", ",", "1.5", ",", "0.5", ",", "0.275", ",", "0.225", "]", ",", "k", "=", "4", ")", "\n", "val_spline", "=", "UnivariateSpline", "(", "[", "1", ",", "10", ",", "20", ",", "30", ",", "40", "]", ",", "[", "4.5", ",", "2", ",", "1.4", ",", "1.675", ",", "2.25", "]", ",", "k", "=", "4", ")", "\n", "xs", "=", "np", ".", "linspace", "(", "1", ",", "40", ",", "40", ")", "\n", "return", "spline", "(", "xs", ")", ",", "val_spline", "(", "xs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.evaluation.idealised_learning_curves.idealised_learning_curves": [[16, 23], ["idealised_learning_curves._splines", "pandas.DataFrame", "plotting.plot.idealised_learning_curves_plot"], "function", ["home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.evaluation.idealised_learning_curves._splines", "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.plotting.plot.idealised_learning_curves_plot"], ["", "def", "idealised_learning_curves", "(", ")", ":", "\n", "    ", "spline", ",", "val_spline", "=", "_splines", "(", ")", "\n", "df", "=", "DataFrame", "(", "{", "\n", "Metric", ".", "LOSS", ".", "value", ":", "spline", ",", "\n", "Metric", ".", "VAL_LOSS", ".", "value", ":", "val_spline", "\n", "}", ")", "\n", "idealised_learning_curves_plot", "(", "df", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.evaluation.compare_test_scores.dataframe": [[14, 22], ["testing.utils.scores_file_path", "core.utils.load_dataframe", "testing.utils.scores_file_path", "core.utils.load_dataframe", "pandas.concat", "core.utils.package_path", "core.utils.package_path", "list", "itertools.chain"], "function", ["home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.testing.utils.scores_file_path", "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.core.utils.load_dataframe", "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.testing.utils.scores_file_path", "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.core.utils.load_dataframe", "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.core.utils.package_path", "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.core.utils.package_path"], ["def", "dataframe", "(", ")", ":", "\n", "    ", "path", "=", "scores_file_path", "(", "package_path", "(", "baseline_model_package", ")", ")", "\n", "baseline_df", "=", "load_dataframe", "(", "path", ")", "\n", "path", "=", "scores_file_path", "(", "package_path", "(", "finetuned_model_package", ")", ")", "\n", "finetuned_df", "=", "load_dataframe", "(", "path", ")", "\n", "return", "pd", ".", "concat", "(", "[", "baseline_df", "[", "Metric", ".", "LOSS", ".", "value", "]", ",", "finetuned_df", "[", "Metric", ".", "LOSS", ".", "value", "]", ",", "\n", "baseline_df", "[", "Metric", ".", "ACCURACY", ".", "value", "]", ",", "finetuned_df", "[", "Metric", ".", "ACCURACY", ".", "value", "]", "]", ",", "axis", "=", "1", ",", "\n", "keys", "=", "list", "(", "itertools", ".", "chain", "(", "*", "COMPARE_TEST_SCORES_COLUMNS", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.evaluation.compare_test_scores.compare_test_scores": [[24, 27], ["compare_test_scores.dataframe", "plotting.plot.compare_test_scores_plot"], "function", ["home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.evaluation.compare_training.dataframe", "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.plotting.plot.compare_test_scores_plot"], ["", "def", "compare_test_scores", "(", ")", ":", "\n", "    ", "df", "=", "dataframe", "(", ")", "\n", "compare_test_scores_plot", "(", "df", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.evaluation.test_scores_improvement.dataframe": [[11, 19], ["testing.utils.scores_file_path", "core.utils.load_dataframe", "testing.utils.scores_file_path", "core.utils.load_dataframe", "pandas.concat().transpose", "core.utils.package_path", "core.utils.package_path", "pandas.concat"], "function", ["home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.testing.utils.scores_file_path", "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.core.utils.load_dataframe", "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.testing.utils.scores_file_path", "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.core.utils.load_dataframe", "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.core.utils.package_path", "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.core.utils.package_path"], ["def", "dataframe", "(", ")", ":", "\n", "    ", "path", "=", "scores_file_path", "(", "package_path", "(", "baseline_model_package", ")", ")", "\n", "baseline_df", "=", "load_dataframe", "(", "path", ")", "\n", "path", "=", "scores_file_path", "(", "package_path", "(", "finetuned_model_package", ")", ")", "\n", "finetuned_df", "=", "load_dataframe", "(", "path", ")", "\n", "return", "pd", ".", "concat", "(", "[", "finetuned_df", "[", "Metric", ".", "LOSS", ".", "value", "]", "-", "baseline_df", "[", "Metric", ".", "LOSS", ".", "value", "]", ",", "\n", "finetuned_df", "[", "Metric", ".", "ACCURACY", ".", "value", "]", "-", "baseline_df", "[", "Metric", ".", "ACCURACY", ".", "value", "]", "]", ",", "axis", "=", "1", ",", "\n", "keys", "=", "[", "Metric", ".", "LOSS", ".", "value", ",", "Metric", ".", "ACCURACY", ".", "value", "]", ")", ".", "transpose", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.evaluation.test_scores_improvement.test_scores_improvement": [[21, 24], ["test_scores_improvement.dataframe", "plotting.plot.test_scores_improvement_plot"], "function", ["home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.evaluation.compare_training.dataframe", "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.plotting.plot.test_scores_improvement_plot"], ["", "def", "test_scores_improvement", "(", ")", ":", "\n", "    ", "df", "=", "dataframe", "(", ")", "\n", "test_scores_improvement_plot", "(", "df", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.evaluation.compare_training.dataframe": [[16, 24], ["training.utils.history_path", "core.utils.load_dataframe", "training.utils.history_path", "core.utils.load_dataframe", "pandas.concat", "core.utils.package_path", "core.utils.package_path", "list", "itertools.chain"], "function", ["home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.training.utils.history_path", "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.core.utils.load_dataframe", "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.training.utils.history_path", "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.core.utils.load_dataframe", "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.core.utils.package_path", "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.core.utils.package_path"], ["def", "dataframe", "(", "baseline_training_run", ",", "finetuned_training_run", ")", ":", "\n", "    ", "path", "=", "history_path", "(", "package_path", "(", "baseline_model_package", ")", ",", "baseline_training_run", ")", "\n", "baseline_df", "=", "load_dataframe", "(", "path", ")", "\n", "path", "=", "history_path", "(", "package_path", "(", "finetuned_model_package", ")", ",", "finetuned_training_run", ")", "\n", "finetuned_df", "=", "load_dataframe", "(", "path", ")", "\n", "return", "pd", ".", "concat", "(", "[", "baseline_df", "[", "Metric", ".", "ACCURACY", ".", "value", "]", ",", "finetuned_df", "[", "Metric", ".", "ACCURACY", ".", "value", "]", ",", "\n", "baseline_df", "[", "Metric", ".", "VAL_ACCURACY", ".", "value", "]", ",", "finetuned_df", "[", "Metric", ".", "VAL_ACCURACY", ".", "value", "]", "]", ",", "axis", "=", "1", ",", "\n", "keys", "=", "list", "(", "itertools", ".", "chain", "(", "*", "COMPARE_TRAINING_COLUMNS", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.evaluation.compare_training.compare_training": [[26, 30], ["enumerate", "compare_training.dataframe", "plotting.plot.compare_training_plot"], "function", ["home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.evaluation.compare_training.dataframe", "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.plotting.plot.compare_training_plot"], ["", "def", "compare_training", "(", ")", ":", "\n", "    ", "for", "i", ",", "baseline_training_run", "in", "enumerate", "(", "BASELINE_TRAINING_RUNS", ")", ":", "\n", "        ", "df", "=", "dataframe", "(", "baseline_training_run", ",", "FINETUNED_TRAINING_RUNS", "[", "i", "]", ")", "\n", "compare_training_plot", "(", "df", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.evaluation.dataset_size_learning_curves.dataset_size_learning_curves": [[7, 13], ["pandas.DataFrame", "plotting.plot.dataset_size_learning_curves_plot"], "function", ["home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.plotting.plot.dataset_size_learning_curves_plot"], ["def", "dataset_size_learning_curves", "(", ")", ":", "\n", "    ", "df", "=", "DataFrame", "(", "{", "\n", "Metric", ".", "ACCURACY", ".", "value", ":", "[", "0.9989", ",", "0.9953", ",", "1.0", "]", ",", "\n", "Metric", ".", "VAL_ACCURACY", ".", "value", ":", "[", "0.3217", ",", "0.5128", ",", "0.6689", "]", "\n", "}", ")", "\n", "dataset_size_learning_curves_plot", "(", "df", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.finetuned_model.feature_maps._dataset": [[13, 28], ["datasets.tf_record_utils.tf_record_dataset", "dataset.batch.batch", "dataset.batch.map", "dataset.batch.unbatch", "dataset.batch.filter", "dataset.batch.batch", "dataset.batch.take", "tensorflow.math.equal", "tensorflow.math.equal"], "function", ["home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.datasets.tf_record_utils.tf_record_dataset"], ["def", "_dataset", "(", ")", ":", "\n", "    ", "\"\"\"Returns the `SIGNUM` dataset example for the word `dance`.\n\n    The dataset example corresponding to the label with index 420 and the signer with index 1 shall be returned.\n\n    Returns:\n        The dataset example for the word `dance`.\n    \"\"\"", "\n", "dataset", "=", "tf_record_dataset", "(", "DatasetName", ".", "SIGNUM", ",", "DatasetType", ".", "TRAIN", ")", "\n", "dataset", "=", "dataset", ".", "batch", "(", "1", ")", "\n", "dataset", "=", "dataset", ".", "map", "(", "transform_for_prediction", ")", "\n", "dataset", "=", "dataset", ".", "unbatch", "(", ")", "\n", "dataset", "=", "dataset", ".", "filter", "(", "lambda", "frames", ",", "label", ",", "signer", ":", "tf", ".", "math", ".", "equal", "(", "label", ",", "420", ")", "and", "tf", ".", "math", ".", "equal", "(", "signer", ",", "1", ")", ")", "\n", "dataset", "=", "dataset", ".", "batch", "(", "1", ")", "\n", "return", "dataset", ".", "take", "(", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.finetuned_model.feature_maps._model": [[30, 34], ["training.utils.model_path", "training.utils.load_model", "tensorflow.keras.models.Model", "pathlib.Path"], "function", ["home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.training.utils.model_path", "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.training.utils.load_model"], ["", "def", "_model", "(", ")", ":", "\n", "    ", "path", "=", "model_path", "(", "Path", "(", ")", ",", "TRAINING_RUNS", "[", "_TRAINING_RUNS_INDEX", "]", ")", "\n", "base_model", "=", "load_model", "(", "path", ")", "\n", "return", "Model", "(", "inputs", "=", "base_model", ".", "inputs", ",", "outputs", "=", "[", "base_model", ".", "layers", "[", "i", "]", ".", "output", "for", "i", "in", "_LAYER_INDICES", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.finetuned_model.feature_maps.plot_feature_maps": [[40, 48], ["feature_maps._dataset", "feature_maps._model", "_model.predict", "enumerate", "pathlib.Path().mkdir", "plotting.plot.feature_maps_plot", "pathlib.Path", "pathlib.Path"], "function", ["home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.finetuned_model.feature_maps._dataset", "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.finetuned_model.train._model", "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.plotting.plot.feature_maps_plot"], ["def", "plot_feature_maps", "(", ")", ":", "\n", "    ", "dataset", "=", "_dataset", "(", ")", "\n", "model", "=", "_model", "(", ")", "\n", "feature_maps", "=", "model", ".", "predict", "(", "dataset", ")", "\n", "for", "i", ",", "layer_index", "in", "enumerate", "(", "_LAYER_INDICES", ")", ":", "\n", "        ", "path", "=", "Path", "(", ")", "/", "'feature_maps'", "/", "f'layer_{layer_index:03d}'", "\n", "Path", "(", "path", ")", ".", "mkdir", "(", "parents", "=", "True", ",", "exist_ok", "=", "True", ")", "\n", "feature_maps_plot", "(", "feature_maps", "[", "i", "]", ",", "path", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.finetuned_model.train._train_dataset": [[18, 24], ["datasets.tf_record_utils.tf_record_dataset", "train_dataset.map.shuffle", "train_dataset.map.batch", "train_dataset.map.map", "train_dataset.map.prefetch"], "function", ["home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.datasets.tf_record_utils.tf_record_dataset"], ["train_dataset", "=", "train_dataset", ".", "map", "(", "transform_for_signum_model", ")", "\n", "return", "train_dataset", ".", "prefetch", "(", "2", ")", "\n", "\n", "\n", "", "def", "_validation_dataset", "(", ")", ":", "\n", "    ", "validation_dataset", "=", "tf_record_dataset", "(", "DatasetName", ".", "SIGNUM", ",", "DatasetType", ".", "VALIDATION", ")", "\n", "validation_dataset", "=", "validation_dataset", ".", "batch", "(", "32", ")", "\n"]], "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.finetuned_model.train._validation_dataset": [[26, 31], ["datasets.tf_record_utils.tf_record_dataset", "validation_dataset.map.batch", "validation_dataset.map.map", "validation_dataset.map.prefetch"], "function", ["home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.datasets.tf_record_utils.tf_record_dataset"], ["return", "validation_dataset", ".", "prefetch", "(", "2", ")", "\n", "\n", "\n", "", "def", "_model", "(", ")", ":", "\n", "    ", "model", "=", "Inflated3DInceptionV3", "(", "classes", "=", "N_CLASSES", ")", "\n", "load_inflated_imagenet_weights", "(", "model", ")", "\n"]], "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.finetuned_model.train._align_to_target_task": [[33, 37], ["tensorflow.keras.models.Model", "tensorflow.keras.layers.Dense"], "function", ["None"], ["metrics", "=", "[", "'accuracy'", "]", ")", "\n", "return", "model", "\n", "\n", "\n", "", "def", "train", "(", "path", ":", "Path", ")", ":", "\n"]], "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.finetuned_model.train._model": [[39, 46], ["training.utils.model_path", "training.utils.load_model", "train._align_to_target_task", "_align_to_target_task.compile", "core.utils.package_path", "tensorflow.keras.optimizers.SGD"], "function", ["home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.training.utils.model_path", "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.training.utils.load_model", "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.finetuned_model.train._align_to_target_task", "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.core.utils.package_path"], ["validation_dataset", "=", "_validation_dataset", "(", ")", "\n", "mc", "=", "model_checkpoint", "(", "Metric", ".", "VAL_LOSS", ",", "path", ")", "\n", "model", "=", "_model", "(", ")", "\n", "history", "=", "model", ".", "fit", "(", "train_dataset", ",", "validation_data", "=", "validation_dataset", ",", "epochs", "=", "40", ",", "callbacks", "=", "[", "mc", "]", ")", "\n", "return", "history", ".", "history", "\n", "\n", "\n", "", "if", "__name__", "==", "'__main__'", ":", "\n"]], "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.finetuned_model.train.train": [[48, 55], ["train._train_dataset", "train._validation_dataset", "training.callbacks.model_checkpoint", "train._model", "_model.fit"], "function", ["home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.finetuned_model.train._train_dataset", "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.finetuned_model.train._validation_dataset", "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.training.callbacks.model_checkpoint", "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.finetuned_model.train._model"], ["history", "=", "train", "(", "path", ")", "\n", "save_history", "(", "history", ",", "path", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.finetuned_model.test._test_dataset": [[10, 15], ["datasets.tf_record_utils.tf_record_dataset", "test_dataset.map.batch", "test_dataset.map.map", "test_dataset.map.prefetch"], "function", ["home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.datasets.tf_record_utils.tf_record_dataset"], ["def", "_test_dataset", "(", ")", ":", "\n", "    ", "test_dataset", "=", "tf_record_dataset", "(", "DatasetName", ".", "SIGNUM", ",", "DatasetType", ".", "TEST", ")", "\n", "test_dataset", "=", "test_dataset", ".", "batch", "(", "32", ")", "\n", "test_dataset", "=", "test_dataset", ".", "map", "(", "transform_for_signum_model", ")", "\n", "return", "test_dataset", ".", "prefetch", "(", "2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.finetuned_model.test.test": [[17, 27], ["test._test_dataset", "training.utils.model_path", "training.utils.load_model", "training.utils.load_model.evaluate", "losses.append", "accuracies.append", "pathlib.Path"], "function", ["home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.finetuned_model.test._test_dataset", "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.training.utils.model_path", "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.training.utils.load_model"], ["", "def", "test", "(", ")", ":", "\n", "    ", "losses", ",", "accuracies", "=", "[", "]", ",", "[", "]", "\n", "test_dataset", "=", "_test_dataset", "(", ")", "\n", "for", "training_run", "in", "TRAINING_RUNS", ":", "\n", "        ", "path", "=", "model_path", "(", "Path", "(", ")", ",", "training_run", ")", "\n", "model", "=", "load_model", "(", "path", ")", "\n", "loss", ",", "accuracy", "=", "model", ".", "evaluate", "(", "test_dataset", ")", "\n", "losses", ".", "append", "(", "loss", ")", "\n", "accuracies", ".", "append", "(", "accuracy", ")", "\n", "", "return", "losses", ",", "accuracies", "\n", "\n"]], "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.finetuned_model.learning_curves.plot_learning_curves": [[9, 14], ["training.utils.history_path", "core.utils.load_dataframe", "plotting.plot.loss_accuracy_plot", "pathlib.Path"], "function", ["home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.training.utils.history_path", "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.core.utils.load_dataframe", "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.plotting.plot.loss_accuracy_plot"], ["def", "plot_learning_curves", "(", ")", ":", "\n", "    ", "for", "training_run", "in", "TRAINING_RUNS", ":", "\n", "        ", "path", "=", "history_path", "(", "Path", "(", ")", ",", "training_run", ")", "\n", "df", "=", "load_dataframe", "(", "path", ")", "\n", "loss_accuracy_plot", "(", "df", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.core.utils.package_path": [[7, 17], ["pathlib.Path"], "function", ["None"], ["\n", "def", "_frame_positions", "(", "start", ":", "int", ",", "end", ":", "int", ")", ":", "\n", "    "]], "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.core.utils.save_dataframe": [[19, 27], ["df.to_pickle"], "function", ["None"], ["\n", "range_", "=", "end", "-", "start", "+", "1", "\n", "positions", "=", "np", ".", "random", ".", "choice", "(", "range_", ",", "_N_TIME_STEPS", ",", "replace", "=", "range_", "<", "_N_TIME_STEPS", ")", "+", "start", "\n", "positions", ".", "sort", "(", ")", "\n", "return", "positions", "\n", "\n", "\n"]], "home.repos.pwc.inspect_result.rtoengi_transfer-learning-for-sign-language-recognition.core.utils.load_dataframe": [[29, 39], ["pandas.read_pickle"], "function", ["None"], ["    "]]}