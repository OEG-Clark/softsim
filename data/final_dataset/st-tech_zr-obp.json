{"home.repos.pwc.inspect_result.st-tech_zr-obp.tests.test_utils.test_sample_action_fast": [[7, 22], ["obp.utils.softmax", "list", "numpy.arange", "numpy.concatenate", "numpy.arange", "numpy.random.normal", "list.append", "numpy.isclose().all", "numpy.unique", "obp.utils.sample_action_fast", "numpy.isclose"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.softmax", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.sample_action_fast"], ["def", "test_sample_action_fast", "(", ")", ":", "\n", "    ", "n_rounds", "=", "10", "\n", "n_actions", "=", "5", "\n", "n_sim", "=", "100000", "\n", "\n", "true_probs", "=", "softmax", "(", "np", ".", "random", ".", "normal", "(", "size", "=", "(", "n_rounds", ",", "n_actions", ")", ")", ")", "\n", "sampled_action_list", "=", "list", "(", ")", "\n", "for", "_", "in", "np", ".", "arange", "(", "n_sim", ")", ":", "\n", "        ", "sampled_action_list", ".", "append", "(", "sample_action_fast", "(", "true_probs", ")", "[", ":", ",", "np", ".", "newaxis", "]", ")", "\n", "\n", "", "sampled_action_arr", "=", "np", ".", "concatenate", "(", "sampled_action_list", ",", "1", ")", "\n", "for", "i", "in", "np", ".", "arange", "(", "n_rounds", ")", ":", "\n", "        ", "sampled_action_counts", "=", "np", ".", "unique", "(", "sampled_action_arr", "[", "i", "]", ",", "return_counts", "=", "True", ")", "[", "1", "]", "\n", "empirical_probs", "=", "sampled_action_counts", "/", "n_sim", "\n", "assert", "np", ".", "isclose", "(", "true_probs", "[", "i", "]", ",", "empirical_probs", ",", "rtol", "=", "5e-2", ",", "atol", "=", "1e-3", ")", ".", "all", "(", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.test_synthetic_multi.test_synthetic_multi_init_using_invalid_inputs": [[311, 339], ["pytest.mark.parametrize", "pytest.raises", "obp.dataset.SyntheticMultiLoggersBanditDataset"], "function", ["None"], ["@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"n_actions, dim_context, reward_type, reward_std, betas, rhos, n_deficient_actions, action_context, random_state, err, description\"", ",", "\n", "invalid_input_of_init", ",", "\n", ")", "\n", "def", "test_synthetic_multi_init_using_invalid_inputs", "(", "\n", "n_actions", ",", "\n", "dim_context", ",", "\n", "reward_type", ",", "\n", "reward_std", ",", "\n", "betas", ",", "\n", "rhos", ",", "\n", "n_deficient_actions", ",", "\n", "action_context", ",", "\n", "random_state", ",", "\n", "err", ",", "\n", "description", ",", "\n", ")", ":", "\n", "    ", "with", "pytest", ".", "raises", "(", "err", ",", "match", "=", "f\"{description}*\"", ")", ":", "\n", "        ", "_", "=", "SyntheticMultiLoggersBanditDataset", "(", "\n", "n_actions", "=", "n_actions", ",", "\n", "dim_context", "=", "dim_context", ",", "\n", "reward_type", "=", "reward_type", ",", "\n", "reward_std", "=", "reward_std", ",", "\n", "betas", "=", "betas", ",", "\n", "rhos", "=", "rhos", ",", "\n", "n_deficient_actions", "=", "n_deficient_actions", ",", "\n", "action_context", "=", "action_context", ",", "\n", "random_state", "=", "random_state", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.test_synthetic_multi.test_synthetic_obtain_batch_bandit_feedback": [[342, 413], ["pytest.raises", "obp.dataset.SyntheticMultiLoggersBanditDataset", "obp.dataset.SyntheticMultiLoggersBanditDataset.obtain_batch_bandit_feedback", "pytest.raises", "obp.dataset.SyntheticMultiLoggersBanditDataset", "obp.dataset.SyntheticMultiLoggersBanditDataset.obtain_batch_bandit_feedback", "obp.dataset.SyntheticMultiLoggersBanditDataset", "obp.dataset.SyntheticMultiLoggersBanditDataset.obtain_batch_bandit_feedback", "numpy.allclose", "numpy.allclose", "len", "[].sum", "numpy.ones", "[].sum", "numpy.ones", "len", "len", "len", "len", "len"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.multiclass.MultiClassToBanditReduction.obtain_batch_bandit_feedback", "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.multiclass.MultiClassToBanditReduction.obtain_batch_bandit_feedback", "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.multiclass.MultiClassToBanditReduction.obtain_batch_bandit_feedback"], ["", "", "def", "test_synthetic_obtain_batch_bandit_feedback", "(", ")", ":", "\n", "    ", "betas", "=", "[", "-", "1", ",", "0", ",", "1", "]", "\n", "rhos", "=", "[", "1", ",", "1", ",", "1", "]", "\n", "# n_rounds", "\n", "with", "pytest", ".", "raises", "(", "ValueError", ")", ":", "\n", "        ", "dataset", "=", "SyntheticMultiLoggersBanditDataset", "(", "\n", "n_actions", "=", "2", ",", "betas", "=", "betas", ",", "rhos", "=", "rhos", "\n", ")", "\n", "dataset", ".", "obtain_batch_bandit_feedback", "(", "n_rounds", "=", "0", ")", "\n", "\n", "", "with", "pytest", ".", "raises", "(", "TypeError", ")", ":", "\n", "        ", "dataset", "=", "SyntheticMultiLoggersBanditDataset", "(", "\n", "n_actions", "=", "2", ",", "betas", "=", "betas", ",", "rhos", "=", "rhos", "\n", ")", "\n", "dataset", ".", "obtain_batch_bandit_feedback", "(", "n_rounds", "=", "\"3\"", ")", "\n", "\n", "# bandit feedback", "\n", "", "n_rounds", "=", "10", "\n", "n_actions", "=", "5", "\n", "for", "n_deficient_actions", "in", "[", "0", ",", "2", "]", ":", "\n", "        ", "dataset", "=", "SyntheticMultiLoggersBanditDataset", "(", "\n", "n_actions", "=", "n_actions", ",", "\n", "betas", "=", "betas", ",", "\n", "rhos", "=", "rhos", ",", "\n", "n_deficient_actions", "=", "n_deficient_actions", ",", "\n", ")", "\n", "bandit_feedback", "=", "dataset", ".", "obtain_batch_bandit_feedback", "(", "n_rounds", "=", "n_rounds", ")", "\n", "assert", "bandit_feedback", "[", "\"n_rounds\"", "]", "==", "n_rounds", "\n", "assert", "bandit_feedback", "[", "\"n_actions\"", "]", "==", "n_actions", "\n", "assert", "bandit_feedback", "[", "\"n_strata\"", "]", "==", "len", "(", "betas", ")", "\n", "assert", "(", "\n", "bandit_feedback", "[", "\"context\"", "]", ".", "shape", "[", "0", "]", "==", "n_rounds", "# n_rounds", "\n", "and", "bandit_feedback", "[", "\"context\"", "]", ".", "shape", "[", "1", "]", "==", "1", "# default dim_context", "\n", ")", "\n", "assert", "(", "\n", "bandit_feedback", "[", "\"action_context\"", "]", ".", "shape", "[", "0", "]", "==", "n_actions", "\n", "and", "bandit_feedback", "[", "\"action_context\"", "]", ".", "shape", "[", "1", "]", "==", "n_actions", "\n", ")", "\n", "assert", "(", "\n", "bandit_feedback", "[", "\"action\"", "]", ".", "ndim", "==", "1", "\n", "and", "len", "(", "bandit_feedback", "[", "\"action\"", "]", ")", "==", "n_rounds", "\n", ")", "\n", "assert", "(", "\n", "bandit_feedback", "[", "\"stratum_idx\"", "]", ".", "ndim", "==", "1", "\n", "and", "len", "(", "bandit_feedback", "[", "\"stratum_idx\"", "]", ")", "==", "n_rounds", "\n", ")", "\n", "assert", "bandit_feedback", "[", "\"position\"", "]", "is", "None", "\n", "assert", "(", "\n", "bandit_feedback", "[", "\"reward\"", "]", ".", "ndim", "==", "1", "\n", "and", "len", "(", "bandit_feedback", "[", "\"reward\"", "]", ")", "==", "n_rounds", "\n", ")", "\n", "assert", "(", "\n", "bandit_feedback", "[", "\"expected_reward\"", "]", ".", "shape", "[", "0", "]", "==", "n_rounds", "\n", "and", "bandit_feedback", "[", "\"expected_reward\"", "]", ".", "shape", "[", "1", "]", "==", "n_actions", "\n", ")", "\n", "assert", "(", "\n", "bandit_feedback", "[", "\"pi_b\"", "]", ".", "shape", "[", "0", "]", "==", "n_rounds", "\n", "and", "bandit_feedback", "[", "\"pi_b\"", "]", ".", "shape", "[", "1", "]", "==", "n_actions", "\n", ")", "\n", "assert", "np", ".", "allclose", "(", "bandit_feedback", "[", "\"pi_b\"", "]", "[", ":", ",", ":", ",", "0", "]", ".", "sum", "(", "1", ")", ",", "np", ".", "ones", "(", "n_rounds", ")", ")", "\n", "assert", "(", "bandit_feedback", "[", "\"pi_b\"", "]", "==", "0", ")", ".", "sum", "(", ")", "==", "n_deficient_actions", "*", "n_rounds", "\n", "assert", "np", ".", "allclose", "(", "\n", "bandit_feedback", "[", "\"pi_b_avg\"", "]", "[", ":", ",", ":", ",", "0", "]", ".", "sum", "(", "1", ")", ",", "np", ".", "ones", "(", "n_rounds", ")", "\n", ")", "\n", "assert", "(", "\n", "bandit_feedback", "[", "\"pscore\"", "]", ".", "ndim", "==", "1", "\n", "and", "len", "(", "bandit_feedback", "[", "\"pscore\"", "]", ")", "==", "n_rounds", "\n", ")", "\n", "assert", "(", "\n", "bandit_feedback", "[", "\"pscore_avg\"", "]", ".", "ndim", "==", "1", "\n", "and", "len", "(", "bandit_feedback", "[", "\"pscore_avg\"", "]", ")", "==", "n_rounds", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.test_synthetic_slate.test_synthetic_slate_init_using_invalid_inputs": [[201, 229], ["pytest.mark.parametrize", "pytest.raises", "obp.dataset.SyntheticSlateBanditDataset"], "function", ["None"], ["@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"n_unique_action, len_list, dim_context, reward_type, reward_structure, decay_function, click_model, eta, random_state, err, description\"", ",", "\n", "invalid_input_of_init", ",", "\n", ")", "\n", "def", "test_synthetic_slate_init_using_invalid_inputs", "(", "\n", "n_unique_action", ",", "\n", "len_list", ",", "\n", "dim_context", ",", "\n", "reward_type", ",", "\n", "reward_structure", ",", "\n", "decay_function", ",", "\n", "click_model", ",", "\n", "eta", ",", "\n", "random_state", ",", "\n", "err", ",", "\n", "description", ",", "\n", ")", ":", "\n", "    ", "with", "pytest", ".", "raises", "(", "err", ",", "match", "=", "f\"{description}*\"", ")", ":", "\n", "        ", "_", "=", "SyntheticSlateBanditDataset", "(", "\n", "n_unique_action", "=", "n_unique_action", ",", "\n", "len_list", "=", "len_list", ",", "\n", "dim_context", "=", "dim_context", ",", "\n", "reward_type", "=", "reward_type", ",", "\n", "reward_structure", "=", "reward_structure", ",", "\n", "decay_function", "=", "decay_function", ",", "\n", "click_model", "=", "click_model", ",", "\n", "eta", "=", "eta", ",", "\n", "random_state", "=", "random_state", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.test_synthetic_slate.check_slate_bandit_feedback": [[232, 313], ["pandas.DataFrame", "bandit_feedback_df.sort_values().reset_index().copy.sort_values().reset_index().copy", "len", "bandit_feedback_df.sort_values().reset_index().copy.duplicated().sum", "bandit_feedback_df.sort_values().reset_index().copy.groupby().apply", "bandit_feedback_df.sort_values().reset_index().copy.drop_duplicates", "pscore_columns.append", "pscore_columns.append", "bandit_feedback_df.sort_values().reset_index().copy.sort_values().reset_index", "bandit_feedback_df.sort_values().reset_index().copy.duplicated().sum", "invalid_pscore_flgs.sum", "[].expanding().min", "bandit_feedback_df.sort_values().reset_index().copy.duplicated", "bandit_feedback_df.sort_values().reset_index().copy.groupby", "bandit_feedback_df.sort_values().reset_index().copy.sort_values", "bandit_feedback_df.sort_values().reset_index().copy.duplicated", "[].expanding", "x[].unique", "bandit_feedback_df.sort_values().reset_index().copy.groupby"], "function", ["None"], ["", "", "def", "check_slate_bandit_feedback", "(", "\n", "bandit_feedback", ":", "BanditFeedback", ",", "is_factorizable", ":", "bool", "=", "False", "\n", ")", ":", "\n", "# check pscore columns", "\n", "    ", "pscore_columns", ":", "List", "[", "str", "]", "=", "[", "]", "\n", "pscore_candidate_columns", "=", "[", "\n", "\"pscore_cascade\"", ",", "\n", "\"pscore\"", ",", "\n", "\"pscore_item_position\"", ",", "\n", "]", "\n", "for", "column", "in", "pscore_candidate_columns", ":", "\n", "        ", "if", "column", "in", "bandit_feedback", "and", "bandit_feedback", "[", "column", "]", "is", "not", "None", ":", "\n", "            ", "pscore_columns", ".", "append", "(", "column", ")", "\n", "", "else", ":", "\n", "            ", "pscore_columns", ".", "append", "(", "column", ")", "\n", "", "", "assert", "(", "\n", "len", "(", "pscore_columns", ")", ">", "0", "\n", ")", ",", "f\"bandit feedback must contain at least one of the following pscore columns: {pscore_candidate_columns}\"", "\n", "bandit_feedback_df", "=", "pd", ".", "DataFrame", "(", ")", "\n", "for", "column", "in", "[", "\"slate_id\"", ",", "\"position\"", ",", "\"action\"", "]", "+", "pscore_columns", ":", "\n", "        ", "bandit_feedback_df", "[", "column", "]", "=", "bandit_feedback", "[", "column", "]", "\n", "# sort dataframe", "\n", "", "bandit_feedback_df", "=", "(", "\n", "bandit_feedback_df", ".", "sort_values", "(", "[", "\"slate_id\"", ",", "\"position\"", "]", ")", "\n", ".", "reset_index", "(", "drop", "=", "True", ")", "\n", ".", "copy", "(", ")", "\n", ")", "\n", "# check uniqueness", "\n", "assert", "(", "\n", "bandit_feedback_df", ".", "duplicated", "(", "[", "\"slate_id\"", ",", "\"position\"", "]", ")", ".", "sum", "(", ")", "==", "0", "\n", ")", ",", "\"`position` must not be duplicated in each slate\"", "\n", "assert", "(", "\n", "bandit_feedback_df", ".", "duplicated", "(", "[", "\"slate_id\"", ",", "\"action\"", "]", ")", ".", "sum", "(", ")", "==", "0", "\n", "if", "not", "is_factorizable", "\n", "else", "True", "\n", ")", ",", "\"action must not be duplicated in each slate\"", "\n", "# check pscores", "\n", "for", "column", "in", "pscore_columns", ":", "\n", "        ", "invalid_pscore_flgs", "=", "(", "bandit_feedback_df", "[", "column", "]", "<", "0", ")", "|", "(", "\n", "bandit_feedback_df", "[", "column", "]", ">", "1", "\n", ")", "\n", "assert", "invalid_pscore_flgs", ".", "sum", "(", ")", "==", "0", ",", "\"the range of pscores must be [0, 1]\"", "\n", "", "if", "\"pscore_cascade\"", "in", "pscore_columns", "and", "\"pscore\"", "in", "pscore_columns", ":", "\n", "        ", "assert", "(", "\n", "bandit_feedback_df", "[", "\"pscore_cascade\"", "]", "<", "bandit_feedback_df", "[", "\"pscore\"", "]", "\n", ")", ".", "sum", "(", ")", "==", "0", ",", "\"pscore must be smaller than or equal to pscore_cascade\"", "\n", "", "if", "\"pscore_item_position\"", "in", "pscore_columns", "and", "\"pscore\"", "in", "pscore_columns", ":", "\n", "        ", "assert", "(", "\n", "bandit_feedback_df", "[", "\"pscore_item_position\"", "]", "<", "bandit_feedback_df", "[", "\"pscore\"", "]", "\n", ")", ".", "sum", "(", ")", "==", "0", ",", "\"pscore must be smaller than or equal to pscore_item_position\"", "\n", "", "if", "\"pscore_item_position\"", "in", "pscore_columns", "and", "\"pscore_cascade\"", "in", "pscore_columns", ":", "\n", "        ", "assert", "(", "\n", "bandit_feedback_df", "[", "\"pscore_item_position\"", "]", "\n", "<", "bandit_feedback_df", "[", "\"pscore_cascade\"", "]", "\n", ")", ".", "sum", "(", ")", "==", "0", ",", "(", "\n", "\"pscore_cascade must be smaller than or equal to pscore_item_position\"", "\n", ")", "\n", "", "if", "\"pscore_cascade\"", "in", "pscore_columns", ":", "\n", "        ", "previous_minimum_pscore_cascade", "=", "(", "\n", "bandit_feedback_df", ".", "groupby", "(", "\"slate_id\"", ")", "[", "\"pscore_cascade\"", "]", "\n", ".", "expanding", "(", ")", "\n", ".", "min", "(", ")", "\n", ".", "values", "\n", ")", "\n", "assert", "(", "\n", "previous_minimum_pscore_cascade", "<", "bandit_feedback_df", "[", "\"pscore_cascade\"", "]", "\n", ")", ".", "sum", "(", ")", "==", "0", ",", "\"pscore_cascade must be non-decresing sequence in each slate\"", "\n", "", "if", "\"pscore\"", "in", "pscore_columns", ":", "\n", "        ", "count_pscore_in_expression", "=", "bandit_feedback_df", ".", "groupby", "(", "\"slate_id\"", ")", ".", "apply", "(", "\n", "lambda", "x", ":", "x", "[", "\"pscore\"", "]", ".", "unique", "(", ")", ".", "shape", "[", "0", "]", "\n", ")", "\n", "assert", "(", "\n", "count_pscore_in_expression", "!=", "1", "\n", ")", ".", "sum", "(", ")", "==", "0", ",", "\"`pscore` must be unique in each slate\"", "\n", "", "if", "\"pscore\"", "in", "pscore_columns", "and", "\"pscore_cascade\"", "in", "pscore_columns", ":", "\n", "        ", "last_slot_feedback_df", "=", "bandit_feedback_df", ".", "drop_duplicates", "(", "\n", "\"slate_id\"", ",", "keep", "=", "\"last\"", "\n", ")", "\n", "assert", "(", "\n", "last_slot_feedback_df", "[", "\"pscore\"", "]", "!=", "last_slot_feedback_df", "[", "\"pscore_cascade\"", "]", "\n", ")", ".", "sum", "(", ")", "==", "0", ",", "\"pscore must be the same as pscore_cascade in the last slot\"", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.test_synthetic_slate.test_synthetic_slate_obtain_batch_bandit_feedback_using_uniform_random_behavior_policy": [[315, 359], ["obp.dataset.SyntheticSlateBanditDataset", "obp.dataset.SyntheticSlateBanditDataset.obtain_batch_bandit_feedback", "test_synthetic_slate.check_slate_bandit_feedback", "pandas.DataFrame", "numpy.allclose", "numpy.arange", "numpy.allclose", "numpy.allclose", "bandit_feedback_df[].unique", "pscore_cascade.append", "numpy.tile", "bandit_feedback_df[].unique", "bandit_feedback_df[].unique"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.multiclass.MultiClassToBanditReduction.obtain_batch_bandit_feedback", "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.test_synthetic_slate.check_slate_bandit_feedback"], ["", "", "def", "test_synthetic_slate_obtain_batch_bandit_feedback_using_uniform_random_behavior_policy", "(", ")", ":", "\n", "# set parameters", "\n", "    ", "n_unique_action", "=", "10", "\n", "len_list", "=", "3", "\n", "dim_context", "=", "2", "\n", "reward_type", "=", "\"binary\"", "\n", "random_state", "=", "12345", "\n", "n_rounds", "=", "100", "\n", "dataset", "=", "SyntheticSlateBanditDataset", "(", "\n", "n_unique_action", "=", "n_unique_action", ",", "\n", "len_list", "=", "len_list", ",", "\n", "dim_context", "=", "dim_context", ",", "\n", "reward_type", "=", "reward_type", ",", "\n", "random_state", "=", "random_state", ",", "\n", ")", "\n", "# obtain feedback", "\n", "bandit_feedback", "=", "dataset", ".", "obtain_batch_bandit_feedback", "(", "n_rounds", "=", "n_rounds", ")", "\n", "# check slate bandit feedback (common test)", "\n", "check_slate_bandit_feedback", "(", "bandit_feedback", "=", "bandit_feedback", ")", "\n", "pscore_columns", "=", "[", "\n", "\"pscore_cascade\"", ",", "\n", "\"pscore\"", ",", "\n", "\"pscore_item_position\"", ",", "\n", "]", "\n", "bandit_feedback_df", "=", "pd", ".", "DataFrame", "(", ")", "\n", "for", "column", "in", "[", "\"slate_id\"", ",", "\"position\"", ",", "\"action\"", "]", "+", "pscore_columns", ":", "\n", "        ", "bandit_feedback_df", "[", "column", "]", "=", "bandit_feedback", "[", "column", "]", "\n", "# check pscore marginal", "\n", "", "pscore_item_position", "=", "1", "/", "n_unique_action", "\n", "assert", "np", ".", "allclose", "(", "\n", "bandit_feedback_df", "[", "\"pscore_item_position\"", "]", ".", "unique", "(", ")", ",", "pscore_item_position", "\n", ")", ",", "f\"pscore_item_position must be [{pscore_item_position}], but {bandit_feedback_df['pscore_item_position'].unique()}\"", "\n", "# check pscore joint", "\n", "pscore_cascade", "=", "[", "]", "\n", "pscore_above", "=", "1.0", "\n", "for", "pos_", "in", "np", ".", "arange", "(", "len_list", ")", ":", "\n", "        ", "pscore_above", "*=", "1.0", "/", "(", "n_unique_action", "-", "pos_", ")", "\n", "pscore_cascade", ".", "append", "(", "pscore_above", ")", "\n", "", "assert", "np", ".", "allclose", "(", "\n", "bandit_feedback_df", "[", "\"pscore_cascade\"", "]", ",", "np", ".", "tile", "(", "pscore_cascade", ",", "n_rounds", ")", "\n", ")", ",", "f\"pscore_cascade must be {pscore_cascade} for all slates\"", "\n", "assert", "np", ".", "allclose", "(", "\n", "bandit_feedback_df", "[", "\"pscore\"", "]", ".", "unique", "(", ")", ",", "[", "pscore_above", "]", "\n", ")", ",", "f\"pscore must be {pscore_above} for all slates\"", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.test_synthetic_slate.test_synthetic_slate_obtain_batch_bandit_feedback_using_uniform_random_factorizable_behavior_policy": [[361, 406], ["obp.dataset.SyntheticSlateBanditDataset", "obp.dataset.SyntheticSlateBanditDataset.obtain_batch_bandit_feedback", "test_synthetic_slate.check_slate_bandit_feedback", "pandas.DataFrame", "numpy.allclose", "numpy.arange", "numpy.allclose", "numpy.allclose", "bandit_feedback_df[].unique", "pscore_cascade.append", "numpy.tile", "bandit_feedback_df[].unique", "bandit_feedback_df[].unique"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.multiclass.MultiClassToBanditReduction.obtain_batch_bandit_feedback", "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.test_synthetic_slate.check_slate_bandit_feedback"], ["", "def", "test_synthetic_slate_obtain_batch_bandit_feedback_using_uniform_random_factorizable_behavior_policy", "(", ")", ":", "\n", "# set parameters", "\n", "    ", "n_unique_action", "=", "10", "\n", "len_list", "=", "3", "\n", "dim_context", "=", "2", "\n", "reward_type", "=", "\"binary\"", "\n", "random_state", "=", "12345", "\n", "n_rounds", "=", "100", "\n", "dataset", "=", "SyntheticSlateBanditDataset", "(", "\n", "n_unique_action", "=", "n_unique_action", ",", "\n", "len_list", "=", "len_list", ",", "\n", "dim_context", "=", "dim_context", ",", "\n", "reward_type", "=", "reward_type", ",", "\n", "is_factorizable", "=", "True", ",", "\n", "random_state", "=", "random_state", ",", "\n", ")", "\n", "# obtain feedback", "\n", "bandit_feedback", "=", "dataset", ".", "obtain_batch_bandit_feedback", "(", "n_rounds", "=", "n_rounds", ")", "\n", "# check slate bandit feedback (common test)", "\n", "check_slate_bandit_feedback", "(", "bandit_feedback", "=", "bandit_feedback", ",", "is_factorizable", "=", "True", ")", "\n", "pscore_columns", "=", "[", "\n", "\"pscore_cascade\"", ",", "\n", "\"pscore\"", ",", "\n", "\"pscore_item_position\"", ",", "\n", "]", "\n", "bandit_feedback_df", "=", "pd", ".", "DataFrame", "(", ")", "\n", "for", "column", "in", "[", "\"slate_id\"", ",", "\"position\"", ",", "\"action\"", "]", "+", "pscore_columns", ":", "\n", "        ", "bandit_feedback_df", "[", "column", "]", "=", "bandit_feedback", "[", "column", "]", "\n", "# check pscore marginal", "\n", "", "pscore_item_position", "=", "1", "/", "n_unique_action", "\n", "assert", "np", ".", "allclose", "(", "\n", "bandit_feedback_df", "[", "\"pscore_item_position\"", "]", ".", "unique", "(", ")", ",", "pscore_item_position", "\n", ")", ",", "f\"pscore_item_position must be [{pscore_item_position}], but {bandit_feedback_df['pscore_item_position'].unique()}\"", "\n", "# check pscore joint", "\n", "pscore_cascade", "=", "[", "]", "\n", "pscore_above", "=", "1.0", "\n", "for", "pos_", "in", "np", ".", "arange", "(", "len_list", ")", ":", "\n", "        ", "pscore_above", "*=", "1.0", "/", "n_unique_action", "\n", "pscore_cascade", ".", "append", "(", "pscore_above", ")", "\n", "", "assert", "np", ".", "allclose", "(", "\n", "bandit_feedback_df", "[", "\"pscore_cascade\"", "]", ",", "np", ".", "tile", "(", "pscore_cascade", ",", "n_rounds", ")", "\n", ")", ",", "f\"pscore_cascade must be {pscore_cascade} for all slates\"", "\n", "assert", "np", ".", "allclose", "(", "\n", "bandit_feedback_df", "[", "\"pscore\"", "]", ".", "unique", "(", ")", ",", "[", "pscore_above", "]", "\n", ")", ",", "f\"pscore must be {pscore_above} for all slates\"", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.test_synthetic_slate.test_synthetic_slate_obtain_batch_bandit_feedback_using_uniform_random_behavior_policy_largescale": [[408, 432], ["obp.dataset.SyntheticSlateBanditDataset", "obp.dataset.SyntheticSlateBanditDataset.obtain_batch_bandit_feedback", "test_synthetic_slate.check_slate_bandit_feedback", "numpy.allclose", "numpy.unique", "numpy.unique"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.multiclass.MultiClassToBanditReduction.obtain_batch_bandit_feedback", "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.test_synthetic_slate.check_slate_bandit_feedback"], ["", "def", "test_synthetic_slate_obtain_batch_bandit_feedback_using_uniform_random_behavior_policy_largescale", "(", ")", ":", "\n", "# set parameters", "\n", "    ", "n_unique_action", "=", "100", "\n", "len_list", "=", "10", "\n", "dim_context", "=", "2", "\n", "reward_type", "=", "\"binary\"", "\n", "random_state", "=", "12345", "\n", "n_rounds", "=", "10000", "\n", "dataset", "=", "SyntheticSlateBanditDataset", "(", "\n", "n_unique_action", "=", "n_unique_action", ",", "\n", "len_list", "=", "len_list", ",", "\n", "dim_context", "=", "dim_context", ",", "\n", "reward_type", "=", "reward_type", ",", "\n", "random_state", "=", "random_state", ",", "\n", ")", "\n", "# obtain feedback", "\n", "bandit_feedback", "=", "dataset", ".", "obtain_batch_bandit_feedback", "(", "n_rounds", "=", "n_rounds", ")", "\n", "# check slate bandit feedback (common test)", "\n", "check_slate_bandit_feedback", "(", "bandit_feedback", "=", "bandit_feedback", ")", "\n", "# check pscore marginal", "\n", "pscore_item_position", "=", "1", "/", "n_unique_action", "\n", "assert", "np", ".", "allclose", "(", "\n", "np", ".", "unique", "(", "bandit_feedback", "[", "\"pscore_item_position\"", "]", ")", ",", "pscore_item_position", "\n", ")", ",", "f\"pscore_item_position must be [{pscore_item_position}], but {np.unique(bandit_feedback['pscore_item_position'])}\"", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.test_synthetic_slate.test_synthetic_slate_obtain_batch_bandit_feedback_using_linear_behavior_policy": [[434, 471], ["obp.dataset.SyntheticSlateBanditDataset", "obp.dataset.SyntheticSlateBanditDataset.obtain_batch_bandit_feedback", "test_synthetic_slate.check_slate_bandit_feedback", "pandas.DataFrame", "print", "pytest.raises", "obp.dataset.SyntheticSlateBanditDataset.obtain_batch_bandit_feedback", "pytest.raises", "obp.dataset.SyntheticSlateBanditDataset.obtain_batch_bandit_feedback", "[].describe", "set", "set", "numpy.unique", "pd.DataFrame.groupby"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.multiclass.MultiClassToBanditReduction.obtain_batch_bandit_feedback", "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.test_synthetic_slate.check_slate_bandit_feedback", "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.multiclass.MultiClassToBanditReduction.obtain_batch_bandit_feedback", "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.multiclass.MultiClassToBanditReduction.obtain_batch_bandit_feedback"], ["", "def", "test_synthetic_slate_obtain_batch_bandit_feedback_using_linear_behavior_policy", "(", ")", ":", "\n", "# set parameters", "\n", "    ", "n_unique_action", "=", "10", "\n", "len_list", "=", "3", "\n", "dim_context", "=", "2", "\n", "reward_type", "=", "\"binary\"", "\n", "random_state", "=", "12345", "\n", "n_rounds", "=", "100", "\n", "dataset", "=", "SyntheticSlateBanditDataset", "(", "\n", "n_unique_action", "=", "n_unique_action", ",", "\n", "len_list", "=", "len_list", ",", "\n", "dim_context", "=", "dim_context", ",", "\n", "reward_type", "=", "reward_type", ",", "\n", "random_state", "=", "random_state", ",", "\n", "behavior_policy_function", "=", "linear_behavior_policy_logit", ",", "\n", ")", "\n", "with", "pytest", ".", "raises", "(", "ValueError", ")", ":", "\n", "        ", "_", "=", "dataset", ".", "obtain_batch_bandit_feedback", "(", "n_rounds", "=", "-", "1", ")", "\n", "", "with", "pytest", ".", "raises", "(", "TypeError", ")", ":", "\n", "        ", "_", "=", "dataset", ".", "obtain_batch_bandit_feedback", "(", "n_rounds", "=", "\"a\"", ")", "\n", "\n", "# obtain feedback", "\n", "", "bandit_feedback", "=", "dataset", ".", "obtain_batch_bandit_feedback", "(", "n_rounds", "=", "n_rounds", ")", "\n", "# check slate bandit feedback (common test)", "\n", "check_slate_bandit_feedback", "(", "bandit_feedback", "=", "bandit_feedback", ")", "\n", "# print reward", "\n", "pscore_columns", "=", "[", "\n", "\"pscore_cascade\"", ",", "\n", "\"pscore\"", ",", "\n", "\"pscore_item_position\"", ",", "\n", "]", "\n", "bandit_feedback_df", "=", "pd", ".", "DataFrame", "(", ")", "\n", "for", "column", "in", "[", "\"slate_id\"", ",", "\"position\"", ",", "\"action\"", ",", "\"reward\"", "]", "+", "pscore_columns", ":", "\n", "        ", "bandit_feedback_df", "[", "column", "]", "=", "bandit_feedback", "[", "column", "]", "\n", "", "print", "(", "bandit_feedback_df", ".", "groupby", "(", "\"position\"", ")", "[", "\"reward\"", "]", ".", "describe", "(", ")", ")", "\n", "if", "reward_type", "==", "\"binary\"", ":", "\n", "        ", "assert", "set", "(", "np", ".", "unique", "(", "bandit_feedback", "[", "\"reward\"", "]", ")", ")", "==", "set", "(", "[", "0", ",", "1", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.test_synthetic_slate.test_synthetic_slate_obtain_batch_bandit_feedback_using_linear_behavior_policy_without_pscore_item_position": [[473, 521], ["obp.dataset.SyntheticSlateBanditDataset", "obp.dataset.SyntheticSlateBanditDataset.obtain_batch_bandit_feedback", "test_synthetic_slate.check_slate_bandit_feedback", "obp.dataset.SyntheticSlateBanditDataset", "obp.dataset.SyntheticSlateBanditDataset.obtain_batch_bandit_feedback", "test_synthetic_slate.check_slate_bandit_feedback", "numpy.allclose", "set", "set", "numpy.unique"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.multiclass.MultiClassToBanditReduction.obtain_batch_bandit_feedback", "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.test_synthetic_slate.check_slate_bandit_feedback", "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.multiclass.MultiClassToBanditReduction.obtain_batch_bandit_feedback", "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.test_synthetic_slate.check_slate_bandit_feedback"], ["", "", "def", "test_synthetic_slate_obtain_batch_bandit_feedback_using_linear_behavior_policy_without_pscore_item_position", "(", ")", ":", "\n", "# set parameters", "\n", "    ", "n_unique_action", "=", "80", "\n", "len_list", "=", "3", "\n", "dim_context", "=", "2", "\n", "reward_type", "=", "\"binary\"", "\n", "random_state", "=", "12345", "\n", "n_rounds", "=", "100", "\n", "dataset", "=", "SyntheticSlateBanditDataset", "(", "\n", "n_unique_action", "=", "n_unique_action", ",", "\n", "len_list", "=", "len_list", ",", "\n", "dim_context", "=", "dim_context", ",", "\n", "reward_type", "=", "reward_type", ",", "\n", "random_state", "=", "random_state", ",", "\n", "behavior_policy_function", "=", "linear_behavior_policy_logit", ",", "\n", ")", "\n", "# obtain feedback", "\n", "bandit_feedback", "=", "dataset", ".", "obtain_batch_bandit_feedback", "(", "\n", "n_rounds", "=", "n_rounds", ",", "return_pscore_item_position", "=", "False", "\n", ")", "\n", "# check slate bandit feedback (common test)", "\n", "check_slate_bandit_feedback", "(", "bandit_feedback", "=", "bandit_feedback", ")", "\n", "assert", "(", "\n", "bandit_feedback", "[", "\"pscore_item_position\"", "]", "is", "None", "\n", ")", ",", "f\"pscore marginal must be None, but {bandit_feedback['pscore_item_position']}\"", "\n", "\n", "# random seed should be fixed", "\n", "dataset2", "=", "SyntheticSlateBanditDataset", "(", "\n", "n_unique_action", "=", "n_unique_action", ",", "\n", "len_list", "=", "len_list", ",", "\n", "dim_context", "=", "dim_context", ",", "\n", "reward_type", "=", "reward_type", ",", "\n", "random_state", "=", "random_state", ",", "\n", "behavior_policy_function", "=", "linear_behavior_policy_logit", ",", "\n", ")", "\n", "# obtain feedback", "\n", "bandit_feedback2", "=", "dataset2", ".", "obtain_batch_bandit_feedback", "(", "\n", "n_rounds", "=", "n_rounds", ",", "return_pscore_item_position", "=", "False", "\n", ")", "\n", "# check slate bandit feedback (common test)", "\n", "check_slate_bandit_feedback", "(", "bandit_feedback", "=", "bandit_feedback2", ")", "\n", "# check random seed effect", "\n", "assert", "np", ".", "allclose", "(", "\n", "bandit_feedback", "[", "\"expected_reward_factual\"", "]", ",", "\n", "bandit_feedback2", "[", "\"expected_reward_factual\"", "]", ",", "\n", ")", "\n", "if", "reward_type", "==", "\"binary\"", ":", "\n", "        ", "assert", "set", "(", "np", ".", "unique", "(", "bandit_feedback", "[", "\"reward\"", "]", ")", ")", "==", "set", "(", "[", "0", ",", "1", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.test_synthetic_slate.test_synthetic_slate_using_valid_inputs": [[1021, 1082], ["pytest.mark.parametrize", "obp.dataset.SyntheticSlateBanditDataset", "obp.dataset.SyntheticSlateBanditDataset.obtain_batch_bandit_feedback", "test_synthetic_slate.check_slate_bandit_feedback", "pandas.DataFrame", "print", "print", "[].describe", "set", "set", "numpy.unique", "pd.DataFrame.groupby"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.multiclass.MultiClassToBanditReduction.obtain_batch_bandit_feedback", "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.test_synthetic_slate.check_slate_bandit_feedback"], ["@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"n_unique_action, len_list, dim_context, reward_type, random_state, n_rounds, reward_structure, decay_function, click_model, eta, behavior_policy_function, is_factorizable, reward_function, return_pscore_item_position, description\"", ",", "\n", "valid_input_of_obtain_batch_bandit_feedback", ",", "\n", ")", "\n", "def", "test_synthetic_slate_using_valid_inputs", "(", "\n", "n_unique_action", ",", "\n", "len_list", ",", "\n", "dim_context", ",", "\n", "reward_type", ",", "\n", "random_state", ",", "\n", "n_rounds", ",", "\n", "reward_structure", ",", "\n", "decay_function", ",", "\n", "click_model", ",", "\n", "eta", ",", "\n", "behavior_policy_function", ",", "\n", "is_factorizable", ",", "\n", "reward_function", ",", "\n", "return_pscore_item_position", ",", "\n", "description", ",", "\n", ")", ":", "\n", "    ", "dataset", "=", "SyntheticSlateBanditDataset", "(", "\n", "n_unique_action", "=", "n_unique_action", ",", "\n", "len_list", "=", "len_list", ",", "\n", "dim_context", "=", "dim_context", ",", "\n", "reward_type", "=", "reward_type", ",", "\n", "reward_structure", "=", "reward_structure", ",", "\n", "decay_function", "=", "decay_function", ",", "\n", "click_model", "=", "click_model", ",", "\n", "eta", "=", "eta", ",", "\n", "random_state", "=", "random_state", ",", "\n", "behavior_policy_function", "=", "behavior_policy_function", ",", "\n", "is_factorizable", "=", "is_factorizable", ",", "\n", "base_reward_function", "=", "reward_function", ",", "\n", ")", "\n", "# obtain feedback", "\n", "bandit_feedback", "=", "dataset", ".", "obtain_batch_bandit_feedback", "(", "\n", "n_rounds", "=", "n_rounds", ",", "return_pscore_item_position", "=", "return_pscore_item_position", "\n", ")", "\n", "# check slate bandit feedback (common test)", "\n", "check_slate_bandit_feedback", "(", "\n", "bandit_feedback", "=", "bandit_feedback", ",", "is_factorizable", "=", "is_factorizable", "\n", ")", "\n", "pscore_columns", "=", "[", "\n", "\"pscore_cascade\"", ",", "\n", "\"pscore\"", ",", "\n", "\"pscore_item_position\"", ",", "\n", "]", "\n", "bandit_feedback_df", "=", "pd", ".", "DataFrame", "(", ")", "\n", "for", "column", "in", "[", "\n", "\"slate_id\"", ",", "\n", "\"position\"", ",", "\n", "\"action\"", ",", "\n", "\"reward\"", ",", "\n", "\"expected_reward_factual\"", ",", "\n", "]", "+", "pscore_columns", ":", "\n", "        ", "bandit_feedback_df", "[", "column", "]", "=", "bandit_feedback", "[", "column", "]", "\n", "", "print", "(", "f\"-------{description}--------\"", ")", "\n", "print", "(", "bandit_feedback_df", ".", "groupby", "(", "\"position\"", ")", "[", "\"reward\"", "]", ".", "describe", "(", ")", ")", "\n", "if", "reward_type", "==", "\"binary\"", ":", "\n", "        ", "assert", "set", "(", "np", ".", "unique", "(", "bandit_feedback", "[", "\"reward\"", "]", ")", ")", "==", "set", "(", "[", "0", ",", "1", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.test_synthetic_slate.test_calc_on_policy_policy_value_using_invalid_input_data": [[1116, 1138], ["pytest.mark.parametrize", "obp.dataset.SyntheticSlateBanditDataset", "pytest.raises", "obp.dataset.SyntheticSlateBanditDataset.calc_on_policy_policy_value"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.synthetic_slate.SyntheticSlateBanditDataset.calc_on_policy_policy_value"], ["@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"slate_id, reward, description\"", ",", "\n", "invalid_input_of_calc_on_policy_policy_value", ",", "\n", ")", "\n", "def", "test_calc_on_policy_policy_value_using_invalid_input_data", "(", "\n", "slate_id", ",", "reward", ",", "description", "\n", ")", "->", "None", ":", "\n", "# set parameters", "\n", "    ", "n_unique_action", "=", "10", "\n", "len_list", "=", "3", "\n", "dim_context", "=", "2", "\n", "reward_type", "=", "\"binary\"", "\n", "random_state", "=", "12345", "\n", "dataset", "=", "SyntheticSlateBanditDataset", "(", "\n", "n_unique_action", "=", "n_unique_action", ",", "\n", "len_list", "=", "len_list", ",", "\n", "dim_context", "=", "dim_context", ",", "\n", "reward_type", "=", "reward_type", ",", "\n", "random_state", "=", "random_state", ",", "\n", ")", "\n", "with", "pytest", ".", "raises", "(", "ValueError", ",", "match", "=", "f\"{description}*\"", ")", ":", "\n", "        ", "_", "=", "dataset", ".", "calc_on_policy_policy_value", "(", "reward", "=", "reward", ",", "slate_id", "=", "slate_id", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.test_synthetic_slate.test_calc_on_policy_policy_value_using_valid_input_data": [[1157, 1180], ["pytest.mark.parametrize", "obp.dataset.SyntheticSlateBanditDataset", "obp.dataset.SyntheticSlateBanditDataset.calc_on_policy_policy_value"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.synthetic_slate.SyntheticSlateBanditDataset.calc_on_policy_policy_value"], ["@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"slate_id, reward, result, description\"", ",", "\n", "valid_input_of_calc_on_policy_policy_value", ",", "\n", ")", "\n", "def", "test_calc_on_policy_policy_value_using_valid_input_data", "(", "\n", "slate_id", ",", "reward", ",", "result", ",", "description", "\n", ")", "->", "None", ":", "\n", "# set parameters", "\n", "    ", "n_unique_action", "=", "10", "\n", "len_list", "=", "3", "\n", "dim_context", "=", "2", "\n", "reward_type", "=", "\"binary\"", "\n", "random_state", "=", "12345", "\n", "dataset", "=", "SyntheticSlateBanditDataset", "(", "\n", "n_unique_action", "=", "n_unique_action", ",", "\n", "len_list", "=", "len_list", ",", "\n", "dim_context", "=", "dim_context", ",", "\n", "reward_type", "=", "reward_type", ",", "\n", "random_state", "=", "random_state", ",", "\n", "behavior_policy_function", "=", "linear_behavior_policy_logit", ",", "\n", ")", "\n", "assert", "result", "==", "dataset", ".", "calc_on_policy_policy_value", "(", "\n", "reward", "=", "reward", ",", "slate_id", "=", "slate_id", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.test_synthetic_slate.test_generate_evaluation_policy_pscore_using_invalid_input_data": [[1252, 1284], ["pytest.mark.parametrize", "obp.dataset.SyntheticSlateBanditDataset", "pytest.raises", "obp.dataset.SyntheticSlateBanditDataset.generate_evaluation_policy_pscore"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.synthetic_slate.SyntheticSlateBanditDataset.generate_evaluation_policy_pscore"], ["@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"evaluation_policy_type, epsilon, context, action, err, description\"", ",", "\n", "invalid_input_of_generate_evaluation_policy_pscore", ",", "\n", ")", "\n", "def", "test_generate_evaluation_policy_pscore_using_invalid_input_data", "(", "\n", "evaluation_policy_type", ",", "\n", "epsilon", ",", "\n", "context", ",", "\n", "action", ",", "\n", "err", ",", "\n", "description", ",", "\n", ")", "->", "None", ":", "\n", "# set parameters", "\n", "    ", "n_unique_action", "=", "10", "\n", "len_list", "=", "3", "\n", "dim_context", "=", "2", "\n", "reward_type", "=", "\"binary\"", "\n", "random_state", "=", "12345", "\n", "dataset", "=", "SyntheticSlateBanditDataset", "(", "\n", "n_unique_action", "=", "n_unique_action", ",", "\n", "len_list", "=", "len_list", ",", "\n", "dim_context", "=", "dim_context", ",", "\n", "reward_type", "=", "reward_type", ",", "\n", "random_state", "=", "random_state", ",", "\n", "base_reward_function", "=", "logistic_reward_function", ",", "\n", ")", "\n", "with", "pytest", ".", "raises", "(", "err", ",", "match", "=", "f\"{description}*\"", ")", ":", "\n", "        ", "_", "=", "dataset", ".", "generate_evaluation_policy_pscore", "(", "\n", "evaluation_policy_type", "=", "evaluation_policy_type", ",", "\n", "epsilon", "=", "epsilon", ",", "\n", "context", "=", "context", ",", "\n", "action", "=", "action", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.test_synthetic_slate.test_generate_evaluation_policy_pscore_using_valid_input_data": [[1341, 1435], ["pytest.mark.parametrize", "obp.dataset.SyntheticSlateBanditDataset", "obp.dataset.SyntheticSlateBanditDataset.obtain_batch_bandit_feedback", "obp.dataset.SyntheticSlateBanditDataset.generate_evaluation_policy_pscore", "test_synthetic_slate.check_slate_bandit_feedback", "pandas.DataFrame", "pd.DataFrame.groupby().apply", "pd.DataFrame.drop_duplicates", "numpy.allclose", "numpy.allclose", "numpy.allclose", "numpy.allclose", "[].expanding().min", "len", "len", "len", "pd.DataFrame.groupby", "[].expanding", "set", "set", "set", "set", "set", "set", "x[].unique", "numpy.unique", "numpy.unique", "numpy.unique", "pd.DataFrame.groupby"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.multiclass.MultiClassToBanditReduction.obtain_batch_bandit_feedback", "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.synthetic_slate.SyntheticSlateBanditDataset.generate_evaluation_policy_pscore", "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.test_synthetic_slate.check_slate_bandit_feedback"], ["@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"n_unique_action, is_factorizable, evaluation_policy_type, epsilon, description\"", ",", "\n", "valid_input_of_generate_evaluation_policy_pscore", ",", "\n", ")", "\n", "def", "test_generate_evaluation_policy_pscore_using_valid_input_data", "(", "\n", "n_unique_action", ",", "\n", "is_factorizable", ",", "\n", "evaluation_policy_type", ",", "\n", "epsilon", ",", "\n", "description", ",", "\n", ")", "->", "None", ":", "\n", "# set parameters", "\n", "    ", "len_list", "=", "3", "\n", "dim_context", "=", "2", "\n", "reward_type", "=", "\"binary\"", "\n", "random_state", "=", "12345", "\n", "n_rounds", "=", "100", "\n", "dataset", "=", "SyntheticSlateBanditDataset", "(", "\n", "n_unique_action", "=", "n_unique_action", ",", "\n", "len_list", "=", "len_list", ",", "\n", "dim_context", "=", "dim_context", ",", "\n", "reward_type", "=", "reward_type", ",", "\n", "random_state", "=", "random_state", ",", "\n", "is_factorizable", "=", "is_factorizable", ",", "\n", "base_reward_function", "=", "logistic_reward_function", ",", "\n", ")", "\n", "# obtain feedback", "\n", "bandit_feedback", "=", "dataset", ".", "obtain_batch_bandit_feedback", "(", "\n", "n_rounds", "=", "n_rounds", ",", "return_pscore_item_position", "=", "True", "\n", ")", "\n", "# generate pscores", "\n", "(", "\n", "pscore", ",", "\n", "pscore_item_position", ",", "\n", "pscore_cascade", ",", "\n", ")", "=", "dataset", ".", "generate_evaluation_policy_pscore", "(", "\n", "evaluation_policy_type", "=", "evaluation_policy_type", ",", "\n", "context", "=", "bandit_feedback", "[", "\"context\"", "]", ",", "\n", "epsilon", "=", "epsilon", ",", "\n", "action", "=", "bandit_feedback", "[", "\"action\"", "]", ",", "\n", ")", "\n", "if", "evaluation_policy_type", "==", "\"random\"", "or", "epsilon", "==", "1.0", ":", "\n", "# pscores of random evaluation policy must be the same as those of bandit feedback using random behavior policy", "\n", "        ", "assert", "np", ".", "allclose", "(", "pscore", ",", "bandit_feedback", "[", "\"pscore\"", "]", ")", "\n", "assert", "np", ".", "allclose", "(", "\n", "pscore_item_position", ",", "bandit_feedback", "[", "\"pscore_item_position\"", "]", "\n", ")", "\n", "assert", "np", ".", "allclose", "(", "pscore_cascade", ",", "bandit_feedback", "[", "\"pscore_cascade\"", "]", ")", "\n", "", "if", "epsilon", "==", "0.0", ":", "\n", "# pscore element of greedy evaluation policy must be either 0 or 1", "\n", "        ", "assert", "len", "(", "set", "(", "np", ".", "unique", "(", "pscore", ")", ")", "-", "set", "(", "[", "0.0", ",", "1.0", "]", ")", ")", "==", "0", "\n", "assert", "len", "(", "set", "(", "np", ".", "unique", "(", "pscore_item_position", ")", ")", "-", "set", "(", "[", "0.0", ",", "1.0", "]", ")", ")", "==", "0", "\n", "assert", "len", "(", "set", "(", "np", ".", "unique", "(", "pscore_cascade", ")", ")", "-", "set", "(", "[", "0.0", ",", "1.0", "]", ")", ")", "==", "0", "\n", "# check pscores", "\n", "", "assert", "(", "\n", "pscore_cascade", "<", "pscore", "\n", ")", ".", "sum", "(", ")", "==", "0", ",", "\"pscore must be smaller than or equal to pscore_cascade\"", "\n", "assert", "(", "\n", "pscore_item_position", "<", "pscore", "\n", ")", ".", "sum", "(", ")", "==", "0", ",", "\"pscore must be smaller than or equal to pscore_item_position\"", "\n", "assert", "(", "\n", "pscore_item_position", "<", "pscore_cascade", "\n", ")", ".", "sum", "(", ")", "==", "0", ",", "\"pscore_cascade must be smaller than or equal to pscore_item_position\"", "\n", "\n", "# check slate bandit feedback (common test)", "\n", "check_slate_bandit_feedback", "(", "\n", "bandit_feedback", "=", "bandit_feedback", ",", "is_factorizable", "=", "is_factorizable", "\n", ")", "\n", "bandit_feedback_df", "=", "pd", ".", "DataFrame", "(", ")", "\n", "for", "column", "in", "[", "\"slate_id\"", ",", "\"position\"", ",", "\"action\"", "]", ":", "\n", "        ", "bandit_feedback_df", "[", "column", "]", "=", "bandit_feedback", "[", "column", "]", "\n", "", "bandit_feedback_df", "[", "\"pscore\"", "]", "=", "pscore", "\n", "bandit_feedback_df", "[", "\"pscore_cascade\"", "]", "=", "pscore_cascade", "\n", "bandit_feedback_df", "[", "\"pscore_item_position\"", "]", "=", "pscore_item_position", "\n", "\n", "previous_minimum_pscore_cascade", "=", "(", "\n", "bandit_feedback_df", ".", "groupby", "(", "\"slate_id\"", ")", "[", "\"pscore_cascade\"", "]", "\n", ".", "expanding", "(", ")", "\n", ".", "min", "(", ")", "\n", ".", "values", "\n", ")", "\n", "assert", "(", "\n", "previous_minimum_pscore_cascade", "<", "pscore_cascade", "\n", ")", ".", "sum", "(", ")", "==", "0", ",", "\"pscore_cascade must be non-decresing sequence in each slate\"", "\n", "count_pscore_in_expression", "=", "bandit_feedback_df", ".", "groupby", "(", "\"slate_id\"", ")", ".", "apply", "(", "\n", "lambda", "x", ":", "x", "[", "\"pscore\"", "]", ".", "unique", "(", ")", ".", "shape", "[", "0", "]", "\n", ")", "\n", "assert", "(", "\n", "count_pscore_in_expression", "!=", "1", "\n", ")", ".", "sum", "(", ")", "==", "0", ",", "\"`pscore` must be unique in each slate\"", "\n", "last_slot_feedback_df", "=", "bandit_feedback_df", ".", "drop_duplicates", "(", "\"slate_id\"", ",", "keep", "=", "\"last\"", ")", "\n", "assert", "np", ".", "allclose", "(", "\n", "last_slot_feedback_df", "[", "\"pscore\"", "]", ",", "last_slot_feedback_df", "[", "\"pscore_cascade\"", "]", "\n", ")", ",", "\"pscore must be the same as pscore_cascade in the last slot\"", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.test_synthetic_slate.test_calc_epsilon_greedy_pscore_using_valid_input_data": [[1472, 1517], ["pytest.mark.parametrize", "obp.dataset.SyntheticSlateBanditDataset", "obp.dataset.SyntheticSlateBanditDataset._calc_epsilon_greedy_pscore", "numpy.allclose", "numpy.allclose", "numpy.allclose"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.synthetic_slate.SyntheticSlateBanditDataset._calc_epsilon_greedy_pscore"], ["@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"n_unique_action, len_list, epsilon, action_2d, sorted_actions, random_pscore, random_pscore_item_position, random_pscore_cascade, true_pscore, true_pscore_item_position, true_pscore_cascade, description\"", ",", "\n", "valid_input_of_calc_epsilon_greedy_pscore", ",", "\n", ")", "\n", "def", "test_calc_epsilon_greedy_pscore_using_valid_input_data", "(", "\n", "n_unique_action", ",", "\n", "len_list", ",", "\n", "epsilon", ",", "\n", "action_2d", ",", "\n", "sorted_actions", ",", "\n", "random_pscore", ",", "\n", "random_pscore_item_position", ",", "\n", "random_pscore_cascade", ",", "\n", "true_pscore", ",", "\n", "true_pscore_item_position", ",", "\n", "true_pscore_cascade", ",", "\n", "description", ",", "\n", ")", "->", "None", ":", "\n", "# set parameters", "\n", "    ", "dim_context", "=", "2", "\n", "reward_type", "=", "\"binary\"", "\n", "random_state", "=", "12345", "\n", "dataset", "=", "SyntheticSlateBanditDataset", "(", "\n", "n_unique_action", "=", "n_unique_action", ",", "\n", "len_list", "=", "len_list", ",", "\n", "dim_context", "=", "dim_context", ",", "\n", "reward_type", "=", "reward_type", ",", "\n", "random_state", "=", "random_state", ",", "\n", "base_reward_function", "=", "logistic_reward_function", ",", "\n", ")", "\n", "(", "\n", "pscore", ",", "\n", "pscore_item_position", ",", "\n", "pscore_cascade", ",", "\n", ")", "=", "dataset", ".", "_calc_epsilon_greedy_pscore", "(", "\n", "epsilon", "=", "epsilon", ",", "\n", "action_2d", "=", "action_2d", ",", "\n", "sorted_actions", "=", "sorted_actions", ",", "\n", "random_pscore", "=", "random_pscore", ",", "\n", "random_pscore_item_position", "=", "random_pscore_item_position", ",", "\n", "random_pscore_cascade", "=", "random_pscore_cascade", ",", "\n", ")", "\n", "assert", "np", ".", "allclose", "(", "true_pscore", ",", "pscore", ")", "\n", "assert", "np", ".", "allclose", "(", "true_pscore_item_position", ",", "pscore_item_position", ")", "\n", "assert", "np", ".", "allclose", "(", "true_pscore_cascade", ",", "pscore_cascade", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.test_synthetic_slate.test_calc_ground_truth_policy_value_using_invalid_input_data": [[1589, 1620], ["pytest.mark.parametrize", "obp.dataset.SyntheticSlateBanditDataset", "obp.dataset.SyntheticSlateBanditDataset.obtain_batch_bandit_feedback", "pytest.raises", "obp.dataset.SyntheticSlateBanditDataset.calc_ground_truth_policy_value"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.multiclass.MultiClassToBanditReduction.obtain_batch_bandit_feedback", "home.repos.pwc.inspect_result.st-tech_zr-obp.simulator.simulator.calc_ground_truth_policy_value"], ["@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"n_rounds, n_unique_action, len_list, dim_context, reward_type, reward_structure, click_model, evaluation_policy_logit_, context, err, description\"", ",", "\n", "invalid_input_of_calc_ground_truth_policy_value", ",", "\n", ")", "\n", "def", "test_calc_ground_truth_policy_value_using_invalid_input_data", "(", "\n", "n_rounds", ",", "\n", "n_unique_action", ",", "\n", "len_list", ",", "\n", "dim_context", ",", "\n", "reward_type", ",", "\n", "reward_structure", ",", "\n", "click_model", ",", "\n", "evaluation_policy_logit_", ",", "\n", "context", ",", "\n", "err", ",", "\n", "description", ",", "\n", ")", ":", "\n", "    ", "dataset", "=", "SyntheticSlateBanditDataset", "(", "\n", "n_unique_action", "=", "n_unique_action", ",", "\n", "len_list", "=", "len_list", ",", "\n", "dim_context", "=", "dim_context", ",", "\n", "reward_type", "=", "reward_type", ",", "\n", "reward_structure", "=", "reward_structure", ",", "\n", "click_model", "=", "click_model", ",", "\n", "base_reward_function", "=", "logistic_reward_function", ",", "\n", ")", "\n", "_", "=", "dataset", ".", "obtain_batch_bandit_feedback", "(", "n_rounds", "=", "n_rounds", ")", "\n", "with", "pytest", ".", "raises", "(", "err", ",", "match", "=", "f\"{description}*\"", ")", ":", "\n", "        ", "dataset", ".", "calc_ground_truth_policy_value", "(", "\n", "evaluation_policy_logit_", "=", "evaluation_policy_logit_", ",", "\n", "context", "=", "context", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.test_synthetic_slate.test_calc_ground_truth_policy_value_using_valid_input_data": [[1784, 1817], ["pytest.mark.parametrize", "obp.dataset.SyntheticSlateBanditDataset", "obp.dataset.SyntheticSlateBanditDataset.obtain_batch_bandit_feedback", "obp.dataset.SyntheticSlateBanditDataset.calc_ground_truth_policy_value", "isinstance"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.multiclass.MultiClassToBanditReduction.obtain_batch_bandit_feedback", "home.repos.pwc.inspect_result.st-tech_zr-obp.simulator.simulator.calc_ground_truth_policy_value"], ["@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"n_rounds, n_unique_action, len_list, dim_context, reward_type, reward_structure, click_model, base_reward_function, is_factorizable, evaluation_policy_logit_, description\"", ",", "\n", "valid_input_of_calc_ground_truth_policy_value", ",", "\n", ")", "\n", "def", "test_calc_ground_truth_policy_value_using_valid_input_data", "(", "\n", "n_rounds", ",", "\n", "n_unique_action", ",", "\n", "len_list", ",", "\n", "dim_context", ",", "\n", "reward_type", ",", "\n", "reward_structure", ",", "\n", "click_model", ",", "\n", "base_reward_function", ",", "\n", "is_factorizable", ",", "\n", "evaluation_policy_logit_", ",", "\n", "description", ",", "\n", ")", ":", "\n", "    ", "dataset", "=", "SyntheticSlateBanditDataset", "(", "\n", "n_unique_action", "=", "n_unique_action", ",", "\n", "len_list", "=", "len_list", ",", "\n", "dim_context", "=", "dim_context", ",", "\n", "reward_type", "=", "reward_type", ",", "\n", "reward_structure", "=", "reward_structure", ",", "\n", "click_model", "=", "click_model", ",", "\n", "base_reward_function", "=", "base_reward_function", ",", "\n", "is_factorizable", "=", "is_factorizable", ",", "\n", ")", "\n", "logged_bandit_feedback", "=", "dataset", ".", "obtain_batch_bandit_feedback", "(", "n_rounds", "=", "n_rounds", ")", "\n", "policy_value", "=", "dataset", ".", "calc_ground_truth_policy_value", "(", "\n", "evaluation_policy_logit_", "=", "evaluation_policy_logit_", ",", "\n", "context", "=", "logged_bandit_feedback", "[", "\"context\"", "]", ",", "\n", ")", "\n", "assert", "isinstance", "(", "policy_value", ",", "float", ")", "and", "0", "<=", "policy_value", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.test_synthetic_slate.test_calc_ground_truth_policy_value_value_check_with_click_model": [[1819, 1888], ["pytest.mark.parametrize", "numpy.array", "obp.dataset.SyntheticSlateBanditDataset", "obp.dataset.SyntheticSlateBanditDataset.obtain_batch_bandit_feedback", "obp.dataset.SyntheticSlateBanditDataset.calc_ground_truth_policy_value", "obp.dataset.SyntheticSlateBanditDataset", "obp.dataset.SyntheticSlateBanditDataset.obtain_batch_bandit_feedback", "obp.dataset.SyntheticSlateBanditDataset.calc_ground_truth_policy_value", "obp.dataset.SyntheticSlateBanditDataset", "obp.dataset.SyntheticSlateBanditDataset.obtain_batch_bandit_feedback", "obp.dataset.SyntheticSlateBanditDataset.calc_ground_truth_policy_value"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.multiclass.MultiClassToBanditReduction.obtain_batch_bandit_feedback", "home.repos.pwc.inspect_result.st-tech_zr-obp.simulator.simulator.calc_ground_truth_policy_value", "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.multiclass.MultiClassToBanditReduction.obtain_batch_bandit_feedback", "home.repos.pwc.inspect_result.st-tech_zr-obp.simulator.simulator.calc_ground_truth_policy_value", "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.multiclass.MultiClassToBanditReduction.obtain_batch_bandit_feedback", "home.repos.pwc.inspect_result.st-tech_zr-obp.simulator.simulator.calc_ground_truth_policy_value"], ["", "@", "pytest", ".", "mark", ".", "parametrize", "(", "\"is_factorizable\"", ",", "[", "(", "True", ")", ",", "(", "False", ")", "]", ")", "\n", "def", "test_calc_ground_truth_policy_value_value_check_with_click_model", "(", "is_factorizable", ")", ":", "\n", "    ", "n_rounds", "=", "3", "\n", "n_unique_action", "=", "4", "\n", "len_list", "=", "3", "\n", "dim_context", "=", "3", "\n", "reward_type", "=", "\"binary\"", "\n", "reward_structure", "=", "\"cascade_additive\"", "\n", "evaluation_policy_logit_", "=", "np", ".", "array", "(", "[", "[", "1", ",", "2", ",", "3", ",", "4", "]", ",", "[", "5", ",", "6", ",", "7", ",", "8", "]", ",", "[", "3", ",", "4", ",", "5", ",", "6", "]", "]", ")", "\n", "\n", "dataset_none", "=", "SyntheticSlateBanditDataset", "(", "\n", "n_unique_action", "=", "n_unique_action", ",", "\n", "len_list", "=", "len_list", ",", "\n", "dim_context", "=", "dim_context", ",", "\n", "reward_type", "=", "reward_type", ",", "\n", "reward_structure", "=", "reward_structure", ",", "\n", "click_model", "=", "None", ",", "\n", "random_state", "=", "12345", ",", "\n", "base_reward_function", "=", "logistic_reward_function", ",", "\n", "is_factorizable", "=", "is_factorizable", ",", "\n", ")", "\n", "logged_bandit_feedback_none", "=", "dataset_none", ".", "obtain_batch_bandit_feedback", "(", "\n", "n_rounds", "=", "n_rounds", "\n", ")", "\n", "policy_value_none", "=", "dataset_none", ".", "calc_ground_truth_policy_value", "(", "\n", "evaluation_policy_logit_", "=", "evaluation_policy_logit_", ",", "\n", "context", "=", "logged_bandit_feedback_none", "[", "\"context\"", "]", ",", "\n", ")", "\n", "\n", "dataset_pbm", "=", "SyntheticSlateBanditDataset", "(", "\n", "n_unique_action", "=", "n_unique_action", ",", "\n", "len_list", "=", "len_list", ",", "\n", "dim_context", "=", "dim_context", ",", "\n", "reward_type", "=", "reward_type", ",", "\n", "reward_structure", "=", "reward_structure", ",", "\n", "click_model", "=", "\"pbm\"", ",", "\n", "random_state", "=", "12345", ",", "\n", "base_reward_function", "=", "logistic_reward_function", ",", "\n", "is_factorizable", "=", "is_factorizable", ",", "\n", ")", "\n", "logged_bandit_feedback_pbm", "=", "dataset_pbm", ".", "obtain_batch_bandit_feedback", "(", "\n", "n_rounds", "=", "n_rounds", "\n", ")", "\n", "policy_value_pbm", "=", "dataset_pbm", ".", "calc_ground_truth_policy_value", "(", "\n", "evaluation_policy_logit_", "=", "evaluation_policy_logit_", ",", "\n", "context", "=", "logged_bandit_feedback_pbm", "[", "\"context\"", "]", ",", "\n", ")", "\n", "\n", "dataset_cascade", "=", "SyntheticSlateBanditDataset", "(", "\n", "n_unique_action", "=", "n_unique_action", ",", "\n", "len_list", "=", "len_list", ",", "\n", "dim_context", "=", "dim_context", ",", "\n", "reward_type", "=", "reward_type", ",", "\n", "reward_structure", "=", "reward_structure", ",", "\n", "click_model", "=", "\"cascade\"", ",", "\n", "random_state", "=", "12345", ",", "\n", "base_reward_function", "=", "logistic_reward_function", ",", "\n", "is_factorizable", "=", "is_factorizable", ",", "\n", ")", "\n", "logged_bandit_feedback_cascade", "=", "dataset_cascade", ".", "obtain_batch_bandit_feedback", "(", "\n", "n_rounds", "=", "n_rounds", "\n", ")", "\n", "policy_value_cascade", "=", "dataset_cascade", ".", "calc_ground_truth_policy_value", "(", "\n", "evaluation_policy_logit_", "=", "evaluation_policy_logit_", ",", "\n", "context", "=", "logged_bandit_feedback_cascade", "[", "\"context\"", "]", ",", "\n", ")", "\n", "\n", "assert", "policy_value_pbm", "<", "policy_value_none", "\n", "assert", "policy_value_cascade", "<", "policy_value_none", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.test_synthetic_slate.test_calc_ground_truth_policy_value_value_check_with_eta": [[1900, 1971], ["pytest.mark.parametrize", "numpy.array", "obp.dataset.SyntheticSlateBanditDataset", "obp.dataset.SyntheticSlateBanditDataset.obtain_batch_bandit_feedback", "obp.dataset.SyntheticSlateBanditDataset.calc_ground_truth_policy_value", "obp.dataset.SyntheticSlateBanditDataset", "obp.dataset.SyntheticSlateBanditDataset.obtain_batch_bandit_feedback", "obp.dataset.SyntheticSlateBanditDataset.calc_ground_truth_policy_value", "obp.dataset.SyntheticSlateBanditDataset", "obp.dataset.SyntheticSlateBanditDataset.obtain_batch_bandit_feedback", "obp.dataset.SyntheticSlateBanditDataset.calc_ground_truth_policy_value"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.multiclass.MultiClassToBanditReduction.obtain_batch_bandit_feedback", "home.repos.pwc.inspect_result.st-tech_zr-obp.simulator.simulator.calc_ground_truth_policy_value", "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.multiclass.MultiClassToBanditReduction.obtain_batch_bandit_feedback", "home.repos.pwc.inspect_result.st-tech_zr-obp.simulator.simulator.calc_ground_truth_policy_value", "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.multiclass.MultiClassToBanditReduction.obtain_batch_bandit_feedback", "home.repos.pwc.inspect_result.st-tech_zr-obp.simulator.simulator.calc_ground_truth_policy_value"], ["@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"len_list, click_model, is_factorizable\"", ",", "\n", "valid_input_of_calc_ground_truth_policy_value", ",", "\n", ")", "\n", "def", "test_calc_ground_truth_policy_value_value_check_with_eta", "(", "\n", "len_list", ",", "click_model", ",", "is_factorizable", "\n", ")", ":", "\n", "    ", "n_rounds", "=", "3", "\n", "n_unique_action", "=", "4", "\n", "dim_context", "=", "3", "\n", "reward_type", "=", "\"binary\"", "\n", "reward_structure", "=", "\"cascade_additive\"", "\n", "evaluation_policy_logit_", "=", "np", ".", "array", "(", "[", "[", "1", ",", "2", ",", "3", ",", "4", "]", ",", "[", "5", ",", "6", ",", "7", ",", "8", "]", ",", "[", "3", ",", "4", ",", "5", ",", "6", "]", "]", ")", "\n", "\n", "dataset_05", "=", "SyntheticSlateBanditDataset", "(", "\n", "n_unique_action", "=", "n_unique_action", ",", "\n", "len_list", "=", "len_list", ",", "\n", "dim_context", "=", "dim_context", ",", "\n", "reward_type", "=", "reward_type", ",", "\n", "reward_structure", "=", "reward_structure", ",", "\n", "click_model", "=", "click_model", ",", "\n", "eta", "=", "0.5", ",", "\n", "random_state", "=", "12345", ",", "\n", "base_reward_function", "=", "logistic_reward_function", ",", "\n", "is_factorizable", "=", "is_factorizable", ",", "\n", ")", "\n", "logged_bandit_feedback_05", "=", "dataset_05", ".", "obtain_batch_bandit_feedback", "(", "\n", "n_rounds", "=", "n_rounds", "\n", ")", "\n", "policy_value_05", "=", "dataset_05", ".", "calc_ground_truth_policy_value", "(", "\n", "evaluation_policy_logit_", "=", "evaluation_policy_logit_", ",", "\n", "context", "=", "logged_bandit_feedback_05", "[", "\"context\"", "]", ",", "\n", ")", "\n", "\n", "dataset_1", "=", "SyntheticSlateBanditDataset", "(", "\n", "n_unique_action", "=", "n_unique_action", ",", "\n", "len_list", "=", "len_list", ",", "\n", "dim_context", "=", "dim_context", ",", "\n", "reward_type", "=", "reward_type", ",", "\n", "reward_structure", "=", "reward_structure", ",", "\n", "click_model", "=", "click_model", ",", "\n", "eta", "=", "1.0", ",", "\n", "random_state", "=", "12345", ",", "\n", "base_reward_function", "=", "logistic_reward_function", ",", "\n", "is_factorizable", "=", "is_factorizable", ",", "\n", ")", "\n", "logged_bandit_feedback_1", "=", "dataset_1", ".", "obtain_batch_bandit_feedback", "(", "n_rounds", "=", "n_rounds", ")", "\n", "policy_value_1", "=", "dataset_1", ".", "calc_ground_truth_policy_value", "(", "\n", "evaluation_policy_logit_", "=", "evaluation_policy_logit_", ",", "\n", "context", "=", "logged_bandit_feedback_1", "[", "\"context\"", "]", ",", "\n", ")", "\n", "\n", "dataset_2", "=", "SyntheticSlateBanditDataset", "(", "\n", "n_unique_action", "=", "n_unique_action", ",", "\n", "len_list", "=", "len_list", ",", "\n", "dim_context", "=", "dim_context", ",", "\n", "reward_type", "=", "reward_type", ",", "\n", "reward_structure", "=", "reward_structure", ",", "\n", "click_model", "=", "click_model", ",", "\n", "eta", "=", "2.0", ",", "\n", "random_state", "=", "12345", ",", "\n", "base_reward_function", "=", "logistic_reward_function", ",", "\n", "is_factorizable", "=", "is_factorizable", ",", "\n", ")", "\n", "logged_bandit_feedback_2", "=", "dataset_2", ".", "obtain_batch_bandit_feedback", "(", "n_rounds", "=", "n_rounds", ")", "\n", "policy_value_2", "=", "dataset_2", ".", "calc_ground_truth_policy_value", "(", "\n", "evaluation_policy_logit_", "=", "evaluation_policy_logit_", ",", "\n", "context", "=", "logged_bandit_feedback_2", "[", "\"context\"", "]", ",", "\n", ")", "\n", "\n", "assert", "policy_value_2", "<", "policy_value_1", "<", "policy_value_05", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.test_synthetic_slate.test_obtain_pscore_given_evaluation_policy_logit": [[2011, 2026], ["pytest.mark.parametrize", "obp.dataset.SyntheticSlateBanditDataset", "pytest.raises", "obp.dataset.SyntheticSlateBanditDataset.obtain_pscore_given_evaluation_policy_logit"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.synthetic_slate.SyntheticSlateBanditDataset.obtain_pscore_given_evaluation_policy_logit"], ["@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"action, evaluation_policy_logit_, err, description\"", ",", "\n", "invalid_input_of_obtain_pscore_given_evaluation_policy_logit", ",", "\n", ")", "\n", "def", "test_obtain_pscore_given_evaluation_policy_logit", "(", "\n", "action", ",", "evaluation_policy_logit_", ",", "err", ",", "description", "\n", ")", ":", "\n", "    ", "dataset", "=", "SyntheticSlateBanditDataset", "(", "\n", "n_unique_action", "=", "n_unique_action", ",", "\n", "len_list", "=", "len_list", ",", "\n", ")", "\n", "with", "pytest", ".", "raises", "(", "err", ",", "match", "=", "f\"{description}*\"", ")", ":", "\n", "        ", "dataset", ".", "obtain_pscore_given_evaluation_policy_logit", "(", "\n", "action", "=", "action", ",", "\n", "evaluation_policy_logit_", "=", "evaluation_policy_logit_", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.test_synthetic_slate.test_obtain_pscore_given_evaluation_policy_logit_value_check": [[2039, 2088], ["pytest.mark.parametrize", "obp.dataset.SyntheticSlateBanditDataset", "obp.dataset.SyntheticSlateBanditDataset.obtain_batch_bandit_feedback", "obp.dataset.SyntheticSlateBanditDataset.behavior_policy_function", "obp.dataset.SyntheticSlateBanditDataset.obtain_pscore_given_evaluation_policy_logit", "print", "print", "numpy.allclose", "numpy.allclose", "numpy.allclose"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.multiclass.MultiClassToBanditReduction.obtain_batch_bandit_feedback", "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.synthetic_slate.SyntheticSlateBanditDataset.obtain_pscore_given_evaluation_policy_logit"], ["@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"n_unique_action, return_pscore_item_position, is_factorizable\"", ",", "\n", "valid_input_of_obtain_pscore_given_evaluation_policy_logit", ",", "\n", ")", "\n", "def", "test_obtain_pscore_given_evaluation_policy_logit_value_check", "(", "\n", "n_unique_action", ",", "\n", "return_pscore_item_position", ",", "\n", "is_factorizable", ",", "\n", ")", ":", "\n", "    ", "dataset", "=", "SyntheticSlateBanditDataset", "(", "\n", "n_unique_action", "=", "n_unique_action", ",", "\n", "len_list", "=", "5", ",", "\n", "behavior_policy_function", "=", "linear_behavior_policy_logit", ",", "\n", "is_factorizable", "=", "is_factorizable", ",", "\n", "random_state", "=", "12345", ",", "\n", ")", "\n", "bandit_feedback", "=", "dataset", ".", "obtain_batch_bandit_feedback", "(", "\n", "n_rounds", "=", "2", ",", "\n", "return_pscore_item_position", "=", "return_pscore_item_position", ",", "\n", ")", "\n", "behavior_and_evaluation_policy_logit_", "=", "dataset", ".", "behavior_policy_function", "(", "\n", "context", "=", "bandit_feedback", "[", "\"context\"", "]", ",", "\n", "action_context", "=", "bandit_feedback", "[", "\"action_context\"", "]", ",", "\n", "random_state", "=", "dataset", ".", "random_state", ",", "\n", ")", "\n", "(", "\n", "evaluation_policy_pscore", ",", "\n", "evaluation_policy_pscore_item_position", ",", "\n", "evaluation_policy_pscore_cascade", ",", "\n", ")", "=", "dataset", ".", "obtain_pscore_given_evaluation_policy_logit", "(", "\n", "action", "=", "bandit_feedback", "[", "\"action\"", "]", ",", "\n", "evaluation_policy_logit_", "=", "behavior_and_evaluation_policy_logit_", ",", "\n", "return_pscore_item_position", "=", "return_pscore_item_position", ",", "\n", ")", "\n", "print", "(", "bandit_feedback", "[", "\"pscore\"", "]", ")", "\n", "print", "(", "evaluation_policy_pscore", ")", "\n", "\n", "assert", "np", ".", "allclose", "(", "bandit_feedback", "[", "\"pscore\"", "]", ",", "evaluation_policy_pscore", ")", "\n", "assert", "np", ".", "allclose", "(", "\n", "bandit_feedback", "[", "\"pscore_cascade\"", "]", ",", "evaluation_policy_pscore_cascade", "\n", ")", "\n", "assert", "(", "\n", "np", ".", "allclose", "(", "\n", "bandit_feedback", "[", "\"pscore_item_position\"", "]", ",", "\n", "evaluation_policy_pscore_item_position", ",", "\n", ")", "\n", "if", "return_pscore_item_position", "\n", "else", "bandit_feedback", "[", "\"pscore_item_position\"", "]", "\n", "==", "evaluation_policy_pscore_item_position", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.test_synthetic_slate.test_calc_pscore_given_policy_logit_using_valid_input_data": [[2117, 2149], ["pytest.mark.parametrize", "obp.dataset.SyntheticSlateBanditDataset", "obp.dataset.SyntheticSlateBanditDataset._calc_pscore_given_policy_logit", "numpy.allclose", "obp.dataset.SyntheticSlateBanditDataset._calc_pscore_given_policy_softmax", "numpy.allclose", "numpy.exp"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.synthetic_slate.SyntheticSlateBanditDataset._calc_pscore_given_policy_logit", "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.synthetic_slate.SyntheticSlateBanditDataset._calc_pscore_given_policy_softmax"], ["@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"n_unique_action, len_list, all_slate_actions, policy_logit_i_, true_pscores, description\"", ",", "\n", "valid_input_of_calc_pscore_given_policy_logit", ",", "\n", ")", "\n", "def", "test_calc_pscore_given_policy_logit_using_valid_input_data", "(", "\n", "n_unique_action", ",", "\n", "len_list", ",", "\n", "all_slate_actions", ",", "\n", "policy_logit_i_", ",", "\n", "true_pscores", ",", "\n", "description", ",", "\n", ")", "->", "None", ":", "\n", "# set parameters", "\n", "    ", "dim_context", "=", "2", "\n", "reward_type", "=", "\"binary\"", "\n", "random_state", "=", "12345", "\n", "dataset", "=", "SyntheticSlateBanditDataset", "(", "\n", "n_unique_action", "=", "n_unique_action", ",", "\n", "len_list", "=", "len_list", ",", "\n", "dim_context", "=", "dim_context", ",", "\n", "reward_type", "=", "reward_type", ",", "\n", "random_state", "=", "random_state", ",", "\n", "base_reward_function", "=", "logistic_reward_function", ",", "\n", ")", "\n", "pscores", "=", "dataset", ".", "_calc_pscore_given_policy_logit", "(", "\n", "all_slate_actions", ",", "policy_logit_i_", "\n", ")", "\n", "assert", "np", ".", "allclose", "(", "true_pscores", ",", "pscores", ")", "\n", "pscores", "=", "dataset", ".", "_calc_pscore_given_policy_softmax", "(", "\n", "all_slate_actions", ",", "np", ".", "exp", "(", "policy_logit_i_", ")", "\n", ")", "\n", "assert", "np", ".", "allclose", "(", "true_pscores", ",", "pscores", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.test_synthetic_slate.test_obtain_pscore_given_evaluation_policy_logit_using_mock_input_data": [[2236, 2289], ["pytest.mark.parametrize", "obp.dataset.SyntheticSlateBanditDataset", "obp.dataset.SyntheticSlateBanditDataset.obtain_pscore_given_evaluation_policy_logit", "numpy.allclose", "numpy.allclose", "numpy.allclose", "obp.dataset.SyntheticSlateBanditDataset.obtain_pscore_given_evaluation_policy_logit", "numpy.allclose", "numpy.allclose", "numpy.allclose"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.synthetic_slate.SyntheticSlateBanditDataset.obtain_pscore_given_evaluation_policy_logit", "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.synthetic_slate.SyntheticSlateBanditDataset.obtain_pscore_given_evaluation_policy_logit"], ["@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"n_unique_action, len_list, evaluation_policy_logit_, action, true_pscores, true_pscores_cascade, true_pscores_item_position,description\"", ",", "\n", "mock_input_of_obtain_pscore_given_evaluation_policy_logit", ",", "\n", ")", "\n", "def", "test_obtain_pscore_given_evaluation_policy_logit_using_mock_input_data", "(", "\n", "n_unique_action", ",", "\n", "len_list", ",", "\n", "evaluation_policy_logit_", ",", "\n", "action", ",", "\n", "true_pscores", ",", "\n", "true_pscores_cascade", ",", "\n", "true_pscores_item_position", ",", "\n", "description", ",", "\n", ")", "->", "None", ":", "\n", "# set parameters", "\n", "    ", "dim_context", "=", "2", "\n", "reward_type", "=", "\"binary\"", "\n", "random_state", "=", "12345", "\n", "dataset", "=", "SyntheticSlateBanditDataset", "(", "\n", "n_unique_action", "=", "n_unique_action", ",", "\n", "len_list", "=", "len_list", ",", "\n", "dim_context", "=", "dim_context", ",", "\n", "reward_type", "=", "reward_type", ",", "\n", "random_state", "=", "random_state", ",", "\n", "base_reward_function", "=", "logistic_reward_function", ",", "\n", ")", "\n", "(", "\n", "evaluation_policy_pscore", ",", "\n", "evaluation_policy_pscore_item_position", ",", "\n", "evaluation_policy_pscore_cascade", ",", "\n", ")", "=", "dataset", ".", "obtain_pscore_given_evaluation_policy_logit", "(", "\n", "action", ",", "evaluation_policy_logit_", ",", "return_pscore_item_position", "=", "True", "\n", ")", "\n", "assert", "np", ".", "allclose", "(", "true_pscores", ",", "evaluation_policy_pscore", ")", "\n", "assert", "np", ".", "allclose", "(", "true_pscores_cascade", ",", "evaluation_policy_pscore_cascade", ")", "\n", "assert", "np", ".", "allclose", "(", "\n", "true_pscores_item_position", ",", "evaluation_policy_pscore_item_position", "\n", ")", "\n", "\n", "(", "\n", "evaluation_policy_pscore", ",", "\n", "evaluation_policy_pscore_item_position", ",", "\n", "evaluation_policy_pscore_cascade", ",", "\n", ")", "=", "dataset", ".", "obtain_pscore_given_evaluation_policy_logit", "(", "\n", "action", ",", "\n", "evaluation_policy_logit_", ",", "\n", "return_pscore_item_position", "=", "True", ",", "\n", "clip_logit_value", "=", "100.0", ",", "\n", ")", "\n", "assert", "np", ".", "allclose", "(", "true_pscores", ",", "evaluation_policy_pscore", ")", "\n", "assert", "np", ".", "allclose", "(", "true_pscores_cascade", ",", "evaluation_policy_pscore_cascade", ")", "\n", "assert", "np", ".", "allclose", "(", "\n", "true_pscores_item_position", ",", "evaluation_policy_pscore_item_position", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.test_synthetic_slate.test_calc_evaluation_policy_action_dist_using_invalid_input_data": [[2327, 2357], ["pytest.mark.parametrize", "obp.dataset.SyntheticSlateBanditDataset", "pytest.raises", "obp.dataset.SyntheticSlateBanditDataset.calc_evaluation_policy_action_dist"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.synthetic_slate.SyntheticSlateBanditDataset.calc_evaluation_policy_action_dist"], ["@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"action, evaluation_policy_logit_, err, description\"", ",", "\n", "invalid_input_of_calc_evaluation_policy_action_dist", ",", "\n", ")", "\n", "def", "test_calc_evaluation_policy_action_dist_using_invalid_input_data", "(", "\n", "action", ",", "\n", "evaluation_policy_logit_", ",", "\n", "err", ",", "\n", "description", ",", "\n", ")", ":", "\n", "# set parameters", "\n", "    ", "n_unique_action", "=", "3", "\n", "len_list", "=", "3", "\n", "dim_context", "=", "2", "\n", "reward_type", "=", "\"binary\"", "\n", "is_factorizable", "=", "True", "\n", "random_state", "=", "12345", "\n", "dataset", "=", "SyntheticSlateBanditDataset", "(", "\n", "n_unique_action", "=", "n_unique_action", ",", "\n", "len_list", "=", "len_list", ",", "\n", "dim_context", "=", "dim_context", ",", "\n", "reward_type", "=", "reward_type", ",", "\n", "random_state", "=", "random_state", ",", "\n", "is_factorizable", "=", "is_factorizable", ",", "\n", "base_reward_function", "=", "logistic_reward_function", ",", "\n", ")", "\n", "with", "pytest", ".", "raises", "(", "err", ",", "match", "=", "f\"{description}*\"", ")", ":", "\n", "        ", "dataset", ".", "calc_evaluation_policy_action_dist", "(", "\n", "action", "=", "action", ",", "\n", "evaluation_policy_logit_", "=", "evaluation_policy_logit_", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.test_synthetic_slate.test_calc_evaluation_policy_action_dist_using_valid_input_data_factorizable_case": [[2370, 2407], ["pytest.mark.parametrize", "obp.dataset.SyntheticSlateBanditDataset", "obp.dataset.SyntheticSlateBanditDataset.calc_evaluation_policy_action_dist", "numpy.allclose", "numpy.allclose", "len", "dataset.calc_evaluation_policy_action_dist.reshape().sum", "numpy.ones", "numpy.ones_like", "dataset.calc_evaluation_policy_action_dist.reshape"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.synthetic_slate.SyntheticSlateBanditDataset.calc_evaluation_policy_action_dist"], ["@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"action, evaluation_policy_logit_, description\"", ",", "\n", "valid_input_of_calc_evaluation_policy_action_dist", ",", "\n", ")", "\n", "def", "test_calc_evaluation_policy_action_dist_using_valid_input_data_factorizable_case", "(", "\n", "action", ",", "\n", "evaluation_policy_logit_", ",", "\n", "description", ",", "\n", ")", ":", "\n", "# set parameters", "\n", "    ", "n_unique_action", "=", "3", "\n", "len_list", "=", "3", "\n", "dim_context", "=", "2", "\n", "reward_type", "=", "\"binary\"", "\n", "is_factorizable", "=", "True", "\n", "random_state", "=", "12345", "\n", "dataset", "=", "SyntheticSlateBanditDataset", "(", "\n", "n_unique_action", "=", "n_unique_action", ",", "\n", "len_list", "=", "len_list", ",", "\n", "dim_context", "=", "dim_context", ",", "\n", "reward_type", "=", "reward_type", ",", "\n", "random_state", "=", "random_state", ",", "\n", "is_factorizable", "=", "is_factorizable", ",", "\n", "base_reward_function", "=", "logistic_reward_function", ",", "\n", ")", "\n", "evaluation_policy_action_dist", "=", "dataset", ".", "calc_evaluation_policy_action_dist", "(", "\n", "action", "=", "action", ",", "\n", "evaluation_policy_logit_", "=", "evaluation_policy_logit_", ",", "\n", ")", "\n", "assert", "len", "(", "evaluation_policy_action_dist", ")", "==", "n_rounds", "*", "len_list", "*", "n_unique_action", "\n", "assert", "np", ".", "allclose", "(", "\n", "evaluation_policy_action_dist", ".", "reshape", "(", "(", "-", "1", ",", "n_unique_action", ")", ")", ".", "sum", "(", "axis", "=", "1", ")", ",", "\n", "np", ".", "ones", "(", "(", "n_rounds", "*", "len_list", ",", ")", ")", ",", "\n", ")", "\n", "assert", "np", ".", "allclose", "(", "\n", "evaluation_policy_action_dist", ",", "\n", "np", ".", "ones_like", "(", "evaluation_policy_action_dist", ")", "/", "n_unique_action", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.test_synthetic_slate.test_calc_evaluation_policy_action_dist_using_valid_input_data_non_factorizable_case": [[2410, 2447], ["pytest.mark.parametrize", "obp.dataset.SyntheticSlateBanditDataset", "obp.dataset.SyntheticSlateBanditDataset.calc_evaluation_policy_action_dist", "numpy.allclose", "len", "dataset.calc_evaluation_policy_action_dist.reshape().sum", "numpy.ones", "numpy.allclose", "dataset.calc_evaluation_policy_action_dist.reshape", "numpy.ones_like"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.synthetic_slate.SyntheticSlateBanditDataset.calc_evaluation_policy_action_dist"], ["", "@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"action, evaluation_policy_logit_, description\"", ",", "\n", "valid_input_of_calc_evaluation_policy_action_dist", ",", "\n", ")", "\n", "def", "test_calc_evaluation_policy_action_dist_using_valid_input_data_non_factorizable_case", "(", "\n", "action", ",", "\n", "evaluation_policy_logit_", ",", "\n", "description", ",", "\n", ")", ":", "\n", "# set parameters", "\n", "    ", "n_unique_action", "=", "3", "\n", "len_list", "=", "3", "\n", "dim_context", "=", "2", "\n", "reward_type", "=", "\"binary\"", "\n", "is_factorizable", "=", "False", "\n", "random_state", "=", "12345", "\n", "dataset", "=", "SyntheticSlateBanditDataset", "(", "\n", "n_unique_action", "=", "n_unique_action", ",", "\n", "len_list", "=", "len_list", ",", "\n", "dim_context", "=", "dim_context", ",", "\n", "reward_type", "=", "reward_type", ",", "\n", "random_state", "=", "random_state", ",", "\n", "is_factorizable", "=", "is_factorizable", ",", "\n", "base_reward_function", "=", "logistic_reward_function", ",", "\n", ")", "\n", "evaluation_policy_action_dist", "=", "dataset", ".", "calc_evaluation_policy_action_dist", "(", "\n", "action", "=", "action", ",", "\n", "evaluation_policy_logit_", "=", "evaluation_policy_logit_", ",", "\n", ")", "\n", "assert", "len", "(", "evaluation_policy_action_dist", ")", "==", "n_rounds", "*", "len_list", "*", "n_unique_action", "\n", "assert", "np", ".", "allclose", "(", "\n", "evaluation_policy_action_dist", ".", "reshape", "(", "(", "-", "1", ",", "n_unique_action", ")", ")", ".", "sum", "(", "axis", "=", "1", ")", ",", "\n", "np", ".", "ones", "(", "(", "n_rounds", "*", "len_list", ",", ")", ")", ",", "\n", ")", "\n", "assert", "not", "np", ".", "allclose", "(", "\n", "evaluation_policy_action_dist", ",", "\n", "np", ".", "ones_like", "(", "evaluation_policy_action_dist", ")", "/", "n_unique_action", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.test_multiclass.raw_data": [[11, 15], ["pytest.fixture", "sklearn.datasets.load_digits"], "function", ["None"], ["@", "pytest", ".", "fixture", "(", "scope", "=", "\"session\"", ")", "\n", "def", "raw_data", "(", ")", "->", "Tuple", "[", "np", ".", "ndarray", ",", "np", ".", "ndarray", "]", ":", "\n", "    ", "X", ",", "y", "=", "load_digits", "(", "return_X_y", "=", "True", ")", "\n", "return", "X", ",", "y", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.test_multiclass.test_invalid_initialization": [[17, 68], ["pytest.raises", "obp.dataset.MultiClassToBanditReduction", "pytest.raises", "obp.dataset.MultiClassToBanditReduction", "pytest.raises", "obp.dataset.MultiClassToBanditReduction", "pytest.raises", "obp.dataset.MultiClassToBanditReduction", "pytest.raises", "obp.dataset.MultiClassToBanditReduction", "pytest.raises", "obp.dataset.MultiClassToBanditReduction", "pytest.raises", "obp.dataset.MultiClassToBanditReduction", "sklearn.linear_model.LogisticRegression", "sklearn.linear_model.LogisticRegression", "sklearn.linear_model.LogisticRegression", "sklearn.linear_model.LogisticRegression", "sklearn.linear_model.LogisticRegression", "sklearn.linear_model.LogisticRegression"], "function", ["None"], ["", "def", "test_invalid_initialization", "(", "raw_data", ")", ":", "\n", "    ", "X", ",", "y", "=", "raw_data", "\n", "\n", "# invalid alpha_b", "\n", "with", "pytest", ".", "raises", "(", "ValueError", ")", ":", "\n", "        ", "MultiClassToBanditReduction", "(", "\n", "X", "=", "X", ",", "y", "=", "y", ",", "base_classifier_b", "=", "LogisticRegression", "(", "max_iter", "=", "10000", ")", ",", "alpha_b", "=", "-", "0.3", "\n", ")", "\n", "\n", "", "with", "pytest", ".", "raises", "(", "ValueError", ")", ":", "\n", "        ", "MultiClassToBanditReduction", "(", "\n", "X", "=", "X", ",", "y", "=", "y", ",", "base_classifier_b", "=", "LogisticRegression", "(", "max_iter", "=", "10000", ")", ",", "alpha_b", "=", "1.3", "\n", ")", "\n", "\n", "# invalid classifier", "\n", "", "with", "pytest", ".", "raises", "(", "ValueError", ")", ":", "\n", "        ", "from", "sklearn", ".", "tree", "import", "DecisionTreeRegressor", "\n", "\n", "MultiClassToBanditReduction", "(", "X", "=", "X", ",", "y", "=", "y", ",", "base_classifier_b", "=", "DecisionTreeRegressor", ")", "\n", "\n", "# invalid n_def_actions", "\n", "", "with", "pytest", ".", "raises", "(", "TypeError", ")", ":", "\n", "        ", "MultiClassToBanditReduction", "(", "\n", "X", "=", "X", ",", "\n", "y", "=", "y", ",", "\n", "base_classifier_b", "=", "LogisticRegression", "(", "max_iter", "=", "10000", ")", ",", "\n", "n_deficient_actions", "=", "\"aaa\"", ",", "\n", ")", "\n", "\n", "", "with", "pytest", ".", "raises", "(", "TypeError", ")", ":", "\n", "        ", "MultiClassToBanditReduction", "(", "\n", "X", "=", "X", ",", "\n", "y", "=", "y", ",", "\n", "base_classifier_b", "=", "LogisticRegression", "(", "max_iter", "=", "10000", ")", ",", "\n", "n_deficient_actions", "=", "None", ",", "\n", ")", "\n", "\n", "", "with", "pytest", ".", "raises", "(", "ValueError", ")", ":", "\n", "        ", "MultiClassToBanditReduction", "(", "\n", "X", "=", "X", ",", "\n", "y", "=", "y", ",", "\n", "base_classifier_b", "=", "LogisticRegression", "(", "max_iter", "=", "10000", ")", ",", "\n", "n_deficient_actions", "=", "-", "1", ",", "\n", ")", "\n", "\n", "", "with", "pytest", ".", "raises", "(", "ValueError", ")", ":", "\n", "        ", "MultiClassToBanditReduction", "(", "\n", "X", "=", "X", ",", "\n", "y", "=", "y", ",", "\n", "base_classifier_b", "=", "LogisticRegression", "(", "max_iter", "=", "10000", ")", ",", "\n", "n_deficient_actions", "=", "1000", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.test_multiclass.test_split_train_eval": [[71, 81], ["obp.dataset.MultiClassToBanditReduction", "obp.dataset.MultiClassToBanditReduction.split_train_eval", "sklearn.linear_model.LogisticRegression"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.multiclass.MultiClassToBanditReduction.split_train_eval"], ["", "", "def", "test_split_train_eval", "(", "raw_data", ")", ":", "\n", "    ", "X", ",", "y", "=", "raw_data", "\n", "\n", "eval_size", "=", "1000", "\n", "mcbr", "=", "MultiClassToBanditReduction", "(", "\n", "X", "=", "X", ",", "y", "=", "y", ",", "base_classifier_b", "=", "LogisticRegression", "(", "max_iter", "=", "10000", ")", ",", "alpha_b", "=", "0.3", "\n", ")", "\n", "mcbr", ".", "split_train_eval", "(", "eval_size", "=", "eval_size", ")", "\n", "\n", "assert", "eval_size", "==", "mcbr", ".", "n_rounds_ev", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.test_multiclass.test_obtain_batch_bandit_feedback": [[83, 114], ["obp.dataset.MultiClassToBanditReduction", "obp.dataset.MultiClassToBanditReduction.split_train_eval", "obp.dataset.MultiClassToBanditReduction.obtain_batch_bandit_feedback", "numpy.allclose", "mcbr.obtain_batch_bandit_feedback.keys", "mcbr.obtain_batch_bandit_feedback.keys", "mcbr.obtain_batch_bandit_feedback.keys", "mcbr.obtain_batch_bandit_feedback.keys", "mcbr.obtain_batch_bandit_feedback.keys", "mcbr.obtain_batch_bandit_feedback.keys", "mcbr.obtain_batch_bandit_feedback.keys", "mcbr.obtain_batch_bandit_feedback.keys", "pi_b[].sum", "numpy.ones", "sklearn.linear_model.LogisticRegression", "numpy.unique"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.multiclass.MultiClassToBanditReduction.split_train_eval", "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.multiclass.MultiClassToBanditReduction.obtain_batch_bandit_feedback"], ["", "def", "test_obtain_batch_bandit_feedback", "(", "raw_data", ")", ":", "\n", "    ", "X", ",", "y", "=", "raw_data", "\n", "\n", "for", "n_deficient_actions", "in", "[", "0", ",", "2", "]", ":", "\n", "        ", "mcbr", "=", "MultiClassToBanditReduction", "(", "\n", "X", "=", "X", ",", "\n", "y", "=", "y", ",", "\n", "base_classifier_b", "=", "LogisticRegression", "(", "max_iter", "=", "10000", ")", ",", "\n", "alpha_b", "=", "0.3", ",", "\n", "n_deficient_actions", "=", "n_deficient_actions", ",", "\n", ")", "\n", "mcbr", ".", "split_train_eval", "(", ")", "\n", "bandit_feedback", "=", "mcbr", ".", "obtain_batch_bandit_feedback", "(", ")", "\n", "\n", "assert", "\"n_actions\"", "in", "bandit_feedback", ".", "keys", "(", ")", "\n", "assert", "\"n_rounds\"", "in", "bandit_feedback", ".", "keys", "(", ")", "\n", "assert", "\"context\"", "in", "bandit_feedback", ".", "keys", "(", ")", "\n", "assert", "\"action\"", "in", "bandit_feedback", ".", "keys", "(", ")", "\n", "assert", "\"reward\"", "in", "bandit_feedback", ".", "keys", "(", ")", "\n", "assert", "\"position\"", "in", "bandit_feedback", ".", "keys", "(", ")", "\n", "assert", "\"pi_b\"", "in", "bandit_feedback", ".", "keys", "(", ")", "\n", "assert", "\"pscore\"", "in", "bandit_feedback", ".", "keys", "(", ")", "\n", "\n", "n_rounds", "=", "bandit_feedback", "[", "\"n_rounds\"", "]", "\n", "pi_b", "=", "bandit_feedback", "[", "\"pi_b\"", "]", "\n", "assert", "pi_b", ".", "shape", "[", "0", "]", "==", "n_rounds", "\n", "n_actions", "=", "np", ".", "unique", "(", "y", ")", ".", "shape", "[", "0", "]", "\n", "assert", "pi_b", ".", "shape", "[", "1", "]", "==", "n_actions", "\n", "assert", "pi_b", ".", "shape", "[", "2", "]", "==", "1", "\n", "assert", "np", ".", "allclose", "(", "pi_b", "[", ":", ",", ":", ",", "0", "]", ".", "sum", "(", "1", ")", ",", "np", ".", "ones", "(", "n_rounds", ")", ")", "\n", "assert", "(", "pi_b", "==", "0", ")", ".", "sum", "(", ")", "==", "n_deficient_actions", "*", "n_rounds", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.test_multiclass.test_obtain_action_dist_by_eval_policy": [[116, 140], ["obp.dataset.MultiClassToBanditReduction", "obp.dataset.MultiClassToBanditReduction.split_train_eval", "obp.dataset.MultiClassToBanditReduction.obtain_action_dist_by_eval_policy", "numpy.allclose", "pytest.raises", "obp.dataset.MultiClassToBanditReduction.obtain_action_dist_by_eval_policy", "pytest.raises", "obp.dataset.MultiClassToBanditReduction.obtain_action_dist_by_eval_policy", "action_dist[].sum", "numpy.ones", "sklearn.linear_model.LogisticRegression", "numpy.unique"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.multiclass.MultiClassToBanditReduction.split_train_eval", "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.multiclass.MultiClassToBanditReduction.obtain_action_dist_by_eval_policy", "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.multiclass.MultiClassToBanditReduction.obtain_action_dist_by_eval_policy", "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.multiclass.MultiClassToBanditReduction.obtain_action_dist_by_eval_policy"], ["", "", "def", "test_obtain_action_dist_by_eval_policy", "(", "raw_data", ")", ":", "\n", "    ", "X", ",", "y", "=", "raw_data", "\n", "\n", "eval_size", "=", "1000", "\n", "mcbr", "=", "MultiClassToBanditReduction", "(", "\n", "X", "=", "X", ",", "y", "=", "y", ",", "base_classifier_b", "=", "LogisticRegression", "(", "max_iter", "=", "10000", ")", ",", "alpha_b", "=", "0.3", "\n", ")", "\n", "mcbr", ".", "split_train_eval", "(", "eval_size", "=", "eval_size", ")", "\n", "\n", "# invalid alpha_e", "\n", "with", "pytest", ".", "raises", "(", "ValueError", ")", ":", "\n", "        ", "mcbr", ".", "obtain_action_dist_by_eval_policy", "(", "alpha_e", "=", "-", "0.3", ")", "\n", "\n", "", "with", "pytest", ".", "raises", "(", "ValueError", ")", ":", "\n", "        ", "mcbr", ".", "obtain_action_dist_by_eval_policy", "(", "alpha_e", "=", "1.3", ")", "\n", "\n", "# valid type", "\n", "", "action_dist", "=", "mcbr", ".", "obtain_action_dist_by_eval_policy", "(", ")", "\n", "\n", "assert", "action_dist", ".", "shape", "[", "0", "]", "==", "eval_size", "\n", "n_actions", "=", "np", ".", "unique", "(", "y", ")", ".", "shape", "[", "0", "]", "\n", "assert", "action_dist", ".", "shape", "[", "1", "]", "==", "n_actions", "\n", "assert", "action_dist", ".", "shape", "[", "2", "]", "==", "1", "\n", "assert", "np", ".", "allclose", "(", "action_dist", "[", ":", ",", ":", ",", "0", "]", ".", "sum", "(", "1", ")", ",", "np", ".", "ones", "(", "eval_size", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.test_multiclass.test_calc_ground_truth_policy_value": [[142, 166], ["obp.dataset.MultiClassToBanditReduction", "obp.dataset.MultiClassToBanditReduction.split_train_eval", "obp.dataset.MultiClassToBanditReduction.obtain_action_dist_by_eval_policy", "obp.dataset.MultiClassToBanditReduction.calc_ground_truth_policy_value", "isinstance", "pytest.raises", "numpy.zeros", "obp.dataset.MultiClassToBanditReduction.calc_ground_truth_policy_value", "pytest.raises", "obp.dataset.MultiClassToBanditReduction.obtain_action_dist_by_eval_policy().reshape", "obp.dataset.MultiClassToBanditReduction.calc_ground_truth_policy_value", "sklearn.linear_model.LogisticRegression", "obp.dataset.MultiClassToBanditReduction.obtain_action_dist_by_eval_policy"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.multiclass.MultiClassToBanditReduction.split_train_eval", "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.multiclass.MultiClassToBanditReduction.obtain_action_dist_by_eval_policy", "home.repos.pwc.inspect_result.st-tech_zr-obp.simulator.simulator.calc_ground_truth_policy_value", "home.repos.pwc.inspect_result.st-tech_zr-obp.simulator.simulator.calc_ground_truth_policy_value", "home.repos.pwc.inspect_result.st-tech_zr-obp.simulator.simulator.calc_ground_truth_policy_value", "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.multiclass.MultiClassToBanditReduction.obtain_action_dist_by_eval_policy"], ["", "def", "test_calc_ground_truth_policy_value", "(", "raw_data", ")", ":", "\n", "    ", "X", ",", "y", "=", "raw_data", "\n", "\n", "eval_size", "=", "1000", "\n", "mcbr", "=", "MultiClassToBanditReduction", "(", "\n", "X", "=", "X", ",", "y", "=", "y", ",", "base_classifier_b", "=", "LogisticRegression", "(", "max_iter", "=", "10000", ")", ",", "alpha_b", "=", "0.3", "\n", ")", "\n", "mcbr", ".", "split_train_eval", "(", "eval_size", "=", "eval_size", ")", "\n", "\n", "with", "pytest", ".", "raises", "(", "ValueError", ")", ":", "\n", "        ", "invalid_action_dist", "=", "np", ".", "zeros", "(", "eval_size", ")", "\n", "mcbr", ".", "calc_ground_truth_policy_value", "(", "action_dist", "=", "invalid_action_dist", ")", "\n", "\n", "", "with", "pytest", ".", "raises", "(", "ValueError", ")", ":", "\n", "        ", "reshaped_action_dist", "=", "mcbr", ".", "obtain_action_dist_by_eval_policy", "(", ")", ".", "reshape", "(", "\n", "1", ",", "-", "1", ",", "1", "\n", ")", "\n", "mcbr", ".", "calc_ground_truth_policy_value", "(", "action_dist", "=", "reshaped_action_dist", ")", "\n", "\n", "", "action_dist", "=", "mcbr", ".", "obtain_action_dist_by_eval_policy", "(", ")", "\n", "ground_truth_policy_value", "=", "mcbr", ".", "calc_ground_truth_policy_value", "(", "\n", "action_dist", "=", "action_dist", "\n", ")", "\n", "assert", "isinstance", "(", "ground_truth_policy_value", ",", "float", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.test_synthetic_continuous.test_synthetic_continuous_init_using_invalid_inputs": [[188, 210], ["pytest.mark.parametrize", "pytest.raises", "obp.dataset.SyntheticContinuousBanditDataset"], "function", ["None"], ["@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"dim_context, action_noise, reward_noise, min_action_value, max_action_value, random_state, err, description\"", ",", "\n", "invalid_input_of_init", ",", "\n", ")", "\n", "def", "test_synthetic_continuous_init_using_invalid_inputs", "(", "\n", "dim_context", ",", "\n", "action_noise", ",", "\n", "reward_noise", ",", "\n", "min_action_value", ",", "\n", "max_action_value", ",", "\n", "random_state", ",", "\n", "err", ",", "\n", "description", ",", "\n", ")", ":", "\n", "    ", "with", "pytest", ".", "raises", "(", "err", ",", "match", "=", "f\"{description}*\"", ")", ":", "\n", "        ", "_", "=", "SyntheticContinuousBanditDataset", "(", "\n", "dim_context", "=", "dim_context", ",", "\n", "action_noise", "=", "action_noise", ",", "\n", "reward_noise", "=", "reward_noise", ",", "\n", "min_action_value", "=", "min_action_value", ",", "\n", "max_action_value", "=", "max_action_value", ",", "\n", "random_state", "=", "random_state", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.test_synthetic_continuous.test_synthetic_continuous_obtain_batch_bandit_feedback_using_invalid_inputs": [[238, 251], ["pytest.mark.parametrize", "obp.dataset.SyntheticContinuousBanditDataset", "pytest.raises", "obp.dataset.SyntheticContinuousBanditDataset.obtain_batch_bandit_feedback"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.multiclass.MultiClassToBanditReduction.obtain_batch_bandit_feedback"], ["@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"n_rounds, err, description\"", ",", "\n", "invalid_input_of_obtain_batch_bandit_feedback", ",", "\n", ")", "\n", "def", "test_synthetic_continuous_obtain_batch_bandit_feedback_using_invalid_inputs", "(", "\n", "n_rounds", ",", "\n", "err", ",", "\n", "description", ",", "\n", ")", ":", "\n", "    ", "dataset", "=", "SyntheticContinuousBanditDataset", "(", ")", "\n", "\n", "with", "pytest", ".", "raises", "(", "err", ",", "match", "=", "f\"{description}*\"", ")", ":", "\n", "        ", "_", "=", "dataset", ".", "obtain_batch_bandit_feedback", "(", "n_rounds", "=", "n_rounds", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.test_synthetic_continuous.test_synthetic_continuous_obtain_batch_bandit_feedback": [[253, 289], ["obp.dataset.SyntheticContinuousBanditDataset", "obp.dataset.SyntheticContinuousBanditDataset.obtain_batch_bandit_feedback", "numpy.all", "numpy.all", "len", "len", "len", "len"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.multiclass.MultiClassToBanditReduction.obtain_batch_bandit_feedback"], ["", "", "def", "test_synthetic_continuous_obtain_batch_bandit_feedback", "(", ")", ":", "\n", "# bandit feedback", "\n", "    ", "n_rounds", "=", "10", "\n", "min_action_value", "=", "-", "1.0", "\n", "max_action_value", "=", "1.0", "\n", "dataset", "=", "SyntheticContinuousBanditDataset", "(", "\n", "min_action_value", "=", "min_action_value", ",", "\n", "max_action_value", "=", "max_action_value", ",", "\n", ")", "\n", "bandit_feedback", "=", "dataset", ".", "obtain_batch_bandit_feedback", "(", "\n", "n_rounds", "=", "n_rounds", ",", "\n", ")", "\n", "assert", "bandit_feedback", "[", "\"n_rounds\"", "]", "==", "n_rounds", "\n", "assert", "(", "\n", "bandit_feedback", "[", "\"context\"", "]", ".", "shape", "[", "0", "]", "==", "n_rounds", "# n_rounds", "\n", "and", "bandit_feedback", "[", "\"context\"", "]", ".", "shape", "[", "1", "]", "==", "1", "# default dim_context", "\n", ")", "\n", "assert", "(", "\n", "bandit_feedback", "[", "\"action\"", "]", ".", "ndim", "==", "1", "\n", "and", "len", "(", "bandit_feedback", "[", "\"action\"", "]", ")", "==", "n_rounds", "\n", ")", "\n", "assert", "np", ".", "all", "(", "min_action_value", "<=", "bandit_feedback", "[", "\"action\"", "]", ")", "and", "np", ".", "all", "(", "\n", "bandit_feedback", "[", "\"action\"", "]", "<=", "max_action_value", "\n", ")", "\n", "assert", "bandit_feedback", "[", "\"position\"", "]", "is", "None", "\n", "assert", "(", "\n", "bandit_feedback", "[", "\"reward\"", "]", ".", "ndim", "==", "1", "\n", "and", "len", "(", "bandit_feedback", "[", "\"reward\"", "]", ")", "==", "n_rounds", "\n", ")", "\n", "assert", "(", "\n", "bandit_feedback", "[", "\"expected_reward\"", "]", ".", "ndim", "==", "1", "\n", "and", "len", "(", "bandit_feedback", "[", "\"expected_reward\"", "]", ")", "==", "n_rounds", "\n", ")", "\n", "assert", "(", "\n", "bandit_feedback", "[", "\"pscore\"", "]", ".", "ndim", "==", "1", "\n", "and", "len", "(", "bandit_feedback", "[", "\"pscore\"", "]", ")", "==", "n_rounds", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.test_synthetic_continuous.test_synthetic_continuous_calc_policy_value_using_invalid_inputs": [[316, 331], ["pytest.mark.parametrize", "obp.dataset.SyntheticContinuousBanditDataset", "pytest.raises", "obp.dataset.SyntheticContinuousBanditDataset.calc_ground_truth_policy_value"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.simulator.simulator.calc_ground_truth_policy_value"], ["@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"context, action, description\"", ",", "\n", "invalid_input_of_calc_policy_value", ",", "\n", ")", "\n", "def", "test_synthetic_continuous_calc_policy_value_using_invalid_inputs", "(", "\n", "context", ",", "\n", "action", ",", "\n", "description", ",", "\n", ")", ":", "\n", "    ", "dataset", "=", "SyntheticContinuousBanditDataset", "(", ")", "\n", "\n", "with", "pytest", ".", "raises", "(", "ValueError", ",", "match", "=", "f\"{description}*\"", ")", ":", "\n", "        ", "_", "=", "dataset", ".", "calc_ground_truth_policy_value", "(", "\n", "context", "=", "context", ",", "\n", "action", "=", "action", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.test_synthetic_continuous.test_synthetic_continuous_calc_policy_value": [[334, 354], ["obp.dataset.SyntheticContinuousBanditDataset", "obp.dataset.SyntheticContinuousBanditDataset.obtain_batch_bandit_feedback", "obp.dataset.SyntheticContinuousBanditDataset.calc_ground_truth_policy_value", "isinstance", "bandit_feedback[].mean"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.multiclass.MultiClassToBanditReduction.obtain_batch_bandit_feedback", "home.repos.pwc.inspect_result.st-tech_zr-obp.simulator.simulator.calc_ground_truth_policy_value"], ["", "", "def", "test_synthetic_continuous_calc_policy_value", "(", ")", ":", "\n", "    ", "n_rounds", "=", "10", "\n", "dim_context", "=", "3", "\n", "dataset", "=", "SyntheticContinuousBanditDataset", "(", "\n", "dim_context", "=", "dim_context", ",", "\n", "min_action_value", "=", "1", ",", "\n", "max_action_value", "=", "10", ",", "\n", ")", "\n", "bandit_feedback", "=", "dataset", ".", "obtain_batch_bandit_feedback", "(", "\n", "n_rounds", "=", "n_rounds", ",", "\n", ")", "\n", "\n", "policy_value", "=", "dataset", ".", "calc_ground_truth_policy_value", "(", "\n", "context", "=", "bandit_feedback", "[", "\"context\"", "]", ",", "\n", "action", "=", "bandit_feedback", "[", "\"action\"", "]", ",", "\n", ")", "\n", "assert", "isinstance", "(", "\n", "policy_value", ",", "float", "\n", ")", ",", "\"Invalid response of calc_ground_truth_policy_value\"", "\n", "assert", "policy_value", "==", "bandit_feedback", "[", "\"expected_reward\"", "]", ".", "mean", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.test_synthetic_continuous.test_synthetic_linear_reward_funcion_continuous": [[356, 385], ["numpy.ones", "numpy.ones", "obp.dataset.synthetic_continuous.linear_reward_funcion_continuous", "pytest.raises", "numpy.array", "obp.dataset.synthetic_continuous.linear_reward_funcion_continuous", "pytest.raises", "obp.dataset.synthetic_continuous.linear_reward_funcion_continuous", "pytest.raises", "numpy.array", "obp.dataset.synthetic_continuous.linear_reward_funcion_continuous", "pytest.raises", "obp.dataset.synthetic_continuous.linear_reward_funcion_continuous", "pytest.raises", "obp.dataset.synthetic_continuous.linear_reward_funcion_continuous", "numpy.ones", "numpy.ones", "numpy.ones", "numpy.ones", "numpy.ones", "numpy.ones"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.synthetic_continuous.linear_reward_funcion_continuous", "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.synthetic_continuous.linear_reward_funcion_continuous", "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.synthetic_continuous.linear_reward_funcion_continuous", "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.synthetic_continuous.linear_reward_funcion_continuous", "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.synthetic_continuous.linear_reward_funcion_continuous", "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.synthetic_continuous.linear_reward_funcion_continuous"], ["", "def", "test_synthetic_linear_reward_funcion_continuous", "(", ")", ":", "\n", "# context", "\n", "    ", "with", "pytest", ".", "raises", "(", "ValueError", ")", ":", "\n", "        ", "context", "=", "np", ".", "array", "(", "[", "1.0", ",", "1.0", "]", ")", "\n", "linear_reward_funcion_continuous", "(", "context", "=", "context", ",", "action", "=", "np", ".", "ones", "(", "2", ")", ")", "\n", "\n", "", "with", "pytest", ".", "raises", "(", "ValueError", ")", ":", "\n", "        ", "context", "=", "[", "1.0", ",", "1.0", "]", "\n", "linear_reward_funcion_continuous", "(", "context", "=", "context", ",", "action", "=", "np", ".", "ones", "(", "[", "2", ",", "2", "]", ")", ")", "\n", "\n", "# action", "\n", "", "with", "pytest", ".", "raises", "(", "ValueError", ")", ":", "\n", "        ", "action", "=", "np", ".", "array", "(", "[", "1.0", "]", ")", "\n", "linear_reward_funcion_continuous", "(", "context", "=", "np", ".", "ones", "(", "[", "2", ",", "2", "]", ")", ",", "action", "=", "action", ")", "\n", "\n", "", "with", "pytest", ".", "raises", "(", "ValueError", ")", ":", "\n", "        ", "action", "=", "[", "1.0", ",", "1.0", "]", "\n", "linear_reward_funcion_continuous", "(", "context", "=", "np", ".", "ones", "(", "[", "2", ",", "2", "]", ")", ",", "action", "=", "action", ")", "\n", "\n", "", "with", "pytest", ".", "raises", "(", "ValueError", ")", ":", "\n", "        ", "linear_reward_funcion_continuous", "(", "context", "=", "np", ".", "ones", "(", "[", "2", ",", "2", "]", ")", ",", "action", "=", "np", ".", "ones", "(", "3", ")", ")", "\n", "\n", "# expected_reward", "\n", "", "n_rounds", "=", "10", "\n", "dim_context", "=", "3", "\n", "context", "=", "np", ".", "ones", "(", "[", "n_rounds", ",", "dim_context", "]", ")", "\n", "action", "=", "np", ".", "ones", "(", "n_rounds", ")", "\n", "expected_reward", "=", "linear_reward_funcion_continuous", "(", "context", "=", "context", ",", "action", "=", "action", ")", "\n", "assert", "expected_reward", ".", "shape", "[", "0", "]", "==", "n_rounds", "and", "expected_reward", ".", "ndim", "==", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.test_synthetic_continuous.test_synthetic_quadratic_reward_funcion_continuous": [[387, 418], ["numpy.ones", "numpy.ones", "obp.dataset.synthetic_continuous.quadratic_reward_funcion_continuous", "pytest.raises", "numpy.array", "obp.dataset.synthetic_continuous.quadratic_reward_funcion_continuous", "pytest.raises", "obp.dataset.synthetic_continuous.quadratic_reward_funcion_continuous", "pytest.raises", "numpy.array", "obp.dataset.synthetic_continuous.quadratic_reward_funcion_continuous", "pytest.raises", "obp.dataset.synthetic_continuous.quadratic_reward_funcion_continuous", "pytest.raises", "obp.dataset.synthetic_continuous.quadratic_reward_funcion_continuous", "numpy.ones", "numpy.ones", "numpy.ones", "numpy.ones", "numpy.ones", "numpy.ones"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.synthetic_continuous.quadratic_reward_funcion_continuous", "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.synthetic_continuous.quadratic_reward_funcion_continuous", "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.synthetic_continuous.quadratic_reward_funcion_continuous", "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.synthetic_continuous.quadratic_reward_funcion_continuous", "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.synthetic_continuous.quadratic_reward_funcion_continuous", "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.synthetic_continuous.quadratic_reward_funcion_continuous"], ["", "def", "test_synthetic_quadratic_reward_funcion_continuous", "(", ")", ":", "\n", "# context", "\n", "    ", "with", "pytest", ".", "raises", "(", "ValueError", ")", ":", "\n", "        ", "context", "=", "np", ".", "array", "(", "[", "1.0", ",", "1.0", "]", ")", "\n", "quadratic_reward_funcion_continuous", "(", "context", "=", "context", ",", "action", "=", "np", ".", "ones", "(", "2", ")", ")", "\n", "\n", "", "with", "pytest", ".", "raises", "(", "ValueError", ")", ":", "\n", "        ", "context", "=", "[", "1.0", ",", "1.0", "]", "\n", "quadratic_reward_funcion_continuous", "(", "context", "=", "context", ",", "action", "=", "np", ".", "ones", "(", "[", "2", ",", "2", "]", ")", ")", "\n", "\n", "# action", "\n", "", "with", "pytest", ".", "raises", "(", "ValueError", ")", ":", "\n", "        ", "action", "=", "np", ".", "array", "(", "[", "1.0", "]", ")", "\n", "quadratic_reward_funcion_continuous", "(", "context", "=", "np", ".", "ones", "(", "[", "2", ",", "2", "]", ")", ",", "action", "=", "action", ")", "\n", "\n", "", "with", "pytest", ".", "raises", "(", "ValueError", ")", ":", "\n", "        ", "action", "=", "[", "1.0", ",", "1.0", "]", "\n", "quadratic_reward_funcion_continuous", "(", "context", "=", "np", ".", "ones", "(", "[", "2", ",", "2", "]", ")", ",", "action", "=", "action", ")", "\n", "\n", "", "with", "pytest", ".", "raises", "(", "ValueError", ")", ":", "\n", "        ", "quadratic_reward_funcion_continuous", "(", "context", "=", "np", ".", "ones", "(", "[", "2", ",", "2", "]", ")", ",", "action", "=", "np", ".", "ones", "(", "3", ")", ")", "\n", "\n", "# expected_reward", "\n", "", "n_rounds", "=", "10", "\n", "dim_context", "=", "3", "\n", "context", "=", "np", ".", "ones", "(", "[", "n_rounds", ",", "dim_context", "]", ")", "\n", "action", "=", "np", ".", "ones", "(", "n_rounds", ")", "\n", "expected_reward", "=", "quadratic_reward_funcion_continuous", "(", "\n", "context", "=", "context", ",", "action", "=", "action", "\n", ")", "\n", "assert", "expected_reward", ".", "shape", "[", "0", "]", "==", "n_rounds", "and", "expected_reward", ".", "ndim", "==", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.test_synthetic_continuous.test_synthetic_linear_behavior_policy_continuous": [[420, 438], ["numpy.ones", "obp.dataset.synthetic_continuous.linear_behavior_policy_continuous", "pytest.raises", "numpy.array", "obp.dataset.synthetic_continuous.linear_behavior_policy_continuous", "pytest.raises", "obp.dataset.synthetic_continuous.linear_behavior_policy_continuous"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.synthetic_continuous.linear_behavior_policy_continuous", "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.synthetic_continuous.linear_behavior_policy_continuous", "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.synthetic_continuous.linear_behavior_policy_continuous"], ["", "def", "test_synthetic_linear_behavior_policy_continuous", "(", ")", ":", "\n", "# context", "\n", "    ", "with", "pytest", ".", "raises", "(", "ValueError", ")", ":", "\n", "        ", "context", "=", "np", ".", "array", "(", "[", "1.0", ",", "1.0", "]", ")", "\n", "linear_behavior_policy_continuous", "(", "context", "=", "context", ")", "\n", "\n", "", "with", "pytest", ".", "raises", "(", "ValueError", ")", ":", "\n", "        ", "context", "=", "[", "1.0", ",", "1.0", "]", "\n", "linear_behavior_policy_continuous", "(", "context", "=", "context", ")", "\n", "\n", "# expected continuous action values by behavior policy", "\n", "", "n_rounds", "=", "10", "\n", "dim_context", "=", "3", "\n", "context", "=", "np", ".", "ones", "(", "[", "n_rounds", ",", "dim_context", "]", ")", "\n", "expected_continuous_actions", "=", "linear_behavior_policy_continuous", "(", "context", "=", "context", ")", "\n", "assert", "(", "\n", "expected_continuous_actions", ".", "shape", "[", "0", "]", "==", "n_rounds", "\n", "and", "expected_continuous_actions", ".", "ndim", "==", "1", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.test_synthetic_continuous.test_linear_synthetic_policy_continuous": [[441, 457], ["numpy.ones", "obp.dataset.synthetic_continuous.linear_synthetic_policy_continuous", "pytest.raises", "numpy.array", "obp.dataset.synthetic_continuous.linear_behavior_policy_continuous", "pytest.raises", "obp.dataset.synthetic_continuous.linear_behavior_policy_continuous"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.synthetic_continuous.linear_synthetic_policy_continuous", "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.synthetic_continuous.linear_behavior_policy_continuous", "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.synthetic_continuous.linear_behavior_policy_continuous"], ["", "def", "test_linear_synthetic_policy_continuous", "(", ")", ":", "\n", "# context", "\n", "    ", "with", "pytest", ".", "raises", "(", "ValueError", ")", ":", "\n", "        ", "context", "=", "np", ".", "array", "(", "[", "1.0", ",", "1.0", "]", ")", "\n", "linear_behavior_policy_continuous", "(", "context", "=", "context", ")", "\n", "\n", "", "with", "pytest", ".", "raises", "(", "ValueError", ")", ":", "\n", "        ", "context", "=", "[", "1.0", ",", "1.0", "]", "\n", "linear_behavior_policy_continuous", "(", "context", "=", "context", ")", "\n", "\n", "# continuous action values given by a synthetic policy", "\n", "", "n_rounds", "=", "10", "\n", "dim_context", "=", "3", "\n", "context", "=", "np", ".", "ones", "(", "[", "n_rounds", ",", "dim_context", "]", ")", "\n", "continuous_actions", "=", "linear_synthetic_policy_continuous", "(", "context", "=", "context", ")", "\n", "assert", "continuous_actions", ".", "shape", "[", "0", "]", "==", "n_rounds", "and", "continuous_actions", ".", "ndim", "==", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.test_synthetic_continuous.test_threshold_synthetic_policy_continuous": [[459, 475], ["numpy.ones", "obp.dataset.synthetic_continuous.threshold_synthetic_policy_continuous", "pytest.raises", "numpy.array", "obp.dataset.synthetic_continuous.threshold_synthetic_policy_continuous", "pytest.raises", "obp.dataset.synthetic_continuous.threshold_synthetic_policy_continuous"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.synthetic_continuous.threshold_synthetic_policy_continuous", "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.synthetic_continuous.threshold_synthetic_policy_continuous", "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.synthetic_continuous.threshold_synthetic_policy_continuous"], ["", "def", "test_threshold_synthetic_policy_continuous", "(", ")", ":", "\n", "# context", "\n", "    ", "with", "pytest", ".", "raises", "(", "ValueError", ")", ":", "\n", "        ", "context", "=", "np", ".", "array", "(", "[", "1.0", ",", "1.0", "]", ")", "\n", "threshold_synthetic_policy_continuous", "(", "context", "=", "context", ")", "\n", "\n", "", "with", "pytest", ".", "raises", "(", "ValueError", ")", ":", "\n", "        ", "context", "=", "[", "1.0", ",", "1.0", "]", "\n", "threshold_synthetic_policy_continuous", "(", "context", "=", "context", ")", "\n", "\n", "# continuous action values given by a synthetic policy", "\n", "", "n_rounds", "=", "10", "\n", "dim_context", "=", "3", "\n", "context", "=", "np", ".", "ones", "(", "[", "n_rounds", ",", "dim_context", "]", ")", "\n", "continuous_actions", "=", "threshold_synthetic_policy_continuous", "(", "context", "=", "context", ")", "\n", "assert", "continuous_actions", ".", "shape", "[", "0", "]", "==", "n_rounds", "and", "continuous_actions", ".", "ndim", "==", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.test_synthetic_continuous.test_sign_synthetic_policy_continuous": [[477, 493], ["numpy.ones", "obp.dataset.synthetic_continuous.sign_synthetic_policy_continuous", "pytest.raises", "numpy.array", "obp.dataset.synthetic_continuous.sign_synthetic_policy_continuous", "pytest.raises", "obp.dataset.synthetic_continuous.sign_synthetic_policy_continuous"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.synthetic_continuous.sign_synthetic_policy_continuous", "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.synthetic_continuous.sign_synthetic_policy_continuous", "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.synthetic_continuous.sign_synthetic_policy_continuous"], ["", "def", "test_sign_synthetic_policy_continuous", "(", ")", ":", "\n", "# context", "\n", "    ", "with", "pytest", ".", "raises", "(", "ValueError", ")", ":", "\n", "        ", "context", "=", "np", ".", "array", "(", "[", "1.0", ",", "1.0", "]", ")", "\n", "sign_synthetic_policy_continuous", "(", "context", "=", "context", ")", "\n", "\n", "", "with", "pytest", ".", "raises", "(", "ValueError", ")", ":", "\n", "        ", "context", "=", "[", "1.0", ",", "1.0", "]", "\n", "sign_synthetic_policy_continuous", "(", "context", "=", "context", ")", "\n", "\n", "# continuous action values given by a synthetic policy", "\n", "", "n_rounds", "=", "10", "\n", "dim_context", "=", "3", "\n", "context", "=", "np", ".", "ones", "(", "[", "n_rounds", ",", "dim_context", "]", ")", "\n", "continuous_actions", "=", "sign_synthetic_policy_continuous", "(", "context", "=", "context", ")", "\n", "assert", "continuous_actions", ".", "shape", "[", "0", "]", "==", "n_rounds", "and", "continuous_actions", ".", "ndim", "==", "1", "\n", "", ""]], "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.test_synthetic_slate_functions.test_generate_symmetric_matrix": [[10, 14], ["obp.dataset.synthetic_slate.generate_symmetric_matrix", "numpy.allclose"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.synthetic_slate.generate_symmetric_matrix"], ["def", "test_generate_symmetric_matrix", "(", ")", ":", "\n", "    ", "matrix", "=", "generate_symmetric_matrix", "(", "n_unique_action", "=", "3", ",", "random_state", "=", "1", ")", "\n", "assert", "matrix", ".", "shape", "==", "(", "3", ",", "3", ")", "\n", "assert", "np", ".", "allclose", "(", "matrix", ",", "matrix", ".", "T", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.test_synthetic_slate_functions.test_linear_behavior_policy_logit_using_invalid_input": [[51, 67], ["pytest.mark.parametrize", "pytest.raises", "obp.dataset.synthetic_slate.linear_behavior_policy_logit", "pytest.raises", "obp.dataset.synthetic_slate.linear_behavior_policy_logit"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.synthetic_slate.linear_behavior_policy_logit", "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.synthetic_slate.linear_behavior_policy_logit"], ["@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"context, action_context, tau, err, description\"", ",", "\n", "invalid_input_of_linear_behavior_policy_logit", ",", "\n", ")", "\n", "def", "test_linear_behavior_policy_logit_using_invalid_input", "(", "\n", "context", ",", "action_context", ",", "tau", ",", "err", ",", "description", "\n", ")", ":", "\n", "    ", "if", "description", "==", "\"\"", ":", "\n", "        ", "with", "pytest", ".", "raises", "(", "err", ")", ":", "\n", "            ", "linear_behavior_policy_logit", "(", "\n", "context", "=", "context", ",", "action_context", "=", "action_context", ",", "tau", "=", "tau", "\n", ")", "\n", "", "", "else", ":", "\n", "        ", "with", "pytest", ".", "raises", "(", "err", ",", "match", "=", "f\"{description}*\"", ")", ":", "\n", "            ", "linear_behavior_policy_logit", "(", "\n", "context", "=", "context", ",", "action_context", "=", "action_context", ",", "tau", "=", "tau", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.test_synthetic_slate_functions.test_linear_behavior_policy_logit_using_valid_input": [[76, 87], ["pytest.mark.parametrize", "obp.dataset.synthetic_slate.linear_behavior_policy_logit"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.synthetic_slate.linear_behavior_policy_logit"], ["@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"context, action_context, tau, description\"", ",", "\n", "valid_input_of_linear_behavior_policy_logit", ",", "\n", ")", "\n", "def", "test_linear_behavior_policy_logit_using_valid_input", "(", "\n", "context", ",", "action_context", ",", "tau", ",", "description", "\n", ")", ":", "\n", "    ", "logit_value", "=", "linear_behavior_policy_logit", "(", "\n", "context", "=", "context", ",", "action_context", "=", "action_context", ",", "tau", "=", "tau", "\n", ")", "\n", "assert", "logit_value", ".", "shape", "==", "(", "context", ".", "shape", "[", "0", "]", ",", "action_context", ".", "shape", "[", "0", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.test_synthetic_slate_functions.test_action_interaction_reward_function_using_invalid_input": [[248, 278], ["pytest.mark.parametrize", "pytest.raises", "obp.dataset.synthetic_slate.action_interaction_reward_function"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.synthetic_slate.action_interaction_reward_function"], ["@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"context, action_context, action, base_reward_function, action_interaction_weight_matrix, reward_type, reward_structure, len_list, is_enumerated, random_state, err, description\"", ",", "\n", "invalid_input_of_action_interaction_reward_function", ",", "\n", ")", "\n", "def", "test_action_interaction_reward_function_using_invalid_input", "(", "\n", "context", ",", "\n", "action_context", ",", "\n", "action", ",", "\n", "base_reward_function", ",", "\n", "action_interaction_weight_matrix", ",", "\n", "reward_type", ",", "\n", "reward_structure", ",", "\n", "len_list", ",", "\n", "is_enumerated", ",", "\n", "random_state", ",", "\n", "err", ",", "\n", "description", ",", "\n", ")", ":", "\n", "    ", "with", "pytest", ".", "raises", "(", "err", ",", "match", "=", "f\"{description}*\"", ")", ":", "\n", "        ", "_", "=", "action_interaction_reward_function", "(", "\n", "context", "=", "context", ",", "\n", "action_context", "=", "action_context", ",", "\n", "action", "=", "action", ",", "\n", "action_interaction_weight_matrix", "=", "action_interaction_weight_matrix", ",", "\n", "base_reward_function", "=", "base_reward_function", ",", "\n", "reward_type", "=", "reward_type", ",", "\n", "reward_structure", "=", "reward_structure", ",", "\n", "len_list", "=", "len_list", ",", "\n", "is_enumerated", "=", "is_enumerated", ",", "\n", "random_state", "=", "random_state", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.test_synthetic_slate_functions.test_action_interaction_decay_reward_function_using_valid_input": [[390, 430], ["pytest.mark.parametrize", "obp.dataset.synthetic_slate.action_interaction_reward_function", "numpy.all", "numpy.all"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.synthetic_slate.action_interaction_reward_function"], ["@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"context, action_context, action, base_reward_function, action_interaction_weight_matrix, reward_type, reward_structure, len_list, is_enumerated, random_state, description\"", ",", "\n", "valid_input_of_action_interaction_reward_function", ",", "\n", ")", "\n", "def", "test_action_interaction_decay_reward_function_using_valid_input", "(", "\n", "context", ",", "\n", "action_context", ",", "\n", "action", ",", "\n", "base_reward_function", ",", "\n", "action_interaction_weight_matrix", ",", "\n", "reward_type", ",", "\n", "reward_structure", ",", "\n", "len_list", ",", "\n", "is_enumerated", ",", "\n", "random_state", ",", "\n", "description", ",", "\n", ")", ":", "\n", "    ", "expected_reward_factual", "=", "action_interaction_reward_function", "(", "\n", "context", "=", "context", ",", "\n", "action_context", "=", "action_context", ",", "\n", "action", "=", "action", ",", "\n", "action_interaction_weight_matrix", "=", "action_interaction_weight_matrix", ",", "\n", "base_reward_function", "=", "base_reward_function", ",", "\n", "reward_type", "=", "reward_type", ",", "\n", "reward_structure", "=", "reward_structure", ",", "\n", "len_list", "=", "len_list", ",", "\n", "is_enumerated", "=", "is_enumerated", ",", "\n", "random_state", "=", "random_state", ",", "\n", ")", "\n", "if", "not", "is_enumerated", ":", "\n", "        ", "assert", "expected_reward_factual", ".", "shape", "==", "(", "\n", "context", ".", "shape", "[", "0", "]", ",", "\n", "len_list", ",", "\n", ")", "\n", "", "else", ":", "\n", "        ", "assert", "expected_reward_factual", ".", "shape", "[", "0", "]", "%", "context", ".", "shape", "[", "0", "]", "==", "0", "\n", "assert", "expected_reward_factual", ".", "shape", "[", "1", "]", "==", "len_list", "\n", "", "if", "reward_type", "==", "\"binary\"", ":", "\n", "        ", "assert", "np", ".", "all", "(", "0", "<=", "expected_reward_factual", ")", "and", "np", ".", "all", "(", "\n", "expected_reward_factual", "<=", "1", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.test_synthetic.test_synthetic_init_using_invalid_inputs": [[213, 239], ["pytest.mark.parametrize", "pytest.raises", "obp.dataset.SyntheticBanditDataset"], "function", ["None"], ["@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"n_actions, dim_context, reward_type, reward_std, beta, n_deficient_actions, action_context, random_state, err, description\"", ",", "\n", "invalid_input_of_init", ",", "\n", ")", "\n", "def", "test_synthetic_init_using_invalid_inputs", "(", "\n", "n_actions", ",", "\n", "dim_context", ",", "\n", "reward_type", ",", "\n", "reward_std", ",", "\n", "beta", ",", "\n", "n_deficient_actions", ",", "\n", "action_context", ",", "\n", "random_state", ",", "\n", "err", ",", "\n", "description", ",", "\n", ")", ":", "\n", "    ", "with", "pytest", ".", "raises", "(", "err", ",", "match", "=", "f\"{description}*\"", ")", ":", "\n", "        ", "_", "=", "SyntheticBanditDataset", "(", "\n", "n_actions", "=", "n_actions", ",", "\n", "dim_context", "=", "dim_context", ",", "\n", "reward_type", "=", "reward_type", ",", "\n", "reward_std", "=", "reward_std", ",", "\n", "beta", "=", "beta", ",", "\n", "n_deficient_actions", "=", "n_deficient_actions", ",", "\n", "action_context", "=", "action_context", ",", "\n", "random_state", "=", "random_state", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.test_synthetic.test_synthetic_init": [[242, 252], ["obp.dataset.SyntheticBanditDataset", "numpy.eye", "numpy.allclose", "len", "numpy.all", "numpy.all"], "function", ["None"], ["", "", "def", "test_synthetic_init", "(", ")", ":", "\n", "# when reward_function is None, expected_reward is randomly sampled in [0, 1]", "\n", "# this check includes the test of `sample_contextfree_expected_reward` function", "\n", "    ", "dataset", "=", "SyntheticBanditDataset", "(", "n_actions", "=", "2", ",", "beta", "=", "0", ")", "\n", "assert", "len", "(", "dataset", ".", "expected_reward", ")", "==", "2", "\n", "assert", "np", ".", "all", "(", "0", "<=", "dataset", ".", "expected_reward", ")", "and", "np", ".", "all", "(", "dataset", ".", "expected_reward", "<=", "1", ")", "\n", "\n", "# one-hot action_context when None is given", "\n", "ohe", "=", "np", ".", "eye", "(", "2", ",", "dtype", "=", "int", ")", "\n", "assert", "np", ".", "allclose", "(", "dataset", ".", "action_context", ",", "ohe", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.test_synthetic.test_synthetic_sample_reward_using_invalid_inputs": [[287, 297], ["pytest.mark.parametrize", "obp.dataset.SyntheticBanditDataset", "pytest.raises", "obp.dataset.SyntheticBanditDataset.sample_reward"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.synthetic.SyntheticBanditDataset.sample_reward"], ["@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"context, action, description\"", ",", "\n", "invalid_input_of_sample_reward", ",", "\n", ")", "\n", "def", "test_synthetic_sample_reward_using_invalid_inputs", "(", "context", ",", "action", ",", "description", ")", ":", "\n", "    ", "n_actions", "=", "10", "\n", "dataset", "=", "SyntheticBanditDataset", "(", "n_actions", "=", "n_actions", ")", "\n", "\n", "with", "pytest", ".", "raises", "(", "ValueError", ",", "match", "=", "f\"{description}*\"", ")", ":", "\n", "        ", "_", "=", "dataset", ".", "sample_reward", "(", "context", "=", "context", ",", "action", "=", "action", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.test_synthetic.test_synthetic_sample_reward_using_valid_inputs": [[299, 310], ["pytest.mark.parametrize", "obp.dataset.SyntheticBanditDataset", "obp.dataset.SyntheticBanditDataset.sample_reward", "isinstance"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.synthetic.SyntheticBanditDataset.sample_reward"], ["", "", "@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"context, action, description\"", ",", "\n", "valid_input_of_sample_reward", ",", "\n", ")", "\n", "def", "test_synthetic_sample_reward_using_valid_inputs", "(", "context", ",", "action", ",", "description", ")", ":", "\n", "    ", "n_actions", "=", "10", "\n", "dataset", "=", "SyntheticBanditDataset", "(", "n_actions", "=", "n_actions", ",", "dim_context", "=", "3", ")", "\n", "\n", "reward", "=", "dataset", ".", "sample_reward", "(", "context", "=", "context", ",", "action", "=", "action", ")", "\n", "assert", "isinstance", "(", "reward", ",", "np", ".", "ndarray", ")", ",", "\"Invalid response of sample_reward\"", "\n", "assert", "reward", ".", "shape", "==", "action", ".", "shape", ",", "\"Invalid response of sample_reward\"", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.test_synthetic.test_synthetic_obtain_batch_bandit_feedback": [[312, 366], ["pytest.raises", "obp.dataset.SyntheticBanditDataset", "obp.dataset.SyntheticBanditDataset.obtain_batch_bandit_feedback", "pytest.raises", "obp.dataset.SyntheticBanditDataset", "obp.dataset.SyntheticBanditDataset.obtain_batch_bandit_feedback", "obp.dataset.SyntheticBanditDataset", "obp.dataset.SyntheticBanditDataset.obtain_batch_bandit_feedback", "numpy.allclose", "numpy.allclose", "[].sum", "numpy.ones", "len", "len", "numpy.ones_like", "len"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.multiclass.MultiClassToBanditReduction.obtain_batch_bandit_feedback", "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.multiclass.MultiClassToBanditReduction.obtain_batch_bandit_feedback", "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.multiclass.MultiClassToBanditReduction.obtain_batch_bandit_feedback"], ["", "def", "test_synthetic_obtain_batch_bandit_feedback", "(", ")", ":", "\n", "# n_rounds", "\n", "    ", "with", "pytest", ".", "raises", "(", "ValueError", ")", ":", "\n", "        ", "dataset", "=", "SyntheticBanditDataset", "(", "n_actions", "=", "2", ")", "\n", "dataset", ".", "obtain_batch_bandit_feedback", "(", "n_rounds", "=", "0", ")", "\n", "\n", "", "with", "pytest", ".", "raises", "(", "TypeError", ")", ":", "\n", "        ", "dataset", "=", "SyntheticBanditDataset", "(", "n_actions", "=", "2", ")", "\n", "dataset", ".", "obtain_batch_bandit_feedback", "(", "n_rounds", "=", "\"3\"", ")", "\n", "\n", "# bandit feedback", "\n", "", "n_rounds", "=", "10", "\n", "n_actions", "=", "5", "\n", "for", "n_deficient_actions", "in", "[", "0", ",", "2", "]", ":", "\n", "        ", "dataset", "=", "SyntheticBanditDataset", "(", "\n", "n_actions", "=", "n_actions", ",", "beta", "=", "0", ",", "n_deficient_actions", "=", "n_deficient_actions", "\n", ")", "\n", "bandit_feedback", "=", "dataset", ".", "obtain_batch_bandit_feedback", "(", "n_rounds", "=", "n_rounds", ")", "\n", "assert", "bandit_feedback", "[", "\"n_rounds\"", "]", "==", "n_rounds", "\n", "assert", "bandit_feedback", "[", "\"n_actions\"", "]", "==", "n_actions", "\n", "assert", "(", "\n", "bandit_feedback", "[", "\"context\"", "]", ".", "shape", "[", "0", "]", "==", "n_rounds", "# n_rounds", "\n", "and", "bandit_feedback", "[", "\"context\"", "]", ".", "shape", "[", "1", "]", "==", "1", "# default dim_context", "\n", ")", "\n", "assert", "(", "\n", "bandit_feedback", "[", "\"action_context\"", "]", ".", "shape", "[", "0", "]", "==", "n_actions", "\n", "and", "bandit_feedback", "[", "\"action_context\"", "]", ".", "shape", "[", "1", "]", "==", "n_actions", "\n", ")", "\n", "assert", "(", "\n", "bandit_feedback", "[", "\"action\"", "]", ".", "ndim", "==", "1", "\n", "and", "len", "(", "bandit_feedback", "[", "\"action\"", "]", ")", "==", "n_rounds", "\n", ")", "\n", "assert", "bandit_feedback", "[", "\"position\"", "]", "is", "None", "\n", "assert", "(", "\n", "bandit_feedback", "[", "\"reward\"", "]", ".", "ndim", "==", "1", "\n", "and", "len", "(", "bandit_feedback", "[", "\"reward\"", "]", ")", "==", "n_rounds", "\n", ")", "\n", "assert", "(", "\n", "bandit_feedback", "[", "\"expected_reward\"", "]", ".", "shape", "[", "0", "]", "==", "n_rounds", "\n", "and", "bandit_feedback", "[", "\"expected_reward\"", "]", ".", "shape", "[", "1", "]", "==", "n_actions", "\n", ")", "\n", "assert", "(", "\n", "bandit_feedback", "[", "\"pi_b\"", "]", ".", "shape", "[", "0", "]", "==", "n_rounds", "\n", "and", "bandit_feedback", "[", "\"pi_b\"", "]", ".", "shape", "[", "1", "]", "==", "n_actions", "\n", ")", "\n", "# when `beta=0`, behavior_policy should be uniform", "\n", "if", "n_deficient_actions", "==", "0", ":", "\n", "            ", "uniform_policy", "=", "np", ".", "ones_like", "(", "bandit_feedback", "[", "\"pi_b\"", "]", ")", "/", "n_actions", "\n", "assert", "np", ".", "allclose", "(", "bandit_feedback", "[", "\"pi_b\"", "]", ",", "uniform_policy", ")", "\n", "", "assert", "np", ".", "allclose", "(", "bandit_feedback", "[", "\"pi_b\"", "]", "[", ":", ",", ":", ",", "0", "]", ".", "sum", "(", "1", ")", ",", "np", ".", "ones", "(", "n_rounds", ")", ")", "\n", "assert", "(", "bandit_feedback", "[", "\"pi_b\"", "]", "==", "0", ")", ".", "sum", "(", ")", "==", "n_deficient_actions", "*", "n_rounds", "\n", "assert", "(", "\n", "bandit_feedback", "[", "\"pscore\"", "]", ".", "ndim", "==", "1", "\n", "and", "len", "(", "bandit_feedback", "[", "\"pscore\"", "]", ")", "==", "n_rounds", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.test_synthetic.test_synthetic_calc_policy_value_using_invalid_inputs": [[397, 412], ["pytest.mark.parametrize", "obp.dataset.SyntheticBanditDataset", "pytest.raises", "obp.dataset.SyntheticBanditDataset.calc_ground_truth_policy_value"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.simulator.simulator.calc_ground_truth_policy_value"], ["@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"expected_reward, action_dist, description\"", ",", "\n", "invalid_input_of_calc_policy_value", ",", "\n", ")", "\n", "def", "test_synthetic_calc_policy_value_using_invalid_inputs", "(", "\n", "expected_reward", ",", "\n", "action_dist", ",", "\n", "description", ",", "\n", ")", ":", "\n", "    ", "n_actions", "=", "10", "\n", "dataset", "=", "SyntheticBanditDataset", "(", "n_actions", "=", "n_actions", ")", "\n", "\n", "with", "pytest", ".", "raises", "(", "ValueError", ",", "match", "=", "f\"{description}*\"", ")", ":", "\n", "        ", "_", "=", "dataset", ".", "calc_ground_truth_policy_value", "(", "\n", "expected_reward", "=", "expected_reward", ",", "action_dist", "=", "action_dist", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.test_synthetic.test_synthetic_calc_policy_value_using_valid_inputs": [[415, 433], ["pytest.mark.parametrize", "obp.dataset.SyntheticBanditDataset", "obp.dataset.SyntheticBanditDataset.calc_ground_truth_policy_value", "isinstance"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.simulator.simulator.calc_ground_truth_policy_value"], ["", "", "@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"expected_reward, action_dist, description\"", ",", "\n", "valid_input_of_calc_policy_value", ",", "\n", ")", "\n", "def", "test_synthetic_calc_policy_value_using_valid_inputs", "(", "\n", "expected_reward", ",", "\n", "action_dist", ",", "\n", "description", ",", "\n", ")", ":", "\n", "    ", "n_actions", "=", "10", "\n", "dataset", "=", "SyntheticBanditDataset", "(", "n_actions", "=", "n_actions", ")", "\n", "\n", "policy_value", "=", "dataset", ".", "calc_ground_truth_policy_value", "(", "\n", "expected_reward", "=", "expected_reward", ",", "action_dist", "=", "action_dist", "\n", ")", "\n", "assert", "isinstance", "(", "\n", "policy_value", ",", "float", "\n", ")", ",", "\"Invalid response of calc_ground_truth_policy_value\"", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.test_synthetic.test_synthetic_logistic_reward_function": [[435, 477], ["numpy.ones", "numpy.eye", "logistic_reward_function_", "pytest.raises", "numpy.array", "logistic_reward_function_", "pytest.raises", "logistic_reward_function_", "pytest.raises", "numpy.array", "logistic_reward_function_", "pytest.raises", "logistic_reward_function_", "numpy.all", "numpy.all", "numpy.eye", "numpy.eye", "numpy.ones", "numpy.ones"], "function", ["None"], ["", "def", "test_synthetic_logistic_reward_function", "(", ")", ":", "\n", "    ", "for", "logistic_reward_function_", "in", "[", "\n", "logistic_reward_function", ",", "\n", "logistic_polynomial_reward_function", ",", "\n", "logistic_sparse_reward_function", ",", "\n", "]", ":", "\n", "# context", "\n", "        ", "with", "pytest", ".", "raises", "(", "ValueError", ")", ":", "\n", "            ", "context", "=", "np", ".", "array", "(", "[", "1.0", ",", "1.0", "]", ")", "\n", "logistic_reward_function_", "(", "context", "=", "context", ",", "action_context", "=", "np", ".", "eye", "(", "2", ")", ")", "\n", "\n", "", "with", "pytest", ".", "raises", "(", "ValueError", ")", ":", "\n", "            ", "context", "=", "[", "1.0", ",", "1.0", "]", "\n", "logistic_reward_function_", "(", "context", "=", "context", ",", "action_context", "=", "np", ".", "eye", "(", "2", ")", ")", "\n", "\n", "# action_context", "\n", "", "with", "pytest", ".", "raises", "(", "ValueError", ")", ":", "\n", "            ", "action_context", "=", "np", ".", "array", "(", "[", "1.0", ",", "1.0", "]", ")", "\n", "logistic_reward_function_", "(", "\n", "context", "=", "np", ".", "ones", "(", "[", "2", ",", "2", "]", ")", ",", "action_context", "=", "action_context", "\n", ")", "\n", "\n", "", "with", "pytest", ".", "raises", "(", "ValueError", ")", ":", "\n", "            ", "action_context", "=", "[", "1.0", ",", "1.0", "]", "\n", "logistic_reward_function_", "(", "\n", "context", "=", "np", ".", "ones", "(", "[", "2", ",", "2", "]", ")", ",", "action_context", "=", "action_context", "\n", ")", "\n", "\n", "# expected_reward", "\n", "", "n_rounds", "=", "10", "\n", "dim_context", "=", "3", "\n", "n_actions", "=", "5", "\n", "context", "=", "np", ".", "ones", "(", "[", "n_rounds", ",", "dim_context", "]", ")", "\n", "action_context", "=", "np", ".", "eye", "(", "n_actions", ")", "\n", "expected_reward", "=", "logistic_reward_function_", "(", "\n", "context", "=", "context", ",", "action_context", "=", "action_context", "\n", ")", "\n", "assert", "(", "\n", "expected_reward", ".", "shape", "[", "0", "]", "==", "n_rounds", "\n", "and", "expected_reward", ".", "shape", "[", "1", "]", "==", "n_actions", "\n", ")", "\n", "assert", "np", ".", "all", "(", "0", "<=", "expected_reward", ")", "and", "np", ".", "all", "(", "expected_reward", "<=", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.test_synthetic.test_synthetic_continuous_reward_function": [[479, 519], ["numpy.ones", "numpy.eye", "continuous_reward_function", "pytest.raises", "numpy.array", "continuous_reward_function", "pytest.raises", "continuous_reward_function", "pytest.raises", "numpy.array", "continuous_reward_function", "pytest.raises", "continuous_reward_function", "numpy.eye", "numpy.eye", "numpy.ones", "numpy.ones"], "function", ["None"], ["", "", "def", "test_synthetic_continuous_reward_function", "(", ")", ":", "\n", "    ", "for", "continuous_reward_function", "in", "[", "\n", "linear_reward_function", ",", "\n", "polynomial_reward_function", ",", "\n", "sparse_reward_function", ",", "\n", "]", ":", "\n", "# context", "\n", "        ", "with", "pytest", ".", "raises", "(", "ValueError", ")", ":", "\n", "            ", "context", "=", "np", ".", "array", "(", "[", "1.0", ",", "1.0", "]", ")", "\n", "continuous_reward_function", "(", "context", "=", "context", ",", "action_context", "=", "np", ".", "eye", "(", "2", ")", ")", "\n", "\n", "", "with", "pytest", ".", "raises", "(", "ValueError", ")", ":", "\n", "            ", "context", "=", "[", "1.0", ",", "1.0", "]", "\n", "continuous_reward_function", "(", "context", "=", "context", ",", "action_context", "=", "np", ".", "eye", "(", "2", ")", ")", "\n", "\n", "# action_context", "\n", "", "with", "pytest", ".", "raises", "(", "ValueError", ")", ":", "\n", "            ", "action_context", "=", "np", ".", "array", "(", "[", "1.0", ",", "1.0", "]", ")", "\n", "continuous_reward_function", "(", "\n", "context", "=", "np", ".", "ones", "(", "[", "2", ",", "2", "]", ")", ",", "action_context", "=", "action_context", "\n", ")", "\n", "\n", "", "with", "pytest", ".", "raises", "(", "ValueError", ")", ":", "\n", "            ", "action_context", "=", "[", "1.0", ",", "1.0", "]", "\n", "continuous_reward_function", "(", "\n", "context", "=", "np", ".", "ones", "(", "[", "2", ",", "2", "]", ")", ",", "action_context", "=", "action_context", "\n", ")", "\n", "\n", "# expected_reward", "\n", "", "n_rounds", "=", "10", "\n", "dim_context", "=", "3", "\n", "n_actions", "=", "5", "\n", "context", "=", "np", ".", "ones", "(", "[", "n_rounds", ",", "dim_context", "]", ")", "\n", "action_context", "=", "np", ".", "eye", "(", "n_actions", ")", "\n", "expected_reward", "=", "continuous_reward_function", "(", "\n", "context", "=", "context", ",", "action_context", "=", "action_context", "\n", ")", "\n", "assert", "(", "\n", "expected_reward", ".", "shape", "[", "0", "]", "==", "n_rounds", "\n", "and", "expected_reward", ".", "shape", "[", "1", "]", "==", "n_actions", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.test_synthetic.test_synthetic_behavior_policy_function": [[522, 560], ["numpy.ones", "numpy.eye", "obp.utils.softmax", "pytest.raises", "numpy.array", "behavior_policy_function", "pytest.raises", "behavior_policy_function", "pytest.raises", "numpy.array", "behavior_policy_function", "pytest.raises", "behavior_policy_function", "behavior_policy_function", "numpy.all", "numpy.all", "numpy.eye", "numpy.eye", "numpy.ones", "numpy.ones"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.softmax"], ["", "", "def", "test_synthetic_behavior_policy_function", "(", ")", ":", "\n", "    ", "for", "behavior_policy_function", "in", "[", "\n", "linear_behavior_policy", ",", "\n", "polynomial_behavior_policy", ",", "\n", "]", ":", "\n", "# context", "\n", "        ", "with", "pytest", ".", "raises", "(", "ValueError", ")", ":", "\n", "            ", "context", "=", "np", ".", "array", "(", "[", "1.0", ",", "1.0", "]", ")", "\n", "behavior_policy_function", "(", "context", "=", "context", ",", "action_context", "=", "np", ".", "eye", "(", "2", ")", ")", "\n", "\n", "", "with", "pytest", ".", "raises", "(", "ValueError", ")", ":", "\n", "            ", "context", "=", "[", "1.0", ",", "1.0", "]", "\n", "behavior_policy_function", "(", "context", "=", "context", ",", "action_context", "=", "np", ".", "eye", "(", "2", ")", ")", "\n", "\n", "# action_context", "\n", "", "with", "pytest", ".", "raises", "(", "ValueError", ")", ":", "\n", "            ", "action_context", "=", "np", ".", "array", "(", "[", "1.0", ",", "1.0", "]", ")", "\n", "behavior_policy_function", "(", "\n", "context", "=", "np", ".", "ones", "(", "[", "2", ",", "2", "]", ")", ",", "action_context", "=", "action_context", "\n", ")", "\n", "\n", "", "with", "pytest", ".", "raises", "(", "ValueError", ")", ":", "\n", "            ", "action_context", "=", "[", "1.0", ",", "1.0", "]", "\n", "behavior_policy_function", "(", "\n", "context", "=", "np", ".", "ones", "(", "[", "2", ",", "2", "]", ")", ",", "action_context", "=", "action_context", "\n", ")", "\n", "\n", "# pscore (action choice probabilities by behavior policy)", "\n", "", "n_rounds", "=", "10", "\n", "dim_context", "=", "3", "\n", "n_actions", "=", "5", "\n", "context", "=", "np", ".", "ones", "(", "[", "n_rounds", ",", "dim_context", "]", ")", "\n", "action_context", "=", "np", ".", "eye", "(", "n_actions", ")", "\n", "action_prob", "=", "softmax", "(", "\n", "behavior_policy_function", "(", "context", "=", "context", ",", "action_context", "=", "action_context", ")", "\n", ")", "\n", "assert", "action_prob", ".", "shape", "[", "0", "]", "==", "n_rounds", "and", "action_prob", ".", "shape", "[", "1", "]", "==", "n_actions", "\n", "assert", "np", ".", "all", "(", "0", "<=", "action_prob", ")", "and", "np", ".", "all", "(", "action_prob", "<=", "1", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.test_synthetic_embed.test_synthetic_init_using_invalid_inputs": [[553, 591], ["pytest.mark.parametrize", "pytest.raises", "obp.dataset.SyntheticBanditDatasetWithActionEmbeds"], "function", ["None"], ["@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"n_actions, dim_context, reward_type, reward_std, beta, n_cat_per_dim, latent_param_mat_dim, n_cat_dim, p_e_a_param_std, n_unobserved_cat_dim, n_irrelevant_cat_dim, n_deficient_actions, action_context, random_state, err, description\"", ",", "\n", "invalid_input_of_init", ",", "\n", ")", "\n", "def", "test_synthetic_init_using_invalid_inputs", "(", "\n", "n_actions", ",", "\n", "dim_context", ",", "\n", "reward_type", ",", "\n", "reward_std", ",", "\n", "beta", ",", "\n", "n_cat_per_dim", ",", "\n", "latent_param_mat_dim", ",", "\n", "n_cat_dim", ",", "\n", "p_e_a_param_std", ",", "\n", "n_unobserved_cat_dim", ",", "\n", "n_irrelevant_cat_dim", ",", "\n", "n_deficient_actions", ",", "\n", "action_context", ",", "\n", "random_state", ",", "\n", "err", ",", "\n", "description", ",", "\n", ")", ":", "\n", "    ", "with", "pytest", ".", "raises", "(", "err", ",", "match", "=", "f\"{description}*\"", ")", ":", "\n", "        ", "_", "=", "SyntheticBanditDatasetWithActionEmbeds", "(", "\n", "n_actions", "=", "n_actions", ",", "\n", "dim_context", "=", "dim_context", ",", "\n", "reward_type", "=", "reward_type", ",", "\n", "reward_std", "=", "reward_std", ",", "\n", "beta", "=", "beta", ",", "\n", "n_deficient_actions", "=", "n_deficient_actions", ",", "\n", "n_cat_per_dim", "=", "n_cat_per_dim", ",", "\n", "latent_param_mat_dim", "=", "latent_param_mat_dim", ",", "\n", "n_cat_dim", "=", "n_cat_dim", ",", "\n", "p_e_a_param_std", "=", "p_e_a_param_std", ",", "\n", "n_unobserved_cat_dim", "=", "n_unobserved_cat_dim", ",", "\n", "n_irrelevant_cat_dim", "=", "n_irrelevant_cat_dim", ",", "\n", "action_context", "=", "action_context", ",", "\n", "random_state", "=", "random_state", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.test_synthetic_embed.test_synthetic_init": [[594, 604], ["obp.dataset.SyntheticBanditDatasetWithActionEmbeds", "numpy.eye", "numpy.allclose", "len", "numpy.all", "numpy.all"], "function", ["None"], ["", "", "def", "test_synthetic_init", "(", ")", ":", "\n", "# when reward_function is None, expected_reward is randomly sampled in [0, 1]", "\n", "# this check includes the test of `sample_contextfree_expected_reward` function", "\n", "    ", "dataset", "=", "SyntheticBanditDatasetWithActionEmbeds", "(", "n_actions", "=", "2", ",", "beta", "=", "0", ")", "\n", "assert", "len", "(", "dataset", ".", "expected_reward", ")", "==", "2", "\n", "assert", "np", ".", "all", "(", "0", "<=", "dataset", ".", "expected_reward", ")", "and", "np", ".", "all", "(", "dataset", ".", "expected_reward", "<=", "1", ")", "\n", "\n", "# one-hot action_context when None is given", "\n", "ohe", "=", "np", ".", "eye", "(", "2", ",", "dtype", "=", "int", ")", "\n", "assert", "np", ".", "allclose", "(", "dataset", ".", "action_context", ",", "ohe", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.test_synthetic_embed.test_synthetic_sample_reward_using_invalid_inputs": [[639, 649], ["pytest.mark.parametrize", "obp.dataset.SyntheticBanditDatasetWithActionEmbeds", "pytest.raises", "obp.dataset.SyntheticBanditDatasetWithActionEmbeds.sample_reward"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.synthetic.SyntheticBanditDataset.sample_reward"], ["@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"context, action, description\"", ",", "\n", "invalid_input_of_sample_reward", ",", "\n", ")", "\n", "def", "test_synthetic_sample_reward_using_invalid_inputs", "(", "context", ",", "action", ",", "description", ")", ":", "\n", "    ", "n_actions", "=", "10", "\n", "dataset", "=", "SyntheticBanditDatasetWithActionEmbeds", "(", "n_actions", "=", "n_actions", ")", "\n", "\n", "with", "pytest", ".", "raises", "(", "ValueError", ",", "match", "=", "f\"{description}*\"", ")", ":", "\n", "        ", "_", "=", "dataset", ".", "sample_reward", "(", "context", "=", "context", ",", "action", "=", "action", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.test_synthetic_embed.test_synthetic_sample_reward_using_valid_inputs": [[651, 662], ["pytest.mark.parametrize", "obp.dataset.SyntheticBanditDatasetWithActionEmbeds", "obp.dataset.SyntheticBanditDatasetWithActionEmbeds.sample_reward", "isinstance"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.synthetic.SyntheticBanditDataset.sample_reward"], ["", "", "@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"context, action, description\"", ",", "\n", "valid_input_of_sample_reward", ",", "\n", ")", "\n", "def", "test_synthetic_sample_reward_using_valid_inputs", "(", "context", ",", "action", ",", "description", ")", ":", "\n", "    ", "n_actions", "=", "10", "\n", "dataset", "=", "SyntheticBanditDatasetWithActionEmbeds", "(", "n_actions", "=", "n_actions", ",", "dim_context", "=", "3", ")", "\n", "\n", "reward", "=", "dataset", ".", "sample_reward", "(", "context", "=", "context", ",", "action", "=", "action", ")", "\n", "assert", "isinstance", "(", "reward", ",", "np", ".", "ndarray", ")", ",", "\"Invalid response of sample_reward\"", "\n", "assert", "reward", ".", "shape", "==", "action", ".", "shape", ",", "\"Invalid response of sample_reward\"", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.test_synthetic_embed.test_synthetic_obtain_batch_bandit_feedback": [[664, 739], ["pytest.raises", "obp.dataset.SyntheticBanditDatasetWithActionEmbeds", "obp.dataset.SyntheticBanditDatasetWithActionEmbeds.obtain_batch_bandit_feedback", "pytest.raises", "obp.dataset.SyntheticBanditDatasetWithActionEmbeds", "obp.dataset.SyntheticBanditDatasetWithActionEmbeds.obtain_batch_bandit_feedback", "obp.dataset.SyntheticBanditDatasetWithActionEmbeds", "obp.dataset.SyntheticBanditDatasetWithActionEmbeds.obtain_batch_bandit_feedback", "numpy.allclose", "numpy.allclose", "[].sum", "numpy.ones", "len", "len", "numpy.ones_like", "len"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.multiclass.MultiClassToBanditReduction.obtain_batch_bandit_feedback", "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.multiclass.MultiClassToBanditReduction.obtain_batch_bandit_feedback", "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.multiclass.MultiClassToBanditReduction.obtain_batch_bandit_feedback"], ["", "def", "test_synthetic_obtain_batch_bandit_feedback", "(", ")", ":", "\n", "# n_rounds", "\n", "    ", "with", "pytest", ".", "raises", "(", "ValueError", ")", ":", "\n", "        ", "dataset", "=", "SyntheticBanditDatasetWithActionEmbeds", "(", "n_actions", "=", "2", ")", "\n", "dataset", ".", "obtain_batch_bandit_feedback", "(", "n_rounds", "=", "0", ")", "\n", "\n", "", "with", "pytest", ".", "raises", "(", "TypeError", ")", ":", "\n", "        ", "dataset", "=", "SyntheticBanditDatasetWithActionEmbeds", "(", "n_actions", "=", "2", ")", "\n", "dataset", ".", "obtain_batch_bandit_feedback", "(", "n_rounds", "=", "\"3\"", ")", "\n", "\n", "# bandit feedback", "\n", "", "n_rounds", "=", "10", "\n", "n_actions", "=", "5", "\n", "n_cat_dim", "=", "3", "\n", "n_cat_per_dim", "=", "5", "\n", "for", "n_deficient_actions", "in", "[", "0", ",", "2", "]", ":", "\n", "        ", "dataset", "=", "SyntheticBanditDatasetWithActionEmbeds", "(", "\n", "n_actions", "=", "n_actions", ",", "\n", "beta", "=", "0", ",", "\n", "n_cat_per_dim", "=", "n_cat_per_dim", ",", "\n", "n_cat_dim", "=", "n_cat_dim", ",", "\n", "reward_function", "=", "logistic_reward_function", ",", "\n", "n_deficient_actions", "=", "n_deficient_actions", ",", "\n", ")", "\n", "bandit_feedback", "=", "dataset", ".", "obtain_batch_bandit_feedback", "(", "n_rounds", "=", "n_rounds", ")", "\n", "assert", "bandit_feedback", "[", "\"n_rounds\"", "]", "==", "n_rounds", "\n", "assert", "bandit_feedback", "[", "\"n_actions\"", "]", "==", "n_actions", "\n", "assert", "(", "\n", "bandit_feedback", "[", "\"context\"", "]", ".", "shape", "[", "0", "]", "==", "n_rounds", "# n_rounds", "\n", "and", "bandit_feedback", "[", "\"context\"", "]", ".", "shape", "[", "1", "]", "==", "1", "# default dim_context", "\n", ")", "\n", "assert", "(", "\n", "bandit_feedback", "[", "\"action_context\"", "]", ".", "shape", "[", "0", "]", "==", "n_actions", "\n", "and", "bandit_feedback", "[", "\"action_context\"", "]", ".", "shape", "[", "1", "]", "==", "n_cat_dim", "\n", ")", "\n", "assert", "(", "\n", "bandit_feedback", "[", "\"action_embed\"", "]", ".", "shape", "[", "0", "]", "==", "n_rounds", "\n", "and", "bandit_feedback", "[", "\"action_embed\"", "]", ".", "shape", "[", "1", "]", "==", "n_cat_dim", "\n", ")", "\n", "assert", "(", "\n", "bandit_feedback", "[", "\"action\"", "]", ".", "ndim", "==", "1", "\n", "and", "len", "(", "bandit_feedback", "[", "\"action\"", "]", ")", "==", "n_rounds", "\n", ")", "\n", "assert", "bandit_feedback", "[", "\"position\"", "]", "is", "None", "\n", "assert", "(", "\n", "bandit_feedback", "[", "\"reward\"", "]", ".", "ndim", "==", "1", "\n", "and", "len", "(", "bandit_feedback", "[", "\"reward\"", "]", ")", "==", "n_rounds", "\n", ")", "\n", "assert", "(", "\n", "bandit_feedback", "[", "\"expected_reward\"", "]", ".", "shape", "[", "0", "]", "==", "n_rounds", "\n", "and", "bandit_feedback", "[", "\"expected_reward\"", "]", ".", "shape", "[", "1", "]", "==", "n_actions", "\n", ")", "\n", "assert", "(", "\n", "bandit_feedback", "[", "\"q_x_e\"", "]", ".", "shape", "[", "0", "]", "==", "n_rounds", "\n", "and", "bandit_feedback", "[", "\"q_x_e\"", "]", ".", "shape", "[", "1", "]", "==", "n_cat_per_dim", "\n", "and", "bandit_feedback", "[", "\"q_x_e\"", "]", ".", "shape", "[", "2", "]", "==", "n_cat_dim", "\n", ")", "\n", "assert", "(", "\n", "bandit_feedback", "[", "\"p_e_a\"", "]", ".", "shape", "[", "0", "]", "==", "n_actions", "\n", "and", "bandit_feedback", "[", "\"p_e_a\"", "]", ".", "shape", "[", "1", "]", "==", "n_cat_per_dim", "\n", "and", "bandit_feedback", "[", "\"p_e_a\"", "]", ".", "shape", "[", "2", "]", "==", "n_cat_dim", "\n", ")", "\n", "assert", "(", "\n", "bandit_feedback", "[", "\"pi_b\"", "]", ".", "shape", "[", "0", "]", "==", "n_rounds", "\n", "and", "bandit_feedback", "[", "\"pi_b\"", "]", ".", "shape", "[", "1", "]", "==", "n_actions", "\n", ")", "\n", "# when `beta=0`, behavior_policy should be uniform", "\n", "if", "n_deficient_actions", "==", "0", ":", "\n", "            ", "uniform_policy", "=", "np", ".", "ones_like", "(", "bandit_feedback", "[", "\"pi_b\"", "]", ")", "/", "n_actions", "\n", "assert", "np", ".", "allclose", "(", "bandit_feedback", "[", "\"pi_b\"", "]", ",", "uniform_policy", ")", "\n", "", "assert", "np", ".", "allclose", "(", "bandit_feedback", "[", "\"pi_b\"", "]", "[", ":", ",", ":", ",", "0", "]", ".", "sum", "(", "1", ")", ",", "np", ".", "ones", "(", "n_rounds", ")", ")", "\n", "assert", "(", "bandit_feedback", "[", "\"pi_b\"", "]", "==", "0", ")", ".", "sum", "(", ")", "==", "n_deficient_actions", "*", "n_rounds", "\n", "assert", "(", "\n", "bandit_feedback", "[", "\"pscore\"", "]", ".", "ndim", "==", "1", "\n", "and", "len", "(", "bandit_feedback", "[", "\"pscore\"", "]", ")", "==", "n_rounds", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.test_synthetic_embed.test_synthetic_calc_policy_value_using_invalid_inputs": [[770, 785], ["pytest.mark.parametrize", "obp.dataset.SyntheticBanditDatasetWithActionEmbeds", "pytest.raises", "obp.dataset.SyntheticBanditDatasetWithActionEmbeds.calc_ground_truth_policy_value"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.simulator.simulator.calc_ground_truth_policy_value"], ["@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"expected_reward, action_dist, description\"", ",", "\n", "invalid_input_of_calc_policy_value", ",", "\n", ")", "\n", "def", "test_synthetic_calc_policy_value_using_invalid_inputs", "(", "\n", "expected_reward", ",", "\n", "action_dist", ",", "\n", "description", ",", "\n", ")", ":", "\n", "    ", "n_actions", "=", "10", "\n", "dataset", "=", "SyntheticBanditDatasetWithActionEmbeds", "(", "n_actions", "=", "n_actions", ")", "\n", "\n", "with", "pytest", ".", "raises", "(", "ValueError", ",", "match", "=", "f\"{description}*\"", ")", ":", "\n", "        ", "_", "=", "dataset", ".", "calc_ground_truth_policy_value", "(", "\n", "expected_reward", "=", "expected_reward", ",", "action_dist", "=", "action_dist", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.test_synthetic_embed.test_synthetic_calc_policy_value_using_valid_inputs": [[788, 806], ["pytest.mark.parametrize", "obp.dataset.SyntheticBanditDatasetWithActionEmbeds", "obp.dataset.SyntheticBanditDatasetWithActionEmbeds.calc_ground_truth_policy_value", "isinstance"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.simulator.simulator.calc_ground_truth_policy_value"], ["", "", "@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"expected_reward, action_dist, description\"", ",", "\n", "valid_input_of_calc_policy_value", ",", "\n", ")", "\n", "def", "test_synthetic_calc_policy_value_using_valid_inputs", "(", "\n", "expected_reward", ",", "\n", "action_dist", ",", "\n", "description", ",", "\n", ")", ":", "\n", "    ", "n_actions", "=", "10", "\n", "dataset", "=", "SyntheticBanditDatasetWithActionEmbeds", "(", "n_actions", "=", "n_actions", ")", "\n", "\n", "policy_value", "=", "dataset", ".", "calc_ground_truth_policy_value", "(", "\n", "expected_reward", "=", "expected_reward", ",", "action_dist", "=", "action_dist", "\n", ")", "\n", "assert", "isinstance", "(", "\n", "policy_value", ",", "float", "\n", ")", ",", "\"Invalid response of calc_ground_truth_policy_value\"", "\n", "", ""]], "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.test_real.test_real_init": [[11, 39], ["obp.dataset.OpenBanditDataset", "pytest.raises", "obp.dataset.OpenBanditDataset", "pytest.raises", "obp.dataset.OpenBanditDataset", "pytest.raises", "obp.dataset.OpenBanditDataset", "isinstance", "isinstance", "isinstance", "isinstance", "isinstance", "isinstance", "isinstance", "isinstance"], "function", ["None"], ["def", "test_real_init", "(", ")", ":", "\n", "# behavior_policy", "\n", "    ", "with", "pytest", ".", "raises", "(", "ValueError", ")", ":", "\n", "        ", "OpenBanditDataset", "(", "behavior_policy", "=", "\"aaa\"", ",", "campaign", "=", "\"all\"", ")", "\n", "\n", "# campaign", "\n", "", "with", "pytest", ".", "raises", "(", "ValueError", ")", ":", "\n", "        ", "OpenBanditDataset", "(", "behavior_policy", "=", "\"random\"", ",", "campaign", "=", "\"aaa\"", ")", "\n", "\n", "# data_path", "\n", "", "with", "pytest", ".", "raises", "(", "ValueError", ")", ":", "\n", "        ", "OpenBanditDataset", "(", "behavior_policy", "=", "\"random\"", ",", "campaign", "=", "\"all\"", ",", "data_path", "=", "5", ")", "\n", "\n", "# load_raw_data", "\n", "", "obd", "=", "OpenBanditDataset", "(", "behavior_policy", "=", "\"random\"", ",", "campaign", "=", "\"all\"", ")", "\n", "# check the value exists and has the right type", "\n", "assert", "(", "\n", "isinstance", "(", "obd", ".", "data", ",", "pd", ".", "DataFrame", ")", "\n", "and", "isinstance", "(", "obd", ".", "item_context", ",", "pd", ".", "DataFrame", ")", "\n", "and", "isinstance", "(", "obd", ".", "action", ",", "np", ".", "ndarray", ")", "\n", "and", "isinstance", "(", "obd", ".", "position", ",", "np", ".", "ndarray", ")", "\n", "and", "isinstance", "(", "obd", ".", "reward", ",", "np", ".", "ndarray", ")", "\n", "and", "isinstance", "(", "obd", ".", "pscore", ",", "np", ".", "ndarray", ")", "\n", ")", "\n", "\n", "# pre_process (context and action_context)", "\n", "assert", "isinstance", "(", "obd", ".", "context", ",", "np", ".", "ndarray", ")", "and", "isinstance", "(", "\n", "obd", ".", "action_context", ",", "np", ".", "ndarray", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.test_real.test_obtain_batch_bandit_feedback": [[42, 94], ["obp.dataset.OpenBanditDataset", "obp.dataset.OpenBanditDataset.obtain_batch_bandit_feedback", "obp.dataset.OpenBanditDataset.obtain_batch_bandit_feedback", "isinstance", "all", "all", "pytest.raises", "obp.dataset.OpenBanditDataset", "obp.dataset.OpenBanditDataset.obtain_batch_bandit_feedback", "pytest.raises", "obp.dataset.OpenBanditDataset", "obp.dataset.OpenBanditDataset.obtain_batch_bandit_feedback", "pytest.raises", "obp.dataset.OpenBanditDataset", "obp.dataset.OpenBanditDataset.obtain_batch_bandit_feedback", "pytest.raises", "obp.dataset.OpenBanditDataset", "obp.dataset.OpenBanditDataset.obtain_batch_bandit_feedback", "dataset.obtain_batch_bandit_feedback.keys", "dataset.obtain_batch_bandit_feedback.keys", "dataset.obtain_batch_bandit_feedback.keys", "dataset.obtain_batch_bandit_feedback.keys", "dataset.obtain_batch_bandit_feedback.keys", "dataset.obtain_batch_bandit_feedback.keys", "dataset.obtain_batch_bandit_feedback.keys", "dataset.obtain_batch_bandit_feedback.keys", "bandit_feedback_train.keys", "bandit_feedback_test.keys"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.multiclass.MultiClassToBanditReduction.obtain_batch_bandit_feedback", "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.multiclass.MultiClassToBanditReduction.obtain_batch_bandit_feedback", "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.multiclass.MultiClassToBanditReduction.obtain_batch_bandit_feedback", "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.multiclass.MultiClassToBanditReduction.obtain_batch_bandit_feedback", "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.multiclass.MultiClassToBanditReduction.obtain_batch_bandit_feedback", "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.multiclass.MultiClassToBanditReduction.obtain_batch_bandit_feedback"], ["", "def", "test_obtain_batch_bandit_feedback", "(", ")", ":", "\n", "# invalid test_size", "\n", "    ", "with", "pytest", ".", "raises", "(", "ValueError", ")", ":", "\n", "        ", "dataset", "=", "OpenBanditDataset", "(", "behavior_policy", "=", "\"random\"", ",", "campaign", "=", "\"all\"", ")", "\n", "dataset", ".", "obtain_batch_bandit_feedback", "(", "is_timeseries_split", "=", "True", ",", "test_size", "=", "1.3", ")", "\n", "\n", "", "with", "pytest", ".", "raises", "(", "ValueError", ")", ":", "\n", "        ", "dataset", "=", "OpenBanditDataset", "(", "behavior_policy", "=", "\"random\"", ",", "campaign", "=", "\"all\"", ")", "\n", "dataset", ".", "obtain_batch_bandit_feedback", "(", "is_timeseries_split", "=", "True", ",", "test_size", "=", "-", "0.5", ")", "\n", "\n", "", "with", "pytest", ".", "raises", "(", "TypeError", ")", ":", "\n", "        ", "dataset", "=", "OpenBanditDataset", "(", "behavior_policy", "=", "\"random\"", ",", "campaign", "=", "\"all\"", ")", "\n", "dataset", ".", "obtain_batch_bandit_feedback", "(", "is_timeseries_split", "=", "True", ",", "test_size", "=", "\"0.5\"", ")", "\n", "\n", "", "with", "pytest", ".", "raises", "(", "TypeError", ")", ":", "\n", "        ", "dataset", "=", "OpenBanditDataset", "(", "behavior_policy", "=", "\"random\"", ",", "campaign", "=", "\"all\"", ")", "\n", "dataset", ".", "obtain_batch_bandit_feedback", "(", "is_timeseries_split", "=", "\"True\"", ",", "test_size", "=", "0.5", ")", "\n", "\n", "# existence of keys", "\n", "# is_timeseries_split=False (default)", "\n", "", "dataset", "=", "OpenBanditDataset", "(", "behavior_policy", "=", "\"random\"", ",", "campaign", "=", "\"all\"", ")", "\n", "bandit_feedback", "=", "dataset", ".", "obtain_batch_bandit_feedback", "(", ")", "\n", "\n", "assert", "\"n_rounds\"", "in", "bandit_feedback", ".", "keys", "(", ")", "\n", "assert", "\"n_actions\"", "in", "bandit_feedback", ".", "keys", "(", ")", "\n", "assert", "\"action\"", "in", "bandit_feedback", ".", "keys", "(", ")", "\n", "assert", "\"position\"", "in", "bandit_feedback", ".", "keys", "(", ")", "\n", "assert", "\"reward\"", "in", "bandit_feedback", ".", "keys", "(", ")", "\n", "assert", "\"pscore\"", "in", "bandit_feedback", ".", "keys", "(", ")", "\n", "assert", "\"context\"", "in", "bandit_feedback", ".", "keys", "(", ")", "\n", "assert", "\"action_context\"", "in", "bandit_feedback", ".", "keys", "(", ")", "\n", "\n", "# is_timeseries_split=True", "\n", "bandit_feedback_timeseries", "=", "dataset", ".", "obtain_batch_bandit_feedback", "(", "\n", "is_timeseries_split", "=", "True", "\n", ")", "\n", "assert", "isinstance", "(", "bandit_feedback_timeseries", ",", "Tuple", ")", "\n", "bandit_feedback_train", "=", "bandit_feedback_timeseries", "[", "0", "]", "\n", "bandit_feedback_test", "=", "bandit_feedback_timeseries", "[", "1", "]", "\n", "\n", "bf_elems", "=", "{", "\n", "\"n_rounds\"", ",", "\n", "\"n_actions\"", ",", "\n", "\"action\"", ",", "\n", "\"position\"", ",", "\n", "\"reward\"", ",", "\n", "\"pscore\"", ",", "\n", "\"context\"", ",", "\n", "\"action_context\"", ",", "\n", "}", "\n", "assert", "all", "(", "k", "in", "bandit_feedback_train", ".", "keys", "(", ")", "for", "k", "in", "bf_elems", ")", "\n", "assert", "all", "(", "k", "in", "bandit_feedback_test", ".", "keys", "(", ")", "for", "k", "in", "bf_elems", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.test_real.test_calc_on_policy_policy_value_estimate": [[96, 101], ["obp.dataset.OpenBanditDataset.calc_on_policy_policy_value_estimate", "isinstance"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.real.OpenBanditDataset.calc_on_policy_policy_value_estimate"], ["", "def", "test_calc_on_policy_policy_value_estimate", "(", ")", ":", "\n", "    ", "ground_truth_policy_value", "=", "OpenBanditDataset", ".", "calc_on_policy_policy_value_estimate", "(", "\n", "behavior_policy", "=", "\"random\"", ",", "campaign", "=", "\"all\"", "\n", ")", "\n", "assert", "isinstance", "(", "ground_truth_policy_value", ",", "float", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.test_real.test_sample_bootstrap_bandit_feedback": [[103, 151], ["obp.dataset.OpenBanditDataset", "obp.dataset.OpenBanditDataset.obtain_batch_bandit_feedback", "obp.dataset.OpenBanditDataset.sample_bootstrap_bandit_feedback", "obp.dataset.OpenBanditDataset.sample_bootstrap_bandit_feedback", "obp.dataset.OpenBanditDataset", "obp.dataset.OpenBanditDataset.sample_bootstrap_bandit_feedback", "pytest.raises", "obp.dataset.OpenBanditDataset", "obp.dataset.OpenBanditDataset.sample_bootstrap_bandit_feedback", "pytest.raises", "obp.dataset.OpenBanditDataset", "obp.dataset.OpenBanditDataset.sample_bootstrap_bandit_feedback", "pytest.raises", "obp.dataset.OpenBanditDataset", "obp.dataset.OpenBanditDataset.sample_bootstrap_bandit_feedback", "pytest.raises", "obp.dataset.OpenBanditDataset", "obp.dataset.OpenBanditDataset.sample_bootstrap_bandit_feedback", "pytest.raises", "obp.dataset.OpenBanditDataset", "obp.dataset.OpenBanditDataset.sample_bootstrap_bandit_feedback", "obp.dataset.OpenBanditDataset.obtain_batch_bandit_feedback", "len", "len", "len", "len", "len"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.multiclass.MultiClassToBanditReduction.obtain_batch_bandit_feedback", "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.real.OpenBanditDataset.sample_bootstrap_bandit_feedback", "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.real.OpenBanditDataset.sample_bootstrap_bandit_feedback", "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.real.OpenBanditDataset.sample_bootstrap_bandit_feedback", "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.real.OpenBanditDataset.sample_bootstrap_bandit_feedback", "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.real.OpenBanditDataset.sample_bootstrap_bandit_feedback", "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.real.OpenBanditDataset.sample_bootstrap_bandit_feedback", "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.real.OpenBanditDataset.sample_bootstrap_bandit_feedback", "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.real.OpenBanditDataset.sample_bootstrap_bandit_feedback", "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.multiclass.MultiClassToBanditReduction.obtain_batch_bandit_feedback"], ["", "def", "test_sample_bootstrap_bandit_feedback", "(", ")", ":", "\n", "    ", "with", "pytest", ".", "raises", "(", "ValueError", ")", ":", "\n", "        ", "dataset", "=", "OpenBanditDataset", "(", "behavior_policy", "=", "\"random\"", ",", "campaign", "=", "\"all\"", ")", "\n", "dataset", ".", "sample_bootstrap_bandit_feedback", "(", "\n", "is_timeseries_split", "=", "True", ",", "test_size", "=", "1.3", "\n", ")", "\n", "\n", "", "with", "pytest", ".", "raises", "(", "ValueError", ")", ":", "\n", "        ", "dataset", "=", "OpenBanditDataset", "(", "behavior_policy", "=", "\"random\"", ",", "campaign", "=", "\"all\"", ")", "\n", "dataset", ".", "sample_bootstrap_bandit_feedback", "(", "\n", "is_timeseries_split", "=", "True", ",", "test_size", "=", "-", "0.5", "\n", ")", "\n", "\n", "", "with", "pytest", ".", "raises", "(", "ValueError", ")", ":", "\n", "        ", "dataset", "=", "OpenBanditDataset", "(", "behavior_policy", "=", "\"random\"", ",", "campaign", "=", "\"all\"", ")", "\n", "dataset", ".", "sample_bootstrap_bandit_feedback", "(", "sample_size", "=", "-", "50", ")", "\n", "\n", "", "with", "pytest", ".", "raises", "(", "TypeError", ")", ":", "\n", "        ", "dataset", "=", "OpenBanditDataset", "(", "behavior_policy", "=", "\"random\"", ",", "campaign", "=", "\"all\"", ")", "\n", "dataset", ".", "sample_bootstrap_bandit_feedback", "(", "sample_size", "=", "50.0", ")", "\n", "\n", "", "with", "pytest", ".", "raises", "(", "ValueError", ")", ":", "\n", "        ", "dataset", "=", "OpenBanditDataset", "(", "behavior_policy", "=", "\"random\"", ",", "campaign", "=", "\"all\"", ")", "\n", "dataset", ".", "sample_bootstrap_bandit_feedback", "(", "sample_size", "=", "10000000", ")", "\n", "\n", "", "dataset", "=", "OpenBanditDataset", "(", "behavior_policy", "=", "\"random\"", ",", "campaign", "=", "\"all\"", ")", "\n", "bandit_feedback", "=", "dataset", ".", "obtain_batch_bandit_feedback", "(", ")", "\n", "bootstrap_bf", "=", "dataset", ".", "sample_bootstrap_bandit_feedback", "(", ")", "\n", "\n", "bf_keys", "=", "{", "\"action\"", ",", "\"position\"", ",", "\"reward\"", ",", "\"pscore\"", ",", "\"context\"", "}", "\n", "for", "k", "in", "bf_keys", ":", "\n", "        ", "assert", "len", "(", "bandit_feedback", "[", "k", "]", ")", "==", "len", "(", "bootstrap_bf", "[", "k", "]", ")", "\n", "\n", "", "bandit_feedback_timeseries", ":", "Dict", "=", "dataset", ".", "obtain_batch_bandit_feedback", "(", "\n", "is_timeseries_split", "=", "True", "\n", ")", "[", "0", "]", "\n", "bootstrap_bf_timeseries", "=", "dataset", ".", "sample_bootstrap_bandit_feedback", "(", "\n", "is_timeseries_split", "=", "True", "\n", ")", "\n", "for", "k", "in", "bf_keys", ":", "\n", "        ", "assert", "len", "(", "bandit_feedback_timeseries", "[", "k", "]", ")", "==", "len", "(", "bootstrap_bf_timeseries", "[", "k", "]", ")", "\n", "\n", "", "sample_size", "=", "1000", "\n", "dataset", "=", "OpenBanditDataset", "(", "behavior_policy", "=", "\"random\"", ",", "campaign", "=", "\"all\"", ")", "\n", "bootstrap_bf", "=", "dataset", ".", "sample_bootstrap_bandit_feedback", "(", "sample_size", "=", "sample_size", ")", "\n", "assert", "bootstrap_bf", "[", "\"n_rounds\"", "]", "==", "sample_size", "\n", "for", "k", "in", "bf_keys", ":", "\n", "        ", "assert", "len", "(", "bootstrap_bf", "[", "k", "]", ")", "==", "sample_size", "\n", "", "", ""]], "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.base.BaseBanditDataset.obtain_batch_bandit_feedback": [[12, 16], ["None"], "methods", ["None"], ["@", "abstractmethod", "\n", "def", "obtain_batch_bandit_feedback", "(", "self", ")", "->", "None", ":", "\n", "        ", "\"\"\"Obtain batch logged bandit data.\"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.base.BaseRealBanditDataset.load_raw_data": [[21, 25], ["None"], "methods", ["None"], ["@", "abstractmethod", "\n", "def", "load_raw_data", "(", "self", ")", "->", "None", ":", "\n", "        ", "\"\"\"Load raw dataset.\"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.base.BaseRealBanditDataset.pre_process": [[26, 30], ["None"], "methods", ["None"], ["", "@", "abstractmethod", "\n", "def", "pre_process", "(", "self", ")", "->", "None", ":", "\n", "        ", "\"\"\"Preprocess raw dataset.\"\"\"", "\n", "raise", "NotImplementedError", "\n", "", "", ""]], "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.synthetic_continuous.SyntheticContinuousBanditDataset.__post_init__": [[125, 147], ["sklearn.utils.check_scalar", "sklearn.utils.check_scalar", "sklearn.utils.check_scalar", "sklearn.utils.check_scalar", "sklearn.utils.check_scalar", "sklearn.utils.check_random_state", "ValueError", "ValueError"], "methods", ["None"], ["def", "__post_init__", "(", "self", ")", "->", "None", ":", "\n", "        ", "\"\"\"Initialize Class.\"\"\"", "\n", "check_scalar", "(", "self", ".", "dim_context", ",", "name", "=", "\"dim_context\"", ",", "target_type", "=", "int", ",", "min_val", "=", "1", ")", "\n", "check_scalar", "(", "\n", "self", ".", "action_noise", ",", "name", "=", "\"action_noise\"", ",", "target_type", "=", "(", "int", ",", "float", ")", ",", "min_val", "=", "0", "\n", ")", "\n", "check_scalar", "(", "\n", "self", ".", "reward_noise", ",", "name", "=", "\"reward_noise\"", ",", "target_type", "=", "(", "int", ",", "float", ")", ",", "min_val", "=", "0", "\n", ")", "\n", "check_scalar", "(", "\n", "self", ".", "min_action_value", ",", "name", "=", "\"min_action_value\"", ",", "target_type", "=", "(", "int", ",", "float", ")", "\n", ")", "\n", "check_scalar", "(", "\n", "self", ".", "max_action_value", ",", "name", "=", "\"max_action_value\"", ",", "target_type", "=", "(", "int", ",", "float", ")", "\n", ")", "\n", "if", "self", ".", "max_action_value", "<=", "self", ".", "min_action_value", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"`max_action_value` must be larger than `min_action_value`\"", "\n", ")", "\n", "", "if", "self", ".", "random_state", "is", "None", ":", "\n", "            ", "raise", "ValueError", "(", "\"random_state must be given\"", ")", "\n", "", "self", ".", "random_", "=", "check_random_state", "(", "self", ".", "random_state", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.synthetic_continuous.SyntheticContinuousBanditDataset._contextfree_reward_function": [[148, 154], ["numpy.power", "numpy.abs"], "methods", ["None"], ["", "def", "_contextfree_reward_function", "(", "self", ",", "action", ":", "np", ".", "ndarray", ")", "->", "np", ".", "ndarray", ":", "\n", "        ", "\"\"\"\n        Calculate context-free expected rewards given only continuous action values.\n        This is just an example synthetic (expected) reward function.\n        \"\"\"", "\n", "return", "2", "*", "np", ".", "power", "(", "np", ".", "abs", "(", "action", ")", ",", "1.5", ")", "-", "(", "5", "*", "action", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.synthetic_continuous.SyntheticContinuousBanditDataset.obtain_batch_bandit_feedback": [[155, 224], ["sklearn.utils.check_scalar", "synthetic_continuous.SyntheticContinuousBanditDataset.random_.normal", "dict", "synthetic_continuous.SyntheticContinuousBanditDataset.behavior_policy_function", "scipy.stats.truncnorm.rvs", "scipy.stats.truncnorm.pdf", "scipy.stats.uniform.rvs", "scipy.stats.uniform.pdf", "synthetic_continuous.SyntheticContinuousBanditDataset._contextfree_reward_function", "synthetic_continuous.SyntheticContinuousBanditDataset.reward_function", "synthetic_continuous.SyntheticContinuousBanditDataset.random_.normal"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.synthetic_continuous.SyntheticContinuousBanditDataset._contextfree_reward_function"], ["", "def", "obtain_batch_bandit_feedback", "(", "\n", "self", ",", "\n", "n_rounds", ":", "int", ",", "\n", ")", "->", "BanditFeedback", ":", "\n", "        ", "\"\"\"Obtain batch logged bandit data.\n\n        Parameters\n        ----------\n        n_rounds: int\n            Data size of synthetic logged data.\n\n        Returns\n        ---------\n        bandit_feedback: BanditFeedback\n            Synthesized logged bandit dataset with continuous actions.\n\n        \"\"\"", "\n", "check_scalar", "(", "n_rounds", ",", "name", "=", "\"n_rounds\"", ",", "target_type", "=", "int", ",", "min_val", "=", "1", ")", "\n", "\n", "context", "=", "self", ".", "random_", ".", "normal", "(", "size", "=", "(", "n_rounds", ",", "self", ".", "dim_context", ")", ")", "\n", "# sample actions for each round based on the behavior policy", "\n", "if", "self", ".", "behavior_policy_function", "is", "not", "None", ":", "\n", "            ", "expected_action_values", "=", "self", ".", "behavior_policy_function", "(", "\n", "context", "=", "context", ",", "\n", "random_state", "=", "self", ".", "random_state", ",", "\n", ")", "\n", "a", "=", "(", "self", ".", "min_action_value", "-", "expected_action_values", ")", "/", "self", ".", "action_noise", "\n", "b", "=", "(", "self", ".", "max_action_value", "-", "expected_action_values", ")", "/", "self", ".", "action_noise", "\n", "action", "=", "truncnorm", ".", "rvs", "(", "\n", "a", ",", "\n", "b", ",", "\n", "loc", "=", "expected_action_values", ",", "\n", "scale", "=", "self", ".", "action_noise", ",", "\n", "random_state", "=", "self", ".", "random_state", ",", "\n", ")", "\n", "pscore", "=", "truncnorm", ".", "pdf", "(", "\n", "action", ",", "a", ",", "b", ",", "loc", "=", "expected_action_values", ",", "scale", "=", "self", ".", "action_noise", "\n", ")", "\n", "", "else", ":", "\n", "            ", "action", "=", "uniform", ".", "rvs", "(", "\n", "loc", "=", "self", ".", "min_action_value", ",", "\n", "scale", "=", "(", "self", ".", "max_action_value", "-", "self", ".", "min_action_value", ")", ",", "\n", "size", "=", "n_rounds", ",", "\n", "random_state", "=", "self", ".", "random_state", ",", "\n", ")", "\n", "pscore", "=", "uniform", ".", "pdf", "(", "\n", "action", ",", "\n", "loc", "=", "self", ".", "min_action_value", ",", "\n", "scale", "=", "(", "self", ".", "max_action_value", "-", "self", ".", "min_action_value", ")", ",", "\n", ")", "\n", "\n", "", "if", "self", ".", "reward_function", "is", "None", ":", "\n", "            ", "expected_reward_", "=", "self", ".", "_contextfree_reward_function", "(", "action", "=", "action", ")", "\n", "", "else", ":", "\n", "            ", "expected_reward_", "=", "self", ".", "reward_function", "(", "\n", "context", "=", "context", ",", "action", "=", "action", ",", "random_state", "=", "self", ".", "random_state", "\n", ")", "\n", "", "reward", "=", "expected_reward_", "+", "self", ".", "random_", ".", "normal", "(", "\n", "scale", "=", "self", ".", "reward_noise", ",", "size", "=", "n_rounds", "\n", ")", "\n", "\n", "return", "dict", "(", "\n", "n_rounds", "=", "n_rounds", ",", "\n", "context", "=", "context", ",", "\n", "action", "=", "action", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore", "=", "pscore", ",", "\n", "position", "=", "None", ",", "# position is irrelevant for continuous action data", "\n", "expected_reward", "=", "expected_reward_", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.synthetic_continuous.SyntheticContinuousBanditDataset.calc_ground_truth_policy_value": [[226, 264], ["utils.check_array", "utils.check_array", "ValueError", "ValueError", "synthetic_continuous.SyntheticContinuousBanditDataset._contextfree_reward_function().mean", "synthetic_continuous.SyntheticContinuousBanditDataset.reward_function().mean", "synthetic_continuous.SyntheticContinuousBanditDataset._contextfree_reward_function", "synthetic_continuous.SyntheticContinuousBanditDataset.reward_function"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.synthetic_continuous.SyntheticContinuousBanditDataset._contextfree_reward_function"], ["", "def", "calc_ground_truth_policy_value", "(", "\n", "self", ",", "\n", "context", ":", "np", ".", "ndarray", ",", "\n", "action", ":", "np", ".", "ndarray", ",", "\n", ")", "->", "float", ":", "\n", "        ", "\"\"\"Calculate the policy value of a particular action sequence.\n\n        Parameters\n        -----------\n        context: array-like, shape (n_rounds_of_test_data, dim_context)\n            Context vectors of test data.\n\n        action: array-like, shape (n_rounds_of_test_data,)\n            Continuous action values for the test data predicted by the (deterministic) evaluation policy,\n            i.e., :math:`\\\\pi_e(x_t)`.\n\n        Returns\n        ----------\n        policy_value: float\n            The policy value of the evaluation policy calculated on the given test logged bandit data.\n\n        \"\"\"", "\n", "check_array", "(", "array", "=", "context", ",", "name", "=", "\"context\"", ",", "expected_dim", "=", "2", ")", "\n", "check_array", "(", "array", "=", "action", ",", "name", "=", "\"action\"", ",", "expected_dim", "=", "1", ")", "\n", "if", "context", ".", "shape", "[", "1", "]", "!=", "self", ".", "dim_context", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"Expected `context.shape[1] == self.dim_context`, but found it False\"", "\n", ")", "\n", "", "if", "context", ".", "shape", "[", "0", "]", "!=", "action", ".", "shape", "[", "0", "]", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"Expected `context.shape[0] == action.shape[0]`, but found it False\"", "\n", ")", "\n", "\n", "", "if", "self", ".", "reward_function", "is", "None", ":", "\n", "            ", "return", "self", ".", "_contextfree_reward_function", "(", "action", "=", "action", ")", ".", "mean", "(", ")", "\n", "", "else", ":", "\n", "            ", "return", "self", ".", "reward_function", "(", "\n", "context", "=", "context", ",", "action", "=", "action", ",", "random_state", "=", "self", ".", "random_state", "\n", ")", ".", "mean", "(", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.synthetic_continuous.linear_reward_funcion_continuous": [[270, 305], ["utils.check_array", "utils.check_array", "sklearn.utils.check_random_state", "sklearn.utils.check_random_state.normal", "sklearn.utils.check_random_state.uniform", "ValueError", "numpy.abs"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array"], ["", "", "", "def", "linear_reward_funcion_continuous", "(", "\n", "context", ":", "np", ".", "ndarray", ",", "\n", "action", ":", "np", ".", "ndarray", ",", "\n", "random_state", ":", "Optional", "[", "int", "]", "=", "None", ",", "\n", ")", "->", "np", ".", "ndarray", ":", "\n", "    ", "\"\"\"Linear reward function to generate synthetic continuous bandit datasets.\n\n    Parameters\n    -----------\n    context: array-like, shape (n_rounds, dim_context)\n        Context vectors characterizing each data (such as user information).\n\n    action: array-like, shape (n_rounds,)\n        Continuous action values.\n\n    random_state: int, default=None\n        Controls the random seed in sampling parameters.\n\n    Returns\n    ---------\n    expected_reward: array-like, shape (n_rounds,)\n        Expected reward given context (:math:`x`) and continuous action (:math:`a`).\n\n    \"\"\"", "\n", "check_array", "(", "array", "=", "context", ",", "name", "=", "\"context\"", ",", "expected_dim", "=", "2", ")", "\n", "check_array", "(", "array", "=", "action", ",", "name", "=", "\"action\"", ",", "expected_dim", "=", "1", ")", "\n", "if", "context", ".", "shape", "[", "0", "]", "!=", "action", ".", "shape", "[", "0", "]", ":", "\n", "        ", "raise", "ValueError", "(", "\n", "\"Expected `context.shape[0] == action.shape[0]`, but found it False\"", "\n", ")", "\n", "\n", "", "random_", "=", "check_random_state", "(", "random_state", ")", "\n", "coef_", "=", "random_", ".", "normal", "(", "size", "=", "context", ".", "shape", "[", "1", "]", ")", "\n", "pow_", ",", "bias", "=", "random_", ".", "uniform", "(", "size", "=", "2", ")", "\n", "return", "(", "np", ".", "abs", "(", "context", "@", "coef_", "-", "action", ")", "**", "pow_", ")", "+", "bias", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.synthetic_continuous.quadratic_reward_funcion_continuous": [[307, 348], ["utils.check_array", "utils.check_array", "sklearn.utils.check_random_state", "sklearn.utils.check_random_state.normal", "sklearn.utils.check_random_state.normal", "sklearn.utils.check_random_state.normal", "sklearn.utils.check_random_state.normal", "ValueError"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array"], ["", "def", "quadratic_reward_funcion_continuous", "(", "\n", "context", ":", "np", ".", "ndarray", ",", "\n", "action", ":", "np", ".", "ndarray", ",", "\n", "random_state", ":", "Optional", "[", "int", "]", "=", "None", ",", "\n", ")", "->", "np", ".", "ndarray", ":", "\n", "    ", "\"\"\"Quadratic reward function to generate synthetic continuous bandit datasets.\n\n    Parameters\n    -----------\n    context: array-like, shape (n_rounds, dim_context)\n        Context vectors characterizing each data (such as user information).\n\n    action: array-like, shape (n_rounds,)\n        Continuous action values.\n\n    random_state: int, default=None\n        Controls the random seed in sampling parameters.\n\n    Returns\n    ---------\n    expected_reward: array-like, shape (n_rounds,)\n        Expected reward given context (:math:`x`) and continuous action (:math:`a`).\n\n    \"\"\"", "\n", "check_array", "(", "array", "=", "context", ",", "name", "=", "\"context\"", ",", "expected_dim", "=", "2", ")", "\n", "check_array", "(", "array", "=", "action", ",", "name", "=", "\"action\"", ",", "expected_dim", "=", "1", ")", "\n", "if", "context", ".", "shape", "[", "0", "]", "!=", "action", ".", "shape", "[", "0", "]", ":", "\n", "        ", "raise", "ValueError", "(", "\n", "\"Expected `context.shape[0] == action.shape[0]`, but found it False\"", "\n", ")", "\n", "\n", "", "random_", "=", "check_random_state", "(", "random_state", ")", "\n", "coef_x", "=", "random_", ".", "normal", "(", "size", "=", "context", ".", "shape", "[", "1", "]", ")", "\n", "coef_x_a", "=", "random_", ".", "normal", "(", "size", "=", "context", ".", "shape", "[", "1", "]", ")", "\n", "coef_x_a_squared", "=", "random_", ".", "normal", "(", "size", "=", "context", ".", "shape", "[", "1", "]", ")", "\n", "coef_a", "=", "random_", ".", "normal", "(", "size", "=", "1", ")", "\n", "\n", "expected_reward", "=", "(", "coef_a", "*", "action", ")", "*", "(", "context", "@", "coef_x", ")", "\n", "expected_reward", "+=", "(", "context", "@", "coef_x_a", ")", "*", "action", "\n", "expected_reward", "+=", "(", "action", "-", "context", "@", "coef_x_a_squared", ")", "**", "2", "\n", "return", "expected_reward", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.synthetic_continuous.linear_behavior_policy_continuous": [[350, 376], ["utils.check_array", "sklearn.utils.check_random_state", "sklearn.utils.check_random_state.normal", "sklearn.utils.check_random_state.uniform"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array"], ["", "def", "linear_behavior_policy_continuous", "(", "\n", "context", ":", "np", ".", "ndarray", ",", "\n", "random_state", ":", "Optional", "[", "int", "]", "=", "None", ",", "\n", ")", "->", "np", ".", "ndarray", ":", "\n", "    ", "\"\"\"Linear behavior policy function to generate synthetic continuous bandit datasets.\n\n    Parameters\n    -----------\n    context: array-like, shape (n_rounds, dim_context)\n        Context vectors characterizing each data (such as user information).\n\n    random_state: int, default=None\n        Controls the random seed in sampling parameters.\n\n    Returns\n    ---------\n    expected_action_value: array-like, shape (n_rounds,)\n        Expected continuous action values given context (:math:`x`).\n\n    \"\"\"", "\n", "check_array", "(", "array", "=", "context", ",", "name", "=", "\"context\"", ",", "expected_dim", "=", "2", ")", "\n", "\n", "random_", "=", "check_random_state", "(", "random_state", ")", "\n", "coef_", "=", "random_", ".", "normal", "(", "size", "=", "context", ".", "shape", "[", "1", "]", ")", "\n", "bias", "=", "random_", ".", "uniform", "(", "size", "=", "1", ")", "\n", "return", "context", "@", "coef_", "+", "bias", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.synthetic_continuous.linear_synthetic_policy_continuous": [[381, 398], ["utils.check_array", "context.mean"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array"], ["", "def", "linear_synthetic_policy_continuous", "(", "context", ":", "np", ".", "ndarray", ")", "->", "np", ".", "ndarray", ":", "\n", "    ", "\"\"\"Linear synthtic policy for continuous actions.\n\n    Parameters\n    -----------\n    context: array-like, shape (n_rounds, dim_context)\n        Context vectors characterizing each data (such as user information).\n\n    Returns\n    ---------\n    action_by_evaluation_policy: array-like, shape (n_rounds,)\n        Continuous action values given by a synthetic (deterministic) evaluation policy, i.e., :math:`\\\\pi_e(x_t)`.\n\n    \"\"\"", "\n", "check_array", "(", "array", "=", "context", ",", "name", "=", "\"context\"", ",", "expected_dim", "=", "2", ")", "\n", "\n", "return", "context", ".", "mean", "(", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.synthetic_continuous.threshold_synthetic_policy_continuous": [[400, 417], ["utils.check_array", "numpy.sign", "context.mean"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array"], ["", "def", "threshold_synthetic_policy_continuous", "(", "context", ":", "np", ".", "ndarray", ")", "->", "np", ".", "ndarray", ":", "\n", "    ", "\"\"\"Threshold synthtic policy for continuous actions.\n\n    Parameters\n    -----------\n    context: array-like, shape (n_rounds, dim_context)\n        Context vectors characterizing each data (such as user information).\n\n    Returns\n    ---------\n    action_by_evaluation_policy: array-like, shape (n_rounds,)\n        Continuous action values given by a synthetic (deterministic) evaluation policy, i.e., :math:`\\\\pi_e(x_t)`.\n\n    \"\"\"", "\n", "check_array", "(", "array", "=", "context", ",", "name", "=", "\"context\"", ",", "expected_dim", "=", "2", ")", "\n", "\n", "return", "1.0", "+", "np", ".", "sign", "(", "context", ".", "mean", "(", "1", ")", "-", "1.5", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.synthetic_continuous.sign_synthetic_policy_continuous": [[419, 436], ["utils.check_array", "numpy.sin", "context.mean"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array"], ["", "def", "sign_synthetic_policy_continuous", "(", "context", ":", "np", ".", "ndarray", ")", "->", "np", ".", "ndarray", ":", "\n", "    ", "\"\"\"Sign synthtic policy for continuous actions.\n\n    Parameters\n    -----------\n    context: array-like, shape (n_rounds, dim_context)\n        Context vectors characterizing each data (such as user information).\n\n    Returns\n    ---------\n    action_by_evaluation_policy: array-like, shape (n_rounds,)\n        Continuous action values given by a synthetic (deterministic) evaluation policy, i.e., :math:`\\\\pi_e(x_t)`.\n\n    \"\"\"", "\n", "check_array", "(", "array", "=", "context", ",", "name", "=", "\"context\"", ",", "expected_dim", "=", "2", ")", "\n", "\n", "return", "np", ".", "sin", "(", "context", ".", "mean", "(", "1", ")", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.synthetic_embed.SyntheticBanditDatasetWithActionEmbeds.__post_init__": [[200, 231], ["super().__post_init__", "sklearn.utils.check_scalar", "sklearn.utils.check_scalar", "sklearn.utils.check_scalar", "sklearn.utils.check_scalar", "sklearn.utils.check_scalar", "sklearn.utils.check_scalar", "synthetic_embed.SyntheticBanditDatasetWithActionEmbeds._define_action_embed", "reward_type.RewardType", "reward_type.RewardType"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.__post_init__", "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.synthetic_embed.SyntheticBanditDatasetWithActionEmbeds._define_action_embed"], ["def", "__post_init__", "(", "self", ")", "->", "None", ":", "\n", "        ", "\"\"\"Initialize Class.\"\"\"", "\n", "super", "(", ")", ".", "__post_init__", "(", ")", "\n", "check_scalar", "(", "self", ".", "n_cat_per_dim", ",", "\"n_cat_per_dim\"", ",", "int", ",", "min_val", "=", "1", ")", "\n", "check_scalar", "(", "self", ".", "latent_param_mat_dim", ",", "\"latent_param_mat_dim\"", ",", "int", ",", "min_val", "=", "1", ")", "\n", "check_scalar", "(", "self", ".", "n_cat_dim", ",", "\"n_cat_dim\"", ",", "int", ",", "min_val", "=", "1", ")", "\n", "check_scalar", "(", "self", ".", "p_e_a_param_std", ",", "\"p_e_a_param_std\"", ",", "(", "int", ",", "float", ")", ",", "min_val", "=", "0.0", ")", "\n", "check_scalar", "(", "\n", "self", ".", "n_unobserved_cat_dim", ",", "\n", "\"n_unobserved_cat_dim\"", ",", "\n", "int", ",", "\n", "min_val", "=", "0", ",", "\n", "max_val", "=", "self", ".", "n_cat_dim", ",", "\n", ")", "\n", "check_scalar", "(", "\n", "self", ".", "n_irrelevant_cat_dim", ",", "\n", "\"n_irrelevant_cat_dim\"", ",", "\n", "int", ",", "\n", "min_val", "=", "0", ",", "\n", "max_val", "=", "self", ".", "n_cat_dim", ",", "\n", ")", "\n", "self", ".", "n_cat_dim", "+=", "1", "\n", "self", ".", "n_unobserved_cat_dim", "+=", "1", "\n", "self", ".", "n_irrelevant_cat_dim", "+=", "1", "\n", "self", ".", "_define_action_embed", "(", ")", "\n", "\n", "if", "self", ".", "reward_function", "is", "None", ":", "\n", "            ", "if", "RewardType", "(", "self", ".", "reward_type", ")", "==", "RewardType", ".", "BINARY", ":", "\n", "                ", "self", ".", "reward_function", "=", "logistic_reward_function", "\n", "", "elif", "RewardType", "(", "self", ".", "reward_type", ")", "==", "RewardType", ".", "CONTINUOUS", ":", "\n", "                ", "self", ".", "reward_function", "=", "linear_reward_function", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.synthetic_embed.SyntheticBanditDatasetWithActionEmbeds._define_action_embed": [[232, 248], ["synthetic_embed.SyntheticBanditDatasetWithActionEmbeds.random_.normal", "utils.softmax", "numpy.zeros", "numpy.arange", "synthetic_embed.SyntheticBanditDatasetWithActionEmbeds.random_.normal", "utils.sample_action_fast", "numpy.arange"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.softmax", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.sample_action_fast"], ["", "", "", "def", "_define_action_embed", "(", "self", ")", "->", "None", ":", "\n", "        ", "\"\"\"Define action embeddings and latent category parameter matrices.\"\"\"", "\n", "self", ".", "latent_cat_param", "=", "self", ".", "random_", ".", "normal", "(", "\n", "size", "=", "(", "self", ".", "n_cat_dim", ",", "self", ".", "n_cat_per_dim", ",", "self", ".", "latent_param_mat_dim", ")", "\n", ")", "\n", "self", ".", "p_e_a", "=", "softmax", "(", "\n", "self", ".", "random_", ".", "normal", "(", "\n", "scale", "=", "self", ".", "p_e_a_param_std", ",", "\n", "size", "=", "(", "self", ".", "n_actions", ",", "self", ".", "n_cat_per_dim", ",", "self", ".", "n_cat_dim", ")", ",", "\n", ")", ",", "\n", ")", "\n", "self", ".", "action_context_reg", "=", "np", ".", "zeros", "(", "(", "self", ".", "n_actions", ",", "self", ".", "n_cat_dim", ")", ",", "dtype", "=", "int", ")", "\n", "for", "d", "in", "np", ".", "arange", "(", "self", ".", "n_cat_dim", ")", ":", "\n", "            ", "self", ".", "action_context_reg", "[", ":", ",", "d", "]", "=", "sample_action_fast", "(", "\n", "self", ".", "p_e_a", "[", "np", ".", "arange", "(", "self", ".", "n_actions", ",", "dtype", "=", "int", ")", ",", ":", ",", "d", "]", ",", "\n", "random_state", "=", "self", ".", "random_state", "+", "d", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.synthetic_embed.SyntheticBanditDatasetWithActionEmbeds.obtain_batch_bandit_feedback": [[250, 353], ["sklearn.utils.check_scalar", "synthetic_embed.SyntheticBanditDatasetWithActionEmbeds.random_.normal", "numpy.zeros", "synthetic_embed.SyntheticBanditDatasetWithActionEmbeds.random_.dirichlet", "cat_dim_importance.reshape.reshape.reshape", "numpy.zeros", "numpy.zeros", "numpy.arange", "utils.sample_action_fast", "numpy.zeros", "numpy.arange", "numpy.zeros", "numpy.arange", "dict", "synthetic_embed.SyntheticBanditDatasetWithActionEmbeds.reward_function", "synthetic_embed.SyntheticBanditDatasetWithActionEmbeds.behavior_policy_function", "numpy.zeros_like", "utils.softmax", "utils.softmax", "utils.sample_action_fast", "reward_type.RewardType", "synthetic_embed.SyntheticBanditDatasetWithActionEmbeds.random_.binomial", "synthetic_embed.SyntheticBanditDatasetWithActionEmbeds.random_.uniform", "reward_type.RewardType", "synthetic_embed.SyntheticBanditDatasetWithActionEmbeds.random_.normal", "numpy.argsort", "numpy.tile", "synthetic_embed.SyntheticBanditDatasetWithActionEmbeds.random_.gumbel", "numpy.arange", "numpy.arange", "numpy.arange"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.sample_action_fast", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.softmax", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.softmax", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.sample_action_fast"], ["", "", "def", "obtain_batch_bandit_feedback", "(", "self", ",", "n_rounds", ":", "int", ")", "->", "BanditFeedback", ":", "\n", "        ", "\"\"\"Obtain batch logged bandit data.\n\n        Parameters\n        ----------\n        n_rounds: int\n            Data size of the synthetic logged bandit data.\n\n        Returns\n        ---------\n        bandit_feedback: BanditFeedback\n            Synthesized logged bandit data with action category information.\n\n        \"\"\"", "\n", "check_scalar", "(", "n_rounds", ",", "\"n_rounds\"", ",", "int", ",", "min_val", "=", "1", ")", "\n", "contexts", "=", "self", ".", "random_", ".", "normal", "(", "size", "=", "(", "n_rounds", ",", "self", ".", "dim_context", ")", ")", "\n", "cat_dim_importance", "=", "np", ".", "zeros", "(", "self", ".", "n_cat_dim", ")", "\n", "cat_dim_importance", "[", "self", ".", "n_irrelevant_cat_dim", ":", "]", "=", "self", ".", "random_", ".", "dirichlet", "(", "\n", "alpha", "=", "self", ".", "random_", ".", "uniform", "(", "size", "=", "self", ".", "n_cat_dim", "-", "self", ".", "n_irrelevant_cat_dim", ")", ",", "\n", "size", "=", "1", ",", "\n", ")", "\n", "cat_dim_importance", "=", "cat_dim_importance", ".", "reshape", "(", "(", "1", ",", "1", ",", "self", ".", "n_cat_dim", ")", ")", "\n", "\n", "# calc expected rewards given context and action (n_data, n_actions)", "\n", "q_x_e", "=", "np", ".", "zeros", "(", "(", "n_rounds", ",", "self", ".", "n_cat_per_dim", ",", "self", ".", "n_cat_dim", ")", ")", "\n", "q_x_a", "=", "np", ".", "zeros", "(", "(", "n_rounds", ",", "self", ".", "n_actions", ",", "self", ".", "n_cat_dim", ")", ")", "\n", "for", "d", "in", "np", ".", "arange", "(", "self", ".", "n_cat_dim", ")", ":", "\n", "            ", "q_x_e", "[", ":", ",", ":", ",", "d", "]", "=", "self", ".", "reward_function", "(", "\n", "context", "=", "contexts", ",", "\n", "action_context", "=", "self", ".", "latent_cat_param", "[", "d", "]", ",", "\n", "random_state", "=", "self", ".", "random_state", "+", "d", ",", "\n", ")", "\n", "q_x_a", "[", ":", ",", ":", ",", "d", "]", "=", "q_x_e", "[", ":", ",", ":", ",", "d", "]", "@", "self", ".", "p_e_a", "[", ":", ",", ":", ",", "d", "]", ".", "T", "\n", "", "q_x_a", "=", "(", "q_x_a", "*", "cat_dim_importance", ")", ".", "sum", "(", "2", ")", "\n", "\n", "# sample actions for each round based on the behavior policy", "\n", "if", "self", ".", "behavior_policy_function", "is", "None", ":", "\n", "            ", "pi_b_logits", "=", "q_x_a", "\n", "", "else", ":", "\n", "            ", "pi_b_logits", "=", "self", ".", "behavior_policy_function", "(", "\n", "context", "=", "contexts", ",", "\n", "action_context", "=", "self", ".", "action_context", ",", "\n", "random_state", "=", "self", ".", "random_state", ",", "\n", ")", "\n", "", "if", "self", ".", "n_deficient_actions", ">", "0", ":", "\n", "            ", "pi_b", "=", "np", ".", "zeros_like", "(", "q_x_a", ")", "\n", "n_supported_actions", "=", "self", ".", "n_actions", "-", "self", ".", "n_deficient_actions", "\n", "supported_actions", "=", "np", ".", "argsort", "(", "\n", "self", ".", "random_", ".", "gumbel", "(", "size", "=", "(", "n_rounds", ",", "self", ".", "n_actions", ")", ")", ",", "axis", "=", "1", "\n", ")", "[", ":", ",", ":", ":", "-", "1", "]", "[", ":", ",", ":", "n_supported_actions", "]", "\n", "supported_actions_idx", "=", "(", "\n", "np", ".", "tile", "(", "np", ".", "arange", "(", "n_rounds", ")", ",", "(", "n_supported_actions", ",", "1", ")", ")", ".", "T", ",", "\n", "supported_actions", ",", "\n", ")", "\n", "pi_b", "[", "supported_actions_idx", "]", "=", "softmax", "(", "\n", "self", ".", "beta", "*", "pi_b_logits", "[", "supported_actions_idx", "]", "\n", ")", "\n", "", "else", ":", "\n", "            ", "pi_b", "=", "softmax", "(", "self", ".", "beta", "*", "pi_b_logits", ")", "\n", "", "actions", "=", "sample_action_fast", "(", "pi_b", ",", "random_state", "=", "self", ".", "random_state", ")", "\n", "\n", "# sample action embeddings based on sampled actions", "\n", "action_embed", "=", "np", ".", "zeros", "(", "(", "n_rounds", ",", "self", ".", "n_cat_dim", ")", ",", "dtype", "=", "int", ")", "\n", "for", "d", "in", "np", ".", "arange", "(", "self", ".", "n_cat_dim", ")", ":", "\n", "            ", "action_embed", "[", ":", ",", "d", "]", "=", "sample_action_fast", "(", "\n", "self", ".", "p_e_a", "[", "actions", ",", ":", ",", "d", "]", ",", "\n", "random_state", "=", "d", ",", "\n", ")", "\n", "\n", "# sample rewards given the context and action embeddings", "\n", "", "expected_rewards_factual", "=", "np", ".", "zeros", "(", "n_rounds", ")", "\n", "for", "d", "in", "np", ".", "arange", "(", "self", ".", "n_cat_dim", ")", ":", "\n", "            ", "expected_rewards_factual", "+=", "(", "\n", "cat_dim_importance", "[", "0", ",", "0", ",", "d", "]", "\n", "*", "q_x_e", "[", "np", ".", "arange", "(", "n_rounds", ")", ",", "action_embed", "[", ":", ",", "d", "]", ",", "d", "]", "\n", ")", "\n", "", "if", "RewardType", "(", "self", ".", "reward_type", ")", "==", "RewardType", ".", "BINARY", ":", "\n", "            ", "rewards", "=", "self", ".", "random_", ".", "binomial", "(", "n", "=", "1", ",", "p", "=", "expected_rewards_factual", ")", "\n", "", "elif", "RewardType", "(", "self", ".", "reward_type", ")", "==", "RewardType", ".", "CONTINUOUS", ":", "\n", "            ", "rewards", "=", "self", ".", "random_", ".", "normal", "(", "\n", "loc", "=", "expected_rewards_factual", ",", "scale", "=", "self", ".", "reward_std", ",", "size", "=", "n_rounds", "\n", ")", "\n", "\n", "", "return", "dict", "(", "\n", "n_rounds", "=", "n_rounds", ",", "\n", "n_actions", "=", "self", ".", "n_actions", ",", "\n", "action_context", "=", "self", ".", "action_context_reg", "[", "\n", ":", ",", "self", ".", "n_unobserved_cat_dim", ":", "\n", "]", ",", "# action context used for training a reg model", "\n", "action_embed", "=", "action_embed", "[", "\n", ":", ",", "self", ".", "n_unobserved_cat_dim", ":", "\n", "]", ",", "# action embeddings used for OPE with MIPW", "\n", "context", "=", "contexts", ",", "\n", "action", "=", "actions", ",", "\n", "position", "=", "None", ",", "# position effect is not considered in synthetic data", "\n", "reward", "=", "rewards", ",", "\n", "expected_reward", "=", "q_x_a", ",", "\n", "q_x_e", "=", "q_x_e", "[", ":", ",", ":", ",", "self", ".", "n_unobserved_cat_dim", ":", "]", ",", "\n", "p_e_a", "=", "self", ".", "p_e_a", "[", "\n", ":", ",", ":", ",", "self", ".", "n_unobserved_cat_dim", ":", "\n", "]", ",", "# true probability distribution of the action embeddings", "\n", "pi_b", "=", "pi_b", "[", ":", ",", ":", ",", "np", ".", "newaxis", "]", ",", "\n", "pscore", "=", "pi_b", "[", "np", ".", "arange", "(", "n_rounds", ")", ",", "actions", "]", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.synthetic_multi.SyntheticMultiLoggersBanditDataset.__post_init__": [[167, 234], ["sklearn.utils.check_scalar", "sklearn.utils.check_scalar", "sklearn.utils.check_scalar", "sklearn.utils.check_random_state", "sklearn.utils.check_scalar", "sklearn.utils.check_random_state", "ValueError", "print", "isinstance", "TypeError", "enumerate", "isinstance", "TypeError", "enumerate", "len", "len", "ValueError", "reward_type.RewardType", "ValueError", "synthetic_multi.SyntheticMultiLoggersBanditDataset.sample_contextfree_expected_reward", "reward_type.RewardType", "numpy.eye", "utils.check_array", "ValueError", "sklearn.utils.check_scalar", "sklearn.utils.check_scalar", "numpy.array", "numpy.sum", "ValueError", "type", "type"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.synthetic.SyntheticBanditDataset.sample_contextfree_expected_reward", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array"], ["def", "__post_init__", "(", "self", ")", "->", "None", ":", "\n", "        ", "\"\"\"Initialize Class.\"\"\"", "\n", "check_scalar", "(", "self", ".", "n_actions", ",", "\"n_actions\"", ",", "int", ",", "min_val", "=", "2", ")", "\n", "check_scalar", "(", "self", ".", "dim_context", ",", "\"dim_context\"", ",", "int", ",", "min_val", "=", "1", ")", "\n", "check_scalar", "(", "\n", "self", ".", "n_deficient_actions", ",", "\n", "\"n_deficient_actions\"", ",", "\n", "int", ",", "\n", "min_val", "=", "0", ",", "\n", "max_val", "=", "self", ".", "n_actions", "-", "1", ",", "\n", ")", "\n", "\n", "if", "self", ".", "random_state", "is", "None", ":", "\n", "            ", "raise", "ValueError", "(", "\"`random_state` must be given\"", ")", "\n", "", "self", ".", "random_", "=", "check_random_state", "(", "self", ".", "random_state", ")", "\n", "\n", "if", "self", ".", "behavior_policy_function", "is", "not", "None", ":", "\n", "            ", "print", "(", "\n", "\"something is given as `behavior_policy_function`, but it will not be used in this class.\"", "\n", ")", "\n", "\n", "", "if", "not", "isinstance", "(", "self", ".", "betas", ",", "list", ")", ":", "\n", "            ", "raise", "TypeError", "(", "f\"`betas` must be a list, but {type(self.betas)} is given.\"", ")", "\n", "", "else", ":", "\n", "            ", "for", "k", ",", "beta", "in", "enumerate", "(", "self", ".", "betas", ")", ":", "\n", "                ", "check_scalar", "(", "beta", ",", "f\"betas[{k}]\"", ",", "(", "int", ",", "float", ")", ")", "\n", "", "", "if", "not", "isinstance", "(", "self", ".", "rhos", ",", "list", ")", ":", "\n", "            ", "raise", "TypeError", "(", "f\"`rhos` must be a list, but {type(self.betas)} is given.\"", ")", "\n", "", "else", ":", "\n", "            ", "for", "k", ",", "rho", "in", "enumerate", "(", "self", ".", "rhos", ")", ":", "\n", "                ", "check_scalar", "(", "rho", ",", "f\"rhos[{k}]\"", ",", "(", "int", ",", "float", ")", ",", "min_val", "=", "0.0", ")", "\n", "", "self", ".", "rhos", "=", "np", ".", "array", "(", "self", ".", "rhos", ")", "/", "np", ".", "sum", "(", "self", ".", "rhos", ")", "\n", "", "if", "len", "(", "self", ".", "betas", ")", "!=", "len", "(", "self", ".", "rhos", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"Expected `len(self.betas) == len(self.rhos)`, but Found it False.\"", "\n", ")", "\n", "\n", "", "if", "RewardType", "(", "self", ".", "reward_type", ")", "not", "in", "[", "\n", "RewardType", ".", "BINARY", ",", "\n", "RewardType", ".", "CONTINUOUS", ",", "\n", "]", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "f\"`reward_type` must be either '{RewardType.BINARY.value}' or '{RewardType.CONTINUOUS.value}',\"", "\n", "f\"but {self.reward_type} is given.'\"", "\n", ")", "\n", "", "check_scalar", "(", "self", ".", "reward_std", ",", "\"reward_std\"", ",", "(", "int", ",", "float", ")", ",", "min_val", "=", "0", ")", "\n", "if", "self", ".", "reward_function", "is", "None", ":", "\n", "            ", "self", ".", "expected_reward", "=", "self", ".", "sample_contextfree_expected_reward", "(", ")", "\n", "", "if", "RewardType", "(", "self", ".", "reward_type", ")", "==", "RewardType", ".", "CONTINUOUS", ":", "\n", "            ", "self", ".", "reward_min", "=", "0", "\n", "self", ".", "reward_max", "=", "1e10", "\n", "\n", "# one-hot encoding characterizing actions.", "\n", "", "if", "self", ".", "action_context", "is", "None", ":", "\n", "            ", "self", ".", "action_context", "=", "np", ".", "eye", "(", "self", ".", "n_actions", ",", "dtype", "=", "int", ")", "\n", "", "else", ":", "\n", "            ", "check_array", "(", "\n", "array", "=", "self", ".", "action_context", ",", "name", "=", "\"action_context\"", ",", "expected_dim", "=", "2", "\n", ")", "\n", "if", "self", ".", "action_context", ".", "shape", "[", "0", "]", "!=", "self", ".", "n_actions", ":", "\n", "                ", "raise", "ValueError", "(", "\n", "\"Expected `action_context.shape[0] == n_actions`, but found it False.\"", "\n", ")", "\n", "\n", "", "", "if", "self", ".", "random_state", "is", "None", ":", "\n", "            ", "raise", "ValueError", "(", "\"`random_state` must be given\"", ")", "\n", "", "self", ".", "random_", "=", "check_random_state", "(", "self", ".", "random_state", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.synthetic_multi.SyntheticMultiLoggersBanditDataset.len_list": [[235, 239], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "len_list", "(", "self", ")", "->", "int", ":", "\n", "        ", "\"\"\"Length of recommendation lists, slate size.\"\"\"", "\n", "return", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.synthetic_multi.SyntheticMultiLoggersBanditDataset.n_strata": [[240, 244], ["len"], "methods", ["None"], ["", "@", "property", "\n", "def", "n_strata", "(", "self", ")", "->", "int", ":", "\n", "        ", "\"\"\"Number of strata, number of logging/behavior policies.\"\"\"", "\n", "return", "len", "(", "self", ".", "betas", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.synthetic_multi.SyntheticMultiLoggersBanditDataset.obtain_batch_bandit_feedback": [[245, 325], ["sklearn.utils.check_scalar", "synthetic_multi.SyntheticMultiLoggersBanditDataset.random_.normal", "numpy.concatenate", "synthetic_multi.SyntheticMultiLoggersBanditDataset.calc_expected_reward", "utils.sample_action_fast", "numpy.zeros_like", "zip", "synthetic_multi.SyntheticMultiLoggersBanditDataset.sample_reward_given_expected_reward", "dict", "int", "numpy.sum", "numpy.concatenate", "reward_type.RewardType", "scipy.stats.truncnorm.stats", "numpy.zeros_like", "utils.softmax", "utils.softmax", "numpy.round", "utils.softmax", "numpy.ones", "enumerate", "numpy.argsort", "numpy.tile", "numpy.ones", "enumerate", "synthetic_multi.SyntheticMultiLoggersBanditDataset.random_.gumbel", "numpy.arange", "numpy.arange", "numpy.arange"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.synthetic.SyntheticBanditDataset.calc_expected_reward", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.sample_action_fast", "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.synthetic.SyntheticBanditDataset.sample_reward_given_expected_reward", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.softmax", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.softmax", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.softmax"], ["", "def", "obtain_batch_bandit_feedback", "(", "self", ",", "n_rounds", ":", "int", ")", "->", "BanditFeedback", ":", "\n", "        ", "\"\"\"Obtain batch logged bandit data.\n\n        Parameters\n        ----------\n        n_rounds: int\n            Total data size (= sum of size of each stratum) of the synthetic logged bandit data.\n\n        Returns\n        ---------\n        bandit_feedback_multi: BanditFeedback\n            Synthesized logged bandit data with multiple loggers.\n\n        \"\"\"", "\n", "check_scalar", "(", "n_rounds", ",", "\"n_rounds\"", ",", "int", ",", "min_val", "=", "1", ")", "\n", "contexts", "=", "self", ".", "random_", ".", "normal", "(", "size", "=", "(", "n_rounds", ",", "self", ".", "dim_context", ")", ")", "\n", "\n", "n_rounds_strata", "=", "[", "int", "(", "np", ".", "round", "(", "n_rounds", "*", "rho", ")", ")", "for", "rho", "in", "self", ".", "rhos", "]", "\n", "n_rounds_strata", "[", "-", "1", "]", "+=", "n_rounds", "-", "np", ".", "sum", "(", "n_rounds_strata", ")", "\n", "stratum_idx", "=", "np", ".", "concatenate", "(", "\n", "[", "np", ".", "ones", "(", "n_k", ",", "dtype", "=", "int", ")", "*", "k", "for", "k", ",", "n_k", "in", "enumerate", "(", "n_rounds_strata", ")", "]", "\n", ")", "\n", "beta_", "=", "np", ".", "concatenate", "(", "\n", "[", "np", ".", "ones", "(", "n_k", ")", "*", "self", ".", "betas", "[", "k", "]", "for", "k", ",", "n_k", "in", "enumerate", "(", "n_rounds_strata", ")", "]", "\n", ")", "[", ":", ",", "np", ".", "newaxis", "]", "\n", "\n", "# calc expected reward given context and action", "\n", "expected_reward_", "=", "self", ".", "calc_expected_reward", "(", "contexts", ")", "\n", "if", "RewardType", "(", "self", ".", "reward_type", ")", "==", "RewardType", ".", "CONTINUOUS", ":", "\n", "# correct expected_reward_, as we use truncated normal distribution here", "\n", "            ", "mean", "=", "expected_reward_", "\n", "a", "=", "(", "self", ".", "reward_min", "-", "mean", ")", "/", "self", ".", "reward_std", "\n", "b", "=", "(", "self", ".", "reward_max", "-", "mean", ")", "/", "self", ".", "reward_std", "\n", "expected_reward_", "=", "truncnorm", ".", "stats", "(", "\n", "a", "=", "a", ",", "b", "=", "b", ",", "loc", "=", "mean", ",", "scale", "=", "self", ".", "reward_std", ",", "moments", "=", "\"m\"", "\n", ")", "\n", "\n", "# calculate the action choice probabilities of the behavior policy", "\n", "", "pi_b_logits", "=", "expected_reward_", "\n", "# create some deficient actions based on the value of `n_deficient_actions`", "\n", "if", "self", ".", "n_deficient_actions", ">", "0", ":", "\n", "            ", "pi_b", "=", "np", ".", "zeros_like", "(", "pi_b_logits", ")", "\n", "n_supported_actions", "=", "self", ".", "n_actions", "-", "self", ".", "n_deficient_actions", "\n", "supported_actions", "=", "np", ".", "argsort", "(", "\n", "self", ".", "random_", ".", "gumbel", "(", "size", "=", "(", "n_rounds", ",", "self", ".", "n_actions", ")", ")", ",", "axis", "=", "1", "\n", ")", "[", ":", ",", ":", ":", "-", "1", "]", "[", ":", ",", ":", "n_supported_actions", "]", "\n", "supported_actions_idx", "=", "(", "\n", "np", ".", "tile", "(", "np", ".", "arange", "(", "n_rounds", ")", ",", "(", "n_supported_actions", ",", "1", ")", ")", ".", "T", ",", "\n", "supported_actions", ",", "\n", ")", "\n", "pi_b", "[", "supported_actions_idx", "]", "=", "softmax", "(", "\n", "self", ".", "beta", "*", "pi_b_logits", "[", "supported_actions_idx", "]", "\n", ")", "\n", "", "else", ":", "\n", "            ", "pi_b", "=", "softmax", "(", "beta_", "*", "pi_b_logits", ")", "\n", "# sample actions for each round based on the behavior policy", "\n", "", "actions", "=", "sample_action_fast", "(", "pi_b", ",", "random_state", "=", "self", ".", "random_state", ")", "\n", "\n", "pi_b_avg", "=", "np", ".", "zeros_like", "(", "pi_b_logits", ")", "\n", "for", "beta", ",", "rho", "in", "zip", "(", "self", ".", "betas", ",", "self", ".", "rhos", ")", ":", "\n", "            ", "pi_b_avg", "+=", "rho", "*", "softmax", "(", "beta", "*", "pi_b_logits", ")", "\n", "\n", "# sample rewards based on the context and action", "\n", "", "rewards", "=", "self", ".", "sample_reward_given_expected_reward", "(", "expected_reward_", ",", "actions", ")", "\n", "\n", "return", "dict", "(", "\n", "n_rounds", "=", "n_rounds", ",", "\n", "n_actions", "=", "self", ".", "n_actions", ",", "\n", "n_strata", "=", "self", ".", "n_strata", ",", "\n", "context", "=", "contexts", ",", "\n", "action_context", "=", "self", ".", "action_context", ",", "\n", "action", "=", "actions", ",", "\n", "position", "=", "None", ",", "# position effect is not considered in synthetic data", "\n", "reward", "=", "rewards", ",", "\n", "expected_reward", "=", "expected_reward_", ",", "\n", "stratum_idx", "=", "stratum_idx", ",", "\n", "pi_b", "=", "pi_b", "[", ":", ",", ":", ",", "np", ".", "newaxis", "]", ",", "\n", "pi_b_avg", "=", "pi_b_avg", "[", ":", ",", ":", ",", "np", ".", "newaxis", "]", ",", "\n", "pscore", "=", "pi_b", "[", "np", ".", "arange", "(", "n_rounds", ")", ",", "actions", "]", ",", "\n", "pscore_avg", "=", "pi_b_avg", "[", "np", ".", "arange", "(", "n_rounds", ")", ",", "actions", "]", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.real.OpenBanditDataset.__post_init__": [[66, 102], ["real.OpenBanditDataset.load_raw_data", "real.OpenBanditDataset.pre_process", "ValueError", "ValueError", "logger.info", "isinstance", "isinstance", "pathlib.Path", "pathlib.Path", "ValueError"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.real.OpenBanditDataset.load_raw_data", "home.repos.pwc.inspect_result.st-tech_zr-obp.cf_policy_search.custom_dataset.OBDWithInteractionFeatures.pre_process"], ["def", "__post_init__", "(", "self", ")", "->", "None", ":", "\n", "        ", "\"\"\"Initialize Open Bandit Dataset Class.\"\"\"", "\n", "if", "self", ".", "behavior_policy", "not", "in", "[", "\n", "\"bts\"", ",", "\n", "\"random\"", ",", "\n", "]", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "f\"`behavior_policy` must be either of 'bts' or 'random', but {self.behavior_policy} is given\"", "\n", ")", "\n", "\n", "", "if", "self", ".", "campaign", "not", "in", "[", "\n", "\"all\"", ",", "\n", "\"men\"", ",", "\n", "\"women\"", ",", "\n", "]", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "f\"`campaign` must be one of 'all', 'men', or 'women', but {self.campaign} is given\"", "\n", ")", "\n", "\n", "", "if", "self", ".", "data_path", "is", "None", ":", "\n", "            ", "logger", ".", "info", "(", "\n", "\"When `data_path` is not given, this class downloads the small-sized version of Open Bandit Dataset.\"", "\n", ")", "\n", "self", ".", "data_path", "=", "Path", "(", "__file__", ")", ".", "parent", "/", "\"obd\"", "\n", "", "else", ":", "\n", "            ", "if", "isinstance", "(", "self", ".", "data_path", ",", "Path", ")", ":", "\n", "                ", "pass", "\n", "", "elif", "isinstance", "(", "self", ".", "data_path", ",", "str", ")", ":", "\n", "                ", "self", ".", "data_path", "=", "Path", "(", "self", ".", "data_path", ")", "\n", "", "else", ":", "\n", "                ", "raise", "ValueError", "(", "\"`data_path` must be a string or Path\"", ")", "\n", "", "", "self", ".", "data_path", "=", "self", ".", "data_path", "/", "self", ".", "behavior_policy", "/", "self", ".", "campaign", "\n", "self", ".", "raw_data_file", "=", "f\"{self.campaign}.csv\"", "\n", "\n", "self", ".", "load_raw_data", "(", ")", "\n", "self", ".", "pre_process", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.real.OpenBanditDataset.n_rounds": [[103, 107], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "n_rounds", "(", "self", ")", "->", "int", ":", "\n", "        ", "\"\"\"Size of the logged bandit data.\"\"\"", "\n", "return", "self", ".", "data", ".", "shape", "[", "0", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.real.OpenBanditDataset.n_actions": [[108, 112], ["int", "real.OpenBanditDataset.action.max"], "methods", ["None"], ["", "@", "property", "\n", "def", "n_actions", "(", "self", ")", "->", "int", ":", "\n", "        ", "\"\"\"Number of actions.\"\"\"", "\n", "return", "int", "(", "self", ".", "action", ".", "max", "(", ")", "+", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.real.OpenBanditDataset.dim_context": [[113, 117], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "dim_context", "(", "self", ")", "->", "int", ":", "\n", "        ", "\"\"\"Dimensions of context vectors.\"\"\"", "\n", "return", "self", ".", "context", ".", "shape", "[", "1", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.real.OpenBanditDataset.len_list": [[118, 122], ["int", "real.OpenBanditDataset.position.max"], "methods", ["None"], ["", "@", "property", "\n", "def", "len_list", "(", "self", ")", "->", "int", ":", "\n", "        ", "\"\"\"Length of recommendation lists, slate size.\"\"\"", "\n", "return", "int", "(", "self", ".", "position", ".", "max", "(", ")", "+", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.real.OpenBanditDataset.calc_on_policy_policy_value_estimate": [[123, 173], ["cls().obtain_batch_bandit_feedback", "bandit_feedback_test[].mean", "cls"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.multiclass.MultiClassToBanditReduction.obtain_batch_bandit_feedback"], ["", "@", "classmethod", "\n", "def", "calc_on_policy_policy_value_estimate", "(", "\n", "cls", ",", "\n", "behavior_policy", ":", "str", ",", "\n", "campaign", ":", "str", ",", "\n", "data_path", ":", "Optional", "[", "Path", "]", "=", "None", ",", "\n", "test_size", ":", "float", "=", "0.3", ",", "\n", "is_timeseries_split", ":", "bool", "=", "False", ",", "\n", ")", "->", "float", ":", "\n", "        ", "\"\"\"Calculate the on-policy policy value estimate (used as a ground-truth policy value).\n\n        Parameters\n        ----------\n        behavior_policy: str\n            Name of the behavior policy that generated the logged bandit data.\n            Must be either 'random' or 'bts'.\n\n        campaign: str\n            One of the three possible campaigns considered in ZOZOTOWN (i.e., \"all\", \"men\", and \"women\").\n\n        data_path: Path, default=None\n            Path where the Open Bandit Dataset exists.\n            When `None` is given, this class downloads the example small-sized version of the dataset.\n\n        test_size: float, default=0.3\n            Proportion of the dataset included in the test split.\n            If float, should be between 0.0 and 1.0.\n            This argument matters only when `is_timeseries_split=True` (the out-sample case).\n\n        is_timeseries_split: bool, default=False\n            If true, split the original logged bandit data by time series.\n\n        Returns\n        ---------\n        on_policy_policy_value_estimate: float\n            Policy value of the behavior policy estimated by on-policy estimation, i.e., :math:`\\\\mathbb{E}_{n} [r_i]`.\n            where :math:`\\\\mathbb{E}_{n}[\\\\cdot]` is the empirical average over :math:`n` observations in :math:`\\\\mathcal{D}`.\n            This is used as a ground-truth policy value in the evaluation of OPE estimators.\n\n        \"\"\"", "\n", "bandit_feedback", "=", "cls", "(", "\n", "behavior_policy", "=", "behavior_policy", ",", "campaign", "=", "campaign", ",", "data_path", "=", "data_path", "\n", ")", ".", "obtain_batch_bandit_feedback", "(", "\n", "test_size", "=", "test_size", ",", "is_timeseries_split", "=", "is_timeseries_split", "\n", ")", "\n", "if", "is_timeseries_split", ":", "\n", "            ", "bandit_feedback_test", "=", "bandit_feedback", "[", "1", "]", "\n", "", "else", ":", "\n", "            ", "bandit_feedback_test", "=", "bandit_feedback", "\n", "", "return", "bandit_feedback_test", "[", "\"reward\"", "]", ".", "mean", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.real.OpenBanditDataset.load_raw_data": [[174, 187], ["pandas.read_csv", "pandas.read_csv", "real.OpenBanditDataset.data.sort_values", "scipy.stats.rankdata"], "methods", ["None"], ["", "def", "load_raw_data", "(", "self", ")", "->", "None", ":", "\n", "        ", "\"\"\"Load raw open bandit dataset.\"\"\"", "\n", "self", ".", "data", "=", "pd", ".", "read_csv", "(", "self", ".", "data_path", "/", "self", ".", "raw_data_file", ",", "index_col", "=", "0", ")", "\n", "self", ".", "item_context", "=", "pd", ".", "read_csv", "(", "\n", "self", ".", "data_path", "/", "\"item_context.csv\"", ",", "index_col", "=", "0", "\n", ")", "\n", "self", ".", "data", ".", "sort_values", "(", "\"timestamp\"", ",", "inplace", "=", "True", ")", "\n", "self", ".", "action", "=", "self", ".", "data", "[", "\"item_id\"", "]", ".", "values", "\n", "self", ".", "position", "=", "(", "rankdata", "(", "self", ".", "data", "[", "\"position\"", "]", ".", "values", ",", "\"dense\"", ")", "-", "1", ")", ".", "astype", "(", "\n", "int", "\n", ")", "\n", "self", ".", "reward", "=", "self", ".", "data", "[", "\"click\"", "]", ".", "values", "\n", "self", ".", "pscore", "=", "self", ".", "data", "[", "\"propensity_score\"", "]", ".", "values", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.real.OpenBanditDataset.pre_process": [[188, 208], ["real.OpenBanditDataset.data.columns.str.contains", "real.OpenBanditDataset.item_context[].to_frame", "real.OpenBanditDataset.item_context.drop().apply", "pandas.get_dummies", "pandas.concat", "real.OpenBanditDataset.item_context.drop", "sklearn.preprocessing.LabelEncoder"], "methods", ["None"], ["", "def", "pre_process", "(", "self", ")", "->", "None", ":", "\n", "        ", "\"\"\"Preprocess raw open bandit dataset.\n\n        Note\n        -----\n        This is the default feature engineering and please override this method to\n        implement your own preprocessing.\n        see https://github.com/st-tech/zr-obp/blob/master/examples/examples_with_obd/custom_dataset.py for example.\n\n        \"\"\"", "\n", "user_cols", "=", "self", ".", "data", ".", "columns", ".", "str", ".", "contains", "(", "\"user_feature\"", ")", "\n", "self", ".", "context", "=", "pd", ".", "get_dummies", "(", "\n", "self", ".", "data", ".", "loc", "[", ":", ",", "user_cols", "]", ",", "drop_first", "=", "True", "\n", ")", ".", "values", "\n", "item_feature_0", "=", "self", ".", "item_context", "[", "\"item_feature_0\"", "]", ".", "to_frame", "(", ")", "\n", "item_feature_cat", "=", "self", ".", "item_context", ".", "drop", "(", "\n", "columns", "=", "[", "\"item_id\"", ",", "\"item_feature_0\"", "]", ",", "axis", "=", "1", "\n", ")", ".", "apply", "(", "LabelEncoder", "(", ")", ".", "fit_transform", ")", "\n", "self", ".", "action_context", "=", "pd", ".", "concat", "(", "\n", "objs", "=", "[", "item_feature_cat", ",", "item_feature_0", "]", ",", "axis", "=", "1", "\n", ")", ".", "values", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.real.OpenBanditDataset.obtain_batch_bandit_feedback": [[210, 285], ["isinstance", "TypeError", "sklearn.utils.check_scalar", "numpy.int32", "dict", "dict", "dict", "type"], "methods", ["None"], ["", "def", "obtain_batch_bandit_feedback", "(", "\n", "self", ",", "test_size", ":", "float", "=", "0.3", ",", "is_timeseries_split", ":", "bool", "=", "False", "\n", ")", "->", "Union", "[", "BanditFeedback", ",", "Tuple", "[", "BanditFeedback", ",", "BanditFeedback", "]", "]", ":", "\n", "        ", "\"\"\"Obtain batch logged bandit data.\n\n        Parameters\n        -----------\n        test_size: float, default=0.3\n            Proportion of the dataset included in the test split.\n            If float, should be between 0.0 and 1.0.\n            This argument matters only when `is_timeseries_split=True` (the out-sample case).\n\n        is_timeseries_split: bool, default=False\n            If true, split the original logged bandit data into train and test sets based on time series.\n\n        Returns\n        --------\n        bandit_feedback: BanditFeedback\n            A dictionary containing batch logged bandit data collected by the behavior policy.\n            The keys of the dictionary are as follows.\n            - n_rounds: number of rounds, data size of the logged bandit data\n            - n_actions: number of actions (:math:`|\\mathcal{A}|`)\n            - action: action variables sampled by the behavior policy\n            - position: positions where actions are recommended, there are three positions in the ZOZOTOWN rec interface\n            - reward: binary reward variables, click indicators\n            - pscore: action choice probabilities by the behavior policy, propensity scores\n            - context: context vectors such as user-related features and user-item affinity scores\n            - action_context: item-related context vectors\n\n        \"\"\"", "\n", "if", "not", "isinstance", "(", "is_timeseries_split", ",", "bool", ")", ":", "\n", "            ", "raise", "TypeError", "(", "\n", "f\"`is_timeseries_split` must be a bool, but {type(is_timeseries_split)} is given\"", "\n", ")", "\n", "\n", "", "if", "is_timeseries_split", ":", "\n", "            ", "check_scalar", "(", "\n", "test_size", ",", "\n", "name", "=", "\"target_size\"", ",", "\n", "target_type", "=", "(", "float", ")", ",", "\n", "min_val", "=", "0.0", ",", "\n", "max_val", "=", "1.0", ",", "\n", ")", "\n", "n_rounds_train", "=", "np", ".", "int32", "(", "self", ".", "n_rounds", "*", "(", "1.0", "-", "test_size", ")", ")", "\n", "bandit_feedback_train", "=", "dict", "(", "\n", "n_rounds", "=", "n_rounds_train", ",", "\n", "n_actions", "=", "self", ".", "n_actions", ",", "\n", "action", "=", "self", ".", "action", "[", ":", "n_rounds_train", "]", ",", "\n", "position", "=", "self", ".", "position", "[", ":", "n_rounds_train", "]", ",", "\n", "reward", "=", "self", ".", "reward", "[", ":", "n_rounds_train", "]", ",", "\n", "pscore", "=", "self", ".", "pscore", "[", ":", "n_rounds_train", "]", ",", "\n", "context", "=", "self", ".", "context", "[", ":", "n_rounds_train", "]", ",", "\n", "action_context", "=", "self", ".", "action_context", ",", "\n", ")", "\n", "bandit_feedback_test", "=", "dict", "(", "\n", "n_rounds", "=", "(", "self", ".", "n_rounds", "-", "n_rounds_train", ")", ",", "\n", "n_actions", "=", "self", ".", "n_actions", ",", "\n", "action", "=", "self", ".", "action", "[", "n_rounds_train", ":", "]", ",", "\n", "position", "=", "self", ".", "position", "[", "n_rounds_train", ":", "]", ",", "\n", "reward", "=", "self", ".", "reward", "[", "n_rounds_train", ":", "]", ",", "\n", "pscore", "=", "self", ".", "pscore", "[", "n_rounds_train", ":", "]", ",", "\n", "context", "=", "self", ".", "context", "[", "n_rounds_train", ":", "]", ",", "\n", "action_context", "=", "self", ".", "action_context", ",", "\n", ")", "\n", "return", "bandit_feedback_train", ",", "bandit_feedback_test", "\n", "", "else", ":", "\n", "            ", "return", "dict", "(", "\n", "n_rounds", "=", "self", ".", "n_rounds", ",", "\n", "n_actions", "=", "self", ".", "n_actions", ",", "\n", "action", "=", "self", ".", "action", ",", "\n", "position", "=", "self", ".", "position", ",", "\n", "reward", "=", "self", ".", "reward", ",", "\n", "pscore", "=", "self", ".", "pscore", ",", "\n", "context", "=", "self", ".", "context", ",", "\n", "action_context", "=", "self", ".", "action_context", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.real.OpenBanditDataset.sample_bootstrap_bandit_feedback": [[287, 356], ["sklearn.utils.check_random_state", "sklearn.utils.check_random_state.choice", "real.OpenBanditDataset.obtain_batch_bandit_feedback", "sklearn.utils.check_scalar", "numpy.arange", "real.OpenBanditDataset.obtain_batch_bandit_feedback"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.multiclass.MultiClassToBanditReduction.obtain_batch_bandit_feedback", "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.multiclass.MultiClassToBanditReduction.obtain_batch_bandit_feedback"], ["", "", "def", "sample_bootstrap_bandit_feedback", "(", "\n", "self", ",", "\n", "sample_size", ":", "Optional", "[", "int", "]", "=", "None", ",", "\n", "test_size", ":", "float", "=", "0.3", ",", "\n", "is_timeseries_split", ":", "bool", "=", "False", ",", "\n", "random_state", ":", "Optional", "[", "int", "]", "=", "None", ",", "\n", ")", "->", "BanditFeedback", ":", "\n", "        ", "\"\"\"Obtain bootstrap logged bandit feedback.\n\n        Parameters\n        -----------\n        sample_size: int, default=None\n            Number of data sampled by bootstrap.\n            If None, the original data size (n_rounds) is used as `sample_size`.\n            The value must be smaller than the original data size.\n\n        test_size: float, default=0.3\n            Proportion of the dataset included in the test split.\n            If float, should be between 0.0 and 1.0.\n            This argument matters only when `is_timeseries_split=True` (the out-sample case).\n\n        is_timeseries_split: bool, default=False\n            If true, split the original logged bandit data into train and test sets based on time series.\n\n        random_state: int, default=None\n            Controls the random seed in bootstrap sampling.\n\n        Returns\n        --------\n        bandit_feedback: BanditFeedback\n            A dictionary containing logged bandit data collected by the behavior policy.\n            The keys of the dictionary are as follows.\n            - n_rounds: number of rounds, data size of the logged bandit data\n            - n_actions: number of actions (:math:`|\\mathcal{A}|`)\n            - action: action variables sampled by the behavior policy\n            - position: positions where actions are recommended, there are three positions in the ZOZOTOWN rec interface\n            - reward: binary reward variables, click indicators\n            - pscore: action choice probabilities by the behavior policy, propensity scores\n            - context: context vectors such as user-related features and user-item affinity scores\n            - action_context: item-related context vectors\n\n        \"\"\"", "\n", "if", "is_timeseries_split", ":", "\n", "            ", "bandit_feedback", "=", "self", ".", "obtain_batch_bandit_feedback", "(", "\n", "test_size", "=", "test_size", ",", "is_timeseries_split", "=", "is_timeseries_split", "\n", ")", "[", "0", "]", "\n", "", "else", ":", "\n", "            ", "bandit_feedback", "=", "self", ".", "obtain_batch_bandit_feedback", "(", "\n", "test_size", "=", "test_size", ",", "is_timeseries_split", "=", "is_timeseries_split", "\n", ")", "\n", "", "n_rounds", "=", "bandit_feedback", "[", "\"n_rounds\"", "]", "\n", "if", "sample_size", "is", "None", ":", "\n", "            ", "sample_size", "=", "bandit_feedback", "[", "\"n_rounds\"", "]", "\n", "", "else", ":", "\n", "            ", "check_scalar", "(", "\n", "sample_size", ",", "\n", "name", "=", "\"sample_size\"", ",", "\n", "target_type", "=", "(", "int", ")", ",", "\n", "min_val", "=", "0", ",", "\n", "max_val", "=", "n_rounds", ",", "\n", ")", "\n", "", "random_", "=", "check_random_state", "(", "random_state", ")", "\n", "bootstrap_idx", "=", "random_", ".", "choice", "(", "\n", "np", ".", "arange", "(", "n_rounds", ")", ",", "size", "=", "sample_size", ",", "replace", "=", "True", "\n", ")", "\n", "for", "key_", "in", "[", "\"action\"", ",", "\"position\"", ",", "\"reward\"", ",", "\"pscore\"", ",", "\"context\"", "]", ":", "\n", "            ", "bandit_feedback", "[", "key_", "]", "=", "bandit_feedback", "[", "key_", "]", "[", "bootstrap_idx", "]", "\n", "", "bandit_feedback", "[", "\"n_rounds\"", "]", "=", "sample_size", "\n", "return", "bandit_feedback", "\n", "", "", ""]], "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.synthetic_slate.SyntheticSlateBanditDataset.__post_init__": [[216, 309], ["sklearn.utils.check_scalar", "sklearn.utils.check_scalar", "sklearn.utils.check_scalar", "sklearn.utils.check_random_state", "numpy.eye", "ValueError", "ValueError", "ValueError", "ValueError", "sklearn.utils.check_scalar", "numpy.ones", "ValueError", "synthetic_slate.generate_symmetric_matrix", "sklearn.utils.check_scalar", "numpy.ones", "numpy.ones", "numpy.ones", "synthetic_slate.SyntheticSlateBanditDataset.obtain_standard_decay_action_interaction_weight_matrix", "numpy.ones", "numpy.arange", "synthetic_slate.SyntheticSlateBanditDataset.obtain_cascade_decay_action_interaction_weight_matrix", "numpy.zeros", "numpy.arange"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.synthetic_slate.generate_symmetric_matrix", "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.synthetic_slate.SyntheticSlateBanditDataset.obtain_standard_decay_action_interaction_weight_matrix", "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.synthetic_slate.SyntheticSlateBanditDataset.obtain_cascade_decay_action_interaction_weight_matrix"], ["def", "__post_init__", "(", "self", ")", "->", "None", ":", "\n", "        ", "\"\"\"Initialize Class.\"\"\"", "\n", "check_scalar", "(", "self", ".", "n_unique_action", ",", "\"n_unique_action\"", ",", "int", ",", "min_val", "=", "2", ")", "\n", "if", "self", ".", "is_factorizable", ":", "\n", "            ", "max_len_list", "=", "None", "\n", "", "else", ":", "\n", "            ", "max_len_list", "=", "self", ".", "n_unique_action", "\n", "", "check_scalar", "(", "self", ".", "len_list", ",", "\"len_list\"", ",", "int", ",", "min_val", "=", "2", ",", "max_val", "=", "max_len_list", ")", "\n", "\n", "check_scalar", "(", "self", ".", "dim_context", ",", "\"dim_context\"", ",", "int", ",", "min_val", "=", "1", ")", "\n", "self", ".", "random_", "=", "check_random_state", "(", "self", ".", "random_state", ")", "\n", "if", "self", ".", "reward_type", "not", "in", "[", "\n", "\"binary\"", ",", "\n", "\"continuous\"", ",", "\n", "]", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "f\"`reward_type` must be either 'binary' or 'continuous', but {self.reward_type} is given.\"", "\n", ")", "\n", "", "if", "self", ".", "reward_structure", "not", "in", "[", "\n", "\"cascade_additive\"", ",", "\n", "\"cascade_decay\"", ",", "\n", "\"independent\"", ",", "\n", "\"standard_additive\"", ",", "\n", "\"standard_decay\"", ",", "\n", "]", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "f\"`reward_structure` must be one of 'cascade_additive', 'cascade_decay', 'independent', 'standard_additive', or 'standard_decay', but {self.reward_structure} is given.\"", "\n", ")", "\n", "", "if", "self", ".", "decay_function", "not", "in", "[", "\"exponential\"", ",", "\"inverse\"", "]", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "f\"`decay_function` must be either 'exponential' or 'inverse', but {self.decay_function} is given\"", "\n", ")", "\n", "", "if", "self", ".", "click_model", "not", "in", "[", "\"cascade\"", ",", "\"pbm\"", ",", "None", "]", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "f\"`click_model` must be one of 'cascade', 'pbm', or None, but {self.click_model} is given.\"", "\n", ")", "\n", "# set exam_weight (slot-level examination probability).", "\n", "# When click_model is 'pbm', exam_weight is :math:`(1 / k)^{\\\\eta}`, where :math:`k` is the position.", "\n", "", "if", "self", ".", "click_model", "==", "\"pbm\"", ":", "\n", "            ", "check_scalar", "(", "self", ".", "eta", ",", "name", "=", "\"eta\"", ",", "target_type", "=", "float", ",", "min_val", "=", "0.0", ")", "\n", "self", ".", "exam_weight", "=", "(", "1.0", "/", "np", ".", "arange", "(", "1", ",", "self", ".", "len_list", "+", "1", ")", ")", "**", "self", ".", "eta", "\n", "self", ".", "attractiveness", "=", "np", ".", "ones", "(", "self", ".", "len_list", ",", "dtype", "=", "float", ")", "\n", "", "elif", "self", ".", "click_model", "==", "\"cascade\"", ":", "\n", "            ", "check_scalar", "(", "self", ".", "eta", ",", "name", "=", "\"eta\"", ",", "target_type", "=", "float", ",", "min_val", "=", "0.0", ")", "\n", "self", ".", "attractiveness", "=", "(", "1.0", "/", "np", ".", "arange", "(", "1", ",", "self", ".", "len_list", "+", "1", ")", ")", "**", "self", ".", "eta", "\n", "self", ".", "exam_weight", "=", "np", ".", "ones", "(", "self", ".", "len_list", ",", "dtype", "=", "float", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "attractiveness", "=", "np", ".", "ones", "(", "self", ".", "len_list", ",", "dtype", "=", "float", ")", "\n", "self", ".", "exam_weight", "=", "np", ".", "ones", "(", "self", ".", "len_list", ",", "dtype", "=", "float", ")", "\n", "", "if", "self", ".", "click_model", "is", "not", "None", "and", "self", ".", "reward_type", "==", "\"continuous\"", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"continuous rewards cannot be used when `click_model` is given\"", "\n", ")", "\n", "", "if", "self", ".", "base_reward_function", "is", "not", "None", ":", "\n", "            ", "self", ".", "reward_function", "=", "action_interaction_reward_function", "\n", "", "if", "self", ".", "reward_structure", "in", "[", "\"cascade_additive\"", ",", "\"standard_additive\"", "]", ":", "\n", "# generate additive action interaction weight matrix of (n_unique_action, n_unique_action)", "\n", "            ", "self", ".", "action_interaction_weight_matrix", "=", "generate_symmetric_matrix", "(", "\n", "n_unique_action", "=", "self", ".", "n_unique_action", ",", "random_state", "=", "self", ".", "random_state", "\n", ")", "\n", "", "else", ":", "\n", "# set decay function", "\n", "            ", "if", "self", ".", "decay_function", "==", "\"exponential\"", ":", "\n", "                ", "self", ".", "decay_function", "=", "exponential_decay_function", "\n", "", "else", ":", "# \"inverse\"", "\n", "                ", "self", ".", "decay_function", "=", "inverse_decay_function", "\n", "# generate decay action interaction weight matrix of (len_list, len_list)", "\n", "", "if", "self", ".", "reward_structure", "==", "\"standard_decay\"", ":", "\n", "                ", "self", ".", "action_interaction_weight_matrix", "=", "(", "\n", "self", ".", "obtain_standard_decay_action_interaction_weight_matrix", "(", "\n", "self", ".", "len_list", "\n", ")", "\n", ")", "\n", "", "elif", "self", ".", "reward_structure", "==", "\"cascade_decay\"", ":", "\n", "                ", "self", ".", "action_interaction_weight_matrix", "=", "(", "\n", "self", ".", "obtain_cascade_decay_action_interaction_weight_matrix", "(", "\n", "self", ".", "len_list", "\n", ")", "\n", ")", "\n", "", "else", ":", "\n", "                ", "self", ".", "action_interaction_weight_matrix", "=", "np", ".", "zeros", "(", "\n", "(", "self", ".", "len_list", ",", "self", ".", "len_list", ")", "\n", ")", "\n", "", "", "if", "self", ".", "behavior_policy_function", "is", "None", ":", "\n", "            ", "self", ".", "uniform_behavior_policy", "=", "(", "\n", "np", ".", "ones", "(", "self", ".", "n_unique_action", ")", "/", "self", ".", "n_unique_action", "\n", ")", "\n", "", "if", "self", ".", "reward_type", "==", "\"continuous\"", ":", "\n", "            ", "self", ".", "reward_min", "=", "0", "\n", "self", ".", "reward_max", "=", "1e10", "\n", "self", ".", "reward_std", "=", "1.0", "\n", "# one-hot encoding characterizing each action", "\n", "", "self", ".", "action_context", "=", "np", ".", "eye", "(", "self", ".", "n_unique_action", ",", "dtype", "=", "int", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.synthetic_slate.SyntheticSlateBanditDataset.obtain_standard_decay_action_interaction_weight_matrix": [[310, 322], ["numpy.identity", "numpy.arange", "synthetic_slate.SyntheticSlateBanditDataset.decay_function", "numpy.abs", "numpy.arange"], "methods", ["None"], ["", "def", "obtain_standard_decay_action_interaction_weight_matrix", "(", "\n", "self", ",", "\n", "len_list", ",", "\n", ")", "->", "np", ".", "ndarray", ":", "\n", "        ", "\"\"\"Obtain an action interaction weight matrix for standard decay reward structure (symmetric matrix)\"\"\"", "\n", "action_interaction_weight_matrix", "=", "np", ".", "identity", "(", "len_list", ")", "\n", "for", "pos_", "in", "np", ".", "arange", "(", "len_list", ")", ":", "\n", "            ", "action_interaction_weight_matrix", "[", ":", ",", "pos_", "]", "=", "-", "self", ".", "decay_function", "(", "\n", "np", ".", "abs", "(", "np", ".", "arange", "(", "len_list", ")", "-", "pos_", ")", "\n", ")", "\n", "action_interaction_weight_matrix", "[", "pos_", ",", "pos_", "]", "=", "0", "\n", "", "return", "action_interaction_weight_matrix", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.synthetic_slate.SyntheticSlateBanditDataset.obtain_cascade_decay_action_interaction_weight_matrix": [[323, 337], ["numpy.identity", "numpy.arange", "numpy.arange", "synthetic_slate.SyntheticSlateBanditDataset.decay_function", "numpy.abs", "numpy.arange"], "methods", ["None"], ["", "def", "obtain_cascade_decay_action_interaction_weight_matrix", "(", "\n", "self", ",", "\n", "len_list", ",", "\n", ")", "->", "np", ".", "ndarray", ":", "\n", "        ", "\"\"\"Obtain an action interaction weight matrix for cascade decay reward structure (upper triangular matrix)\"\"\"", "\n", "action_interaction_weight_matrix", "=", "np", ".", "identity", "(", "len_list", ")", "\n", "for", "pos_", "in", "np", ".", "arange", "(", "len_list", ")", ":", "\n", "            ", "action_interaction_weight_matrix", "[", ":", ",", "pos_", "]", "=", "-", "self", ".", "decay_function", "(", "\n", "np", ".", "abs", "(", "np", ".", "arange", "(", "len_list", ")", "-", "pos_", ")", "\n", ")", "\n", "for", "pos_2", "in", "np", ".", "arange", "(", "len_list", ")", ":", "\n", "                ", "if", "pos_", "<=", "pos_2", ":", "\n", "                    ", "action_interaction_weight_matrix", "[", "pos_2", ",", "pos_", "]", "=", "0", "\n", "", "", "", "return", "action_interaction_weight_matrix", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.synthetic_slate.SyntheticSlateBanditDataset._calc_pscore_given_policy_logit": [[338, 376], ["len", "numpy.tile", "numpy.ones", "numpy.arange", "numpy.arange", "numpy.where", "utils.softmax", "numpy.ones", "unique_action_set_2d[].reshape", "numpy.arange", "numpy.arange", "numpy.ones.astype"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.softmax"], ["", "def", "_calc_pscore_given_policy_logit", "(", "\n", "self", ",", "all_slate_actions", ":", "np", ".", "ndarray", ",", "policy_logit_i_", ":", "np", ".", "ndarray", "\n", ")", "->", "np", ".", "ndarray", ":", "\n", "        ", "\"\"\"Calculate the propensity score of all possible slate actions given a particular policy_logit.\n\n        Parameters\n        ------------\n        all_slate_actions: array-like, (n_action, len_list)\n            All possible slate actions.\n\n        policy_logit_i_: array-like, (n_unique_action, )\n            Logit values given context (:math:`x`), which defines the distribution over actions of the policy.\n\n        Returns\n        ------------\n        pscores: array-like, (n_action, )\n            Propensity scores of all slate actions.\n\n        \"\"\"", "\n", "n_actions", "=", "len", "(", "all_slate_actions", ")", "\n", "unique_action_set_2d", "=", "np", ".", "tile", "(", "np", ".", "arange", "(", "self", ".", "n_unique_action", ")", ",", "(", "n_actions", ",", "1", ")", ")", "\n", "pscores", "=", "np", ".", "ones", "(", "n_actions", ")", "\n", "for", "pos_", "in", "np", ".", "arange", "(", "self", ".", "len_list", ")", ":", "\n", "            ", "action_index", "=", "np", ".", "where", "(", "\n", "unique_action_set_2d", "==", "all_slate_actions", "[", ":", ",", "pos_", "]", "[", ":", ",", "np", ".", "newaxis", "]", "\n", ")", "[", "1", "]", "\n", "pscores", "*=", "softmax", "(", "policy_logit_i_", "[", "unique_action_set_2d", "]", ")", "[", "\n", "np", ".", "arange", "(", "n_actions", ")", ",", "action_index", "\n", "]", "\n", "# delete actions", "\n", "if", "pos_", "+", "1", "!=", "self", ".", "len_list", ":", "\n", "                ", "mask", "=", "np", ".", "ones", "(", "(", "n_actions", ",", "self", ".", "n_unique_action", "-", "pos_", ")", ")", "\n", "mask", "[", "np", ".", "arange", "(", "n_actions", ")", ",", "action_index", "]", "=", "0", "\n", "unique_action_set_2d", "=", "unique_action_set_2d", "[", "mask", ".", "astype", "(", "bool", ")", "]", ".", "reshape", "(", "\n", "(", "-", "1", ",", "self", ".", "n_unique_action", "-", "pos_", "-", "1", ")", "\n", ")", "\n", "\n", "", "", "return", "pscores", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.synthetic_slate.SyntheticSlateBanditDataset._calc_pscore_given_policy_softmax": [[377, 416], ["len", "numpy.tile", "numpy.ones", "numpy.arange", "numpy.arange", "numpy.where", "numpy.divide", "numpy.ones", "unique_action_set_2d[].reshape", "score_.sum", "numpy.arange", "numpy.arange", "numpy.ones.astype"], "methods", ["None"], ["", "def", "_calc_pscore_given_policy_softmax", "(", "\n", "self", ",", "all_slate_actions", ":", "np", ".", "ndarray", ",", "policy_softmax_i_", ":", "np", ".", "ndarray", "\n", ")", "->", "np", ".", "ndarray", ":", "\n", "        ", "\"\"\"Calculate the propensity score of all possible slate actions given a particular policy_softmax.\n\n        Parameters\n        ------------\n        all_slate_actions: array-like, (n_action, len_list)\n            All possible slate actions.\n\n        policy_softmax_i_: array-like, (n_unique_action, )\n            Policy softmax values given context (:math:`x`).\n\n        Returns\n        ------------\n        pscores: array-like, (n_action, )\n            Propensity scores of all slate actions.\n\n        \"\"\"", "\n", "n_actions", "=", "len", "(", "all_slate_actions", ")", "\n", "unique_action_set_2d", "=", "np", ".", "tile", "(", "np", ".", "arange", "(", "self", ".", "n_unique_action", ")", ",", "(", "n_actions", ",", "1", ")", ")", "\n", "pscores", "=", "np", ".", "ones", "(", "n_actions", ")", "\n", "for", "pos_", "in", "np", ".", "arange", "(", "self", ".", "len_list", ")", ":", "\n", "            ", "action_index", "=", "np", ".", "where", "(", "\n", "unique_action_set_2d", "==", "all_slate_actions", "[", ":", ",", "pos_", "]", "[", ":", ",", "np", ".", "newaxis", "]", "\n", ")", "[", "1", "]", "\n", "score_", "=", "policy_softmax_i_", "[", "unique_action_set_2d", "]", "\n", "pscores", "*=", "np", ".", "divide", "(", "score_", ",", "score_", ".", "sum", "(", "axis", "=", "1", ",", "keepdims", "=", "True", ")", ")", "[", "\n", "np", ".", "arange", "(", "n_actions", ")", ",", "action_index", "\n", "]", "\n", "# delete actions", "\n", "if", "pos_", "+", "1", "!=", "self", ".", "len_list", ":", "\n", "                ", "mask", "=", "np", ".", "ones", "(", "(", "n_actions", ",", "self", ".", "n_unique_action", "-", "pos_", ")", ")", "\n", "mask", "[", "np", ".", "arange", "(", "n_actions", ")", ",", "action_index", "]", "=", "0", "\n", "unique_action_set_2d", "=", "unique_action_set_2d", "[", "mask", ".", "astype", "(", "bool", ")", "]", ".", "reshape", "(", "\n", "(", "-", "1", ",", "self", ".", "n_unique_action", "-", "pos_", "-", "1", ")", "\n", ")", "\n", "\n", "", "", "return", "pscores", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.synthetic_slate.SyntheticSlateBanditDataset.obtain_pscore_given_evaluation_policy_logit": [[417, 534], ["utils.check_array", "utils.check_array", "numpy.zeros", "numpy.zeros", "tqdm.tqdm.tqdm", "ValueError", "numpy.zeros", "sklearn.utils.check_scalar", "numpy.exp", "numpy.arange", "numpy.arange", "numpy.arange", "len", "action.reshape", "numpy.array", "numpy.minimum", "utils.softmax", "len", "numpy.delete", "itertools.permutations", "numpy.where", "utils.softmax", "numpy.arange", "isinstance", "pscores[].sum", "synthetic_slate.SyntheticSlateBanditDataset._calc_pscore_given_policy_softmax", "synthetic_slate.SyntheticSlateBanditDataset._calc_pscore_given_policy_logit"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.softmax", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.softmax", "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.synthetic_slate.SyntheticSlateBanditDataset._calc_pscore_given_policy_softmax", "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.synthetic_slate.SyntheticSlateBanditDataset._calc_pscore_given_policy_logit"], ["", "def", "obtain_pscore_given_evaluation_policy_logit", "(", "\n", "self", ",", "\n", "action", ":", "np", ".", "ndarray", ",", "\n", "evaluation_policy_logit_", ":", "np", ".", "ndarray", ",", "\n", "return_pscore_item_position", ":", "bool", "=", "True", ",", "\n", "clip_logit_value", ":", "Optional", "[", "float", "]", "=", "None", ",", "\n", ")", ":", "\n", "        ", "\"\"\"Calculate the propensity score given particular logit values to define the evaluation policy.\n\n        Parameters\n        ------------\n        action: array-like, (n_rounds * len_list, )\n            Action chosen by the behavior policy.\n\n        evaluation_policy_logit_: array-like, (n_rounds, n_unique_action)\n            Logit values to define the evaluation policy.\n\n        return_pscore_item_position: bool, default=True\n            Whether to compute `pscore_item_position` and include it in the logged data.\n            When `n_actions` and `len_list` are large, `return_pscore_item_position`=True can lead to a long computation time.\n\n        clip_logit_value: Optional[float], default=None\n            A float parameter used to clip logit values (<= `700.`).\n            When None, clipping is not applied to softmax values when obtaining `pscore_item_position`.\n            When a float value is given, logit values are clipped when calculating softmax values.\n            When `n_actions` and `len_list` are large, `clip_logit_value`=None can lead to a long computation time.\n\n        \"\"\"", "\n", "check_array", "(", "array", "=", "action", ",", "name", "=", "\"action\"", ",", "expected_dim", "=", "1", ")", "\n", "check_array", "(", "\n", "array", "=", "evaluation_policy_logit_", ",", "\n", "name", "=", "\"evaluation_policy_logit_\"", ",", "\n", "expected_dim", "=", "2", ",", "\n", ")", "\n", "if", "(", "\n", "len", "(", "action", ")", "/", "self", ".", "len_list", "!=", "len", "(", "evaluation_policy_logit_", ")", "\n", "or", "evaluation_policy_logit_", ".", "shape", "[", "1", "]", "!=", "self", ".", "n_unique_action", "\n", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"the shape of `action` and `evaluation_policy_logit_` must be (n_rounds * len_list, )\"", "\n", "\"and (n_rounds, n_unique_action) respectively\"", "\n", ")", "\n", "\n", "", "n_rounds", "=", "action", ".", "reshape", "(", "(", "-", "1", ",", "self", ".", "len_list", ")", ")", ".", "shape", "[", "0", "]", "\n", "pscore_cascade", "=", "np", ".", "zeros", "(", "n_rounds", "*", "self", ".", "len_list", ")", "\n", "pscore", "=", "np", ".", "zeros", "(", "n_rounds", "*", "self", ".", "len_list", ")", "\n", "if", "return_pscore_item_position", ":", "\n", "            ", "pscore_item_position", "=", "np", ".", "zeros", "(", "n_rounds", "*", "self", ".", "len_list", ")", "\n", "if", "not", "self", ".", "is_factorizable", ":", "\n", "                ", "enumerated_slate_actions", "=", "[", "\n", "_", "\n", "for", "_", "in", "permutations", "(", "\n", "np", ".", "arange", "(", "self", ".", "n_unique_action", ")", ",", "self", ".", "len_list", "\n", ")", "\n", "]", "\n", "enumerated_slate_actions", "=", "np", ".", "array", "(", "enumerated_slate_actions", ")", "\n", "", "", "else", ":", "\n", "            ", "pscore_item_position", "=", "None", "\n", "", "if", "return_pscore_item_position", "and", "clip_logit_value", "is", "not", "None", ":", "\n", "            ", "check_scalar", "(", "\n", "clip_logit_value", ",", "\n", "name", "=", "\"clip_logit_value\"", ",", "\n", "target_type", "=", "(", "float", ")", ",", "\n", "max_val", "=", "700.0", ",", "\n", ")", "\n", "evaluation_policy_softmax_", "=", "np", ".", "exp", "(", "\n", "np", ".", "minimum", "(", "evaluation_policy_logit_", ",", "clip_logit_value", ")", "\n", ")", "\n", "", "for", "i", "in", "tqdm", "(", "\n", "np", ".", "arange", "(", "n_rounds", ")", ",", "\n", "desc", "=", "\"[obtain_pscore_given_evaluation_policy_logit]\"", ",", "\n", "total", "=", "n_rounds", ",", "\n", ")", ":", "\n", "            ", "unique_action_set", "=", "np", ".", "arange", "(", "self", ".", "n_unique_action", ")", "\n", "score_", "=", "softmax", "(", "evaluation_policy_logit_", "[", "i", ":", "i", "+", "1", "]", ")", "[", "0", "]", "\n", "pscore_i", "=", "1.0", "\n", "for", "pos_", "in", "np", ".", "arange", "(", "self", ".", "len_list", ")", ":", "\n", "                ", "action_", "=", "action", "[", "i", "*", "self", ".", "len_list", "+", "pos_", "]", "\n", "action_index_", "=", "np", ".", "where", "(", "unique_action_set", "==", "action_", ")", "[", "0", "]", "[", "0", "]", "\n", "# calculate joint pscore", "\n", "pscore_i", "*=", "score_", "[", "action_index_", "]", "\n", "pscore_cascade", "[", "i", "*", "self", ".", "len_list", "+", "pos_", "]", "=", "pscore_i", "\n", "# update the pscore given the remaining items for nonfactorizable policy", "\n", "if", "not", "self", ".", "is_factorizable", "and", "pos_", "!=", "self", ".", "len_list", "-", "1", ":", "\n", "                    ", "unique_action_set", "=", "np", ".", "delete", "(", "\n", "unique_action_set", ",", "unique_action_set", "==", "action_", "\n", ")", "\n", "score_", "=", "softmax", "(", "\n", "evaluation_policy_logit_", "[", "i", ":", "i", "+", "1", ",", "unique_action_set", "]", "\n", ")", "[", "0", "]", "\n", "# calculate pscore_item_position", "\n", "", "if", "return_pscore_item_position", ":", "\n", "                    ", "if", "pos_", "==", "0", ":", "\n", "                        ", "pscore_item_pos_i_l", "=", "pscore_i", "\n", "", "elif", "self", ".", "is_factorizable", ":", "\n", "                        ", "pscore_item_pos_i_l", "=", "score_", "[", "action_index_", "]", "\n", "", "else", ":", "\n", "                        ", "if", "isinstance", "(", "clip_logit_value", ",", "float", ")", ":", "\n", "                            ", "pscores", "=", "self", ".", "_calc_pscore_given_policy_softmax", "(", "\n", "all_slate_actions", "=", "enumerated_slate_actions", ",", "\n", "policy_softmax_i_", "=", "evaluation_policy_softmax_", "[", "i", "]", ",", "\n", ")", "\n", "", "else", ":", "\n", "                            ", "pscores", "=", "self", ".", "_calc_pscore_given_policy_logit", "(", "\n", "all_slate_actions", "=", "enumerated_slate_actions", ",", "\n", "policy_logit_i_", "=", "evaluation_policy_logit_", "[", "i", "]", ",", "\n", ")", "\n", "", "pscore_item_pos_i_l", "=", "pscores", "[", "\n", "enumerated_slate_actions", "[", ":", ",", "pos_", "]", "==", "action_", "\n", "]", ".", "sum", "(", ")", "\n", "", "pscore_item_position", "[", "i", "*", "self", ".", "len_list", "+", "pos_", "]", "=", "pscore_item_pos_i_l", "\n", "# impute joint pscore", "\n", "", "", "start_idx", "=", "i", "*", "self", ".", "len_list", "\n", "end_idx", "=", "start_idx", "+", "self", ".", "len_list", "\n", "pscore", "[", "start_idx", ":", "end_idx", "]", "=", "pscore_i", "\n", "\n", "", "return", "pscore", ",", "pscore_item_position", ",", "pscore_cascade", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.synthetic_slate.SyntheticSlateBanditDataset.sample_action_and_obtain_pscore": [[535, 662], ["numpy.zeros", "numpy.zeros", "numpy.zeros", "tqdm.tqdm.tqdm", "numpy.zeros", "sklearn.utils.check_scalar", "numpy.exp", "numpy.arange", "numpy.arange", "numpy.arange", "numpy.array", "numpy.minimum", "utils.softmax", "synthetic_slate.SyntheticSlateBanditDataset.random_.choice", "numpy.delete", "itertools.permutations", "numpy.where", "utils.softmax", "numpy.arange", "isinstance", "pscores[].sum", "synthetic_slate.SyntheticSlateBanditDataset._calc_pscore_given_policy_softmax", "synthetic_slate.SyntheticSlateBanditDataset._calc_pscore_given_policy_logit"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.softmax", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.softmax", "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.synthetic_slate.SyntheticSlateBanditDataset._calc_pscore_given_policy_softmax", "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.synthetic_slate.SyntheticSlateBanditDataset._calc_pscore_given_policy_logit"], ["", "def", "sample_action_and_obtain_pscore", "(", "\n", "self", ",", "\n", "behavior_policy_logit_", ":", "np", ".", "ndarray", ",", "\n", "n_rounds", ":", "int", ",", "\n", "return_pscore_item_position", ":", "bool", "=", "True", ",", "\n", "clip_logit_value", ":", "Optional", "[", "float", "]", "=", "None", ",", "\n", ")", "->", "Tuple", "[", "np", ".", "ndarray", ",", "np", ".", "ndarray", ",", "np", ".", "ndarray", ",", "Optional", "[", "np", ".", "ndarray", "]", "]", ":", "\n", "        ", "\"\"\"Sample action and obtain the three variants of the propensity scores.\n\n        Parameters\n        ------------\n        behavior_policy_logit_: array-like, shape (n_rounds, n_actions)\n            Logit values given context (:math:`x`).\n\n        n_rounds: int\n            Data size of synthetic logged data.\n\n        return_pscore_item_position: bool, default=True\n            Whether to compute `pscore_item_position` and include it in the logged data.\n            When `n_actions` and `len_list` are large, `return_pscore_item_position`=True can lead to a long computation time.\n\n        clip_logit_value: Optional[float], default=None\n            A float parameter used to clip logit values (<= `700.`).\n            When None, clipping is not applied to softmax values when obtaining `pscore_item_position`.\n            When a float value is given, logit values are clipped when calculating softmax values.\n            When `n_actions` and `len_list` are large, `clip_logit_value`=None can lead to a long computation time.\n\n        Returns\n        ----------\n        action: array-like, shape (n_rounds * len_list)\n            Actions sampled by the behavior policy.\n            Actions sampled within slate `i` is stored in `action[`i` * `len_list`: (`i + 1`) * `len_list`]`.\n\n        pscore: array-like, shape (n_unique_action * len_list)\n            Probabilities of choosing the slate actions given context (:math:`x`),\n            i.e., :math:`\\\\pi(a_{i,1}, a_{i,2}, \\\\ldots, a_{i,L} | x_{i} )`.\n\n        pscore_item_position: array-like, shape (n_unique_action * len_list)\n            Probabilities of choosing the action of the :math:`l`-th slot given context (:math:`x`),\n            i.e., :math:`\\\\pi(a_{i,l} | x_{i} )`.\n\n        pscore_cascade: array-like, shape (n_unique_action * len_list)\n            Probabilities of choosing the actions of the top :math:`l` slots given context (:math:`x`),\n            i.e., :math:`\\\\pi(a_{i,1}, a_{i,2}, \\\\ldots, a_{i,l} | x_{i} )`.\n\n        \"\"\"", "\n", "action", "=", "np", ".", "zeros", "(", "n_rounds", "*", "self", ".", "len_list", ",", "dtype", "=", "int", ")", "\n", "pscore_cascade", "=", "np", ".", "zeros", "(", "n_rounds", "*", "self", ".", "len_list", ")", "\n", "pscore", "=", "np", ".", "zeros", "(", "n_rounds", "*", "self", ".", "len_list", ")", "\n", "if", "return_pscore_item_position", ":", "\n", "            ", "pscore_item_position", "=", "np", ".", "zeros", "(", "n_rounds", "*", "self", ".", "len_list", ")", "\n", "if", "not", "self", ".", "is_factorizable", "and", "self", ".", "behavior_policy_function", "is", "not", "None", ":", "\n", "                ", "enumerated_slate_actions", "=", "[", "\n", "_", "\n", "for", "_", "in", "permutations", "(", "\n", "np", ".", "arange", "(", "self", ".", "n_unique_action", ")", ",", "self", ".", "len_list", "\n", ")", "\n", "]", "\n", "enumerated_slate_actions", "=", "np", ".", "array", "(", "enumerated_slate_actions", ")", "\n", "", "", "else", ":", "\n", "            ", "pscore_item_position", "=", "None", "\n", "", "if", "return_pscore_item_position", "and", "clip_logit_value", "is", "not", "None", ":", "\n", "            ", "check_scalar", "(", "\n", "clip_logit_value", ",", "\n", "name", "=", "\"clip_logit_value\"", ",", "\n", "target_type", "=", "(", "float", ")", ",", "\n", "max_val", "=", "700.0", ",", "\n", ")", "\n", "behavior_policy_softmax_", "=", "np", ".", "exp", "(", "\n", "np", ".", "minimum", "(", "behavior_policy_logit_", ",", "clip_logit_value", ")", "\n", ")", "\n", "", "for", "i", "in", "tqdm", "(", "\n", "np", ".", "arange", "(", "n_rounds", ")", ",", "\n", "desc", "=", "\"[sample_action_and_obtain_pscore]\"", ",", "\n", "total", "=", "n_rounds", ",", "\n", ")", ":", "\n", "            ", "unique_action_set", "=", "np", ".", "arange", "(", "self", ".", "n_unique_action", ")", "\n", "score_", "=", "softmax", "(", "behavior_policy_logit_", "[", "i", ":", "i", "+", "1", ",", "unique_action_set", "]", ")", "[", "0", "]", "\n", "pscore_i", "=", "1.0", "\n", "for", "pos_", "in", "np", ".", "arange", "(", "self", ".", "len_list", ")", ":", "\n", "                ", "sampled_action", "=", "self", ".", "random_", ".", "choice", "(", "\n", "unique_action_set", ",", "p", "=", "score_", ",", "replace", "=", "False", "\n", ")", "\n", "action", "[", "i", "*", "self", ".", "len_list", "+", "pos_", "]", "=", "sampled_action", "\n", "sampled_action_index", "=", "np", ".", "where", "(", "unique_action_set", "==", "sampled_action", ")", "[", "0", "]", "[", "\n", "0", "\n", "]", "\n", "# calculate joint pscore", "\n", "pscore_i", "*=", "score_", "[", "sampled_action_index", "]", "\n", "pscore_cascade", "[", "i", "*", "self", ".", "len_list", "+", "pos_", "]", "=", "pscore_i", "\n", "# update the pscore given the remaining items for nonfactorizable behavior policy", "\n", "if", "not", "self", ".", "is_factorizable", "and", "pos_", "!=", "self", ".", "len_list", "-", "1", ":", "\n", "                    ", "unique_action_set", "=", "np", ".", "delete", "(", "\n", "unique_action_set", ",", "unique_action_set", "==", "sampled_action", "\n", ")", "\n", "score_", "=", "softmax", "(", "\n", "behavior_policy_logit_", "[", "i", ":", "i", "+", "1", ",", "unique_action_set", "]", "\n", ")", "[", "0", "]", "\n", "# calculate pscore_item_position", "\n", "", "if", "return_pscore_item_position", ":", "\n", "                    ", "if", "self", ".", "behavior_policy_function", "is", "None", ":", "# uniform random", "\n", "                        ", "pscore_item_pos_i_l", "=", "1", "/", "self", ".", "n_unique_action", "\n", "", "elif", "self", ".", "is_factorizable", ":", "\n", "                        ", "pscore_item_pos_i_l", "=", "score_", "[", "sampled_action_index", "]", "\n", "", "elif", "pos_", "==", "0", ":", "\n", "                        ", "pscore_item_pos_i_l", "=", "pscore_i", "\n", "", "else", ":", "\n", "                        ", "if", "isinstance", "(", "clip_logit_value", ",", "float", ")", ":", "\n", "                            ", "pscores", "=", "self", ".", "_calc_pscore_given_policy_softmax", "(", "\n", "all_slate_actions", "=", "enumerated_slate_actions", ",", "\n", "policy_softmax_i_", "=", "behavior_policy_softmax_", "[", "i", "]", ",", "\n", ")", "\n", "", "else", ":", "\n", "                            ", "pscores", "=", "self", ".", "_calc_pscore_given_policy_logit", "(", "\n", "all_slate_actions", "=", "enumerated_slate_actions", ",", "\n", "policy_logit_i_", "=", "behavior_policy_logit_", "[", "i", "]", ",", "\n", ")", "\n", "", "pscore_item_pos_i_l", "=", "pscores", "[", "\n", "enumerated_slate_actions", "[", ":", ",", "pos_", "]", "==", "sampled_action", "\n", "]", ".", "sum", "(", ")", "\n", "", "pscore_item_position", "[", "i", "*", "self", ".", "len_list", "+", "pos_", "]", "=", "pscore_item_pos_i_l", "\n", "# impute joint pscore", "\n", "", "", "start_idx", "=", "i", "*", "self", ".", "len_list", "\n", "end_idx", "=", "start_idx", "+", "self", ".", "len_list", "\n", "pscore", "[", "start_idx", ":", "end_idx", "]", "=", "pscore_i", "\n", "\n", "", "return", "action", ",", "pscore_cascade", ",", "pscore", ",", "pscore_item_position", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.synthetic_slate.SyntheticSlateBanditDataset.sample_contextfree_expected_reward": [[663, 676], ["sklearn.utils.check_random_state", "sklearn.utils.check_random_state.uniform"], "methods", ["None"], ["", "def", "sample_contextfree_expected_reward", "(", "\n", "self", ",", "random_state", ":", "Optional", "[", "int", "]", "=", "None", "\n", ")", "->", "np", ".", "ndarray", ":", "\n", "        ", "\"\"\"Define context independent expected rewards for each action and slot.\n\n        Parameters\n        -----------\n        random_state: int, default=None\n            Controls the random seed in sampling dataset.\n\n        \"\"\"", "\n", "random_", "=", "check_random_state", "(", "random_state", ")", "\n", "return", "random_", ".", "uniform", "(", "size", "=", "(", "self", ".", "n_unique_action", ",", "self", ".", "len_list", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.synthetic_slate.SyntheticSlateBanditDataset.sample_reward_given_expected_reward": [[677, 728], ["list", "numpy.ones", "numpy.zeros", "numpy.arange", "synthetic_slate.SyntheticSlateBanditDataset.random_.binomial", "list.append", "numpy.array", "numpy.zeros", "numpy.arange", "scipy.stats.truncnorm.rvs"], "methods", ["None"], ["", "def", "sample_reward_given_expected_reward", "(", "\n", "self", ",", "expected_reward_factual", ":", "np", ".", "ndarray", "\n", ")", "->", "np", ".", "ndarray", ":", "\n", "        ", "\"\"\"Sample reward variables given actions observed at each slot.\n\n        Parameters\n        ------------\n        expected_reward_factual: array-like, shape (n_rounds, len_list)\n            Expected rewards given observed actions and contexts.\n\n        Returns\n        ----------\n        reward: array-like, shape (n_rounds, len_list)\n            Sampled rewards.\n\n        \"\"\"", "\n", "expected_reward_factual", "*=", "self", ".", "exam_weight", "\n", "if", "self", ".", "reward_type", "==", "\"binary\"", ":", "\n", "            ", "sampled_reward_list", "=", "list", "(", ")", "\n", "discount_factors", "=", "np", ".", "ones", "(", "expected_reward_factual", ".", "shape", "[", "0", "]", ")", "\n", "sampled_rewards_at_position", "=", "np", ".", "zeros", "(", "expected_reward_factual", ".", "shape", "[", "0", "]", ")", "\n", "for", "pos_", "in", "np", ".", "arange", "(", "self", ".", "len_list", ")", ":", "\n", "                ", "discount_factors", "*=", "sampled_rewards_at_position", "*", "self", ".", "attractiveness", "[", "\n", "pos_", "\n", "]", "+", "(", "1", "-", "sampled_rewards_at_position", ")", "\n", "expected_reward_factual_at_position", "=", "(", "\n", "discount_factors", "*", "expected_reward_factual", "[", ":", ",", "pos_", "]", "\n", ")", "\n", "sampled_rewards_at_position", "=", "self", ".", "random_", ".", "binomial", "(", "\n", "n", "=", "1", ",", "p", "=", "expected_reward_factual_at_position", "\n", ")", "\n", "sampled_reward_list", ".", "append", "(", "sampled_rewards_at_position", ")", "\n", "", "reward", "=", "np", ".", "array", "(", "sampled_reward_list", ")", ".", "T", "\n", "\n", "", "elif", "self", ".", "reward_type", "==", "\"continuous\"", ":", "\n", "            ", "reward", "=", "np", ".", "zeros", "(", "expected_reward_factual", ".", "shape", ")", "\n", "for", "pos_", "in", "np", ".", "arange", "(", "self", ".", "len_list", ")", ":", "\n", "                ", "mean", "=", "expected_reward_factual", "[", ":", ",", "pos_", "]", "\n", "a", "=", "(", "self", ".", "reward_min", "-", "mean", ")", "/", "self", ".", "reward_std", "\n", "b", "=", "(", "self", ".", "reward_max", "-", "mean", ")", "/", "self", ".", "reward_std", "\n", "reward", "[", ":", ",", "pos_", "]", "=", "truncnorm", ".", "rvs", "(", "\n", "a", "=", "a", ",", "\n", "b", "=", "b", ",", "\n", "loc", "=", "mean", ",", "\n", "scale", "=", "self", ".", "reward_std", ",", "\n", "random_state", "=", "self", ".", "random_state", ",", "\n", ")", "\n", "", "", "else", ":", "\n", "            ", "raise", "NotImplementedError", "\n", "# return: array-like, shape (n_rounds, len_list)", "\n", "", "return", "reward", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.synthetic_slate.SyntheticSlateBanditDataset.obtain_batch_bandit_feedback": [[729, 840], ["sklearn.utils.check_scalar", "synthetic_slate.SyntheticSlateBanditDataset.random_.normal", "synthetic_slate.SyntheticSlateBanditDataset.sample_action_and_obtain_pscore", "synthetic_slate.SyntheticSlateBanditDataset.sample_reward_given_expected_reward", "dict", "numpy.tile", "synthetic_slate.SyntheticSlateBanditDataset.behavior_policy_function", "ValueError", "synthetic_slate.SyntheticSlateBanditDataset.sample_contextfree_expected_reward", "numpy.tile", "action.reshape", "synthetic_slate.SyntheticSlateBanditDataset.reward_function", "ValueError", "isinstance", "numpy.array", "isinstance", "numpy.repeat", "numpy.tile", "synthetic_slate.SyntheticSlateBanditDataset.reshape", "synthetic_slate.SyntheticSlateBanditDataset.reshape", "numpy.arange", "numpy.arange", "numpy.arange", "numpy.arange"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.synthetic_slate.SyntheticSlateBanditDataset.sample_action_and_obtain_pscore", "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.synthetic.SyntheticBanditDataset.sample_reward_given_expected_reward", "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.synthetic.SyntheticBanditDataset.sample_contextfree_expected_reward"], ["", "def", "obtain_batch_bandit_feedback", "(", "\n", "self", ",", "\n", "n_rounds", ":", "int", ",", "\n", "return_pscore_item_position", ":", "bool", "=", "True", ",", "\n", "clip_logit_value", ":", "Optional", "[", "float", "]", "=", "None", ",", "\n", ")", "->", "BanditFeedback", ":", "\n", "        ", "\"\"\"Obtain batch logged bandit data.\n\n        Parameters\n        ----------\n        n_rounds: int\n            Data size of the synthetic logged bandit data.\n\n        return_pscore_item_position: bool, default=True\n            Whether to compute `pscore_item_position` and include it in the logged data.\n            When `n_unique_action` and `len_list` are large, this should be set to False due to computation time.\n\n        clip_logit_value: Optional[float], default=None\n            A float parameter to clip logit values.\n            When None, we calculate softmax values without clipping to obtain `pscore_item_position`.\n            When a float value is given, we clip logit values to calculate softmax values to obtain `pscore_item_position`.\n            When `n_actions` and `len_list` are large, `clip_logit_value`=None can lead to a long computation time.\n\n        Returns\n        ---------\n        bandit_feedback: BanditFeedback\n            Synthesized slate logged bandit dataset.\n\n        \"\"\"", "\n", "check_scalar", "(", "n_rounds", ",", "\"n_rounds\"", ",", "int", ",", "min_val", "=", "1", ")", "\n", "context", "=", "self", ".", "random_", ".", "normal", "(", "size", "=", "(", "n_rounds", ",", "self", ".", "dim_context", ")", ")", "\n", "# sample actions for each round based on the behavior policy", "\n", "if", "self", ".", "behavior_policy_function", "is", "None", ":", "\n", "            ", "behavior_policy_logit_", "=", "np", ".", "tile", "(", "\n", "self", ".", "uniform_behavior_policy", ",", "(", "n_rounds", ",", "1", ")", "\n", ")", "\n", "", "else", ":", "\n", "            ", "behavior_policy_logit_", "=", "self", ".", "behavior_policy_function", "(", "\n", "context", "=", "context", ",", "\n", "action_context", "=", "self", ".", "action_context", ",", "\n", "random_state", "=", "self", ".", "random_state", ",", "\n", ")", "\n", "# check the shape of behavior_policy_logit_", "\n", "", "if", "not", "(", "\n", "isinstance", "(", "behavior_policy_logit_", ",", "np", ".", "ndarray", ")", "\n", "and", "behavior_policy_logit_", ".", "shape", "==", "(", "n_rounds", ",", "self", ".", "n_unique_action", ")", "\n", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\"`behavior_policy_logit_` has an invalid shape\"", ")", "\n", "# sample actions and calculate the three variants of the propensity scores", "\n", "", "(", "\n", "action", ",", "\n", "pscore_cascade", ",", "\n", "pscore", ",", "\n", "pscore_item_position", ",", "\n", ")", "=", "self", ".", "sample_action_and_obtain_pscore", "(", "\n", "behavior_policy_logit_", "=", "behavior_policy_logit_", ",", "\n", "n_rounds", "=", "n_rounds", ",", "\n", "return_pscore_item_position", "=", "return_pscore_item_position", ",", "\n", "clip_logit_value", "=", "clip_logit_value", ",", "\n", ")", "\n", "# sample expected reward factual", "\n", "if", "self", ".", "base_reward_function", "is", "None", ":", "\n", "            ", "expected_reward", "=", "self", ".", "sample_contextfree_expected_reward", "(", "\n", "random_state", "=", "self", ".", "random_state", "\n", ")", "\n", "expected_reward_tile", "=", "np", ".", "tile", "(", "expected_reward", ",", "(", "n_rounds", ",", "1", ",", "1", ")", ")", "\n", "# action_2d: array-like, shape (n_rounds, len_list)", "\n", "action_2d", "=", "action", ".", "reshape", "(", "(", "n_rounds", ",", "self", ".", "len_list", ")", ")", "\n", "# expected_reward_factual: array-like, shape (n_rounds, len_list)", "\n", "expected_reward_factual", "=", "np", ".", "array", "(", "\n", "[", "\n", "expected_reward_tile", "[", "np", ".", "arange", "(", "n_rounds", ")", ",", "action_2d", "[", ":", ",", "pos_", "]", ",", "pos_", "]", "\n", "for", "pos_", "in", "np", ".", "arange", "(", "self", ".", "len_list", ")", "\n", "]", "\n", ")", ".", "T", "\n", "", "else", ":", "\n", "            ", "expected_reward_factual", "=", "self", ".", "reward_function", "(", "\n", "context", "=", "context", ",", "\n", "action_context", "=", "self", ".", "action_context", ",", "\n", "action", "=", "action", ",", "\n", "action_interaction_weight_matrix", "=", "self", ".", "action_interaction_weight_matrix", ",", "\n", "base_reward_function", "=", "self", ".", "base_reward_function", ",", "\n", "reward_type", "=", "self", ".", "reward_type", ",", "\n", "reward_structure", "=", "self", ".", "reward_structure", ",", "\n", "len_list", "=", "self", ".", "len_list", ",", "\n", "random_state", "=", "self", ".", "random_state", ",", "\n", ")", "\n", "# check the shape of expected_reward_factual", "\n", "", "if", "not", "(", "\n", "isinstance", "(", "expected_reward_factual", ",", "np", ".", "ndarray", ")", "\n", "and", "expected_reward_factual", ".", "shape", "==", "(", "n_rounds", ",", "self", ".", "len_list", ")", "\n", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\"`expected_reward_factual` has an invalid shape\"", ")", "\n", "# sample reward", "\n", "", "reward", "=", "self", ".", "sample_reward_given_expected_reward", "(", "\n", "expected_reward_factual", "=", "expected_reward_factual", "\n", ")", "\n", "\n", "return", "dict", "(", "\n", "n_rounds", "=", "n_rounds", ",", "\n", "n_unique_action", "=", "self", ".", "n_unique_action", ",", "\n", "slate_id", "=", "np", ".", "repeat", "(", "np", ".", "arange", "(", "n_rounds", ")", ",", "self", ".", "len_list", ")", ",", "\n", "context", "=", "context", ",", "\n", "action_context", "=", "self", ".", "action_context", ",", "\n", "action", "=", "action", ",", "\n", "position", "=", "np", ".", "tile", "(", "np", ".", "arange", "(", "self", ".", "len_list", ")", ",", "n_rounds", ")", ",", "\n", "reward", "=", "reward", ".", "reshape", "(", "action", ".", "shape", "[", "0", "]", ")", ",", "\n", "expected_reward_factual", "=", "expected_reward_factual", ".", "reshape", "(", "action", ".", "shape", "[", "0", "]", ")", ",", "\n", "pscore_cascade", "=", "pscore_cascade", ",", "\n", "pscore", "=", "pscore", ",", "\n", "pscore_item_position", "=", "pscore_item_position", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.synthetic_slate.SyntheticSlateBanditDataset.calc_on_policy_policy_value": [[842, 869], ["utils.check_array", "utils.check_array", "ValueError", "reward.sum", "numpy.unique"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array"], ["", "def", "calc_on_policy_policy_value", "(", "\n", "self", ",", "reward", ":", "np", ".", "ndarray", ",", "slate_id", ":", "np", ".", "ndarray", "\n", ")", "->", "float", ":", "\n", "        ", "\"\"\"Calculate the policy value of given reward and slate_id.\n\n        Parameters\n        -----------\n        reward: array-like, shape (<= n_rounds * len_list,)\n            Slot-level rewards, i.e., :math:`r_{i}(l)`.\n\n        slate_id: array-like, shape (<= n_rounds * len_list,)\n            Slate index.\n\n        Returns\n        ----------\n        policy_value: float\n            The on-policy policy value estimate of the behavior policy.\n\n        \"\"\"", "\n", "check_array", "(", "array", "=", "slate_id", ",", "name", "=", "\"slate_id\"", ",", "expected_dim", "=", "1", ")", "\n", "check_array", "(", "array", "=", "reward", ",", "name", "=", "\"reward\"", ",", "expected_dim", "=", "1", ")", "\n", "if", "reward", ".", "shape", "[", "0", "]", "!=", "slate_id", ".", "shape", "[", "0", "]", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"Expected `reward.shape[0] == slate_id.shape[0]`, but found it False\"", "\n", ")", "\n", "\n", "", "return", "reward", ".", "sum", "(", ")", "/", "np", ".", "unique", "(", "slate_id", ")", ".", "shape", "[", "0", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.synthetic_slate.SyntheticSlateBanditDataset.calc_ground_truth_policy_value": [[870, 1021], ["utils.check_array", "utils.check_array", "numpy.array().astype", "len", "len", "len", "ValueError", "ValueError", "ValueError", "tqdm.tqdm.tqdm", "tqdm.tqdm.tqdm", "numpy.array", "synthetic_slate.SyntheticSlateBanditDataset.sample_contextfree_expected_reward", "numpy.tile", "tqdm.tqdm.tqdm", "numpy.array", "numpy.array.append", "numpy.array", "numpy.arange", "numpy.array.append", "numpy.array", "numpy.arange", "synthetic_slate.SyntheticSlateBanditDataset.reward_function", "itertools.product", "itertools.permutations", "[].prod", "synthetic_slate.SyntheticSlateBanditDataset._calc_pscore_given_policy_logit", "numpy.ones", "numpy.zeros", "numpy.arange", "numpy.arange", "numpy.arange", "expected_slate_rewards.sum", "numpy.array().astype.flatten", "numpy.arange", "pscores_.flatten", "synthetic_slate.SyntheticSlateBanditDataset.sum", "utils.softmax", "numpy.arange", "numpy.array"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.synthetic.SyntheticBanditDataset.sample_contextfree_expected_reward", "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.synthetic_slate.SyntheticSlateBanditDataset._calc_pscore_given_policy_logit", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.softmax"], ["", "def", "calc_ground_truth_policy_value", "(", "\n", "self", ",", "\n", "context", ":", "np", ".", "ndarray", ",", "\n", "evaluation_policy_logit_", ":", "np", ".", "ndarray", ",", "\n", ")", ":", "\n", "        ", "\"\"\"Calculate the ground-truth policy value of given evaluation policy logit and contexts.\n\n        Parameters\n        -----------\n        context: array-like, shape (n_rounds, dim_context)\n            Context vectors characterizing each data (such as user information).\n\n        evaluation_policy_logit_: array-like, shape (n_rounds, n_unique_action)\n            Logit values to define the evaluation policy.\n\n        \"\"\"", "\n", "check_array", "(", "array", "=", "context", ",", "name", "=", "\"context\"", ",", "expected_dim", "=", "2", ")", "\n", "check_array", "(", "\n", "array", "=", "evaluation_policy_logit_", ",", "\n", "name", "=", "\"evaluation_policy_logit_\"", ",", "\n", "expected_dim", "=", "2", ",", "\n", ")", "\n", "if", "evaluation_policy_logit_", ".", "shape", "[", "1", "]", "!=", "self", ".", "n_unique_action", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"Expected `evaluation_policy_logit_.shape[1] != self.n_unique_action`,\"", "\n", "\"but found it False\"", "\n", ")", "\n", "", "if", "context", ".", "shape", "[", "1", "]", "!=", "self", ".", "dim_context", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"Expected `context.shape[1] == self.dim_context`, but found it False\"", "\n", ")", "\n", "", "if", "evaluation_policy_logit_", ".", "shape", "[", "0", "]", "!=", "context", ".", "shape", "[", "0", "]", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"Expected `evaluation_policy_logit_.shape[0] == context.shape[0]`,\"", "\n", "\"but found it False\"", "\n", ")", "\n", "\n", "", "if", "self", ".", "is_factorizable", ":", "\n", "            ", "enumerated_slate_actions", "=", "[", "\n", "_", "\n", "for", "_", "in", "product", "(", "np", ".", "arange", "(", "self", ".", "n_unique_action", ")", ",", "repeat", "=", "self", ".", "len_list", ")", "\n", "]", "\n", "", "else", ":", "\n", "            ", "enumerated_slate_actions", "=", "[", "\n", "_", "for", "_", "in", "permutations", "(", "np", ".", "arange", "(", "self", ".", "n_unique_action", ")", ",", "self", ".", "len_list", ")", "\n", "]", "\n", "", "enumerated_slate_actions", "=", "np", ".", "array", "(", "enumerated_slate_actions", ")", ".", "astype", "(", "\"int8\"", ")", "\n", "n_slate_actions", "=", "len", "(", "enumerated_slate_actions", ")", "\n", "n_rounds", "=", "len", "(", "evaluation_policy_logit_", ")", "\n", "\n", "pscores", "=", "[", "]", "\n", "n_enumerated_slate_actions", "=", "len", "(", "enumerated_slate_actions", ")", "\n", "if", "self", ".", "is_factorizable", ":", "\n", "            ", "for", "action_list", "in", "tqdm", "(", "\n", "enumerated_slate_actions", ",", "\n", "desc", "=", "\"[calc_ground_truth_policy_value (pscore)]\"", ",", "\n", "total", "=", "n_enumerated_slate_actions", ",", "\n", ")", ":", "\n", "                ", "pscores", ".", "append", "(", "\n", "softmax", "(", "evaluation_policy_logit_", ")", "[", ":", ",", "action_list", "]", ".", "prod", "(", "1", ")", "\n", ")", "\n", "", "pscores", "=", "np", ".", "array", "(", "pscores", ")", ".", "T", "\n", "", "else", ":", "\n", "            ", "for", "i", "in", "tqdm", "(", "\n", "np", ".", "arange", "(", "n_rounds", ")", ",", "\n", "desc", "=", "\"[calc_ground_truth_policy_value (pscore)]\"", ",", "\n", "total", "=", "n_rounds", ",", "\n", ")", ":", "\n", "                ", "pscores", ".", "append", "(", "\n", "self", ".", "_calc_pscore_given_policy_logit", "(", "\n", "all_slate_actions", "=", "enumerated_slate_actions", ",", "\n", "policy_logit_i_", "=", "evaluation_policy_logit_", "[", "i", "]", ",", "\n", ")", "\n", ")", "\n", "", "pscores", "=", "np", ".", "array", "(", "pscores", ")", "\n", "\n", "# calculate expected slate-level reward for each combinatorial set of items (i.e., slate actions)", "\n", "", "if", "self", ".", "base_reward_function", "is", "None", ":", "\n", "            ", "expected_slot_reward", "=", "self", ".", "sample_contextfree_expected_reward", "(", "\n", "random_state", "=", "self", ".", "random_state", "\n", ")", "\n", "expected_slot_reward_tile", "=", "np", ".", "tile", "(", "\n", "expected_slot_reward", ",", "(", "n_rounds", "*", "n_slate_actions", ",", "1", ",", "1", ")", "\n", ")", "\n", "expected_slate_rewards", "=", "np", ".", "array", "(", "\n", "[", "\n", "expected_slot_reward_tile", "[", "\n", "np", ".", "arange", "(", "n_slate_actions", ")", "%", "n_slate_actions", ",", "\n", "np", ".", "array", "(", "enumerated_slate_actions", ")", "[", ":", ",", "pos_", "]", ",", "\n", "pos_", ",", "\n", "]", "\n", "for", "pos_", "in", "np", ".", "arange", "(", "self", ".", "len_list", ")", "\n", "]", "\n", ")", ".", "T", "\n", "policy_value", "=", "(", "pscores", "*", "expected_slate_rewards", ".", "sum", "(", "axis", "=", "1", ")", ")", ".", "sum", "(", ")", "\n", "", "else", ":", "\n", "            ", "n_batch", "=", "(", "\n", "n_rounds", "*", "n_enumerated_slate_actions", "*", "self", ".", "len_list", "-", "1", "\n", ")", "//", "10", "**", "7", "+", "1", "\n", "batch_size", "=", "(", "n_rounds", "-", "1", ")", "//", "n_batch", "+", "1", "\n", "n_batch", "=", "(", "n_rounds", "-", "1", ")", "//", "batch_size", "+", "1", "\n", "\n", "policy_value", "=", "0.0", "\n", "for", "batch_idx", "in", "tqdm", "(", "\n", "np", ".", "arange", "(", "n_batch", ")", ",", "\n", "desc", "=", "f\"[calc_ground_truth_policy_value (expected reward), batch_size={batch_size}]\"", ",", "\n", "total", "=", "n_batch", ",", "\n", ")", ":", "\n", "                ", "context_", "=", "context", "[", "\n", "batch_idx", "*", "batch_size", ":", "(", "batch_idx", "+", "1", ")", "*", "batch_size", "\n", "]", "\n", "pscores_", "=", "pscores", "[", "\n", "batch_idx", "*", "batch_size", ":", "(", "batch_idx", "+", "1", ")", "*", "batch_size", "\n", "]", "\n", "\n", "expected_slate_rewards_", "=", "self", ".", "reward_function", "(", "\n", "context", "=", "context_", ",", "\n", "action_context", "=", "self", ".", "action_context", ",", "\n", "action", "=", "enumerated_slate_actions", ".", "flatten", "(", ")", ",", "\n", "action_interaction_weight_matrix", "=", "self", ".", "action_interaction_weight_matrix", ",", "\n", "base_reward_function", "=", "self", ".", "base_reward_function", ",", "\n", "reward_type", "=", "self", ".", "reward_type", ",", "\n", "reward_structure", "=", "self", ".", "reward_structure", ",", "\n", "len_list", "=", "self", ".", "len_list", ",", "\n", "is_enumerated", "=", "True", ",", "\n", "random_state", "=", "self", ".", "random_state", ",", "\n", ")", "\n", "\n", "# click models based on expected reward", "\n", "expected_slate_rewards_", "*=", "self", ".", "exam_weight", "\n", "if", "self", ".", "reward_type", "==", "\"binary\"", ":", "\n", "                    ", "discount_factors", "=", "np", ".", "ones", "(", "expected_slate_rewards_", ".", "shape", "[", "0", "]", ")", "\n", "previous_slot_expected_reward", "=", "np", ".", "zeros", "(", "\n", "expected_slate_rewards_", ".", "shape", "[", "0", "]", "\n", ")", "\n", "for", "pos_", "in", "np", ".", "arange", "(", "self", ".", "len_list", ")", ":", "\n", "                        ", "discount_factors", "*=", "(", "\n", "previous_slot_expected_reward", "*", "self", ".", "attractiveness", "[", "pos_", "]", "\n", "+", "(", "1", "-", "previous_slot_expected_reward", ")", "\n", ")", "\n", "expected_slate_rewards_", "[", ":", ",", "pos_", "]", "=", "(", "\n", "discount_factors", "*", "expected_slate_rewards_", "[", ":", ",", "pos_", "]", "\n", ")", "\n", "previous_slot_expected_reward", "=", "expected_slate_rewards_", "[", ":", ",", "pos_", "]", "\n", "\n", "", "", "policy_value", "+=", "(", "\n", "pscores_", ".", "flatten", "(", ")", "*", "expected_slate_rewards_", ".", "sum", "(", "axis", "=", "1", ")", "\n", ")", ".", "sum", "(", ")", "\n", "", "policy_value", "/=", "n_rounds", "\n", "\n", "", "return", "policy_value", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.synthetic_slate.SyntheticSlateBanditDataset.generate_evaluation_policy_pscore": [[1022, 1147], ["utils.check_array", "ValueError", "numpy.ones", "synthetic_slate.SyntheticSlateBanditDataset.base_reward_function", "utils.check_array", "action.reshape", "sklearn.utils.check_scalar", "synthetic_slate.SyntheticSlateBanditDataset._calc_epsilon_greedy_pscore", "numpy.ones", "numpy.tile().cumprod().flatten", "numpy.ones", "scipy.special.perm", "ValueError", "ValueError", "synthetic_slate.SyntheticSlateBanditDataset.argsort", "synthetic_slate.SyntheticSlateBanditDataset.argsort", "numpy.tile().cumprod", "numpy.ones", "numpy.tile", "numpy.arange"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.synthetic_slate.SyntheticSlateBanditDataset._calc_epsilon_greedy_pscore"], ["", "def", "generate_evaluation_policy_pscore", "(", "\n", "self", ",", "\n", "evaluation_policy_type", ":", "str", ",", "\n", "context", ":", "np", ".", "ndarray", ",", "\n", "action", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "epsilon", ":", "Optional", "[", "float", "]", "=", "1.0", ",", "\n", ")", "->", "Tuple", "[", "np", ".", "ndarray", ",", "np", ".", "ndarray", ",", "np", ".", "ndarray", "]", ":", "\n", "        ", "\"\"\"Generate three variants of propensity scores of synthetic evaluation policies.\n\n        Parameters\n        -----------\n        evaluation_policy_type: str\n            Specify the type of evaluation policy to generate, which must be one of 'optimal', 'anti-optimal', or 'random'.\n            When 'optimal' is given, we sort actions based on the base expected rewards (outputs of `base_reward_function`) and extract top-L actions (L=`len_list`) for each slate.\n            When 'anti-optimal' is given, we sort actions based on the base expected rewards (outputs of `base_reward_function`) and extract bottom-L actions (L=`len_list`) for each slate.\n            We calculate the three variants of the propensity scores (pscore, `pscore_item_position`, and pscore_cascade) of the epsilon-greedy policy when either 'optimal' or 'anti-optimal' is given.\n            When 'random' is given, we calculate the three variants of the propensity scores of the uniform random policy.\n\n        context: array-like, shape (n_rounds, dim_context)\n            Context vectors characterizing each data (such as user information).\n\n        action: array-like, shape (n_rounds * len_list,), default=None\n            Actions sampled by the behavior policy.\n            Actions sampled within slate `i` is stored in `action[`i` * `len_list`: (`i + 1`) * `len_list`]`.\n            When `evaluation_policy_type`='random', this argument is irrelevant.\n\n        epsilon: float, default=1.\n            Exploration hyperparameter that must take value in the range of [0., 1.].\n            When `evaluation_policy_type`='random', this argument is irrelevant.\n\n        Returns\n        ----------\n        pscore: array-like, shape (n_unique_action * len_list)\n            Probabilities of choosing the slate actions given context (:math:`x`),\n            i.e., :math:`\\\\pi(a_{i,1}, a_{i,2}, \\\\ldots, a_{i,L} | x_{i} )`.\n\n        pscore_item_position: array-like, shape (n_unique_action * len_list)\n            Probabilities of choosing the action of the :math:`l`-th slot given context (:math:`x`),\n            i.e., :math:`\\\\pi(a_{i,l} | x_{i} )`.\n\n        pscore_cascade: array-like, shape (n_unique_action * len_list)\n            Probabilities of choosing the actions of the top :math:`l` slots given context (:math:`x`),\n            i.e., :math:`\\\\pi(a_{i,1}, a_{i,2}, \\\\ldots, a_{i,l} | x_{i} )`.\n\n        \"\"\"", "\n", "check_array", "(", "array", "=", "context", ",", "name", "=", "\"context\"", ",", "expected_dim", "=", "2", ")", "\n", "if", "evaluation_policy_type", "not", "in", "[", "\"optimal\"", ",", "\"anti-optimal\"", ",", "\"random\"", "]", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "f\"`evaluation_policy_type` must be 'optimal', 'anti-optimal', or 'random', but {evaluation_policy_type} is given\"", "\n", ")", "\n", "\n", "# [Caution]: OverflowError raises when integer division result is too large for a float", "\n", "", "if", "self", ".", "is_factorizable", ":", "\n", "            ", "random_pscore_cascade", "=", "(", "\n", "(", "np", ".", "ones", "(", "(", "context", ".", "shape", "[", "0", "]", ",", "self", ".", "len_list", ")", ")", "/", "self", ".", "n_unique_action", ")", "\n", ".", "cumprod", "(", "axis", "=", "1", ")", "\n", ".", "flatten", "(", ")", "\n", ")", "\n", "random_pscore", "=", "np", ".", "ones", "(", "context", ".", "shape", "[", "0", "]", "*", "self", ".", "len_list", ")", "/", "(", "\n", "self", ".", "n_unique_action", "**", "self", ".", "len_list", "\n", ")", "\n", "", "else", ":", "\n", "            ", "random_pscore_cascade", "=", "(", "\n", "1.0", "\n", "/", "np", ".", "tile", "(", "\n", "np", ".", "arange", "(", "\n", "self", ".", "n_unique_action", ",", "self", ".", "n_unique_action", "-", "self", ".", "len_list", ",", "-", "1", "\n", ")", ",", "\n", "(", "context", ".", "shape", "[", "0", "]", ",", "1", ")", ",", "\n", ")", "\n", ".", "cumprod", "(", "axis", "=", "1", ")", "\n", ".", "flatten", "(", ")", "\n", ")", "\n", "random_pscore", "=", "np", ".", "ones", "(", "context", ".", "shape", "[", "0", "]", "*", "self", ".", "len_list", ")", "/", "perm", "(", "\n", "self", ".", "n_unique_action", ",", "self", ".", "len_list", "\n", ")", "\n", "", "random_pscore_item_position", "=", "(", "\n", "np", ".", "ones", "(", "context", ".", "shape", "[", "0", "]", "*", "self", ".", "len_list", ")", "/", "self", ".", "n_unique_action", "\n", ")", "\n", "if", "evaluation_policy_type", "==", "\"random\"", ":", "\n", "            ", "return", "random_pscore", ",", "random_pscore_item_position", ",", "random_pscore_cascade", "\n", "\n", "", "else", ":", "\n", "# base_expected_reward: array-like, shape (n_rounds, n_unique_action)", "\n", "            ", "base_expected_reward", "=", "self", ".", "base_reward_function", "(", "\n", "context", "=", "context", ",", "\n", "action_context", "=", "self", ".", "action_context", ",", "\n", "random_state", "=", "self", ".", "random_state", ",", "\n", ")", "\n", "check_array", "(", "array", "=", "action", ",", "name", "=", "\"action\"", ",", "expected_dim", "=", "1", ")", "\n", "if", "action", ".", "shape", "[", "0", "]", "!=", "context", ".", "shape", "[", "0", "]", "*", "self", ".", "len_list", ":", "\n", "                ", "raise", "ValueError", "(", "\n", "\"Expected `action.shape[0] == context.shape[0] * self.len_list`,\"", "\n", "\"but found it False\"", "\n", ")", "\n", "", "action_2d", "=", "action", ".", "reshape", "(", "(", "context", ".", "shape", "[", "0", "]", ",", "self", ".", "len_list", ")", ")", "\n", "if", "context", ".", "shape", "[", "0", "]", "!=", "action_2d", ".", "shape", "[", "0", "]", ":", "\n", "                ", "raise", "ValueError", "(", "\n", "\"Expected `context.shape[0] == action_2d.shape[0]`, but found it False\"", "\n", ")", "\n", "\n", "", "check_scalar", "(", "\n", "epsilon", ",", "name", "=", "\"epsilon\"", ",", "target_type", "=", "(", "float", ")", ",", "min_val", "=", "0.0", ",", "max_val", "=", "1.0", "\n", ")", "\n", "if", "evaluation_policy_type", "==", "\"optimal\"", ":", "\n", "                ", "sorted_actions", "=", "base_expected_reward", ".", "argsort", "(", "axis", "=", "1", ")", "[", "\n", ":", ",", ":", "self", ".", "len_list", "\n", "]", "\n", "", "else", ":", "\n", "                ", "sorted_actions", "=", "base_expected_reward", ".", "argsort", "(", "axis", "=", "1", ")", "[", "\n", ":", ",", "-", "self", ".", "len_list", ":", "\n", "]", "\n", "", "(", "\n", "pscore", ",", "\n", "pscore_item_position", ",", "\n", "pscore_cascade", ",", "\n", ")", "=", "self", ".", "_calc_epsilon_greedy_pscore", "(", "\n", "epsilon", "=", "epsilon", ",", "\n", "action_2d", "=", "action_2d", ",", "\n", "sorted_actions", "=", "sorted_actions", ",", "\n", "random_pscore", "=", "random_pscore", ",", "\n", "random_pscore_item_position", "=", "random_pscore_item_position", ",", "\n", "random_pscore_cascade", "=", "random_pscore_cascade", ",", "\n", ")", "\n", "", "return", "pscore", ",", "pscore_item_position", ",", "pscore_cascade", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.synthetic_slate.SyntheticSlateBanditDataset.calc_evaluation_policy_action_dist": [[1148, 1212], ["utils.check_array", "utils.check_array", "action.reshape.reshape.reshape", "numpy.array", "range", "numpy.array().flatten", "ValueError", "len", "ValueError", "numpy.array().flatten.append", "range", "utils.softmax", "numpy.array", "range", "range"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.softmax"], ["", "def", "calc_evaluation_policy_action_dist", "(", "\n", "self", ",", "\n", "action", ":", "np", ".", "ndarray", ",", "\n", "evaluation_policy_logit_", ":", "np", ".", "ndarray", ",", "\n", ")", ":", "\n", "        ", "\"\"\"Calculate action distribution at each slot from a given evaluation policy logit.\n\n        Parameters\n        ----------\n        action: array-like, shape (n_rounds * len_list, )\n            Action chosen by behavior policy.\n\n        evaluation_policy_logit_: array-like, shape (n_rounds, n_unique_action)\n            Logit values of evaluation policy given context (:math:`x`), i.e., :math:`\\\\f: \\\\mathcal{X} \\\\rightarrow \\\\mathbb{R}^{\\\\mathcal{A}}`.\n\n        Returns\n        ----------\n        evaluation_policy_action_dist: array-like, shape (n_rounds * len_list * n_unique_action, )\n            Plackett-luce style action distribution induced by evaluation policy\n            (action choice probabilities at each slot given previous action choices)\n            , i.e., :math:`\\\\pi_e(a_i(l) | x_i, a_i(1), \\\\ldots, a_i(l-1)) \\\\forall a_i(l) \\\\in \\\\mathcal{A}`.\n\n        \"\"\"", "\n", "check_array", "(", "action", ",", "name", "=", "\"action\"", ",", "expected_dim", "=", "1", ")", "\n", "check_array", "(", "\n", "evaluation_policy_logit_", ",", "name", "=", "\"evaluation_policy_logit_\"", ",", "expected_dim", "=", "2", "\n", ")", "\n", "if", "evaluation_policy_logit_", ".", "shape", "[", "1", "]", "!=", "self", ".", "n_unique_action", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"Expected `evaluation_policy_logit_.shape[1] == n_unique_action`, but found it False\"", "\n", ")", "\n", "", "if", "len", "(", "action", ")", "!=", "evaluation_policy_logit_", ".", "shape", "[", "0", "]", "*", "self", ".", "len_list", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"Expected `len(action) == evaluation_policy_logit_.shape[0] * len_list`, but found it False\"", "\n", ")", "\n", "", "n_rounds", "=", "evaluation_policy_logit_", ".", "shape", "[", "0", "]", "\n", "\n", "# (n_rounds * len_list, ) -> (n_rounds, len_list)", "\n", "action", "=", "action", ".", "reshape", "(", "(", "n_rounds", ",", "self", ".", "len_list", ")", ")", "\n", "# (n_rounds, n_unique_action) -> (n_rounds, len_list, n_unique_action)", "\n", "evaluation_policy_logit_", "=", "np", ".", "array", "(", "\n", "[", "\n", "[", "evaluation_policy_logit_", "[", "i", "]", "for", "_", "in", "range", "(", "self", ".", "len_list", ")", "]", "\n", "for", "i", "in", "range", "(", "n_rounds", ")", "\n", "]", "\n", ")", "\n", "# calculate action probabilities for all the counterfactual actions at the position", "\n", "# (n_rounds, len_list, n_unique_action)", "\n", "evaluation_policy_action_dist", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "n_rounds", ")", ":", "\n", "            ", "if", "not", "self", ".", "is_factorizable", ":", "\n", "                ", "for", "pos_", "in", "range", "(", "self", ".", "len_list", "-", "1", ")", ":", "\n", "                    ", "action_", "=", "action", "[", "i", "]", "[", "pos_", "]", "\n", "# mask action choice probability of the previously chosen action", "\n", "# to avoid overflow in softmax function, set -1e4 instead of -np.inf", "\n", "# (make action choice probability 0 for the previously chosen action by softmax)", "\n", "evaluation_policy_logit_", "[", "i", ",", "pos_", "+", "1", ":", ",", "action_", "]", "=", "-", "1e4", "\n", "# (len_list, n_unique_action)", "\n", "", "", "evaluation_policy_action_dist", ".", "append", "(", "softmax", "(", "evaluation_policy_logit_", "[", "i", "]", ")", ")", "\n", "# (n_rounds, len_list, n_unique_action) -> (n_rounds * len_list * n_unique_action, )", "\n", "", "evaluation_policy_action_dist", "=", "np", ".", "array", "(", "\n", "evaluation_policy_action_dist", "\n", ")", ".", "flatten", "(", ")", "\n", "return", "evaluation_policy_action_dist", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.synthetic_slate.SyntheticSlateBanditDataset._calc_epsilon_greedy_pscore": [[1213, 1287], ["utils.check_array", "numpy.repeat", "action_match_flg.flatten", "action_match_flg.cumprod().flatten", "ValueError", "action_match_flg.all", "set", "set", "action_match_flg.cumprod", "numpy.tile", "numpy.unique"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array"], ["", "def", "_calc_epsilon_greedy_pscore", "(", "\n", "self", ",", "\n", "epsilon", ":", "float", ",", "\n", "action_2d", ":", "np", ".", "ndarray", ",", "\n", "sorted_actions", ":", "np", ".", "ndarray", ",", "\n", "random_pscore", ":", "np", ".", "ndarray", ",", "\n", "random_pscore_item_position", ":", "np", ".", "ndarray", ",", "\n", "random_pscore_cascade", ":", "np", ".", "ndarray", ",", "\n", ")", "->", "Tuple", "[", "np", ".", "ndarray", ",", "np", ".", "ndarray", ",", "np", ".", "ndarray", "]", ":", "\n", "        ", "\"\"\"Calculate three variants of the propensity scores of synthetic evaluation policies via epsilon-greedy.\n\n        Parameters\n        -----------\n        epsilon: float, default=1.\n            Exploration hyperparameter in the epsilon-greedy rule.\n            Must take value in the range of [0., 1.].\n\n        action_2d: array-like, shape (n_rounds, len_list), default=None\n            Actions sampled by the behavior policy.\n            Actions sampled within slate `i` is stored in `action[i]`.\n            When bandit_feedback is obtained by `obtain_batch_bandit_feedback`, we can obtain action_2d as follows: bandit_feedback[\"action\"].reshape((n_rounds, len_list))\n            When `evaluation_policy_type`='random', this argument is unnecessary.\n\n        random_pscore: array-like, shape (n_unique_action * len_list, )\n            Probabilities of the uniform random policy choosing the slate actions given context (:math:`x`),\n            i.e., :math:`\\\\pi_{unif} (a_{i,1}, a_{i,2}, \\\\ldots, a_{i,L} | x_{i} )`.\n\n        random_pscore_item_position: array-like, shape (n_unique_action * len_list, )\n            Probabilities of the uniform random policy choosing the action of the :math:`l`-th slot given context (:math:`x`), i.e., :math:`\\\\pi_{unif}(a_{i,l} | x_{i} )`.\n\n        random_pscore_cascade: array-like, shape (n_unique_action * len_list, )\n            Probabilities of the uniform random policy choosing the actions of the top :math:`l` slots given context (:math:`x`), i.e., :math:`\\\\pi_{unif}(a_{i,1}, a_{i,2}, \\\\ldots, a_{i,l} | x_{i} )`.\n\n        Returns\n        ----------\n        pscore: array-like, shape (n_unique_action * len_list)\n            Probabilities of choosing the slate actions given context (:math:`x`),\n            i.e., :math:`\\\\pi(a_{i,1}, a_{i,2}, \\\\ldots, a_{i,L} | x_{i} )`.\n\n        pscore_item_position: array-like, shape (n_unique_action * len_list)\n            Probabilities of choosing the action of the :math:`l`-th slot given context (:math:`x`),\n            i.e., :math:`\\\\pi(a_{i,l} | x_{i} )`.\n\n        pscore_cascade: array-like, shape (n_unique_action * len_list)\n            Probabilities of choosing the actions of the top :math:`l` slots given context (:math:`x`),\n            i.e., :math:`\\\\pi(a_{i,1}, a_{i,2}, \\\\ldots, a_{i,l} | x_{i} )`.\n\n        \"\"\"", "\n", "check_array", "(", "array", "=", "action_2d", ",", "name", "=", "\"action_2d\"", ",", "expected_dim", "=", "2", ")", "\n", "if", "not", "self", ".", "is_factorizable", "and", "set", "(", "\n", "[", "np", ".", "unique", "(", "x", ")", ".", "shape", "[", "0", "]", "for", "x", "in", "action_2d", "]", "\n", ")", "!=", "set", "(", "[", "self", ".", "len_list", "]", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"when `is_factorizable`=False, actions observed within each slate must be unique\"", "\n", ")", "\n", "", "if", "self", ".", "is_factorizable", ":", "\n", "            ", "action_match_flg", "=", "(", "\n", "np", ".", "tile", "(", "sorted_actions", "[", ":", ",", "0", "]", ",", "(", "action_2d", ".", "shape", "[", "1", "]", ",", "1", ")", ")", ".", "T", "==", "action_2d", "\n", ")", "\n", "", "else", ":", "\n", "            ", "action_match_flg", "=", "sorted_actions", "==", "action_2d", "\n", "", "pscore_flg", "=", "np", ".", "repeat", "(", "action_match_flg", ".", "all", "(", "axis", "=", "1", ")", ",", "self", ".", "len_list", ")", "\n", "pscore_item_position_flg", "=", "action_match_flg", ".", "flatten", "(", ")", "\n", "pscore_cascade_flg", "=", "action_match_flg", ".", "cumprod", "(", "axis", "=", "1", ")", ".", "flatten", "(", ")", "\n", "# calculate the three variants of the propensity scores based on the given epsilon value", "\n", "pscore", "=", "pscore_flg", "*", "(", "1", "-", "epsilon", ")", "+", "epsilon", "*", "random_pscore", "\n", "pscore_item_position", "=", "(", "\n", "pscore_item_position_flg", "*", "(", "1", "-", "epsilon", ")", "\n", "+", "epsilon", "*", "random_pscore_item_position", "\n", ")", "\n", "pscore_cascade", "=", "(", "\n", "pscore_cascade_flg", "*", "(", "1", "-", "epsilon", ")", "+", "epsilon", "*", "random_pscore_cascade", "\n", ")", "\n", "return", "pscore", ",", "pscore_item_position", ",", "pscore_cascade", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.synthetic_slate.generate_symmetric_matrix": [[1289, 1311], ["sklearn.utils.check_random_state", "sklearn.utils.check_random_state.normal", "numpy.diag", "numpy.tril", "random_.normal.diagonal", "numpy.tril"], "function", ["None"], ["", "", "def", "generate_symmetric_matrix", "(", "n_unique_action", ":", "int", ",", "random_state", ":", "int", ")", "->", "np", ".", "ndarray", ":", "\n", "    ", "\"\"\"Generate symmetric matrix\n\n    Parameters\n    -----------\n    n_unique_action: int (>= len_list)\n        Number of unique actions.\n\n    random_state: int\n        Controls the random seed in sampling elements of matrix.\n\n    Returns\n    ---------\n    symmetric_matrix: array-like, shape (n_unique_action, n_unique_action)\n\n    \"\"\"", "\n", "random_", "=", "check_random_state", "(", "random_state", ")", "\n", "base_matrix", "=", "random_", ".", "normal", "(", "scale", "=", "5", ",", "size", "=", "(", "n_unique_action", ",", "n_unique_action", ")", ")", "\n", "symmetric_matrix", "=", "(", "\n", "np", ".", "tril", "(", "base_matrix", ")", "+", "np", ".", "tril", "(", "base_matrix", ")", ".", "T", "-", "np", ".", "diag", "(", "base_matrix", ".", "diagonal", "(", ")", ")", "\n", ")", "\n", "return", "symmetric_matrix", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.synthetic_slate.action_interaction_reward_function": [[1313, 1480], ["utils.check_array", "utils.check_array", "utils.check_array", "np.tile.reshape().astype", "base_reward_function", "numpy.zeros_like", "numpy.arange", "ValueError", "ValueError", "ValueError", "ValueError", "numpy.tile", "len", "scipy.special.logit", "utils.sigmoid", "numpy.clip", "ValueError", "ValueError", "np.tile.reshape", "numpy.arange", "numpy.arange", "numpy.arange", "len", "numpy.arange", "len"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.sigmoid"], ["", "def", "action_interaction_reward_function", "(", "\n", "context", ":", "np", ".", "ndarray", ",", "\n", "action_context", ":", "np", ".", "ndarray", ",", "\n", "action", ":", "np", ".", "ndarray", ",", "\n", "base_reward_function", ":", "Callable", "[", "[", "np", ".", "ndarray", ",", "np", ".", "ndarray", "]", ",", "np", ".", "ndarray", "]", ",", "\n", "reward_type", ":", "str", ",", "\n", "reward_structure", ":", "str", ",", "\n", "action_interaction_weight_matrix", ":", "np", ".", "ndarray", ",", "\n", "len_list", ":", "int", ",", "\n", "is_enumerated", ":", "bool", "=", "False", ",", "\n", "random_state", ":", "Optional", "[", "int", "]", "=", "None", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "np", ".", "ndarray", ":", "\n", "    ", "\"\"\"Reward function incorporating interactions among combinatorial action\n\n    Parameters\n    -----------\n    context: array-like, shape (n_rounds, dim_context)\n        Context vectors characterizing each data (such as user information).\n\n    action_context: array-like, shape (n_unique_action, dim_action_context)\n        Vector representation of actions.\n\n    action: array-like, shape (n_rounds * len_list, ) or (len(enumerated_slate_actions) * len_list, )\n        When `is_enumerated`=False, action corresponds to actions sampled by a (often behavior) policy.\n        In this case, actions sampled within slate `i` is stored in `action[`i` * `len_list`: (`i + 1`) * `len_list`]`.\n        When `is_enumerated`=True, action corresponds to the enumerated all possible combinatorial actions.\n\n    base_reward_function: Callable[[np.ndarray, np.ndarray], np.ndarray]], default=None\n        Function to define the expected reward, i.e., :math:`q: \\\\mathcal{X} \\\\times \\\\mathcal{A} \\\\rightarrow \\\\mathbb{R}`.\n\n    reward_type: str, default='binary'\n        Type of reward variable, which must be either 'binary' or 'continuous'.\n        When 'binary',the expected rewards are transformed by logit function.\n\n    reward_structure: str\n        Reward structure.\n        Must be one of 'standard_additive', 'cascade_additive', 'standard_decay', or 'cascade_decay'.\n\n    action_interaction_weight_matrix (`W`): array-like, shape (n_unique_action, n_unique_action) or (len_list, len_list)\n        When using an additive-type reward_structure, `W(i, j)` defines the interaction between action `i` and `j`.\n        When using an decay-type reward_structure, `W(i, j)` defines the weight of how the expected reward of slot `i` affects that of slot `j`.\n        See the experiment section of Kiyohara et al.(2022) for details.\n\n    len_list: int (> 1)\n        Length of a list/ranking of actions, slate size.\n\n    is_enumerate: bool\n        Whether `action` corresponds to `enumerated_slate_actions`.\n\n    random_state: int, default=None\n        Controls the random seed in sampling dataset.\n\n    Returns\n    ---------\n    expected_reward_factual: array-like, shape (n_rounds, len_list)\n        When reward_structure='standard_additive', :math:`q_k(x, a) = g(g^{-1}(f(x, a(k))) + \\\\sum_{j \\\\neq k} W(a(k), a(j)))`.\n        When reward_structure='cascade_additive', :math:`q_k(x, a) = g(g^{-1}(f(x, a(k))) + \\\\sum_{j < k} W(a(k), a(j)))`.\n        Otherwise, :math:`q_k(x, a) = g(g^{-1}(f(x, a(k))) + \\\\sum_{j \\\\neq k} g^{-1}(f(x, a(j))) * W(k, j)`\n\n    \"\"\"", "\n", "check_array", "(", "array", "=", "context", ",", "name", "=", "\"context\"", ",", "expected_dim", "=", "2", ")", "\n", "check_array", "(", "array", "=", "action_context", ",", "name", "=", "\"action_context\"", ",", "expected_dim", "=", "2", ")", "\n", "check_array", "(", "array", "=", "action", ",", "name", "=", "\"action\"", ",", "expected_dim", "=", "1", ")", "\n", "if", "is_enumerated", "and", "action", ".", "shape", "[", "0", "]", "%", "len_list", "!=", "0", ":", "\n", "        ", "raise", "ValueError", "(", "\n", "\"Expected `action.shape[0] % len_list == 0` if `is_enumerated is True`,\"", "\n", "\"but found it False\"", "\n", ")", "\n", "", "if", "not", "is_enumerated", "and", "action", ".", "shape", "[", "0", "]", "!=", "len_list", "*", "context", ".", "shape", "[", "0", "]", ":", "\n", "        ", "raise", "ValueError", "(", "\n", "\"Expected `action.shape[0] == len_list * context.shape[0]` if `is_enumerated is False`, but found it False\"", "\n", ")", "\n", "", "if", "reward_type", "not", "in", "[", "\n", "\"binary\"", ",", "\n", "\"continuous\"", ",", "\n", "]", ":", "\n", "        ", "raise", "ValueError", "(", "\n", "f\"`reward_type` must be either 'binary' or 'continuous', but {reward_type} is given.\"", "\n", ")", "\n", "", "if", "reward_structure", "not", "in", "[", "\n", "\"standard_additive\"", ",", "\n", "\"cascade_additive\"", ",", "\n", "\"standard_decay\"", ",", "\n", "\"cascade_decay\"", ",", "\n", "\"independent\"", ",", "\n", "]", ":", "\n", "        ", "raise", "ValueError", "(", "\n", "f\"`reward_structure` must be either 'standard_additive', 'cascade_additive', 'standard_decay' or 'cascade_decay', but {reward_structure} is given.\"", "\n", ")", "\n", "\n", "", "is_additive", "=", "reward_structure", "in", "[", "\"standard_additive\"", ",", "\"cascade_additive\"", "]", "\n", "is_cascade", "=", "reward_structure", "in", "[", "\"cascade_additive\"", ",", "\"cascade_decay\"", "]", "\n", "\n", "if", "is_additive", ":", "\n", "        ", "if", "action_interaction_weight_matrix", ".", "shape", "!=", "(", "\n", "action_context", ".", "shape", "[", "0", "]", ",", "\n", "action_context", ".", "shape", "[", "0", "]", ",", "\n", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "f\"the shape of `action_interaction_weight_matrix` must be `(action_context.shape[0], action_context.shape[0])`, but {action_interaction_weight_matrix.shape}\"", "\n", ")", "\n", "", "", "else", ":", "# decay", "\n", "        ", "if", "action_interaction_weight_matrix", ".", "shape", "!=", "(", "\n", "len_list", ",", "\n", "len_list", ",", "\n", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "f\"the shape of `action_interaction_weight_matrix` must be `(len_list, len_list)`, but {action_interaction_weight_matrix.shape}\"", "\n", ")", "\n", "\n", "", "", "n_rounds", "=", "context", ".", "shape", "[", "0", "]", "\n", "# duplicate action", "\n", "if", "is_enumerated", ":", "\n", "        ", "action", "=", "np", ".", "tile", "(", "action", ",", "n_rounds", ")", "\n", "# action_2d: array-like, shape (n_rounds (* len(enumerated_action_list)), len_list)", "\n", "", "action_2d", "=", "action", ".", "reshape", "(", "(", "-", "1", ",", "len_list", ")", ")", ".", "astype", "(", "\"int8\"", ")", "\n", "n_enumerated_slate_actions", "=", "len", "(", "action", ")", "//", "n_rounds", "\n", "# expected_reward: array-like, shape (n_rounds, n_unique_action)", "\n", "expected_reward", "=", "base_reward_function", "(", "\n", "context", "=", "context", ",", "action_context", "=", "action_context", ",", "random_state", "=", "random_state", "\n", ")", "\n", "if", "reward_type", "==", "\"binary\"", ":", "\n", "        ", "expected_reward", "=", "logit", "(", "expected_reward", ")", "\n", "", "expected_reward_factual", "=", "np", ".", "zeros_like", "(", "action_2d", ",", "dtype", "=", "\"float16\"", ")", "\n", "for", "pos_", "in", "np", ".", "arange", "(", "len_list", ")", ":", "\n", "        ", "tmp_fixed_reward", "=", "expected_reward", "[", "\n", "np", ".", "arange", "(", "len", "(", "action_2d", ")", ")", "//", "n_enumerated_slate_actions", ",", "\n", "action_2d", "[", ":", ",", "pos_", "]", ",", "\n", "]", "\n", "if", "reward_structure", "==", "\"independent\"", ":", "\n", "            ", "pass", "\n", "", "elif", "is_additive", ":", "\n", "            ", "for", "pos2_", "in", "np", ".", "arange", "(", "len_list", ")", ":", "\n", "                ", "if", "is_cascade", ":", "\n", "                    ", "if", "pos_", "<=", "pos2_", ":", "\n", "                        ", "break", "\n", "", "", "elif", "pos_", "==", "pos2_", ":", "\n", "                    ", "continue", "\n", "", "tmp_fixed_reward", "+=", "action_interaction_weight_matrix", "[", "\n", "action_2d", "[", ":", ",", "pos_", "]", ",", "action_2d", "[", ":", ",", "pos2_", "]", "\n", "]", "\n", "", "", "else", ":", "\n", "            ", "for", "pos2_", "in", "np", ".", "arange", "(", "len_list", ")", ":", "\n", "                ", "if", "is_cascade", ":", "\n", "                    ", "if", "pos_", "<=", "pos2_", ":", "\n", "                        ", "break", "\n", "", "", "elif", "pos_", "==", "pos2_", ":", "\n", "                    ", "continue", "\n", "", "expected_reward_", "=", "expected_reward", "[", "\n", "np", ".", "arange", "(", "len", "(", "action_2d", ")", ")", "//", "n_enumerated_slate_actions", ",", "\n", "action_2d", "[", ":", ",", "pos2_", "]", ",", "\n", "]", "\n", "weight_", "=", "action_interaction_weight_matrix", "[", "pos_", ",", "pos2_", "]", "\n", "tmp_fixed_reward", "+=", "expected_reward_", "*", "weight_", "\n", "", "", "expected_reward_factual", "[", ":", ",", "pos_", "]", "=", "tmp_fixed_reward", "\n", "\n", "", "if", "reward_type", "==", "\"binary\"", ":", "\n", "        ", "expected_reward_factual", "=", "sigmoid", "(", "expected_reward_factual", ")", "\n", "", "else", ":", "\n", "        ", "expected_reward_factual", "=", "np", ".", "clip", "(", "expected_reward_factual", ",", "0", ",", "None", ")", "\n", "\n", "", "assert", "expected_reward_factual", ".", "shape", "==", "(", "\n", "action_2d", ".", "shape", "[", "0", "]", ",", "\n", "len_list", ",", "\n", ")", ",", "f\"response shape must be (n_rounds (* enumerated_slate_actions), len_list), but {expected_reward_factual.shape}\"", "\n", "return", "expected_reward_factual", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.synthetic_slate.linear_behavior_policy_logit": [[1482, 1523], ["utils.check_array", "utils.check_array", "sklearn.utils.check_scalar", "sklearn.utils.check_random_state", "numpy.zeros", "sklearn.utils.check_random_state.uniform", "sklearn.utils.check_random_state.uniform", "numpy.arange"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array"], ["", "def", "linear_behavior_policy_logit", "(", "\n", "context", ":", "np", ".", "ndarray", ",", "\n", "action_context", ":", "np", ".", "ndarray", ",", "\n", "random_state", ":", "Optional", "[", "int", "]", "=", "None", ",", "\n", "tau", ":", "Union", "[", "int", ",", "float", "]", "=", "1.0", ",", "\n", ")", "->", "np", ".", "ndarray", ":", "\n", "    ", "\"\"\"Linear contextual behavior policy for synthetic slate bandit datasets.\n\n    Parameters\n    -----------\n    context: array-like, shape (n_rounds, dim_context)\n        Context vectors characterizing each data (such as user information).\n\n    action_context: array-like, shape (n_unique_action, dim_action_context)\n        Vector representation of actions.\n\n    random_state: int, default=None\n        Controls the random seed in sampling dataset.\n\n    tau: int or float, default=1.0\n        A temperature parameter to control the entropy of the behavior policy.\n        As :math:`\\\\tau \\\\rightarrow \\\\infty`, the algorithm will select arms uniformly at random.\n\n    Returns\n    ---------\n    logit value: array-like, shape (n_rounds, n_unique_action)\n        Logit values to define the behavior policy.\n\n    \"\"\"", "\n", "check_array", "(", "array", "=", "context", ",", "name", "=", "\"context\"", ",", "expected_dim", "=", "2", ")", "\n", "check_array", "(", "array", "=", "action_context", ",", "name", "=", "\"action_context\"", ",", "expected_dim", "=", "2", ")", "\n", "check_scalar", "(", "tau", ",", "name", "=", "\"tau\"", ",", "target_type", "=", "(", "int", ",", "float", ")", ",", "min_val", "=", "0", ")", "\n", "\n", "random_", "=", "check_random_state", "(", "random_state", ")", "\n", "logits", "=", "np", ".", "zeros", "(", "(", "context", ".", "shape", "[", "0", "]", ",", "action_context", ".", "shape", "[", "0", "]", ")", ")", "\n", "coef_", "=", "random_", ".", "uniform", "(", "size", "=", "context", ".", "shape", "[", "1", "]", ")", "\n", "action_coef_", "=", "random_", ".", "uniform", "(", "size", "=", "action_context", ".", "shape", "[", "1", "]", ")", "\n", "for", "d", "in", "np", ".", "arange", "(", "action_context", ".", "shape", "[", "0", "]", ")", ":", "\n", "        ", "logits", "[", ":", ",", "d", "]", "=", "context", "@", "coef_", "+", "action_context", "[", "d", "]", "@", "action_coef_", "\n", "\n", "", "return", "logits", "/", "tau", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.synthetic_slate.exponential_decay_function": [[1525, 1537], ["utils.check_array", "numpy.exp"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array"], ["", "def", "exponential_decay_function", "(", "distance", ":", "np", ".", "ndarray", ")", "->", "np", ".", "ndarray", ":", "\n", "    ", "\"\"\"Calculate exponential discount factor for action interaction weight matrix.\n\n    Parameters\n    -----------\n    distance: array-like, shape (len_list, )\n        Distance between two slots.\n\n    \"\"\"", "\n", "check_array", "(", "array", "=", "distance", ",", "name", "=", "\"distance\"", ",", "expected_dim", "=", "1", ")", "\n", "\n", "return", "np", ".", "exp", "(", "-", "distance", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.synthetic_slate.inverse_decay_function": [[1539, 1551], ["utils.check_array"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array"], ["", "def", "inverse_decay_function", "(", "distance", ":", "np", ".", "ndarray", ")", "->", "np", ".", "ndarray", ":", "\n", "    ", "\"\"\"Calculate inverse discount factor for action interaction weight matrix.\n\n    Parameters\n    -----------\n    distance: array-like, shape (len_list, )\n        Distance between two slots.\n\n    \"\"\"", "\n", "check_array", "(", "array", "=", "distance", ",", "name", "=", "\"distance\"", ",", "expected_dim", "=", "1", ")", "\n", "\n", "return", "1", "/", "(", "distance", "+", "1", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.synthetic.SyntheticBanditDataset.__post_init__": [[170, 212], ["sklearn.utils.check_scalar", "sklearn.utils.check_scalar", "sklearn.utils.check_scalar", "sklearn.utils.check_scalar", "sklearn.utils.check_random_state", "sklearn.utils.check_scalar", "ValueError", "reward_type.RewardType", "ValueError", "synthetic.SyntheticBanditDataset.sample_contextfree_expected_reward", "reward_type.RewardType", "numpy.eye", "utils.check_array", "ValueError"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.synthetic.SyntheticBanditDataset.sample_contextfree_expected_reward", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array"], ["def", "__post_init__", "(", "self", ")", "->", "None", ":", "\n", "        ", "\"\"\"Initialize Class.\"\"\"", "\n", "check_scalar", "(", "self", ".", "n_actions", ",", "\"n_actions\"", ",", "int", ",", "min_val", "=", "2", ")", "\n", "check_scalar", "(", "self", ".", "dim_context", ",", "\"dim_context\"", ",", "int", ",", "min_val", "=", "1", ")", "\n", "check_scalar", "(", "self", ".", "beta", ",", "\"beta\"", ",", "(", "int", ",", "float", ")", ")", "\n", "check_scalar", "(", "\n", "self", ".", "n_deficient_actions", ",", "\n", "\"n_deficient_actions\"", ",", "\n", "int", ",", "\n", "min_val", "=", "0", ",", "\n", "max_val", "=", "self", ".", "n_actions", "-", "1", ",", "\n", ")", "\n", "\n", "if", "self", ".", "random_state", "is", "None", ":", "\n", "            ", "raise", "ValueError", "(", "\"`random_state` must be given\"", ")", "\n", "", "self", ".", "random_", "=", "check_random_state", "(", "self", ".", "random_state", ")", "\n", "\n", "if", "RewardType", "(", "self", ".", "reward_type", ")", "not", "in", "[", "\n", "RewardType", ".", "BINARY", ",", "\n", "RewardType", ".", "CONTINUOUS", ",", "\n", "]", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "f\"`reward_type` must be either '{RewardType.BINARY.value}' or '{RewardType.CONTINUOUS.value}',\"", "\n", "f\"but {self.reward_type} is given.'\"", "\n", ")", "\n", "", "check_scalar", "(", "self", ".", "reward_std", ",", "\"reward_std\"", ",", "(", "int", ",", "float", ")", ",", "min_val", "=", "0", ")", "\n", "if", "self", ".", "reward_function", "is", "None", ":", "\n", "            ", "self", ".", "expected_reward", "=", "self", ".", "sample_contextfree_expected_reward", "(", ")", "\n", "", "if", "RewardType", "(", "self", ".", "reward_type", ")", "==", "RewardType", ".", "CONTINUOUS", ":", "\n", "            ", "self", ".", "reward_min", "=", "0", "\n", "self", ".", "reward_max", "=", "1e10", "\n", "\n", "# one-hot encoding characterizing actions.", "\n", "", "if", "self", ".", "action_context", "is", "None", ":", "\n", "            ", "self", ".", "action_context", "=", "np", ".", "eye", "(", "self", ".", "n_actions", ",", "dtype", "=", "int", ")", "\n", "", "else", ":", "\n", "            ", "check_array", "(", "\n", "array", "=", "self", ".", "action_context", ",", "name", "=", "\"action_context\"", ",", "expected_dim", "=", "2", "\n", ")", "\n", "if", "self", ".", "action_context", ".", "shape", "[", "0", "]", "!=", "self", ".", "n_actions", ":", "\n", "                ", "raise", "ValueError", "(", "\n", "\"Expected `action_context.shape[0] == n_actions`, but found it False.\"", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.synthetic.SyntheticBanditDataset.len_list": [[214, 218], ["None"], "methods", ["None"], ["", "", "", "@", "property", "\n", "def", "len_list", "(", "self", ")", "->", "int", ":", "\n", "        ", "\"\"\"Length of recommendation lists, slate size.\"\"\"", "\n", "return", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.synthetic.SyntheticBanditDataset.sample_contextfree_expected_reward": [[219, 222], ["synthetic.SyntheticBanditDataset.random_.uniform"], "methods", ["None"], ["", "def", "sample_contextfree_expected_reward", "(", "self", ")", "->", "np", ".", "ndarray", ":", "\n", "        ", "\"\"\"Sample expected reward for each action from the uniform distribution.\"\"\"", "\n", "return", "self", ".", "random_", ".", "uniform", "(", "size", "=", "self", ".", "n_actions", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.synthetic.SyntheticBanditDataset.calc_expected_reward": [[223, 236], ["numpy.tile", "synthetic.SyntheticBanditDataset.reward_function"], "methods", ["None"], ["", "def", "calc_expected_reward", "(", "self", ",", "context", ":", "np", ".", "ndarray", ")", "->", "np", ".", "ndarray", ":", "\n", "        ", "\"\"\"Sample expected rewards given contexts\"\"\"", "\n", "# sample reward for each round based on the reward function", "\n", "if", "self", ".", "reward_function", "is", "None", ":", "\n", "            ", "expected_reward_", "=", "np", ".", "tile", "(", "self", ".", "expected_reward", ",", "(", "context", ".", "shape", "[", "0", "]", ",", "1", ")", ")", "\n", "", "else", ":", "\n", "            ", "expected_reward_", "=", "self", ".", "reward_function", "(", "\n", "context", "=", "context", ",", "\n", "action_context", "=", "self", ".", "action_context", ",", "\n", "random_state", "=", "self", ".", "random_state", ",", "\n", ")", "\n", "\n", "", "return", "expected_reward_", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.synthetic.SyntheticBanditDataset.sample_reward_given_expected_reward": [[237, 261], ["reward_type.RewardType", "synthetic.SyntheticBanditDataset.random_.binomial", "reward_type.RewardType", "scipy.stats.truncnorm.rvs", "numpy.arange"], "methods", ["None"], ["", "def", "sample_reward_given_expected_reward", "(", "\n", "self", ",", "\n", "expected_reward", ":", "np", ".", "ndarray", ",", "\n", "action", ":", "np", ".", "ndarray", ",", "\n", ")", "->", "np", ".", "ndarray", ":", "\n", "        ", "\"\"\"Sample reward given expected rewards\"\"\"", "\n", "expected_reward_factual", "=", "expected_reward", "[", "np", ".", "arange", "(", "action", ".", "shape", "[", "0", "]", ")", ",", "action", "]", "\n", "if", "RewardType", "(", "self", ".", "reward_type", ")", "==", "RewardType", ".", "BINARY", ":", "\n", "            ", "reward", "=", "self", ".", "random_", ".", "binomial", "(", "n", "=", "1", ",", "p", "=", "expected_reward_factual", ")", "\n", "", "elif", "RewardType", "(", "self", ".", "reward_type", ")", "==", "RewardType", ".", "CONTINUOUS", ":", "\n", "            ", "mean", "=", "expected_reward_factual", "\n", "a", "=", "(", "self", ".", "reward_min", "-", "mean", ")", "/", "self", ".", "reward_std", "\n", "b", "=", "(", "self", ".", "reward_max", "-", "mean", ")", "/", "self", ".", "reward_std", "\n", "reward", "=", "truncnorm", ".", "rvs", "(", "\n", "a", "=", "a", ",", "\n", "b", "=", "b", ",", "\n", "loc", "=", "mean", ",", "\n", "scale", "=", "self", ".", "reward_std", ",", "\n", "random_state", "=", "self", ".", "random_state", ",", "\n", ")", "\n", "", "else", ":", "\n", "            ", "raise", "NotImplementedError", "\n", "\n", "", "return", "reward", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.synthetic.SyntheticBanditDataset.sample_reward": [[262, 291], ["utils.check_array", "utils.check_array", "synthetic.SyntheticBanditDataset.calc_expected_reward", "synthetic.SyntheticBanditDataset.sample_reward_given_expected_reward", "ValueError", "numpy.issubdtype", "ValueError"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.synthetic.SyntheticBanditDataset.calc_expected_reward", "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.synthetic.SyntheticBanditDataset.sample_reward_given_expected_reward"], ["", "def", "sample_reward", "(", "self", ",", "context", ":", "np", ".", "ndarray", ",", "action", ":", "np", ".", "ndarray", ")", "->", "np", ".", "ndarray", ":", "\n", "        ", "\"\"\"Sample rewards given contexts and actions, i.e., :math:`r \\\\sim p(r | x, a)`.\n\n        Parameters\n        -----------\n        context: array-like, shape (n_rounds, dim_context)\n            Context vectors characterizing each data (such as user information).\n\n        action: array-like, shape (n_rounds,)\n            Actions chosen by the behavior policy for each context.\n\n        Returns\n        ---------\n        reward: array-like, shape (n_rounds,)\n            Sampled rewards given contexts and actions.\n\n        \"\"\"", "\n", "check_array", "(", "array", "=", "context", ",", "name", "=", "\"context\"", ",", "expected_dim", "=", "2", ")", "\n", "check_array", "(", "array", "=", "action", ",", "name", "=", "\"action\"", ",", "expected_dim", "=", "1", ")", "\n", "if", "context", ".", "shape", "[", "0", "]", "!=", "action", ".", "shape", "[", "0", "]", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"Expected `context.shape[0] == action.shape[0]`, but found it False\"", "\n", ")", "\n", "", "if", "not", "np", ".", "issubdtype", "(", "action", ".", "dtype", ",", "np", ".", "integer", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\"the dtype of action must be a subdtype of int\"", ")", "\n", "\n", "", "expected_reward_", "=", "self", ".", "calc_expected_reward", "(", "context", ")", "\n", "\n", "return", "self", ".", "sample_reward_given_expected_reward", "(", "expected_reward_", ",", "action", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.synthetic.SyntheticBanditDataset.obtain_batch_bandit_feedback": [[292, 362], ["sklearn.utils.check_scalar", "synthetic.SyntheticBanditDataset.random_.normal", "synthetic.SyntheticBanditDataset.calc_expected_reward", "utils.sample_action_fast", "synthetic.SyntheticBanditDataset.sample_reward_given_expected_reward", "dict", "reward_type.RewardType", "scipy.stats.truncnorm.stats", "synthetic.SyntheticBanditDataset.behavior_policy_function", "numpy.zeros_like", "utils.softmax", "utils.softmax", "numpy.argsort", "numpy.tile", "synthetic.SyntheticBanditDataset.random_.gumbel", "numpy.arange", "numpy.arange"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.synthetic.SyntheticBanditDataset.calc_expected_reward", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.sample_action_fast", "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.synthetic.SyntheticBanditDataset.sample_reward_given_expected_reward", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.softmax", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.softmax"], ["", "def", "obtain_batch_bandit_feedback", "(", "self", ",", "n_rounds", ":", "int", ")", "->", "BanditFeedback", ":", "\n", "        ", "\"\"\"Obtain batch logged bandit data.\n\n        Parameters\n        ----------\n        n_rounds: int\n            Data size of the synthetic logged bandit data.\n\n        Returns\n        ---------\n        bandit_feedback: BanditFeedback\n            Synthesized logged bandit data.\n\n        \"\"\"", "\n", "check_scalar", "(", "n_rounds", ",", "\"n_rounds\"", ",", "int", ",", "min_val", "=", "1", ")", "\n", "contexts", "=", "self", ".", "random_", ".", "normal", "(", "size", "=", "(", "n_rounds", ",", "self", ".", "dim_context", ")", ")", "\n", "\n", "# calc expected reward given context and action", "\n", "expected_reward_", "=", "self", ".", "calc_expected_reward", "(", "contexts", ")", "\n", "if", "RewardType", "(", "self", ".", "reward_type", ")", "==", "RewardType", ".", "CONTINUOUS", ":", "\n", "# correct expected_reward_, as we use truncated normal distribution here", "\n", "            ", "mean", "=", "expected_reward_", "\n", "a", "=", "(", "self", ".", "reward_min", "-", "mean", ")", "/", "self", ".", "reward_std", "\n", "b", "=", "(", "self", ".", "reward_max", "-", "mean", ")", "/", "self", ".", "reward_std", "\n", "expected_reward_", "=", "truncnorm", ".", "stats", "(", "\n", "a", "=", "a", ",", "b", "=", "b", ",", "loc", "=", "mean", ",", "scale", "=", "self", ".", "reward_std", ",", "moments", "=", "\"m\"", "\n", ")", "\n", "\n", "# calculate the action choice probabilities of the behavior policy", "\n", "", "if", "self", ".", "behavior_policy_function", "is", "None", ":", "\n", "            ", "pi_b_logits", "=", "expected_reward_", "\n", "", "else", ":", "\n", "            ", "pi_b_logits", "=", "self", ".", "behavior_policy_function", "(", "\n", "context", "=", "contexts", ",", "\n", "action_context", "=", "self", ".", "action_context", ",", "\n", "random_state", "=", "self", ".", "random_state", ",", "\n", ")", "\n", "# create some deficient actions based on the value of `n_deficient_actions`", "\n", "", "if", "self", ".", "n_deficient_actions", ">", "0", ":", "\n", "            ", "pi_b", "=", "np", ".", "zeros_like", "(", "pi_b_logits", ")", "\n", "n_supported_actions", "=", "self", ".", "n_actions", "-", "self", ".", "n_deficient_actions", "\n", "supported_actions", "=", "np", ".", "argsort", "(", "\n", "self", ".", "random_", ".", "gumbel", "(", "size", "=", "(", "n_rounds", ",", "self", ".", "n_actions", ")", ")", ",", "axis", "=", "1", "\n", ")", "[", ":", ",", ":", ":", "-", "1", "]", "[", ":", ",", ":", "n_supported_actions", "]", "\n", "supported_actions_idx", "=", "(", "\n", "np", ".", "tile", "(", "np", ".", "arange", "(", "n_rounds", ")", ",", "(", "n_supported_actions", ",", "1", ")", ")", ".", "T", ",", "\n", "supported_actions", ",", "\n", ")", "\n", "pi_b", "[", "supported_actions_idx", "]", "=", "softmax", "(", "\n", "self", ".", "beta", "*", "pi_b_logits", "[", "supported_actions_idx", "]", "\n", ")", "\n", "", "else", ":", "\n", "            ", "pi_b", "=", "softmax", "(", "self", ".", "beta", "*", "pi_b_logits", ")", "\n", "# sample actions for each round based on the behavior policy", "\n", "", "actions", "=", "sample_action_fast", "(", "pi_b", ",", "random_state", "=", "self", ".", "random_state", ")", "\n", "\n", "# sample rewards based on the context and action", "\n", "rewards", "=", "self", ".", "sample_reward_given_expected_reward", "(", "expected_reward_", ",", "actions", ")", "\n", "\n", "return", "dict", "(", "\n", "n_rounds", "=", "n_rounds", ",", "\n", "n_actions", "=", "self", ".", "n_actions", ",", "\n", "context", "=", "contexts", ",", "\n", "action_context", "=", "self", ".", "action_context", ",", "\n", "action", "=", "actions", ",", "\n", "position", "=", "None", ",", "# position effect is not considered in synthetic data", "\n", "reward", "=", "rewards", ",", "\n", "expected_reward", "=", "expected_reward_", ",", "\n", "pi_b", "=", "pi_b", "[", ":", ",", ":", ",", "np", ".", "newaxis", "]", ",", "\n", "pscore", "=", "pi_b", "[", "np", ".", "arange", "(", "n_rounds", ")", ",", "actions", "]", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.synthetic.SyntheticBanditDataset.calc_ground_truth_policy_value": [[364, 396], ["utils.check_array", "utils.check_array", "numpy.average().mean", "ValueError", "ValueError", "numpy.average"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array"], ["", "def", "calc_ground_truth_policy_value", "(", "\n", "self", ",", "expected_reward", ":", "np", ".", "ndarray", ",", "action_dist", ":", "np", ".", "ndarray", "\n", ")", "->", "float", ":", "\n", "        ", "\"\"\"Calculate the policy value of given action distribution on the given expected_reward.\n\n        Parameters\n        -----------\n        expected_reward: array-like, shape (n_rounds, n_actions)\n            Expected reward given context (:math:`x`) and action (:math:`a`), i.e., :math:`q(x,a):=\\\\mathbb{E}[r|x,a]`.\n            This is often the `expected_reward` of the test set of logged bandit data.\n\n        action_dist: array-like, shape (n_rounds, n_actions, len_list)\n            Action choice probabilities of the evaluation policy (can be deterministic), i.e., :math:`\\\\pi_e(a_i|x_i)`.\n\n        Returns\n        ----------\n        policy_value: float\n            The policy value of the given action distribution on the given logged bandit data.\n\n        \"\"\"", "\n", "check_array", "(", "array", "=", "expected_reward", ",", "name", "=", "\"expected_reward\"", ",", "expected_dim", "=", "2", ")", "\n", "check_array", "(", "array", "=", "action_dist", ",", "name", "=", "\"action_dist\"", ",", "expected_dim", "=", "3", ")", "\n", "if", "expected_reward", ".", "shape", "[", "0", "]", "!=", "action_dist", ".", "shape", "[", "0", "]", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"Expected `expected_reward.shape[0] = action_dist.shape[0]`, but found it False\"", "\n", ")", "\n", "", "if", "expected_reward", ".", "shape", "[", "1", "]", "!=", "action_dist", ".", "shape", "[", "1", "]", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"Expected `expected_reward.shape[1] = action_dist.shape[1]`, but found it False\"", "\n", ")", "\n", "\n", "", "return", "np", ".", "average", "(", "expected_reward", ",", "weights", "=", "action_dist", "[", ":", ",", ":", ",", "0", "]", ",", "axis", "=", "1", ")", ".", "mean", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.synthetic.logistic_reward_function": [[398, 431], ["synthetic._base_reward_function", "utils.sigmoid"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.synthetic._base_reward_function", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.sigmoid"], ["", "", "def", "logistic_reward_function", "(", "\n", "context", ":", "np", ".", "ndarray", ",", "\n", "action_context", ":", "np", ".", "ndarray", ",", "\n", "random_state", ":", "Optional", "[", "int", "]", "=", "None", ",", "\n", ")", "->", "np", ".", "ndarray", ":", "\n", "    ", "\"\"\"Logistic mean reward function for binary rewards.\n\n    Parameters\n    -----------\n    context: array-like, shape (n_rounds, dim_context)\n        Context vectors characterizing each data (such as user information).\n\n    action_context: array-like, shape (n_actions, dim_action_context)\n        Vector representation of actions.\n\n    random_state: int, default=None\n        Controls the random seed in sampling dataset.\n\n    Returns\n    ---------\n    expected_reward: array-like, shape (n_rounds, n_actions)\n        Expected reward given context (:math:`x`) and action (:math:`a`),\n        i.e., :math:`q(x,a):=\\\\mathbb{E}[r|x,a]`.\n\n    \"\"\"", "\n", "logits", "=", "_base_reward_function", "(", "\n", "context", "=", "context", ",", "\n", "action_context", "=", "action_context", ",", "\n", "degree", "=", "1", ",", "\n", "random_state", "=", "random_state", ",", "\n", ")", "\n", "\n", "return", "sigmoid", "(", "logits", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.synthetic.logistic_polynomial_reward_function": [[433, 471], ["synthetic._base_reward_function", "utils.sigmoid"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.synthetic._base_reward_function", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.sigmoid"], ["", "def", "logistic_polynomial_reward_function", "(", "\n", "context", ":", "np", ".", "ndarray", ",", "\n", "action_context", ":", "np", ".", "ndarray", ",", "\n", "random_state", ":", "Optional", "[", "int", "]", "=", "None", ",", "\n", ")", "->", "np", ".", "ndarray", ":", "\n", "    ", "\"\"\"Logistic mean reward function for binary rewards with polynomial feature transformations.\n\n    Note\n    ------\n    Polynomial and interaction features will be used to calculate the expected rewards.\n    Feature transformation is based on `sklearn.preprocessing.PolynomialFeatures(degree=3)`\n\n    Parameters\n    -----------\n    context: array-like, shape (n_rounds, dim_context)\n        Context vectors characterizing each data (such as user information).\n\n    action_context: array-like, shape (n_actions, dim_action_context)\n        Vector representation of actions.\n\n    random_state: int, default=None\n        Controls the random seed in sampling dataset.\n\n    Returns\n    ---------\n    expected_reward: array-like, shape (n_rounds, n_actions)\n        Expected reward given context (:math:`x`) and action (:math:`a`),\n        i.e., :math:`q(x,a):=\\\\mathbb{E}[r|x,a]`.\n\n    \"\"\"", "\n", "logits", "=", "_base_reward_function", "(", "\n", "context", "=", "context", ",", "\n", "action_context", "=", "action_context", ",", "\n", "degree", "=", "3", ",", "\n", "random_state", "=", "random_state", ",", "\n", ")", "\n", "\n", "return", "sigmoid", "(", "logits", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.synthetic.logistic_sparse_reward_function": [[473, 513], ["synthetic._base_reward_function", "utils.sigmoid"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.synthetic._base_reward_function", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.sigmoid"], ["", "def", "logistic_sparse_reward_function", "(", "\n", "context", ":", "np", ".", "ndarray", ",", "\n", "action_context", ":", "np", ".", "ndarray", ",", "\n", "random_state", ":", "Optional", "[", "int", "]", "=", "None", ",", "\n", ")", "->", "np", ".", "ndarray", ":", "\n", "    ", "\"\"\"Logistic mean reward function for binary rewards with small effective feature dimension.\n\n    Note\n    ------\n    Polynomial and interaction features will be used to calculate the expected rewards.\n    `sklearn.preprocessing.PolynomialFeatures(degree=4)` is applied to generate high-dimensional feature vector.\n    After that, some dimensions will be dropped as irrelevant dimensions, producing sparse feature vector.\n\n    Parameters\n    -----------\n    context: array-like, shape (n_rounds, dim_context)\n        Context vectors characterizing each data (such as user information).\n\n    action_context: array-like, shape (n_actions, dim_action_context)\n        Vector representation of actions.\n\n    random_state: int, default=None\n        Controls the random seed in sampling dataset.\n\n    Returns\n    ---------\n    expected_reward: array-like, shape (n_rounds, n_actions)\n        Expected reward given context (:math:`x`) and action (:math:`a`),\n        i.e., :math:`q(x,a):=\\\\mathbb{E}[r|x,a]`.\n\n    \"\"\"", "\n", "logits", "=", "_base_reward_function", "(", "\n", "context", "=", "context", ",", "\n", "action_context", "=", "action_context", ",", "\n", "degree", "=", "4", ",", "\n", "effective_dim_ratio", "=", "0.3", ",", "\n", "random_state", "=", "random_state", ",", "\n", ")", "\n", "\n", "return", "sigmoid", "(", "logits", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.synthetic.linear_reward_function": [[515, 545], ["synthetic._base_reward_function"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.synthetic._base_reward_function"], ["", "def", "linear_reward_function", "(", "\n", "context", ":", "np", ".", "ndarray", ",", "\n", "action_context", ":", "np", ".", "ndarray", ",", "\n", "random_state", ":", "Optional", "[", "int", "]", "=", "None", ",", "\n", ")", "->", "np", ".", "ndarray", ":", "\n", "    ", "\"\"\"Linear mean reward function for continuous rewards.\n\n    Parameters\n    -----------\n    context: array-like, shape (n_rounds, dim_context)\n        Context vectors characterizing each data (such as user information).\n\n    action_context: array-like, shape (n_actions, dim_action_context)\n        Vector representation of actions.\n\n    random_state: int, default=None\n        Controls the random seed in sampling dataset.\n\n    Returns\n    ---------\n    expected_rewards: array-like, shape (n_rounds, n_actions)\n        Expected reward given context (:math:`x`) and action (:math:`a`),\n        i.e., :math:`q(x,a):=\\\\mathbb{E}[r|x,a]`.\n\n    \"\"\"", "\n", "return", "_base_reward_function", "(", "\n", "context", "=", "context", ",", "\n", "action_context", "=", "action_context", ",", "\n", "degree", "=", "1", ",", "\n", "random_state", "=", "random_state", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.synthetic.polynomial_reward_function": [[548, 583], ["synthetic._base_reward_function"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.synthetic._base_reward_function"], ["", "def", "polynomial_reward_function", "(", "\n", "context", ":", "np", ".", "ndarray", ",", "\n", "action_context", ":", "np", ".", "ndarray", ",", "\n", "random_state", ":", "Optional", "[", "int", "]", "=", "None", ",", "\n", ")", "->", "np", ".", "ndarray", ":", "\n", "    ", "\"\"\"Polynomial mean reward function for continuous rewards.\n\n    Note\n    ------\n    Polynomial and interaction features will be used to calculate the expected rewards.\n    Feature transformation is based on `sklearn.preprocessing.PolynomialFeatures(degree=3)`.\n\n    Parameters\n    -----------\n    context: array-like, shape (n_rounds, dim_context)\n        Context vectors characterizing each data (such as user information).\n\n    action_context: array-like, shape (n_actions, dim_action_context)\n        Vector representation of actions.\n\n    random_state: int, default=None\n        Controls the random seed in sampling dataset.\n\n    Returns\n    ---------\n    expected_rewards: array-like, shape (n_rounds, n_actions)\n        Expected reward given context (:math:`x`) and action (:math:`a`),\n        i.e., :math:`q(x,a):=\\\\mathbb{E}[r|x,a]`.\n\n    \"\"\"", "\n", "return", "_base_reward_function", "(", "\n", "context", "=", "context", ",", "\n", "action_context", "=", "action_context", ",", "\n", "degree", "=", "3", ",", "\n", "random_state", "=", "random_state", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.synthetic.sparse_reward_function": [[586, 623], ["synthetic._base_reward_function"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.synthetic._base_reward_function"], ["", "def", "sparse_reward_function", "(", "\n", "context", ":", "np", ".", "ndarray", ",", "\n", "action_context", ":", "np", ".", "ndarray", ",", "\n", "random_state", ":", "Optional", "[", "int", "]", "=", "None", ",", "\n", ")", "->", "np", ".", "ndarray", ":", "\n", "    ", "\"\"\"Sparse mean reward function for continuous rewards.\n\n    Note\n    ------\n    Polynomial and interaction features will be used to calculate the expected rewards.\n    `sklearn.preprocessing.PolynomialFeatures(degree=4)` is applied to generate high-dimensional feature vector.\n    After that, some dimensions will be dropped as irrelevant dimensions, producing sparse feature vector.\n\n    Parameters\n    -----------\n    context: array-like, shape (n_rounds, dim_context)\n        Context vectors characterizing each data (such as user information).\n\n    action_context: array-like, shape (n_actions, dim_action_context)\n        Vector representation of actions.\n\n    random_state: int, default=None\n        Controls the random seed in sampling dataset.\n\n    Returns\n    ---------\n    expected_rewards: array-like, shape (n_rounds, n_actions)\n        Expected reward given context (:math:`x`) and action (:math:`a`),\n        i.e., :math:`q(x,a):=\\\\mathbb{E}[r|x,a]`.\n\n    \"\"\"", "\n", "return", "_base_reward_function", "(", "\n", "context", "=", "context", ",", "\n", "action_context", "=", "action_context", ",", "\n", "degree", "=", "4", ",", "\n", "effective_dim_ratio", "=", "0.3", ",", "\n", "random_state", "=", "random_state", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.synthetic._base_reward_function": [[626, 744], ["sklearn.utils.check_scalar", "sklearn.utils.check_scalar", "utils.check_array", "utils.check_array", "sklearn.preprocessing.PolynomialFeatures", "sklearn.preprocessing.PolynomialFeatures.fit_transform", "sklearn.preprocessing.PolynomialFeatures.fit_transform", "sklearn.utils.check_random_state", "sklearn.utils.check_random_state.uniform", "sklearn.utils.check_random_state.uniform", "sklearn.utils.check_random_state.uniform", "numpy.tile", "numpy.maximum", "numpy.maximum", "numpy.tile", "expected_rewards.std", "numpy.int32", "numpy.int32", "expected_rewards.mean", "sklearn.utils.check_random_state.choice", "sklearn.utils.check_random_state.choice"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array"], ["", "def", "_base_reward_function", "(", "\n", "context", ":", "np", ".", "ndarray", ",", "\n", "action_context", ":", "np", ".", "ndarray", ",", "\n", "degree", ":", "int", "=", "3", ",", "\n", "effective_dim_ratio", ":", "float", "=", "1.0", ",", "\n", "random_state", ":", "Optional", "[", "int", "]", "=", "None", ",", "\n", ")", "->", "np", ".", "ndarray", ":", "\n", "    ", "\"\"\"Base function to define mean reward functions.\n\n    Note\n    ------\n    Given context :math:`x` and action_context :math:`a`, this function is used to define\n    mean reward function :math:`q(x,a) = \\\\mathbb{E}[r|x,a]` as follows.\n\n    .. math::\n\n        q(x,a) := \\\\tilde{x}^T M_{X,A} \\\\tilde{a} + \\\\theta_x^T \\\\tilde{x} + \\\\theta_a^T \\\\tilde{a},\n\n    where :math:`x` is a original context vector,\n    and :math:`a` is a original action_context vector representing actions.\n    Polynomial transformation is applied to original context and action vectors,\n    producing :math:`\\\\tilde{x} \\\\in \\\\mathbb{R}^{d_X}` and :math:`\\\\tilde{a} \\\\in \\\\mathbb{R}^{d_A}`.\n    Moreover, some dimensions of context and action_context might be randomly dropped according to `effective_dim_ratio`.\n    :math:`M_{X,A} \\\\mathbb{R}^{d_X \\\\times d_A}`, :math:`\\\\theta_x \\\\in \\\\mathbb{R}^{d_X}`,\n    and :math:`\\\\theta_a \\\\in \\\\mathbb{R}^{d_A}` are parameter matrix and vectors,\n    all sampled from the uniform distribution.\n    The logistic function will be applied to :math:`q(x,a)` in logistic reward functions\n    to adjust the range of the function output.\n\n    Currently, this function is used to define\n    `obp.dataset.linear_reward function` (degree=1),\n    `obp.dataset.polynomial_reward function` (degree=3),\n    `obp.dataset.sparse_reward function` (degree=4, effective_dim_ratio=0.1),\n     `obp.dataset.logistic_reward function` (degree=1),\n     `obp.dataset.logistic_polynomial_reward_function` (degree=3),\n     and `obp.dataset.logistic_sparse_reward_function` (degree=4, effective_dim_ratio=0.1).\n\n    Parameters\n    -----------\n    context: array-like, shape (n_rounds, dim_context)\n        Context vectors characterizing each data (such as user information).\n\n    action_context: array-like, shape (n_actions, dim_action_context)\n        Vector representation of actions.\n\n    degree: int, default=3\n        Specifies the maximal degree of the polynomial feature transformations\n        applied to both `context` and `action_context`.\n\n    effective_dim_ratio: int, default=1.0\n        Propotion of context dimensions relevant to the expected rewards.\n        Specifically, after the polynomial feature transformation is applied to the original context vectors,\n        only `dim_context * effective_dim_ratio` fraction of randomly selected dimensions\n        will be used as relevant dimensions to generate expected rewards.\n\n    random_state: int, default=None\n        Controls the random seed in sampling dataset.\n\n    Returns\n    ---------\n    expected_rewards: array-like, shape (n_rounds, n_actions)\n        Expected reward given context (:math:`x`) and action (:math:`a`),\n        i.e., :math:`q(x,a):=\\\\mathbb{E}[r|x,a]`.\n\n    \"\"\"", "\n", "check_scalar", "(", "degree", ",", "\"degree\"", ",", "int", ",", "min_val", "=", "1", ")", "\n", "check_scalar", "(", "\n", "effective_dim_ratio", ",", "\"effective_dim_ratio\"", ",", "float", ",", "min_val", "=", "0", ",", "max_val", "=", "1", "\n", ")", "\n", "check_array", "(", "array", "=", "context", ",", "name", "=", "\"context\"", ",", "expected_dim", "=", "2", ")", "\n", "check_array", "(", "array", "=", "action_context", ",", "name", "=", "\"action_context\"", ",", "expected_dim", "=", "2", ")", "\n", "\n", "poly", "=", "PolynomialFeatures", "(", "degree", "=", "degree", ")", "\n", "context_", "=", "poly", ".", "fit_transform", "(", "context", ")", "\n", "action_context_", "=", "poly", ".", "fit_transform", "(", "action_context", ")", "\n", "datasize", ",", "dim_context", "=", "context_", ".", "shape", "\n", "n_actions", ",", "dim_action_context", "=", "action_context_", ".", "shape", "\n", "random_", "=", "check_random_state", "(", "random_state", ")", "\n", "\n", "if", "effective_dim_ratio", "<", "1.0", ":", "\n", "        ", "effective_dim_context", "=", "np", ".", "maximum", "(", "\n", "np", ".", "int32", "(", "dim_context", "*", "effective_dim_ratio", ")", ",", "1", "\n", ")", "\n", "effective_dim_action_context", "=", "np", ".", "maximum", "(", "\n", "np", ".", "int32", "(", "dim_action_context", "*", "effective_dim_ratio", ")", ",", "1", "\n", ")", "\n", "effective_context_", "=", "context_", "[", "\n", ":", ",", "random_", ".", "choice", "(", "dim_context", ",", "effective_dim_context", ",", "replace", "=", "False", ")", "\n", "]", "\n", "effective_action_context_", "=", "action_context_", "[", "\n", ":", ",", "\n", "random_", ".", "choice", "(", "\n", "dim_action_context", ",", "effective_dim_action_context", ",", "replace", "=", "False", "\n", ")", ",", "\n", "]", "\n", "", "else", ":", "\n", "        ", "effective_dim_context", "=", "dim_context", "\n", "effective_dim_action_context", "=", "dim_action_context", "\n", "effective_context_", "=", "context_", "\n", "effective_action_context_", "=", "action_context_", "\n", "\n", "", "context_coef_", "=", "random_", ".", "uniform", "(", "-", "1", ",", "1", ",", "size", "=", "effective_dim_context", ")", "\n", "action_coef_", "=", "random_", ".", "uniform", "(", "-", "1", ",", "1", ",", "size", "=", "effective_dim_action_context", ")", "\n", "context_action_coef_", "=", "random_", ".", "uniform", "(", "\n", "-", "1", ",", "1", ",", "size", "=", "(", "effective_dim_context", ",", "effective_dim_action_context", ")", "\n", ")", "\n", "\n", "context_values", "=", "np", ".", "tile", "(", "effective_context_", "@", "context_coef_", ",", "(", "n_actions", ",", "1", ")", ")", ".", "T", "\n", "action_values", "=", "np", ".", "tile", "(", "action_coef_", "@", "effective_action_context_", ".", "T", ",", "(", "datasize", ",", "1", ")", ")", "\n", "context_action_values", "=", "(", "\n", "effective_context_", "@", "context_action_coef_", "@", "effective_action_context_", ".", "T", "\n", ")", "\n", "expected_rewards", "=", "context_values", "+", "action_values", "+", "context_action_values", "\n", "expected_rewards", "=", "(", "\n", "degree", "*", "(", "expected_rewards", "-", "expected_rewards", ".", "mean", "(", ")", ")", "/", "expected_rewards", ".", "std", "(", ")", "\n", ")", "\n", "\n", "return", "expected_rewards", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.synthetic.linear_behavior_policy": [[746, 776], ["synthetic._base_behavior_policy_function"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.synthetic._base_behavior_policy_function"], ["", "def", "linear_behavior_policy", "(", "\n", "context", ":", "np", ".", "ndarray", ",", "\n", "action_context", ":", "np", ".", "ndarray", ",", "\n", "random_state", ":", "Optional", "[", "int", "]", "=", "None", ",", "\n", ")", "->", "np", ".", "ndarray", ":", "\n", "    ", "\"\"\"Linear behavior policy function.\n\n    Parameters\n    -----------\n    context: array-like, shape (n_rounds, dim_context)\n        Context vectors characterizing each data (such as user information).\n\n    action_context: array-like, shape (n_actions, dim_action_context)\n        Vector representation of actions.\n\n    random_state: int, default=None\n        Controls the random seed in sampling dataset.\n\n    Returns\n    ---------\n    pi_b_logits: array-like, shape (n_rounds, n_actions)\n        Logit values given context (:math:`x`).\n        The softmax function will be applied to transform it to action choice probabilities.\n\n    \"\"\"", "\n", "return", "_base_behavior_policy_function", "(", "\n", "context", "=", "context", ",", "\n", "action_context", "=", "action_context", ",", "\n", "degree", "=", "1", ",", "\n", "random_state", "=", "random_state", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.synthetic.polynomial_behavior_policy": [[779, 814], ["synthetic._base_behavior_policy_function"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.synthetic._base_behavior_policy_function"], ["", "def", "polynomial_behavior_policy", "(", "\n", "context", ":", "np", ".", "ndarray", ",", "\n", "action_context", ":", "np", ".", "ndarray", ",", "\n", "random_state", ":", "Optional", "[", "int", "]", "=", "None", ",", "\n", ")", "->", "np", ".", "ndarray", ":", "\n", "    ", "\"\"\"Polynomial behavior policy function.\n\n    Note\n    ------\n    Polynomial and interaction features will be used to calculate the expected rewards.\n    Feature transformation is based on `sklearn.preprocessing.PolynomialFeatures(degree=3)`\n\n    Parameters\n    -----------\n    context: array-like, shape (n_rounds, dim_context)\n        Context vectors characterizing each data (such as user information).\n\n    action_context: array-like, shape (n_actions, dim_action_context)\n        Vector representation of actions.\n\n    random_state: int, default=None\n        Controls the random seed in sampling dataset.\n\n    Returns\n    ---------\n    pi_b_logits: array-like, shape (n_rounds, n_actions)\n        Logit values given context (:math:`x`).\n        The softmax function will be applied to transform it to action choice probabilities.\n\n    \"\"\"", "\n", "return", "_base_behavior_policy_function", "(", "\n", "context", "=", "context", ",", "\n", "action_context", "=", "action_context", ",", "\n", "degree", "=", "3", ",", "\n", "random_state", "=", "random_state", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.synthetic._base_behavior_policy_function": [[817, 888], ["sklearn.utils.check_scalar", "utils.check_array", "utils.check_array", "sklearn.preprocessing.PolynomialFeatures", "sklearn.preprocessing.PolynomialFeatures.fit_transform", "sklearn.preprocessing.PolynomialFeatures.fit_transform", "sklearn.utils.check_random_state", "sklearn.utils.check_random_state.uniform", "sklearn.utils.check_random_state.uniform", "pi_b_logits.std", "pi_b_logits.mean"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array"], ["", "def", "_base_behavior_policy_function", "(", "\n", "context", ":", "np", ".", "ndarray", ",", "\n", "action_context", ":", "np", ".", "ndarray", ",", "\n", "degree", ":", "int", "=", "3", ",", "\n", "random_state", ":", "Optional", "[", "int", "]", "=", "None", ",", "\n", ")", "->", "np", ".", "ndarray", ":", "\n", "    ", "\"\"\"Base function to define behavior policy functions.\n\n    Note\n    ------\n    Given context :math:`x` and action_context :math:`x_a`, this function generates\n    logit values for defining a behavior policy as follows.\n\n    .. math::\n\n        f_b(x,a) := \\\\tilde{x}^T M_{X,A} \\\\tilde{a} + \\\\theta_a^T \\\\tilde{a},\n\n    where :math:`x` is a original context vector,\n    and :math:`a` is a original action_context vector representing actions.\n    Polynomial transformation is applied to original context and action vectors,\n    producing :math:`\\\\tilde{x} \\\\in \\\\mathbb{R}^{d_X}` and :math:`\\\\tilde{a} \\\\in \\\\mathbb{R}^{d_A}`.\n    :math:`M_{X,A} \\\\mathbb{R}^{d_X \\\\times d_A}` and :math:`\\\\theta_a \\\\in \\\\mathbb{R}^{d_A}` are\n    parameter matrix and vector, each sampled from the uniform distribution.\n    The softmax function will be applied to :math:`f_b(x,\\\\cdot)` in `obp.dataset.SyntheticDataset`\n    to generate distribution over actions (behavior policy).\n\n    Currently, this function is used to define\n    `obp.dataset.linear_behavior_policy` (degree=1)\n    and `obp.dataset.polynomial_behavior_policy` (degree=3).\n\n    Parameters\n    -----------\n    context: array-like, shape (n_rounds, dim_context)\n        Context vectors characterizing each data (such as user information).\n\n    action_context: array-like, shape (n_actions, dim_action_context)\n        Vector representation of actions.\n\n    degree: int, default=3\n        Specifies the maximal degree of the polynomial feature transformations\n        applied to both `context` and `action_context`.\n\n    random_state: int, default=None\n        Controls the random seed in sampling dataset.\n\n    Returns\n    ---------\n    pi_b_logits: array-like, shape (n_rounds, n_actions)\n        Logit values given context (:math:`x`).\n        The softmax function will be applied to transform it to action choice probabilities.\n\n    \"\"\"", "\n", "check_scalar", "(", "degree", ",", "\"degree\"", ",", "int", ",", "min_val", "=", "1", ")", "\n", "check_array", "(", "array", "=", "context", ",", "name", "=", "\"context\"", ",", "expected_dim", "=", "2", ")", "\n", "check_array", "(", "array", "=", "action_context", ",", "name", "=", "\"action_context\"", ",", "expected_dim", "=", "2", ")", "\n", "\n", "poly", "=", "PolynomialFeatures", "(", "degree", "=", "degree", ")", "\n", "context_", "=", "poly", ".", "fit_transform", "(", "context", ")", "\n", "action_context_", "=", "poly", ".", "fit_transform", "(", "action_context", ")", "\n", "dim_context", "=", "context_", ".", "shape", "[", "1", "]", "\n", "dim_action_context", "=", "action_context_", ".", "shape", "[", "1", "]", "\n", "\n", "random_", "=", "check_random_state", "(", "random_state", ")", "\n", "action_coef", "=", "random_", ".", "uniform", "(", "size", "=", "dim_action_context", ")", "\n", "context_action_coef", "=", "random_", ".", "uniform", "(", "size", "=", "(", "dim_context", ",", "dim_action_context", ")", ")", "\n", "\n", "pi_b_logits", "=", "context_", "@", "context_action_coef", "@", "action_context_", ".", "T", "\n", "pi_b_logits", "+=", "action_coef", "@", "action_context_", ".", "T", "\n", "pi_b_logits", "=", "degree", "*", "(", "pi_b_logits", "-", "pi_b_logits", ".", "mean", "(", ")", ")", "/", "pi_b_logits", ".", "std", "(", ")", "\n", "\n", "return", "pi_b_logits", "\n", "", ""]], "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.multiclass.MultiClassToBanditReduction.__post_init__": [[167, 187], ["sklearn.utils.check_scalar", "sklearn.utils.check_scalar", "sklearn.utils.check_X_y", "numpy.zeros", "sklearn.base.is_classifier", "ValueError", "ValueError", "scipy.stats.rankdata", "numpy.arange"], "methods", ["None"], ["def", "__post_init__", "(", "self", ")", "->", "None", ":", "\n", "        ", "\"\"\"Initialize Class.\"\"\"", "\n", "if", "not", "is_classifier", "(", "self", ".", "base_classifier_b", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\"`base_classifier_b` must be a classifier\"", ")", "\n", "", "check_scalar", "(", "self", ".", "alpha_b", ",", "\"alpha_b\"", ",", "float", ",", "min_val", "=", "0.0", ")", "\n", "check_scalar", "(", "\n", "self", ".", "n_deficient_actions", ",", "\n", "\"n_deficient_actions\"", ",", "\n", "int", ",", "\n", "min_val", "=", "0", ",", "\n", "max_val", "=", "self", ".", "n_actions", "-", "1", ",", "\n", ")", "\n", "if", "self", ".", "alpha_b", ">=", "1.0", ":", "\n", "            ", "raise", "ValueError", "(", "f\"`alpha_b`= {self.alpha_b}, must be < 1.0.\"", ")", "\n", "\n", "", "self", ".", "X", ",", "y", "=", "check_X_y", "(", "X", "=", "self", ".", "X", ",", "y", "=", "self", ".", "y", ",", "ensure_2d", "=", "True", ",", "multi_output", "=", "False", ")", "\n", "self", ".", "y", "=", "(", "rankdata", "(", "y", ",", "\"dense\"", ")", "-", "1", ")", ".", "astype", "(", "int", ")", "# re-index actions", "\n", "# fully observed labels (full bandit feedback)", "\n", "self", ".", "y_full", "=", "np", ".", "zeros", "(", "(", "self", ".", "n_rounds", ",", "self", ".", "n_actions", ")", ")", "\n", "self", ".", "y_full", "[", "np", ".", "arange", "(", "self", ".", "n_rounds", ")", ",", "y", "]", "=", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.multiclass.MultiClassToBanditReduction.len_list": [[188, 192], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "len_list", "(", "self", ")", "->", "int", ":", "\n", "        ", "\"\"\"Length of recommendation lists, slate size.\"\"\"", "\n", "return", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.multiclass.MultiClassToBanditReduction.n_actions": [[193, 197], ["numpy.unique"], "methods", ["None"], ["", "@", "property", "\n", "def", "n_actions", "(", "self", ")", "->", "int", ":", "\n", "        ", "\"\"\"Number of actions (number of classes).\"\"\"", "\n", "return", "np", ".", "unique", "(", "self", ".", "y", ")", ".", "shape", "[", "0", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.multiclass.MultiClassToBanditReduction.n_rounds": [[198, 202], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "n_rounds", "(", "self", ")", "->", "int", ":", "\n", "        ", "\"\"\"Number of samples in the original multi-class classification data.\"\"\"", "\n", "return", "self", ".", "y", ".", "shape", "[", "0", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.multiclass.MultiClassToBanditReduction.split_train_eval": [[203, 231], ["sklearn.model_selection.train_test_split"], "methods", ["None"], ["", "def", "split_train_eval", "(", "\n", "self", ",", "\n", "eval_size", ":", "Union", "[", "int", ",", "float", "]", "=", "0.25", ",", "\n", "random_state", ":", "Optional", "[", "int", "]", "=", "None", ",", "\n", ")", "->", "None", ":", "\n", "        ", "\"\"\"Split the original data into the training (used for policy learning) and evaluation (used for OPE) sets.\n\n        Parameters\n        ----------\n        eval_size: float or int, default=0.25\n            If float, should be between 0.0 and 1.0 and represent the proportion of the data to include in the evaluation split.\n            If int, represents the absolute number of test samples.\n\n        random_state: int, default=None\n            Controls the random seed in train-evaluation split.\n\n        \"\"\"", "\n", "(", "\n", "self", ".", "X_tr", ",", "\n", "self", ".", "X_ev", ",", "\n", "self", ".", "y_tr", ",", "\n", "self", ".", "y_ev", ",", "\n", "_", ",", "\n", "self", ".", "y_full_ev", ",", "\n", ")", "=", "train_test_split", "(", "\n", "self", ".", "X", ",", "self", ".", "y", ",", "self", ".", "y_full", ",", "test_size", "=", "eval_size", ",", "random_state", "=", "random_state", "\n", ")", "\n", "self", ".", "n_rounds_ev", "=", "self", ".", "X_ev", ".", "shape", "[", "0", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.multiclass.MultiClassToBanditReduction.obtain_batch_bandit_feedback": [[232, 289], ["sklearn.utils.check_random_state", "sklearn.base.clone", "sklearn.base.clone.fit", "sklearn.base.clone.predict().astype", "numpy.zeros", "utils.sample_action_fast", "dict", "sklearn.base.clone.predict", "numpy.zeros.sum", "numpy.arange", "numpy.argsort", "numpy.tile", "numpy.arange", "sklearn.utils.check_random_state.gumbel", "numpy.arange", "numpy.arange"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.classification_model.PropensityScoreEstimator.fit", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.sample_action_fast", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.classification_model.PropensityScoreEstimator.predict"], ["", "def", "obtain_batch_bandit_feedback", "(", "\n", "self", ",", "\n", "random_state", ":", "Optional", "[", "int", "]", "=", "None", ",", "\n", ")", "->", "BanditFeedback", ":", "\n", "        ", "\"\"\"Obtain batch logged bandit data, an evaluation policy, and its ground-truth policy value.\n\n        Note\n        -------\n        Please call `self.split_train_eval()` before calling this method.\n\n        Parameters\n        -----------\n        random_state: int, default=None\n            Controls the random seed in sampling actions.\n\n        Returns\n        ---------\n        bandit_feedback: BanditFeedback\n            bandit_feedback is logged bandit data generated from a multi-class classification dataset.\n\n        \"\"\"", "\n", "random_", "=", "check_random_state", "(", "random_state", ")", "\n", "# train a base ML classifier", "\n", "base_clf_b", "=", "clone", "(", "self", ".", "base_classifier_b", ")", "\n", "base_clf_b", ".", "fit", "(", "X", "=", "self", ".", "X_tr", ",", "y", "=", "self", ".", "y_tr", ")", "\n", "preds", "=", "base_clf_b", ".", "predict", "(", "self", ".", "X_ev", ")", ".", "astype", "(", "int", ")", "\n", "# construct a behavior policy", "\n", "pi_b", "=", "np", ".", "zeros", "(", "(", "self", ".", "n_rounds_ev", ",", "self", ".", "n_actions", ")", ")", "\n", "pi_b", "[", ":", ",", ":", "]", "=", "(", "1.0", "-", "self", ".", "alpha_b", ")", "/", "self", ".", "n_actions", "\n", "pi_b", "[", "np", ".", "arange", "(", "self", ".", "n_rounds_ev", ")", ",", "preds", "]", "=", "(", "\n", "self", ".", "alpha_b", "+", "(", "1.0", "-", "self", ".", "alpha_b", ")", "/", "self", ".", "n_actions", "\n", ")", "\n", "if", "self", ".", "n_deficient_actions", ">", "0", ":", "\n", "            ", "deficient_actions", "=", "np", ".", "argsort", "(", "\n", "random_", ".", "gumbel", "(", "size", "=", "(", "self", ".", "n_rounds_ev", ",", "self", ".", "n_actions", ")", ")", ",", "axis", "=", "1", "\n", ")", "[", ":", ",", ":", ":", "-", "1", "]", "[", ":", ",", ":", "self", ".", "n_deficient_actions", "]", "\n", "deficient_actions_idx", "=", "(", "\n", "np", ".", "tile", "(", "np", ".", "arange", "(", "self", ".", "n_rounds_ev", ")", ",", "(", "self", ".", "n_deficient_actions", ",", "1", ")", ")", ".", "T", ",", "\n", "deficient_actions", ",", "\n", ")", "\n", "pi_b", "[", "deficient_actions_idx", "]", "=", "0.0", "# create some deficient actions", "\n", "pi_b", "/=", "pi_b", ".", "sum", "(", "1", ")", "[", "\n", ":", ",", "np", ".", "newaxis", "\n", "]", "# re-normalize the probability distribution", "\n", "# sample actions and factual rewards", "\n", "", "actions", "=", "sample_action_fast", "(", "pi_b", ",", "random_state", "=", "random_state", ")", "\n", "rewards", "=", "self", ".", "y_full_ev", "[", "np", ".", "arange", "(", "self", ".", "n_rounds_ev", ")", ",", "actions", "]", "\n", "\n", "return", "dict", "(", "\n", "n_actions", "=", "self", ".", "n_actions", ",", "\n", "n_rounds", "=", "self", ".", "n_rounds_ev", ",", "\n", "context", "=", "self", ".", "X_ev", ",", "\n", "action", "=", "actions", ",", "\n", "reward", "=", "rewards", ",", "\n", "position", "=", "None", ",", "# position effect is not considered in classification data", "\n", "pi_b", "=", "pi_b", "[", ":", ",", ":", ",", "np", ".", "newaxis", "]", ",", "\n", "pscore", "=", "pi_b", "[", "np", ".", "arange", "(", "self", ".", "n_rounds_ev", ")", ",", "actions", "]", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.multiclass.MultiClassToBanditReduction.obtain_action_dist_by_eval_policy": [[291, 331], ["sklearn.utils.check_scalar", "sklearn.base.clone.fit", "sklearn.base.clone.predict().astype", "numpy.zeros", "sklearn.base.clone", "sklearn.base.is_classifier", "sklearn.base.clone", "sklearn.base.clone.predict", "numpy.arange"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.classification_model.PropensityScoreEstimator.fit", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.classification_model.PropensityScoreEstimator.predict"], ["", "def", "obtain_action_dist_by_eval_policy", "(", "\n", "self", ",", "base_classifier_e", ":", "Optional", "[", "ClassifierMixin", "]", "=", "None", ",", "alpha_e", ":", "float", "=", "1.0", "\n", ")", "->", "np", ".", "ndarray", ":", "\n", "        ", "\"\"\"Obtain action choice probabilities by an evaluation policy.\n\n        Parameters\n        -----------\n        base_classifier_e: ClassifierMixin, default=None\n            Machine learning classifier used to construct a behavior policy.\n\n        alpha_e: float, default=1.0\n            Ratio of a uniform random policy when constructing an **evaluation** policy.\n            Must be in the [0, 1] interval (evaluation policy can be deterministic).\n\n        Returns\n        ---------\n        action_dist_by_eval_policy: array-like, shape (n_rounds_ev, n_actions, 1)\n            `action_dist_by_eval_policy` is the action choice probabilities of the evaluation policy.\n            where `n_rounds_ev` is the number of samples in the evaluation set given the current train-eval split.\n            `n_actions` is the number of actions.\n\n        \"\"\"", "\n", "check_scalar", "(", "alpha_e", ",", "\"alpha_e\"", ",", "float", ",", "min_val", "=", "0.0", ",", "max_val", "=", "1.0", ")", "\n", "# train a base ML classifier", "\n", "if", "base_classifier_e", "is", "None", ":", "\n", "            ", "base_clf_e", "=", "clone", "(", "self", ".", "base_classifier_b", ")", "\n", "", "else", ":", "\n", "            ", "assert", "is_classifier", "(", "\n", "base_classifier_e", "\n", ")", ",", "\"`base_classifier_e` must be a classifier\"", "\n", "base_clf_e", "=", "clone", "(", "base_classifier_e", ")", "\n", "", "base_clf_e", ".", "fit", "(", "X", "=", "self", ".", "X_tr", ",", "y", "=", "self", ".", "y_tr", ")", "\n", "preds", "=", "base_clf_e", ".", "predict", "(", "self", ".", "X_ev", ")", ".", "astype", "(", "int", ")", "\n", "# construct an evaluation policy", "\n", "pi_e", "=", "np", ".", "zeros", "(", "(", "self", ".", "n_rounds_ev", ",", "self", ".", "n_actions", ")", ")", "\n", "pi_e", "[", ":", ",", ":", "]", "=", "(", "1.0", "-", "alpha_e", ")", "/", "self", ".", "n_actions", "\n", "pi_e", "[", "np", ".", "arange", "(", "self", ".", "n_rounds_ev", ")", ",", "preds", "]", "=", "(", "\n", "alpha_e", "+", "(", "1.0", "-", "alpha_e", ")", "/", "self", ".", "n_actions", "\n", ")", "\n", "return", "pi_e", "[", ":", ",", ":", ",", "np", ".", "newaxis", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.multiclass.MultiClassToBanditReduction.calc_ground_truth_policy_value": [[332, 354], ["utils.check_array", "action_dist[].mean", "ValueError", "numpy.arange"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array"], ["", "def", "calc_ground_truth_policy_value", "(", "self", ",", "action_dist", ":", "np", ".", "ndarray", ")", "->", "float", ":", "\n", "        ", "\"\"\"Calculate the ground-truth policy value of the given action distribution.\n\n        Parameters\n        ----------\n        action_dist: array-like, shape (n_rounds_ev, n_actions, 1)\n            Action distribution or action choice probabilities of a policy whose ground-truth is to be caliculated here.\n            where `n_rounds_ev` is the number of samples in the evaluation set given the current train-eval split.\n            `n_actions` is the number of actions.\n\n        Returns\n        ---------\n        ground_truth_policy_value: float\n            policy value of given action distribution (mostly evaluation policy).\n\n        \"\"\"", "\n", "check_array", "(", "array", "=", "action_dist", ",", "name", "=", "\"action_dist\"", ",", "expected_dim", "=", "3", ")", "\n", "if", "action_dist", ".", "shape", "[", "0", "]", "!=", "self", ".", "n_rounds_ev", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"Expected `action_dist.shape[0] == self.n_rounds_ev`, but found it False\"", "\n", ")", "\n", "", "return", "action_dist", "[", "np", ".", "arange", "(", "self", ".", "n_rounds_ev", ")", ",", "self", ".", "y_ev", "]", ".", "mean", "(", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.reward_type.RewardType.__repr__": [[18, 21], ["str"], "methods", ["None"], ["def", "__repr__", "(", "self", ")", "->", "str", ":", "\n", "\n", "        ", "return", "str", "(", "self", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.test_logistic.test_logistic_base_exception": [[10, 57], ["pytest.raises", "obp.policy.logistic.LogisticEpsilonGreedy", "pytest.raises", "obp.policy.logistic.LogisticEpsilonGreedy", "pytest.raises", "obp.policy.logistic.LogisticEpsilonGreedy", "pytest.raises", "obp.policy.logistic.LogisticEpsilonGreedy", "pytest.raises", "obp.policy.logistic.LogisticEpsilonGreedy", "pytest.raises", "obp.policy.logistic.LogisticEpsilonGreedy", "pytest.raises", "obp.policy.logistic.LogisticEpsilonGreedy", "pytest.raises", "obp.policy.logistic.LogisticEpsilonGreedy", "pytest.raises", "obp.policy.logistic.LogisticEpsilonGreedy", "pytest.raises", "obp.policy.logistic.LogisticEpsilonGreedy", "pytest.raises", "obp.policy.logistic.LogisticEpsilonGreedy", "pytest.raises", "obp.policy.logistic.LogisticEpsilonGreedy", "pytest.raises", "obp.policy.logistic.LogisticEpsilonGreedy", "pytest.raises", "obp.policy.logistic.LogisticEpsilonGreedy"], "function", ["None"], ["def", "test_logistic_base_exception", "(", ")", ":", "\n", "# invalid dim", "\n", "    ", "with", "pytest", ".", "raises", "(", "ValueError", ")", ":", "\n", "        ", "LogisticEpsilonGreedy", "(", "n_actions", "=", "2", ",", "dim", "=", "-", "3", ")", "\n", "\n", "", "with", "pytest", ".", "raises", "(", "ValueError", ")", ":", "\n", "        ", "LogisticEpsilonGreedy", "(", "n_actions", "=", "2", ",", "dim", "=", "0", ")", "\n", "\n", "", "with", "pytest", ".", "raises", "(", "TypeError", ")", ":", "\n", "        ", "LogisticEpsilonGreedy", "(", "n_actions", "=", "2", ",", "dim", "=", "\"3\"", ")", "\n", "\n", "# invalid n_actions", "\n", "", "with", "pytest", ".", "raises", "(", "ValueError", ")", ":", "\n", "        ", "LogisticEpsilonGreedy", "(", "n_actions", "=", "-", "3", ",", "dim", "=", "2", ")", "\n", "\n", "", "with", "pytest", ".", "raises", "(", "ValueError", ")", ":", "\n", "        ", "LogisticEpsilonGreedy", "(", "n_actions", "=", "1", ",", "dim", "=", "2", ")", "\n", "\n", "", "with", "pytest", ".", "raises", "(", "TypeError", ")", ":", "\n", "        ", "LogisticEpsilonGreedy", "(", "n_actions", "=", "\"2\"", ",", "dim", "=", "2", ")", "\n", "\n", "# invalid len_list", "\n", "", "with", "pytest", ".", "raises", "(", "ValueError", ")", ":", "\n", "        ", "LogisticEpsilonGreedy", "(", "n_actions", "=", "2", ",", "dim", "=", "2", ",", "len_list", "=", "-", "3", ")", "\n", "\n", "", "with", "pytest", ".", "raises", "(", "ValueError", ")", ":", "\n", "        ", "LogisticEpsilonGreedy", "(", "n_actions", "=", "2", ",", "dim", "=", "2", ",", "len_list", "=", "0", ")", "\n", "\n", "", "with", "pytest", ".", "raises", "(", "TypeError", ")", ":", "\n", "        ", "LogisticEpsilonGreedy", "(", "n_actions", "=", "2", ",", "dim", "=", "2", ",", "len_list", "=", "\"3\"", ")", "\n", "\n", "# invalid batch_size", "\n", "", "with", "pytest", ".", "raises", "(", "ValueError", ")", ":", "\n", "        ", "LogisticEpsilonGreedy", "(", "n_actions", "=", "2", ",", "dim", "=", "2", ",", "batch_size", "=", "-", "2", ")", "\n", "\n", "", "with", "pytest", ".", "raises", "(", "ValueError", ")", ":", "\n", "        ", "LogisticEpsilonGreedy", "(", "n_actions", "=", "2", ",", "dim", "=", "2", ",", "batch_size", "=", "0", ")", "\n", "\n", "", "with", "pytest", ".", "raises", "(", "TypeError", ")", ":", "\n", "        ", "LogisticEpsilonGreedy", "(", "n_actions", "=", "2", ",", "dim", "=", "2", ",", "batch_size", "=", "\"10\"", ")", "\n", "\n", "# invalid relationship between n_actions and len_list", "\n", "", "with", "pytest", ".", "raises", "(", "ValueError", ")", ":", "\n", "        ", "LogisticEpsilonGreedy", "(", "n_actions", "=", "5", ",", "len_list", "=", "10", ",", "dim", "=", "2", ")", "\n", "\n", "", "with", "pytest", ".", "raises", "(", "ValueError", ")", ":", "\n", "        ", "LogisticEpsilonGreedy", "(", "n_actions", "=", "2", ",", "len_list", "=", "3", ",", "dim", "=", "2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.test_logistic.test_logistic_epsilon_normal_epsilon": [[59, 66], ["obp.policy.logistic.LogisticEpsilonGreedy", "obp.policy.logistic.LogisticEpsilonGreedy"], "function", ["None"], ["", "", "def", "test_logistic_epsilon_normal_epsilon", "(", ")", ":", "\n", "\n", "    ", "policy1", "=", "LogisticEpsilonGreedy", "(", "n_actions", "=", "2", ",", "dim", "=", "2", ")", "\n", "assert", "0", "<=", "policy1", ".", "epsilon", "<=", "1", "\n", "\n", "policy2", "=", "LogisticEpsilonGreedy", "(", "n_actions", "=", "2", ",", "dim", "=", "2", ",", "epsilon", "=", "0.5", ")", "\n", "assert", "policy2", ".", "epsilon", "==", "0.5", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.test_logistic.test_logistic_epsilon_abnormal_epsilon": [[68, 75], ["pytest.raises", "obp.policy.logistic.LogisticEpsilonGreedy", "pytest.raises", "obp.policy.logistic.LogisticEpsilonGreedy"], "function", ["None"], ["", "def", "test_logistic_epsilon_abnormal_epsilon", "(", ")", ":", "\n", "\n", "    ", "with", "pytest", ".", "raises", "(", "ValueError", ")", ":", "\n", "        ", "LogisticEpsilonGreedy", "(", "n_actions", "=", "2", ",", "dim", "=", "2", ",", "epsilon", "=", "1.3", ")", "\n", "\n", "", "with", "pytest", ".", "raises", "(", "ValueError", ")", ":", "\n", "        ", "LogisticEpsilonGreedy", "(", "n_actions", "=", "2", ",", "dim", "=", "2", ",", "epsilon", "=", "-", "0.3", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.test_logistic.test_logistic_epsilon_each_action_model": [[77, 82], ["obp.policy.logistic.LogisticEpsilonGreedy", "range", "isinstance"], "function", ["None"], ["", "", "def", "test_logistic_epsilon_each_action_model", "(", ")", ":", "\n", "    ", "n_actions", "=", "3", "\n", "policy", "=", "LogisticEpsilonGreedy", "(", "n_actions", "=", "n_actions", ",", "dim", "=", "2", ",", "epsilon", "=", "0.5", ")", "\n", "for", "i", "in", "range", "(", "n_actions", ")", ":", "\n", "        ", "assert", "isinstance", "(", "policy", ".", "model_list", "[", "i", "]", ",", "MiniBatchLogisticRegression", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.test_logistic.test_logistic_epsilon_select_action_exploitation": [[84, 94], ["obp.policy.logistic.LogisticEpsilonGreedy", "numpy.array().reshape", "obp.policy.logistic.LogisticEpsilonGreedy.update_params", "obp.policy.logistic.LogisticEpsilonGreedy.update_params", "obp.policy.logistic.LogisticEpsilonGreedy.update_params", "obp.policy.logistic.LogisticEpsilonGreedy.update_params", "range", "numpy.array", "obp.policy.logistic.LogisticEpsilonGreedy.select_action"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.policy.linear.BaseLinPolicy.update_params", "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.linear.BaseLinPolicy.update_params", "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.linear.BaseLinPolicy.update_params", "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.linear.BaseLinPolicy.update_params", "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.linear.LinTS.select_action"], ["", "", "def", "test_logistic_epsilon_select_action_exploitation", "(", ")", ":", "\n", "    ", "trial_num", "=", "50", "\n", "policy", "=", "LogisticEpsilonGreedy", "(", "n_actions", "=", "2", ",", "dim", "=", "2", ",", "epsilon", "=", "0.0", ")", "\n", "context", "=", "np", ".", "array", "(", "[", "1.0", ",", "1.0", "]", ")", ".", "reshape", "(", "1", ",", "-", "1", ")", "\n", "policy", ".", "update_params", "(", "action", "=", "0", ",", "reward", "=", "1.0", ",", "context", "=", "context", ")", "\n", "policy", ".", "update_params", "(", "action", "=", "0", ",", "reward", "=", "1.0", ",", "context", "=", "context", ")", "\n", "policy", ".", "update_params", "(", "action", "=", "1", ",", "reward", "=", "1.0", ",", "context", "=", "context", ")", "\n", "policy", ".", "update_params", "(", "action", "=", "1", ",", "reward", "=", "0.0", ",", "context", "=", "context", ")", "\n", "for", "_", "in", "range", "(", "trial_num", ")", ":", "\n", "        ", "assert", "policy", ".", "select_action", "(", "context", "=", "context", ")", "[", "0", "]", "==", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.test_logistic.test_logistic_epsilon_select_action_exploration": [[96, 106], ["obp.policy.logistic.LogisticEpsilonGreedy", "numpy.array().reshape", "obp.policy.logistic.LogisticEpsilonGreedy.update_params", "obp.policy.logistic.LogisticEpsilonGreedy.update_params", "obp.policy.logistic.LogisticEpsilonGreedy.update_params", "obp.policy.logistic.LogisticEpsilonGreedy.update_params", "obp.policy.logistic.LogisticEpsilonGreedy.select_action", "numpy.array", "range", "sum"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.policy.linear.BaseLinPolicy.update_params", "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.linear.BaseLinPolicy.update_params", "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.linear.BaseLinPolicy.update_params", "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.linear.BaseLinPolicy.update_params", "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.linear.LinTS.select_action"], ["", "", "def", "test_logistic_epsilon_select_action_exploration", "(", ")", ":", "\n", "    ", "trial_num", "=", "50", "\n", "policy", "=", "LogisticEpsilonGreedy", "(", "n_actions", "=", "2", ",", "dim", "=", "2", ",", "epsilon", "=", "1.0", ")", "\n", "context", "=", "np", ".", "array", "(", "[", "1.0", ",", "1.0", "]", ")", ".", "reshape", "(", "1", ",", "-", "1", ")", "\n", "policy", ".", "update_params", "(", "action", "=", "0", ",", "reward", "=", "1.0", ",", "context", "=", "context", ")", "\n", "policy", ".", "update_params", "(", "action", "=", "0", ",", "reward", "=", "1.0", ",", "context", "=", "context", ")", "\n", "policy", ".", "update_params", "(", "action", "=", "1", ",", "reward", "=", "1.0", ",", "context", "=", "context", ")", "\n", "policy", ".", "update_params", "(", "action", "=", "1", ",", "reward", "=", "0.0", ",", "context", "=", "context", ")", "\n", "selected_action", "=", "[", "policy", ".", "select_action", "(", "context", "=", "context", ")", "for", "_", "in", "range", "(", "trial_num", ")", "]", "\n", "assert", "0", "<", "sum", "(", "selected_action", ")", "[", "0", "]", "<", "trial_num", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.test_logistic.test_logistic_ucb_initialize": [[108, 117], ["obp.policy.logistic.LogisticUCB", "range", "pytest.raises", "obp.policy.logistic.LogisticUCB", "isinstance"], "function", ["None"], ["", "def", "test_logistic_ucb_initialize", "(", ")", ":", "\n", "# note that the meaning of epsilon is different from that of LogisticEpsilonGreedy", "\n", "    ", "with", "pytest", ".", "raises", "(", "ValueError", ")", ":", "\n", "        ", "LogisticUCB", "(", "n_actions", "=", "2", ",", "dim", "=", "2", ",", "epsilon", "=", "-", "0.2", ")", "\n", "\n", "", "n_actions", "=", "3", "\n", "policy", "=", "LogisticUCB", "(", "n_actions", "=", "n_actions", ",", "dim", "=", "2", ",", "epsilon", "=", "0.5", ")", "\n", "for", "i", "in", "range", "(", "n_actions", ")", ":", "\n", "        ", "assert", "isinstance", "(", "policy", ".", "model_list", "[", "i", "]", ",", "MiniBatchLogisticRegression", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.test_logistic.test_logistic_ucb_select_action": [[119, 126], ["obp.policy.logistic.LogisticUCB", "numpy.ones().reshape", "obp.policy.logistic.LogisticUCB.select_action", "len", "numpy.ones"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.policy.linear.LinTS.select_action"], ["", "", "def", "test_logistic_ucb_select_action", "(", ")", ":", "\n", "    ", "dim", "=", "3", "\n", "len_list", "=", "2", "\n", "policy", "=", "LogisticUCB", "(", "n_actions", "=", "4", ",", "dim", "=", "dim", ",", "len_list", "=", "2", ",", "epsilon", "=", "0.0", ")", "\n", "context", "=", "np", ".", "ones", "(", "dim", ")", ".", "reshape", "(", "1", ",", "-", "1", ")", "\n", "action", "=", "policy", ".", "select_action", "(", "context", "=", "context", ")", "\n", "assert", "len", "(", "action", ")", "==", "len_list", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.test_logistic.test_logistic_ts_initialize": [[128, 133], ["obp.policy.logistic.LogisticTS", "range", "isinstance"], "function", ["None"], ["", "def", "test_logistic_ts_initialize", "(", ")", ":", "\n", "    ", "n_actions", "=", "3", "\n", "policy", "=", "LogisticTS", "(", "n_actions", "=", "n_actions", ",", "dim", "=", "2", ")", "\n", "for", "i", "in", "range", "(", "n_actions", ")", ":", "\n", "        ", "assert", "isinstance", "(", "policy", ".", "model_list", "[", "i", "]", ",", "MiniBatchLogisticRegression", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.test_logistic.test_logistic_ts_select_action": [[135, 142], ["obp.policy.logistic.LogisticTS", "numpy.ones().reshape", "obp.policy.logistic.LogisticTS.select_action", "len", "numpy.ones"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.policy.linear.LinTS.select_action"], ["", "", "def", "test_logistic_ts_select_action", "(", ")", ":", "\n", "    ", "dim", "=", "3", "\n", "len_list", "=", "2", "\n", "policy", "=", "LogisticTS", "(", "n_actions", "=", "4", ",", "dim", "=", "dim", ",", "len_list", "=", "2", ")", "\n", "context", "=", "np", ".", "ones", "(", "dim", ")", ".", "reshape", "(", "1", ",", "-", "1", ")", "\n", "action", "=", "policy", ".", "select_action", "(", "context", "=", "context", ")", "\n", "assert", "len", "(", "action", ")", "==", "len_list", "\n", "", ""]], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.test_offline_learner_continuous_performance.RandomPolicy.fit": [[47, 49], ["None"], "methods", ["None"], ["def", "fit", "(", "self", ")", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.test_offline_learner_continuous_performance.RandomPolicy.predict": [[50, 57], ["numpy.random.uniform"], "methods", ["None"], ["", "def", "predict", "(", "self", ",", "context", ":", "np", ".", "ndarray", ")", "->", "np", ".", "ndarray", ":", "\n", "\n", "        ", "n_rounds", "=", "context", ".", "shape", "[", "0", "]", "\n", "predicted_actions", "=", "np", ".", "random", ".", "uniform", "(", "\n", "self", ".", "output_space", "[", "0", "]", ",", "self", ".", "output_space", "[", "1", "]", ",", "size", "=", "n_rounds", "\n", ")", "\n", "return", "predicted_actions", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.test_offline_learner_continuous_performance.test_offline_nn_policy_learner_performance": [[59, 144], ["pytest.mark.parametrize", "enumerate", "obp.dataset.SyntheticContinuousBanditDataset", "obp.policy.ContinuousNNPolicyLearner", "test_offline_learner_continuous_performance.RandomPolicy", "obp.dataset.SyntheticContinuousBanditDataset.obtain_batch_bandit_feedback", "obp.dataset.SyntheticContinuousBanditDataset.obtain_batch_bandit_feedback", "obp.policy.ContinuousNNPolicyLearner.fit", "obp.policy.ContinuousNNPolicyLearner.predict", "test_offline_learner_continuous_performance.RandomPolicy.predict", "obp.dataset.SyntheticContinuousBanditDataset.calc_ground_truth_policy_value", "obp.dataset.SyntheticContinuousBanditDataset.calc_ground_truth_policy_value", "joblib.Parallel", "list_gt_nn_policy.append", "list_gt_random.append", "numpy.mean", "numpy.mean", "joblib.delayed", "numpy.arange"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.multiclass.MultiClassToBanditReduction.obtain_batch_bandit_feedback", "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.multiclass.MultiClassToBanditReduction.obtain_batch_bandit_feedback", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.classification_model.PropensityScoreEstimator.fit", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.classification_model.PropensityScoreEstimator.predict", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.classification_model.PropensityScoreEstimator.predict", "home.repos.pwc.inspect_result.st-tech_zr-obp.simulator.simulator.calc_ground_truth_policy_value", "home.repos.pwc.inspect_result.st-tech_zr-obp.simulator.simulator.calc_ground_truth_policy_value"], ["", "", "@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"n_rounds, dim_context, action_noise, reward_noise, min_action_value, max_action_value, pg_method, bandwidth\"", ",", "\n", "offline_experiment_configurations", ",", "\n", ")", "\n", "def", "test_offline_nn_policy_learner_performance", "(", "\n", "n_rounds", ":", "int", ",", "\n", "dim_context", ":", "int", ",", "\n", "action_noise", ":", "float", ",", "\n", "reward_noise", ":", "float", ",", "\n", "min_action_value", ":", "float", ",", "\n", "max_action_value", ":", "float", ",", "\n", "pg_method", ":", "str", ",", "\n", "bandwidth", ":", "Optional", "[", "float", "]", ",", "\n", ")", "->", "None", ":", "\n", "    ", "def", "process", "(", "i", ":", "int", ")", ":", "\n", "# synthetic data generator", "\n", "        ", "dataset", "=", "SyntheticContinuousBanditDataset", "(", "\n", "dim_context", "=", "dim_context", ",", "\n", "action_noise", "=", "action_noise", ",", "\n", "reward_noise", "=", "reward_noise", ",", "\n", "min_action_value", "=", "min_action_value", ",", "\n", "max_action_value", "=", "max_action_value", ",", "\n", "reward_function", "=", "linear_reward_funcion_continuous", ",", "\n", "behavior_policy_function", "=", "linear_behavior_policy_continuous", ",", "\n", "random_state", "=", "i", ",", "\n", ")", "\n", "# define evaluation policy using NNPolicyLearner", "\n", "nn_policy", "=", "ContinuousNNPolicyLearner", "(", "\n", "dim_context", "=", "dim_context", ",", "\n", "pg_method", "=", "pg_method", ",", "\n", "bandwidth", "=", "bandwidth", ",", "\n", "output_space", "=", "(", "min_action_value", ",", "max_action_value", ")", ",", "\n", "hidden_layer_size", "=", "(", "10", ",", "10", ")", ",", "\n", "learning_rate_init", "=", "0.001", ",", "\n", "max_iter", "=", "200", ",", "\n", "solver", "=", "\"sgd\"", ",", "\n", "q_func_estimator_hyperparams", "=", "{", "\"max_iter\"", ":", "200", "}", ",", "\n", ")", "\n", "# baseline method 1. RandomPolicy", "\n", "random_policy", "=", "RandomPolicy", "(", "output_space", "=", "(", "min_action_value", ",", "max_action_value", ")", ")", "\n", "# sample new training and test sets of synthetic logged bandit data", "\n", "bandit_feedback_train", "=", "dataset", ".", "obtain_batch_bandit_feedback", "(", "\n", "n_rounds", "=", "n_rounds", ",", "\n", ")", "\n", "bandit_feedback_test", "=", "dataset", ".", "obtain_batch_bandit_feedback", "(", "\n", "n_rounds", "=", "n_rounds", ",", "\n", ")", "\n", "# train the evaluation policy on the training set of the synthetic logged bandit data", "\n", "nn_policy", ".", "fit", "(", "\n", "context", "=", "bandit_feedback_train", "[", "\"context\"", "]", ",", "\n", "action", "=", "bandit_feedback_train", "[", "\"action\"", "]", ",", "\n", "reward", "=", "bandit_feedback_train", "[", "\"reward\"", "]", ",", "\n", "pscore", "=", "bandit_feedback_train", "[", "\"pscore\"", "]", ",", "\n", ")", "\n", "# predict the action decisions for the test set of the synthetic logged bandit data", "\n", "actions_predicted_by_nn_policy", "=", "nn_policy", ".", "predict", "(", "\n", "context", "=", "bandit_feedback_test", "[", "\"context\"", "]", ",", "\n", ")", "\n", "actions_predicted_by_random", "=", "random_policy", ".", "predict", "(", "\n", "context", "=", "bandit_feedback_test", "[", "\"context\"", "]", ",", "\n", ")", "\n", "# get the ground truth policy value for each learner", "\n", "gt_nn_policy_learner", "=", "dataset", ".", "calc_ground_truth_policy_value", "(", "\n", "context", "=", "bandit_feedback_test", "[", "\"context\"", "]", ",", "\n", "action", "=", "actions_predicted_by_nn_policy", ",", "\n", ")", "\n", "gt_random_policy", "=", "dataset", ".", "calc_ground_truth_policy_value", "(", "\n", "context", "=", "bandit_feedback_test", "[", "\"context\"", "]", ",", "\n", "action", "=", "actions_predicted_by_random", ",", "\n", ")", "\n", "\n", "return", "gt_nn_policy_learner", ",", "gt_random_policy", "\n", "\n", "", "n_runs", "=", "10", "\n", "processed", "=", "Parallel", "(", "\n", "n_jobs", "=", "1", ",", "# PyTorch uses multiple threads", "\n", "verbose", "=", "0", ",", "\n", ")", "(", "[", "delayed", "(", "process", ")", "(", "i", ")", "for", "i", "in", "np", ".", "arange", "(", "n_runs", ")", "]", ")", "\n", "list_gt_nn_policy", ",", "list_gt_random", "=", "[", "]", ",", "[", "]", "\n", "for", "i", ",", "ground_truth_policy_values", "in", "enumerate", "(", "processed", ")", ":", "\n", "        ", "gt_nn_policy", ",", "gt_random", "=", "ground_truth_policy_values", "\n", "list_gt_nn_policy", ".", "append", "(", "gt_nn_policy", ")", "\n", "list_gt_random", ".", "append", "(", "gt_random", ")", "\n", "\n", "", "assert", "np", ".", "mean", "(", "list_gt_nn_policy", ")", ">", "np", ".", "mean", "(", "list_gt_random", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.test_contextfree.test_contextfree_base_exception": [[10, 38], ["pytest.raises", "obp.policy.contextfree.EpsilonGreedy", "pytest.raises", "obp.policy.contextfree.EpsilonGreedy", "pytest.raises", "obp.policy.contextfree.EpsilonGreedy", "pytest.raises", "obp.policy.contextfree.EpsilonGreedy", "pytest.raises", "obp.policy.contextfree.EpsilonGreedy", "pytest.raises", "obp.policy.contextfree.EpsilonGreedy", "pytest.raises", "obp.policy.contextfree.EpsilonGreedy", "pytest.raises", "obp.policy.contextfree.EpsilonGreedy"], "function", ["None"], ["def", "test_contextfree_base_exception", "(", ")", ":", "\n", "# invalid n_actions", "\n", "    ", "with", "pytest", ".", "raises", "(", "ValueError", ")", ":", "\n", "        ", "EpsilonGreedy", "(", "n_actions", "=", "0", ")", "\n", "\n", "", "with", "pytest", ".", "raises", "(", "TypeError", ")", ":", "\n", "        ", "EpsilonGreedy", "(", "n_actions", "=", "\"3\"", ")", "\n", "\n", "# invalid len_list", "\n", "", "with", "pytest", ".", "raises", "(", "ValueError", ")", ":", "\n", "        ", "EpsilonGreedy", "(", "n_actions", "=", "2", ",", "len_list", "=", "-", "1", ")", "\n", "\n", "", "with", "pytest", ".", "raises", "(", "TypeError", ")", ":", "\n", "        ", "EpsilonGreedy", "(", "n_actions", "=", "2", ",", "len_list", "=", "\"5\"", ")", "\n", "\n", "# invalid batch_size", "\n", "", "with", "pytest", ".", "raises", "(", "ValueError", ")", ":", "\n", "        ", "EpsilonGreedy", "(", "n_actions", "=", "2", ",", "batch_size", "=", "-", "3", ")", "\n", "\n", "", "with", "pytest", ".", "raises", "(", "TypeError", ")", ":", "\n", "        ", "EpsilonGreedy", "(", "n_actions", "=", "2", ",", "batch_size", "=", "\"3\"", ")", "\n", "\n", "# invalid relationship between n_actions and len_list", "\n", "", "with", "pytest", ".", "raises", "(", "ValueError", ")", ":", "\n", "        ", "EpsilonGreedy", "(", "n_actions", "=", "5", ",", "len_list", "=", "10", ")", "\n", "\n", "", "with", "pytest", ".", "raises", "(", "ValueError", ")", ":", "\n", "        ", "EpsilonGreedy", "(", "n_actions", "=", "2", ",", "len_list", "=", "3", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.test_contextfree.test_egreedy_normal_epsilon": [[40, 50], ["obp.policy.contextfree.EpsilonGreedy", "obp.policy.contextfree.EpsilonGreedy", "obp.policy.contextfree.EpsilonGreedy"], "function", ["None"], ["", "", "def", "test_egreedy_normal_epsilon", "(", ")", ":", "\n", "\n", "    ", "policy1", "=", "EpsilonGreedy", "(", "n_actions", "=", "2", ")", "\n", "assert", "0", "<=", "policy1", ".", "epsilon", "<=", "1", "\n", "\n", "policy2", "=", "EpsilonGreedy", "(", "n_actions", "=", "3", ",", "epsilon", "=", "0.3", ")", "\n", "assert", "0", "<=", "policy2", ".", "epsilon", "<=", "1", "\n", "\n", "# policy type", "\n", "assert", "EpsilonGreedy", "(", "n_actions", "=", "2", ")", ".", "policy_type", "==", "PolicyType", ".", "CONTEXT_FREE", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.test_contextfree.test_egreedy_abnormal_epsilon": [[52, 59], ["pytest.raises", "obp.policy.contextfree.EpsilonGreedy", "pytest.raises", "obp.policy.contextfree.EpsilonGreedy"], "function", ["None"], ["", "def", "test_egreedy_abnormal_epsilon", "(", ")", ":", "\n", "\n", "    ", "with", "pytest", ".", "raises", "(", "ValueError", ")", ":", "\n", "        ", "EpsilonGreedy", "(", "n_actions", "=", "2", ",", "epsilon", "=", "1.2", ")", "\n", "\n", "", "with", "pytest", ".", "raises", "(", "ValueError", ")", ":", "\n", "        ", "EpsilonGreedy", "(", "n_actions", "=", "5", ",", "epsilon", "=", "-", "0.2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.test_contextfree.test_egreedy_select_action_exploitation": [[61, 68], ["obp.policy.contextfree.EpsilonGreedy", "numpy.array", "numpy.array", "range", "obp.policy.contextfree.EpsilonGreedy.select_action"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.policy.linear.LinTS.select_action"], ["", "", "def", "test_egreedy_select_action_exploitation", "(", ")", ":", "\n", "    ", "trial_num", "=", "50", "\n", "policy", "=", "EpsilonGreedy", "(", "n_actions", "=", "2", ",", "epsilon", "=", "0.0", ")", "\n", "policy", ".", "action_counts", "=", "np", ".", "array", "(", "[", "3", ",", "3", "]", ")", "\n", "policy", ".", "reward_counts", "=", "np", ".", "array", "(", "[", "3", ",", "0", "]", ")", "\n", "for", "_", "in", "range", "(", "trial_num", ")", ":", "\n", "        ", "assert", "policy", ".", "select_action", "(", ")", "[", "0", "]", "==", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.test_contextfree.test_egreedy_select_action_exploration": [[70, 77], ["obp.policy.contextfree.EpsilonGreedy", "numpy.array", "numpy.array", "obp.policy.contextfree.EpsilonGreedy.select_action", "range", "sum"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.policy.linear.LinTS.select_action"], ["", "", "def", "test_egreedy_select_action_exploration", "(", ")", ":", "\n", "    ", "trial_num", "=", "50", "\n", "policy", "=", "EpsilonGreedy", "(", "n_actions", "=", "2", ",", "epsilon", "=", "1.0", ")", "\n", "policy", ".", "action_counts", "=", "np", ".", "array", "(", "[", "3", ",", "3", "]", ")", "\n", "policy", ".", "reward_counts", "=", "np", ".", "array", "(", "[", "3", ",", "0", "]", ")", "\n", "selected_action", "=", "[", "policy", ".", "select_action", "(", ")", "for", "_", "in", "range", "(", "trial_num", ")", "]", "\n", "assert", "0", "<", "sum", "(", "selected_action", ")", "[", "0", "]", "<", "trial_num", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.test_contextfree.test_egreedy_update_params": [[79, 90], ["obp.policy.contextfree.EpsilonGreedy", "numpy.array", "numpy.copy", "numpy.array", "numpy.copy", "obp.policy.contextfree.EpsilonGreedy.update_params", "numpy.array_equal", "numpy.allclose", "numpy.array", "numpy.array"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.policy.linear.BaseLinPolicy.update_params"], ["", "def", "test_egreedy_update_params", "(", ")", ":", "\n", "    ", "policy", "=", "EpsilonGreedy", "(", "n_actions", "=", "2", ",", "epsilon", "=", "1.0", ")", "\n", "policy", ".", "action_counts_temp", "=", "np", ".", "array", "(", "[", "4", ",", "3", "]", ")", "\n", "policy", ".", "action_counts", "=", "np", ".", "copy", "(", "policy", ".", "action_counts_temp", ")", "\n", "policy", ".", "reward_counts_temp", "=", "np", ".", "array", "(", "[", "2.0", ",", "0.0", "]", ")", "\n", "policy", ".", "reward_counts", "=", "np", ".", "copy", "(", "policy", ".", "reward_counts_temp", ")", "\n", "action", "=", "0", "\n", "reward", "=", "1.0", "\n", "policy", ".", "update_params", "(", "action", ",", "reward", ")", "\n", "assert", "np", ".", "array_equal", "(", "policy", ".", "action_counts", ",", "np", ".", "array", "(", "[", "5", ",", "3", "]", ")", ")", "\n", "assert", "np", ".", "allclose", "(", "policy", ".", "reward_counts", ",", "np", ".", "array", "(", "[", "2.0", "+", "reward", ",", "0.0", "]", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.test_contextfree.test_random_compute_batch_action_dist": [[92, 103], ["obp.policy.contextfree.Random", "obp.policy.contextfree.Random.compute_batch_action_dist", "len", "numpy.unique", "numpy.unique"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.policy.contextfree.BernoulliTS.compute_batch_action_dist"], ["", "def", "test_random_compute_batch_action_dist", "(", ")", ":", "\n", "    ", "n_actions", "=", "10", "\n", "len_list", "=", "5", "\n", "n_rounds", "=", "100", "\n", "policy", "=", "Random", "(", "n_actions", "=", "n_actions", ",", "len_list", "=", "len_list", ")", "\n", "action_dist", "=", "policy", ".", "compute_batch_action_dist", "(", "n_rounds", "=", "n_rounds", ")", "\n", "assert", "action_dist", ".", "shape", "[", "0", "]", "==", "n_rounds", "\n", "assert", "action_dist", ".", "shape", "[", "1", "]", "==", "n_actions", "\n", "assert", "action_dist", ".", "shape", "[", "2", "]", "==", "len_list", "\n", "assert", "len", "(", "np", ".", "unique", "(", "action_dist", ")", ")", "==", "1", "\n", "assert", "np", ".", "unique", "(", "action_dist", ")", "[", "0", "]", "==", "1", "/", "n_actions", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.test_contextfree.test_bernoulli_ts_zozotown_prior": [[105, 122], ["obp.policy.contextfree.BernoulliTS", "obp.policy.contextfree.BernoulliTS", "obp.policy.contextfree.BernoulliTS", "pytest.raises", "obp.policy.contextfree.BernoulliTS", "len", "len", "len", "len", "len", "len", "numpy.unique", "numpy.unique", "numpy.unique", "numpy.unique", "numpy.unique", "numpy.unique"], "function", ["None"], ["", "def", "test_bernoulli_ts_zozotown_prior", "(", ")", ":", "\n", "\n", "    ", "with", "pytest", ".", "raises", "(", "Exception", ")", ":", "\n", "        ", "BernoulliTS", "(", "n_actions", "=", "2", ",", "is_zozotown_prior", "=", "True", ")", "\n", "\n", "", "policy_all", "=", "BernoulliTS", "(", "n_actions", "=", "2", ",", "is_zozotown_prior", "=", "True", ",", "campaign", "=", "\"all\"", ")", "\n", "# check whether it is not an non-informative prior parameter (i.e., default parameter)", "\n", "assert", "len", "(", "np", ".", "unique", "(", "policy_all", ".", "alpha", ")", ")", "!=", "1", "\n", "assert", "len", "(", "np", ".", "unique", "(", "policy_all", ".", "beta", ")", ")", "!=", "1", "\n", "\n", "policy_men", "=", "BernoulliTS", "(", "n_actions", "=", "2", ",", "is_zozotown_prior", "=", "True", ",", "campaign", "=", "\"men\"", ")", "\n", "assert", "len", "(", "np", ".", "unique", "(", "policy_men", ".", "alpha", ")", ")", "!=", "1", "\n", "assert", "len", "(", "np", ".", "unique", "(", "policy_men", ".", "beta", ")", ")", "!=", "1", "\n", "\n", "policy_women", "=", "BernoulliTS", "(", "n_actions", "=", "2", ",", "is_zozotown_prior", "=", "True", ",", "campaign", "=", "\"women\"", ")", "\n", "assert", "len", "(", "np", ".", "unique", "(", "policy_women", ".", "alpha", ")", ")", "!=", "1", "\n", "assert", "len", "(", "np", ".", "unique", "(", "policy_women", ".", "beta", ")", ")", "!=", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.test_contextfree.test_bernoulli_ts_select_action": [[124, 137], ["obp.policy.contextfree.BernoulliTS", "numpy.allclose", "obp.policy.contextfree.BernoulliTS", "pytest.raises", "obp.policy.contextfree.BernoulliTS", "pytest.raises", "obp.policy.contextfree.BernoulliTS", "numpy.sort", "numpy.array", "len", "obp.policy.contextfree.BernoulliTS.select_action", "obp.policy.contextfree.BernoulliTS.select_action"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.policy.linear.LinTS.select_action", "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.linear.LinTS.select_action"], ["", "def", "test_bernoulli_ts_select_action", "(", ")", ":", "\n", "# invalid relationship between n_actions and len_list", "\n", "    ", "with", "pytest", ".", "raises", "(", "ValueError", ")", ":", "\n", "        ", "BernoulliTS", "(", "n_actions", "=", "5", ",", "len_list", "=", "10", ")", "\n", "\n", "", "with", "pytest", ".", "raises", "(", "ValueError", ")", ":", "\n", "        ", "BernoulliTS", "(", "n_actions", "=", "2", ",", "len_list", "=", "3", ")", "\n", "\n", "", "policy1", "=", "BernoulliTS", "(", "n_actions", "=", "3", ",", "len_list", "=", "3", ")", "\n", "assert", "np", ".", "allclose", "(", "np", ".", "sort", "(", "policy1", ".", "select_action", "(", ")", ")", ",", "np", ".", "array", "(", "[", "0", ",", "1", ",", "2", "]", ")", ")", "\n", "\n", "policy", "=", "BernoulliTS", "(", "n_actions", "=", "5", ",", "len_list", "=", "3", ")", "\n", "assert", "len", "(", "policy", ".", "select_action", "(", ")", ")", "==", "3", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.test_contextfree.test_bernoulli_ts_update_params": [[139, 152], ["obp.policy.contextfree.BernoulliTS", "numpy.array", "numpy.copy", "numpy.array", "numpy.copy", "obp.policy.contextfree.BernoulliTS.update_params", "numpy.array_equal", "numpy.allclose", "numpy.array", "numpy.array"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.policy.linear.BaseLinPolicy.update_params"], ["", "def", "test_bernoulli_ts_update_params", "(", ")", ":", "\n", "    ", "policy", "=", "BernoulliTS", "(", "n_actions", "=", "2", ")", "\n", "policy", ".", "action_counts_temp", "=", "np", ".", "array", "(", "[", "4", ",", "3", "]", ")", "\n", "policy", ".", "action_counts", "=", "np", ".", "copy", "(", "policy", ".", "action_counts_temp", ")", "\n", "policy", ".", "reward_counts_temp", "=", "np", ".", "array", "(", "[", "2.0", ",", "0.0", "]", ")", "\n", "policy", ".", "reward_counts", "=", "np", ".", "copy", "(", "policy", ".", "reward_counts_temp", ")", "\n", "action", "=", "0", "\n", "reward", "=", "1.0", "\n", "policy", ".", "update_params", "(", "action", ",", "reward", ")", "\n", "assert", "np", ".", "array_equal", "(", "policy", ".", "action_counts", ",", "np", ".", "array", "(", "[", "5", ",", "3", "]", ")", ")", "\n", "# in bernoulli ts, reward_counts is defined as the sum of observed rewards for each action", "\n", "next_reward", "=", "2.0", "+", "reward", "\n", "assert", "np", ".", "allclose", "(", "policy", ".", "reward_counts", ",", "np", ".", "array", "(", "[", "next_reward", ",", "0.0", "]", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.test_contextfree.test_bernoulli_ts_compute_batch_action_dist": [[154, 163], ["obp.policy.contextfree.BernoulliTS", "obp.policy.contextfree.BernoulliTS.compute_batch_action_dist"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.policy.contextfree.BernoulliTS.compute_batch_action_dist"], ["", "def", "test_bernoulli_ts_compute_batch_action_dist", "(", ")", ":", "\n", "    ", "n_rounds", "=", "10", "\n", "n_actions", "=", "5", "\n", "len_list", "=", "2", "\n", "policy", "=", "BernoulliTS", "(", "n_actions", "=", "n_actions", ",", "len_list", "=", "len_list", ")", "\n", "action_dist", "=", "policy", ".", "compute_batch_action_dist", "(", "n_rounds", "=", "n_rounds", ",", "n_sim", "=", "30", ")", "\n", "assert", "action_dist", ".", "shape", "[", "0", "]", "==", "n_rounds", "\n", "assert", "action_dist", ".", "shape", "[", "1", "]", "==", "n_actions", "\n", "assert", "action_dist", ".", "shape", "[", "2", "]", "==", "len_list", "\n", "", ""]], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.test_linear.test_linear_base_exception": [[10, 57], ["pytest.raises", "obp.policy.linear.LinEpsilonGreedy", "pytest.raises", "obp.policy.linear.LinEpsilonGreedy", "pytest.raises", "obp.policy.linear.LinEpsilonGreedy", "pytest.raises", "obp.policy.linear.LinEpsilonGreedy", "pytest.raises", "obp.policy.linear.LinEpsilonGreedy", "pytest.raises", "obp.policy.linear.LinEpsilonGreedy", "pytest.raises", "obp.policy.linear.LinEpsilonGreedy", "pytest.raises", "obp.policy.linear.LinEpsilonGreedy", "pytest.raises", "obp.policy.linear.LinEpsilonGreedy", "pytest.raises", "obp.policy.linear.LinEpsilonGreedy", "pytest.raises", "obp.policy.linear.LinEpsilonGreedy", "pytest.raises", "obp.policy.linear.LinEpsilonGreedy", "pytest.raises", "obp.policy.linear.LinEpsilonGreedy", "pytest.raises", "obp.policy.linear.LinEpsilonGreedy"], "function", ["None"], ["def", "test_linear_base_exception", "(", ")", ":", "\n", "# invalid dim", "\n", "    ", "with", "pytest", ".", "raises", "(", "ValueError", ")", ":", "\n", "        ", "LinEpsilonGreedy", "(", "n_actions", "=", "2", ",", "dim", "=", "-", "3", ")", "\n", "\n", "", "with", "pytest", ".", "raises", "(", "ValueError", ")", ":", "\n", "        ", "LinEpsilonGreedy", "(", "n_actions", "=", "2", ",", "dim", "=", "0", ")", "\n", "\n", "", "with", "pytest", ".", "raises", "(", "TypeError", ")", ":", "\n", "        ", "LinEpsilonGreedy", "(", "n_actions", "=", "2", ",", "dim", "=", "\"3\"", ")", "\n", "\n", "# invalid n_actions", "\n", "", "with", "pytest", ".", "raises", "(", "ValueError", ")", ":", "\n", "        ", "LinEpsilonGreedy", "(", "n_actions", "=", "-", "3", ",", "dim", "=", "2", ")", "\n", "\n", "", "with", "pytest", ".", "raises", "(", "ValueError", ")", ":", "\n", "        ", "LinEpsilonGreedy", "(", "n_actions", "=", "1", ",", "dim", "=", "2", ")", "\n", "\n", "", "with", "pytest", ".", "raises", "(", "TypeError", ")", ":", "\n", "        ", "LinEpsilonGreedy", "(", "n_actions", "=", "\"2\"", ",", "dim", "=", "2", ")", "\n", "\n", "# invalid len_list", "\n", "", "with", "pytest", ".", "raises", "(", "ValueError", ")", ":", "\n", "        ", "LinEpsilonGreedy", "(", "n_actions", "=", "2", ",", "dim", "=", "2", ",", "len_list", "=", "-", "3", ")", "\n", "\n", "", "with", "pytest", ".", "raises", "(", "ValueError", ")", ":", "\n", "        ", "LinEpsilonGreedy", "(", "n_actions", "=", "2", ",", "dim", "=", "2", ",", "len_list", "=", "0", ")", "\n", "\n", "", "with", "pytest", ".", "raises", "(", "TypeError", ")", ":", "\n", "        ", "LinEpsilonGreedy", "(", "n_actions", "=", "2", ",", "dim", "=", "2", ",", "len_list", "=", "\"3\"", ")", "\n", "\n", "# invalid batch_size", "\n", "", "with", "pytest", ".", "raises", "(", "ValueError", ")", ":", "\n", "        ", "LinEpsilonGreedy", "(", "n_actions", "=", "2", ",", "dim", "=", "2", ",", "batch_size", "=", "-", "2", ")", "\n", "\n", "", "with", "pytest", ".", "raises", "(", "ValueError", ")", ":", "\n", "        ", "LinEpsilonGreedy", "(", "n_actions", "=", "2", ",", "dim", "=", "2", ",", "batch_size", "=", "0", ")", "\n", "\n", "", "with", "pytest", ".", "raises", "(", "TypeError", ")", ":", "\n", "        ", "LinEpsilonGreedy", "(", "n_actions", "=", "2", ",", "dim", "=", "2", ",", "batch_size", "=", "\"10\"", ")", "\n", "\n", "# invalid relationship between n_actions and len_list", "\n", "", "with", "pytest", ".", "raises", "(", "ValueError", ")", ":", "\n", "        ", "LinEpsilonGreedy", "(", "n_actions", "=", "5", ",", "len_list", "=", "10", ",", "dim", "=", "2", ")", "\n", "\n", "", "with", "pytest", ".", "raises", "(", "ValueError", ")", ":", "\n", "        ", "LinEpsilonGreedy", "(", "n_actions", "=", "2", ",", "len_list", "=", "3", ",", "dim", "=", "2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.test_linear.test_lin_epsilon_normal_epsilon": [[59, 66], ["obp.policy.linear.LinEpsilonGreedy", "obp.policy.linear.LinEpsilonGreedy"], "function", ["None"], ["", "", "def", "test_lin_epsilon_normal_epsilon", "(", ")", ":", "\n", "\n", "    ", "policy1", "=", "LinEpsilonGreedy", "(", "n_actions", "=", "2", ",", "dim", "=", "2", ")", "\n", "assert", "0", "<=", "policy1", ".", "epsilon", "<=", "1", "\n", "\n", "policy2", "=", "LinEpsilonGreedy", "(", "n_actions", "=", "2", ",", "dim", "=", "2", ",", "epsilon", "=", "0.3", ")", "\n", "assert", "policy2", ".", "epsilon", "==", "0.3", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.test_linear.test_lin_epsilon_abnormal_epsilon": [[68, 75], ["pytest.raises", "obp.policy.linear.LinEpsilonGreedy", "pytest.raises", "obp.policy.linear.LinEpsilonGreedy"], "function", ["None"], ["", "def", "test_lin_epsilon_abnormal_epsilon", "(", ")", ":", "\n", "\n", "    ", "with", "pytest", ".", "raises", "(", "ValueError", ")", ":", "\n", "        ", "LinEpsilonGreedy", "(", "n_actions", "=", "2", ",", "dim", "=", "2", ",", "epsilon", "=", "1.2", ")", "\n", "\n", "", "with", "pytest", ".", "raises", "(", "ValueError", ")", ":", "\n", "        ", "LinEpsilonGreedy", "(", "n_actions", "=", "2", ",", "dim", "=", "2", ",", "epsilon", "=", "-", "0.2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.test_linear.test_lin_epsilon_select_action_exploitation": [[77, 91], ["obp.policy.linear.LinEpsilonGreedy", "numpy.array().reshape", "obp.policy.linear.LinEpsilonGreedy.update_params", "obp.policy.linear.LinEpsilonGreedy.update_params", "obp.policy.linear.LinEpsilonGreedy.update_params", "obp.policy.linear.LinEpsilonGreedy.update_params", "range", "numpy.array", "obp.policy.linear.LinEpsilonGreedy.select_action"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.policy.linear.BaseLinPolicy.update_params", "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.linear.BaseLinPolicy.update_params", "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.linear.BaseLinPolicy.update_params", "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.linear.BaseLinPolicy.update_params", "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.linear.LinTS.select_action"], ["", "", "def", "test_lin_epsilon_select_action_exploitation", "(", ")", ":", "\n", "    ", "trial_num", "=", "50", "\n", "policy", "=", "LinEpsilonGreedy", "(", "n_actions", "=", "2", ",", "dim", "=", "2", ",", "epsilon", "=", "0.0", ")", "\n", "\n", "# policy type", "\n", "assert", "policy", ".", "policy_type", "==", "PolicyType", ".", "CONTEXTUAL", "\n", "\n", "context", "=", "np", ".", "array", "(", "[", "1.0", ",", "1.0", "]", ")", ".", "reshape", "(", "1", ",", "-", "1", ")", "\n", "policy", ".", "update_params", "(", "action", "=", "0", ",", "reward", "=", "1.0", ",", "context", "=", "context", ")", "\n", "policy", ".", "update_params", "(", "action", "=", "0", ",", "reward", "=", "1.0", ",", "context", "=", "context", ")", "\n", "policy", ".", "update_params", "(", "action", "=", "1", ",", "reward", "=", "1.0", ",", "context", "=", "context", ")", "\n", "policy", ".", "update_params", "(", "action", "=", "1", ",", "reward", "=", "0.0", ",", "context", "=", "context", ")", "\n", "for", "_", "in", "range", "(", "trial_num", ")", ":", "\n", "        ", "assert", "policy", ".", "select_action", "(", "context", "=", "context", ")", "[", "0", "]", "==", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.test_linear.test_lin_epsilon_select_action_exploration": [[93, 103], ["obp.policy.linear.LinEpsilonGreedy", "numpy.array().reshape", "obp.policy.linear.LinEpsilonGreedy.update_params", "obp.policy.linear.LinEpsilonGreedy.update_params", "obp.policy.linear.LinEpsilonGreedy.update_params", "obp.policy.linear.LinEpsilonGreedy.update_params", "obp.policy.linear.LinEpsilonGreedy.select_action", "numpy.array", "range", "sum"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.policy.linear.BaseLinPolicy.update_params", "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.linear.BaseLinPolicy.update_params", "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.linear.BaseLinPolicy.update_params", "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.linear.BaseLinPolicy.update_params", "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.linear.LinTS.select_action"], ["", "", "def", "test_lin_epsilon_select_action_exploration", "(", ")", ":", "\n", "    ", "trial_num", "=", "50", "\n", "policy", "=", "LinEpsilonGreedy", "(", "n_actions", "=", "2", ",", "dim", "=", "2", ",", "epsilon", "=", "1.0", ")", "\n", "context", "=", "np", ".", "array", "(", "[", "1.0", ",", "1.0", "]", ")", ".", "reshape", "(", "1", ",", "-", "1", ")", "\n", "policy", ".", "update_params", "(", "action", "=", "0", ",", "reward", "=", "1.0", ",", "context", "=", "context", ")", "\n", "policy", ".", "update_params", "(", "action", "=", "0", ",", "reward", "=", "1.0", ",", "context", "=", "context", ")", "\n", "policy", ".", "update_params", "(", "action", "=", "1", ",", "reward", "=", "1.0", ",", "context", "=", "context", ")", "\n", "policy", ".", "update_params", "(", "action", "=", "1", ",", "reward", "=", "0.0", ",", "context", "=", "context", ")", "\n", "selected_action", "=", "[", "policy", ".", "select_action", "(", "context", "=", "context", ")", "for", "_", "in", "range", "(", "trial_num", ")", "]", "\n", "assert", "0", "<", "sum", "(", "selected_action", ")", "[", "0", "]", "<", "trial_num", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.test_linear.test_lin_epsilon_update_params": [[105, 120], ["obp.policy.linear.LinEpsilonGreedy", "numpy.array().reshape", "numpy.array", "numpy.array", "numpy.copy", "numpy.copy", "obp.policy.linear.LinEpsilonGreedy.update_params", "numpy.allclose", "numpy.allclose", "numpy.array", "numpy.array"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.policy.linear.BaseLinPolicy.update_params"], ["", "def", "test_lin_epsilon_update_params", "(", ")", ":", "\n", "# check the consistency with Sherman\u2013Morrison formula", "\n", "    ", "policy", "=", "LinEpsilonGreedy", "(", "n_actions", "=", "2", ",", "dim", "=", "2", ",", "epsilon", "=", "1.0", ")", "\n", "action", "=", "0", "\n", "reward", "=", "1.0", "\n", "context", "=", "np", ".", "array", "(", "[", "1", ",", "0", "]", ")", ".", "reshape", "(", "1", ",", "-", "1", ")", "\n", "A_inv_temp", "=", "np", ".", "array", "(", "[", "[", "1", "/", "2", ",", "0", "]", ",", "[", "0", ",", "1", "]", "]", ")", "\n", "b_temp", "=", "np", ".", "array", "(", "[", "1", ",", "1", "]", ")", "\n", "policy", ".", "A_inv_temp", "[", "action", "]", "=", "np", ".", "copy", "(", "A_inv_temp", ")", "\n", "policy", ".", "b_temp", "[", ":", ",", "action", "]", "=", "np", ".", "copy", "(", "b_temp", ")", "\n", "policy", ".", "update_params", "(", "action", "=", "action", ",", "reward", "=", "reward", ",", "context", "=", "context", ")", "\n", "next_A_inv", "=", "A_inv_temp", "-", "np", ".", "array", "(", "[", "[", "1", "/", "4", ",", "0", "]", ",", "[", "0", ",", "0", "]", "]", ")", "/", "(", "1", "+", "1", "/", "2", ")", "\n", "next_b", "=", "b_temp", "+", "reward", "*", "context", "\n", "assert", "np", ".", "allclose", "(", "policy", ".", "A_inv", "[", "action", "]", ",", "next_A_inv", ")", "\n", "assert", "np", ".", "allclose", "(", "policy", ".", "b", "[", ":", ",", "action", "]", ",", "next_b", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.test_linear.test_lin_ucb_initialize": [[122, 135], ["obp.policy.linear.LinUCB", "pytest.raises", "obp.policy.linear.LinUCB"], "function", ["None"], ["", "def", "test_lin_ucb_initialize", "(", ")", ":", "\n", "# note that the meaning of epsilon is different from that of LinEpsilonGreedy", "\n", "    ", "with", "pytest", ".", "raises", "(", "ValueError", ")", ":", "\n", "        ", "LinUCB", "(", "n_actions", "=", "2", ",", "dim", "=", "2", ",", "epsilon", "=", "-", "0.2", ")", "\n", "\n", "", "n_actions", "=", "3", "\n", "dim", "=", "2", "\n", "policy", "=", "LinUCB", "(", "n_actions", "=", "n_actions", ",", "dim", "=", "dim", ",", "epsilon", "=", "2.0", ")", "\n", "assert", "policy", ".", "theta_hat", ".", "shape", "==", "(", "dim", ",", "n_actions", ")", "\n", "assert", "policy", ".", "A_inv", ".", "shape", "==", "(", "n_actions", ",", "dim", ",", "dim", ")", "\n", "assert", "policy", ".", "b", ".", "shape", "==", "(", "dim", ",", "n_actions", ")", "\n", "assert", "policy", ".", "A_inv_temp", ".", "shape", "==", "(", "n_actions", ",", "dim", ",", "dim", ")", "\n", "assert", "policy", ".", "b_temp", ".", "shape", "==", "(", "dim", ",", "n_actions", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.test_linear.test_lin_ucb_select_action": [[137, 144], ["obp.policy.linear.LinUCB", "numpy.ones().reshape", "obp.policy.linear.LinUCB.select_action", "len", "numpy.ones"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.policy.linear.LinTS.select_action"], ["", "def", "test_lin_ucb_select_action", "(", ")", ":", "\n", "    ", "dim", "=", "3", "\n", "len_list", "=", "2", "\n", "policy", "=", "LinUCB", "(", "n_actions", "=", "4", ",", "dim", "=", "dim", ",", "len_list", "=", "2", ",", "epsilon", "=", "0.0", ")", "\n", "context", "=", "np", ".", "ones", "(", "dim", ")", ".", "reshape", "(", "1", ",", "-", "1", ")", "\n", "action", "=", "policy", ".", "select_action", "(", "context", "=", "context", ")", "\n", "assert", "len", "(", "action", ")", "==", "len_list", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.test_linear.test_lin_ucb_update_params": [[146, 161], ["obp.policy.linear.LinUCB", "numpy.array().reshape", "numpy.array", "numpy.array", "numpy.copy", "numpy.copy", "obp.policy.linear.LinUCB.update_params", "numpy.allclose", "numpy.allclose", "numpy.array", "numpy.array"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.policy.linear.BaseLinPolicy.update_params"], ["", "def", "test_lin_ucb_update_params", "(", ")", ":", "\n", "# check the consistency with Sherman\u2013Morrison formula", "\n", "    ", "policy", "=", "LinUCB", "(", "n_actions", "=", "2", ",", "dim", "=", "2", ",", "epsilon", "=", "1.0", ")", "\n", "action", "=", "0", "\n", "reward", "=", "1.0", "\n", "context", "=", "np", ".", "array", "(", "[", "1", ",", "0", "]", ")", ".", "reshape", "(", "1", ",", "-", "1", ")", "\n", "A_inv_temp", "=", "np", ".", "array", "(", "[", "[", "1", "/", "2", ",", "0", "]", ",", "[", "0", ",", "1", "]", "]", ")", "\n", "b_temp", "=", "np", ".", "array", "(", "[", "1", ",", "1", "]", ")", "\n", "policy", ".", "A_inv_temp", "[", "action", "]", "=", "np", ".", "copy", "(", "A_inv_temp", ")", "\n", "policy", ".", "b_temp", "[", ":", ",", "action", "]", "=", "np", ".", "copy", "(", "b_temp", ")", "\n", "policy", ".", "update_params", "(", "action", "=", "action", ",", "reward", "=", "reward", ",", "context", "=", "context", ")", "\n", "next_A_inv", "=", "A_inv_temp", "-", "np", ".", "array", "(", "[", "[", "1", "/", "4", ",", "0", "]", ",", "[", "0", ",", "0", "]", "]", ")", "/", "(", "1", "+", "1", "/", "2", ")", "\n", "next_b", "=", "b_temp", "+", "reward", "*", "context", "\n", "assert", "np", ".", "allclose", "(", "policy", ".", "A_inv", "[", "action", "]", ",", "next_A_inv", ")", "\n", "assert", "np", ".", "allclose", "(", "policy", ".", "b", "[", ":", ",", "action", "]", ",", "next_b", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.test_linear.test_lin_ts_initialize": [[163, 171], ["obp.policy.linear.LinTS"], "function", ["None"], ["", "def", "test_lin_ts_initialize", "(", ")", ":", "\n", "    ", "n_actions", "=", "3", "\n", "dim", "=", "2", "\n", "policy", "=", "LinTS", "(", "n_actions", "=", "n_actions", ",", "dim", "=", "dim", ")", "\n", "assert", "policy", ".", "A_inv", ".", "shape", "==", "(", "n_actions", ",", "dim", ",", "dim", ")", "\n", "assert", "policy", ".", "b", ".", "shape", "==", "(", "dim", ",", "n_actions", ")", "\n", "assert", "policy", ".", "A_inv_temp", ".", "shape", "==", "(", "n_actions", ",", "dim", ",", "dim", ")", "\n", "assert", "policy", ".", "b_temp", ".", "shape", "==", "(", "dim", ",", "n_actions", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.test_linear.test_lin_ts_select_action": [[173, 180], ["obp.policy.linear.LinTS", "numpy.ones().reshape", "obp.policy.linear.LinTS.select_action", "len", "numpy.ones"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.policy.linear.LinTS.select_action"], ["", "def", "test_lin_ts_select_action", "(", ")", ":", "\n", "    ", "dim", "=", "3", "\n", "len_list", "=", "2", "\n", "policy", "=", "LinTS", "(", "n_actions", "=", "4", ",", "dim", "=", "dim", ",", "len_list", "=", "2", ")", "\n", "context", "=", "np", ".", "ones", "(", "dim", ")", ".", "reshape", "(", "1", ",", "-", "1", ")", "\n", "action", "=", "policy", ".", "select_action", "(", "context", "=", "context", ")", "\n", "assert", "len", "(", "action", ")", "==", "len_list", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.test_linear.test_lin_ts_update_params": [[182, 197], ["obp.policy.linear.LinTS", "numpy.array().reshape", "numpy.array", "numpy.array", "numpy.copy", "numpy.copy", "obp.policy.linear.LinTS.update_params", "numpy.allclose", "numpy.allclose", "numpy.array", "numpy.array"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.policy.linear.BaseLinPolicy.update_params"], ["", "def", "test_lin_ts_update_params", "(", ")", ":", "\n", "# check the consistency with Sherman\u2013Morrison formula", "\n", "    ", "policy", "=", "LinTS", "(", "n_actions", "=", "2", ",", "dim", "=", "2", ")", "\n", "action", "=", "0", "\n", "reward", "=", "1.0", "\n", "context", "=", "np", ".", "array", "(", "[", "1", ",", "0", "]", ")", ".", "reshape", "(", "1", ",", "-", "1", ")", "\n", "A_inv_temp", "=", "np", ".", "array", "(", "[", "[", "1", "/", "2", ",", "0", "]", ",", "[", "0", ",", "1", "]", "]", ")", "\n", "b_temp", "=", "np", ".", "array", "(", "[", "1", ",", "1", "]", ")", "\n", "policy", ".", "A_inv_temp", "[", "action", "]", "=", "np", ".", "copy", "(", "A_inv_temp", ")", "\n", "policy", ".", "b_temp", "[", ":", ",", "action", "]", "=", "np", ".", "copy", "(", "b_temp", ")", "\n", "policy", ".", "update_params", "(", "action", "=", "action", ",", "reward", "=", "reward", ",", "context", "=", "context", ")", "\n", "next_A_inv", "=", "A_inv_temp", "-", "np", ".", "array", "(", "[", "[", "1", "/", "4", ",", "0", "]", ",", "[", "0", ",", "0", "]", "]", ")", "/", "(", "1", "+", "1", "/", "2", ")", "\n", "next_b", "=", "b_temp", "+", "reward", "*", "context", "\n", "assert", "np", ".", "allclose", "(", "policy", ".", "A_inv", "[", "action", "]", ",", "next_A_inv", ")", "\n", "assert", "np", ".", "allclose", "(", "policy", ".", "b", "[", ":", ",", "action", "]", ",", "next_b", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.test_offline_continuous.test_nn_policy_learner_init_using_invalid_inputs": [[720, 775], ["pytest.mark.parametrize", "pytest.raises", "obp.policy.offline_continuous.ContinuousNNPolicyLearner"], "function", ["None"], ["@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"dim_context, pg_method, bandwidth, output_space, hidden_layer_size, activation, solver, alpha, batch_size, learning_rate_init, max_iter, shuffle, random_state, tol, momentum, nesterovs_momentum, early_stopping, validation_fraction, beta_1, beta_2, epsilon, n_iter_no_change, q_func_estimator_hyperparams, description\"", ",", "\n", "invalid_input_of_nn_policy_learner_init", ",", "\n", ")", "\n", "def", "test_nn_policy_learner_init_using_invalid_inputs", "(", "\n", "dim_context", ",", "\n", "pg_method", ",", "\n", "bandwidth", ",", "\n", "output_space", ",", "\n", "hidden_layer_size", ",", "\n", "activation", ",", "\n", "solver", ",", "\n", "alpha", ",", "\n", "batch_size", ",", "\n", "learning_rate_init", ",", "\n", "max_iter", ",", "\n", "shuffle", ",", "\n", "random_state", ",", "\n", "tol", ",", "\n", "momentum", ",", "\n", "nesterovs_momentum", ",", "\n", "early_stopping", ",", "\n", "validation_fraction", ",", "\n", "beta_1", ",", "\n", "beta_2", ",", "\n", "epsilon", ",", "\n", "n_iter_no_change", ",", "\n", "q_func_estimator_hyperparams", ",", "\n", "description", ",", "\n", ")", ":", "\n", "    ", "with", "pytest", ".", "raises", "(", "ValueError", ",", "match", "=", "f\"{description}*\"", ")", ":", "\n", "        ", "_", "=", "ContinuousNNPolicyLearner", "(", "\n", "dim_context", "=", "dim_context", ",", "\n", "pg_method", "=", "pg_method", ",", "\n", "bandwidth", "=", "bandwidth", ",", "\n", "output_space", "=", "output_space", ",", "\n", "hidden_layer_size", "=", "hidden_layer_size", ",", "\n", "activation", "=", "activation", ",", "\n", "solver", "=", "solver", ",", "\n", "alpha", "=", "alpha", ",", "\n", "batch_size", "=", "batch_size", ",", "\n", "learning_rate_init", "=", "learning_rate_init", ",", "\n", "max_iter", "=", "max_iter", ",", "\n", "shuffle", "=", "shuffle", ",", "\n", "random_state", "=", "random_state", ",", "\n", "tol", "=", "tol", ",", "\n", "momentum", "=", "momentum", ",", "\n", "nesterovs_momentum", "=", "nesterovs_momentum", ",", "\n", "early_stopping", "=", "early_stopping", ",", "\n", "validation_fraction", "=", "validation_fraction", ",", "\n", "beta_1", "=", "beta_1", ",", "\n", "beta_2", "=", "beta_2", ",", "\n", "epsilon", "=", "epsilon", ",", "\n", "n_iter_no_change", "=", "n_iter_no_change", ",", "\n", "q_func_estimator_hyperparams", "=", "q_func_estimator_hyperparams", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.test_offline_continuous.test_nn_policy_learner_init_using_valid_inputs": [[778, 834], ["pytest.mark.parametrize", "obp.policy.offline_continuous.ContinuousNNPolicyLearner", "isinstance"], "function", ["None"], ["", "", "@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"dim_context, pg_method, bandwidth, output_space, hidden_layer_size, activation, solver, alpha, batch_size, learning_rate_init, max_iter, shuffle, random_state, tol, momentum, nesterovs_momentum, early_stopping, validation_fraction, beta_1, beta_2, epsilon, n_iter_no_change, q_func_estimator_hyperparams, description\"", ",", "\n", "valid_input_of_nn_policy_learner_init", ",", "\n", ")", "\n", "def", "test_nn_policy_learner_init_using_valid_inputs", "(", "\n", "dim_context", ",", "\n", "pg_method", ",", "\n", "bandwidth", ",", "\n", "output_space", ",", "\n", "hidden_layer_size", ",", "\n", "activation", ",", "\n", "solver", ",", "\n", "alpha", ",", "\n", "batch_size", ",", "\n", "learning_rate_init", ",", "\n", "max_iter", ",", "\n", "shuffle", ",", "\n", "random_state", ",", "\n", "tol", ",", "\n", "momentum", ",", "\n", "nesterovs_momentum", ",", "\n", "early_stopping", ",", "\n", "validation_fraction", ",", "\n", "beta_1", ",", "\n", "beta_2", ",", "\n", "epsilon", ",", "\n", "n_iter_no_change", ",", "\n", "q_func_estimator_hyperparams", ",", "\n", "description", ",", "\n", ")", ":", "\n", "    ", "nn_policy_learner", "=", "ContinuousNNPolicyLearner", "(", "\n", "dim_context", "=", "dim_context", ",", "\n", "pg_method", "=", "pg_method", ",", "\n", "bandwidth", "=", "bandwidth", ",", "\n", "output_space", "=", "output_space", ",", "\n", "hidden_layer_size", "=", "hidden_layer_size", ",", "\n", "activation", "=", "activation", ",", "\n", "solver", "=", "solver", ",", "\n", "alpha", "=", "alpha", ",", "\n", "batch_size", "=", "batch_size", ",", "\n", "learning_rate_init", "=", "learning_rate_init", ",", "\n", "max_iter", "=", "max_iter", ",", "\n", "shuffle", "=", "shuffle", ",", "\n", "random_state", "=", "random_state", ",", "\n", "tol", "=", "tol", ",", "\n", "momentum", "=", "momentum", ",", "\n", "nesterovs_momentum", "=", "nesterovs_momentum", ",", "\n", "early_stopping", "=", "early_stopping", ",", "\n", "validation_fraction", "=", "validation_fraction", ",", "\n", "beta_1", "=", "beta_1", ",", "\n", "beta_2", "=", "beta_2", ",", "\n", "epsilon", "=", "epsilon", ",", "\n", "n_iter_no_change", "=", "n_iter_no_change", ",", "\n", "q_func_estimator_hyperparams", "=", "q_func_estimator_hyperparams", ",", "\n", ")", "\n", "assert", "isinstance", "(", "nn_policy_learner", ",", "ContinuousNNPolicyLearner", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.test_offline_continuous.test_nn_policy_learner_create_train_data_for_opl": [[836, 868], ["numpy.ones", "numpy.zeros", "numpy.ones", "numpy.array", "obp.policy.offline_continuous.ContinuousNNPolicyLearner", "obp.policy.offline_continuous.ContinuousNNPolicyLearner._create_train_data_for_opl", "isinstance", "obp.policy.offline_continuous.ContinuousNNPolicyLearner", "obp.policy.offline_continuous.ContinuousNNPolicyLearner._create_train_data_for_opl", "isinstance", "isinstance"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.policy.offline.NNPolicyLearner._create_train_data_for_opl", "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.offline.NNPolicyLearner._create_train_data_for_opl"], ["", "def", "test_nn_policy_learner_create_train_data_for_opl", "(", ")", ":", "\n", "    ", "context", "=", "np", ".", "ones", "(", "(", "100", ",", "2", ")", ",", "dtype", "=", "np", ".", "int32", ")", "\n", "action", "=", "np", ".", "zeros", "(", "100", ",", "dtype", "=", "np", ".", "int32", ")", "\n", "reward", "=", "np", ".", "ones", "(", "(", "100", ",", ")", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "pscore", "=", "np", ".", "array", "(", "[", "0.5", "]", "*", "100", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "\n", "learner1", "=", "ContinuousNNPolicyLearner", "(", "dim_context", "=", "2", ",", "pg_method", "=", "\"dpg\"", ")", "\n", "training_loader", ",", "validation_loader", "=", "learner1", ".", "_create_train_data_for_opl", "(", "\n", "context", "=", "context", ",", "\n", "action", "=", "action", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore", "=", "pscore", ",", "\n", ")", "\n", "\n", "assert", "isinstance", "(", "training_loader", ",", "torch", ".", "utils", ".", "data", ".", "DataLoader", ")", "\n", "assert", "validation_loader", "is", "None", "\n", "\n", "learner2", "=", "ContinuousNNPolicyLearner", "(", "\n", "dim_context", "=", "2", ",", "\n", "pg_method", "=", "\"dpg\"", ",", "\n", "early_stopping", "=", "True", ",", "\n", ")", "\n", "\n", "training_loader", ",", "validation_loader", "=", "learner2", ".", "_create_train_data_for_opl", "(", "\n", "context", "=", "context", ",", "\n", "action", "=", "action", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore", "=", "pscore", ",", "\n", ")", "\n", "\n", "assert", "isinstance", "(", "training_loader", ",", "torch", ".", "utils", ".", "data", ".", "DataLoader", ")", "\n", "assert", "isinstance", "(", "validation_loader", ",", "torch", ".", "utils", ".", "data", ".", "DataLoader", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.test_offline_continuous.test_nn_policy_learner_fit_using_invalid_inputs": [[983, 1006], ["pytest.mark.parametrize", "pytest.raises", "obp.policy.offline_continuous.ContinuousNNPolicyLearner", "obp.policy.offline_continuous.ContinuousNNPolicyLearner.fit"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.classification_model.PropensityScoreEstimator.fit"], ["@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"context, action, reward, pscore, description\"", ",", "\n", "invalid_input_of_nn_policy_learner_fit", ",", "\n", ")", "\n", "def", "test_nn_policy_learner_fit_using_invalid_inputs", "(", "\n", "context", ",", "\n", "action", ",", "\n", "reward", ",", "\n", "pscore", ",", "\n", "description", ",", "\n", ")", ":", "\n", "    ", "with", "pytest", ".", "raises", "(", "ValueError", ",", "match", "=", "f\"{description}*\"", ")", ":", "\n", "# set parameters", "\n", "        ", "dim_context", "=", "2", "\n", "pg_method", "=", "\"dpg\"", "\n", "learner", "=", "ContinuousNNPolicyLearner", "(", "\n", "dim_context", "=", "dim_context", ",", "pg_method", "=", "pg_method", "\n", ")", "\n", "learner", ".", "fit", "(", "\n", "context", "=", "context", ",", "\n", "action", "=", "action", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore", "=", "pscore", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.test_offline_continuous.test_nn_policy_learner_fit_using_valid_inputs": [[1009, 1029], ["pytest.mark.parametrize", "obp.policy.offline_continuous.ContinuousNNPolicyLearner", "obp.policy.offline_continuous.ContinuousNNPolicyLearner.fit"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.classification_model.PropensityScoreEstimator.fit"], ["", "", "@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"context, action, reward, pscore, description\"", ",", "\n", "valid_input_of_nn_policy_learner_fit", ",", "\n", ")", "\n", "def", "test_nn_policy_learner_fit_using_valid_inputs", "(", "\n", "context", ",", "\n", "action", ",", "\n", "reward", ",", "\n", "pscore", ",", "\n", "description", ",", "\n", ")", ":", "\n", "# set parameters", "\n", "    ", "dim_context", "=", "2", "\n", "pg_method", "=", "\"dpg\"", "\n", "learner", "=", "ContinuousNNPolicyLearner", "(", "dim_context", "=", "dim_context", ",", "pg_method", "=", "pg_method", ")", "\n", "learner", ".", "fit", "(", "\n", "context", "=", "context", ",", "\n", "action", "=", "action", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore", "=", "pscore", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.test_offline_continuous.test_nn_policy_learner_predict": [[1032, 1068], ["numpy.ones", "numpy.ones", "numpy.ones", "obp.policy.offline_continuous.ContinuousNNPolicyLearner", "obp.policy.offline_continuous.ContinuousNNPolicyLearner.fit", "obp.policy.offline_continuous.ContinuousNNPolicyLearner.predict", "pytest.raises", "obp.policy.offline_continuous.ContinuousNNPolicyLearner.predict", "pytest.raises", "obp.policy.offline_continuous.ContinuousNNPolicyLearner.predict", "pytest.raises", "obp.policy.offline_continuous.ContinuousNNPolicyLearner.predict", "numpy.all", "numpy.all", "numpy.ones", "numpy.ones"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.classification_model.PropensityScoreEstimator.fit", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.classification_model.PropensityScoreEstimator.predict", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.classification_model.PropensityScoreEstimator.predict", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.classification_model.PropensityScoreEstimator.predict", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.classification_model.PropensityScoreEstimator.predict"], ["", "def", "test_nn_policy_learner_predict", "(", ")", ":", "\n", "# synthetic data", "\n", "    ", "context", "=", "np", ".", "ones", "(", "(", "5", ",", "2", ")", ")", "\n", "action", "=", "np", ".", "ones", "(", "5", ")", "\n", "reward", "=", "np", ".", "ones", "(", "5", ")", "\n", "\n", "# set parameters", "\n", "dim_context", "=", "2", "\n", "pg_method", "=", "\"dpg\"", "\n", "output_space", "=", "(", "-", "10", ",", "10", ")", "\n", "learner", "=", "ContinuousNNPolicyLearner", "(", "\n", "dim_context", "=", "dim_context", ",", "pg_method", "=", "pg_method", ",", "output_space", "=", "output_space", "\n", ")", "\n", "learner", ".", "fit", "(", "\n", "context", "=", "context", ",", "\n", "action", "=", "action", ",", "\n", "reward", "=", "reward", ",", "\n", ")", "\n", "\n", "# shape error", "\n", "with", "pytest", ".", "raises", "(", "ValueError", ",", "match", "=", "\"`context` must be 2D array\"", ")", ":", "\n", "        ", "learner", ".", "predict", "(", "context", "=", "np", ".", "ones", "(", "5", ")", ")", "\n", "\n", "", "with", "pytest", ".", "raises", "(", "ValueError", ",", "match", "=", "\"`context` must be 2D array\"", ")", ":", "\n", "        ", "learner", ".", "predict", "(", "context", "=", "\"np.ones(5)\"", ")", "\n", "\n", "# inconsistency between dim_context and context", "\n", "", "with", "pytest", ".", "raises", "(", "ValueError", ",", "match", "=", "\"Expected `context.shape[1]*\"", ")", ":", "\n", "        ", "learner", ".", "predict", "(", "context", "=", "np", ".", "ones", "(", "(", "5", ",", "3", ")", ")", ")", "\n", "\n", "# check output shape", "\n", "", "predicted_actions", "=", "learner", ".", "predict", "(", "context", "=", "context", ")", "\n", "assert", "predicted_actions", ".", "shape", "[", "0", "]", "==", "context", ".", "shape", "[", "0", "]", "\n", "assert", "predicted_actions", ".", "ndim", "==", "1", "\n", "assert", "np", ".", "all", "(", "output_space", "[", "0", "]", "<=", "predicted_actions", ")", "or", "np", ".", "all", "(", "\n", "predicted_actions", "<=", "output_space", "[", "1", "]", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.test_offline.test_ipw_learner_init_using_invalid_inputs": [[55, 70], ["pytest.mark.parametrize", "pytest.raises", "obp.policy.offline.IPWLearner"], "function", ["None"], ["@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"n_actions, len_list, base_classifier, description\"", ",", "\n", "invalid_input_of_ipw_learner_init", ",", "\n", ")", "\n", "def", "test_ipw_learner_init_using_invalid_inputs", "(", "\n", "n_actions", ",", "\n", "len_list", ",", "\n", "base_classifier", ",", "\n", "description", ",", "\n", ")", ":", "\n", "    ", "with", "pytest", ".", "raises", "(", "ValueError", ",", "match", "=", "f\"{description}*\"", ")", ":", "\n", "        ", "_", "=", "IPWLearner", "(", "\n", "n_actions", "=", "n_actions", ",", "\n", "len_list", "=", "len_list", ",", "\n", "base_classifier", "=", "base_classifier", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.test_offline.test_ipw_learner_init_using_valid_inputs": [[73, 90], ["pytest.mark.parametrize", "obp.policy.offline.IPWLearner"], "function", ["None"], ["", "", "@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"n_actions, len_list, base_classifier, description\"", ",", "\n", "valid_input_of_ipw_learner_init", ",", "\n", ")", "\n", "def", "test_ipw_learner_init_using_valid_inputs", "(", "\n", "n_actions", ",", "\n", "len_list", ",", "\n", "base_classifier", ",", "\n", "description", ",", "\n", ")", ":", "\n", "    ", "ipw_learner", "=", "IPWLearner", "(", "\n", "n_actions", "=", "n_actions", ",", "\n", "len_list", "=", "len_list", ",", "\n", "base_classifier", "=", "base_classifier", ",", "\n", ")", "\n", "# policy_type", "\n", "assert", "ipw_learner", ".", "policy_type", "==", "PolicyType", ".", "OFFLINE", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.test_offline.test_ipw_learner_init_base_classifier_list": [[92, 106], ["obp.policy.offline.IPWLearner", "isinstance", "range", "obp.policy.offline.IPWLearner", "isinstance", "range", "isinstance", "isinstance", "GaussianNB"], "function", ["None"], ["", "def", "test_ipw_learner_init_base_classifier_list", "(", ")", ":", "\n", "# base classifier", "\n", "    ", "len_list", "=", "2", "\n", "learner1", "=", "IPWLearner", "(", "n_actions", "=", "2", ",", "len_list", "=", "len_list", ")", "\n", "assert", "isinstance", "(", "learner1", ".", "base_classifier", ",", "LogisticRegression", ")", "\n", "for", "i", "in", "range", "(", "len_list", ")", ":", "\n", "        ", "assert", "isinstance", "(", "learner1", ".", "base_classifier_list", "[", "i", "]", ",", "LogisticRegression", ")", "\n", "\n", "", "from", "sklearn", ".", "naive_bayes", "import", "GaussianNB", "\n", "\n", "learner2", "=", "IPWLearner", "(", "n_actions", "=", "2", ",", "len_list", "=", "len_list", ",", "base_classifier", "=", "GaussianNB", "(", ")", ")", "\n", "assert", "isinstance", "(", "learner2", ".", "base_classifier", ",", "GaussianNB", ")", "\n", "for", "i", "in", "range", "(", "len_list", ")", ":", "\n", "        ", "assert", "isinstance", "(", "learner2", ".", "base_classifier_list", "[", "i", "]", ",", "GaussianNB", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.test_offline.test_ipw_learner_create_train_data_for_opl": [[108, 122], ["numpy.array().reshape", "obp.policy.offline.IPWLearner", "numpy.array", "numpy.array", "numpy.array", "obp.policy.offline.IPWLearner._create_train_data_for_opl", "numpy.allclose", "numpy.allclose", "numpy.allclose", "numpy.array().reshape", "numpy.array", "numpy.array", "numpy.array", "numpy.array"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.policy.offline.NNPolicyLearner._create_train_data_for_opl"], ["", "", "def", "test_ipw_learner_create_train_data_for_opl", "(", ")", ":", "\n", "    ", "context", "=", "np", ".", "array", "(", "[", "1.0", ",", "1.0", "]", ")", ".", "reshape", "(", "1", ",", "-", "1", ")", "\n", "learner", "=", "IPWLearner", "(", "n_actions", "=", "2", ")", "\n", "action", "=", "np", ".", "array", "(", "[", "0", "]", ")", "\n", "reward", "=", "np", ".", "array", "(", "[", "1.0", "]", ")", "\n", "pscore", "=", "np", ".", "array", "(", "[", "0.5", "]", ")", "\n", "\n", "X", ",", "sample_weight", ",", "y", "=", "learner", ".", "_create_train_data_for_opl", "(", "\n", "context", "=", "context", ",", "action", "=", "action", ",", "reward", "=", "reward", ",", "pscore", "=", "pscore", "\n", ")", "\n", "\n", "assert", "np", ".", "allclose", "(", "X", ",", "np", ".", "array", "(", "[", "1.0", ",", "1.0", "]", ")", ".", "reshape", "(", "1", ",", "-", "1", ")", ")", "\n", "assert", "np", ".", "allclose", "(", "sample_weight", ",", "np", ".", "array", "(", "[", "2.0", "]", ")", ")", "\n", "assert", "np", ".", "allclose", "(", "y", ",", "np", ".", "array", "(", "[", "0", "]", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.test_offline.test_ipw_learner_fit": [[124, 168], ["numpy.ones", "numpy.random.choice", "numpy.random.choice", "numpy.random.choice", "numpy.arange", "numpy.arange", "numpy.arange", "pytest.raises", "obp.policy.offline.IPWLearner", "numpy.random.normal", "obp.policy.offline.IPWLearner.fit", "pytest.raises", "obp.policy.offline.IPWLearner", "obp.policy.offline.IPWLearner.fit", "pytest.raises", "obp.policy.offline.IPWLearner", "obp.policy.offline.IPWLearner.fit", "pytest.raises", "obp.policy.offline.IPWLearner", "obp.policy.offline.IPWLearner.fit"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.classification_model.PropensityScoreEstimator.fit", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.classification_model.PropensityScoreEstimator.fit", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.classification_model.PropensityScoreEstimator.fit", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.classification_model.PropensityScoreEstimator.fit"], ["", "def", "test_ipw_learner_fit", "(", ")", ":", "\n", "    ", "n_rounds", "=", "1000", "\n", "dim_context", "=", "5", "\n", "n_actions", "=", "3", "\n", "len_list", "=", "2", "\n", "context", "=", "np", ".", "ones", "(", "(", "n_rounds", ",", "dim_context", ")", ")", "\n", "action", "=", "np", ".", "random", ".", "choice", "(", "np", ".", "arange", "(", "len_list", ",", "dtype", "=", "int", ")", ",", "size", "=", "n_rounds", ")", "\n", "reward", "=", "np", ".", "random", ".", "choice", "(", "np", ".", "arange", "(", "2", ")", ",", "size", "=", "n_rounds", ")", "\n", "position", "=", "np", ".", "random", ".", "choice", "(", "np", ".", "arange", "(", "len_list", ",", "dtype", "=", "int", ")", ",", "size", "=", "n_rounds", ")", "\n", "\n", "# inconsistency with the shape", "\n", "desc", "=", "\"Expected `context.shape[0]\"", "\n", "with", "pytest", ".", "raises", "(", "ValueError", ",", "match", "=", "f\"{desc}*\"", ")", ":", "\n", "        ", "learner", "=", "IPWLearner", "(", "n_actions", "=", "n_actions", ",", "len_list", "=", "len_list", ")", "\n", "variant_context", "=", "np", ".", "random", ".", "normal", "(", "size", "=", "(", "n_rounds", "+", "1", ",", "n_actions", ")", ")", "\n", "learner", ".", "fit", "(", "\n", "context", "=", "variant_context", ",", "\n", "action", "=", "action", ",", "\n", "reward", "=", "reward", ",", "\n", "position", "=", "position", ",", "\n", ")", "\n", "\n", "# len_list > 2, but position is not set", "\n", "", "desc", "=", "\"When `self.len_list > 1\"", "\n", "with", "pytest", ".", "raises", "(", "ValueError", ",", "match", "=", "f\"{desc}*\"", ")", ":", "\n", "        ", "learner", "=", "IPWLearner", "(", "n_actions", "=", "n_actions", ",", "len_list", "=", "len_list", ")", "\n", "learner", ".", "fit", "(", "context", "=", "context", ",", "action", "=", "action", ",", "reward", "=", "reward", ")", "\n", "\n", "# position must be non-negative", "\n", "", "desc", "=", "\"`position` elements must be non-negative integers\"", "\n", "with", "pytest", ".", "raises", "(", "ValueError", ",", "match", "=", "f\"{desc}*\"", ")", ":", "\n", "        ", "negative_position", "=", "position", "-", "1", "\n", "learner", "=", "IPWLearner", "(", "n_actions", "=", "n_actions", ",", "len_list", "=", "len_list", ")", "\n", "learner", ".", "fit", "(", "\n", "context", "=", "context", ",", "action", "=", "action", ",", "reward", "=", "reward", ",", "position", "=", "negative_position", "\n", ")", "\n", "\n", "# IPWLearner cannot handle negative rewards", "\n", "", "desc", "=", "\"A negative value is found in\"", "\n", "with", "pytest", ".", "raises", "(", "ValueError", ",", "match", "=", "f\"{desc}*\"", ")", ":", "\n", "        ", "negative_reward", "=", "reward", "-", "1.0", "\n", "learner", "=", "IPWLearner", "(", "n_actions", "=", "n_actions", ",", "len_list", "=", "len_list", ")", "\n", "learner", ".", "fit", "(", "\n", "context", "=", "context", ",", "action", "=", "action", ",", "reward", "=", "negative_reward", ",", "position", "=", "position", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.test_offline.test_ipw_learner_predict": [[171, 199], ["numpy.array().reshape", "numpy.array", "numpy.array", "numpy.array", "obp.policy.offline.IPWLearner", "obp.policy.offline.IPWLearner.fit", "numpy.array().reshape", "obp.policy.offline.IPWLearner.predict", "numpy.allclose", "pytest.raises", "numpy.array", "obp.policy.offline.IPWLearner", "obp.policy.offline.IPWLearner.predict", "learner.predict.sum", "numpy.ones_like", "numpy.array", "numpy.array", "range"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.classification_model.PropensityScoreEstimator.fit", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.classification_model.PropensityScoreEstimator.predict", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.classification_model.PropensityScoreEstimator.predict"], ["", "", "def", "test_ipw_learner_predict", "(", ")", ":", "\n", "    ", "n_actions", "=", "2", "\n", "len_list", "=", "1", "\n", "\n", "# shape error", "\n", "desc", "=", "\"`context` must be 2D array\"", "\n", "with", "pytest", ".", "raises", "(", "ValueError", ",", "match", "=", "f\"{desc}*\"", ")", ":", "\n", "        ", "context", "=", "np", ".", "array", "(", "[", "1.0", ",", "1.0", "]", ")", "\n", "learner", "=", "IPWLearner", "(", "n_actions", "=", "n_actions", ",", "len_list", "=", "len_list", ")", "\n", "learner", ".", "predict", "(", "context", "=", "context", ")", "\n", "\n", "# shape consistency of action_dist", "\n", "# n_rounds is 5, dim_context is 2", "\n", "", "context", "=", "np", ".", "array", "(", "[", "1.0", ",", "1.0", ",", "1.0", ",", "1.0", "]", ")", ".", "reshape", "(", "2", ",", "-", "1", ")", "\n", "action", "=", "np", ".", "array", "(", "[", "0", ",", "1", "]", ")", "\n", "reward", "=", "np", ".", "array", "(", "[", "1.0", ",", "0.0", "]", ")", "\n", "position", "=", "np", ".", "array", "(", "[", "0", ",", "0", "]", ")", "\n", "learner", "=", "IPWLearner", "(", "n_actions", "=", "2", ",", "len_list", "=", "1", ")", "\n", "learner", ".", "fit", "(", "context", "=", "context", ",", "action", "=", "action", ",", "reward", "=", "reward", ",", "position", "=", "position", ")", "\n", "\n", "context_test", "=", "np", ".", "array", "(", "[", "i", "for", "i", "in", "range", "(", "10", ")", "]", ")", ".", "reshape", "(", "5", ",", "2", ")", "\n", "action_dist", "=", "learner", ".", "predict", "(", "context", "=", "context_test", ")", "\n", "assert", "np", ".", "allclose", "(", "\n", "action_dist", ".", "sum", "(", "1", ")", ",", "np", ".", "ones_like", "(", "(", "context_test", ".", "shape", "[", "0", "]", ",", "len_list", ")", ")", "\n", ")", "\n", "assert", "action_dist", ".", "shape", "[", "0", "]", "==", "5", "\n", "assert", "action_dist", ".", "shape", "[", "1", "]", "==", "n_actions", "\n", "assert", "action_dist", ".", "shape", "[", "2", "]", "==", "len_list", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.test_offline.test_ipw_learner_sample_action": [[201, 227], ["numpy.array().reshape", "numpy.array", "numpy.array", "numpy.array", "obp.policy.offline.IPWLearner", "obp.policy.offline.IPWLearner.fit", "numpy.array().reshape", "obp.policy.offline.IPWLearner.sample_action", "pytest.raises", "obp.policy.offline.IPWLearner.sample_action", "pytest.raises", "numpy.array", "obp.policy.offline.IPWLearner.sample_action", "numpy.array", "numpy.array"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.classification_model.PropensityScoreEstimator.fit", "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.offline.NNPolicyLearner.sample_action", "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.offline.NNPolicyLearner.sample_action", "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.offline.NNPolicyLearner.sample_action"], ["", "def", "test_ipw_learner_sample_action", "(", ")", ":", "\n", "    ", "n_actions", "=", "2", "\n", "len_list", "=", "1", "\n", "context", "=", "np", ".", "array", "(", "[", "1.0", ",", "1.0", ",", "1.0", ",", "1.0", "]", ")", ".", "reshape", "(", "2", ",", "-", "1", ")", "\n", "action", "=", "np", ".", "array", "(", "[", "0", ",", "1", "]", ")", "\n", "reward", "=", "np", ".", "array", "(", "[", "1.0", ",", "0.0", "]", ")", "\n", "position", "=", "np", ".", "array", "(", "[", "0", ",", "0", "]", ")", "\n", "learner", "=", "IPWLearner", "(", "n_actions", "=", "n_actions", ",", "len_list", "=", "len_list", ")", "\n", "learner", ".", "fit", "(", "context", "=", "context", ",", "action", "=", "action", ",", "reward", "=", "reward", ",", "position", "=", "position", ")", "\n", "\n", "desc", "=", "\"`context` must be 2D array\"", "\n", "with", "pytest", ".", "raises", "(", "ValueError", ",", "match", "=", "f\"{desc}*\"", ")", ":", "\n", "        ", "invalid_type_context", "=", "[", "1.0", ",", "2.0", "]", "\n", "learner", ".", "sample_action", "(", "context", "=", "invalid_type_context", ")", "\n", "\n", "", "with", "pytest", ".", "raises", "(", "ValueError", ",", "match", "=", "f\"{desc}*\"", ")", ":", "\n", "        ", "invalid_ndim_context", "=", "np", ".", "array", "(", "[", "1.0", ",", "2.0", ",", "3.0", ",", "4.0", "]", ")", "\n", "learner", ".", "sample_action", "(", "context", "=", "invalid_ndim_context", ")", "\n", "\n", "", "context", "=", "np", ".", "array", "(", "[", "1.0", ",", "1.0", ",", "1.0", ",", "1.0", "]", ")", ".", "reshape", "(", "2", ",", "-", "1", ")", "\n", "n_rounds", "=", "context", ".", "shape", "[", "0", "]", "\n", "sampled_action", "=", "learner", ".", "sample_action", "(", "context", "=", "context", ")", "\n", "\n", "assert", "sampled_action", ".", "shape", "[", "0", "]", "==", "n_rounds", "\n", "assert", "sampled_action", ".", "shape", "[", "1", "]", "==", "n_actions", "\n", "assert", "sampled_action", ".", "shape", "[", "2", "]", "==", "len_list", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.test_offline.test_q_learner_init_using_invalid_inputs": [[287, 304], ["pytest.mark.parametrize", "pytest.raises", "obp.policy.offline.QLearner"], "function", ["None"], ["@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"n_actions, len_list, base_model, fitting_method, description\"", ",", "\n", "invalid_input_of_q_learner_init", ",", "\n", ")", "\n", "def", "test_q_learner_init_using_invalid_inputs", "(", "\n", "n_actions", ",", "\n", "len_list", ",", "\n", "base_model", ",", "\n", "fitting_method", ",", "\n", "description", ",", "\n", ")", ":", "\n", "    ", "with", "pytest", ".", "raises", "(", "ValueError", ",", "match", "=", "f\"{description}*\"", ")", ":", "\n", "        ", "_", "=", "QLearner", "(", "\n", "n_actions", "=", "n_actions", ",", "\n", "len_list", "=", "len_list", ",", "\n", "base_model", "=", "base_model", ",", "\n", "fitting_method", "=", "fitting_method", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.test_offline.test_q_learner_init_using_valid_inputs": [[307, 326], ["pytest.mark.parametrize", "obp.policy.offline.QLearner"], "function", ["None"], ["", "", "@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"n_actions, len_list, base_model, fitting_method, description\"", ",", "\n", "valid_input_of_q_learner_init", ",", "\n", ")", "\n", "def", "test_q_learner_init_using_valid_inputs", "(", "\n", "n_actions", ",", "\n", "len_list", ",", "\n", "base_model", ",", "\n", "fitting_method", ",", "\n", "description", ",", "\n", ")", ":", "\n", "    ", "q_learner", "=", "QLearner", "(", "\n", "n_actions", "=", "n_actions", ",", "\n", "len_list", "=", "len_list", ",", "\n", "base_model", "=", "base_model", ",", "\n", "fitting_method", "=", "fitting_method", ",", "\n", ")", "\n", "# policy_type", "\n", "assert", "q_learner", ".", "policy_type", "==", "PolicyType", ".", "OFFLINE", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.test_offline.test_q_learner_fit": [[328, 369], ["numpy.ones", "numpy.random.choice", "numpy.random.choice", "numpy.random.choice", "numpy.arange", "numpy.arange", "numpy.arange", "pytest.raises", "obp.policy.offline.QLearner", "numpy.random.normal", "obp.policy.offline.QLearner.fit", "pytest.raises", "obp.policy.offline.QLearner", "obp.policy.offline.QLearner.fit", "pytest.raises", "obp.policy.offline.QLearner", "obp.policy.offline.QLearner.fit"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.classification_model.PropensityScoreEstimator.fit", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.classification_model.PropensityScoreEstimator.fit", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.classification_model.PropensityScoreEstimator.fit"], ["", "def", "test_q_learner_fit", "(", ")", ":", "\n", "    ", "n_rounds", "=", "1000", "\n", "dim_context", "=", "5", "\n", "n_actions", "=", "3", "\n", "len_list", "=", "2", "\n", "context", "=", "np", ".", "ones", "(", "(", "n_rounds", ",", "dim_context", ")", ")", "\n", "action", "=", "np", ".", "random", ".", "choice", "(", "np", ".", "arange", "(", "len_list", ",", "dtype", "=", "int", ")", ",", "size", "=", "n_rounds", ")", "\n", "reward", "=", "np", ".", "random", ".", "choice", "(", "np", ".", "arange", "(", "2", ")", ",", "size", "=", "n_rounds", ")", "\n", "position", "=", "np", ".", "random", ".", "choice", "(", "np", ".", "arange", "(", "len_list", ",", "dtype", "=", "int", ")", ",", "size", "=", "n_rounds", ")", "\n", "\n", "# inconsistency with the shape", "\n", "desc", "=", "\"Expected `context.shape[0]\"", "\n", "with", "pytest", ".", "raises", "(", "ValueError", ",", "match", "=", "f\"{desc}*\"", ")", ":", "\n", "        ", "learner", "=", "QLearner", "(", "\n", "n_actions", "=", "n_actions", ",", "len_list", "=", "len_list", ",", "base_model", "=", "base_classifier", "\n", ")", "\n", "variant_context", "=", "np", ".", "random", ".", "normal", "(", "size", "=", "(", "n_rounds", "+", "1", ",", "n_actions", ")", ")", "\n", "learner", ".", "fit", "(", "\n", "context", "=", "variant_context", ",", "\n", "action", "=", "action", ",", "\n", "reward", "=", "reward", ",", "\n", "position", "=", "position", ",", "\n", ")", "\n", "\n", "# len_list > 2, but position is not set", "\n", "", "desc", "=", "\"When `self.len_list > 1\"", "\n", "with", "pytest", ".", "raises", "(", "ValueError", ",", "match", "=", "f\"{desc}*\"", ")", ":", "\n", "        ", "learner", "=", "QLearner", "(", "\n", "n_actions", "=", "n_actions", ",", "len_list", "=", "len_list", ",", "base_model", "=", "base_classifier", "\n", ")", "\n", "learner", ".", "fit", "(", "context", "=", "context", ",", "action", "=", "action", ",", "reward", "=", "reward", ")", "\n", "\n", "# position must be non-negative", "\n", "", "desc", "=", "\"`position` elements must be non-negative integers\"", "\n", "with", "pytest", ".", "raises", "(", "ValueError", ",", "match", "=", "f\"{desc}*\"", ")", ":", "\n", "        ", "negative_position", "=", "position", "-", "1", "\n", "learner", "=", "QLearner", "(", "\n", "n_actions", "=", "n_actions", ",", "len_list", "=", "len_list", ",", "base_model", "=", "base_classifier", "\n", ")", "\n", "learner", ".", "fit", "(", "\n", "context", "=", "context", ",", "action", "=", "action", ",", "reward", "=", "reward", ",", "position", "=", "negative_position", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.test_offline.test_q_learner_predict": [[372, 404], ["numpy.array().reshape", "numpy.array", "numpy.array", "numpy.array", "obp.policy.offline.QLearner", "obp.policy.offline.QLearner.fit", "numpy.array().reshape", "obp.policy.offline.QLearner.predict", "numpy.allclose", "pytest.raises", "numpy.array", "obp.policy.offline.QLearner", "obp.policy.offline.QLearner.predict", "learner.predict.sum", "numpy.ones_like", "numpy.array", "numpy.array", "range"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.classification_model.PropensityScoreEstimator.fit", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.classification_model.PropensityScoreEstimator.predict", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.classification_model.PropensityScoreEstimator.predict"], ["", "", "def", "test_q_learner_predict", "(", ")", ":", "\n", "    ", "n_actions", "=", "2", "\n", "len_list", "=", "1", "\n", "\n", "# shape error", "\n", "desc", "=", "\"`context` must be 2D array\"", "\n", "with", "pytest", ".", "raises", "(", "ValueError", ",", "match", "=", "f\"{desc}*\"", ")", ":", "\n", "        ", "context", "=", "np", ".", "array", "(", "[", "1.0", ",", "1.0", "]", ")", "\n", "learner", "=", "QLearner", "(", "\n", "n_actions", "=", "n_actions", ",", "len_list", "=", "len_list", ",", "base_model", "=", "base_classifier", "\n", ")", "\n", "learner", ".", "predict", "(", "context", "=", "context", ")", "\n", "\n", "# shape consistency of action_dist", "\n", "# n_rounds is 5, dim_context is 2", "\n", "", "context", "=", "np", ".", "array", "(", "[", "1.0", ",", "1.0", ",", "1.0", ",", "1.0", "]", ")", ".", "reshape", "(", "2", ",", "-", "1", ")", "\n", "action", "=", "np", ".", "array", "(", "[", "0", ",", "1", "]", ")", "\n", "reward", "=", "np", ".", "array", "(", "[", "1.0", ",", "0.0", "]", ")", "\n", "position", "=", "np", ".", "array", "(", "[", "0", ",", "0", "]", ")", "\n", "learner", "=", "QLearner", "(", "\n", "n_actions", "=", "n_actions", ",", "len_list", "=", "len_list", ",", "base_model", "=", "base_classifier", "\n", ")", "\n", "learner", ".", "fit", "(", "context", "=", "context", ",", "action", "=", "action", ",", "reward", "=", "reward", ",", "position", "=", "position", ")", "\n", "\n", "context_test", "=", "np", ".", "array", "(", "[", "i", "for", "i", "in", "range", "(", "10", ")", "]", ")", ".", "reshape", "(", "5", ",", "2", ")", "\n", "action_dist", "=", "learner", ".", "predict", "(", "context", "=", "context_test", ")", "\n", "assert", "np", ".", "allclose", "(", "\n", "action_dist", ".", "sum", "(", "1", ")", ",", "np", ".", "ones_like", "(", "(", "context_test", ".", "shape", "[", "0", "]", ",", "len_list", ")", ")", "\n", ")", "\n", "assert", "action_dist", ".", "shape", "[", "0", "]", "==", "5", "\n", "assert", "action_dist", ".", "shape", "[", "1", "]", "==", "n_actions", "\n", "assert", "action_dist", ".", "shape", "[", "2", "]", "==", "len_list", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.test_offline.test_q_learner_sample_action": [[406, 434], ["numpy.array().reshape", "numpy.array", "numpy.array", "numpy.array", "obp.policy.offline.QLearner", "obp.policy.offline.QLearner.fit", "numpy.array().reshape", "obp.policy.offline.QLearner.sample_action", "pytest.raises", "obp.policy.offline.QLearner.sample_action", "pytest.raises", "numpy.array", "obp.policy.offline.QLearner.sample_action", "numpy.array", "numpy.array"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.classification_model.PropensityScoreEstimator.fit", "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.offline.NNPolicyLearner.sample_action", "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.offline.NNPolicyLearner.sample_action", "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.offline.NNPolicyLearner.sample_action"], ["", "def", "test_q_learner_sample_action", "(", ")", ":", "\n", "    ", "n_actions", "=", "2", "\n", "len_list", "=", "1", "\n", "context", "=", "np", ".", "array", "(", "[", "1.0", ",", "1.0", ",", "1.0", ",", "1.0", "]", ")", ".", "reshape", "(", "2", ",", "-", "1", ")", "\n", "action", "=", "np", ".", "array", "(", "[", "0", ",", "1", "]", ")", "\n", "reward", "=", "np", ".", "array", "(", "[", "1.0", ",", "0.0", "]", ")", "\n", "position", "=", "np", ".", "array", "(", "[", "0", ",", "0", "]", ")", "\n", "learner", "=", "QLearner", "(", "\n", "n_actions", "=", "n_actions", ",", "len_list", "=", "len_list", ",", "base_model", "=", "base_classifier", "\n", ")", "\n", "learner", ".", "fit", "(", "context", "=", "context", ",", "action", "=", "action", ",", "reward", "=", "reward", ",", "position", "=", "position", ")", "\n", "\n", "desc", "=", "\"`context` must be 2D array\"", "\n", "with", "pytest", ".", "raises", "(", "ValueError", ",", "match", "=", "f\"{desc}*\"", ")", ":", "\n", "        ", "invalid_type_context", "=", "[", "1.0", ",", "2.0", "]", "\n", "learner", ".", "sample_action", "(", "context", "=", "invalid_type_context", ")", "\n", "\n", "", "with", "pytest", ".", "raises", "(", "ValueError", ",", "match", "=", "f\"{desc}*\"", ")", ":", "\n", "        ", "invalid_ndim_context", "=", "np", ".", "array", "(", "[", "1.0", ",", "2.0", ",", "3.0", ",", "4.0", "]", ")", "\n", "learner", ".", "sample_action", "(", "context", "=", "invalid_ndim_context", ")", "\n", "\n", "", "context", "=", "np", ".", "array", "(", "[", "1.0", ",", "1.0", ",", "1.0", ",", "1.0", "]", ")", ".", "reshape", "(", "2", ",", "-", "1", ")", "\n", "n_rounds", "=", "context", ".", "shape", "[", "0", "]", "\n", "sampled_action", "=", "learner", ".", "sample_action", "(", "context", "=", "context", ")", "\n", "\n", "assert", "sampled_action", ".", "shape", "[", "0", "]", "==", "n_rounds", "\n", "assert", "sampled_action", ".", "shape", "[", "1", "]", "==", "n_actions", "\n", "assert", "sampled_action", ".", "shape", "[", "2", "]", "==", "len_list", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.test_offline.test_nn_policy_learner_init_using_invalid_inputs": [[1602, 1662], ["pytest.mark.parametrize", "pytest.raises", "obp.policy.offline.NNPolicyLearner"], "function", ["None"], ["@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"n_actions, len_list, dim_context, off_policy_objective, lambda_, policy_reg_param, var_reg_param, hidden_layer_size, activation, solver, alpha, batch_size, learning_rate_init, max_iter, shuffle, random_state, tol, momentum, nesterovs_momentum, early_stopping, validation_fraction, beta_1, beta_2, epsilon, n_iter_no_change, err, description\"", ",", "\n", "invalid_input_of_nn_policy_learner_init", ",", "\n", ")", "\n", "def", "test_nn_policy_learner_init_using_invalid_inputs", "(", "\n", "n_actions", ",", "\n", "len_list", ",", "\n", "dim_context", ",", "\n", "off_policy_objective", ",", "\n", "lambda_", ",", "\n", "policy_reg_param", ",", "\n", "var_reg_param", ",", "\n", "hidden_layer_size", ",", "\n", "activation", ",", "\n", "solver", ",", "\n", "alpha", ",", "\n", "batch_size", ",", "\n", "learning_rate_init", ",", "\n", "max_iter", ",", "\n", "shuffle", ",", "\n", "random_state", ",", "\n", "tol", ",", "\n", "momentum", ",", "\n", "nesterovs_momentum", ",", "\n", "early_stopping", ",", "\n", "validation_fraction", ",", "\n", "beta_1", ",", "\n", "beta_2", ",", "\n", "epsilon", ",", "\n", "n_iter_no_change", ",", "\n", "err", ",", "\n", "description", ",", "\n", ")", ":", "\n", "    ", "with", "pytest", ".", "raises", "(", "err", ",", "match", "=", "f\"{description}*\"", ")", ":", "\n", "        ", "_", "=", "NNPolicyLearner", "(", "\n", "n_actions", "=", "n_actions", ",", "\n", "len_list", "=", "len_list", ",", "\n", "dim_context", "=", "dim_context", ",", "\n", "off_policy_objective", "=", "off_policy_objective", ",", "\n", "lambda_", "=", "lambda_", ",", "\n", "policy_reg_param", "=", "policy_reg_param", ",", "\n", "var_reg_param", "=", "var_reg_param", ",", "\n", "hidden_layer_size", "=", "hidden_layer_size", ",", "\n", "activation", "=", "activation", ",", "\n", "solver", "=", "solver", ",", "\n", "alpha", "=", "alpha", ",", "\n", "batch_size", "=", "batch_size", ",", "\n", "learning_rate_init", "=", "learning_rate_init", ",", "\n", "max_iter", "=", "max_iter", ",", "\n", "shuffle", "=", "shuffle", ",", "\n", "random_state", "=", "random_state", ",", "\n", "tol", "=", "tol", ",", "\n", "momentum", "=", "momentum", ",", "\n", "nesterovs_momentum", "=", "nesterovs_momentum", ",", "\n", "early_stopping", "=", "early_stopping", ",", "\n", "validation_fraction", "=", "validation_fraction", ",", "\n", "beta_1", "=", "beta_1", ",", "\n", "beta_2", "=", "beta_2", ",", "\n", "epsilon", "=", "epsilon", ",", "\n", "n_iter_no_change", "=", "n_iter_no_change", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.test_offline.test_nn_policy_learner_init_using_valid_inputs": [[1665, 1725], ["pytest.mark.parametrize", "obp.policy.offline.NNPolicyLearner", "isinstance"], "function", ["None"], ["", "", "@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"n_actions, len_list, dim_context, off_policy_objective, lambda_, policy_reg_param, var_reg_param, hidden_layer_size, activation, solver, alpha, batch_size, learning_rate_init, max_iter, shuffle, random_state, tol, momentum, nesterovs_momentum, early_stopping, validation_fraction, beta_1, beta_2, epsilon, n_iter_no_change, description\"", ",", "\n", "valid_input_of_nn_policy_learner_init", ",", "\n", ")", "\n", "def", "test_nn_policy_learner_init_using_valid_inputs", "(", "\n", "n_actions", ",", "\n", "len_list", ",", "\n", "dim_context", ",", "\n", "off_policy_objective", ",", "\n", "lambda_", ",", "\n", "policy_reg_param", ",", "\n", "var_reg_param", ",", "\n", "hidden_layer_size", ",", "\n", "activation", ",", "\n", "solver", ",", "\n", "alpha", ",", "\n", "batch_size", ",", "\n", "learning_rate_init", ",", "\n", "max_iter", ",", "\n", "shuffle", ",", "\n", "random_state", ",", "\n", "tol", ",", "\n", "momentum", ",", "\n", "nesterovs_momentum", ",", "\n", "early_stopping", ",", "\n", "validation_fraction", ",", "\n", "beta_1", ",", "\n", "beta_2", ",", "\n", "epsilon", ",", "\n", "n_iter_no_change", ",", "\n", "description", ",", "\n", ")", ":", "\n", "    ", "nn_policy_learner", "=", "NNPolicyLearner", "(", "\n", "n_actions", "=", "n_actions", ",", "\n", "len_list", "=", "len_list", ",", "\n", "dim_context", "=", "dim_context", ",", "\n", "off_policy_objective", "=", "off_policy_objective", ",", "\n", "lambda_", "=", "lambda_", ",", "\n", "policy_reg_param", "=", "policy_reg_param", ",", "\n", "var_reg_param", "=", "var_reg_param", ",", "\n", "hidden_layer_size", "=", "hidden_layer_size", ",", "\n", "activation", "=", "activation", ",", "\n", "solver", "=", "solver", ",", "\n", "alpha", "=", "alpha", ",", "\n", "batch_size", "=", "batch_size", ",", "\n", "learning_rate_init", "=", "learning_rate_init", ",", "\n", "max_iter", "=", "max_iter", ",", "\n", "shuffle", "=", "shuffle", ",", "\n", "random_state", "=", "random_state", ",", "\n", "tol", "=", "tol", ",", "\n", "momentum", "=", "momentum", ",", "\n", "nesterovs_momentum", "=", "nesterovs_momentum", ",", "\n", "early_stopping", "=", "early_stopping", ",", "\n", "validation_fraction", "=", "validation_fraction", ",", "\n", "beta_1", "=", "beta_1", ",", "\n", "beta_2", "=", "beta_2", ",", "\n", "epsilon", "=", "epsilon", ",", "\n", "n_iter_no_change", "=", "n_iter_no_change", ",", "\n", ")", "\n", "assert", "isinstance", "(", "nn_policy_learner", ",", "NNPolicyLearner", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.test_offline.test_nn_policy_learner_create_train_data_for_opl": [[1727, 1767], ["numpy.ones", "numpy.zeros", "numpy.ones", "numpy.array", "numpy.ones", "numpy.zeros", "obp.policy.offline.NNPolicyLearner", "obp.policy.offline.NNPolicyLearner._create_train_data_for_opl", "isinstance", "obp.policy.offline.NNPolicyLearner", "obp.policy.offline.NNPolicyLearner._create_train_data_for_opl", "isinstance", "isinstance"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.policy.offline.NNPolicyLearner._create_train_data_for_opl", "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.offline.NNPolicyLearner._create_train_data_for_opl"], ["", "def", "test_nn_policy_learner_create_train_data_for_opl", "(", ")", ":", "\n", "    ", "context", "=", "np", ".", "ones", "(", "(", "100", ",", "2", ")", ",", "dtype", "=", "np", ".", "int32", ")", "\n", "action", "=", "np", ".", "zeros", "(", "(", "100", ",", ")", ",", "dtype", "=", "np", ".", "int32", ")", "\n", "reward", "=", "np", ".", "ones", "(", "(", "100", ",", ")", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "pscore", "=", "np", ".", "array", "(", "[", "0.5", "]", "*", "100", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "estimated_rewards_by_reg_model", "=", "np", ".", "ones", "(", "(", "100", ",", "2", ")", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "position", "=", "np", ".", "zeros", "(", "(", "100", ",", ")", ",", "dtype", "=", "np", ".", "int32", ")", "\n", "\n", "learner1", "=", "NNPolicyLearner", "(", "n_actions", "=", "2", ",", "dim_context", "=", "2", ",", "off_policy_objective", "=", "\"ipw\"", ")", "\n", "\n", "training_loader", ",", "validation_loader", "=", "learner1", ".", "_create_train_data_for_opl", "(", "\n", "context", "=", "context", ",", "\n", "action", "=", "action", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore", "=", "pscore", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", "position", "=", "position", ",", "\n", ")", "\n", "\n", "assert", "isinstance", "(", "training_loader", ",", "torch", ".", "utils", ".", "data", ".", "DataLoader", ")", "\n", "assert", "validation_loader", "is", "None", "\n", "\n", "learner2", "=", "NNPolicyLearner", "(", "\n", "n_actions", "=", "2", ",", "\n", "dim_context", "=", "2", ",", "\n", "off_policy_objective", "=", "\"ipw\"", ",", "\n", "early_stopping", "=", "True", ",", "\n", ")", "\n", "\n", "training_loader", ",", "validation_loader", "=", "learner2", ".", "_create_train_data_for_opl", "(", "\n", "context", "=", "context", ",", "\n", "action", "=", "action", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore", "=", "pscore", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", "position", "=", "position", ",", "\n", ")", "\n", "\n", "assert", "isinstance", "(", "training_loader", ",", "torch", ".", "utils", ".", "data", ".", "DataLoader", ")", "\n", "assert", "isinstance", "(", "validation_loader", ",", "torch", ".", "utils", ".", "data", ".", "DataLoader", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.test_offline.test_nn_policy_learner_fit": [[1769, 1793], ["numpy.ones", "numpy.zeros", "numpy.ones", "numpy.array", "pytest.raises", "obp.policy.offline.NNPolicyLearner", "numpy.ones", "obp.policy.offline.NNPolicyLearner.fit", "pytest.raises", "obp.policy.offline.NNPolicyLearner", "obp.policy.offline.NNPolicyLearner.fit"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.classification_model.PropensityScoreEstimator.fit", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.classification_model.PropensityScoreEstimator.fit"], ["", "def", "test_nn_policy_learner_fit", "(", ")", ":", "\n", "    ", "context", "=", "np", ".", "ones", "(", "(", "100", ",", "2", ")", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "action", "=", "np", ".", "zeros", "(", "(", "100", ",", ")", ",", "dtype", "=", "int", ")", "\n", "reward", "=", "np", ".", "ones", "(", "(", "100", ",", ")", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "pscore", "=", "np", ".", "array", "(", "[", "0.5", "]", "*", "100", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "\n", "# inconsistency with the shape", "\n", "desc", "=", "\"Expected `context.shape[0]\"", "\n", "with", "pytest", ".", "raises", "(", "ValueError", ",", "match", "=", "f\"{desc}*\"", ")", ":", "\n", "        ", "learner", "=", "NNPolicyLearner", "(", "\n", "n_actions", "=", "2", ",", "dim_context", "=", "2", ",", "off_policy_objective", "=", "\"ipw\"", "\n", ")", "\n", "variant_context", "=", "np", ".", "ones", "(", "(", "101", ",", "2", ")", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "learner", ".", "fit", "(", "\n", "context", "=", "variant_context", ",", "action", "=", "action", ",", "reward", "=", "reward", ",", "pscore", "=", "pscore", "\n", ")", "\n", "\n", "# inconsistency between dim_context and context", "\n", "", "desc", "=", "\"Expected `context.shape[1]\"", "\n", "with", "pytest", ".", "raises", "(", "ValueError", ",", "match", "=", "f\"{desc}*\"", ")", ":", "\n", "        ", "learner", "=", "NNPolicyLearner", "(", "\n", "n_actions", "=", "2", ",", "dim_context", "=", "3", ",", "off_policy_objective", "=", "\"ipw\"", "\n", ")", "\n", "learner", ".", "fit", "(", "context", "=", "context", ",", "action", "=", "action", ",", "reward", "=", "reward", ",", "pscore", "=", "pscore", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.test_offline.test_nn_policy_learner_predict": [[1795, 1846], ["numpy.ones", "numpy.array().reshape", "numpy.zeros", "numpy.ones", "numpy.array", "obp.policy.offline.NNPolicyLearner", "obp.policy.offline.NNPolicyLearner.fit", "obp.policy.offline.NNPolicyLearner.predict", "numpy.allclose", "pytest.raises", "obp.policy.offline.NNPolicyLearner", "obp.policy.offline.NNPolicyLearner.fit", "numpy.array", "obp.policy.offline.NNPolicyLearner.predict", "pytest.raises", "obp.policy.offline.NNPolicyLearner", "obp.policy.offline.NNPolicyLearner.fit", "numpy.array", "obp.policy.offline.NNPolicyLearner.predict", "learner.predict.sum", "numpy.ones_like", "numpy.array", "range"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.classification_model.PropensityScoreEstimator.fit", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.classification_model.PropensityScoreEstimator.predict", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.classification_model.PropensityScoreEstimator.fit", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.classification_model.PropensityScoreEstimator.predict", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.classification_model.PropensityScoreEstimator.fit", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.classification_model.PropensityScoreEstimator.predict"], ["", "", "def", "test_nn_policy_learner_predict", "(", ")", ":", "\n", "    ", "n_actions", "=", "2", "\n", "len_list", "=", "1", "\n", "context", "=", "np", ".", "ones", "(", "(", "100", ",", "2", ")", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "context_test", "=", "np", ".", "array", "(", "[", "i", "for", "i", "in", "range", "(", "10", ")", "]", ",", "dtype", "=", "np", ".", "float32", ")", ".", "reshape", "(", "5", ",", "2", ")", "\n", "action", "=", "np", ".", "zeros", "(", "(", "100", ",", ")", ",", "dtype", "=", "int", ")", "\n", "reward", "=", "np", ".", "ones", "(", "(", "100", ",", ")", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "pscore", "=", "np", ".", "array", "(", "[", "0.5", "]", "*", "100", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "\n", "# shape error", "\n", "desc", "=", "\"`context` must be 2D array\"", "\n", "with", "pytest", ".", "raises", "(", "ValueError", ",", "match", "=", "f\"{desc}*\"", ")", ":", "\n", "        ", "learner", "=", "NNPolicyLearner", "(", "\n", "n_actions", "=", "n_actions", ",", "\n", "len_list", "=", "len_list", ",", "\n", "dim_context", "=", "2", ",", "\n", "off_policy_objective", "=", "\"ipw\"", ",", "\n", ")", "\n", "learner", ".", "fit", "(", "context", "=", "context", ",", "action", "=", "action", ",", "reward", "=", "reward", ",", "pscore", "=", "pscore", ")", "\n", "invalid_context", "=", "np", ".", "array", "(", "[", "1.0", ",", "1.0", "]", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "learner", ".", "predict", "(", "context", "=", "invalid_context", ")", "\n", "\n", "# inconsistency between dim_context and context", "\n", "", "desc", "=", "\"Expected `context.shape[1]\"", "\n", "with", "pytest", ".", "raises", "(", "ValueError", ",", "match", "=", "f\"{desc}*\"", ")", ":", "\n", "        ", "learner", "=", "NNPolicyLearner", "(", "\n", "n_actions", "=", "n_actions", ",", "\n", "len_list", "=", "len_list", ",", "\n", "dim_context", "=", "2", ",", "\n", "off_policy_objective", "=", "\"ipw\"", ",", "\n", ")", "\n", "learner", ".", "fit", "(", "context", "=", "context", ",", "action", "=", "action", ",", "reward", "=", "reward", ",", "pscore", "=", "pscore", ")", "\n", "invalid_context", "=", "np", ".", "array", "(", "[", "[", "1.0", ",", "1.0", ",", "1.0", "]", "]", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "learner", ".", "predict", "(", "context", "=", "invalid_context", ")", "\n", "\n", "# shape consistency of action_dist", "\n", "# n_rounds is 5, dim_context is 2", "\n", "", "learner", "=", "NNPolicyLearner", "(", "\n", "n_actions", "=", "n_actions", ",", "\n", "len_list", "=", "len_list", ",", "\n", "dim_context", "=", "2", ",", "\n", "off_policy_objective", "=", "\"ipw\"", ",", "\n", ")", "\n", "learner", ".", "fit", "(", "context", "=", "context", ",", "action", "=", "action", ",", "reward", "=", "reward", ",", "pscore", "=", "pscore", ")", "\n", "action_dist", "=", "learner", ".", "predict", "(", "context", "=", "context_test", ")", "\n", "assert", "np", ".", "allclose", "(", "\n", "action_dist", ".", "sum", "(", "1", ")", ",", "np", ".", "ones_like", "(", "(", "context_test", ".", "shape", "[", "0", "]", ",", "len_list", ")", ")", "\n", ")", "\n", "assert", "action_dist", ".", "shape", "[", "0", "]", "==", "5", "\n", "assert", "action_dist", ".", "shape", "[", "1", "]", "==", "n_actions", "\n", "assert", "action_dist", ".", "shape", "[", "2", "]", "==", "len_list", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.test_offline.test_nn_policy_learner_sample_action": [[1848, 1897], ["numpy.ones", "numpy.array().reshape", "numpy.zeros", "numpy.ones", "numpy.array", "obp.policy.offline.NNPolicyLearner", "obp.policy.offline.NNPolicyLearner.fit", "obp.policy.offline.NNPolicyLearner.sample_action", "numpy.allclose", "pytest.raises", "obp.policy.offline.NNPolicyLearner", "obp.policy.offline.NNPolicyLearner.fit", "numpy.array", "obp.policy.offline.NNPolicyLearner.sample_action", "pytest.raises", "obp.policy.offline.NNPolicyLearner", "obp.policy.offline.NNPolicyLearner.fit", "numpy.array", "obp.policy.offline.NNPolicyLearner.sample_action", "learner.sample_action.sum", "numpy.ones_like", "numpy.array", "range"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.classification_model.PropensityScoreEstimator.fit", "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.offline.NNPolicyLearner.sample_action", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.classification_model.PropensityScoreEstimator.fit", "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.offline.NNPolicyLearner.sample_action", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.classification_model.PropensityScoreEstimator.fit", "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.offline.NNPolicyLearner.sample_action"], ["", "def", "test_nn_policy_learner_sample_action", "(", ")", ":", "\n", "    ", "n_actions", "=", "2", "\n", "len_list", "=", "1", "\n", "context", "=", "np", ".", "ones", "(", "(", "100", ",", "2", ")", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "context_test", "=", "np", ".", "array", "(", "[", "i", "for", "i", "in", "range", "(", "10", ")", "]", ",", "dtype", "=", "np", ".", "float32", ")", ".", "reshape", "(", "5", ",", "2", ")", "\n", "action", "=", "np", ".", "zeros", "(", "(", "100", ",", ")", ",", "dtype", "=", "int", ")", "\n", "reward", "=", "np", ".", "ones", "(", "(", "100", ",", ")", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "pscore", "=", "np", ".", "array", "(", "[", "0.5", "]", "*", "100", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "\n", "# shape error", "\n", "desc", "=", "\"`context` must be 2D array\"", "\n", "with", "pytest", ".", "raises", "(", "ValueError", ",", "match", "=", "f\"{desc}*\"", ")", ":", "\n", "        ", "learner", "=", "NNPolicyLearner", "(", "\n", "n_actions", "=", "n_actions", ",", "\n", "len_list", "=", "len_list", ",", "\n", "dim_context", "=", "2", ",", "\n", "off_policy_objective", "=", "\"ipw\"", ",", "\n", ")", "\n", "learner", ".", "fit", "(", "context", "=", "context", ",", "action", "=", "action", ",", "reward", "=", "reward", ",", "pscore", "=", "pscore", ")", "\n", "invalid_context", "=", "np", ".", "array", "(", "[", "1.0", ",", "1.0", "]", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "learner", ".", "sample_action", "(", "context", "=", "invalid_context", ")", "\n", "\n", "# inconsistency between dim_context and context", "\n", "", "desc", "=", "\"Expected `context.shape[1]\"", "\n", "with", "pytest", ".", "raises", "(", "ValueError", ",", "match", "=", "f\"{desc}*\"", ")", ":", "\n", "        ", "learner", "=", "NNPolicyLearner", "(", "\n", "n_actions", "=", "n_actions", ",", "\n", "len_list", "=", "len_list", ",", "\n", "dim_context", "=", "2", ",", "\n", "off_policy_objective", "=", "\"ipw\"", ",", "\n", ")", "\n", "learner", ".", "fit", "(", "context", "=", "context", ",", "action", "=", "action", ",", "reward", "=", "reward", ",", "pscore", "=", "pscore", ")", "\n", "invalid_context", "=", "np", ".", "array", "(", "[", "[", "1.0", ",", "1.0", ",", "1.0", "]", "]", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "learner", ".", "sample_action", "(", "context", "=", "invalid_context", ")", "\n", "\n", "", "learner", "=", "NNPolicyLearner", "(", "\n", "n_actions", "=", "n_actions", ",", "\n", "len_list", "=", "len_list", ",", "\n", "dim_context", "=", "2", ",", "\n", "off_policy_objective", "=", "\"ipw\"", ",", "\n", ")", "\n", "learner", ".", "fit", "(", "context", "=", "context", ",", "action", "=", "action", ",", "reward", "=", "reward", ",", "pscore", "=", "pscore", ")", "\n", "action_dist", "=", "learner", ".", "sample_action", "(", "context", "=", "context_test", ")", "\n", "assert", "np", ".", "allclose", "(", "\n", "action_dist", ".", "sum", "(", "1", ")", ",", "np", ".", "ones_like", "(", "(", "context_test", ".", "shape", "[", "0", "]", ",", "len_list", ")", ")", "\n", ")", "\n", "assert", "action_dist", ".", "shape", "[", "0", "]", "==", "context_test", ".", "shape", "[", "0", "]", "\n", "assert", "action_dist", ".", "shape", "[", "1", "]", "==", "n_actions", "\n", "assert", "action_dist", ".", "shape", "[", "2", "]", "==", "len_list", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.test_offline.test_nn_policy_learner_predict_proba": [[1899, 1948], ["numpy.ones", "numpy.array().reshape", "numpy.zeros", "numpy.ones", "numpy.array", "obp.policy.offline.NNPolicyLearner", "obp.policy.offline.NNPolicyLearner.fit", "obp.policy.offline.NNPolicyLearner.predict_proba", "numpy.allclose", "pytest.raises", "obp.policy.offline.NNPolicyLearner", "obp.policy.offline.NNPolicyLearner.fit", "numpy.array", "obp.policy.offline.NNPolicyLearner.predict_proba", "pytest.raises", "obp.policy.offline.NNPolicyLearner", "obp.policy.offline.NNPolicyLearner.fit", "numpy.array", "obp.policy.offline.NNPolicyLearner.predict_proba", "learner.predict_proba.sum", "numpy.ones_like", "numpy.array", "range"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.classification_model.PropensityScoreEstimator.fit", "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.offline.NNPolicyLearner.predict_proba", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.classification_model.PropensityScoreEstimator.fit", "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.offline.NNPolicyLearner.predict_proba", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.classification_model.PropensityScoreEstimator.fit", "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.offline.NNPolicyLearner.predict_proba"], ["", "def", "test_nn_policy_learner_predict_proba", "(", ")", ":", "\n", "    ", "n_actions", "=", "2", "\n", "len_list", "=", "1", "\n", "context", "=", "np", ".", "ones", "(", "(", "100", ",", "2", ")", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "context_test", "=", "np", ".", "array", "(", "[", "i", "for", "i", "in", "range", "(", "10", ")", "]", ",", "dtype", "=", "np", ".", "float32", ")", ".", "reshape", "(", "5", ",", "2", ")", "\n", "action", "=", "np", ".", "zeros", "(", "(", "100", ",", ")", ",", "dtype", "=", "int", ")", "\n", "reward", "=", "np", ".", "ones", "(", "(", "100", ",", ")", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "pscore", "=", "np", ".", "array", "(", "[", "0.5", "]", "*", "100", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "\n", "# shape error", "\n", "desc", "=", "\"`context` must be 2D array\"", "\n", "with", "pytest", ".", "raises", "(", "ValueError", ",", "match", "=", "f\"{desc}*\"", ")", ":", "\n", "        ", "learner", "=", "NNPolicyLearner", "(", "\n", "n_actions", "=", "n_actions", ",", "\n", "len_list", "=", "len_list", ",", "\n", "dim_context", "=", "2", ",", "\n", "off_policy_objective", "=", "\"ipw\"", ",", "\n", ")", "\n", "learner", ".", "fit", "(", "context", "=", "context", ",", "action", "=", "action", ",", "reward", "=", "reward", ",", "pscore", "=", "pscore", ")", "\n", "invalid_context", "=", "np", ".", "array", "(", "[", "1.0", ",", "1.0", "]", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "learner", ".", "predict_proba", "(", "context", "=", "invalid_context", ")", "\n", "\n", "# inconsistency between dim_context and context", "\n", "", "desc", "=", "\"Expected `context.shape[1]\"", "\n", "with", "pytest", ".", "raises", "(", "ValueError", ",", "match", "=", "f\"{desc}*\"", ")", ":", "\n", "        ", "learner", "=", "NNPolicyLearner", "(", "\n", "n_actions", "=", "n_actions", ",", "\n", "len_list", "=", "len_list", ",", "\n", "dim_context", "=", "2", ",", "\n", "off_policy_objective", "=", "\"ipw\"", ",", "\n", ")", "\n", "learner", ".", "fit", "(", "context", "=", "context", ",", "action", "=", "action", ",", "reward", "=", "reward", ",", "pscore", "=", "pscore", ")", "\n", "invalid_context", "=", "np", ".", "array", "(", "[", "[", "1.0", ",", "1.0", ",", "1.0", "]", "]", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "learner", ".", "predict_proba", "(", "context", "=", "invalid_context", ")", "\n", "\n", "", "learner", "=", "NNPolicyLearner", "(", "\n", "n_actions", "=", "n_actions", ",", "\n", "len_list", "=", "len_list", ",", "\n", "dim_context", "=", "2", ",", "\n", "off_policy_objective", "=", "\"ipw\"", ",", "\n", ")", "\n", "learner", ".", "fit", "(", "context", "=", "context", ",", "action", "=", "action", ",", "reward", "=", "reward", ",", "pscore", "=", "pscore", ")", "\n", "action_dist", "=", "learner", ".", "predict_proba", "(", "context", "=", "context_test", ")", "\n", "assert", "np", ".", "allclose", "(", "\n", "action_dist", ".", "sum", "(", "1", ")", ",", "np", ".", "ones_like", "(", "(", "context_test", ".", "shape", "[", "0", "]", ",", "len_list", ")", ")", "\n", ")", "\n", "assert", "action_dist", ".", "shape", "[", "0", "]", "==", "context_test", ".", "shape", "[", "0", "]", "\n", "assert", "action_dist", ".", "shape", "[", "1", "]", "==", "n_actions", "\n", "assert", "action_dist", ".", "shape", "[", "2", "]", "==", "len_list", "\n", "", ""]], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.test_offline_learner_performance.RandomPolicy.__post_init__": [[95, 97], ["super().__post_init__"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.__post_init__"], ["    ", "def", "__post_init__", "(", "self", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__post_init__", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.test_offline_learner_performance.RandomPolicy.fit": [[98, 100], ["None"], "methods", ["None"], ["", "def", "fit", "(", "self", ")", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.test_offline_learner_performance.RandomPolicy.predict": [[101, 106], ["numpy.random.rand"], "methods", ["None"], ["", "def", "predict", "(", "self", ",", "context", ":", "np", ".", "ndarray", ")", "->", "np", ".", "ndarray", ":", "\n", "\n", "        ", "n_rounds", "=", "context", ".", "shape", "[", "0", "]", "\n", "action_dist", "=", "np", ".", "random", ".", "rand", "(", "n_rounds", ",", "self", ".", "n_actions", ",", "self", ".", "len_list", ")", "\n", "return", "action_dist", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.test_offline_learner_performance.UniformSampleWeightLearner.__post_init__": [[113, 122], ["super().__post_init__", "sklearn.linear_model.LogisticRegression", "sklearn.base.clone", "sklearn.base.is_classifier", "ValueError", "numpy.arange"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.__post_init__"], ["def", "__post_init__", "(", "self", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__post_init__", "(", ")", "\n", "if", "self", ".", "base_classifier", "is", "None", ":", "\n", "            ", "self", ".", "base_classifier", "=", "LogisticRegression", "(", "random_state", "=", "12345", ")", "\n", "", "else", ":", "\n", "            ", "if", "not", "is_classifier", "(", "self", ".", "base_classifier", ")", ":", "\n", "                ", "raise", "ValueError", "(", "\"`base_classifier` must be a classifier\"", ")", "\n", "", "", "self", ".", "base_classifier_list", "=", "[", "\n", "clone", "(", "self", ".", "base_classifier", ")", "for", "_", "in", "np", ".", "arange", "(", "self", ".", "len_list", ")", "\n", "]", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.test_offline_learner_performance.UniformSampleWeightLearner._create_train_data_for_opl": [[124, 134], ["None"], "methods", ["None"], ["", "def", "_create_train_data_for_opl", "(", "\n", "self", ",", "\n", "context", ":", "np", ".", "ndarray", ",", "\n", "action", ":", "np", ".", "ndarray", ",", "\n", "reward", ":", "np", ".", "ndarray", ",", "\n", "pscore", ":", "np", ".", "ndarray", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "Tuple", "[", "np", ".", "ndarray", ",", "np", ".", "ndarray", ",", "np", ".", "ndarray", "]", ":", "\n", "\n", "        ", "return", "context", ",", "(", "reward", "/", "pscore", ")", ",", "action", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.test_offline_learner_performance.UniformSampleWeightLearner.fit": [[135, 158], ["numpy.arange", "numpy.int32", "numpy.zeros_like", "test_offline_learner_performance.UniformSampleWeightLearner._create_train_data_for_opl", "test_offline_learner_performance.UniformSampleWeightLearner.base_classifier_list[].fit", "numpy.ones_like", "action.max"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.policy.offline.NNPolicyLearner._create_train_data_for_opl", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.classification_model.PropensityScoreEstimator.fit"], ["", "def", "fit", "(", "\n", "self", ",", "\n", "context", ":", "np", ".", "ndarray", ",", "\n", "action", ":", "np", ".", "ndarray", ",", "\n", "reward", ":", "np", ".", "ndarray", ",", "\n", "pscore", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "position", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", ")", "->", "None", ":", "\n", "\n", "        ", "if", "pscore", "is", "None", ":", "\n", "            ", "n_actions", "=", "np", ".", "int32", "(", "action", ".", "max", "(", ")", "+", "1", ")", "\n", "pscore", "=", "np", ".", "ones_like", "(", "action", ")", "/", "n_actions", "\n", "", "if", "position", "is", "None", "or", "self", ".", "len_list", "==", "1", ":", "\n", "            ", "position", "=", "np", ".", "zeros_like", "(", "action", ",", "dtype", "=", "int", ")", "\n", "\n", "", "for", "pos_", "in", "np", ".", "arange", "(", "self", ".", "len_list", ")", ":", "\n", "            ", "X", ",", "sample_weight", ",", "y", "=", "self", ".", "_create_train_data_for_opl", "(", "\n", "context", "=", "context", "[", "position", "==", "pos_", "]", ",", "\n", "action", "=", "action", "[", "position", "==", "pos_", "]", ",", "\n", "reward", "=", "reward", "[", "position", "==", "pos_", "]", ",", "\n", "pscore", "=", "pscore", "[", "position", "==", "pos_", "]", ",", "\n", ")", "\n", "self", ".", "base_classifier_list", "[", "pos_", "]", ".", "fit", "(", "X", "=", "X", ",", "y", "=", "y", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.test_offline_learner_performance.UniformSampleWeightLearner.predict": [[159, 173], ["numpy.zeros", "numpy.arange", "test_offline_learner_performance.UniformSampleWeightLearner.base_classifier_list[].predict", "numpy.arange", "numpy.ones"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.classification_model.PropensityScoreEstimator.predict"], ["", "", "def", "predict", "(", "self", ",", "context", ":", "np", ".", "ndarray", ")", "->", "np", ".", "ndarray", ":", "\n", "\n", "        ", "n_rounds", "=", "context", ".", "shape", "[", "0", "]", "\n", "action_dist", "=", "np", ".", "zeros", "(", "(", "n_rounds", ",", "self", ".", "n_actions", ",", "self", ".", "len_list", ")", ")", "\n", "for", "pos_", "in", "np", ".", "arange", "(", "self", ".", "len_list", ")", ":", "\n", "            ", "predicted_actions_at_position", "=", "self", ".", "base_classifier_list", "[", "pos_", "]", ".", "predict", "(", "\n", "context", "\n", ")", "\n", "action_dist", "[", "\n", "np", ".", "arange", "(", "n_rounds", ")", ",", "\n", "predicted_actions_at_position", ",", "\n", "np", ".", "ones", "(", "n_rounds", ",", "dtype", "=", "int", ")", "*", "pos_", ",", "\n", "]", "+=", "1", "\n", "", "return", "action_dist", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.test_offline_learner_performance.test_offline_policy_learner_performance": [[175, 335], ["pytest.mark.parametrize", "list", "list", "list", "list", "list", "enumerate", "print", "print", "print", "print", "print", "obp.dataset.SyntheticBanditDataset", "obp.dataset.SyntheticBanditDataset.obtain_batch_bandit_feedback", "obp.dataset.SyntheticBanditDataset.obtain_batch_bandit_feedback", "obp.policy.IPWLearner", "obp.policy.QLearner", "obp.policy.NNPolicyLearner", "test_offline_learner_performance.RandomPolicy", "test_offline_learner_performance.UniformSampleWeightLearner", "obp.policy.IPWLearner.fit", "obp.policy.QLearner.fit", "obp.policy.NNPolicyLearner.fit", "test_offline_learner_performance.UniformSampleWeightLearner.fit", "obp.policy.IPWLearner.predict", "obp.policy.QLearner.predict", "obp.policy.NNPolicyLearner.predict", "test_offline_learner_performance.RandomPolicy.predict", "test_offline_learner_performance.UniformSampleWeightLearner.predict", "obp.dataset.SyntheticBanditDataset.calc_ground_truth_policy_value", "obp.dataset.SyntheticBanditDataset.calc_ground_truth_policy_value", "obp.dataset.SyntheticBanditDataset.calc_ground_truth_policy_value", "obp.dataset.SyntheticBanditDataset.calc_ground_truth_policy_value", "obp.dataset.SyntheticBanditDataset.calc_ground_truth_policy_value", "joblib.Parallel", "list.append", "list.append", "list.append", "list.append", "list.append", "numpy.mean", "numpy.mean", "numpy.mean", "numpy.mean", "numpy.mean", "numpy.mean", "numpy.mean", "numpy.mean", "numpy.mean", "numpy.mean", "numpy.mean", "numpy.mean", "joblib.delayed", "numpy.arange", "numpy.mean", "numpy.mean", "numpy.mean", "numpy.mean", "numpy.mean"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.multiclass.MultiClassToBanditReduction.obtain_batch_bandit_feedback", "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.multiclass.MultiClassToBanditReduction.obtain_batch_bandit_feedback", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.classification_model.PropensityScoreEstimator.fit", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.classification_model.PropensityScoreEstimator.fit", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.classification_model.PropensityScoreEstimator.fit", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.classification_model.PropensityScoreEstimator.fit", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.classification_model.PropensityScoreEstimator.predict", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.classification_model.PropensityScoreEstimator.predict", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.classification_model.PropensityScoreEstimator.predict", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.classification_model.PropensityScoreEstimator.predict", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.classification_model.PropensityScoreEstimator.predict", "home.repos.pwc.inspect_result.st-tech_zr-obp.simulator.simulator.calc_ground_truth_policy_value", "home.repos.pwc.inspect_result.st-tech_zr-obp.simulator.simulator.calc_ground_truth_policy_value", "home.repos.pwc.inspect_result.st-tech_zr-obp.simulator.simulator.calc_ground_truth_policy_value", "home.repos.pwc.inspect_result.st-tech_zr-obp.simulator.simulator.calc_ground_truth_policy_value", "home.repos.pwc.inspect_result.st-tech_zr-obp.simulator.simulator.calc_ground_truth_policy_value"], ["", "", "@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"n_rounds, n_actions, dim_context, base_model_for_evaluation_policy, base_model_for_reg_model\"", ",", "\n", "offline_experiment_configurations", ",", "\n", ")", "\n", "def", "test_offline_policy_learner_performance", "(", "\n", "n_rounds", ":", "int", ",", "\n", "n_actions", ":", "int", ",", "\n", "dim_context", ":", "int", ",", "\n", "base_model_for_evaluation_policy", ":", "str", ",", "\n", "base_model_for_reg_model", ":", "str", ",", "\n", ")", "->", "None", ":", "\n", "    ", "def", "process", "(", "i", ":", "int", ")", ":", "\n", "# synthetic data generator", "\n", "        ", "dataset", "=", "SyntheticBanditDataset", "(", "\n", "n_actions", "=", "n_actions", ",", "\n", "dim_context", "=", "dim_context", ",", "\n", "reward_function", "=", "logistic_reward_function", ",", "\n", "behavior_policy_function", "=", "linear_behavior_policy", ",", "\n", "random_state", "=", "i", ",", "\n", ")", "\n", "# sample new training and test sets of synthetic logged bandit data", "\n", "bandit_feedback_train", "=", "dataset", ".", "obtain_batch_bandit_feedback", "(", "n_rounds", "=", "n_rounds", ")", "\n", "bandit_feedback_test", "=", "dataset", ".", "obtain_batch_bandit_feedback", "(", "n_rounds", "=", "n_rounds", ")", "\n", "\n", "# defining policy learners", "\n", "ipw_policy", "=", "IPWLearner", "(", "\n", "n_actions", "=", "dataset", ".", "n_actions", ",", "\n", "base_classifier", "=", "base_model_dict", "[", "base_model_for_evaluation_policy", "]", "(", "\n", "**", "hyperparams", "[", "base_model_for_evaluation_policy", "]", "\n", ")", ",", "\n", ")", "\n", "q_policy", "=", "QLearner", "(", "\n", "n_actions", "=", "dataset", ".", "n_actions", ",", "\n", "base_model", "=", "base_model_dict", "[", "base_model_for_evaluation_policy", "]", "(", "\n", "**", "hyperparams", "[", "base_model_for_evaluation_policy", "]", "\n", ")", ",", "\n", ")", "\n", "nn_policy", "=", "NNPolicyLearner", "(", "\n", "n_actions", "=", "dataset", ".", "n_actions", ",", "\n", "dim_context", "=", "dim_context", ",", "\n", "off_policy_objective", "=", "\"ipw\"", ",", "\n", ")", "\n", "# baseline method 1. RandomPolicy", "\n", "random_policy", "=", "RandomPolicy", "(", "n_actions", "=", "dataset", ".", "n_actions", ")", "\n", "# baseline method 2. UniformSampleWeightLearner", "\n", "uniform_sample_weight_policy", "=", "UniformSampleWeightLearner", "(", "\n", "n_actions", "=", "dataset", ".", "n_actions", ",", "\n", "base_classifier", "=", "base_model_dict", "[", "base_model_for_evaluation_policy", "]", "(", "\n", "**", "hyperparams", "[", "base_model_for_evaluation_policy", "]", "\n", ")", ",", "\n", ")", "\n", "\n", "# policy training", "\n", "ipw_policy", ".", "fit", "(", "\n", "context", "=", "bandit_feedback_train", "[", "\"context\"", "]", ",", "\n", "action", "=", "bandit_feedback_train", "[", "\"action\"", "]", ",", "\n", "reward", "=", "bandit_feedback_train", "[", "\"reward\"", "]", ",", "\n", "pscore", "=", "bandit_feedback_train", "[", "\"pscore\"", "]", ",", "\n", ")", "\n", "q_policy", ".", "fit", "(", "\n", "context", "=", "bandit_feedback_train", "[", "\"context\"", "]", ",", "\n", "action", "=", "bandit_feedback_train", "[", "\"action\"", "]", ",", "\n", "reward", "=", "bandit_feedback_train", "[", "\"reward\"", "]", ",", "\n", "pscore", "=", "bandit_feedback_train", "[", "\"pscore\"", "]", ",", "\n", ")", "\n", "nn_policy", ".", "fit", "(", "\n", "context", "=", "bandit_feedback_train", "[", "\"context\"", "]", ",", "\n", "action", "=", "bandit_feedback_train", "[", "\"action\"", "]", ",", "\n", "reward", "=", "bandit_feedback_train", "[", "\"reward\"", "]", ",", "\n", "pscore", "=", "bandit_feedback_train", "[", "\"pscore\"", "]", ",", "\n", ")", "\n", "uniform_sample_weight_policy", ".", "fit", "(", "\n", "context", "=", "bandit_feedback_train", "[", "\"context\"", "]", ",", "\n", "action", "=", "bandit_feedback_train", "[", "\"action\"", "]", ",", "\n", "reward", "=", "bandit_feedback_train", "[", "\"reward\"", "]", ",", "\n", "pscore", "=", "bandit_feedback_train", "[", "\"pscore\"", "]", ",", "\n", ")", "\n", "\n", "# prediction/making decisions", "\n", "ipw_action_dist", "=", "ipw_policy", ".", "predict", "(", "\n", "context", "=", "bandit_feedback_test", "[", "\"context\"", "]", ",", "\n", ")", "\n", "q_action_dist", "=", "q_policy", ".", "predict", "(", "\n", "context", "=", "bandit_feedback_test", "[", "\"context\"", "]", ",", "\n", ")", "\n", "nn_action_dist", "=", "nn_policy", ".", "predict", "(", "\n", "context", "=", "bandit_feedback_test", "[", "\"context\"", "]", ",", "\n", ")", "\n", "random_action_dist", "=", "random_policy", ".", "predict", "(", "\n", "context", "=", "bandit_feedback_test", "[", "\"context\"", "]", ",", "\n", ")", "\n", "uniform_sample_weight_action_dist", "=", "uniform_sample_weight_policy", ".", "predict", "(", "\n", "context", "=", "bandit_feedback_test", "[", "\"context\"", "]", ",", "\n", ")", "\n", "\n", "# evaluation", "\n", "gt_ipw_learner", "=", "dataset", ".", "calc_ground_truth_policy_value", "(", "\n", "expected_reward", "=", "bandit_feedback_test", "[", "\"expected_reward\"", "]", ",", "\n", "action_dist", "=", "ipw_action_dist", ",", "\n", ")", "\n", "gt_q_learner", "=", "dataset", ".", "calc_ground_truth_policy_value", "(", "\n", "expected_reward", "=", "bandit_feedback_test", "[", "\"expected_reward\"", "]", ",", "\n", "action_dist", "=", "q_action_dist", ",", "\n", ")", "\n", "gt_nn_learner", "=", "dataset", ".", "calc_ground_truth_policy_value", "(", "\n", "expected_reward", "=", "bandit_feedback_test", "[", "\"expected_reward\"", "]", ",", "\n", "action_dist", "=", "nn_action_dist", ",", "\n", ")", "\n", "gt_random_policy", "=", "dataset", ".", "calc_ground_truth_policy_value", "(", "\n", "expected_reward", "=", "bandit_feedback_test", "[", "\"expected_reward\"", "]", ",", "\n", "action_dist", "=", "random_action_dist", ",", "\n", ")", "\n", "gt_uniform_sample_weight_learner", "=", "dataset", ".", "calc_ground_truth_policy_value", "(", "\n", "expected_reward", "=", "bandit_feedback_test", "[", "\"expected_reward\"", "]", ",", "\n", "action_dist", "=", "uniform_sample_weight_action_dist", ",", "\n", ")", "\n", "\n", "return", "(", "\n", "gt_ipw_learner", ",", "\n", "gt_q_learner", ",", "\n", "gt_nn_learner", ",", "\n", "gt_random_policy", ",", "\n", "gt_uniform_sample_weight_learner", ",", "\n", ")", "\n", "\n", "", "n_runs", "=", "10", "\n", "processed", "=", "Parallel", "(", "\n", "n_jobs", "=", "-", "1", ",", "\n", "verbose", "=", "0", ",", "\n", ")", "(", "[", "delayed", "(", "process", ")", "(", "i", ")", "for", "i", "in", "np", ".", "arange", "(", "n_runs", ")", "]", ")", "\n", "list_gt_ipw", "=", "list", "(", ")", "\n", "list_gt_q", "=", "list", "(", ")", "\n", "list_gt_nn", "=", "list", "(", ")", "\n", "list_gt_random", "=", "list", "(", ")", "\n", "list_gt_unif_ipw", "=", "list", "(", ")", "\n", "for", "i", ",", "gt_policy_values", "in", "enumerate", "(", "processed", ")", ":", "\n", "        ", "gt_ipw", ",", "gt_q", ",", "gt_nn", ",", "gt_random", ",", "gt_unif_ipw", "=", "gt_policy_values", "\n", "list_gt_ipw", ".", "append", "(", "gt_ipw", ")", "\n", "list_gt_q", ".", "append", "(", "gt_q", ")", "\n", "list_gt_nn", ".", "append", "(", "gt_nn", ")", "\n", "list_gt_random", ".", "append", "(", "gt_random", ")", "\n", "list_gt_unif_ipw", ".", "append", "(", "gt_unif_ipw", ")", "\n", "\n", "# baseline learner performance", "\n", "", "print", "(", "f\"Performance of Random is {np.mean(list_gt_random)}\"", ")", "\n", "print", "(", "\n", "f\"Performance of IPWLearner with Uniform Weight is {np.mean(list_gt_unif_ipw)}\"", "\n", ")", "\n", "# ipw learner performance", "\n", "print", "(", "f\"Performance of IPWLearner is {np.mean(list_gt_ipw)}\"", ")", "\n", "assert", "np", ".", "mean", "(", "list_gt_ipw", ")", ">", "np", ".", "mean", "(", "list_gt_random", ")", "\n", "assert", "np", ".", "mean", "(", "list_gt_ipw", ")", ">", "np", ".", "mean", "(", "list_gt_unif_ipw", ")", "\n", "# q learner performance", "\n", "print", "(", "f\"Performance of QLearner is {np.mean(list_gt_q)}\"", ")", "\n", "assert", "np", ".", "mean", "(", "list_gt_q", ")", ">", "np", ".", "mean", "(", "list_gt_random", ")", "\n", "assert", "np", ".", "mean", "(", "list_gt_q", ")", ">", "np", ".", "mean", "(", "list_gt_unif_ipw", ")", "\n", "# nn policy learner performance", "\n", "print", "(", "f\"Performance of NNPolicyLearner is {np.mean(list_gt_nn)}\"", ")", "\n", "assert", "np", ".", "mean", "(", "list_gt_nn", ")", ">", "np", ".", "mean", "(", "list_gt_random", ")", "\n", "assert", "np", ".", "mean", "(", "list_gt_nn", ")", ">", "np", ".", "mean", "(", "list_gt_unif_ipw", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.logistic.BaseLogisticPolicy.__post_init__": [[50, 71], ["super().__post_init__", "sklearn.utils.check_scalar", "sklearn.utils.check_scalar", "ValueError", "ValueError", "numpy.ones", "numpy.ones", "logistic.MiniBatchLogisticRegression", "numpy.arange"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.__post_init__"], ["def", "__post_init__", "(", "self", ")", "->", "None", ":", "\n", "        ", "\"\"\"Initialize class.\"\"\"", "\n", "super", "(", ")", ".", "__post_init__", "(", ")", "\n", "check_scalar", "(", "self", ".", "alpha_", ",", "\"alpha_\"", ",", "float", ")", "\n", "if", "self", ".", "alpha_", "<=", "0.0", ":", "\n", "            ", "raise", "ValueError", "(", "f\"`alpha_`= {self.alpha_}, must be > 0.0.\"", ")", "\n", "\n", "", "check_scalar", "(", "self", ".", "lambda_", ",", "\"lambda_\"", ",", "float", ")", "\n", "if", "self", ".", "lambda_", "<=", "0.0", ":", "\n", "            ", "raise", "ValueError", "(", "f\"`lambda_`= {self.lambda_}, must be > 0.0.\"", ")", "\n", "\n", "", "self", ".", "alpha_list", "=", "self", ".", "alpha_", "*", "np", ".", "ones", "(", "self", ".", "n_actions", ")", "\n", "self", ".", "lambda_list", "=", "self", ".", "lambda_", "*", "np", ".", "ones", "(", "self", ".", "n_actions", ")", "\n", "self", ".", "model_list", "=", "[", "\n", "MiniBatchLogisticRegression", "(", "\n", "lambda_", "=", "self", ".", "lambda_list", "[", "i", "]", ",", "\n", "alpha", "=", "self", ".", "alpha_list", "[", "i", "]", ",", "\n", "dim", "=", "self", ".", "dim", ",", "\n", "random_state", "=", "self", ".", "random_state", ",", "\n", ")", "\n", "for", "i", "in", "np", ".", "arange", "(", "self", ".", "n_actions", ")", "\n", "]", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.logistic.BaseLogisticPolicy.update_params": [[73, 101], ["logistic.BaseLogisticPolicy.reward_lists[].append", "logistic.BaseLogisticPolicy.context_lists[].append", "enumerate", "model.fit", "numpy.arange", "numpy.arange", "len", "numpy.concatenate", "numpy.array"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.classification_model.PropensityScoreEstimator.fit"], ["", "def", "update_params", "(", "self", ",", "action", ":", "int", ",", "reward", ":", "float", ",", "context", ":", "np", ".", "ndarray", ")", "->", "None", ":", "\n", "        ", "\"\"\"Update policy parameters.\n\n        Parameters\n        ----------\n        action: int\n            Selected action by the policy.\n\n        reward: float\n            Observed reward for the chosen action and position.\n\n        context: array-like, shape (1, dim_context)\n            Observed context vector.\n\n        \"\"\"", "\n", "self", ".", "n_trial", "+=", "1", "\n", "self", ".", "action_counts", "[", "action", "]", "+=", "1", "\n", "self", ".", "reward_lists", "[", "action", "]", ".", "append", "(", "reward", ")", "\n", "self", ".", "context_lists", "[", "action", "]", ".", "append", "(", "context", ")", "\n", "if", "self", ".", "n_trial", "%", "self", ".", "batch_size", "==", "0", ":", "\n", "            ", "for", "action", ",", "model", "in", "enumerate", "(", "self", ".", "model_list", ")", ":", "\n", "                ", "if", "not", "len", "(", "self", ".", "reward_lists", "[", "action", "]", ")", "==", "0", ":", "\n", "                    ", "model", ".", "fit", "(", "\n", "X", "=", "np", ".", "concatenate", "(", "self", ".", "context_lists", "[", "action", "]", ",", "axis", "=", "0", ")", ",", "\n", "y", "=", "np", ".", "array", "(", "self", ".", "reward_lists", "[", "action", "]", ")", ",", "\n", ")", "\n", "", "", "self", ".", "reward_lists", "=", "[", "[", "]", "for", "_", "in", "np", ".", "arange", "(", "self", ".", "n_actions", ")", "]", "\n", "self", ".", "context_lists", "=", "[", "[", "]", "for", "_", "in", "np", ".", "arange", "(", "self", ".", "n_actions", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.logistic.LogisticEpsilonGreedy.__post_init__": [[138, 144], ["sklearn.utils.check_scalar", "logistic.BaseLogisticPolicy.__post_init__"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.__post_init__"], ["def", "__post_init__", "(", "self", ")", "->", "None", ":", "\n", "        ", "\"\"\"Initialize class.\"\"\"", "\n", "check_scalar", "(", "self", ".", "epsilon", ",", "\"epsilon\"", ",", "float", ",", "min_val", "=", "0.0", ",", "max_val", "=", "1.0", ")", "\n", "self", ".", "policy_name", "=", "f\"logistic_egreedy_{self.epsilon}\"", "\n", "\n", "super", "(", ")", ".", "__post_init__", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.logistic.LogisticEpsilonGreedy.select_action": [[145, 167], ["logistic.LogisticEpsilonGreedy.random_.rand", "numpy.array().flatten", "logistic.LogisticEpsilonGreedy.random_.choice", "numpy.array", "numpy.array().flatten.argsort", "model.predict_proba"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.policy.offline.NNPolicyLearner.predict_proba"], ["", "def", "select_action", "(", "self", ",", "context", ":", "np", ".", "ndarray", ")", "->", "np", ".", "ndarray", ":", "\n", "        ", "\"\"\"Select action for new data.\n\n        Parameters\n        ----------\n        context: array-like, shape (1, dim_context)\n            Observed context vector.\n\n        Returns\n        ----------\n        selected_actions: array-like, shape (len_list, )\n            List of selected actions.\n\n        \"\"\"", "\n", "if", "self", ".", "random_", ".", "rand", "(", ")", ">", "self", ".", "epsilon", ":", "\n", "            ", "theta", "=", "np", ".", "array", "(", "\n", "[", "model", ".", "predict_proba", "(", "context", ")", "for", "model", "in", "self", ".", "model_list", "]", "\n", ")", ".", "flatten", "(", ")", "\n", "return", "theta", ".", "argsort", "(", ")", "[", ":", ":", "-", "1", "]", "[", ":", "self", ".", "len_list", "]", "\n", "", "else", ":", "\n", "            ", "return", "self", ".", "random_", ".", "choice", "(", "\n", "self", ".", "n_actions", ",", "size", "=", "self", ".", "len_list", ",", "replace", "=", "False", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.logistic.LogisticUCB.__post_init__": [[210, 216], ["sklearn.utils.check_scalar", "logistic.BaseLogisticPolicy.__post_init__"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.__post_init__"], ["def", "__post_init__", "(", "self", ")", "->", "None", ":", "\n", "        ", "\"\"\"Initialize class.\"\"\"", "\n", "check_scalar", "(", "self", ".", "epsilon", ",", "\"epsilon\"", ",", "float", ",", "min_val", "=", "0.0", ")", "\n", "self", ".", "policy_name", "=", "f\"logistic_ucb_{self.epsilon}\"", "\n", "\n", "super", "(", ")", ".", "__post_init__", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.logistic.LogisticUCB.select_action": [[217, 242], ["numpy.array().flatten", "numpy.array().flatten", "numpy.array", "numpy.array", "ucb_score.argsort", "model.predict_proba", "numpy.sqrt", "numpy.sum"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.policy.offline.NNPolicyLearner.predict_proba"], ["", "def", "select_action", "(", "self", ",", "context", ":", "np", ".", "ndarray", ")", "->", "np", ".", "ndarray", ":", "\n", "        ", "\"\"\"Select action for new data.\n\n        Parameters\n        ------------\n        context: array-like, shape (1, dim_context)\n            Observed context vector.\n\n        Returns\n        ----------\n        selected_actions: array-like, shape (len_list, )\n            List of selected actions.\n\n        \"\"\"", "\n", "theta", "=", "np", ".", "array", "(", "\n", "[", "model", ".", "predict_proba", "(", "context", ")", "for", "model", "in", "self", ".", "model_list", "]", "\n", ")", ".", "flatten", "(", ")", "\n", "std", "=", "np", ".", "array", "(", "\n", "[", "\n", "np", ".", "sqrt", "(", "np", ".", "sum", "(", "(", "model", ".", "_q", "**", "(", "-", "1", ")", ")", "*", "(", "context", "**", "2", ")", ")", ")", "\n", "for", "model", "in", "self", ".", "model_list", "\n", "]", "\n", ")", ".", "flatten", "(", ")", "\n", "ucb_score", "=", "theta", "+", "self", ".", "epsilon", "*", "std", "\n", "return", "ucb_score", ".", "argsort", "(", ")", "[", ":", ":", "-", "1", "]", "[", ":", "self", ".", "len_list", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.logistic.LogisticTS.__post_init__": [[281, 284], ["logistic.BaseLogisticPolicy.__post_init__"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.__post_init__"], ["def", "__post_init__", "(", "self", ")", "->", "None", ":", "\n", "        ", "\"\"\"Initialize class.\"\"\"", "\n", "super", "(", ")", ".", "__post_init__", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.logistic.LogisticTS.select_action": [[285, 303], ["numpy.array().flatten", "numpy.array", "numpy.array().flatten.argsort", "model.predict_proba_with_sampling"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.policy.logistic.MiniBatchLogisticRegression.predict_proba_with_sampling"], ["", "def", "select_action", "(", "self", ",", "context", ":", "np", ".", "ndarray", ")", "->", "np", ".", "ndarray", ":", "\n", "        ", "\"\"\"Select action for new data.\n\n        Parameters\n        ----------\n        context: array-like, shape (1, dim_context)\n            Observed context vector.\n\n        Returns\n        ----------\n        selected_actions: array-like, shape (len_list, )\n            List of selected actions.\n\n        \"\"\"", "\n", "theta", "=", "np", ".", "array", "(", "\n", "[", "model", ".", "predict_proba_with_sampling", "(", "context", ")", "for", "model", "in", "self", ".", "model_list", "]", "\n", ")", ".", "flatten", "(", ")", "\n", "return", "theta", ".", "argsort", "(", ")", "[", ":", ":", "-", "1", "]", "[", ":", "self", ".", "len_list", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.logistic.MiniBatchLogisticRegression.__post_init__": [[314, 319], ["numpy.zeros", "sklearn.utils.check_random_state", "numpy.ones"], "methods", ["None"], ["def", "__post_init__", "(", "self", ")", "->", "None", ":", "\n", "        ", "\"\"\"Initialize Class.\"\"\"", "\n", "self", ".", "_m", "=", "np", ".", "zeros", "(", "self", ".", "dim", ")", "\n", "self", ".", "_q", "=", "np", ".", "ones", "(", "self", ".", "dim", ")", "*", "self", ".", "lambda_", "\n", "self", ".", "random_", "=", "check_random_state", "(", "self", ".", "random_state", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.logistic.MiniBatchLogisticRegression.loss": [[320, 326], ["numpy.log().sum", "numpy.log", "numpy.exp", "w.dot"], "methods", ["None"], ["", "def", "loss", "(", "self", ",", "w", ":", "np", ".", "ndarray", ",", "*", "args", ")", "->", "float", ":", "\n", "        ", "\"\"\"Calculate loss function.\"\"\"", "\n", "X", ",", "y", "=", "args", "\n", "return", "(", "\n", "0.5", "*", "(", "self", ".", "_q", "*", "(", "w", "-", "self", ".", "_m", ")", ")", ".", "dot", "(", "w", "-", "self", ".", "_m", ")", "\n", "+", "np", ".", "log", "(", "1", "+", "np", ".", "exp", "(", "-", "y", "*", "w", ".", "dot", "(", "X", ".", "T", ")", ")", ")", ".", "sum", "(", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.logistic.MiniBatchLogisticRegression.grad": [[328, 334], ["numpy.exp", "w.dot"], "methods", ["None"], ["", "def", "grad", "(", "self", ",", "w", ":", "np", ".", "ndarray", ",", "*", "args", ")", "->", "np", ".", "ndarray", ":", "\n", "        ", "\"\"\"Calculate gradient.\"\"\"", "\n", "X", ",", "y", "=", "args", "\n", "return", "self", ".", "_q", "*", "(", "w", "-", "self", ".", "_m", ")", "+", "(", "-", "1", ")", "*", "(", "\n", "(", "(", "y", "*", "X", ".", "T", ")", "/", "(", "1.0", "+", "np", ".", "exp", "(", "y", "*", "w", ".", "dot", "(", "X", ".", "T", ")", ")", ")", ")", ".", "T", "\n", ")", ".", "sum", "(", "axis", "=", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.logistic.MiniBatchLogisticRegression.sample": [[335, 338], ["logistic.MiniBatchLogisticRegression.random_.normal", "logistic.MiniBatchLogisticRegression.sd"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.policy.logistic.MiniBatchLogisticRegression.sd"], ["", "def", "sample", "(", "self", ")", "->", "np", ".", "ndarray", ":", "\n", "        ", "\"\"\"Sample coefficient vector from the prior distribution.\"\"\"", "\n", "return", "self", ".", "random_", ".", "normal", "(", "self", ".", "_m", ",", "self", ".", "sd", "(", ")", ",", "size", "=", "self", ".", "dim", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.logistic.MiniBatchLogisticRegression.fit": [[339, 351], ["scipy.optimize.minimize", "numpy.exp", "X.dot"], "methods", ["None"], ["", "def", "fit", "(", "self", ",", "X", ":", "np", ".", "ndarray", ",", "y", ":", "np", ".", "ndarray", ")", ":", "\n", "        ", "\"\"\"Update coefficient vector by the mini-batch data.\"\"\"", "\n", "self", ".", "_m", "=", "minimize", "(", "\n", "self", ".", "loss", ",", "\n", "self", ".", "_m", ",", "\n", "args", "=", "(", "X", ",", "y", ")", ",", "\n", "jac", "=", "self", ".", "grad", ",", "\n", "method", "=", "\"L-BFGS-B\"", ",", "\n", "options", "=", "{", "\"maxiter\"", ":", "20", ",", "\"disp\"", ":", "False", "}", ",", "\n", ")", ".", "x", "\n", "P", "=", "(", "1", "+", "np", ".", "exp", "(", "1", "+", "X", ".", "dot", "(", "self", ".", "_m", ")", ")", ")", "**", "(", "-", "1", ")", "\n", "self", ".", "_q", "=", "self", ".", "_q", "+", "(", "P", "*", "(", "1", "-", "P", ")", ")", ".", "dot", "(", "X", "**", "2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.logistic.MiniBatchLogisticRegression.sd": [[352, 355], ["None"], "methods", ["None"], ["", "def", "sd", "(", "self", ")", "->", "np", ".", "ndarray", ":", "\n", "        ", "\"\"\"Standard deviation for the coefficient vector.\"\"\"", "\n", "return", "self", ".", "alpha", "*", "(", "self", ".", "_q", ")", "**", "(", "-", "1.0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.logistic.MiniBatchLogisticRegression.predict_proba": [[356, 359], ["utils.sigmoid", "X.dot"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.sigmoid"], ["", "def", "predict_proba", "(", "self", ",", "X", ":", "np", ".", "ndarray", ")", "->", "np", ".", "ndarray", ":", "\n", "        ", "\"\"\"Predict extected probability by the expected coefficient.\"\"\"", "\n", "return", "sigmoid", "(", "X", ".", "dot", "(", "self", ".", "_m", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.logistic.MiniBatchLogisticRegression.predict_proba_with_sampling": [[360, 363], ["utils.sigmoid", "X.dot", "logistic.MiniBatchLogisticRegression.sample"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.sigmoid", "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.logistic.MiniBatchLogisticRegression.sample"], ["", "def", "predict_proba_with_sampling", "(", "self", ",", "X", ":", "np", ".", "ndarray", ")", "->", "np", ".", "ndarray", ":", "\n", "        ", "\"\"\"Predict extected probability by the sampled coefficient.\"\"\"", "\n", "return", "sigmoid", "(", "X", ".", "dot", "(", "self", ".", "sample", "(", ")", ")", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.base.BaseContextFreePolicy.__post_init__": [[43, 54], ["sklearn.utils.check_scalar", "sklearn.utils.check_scalar", "sklearn.utils.check_scalar", "sklearn.utils.check_random_state", "numpy.zeros", "numpy.zeros", "numpy.zeros", "numpy.zeros"], "methods", ["None"], []], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.base.BaseContextFreePolicy.policy_type": [[55, 59], ["None"], "methods", ["None"], []], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.base.BaseContextFreePolicy.initialize": [[60, 68], ["sklearn.utils.check_random_state", "numpy.zeros", "numpy.zeros", "numpy.zeros", "numpy.zeros"], "methods", ["None"], []], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.base.BaseContextFreePolicy.select_action": [[69, 73], ["None"], "methods", ["None"], []], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.base.BaseContextFreePolicy.update_params": [[74, 78], ["None"], "methods", ["None"], []], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.base.BaseContextualPolicy.__post_init__": [[110, 121], ["sklearn.utils.check_scalar", "sklearn.utils.check_scalar", "sklearn.utils.check_scalar", "sklearn.utils.check_scalar", "sklearn.utils.check_random_state", "numpy.zeros", "numpy.arange", "numpy.arange"], "methods", ["None"], []], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.base.BaseContextualPolicy.policy_type": [[122, 126], ["None"], "methods", ["None"], []], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.base.BaseContextualPolicy.initialize": [[127, 134], ["sklearn.utils.check_random_state", "numpy.zeros", "numpy.arange", "numpy.arange"], "methods", ["None"], []], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.base.BaseContextualPolicy.select_action": [[135, 139], ["None"], "methods", ["None"], []], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.base.BaseContextualPolicy.update_params": [[140, 144], ["None"], "methods", ["None"], []], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.base.BaseOfflinePolicyLearner.__post_init__": [[164, 168], ["sklearn.utils.check_scalar", "sklearn.utils.check_scalar"], "methods", ["None"], []], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.base.BaseOfflinePolicyLearner.policy_type": [[169, 173], ["None"], "methods", ["None"], []], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.base.BaseOfflinePolicyLearner.fit": [[174, 180], ["None"], "methods", ["None"], []], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.base.BaseOfflinePolicyLearner.predict": [[181, 197], ["None"], "methods", ["None"], []], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.base.BaseContinuousOfflinePolicyLearner.policy_type": [[203, 207], ["None"], "methods", ["None"], []], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.base.BaseContinuousOfflinePolicyLearner.fit": [[208, 214], ["None"], "methods", ["None"], []], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.base.BaseContinuousOfflinePolicyLearner.predict": [[215, 231], ["None"], "methods", ["None"], []], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.offline_continuous.ContinuousNNPolicyLearner.__post_init__": [[175, 304], ["sklearn.utils.check_scalar", "sklearn.utils.check_scalar", "sklearn.utils.check_scalar", "sklearn.utils.check_scalar", "sklearn.utils.check_scalar", "sklearn.utils.check_scalar", "sklearn.utils.check_scalar", "sklearn.utils.check_scalar", "sklearn.utils.check_scalar", "sklearn.utils.check_scalar", "sklearn.utils.check_scalar", "enumerate", "layer_list.append", "torch.Sequential", "torch.Sequential", "torch.Sequential", "ValueError", "sklearn.utils.check_scalar", "any", "ValueError", "ValueError", "ValueError", "ValueError", "isinstance", "ValueError", "ValueError", "isinstance", "ValueError", "isinstance", "ValueError", "ValueError", "sklearn.utils.check_random_state", "torch.manual_seed", "torch.manual_seed", "torch.manual_seed", "torch.manual_seed", "torch.manual_seed", "torch.manual_seed", "torch.manual_seed", "torch.manual_seed", "torch.manual_seed", "layer_list.append", "layer_list.append", "collections.OrderedDict", "ValueError", "any", "ValueError", "isinstance", "isinstance", "ValueError", "torch.Linear", "torch.Linear", "torch.Linear", "offline_continuous.QFuncEstimatorForContinuousAction", "offline_continuous.QFuncEstimatorForContinuousAction", "isinstance", "isinstance", "torch.Linear", "torch.Linear", "torch.Linear", "activation_layer", "isinstance", "isinstance", "type", "ValueError"], "methods", ["None"], ["def", "__post_init__", "(", "self", ")", "->", "None", ":", "\n", "        ", "\"\"\"Initialize class.\"\"\"", "\n", "check_scalar", "(", "self", ".", "dim_context", ",", "\"dim_context\"", ",", "int", ",", "min_val", "=", "1", ")", "\n", "\n", "if", "self", ".", "pg_method", "not", "in", "[", "\"dpg\"", ",", "\"ipw\"", ",", "\"dr\"", "]", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "f\"pg_method must be one of 'dgp', 'ipw', or 'dr', but {self.pg_method} is given\"", "\n", ")", "\n", "\n", "", "if", "self", ".", "pg_method", "!=", "\"dpg\"", ":", "\n", "            ", "check_scalar", "(", "self", ".", "bandwidth", ",", "\"bandwidth\"", ",", "(", "int", ",", "float", ")", ")", "\n", "if", "self", ".", "bandwidth", "<=", "0", ":", "\n", "                ", "raise", "ValueError", "(", "f\"`bandwidth`= {self.bandwidth}, must be > 0.\"", ")", "\n", "\n", "", "", "if", "self", ".", "output_space", "is", "not", "None", ":", "\n", "            ", "if", "not", "isinstance", "(", "self", ".", "output_space", ",", "tuple", ")", "or", "any", "(", "\n", "[", "not", "isinstance", "(", "o", ",", "(", "int", ",", "float", ")", ")", "for", "o", "in", "self", ".", "output_space", "]", "\n", ")", ":", "\n", "                ", "raise", "ValueError", "(", "\n", "f\"output_space must be tuple of integers or floats, but {self.output_space} is given\"", "\n", ")", "\n", "\n", "", "", "if", "not", "isinstance", "(", "self", ".", "hidden_layer_size", ",", "tuple", ")", "or", "any", "(", "\n", "[", "not", "isinstance", "(", "h", ",", "int", ")", "or", "h", "<=", "0", "for", "h", "in", "self", ".", "hidden_layer_size", "]", "\n", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "f\"`hidden_layer_size` must be a tuple of positive integers, but {self.hidden_layer_size} is given\"", "\n", ")", "\n", "\n", "", "if", "self", ".", "solver", "not", "in", "(", "\"adagrad\"", ",", "\"sgd\"", ",", "\"adam\"", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "f\"`solver` must be one of 'adam', 'adagrad', or 'sgd', but {self.solver} is given\"", "\n", ")", "\n", "\n", "", "check_scalar", "(", "self", ".", "alpha", ",", "\"alpha\"", ",", "float", ",", "min_val", "=", "0.0", ")", "\n", "\n", "if", "self", ".", "batch_size", "!=", "\"auto\"", "and", "(", "\n", "not", "isinstance", "(", "self", ".", "batch_size", ",", "int", ")", "or", "self", ".", "batch_size", "<=", "0", "\n", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "f\"`batch_size` must be a positive integer or 'auto', but {self.batch_size} is given\"", "\n", ")", "\n", "\n", "", "check_scalar", "(", "self", ".", "learning_rate_init", ",", "\"learning_rate_init\"", ",", "float", ")", "\n", "if", "self", ".", "learning_rate_init", "<=", "0.0", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "f\"`learning_rate_init`= {self.learning_rate_init}, must be > 0.0\"", "\n", ")", "\n", "\n", "", "check_scalar", "(", "self", ".", "max_iter", ",", "\"max_iter\"", ",", "int", ",", "min_val", "=", "1", ")", "\n", "\n", "if", "not", "isinstance", "(", "self", ".", "shuffle", ",", "bool", ")", ":", "\n", "            ", "raise", "ValueError", "(", "f\"`shuffle` must be a bool, but {self.shuffle} is given\"", ")", "\n", "\n", "", "check_scalar", "(", "self", ".", "tol", ",", "\"tol\"", ",", "float", ")", "\n", "if", "self", ".", "tol", "<=", "0.0", ":", "\n", "            ", "raise", "ValueError", "(", "f\"`tol`= {self.tol}, must be > 0.0\"", ")", "\n", "\n", "", "check_scalar", "(", "self", ".", "momentum", ",", "\"momentum\"", ",", "float", ",", "min_val", "=", "0.0", ",", "max_val", "=", "1.0", ")", "\n", "\n", "if", "not", "isinstance", "(", "self", ".", "nesterovs_momentum", ",", "bool", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "f\"`nesterovs_momentum` must be a bool, but {self.nesterovs_momentum} is given\"", "\n", ")", "\n", "\n", "", "if", "not", "isinstance", "(", "self", ".", "early_stopping", ",", "bool", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "f\"`early_stopping` must be a bool, but {self.early_stopping} is given\"", "\n", ")", "\n", "\n", "", "check_scalar", "(", "\n", "self", ".", "validation_fraction", ",", "\"validation_fraction\"", ",", "float", ",", "max_val", "=", "1.0", "\n", ")", "\n", "if", "self", ".", "validation_fraction", "<=", "0.0", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "f\"`validation_fraction`= {self.validation_fraction}, must be > 0.0\"", "\n", ")", "\n", "\n", "", "check_scalar", "(", "self", ".", "beta_1", ",", "\"beta_1\"", ",", "float", ",", "min_val", "=", "0.0", ",", "max_val", "=", "1.0", ")", "\n", "check_scalar", "(", "self", ".", "beta_2", ",", "\"beta_2\"", ",", "float", ",", "min_val", "=", "0.0", ",", "max_val", "=", "1.0", ")", "\n", "check_scalar", "(", "self", ".", "epsilon", ",", "\"epsilon\"", ",", "float", ",", "min_val", "=", "0.0", ")", "\n", "check_scalar", "(", "self", ".", "n_iter_no_change", ",", "\"n_iter_no_change\"", ",", "int", ",", "min_val", "=", "1", ")", "\n", "\n", "if", "self", ".", "q_func_estimator_hyperparams", "is", "not", "None", ":", "\n", "            ", "if", "not", "isinstance", "(", "self", ".", "q_func_estimator_hyperparams", ",", "dict", ")", ":", "\n", "                ", "raise", "ValueError", "(", "\n", "\"`q_func_estimator_hyperparams` must be a dict\"", "\n", "f\", but {type(self.q_func_estimator_hyperparams)} is given\"", "\n", ")", "\n", "\n", "", "", "if", "self", ".", "random_state", "is", "not", "None", ":", "\n", "            ", "self", ".", "random_", "=", "check_random_state", "(", "self", ".", "random_state", ")", "\n", "torch", ".", "manual_seed", "(", "self", ".", "random_state", ")", "\n", "\n", "", "if", "self", ".", "activation", "==", "\"identity\"", ":", "\n", "            ", "activation_layer", "=", "nn", ".", "Identity", "\n", "", "elif", "self", ".", "activation", "==", "\"logistic\"", ":", "\n", "            ", "activation_layer", "=", "nn", ".", "Sigmoid", "\n", "", "elif", "self", ".", "activation", "==", "\"tanh\"", ":", "\n", "            ", "activation_layer", "=", "nn", ".", "Tanh", "\n", "", "elif", "self", ".", "activation", "==", "\"relu\"", ":", "\n", "            ", "activation_layer", "=", "nn", ".", "ReLU", "\n", "", "elif", "self", ".", "activation", "==", "\"elu\"", ":", "\n", "            ", "activation_layer", "=", "nn", ".", "ELU", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"`activation` must be one of 'identity', 'logistic', 'tanh', 'relu', or 'elu'\"", "\n", ")", "\n", "\n", "", "layer_list", "=", "[", "]", "\n", "input_size", "=", "self", ".", "dim_context", "\n", "\n", "for", "i", ",", "h", "in", "enumerate", "(", "self", ".", "hidden_layer_size", ")", ":", "\n", "            ", "layer_list", ".", "append", "(", "(", "\"l{}\"", ".", "format", "(", "i", ")", ",", "nn", ".", "Linear", "(", "input_size", ",", "h", ")", ")", ")", "\n", "layer_list", ".", "append", "(", "(", "\"a{}\"", ".", "format", "(", "i", ")", ",", "activation_layer", "(", ")", ")", ")", "\n", "input_size", "=", "h", "\n", "", "layer_list", ".", "append", "(", "(", "\"output\"", ",", "nn", ".", "Linear", "(", "input_size", ",", "1", ")", ")", ")", "\n", "\n", "self", ".", "nn_model", "=", "nn", ".", "Sequential", "(", "OrderedDict", "(", "layer_list", ")", ")", "\n", "\n", "if", "self", ".", "pg_method", "!=", "\"ipw\"", ":", "\n", "            ", "if", "self", ".", "q_func_estimator_hyperparams", "is", "not", "None", ":", "\n", "                ", "self", ".", "q_func_estimator_hyperparams", "[", "\"dim_context\"", "]", "=", "self", ".", "dim_context", "\n", "self", ".", "q_func_estimator", "=", "QFuncEstimatorForContinuousAction", "(", "\n", "**", "self", ".", "q_func_estimator_hyperparams", "\n", ")", "\n", "", "else", ":", "\n", "                ", "self", ".", "q_func_estimator", "=", "QFuncEstimatorForContinuousAction", "(", "\n", "dim_context", "=", "self", ".", "dim_context", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.offline_continuous.ContinuousNNPolicyLearner._create_train_data_for_opl": [[306, 380], ["offline_continuous.NNPolicyDatasetForContinuousAction", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "min", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "max", "torch.utils.data.random_split", "torch.utils.data.random_split", "torch.utils.data.random_split", "torch.utils.data.random_split", "torch.utils.data.random_split", "torch.utils.data.random_split", "torch.utils.data.random_split", "torch.utils.data.random_split", "torch.utils.data.random_split", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "isinstance", "ValueError", "ValueError", "int", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy"], "methods", ["None"], ["", "", "", "def", "_create_train_data_for_opl", "(", "\n", "self", ",", "\n", "context", ":", "np", ".", "ndarray", ",", "\n", "action", ":", "np", ".", "ndarray", ",", "\n", "reward", ":", "np", ".", "ndarray", ",", "\n", "pscore", ":", "np", ".", "ndarray", ",", "\n", ")", "->", "Tuple", "[", "torch", ".", "utils", ".", "data", ".", "DataLoader", ",", "Optional", "[", "torch", ".", "utils", ".", "data", ".", "DataLoader", "]", "]", ":", "\n", "        ", "\"\"\"Create training data for off-policy learning.\n\n        Parameters\n        -----------\n        context: array-like, shape (n_rounds, dim_context)\n            Context vectors observed for each data, i.e., :math:`x_i`.\n\n        action: array-like, shape (n_rounds,)\n            Continuous action values sampled by the logging/behavior policy for each data in logged bandit data, i.e., :math:`a_i`.\n\n        reward: array-like, shape (n_rounds,)\n            Rewards observed for each data in logged bandit data, i.e., :math:`r_i`.\n\n        pscore: array-like, shape (n_rounds,), default=None\n            Propensity scores, the probability of selecting each action by behavior policy in the given logged bandit data.\n\n        Returns\n        --------\n        (training_data_loader, validation_data_loader): Tuple[DataLoader, Optional[DataLoader]]\n            Training and validation data loaders in PyTorch\n\n        \"\"\"", "\n", "if", "self", ".", "batch_size", "==", "\"auto\"", ":", "\n", "            ", "batch_size_", "=", "min", "(", "200", ",", "context", ".", "shape", "[", "0", "]", ")", "\n", "", "elif", "isinstance", "(", "self", ".", "batch_size", ",", "int", ")", "and", "self", ".", "batch_size", ">", "0", ":", "\n", "            ", "batch_size_", "=", "self", ".", "batch_size", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"`batch_size` must be a positive integer or 'auto'\"", ")", "\n", "\n", "", "dataset", "=", "NNPolicyDatasetForContinuousAction", "(", "\n", "torch", ".", "from_numpy", "(", "context", ")", ".", "float", "(", ")", ",", "\n", "torch", ".", "from_numpy", "(", "action", ")", ".", "float", "(", ")", ",", "\n", "torch", ".", "from_numpy", "(", "reward", ")", ".", "float", "(", ")", ",", "\n", "torch", ".", "from_numpy", "(", "pscore", ")", ".", "float", "(", ")", ",", "\n", ")", "\n", "\n", "if", "self", ".", "early_stopping", ":", "\n", "            ", "if", "context", ".", "shape", "[", "0", "]", "<=", "1", ":", "\n", "                ", "raise", "ValueError", "(", "\n", "f\"the number of samples is too small ({context.shape[0]}) to create validation data\"", "\n", ")", "\n", "\n", "", "validation_size", "=", "max", "(", "int", "(", "context", ".", "shape", "[", "0", "]", "*", "self", ".", "validation_fraction", ")", ",", "1", ")", "\n", "training_size", "=", "context", ".", "shape", "[", "0", "]", "-", "validation_size", "\n", "training_dataset", ",", "validation_dataset", "=", "torch", ".", "utils", ".", "data", ".", "random_split", "(", "\n", "dataset", ",", "[", "training_size", ",", "validation_size", "]", "\n", ")", "\n", "training_data_loader", "=", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "\n", "training_dataset", ",", "\n", "batch_size", "=", "batch_size_", ",", "\n", "shuffle", "=", "self", ".", "shuffle", ",", "\n", ")", "\n", "validation_data_loader", "=", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "\n", "validation_dataset", ",", "\n", "batch_size", "=", "batch_size_", ",", "\n", "shuffle", "=", "self", ".", "shuffle", ",", "\n", ")", "\n", "\n", "return", "training_data_loader", ",", "validation_data_loader", "\n", "\n", "", "data_loader", "=", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "\n", "dataset", ",", "\n", "batch_size", "=", "batch_size_", ",", "\n", "shuffle", "=", "self", ".", "shuffle", ",", "\n", ")", "\n", "\n", "return", "data_loader", ",", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.offline_continuous.ContinuousNNPolicyLearner.fit": [[381, 521], ["utils.check_continuous_bandit_feedback_inputs", "offline_continuous.ContinuousNNPolicyLearner._create_train_data_for_opl", "list", "tqdm.tqdm.tqdm", "ValueError", "numpy.ones_like", "offline_continuous.ContinuousNNPolicyLearner.q_func_estimator.fit", "torch.SGD", "torch.SGD", "torch.SGD", "numpy.arange", "offline_continuous.ContinuousNNPolicyLearner.nn_model.train", "offline_continuous.ContinuousNNPolicyLearner.nn_model.parameters", "torch.Adagrad", "torch.Adagrad", "torch.Adagrad", "torch.Adam.zero_grad", "offline_continuous.ContinuousNNPolicyLearner.nn_model().flatten", "loss.backward", "torch.Adam.step", "loss.item", "offline_continuous.ContinuousNNPolicyLearner.nn_model.eval", "offline_continuous.ContinuousNNPolicyLearner.nn_model.parameters", "torch.Adam", "torch.Adam", "torch.Adam", "NotImplementedError", "offline_continuous.ContinuousNNPolicyLearner._estimate_policy_gradient().mean", "offline_continuous.ContinuousNNPolicyLearner.nn_model().flatten", "loss.item", "offline_continuous.ContinuousNNPolicyLearner.val_loss_curve.append", "offline_continuous.ContinuousNNPolicyLearner.nn_model.parameters", "offline_continuous.ContinuousNNPolicyLearner.nn_model", "offline_continuous.ContinuousNNPolicyLearner._estimate_policy_gradient().mean", "offline_continuous.ContinuousNNPolicyLearner._estimate_policy_gradient", "offline_continuous.ContinuousNNPolicyLearner.nn_model", "offline_continuous.ContinuousNNPolicyLearner._estimate_policy_gradient"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_continuous_bandit_feedback_inputs", "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.offline.NNPolicyLearner._create_train_data_for_opl", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.classification_model.PropensityScoreEstimator.fit", "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.offline.NNPolicyLearner._estimate_policy_gradient", "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.offline.NNPolicyLearner._estimate_policy_gradient"], ["", "def", "fit", "(", "\n", "self", ",", "\n", "context", ":", "np", ".", "ndarray", ",", "\n", "action", ":", "np", ".", "ndarray", ",", "\n", "reward", ":", "np", ".", "ndarray", ",", "\n", "pscore", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", ")", "->", "None", ":", "\n", "        ", "\"\"\"Fits an offline bandit policy on the given logged bandit data.\n\n        Note\n        ----------\n        Given the training data :math:`\\\\mathcal{D}`, this policy maximizes the following objective function:\n\n        .. math::\n            \\\\hat{V}(\\\\pi_\\\\theta; \\\\mathcal{D}) - \\\\alpha \\\\Omega(\\\\theta)\n\n        where :math:`\\\\hat{V}` is an OPE estimator and :math:`\\\\alpha \\\\Omega(\\\\theta)` is a regularization term.\n\n        Parameters\n        -----------\n        context: array-like, shape (n_rounds, dim_context)\n            Context vectors observed for each data, i.e., :math:`x_i`.\n\n        action: array-like, shape (n_rounds,)\n            Continuous action values sampled by the logging/behavior policy for each data in logged bandit data, i.e., :math:`a_i`.\n\n        reward: array-like, shape (n_rounds,)\n            Rewards observed for each data in logged bandit data, i.e., :math:`r_i`.\n\n        pscore: array-like, shape (n_rounds,), default=None\n            Action choice probabilities by a behavior policy (generalized propensity scores), i.e., :math:`\\\\pi_b(a_i|x_i)`.\n\n        \"\"\"", "\n", "check_continuous_bandit_feedback_inputs", "(", "\n", "context", "=", "context", ",", "\n", "action_by_behavior_policy", "=", "action", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore", "=", "pscore", ",", "\n", ")", "\n", "\n", "if", "context", ".", "shape", "[", "1", "]", "!=", "self", ".", "dim_context", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"Expected `context.shape[1] == self.dim_context`, but found it False\"", "\n", ")", "\n", "\n", "", "if", "pscore", "is", "None", ":", "\n", "            ", "pscore", "=", "np", ".", "ones_like", "(", "action", ")", "\n", "\n", "# train q function estimator when it is needed to train NNPolicy", "\n", "", "if", "self", ".", "pg_method", "!=", "\"ipw\"", ":", "\n", "            ", "self", ".", "q_func_estimator", ".", "fit", "(", "\n", "context", "=", "context", ",", "\n", "action", "=", "action", ",", "\n", "reward", "=", "reward", ",", "\n", ")", "\n", "", "if", "self", ".", "solver", "==", "\"sgd\"", ":", "\n", "            ", "optimizer", "=", "optim", ".", "SGD", "(", "\n", "self", ".", "nn_model", ".", "parameters", "(", ")", ",", "\n", "lr", "=", "self", ".", "learning_rate_init", ",", "\n", "momentum", "=", "self", ".", "momentum", ",", "\n", "weight_decay", "=", "self", ".", "alpha", ",", "\n", "nesterov", "=", "self", ".", "nesterovs_momentum", ",", "\n", ")", "\n", "", "elif", "self", ".", "solver", "==", "\"adagrad\"", ":", "\n", "            ", "optimizer", "=", "optim", ".", "Adagrad", "(", "\n", "self", ".", "nn_model", ".", "parameters", "(", ")", ",", "\n", "lr", "=", "self", ".", "learning_rate_init", ",", "\n", "eps", "=", "self", ".", "epsilon", ",", "\n", "weight_decay", "=", "self", ".", "alpha", ",", "\n", ")", "\n", "", "elif", "self", ".", "solver", "==", "\"adam\"", ":", "\n", "            ", "optimizer", "=", "optim", ".", "Adam", "(", "\n", "self", ".", "nn_model", ".", "parameters", "(", ")", ",", "\n", "lr", "=", "self", ".", "learning_rate_init", ",", "\n", "betas", "=", "(", "self", ".", "beta_1", ",", "self", ".", "beta_2", ")", ",", "\n", "eps", "=", "self", ".", "epsilon", ",", "\n", "weight_decay", "=", "self", ".", "alpha", ",", "\n", ")", "\n", "", "else", ":", "\n", "            ", "raise", "NotImplementedError", "(", "\n", "\"`solver` must be one of 'adam', 'adagrad', or 'sgd'\"", "\n", ")", "\n", "\n", "", "training_data_loader", ",", "validation_data_loader", "=", "self", ".", "_create_train_data_for_opl", "(", "\n", "context", ",", "\n", "action", ",", "\n", "reward", ",", "\n", "pscore", ",", "\n", ")", "\n", "\n", "n_not_improving_training", "=", "0", "\n", "previous_training_loss", "=", "None", "\n", "n_not_improving_validation", "=", "0", "\n", "previous_validation_loss", "=", "None", "\n", "self", ".", "val_loss_curve", "=", "list", "(", ")", "\n", "for", "_", "in", "tqdm", "(", "np", ".", "arange", "(", "self", ".", "max_iter", ")", ",", "desc", "=", "\"policy learning\"", ")", ":", "\n", "            ", "self", ".", "nn_model", ".", "train", "(", ")", "\n", "for", "x", ",", "a", ",", "r", ",", "p", "in", "training_data_loader", ":", "\n", "                ", "optimizer", ".", "zero_grad", "(", ")", "\n", "action_by_current_policy", "=", "self", ".", "nn_model", "(", "x", ")", ".", "flatten", "(", ")", "\n", "loss", "=", "-", "self", ".", "_estimate_policy_gradient", "(", "\n", "context", "=", "x", ",", "\n", "reward", "=", "r", ",", "\n", "action", "=", "a", ",", "\n", "pscore", "=", "p", ",", "\n", "action_by_current_policy", "=", "action_by_current_policy", ",", "\n", ")", ".", "mean", "(", ")", "\n", "loss", ".", "backward", "(", ")", "\n", "optimizer", ".", "step", "(", ")", "\n", "loss_value", "=", "loss", ".", "item", "(", ")", "\n", "if", "previous_training_loss", "is", "not", "None", ":", "\n", "                    ", "if", "loss_value", "-", "previous_training_loss", "<", "self", ".", "tol", ":", "\n", "                        ", "n_not_improving_training", "+=", "1", "\n", "", "else", ":", "\n", "                        ", "n_not_improving_training", "=", "0", "\n", "", "", "if", "n_not_improving_training", ">=", "self", ".", "n_iter_no_change", ":", "\n", "                    ", "break", "\n", "", "previous_training_loss", "=", "loss_value", "\n", "\n", "", "if", "self", ".", "early_stopping", ":", "\n", "                ", "self", ".", "nn_model", ".", "eval", "(", ")", "\n", "for", "x", ",", "a", ",", "r", ",", "p", "in", "validation_data_loader", ":", "\n", "                    ", "action_by_current_policy", "=", "self", ".", "nn_model", "(", "x", ")", ".", "flatten", "(", ")", "\n", "loss", "=", "-", "self", ".", "_estimate_policy_gradient", "(", "\n", "context", "=", "x", ",", "\n", "reward", "=", "r", ",", "\n", "action", "=", "a", ",", "\n", "pscore", "=", "p", ",", "\n", "action_by_current_policy", "=", "action_by_current_policy", ",", "\n", ")", ".", "mean", "(", ")", "\n", "loss_value", "=", "loss", ".", "item", "(", ")", "\n", "self", ".", "val_loss_curve", ".", "append", "(", "-", "loss_value", ")", "\n", "if", "previous_validation_loss", "is", "not", "None", ":", "\n", "                        ", "if", "loss_value", "-", "previous_validation_loss", "<", "self", ".", "tol", ":", "\n", "                            ", "n_not_improving_validation", "+=", "1", "\n", "", "else", ":", "\n", "                            ", "n_not_improving_validation", "=", "0", "\n", "", "", "if", "n_not_improving_validation", ">", "self", ".", "n_iter_no_change", ":", "\n", "                        ", "break", "\n", "", "previous_validation_loss", "=", "loss_value", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.offline_continuous.ContinuousNNPolicyLearner._estimate_policy_gradient": [[522, 590], ["torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "offline_continuous.ContinuousNNPolicyLearner.q_func_estimator.predict", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "offline_continuous.ContinuousNNPolicyLearner.q_func_estimator.predict", "offline_continuous.ContinuousNNPolicyLearner._estimate_policy_gradient.gaussian_kernel"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.classification_model.PropensityScoreEstimator.predict", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.classification_model.PropensityScoreEstimator.predict", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_continuous.gaussian_kernel"], ["", "", "", "", "def", "_estimate_policy_gradient", "(", "\n", "self", ",", "\n", "context", ":", "torch", ".", "Tensor", ",", "\n", "action", ":", "torch", ".", "Tensor", ",", "\n", "reward", ":", "torch", ".", "Tensor", ",", "\n", "pscore", ":", "torch", ".", "Tensor", ",", "\n", "action_by_current_policy", ":", "torch", ".", "Tensor", ",", "\n", ")", "->", "float", ":", "\n", "        ", "\"\"\"Estimate the policy gradient.\n\n        Parameters\n        -----------\n        context: Tensor, shape (batch_size, dim_context)\n            Context vectors observed for each data, i.e., :math:`x_i`.\n\n        action: Tensor, shape (batch_size,)\n            Continuous action values sampled by the logging/behavior policy for each data in logged bandit data, i.e., :math:`a_i`.\n\n        reward: Tensor, shape (batch_size,)\n            Rewards observed for each data in logged bandit data, i.e., :math:`r_i`.\n\n        pscore: Tensor, shape (batch_size,)\n            Action choice probabilities of the logging/behavior policy (generalized propensity scores), i.e., :math:`\\\\pi_b(a_i|x_i)`.\n\n        action_by_current_policy: Tensor, shape (batch_size,)\n            Continuous action values given by the current policy.\n\n        Returns\n        ----------\n        estimated_policy_grad_arr: array-like, shape (batch_size,)\n            Rewards of each data estimated by an OPE estimator.\n\n        \"\"\"", "\n", "\n", "def", "gaussian_kernel", "(", "u", ":", "torch", ".", "Tensor", ")", "->", "torch", ".", "Tensor", ":", "\n", "            ", "return", "torch", ".", "exp", "(", "-", "(", "u", "**", "2", ")", "/", "2", ")", "/", "(", "(", "2", "*", "np", ".", "pi", ")", "**", "0.5", ")", "\n", "\n", "", "if", "self", ".", "output_space", "is", "not", "None", ":", "\n", "            ", "action_by_current_policy", "=", "torch", ".", "clamp", "(", "\n", "action_by_current_policy", ",", "\n", "min", "=", "self", ".", "output_space", "[", "0", "]", ",", "\n", "max", "=", "self", ".", "output_space", "[", "1", "]", ",", "\n", ")", "\n", "\n", "", "if", "self", ".", "pg_method", "==", "\"dpg\"", ":", "\n", "            ", "estimated_policy_grad_arr", "=", "self", ".", "q_func_estimator", ".", "predict", "(", "\n", "context", "=", "context", ",", "\n", "action", "=", "action_by_current_policy", ",", "\n", ")", "\n", "\n", "", "elif", "self", ".", "pg_method", "==", "\"ipw\"", ":", "\n", "            ", "u", "=", "action_by_current_policy", "-", "action", "\n", "u", "/=", "self", ".", "bandwidth", "\n", "estimated_policy_grad_arr", "=", "gaussian_kernel", "(", "u", ")", "*", "reward", "/", "pscore", "\n", "estimated_policy_grad_arr", "/=", "self", ".", "bandwidth", "\n", "\n", "", "elif", "self", ".", "pg_method", "==", "\"dr\"", ":", "\n", "            ", "u", "=", "action_by_current_policy", "-", "action", "\n", "u", "/=", "self", ".", "bandwidth", "\n", "q_hat", "=", "self", ".", "q_func_estimator", ".", "predict", "(", "\n", "context", "=", "context", ",", "\n", "action", "=", "action_by_current_policy", ",", "\n", ")", "\n", "estimated_policy_grad_arr", "=", "gaussian_kernel", "(", "u", ")", "*", "(", "reward", "-", "q_hat", ")", "/", "pscore", "\n", "estimated_policy_grad_arr", "/=", "self", ".", "bandwidth", "\n", "estimated_policy_grad_arr", "+=", "q_hat", "\n", "\n", "", "return", "estimated_policy_grad_arr", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.offline_continuous.ContinuousNNPolicyLearner.predict": [[591, 622], ["utils.check_array", "offline_continuous.ContinuousNNPolicyLearner.nn_model.eval", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "offline_continuous.ContinuousNNPolicyLearner.nn_model().detach().numpy().flatten", "ValueError", "numpy.clip", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "offline_continuous.ContinuousNNPolicyLearner.nn_model().detach().numpy", "offline_continuous.ContinuousNNPolicyLearner.nn_model().detach", "offline_continuous.ContinuousNNPolicyLearner.nn_model"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array"], ["", "def", "predict", "(", "self", ",", "context", ":", "np", ".", "ndarray", ")", "->", "np", ".", "ndarray", ":", "\n", "        ", "\"\"\"Predict best continuous actions for new data.\n\n        Parameters\n        -----------\n        context: array-like, shape (n_rounds_of_new_data, dim_context)\n            Context vectors for new data.\n\n        Returns\n        -----------\n        predicted_actions: array-like, shape (n_rounds_of_new_data,)\n            Continuous action values given by a neural network policy.\n\n        \"\"\"", "\n", "check_array", "(", "array", "=", "context", ",", "name", "=", "\"context\"", ",", "expected_dim", "=", "2", ")", "\n", "if", "context", ".", "shape", "[", "1", "]", "!=", "self", ".", "dim_context", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"Expected `context.shape[1] == self.dim_context`, but found it False\"", "\n", ")", "\n", "\n", "", "self", ".", "nn_model", ".", "eval", "(", ")", "\n", "x", "=", "torch", ".", "from_numpy", "(", "context", ")", ".", "float", "(", ")", "\n", "predicted_actions", "=", "self", ".", "nn_model", "(", "x", ")", ".", "detach", "(", ")", ".", "numpy", "(", ")", ".", "flatten", "(", ")", "\n", "if", "self", ".", "output_space", "is", "not", "None", ":", "\n", "            ", "predicted_actions", "=", "np", ".", "clip", "(", "\n", "predicted_actions", ",", "\n", "a_min", "=", "self", ".", "output_space", "[", "0", "]", ",", "\n", "a_max", "=", "self", ".", "output_space", "[", "1", "]", ",", "\n", ")", "\n", "\n", "", "return", "predicted_actions", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.offline_continuous.QFuncEstimatorForContinuousAction.__post_init__": [[749, 844], ["sklearn.utils.check_scalar", "sklearn.utils.check_scalar", "sklearn.utils.check_scalar", "sklearn.utils.check_scalar", "sklearn.utils.check_scalar", "sklearn.utils.check_scalar", "sklearn.utils.check_scalar", "sklearn.utils.check_scalar", "sklearn.utils.check_scalar", "sklearn.utils.check_scalar", "sklearn.utils.check_scalar", "enumerate", "layer_list.append", "torch.Sequential", "torch.Sequential", "torch.Sequential", "any", "ValueError", "ValueError", "ValueError", "ValueError", "isinstance", "ValueError", "ValueError", "isinstance", "ValueError", "isinstance", "ValueError", "ValueError", "sklearn.utils.check_random_state", "torch.manual_seed", "torch.manual_seed", "torch.manual_seed", "torch.manual_seed", "torch.manual_seed", "torch.manual_seed", "torch.manual_seed", "torch.manual_seed", "torch.manual_seed", "layer_list.append", "layer_list.append", "collections.OrderedDict", "isinstance", "torch.Linear", "torch.Linear", "torch.Linear", "isinstance", "torch.Linear", "torch.Linear", "torch.Linear", "activation_layer", "isinstance", "ValueError"], "methods", ["None"], ["def", "__post_init__", "(", "self", ")", "->", "None", ":", "\n", "        ", "\"\"\"Initialize class.\"\"\"", "\n", "check_scalar", "(", "self", ".", "dim_context", ",", "\"dim_context\"", ",", "int", ",", "min_val", "=", "1", ")", "\n", "\n", "if", "not", "isinstance", "(", "self", ".", "hidden_layer_size", ",", "tuple", ")", "or", "any", "(", "\n", "[", "not", "isinstance", "(", "h", ",", "int", ")", "or", "h", "<=", "0", "for", "h", "in", "self", ".", "hidden_layer_size", "]", "\n", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "f\"`hidden_layer_size` must be a tuple of positive integers, but {self.hidden_layer_size} is given\"", "\n", ")", "\n", "\n", "", "if", "self", ".", "solver", "not", "in", "(", "\"adagrad\"", ",", "\"sgd\"", ",", "\"adam\"", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "f\"`solver` must be one of 'adam', 'adagrad', or 'sgd', but {self.solver} is given\"", "\n", ")", "\n", "\n", "", "check_scalar", "(", "self", ".", "alpha", ",", "\"alpha\"", ",", "float", ",", "min_val", "=", "0.0", ")", "\n", "\n", "if", "self", ".", "batch_size", "!=", "\"auto\"", "and", "(", "\n", "not", "isinstance", "(", "self", ".", "batch_size", ",", "int", ")", "or", "self", ".", "batch_size", "<=", "0", "\n", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "f\"`batch_size` must be a positive integer or 'auto', but {self.batch_size} is given\"", "\n", ")", "\n", "\n", "", "check_scalar", "(", "self", ".", "learning_rate_init", ",", "\"learning_rate_init\"", ",", "float", ")", "\n", "if", "self", ".", "learning_rate_init", "<=", "0.0", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "f\"`learning_rate_init`= {self.learning_rate_init}, must be > 0.0\"", "\n", ")", "\n", "\n", "", "check_scalar", "(", "self", ".", "max_iter", ",", "\"max_iter\"", ",", "int", ",", "min_val", "=", "1", ")", "\n", "\n", "if", "not", "isinstance", "(", "self", ".", "shuffle", ",", "bool", ")", ":", "\n", "            ", "raise", "ValueError", "(", "f\"`shuffle` must be a bool, but {self.shuffle} is given\"", ")", "\n", "\n", "", "check_scalar", "(", "self", ".", "tol", ",", "\"tol\"", ",", "float", ")", "\n", "if", "self", ".", "tol", "<=", "0.0", ":", "\n", "            ", "raise", "ValueError", "(", "f\"`tol`= {self.tol}, must be > 0.0\"", ")", "\n", "\n", "", "check_scalar", "(", "self", ".", "momentum", ",", "\"momentum\"", ",", "float", ",", "min_val", "=", "0.0", ",", "max_val", "=", "1.0", ")", "\n", "\n", "if", "not", "isinstance", "(", "self", ".", "nesterovs_momentum", ",", "bool", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "f\"`nesterovs_momentum` must be a bool, but {self.nesterovs_momentum} is given\"", "\n", ")", "\n", "\n", "", "if", "not", "isinstance", "(", "self", ".", "early_stopping", ",", "bool", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "f\"`early_stopping` must be a bool, but {self.early_stopping} is given\"", "\n", ")", "\n", "\n", "", "check_scalar", "(", "\n", "self", ".", "validation_fraction", ",", "\"validation_fraction\"", ",", "float", ",", "max_val", "=", "1.0", "\n", ")", "\n", "if", "self", ".", "validation_fraction", "<=", "0.0", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "f\"`validation_fraction`= {self.validation_fraction}, must be > 0.0\"", "\n", ")", "\n", "\n", "", "check_scalar", "(", "self", ".", "beta_1", ",", "\"beta_1\"", ",", "float", ",", "min_val", "=", "0.0", ",", "max_val", "=", "1.0", ")", "\n", "check_scalar", "(", "self", ".", "beta_2", ",", "\"beta_2\"", ",", "float", ",", "min_val", "=", "0.0", ",", "max_val", "=", "1.0", ")", "\n", "check_scalar", "(", "self", ".", "epsilon", ",", "\"epsilon\"", ",", "float", ",", "min_val", "=", "0.0", ")", "\n", "check_scalar", "(", "self", ".", "n_iter_no_change", ",", "\"n_iter_no_change\"", ",", "int", ",", "min_val", "=", "1", ")", "\n", "\n", "if", "self", ".", "random_state", "is", "not", "None", ":", "\n", "            ", "self", ".", "random_", "=", "check_random_state", "(", "self", ".", "random_state", ")", "\n", "torch", ".", "manual_seed", "(", "self", ".", "random_state", ")", "\n", "\n", "", "if", "self", ".", "activation", "==", "\"identity\"", ":", "\n", "            ", "activation_layer", "=", "nn", ".", "Identity", "\n", "", "elif", "self", ".", "activation", "==", "\"logistic\"", ":", "\n", "            ", "activation_layer", "=", "nn", ".", "Sigmoid", "\n", "", "elif", "self", ".", "activation", "==", "\"tanh\"", ":", "\n", "            ", "activation_layer", "=", "nn", ".", "Tanh", "\n", "", "elif", "self", ".", "activation", "==", "\"relu\"", ":", "\n", "            ", "activation_layer", "=", "nn", ".", "ReLU", "\n", "", "elif", "self", ".", "activation", "==", "\"elu\"", ":", "\n", "            ", "activation_layer", "=", "nn", ".", "ELU", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"`activation` must be one of 'identity', 'logistic', 'tanh', 'relu', or 'elu'\"", "\n", "f\", but {self.activation} is given\"", "\n", ")", "\n", "\n", "", "layer_list", "=", "[", "]", "\n", "input_size", "=", "self", ".", "dim_context", "+", "1", "\n", "\n", "for", "i", ",", "h", "in", "enumerate", "(", "self", ".", "hidden_layer_size", ")", ":", "\n", "            ", "layer_list", ".", "append", "(", "(", "\"l{}\"", ".", "format", "(", "i", ")", ",", "nn", ".", "Linear", "(", "input_size", ",", "h", ")", ")", ")", "\n", "layer_list", ".", "append", "(", "(", "\"a{}\"", ".", "format", "(", "i", ")", ",", "activation_layer", "(", ")", ")", ")", "\n", "input_size", "=", "h", "\n", "", "layer_list", ".", "append", "(", "(", "\"output\"", ",", "nn", ".", "Linear", "(", "input_size", ",", "1", ")", ")", ")", "\n", "\n", "self", ".", "nn_model", "=", "nn", ".", "Sequential", "(", "OrderedDict", "(", "layer_list", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.offline_continuous.QFuncEstimatorForContinuousAction._create_train_data_for_q_func_estimation": [[845, 915], ["offline_continuous.QFuncEstimatorDatasetForContinuousAction", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "min", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "max", "torch.utils.data.random_split", "torch.utils.data.random_split", "torch.utils.data.random_split", "torch.utils.data.random_split", "torch.utils.data.random_split", "torch.utils.data.random_split", "torch.utils.data.random_split", "torch.utils.data.random_split", "torch.utils.data.random_split", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "isinstance", "ValueError", "ValueError", "int", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy"], "methods", ["None"], ["", "def", "_create_train_data_for_q_func_estimation", "(", "\n", "self", ",", "\n", "context", ":", "np", ".", "ndarray", ",", "\n", "action", ":", "np", ".", "ndarray", ",", "\n", "reward", ":", "np", ".", "ndarray", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "Tuple", "[", "torch", ".", "utils", ".", "data", ".", "DataLoader", ",", "Optional", "[", "torch", ".", "utils", ".", "data", ".", "DataLoader", "]", "]", ":", "\n", "        ", "\"\"\"Create training data for off-policy learning.\n\n        Parameters\n        -----------\n        context: array-like, shape (n_rounds, dim_context)\n            Context vectors observed for each data, i.e., :math:`x_i`.\n\n        action: array-like, shape (n_rounds,)\n            Continuous action values sampled by the logging/behavior policy for each data in logged bandit data, i.e., :math:`a_i`.\n\n        reward: array-like, shape (n_rounds,)\n            Rewards observed for each data in logged bandit data, i.e., :math:`r_i`.\n\n        Returns\n        --------\n        (training_data_loader, validation_data_loader): Tuple[DataLoader, Optional[DataLoader]]\n            Training and validation data loaders in PyTorch\n\n        \"\"\"", "\n", "if", "self", ".", "batch_size", "==", "\"auto\"", ":", "\n", "            ", "batch_size_", "=", "min", "(", "200", ",", "context", ".", "shape", "[", "0", "]", ")", "\n", "", "elif", "isinstance", "(", "self", ".", "batch_size", ",", "int", ")", "and", "self", ".", "batch_size", ">", "0", ":", "\n", "            ", "batch_size_", "=", "self", ".", "batch_size", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"`batch_size` must be a positive integer or 'auto'\"", ")", "\n", "\n", "", "feature", "=", "np", ".", "c_", "[", "context", ",", "action", "[", ":", ",", "np", ".", "newaxis", "]", "]", "\n", "dataset", "=", "QFuncEstimatorDatasetForContinuousAction", "(", "\n", "torch", ".", "from_numpy", "(", "feature", ")", ".", "float", "(", ")", ",", "\n", "torch", ".", "from_numpy", "(", "reward", ")", ".", "float", "(", ")", ",", "\n", ")", "\n", "\n", "if", "self", ".", "early_stopping", ":", "\n", "            ", "if", "context", ".", "shape", "[", "0", "]", "<=", "1", ":", "\n", "                ", "raise", "ValueError", "(", "\n", "f\"the number of samples is too small ({context.shape[0]}) to create validation data\"", "\n", ")", "\n", "\n", "", "validation_size", "=", "max", "(", "int", "(", "context", ".", "shape", "[", "0", "]", "*", "self", ".", "validation_fraction", ")", ",", "1", ")", "\n", "training_size", "=", "context", ".", "shape", "[", "0", "]", "-", "validation_size", "\n", "training_dataset", ",", "validation_dataset", "=", "torch", ".", "utils", ".", "data", ".", "random_split", "(", "\n", "dataset", ",", "[", "training_size", ",", "validation_size", "]", "\n", ")", "\n", "training_data_loader", "=", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "\n", "training_dataset", ",", "\n", "batch_size", "=", "batch_size_", ",", "\n", "shuffle", "=", "self", ".", "shuffle", ",", "\n", ")", "\n", "validation_data_loader", "=", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "\n", "validation_dataset", ",", "\n", "batch_size", "=", "batch_size_", ",", "\n", "shuffle", "=", "self", ".", "shuffle", ",", "\n", ")", "\n", "\n", "return", "training_data_loader", ",", "validation_data_loader", "\n", "\n", "", "data_loader", "=", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "\n", "dataset", ",", "\n", "batch_size", "=", "batch_size_", ",", "\n", "shuffle", "=", "self", ".", "shuffle", ",", "\n", ")", "\n", "\n", "return", "data_loader", ",", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.offline_continuous.QFuncEstimatorForContinuousAction.fit": [[916, 1021], ["utils.check_continuous_bandit_feedback_inputs", "offline_continuous.QFuncEstimatorForContinuousAction._create_train_data_for_q_func_estimation", "tqdm.tqdm.tqdm", "ValueError", "torch.SGD", "torch.SGD", "torch.SGD", "numpy.arange", "offline_continuous.QFuncEstimatorForContinuousAction.nn_model.train", "offline_continuous.QFuncEstimatorForContinuousAction.nn_model.parameters", "torch.Adagrad", "torch.Adagrad", "torch.Adagrad", "torch.Adam.zero_grad", "offline_continuous.QFuncEstimatorForContinuousAction.nn_model().flatten", "torch.functional.mse_loss", "torch.functional.mse_loss", "torch.functional.mse_loss", "torch.functional.mse_loss.backward", "torch.Adam.step", "torch.functional.mse_loss.item", "offline_continuous.QFuncEstimatorForContinuousAction.nn_model.eval", "offline_continuous.QFuncEstimatorForContinuousAction.nn_model.parameters", "torch.Adam", "torch.Adam", "torch.Adam", "NotImplementedError", "offline_continuous.QFuncEstimatorForContinuousAction.nn_model().flatten", "torch.functional.mse_loss", "torch.functional.mse_loss", "torch.functional.mse_loss", "torch.functional.mse_loss.item", "offline_continuous.QFuncEstimatorForContinuousAction.nn_model.parameters", "offline_continuous.QFuncEstimatorForContinuousAction.nn_model", "offline_continuous.QFuncEstimatorForContinuousAction.nn_model"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_continuous_bandit_feedback_inputs", "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.offline.QFuncEstimator._create_train_data_for_q_func_estimation"], ["", "def", "fit", "(", "\n", "self", ",", "\n", "context", ":", "np", ".", "ndarray", ",", "\n", "action", ":", "np", ".", "ndarray", ",", "\n", "reward", ":", "np", ".", "ndarray", ",", "\n", ")", "->", "None", ":", "\n", "        ", "\"\"\"Fits an offline bandit policy on the given logged bandit data.\n\n        Parameters\n        -----------\n        context: array-like, shape (n_rounds, dim_context)\n            Context vectors observed for each data, i.e., :math:`x_i`.\n\n        action: array-like, shape (n_rounds,)\n            Continuous action values sampled by the logging/behavior policy for each data in logged bandit data, i.e., :math:`a_i`.\n\n        reward: array-like, shape (n_rounds,)\n            Rewards observed for each data in logged bandit data, i.e., :math:`r_i`.\n\n        \"\"\"", "\n", "check_continuous_bandit_feedback_inputs", "(", "\n", "context", "=", "context", ",", "\n", "action_by_behavior_policy", "=", "action", ",", "\n", "reward", "=", "reward", ",", "\n", ")", "\n", "\n", "if", "context", ".", "shape", "[", "1", "]", "!=", "self", ".", "dim_context", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"Expected `context.shape[1] == self.dim_context`, but found it False\"", "\n", ")", "\n", "\n", "", "if", "self", ".", "solver", "==", "\"sgd\"", ":", "\n", "            ", "optimizer", "=", "optim", ".", "SGD", "(", "\n", "self", ".", "nn_model", ".", "parameters", "(", ")", ",", "\n", "lr", "=", "self", ".", "learning_rate_init", ",", "\n", "momentum", "=", "self", ".", "momentum", ",", "\n", "weight_decay", "=", "self", ".", "alpha", ",", "\n", "nesterov", "=", "self", ".", "nesterovs_momentum", ",", "\n", ")", "\n", "", "elif", "self", ".", "solver", "==", "\"adagrad\"", ":", "\n", "            ", "optimizer", "=", "optim", ".", "Adagrad", "(", "\n", "self", ".", "nn_model", ".", "parameters", "(", ")", ",", "\n", "lr", "=", "self", ".", "learning_rate_init", ",", "\n", "eps", "=", "self", ".", "epsilon", ",", "\n", "weight_decay", "=", "self", ".", "alpha", ",", "\n", ")", "\n", "", "elif", "self", ".", "solver", "==", "\"adam\"", ":", "\n", "            ", "optimizer", "=", "optim", ".", "Adam", "(", "\n", "self", ".", "nn_model", ".", "parameters", "(", ")", ",", "\n", "lr", "=", "self", ".", "learning_rate_init", ",", "\n", "betas", "=", "(", "self", ".", "beta_1", ",", "self", ".", "beta_2", ")", ",", "\n", "eps", "=", "self", ".", "epsilon", ",", "\n", "weight_decay", "=", "self", ".", "alpha", ",", "\n", ")", "\n", "", "else", ":", "\n", "            ", "raise", "NotImplementedError", "(", "\n", "\"`solver` must be one of 'adam', 'adagrad', or 'sgd'\"", "\n", ")", "\n", "\n", "", "(", "\n", "training_data_loader", ",", "\n", "validation_data_loader", ",", "\n", ")", "=", "self", ".", "_create_train_data_for_q_func_estimation", "(", "\n", "context", ",", "\n", "action", ",", "\n", "reward", ",", "\n", ")", "\n", "\n", "n_not_improving_training", "=", "0", "\n", "previous_training_loss", "=", "None", "\n", "n_not_improving_validation", "=", "0", "\n", "previous_validation_loss", "=", "None", "\n", "for", "_", "in", "tqdm", "(", "np", ".", "arange", "(", "self", ".", "max_iter", ")", ",", "desc", "=", "\"q-func learning\"", ")", ":", "\n", "            ", "self", ".", "nn_model", ".", "train", "(", ")", "\n", "for", "x", ",", "r", "in", "training_data_loader", ":", "\n", "                ", "optimizer", ".", "zero_grad", "(", ")", "\n", "q_hat", "=", "self", ".", "nn_model", "(", "x", ")", ".", "flatten", "(", ")", "\n", "loss", "=", "nn", ".", "functional", ".", "mse_loss", "(", "r", ",", "q_hat", ")", "\n", "loss", ".", "backward", "(", ")", "\n", "optimizer", ".", "step", "(", ")", "\n", "\n", "loss_value", "=", "loss", ".", "item", "(", ")", "\n", "if", "previous_training_loss", "is", "not", "None", ":", "\n", "                    ", "if", "loss_value", "-", "previous_training_loss", "<", "self", ".", "tol", ":", "\n", "                        ", "n_not_improving_training", "+=", "1", "\n", "", "else", ":", "\n", "                        ", "n_not_improving_training", "=", "0", "\n", "", "", "if", "n_not_improving_training", ">=", "self", ".", "n_iter_no_change", ":", "\n", "                    ", "break", "\n", "", "previous_training_loss", "=", "loss_value", "\n", "\n", "", "if", "self", ".", "early_stopping", ":", "\n", "                ", "self", ".", "nn_model", ".", "eval", "(", ")", "\n", "for", "x", ",", "r", "in", "validation_data_loader", ":", "\n", "                    ", "q_hat", "=", "self", ".", "nn_model", "(", "x", ")", ".", "flatten", "(", ")", "\n", "loss", "=", "nn", ".", "functional", ".", "mse_loss", "(", "r", ",", "q_hat", ")", "\n", "loss_value", "=", "loss", ".", "item", "(", ")", "\n", "if", "previous_validation_loss", "is", "not", "None", ":", "\n", "                        ", "if", "loss_value", "-", "previous_validation_loss", "<", "self", ".", "tol", ":", "\n", "                            ", "n_not_improving_validation", "+=", "1", "\n", "", "else", ":", "\n", "                            ", "n_not_improving_validation", "=", "0", "\n", "", "", "if", "n_not_improving_validation", ">", "self", ".", "n_iter_no_change", ":", "\n", "                        ", "break", "\n", "", "previous_validation_loss", "=", "loss_value", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.offline_continuous.QFuncEstimatorForContinuousAction.predict": [[1022, 1055], ["utils.check_tensor", "utils.check_tensor", "offline_continuous.QFuncEstimatorForContinuousAction.nn_model.eval", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "offline_continuous.QFuncEstimatorForContinuousAction.nn_model().flatten", "ValueError", "action.unsqueeze", "offline_continuous.QFuncEstimatorForContinuousAction.nn_model"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_tensor", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_tensor"], ["", "", "", "", "def", "predict", "(", "\n", "self", ",", "\n", "context", ":", "torch", ".", "Tensor", ",", "\n", "action", ":", "torch", ".", "Tensor", ",", "\n", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "\"\"\"Predict best continuous actions for new data.\n\n        Parameters\n        -----------\n        context: Tensor, shape (n_rounds_of_new_data, dim_context)\n            Context vectors for new data.\n\n        action: Tensor, shape (n_rounds,)\n            Continuous action values for new data.\n\n        Returns\n        -----------\n        predicted_rewards: Tensor, shape (n_rounds_of_new_data,)\n            Expected rewards given context and action for new data estimated by the regression model.\n\n        \"\"\"", "\n", "check_tensor", "(", "tensor", "=", "context", ",", "name", "=", "\"context\"", ",", "expected_dim", "=", "2", ")", "\n", "check_tensor", "(", "tensor", "=", "action", ",", "name", "=", "\"action\"", ",", "expected_dim", "=", "1", ")", "\n", "if", "context", ".", "shape", "[", "1", "]", "!=", "self", ".", "dim_context", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"Expected `context.shape[1] == self.dim_context`, but found it False\"", "\n", ")", "\n", "\n", "", "self", ".", "nn_model", ".", "eval", "(", ")", "\n", "x", "=", "torch", ".", "cat", "(", "(", "context", ",", "action", ".", "unsqueeze", "(", "-", "1", ")", ")", ",", "1", ")", "\n", "predicted_rewards", "=", "self", ".", "nn_model", "(", "x", ")", ".", "flatten", "(", ")", "\n", "\n", "return", "predicted_rewards", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.offline_continuous.NNPolicyDatasetForContinuousAction.__post_init__": [[1066, 1073], ["None"], "methods", ["None"], ["def", "__post_init__", "(", "self", ")", ":", "\n", "        ", "\"\"\"initialize class\"\"\"", "\n", "assert", "(", "\n", "self", ".", "context", ".", "shape", "[", "0", "]", "\n", "==", "self", ".", "action", ".", "shape", "[", "0", "]", "\n", "==", "self", ".", "reward", ".", "shape", "[", "0", "]", "\n", "==", "self", ".", "pscore", ".", "shape", "[", "0", "]", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.offline_continuous.NNPolicyDatasetForContinuousAction.__getitem__": [[1075, 1081], ["None"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "index", ")", ":", "\n", "        ", "return", "(", "\n", "self", ".", "context", "[", "index", "]", ",", "\n", "self", ".", "action", "[", "index", "]", ",", "\n", "self", ".", "reward", "[", "index", "]", ",", "\n", "self", ".", "pscore", "[", "index", "]", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.offline_continuous.NNPolicyDatasetForContinuousAction.__len__": [[1083, 1085], ["None"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "context", ".", "shape", "[", "0", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.offline_continuous.QFuncEstimatorDatasetForContinuousAction.__post_init__": [[1094, 1097], ["None"], "methods", ["None"], ["def", "__post_init__", "(", "self", ")", ":", "\n", "        ", "\"\"\"initialize class\"\"\"", "\n", "assert", "self", ".", "feature", ".", "shape", "[", "0", "]", "==", "self", ".", "reward", ".", "shape", "[", "0", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.offline_continuous.QFuncEstimatorDatasetForContinuousAction.__getitem__": [[1098, 1102], ["None"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "index", ")", ":", "\n", "        ", "return", "(", "\n", "self", ".", "feature", "[", "index", "]", ",", "\n", "self", ".", "reward", "[", "index", "]", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.offline_continuous.QFuncEstimatorDatasetForContinuousAction.__len__": [[1104, 1106], ["None"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "feature", ".", "shape", "[", "0", "]", "\n", "", "", ""]], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.contextfree.EpsilonGreedy.__post_init__": [[53, 58], ["sklearn.utils.check_scalar", "super().__post_init__"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.__post_init__"], ["def", "__post_init__", "(", "self", ")", "->", "None", ":", "\n", "        ", "\"\"\"Initialize Class.\"\"\"", "\n", "check_scalar", "(", "self", ".", "epsilon", ",", "\"epsilon\"", ",", "float", ",", "min_val", "=", "0.0", ",", "max_val", "=", "1.0", ")", "\n", "self", ".", "policy_name", "=", "f\"egreedy_{self.epsilon}\"", "\n", "super", "(", ")", ".", "__post_init__", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.contextfree.EpsilonGreedy.select_action": [[59, 74], ["contextfree.EpsilonGreedy.random_.choice", "contextfree.EpsilonGreedy.random_.rand", "contextfree.EpsilonGreedy.action_counts.min", "predicted_rewards.argsort"], "methods", ["None"], ["", "def", "select_action", "(", "self", ")", "->", "np", ".", "ndarray", ":", "\n", "        ", "\"\"\"Select a list of actions.\n\n        Returns\n        ----------\n        selected_actions: array-like, shape (len_list, )\n            List of selected actions.\n\n        \"\"\"", "\n", "if", "(", "self", ".", "random_", ".", "rand", "(", ")", ">", "self", ".", "epsilon", ")", "and", "(", "self", ".", "action_counts", ".", "min", "(", ")", ">", "0", ")", ":", "\n", "            ", "predicted_rewards", "=", "self", ".", "reward_counts", "/", "self", ".", "action_counts", "\n", "return", "predicted_rewards", ".", "argsort", "(", ")", "[", ":", ":", "-", "1", "]", "[", ":", "self", ".", "len_list", "]", "\n", "", "else", ":", "\n", "            ", "return", "self", ".", "random_", ".", "choice", "(", "\n", "self", ".", "n_actions", ",", "size", "=", "self", ".", "len_list", ",", "replace", "=", "False", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.contextfree.EpsilonGreedy.update_params": [[76, 94], ["numpy.copy", "numpy.copy"], "methods", ["None"], ["", "", "def", "update_params", "(", "self", ",", "action", ":", "int", ",", "reward", ":", "float", ")", "->", "None", ":", "\n", "        ", "\"\"\"Update policy parameters.\n\n        Parameters\n        ----------\n        action: int\n            Selected action by the policy.\n\n        reward: float\n            Observed reward for the chosen action and position.\n\n        \"\"\"", "\n", "self", ".", "n_trial", "+=", "1", "\n", "self", ".", "action_counts_temp", "[", "action", "]", "+=", "1", "\n", "self", ".", "reward_counts_temp", "[", "action", "]", "+=", "reward", "\n", "if", "self", ".", "n_trial", "%", "self", ".", "batch_size", "==", "0", ":", "\n", "            ", "self", ".", "action_counts", "=", "np", ".", "copy", "(", "self", ".", "action_counts_temp", ")", "\n", "self", ".", "reward_counts", "=", "np", ".", "copy", "(", "self", ".", "reward_counts_temp", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.contextfree.Random.compute_batch_action_dist": [[125, 147], ["numpy.ones"], "methods", ["None"], ["def", "compute_batch_action_dist", "(", "\n", "self", ",", "\n", "n_rounds", ":", "int", "=", "1", ",", "\n", ")", "->", "np", ".", "ndarray", ":", "\n", "        ", "\"\"\"Compute the distribution over actions by Monte Carlo simulation.\n\n        Parameters\n        ----------\n        n_rounds: int, default=1\n            Number of rounds in the distribution over actions.\n            (the size of the first axis of `action_dist`)\n\n        Returns\n        ----------\n        action_dist: array-like, shape (n_rounds, n_actions, len_list)\n            Probability estimates of each arm being the best one for each sample, action, and position.\n\n        \"\"\"", "\n", "action_dist", "=", "np", ".", "ones", "(", "(", "n_rounds", ",", "self", ".", "n_actions", ",", "self", ".", "len_list", ")", ")", "*", "(", "\n", "1", "/", "self", ".", "n_actions", "\n", ")", "\n", "return", "action_dist", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.contextfree.BernoulliTS.__post_init__": [[192, 205], ["super().__post_init__", "Exception", "numpy.ones", "numpy.ones"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.__post_init__"], ["def", "__post_init__", "(", "self", ")", "->", "None", ":", "\n", "        ", "\"\"\"Initialize class.\"\"\"", "\n", "super", "(", ")", ".", "__post_init__", "(", ")", "\n", "if", "self", ".", "is_zozotown_prior", ":", "\n", "            ", "if", "self", ".", "campaign", "is", "None", ":", "\n", "                ", "raise", "Exception", "(", "\n", "\"`campaign` must be specified when `is_zozotown_prior` is True.\"", "\n", ")", "\n", "", "self", ".", "alpha", "=", "production_prior_for_bts", "[", "self", ".", "campaign", "]", "[", "\"alpha\"", "]", "\n", "self", ".", "beta", "=", "production_prior_for_bts", "[", "self", ".", "campaign", "]", "[", "\"beta\"", "]", "\n", "", "else", ":", "\n", "            ", "self", ".", "alpha", "=", "np", ".", "ones", "(", "self", ".", "n_actions", ")", "if", "self", ".", "alpha", "is", "None", "else", "self", ".", "alpha", "\n", "self", ".", "beta", "=", "np", ".", "ones", "(", "self", ".", "n_actions", ")", "if", "self", ".", "beta", "is", "None", "else", "self", ".", "beta", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.contextfree.BernoulliTS.select_action": [[206, 220], ["contextfree.BernoulliTS.random_.beta", "contextfree.BernoulliTS.argsort"], "methods", ["None"], ["", "", "def", "select_action", "(", "self", ")", "->", "np", ".", "ndarray", ":", "\n", "        ", "\"\"\"Select a list of actions.\n\n        Returns\n        ----------\n        selected_actions: array-like, shape (len_list, )\n            List of selected actions.\n\n        \"\"\"", "\n", "predicted_rewards", "=", "self", ".", "random_", ".", "beta", "(", "\n", "a", "=", "self", ".", "reward_counts", "+", "self", ".", "alpha", ",", "\n", "b", "=", "(", "self", ".", "action_counts", "-", "self", ".", "reward_counts", ")", "+", "self", ".", "beta", ",", "\n", ")", "\n", "return", "predicted_rewards", ".", "argsort", "(", ")", "[", ":", ":", "-", "1", "]", "[", ":", "self", ".", "len_list", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.contextfree.BernoulliTS.update_params": [[221, 239], ["numpy.copy", "numpy.copy"], "methods", ["None"], ["", "def", "update_params", "(", "self", ",", "action", ":", "int", ",", "reward", ":", "float", ")", "->", "None", ":", "\n", "        ", "\"\"\"Update policy parameters.\n\n        Parameters\n        ----------\n        action: int\n            Selected action by the policy.\n\n        reward: float\n            Observed reward for the chosen action and position.\n\n        \"\"\"", "\n", "self", ".", "n_trial", "+=", "1", "\n", "self", ".", "action_counts_temp", "[", "action", "]", "+=", "1", "\n", "self", ".", "reward_counts_temp", "[", "action", "]", "+=", "reward", "\n", "if", "self", ".", "n_trial", "%", "self", ".", "batch_size", "==", "0", ":", "\n", "            ", "self", ".", "action_counts", "=", "np", ".", "copy", "(", "self", ".", "action_counts_temp", ")", "\n", "self", ".", "reward_counts", "=", "np", ".", "copy", "(", "self", ".", "reward_counts_temp", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.contextfree.BernoulliTS.compute_batch_action_dist": [[240, 272], ["numpy.zeros", "numpy.arange", "numpy.tile", "contextfree.BernoulliTS.select_action", "numpy.arange"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.policy.linear.LinTS.select_action"], ["", "", "def", "compute_batch_action_dist", "(", "\n", "self", ",", "\n", "n_rounds", ":", "int", "=", "1", ",", "\n", "n_sim", ":", "int", "=", "100000", ",", "\n", ")", "->", "np", ".", "ndarray", ":", "\n", "        ", "\"\"\"Compute the distribution over actions by Monte Carlo simulation.\n\n        Parameters\n        ----------\n        n_rounds: int, default=1\n            Number of rounds in the distribution over actions.\n            (the size of the first axis of `action_dist`)\n\n        n_sim: int, default=100000\n            Number of simulations in the Monte Carlo simulation to compute the distribution over actions.\n\n        Returns\n        ----------\n        action_dist: array-like, shape (n_rounds, n_actions, len_list)\n            Probability estimates of each arm being the best one for each sample, action, and position.\n\n        \"\"\"", "\n", "action_count", "=", "np", ".", "zeros", "(", "(", "self", ".", "n_actions", ",", "self", ".", "len_list", ")", ")", "\n", "for", "_", "in", "np", ".", "arange", "(", "n_sim", ")", ":", "\n", "            ", "selected_actions", "=", "self", ".", "select_action", "(", ")", "\n", "for", "pos", "in", "np", ".", "arange", "(", "self", ".", "len_list", ")", ":", "\n", "                ", "action_count", "[", "selected_actions", "[", "pos", "]", ",", "pos", "]", "+=", "1", "\n", "", "", "action_dist", "=", "np", ".", "tile", "(", "\n", "action_count", "/", "n_sim", ",", "\n", "(", "n_rounds", ",", "1", ",", "1", ")", ",", "\n", ")", "\n", "return", "action_dist", "\n", "", "", ""]], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.offline.IPWLearner.__post_init__": [[64, 74], ["super().__post_init__", "sklearn.linear_model.LogisticRegression", "sklearn.base.clone", "sklearn.base.is_classifier", "ValueError", "numpy.arange"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.__post_init__"], ["def", "__post_init__", "(", "self", ")", "->", "None", ":", "\n", "        ", "\"\"\"Initialize class.\"\"\"", "\n", "super", "(", ")", ".", "__post_init__", "(", ")", "\n", "if", "self", ".", "base_classifier", "is", "None", ":", "\n", "            ", "self", ".", "base_classifier", "=", "LogisticRegression", "(", "random_state", "=", "12345", ")", "\n", "", "else", ":", "\n", "            ", "if", "not", "is_classifier", "(", "self", ".", "base_classifier", ")", ":", "\n", "                ", "raise", "ValueError", "(", "\"`base_classifier` must be a classifier\"", ")", "\n", "", "", "self", ".", "base_classifier_list", "=", "[", "\n", "clone", "(", "self", ".", "base_classifier", ")", "for", "_", "in", "np", ".", "arange", "(", "self", ".", "len_list", ")", "\n", "]", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.offline.IPWLearner._create_train_data_for_opl": [[76, 106], ["None"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "_create_train_data_for_opl", "(", "\n", "context", ":", "np", ".", "ndarray", ",", "\n", "action", ":", "np", ".", "ndarray", ",", "\n", "reward", ":", "np", ".", "ndarray", ",", "\n", "pscore", ":", "np", ".", "ndarray", ",", "\n", ")", "->", "Tuple", "[", "np", ".", "ndarray", ",", "np", ".", "ndarray", ",", "np", ".", "ndarray", "]", ":", "\n", "        ", "\"\"\"Create training data for off-policy learning.\n\n        Parameters\n        -----------\n        context: array-like, shape (n_rounds, dim_context)\n            Context vectors observed for each data, i.e., :math:`x_i`.\n\n        action: array-like, shape (n_rounds,)\n            Actions sampled by the logging/behavior policy for each data in logged bandit data, i.e., :math:`a_i`.\n\n        reward: array-like, shape (n_rounds,)\n            Rewards observed for each data in logged bandit data, i.e., :math:`r_i`.\n\n        pscore: array-like, shape (n_rounds,), default=None\n            Action choice probabilities of the logging/behavior policy (propensity scores), i.e., :math:`\\\\pi_b(a_i|x_i)`.\n\n        Returns\n        --------\n        (X, sample_weight, y): Tuple[np.ndarray, np.ndarray, np.ndarray]\n            Feature vectors, sample weights, and outcome for training the base machine learning model.\n\n        \"\"\"", "\n", "return", "context", ",", "(", "reward", "/", "pscore", ")", ",", "action", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.offline.IPWLearner.fit": [[107, 182], ["utils.check_bandit_feedback_inputs", "numpy.arange", "ValueError", "numpy.int32", "numpy.zeros_like", "offline.IPWLearner._create_train_data_for_opl", "offline.IPWLearner.base_classifier_list[].fit", "numpy.ones_like", "ValueError", "action.max"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_bandit_feedback_inputs", "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.offline.NNPolicyLearner._create_train_data_for_opl", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.classification_model.PropensityScoreEstimator.fit"], ["", "def", "fit", "(", "\n", "self", ",", "\n", "context", ":", "np", ".", "ndarray", ",", "\n", "action", ":", "np", ".", "ndarray", ",", "\n", "reward", ":", "np", ".", "ndarray", ",", "\n", "pscore", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "position", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", ")", "->", "None", ":", "\n", "        ", "\"\"\"Fits an offline bandit policy on the given logged bandit data.\n\n        Note\n        --------\n        This `fit` method trains a deterministic policy :math:`\\\\pi: \\\\mathcal{X} \\\\rightarrow \\\\mathcal{A}`\n        via a cost-sensitive classification reduction as follows:\n\n        .. math::\n\n            \\\\hat{\\\\pi}\n            & \\\\in \\\\arg \\\\max_{\\\\pi \\\\in \\\\Pi} \\\\hat{V}_{\\\\mathrm{IPW}} (\\\\pi ; \\\\mathcal{D}) \\\\\\\\\n            & = \\\\arg \\\\max_{\\\\pi \\\\in \\\\Pi} \\\\mathbb{E}_{n} \\\\left[\\\\frac{\\\\mathbb{I} \\\\{\\\\pi (x_{i})=a_{i} \\\\}}{\\\\pi_{b}(a_{i} | x_{i})} r_{i} \\\\right] \\\\\\\\\n            & = \\\\arg \\\\min_{\\\\pi \\\\in \\\\Pi} \\\\mathbb{E}_{n} \\\\left[\\\\frac{r_i}{\\\\pi_{b}(a_{i} | x_{i})} \\\\mathbb{I} \\\\{\\\\pi (x_{i}) \\\\neq a_{i} \\\\} \\\\right],\n\n        where :math:`\\\\mathbb{E}_{n} [\\cdot]` is the empirical average over observations in :math:`\\\\mathcal{D}`.\n        See the reference for the details.\n\n\n        Parameters\n        -----------\n        context: array-like, shape (n_rounds, dim_context)\n            Context vectors observed for each data, i.e., :math:`x_i`.\n\n        action: array-like, shape (n_rounds,)\n            Actions sampled by the logging/behavior policy for each data in logged bandit data, i.e., :math:`a_i`.\n\n        reward: array-like, shape (n_rounds,)\n            Rewards observed for each data in logged bandit data, i.e., :math:`r_i`.\n\n        pscore: array-like, shape (n_rounds,), default=None\n            Action choice probabilities of the logging/behavior policy (propensity scores), i.e., :math:`\\\\pi_b(a_i|x_i)`.\n\n        position: array-like, shape (n_rounds,), default=None\n            Indices to differentiate positions in a recommendation interface where the actions are presented.\n            If None, a learner assumes that only a single action is chosen for each data.\n\n        \"\"\"", "\n", "check_bandit_feedback_inputs", "(", "\n", "context", "=", "context", ",", "\n", "action", "=", "action", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore", "=", "pscore", ",", "\n", "position", "=", "position", ",", "\n", ")", "\n", "if", "(", "reward", "<", "0", ")", ".", "any", "(", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"A negative value is found in `reward`.\"", "\n", "\"`obp.policy.IPWLearner` cannot handle negative rewards,\"", "\n", "\"and please use `obp.policy.NNPolicyLearner` instead.\"", "\n", ")", "\n", "", "if", "pscore", "is", "None", ":", "\n", "            ", "n_actions", "=", "np", ".", "int32", "(", "action", ".", "max", "(", ")", "+", "1", ")", "\n", "pscore", "=", "np", ".", "ones_like", "(", "action", ")", "/", "n_actions", "\n", "", "if", "self", ".", "len_list", "==", "1", ":", "\n", "            ", "position", "=", "np", ".", "zeros_like", "(", "action", ",", "dtype", "=", "int", ")", "\n", "", "else", ":", "\n", "            ", "if", "position", "is", "None", ":", "\n", "                ", "raise", "ValueError", "(", "\"When `self.len_list > 1`, `position` must be given.\"", ")", "\n", "\n", "", "", "for", "p", "in", "np", ".", "arange", "(", "self", ".", "len_list", ")", ":", "\n", "            ", "X", ",", "sample_weight", ",", "y", "=", "self", ".", "_create_train_data_for_opl", "(", "\n", "context", "=", "context", "[", "position", "==", "p", "]", ",", "\n", "action", "=", "action", "[", "position", "==", "p", "]", ",", "\n", "reward", "=", "reward", "[", "position", "==", "p", "]", ",", "\n", "pscore", "=", "pscore", "[", "position", "==", "p", "]", ",", "\n", ")", "\n", "self", ".", "base_classifier_list", "[", "p", "]", ".", "fit", "(", "X", "=", "X", ",", "y", "=", "y", ",", "sample_weight", "=", "sample_weight", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.offline.IPWLearner.predict": [[183, 217], ["utils.check_array", "numpy.zeros", "numpy.arange", "offline.IPWLearner.base_classifier_list[].predict", "numpy.arange", "numpy.ones"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.classification_model.PropensityScoreEstimator.predict"], ["", "", "def", "predict", "(", "self", ",", "context", ":", "np", ".", "ndarray", ")", "->", "np", ".", "ndarray", ":", "\n", "        ", "\"\"\"Predict best actions for new data.\n\n        Note\n        --------\n        Action set predicted by this `predict` method can contain duplicate items.\n        If a non-repetitive action set is needed, please use the `sample_action` method.\n\n        Parameters\n        -----------\n        context: array-like, shape (n_rounds_of_new_data, dim_context)\n            Context vectors for new data.\n\n        Returns\n        -----------\n        action_dist: array-like, shape (n_rounds_of_new_data, n_actions, len_list)\n            Action choices made by a classifier, which can contain duplicate items.\n            If a non-repetitive action set is needed, please use the `sample_action` method.\n\n        \"\"\"", "\n", "check_array", "(", "array", "=", "context", ",", "name", "=", "\"context\"", ",", "expected_dim", "=", "2", ")", "\n", "\n", "n_rounds", "=", "context", ".", "shape", "[", "0", "]", "\n", "action_dist", "=", "np", ".", "zeros", "(", "(", "n_rounds", ",", "self", ".", "n_actions", ",", "self", ".", "len_list", ")", ")", "\n", "for", "p", "in", "np", ".", "arange", "(", "self", ".", "len_list", ")", ":", "\n", "            ", "predicted_actions_at_position", "=", "self", ".", "base_classifier_list", "[", "p", "]", ".", "predict", "(", "\n", "context", "\n", ")", "\n", "action_dist", "[", "\n", "np", ".", "arange", "(", "n_rounds", ")", ",", "\n", "predicted_actions_at_position", ",", "\n", "np", ".", "ones", "(", "n_rounds", ",", "dtype", "=", "int", ")", "*", "p", ",", "\n", "]", "+=", "1", "\n", "", "return", "action_dist", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.offline.IPWLearner.predict_score": [[218, 242], ["utils.check_array", "numpy.zeros", "numpy.arange", "offline.IPWLearner.base_classifier_list[].predict_proba"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.offline.NNPolicyLearner.predict_proba"], ["", "def", "predict_score", "(", "self", ",", "context", ":", "np", ".", "ndarray", ")", "->", "np", ".", "ndarray", ":", "\n", "        ", "\"\"\"Predict non-negative scores for all possible pairs of actions and positions.\n\n        Parameters\n        -----------\n        context: array-like, shape (n_rounds_of_new_data, dim_context)\n            Context vectors for new data.\n\n        Returns\n        -----------\n        score_predicted: array-like, shape (n_rounds_of_new_data, n_actions, len_list)\n            Scores for all possible pairs of actions and positions predicted by a classifier.\n\n        \"\"\"", "\n", "check_array", "(", "array", "=", "context", ",", "name", "=", "\"context\"", ",", "expected_dim", "=", "2", ")", "\n", "\n", "n", "=", "context", ".", "shape", "[", "0", "]", "\n", "score_predicted", "=", "np", ".", "zeros", "(", "(", "n", ",", "self", ".", "n_actions", ",", "self", ".", "len_list", ")", ")", "\n", "for", "p", "in", "np", ".", "arange", "(", "self", ".", "len_list", ")", ":", "\n", "            ", "score_predicteds_at_position", "=", "self", ".", "base_classifier_list", "[", "p", "]", ".", "predict_proba", "(", "\n", "context", "\n", ")", "\n", "score_predicted", "[", ":", ",", ":", ",", "p", "]", "=", "score_predicteds_at_position", "\n", "", "return", "score_predicted", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.offline.IPWLearner.sample_action": [[243, 299], ["utils.check_array", "sklearn.utils.check_scalar", "sklearn.utils.check_random_state", "numpy.zeros", "sklearn.utils.check_random_state.gumbel", "numpy.argsort", "numpy.arange", "offline.IPWLearner.predict_score().mean", "offline.IPWLearner.predict_score", "numpy.arange"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.offline.QLearner.predict_score"], ["", "def", "sample_action", "(", "\n", "self", ",", "\n", "context", ":", "np", ".", "ndarray", ",", "\n", "tau", ":", "Union", "[", "int", ",", "float", "]", "=", "1.0", ",", "\n", "random_state", ":", "Optional", "[", "int", "]", "=", "None", ",", "\n", ")", "->", "np", ".", "ndarray", ":", "\n", "        ", "\"\"\"Sample a ranking of (non-repetitive) actions from the Plackett-Luce ranking distribution.\n\n        Note\n        --------\n        This `sample_action` method samples a **non-repetitive** ranking of actions for new data\n        :math:`x \\\\in \\\\mathcal{X}` via the so-called \"Gumbel Softmax trick\" as follows.\n\n        .. math::\n\n            \\\\s (x,a) = \\\\hat{f}(x,a) / \\\\tau + \\\\gamma_{x,a}, \\\\quad \\\\gamma_{x,a} \\\\sim \\\\mathrm{Gumbel}(0,1)\n\n        :math:`\\\\tau` is a temperature hyperparameter.\n        :math:`f: \\\\mathcal{X} \\\\times \\\\mathcal{A} \\\\times \\\\mathcal{K} \\\\rightarrow \\\\mathbb{R}_{+}`\n        is a scoring function which is now implemented in the `predict_score` method.\n        When `len_list > 0`,  the expected rewards estimated at different positions will be averaged to form :math:`f(x,a)`.\n        :math:`\\\\gamma_{x,a}` is a random variable sampled from the Gumbel distribution.\n        By sorting the actions based on :math:`\\\\s (x,a)` for each context, we can efficiently sample a ranking from\n        the Plackett-Luce ranking distribution.\n\n        Parameters\n        ----------------\n        context: array-like, shape (n_rounds_of_new_data, dim_context)\n            Context vectors for new data.\n\n        tau: int or float, default=1.0\n            A temperature parameter that controls the randomness of the action choice\n            by scaling the scores before applying softmax.\n            As :math:`\\\\tau \\\\rightarrow \\\\infty`, the algorithm will select arms uniformly at random.\n\n        random_state: int, default=None\n            Controls the random seed in sampling actions.\n\n        Returns\n        -----------\n        sampled_ranking: array-like, shape (n_rounds_of_new_data, n_actions, len_list)\n            Ranking of actions sampled via the Gumbel softmax trick.\n\n        \"\"\"", "\n", "check_array", "(", "array", "=", "context", ",", "name", "=", "\"context\"", ",", "expected_dim", "=", "2", ")", "\n", "check_scalar", "(", "tau", ",", "name", "=", "\"tau\"", ",", "target_type", "=", "(", "int", ",", "float", ")", ",", "min_val", "=", "0", ")", "\n", "\n", "n", "=", "context", ".", "shape", "[", "0", "]", "\n", "random_", "=", "check_random_state", "(", "random_state", ")", "\n", "sampled_ranking", "=", "np", ".", "zeros", "(", "(", "n", ",", "self", ".", "n_actions", ",", "self", ".", "len_list", ")", ")", "\n", "scores", "=", "self", ".", "predict_score", "(", "context", "=", "context", ")", ".", "mean", "(", "2", ")", "/", "tau", "\n", "scores", "+=", "random_", ".", "gumbel", "(", "size", "=", "scores", ".", "shape", ")", "\n", "sampled_ranking_full", "=", "np", ".", "argsort", "(", "-", "scores", ",", "axis", "=", "1", ")", "\n", "for", "p", "in", "np", ".", "arange", "(", "self", ".", "len_list", ")", ":", "\n", "            ", "sampled_ranking", "[", "np", ".", "arange", "(", "n", ")", ",", "sampled_ranking_full", "[", ":", ",", "p", "]", ",", "p", "]", "=", "1", "\n", "", "return", "sampled_ranking", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.offline.IPWLearner.predict_proba": [[300, 347], ["utils.check_array", "sklearn.utils.check_scalar", "offline.IPWLearner.predict_score", "scipy.special.softmax"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.offline.QLearner.predict_score", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.softmax"], ["", "def", "predict_proba", "(", "\n", "self", ",", "\n", "context", ":", "np", ".", "ndarray", ",", "\n", "tau", ":", "Union", "[", "int", ",", "float", "]", "=", "1.0", ",", "\n", ")", "->", "np", ".", "ndarray", ":", "\n", "        ", "\"\"\"Obtains action choice probabilities for new data based on scores predicted by a classifier.\n\n        Note\n        --------\n        This `predict_proba` method obtains action choice probabilities for new data :math:`x \\\\in \\\\mathcal{X}`\n        by applying the softmax function as follows:\n\n        .. math::\n\n            P (A = a | x) = \\\\frac{\\\\mathrm{exp}(f(x,a) / \\\\tau)}{\\\\sum_{a^{\\\\prime} \\\\in \\\\mathcal{A}} \\\\mathrm{exp}(f(x,a^{\\\\prime}) / \\\\tau)},\n\n        where :math:`A` is a random variable representing an action, and :math:`\\\\tau` is a temperature hyperparameter.\n        :math:`f: \\\\mathcal{X} \\\\times \\\\mathcal{A} \\\\rightarrow \\\\mathbb{R}_{+}`\n        is a scoring function which is now implemented in the `predict_score` method.\n\n        **Note that this method can be used only when `len_list=1`, please use the `sample_action` method otherwise.**\n\n        Parameters\n        ----------------\n        context: array-like, shape (n_rounds_of_new_data, dim_context)\n            Context vectors for new data.\n\n        tau: int or float, default=1.0\n            A temperature parameter that controls the randomness of the action choice\n            by scaling the scores before applying softmax.\n            As :math:`\\\\tau \\\\rightarrow \\\\infty`, the algorithm will select arms uniformly at random.\n\n        Returns\n        -----------\n        choice_prob: array-like, shape (n_rounds_of_new_data, n_actions, len_list)\n            Action choice probabilities obtained by a trained classifier.\n\n        \"\"\"", "\n", "assert", "(", "\n", "self", ".", "len_list", "==", "1", "\n", ")", ",", "\"predict_proba method cannot be used when `len_list != 1`\"", "\n", "check_array", "(", "array", "=", "context", ",", "name", "=", "\"context\"", ",", "expected_dim", "=", "2", ")", "\n", "check_scalar", "(", "tau", ",", "name", "=", "\"tau\"", ",", "target_type", "=", "(", "int", ",", "float", ")", ",", "min_val", "=", "0", ")", "\n", "\n", "score_predicted", "=", "self", ".", "predict_score", "(", "context", "=", "context", ")", "\n", "choice_prob", "=", "softmax", "(", "score_predicted", "/", "tau", ",", "axis", "=", "1", ")", "\n", "return", "choice_prob", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.offline.QLearner.__post_init__": [[374, 383], ["super().__post_init__", "obp.ope.RegressionModel"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.__post_init__"], ["def", "__post_init__", "(", "self", ")", "->", "None", ":", "\n", "        ", "\"\"\"Initialize class.\"\"\"", "\n", "super", "(", ")", ".", "__post_init__", "(", ")", "\n", "\n", "self", ".", "q_estimator", "=", "RegressionModel", "(", "\n", "n_actions", "=", "self", ".", "n_actions", ",", "\n", "len_list", "=", "self", ".", "len_list", ",", "\n", "base_model", "=", "self", ".", "base_model", ",", "\n", "fitting_method", "=", "self", ".", "fitting_method", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.offline.QLearner.fit": [[385, 450], ["utils.check_bandit_feedback_inputs", "numpy.ones", "offline.QLearner.q_estimator.fit", "numpy.int32", "numpy.zeros_like", "numpy.ones_like", "ValueError", "action.max"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_bandit_feedback_inputs", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.classification_model.PropensityScoreEstimator.fit"], ["", "def", "fit", "(", "\n", "self", ",", "\n", "context", ":", "np", ".", "ndarray", ",", "\n", "action", ":", "np", ".", "ndarray", ",", "\n", "reward", ":", "np", ".", "ndarray", ",", "\n", "pscore", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "position", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", ")", "->", "None", ":", "\n", "        ", "\"\"\"Fits an offline bandit policy on the given logged bandit feedback data.\n\n        Note\n        --------\n        This `fit` method trains an estimator for the q function :math:`\\\\q(x,a) := \\\\mathbb{E} [r \\\\mid x, a]` as follows.\n\n        .. math::\n\n            \\\\hat{\\\\q} \\\\in \\\\arg \\\\min_{\\\\q \\\\in \\\\Q} \\\\mathbb{E}_{n} [ \\\\ell ( r_i, q (x_i,a_i) )  ]\n\n        where :math:`\\\\ell` is a loss function in training the q estimator.\n\n\n        Parameters\n        -----------\n        context: array-like, shape (n_rounds, dim_context)\n            Context vectors observed for each data, i.e., :math:`x_i`.\n\n        action: array-like, shape (n_rounds,)\n            Actions sampled by the logging/behavior policy for each data in logged bandit data, i.e., :math:`a_i`.\n\n        reward: array-like, shape (n_rounds,)\n            Rewards observed for each data in logged bandit data, i.e., :math:`r_i`.\n\n        pscore: array-like, shape (n_rounds,), default=None\n            Action choice probabilities of the logging/behavior policy (propensity scores), i.e., :math:`\\\\pi_b(a_i|x_i)`.\n\n        position: array-like, shape (n_rounds,), default=None\n            Indices to differentiate positions in a recommendation interface where the actions are presented.\n            If None, a learner assumes that only a single action is chosen for each data.\n            When `len_list` > 1, position has to be set.\n\n        \"\"\"", "\n", "check_bandit_feedback_inputs", "(", "\n", "context", "=", "context", ",", "\n", "action", "=", "action", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore", "=", "pscore", ",", "\n", "position", "=", "position", ",", "\n", ")", "\n", "if", "pscore", "is", "None", ":", "\n", "            ", "n_actions", "=", "np", ".", "int32", "(", "action", ".", "max", "(", ")", "+", "1", ")", "\n", "pscore", "=", "np", ".", "ones_like", "(", "action", ")", "/", "n_actions", "\n", "", "if", "self", ".", "len_list", "==", "1", ":", "\n", "            ", "position", "=", "np", ".", "zeros_like", "(", "action", ",", "dtype", "=", "int", ")", "\n", "", "else", ":", "\n", "            ", "if", "position", "is", "None", ":", "\n", "                ", "raise", "ValueError", "(", "\"When `self.len_list > 1`, `position` must be given.\"", ")", "\n", "\n", "", "", "unif_action_dist", "=", "np", ".", "ones", "(", "(", "context", ".", "shape", "[", "0", "]", ",", "self", ".", "n_actions", ",", "self", ".", "len_list", ")", ")", "\n", "self", ".", "q_estimator", ".", "fit", "(", "\n", "context", "=", "context", ",", "\n", "action", "=", "action", ",", "\n", "reward", "=", "reward", ",", "\n", "position", "=", "position", ",", "\n", "pscore", "=", "pscore", ",", "\n", "action_dist", "=", "unif_action_dist", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.offline.QLearner.predict": [[452, 494], ["utils.check_array", "sklearn.utils.check_scalar", "offline.QLearner.predict_score", "numpy.argmax().astype", "numpy.zeros_like", "numpy.arange", "numpy.argmax", "numpy.arange"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.offline.QLearner.predict_score"], ["", "def", "predict", "(", "\n", "self", ",", "\n", "context", ":", "np", ".", "ndarray", ",", "\n", "tau", ":", "Union", "[", "int", ",", "float", "]", "=", "1.0", ",", "\n", ")", "->", "np", ".", "ndarray", ":", "\n", "        ", "\"\"\"Predict best actions for new data deterministically.\n\n        Note\n        --------\n        This `predict` method predicts the best actions for new data deterministically as follows.\n\n        .. math::\n\n            \\\\hat{a}_i \\\\in \\\\arg \\\\max_{a \\\\in \\\\mathcal{A}} \\\\hat{q}(x_i, a)\n\n        where :math:`\\\\hat{q}(x,a)` is an estimator for the q function :math:`\\\\q(x,a) := \\\\mathbb{E} [r \\\\mid x, a]`.\n        Note that action sets predicted by this `predict` method can contain duplicate items.\n        If a non-repetitive action set is needed, please use the `sample_action` method.\n\n        Parameters\n        -----------\n        context: array-like, shape (n_rounds_of_new_data, dim_context)\n            Context vectors for new data.\n\n        Returns\n        -----------\n        action_dist: array-like, shape (n_rounds_of_new_data, n_actions, len_list)\n            Deterministic action choices made by the QLearner.\n            The output can contain duplicated items (when `len_list > 1`).\n\n        \"\"\"", "\n", "check_array", "(", "array", "=", "context", ",", "name", "=", "\"context\"", ",", "expected_dim", "=", "2", ")", "\n", "check_scalar", "(", "tau", ",", "name", "=", "\"tau\"", ",", "target_type", "=", "(", "int", ",", "float", ")", ",", "min_val", "=", "0", ")", "\n", "\n", "q_hat", "=", "self", ".", "predict_score", "(", "context", "=", "context", ")", "\n", "q_hat_argmax", "=", "np", ".", "argmax", "(", "q_hat", ",", "axis", "=", "1", ")", ".", "astype", "(", "int", ")", "\n", "\n", "n", "=", "context", ".", "shape", "[", "0", "]", "\n", "action_dist", "=", "np", ".", "zeros_like", "(", "q_hat", ")", "\n", "for", "p", "in", "np", ".", "arange", "(", "self", ".", "len_list", ")", ":", "\n", "            ", "action_dist", "[", "np", ".", "arange", "(", "n", ")", ",", "q_hat_argmax", "[", ":", ",", "p", "]", ",", "p", "]", "=", "1", "\n", "", "return", "action_dist", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.offline.QLearner.predict_score": [[495, 513], ["utils.check_array", "offline.QLearner.q_estimator.predict"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.classification_model.PropensityScoreEstimator.predict"], ["", "def", "predict_score", "(", "self", ",", "context", ":", "np", ".", "ndarray", ")", "->", "np", ".", "ndarray", ":", "\n", "        ", "\"\"\"Predict the expected rewards for all possible pairs of actions and positions.\n\n        Parameters\n        -----------\n        context: array-like, shape (n_rounds_of_new_data, dim_context)\n            Context vectors for new data.\n\n        Returns\n        -----------\n        q_hat: array-like, shape (n_rounds_of_new_data, n_actions, len_list)\n            Expected rewards for all possible pairs of actions and positions. :math:`\\\\hat{q}(x,a)`.\n\n        \"\"\"", "\n", "check_array", "(", "array", "=", "context", ",", "name", "=", "\"context\"", ",", "expected_dim", "=", "2", ")", "\n", "\n", "q_hat", "=", "self", ".", "q_estimator", ".", "predict", "(", "context", "=", "context", ")", "\n", "return", "q_hat", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.offline.QLearner.sample_action": [[514, 570], ["utils.check_array", "sklearn.utils.check_scalar", "sklearn.utils.check_random_state", "numpy.zeros", "sklearn.utils.check_random_state.gumbel", "numpy.argsort", "numpy.arange", "offline.QLearner.predict_score().mean", "offline.QLearner.predict_score", "numpy.arange"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.offline.QLearner.predict_score"], ["", "def", "sample_action", "(", "\n", "self", ",", "\n", "context", ":", "np", ".", "ndarray", ",", "\n", "tau", ":", "Union", "[", "int", ",", "float", "]", "=", "1.0", ",", "\n", "random_state", ":", "Optional", "[", "int", "]", "=", "None", ",", "\n", ")", "->", "np", ".", "ndarray", ":", "\n", "        ", "\"\"\"Sample a ranking of (non-repetitive) actions from the Plackett-Luce ranking distribution.\n\n        Note\n        --------\n        This `sample_action` method samples a ranking of (non-repetitive) actions for new data\n        based on :math:`\\\\hat{q}` and the so-called \"Gumbel Softmax trick\" as follows.\n\n        .. math::\n\n            \\\\s (x,a) = \\\\hat{q}(x,a) / \\\\tau + \\\\gamma_{x,a}, \\\\quad \\\\gamma_{x,a} \\\\sim \\\\mathrm{Gumbel}(0,1)\n\n        :math:`\\\\tau` is a temperature hyperparameter.\n        :math:`\\\\hat{q}: \\\\mathcal{X} \\\\times \\\\mathcal{A} \\\\times \\\\mathcal{K} \\\\rightarrow \\\\mathbb{R}_{+}`\n        is a q function estimator, which is now implemented in the `predict_score` method.\n        When `len_list > 0`,  the expected rewards estimated at different positions will be averaged to form :math:`f(x,a)`.\n        :math:`\\\\gamma_{x,a}` is a random variable sampled from the Gumbel distribution.\n        By sorting the actions based on :math:`\\\\s (x,a)` for each context, we can efficiently sample a ranking from\n        the Plackett-Luce ranking distribution.\n\n        Parameters\n        ----------------\n        context: array-like, shape (n_rounds_of_new_data, dim_context)\n            Context vectors for new data.\n\n        tau: int or float, default=1.0\n            A temperature parameter that controls the randomness of the action choice\n            by scaling the scores before applying softmax.\n            As :math:`\\\\tau \\\\rightarrow \\\\infty`, the algorithm will select arms uniformly at random.\n\n        random_state: int, default=None\n            Controls the random seed in sampling actions.\n\n        Returns\n        -----------\n        sampled_action: array-like, shape (n_rounds_of_new_data, n_actions, len_list)\n            Ranking of actions sampled from the Plackett-Luce ranking distribution via the Gumbel softmax trick.\n\n        \"\"\"", "\n", "check_array", "(", "array", "=", "context", ",", "name", "=", "\"context\"", ",", "expected_dim", "=", "2", ")", "\n", "check_scalar", "(", "tau", ",", "name", "=", "\"tau\"", ",", "target_type", "=", "(", "int", ",", "float", ")", ",", "min_val", "=", "0", ")", "\n", "\n", "n", "=", "context", ".", "shape", "[", "0", "]", "\n", "random_", "=", "check_random_state", "(", "random_state", ")", "\n", "sampled_action", "=", "np", ".", "zeros", "(", "(", "n", ",", "self", ".", "n_actions", ",", "self", ".", "len_list", ")", ")", "\n", "scores", "=", "self", ".", "predict_score", "(", "context", "=", "context", ")", ".", "mean", "(", "2", ")", "/", "tau", "\n", "scores", "+=", "random_", ".", "gumbel", "(", "size", "=", "scores", ".", "shape", ")", "\n", "ranking", "=", "np", ".", "argsort", "(", "-", "scores", ",", "axis", "=", "1", ")", "\n", "for", "p", "in", "np", ".", "arange", "(", "self", ".", "len_list", ")", ":", "\n", "            ", "sampled_action", "[", "np", ".", "arange", "(", "n", ")", ",", "ranking", "[", ":", ",", "p", "]", ",", "p", "]", "=", "1", "\n", "", "return", "sampled_action", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.offline.QLearner.predict_proba": [[571, 613], ["utils.check_array", "sklearn.utils.check_scalar", "offline.QLearner.predict_score", "utils.softmax"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.offline.QLearner.predict_score", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.softmax"], ["", "def", "predict_proba", "(", "\n", "self", ",", "\n", "context", ":", "np", ".", "ndarray", ",", "\n", "tau", ":", "Union", "[", "int", ",", "float", "]", "=", "1.0", ",", "\n", ")", "->", "np", ".", "ndarray", ":", "\n", "        ", "\"\"\"Obtains action choice probabilities for new data based on the estimated expected rewards.\n\n        Note\n        --------\n        This `predict_proba` method obtains action choice probabilities for new data based on :math:`\\\\hat{q}` as follows.\n\n        .. math::\n\n            \\\\pi_{l} (a|x) = \\\\frac{\\\\mathrm{exp}( \\\\hat{q}_{l}(x,a) / \\\\tau)}{\\\\sum_{a^{\\\\prime} \\\\in \\\\mathcal{A}} \\\\mathrm{exp}( \\\\hat{q}_{l}(x,a^{\\\\prime}) / \\\\tau)}\n\n        where :math:`\\\\pi_{l} (a|x)` is the resulting action choice probabilities at position :math:`l`.\n        :math:`\\\\tau` is a temperature hyperparameter.\n        :math:`\\\\hat{q}: \\\\mathcal{X} \\\\times \\\\mathcal{A} \\\\times \\\\mathcal{K} \\\\rightarrow \\\\mathbb{R}_{+}`\n        is a q function estimator for position :math:`l`, which is now implemented in the `predict_score` method.\n\n        Parameters\n        ----------------\n        context: array-like, shape (n_rounds_of_new_data, dim_context)\n            Context vectors for new data.\n\n        tau: int or float, default=1.0\n            A temperature parameter that controls the randomness of the action choice\n            by scaling the scores before applying softmax.\n            As :math:`\\\\tau \\\\rightarrow \\\\infty`, the algorithm will select arms uniformly at random.\n\n        Returns\n        -----------\n        action_dist: array-like, shape (n_rounds_of_new_data, n_actions, len_list)\n            Action choice probabilities obtained from the estimated expected rewards.\n\n        \"\"\"", "\n", "check_array", "(", "array", "=", "context", ",", "name", "=", "\"context\"", ",", "expected_dim", "=", "2", ")", "\n", "check_scalar", "(", "tau", ",", "name", "=", "\"tau\"", ",", "target_type", "=", "(", "int", ",", "float", ")", ",", "min_val", "=", "0", ")", "\n", "\n", "q_hat", "=", "self", ".", "predict_score", "(", "context", "=", "context", ")", "\n", "action_dist", "=", "softmax_axis1", "(", "q_hat", "/", "tau", ")", "\n", "return", "action_dist", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.offline.NNPolicyLearner.__post_init__": [[779, 952], ["super().__post_init__", "sklearn.utils.check_scalar", "sklearn.utils.check_scalar", "sklearn.utils.check_scalar", "sklearn.utils.check_scalar", "sklearn.utils.check_scalar", "sklearn.utils.check_scalar", "sklearn.utils.check_scalar", "sklearn.utils.check_scalar", "sklearn.utils.check_scalar", "sklearn.utils.check_scalar", "sklearn.utils.check_scalar", "sklearn.utils.check_scalar", "sklearn.utils.check_scalar", "enumerate", "layer_list.append", "layer_list.append", "torch.Sequential", "torch.Sequential", "torch.Sequential", "ValueError", "sklearn.utils.check_scalar", "any", "ValueError", "ValueError", "ValueError", "ValueError", "isinstance", "ValueError", "ValueError", "isinstance", "ValueError", "isinstance", "ValueError", "ValueError", "sklearn.utils.check_random_state", "torch.manual_seed", "torch.manual_seed", "torch.manual_seed", "torch.manual_seed", "torch.manual_seed", "torch.manual_seed", "torch.manual_seed", "torch.manual_seed", "torch.manual_seed", "layer_list.append", "layer_list.append", "collections.OrderedDict", "sklearn.utils.check_scalar", "isinstance", "isinstance", "ValueError", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Softmax", "torch.Softmax", "torch.Softmax", "offline.QFuncEstimator", "offline.QFuncEstimator", "sklearn.utils.check_scalar", "isinstance", "torch.Linear", "torch.Linear", "torch.Linear", "activation_layer", "isinstance", "type", "ValueError"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.__post_init__"], ["def", "__post_init__", "(", "self", ")", "->", "None", ":", "\n", "        ", "\"\"\"Initialize class.\"\"\"", "\n", "super", "(", ")", ".", "__post_init__", "(", ")", "\n", "\n", "check_scalar", "(", "self", ".", "dim_context", ",", "\"dim_context\"", ",", "int", ",", "min_val", "=", "1", ")", "\n", "\n", "if", "self", ".", "off_policy_objective", "not", "in", "[", "\n", "\"dm\"", ",", "\n", "\"ipw\"", ",", "\n", "\"dr\"", ",", "\n", "\"snipw\"", ",", "\n", "\"ipw-os\"", ",", "\n", "\"ipw-subgauss\"", ",", "\n", "]", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"`off_policy_objective` must be one of 'dm', 'ipw', 'dr', 'snipw', 'ipw-os', 'ipw-subgauss'\"", "\n", "f\", but {self.off_policy_objective} is given\"", "\n", ")", "\n", "\n", "", "if", "self", ".", "off_policy_objective", "==", "\"ipw-subgauss\"", ":", "\n", "            ", "if", "self", ".", "lambda_", "is", "None", ":", "\n", "                ", "self", ".", "lambda_", "=", "0.001", "\n", "", "check_scalar", "(", "\n", "self", ".", "lambda_", ",", "\n", "\"lambda_\"", ",", "\n", "(", "int", ",", "float", ")", ",", "\n", "min_val", "=", "0.0", ",", "\n", "max_val", "=", "1.0", ",", "\n", ")", "\n", "\n", "", "elif", "self", ".", "off_policy_objective", "==", "\"snipw\"", ":", "\n", "            ", "if", "self", ".", "lambda_", "is", "None", ":", "\n", "                ", "self", ".", "lambda_", "=", "0.0", "\n", "", "check_scalar", "(", "\n", "self", ".", "lambda_", ",", "\n", "\"lambda_\"", ",", "\n", "(", "int", ",", "float", ")", ",", "\n", "min_val", "=", "0.0", ",", "\n", ")", "\n", "\n", "", "elif", "self", ".", "off_policy_objective", "==", "\"ipw-os\"", ":", "\n", "            ", "if", "self", ".", "lambda_", "is", "None", ":", "\n", "                ", "self", ".", "lambda_", "=", "10000", "\n", "", "check_scalar", "(", "\n", "self", ".", "lambda_", ",", "\n", "\"lambda_\"", ",", "\n", "(", "int", ",", "float", ")", ",", "\n", "min_val", "=", "0.0", ",", "\n", ")", "\n", "\n", "", "check_scalar", "(", "\n", "self", ".", "policy_reg_param", ",", "\n", "\"policy_reg_param\"", ",", "\n", "(", "int", ",", "float", ")", ",", "\n", "min_val", "=", "0.0", ",", "\n", ")", "\n", "\n", "check_scalar", "(", "\n", "self", ".", "var_reg_param", ",", "\n", "\"var_reg_param\"", ",", "\n", "(", "int", ",", "float", ")", ",", "\n", "min_val", "=", "0.0", ",", "\n", ")", "\n", "\n", "if", "not", "isinstance", "(", "self", ".", "hidden_layer_size", ",", "tuple", ")", "or", "any", "(", "\n", "[", "not", "isinstance", "(", "h", ",", "int", ")", "or", "h", "<=", "0", "for", "h", "in", "self", ".", "hidden_layer_size", "]", "\n", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "f\"`hidden_layer_size` must be a tuple of positive integers, but {self.hidden_layer_size} is given\"", "\n", ")", "\n", "\n", "", "if", "self", ".", "solver", "not", "in", "(", "\"adagrad\"", ",", "\"sgd\"", ",", "\"adam\"", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "f\"`solver` must be one of 'adam', 'adagrad', or 'sgd', but {self.solver} is given\"", "\n", ")", "\n", "\n", "", "check_scalar", "(", "self", ".", "alpha", ",", "\"alpha\"", ",", "float", ",", "min_val", "=", "0.0", ")", "\n", "\n", "if", "self", ".", "batch_size", "!=", "\"auto\"", "and", "(", "\n", "not", "isinstance", "(", "self", ".", "batch_size", ",", "int", ")", "or", "self", ".", "batch_size", "<=", "0", "\n", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "f\"`batch_size` must be a positive integer or 'auto', but {self.batch_size} is given\"", "\n", ")", "\n", "\n", "", "check_scalar", "(", "self", ".", "learning_rate_init", ",", "\"learning_rate_init\"", ",", "float", ")", "\n", "if", "self", ".", "learning_rate_init", "<=", "0.0", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "f\"`learning_rate_init`= {self.learning_rate_init}, must be > 0.0\"", "\n", ")", "\n", "\n", "", "check_scalar", "(", "self", ".", "max_iter", ",", "\"max_iter\"", ",", "int", ",", "min_val", "=", "1", ")", "\n", "\n", "if", "not", "isinstance", "(", "self", ".", "shuffle", ",", "bool", ")", ":", "\n", "            ", "raise", "ValueError", "(", "f\"`shuffle` must be a bool, but {self.shuffle} is given\"", ")", "\n", "\n", "", "check_scalar", "(", "self", ".", "tol", ",", "\"tol\"", ",", "float", ")", "\n", "if", "self", ".", "tol", "<=", "0.0", ":", "\n", "            ", "raise", "ValueError", "(", "f\"`tol`= {self.tol}, must be > 0.0\"", ")", "\n", "\n", "", "check_scalar", "(", "self", ".", "momentum", ",", "\"momentum\"", ",", "float", ",", "min_val", "=", "0.0", ",", "max_val", "=", "1.0", ")", "\n", "\n", "if", "not", "isinstance", "(", "self", ".", "nesterovs_momentum", ",", "bool", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "f\"`nesterovs_momentum` must be a bool, but {self.nesterovs_momentum} is given\"", "\n", ")", "\n", "\n", "", "if", "not", "isinstance", "(", "self", ".", "early_stopping", ",", "bool", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "f\"`early_stopping` must be a bool, but {self.early_stopping} is given\"", "\n", ")", "\n", "\n", "", "check_scalar", "(", "\n", "self", ".", "validation_fraction", ",", "\"validation_fraction\"", ",", "float", ",", "max_val", "=", "1.0", "\n", ")", "\n", "if", "self", ".", "validation_fraction", "<=", "0.0", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "f\"`validation_fraction`= {self.validation_fraction}, must be > 0.0\"", "\n", ")", "\n", "\n", "", "if", "self", ".", "q_func_estimator_hyperparams", "is", "not", "None", ":", "\n", "            ", "if", "not", "isinstance", "(", "self", ".", "q_func_estimator_hyperparams", ",", "dict", ")", ":", "\n", "                ", "raise", "ValueError", "(", "\n", "\"`q_func_estimator_hyperparams` must be a dict\"", "\n", "f\", but {type(self.q_func_estimator_hyperparams)} is given\"", "\n", ")", "\n", "", "", "check_scalar", "(", "self", ".", "beta_1", ",", "\"beta_1\"", ",", "float", ",", "min_val", "=", "0.0", ",", "max_val", "=", "1.0", ")", "\n", "check_scalar", "(", "self", ".", "beta_2", ",", "\"beta_2\"", ",", "float", ",", "min_val", "=", "0.0", ",", "max_val", "=", "1.0", ")", "\n", "check_scalar", "(", "self", ".", "epsilon", ",", "\"epsilon\"", ",", "float", ",", "min_val", "=", "0.0", ")", "\n", "check_scalar", "(", "self", ".", "n_iter_no_change", ",", "\"n_iter_no_change\"", ",", "int", ",", "min_val", "=", "1", ")", "\n", "\n", "if", "self", ".", "random_state", "is", "not", "None", ":", "\n", "            ", "self", ".", "random_", "=", "check_random_state", "(", "self", ".", "random_state", ")", "\n", "torch", ".", "manual_seed", "(", "self", ".", "random_state", ")", "\n", "\n", "", "if", "self", ".", "activation", "==", "\"identity\"", ":", "\n", "            ", "activation_layer", "=", "nn", ".", "Identity", "\n", "", "elif", "self", ".", "activation", "==", "\"logistic\"", ":", "\n", "            ", "activation_layer", "=", "nn", ".", "Sigmoid", "\n", "", "elif", "self", ".", "activation", "==", "\"tanh\"", ":", "\n", "            ", "activation_layer", "=", "nn", ".", "Tanh", "\n", "", "elif", "self", ".", "activation", "==", "\"relu\"", ":", "\n", "            ", "activation_layer", "=", "nn", ".", "ReLU", "\n", "", "elif", "self", ".", "activation", "==", "\"elu\"", ":", "\n", "            ", "activation_layer", "=", "nn", ".", "ELU", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"`activation` must be one of 'identity', 'logistic', 'tanh', 'relu', or 'elu'\"", "\n", "f\", but {self.activation} is given\"", "\n", ")", "\n", "\n", "", "layer_list", "=", "[", "]", "\n", "input_size", "=", "self", ".", "dim_context", "\n", "\n", "for", "i", ",", "h", "in", "enumerate", "(", "self", ".", "hidden_layer_size", ")", ":", "\n", "            ", "layer_list", ".", "append", "(", "(", "\"l{}\"", ".", "format", "(", "i", ")", ",", "nn", ".", "Linear", "(", "input_size", ",", "h", ")", ")", ")", "\n", "layer_list", ".", "append", "(", "(", "\"a{}\"", ".", "format", "(", "i", ")", ",", "activation_layer", "(", ")", ")", ")", "\n", "input_size", "=", "h", "\n", "", "layer_list", ".", "append", "(", "(", "\"output\"", ",", "nn", ".", "Linear", "(", "input_size", ",", "self", ".", "n_actions", ")", ")", ")", "\n", "layer_list", ".", "append", "(", "(", "\"softmax\"", ",", "nn", ".", "Softmax", "(", "dim", "=", "1", ")", ")", ")", "\n", "\n", "self", ".", "nn_model", "=", "nn", ".", "Sequential", "(", "OrderedDict", "(", "layer_list", ")", ")", "\n", "\n", "if", "self", ".", "off_policy_objective", "in", "[", "\"dr\"", ",", "\"dm\"", "]", ":", "\n", "            ", "if", "self", ".", "q_func_estimator_hyperparams", "is", "not", "None", ":", "\n", "                ", "self", ".", "q_func_estimator_hyperparams", "[", "\"n_actions\"", "]", "=", "self", ".", "n_actions", "\n", "self", ".", "q_func_estimator_hyperparams", "[", "\"dim_context\"", "]", "=", "self", ".", "dim_context", "\n", "self", ".", "q_func_estimator", "=", "QFuncEstimator", "(", "\n", "**", "self", ".", "q_func_estimator_hyperparams", "\n", ")", "\n", "", "else", ":", "\n", "                ", "self", ".", "q_func_estimator", "=", "QFuncEstimator", "(", "\n", "n_actions", "=", "self", ".", "n_actions", ",", "dim_context", "=", "self", ".", "dim_context", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.offline.NNPolicyLearner._create_train_data_for_opl": [[954, 1034], ["offline.NNPolicyDataset", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "min", "sklearn.utils.check_scalar", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().long", "torch.from_numpy().long", "torch.from_numpy().long", "torch.from_numpy().long", "torch.from_numpy().long", "torch.from_numpy().long", "torch.from_numpy().long", "torch.from_numpy().long", "torch.from_numpy().long", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "max", "torch.utils.data.random_split", "torch.utils.data.random_split", "torch.utils.data.random_split", "torch.utils.data.random_split", "torch.utils.data.random_split", "torch.utils.data.random_split", "torch.utils.data.random_split", "torch.utils.data.random_split", "torch.utils.data.random_split", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "ValueError", "int", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy"], "methods", ["None"], ["", "", "", "def", "_create_train_data_for_opl", "(", "\n", "self", ",", "\n", "context", ":", "np", ".", "ndarray", ",", "\n", "action", ":", "np", ".", "ndarray", ",", "\n", "reward", ":", "np", ".", "ndarray", ",", "\n", "pscore", ":", "np", ".", "ndarray", ",", "\n", "position", ":", "np", ".", "ndarray", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "Tuple", "[", "torch", ".", "utils", ".", "data", ".", "DataLoader", ",", "Optional", "[", "torch", ".", "utils", ".", "data", ".", "DataLoader", "]", "]", ":", "\n", "        ", "\"\"\"Create training data for off-policy learning.\n\n        Parameters\n        -----------\n        context: array-like, shape (n_rounds, dim_context)\n            Context vectors observed for each data, i.e., :math:`x_i`.\n\n        action: array-like, shape (n_rounds,)\n            Actions sampled by the logging/behavior policy for each data in logged bandit data, i.e., :math:`a_i`.\n\n        reward: array-like, shape (n_rounds,)\n            Rewards observed for each data in logged bandit data, i.e., :math:`r_i`.\n\n        pscore: array-like, shape (n_rounds,), default=None\n            Action choice probabilities of the logging/behavior policy (propensity scores), i.e., :math:`\\\\pi_b(a_i|x_i)`.\n\n        position: array-like, shape (n_rounds,), default=None\n            Indices to differentiate positions in a recommendation interface where the actions are presented.\n            If None, a learner assumes that only a single action is chosen for each data.\n\n        Returns\n        --------\n        (training_data_loader, validation_data_loader): Tuple[DataLoader, Optional[DataLoader]]\n            Training and validation data loaders in PyTorch\n\n        \"\"\"", "\n", "if", "self", ".", "batch_size", "==", "\"auto\"", ":", "\n", "            ", "batch_size_", "=", "min", "(", "200", ",", "context", ".", "shape", "[", "0", "]", ")", "\n", "", "else", ":", "\n", "            ", "check_scalar", "(", "self", ".", "batch_size", ",", "\"batch_size\"", ",", "int", ",", "min_val", "=", "1", ")", "\n", "batch_size_", "=", "self", ".", "batch_size", "\n", "\n", "", "dataset", "=", "NNPolicyDataset", "(", "\n", "torch", ".", "from_numpy", "(", "context", ")", ".", "float", "(", ")", ",", "\n", "torch", ".", "from_numpy", "(", "action", ")", ".", "long", "(", ")", ",", "\n", "torch", ".", "from_numpy", "(", "reward", ")", ".", "float", "(", ")", ",", "\n", "torch", ".", "from_numpy", "(", "pscore", ")", ".", "float", "(", ")", ",", "\n", "torch", ".", "from_numpy", "(", "position", ")", ".", "float", "(", ")", ",", "\n", ")", "\n", "\n", "if", "self", ".", "early_stopping", ":", "\n", "            ", "if", "context", ".", "shape", "[", "0", "]", "<=", "1", ":", "\n", "                ", "raise", "ValueError", "(", "\n", "f\"the number of samples is too small ({context.shape[0]}) to create validation data\"", "\n", ")", "\n", "\n", "", "validation_size", "=", "max", "(", "int", "(", "context", ".", "shape", "[", "0", "]", "*", "self", ".", "validation_fraction", ")", ",", "1", ")", "\n", "training_size", "=", "context", ".", "shape", "[", "0", "]", "-", "validation_size", "\n", "training_dataset", ",", "validation_dataset", "=", "torch", ".", "utils", ".", "data", ".", "random_split", "(", "\n", "dataset", ",", "[", "training_size", ",", "validation_size", "]", "\n", ")", "\n", "training_data_loader", "=", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "\n", "training_dataset", ",", "\n", "batch_size", "=", "batch_size_", ",", "\n", "shuffle", "=", "self", ".", "shuffle", ",", "\n", ")", "\n", "validation_data_loader", "=", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "\n", "validation_dataset", ",", "\n", "batch_size", "=", "batch_size_", ",", "\n", "shuffle", "=", "self", ".", "shuffle", ",", "\n", ")", "\n", "\n", "return", "training_data_loader", ",", "validation_data_loader", "\n", "\n", "", "data_loader", "=", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "\n", "dataset", ",", "\n", "batch_size", "=", "batch_size_", ",", "\n", "shuffle", "=", "self", ".", "shuffle", ",", "\n", ")", "\n", "\n", "return", "data_loader", ",", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.offline.NNPolicyLearner.fit": [[1035, 1199], ["utils.check_bandit_feedback_inputs", "offline.NNPolicyLearner._create_train_data_for_opl", "tqdm.tqdm.tqdm", "ValueError", "numpy.zeros_like", "offline.NNPolicyLearner.q_func_estimator.fit", "torch.SGD", "torch.SGD", "torch.SGD", "numpy.arange", "offline.NNPolicyLearner.nn_model.train", "numpy.ones_like", "offline.NNPolicyLearner.nn_model.parameters", "torch.Adagrad", "torch.Adagrad", "torch.Adagrad", "torch.Adam.zero_grad", "offline.NNPolicyLearner.nn_model().unsqueeze", "offline.NNPolicyLearner._estimate_policy_gradient", "offline.NNPolicyLearner._estimate_policy_constraint", "loss.backward", "torch.Adam.step", "loss.item", "offline.NNPolicyLearner.nn_model.eval", "offline.NNPolicyLearner.nn_model.parameters", "torch.Adam", "torch.Adam", "torch.Adam", "NotImplementedError", "offline.NNPolicyLearner.mean", "torch.var", "torch.var", "torch.var", "torch.var", "torch.var", "torch.var", "torch.var", "torch.var", "torch.var", "offline.NNPolicyLearner.nn_model().unsqueeze", "offline.NNPolicyLearner._estimate_policy_gradient", "offline.NNPolicyLearner._estimate_policy_constraint", "loss.item", "offline.NNPolicyLearner.nn_model.parameters", "offline.NNPolicyLearner.nn_model", "offline.NNPolicyLearner.mean", "torch.var", "torch.var", "torch.var", "torch.var", "torch.var", "torch.var", "torch.var", "torch.var", "torch.var", "offline.NNPolicyLearner.nn_model"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_bandit_feedback_inputs", "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.offline.NNPolicyLearner._create_train_data_for_opl", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.classification_model.PropensityScoreEstimator.fit", "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.offline.NNPolicyLearner._estimate_policy_gradient", "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.offline.NNPolicyLearner._estimate_policy_constraint", "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.offline.NNPolicyLearner._estimate_policy_gradient", "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.offline.NNPolicyLearner._estimate_policy_constraint"], ["", "def", "fit", "(", "\n", "self", ",", "\n", "context", ":", "np", ".", "ndarray", ",", "\n", "action", ":", "np", ".", "ndarray", ",", "\n", "reward", ":", "np", ".", "ndarray", ",", "\n", "pscore", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "position", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", ")", "->", "None", ":", "\n", "        ", "\"\"\"Fits an offline bandit policy on the given logged bandit data.\n\n        Note\n        ----------\n        Given the training data :math:`\\\\mathcal{D}`, this policy maximizes the following objective function:\n\n        .. math::\n\n            \\\\hat{V}(\\\\pi_\\\\theta; \\\\mathcal{D}) - \\\\alpha \\\\Omega(\\\\theta)\n\n        where :math:`\\\\hat{V}` is an OPE estimator and :math:`\\\\alpha \\\\Omega(\\\\theta)` is a regularization term.\n\n        Parameters\n        -----------\n        context: array-like, shape (n_rounds, dim_context)\n            Context vectors observed for each data, i.e., :math:`x_i`.\n\n        action: array-like, shape (n_rounds,)\n            Actions sampled by the logging/behavior policy for each data in logged bandit data, i.e., :math:`a_i`.\n\n        reward: array-like, shape (n_rounds,)\n            Rewards observed for each data in logged bandit data, i.e., :math:`r_i`.\n\n        pscore: array-like, shape (n_rounds,), default=None\n            Action choice probabilities of the logging/behavior policy (propensity scores), i.e., :math:`\\\\pi_b(a_i|x_i)`.\n\n        position: array-like, shape (n_rounds,), default=None\n            Indices to differentiate positions in a recommendation interface where the actions are presented.\n            If None, a learner assumes that only a single action is chosen for each data.\n            When `len_list` > 1, position has to be set.\n            Currently, this feature is not supported.\n\n        \"\"\"", "\n", "check_bandit_feedback_inputs", "(", "\n", "context", "=", "context", ",", "\n", "action", "=", "action", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore", "=", "pscore", ",", "\n", "position", "=", "position", ",", "\n", ")", "\n", "if", "context", ".", "shape", "[", "1", "]", "!=", "self", ".", "dim_context", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"Expected `context.shape[1] == self.dim_context`, but found it False\"", "\n", ")", "\n", "", "if", "pscore", "is", "None", ":", "\n", "            ", "pscore", "=", "np", ".", "ones_like", "(", "action", ")", "/", "self", ".", "n_actions", "\n", "", "if", "self", ".", "len_list", "==", "1", ":", "\n", "            ", "position", "=", "np", ".", "zeros_like", "(", "action", ",", "dtype", "=", "int", ")", "\n", "\n", "# train q function estimator when it is needed to train NNPolicy", "\n", "", "if", "self", ".", "off_policy_objective", "in", "[", "\"dr\"", ",", "\"dm\"", "]", ":", "\n", "            ", "self", ".", "q_func_estimator", ".", "fit", "(", "\n", "context", "=", "context", ",", "\n", "action", "=", "action", ",", "\n", "reward", "=", "reward", ",", "\n", ")", "\n", "", "if", "self", ".", "solver", "==", "\"sgd\"", ":", "\n", "            ", "optimizer", "=", "optim", ".", "SGD", "(", "\n", "self", ".", "nn_model", ".", "parameters", "(", ")", ",", "\n", "lr", "=", "self", ".", "learning_rate_init", ",", "\n", "momentum", "=", "self", ".", "momentum", ",", "\n", "weight_decay", "=", "self", ".", "alpha", ",", "\n", "nesterov", "=", "self", ".", "nesterovs_momentum", ",", "\n", ")", "\n", "", "elif", "self", ".", "solver", "==", "\"adagrad\"", ":", "\n", "            ", "optimizer", "=", "optim", ".", "Adagrad", "(", "\n", "self", ".", "nn_model", ".", "parameters", "(", ")", ",", "\n", "lr", "=", "self", ".", "learning_rate_init", ",", "\n", "eps", "=", "self", ".", "epsilon", ",", "\n", "weight_decay", "=", "self", ".", "alpha", ",", "\n", ")", "\n", "", "elif", "self", ".", "solver", "==", "\"adam\"", ":", "\n", "            ", "optimizer", "=", "optim", ".", "Adam", "(", "\n", "self", ".", "nn_model", ".", "parameters", "(", ")", ",", "\n", "lr", "=", "self", ".", "learning_rate_init", ",", "\n", "betas", "=", "(", "self", ".", "beta_1", ",", "self", ".", "beta_2", ")", ",", "\n", "eps", "=", "self", ".", "epsilon", ",", "\n", "weight_decay", "=", "self", ".", "alpha", ",", "\n", ")", "\n", "", "else", ":", "\n", "            ", "raise", "NotImplementedError", "(", "\n", "\"`solver` must be one of 'adam', 'adagrad', or 'sgd'\"", "\n", ")", "\n", "\n", "", "training_data_loader", ",", "validation_data_loader", "=", "self", ".", "_create_train_data_for_opl", "(", "\n", "context", ",", "action", ",", "reward", ",", "pscore", ",", "position", "\n", ")", "\n", "\n", "# start policy training", "\n", "n_not_improving_training", "=", "0", "\n", "previous_training_loss", "=", "None", "\n", "n_not_improving_validation", "=", "0", "\n", "previous_validation_loss", "=", "None", "\n", "for", "_", "in", "tqdm", "(", "np", ".", "arange", "(", "self", ".", "max_iter", ")", ",", "desc", "=", "\"policy learning\"", ")", ":", "\n", "            ", "self", ".", "nn_model", ".", "train", "(", ")", "\n", "for", "x", ",", "a", ",", "r", ",", "p", ",", "pos", "in", "training_data_loader", ":", "\n", "                ", "optimizer", ".", "zero_grad", "(", ")", "\n", "pi", "=", "self", ".", "nn_model", "(", "x", ")", ".", "unsqueeze", "(", "-", "1", ")", "\n", "policy_grad_arr", "=", "self", ".", "_estimate_policy_gradient", "(", "\n", "context", "=", "x", ",", "\n", "reward", "=", "r", ",", "\n", "action", "=", "a", ",", "\n", "pscore", "=", "p", ",", "\n", "action_dist", "=", "pi", ",", "\n", "position", "=", "pos", ",", "\n", ")", "\n", "policy_constraint", "=", "self", ".", "_estimate_policy_constraint", "(", "\n", "action", "=", "a", ",", "\n", "pscore", "=", "p", ",", "\n", "action_dist", "=", "pi", ",", "\n", ")", "\n", "loss", "=", "-", "policy_grad_arr", ".", "mean", "(", ")", "\n", "loss", "+=", "self", ".", "policy_reg_param", "*", "policy_constraint", "\n", "loss", "+=", "self", ".", "var_reg_param", "*", "torch", ".", "var", "(", "policy_grad_arr", ")", "\n", "loss", ".", "backward", "(", ")", "\n", "optimizer", ".", "step", "(", ")", "\n", "\n", "loss_value", "=", "loss", ".", "item", "(", ")", "\n", "if", "previous_training_loss", "is", "not", "None", ":", "\n", "                    ", "if", "loss_value", "-", "previous_training_loss", "<", "self", ".", "tol", ":", "\n", "                        ", "n_not_improving_training", "+=", "1", "\n", "", "else", ":", "\n", "                        ", "n_not_improving_training", "=", "0", "\n", "", "", "if", "n_not_improving_training", ">=", "self", ".", "n_iter_no_change", ":", "\n", "                    ", "break", "\n", "", "previous_training_loss", "=", "loss_value", "\n", "\n", "", "if", "self", ".", "early_stopping", ":", "\n", "                ", "self", ".", "nn_model", ".", "eval", "(", ")", "\n", "for", "x", ",", "a", ",", "r", ",", "p", ",", "pos", "in", "validation_data_loader", ":", "\n", "                    ", "pi", "=", "self", ".", "nn_model", "(", "x", ")", ".", "unsqueeze", "(", "-", "1", ")", "\n", "policy_grad_arr", "=", "self", ".", "_estimate_policy_gradient", "(", "\n", "context", "=", "x", ",", "\n", "reward", "=", "r", ",", "\n", "action", "=", "a", ",", "\n", "pscore", "=", "p", ",", "\n", "action_dist", "=", "pi", ",", "\n", "position", "=", "pos", ",", "\n", ")", "\n", "policy_constraint", "=", "self", ".", "_estimate_policy_constraint", "(", "\n", "action", "=", "a", ",", "\n", "pscore", "=", "p", ",", "\n", "action_dist", "=", "pi", ",", "\n", ")", "\n", "loss", "=", "-", "policy_grad_arr", ".", "mean", "(", ")", "\n", "loss", "+=", "self", ".", "policy_reg_param", "*", "policy_constraint", "\n", "loss", "+=", "self", ".", "var_reg_param", "*", "torch", ".", "var", "(", "policy_grad_arr", ")", "\n", "loss_value", "=", "loss", ".", "item", "(", ")", "\n", "if", "previous_validation_loss", "is", "not", "None", ":", "\n", "                        ", "if", "loss_value", "-", "previous_validation_loss", "<", "self", ".", "tol", ":", "\n", "                            ", "n_not_improving_validation", "+=", "1", "\n", "", "else", ":", "\n", "                            ", "n_not_improving_validation", "=", "0", "\n", "", "", "if", "n_not_improving_validation", ">", "self", ".", "n_iter_no_change", ":", "\n", "                        ", "break", "\n", "", "previous_validation_loss", "=", "loss_value", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.offline.NNPolicyLearner._estimate_policy_gradient": [[1200, 1279], ["action_dist[].detach", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "offline.NNPolicyLearner.q_func_estimator.predict", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "offline.NNPolicyLearner.q_func_estimator.predict", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.classification_model.PropensityScoreEstimator.predict", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.classification_model.PropensityScoreEstimator.predict"], ["", "", "", "", "def", "_estimate_policy_gradient", "(", "\n", "self", ",", "\n", "context", ":", "torch", ".", "Tensor", ",", "\n", "action", ":", "torch", ".", "Tensor", ",", "\n", "reward", ":", "torch", ".", "Tensor", ",", "\n", "pscore", ":", "torch", ".", "Tensor", ",", "\n", "action_dist", ":", "torch", ".", "Tensor", ",", "\n", "position", ":", "torch", ".", "Tensor", ",", "\n", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "\"\"\"Estimate the policy gradient.\n\n        Parameters\n        -----------\n        context: array-like, shape (batch_size, dim_context)\n            Context vectors observed for each data, i.e., :math:`x_i`.\n\n        action: array-like, shape (batch_size,)\n            Actions sampled by the logging/behavior policy for each data in logged bandit data, i.e., :math:`a_i`.\n\n        reward: array-like, shape (batch_size,)\n            Rewards observed for each data in logged bandit data, i.e., :math:`r_i`.\n\n        pscore: array-like, shape (batch_size,), default=None\n            Action choice probabilities of the logging/behavior policy (propensity scores), i.e., :math:`\\\\pi_b(a_i|x_i)`.\n\n        action_dist: array-like, shape (batch_size, n_actions, len_list)\n            Action choice probabilities of the evaluation policy (can be deterministic), i.e., :math:`\\\\pi_e(a_i|x_i)`.\n\n        Returns\n        ----------\n        estimated_policy_grad_arr: array-like, shape (batch_size,)\n            Rewards of each data estimated by an OPE estimator.\n\n        \"\"\"", "\n", "current_pi", "=", "action_dist", "[", ":", ",", ":", ",", "0", "]", ".", "detach", "(", ")", "\n", "log_prob", "=", "torch", ".", "log", "(", "action_dist", "[", ":", ",", ":", ",", "0", "]", ")", "\n", "idx_tensor", "=", "torch", ".", "arange", "(", "action", ".", "shape", "[", "0", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "\n", "if", "self", ".", "off_policy_objective", "==", "\"dm\"", ":", "\n", "            ", "q_hat", "=", "self", ".", "q_func_estimator", ".", "predict", "(", "\n", "context", "=", "context", ",", "\n", ")", "\n", "estimated_policy_grad_arr", "=", "torch", ".", "sum", "(", "q_hat", "*", "current_pi", "*", "log_prob", ",", "dim", "=", "1", ")", "\n", "\n", "", "elif", "self", ".", "off_policy_objective", "==", "\"ipw\"", ":", "\n", "            ", "iw", "=", "current_pi", "[", "idx_tensor", ",", "action", "]", "/", "pscore", "\n", "estimated_policy_grad_arr", "=", "iw", "*", "reward", "\n", "estimated_policy_grad_arr", "*=", "log_prob", "[", "idx_tensor", ",", "action", "]", "\n", "\n", "", "elif", "self", ".", "off_policy_objective", "==", "\"dr\"", ":", "\n", "            ", "q_hat", "=", "self", ".", "q_func_estimator", ".", "predict", "(", "\n", "context", "=", "context", ",", "\n", ")", "\n", "q_hat_factual", "=", "q_hat", "[", "idx_tensor", ",", "action", "]", "\n", "iw", "=", "current_pi", "[", "idx_tensor", ",", "action", "]", "/", "pscore", "\n", "estimated_policy_grad_arr", "=", "iw", "*", "(", "reward", "-", "q_hat_factual", ")", "\n", "estimated_policy_grad_arr", "*=", "log_prob", "[", "idx_tensor", ",", "action", "]", "\n", "estimated_policy_grad_arr", "+=", "torch", ".", "sum", "(", "q_hat", "*", "current_pi", "*", "log_prob", ",", "dim", "=", "1", ")", "\n", "\n", "", "elif", "self", ".", "off_policy_objective", "==", "\"snipw\"", ":", "\n", "            ", "iw", "=", "current_pi", "[", "idx_tensor", ",", "action", "]", "/", "pscore", "\n", "estimated_policy_grad_arr", "=", "iw", "*", "(", "reward", "-", "self", ".", "lambda_", ")", "\n", "estimated_policy_grad_arr", "*=", "log_prob", "[", "idx_tensor", ",", "action", "]", "\n", "\n", "", "elif", "self", ".", "off_policy_objective", "==", "\"ipw-os\"", ":", "\n", "            ", "iw", "=", "current_pi", "[", "idx_tensor", ",", "action", "]", "/", "pscore", "\n", "iw_", "=", "(", "self", ".", "lambda_", "-", "(", "iw", "**", "2", ")", ")", "/", "(", "(", "iw", "**", "2", "+", "self", ".", "lambda_", ")", "**", "2", ")", "\n", "iw_", "*=", "self", ".", "lambda_", "*", "iw", "\n", "estimated_policy_grad_arr", "=", "iw_", "*", "reward", "\n", "estimated_policy_grad_arr", "*=", "log_prob", "[", "idx_tensor", ",", "action", "]", "\n", "\n", "", "elif", "self", ".", "off_policy_objective", "==", "\"ipw-subgauss\"", ":", "\n", "            ", "iw", "=", "current_pi", "[", "idx_tensor", ",", "action", "]", "/", "pscore", "\n", "iw_", "=", "(", "1", "-", "self", ".", "lambda_", ")", "*", "iw", "\n", "iw_", "/=", "(", "1", "-", "self", ".", "lambda_", "+", "self", ".", "lambda_", "*", "iw", ")", "**", "2", "\n", "estimated_policy_grad_arr", "=", "iw_", "*", "reward", "\n", "estimated_policy_grad_arr", "*=", "log_prob", "[", "idx_tensor", ",", "action", "]", "\n", "\n", "", "return", "estimated_policy_grad_arr", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.offline.NNPolicyLearner._estimate_policy_constraint": [[1280, 1304], ["torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "iw.mean"], "methods", ["None"], ["", "def", "_estimate_policy_constraint", "(", "\n", "self", ",", "\n", "action", ":", "torch", ".", "Tensor", ",", "\n", "pscore", ":", "torch", ".", "Tensor", ",", "\n", "action_dist", ":", "torch", ".", "Tensor", ",", "\n", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "\"\"\"Estimate the policy constraint term.\n\n        Parameters\n        -----------\n        action: array-like, shape (n_rounds,)\n            Actions sampled by the logging/behavior policy for each data in logged bandit data, i.e., :math:`a_i`.\n\n        pscore: array-like, shape (n_rounds,), default=None\n            Action choice probabilities of the logging/behavior policy (propensity scores), i.e., :math:`\\\\pi_b(a_i|x_i)`.\n\n        action_dist: array-like, shape (n_rounds, n_actions, len_list)\n            Action choice probabilities of the evaluation policy (can be deterministic), i.e., :math:`\\\\pi_e(a_i|x_i)`.\n\n        \"\"\"", "\n", "idx_tensor", "=", "torch", ".", "arange", "(", "action", ".", "shape", "[", "0", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "iw", "=", "action_dist", "[", "idx_tensor", ",", "action", ",", "0", "]", "/", "pscore", "\n", "\n", "return", "torch", ".", "log", "(", "iw", ".", "mean", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.offline.NNPolicyLearner.predict": [[1305, 1340], ["utils.check_array", "offline.NNPolicyLearner.nn_model.eval", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "offline.NNPolicyLearner.nn_model().detach().numpy", "numpy.argmax", "numpy.zeros", "ValueError", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "offline.NNPolicyLearner.nn_model().detach", "numpy.arange", "offline.NNPolicyLearner.nn_model"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array"], ["", "def", "predict", "(", "self", ",", "context", ":", "np", ".", "ndarray", ")", "->", "np", ".", "ndarray", ":", "\n", "        ", "\"\"\"Predict best actions for new data.\n\n        Note\n        --------\n        Action set predicted by this `predict` method can contain duplicate items.\n        If a non-repetitive action set is needed, please use the `sample_action` method.\n\n        Parameters\n        -----------\n        context: array-like, shape (n_rounds_of_new_data, dim_context)\n            Context vectors for new data.\n\n        Returns\n        -----------\n        action_dist: array-like, shape (n_rounds_of_new_data, n_actions, len_list)\n            Action choices made by a classifier, which can contain duplicate items.\n            If a non-repetitive action set is needed, please use the `sample_action` method.\n\n        \"\"\"", "\n", "check_array", "(", "array", "=", "context", ",", "name", "=", "\"context\"", ",", "expected_dim", "=", "2", ")", "\n", "if", "context", ".", "shape", "[", "1", "]", "!=", "self", ".", "dim_context", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"Expected `context.shape[1] == self.dim_context`, but found it False\"", "\n", ")", "\n", "\n", "", "self", ".", "nn_model", ".", "eval", "(", ")", "\n", "x", "=", "torch", ".", "from_numpy", "(", "context", ")", ".", "float", "(", ")", "\n", "y", "=", "self", ".", "nn_model", "(", "x", ")", ".", "detach", "(", ")", ".", "numpy", "(", ")", "\n", "n", "=", "context", ".", "shape", "[", "0", "]", "\n", "predicted_actions", "=", "np", ".", "argmax", "(", "y", ",", "axis", "=", "1", ")", "\n", "action_dist", "=", "np", ".", "zeros", "(", "(", "n", ",", "self", ".", "n_actions", ",", "1", ")", ")", "\n", "action_dist", "[", "np", ".", "arange", "(", "n", ")", ",", "predicted_actions", ",", "0", "]", "=", "1", "\n", "\n", "return", "action_dist", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.offline.NNPolicyLearner.sample_action": [[1341, 1401], ["utils.check_array", "sklearn.utils.check_scalar", "sklearn.utils.check_random_state", "numpy.zeros", "sklearn.utils.check_random_state.gumbel", "numpy.argsort", "numpy.arange", "ValueError", "offline.NNPolicyLearner.predict_proba().mean", "offline.NNPolicyLearner.predict_proba", "numpy.arange"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.offline.NNPolicyLearner.predict_proba"], ["", "def", "sample_action", "(", "\n", "self", ",", "\n", "context", ":", "np", ".", "ndarray", ",", "\n", "tau", ":", "Union", "[", "int", ",", "float", "]", "=", "1.0", ",", "\n", "random_state", ":", "Optional", "[", "int", "]", "=", "None", ",", "\n", ")", "->", "np", ".", "ndarray", ":", "\n", "        ", "\"\"\"Sample a ranking of (non-repetitive) actions from the Plackett-Luce ranking distribution.\n\n        Note\n        --------\n        This `sample_action` method samples a **non-repetitive** ranking of actions for new data\n        :math:`x \\\\in \\\\mathcal{X}` via the so-called \"Gumbel Softmax trick\" as follows.\n\n        .. math::\n\n            \\\\s (x,a) = \\\\hat{f}(x,a) / \\\\tau + \\\\gamma_{x,a}, \\\\quad \\\\gamma_{x,a} \\\\sim \\\\mathrm{Gumbel}(0,1)\n\n        :math:`\\\\tau` is a temperature hyperparameter.\n        :math:`f: \\\\mathcal{X} \\\\times \\\\mathcal{A} \\\\times \\\\mathcal{K} \\\\rightarrow \\\\mathbb{R}_{+}`\n        is a scoring function which is now implemented in the `predict_score` method.\n        When `len_list > 0`,  the expected rewards estimated at different positions will be averaged to form :math:`f(x,a)`.\n        :math:`\\\\gamma_{x,a}` is a random variable sampled from the Gumbel distribution.\n        By sorting the actions based on :math:`\\\\s (x,a)` for each context, we can efficiently sample a ranking from\n        the Plackett-Luce ranking distribution.\n\n        Parameters\n        ----------------\n        context: array-like, shape (n_rounds_of_new_data, dim_context)\n            Context vectors for new data.\n\n        tau: int or float, default=1.0\n            A temperature parameter that controls the randomness of the action choice\n            by scaling the scores before applying softmax.\n            As :math:`\\\\tau \\\\rightarrow \\\\infty`, the algorithm will select arms uniformly at random.\n\n        random_state: int, default=None\n            Controls the random seed in sampling actions.\n\n        Returns\n        -----------\n        sampled_action: array-like, shape (n_rounds_of_new_data, n_actions, len_list)\n            Ranking of actions sampled from the Plackett-Luce ranking distribution via the Gumbel softmax trick.\n\n        \"\"\"", "\n", "check_array", "(", "array", "=", "context", ",", "name", "=", "\"context\"", ",", "expected_dim", "=", "2", ")", "\n", "check_scalar", "(", "tau", ",", "name", "=", "\"tau\"", ",", "target_type", "=", "(", "int", ",", "float", ")", ",", "min_val", "=", "0", ")", "\n", "if", "context", ".", "shape", "[", "1", "]", "!=", "self", ".", "dim_context", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"Expected `context.shape[1] == self.dim_context`, but found it False\"", "\n", ")", "\n", "\n", "", "n", "=", "context", ".", "shape", "[", "0", "]", "\n", "random_", "=", "check_random_state", "(", "random_state", ")", "\n", "sampled_action", "=", "np", ".", "zeros", "(", "(", "n", ",", "self", ".", "n_actions", ",", "self", ".", "len_list", ")", ")", "\n", "scores", "=", "self", ".", "predict_proba", "(", "context", "=", "context", ")", ".", "mean", "(", "2", ")", "/", "tau", "\n", "scores", "+=", "random_", ".", "gumbel", "(", "size", "=", "scores", ".", "shape", ")", "\n", "ranking", "=", "np", ".", "argsort", "(", "-", "scores", ",", "axis", "=", "1", ")", "\n", "for", "p", "in", "np", ".", "arange", "(", "self", ".", "len_list", ")", ":", "\n", "            ", "sampled_action", "[", "np", ".", "arange", "(", "n", ")", ",", "ranking", "[", ":", ",", "p", "]", ",", "p", "]", "=", "1", "\n", "", "return", "sampled_action", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.offline.NNPolicyLearner.predict_proba": [[1402, 1440], ["utils.check_array", "offline.NNPolicyLearner.nn_model.eval", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "offline.NNPolicyLearner.nn_model().detach().numpy", "ValueError", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "offline.NNPolicyLearner.nn_model().detach", "offline.NNPolicyLearner.nn_model"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array"], ["", "def", "predict_proba", "(", "\n", "self", ",", "\n", "context", ":", "np", ".", "ndarray", ",", "\n", ")", "->", "np", ".", "ndarray", ":", "\n", "        ", "\"\"\"Obtains action choice probabilities for new data.\n\n        Note\n        --------\n        This policy uses multi-layer perceptron (MLP) and the softmax function as the last layer.\n        This is a stochastic policy and represented as follows:\n\n        .. math::\n\n            \\\\pi_\\\\theta (a \\\\mid x) = \\\\frac{\\\\exp(f_\\\\theta(x, a))}{\\\\sum_{a' \\\\in \\\\mathcal{A}} \\\\exp(f_\\\\theta(x, a'))}\n\n        where :math:`f__\\\\theta(x, a)` is MLP with parameter :math:`\\\\theta`.\n\n        Parameters\n        ----------------\n        context: array-like, shape (n_rounds_of_new_data, dim_context)\n            Context vectors for new data.\n\n        Returns\n        -----------\n        choice_prob: array-like, shape (n_rounds_of_new_data, n_actions, len_list)\n            Action choice probabilities obtained by a trained classifier.\n\n        \"\"\"", "\n", "check_array", "(", "array", "=", "context", ",", "name", "=", "\"context\"", ",", "expected_dim", "=", "2", ")", "\n", "if", "context", ".", "shape", "[", "1", "]", "!=", "self", ".", "dim_context", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"Expected `context.shape[1] == self.dim_context`, but found it False\"", "\n", ")", "\n", "\n", "", "self", ".", "nn_model", ".", "eval", "(", ")", "\n", "x", "=", "torch", ".", "from_numpy", "(", "context", ")", ".", "float", "(", ")", "\n", "y", "=", "self", ".", "nn_model", "(", "x", ")", ".", "detach", "(", ")", ".", "numpy", "(", ")", "\n", "return", "y", "[", ":", ",", ":", ",", "np", ".", "newaxis", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.offline.QFuncEstimator.__post_init__": [[1565, 1660], ["sklearn.utils.check_scalar", "sklearn.utils.check_scalar", "sklearn.utils.check_scalar", "sklearn.utils.check_scalar", "sklearn.utils.check_scalar", "sklearn.utils.check_scalar", "sklearn.utils.check_scalar", "sklearn.utils.check_scalar", "sklearn.utils.check_scalar", "sklearn.utils.check_scalar", "sklearn.utils.check_scalar", "enumerate", "layer_list.append", "torch.Sequential", "torch.Sequential", "torch.Sequential", "any", "ValueError", "ValueError", "ValueError", "ValueError", "isinstance", "ValueError", "ValueError", "isinstance", "ValueError", "isinstance", "ValueError", "ValueError", "sklearn.utils.check_random_state", "torch.manual_seed", "torch.manual_seed", "torch.manual_seed", "torch.manual_seed", "torch.manual_seed", "torch.manual_seed", "torch.manual_seed", "torch.manual_seed", "torch.manual_seed", "layer_list.append", "layer_list.append", "collections.OrderedDict", "isinstance", "torch.Linear", "torch.Linear", "torch.Linear", "isinstance", "torch.Linear", "torch.Linear", "torch.Linear", "activation_layer", "isinstance", "ValueError"], "methods", ["None"], ["def", "__post_init__", "(", "self", ")", "->", "None", ":", "\n", "        ", "\"\"\"Initialize class.\"\"\"", "\n", "check_scalar", "(", "self", ".", "dim_context", ",", "\"dim_context\"", ",", "int", ",", "min_val", "=", "1", ")", "\n", "\n", "if", "not", "isinstance", "(", "self", ".", "hidden_layer_size", ",", "tuple", ")", "or", "any", "(", "\n", "[", "not", "isinstance", "(", "h", ",", "int", ")", "or", "h", "<=", "0", "for", "h", "in", "self", ".", "hidden_layer_size", "]", "\n", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "f\"`hidden_layer_size` must be a tuple of positive integers, but {self.hidden_layer_size} is given\"", "\n", ")", "\n", "\n", "", "if", "self", ".", "solver", "not", "in", "(", "\"adagrad\"", ",", "\"sgd\"", ",", "\"adam\"", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "f\"`solver` must be one of 'adam', 'adagrad', or 'sgd', but {self.solver} is given\"", "\n", ")", "\n", "\n", "", "check_scalar", "(", "self", ".", "alpha", ",", "\"alpha\"", ",", "float", ",", "min_val", "=", "0.0", ")", "\n", "\n", "if", "self", ".", "batch_size", "!=", "\"auto\"", "and", "(", "\n", "not", "isinstance", "(", "self", ".", "batch_size", ",", "int", ")", "or", "self", ".", "batch_size", "<=", "0", "\n", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "f\"`batch_size` must be a positive integer or 'auto', but {self.batch_size} is given\"", "\n", ")", "\n", "\n", "", "check_scalar", "(", "self", ".", "learning_rate_init", ",", "\"learning_rate_init\"", ",", "float", ")", "\n", "if", "self", ".", "learning_rate_init", "<=", "0.0", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "f\"`learning_rate_init`= {self.learning_rate_init}, must be > 0.0\"", "\n", ")", "\n", "\n", "", "check_scalar", "(", "self", ".", "max_iter", ",", "\"max_iter\"", ",", "int", ",", "min_val", "=", "1", ")", "\n", "\n", "if", "not", "isinstance", "(", "self", ".", "shuffle", ",", "bool", ")", ":", "\n", "            ", "raise", "ValueError", "(", "f\"`shuffle` must be a bool, but {self.shuffle} is given\"", ")", "\n", "\n", "", "check_scalar", "(", "self", ".", "tol", ",", "\"tol\"", ",", "float", ")", "\n", "if", "self", ".", "tol", "<=", "0.0", ":", "\n", "            ", "raise", "ValueError", "(", "f\"`tol`= {self.tol}, must be > 0.0\"", ")", "\n", "\n", "", "check_scalar", "(", "self", ".", "momentum", ",", "\"momentum\"", ",", "float", ",", "min_val", "=", "0.0", ",", "max_val", "=", "1.0", ")", "\n", "\n", "if", "not", "isinstance", "(", "self", ".", "nesterovs_momentum", ",", "bool", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "f\"`nesterovs_momentum` must be a bool, but {self.nesterovs_momentum} is given\"", "\n", ")", "\n", "\n", "", "if", "not", "isinstance", "(", "self", ".", "early_stopping", ",", "bool", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "f\"`early_stopping` must be a bool, but {self.early_stopping} is given\"", "\n", ")", "\n", "\n", "", "check_scalar", "(", "\n", "self", ".", "validation_fraction", ",", "\"validation_fraction\"", ",", "float", ",", "max_val", "=", "1.0", "\n", ")", "\n", "if", "self", ".", "validation_fraction", "<=", "0.0", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "f\"`validation_fraction`= {self.validation_fraction}, must be > 0.0\"", "\n", ")", "\n", "\n", "", "check_scalar", "(", "self", ".", "beta_1", ",", "\"beta_1\"", ",", "float", ",", "min_val", "=", "0.0", ",", "max_val", "=", "1.0", ")", "\n", "check_scalar", "(", "self", ".", "beta_2", ",", "\"beta_2\"", ",", "float", ",", "min_val", "=", "0.0", ",", "max_val", "=", "1.0", ")", "\n", "check_scalar", "(", "self", ".", "epsilon", ",", "\"epsilon\"", ",", "float", ",", "min_val", "=", "0.0", ")", "\n", "check_scalar", "(", "self", ".", "n_iter_no_change", ",", "\"n_iter_no_change\"", ",", "int", ",", "min_val", "=", "1", ")", "\n", "\n", "if", "self", ".", "random_state", "is", "not", "None", ":", "\n", "            ", "self", ".", "random_", "=", "check_random_state", "(", "self", ".", "random_state", ")", "\n", "torch", ".", "manual_seed", "(", "self", ".", "random_state", ")", "\n", "\n", "", "if", "self", ".", "activation", "==", "\"identity\"", ":", "\n", "            ", "activation_layer", "=", "nn", ".", "Identity", "\n", "", "elif", "self", ".", "activation", "==", "\"logistic\"", ":", "\n", "            ", "activation_layer", "=", "nn", ".", "Sigmoid", "\n", "", "elif", "self", ".", "activation", "==", "\"tanh\"", ":", "\n", "            ", "activation_layer", "=", "nn", ".", "Tanh", "\n", "", "elif", "self", ".", "activation", "==", "\"relu\"", ":", "\n", "            ", "activation_layer", "=", "nn", ".", "ReLU", "\n", "", "elif", "self", ".", "activation", "==", "\"elu\"", ":", "\n", "            ", "activation_layer", "=", "nn", ".", "ELU", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"`activation` must be one of 'identity', 'logistic', 'tanh', 'relu', or 'elu'\"", "\n", "f\", but {self.activation} is given\"", "\n", ")", "\n", "\n", "", "layer_list", "=", "[", "]", "\n", "input_size", "=", "self", ".", "dim_context", "\n", "\n", "for", "i", ",", "h", "in", "enumerate", "(", "self", ".", "hidden_layer_size", ")", ":", "\n", "            ", "layer_list", ".", "append", "(", "(", "\"l{}\"", ".", "format", "(", "i", ")", ",", "nn", ".", "Linear", "(", "input_size", ",", "h", ")", ")", ")", "\n", "layer_list", ".", "append", "(", "(", "\"a{}\"", ".", "format", "(", "i", ")", ",", "activation_layer", "(", ")", ")", ")", "\n", "input_size", "=", "h", "\n", "", "layer_list", ".", "append", "(", "(", "\"output\"", ",", "nn", ".", "Linear", "(", "input_size", ",", "self", ".", "n_actions", ")", ")", ")", "\n", "\n", "self", ".", "nn_model", "=", "nn", ".", "Sequential", "(", "OrderedDict", "(", "layer_list", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.offline.QFuncEstimator._create_train_data_for_q_func_estimation": [[1661, 1730], ["offline.QFuncEstimatorDataset", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "min", "sklearn.utils.check_scalar", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().long", "torch.from_numpy().long", "torch.from_numpy().long", "torch.from_numpy().long", "torch.from_numpy().long", "torch.from_numpy().long", "torch.from_numpy().long", "torch.from_numpy().long", "torch.from_numpy().long", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "max", "torch.utils.data.random_split", "torch.utils.data.random_split", "torch.utils.data.random_split", "torch.utils.data.random_split", "torch.utils.data.random_split", "torch.utils.data.random_split", "torch.utils.data.random_split", "torch.utils.data.random_split", "torch.utils.data.random_split", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "ValueError", "int", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy"], "methods", ["None"], ["", "def", "_create_train_data_for_q_func_estimation", "(", "\n", "self", ",", "\n", "context", ":", "np", ".", "ndarray", ",", "\n", "action", ":", "np", ".", "ndarray", ",", "\n", "reward", ":", "np", ".", "ndarray", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "Tuple", "[", "torch", ".", "utils", ".", "data", ".", "DataLoader", ",", "Optional", "[", "torch", ".", "utils", ".", "data", ".", "DataLoader", "]", "]", ":", "\n", "        ", "\"\"\"Create training data for off-policy learning.\n\n        Parameters\n        -----------\n        context: array-like, shape (n_rounds, dim_context)\n            Context vectors observed for each data, i.e., :math:`x_i`.\n\n        action: array-like, shape (n_rounds,)\n            Actions sampled by the logging/behavior policy for each data in logged bandit data, i.e., :math:`a_i`.\n\n        reward: array-like, shape (n_rounds,)\n            Rewards observed for each data in logged bandit data, i.e., :math:`r_i`.\n\n        Returns\n        --------\n        (training_data_loader, validation_data_loader): Tuple[DataLoader, Optional[DataLoader]]\n            Training and validation data loaders in PyTorch\n\n        \"\"\"", "\n", "if", "self", ".", "batch_size", "==", "\"auto\"", ":", "\n", "            ", "batch_size_", "=", "min", "(", "200", ",", "context", ".", "shape", "[", "0", "]", ")", "\n", "", "else", ":", "\n", "            ", "check_scalar", "(", "self", ".", "batch_size", ",", "\"batch_size\"", ",", "int", ",", "min_val", "=", "1", ")", "\n", "batch_size_", "=", "self", ".", "batch_size", "\n", "\n", "", "dataset", "=", "QFuncEstimatorDataset", "(", "\n", "torch", ".", "from_numpy", "(", "context", ")", ".", "float", "(", ")", ",", "\n", "torch", ".", "from_numpy", "(", "action", ")", ".", "long", "(", ")", ",", "\n", "torch", ".", "from_numpy", "(", "reward", ")", ".", "float", "(", ")", ",", "\n", ")", "\n", "\n", "if", "self", ".", "early_stopping", ":", "\n", "            ", "if", "context", ".", "shape", "[", "0", "]", "<=", "1", ":", "\n", "                ", "raise", "ValueError", "(", "\n", "f\"the number of samples is too small ({context.shape[0]}) to create validation data\"", "\n", ")", "\n", "\n", "", "validation_size", "=", "max", "(", "int", "(", "context", ".", "shape", "[", "0", "]", "*", "self", ".", "validation_fraction", ")", ",", "1", ")", "\n", "training_size", "=", "context", ".", "shape", "[", "0", "]", "-", "validation_size", "\n", "training_dataset", ",", "validation_dataset", "=", "torch", ".", "utils", ".", "data", ".", "random_split", "(", "\n", "dataset", ",", "[", "training_size", ",", "validation_size", "]", "\n", ")", "\n", "training_data_loader", "=", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "\n", "training_dataset", ",", "\n", "batch_size", "=", "batch_size_", ",", "\n", "shuffle", "=", "self", ".", "shuffle", ",", "\n", ")", "\n", "validation_data_loader", "=", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "\n", "validation_dataset", ",", "\n", "batch_size", "=", "batch_size_", ",", "\n", "shuffle", "=", "self", ".", "shuffle", ",", "\n", ")", "\n", "\n", "return", "training_data_loader", ",", "validation_data_loader", "\n", "\n", "", "data_loader", "=", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "\n", "dataset", ",", "\n", "batch_size", "=", "batch_size_", ",", "\n", "shuffle", "=", "self", ".", "shuffle", ",", "\n", ")", "\n", "\n", "return", "data_loader", ",", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.offline.QFuncEstimator.fit": [[1731, 1838], ["utils.check_bandit_feedback_inputs", "offline.QFuncEstimator._create_train_data_for_q_func_estimation", "tqdm.tqdm.tqdm", "ValueError", "torch.SGD", "torch.SGD", "torch.SGD", "numpy.arange", "offline.QFuncEstimator.nn_model.train", "offline.QFuncEstimator.nn_model.parameters", "torch.Adagrad", "torch.Adagrad", "torch.Adagrad", "torch.Adam.zero_grad", "torch.nn.functional.mse_loss", "torch.nn.functional.mse_loss", "torch.nn.functional.mse_loss", "torch.nn.functional.mse_loss.backward", "torch.nn.functional.mse_loss.backward", "torch.nn.functional.mse_loss.backward", "torch.Adam.step", "torch.nn.functional.mse_loss.item", "torch.nn.functional.mse_loss.item", "torch.nn.functional.mse_loss.item", "offline.QFuncEstimator.nn_model.eval", "offline.QFuncEstimator.nn_model.parameters", "torch.Adam", "torch.Adam", "torch.Adam", "NotImplementedError", "offline.QFuncEstimator.nn_model", "torch.nn.functional.mse_loss", "torch.nn.functional.mse_loss", "torch.nn.functional.mse_loss", "torch.nn.functional.mse_loss.item", "torch.nn.functional.mse_loss.item", "torch.nn.functional.mse_loss.item", "offline.QFuncEstimator.nn_model.parameters", "offline.QFuncEstimator.nn_model", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_bandit_feedback_inputs", "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.offline.QFuncEstimator._create_train_data_for_q_func_estimation"], ["", "def", "fit", "(", "\n", "self", ",", "\n", "context", ":", "np", ".", "ndarray", ",", "\n", "action", ":", "np", ".", "ndarray", ",", "\n", "reward", ":", "np", ".", "ndarray", ",", "\n", ")", "->", "None", ":", "\n", "        ", "\"\"\"Fits an offline bandit policy on the given logged bandit data.\n\n        Parameters\n        -----------\n        context: array-like, shape (n_rounds, dim_context)\n            Context vectors observed for each data, i.e., :math:`x_i`.\n\n        action: array-like, shape (n_rounds,)\n            Actions sampled by the logging/behavior policy for each data in logged bandit data, i.e., :math:`a_i`.\n\n        reward: array-like, shape (n_rounds,)\n            Rewards observed for each data in logged bandit data, i.e., :math:`r_i`.\n\n        \"\"\"", "\n", "check_bandit_feedback_inputs", "(", "\n", "context", "=", "context", ",", "\n", "action", "=", "action", ",", "\n", "reward", "=", "reward", ",", "\n", ")", "\n", "\n", "if", "context", ".", "shape", "[", "1", "]", "!=", "self", ".", "dim_context", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"Expected `context.shape[1] == self.dim_context`, but found it False\"", "\n", ")", "\n", "\n", "", "if", "self", ".", "solver", "==", "\"sgd\"", ":", "\n", "            ", "optimizer", "=", "optim", ".", "SGD", "(", "\n", "self", ".", "nn_model", ".", "parameters", "(", ")", ",", "\n", "lr", "=", "self", ".", "learning_rate_init", ",", "\n", "momentum", "=", "self", ".", "momentum", ",", "\n", "weight_decay", "=", "self", ".", "alpha", ",", "\n", "nesterov", "=", "self", ".", "nesterovs_momentum", ",", "\n", ")", "\n", "", "elif", "self", ".", "solver", "==", "\"adagrad\"", ":", "\n", "            ", "optimizer", "=", "optim", ".", "Adagrad", "(", "\n", "self", ".", "nn_model", ".", "parameters", "(", ")", ",", "\n", "lr", "=", "self", ".", "learning_rate_init", ",", "\n", "eps", "=", "self", ".", "epsilon", ",", "\n", "weight_decay", "=", "self", ".", "alpha", ",", "\n", ")", "\n", "", "elif", "self", ".", "solver", "==", "\"adam\"", ":", "\n", "            ", "optimizer", "=", "optim", ".", "Adam", "(", "\n", "self", ".", "nn_model", ".", "parameters", "(", ")", ",", "\n", "lr", "=", "self", ".", "learning_rate_init", ",", "\n", "betas", "=", "(", "self", ".", "beta_1", ",", "self", ".", "beta_2", ")", ",", "\n", "eps", "=", "self", ".", "epsilon", ",", "\n", "weight_decay", "=", "self", ".", "alpha", ",", "\n", ")", "\n", "", "else", ":", "\n", "            ", "raise", "NotImplementedError", "(", "\n", "\"`solver` must be one of 'adam', 'adagrad', or 'sgd'\"", "\n", ")", "\n", "\n", "", "(", "\n", "training_data_loader", ",", "\n", "validation_data_loader", ",", "\n", ")", "=", "self", ".", "_create_train_data_for_q_func_estimation", "(", "\n", "context", ",", "\n", "action", ",", "\n", "reward", ",", "\n", ")", "\n", "\n", "n_not_improving_training", "=", "0", "\n", "previous_training_loss", "=", "None", "\n", "n_not_improving_validation", "=", "0", "\n", "previous_validation_loss", "=", "None", "\n", "for", "_", "in", "tqdm", "(", "np", ".", "arange", "(", "self", ".", "max_iter", ")", ",", "desc", "=", "\"q-func learning\"", ")", ":", "\n", "            ", "self", ".", "nn_model", ".", "train", "(", ")", "\n", "for", "x", ",", "a", ",", "r", "in", "training_data_loader", ":", "\n", "                ", "optimizer", ".", "zero_grad", "(", ")", "\n", "q_hat", "=", "self", ".", "nn_model", "(", "x", ")", "[", "torch", ".", "arange", "(", "a", ".", "shape", "[", "0", "]", ",", "dtype", "=", "torch", ".", "long", ")", ",", "a", "]", "\n", "loss", "=", "mse_loss", "(", "r", ",", "q_hat", ")", "\n", "loss", ".", "backward", "(", ")", "\n", "optimizer", ".", "step", "(", ")", "\n", "\n", "loss_value", "=", "loss", ".", "item", "(", ")", "\n", "if", "previous_training_loss", "is", "not", "None", ":", "\n", "                    ", "if", "loss_value", "-", "previous_training_loss", "<", "self", ".", "tol", ":", "\n", "                        ", "n_not_improving_training", "+=", "1", "\n", "", "else", ":", "\n", "                        ", "n_not_improving_training", "=", "0", "\n", "", "", "if", "n_not_improving_training", ">=", "self", ".", "n_iter_no_change", ":", "\n", "                    ", "break", "\n", "", "previous_training_loss", "=", "loss_value", "\n", "\n", "", "if", "self", ".", "early_stopping", ":", "\n", "                ", "self", ".", "nn_model", ".", "eval", "(", ")", "\n", "for", "x", ",", "a", ",", "r", "in", "validation_data_loader", ":", "\n", "                    ", "q_hat", "=", "self", ".", "nn_model", "(", "x", ")", "[", "\n", "torch", ".", "arange", "(", "a", ".", "shape", "[", "0", "]", ",", "dtype", "=", "torch", ".", "long", ")", ",", "a", "\n", "]", "\n", "loss", "=", "mse_loss", "(", "r", ",", "q_hat", ")", "\n", "loss_value", "=", "loss", ".", "item", "(", ")", "\n", "if", "previous_validation_loss", "is", "not", "None", ":", "\n", "                        ", "if", "loss_value", "-", "previous_validation_loss", "<", "self", ".", "tol", ":", "\n", "                            ", "n_not_improving_validation", "+=", "1", "\n", "", "else", ":", "\n", "                            ", "n_not_improving_validation", "=", "0", "\n", "", "", "if", "n_not_improving_validation", ">", "self", ".", "n_iter_no_change", ":", "\n", "                        ", "break", "\n", "", "previous_validation_loss", "=", "loss_value", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.offline.QFuncEstimator.predict": [[1839, 1864], ["utils.check_tensor", "offline.QFuncEstimator.nn_model.eval", "offline.QFuncEstimator.nn_model", "ValueError"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_tensor"], ["", "", "", "", "def", "predict", "(", "\n", "self", ",", "\n", "context", ":", "torch", ".", "Tensor", ",", "\n", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "\"\"\"Predict best continuous actions for new data.\n\n        Parameters\n        -----------\n        context: Tensor, shape (n_rounds_of_new_data, dim_context)\n            Context vectors for new data.\n\n        Returns\n        -----------\n        estimated_expected_rewards: Tensor, shape (n_rounds_of_new_data,)\n            Expected rewards given context and action for new data estimated by the regression model.\n\n        \"\"\"", "\n", "check_tensor", "(", "tensor", "=", "context", ",", "name", "=", "\"context\"", ",", "expected_dim", "=", "2", ")", "\n", "if", "context", ".", "shape", "[", "1", "]", "!=", "self", ".", "dim_context", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"Expected `context.shape[1] == self.dim_context`, but found it False\"", "\n", ")", "\n", "\n", "", "self", ".", "nn_model", ".", "eval", "(", ")", "\n", "return", "self", ".", "nn_model", "(", "context", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.offline.NNPolicyDataset.__post_init__": [[1876, 1884], ["None"], "methods", ["None"], ["def", "__post_init__", "(", "self", ")", ":", "\n", "        ", "\"\"\"initialize class\"\"\"", "\n", "assert", "(", "\n", "self", ".", "context", ".", "shape", "[", "0", "]", "\n", "==", "self", ".", "action", ".", "shape", "[", "0", "]", "\n", "==", "self", ".", "reward", ".", "shape", "[", "0", "]", "\n", "==", "self", ".", "pscore", ".", "shape", "[", "0", "]", "\n", "==", "self", ".", "position", ".", "shape", "[", "0", "]", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.offline.NNPolicyDataset.__getitem__": [[1886, 1893], ["None"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "index", ")", ":", "\n", "        ", "return", "(", "\n", "self", ".", "context", "[", "index", "]", ",", "\n", "self", ".", "action", "[", "index", "]", ",", "\n", "self", ".", "reward", "[", "index", "]", ",", "\n", "self", ".", "pscore", "[", "index", "]", ",", "\n", "self", ".", "position", "[", "index", "]", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.offline.NNPolicyDataset.__len__": [[1895, 1897], ["None"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "context", ".", "shape", "[", "0", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.offline.QFuncEstimatorDataset.__post_init__": [[1907, 1910], ["None"], "methods", ["None"], ["def", "__post_init__", "(", "self", ")", ":", "\n", "        ", "\"\"\"initialize class\"\"\"", "\n", "assert", "self", ".", "feature", ".", "shape", "[", "0", "]", "==", "self", ".", "action", ".", "shape", "[", "0", "]", "==", "self", ".", "reward", ".", "shape", "[", "0", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.offline.QFuncEstimatorDataset.__getitem__": [[1911, 1916], ["None"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "index", ")", ":", "\n", "        ", "return", "(", "\n", "self", ".", "feature", "[", "index", "]", ",", "\n", "self", ".", "action", "[", "index", "]", ",", "\n", "self", ".", "reward", "[", "index", "]", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.offline.QFuncEstimatorDataset.__len__": [[1918, 1920], ["None"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "feature", ".", "shape", "[", "0", "]", "\n", "", "", ""]], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.policy_type.PolicyType.__repr__": [[21, 24], ["str"], "methods", ["None"], ["def", "__repr__", "(", "self", ")", "->", "str", ":", "\n", "\n", "        ", "return", "str", "(", "self", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.linear.BaseLinPolicy.__post_init__": [[41, 54], ["super().__post_init__", "numpy.zeros", "numpy.concatenate().reshape", "numpy.zeros", "numpy.concatenate().reshape", "numpy.zeros", "numpy.concatenate", "numpy.concatenate", "numpy.identity", "numpy.identity", "numpy.arange", "numpy.arange"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.__post_init__"], ["def", "__post_init__", "(", "self", ")", "->", "None", ":", "\n", "        ", "\"\"\"Initialize class.\"\"\"", "\n", "super", "(", ")", ".", "__post_init__", "(", ")", "\n", "self", ".", "theta_hat", "=", "np", ".", "zeros", "(", "(", "self", ".", "dim", ",", "self", ".", "n_actions", ")", ")", "\n", "self", ".", "A_inv", "=", "np", ".", "concatenate", "(", "\n", "[", "np", ".", "identity", "(", "self", ".", "dim", ")", "for", "_", "in", "np", ".", "arange", "(", "self", ".", "n_actions", ")", "]", "\n", ")", ".", "reshape", "(", "self", ".", "n_actions", ",", "self", ".", "dim", ",", "self", ".", "dim", ")", "\n", "self", ".", "b", "=", "np", ".", "zeros", "(", "(", "self", ".", "dim", ",", "self", ".", "n_actions", ")", ")", "\n", "\n", "self", ".", "A_inv_temp", "=", "np", ".", "concatenate", "(", "\n", "[", "np", ".", "identity", "(", "self", ".", "dim", ")", "for", "_", "in", "np", ".", "arange", "(", "self", ".", "n_actions", ")", "]", "\n", ")", ".", "reshape", "(", "self", ".", "n_actions", ",", "self", ".", "dim", ",", "self", ".", "dim", ")", "\n", "self", ".", "b_temp", "=", "np", ".", "zeros", "(", "(", "self", ".", "dim", ",", "self", ".", "n_actions", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.linear.BaseLinPolicy.update_params": [[55, 85], ["context.flatten", "numpy.copy", "numpy.copy"], "methods", ["None"], ["", "def", "update_params", "(", "self", ",", "action", ":", "int", ",", "reward", ":", "float", ",", "context", ":", "np", ".", "ndarray", ")", "->", "None", ":", "\n", "        ", "\"\"\"Update policy parameters.\n\n        Parameters\n        ------------\n        action: int\n            Selected action by the policy.\n\n        reward: float\n            Observed reward for the chosen action and position.\n\n        context: array-like, shape (1, dim_context)\n            Observed context vector.\n\n        \"\"\"", "\n", "self", ".", "n_trial", "+=", "1", "\n", "self", ".", "action_counts", "[", "action", "]", "+=", "1", "\n", "# update the inverse matrix by the Woodbury formula", "\n", "self", ".", "A_inv_temp", "[", "action", "]", "-=", "(", "\n", "self", ".", "A_inv_temp", "[", "action", "]", "\n", "@", "context", ".", "T", "\n", "@", "context", "\n", "@", "self", ".", "A_inv_temp", "[", "action", "]", "\n", "/", "(", "1", "+", "context", "@", "self", ".", "A_inv_temp", "[", "action", "]", "@", "context", ".", "T", ")", "[", "0", "]", "[", "0", "]", "\n", ")", "\n", "self", ".", "b_temp", "[", ":", ",", "action", "]", "+=", "reward", "*", "context", ".", "flatten", "(", ")", "\n", "if", "self", ".", "n_trial", "%", "self", ".", "batch_size", "==", "0", ":", "\n", "            ", "self", ".", "A_inv", ",", "self", ".", "b", "=", "(", "\n", "np", ".", "copy", "(", "self", ".", "A_inv_temp", ")", ",", "\n", "np", ".", "copy", "(", "self", ".", "b_temp", ")", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.linear.LinEpsilonGreedy.__post_init__": [[126, 132], ["sklearn.utils.check_scalar", "linear.BaseLinPolicy.__post_init__"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.__post_init__"], ["def", "__post_init__", "(", "self", ")", "->", "None", ":", "\n", "        ", "\"\"\"Initialize class.\"\"\"", "\n", "check_scalar", "(", "self", ".", "epsilon", ",", "\"epsilon\"", ",", "float", ",", "min_val", "=", "0.0", ",", "max_val", "=", "1.0", ")", "\n", "self", ".", "policy_name", "=", "f\"linear_epsilon_greedy_{self.epsilon}\"", "\n", "\n", "super", "(", ")", ".", "__post_init__", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.linear.LinEpsilonGreedy.select_action": [[133, 164], ["utils.check_array", "ValueError", "linear.LinEpsilonGreedy.random_.rand", "numpy.concatenate", "linear.LinEpsilonGreedy.random_.choice", "predicted_rewards.argsort", "numpy.arange"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array"], ["", "def", "select_action", "(", "self", ",", "context", ":", "np", ".", "ndarray", ")", "->", "np", ".", "ndarray", ":", "\n", "        ", "\"\"\"Select action for new data.\n\n        Parameters\n        ------------\n        context: array-like, shape (1, dim_context)\n            Observed context vector.\n\n        Returns\n        ----------\n        selected_actions: array-like, shape (len_list, )\n            List of selected actions.\n\n        \"\"\"", "\n", "check_array", "(", "array", "=", "context", ",", "name", "=", "\"context\"", ",", "expected_dim", "=", "2", ")", "\n", "if", "context", ".", "shape", "[", "0", "]", "!=", "1", ":", "\n", "            ", "raise", "ValueError", "(", "\"Expected `context.shape[0] == 1`, but found it False\"", ")", "\n", "\n", "", "if", "self", ".", "random_", ".", "rand", "(", ")", ">", "self", ".", "epsilon", ":", "\n", "            ", "self", ".", "theta_hat", "=", "np", ".", "concatenate", "(", "\n", "[", "\n", "self", ".", "A_inv", "[", "i", "]", "@", "self", ".", "b", "[", ":", ",", "i", "]", "[", ":", ",", "np", ".", "newaxis", "]", "\n", "for", "i", "in", "np", ".", "arange", "(", "self", ".", "n_actions", ")", "\n", "]", ",", "\n", "axis", "=", "1", ",", "\n", ")", "# dim * n_actions", "\n", "predicted_rewards", "=", "(", "context", "@", "self", ".", "theta_hat", ")", ".", "flatten", "(", ")", "\n", "return", "predicted_rewards", ".", "argsort", "(", ")", "[", ":", ":", "-", "1", "]", "[", ":", "self", ".", "len_list", "]", "\n", "", "else", ":", "\n", "            ", "return", "self", ".", "random_", ".", "choice", "(", "\n", "self", ".", "n_actions", ",", "size", "=", "self", ".", "len_list", ",", "replace", "=", "False", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.linear.LinUCB.__post_init__": [[202, 208], ["sklearn.utils.check_scalar", "linear.BaseLinPolicy.__post_init__"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.__post_init__"], ["def", "__post_init__", "(", "self", ")", "->", "None", ":", "\n", "        ", "\"\"\"Initialize class.\"\"\"", "\n", "check_scalar", "(", "self", ".", "epsilon", ",", "\"epsilon\"", ",", "float", ",", "min_val", "=", "0.0", ")", "\n", "self", ".", "policy_name", "=", "f\"linear_ucb_{self.epsilon}\"", "\n", "\n", "super", "(", ")", ".", "__post_init__", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.linear.LinUCB.select_action": [[209, 243], ["utils.check_array", "numpy.concatenate", "numpy.concatenate", "ValueError", "numpy.sqrt", "ucb_scores.argsort", "numpy.arange", "numpy.arange"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array"], ["", "def", "select_action", "(", "self", ",", "context", ":", "np", ".", "ndarray", ")", "->", "np", ".", "ndarray", ":", "\n", "        ", "\"\"\"Select action for new data.\n\n        Parameters\n        ----------\n        context: array\n            Observed context vector.\n\n        Returns\n        ----------\n        selected_actions: array-like, shape (len_list, )\n            List of selected actions.\n\n        \"\"\"", "\n", "check_array", "(", "array", "=", "context", ",", "name", "=", "\"context\"", ",", "expected_dim", "=", "2", ")", "\n", "if", "context", ".", "shape", "[", "0", "]", "!=", "1", ":", "\n", "            ", "raise", "ValueError", "(", "\"Expected `context.shape[0] == 1`, but found it False\"", ")", "\n", "\n", "", "self", ".", "theta_hat", "=", "np", ".", "concatenate", "(", "\n", "[", "\n", "self", ".", "A_inv", "[", "i", "]", "@", "self", ".", "b", "[", ":", ",", "i", "]", "[", ":", ",", "np", ".", "newaxis", "]", "\n", "for", "i", "in", "np", ".", "arange", "(", "self", ".", "n_actions", ")", "\n", "]", ",", "\n", "axis", "=", "1", ",", "\n", ")", "# dim * n_actions", "\n", "sigma_hat", "=", "np", ".", "concatenate", "(", "\n", "[", "\n", "np", ".", "sqrt", "(", "context", "@", "self", ".", "A_inv", "[", "i", "]", "@", "context", ".", "T", ")", "\n", "for", "i", "in", "np", ".", "arange", "(", "self", ".", "n_actions", ")", "\n", "]", ",", "\n", "axis", "=", "1", ",", "\n", ")", "# 1 * n_actions", "\n", "ucb_scores", "=", "(", "context", "@", "self", ".", "theta_hat", "+", "self", ".", "epsilon", "*", "sigma_hat", ")", ".", "flatten", "(", ")", "\n", "return", "ucb_scores", ".", "argsort", "(", ")", "[", ":", ":", "-", "1", "]", "[", ":", "self", ".", "len_list", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.linear.LinTS.__post_init__": [[269, 274], ["linear.BaseLinPolicy.__post_init__"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.__post_init__"], ["def", "__post_init__", "(", "self", ")", "->", "None", ":", "\n", "        ", "\"\"\"Initialize class.\"\"\"", "\n", "self", ".", "policy_name", "=", "\"linear_ts\"", "\n", "\n", "super", "(", ")", ".", "__post_init__", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.linear.LinTS.select_action": [[275, 308], ["numpy.concatenate", "numpy.concatenate", "predicted_rewards.argsort", "numpy.arange", "linear.LinTS.random_.multivariate_normal", "numpy.arange"], "methods", ["None"], ["", "def", "select_action", "(", "self", ",", "context", ":", "np", ".", "ndarray", ")", "->", "np", ".", "ndarray", ":", "\n", "        ", "\"\"\"Select action for new data.\n\n        Parameters\n        ----------\n        context: array-like, shape (1, dim_context)\n            Observed context vector.\n\n        Returns\n        ----------\n        selected_actions: array-like, shape (len_list, )\n            List of selected actions.\n\n        \"\"\"", "\n", "self", ".", "theta_hat", "=", "np", ".", "concatenate", "(", "\n", "[", "\n", "self", ".", "A_inv", "[", "i", "]", "@", "self", ".", "b", "[", ":", ",", "i", "]", "[", ":", ",", "np", ".", "newaxis", "]", "\n", "for", "i", "in", "np", ".", "arange", "(", "self", ".", "n_actions", ")", "\n", "]", ",", "\n", "axis", "=", "1", ",", "\n", ")", "\n", "theta_sampled", "=", "np", ".", "concatenate", "(", "\n", "[", "\n", "self", ".", "random_", ".", "multivariate_normal", "(", "self", ".", "theta_hat", "[", ":", ",", "i", "]", ",", "self", ".", "A_inv", "[", "i", "]", ")", "[", "\n", ":", ",", "np", ".", "newaxis", "\n", "]", "\n", "for", "i", "in", "np", ".", "arange", "(", "self", ".", "n_actions", ")", "\n", "]", ",", "\n", "axis", "=", "1", ",", "\n", ")", "\n", "\n", "predicted_rewards", "=", "(", "context", "@", "theta_sampled", ")", ".", "flatten", "(", ")", "\n", "return", "predicted_rewards", ".", "argsort", "(", ")", "[", ":", ":", "-", "1", "]", "[", ":", "self", ".", "len_list", "]", "\n", "", "", ""]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_meta_multi.DirectMethodMock._estimate_round_rewards": [[33, 41], ["None"], "methods", ["None"], ["def", "_estimate_round_rewards", "(", "\n", "self", ",", "\n", "position", ":", "np", ".", "ndarray", ",", "\n", "action_dist", ":", "np", ".", "ndarray", ",", "\n", "estimated_rewards_by_reg_model", ":", "np", ".", "ndarray", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "float", ":", "\n", "        ", "return", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_meta_multi.DirectMethodMock.estimate_policy_value": [[42, 67], ["None"], "methods", ["None"], ["", "def", "estimate_policy_value", "(", "\n", "self", ",", "\n", "position", ":", "np", ".", "ndarray", ",", "\n", "action_dist", ":", "np", ".", "ndarray", ",", "\n", "estimated_rewards_by_reg_model", ":", "np", ".", "ndarray", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "float", ":", "\n", "        ", "\"\"\"Estimate the policy value of evaluation policy.\n\n        Parameters\n        ----------\n        position: array-like, shape (n_rounds,)\n            Indices to differentiate positions in a recommendation interface where the actions are presented.\n\n        action_dist: array-like, shape (n_rounds, n_actions, len_list)\n            Action choice probabilities of the evaluation policy (can be deterministic), i.e., :math:`\\\\pi_e(a_i|x_i)`.\n\n        estimated_rewards_by_reg_model: array-like, shape (n_rounds, n_actions, len_list)\n            Estimated expected rewards given context, action, and position, i.e., :math:`\\\\hat{q}(x_i,a_i)`.\n\n        Returns\n        ----------\n        mock_policy_value: float\n        \"\"\"", "\n", "return", "mock_policy_value", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_meta_multi.DirectMethodMock.estimate_interval": [[68, 111], ["obp.utils.check_confidence_interval_arguments"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_confidence_interval_arguments"], ["", "def", "estimate_interval", "(", "\n", "self", ",", "\n", "position", ":", "np", ".", "ndarray", ",", "\n", "action_dist", ":", "np", ".", "ndarray", ",", "\n", "estimated_rewards_by_reg_model", ":", "np", ".", "ndarray", ",", "\n", "alpha", ":", "float", "=", "0.05", ",", "\n", "n_bootstrap_samples", ":", "int", "=", "10000", ",", "\n", "random_state", ":", "Optional", "[", "int", "]", "=", "None", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "Dict", "[", "str", ",", "float", "]", ":", "\n", "        ", "\"\"\"Estimate the confidence interval of the policy value using bootstrap.\n\n        Parameters\n        ----------\n        position: array-like, shape (n_rounds,)\n            Indices to differentiate positions in a recommendation interface where the actions are presented.\n\n        action_dist: array-like, shape (n_rounds, n_actions, len_list)\n            Action choice probabilities of the evaluation policy (can be deterministic), i.e., :math:`\\\\pi_e(a_i|x_i)`.\n\n        estimated_rewards_by_reg_model: array-like, shape (n_rounds, n_actions, len_list)\n            Estimated expected rewards given context, action, and position, i.e., :math:`\\\\hat{q}(x_i,a_i)`.\n\n        alpha: float, default=0.05\n            Significance level.\n\n        n_bootstrap_samples: int, default=10000\n            Number of resampling performed in bootstrap sampling.\n\n        random_state: int, default=None\n            Controls the random seed in bootstrap sampling.\n\n        Returns\n        ----------\n        mock_confidence_interval: Dict[str, float]\n            Dictionary storing the estimated mean and upper-lower confidence bounds.\n        \"\"\"", "\n", "check_confidence_interval_arguments", "(", "\n", "alpha", "=", "alpha", ",", "\n", "n_bootstrap_samples", "=", "n_bootstrap_samples", ",", "\n", "random_state", "=", "random_state", ",", "\n", ")", "\n", "return", "mock_confidence_interval", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_meta_multi.InverseProbabilityWeightingMock._estimate_round_rewards": [[120, 128], ["None"], "methods", ["None"], ["def", "_estimate_round_rewards", "(", "\n", "self", ",", "\n", "position", ":", "np", ".", "ndarray", ",", "\n", "action_dist", ":", "np", ".", "ndarray", ",", "\n", "estimated_rewards_by_reg_model", ":", "np", ".", "ndarray", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "float", ":", "\n", "        ", "return", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_meta_multi.InverseProbabilityWeightingMock.estimate_policy_value": [[129, 171], ["None"], "methods", ["None"], ["", "def", "estimate_policy_value", "(", "\n", "self", ",", "\n", "reward", ":", "np", ".", "ndarray", ",", "\n", "action", ":", "np", ".", "ndarray", ",", "\n", "stratum_idx", ":", "np", ".", "ndarray", ",", "\n", "position", ":", "np", ".", "ndarray", ",", "\n", "pscore", ":", "np", ".", "ndarray", ",", "\n", "action_dist", ":", "np", ".", "ndarray", ",", "\n", "estimated_pscore", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "np", ".", "ndarray", ":", "\n", "        ", "\"\"\"Estimate the policy value of evaluation policy.\n\n        Parameters\n        ----------\n        reward: array-like, shape (n_rounds,)\n            Rewards observed for each data in logged bandit data, i.e., :math:`r_i`.\n\n        action: array-like, shape (n_rounds,)\n            Actions sampled by the logging/behavior policy for each data in logged bandit data, i.e., :math:`a_i`.\n\n        position: array-like, shape (n_rounds,)\n            Indices to differentiate positions in a recommendation interface where the actions are presented.\n\n        stratum_idx: array-like, shape (n_rounds,)\n            Indices to differentiate the logging/behavior policy that generate each data, i.e., :math:`k`.\n\n        pscore: array-like, shape (n_rounds,)\n            Action choice probabilities of the logging/behavior policy (propensity scores), i.e., :math:`\\\\pi_b(a_i|x_i)`.\n\n        action_dist: array-like, shape (n_rounds, n_actions, len_list)\n            Action choice probabilities of the evaluation policy (can be deterministic), i.e., :math:`\\\\pi_e(a_i|x_i)`.\n\n        estimated_pscore: array-like, shape (n_rounds,), default=None\n            Estimated behavior policy (propensity scores), i.e., :math:`\\\\hat{\\\\pi}_b(a_i|x_i)`.\n\n        Returns\n        ----------\n        mock_policy_value: float\n\n        \"\"\"", "\n", "return", "mock_policy_value", "+", "self", ".", "eps", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_meta_multi.InverseProbabilityWeightingMock.estimate_interval": [[172, 228], ["mock_confidence_interval.items"], "methods", ["None"], ["", "def", "estimate_interval", "(", "\n", "self", ",", "\n", "reward", ":", "np", ".", "ndarray", ",", "\n", "action", ":", "np", ".", "ndarray", ",", "\n", "position", ":", "np", ".", "ndarray", ",", "\n", "stratum_idx", ":", "np", ".", "ndarray", ",", "\n", "pscore", ":", "np", ".", "ndarray", ",", "\n", "action_dist", ":", "np", ".", "ndarray", ",", "\n", "estimated_pscore", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "alpha", ":", "float", "=", "0.05", ",", "\n", "n_bootstrap_samples", ":", "int", "=", "10000", ",", "\n", "random_state", ":", "Optional", "[", "int", "]", "=", "None", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "Dict", "[", "str", ",", "float", "]", ":", "\n", "        ", "\"\"\"Estimate the confidence interval of the policy value using bootstrap.\n\n        Parameters\n        ----------\n        reward: array-like, shape (n_rounds,)\n            Rewards observed for each data in logged bandit data, i.e., :math:`r_i`.\n\n        action: array-like, shape (n_rounds,)\n            Actions sampled by the logging/behavior policy for each data in logged bandit data, i.e., :math:`a_i`.\n\n        position: array-like, shape (n_rounds,)\n            Indices to differentiate positions in a recommendation interface where the actions are presented.\n\n        stratum_idx: array-like, shape (n_rounds,)\n            Indices to differentiate the logging/behavior policy that generate each data, i.e., :math:`k`.\n\n        pscore: array-like, shape (n_rounds,)\n            Action choice probabilities of the logging/behavior policy (propensity scores), i.e., :math:`\\\\pi_b(a_i|x_i)`.\n\n        action_dist: array-like, shape (n_rounds, n_actions, len_list)\n            Action choice probabilities\n            by the evaluation policy (can be deterministic), i.e., :math:`\\\\pi_e(a_i|x_i)`.\n\n        estimated_pscore: array-like, shape (n_rounds,), default=None\n            Estimated behavior policy (propensity scores), i.e., :math:`\\\\hat{\\\\pi}_b(a_i|x_i)`.\n\n        alpha: float, default=0.05\n            Significance level.\n\n        n_bootstrap_samples: int, default=10000\n            Number of resampling performed in bootstrap sampling.\n\n        random_state: int, default=None\n            Controls the random seed in bootstrap sampling.\n\n        Returns\n        ----------\n        mock_confidence_interval: Dict[str, float]\n            Dictionary storing the estimated mean and upper-lower confidence bounds.\n\n        \"\"\"", "\n", "return", "{", "k", ":", "v", "+", "self", ".", "eps", "for", "k", ",", "v", "in", "mock_confidence_interval", ".", "items", "(", ")", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_meta_multi.BalancedInverseProbabilityWeightingMock._estimate_round_rewards": [[237, 245], ["None"], "methods", ["None"], ["def", "_estimate_round_rewards", "(", "\n", "self", ",", "\n", "position", ":", "np", ".", "ndarray", ",", "\n", "action_dist", ":", "np", ".", "ndarray", ",", "\n", "estimated_rewards_by_reg_model", ":", "np", ".", "ndarray", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "float", ":", "\n", "        ", "return", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_meta_multi.BalancedInverseProbabilityWeightingMock.estimate_policy_value": [[246, 285], ["None"], "methods", ["None"], ["", "def", "estimate_policy_value", "(", "\n", "self", ",", "\n", "reward", ":", "np", ".", "ndarray", ",", "\n", "action", ":", "np", ".", "ndarray", ",", "\n", "position", ":", "np", ".", "ndarray", ",", "\n", "pscore_avg", ":", "np", ".", "ndarray", ",", "\n", "action_dist", ":", "np", ".", "ndarray", ",", "\n", "estimated_pscore", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "np", ".", "ndarray", ":", "\n", "        ", "\"\"\"Estimate the policy value of evaluation policy.\n\n        Parameters\n        ----------\n        reward: array-like, shape (n_rounds,)\n            Rewards observed for each data in logged bandit data, i.e., :math:`r_i`.\n\n        action: array-like, shape (n_rounds,)\n            Actions sampled by the logging/behavior policy for each data in logged bandit data, i.e., :math:`a_i`.\n\n        position: array-like, shape (n_rounds,)\n            Indices to differentiate positions in a recommendation interface where the actions are presented.\n\n        pscore_avg: array-like, shape (n_rounds,)\n            Action choice probabilities of the average logging/behavior policy, i.e., :math:`\\\\pi_{avg}(a_i|x_i)`.\n            If `use_estimated_pscore` is False, `pscore_avg` must be given.\n\n        action_dist: array-like, shape (n_rounds, n_actions, len_list)\n            Action choice probabilities of the evaluation policy (can be deterministic), i.e., :math:`\\\\pi_e(a_i|x_i)`.\n\n        estimated_pscore: array-like, shape (n_rounds,), default=None\n            Estimated behavior policy (propensity scores), i.e., :math:`\\\\hat{\\\\pi}_b(a_i|x_i)`.\n\n        Returns\n        ----------\n        mock_policy_value: float\n\n        \"\"\"", "\n", "return", "mock_policy_value", "+", "self", ".", "eps", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_meta_multi.BalancedInverseProbabilityWeightingMock.estimate_interval": [[286, 339], ["mock_confidence_interval.items"], "methods", ["None"], ["", "def", "estimate_interval", "(", "\n", "self", ",", "\n", "reward", ":", "np", ".", "ndarray", ",", "\n", "action", ":", "np", ".", "ndarray", ",", "\n", "position", ":", "np", ".", "ndarray", ",", "\n", "pscore_avg", ":", "np", ".", "ndarray", ",", "\n", "action_dist", ":", "np", ".", "ndarray", ",", "\n", "estimated_pscore", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "alpha", ":", "float", "=", "0.05", ",", "\n", "n_bootstrap_samples", ":", "int", "=", "10000", ",", "\n", "random_state", ":", "Optional", "[", "int", "]", "=", "None", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "Dict", "[", "str", ",", "float", "]", ":", "\n", "        ", "\"\"\"Estimate the confidence interval of the policy value using bootstrap.\n\n        Parameters\n        ----------\n        reward: array-like, shape (n_rounds,)\n            Rewards observed for each data in logged bandit data, i.e., :math:`r_i`.\n\n        action: array-like, shape (n_rounds,)\n            Actions sampled by the logging/behavior policy for each data in logged bandit data, i.e., :math:`a_i`.\n\n        position: array-like, shape (n_rounds,)\n            Indices to differentiate positions in a recommendation interface where the actions are presented.\n\n        pscore_avg: array-like, shape (n_rounds,)\n            Action choice probabilities of the average logging/behavior policy, i.e., :math:`\\\\pi_{avg}(a_i|x_i)`.\n            If `use_estimated_pscore` is False, `pscore_avg` must be given.\n\n        action_dist: array-like, shape (n_rounds, n_actions, len_list)\n            Action choice probabilities\n            by the evaluation policy (can be deterministic), i.e., :math:`\\\\pi_e(a_i|x_i)`.\n\n        estimated_pscore: array-like, shape (n_rounds,), default=None\n            Estimated behavior policy (propensity scores), i.e., :math:`\\\\hat{\\\\pi}_b(a_i|x_i)`.\n\n        alpha: float, default=0.05\n            Significance level.\n\n        n_bootstrap_samples: int, default=10000\n            Number of resampling performed in bootstrap sampling.\n\n        random_state: int, default=None\n            Controls the random seed in bootstrap sampling.\n\n        Returns\n        ----------\n        mock_confidence_interval: Dict[str, float]\n            Dictionary storing the estimated mean and upper-lower confidence bounds.\n\n        \"\"\"", "\n", "return", "{", "k", ":", "v", "+", "self", ".", "eps", "for", "k", ",", "v", "in", "mock_confidence_interval", ".", "items", "(", ")", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_meta_multi.test_meta_post_init": [[348, 376], ["obp.ope.MultiLoggersOffPolicyEvaluation", "obp.ope.MultiLoggersOffPolicyEvaluation", "range", "len", "itertools.combinations", "pytest.raises", "obp.ope.MultiLoggersOffPolicyEvaluation"], "function", ["None"], ["def", "test_meta_post_init", "(", "synthetic_multi_bandit_feedback", ":", "BanditFeedback", ")", "->", "None", ":", "\n", "    ", "\"\"\"\n    Test the __post_init__ function\n    \"\"\"", "\n", "# __post_init__ saves the latter estimator when the same estimator name is used", "\n", "ope_", "=", "MultiLoggersOffPolicyEvaluation", "(", "\n", "bandit_feedback", "=", "synthetic_multi_bandit_feedback", ",", "ope_estimators", "=", "[", "ipw", ",", "ipw2", "]", "\n", ")", "\n", "assert", "ope_", ".", "ope_estimators_", "==", "{", "\"ipw\"", ":", "ipw2", "}", ",", "\"__post_init__ returns a wrong value\"", "\n", "# __post_init__ can handle the same estimator if the estimator names are different", "\n", "ope_", "=", "MultiLoggersOffPolicyEvaluation", "(", "\n", "bandit_feedback", "=", "synthetic_multi_bandit_feedback", ",", "ope_estimators", "=", "[", "ipw", ",", "ipw3", "]", "\n", ")", "\n", "assert", "ope_", ".", "ope_estimators_", "==", "{", "\n", "\"ipw\"", ":", "ipw", ",", "\n", "\"ipw3\"", ":", "ipw3", ",", "\n", "}", ",", "\"__post_init__ returns a wrong value\"", "\n", "# __post__init__ raises RuntimeError when necessary_keys are not included in the bandit_feedback", "\n", "necessary_keys", "=", "[", "\"action\"", ",", "\"position\"", ",", "\"reward\"", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "necessary_keys", ")", ")", ":", "\n", "        ", "for", "deleted_keys", "in", "itertools", ".", "combinations", "(", "necessary_keys", ",", "i", "+", "1", ")", ":", "\n", "            ", "invalid_bandit_feedback_dict", "=", "{", "key", ":", "\"_\"", "for", "key", "in", "necessary_keys", "}", "\n", "# delete", "\n", "for", "k", "in", "deleted_keys", ":", "\n", "                ", "del", "invalid_bandit_feedback_dict", "[", "k", "]", "\n", "", "with", "pytest", ".", "raises", "(", "RuntimeError", ",", "match", "=", "r\"Missing key*\"", ")", ":", "\n", "                ", "_", "=", "MultiLoggersOffPolicyEvaluation", "(", "\n", "bandit_feedback", "=", "invalid_bandit_feedback_dict", ",", "ope_estimators", "=", "[", "ipw", "]", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_meta_multi.test_meta_estimated_rewards_by_reg_model_inputs": [[379, 405], ["obp.ope.MultiLoggersOffPolicyEvaluation", "numpy.zeros", "pytest.raises", "obp.ope.MultiLoggersOffPolicyEvaluation.estimate_policy_values", "pytest.raises", "obp.ope.MultiLoggersOffPolicyEvaluation.estimate_intervals", "obp.ope.DirectMethod"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta.OffPolicyEvaluation.estimate_policy_values", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta.OffPolicyEvaluation.estimate_intervals"], ["", "", "", "", "def", "test_meta_estimated_rewards_by_reg_model_inputs", "(", "\n", "synthetic_multi_bandit_feedback", ":", "BanditFeedback", ",", "\n", ")", "->", "None", ":", "\n", "    ", "\"\"\"\n    Test the estimate_policy_values/estimate_intervals functions wrt estimated_rewards_by_reg_model\n    \"\"\"", "\n", "ope_", "=", "MultiLoggersOffPolicyEvaluation", "(", "\n", "bandit_feedback", "=", "synthetic_multi_bandit_feedback", ",", "ope_estimators", "=", "[", "DirectMethod", "(", ")", "]", "\n", ")", "\n", "\n", "action_dist", "=", "np", ".", "zeros", "(", "\n", "(", "\n", "synthetic_multi_bandit_feedback", "[", "\"n_rounds\"", "]", ",", "\n", "synthetic_multi_bandit_feedback", "[", "\"n_actions\"", "]", ",", "\n", ")", "\n", ")", "\n", "with", "pytest", ".", "raises", "(", "ValueError", ")", ":", "\n", "        ", "ope_", ".", "estimate_policy_values", "(", "\n", "action_dist", "=", "action_dist", ",", "\n", "estimated_rewards_by_reg_model", "=", "None", ",", "\n", ")", "\n", "\n", "", "with", "pytest", ".", "raises", "(", "ValueError", ")", ":", "\n", "        ", "ope_", ".", "estimate_intervals", "(", "\n", "action_dist", "=", "action_dist", ",", "\n", "estimated_rewards_by_reg_model", "=", "None", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_meta_multi.test_meta_create_estimator_inputs_using_invalid_input_data": [[445, 494], ["pytest.mark.parametrize", "obp.ope.MultiLoggersOffPolicyEvaluation", "pytest.raises", "obp.ope.MultiLoggersOffPolicyEvaluation._create_estimator_inputs", "pytest.raises", "obp.ope.MultiLoggersOffPolicyEvaluation.estimate_policy_values", "pytest.raises", "obp.ope.MultiLoggersOffPolicyEvaluation.estimate_intervals", "pytest.raises", "obp.ope.MultiLoggersOffPolicyEvaluation.summarize_off_policy_estimates", "pytest.raises", "obp.ope.MultiLoggersOffPolicyEvaluation.evaluate_performance_of_estimators", "pytest.raises", "obp.ope.MultiLoggersOffPolicyEvaluation.summarize_estimators_comparison"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta.OffPolicyEvaluation._create_estimator_inputs", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta.OffPolicyEvaluation.estimate_policy_values", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta.OffPolicyEvaluation.estimate_intervals", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta.OffPolicyEvaluation.summarize_off_policy_estimates", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta.OffPolicyEvaluation.evaluate_performance_of_estimators", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta.OffPolicyEvaluation.summarize_estimators_comparison"], ["@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"action_dist, estimated_rewards_by_reg_model, description\"", ",", "\n", "invalid_input_of_create_estimator_inputs", ",", "\n", ")", "\n", "def", "test_meta_create_estimator_inputs_using_invalid_input_data", "(", "\n", "action_dist", ",", "\n", "estimated_rewards_by_reg_model", ",", "\n", "description", ":", "str", ",", "\n", "synthetic_multi_bandit_feedback", ":", "BanditFeedback", ",", "\n", ")", "->", "None", ":", "\n", "    ", "\"\"\"\n    Test the _create_estimator_inputs using valid data\n    \"\"\"", "\n", "ope_", "=", "MultiLoggersOffPolicyEvaluation", "(", "\n", "bandit_feedback", "=", "synthetic_multi_bandit_feedback", ",", "ope_estimators", "=", "[", "ipw", "]", "\n", ")", "\n", "# raise ValueError when the shape of two arrays are different", "\n", "with", "pytest", ".", "raises", "(", "ValueError", ",", "match", "=", "f\"{description}*\"", ")", ":", "\n", "        ", "_", "=", "ope_", ".", "_create_estimator_inputs", "(", "\n", "action_dist", "=", "action_dist", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", ")", "\n", "# _create_estimator_inputs function is called in the following functions", "\n", "", "with", "pytest", ".", "raises", "(", "ValueError", ",", "match", "=", "f\"{description}*\"", ")", ":", "\n", "        ", "_", "=", "ope_", ".", "estimate_policy_values", "(", "\n", "action_dist", "=", "action_dist", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", ")", "\n", "", "with", "pytest", ".", "raises", "(", "ValueError", ",", "match", "=", "f\"{description}*\"", ")", ":", "\n", "        ", "_", "=", "ope_", ".", "estimate_intervals", "(", "\n", "action_dist", "=", "action_dist", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", ")", "\n", "", "with", "pytest", ".", "raises", "(", "ValueError", ",", "match", "=", "f\"{description}*\"", ")", ":", "\n", "        ", "_", "=", "ope_", ".", "summarize_off_policy_estimates", "(", "\n", "action_dist", "=", "action_dist", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", ")", "\n", "", "with", "pytest", ".", "raises", "(", "ValueError", ",", "match", "=", "f\"{description}*\"", ")", ":", "\n", "        ", "_", "=", "ope_", ".", "evaluate_performance_of_estimators", "(", "\n", "ground_truth_policy_value", "=", "0.1", ",", "\n", "action_dist", "=", "action_dist", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", ")", "\n", "", "with", "pytest", ".", "raises", "(", "ValueError", ",", "match", "=", "f\"{description}*\"", ")", ":", "\n", "        ", "_", "=", "ope_", ".", "summarize_estimators_comparison", "(", "\n", "ground_truth_policy_value", "=", "0.1", ",", "\n", "action_dist", "=", "action_dist", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_meta_multi.test_meta_create_estimator_inputs_using_valid_input_data": [[497, 554], ["pytest.mark.parametrize", "obp.ope.MultiLoggersOffPolicyEvaluation", "obp.ope.MultiLoggersOffPolicyEvaluation._create_estimator_inputs", "obp.ope.MultiLoggersOffPolicyEvaluation.estimate_policy_values", "obp.ope.MultiLoggersOffPolicyEvaluation.estimate_intervals", "obp.ope.MultiLoggersOffPolicyEvaluation.summarize_off_policy_estimates", "obp.ope.MultiLoggersOffPolicyEvaluation.evaluate_performance_of_estimators", "obp.ope.MultiLoggersOffPolicyEvaluation.summarize_estimators_comparison", "set", "set", "set", "set", "ope_._create_estimator_inputs.keys", "estimator_inputs[].keys"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta.OffPolicyEvaluation._create_estimator_inputs", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta.OffPolicyEvaluation.estimate_policy_values", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta.OffPolicyEvaluation.estimate_intervals", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta.OffPolicyEvaluation.summarize_off_policy_estimates", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta.OffPolicyEvaluation.evaluate_performance_of_estimators", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta.OffPolicyEvaluation.summarize_estimators_comparison"], ["", "", "@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"action_dist, estimated_rewards_by_reg_model, description\"", ",", "\n", "valid_input_of_create_estimator_inputs", ",", "\n", ")", "\n", "def", "test_meta_create_estimator_inputs_using_valid_input_data", "(", "\n", "action_dist", ",", "\n", "estimated_rewards_by_reg_model", ",", "\n", "description", ":", "str", ",", "\n", "synthetic_multi_bandit_feedback", ":", "BanditFeedback", ",", "\n", ")", "->", "None", ":", "\n", "    ", "\"\"\"\n    Test the _create_estimator_inputs using invalid data\n    \"\"\"", "\n", "ope_", "=", "MultiLoggersOffPolicyEvaluation", "(", "\n", "bandit_feedback", "=", "synthetic_multi_bandit_feedback", ",", "ope_estimators", "=", "[", "ipw", "]", "\n", ")", "\n", "estimator_inputs", "=", "ope_", ".", "_create_estimator_inputs", "(", "\n", "action_dist", "=", "action_dist", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", ")", "\n", "assert", "set", "(", "estimator_inputs", ".", "keys", "(", ")", ")", "==", "set", "(", "[", "\"ipw\"", "]", ")", "\n", "assert", "set", "(", "estimator_inputs", "[", "\"ipw\"", "]", ".", "keys", "(", ")", ")", "==", "set", "(", "\n", "[", "\n", "\"reward\"", ",", "\n", "\"action\"", ",", "\n", "\"pscore\"", ",", "\n", "\"position\"", ",", "\n", "\"action_dist\"", ",", "\n", "\"stratum_idx\"", ",", "\n", "\"pscore_avg\"", ",", "\n", "\"estimated_rewards_by_reg_model\"", ",", "\n", "\"estimated_pscore\"", ",", "\n", "\"estimated_pscore_avg\"", ",", "\n", "]", "\n", ")", ",", "f\"Invalid response of _create_estimator_inputs (test case: {description})\"", "\n", "# _create_estimator_inputs function is called in the following functions", "\n", "_", "=", "ope_", ".", "estimate_policy_values", "(", "\n", "action_dist", "=", "action_dist", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", ")", "\n", "_", "=", "ope_", ".", "estimate_intervals", "(", "\n", "action_dist", "=", "action_dist", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", ")", "\n", "_", "=", "ope_", ".", "summarize_off_policy_estimates", "(", "\n", "action_dist", "=", "action_dist", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", ")", "\n", "_", "=", "ope_", ".", "evaluate_performance_of_estimators", "(", "\n", "ground_truth_policy_value", "=", "0.1", ",", "\n", "action_dist", "=", "action_dist", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", ")", "\n", "_", "=", "ope_", ".", "summarize_estimators_comparison", "(", "\n", "ground_truth_policy_value", "=", "0.1", ",", "\n", "action_dist", "=", "action_dist", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_meta_multi.test_meta_estimate_policy_values_using_valid_input_data": [[557, 593], ["pytest.mark.parametrize", "obp.ope.MultiLoggersOffPolicyEvaluation", "obp.ope.MultiLoggersOffPolicyEvaluation", "obp.ope.MultiLoggersOffPolicyEvaluation.estimate_policy_values", "obp.ope.MultiLoggersOffPolicyEvaluation.estimate_policy_values"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta.OffPolicyEvaluation.estimate_policy_values", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta.OffPolicyEvaluation.estimate_policy_values"], ["", "@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"action_dist, estimated_rewards_by_reg_model, description\"", ",", "\n", "valid_input_of_create_estimator_inputs", "[", ":", "-", "1", "]", ",", "\n", ")", "\n", "def", "test_meta_estimate_policy_values_using_valid_input_data", "(", "\n", "action_dist", ",", "\n", "estimated_rewards_by_reg_model", ",", "\n", "description", ":", "str", ",", "\n", "synthetic_multi_bandit_feedback", ":", "BanditFeedback", ",", "\n", ")", "->", "None", ":", "\n", "    ", "\"\"\"\n    Test the response of estimate_policy_values using valid data\n    \"\"\"", "\n", "# single ope estimator", "\n", "ope_", "=", "MultiLoggersOffPolicyEvaluation", "(", "\n", "bandit_feedback", "=", "synthetic_multi_bandit_feedback", ",", "ope_estimators", "=", "[", "dm", "]", "\n", ")", "\n", "ope_", ".", "is_model_dependent", "=", "True", "\n", "assert", "ope_", ".", "estimate_policy_values", "(", "\n", "action_dist", "=", "action_dist", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", ")", "==", "{", "\n", "\"dm\"", ":", "mock_policy_value", "\n", "}", ",", "\"OffPolicyEvaluation.estimate_policy_values ([DirectMethod]) returns a wrong value\"", "\n", "# multiple ope estimators", "\n", "ope_", "=", "MultiLoggersOffPolicyEvaluation", "(", "\n", "bandit_feedback", "=", "synthetic_multi_bandit_feedback", ",", "ope_estimators", "=", "[", "dm", ",", "ipw", "]", "\n", ")", "\n", "ope_", ".", "is_model_dependent", "=", "True", "\n", "assert", "ope_", ".", "estimate_policy_values", "(", "\n", "action_dist", "=", "action_dist", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", ")", "==", "{", "\n", "\"dm\"", ":", "mock_policy_value", ",", "\n", "\"ipw\"", ":", "mock_policy_value", "+", "ipw", ".", "eps", ",", "\n", "}", ",", "\"OffPolicyEvaluation.estimate_policy_values ([DirectMethod, IPW]) returns a wrong value\"", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_meta_multi.test_meta_estimate_intervals_using_invalid_input_data": [[629, 670], ["pytest.mark.parametrize", "pytest.mark.parametrize", "obp.ope.MultiLoggersOffPolicyEvaluation", "pytest.raises", "obp.ope.MultiLoggersOffPolicyEvaluation.estimate_intervals", "pytest.raises", "obp.ope.MultiLoggersOffPolicyEvaluation.summarize_off_policy_estimates"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta.OffPolicyEvaluation.estimate_intervals", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta.OffPolicyEvaluation.summarize_off_policy_estimates"], ["@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"action_dist, estimated_rewards_by_reg_model, description_1\"", ",", "\n", "valid_input_of_create_estimator_inputs", ",", "\n", ")", "\n", "@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"alpha, n_bootstrap_samples, random_state, err, description_2\"", ",", "\n", "invalid_input_of_estimate_intervals", ",", "\n", ")", "\n", "def", "test_meta_estimate_intervals_using_invalid_input_data", "(", "\n", "action_dist", ",", "\n", "estimated_rewards_by_reg_model", ",", "\n", "description_1", ":", "str", ",", "\n", "alpha", ",", "\n", "n_bootstrap_samples", ",", "\n", "random_state", ",", "\n", "err", ",", "\n", "description_2", ":", "str", ",", "\n", "synthetic_multi_bandit_feedback", ":", "BanditFeedback", ",", "\n", ")", "->", "None", ":", "\n", "    ", "\"\"\"\n    Test the response of estimate_intervals using invalid data\n    \"\"\"", "\n", "ope_", "=", "MultiLoggersOffPolicyEvaluation", "(", "\n", "bandit_feedback", "=", "synthetic_multi_bandit_feedback", ",", "ope_estimators", "=", "[", "dm", "]", "\n", ")", "\n", "with", "pytest", ".", "raises", "(", "err", ",", "match", "=", "f\"{description_2}*\"", ")", ":", "\n", "        ", "_", "=", "ope_", ".", "estimate_intervals", "(", "\n", "action_dist", "=", "action_dist", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", "alpha", "=", "alpha", ",", "\n", "n_bootstrap_samples", "=", "n_bootstrap_samples", ",", "\n", "random_state", "=", "random_state", ",", "\n", ")", "\n", "# estimate_intervals function is called in summarize_off_policy_estimates", "\n", "", "with", "pytest", ".", "raises", "(", "err", ",", "match", "=", "f\"{description_2}*\"", ")", ":", "\n", "        ", "_", "=", "ope_", ".", "summarize_off_policy_estimates", "(", "\n", "action_dist", "=", "action_dist", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", "alpha", "=", "alpha", ",", "\n", "n_bootstrap_samples", "=", "n_bootstrap_samples", ",", "\n", "random_state", "=", "random_state", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_meta_multi.test_meta_estimate_intervals_using_valid_input_data": [[673, 721], ["pytest.mark.parametrize", "pytest.mark.parametrize", "obp.ope.MultiLoggersOffPolicyEvaluation", "obp.ope.MultiLoggersOffPolicyEvaluation", "obp.ope.MultiLoggersOffPolicyEvaluation.estimate_intervals", "obp.ope.MultiLoggersOffPolicyEvaluation.estimate_intervals", "mock_confidence_interval.items"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta.OffPolicyEvaluation.estimate_intervals", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta.OffPolicyEvaluation.estimate_intervals"], ["", "", "@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"action_dist, estimated_rewards_by_reg_model, description_1\"", ",", "\n", "valid_input_of_create_estimator_inputs", ",", "\n", ")", "\n", "@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"alpha, n_bootstrap_samples, random_state, description_2\"", ",", "\n", "valid_input_of_estimate_intervals", ",", "\n", ")", "\n", "def", "test_meta_estimate_intervals_using_valid_input_data", "(", "\n", "action_dist", ",", "\n", "estimated_rewards_by_reg_model", ",", "\n", "description_1", ":", "str", ",", "\n", "alpha", ":", "float", ",", "\n", "n_bootstrap_samples", ":", "int", ",", "\n", "random_state", ":", "int", ",", "\n", "description_2", ":", "str", ",", "\n", "synthetic_multi_bandit_feedback", ":", "BanditFeedback", ",", "\n", ")", "->", "None", ":", "\n", "    ", "\"\"\"\n    Test the response of estimate_intervals using valid data\n    \"\"\"", "\n", "# single ope estimator", "\n", "ope_", "=", "MultiLoggersOffPolicyEvaluation", "(", "\n", "bandit_feedback", "=", "synthetic_multi_bandit_feedback", ",", "ope_estimators", "=", "[", "dm", "]", "\n", ")", "\n", "assert", "ope_", ".", "estimate_intervals", "(", "\n", "action_dist", "=", "action_dist", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", "alpha", "=", "alpha", ",", "\n", "n_bootstrap_samples", "=", "n_bootstrap_samples", ",", "\n", "random_state", "=", "random_state", ",", "\n", ")", "==", "{", "\n", "\"dm\"", ":", "mock_confidence_interval", "\n", "}", ",", "\"OffPolicyEvaluation.estimate_intervals ([DirectMethod]) returns a wrong value\"", "\n", "# multiple ope estimators", "\n", "ope_", "=", "MultiLoggersOffPolicyEvaluation", "(", "\n", "bandit_feedback", "=", "synthetic_multi_bandit_feedback", ",", "ope_estimators", "=", "[", "dm", ",", "ipw", "]", "\n", ")", "\n", "assert", "ope_", ".", "estimate_intervals", "(", "\n", "action_dist", "=", "action_dist", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", "alpha", "=", "alpha", ",", "\n", "n_bootstrap_samples", "=", "n_bootstrap_samples", ",", "\n", "random_state", "=", "random_state", ",", "\n", ")", "==", "{", "\n", "\"dm\"", ":", "mock_confidence_interval", ",", "\n", "\"ipw\"", ":", "{", "k", ":", "v", "+", "ipw", ".", "eps", "for", "k", ",", "v", "in", "mock_confidence_interval", ".", "items", "(", ")", "}", ",", "\n", "}", ",", "\"OffPolicyEvaluation.estimate_intervals ([DirectMethod, IPW]) returns a wrong value\"", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_meta_multi.test_meta_summarize_off_policy_estimates": [[723, 797], ["pytest.mark.parametrize", "pytest.mark.parametrize", "obp.ope.MultiLoggersOffPolicyEvaluation", "obp.ope.MultiLoggersOffPolicyEvaluation.summarize_off_policy_estimates", "copy.deepcopy", "numpy.zeros", "obp.ope.MultiLoggersOffPolicyEvaluation", "obp.ope.MultiLoggersOffPolicyEvaluation.summarize_off_policy_estimates", "pandas.DataFrame", "synthetic_multi_bandit_feedback[].mean", "pandas.DataFrame", "pandas.testing.assert_frame_equal", "pandas.testing.assert_frame_equal", "pandas.DataFrame", "pandas.testing.assert_frame_equal", "mock_confidence_interval.items", "mock_confidence_interval.items"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta.OffPolicyEvaluation.summarize_off_policy_estimates", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta.OffPolicyEvaluation.summarize_off_policy_estimates"], ["", "@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"action_dist, estimated_rewards_by_reg_model, description_1\"", ",", "\n", "valid_input_of_create_estimator_inputs", ",", "\n", ")", "\n", "@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"alpha, n_bootstrap_samples, random_state, description_2\"", ",", "\n", "valid_input_of_estimate_intervals", ",", "\n", ")", "\n", "def", "test_meta_summarize_off_policy_estimates", "(", "\n", "action_dist", ",", "\n", "estimated_rewards_by_reg_model", ",", "\n", "description_1", ":", "str", ",", "\n", "alpha", ":", "float", ",", "\n", "n_bootstrap_samples", ":", "int", ",", "\n", "random_state", ":", "int", ",", "\n", "description_2", ":", "str", ",", "\n", "synthetic_multi_bandit_feedback", ":", "BanditFeedback", ",", "\n", ")", "->", "None", ":", "\n", "    ", "\"\"\"\n    Test the response of summarize_off_policy_estimates using valid data\n    \"\"\"", "\n", "ope_", "=", "MultiLoggersOffPolicyEvaluation", "(", "\n", "bandit_feedback", "=", "synthetic_multi_bandit_feedback", ",", "ope_estimators", "=", "[", "ipw", ",", "ipw3", "]", "\n", ")", "\n", "value", ",", "interval", "=", "ope_", ".", "summarize_off_policy_estimates", "(", "\n", "action_dist", "=", "action_dist", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", "alpha", "=", "alpha", ",", "\n", "n_bootstrap_samples", "=", "n_bootstrap_samples", ",", "\n", "random_state", "=", "random_state", ",", "\n", ")", "\n", "expected_value", "=", "pd", ".", "DataFrame", "(", "\n", "{", "\n", "\"ipw\"", ":", "mock_policy_value", "+", "ipw", ".", "eps", ",", "\n", "\"ipw3\"", ":", "mock_policy_value", "+", "ipw3", ".", "eps", ",", "\n", "}", ",", "\n", "index", "=", "[", "\"estimated_policy_value\"", "]", ",", "\n", ")", ".", "T", "\n", "expected_value", "[", "\"relative_estimated_policy_value\"", "]", "=", "(", "\n", "expected_value", "[", "\"estimated_policy_value\"", "]", "\n", "/", "synthetic_multi_bandit_feedback", "[", "\"reward\"", "]", ".", "mean", "(", ")", "\n", ")", "\n", "expected_interval", "=", "pd", ".", "DataFrame", "(", "\n", "{", "\n", "\"ipw\"", ":", "{", "k", ":", "v", "+", "ipw", ".", "eps", "for", "k", ",", "v", "in", "mock_confidence_interval", ".", "items", "(", ")", "}", ",", "\n", "\"ipw3\"", ":", "{", "k", ":", "v", "+", "ipw3", ".", "eps", "for", "k", ",", "v", "in", "mock_confidence_interval", ".", "items", "(", ")", "}", ",", "\n", "}", "\n", ")", ".", "T", "\n", "assert_frame_equal", "(", "value", ",", "expected_value", ")", ",", "\"Invalid summarization (policy value)\"", "\n", "assert_frame_equal", "(", "interval", ",", "expected_interval", ")", ",", "\"Invalid summarization (interval)\"", "\n", "# check relative estimated policy value when the average of bandit_feedback[\"reward\"] is zero", "\n", "zero_reward_bandit_feedback", "=", "deepcopy", "(", "synthetic_multi_bandit_feedback", ")", "\n", "zero_reward_bandit_feedback", "[", "\"reward\"", "]", "=", "np", ".", "zeros", "(", "\n", "zero_reward_bandit_feedback", "[", "\"reward\"", "]", ".", "shape", "[", "0", "]", "\n", ")", "\n", "ope_", "=", "MultiLoggersOffPolicyEvaluation", "(", "\n", "bandit_feedback", "=", "zero_reward_bandit_feedback", ",", "ope_estimators", "=", "[", "ipw", ",", "ipw3", "]", "\n", ")", "\n", "value", ",", "_", "=", "ope_", ".", "summarize_off_policy_estimates", "(", "\n", "action_dist", "=", "action_dist", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", "alpha", "=", "alpha", ",", "\n", "n_bootstrap_samples", "=", "n_bootstrap_samples", ",", "\n", "random_state", "=", "random_state", ",", "\n", ")", "\n", "expected_value", "=", "pd", ".", "DataFrame", "(", "\n", "{", "\n", "\"ipw\"", ":", "mock_policy_value", "+", "ipw", ".", "eps", ",", "\n", "\"ipw3\"", ":", "mock_policy_value", "+", "ipw3", ".", "eps", ",", "\n", "}", ",", "\n", "index", "=", "[", "\"estimated_policy_value\"", "]", ",", "\n", ")", ".", "T", "\n", "expected_value", "[", "\"relative_estimated_policy_value\"", "]", "=", "np", ".", "nan", "\n", "assert_frame_equal", "(", "value", ",", "expected_value", ")", ",", "\"Invalid summarization (policy value)\"", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_meta_multi.test_meta_evaluate_performance_of_estimators_using_invalid_input_data": [[827, 865], ["pytest.mark.parametrize", "pytest.mark.parametrize", "obp.ope.MultiLoggersOffPolicyEvaluation", "pytest.raises", "obp.ope.MultiLoggersOffPolicyEvaluation.evaluate_performance_of_estimators", "pytest.raises", "obp.ope.MultiLoggersOffPolicyEvaluation.summarize_estimators_comparison"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta.OffPolicyEvaluation.evaluate_performance_of_estimators", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta.OffPolicyEvaluation.summarize_estimators_comparison"], ["@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"action_dist, estimated_rewards_by_reg_model, description_1\"", ",", "\n", "valid_input_of_create_estimator_inputs", ",", "\n", ")", "\n", "@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"metric, ground_truth_policy_value, err, description_2\"", ",", "\n", "invalid_input_of_evaluation_performance_of_estimators", ",", "\n", ")", "\n", "def", "test_meta_evaluate_performance_of_estimators_using_invalid_input_data", "(", "\n", "action_dist", ",", "\n", "estimated_rewards_by_reg_model", ",", "\n", "description_1", ":", "str", ",", "\n", "metric", ",", "\n", "ground_truth_policy_value", ",", "\n", "err", ",", "\n", "description_2", ":", "str", ",", "\n", "synthetic_multi_bandit_feedback", ":", "BanditFeedback", ",", "\n", ")", "->", "None", ":", "\n", "    ", "\"\"\"\n    Test the response of evaluate_performance_of_estimators using invalid data\n    \"\"\"", "\n", "ope_", "=", "MultiLoggersOffPolicyEvaluation", "(", "\n", "bandit_feedback", "=", "synthetic_multi_bandit_feedback", ",", "ope_estimators", "=", "[", "dm", "]", "\n", ")", "\n", "with", "pytest", ".", "raises", "(", "err", ",", "match", "=", "f\"{description_2}*\"", ")", ":", "\n", "        ", "_", "=", "ope_", ".", "evaluate_performance_of_estimators", "(", "\n", "ground_truth_policy_value", "=", "ground_truth_policy_value", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", "action_dist", "=", "action_dist", ",", "\n", "metric", "=", "metric", ",", "\n", ")", "\n", "# estimate_intervals function is called in summarize_off_policy_estimates", "\n", "", "with", "pytest", ".", "raises", "(", "err", ",", "match", "=", "f\"{description_2}*\"", ")", ":", "\n", "        ", "_", "=", "ope_", ".", "summarize_estimators_comparison", "(", "\n", "ground_truth_policy_value", "=", "ground_truth_policy_value", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", "action_dist", "=", "action_dist", ",", "\n", "metric", "=", "metric", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_meta_multi.test_meta_evaluate_performance_of_estimators_using_valid_input_data": [[868, 928], ["pytest.mark.parametrize", "pytest.mark.parametrize", "obp.ope.MultiLoggersOffPolicyEvaluation", "obp.ope.MultiLoggersOffPolicyEvaluation.evaluate_performance_of_estimators", "ope_.evaluate_performance_of_estimators.items", "obp.ope.MultiLoggersOffPolicyEvaluation.summarize_estimators_comparison", "pandas.testing.assert_frame_equal", "numpy.abs", "numpy.abs", "pandas.DataFrame"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta.OffPolicyEvaluation.evaluate_performance_of_estimators", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta.OffPolicyEvaluation.summarize_estimators_comparison"], ["", "", "@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"action_dist, estimated_rewards_by_reg_model, description_1\"", ",", "\n", "valid_input_of_create_estimator_inputs", ",", "\n", ")", "\n", "@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"metric, ground_truth_policy_value, description_2\"", ",", "\n", "valid_input_of_evaluation_performance_of_estimators", ",", "\n", ")", "\n", "def", "test_meta_evaluate_performance_of_estimators_using_valid_input_data", "(", "\n", "action_dist", ",", "\n", "estimated_rewards_by_reg_model", ",", "\n", "description_1", ":", "str", ",", "\n", "metric", ",", "\n", "ground_truth_policy_value", ",", "\n", "description_2", ":", "str", ",", "\n", "synthetic_multi_bandit_feedback", ":", "BanditFeedback", ",", "\n", ")", "->", "None", ":", "\n", "    ", "\"\"\"\n    Test the response of evaluate_performance_of_estimators using valid data\n    \"\"\"", "\n", "if", "metric", "==", "\"relative-ee\"", ":", "\n", "# calculate relative-ee", "\n", "        ", "eval_metric_ope_dict", "=", "{", "\n", "\"ipw\"", ":", "np", ".", "abs", "(", "\n", "(", "mock_policy_value", "+", "ipw", ".", "eps", "-", "ground_truth_policy_value", ")", "\n", "/", "ground_truth_policy_value", "\n", ")", ",", "\n", "\"ipw3\"", ":", "np", ".", "abs", "(", "\n", "(", "mock_policy_value", "+", "ipw3", ".", "eps", "-", "ground_truth_policy_value", ")", "\n", "/", "ground_truth_policy_value", "\n", ")", ",", "\n", "}", "\n", "", "else", ":", "\n", "# calculate se", "\n", "        ", "eval_metric_ope_dict", "=", "{", "\n", "\"ipw\"", ":", "(", "mock_policy_value", "+", "ipw", ".", "eps", "-", "ground_truth_policy_value", ")", "**", "2", ",", "\n", "\"ipw3\"", ":", "(", "mock_policy_value", "+", "ipw3", ".", "eps", "-", "ground_truth_policy_value", ")", "**", "2", ",", "\n", "}", "\n", "# check performance estimators", "\n", "", "ope_", "=", "MultiLoggersOffPolicyEvaluation", "(", "\n", "bandit_feedback", "=", "synthetic_multi_bandit_feedback", ",", "ope_estimators", "=", "[", "ipw", ",", "ipw3", "]", "\n", ")", "\n", "performance", "=", "ope_", ".", "evaluate_performance_of_estimators", "(", "\n", "ground_truth_policy_value", "=", "ground_truth_policy_value", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", "action_dist", "=", "action_dist", ",", "\n", "metric", "=", "metric", ",", "\n", ")", "\n", "for", "k", ",", "v", "in", "performance", ".", "items", "(", ")", ":", "\n", "        ", "assert", "k", "in", "eval_metric_ope_dict", ",", "\"Invalid key of performance response\"", "\n", "assert", "v", "==", "eval_metric_ope_dict", "[", "k", "]", ",", "\"Invalid value of performance response\"", "\n", "", "performance_df", "=", "ope_", ".", "summarize_estimators_comparison", "(", "\n", "ground_truth_policy_value", "=", "ground_truth_policy_value", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", "action_dist", "=", "action_dist", ",", "\n", "metric", "=", "metric", ",", "\n", ")", "\n", "assert_frame_equal", "(", "\n", "performance_df", ",", "pd", ".", "DataFrame", "(", "eval_metric_ope_dict", ",", "index", "=", "[", "metric", "]", ")", ".", "T", "\n", ")", ",", "\"Invalid summarization (performance)\"", "\n", "", ""]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_meta_continuous.KernelizedInverseProbabilityWeightingMock._estimate_round_rewards": [[34, 43], ["numpy.ones_like"], "methods", ["None"], ["def", "_estimate_round_rewards", "(", "\n", "self", ",", "\n", "reward", ":", "np", ".", "ndarray", ",", "\n", "action_by_behavior_policy", ":", "np", ".", "ndarray", ",", "\n", "pscore", ":", "np", ".", "ndarray", ",", "\n", "action_by_evaluation_policy", ":", "np", ".", "ndarray", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "np", ".", "ndarray", ":", "\n", "        ", "return", "np", ".", "ones_like", "(", "reward", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_meta_continuous.KernelizedInverseProbabilityWeightingMock.estimate_policy_value": [[44, 60], ["None"], "methods", ["None"], ["", "def", "estimate_policy_value", "(", "\n", "self", ",", "\n", "reward", ":", "np", ".", "ndarray", ",", "\n", "action_by_behavior_policy", ":", "np", ".", "ndarray", ",", "\n", "pscore", ":", "np", ".", "ndarray", ",", "\n", "action_by_evaluation_policy", ":", "np", ".", "ndarray", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "float", ":", "\n", "        ", "\"\"\"Estimate policy value of an evaluation policy.\n\n        Returns\n        ----------\n        mock_policy_value: float\n\n        \"\"\"", "\n", "return", "mock_policy_value", "+", "self", ".", "eps", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_meta_continuous.KernelizedInverseProbabilityWeightingMock.estimate_interval": [[61, 86], ["obp.utils.check_confidence_interval_arguments", "mock_confidence_interval.items"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_confidence_interval_arguments"], ["", "def", "estimate_interval", "(", "\n", "self", ",", "\n", "reward", ":", "np", ".", "ndarray", ",", "\n", "action_by_behavior_policy", ":", "np", ".", "ndarray", ",", "\n", "pscore", ":", "np", ".", "ndarray", ",", "\n", "action_by_evaluation_policy", ":", "np", ".", "ndarray", ",", "\n", "alpha", ":", "float", "=", "0.05", ",", "\n", "n_bootstrap_samples", ":", "int", "=", "10000", ",", "\n", "random_state", ":", "Optional", "[", "int", "]", "=", "None", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "Dict", "[", "str", ",", "float", "]", ":", "\n", "        ", "\"\"\"Estimate the confidence interval of the policy value using bootstrap.\n\n        Returns\n        ----------\n        mock_confidence_interval: Dict[str, float]\n            Dictionary storing the estimated mean and upper-lower confidence bounds.\n\n        \"\"\"", "\n", "check_confidence_interval_arguments", "(", "\n", "alpha", "=", "alpha", ",", "\n", "n_bootstrap_samples", "=", "n_bootstrap_samples", ",", "\n", "random_state", "=", "random_state", ",", "\n", ")", "\n", "return", "{", "k", ":", "v", "+", "self", ".", "eps", "for", "k", ",", "v", "in", "mock_confidence_interval", ".", "items", "(", ")", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_meta_continuous.KernelizedDoublyRobustMock._estimate_round_rewards": [[94, 104], ["numpy.ones_like"], "methods", ["None"], ["def", "_estimate_round_rewards", "(", "\n", "self", ",", "\n", "reward", ":", "np", ".", "ndarray", ",", "\n", "action_by_behavior_policy", ":", "np", ".", "ndarray", ",", "\n", "pscore", ":", "np", ".", "ndarray", ",", "\n", "action_by_evaluation_policy", ":", "np", ".", "ndarray", ",", "\n", "estimated_rewards_by_reg_model", ":", "np", ".", "ndarray", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "np", ".", "ndarray", ":", "\n", "        ", "return", "np", ".", "ones_like", "(", "reward", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_meta_continuous.KernelizedDoublyRobustMock.estimate_policy_value": [[105, 122], ["None"], "methods", ["None"], ["", "def", "estimate_policy_value", "(", "\n", "self", ",", "\n", "reward", ":", "np", ".", "ndarray", ",", "\n", "action_by_behavior_policy", ":", "np", ".", "ndarray", ",", "\n", "pscore", ":", "np", ".", "ndarray", ",", "\n", "action_by_evaluation_policy", ":", "np", ".", "ndarray", ",", "\n", "estimated_rewards_by_reg_model", ":", "np", ".", "ndarray", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "float", ":", "\n", "        ", "\"\"\"Estimate policy value of an evaluation policy.\n\n        Returns\n        ----------\n        mock_policy_value: float\n\n        \"\"\"", "\n", "return", "mock_policy_value", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_meta_continuous.KernelizedDoublyRobustMock.estimate_interval": [[123, 149], ["obp.utils.check_confidence_interval_arguments", "mock_confidence_interval.items"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_confidence_interval_arguments"], ["", "def", "estimate_interval", "(", "\n", "self", ",", "\n", "reward", ":", "np", ".", "ndarray", ",", "\n", "action_by_behavior_policy", ":", "np", ".", "ndarray", ",", "\n", "pscore", ":", "np", ".", "ndarray", ",", "\n", "action_by_evaluation_policy", ":", "np", ".", "ndarray", ",", "\n", "estimated_rewards_by_reg_model", ":", "np", ".", "ndarray", ",", "\n", "alpha", ":", "float", "=", "0.05", ",", "\n", "n_bootstrap_samples", ":", "int", "=", "10000", ",", "\n", "random_state", ":", "Optional", "[", "int", "]", "=", "None", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "Dict", "[", "str", ",", "float", "]", ":", "\n", "        ", "\"\"\"Estimate the confidence interval of the policy value using bootstrap.\n\n        Returns\n        ----------\n        mock_confidence_interval: Dict[str, float]\n            Dictionary storing the estimated mean and upper-lower confidence bounds.\n\n        \"\"\"", "\n", "check_confidence_interval_arguments", "(", "\n", "alpha", "=", "alpha", ",", "\n", "n_bootstrap_samples", "=", "n_bootstrap_samples", ",", "\n", "random_state", "=", "random_state", ",", "\n", ")", "\n", "return", "{", "k", ":", "v", "for", "k", ",", "v", "in", "mock_confidence_interval", ".", "items", "(", ")", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_meta_continuous.test_meta_post_init": [[158, 186], ["obp.ope.ContinuousOffPolicyEvaluation", "obp.ope.ContinuousOffPolicyEvaluation", "range", "len", "itertools.combinations", "pytest.raises", "obp.ope.ContinuousOffPolicyEvaluation"], "function", ["None"], ["def", "test_meta_post_init", "(", "synthetic_continuous_bandit_feedback", ":", "BanditFeedback", ")", "->", "None", ":", "\n", "    ", "\"\"\"\n    Test the __post_init__ function\n    \"\"\"", "\n", "# __post_init__ saves the latter estimator when the same estimator name is used", "\n", "ope_", "=", "ContinuousOffPolicyEvaluation", "(", "\n", "bandit_feedback", "=", "synthetic_continuous_bandit_feedback", ",", "ope_estimators", "=", "[", "ipw", ",", "ipw2", "]", "\n", ")", "\n", "assert", "ope_", ".", "ope_estimators_", "==", "{", "\"ipw\"", ":", "ipw2", "}", ",", "\"__post_init__ returns a wrong value\"", "\n", "# __post_init__ can handle the same estimator if the estimator names are different", "\n", "ope_", "=", "ContinuousOffPolicyEvaluation", "(", "\n", "bandit_feedback", "=", "synthetic_continuous_bandit_feedback", ",", "ope_estimators", "=", "[", "ipw", ",", "ipw3", "]", "\n", ")", "\n", "assert", "ope_", ".", "ope_estimators_", "==", "{", "\n", "\"ipw\"", ":", "ipw", ",", "\n", "\"ipw3\"", ":", "ipw3", ",", "\n", "}", ",", "\"__post_init__ returns a wrong value\"", "\n", "# __post__init__ raises RuntimeError when necessary_keys are not included in the bandit_feedback", "\n", "necessary_keys", "=", "[", "\"action_by_behavior_policy\"", ",", "\"reward\"", ",", "\"pscore\"", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "necessary_keys", ")", ")", ":", "\n", "        ", "for", "deleted_keys", "in", "itertools", ".", "combinations", "(", "necessary_keys", ",", "i", "+", "1", ")", ":", "\n", "            ", "invalid_bandit_feedback_dict", "=", "{", "key", ":", "\"_\"", "for", "key", "in", "necessary_keys", "}", "\n", "# delete", "\n", "for", "k", "in", "deleted_keys", ":", "\n", "                ", "del", "invalid_bandit_feedback_dict", "[", "k", "]", "\n", "", "with", "pytest", ".", "raises", "(", "RuntimeError", ",", "match", "=", "r\"Missing key*\"", ")", ":", "\n", "                ", "_", "=", "ContinuousOffPolicyEvaluation", "(", "\n", "bandit_feedback", "=", "invalid_bandit_feedback_dict", ",", "ope_estimators", "=", "[", "ipw", "]", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_meta_continuous.test_meta_estimated_rewards_by_reg_model_inputs": [[189, 212], ["obp.ope.KernelizedDoublyRobust", "obp.ope.ContinuousOffPolicyEvaluation", "numpy.zeros", "pytest.raises", "obp.ope.ContinuousOffPolicyEvaluation.estimate_policy_values", "pytest.raises", "obp.ope.ContinuousOffPolicyEvaluation.estimate_intervals"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta.OffPolicyEvaluation.estimate_policy_values", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta.OffPolicyEvaluation.estimate_intervals"], ["", "", "", "", "def", "test_meta_estimated_rewards_by_reg_model_inputs", "(", "\n", "synthetic_bandit_feedback", ":", "BanditFeedback", ",", "\n", ")", "->", "None", ":", "\n", "    ", "\"\"\"\n    Test the estimate_policy_values/estimate_intervals functions wrt estimated_rewards_by_reg_model\n    \"\"\"", "\n", "kdr", "=", "KernelizedDoublyRobust", "(", "kernel", "=", "\"cosine\"", ",", "bandwidth", "=", "0.1", ")", "\n", "ope_", "=", "ContinuousOffPolicyEvaluation", "(", "\n", "bandit_feedback", "=", "synthetic_bandit_feedback", ",", "\n", "ope_estimators", "=", "[", "kdr", "]", ",", "\n", ")", "\n", "\n", "action_by_evaluation_policy", "=", "np", ".", "zeros", "(", "(", "synthetic_bandit_feedback", "[", "\"n_rounds\"", "]", ",", ")", ")", "\n", "with", "pytest", ".", "raises", "(", "ValueError", ")", ":", "\n", "        ", "ope_", ".", "estimate_policy_values", "(", "\n", "action_by_evaluation_policy", "=", "action_by_evaluation_policy", ",", "\n", "estimated_rewards_by_reg_model", "=", "None", ",", "\n", ")", "\n", "\n", "", "with", "pytest", ".", "raises", "(", "ValueError", ")", ":", "\n", "        ", "ope_", ".", "estimate_intervals", "(", "\n", "action_by_evaluation_policy", "=", "action_by_evaluation_policy", ",", "\n", "estimated_rewards_by_reg_model", "=", "None", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_meta_continuous.test_meta_create_estimator_inputs_using_invalid_input_data": [[256, 305], ["pytest.mark.parametrize", "obp.ope.ContinuousOffPolicyEvaluation", "pytest.raises", "obp.ope.ContinuousOffPolicyEvaluation._create_estimator_inputs", "pytest.raises", "obp.ope.ContinuousOffPolicyEvaluation.estimate_policy_values", "pytest.raises", "obp.ope.ContinuousOffPolicyEvaluation.estimate_intervals", "pytest.raises", "obp.ope.ContinuousOffPolicyEvaluation.summarize_off_policy_estimates", "pytest.raises", "obp.ope.ContinuousOffPolicyEvaluation.evaluate_performance_of_estimators", "pytest.raises", "obp.ope.ContinuousOffPolicyEvaluation.summarize_estimators_comparison"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta.OffPolicyEvaluation._create_estimator_inputs", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta.OffPolicyEvaluation.estimate_policy_values", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta.OffPolicyEvaluation.estimate_intervals", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta.OffPolicyEvaluation.summarize_off_policy_estimates", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta.OffPolicyEvaluation.evaluate_performance_of_estimators", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta.OffPolicyEvaluation.summarize_estimators_comparison"], ["@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"action_by_evaluation_policy, estimated_rewards_by_reg_model, description\"", ",", "\n", "invalid_input_of_create_estimator_inputs", ",", "\n", ")", "\n", "def", "test_meta_create_estimator_inputs_using_invalid_input_data", "(", "\n", "action_by_evaluation_policy", ",", "\n", "estimated_rewards_by_reg_model", ",", "\n", "description", ":", "str", ",", "\n", "synthetic_continuous_bandit_feedback", ":", "BanditFeedback", ",", "\n", ")", "->", "None", ":", "\n", "    ", "\"\"\"\n    Test the _create_estimator_inputs using valid data\n    \"\"\"", "\n", "ope_", "=", "ContinuousOffPolicyEvaluation", "(", "\n", "bandit_feedback", "=", "synthetic_continuous_bandit_feedback", ",", "ope_estimators", "=", "[", "ipw", "]", "\n", ")", "\n", "# raise ValueError when the shape of two arrays are different", "\n", "with", "pytest", ".", "raises", "(", "ValueError", ",", "match", "=", "f\"{description}*\"", ")", ":", "\n", "        ", "_", "=", "ope_", ".", "_create_estimator_inputs", "(", "\n", "action_by_evaluation_policy", "=", "action_by_evaluation_policy", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", ")", "\n", "# _create_estimator_inputs function is called in the following functions", "\n", "", "with", "pytest", ".", "raises", "(", "ValueError", ",", "match", "=", "f\"{description}*\"", ")", ":", "\n", "        ", "_", "=", "ope_", ".", "estimate_policy_values", "(", "\n", "action_by_evaluation_policy", "=", "action_by_evaluation_policy", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", ")", "\n", "", "with", "pytest", ".", "raises", "(", "ValueError", ",", "match", "=", "f\"{description}*\"", ")", ":", "\n", "        ", "_", "=", "ope_", ".", "estimate_intervals", "(", "\n", "action_by_evaluation_policy", "=", "action_by_evaluation_policy", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", ")", "\n", "", "with", "pytest", ".", "raises", "(", "ValueError", ",", "match", "=", "f\"{description}*\"", ")", ":", "\n", "        ", "_", "=", "ope_", ".", "summarize_off_policy_estimates", "(", "\n", "action_by_evaluation_policy", "=", "action_by_evaluation_policy", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", ")", "\n", "", "with", "pytest", ".", "raises", "(", "ValueError", ",", "match", "=", "f\"{description}*\"", ")", ":", "\n", "        ", "_", "=", "ope_", ".", "evaluate_performance_of_estimators", "(", "\n", "ground_truth_policy_value", "=", "0.1", ",", "\n", "action_by_evaluation_policy", "=", "action_by_evaluation_policy", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", ")", "\n", "", "with", "pytest", ".", "raises", "(", "ValueError", ",", "match", "=", "f\"{description}*\"", ")", ":", "\n", "        ", "_", "=", "ope_", ".", "summarize_estimators_comparison", "(", "\n", "ground_truth_policy_value", "=", "0.1", ",", "\n", "action_by_evaluation_policy", "=", "action_by_evaluation_policy", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_meta_continuous.test_meta_create_estimator_inputs_using_valid_input_data": [[308, 360], ["pytest.mark.parametrize", "obp.ope.ContinuousOffPolicyEvaluation", "obp.ope.ContinuousOffPolicyEvaluation._create_estimator_inputs", "obp.ope.ContinuousOffPolicyEvaluation.estimate_policy_values", "obp.ope.ContinuousOffPolicyEvaluation.estimate_intervals", "obp.ope.ContinuousOffPolicyEvaluation.summarize_off_policy_estimates", "obp.ope.ContinuousOffPolicyEvaluation.evaluate_performance_of_estimators", "obp.ope.ContinuousOffPolicyEvaluation.summarize_estimators_comparison", "set", "set", "set", "set", "ope_._create_estimator_inputs.keys", "estimator_inputs[].keys"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta.OffPolicyEvaluation._create_estimator_inputs", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta.OffPolicyEvaluation.estimate_policy_values", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta.OffPolicyEvaluation.estimate_intervals", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta.OffPolicyEvaluation.summarize_off_policy_estimates", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta.OffPolicyEvaluation.evaluate_performance_of_estimators", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta.OffPolicyEvaluation.summarize_estimators_comparison"], ["", "", "@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"action_by_evaluation_policy, estimated_rewards_by_reg_model, description\"", ",", "\n", "valid_input_of_create_estimator_inputs", ",", "\n", ")", "\n", "def", "test_meta_create_estimator_inputs_using_valid_input_data", "(", "\n", "action_by_evaluation_policy", ",", "\n", "estimated_rewards_by_reg_model", ",", "\n", "description", ":", "str", ",", "\n", "synthetic_continuous_bandit_feedback", ":", "BanditFeedback", ",", "\n", ")", "->", "None", ":", "\n", "    ", "\"\"\"\n    Test the _create_estimator_inputs using invalid data\n    \"\"\"", "\n", "ope_", "=", "ContinuousOffPolicyEvaluation", "(", "\n", "bandit_feedback", "=", "synthetic_continuous_bandit_feedback", ",", "ope_estimators", "=", "[", "ipw", "]", "\n", ")", "\n", "estimator_inputs", "=", "ope_", ".", "_create_estimator_inputs", "(", "\n", "action_by_evaluation_policy", "=", "action_by_evaluation_policy", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", ")", "\n", "assert", "set", "(", "estimator_inputs", ".", "keys", "(", ")", ")", "==", "set", "(", "[", "\"ipw\"", "]", ")", "\n", "assert", "set", "(", "estimator_inputs", "[", "\"ipw\"", "]", ".", "keys", "(", ")", ")", "==", "set", "(", "\n", "[", "\n", "\"reward\"", ",", "\n", "\"action_by_behavior_policy\"", ",", "\n", "\"pscore\"", ",", "\n", "\"action_by_evaluation_policy\"", ",", "\n", "\"estimated_rewards_by_reg_model\"", ",", "\n", "]", "\n", ")", ",", "f\"Invalid response of _create_estimator_inputs (test case: {description})\"", "\n", "# _create_estimator_inputs function is called in the following functions", "\n", "_", "=", "ope_", ".", "estimate_policy_values", "(", "\n", "action_by_evaluation_policy", "=", "action_by_evaluation_policy", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", ")", "\n", "_", "=", "ope_", ".", "estimate_intervals", "(", "\n", "action_by_evaluation_policy", "=", "action_by_evaluation_policy", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", ")", "\n", "_", "=", "ope_", ".", "summarize_off_policy_estimates", "(", "\n", "action_by_evaluation_policy", "=", "action_by_evaluation_policy", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", ")", "\n", "_", "=", "ope_", ".", "evaluate_performance_of_estimators", "(", "\n", "ground_truth_policy_value", "=", "0.1", ",", "\n", "action_by_evaluation_policy", "=", "action_by_evaluation_policy", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", ")", "\n", "_", "=", "ope_", ".", "summarize_estimators_comparison", "(", "\n", "ground_truth_policy_value", "=", "0.1", ",", "\n", "action_by_evaluation_policy", "=", "action_by_evaluation_policy", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_meta_continuous.test_meta_estimate_policy_values_using_valid_input_data": [[363, 397], ["pytest.mark.parametrize", "obp.ope.ContinuousOffPolicyEvaluation", "obp.ope.ContinuousOffPolicyEvaluation", "obp.ope.ContinuousOffPolicyEvaluation.estimate_policy_values", "obp.ope.ContinuousOffPolicyEvaluation.estimate_policy_values"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta.OffPolicyEvaluation.estimate_policy_values", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta.OffPolicyEvaluation.estimate_policy_values"], ["", "@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"action_by_evaluation_policy, estimated_rewards_by_reg_model, description\"", ",", "\n", "valid_input_of_create_estimator_inputs", ",", "\n", ")", "\n", "def", "test_meta_estimate_policy_values_using_valid_input_data", "(", "\n", "action_by_evaluation_policy", ",", "\n", "estimated_rewards_by_reg_model", ",", "\n", "description", ":", "str", ",", "\n", "synthetic_continuous_bandit_feedback", ":", "BanditFeedback", ",", "\n", ")", "->", "None", ":", "\n", "    ", "\"\"\"\n    Test the response of estimate_policy_values using valid data\n    \"\"\"", "\n", "# single ope estimator", "\n", "ope_", "=", "ContinuousOffPolicyEvaluation", "(", "\n", "bandit_feedback", "=", "synthetic_continuous_bandit_feedback", ",", "ope_estimators", "=", "[", "dr", "]", "\n", ")", "\n", "assert", "ope_", ".", "estimate_policy_values", "(", "\n", "action_by_evaluation_policy", "=", "action_by_evaluation_policy", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", ")", "==", "{", "\n", "\"dr\"", ":", "mock_policy_value", "\n", "}", ",", "\"OffPolicyEvaluation.estimate_policy_values ([DoublyRobust]) returns a wrong value\"", "\n", "# multiple ope estimators", "\n", "ope_", "=", "ContinuousOffPolicyEvaluation", "(", "\n", "bandit_feedback", "=", "synthetic_continuous_bandit_feedback", ",", "ope_estimators", "=", "[", "dr", ",", "ipw", "]", "\n", ")", "\n", "assert", "ope_", ".", "estimate_policy_values", "(", "\n", "action_by_evaluation_policy", "=", "action_by_evaluation_policy", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", ")", "==", "{", "\n", "\"dr\"", ":", "mock_policy_value", ",", "\n", "\"ipw\"", ":", "mock_policy_value", "+", "ipw", ".", "eps", ",", "\n", "}", ",", "\"OffPolicyEvaluation.estimate_policy_values ([DoublyRobust, IPW]) returns a wrong value\"", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_meta_continuous.test_meta_estimate_intervals_using_invalid_input_data": [[433, 474], ["pytest.mark.parametrize", "pytest.mark.parametrize", "obp.ope.ContinuousOffPolicyEvaluation", "pytest.raises", "obp.ope.ContinuousOffPolicyEvaluation.estimate_intervals", "pytest.raises", "obp.ope.ContinuousOffPolicyEvaluation.summarize_off_policy_estimates"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta.OffPolicyEvaluation.estimate_intervals", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta.OffPolicyEvaluation.summarize_off_policy_estimates"], ["@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"action_by_evaluation_policy, estimated_rewards_by_reg_model, description_1\"", ",", "\n", "valid_input_of_create_estimator_inputs", ",", "\n", ")", "\n", "@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"alpha, n_bootstrap_samples, random_state, err, description_2\"", ",", "\n", "invalid_input_of_estimate_intervals", ",", "\n", ")", "\n", "def", "test_meta_estimate_intervals_using_invalid_input_data", "(", "\n", "action_by_evaluation_policy", ",", "\n", "estimated_rewards_by_reg_model", ",", "\n", "description_1", ":", "str", ",", "\n", "alpha", ",", "\n", "n_bootstrap_samples", ",", "\n", "random_state", ",", "\n", "err", ",", "\n", "description_2", ":", "str", ",", "\n", "synthetic_continuous_bandit_feedback", ":", "BanditFeedback", ",", "\n", ")", "->", "None", ":", "\n", "    ", "\"\"\"\n    Test the response of estimate_intervals using invalid data\n    \"\"\"", "\n", "ope_", "=", "ContinuousOffPolicyEvaluation", "(", "\n", "bandit_feedback", "=", "synthetic_continuous_bandit_feedback", ",", "ope_estimators", "=", "[", "dr", "]", "\n", ")", "\n", "with", "pytest", ".", "raises", "(", "err", ",", "match", "=", "f\"{description_2}*\"", ")", ":", "\n", "        ", "_", "=", "ope_", ".", "estimate_intervals", "(", "\n", "action_by_evaluation_policy", "=", "action_by_evaluation_policy", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", "alpha", "=", "alpha", ",", "\n", "n_bootstrap_samples", "=", "n_bootstrap_samples", ",", "\n", "random_state", "=", "random_state", ",", "\n", ")", "\n", "# estimate_intervals function is called in summarize_off_policy_estimates", "\n", "", "with", "pytest", ".", "raises", "(", "err", ",", "match", "=", "f\"{description_2}*\"", ")", ":", "\n", "        ", "_", "=", "ope_", ".", "summarize_off_policy_estimates", "(", "\n", "action_by_evaluation_policy", "=", "action_by_evaluation_policy", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", "alpha", "=", "alpha", ",", "\n", "n_bootstrap_samples", "=", "n_bootstrap_samples", ",", "\n", "random_state", "=", "random_state", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_meta_continuous.test_meta_estimate_intervals_using_valid_input_data": [[477, 525], ["pytest.mark.parametrize", "pytest.mark.parametrize", "obp.ope.ContinuousOffPolicyEvaluation", "obp.ope.ContinuousOffPolicyEvaluation", "obp.ope.ContinuousOffPolicyEvaluation.estimate_intervals", "obp.ope.ContinuousOffPolicyEvaluation.estimate_intervals", "mock_confidence_interval.items"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta.OffPolicyEvaluation.estimate_intervals", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta.OffPolicyEvaluation.estimate_intervals"], ["", "", "@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"action_by_evaluation_policy, estimated_rewards_by_reg_model, description_1\"", ",", "\n", "valid_input_of_create_estimator_inputs", ",", "\n", ")", "\n", "@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"alpha, n_bootstrap_samples, random_state, description_2\"", ",", "\n", "valid_input_of_estimate_intervals", ",", "\n", ")", "\n", "def", "test_meta_estimate_intervals_using_valid_input_data", "(", "\n", "action_by_evaluation_policy", ",", "\n", "estimated_rewards_by_reg_model", ",", "\n", "description_1", ":", "str", ",", "\n", "alpha", ":", "float", ",", "\n", "n_bootstrap_samples", ":", "int", ",", "\n", "random_state", ":", "int", ",", "\n", "description_2", ":", "str", ",", "\n", "synthetic_continuous_bandit_feedback", ":", "BanditFeedback", ",", "\n", ")", "->", "None", ":", "\n", "    ", "\"\"\"\n    Test the response of estimate_intervals using valid data\n    \"\"\"", "\n", "# single ope estimator", "\n", "ope_", "=", "ContinuousOffPolicyEvaluation", "(", "\n", "bandit_feedback", "=", "synthetic_continuous_bandit_feedback", ",", "ope_estimators", "=", "[", "dr", "]", "\n", ")", "\n", "assert", "ope_", ".", "estimate_intervals", "(", "\n", "action_by_evaluation_policy", "=", "action_by_evaluation_policy", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", "alpha", "=", "alpha", ",", "\n", "n_bootstrap_samples", "=", "n_bootstrap_samples", ",", "\n", "random_state", "=", "random_state", ",", "\n", ")", "==", "{", "\n", "\"dr\"", ":", "mock_confidence_interval", "\n", "}", ",", "\"OffPolicyEvaluation.estimate_intervals ([DoublyRobust]) returns a wrong value\"", "\n", "# multiple ope estimators", "\n", "ope_", "=", "ContinuousOffPolicyEvaluation", "(", "\n", "bandit_feedback", "=", "synthetic_continuous_bandit_feedback", ",", "ope_estimators", "=", "[", "dr", ",", "ipw", "]", "\n", ")", "\n", "assert", "ope_", ".", "estimate_intervals", "(", "\n", "action_by_evaluation_policy", "=", "action_by_evaluation_policy", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", "alpha", "=", "alpha", ",", "\n", "n_bootstrap_samples", "=", "n_bootstrap_samples", ",", "\n", "random_state", "=", "random_state", ",", "\n", ")", "==", "{", "\n", "\"dr\"", ":", "mock_confidence_interval", ",", "\n", "\"ipw\"", ":", "{", "k", ":", "v", "+", "ipw", ".", "eps", "for", "k", ",", "v", "in", "mock_confidence_interval", ".", "items", "(", ")", "}", ",", "\n", "}", ",", "\"OffPolicyEvaluation.estimate_intervals ([DoublyRobust, IPW]) returns a wrong value\"", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_meta_continuous.test_meta_summarize_off_policy_estimates": [[527, 601], ["pytest.mark.parametrize", "pytest.mark.parametrize", "obp.ope.ContinuousOffPolicyEvaluation", "obp.ope.ContinuousOffPolicyEvaluation.summarize_off_policy_estimates", "copy.deepcopy", "numpy.zeros", "obp.ope.ContinuousOffPolicyEvaluation", "obp.ope.ContinuousOffPolicyEvaluation.summarize_off_policy_estimates", "pandas.DataFrame", "synthetic_continuous_bandit_feedback[].mean", "pandas.DataFrame", "pandas.testing.assert_frame_equal", "pandas.testing.assert_frame_equal", "pandas.DataFrame", "pandas.testing.assert_frame_equal", "mock_confidence_interval.items", "mock_confidence_interval.items"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta.OffPolicyEvaluation.summarize_off_policy_estimates", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta.OffPolicyEvaluation.summarize_off_policy_estimates"], ["", "@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"action_by_evaluation_policy, estimated_rewards_by_reg_model, description_1\"", ",", "\n", "valid_input_of_create_estimator_inputs", ",", "\n", ")", "\n", "@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"alpha, n_bootstrap_samples, random_state, description_2\"", ",", "\n", "valid_input_of_estimate_intervals", ",", "\n", ")", "\n", "def", "test_meta_summarize_off_policy_estimates", "(", "\n", "action_by_evaluation_policy", ",", "\n", "estimated_rewards_by_reg_model", ",", "\n", "description_1", ":", "str", ",", "\n", "alpha", ":", "float", ",", "\n", "n_bootstrap_samples", ":", "int", ",", "\n", "random_state", ":", "int", ",", "\n", "description_2", ":", "str", ",", "\n", "synthetic_continuous_bandit_feedback", ":", "BanditFeedback", ",", "\n", ")", "->", "None", ":", "\n", "    ", "\"\"\"\n    Test the response of summarize_off_policy_estimates using valid data\n    \"\"\"", "\n", "ope_", "=", "ContinuousOffPolicyEvaluation", "(", "\n", "bandit_feedback", "=", "synthetic_continuous_bandit_feedback", ",", "ope_estimators", "=", "[", "ipw", ",", "ipw3", "]", "\n", ")", "\n", "value", ",", "interval", "=", "ope_", ".", "summarize_off_policy_estimates", "(", "\n", "action_by_evaluation_policy", "=", "action_by_evaluation_policy", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", "alpha", "=", "alpha", ",", "\n", "n_bootstrap_samples", "=", "n_bootstrap_samples", ",", "\n", "random_state", "=", "random_state", ",", "\n", ")", "\n", "expected_value", "=", "pd", ".", "DataFrame", "(", "\n", "{", "\n", "\"ipw\"", ":", "mock_policy_value", "+", "ipw", ".", "eps", ",", "\n", "\"ipw3\"", ":", "mock_policy_value", "+", "ipw3", ".", "eps", ",", "\n", "}", ",", "\n", "index", "=", "[", "\"estimated_policy_value\"", "]", ",", "\n", ")", ".", "T", "\n", "expected_value", "[", "\"relative_estimated_policy_value\"", "]", "=", "(", "\n", "expected_value", "[", "\"estimated_policy_value\"", "]", "\n", "/", "synthetic_continuous_bandit_feedback", "[", "\"reward\"", "]", ".", "mean", "(", ")", "\n", ")", "\n", "expected_interval", "=", "pd", ".", "DataFrame", "(", "\n", "{", "\n", "\"ipw\"", ":", "{", "k", ":", "v", "+", "ipw", ".", "eps", "for", "k", ",", "v", "in", "mock_confidence_interval", ".", "items", "(", ")", "}", ",", "\n", "\"ipw3\"", ":", "{", "k", ":", "v", "+", "ipw3", ".", "eps", "for", "k", ",", "v", "in", "mock_confidence_interval", ".", "items", "(", ")", "}", ",", "\n", "}", "\n", ")", ".", "T", "\n", "assert_frame_equal", "(", "value", ",", "expected_value", ")", ",", "\"Invalid summarization (policy value)\"", "\n", "assert_frame_equal", "(", "interval", ",", "expected_interval", ")", ",", "\"Invalid summarization (interval)\"", "\n", "# check relative estimated policy value when the average of bandit_feedback[\"reward\"] is zero", "\n", "zero_reward_bandit_feedback", "=", "deepcopy", "(", "synthetic_continuous_bandit_feedback", ")", "\n", "zero_reward_bandit_feedback", "[", "\"reward\"", "]", "=", "np", ".", "zeros", "(", "\n", "zero_reward_bandit_feedback", "[", "\"reward\"", "]", ".", "shape", "[", "0", "]", "\n", ")", "\n", "ope_", "=", "ContinuousOffPolicyEvaluation", "(", "\n", "bandit_feedback", "=", "zero_reward_bandit_feedback", ",", "ope_estimators", "=", "[", "ipw", ",", "ipw3", "]", "\n", ")", "\n", "value", ",", "_", "=", "ope_", ".", "summarize_off_policy_estimates", "(", "\n", "action_by_evaluation_policy", "=", "action_by_evaluation_policy", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", "alpha", "=", "alpha", ",", "\n", "n_bootstrap_samples", "=", "n_bootstrap_samples", ",", "\n", "random_state", "=", "random_state", ",", "\n", ")", "\n", "expected_value", "=", "pd", ".", "DataFrame", "(", "\n", "{", "\n", "\"ipw\"", ":", "mock_policy_value", "+", "ipw", ".", "eps", ",", "\n", "\"ipw3\"", ":", "mock_policy_value", "+", "ipw3", ".", "eps", ",", "\n", "}", ",", "\n", "index", "=", "[", "\"estimated_policy_value\"", "]", ",", "\n", ")", ".", "T", "\n", "expected_value", "[", "\"relative_estimated_policy_value\"", "]", "=", "np", ".", "nan", "\n", "assert_frame_equal", "(", "value", ",", "expected_value", ")", ",", "\"Invalid summarization (policy value)\"", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_meta_continuous.test_meta_evaluate_performance_of_estimators_using_invalid_input_data": [[631, 669], ["pytest.mark.parametrize", "pytest.mark.parametrize", "obp.ope.ContinuousOffPolicyEvaluation", "pytest.raises", "obp.ope.ContinuousOffPolicyEvaluation.evaluate_performance_of_estimators", "pytest.raises", "obp.ope.ContinuousOffPolicyEvaluation.summarize_estimators_comparison"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta.OffPolicyEvaluation.evaluate_performance_of_estimators", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta.OffPolicyEvaluation.summarize_estimators_comparison"], ["@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"action_by_evaluation_policy, estimated_rewards_by_reg_model, description_1\"", ",", "\n", "valid_input_of_create_estimator_inputs", ",", "\n", ")", "\n", "@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"metric, ground_truth_policy_value, err, description_2\"", ",", "\n", "invalid_input_of_evaluation_performance_of_estimators", ",", "\n", ")", "\n", "def", "test_meta_evaluate_performance_of_estimators_using_invalid_input_data", "(", "\n", "action_by_evaluation_policy", ",", "\n", "estimated_rewards_by_reg_model", ",", "\n", "description_1", ":", "str", ",", "\n", "metric", ",", "\n", "ground_truth_policy_value", ",", "\n", "err", ",", "\n", "description_2", ":", "str", ",", "\n", "synthetic_continuous_bandit_feedback", ":", "BanditFeedback", ",", "\n", ")", "->", "None", ":", "\n", "    ", "\"\"\"\n    Test the response of evaluate_performance_of_estimators using invalid data\n    \"\"\"", "\n", "ope_", "=", "ContinuousOffPolicyEvaluation", "(", "\n", "bandit_feedback", "=", "synthetic_continuous_bandit_feedback", ",", "ope_estimators", "=", "[", "dr", "]", "\n", ")", "\n", "with", "pytest", ".", "raises", "(", "err", ",", "match", "=", "f\"{description_2}*\"", ")", ":", "\n", "        ", "_", "=", "ope_", ".", "evaluate_performance_of_estimators", "(", "\n", "ground_truth_policy_value", "=", "ground_truth_policy_value", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", "action_by_evaluation_policy", "=", "action_by_evaluation_policy", ",", "\n", "metric", "=", "metric", ",", "\n", ")", "\n", "# estimate_intervals function is called in summarize_off_policy_estimates", "\n", "", "with", "pytest", ".", "raises", "(", "err", ",", "match", "=", "f\"{description_2}*\"", ")", ":", "\n", "        ", "_", "=", "ope_", ".", "summarize_estimators_comparison", "(", "\n", "ground_truth_policy_value", "=", "ground_truth_policy_value", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", "action_by_evaluation_policy", "=", "action_by_evaluation_policy", ",", "\n", "metric", "=", "metric", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_meta_continuous.test_meta_evaluate_performance_of_estimators_using_valid_input_data": [[672, 732], ["pytest.mark.parametrize", "pytest.mark.parametrize", "obp.ope.ContinuousOffPolicyEvaluation", "obp.ope.ContinuousOffPolicyEvaluation.evaluate_performance_of_estimators", "ope_.evaluate_performance_of_estimators.items", "obp.ope.ContinuousOffPolicyEvaluation.summarize_estimators_comparison", "pandas.testing.assert_frame_equal", "numpy.abs", "numpy.abs", "pandas.DataFrame"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta.OffPolicyEvaluation.evaluate_performance_of_estimators", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta.OffPolicyEvaluation.summarize_estimators_comparison"], ["", "", "@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"action_by_evaluation_policy, estimated_rewards_by_reg_model, description_1\"", ",", "\n", "valid_input_of_create_estimator_inputs", ",", "\n", ")", "\n", "@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"metric, ground_truth_policy_value, description_2\"", ",", "\n", "valid_input_of_evaluation_performance_of_estimators", ",", "\n", ")", "\n", "def", "test_meta_evaluate_performance_of_estimators_using_valid_input_data", "(", "\n", "action_by_evaluation_policy", ",", "\n", "estimated_rewards_by_reg_model", ",", "\n", "description_1", ":", "str", ",", "\n", "metric", ",", "\n", "ground_truth_policy_value", ",", "\n", "description_2", ":", "str", ",", "\n", "synthetic_continuous_bandit_feedback", ":", "BanditFeedback", ",", "\n", ")", "->", "None", ":", "\n", "    ", "\"\"\"\n    Test the response of evaluate_performance_of_estimators using valid data\n    \"\"\"", "\n", "if", "metric", "==", "\"relative-ee\"", ":", "\n", "# calculate relative-ee", "\n", "        ", "eval_metric_ope_dict", "=", "{", "\n", "\"ipw\"", ":", "np", ".", "abs", "(", "\n", "(", "mock_policy_value", "+", "ipw", ".", "eps", "-", "ground_truth_policy_value", ")", "\n", "/", "ground_truth_policy_value", "\n", ")", ",", "\n", "\"ipw3\"", ":", "np", ".", "abs", "(", "\n", "(", "mock_policy_value", "+", "ipw3", ".", "eps", "-", "ground_truth_policy_value", ")", "\n", "/", "ground_truth_policy_value", "\n", ")", ",", "\n", "}", "\n", "", "else", ":", "\n", "# calculate se", "\n", "        ", "eval_metric_ope_dict", "=", "{", "\n", "\"ipw\"", ":", "(", "mock_policy_value", "+", "ipw", ".", "eps", "-", "ground_truth_policy_value", ")", "**", "2", ",", "\n", "\"ipw3\"", ":", "(", "mock_policy_value", "+", "ipw3", ".", "eps", "-", "ground_truth_policy_value", ")", "**", "2", ",", "\n", "}", "\n", "# check performance estimators", "\n", "", "ope_", "=", "ContinuousOffPolicyEvaluation", "(", "\n", "bandit_feedback", "=", "synthetic_continuous_bandit_feedback", ",", "ope_estimators", "=", "[", "ipw", ",", "ipw3", "]", "\n", ")", "\n", "performance", "=", "ope_", ".", "evaluate_performance_of_estimators", "(", "\n", "ground_truth_policy_value", "=", "ground_truth_policy_value", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", "action_by_evaluation_policy", "=", "action_by_evaluation_policy", ",", "\n", "metric", "=", "metric", ",", "\n", ")", "\n", "for", "k", ",", "v", "in", "performance", ".", "items", "(", ")", ":", "\n", "        ", "assert", "k", "in", "eval_metric_ope_dict", ",", "\"Invalid key of performance response\"", "\n", "assert", "v", "==", "eval_metric_ope_dict", "[", "k", "]", ",", "\"Invalid value of performance response\"", "\n", "", "performance_df", "=", "ope_", ".", "summarize_estimators_comparison", "(", "\n", "ground_truth_policy_value", "=", "ground_truth_policy_value", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", "action_by_evaluation_policy", "=", "action_by_evaluation_policy", ",", "\n", "metric", "=", "metric", ",", "\n", ")", "\n", "assert_frame_equal", "(", "\n", "performance_df", ",", "pd", ".", "DataFrame", "(", "eval_metric_ope_dict", ",", "index", "=", "[", "metric", "]", ")", ".", "T", "\n", ")", ",", "\"Invalid summarization (performance)\"", "\n", "", ""]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_ipw_estimators_slate.test_slate_estimators_using_invalid_input_data": [[105, 197], ["pytest.mark.parametrize", "pytest.raises", "sips.estimate_policy_value", "sips.estimate_interval", "iips.estimate_policy_value", "iips.estimate_interval", "rips.estimate_policy_value", "rips.estimate_interval", "snsips.estimate_policy_value", "snsips.estimate_interval", "sniips.estimate_policy_value", "sniips.estimate_interval", "snrips.estimate_policy_value", "snrips.estimate_interval"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_policy_value", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_interval", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_policy_value", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_interval", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_policy_value", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_interval", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_policy_value", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_interval", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_policy_value", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_interval", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_policy_value", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_interval"], ["@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"slate_id, reward, pscore, position, evaluation_policy_pscore, description\"", ",", "\n", "invalid_input_of_slate_estimators", ",", "\n", ")", "\n", "def", "test_slate_estimators_using_invalid_input_data", "(", "\n", "slate_id", ",", "reward", ",", "pscore", ",", "position", ",", "evaluation_policy_pscore", ",", "description", "\n", ")", "->", "None", ":", "\n", "    ", "with", "pytest", ".", "raises", "(", "ValueError", ",", "match", "=", "f\"{description}*\"", ")", ":", "\n", "        ", "_", "=", "sips", ".", "estimate_policy_value", "(", "\n", "slate_id", "=", "slate_id", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore", "=", "pscore", ",", "\n", "position", "=", "position", ",", "\n", "evaluation_policy_pscore", "=", "evaluation_policy_pscore", ",", "\n", ")", "\n", "_", "=", "sips", ".", "estimate_interval", "(", "\n", "slate_id", "=", "slate_id", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore", "=", "pscore", ",", "\n", "position", "=", "position", ",", "\n", "evaluation_policy_pscore", "=", "evaluation_policy_pscore", ",", "\n", ")", "\n", "_", "=", "iips", ".", "estimate_policy_value", "(", "\n", "slate_id", "=", "slate_id", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore_item_position", "=", "pscore", ",", "\n", "position", "=", "position", ",", "\n", "evaluation_policy_pscore_item_position", "=", "evaluation_policy_pscore", ",", "\n", ")", "\n", "_", "=", "iips", ".", "estimate_interval", "(", "\n", "slate_id", "=", "slate_id", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore_item_position", "=", "pscore", ",", "\n", "position", "=", "position", ",", "\n", "evaluation_policy_pscore_item_position", "=", "evaluation_policy_pscore", ",", "\n", ")", "\n", "_", "=", "rips", ".", "estimate_policy_value", "(", "\n", "slate_id", "=", "slate_id", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore_cascade", "=", "pscore", ",", "\n", "position", "=", "position", ",", "\n", "evaluation_policy_pscore_cascade", "=", "evaluation_policy_pscore", ",", "\n", ")", "\n", "_", "=", "rips", ".", "estimate_interval", "(", "\n", "slate_id", "=", "slate_id", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore_cascade", "=", "pscore", ",", "\n", "position", "=", "position", ",", "\n", "evaluation_policy_pscore_cascade", "=", "evaluation_policy_pscore", ",", "\n", ")", "\n", "# self normalized", "\n", "_", "=", "snsips", ".", "estimate_policy_value", "(", "\n", "slate_id", "=", "slate_id", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore", "=", "pscore", ",", "\n", "position", "=", "position", ",", "\n", "evaluation_policy_pscore", "=", "evaluation_policy_pscore", ",", "\n", ")", "\n", "_", "=", "snsips", ".", "estimate_interval", "(", "\n", "slate_id", "=", "slate_id", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore", "=", "pscore", ",", "\n", "position", "=", "position", ",", "\n", "evaluation_policy_pscore", "=", "evaluation_policy_pscore", ",", "\n", ")", "\n", "_", "=", "sniips", ".", "estimate_policy_value", "(", "\n", "slate_id", "=", "slate_id", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore_item_position", "=", "pscore", ",", "\n", "position", "=", "position", ",", "\n", "evaluation_policy_pscore_item_position", "=", "evaluation_policy_pscore", ",", "\n", ")", "\n", "_", "=", "sniips", ".", "estimate_interval", "(", "\n", "slate_id", "=", "slate_id", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore_item_position", "=", "pscore", ",", "\n", "position", "=", "position", ",", "\n", "evaluation_policy_pscore_item_position", "=", "evaluation_policy_pscore", ",", "\n", ")", "\n", "_", "=", "snrips", ".", "estimate_policy_value", "(", "\n", "slate_id", "=", "slate_id", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore_cascade", "=", "pscore", ",", "\n", "position", "=", "position", ",", "\n", "evaluation_policy_pscore_cascade", "=", "evaluation_policy_pscore", ",", "\n", ")", "\n", "_", "=", "snrips", ".", "estimate_interval", "(", "\n", "slate_id", "=", "slate_id", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore_cascade", "=", "pscore", ",", "\n", "position", "=", "position", ",", "\n", "evaluation_policy_pscore_cascade", "=", "evaluation_policy_pscore", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_ipw_estimators_slate.test_slate_estimators_using_valid_input_data": [[222, 313], ["pytest.mark.parametrize", "sips.estimate_policy_value", "sips.estimate_interval", "iips.estimate_policy_value", "iips.estimate_interval", "rips.estimate_policy_value", "rips.estimate_interval", "snsips.estimate_policy_value", "snsips.estimate_interval", "sniips.estimate_policy_value", "sniips.estimate_interval", "snrips.estimate_policy_value", "snrips.estimate_interval"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_policy_value", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_interval", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_policy_value", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_interval", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_policy_value", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_interval", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_policy_value", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_interval", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_policy_value", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_interval", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_policy_value", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_interval"], ["@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"slate_id, reward, pscore, position, evaluation_policy_pscore, description\"", ",", "\n", "valid_input_of_slate_estimators", ",", "\n", ")", "\n", "def", "test_slate_estimators_using_valid_input_data", "(", "\n", "slate_id", ",", "reward", ",", "pscore", ",", "position", ",", "evaluation_policy_pscore", ",", "description", "\n", ")", "->", "None", ":", "\n", "    ", "_", "=", "sips", ".", "estimate_policy_value", "(", "\n", "slate_id", "=", "slate_id", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore", "=", "pscore", ",", "\n", "position", "=", "position", ",", "\n", "evaluation_policy_pscore", "=", "evaluation_policy_pscore", ",", "\n", ")", "\n", "_", "=", "sips", ".", "estimate_interval", "(", "\n", "slate_id", "=", "slate_id", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore", "=", "pscore", ",", "\n", "position", "=", "position", ",", "\n", "evaluation_policy_pscore", "=", "evaluation_policy_pscore", ",", "\n", ")", "\n", "_", "=", "iips", ".", "estimate_policy_value", "(", "\n", "slate_id", "=", "slate_id", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore_item_position", "=", "pscore", ",", "\n", "position", "=", "position", ",", "\n", "evaluation_policy_pscore_item_position", "=", "evaluation_policy_pscore", ",", "\n", ")", "\n", "_", "=", "iips", ".", "estimate_interval", "(", "\n", "slate_id", "=", "slate_id", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore_item_position", "=", "pscore", ",", "\n", "position", "=", "position", ",", "\n", "evaluation_policy_pscore_item_position", "=", "evaluation_policy_pscore", ",", "\n", ")", "\n", "_", "=", "rips", ".", "estimate_policy_value", "(", "\n", "slate_id", "=", "slate_id", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore_cascade", "=", "pscore", ",", "\n", "position", "=", "position", ",", "\n", "evaluation_policy_pscore_cascade", "=", "evaluation_policy_pscore", ",", "\n", ")", "\n", "_", "=", "rips", ".", "estimate_interval", "(", "\n", "slate_id", "=", "slate_id", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore_cascade", "=", "pscore", ",", "\n", "position", "=", "position", ",", "\n", "evaluation_policy_pscore_cascade", "=", "evaluation_policy_pscore", ",", "\n", ")", "\n", "# self normalized", "\n", "_", "=", "snsips", ".", "estimate_policy_value", "(", "\n", "slate_id", "=", "slate_id", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore", "=", "pscore", ",", "\n", "position", "=", "position", ",", "\n", "evaluation_policy_pscore", "=", "evaluation_policy_pscore", ",", "\n", ")", "\n", "_", "=", "snsips", ".", "estimate_interval", "(", "\n", "slate_id", "=", "slate_id", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore", "=", "pscore", ",", "\n", "position", "=", "position", ",", "\n", "evaluation_policy_pscore", "=", "evaluation_policy_pscore", ",", "\n", ")", "\n", "_", "=", "sniips", ".", "estimate_policy_value", "(", "\n", "slate_id", "=", "slate_id", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore_item_position", "=", "pscore", ",", "\n", "position", "=", "position", ",", "\n", "evaluation_policy_pscore_item_position", "=", "evaluation_policy_pscore", ",", "\n", ")", "\n", "_", "=", "sniips", ".", "estimate_interval", "(", "\n", "slate_id", "=", "slate_id", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore_item_position", "=", "pscore", ",", "\n", "position", "=", "position", ",", "\n", "evaluation_policy_pscore_item_position", "=", "evaluation_policy_pscore", ",", "\n", ")", "\n", "_", "=", "snrips", ".", "estimate_policy_value", "(", "\n", "slate_id", "=", "slate_id", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore_cascade", "=", "pscore", ",", "\n", "position", "=", "position", ",", "\n", "evaluation_policy_pscore_cascade", "=", "evaluation_policy_pscore", ",", "\n", ")", "\n", "_", "=", "snrips", ".", "estimate_interval", "(", "\n", "slate_id", "=", "slate_id", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore_cascade", "=", "pscore", ",", "\n", "position", "=", "position", ",", "\n", "evaluation_policy_pscore_cascade", "=", "evaluation_policy_pscore", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_ipw_estimators_slate.test_sips_using_invalid_input_data": [[409, 445], ["pytest.mark.parametrize", "pytest.raises", "sips.estimate_policy_value", "sips.estimate_interval", "snsips.estimate_policy_value", "snsips.estimate_interval"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_policy_value", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_interval", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_policy_value", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_interval"], ["@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"slate_id, reward, pscore, position, evaluation_policy_pscore, description\"", ",", "\n", "invalid_input_of_sips", ",", "\n", ")", "\n", "def", "test_sips_using_invalid_input_data", "(", "\n", "slate_id", ",", "reward", ",", "pscore", ",", "position", ",", "evaluation_policy_pscore", ",", "description", "\n", ")", "->", "None", ":", "\n", "    ", "with", "pytest", ".", "raises", "(", "ValueError", ",", "match", "=", "f\"{description}*\"", ")", ":", "\n", "        ", "_", "=", "sips", ".", "estimate_policy_value", "(", "\n", "slate_id", "=", "slate_id", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore", "=", "pscore", ",", "\n", "position", "=", "position", ",", "\n", "evaluation_policy_pscore", "=", "evaluation_policy_pscore", ",", "\n", ")", "\n", "_", "=", "sips", ".", "estimate_interval", "(", "\n", "slate_id", "=", "slate_id", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore", "=", "pscore", ",", "\n", "position", "=", "position", ",", "\n", "evaluation_policy_pscore", "=", "evaluation_policy_pscore", ",", "\n", ")", "\n", "# self normalized", "\n", "_", "=", "snsips", ".", "estimate_policy_value", "(", "\n", "slate_id", "=", "slate_id", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore", "=", "pscore", ",", "\n", "position", "=", "position", ",", "\n", "evaluation_policy_pscore", "=", "evaluation_policy_pscore", ",", "\n", ")", "\n", "_", "=", "snsips", ".", "estimate_interval", "(", "\n", "slate_id", "=", "slate_id", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore", "=", "pscore", ",", "\n", "position", "=", "position", ",", "\n", "evaluation_policy_pscore", "=", "evaluation_policy_pscore", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_ipw_estimators_slate.test_iips_using_invalid_input_data": [[525, 566], ["pytest.mark.parametrize", "pytest.raises", "iips.estimate_policy_value", "iips.estimate_interval", "sniips.estimate_policy_value", "sniips.estimate_interval"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_policy_value", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_interval", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_policy_value", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_interval"], ["@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"slate_id, reward, pscore_item_position, position, evaluation_policy_pscore_item_position, description\"", ",", "\n", "invalid_input_of_iips", ",", "\n", ")", "\n", "def", "test_iips_using_invalid_input_data", "(", "\n", "slate_id", ",", "\n", "reward", ",", "\n", "pscore_item_position", ",", "\n", "position", ",", "\n", "evaluation_policy_pscore_item_position", ",", "\n", "description", ",", "\n", ")", "->", "None", ":", "\n", "    ", "with", "pytest", ".", "raises", "(", "ValueError", ",", "match", "=", "f\"{description}*\"", ")", ":", "\n", "        ", "_", "=", "iips", ".", "estimate_policy_value", "(", "\n", "slate_id", "=", "slate_id", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore_item_position", "=", "pscore_item_position", ",", "\n", "position", "=", "position", ",", "\n", "evaluation_policy_pscore_item_position", "=", "evaluation_policy_pscore_item_position", ",", "\n", ")", "\n", "_", "=", "iips", ".", "estimate_interval", "(", "\n", "slate_id", "=", "slate_id", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore_item_position", "=", "pscore_item_position", ",", "\n", "position", "=", "position", ",", "\n", "evaluation_policy_pscore_item_position", "=", "evaluation_policy_pscore_item_position", ",", "\n", ")", "\n", "# self normalized", "\n", "_", "=", "sniips", ".", "estimate_policy_value", "(", "\n", "slate_id", "=", "slate_id", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore_item_position", "=", "pscore_item_position", ",", "\n", "position", "=", "position", ",", "\n", "evaluation_policy_pscore_item_position", "=", "evaluation_policy_pscore_item_position", ",", "\n", ")", "\n", "_", "=", "sniips", ".", "estimate_interval", "(", "\n", "slate_id", "=", "slate_id", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore_item_position", "=", "pscore_item_position", ",", "\n", "position", "=", "position", ",", "\n", "evaluation_policy_pscore_item_position", "=", "evaluation_policy_pscore_item_position", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_ipw_estimators_slate.test_rips_using_invalid_input_data": [[662, 703], ["pytest.mark.parametrize", "pytest.raises", "rips.estimate_policy_value", "rips.estimate_interval", "snrips.estimate_policy_value", "snrips.estimate_interval"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_policy_value", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_interval", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_policy_value", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_interval"], ["@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"slate_id, reward, pscore_cascade, position, evaluation_policy_pscore_cascade, description\"", ",", "\n", "invalid_input_of_rips", ",", "\n", ")", "\n", "def", "test_rips_using_invalid_input_data", "(", "\n", "slate_id", ",", "\n", "reward", ",", "\n", "pscore_cascade", ",", "\n", "position", ",", "\n", "evaluation_policy_pscore_cascade", ",", "\n", "description", ",", "\n", ")", "->", "None", ":", "\n", "    ", "with", "pytest", ".", "raises", "(", "ValueError", ",", "match", "=", "f\"{description}*\"", ")", ":", "\n", "        ", "_", "=", "rips", ".", "estimate_policy_value", "(", "\n", "slate_id", "=", "slate_id", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore_cascade", "=", "pscore_cascade", ",", "\n", "position", "=", "position", ",", "\n", "evaluation_policy_pscore_cascade", "=", "evaluation_policy_pscore_cascade", ",", "\n", ")", "\n", "_", "=", "rips", ".", "estimate_interval", "(", "\n", "slate_id", "=", "slate_id", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore_cascade", "=", "pscore_cascade", ",", "\n", "position", "=", "position", ",", "\n", "evaluation_policy_pscore_cascade", "=", "evaluation_policy_pscore_cascade", ",", "\n", ")", "\n", "# self normalized", "\n", "_", "=", "snrips", ".", "estimate_policy_value", "(", "\n", "slate_id", "=", "slate_id", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore_cascade", "=", "pscore_cascade", ",", "\n", "position", "=", "position", ",", "\n", "evaluation_policy_pscore_cascade", "=", "evaluation_policy_pscore_cascade", ",", "\n", ")", "\n", "_", "=", "snrips", ".", "estimate_interval", "(", "\n", "slate_id", "=", "slate_id", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore_cascade", "=", "pscore_cascade", ",", "\n", "position", "=", "position", ",", "\n", "evaluation_policy_pscore_cascade", "=", "evaluation_policy_pscore_cascade", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_ipw_estimators_slate.test_estimate_intervals_of_all_estimators_using_invalid_input_data": [[741, 823], ["pytest.mark.parametrize", "pytest.mark.parametrize", "pytest.raises", "sips.estimate_interval", "iips.estimate_interval", "rips.estimate_interval", "snsips.estimate_interval", "sniips.estimate_interval", "snrips.estimate_interval"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_interval", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_interval", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_interval", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_interval", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_interval", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_interval"], ["@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"slate_id, reward, pscore, position, evaluation_policy_pscore, description_1\"", ",", "\n", "valid_input_of_slate_estimators", ",", "\n", ")", "\n", "@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"alpha, n_bootstrap_samples, random_state, err, description_2\"", ",", "\n", "invalid_input_of_estimate_intervals", ",", "\n", ")", "\n", "def", "test_estimate_intervals_of_all_estimators_using_invalid_input_data", "(", "\n", "slate_id", ",", "\n", "reward", ",", "\n", "pscore", ",", "\n", "position", ",", "\n", "evaluation_policy_pscore", ",", "\n", "description_1", ",", "\n", "alpha", ",", "\n", "n_bootstrap_samples", ",", "\n", "random_state", ",", "\n", "err", ",", "\n", "description_2", ",", "\n", ")", "->", "None", ":", "\n", "    ", "with", "pytest", ".", "raises", "(", "err", ",", "match", "=", "f\"{description_2}*\"", ")", ":", "\n", "        ", "_", "=", "sips", ".", "estimate_interval", "(", "\n", "slate_id", "=", "slate_id", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore", "=", "pscore", ",", "\n", "position", "=", "position", ",", "\n", "evaluation_policy_pscore", "=", "evaluation_policy_pscore", ",", "\n", "alpha", "=", "alpha", ",", "\n", "n_bootstrap_samples", "=", "n_bootstrap_samples", ",", "\n", "random_state", "=", "random_state", ",", "\n", ")", "\n", "_", "=", "iips", ".", "estimate_interval", "(", "\n", "slate_id", "=", "slate_id", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore_item_position", "=", "pscore", ",", "\n", "position", "=", "position", ",", "\n", "evaluation_policy_pscore_item_position", "=", "evaluation_policy_pscore", ",", "\n", "alpha", "=", "alpha", ",", "\n", "n_bootstrap_samples", "=", "n_bootstrap_samples", ",", "\n", "random_state", "=", "random_state", ",", "\n", ")", "\n", "_", "=", "rips", ".", "estimate_interval", "(", "\n", "slate_id", "=", "slate_id", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore_cascade", "=", "pscore", ",", "\n", "position", "=", "position", ",", "\n", "evaluation_policy_pscore_cascade", "=", "evaluation_policy_pscore", ",", "\n", "alpha", "=", "alpha", ",", "\n", "n_bootstrap_samples", "=", "n_bootstrap_samples", ",", "\n", "random_state", "=", "random_state", ",", "\n", ")", "\n", "# self normalized", "\n", "_", "=", "snsips", ".", "estimate_interval", "(", "\n", "slate_id", "=", "slate_id", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore", "=", "pscore", ",", "\n", "position", "=", "position", ",", "\n", "evaluation_policy_pscore", "=", "evaluation_policy_pscore", ",", "\n", "alpha", "=", "alpha", ",", "\n", "n_bootstrap_samples", "=", "n_bootstrap_samples", ",", "\n", "random_state", "=", "random_state", ",", "\n", ")", "\n", "_", "=", "sniips", ".", "estimate_interval", "(", "\n", "slate_id", "=", "slate_id", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore_item_position", "=", "pscore", ",", "\n", "position", "=", "position", ",", "\n", "evaluation_policy_pscore_item_position", "=", "evaluation_policy_pscore", ",", "\n", "alpha", "=", "alpha", ",", "\n", "n_bootstrap_samples", "=", "n_bootstrap_samples", ",", "\n", "random_state", "=", "random_state", ",", "\n", ")", "\n", "_", "=", "snrips", ".", "estimate_interval", "(", "\n", "slate_id", "=", "slate_id", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore_cascade", "=", "pscore", ",", "\n", "position", "=", "position", ",", "\n", "evaluation_policy_pscore_cascade", "=", "evaluation_policy_pscore", ",", "\n", "alpha", "=", "alpha", ",", "\n", "n_bootstrap_samples", "=", "n_bootstrap_samples", ",", "\n", "random_state", "=", "random_state", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_ipw_estimators_slate.test_estimate_intervals_of_all_estimators_using_valid_input_data": [[826, 906], ["pytest.mark.parametrize", "pytest.mark.parametrize", "sips.estimate_interval", "iips.estimate_interval", "rips.estimate_interval", "snsips.estimate_interval", "sniips.estimate_interval", "snrips.estimate_interval"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_interval", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_interval", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_interval", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_interval", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_interval", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_interval"], ["", "", "@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"slate_id, reward, pscore, position, evaluation_policy_pscore, description_1\"", ",", "\n", "valid_input_of_slate_estimators", ",", "\n", ")", "\n", "@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"alpha, n_bootstrap_samples, random_state, description_2\"", ",", "\n", "valid_input_of_estimate_intervals", ",", "\n", ")", "\n", "def", "test_estimate_intervals_of_all_estimators_using_valid_input_data", "(", "\n", "slate_id", ",", "\n", "reward", ",", "\n", "pscore", ",", "\n", "position", ",", "\n", "evaluation_policy_pscore", ",", "\n", "description_1", ",", "\n", "alpha", ",", "\n", "n_bootstrap_samples", ",", "\n", "random_state", ",", "\n", "description_2", ",", "\n", ")", "->", "None", ":", "\n", "    ", "_", "=", "sips", ".", "estimate_interval", "(", "\n", "slate_id", "=", "slate_id", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore", "=", "pscore", ",", "\n", "position", "=", "position", ",", "\n", "evaluation_policy_pscore", "=", "evaluation_policy_pscore", ",", "\n", "alpha", "=", "alpha", ",", "\n", "n_bootstrap_samples", "=", "n_bootstrap_samples", ",", "\n", "random_state", "=", "random_state", ",", "\n", ")", "\n", "_", "=", "iips", ".", "estimate_interval", "(", "\n", "slate_id", "=", "slate_id", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore_item_position", "=", "pscore", ",", "\n", "position", "=", "position", ",", "\n", "evaluation_policy_pscore_item_position", "=", "evaluation_policy_pscore", ",", "\n", "alpha", "=", "alpha", ",", "\n", "n_bootstrap_samples", "=", "n_bootstrap_samples", ",", "\n", "random_state", "=", "random_state", ",", "\n", ")", "\n", "_", "=", "rips", ".", "estimate_interval", "(", "\n", "slate_id", "=", "slate_id", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore_cascade", "=", "pscore", ",", "\n", "position", "=", "position", ",", "\n", "evaluation_policy_pscore_cascade", "=", "evaluation_policy_pscore", ",", "\n", "alpha", "=", "alpha", ",", "\n", "n_bootstrap_samples", "=", "n_bootstrap_samples", ",", "\n", "random_state", "=", "random_state", ",", "\n", ")", "\n", "# self normalized", "\n", "_", "=", "snsips", ".", "estimate_interval", "(", "\n", "slate_id", "=", "slate_id", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore", "=", "pscore", ",", "\n", "position", "=", "position", ",", "\n", "evaluation_policy_pscore", "=", "evaluation_policy_pscore", ",", "\n", "alpha", "=", "alpha", ",", "\n", "n_bootstrap_samples", "=", "n_bootstrap_samples", ",", "\n", "random_state", "=", "random_state", ",", "\n", ")", "\n", "_", "=", "sniips", ".", "estimate_interval", "(", "\n", "slate_id", "=", "slate_id", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore_item_position", "=", "pscore", ",", "\n", "position", "=", "position", ",", "\n", "evaluation_policy_pscore_item_position", "=", "evaluation_policy_pscore", ",", "\n", "alpha", "=", "alpha", ",", "\n", "n_bootstrap_samples", "=", "n_bootstrap_samples", ",", "\n", "random_state", "=", "random_state", ",", "\n", ")", "\n", "_", "=", "snrips", ".", "estimate_interval", "(", "\n", "slate_id", "=", "slate_id", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore_cascade", "=", "pscore", ",", "\n", "position", "=", "position", ",", "\n", "evaluation_policy_pscore_cascade", "=", "evaluation_policy_pscore", ",", "\n", "alpha", "=", "alpha", ",", "\n", "n_bootstrap_samples", "=", "n_bootstrap_samples", ",", "\n", "random_state", "=", "random_state", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_ipw_estimators_slate.test_slate_ope_performance_using_cascade_additive_log": [[909, 1032], ["obp.dataset.SyntheticSlateBanditDataset", "obp.dataset.SyntheticSlateBanditDataset", "obp.dataset.SyntheticSlateBanditDataset.obtain_batch_bandit_feedback", "obp.dataset.SyntheticSlateBanditDataset.obtain_batch_bandit_feedback", "sips.estimate_policy_value", "iips.estimate_policy_value", "rips.estimate_policy_value", "snsips.estimate_policy_value", "sniips.estimate_policy_value", "snrips.estimate_policy_value", "random_behavior_feedback[].reshape().sum", "random_behavior_feedback[].reshape().sum.mean", "random_behavior_feedback[].reshape().sum.std", "print", "print", "numpy.sqrt", "print", "random_behavior_feedback[].reshape", "numpy.abs"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.multiclass.MultiClassToBanditReduction.obtain_batch_bandit_feedback", "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.multiclass.MultiClassToBanditReduction.obtain_batch_bandit_feedback", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_policy_value", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_policy_value", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_policy_value", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_policy_value", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_policy_value", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_policy_value"], ["", "def", "test_slate_ope_performance_using_cascade_additive_log", "(", ")", ":", "\n", "# set parameters", "\n", "    ", "n_unique_action", "=", "10", "\n", "len_list", "=", "3", "\n", "dim_context", "=", "2", "\n", "reward_type", "=", "\"binary\"", "\n", "random_state", "=", "12345", "\n", "n_rounds", "=", "1000", "\n", "reward_structure", "=", "\"cascade_additive\"", "\n", "click_model", "=", "None", "\n", "behavior_policy_function", "=", "linear_behavior_policy_logit", "\n", "reward_function", "=", "logistic_reward_function", "\n", "dataset", "=", "SyntheticSlateBanditDataset", "(", "\n", "n_unique_action", "=", "n_unique_action", ",", "\n", "len_list", "=", "len_list", ",", "\n", "dim_context", "=", "dim_context", ",", "\n", "reward_type", "=", "reward_type", ",", "\n", "reward_structure", "=", "reward_structure", ",", "\n", "click_model", "=", "click_model", ",", "\n", "random_state", "=", "random_state", ",", "\n", "behavior_policy_function", "=", "behavior_policy_function", ",", "\n", "base_reward_function", "=", "reward_function", ",", "\n", ")", "\n", "random_behavior_dataset", "=", "SyntheticSlateBanditDataset", "(", "\n", "n_unique_action", "=", "n_unique_action", ",", "\n", "len_list", "=", "len_list", ",", "\n", "dim_context", "=", "dim_context", ",", "\n", "reward_type", "=", "reward_type", ",", "\n", "reward_structure", "=", "reward_structure", ",", "\n", "click_model", "=", "click_model", ",", "\n", "random_state", "=", "random_state", ",", "\n", "behavior_policy_function", "=", "None", ",", "\n", "base_reward_function", "=", "reward_function", ",", "\n", ")", "\n", "# obtain feedback", "\n", "bandit_feedback", "=", "dataset", ".", "obtain_batch_bandit_feedback", "(", "n_rounds", "=", "n_rounds", ")", "\n", "slate_id", "=", "bandit_feedback", "[", "\"slate_id\"", "]", "\n", "reward", "=", "bandit_feedback", "[", "\"reward\"", "]", "\n", "pscore", "=", "bandit_feedback", "[", "\"pscore\"", "]", "\n", "pscore_item_position", "=", "bandit_feedback", "[", "\"pscore_item_position\"", "]", "\n", "pscore_cascade", "=", "bandit_feedback", "[", "\"pscore_cascade\"", "]", "\n", "position", "=", "bandit_feedback", "[", "\"position\"", "]", "\n", "\n", "# obtain random behavior feedback", "\n", "random_behavior_feedback", "=", "random_behavior_dataset", ".", "obtain_batch_bandit_feedback", "(", "\n", "n_rounds", "=", "n_rounds", "\n", ")", "\n", "\n", "sips_estimated_policy_value", "=", "sips", ".", "estimate_policy_value", "(", "\n", "slate_id", "=", "slate_id", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore", "=", "pscore", ",", "\n", "position", "=", "position", ",", "\n", "evaluation_policy_pscore", "=", "random_behavior_feedback", "[", "\"pscore\"", "]", ",", "\n", ")", "\n", "iips_estimated_policy_value", "=", "iips", ".", "estimate_policy_value", "(", "\n", "slate_id", "=", "slate_id", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore_item_position", "=", "pscore_item_position", ",", "\n", "position", "=", "position", ",", "\n", "evaluation_policy_pscore_item_position", "=", "random_behavior_feedback", "[", "\n", "\"pscore_item_position\"", "\n", "]", ",", "\n", ")", "\n", "rips_estimated_policy_value", "=", "rips", ".", "estimate_policy_value", "(", "\n", "slate_id", "=", "slate_id", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore_cascade", "=", "pscore_cascade", ",", "\n", "position", "=", "position", ",", "\n", "evaluation_policy_pscore_cascade", "=", "random_behavior_feedback", "[", "\"pscore_cascade\"", "]", ",", "\n", ")", "\n", "# self normalized", "\n", "snsips_estimated_policy_value", "=", "snsips", ".", "estimate_policy_value", "(", "\n", "slate_id", "=", "slate_id", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore", "=", "pscore", ",", "\n", "position", "=", "position", ",", "\n", "evaluation_policy_pscore", "=", "random_behavior_feedback", "[", "\"pscore\"", "]", ",", "\n", ")", "\n", "sniips_estimated_policy_value", "=", "sniips", ".", "estimate_policy_value", "(", "\n", "slate_id", "=", "slate_id", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore_item_position", "=", "pscore_item_position", ",", "\n", "position", "=", "position", ",", "\n", "evaluation_policy_pscore_item_position", "=", "random_behavior_feedback", "[", "\n", "\"pscore_item_position\"", "\n", "]", ",", "\n", ")", "\n", "snrips_estimated_policy_value", "=", "snrips", ".", "estimate_policy_value", "(", "\n", "slate_id", "=", "slate_id", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore_cascade", "=", "pscore_cascade", ",", "\n", "position", "=", "position", ",", "\n", "evaluation_policy_pscore_cascade", "=", "random_behavior_feedback", "[", "\"pscore_cascade\"", "]", ",", "\n", ")", "\n", "# compute statistics of ground truth policy value", "\n", "q_pi_e", "=", "(", "\n", "random_behavior_feedback", "[", "\"reward\"", "]", "\n", ".", "reshape", "(", "(", "n_rounds", ",", "dataset", ".", "len_list", ")", ")", "\n", ".", "sum", "(", "axis", "=", "1", ")", "\n", ")", "\n", "gt_mean", "=", "q_pi_e", ".", "mean", "(", ")", "\n", "gt_std", "=", "q_pi_e", ".", "std", "(", "ddof", "=", "1", ")", "\n", "print", "(", "\"Cascade additive\"", ")", "\n", "# check the performance of OPE", "\n", "ci_bound", "=", "gt_std", "*", "3", "/", "np", ".", "sqrt", "(", "q_pi_e", ".", "shape", "[", "0", "]", ")", "\n", "print", "(", "f\"gt_mean: {gt_mean}, 3 * gt_std / sqrt(n): {ci_bound}\"", ")", "\n", "estimated_policy_value", "=", "{", "\n", "\"sips\"", ":", "sips_estimated_policy_value", ",", "\n", "\"iips\"", ":", "iips_estimated_policy_value", ",", "\n", "\"rips\"", ":", "rips_estimated_policy_value", ",", "\n", "\"snsips\"", ":", "snsips_estimated_policy_value", ",", "\n", "\"sniips\"", ":", "sniips_estimated_policy_value", ",", "\n", "\"snrips\"", ":", "snrips_estimated_policy_value", ",", "\n", "}", "\n", "for", "key", "in", "estimated_policy_value", ":", "\n", "        ", "print", "(", "\n", "f\"estimated_value: {estimated_policy_value[key]} ------ estimator: {key}, \"", "\n", ")", "\n", "# test the performance of each estimator", "\n", "assert", "(", "\n", "np", ".", "abs", "(", "gt_mean", "-", "estimated_policy_value", "[", "key", "]", ")", "<=", "ci_bound", "\n", ")", ",", "f\"OPE of {key} did not work well (absolute error is greater than 3*sigma)\"", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_ipw_estimators_slate.test_slate_ope_performance_using_independent_log": [[1034, 1157], ["obp.dataset.SyntheticSlateBanditDataset", "obp.dataset.SyntheticSlateBanditDataset", "obp.dataset.SyntheticSlateBanditDataset.obtain_batch_bandit_feedback", "obp.dataset.SyntheticSlateBanditDataset.obtain_batch_bandit_feedback", "sips.estimate_policy_value", "iips.estimate_policy_value", "rips.estimate_policy_value", "snsips.estimate_policy_value", "sniips.estimate_policy_value", "snrips.estimate_policy_value", "random_behavior_feedback[].reshape().sum", "random_behavior_feedback[].reshape().sum.mean", "random_behavior_feedback[].reshape().sum.std", "print", "print", "numpy.sqrt", "print", "random_behavior_feedback[].reshape", "numpy.abs"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.multiclass.MultiClassToBanditReduction.obtain_batch_bandit_feedback", "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.multiclass.MultiClassToBanditReduction.obtain_batch_bandit_feedback", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_policy_value", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_policy_value", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_policy_value", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_policy_value", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_policy_value", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_policy_value"], ["", "", "def", "test_slate_ope_performance_using_independent_log", "(", ")", ":", "\n", "# set parameters", "\n", "    ", "n_unique_action", "=", "10", "\n", "len_list", "=", "3", "\n", "dim_context", "=", "2", "\n", "reward_type", "=", "\"binary\"", "\n", "random_state", "=", "12345", "\n", "n_rounds", "=", "1000", "\n", "reward_structure", "=", "\"independent\"", "\n", "click_model", "=", "None", "\n", "behavior_policy_function", "=", "linear_behavior_policy_logit", "\n", "reward_function", "=", "logistic_reward_function", "\n", "dataset", "=", "SyntheticSlateBanditDataset", "(", "\n", "n_unique_action", "=", "n_unique_action", ",", "\n", "len_list", "=", "len_list", ",", "\n", "dim_context", "=", "dim_context", ",", "\n", "reward_type", "=", "reward_type", ",", "\n", "reward_structure", "=", "reward_structure", ",", "\n", "click_model", "=", "click_model", ",", "\n", "random_state", "=", "random_state", ",", "\n", "behavior_policy_function", "=", "behavior_policy_function", ",", "\n", "base_reward_function", "=", "reward_function", ",", "\n", ")", "\n", "random_behavior_dataset", "=", "SyntheticSlateBanditDataset", "(", "\n", "n_unique_action", "=", "n_unique_action", ",", "\n", "len_list", "=", "len_list", ",", "\n", "dim_context", "=", "dim_context", ",", "\n", "reward_type", "=", "reward_type", ",", "\n", "reward_structure", "=", "reward_structure", ",", "\n", "click_model", "=", "click_model", ",", "\n", "random_state", "=", "random_state", ",", "\n", "behavior_policy_function", "=", "None", ",", "\n", "base_reward_function", "=", "reward_function", ",", "\n", ")", "\n", "# obtain feedback", "\n", "bandit_feedback", "=", "dataset", ".", "obtain_batch_bandit_feedback", "(", "n_rounds", "=", "n_rounds", ")", "\n", "slate_id", "=", "bandit_feedback", "[", "\"slate_id\"", "]", "\n", "reward", "=", "bandit_feedback", "[", "\"reward\"", "]", "\n", "pscore", "=", "bandit_feedback", "[", "\"pscore\"", "]", "\n", "pscore_item_position", "=", "bandit_feedback", "[", "\"pscore_item_position\"", "]", "\n", "pscore_cascade", "=", "bandit_feedback", "[", "\"pscore_cascade\"", "]", "\n", "position", "=", "bandit_feedback", "[", "\"position\"", "]", "\n", "\n", "# obtain random behavior feedback", "\n", "random_behavior_feedback", "=", "random_behavior_dataset", ".", "obtain_batch_bandit_feedback", "(", "\n", "n_rounds", "=", "n_rounds", "\n", ")", "\n", "\n", "sips_estimated_policy_value", "=", "sips", ".", "estimate_policy_value", "(", "\n", "slate_id", "=", "slate_id", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore", "=", "pscore", ",", "\n", "position", "=", "position", ",", "\n", "evaluation_policy_pscore", "=", "random_behavior_feedback", "[", "\"pscore\"", "]", ",", "\n", ")", "\n", "iips_estimated_policy_value", "=", "iips", ".", "estimate_policy_value", "(", "\n", "slate_id", "=", "slate_id", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore_item_position", "=", "pscore_item_position", ",", "\n", "position", "=", "position", ",", "\n", "evaluation_policy_pscore_item_position", "=", "random_behavior_feedback", "[", "\n", "\"pscore_item_position\"", "\n", "]", ",", "\n", ")", "\n", "rips_estimated_policy_value", "=", "rips", ".", "estimate_policy_value", "(", "\n", "slate_id", "=", "slate_id", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore_cascade", "=", "pscore_cascade", ",", "\n", "position", "=", "position", ",", "\n", "evaluation_policy_pscore_cascade", "=", "random_behavior_feedback", "[", "\"pscore_cascade\"", "]", ",", "\n", ")", "\n", "# self normalized", "\n", "snsips_estimated_policy_value", "=", "snsips", ".", "estimate_policy_value", "(", "\n", "slate_id", "=", "slate_id", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore", "=", "pscore", ",", "\n", "position", "=", "position", ",", "\n", "evaluation_policy_pscore", "=", "random_behavior_feedback", "[", "\"pscore\"", "]", ",", "\n", ")", "\n", "sniips_estimated_policy_value", "=", "sniips", ".", "estimate_policy_value", "(", "\n", "slate_id", "=", "slate_id", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore_item_position", "=", "pscore_item_position", ",", "\n", "position", "=", "position", ",", "\n", "evaluation_policy_pscore_item_position", "=", "random_behavior_feedback", "[", "\n", "\"pscore_item_position\"", "\n", "]", ",", "\n", ")", "\n", "snrips_estimated_policy_value", "=", "snrips", ".", "estimate_policy_value", "(", "\n", "slate_id", "=", "slate_id", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore_cascade", "=", "pscore_cascade", ",", "\n", "position", "=", "position", ",", "\n", "evaluation_policy_pscore_cascade", "=", "random_behavior_feedback", "[", "\"pscore_cascade\"", "]", ",", "\n", ")", "\n", "# compute statistics of ground truth policy value", "\n", "q_pi_e", "=", "(", "\n", "random_behavior_feedback", "[", "\"reward\"", "]", "\n", ".", "reshape", "(", "(", "n_rounds", ",", "dataset", ".", "len_list", ")", ")", "\n", ".", "sum", "(", "axis", "=", "1", ")", "\n", ")", "\n", "gt_mean", "=", "q_pi_e", ".", "mean", "(", ")", "\n", "gt_std", "=", "q_pi_e", ".", "std", "(", "ddof", "=", "1", ")", "\n", "print", "(", "\"Independent\"", ")", "\n", "# check the performance of OPE", "\n", "ci_bound", "=", "gt_std", "*", "3", "/", "np", ".", "sqrt", "(", "q_pi_e", ".", "shape", "[", "0", "]", ")", "\n", "print", "(", "f\"gt_mean: {gt_mean}, 3 * gt_std / sqrt(n): {ci_bound}\"", ")", "\n", "estimated_policy_value", "=", "{", "\n", "\"sips\"", ":", "sips_estimated_policy_value", ",", "\n", "\"iips\"", ":", "iips_estimated_policy_value", ",", "\n", "\"rips\"", ":", "rips_estimated_policy_value", ",", "\n", "\"snsips\"", ":", "snsips_estimated_policy_value", ",", "\n", "\"sniips\"", ":", "sniips_estimated_policy_value", ",", "\n", "\"snrips\"", ":", "snrips_estimated_policy_value", ",", "\n", "}", "\n", "for", "key", "in", "estimated_policy_value", ":", "\n", "        ", "print", "(", "\n", "f\"estimated_value: {estimated_policy_value[key]} ------ estimator: {key}, \"", "\n", ")", "\n", "# test the performance of each estimator", "\n", "assert", "(", "\n", "np", ".", "abs", "(", "gt_mean", "-", "estimated_policy_value", "[", "key", "]", ")", "<=", "ci_bound", "\n", ")", ",", "f\"OPE of {key} did not work well (absolute error is greater than 3*sigma)\"", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_ipw_estimators_slate.test_slate_ope_performance_using_standard_additive_log": [[1159, 1282], ["obp.dataset.SyntheticSlateBanditDataset", "obp.dataset.SyntheticSlateBanditDataset", "obp.dataset.SyntheticSlateBanditDataset.obtain_batch_bandit_feedback", "obp.dataset.SyntheticSlateBanditDataset.obtain_batch_bandit_feedback", "sips.estimate_policy_value", "iips.estimate_policy_value", "rips.estimate_policy_value", "snsips.estimate_policy_value", "sniips.estimate_policy_value", "snrips.estimate_policy_value", "random_behavior_feedback[].reshape().sum", "random_behavior_feedback[].reshape().sum.mean", "random_behavior_feedback[].reshape().sum.std", "print", "print", "numpy.sqrt", "print", "random_behavior_feedback[].reshape", "numpy.abs"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.multiclass.MultiClassToBanditReduction.obtain_batch_bandit_feedback", "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.multiclass.MultiClassToBanditReduction.obtain_batch_bandit_feedback", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_policy_value", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_policy_value", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_policy_value", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_policy_value", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_policy_value", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_policy_value"], ["", "", "def", "test_slate_ope_performance_using_standard_additive_log", "(", ")", ":", "\n", "# set parameters", "\n", "    ", "n_unique_action", "=", "10", "\n", "len_list", "=", "3", "\n", "dim_context", "=", "2", "\n", "reward_type", "=", "\"binary\"", "\n", "random_state", "=", "12345", "\n", "n_rounds", "=", "1000", "\n", "reward_structure", "=", "\"standard_additive\"", "\n", "click_model", "=", "None", "\n", "behavior_policy_function", "=", "linear_behavior_policy_logit", "\n", "reward_function", "=", "logistic_reward_function", "\n", "dataset", "=", "SyntheticSlateBanditDataset", "(", "\n", "n_unique_action", "=", "n_unique_action", ",", "\n", "len_list", "=", "len_list", ",", "\n", "dim_context", "=", "dim_context", ",", "\n", "reward_type", "=", "reward_type", ",", "\n", "reward_structure", "=", "reward_structure", ",", "\n", "click_model", "=", "click_model", ",", "\n", "random_state", "=", "random_state", ",", "\n", "behavior_policy_function", "=", "behavior_policy_function", ",", "\n", "base_reward_function", "=", "reward_function", ",", "\n", ")", "\n", "random_behavior_dataset", "=", "SyntheticSlateBanditDataset", "(", "\n", "n_unique_action", "=", "n_unique_action", ",", "\n", "len_list", "=", "len_list", ",", "\n", "dim_context", "=", "dim_context", ",", "\n", "reward_type", "=", "reward_type", ",", "\n", "reward_structure", "=", "reward_structure", ",", "\n", "click_model", "=", "click_model", ",", "\n", "random_state", "=", "random_state", ",", "\n", "behavior_policy_function", "=", "None", ",", "\n", "base_reward_function", "=", "reward_function", ",", "\n", ")", "\n", "# obtain feedback", "\n", "bandit_feedback", "=", "dataset", ".", "obtain_batch_bandit_feedback", "(", "n_rounds", "=", "n_rounds", ")", "\n", "slate_id", "=", "bandit_feedback", "[", "\"slate_id\"", "]", "\n", "reward", "=", "bandit_feedback", "[", "\"reward\"", "]", "\n", "pscore", "=", "bandit_feedback", "[", "\"pscore\"", "]", "\n", "pscore_item_position", "=", "bandit_feedback", "[", "\"pscore_item_position\"", "]", "\n", "pscore_cascade", "=", "bandit_feedback", "[", "\"pscore_cascade\"", "]", "\n", "position", "=", "bandit_feedback", "[", "\"position\"", "]", "\n", "\n", "# obtain random behavior feedback", "\n", "random_behavior_feedback", "=", "random_behavior_dataset", ".", "obtain_batch_bandit_feedback", "(", "\n", "n_rounds", "=", "n_rounds", "\n", ")", "\n", "\n", "sips_estimated_policy_value", "=", "sips", ".", "estimate_policy_value", "(", "\n", "slate_id", "=", "slate_id", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore", "=", "pscore", ",", "\n", "position", "=", "position", ",", "\n", "evaluation_policy_pscore", "=", "random_behavior_feedback", "[", "\"pscore\"", "]", ",", "\n", ")", "\n", "iips_estimated_policy_value", "=", "iips", ".", "estimate_policy_value", "(", "\n", "slate_id", "=", "slate_id", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore_item_position", "=", "pscore_item_position", ",", "\n", "position", "=", "position", ",", "\n", "evaluation_policy_pscore_item_position", "=", "random_behavior_feedback", "[", "\n", "\"pscore_item_position\"", "\n", "]", ",", "\n", ")", "\n", "rips_estimated_policy_value", "=", "rips", ".", "estimate_policy_value", "(", "\n", "slate_id", "=", "slate_id", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore_cascade", "=", "pscore_cascade", ",", "\n", "position", "=", "position", ",", "\n", "evaluation_policy_pscore_cascade", "=", "random_behavior_feedback", "[", "\"pscore_cascade\"", "]", ",", "\n", ")", "\n", "# self normalized", "\n", "snsips_estimated_policy_value", "=", "snsips", ".", "estimate_policy_value", "(", "\n", "slate_id", "=", "slate_id", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore", "=", "pscore", ",", "\n", "position", "=", "position", ",", "\n", "evaluation_policy_pscore", "=", "random_behavior_feedback", "[", "\"pscore\"", "]", ",", "\n", ")", "\n", "sniips_estimated_policy_value", "=", "sniips", ".", "estimate_policy_value", "(", "\n", "slate_id", "=", "slate_id", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore_item_position", "=", "pscore_item_position", ",", "\n", "position", "=", "position", ",", "\n", "evaluation_policy_pscore_item_position", "=", "random_behavior_feedback", "[", "\n", "\"pscore_item_position\"", "\n", "]", ",", "\n", ")", "\n", "snrips_estimated_policy_value", "=", "snrips", ".", "estimate_policy_value", "(", "\n", "slate_id", "=", "slate_id", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore_cascade", "=", "pscore_cascade", ",", "\n", "position", "=", "position", ",", "\n", "evaluation_policy_pscore_cascade", "=", "random_behavior_feedback", "[", "\"pscore_cascade\"", "]", ",", "\n", ")", "\n", "# compute statistics of ground truth policy value", "\n", "q_pi_e", "=", "(", "\n", "random_behavior_feedback", "[", "\"reward\"", "]", "\n", ".", "reshape", "(", "(", "n_rounds", ",", "dataset", ".", "len_list", ")", ")", "\n", ".", "sum", "(", "axis", "=", "1", ")", "\n", ")", "\n", "gt_mean", "=", "q_pi_e", ".", "mean", "(", ")", "\n", "gt_std", "=", "q_pi_e", ".", "std", "(", "ddof", "=", "1", ")", "\n", "print", "(", "\"Standard additive\"", ")", "\n", "# check the performance of OPE", "\n", "ci_bound", "=", "gt_std", "*", "3", "/", "np", ".", "sqrt", "(", "q_pi_e", ".", "shape", "[", "0", "]", ")", "\n", "print", "(", "f\"gt_mean: {gt_mean}, 3 * gt_std / sqrt(n): {ci_bound}\"", ")", "\n", "estimated_policy_value", "=", "{", "\n", "\"sips\"", ":", "sips_estimated_policy_value", ",", "\n", "\"iips\"", ":", "iips_estimated_policy_value", ",", "\n", "\"rips\"", ":", "rips_estimated_policy_value", ",", "\n", "\"snsips\"", ":", "snsips_estimated_policy_value", ",", "\n", "\"sniips\"", ":", "sniips_estimated_policy_value", ",", "\n", "\"snrips\"", ":", "snrips_estimated_policy_value", ",", "\n", "}", "\n", "for", "key", "in", "estimated_policy_value", ":", "\n", "        ", "print", "(", "\n", "f\"estimated_value: {estimated_policy_value[key]} ------ estimator: {key}, \"", "\n", ")", "\n", "# test the performance of each estimator", "\n", "assert", "(", "\n", "np", ".", "abs", "(", "gt_mean", "-", "estimated_policy_value", "[", "key", "]", ")", "<=", "ci_bound", "\n", ")", ",", "f\"OPE of {key} did not work well (absolute error is greater than 3*sigma)\"", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_ipw_estimators_slate.test_boundedness_of_slate_snipw_using_random_evaluation_policy": [[1284, 1370], ["obp.dataset.SyntheticSlateBanditDataset", "obp.dataset.SyntheticSlateBanditDataset", "obp.dataset.SyntheticSlateBanditDataset.obtain_batch_bandit_feedback", "obp.dataset.SyntheticSlateBanditDataset.obtain_batch_bandit_feedback", "snsips.estimate_policy_value", "sniips.estimate_policy_value", "snrips.estimate_policy_value"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.multiclass.MultiClassToBanditReduction.obtain_batch_bandit_feedback", "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.multiclass.MultiClassToBanditReduction.obtain_batch_bandit_feedback", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_policy_value", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_policy_value", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_policy_value"], ["", "", "def", "test_boundedness_of_slate_snipw_using_random_evaluation_policy", "(", ")", "->", "None", ":", "\n", "    ", "\"\"\"\n    Test the boundedness of snipw estimators using synthetic bandit data and random evaluation policy\n    \"\"\"", "\n", "# set parameters", "\n", "n_unique_action", "=", "10", "\n", "len_list", "=", "3", "\n", "dim_context", "=", "2", "\n", "reward_type", "=", "\"binary\"", "\n", "random_state", "=", "12345", "\n", "n_rounds", "=", "1000", "\n", "reward_structure", "=", "\"standard_additive\"", "\n", "click_model", "=", "None", "\n", "behavior_policy_function", "=", "linear_behavior_policy_logit", "\n", "reward_function", "=", "logistic_reward_function", "\n", "dataset", "=", "SyntheticSlateBanditDataset", "(", "\n", "n_unique_action", "=", "n_unique_action", ",", "\n", "len_list", "=", "len_list", ",", "\n", "dim_context", "=", "dim_context", ",", "\n", "reward_type", "=", "reward_type", ",", "\n", "reward_structure", "=", "reward_structure", ",", "\n", "click_model", "=", "click_model", ",", "\n", "random_state", "=", "random_state", ",", "\n", "behavior_policy_function", "=", "behavior_policy_function", ",", "\n", "base_reward_function", "=", "reward_function", ",", "\n", ")", "\n", "random_behavior_dataset", "=", "SyntheticSlateBanditDataset", "(", "\n", "n_unique_action", "=", "n_unique_action", ",", "\n", "len_list", "=", "len_list", ",", "\n", "dim_context", "=", "dim_context", ",", "\n", "reward_type", "=", "reward_type", ",", "\n", "reward_structure", "=", "reward_structure", ",", "\n", "click_model", "=", "click_model", ",", "\n", "random_state", "=", "random_state", ",", "\n", "behavior_policy_function", "=", "None", ",", "\n", "base_reward_function", "=", "reward_function", ",", "\n", ")", "\n", "# obtain feedback", "\n", "bandit_feedback", "=", "dataset", ".", "obtain_batch_bandit_feedback", "(", "n_rounds", "=", "n_rounds", ")", "\n", "slate_id", "=", "bandit_feedback", "[", "\"slate_id\"", "]", "\n", "reward", "=", "bandit_feedback", "[", "\"reward\"", "]", "\n", "# make pscore too small (to check the boundedness of snipw)", "\n", "pscore", "=", "bandit_feedback", "[", "\"pscore\"", "]", "**", "3", "\n", "pscore_item_position", "=", "bandit_feedback", "[", "\"pscore_item_position\"", "]", "**", "3", "\n", "pscore_cascade", "=", "bandit_feedback", "[", "\"pscore_cascade\"", "]", "**", "3", "\n", "position", "=", "bandit_feedback", "[", "\"position\"", "]", "\n", "\n", "# obtain random behavior feedback", "\n", "random_behavior_feedback", "=", "random_behavior_dataset", ".", "obtain_batch_bandit_feedback", "(", "\n", "n_rounds", "=", "n_rounds", "\n", ")", "\n", "\n", "# self normalized", "\n", "snsips_estimated_policy_value", "=", "snsips", ".", "estimate_policy_value", "(", "\n", "slate_id", "=", "slate_id", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore", "=", "pscore", ",", "\n", "position", "=", "position", ",", "\n", "evaluation_policy_pscore", "=", "random_behavior_feedback", "[", "\"pscore\"", "]", ",", "\n", ")", "\n", "sniips_estimated_policy_value", "=", "sniips", ".", "estimate_policy_value", "(", "\n", "slate_id", "=", "slate_id", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore_item_position", "=", "pscore_item_position", ",", "\n", "position", "=", "position", ",", "\n", "evaluation_policy_pscore_item_position", "=", "random_behavior_feedback", "[", "\n", "\"pscore_item_position\"", "\n", "]", ",", "\n", ")", "\n", "snrips_estimated_policy_value", "=", "snrips", ".", "estimate_policy_value", "(", "\n", "slate_id", "=", "slate_id", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore_cascade", "=", "pscore_cascade", ",", "\n", "position", "=", "position", ",", "\n", "evaluation_policy_pscore_cascade", "=", "random_behavior_feedback", "[", "\"pscore_cascade\"", "]", ",", "\n", ")", "\n", "\n", "estimated_policy_value", "=", "{", "\n", "\"snsips\"", ":", "snsips_estimated_policy_value", ",", "\n", "\"sniips\"", ":", "sniips_estimated_policy_value", ",", "\n", "\"snrips\"", ":", "snrips_estimated_policy_value", ",", "\n", "}", "\n", "for", "key", "in", "estimated_policy_value", ":", "\n", "        ", "assert", "(", "\n", "estimated_policy_value", "[", "key", "]", "<=", "len_list", "\n", ")", ",", "f\"estimated policy value of snipw should be smaller than or equal to {len_list} (because of its 1-boundedness for each position), but the value is: {estimated_policy_value}\"", "\n", "", "", ""]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_ipw_estimators_continuous.test_synthetic_init": [[12, 49], ["pytest.raises", "obp.ope.KernelizedInverseProbabilityWeighting", "pytest.raises", "obp.ope.KernelizedInverseProbabilityWeighting", "pytest.raises", "obp.ope.KernelizedSelfNormalizedInverseProbabilityWeighting", "pytest.raises", "obp.ope.KernelizedSelfNormalizedInverseProbabilityWeighting", "pytest.raises", "obp.ope.KernelizedInverseProbabilityWeighting", "pytest.raises", "obp.ope.KernelizedInverseProbabilityWeighting", "pytest.raises", "obp.ope.KernelizedInverseProbabilityWeighting", "pytest.raises", "obp.ope.KernelizedSelfNormalizedInverseProbabilityWeighting", "pytest.raises", "obp.ope.KernelizedSelfNormalizedInverseProbabilityWeighting", "pytest.raises", "obp.ope.KernelizedSelfNormalizedInverseProbabilityWeighting"], "function", ["None"], ["def", "test_synthetic_init", "(", ")", ":", "\n", "# kernel", "\n", "    ", "with", "pytest", ".", "raises", "(", "ValueError", ")", ":", "\n", "        ", "KernelizedInverseProbabilityWeighting", "(", "kernel", "=", "\"a\"", ",", "bandwidth", "=", "0.1", ")", "\n", "\n", "", "with", "pytest", ".", "raises", "(", "ValueError", ")", ":", "\n", "        ", "KernelizedInverseProbabilityWeighting", "(", "kernel", "=", "None", ",", "bandwidth", "=", "0.1", ")", "\n", "\n", "", "with", "pytest", ".", "raises", "(", "ValueError", ")", ":", "\n", "        ", "KernelizedSelfNormalizedInverseProbabilityWeighting", "(", "kernel", "=", "\"a\"", ",", "bandwidth", "=", "0.1", ")", "\n", "\n", "", "with", "pytest", ".", "raises", "(", "ValueError", ")", ":", "\n", "        ", "KernelizedSelfNormalizedInverseProbabilityWeighting", "(", "kernel", "=", "None", ",", "bandwidth", "=", "0.1", ")", "\n", "\n", "# bandwidth", "\n", "", "with", "pytest", ".", "raises", "(", "TypeError", ")", ":", "\n", "        ", "KernelizedInverseProbabilityWeighting", "(", "kernel", "=", "\"gaussian\"", ",", "bandwidth", "=", "\"a\"", ")", "\n", "\n", "", "with", "pytest", ".", "raises", "(", "TypeError", ")", ":", "\n", "        ", "KernelizedInverseProbabilityWeighting", "(", "kernel", "=", "\"gaussian\"", ",", "bandwidth", "=", "None", ")", "\n", "\n", "", "with", "pytest", ".", "raises", "(", "ValueError", ")", ":", "\n", "        ", "KernelizedInverseProbabilityWeighting", "(", "kernel", "=", "\"gaussian\"", ",", "bandwidth", "=", "-", "1.0", ")", "\n", "\n", "", "with", "pytest", ".", "raises", "(", "TypeError", ")", ":", "\n", "        ", "KernelizedSelfNormalizedInverseProbabilityWeighting", "(", "\n", "kernel", "=", "\"gaussian\"", ",", "bandwidth", "=", "\"a\"", "\n", ")", "\n", "\n", "", "with", "pytest", ".", "raises", "(", "TypeError", ")", ":", "\n", "        ", "KernelizedSelfNormalizedInverseProbabilityWeighting", "(", "\n", "kernel", "=", "\"gaussian\"", ",", "bandwidth", "=", "None", "\n", ")", "\n", "\n", "", "with", "pytest", ".", "raises", "(", "ValueError", ")", ":", "\n", "        ", "KernelizedSelfNormalizedInverseProbabilityWeighting", "(", "\n", "kernel", "=", "\"gaussian\"", ",", "bandwidth", "=", "-", "1.0", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_ipw_estimators_continuous.test_ipw_continuous_using_invalid_input_data": [[149, 187], ["pytest.mark.parametrize", "pytest.raises", "ipw.estimate_policy_value", "pytest.raises", "ipw.estimate_interval", "pytest.raises", "snipw.estimate_policy_value", "pytest.raises", "snipw.estimate_interval"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_policy_value", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_interval", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_policy_value", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_interval"], ["@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"action_by_evaluation_policy, action_by_behavior_policy, reward, pscore, description\"", ",", "\n", "invalid_input_of_ipw", ",", "\n", ")", "\n", "def", "test_ipw_continuous_using_invalid_input_data", "(", "\n", "action_by_evaluation_policy", ",", "\n", "action_by_behavior_policy", ",", "\n", "reward", ",", "\n", "pscore", ",", "\n", "description", ":", "str", ",", "\n", ")", "->", "None", ":", "\n", "    ", "with", "pytest", ".", "raises", "(", "ValueError", ",", "match", "=", "f\"{description}*\"", ")", ":", "\n", "        ", "_", "=", "ipw", ".", "estimate_policy_value", "(", "\n", "action_by_evaluation_policy", "=", "action_by_evaluation_policy", ",", "\n", "action_by_behavior_policy", "=", "action_by_behavior_policy", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore", "=", "pscore", ",", "\n", ")", "\n", "", "with", "pytest", ".", "raises", "(", "ValueError", ",", "match", "=", "f\"{description}*\"", ")", ":", "\n", "        ", "_", "=", "ipw", ".", "estimate_interval", "(", "\n", "action_by_evaluation_policy", "=", "action_by_evaluation_policy", ",", "\n", "action_by_behavior_policy", "=", "action_by_behavior_policy", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore", "=", "pscore", ",", "\n", ")", "\n", "", "with", "pytest", ".", "raises", "(", "ValueError", ",", "match", "=", "f\"{description}*\"", ")", ":", "\n", "        ", "_", "=", "snipw", ".", "estimate_policy_value", "(", "\n", "action_by_evaluation_policy", "=", "action_by_evaluation_policy", ",", "\n", "action_by_behavior_policy", "=", "action_by_behavior_policy", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore", "=", "pscore", ",", "\n", ")", "\n", "", "with", "pytest", ".", "raises", "(", "ValueError", ",", "match", "=", "f\"{description}*\"", ")", ":", "\n", "        ", "_", "=", "snipw", ".", "estimate_interval", "(", "\n", "action_by_evaluation_policy", "=", "action_by_evaluation_policy", ",", "\n", "action_by_behavior_policy", "=", "action_by_behavior_policy", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore", "=", "pscore", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_ipw_estimators_continuous.test_ipw_continuous_using_valid_input_data": [[203, 236], ["pytest.mark.parametrize", "ipw.estimate_policy_value", "ipw.estimate_interval", "snipw.estimate_policy_value", "snipw.estimate_interval"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_policy_value", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_interval", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_policy_value", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_interval"], ["@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"action_by_evaluation_policy, action_by_behavior_policy, reward, pscore\"", ",", "\n", "valid_input_of_ipw", ",", "\n", ")", "\n", "def", "test_ipw_continuous_using_valid_input_data", "(", "\n", "action_by_evaluation_policy", ",", "\n", "action_by_behavior_policy", ",", "\n", "reward", ",", "\n", "pscore", ",", "\n", ")", "->", "None", ":", "\n", "    ", "_", "=", "ipw", ".", "estimate_policy_value", "(", "\n", "action_by_evaluation_policy", "=", "action_by_evaluation_policy", ",", "\n", "action_by_behavior_policy", "=", "action_by_behavior_policy", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore", "=", "pscore", ",", "\n", ")", "\n", "_", "=", "ipw", ".", "estimate_interval", "(", "\n", "action_by_evaluation_policy", "=", "action_by_evaluation_policy", ",", "\n", "action_by_behavior_policy", "=", "action_by_behavior_policy", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore", "=", "pscore", ",", "\n", ")", "\n", "_", "=", "snipw", ".", "estimate_policy_value", "(", "\n", "action_by_evaluation_policy", "=", "action_by_evaluation_policy", ",", "\n", "action_by_behavior_policy", "=", "action_by_behavior_policy", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore", "=", "pscore", ",", "\n", ")", "\n", "_", "=", "snipw", ".", "estimate_interval", "(", "\n", "action_by_evaluation_policy", "=", "action_by_evaluation_policy", ",", "\n", "action_by_behavior_policy", "=", "action_by_behavior_policy", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore", "=", "pscore", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_ipw_estimators_continuous.test_estimate_intervals_of_all_estimators_using_invalid_input_data": [[274, 311], ["pytest.mark.parametrize", "pytest.mark.parametrize", "pytest.raises", "ipw.estimate_interval", "snipw.estimate_interval"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_interval", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_interval"], ["@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"action_by_evaluation_policy, action_by_behavior_policy, reward, pscore\"", ",", "\n", "valid_input_of_ipw", ",", "\n", ")", "\n", "@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"alpha, n_bootstrap_samples, random_state, err, description\"", ",", "\n", "invalid_input_of_estimate_intervals", ",", "\n", ")", "\n", "def", "test_estimate_intervals_of_all_estimators_using_invalid_input_data", "(", "\n", "action_by_evaluation_policy", ",", "\n", "action_by_behavior_policy", ",", "\n", "reward", ",", "\n", "pscore", ",", "\n", "alpha", ",", "\n", "n_bootstrap_samples", ",", "\n", "random_state", ",", "\n", "err", ",", "\n", "description", ",", "\n", ")", "->", "None", ":", "\n", "    ", "with", "pytest", ".", "raises", "(", "err", ",", "match", "=", "f\"{description}*\"", ")", ":", "\n", "        ", "_", "=", "ipw", ".", "estimate_interval", "(", "\n", "action_by_evaluation_policy", "=", "action_by_evaluation_policy", ",", "\n", "action_by_behavior_policy", "=", "action_by_behavior_policy", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore", "=", "pscore", ",", "\n", "alpha", "=", "alpha", ",", "\n", "n_bootstrap_samples", "=", "n_bootstrap_samples", ",", "\n", "random_state", "=", "random_state", ",", "\n", ")", "\n", "_", "=", "snipw", ".", "estimate_interval", "(", "\n", "action_by_evaluation_policy", "=", "action_by_evaluation_policy", ",", "\n", "action_by_behavior_policy", "=", "action_by_behavior_policy", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore", "=", "pscore", ",", "\n", "alpha", "=", "alpha", ",", "\n", "n_bootstrap_samples", "=", "n_bootstrap_samples", ",", "\n", "random_state", "=", "random_state", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_ipw_estimators_continuous.test_estimate_intervals_of_all_estimators_using_valid_input_data": [[314, 349], ["pytest.mark.parametrize", "pytest.mark.parametrize", "ipw.estimate_interval", "snipw.estimate_interval"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_interval", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_interval"], ["", "", "@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"action_by_evaluation_policy, action_by_behavior_policy, reward, pscore\"", ",", "\n", "valid_input_of_ipw", ",", "\n", ")", "\n", "@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"alpha, n_bootstrap_samples, random_state, description\"", ",", "\n", "valid_input_of_estimate_intervals", ",", "\n", ")", "\n", "def", "test_estimate_intervals_of_all_estimators_using_valid_input_data", "(", "\n", "action_by_evaluation_policy", ",", "\n", "action_by_behavior_policy", ",", "\n", "reward", ",", "\n", "pscore", ",", "\n", "alpha", ",", "\n", "n_bootstrap_samples", ",", "\n", "random_state", ",", "\n", "description", ",", "\n", ")", "->", "None", ":", "\n", "    ", "_", "=", "ipw", ".", "estimate_interval", "(", "\n", "action_by_evaluation_policy", "=", "action_by_evaluation_policy", ",", "\n", "action_by_behavior_policy", "=", "action_by_behavior_policy", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore", "=", "pscore", ",", "\n", "alpha", "=", "alpha", ",", "\n", "n_bootstrap_samples", "=", "n_bootstrap_samples", ",", "\n", "random_state", "=", "random_state", ",", "\n", ")", "\n", "_", "=", "snipw", ".", "estimate_interval", "(", "\n", "action_by_evaluation_policy", "=", "action_by_evaluation_policy", ",", "\n", "action_by_behavior_policy", "=", "action_by_behavior_policy", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore", "=", "pscore", ",", "\n", "alpha", "=", "alpha", ",", "\n", "n_bootstrap_samples", "=", "n_bootstrap_samples", ",", "\n", "random_state", "=", "random_state", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_ipw_estimators_continuous.test_continuous_ope_performance": [[352, 425], ["pytest.mark.parametrize", "obp.ope.KernelizedInverseProbabilityWeighting", "obp.ope.KernelizedSelfNormalizedInverseProbabilityWeighting", "obp.dataset.SyntheticContinuousBanditDataset", "obp.dataset.SyntheticContinuousBanditDataset.obtain_batch_bandit_feedback", "obp.dataset.linear_synthetic_policy_continuous", "obp.dataset.linear_reward_funcion_continuous", "obp.dataset.linear_reward_funcion_continuous.mean", "print", "obp.ope.KernelizedInverseProbabilityWeighting.estimate_policy_value", "obp.ope.KernelizedSelfNormalizedInverseProbabilityWeighting.estimate_policy_value", "print", "numpy.abs"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.multiclass.MultiClassToBanditReduction.obtain_batch_bandit_feedback", "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.synthetic_continuous.linear_synthetic_policy_continuous", "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.synthetic_continuous.linear_reward_funcion_continuous", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_policy_value", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_policy_value"], ["", "@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"kernel\"", ",", "\n", "[", "\"triangular\"", ",", "\"gaussian\"", ",", "\"epanechnikov\"", ",", "\"cosine\"", "]", ",", "\n", ")", "\n", "def", "test_continuous_ope_performance", "(", "kernel", ")", ":", "\n", "# define ipw instances", "\n", "    ", "ipw", "=", "KernelizedInverseProbabilityWeighting", "(", "kernel", "=", "kernel", ",", "bandwidth", "=", "0.1", ")", "\n", "snipw", "=", "KernelizedSelfNormalizedInverseProbabilityWeighting", "(", "\n", "kernel", "=", "kernel", ",", "bandwidth", "=", "0.1", "\n", ")", "\n", "# set parameters", "\n", "dim_context", "=", "2", "\n", "reward_noise", "=", "0.1", "\n", "random_state", "=", "12345", "\n", "n_rounds", "=", "10000", "\n", "min_action_value", "=", "-", "10", "\n", "max_action_value", "=", "10", "\n", "behavior_policy_function", "=", "linear_behavior_policy_continuous", "\n", "reward_function", "=", "linear_reward_funcion_continuous", "\n", "dataset", "=", "SyntheticContinuousBanditDataset", "(", "\n", "dim_context", "=", "dim_context", ",", "\n", "reward_noise", "=", "reward_noise", ",", "\n", "min_action_value", "=", "min_action_value", ",", "\n", "max_action_value", "=", "max_action_value", ",", "\n", "reward_function", "=", "reward_function", ",", "\n", "behavior_policy_function", "=", "behavior_policy_function", ",", "\n", "random_state", "=", "random_state", ",", "\n", ")", "\n", "# obtain feedback", "\n", "bandit_feedback", "=", "dataset", ".", "obtain_batch_bandit_feedback", "(", "\n", "n_rounds", "=", "n_rounds", ",", "\n", ")", "\n", "context", "=", "bandit_feedback", "[", "\"context\"", "]", "\n", "action_by_evaluation_policy", "=", "linear_synthetic_policy_continuous", "(", "context", ")", "\n", "action_by_behavior_policy", "=", "bandit_feedback", "[", "\"action\"", "]", "\n", "reward", "=", "bandit_feedback", "[", "\"reward\"", "]", "\n", "pscore", "=", "bandit_feedback", "[", "\"pscore\"", "]", "\n", "\n", "# compute statistics of ground truth policy value", "\n", "q_pi_e", "=", "linear_reward_funcion_continuous", "(", "\n", "context", "=", "context", ",", "action", "=", "action_by_evaluation_policy", ",", "random_state", "=", "random_state", "\n", ")", "\n", "true_policy_value", "=", "q_pi_e", ".", "mean", "(", ")", "\n", "print", "(", "f\"true_policy_value: {true_policy_value}\"", ")", "\n", "\n", "# OPE", "\n", "policy_value_estimated_by_ipw", "=", "ipw", ".", "estimate_policy_value", "(", "\n", "action_by_evaluation_policy", "=", "action_by_evaluation_policy", ",", "\n", "action_by_behavior_policy", "=", "action_by_behavior_policy", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore", "=", "pscore", ",", "\n", ")", "\n", "policy_value_estimated_by_snipw", "=", "snipw", ".", "estimate_policy_value", "(", "\n", "action_by_evaluation_policy", "=", "action_by_evaluation_policy", ",", "\n", "action_by_behavior_policy", "=", "action_by_behavior_policy", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore", "=", "pscore", ",", "\n", ")", "\n", "\n", "# check the performance of OPE", "\n", "estimated_policy_value", "=", "{", "\n", "\"ipw\"", ":", "policy_value_estimated_by_ipw", ",", "\n", "\"snipw\"", ":", "policy_value_estimated_by_snipw", ",", "\n", "}", "\n", "for", "key", "in", "estimated_policy_value", ":", "\n", "        ", "print", "(", "\n", "f\"estimated_value: {estimated_policy_value[key]} ------ estimator: {key}, \"", "\n", ")", "\n", "# test the performance of each estimator", "\n", "assert", "(", "\n", "np", ".", "abs", "(", "true_policy_value", "-", "estimated_policy_value", "[", "key", "]", ")", "/", "true_policy_value", "\n", "<=", "0.1", "\n", ")", ",", "f\"{key} does not work well (relative estimation error is greater than 10%)\"", "\n", "", "", ""]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_dr_estimators_slate.test_estimate_policy_value_using_invalid_input_data": [[389, 424], ["pytest.mark.parametrize", "pytest.raises", "dr.estimate_policy_value", "dr.estimate_interval"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_policy_value", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_interval"], ["@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"slate_id, action, reward, pscore, position, evaluation_policy_pscore, q_hat, evaluation_policy_action_dist, description\"", ",", "\n", "invalid_input_of_slate_estimators", ",", "\n", ")", "\n", "def", "test_estimate_policy_value_using_invalid_input_data", "(", "\n", "slate_id", ",", "\n", "action", ",", "\n", "reward", ",", "\n", "pscore", ",", "\n", "position", ",", "\n", "evaluation_policy_pscore", ",", "\n", "q_hat", ",", "\n", "evaluation_policy_action_dist", ",", "\n", "description", ",", "\n", ")", "->", "None", ":", "\n", "    ", "with", "pytest", ".", "raises", "(", "ValueError", ",", "match", "=", "f\"{description}*\"", ")", ":", "\n", "        ", "_", "=", "dr", ".", "estimate_policy_value", "(", "\n", "slate_id", "=", "slate_id", ",", "\n", "action", "=", "action", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore_cascade", "=", "pscore", ",", "\n", "position", "=", "position", ",", "\n", "evaluation_policy_pscore_cascade", "=", "evaluation_policy_pscore", ",", "\n", "q_hat", "=", "q_hat", ",", "\n", "evaluation_policy_action_dist", "=", "evaluation_policy_action_dist", ",", "\n", ")", "\n", "_", "=", "dr", ".", "estimate_interval", "(", "\n", "slate_id", "=", "slate_id", ",", "\n", "action", "=", "action", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore_cascade", "=", "pscore", ",", "\n", "position", "=", "position", ",", "\n", "evaluation_policy_pscore_cascade", "=", "evaluation_policy_pscore", ",", "\n", "q_hat", "=", "q_hat", ",", "\n", "evaluation_policy_action_dist", "=", "evaluation_policy_action_dist", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_dr_estimators_slate.test_cascade_dr_using_valid_input_data": [[443, 477], ["pytest.mark.parametrize", "dr.estimate_policy_value", "dr.estimate_interval"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_policy_value", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_interval"], ["@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"slate_id, action, reward, pscore, position, evaluation_policy_pscore, q_hat, evaluation_policy_action_dist, description\"", ",", "\n", "valid_input_of_slate_estimators", ",", "\n", ")", "\n", "def", "test_cascade_dr_using_valid_input_data", "(", "\n", "slate_id", ",", "\n", "action", ",", "\n", "reward", ",", "\n", "pscore", ",", "\n", "position", ",", "\n", "evaluation_policy_pscore", ",", "\n", "q_hat", ",", "\n", "evaluation_policy_action_dist", ",", "\n", "description", ",", "\n", ")", "->", "None", ":", "\n", "    ", "_", "=", "dr", ".", "estimate_policy_value", "(", "\n", "slate_id", "=", "slate_id", ",", "\n", "action", "=", "action", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore_cascade", "=", "pscore", ",", "\n", "position", "=", "position", ",", "\n", "evaluation_policy_pscore_cascade", "=", "evaluation_policy_pscore", ",", "\n", "q_hat", "=", "q_hat", ",", "\n", "evaluation_policy_action_dist", "=", "evaluation_policy_action_dist", ",", "\n", ")", "\n", "_", "=", "dr", ".", "estimate_interval", "(", "\n", "slate_id", "=", "slate_id", ",", "\n", "action", "=", "action", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore_cascade", "=", "pscore", ",", "\n", "position", "=", "position", ",", "\n", "evaluation_policy_pscore_cascade", "=", "evaluation_policy_pscore", ",", "\n", "q_hat", "=", "q_hat", ",", "\n", "evaluation_policy_action_dist", "=", "evaluation_policy_action_dist", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_dr_estimators_slate.test_estimate_interval_using_invalid_input_data": [[515, 552], ["pytest.mark.parametrize", "pytest.mark.parametrize", "pytest.raises", "dr.estimate_interval"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_interval"], ["@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"slate_id, action, reward, pscore, position, evaluation_policy_pscore, q_hat, evaluation_policy_action_dist, description_1\"", ",", "\n", "valid_input_of_slate_estimators", ",", "\n", ")", "\n", "@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"alpha, n_bootstrap_samples, random_state, err, description_2\"", ",", "\n", "invalid_input_of_estimate_intervals", ",", "\n", ")", "\n", "def", "test_estimate_interval_using_invalid_input_data", "(", "\n", "slate_id", ",", "\n", "action", ",", "\n", "reward", ",", "\n", "pscore", ",", "\n", "position", ",", "\n", "evaluation_policy_pscore", ",", "\n", "q_hat", ",", "\n", "evaluation_policy_action_dist", ",", "\n", "description_1", ",", "\n", "alpha", ",", "\n", "n_bootstrap_samples", ",", "\n", "random_state", ",", "\n", "err", ",", "\n", "description_2", ",", "\n", ")", "->", "None", ":", "\n", "    ", "with", "pytest", ".", "raises", "(", "err", ",", "match", "=", "f\"{description_2}*\"", ")", ":", "\n", "        ", "_", "=", "dr", ".", "estimate_interval", "(", "\n", "slate_id", "=", "slate_id", ",", "\n", "action", "=", "action", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore_cascade", "=", "pscore", ",", "\n", "position", "=", "position", ",", "\n", "evaluation_policy_pscore_cascade", "=", "evaluation_policy_pscore", ",", "\n", "q_hat", "=", "q_hat", ",", "\n", "evaluation_policy_action_dist", "=", "evaluation_policy_action_dist", ",", "\n", "alpha", "=", "alpha", ",", "\n", "n_bootstrap_samples", "=", "n_bootstrap_samples", ",", "\n", "random_state", "=", "random_state", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_dr_estimators_slate.test_estimate_interval_using_valid_input_data": [[555, 590], ["pytest.mark.parametrize", "pytest.mark.parametrize", "dr.estimate_interval"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_interval"], ["", "", "@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"slate_id, action, reward, pscore, position, evaluation_policy_pscore, q_hat, evaluation_policy_action_dist, description_1\"", ",", "\n", "valid_input_of_slate_estimators", ",", "\n", ")", "\n", "@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"alpha, n_bootstrap_samples, random_state, description_2\"", ",", "\n", "valid_input_of_estimate_intervals", ",", "\n", ")", "\n", "def", "test_estimate_interval_using_valid_input_data", "(", "\n", "slate_id", ",", "\n", "action", ",", "\n", "reward", ",", "\n", "pscore", ",", "\n", "position", ",", "\n", "evaluation_policy_pscore", ",", "\n", "q_hat", ",", "\n", "evaluation_policy_action_dist", ",", "\n", "description_1", ",", "\n", "alpha", ",", "\n", "n_bootstrap_samples", ",", "\n", "random_state", ",", "\n", "description_2", ",", "\n", ")", "->", "None", ":", "\n", "    ", "_", "=", "dr", ".", "estimate_interval", "(", "\n", "slate_id", "=", "slate_id", ",", "\n", "action", "=", "action", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore_cascade", "=", "pscore", ",", "\n", "position", "=", "position", ",", "\n", "evaluation_policy_pscore_cascade", "=", "evaluation_policy_pscore", ",", "\n", "q_hat", "=", "q_hat", ",", "\n", "evaluation_policy_action_dist", "=", "evaluation_policy_action_dist", ",", "\n", "alpha", "=", "alpha", ",", "\n", "n_bootstrap_samples", "=", "n_bootstrap_samples", ",", "\n", "random_state", "=", "random_state", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_dr_estimators_slate.test_slate_ope_performance_using_cascade_additive_log": [[593, 730], ["obp.dataset.SyntheticSlateBanditDataset", "obp.dataset.SyntheticSlateBanditDataset", "obp.dataset.SyntheticSlateBanditDataset.obtain_batch_bandit_feedback", "obp.dataset.SyntheticSlateBanditDataset.obtain_batch_bandit_feedback", "obp.dataset.SyntheticSlateBanditDataset.obtain_pscore_given_evaluation_policy_logit", "obp.dataset.SyntheticSlateBanditDataset.calc_evaluation_policy_action_dist", "obp.ope.SlateRegressionModel", "obp.ope.SlateRegressionModel.fit_predict", "dr.estimate_policy_value", "random_behavior_feedback[].reshape().sum", "random_behavior_feedback[].reshape().sum.mean", "random_behavior_feedback[].reshape().sum.std", "print", "print", "dr.estimate_policy_value", "rips.estimate_policy_value", "numpy.allclose", "numpy.ones", "numpy.ones", "numpy.sqrt", "print", "numpy.array", "numpy.array", "sklearn.tree.DecisionTreeRegressor", "random_behavior_feedback[].reshape", "numpy.abs", "numpy.zeros_like"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.multiclass.MultiClassToBanditReduction.obtain_batch_bandit_feedback", "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.multiclass.MultiClassToBanditReduction.obtain_batch_bandit_feedback", "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.synthetic_slate.SyntheticSlateBanditDataset.obtain_pscore_given_evaluation_policy_logit", "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.synthetic_slate.SyntheticSlateBanditDataset.calc_evaluation_policy_action_dist", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.classification_model.PropensityScoreEstimator.fit_predict", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_policy_value", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_policy_value", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_policy_value"], ["", "def", "test_slate_ope_performance_using_cascade_additive_log", "(", ")", ":", "\n", "# set parameters", "\n", "    ", "n_unique_action", "=", "10", "\n", "len_list", "=", "3", "\n", "dim_context", "=", "2", "\n", "reward_type", "=", "\"binary\"", "\n", "random_state", "=", "12345", "\n", "n_rounds", "=", "1000", "\n", "reward_structure", "=", "\"cascade_additive\"", "\n", "click_model", "=", "None", "\n", "behavior_policy_function", "=", "linear_behavior_policy_logit", "\n", "reward_function", "=", "logistic_reward_function", "\n", "dataset", "=", "SyntheticSlateBanditDataset", "(", "\n", "n_unique_action", "=", "n_unique_action", ",", "\n", "len_list", "=", "len_list", ",", "\n", "dim_context", "=", "dim_context", ",", "\n", "reward_type", "=", "reward_type", ",", "\n", "reward_structure", "=", "reward_structure", ",", "\n", "click_model", "=", "click_model", ",", "\n", "random_state", "=", "random_state", ",", "\n", "behavior_policy_function", "=", "behavior_policy_function", ",", "\n", "base_reward_function", "=", "reward_function", ",", "\n", ")", "\n", "random_behavior_dataset", "=", "SyntheticSlateBanditDataset", "(", "\n", "n_unique_action", "=", "n_unique_action", ",", "\n", "len_list", "=", "len_list", ",", "\n", "dim_context", "=", "dim_context", ",", "\n", "reward_type", "=", "reward_type", ",", "\n", "reward_structure", "=", "reward_structure", ",", "\n", "click_model", "=", "click_model", ",", "\n", "random_state", "=", "random_state", ",", "\n", "behavior_policy_function", "=", "None", ",", "\n", "base_reward_function", "=", "reward_function", ",", "\n", ")", "\n", "# obtain feedback", "\n", "bandit_feedback", "=", "dataset", ".", "obtain_batch_bandit_feedback", "(", "n_rounds", "=", "n_rounds", ")", "\n", "slate_id", "=", "bandit_feedback", "[", "\"slate_id\"", "]", "\n", "context", "=", "bandit_feedback", "[", "\"context\"", "]", "\n", "action", "=", "bandit_feedback", "[", "\"action\"", "]", "\n", "reward", "=", "bandit_feedback", "[", "\"reward\"", "]", "\n", "pscore", "=", "bandit_feedback", "[", "\"pscore_cascade\"", "]", "\n", "position", "=", "bandit_feedback", "[", "\"position\"", "]", "\n", "\n", "# obtain random behavior feedback", "\n", "random_behavior_feedback", "=", "random_behavior_dataset", ".", "obtain_batch_bandit_feedback", "(", "\n", "n_rounds", "=", "n_rounds", "\n", ")", "\n", "evaluation_policy_logit_", "=", "np", ".", "ones", "(", "(", "n_rounds", ",", "n_unique_action", ")", ")", "/", "n_unique_action", "\n", "evaluation_policy_action_dist", "=", "(", "\n", "np", ".", "ones", "(", "n_rounds", "*", "len_list", "*", "n_unique_action", ")", "/", "n_unique_action", "\n", ")", "\n", "(", "\n", "_", ",", "\n", "_", ",", "\n", "evaluation_policy_pscore", ",", "\n", ")", "=", "dataset", ".", "obtain_pscore_given_evaluation_policy_logit", "(", "\n", "action", "=", "action", ",", "\n", "evaluation_policy_logit_", "=", "evaluation_policy_logit_", ",", "\n", "return_pscore_item_position", "=", "False", ",", "\n", ")", "\n", "evaluation_policy_action_dist", "=", "dataset", ".", "calc_evaluation_policy_action_dist", "(", "\n", "action", "=", "action", ",", "\n", "evaluation_policy_logit_", "=", "evaluation_policy_logit_", ",", "\n", ")", "\n", "\n", "# obtain q_hat", "\n", "base_regression_model", "=", "SlateRegressionModel", "(", "\n", "base_model", "=", "DecisionTreeRegressor", "(", "max_depth", "=", "3", ",", "random_state", "=", "12345", ")", ",", "\n", "len_list", "=", "len_list", ",", "\n", "n_unique_action", "=", "n_unique_action", ",", "\n", "fitting_method", "=", "\"iw\"", ",", "\n", ")", "\n", "q_hat", "=", "base_regression_model", ".", "fit_predict", "(", "\n", "context", "=", "context", ",", "\n", "action", "=", "action", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore_cascade", "=", "pscore", ",", "\n", "evaluation_policy_pscore_cascade", "=", "evaluation_policy_pscore", ",", "\n", "evaluation_policy_action_dist", "=", "evaluation_policy_action_dist", ",", "\n", ")", "\n", "\n", "# check if q_hat=0 case coincides with rips", "\n", "cascade_dr_estimated_policy_value", "=", "dr", ".", "estimate_policy_value", "(", "\n", "slate_id", "=", "slate_id", ",", "\n", "action", "=", "action", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore_cascade", "=", "pscore", ",", "\n", "position", "=", "position", ",", "\n", "evaluation_policy_pscore_cascade", "=", "evaluation_policy_pscore", ",", "\n", "q_hat", "=", "q_hat", ",", "\n", "evaluation_policy_action_dist", "=", "evaluation_policy_action_dist", ",", "\n", ")", "\n", "# compute statistics of ground truth policy value", "\n", "q_pi_e", "=", "(", "\n", "random_behavior_feedback", "[", "\"reward\"", "]", "\n", ".", "reshape", "(", "(", "n_rounds", ",", "dataset", ".", "len_list", ")", ")", "\n", ".", "sum", "(", "axis", "=", "1", ")", "\n", ")", "\n", "gt_mean", "=", "q_pi_e", ".", "mean", "(", ")", "\n", "gt_std", "=", "q_pi_e", ".", "std", "(", "ddof", "=", "1", ")", "\n", "print", "(", "\"Cascade additive\"", ")", "\n", "# check the performance of OPE", "\n", "ci_bound", "=", "gt_std", "*", "3", "/", "np", ".", "sqrt", "(", "q_pi_e", ".", "shape", "[", "0", "]", ")", "\n", "print", "(", "f\"gt_mean: {gt_mean}, 3 * gt_std / sqrt(n): {ci_bound}\"", ")", "\n", "estimated_policy_value", "=", "{", "\n", "\"cascade-dr\"", ":", "cascade_dr_estimated_policy_value", ",", "\n", "}", "\n", "for", "key", "in", "estimated_policy_value", ":", "\n", "        ", "print", "(", "\n", "f\"estimated_value: {estimated_policy_value[key]} ------ estimator: {key}, \"", "\n", ")", "\n", "# test the performance of each estimator", "\n", "assert", "(", "\n", "np", ".", "abs", "(", "gt_mean", "-", "estimated_policy_value", "[", "key", "]", ")", "<=", "ci_bound", "\n", ")", ",", "f\"OPE of {key} did not work well (absolute error is greater than 3*sigma)\"", "\n", "\n", "# check if q_hat = 0 case of cascade-dr coincides with rips", "\n", "", "cascade_dr_estimated_policy_value_", "=", "dr", ".", "estimate_policy_value", "(", "\n", "slate_id", "=", "slate_id", ",", "\n", "action", "=", "action", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore_cascade", "=", "pscore", ",", "\n", "position", "=", "position", ",", "\n", "evaluation_policy_pscore_cascade", "=", "evaluation_policy_pscore", ",", "\n", "q_hat", "=", "np", ".", "zeros_like", "(", "q_hat", ")", ",", "\n", "evaluation_policy_action_dist", "=", "evaluation_policy_action_dist", ",", "\n", ")", "\n", "rips_estimated_policy_value", "=", "rips", ".", "estimate_policy_value", "(", "\n", "slate_id", "=", "slate_id", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore_cascade", "=", "pscore", ",", "\n", "position", "=", "position", ",", "\n", "evaluation_policy_pscore_cascade", "=", "evaluation_policy_pscore", ",", "\n", ")", "\n", "assert", "np", ".", "allclose", "(", "\n", "np", ".", "array", "(", "[", "cascade_dr_estimated_policy_value_", "]", ")", ",", "\n", "np", ".", "array", "(", "[", "rips_estimated_policy_value", "]", ")", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_dr_estimators_slate.test_slate_ope_performance_using_independent_log": [[733, 870], ["obp.dataset.SyntheticSlateBanditDataset", "obp.dataset.SyntheticSlateBanditDataset", "obp.dataset.SyntheticSlateBanditDataset.obtain_batch_bandit_feedback", "obp.dataset.SyntheticSlateBanditDataset.obtain_batch_bandit_feedback", "obp.dataset.SyntheticSlateBanditDataset.obtain_pscore_given_evaluation_policy_logit", "obp.dataset.SyntheticSlateBanditDataset.calc_evaluation_policy_action_dist", "obp.ope.SlateRegressionModel", "obp.ope.SlateRegressionModel.fit_predict", "dr.estimate_policy_value", "random_behavior_feedback[].reshape().sum", "random_behavior_feedback[].reshape().sum.mean", "random_behavior_feedback[].reshape().sum.std", "print", "print", "dr.estimate_policy_value", "rips.estimate_policy_value", "numpy.allclose", "numpy.ones", "numpy.ones", "numpy.sqrt", "print", "numpy.array", "numpy.array", "sklearn.tree.DecisionTreeRegressor", "random_behavior_feedback[].reshape", "numpy.abs", "numpy.zeros_like"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.multiclass.MultiClassToBanditReduction.obtain_batch_bandit_feedback", "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.multiclass.MultiClassToBanditReduction.obtain_batch_bandit_feedback", "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.synthetic_slate.SyntheticSlateBanditDataset.obtain_pscore_given_evaluation_policy_logit", "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.synthetic_slate.SyntheticSlateBanditDataset.calc_evaluation_policy_action_dist", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.classification_model.PropensityScoreEstimator.fit_predict", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_policy_value", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_policy_value", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_policy_value"], ["", "def", "test_slate_ope_performance_using_independent_log", "(", ")", ":", "\n", "# set parameters", "\n", "    ", "n_unique_action", "=", "10", "\n", "len_list", "=", "3", "\n", "dim_context", "=", "2", "\n", "reward_type", "=", "\"binary\"", "\n", "random_state", "=", "12345", "\n", "n_rounds", "=", "1000", "\n", "reward_structure", "=", "\"independent\"", "\n", "click_model", "=", "None", "\n", "behavior_policy_function", "=", "linear_behavior_policy_logit", "\n", "reward_function", "=", "logistic_reward_function", "\n", "dataset", "=", "SyntheticSlateBanditDataset", "(", "\n", "n_unique_action", "=", "n_unique_action", ",", "\n", "len_list", "=", "len_list", ",", "\n", "dim_context", "=", "dim_context", ",", "\n", "reward_type", "=", "reward_type", ",", "\n", "reward_structure", "=", "reward_structure", ",", "\n", "click_model", "=", "click_model", ",", "\n", "random_state", "=", "random_state", ",", "\n", "behavior_policy_function", "=", "behavior_policy_function", ",", "\n", "base_reward_function", "=", "reward_function", ",", "\n", ")", "\n", "random_behavior_dataset", "=", "SyntheticSlateBanditDataset", "(", "\n", "n_unique_action", "=", "n_unique_action", ",", "\n", "len_list", "=", "len_list", ",", "\n", "dim_context", "=", "dim_context", ",", "\n", "reward_type", "=", "reward_type", ",", "\n", "reward_structure", "=", "reward_structure", ",", "\n", "click_model", "=", "click_model", ",", "\n", "random_state", "=", "random_state", ",", "\n", "behavior_policy_function", "=", "None", ",", "\n", "base_reward_function", "=", "reward_function", ",", "\n", ")", "\n", "# obtain feedback", "\n", "bandit_feedback", "=", "dataset", ".", "obtain_batch_bandit_feedback", "(", "n_rounds", "=", "n_rounds", ")", "\n", "slate_id", "=", "bandit_feedback", "[", "\"slate_id\"", "]", "\n", "context", "=", "bandit_feedback", "[", "\"context\"", "]", "\n", "action", "=", "bandit_feedback", "[", "\"action\"", "]", "\n", "reward", "=", "bandit_feedback", "[", "\"reward\"", "]", "\n", "pscore", "=", "bandit_feedback", "[", "\"pscore_cascade\"", "]", "\n", "position", "=", "bandit_feedback", "[", "\"position\"", "]", "\n", "\n", "# obtain random behavior feedback", "\n", "random_behavior_feedback", "=", "random_behavior_dataset", ".", "obtain_batch_bandit_feedback", "(", "\n", "n_rounds", "=", "n_rounds", "\n", ")", "\n", "evaluation_policy_logit_", "=", "np", ".", "ones", "(", "(", "n_rounds", ",", "n_unique_action", ")", ")", "/", "n_unique_action", "\n", "evaluation_policy_action_dist", "=", "(", "\n", "np", ".", "ones", "(", "n_rounds", "*", "len_list", "*", "n_unique_action", ")", "/", "n_unique_action", "\n", ")", "\n", "(", "\n", "_", ",", "\n", "_", ",", "\n", "evaluation_policy_pscore", ",", "\n", ")", "=", "dataset", ".", "obtain_pscore_given_evaluation_policy_logit", "(", "\n", "action", "=", "action", ",", "\n", "evaluation_policy_logit_", "=", "evaluation_policy_logit_", ",", "\n", "return_pscore_item_position", "=", "False", ",", "\n", ")", "\n", "evaluation_policy_action_dist", "=", "dataset", ".", "calc_evaluation_policy_action_dist", "(", "\n", "action", "=", "action", ",", "\n", "evaluation_policy_logit_", "=", "evaluation_policy_logit_", ",", "\n", ")", "\n", "\n", "# obtain q_hat", "\n", "base_regression_model", "=", "SlateRegressionModel", "(", "\n", "base_model", "=", "DecisionTreeRegressor", "(", "max_depth", "=", "3", ",", "random_state", "=", "12345", ")", ",", "\n", "len_list", "=", "len_list", ",", "\n", "n_unique_action", "=", "n_unique_action", ",", "\n", "fitting_method", "=", "\"iw\"", ",", "\n", ")", "\n", "q_hat", "=", "base_regression_model", ".", "fit_predict", "(", "\n", "context", "=", "context", ",", "\n", "action", "=", "action", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore_cascade", "=", "pscore", ",", "\n", "evaluation_policy_pscore_cascade", "=", "evaluation_policy_pscore", ",", "\n", "evaluation_policy_action_dist", "=", "evaluation_policy_action_dist", ",", "\n", ")", "\n", "\n", "# check if q_hat=0 case coincides with rips", "\n", "cascade_dr_estimated_policy_value", "=", "dr", ".", "estimate_policy_value", "(", "\n", "slate_id", "=", "slate_id", ",", "\n", "action", "=", "action", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore_cascade", "=", "pscore", ",", "\n", "position", "=", "position", ",", "\n", "evaluation_policy_pscore_cascade", "=", "evaluation_policy_pscore", ",", "\n", "q_hat", "=", "q_hat", ",", "\n", "evaluation_policy_action_dist", "=", "evaluation_policy_action_dist", ",", "\n", ")", "\n", "# compute statistics of ground truth policy value", "\n", "q_pi_e", "=", "(", "\n", "random_behavior_feedback", "[", "\"reward\"", "]", "\n", ".", "reshape", "(", "(", "n_rounds", ",", "dataset", ".", "len_list", ")", ")", "\n", ".", "sum", "(", "axis", "=", "1", ")", "\n", ")", "\n", "gt_mean", "=", "q_pi_e", ".", "mean", "(", ")", "\n", "gt_std", "=", "q_pi_e", ".", "std", "(", "ddof", "=", "1", ")", "\n", "print", "(", "\"Cascade additive\"", ")", "\n", "# check the performance of OPE", "\n", "ci_bound", "=", "gt_std", "*", "3", "/", "np", ".", "sqrt", "(", "q_pi_e", ".", "shape", "[", "0", "]", ")", "\n", "print", "(", "f\"gt_mean: {gt_mean}, 3 * gt_std / sqrt(n): {ci_bound}\"", ")", "\n", "estimated_policy_value", "=", "{", "\n", "\"cascade-dr\"", ":", "cascade_dr_estimated_policy_value", ",", "\n", "}", "\n", "for", "key", "in", "estimated_policy_value", ":", "\n", "        ", "print", "(", "\n", "f\"estimated_value: {estimated_policy_value[key]} ------ estimator: {key}, \"", "\n", ")", "\n", "# test the performance of each estimator", "\n", "assert", "(", "\n", "np", ".", "abs", "(", "gt_mean", "-", "estimated_policy_value", "[", "key", "]", ")", "<=", "ci_bound", "\n", ")", ",", "f\"OPE of {key} did not work well (absolute error is greater than 3*sigma)\"", "\n", "\n", "# check if q_hat = 0 case of cascade-dr coincides with rips", "\n", "", "cascade_dr_estimated_policy_value_", "=", "dr", ".", "estimate_policy_value", "(", "\n", "slate_id", "=", "slate_id", ",", "\n", "action", "=", "action", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore_cascade", "=", "pscore", ",", "\n", "position", "=", "position", ",", "\n", "evaluation_policy_pscore_cascade", "=", "evaluation_policy_pscore", ",", "\n", "q_hat", "=", "np", ".", "zeros_like", "(", "q_hat", ")", ",", "\n", "evaluation_policy_action_dist", "=", "evaluation_policy_action_dist", ",", "\n", ")", "\n", "rips_estimated_policy_value", "=", "rips", ".", "estimate_policy_value", "(", "\n", "slate_id", "=", "slate_id", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore_cascade", "=", "pscore", ",", "\n", "position", "=", "position", ",", "\n", "evaluation_policy_pscore_cascade", "=", "evaluation_policy_pscore", ",", "\n", ")", "\n", "assert", "np", ".", "allclose", "(", "\n", "np", ".", "array", "(", "[", "cascade_dr_estimated_policy_value_", "]", ")", ",", "\n", "np", ".", "array", "(", "[", "rips_estimated_policy_value", "]", ")", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_dr_estimators_slate.test_slate_ope_performance_using_standard_additive_log": [[873, 1010], ["obp.dataset.SyntheticSlateBanditDataset", "obp.dataset.SyntheticSlateBanditDataset", "obp.dataset.SyntheticSlateBanditDataset.obtain_batch_bandit_feedback", "obp.dataset.SyntheticSlateBanditDataset.obtain_batch_bandit_feedback", "obp.dataset.SyntheticSlateBanditDataset.obtain_pscore_given_evaluation_policy_logit", "obp.dataset.SyntheticSlateBanditDataset.calc_evaluation_policy_action_dist", "obp.ope.SlateRegressionModel", "obp.ope.SlateRegressionModel.fit_predict", "dr.estimate_policy_value", "random_behavior_feedback[].reshape().sum", "random_behavior_feedback[].reshape().sum.mean", "random_behavior_feedback[].reshape().sum.std", "print", "print", "dr.estimate_policy_value", "rips.estimate_policy_value", "numpy.allclose", "numpy.ones", "numpy.ones", "numpy.sqrt", "print", "numpy.array", "numpy.array", "sklearn.tree.DecisionTreeRegressor", "random_behavior_feedback[].reshape", "numpy.abs", "numpy.zeros_like"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.multiclass.MultiClassToBanditReduction.obtain_batch_bandit_feedback", "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.multiclass.MultiClassToBanditReduction.obtain_batch_bandit_feedback", "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.synthetic_slate.SyntheticSlateBanditDataset.obtain_pscore_given_evaluation_policy_logit", "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.synthetic_slate.SyntheticSlateBanditDataset.calc_evaluation_policy_action_dist", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.classification_model.PropensityScoreEstimator.fit_predict", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_policy_value", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_policy_value", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_policy_value"], ["", "def", "test_slate_ope_performance_using_standard_additive_log", "(", ")", ":", "\n", "# set parameters", "\n", "    ", "n_unique_action", "=", "10", "\n", "len_list", "=", "3", "\n", "dim_context", "=", "2", "\n", "reward_type", "=", "\"binary\"", "\n", "random_state", "=", "12345", "\n", "n_rounds", "=", "1000", "\n", "reward_structure", "=", "\"standard_additive\"", "\n", "click_model", "=", "None", "\n", "behavior_policy_function", "=", "linear_behavior_policy_logit", "\n", "reward_function", "=", "logistic_reward_function", "\n", "dataset", "=", "SyntheticSlateBanditDataset", "(", "\n", "n_unique_action", "=", "n_unique_action", ",", "\n", "len_list", "=", "len_list", ",", "\n", "dim_context", "=", "dim_context", ",", "\n", "reward_type", "=", "reward_type", ",", "\n", "reward_structure", "=", "reward_structure", ",", "\n", "click_model", "=", "click_model", ",", "\n", "random_state", "=", "random_state", ",", "\n", "behavior_policy_function", "=", "behavior_policy_function", ",", "\n", "base_reward_function", "=", "reward_function", ",", "\n", ")", "\n", "random_behavior_dataset", "=", "SyntheticSlateBanditDataset", "(", "\n", "n_unique_action", "=", "n_unique_action", ",", "\n", "len_list", "=", "len_list", ",", "\n", "dim_context", "=", "dim_context", ",", "\n", "reward_type", "=", "reward_type", ",", "\n", "reward_structure", "=", "reward_structure", ",", "\n", "click_model", "=", "click_model", ",", "\n", "random_state", "=", "random_state", ",", "\n", "behavior_policy_function", "=", "None", ",", "\n", "base_reward_function", "=", "reward_function", ",", "\n", ")", "\n", "# obtain feedback", "\n", "bandit_feedback", "=", "dataset", ".", "obtain_batch_bandit_feedback", "(", "n_rounds", "=", "n_rounds", ")", "\n", "slate_id", "=", "bandit_feedback", "[", "\"slate_id\"", "]", "\n", "context", "=", "bandit_feedback", "[", "\"context\"", "]", "\n", "action", "=", "bandit_feedback", "[", "\"action\"", "]", "\n", "reward", "=", "bandit_feedback", "[", "\"reward\"", "]", "\n", "pscore", "=", "bandit_feedback", "[", "\"pscore_cascade\"", "]", "\n", "position", "=", "bandit_feedback", "[", "\"position\"", "]", "\n", "\n", "# obtain random behavior feedback", "\n", "random_behavior_feedback", "=", "random_behavior_dataset", ".", "obtain_batch_bandit_feedback", "(", "\n", "n_rounds", "=", "n_rounds", "\n", ")", "\n", "evaluation_policy_logit_", "=", "np", ".", "ones", "(", "(", "n_rounds", ",", "n_unique_action", ")", ")", "/", "n_unique_action", "\n", "evaluation_policy_action_dist", "=", "(", "\n", "np", ".", "ones", "(", "n_rounds", "*", "len_list", "*", "n_unique_action", ")", "/", "n_unique_action", "\n", ")", "\n", "(", "\n", "_", ",", "\n", "_", ",", "\n", "evaluation_policy_pscore", ",", "\n", ")", "=", "dataset", ".", "obtain_pscore_given_evaluation_policy_logit", "(", "\n", "action", "=", "action", ",", "\n", "evaluation_policy_logit_", "=", "evaluation_policy_logit_", ",", "\n", "return_pscore_item_position", "=", "False", ",", "\n", ")", "\n", "evaluation_policy_action_dist", "=", "dataset", ".", "calc_evaluation_policy_action_dist", "(", "\n", "action", "=", "action", ",", "\n", "evaluation_policy_logit_", "=", "evaluation_policy_logit_", ",", "\n", ")", "\n", "\n", "# obtain q_hat", "\n", "base_regression_model", "=", "SlateRegressionModel", "(", "\n", "base_model", "=", "DecisionTreeRegressor", "(", "max_depth", "=", "3", ",", "random_state", "=", "12345", ")", ",", "\n", "len_list", "=", "len_list", ",", "\n", "n_unique_action", "=", "n_unique_action", ",", "\n", "fitting_method", "=", "\"iw\"", ",", "\n", ")", "\n", "q_hat", "=", "base_regression_model", ".", "fit_predict", "(", "\n", "context", "=", "context", ",", "\n", "action", "=", "action", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore_cascade", "=", "pscore", ",", "\n", "evaluation_policy_pscore_cascade", "=", "evaluation_policy_pscore", ",", "\n", "evaluation_policy_action_dist", "=", "evaluation_policy_action_dist", ",", "\n", ")", "\n", "\n", "# check if q_hat=0 case coincides with rips", "\n", "cascade_dr_estimated_policy_value", "=", "dr", ".", "estimate_policy_value", "(", "\n", "slate_id", "=", "slate_id", ",", "\n", "action", "=", "action", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore_cascade", "=", "pscore", ",", "\n", "position", "=", "position", ",", "\n", "evaluation_policy_pscore_cascade", "=", "evaluation_policy_pscore", ",", "\n", "q_hat", "=", "q_hat", ",", "\n", "evaluation_policy_action_dist", "=", "evaluation_policy_action_dist", ",", "\n", ")", "\n", "# compute statistics of ground truth policy value", "\n", "q_pi_e", "=", "(", "\n", "random_behavior_feedback", "[", "\"reward\"", "]", "\n", ".", "reshape", "(", "(", "n_rounds", ",", "dataset", ".", "len_list", ")", ")", "\n", ".", "sum", "(", "axis", "=", "1", ")", "\n", ")", "\n", "gt_mean", "=", "q_pi_e", ".", "mean", "(", ")", "\n", "gt_std", "=", "q_pi_e", ".", "std", "(", "ddof", "=", "1", ")", "\n", "print", "(", "\"Cascade additive\"", ")", "\n", "# check the performance of OPE", "\n", "ci_bound", "=", "gt_std", "*", "3", "/", "np", ".", "sqrt", "(", "q_pi_e", ".", "shape", "[", "0", "]", ")", "\n", "print", "(", "f\"gt_mean: {gt_mean}, 3 * gt_std / sqrt(n): {ci_bound}\"", ")", "\n", "estimated_policy_value", "=", "{", "\n", "\"cascade-dr\"", ":", "cascade_dr_estimated_policy_value", ",", "\n", "}", "\n", "for", "key", "in", "estimated_policy_value", ":", "\n", "        ", "print", "(", "\n", "f\"estimated_value: {estimated_policy_value[key]} ------ estimator: {key}, \"", "\n", ")", "\n", "# test the performance of each estimator", "\n", "assert", "(", "\n", "np", ".", "abs", "(", "gt_mean", "-", "estimated_policy_value", "[", "key", "]", ")", "<=", "ci_bound", "\n", ")", ",", "f\"OPE of {key} did not work well (absolute error is greater than 3*sigma)\"", "\n", "\n", "# check if q_hat = 0 case of cascade-dr coincides with rips", "\n", "", "cascade_dr_estimated_policy_value_", "=", "dr", ".", "estimate_policy_value", "(", "\n", "slate_id", "=", "slate_id", ",", "\n", "action", "=", "action", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore_cascade", "=", "pscore", ",", "\n", "position", "=", "position", ",", "\n", "evaluation_policy_pscore_cascade", "=", "evaluation_policy_pscore", ",", "\n", "q_hat", "=", "np", ".", "zeros_like", "(", "q_hat", ")", ",", "\n", "evaluation_policy_action_dist", "=", "evaluation_policy_action_dist", ",", "\n", ")", "\n", "rips_estimated_policy_value", "=", "rips", ".", "estimate_policy_value", "(", "\n", "slate_id", "=", "slate_id", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore_cascade", "=", "pscore", ",", "\n", "position", "=", "position", ",", "\n", "evaluation_policy_pscore_cascade", "=", "evaluation_policy_pscore", ",", "\n", ")", "\n", "assert", "np", ".", "allclose", "(", "\n", "np", ".", "array", "(", "[", "cascade_dr_estimated_policy_value_", "]", ")", ",", "\n", "np", ".", "array", "(", "[", "rips_estimated_policy_value", "]", ")", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_regression_models_slate.test_initializing_regression_models_using_invalid_input_data": [[330, 349], ["pytest.mark.parametrize", "pytest.raises", "obp.ope.SlateRegressionModel"], "function", ["None"], ["@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"n_unique_action, len_list, fitting_method, base_model, err, description\"", ",", "\n", "invalid_input_of_initializing_regression_models", ",", "\n", ")", "\n", "def", "test_initializing_regression_models_using_invalid_input_data", "(", "\n", "n_unique_action", ",", "\n", "len_list", ",", "\n", "fitting_method", ",", "\n", "base_model", ",", "\n", "err", ",", "\n", "description", ",", "\n", ")", "->", "None", ":", "\n", "# initialization raises ValueError", "\n", "    ", "with", "pytest", ".", "raises", "(", "err", ",", "match", "=", "f\"{description}*\"", ")", ":", "\n", "        ", "_", "=", "SlateRegressionModel", "(", "\n", "n_unique_action", "=", "n_unique_action", ",", "\n", "len_list", "=", "len_list", ",", "\n", "base_model", "=", "base_model", ",", "\n", "fitting_method", "=", "fitting_method", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_regression_models_slate.test_fitting_regression_models_using_invalid_input_data": [[352, 381], ["pytest.mark.parametrize", "pytest.raises", "obp.ope.SlateRegressionModel", "obp.ope.SlateRegressionModel.fit_predict", "sklearn.linear_model.Ridge"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.classification_model.PropensityScoreEstimator.fit_predict"], ["", "", "@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"context, action, reward, pscore, evaluation_policy_pscore, evaluation_policy_action_dist, err, description\"", ",", "\n", "invalid_input_of_fitting_regression_models", ",", "\n", ")", "\n", "def", "test_fitting_regression_models_using_invalid_input_data", "(", "\n", "context", ",", "\n", "action", ",", "\n", "reward", ",", "\n", "pscore", ",", "\n", "evaluation_policy_pscore", ",", "\n", "evaluation_policy_action_dist", ",", "\n", "err", ",", "\n", "description", ",", "\n", ")", "->", "None", ":", "\n", "# fit_predict function raises ValueError", "\n", "    ", "with", "pytest", ".", "raises", "(", "err", ",", "match", "=", "f\"{description}*\"", ")", ":", "\n", "        ", "regression_model", "=", "SlateRegressionModel", "(", "\n", "n_unique_action", "=", "n_unique_action", ",", "\n", "len_list", "=", "len_list", ",", "\n", "base_model", "=", "Ridge", "(", "**", "hyperparams", "[", "\"ridge\"", "]", ")", ",", "\n", "fitting_method", "=", "\"normal\"", ",", "\n", ")", "\n", "_", "=", "regression_model", ".", "fit_predict", "(", "\n", "context", "=", "context", ",", "\n", "action", "=", "action", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore_cascade", "=", "pscore", ",", "\n", "evaluation_policy_pscore_cascade", "=", "evaluation_policy_pscore", ",", "\n", "evaluation_policy_action_dist", "=", "evaluation_policy_action_dist", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_regression_models_slate.test_regression_models_using_valid_input_data": [[384, 412], ["pytest.mark.parametrize", "obp.ope.SlateRegressionModel", "obp.ope.SlateRegressionModel.fit_predict", "sklearn.linear_model.Ridge"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.classification_model.PropensityScoreEstimator.fit_predict"], ["", "", "@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"context, action, reward, pscore, evaluation_policy_pscore, evaluation_policy_action_dist, description\"", ",", "\n", "valid_input_of_regression_models", ",", "\n", ")", "\n", "def", "test_regression_models_using_valid_input_data", "(", "\n", "context", ",", "\n", "action", ",", "\n", "reward", ",", "\n", "pscore", ",", "\n", "evaluation_policy_pscore", ",", "\n", "evaluation_policy_action_dist", ",", "\n", "description", ",", "\n", ")", "->", "None", ":", "\n", "# fit_predict", "\n", "    ", "for", "fitting_method", "in", "[", "\"normal\"", ",", "\"iw\"", "]", ":", "\n", "        ", "regression_model", "=", "SlateRegressionModel", "(", "\n", "n_unique_action", "=", "n_unique_action", ",", "\n", "len_list", "=", "len_list", ",", "\n", "base_model", "=", "Ridge", "(", "**", "hyperparams", "[", "\"ridge\"", "]", ")", ",", "\n", "fitting_method", "=", "fitting_method", ",", "\n", ")", "\n", "_", "=", "regression_model", ".", "fit_predict", "(", "\n", "context", "=", "context", ",", "\n", "action", "=", "action", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore_cascade", "=", "pscore", ",", "\n", "evaluation_policy_pscore_cascade", "=", "evaluation_policy_pscore", ",", "\n", "evaluation_policy_action_dist", "=", "evaluation_policy_action_dist", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_regression_models_slate.test_cascade_dr_criterion_using_cascade_additive_log": [[415, 496], ["obp.dataset.SyntheticSlateBanditDataset", "obp.dataset.SyntheticSlateBanditDataset.obtain_batch_bandit_feedback", "obp.dataset.SyntheticSlateBanditDataset.obtain_pscore_given_evaluation_policy_logit", "obp.dataset.SyntheticSlateBanditDataset.calc_evaluation_policy_action_dist", "test_regression_models_slate.calc_ground_truth_mean_reward_function", "numpy.ones", "numpy.ones", "model_dict.items", "obp.ope.SlateRegressionModel", "obp.ope.SlateRegressionModel.fit_predict", "print", "numpy.abs", "numpy.abs", "numpy.mean", "model", "numpy.mean"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.multiclass.MultiClassToBanditReduction.obtain_batch_bandit_feedback", "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.synthetic_slate.SyntheticSlateBanditDataset.obtain_pscore_given_evaluation_policy_logit", "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.synthetic_slate.SyntheticSlateBanditDataset.calc_evaluation_policy_action_dist", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_regression_models_slate.calc_ground_truth_mean_reward_function", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.classification_model.PropensityScoreEstimator.fit_predict"], ["", "", "def", "test_cascade_dr_criterion_using_cascade_additive_log", "(", ")", ":", "\n", "# set parameters", "\n", "    ", "n_unique_action", "=", "3", "\n", "len_list", "=", "3", "\n", "dim_context", "=", "2", "\n", "reward_type", "=", "\"binary\"", "\n", "random_state", "=", "12345", "\n", "n_rounds", "=", "1000", "\n", "reward_structure", "=", "\"cascade_additive\"", "\n", "click_model", "=", "None", "\n", "behavior_policy_function", "=", "linear_behavior_policy_logit", "\n", "reward_function", "=", "logistic_reward_function", "\n", "dataset", "=", "SyntheticSlateBanditDataset", "(", "\n", "n_unique_action", "=", "n_unique_action", ",", "\n", "len_list", "=", "len_list", ",", "\n", "dim_context", "=", "dim_context", ",", "\n", "reward_type", "=", "reward_type", ",", "\n", "reward_structure", "=", "reward_structure", ",", "\n", "click_model", "=", "click_model", ",", "\n", "random_state", "=", "random_state", ",", "\n", "behavior_policy_function", "=", "behavior_policy_function", ",", "\n", "base_reward_function", "=", "reward_function", ",", "\n", ")", "\n", "# obtain feedback", "\n", "bandit_feedback", "=", "dataset", ".", "obtain_batch_bandit_feedback", "(", "n_rounds", "=", "n_rounds", ")", "\n", "context", "=", "bandit_feedback", "[", "\"context\"", "]", "\n", "action", "=", "bandit_feedback", "[", "\"action\"", "]", "\n", "reward", "=", "bandit_feedback", "[", "\"reward\"", "]", "\n", "pscore", "=", "bandit_feedback", "[", "\"pscore_cascade\"", "]", "\n", "\n", "# random evaluation policy", "\n", "evaluation_policy_logit_", "=", "np", ".", "ones", "(", "(", "n_rounds", ",", "n_unique_action", ")", ")", "/", "n_unique_action", "\n", "evaluation_policy_action_dist", "=", "(", "\n", "np", ".", "ones", "(", "n_rounds", "*", "len_list", "*", "n_unique_action", ")", "/", "n_unique_action", "\n", ")", "\n", "(", "\n", "_", ",", "\n", "_", ",", "\n", "evaluation_policy_pscore", ",", "\n", ")", "=", "dataset", ".", "obtain_pscore_given_evaluation_policy_logit", "(", "\n", "action", "=", "action", ",", "\n", "evaluation_policy_logit_", "=", "evaluation_policy_logit_", ",", "\n", "return_pscore_item_position", "=", "False", ",", "\n", ")", "\n", "evaluation_policy_action_dist", "=", "dataset", ".", "calc_evaluation_policy_action_dist", "(", "\n", "action", "=", "action", ",", "\n", "evaluation_policy_logit_", "=", "evaluation_policy_logit_", ",", "\n", ")", "\n", "q_expected", "=", "calc_ground_truth_mean_reward_function", "(", "\n", "dataset", "=", "dataset", ",", "\n", "context", "=", "context", ",", "\n", "action", "=", "action", ",", "\n", "evaluation_policy_logit_", "=", "evaluation_policy_logit_", ",", "\n", ")", "\n", "\n", "# obtain q_hat and check if q_hat is effective", "\n", "cascade_dr_criterion_pass_rate", "=", "0.7", "\n", "for", "fitting_method", "in", "[", "\"normal\"", ",", "\"iw\"", "]", ":", "\n", "        ", "for", "model_name", ",", "model", "in", "model_dict", ".", "items", "(", ")", ":", "\n", "            ", "base_regression_model", "=", "SlateRegressionModel", "(", "\n", "base_model", "=", "model", "(", "**", "hyperparams", "[", "model_name", "]", ")", ",", "\n", "len_list", "=", "len_list", ",", "\n", "n_unique_action", "=", "n_unique_action", ",", "\n", "fitting_method", "=", "fitting_method", ",", "\n", ")", "\n", "q_hat", "=", "base_regression_model", ".", "fit_predict", "(", "\n", "context", "=", "context", ",", "\n", "action", "=", "action", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore_cascade", "=", "pscore", ",", "\n", "evaluation_policy_pscore_cascade", "=", "evaluation_policy_pscore", ",", "\n", "evaluation_policy_action_dist", "=", "evaluation_policy_action_dist", ",", "\n", ")", "\n", "# compare dr criterion", "\n", "cascade_dr_criterion", "=", "np", ".", "abs", "(", "(", "q_expected", "-", "q_hat", ")", ")", "-", "np", ".", "abs", "(", "q_hat", ")", "\n", "print", "(", "\n", "f\"Dr criterion is satisfied with probability {np.mean(cascade_dr_criterion <= 0)} ------ model: {model_name} ({fitting_method}),\"", "\n", ")", "\n", "assert", "(", "\n", "np", ".", "mean", "(", "cascade_dr_criterion", "<=", "0", ")", ">=", "cascade_dr_criterion_pass_rate", "\n", ")", ",", "f\" should be satisfied with a probability at least {cascade_dr_criterion_pass_rate}\"", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_regression_models_slate.test_cascade_dr_criterion_using_independent_log": [[498, 579], ["obp.dataset.SyntheticSlateBanditDataset", "obp.dataset.SyntheticSlateBanditDataset.obtain_batch_bandit_feedback", "obp.dataset.SyntheticSlateBanditDataset.obtain_pscore_given_evaluation_policy_logit", "obp.dataset.SyntheticSlateBanditDataset.calc_evaluation_policy_action_dist", "test_regression_models_slate.calc_ground_truth_mean_reward_function", "numpy.ones", "numpy.ones", "model_dict.items", "obp.ope.SlateRegressionModel", "obp.ope.SlateRegressionModel.fit_predict", "print", "numpy.abs", "numpy.abs", "numpy.mean", "model", "numpy.mean"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.multiclass.MultiClassToBanditReduction.obtain_batch_bandit_feedback", "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.synthetic_slate.SyntheticSlateBanditDataset.obtain_pscore_given_evaluation_policy_logit", "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.synthetic_slate.SyntheticSlateBanditDataset.calc_evaluation_policy_action_dist", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_regression_models_slate.calc_ground_truth_mean_reward_function", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.classification_model.PropensityScoreEstimator.fit_predict"], ["", "", "", "def", "test_cascade_dr_criterion_using_independent_log", "(", ")", ":", "\n", "# set parameters", "\n", "    ", "n_unique_action", "=", "3", "\n", "len_list", "=", "3", "\n", "dim_context", "=", "2", "\n", "reward_type", "=", "\"binary\"", "\n", "random_state", "=", "12345", "\n", "n_rounds", "=", "1000", "\n", "reward_structure", "=", "\"independent\"", "\n", "click_model", "=", "None", "\n", "behavior_policy_function", "=", "linear_behavior_policy_logit", "\n", "reward_function", "=", "logistic_reward_function", "\n", "dataset", "=", "SyntheticSlateBanditDataset", "(", "\n", "n_unique_action", "=", "n_unique_action", ",", "\n", "len_list", "=", "len_list", ",", "\n", "dim_context", "=", "dim_context", ",", "\n", "reward_type", "=", "reward_type", ",", "\n", "reward_structure", "=", "reward_structure", ",", "\n", "click_model", "=", "click_model", ",", "\n", "random_state", "=", "random_state", ",", "\n", "behavior_policy_function", "=", "behavior_policy_function", ",", "\n", "base_reward_function", "=", "reward_function", ",", "\n", ")", "\n", "# obtain feedback", "\n", "bandit_feedback", "=", "dataset", ".", "obtain_batch_bandit_feedback", "(", "n_rounds", "=", "n_rounds", ")", "\n", "context", "=", "bandit_feedback", "[", "\"context\"", "]", "\n", "action", "=", "bandit_feedback", "[", "\"action\"", "]", "\n", "reward", "=", "bandit_feedback", "[", "\"reward\"", "]", "\n", "pscore", "=", "bandit_feedback", "[", "\"pscore_cascade\"", "]", "\n", "\n", "# random evaluation policy", "\n", "evaluation_policy_logit_", "=", "np", ".", "ones", "(", "(", "n_rounds", ",", "n_unique_action", ")", ")", "/", "n_unique_action", "\n", "evaluation_policy_action_dist", "=", "(", "\n", "np", ".", "ones", "(", "n_rounds", "*", "len_list", "*", "n_unique_action", ")", "/", "n_unique_action", "\n", ")", "\n", "(", "\n", "_", ",", "\n", "_", ",", "\n", "evaluation_policy_pscore", ",", "\n", ")", "=", "dataset", ".", "obtain_pscore_given_evaluation_policy_logit", "(", "\n", "action", "=", "action", ",", "\n", "evaluation_policy_logit_", "=", "evaluation_policy_logit_", ",", "\n", "return_pscore_item_position", "=", "False", ",", "\n", ")", "\n", "evaluation_policy_action_dist", "=", "dataset", ".", "calc_evaluation_policy_action_dist", "(", "\n", "action", "=", "action", ",", "\n", "evaluation_policy_logit_", "=", "evaluation_policy_logit_", ",", "\n", ")", "\n", "q_expected", "=", "calc_ground_truth_mean_reward_function", "(", "\n", "dataset", "=", "dataset", ",", "\n", "context", "=", "context", ",", "\n", "action", "=", "action", ",", "\n", "evaluation_policy_logit_", "=", "evaluation_policy_logit_", ",", "\n", ")", "\n", "\n", "# obtain q_hat and check if q_hat is effective", "\n", "cascade_dr_criterion_pass_rate", "=", "0.7", "\n", "for", "fitting_method", "in", "[", "\"normal\"", ",", "\"iw\"", "]", ":", "\n", "        ", "for", "model_name", ",", "model", "in", "model_dict", ".", "items", "(", ")", ":", "\n", "            ", "base_regression_model", "=", "SlateRegressionModel", "(", "\n", "base_model", "=", "model", "(", "**", "hyperparams", "[", "model_name", "]", ")", ",", "\n", "len_list", "=", "len_list", ",", "\n", "n_unique_action", "=", "n_unique_action", ",", "\n", "fitting_method", "=", "fitting_method", ",", "\n", ")", "\n", "q_hat", "=", "base_regression_model", ".", "fit_predict", "(", "\n", "context", "=", "context", ",", "\n", "action", "=", "action", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore_cascade", "=", "pscore", ",", "\n", "evaluation_policy_pscore_cascade", "=", "evaluation_policy_pscore", ",", "\n", "evaluation_policy_action_dist", "=", "evaluation_policy_action_dist", ",", "\n", ")", "\n", "# compare dr criterion", "\n", "cascade_dr_criterion", "=", "np", ".", "abs", "(", "(", "q_expected", "-", "q_hat", ")", ")", "-", "np", ".", "abs", "(", "q_hat", ")", "\n", "print", "(", "\n", "f\"Dr criterion is satisfied with probability {np.mean(cascade_dr_criterion <= 0)} ------ model: {model_name} ({fitting_method}),\"", "\n", ")", "\n", "assert", "(", "\n", "np", ".", "mean", "(", "cascade_dr_criterion", "<=", "0", ")", ">=", "cascade_dr_criterion_pass_rate", "\n", ")", ",", "f\" should be satisfied with a probability at least {cascade_dr_criterion_pass_rate}\"", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_regression_models_slate.test_cascade_dr_criterion_using_standard_additive_log": [[581, 662], ["obp.dataset.SyntheticSlateBanditDataset", "obp.dataset.SyntheticSlateBanditDataset.obtain_batch_bandit_feedback", "obp.dataset.SyntheticSlateBanditDataset.obtain_pscore_given_evaluation_policy_logit", "obp.dataset.SyntheticSlateBanditDataset.calc_evaluation_policy_action_dist", "test_regression_models_slate.calc_ground_truth_mean_reward_function", "numpy.ones", "numpy.ones", "model_dict.items", "obp.ope.SlateRegressionModel", "obp.ope.SlateRegressionModel.fit_predict", "print", "numpy.abs", "numpy.abs", "numpy.mean", "model", "numpy.mean"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.multiclass.MultiClassToBanditReduction.obtain_batch_bandit_feedback", "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.synthetic_slate.SyntheticSlateBanditDataset.obtain_pscore_given_evaluation_policy_logit", "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.synthetic_slate.SyntheticSlateBanditDataset.calc_evaluation_policy_action_dist", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_regression_models_slate.calc_ground_truth_mean_reward_function", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.classification_model.PropensityScoreEstimator.fit_predict"], ["", "", "", "def", "test_cascade_dr_criterion_using_standard_additive_log", "(", ")", ":", "\n", "# set parameters", "\n", "    ", "n_unique_action", "=", "3", "\n", "len_list", "=", "3", "\n", "dim_context", "=", "2", "\n", "reward_type", "=", "\"binary\"", "\n", "random_state", "=", "12345", "\n", "n_rounds", "=", "1000", "\n", "reward_structure", "=", "\"standard_additive\"", "\n", "click_model", "=", "None", "\n", "behavior_policy_function", "=", "linear_behavior_policy_logit", "\n", "reward_function", "=", "logistic_reward_function", "\n", "dataset", "=", "SyntheticSlateBanditDataset", "(", "\n", "n_unique_action", "=", "n_unique_action", ",", "\n", "len_list", "=", "len_list", ",", "\n", "dim_context", "=", "dim_context", ",", "\n", "reward_type", "=", "reward_type", ",", "\n", "reward_structure", "=", "reward_structure", ",", "\n", "click_model", "=", "click_model", ",", "\n", "random_state", "=", "random_state", ",", "\n", "behavior_policy_function", "=", "behavior_policy_function", ",", "\n", "base_reward_function", "=", "reward_function", ",", "\n", ")", "\n", "# obtain feedback", "\n", "bandit_feedback", "=", "dataset", ".", "obtain_batch_bandit_feedback", "(", "n_rounds", "=", "n_rounds", ")", "\n", "context", "=", "bandit_feedback", "[", "\"context\"", "]", "\n", "action", "=", "bandit_feedback", "[", "\"action\"", "]", "\n", "reward", "=", "bandit_feedback", "[", "\"reward\"", "]", "\n", "pscore", "=", "bandit_feedback", "[", "\"pscore_cascade\"", "]", "\n", "\n", "# random evaluation policy", "\n", "evaluation_policy_logit_", "=", "np", ".", "ones", "(", "(", "n_rounds", ",", "n_unique_action", ")", ")", "/", "n_unique_action", "\n", "evaluation_policy_action_dist", "=", "(", "\n", "np", ".", "ones", "(", "n_rounds", "*", "len_list", "*", "n_unique_action", ")", "/", "n_unique_action", "\n", ")", "\n", "(", "\n", "_", ",", "\n", "_", ",", "\n", "evaluation_policy_pscore", ",", "\n", ")", "=", "dataset", ".", "obtain_pscore_given_evaluation_policy_logit", "(", "\n", "action", "=", "action", ",", "\n", "evaluation_policy_logit_", "=", "evaluation_policy_logit_", ",", "\n", "return_pscore_item_position", "=", "False", ",", "\n", ")", "\n", "evaluation_policy_action_dist", "=", "dataset", ".", "calc_evaluation_policy_action_dist", "(", "\n", "action", "=", "action", ",", "\n", "evaluation_policy_logit_", "=", "evaluation_policy_logit_", ",", "\n", ")", "\n", "q_expected", "=", "calc_ground_truth_mean_reward_function", "(", "\n", "dataset", "=", "dataset", ",", "\n", "context", "=", "context", ",", "\n", "action", "=", "action", ",", "\n", "evaluation_policy_logit_", "=", "evaluation_policy_logit_", ",", "\n", ")", "\n", "\n", "# obtain q_hat and check if q_hat is effective", "\n", "cascade_dr_criterion_pass_rate", "=", "0.7", "\n", "for", "fitting_method", "in", "[", "\"normal\"", ",", "\"iw\"", "]", ":", "\n", "        ", "for", "model_name", ",", "model", "in", "model_dict", ".", "items", "(", ")", ":", "\n", "            ", "base_regression_model", "=", "SlateRegressionModel", "(", "\n", "base_model", "=", "model", "(", "**", "hyperparams", "[", "model_name", "]", ")", ",", "\n", "len_list", "=", "len_list", ",", "\n", "n_unique_action", "=", "n_unique_action", ",", "\n", "fitting_method", "=", "fitting_method", ",", "\n", ")", "\n", "q_hat", "=", "base_regression_model", ".", "fit_predict", "(", "\n", "context", "=", "context", ",", "\n", "action", "=", "action", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore_cascade", "=", "pscore", ",", "\n", "evaluation_policy_pscore_cascade", "=", "evaluation_policy_pscore", ",", "\n", "evaluation_policy_action_dist", "=", "evaluation_policy_action_dist", ",", "\n", ")", "\n", "# compare dr criterion", "\n", "cascade_dr_criterion", "=", "np", ".", "abs", "(", "(", "q_expected", "-", "q_hat", ")", ")", "-", "np", ".", "abs", "(", "q_hat", ")", "\n", "print", "(", "\n", "f\"Dr criterion is satisfied with probability {np.mean(cascade_dr_criterion <= 0)} ------ model: {model_name} ({fitting_method}),\"", "\n", ")", "\n", "assert", "(", "\n", "np", ".", "mean", "(", "cascade_dr_criterion", "<=", "0", ")", ">=", "cascade_dr_criterion_pass_rate", "\n", ")", ",", "f\" should be satisfied with a probability at least {cascade_dr_criterion_pass_rate}\"", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_regression_models_slate.calc_ground_truth_mean_reward_function": [[664, 738], ["len", "action.reshape.reshape", "numpy.zeros", "range", "np.zeros.flatten", "range", "numpy.array().astype", "len", "numpy.tile", "range", "action[].reshape", "range", "numpy.array", "action[].reshape.copy", "numpy.concatenate", "test_regression_models_slate.calc_ground_truth_mean_reward_function_given_enumerated_slate_actions", "action[].reshape.copy", "test_regression_models_slate.calc_ground_truth_mean_reward_function_given_enumerated_slate_actions", "itertools.product", "itertools.permutations", "numpy.arange", "numpy.arange"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_regression_models_slate.calc_ground_truth_mean_reward_function_given_enumerated_slate_actions", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_regression_models_slate.calc_ground_truth_mean_reward_function_given_enumerated_slate_actions"], ["", "", "", "def", "calc_ground_truth_mean_reward_function", "(", "\n", "dataset", ",", "\n", "context", ":", "np", ".", "ndarray", ",", "\n", "action", ":", "np", ".", "ndarray", ",", "\n", "evaluation_policy_logit_", ":", "np", ".", "ndarray", ",", "\n", ")", ":", "\n", "    ", "n_rounds", "=", "len", "(", "context", ")", "\n", "action", "=", "action", ".", "reshape", "(", "(", "n_rounds", ",", "dataset", ".", "len_list", ")", ")", "\n", "ground_truth_mean_reward_function", "=", "np", ".", "zeros", "(", "\n", "(", "n_rounds", ",", "dataset", ".", "len_list", ",", "dataset", ".", "n_unique_action", ")", ",", "dtype", "=", "float", "\n", ")", "\n", "\n", "for", "position", "in", "range", "(", "dataset", ".", "len_list", ")", ":", "\n", "        ", "if", "position", "!=", "dataset", ".", "len_list", "-", "1", ":", "\n", "            ", "if", "dataset", ".", "is_factorizable", ":", "\n", "                ", "enumerated_slate_actions", "=", "[", "\n", "_", "\n", "for", "_", "in", "product", "(", "\n", "np", ".", "arange", "(", "dataset", ".", "n_unique_action", ")", ",", "\n", "repeat", "=", "dataset", ".", "len_list", "-", "position", "-", "1", ",", "\n", ")", "\n", "]", "\n", "", "else", ":", "\n", "                ", "enumerated_slate_actions", "=", "[", "\n", "_", "\n", "for", "_", "in", "permutations", "(", "\n", "np", ".", "arange", "(", "dataset", ".", "n_unique_action", ")", ",", "\n", "dataset", ".", "len_list", "-", "position", "-", "1", ",", "\n", ")", "\n", "]", "\n", "", "enumerated_slate_actions", "=", "np", ".", "array", "(", "enumerated_slate_actions", ")", ".", "astype", "(", "\"int8\"", ")", "\n", "n_enumerated_slate_actions", "=", "len", "(", "enumerated_slate_actions", ")", "\n", "\n", "", "for", "i", "in", "range", "(", "n_rounds", ")", ":", "\n", "            ", "if", "position", "!=", "dataset", ".", "len_list", "-", "1", ":", "\n", "                ", "action_", "=", "np", ".", "tile", "(", "\n", "action", "[", "i", "]", "[", ":", "position", "+", "1", "]", ",", "(", "n_enumerated_slate_actions", ",", "1", ")", "\n", ")", "\n", "for", "a_", "in", "range", "(", "dataset", ".", "n_unique_action", ")", ":", "\n", "                    ", "action__", "=", "action_", ".", "copy", "(", ")", "\n", "action__", "[", ":", ",", "position", "]", "=", "a_", "\n", "enumerated_slate_actions_", "=", "np", ".", "concatenate", "(", "\n", "[", "action_", ",", "enumerated_slate_actions", "]", ",", "axis", "=", "1", "\n", ")", "\n", "ground_truth_mean_reward_function", "[", "\n", "i", ",", "position", ",", "a_", "\n", "]", "=", "calc_ground_truth_mean_reward_function_given_enumerated_slate_actions", "(", "\n", "dataset", "=", "dataset", ",", "\n", "context", "=", "context", ",", "\n", "evaluation_policy_logit_", "=", "evaluation_policy_logit_", ",", "\n", "enumerated_slate_actions", "=", "enumerated_slate_actions_", ",", "\n", "i", "=", "i", ",", "\n", "position", "=", "position", ",", "\n", ")", "\n", "\n", "", "", "else", ":", "\n", "                ", "action_", "=", "action", "[", "i", "]", ".", "reshape", "(", "(", "1", ",", "dataset", ".", "len_list", ")", ")", "\n", "for", "a_", "in", "range", "(", "dataset", ".", "n_unique_action", ")", ":", "\n", "                    ", "action__", "=", "action_", ".", "copy", "(", ")", "\n", "action__", "[", ":", ",", "position", "]", "=", "a_", "\n", "enumerated_slate_actions_", "=", "action__", "\n", "n_enumerated_slate_actions", "=", "1", "\n", "ground_truth_mean_reward_function", "[", "\n", "i", ",", "position", ",", "a_", "\n", "]", "=", "calc_ground_truth_mean_reward_function_given_enumerated_slate_actions", "(", "\n", "dataset", "=", "dataset", ",", "\n", "context", "=", "context", ",", "\n", "evaluation_policy_logit_", "=", "evaluation_policy_logit_", ",", "\n", "enumerated_slate_actions", "=", "enumerated_slate_actions_", ",", "\n", "i", "=", "i", ",", "\n", "position", "=", "position", ",", "\n", ")", "\n", "\n", "", "", "", "", "return", "ground_truth_mean_reward_function", ".", "flatten", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_regression_models_slate.calc_ground_truth_mean_reward_function_given_enumerated_slate_actions": [[740, 826], ["evaluation_policy_logit_[].reshape", "len", "numpy.array", "dataset.sample_contextfree_expected_reward", "numpy.tile", "dataset.reward_function", "obp.utils.softmax", "range", "np.array.append", "evaluation_policy_logit_[].reshape.copy", "range", "np.array.append", "numpy.array", "numpy.ones", "numpy.zeros", "numpy.arange", "context[].reshape", "enumerated_slate_actions.flatten", "dataset.reward_function.sum", "obp.utils.softmax", "numpy.arange", "numpy.arange", "numpy.array"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.synthetic.SyntheticBanditDataset.sample_contextfree_expected_reward", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.softmax", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.softmax"], ["", "def", "calc_ground_truth_mean_reward_function_given_enumerated_slate_actions", "(", "\n", "dataset", ",", "\n", "context", ":", "np", ".", "ndarray", ",", "\n", "evaluation_policy_logit_", ":", "np", ".", "ndarray", ",", "\n", "enumerated_slate_actions", ":", "np", ".", "ndarray", ",", "\n", "i", ":", "int", ",", "\n", "position", ":", "int", ",", "\n", ")", ":", "\n", "    ", "pscores", "=", "[", "]", "\n", "evaluation_policy_logit_i", "=", "evaluation_policy_logit_", "[", "i", "]", ".", "reshape", "(", "\n", "(", "1", ",", "dataset", ".", "n_unique_action", ")", "\n", ")", "\n", "n_enumerated_slate_actions", "=", "len", "(", "enumerated_slate_actions", ")", "\n", "\n", "if", "dataset", ".", "is_factorizable", ":", "\n", "        ", "action_dist", "=", "softmax", "(", "evaluation_policy_logit_i", ")", "[", "0", "]", "\n", "\n", "for", "action_list", "in", "enumerated_slate_actions", ":", "\n", "            ", "pscore", "=", "1", "\n", "\n", "for", "position", "in", "range", "(", "dataset", ".", "len_list", ")", ":", "\n", "                ", "pscore", "*=", "action_dist", "[", "action_list", "[", "position", "]", "]", "\n", "\n", "", "pscores", ".", "append", "(", "pscore", ")", "\n", "\n", "", "", "else", ":", "\n", "        ", "for", "action_list", "in", "enumerated_slate_actions", ":", "\n", "            ", "pscore", "=", "1", "\n", "evaluation_policy_logit_i_", "=", "evaluation_policy_logit_i", ".", "copy", "(", ")", "\n", "\n", "for", "position", "in", "range", "(", "dataset", ".", "len_list", ")", ":", "\n", "                ", "action_dist", "=", "softmax", "(", "evaluation_policy_logit_i_", ")", "[", "0", "]", "\n", "pscore", "*=", "action_dist", "[", "action_list", "[", "position", "]", "]", "\n", "evaluation_policy_logit_i_", "[", "0", "]", "[", "action_list", "[", "position", "]", "]", "=", "-", "1e10", "\n", "\n", "", "pscores", ".", "append", "(", "pscore", ")", "\n", "\n", "", "", "pscores", "=", "np", ".", "array", "(", "pscores", ")", "\n", "\n", "# calculate expected slate-level reward for each combinatorial set of items (i.e., slate actions)", "\n", "if", "dataset", ".", "base_reward_function", "is", "None", ":", "\n", "        ", "expected_slot_reward", "=", "dataset", ".", "sample_contextfree_expected_reward", "(", "\n", "random_state", "=", "dataset", ".", "random_state", "\n", ")", "\n", "expected_slot_reward_tile", "=", "np", ".", "tile", "(", "\n", "expected_slot_reward", ",", "(", "n_enumerated_slate_actions", ",", "1", ",", "1", ")", "\n", ")", "\n", "expected_slate_rewards", "=", "np", ".", "array", "(", "\n", "[", "\n", "expected_slot_reward_tile", "[", "\n", "np", ".", "arange", "(", "n_enumerated_slate_actions", ")", "%", "n_enumerated_slate_actions", ",", "\n", "np", ".", "array", "(", "enumerated_slate_actions", ")", "[", ":", ",", "pos_", "]", ",", "\n", "pos_", ",", "\n", "]", "\n", "for", "pos_", "in", "np", ".", "arange", "(", "dataset", ".", "len_list", ")", "\n", "]", "\n", ")", ".", "T", "\n", "", "else", ":", "\n", "        ", "expected_slate_rewards", "=", "dataset", ".", "reward_function", "(", "\n", "context", "=", "context", "[", "i", "]", ".", "reshape", "(", "(", "1", ",", "-", "1", ")", ")", ",", "\n", "action_context", "=", "dataset", ".", "action_context", ",", "\n", "action", "=", "enumerated_slate_actions", ".", "flatten", "(", ")", ",", "\n", "action_interaction_weight_matrix", "=", "dataset", ".", "action_interaction_weight_matrix", ",", "\n", "base_reward_function", "=", "dataset", ".", "base_reward_function", ",", "\n", "reward_type", "=", "dataset", ".", "reward_type", ",", "\n", "reward_structure", "=", "dataset", ".", "reward_structure", ",", "\n", "len_list", "=", "dataset", ".", "len_list", ",", "\n", "is_enumerated", "=", "True", ",", "\n", "random_state", "=", "dataset", ".", "random_state", ",", "\n", ")", "\n", "# click models based on expected reward", "\n", "expected_slate_rewards", "*=", "dataset", ".", "exam_weight", "\n", "if", "dataset", ".", "reward_type", "==", "\"binary\"", ":", "\n", "            ", "discount_factors", "=", "np", ".", "ones", "(", "expected_slate_rewards", ".", "shape", "[", "0", "]", ")", "\n", "previous_slot_expected_reward", "=", "np", ".", "zeros", "(", "expected_slate_rewards", ".", "shape", "[", "0", "]", ")", "\n", "for", "pos_", "in", "np", ".", "arange", "(", "dataset", ".", "len_list", ")", ":", "\n", "                ", "discount_factors", "*=", "(", "\n", "previous_slot_expected_reward", "*", "dataset", ".", "attractiveness", "[", "pos_", "]", "\n", "+", "(", "1", "-", "previous_slot_expected_reward", ")", "\n", ")", "\n", "expected_slate_rewards", "[", ":", ",", "pos_", "]", "=", "(", "\n", "discount_factors", "*", "expected_slate_rewards", "[", ":", ",", "pos_", "]", "\n", ")", "\n", "previous_slot_expected_reward", "=", "expected_slate_rewards", "[", ":", ",", "pos_", "]", "\n", "\n", "", "", "", "return", "(", "pscores", "*", "expected_slate_rewards", ".", "sum", "(", "axis", "=", "1", ")", ")", ".", "sum", "(", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_meta.DirectMethodMock._estimate_round_rewards": [[33, 41], ["None"], "methods", ["None"], ["def", "_estimate_round_rewards", "(", "\n", "self", ",", "\n", "position", ":", "np", ".", "ndarray", ",", "\n", "action_dist", ":", "np", ".", "ndarray", ",", "\n", "estimated_rewards_by_reg_model", ":", "np", ".", "ndarray", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "float", ":", "\n", "        ", "return", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_meta.DirectMethodMock.estimate_policy_value": [[42, 67], ["None"], "methods", ["None"], ["", "def", "estimate_policy_value", "(", "\n", "self", ",", "\n", "position", ":", "np", ".", "ndarray", ",", "\n", "action_dist", ":", "np", ".", "ndarray", ",", "\n", "estimated_rewards_by_reg_model", ":", "np", ".", "ndarray", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "float", ":", "\n", "        ", "\"\"\"Estimate the policy value of evaluation policy.\n\n        Parameters\n        ----------\n        position: array-like, shape (n_rounds,)\n            Indices to differentiate positions in a recommendation interface where the actions are presented.\n\n        action_dist: array-like, shape (n_rounds, n_actions, len_list)\n            Action choice probabilities of the evaluation policy (can be deterministic), i.e., :math:`\\\\pi_e(a_i|x_i)`.\n\n        estimated_rewards_by_reg_model: array-like, shape (n_rounds, n_actions, len_list)\n            Estimated expected rewards given context, action, and position, i.e., :math:`\\\\hat{q}(x_i,a_i)`.\n\n        Returns\n        ----------\n        mock_policy_value: float\n        \"\"\"", "\n", "return", "mock_policy_value", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_meta.DirectMethodMock.estimate_interval": [[68, 111], ["obp.utils.check_confidence_interval_arguments"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_confidence_interval_arguments"], ["", "def", "estimate_interval", "(", "\n", "self", ",", "\n", "position", ":", "np", ".", "ndarray", ",", "\n", "action_dist", ":", "np", ".", "ndarray", ",", "\n", "estimated_rewards_by_reg_model", ":", "np", ".", "ndarray", ",", "\n", "alpha", ":", "float", "=", "0.05", ",", "\n", "n_bootstrap_samples", ":", "int", "=", "10000", ",", "\n", "random_state", ":", "Optional", "[", "int", "]", "=", "None", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "Dict", "[", "str", ",", "float", "]", ":", "\n", "        ", "\"\"\"Estimate the confidence interval of the policy value using bootstrap.\n\n        Parameters\n        ----------\n        position: array-like, shape (n_rounds,)\n            Indices to differentiate positions in a recommendation interface where the actions are presented.\n\n        action_dist: array-like, shape (n_rounds, n_actions, len_list)\n            Action choice probabilities of the evaluation policy (can be deterministic), i.e., :math:`\\\\pi_e(a_i|x_i)`.\n\n        estimated_rewards_by_reg_model: array-like, shape (n_rounds, n_actions, len_list)\n            Estimated expected rewards given context, action, and position, i.e., :math:`\\\\hat{q}(x_i,a_i)`.\n\n        alpha: float, default=0.05\n            Significance level.\n\n        n_bootstrap_samples: int, default=10000\n            Number of resampling performed in bootstrap sampling.\n\n        random_state: int, default=None\n            Controls the random seed in bootstrap sampling.\n\n        Returns\n        ----------\n        mock_confidence_interval: Dict[str, float]\n            Dictionary storing the estimated mean and upper-lower confidence bounds.\n        \"\"\"", "\n", "check_confidence_interval_arguments", "(", "\n", "alpha", "=", "alpha", ",", "\n", "n_bootstrap_samples", "=", "n_bootstrap_samples", ",", "\n", "random_state", "=", "random_state", ",", "\n", ")", "\n", "return", "mock_confidence_interval", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_meta.InverseProbabilityWeightingMock._estimate_round_rewards": [[120, 128], ["None"], "methods", ["None"], ["def", "_estimate_round_rewards", "(", "\n", "self", ",", "\n", "position", ":", "np", ".", "ndarray", ",", "\n", "action_dist", ":", "np", ".", "ndarray", ",", "\n", "estimated_rewards_by_reg_model", ":", "np", ".", "ndarray", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "float", ":", "\n", "        ", "return", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_meta.InverseProbabilityWeightingMock.estimate_policy_value": [[129, 167], ["None"], "methods", ["None"], ["", "def", "estimate_policy_value", "(", "\n", "self", ",", "\n", "reward", ":", "np", ".", "ndarray", ",", "\n", "action", ":", "np", ".", "ndarray", ",", "\n", "position", ":", "np", ".", "ndarray", ",", "\n", "pscore", ":", "np", ".", "ndarray", ",", "\n", "action_dist", ":", "np", ".", "ndarray", ",", "\n", "estimated_pscore", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "np", ".", "ndarray", ":", "\n", "        ", "\"\"\"Estimate the policy value of evaluation policy.\n\n        Parameters\n        ----------\n        reward: array-like, shape (n_rounds,)\n            Rewards observed for each data in logged bandit data, i.e., :math:`r_i`.\n\n        action: array-like, shape (n_rounds,)\n            Actions sampled by the logging/behavior policy for each data in logged bandit data, i.e., :math:`a_i`.\n\n        position: array-like, shape (n_rounds,)\n            Indices to differentiate positions in a recommendation interface where the actions are presented.\n\n        pscore: array-like, shape (n_rounds,)\n            Action choice probabilities of the logging/behavior policy (propensity scores), i.e., :math:`\\\\pi_b(a_i|x_i)`.\n\n        action_dist: array-like, shape (n_rounds, n_actions, len_list)\n            Action choice probabilities of the evaluation policy (can be deterministic), i.e., :math:`\\\\pi_e(a_i|x_i)`.\n\n        estimated_pscore: array-like, shape (n_rounds,), default=None\n            Estimated behavior policy (propensity scores), i.e., :math:`\\\\hat{\\\\pi}_b(a_i|x_i)`.\n\n        Returns\n        ----------\n        mock_policy_value: float\n\n        \"\"\"", "\n", "return", "mock_policy_value", "+", "self", ".", "eps", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_meta.InverseProbabilityWeightingMock.estimate_interval": [[168, 220], ["mock_confidence_interval.items"], "methods", ["None"], ["", "def", "estimate_interval", "(", "\n", "self", ",", "\n", "reward", ":", "np", ".", "ndarray", ",", "\n", "action", ":", "np", ".", "ndarray", ",", "\n", "position", ":", "np", ".", "ndarray", ",", "\n", "pscore", ":", "np", ".", "ndarray", ",", "\n", "action_dist", ":", "np", ".", "ndarray", ",", "\n", "estimated_pscore", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "alpha", ":", "float", "=", "0.05", ",", "\n", "n_bootstrap_samples", ":", "int", "=", "10000", ",", "\n", "random_state", ":", "Optional", "[", "int", "]", "=", "None", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "Dict", "[", "str", ",", "float", "]", ":", "\n", "        ", "\"\"\"Estimate the confidence interval of the policy value using bootstrap.\n\n        Parameters\n        ----------\n        reward: array-like, shape (n_rounds,)\n            Rewards observed for each data in logged bandit data, i.e., :math:`r_i`.\n\n        action: array-like, shape (n_rounds,)\n            Actions sampled by the logging/behavior policy for each data in logged bandit data, i.e., :math:`a_i`.\n\n        position: array-like, shape (n_rounds,)\n            Indices to differentiate positions in a recommendation interface where the actions are presented.\n\n        pscore: array-like, shape (n_rounds,)\n            Action choice probabilities of the logging/behavior policy (propensity scores), i.e., :math:`\\\\pi_b(a_i|x_i)`.\n\n        action_dist: array-like, shape (n_rounds, n_actions, len_list)\n            Action choice probabilities\n            by the evaluation policy (can be deterministic), i.e., :math:`\\\\pi_e(a_i|x_i)`.\n\n        estimated_pscore: array-like, shape (n_rounds,), default=None\n            Estimated behavior policy (propensity scores), i.e., :math:`\\\\hat{\\\\pi}_b(a_i|x_i)`.\n\n        alpha: float, default=0.05\n            Significance level.\n\n        n_bootstrap_samples: int, default=10000\n            Number of resampling performed in bootstrap sampling.\n\n        random_state: int, default=None\n            Controls the random seed in bootstrap sampling.\n\n        Returns\n        ----------\n        mock_confidence_interval: Dict[str, float]\n            Dictionary storing the estimated mean and upper-lower confidence bounds.\n\n        \"\"\"", "\n", "return", "{", "k", ":", "v", "+", "self", ".", "eps", "for", "k", ",", "v", "in", "mock_confidence_interval", ".", "items", "(", ")", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_meta.test_meta_post_init": [[229, 257], ["obp.ope.OffPolicyEvaluation", "obp.ope.OffPolicyEvaluation", "range", "len", "itertools.combinations", "pytest.raises", "obp.ope.OffPolicyEvaluation"], "function", ["None"], ["def", "test_meta_post_init", "(", "synthetic_bandit_feedback", ":", "BanditFeedback", ")", "->", "None", ":", "\n", "    ", "\"\"\"\n    Test the __post_init__ function\n    \"\"\"", "\n", "# __post_init__ saves the latter estimator when the same estimator name is used", "\n", "ope_", "=", "OffPolicyEvaluation", "(", "\n", "bandit_feedback", "=", "synthetic_bandit_feedback", ",", "ope_estimators", "=", "[", "ipw", ",", "ipw2", "]", "\n", ")", "\n", "assert", "ope_", ".", "ope_estimators_", "==", "{", "\"ipw\"", ":", "ipw2", "}", ",", "\"__post_init__ returns a wrong value\"", "\n", "# __post_init__ can handle the same estimator if the estimator names are different", "\n", "ope_", "=", "OffPolicyEvaluation", "(", "\n", "bandit_feedback", "=", "synthetic_bandit_feedback", ",", "ope_estimators", "=", "[", "ipw", ",", "ipw3", "]", "\n", ")", "\n", "assert", "ope_", ".", "ope_estimators_", "==", "{", "\n", "\"ipw\"", ":", "ipw", ",", "\n", "\"ipw3\"", ":", "ipw3", ",", "\n", "}", ",", "\"__post_init__ returns a wrong value\"", "\n", "# __post__init__ raises RuntimeError when necessary_keys are not included in the bandit_feedback", "\n", "necessary_keys", "=", "[", "\"action\"", ",", "\"position\"", ",", "\"reward\"", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "necessary_keys", ")", ")", ":", "\n", "        ", "for", "deleted_keys", "in", "itertools", ".", "combinations", "(", "necessary_keys", ",", "i", "+", "1", ")", ":", "\n", "            ", "invalid_bandit_feedback_dict", "=", "{", "key", ":", "\"_\"", "for", "key", "in", "necessary_keys", "}", "\n", "# delete", "\n", "for", "k", "in", "deleted_keys", ":", "\n", "                ", "del", "invalid_bandit_feedback_dict", "[", "k", "]", "\n", "", "with", "pytest", ".", "raises", "(", "RuntimeError", ",", "match", "=", "r\"Missing key*\"", ")", ":", "\n", "                ", "_", "=", "OffPolicyEvaluation", "(", "\n", "bandit_feedback", "=", "invalid_bandit_feedback_dict", ",", "ope_estimators", "=", "[", "ipw", "]", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_meta.test_meta_estimated_rewards_by_reg_model_inputs": [[260, 283], ["obp.ope.OffPolicyEvaluation", "numpy.zeros", "pytest.raises", "obp.ope.OffPolicyEvaluation.estimate_policy_values", "pytest.raises", "obp.ope.OffPolicyEvaluation.estimate_intervals", "obp.ope.DirectMethod"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta.OffPolicyEvaluation.estimate_policy_values", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta.OffPolicyEvaluation.estimate_intervals"], ["", "", "", "", "def", "test_meta_estimated_rewards_by_reg_model_inputs", "(", "\n", "synthetic_bandit_feedback", ":", "BanditFeedback", ",", "\n", ")", "->", "None", ":", "\n", "    ", "\"\"\"\n    Test the estimate_policy_values/estimate_intervals functions wrt estimated_rewards_by_reg_model\n    \"\"\"", "\n", "ope_", "=", "OffPolicyEvaluation", "(", "\n", "bandit_feedback", "=", "synthetic_bandit_feedback", ",", "ope_estimators", "=", "[", "DirectMethod", "(", ")", "]", "\n", ")", "\n", "\n", "action_dist", "=", "np", ".", "zeros", "(", "\n", "(", "synthetic_bandit_feedback", "[", "\"n_rounds\"", "]", ",", "synthetic_bandit_feedback", "[", "\"n_actions\"", "]", ")", "\n", ")", "\n", "with", "pytest", ".", "raises", "(", "ValueError", ")", ":", "\n", "        ", "ope_", ".", "estimate_policy_values", "(", "\n", "action_dist", "=", "action_dist", ",", "\n", "estimated_rewards_by_reg_model", "=", "None", ",", "\n", ")", "\n", "\n", "", "with", "pytest", ".", "raises", "(", "ValueError", ")", ":", "\n", "        ", "ope_", ".", "estimate_intervals", "(", "\n", "action_dist", "=", "action_dist", ",", "\n", "estimated_rewards_by_reg_model", "=", "None", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_meta.test_meta_create_estimator_inputs_using_invalid_input_data": [[323, 372], ["pytest.mark.parametrize", "obp.ope.OffPolicyEvaluation", "pytest.raises", "obp.ope.OffPolicyEvaluation._create_estimator_inputs", "pytest.raises", "obp.ope.OffPolicyEvaluation.estimate_policy_values", "pytest.raises", "obp.ope.OffPolicyEvaluation.estimate_intervals", "pytest.raises", "obp.ope.OffPolicyEvaluation.summarize_off_policy_estimates", "pytest.raises", "obp.ope.OffPolicyEvaluation.evaluate_performance_of_estimators", "pytest.raises", "obp.ope.OffPolicyEvaluation.summarize_estimators_comparison"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta.OffPolicyEvaluation._create_estimator_inputs", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta.OffPolicyEvaluation.estimate_policy_values", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta.OffPolicyEvaluation.estimate_intervals", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta.OffPolicyEvaluation.summarize_off_policy_estimates", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta.OffPolicyEvaluation.evaluate_performance_of_estimators", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta.OffPolicyEvaluation.summarize_estimators_comparison"], ["@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"action_dist, estimated_rewards_by_reg_model, description\"", ",", "\n", "invalid_input_of_create_estimator_inputs", ",", "\n", ")", "\n", "def", "test_meta_create_estimator_inputs_using_invalid_input_data", "(", "\n", "action_dist", ",", "\n", "estimated_rewards_by_reg_model", ",", "\n", "description", ":", "str", ",", "\n", "synthetic_bandit_feedback", ":", "BanditFeedback", ",", "\n", ")", "->", "None", ":", "\n", "    ", "\"\"\"\n    Test the _create_estimator_inputs using valid data\n    \"\"\"", "\n", "ope_", "=", "OffPolicyEvaluation", "(", "\n", "bandit_feedback", "=", "synthetic_bandit_feedback", ",", "ope_estimators", "=", "[", "ipw", "]", "\n", ")", "\n", "# raise ValueError when the shape of two arrays are different", "\n", "with", "pytest", ".", "raises", "(", "ValueError", ",", "match", "=", "f\"{description}*\"", ")", ":", "\n", "        ", "_", "=", "ope_", ".", "_create_estimator_inputs", "(", "\n", "action_dist", "=", "action_dist", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", ")", "\n", "# _create_estimator_inputs function is called in the following functions", "\n", "", "with", "pytest", ".", "raises", "(", "ValueError", ",", "match", "=", "f\"{description}*\"", ")", ":", "\n", "        ", "_", "=", "ope_", ".", "estimate_policy_values", "(", "\n", "action_dist", "=", "action_dist", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", ")", "\n", "", "with", "pytest", ".", "raises", "(", "ValueError", ",", "match", "=", "f\"{description}*\"", ")", ":", "\n", "        ", "_", "=", "ope_", ".", "estimate_intervals", "(", "\n", "action_dist", "=", "action_dist", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", ")", "\n", "", "with", "pytest", ".", "raises", "(", "ValueError", ",", "match", "=", "f\"{description}*\"", ")", ":", "\n", "        ", "_", "=", "ope_", ".", "summarize_off_policy_estimates", "(", "\n", "action_dist", "=", "action_dist", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", ")", "\n", "", "with", "pytest", ".", "raises", "(", "ValueError", ",", "match", "=", "f\"{description}*\"", ")", ":", "\n", "        ", "_", "=", "ope_", ".", "evaluate_performance_of_estimators", "(", "\n", "ground_truth_policy_value", "=", "0.1", ",", "\n", "action_dist", "=", "action_dist", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", ")", "\n", "", "with", "pytest", ".", "raises", "(", "ValueError", ",", "match", "=", "f\"{description}*\"", ")", ":", "\n", "        ", "_", "=", "ope_", ".", "summarize_estimators_comparison", "(", "\n", "ground_truth_policy_value", "=", "0.1", ",", "\n", "action_dist", "=", "action_dist", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_meta.test_meta_create_estimator_inputs_using_valid_input_data": [[375, 434], ["pytest.mark.parametrize", "obp.ope.OffPolicyEvaluation", "obp.ope.OffPolicyEvaluation._create_estimator_inputs", "obp.ope.OffPolicyEvaluation.estimate_policy_values", "obp.ope.OffPolicyEvaluation.estimate_intervals", "obp.ope.OffPolicyEvaluation.summarize_off_policy_estimates", "obp.ope.OffPolicyEvaluation.evaluate_performance_of_estimators", "obp.ope.OffPolicyEvaluation.summarize_estimators_comparison", "set", "set", "set", "set", "ope_._create_estimator_inputs.keys", "estimator_inputs[].keys"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta.OffPolicyEvaluation._create_estimator_inputs", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta.OffPolicyEvaluation.estimate_policy_values", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta.OffPolicyEvaluation.estimate_intervals", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta.OffPolicyEvaluation.summarize_off_policy_estimates", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta.OffPolicyEvaluation.evaluate_performance_of_estimators", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta.OffPolicyEvaluation.summarize_estimators_comparison"], ["", "", "@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"action_dist, estimated_rewards_by_reg_model, description\"", ",", "\n", "valid_input_of_create_estimator_inputs", ",", "\n", ")", "\n", "def", "test_meta_create_estimator_inputs_using_valid_input_data", "(", "\n", "action_dist", ",", "\n", "estimated_rewards_by_reg_model", ",", "\n", "description", ":", "str", ",", "\n", "synthetic_bandit_feedback", ":", "BanditFeedback", ",", "\n", ")", "->", "None", ":", "\n", "    ", "\"\"\"\n    Test the _create_estimator_inputs using invalid data\n    \"\"\"", "\n", "ope_", "=", "OffPolicyEvaluation", "(", "\n", "bandit_feedback", "=", "synthetic_bandit_feedback", ",", "ope_estimators", "=", "[", "ipw", "]", "\n", ")", "\n", "estimator_inputs", "=", "ope_", ".", "_create_estimator_inputs", "(", "\n", "action_dist", "=", "action_dist", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", ")", "\n", "assert", "set", "(", "estimator_inputs", ".", "keys", "(", ")", ")", "==", "set", "(", "[", "\"ipw\"", "]", ")", "\n", "assert", "set", "(", "estimator_inputs", "[", "\"ipw\"", "]", ".", "keys", "(", ")", ")", "==", "set", "(", "\n", "[", "\n", "\"reward\"", ",", "\n", "\"action\"", ",", "\n", "\"pscore\"", ",", "\n", "\"position\"", ",", "\n", "\"action_dist\"", ",", "\n", "\"estimated_rewards_by_reg_model\"", ",", "\n", "\"estimated_pscore\"", ",", "\n", "\"estimated_importance_weights\"", ",", "\n", "\"p_e_a\"", ",", "\n", "\"pi_b\"", ",", "\n", "\"context\"", ",", "\n", "\"action_embed\"", ",", "\n", "]", "\n", ")", ",", "f\"Invalid response of _create_estimator_inputs (test case: {description})\"", "\n", "# _create_estimator_inputs function is called in the following functions", "\n", "_", "=", "ope_", ".", "estimate_policy_values", "(", "\n", "action_dist", "=", "action_dist", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", ")", "\n", "_", "=", "ope_", ".", "estimate_intervals", "(", "\n", "action_dist", "=", "action_dist", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", ")", "\n", "_", "=", "ope_", ".", "summarize_off_policy_estimates", "(", "\n", "action_dist", "=", "action_dist", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", ")", "\n", "_", "=", "ope_", ".", "evaluate_performance_of_estimators", "(", "\n", "ground_truth_policy_value", "=", "0.1", ",", "\n", "action_dist", "=", "action_dist", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", ")", "\n", "_", "=", "ope_", ".", "summarize_estimators_comparison", "(", "\n", "ground_truth_policy_value", "=", "0.1", ",", "\n", "action_dist", "=", "action_dist", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_meta.test_meta_estimate_policy_values_using_valid_input_data": [[437, 473], ["pytest.mark.parametrize", "obp.ope.OffPolicyEvaluation", "obp.ope.OffPolicyEvaluation", "obp.ope.OffPolicyEvaluation.estimate_policy_values", "obp.ope.OffPolicyEvaluation.estimate_policy_values"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta.OffPolicyEvaluation.estimate_policy_values", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta.OffPolicyEvaluation.estimate_policy_values"], ["", "@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"action_dist, estimated_rewards_by_reg_model, description\"", ",", "\n", "valid_input_of_create_estimator_inputs", "[", ":", "-", "1", "]", ",", "\n", ")", "\n", "def", "test_meta_estimate_policy_values_using_valid_input_data", "(", "\n", "action_dist", ",", "\n", "estimated_rewards_by_reg_model", ",", "\n", "description", ":", "str", ",", "\n", "synthetic_bandit_feedback", ":", "BanditFeedback", ",", "\n", ")", "->", "None", ":", "\n", "    ", "\"\"\"\n    Test the response of estimate_policy_values using valid data\n    \"\"\"", "\n", "# single ope estimator", "\n", "ope_", "=", "OffPolicyEvaluation", "(", "\n", "bandit_feedback", "=", "synthetic_bandit_feedback", ",", "ope_estimators", "=", "[", "dm", "]", "\n", ")", "\n", "ope_", ".", "is_model_dependent", "=", "True", "\n", "assert", "ope_", ".", "estimate_policy_values", "(", "\n", "action_dist", "=", "action_dist", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", ")", "==", "{", "\n", "\"dm\"", ":", "mock_policy_value", "\n", "}", ",", "\"OffPolicyEvaluation.estimate_policy_values ([DirectMethod]) returns a wrong value\"", "\n", "# multiple ope estimators", "\n", "ope_", "=", "OffPolicyEvaluation", "(", "\n", "bandit_feedback", "=", "synthetic_bandit_feedback", ",", "ope_estimators", "=", "[", "dm", ",", "ipw", "]", "\n", ")", "\n", "ope_", ".", "is_model_dependent", "=", "True", "\n", "assert", "ope_", ".", "estimate_policy_values", "(", "\n", "action_dist", "=", "action_dist", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", ")", "==", "{", "\n", "\"dm\"", ":", "mock_policy_value", ",", "\n", "\"ipw\"", ":", "mock_policy_value", "+", "ipw", ".", "eps", ",", "\n", "}", ",", "\"OffPolicyEvaluation.estimate_policy_values ([DirectMethod, IPW]) returns a wrong value\"", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_meta.test_meta_estimate_intervals_using_invalid_input_data": [[509, 550], ["pytest.mark.parametrize", "pytest.mark.parametrize", "obp.ope.OffPolicyEvaluation", "pytest.raises", "obp.ope.OffPolicyEvaluation.estimate_intervals", "pytest.raises", "obp.ope.OffPolicyEvaluation.summarize_off_policy_estimates"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta.OffPolicyEvaluation.estimate_intervals", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta.OffPolicyEvaluation.summarize_off_policy_estimates"], ["@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"action_dist, estimated_rewards_by_reg_model, description_1\"", ",", "\n", "valid_input_of_create_estimator_inputs", ",", "\n", ")", "\n", "@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"alpha, n_bootstrap_samples, random_state, err, description_2\"", ",", "\n", "invalid_input_of_estimate_intervals", ",", "\n", ")", "\n", "def", "test_meta_estimate_intervals_using_invalid_input_data", "(", "\n", "action_dist", ",", "\n", "estimated_rewards_by_reg_model", ",", "\n", "description_1", ":", "str", ",", "\n", "alpha", ",", "\n", "n_bootstrap_samples", ",", "\n", "random_state", ",", "\n", "err", ",", "\n", "description_2", ":", "str", ",", "\n", "synthetic_bandit_feedback", ":", "BanditFeedback", ",", "\n", ")", "->", "None", ":", "\n", "    ", "\"\"\"\n    Test the response of estimate_intervals using invalid data\n    \"\"\"", "\n", "ope_", "=", "OffPolicyEvaluation", "(", "\n", "bandit_feedback", "=", "synthetic_bandit_feedback", ",", "ope_estimators", "=", "[", "dm", "]", "\n", ")", "\n", "with", "pytest", ".", "raises", "(", "err", ",", "match", "=", "f\"{description_2}*\"", ")", ":", "\n", "        ", "_", "=", "ope_", ".", "estimate_intervals", "(", "\n", "action_dist", "=", "action_dist", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", "alpha", "=", "alpha", ",", "\n", "n_bootstrap_samples", "=", "n_bootstrap_samples", ",", "\n", "random_state", "=", "random_state", ",", "\n", ")", "\n", "# estimate_intervals function is called in summarize_off_policy_estimates", "\n", "", "with", "pytest", ".", "raises", "(", "err", ",", "match", "=", "f\"{description_2}*\"", ")", ":", "\n", "        ", "_", "=", "ope_", ".", "summarize_off_policy_estimates", "(", "\n", "action_dist", "=", "action_dist", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", "alpha", "=", "alpha", ",", "\n", "n_bootstrap_samples", "=", "n_bootstrap_samples", ",", "\n", "random_state", "=", "random_state", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_meta.test_meta_estimate_intervals_using_valid_input_data": [[553, 601], ["pytest.mark.parametrize", "pytest.mark.parametrize", "obp.ope.OffPolicyEvaluation", "obp.ope.OffPolicyEvaluation", "obp.ope.OffPolicyEvaluation.estimate_intervals", "obp.ope.OffPolicyEvaluation.estimate_intervals", "mock_confidence_interval.items"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta.OffPolicyEvaluation.estimate_intervals", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta.OffPolicyEvaluation.estimate_intervals"], ["", "", "@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"action_dist, estimated_rewards_by_reg_model, description_1\"", ",", "\n", "valid_input_of_create_estimator_inputs", ",", "\n", ")", "\n", "@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"alpha, n_bootstrap_samples, random_state, description_2\"", ",", "\n", "valid_input_of_estimate_intervals", ",", "\n", ")", "\n", "def", "test_meta_estimate_intervals_using_valid_input_data", "(", "\n", "action_dist", ",", "\n", "estimated_rewards_by_reg_model", ",", "\n", "description_1", ":", "str", ",", "\n", "alpha", ":", "float", ",", "\n", "n_bootstrap_samples", ":", "int", ",", "\n", "random_state", ":", "int", ",", "\n", "description_2", ":", "str", ",", "\n", "synthetic_bandit_feedback", ":", "BanditFeedback", ",", "\n", ")", "->", "None", ":", "\n", "    ", "\"\"\"\n    Test the response of estimate_intervals using valid data\n    \"\"\"", "\n", "# single ope estimator", "\n", "ope_", "=", "OffPolicyEvaluation", "(", "\n", "bandit_feedback", "=", "synthetic_bandit_feedback", ",", "ope_estimators", "=", "[", "dm", "]", "\n", ")", "\n", "assert", "ope_", ".", "estimate_intervals", "(", "\n", "action_dist", "=", "action_dist", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", "alpha", "=", "alpha", ",", "\n", "n_bootstrap_samples", "=", "n_bootstrap_samples", ",", "\n", "random_state", "=", "random_state", ",", "\n", ")", "==", "{", "\n", "\"dm\"", ":", "mock_confidence_interval", "\n", "}", ",", "\"OffPolicyEvaluation.estimate_intervals ([DirectMethod]) returns a wrong value\"", "\n", "# multiple ope estimators", "\n", "ope_", "=", "OffPolicyEvaluation", "(", "\n", "bandit_feedback", "=", "synthetic_bandit_feedback", ",", "ope_estimators", "=", "[", "dm", ",", "ipw", "]", "\n", ")", "\n", "assert", "ope_", ".", "estimate_intervals", "(", "\n", "action_dist", "=", "action_dist", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", "alpha", "=", "alpha", ",", "\n", "n_bootstrap_samples", "=", "n_bootstrap_samples", ",", "\n", "random_state", "=", "random_state", ",", "\n", ")", "==", "{", "\n", "\"dm\"", ":", "mock_confidence_interval", ",", "\n", "\"ipw\"", ":", "{", "k", ":", "v", "+", "ipw", ".", "eps", "for", "k", ",", "v", "in", "mock_confidence_interval", ".", "items", "(", ")", "}", ",", "\n", "}", ",", "\"OffPolicyEvaluation.estimate_intervals ([DirectMethod, IPW]) returns a wrong value\"", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_meta.test_meta_summarize_off_policy_estimates": [[603, 677], ["pytest.mark.parametrize", "pytest.mark.parametrize", "obp.ope.OffPolicyEvaluation", "obp.ope.OffPolicyEvaluation.summarize_off_policy_estimates", "copy.deepcopy", "numpy.zeros", "obp.ope.OffPolicyEvaluation", "obp.ope.OffPolicyEvaluation.summarize_off_policy_estimates", "pandas.DataFrame", "synthetic_bandit_feedback[].mean", "pandas.DataFrame", "pandas.testing.assert_frame_equal", "pandas.testing.assert_frame_equal", "pandas.DataFrame", "pandas.testing.assert_frame_equal", "mock_confidence_interval.items", "mock_confidence_interval.items"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta.OffPolicyEvaluation.summarize_off_policy_estimates", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta.OffPolicyEvaluation.summarize_off_policy_estimates"], ["", "@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"action_dist, estimated_rewards_by_reg_model, description_1\"", ",", "\n", "valid_input_of_create_estimator_inputs", ",", "\n", ")", "\n", "@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"alpha, n_bootstrap_samples, random_state, description_2\"", ",", "\n", "valid_input_of_estimate_intervals", ",", "\n", ")", "\n", "def", "test_meta_summarize_off_policy_estimates", "(", "\n", "action_dist", ",", "\n", "estimated_rewards_by_reg_model", ",", "\n", "description_1", ":", "str", ",", "\n", "alpha", ":", "float", ",", "\n", "n_bootstrap_samples", ":", "int", ",", "\n", "random_state", ":", "int", ",", "\n", "description_2", ":", "str", ",", "\n", "synthetic_bandit_feedback", ":", "BanditFeedback", ",", "\n", ")", "->", "None", ":", "\n", "    ", "\"\"\"\n    Test the response of summarize_off_policy_estimates using valid data\n    \"\"\"", "\n", "ope_", "=", "OffPolicyEvaluation", "(", "\n", "bandit_feedback", "=", "synthetic_bandit_feedback", ",", "ope_estimators", "=", "[", "ipw", ",", "ipw3", "]", "\n", ")", "\n", "value", ",", "interval", "=", "ope_", ".", "summarize_off_policy_estimates", "(", "\n", "action_dist", "=", "action_dist", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", "alpha", "=", "alpha", ",", "\n", "n_bootstrap_samples", "=", "n_bootstrap_samples", ",", "\n", "random_state", "=", "random_state", ",", "\n", ")", "\n", "expected_value", "=", "pd", ".", "DataFrame", "(", "\n", "{", "\n", "\"ipw\"", ":", "mock_policy_value", "+", "ipw", ".", "eps", ",", "\n", "\"ipw3\"", ":", "mock_policy_value", "+", "ipw3", ".", "eps", ",", "\n", "}", ",", "\n", "index", "=", "[", "\"estimated_policy_value\"", "]", ",", "\n", ")", ".", "T", "\n", "expected_value", "[", "\"relative_estimated_policy_value\"", "]", "=", "(", "\n", "expected_value", "[", "\"estimated_policy_value\"", "]", "\n", "/", "synthetic_bandit_feedback", "[", "\"reward\"", "]", ".", "mean", "(", ")", "\n", ")", "\n", "expected_interval", "=", "pd", ".", "DataFrame", "(", "\n", "{", "\n", "\"ipw\"", ":", "{", "k", ":", "v", "+", "ipw", ".", "eps", "for", "k", ",", "v", "in", "mock_confidence_interval", ".", "items", "(", ")", "}", ",", "\n", "\"ipw3\"", ":", "{", "k", ":", "v", "+", "ipw3", ".", "eps", "for", "k", ",", "v", "in", "mock_confidence_interval", ".", "items", "(", ")", "}", ",", "\n", "}", "\n", ")", ".", "T", "\n", "assert_frame_equal", "(", "value", ",", "expected_value", ")", ",", "\"Invalid summarization (policy value)\"", "\n", "assert_frame_equal", "(", "interval", ",", "expected_interval", ")", ",", "\"Invalid summarization (interval)\"", "\n", "# check relative estimated policy value when the average of bandit_feedback[\"reward\"] is zero", "\n", "zero_reward_bandit_feedback", "=", "deepcopy", "(", "synthetic_bandit_feedback", ")", "\n", "zero_reward_bandit_feedback", "[", "\"reward\"", "]", "=", "np", ".", "zeros", "(", "\n", "zero_reward_bandit_feedback", "[", "\"reward\"", "]", ".", "shape", "[", "0", "]", "\n", ")", "\n", "ope_", "=", "OffPolicyEvaluation", "(", "\n", "bandit_feedback", "=", "zero_reward_bandit_feedback", ",", "ope_estimators", "=", "[", "ipw", ",", "ipw3", "]", "\n", ")", "\n", "value", ",", "_", "=", "ope_", ".", "summarize_off_policy_estimates", "(", "\n", "action_dist", "=", "action_dist", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", "alpha", "=", "alpha", ",", "\n", "n_bootstrap_samples", "=", "n_bootstrap_samples", ",", "\n", "random_state", "=", "random_state", ",", "\n", ")", "\n", "expected_value", "=", "pd", ".", "DataFrame", "(", "\n", "{", "\n", "\"ipw\"", ":", "mock_policy_value", "+", "ipw", ".", "eps", ",", "\n", "\"ipw3\"", ":", "mock_policy_value", "+", "ipw3", ".", "eps", ",", "\n", "}", ",", "\n", "index", "=", "[", "\"estimated_policy_value\"", "]", ",", "\n", ")", ".", "T", "\n", "expected_value", "[", "\"relative_estimated_policy_value\"", "]", "=", "np", ".", "nan", "\n", "assert_frame_equal", "(", "value", ",", "expected_value", ")", ",", "\"Invalid summarization (policy value)\"", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_meta.test_meta_evaluate_performance_of_estimators_using_invalid_input_data": [[707, 745], ["pytest.mark.parametrize", "pytest.mark.parametrize", "obp.ope.OffPolicyEvaluation", "pytest.raises", "obp.ope.OffPolicyEvaluation.evaluate_performance_of_estimators", "pytest.raises", "obp.ope.OffPolicyEvaluation.summarize_estimators_comparison"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta.OffPolicyEvaluation.evaluate_performance_of_estimators", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta.OffPolicyEvaluation.summarize_estimators_comparison"], ["@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"action_dist, estimated_rewards_by_reg_model, description_1\"", ",", "\n", "valid_input_of_create_estimator_inputs", ",", "\n", ")", "\n", "@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"metric, ground_truth_policy_value, err, description_2\"", ",", "\n", "invalid_input_of_evaluation_performance_of_estimators", ",", "\n", ")", "\n", "def", "test_meta_evaluate_performance_of_estimators_using_invalid_input_data", "(", "\n", "action_dist", ",", "\n", "estimated_rewards_by_reg_model", ",", "\n", "description_1", ":", "str", ",", "\n", "metric", ",", "\n", "ground_truth_policy_value", ",", "\n", "err", ",", "\n", "description_2", ":", "str", ",", "\n", "synthetic_bandit_feedback", ":", "BanditFeedback", ",", "\n", ")", "->", "None", ":", "\n", "    ", "\"\"\"\n    Test the response of evaluate_performance_of_estimators using invalid data\n    \"\"\"", "\n", "ope_", "=", "OffPolicyEvaluation", "(", "\n", "bandit_feedback", "=", "synthetic_bandit_feedback", ",", "ope_estimators", "=", "[", "dm", "]", "\n", ")", "\n", "with", "pytest", ".", "raises", "(", "err", ",", "match", "=", "f\"{description_2}*\"", ")", ":", "\n", "        ", "_", "=", "ope_", ".", "evaluate_performance_of_estimators", "(", "\n", "ground_truth_policy_value", "=", "ground_truth_policy_value", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", "action_dist", "=", "action_dist", ",", "\n", "metric", "=", "metric", ",", "\n", ")", "\n", "# estimate_intervals function is called in summarize_off_policy_estimates", "\n", "", "with", "pytest", ".", "raises", "(", "err", ",", "match", "=", "f\"{description_2}*\"", ")", ":", "\n", "        ", "_", "=", "ope_", ".", "summarize_estimators_comparison", "(", "\n", "ground_truth_policy_value", "=", "ground_truth_policy_value", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", "action_dist", "=", "action_dist", ",", "\n", "metric", "=", "metric", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_meta.test_meta_evaluate_performance_of_estimators_using_valid_input_data": [[748, 808], ["pytest.mark.parametrize", "pytest.mark.parametrize", "obp.ope.OffPolicyEvaluation", "obp.ope.OffPolicyEvaluation.evaluate_performance_of_estimators", "ope_.evaluate_performance_of_estimators.items", "obp.ope.OffPolicyEvaluation.summarize_estimators_comparison", "pandas.testing.assert_frame_equal", "numpy.abs", "numpy.abs", "pandas.DataFrame"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta.OffPolicyEvaluation.evaluate_performance_of_estimators", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta.OffPolicyEvaluation.summarize_estimators_comparison"], ["", "", "@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"action_dist, estimated_rewards_by_reg_model, description_1\"", ",", "\n", "valid_input_of_create_estimator_inputs", ",", "\n", ")", "\n", "@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"metric, ground_truth_policy_value, description_2\"", ",", "\n", "valid_input_of_evaluation_performance_of_estimators", ",", "\n", ")", "\n", "def", "test_meta_evaluate_performance_of_estimators_using_valid_input_data", "(", "\n", "action_dist", ",", "\n", "estimated_rewards_by_reg_model", ",", "\n", "description_1", ":", "str", ",", "\n", "metric", ",", "\n", "ground_truth_policy_value", ",", "\n", "description_2", ":", "str", ",", "\n", "synthetic_bandit_feedback", ":", "BanditFeedback", ",", "\n", ")", "->", "None", ":", "\n", "    ", "\"\"\"\n    Test the response of evaluate_performance_of_estimators using valid data\n    \"\"\"", "\n", "if", "metric", "==", "\"relative-ee\"", ":", "\n", "# calculate relative-ee", "\n", "        ", "eval_metric_ope_dict", "=", "{", "\n", "\"ipw\"", ":", "np", ".", "abs", "(", "\n", "(", "mock_policy_value", "+", "ipw", ".", "eps", "-", "ground_truth_policy_value", ")", "\n", "/", "ground_truth_policy_value", "\n", ")", ",", "\n", "\"ipw3\"", ":", "np", ".", "abs", "(", "\n", "(", "mock_policy_value", "+", "ipw3", ".", "eps", "-", "ground_truth_policy_value", ")", "\n", "/", "ground_truth_policy_value", "\n", ")", ",", "\n", "}", "\n", "", "else", ":", "\n", "# calculate se", "\n", "        ", "eval_metric_ope_dict", "=", "{", "\n", "\"ipw\"", ":", "(", "mock_policy_value", "+", "ipw", ".", "eps", "-", "ground_truth_policy_value", ")", "**", "2", ",", "\n", "\"ipw3\"", ":", "(", "mock_policy_value", "+", "ipw3", ".", "eps", "-", "ground_truth_policy_value", ")", "**", "2", ",", "\n", "}", "\n", "# check performance estimators", "\n", "", "ope_", "=", "OffPolicyEvaluation", "(", "\n", "bandit_feedback", "=", "synthetic_bandit_feedback", ",", "ope_estimators", "=", "[", "ipw", ",", "ipw3", "]", "\n", ")", "\n", "performance", "=", "ope_", ".", "evaluate_performance_of_estimators", "(", "\n", "ground_truth_policy_value", "=", "ground_truth_policy_value", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", "action_dist", "=", "action_dist", ",", "\n", "metric", "=", "metric", ",", "\n", ")", "\n", "for", "k", ",", "v", "in", "performance", ".", "items", "(", ")", ":", "\n", "        ", "assert", "k", "in", "eval_metric_ope_dict", ",", "\"Invalid key of performance response\"", "\n", "assert", "v", "==", "eval_metric_ope_dict", "[", "k", "]", ",", "\"Invalid value of performance response\"", "\n", "", "performance_df", "=", "ope_", ".", "summarize_estimators_comparison", "(", "\n", "ground_truth_policy_value", "=", "ground_truth_policy_value", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", "action_dist", "=", "action_dist", ",", "\n", "metric", "=", "metric", ",", "\n", ")", "\n", "assert_frame_equal", "(", "\n", "performance_df", ",", "pd", ".", "DataFrame", "(", "eval_metric_ope_dict", ",", "index", "=", "[", "metric", "]", ")", ".", "T", "\n", ")", ",", "\"Invalid summarization (performance)\"", "\n", "", ""]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_ipw_estimators.test_ipw_init_using_invalid_inputs": [[40, 53], ["pytest.mark.parametrize", "pytest.raises", "obp.ope.InverseProbabilityWeighting"], "function", ["None"], ["@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"lambda_, use_estimated_pscore, err, description\"", ",", "\n", "invalid_input_of_ipw_init", ",", "\n", ")", "\n", "def", "test_ipw_init_using_invalid_inputs", "(", "\n", "lambda_", ",", "\n", "use_estimated_pscore", ",", "\n", "err", ",", "\n", "description", ",", "\n", ")", ":", "\n", "    ", "with", "pytest", ".", "raises", "(", "err", ",", "match", "=", "f\"{description}*\"", ")", ":", "\n", "        ", "_", "=", "InverseProbabilityWeighting", "(", "\n", "lambda_", "=", "lambda_", ",", "use_estimated_pscore", "=", "use_estimated_pscore", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_ipw_estimators.test_ipw_tuning_init_using_invalid_inputs": [[196, 216], ["pytest.mark.parametrize", "pytest.raises", "obp.ope.InverseProbabilityWeightingTuning"], "function", ["None"], ["@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"lambdas, tuning_method, use_bias_upper_bound, delta, use_estimated_pscore, err, description\"", ",", "\n", "invalid_input_of_ipw_tuning_init", ",", "\n", ")", "\n", "def", "test_ipw_tuning_init_using_invalid_inputs", "(", "\n", "lambdas", ",", "\n", "tuning_method", ",", "\n", "use_bias_upper_bound", ",", "\n", "delta", ",", "\n", "use_estimated_pscore", ",", "\n", "err", ",", "\n", "description", ",", "\n", ")", ":", "\n", "    ", "with", "pytest", ".", "raises", "(", "err", ",", "match", "=", "f\"{description}*\"", ")", ":", "\n", "        ", "_", "=", "InverseProbabilityWeightingTuning", "(", "\n", "use_bias_upper_bound", "=", "use_bias_upper_bound", ",", "\n", "delta", "=", "delta", ",", "\n", "lambdas", "=", "lambdas", ",", "\n", "tuning_method", "=", "tuning_method", ",", "\n", "use_estimated_pscore", "=", "use_estimated_pscore", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_ipw_estimators.test_ipw_using_invalid_input_data": [[411, 528], ["pytest.mark.parametrize", "obp.ope.InverseProbabilityWeighting", "obp.ope.SelfNormalizedInverseProbabilityWeighting", "obp.ope.SubGaussianInverseProbabilityWeighting", "obp.ope.InverseProbabilityWeightingTuning", "obp.ope.SubGaussianInverseProbabilityWeightingTuning", "pytest.raises", "obp.ope.InverseProbabilityWeighting.estimate_policy_value", "pytest.raises", "obp.ope.InverseProbabilityWeighting.estimate_interval", "pytest.raises", "obp.ope.SelfNormalizedInverseProbabilityWeighting.estimate_policy_value", "pytest.raises", "obp.ope.SelfNormalizedInverseProbabilityWeighting.estimate_interval", "pytest.raises", "obp.ope.InverseProbabilityWeightingTuning.estimate_policy_value", "pytest.raises", "obp.ope.InverseProbabilityWeightingTuning.estimate_interval", "pytest.raises", "obp.ope.SubGaussianInverseProbabilityWeighting.estimate_policy_value", "pytest.raises", "obp.ope.SubGaussianInverseProbabilityWeighting.estimate_interval", "pytest.raises", "obp.ope.SubGaussianInverseProbabilityWeightingTuning.estimate_policy_value", "pytest.raises", "obp.ope.SubGaussianInverseProbabilityWeightingTuning.estimate_interval"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_policy_value", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_interval", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_policy_value", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_interval", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_policy_value", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_interval", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_policy_value", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_interval", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_policy_value", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_interval"], ["@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"action_dist, action, reward, pscore, position, use_estimated_pscore, estimated_pscore, description\"", ",", "\n", "invalid_input_of_ipw", ",", "\n", ")", "\n", "def", "test_ipw_using_invalid_input_data", "(", "\n", "action_dist", ":", "np", ".", "ndarray", ",", "\n", "action", ":", "np", ".", "ndarray", ",", "\n", "reward", ":", "np", ".", "ndarray", ",", "\n", "pscore", ":", "np", ".", "ndarray", ",", "\n", "position", ":", "np", ".", "ndarray", ",", "\n", "use_estimated_pscore", ":", "bool", ",", "\n", "estimated_pscore", ":", "np", ".", "ndarray", ",", "\n", "description", ":", "str", ",", "\n", ")", "->", "None", ":", "\n", "# prepare ipw instances", "\n", "    ", "ipw", "=", "InverseProbabilityWeighting", "(", "use_estimated_pscore", "=", "use_estimated_pscore", ")", "\n", "snipw", "=", "SelfNormalizedInverseProbabilityWeighting", "(", "\n", "use_estimated_pscore", "=", "use_estimated_pscore", "\n", ")", "\n", "sgipw", "=", "SubGaussianInverseProbabilityWeighting", "(", "\n", "use_estimated_pscore", "=", "use_estimated_pscore", "\n", ")", "\n", "ipw_tuning", "=", "InverseProbabilityWeightingTuning", "(", "\n", "lambdas", "=", "[", "10", ",", "1000", "]", ",", "use_estimated_pscore", "=", "use_estimated_pscore", "\n", ")", "\n", "sgipw_tuning", "=", "SubGaussianInverseProbabilityWeightingTuning", "(", "\n", "lambdas", "=", "[", "0.01", ",", "0.1", "]", ",", "use_estimated_pscore", "=", "use_estimated_pscore", "\n", ")", "\n", "with", "pytest", ".", "raises", "(", "ValueError", ",", "match", "=", "f\"{description}*\"", ")", ":", "\n", "        ", "_", "=", "ipw", ".", "estimate_policy_value", "(", "\n", "action_dist", "=", "action_dist", ",", "\n", "action", "=", "action", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore", "=", "pscore", ",", "\n", "position", "=", "position", ",", "\n", "estimated_pscore", "=", "estimated_pscore", ",", "\n", ")", "\n", "", "with", "pytest", ".", "raises", "(", "ValueError", ",", "match", "=", "f\"{description}*\"", ")", ":", "\n", "        ", "_", "=", "ipw", ".", "estimate_interval", "(", "\n", "action_dist", "=", "action_dist", ",", "\n", "action", "=", "action", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore", "=", "pscore", ",", "\n", "position", "=", "position", ",", "\n", "estimated_pscore", "=", "estimated_pscore", ",", "\n", ")", "\n", "", "with", "pytest", ".", "raises", "(", "ValueError", ",", "match", "=", "f\"{description}*\"", ")", ":", "\n", "        ", "_", "=", "snipw", ".", "estimate_policy_value", "(", "\n", "action_dist", "=", "action_dist", ",", "\n", "action", "=", "action", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore", "=", "pscore", ",", "\n", "position", "=", "position", ",", "\n", "estimated_pscore", "=", "estimated_pscore", ",", "\n", ")", "\n", "", "with", "pytest", ".", "raises", "(", "ValueError", ",", "match", "=", "f\"{description}*\"", ")", ":", "\n", "        ", "_", "=", "snipw", ".", "estimate_interval", "(", "\n", "action_dist", "=", "action_dist", ",", "\n", "action", "=", "action", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore", "=", "pscore", ",", "\n", "position", "=", "position", ",", "\n", "estimated_pscore", "=", "estimated_pscore", ",", "\n", ")", "\n", "", "with", "pytest", ".", "raises", "(", "ValueError", ",", "match", "=", "f\"{description}*\"", ")", ":", "\n", "        ", "_", "=", "ipw_tuning", ".", "estimate_policy_value", "(", "\n", "action_dist", "=", "action_dist", ",", "\n", "action", "=", "action", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore", "=", "pscore", ",", "\n", "position", "=", "position", ",", "\n", "estimated_pscore", "=", "estimated_pscore", ",", "\n", ")", "\n", "", "with", "pytest", ".", "raises", "(", "ValueError", ",", "match", "=", "f\"{description}*\"", ")", ":", "\n", "        ", "_", "=", "ipw_tuning", ".", "estimate_interval", "(", "\n", "action_dist", "=", "action_dist", ",", "\n", "action", "=", "action", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore", "=", "pscore", ",", "\n", "position", "=", "position", ",", "\n", "estimated_pscore", "=", "estimated_pscore", ",", "\n", ")", "\n", "", "with", "pytest", ".", "raises", "(", "ValueError", ",", "match", "=", "f\"{description}*\"", ")", ":", "\n", "        ", "_", "=", "sgipw", ".", "estimate_policy_value", "(", "\n", "action_dist", "=", "action_dist", ",", "\n", "action", "=", "action", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore", "=", "pscore", ",", "\n", "position", "=", "position", ",", "\n", "estimated_pscore", "=", "estimated_pscore", ",", "\n", ")", "\n", "", "with", "pytest", ".", "raises", "(", "ValueError", ",", "match", "=", "f\"{description}*\"", ")", ":", "\n", "        ", "_", "=", "sgipw", ".", "estimate_interval", "(", "\n", "action_dist", "=", "action_dist", ",", "\n", "action", "=", "action", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore", "=", "pscore", ",", "\n", "position", "=", "position", ",", "\n", "estimated_pscore", "=", "estimated_pscore", ",", "\n", ")", "\n", "", "with", "pytest", ".", "raises", "(", "ValueError", ",", "match", "=", "f\"{description}*\"", ")", ":", "\n", "        ", "_", "=", "sgipw_tuning", ".", "estimate_policy_value", "(", "\n", "action_dist", "=", "action_dist", ",", "\n", "action", "=", "action", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore", "=", "pscore", ",", "\n", "position", "=", "position", ",", "\n", "estimated_pscore", "=", "estimated_pscore", ",", "\n", ")", "\n", "", "with", "pytest", ".", "raises", "(", "ValueError", ",", "match", "=", "f\"{description}*\"", ")", ":", "\n", "        ", "_", "=", "sgipw_tuning", ".", "estimate_interval", "(", "\n", "action_dist", "=", "action_dist", ",", "\n", "action", "=", "action", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore", "=", "pscore", ",", "\n", "position", "=", "position", ",", "\n", "estimated_pscore", "=", "estimated_pscore", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_ipw_estimators.test_ipw_using_random_evaluation_policy": [[531, 583], ["obp.ope.InverseProbabilityWeighting", "obp.ope.SelfNormalizedInverseProbabilityWeighting", "obp.ope.InverseProbabilityWeightingTuning", "estimator.estimate_policy_value", "isinstance", "estimator.estimate_policy_value", "isinstance", "synthetic_bandit_feedback.items", "pytest.raises", "estimator.estimate_policy_value", "re.escape"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_policy_value", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_policy_value", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_policy_value"], ["", "", "def", "test_ipw_using_random_evaluation_policy", "(", "\n", "synthetic_bandit_feedback", ":", "BanditFeedback", ",", "random_action_dist", ":", "np", ".", "ndarray", "\n", ")", "->", "None", ":", "\n", "    ", "\"\"\"\n    Test the format of ipw variants using synthetic bandit data and random evaluation policy\n    \"\"\"", "\n", "action_dist", "=", "random_action_dist", "\n", "# prepare input dict", "\n", "input_dict", "=", "{", "\n", "k", ":", "v", "\n", "for", "k", ",", "v", "in", "synthetic_bandit_feedback", ".", "items", "(", ")", "\n", "if", "k", "in", "[", "\"reward\"", ",", "\"action\"", ",", "\"pscore\"", ",", "\"position\"", "]", "\n", "}", "\n", "input_dict", "[", "\"action_dist\"", "]", "=", "action_dist", "\n", "# ipw estimators can be used without estimated_rewards_by_reg_model", "\n", "for", "estimator", "in", "[", "ipw", ",", "snipw", ",", "ipw_tuning_mse", ",", "ipw_tuning_slope", "]", ":", "\n", "        ", "estimated_policy_value", "=", "estimator", ".", "estimate_policy_value", "(", "**", "input_dict", ")", "\n", "assert", "isinstance", "(", "\n", "estimated_policy_value", ",", "float", "\n", ")", ",", "f\"invalid type response: {estimator}\"", "\n", "\n", "# ipw with estimated pscore", "\n", "", "ipw_estimated_pscore", "=", "InverseProbabilityWeighting", "(", "use_estimated_pscore", "=", "True", ")", "\n", "snipw_estimated_pscore", "=", "SelfNormalizedInverseProbabilityWeighting", "(", "\n", "use_estimated_pscore", "=", "True", "\n", ")", "\n", "ipw_tuning_estimated_pscore", "=", "InverseProbabilityWeightingTuning", "(", "\n", "lambdas", "=", "[", "10", ",", "1000", "]", ",", "use_estimated_pscore", "=", "True", "\n", ")", "\n", "input_dict", "[", "\"estimated_pscore\"", "]", "=", "input_dict", "[", "\"pscore\"", "]", "\n", "del", "input_dict", "[", "\"pscore\"", "]", "\n", "for", "estimator", "in", "[", "\n", "ipw_estimated_pscore", ",", "\n", "snipw_estimated_pscore", ",", "\n", "ipw_tuning_estimated_pscore", ",", "\n", "]", ":", "\n", "        ", "estimated_policy_value", "=", "estimator", ".", "estimate_policy_value", "(", "**", "input_dict", ")", "\n", "assert", "isinstance", "(", "\n", "estimated_policy_value", ",", "float", "\n", ")", ",", "f\"invalid type response: {estimator}\"", "\n", "\n", "# remove necessary keys", "\n", "", "del", "input_dict", "[", "\"reward\"", "]", "\n", "del", "input_dict", "[", "\"action\"", "]", "\n", "for", "estimator", "in", "[", "ipw", ",", "snipw", "]", ":", "\n", "        ", "with", "pytest", ".", "raises", "(", "\n", "TypeError", ",", "\n", "match", "=", "re", ".", "escape", "(", "\n", "\"estimate_policy_value() missing 2 required positional arguments: 'reward' and 'action'\"", "\n", ")", ",", "\n", ")", ":", "\n", "            ", "_", "=", "estimator", ".", "estimate_policy_value", "(", "**", "input_dict", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_ipw_estimators.test_boundedness_of_snipw_using_random_evaluation_policy": [[585, 618], ["obp.ope.SelfNormalizedInverseProbabilityWeighting", "obp.ope.SelfNormalizedInverseProbabilityWeighting.estimate_policy_value", "obp.ope.SelfNormalizedInverseProbabilityWeighting", "obp.ope.SelfNormalizedInverseProbabilityWeighting.estimate_policy_value", "synthetic_bandit_feedback.items"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_policy_value", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_policy_value"], ["", "", "", "def", "test_boundedness_of_snipw_using_random_evaluation_policy", "(", "\n", "synthetic_bandit_feedback", ":", "BanditFeedback", ",", "random_action_dist", ":", "np", ".", "ndarray", "\n", ")", "->", "None", ":", "\n", "    ", "\"\"\"\n    Test the boundedness of snipw estimators using synthetic bandit data and random evaluation policy\n    \"\"\"", "\n", "action_dist", "=", "random_action_dist", "\n", "# prepare snipw", "\n", "snipw", "=", "SelfNormalizedInverseProbabilityWeighting", "(", ")", "\n", "# prepare input dict", "\n", "input_dict", "=", "{", "\n", "k", ":", "v", "\n", "for", "k", ",", "v", "in", "synthetic_bandit_feedback", ".", "items", "(", ")", "\n", "if", "k", "in", "[", "\"reward\"", ",", "\"action\"", ",", "\"pscore\"", ",", "\"position\"", "]", "\n", "}", "\n", "input_dict", "[", "\"action_dist\"", "]", "=", "action_dist", "\n", "# make pscore too small (to check the boundedness of snipw)", "\n", "input_dict", "[", "\"pscore\"", "]", "=", "input_dict", "[", "\"pscore\"", "]", "**", "3", "\n", "estimated_policy_value", "=", "snipw", ".", "estimate_policy_value", "(", "**", "input_dict", ")", "\n", "assert", "(", "\n", "estimated_policy_value", "<=", "1", "\n", ")", ",", "f\"estimated policy value of snipw should be smaller than or equal to 1 (because of its 1-boundedness), but the value is: {estimated_policy_value}\"", "\n", "\n", "# ipw with estimated pscore", "\n", "snipw_estimated_pscore", "=", "SelfNormalizedInverseProbabilityWeighting", "(", "\n", "use_estimated_pscore", "=", "True", "\n", ")", "\n", "input_dict", "[", "\"estimated_pscore\"", "]", "=", "input_dict", "[", "\"pscore\"", "]", "\n", "del", "input_dict", "[", "\"pscore\"", "]", "\n", "estimated_policy_value", "=", "snipw_estimated_pscore", ".", "estimate_policy_value", "(", "**", "input_dict", ")", "\n", "assert", "(", "\n", "estimated_policy_value", "<=", "1", "\n", ")", ",", "f\"estimated policy value of snipw should be smaller than or equal to 1 (because of its 1-boundedness), but the value is: {estimated_policy_value}\"", "\n", "", ""]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_kernel_functions.test_kernel_functions": [[10, 44], ["numpy.isclose", "numpy.isclose", "numpy.isclose", "numpy.isclose", "numpy.isclose", "numpy.isclose", "numpy.isclose", "numpy.isclose", "scipy.integrate.quad", "scipy.integrate.quad", "scipy.integrate.quad", "scipy.integrate.quad", "scipy.integrate.quad", "scipy.integrate.quad", "scipy.integrate.quad", "scipy.integrate.quad", "scipy.integrate.quad", "scipy.integrate.quad", "scipy.integrate.quad", "scipy.integrate.quad", "obp.ope.triangular_kernel", "obp.ope.epanechnikov_kernel", "obp.ope.gaussian_kernel", "obp.ope.cosine_kernel", "obp.ope.triangular_kernel", "obp.ope.triangular_kernel", "obp.ope.epanechnikov_kernel", "obp.ope.epanechnikov_kernel", "obp.ope.gaussian_kernel", "obp.ope.gaussian_kernel", "obp.ope.cosine_kernel", "obp.ope.cosine_kernel"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_continuous.triangular_kernel", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_continuous.epanechnikov_kernel", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_continuous.gaussian_kernel", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_continuous.cosine_kernel", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_continuous.triangular_kernel", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_continuous.triangular_kernel", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_continuous.epanechnikov_kernel", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_continuous.epanechnikov_kernel", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_continuous.gaussian_kernel", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_continuous.gaussian_kernel", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_continuous.cosine_kernel", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_continuous.cosine_kernel"], ["def", "test_kernel_functions", "(", ")", ":", "\n", "# triangular", "\n", "    ", "assert", "np", ".", "isclose", "(", "\n", "integrate", ".", "quad", "(", "lambda", "x", ":", "triangular_kernel", "(", "x", ")", ",", "-", "np", ".", "inf", ",", "np", ".", "inf", ")", "[", "0", "]", ",", "1", "\n", ")", "\n", "assert", "np", ".", "isclose", "(", "\n", "integrate", ".", "quad", "(", "lambda", "x", ":", "x", "*", "triangular_kernel", "(", "x", ")", ",", "-", "np", ".", "inf", ",", "np", ".", "inf", ")", "[", "0", "]", ",", "0", "\n", ")", "\n", "assert", "integrate", ".", "quad", "(", "lambda", "x", ":", "triangular_kernel", "(", "x", ")", "**", "2", ",", "-", "np", ".", "inf", ",", "np", ".", "inf", ")", "[", "0", "]", ">", "0", "\n", "\n", "# epanechnikov", "\n", "assert", "np", ".", "isclose", "(", "\n", "integrate", ".", "quad", "(", "lambda", "x", ":", "epanechnikov_kernel", "(", "x", ")", ",", "-", "np", ".", "inf", ",", "np", ".", "inf", ")", "[", "0", "]", ",", "1", "\n", ")", "\n", "assert", "np", ".", "isclose", "(", "\n", "integrate", ".", "quad", "(", "lambda", "x", ":", "x", "*", "epanechnikov_kernel", "(", "x", ")", ",", "-", "np", ".", "inf", ",", "np", ".", "inf", ")", "[", "0", "]", ",", "0", "\n", ")", "\n", "assert", "integrate", ".", "quad", "(", "lambda", "x", ":", "epanechnikov_kernel", "(", "x", ")", "**", "2", ",", "-", "np", ".", "inf", ",", "np", ".", "inf", ")", "[", "0", "]", ">", "0", "\n", "\n", "# gaussian", "\n", "assert", "np", ".", "isclose", "(", "\n", "integrate", ".", "quad", "(", "lambda", "x", ":", "gaussian_kernel", "(", "x", ")", ",", "-", "np", ".", "inf", ",", "np", ".", "inf", ")", "[", "0", "]", ",", "1", "\n", ")", "\n", "assert", "np", ".", "isclose", "(", "\n", "integrate", ".", "quad", "(", "lambda", "x", ":", "x", "*", "gaussian_kernel", "(", "x", ")", ",", "-", "np", ".", "inf", ",", "np", ".", "inf", ")", "[", "0", "]", ",", "0", "\n", ")", "\n", "assert", "integrate", ".", "quad", "(", "lambda", "x", ":", "gaussian_kernel", "(", "x", ")", "**", "2", ",", "-", "np", ".", "inf", ",", "np", ".", "inf", ")", "[", "0", "]", ">", "0", "\n", "\n", "# cosine", "\n", "assert", "np", ".", "isclose", "(", "integrate", ".", "quad", "(", "lambda", "x", ":", "cosine_kernel", "(", "x", ")", ",", "-", "np", ".", "inf", ",", "np", ".", "inf", ")", "[", "0", "]", ",", "1", ")", "\n", "assert", "np", ".", "isclose", "(", "\n", "integrate", ".", "quad", "(", "lambda", "x", ":", "x", "*", "cosine_kernel", "(", "x", ")", ",", "-", "np", ".", "inf", ",", "np", ".", "inf", ")", "[", "0", "]", ",", "0", "\n", ")", "\n", "assert", "integrate", ".", "quad", "(", "lambda", "x", ":", "cosine_kernel", "(", "x", ")", "**", "2", ",", "-", "np", ".", "inf", ",", "np", ".", "inf", ")", "[", "0", "]", ">", "0", "\n", "", ""]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_all_estimators.test_estimation_of_all_estimators_using_invalid_input_data": [[163, 263], ["pytest.mark.parametrize", "getattr", "getattr", "getattr", "pytest.raises", "estimator.estimate_policy_value", "pytest.raises", "estimator.estimate_interval", "pytest.raises", "estimator_tuning.estimate_policy_value", "hasattr", "pytest.raises", "estimator_tuning.estimate_interval", "hasattr", "hasattr", "hasattr"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_policy_value", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_interval", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_policy_value", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_interval"], ["@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"action_dist, action, reward, pscore, position, estimated_rewards_by_reg_model, estimated_pscore, estimated_importance_weights, description\"", ",", "\n", "invalid_input_of_estimation", ",", "\n", ")", "\n", "def", "test_estimation_of_all_estimators_using_invalid_input_data", "(", "\n", "action_dist", ":", "np", ".", "ndarray", ",", "\n", "action", ":", "np", ".", "ndarray", ",", "\n", "reward", ":", "np", ".", "ndarray", ",", "\n", "pscore", ":", "np", ".", "ndarray", ",", "\n", "position", ":", "np", ".", "ndarray", ",", "\n", "estimated_rewards_by_reg_model", ":", "np", ".", "ndarray", ",", "\n", "estimated_pscore", ":", "np", ".", "ndarray", ",", "\n", "estimated_importance_weights", ":", "np", ".", "ndarray", ",", "\n", "description", ":", "str", ",", "\n", ")", "->", "None", ":", "\n", "    ", "all_estimators", "=", "ope", ".", "__all_estimators__", "\n", "estimators", "=", "[", "\n", "getattr", "(", "ope", ".", "estimators", ",", "estimator_name", ")", "(", ")", "for", "estimator_name", "in", "all_estimators", "\n", "]", "\n", "all_estimators_tuning", "=", "ope", ".", "__all_estimators_tuning__", "\n", "estimators_tuning", "=", "[", "\n", "getattr", "(", "ope", ".", "estimators_tuning", ",", "estimator_name", ")", "(", "\n", "lambdas", "=", "[", "1", ",", "100", ",", "10000", ",", "np", ".", "inf", "]", ",", "\n", "tuning_method", "=", "tuning_method", ",", "\n", ")", "\n", "for", "estimator_name", "in", "all_estimators_tuning", "\n", "for", "tuning_method", "in", "[", "\"slope\"", ",", "\"mse\"", "]", "\n", "]", "\n", "all_estimators_tuning_sg", "=", "ope", ".", "__all_estimators_tuning_sg__", "\n", "estimators_tuning_sg", "=", "[", "\n", "getattr", "(", "ope", ".", "estimators_tuning", ",", "estimator_name", ")", "(", "\n", "lambdas", "=", "[", "0.001", ",", "0.01", ",", "0.1", ",", "1.0", "]", ",", "\n", "tuning_method", "=", "tuning_method", ",", "\n", ")", "\n", "for", "estimator_name", "in", "all_estimators_tuning_sg", "\n", "for", "tuning_method", "in", "[", "\"slope\"", ",", "\"mse\"", "]", "\n", "]", "\n", "estimators_tuning", "=", "estimators_tuning", "+", "estimators_tuning_sg", "\n", "# estimate_intervals function raises ValueError of all estimators", "\n", "for", "estimator", "in", "estimators", ":", "\n", "        ", "with", "pytest", ".", "raises", "(", "ValueError", ",", "match", "=", "f\"{description}*\"", ")", ":", "\n", "            ", "est", "=", "estimator", ".", "estimate_policy_value", "(", "\n", "action_dist", "=", "action_dist", ",", "\n", "action", "=", "action", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore", "=", "pscore", ",", "\n", "position", "=", "position", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", "estimated_pscore", "=", "estimated_pscore", ",", "\n", "estimated_importance_weights", "=", "estimated_importance_weights", ",", "\n", ")", "\n", "assert", "est", "==", "0.0", ",", "f\"policy value must be 0, but {est}\"", "\n", "", "with", "pytest", ".", "raises", "(", "ValueError", ",", "match", "=", "f\"{description}*\"", ")", ":", "\n", "            ", "_", "=", "estimator", ".", "estimate_interval", "(", "\n", "action_dist", "=", "action_dist", ",", "\n", "action", "=", "action", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore", "=", "pscore", ",", "\n", "position", "=", "position", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", "estimated_pscore", "=", "estimated_pscore", ",", "\n", "estimated_importance_weights", "=", "estimated_importance_weights", ",", "\n", ")", "\n", "\n", "", "", "for", "estimator_tuning", "in", "estimators_tuning", ":", "\n", "        ", "with", "pytest", ".", "raises", "(", "ValueError", ",", "match", "=", "f\"{description}*\"", ")", ":", "\n", "            ", "est", "=", "estimator_tuning", ".", "estimate_policy_value", "(", "\n", "action_dist", "=", "action_dist", ",", "\n", "action", "=", "action", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore", "=", "pscore", ",", "\n", "position", "=", "position", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", "estimated_pscore", "=", "estimated_pscore", ",", "\n", ")", "\n", "assert", "est", "==", "0.0", ",", "f\"policy value must be 0, but {est}\"", "\n", "assert", "hasattr", "(", "\n", "estimator_tuning", ",", "\"best_hyperparam\"", "\n", ")", ",", "\"estimator_tuning should have `best_hyperparam` attr\"", "\n", "if", "estimator_tuning", ".", "tuning_method", "==", "\"mse\"", ":", "\n", "                ", "assert", "hasattr", "(", "\n", "estimator_tuning", ",", "\"estimated_mse_score_dict\"", "\n", ")", ",", "\"estimator_tuning should have `estimated_mse_score_dict` attr\"", "\n", "", "", "with", "pytest", ".", "raises", "(", "ValueError", ",", "match", "=", "f\"{description}*\"", ")", ":", "\n", "            ", "_", "=", "estimator_tuning", ".", "estimate_interval", "(", "\n", "action_dist", "=", "action_dist", ",", "\n", "action", "=", "action", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore", "=", "pscore", ",", "\n", "position", "=", "position", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", "estimated_pscore", "=", "estimated_pscore", ",", "\n", ")", "\n", "assert", "hasattr", "(", "\n", "estimator_tuning", ",", "\"best_hyperparam\"", "\n", ")", ",", "\"estimator_tuning should have `best_hyperparam` attr\"", "\n", "if", "estimator_tuning", ".", "tuning_method", "==", "\"mse\"", ":", "\n", "                ", "assert", "hasattr", "(", "\n", "estimator_tuning", ",", "\"estimated_mse_score_dict\"", "\n", ")", ",", "\"estimator_tuning should have `estimated_mse_score_dict` attr\"", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_all_estimators.test_estimation_of_all_estimators_using_valid_input_data": [[265, 343], ["pytest.mark.parametrize", "estimator.estimate_policy_value", "estimator.estimate_interval", "estimator_tuning.estimate_policy_value", "estimator_tuning.estimate_interval", "getattr", "getattr", "getattr"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_policy_value", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_interval", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_policy_value", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_interval"], ["", "", "", "", "@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"action_dist, action, reward, pscore, position, estimated_rewards_by_reg_model, estimated_pscore, estimated_importance_weights, description\"", ",", "\n", "valid_input_of_estimation", ",", "\n", ")", "\n", "def", "test_estimation_of_all_estimators_using_valid_input_data", "(", "\n", "action_dist", ":", "np", ".", "ndarray", ",", "\n", "action", ":", "np", ".", "ndarray", ",", "\n", "reward", ":", "np", ".", "ndarray", ",", "\n", "pscore", ":", "np", ".", "ndarray", ",", "\n", "position", ":", "np", ".", "ndarray", ",", "\n", "estimated_rewards_by_reg_model", ":", "np", ".", "ndarray", ",", "\n", "estimated_pscore", ":", "np", ".", "ndarray", ",", "\n", "estimated_importance_weights", ":", "np", ".", "ndarray", ",", "\n", "description", ":", "str", ",", "\n", ")", "->", "None", ":", "\n", "    ", "all_estimators", "=", "ope", ".", "__all_estimators__", "\n", "estimators", "=", "[", "\n", "getattr", "(", "ope", ".", "estimators", ",", "estimator_name", ")", "(", ")", "for", "estimator_name", "in", "all_estimators", "\n", "]", "\n", "all_estimators_tuning", "=", "ope", ".", "__all_estimators_tuning__", "\n", "estimators_tuning", "=", "[", "\n", "getattr", "(", "ope", ".", "estimators_tuning", ",", "estimator_name", ")", "(", "\n", "lambdas", "=", "[", "1", ",", "100", ",", "10000", ",", "np", ".", "inf", "]", ",", "\n", "tuning_method", "=", "tuning_method", ",", "\n", ")", "\n", "for", "estimator_name", "in", "all_estimators_tuning", "\n", "for", "tuning_method", "in", "[", "\"slope\"", ",", "\"mse\"", "]", "\n", "]", "\n", "all_estimators_tuning_sg", "=", "ope", ".", "__all_estimators_tuning_sg__", "\n", "estimators_tuning_sg", "=", "[", "\n", "getattr", "(", "ope", ".", "estimators_tuning", ",", "estimator_name", ")", "(", "\n", "lambdas", "=", "[", "0.001", ",", "0.01", ",", "0.1", ",", "1.0", "]", ",", "\n", "tuning_method", "=", "tuning_method", ",", "\n", ")", "\n", "for", "estimator_name", "in", "all_estimators_tuning_sg", "\n", "for", "tuning_method", "in", "[", "\"slope\"", ",", "\"mse\"", "]", "\n", "]", "\n", "estimators_tuning", "=", "estimators_tuning", "+", "estimators_tuning_sg", "\n", "# estimate_intervals function raises ValueError of all estimators", "\n", "for", "estimator", "in", "estimators", ":", "\n", "        ", "_", "=", "estimator", ".", "estimate_policy_value", "(", "\n", "action_dist", "=", "action_dist", ",", "\n", "action", "=", "action", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore", "=", "pscore", ",", "\n", "position", "=", "position", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", "estimated_pscore", "=", "estimated_pscore", ",", "\n", "estimated_importance_weights", "=", "estimated_importance_weights", ",", "\n", ")", "\n", "_", "=", "estimator", ".", "estimate_interval", "(", "\n", "action_dist", "=", "action_dist", ",", "\n", "action", "=", "action", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore", "=", "pscore", ",", "\n", "position", "=", "position", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", "estimated_pscore", "=", "estimated_pscore", ",", "\n", "estimated_importance_weights", "=", "estimated_importance_weights", ",", "\n", ")", "\n", "", "for", "estimator_tuning", "in", "estimators_tuning", ":", "\n", "        ", "_", "=", "estimator_tuning", ".", "estimate_policy_value", "(", "\n", "action_dist", "=", "action_dist", ",", "\n", "action", "=", "action", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore", "=", "pscore", ",", "\n", "position", "=", "position", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", "estimated_pscore", "=", "estimated_pscore", ",", "\n", ")", "\n", "_", "=", "estimator_tuning", ".", "estimate_interval", "(", "\n", "action_dist", "=", "action_dist", ",", "\n", "action", "=", "action", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore", "=", "pscore", ",", "\n", "position", "=", "position", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", "estimated_pscore", "=", "estimated_pscore", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_all_estimators.test_estimate_intervals_of_all_estimators_using_invalid_input_data": [[380, 454], ["pytest.mark.parametrize", "numpy.ones", "getattr", "getattr", "getattr", "pytest.raises", "estimator.estimate_interval", "pytest.raises", "estimator_tuning.estimate_interval"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_interval", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_interval"], ["@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"alpha, n_bootstrap_samples, random_state, err, description\"", ",", "\n", "invalid_input_of_estimate_intervals", ",", "\n", ")", "\n", "def", "test_estimate_intervals_of_all_estimators_using_invalid_input_data", "(", "\n", "alpha", ",", "\n", "n_bootstrap_samples", ",", "\n", "random_state", ",", "\n", "description", ":", "str", ",", "\n", "synthetic_bandit_feedback", ":", "BanditFeedback", ",", "\n", "err", ",", "\n", "random_action_dist", ":", "np", ".", "ndarray", ",", "\n", ")", "->", "None", ":", "\n", "    ", "\"\"\"\n    Test the response of estimate_intervals using invalid data\n    \"\"\"", "\n", "bandit_feedback", "=", "synthetic_bandit_feedback", "\n", "action_dist", "=", "random_action_dist", "\n", "expected_reward", "=", "synthetic_bandit_feedback", "[", "\"expected_reward\"", "]", "[", ":", ",", ":", ",", "np", ".", "newaxis", "]", "\n", "# test all estimators", "\n", "all_estimators", "=", "ope", ".", "__all_estimators__", "\n", "estimators", "=", "[", "\n", "getattr", "(", "ope", ".", "estimators", ",", "estimator_name", ")", "(", ")", "for", "estimator_name", "in", "all_estimators", "\n", "]", "\n", "all_estimators_tuning", "=", "ope", ".", "__all_estimators_tuning__", "\n", "estimators_tuning", "=", "[", "\n", "getattr", "(", "ope", ".", "estimators_tuning", ",", "estimator_name", ")", "(", "\n", "lambdas", "=", "[", "1", ",", "100", ",", "10000", ",", "np", ".", "inf", "]", ",", "\n", "tuning_method", "=", "tuning_method", ",", "\n", ")", "\n", "for", "estimator_name", "in", "all_estimators_tuning", "\n", "for", "tuning_method", "in", "[", "\"slope\"", ",", "\"mse\"", "]", "\n", "]", "\n", "all_estimators_tuning_sg", "=", "ope", ".", "__all_estimators_tuning_sg__", "\n", "estimators_tuning_sg", "=", "[", "\n", "getattr", "(", "ope", ".", "estimators_tuning", ",", "estimator_name", ")", "(", "\n", "lambdas", "=", "[", "0.001", ",", "0.01", ",", "0.1", ",", "1.0", "]", ",", "\n", "tuning_method", "=", "tuning_method", ",", "\n", ")", "\n", "for", "estimator_name", "in", "all_estimators_tuning_sg", "\n", "for", "tuning_method", "in", "[", "\"slope\"", ",", "\"mse\"", "]", "\n", "]", "\n", "estimators_tuning", "=", "estimators_tuning", "+", "estimators_tuning_sg", "\n", "estimated_pscore", "=", "None", "\n", "estimated_importance_weights", "=", "np", ".", "ones", "(", "bandit_feedback", "[", "\"action\"", "]", ".", "shape", "[", "0", "]", ")", "\n", "# estimate_intervals function raises ValueError of all estimators", "\n", "for", "estimator", "in", "estimators", ":", "\n", "        ", "with", "pytest", ".", "raises", "(", "err", ",", "match", "=", "f\"{description}*\"", ")", ":", "\n", "            ", "_", "=", "estimator", ".", "estimate_interval", "(", "\n", "reward", "=", "bandit_feedback", "[", "\"reward\"", "]", ",", "\n", "action", "=", "bandit_feedback", "[", "\"action\"", "]", ",", "\n", "position", "=", "bandit_feedback", "[", "\"position\"", "]", ",", "\n", "pscore", "=", "bandit_feedback", "[", "\"pscore\"", "]", ",", "\n", "action_dist", "=", "action_dist", ",", "\n", "estimated_rewards_by_reg_model", "=", "expected_reward", ",", "\n", "estimated_pscore", "=", "estimated_pscore", ",", "\n", "estimated_importance_weights", "=", "estimated_importance_weights", ",", "\n", "alpha", "=", "alpha", ",", "\n", "n_bootstrap_samples", "=", "n_bootstrap_samples", ",", "\n", "random_state", "=", "random_state", ",", "\n", ")", "\n", "", "", "for", "estimator_tuning", "in", "estimators_tuning", ":", "\n", "        ", "with", "pytest", ".", "raises", "(", "err", ",", "match", "=", "f\"{description}*\"", ")", ":", "\n", "            ", "_", "=", "estimator_tuning", ".", "estimate_interval", "(", "\n", "reward", "=", "bandit_feedback", "[", "\"reward\"", "]", ",", "\n", "action", "=", "bandit_feedback", "[", "\"action\"", "]", ",", "\n", "position", "=", "bandit_feedback", "[", "\"position\"", "]", ",", "\n", "pscore", "=", "bandit_feedback", "[", "\"pscore\"", "]", ",", "\n", "action_dist", "=", "action_dist", ",", "\n", "estimated_rewards_by_reg_model", "=", "expected_reward", ",", "\n", "estimated_pscore", "=", "estimated_pscore", ",", "\n", "alpha", "=", "alpha", ",", "\n", "n_bootstrap_samples", "=", "n_bootstrap_samples", ",", "\n", "random_state", "=", "random_state", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_all_estimators.test_estimate_intervals_of_all_estimators_using_valid_input_data": [[457, 528], ["pytest.mark.parametrize", "numpy.ones", "estimator.estimate_interval", "estimator_tuning.estimate_interval", "getattr", "getattr", "getattr"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_interval", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_interval"], ["", "", "", "@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"alpha, n_bootstrap_samples, random_state, description\"", ",", "\n", "valid_input_of_estimate_intervals", ",", "\n", ")", "\n", "def", "test_estimate_intervals_of_all_estimators_using_valid_input_data", "(", "\n", "alpha", ",", "\n", "n_bootstrap_samples", ",", "\n", "random_state", ",", "\n", "description", ":", "str", ",", "\n", "synthetic_bandit_feedback", ":", "BanditFeedback", ",", "\n", "random_action_dist", ":", "np", ".", "ndarray", ",", "\n", ")", "->", "None", ":", "\n", "    ", "\"\"\"\n    Test the response of estimate_intervals using valid data\n    \"\"\"", "\n", "bandit_feedback", "=", "synthetic_bandit_feedback", "\n", "action_dist", "=", "random_action_dist", "\n", "expected_reward", "=", "synthetic_bandit_feedback", "[", "\"expected_reward\"", "]", "[", ":", ",", ":", ",", "np", ".", "newaxis", "]", "\n", "# test all estimators", "\n", "all_estimators", "=", "ope", ".", "__all_estimators__", "\n", "estimators", "=", "[", "\n", "getattr", "(", "ope", ".", "estimators", ",", "estimator_name", ")", "(", ")", "for", "estimator_name", "in", "all_estimators", "\n", "]", "\n", "all_estimators_tuning", "=", "ope", ".", "__all_estimators_tuning__", "\n", "estimators_tuning", "=", "[", "\n", "getattr", "(", "ope", ".", "estimators_tuning", ",", "estimator_name", ")", "(", "\n", "lambdas", "=", "[", "1", ",", "100", ",", "10000", ",", "np", ".", "inf", "]", ",", "\n", "tuning_method", "=", "tuning_method", ",", "\n", ")", "\n", "for", "estimator_name", "in", "all_estimators_tuning", "\n", "for", "tuning_method", "in", "[", "\"slope\"", ",", "\"mse\"", "]", "\n", "]", "\n", "all_estimators_tuning_sg", "=", "ope", ".", "__all_estimators_tuning_sg__", "\n", "estimators_tuning_sg", "=", "[", "\n", "getattr", "(", "ope", ".", "estimators_tuning", ",", "estimator_name", ")", "(", "\n", "lambdas", "=", "[", "0.001", ",", "0.01", ",", "0.1", ",", "1.0", "]", ",", "\n", "tuning_method", "=", "tuning_method", ",", "\n", ")", "\n", "for", "estimator_name", "in", "all_estimators_tuning_sg", "\n", "for", "tuning_method", "in", "[", "\"slope\"", ",", "\"mse\"", "]", "\n", "]", "\n", "estimators_tuning", "=", "estimators_tuning", "+", "estimators_tuning_sg", "\n", "estimated_pscore", "=", "None", "\n", "estimated_importance_weights", "=", "np", ".", "ones", "(", "bandit_feedback", "[", "\"action\"", "]", ".", "shape", "[", "0", "]", ")", "\n", "# estimate_intervals function raises ValueError of all estimators", "\n", "for", "estimator", "in", "estimators", ":", "\n", "        ", "_", "=", "estimator", ".", "estimate_interval", "(", "\n", "reward", "=", "bandit_feedback", "[", "\"reward\"", "]", ",", "\n", "action", "=", "bandit_feedback", "[", "\"action\"", "]", ",", "\n", "position", "=", "bandit_feedback", "[", "\"position\"", "]", ",", "\n", "pscore", "=", "bandit_feedback", "[", "\"pscore\"", "]", ",", "\n", "action_dist", "=", "action_dist", ",", "\n", "estimated_rewards_by_reg_model", "=", "expected_reward", ",", "\n", "estimated_pscore", "=", "estimated_pscore", ",", "\n", "estimated_importance_weights", "=", "estimated_importance_weights", ",", "\n", "alpha", "=", "alpha", ",", "\n", "n_bootstrap_samples", "=", "n_bootstrap_samples", ",", "\n", "random_state", "=", "random_state", ",", "\n", ")", "\n", "", "for", "estimator_tuning", "in", "estimators_tuning", ":", "\n", "        ", "_", "=", "estimator_tuning", ".", "estimate_interval", "(", "\n", "reward", "=", "bandit_feedback", "[", "\"reward\"", "]", ",", "\n", "action", "=", "bandit_feedback", "[", "\"action\"", "]", ",", "\n", "position", "=", "bandit_feedback", "[", "\"position\"", "]", ",", "\n", "pscore", "=", "bandit_feedback", "[", "\"pscore\"", "]", ",", "\n", "action_dist", "=", "action_dist", ",", "\n", "estimated_rewards_by_reg_model", "=", "expected_reward", ",", "\n", "estimated_pscore", "=", "estimated_pscore", ",", "\n", "alpha", "=", "alpha", ",", "\n", "n_bootstrap_samples", "=", "n_bootstrap_samples", ",", "\n", "random_state", "=", "random_state", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_all_estimators.test_fixture": [[531, 546], ["numpy.testing.assert_array_almost_equal", "set", "synthetic_bandit_feedback.keys", "synthetic_bandit_feedback.keys"], "function", ["None"], ["", "", "def", "test_fixture", "(", "\n", "synthetic_bandit_feedback", ":", "BanditFeedback", ",", "\n", "expected_reward_0", ":", "np", ".", "ndarray", ",", "\n", "feedback_key_set", ":", "Set", "[", "str", "]", ",", "\n", "random_action_dist", ":", "np", ".", "ndarray", ",", "\n", ")", "->", "None", ":", "\n", "    ", "\"\"\"\n    Check the validity of the fixture data generated by conftest.py\n    \"\"\"", "\n", "np", ".", "testing", ".", "assert_array_almost_equal", "(", "\n", "expected_reward_0", ",", "synthetic_bandit_feedback", "[", "\"expected_reward\"", "]", "[", "0", "]", "\n", ")", "\n", "assert", "feedback_key_set", "==", "set", "(", "\n", "synthetic_bandit_feedback", ".", "keys", "(", ")", "\n", ")", ",", "f\"Key set of bandit feedback should be {feedback_key_set}, but {synthetic_bandit_feedback.keys()}\"", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_all_estimators.test_performance_of_ope_estimators_using_random_evaluation_policy": [[548, 618], ["numpy.average", "np.average.mean", "obp.ope.OffPolicyEvaluation", "obp.ope.OffPolicyEvaluation.estimate_policy_values", "print", "print", "getattr", "getattr", "getattr", "numpy.abs", "numpy.arange", "numpy.zeros"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta.OffPolicyEvaluation.estimate_policy_values"], ["", "def", "test_performance_of_ope_estimators_using_random_evaluation_policy", "(", "\n", "synthetic_bandit_feedback", ":", "BanditFeedback", ",", "random_action_dist", ":", "np", ".", "ndarray", "\n", ")", "->", "None", ":", "\n", "    ", "\"\"\"\n    Test the performance of ope estimators using synthetic bandit data and random evaluation policy\n    \"\"\"", "\n", "expected_reward", "=", "synthetic_bandit_feedback", "[", "\"expected_reward\"", "]", "[", ":", ",", ":", ",", "np", ".", "newaxis", "]", "\n", "action_dist", "=", "random_action_dist", "\n", "# compute ground truth policy value using expected reward", "\n", "q_pi_e", "=", "np", ".", "average", "(", "expected_reward", "[", ":", ",", ":", ",", "0", "]", ",", "weights", "=", "action_dist", "[", ":", ",", ":", ",", "0", "]", ",", "axis", "=", "1", ")", "\n", "# compute statistics of ground truth policy value", "\n", "gt_mean", "=", "q_pi_e", ".", "mean", "(", ")", "\n", "# test most of the estimators (ReplayMethod is not tested because it is out of scope)", "\n", "all_estimators", "=", "ope", ".", "__all_estimators__", "\n", "estimators_standard", "=", "[", "\n", "getattr", "(", "ope", ".", "estimators", ",", "estimator_name", ")", "(", ")", "\n", "for", "estimator_name", "in", "all_estimators", "\n", "if", "estimator_name", "not", "in", "[", "\"ReplayMethod\"", "]", "\n", "]", "\n", "all_estimators_tuning", "=", "ope", ".", "__all_estimators_tuning__", "\n", "estimators_tuning", "=", "[", "\n", "getattr", "(", "ope", ".", "estimators_tuning", ",", "estimator_name", ")", "(", "\n", "lambdas", "=", "[", "1", ",", "100", ",", "10000", ",", "np", ".", "inf", "]", ",", "\n", "tuning_method", "=", "tuning_method", ",", "\n", ")", "\n", "for", "estimator_name", "in", "all_estimators_tuning", "\n", "for", "tuning_method", "in", "[", "\"slope\"", ",", "\"mse\"", "]", "\n", "]", "\n", "all_estimators_tuning_sg", "=", "ope", ".", "__all_estimators_tuning_sg__", "\n", "estimators_tuning_sg", "=", "[", "\n", "getattr", "(", "ope", ".", "estimators_tuning", ",", "estimator_name", ")", "(", "\n", "lambdas", "=", "[", "0.001", ",", "0.01", ",", "0.1", ",", "1.0", "]", ",", "\n", "tuning_method", "=", "tuning_method", ",", "\n", ")", "\n", "for", "estimator_name", "in", "all_estimators_tuning_sg", "\n", "for", "tuning_method", "in", "[", "\"slope\"", ",", "\"mse\"", "]", "\n", "]", "\n", "estimators", "=", "estimators_standard", "+", "estimators_tuning", "+", "estimators_tuning_sg", "\n", "# skip estimation", "\n", "estimated_pscore", "=", "None", "\n", "estimated_importance_weights", "=", "(", "\n", "random_action_dist", "[", "\n", "np", ".", "arange", "(", "synthetic_bandit_feedback", "[", "\"action\"", "]", ".", "shape", "[", "0", "]", ")", ",", "\n", "synthetic_bandit_feedback", "[", "\"action\"", "]", ",", "\n", "np", ".", "zeros", "(", "\n", "synthetic_bandit_feedback", "[", "\"action\"", "]", ".", "shape", "[", "0", "]", ",", "dtype", "=", "int", "\n", ")", ",", "# position is None", "\n", "]", "\n", "/", "synthetic_bandit_feedback", "[", "\"pscore\"", "]", "\n", ")", "\n", "# conduct OPE", "\n", "ope_instance", "=", "OffPolicyEvaluation", "(", "\n", "bandit_feedback", "=", "synthetic_bandit_feedback", ",", "ope_estimators", "=", "estimators", "\n", ")", "\n", "estimated_policy_value", "=", "ope_instance", ".", "estimate_policy_values", "(", "\n", "action_dist", "=", "action_dist", ",", "\n", "estimated_rewards_by_reg_model", "=", "expected_reward", ",", "\n", "estimated_pscore", "=", "estimated_pscore", ",", "\n", "estimated_importance_weights", "=", "estimated_importance_weights", ",", "\n", ")", "\n", "# check the performance of OPE", "\n", "print", "(", "f\"gt_mean: {gt_mean}\"", ")", "\n", "for", "key", "in", "estimated_policy_value", ":", "\n", "        ", "print", "(", "\n", "f\"estimated_value: {estimated_policy_value[key]} ------ estimator: {key}, \"", "\n", ")", "\n", "# test the performance of each estimator", "\n", "assert", "(", "\n", "np", ".", "abs", "(", "gt_mean", "-", "estimated_policy_value", "[", "key", "]", ")", "/", "gt_mean", "<=", "0.1", "\n", ")", ",", "f\"OPE of {key} did not work well (relative absolute error is greater than 10%)\"", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_all_estimators.test_response_format_of_ope_estimators_using_random_evaluation_policy": [[620, 701], ["obp.ope.OffPolicyEvaluation", "obp.ope.OffPolicyEvaluation.estimate_policy_values", "obp.ope.OffPolicyEvaluation.estimate_intervals", "getattr", "getattr", "getattr", "set", "set", "estimated_intervals[].keys", "numpy.arange", "numpy.zeros"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta.OffPolicyEvaluation.estimate_policy_values", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta.OffPolicyEvaluation.estimate_intervals"], ["", "", "def", "test_response_format_of_ope_estimators_using_random_evaluation_policy", "(", "\n", "synthetic_bandit_feedback", ":", "BanditFeedback", ",", "random_action_dist", ":", "np", ".", "ndarray", "\n", ")", "->", "None", ":", "\n", "    ", "\"\"\"\n    Test the response format of ope estimators using synthetic bandit data and random evaluation policy\n    \"\"\"", "\n", "expected_reward", "=", "synthetic_bandit_feedback", "[", "\"expected_reward\"", "]", "[", ":", ",", ":", ",", "np", ".", "newaxis", "]", "\n", "action_dist", "=", "random_action_dist", "\n", "# test all estimators", "\n", "all_estimators", "=", "ope", ".", "__all_estimators__", "\n", "estimators_standard", "=", "[", "\n", "getattr", "(", "ope", ".", "estimators", ",", "estimator_name", ")", "(", ")", "for", "estimator_name", "in", "all_estimators", "\n", "]", "\n", "all_estimators_tuning", "=", "ope", ".", "__all_estimators_tuning__", "\n", "estimators_tuning", "=", "[", "\n", "getattr", "(", "ope", ".", "estimators_tuning", ",", "estimator_name", ")", "(", "\n", "lambdas", "=", "[", "1", ",", "100", ",", "10000", ",", "np", ".", "inf", "]", ",", "\n", "tuning_method", "=", "tuning_method", ",", "\n", ")", "\n", "for", "estimator_name", "in", "all_estimators_tuning", "\n", "for", "tuning_method", "in", "[", "\"slope\"", ",", "\"mse\"", "]", "\n", "]", "\n", "all_estimators_tuning_sg", "=", "ope", ".", "__all_estimators_tuning_sg__", "\n", "estimators_tuning_sg", "=", "[", "\n", "getattr", "(", "ope", ".", "estimators_tuning", ",", "estimator_name", ")", "(", "\n", "lambdas", "=", "[", "0.001", ",", "0.01", ",", "0.1", ",", "1.0", "]", ",", "\n", "tuning_method", "=", "tuning_method", ",", "\n", ")", "\n", "for", "estimator_name", "in", "all_estimators_tuning_sg", "\n", "for", "tuning_method", "in", "[", "\"slope\"", ",", "\"mse\"", "]", "\n", "]", "\n", "estimators", "=", "estimators_standard", "+", "estimators_tuning", "+", "estimators_tuning_sg", "\n", "# skip estimation", "\n", "estimated_pscore", "=", "None", "\n", "estimated_importance_weights", "=", "(", "\n", "random_action_dist", "[", "\n", "np", ".", "arange", "(", "synthetic_bandit_feedback", "[", "\"action\"", "]", ".", "shape", "[", "0", "]", ")", ",", "\n", "synthetic_bandit_feedback", "[", "\"action\"", "]", ",", "\n", "np", ".", "zeros", "(", "\n", "synthetic_bandit_feedback", "[", "\"action\"", "]", ".", "shape", "[", "0", "]", ",", "dtype", "=", "int", "\n", ")", ",", "# position is None", "\n", "]", "\n", "/", "synthetic_bandit_feedback", "[", "\"pscore\"", "]", "\n", ")", "\n", "# conduct OPE", "\n", "ope_instance", "=", "OffPolicyEvaluation", "(", "\n", "bandit_feedback", "=", "synthetic_bandit_feedback", ",", "ope_estimators", "=", "estimators", "\n", ")", "\n", "estimated_policy_value", "=", "ope_instance", ".", "estimate_policy_values", "(", "\n", "action_dist", "=", "action_dist", ",", "\n", "estimated_rewards_by_reg_model", "=", "expected_reward", ",", "\n", "estimated_pscore", "=", "estimated_pscore", ",", "\n", "estimated_importance_weights", "=", "estimated_importance_weights", ",", "\n", ")", "\n", "estimated_intervals", "=", "ope_instance", ".", "estimate_intervals", "(", "\n", "action_dist", "=", "action_dist", ",", "\n", "estimated_rewards_by_reg_model", "=", "expected_reward", ",", "\n", "estimated_pscore", "=", "estimated_pscore", ",", "\n", "estimated_importance_weights", "=", "estimated_importance_weights", ",", "\n", "random_state", "=", "12345", ",", "\n", ")", "\n", "# check the format of OPE", "\n", "for", "key", "in", "estimated_policy_value", ":", "\n", "# check the keys of the output dictionary of the estimate_intervals method", "\n", "        ", "assert", "set", "(", "estimated_intervals", "[", "key", "]", ".", "keys", "(", ")", ")", "==", "set", "(", "\n", "[", "\"mean\"", ",", "\"95.0% CI (lower)\"", ",", "\"95.0% CI (upper)\"", "]", "\n", ")", ",", "f\"Confidence interval of {key} has invalid keys\"", "\n", "# check the relationship between the means and the confidence bounds estimated by OPE estimators", "\n", "assert", "(", "\n", "estimated_intervals", "[", "key", "]", "[", "\"95.0% CI (lower)\"", "]", "<=", "estimated_policy_value", "[", "key", "]", "\n", ")", "and", "(", "\n", "estimated_intervals", "[", "key", "]", "[", "\"95.0% CI (upper)\"", "]", ">=", "estimated_policy_value", "[", "key", "]", "\n", ")", ",", "f\"Estimated policy value of {key} is not included in estimated intervals of that estimator\"", "\n", "assert", "(", "\n", "estimated_intervals", "[", "key", "]", "[", "\"mean\"", "]", "\n", ">=", "estimated_intervals", "[", "key", "]", "[", "\"95.0% CI (lower)\"", "]", "\n", ")", ",", "f\"Invalid confidence interval of {key}: lower bound > mean\"", "\n", "assert", "(", "\n", "estimated_intervals", "[", "key", "]", "[", "\"mean\"", "]", "\n", "<=", "estimated_intervals", "[", "key", "]", "[", "\"95.0% CI (upper)\"", "]", "\n", ")", ",", "f\"Invalid confidence interval of {key}: upper bound < mean\"", "\n", "", "", ""]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_meta_slate.SlateStandardIPSMock.estimate_policy_value": [[37, 54], ["None"], "methods", ["None"], ["def", "estimate_policy_value", "(", "\n", "self", ",", "\n", "slate_id", ":", "np", ".", "ndarray", ",", "\n", "reward", ":", "np", ".", "ndarray", ",", "\n", "position", ":", "np", ".", "ndarray", ",", "\n", "pscore", ":", "np", ".", "ndarray", ",", "\n", "evaluation_policy_pscore", ":", "np", ".", "ndarray", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "float", ":", "\n", "        ", "\"\"\"Estimate the policy value of evaluation policy.\n\n        Returns\n        ----------\n        mock_policy_value: float\n\n        \"\"\"", "\n", "return", "mock_policy_value", "+", "self", ".", "eps", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_meta_slate.SlateStandardIPSMock.estimate_interval": [[55, 81], ["obp.utils.check_confidence_interval_arguments", "mock_confidence_interval.items"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_confidence_interval_arguments"], ["", "def", "estimate_interval", "(", "\n", "self", ",", "\n", "slate_id", ":", "np", ".", "ndarray", ",", "\n", "reward", ":", "np", ".", "ndarray", ",", "\n", "position", ":", "np", ".", "ndarray", ",", "\n", "pscore", ":", "np", ".", "ndarray", ",", "\n", "evaluation_policy_pscore", ":", "np", ".", "ndarray", ",", "\n", "alpha", ":", "float", "=", "0.05", ",", "\n", "n_bootstrap_samples", ":", "int", "=", "10000", ",", "\n", "random_state", ":", "Optional", "[", "int", "]", "=", "None", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "Dict", "[", "str", ",", "float", "]", ":", "\n", "        ", "\"\"\"Estimate the confidence interval of the policy value using bootstrap.\n\n        Returns\n        ----------\n        mock_confidence_interval: Dict[str, float]\n            Dictionary storing the estimated mean and upper-lower confidence bounds.\n\n        \"\"\"", "\n", "check_confidence_interval_arguments", "(", "\n", "alpha", "=", "alpha", ",", "\n", "n_bootstrap_samples", "=", "n_bootstrap_samples", ",", "\n", "random_state", "=", "random_state", ",", "\n", ")", "\n", "return", "{", "k", ":", "v", "+", "self", ".", "eps", "for", "k", ",", "v", "in", "mock_confidence_interval", ".", "items", "(", ")", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_meta_slate.SlateIndependentIPSMock.estimate_policy_value": [[89, 106], ["None"], "methods", ["None"], ["def", "estimate_policy_value", "(", "\n", "self", ",", "\n", "slate_id", ":", "np", ".", "ndarray", ",", "\n", "reward", ":", "np", ".", "ndarray", ",", "\n", "position", ":", "np", ".", "ndarray", ",", "\n", "pscore_item_position", ":", "np", ".", "ndarray", ",", "\n", "evaluation_policy_pscore_item_position", ":", "np", ".", "ndarray", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "float", ":", "\n", "        ", "\"\"\"Estimate the policy value of evaluation policy.\n\n        Returns\n        ----------\n        mock_policy_value: float\n\n        \"\"\"", "\n", "return", "mock_policy_value", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_meta_slate.SlateIndependentIPSMock.estimate_interval": [[107, 133], ["obp.utils.check_confidence_interval_arguments", "mock_confidence_interval.items"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_confidence_interval_arguments"], ["", "def", "estimate_interval", "(", "\n", "self", ",", "\n", "slate_id", ":", "np", ".", "ndarray", ",", "\n", "reward", ":", "np", ".", "ndarray", ",", "\n", "position", ":", "np", ".", "ndarray", ",", "\n", "pscore_item_position", ":", "np", ".", "ndarray", ",", "\n", "evaluation_policy_pscore_item_position", ":", "np", ".", "ndarray", ",", "\n", "alpha", ":", "float", "=", "0.05", ",", "\n", "n_bootstrap_samples", ":", "int", "=", "10000", ",", "\n", "random_state", ":", "Optional", "[", "int", "]", "=", "None", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "Dict", "[", "str", ",", "float", "]", ":", "\n", "        ", "\"\"\"Estimate the confidence interval of the policy value using bootstrap.\n\n        Returns\n        ----------\n        mock_confidence_interval: Dict[str, float]\n            Dictionary storing the estimated mean and upper-lower confidence bounds.\n\n        \"\"\"", "\n", "check_confidence_interval_arguments", "(", "\n", "alpha", "=", "alpha", ",", "\n", "n_bootstrap_samples", "=", "n_bootstrap_samples", ",", "\n", "random_state", "=", "random_state", ",", "\n", ")", "\n", "return", "{", "k", ":", "v", "for", "k", ",", "v", "in", "mock_confidence_interval", ".", "items", "(", ")", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_meta_slate.SlateRewardInteractionIPSMock.estimate_policy_value": [[141, 158], ["None"], "methods", ["None"], ["def", "estimate_policy_value", "(", "\n", "self", ",", "\n", "slate_id", ":", "np", ".", "ndarray", ",", "\n", "reward", ":", "np", ".", "ndarray", ",", "\n", "position", ":", "np", ".", "ndarray", ",", "\n", "pscore_cascade", ":", "np", ".", "ndarray", ",", "\n", "evaluation_policy_pscore_cascade", ":", "np", ".", "ndarray", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "float", ":", "\n", "        ", "\"\"\"Estimate the policy value of evaluation policy.\n\n        Returns\n        ----------\n        mock_policy_value: float\n\n        \"\"\"", "\n", "return", "mock_policy_value", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_meta_slate.SlateRewardInteractionIPSMock.estimate_interval": [[159, 185], ["obp.utils.check_confidence_interval_arguments", "mock_confidence_interval.items"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_confidence_interval_arguments"], ["", "def", "estimate_interval", "(", "\n", "self", ",", "\n", "slate_id", ":", "np", ".", "ndarray", ",", "\n", "reward", ":", "np", ".", "ndarray", ",", "\n", "position", ":", "np", ".", "ndarray", ",", "\n", "pscore_cascade", ":", "np", ".", "ndarray", ",", "\n", "evaluation_policy_pscore_cascade", ":", "np", ".", "ndarray", ",", "\n", "alpha", ":", "float", "=", "0.05", ",", "\n", "n_bootstrap_samples", ":", "int", "=", "10000", ",", "\n", "random_state", ":", "Optional", "[", "int", "]", "=", "None", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "Dict", "[", "str", ",", "float", "]", ":", "\n", "        ", "\"\"\"Estimate the confidence interval of the policy value using bootstrap.\n\n        Returns\n        ----------\n        mock_confidence_interval: Dict[str, float]\n            Dictionary storing the estimated mean and upper-lower confidence bounds.\n\n        \"\"\"", "\n", "check_confidence_interval_arguments", "(", "\n", "alpha", "=", "alpha", ",", "\n", "n_bootstrap_samples", "=", "n_bootstrap_samples", ",", "\n", "random_state", "=", "random_state", ",", "\n", ")", "\n", "return", "{", "k", ":", "v", "for", "k", ",", "v", "in", "mock_confidence_interval", ".", "items", "(", ")", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_meta_slate.SlateCascadeDoublyRobustMock.estimate_policy_value": [[193, 213], ["None"], "methods", ["None"], ["def", "estimate_policy_value", "(", "\n", "self", ",", "\n", "slate_id", ":", "np", ".", "ndarray", ",", "\n", "action", ":", "np", ".", "ndarray", ",", "\n", "reward", ":", "np", ".", "ndarray", ",", "\n", "position", ":", "np", ".", "ndarray", ",", "\n", "pscore_cascade", ":", "np", ".", "ndarray", ",", "\n", "evaluation_policy_pscore_cascade", ":", "np", ".", "ndarray", ",", "\n", "evaluation_policy_action_dist", ":", "np", ".", "ndarray", ",", "\n", "q_hat", ":", "np", ".", "ndarray", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "float", ":", "\n", "        ", "\"\"\"Estimate the policy value of evaluation policy.\n\n        Returns\n        ----------\n        mock_policy_value: float\n\n        \"\"\"", "\n", "return", "mock_policy_value", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_meta_slate.SlateCascadeDoublyRobustMock.estimate_interval": [[214, 243], ["obp.utils.check_confidence_interval_arguments", "mock_confidence_interval.items"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_confidence_interval_arguments"], ["", "def", "estimate_interval", "(", "\n", "self", ",", "\n", "slate_id", ":", "np", ".", "ndarray", ",", "\n", "action", ":", "np", ".", "ndarray", ",", "\n", "reward", ":", "np", ".", "ndarray", ",", "\n", "position", ":", "np", ".", "ndarray", ",", "\n", "pscore_cascade", ":", "np", ".", "ndarray", ",", "\n", "evaluation_policy_pscore_cascade", ":", "np", ".", "ndarray", ",", "\n", "evaluation_policy_action_dist", ":", "np", ".", "ndarray", ",", "\n", "q_hat", ":", "np", ".", "ndarray", ",", "\n", "alpha", ":", "float", "=", "0.05", ",", "\n", "n_bootstrap_samples", ":", "int", "=", "10000", ",", "\n", "random_state", ":", "Optional", "[", "int", "]", "=", "None", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "Dict", "[", "str", ",", "float", "]", ":", "\n", "        ", "\"\"\"Estimate the confidence interval of the policy value using bootstrap.\n\n        Returns\n        ----------\n        mock_confidence_interval: Dict[str, float]\n            Dictionary storing the estimated mean and upper-lower confidence bounds.\n\n        \"\"\"", "\n", "check_confidence_interval_arguments", "(", "\n", "alpha", "=", "alpha", ",", "\n", "n_bootstrap_samples", "=", "n_bootstrap_samples", ",", "\n", "random_state", "=", "random_state", ",", "\n", ")", "\n", "return", "{", "k", ":", "v", "for", "k", ",", "v", "in", "mock_confidence_interval", ".", "items", "(", ")", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_meta_slate.test_meta_post_init": [[258, 294], ["obp.ope.SlateOffPolicyEvaluation", "obp.ope.SlateOffPolicyEvaluation", "range", "len", "itertools.combinations", "pytest.raises", "obp.ope.SlateOffPolicyEvaluation"], "function", ["None"], ["def", "test_meta_post_init", "(", "synthetic_slate_bandit_feedback", ":", "BanditFeedback", ")", "->", "None", ":", "\n", "    ", "\"\"\"\n    Test the __post_init__ function\n    \"\"\"", "\n", "# __post_init__ saves the latter estimator when the same estimator name is used", "\n", "ope_", "=", "SlateOffPolicyEvaluation", "(", "\n", "bandit_feedback", "=", "synthetic_slate_bandit_feedback", ",", "ope_estimators", "=", "[", "sips", ",", "sips2", "]", "\n", ")", "\n", "assert", "ope_", ".", "ope_estimators_", "==", "{", "\n", "\"sips\"", ":", "sips2", "\n", "}", ",", "\"__post_init__ returns a wrong value\"", "\n", "# __post_init__ can handle the same estimator if the estimator names are different", "\n", "ope_", "=", "SlateOffPolicyEvaluation", "(", "\n", "bandit_feedback", "=", "synthetic_slate_bandit_feedback", ",", "ope_estimators", "=", "[", "sips", ",", "sips3", "]", "\n", ")", "\n", "assert", "ope_", ".", "ope_estimators_", "==", "{", "\n", "\"sips\"", ":", "sips", ",", "\n", "\"sips3\"", ":", "sips3", ",", "\n", "}", ",", "\"__post_init__ returns a wrong value\"", "\n", "# __post__init__ raises RuntimeError when necessary_keys are not included in the bandit_feedback", "\n", "necessary_keys", "=", "[", "\n", "\"slate_id\"", ",", "\n", "\"context\"", ",", "\n", "\"action\"", ",", "\n", "\"reward\"", ",", "\n", "\"position\"", ",", "\n", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "necessary_keys", ")", ")", ":", "\n", "        ", "for", "deleted_keys", "in", "itertools", ".", "combinations", "(", "necessary_keys", ",", "i", "+", "1", ")", ":", "\n", "            ", "invalid_bandit_feedback_dict", "=", "{", "key", ":", "\"_\"", "for", "key", "in", "necessary_keys", "}", "\n", "# delete", "\n", "for", "k", "in", "deleted_keys", ":", "\n", "                ", "del", "invalid_bandit_feedback_dict", "[", "k", "]", "\n", "", "with", "pytest", ".", "raises", "(", "RuntimeError", ",", "match", "=", "r\"Missing key*\"", ")", ":", "\n", "                ", "_", "=", "SlateOffPolicyEvaluation", "(", "\n", "bandit_feedback", "=", "invalid_bandit_feedback_dict", ",", "ope_estimators", "=", "[", "sips", "]", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_meta_slate.test_meta_create_estimator_inputs_using_invalid_input_data": [[332, 389], ["pytest.mark.parametrize", "obp.ope.SlateOffPolicyEvaluation", "pytest.raises", "obp.ope.SlateOffPolicyEvaluation._create_estimator_inputs", "pytest.raises", "obp.ope.SlateOffPolicyEvaluation.estimate_policy_values", "pytest.raises", "obp.ope.SlateOffPolicyEvaluation.estimate_intervals", "pytest.raises", "obp.ope.SlateOffPolicyEvaluation.summarize_off_policy_estimates", "pytest.raises", "obp.ope.SlateOffPolicyEvaluation.evaluate_performance_of_estimators", "pytest.raises", "obp.ope.SlateOffPolicyEvaluation.summarize_estimators_comparison"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta.OffPolicyEvaluation._create_estimator_inputs", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta.OffPolicyEvaluation.estimate_policy_values", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta.OffPolicyEvaluation.estimate_intervals", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta.OffPolicyEvaluation.summarize_off_policy_estimates", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta.OffPolicyEvaluation.evaluate_performance_of_estimators", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta.OffPolicyEvaluation.summarize_estimators_comparison"], ["@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"evaluation_policy_pscore_cascade, evaluation_policy_action_dist, q_hat, description\"", ",", "\n", "invalid_input_of_create_estimator_inputs", ",", "\n", ")", "\n", "def", "test_meta_create_estimator_inputs_using_invalid_input_data", "(", "\n", "evaluation_policy_pscore_cascade", ",", "\n", "evaluation_policy_action_dist", ",", "\n", "q_hat", ",", "\n", "description", ":", "str", ",", "\n", "synthetic_slate_bandit_feedback", ":", "BanditFeedback", ",", "\n", ")", "->", "None", ":", "\n", "    ", "\"\"\"\n    Test the _create_estimator_inputs using valid data and a sips estimator\n    \"\"\"", "\n", "ope_", "=", "SlateOffPolicyEvaluation", "(", "\n", "bandit_feedback", "=", "synthetic_slate_bandit_feedback", ",", "\n", "ope_estimators", "=", "[", "cascade_dr", "]", ",", "\n", ")", "\n", "# raise ValueError when the shape of two arrays are different", "\n", "with", "pytest", ".", "raises", "(", "ValueError", ",", "match", "=", "f\"{description}*\"", ")", ":", "\n", "        ", "_", "=", "ope_", ".", "_create_estimator_inputs", "(", "\n", "evaluation_policy_pscore_cascade", "=", "evaluation_policy_pscore_cascade", ",", "\n", "evaluation_policy_action_dist", "=", "evaluation_policy_action_dist", ",", "\n", "q_hat", "=", "q_hat", ",", "\n", ")", "\n", "# _create_estimator_inputs function is called in the following functions", "\n", "", "with", "pytest", ".", "raises", "(", "ValueError", ",", "match", "=", "f\"{description}*\"", ")", ":", "\n", "        ", "_", "=", "ope_", ".", "estimate_policy_values", "(", "\n", "evaluation_policy_pscore_cascade", "=", "evaluation_policy_pscore_cascade", ",", "\n", "evaluation_policy_action_dist", "=", "evaluation_policy_action_dist", ",", "\n", "q_hat", "=", "q_hat", ",", "\n", ")", "\n", "", "with", "pytest", ".", "raises", "(", "ValueError", ",", "match", "=", "f\"{description}*\"", ")", ":", "\n", "        ", "_", "=", "ope_", ".", "estimate_intervals", "(", "\n", "evaluation_policy_pscore_cascade", "=", "evaluation_policy_pscore_cascade", ",", "\n", "evaluation_policy_action_dist", "=", "evaluation_policy_action_dist", ",", "\n", "q_hat", "=", "q_hat", ",", "\n", ")", "\n", "", "with", "pytest", ".", "raises", "(", "ValueError", ",", "match", "=", "f\"{description}*\"", ")", ":", "\n", "        ", "_", "=", "ope_", ".", "summarize_off_policy_estimates", "(", "\n", "evaluation_policy_pscore_cascade", "=", "evaluation_policy_pscore_cascade", ",", "\n", "evaluation_policy_action_dist", "=", "evaluation_policy_action_dist", ",", "\n", "q_hat", "=", "q_hat", ",", "\n", ")", "\n", "", "with", "pytest", ".", "raises", "(", "ValueError", ",", "match", "=", "f\"{description}*\"", ")", ":", "\n", "        ", "_", "=", "ope_", ".", "evaluate_performance_of_estimators", "(", "\n", "ground_truth_policy_value", "=", "0.1", ",", "\n", "evaluation_policy_pscore_cascade", "=", "evaluation_policy_pscore_cascade", ",", "\n", "evaluation_policy_action_dist", "=", "evaluation_policy_action_dist", ",", "\n", "q_hat", "=", "q_hat", ",", "\n", ")", "\n", "", "with", "pytest", ".", "raises", "(", "ValueError", ",", "match", "=", "f\"{description}*\"", ")", ":", "\n", "        ", "_", "=", "ope_", ".", "summarize_estimators_comparison", "(", "\n", "ground_truth_policy_value", "=", "0.1", ",", "\n", "evaluation_policy_pscore_cascade", "=", "evaluation_policy_pscore_cascade", ",", "\n", "evaluation_policy_action_dist", "=", "evaluation_policy_action_dist", ",", "\n", "q_hat", "=", "q_hat", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_meta_slate.test_meta_create_estimator_inputs_using_valid_input_data": [[392, 455], ["pytest.mark.parametrize", "obp.ope.SlateOffPolicyEvaluation", "obp.ope.SlateOffPolicyEvaluation._create_estimator_inputs", "obp.ope.SlateOffPolicyEvaluation.estimate_policy_values", "obp.ope.SlateOffPolicyEvaluation.estimate_intervals", "obp.ope.SlateOffPolicyEvaluation.summarize_off_policy_estimates", "obp.ope.SlateOffPolicyEvaluation.evaluate_performance_of_estimators", "obp.ope.SlateOffPolicyEvaluation.summarize_estimators_comparison", "obp.ope.SlateOffPolicyEvaluation", "obp.ope.SlateOffPolicyEvaluation._create_estimator_inputs", "set", "set", "ope_._create_estimator_inputs.keys"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta.OffPolicyEvaluation._create_estimator_inputs", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta.OffPolicyEvaluation.estimate_policy_values", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta.OffPolicyEvaluation.estimate_intervals", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta.OffPolicyEvaluation.summarize_off_policy_estimates", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta.OffPolicyEvaluation.evaluate_performance_of_estimators", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta.OffPolicyEvaluation.summarize_estimators_comparison", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta.OffPolicyEvaluation._create_estimator_inputs"], ["", "", "@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"evaluation_policy_pscore, evaluation_policy_pscore_item_position, evaluation_policy_pscore_cascade, evaluation_policy_action_dist, q_hat, description\"", ",", "\n", "valid_input_of_create_estimator_inputs", ",", "\n", ")", "\n", "def", "test_meta_create_estimator_inputs_using_valid_input_data", "(", "\n", "evaluation_policy_pscore", ",", "\n", "evaluation_policy_pscore_item_position", ",", "\n", "evaluation_policy_pscore_cascade", ",", "\n", "evaluation_policy_action_dist", ",", "\n", "q_hat", ",", "\n", "description", ":", "str", ",", "\n", "synthetic_slate_bandit_feedback", ":", "BanditFeedback", ",", "\n", ")", "->", "None", ":", "\n", "    ", "\"\"\"\n    Test the _create_estimator_inputs using invalid data\n    \"\"\"", "\n", "ope_", "=", "SlateOffPolicyEvaluation", "(", "\n", "bandit_feedback", "=", "synthetic_slate_bandit_feedback", ",", "ope_estimators", "=", "[", "sips", "]", "\n", ")", "\n", "estimator_inputs", "=", "ope_", ".", "_create_estimator_inputs", "(", "\n", "evaluation_policy_pscore", "=", "evaluation_policy_pscore", "\n", ")", "\n", "assert", "set", "(", "estimator_inputs", ".", "keys", "(", ")", ")", "==", "set", "(", "\n", "[", "\n", "\"slate_id\"", ",", "\n", "\"action\"", ",", "\n", "\"reward\"", ",", "\n", "\"pscore\"", ",", "\n", "\"pscore_item_position\"", ",", "\n", "\"pscore_cascade\"", ",", "\n", "\"position\"", ",", "\n", "\"evaluation_policy_pscore\"", ",", "\n", "\"evaluation_policy_pscore_item_position\"", ",", "\n", "\"evaluation_policy_pscore_cascade\"", ",", "\n", "\"slate_id\"", ",", "\n", "\"evaluation_policy_action_dist\"", ",", "\n", "\"q_hat\"", ",", "\n", "]", ",", "\n", ")", ",", "f\"Invalid response of _create_estimator_inputs (test case: {description})\"", "\n", "# _create_estimator_inputs function is called in the following functions", "\n", "_", "=", "ope_", ".", "estimate_policy_values", "(", "evaluation_policy_pscore", "=", "evaluation_policy_pscore", ")", "\n", "_", "=", "ope_", ".", "estimate_intervals", "(", "evaluation_policy_pscore", "=", "evaluation_policy_pscore", ")", "\n", "_", "=", "ope_", ".", "summarize_off_policy_estimates", "(", "\n", "evaluation_policy_pscore", "=", "evaluation_policy_pscore", "\n", ")", "\n", "_", "=", "ope_", ".", "evaluate_performance_of_estimators", "(", "\n", "ground_truth_policy_value", "=", "0.1", ",", "evaluation_policy_pscore", "=", "evaluation_policy_pscore", "\n", ")", "\n", "_", "=", "ope_", ".", "summarize_estimators_comparison", "(", "\n", "ground_truth_policy_value", "=", "0.1", ",", "evaluation_policy_pscore", "=", "evaluation_policy_pscore", "\n", ")", "\n", "# check if the valid values are returned when using cascade-dr", "\n", "ope_", "=", "SlateOffPolicyEvaluation", "(", "\n", "bandit_feedback", "=", "synthetic_slate_bandit_feedback", ",", "\n", "ope_estimators", "=", "[", "cascade_dr", "]", ",", "\n", ")", "\n", "estimator_inputs", "=", "ope_", ".", "_create_estimator_inputs", "(", "\n", "evaluation_policy_pscore_cascade", "=", "evaluation_policy_pscore_cascade", ",", "\n", "evaluation_policy_action_dist", "=", "evaluation_policy_action_dist", ",", "\n", "q_hat", "=", "q_hat", ",", "\n", ")", "\n", "assert", "estimator_inputs", "[", "\"evaluation_policy_action_dist\"", "]", "is", "not", "None", "\n", "assert", "estimator_inputs", "[", "\"q_hat\"", "]", "is", "not", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_meta_slate.test_meta_estimate_policy_values_using_valid_input_data": [[457, 499], ["pytest.mark.parametrize", "obp.ope.SlateOffPolicyEvaluation", "obp.ope.SlateOffPolicyEvaluation", "obp.ope.SlateOffPolicyEvaluation.estimate_policy_values", "obp.ope.SlateOffPolicyEvaluation.estimate_policy_values"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta.OffPolicyEvaluation.estimate_policy_values", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta.OffPolicyEvaluation.estimate_policy_values"], ["", "@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"evaluation_policy_pscore, evaluation_policy_pscore_item_position, evaluation_policy_pscore_cascade, evaluation_policy_action_dist, q_hat, description\"", ",", "\n", "valid_input_of_create_estimator_inputs", ",", "\n", ")", "\n", "def", "test_meta_estimate_policy_values_using_valid_input_data", "(", "\n", "evaluation_policy_pscore", ",", "\n", "evaluation_policy_pscore_item_position", ",", "\n", "evaluation_policy_pscore_cascade", ",", "\n", "evaluation_policy_action_dist", ",", "\n", "q_hat", ",", "\n", "description", ":", "str", ",", "\n", "synthetic_slate_bandit_feedback", ":", "BanditFeedback", ",", "\n", ")", "->", "None", ":", "\n", "    ", "\"\"\"\n    Test the response of estimate_policy_values using valid data\n    \"\"\"", "\n", "# single ope estimator (iips)", "\n", "ope_", "=", "SlateOffPolicyEvaluation", "(", "\n", "bandit_feedback", "=", "synthetic_slate_bandit_feedback", ",", "ope_estimators", "=", "[", "iips", "]", "\n", ")", "\n", "assert", "ope_", ".", "estimate_policy_values", "(", "\n", "evaluation_policy_pscore_item_position", "=", "evaluation_policy_pscore_item_position", "\n", ")", "==", "{", "\n", "\"iips\"", ":", "mock_policy_value", "\n", "}", ",", "\"SlateOffPolicyEvaluation.estimate_policy_values ([IIPS]) returns a wrong value\"", "\n", "# multiple ope estimators", "\n", "ope_", "=", "SlateOffPolicyEvaluation", "(", "\n", "bandit_feedback", "=", "synthetic_slate_bandit_feedback", ",", "\n", "ope_estimators", "=", "[", "iips", ",", "sips", ",", "rips", ",", "cascade_dr", "]", ",", "\n", ")", "\n", "assert", "ope_", ".", "estimate_policy_values", "(", "\n", "evaluation_policy_pscore", "=", "evaluation_policy_pscore", ",", "\n", "evaluation_policy_pscore_item_position", "=", "evaluation_policy_pscore_item_position", ",", "\n", "evaluation_policy_pscore_cascade", "=", "evaluation_policy_pscore_cascade", ",", "\n", "evaluation_policy_action_dist", "=", "evaluation_policy_action_dist", ",", "\n", "q_hat", "=", "q_hat", ",", "\n", ")", "==", "{", "\n", "\"iips\"", ":", "mock_policy_value", ",", "\n", "\"sips\"", ":", "mock_policy_value", "+", "sips", ".", "eps", ",", "\n", "\"rips\"", ":", "mock_policy_value", ",", "\n", "\"cascade-dr\"", ":", "mock_policy_value", ",", "\n", "}", ",", "\"SlateOffPolicyEvaluation.estimate_policy_values ([IIPS, SIPS, RIPS, Cascade-DR]) returns a wrong value\"", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_meta_slate.test_meta_estimate_policy_values_using_various_pscores": [[501, 564], ["pytest.mark.parametrize", "range", "copy.deepcopy", "obp.ope.SlateOffPolicyEvaluation", "obp.ope.SlateOffPolicyEvaluation.estimate_policy_values", "obp.ope.SlateOffPolicyEvaluation", "obp.ope.SlateOffPolicyEvaluation.estimate_policy_values", "len", "itertools.combinations", "copy.deepcopy", "pytest.raises", "obp.ope.SlateOffPolicyEvaluation", "obp.ope.SlateOffPolicyEvaluation.estimate_policy_values", "re.escape"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta.OffPolicyEvaluation.estimate_policy_values", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta.OffPolicyEvaluation.estimate_policy_values", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta.OffPolicyEvaluation.estimate_policy_values"], ["", "@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"evaluation_policy_pscore, evaluation_policy_pscore_item_position, evaluation_policy_pscore_cascade, evaluation_policy_action_dist, q_hat, description\"", ",", "\n", "valid_input_of_create_estimator_inputs", ",", "\n", ")", "\n", "def", "test_meta_estimate_policy_values_using_various_pscores", "(", "\n", "evaluation_policy_pscore", ",", "\n", "evaluation_policy_pscore_item_position", ",", "\n", "evaluation_policy_pscore_cascade", ",", "\n", "evaluation_policy_action_dist", ",", "\n", "q_hat", ",", "\n", "description", ":", "str", ",", "\n", "synthetic_slate_bandit_feedback", ":", "BanditFeedback", ",", "\n", ")", "->", "None", ":", "\n", "    ", "pscore_keys", "=", "[", "\n", "\"pscore\"", ",", "\n", "\"pscore_item_position\"", ",", "\n", "\"pscore_cascade\"", ",", "\n", "]", "\n", "# TypeError must be raised when required positional arguments are missing", "\n", "for", "i", "in", "range", "(", "len", "(", "pscore_keys", ")", ")", ":", "\n", "        ", "for", "deleted_keys", "in", "itertools", ".", "combinations", "(", "pscore_keys", ",", "i", "+", "1", ")", ":", "\n", "            ", "copied_feedback", "=", "deepcopy", "(", "synthetic_slate_bandit_feedback", ")", "\n", "# delete", "\n", "for", "k", "in", "deleted_keys", ":", "\n", "                ", "del", "copied_feedback", "[", "k", "]", "\n", "", "with", "pytest", ".", "raises", "(", "\n", "TypeError", ",", "\n", "match", "=", "re", ".", "escape", "(", "\"estimate_policy_value() missing\"", ")", ",", "\n", ")", ":", "\n", "                ", "ope_", "=", "SlateOffPolicyEvaluation", "(", "\n", "bandit_feedback", "=", "copied_feedback", ",", "\n", "ope_estimators", "=", "[", "sips", ",", "iips", ",", "rips", ",", "cascade_dr", "]", ",", "\n", ")", "\n", "_", "=", "ope_", ".", "estimate_policy_values", "(", "\n", "evaluation_policy_pscore", "=", "evaluation_policy_pscore", ",", "\n", "evaluation_policy_pscore_item_position", "=", "evaluation_policy_pscore_item_position", ",", "\n", "evaluation_policy_pscore_cascade", "=", "evaluation_policy_pscore_cascade", ",", "\n", "evaluation_policy_action_dist", "=", "evaluation_policy_action_dist", ",", "\n", "q_hat", "=", "q_hat", ",", "\n", ")", "\n", "\n", "# pscore_item_position and evaluation_policy_pscore_item_position are not necessary when iips is not evaluated", "\n", "", "", "", "copied_feedback", "=", "deepcopy", "(", "synthetic_slate_bandit_feedback", ")", "\n", "del", "copied_feedback", "[", "\"pscore_item_position\"", "]", "\n", "ope_", "=", "SlateOffPolicyEvaluation", "(", "\n", "bandit_feedback", "=", "copied_feedback", ",", "\n", "ope_estimators", "=", "[", "sips", ",", "rips", ",", "cascade_dr", "]", ",", "\n", ")", "\n", "_", "=", "ope_", ".", "estimate_policy_values", "(", "\n", "evaluation_policy_pscore", "=", "evaluation_policy_pscore", ",", "\n", "evaluation_policy_pscore_cascade", "=", "evaluation_policy_pscore_cascade", ",", "\n", "evaluation_policy_action_dist", "=", "evaluation_policy_action_dist", ",", "\n", "q_hat", "=", "q_hat", ",", "\n", ")", "\n", "# evaluation_policy_action_dist and q_hat are not necessary when cascade-dr is not used", "\n", "ope_", "=", "SlateOffPolicyEvaluation", "(", "\n", "bandit_feedback", "=", "synthetic_slate_bandit_feedback", ",", "\n", "ope_estimators", "=", "[", "sips", ",", "rips", ",", "iips", "]", ",", "\n", ")", "\n", "_", "=", "ope_", ".", "estimate_policy_values", "(", "\n", "evaluation_policy_pscore", "=", "evaluation_policy_pscore", ",", "\n", "evaluation_policy_pscore_cascade", "=", "evaluation_policy_pscore_cascade", ",", "\n", "evaluation_policy_pscore_item_position", "=", "evaluation_policy_pscore_item_position", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_meta_slate.test_meta_estimate_intervals_using_invalid_input_data": [[601, 643], ["pytest.mark.parametrize", "pytest.mark.parametrize", "obp.ope.SlateOffPolicyEvaluation", "pytest.raises", "obp.ope.SlateOffPolicyEvaluation.estimate_intervals", "pytest.raises", "obp.ope.SlateOffPolicyEvaluation.summarize_off_policy_estimates"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta.OffPolicyEvaluation.estimate_intervals", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta.OffPolicyEvaluation.summarize_off_policy_estimates"], ["@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"evaluation_policy_pscore, evaluation_policy_pscore_item_position, evaluation_policy_pscore_cascade, evaluation_policy_action_dist, q_hat, description_1\"", ",", "\n", "valid_input_of_create_estimator_inputs", ",", "\n", ")", "\n", "@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"alpha, n_bootstrap_samples, random_state, err, description_2\"", ",", "\n", "invalid_input_of_estimate_intervals", ",", "\n", ")", "\n", "def", "test_meta_estimate_intervals_using_invalid_input_data", "(", "\n", "evaluation_policy_pscore", ",", "\n", "evaluation_policy_pscore_item_position", ",", "\n", "evaluation_policy_pscore_cascade", ",", "\n", "evaluation_policy_action_dist", ",", "\n", "q_hat", ",", "\n", "description_1", ":", "str", ",", "\n", "alpha", ",", "\n", "n_bootstrap_samples", ",", "\n", "random_state", ",", "\n", "err", ",", "\n", "description_2", ":", "str", ",", "\n", "synthetic_slate_bandit_feedback", ":", "BanditFeedback", ",", "\n", ")", "->", "None", ":", "\n", "    ", "\"\"\"\n    Test the response of estimate_intervals using invalid data\n    \"\"\"", "\n", "ope_", "=", "SlateOffPolicyEvaluation", "(", "\n", "bandit_feedback", "=", "synthetic_slate_bandit_feedback", ",", "ope_estimators", "=", "[", "iips", "]", "\n", ")", "\n", "with", "pytest", ".", "raises", "(", "err", ",", "match", "=", "f\"{description_2}*\"", ")", ":", "\n", "        ", "_", "=", "ope_", ".", "estimate_intervals", "(", "\n", "evaluation_policy_pscore_item_position", "=", "evaluation_policy_pscore_item_position", ",", "\n", "alpha", "=", "alpha", ",", "\n", "n_bootstrap_samples", "=", "n_bootstrap_samples", ",", "\n", "random_state", "=", "random_state", ",", "\n", ")", "\n", "# estimate_intervals function is called in summarize_off_policy_estimates", "\n", "", "with", "pytest", ".", "raises", "(", "err", ",", "match", "=", "f\"{description_2}*\"", ")", ":", "\n", "        ", "_", "=", "ope_", ".", "summarize_off_policy_estimates", "(", "\n", "evaluation_policy_pscore_item_position", "=", "evaluation_policy_pscore_item_position", ",", "\n", "alpha", "=", "alpha", ",", "\n", "n_bootstrap_samples", "=", "n_bootstrap_samples", ",", "\n", "random_state", "=", "random_state", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_meta_slate.test_meta_estimate_intervals_using_valid_input_data": [[646, 702], ["pytest.mark.parametrize", "pytest.mark.parametrize", "obp.ope.SlateOffPolicyEvaluation", "obp.ope.SlateOffPolicyEvaluation", "obp.ope.SlateOffPolicyEvaluation.estimate_intervals", "obp.ope.SlateOffPolicyEvaluation.estimate_intervals", "mock_confidence_interval.items"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta.OffPolicyEvaluation.estimate_intervals", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta.OffPolicyEvaluation.estimate_intervals"], ["", "", "@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"evaluation_policy_pscore, evaluation_policy_pscore_item_position, evaluation_policy_pscore_cascade, evaluation_policy_action_dist, q_hat, description_1\"", ",", "\n", "valid_input_of_create_estimator_inputs", ",", "\n", ")", "\n", "@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"alpha, n_bootstrap_samples, random_state, description_2\"", ",", "\n", "valid_input_of_estimate_intervals", ",", "\n", ")", "\n", "def", "test_meta_estimate_intervals_using_valid_input_data", "(", "\n", "evaluation_policy_pscore", ",", "\n", "evaluation_policy_pscore_item_position", ",", "\n", "evaluation_policy_pscore_cascade", ",", "\n", "evaluation_policy_action_dist", ",", "\n", "q_hat", ",", "\n", "description_1", ":", "str", ",", "\n", "alpha", ":", "float", ",", "\n", "n_bootstrap_samples", ":", "int", ",", "\n", "random_state", ":", "int", ",", "\n", "description_2", ":", "str", ",", "\n", "synthetic_slate_bandit_feedback", ":", "BanditFeedback", ",", "\n", ")", "->", "None", ":", "\n", "    ", "\"\"\"\n    Test the response of estimate_intervals using valid data\n    \"\"\"", "\n", "# single ope estimator", "\n", "ope_", "=", "SlateOffPolicyEvaluation", "(", "\n", "bandit_feedback", "=", "synthetic_slate_bandit_feedback", ",", "ope_estimators", "=", "[", "iips", "]", "\n", ")", "\n", "assert", "ope_", ".", "estimate_intervals", "(", "\n", "evaluation_policy_pscore_item_position", "=", "evaluation_policy_pscore_item_position", ",", "\n", "alpha", "=", "alpha", ",", "\n", "n_bootstrap_samples", "=", "n_bootstrap_samples", ",", "\n", "random_state", "=", "random_state", ",", "\n", ")", "==", "{", "\n", "\"iips\"", ":", "mock_confidence_interval", "\n", "}", ",", "\"SlateOffPolicyEvaluation.estimate_intervals ([IIPS]) returns a wrong value\"", "\n", "# multiple ope estimators", "\n", "ope_", "=", "SlateOffPolicyEvaluation", "(", "\n", "bandit_feedback", "=", "synthetic_slate_bandit_feedback", ",", "\n", "ope_estimators", "=", "[", "iips", ",", "rips", ",", "cascade_dr", ",", "sips", "]", ",", "\n", ")", "\n", "assert", "ope_", ".", "estimate_intervals", "(", "\n", "evaluation_policy_pscore", "=", "evaluation_policy_pscore", ",", "\n", "evaluation_policy_pscore_cascade", "=", "evaluation_policy_pscore_cascade", ",", "\n", "evaluation_policy_pscore_item_position", "=", "evaluation_policy_pscore_item_position", ",", "\n", "evaluation_policy_action_dist", "=", "evaluation_policy_action_dist", ",", "\n", "q_hat", "=", "q_hat", ",", "\n", "alpha", "=", "alpha", ",", "\n", "n_bootstrap_samples", "=", "n_bootstrap_samples", ",", "\n", "random_state", "=", "random_state", ",", "\n", ")", "==", "{", "\n", "\"iips\"", ":", "mock_confidence_interval", ",", "\n", "\"rips\"", ":", "mock_confidence_interval", ",", "\n", "\"cascade-dr\"", ":", "mock_confidence_interval", ",", "\n", "\"sips\"", ":", "{", "k", ":", "v", "+", "sips", ".", "eps", "for", "k", ",", "v", "in", "mock_confidence_interval", ".", "items", "(", ")", "}", ",", "\n", "}", ",", "\"SlateOffPolicyEvaluation.estimate_intervals ([IIPS, SIPS]) returns a wrong value\"", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_meta_slate.test_meta_summarize_off_policy_estimates": [[704, 781], ["pytest.mark.parametrize", "pytest.mark.parametrize", "obp.ope.SlateOffPolicyEvaluation", "obp.ope.SlateOffPolicyEvaluation.summarize_off_policy_estimates", "copy.deepcopy", "numpy.zeros", "obp.ope.SlateOffPolicyEvaluation", "obp.ope.SlateOffPolicyEvaluation.summarize_off_policy_estimates", "pandas.DataFrame", "pandas.DataFrame", "pandas.testing.assert_frame_equal", "pandas.testing.assert_frame_equal", "pandas.DataFrame", "pandas.testing.assert_frame_equal", "synthetic_slate_bandit_feedback[].sum", "numpy.unique", "mock_confidence_interval.items", "mock_confidence_interval.items"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta.OffPolicyEvaluation.summarize_off_policy_estimates", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta.OffPolicyEvaluation.summarize_off_policy_estimates"], ["", "@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"evaluation_policy_pscore, evaluation_policy_pscore_item_position, evaluation_policy_pscore_cascade, evaluation_policy_action_dist, q_hat, description_1\"", ",", "\n", "valid_input_of_create_estimator_inputs", ",", "\n", ")", "\n", "@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"alpha, n_bootstrap_samples, random_state, description_2\"", ",", "\n", "valid_input_of_estimate_intervals", ",", "\n", ")", "\n", "def", "test_meta_summarize_off_policy_estimates", "(", "\n", "evaluation_policy_pscore", ",", "\n", "evaluation_policy_pscore_item_position", ",", "\n", "evaluation_policy_pscore_cascade", ",", "\n", "evaluation_policy_action_dist", ",", "\n", "q_hat", ",", "\n", "description_1", ":", "str", ",", "\n", "alpha", ":", "float", ",", "\n", "n_bootstrap_samples", ":", "int", ",", "\n", "random_state", ":", "int", ",", "\n", "description_2", ":", "str", ",", "\n", "synthetic_slate_bandit_feedback", ":", "BanditFeedback", ",", "\n", ")", "->", "None", ":", "\n", "    ", "\"\"\"\n    Test the response of summarize_off_policy_estimates using valid data\n    \"\"\"", "\n", "ope_", "=", "SlateOffPolicyEvaluation", "(", "\n", "bandit_feedback", "=", "synthetic_slate_bandit_feedback", ",", "ope_estimators", "=", "[", "sips", ",", "sips3", "]", "\n", ")", "\n", "value", ",", "interval", "=", "ope_", ".", "summarize_off_policy_estimates", "(", "\n", "evaluation_policy_pscore", "=", "evaluation_policy_pscore", ",", "\n", "alpha", "=", "alpha", ",", "\n", "n_bootstrap_samples", "=", "n_bootstrap_samples", ",", "\n", "random_state", "=", "random_state", ",", "\n", ")", "\n", "expected_value", "=", "pd", ".", "DataFrame", "(", "\n", "{", "\n", "\"sips\"", ":", "mock_policy_value", "+", "sips", ".", "eps", ",", "\n", "\"sips3\"", ":", "mock_policy_value", "+", "sips3", ".", "eps", ",", "\n", "}", ",", "\n", "index", "=", "[", "\"estimated_policy_value\"", "]", ",", "\n", ")", ".", "T", "\n", "expected_value", "[", "\"relative_estimated_policy_value\"", "]", "=", "expected_value", "[", "\n", "\"estimated_policy_value\"", "\n", "]", "/", "(", "\n", "synthetic_slate_bandit_feedback", "[", "\"reward\"", "]", ".", "sum", "(", ")", "\n", "/", "np", ".", "unique", "(", "synthetic_slate_bandit_feedback", "[", "\"slate_id\"", "]", ")", ".", "shape", "[", "0", "]", "\n", ")", "\n", "expected_interval", "=", "pd", ".", "DataFrame", "(", "\n", "{", "\n", "\"sips\"", ":", "{", "k", ":", "v", "+", "sips", ".", "eps", "for", "k", ",", "v", "in", "mock_confidence_interval", ".", "items", "(", ")", "}", ",", "\n", "\"sips3\"", ":", "{", "k", ":", "v", "+", "sips3", ".", "eps", "for", "k", ",", "v", "in", "mock_confidence_interval", ".", "items", "(", ")", "}", ",", "\n", "}", "\n", ")", ".", "T", "\n", "assert_frame_equal", "(", "value", ",", "expected_value", ")", ",", "\"Invalid summarization (policy value)\"", "\n", "assert_frame_equal", "(", "interval", ",", "expected_interval", ")", ",", "\"Invalid summarization (interval)\"", "\n", "# check relative estimated policy value when the average of bandit_feedback[\"reward\"] is zero", "\n", "zero_reward_bandit_feedback", "=", "deepcopy", "(", "synthetic_slate_bandit_feedback", ")", "\n", "zero_reward_bandit_feedback", "[", "\"reward\"", "]", "=", "np", ".", "zeros", "(", "\n", "zero_reward_bandit_feedback", "[", "\"reward\"", "]", ".", "shape", "[", "0", "]", "\n", ")", "\n", "ope_", "=", "SlateOffPolicyEvaluation", "(", "\n", "bandit_feedback", "=", "zero_reward_bandit_feedback", ",", "ope_estimators", "=", "[", "sips", ",", "sips3", "]", "\n", ")", "\n", "value", ",", "_", "=", "ope_", ".", "summarize_off_policy_estimates", "(", "\n", "evaluation_policy_pscore", "=", "evaluation_policy_pscore", ",", "\n", "alpha", "=", "alpha", ",", "\n", "n_bootstrap_samples", "=", "n_bootstrap_samples", ",", "\n", "random_state", "=", "random_state", ",", "\n", ")", "\n", "expected_value", "=", "pd", ".", "DataFrame", "(", "\n", "{", "\n", "\"sips\"", ":", "mock_policy_value", "+", "sips", ".", "eps", ",", "\n", "\"sips3\"", ":", "mock_policy_value", "+", "sips3", ".", "eps", ",", "\n", "}", ",", "\n", "index", "=", "[", "\"estimated_policy_value\"", "]", ",", "\n", ")", ".", "T", "\n", "expected_value", "[", "\"relative_estimated_policy_value\"", "]", "=", "np", ".", "nan", "\n", "assert_frame_equal", "(", "value", ",", "expected_value", ")", ",", "\"Invalid summarization (policy value)\"", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_meta_slate.test_meta_evaluate_performance_of_estimators_using_invalid_input_data": [[811, 850], ["pytest.mark.parametrize", "pytest.mark.parametrize", "obp.ope.SlateOffPolicyEvaluation", "pytest.raises", "obp.ope.SlateOffPolicyEvaluation.evaluate_performance_of_estimators", "pytest.raises", "obp.ope.SlateOffPolicyEvaluation.summarize_estimators_comparison"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta.OffPolicyEvaluation.evaluate_performance_of_estimators", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta.OffPolicyEvaluation.summarize_estimators_comparison"], ["@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"evaluation_policy_pscore, evaluation_policy_pscore_item_position, evaluation_policy_pscore_cascade, evaluation_policy_action_dist, q_hat, description_1\"", ",", "\n", "valid_input_of_create_estimator_inputs", ",", "\n", ")", "\n", "@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"metric, ground_truth_policy_value, err, description_2\"", ",", "\n", "invalid_input_of_evaluation_performance_of_estimators", ",", "\n", ")", "\n", "def", "test_meta_evaluate_performance_of_estimators_using_invalid_input_data", "(", "\n", "evaluation_policy_pscore", ",", "\n", "evaluation_policy_pscore_item_position", ",", "\n", "evaluation_policy_pscore_cascade", ",", "\n", "evaluation_policy_action_dist", ",", "\n", "q_hat", ",", "\n", "description_1", ":", "str", ",", "\n", "metric", ",", "\n", "ground_truth_policy_value", ",", "\n", "err", ",", "\n", "description_2", ":", "str", ",", "\n", "synthetic_slate_bandit_feedback", ":", "BanditFeedback", ",", "\n", ")", "->", "None", ":", "\n", "    ", "\"\"\"\n    Test the response of evaluate_performance_of_estimators using invalid data\n    \"\"\"", "\n", "ope_", "=", "SlateOffPolicyEvaluation", "(", "\n", "bandit_feedback", "=", "synthetic_slate_bandit_feedback", ",", "ope_estimators", "=", "[", "iips", "]", "\n", ")", "\n", "with", "pytest", ".", "raises", "(", "err", ",", "match", "=", "f\"{description_2}*\"", ")", ":", "\n", "        ", "_", "=", "ope_", ".", "evaluate_performance_of_estimators", "(", "\n", "ground_truth_policy_value", "=", "ground_truth_policy_value", ",", "\n", "evaluation_policy_pscore_item_position", "=", "evaluation_policy_pscore_item_position", ",", "\n", "metric", "=", "metric", ",", "\n", ")", "\n", "# estimate_intervals function is called in summarize_off_policy_estimates", "\n", "", "with", "pytest", ".", "raises", "(", "err", ",", "match", "=", "f\"{description_2}*\"", ")", ":", "\n", "        ", "_", "=", "ope_", ".", "summarize_estimators_comparison", "(", "\n", "ground_truth_policy_value", "=", "ground_truth_policy_value", ",", "\n", "evaluation_policy_pscore_item_position", "=", "evaluation_policy_pscore_item_position", ",", "\n", "metric", "=", "metric", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_meta_slate.test_meta_evaluate_performance_of_estimators_using_valid_input_data": [[853, 914], ["pytest.mark.parametrize", "pytest.mark.parametrize", "obp.ope.SlateOffPolicyEvaluation", "obp.ope.SlateOffPolicyEvaluation.evaluate_performance_of_estimators", "ope_.evaluate_performance_of_estimators.items", "obp.ope.SlateOffPolicyEvaluation.summarize_estimators_comparison", "pandas.testing.assert_frame_equal", "numpy.abs", "numpy.abs", "pandas.DataFrame"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta.OffPolicyEvaluation.evaluate_performance_of_estimators", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta.OffPolicyEvaluation.summarize_estimators_comparison"], ["", "", "@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"evaluation_policy_pscore, evaluation_policy_pscore_item_position, evaluation_policy_pscore_cascade, evaluation_policy_action_dist, q_hat, description_1\"", ",", "\n", "valid_input_of_create_estimator_inputs", ",", "\n", ")", "\n", "@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"metric, ground_truth_policy_value, description_2\"", ",", "\n", "valid_input_of_evaluation_performance_of_estimators", ",", "\n", ")", "\n", "def", "test_meta_evaluate_performance_of_estimators_using_valid_input_data", "(", "\n", "evaluation_policy_pscore", ",", "\n", "evaluation_policy_pscore_item_position", ",", "\n", "evaluation_policy_pscore_cascade", ",", "\n", "evaluation_policy_action_dist", ",", "\n", "q_hat", ",", "\n", "description_1", ":", "str", ",", "\n", "metric", ",", "\n", "ground_truth_policy_value", ",", "\n", "description_2", ":", "str", ",", "\n", "synthetic_slate_bandit_feedback", ":", "BanditFeedback", ",", "\n", ")", "->", "None", ":", "\n", "    ", "\"\"\"\n    Test the response of evaluate_performance_of_estimators using valid data\n    \"\"\"", "\n", "if", "metric", "==", "\"relative-ee\"", ":", "\n", "# calculate relative-ee", "\n", "        ", "eval_metric_ope_dict", "=", "{", "\n", "\"sips\"", ":", "np", ".", "abs", "(", "\n", "(", "mock_policy_value", "+", "sips", ".", "eps", "-", "ground_truth_policy_value", ")", "\n", "/", "ground_truth_policy_value", "\n", ")", ",", "\n", "\"sips3\"", ":", "np", ".", "abs", "(", "\n", "(", "mock_policy_value", "+", "sips3", ".", "eps", "-", "ground_truth_policy_value", ")", "\n", "/", "ground_truth_policy_value", "\n", ")", ",", "\n", "}", "\n", "", "else", ":", "\n", "# calculate se", "\n", "        ", "eval_metric_ope_dict", "=", "{", "\n", "\"sips\"", ":", "(", "mock_policy_value", "+", "sips", ".", "eps", "-", "ground_truth_policy_value", ")", "**", "2", ",", "\n", "\"sips3\"", ":", "(", "mock_policy_value", "+", "sips3", ".", "eps", "-", "ground_truth_policy_value", ")", "**", "2", ",", "\n", "}", "\n", "# check performance estimators", "\n", "", "ope_", "=", "SlateOffPolicyEvaluation", "(", "\n", "bandit_feedback", "=", "synthetic_slate_bandit_feedback", ",", "ope_estimators", "=", "[", "sips", ",", "sips3", "]", "\n", ")", "\n", "performance", "=", "ope_", ".", "evaluate_performance_of_estimators", "(", "\n", "ground_truth_policy_value", "=", "ground_truth_policy_value", ",", "\n", "evaluation_policy_pscore", "=", "evaluation_policy_pscore", ",", "\n", "metric", "=", "metric", ",", "\n", ")", "\n", "for", "k", ",", "v", "in", "performance", ".", "items", "(", ")", ":", "\n", "        ", "assert", "k", "in", "eval_metric_ope_dict", ",", "\"Invalid key of performance response\"", "\n", "assert", "v", "==", "eval_metric_ope_dict", "[", "k", "]", ",", "\"Invalid value of performance response\"", "\n", "", "performance_df", "=", "ope_", ".", "summarize_estimators_comparison", "(", "\n", "ground_truth_policy_value", "=", "ground_truth_policy_value", ",", "\n", "evaluation_policy_pscore", "=", "evaluation_policy_pscore", ",", "\n", "metric", "=", "metric", ",", "\n", ")", "\n", "assert_frame_equal", "(", "\n", "performance_df", ",", "pd", ".", "DataFrame", "(", "eval_metric_ope_dict", ",", "index", "=", "[", "metric", "]", ")", ".", "T", "\n", ")", ",", "\"Invalid summarization (performance)\"", "\n", "", ""]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_dm_estimators.test_dm_using_invalid_input_data": [[34, 56], ["pytest.mark.parametrize", "obp.ope.DirectMethod", "pytest.raises", "obp.ope.DirectMethod.estimate_policy_value", "pytest.raises", "obp.ope.DirectMethod.estimate_interval"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_policy_value", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_interval"], ["@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"action_dist, position, estimated_rewards_by_reg_model, description\"", ",", "\n", "invalid_input_of_dm", ",", "\n", ")", "\n", "def", "test_dm_using_invalid_input_data", "(", "\n", "action_dist", ":", "np", ".", "ndarray", ",", "\n", "position", ":", "np", ".", "ndarray", ",", "\n", "estimated_rewards_by_reg_model", ":", "np", ".", "ndarray", ",", "\n", "description", ":", "str", ",", "\n", ")", "->", "None", ":", "\n", "    ", "dm", "=", "DirectMethod", "(", ")", "\n", "with", "pytest", ".", "raises", "(", "ValueError", ",", "match", "=", "f\"{description}*\"", ")", ":", "\n", "        ", "_", "=", "dm", ".", "estimate_policy_value", "(", "\n", "action_dist", "=", "action_dist", ",", "\n", "position", "=", "position", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", ")", "\n", "", "with", "pytest", ".", "raises", "(", "ValueError", ",", "match", "=", "f\"{description}*\"", ")", ":", "\n", "        ", "_", "=", "dm", ".", "estimate_interval", "(", "\n", "action_dist", "=", "action_dist", ",", "\n", "position", "=", "position", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_dm_estimators.test_dm_using_random_evaluation_policy": [[59, 103], ["numpy.average", "np.average.mean", "obp.ope.DirectMethod", "obp.ope.DirectMethod.estimate_policy_value", "obp.ope.DirectMethod.estimate_policy_value", "pytest.raises", "obp.ope.DirectMethod.estimate_policy_value", "synthetic_bandit_feedback.items", "re.escape"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_policy_value", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_policy_value", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_policy_value"], ["", "", "def", "test_dm_using_random_evaluation_policy", "(", "\n", "synthetic_bandit_feedback", ":", "BanditFeedback", ",", "random_action_dist", ":", "np", ".", "ndarray", "\n", ")", "->", "None", ":", "\n", "    ", "\"\"\"\n    Test the performance of the direct method using synthetic bandit data and random evaluation policy\n    \"\"\"", "\n", "expected_reward", "=", "synthetic_bandit_feedback", "[", "\"expected_reward\"", "]", "[", ":", ",", ":", ",", "np", ".", "newaxis", "]", "\n", "action_dist", "=", "random_action_dist", "\n", "# compute ground truth policy value using expected reward", "\n", "q_pi_e", "=", "np", ".", "average", "(", "expected_reward", "[", ":", ",", ":", ",", "0", "]", ",", "weights", "=", "action_dist", "[", ":", ",", ":", ",", "0", "]", ",", "axis", "=", "1", ")", "\n", "# compute statistics of ground truth policy value", "\n", "gt_mean", "=", "q_pi_e", ".", "mean", "(", ")", "\n", "# prepare dm", "\n", "dm", "=", "DirectMethod", "(", ")", "\n", "# prepare input dict", "\n", "input_dict", "=", "{", "\n", "k", ":", "v", "\n", "for", "k", ",", "v", "in", "synthetic_bandit_feedback", ".", "items", "(", ")", "\n", "if", "k", "in", "[", "\"reward\"", ",", "\"action\"", ",", "\"pscore\"", ",", "\"position\"", "]", "\n", "}", "\n", "input_dict", "[", "\"action_dist\"", "]", "=", "action_dist", "\n", "# estimated_rewards_by_reg_model is required", "\n", "with", "pytest", ".", "raises", "(", "\n", "TypeError", ",", "\n", "match", "=", "re", ".", "escape", "(", "\n", "\"estimate_policy_value() missing 1 required positional argument: 'estimated_rewards_by_reg_model'\"", "\n", ")", ",", "\n", ")", ":", "\n", "        ", "_", "=", "dm", ".", "estimate_policy_value", "(", "**", "input_dict", ")", "\n", "# add estimated_rewards_by_reg_model", "\n", "", "input_dict", "[", "\"estimated_rewards_by_reg_model\"", "]", "=", "expected_reward", "\n", "# check expectation", "\n", "estimated_policy_value", "=", "dm", ".", "estimate_policy_value", "(", "**", "input_dict", ")", "\n", "assert", "(", "\n", "gt_mean", "==", "estimated_policy_value", "\n", ")", ",", "\"DM should be perfect when the regression model is perfect\"", "\n", "# remove unnecessary keys", "\n", "del", "input_dict", "[", "\"reward\"", "]", "\n", "del", "input_dict", "[", "\"pscore\"", "]", "\n", "del", "input_dict", "[", "\"action\"", "]", "\n", "estimated_policy_value", "=", "dm", ".", "estimate_policy_value", "(", "**", "input_dict", ")", "\n", "assert", "(", "\n", "gt_mean", "==", "estimated_policy_value", "\n", ")", ",", "\"DM should be perfect when the regression model is perfect\"", "\n", "", ""]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_dr_estimators.test_dr_init_using_invalid_inputs": [[45, 66], ["pytest.mark.parametrize", "pytest.raises", "obp.ope.DoublyRobust", "pytest.raises", "obp.ope.SwitchDoublyRobust", "pytest.raises", "obp.ope.DoublyRobustWithShrinkage"], "function", ["None"], ["@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"lambda_, use_estimated_pscore, err, description\"", ",", "\n", "invalid_input_of_dr_init", ",", "\n", ")", "\n", "def", "test_dr_init_using_invalid_inputs", "(", "\n", "lambda_", ",", "\n", "use_estimated_pscore", ",", "\n", "err", ",", "\n", "description", ",", "\n", ")", ":", "\n", "    ", "with", "pytest", ".", "raises", "(", "err", ",", "match", "=", "f\"{description}*\"", ")", ":", "\n", "        ", "_", "=", "DoublyRobust", "(", "lambda_", "=", "lambda_", ",", "use_estimated_pscore", "=", "use_estimated_pscore", ")", "\n", "\n", "", "with", "pytest", ".", "raises", "(", "err", ",", "match", "=", "f\"{description}*\"", ")", ":", "\n", "        ", "_", "=", "SwitchDoublyRobust", "(", "\n", "lambda_", "=", "lambda_", ",", "use_estimated_pscore", "=", "use_estimated_pscore", "\n", ")", "\n", "\n", "", "with", "pytest", ".", "raises", "(", "err", ",", "match", "=", "f\"{description}*\"", ")", ":", "\n", "        ", "_", "=", "DoublyRobustWithShrinkage", "(", "\n", "lambda_", "=", "lambda_", ",", "use_estimated_pscore", "=", "use_estimated_pscore", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_dr_estimators.test_dr_tuning_init_using_invalid_inputs": [[209, 256], ["pytest.mark.parametrize", "pytest.raises", "obp.ope.DoublyRobustTuning", "pytest.raises", "obp.ope.SwitchDoublyRobustTuning", "pytest.raises", "obp.ope.DoublyRobustWithShrinkageTuning", "pytest.raises", "obp.ope.SubGaussianDoublyRobustTuning"], "function", ["None"], ["@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"lambdas, tuning_method, use_bias_upper_bound, delta, use_estimated_pscore, err, description\"", ",", "\n", "invalid_input_of_dr_tuning_init", ",", "\n", ")", "\n", "def", "test_dr_tuning_init_using_invalid_inputs", "(", "\n", "lambdas", ",", "\n", "tuning_method", ",", "\n", "use_bias_upper_bound", ",", "\n", "delta", ",", "\n", "use_estimated_pscore", ",", "\n", "err", ",", "\n", "description", ",", "\n", ")", ":", "\n", "    ", "with", "pytest", ".", "raises", "(", "err", ",", "match", "=", "f\"{description}*\"", ")", ":", "\n", "        ", "_", "=", "DoublyRobustTuning", "(", "\n", "use_bias_upper_bound", "=", "use_bias_upper_bound", ",", "\n", "delta", "=", "delta", ",", "\n", "lambdas", "=", "lambdas", ",", "\n", "tuning_method", "=", "tuning_method", ",", "\n", "use_estimated_pscore", "=", "use_estimated_pscore", ",", "\n", ")", "\n", "\n", "", "with", "pytest", ".", "raises", "(", "err", ",", "match", "=", "f\"{description}*\"", ")", ":", "\n", "        ", "_", "=", "SwitchDoublyRobustTuning", "(", "\n", "use_bias_upper_bound", "=", "use_bias_upper_bound", ",", "\n", "delta", "=", "delta", ",", "\n", "lambdas", "=", "lambdas", ",", "\n", "tuning_method", "=", "tuning_method", ",", "\n", "use_estimated_pscore", "=", "use_estimated_pscore", ",", "\n", ")", "\n", "\n", "", "with", "pytest", ".", "raises", "(", "err", ",", "match", "=", "f\"{description}*\"", ")", ":", "\n", "        ", "_", "=", "DoublyRobustWithShrinkageTuning", "(", "\n", "use_bias_upper_bound", "=", "use_bias_upper_bound", ",", "\n", "delta", "=", "delta", ",", "\n", "lambdas", "=", "lambdas", ",", "\n", "tuning_method", "=", "tuning_method", ",", "\n", "use_estimated_pscore", "=", "use_estimated_pscore", ",", "\n", ")", "\n", "\n", "", "with", "pytest", ".", "raises", "(", "err", ",", "match", "=", "f\"{description}*\"", ")", ":", "\n", "        ", "_", "=", "SubGaussianDoublyRobustTuning", "(", "\n", "use_bias_upper_bound", "=", "use_bias_upper_bound", ",", "\n", "delta", "=", "delta", ",", "\n", "lambdas", "=", "lambdas", ",", "\n", "tuning_method", "=", "tuning_method", ",", "\n", "use_estimated_pscore", "=", "use_estimated_pscore", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_dr_estimators.test_dr_init_using_valid_input_data": [[266, 276], ["pytest.mark.parametrize", "obp.ope.DoublyRobust", "obp.ope.DoublyRobustWithShrinkage", "obp.ope.SwitchDoublyRobust", "obp.ope.SubGaussianDoublyRobust"], "function", ["None"], ["@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"lambda_, description\"", ",", "\n", "valid_input_of_dr_init", ",", "\n", ")", "\n", "def", "test_dr_init_using_valid_input_data", "(", "lambda_", ":", "float", ",", "description", ":", "str", ")", "->", "None", ":", "\n", "    ", "_", "=", "DoublyRobust", "(", "lambda_", "=", "lambda_", ")", "\n", "_", "=", "DoublyRobustWithShrinkage", "(", "lambda_", "=", "lambda_", ")", "\n", "_", "=", "SwitchDoublyRobust", "(", "lambda_", "=", "lambda_", ")", "\n", "if", "lambda_", "<", "np", ".", "inf", ":", "\n", "        ", "_", "=", "SubGaussianDoublyRobust", "(", "lambda_", "=", "lambda_", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_dr_estimators.test_dr_tuning_init_using_valid_input_data": [[284, 301], ["pytest.mark.parametrize", "obp.ope.DoublyRobustTuning", "obp.ope.DoublyRobustWithShrinkageTuning", "obp.ope.SwitchDoublyRobustTuning", "obp.ope.SubGaussianDoublyRobustTuning"], "function", ["None"], ["@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"lambdas, tuning_method, description\"", ",", "\n", "valid_input_of_dr_tuning_init", ",", "\n", ")", "\n", "def", "test_dr_tuning_init_using_valid_input_data", "(", "lambdas", ",", "tuning_method", ",", "description", ")", ":", "\n", "    ", "_", "=", "DoublyRobustTuning", "(", "lambdas", "=", "lambdas", ",", "tuning_method", "=", "tuning_method", ")", "\n", "_", "=", "DoublyRobustWithShrinkageTuning", "(", "\n", "lambdas", "=", "lambdas", ",", "\n", "tuning_method", "=", "tuning_method", ",", "\n", ")", "\n", "_", "=", "SwitchDoublyRobustTuning", "(", "\n", "lambdas", "=", "lambdas", ",", "\n", "tuning_method", "=", "tuning_method", ",", "\n", ")", "\n", "_", "=", "SubGaussianDoublyRobustTuning", "(", "\n", "lambdas", "=", "lambdas", ",", "\n", "tuning_method", "=", "tuning_method", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_dr_estimators.test_dr_using_invalid_input_data": [[613, 656], ["pytest.mark.parametrize", "obp.ope.DoublyRobust", "obp.ope.DoublyRobustTuning", "obp.ope.SelfNormalizedDoublyRobust", "pytest.raises", "estimator.estimate_policy_value", "pytest.raises", "estimator.estimate_interval"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_policy_value", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_interval"], ["@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"action_dist, action, reward, pscore, position, estimated_rewards_by_reg_model, use_estimated_pscore, estimated_pscore, description\"", ",", "\n", "invalid_input_of_dr", ",", "\n", ")", "\n", "def", "test_dr_using_invalid_input_data", "(", "\n", "action_dist", ":", "np", ".", "ndarray", ",", "\n", "action", ":", "np", ".", "ndarray", ",", "\n", "reward", ":", "np", ".", "ndarray", ",", "\n", "pscore", ":", "np", ".", "ndarray", ",", "\n", "position", ":", "np", ".", "ndarray", ",", "\n", "estimated_rewards_by_reg_model", ":", "np", ".", "ndarray", ",", "\n", "use_estimated_pscore", ":", "bool", ",", "\n", "estimated_pscore", ":", "np", ".", "ndarray", ",", "\n", "description", ":", "str", ",", "\n", ")", "->", "None", ":", "\n", "    ", "dr", "=", "DoublyRobust", "(", "use_estimated_pscore", "=", "use_estimated_pscore", ")", "\n", "dr_tuning", "=", "DoublyRobustTuning", "(", "\n", "lambdas", "=", "[", "1", ",", "100", "]", ",", "\n", "estimator_name", "=", "\"dr_tuning\"", ",", "\n", "use_estimated_pscore", "=", "use_estimated_pscore", ",", "\n", ")", "\n", "sndr", "=", "SelfNormalizedDoublyRobust", "(", "use_estimated_pscore", "=", "use_estimated_pscore", ")", "\n", "# estimate_intervals function raises ValueError of all estimators", "\n", "for", "estimator", "in", "[", "dr", ",", "sndr", ",", "dr_tuning", "]", ":", "\n", "        ", "with", "pytest", ".", "raises", "(", "ValueError", ",", "match", "=", "f\"{description}*\"", ")", ":", "\n", "            ", "_", "=", "estimator", ".", "estimate_policy_value", "(", "\n", "action_dist", "=", "action_dist", ",", "\n", "action", "=", "action", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore", "=", "pscore", ",", "\n", "position", "=", "position", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", "estimated_pscore", "=", "estimated_pscore", ",", "\n", ")", "\n", "", "with", "pytest", ".", "raises", "(", "ValueError", ",", "match", "=", "f\"{description}*\"", ")", ":", "\n", "            ", "_", "=", "estimator", ".", "estimate_interval", "(", "\n", "action_dist", "=", "action_dist", ",", "\n", "action", "=", "action", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore", "=", "pscore", ",", "\n", "position", "=", "position", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", "estimated_pscore", "=", "estimated_pscore", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_dr_estimators.test_dr_variants_using_valid_input_data": [[675, 755], ["pytest.mark.parametrize", "obp.ope.SwitchDoublyRobust", "obp.ope.SwitchDoublyRobustTuning", "obp.ope.SwitchDoublyRobustTuning", "obp.ope.DoublyRobustWithShrinkage", "obp.ope.DoublyRobustWithShrinkageTuning", "obp.ope.DoublyRobustWithShrinkageTuning", "obp.ope.SubGaussianDoublyRobust", "obp.ope.SubGaussianDoublyRobustTuning", "obp.ope.SubGaussianDoublyRobustTuning", "obp.ope.SwitchDoublyRobust", "obp.ope.SwitchDoublyRobustTuning", "obp.ope.DoublyRobustWithShrinkage", "obp.ope.DoublyRobustWithShrinkageTuning", "estimator.estimate_policy_value"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_policy_value"], ["@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"action_dist, action, reward, pscore, position, estimated_rewards_by_reg_model, estimated_pscore, hyperparameter, description\"", ",", "\n", "valid_input_of_dr_variants", ",", "\n", ")", "\n", "def", "test_dr_variants_using_valid_input_data", "(", "\n", "action_dist", ":", "np", ".", "ndarray", ",", "\n", "action", ":", "np", ".", "ndarray", ",", "\n", "reward", ":", "np", ".", "ndarray", ",", "\n", "pscore", ":", "np", ".", "ndarray", ",", "\n", "position", ":", "np", ".", "ndarray", ",", "\n", "estimated_rewards_by_reg_model", ":", "np", ".", "ndarray", ",", "\n", "estimated_pscore", ":", "np", ".", "ndarray", ",", "\n", "hyperparameter", ":", "float", ",", "\n", "description", ":", "str", ",", "\n", ")", "->", "None", ":", "\n", "# check dr variants", "\n", "    ", "switch_dr", "=", "SwitchDoublyRobust", "(", "lambda_", "=", "hyperparameter", ")", "\n", "switch_dr_tuning_mse", "=", "SwitchDoublyRobustTuning", "(", "\n", "lambdas", "=", "[", "hyperparameter", ",", "hyperparameter", "*", "10", "]", ",", "\n", "tuning_method", "=", "\"mse\"", ",", "\n", ")", "\n", "switch_dr_tuning_slope", "=", "SwitchDoublyRobustTuning", "(", "\n", "lambdas", "=", "[", "hyperparameter", ",", "hyperparameter", "*", "10", "]", ",", "\n", "tuning_method", "=", "\"slope\"", ",", "\n", ")", "\n", "dr_os", "=", "DoublyRobustWithShrinkage", "(", "lambda_", "=", "hyperparameter", ")", "\n", "dr_os_tuning_mse", "=", "DoublyRobustWithShrinkageTuning", "(", "\n", "lambdas", "=", "[", "hyperparameter", ",", "hyperparameter", "*", "10", "]", ",", "\n", "tuning_method", "=", "\"mse\"", ",", "\n", ")", "\n", "dr_os_tuning_slope", "=", "DoublyRobustWithShrinkageTuning", "(", "\n", "lambdas", "=", "[", "hyperparameter", ",", "hyperparameter", "*", "10", "]", ",", "\n", "tuning_method", "=", "\"slope\"", ",", "\n", ")", "\n", "sg_dr", "=", "SubGaussianDoublyRobust", "(", "lambda_", "=", "hyperparameter", ")", "\n", "sg_dr_tuning_mse", "=", "SubGaussianDoublyRobustTuning", "(", "\n", "lambdas", "=", "[", "hyperparameter", ",", "hyperparameter", "/", "10", "]", ",", "\n", "tuning_method", "=", "\"mse\"", ",", "\n", ")", "\n", "sg_dr_tuning_slope", "=", "SubGaussianDoublyRobustTuning", "(", "\n", "lambdas", "=", "[", "hyperparameter", ",", "hyperparameter", "/", "10", "]", ",", "\n", "tuning_method", "=", "\"slope\"", ",", "\n", ")", "\n", "switch_dr_estimated_pscore", "=", "SwitchDoublyRobust", "(", "\n", "lambda_", "=", "hyperparameter", ",", "use_estimated_pscore", "=", "True", "\n", ")", "\n", "switch_dr_tuning_estimated_pscore", "=", "SwitchDoublyRobustTuning", "(", "\n", "lambdas", "=", "[", "hyperparameter", ",", "hyperparameter", "*", "10", "]", ",", "use_estimated_pscore", "=", "True", "\n", ")", "\n", "dr_os_estimated_pscore", "=", "DoublyRobustWithShrinkage", "(", "\n", "lambda_", "=", "hyperparameter", ",", "use_estimated_pscore", "=", "True", "\n", ")", "\n", "dr_os_tuning_estimated_pscore", "=", "DoublyRobustWithShrinkageTuning", "(", "\n", "lambdas", "=", "[", "hyperparameter", ",", "hyperparameter", "*", "10", "]", ",", "use_estimated_pscore", "=", "True", "\n", ")", "\n", "for", "estimator", "in", "[", "\n", "sg_dr", ",", "\n", "sg_dr_tuning_mse", ",", "\n", "sg_dr_tuning_slope", ",", "\n", "switch_dr", ",", "\n", "switch_dr_tuning_mse", ",", "\n", "switch_dr_tuning_slope", ",", "\n", "switch_dr_estimated_pscore", ",", "\n", "switch_dr_tuning_estimated_pscore", ",", "\n", "dr_os", ",", "\n", "dr_os_tuning_mse", ",", "\n", "dr_os_tuning_slope", ",", "\n", "dr_os_estimated_pscore", ",", "\n", "dr_os_tuning_estimated_pscore", ",", "\n", "]", ":", "\n", "        ", "est", "=", "estimator", ".", "estimate_policy_value", "(", "\n", "action_dist", "=", "action_dist", ",", "\n", "action", "=", "action", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore", "=", "pscore", ",", "\n", "position", "=", "position", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", "estimated_pscore", "=", "estimated_pscore", ",", "\n", ")", "\n", "assert", "est", "==", "0.0", ",", "f\"policy value must be 0, but {est}\"", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_dr_estimators.test_dr_using_random_evaluation_policy": [[757, 792], ["estimator.estimate_policy_value", "isinstance", "synthetic_bandit_feedback.items", "pytest.raises", "estimator.estimate_policy_value", "re.escape"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_policy_value", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_policy_value"], ["", "", "def", "test_dr_using_random_evaluation_policy", "(", "\n", "synthetic_bandit_feedback", ":", "BanditFeedback", ",", "random_action_dist", ":", "np", ".", "ndarray", "\n", ")", "->", "None", ":", "\n", "    ", "\"\"\"\n    Test the format of dr variants using synthetic bandit data and random evaluation policy\n    \"\"\"", "\n", "expected_reward", "=", "synthetic_bandit_feedback", "[", "\"expected_reward\"", "]", "[", ":", ",", ":", ",", "np", ".", "newaxis", "]", "\n", "action_dist", "=", "random_action_dist", "\n", "# prepare input dict", "\n", "input_dict", "=", "{", "\n", "k", ":", "v", "\n", "for", "k", ",", "v", "in", "synthetic_bandit_feedback", ".", "items", "(", ")", "\n", "if", "k", "in", "[", "\"reward\"", ",", "\"action\"", ",", "\"pscore\"", ",", "\"position\"", "]", "\n", "}", "\n", "input_dict", "[", "\"action_dist\"", "]", "=", "action_dist", "\n", "input_dict", "[", "\"estimated_rewards_by_reg_model\"", "]", "=", "expected_reward", "\n", "input_dict", "[", "\"estimated_pscore\"", "]", "=", "input_dict", "[", "\"pscore\"", "]", "\n", "# dr estimators require all arguments", "\n", "for", "estimator", "in", "dr_estimators", ":", "\n", "        ", "estimated_policy_value", "=", "estimator", ".", "estimate_policy_value", "(", "**", "input_dict", ")", "\n", "assert", "isinstance", "(", "\n", "estimated_policy_value", ",", "float", "\n", ")", ",", "f\"invalid type response: {estimator}\"", "\n", "# remove necessary keys", "\n", "", "del", "input_dict", "[", "\"reward\"", "]", "\n", "del", "input_dict", "[", "\"action\"", "]", "\n", "del", "input_dict", "[", "\"estimated_rewards_by_reg_model\"", "]", "\n", "for", "estimator", "in", "dr_estimators", ":", "\n", "        ", "with", "pytest", ".", "raises", "(", "\n", "TypeError", ",", "\n", "match", "=", "re", ".", "escape", "(", "\n", "\"estimate_policy_value() missing 3 required positional arguments: 'reward', 'action', and 'estimated_rewards_by_reg_model'\"", "\n", ")", ",", "\n", ")", ":", "\n", "            ", "_", "=", "estimator", ".", "estimate_policy_value", "(", "**", "input_dict", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_dr_estimators.test_boundedness_of_sndr_using_random_evaluation_policy": [[794, 816], ["sndr.estimate_policy_value", "synthetic_bandit_feedback.items"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_policy_value"], ["", "", "", "def", "test_boundedness_of_sndr_using_random_evaluation_policy", "(", "\n", "synthetic_bandit_feedback", ":", "BanditFeedback", ",", "random_action_dist", ":", "np", ".", "ndarray", "\n", ")", "->", "None", ":", "\n", "    ", "\"\"\"\n    Test the boundedness of sndr estimators using synthetic bandit data and random evaluation policy\n    \"\"\"", "\n", "expected_reward", "=", "synthetic_bandit_feedback", "[", "\"expected_reward\"", "]", "[", ":", ",", ":", ",", "np", ".", "newaxis", "]", "\n", "action_dist", "=", "random_action_dist", "\n", "# prepare input dict", "\n", "input_dict", "=", "{", "\n", "k", ":", "v", "\n", "for", "k", ",", "v", "in", "synthetic_bandit_feedback", ".", "items", "(", ")", "\n", "if", "k", "in", "[", "\"reward\"", ",", "\"action\"", ",", "\"pscore\"", ",", "\"position\"", "]", "\n", "}", "\n", "input_dict", "[", "\"action_dist\"", "]", "=", "action_dist", "\n", "input_dict", "[", "\"estimated_rewards_by_reg_model\"", "]", "=", "expected_reward", "\n", "# make pscore too small (to check the boundedness of sndr)", "\n", "input_dict", "[", "\"pscore\"", "]", "=", "input_dict", "[", "\"pscore\"", "]", "**", "3", "\n", "estimated_policy_value", "=", "sndr", ".", "estimate_policy_value", "(", "**", "input_dict", ")", "\n", "assert", "(", "\n", "estimated_policy_value", "<=", "2", "\n", ")", ",", "f\"estimated policy value of sndr should be smaller than or equal to 2 (because of its 2-boundedness), but the value is: {estimated_policy_value}\"", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_dr_estimators.test_dr_os_using_random_evaluation_policy": [[818, 844], ["dm.estimate_policy_value", "dr.estimate_policy_value", "dr_os_0.estimate_policy_value", "dr_os_max.estimate_policy_value", "numpy.abs", "synthetic_bandit_feedback.items"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_policy_value", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_policy_value", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_policy_value", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_policy_value"], ["", "def", "test_dr_os_using_random_evaluation_policy", "(", "\n", "synthetic_bandit_feedback", ":", "BanditFeedback", ",", "random_action_dist", ":", "np", ".", "ndarray", "\n", ")", "->", "None", ":", "\n", "    ", "\"\"\"\n    Test the dr shrinkage estimators using synthetic bandit data and random evaluation policy\n    \"\"\"", "\n", "expected_reward", "=", "synthetic_bandit_feedback", "[", "\"expected_reward\"", "]", "[", ":", ",", ":", ",", "np", ".", "newaxis", "]", "\n", "action_dist", "=", "random_action_dist", "\n", "# prepare input dict", "\n", "input_dict", "=", "{", "\n", "k", ":", "v", "\n", "for", "k", ",", "v", "in", "synthetic_bandit_feedback", ".", "items", "(", ")", "\n", "if", "k", "in", "[", "\"reward\"", ",", "\"action\"", ",", "\"pscore\"", ",", "\"position\"", "]", "\n", "}", "\n", "input_dict", "[", "\"action_dist\"", "]", "=", "action_dist", "\n", "input_dict", "[", "\"estimated_rewards_by_reg_model\"", "]", "=", "expected_reward", "\n", "dm_value", "=", "dm", ".", "estimate_policy_value", "(", "**", "input_dict", ")", "\n", "dr_value", "=", "dr", ".", "estimate_policy_value", "(", "**", "input_dict", ")", "\n", "dr_os_0_value", "=", "dr_os_0", ".", "estimate_policy_value", "(", "**", "input_dict", ")", "\n", "dr_os_max_value", "=", "dr_os_max", ".", "estimate_policy_value", "(", "**", "input_dict", ")", "\n", "assert", "(", "\n", "dm_value", "==", "dr_os_0_value", "\n", ")", ",", "\"DoublyRobustWithShrinkage (lambda=0) should be the same as DirectMethod\"", "\n", "assert", "(", "\n", "np", ".", "abs", "(", "dr_value", "-", "dr_os_max_value", ")", "<", "1e-5", "\n", ")", ",", "\"DoublyRobustWithShrinkage (lambda=inf) should be almost the same as DoublyRobust\"", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_dr_estimators.test_switch_dr_using_random_evaluation_policy": [[846, 872], ["dm.estimate_policy_value", "dr.estimate_policy_value", "switch_dr_0.estimate_policy_value", "switch_dr_max.estimate_policy_value", "synthetic_bandit_feedback.items"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_policy_value", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_policy_value", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_policy_value", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_policy_value"], ["", "def", "test_switch_dr_using_random_evaluation_policy", "(", "\n", "synthetic_bandit_feedback", ":", "BanditFeedback", ",", "random_action_dist", ":", "np", ".", "ndarray", "\n", ")", "->", "None", ":", "\n", "    ", "\"\"\"\n    Test the switch_dr using synthetic bandit data and random evaluation policy\n    \"\"\"", "\n", "expected_reward", "=", "synthetic_bandit_feedback", "[", "\"expected_reward\"", "]", "[", ":", ",", ":", ",", "np", ".", "newaxis", "]", "\n", "action_dist", "=", "random_action_dist", "\n", "# prepare input dict", "\n", "input_dict", "=", "{", "\n", "k", ":", "v", "\n", "for", "k", ",", "v", "in", "synthetic_bandit_feedback", ".", "items", "(", ")", "\n", "if", "k", "in", "[", "\"reward\"", ",", "\"action\"", ",", "\"pscore\"", ",", "\"position\"", "]", "\n", "}", "\n", "input_dict", "[", "\"action_dist\"", "]", "=", "action_dist", "\n", "input_dict", "[", "\"estimated_rewards_by_reg_model\"", "]", "=", "expected_reward", "\n", "dm_value", "=", "dm", ".", "estimate_policy_value", "(", "**", "input_dict", ")", "\n", "dr_value", "=", "dr", ".", "estimate_policy_value", "(", "**", "input_dict", ")", "\n", "switch_dr_0_value", "=", "switch_dr_0", ".", "estimate_policy_value", "(", "**", "input_dict", ")", "\n", "switch_dr_max_value", "=", "switch_dr_max", ".", "estimate_policy_value", "(", "**", "input_dict", ")", "\n", "assert", "(", "\n", "dm_value", "==", "switch_dr_0_value", "\n", ")", ",", "\"SwitchDR (lambda=0) should be the same as DirectMethod\"", "\n", "assert", "(", "\n", "dr_value", "==", "switch_dr_max_value", "\n", ")", ",", "\"SwitchDR (lambda=1e10) should be the same as DoublyRobust\"", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_dr_estimators.test_sg_dr_using_random_evaluation_policy": [[874, 901], ["dr.estimate_policy_value", "sg_dr_0.estimate_policy_value", "dr_estimated_pscore.estimate_policy_value", "synthetic_bandit_feedback.items"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_policy_value", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_policy_value", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_policy_value"], ["", "def", "test_sg_dr_using_random_evaluation_policy", "(", "\n", "synthetic_bandit_feedback", ":", "BanditFeedback", ",", "random_action_dist", ":", "np", ".", "ndarray", "\n", ")", "->", "None", ":", "\n", "    ", "\"\"\"\n    Test the switch_dr using synthetic bandit data and random evaluation policy\n    \"\"\"", "\n", "expected_reward", "=", "synthetic_bandit_feedback", "[", "\"expected_reward\"", "]", "[", ":", ",", ":", ",", "np", ".", "newaxis", "]", "\n", "action_dist", "=", "random_action_dist", "\n", "# prepare input dict", "\n", "input_dict", "=", "{", "\n", "k", ":", "v", "\n", "for", "k", ",", "v", "in", "synthetic_bandit_feedback", ".", "items", "(", ")", "\n", "if", "k", "in", "[", "\"reward\"", ",", "\"action\"", ",", "\"pscore\"", ",", "\"position\"", "]", "\n", "}", "\n", "input_dict", "[", "\"action_dist\"", "]", "=", "action_dist", "\n", "input_dict", "[", "\"estimated_rewards_by_reg_model\"", "]", "=", "expected_reward", "\n", "dr_value", "=", "dr", ".", "estimate_policy_value", "(", "**", "input_dict", ")", "\n", "sg_dr_0_value", "=", "sg_dr_0", ".", "estimate_policy_value", "(", "**", "input_dict", ")", "\n", "assert", "(", "\n", "dr_value", "==", "sg_dr_0_value", "\n", ")", ",", "\"SG-DR (lambda=0) should be the same as DoublyRobust\"", "\n", "input_dict", "[", "\"estimated_pscore\"", "]", "=", "input_dict", "[", "\"pscore\"", "]", "\n", "del", "input_dict", "[", "\"pscore\"", "]", "\n", "dr_value_estimated_pscore", "=", "dr_estimated_pscore", ".", "estimate_policy_value", "(", "**", "input_dict", ")", "\n", "assert", "(", "\n", "dr_value", "==", "dr_value_estimated_pscore", "\n", ")", ",", "\"DoublyRobust with estimated_pscore (which is the same as pscore) should be the same as DoublyRobust\"", "\n", "", ""]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_bipw_estimators.test_bipw_init_using_invalid_inputs": [[28, 40], ["pytest.mark.parametrize", "pytest.raises", "obp.ope.BalancedInverseProbabilityWeighting"], "function", ["None"], ["@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"lambda_, err, description\"", ",", "\n", "invalid_input_of_bipw_init", ",", "\n", ")", "\n", "def", "test_bipw_init_using_invalid_inputs", "(", "\n", "lambda_", ",", "\n", "err", ",", "\n", "description", ",", "\n", ")", ":", "\n", "    ", "with", "pytest", ".", "raises", "(", "err", ",", "match", "=", "f\"{description}*\"", ")", ":", "\n", "        ", "_", "=", "BalancedInverseProbabilityWeighting", "(", "\n", "lambda_", "=", "lambda_", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_bipw_estimators.test_bipw_using_invalid_input_data": [[179, 208], ["pytest.mark.parametrize", "obp.ope.BalancedInverseProbabilityWeighting", "pytest.raises", "obp.ope.BalancedInverseProbabilityWeighting.estimate_policy_value", "pytest.raises", "obp.ope.BalancedInverseProbabilityWeighting.estimate_interval"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_policy_value", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_interval"], ["@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"action_dist, action, reward, position, estimated_importance_weights, description\"", ",", "\n", "invalid_input_of_bipw", ",", "\n", ")", "\n", "def", "test_bipw_using_invalid_input_data", "(", "\n", "action_dist", ":", "np", ".", "ndarray", ",", "\n", "action", ":", "np", ".", "ndarray", ",", "\n", "reward", ":", "np", ".", "ndarray", ",", "\n", "position", ":", "np", ".", "ndarray", ",", "\n", "estimated_importance_weights", ":", "np", ".", "ndarray", ",", "\n", "description", ":", "str", ",", "\n", ")", "->", "None", ":", "\n", "# prepare bipw instances", "\n", "    ", "bipw", "=", "BalancedInverseProbabilityWeighting", "(", ")", "\n", "with", "pytest", ".", "raises", "(", "ValueError", ",", "match", "=", "f\"{description}*\"", ")", ":", "\n", "        ", "_", "=", "bipw", ".", "estimate_policy_value", "(", "\n", "action_dist", "=", "action_dist", ",", "\n", "action", "=", "action", ",", "\n", "reward", "=", "reward", ",", "\n", "position", "=", "position", ",", "\n", "estimated_importance_weights", "=", "estimated_importance_weights", ",", "\n", ")", "\n", "", "with", "pytest", ".", "raises", "(", "ValueError", ",", "match", "=", "f\"{description}*\"", ")", ":", "\n", "        ", "_", "=", "bipw", ".", "estimate_interval", "(", "\n", "action_dist", "=", "action_dist", ",", "\n", "action", "=", "action", ",", "\n", "reward", "=", "reward", ",", "\n", "position", "=", "position", ",", "\n", "estimated_importance_weights", "=", "estimated_importance_weights", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_bipw_estimators.test_bipw_using_random_evaluation_policy": [[211, 253], ["numpy.ones", "bipw.estimate_policy_value", "estimator.estimate_policy_value", "isinstance", "synthetic_bandit_feedback.items", "pytest.raises", "estimator.estimate_policy_value", "re.escape"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_policy_value", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_policy_value", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_policy_value"], ["", "", "def", "test_bipw_using_random_evaluation_policy", "(", "\n", "synthetic_bandit_feedback", ":", "BanditFeedback", ",", "random_action_dist", ":", "np", ".", "ndarray", "\n", ")", "->", "None", ":", "\n", "    ", "\"\"\"\n    Test the format of bipw variants using synthetic bandit data and random evaluation policy\n    \"\"\"", "\n", "action_dist", "=", "random_action_dist", "\n", "# prepare input dict", "\n", "input_dict", "=", "{", "\n", "k", ":", "v", "\n", "for", "k", ",", "v", "in", "synthetic_bandit_feedback", ".", "items", "(", ")", "\n", "if", "k", "in", "[", "\"reward\"", ",", "\"action\"", ",", "\"pscore\"", ",", "\"position\"", "]", "\n", "}", "\n", "input_dict", "[", "\"action_dist\"", "]", "=", "action_dist", "\n", "# insert dummy values", "\n", "input_dict", "[", "\"estimated_importance_weights\"", "]", "=", "np", ".", "ones", "(", "action_dist", ".", "shape", "[", "0", "]", ")", "\n", "# check responce", "\n", "for", "estimator", "in", "[", "bipw", "]", ":", "\n", "        ", "estimated_policy_value", "=", "estimator", ".", "estimate_policy_value", "(", "**", "input_dict", ")", "\n", "assert", "isinstance", "(", "\n", "estimated_policy_value", ",", "float", "\n", ")", ",", "f\"invalid type response: {estimator}\"", "\n", "\n", "# make estimated_importance_weights too small (to check the boundedness of snbipw)", "\n", "", "input_dict", "[", "\"estimated_importance_weights\"", "]", "=", "input_dict", "[", "\"pscore\"", "]", "**", "3", "\n", "estimated_policy_value", "=", "bipw", ".", "estimate_policy_value", "(", "**", "input_dict", ")", "\n", "assert", "(", "\n", "estimated_policy_value", "<=", "1", "\n", ")", ",", "f\"estimated policy value of bipw should be smaller than or equal to 1 (because of its 1-boundedness), but the value is: {estimated_policy_value}\"", "\n", "\n", "# remove necessary keys", "\n", "del", "input_dict", "[", "\"reward\"", "]", "\n", "del", "input_dict", "[", "\"action\"", "]", "\n", "del", "input_dict", "[", "\"estimated_importance_weights\"", "]", "\n", "for", "estimator", "in", "[", "bipw", "]", ":", "\n", "        ", "with", "pytest", ".", "raises", "(", "\n", "TypeError", ",", "\n", "match", "=", "re", ".", "escape", "(", "\n", "\"estimate_policy_value() missing 3 required positional arguments: 'reward', 'action', and 'estimated_importance_weights'\"", "\n", ")", ",", "\n", ")", ":", "\n", "            ", "_", "=", "estimator", ".", "estimate_policy_value", "(", "**", "input_dict", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_regression_models.test_initializing_regression_models_using_invalid_input_data": [[722, 743], ["pytest.mark.parametrize", "pytest.raises", "obp.ope.RegressionModel"], "function", ["None"], ["@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"action_context, n_actions, len_list, fitting_method, base_model, err, description\"", ",", "\n", "invalid_input_of_initializing_regression_models", ",", "\n", ")", "\n", "def", "test_initializing_regression_models_using_invalid_input_data", "(", "\n", "action_context", ":", "np", ".", "ndarray", ",", "\n", "n_actions", ":", "int", ",", "\n", "len_list", ":", "int", ",", "\n", "fitting_method", ":", "str", ",", "\n", "base_model", ":", "BaseEstimator", ",", "\n", "err", ",", "\n", "description", ":", "str", ",", "\n", ")", "->", "None", ":", "\n", "# initialization raises ValueError", "\n", "    ", "with", "pytest", ".", "raises", "(", "err", ",", "match", "=", "f\"{description}*\"", ")", ":", "\n", "        ", "_", "=", "RegressionModel", "(", "\n", "n_actions", "=", "n_actions", ",", "\n", "len_list", "=", "len_list", ",", "\n", "action_context", "=", "action_context", ",", "\n", "base_model", "=", "base_model", ",", "\n", "fitting_method", "=", "fitting_method", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_regression_models.test_fitting_regression_models_using_invalid_input_data": [[746, 797], ["pytest.mark.parametrize", "pytest.raises", "obp.ope.RegressionModel", "obp.ope.RegressionModel.fit_predict", "obp.ope.RegressionModel.fit_predict"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.classification_model.PropensityScoreEstimator.fit_predict", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.classification_model.PropensityScoreEstimator.fit_predict"], ["", "", "@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"context, action, reward, pscore, position, action_context, n_actions, len_list, fitting_method, base_model, action_dist, n_folds, random_state, err, description\"", ",", "\n", "invalid_input_of_fitting_regression_models", ",", "\n", ")", "\n", "def", "test_fitting_regression_models_using_invalid_input_data", "(", "\n", "context", ":", "np", ".", "ndarray", ",", "\n", "action", ":", "np", ".", "ndarray", ",", "\n", "reward", ":", "np", ".", "ndarray", ",", "\n", "pscore", ":", "np", ".", "ndarray", ",", "\n", "position", ":", "np", ".", "ndarray", ",", "\n", "action_context", ":", "np", ".", "ndarray", ",", "\n", "n_actions", ":", "int", ",", "\n", "len_list", ":", "int", ",", "\n", "fitting_method", ":", "str", ",", "\n", "base_model", ":", "BaseEstimator", ",", "\n", "action_dist", ":", "np", ".", "ndarray", ",", "\n", "n_folds", ":", "int", ",", "\n", "random_state", ":", "int", ",", "\n", "err", ",", "\n", "description", ":", "str", ",", "\n", ")", "->", "None", ":", "\n", "# fit_predict function raises ValueError", "\n", "    ", "with", "pytest", ".", "raises", "(", "err", ",", "match", "=", "f\"{description}*\"", ")", ":", "\n", "        ", "regression_model", "=", "RegressionModel", "(", "\n", "n_actions", "=", "n_actions", ",", "\n", "len_list", "=", "len_list", ",", "\n", "action_context", "=", "action_context", ",", "\n", "base_model", "=", "base_model", ",", "\n", "fitting_method", "=", "fitting_method", ",", "\n", ")", "\n", "if", "fitting_method", "==", "\"normal\"", ":", "\n", "# train regression model on logged bandit feedback data", "\n", "            ", "_", "=", "regression_model", ".", "fit_predict", "(", "\n", "context", "=", "context", ",", "\n", "action", "=", "action", ",", "\n", "reward", "=", "reward", ",", "\n", "position", "=", "position", ",", "\n", "n_folds", "=", "n_folds", ",", "\n", "random_state", "=", "random_state", ",", "\n", ")", "\n", "", "else", ":", "\n", "# train regression model on logged bandit feedback data", "\n", "            ", "_", "=", "regression_model", ".", "fit_predict", "(", "\n", "context", "=", "context", ",", "\n", "action", "=", "action", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore", "=", "pscore", ",", "\n", "position", "=", "position", ",", "\n", "action_dist", "=", "action_dist", ",", "\n", "n_folds", "=", "n_folds", ",", "\n", "random_state", "=", "random_state", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_regression_models.test_regression_models_using_valid_input_data": [[800, 849], ["pytest.mark.parametrize", "obp.ope.RegressionModel", "obp.ope.RegressionModel.fit_predict", "obp.ope.RegressionModel.fit_predict"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.classification_model.PropensityScoreEstimator.fit_predict", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.classification_model.PropensityScoreEstimator.fit_predict"], ["", "", "", "@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"context, action, reward, pscore, position, action_context, n_actions, len_list, fitting_method, base_model, action_dist, n_folds, random_state, description\"", ",", "\n", "valid_input_of_regression_models", ",", "\n", ")", "\n", "def", "test_regression_models_using_valid_input_data", "(", "\n", "context", ":", "np", ".", "ndarray", ",", "\n", "action", ":", "np", ".", "ndarray", ",", "\n", "reward", ":", "np", ".", "ndarray", ",", "\n", "pscore", ":", "np", ".", "ndarray", ",", "\n", "position", ":", "np", ".", "ndarray", ",", "\n", "action_context", ":", "np", ".", "ndarray", ",", "\n", "n_actions", ":", "int", ",", "\n", "len_list", ":", "int", ",", "\n", "fitting_method", ":", "str", ",", "\n", "base_model", ":", "BaseEstimator", ",", "\n", "action_dist", ":", "np", ".", "ndarray", ",", "\n", "n_folds", ":", "int", ",", "\n", "random_state", ":", "int", ",", "\n", "description", ":", "str", ",", "\n", ")", "->", "None", ":", "\n", "# fit_predict", "\n", "    ", "regression_model", "=", "RegressionModel", "(", "\n", "n_actions", "=", "n_actions", ",", "\n", "len_list", "=", "len_list", ",", "\n", "action_context", "=", "action_context", ",", "\n", "base_model", "=", "base_model", ",", "\n", "fitting_method", "=", "fitting_method", ",", "\n", ")", "\n", "if", "fitting_method", "==", "\"normal\"", ":", "\n", "# train regression model on logged bandit feedback data", "\n", "        ", "_", "=", "regression_model", ".", "fit_predict", "(", "\n", "context", "=", "context", ",", "\n", "action", "=", "action", ",", "\n", "reward", "=", "reward", ",", "\n", "position", "=", "position", ",", "\n", "n_folds", "=", "n_folds", ",", "\n", "random_state", "=", "random_state", ",", "\n", ")", "\n", "", "else", ":", "\n", "# train regression model on logged bandit feedback data", "\n", "        ", "_", "=", "regression_model", ".", "fit_predict", "(", "\n", "context", "=", "context", ",", "\n", "action", "=", "action", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore", "=", "pscore", ",", "\n", "position", "=", "position", ",", "\n", "action_dist", "=", "action_dist", ",", "\n", "n_folds", "=", "n_folds", ",", "\n", "random_state", "=", "random_state", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_regression_models.test_performance_of_binary_outcome_models": [[852, 925], ["fixed_synthetic_bandit_feedback.copy", "numpy.average", "np.average.mean", "print", "binary_model_dict.items", "print", "obp.ope.RegressionModel", "sklearn.metrics.roc_auc_score", "print", "obp.ope.RegressionModel.fit_predict", "obp.ope.RegressionModel.fit_predict", "numpy.abs", "numpy.abs", "numpy.mean", "model", "numpy.mean", "numpy.arange", "numpy.zeros_like"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.classification_model.PropensityScoreEstimator.fit_predict", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.classification_model.PropensityScoreEstimator.fit_predict"], ["", "", "def", "test_performance_of_binary_outcome_models", "(", "\n", "fixed_synthetic_bandit_feedback", ":", "BanditFeedback", ",", "random_action_dist", ":", "np", ".", "ndarray", "\n", ")", "->", "None", ":", "\n", "    ", "\"\"\"\n    Test the performance of ope estimators using synthetic bandit data and random evaluation policy\n    when the regression model is estimated by a logistic regression\n    \"\"\"", "\n", "bandit_feedback", "=", "fixed_synthetic_bandit_feedback", ".", "copy", "(", ")", "\n", "expected_reward", "=", "bandit_feedback", "[", "\"expected_reward\"", "]", "[", ":", ",", ":", ",", "np", ".", "newaxis", "]", "\n", "action_dist", "=", "random_action_dist", "\n", "# compute ground truth policy value using expected reward", "\n", "q_pi_e", "=", "np", ".", "average", "(", "expected_reward", "[", ":", ",", ":", ",", "0", "]", ",", "weights", "=", "action_dist", "[", ":", ",", ":", ",", "0", "]", ",", "axis", "=", "1", ")", "\n", "# compute statistics of ground truth policy value", "\n", "gt_mean", "=", "q_pi_e", ".", "mean", "(", ")", "\n", "random_state", "=", "12345", "\n", "auc_scores", ":", "Dict", "[", "str", ",", "float", "]", "=", "{", "}", "\n", "# check ground truth", "\n", "print", "(", "f\"gt_mean: {gt_mean}\"", ")", "\n", "# check the performance of regression models using doubly robust criterion (|\\hat{q} - q| <= |q| is satisfied with a high probability)", "\n", "dr_criterion_pass_rate", "=", "0.7", "\n", "fit_methods", "=", "[", "\"normal\"", ",", "\"iw\"", ",", "\"mrdr\"", "]", "\n", "for", "fit_method", "in", "fit_methods", ":", "\n", "        ", "for", "model_name", ",", "model", "in", "binary_model_dict", ".", "items", "(", ")", ":", "\n", "            ", "regression_model", "=", "RegressionModel", "(", "\n", "n_actions", "=", "bandit_feedback", "[", "\"n_actions\"", "]", ",", "\n", "action_context", "=", "bandit_feedback", "[", "\"action_context\"", "]", ",", "\n", "base_model", "=", "model", "(", "**", "hyperparams", "[", "model_name", "]", ")", ",", "\n", "fitting_method", "=", "fit_method", ",", "\n", ")", "\n", "if", "fit_method", "==", "\"normal\"", ":", "\n", "# train regression model on logged bandit feedback data", "\n", "                ", "estimated_rewards_by_reg_model", "=", "regression_model", ".", "fit_predict", "(", "\n", "context", "=", "bandit_feedback", "[", "\"context\"", "]", ",", "\n", "action", "=", "bandit_feedback", "[", "\"action\"", "]", ",", "\n", "reward", "=", "bandit_feedback", "[", "\"reward\"", "]", ",", "\n", "n_folds", "=", "3", ",", "# 3-fold cross-fitting", "\n", "random_state", "=", "random_state", ",", "\n", ")", "\n", "", "else", ":", "\n", "# train regression model on logged bandit feedback data", "\n", "                ", "estimated_rewards_by_reg_model", "=", "regression_model", ".", "fit_predict", "(", "\n", "context", "=", "bandit_feedback", "[", "\"context\"", "]", ",", "\n", "action", "=", "bandit_feedback", "[", "\"action\"", "]", ",", "\n", "reward", "=", "bandit_feedback", "[", "\"reward\"", "]", ",", "\n", "pscore", "=", "bandit_feedback", "[", "\"pscore\"", "]", ",", "\n", "action_dist", "=", "action_dist", ",", "\n", "n_folds", "=", "3", ",", "# 3-fold cross-fitting", "\n", "random_state", "=", "random_state", ",", "\n", ")", "\n", "", "auc_scores", "[", "model_name", "+", "\"_\"", "+", "fit_method", "]", "=", "roc_auc_score", "(", "\n", "y_true", "=", "bandit_feedback", "[", "\"reward\"", "]", ",", "\n", "y_score", "=", "estimated_rewards_by_reg_model", "[", "\n", "np", ".", "arange", "(", "bandit_feedback", "[", "\"reward\"", "]", ".", "shape", "[", "0", "]", ")", ",", "\n", "bandit_feedback", "[", "\"action\"", "]", ",", "\n", "np", ".", "zeros_like", "(", "bandit_feedback", "[", "\"action\"", "]", ",", "dtype", "=", "int", ")", ",", "\n", "]", ",", "\n", ")", "\n", "# compare dr criterion", "\n", "dr_criterion", "=", "np", ".", "abs", "(", "(", "gt_mean", "-", "estimated_rewards_by_reg_model", ")", ")", "-", "np", ".", "abs", "(", "\n", "gt_mean", "\n", ")", "\n", "print", "(", "\n", "f\"Dr criterion is satisfied with probability {np.mean(dr_criterion <= 0)} ------ model: {model_name} ({fit_method}),\"", "\n", ")", "\n", "assert", "(", "\n", "np", ".", "mean", "(", "dr_criterion", "<=", "0", ")", ">=", "dr_criterion_pass_rate", "\n", ")", ",", "f\" should be satisfied with a probability at least {dr_criterion_pass_rate}\"", "\n", "\n", "", "", "for", "model_name", "in", "auc_scores", ":", "\n", "        ", "print", "(", "f\"AUC of {model_name} is {auc_scores[model_name]}\"", ")", "\n", "assert", "(", "\n", "auc_scores", "[", "model_name", "]", ">", "0.5", "\n", ")", ",", "f\"AUC of {model_name} should be greater than 0.5\"", "\n", "", "", ""]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_dr_estimators_continuous.test_synthetic_init": [[11, 28], ["pytest.raises", "obp.ope.KernelizedDoublyRobust", "pytest.raises", "obp.ope.KernelizedDoublyRobust", "pytest.raises", "obp.ope.KernelizedDoublyRobust", "pytest.raises", "obp.ope.KernelizedDoublyRobust", "pytest.raises", "obp.ope.KernelizedDoublyRobust"], "function", ["None"], ["def", "test_synthetic_init", "(", ")", ":", "\n", "# kernel", "\n", "    ", "with", "pytest", ".", "raises", "(", "ValueError", ")", ":", "\n", "        ", "KernelizedDoublyRobust", "(", "kernel", "=", "\"a\"", ",", "bandwidth", "=", "0.1", ")", "\n", "\n", "", "with", "pytest", ".", "raises", "(", "ValueError", ")", ":", "\n", "        ", "KernelizedDoublyRobust", "(", "kernel", "=", "None", ",", "bandwidth", "=", "0.1", ")", "\n", "\n", "# bandwidth", "\n", "", "with", "pytest", ".", "raises", "(", "TypeError", ")", ":", "\n", "        ", "KernelizedDoublyRobust", "(", "kernel", "=", "\"gaussian\"", ",", "bandwidth", "=", "\"a\"", ")", "\n", "\n", "", "with", "pytest", ".", "raises", "(", "TypeError", ")", ":", "\n", "        ", "KernelizedDoublyRobust", "(", "kernel", "=", "\"gaussian\"", ",", "bandwidth", "=", "None", ")", "\n", "\n", "", "with", "pytest", ".", "raises", "(", "ValueError", ")", ":", "\n", "        ", "KernelizedDoublyRobust", "(", "kernel", "=", "\"gaussian\"", ",", "bandwidth", "=", "-", "1.0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_dr_estimators_continuous.test_dr_continuous_using_invalid_input_data": [[160, 187], ["pytest.mark.parametrize", "pytest.raises", "dr.estimate_policy_value", "pytest.raises", "dr.estimate_interval"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_policy_value", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_interval"], ["@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"action_by_evaluation_policy, estimated_rewards_by_reg_model, action_by_behavior_policy, reward, pscore, description\"", ",", "\n", "invalid_input_of_dr", ",", "\n", ")", "\n", "def", "test_dr_continuous_using_invalid_input_data", "(", "\n", "action_by_evaluation_policy", ",", "\n", "estimated_rewards_by_reg_model", ",", "\n", "action_by_behavior_policy", ",", "\n", "reward", ",", "\n", "pscore", ",", "\n", "description", ":", "str", ",", "\n", ")", "->", "None", ":", "\n", "    ", "with", "pytest", ".", "raises", "(", "ValueError", ",", "match", "=", "f\"{description}*\"", ")", ":", "\n", "        ", "_", "=", "dr", ".", "estimate_policy_value", "(", "\n", "action_by_evaluation_policy", "=", "action_by_evaluation_policy", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", "action_by_behavior_policy", "=", "action_by_behavior_policy", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore", "=", "pscore", ",", "\n", ")", "\n", "", "with", "pytest", ".", "raises", "(", "ValueError", ",", "match", "=", "f\"{description}*\"", ")", ":", "\n", "        ", "_", "=", "dr", ".", "estimate_interval", "(", "\n", "action_by_evaluation_policy", "=", "action_by_evaluation_policy", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", "action_by_behavior_policy", "=", "action_by_behavior_policy", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore", "=", "pscore", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_dr_estimators_continuous.test_dr_continuous_using_valid_input_data": [[204, 221], ["pytest.mark.parametrize", "dr.estimate_policy_value"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_policy_value"], ["@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"action_by_evaluation_policy, estimated_rewards_by_reg_model, action_by_behavior_policy, reward, pscore\"", ",", "\n", "valid_input_of_dr", ",", "\n", ")", "\n", "def", "test_dr_continuous_using_valid_input_data", "(", "\n", "action_by_evaluation_policy", ",", "\n", "estimated_rewards_by_reg_model", ",", "\n", "action_by_behavior_policy", ",", "\n", "reward", ",", "\n", "pscore", ",", "\n", ")", "->", "None", ":", "\n", "    ", "_", "=", "dr", ".", "estimate_policy_value", "(", "\n", "action_by_evaluation_policy", "=", "action_by_evaluation_policy", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", "action_by_behavior_policy", "=", "action_by_behavior_policy", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore", "=", "pscore", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_dr_estimators_continuous.test_estimate_intervals_of_all_estimators_using_invalid_input_data": [[259, 289], ["pytest.mark.parametrize", "pytest.mark.parametrize", "pytest.raises", "dr.estimate_interval"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_interval"], ["@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"action_by_evaluation_policy, estimated_rewards_by_reg_model, action_by_behavior_policy, reward, pscore\"", ",", "\n", "valid_input_of_dr", ",", "\n", ")", "\n", "@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"alpha, n_bootstrap_samples, random_state, err, description\"", ",", "\n", "invalid_input_of_estimate_intervals", ",", "\n", ")", "\n", "def", "test_estimate_intervals_of_all_estimators_using_invalid_input_data", "(", "\n", "action_by_evaluation_policy", ",", "\n", "estimated_rewards_by_reg_model", ",", "\n", "action_by_behavior_policy", ",", "\n", "reward", ",", "\n", "pscore", ",", "\n", "alpha", ",", "\n", "n_bootstrap_samples", ",", "\n", "random_state", ",", "\n", "err", ",", "\n", "description", ",", "\n", ")", "->", "None", ":", "\n", "    ", "with", "pytest", ".", "raises", "(", "err", ",", "match", "=", "f\"{description}*\"", ")", ":", "\n", "        ", "_", "=", "dr", ".", "estimate_interval", "(", "\n", "action_by_evaluation_policy", "=", "action_by_evaluation_policy", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", "action_by_behavior_policy", "=", "action_by_behavior_policy", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore", "=", "pscore", ",", "\n", "alpha", "=", "alpha", ",", "\n", "n_bootstrap_samples", "=", "n_bootstrap_samples", ",", "\n", "random_state", "=", "random_state", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_dr_estimators_continuous.test_estimate_intervals_of_all_estimators_using_valid_input_data": [[292, 320], ["pytest.mark.parametrize", "pytest.mark.parametrize", "dr.estimate_interval"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_interval"], ["", "", "@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"action_by_evaluation_policy, estimated_rewards_by_reg_model, action_by_behavior_policy, reward, pscore\"", ",", "\n", "valid_input_of_dr", ",", "\n", ")", "\n", "@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"alpha, n_bootstrap_samples, random_state, description\"", ",", "\n", "valid_input_of_estimate_intervals", ",", "\n", ")", "\n", "def", "test_estimate_intervals_of_all_estimators_using_valid_input_data", "(", "\n", "action_by_evaluation_policy", ",", "\n", "estimated_rewards_by_reg_model", ",", "\n", "action_by_behavior_policy", ",", "\n", "reward", ",", "\n", "pscore", ",", "\n", "alpha", ",", "\n", "n_bootstrap_samples", ",", "\n", "random_state", ",", "\n", "description", ",", "\n", ")", "->", "None", ":", "\n", "    ", "_", "=", "dr", ".", "estimate_interval", "(", "\n", "action_by_evaluation_policy", "=", "action_by_evaluation_policy", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", "action_by_behavior_policy", "=", "action_by_behavior_policy", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore", "=", "pscore", ",", "\n", "alpha", "=", "alpha", ",", "\n", "n_bootstrap_samples", "=", "n_bootstrap_samples", ",", "\n", "random_state", "=", "random_state", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_dr_estimators_continuous.test_continuous_ope_performance": [[323, 387], ["pytest.mark.parametrize", "obp.ope.KernelizedDoublyRobust", "obp.dataset.SyntheticContinuousBanditDataset", "obp.dataset.SyntheticContinuousBanditDataset.obtain_batch_bandit_feedback", "obp.dataset.linear_synthetic_policy_continuous", "obp.dataset.linear_reward_funcion_continuous", "obp.dataset.linear_reward_funcion_continuous.mean", "print", "obp.ope.KernelizedDoublyRobust.estimate_policy_value", "print", "numpy.abs"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.multiclass.MultiClassToBanditReduction.obtain_batch_bandit_feedback", "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.synthetic_continuous.linear_synthetic_policy_continuous", "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.synthetic_continuous.linear_reward_funcion_continuous", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_policy_value"], ["", "@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"kernel\"", ",", "\n", "[", "\"triangular\"", ",", "\"gaussian\"", ",", "\"epanechnikov\"", ",", "\"cosine\"", "]", ",", "\n", ")", "\n", "def", "test_continuous_ope_performance", "(", "kernel", ")", ":", "\n", "# define dr instances", "\n", "    ", "dr", "=", "KernelizedDoublyRobust", "(", "kernel", "=", "kernel", ",", "bandwidth", "=", "0.1", ")", "\n", "# set parameters", "\n", "dim_context", "=", "2", "\n", "reward_noise", "=", "0.1", "\n", "random_state", "=", "12345", "\n", "n_rounds", "=", "10000", "\n", "min_action_value", "=", "-", "10", "\n", "max_action_value", "=", "10", "\n", "behavior_policy_function", "=", "linear_behavior_policy_continuous", "\n", "reward_function", "=", "linear_reward_funcion_continuous", "\n", "dataset", "=", "SyntheticContinuousBanditDataset", "(", "\n", "dim_context", "=", "dim_context", ",", "\n", "reward_noise", "=", "reward_noise", ",", "\n", "min_action_value", "=", "min_action_value", ",", "\n", "max_action_value", "=", "max_action_value", ",", "\n", "reward_function", "=", "reward_function", ",", "\n", "behavior_policy_function", "=", "behavior_policy_function", ",", "\n", "random_state", "=", "random_state", ",", "\n", ")", "\n", "# obtain feedback", "\n", "bandit_feedback", "=", "dataset", ".", "obtain_batch_bandit_feedback", "(", "\n", "n_rounds", "=", "n_rounds", ",", "\n", ")", "\n", "context", "=", "bandit_feedback", "[", "\"context\"", "]", "\n", "action_by_evaluation_policy", "=", "linear_synthetic_policy_continuous", "(", "context", ")", "\n", "action_by_behavior_policy", "=", "bandit_feedback", "[", "\"action\"", "]", "\n", "reward", "=", "bandit_feedback", "[", "\"reward\"", "]", "\n", "pscore", "=", "bandit_feedback", "[", "\"pscore\"", "]", "\n", "\n", "# compute statistics of ground truth policy value", "\n", "q_pi_e", "=", "linear_reward_funcion_continuous", "(", "\n", "context", "=", "context", ",", "action", "=", "action_by_evaluation_policy", ",", "random_state", "=", "random_state", "\n", ")", "\n", "true_policy_value", "=", "q_pi_e", ".", "mean", "(", ")", "\n", "print", "(", "f\"true_policy_value: {true_policy_value}\"", ")", "\n", "\n", "# OPE", "\n", "policy_value_estimated_by_dr", "=", "dr", ".", "estimate_policy_value", "(", "\n", "action_by_evaluation_policy", "=", "action_by_evaluation_policy", ",", "\n", "estimated_rewards_by_reg_model", "=", "q_pi_e", ",", "\n", "action_by_behavior_policy", "=", "action_by_behavior_policy", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore", "=", "pscore", ",", "\n", ")", "\n", "\n", "# check the performance of OPE", "\n", "estimated_policy_value", "=", "{", "\n", "\"dr\"", ":", "policy_value_estimated_by_dr", ",", "\n", "}", "\n", "for", "key", "in", "estimated_policy_value", ":", "\n", "        ", "print", "(", "\n", "f\"estimated_value: {estimated_policy_value[key]} ------ estimator: {key}, \"", "\n", ")", "\n", "# test the performance of each estimator", "\n", "assert", "(", "\n", "np", ".", "abs", "(", "true_policy_value", "-", "estimated_policy_value", "[", "key", "]", ")", "/", "true_policy_value", "\n", "<=", "0.1", "\n", ")", ",", "f\"{key} does not work well (relative estimation error is greater than 10%)\"", "\n", "", "", ""]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_ipw_estimators_multi.test_ipw_init_using_invalid_inputs": [[38, 56], ["pytest.mark.parametrize", "pytest.raises", "obp.ope.MultiLoggersNaiveInverseProbabilityWeighting", "pytest.raises", "obp.ope.MultiLoggersBalancedInverseProbabilityWeighting", "pytest.raises", "obp.ope.MultiLoggersWeightedInverseProbabilityWeighting"], "function", ["None"], ["@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"lambda_, use_estimated_pscore, err, description\"", ",", "\n", "invalid_input_of_ipw_init", ",", "\n", ")", "\n", "def", "test_ipw_init_using_invalid_inputs", "(", "\n", "lambda_", ",", "\n", "use_estimated_pscore", ",", "\n", "err", ",", "\n", "description", ",", "\n", ")", ":", "\n", "    ", "with", "pytest", ".", "raises", "(", "err", ",", "match", "=", "f\"{description}*\"", ")", ":", "\n", "        ", "_", "=", "NaiveIPW", "(", "lambda_", "=", "lambda_", ",", "use_estimated_pscore", "=", "use_estimated_pscore", ")", "\n", "\n", "", "with", "pytest", ".", "raises", "(", "err", ",", "match", "=", "f\"{description}*\"", ")", ":", "\n", "        ", "_", "=", "BalIPW", "(", "lambda_", "=", "lambda_", ",", "use_estimated_pscore", "=", "use_estimated_pscore", ")", "\n", "\n", "", "with", "pytest", ".", "raises", "(", "err", ",", "match", "=", "f\"{description}*\"", ")", ":", "\n", "        ", "_", "=", "WeightedIPW", "(", "lambda_", "=", "lambda_", ",", "use_estimated_pscore", "=", "use_estimated_pscore", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_ipw_estimators_multi.test_weighted_ipw_using_invalid_input_data": [[283, 319], ["pytest.mark.parametrize", "obp.ope.MultiLoggersWeightedInverseProbabilityWeighting", "pytest.raises", "obp.ope.MultiLoggersWeightedInverseProbabilityWeighting.estimate_policy_value", "pytest.raises", "obp.ope.MultiLoggersWeightedInverseProbabilityWeighting.estimate_interval"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_policy_value", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_interval"], ["@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"action_dist, action, reward, pscore, stratum_idx, position, use_estimated_pscore, estimated_pscore, description\"", ",", "\n", "invalid_input_of_weighted_ipw", ",", "\n", ")", "\n", "def", "test_weighted_ipw_using_invalid_input_data", "(", "\n", "action_dist", ":", "np", ".", "ndarray", ",", "\n", "action", ":", "np", ".", "ndarray", ",", "\n", "reward", ":", "np", ".", "ndarray", ",", "\n", "pscore", ":", "np", ".", "ndarray", ",", "\n", "stratum_idx", ":", "np", ".", "ndarray", ",", "\n", "position", ":", "np", ".", "ndarray", ",", "\n", "use_estimated_pscore", ":", "bool", ",", "\n", "estimated_pscore", ":", "np", ".", "ndarray", ",", "\n", "description", ":", "str", ",", "\n", ")", "->", "None", ":", "\n", "# prepare ipw instances", "\n", "    ", "weighted_ipw", "=", "WeightedIPW", "(", "use_estimated_pscore", "=", "use_estimated_pscore", ")", "\n", "with", "pytest", ".", "raises", "(", "ValueError", ",", "match", "=", "f\"{description}*\"", ")", ":", "\n", "        ", "_", "=", "weighted_ipw", ".", "estimate_policy_value", "(", "\n", "action_dist", "=", "action_dist", ",", "\n", "action", "=", "action", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore", "=", "pscore", ",", "\n", "stratum_idx", "=", "stratum_idx", ",", "\n", "position", "=", "position", ",", "\n", "estimated_pscore", "=", "estimated_pscore", ",", "\n", ")", "\n", "", "with", "pytest", ".", "raises", "(", "ValueError", ",", "match", "=", "f\"{description}*\"", ")", ":", "\n", "        ", "_", "=", "weighted_ipw", ".", "estimate_interval", "(", "\n", "action_dist", "=", "action_dist", ",", "\n", "action", "=", "action", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore", "=", "pscore", ",", "\n", "stratum_idx", "=", "stratum_idx", ",", "\n", "position", "=", "position", ",", "\n", "estimated_pscore", "=", "estimated_pscore", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_ipw_estimators_multi.test_naive_ipw_using_invalid_input_data": [[497, 530], ["pytest.mark.parametrize", "obp.ope.MultiLoggersNaiveInverseProbabilityWeighting", "pytest.raises", "obp.ope.MultiLoggersNaiveInverseProbabilityWeighting.estimate_policy_value", "pytest.raises", "obp.ope.MultiLoggersNaiveInverseProbabilityWeighting.estimate_interval"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_policy_value", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_interval"], ["@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"action_dist, action, reward, pscore, position, use_estimated_pscore, estimated_pscore, description\"", ",", "\n", "invalid_input_of_naive_ipw", ",", "\n", ")", "\n", "def", "test_naive_ipw_using_invalid_input_data", "(", "\n", "action_dist", ":", "np", ".", "ndarray", ",", "\n", "action", ":", "np", ".", "ndarray", ",", "\n", "reward", ":", "np", ".", "ndarray", ",", "\n", "pscore", ":", "np", ".", "ndarray", ",", "\n", "position", ":", "np", ".", "ndarray", ",", "\n", "use_estimated_pscore", ":", "bool", ",", "\n", "estimated_pscore", ":", "np", ".", "ndarray", ",", "\n", "description", ":", "str", ",", "\n", ")", "->", "None", ":", "\n", "# prepare ipw instances", "\n", "    ", "naive_ipw", "=", "NaiveIPW", "(", "use_estimated_pscore", "=", "use_estimated_pscore", ")", "\n", "with", "pytest", ".", "raises", "(", "ValueError", ",", "match", "=", "f\"{description}*\"", ")", ":", "\n", "        ", "_", "=", "naive_ipw", ".", "estimate_policy_value", "(", "\n", "action_dist", "=", "action_dist", ",", "\n", "action", "=", "action", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore", "=", "pscore", ",", "\n", "position", "=", "position", ",", "\n", "estimated_pscore", "=", "estimated_pscore", ",", "\n", ")", "\n", "", "with", "pytest", ".", "raises", "(", "ValueError", ",", "match", "=", "f\"{description}*\"", ")", ":", "\n", "        ", "_", "=", "naive_ipw", ".", "estimate_interval", "(", "\n", "action_dist", "=", "action_dist", ",", "\n", "action", "=", "action", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore", "=", "pscore", ",", "\n", "position", "=", "position", ",", "\n", "estimated_pscore", "=", "estimated_pscore", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_ipw_estimators_multi.test_bal_ipw_using_invalid_input_data": [[708, 741], ["pytest.mark.parametrize", "obp.ope.MultiLoggersBalancedInverseProbabilityWeighting", "pytest.raises", "obp.ope.MultiLoggersBalancedInverseProbabilityWeighting.estimate_policy_value", "pytest.raises", "obp.ope.MultiLoggersBalancedInverseProbabilityWeighting.estimate_interval"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_policy_value", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_interval"], ["@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"action_dist, action, reward, pscore_avg, position, use_estimated_pscore, estimated_pscore, description\"", ",", "\n", "invalid_input_of_bal_ipw", ",", "\n", ")", "\n", "def", "test_bal_ipw_using_invalid_input_data", "(", "\n", "action_dist", ":", "np", ".", "ndarray", ",", "\n", "action", ":", "np", ".", "ndarray", ",", "\n", "reward", ":", "np", ".", "ndarray", ",", "\n", "pscore_avg", ":", "np", ".", "ndarray", ",", "\n", "position", ":", "np", ".", "ndarray", ",", "\n", "use_estimated_pscore", ":", "bool", ",", "\n", "estimated_pscore", ":", "np", ".", "ndarray", ",", "\n", "description", ":", "str", ",", "\n", ")", "->", "None", ":", "\n", "# prepare ipw instances", "\n", "    ", "bal_ipw", "=", "BalIPW", "(", "use_estimated_pscore", "=", "use_estimated_pscore", ")", "\n", "with", "pytest", ".", "raises", "(", "ValueError", ",", "match", "=", "f\"{description}*\"", ")", ":", "\n", "        ", "_", "=", "bal_ipw", ".", "estimate_policy_value", "(", "\n", "action_dist", "=", "action_dist", ",", "\n", "action", "=", "action", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore_avg", "=", "pscore_avg", ",", "\n", "position", "=", "position", ",", "\n", "estimated_pscore_avg", "=", "estimated_pscore", ",", "\n", ")", "\n", "", "with", "pytest", ".", "raises", "(", "ValueError", ",", "match", "=", "f\"{description}*\"", ")", ":", "\n", "        ", "_", "=", "bal_ipw", ".", "estimate_interval", "(", "\n", "action_dist", "=", "action_dist", ",", "\n", "action", "=", "action", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore_avg", "=", "pscore_avg", ",", "\n", "position", "=", "position", ",", "\n", "estimated_pscore_avg", "=", "estimated_pscore", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_ipw_estimators_multi.test_ipw_using_random_evaluation_policy": [[744, 793], ["obp.ope.MultiLoggersNaiveInverseProbabilityWeighting", "obp.ope.MultiLoggersBalancedInverseProbabilityWeighting", "obp.ope.MultiLoggersWeightedInverseProbabilityWeighting", "obp.ope.MultiLoggersNaiveInverseProbabilityWeighting", "obp.ope.MultiLoggersBalancedInverseProbabilityWeighting", "obp.ope.MultiLoggersWeightedInverseProbabilityWeighting", "estimator.estimate_policy_value", "isinstance", "estimator.estimate_policy_value", "isinstance", "synthetic_multi_bandit_feedback.items", "pytest.raises", "estimator.estimate_policy_value", "re.escape"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_policy_value", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_policy_value", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_policy_value"], ["", "", "def", "test_ipw_using_random_evaluation_policy", "(", "\n", "synthetic_multi_bandit_feedback", ":", "BanditFeedback", ",", "random_action_dist", ":", "np", ".", "ndarray", "\n", ")", "->", "None", ":", "\n", "    ", "\"\"\"\n    Test the format of ipw variants using synthetic bandit data and random evaluation policy\n    \"\"\"", "\n", "action_dist", "=", "random_action_dist", "\n", "# prepare input dict", "\n", "input_dict", "=", "{", "\n", "k", ":", "v", "\n", "for", "k", ",", "v", "in", "synthetic_multi_bandit_feedback", ".", "items", "(", ")", "\n", "if", "k", "in", "[", "\"reward\"", ",", "\"action\"", ",", "\"pscore\"", ",", "\"pscore_avg\"", ",", "\"stratum_idx\"", ",", "\"position\"", "]", "\n", "}", "\n", "input_dict", "[", "\"action_dist\"", "]", "=", "action_dist", "\n", "naive_ipw", "=", "NaiveIPW", "(", ")", "\n", "bal_ipw", "=", "BalIPW", "(", ")", "\n", "weighted_ipw", "=", "WeightedIPW", "(", ")", "\n", "# ipw estimators can be used without estimated_rewards_by_reg_model", "\n", "for", "estimator", "in", "[", "naive_ipw", ",", "bal_ipw", ",", "weighted_ipw", "]", ":", "\n", "        ", "estimated_policy_value", "=", "estimator", ".", "estimate_policy_value", "(", "**", "input_dict", ")", "\n", "assert", "isinstance", "(", "\n", "estimated_policy_value", ",", "float", "\n", ")", ",", "f\"invalid type response: {estimator}\"", "\n", "\n", "# ipw with estimated pscore", "\n", "", "naive_ipw", "=", "NaiveIPW", "(", "use_estimated_pscore", "=", "True", ")", "\n", "bal_ipw", "=", "BalIPW", "(", "use_estimated_pscore", "=", "True", ")", "\n", "weighted_ipw", "=", "WeightedIPW", "(", "use_estimated_pscore", "=", "True", ")", "\n", "input_dict", "[", "\"estimated_pscore\"", "]", "=", "input_dict", "[", "\"pscore\"", "]", "\n", "input_dict", "[", "\"estimated_pscore_avg\"", "]", "=", "input_dict", "[", "\"pscore\"", "]", "\n", "del", "input_dict", "[", "\"pscore\"", "]", "\n", "del", "input_dict", "[", "\"pscore_avg\"", "]", "\n", "for", "estimator", "in", "[", "naive_ipw", ",", "bal_ipw", ",", "weighted_ipw", "]", ":", "\n", "        ", "estimated_policy_value", "=", "estimator", ".", "estimate_policy_value", "(", "**", "input_dict", ")", "\n", "assert", "isinstance", "(", "\n", "estimated_policy_value", ",", "float", "\n", ")", ",", "f\"invalid type response: {estimator}\"", "\n", "\n", "# remove necessary keys", "\n", "", "del", "input_dict", "[", "\"reward\"", "]", "\n", "del", "input_dict", "[", "\"action\"", "]", "\n", "for", "estimator", "in", "[", "naive_ipw", ",", "weighted_ipw", ",", "bal_ipw", "]", ":", "\n", "        ", "with", "pytest", ".", "raises", "(", "\n", "TypeError", ",", "\n", "match", "=", "re", ".", "escape", "(", "\n", "\"estimate_policy_value() missing 2 required positional arguments: 'reward' and 'action'\"", "\n", ")", ",", "\n", ")", ":", "\n", "            ", "_", "=", "estimator", ".", "estimate_policy_value", "(", "**", "input_dict", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_dr_estimators_multi.test_dr_init_using_invalid_inputs": [[38, 56], ["pytest.mark.parametrize", "pytest.raises", "obp.ope.MultiLoggersNaiveDoublyRobust", "pytest.raises", "obp.ope.MultiLoggersWeightedDoublyRobust", "pytest.raises", "obp.ope.MultiLoggersBalancedDoublyRobust"], "function", ["None"], ["@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"lambda_, use_estimated_pscore, err, description\"", ",", "\n", "invalid_input_of_dr_init", ",", "\n", ")", "\n", "def", "test_dr_init_using_invalid_inputs", "(", "\n", "lambda_", ",", "\n", "use_estimated_pscore", ",", "\n", "err", ",", "\n", "description", ",", "\n", ")", ":", "\n", "    ", "with", "pytest", ".", "raises", "(", "err", ",", "match", "=", "f\"{description}*\"", ")", ":", "\n", "        ", "_", "=", "NaiveDR", "(", "lambda_", "=", "lambda_", ",", "use_estimated_pscore", "=", "use_estimated_pscore", ")", "\n", "\n", "", "with", "pytest", ".", "raises", "(", "err", ",", "match", "=", "f\"{description}*\"", ")", ":", "\n", "        ", "_", "=", "WeightedDR", "(", "lambda_", "=", "lambda_", ",", "use_estimated_pscore", "=", "use_estimated_pscore", ")", "\n", "\n", "", "with", "pytest", ".", "raises", "(", "err", ",", "match", "=", "f\"{description}*\"", ")", ":", "\n", "        ", "_", "=", "BalDR", "(", "lambda_", "=", "lambda_", ",", "use_estimated_pscore", "=", "use_estimated_pscore", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_dr_estimators_multi.test_dr_init_using_valid_input_data": [[65, 73], ["pytest.mark.parametrize", "obp.ope.MultiLoggersNaiveDoublyRobust", "obp.ope.MultiLoggersWeightedDoublyRobust", "obp.ope.MultiLoggersBalancedDoublyRobust"], "function", ["None"], ["@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"lambda_, description\"", ",", "\n", "valid_input_of_dr_init", ",", "\n", ")", "\n", "def", "test_dr_init_using_valid_input_data", "(", "lambda_", ":", "float", ",", "description", ":", "str", ")", "->", "None", ":", "\n", "    ", "_", "=", "NaiveDR", "(", "lambda_", "=", "lambda_", ")", "\n", "_", "=", "WeightedDR", "(", "lambda_", "=", "lambda_", ")", "\n", "_", "=", "BalDR", "(", "lambda_", "=", "lambda_", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_dr_estimators_multi.test_weighted_dr_using_invalid_input_data": [[356, 395], ["pytest.mark.parametrize", "obp.ope.MultiLoggersWeightedDoublyRobust", "pytest.raises", "obp.ope.MultiLoggersWeightedDoublyRobust.estimate_policy_value", "pytest.raises", "obp.ope.MultiLoggersWeightedDoublyRobust.estimate_interval"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_policy_value", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_interval"], ["@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"action_dist, action, reward, pscore, stratum_idx, position, estimated_rewards_by_reg_model, use_estimated_pscore, estimated_pscore, description\"", ",", "\n", "invalid_input_of_weighted_dr", ",", "\n", ")", "\n", "def", "test_weighted_dr_using_invalid_input_data", "(", "\n", "action_dist", ":", "np", ".", "ndarray", ",", "\n", "action", ":", "np", ".", "ndarray", ",", "\n", "reward", ":", "np", ".", "ndarray", ",", "\n", "pscore", ":", "np", ".", "ndarray", ",", "\n", "stratum_idx", ":", "np", ".", "ndarray", ",", "\n", "position", ":", "np", ".", "ndarray", ",", "\n", "estimated_rewards_by_reg_model", ":", "np", ".", "ndarray", ",", "\n", "use_estimated_pscore", ":", "bool", ",", "\n", "estimated_pscore", ":", "np", ".", "ndarray", ",", "\n", "description", ":", "str", ",", "\n", ")", "->", "None", ":", "\n", "    ", "weighted_dr", "=", "WeightedDR", "(", "use_estimated_pscore", "=", "use_estimated_pscore", ")", "\n", "# estimate_intervals function raises ValueError of all estimators", "\n", "with", "pytest", ".", "raises", "(", "ValueError", ",", "match", "=", "f\"{description}*\"", ")", ":", "\n", "        ", "_", "=", "weighted_dr", ".", "estimate_policy_value", "(", "\n", "action_dist", "=", "action_dist", ",", "\n", "action", "=", "action", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore", "=", "pscore", ",", "\n", "stratum_idx", "=", "stratum_idx", ",", "\n", "position", "=", "position", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", "estimated_pscore", "=", "estimated_pscore", ",", "\n", ")", "\n", "", "with", "pytest", ".", "raises", "(", "ValueError", ",", "match", "=", "f\"{description}*\"", ")", ":", "\n", "        ", "_", "=", "weighted_dr", ".", "estimate_interval", "(", "\n", "action_dist", "=", "action_dist", ",", "\n", "action", "=", "action", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore", "=", "pscore", ",", "\n", "stratum_idx", "=", "stratum_idx", ",", "\n", "position", "=", "position", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", "estimated_pscore", "=", "estimated_pscore", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_dr_estimators_multi.test_naive_dr_using_invalid_input_data": [[623, 659], ["pytest.mark.parametrize", "obp.ope.MultiLoggersNaiveDoublyRobust", "pytest.raises", "obp.ope.MultiLoggersNaiveDoublyRobust.estimate_policy_value", "pytest.raises", "obp.ope.MultiLoggersNaiveDoublyRobust.estimate_interval"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_policy_value", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_interval"], ["@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"action_dist, action, reward, pscore, position, estimated_rewards_by_reg_model, use_estimated_pscore, estimated_pscore, description\"", ",", "\n", "invalid_input_of_naive_dr", ",", "\n", ")", "\n", "def", "test_naive_dr_using_invalid_input_data", "(", "\n", "action_dist", ":", "np", ".", "ndarray", ",", "\n", "action", ":", "np", ".", "ndarray", ",", "\n", "reward", ":", "np", ".", "ndarray", ",", "\n", "pscore", ":", "np", ".", "ndarray", ",", "\n", "position", ":", "np", ".", "ndarray", ",", "\n", "estimated_rewards_by_reg_model", ":", "np", ".", "ndarray", ",", "\n", "use_estimated_pscore", ":", "bool", ",", "\n", "estimated_pscore", ":", "np", ".", "ndarray", ",", "\n", "description", ":", "str", ",", "\n", ")", "->", "None", ":", "\n", "    ", "naive_dr", "=", "NaiveDR", "(", "use_estimated_pscore", "=", "use_estimated_pscore", ")", "\n", "# estimate_intervals function raises ValueError of all estimators", "\n", "with", "pytest", ".", "raises", "(", "ValueError", ",", "match", "=", "f\"{description}*\"", ")", ":", "\n", "        ", "_", "=", "naive_dr", ".", "estimate_policy_value", "(", "\n", "action_dist", "=", "action_dist", ",", "\n", "action", "=", "action", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore", "=", "pscore", ",", "\n", "position", "=", "position", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", "estimated_pscore", "=", "estimated_pscore", ",", "\n", ")", "\n", "", "with", "pytest", ".", "raises", "(", "ValueError", ",", "match", "=", "f\"{description}*\"", ")", ":", "\n", "        ", "_", "=", "naive_dr", ".", "estimate_interval", "(", "\n", "action_dist", "=", "action_dist", ",", "\n", "action", "=", "action", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore", "=", "pscore", ",", "\n", "position", "=", "position", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", "estimated_pscore", "=", "estimated_pscore", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_dr_estimators_multi.test_bal_dr_using_invalid_input_data": [[887, 923], ["pytest.mark.parametrize", "obp.ope.MultiLoggersBalancedDoublyRobust", "pytest.raises", "obp.ope.MultiLoggersBalancedDoublyRobust.estimate_policy_value", "pytest.raises", "obp.ope.MultiLoggersBalancedDoublyRobust.estimate_interval"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_policy_value", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_interval"], ["@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"action_dist, action, reward, pscore_avg, position, estimated_rewards_by_reg_model, use_estimated_pscore, estimated_pscore, description\"", ",", "\n", "invalid_input_of_bal_dr", ",", "\n", ")", "\n", "def", "test_bal_dr_using_invalid_input_data", "(", "\n", "action_dist", ":", "np", ".", "ndarray", ",", "\n", "action", ":", "np", ".", "ndarray", ",", "\n", "reward", ":", "np", ".", "ndarray", ",", "\n", "pscore_avg", ":", "np", ".", "ndarray", ",", "\n", "position", ":", "np", ".", "ndarray", ",", "\n", "estimated_rewards_by_reg_model", ":", "np", ".", "ndarray", ",", "\n", "use_estimated_pscore", ":", "bool", ",", "\n", "estimated_pscore", ":", "np", ".", "ndarray", ",", "\n", "description", ":", "str", ",", "\n", ")", "->", "None", ":", "\n", "    ", "bal_dr", "=", "BalDR", "(", "use_estimated_pscore", "=", "use_estimated_pscore", ")", "\n", "# estimate_intervals function raises ValueError of all estimators", "\n", "with", "pytest", ".", "raises", "(", "ValueError", ",", "match", "=", "f\"{description}*\"", ")", ":", "\n", "        ", "_", "=", "bal_dr", ".", "estimate_policy_value", "(", "\n", "action_dist", "=", "action_dist", ",", "\n", "action", "=", "action", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore_avg", "=", "pscore_avg", ",", "\n", "position", "=", "position", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", "estimated_pscore_avg", "=", "estimated_pscore", ",", "\n", ")", "\n", "", "with", "pytest", ".", "raises", "(", "ValueError", ",", "match", "=", "f\"{description}*\"", ")", ":", "\n", "        ", "_", "=", "bal_dr", ".", "estimate_interval", "(", "\n", "action_dist", "=", "action_dist", ",", "\n", "action", "=", "action", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore_avg", "=", "pscore_avg", ",", "\n", "position", "=", "position", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", "estimated_pscore_avg", "=", "estimated_pscore", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_dr_estimators_multi.test_dr_using_random_evaluation_policy": [[926, 978], ["obp.ope.MultiLoggersNaiveDoublyRobust", "obp.ope.MultiLoggersBalancedDoublyRobust", "obp.ope.MultiLoggersWeightedDoublyRobust", "obp.ope.MultiLoggersNaiveDoublyRobust", "obp.ope.MultiLoggersBalancedDoublyRobust", "obp.ope.MultiLoggersWeightedDoublyRobust", "estimator.estimate_policy_value", "isinstance", "estimator.estimate_policy_value", "isinstance", "synthetic_multi_bandit_feedback.items", "pytest.raises", "estimator.estimate_policy_value", "re.escape"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_policy_value", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_policy_value", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_policy_value"], ["", "", "def", "test_dr_using_random_evaluation_policy", "(", "\n", "synthetic_multi_bandit_feedback", ":", "BanditFeedback", ",", "random_action_dist", ":", "np", ".", "ndarray", "\n", ")", "->", "None", ":", "\n", "    ", "\"\"\"\n    Test the format of dr variants using synthetic bandit data and random evaluation policy\n    \"\"\"", "\n", "expected_reward", "=", "synthetic_multi_bandit_feedback", "[", "\"expected_reward\"", "]", "[", "\n", ":", ",", ":", ",", "np", ".", "newaxis", "\n", "]", "\n", "action_dist", "=", "random_action_dist", "\n", "# prepare input dict", "\n", "input_dict", "=", "{", "\n", "k", ":", "v", "\n", "for", "k", ",", "v", "in", "synthetic_multi_bandit_feedback", ".", "items", "(", ")", "\n", "if", "k", "in", "[", "\"reward\"", ",", "\"action\"", ",", "\"pscore\"", ",", "\"pscore_avg\"", ",", "\"stratum_idx\"", ",", "\"position\"", "]", "\n", "}", "\n", "input_dict", "[", "\"action_dist\"", "]", "=", "action_dist", "\n", "input_dict", "[", "\"estimated_rewards_by_reg_model\"", "]", "=", "expected_reward", "\n", "naive_dr", "=", "NaiveDR", "(", ")", "\n", "bal_dr", "=", "BalDR", "(", ")", "\n", "weighted_dr", "=", "WeightedDR", "(", ")", "\n", "# dr estimators require all arguments", "\n", "for", "estimator", "in", "[", "naive_dr", ",", "bal_dr", ",", "weighted_dr", "]", ":", "\n", "        ", "estimated_policy_value", "=", "estimator", ".", "estimate_policy_value", "(", "**", "input_dict", ")", "\n", "assert", "isinstance", "(", "\n", "estimated_policy_value", ",", "float", "\n", ")", ",", "f\"invalid type response: {estimator}\"", "\n", "\n", "", "naive_dr", "=", "NaiveDR", "(", "use_estimated_pscore", "=", "True", ")", "\n", "bal_dr", "=", "BalDR", "(", "use_estimated_pscore", "=", "True", ")", "\n", "weighted_dr", "=", "WeightedDR", "(", "use_estimated_pscore", "=", "True", ")", "\n", "input_dict", "[", "\"estimated_pscore\"", "]", "=", "input_dict", "[", "\"pscore\"", "]", "\n", "input_dict", "[", "\"estimated_pscore_avg\"", "]", "=", "input_dict", "[", "\"pscore\"", "]", "\n", "# dr estimators require all arguments", "\n", "for", "estimator", "in", "[", "naive_dr", ",", "bal_dr", ",", "weighted_dr", "]", ":", "\n", "        ", "estimated_policy_value", "=", "estimator", ".", "estimate_policy_value", "(", "**", "input_dict", ")", "\n", "assert", "isinstance", "(", "\n", "estimated_policy_value", ",", "float", "\n", ")", ",", "f\"invalid type response: {estimator}\"", "\n", "\n", "# remove necessary keys", "\n", "", "del", "input_dict", "[", "\"reward\"", "]", "\n", "del", "input_dict", "[", "\"action\"", "]", "\n", "del", "input_dict", "[", "\"estimated_rewards_by_reg_model\"", "]", "\n", "for", "estimator", "in", "[", "naive_dr", ",", "bal_dr", ",", "weighted_dr", "]", ":", "\n", "        ", "with", "pytest", ".", "raises", "(", "\n", "TypeError", ",", "\n", "match", "=", "re", ".", "escape", "(", "\n", "\"estimate_policy_value() missing 3 required positional arguments: 'reward', 'action', and 'estimated_rewards_by_reg_model'\"", "\n", ")", ",", "\n", ")", ":", "\n", "            ", "_", "=", "estimator", ".", "estimate_policy_value", "(", "**", "input_dict", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_importance_weight_estimator.test_initializing_importance_weight_estimator_using_invalid_input_data": [[620, 643], ["pytest.mark.parametrize", "pytest.raises", "obp.ope.ImportanceWeightEstimator"], "function", ["None"], ["@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"action_context, n_actions, len_list, fitting_method, base_model, calibration_cv, err, description\"", ",", "\n", "invalid_input_of_initializing_importance_weight_estimator", ",", "\n", ")", "\n", "def", "test_initializing_importance_weight_estimator_using_invalid_input_data", "(", "\n", "action_context", ":", "np", ".", "ndarray", ",", "\n", "n_actions", ":", "int", ",", "\n", "len_list", ":", "int", ",", "\n", "fitting_method", ":", "str", ",", "\n", "base_model", ":", "BaseEstimator", ",", "\n", "calibration_cv", ":", "int", ",", "\n", "err", ",", "\n", "description", ":", "str", ",", "\n", ")", "->", "None", ":", "\n", "# initialization raises ValueError", "\n", "    ", "with", "pytest", ".", "raises", "(", "err", ",", "match", "=", "f\"{description}*\"", ")", ":", "\n", "        ", "_", "=", "ImportanceWeightEstimator", "(", "\n", "n_actions", "=", "n_actions", ",", "\n", "len_list", "=", "len_list", ",", "\n", "action_context", "=", "action_context", ",", "\n", "base_model", "=", "base_model", ",", "\n", "fitting_method", "=", "fitting_method", ",", "\n", "calibration_cv", "=", "calibration_cv", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_importance_weight_estimator.test_fitting_importance_weight_estimator_using_invalid_input_data": [[646, 684], ["pytest.mark.parametrize", "pytest.raises", "obp.ope.ImportanceWeightEstimator", "obp.ope.ImportanceWeightEstimator.fit_predict"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.classification_model.PropensityScoreEstimator.fit_predict"], ["", "", "@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"context, action, position, action_context, n_actions, len_list, fitting_method, base_model, action_dist, n_folds, random_state, calibration_cv, err, description\"", ",", "\n", "invalid_input_of_fitting_importance_weight_estimator", ",", "\n", ")", "\n", "def", "test_fitting_importance_weight_estimator_using_invalid_input_data", "(", "\n", "context", ":", "np", ".", "ndarray", ",", "\n", "action", ":", "np", ".", "ndarray", ",", "\n", "position", ":", "np", ".", "ndarray", ",", "\n", "action_context", ":", "np", ".", "ndarray", ",", "\n", "n_actions", ":", "int", ",", "\n", "len_list", ":", "int", ",", "\n", "fitting_method", ":", "str", ",", "\n", "base_model", ":", "BaseEstimator", ",", "\n", "action_dist", ":", "np", ".", "ndarray", ",", "\n", "n_folds", ":", "int", ",", "\n", "random_state", ":", "int", ",", "\n", "calibration_cv", ":", "int", ",", "\n", "err", ",", "\n", "description", ":", "str", ",", "\n", ")", "->", "None", ":", "\n", "# fit_predict function raises ValueError", "\n", "    ", "with", "pytest", ".", "raises", "(", "err", ",", "match", "=", "f\"{description}*\"", ")", ":", "\n", "        ", "importance_weight_estimator", "=", "ImportanceWeightEstimator", "(", "\n", "n_actions", "=", "n_actions", ",", "\n", "len_list", "=", "len_list", ",", "\n", "action_context", "=", "action_context", ",", "\n", "base_model", "=", "base_model", ",", "\n", "fitting_method", "=", "fitting_method", ",", "\n", "calibration_cv", "=", "calibration_cv", ",", "\n", ")", "\n", "# train importance weight estimator on logged bandit feedback data", "\n", "_", "=", "importance_weight_estimator", ".", "fit_predict", "(", "\n", "context", "=", "context", ",", "\n", "action", "=", "action", ",", "\n", "position", "=", "position", ",", "\n", "n_folds", "=", "n_folds", ",", "\n", "random_state", "=", "random_state", ",", "\n", "action_dist", "=", "action_dist", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_importance_weight_estimator.test_importance_weight_estimator_using_valid_input_data": [[687, 723], ["pytest.mark.parametrize", "obp.ope.ImportanceWeightEstimator", "obp.ope.ImportanceWeightEstimator.fit_predict"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.classification_model.PropensityScoreEstimator.fit_predict"], ["", "", "@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"context, action, position, action_context, n_actions, len_list, fitting_method, base_model, action_dist, n_folds, random_state, calibration_cv, description\"", ",", "\n", "valid_input_of_importance_weight_estimator", ",", "\n", ")", "\n", "def", "test_importance_weight_estimator_using_valid_input_data", "(", "\n", "context", ":", "np", ".", "ndarray", ",", "\n", "action", ":", "np", ".", "ndarray", ",", "\n", "position", ":", "np", ".", "ndarray", ",", "\n", "action_context", ":", "np", ".", "ndarray", ",", "\n", "n_actions", ":", "int", ",", "\n", "len_list", ":", "int", ",", "\n", "fitting_method", ":", "str", ",", "\n", "base_model", ":", "BaseEstimator", ",", "\n", "action_dist", ":", "np", ".", "ndarray", ",", "\n", "n_folds", ":", "int", ",", "\n", "random_state", ":", "int", ",", "\n", "calibration_cv", ":", "int", ",", "\n", "description", ":", "str", ",", "\n", ")", "->", "None", ":", "\n", "# fit_predict", "\n", "    ", "importance_weight_estimator", "=", "ImportanceWeightEstimator", "(", "\n", "n_actions", "=", "n_actions", ",", "\n", "len_list", "=", "len_list", ",", "\n", "action_context", "=", "action_context", ",", "\n", "base_model", "=", "base_model", ",", "\n", "fitting_method", "=", "fitting_method", ",", "\n", "calibration_cv", "=", "calibration_cv", ",", "\n", ")", "\n", "# train importance weight estimator on logged bandit feedback data", "\n", "_", "=", "importance_weight_estimator", ".", "fit_predict", "(", "\n", "context", "=", "context", ",", "\n", "action", "=", "action", ",", "\n", "action_dist", "=", "action_dist", ",", "\n", "position", "=", "position", ",", "\n", "n_folds", "=", "n_folds", ",", "\n", "random_state", "=", "random_state", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_importance_weight_estimator.test_performance_of_binary_outcome_models": [[726, 777], ["fixed_synthetic_bandit_feedback.copy", "binary_model_dict.items", "print", "obp.ope.ImportanceWeightEstimator", "obp.ope.ImportanceWeightEstimator.fit_predict", "numpy.all", "range", "numpy.array().flatten", "numpy.array().flatten", "sklearn.metrics.roc_auc_score", "len", "tmp_y.append", "tmp_pred.append", "model", "numpy.array", "numpy.array"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.classification_model.PropensityScoreEstimator.fit_predict"], ["", "def", "test_performance_of_binary_outcome_models", "(", "\n", "fixed_synthetic_bandit_feedback", ":", "BanditFeedback", ",", "random_action_dist", ":", "np", ".", "ndarray", "\n", ")", "->", "None", ":", "\n", "    ", "\"\"\"\n    Test the performance of ope estimators using synthetic bandit data and random evaluation policy\n    when the importance weight estimator is estimated by a logistic regression\n    \"\"\"", "\n", "bandit_feedback", "=", "fixed_synthetic_bandit_feedback", ".", "copy", "(", ")", "\n", "action_dist", "=", "random_action_dist", "\n", "random_state", "=", "12345", "\n", "auc_scores", ":", "Dict", "[", "str", ",", "float", "]", "=", "{", "}", "\n", "fit_methods", "=", "[", "\"sample\"", ",", "\"raw\"", "]", "\n", "for", "fit_method", "in", "fit_methods", ":", "\n", "        ", "for", "model_name", ",", "model", "in", "binary_model_dict", ".", "items", "(", ")", ":", "\n", "            ", "importance_weight_estimator", "=", "ImportanceWeightEstimator", "(", "\n", "n_actions", "=", "bandit_feedback", "[", "\"n_actions\"", "]", ",", "\n", "action_context", "=", "bandit_feedback", "[", "\"action_context\"", "]", ",", "\n", "base_model", "=", "model", "(", "**", "hyperparams", "[", "model_name", "]", ")", ",", "\n", "fitting_method", "=", "fit_method", ",", "\n", "len_list", "=", "1", ",", "\n", ")", "\n", "# train importance weight estimator on logged bandit feedback data", "\n", "estimated_importance_weight", "=", "importance_weight_estimator", ".", "fit_predict", "(", "\n", "context", "=", "bandit_feedback", "[", "\"context\"", "]", ",", "\n", "action", "=", "bandit_feedback", "[", "\"action\"", "]", ",", "\n", "action_dist", "=", "action_dist", ",", "\n", "n_folds", "=", "2", ",", "# 2-fold cross-fitting", "\n", "random_state", "=", "random_state", ",", "\n", "evaluate_model_performance", "=", "True", ",", "\n", ")", "\n", "assert", "np", ".", "all", "(", "\n", "estimated_importance_weight", ">=", "0", "\n", ")", ",", "\"estimated_importance_weight must be non-negative\"", "\n", "# extract predictions", "\n", "tmp_y", "=", "[", "]", "\n", "tmp_pred", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "importance_weight_estimator", ".", "eval_result", "[", "\"y\"", "]", ")", ")", ":", "\n", "                ", "tmp_y", ".", "append", "(", "importance_weight_estimator", ".", "eval_result", "[", "\"y\"", "]", "[", "i", "]", ")", "\n", "tmp_pred", ".", "append", "(", "importance_weight_estimator", ".", "eval_result", "[", "\"proba\"", "]", "[", "i", "]", ")", "\n", "", "y_test", "=", "np", ".", "array", "(", "tmp_y", ")", ".", "flatten", "(", ")", "\n", "y_pred", "=", "np", ".", "array", "(", "tmp_pred", ")", ".", "flatten", "(", ")", "\n", "auc_scores", "[", "model_name", "+", "\"_\"", "+", "fit_method", "]", "=", "roc_auc_score", "(", "\n", "y_true", "=", "y_test", ",", "\n", "y_score", "=", "y_pred", ",", "\n", ")", "\n", "\n", "", "", "for", "model_name", "in", "auc_scores", ":", "\n", "        ", "print", "(", "f\"AUC of {model_name} is {auc_scores[model_name]}\"", ")", "\n", "assert", "(", "\n", "auc_scores", "[", "model_name", "]", ">", "0.5", "\n", ")", ",", "f\"AUC of {model_name} should be greater than 0.5\"", "\n", "", "", ""]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_ipw_estimators_embed.test_mipw_init_using_invalid_inputs": [[109, 138], ["pytest.mark.parametrize", "pytest.raises", "obp.ope.MarginalizedInverseProbabilityWeighting", "pytest.raises", "obp.ope.SelfNormalizedMarginalizedInverseProbabilityWeighting"], "function", ["None"], ["@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"n_actions, delta, pi_a_x_e_estimator, embedding_selection_method, min_emb_dim, err, description\"", ",", "\n", "invalid_input_of_ipw_init", ",", "\n", ")", "\n", "def", "test_mipw_init_using_invalid_inputs", "(", "\n", "n_actions", ",", "\n", "delta", ",", "\n", "pi_a_x_e_estimator", ",", "\n", "embedding_selection_method", ",", "\n", "min_emb_dim", ",", "\n", "err", ",", "\n", "description", ",", "\n", ")", ":", "\n", "    ", "with", "pytest", ".", "raises", "(", "err", ",", "match", "=", "f\"{description}*\"", ")", ":", "\n", "        ", "_", "=", "MIPW", "(", "\n", "n_actions", "=", "n_actions", ",", "\n", "delta", "=", "delta", ",", "\n", "pi_a_x_e_estimator", "=", "pi_a_x_e_estimator", ",", "\n", "embedding_selection_method", "=", "embedding_selection_method", ",", "\n", "min_emb_dim", "=", "min_emb_dim", ",", "\n", ")", "\n", "\n", "", "with", "pytest", ".", "raises", "(", "err", ",", "match", "=", "f\"{description}*\"", ")", ":", "\n", "        ", "_", "=", "SNMIPW", "(", "\n", "n_actions", "=", "n_actions", ",", "\n", "delta", "=", "delta", ",", "\n", "pi_a_x_e_estimator", "=", "pi_a_x_e_estimator", ",", "\n", "embedding_selection_method", "=", "embedding_selection_method", ",", "\n", "min_emb_dim", "=", "min_emb_dim", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_ipw_estimators_embed.test_mipw_using_invalid_input_data": [[300, 342], ["pytest.mark.parametrize", "obp.ope.MarginalizedInverseProbabilityWeighting", "obp.ope.MarginalizedInverseProbabilityWeighting", "obp.ope.MarginalizedInverseProbabilityWeighting", "obp.ope.SelfNormalizedMarginalizedInverseProbabilityWeighting", "pytest.raises", "est.estimate_policy_value", "pytest.raises", "est.estimate_interval"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_policy_value", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_interval"], ["@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"action_dist, context, action, reward, action_embed, pi_b, p_e_a, position, description\"", ",", "\n", "invalid_input_of_mipw", ",", "\n", ")", "\n", "def", "test_mipw_using_invalid_input_data", "(", "\n", "action_dist", ":", "np", ".", "ndarray", ",", "\n", "context", ":", "np", ".", "ndarray", ",", "\n", "action", ":", "np", ".", "ndarray", ",", "\n", "reward", ":", "np", ".", "ndarray", ",", "\n", "action_embed", ":", "np", ".", "ndarray", ",", "\n", "pi_b", ":", "np", ".", "ndarray", ",", "\n", "p_e_a", ":", "np", ".", "ndarray", ",", "\n", "position", ":", "np", ".", "ndarray", ",", "\n", "description", ":", "str", ",", "\n", ")", "->", "None", ":", "\n", "# prepare ipw instances", "\n", "    ", "mipw", "=", "MIPW", "(", "n_actions", "=", "2", ")", "\n", "mipw_exact", "=", "MIPW", "(", "n_actions", "=", "2", ",", "embedding_selection_method", "=", "\"exact\"", ")", "\n", "mipw_greedy", "=", "MIPW", "(", "n_actions", "=", "2", ",", "embedding_selection_method", "=", "\"greedy\"", ")", "\n", "snmipw", "=", "SNMIPW", "(", "n_actions", "=", "2", ")", "\n", "for", "est", "in", "[", "mipw", ",", "mipw_exact", ",", "mipw_greedy", ",", "snmipw", "]", ":", "\n", "        ", "with", "pytest", ".", "raises", "(", "ValueError", ",", "match", "=", "f\"{description}*\"", ")", ":", "\n", "            ", "_", "=", "est", ".", "estimate_policy_value", "(", "\n", "action_dist", "=", "action_dist", ",", "\n", "context", "=", "context", ",", "\n", "action", "=", "action", ",", "\n", "reward", "=", "reward", ",", "\n", "action_embed", "=", "action_embed", ",", "\n", "pi_b", "=", "pi_b", ",", "\n", "p_e_a", "=", "p_e_a", ",", "\n", "position", "=", "position", ",", "\n", ")", "\n", "", "with", "pytest", ".", "raises", "(", "ValueError", ",", "match", "=", "f\"{description}*\"", ")", ":", "\n", "            ", "_", "=", "est", ".", "estimate_interval", "(", "\n", "action_dist", "=", "action_dist", ",", "\n", "context", "=", "context", ",", "\n", "action", "=", "action", ",", "\n", "reward", "=", "reward", ",", "\n", "action_embed", "=", "action_embed", ",", "\n", "pi_b", "=", "pi_b", ",", "\n", "p_e_a", "=", "p_e_a", ",", "\n", "position", "=", "position", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_ipw_estimators_embed.test_ipw_using_random_evaluation_policy": [[345, 387], ["obp.ope.MarginalizedInverseProbabilityWeighting", "obp.ope.MarginalizedInverseProbabilityWeighting", "obp.ope.MarginalizedInverseProbabilityWeighting", "obp.ope.SelfNormalizedMarginalizedInverseProbabilityWeighting", "estimator.estimate_policy_value", "isinstance", "synthetic_bandit_feedback_with_embed.items", "pytest.raises", "estimator.estimate_policy_value", "re.escape"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_policy_value", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_policy_value"], ["", "", "", "def", "test_ipw_using_random_evaluation_policy", "(", "\n", "synthetic_bandit_feedback_with_embed", ":", "BanditFeedback", ",", "random_action_dist", ":", "np", ".", "ndarray", "\n", ")", "->", "None", ":", "\n", "    ", "\"\"\"\n    Test the format of ipw variants using synthetic bandit data and random evaluation policy\n    \"\"\"", "\n", "action_dist", "=", "random_action_dist", "\n", "# prepare input dict", "\n", "input_dict", "=", "{", "\n", "k", ":", "v", "\n", "for", "k", ",", "v", "in", "synthetic_bandit_feedback_with_embed", ".", "items", "(", ")", "\n", "if", "k", "in", "[", "\"reward\"", ",", "\"action\"", ",", "\"pi_b\"", ",", "\"action_embed\"", ",", "\"context\"", ",", "\"position\"", "]", "\n", "}", "\n", "input_dict", "[", "\"action_dist\"", "]", "=", "action_dist", "\n", "mipw", "=", "MIPW", "(", "n_actions", "=", "synthetic_bandit_feedback_with_embed", "[", "\"n_actions\"", "]", ")", "\n", "mipw_exact", "=", "MIPW", "(", "\n", "n_actions", "=", "synthetic_bandit_feedback_with_embed", "[", "\"n_actions\"", "]", ",", "\n", "embedding_selection_method", "=", "\"exact\"", ",", "\n", ")", "\n", "mipw_greedy", "=", "MIPW", "(", "\n", "n_actions", "=", "synthetic_bandit_feedback_with_embed", "[", "\"n_actions\"", "]", ",", "\n", "embedding_selection_method", "=", "\"greedy\"", ",", "\n", ")", "\n", "snmipw", "=", "SNMIPW", "(", "n_actions", "=", "synthetic_bandit_feedback_with_embed", "[", "\"n_actions\"", "]", ")", "\n", "# ipw estimators can be used without estimated_rewards_by_reg_model", "\n", "for", "estimator", "in", "[", "mipw", ",", "mipw_exact", ",", "mipw_greedy", ",", "snmipw", "]", ":", "\n", "        ", "estimated_policy_value", "=", "estimator", ".", "estimate_policy_value", "(", "**", "input_dict", ")", "\n", "assert", "isinstance", "(", "\n", "estimated_policy_value", ",", "float", "\n", ")", ",", "f\"invalid type response: {estimator}\"", "\n", "\n", "# remove necessary keys", "\n", "", "del", "input_dict", "[", "\"reward\"", "]", "\n", "del", "input_dict", "[", "\"action\"", "]", "\n", "for", "estimator", "in", "[", "mipw", ",", "snmipw", "]", ":", "\n", "        ", "with", "pytest", ".", "raises", "(", "\n", "TypeError", ",", "\n", "match", "=", "re", ".", "escape", "(", "\n", "\"estimate_policy_value() missing 2 required positional arguments: 'reward' and 'action'\"", "\n", ")", ",", "\n", ")", ":", "\n", "            ", "_", "=", "estimator", ".", "estimate_policy_value", "(", "**", "input_dict", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.conftest.synthetic_bandit_feedback": [[27, 42], ["pytest.fixture", "obp.dataset.SyntheticBanditDataset", "obp.dataset.SyntheticBanditDataset.obtain_batch_bandit_feedback"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.multiclass.MultiClassToBanditReduction.obtain_batch_bandit_feedback"], ["@", "pytest", ".", "fixture", "(", "scope", "=", "\"session\"", ")", "\n", "def", "synthetic_bandit_feedback", "(", ")", "->", "BanditFeedback", ":", "\n", "    ", "n_actions", "=", "10", "\n", "dim_context", "=", "5", "\n", "random_state", "=", "12345", "\n", "n_rounds", "=", "10000", "\n", "dataset", "=", "SyntheticBanditDataset", "(", "\n", "n_actions", "=", "n_actions", ",", "\n", "dim_context", "=", "dim_context", ",", "\n", "reward_function", "=", "logistic_reward_function", ",", "\n", "behavior_policy_function", "=", "linear_behavior_policy", ",", "\n", "random_state", "=", "random_state", ",", "\n", ")", "\n", "bandit_feedback", "=", "dataset", ".", "obtain_batch_bandit_feedback", "(", "n_rounds", "=", "n_rounds", ")", "\n", "return", "bandit_feedback", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.conftest.synthetic_slate_bandit_feedback": [[45, 64], ["pytest.fixture", "obp.dataset.SyntheticSlateBanditDataset", "obp.dataset.SyntheticSlateBanditDataset.obtain_batch_bandit_feedback"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.multiclass.MultiClassToBanditReduction.obtain_batch_bandit_feedback"], ["", "@", "pytest", ".", "fixture", "(", "scope", "=", "\"session\"", ")", "\n", "def", "synthetic_slate_bandit_feedback", "(", ")", "->", "BanditFeedback", ":", "\n", "# set parameters", "\n", "    ", "n_unique_action", "=", "10", "\n", "len_list", "=", "3", "\n", "dim_context", "=", "2", "\n", "reward_type", "=", "\"binary\"", "\n", "random_state", "=", "12345", "\n", "n_rounds", "=", "100", "\n", "dataset", "=", "SyntheticSlateBanditDataset", "(", "\n", "n_unique_action", "=", "n_unique_action", ",", "\n", "len_list", "=", "len_list", ",", "\n", "dim_context", "=", "dim_context", ",", "\n", "reward_type", "=", "reward_type", ",", "\n", "random_state", "=", "random_state", ",", "\n", ")", "\n", "# obtain feedback", "\n", "bandit_feedback", "=", "dataset", ".", "obtain_batch_bandit_feedback", "(", "n_rounds", "=", "n_rounds", ")", "\n", "return", "bandit_feedback", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.conftest.synthetic_continuous_bandit_feedback": [[67, 84], ["pytest.fixture", "obp.dataset.SyntheticContinuousBanditDataset", "obp.dataset.SyntheticContinuousBanditDataset.obtain_batch_bandit_feedback"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.multiclass.MultiClassToBanditReduction.obtain_batch_bandit_feedback"], ["", "@", "pytest", ".", "fixture", "(", "scope", "=", "\"session\"", ")", "\n", "def", "synthetic_continuous_bandit_feedback", "(", ")", "->", "BanditFeedback", ":", "\n", "# set parameters", "\n", "    ", "dim_context", "=", "2", "\n", "random_state", "=", "12345", "\n", "n_rounds", "=", "100", "\n", "min_action_value", "=", "-", "10", "\n", "max_action_value", "=", "10", "\n", "dataset", "=", "SyntheticContinuousBanditDataset", "(", "\n", "dim_context", "=", "dim_context", ",", "\n", "min_action_value", "=", "min_action_value", ",", "\n", "max_action_value", "=", "max_action_value", ",", "\n", "random_state", "=", "random_state", ",", "\n", ")", "\n", "# obtain feedback", "\n", "bandit_feedback", "=", "dataset", ".", "obtain_batch_bandit_feedback", "(", "n_rounds", "=", "n_rounds", ")", "\n", "return", "bandit_feedback", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.conftest.synthetic_multi_bandit_feedback": [[86, 104], ["pytest.fixture", "obp.dataset.SyntheticMultiLoggersBanditDataset", "obp.dataset.SyntheticMultiLoggersBanditDataset.obtain_batch_bandit_feedback"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.multiclass.MultiClassToBanditReduction.obtain_batch_bandit_feedback"], ["", "@", "pytest", ".", "fixture", "(", "scope", "=", "\"session\"", ")", "\n", "def", "synthetic_multi_bandit_feedback", "(", ")", "->", "BanditFeedback", ":", "\n", "    ", "n_actions", "=", "10", "\n", "dim_context", "=", "5", "\n", "betas", "=", "[", "-", "10", ",", "-", "5", ",", "0", ",", "5", ",", "10", "]", "\n", "rhos", "=", "[", "1", ",", "2", ",", "3", ",", "2", ",", "1", "]", "\n", "random_state", "=", "12345", "\n", "n_rounds", "=", "10000", "\n", "dataset", "=", "SyntheticMultiLoggersBanditDataset", "(", "\n", "n_actions", "=", "n_actions", ",", "\n", "dim_context", "=", "dim_context", ",", "\n", "betas", "=", "betas", ",", "\n", "rhos", "=", "rhos", ",", "\n", "reward_function", "=", "logistic_reward_function", ",", "\n", "random_state", "=", "random_state", ",", "\n", ")", "\n", "bandit_feedback", "=", "dataset", ".", "obtain_batch_bandit_feedback", "(", "n_rounds", "=", "n_rounds", ")", "\n", "return", "bandit_feedback", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.conftest.synthetic_bandit_feedback_with_embed": [[106, 124], ["pytest.fixture", "obp.dataset.SyntheticBanditDatasetWithActionEmbeds", "obp.dataset.SyntheticBanditDatasetWithActionEmbeds.obtain_batch_bandit_feedback"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.multiclass.MultiClassToBanditReduction.obtain_batch_bandit_feedback"], ["", "@", "pytest", ".", "fixture", "(", "scope", "=", "\"session\"", ")", "\n", "def", "synthetic_bandit_feedback_with_embed", "(", ")", "->", "BanditFeedback", ":", "\n", "    ", "n_actions", "=", "10", "\n", "dim_context", "=", "5", "\n", "n_cat_dim", "=", "3", "\n", "n_cat_per_dim", "=", "5", "\n", "random_state", "=", "12345", "\n", "n_rounds", "=", "10000", "\n", "dataset", "=", "SyntheticBanditDatasetWithActionEmbeds", "(", "\n", "n_actions", "=", "n_actions", ",", "\n", "dim_context", "=", "dim_context", ",", "\n", "n_cat_dim", "=", "n_cat_dim", ",", "\n", "n_cat_per_dim", "=", "n_cat_per_dim", ",", "\n", "reward_function", "=", "logistic_reward_function", ",", "\n", "random_state", "=", "random_state", ",", "\n", ")", "\n", "bandit_feedback", "=", "dataset", ".", "obtain_batch_bandit_feedback", "(", "n_rounds", "=", "n_rounds", ")", "\n", "return", "bandit_feedback", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.conftest.fixed_synthetic_bandit_feedback": [[127, 142], ["pytest.fixture", "sklearn.utils.check_random_state", "copy.deepcopy", "scipy.special.logit", "obp.utils.sigmoid", "sklearn.utils.check_random_state.binomial", "numpy.arange"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.sigmoid"], ["", "@", "pytest", ".", "fixture", "(", "scope", "=", "\"session\"", ")", "\n", "def", "fixed_synthetic_bandit_feedback", "(", "synthetic_bandit_feedback", ")", "->", "BanditFeedback", ":", "\n", "# set random", "\n", "    ", "random_state", "=", "12345", "\n", "random_", "=", "check_random_state", "(", "random_state", ")", "\n", "# copy synthetic bandit feedback", "\n", "bandit_feedback", "=", "copy", ".", "deepcopy", "(", "synthetic_bandit_feedback", ")", "\n", "# expected reward would be about 0.65%, which is close to that of the ZOZO dataset", "\n", "logit", "=", "special", ".", "logit", "(", "bandit_feedback", "[", "\"expected_reward\"", "]", ")", "\n", "bandit_feedback", "[", "\"expected_reward\"", "]", "=", "sigmoid", "(", "logit", "-", "4.0", ")", "\n", "expected_reward_factual", "=", "bandit_feedback", "[", "\"expected_reward\"", "]", "[", "\n", "np", ".", "arange", "(", "bandit_feedback", "[", "\"n_rounds\"", "]", ")", ",", "bandit_feedback", "[", "\"action\"", "]", "\n", "]", "\n", "bandit_feedback", "[", "\"reward\"", "]", "=", "random_", ".", "binomial", "(", "n", "=", "1", ",", "p", "=", "expected_reward_factual", ")", "\n", "return", "bandit_feedback", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.conftest.feedback_key_set": [[145, 158], ["pytest.fixture"], "function", ["None"], ["", "@", "pytest", ".", "fixture", "(", "scope", "=", "\"session\"", ")", "\n", "def", "feedback_key_set", "(", ")", "->", "Set", "[", "str", "]", ":", "\n", "    ", "return", "{", "\n", "\"action\"", ",", "\n", "\"action_context\"", ",", "\n", "\"context\"", ",", "\n", "\"expected_reward\"", ",", "\n", "\"n_actions\"", ",", "\n", "\"n_rounds\"", ",", "\n", "\"position\"", ",", "\n", "\"pi_b\"", ",", "\n", "\"pscore\"", ",", "\n", "\"reward\"", ",", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.conftest.expected_reward_0": [[162, 176], ["pytest.fixture", "numpy.array"], "function", ["None"], ["", "@", "pytest", ".", "fixture", "(", "scope", "=", "\"session\"", ")", "\n", "def", "expected_reward_0", "(", ")", "->", "np", ".", "ndarray", ":", "\n", "    ", "return", "np", ".", "array", "(", "\n", "[", "\n", "0.816124", ",", "\n", "0.625855", ",", "\n", "0.386785", ",", "\n", "0.301848", ",", "\n", "0.726634", ",", "\n", "0.218076", ",", "\n", "0.482361", ",", "\n", "0.625271", ",", "\n", "0.586353", ",", "\n", "0.386384", ",", "\n", "]", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.conftest.random_action_dist": [[181, 189], ["pytest.fixture", "obp.policy.Random", "obp.policy.Random.compute_batch_action_dist"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.policy.contextfree.BernoulliTS.compute_batch_action_dist"], ["", "@", "pytest", ".", "fixture", "(", "scope", "=", "\"session\"", ")", "\n", "def", "random_action_dist", "(", "synthetic_bandit_feedback", ")", "->", "np", ".", "ndarray", ":", "\n", "    ", "n_actions", "=", "synthetic_bandit_feedback", "[", "\"n_actions\"", "]", "\n", "evaluation_policy", "=", "Random", "(", "n_actions", "=", "n_actions", ",", "len_list", "=", "1", ")", "\n", "action_dist", "=", "evaluation_policy", ".", "compute_batch_action_dist", "(", "\n", "n_rounds", "=", "synthetic_bandit_feedback", "[", "\"n_rounds\"", "]", "\n", ")", "\n", "return", "action_dist", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.conftest.generate_action_dist": [[191, 195], ["numpy.random.uniform", "np.random.uniform.sum"], "function", ["None"], ["", "def", "generate_action_dist", "(", "i", ",", "j", ",", "k", ")", ":", "\n", "    ", "x", "=", "np", ".", "random", ".", "uniform", "(", "size", "=", "(", "i", ",", "j", ",", "k", ")", ")", "\n", "action_dist", "=", "x", "/", "x", ".", "sum", "(", "axis", "=", "1", ")", "[", ":", ",", "np", ".", "newaxis", ",", ":", "]", "\n", "return", "action_dist", "\n", "", ""]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_propensity_score_estimator.test_initializing_propensity_score_estimator_using_invalid_input_data": [[405, 424], ["pytest.mark.parametrize", "pytest.raises", "obp.ope.PropensityScoreEstimator"], "function", ["None"], ["@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"n_actions, len_list, base_model, calibration_cv, err, description\"", ",", "\n", "invalid_input_of_initializing_propensity_score_estimator", ",", "\n", ")", "\n", "def", "test_initializing_propensity_score_estimator_using_invalid_input_data", "(", "\n", "n_actions", ":", "int", ",", "\n", "len_list", ":", "int", ",", "\n", "base_model", ":", "BaseEstimator", ",", "\n", "calibration_cv", ":", "int", ",", "\n", "err", ",", "\n", "description", ":", "str", ",", "\n", ")", "->", "None", ":", "\n", "# initialization raises ValueError", "\n", "    ", "with", "pytest", ".", "raises", "(", "err", ",", "match", "=", "f\"{description}*\"", ")", ":", "\n", "        ", "_", "=", "PropensityScoreEstimator", "(", "\n", "n_actions", "=", "n_actions", ",", "\n", "len_list", "=", "len_list", ",", "\n", "base_model", "=", "base_model", ",", "\n", "calibration_cv", "=", "calibration_cv", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_propensity_score_estimator.test_fitting_propensity_score_estimator_using_invalid_input_data": [[427, 459], ["pytest.mark.parametrize", "pytest.raises", "obp.ope.PropensityScoreEstimator", "obp.ope.PropensityScoreEstimator.fit_predict"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.classification_model.PropensityScoreEstimator.fit_predict"], ["", "", "@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"context, action, position, n_actions, len_list, base_model, n_folds, random_state, calibration_cv, err, description\"", ",", "\n", "invalid_input_of_fitting_propensity_score_estimator", ",", "\n", ")", "\n", "def", "test_fitting_propensity_score_estimator_using_invalid_input_data", "(", "\n", "context", ":", "np", ".", "ndarray", ",", "\n", "action", ":", "np", ".", "ndarray", ",", "\n", "position", ":", "np", ".", "ndarray", ",", "\n", "n_actions", ":", "int", ",", "\n", "len_list", ":", "int", ",", "\n", "base_model", ":", "BaseEstimator", ",", "\n", "n_folds", ":", "int", ",", "\n", "random_state", ":", "int", ",", "\n", "calibration_cv", ":", "int", ",", "\n", "err", ",", "\n", "description", ":", "str", ",", "\n", ")", "->", "None", ":", "\n", "# fit_predict function raises ValueError", "\n", "    ", "with", "pytest", ".", "raises", "(", "err", ",", "match", "=", "f\"{description}*\"", ")", ":", "\n", "        ", "propensity_score_estimator", "=", "PropensityScoreEstimator", "(", "\n", "n_actions", "=", "n_actions", ",", "\n", "len_list", "=", "len_list", ",", "\n", "base_model", "=", "base_model", ",", "\n", "calibration_cv", "=", "calibration_cv", ",", "\n", ")", "\n", "# train propensity score estimator on logged bandit feedback data", "\n", "_", "=", "propensity_score_estimator", ".", "fit_predict", "(", "\n", "context", "=", "context", ",", "\n", "action", "=", "action", ",", "\n", "position", "=", "position", ",", "\n", "n_folds", "=", "n_folds", ",", "\n", "random_state", "=", "random_state", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_propensity_score_estimator.test_propensity_score_estimator_using_valid_input_data": [[462, 492], ["pytest.mark.parametrize", "obp.ope.PropensityScoreEstimator", "obp.ope.PropensityScoreEstimator.fit_predict"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.classification_model.PropensityScoreEstimator.fit_predict"], ["", "", "@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"context, action, position, n_actions, len_list, base_model, n_folds, random_state, calibration_cv, description\"", ",", "\n", "valid_input_of_propensity_score_estimator", ",", "\n", ")", "\n", "def", "test_propensity_score_estimator_using_valid_input_data", "(", "\n", "context", ":", "np", ".", "ndarray", ",", "\n", "action", ":", "np", ".", "ndarray", ",", "\n", "position", ":", "np", ".", "ndarray", ",", "\n", "n_actions", ":", "int", ",", "\n", "len_list", ":", "int", ",", "\n", "base_model", ":", "BaseEstimator", ",", "\n", "n_folds", ":", "int", ",", "\n", "random_state", ":", "int", ",", "\n", "calibration_cv", ":", "int", ",", "\n", "description", ":", "str", ",", "\n", ")", "->", "None", ":", "\n", "# fit_predict", "\n", "    ", "propensity_score_estimator", "=", "PropensityScoreEstimator", "(", "\n", "n_actions", "=", "n_actions", ",", "\n", "len_list", "=", "len_list", ",", "\n", "base_model", "=", "base_model", ",", "\n", "calibration_cv", "=", "calibration_cv", ",", "\n", ")", "\n", "# train propensity score estimator on logged bandit feedback data", "\n", "_", "=", "propensity_score_estimator", ".", "fit_predict", "(", "\n", "context", "=", "context", ",", "\n", "action", "=", "action", ",", "\n", "position", "=", "position", ",", "\n", "n_folds", "=", "n_folds", ",", "\n", "random_state", "=", "random_state", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_propensity_score_estimator.test_performance_of_binary_outcome_models": [[495, 539], ["fixed_synthetic_bandit_feedback.copy", "binary_model_dict.items", "obp.ope.PropensityScoreEstimator", "obp.ope.PropensityScoreEstimator.fit_predict", "numpy.all", "range", "numpy.array().flatten", "numpy.array().reshape", "sklearn.metrics.roc_auc_score", "print", "len", "tmp_y.append", "tmp_pred.append", "model", "numpy.array", "numpy.array"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.classification_model.PropensityScoreEstimator.fit_predict"], ["", "def", "test_performance_of_binary_outcome_models", "(", "\n", "fixed_synthetic_bandit_feedback", ":", "BanditFeedback", ",", "\n", ")", "->", "None", ":", "\n", "    ", "\"\"\"\n    Test the performance of ope estimators using synthetic bandit data and random evaluation policy\n    when the propensity score estimator is estimated by a logistic regression\n    \"\"\"", "\n", "bandit_feedback", "=", "fixed_synthetic_bandit_feedback", ".", "copy", "(", ")", "\n", "random_state", "=", "12345", "\n", "auc_scores", ":", "Dict", "[", "str", ",", "float", "]", "=", "{", "}", "\n", "for", "model_name", ",", "model", "in", "binary_model_dict", ".", "items", "(", ")", ":", "\n", "        ", "propensity_score_estimator", "=", "PropensityScoreEstimator", "(", "\n", "n_actions", "=", "bandit_feedback", "[", "\"n_actions\"", "]", ",", "\n", "base_model", "=", "model", "(", "**", "hyperparams", "[", "model_name", "]", ")", ",", "\n", "len_list", "=", "1", ",", "\n", ")", "\n", "# train propensity score estimator on logged bandit feedback data", "\n", "estimated_propensity_score", "=", "propensity_score_estimator", ".", "fit_predict", "(", "\n", "context", "=", "bandit_feedback", "[", "\"context\"", "]", ",", "\n", "action", "=", "bandit_feedback", "[", "\"action\"", "]", ",", "\n", "n_folds", "=", "2", ",", "# 2-fold cross-fitting", "\n", "random_state", "=", "random_state", ",", "\n", "evaluate_model_performance", "=", "True", ",", "\n", ")", "\n", "assert", "np", ".", "all", "(", "\n", "estimated_propensity_score", ">=", "0", "\n", ")", ",", "\"estimated_propensity_score must be non-negative\"", "\n", "# extract predictions", "\n", "tmp_y", "=", "[", "]", "\n", "tmp_pred", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "propensity_score_estimator", ".", "eval_result", "[", "\"y\"", "]", ")", ")", ":", "\n", "            ", "tmp_y", ".", "append", "(", "propensity_score_estimator", ".", "eval_result", "[", "\"y\"", "]", "[", "i", "]", ")", "\n", "tmp_pred", ".", "append", "(", "propensity_score_estimator", ".", "eval_result", "[", "\"proba\"", "]", "[", "i", "]", ")", "\n", "", "y_test", "=", "np", ".", "array", "(", "tmp_y", ")", ".", "flatten", "(", ")", "\n", "y_pred", "=", "np", ".", "array", "(", "tmp_pred", ")", ".", "reshape", "(", "-", "1", ",", "tmp_pred", "[", "0", "]", ".", "shape", "[", "1", "]", ")", "\n", "auc_scores", "[", "model_name", "]", "=", "roc_auc_score", "(", "\n", "y_true", "=", "y_test", ",", "y_score", "=", "y_pred", ",", "multi_class", "=", "\"ovo\"", "\n", ")", "\n", "\n", "", "for", "model_name", "in", "auc_scores", ":", "\n", "        ", "print", "(", "f\"AUC (macro-ovo) of {model_name} is {auc_scores[model_name]}\"", ")", "\n", "assert", "(", "\n", "auc_scores", "[", "model_name", "]", ">", "0.5", "\n", ")", ",", "f\"AUC of {model_name} should be greater than 0.5\"", "\n", "", "", ""]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_offline_estimation_performance.NaiveEstimator._estimate_round_rewards": [[111, 117], ["None"], "methods", ["None"], ["def", "_estimate_round_rewards", "(", "\n", "self", ",", "\n", "reward", ":", "np", ".", "ndarray", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "np", ".", "ndarray", ":", "\n", "        ", "return", "reward", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_offline_estimation_performance.NaiveEstimator.estimate_policy_value": [[118, 125], ["test_offline_estimation_performance.NaiveEstimator._estimate_round_rewards().mean", "test_offline_estimation_performance.NaiveEstimator._estimate_round_rewards"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SelfNormalizedSlateStandardIPS._estimate_round_rewards"], ["", "def", "estimate_policy_value", "(", "\n", "self", ",", "\n", "reward", ":", "np", ".", "ndarray", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "float", ":", "\n", "        ", "\"\"\"Estimate the policy value of evaluation policy.\"\"\"", "\n", "return", "self", ".", "_estimate_round_rewards", "(", "reward", "=", "reward", ")", ".", "mean", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_offline_estimation_performance.NaiveEstimator.estimate_policy_value_tensor": [[126, 128], ["None"], "methods", ["None"], ["", "def", "estimate_policy_value_tensor", "(", "self", ",", "**", "kwargs", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "pass", "# not used in this test", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_offline_estimation_performance.NaiveEstimator.estimate_interval": [[129, 131], ["None"], "methods", ["None"], ["", "def", "estimate_interval", "(", "self", ")", "->", "Dict", "[", "str", ",", "float", "]", ":", "\n", "        ", "pass", "# not used in this test", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.test_offline_estimation_performance.test_offline_estimation_performance": [[221, 395], ["pytest.mark.parametrize", "metric_dict.update", "enumerate", "pandas.DataFrame().describe().T.round", "obp.dataset.SyntheticBanditDatasetWithActionEmbeds", "obp.policy.IPWLearner", "obp.dataset.SyntheticBanditDatasetWithActionEmbeds.obtain_batch_bandit_feedback", "obp.dataset.SyntheticBanditDatasetWithActionEmbeds.obtain_batch_bandit_feedback", "obp.policy.IPWLearner.fit", "obp.policy.IPWLearner.predict_proba", "obp.ope.RegressionModel", "obp.ope.RegressionModel.fit_predict", "obp.ope.PropensityScoreEstimator", "obp.ope.PropensityScoreEstimator.fit_predict", "bipw_model_configurations.items", "obp.ope.OffPolicyEvaluation", "obp.ope.OffPolicyEvaluation.evaluate_performance_of_estimators", "joblib.Parallel", "dict", "ope.evaluate_performance_of_estimators.items", "obp.ope.ImportanceWeightEstimator", "obp.ope.ImportanceWeightEstimator.fit_predict", "dict", "dict", "dict", "obp.dataset.SyntheticBanditDatasetWithActionEmbeds.calc_ground_truth_policy_value", "joblib.delayed", "numpy.arange", "pandas.DataFrame().describe", "obp.ope.MarginalizedInverseProbabilityWeighting", "obp.ope.MarginalizedInverseProbabilityWeighting", "obp.ope.SelfNormalizedMarginalizedInverseProbabilityWeighting", "pandas.DataFrame"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.multiclass.MultiClassToBanditReduction.obtain_batch_bandit_feedback", "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.multiclass.MultiClassToBanditReduction.obtain_batch_bandit_feedback", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.classification_model.PropensityScoreEstimator.fit", "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.offline.NNPolicyLearner.predict_proba", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.classification_model.PropensityScoreEstimator.fit_predict", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.classification_model.PropensityScoreEstimator.fit_predict", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta.OffPolicyEvaluation.evaluate_performance_of_estimators", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.classification_model.PropensityScoreEstimator.fit_predict", "home.repos.pwc.inspect_result.st-tech_zr-obp.simulator.simulator.calc_ground_truth_policy_value"], ["@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "\"n_rounds, n_actions, dim_context, base_model_for_iw_estimator, base_model_for_reg_model, base_model_for_pscore_estimator\"", ",", "\n", "offline_experiment_configurations", ",", "\n", ")", "\n", "def", "test_offline_estimation_performance", "(", "\n", "n_rounds", ":", "int", ",", "\n", "n_actions", ":", "int", ",", "\n", "dim_context", ":", "int", ",", "\n", "base_model_for_iw_estimator", ":", "str", ",", "\n", "base_model_for_reg_model", ":", "str", ",", "\n", "base_model_for_pscore_estimator", ":", "str", ",", "\n", ")", "->", "None", ":", "\n", "    ", "def", "process", "(", "i", ":", "int", ")", ":", "\n", "# synthetic data generator", "\n", "        ", "dataset", "=", "SyntheticBanditDatasetWithActionEmbeds", "(", "\n", "n_actions", "=", "n_actions", ",", "\n", "dim_context", "=", "dim_context", ",", "\n", "beta", "=", "3.0", ",", "\n", "n_cat_dim", "=", "3", ",", "\n", "n_cat_per_dim", "=", "5", ",", "\n", "reward_function", "=", "logistic_reward_function", ",", "\n", "random_state", "=", "i", ",", "\n", ")", "\n", "# define evaluation policy using IPWLearner", "\n", "evaluation_policy", "=", "IPWLearner", "(", "\n", "n_actions", "=", "dataset", ".", "n_actions", ",", "\n", "base_classifier", "=", "base_model_dict", "[", "base_model_for_iw_estimator", "]", "(", "\n", "**", "hyperparams", "[", "base_model_for_iw_estimator", "]", "\n", ")", ",", "\n", ")", "\n", "# sample new training and test sets of synthetic logged bandit data", "\n", "bandit_feedback_train", "=", "dataset", ".", "obtain_batch_bandit_feedback", "(", "n_rounds", "=", "n_rounds", ")", "\n", "bandit_feedback_test", "=", "dataset", ".", "obtain_batch_bandit_feedback", "(", "n_rounds", "=", "n_rounds", ")", "\n", "# train the evaluation policy on the training set of the synthetic logged bandit data", "\n", "evaluation_policy", ".", "fit", "(", "\n", "context", "=", "bandit_feedback_train", "[", "\"context\"", "]", ",", "\n", "action", "=", "bandit_feedback_train", "[", "\"action\"", "]", ",", "\n", "reward", "=", "bandit_feedback_train", "[", "\"reward\"", "]", ",", "\n", "pscore", "=", "bandit_feedback_train", "[", "\"pscore\"", "]", ",", "\n", ")", "\n", "# predict the action decisions for the test set of the synthetic logged bandit data", "\n", "action_dist", "=", "evaluation_policy", ".", "predict_proba", "(", "\n", "context", "=", "bandit_feedback_test", "[", "\"context\"", "]", ",", "\n", ")", "\n", "# estimate the reward function of the test set of synthetic bandit feedback with ML model", "\n", "regression_model", "=", "RegressionModel", "(", "\n", "n_actions", "=", "dataset", ".", "n_actions", ",", "\n", "action_context", "=", "dataset", ".", "action_context", ",", "\n", "base_model", "=", "base_model_dict", "[", "base_model_for_reg_model", "]", "(", "\n", "**", "hyperparams", "[", "base_model_for_reg_model", "]", "\n", ")", ",", "\n", ")", "\n", "estimated_rewards_by_reg_model", "=", "regression_model", ".", "fit_predict", "(", "\n", "context", "=", "bandit_feedback_test", "[", "\"context\"", "]", ",", "\n", "action", "=", "bandit_feedback_test", "[", "\"action\"", "]", ",", "\n", "reward", "=", "bandit_feedback_test", "[", "\"reward\"", "]", ",", "\n", "n_folds", "=", "2", ",", "\n", "random_state", "=", "12345", ",", "\n", ")", "\n", "# fit propensity score estimators", "\n", "pscore_estimator", "=", "PropensityScoreEstimator", "(", "\n", "len_list", "=", "1", ",", "\n", "n_actions", "=", "n_actions", ",", "\n", "base_model", "=", "base_model_dict", "[", "base_model_for_pscore_estimator", "]", "(", "\n", "**", "hyperparams", "[", "base_model_for_pscore_estimator", "]", "\n", ")", ",", "\n", "calibration_cv", "=", "3", ",", "\n", ")", "\n", "estimated_pscore", "=", "pscore_estimator", ".", "fit_predict", "(", "\n", "action", "=", "bandit_feedback_test", "[", "\"action\"", "]", ",", "\n", "position", "=", "bandit_feedback_test", "[", "\"position\"", "]", ",", "\n", "context", "=", "bandit_feedback_test", "[", "\"context\"", "]", ",", "\n", "n_folds", "=", "3", ",", "\n", "random_state", "=", "12345", ",", "\n", ")", "\n", "# fit importance weight estimators", "\n", "estimated_importance_weights_dict", "=", "{", "}", "\n", "for", "clf_name", ",", "clf_arguments", "in", "bipw_model_configurations", ".", "items", "(", ")", ":", "\n", "            ", "clf", "=", "ImportanceWeightEstimator", "(", "\n", "len_list", "=", "1", ",", "\n", "n_actions", "=", "n_actions", ",", "\n", "fitting_method", "=", "clf_arguments", "[", "\"fitting_method\"", "]", ",", "\n", "base_model", "=", "clf_arguments", "[", "\"base_model\"", "]", ",", "\n", ")", "\n", "estimated_importance_weights_dict", "[", "clf_name", "]", "=", "clf", ".", "fit_predict", "(", "\n", "action", "=", "bandit_feedback_test", "[", "\"action\"", "]", ",", "\n", "context", "=", "bandit_feedback_test", "[", "\"context\"", "]", ",", "\n", "action_dist", "=", "action_dist", ",", "\n", "position", "=", "bandit_feedback_test", "[", "\"position\"", "]", ",", "\n", "n_folds", "=", "2", ",", "\n", "evaluate_model_performance", "=", "False", ",", "\n", "random_state", "=", "12345", ",", "\n", ")", "\n", "# evaluate estimators' performances using relative estimation error (relative-ee)", "\n", "", "ope", "=", "OffPolicyEvaluation", "(", "\n", "bandit_feedback", "=", "bandit_feedback_test", ",", "\n", "ope_estimators", "=", "ope_estimators", "\n", "+", "[", "\n", "MarginalizedInverseProbabilityWeighting", "(", "\n", "n_actions", "=", "n_actions", ",", "estimator_name", "=", "\"mipw\"", "\n", ")", ",", "\n", "MarginalizedInverseProbabilityWeighting", "(", "\n", "n_actions", "=", "n_actions", ",", "\n", "embedding_selection_method", "=", "\"greedy\"", ",", "\n", "estimator_name", "=", "\"mipw (greedy selection)\"", ",", "\n", ")", ",", "\n", "SelfNormalizedMarginalizedInverseProbabilityWeighting", "(", "\n", "n_actions", "=", "n_actions", ",", "estimator_name", "=", "\"snmipw\"", "\n", ")", ",", "\n", "]", ",", "\n", ")", "\n", "relative_ee_i", "=", "ope", ".", "evaluate_performance_of_estimators", "(", "\n", "ground_truth_policy_value", "=", "dataset", ".", "calc_ground_truth_policy_value", "(", "\n", "expected_reward", "=", "bandit_feedback_test", "[", "\"expected_reward\"", "]", ",", "\n", "action_dist", "=", "action_dist", ",", "\n", ")", ",", "\n", "action_dist", "=", "action_dist", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", "estimated_pscore", "=", "estimated_pscore", ",", "\n", "estimated_importance_weights", "=", "estimated_importance_weights_dict", ",", "\n", "action_embed", "=", "bandit_feedback_test", "[", "\"action_embed\"", "]", ",", "\n", "pi_b", "=", "bandit_feedback_test", "[", "\"pi_b\"", "]", ",", "\n", "metric", "=", "\"relative-ee\"", ",", "\n", ")", "\n", "\n", "return", "relative_ee_i", "\n", "\n", "", "n_runs", "=", "10", "\n", "processed", "=", "Parallel", "(", "\n", "n_jobs", "=", "-", "1", ",", "\n", "verbose", "=", "0", ",", "\n", ")", "(", "[", "delayed", "(", "process", ")", "(", "i", ")", "for", "i", "in", "np", ".", "arange", "(", "n_runs", ")", "]", ")", "\n", "metric_dict", "=", "{", "est", ".", "estimator_name", ":", "dict", "(", ")", "for", "est", "in", "ope_estimators", "}", "\n", "metric_dict", ".", "update", "(", "\n", "{", "\"mipw\"", ":", "dict", "(", ")", ",", "\"mipw (greedy selection)\"", ":", "dict", "(", ")", ",", "\"snmipw\"", ":", "dict", "(", ")", "}", "\n", ")", "\n", "for", "i", ",", "relative_ee_i", "in", "enumerate", "(", "processed", ")", ":", "\n", "        ", "for", "(", "\n", "estimator_name", ",", "\n", "relative_ee_", ",", "\n", ")", "in", "relative_ee_i", ".", "items", "(", ")", ":", "\n", "            ", "metric_dict", "[", "estimator_name", "]", "[", "i", "]", "=", "relative_ee_", "\n", "", "", "relative_ee_df", "=", "DataFrame", "(", "metric_dict", ")", ".", "describe", "(", ")", ".", "T", ".", "round", "(", "6", ")", "\n", "relative_ee_df_mean", "=", "relative_ee_df", "[", "\"mean\"", "]", "\n", "\n", "tested_estimators", "=", "[", "\n", "\"dm\"", ",", "\n", "\"ipw (tuning-mse)\"", ",", "\n", "\"ipw (tuning-slope)\"", ",", "\n", "\"sg-ipw (tuning-mse)\"", ",", "\n", "\"snipw\"", ",", "\n", "\"dr (tuning-mse)\"", ",", "\n", "\"dr (tuning-slope)\"", ",", "\n", "\"sndr\"", ",", "\n", "\"switch-dr (tuning-mse)\"", ",", "\n", "\"switch-dr (tuning-slope)\"", ",", "\n", "\"dr-os (tuning-mse)\"", ",", "\n", "\"dr-os (tuning-slope)\"", ",", "\n", "\"sg-dr (tuning-mse)\"", ",", "\n", "\"sg-dr (tuning-slope)\"", ",", "\n", "\"cipw (estimated pscore)\"", ",", "\n", "\"snipw (estimated pscore)\"", ",", "\n", "\"dr (estimated pscore)\"", ",", "\n", "\"dr-os (estimated pscore)\"", ",", "\n", "\"bipw (svc sample)\"", ",", "\n", "\"bipw (random_forest sample)\"", ",", "\n", "\"mipw\"", ",", "\n", "\"mipw (greedy selection)\"", ",", "\n", "\"snmipw\"", ",", "\n", "]", "\n", "for", "estimator_name", "in", "tested_estimators", ":", "\n", "        ", "assert", "(", "\n", "relative_ee_df_mean", "[", "estimator_name", "]", "/", "relative_ee_df_mean", "[", "\"naive\"", "]", "<", "1.5", "\n", ")", ",", "f\"{estimator_name} is significantly worse than naive (on-policy) estimator\"", "\n", "", "", ""]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.regression_model_slate.SlateRegressionModel.__post_init__": [[56, 75], ["sklearn.utils.check_scalar", "sklearn.utils.check_scalar", "sklearn.base.is_classifier", "numpy.eye", "ValueError", "isinstance", "ValueError", "ValueError", "sklearn.base.clone", "isinstance", "range"], "methods", ["None"], ["def", "__post_init__", "(", "self", ")", ":", "\n", "        ", "\"\"\"Initialize Class.\"\"\"", "\n", "check_scalar", "(", "self", ".", "n_unique_action", ",", "\"n_unique_action\"", ",", "int", ",", "min_val", "=", "2", ")", "\n", "check_scalar", "(", "self", ".", "len_list", ",", "\"len_list\"", ",", "int", ",", "min_val", "=", "1", ")", "\n", "if", "not", "(", "\n", "isinstance", "(", "self", ".", "fitting_method", ",", "str", ")", "\n", "and", "self", ".", "fitting_method", "in", "[", "\"normal\"", ",", "\"iw\"", "]", "\n", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "f\"`fitting_method` must be either 'normal' or 'iw', but {self.fitting_method} is given\"", "\n", ")", "\n", "", "if", "not", "isinstance", "(", "self", ".", "base_model", ",", "BaseEstimator", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"`base_model` must be BaseEstimator or a child class of BaseEstimator\"", "\n", ")", "\n", "", "if", "is_classifier", "(", "self", ".", "base_model", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\"`base_model` must be a regressor, not a classifier\"", ")", "\n", "", "self", ".", "base_model_list", "=", "[", "clone", "(", "self", ".", "base_model", ")", "for", "_", "in", "range", "(", "self", ".", "len_list", ")", "]", "\n", "self", ".", "action_context", "=", "np", ".", "eye", "(", "self", ".", "n_unique_action", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.regression_model_slate.SlateRegressionModel.fit": [[76, 198], ["obp.utils.check_array", "obp.utils.check_array", "obp.utils.check_array", "obp.utils.check_array", "obp.utils.check_array", "obp.utils.check_array", "action.reshape.reshape.reshape", "reward.reshape.reshape.reshape", "len", "numpy.ones", "ValueError", "ValueError", "ValueError", "numpy.any", "numpy.any", "ValueError", "numpy.any", "numpy.any", "ValueError", "numpy.allclose", "ValueError", "range", "regression_model_slate.SlateRegressionModel._preprocess_for_reg_model", "regression_model_slate.SlateRegressionModel.base_model_list[].fit", "numpy.issubdtype", "numpy.ones", "evaluation_policy_action_dist.reshape().sum", "action.reshape.reshape.min", "action.reshape.reshape.max", "evaluation_policy_action_dist.reshape", "evaluation_policy_action_dist.reshape"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.regression_model_slate.SlateRegressionModel._preprocess_for_reg_model", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.classification_model.PropensityScoreEstimator.fit"], ["", "def", "fit", "(", "\n", "self", ",", "\n", "context", ":", "np", ".", "ndarray", ",", "\n", "action", ":", "np", ".", "ndarray", ",", "\n", "reward", ":", "np", ".", "ndarray", ",", "\n", "pscore_cascade", ":", "np", ".", "ndarray", ",", "\n", "evaluation_policy_pscore_cascade", ":", "np", ".", "ndarray", ",", "\n", "evaluation_policy_action_dist", ":", "np", ".", "ndarray", ",", "\n", ")", ":", "\n", "        ", "\"\"\"Fit the regression model on given logged bandit data.\n\n        Parameters\n        ----------\n        context: array-like, shape (n_rounds, dim_context)\n            Context vectors observed for each data, i.e., :math:`x_i`.\n\n        action: array-like, (n_rounds * len_list,)\n            Actions observed at each slot in a ranking/slate in logged bandit data, i.e., :math:`a_{i}(l)`,\n            which is chosen by the behavior policy :math:`\\\\pi_b`.\n\n        reward: array-like, shape (n_rounds * len_list,)\n            Slot-level rewards observed for each data in logged bandit data, i.e., :math:`r_{i}(l)`.\n\n        pscore_cascade: array-like, shape (n_rounds * len_list,)\n            Joint probabilities of behavior policy choosing a particular sequence of actions from the top position to the :math:`l`-th position (:math:`a_{1:l}`).\n\n        evaluation_policy_pscore_cascade: array-like, shape (n_rounds * len_list,)\n            Joint probabilities of evaluation policy choosing a particular sequence of actions from the top position to the :math:`l`-th position (:math:`a_{1:l}`). This type of action choice probabilities corresponds to the cascade model.\n\n        evaluation_policy_action_dist: array-like (n_rounds * len_list * n_unique_actions, )\n            Plackett-luce style action distribution induced by evaluation policy\n            (action choice probabilities at each slot given previous action choices)\n            , i.e., :math:`\\\\pi_e({a'}_t(k) | x_i, a_i(1), \\\\ldots, a_i(l-1)) \\\\forall {a'}_t(k) \\\\in \\\\mathcal{A}`.\n\n        \"\"\"", "\n", "check_array", "(", "array", "=", "context", ",", "name", "=", "\"context\"", ",", "expected_dim", "=", "2", ")", "\n", "check_array", "(", "array", "=", "action", ",", "name", "=", "\"action\"", ",", "expected_dim", "=", "1", ")", "\n", "check_array", "(", "array", "=", "reward", ",", "name", "=", "\"reward\"", ",", "expected_dim", "=", "1", ")", "\n", "check_array", "(", "array", "=", "pscore_cascade", ",", "name", "=", "\"pscore_cascade\"", ",", "expected_dim", "=", "1", ")", "\n", "check_array", "(", "\n", "array", "=", "evaluation_policy_pscore_cascade", ",", "\n", "name", "=", "\"evaluation_policy_pscore_cascade\"", ",", "\n", "expected_dim", "=", "1", ",", "\n", ")", "\n", "check_array", "(", "\n", "array", "=", "evaluation_policy_action_dist", ",", "\n", "name", "=", "\"evaluation_policy_action_dist\"", ",", "\n", "expected_dim", "=", "1", ",", "\n", ")", "\n", "if", "not", "(", "\n", "action", ".", "shape", "\n", "==", "reward", ".", "shape", "\n", "==", "pscore_cascade", ".", "shape", "\n", "==", "evaluation_policy_pscore_cascade", ".", "shape", "\n", "==", "(", "context", ".", "shape", "[", "0", "]", "*", "self", ".", "len_list", ",", ")", "\n", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"Expected `action.shape == reward.shape == pscore_cascade.shape == evaluation_policy_pscore_cascade.shape\"", "\n", "\" == (context.shape[0] * len_list, )`\"", "\n", "\", but found it False\"", "\n", ")", "\n", "", "if", "evaluation_policy_action_dist", ".", "shape", "!=", "(", "\n", "context", ".", "shape", "[", "0", "]", "*", "self", ".", "len_list", "*", "self", ".", "n_unique_action", ",", "\n", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"Expected `evaluation_policy_action_dist.shape == (context.shape[0] * len_list * n_unique_action, )`\"", "\n", "\", but found it False\"", "\n", ")", "\n", "", "if", "not", "(", "\n", "np", ".", "issubdtype", "(", "action", ".", "dtype", ",", "np", ".", "integer", ")", "\n", "and", "action", ".", "min", "(", ")", ">=", "0", "\n", "and", "action", ".", "max", "(", ")", "<", "self", ".", "n_unique_action", "\n", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"`action` elements must be integers in the range of [0, n_unique_action)\"", "\n", ")", "\n", "", "if", "np", ".", "any", "(", "pscore_cascade", "<=", "0", ")", "or", "np", ".", "any", "(", "pscore_cascade", ">", "1", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\"`pscore_cascade` must be in the range of (0, 1]\"", ")", "\n", "", "if", "np", ".", "any", "(", "evaluation_policy_pscore_cascade", "<=", "0", ")", "or", "np", ".", "any", "(", "\n", "evaluation_policy_pscore_cascade", ">", "1", "\n", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"`evaluation_policy_pscore_cascade` must be in the range of (0, 1]\"", "\n", ")", "\n", "", "if", "not", "np", ".", "allclose", "(", "\n", "np", ".", "ones", "(", "\n", "evaluation_policy_action_dist", ".", "reshape", "(", "(", "-", "1", ",", "self", ".", "n_unique_action", ")", ")", ".", "shape", "[", "\n", "0", "\n", "]", "\n", ")", ",", "\n", "evaluation_policy_action_dist", ".", "reshape", "(", "(", "-", "1", ",", "self", ".", "n_unique_action", ")", ")", ".", "sum", "(", "\n", "axis", "=", "1", "\n", ")", ",", "\n", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"`evaluation_policy_action_dist[i * n_unique_action : (i+1) * n_unique_action]` \"", "\n", "\"must sum up to one for all i.\"", "\n", ")", "\n", "# (n_rounds_ * len_list, ) -> (n_rounds_, len_list)", "\n", "", "action", "=", "action", ".", "reshape", "(", "(", "-", "1", ",", "self", ".", "len_list", ")", ")", "\n", "reward", "=", "reward", ".", "reshape", "(", "(", "-", "1", ",", "self", ".", "len_list", ")", ")", "\n", "iw", "=", "(", "evaluation_policy_pscore_cascade", "/", "pscore_cascade", ")", ".", "reshape", "(", "\n", "(", "-", "1", ",", "self", ".", "len_list", ")", "\n", ")", "\n", "\n", "# (n_rounds_, )", "\n", "n_rounds_", "=", "len", "(", "action", ")", "\n", "sample_weight", "=", "np", ".", "ones", "(", "n_rounds_", ")", "\n", "\n", "for", "pos_", "in", "range", "(", "self", ".", "len_list", ")", "[", ":", ":", "-", "1", "]", ":", "\n", "            ", "X", ",", "y", "=", "self", ".", "_preprocess_for_reg_model", "(", "\n", "context", "=", "context", ",", "\n", "action", "=", "action", ",", "\n", "reward", "=", "reward", ",", "\n", "evaluation_policy_action_dist", "=", "evaluation_policy_action_dist", ",", "\n", "position_", "=", "pos_", ",", "\n", ")", "\n", "\n", "if", "self", ".", "fitting_method", "==", "\"iw\"", ":", "\n", "                ", "sample_weight", "=", "iw", "[", ":", ",", "pos_", "]", "\n", "\n", "", "self", ".", "base_model_list", "[", "pos_", "]", ".", "fit", "(", "X", ",", "y", ",", "sample_weight", "=", "sample_weight", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.regression_model_slate.SlateRegressionModel.predict": [[199, 255], ["obp.utils.check_array", "obp.utils.check_array", "len", "action.reshape.reshape.reshape", "numpy.zeros", "numpy.zeros.flatten", "ValueError", "range", "range", "numpy.concatenate", "regression_model_slate.SlateRegressionModel.base_model_list[].predict().reshape", "range", "context_.append", "action_.append", "regression_model_slate.SlateRegressionModel.base_model_list[].predict", "numpy.append"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.classification_model.PropensityScoreEstimator.predict"], ["", "", "def", "predict", "(", "\n", "self", ",", "\n", "context", ":", "np", ".", "ndarray", ",", "\n", "action", ":", "np", ".", "ndarray", ",", "\n", ")", ":", "\n", "        ", "\"\"\"Predict the Q function values.\n\n        Parameters\n        -----------\n        context: array-like, shape (n_rounds_of_new_data, dim_context)\n            Context vectors for new data.\n\n        action: array-like, shape (n_rounds_of_new_data * len_list, )\n            Action vectors for new data.\n\n        Returns\n        -----------\n        q_hat: array-like, shape (n_rounds_of_new_data * len_list * n_unique_action, )\n            Estimated Q function values of new data.\n            :math:`\\\\hat{Q}_{i,l}(x_i, a_i(1), \\\\ldots, a_i(l-1), a_i(l)) \\\\forall a_i(l) \\\\in \\\\mathcal{A}`.\n\n        \"\"\"", "\n", "check_array", "(", "array", "=", "context", ",", "name", "=", "\"context\"", ",", "expected_dim", "=", "2", ")", "\n", "check_array", "(", "array", "=", "action", ",", "name", "=", "\"action\"", ",", "expected_dim", "=", "1", ")", "\n", "if", "action", ".", "shape", "!=", "(", "context", ".", "shape", "[", "0", "]", "*", "self", ".", "len_list", ",", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"Expected `action.shape == (context.shape[0] * len_list, )`\"", "\n", "\", but found it False\"", "\n", ")", "\n", "", "n_rounds_of_new_data", "=", "len", "(", "context", ")", "\n", "# (n_rounds_of_new_data * len_list, ) -> (n_rounds_of_new_data, len_list)", "\n", "action", "=", "action", ".", "reshape", "(", "(", "-", "1", ",", "self", ".", "len_list", ")", ")", "\n", "# (n_rounds_, len_list, n_unique_action, )", "\n", "q_hat", "=", "np", ".", "zeros", "(", "(", "n_rounds_of_new_data", ",", "self", ".", "len_list", ",", "self", ".", "n_unique_action", ")", ")", "\n", "for", "pos_", "in", "range", "(", "self", ".", "len_list", ")", "[", ":", ":", "-", "1", "]", ":", "\n", "# the action vector shrinks every time as the position_ decreases", "\n", "# (n_rounds_of_new_data, position_ - 1)", "\n", "            ", "action", "=", "action", "[", ":", ",", ":", "pos_", "]", "\n", "# (n_rounds_of_new_data, dim_context) -> (n_rounds_of_new_data * n_unique_action, dim_context)", "\n", "context_", "=", "[", "]", "\n", "# (n_rounds_of_new_data, position_) -> (n_rounds_of_new_data * n_unique_action, position_)", "\n", "action_", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "n_rounds_of_new_data", ")", ":", "\n", "                ", "for", "a_", "in", "range", "(", "self", ".", "n_unique_action", ")", ":", "\n", "                    ", "context_", ".", "append", "(", "context", "[", "i", "]", ")", "\n", "action_", ".", "append", "(", "np", ".", "append", "(", "action", "[", "i", "]", ",", "a_", ")", ")", "\n", "# (n_rounds_of_new_data * n_unique_action, dim_context + position_)", "\n", "", "", "X", "=", "np", ".", "concatenate", "(", "[", "context_", ",", "action_", "]", ",", "axis", "=", "1", ")", "\n", "# (n_rounds_of_new_data * n_unique_action, ) -> (n_rounds_of_new_data, n_unique_action)", "\n", "q_hat", "[", ":", ",", "pos_", ",", ":", "]", "=", "(", "\n", "self", ".", "base_model_list", "[", "pos_", "]", "\n", ".", "predict", "(", "X", ")", "\n", ".", "reshape", "(", "(", "-", "1", ",", "self", ".", "n_unique_action", ")", ")", "\n", ")", "\n", "# (n_rounds_of_new_data * len_list * n_unique_action, )", "\n", "", "return", "q_hat", ".", "flatten", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.regression_model_slate.SlateRegressionModel.fit_predict": [[256, 306], ["regression_model_slate.SlateRegressionModel.fit", "regression_model_slate.SlateRegressionModel.predict"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.classification_model.PropensityScoreEstimator.fit", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.classification_model.PropensityScoreEstimator.predict"], ["", "def", "fit_predict", "(", "\n", "self", ",", "\n", "context", ":", "np", ".", "ndarray", ",", "\n", "action", ":", "np", ".", "ndarray", ",", "\n", "reward", ":", "np", ".", "ndarray", ",", "\n", "pscore_cascade", ":", "np", ".", "ndarray", ",", "\n", "evaluation_policy_pscore_cascade", ":", "np", ".", "ndarray", ",", "\n", "evaluation_policy_action_dist", ":", "np", ".", "ndarray", ",", "\n", ")", ":", "\n", "        ", "\"\"\"Fit the regression model on given logged bandit data and predict the Q function values on the same data.\n\n        Parameters\n        ----------\n        context: array-like, shape (n_rounds, dim_context)\n            Context vectors observed for each data, i.e., :math:`x_i`.\n\n        action: array-like, (n_rounds * len_list,)\n            Actions observed at each slot in a ranking/slate in logged bandit data, i.e., :math:`a_{i}(l)`,\n            which is chosen by the behavior policy :math:`\\\\pi_b`.\n\n        reward: array-like, shape (n_rounds * len_list,)\n            Slot-level rewards observed for each data in logged bandit data, i.e., :math:`r_{i}(l)`.\n\n        pscore_cascade: array-like, shape (n_rounds * len_list,)\n            Joint probabilities of behavior policy choosing a particular sequence of actions from the top position to the :math:`l`-th position (:math:`a_{1:l}`).\n\n        evaluation_policy_pscore_cascade: array-like, shape (n_rounds * len_list,)\n            Joint probabilities of evaluation policy choosing a particular sequence of actions from the top position to the :math:`l`-th position (:math:`a_{1:l}`). This type of action choice probabilities corresponds to the cascade model.\n\n        evaluation_policy_action_dist: array-like (n_rounds * len_list * n_unique_actions, )\n            Plackett-luce style action distribution induced by evaluation policy\n            (action choice probabilities at each slot given previous action choices)\n            , i.e., :math:`\\\\pi_e(a_i(l) | x_i, a_i(1), \\\\ldots, a_i(l-1)) \\\\forall a_i(l) \\\\in \\\\mathcal{A}`.\n\n        Returns\n        -----------\n        q_hat: array-like, shape (n_rounds_of_new_data * len_list * n_unique_action, )\n            Estimated Q functions for new data by the regression model.\n\n        \"\"\"", "\n", "self", ".", "fit", "(", "\n", "context", "=", "context", ",", "\n", "action", "=", "action", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore_cascade", "=", "pscore_cascade", ",", "\n", "evaluation_policy_pscore_cascade", "=", "evaluation_policy_pscore_cascade", ",", "\n", "evaluation_policy_action_dist", "=", "evaluation_policy_action_dist", ",", "\n", ")", "\n", "# (n_rounds_test, len_list, n_unique_action, )", "\n", "return", "self", ".", "predict", "(", "context", "=", "context", ",", "action", "=", "action", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.regression_model_slate.SlateRegressionModel._preprocess_for_reg_model": [[307, 391], ["len", "numpy.concatenate", "numpy.zeros", "[].flatten", "range", "numpy.concatenate", "regression_model_slate.SlateRegressionModel.base_model_list[].predict", "range", "context_.append", "action_.append", "evaluation_policy_action_dist.reshape", "numpy.append"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.classification_model.PropensityScoreEstimator.predict"], ["", "def", "_preprocess_for_reg_model", "(", "\n", "self", ",", "\n", "context", ":", "np", ".", "ndarray", ",", "\n", "action", ":", "np", ".", "ndarray", ",", "\n", "reward", ":", "np", ".", "ndarray", ",", "\n", "evaluation_policy_action_dist", ":", "np", ".", "ndarray", ",", "\n", "position_", ":", "int", ",", "\n", ")", ":", "\n", "        ", "\"\"\"Preprocess feature vectors and target variables for training a regression model.\n\n        Note\n        -----\n        Please override this method if you want to use another feature enginnering\n        for training the regression model.\n\n        Parameters\n        -----------\n        context: array-like, shape (n_rounds_, dim_context)\n            Context vectors in the training set of logged bandit data.\n\n        action: array-like, (n_rounds_ * len_list, )\n            Actions observed at each slot in a ranking/slate in logged bandit data, i.e., :math:`a_{i}(l)`,\n            which is chosen by the behavior policy :math:`\\\\pi_b`.\n\n        reward: array-like, shape (n_rounds_ * len_list, )\n            Slot-level rewards observed for each data in logged bandit data, i.e., :math:`r_{i}(l)`.\n\n        evaluation_policy_action_dist: array-like (n_rounds_ * len_list * n_unique_actions, )\n            Plackett-luce style action distribution induced by evaluation policy\n            (action choice probabilities at each slot given previous action choices)\n            , i.e., :math:`\\\\pi_e(a_i(l) | x_i, a_i(1), \\\\ldots, a_i(l-1)) \\\\forall a_i(l) \\\\in \\\\mathcal{A}`.\n\n        position_: int\n            Position id (slot) in a slate.\n\n        Returns\n        -----------\n        X, y: array-like, shape(n_rounds, )\n            Input and target vectors in prediction.\n\n        \"\"\"", "\n", "n_rounds_", "=", "len", "(", "context", ")", "\n", "# (n_rounds_, len_list) -> (n_rounds_, position_)", "\n", "action", "=", "action", "[", ":", ",", ":", "position_", "+", "1", "]", "\n", "# (n_rounds_, len_list) -> (n_rounds_, )", "\n", "reward", "=", "reward", "[", ":", ",", "position_", "]", "\n", "# estimator input", "\n", "X", "=", "np", ".", "concatenate", "(", "[", "context", ",", "action", "]", ",", "axis", "=", "1", ")", "\n", "# estimate the Q function at the next position", "\n", "# (n_rounds_, )", "\n", "if", "position_", "+", "1", "==", "self", ".", "len_list", ":", "\n", "            ", "q_hat_at_next_position", "=", "np", ".", "zeros", "(", "n_rounds_", ")", "\n", "", "else", ":", "\n", "# (n_rounds_ * len_list * n_unique_action, ) -> (n_rounds_, len_list, n_unique_action) -> (n_rounds_, len_list) -> (n_rounds_ * n_unique_action, )", "\n", "            ", "evaluation_policy_action_dist_at_next_position", "=", "(", "\n", "evaluation_policy_action_dist", ".", "reshape", "(", "\n", "(", "-", "1", ",", "self", ".", "len_list", ",", "self", ".", "n_unique_action", ")", "\n", ")", "[", ":", ",", "position_", "+", "1", ",", ":", "]", "\n", ")", ".", "flatten", "(", ")", "\n", "# (n_rounds_, dim_context) -> (n_rounds_ * n_unique_action, dim_context)", "\n", "context_", "=", "[", "]", "\n", "# (n_rounds_, position_ + 1) -> (n_rounds_ * n_unique_action, position_ + 1)", "\n", "action_", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "n_rounds_", ")", ":", "\n", "                ", "for", "a_", "in", "range", "(", "self", ".", "n_unique_action", ")", ":", "\n", "                    ", "context_", ".", "append", "(", "context", "[", "i", "]", ")", "\n", "action_", ".", "append", "(", "np", ".", "append", "(", "action", "[", "i", "]", ",", "a_", ")", ")", "\n", "", "", "X_", "=", "np", ".", "concatenate", "(", "[", "context_", ",", "action_", "]", ",", "axis", "=", "1", ")", "\n", "# (n_rounds_ * n_unique_action, ) -> (n_rounds_, )", "\n", "q_hat_at_next_position", "=", "self", ".", "base_model_list", "[", "position_", "+", "1", "]", ".", "predict", "(", "X_", ")", "\n", "# the expected Q function under the evaluation policy", "\n", "# (n_rounds_ * n_unique_action, ) -> (n_rounds_, n_unique_action) -> (n_rounds_, )", "\n", "q_hat_at_next_position", "=", "(", "\n", "(", "\n", "evaluation_policy_action_dist_at_next_position", "\n", "*", "q_hat_at_next_position", "\n", ")", "\n", ".", "reshape", "(", "(", "-", "1", ",", "self", ".", "n_unique_action", ")", ")", "\n", ".", "sum", "(", "axis", "=", "1", ")", "\n", ")", "\n", "# (n_rounds_, )", "\n", "", "y", "=", "reward", "+", "q_hat_at_next_position", "\n", "# (n_rounds_, dim_context + position_), (n_rounds_, )", "\n", "return", "X", ",", "y", "\n", "", "", ""]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta_slate.SlateOffPolicyEvaluation.__post_init__": [[117, 135], ["dict", "isinstance", "RuntimeError"], "methods", ["None"], ["def", "__post_init__", "(", "self", ")", "->", "None", ":", "\n", "        ", "\"\"\"Initialize class.\"\"\"", "\n", "for", "key_", "in", "[", "\n", "\"slate_id\"", ",", "\n", "\"context\"", ",", "\n", "\"action\"", ",", "\n", "\"reward\"", ",", "\n", "\"position\"", ",", "\n", "]", ":", "\n", "            ", "if", "key_", "not", "in", "self", ".", "bandit_feedback", ":", "\n", "                ", "raise", "RuntimeError", "(", "f\"Missing key of {key_} in 'bandit_feedback'.\"", ")", "\n", "\n", "", "", "self", ".", "ope_estimators_", "=", "dict", "(", ")", "\n", "self", ".", "use_cascade_dr", "=", "False", "\n", "for", "estimator", "in", "self", ".", "ope_estimators", ":", "\n", "            ", "self", ".", "ope_estimators_", "[", "estimator", ".", "estimator_name", "]", "=", "estimator", "\n", "if", "isinstance", "(", "estimator", ",", "CascadeDR", ")", ":", "\n", "                ", "self", ".", "use_cascade_dr", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta_slate.SlateOffPolicyEvaluation._create_estimator_inputs": [[136, 188], ["ValueError", "ValueError", "ValueError"], "methods", ["None"], ["", "", "", "def", "_create_estimator_inputs", "(", "\n", "self", ",", "\n", "evaluation_policy_pscore", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "evaluation_policy_pscore_item_position", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "evaluation_policy_pscore_cascade", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "evaluation_policy_action_dist", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "q_hat", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", ")", "->", "Dict", "[", "str", ",", "np", ".", "ndarray", "]", ":", "\n", "        ", "\"\"\"Create input dictionary to estimate policy value by subclasses of `BaseSlateOffPolicyEstimator`\"\"\"", "\n", "if", "(", "\n", "evaluation_policy_pscore", "is", "None", "\n", "and", "evaluation_policy_pscore_item_position", "is", "None", "\n", "and", "evaluation_policy_pscore_cascade", "is", "None", "\n", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"one of `evaluation_policy_pscore`, `evaluation_policy_pscore_item_position`, or `evaluation_policy_pscore_cascade` must be given\"", "\n", ")", "\n", "", "if", "self", ".", "use_cascade_dr", "and", "evaluation_policy_action_dist", "is", "None", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"`evaluation_policy_action_dist` must be given when using `SlateCascadeDoublyRobust`\"", "\n", ")", "\n", "", "if", "self", ".", "use_cascade_dr", "and", "q_hat", "is", "None", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"`q_hat` must be given when using `SlateCascadeDoublyRobust`\"", "\n", ")", "\n", "\n", "", "estimator_inputs", "=", "{", "\n", "input_", ":", "self", ".", "bandit_feedback", "[", "input_", "]", "\n", "for", "input_", "in", "[", "\n", "\"slate_id\"", ",", "\n", "\"action\"", ",", "\n", "\"reward\"", ",", "\n", "\"position\"", ",", "\n", "\"pscore\"", ",", "\n", "\"pscore_item_position\"", ",", "\n", "\"pscore_cascade\"", ",", "\n", "]", "\n", "if", "input_", "in", "self", ".", "bandit_feedback", "\n", "}", "\n", "estimator_inputs", "[", "\"evaluation_policy_pscore\"", "]", "=", "evaluation_policy_pscore", "\n", "estimator_inputs", "[", "\n", "\"evaluation_policy_pscore_item_position\"", "\n", "]", "=", "evaluation_policy_pscore_item_position", "\n", "estimator_inputs", "[", "\n", "\"evaluation_policy_pscore_cascade\"", "\n", "]", "=", "evaluation_policy_pscore_cascade", "\n", "estimator_inputs", "[", "\n", "\"evaluation_policy_action_dist\"", "\n", "]", "=", "evaluation_policy_action_dist", "\n", "estimator_inputs", "[", "\"q_hat\"", "]", "=", "q_hat", "\n", "\n", "return", "estimator_inputs", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta_slate.SlateOffPolicyEvaluation.estimate_policy_values": [[189, 243], ["dict", "meta_slate.SlateOffPolicyEvaluation._create_estimator_inputs", "meta_slate.SlateOffPolicyEvaluation.ope_estimators_.items", "estimator.estimate_policy_value"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta.OffPolicyEvaluation._create_estimator_inputs", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_policy_value"], ["", "def", "estimate_policy_values", "(", "\n", "self", ",", "\n", "evaluation_policy_pscore", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "evaluation_policy_pscore_item_position", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "evaluation_policy_pscore_cascade", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "evaluation_policy_action_dist", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "q_hat", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", ")", "->", "Dict", "[", "str", ",", "float", "]", ":", "\n", "        ", "\"\"\"Estimate the policy value of evaluation policy.\n\n        Parameters\n        ------------\n        evaluation_policy_pscore: array-like, shape (<= n_rounds * len_list,)\n            Joint probabilities of evaluation policy choosing a slate action, i.e., :math:`\\\\pi_e(a_i|x_i)`.\n            This parameter must be unique in each slate.\n\n        evaluation_policy_pscore_item_position: array-like, shape (<= n_rounds * len_list,)\n            Marginal probabilities of evaluation policy choosing a particular action at each position (slot),\n             i.e., :math:`\\\\pi_e(a_{i}(l) |x_i)`.\n\n        evaluation_policy_pscore_cascade: array-like, shape (n_rounds * len_list,)\n            Joint probabilities of evaluation policy choosing a particular sequence of actions from the top position to the :math:`l`-th position (:math:`a_{1:l}`). This type of action choice probabilities corresponds to the cascade model.\n\n        evaluation_policy_action_dist: array-like, shape (n_rounds * len_list * n_unique_action, )\n            Plackett-luce style action distribution induced by evaluation policy\n            (action choice probabilities at each slot given previous action choices)\n            , i.e., :math:`\\\\pi_e(a_i(l) | x_i, a_i(1), \\\\ldots, a_i(l-1)) \\\\forall a_i(l) \\\\in \\\\mathcal{A}`.\n            Required when using `obp.ope.SlateCascadeDoublyRobust`.\n\n        q_hat: array-like (n_rounds * len_list * n_unique_actions, )\n            :math:`\\\\hat{Q}_l` for all unique actions,\n            i.e., :math:`\\\\hat{Q}_{i,l}(x_i, a_i(1), \\\\ldots, a_i(l-1), a_i(l)) \\\\forall a_i(l) \\\\in \\\\mathcal{A}`.\n            Required when using `obp.ope.SlateCascadeDoublyRobust`.\n\n        Returns\n        ----------\n        policy_value_dict: Dict[str, float]\n            Dictionary containing the policy values estimated by OPE estimators.\n\n        \"\"\"", "\n", "policy_value_dict", "=", "dict", "(", ")", "\n", "estimator_inputs", "=", "self", ".", "_create_estimator_inputs", "(", "\n", "evaluation_policy_pscore", "=", "evaluation_policy_pscore", ",", "\n", "evaluation_policy_pscore_item_position", "=", "evaluation_policy_pscore_item_position", ",", "\n", "evaluation_policy_pscore_cascade", "=", "evaluation_policy_pscore_cascade", ",", "\n", "evaluation_policy_action_dist", "=", "evaluation_policy_action_dist", ",", "\n", "q_hat", "=", "q_hat", ",", "\n", ")", "\n", "for", "estimator_name", ",", "estimator", "in", "self", ".", "ope_estimators_", ".", "items", "(", ")", ":", "\n", "            ", "policy_value_dict", "[", "estimator_name", "]", "=", "estimator", ".", "estimate_policy_value", "(", "\n", "**", "estimator_inputs", "\n", ")", "\n", "\n", "", "return", "policy_value_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta_slate.SlateOffPolicyEvaluation.estimate_intervals": [[244, 317], ["utils.check_confidence_interval_arguments", "dict", "meta_slate.SlateOffPolicyEvaluation._create_estimator_inputs", "meta_slate.SlateOffPolicyEvaluation.ope_estimators_.items", "estimator.estimate_interval"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_confidence_interval_arguments", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta.OffPolicyEvaluation._create_estimator_inputs", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_interval"], ["", "def", "estimate_intervals", "(", "\n", "self", ",", "\n", "evaluation_policy_pscore", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "evaluation_policy_pscore_item_position", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "evaluation_policy_pscore_cascade", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "evaluation_policy_action_dist", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "q_hat", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "alpha", ":", "float", "=", "0.05", ",", "\n", "n_bootstrap_samples", ":", "int", "=", "100", ",", "\n", "random_state", ":", "Optional", "[", "int", "]", "=", "None", ",", "\n", ")", "->", "Dict", "[", "str", ",", "Dict", "[", "str", ",", "float", "]", "]", ":", "\n", "        ", "\"\"\"Estimate the confidence intervals of the policy values using bootstrap.\n\n        Parameters\n        ------------\n        evaluation_policy_pscore: array-like, shape (<= n_rounds * len_list,)\n            Joint probabilities of evaluation policy choosing a slate action, i.e., :math:`\\\\pi_e(a_i|x_i)`.\n            This parameter must be unique in each slate.\n\n        evaluation_policy_pscore_item_position: array-like, shape (<= n_rounds * len_list,)\n            Marginal probabilities of evaluation policy choosing a particular action at each position (slot),\n             i.e., :math:`\\\\pi_e(a_{i}(l) |x_i)`.\n\n        evaluation_policy_pscore_cascade: array-like, shape (n_rounds * len_list,)\n            Joint probabilities of evaluation policy choosing a particular sequence of actions from the top position to the :math:`l`-th position (:math:`a_{1:l}`). This type of action choice probabilities corresponds to the cascade model.\n\n        evaluation_policy_action_dist: array-like, shape (n_rounds * len_list * n_unique_action, )\n            Plackett-luce style action distribution induced by evaluation policy\n            (action choice probabilities at each slot given previous action choices)\n            , i.e., :math:`\\\\pi_e(a_i(l) | x_i, a_i(1), \\\\ldots, a_i(l-1)) \\\\forall a_i(l) \\\\in \\\\mathcal{A}`.\n\n        q_hat: array-like (n_rounds * len_list * n_unique_actions, )\n            :math:`\\\\hat{Q}_l` for all unique actions,\n            i.e., :math:`\\\\hat{Q}_{i,l}(x_i, a_i(1), \\\\ldots, a_i(l-1), a_i(l)) \\\\forall a_i(l) \\\\in \\\\mathcal{A}`.\n            Required when using `obp.ope.SlateCascadeDoublyRobust`.\n\n        alpha: float, default=0.05\n            Significance level.\n\n        n_bootstrap_samples: int, default=100\n            Number of resampling performed in bootstrap sampling.\n\n        random_state: int, default=None\n            Controls the random seed in bootstrap sampling.\n\n        Returns\n        ----------\n        policy_value_interval_dict: Dict[str, Dict[str, float]]\n            Dictionary containing confidence intervals of the estimated policy values.\n\n        \"\"\"", "\n", "check_confidence_interval_arguments", "(", "\n", "alpha", "=", "alpha", ",", "\n", "n_bootstrap_samples", "=", "n_bootstrap_samples", ",", "\n", "random_state", "=", "random_state", ",", "\n", ")", "\n", "policy_value_interval_dict", "=", "dict", "(", ")", "\n", "estimator_inputs", "=", "self", ".", "_create_estimator_inputs", "(", "\n", "evaluation_policy_pscore", "=", "evaluation_policy_pscore", ",", "\n", "evaluation_policy_pscore_item_position", "=", "evaluation_policy_pscore_item_position", ",", "\n", "evaluation_policy_pscore_cascade", "=", "evaluation_policy_pscore_cascade", ",", "\n", "evaluation_policy_action_dist", "=", "evaluation_policy_action_dist", ",", "\n", "q_hat", "=", "q_hat", ",", "\n", ")", "\n", "for", "estimator_name", ",", "estimator", "in", "self", ".", "ope_estimators_", ".", "items", "(", ")", ":", "\n", "            ", "policy_value_interval_dict", "[", "estimator_name", "]", "=", "estimator", ".", "estimate_interval", "(", "\n", "**", "estimator_inputs", ",", "\n", "alpha", "=", "alpha", ",", "\n", "n_bootstrap_samples", "=", "n_bootstrap_samples", ",", "\n", "random_state", "=", "random_state", ",", "\n", ")", "\n", "\n", "", "return", "policy_value_interval_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta_slate.SlateOffPolicyEvaluation.summarize_off_policy_estimates": [[318, 407], ["pandas.DataFrame", "pandas.DataFrame", "meta_slate.SlateOffPolicyEvaluation.estimate_policy_values", "meta_slate.SlateOffPolicyEvaluation.estimate_intervals", "meta_slate.SlateOffPolicyEvaluation.bandit_feedback[].sum", "logger.warning", "numpy.unique"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta.OffPolicyEvaluation.estimate_policy_values", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta.OffPolicyEvaluation.estimate_intervals"], ["", "def", "summarize_off_policy_estimates", "(", "\n", "self", ",", "\n", "evaluation_policy_pscore", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "evaluation_policy_pscore_item_position", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "evaluation_policy_pscore_cascade", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "evaluation_policy_action_dist", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "q_hat", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "alpha", ":", "float", "=", "0.05", ",", "\n", "n_bootstrap_samples", ":", "int", "=", "100", ",", "\n", "random_state", ":", "Optional", "[", "int", "]", "=", "None", ",", "\n", ")", "->", "Tuple", "[", "DataFrame", ",", "DataFrame", "]", ":", "\n", "        ", "\"\"\"Summarize the estimated policy values and their confidence intervals estimated by bootstrap.\n\n        Parameters\n        ------------\n        evaluation_policy_pscore: array-like, shape (<= n_rounds * len_list,)\n            Joint probabilities of evaluation policy choosing a slate action, i.e., :math:`\\\\pi_e(a_i|x_i)`.\n            This parameter must be unique in each slate.\n\n        evaluation_policy_pscore_item_position: array-like, shape (<= n_rounds * len_list,)\n            Marginal probabilities of evaluation policy choosing a particular action at each position (slot),\n             i.e., :math:`\\\\pi_e(a_{i}(l) |x_i)`.\n\n        evaluation_policy_pscore_cascade: array-like, shape (n_rounds * len_list,)\n            Joint probabilities of evaluation policy choosing a particular sequence of actions from the top position to the :math:`l`-th position (:math:`a_{1:l}`). This type of action choice probabilities corresponds to the cascade model.\n\n\n        evaluation_policy_action_dist: array-like, shape (n_rounds * len_list * n_unique_action, )\n            Plackett-luce style action distribution induced by evaluation policy\n            (action choice probabilities at each slot given previous action choices)\n            , i.e., :math:`\\\\pi_e(a_i(l) | x_i, a_i(1), \\\\ldots, a_i(l-1)) \\\\forall a_i(l) \\\\in \\\\mathcal{A}`.\n\n        q_hat: array-like (n_rounds * len_list * n_unique_actions, )\n            :math:`\\\\hat{Q}_l` for all unique actions,\n            i.e., :math:`\\\\hat{Q}_{i,l}(x_i, a_i(1), \\\\ldots, a_i(l-1), a_i(l)) \\\\forall a_i(l) \\\\in \\\\mathcal{A}`.\n            Required when using `obp.ope.SlateCascadeDoublyRobust`.\n\n        alpha: float, default=0.05\n            Significance level.\n\n        n_bootstrap_samples: int, default=100\n            Number of resampling performed in bootstrap sampling.\n\n        random_state: int, default=None\n            Controls the random seed in bootstrap sampling.\n\n        Returns\n        ----------\n        (policy_value_df, policy_value_interval_df): Tuple[DataFrame, DataFrame]\n            Policy values and their confidence intervals estimated by OPE estimators.\n\n        \"\"\"", "\n", "policy_value_df", "=", "DataFrame", "(", "\n", "self", ".", "estimate_policy_values", "(", "\n", "evaluation_policy_pscore", "=", "evaluation_policy_pscore", ",", "\n", "evaluation_policy_pscore_item_position", "=", "evaluation_policy_pscore_item_position", ",", "\n", "evaluation_policy_pscore_cascade", "=", "evaluation_policy_pscore_cascade", ",", "\n", "evaluation_policy_action_dist", "=", "evaluation_policy_action_dist", ",", "\n", "q_hat", "=", "q_hat", ",", "\n", ")", ",", "\n", "index", "=", "[", "\"estimated_policy_value\"", "]", ",", "\n", ")", "\n", "policy_value_interval_df", "=", "DataFrame", "(", "\n", "self", ".", "estimate_intervals", "(", "\n", "evaluation_policy_pscore", "=", "evaluation_policy_pscore", ",", "\n", "evaluation_policy_pscore_item_position", "=", "evaluation_policy_pscore_item_position", ",", "\n", "evaluation_policy_pscore_cascade", "=", "evaluation_policy_pscore_cascade", ",", "\n", "evaluation_policy_action_dist", "=", "evaluation_policy_action_dist", ",", "\n", "q_hat", "=", "q_hat", ",", "\n", "alpha", "=", "alpha", ",", "\n", "n_bootstrap_samples", "=", "n_bootstrap_samples", ",", "\n", "random_state", "=", "random_state", ",", "\n", ")", "\n", ")", "\n", "policy_value_of_behavior_policy", "=", "(", "\n", "self", ".", "bandit_feedback", "[", "\"reward\"", "]", ".", "sum", "(", ")", "\n", "/", "np", ".", "unique", "(", "self", ".", "bandit_feedback", "[", "\"slate_id\"", "]", ")", ".", "shape", "[", "0", "]", "\n", ")", "\n", "policy_value_df", "=", "policy_value_df", ".", "T", "\n", "if", "policy_value_of_behavior_policy", "<=", "0", ":", "\n", "            ", "logger", ".", "warning", "(", "\n", "f\"Policy value of the behavior policy is {policy_value_of_behavior_policy} (<=0); relative estimated policy value is set to np.nan\"", "\n", ")", "\n", "policy_value_df", "[", "\"relative_estimated_policy_value\"", "]", "=", "np", ".", "nan", "\n", "", "else", ":", "\n", "            ", "policy_value_df", "[", "\"relative_estimated_policy_value\"", "]", "=", "(", "\n", "policy_value_df", ".", "estimated_policy_value", "/", "policy_value_of_behavior_policy", "\n", ")", "\n", "", "return", "policy_value_df", ",", "policy_value_interval_df", ".", "T", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta_slate.SlateOffPolicyEvaluation.visualize_off_policy_estimates": [[408, 517], ["meta_slate.SlateOffPolicyEvaluation.summarize_off_policy_estimates", "estimated_interval_a.drop().diff().iloc[].abs", "matplotlib.style.use", "matplotlib.subplots", "seaborn.barplot", "matplotlib.xlabel", "matplotlib.ylabel", "matplotlib.yticks", "matplotlib.xticks", "ax.errorbar", "isinstance", "isinstance", "numpy.arange", "fig.savefig", "meta_slate.SlateOffPolicyEvaluation.bandit_feedback[].sum", "estimated_interval_a[].reset_index", "str", "numpy.int32", "estimated_interval_a.drop().diff", "numpy.unique", "len", "estimated_interval_a.drop"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta.OffPolicyEvaluation.summarize_off_policy_estimates"], ["", "def", "visualize_off_policy_estimates", "(", "\n", "self", ",", "\n", "evaluation_policy_pscore", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "evaluation_policy_pscore_item_position", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "evaluation_policy_pscore_cascade", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "evaluation_policy_action_dist", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "q_hat", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "alpha", ":", "float", "=", "0.05", ",", "\n", "is_relative", ":", "bool", "=", "False", ",", "\n", "n_bootstrap_samples", ":", "int", "=", "100", ",", "\n", "random_state", ":", "Optional", "[", "int", "]", "=", "None", ",", "\n", "fig_dir", ":", "Optional", "[", "Path", "]", "=", "None", ",", "\n", "fig_name", ":", "str", "=", "\"estimated_policy_value.png\"", ",", "\n", ")", "->", "None", ":", "\n", "        ", "\"\"\"Visualize the estimated policy values.\n\n        Parameters\n        ----------\n        evaluation_policy_pscore: array-like, shape (<= n_rounds * len_list,)\n            Joint probabilities of evaluation policy choosing a slate action, i.e., :math:`\\\\pi_e(a_i|x_i)`.\n            This parameter must be unique in each slate.\n\n        evaluation_policy_pscore_item_position: array-like, shape (<= n_rounds * len_list,)\n            Marginal probabilities of evaluation policy choosing a particular action at each position (slot),\n             i.e., :math:`\\\\pi_e(a_{i}(l) |x_i)`.\n\n        evaluation_policy_pscore_cascade: array-like, shape (n_rounds * len_list,)\n            Joint probabilities of evaluation policy choosing a particular sequence of actions from the top position to the :math:`l`-th position (:math:`a_{1:l}`). This type of action choice probabilities corresponds to the cascade model.\n\n        evaluation_policy_action_dist: array-like, shape (n_rounds * len_list * n_unique_action, )\n            Plackett-luce style action distribution induced by evaluation policy\n            (action choice probabilities at each slot given previous action choices)\n            , i.e., :math:`\\\\pi_e(a_i(l) | x_i, a_i(1), \\\\ldots, a_i(l-1)) \\\\forall a_i(l) \\\\in \\\\mathcal{A}`.\n\n        q_hat: array-like (n_rounds * len_list * n_unique_actions, )\n            :math:`\\\\hat{Q}_l` for all unique actions,\n            i.e., :math:`\\\\hat{Q}_{i,l}(x_i, a_i(1), \\\\ldots, a_i(l-1), a_i(l)) \\\\forall a_i(l) \\\\in \\\\mathcal{A}`.\n            Required when using `obp.ope.SlateCascadeDoublyRobust`.\n\n        alpha: float, default=0.05\n            Significance level.\n\n        n_bootstrap_samples: int, default=100\n            Number of resampling performed in bootstrap sampling.\n\n        random_state: int, default=None\n            Controls the random seed in bootstrap sampling.\n\n        is_relative: bool, default=False,\n            If True, the method visualizes the estimated policy values of evaluation policy\n            relative to the ground-truth policy value of behavior policy.\n\n        fig_dir: Path, default=None\n            Path to store the bar figure.\n            If None, the figure will not be saved.\n\n        fig_name: str, default=\"estimated_policy_value.png\"\n            Name of the bar figure.\n\n        \"\"\"", "\n", "if", "fig_dir", "is", "not", "None", ":", "\n", "            ", "assert", "isinstance", "(", "fig_dir", ",", "Path", ")", ",", "\"`fig_dir` must be a Path\"", "\n", "", "if", "fig_name", "is", "not", "None", ":", "\n", "            ", "assert", "isinstance", "(", "fig_name", ",", "str", ")", ",", "\"`fig_dir` must be a string\"", "\n", "\n", "", "_", ",", "estimated_interval_a", "=", "self", ".", "summarize_off_policy_estimates", "(", "\n", "evaluation_policy_pscore", "=", "evaluation_policy_pscore", ",", "\n", "evaluation_policy_pscore_item_position", "=", "evaluation_policy_pscore_item_position", ",", "\n", "evaluation_policy_pscore_cascade", "=", "evaluation_policy_pscore_cascade", ",", "\n", "evaluation_policy_action_dist", "=", "evaluation_policy_action_dist", ",", "\n", "q_hat", "=", "q_hat", ",", "\n", "alpha", "=", "alpha", ",", "\n", "n_bootstrap_samples", "=", "n_bootstrap_samples", ",", "\n", "random_state", "=", "random_state", ",", "\n", ")", "\n", "estimated_interval_a", "[", "\"errbar_length\"", "]", "=", "(", "\n", "estimated_interval_a", ".", "drop", "(", "\"mean\"", ",", "axis", "=", "1", ")", ".", "diff", "(", "axis", "=", "1", ")", ".", "iloc", "[", ":", ",", "-", "1", "]", ".", "abs", "(", ")", "\n", ")", "\n", "if", "is_relative", ":", "\n", "            ", "estimated_interval_a", "/=", "(", "\n", "self", ".", "bandit_feedback", "[", "\"reward\"", "]", ".", "sum", "(", ")", "\n", "/", "np", ".", "unique", "(", "self", ".", "bandit_feedback", "[", "\"slate_id\"", "]", ")", ".", "shape", "[", "0", "]", "\n", ")", "\n", "\n", "", "plt", ".", "style", ".", "use", "(", "\"ggplot\"", ")", "\n", "fig", ",", "ax", "=", "plt", ".", "subplots", "(", "figsize", "=", "(", "8", ",", "6", ")", ")", "\n", "sns", ".", "barplot", "(", "\n", "data", "=", "estimated_interval_a", "[", "[", "\"mean\"", "]", "]", ".", "reset_index", "(", ")", ",", "\n", "x", "=", "\"index\"", ",", "\n", "y", "=", "\"mean\"", ",", "\n", "ax", "=", "ax", ",", "\n", "ci", "=", "None", ",", "\n", ")", "\n", "plt", ".", "xlabel", "(", "\"OPE Estimators\"", ",", "fontsize", "=", "25", ")", "\n", "plt", ".", "ylabel", "(", "\n", "f\"Estimated Policy Value (\u00b1 {np.int32(100*(1 - alpha))}% CI)\"", ",", "fontsize", "=", "20", "\n", ")", "\n", "plt", ".", "yticks", "(", "fontsize", "=", "15", ")", "\n", "plt", ".", "xticks", "(", "fontsize", "=", "25", "-", "2", "*", "len", "(", "self", ".", "ope_estimators", ")", ")", "\n", "ax", ".", "errorbar", "(", "\n", "np", ".", "arange", "(", "estimated_interval_a", ".", "shape", "[", "0", "]", ")", ",", "\n", "estimated_interval_a", "[", "\"mean\"", "]", ",", "\n", "yerr", "=", "estimated_interval_a", "[", "\"errbar_length\"", "]", ",", "\n", "fmt", "=", "\"o\"", ",", "\n", "color", "=", "\"black\"", ",", "\n", ")", "\n", "\n", "if", "fig_dir", ":", "\n", "            ", "fig", ".", "savefig", "(", "str", "(", "fig_dir", "/", "fig_name", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta_slate.SlateOffPolicyEvaluation.evaluate_performance_of_estimators": [[518, 611], ["sklearn.utils.check_scalar", "dict", "meta_slate.SlateOffPolicyEvaluation._create_estimator_inputs", "meta_slate.SlateOffPolicyEvaluation.ope_estimators_.items", "ValueError", "ValueError", "estimator.estimate_policy_value", "numpy.abs"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta.OffPolicyEvaluation._create_estimator_inputs", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_policy_value"], ["", "", "def", "evaluate_performance_of_estimators", "(", "\n", "self", ",", "\n", "ground_truth_policy_value", ":", "float", ",", "\n", "evaluation_policy_pscore", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "evaluation_policy_pscore_item_position", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "evaluation_policy_pscore_cascade", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "evaluation_policy_action_dist", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "q_hat", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "metric", ":", "str", "=", "\"se\"", ",", "\n", ")", "->", "Dict", "[", "str", ",", "float", "]", ":", "\n", "        ", "\"\"\"Evaluate the accuracy of OPE estimators.\n\n        Note\n        ------\n        Evaluate the estimation performance of OPE estimators with relative estimation error (relative-EE) or squared error (SE):\n\n        .. math ::\n\n            \\\\text{Relative-EE} (\\\\hat{V}; \\\\mathcal{D}) = \\\\left|  \\\\frac{\\\\hat{V}(\\\\pi; \\\\mathcal{D}) - V(\\\\pi)}{V(\\\\pi)} \\\\right|,\n\n        .. math ::\n\n            \\\\text{SE} (\\\\hat{V}; \\\\mathcal{D}) = \\\\left(\\\\hat{V}(\\\\pi; \\\\mathcal{D}) - V(\\\\pi) \\\\right)^2,\n\n        where :math:`V({\\\\pi})` is the ground-truth policy value of the evalation policy :math:`\\\\pi_e` (often estimated using on-policy estimation).\n        :math:`\\\\hat{V}(\\\\pi; \\\\mathcal{D})` is the policy value estimated by an OPE estimator :math:`\\\\hat{V}` and logged bandit feedback :math:`\\\\mathcal{D}`.\n\n        Parameters\n        ----------\n        ground_truth_policy_value: float\n            Ground_truth policy value of evaluation policy, i.e., :math:`V(\\\\pi)`.\n            With Open Bandit Dataset, we use an on-policy estimate of the policy value as its ground-truth.\n\n        evaluation_policy_pscore: array-like, shape (<= n_rounds * len_list,)\n            Joint probabilities of evaluation policy choosing a slate action, i.e., :math:`\\\\pi_e(a_i|x_i)`.\n            This parameter must be unique in each slate.\n\n        evaluation_policy_pscore_item_position: array-like, shape (<= n_rounds * len_list,)\n            Marginal probabilities of evaluation policy choosing a particular action at each position (slot),\n             i.e., :math:`\\\\pi_e(a_{i}(l) |x_i)`.\n\n        evaluation_policy_pscore_cascade: array-like, shape (n_rounds * len_list,)\n            Joint probabilities of evaluation policy choosing a particular sequence of actions from the top position to the :math:`l`-th position (:math:`a_{1:l}`). This type of action choice probabilities corresponds to the cascade model.\n\n\n        evaluation_policy_action_dist: array-like, shape (n_rounds * len_list * n_unique_action, )\n            Plackett-luce style action distribution induced by evaluation policy\n            (action choice probabilities at each slot given previous action choices)\n            , i.e., :math:`\\\\pi_e(a_i(l) | x_i, a_i(1), \\\\ldots, a_i(l-1)) \\\\forall a_i(l) \\\\in \\\\mathcal{A}`.\n\n        q_hat: array-like (n_rounds * len_list * n_unique_actions, )\n            :math:`\\\\hat{Q}_l` for all unique actions,\n            i.e., :math:`\\\\hat{Q}_{i,l}(x_i, a_i(1), \\\\ldots, a_i(l-1), a_i(l)) \\\\forall a_i(l) \\\\in \\\\mathcal{A}`.\n            Required when using `obp.ope.SlateCascadeDoublyRobust`.\n\n        metric: str, default=\"se\"\n            Evaluation metric used to evaluate and compare the estimation performance of OPE estimators.\n            Must be either \"relative-ee\" or \"se\".\n\n        Returns\n        ----------\n        eval_metric_ope_dict: Dict[str, float]\n            Dictionary containing the value of evaluation metric for the estimation performance of OPE estimators.\n\n        \"\"\"", "\n", "check_scalar", "(", "ground_truth_policy_value", ",", "\"ground_truth_policy_value\"", ",", "float", ")", "\n", "if", "metric", "not", "in", "[", "\"relative-ee\"", ",", "\"se\"", "]", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "f\"`metric` must be either 'relative-ee' or 'se', but {metric} is given\"", "\n", ")", "\n", "", "if", "metric", "==", "\"relative-ee\"", "and", "ground_truth_policy_value", "==", "0.0", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"`ground_truth_policy_value` must be non-zero when metric is relative-ee\"", "\n", ")", "\n", "\n", "", "eval_metric_ope_dict", "=", "dict", "(", ")", "\n", "estimator_inputs", "=", "self", ".", "_create_estimator_inputs", "(", "\n", "evaluation_policy_pscore", "=", "evaluation_policy_pscore", ",", "\n", "evaluation_policy_pscore_item_position", "=", "evaluation_policy_pscore_item_position", ",", "\n", "evaluation_policy_pscore_cascade", "=", "evaluation_policy_pscore_cascade", ",", "\n", "evaluation_policy_action_dist", "=", "evaluation_policy_action_dist", ",", "\n", "q_hat", "=", "q_hat", ",", "\n", ")", "\n", "for", "estimator_name", ",", "estimator", "in", "self", ".", "ope_estimators_", ".", "items", "(", ")", ":", "\n", "            ", "estimated_policy_value", "=", "estimator", ".", "estimate_policy_value", "(", "**", "estimator_inputs", ")", "\n", "if", "metric", "==", "\"relative-ee\"", ":", "\n", "                ", "relative_ee_", "=", "estimated_policy_value", "-", "ground_truth_policy_value", "\n", "relative_ee_", "/=", "ground_truth_policy_value", "\n", "eval_metric_ope_dict", "[", "estimator_name", "]", "=", "np", ".", "abs", "(", "relative_ee_", ")", "\n", "", "elif", "metric", "==", "\"se\"", ":", "\n", "                ", "se_", "=", "(", "estimated_policy_value", "-", "ground_truth_policy_value", ")", "**", "2", "\n", "eval_metric_ope_dict", "[", "estimator_name", "]", "=", "se_", "\n", "", "", "return", "eval_metric_ope_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta_slate.SlateOffPolicyEvaluation.summarize_estimators_comparison": [[612, 674], ["pandas.DataFrame", "meta_slate.SlateOffPolicyEvaluation.evaluate_performance_of_estimators"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta.OffPolicyEvaluation.evaluate_performance_of_estimators"], ["", "def", "summarize_estimators_comparison", "(", "\n", "self", ",", "\n", "ground_truth_policy_value", ":", "float", ",", "\n", "evaluation_policy_pscore", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "evaluation_policy_pscore_item_position", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "evaluation_policy_pscore_cascade", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "evaluation_policy_action_dist", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "q_hat", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "metric", ":", "str", "=", "\"se\"", ",", "\n", ")", "->", "DataFrame", ":", "\n", "        ", "\"\"\"Summarize the performance comparison among OPE estimators.\n\n        Parameters\n        ----------\n        ground_truth_policy_value: float\n            Ground_truth policy value of evaluation policy, i.e., :math:`V(\\\\pi)`.\n            With Open Bandit Dataset, we use an on-policy estimate of the policy value as ground-truth.\n\n        evaluation_policy_pscore: array-like, shape (<= n_rounds * len_list,)\n            Joint probabilities of evaluation policy choosing a slate action, i.e., :math:`\\\\pi_e(a_i|x_i)`.\n            This parameter must be unique in each slate.\n\n        evaluation_policy_pscore_item_position: array-like, shape (<= n_rounds * len_list,)\n            Marginal probabilities of evaluation policy choosing a particular action at each position (slot),\n             i.e., :math:`\\\\pi_e(a_{i}(l) |x_i)`.\n\n        evaluation_policy_pscore_cascade: array-like, shape (n_rounds * len_list,)\n            Joint probabilities of evaluation policy choosing a particular sequence of actions from the top position to the :math:`l`-th position (:math:`a_{1:l}`). This type of action choice probabilities corresponds to the cascade model.\n\n        evaluation_policy_action_dist: array-like, shape (n_rounds * len_list * n_unique_action, )\n            Plackett-luce style action distribution induced by evaluation policy\n            (action choice probabilities at each slot given previous action choices)\n            , i.e., :math:`\\\\pi_e(a_i(l) | x_i, a_i(1), \\\\ldots, a_i(l-1)) \\\\forall a_i(l) \\\\in \\\\mathcal{A}`.\n\n        q_hat: array-like (n_rounds * len_list * n_unique_actions, )\n            :math:`\\\\hat{Q}_l` for all unique actions,\n            i.e., :math:`\\\\hat{Q}_{i,l}(x_i, a_i(1), \\\\ldots, a_i(l-1), a_i(l)) \\\\forall a_i(l) \\\\in \\\\mathcal{A}`.\n            Required when using `obp.ope.SlateCascadeDoublyRobust`.\n\n        metric: str, default=\"se\"\n            Evaluation metric used to evaluate and compare the estimation performance of OPE estimators.\n            Must be either \"relative-ee\" or \"se\".\n\n        Returns\n        ----------\n        eval_metric_ope_df: DataFrame\n            Results of performance comparison among OPE estimators.\n\n        \"\"\"", "\n", "eval_metric_ope_df", "=", "DataFrame", "(", "\n", "self", ".", "evaluate_performance_of_estimators", "(", "\n", "ground_truth_policy_value", "=", "ground_truth_policy_value", ",", "\n", "evaluation_policy_pscore", "=", "evaluation_policy_pscore", ",", "\n", "evaluation_policy_pscore_item_position", "=", "evaluation_policy_pscore_item_position", ",", "\n", "evaluation_policy_pscore_cascade", "=", "evaluation_policy_pscore_cascade", ",", "\n", "evaluation_policy_action_dist", "=", "evaluation_policy_action_dist", ",", "\n", "q_hat", "=", "q_hat", ",", "\n", "metric", "=", "metric", ",", "\n", ")", ",", "\n", "index", "=", "[", "metric", "]", ",", "\n", ")", "\n", "return", "eval_metric_ope_df", ".", "T", "\n", "", "", ""]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_continuous.BaseContinuousOffPolicyEstimator._estimate_round_rewards": [[57, 61], ["None"], "methods", ["None"], ["@", "abstractmethod", "\n", "def", "_estimate_round_rewards", "(", "self", ")", "->", "np", ".", "ndarray", ":", "\n", "        ", "\"\"\"Estimate round-wise (or sample-wise) rewards.\"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_continuous.BaseContinuousOffPolicyEstimator.estimate_policy_value": [[62, 66], ["None"], "methods", ["None"], ["", "@", "abstractmethod", "\n", "def", "estimate_policy_value", "(", "self", ")", "->", "float", ":", "\n", "        ", "\"\"\"Estimate policy value of evaluation policy.\"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_continuous.BaseContinuousOffPolicyEstimator.estimate_interval": [[67, 71], ["None"], "methods", ["None"], ["", "@", "abstractmethod", "\n", "def", "estimate_interval", "(", "self", ")", "->", "Dict", "[", "str", ",", "float", "]", ":", "\n", "        ", "\"\"\"Estimate the confidence interval of the policy value using bootstrap.\"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_continuous.KernelizedInverseProbabilityWeighting.__post_init__": [[118, 125], ["sklearn.utils.check_scalar", "ValueError"], "methods", ["None"], ["def", "__post_init__", "(", "self", ")", "->", "None", ":", "\n", "        ", "if", "self", ".", "kernel", "not", "in", "[", "\"gaussian\"", ",", "\"epanechnikov\"", ",", "\"triangular\"", ",", "\"cosine\"", "]", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "f\"kernel must be one of 'gaussian', 'epanechnikov', 'triangular', or 'cosine' but {self.kernel} is given\"", "\n", ")", "\n", "", "check_scalar", "(", "\n", "self", ".", "bandwidth", ",", "name", "=", "\"bandwidth\"", ",", "target_type", "=", "(", "int", ",", "float", ")", ",", "min_val", "=", "0", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_continuous.KernelizedInverseProbabilityWeighting._estimate_round_rewards": [[127, 164], ["kernel_func"], "methods", ["None"], ["", "def", "_estimate_round_rewards", "(", "\n", "self", ",", "\n", "reward", ":", "np", ".", "ndarray", ",", "\n", "action_by_behavior_policy", ":", "np", ".", "ndarray", ",", "\n", "pscore", ":", "np", ".", "ndarray", ",", "\n", "action_by_evaluation_policy", ":", "np", ".", "ndarray", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "np", ".", "ndarray", ":", "\n", "        ", "\"\"\"Estimate round-wise (or sample-wise) rewards.\n\n        Parameters\n        ----------\n        reward: array-like, shape (n_rounds,)\n            Rewards observed for each data in logged bandit data, i.e., :math:`r_i`.\n\n        action_by_behavior_policy: array-like, shape (n_rounds,)\n            Continuous action values sampled by behavior policy for each data in logged bandit data, i.e., :math:`a_i`.\n\n        pscore: array-like, shape (n_rounds,)\n            Probability densities of the continuous action values sampled by behavior policy\n            (generalized propensity scores), i.e., :math:`\\\\pi_b(a_i|x_i)`.\n\n        action_by_evaluation_policy: array-like, shape (n_rounds,)\n            Continuous action values given by evaluation policy (can be deterministic), i.e., :math:`\\\\pi_e(x_t)`.\n\n        Returns\n        ----------\n        estimated_rewards: array-like, shape (n_rounds,)\n            Estimated rewards for each observation.\n\n        \"\"\"", "\n", "kernel_func", "=", "kernel_functions", "[", "self", ".", "kernel", "]", "\n", "u", "=", "action_by_evaluation_policy", "-", "action_by_behavior_policy", "\n", "u", "/=", "self", ".", "bandwidth", "\n", "estimated_rewards", "=", "kernel_func", "(", "u", ")", "*", "reward", "/", "pscore", "\n", "estimated_rewards", "/=", "self", ".", "bandwidth", "\n", "return", "estimated_rewards", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_continuous.KernelizedInverseProbabilityWeighting.estimate_policy_value": [[165, 215], ["utils.check_array", "utils.check_array", "utils.check_array", "utils.check_continuous_ope_inputs", "estimators_continuous.KernelizedInverseProbabilityWeighting._estimate_round_rewards().mean", "estimators_continuous.KernelizedInverseProbabilityWeighting._estimate_round_rewards"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_continuous_ope_inputs", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SelfNormalizedSlateStandardIPS._estimate_round_rewards"], ["", "def", "estimate_policy_value", "(", "\n", "self", ",", "\n", "reward", ":", "np", ".", "ndarray", ",", "\n", "action_by_behavior_policy", ":", "np", ".", "ndarray", ",", "\n", "pscore", ":", "np", ".", "ndarray", ",", "\n", "action_by_evaluation_policy", ":", "np", ".", "ndarray", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "np", ".", "ndarray", ":", "\n", "        ", "\"\"\"Estimate policy value of evaluation policy.\n\n        Parameters\n        ----------\n        reward: array-like, shape (n_rounds,)\n            Rewards observed for each data in logged bandit data, i.e., :math:`r_i`.\n\n        action_by_behavior_policy: array-like, shape (n_rounds,)\n            Continuous action values sampled by behavior policy for each data in logged bandit data, i.e., :math:`a_i`.\n\n        pscore: array-like, shape (n_rounds,)\n            Probability densities of the continuous action values sampled by behavior policy\n            (generalized propensity scores), i.e., :math:`\\\\pi_b(a_i|x_i)`.\n\n        action_by_evaluation_policy: array-like, shape (n_rounds,)\n            Continuous action values given by evaluation policy (can be deterministic), i.e., :math:`\\\\pi_e(x_t)`.\n\n        Returns\n        ----------\n        V_hat: float\n            Estimated policy value of evaluation policy.\n\n        \"\"\"", "\n", "check_array", "(", "array", "=", "reward", ",", "name", "=", "\"reward\"", ",", "expected_dim", "=", "1", ")", "\n", "check_array", "(", "\n", "array", "=", "action_by_behavior_policy", ",", "\n", "name", "=", "\"action_by_behavior_policy\"", ",", "\n", "expected_dim", "=", "1", ",", "\n", ")", "\n", "check_array", "(", "array", "=", "pscore", ",", "name", "=", "\"pscore\"", ",", "expected_dim", "=", "1", ")", "\n", "check_continuous_ope_inputs", "(", "\n", "reward", "=", "reward", ",", "\n", "action_by_behavior_policy", "=", "action_by_behavior_policy", ",", "\n", "pscore", "=", "pscore", ",", "\n", "action_by_evaluation_policy", "=", "action_by_evaluation_policy", ",", "\n", ")", "\n", "\n", "return", "self", ".", "_estimate_round_rewards", "(", "\n", "reward", "=", "reward", ",", "\n", "action_by_behavior_policy", "=", "action_by_behavior_policy", ",", "\n", "pscore", "=", "pscore", ",", "\n", "action_by_evaluation_policy", "=", "action_by_evaluation_policy", ",", "\n", ")", ".", "mean", "(", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_continuous.KernelizedInverseProbabilityWeighting.estimate_interval": [[217, 286], ["utils.check_array", "utils.check_array", "utils.check_array", "utils.check_continuous_ope_inputs", "estimators_continuous.KernelizedInverseProbabilityWeighting._estimate_round_rewards", "utils.estimate_confidence_interval_by_bootstrap"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_continuous_ope_inputs", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SelfNormalizedSlateStandardIPS._estimate_round_rewards", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.estimate_confidence_interval_by_bootstrap"], ["", "def", "estimate_interval", "(", "\n", "self", ",", "\n", "reward", ":", "np", ".", "ndarray", ",", "\n", "action_by_behavior_policy", ":", "np", ".", "ndarray", ",", "\n", "pscore", ":", "np", ".", "ndarray", ",", "\n", "action_by_evaluation_policy", ":", "np", ".", "ndarray", ",", "\n", "alpha", ":", "float", "=", "0.05", ",", "\n", "n_bootstrap_samples", ":", "int", "=", "10000", ",", "\n", "random_state", ":", "Optional", "[", "int", "]", "=", "None", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "Dict", "[", "str", ",", "float", "]", ":", "\n", "        ", "\"\"\"Estimate the confidence interval of the policy value using bootstrap.\n\n        Parameters\n        ----------\n        reward: array-like, shape (n_rounds,)\n            Rewards observed for each data in logged bandit data, i.e., :math:`r_i`.\n\n        action_by_behavior_policy: array-like, shape (n_rounds,)\n            Continuous action values sampled by behavior policy for each data in logged bandit data, i.e., :math:`a_i`.\n\n        pscore: array-like, shape (n_rounds,)\n            Probability densities of the continuous action values sampled by behavior policy\n            (generalized propensity scores), i.e., :math:`\\\\pi_b(a_i|x_i)`.\n\n        action_by_evaluation_policy: array-like, shape (n_rounds,)\n            Continuous action values given by evaluation policy (can be deterministic), i.e., :math:`\\\\pi_e(x_t)`.\n\n        alpha: float, default=0.05\n            Significance level.\n\n        n_bootstrap_samples: int, default=10000\n            Number of resampling performed in bootstrap sampling.\n\n        random_state: int, default=None\n            Controls the random seed in bootstrap sampling.\n\n        Returns\n        ----------\n        estimated_confidence_interval: Dict[str, float]\n            Dictionary storing the estimated mean and upper-lower confidence bounds.\n\n        \"\"\"", "\n", "check_array", "(", "array", "=", "reward", ",", "name", "=", "\"reward\"", ",", "expected_dim", "=", "1", ")", "\n", "check_array", "(", "\n", "array", "=", "action_by_behavior_policy", ",", "\n", "name", "=", "\"action_by_behavior_policy\"", ",", "\n", "expected_dim", "=", "1", ",", "\n", ")", "\n", "check_array", "(", "array", "=", "pscore", ",", "name", "=", "\"pscore\"", ",", "expected_dim", "=", "1", ")", "\n", "check_continuous_ope_inputs", "(", "\n", "reward", "=", "reward", ",", "\n", "action_by_behavior_policy", "=", "action_by_behavior_policy", ",", "\n", "pscore", "=", "pscore", ",", "\n", "action_by_evaluation_policy", "=", "action_by_evaluation_policy", ",", "\n", ")", "\n", "\n", "estimated_round_rewards", "=", "self", ".", "_estimate_round_rewards", "(", "\n", "reward", "=", "reward", ",", "\n", "action_by_behavior_policy", "=", "action_by_behavior_policy", ",", "\n", "pscore", "=", "pscore", ",", "\n", "action_by_evaluation_policy", "=", "action_by_evaluation_policy", ",", "\n", ")", "\n", "\n", "return", "estimate_confidence_interval_by_bootstrap", "(", "\n", "samples", "=", "estimated_round_rewards", ",", "\n", "alpha", "=", "alpha", ",", "\n", "n_bootstrap_samples", "=", "n_bootstrap_samples", ",", "\n", "random_state", "=", "random_state", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_continuous.KernelizedSelfNormalizedInverseProbabilityWeighting.__post_init__": [[336, 343], ["sklearn.utils.check_scalar", "ValueError"], "methods", ["None"], ["def", "__post_init__", "(", "self", ")", "->", "None", ":", "\n", "        ", "if", "self", ".", "kernel", "not", "in", "[", "\"gaussian\"", ",", "\"epanechnikov\"", ",", "\"triangular\"", ",", "\"cosine\"", "]", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "f\"kernel must be one of 'gaussian', 'epanechnikov', 'triangular', or 'cosine' but {self.kernel} is given\"", "\n", ")", "\n", "", "check_scalar", "(", "\n", "self", ".", "bandwidth", ",", "name", "=", "\"bandwidth\"", ",", "target_type", "=", "(", "int", ",", "float", ")", ",", "min_val", "=", "0", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_continuous.KernelizedSelfNormalizedInverseProbabilityWeighting._estimate_round_rewards": [[345, 390], ["utils.check_array", "utils.check_array", "utils.check_array", "kernel_func", "kernel_func"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array"], ["", "def", "_estimate_round_rewards", "(", "\n", "self", ",", "\n", "reward", ":", "np", ".", "ndarray", ",", "\n", "action_by_behavior_policy", ":", "np", ".", "ndarray", ",", "\n", "pscore", ":", "np", ".", "ndarray", ",", "\n", "action_by_evaluation_policy", ":", "np", ".", "ndarray", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "np", ".", "ndarray", ":", "\n", "        ", "\"\"\"Estimate round-wise (or sample-wise) rewards.\n\n        Parameters\n        ----------\n        reward: array-like, shape (n_rounds,)\n            Rewards observed for each data in logged bandit data, i.e., :math:`r_i`.\n\n        action_by_behavior_policy: array-like, shape (n_rounds,)\n            Continuous action values sampled by behavior policy for each data in logged bandit data, i.e., :math:`a_i`.\n\n        pscore: array-like, shape (n_rounds,)\n            Probability densities of the continuous action values sampled by behavior policy\n            (generalized propensity scores), i.e., :math:`\\\\pi_b(a_i|x_i)`.\n\n        action_by_evaluation_policy: array-like, shape (n_rounds,)\n            Continuous action values given by evaluation policy (can be deterministic), i.e., :math:`\\\\pi_e(x_t)`.\n\n        Returns\n        ----------\n        estimated_rewards: array-like, shape (n_rounds,)\n            Estimated rewards for each observation.\n\n        \"\"\"", "\n", "check_array", "(", "array", "=", "reward", ",", "name", "=", "\"reward\"", ",", "expected_dim", "=", "1", ")", "\n", "check_array", "(", "\n", "array", "=", "action_by_behavior_policy", ",", "\n", "name", "=", "\"action_by_behavior_policy\"", ",", "\n", "expected_dim", "=", "1", ",", "\n", ")", "\n", "check_array", "(", "array", "=", "pscore", ",", "name", "=", "\"pscore\"", ",", "expected_dim", "=", "1", ")", "\n", "\n", "kernel_func", "=", "kernel_functions", "[", "self", ".", "kernel", "]", "\n", "u", "=", "action_by_evaluation_policy", "-", "action_by_behavior_policy", "\n", "u", "/=", "self", ".", "bandwidth", "\n", "estimated_rewards", "=", "kernel_func", "(", "u", ")", "*", "reward", "/", "pscore", "\n", "estimated_rewards", "/=", "(", "kernel_func", "(", "u", ")", "/", "pscore", ")", ".", "mean", "(", ")", "\n", "return", "estimated_rewards", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_continuous.KernelizedSelfNormalizedInverseProbabilityWeighting.estimate_policy_value": [[391, 441], ["utils.check_array", "utils.check_array", "utils.check_array", "utils.check_continuous_ope_inputs", "estimators_continuous.KernelizedSelfNormalizedInverseProbabilityWeighting._estimate_round_rewards().mean", "estimators_continuous.KernelizedSelfNormalizedInverseProbabilityWeighting._estimate_round_rewards"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_continuous_ope_inputs", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SelfNormalizedSlateStandardIPS._estimate_round_rewards"], ["", "def", "estimate_policy_value", "(", "\n", "self", ",", "\n", "reward", ":", "np", ".", "ndarray", ",", "\n", "action_by_behavior_policy", ":", "np", ".", "ndarray", ",", "\n", "pscore", ":", "np", ".", "ndarray", ",", "\n", "action_by_evaluation_policy", ":", "np", ".", "ndarray", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "np", ".", "ndarray", ":", "\n", "        ", "\"\"\"Estimate policy value of evaluation policy.\n\n        Parameters\n        ----------\n        reward: array-like, shape (n_rounds,)\n            Rewards observed for each data in logged bandit data, i.e., :math:`r_i`.\n\n        action_by_behavior_policy: array-like, shape (n_rounds,)\n            Continuous action values sampled by behavior policy for each data in logged bandit data, i.e., :math:`a_i`.\n\n        pscore: array-like, shape (n_rounds,)\n            Probability densities of the continuous action values sampled by behavior policy\n            (generalized propensity scores), i.e., :math:`\\\\pi_b(a_i|x_i)`.\n\n        action_by_evaluation_policy: array-like, shape (n_rounds,)\n            Continuous action values given by evaluation policy (can be deterministic), i.e., :math:`\\\\pi_e(x_t)`.\n\n        Returns\n        ----------\n        V_hat: float\n            Estimated policy value of evaluation policy.\n\n        \"\"\"", "\n", "check_array", "(", "array", "=", "reward", ",", "name", "=", "\"reward\"", ",", "expected_dim", "=", "1", ")", "\n", "check_array", "(", "\n", "array", "=", "action_by_behavior_policy", ",", "\n", "name", "=", "\"action_by_behavior_policy\"", ",", "\n", "expected_dim", "=", "1", ",", "\n", ")", "\n", "check_array", "(", "array", "=", "pscore", ",", "name", "=", "\"pscore\"", ",", "expected_dim", "=", "1", ")", "\n", "check_continuous_ope_inputs", "(", "\n", "reward", "=", "reward", ",", "\n", "action_by_behavior_policy", "=", "action_by_behavior_policy", ",", "\n", "pscore", "=", "pscore", ",", "\n", "action_by_evaluation_policy", "=", "action_by_evaluation_policy", ",", "\n", ")", "\n", "\n", "return", "self", ".", "_estimate_round_rewards", "(", "\n", "reward", "=", "reward", ",", "\n", "action_by_behavior_policy", "=", "action_by_behavior_policy", ",", "\n", "pscore", "=", "pscore", ",", "\n", "action_by_evaluation_policy", "=", "action_by_evaluation_policy", ",", "\n", ")", ".", "mean", "(", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_continuous.KernelizedSelfNormalizedInverseProbabilityWeighting.estimate_interval": [[443, 512], ["utils.check_array", "utils.check_array", "utils.check_array", "utils.check_continuous_ope_inputs", "estimators_continuous.KernelizedSelfNormalizedInverseProbabilityWeighting._estimate_round_rewards", "utils.estimate_confidence_interval_by_bootstrap"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_continuous_ope_inputs", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SelfNormalizedSlateStandardIPS._estimate_round_rewards", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.estimate_confidence_interval_by_bootstrap"], ["", "def", "estimate_interval", "(", "\n", "self", ",", "\n", "reward", ":", "np", ".", "ndarray", ",", "\n", "action_by_behavior_policy", ":", "np", ".", "ndarray", ",", "\n", "pscore", ":", "np", ".", "ndarray", ",", "\n", "action_by_evaluation_policy", ":", "np", ".", "ndarray", ",", "\n", "alpha", ":", "float", "=", "0.05", ",", "\n", "n_bootstrap_samples", ":", "int", "=", "10000", ",", "\n", "random_state", ":", "Optional", "[", "int", "]", "=", "None", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "Dict", "[", "str", ",", "float", "]", ":", "\n", "        ", "\"\"\"Estimate the confidence interval of the policy value using bootstrap.\n\n        Parameters\n        ----------\n        reward: array-like, shape (n_rounds,)\n            Rewards observed for each data in logged bandit data, i.e., :math:`r_i`.\n\n        action_by_behavior_policy: array-like, shape (n_rounds,)\n            Continuous action values sampled by behavior policy for each data in logged bandit data, i.e., :math:`a_i`.\n\n        pscore: array-like, shape (n_rounds,)\n            Probability densities of the continuous action values sampled by behavior policy\n            (generalized propensity scores), i.e., :math:`\\\\pi_b(a_i|x_i)`.\n\n        action_by_evaluation_policy: array-like, shape (n_rounds,)\n            Continuous action values given by evaluation policy (can be deterministic), i.e., :math:`\\\\pi_e(x_t)`.\n\n        alpha: float, default=0.05\n            Significance level.\n\n        n_bootstrap_samples: int, default=10000\n            Number of resampling performed in bootstrap sampling.\n\n        random_state: int, default=None\n            Controls the random seed in bootstrap sampling.\n\n        Returns\n        ----------\n        estimated_confidence_interval: Dict[str, float]\n            Dictionary storing the estimated mean and upper-lower confidence bounds.\n\n        \"\"\"", "\n", "check_array", "(", "array", "=", "reward", ",", "name", "=", "\"reward\"", ",", "expected_dim", "=", "1", ")", "\n", "check_array", "(", "\n", "array", "=", "action_by_behavior_policy", ",", "\n", "name", "=", "\"action_by_behavior_policy\"", ",", "\n", "expected_dim", "=", "1", ",", "\n", ")", "\n", "check_array", "(", "array", "=", "pscore", ",", "name", "=", "\"pscore\"", ",", "expected_dim", "=", "1", ")", "\n", "check_continuous_ope_inputs", "(", "\n", "reward", "=", "reward", ",", "\n", "action_by_behavior_policy", "=", "action_by_behavior_policy", ",", "\n", "pscore", "=", "pscore", ",", "\n", "action_by_evaluation_policy", "=", "action_by_evaluation_policy", ",", "\n", ")", "\n", "\n", "estimated_round_rewards", "=", "self", ".", "_estimate_round_rewards", "(", "\n", "reward", "=", "reward", ",", "\n", "action_by_behavior_policy", "=", "action_by_behavior_policy", ",", "\n", "pscore", "=", "pscore", ",", "\n", "action_by_evaluation_policy", "=", "action_by_evaluation_policy", ",", "\n", ")", "\n", "\n", "return", "estimate_confidence_interval_by_bootstrap", "(", "\n", "samples", "=", "estimated_round_rewards", ",", "\n", "alpha", "=", "alpha", ",", "\n", "n_bootstrap_samples", "=", "n_bootstrap_samples", ",", "\n", "random_state", "=", "random_state", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_continuous.KernelizedDoublyRobust.__post_init__": [[561, 568], ["sklearn.utils.check_scalar", "ValueError"], "methods", ["None"], ["def", "__post_init__", "(", "self", ")", "->", "None", ":", "\n", "        ", "if", "self", ".", "kernel", "not", "in", "[", "\"gaussian\"", ",", "\"epanechnikov\"", ",", "\"triangular\"", ",", "\"cosine\"", "]", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "f\"kernel must be one of 'gaussian', 'epanechnikov', 'triangular', or 'cosine' but {self.kernel} is given\"", "\n", ")", "\n", "", "check_scalar", "(", "\n", "self", ".", "bandwidth", ",", "name", "=", "\"bandwidth\"", ",", "target_type", "=", "(", "int", ",", "float", ")", ",", "min_val", "=", "0", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_continuous.KernelizedDoublyRobust._estimate_round_rewards": [[570, 614], ["kernel_func"], "methods", ["None"], ["", "def", "_estimate_round_rewards", "(", "\n", "self", ",", "\n", "reward", ":", "np", ".", "ndarray", ",", "\n", "action_by_behavior_policy", ":", "np", ".", "ndarray", ",", "\n", "pscore", ":", "np", ".", "ndarray", ",", "\n", "action_by_evaluation_policy", ":", "np", ".", "ndarray", ",", "\n", "estimated_rewards_by_reg_model", ":", "np", ".", "ndarray", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "np", ".", "ndarray", ":", "\n", "        ", "\"\"\"Estimate round-wise (or sample-wise) rewards.\n\n        Parameters\n        ----------\n        reward: array-like, shape (n_rounds,)\n            Rewards observed for each data in logged bandit data, i.e., :math:`r_i`.\n\n        action_by_behavior_policy: array-like, shape (n_rounds,)\n            Continuous action values sampled by behavior policy for each data in logged bandit data, i.e., :math:`a_i`.\n\n        pscore: array-like, shape (n_rounds,)\n            Probability densities of the continuous action values sampled by behavior policy\n            (generalized propensity scores), i.e., :math:`\\\\pi_b(a_i|x_i)`.\n\n        action_by_evaluation_policy: array-like, shape (n_rounds,)\n            Continuous action values given by evaluation policy (can be deterministic), i.e., :math:`\\\\pi_e(x_t)`.\n\n        estimated_rewards_by_reg_model: array-like, shape (n_rounds,)\n            Expected rewards given context and action estimated by regression model, i.e., :math:`\\\\hat{q}(x_i,a_i)`.\n\n        Returns\n        ----------\n        estimated_rewards: array-like, shape (n_rounds,)\n            Estimated rewards for each observation.\n\n        \"\"\"", "\n", "kernel_func", "=", "kernel_functions", "[", "self", ".", "kernel", "]", "\n", "u", "=", "action_by_evaluation_policy", "-", "action_by_behavior_policy", "\n", "u", "/=", "self", ".", "bandwidth", "\n", "estimated_rewards", "=", "(", "\n", "kernel_func", "(", "u", ")", "*", "(", "reward", "-", "estimated_rewards_by_reg_model", ")", "/", "pscore", "\n", ")", "\n", "estimated_rewards", "/=", "self", ".", "bandwidth", "\n", "estimated_rewards", "+=", "estimated_rewards_by_reg_model", "\n", "return", "estimated_rewards", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_continuous.KernelizedDoublyRobust.estimate_policy_value": [[615, 676], ["utils.check_array", "utils.check_array", "utils.check_array", "utils.check_array", "utils.check_continuous_ope_inputs", "estimators_continuous.KernelizedDoublyRobust._estimate_round_rewards().mean", "estimators_continuous.KernelizedDoublyRobust._estimate_round_rewards"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_continuous_ope_inputs", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SelfNormalizedSlateStandardIPS._estimate_round_rewards"], ["", "def", "estimate_policy_value", "(", "\n", "self", ",", "\n", "reward", ":", "np", ".", "ndarray", ",", "\n", "action_by_behavior_policy", ":", "np", ".", "ndarray", ",", "\n", "pscore", ":", "np", ".", "ndarray", ",", "\n", "action_by_evaluation_policy", ":", "np", ".", "ndarray", ",", "\n", "estimated_rewards_by_reg_model", ":", "np", ".", "ndarray", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "np", ".", "ndarray", ":", "\n", "        ", "\"\"\"Estimate policy value of evaluation policy.\n\n        Parameters\n        ----------\n        reward: array-like, shape (n_rounds,)\n            Rewards observed for each data in logged bandit data, i.e., :math:`r_i`.\n\n        action_by_behavior_policy: array-like, shape (n_rounds,)\n            Continuous action values sampled by behavior policy for each data in logged bandit data, i.e., :math:`a_i`.\n\n        pscore: array-like, shape (n_rounds,)\n            Probability densities of the continuous action values sampled by behavior policy\n            (generalized propensity scores), i.e., :math:`\\\\pi_b(a_i|x_i)`.\n\n        action_by_evaluation_policy: array-like, shape (n_rounds,)\n            Continuous action values given by evaluation policy (can be deterministic), i.e., :math:`\\\\pi_e(x_t)`.\n\n        estimated_rewards_by_reg_model: array-like, shape (n_rounds,)\n            Expected rewards given context and action estimated by regression model, i.e., :math:`\\\\hat{q}(x_i,a_i)`.\n\n        Returns\n        ----------\n        V_hat: float\n            Estimated policy value of evaluation policy.\n\n        \"\"\"", "\n", "check_array", "(", "\n", "array", "=", "estimated_rewards_by_reg_model", ",", "\n", "name", "=", "\"estimated_rewards_by_reg_model\"", ",", "\n", "expected_dim", "=", "1", ",", "\n", ")", "\n", "check_array", "(", "array", "=", "reward", ",", "name", "=", "\"reward\"", ",", "expected_dim", "=", "1", ")", "\n", "check_array", "(", "\n", "array", "=", "action_by_behavior_policy", ",", "\n", "name", "=", "\"action_by_behavior_policy\"", ",", "\n", "expected_dim", "=", "1", ",", "\n", ")", "\n", "check_array", "(", "array", "=", "pscore", ",", "name", "=", "\"pscore\"", ",", "expected_dim", "=", "1", ")", "\n", "check_continuous_ope_inputs", "(", "\n", "reward", "=", "reward", ",", "\n", "action_by_behavior_policy", "=", "action_by_behavior_policy", ",", "\n", "pscore", "=", "pscore", ",", "\n", "action_by_evaluation_policy", "=", "action_by_evaluation_policy", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", ")", "\n", "\n", "return", "self", ".", "_estimate_round_rewards", "(", "\n", "reward", "=", "reward", ",", "\n", "action_by_behavior_policy", "=", "action_by_behavior_policy", ",", "\n", "pscore", "=", "pscore", ",", "\n", "action_by_evaluation_policy", "=", "action_by_evaluation_policy", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", ")", ".", "mean", "(", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_continuous.KernelizedDoublyRobust.estimate_interval": [[678, 758], ["utils.check_array", "utils.check_array", "utils.check_array", "utils.check_array", "utils.check_continuous_ope_inputs", "estimators_continuous.KernelizedDoublyRobust._estimate_round_rewards", "utils.estimate_confidence_interval_by_bootstrap"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_continuous_ope_inputs", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SelfNormalizedSlateStandardIPS._estimate_round_rewards", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.estimate_confidence_interval_by_bootstrap"], ["", "def", "estimate_interval", "(", "\n", "self", ",", "\n", "reward", ":", "np", ".", "ndarray", ",", "\n", "action_by_behavior_policy", ":", "np", ".", "ndarray", ",", "\n", "pscore", ":", "np", ".", "ndarray", ",", "\n", "action_by_evaluation_policy", ":", "np", ".", "ndarray", ",", "\n", "estimated_rewards_by_reg_model", ":", "np", ".", "ndarray", ",", "\n", "alpha", ":", "float", "=", "0.05", ",", "\n", "n_bootstrap_samples", ":", "int", "=", "10000", ",", "\n", "random_state", ":", "Optional", "[", "int", "]", "=", "None", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "Dict", "[", "str", ",", "float", "]", ":", "\n", "        ", "\"\"\"Estimate the confidence interval of the policy value using bootstrap.\n\n        Parameters\n        ----------\n        reward: array-like, shape (n_rounds,)\n            Rewards observed for each data in logged bandit data, i.e., :math:`r_i`.\n\n        action_by_behavior_policy: array-like, shape (n_rounds,)\n            Continuous action values sampled by behavior policy for each data in logged bandit data, i.e., :math:`a_i`.\n\n        pscore: array-like, shape (n_rounds,)\n            Probability densities of the continuous action values sampled by behavior policy\n            (generalized propensity scores), i.e., :math:`\\\\pi_b(a_i|x_i)`.\n\n        action_by_evaluation_policy: array-like, shape (n_rounds,)\n            Continuous action values given by evaluation policy (can be deterministic), i.e., :math:`\\\\pi_e(x_t)`.\n\n        estimated_rewards_by_reg_model: array-like, shape (n_rounds,)\n            Expected rewards given context and action estimated by regression model, i.e., :math:`\\\\hat{q}(x_i,a_i)`.\n\n        alpha: float, default=0.05\n            Significance level.\n\n        n_bootstrap_samples: int, default=10000\n            Number of resampling performed in bootstrap sampling.\n\n        random_state: int, default=None\n            Controls the random seed in bootstrap sampling.\n\n        Returns\n        ----------\n        estimated_confidence_interval: Dict[str, float]\n            Dictionary storing the estimated mean and upper-lower confidence bounds.\n\n        \"\"\"", "\n", "check_array", "(", "\n", "array", "=", "estimated_rewards_by_reg_model", ",", "\n", "name", "=", "\"estimated_rewards_by_reg_model\"", ",", "\n", "expected_dim", "=", "1", ",", "\n", ")", "\n", "check_array", "(", "array", "=", "reward", ",", "name", "=", "\"reward\"", ",", "expected_dim", "=", "1", ")", "\n", "check_array", "(", "\n", "array", "=", "action_by_behavior_policy", ",", "\n", "name", "=", "\"action_by_behavior_policy\"", ",", "\n", "expected_dim", "=", "1", ",", "\n", ")", "\n", "check_array", "(", "array", "=", "pscore", ",", "name", "=", "\"pscore\"", ",", "expected_dim", "=", "1", ")", "\n", "check_continuous_ope_inputs", "(", "\n", "reward", "=", "reward", ",", "\n", "action_by_behavior_policy", "=", "action_by_behavior_policy", ",", "\n", "pscore", "=", "pscore", ",", "\n", "action_by_evaluation_policy", "=", "action_by_evaluation_policy", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", ")", "\n", "\n", "estimated_round_rewards", "=", "self", ".", "_estimate_round_rewards", "(", "\n", "reward", "=", "reward", ",", "\n", "action_by_behavior_policy", "=", "action_by_behavior_policy", ",", "\n", "pscore", "=", "pscore", ",", "\n", "action_by_evaluation_policy", "=", "action_by_evaluation_policy", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", ")", "\n", "\n", "return", "estimate_confidence_interval_by_bootstrap", "(", "\n", "samples", "=", "estimated_round_rewards", ",", "\n", "alpha", "=", "alpha", ",", "\n", "n_bootstrap_samples", "=", "n_bootstrap_samples", ",", "\n", "random_state", "=", "random_state", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_continuous.triangular_kernel": [[22, 26], ["numpy.clip", "numpy.abs"], "function", ["None"], ["def", "triangular_kernel", "(", "u", ":", "np", ".", "ndarray", ")", "->", "np", ".", "ndarray", ":", "\n", "    ", "\"\"\"Calculate triangular kernel function.\"\"\"", "\n", "clipped_u", "=", "np", ".", "clip", "(", "u", ",", "-", "1.0", ",", "1.0", ")", "\n", "return", "1", "-", "np", ".", "abs", "(", "clipped_u", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_continuous.gaussian_kernel": [[28, 31], ["numpy.exp", "numpy.sqrt"], "function", ["None"], ["", "def", "gaussian_kernel", "(", "u", ":", "np", ".", "ndarray", ")", "->", "np", ".", "ndarray", ":", "\n", "    ", "\"\"\"Calculate gaussian kernel function.\"\"\"", "\n", "return", "np", ".", "exp", "(", "-", "(", "u", "**", "2", ")", "/", "2", ")", "/", "np", ".", "sqrt", "(", "2", "*", "np", ".", "pi", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_continuous.epanechnikov_kernel": [[33, 37], ["numpy.clip"], "function", ["None"], ["", "def", "epanechnikov_kernel", "(", "u", ":", "np", ".", "ndarray", ")", "->", "np", ".", "ndarray", ":", "\n", "    ", "\"\"\"Calculate epanechnikov kernel function.\"\"\"", "\n", "clipped_u", "=", "np", ".", "clip", "(", "u", ",", "-", "1.0", ",", "1.0", ")", "\n", "return", "0.75", "*", "(", "1", "-", "clipped_u", "**", "2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_continuous.cosine_kernel": [[39, 43], ["numpy.clip", "numpy.cos"], "function", ["None"], ["", "def", "cosine_kernel", "(", "u", ":", "np", ".", "ndarray", ")", "->", "np", ".", "ndarray", ":", "\n", "    ", "\"\"\"Calculate cosine kernel function.\"\"\"", "\n", "clipped_u", "=", "np", ".", "clip", "(", "u", ",", "-", "1.0", ",", "1.0", ")", "\n", "return", "(", "np", ".", "pi", "/", "4", ")", "*", "np", ".", "cos", "(", "clipped_u", "*", "np", ".", "pi", "/", "2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta_continuous.ContinuousOffPolicyEvaluation.__post_init__": [[98, 112], ["dict", "isinstance", "RuntimeError"], "methods", ["None"], ["def", "__post_init__", "(", "self", ")", "->", "None", ":", "\n", "        ", "\"\"\"Initialize class.\"\"\"", "\n", "for", "key_", "in", "[", "\"action\"", ",", "\"reward\"", ",", "\"pscore\"", "]", ":", "\n", "            ", "if", "key_", "not", "in", "self", ".", "bandit_feedback", ":", "\n", "                ", "raise", "RuntimeError", "(", "f\"Missing key of {key_} in 'bandit_feedback'.\"", ")", "\n", "", "", "self", ".", "bandit_feedback", "[", "\"action_by_behavior_policy\"", "]", "=", "self", ".", "bandit_feedback", "[", "\n", "\"action\"", "\n", "]", "\n", "self", ".", "ope_estimators_", "=", "dict", "(", ")", "\n", "self", ".", "is_model_dependent", "=", "False", "\n", "for", "estimator", "in", "self", ".", "ope_estimators", ":", "\n", "            ", "self", ".", "ope_estimators_", "[", "estimator", ".", "estimator_name", "]", "=", "estimator", "\n", "if", "isinstance", "(", "estimator", ",", "KDR", ")", ":", "\n", "                ", "self", ".", "is_model_dependent", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta_continuous.ContinuousOffPolicyEvaluation._create_estimator_inputs": [[113, 179], ["utils.check_array", "isinstance", "isinstance", "estimated_rewards_by_reg_model.items", "utils.check_array", "utils.check_array", "ValueError", "ValueError"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array"], ["", "", "", "def", "_create_estimator_inputs", "(", "\n", "self", ",", "\n", "action_by_evaluation_policy", ":", "np", ".", "ndarray", ",", "\n", "estimated_rewards_by_reg_model", ":", "Optional", "[", "\n", "Union", "[", "np", ".", "ndarray", ",", "Dict", "[", "str", ",", "np", ".", "ndarray", "]", "]", "\n", "]", "=", "None", ",", "\n", ")", "->", "Dict", "[", "str", ",", "Dict", "[", "str", ",", "np", ".", "ndarray", "]", "]", ":", "\n", "        ", "\"\"\"Create input dictionary to estimate policy value by subclasses of `BaseOffPolicyEstimator`\"\"\"", "\n", "check_array", "(", "\n", "array", "=", "action_by_evaluation_policy", ",", "\n", "name", "=", "\"action_by_evaluation_policy\"", ",", "\n", "expected_dim", "=", "1", ",", "\n", ")", "\n", "if", "estimated_rewards_by_reg_model", "is", "None", ":", "\n", "            ", "pass", "\n", "", "elif", "isinstance", "(", "estimated_rewards_by_reg_model", ",", "dict", ")", ":", "\n", "            ", "for", "estimator_name", ",", "value", "in", "estimated_rewards_by_reg_model", ".", "items", "(", ")", ":", "\n", "                ", "check_array", "(", "\n", "array", "=", "value", ",", "\n", "name", "=", "f\"estimated_rewards_by_reg_model[{estimator_name}]\"", ",", "\n", "expected_dim", "=", "1", ",", "\n", ")", "\n", "if", "value", ".", "shape", "!=", "action_by_evaluation_policy", ".", "shape", ":", "\n", "                    ", "raise", "ValueError", "(", "\n", "f\"Expected `estimated_rewards_by_reg_model[{estimator_name}].shape == action_by_evaluation_policy.shape`, but found it False\"", "\n", ")", "\n", "", "", "", "else", ":", "\n", "            ", "check_array", "(", "\n", "array", "=", "estimated_rewards_by_reg_model", ",", "\n", "name", "=", "\"estimated_rewards_by_reg_model\"", ",", "\n", "expected_dim", "=", "1", ",", "\n", ")", "\n", "if", "(", "\n", "estimated_rewards_by_reg_model", ".", "shape", "\n", "!=", "action_by_evaluation_policy", ".", "shape", "\n", ")", ":", "\n", "                ", "raise", "ValueError", "(", "\n", "\"Expected `estimated_rewards_by_reg_model.shape == action_by_evaluation_policy.shape`, but found it False\"", "\n", ")", "\n", "", "", "estimator_inputs", "=", "{", "\n", "estimator_name", ":", "{", "\n", "input_", ":", "self", ".", "bandit_feedback", "[", "input_", "]", "\n", "for", "input_", "in", "[", "\"reward\"", ",", "\"action_by_behavior_policy\"", ",", "\"pscore\"", "]", "\n", "}", "\n", "for", "estimator_name", "in", "self", ".", "ope_estimators_", "\n", "}", "\n", "\n", "for", "estimator_name", "in", "self", ".", "ope_estimators_", ":", "\n", "            ", "estimator_inputs", "[", "estimator_name", "]", "[", "\n", "\"action_by_evaluation_policy\"", "\n", "]", "=", "action_by_evaluation_policy", "\n", "if", "isinstance", "(", "estimated_rewards_by_reg_model", ",", "dict", ")", ":", "\n", "                ", "if", "estimator_name", "in", "estimated_rewards_by_reg_model", ":", "\n", "                    ", "estimator_inputs", "[", "estimator_name", "]", "[", "\n", "\"estimated_rewards_by_reg_model\"", "\n", "]", "=", "estimated_rewards_by_reg_model", "[", "estimator_name", "]", "\n", "", "else", ":", "\n", "                    ", "estimator_inputs", "[", "estimator_name", "]", "[", "\n", "\"estimated_rewards_by_reg_model\"", "\n", "]", "=", "None", "\n", "", "", "else", ":", "\n", "                ", "estimator_inputs", "[", "estimator_name", "]", "[", "\n", "\"estimated_rewards_by_reg_model\"", "\n", "]", "=", "estimated_rewards_by_reg_model", "\n", "\n", "", "", "return", "estimator_inputs", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta_continuous.ContinuousOffPolicyEvaluation.estimate_policy_values": [[180, 223], ["dict", "meta_continuous.ContinuousOffPolicyEvaluation._create_estimator_inputs", "meta_continuous.ContinuousOffPolicyEvaluation.ope_estimators_.items", "estimator.estimate_policy_value", "ValueError"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta.OffPolicyEvaluation._create_estimator_inputs", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_policy_value"], ["", "def", "estimate_policy_values", "(", "\n", "self", ",", "\n", "action_by_evaluation_policy", ":", "np", ".", "ndarray", ",", "\n", "estimated_rewards_by_reg_model", ":", "Optional", "[", "\n", "Union", "[", "np", ".", "ndarray", ",", "Dict", "[", "str", ",", "np", ".", "ndarray", "]", "]", "\n", "]", "=", "None", ",", "\n", ")", "->", "Dict", "[", "str", ",", "float", "]", ":", "\n", "        ", "\"\"\"Estimate policy value of evaluation policy.\n\n        Parameters\n        ------------\n        action_by_evaluation_policy: array-like, shape (n_rounds,)\n            Continuous action values given by evaluation policy, i.e., :math:`\\\\pi_e(x_t)`.\n\n        estimated_rewards_by_reg_model: array-like, shape (n_rounds,) or Dict[str, array-like], default=None\n            Estimated expected rewards given context, action, and position, i.e., :math:`\\\\hat{q}(x_i,a_i)`.\n            When an array-like is given, all OPE estimators use it.\n            When a dict with an estimator's name as its key is given, the corresponding value is used for the estimator.\n            If None, model-dependent estimators such as DM and DR cannot be used.\n\n        Returns\n        ----------\n        policy_value_dict: Dict[str, float]\n            Dictionary containing the policy values estimated by OPE estimators.\n\n        \"\"\"", "\n", "if", "self", ".", "is_model_dependent", ":", "\n", "            ", "if", "estimated_rewards_by_reg_model", "is", "None", ":", "\n", "                ", "raise", "ValueError", "(", "\n", "\"When model dependent estimators such as DM or DR are used, `estimated_rewards_by_reg_model` must be given\"", "\n", ")", "\n", "\n", "", "", "policy_value_dict", "=", "dict", "(", ")", "\n", "estimator_inputs", "=", "self", ".", "_create_estimator_inputs", "(", "\n", "action_by_evaluation_policy", "=", "action_by_evaluation_policy", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", ")", "\n", "for", "estimator_name", ",", "estimator", "in", "self", ".", "ope_estimators_", ".", "items", "(", ")", ":", "\n", "            ", "policy_value_dict", "[", "estimator_name", "]", "=", "estimator", ".", "estimate_policy_value", "(", "\n", "**", "estimator_inputs", "[", "estimator_name", "]", "\n", ")", "\n", "\n", "", "return", "policy_value_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta_continuous.ContinuousOffPolicyEvaluation.estimate_intervals": [[224, 287], ["utils.check_confidence_interval_arguments", "dict", "meta_continuous.ContinuousOffPolicyEvaluation._create_estimator_inputs", "meta_continuous.ContinuousOffPolicyEvaluation.ope_estimators_.items", "estimator.estimate_interval", "ValueError"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_confidence_interval_arguments", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta.OffPolicyEvaluation._create_estimator_inputs", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_interval"], ["", "def", "estimate_intervals", "(", "\n", "self", ",", "\n", "action_by_evaluation_policy", ":", "np", ".", "ndarray", ",", "\n", "estimated_rewards_by_reg_model", ":", "Optional", "[", "\n", "Union", "[", "np", ".", "ndarray", ",", "Dict", "[", "str", ",", "np", ".", "ndarray", "]", "]", "\n", "]", "=", "None", ",", "\n", "alpha", ":", "float", "=", "0.05", ",", "\n", "n_bootstrap_samples", ":", "int", "=", "100", ",", "\n", "random_state", ":", "Optional", "[", "int", "]", "=", "None", ",", "\n", ")", "->", "Dict", "[", "str", ",", "Dict", "[", "str", ",", "float", "]", "]", ":", "\n", "        ", "\"\"\"Estimate confidence intervals of policy values using bootstrap.\n\n        Parameters\n        ------------\n        action_by_evaluation_policy: array-like, shape (n_rounds,)\n            Continuous action values given by the (deterministic) evaluation policy, i.e., :math:`\\\\pi_e(x_t)`.\n\n        estimated_rewards_by_reg_model: array-like, shape (n_rounds, n_actions, len_list) or Dict[str, array-like], default=None\n            Estimated expected rewards given context, action, and position, i.e., :math:`\\\\hat{q}(x_i,a_i)`.\n            When an array-like is given, all OPE estimators use it.\n            When a dict with an estimator's name as its key is given, the corresponding value is used for the estimator.\n            If None, model-dependent estimators such as DM and DR cannot be used.\n\n        alpha: float, default=0.05\n            Significance level.\n\n        n_bootstrap_samples: int, default=100\n            Number of resampling performed in bootstrap sampling.\n\n        random_state: int, default=None\n            Controls the random seed in bootstrap sampling.\n\n        Returns\n        ----------\n        policy_value_interval_dict: Dict[str, Dict[str, float]]\n            Dictionary containing confidence intervals of the estimated policy values.\n\n        \"\"\"", "\n", "if", "self", ".", "is_model_dependent", ":", "\n", "            ", "if", "estimated_rewards_by_reg_model", "is", "None", ":", "\n", "                ", "raise", "ValueError", "(", "\n", "\"When model dependent estimators such as DM or DR are used, `estimated_rewards_by_reg_model` must be given\"", "\n", ")", "\n", "\n", "", "", "check_confidence_interval_arguments", "(", "\n", "alpha", "=", "alpha", ",", "\n", "n_bootstrap_samples", "=", "n_bootstrap_samples", ",", "\n", "random_state", "=", "random_state", ",", "\n", ")", "\n", "policy_value_interval_dict", "=", "dict", "(", ")", "\n", "estimator_inputs", "=", "self", ".", "_create_estimator_inputs", "(", "\n", "action_by_evaluation_policy", "=", "action_by_evaluation_policy", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", ")", "\n", "for", "estimator_name", ",", "estimator", "in", "self", ".", "ope_estimators_", ".", "items", "(", ")", ":", "\n", "            ", "policy_value_interval_dict", "[", "estimator_name", "]", "=", "estimator", ".", "estimate_interval", "(", "\n", "**", "estimator_inputs", "[", "estimator_name", "]", ",", "\n", "alpha", "=", "alpha", ",", "\n", "n_bootstrap_samples", "=", "n_bootstrap_samples", ",", "\n", "random_state", "=", "random_state", ",", "\n", ")", "\n", "\n", "", "return", "policy_value_interval_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta_continuous.ContinuousOffPolicyEvaluation.summarize_off_policy_estimates": [[288, 354], ["pandas.DataFrame", "pandas.DataFrame", "meta_continuous.ContinuousOffPolicyEvaluation.bandit_feedback[].mean", "meta_continuous.ContinuousOffPolicyEvaluation.estimate_policy_values", "meta_continuous.ContinuousOffPolicyEvaluation.estimate_intervals", "logger.warning"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta.OffPolicyEvaluation.estimate_policy_values", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta.OffPolicyEvaluation.estimate_intervals"], ["", "def", "summarize_off_policy_estimates", "(", "\n", "self", ",", "\n", "action_by_evaluation_policy", ":", "np", ".", "ndarray", ",", "\n", "estimated_rewards_by_reg_model", ":", "Optional", "[", "\n", "Union", "[", "np", ".", "ndarray", ",", "Dict", "[", "str", ",", "np", ".", "ndarray", "]", "]", "\n", "]", "=", "None", ",", "\n", "alpha", ":", "float", "=", "0.05", ",", "\n", "n_bootstrap_samples", ":", "int", "=", "100", ",", "\n", "random_state", ":", "Optional", "[", "int", "]", "=", "None", ",", "\n", ")", "->", "Tuple", "[", "DataFrame", ",", "DataFrame", "]", ":", "\n", "        ", "\"\"\"Summarize policy values and their confidence intervals estimated by OPE estimators.\n\n        Parameters\n        ------------\n        action_by_evaluation_policy: array-like, shape (n_rounds,)\n            Continuous action values given by the (deterministic) evaluation policy, i.e., :math:`\\\\pi_e(x_t)`.\n\n        estimated_rewards_by_reg_model: array-like, shape (n_rounds, n_actions, len_list) or Dict[str, array-like], default=None\n            Estimated expected rewards given context, action, and position, i.e., :math:`\\\\hat{q}(x_i,a_i)`.\n            When an array-like is given, all OPE estimators use it.\n            When a dict with an estimator's name as its key is given, the corresponding value is used for the estimator.\n            If None, model-dependent estimators such as DM and DR cannot be used.\n\n        alpha: float, default=0.05\n            Significance level.\n\n        n_bootstrap_samples: int, default=100\n            Number of resampling performed in bootstrap sampling.\n\n        random_state: int, default=None\n            Controls the random seed in bootstrap sampling.\n\n        Returns\n        ----------\n        (policy_value_df, policy_value_interval_df): Tuple[DataFrame, DataFrame]\n            Policy values and their confidence intervals estimated by OPE estimators.\n\n        \"\"\"", "\n", "policy_value_df", "=", "DataFrame", "(", "\n", "self", ".", "estimate_policy_values", "(", "\n", "action_by_evaluation_policy", "=", "action_by_evaluation_policy", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", ")", ",", "\n", "index", "=", "[", "\"estimated_policy_value\"", "]", ",", "\n", ")", "\n", "policy_value_interval_df", "=", "DataFrame", "(", "\n", "self", ".", "estimate_intervals", "(", "\n", "action_by_evaluation_policy", "=", "action_by_evaluation_policy", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", "alpha", "=", "alpha", ",", "\n", "n_bootstrap_samples", "=", "n_bootstrap_samples", ",", "\n", "random_state", "=", "random_state", ",", "\n", ")", "\n", ")", "\n", "policy_value_of_behavior_policy", "=", "self", ".", "bandit_feedback", "[", "\"reward\"", "]", ".", "mean", "(", ")", "\n", "policy_value_df", "=", "policy_value_df", ".", "T", "\n", "if", "policy_value_of_behavior_policy", "<=", "0", ":", "\n", "            ", "logger", ".", "warning", "(", "\n", "f\"Policy value of the behavior policy is {policy_value_of_behavior_policy} (<=0); relative estimated policy value is set to np.nan\"", "\n", ")", "\n", "policy_value_df", "[", "\"relative_estimated_policy_value\"", "]", "=", "np", ".", "nan", "\n", "", "else", ":", "\n", "            ", "policy_value_df", "[", "\"relative_estimated_policy_value\"", "]", "=", "(", "\n", "policy_value_df", ".", "estimated_policy_value", "/", "policy_value_of_behavior_policy", "\n", ")", "\n", "", "return", "policy_value_df", ",", "policy_value_interval_df", ".", "T", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta_continuous.ContinuousOffPolicyEvaluation.visualize_off_policy_estimates": [[355, 442], ["dict", "meta_continuous.ContinuousOffPolicyEvaluation._create_estimator_inputs", "meta_continuous.ContinuousOffPolicyEvaluation.ope_estimators_.items", "pandas.DataFrame", "pandas.DataFrame.rename", "matplotlib.style.use", "matplotlib.subplots", "seaborn.barplot", "matplotlib.xlabel", "matplotlib.ylabel", "matplotlib.yticks", "matplotlib.xticks", "isinstance", "isinstance", "estimator._estimate_round_rewards", "meta_continuous.ContinuousOffPolicyEvaluation.bandit_feedback[].mean", "fig.savefig", "str", "key.upper", "numpy.int32", "dict.keys", "len"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta.OffPolicyEvaluation._create_estimator_inputs", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SelfNormalizedSlateStandardIPS._estimate_round_rewards"], ["", "def", "visualize_off_policy_estimates", "(", "\n", "self", ",", "\n", "action_by_evaluation_policy", ":", "np", ".", "ndarray", ",", "\n", "estimated_rewards_by_reg_model", ":", "Optional", "[", "\n", "Union", "[", "np", ".", "ndarray", ",", "Dict", "[", "str", ",", "np", ".", "ndarray", "]", "]", "\n", "]", "=", "None", ",", "\n", "alpha", ":", "float", "=", "0.05", ",", "\n", "is_relative", ":", "bool", "=", "False", ",", "\n", "n_bootstrap_samples", ":", "int", "=", "100", ",", "\n", "random_state", ":", "Optional", "[", "int", "]", "=", "None", ",", "\n", "fig_dir", ":", "Optional", "[", "Path", "]", "=", "None", ",", "\n", "fig_name", ":", "str", "=", "\"estimated_policy_value.png\"", ",", "\n", ")", "->", "None", ":", "\n", "        ", "\"\"\"Visualize the estimated policy values.\n\n        Parameters\n        ----------\n        action_by_evaluation_policy: array-like, shape (n_rounds,)\n            Continuous action values given by the (deterministic) evaluation policy, i.e., :math:`\\\\pi_e(x_t)`.\n\n        estimated_rewards_by_reg_model: array-like, shape (n_rounds, n_actions, len_list) or Dict[str, array-like], default=None\n            Estimated expected rewards given context, action, and position, i.e., :math:`\\\\hat{q}(x_i,a_i)`.\n            When an array-like is given, all OPE estimators use it.\n            When a dict with an estimator's name as its key is given, the corresponding value is used for the estimator.\n            If None, model-dependent estimators such as DM and DR cannot be used.\n\n        alpha: float, default=0.05\n            Significance level.\n\n        n_bootstrap_samples: int, default=100\n            Number of resampling performed in bootstrap sampling.\n\n        random_state: int, default=None\n            Controls the random seed in bootstrap sampling.\n\n        is_relative: bool, default=False,\n            If True, the method visualizes the estimated policy values of evaluation policy\n            relative to the ground-truth policy value of behavior policy.\n\n        fig_dir: Path, default=None\n            Path to store the bar figure.\n            If None, the figure will not be saved.\n\n        fig_name: str, default=\"estimated_policy_value.png\"\n            Name of the bar figure.\n\n        \"\"\"", "\n", "if", "fig_dir", "is", "not", "None", ":", "\n", "            ", "assert", "isinstance", "(", "fig_dir", ",", "Path", ")", ",", "\"`fig_dir` must be a Path\"", "\n", "", "if", "fig_name", "is", "not", "None", ":", "\n", "            ", "assert", "isinstance", "(", "fig_name", ",", "str", ")", ",", "\"`fig_dir` must be a string\"", "\n", "\n", "", "estimated_round_rewards_dict", "=", "dict", "(", ")", "\n", "estimator_inputs", "=", "self", ".", "_create_estimator_inputs", "(", "\n", "action_by_evaluation_policy", "=", "action_by_evaluation_policy", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", ")", "\n", "for", "estimator_name", ",", "estimator", "in", "self", ".", "ope_estimators_", ".", "items", "(", ")", ":", "\n", "            ", "estimated_round_rewards_dict", "[", "\n", "estimator_name", "\n", "]", "=", "estimator", ".", "_estimate_round_rewards", "(", "**", "estimator_inputs", "[", "estimator_name", "]", ")", "\n", "", "estimated_round_rewards_df", "=", "DataFrame", "(", "estimated_round_rewards_dict", ")", "\n", "estimated_round_rewards_df", ".", "rename", "(", "\n", "columns", "=", "{", "key", ":", "key", ".", "upper", "(", ")", "for", "key", "in", "estimated_round_rewards_dict", ".", "keys", "(", ")", "}", ",", "\n", "inplace", "=", "True", ",", "\n", ")", "\n", "if", "is_relative", ":", "\n", "            ", "estimated_round_rewards_df", "/=", "self", ".", "bandit_feedback", "[", "\"reward\"", "]", ".", "mean", "(", ")", "\n", "\n", "", "plt", ".", "style", ".", "use", "(", "\"ggplot\"", ")", "\n", "fig", ",", "ax", "=", "plt", ".", "subplots", "(", "figsize", "=", "(", "8", ",", "6", ")", ")", "\n", "sns", ".", "barplot", "(", "\n", "data", "=", "estimated_round_rewards_df", ",", "\n", "ax", "=", "ax", ",", "\n", "ci", "=", "100", "*", "(", "1", "-", "alpha", ")", ",", "\n", "n_boot", "=", "n_bootstrap_samples", ",", "\n", "seed", "=", "random_state", ",", "\n", ")", "\n", "plt", ".", "xlabel", "(", "\"OPE Estimators\"", ",", "fontsize", "=", "25", ")", "\n", "plt", ".", "ylabel", "(", "\n", "f\"Estimated Policy Value (\u00b1 {np.int32(100*(1 - alpha))}% CI)\"", ",", "fontsize", "=", "20", "\n", ")", "\n", "plt", ".", "yticks", "(", "fontsize", "=", "15", ")", "\n", "plt", ".", "xticks", "(", "fontsize", "=", "25", "-", "2", "*", "len", "(", "self", ".", "ope_estimators", ")", ")", "\n", "\n", "if", "fig_dir", ":", "\n", "            ", "fig", ".", "savefig", "(", "str", "(", "fig_dir", "/", "fig_name", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta_continuous.ContinuousOffPolicyEvaluation.evaluate_performance_of_estimators": [[443, 524], ["sklearn.utils.check_scalar", "dict", "meta_continuous.ContinuousOffPolicyEvaluation._create_estimator_inputs", "meta_continuous.ContinuousOffPolicyEvaluation.ope_estimators_.items", "ValueError", "ValueError", "estimator.estimate_policy_value", "numpy.abs"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta.OffPolicyEvaluation._create_estimator_inputs", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_policy_value"], ["", "", "def", "evaluate_performance_of_estimators", "(", "\n", "self", ",", "\n", "ground_truth_policy_value", ":", "float", ",", "\n", "action_by_evaluation_policy", ":", "np", ".", "ndarray", ",", "\n", "estimated_rewards_by_reg_model", ":", "Optional", "[", "\n", "Union", "[", "np", ".", "ndarray", ",", "Dict", "[", "str", ",", "np", ".", "ndarray", "]", "]", "\n", "]", "=", "None", ",", "\n", "metric", ":", "str", "=", "\"se\"", ",", "\n", ")", "->", "Dict", "[", "str", ",", "float", "]", ":", "\n", "        ", "\"\"\"Evaluate the accuracy of OPE estimators.\n\n        Note\n        ------\n        Evaluate the estimation performance of OPE estimators with relative estimation error (relative-EE) or squared error (SE):\n\n        .. math ::\n            \\\\text{Relative-EE} (\\\\hat{V}; \\\\mathcal{D}) = \\\\left|  \\\\frac{\\\\hat{V}(\\\\pi; \\\\mathcal{D}) - V(\\\\pi)}{V(\\\\pi)} \\\\right|,\n\n        .. math ::\n            \\\\text{SE} (\\\\hat{V}; \\\\mathcal{D}) = \\\\left(\\\\hat{V}(\\\\pi; \\\\mathcal{D}) - V(\\\\pi) \\\\right)^2,\n\n        where :math:`V({\\\\pi})` is the ground-truth policy value of the evalation policy :math:`\\\\pi_e` (often estimated using on-policy estimation).\n        :math:`\\\\hat{V}(\\\\pi; \\\\mathcal{D})` is the policy value estimated by an OPE estimator :math:`\\\\hat{V}` and logged bandit feedback :math:`\\\\mathcal{D}`.\n\n        Parameters\n        ----------\n        ground_truth policy value: float\n            Ground_truth policy value of evaluation policy, i.e., :math:`V(\\\\pi)`.\n            With Open Bandit Dataset, we use an on-policy estimate of the policy value as its ground-truth.\n\n        action_by_evaluation_policy: array-like, shape (n_rounds,)\n            Continuous action values given by the (deterministic) evaluation policy, i.e., :math:`\\\\pi_e(x_t)`.\n\n        estimated_rewards_by_reg_model: array-like, shape (n_rounds, n_actions, len_list) or Dict[str, array-like], default=None\n            Estimated expected rewards given context, action, and position, i.e., :math:`\\\\hat{q}(x_i,a_i)`.\n            When an array-like is given, all OPE estimators use it.\n            When a dict with an estimator's name as its key is given, the corresponding value is used for the estimator.\n            If None, model-dependent estimators such as DM and DR cannot be used.\n\n        metric: str, default=\"se\"\n            Results of performance comparison among OPE estimators.\n            Must be either \"relative-ee\" or \"se\".\n\n        Returns\n        ----------\n        eval_metric_ope_dict: Dict[str, float]\n            Dictionary containing the value of evaluation metric for the estimation performance of OPE estimators.\n\n        \"\"\"", "\n", "\n", "check_scalar", "(", "\n", "ground_truth_policy_value", ",", "\n", "\"ground_truth_policy_value\"", ",", "\n", "float", ",", "\n", ")", "\n", "if", "metric", "not", "in", "[", "\"relative-ee\"", ",", "\"se\"", "]", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "f\"`metric` must be either 'relative-ee' or 'se', but {metric} is given\"", "\n", ")", "\n", "", "if", "metric", "==", "\"relative-ee\"", "and", "ground_truth_policy_value", "==", "0.0", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"`ground_truth_policy_value` must be non-zero when metric is relative-ee\"", "\n", ")", "\n", "\n", "", "eval_metric_ope_dict", "=", "dict", "(", ")", "\n", "estimator_inputs", "=", "self", ".", "_create_estimator_inputs", "(", "\n", "action_by_evaluation_policy", "=", "action_by_evaluation_policy", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", ")", "\n", "for", "estimator_name", ",", "estimator", "in", "self", ".", "ope_estimators_", ".", "items", "(", ")", ":", "\n", "            ", "estimated_policy_value", "=", "estimator", ".", "estimate_policy_value", "(", "\n", "**", "estimator_inputs", "[", "estimator_name", "]", "\n", ")", "\n", "if", "metric", "==", "\"relative-ee\"", ":", "\n", "                ", "relative_ee_", "=", "estimated_policy_value", "-", "ground_truth_policy_value", "\n", "relative_ee_", "/=", "ground_truth_policy_value", "\n", "eval_metric_ope_dict", "[", "estimator_name", "]", "=", "np", ".", "abs", "(", "relative_ee_", ")", "\n", "", "elif", "metric", "==", "\"se\"", ":", "\n", "                ", "se_", "=", "(", "estimated_policy_value", "-", "ground_truth_policy_value", ")", "**", "2", "\n", "eval_metric_ope_dict", "[", "estimator_name", "]", "=", "se_", "\n", "", "", "return", "eval_metric_ope_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta_continuous.ContinuousOffPolicyEvaluation.summarize_estimators_comparison": [[525, 569], ["pandas.DataFrame", "meta_continuous.ContinuousOffPolicyEvaluation.evaluate_performance_of_estimators"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta.OffPolicyEvaluation.evaluate_performance_of_estimators"], ["", "def", "summarize_estimators_comparison", "(", "\n", "self", ",", "\n", "ground_truth_policy_value", ":", "float", ",", "\n", "action_by_evaluation_policy", ":", "np", ".", "ndarray", ",", "\n", "estimated_rewards_by_reg_model", ":", "Optional", "[", "\n", "Union", "[", "np", ".", "ndarray", ",", "Dict", "[", "str", ",", "np", ".", "ndarray", "]", "]", "\n", "]", "=", "None", ",", "\n", "metric", ":", "str", "=", "\"se\"", ",", "\n", ")", "->", "DataFrame", ":", "\n", "        ", "\"\"\"Summarize performance comparison of OPE estimators.\n\n        Parameters\n        ----------\n        ground_truth policy value: float\n            Ground_truth policy value of evaluation policy, i.e., :math:`V(\\\\pi)`.\n            With Open Bandit Dataset, we use an on-policy estimate of the policy value as ground-truth.\n\n        action_by_evaluation_policy: array-like, shape (n_rounds,)\n            Continuous action values given by the (deterministic) evaluation policy, i.e., :math:`\\\\pi_e(x_t)`.\n\n        estimated_rewards_by_reg_model: array-like, shape (n_rounds, n_actions, len_list), default=None\n            Estimated expected rewards given context, action, and position, i.e., :math:`\\\\hat{q}(x_i,a_i)`.\n            If None, model-dependent estimators such as DM and DR cannot be used.\n\n        metric: str, default=\"se\"\n            Results of performance comparison among OPE estimators.\n            Must be either \"relative-ee\" or \"se\".\n\n        Returns\n        ----------\n        eval_metric_ope_df: DataFrame\n            Results of performance comparison among OPE estimators.\n\n        \"\"\"", "\n", "eval_metric_ope_df", "=", "DataFrame", "(", "\n", "self", ".", "evaluate_performance_of_estimators", "(", "\n", "ground_truth_policy_value", "=", "ground_truth_policy_value", ",", "\n", "action_by_evaluation_policy", "=", "action_by_evaluation_policy", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", "metric", "=", "metric", ",", "\n", ")", ",", "\n", "index", "=", "[", "metric", "]", ",", "\n", ")", "\n", "return", "eval_metric_ope_df", ".", "T", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta_continuous.ContinuousOffPolicyEvaluation.visualize_off_policy_estimates_of_multiple_policies": [[570, 676], ["zip", "matplotlib.style.use", "matplotlib.figure", "enumerate", "len", "len", "ValueError", "isinstance", "isinstance", "meta_continuous.ContinuousOffPolicyEvaluation._create_estimator_inputs", "meta_continuous.ContinuousOffPolicyEvaluation.ope_estimators_.items", "pandas.DataFrame", "matplotlib.figure.add_subplot", "seaborn.barplot", "plt.figure.add_subplot.set_title", "plt.figure.add_subplot.set_ylabel", "matplotlib.yticks", "matplotlib.xticks", "matplotlib.figure.savefig", "estimator._estimate_round_rewards", "meta_continuous.ContinuousOffPolicyEvaluation.bandit_feedback[].mean", "len", "estimator_name.upper", "str", "len", "numpy.int32", "len"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta.OffPolicyEvaluation._create_estimator_inputs", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SelfNormalizedSlateStandardIPS._estimate_round_rewards"], ["", "def", "visualize_off_policy_estimates_of_multiple_policies", "(", "\n", "self", ",", "\n", "policy_name_list", ":", "List", "[", "str", "]", ",", "\n", "action_by_evaluation_policy_list", ":", "List", "[", "np", ".", "ndarray", "]", ",", "\n", "estimated_rewards_by_reg_model", ":", "Optional", "[", "\n", "Union", "[", "np", ".", "ndarray", ",", "Dict", "[", "str", ",", "np", ".", "ndarray", "]", "]", "\n", "]", "=", "None", ",", "\n", "alpha", ":", "float", "=", "0.05", ",", "\n", "is_relative", ":", "bool", "=", "False", ",", "\n", "n_bootstrap_samples", ":", "int", "=", "100", ",", "\n", "random_state", ":", "Optional", "[", "int", "]", "=", "None", ",", "\n", "fig_dir", ":", "Optional", "[", "Path", "]", "=", "None", ",", "\n", "fig_name", ":", "str", "=", "\"estimated_policy_value.png\"", ",", "\n", ")", "->", "None", ":", "\n", "        ", "\"\"\"Visualize the estimated policy values.\n\n        Parameters\n        ----------\n        policy_name_list: List[str]\n            List of the names of evaluation policies.\n\n        action_by_evaluation_policy_list: List[array-like, shape (n_rounds, n_actions, len_list)]\n            List of action values given by the (deterministic) evaluation policies, i.e., :math:`\\\\pi_e(x_t)`.\n\n        estimated_rewards_by_reg_model: array-like, shape (n_rounds, n_actions, len_list) or Dict[str, array-like], default=None\n            Estimated expected rewards given context, action, and position, i.e., :math:`\\\\hat{q}(x_i,a_i)`.\n            When an array-like is given, all OPE estimators use it.\n            When a dict is given, if the dict has the name of an estimator as a key, the corresponding value is used.\n            If None, model-dependent estimators such as DM and DR cannot be used.\n\n        alpha: float, default=0.05\n            Significance level.\n\n        n_bootstrap_samples: int, default=100\n            Number of resampling performed in bootstrap sampling.\n\n        random_state: int, default=None\n            Controls the random seed in bootstrap sampling.\n\n        is_relative: bool, default=False,\n            If True, the method visualizes the estimated policy values of evaluation policy\n            relative to the ground-truth policy value of behavior policy.\n\n        fig_dir: Path, default=None\n            Path to store the bar figure.\n            If None, the figure will not be saved.\n\n        fig_name: str, default=\"estimated_policy_value.png\"\n            Name of the bar figure.\n\n        \"\"\"", "\n", "if", "len", "(", "policy_name_list", ")", "!=", "len", "(", "action_by_evaluation_policy_list", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"the length of `policy_name_list` must be the same as action_by_evaluation_policy_list\"", "\n", ")", "\n", "", "if", "fig_dir", "is", "not", "None", ":", "\n", "            ", "assert", "isinstance", "(", "fig_dir", ",", "Path", ")", ",", "\"`fig_dir` must be a Path\"", "\n", "", "if", "fig_name", "is", "not", "None", ":", "\n", "            ", "assert", "isinstance", "(", "fig_name", ",", "str", ")", ",", "\"`fig_dir` must be a string\"", "\n", "\n", "", "estimated_round_rewards_dict", "=", "{", "\n", "estimator_name", ":", "{", "}", "for", "estimator_name", "in", "self", ".", "ope_estimators_", "\n", "}", "\n", "\n", "for", "policy_name", ",", "action_by_evaluation_policy", "in", "zip", "(", "\n", "policy_name_list", ",", "action_by_evaluation_policy_list", "\n", ")", ":", "\n", "            ", "estimator_inputs", "=", "self", ".", "_create_estimator_inputs", "(", "\n", "action_by_evaluation_policy", "=", "action_by_evaluation_policy", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", ")", "\n", "for", "estimator_name", ",", "estimator", "in", "self", ".", "ope_estimators_", ".", "items", "(", ")", ":", "\n", "                ", "estimated_round_rewards_dict", "[", "estimator_name", "]", "[", "\n", "policy_name", "\n", "]", "=", "estimator", ".", "_estimate_round_rewards", "(", "\n", "**", "estimator_inputs", "[", "estimator_name", "]", "\n", ")", "\n", "\n", "", "", "plt", ".", "style", ".", "use", "(", "\"ggplot\"", ")", "\n", "fig", "=", "plt", ".", "figure", "(", "figsize", "=", "(", "8", ",", "6.2", "*", "len", "(", "self", ".", "ope_estimators_", ")", ")", ")", "\n", "\n", "for", "i", ",", "estimator_name", "in", "enumerate", "(", "self", ".", "ope_estimators_", ")", ":", "\n", "            ", "estimated_round_rewards_df", "=", "DataFrame", "(", "\n", "estimated_round_rewards_dict", "[", "estimator_name", "]", "\n", ")", "\n", "if", "is_relative", ":", "\n", "                ", "estimated_round_rewards_df", "/=", "self", ".", "bandit_feedback", "[", "\"reward\"", "]", ".", "mean", "(", ")", "\n", "\n", "", "ax", "=", "fig", ".", "add_subplot", "(", "len", "(", "action_by_evaluation_policy_list", ")", ",", "1", ",", "i", "+", "1", ")", "\n", "sns", ".", "barplot", "(", "\n", "data", "=", "estimated_round_rewards_df", ",", "\n", "ax", "=", "ax", ",", "\n", "ci", "=", "100", "*", "(", "1", "-", "alpha", ")", ",", "\n", "n_boot", "=", "n_bootstrap_samples", ",", "\n", "seed", "=", "random_state", ",", "\n", ")", "\n", "ax", ".", "set_title", "(", "estimator_name", ".", "upper", "(", ")", ",", "fontsize", "=", "20", ")", "\n", "ax", ".", "set_ylabel", "(", "\n", "f\"Estimated Policy Value (\u00b1 {np.int32(100*(1 - alpha))}% CI)\"", ",", "\n", "fontsize", "=", "20", ",", "\n", ")", "\n", "plt", ".", "yticks", "(", "fontsize", "=", "15", ")", "\n", "plt", ".", "xticks", "(", "fontsize", "=", "25", "-", "2", "*", "len", "(", "policy_name_list", ")", ")", "\n", "\n", "", "if", "fig_dir", ":", "\n", "            ", "fig", ".", "savefig", "(", "str", "(", "fig_dir", "/", "fig_name", ")", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.regression_model.RegressionModel.__post_init__": [[67, 88], ["sklearn.utils.check_scalar", "sklearn.utils.check_scalar", "ValueError", "isinstance", "ValueError", "sklearn.base.clone", "numpy.eye", "isinstance", "numpy.arange"], "methods", ["None"], ["def", "__post_init__", "(", "self", ")", "->", "None", ":", "\n", "        ", "\"\"\"Initialize Class.\"\"\"", "\n", "check_scalar", "(", "self", ".", "n_actions", ",", "\"n_actions\"", ",", "int", ",", "min_val", "=", "2", ")", "\n", "check_scalar", "(", "self", ".", "len_list", ",", "\"len_list\"", ",", "int", ",", "min_val", "=", "1", ")", "\n", "if", "not", "(", "\n", "isinstance", "(", "self", ".", "fitting_method", ",", "str", ")", "\n", "and", "self", ".", "fitting_method", "in", "[", "\"normal\"", ",", "\"iw\"", ",", "\"mrdr\"", "]", "\n", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "f\"`fitting_method` must be one of 'normal', 'iw', or 'mrdr', but {self.fitting_method} is given\"", "\n", ")", "\n", "", "if", "not", "isinstance", "(", "self", ".", "base_model", ",", "BaseEstimator", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"`base_model` must be BaseEstimator or a child class of BaseEstimator\"", "\n", ")", "\n", "\n", "", "self", ".", "base_model_list", "=", "[", "\n", "clone", "(", "self", ".", "base_model", ")", "for", "_", "in", "np", ".", "arange", "(", "self", ".", "len_list", ")", "\n", "]", "\n", "if", "self", ".", "action_context", "is", "None", ":", "\n", "            ", "self", ".", "action_context", "=", "np", ".", "eye", "(", "self", ".", "n_actions", ",", "dtype", "=", "int", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.regression_model.RegressionModel.fit": [[89, 181], ["utils.check_bandit_feedback_inputs", "numpy.arange", "numpy.zeros_like", "regression_model.RegressionModel._pre_process_for_reg_model", "numpy.zeros_like.max", "ValueError", "ValueError", "ValueError", "numpy.allclose", "ValueError", "numpy.ones_like", "ValueError", "regression_model.RegressionModel.base_model_list[].fit", "isinstance", "action_dist.sum", "regression_model.RegressionModel.base_model_list[].fit", "regression_model.RegressionModel.base_model_list[].fit", "numpy.zeros_like.max", "numpy.arange"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_bandit_feedback_inputs", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.regression_model.RegressionModel._pre_process_for_reg_model", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.classification_model.PropensityScoreEstimator.fit", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.classification_model.PropensityScoreEstimator.fit", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.classification_model.PropensityScoreEstimator.fit"], ["", "", "def", "fit", "(", "\n", "self", ",", "\n", "context", ":", "np", ".", "ndarray", ",", "\n", "action", ":", "np", ".", "ndarray", ",", "\n", "reward", ":", "np", ".", "ndarray", ",", "\n", "pscore", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "position", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "action_dist", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", ")", "->", "None", ":", "\n", "        ", "\"\"\"Fit the regression model on given logged bandit data.\n\n        Parameters\n        ----------\n        context: array-like, shape (n_rounds, dim_context)\n            Context vectors observed for each data in logged bandit data, i.e., :math:`x_i`.\n\n        action: array-like, shape (n_rounds,)\n            Actions sampled by the logging/behavior policy for each data in logged bandit data, i.e., :math:`a_i`.\n\n        reward: array-like, shape (n_rounds,)\n            Rewards observed for each data in logged bandit data, i.e., :math:`r_i`.\n\n        pscore: array-like, shape (n_rounds,)\n            Action choice probabilities of the logging/behavior policy (propensity scores), i.e., :math:`\\\\pi_b(a_i|x_i)`.\n            If None, behavior policy is assumed to be uniform.\n\n        position: array-like, shape (n_rounds,), default=None\n            Indices to differentiate positions in a recommendation interface where the actions are presented.\n            If None, a regression model assumes that only a single action is chosen for each data.\n            When `len_list` > 1, an array must be given as `position`.\n\n        action_dist: array-like, shape (n_rounds, n_actions, len_list), default=None\n            Action choice probabilities of the evaluation policy (can be deterministic), i.e., :math:`\\\\pi_e(a_i|x_i)`.\n            When either 'iw' or 'mrdr' is set to `fitting_method`, `action_dist` must be given.\n\n        \"\"\"", "\n", "check_bandit_feedback_inputs", "(", "\n", "context", "=", "context", ",", "\n", "action", "=", "action", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore", "=", "pscore", ",", "\n", "position", "=", "position", ",", "\n", "action_context", "=", "self", ".", "action_context", ",", "\n", ")", "\n", "n", "=", "context", ".", "shape", "[", "0", "]", "\n", "\n", "if", "position", "is", "None", "or", "self", ".", "len_list", "==", "1", ":", "\n", "            ", "position", "=", "np", ".", "zeros_like", "(", "action", ")", "\n", "", "else", ":", "\n", "            ", "if", "position", ".", "max", "(", ")", ">=", "self", ".", "len_list", ":", "\n", "                ", "raise", "ValueError", "(", "\n", "f\"`position` elements must be smaller than `len_list`, but the maximum value is {position.max()} (>= {self.len_list})\"", "\n", ")", "\n", "", "", "if", "self", ".", "fitting_method", "in", "[", "\"iw\"", ",", "\"mrdr\"", "]", ":", "\n", "            ", "if", "not", "(", "isinstance", "(", "action_dist", ",", "np", ".", "ndarray", ")", "and", "action_dist", ".", "ndim", "==", "3", ")", ":", "\n", "                ", "raise", "ValueError", "(", "\n", "\"when `fitting_method` is either 'iw' or 'mrdr', `action_dist` (a 3-dimensional ndarray) must be given\"", "\n", ")", "\n", "", "if", "action_dist", ".", "shape", "!=", "(", "n", ",", "self", ".", "n_actions", ",", "self", ".", "len_list", ")", ":", "\n", "                ", "raise", "ValueError", "(", "\n", "f\"shape of `action_dist` must be (n_rounds, n_actions, len_list)=({n, self.n_actions, self.len_list}), but is {action_dist.shape}\"", "\n", ")", "\n", "", "if", "not", "np", ".", "allclose", "(", "action_dist", ".", "sum", "(", "axis", "=", "1", ")", ",", "1", ")", ":", "\n", "                ", "raise", "ValueError", "(", "\"`action_dist` must be a probability distribution\"", ")", "\n", "", "", "if", "pscore", "is", "None", ":", "\n", "            ", "pscore", "=", "np", ".", "ones_like", "(", "action", ")", "/", "self", ".", "n_actions", "\n", "\n", "", "for", "pos_", "in", "np", ".", "arange", "(", "self", ".", "len_list", ")", ":", "\n", "            ", "idx", "=", "position", "==", "pos_", "\n", "X", "=", "self", ".", "_pre_process_for_reg_model", "(", "\n", "context", "=", "context", "[", "idx", "]", ",", "\n", "action", "=", "action", "[", "idx", "]", ",", "\n", "action_context", "=", "self", ".", "action_context", ",", "\n", ")", "\n", "if", "X", ".", "shape", "[", "0", "]", "==", "0", ":", "\n", "                ", "raise", "ValueError", "(", "f\"No training data at position {pos_}\"", ")", "\n", "# train the base model according to the given `fitting method`", "\n", "", "if", "self", ".", "fitting_method", "==", "\"normal\"", ":", "\n", "                ", "self", ".", "base_model_list", "[", "pos_", "]", ".", "fit", "(", "X", ",", "reward", "[", "idx", "]", ")", "\n", "", "else", ":", "\n", "                ", "action_dist_at_pos", "=", "action_dist", "[", "np", ".", "arange", "(", "n", ")", ",", "action", ",", "pos_", "]", "[", "idx", "]", "\n", "if", "self", ".", "fitting_method", "==", "\"iw\"", ":", "\n", "                    ", "sample_weight", "=", "action_dist_at_pos", "/", "pscore", "[", "idx", "]", "\n", "self", ".", "base_model_list", "[", "pos_", "]", ".", "fit", "(", "\n", "X", ",", "reward", "[", "idx", "]", ",", "sample_weight", "=", "sample_weight", "\n", ")", "\n", "", "elif", "self", ".", "fitting_method", "==", "\"mrdr\"", ":", "\n", "                    ", "sample_weight", "=", "action_dist_at_pos", "\n", "sample_weight", "*=", "1.0", "-", "pscore", "[", "idx", "]", "\n", "sample_weight", "/=", "pscore", "[", "idx", "]", "**", "2", "\n", "self", ".", "base_model_list", "[", "pos_", "]", ".", "fit", "(", "\n", "X", ",", "reward", "[", "idx", "]", ",", "sample_weight", "=", "sample_weight", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.regression_model.RegressionModel.predict": [[183, 213], ["numpy.zeros", "numpy.arange", "numpy.arange", "regression_model.RegressionModel._pre_process_for_reg_model", "sklearn.base.is_classifier", "regression_model.RegressionModel.base_model_list[].predict", "regression_model.RegressionModel.base_model_list[].predict_proba", "numpy.ones", "numpy.arange"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.regression_model.RegressionModel._pre_process_for_reg_model", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.classification_model.PropensityScoreEstimator.predict", "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.offline.NNPolicyLearner.predict_proba"], ["", "", "", "", "def", "predict", "(", "self", ",", "context", ":", "np", ".", "ndarray", ")", "->", "np", ".", "ndarray", ":", "\n", "        ", "\"\"\"Predict the reward function.\n\n        Parameters\n        -----------\n        context: array-like, shape (n_rounds_of_new_data, dim_context)\n            Context vectors of new data.\n\n        Returns\n        -----------\n        q_hat: array-like, shape (n_rounds_of_new_data, n_actions, len_list)\n            Expected rewards of new data estimated by the regression model.\n\n        \"\"\"", "\n", "n", "=", "context", ".", "shape", "[", "0", "]", "\n", "q_hat", "=", "np", ".", "zeros", "(", "(", "n", ",", "self", ".", "n_actions", ",", "self", ".", "len_list", ")", ")", "\n", "for", "action_", "in", "np", ".", "arange", "(", "self", ".", "n_actions", ")", ":", "\n", "            ", "for", "pos_", "in", "np", ".", "arange", "(", "self", ".", "len_list", ")", ":", "\n", "                ", "X", "=", "self", ".", "_pre_process_for_reg_model", "(", "\n", "context", "=", "context", ",", "\n", "action", "=", "action_", "*", "np", ".", "ones", "(", "n", ",", "int", ")", ",", "\n", "action_context", "=", "self", ".", "action_context", ",", "\n", ")", "\n", "q_hat_", "=", "(", "\n", "self", ".", "base_model_list", "[", "pos_", "]", ".", "predict_proba", "(", "X", ")", "[", ":", ",", "1", "]", "\n", "if", "is_classifier", "(", "self", ".", "base_model_list", "[", "pos_", "]", ")", "\n", "else", "self", ".", "base_model_list", "[", "pos_", "]", ".", "predict", "(", "X", ")", "\n", ")", "\n", "q_hat", "[", "np", ".", "arange", "(", "n", ")", ",", "action_", ",", "pos_", "]", "=", "q_hat_", "\n", "", "", "return", "q_hat", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.regression_model.RegressionModel.fit_predict": [[214, 332], ["utils.check_bandit_feedback_inputs", "sklearn.utils.check_scalar", "sklearn.utils.check_random_state", "sklearn.model_selection.KFold", "sklearn.model_selection.KFold.get_n_splits", "sklearn.model_selection.KFold.split", "numpy.zeros_like", "regression_model.RegressionModel.fit", "regression_model.RegressionModel.predict", "numpy.zeros", "regression_model.RegressionModel.fit", "regression_model.RegressionModel.predict", "numpy.zeros_like.max", "ValueError", "ValueError", "ValueError", "numpy.ones_like", "isinstance", "numpy.zeros_like.max"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_bandit_feedback_inputs", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.classification_model.PropensityScoreEstimator.fit", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.classification_model.PropensityScoreEstimator.predict", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.classification_model.PropensityScoreEstimator.fit", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.classification_model.PropensityScoreEstimator.predict"], ["", "def", "fit_predict", "(", "\n", "self", ",", "\n", "context", ":", "np", ".", "ndarray", ",", "\n", "action", ":", "np", ".", "ndarray", ",", "\n", "reward", ":", "np", ".", "ndarray", ",", "\n", "pscore", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "position", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "action_dist", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "n_folds", ":", "int", "=", "1", ",", "\n", "random_state", ":", "Optional", "[", "int", "]", "=", "None", ",", "\n", ")", "->", "np", ".", "ndarray", ":", "\n", "        ", "\"\"\"Fit the regression model on given logged bandit data and estimate the expected rewards on the same data.\n\n        Note\n        ------\n        When `n_folds` is larger than 1, the cross-fitting procedure is applied.\n        See the reference for the details about the cross-fitting technique.\n\n        Parameters\n        ----------\n        context: array-like, shape (n_rounds, dim_context)\n            Context vectors observed for each data in logged bandit data, i.e., :math:`x_i`.\n\n        action: array-like, shape (n_rounds,)\n            Actions sampled by the logging/behavior policy for each data in logged bandit data, i.e., :math:`a_i`.\n\n        reward: array-like, shape (n_rounds,)\n            Rewards observed for each data in logged bandit data, i.e., :math:`r_i`.\n\n        pscore: array-like, shape (n_rounds,), default=None\n            Action choice probabilities (propensity score) of a behavior policy\n            in the training set of logged bandit data.\n            If None, the the behavior policy is assumed to be uniform random.\n\n        position: array-like, shape (n_rounds,), default=None\n            Indices to differentiate positions in a recommendation interface where the actions are presented.\n            If None, a regression model assumes that only a single action is chosen for each data.\n            When `len_list` > 1, an array must be given as `position`.\n\n        action_dist: array-like, shape (n_rounds, n_actions, len_list), default=None\n            Action choice probabilities of the evaluation policy (can be deterministic), i.e., :math:`\\\\pi_e(a_i|x_i)`.\n            When either 'iw' or 'mrdr' is set to `fitting_method`, `action_dist` must be given.\n\n        n_folds: int, default=1\n            Number of folds in the cross-fitting procedure.\n            When 1 is given, the regression model is trained on the whole logged bandit data.\n            Please refer to https://arxiv.org/abs/2002.08536 about the details of the cross-fitting procedure.\n\n        random_state: int, default=None\n            `random_state` affects the ordering of the indices, which controls the randomness of each fold.\n            See https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html for the details.\n\n        Returns\n        -----------\n        q_hat: array-like, shape (n_rounds, n_actions, len_list)\n            Expected rewards of new data estimated by the regression model.\n\n        \"\"\"", "\n", "check_bandit_feedback_inputs", "(", "\n", "context", "=", "context", ",", "\n", "action", "=", "action", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore", "=", "pscore", ",", "\n", "position", "=", "position", ",", "\n", "action_context", "=", "self", ".", "action_context", ",", "\n", ")", "\n", "n_rounds", "=", "context", ".", "shape", "[", "0", "]", "\n", "\n", "check_scalar", "(", "n_folds", ",", "\"n_folds\"", ",", "int", ",", "min_val", "=", "1", ")", "\n", "check_random_state", "(", "random_state", ")", "\n", "\n", "if", "position", "is", "None", "or", "self", ".", "len_list", "==", "1", ":", "\n", "            ", "position", "=", "np", ".", "zeros_like", "(", "action", ")", "\n", "", "else", ":", "\n", "            ", "if", "position", ".", "max", "(", ")", ">=", "self", ".", "len_list", ":", "\n", "                ", "raise", "ValueError", "(", "\n", "f\"`position` elements must be smaller than `len_list`, but the maximum value is {position.max()} (>= {self.len_list})\"", "\n", ")", "\n", "", "", "if", "self", ".", "fitting_method", "in", "[", "\"iw\"", ",", "\"mrdr\"", "]", ":", "\n", "            ", "if", "not", "(", "isinstance", "(", "action_dist", ",", "np", ".", "ndarray", ")", "and", "action_dist", ".", "ndim", "==", "3", ")", ":", "\n", "                ", "raise", "ValueError", "(", "\n", "\"when `fitting_method` is either 'iw' or 'mrdr', `action_dist` (a 3-dimensional ndarray) must be given\"", "\n", ")", "\n", "", "if", "action_dist", ".", "shape", "!=", "(", "n_rounds", ",", "self", ".", "n_actions", ",", "self", ".", "len_list", ")", ":", "\n", "                ", "raise", "ValueError", "(", "\n", "f\"shape of `action_dist` must be (n_rounds, n_actions, len_list)=({n_rounds, self.n_actions, self.len_list}), but is {action_dist.shape}\"", "\n", ")", "\n", "", "", "if", "pscore", "is", "None", ":", "\n", "            ", "pscore", "=", "np", ".", "ones_like", "(", "action", ")", "/", "self", ".", "n_actions", "\n", "\n", "", "if", "n_folds", "==", "1", ":", "\n", "            ", "self", ".", "fit", "(", "\n", "context", "=", "context", ",", "\n", "action", "=", "action", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore", "=", "pscore", ",", "\n", "position", "=", "position", ",", "\n", "action_dist", "=", "action_dist", ",", "\n", ")", "\n", "return", "self", ".", "predict", "(", "context", "=", "context", ")", "\n", "", "else", ":", "\n", "            ", "q_hat", "=", "np", ".", "zeros", "(", "(", "n_rounds", ",", "self", ".", "n_actions", ",", "self", ".", "len_list", ")", ")", "\n", "", "kf", "=", "KFold", "(", "n_splits", "=", "n_folds", ",", "shuffle", "=", "True", ",", "random_state", "=", "random_state", ")", "\n", "kf", ".", "get_n_splits", "(", "context", ")", "\n", "for", "train_idx", ",", "test_idx", "in", "kf", ".", "split", "(", "context", ")", ":", "\n", "            ", "action_dist_tr", "=", "(", "\n", "action_dist", "[", "train_idx", "]", "if", "action_dist", "is", "not", "None", "else", "action_dist", "\n", ")", "\n", "self", ".", "fit", "(", "\n", "context", "=", "context", "[", "train_idx", "]", ",", "\n", "action", "=", "action", "[", "train_idx", "]", ",", "\n", "reward", "=", "reward", "[", "train_idx", "]", ",", "\n", "pscore", "=", "pscore", "[", "train_idx", "]", ",", "\n", "position", "=", "position", "[", "train_idx", "]", ",", "\n", "action_dist", "=", "action_dist_tr", ",", "\n", ")", "\n", "q_hat", "[", "test_idx", ",", ":", ",", ":", "]", "=", "self", ".", "predict", "(", "context", "=", "context", "[", "test_idx", "]", ")", "\n", "", "return", "q_hat", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.regression_model.RegressionModel._pre_process_for_reg_model": [[333, 359], ["None"], "methods", ["None"], ["", "def", "_pre_process_for_reg_model", "(", "\n", "self", ",", "\n", "context", ":", "np", ".", "ndarray", ",", "\n", "action", ":", "np", ".", "ndarray", ",", "\n", "action_context", ":", "np", ".", "ndarray", ",", "\n", ")", "->", "np", ".", "ndarray", ":", "\n", "        ", "\"\"\"Preprocess feature vectors to train a regression model.\n\n        Note\n        -----\n        Please override this method if you want to use another feature enginnering\n        for training the regression model.\n\n        Parameters\n        -----------\n        context: array-like, shape (n_rounds,)\n            Context vectors observed for each data in logged bandit data, i.e., :math:`x_i`.\n\n        action: array-like, shape (n_rounds,)\n            Actions sampled by the logging/behavior policy for each data in logged bandit data, i.e., :math:`a_i`.\n\n        action_context: array-like, shape shape (n_actions, dim_action_context)\n            Context vectors characterizing actions (i.e., a vector representation or an embedding of each action).\n\n        \"\"\"", "\n", "return", "np", ".", "c_", "[", "context", ",", "action_context", "[", "action", "]", "]", "\n", "", "", ""]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators.BaseOffPolicyEstimator._estimate_round_rewards": [[25, 29], ["None"], "methods", ["None"], ["@", "abstractmethod", "\n", "def", "_estimate_round_rewards", "(", "self", ")", "->", "np", ".", "ndarray", ":", "\n", "        ", "\"\"\"Estimate round-wise (or sample-wise) rewards.\"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators.BaseOffPolicyEstimator.estimate_policy_value": [[30, 34], ["None"], "methods", ["None"], ["", "@", "abstractmethod", "\n", "def", "estimate_policy_value", "(", "self", ")", "->", "float", ":", "\n", "        ", "\"\"\"Estimate the policy value of evaluation policy.\"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators.BaseOffPolicyEstimator.estimate_interval": [[35, 39], ["None"], "methods", ["None"], ["", "@", "abstractmethod", "\n", "def", "estimate_interval", "(", "self", ")", "->", "Dict", "[", "str", ",", "float", "]", ":", "\n", "        ", "\"\"\"Estimate the confidence interval of the policy value using bootstrap.\"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators.ReplayMethod._estimate_round_rewards": [[73, 115], ["numpy.array", "numpy.zeros_like", "numpy.zeros", "numpy.array.sum", "numpy.array.mean", "numpy.arange"], "methods", ["None"], ["def", "_estimate_round_rewards", "(", "\n", "self", ",", "\n", "reward", ":", "np", ".", "ndarray", ",", "\n", "action", ":", "np", ".", "ndarray", ",", "\n", "action_dist", ":", "np", ".", "ndarray", ",", "\n", "position", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "np", ".", "ndarray", ":", "\n", "        ", "\"\"\"Estimate round-wise (or sample-wise) rewards.\n\n        Parameters\n        ------------\n        reward: array-like, shape (n_rounds,)\n            Rewards observed for each data in logged bandit data, i.e., :math:`r_i`.\n\n        action: array-like, shape (n_rounds,)\n            Actions sampled by the logging/behavior policy for each data in logged bandit data, i.e., :math:`a_i`.\n\n        action_dist: array-like, shape (n_rounds, n_actions, len_list)\n            Action choice probabilities of the evaluation policy (can be deterministic), i.e., :math:`\\\\pi_e(a_i|x_i)`.\n\n        position: array-like, shape (n_rounds,), default=None\n            Indices to differentiate positions in a recommendation interface where the actions are presented.\n            If None, the effect of position on the reward will be ignored.\n            (If only a single action is chosen for each data, you can just ignore this argument.)\n\n        Returns\n        ----------\n        estimated_rewards: array-like, shape (n_rounds,)\n            Estimated rewards for each observation.\n\n        \"\"\"", "\n", "if", "position", "is", "None", ":", "\n", "            ", "position", "=", "np", ".", "zeros", "(", "action_dist", ".", "shape", "[", "0", "]", ",", "dtype", "=", "int", ")", "\n", "\n", "", "action_match", "=", "np", ".", "array", "(", "\n", "action_dist", "[", "np", ".", "arange", "(", "action", ".", "shape", "[", "0", "]", ")", ",", "action", ",", "position", "]", "==", "1", "\n", ")", "\n", "estimated_rewards", "=", "np", ".", "zeros_like", "(", "action_match", ")", "\n", "if", "action_match", ".", "sum", "(", ")", ">", "0.0", ":", "\n", "            ", "estimated_rewards", "=", "action_match", "*", "reward", "/", "action_match", ".", "mean", "(", ")", "\n", "", "return", "estimated_rewards", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators.ReplayMethod.estimate_policy_value": [[116, 161], ["utils.check_array", "utils.check_array", "utils.check_ope_inputs", "estimators.ReplayMethod._estimate_round_rewards().mean", "numpy.zeros", "estimators.ReplayMethod._estimate_round_rewards"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_ope_inputs", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SelfNormalizedSlateStandardIPS._estimate_round_rewards"], ["", "def", "estimate_policy_value", "(", "\n", "self", ",", "\n", "reward", ":", "np", ".", "ndarray", ",", "\n", "action", ":", "np", ".", "ndarray", ",", "\n", "action_dist", ":", "np", ".", "ndarray", ",", "\n", "position", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "float", ":", "\n", "        ", "\"\"\"Estimate the policy value of evaluation policy.\n\n        Parameters\n        ------------\n        reward: array-like, shape (n_rounds,)\n            Rewards observed for each data in logged bandit data, i.e., :math:`r_i`.\n\n        action: array-like, shape (n_rounds,)\n            Actions sampled by the logging/behavior policy for each data in logged bandit data, i.e., :math:`a_i`.\n\n        action_dist: array-like, shape (n_rounds, n_actions, len_list)\n            Action choice probabilities of the evaluation policy (can be deterministic), i.e., :math:`\\\\pi_e(a_i|x_i)`.\n\n        position: array-like, shape (n_rounds,), default=None\n            Indices to differentiate positions in a recommendation interface where the actions are presented.\n            If None, the effect of position on the reward will be ignored.\n            (If only a single action is chosen for each data, you can just ignore this argument.)\n\n        Returns\n        ----------\n        V_hat: float\n            Estimated policy value of evaluation policy.\n\n        \"\"\"", "\n", "check_array", "(", "array", "=", "reward", ",", "name", "=", "\"reward\"", ",", "expected_dim", "=", "1", ")", "\n", "check_array", "(", "array", "=", "action", ",", "name", "=", "\"action\"", ",", "expected_dim", "=", "1", ")", "\n", "check_ope_inputs", "(", "\n", "action_dist", "=", "action_dist", ",", "position", "=", "position", ",", "action", "=", "action", ",", "reward", "=", "reward", "\n", ")", "\n", "if", "position", "is", "None", ":", "\n", "            ", "position", "=", "np", ".", "zeros", "(", "action_dist", ".", "shape", "[", "0", "]", ",", "dtype", "=", "int", ")", "\n", "\n", "", "return", "self", ".", "_estimate_round_rewards", "(", "\n", "reward", "=", "reward", ",", "\n", "action", "=", "action", ",", "\n", "position", "=", "position", ",", "\n", "action_dist", "=", "action_dist", ",", "\n", ")", ".", "mean", "(", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators.ReplayMethod.estimate_interval": [[163, 226], ["utils.check_array", "utils.check_array", "utils.check_ope_inputs", "estimators.ReplayMethod._estimate_round_rewards", "utils.estimate_confidence_interval_by_bootstrap", "numpy.zeros"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_ope_inputs", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SelfNormalizedSlateStandardIPS._estimate_round_rewards", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.estimate_confidence_interval_by_bootstrap"], ["", "def", "estimate_interval", "(", "\n", "self", ",", "\n", "reward", ":", "np", ".", "ndarray", ",", "\n", "action", ":", "np", ".", "ndarray", ",", "\n", "action_dist", ":", "np", ".", "ndarray", ",", "\n", "position", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "alpha", ":", "float", "=", "0.05", ",", "\n", "n_bootstrap_samples", ":", "int", "=", "100", ",", "\n", "random_state", ":", "Optional", "[", "int", "]", "=", "None", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "Dict", "[", "str", ",", "float", "]", ":", "\n", "        ", "\"\"\"Estimate the confidence interval of the policy value using bootstrap.\n\n        Parameters\n        ----------\n        reward: array-like, shape (n_rounds,)\n            Rewards observed for each data in logged bandit data, i.e., :math:`r_i`.\n\n        action: array-like, shape (n_rounds,)\n            Actions sampled by the logging/behavior policy for each data in logged bandit data, i.e., :math:`a_i`.\n\n        action_dist: array-like, shape (n_rounds, n_actions, len_list)\n            Action choice probabilities of the evaluation policy (can be deterministic), i.e., :math:`\\\\pi_e(a_i|x_i)`.\n\n        position: array-like, shape (n_rounds,), default=None\n            Indices to differentiate positions in a recommendation interface where the actions are presented.\n            If None, the effect of position on the reward will be ignored.\n            (If only a single action is chosen for each data, you can just ignore this argument.)\n\n        alpha: float, default=0.05\n            Significance level.\n\n        n_bootstrap_samples: int, default=10000\n            Number of resampling performed in bootstrap sampling.\n\n        random_state: int, default=None\n            Controls the random seed in bootstrap sampling.\n\n        Returns\n        ----------\n        estimated_confidence_interval: Dict[str, float]\n            Dictionary storing the estimated mean and upper-lower confidence bounds.\n\n        \"\"\"", "\n", "check_array", "(", "array", "=", "reward", ",", "name", "=", "\"reward\"", ",", "expected_dim", "=", "1", ")", "\n", "check_array", "(", "array", "=", "action", ",", "name", "=", "\"action\"", ",", "expected_dim", "=", "1", ")", "\n", "check_ope_inputs", "(", "\n", "action_dist", "=", "action_dist", ",", "position", "=", "position", ",", "action", "=", "action", ",", "reward", "=", "reward", "\n", ")", "\n", "if", "position", "is", "None", ":", "\n", "            ", "position", "=", "np", ".", "zeros", "(", "action_dist", ".", "shape", "[", "0", "]", ",", "dtype", "=", "int", ")", "\n", "\n", "", "estimated_round_rewards", "=", "self", ".", "_estimate_round_rewards", "(", "\n", "reward", "=", "reward", ",", "\n", "action", "=", "action", ",", "\n", "position", "=", "position", ",", "\n", "action_dist", "=", "action_dist", ",", "\n", ")", "\n", "return", "estimate_confidence_interval_by_bootstrap", "(", "\n", "samples", "=", "estimated_round_rewards", ",", "\n", "alpha", "=", "alpha", ",", "\n", "n_bootstrap_samples", "=", "n_bootstrap_samples", ",", "\n", "random_state", "=", "random_state", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators.InverseProbabilityWeighting.__post_init__": [[280, 293], ["sklearn.utils.check_scalar", "ValueError", "isinstance", "TypeError", "type"], "methods", ["None"], ["def", "__post_init__", "(", "self", ")", "->", "None", ":", "\n", "        ", "\"\"\"Initialize Class.\"\"\"", "\n", "check_scalar", "(", "\n", "self", ".", "lambda_", ",", "\n", "name", "=", "\"lambda_\"", ",", "\n", "target_type", "=", "(", "int", ",", "float", ")", ",", "\n", "min_val", "=", "0.0", ",", "\n", ")", "\n", "if", "self", ".", "lambda_", "!=", "self", ".", "lambda_", ":", "\n", "            ", "raise", "ValueError", "(", "\"`lambda_` must not be nan\"", ")", "\n", "", "if", "not", "isinstance", "(", "self", ".", "use_estimated_pscore", ",", "bool", ")", ":", "\n", "            ", "raise", "TypeError", "(", "\n", "f\"`use_estimated_pscore` must be a bool, but {type(self.use_estimated_pscore)} is given\"", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators.InverseProbabilityWeighting._estimate_round_rewards": [[295, 339], ["isinstance", "numpy.zeros", "numpy.minimum", "numpy.arange"], "methods", ["None"], ["", "", "def", "_estimate_round_rewards", "(", "\n", "self", ",", "\n", "reward", ":", "np", ".", "ndarray", ",", "\n", "action", ":", "np", ".", "ndarray", ",", "\n", "pscore", ":", "np", ".", "ndarray", ",", "\n", "action_dist", ":", "np", ".", "ndarray", ",", "\n", "position", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "np", ".", "ndarray", ":", "\n", "        ", "\"\"\"Estimate round-wise (or sample-wise) rewards.\n\n        Parameters\n        ----------\n        reward: array-like, shape (n_rounds,)\n            Rewards observed for each data in logged bandit data, i.e., :math:`r_i`.\n\n        action: array-like, shape (n_rounds,)\n            Actions sampled by the logging/behavior policy for each data in logged bandit data, i.e., :math:`a_i`.\n\n        pscore: array-like, shape (n_rounds,)\n            Action choice probabilities of the logging/behavior policy (propensity scores), i.e., :math:`\\\\pi_b(a_i|x_i)`.\n\n        action_dist: array-like, shape (n_rounds, n_actions, len_list)\n            Action choice probabilities of the evaluation policy (can be deterministic), i.e., :math:`\\\\pi_e(a_i|x_i)`.\n\n        position: array-like, shape (n_rounds,), default=None\n            Indices to differentiate positions in a recommendation interface where the actions are presented.\n            If None, the effect of position on the reward will be ignored.\n            (If only a single action is chosen for each data, you can just ignore this argument.)\n\n        Returns\n        ----------\n        estimated_rewards: array-like, shape (n_rounds,)\n            Estimated rewards for each observation.\n\n        \"\"\"", "\n", "if", "position", "is", "None", ":", "\n", "            ", "position", "=", "np", ".", "zeros", "(", "action_dist", ".", "shape", "[", "0", "]", ",", "dtype", "=", "int", ")", "\n", "\n", "", "iw", "=", "action_dist", "[", "np", ".", "arange", "(", "action", ".", "shape", "[", "0", "]", ")", ",", "action", ",", "position", "]", "/", "pscore", "\n", "# weight clipping", "\n", "if", "isinstance", "(", "iw", ",", "np", ".", "ndarray", ")", ":", "\n", "            ", "iw", "=", "np", ".", "minimum", "(", "iw", ",", "self", ".", "lambda_", ")", "\n", "", "return", "reward", "*", "iw", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators.InverseProbabilityWeighting.estimate_policy_value": [[340, 407], ["utils.check_array", "utils.check_array", "utils.check_ope_inputs", "estimators.InverseProbabilityWeighting._estimate_round_rewards().mean", "utils.check_array", "utils.check_array", "numpy.zeros", "estimators.InverseProbabilityWeighting._estimate_round_rewards"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_ope_inputs", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SelfNormalizedSlateStandardIPS._estimate_round_rewards"], ["", "def", "estimate_policy_value", "(", "\n", "self", ",", "\n", "reward", ":", "np", ".", "ndarray", ",", "\n", "action", ":", "np", ".", "ndarray", ",", "\n", "action_dist", ":", "np", ".", "ndarray", ",", "\n", "pscore", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "position", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "estimated_pscore", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "np", ".", "ndarray", ":", "\n", "        ", "\"\"\"Estimate the policy value of evaluation policy.\n\n        Parameters\n        ----------\n        reward: array-like, shape (n_rounds,)\n            Rewards observed for each data in logged bandit data, i.e., :math:`r_i`.\n\n        action: array-like, shape (n_rounds,)\n            Actions sampled by the logging/behavior policy for each data in logged bandit data, i.e., :math:`a_i`.\n\n        action_dist: array-like, shape (n_rounds, n_actions, len_list)\n            Action choice probabilities of the evaluation policy (can be deterministic), i.e., :math:`\\\\pi_e(a_i|x_i)`.\n\n        pscore: array-like, shape (n_rounds,), default=None\n            Action choice probabilities of the logging/behavior policy (propensity scores), i.e., :math:`\\\\pi_b(a_i|x_i)`.\n            If `use_estimated_pscore` is False, `pscore` must be given.\n\n        position: array-like, shape (n_rounds,), default=None\n            Indices to differentiate positions in a recommendation interface where the actions are presented.\n            If None, the effect of position on the reward will be ignored.\n            (If only a single action is chosen for each data, you can just ignore this argument.)\n\n        estimated_pscore: array-like, shape (n_rounds,), default=None\n            Estimated behavior policy (propensity scores), i.e., :math:`\\\\hat{\\\\pi}_b(a_i|x_i)`.\n            If `self.use_estimated_pscore` is True, `estimated_pscore` must be given.\n\n        Returns\n        ----------\n        V_hat: float\n            Estimated policy value of evaluation policy.\n\n        \"\"\"", "\n", "check_array", "(", "array", "=", "reward", ",", "name", "=", "\"reward\"", ",", "expected_dim", "=", "1", ")", "\n", "check_array", "(", "array", "=", "action", ",", "name", "=", "\"action\"", ",", "expected_dim", "=", "1", ")", "\n", "if", "self", ".", "use_estimated_pscore", ":", "\n", "            ", "check_array", "(", "array", "=", "estimated_pscore", ",", "name", "=", "\"estimated_pscore\"", ",", "expected_dim", "=", "1", ")", "\n", "pscore_", "=", "estimated_pscore", "\n", "", "else", ":", "\n", "            ", "check_array", "(", "array", "=", "pscore", ",", "name", "=", "\"pscore\"", ",", "expected_dim", "=", "1", ")", "\n", "pscore_", "=", "pscore", "\n", "\n", "", "check_ope_inputs", "(", "\n", "action_dist", "=", "action_dist", ",", "\n", "position", "=", "position", ",", "\n", "action", "=", "action", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore", "=", "pscore_", ",", "\n", ")", "\n", "if", "position", "is", "None", ":", "\n", "            ", "position", "=", "np", ".", "zeros", "(", "action_dist", ".", "shape", "[", "0", "]", ",", "dtype", "=", "int", ")", "\n", "\n", "", "return", "self", ".", "_estimate_round_rewards", "(", "\n", "reward", "=", "reward", ",", "\n", "action", "=", "action", ",", "\n", "position", "=", "position", ",", "\n", "pscore", "=", "pscore_", ",", "\n", "action_dist", "=", "action_dist", ",", "\n", ")", ".", "mean", "(", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators.InverseProbabilityWeighting.estimate_interval": [[409, 494], ["utils.check_array", "utils.check_array", "utils.check_ope_inputs", "estimators.InverseProbabilityWeighting._estimate_round_rewards", "utils.estimate_confidence_interval_by_bootstrap", "utils.check_array", "utils.check_array", "numpy.zeros"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_ope_inputs", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SelfNormalizedSlateStandardIPS._estimate_round_rewards", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.estimate_confidence_interval_by_bootstrap", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array"], ["", "def", "estimate_interval", "(", "\n", "self", ",", "\n", "reward", ":", "np", ".", "ndarray", ",", "\n", "action", ":", "np", ".", "ndarray", ",", "\n", "action_dist", ":", "np", ".", "ndarray", ",", "\n", "pscore", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "position", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "estimated_pscore", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "alpha", ":", "float", "=", "0.05", ",", "\n", "n_bootstrap_samples", ":", "int", "=", "10000", ",", "\n", "random_state", ":", "Optional", "[", "int", "]", "=", "None", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "Dict", "[", "str", ",", "float", "]", ":", "\n", "        ", "\"\"\"Estimate the confidence interval of the policy value using bootstrap.\n\n        Parameters\n        ----------\n        reward: array-like, shape (n_rounds,)\n            Rewards observed for each data in logged bandit data, i.e., :math:`r_i`.\n\n        action: array-like, shape (n_rounds,)\n            Actions sampled by the logging/behavior policy for each data in logged bandit data, i.e., :math:`a_i`.\n\n        action_dist: array-like, shape (n_rounds, n_actions, len_list)\n            Action choice probabilities of the evaluation policy (can be deterministic), i.e., :math:`\\\\pi_e(a_i|x_i)`.\n\n        pscore: array-like, shape (n_rounds,), default=None\n            Action choice probabilities of the logging/behavior policy (propensity scores), i.e., :math:`\\\\pi_b(a_i|x_i)`.\n            If `use_estimated_pscore` is False, `pscore` must be given.\n\n        position: array-like, shape (n_rounds,), default=None\n            Indices to differentiate positions in a recommendation interface where the actions are presented.\n            If None, the effect of position on the reward will be ignored.\n            (If only a single action is chosen for each data, you can just ignore this argument.)\n\n        estimated_pscore: array-like, shape (n_rounds,), default=None\n            Estimated behavior policy (propensity scores), i.e., :math:`\\\\hat{\\\\pi}_b(a_i|x_i)`.\n            If `self.use_estimated_pscore` is True, `estimated_pscore` must be given.\n\n        alpha: float, default=0.05\n            Significance level.\n\n        n_bootstrap_samples: int, default=10000\n            Number of resampling performed in bootstrap sampling.\n\n        random_state: int, default=None\n            Controls the random seed in bootstrap sampling.\n\n        Returns\n        ----------\n        estimated_confidence_interval: Dict[str, float]\n            Dictionary storing the estimated mean and upper-lower confidence bounds.\n\n        \"\"\"", "\n", "check_array", "(", "array", "=", "reward", ",", "name", "=", "\"reward\"", ",", "expected_dim", "=", "1", ")", "\n", "check_array", "(", "array", "=", "action", ",", "name", "=", "\"action\"", ",", "expected_dim", "=", "1", ")", "\n", "if", "self", ".", "use_estimated_pscore", ":", "\n", "            ", "check_array", "(", "array", "=", "estimated_pscore", ",", "name", "=", "\"estimated_pscore\"", ",", "expected_dim", "=", "1", ")", "\n", "pscore_", "=", "estimated_pscore", "\n", "", "else", ":", "\n", "            ", "check_array", "(", "array", "=", "pscore", ",", "name", "=", "\"pscore\"", ",", "expected_dim", "=", "1", ")", "\n", "pscore_", "=", "pscore", "\n", "\n", "", "check_ope_inputs", "(", "\n", "action_dist", "=", "action_dist", ",", "\n", "position", "=", "position", ",", "\n", "action", "=", "action", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore", "=", "pscore_", ",", "\n", ")", "\n", "if", "position", "is", "None", ":", "\n", "            ", "position", "=", "np", ".", "zeros", "(", "action_dist", ".", "shape", "[", "0", "]", ",", "dtype", "=", "int", ")", "\n", "\n", "", "estimated_round_rewards", "=", "self", ".", "_estimate_round_rewards", "(", "\n", "reward", "=", "reward", ",", "\n", "action", "=", "action", ",", "\n", "position", "=", "position", ",", "\n", "pscore", "=", "pscore_", ",", "\n", "action_dist", "=", "action_dist", ",", "\n", ")", "\n", "return", "estimate_confidence_interval_by_bootstrap", "(", "\n", "samples", "=", "estimated_round_rewards", ",", "\n", "alpha", "=", "alpha", ",", "\n", "n_bootstrap_samples", "=", "n_bootstrap_samples", ",", "\n", "random_state", "=", "random_state", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators.InverseProbabilityWeighting._estimate_mse_score": [[496, 572], ["numpy.var", "estimators.InverseProbabilityWeighting._estimate_round_rewards", "helper.estimate_high_probability_upper_bound_bias", "helper.estimate_bias_in_ope", "numpy.minimum", "numpy.minimum", "numpy.arange"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SelfNormalizedSlateStandardIPS._estimate_round_rewards", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.helper.estimate_high_probability_upper_bound_bias", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.helper.estimate_bias_in_ope"], ["", "def", "_estimate_mse_score", "(", "\n", "self", ",", "\n", "reward", ":", "np", ".", "ndarray", ",", "\n", "action", ":", "np", ".", "ndarray", ",", "\n", "pscore", ":", "np", ".", "ndarray", ",", "\n", "action_dist", ":", "np", ".", "ndarray", ",", "\n", "position", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "use_bias_upper_bound", ":", "bool", "=", "True", ",", "\n", "delta", ":", "float", "=", "0.05", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "float", ":", "\n", "        ", "\"\"\"Estimate the MSE score of a given clipping hyperparameter to conduct hyperparameter tuning.\n\n        Parameters\n        ----------\n        reward: array-like, shape (n_rounds,)\n            Rewards observed for each data in logged bandit data, i.e., :math:`r_i`.\n\n        action: array-like, shape (n_rounds,)\n            Actions sampled by the logging/behavior policy for each data in logged bandit data, i.e., :math:`a_i`.\n\n        pscore: array-like, shape (n_rounds,)\n            Action choice probabilities of the logging/behavior policy (propensity scores), i.e., :math:`\\\\pi_b(a_i|x_i)`.\n\n        action_dist: array-like, shape (n_rounds, n_actions, len_list)\n            Action choice probabilities of the evaluation policy (can be deterministic), i.e., :math:`\\\\pi_e(a_i|x_i)`.\n\n        position: array-like, shape (n_rounds,), default=None\n            Indices to differentiate positions in a recommendation interface where the actions are presented.\n            If None, the effect of position on the reward will be ignored.\n            (If only a single action is chosen for each data, you can just ignore this argument.)\n\n        use_bias_upper_bound: bool, default=True\n            Whether to use a bias upper bound in hyperparameter tuning.\n            If False, the direct bias estimator is used to estimate the MSE. See Su et al.(2020) for details.\n\n        delta: float, default=0.05\n            A confidence delta to construct a high probability upper bound used in SLOPE.\n\n        Returns\n        ----------\n        estimated_mse_score: float\n            Estimated MSE score of a given clipping hyperparameter `lambda_`.\n            MSE score is the sum of (high probability) upper bound of bias and the sample variance.\n            This is estimated using the automatic hyperparameter tuning procedure\n            based on Section 5 of Su et al.(2020).\n\n        \"\"\"", "\n", "n", "=", "reward", ".", "shape", "[", "0", "]", "\n", "# estimate the sample variance of IPW with clipping", "\n", "sample_variance", "=", "np", ".", "var", "(", "\n", "self", ".", "_estimate_round_rewards", "(", "\n", "reward", "=", "reward", ",", "\n", "action", "=", "action", ",", "\n", "pscore", "=", "pscore", ",", "\n", "action_dist", "=", "action_dist", ",", "\n", "position", "=", "position", ",", "\n", ")", "\n", ")", "\n", "sample_variance", "/=", "n", "\n", "\n", "# estimate the (high probability) upper bound of the bias of IPW with clipping", "\n", "iw", "=", "action_dist", "[", "np", ".", "arange", "(", "n", ")", ",", "action", ",", "position", "]", "/", "pscore", "\n", "if", "use_bias_upper_bound", ":", "\n", "            ", "bias_term", "=", "estimate_high_probability_upper_bound_bias", "(", "\n", "reward", "=", "reward", ",", "iw", "=", "iw", ",", "iw_hat", "=", "np", ".", "minimum", "(", "iw", ",", "self", ".", "lambda_", ")", ",", "delta", "=", "delta", "\n", ")", "\n", "", "else", ":", "\n", "            ", "bias_term", "=", "estimate_bias_in_ope", "(", "\n", "reward", "=", "reward", ",", "\n", "iw", "=", "iw", ",", "\n", "iw_hat", "=", "np", ".", "minimum", "(", "iw", ",", "self", ".", "lambda_", ")", ",", "\n", ")", "\n", "", "estimated_mse_score", "=", "sample_variance", "+", "(", "bias_term", "**", "2", ")", "\n", "\n", "return", "estimated_mse_score", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators.SelfNormalizedInverseProbabilityWeighting._estimate_round_rewards": [[616, 657], ["numpy.zeros", "iw.mean", "numpy.arange"], "methods", ["None"], ["def", "_estimate_round_rewards", "(", "\n", "self", ",", "\n", "reward", ":", "np", ".", "ndarray", ",", "\n", "action", ":", "np", ".", "ndarray", ",", "\n", "pscore", ":", "np", ".", "ndarray", ",", "\n", "action_dist", ":", "np", ".", "ndarray", ",", "\n", "position", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "np", ".", "ndarray", ":", "\n", "        ", "\"\"\"Estimate round-wise (or sample-wise) rewards.\n\n        Parameters\n        ----------\n        reward: array-like, shape (n_rounds,)\n            Rewards observed for each data in logged bandit data, i.e., :math:`r_i`.\n\n        action: array-like, shape (n_rounds,)\n            Actions sampled by the logging/behavior policy for each data in logged bandit data, i.e., :math:`a_i`.\n\n        pscore: array-like, shape (n_rounds,)\n            Action choice probabilities of the logging/behavior policy (propensity scores), i.e., :math:`\\\\pi_b(a_i|x_i)`.\n\n        action_dist: array-like, shape (n_rounds, n_actions, len_list)\n            Action choice probabilities of the evaluation policy (can be deterministic), i.e., :math:`\\\\pi_e(a_i|x_i)`.\n\n        position: array-like, shape (n_rounds,), default=None\n            Indices to differentiate positions in a recommendation interface where the actions are presented.\n            If None, the effect of position on the reward will be ignored.\n            (If only a single action is chosen for each data, you can just ignore this argument.)\n\n        Returns\n        ----------\n        estimated_rewards: array-like, shape (n_rounds,)\n            Estimated rewards for each observation.\n\n        \"\"\"", "\n", "if", "position", "is", "None", ":", "\n", "            ", "position", "=", "np", ".", "zeros", "(", "action_dist", ".", "shape", "[", "0", "]", ",", "dtype", "=", "int", ")", "\n", "\n", "", "iw", "=", "action_dist", "[", "np", ".", "arange", "(", "action", ".", "shape", "[", "0", "]", ")", ",", "action", ",", "position", "]", "/", "pscore", "\n", "return", "reward", "*", "iw", "/", "iw", ".", "mean", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators.DirectMethod._estimate_round_rewards": [[703, 741], ["numpy.average", "numpy.zeros", "numpy.arange", "numpy.arange"], "methods", ["None"], ["def", "_estimate_round_rewards", "(", "\n", "self", ",", "\n", "action_dist", ":", "np", ".", "ndarray", ",", "\n", "estimated_rewards_by_reg_model", ":", "np", ".", "ndarray", ",", "\n", "position", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "np", ".", "ndarray", ":", "\n", "        ", "\"\"\"Estimate the policy value of evaluation policy.\n\n        Parameters\n        ----------\n        action_dist: array-like, shape (n_rounds, n_actions, len_list)\n            Action choice probabilities of the evaluation policy (can be deterministic), i.e., :math:`\\\\pi_e(a_i|x_i)`.\n\n        estimated_rewards_by_reg_model: array-like, shape (n_rounds, n_actions, len_list)\n            Estimated expected rewards given context, action, and position, i.e., :math:`\\\\hat{q}(x_i,a_i)`.\n\n        position: array-like, shape (n_rounds,), default=None\n            Indices to differentiate positions in a recommendation interface where the actions are presented.\n            If None, the effect of position on the reward will be ignored.\n            (If only a single action is chosen for each data, you can just ignore this argument.)\n\n        Returns\n        ----------\n        estimated_rewards: array-like, shape (n_rounds,)\n            Estimated rewards for each observation.\n\n        \"\"\"", "\n", "if", "position", "is", "None", ":", "\n", "            ", "position", "=", "np", ".", "zeros", "(", "action_dist", ".", "shape", "[", "0", "]", ",", "dtype", "=", "int", ")", "\n", "\n", "", "n", "=", "position", ".", "shape", "[", "0", "]", "\n", "q_hat_at_position", "=", "estimated_rewards_by_reg_model", "[", "np", ".", "arange", "(", "n", ")", ",", ":", ",", "position", "]", "\n", "pi_e_at_position", "=", "action_dist", "[", "np", ".", "arange", "(", "n", ")", ",", ":", ",", "position", "]", "\n", "return", "np", ".", "average", "(", "\n", "q_hat_at_position", ",", "\n", "weights", "=", "pi_e_at_position", ",", "\n", "axis", "=", "1", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators.DirectMethod.estimate_policy_value": [[743, 788], ["utils.check_array", "utils.check_ope_inputs", "estimators.DirectMethod._estimate_round_rewards().mean", "numpy.zeros", "estimators.DirectMethod._estimate_round_rewards"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_ope_inputs", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SelfNormalizedSlateStandardIPS._estimate_round_rewards"], ["", "def", "estimate_policy_value", "(", "\n", "self", ",", "\n", "action_dist", ":", "np", ".", "ndarray", ",", "\n", "estimated_rewards_by_reg_model", ":", "np", ".", "ndarray", ",", "\n", "position", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "float", ":", "\n", "        ", "\"\"\"Estimate the policy value of evaluation policy.\n\n        Parameters\n        ----------\n        action_dist: array-like, shape (n_rounds, n_actions, len_list)\n            Action choice probabilities of the evaluation policy (can be deterministic), i.e., :math:`\\\\pi_e(a_i|x_i)`.\n\n        estimated_rewards_by_reg_model: array-like, shape (n_rounds, n_actions, len_list)\n            Estimated expected rewards given context, action, and position, i.e., :math:`\\\\hat{q}(x_i,a_i)`.\n\n        position: array-like, shape (n_rounds,), default=None\n            Indices to differentiate positions in a recommendation interface where the actions are presented.\n            If None, the effect of position on the reward will be ignored.\n            (If only a single action is chosen for each data, you can just ignore this argument.)\n\n        Returns\n        ----------\n        V_hat: float\n            Estimated policy value of evaluation policy.\n\n        \"\"\"", "\n", "check_array", "(", "\n", "array", "=", "estimated_rewards_by_reg_model", ",", "\n", "name", "=", "\"estimated_rewards_by_reg_model\"", ",", "\n", "expected_dim", "=", "3", ",", "\n", ")", "\n", "check_ope_inputs", "(", "\n", "action_dist", "=", "action_dist", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", "position", "=", "position", ",", "\n", ")", "\n", "if", "position", "is", "None", ":", "\n", "            ", "position", "=", "np", ".", "zeros", "(", "action_dist", ".", "shape", "[", "0", "]", ",", "dtype", "=", "int", ")", "\n", "\n", "", "return", "self", ".", "_estimate_round_rewards", "(", "\n", "position", "=", "position", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", "action_dist", "=", "action_dist", ",", "\n", ")", ".", "mean", "(", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators.DirectMethod.estimate_interval": [[790, 853], ["utils.check_array", "utils.check_ope_inputs", "estimators.DirectMethod._estimate_round_rewards", "utils.estimate_confidence_interval_by_bootstrap", "numpy.zeros"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_ope_inputs", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SelfNormalizedSlateStandardIPS._estimate_round_rewards", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.estimate_confidence_interval_by_bootstrap"], ["", "def", "estimate_interval", "(", "\n", "self", ",", "\n", "action_dist", ":", "np", ".", "ndarray", ",", "\n", "estimated_rewards_by_reg_model", ":", "np", ".", "ndarray", ",", "\n", "position", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "alpha", ":", "float", "=", "0.05", ",", "\n", "n_bootstrap_samples", ":", "int", "=", "10000", ",", "\n", "random_state", ":", "Optional", "[", "int", "]", "=", "None", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "Dict", "[", "str", ",", "float", "]", ":", "\n", "        ", "\"\"\"Estimate the confidence interval of the policy value using bootstrap.\n\n        Parameters\n        ----------\n        action_dist: array-like, shape (n_rounds, n_actions, len_list)\n            Action choice probabilities of the evaluation policy (can be deterministic), i.e., :math:`\\\\pi_e(a_i|x_i)`.\n\n        estimated_rewards_by_reg_model: array-like, shape (n_rounds, n_actions, len_list)\n            Estimated expected rewards given context, action, and position, i.e., :math:`\\\\hat{q}(x_i,a_i)`.\n\n        position: array-like, shape (n_rounds,), default=None\n            Indices to differentiate positions in a recommendation interface where the actions are presented.\n            If None, the effect of position on the reward will be ignored.\n            (If only a single action is chosen for each data, you can just ignore this argument.)\n\n        alpha: float, default=0.05\n            Significance level.\n\n        n_bootstrap_samples: int, default=10000\n            Number of resampling performed in bootstrap sampling.\n\n        random_state: int, default=None\n            Controls the random seed in bootstrap sampling.\n\n        Returns\n        ----------\n        estimated_confidence_interval: Dict[str, float]\n            Dictionary storing the estimated mean and upper-lower confidence bounds.\n\n        \"\"\"", "\n", "check_array", "(", "\n", "array", "=", "estimated_rewards_by_reg_model", ",", "\n", "name", "=", "\"estimated_rewards_by_reg_model\"", ",", "\n", "expected_dim", "=", "3", ",", "\n", ")", "\n", "check_ope_inputs", "(", "\n", "action_dist", "=", "action_dist", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", "position", "=", "position", ",", "\n", ")", "\n", "if", "position", "is", "None", ":", "\n", "            ", "position", "=", "np", ".", "zeros", "(", "action_dist", ".", "shape", "[", "0", "]", ",", "dtype", "=", "int", ")", "\n", "\n", "", "estimated_round_rewards", "=", "self", ".", "_estimate_round_rewards", "(", "\n", "position", "=", "position", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", "action_dist", "=", "action_dist", ",", "\n", ")", "\n", "return", "estimate_confidence_interval_by_bootstrap", "(", "\n", "samples", "=", "estimated_round_rewards", ",", "\n", "alpha", "=", "alpha", ",", "\n", "n_bootstrap_samples", "=", "n_bootstrap_samples", ",", "\n", "random_state", "=", "random_state", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators.DoublyRobust.__post_init__": [[922, 935], ["sklearn.utils.check_scalar", "ValueError", "isinstance", "TypeError", "type"], "methods", ["None"], ["def", "__post_init__", "(", "self", ")", "->", "None", ":", "\n", "        ", "\"\"\"Initialize Class.\"\"\"", "\n", "check_scalar", "(", "\n", "self", ".", "lambda_", ",", "\n", "name", "=", "\"lambda_\"", ",", "\n", "target_type", "=", "(", "int", ",", "float", ")", ",", "\n", "min_val", "=", "0.0", ",", "\n", ")", "\n", "if", "self", ".", "lambda_", "!=", "self", ".", "lambda_", ":", "\n", "            ", "raise", "ValueError", "(", "\"`lambda_` must not be nan\"", ")", "\n", "", "if", "not", "isinstance", "(", "self", ".", "use_estimated_pscore", ",", "bool", ")", ":", "\n", "            ", "raise", "TypeError", "(", "\n", "f\"`use_estimated_pscore` must be a bool, but {type(self.use_estimated_pscore)} is given\"", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators.DoublyRobust._estimate_round_rewards": [[937, 997], ["isinstance", "numpy.average", "numpy.zeros", "numpy.minimum", "numpy.arange", "numpy.arange", "numpy.arange", "numpy.arange"], "methods", ["None"], ["", "", "def", "_estimate_round_rewards", "(", "\n", "self", ",", "\n", "reward", ":", "np", ".", "ndarray", ",", "\n", "action", ":", "np", ".", "ndarray", ",", "\n", "pscore", ":", "np", ".", "ndarray", ",", "\n", "action_dist", ":", "np", ".", "ndarray", ",", "\n", "estimated_rewards_by_reg_model", ":", "np", ".", "ndarray", ",", "\n", "position", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "np", ".", "ndarray", ":", "\n", "        ", "\"\"\"Estimate round-wise (or sample-wise) rewards.\n\n        Parameters\n        ----------\n        reward: array-like, shape (n_rounds,)\n            Rewards observed for each data in logged bandit data, i.e., :math:`r_i`.\n\n        action: array-like, shape (n_rounds,)\n            Actions sampled by the logging/behavior policy for each data in logged bandit data, i.e., :math:`a_i`.\n\n        pscore: array-like, shape (n_rounds,)\n            Action choice probabilities of the logging/behavior policy (propensity scores), i.e., :math:`\\\\pi_b(a_i|x_i)`.\n\n        action_dist: array-like, shape (n_rounds, n_actions, len_list)\n            Action choice probabilities of the evaluation policy (can be deterministic), i.e., :math:`\\\\pi_e(a_i|x_i)`.\n\n        estimated_rewards_by_reg_model: array-like, shape (n_rounds, n_actions, len_list)\n            Estimated expected rewards given context, action, and position, i.e., :math:`\\\\hat{q}(x_i,a_i)`.\n\n        position: array-like, shape (n_rounds,), default=None\n            Indices to differentiate positions in a recommendation interface where the actions are presented.\n            If None, the effect of position on the reward will be ignored.\n            (If only a single action is chosen for each data, you can just ignore this argument.)\n\n        Returns\n        ----------\n        estimated_rewards: array-like, shape (n_rounds,)\n            Estimated rewards for each observation.\n\n        \"\"\"", "\n", "if", "position", "is", "None", ":", "\n", "            ", "position", "=", "np", ".", "zeros", "(", "action_dist", ".", "shape", "[", "0", "]", ",", "dtype", "=", "int", ")", "\n", "\n", "", "n", "=", "action", ".", "shape", "[", "0", "]", "\n", "iw", "=", "action_dist", "[", "np", ".", "arange", "(", "n", ")", ",", "action", ",", "position", "]", "/", "pscore", "\n", "# weight clipping", "\n", "if", "isinstance", "(", "iw", ",", "np", ".", "ndarray", ")", ":", "\n", "            ", "iw", "=", "np", ".", "minimum", "(", "iw", ",", "self", ".", "lambda_", ")", "\n", "\n", "", "q_hat_at_position", "=", "estimated_rewards_by_reg_model", "[", "np", ".", "arange", "(", "n", ")", ",", ":", ",", "position", "]", "\n", "q_hat_factual", "=", "estimated_rewards_by_reg_model", "[", "np", ".", "arange", "(", "n", ")", ",", "action", ",", "position", "]", "\n", "pi_e_at_position", "=", "action_dist", "[", "np", ".", "arange", "(", "n", ")", ",", ":", ",", "position", "]", "\n", "estimated_rewards", "=", "np", ".", "average", "(", "\n", "q_hat_at_position", ",", "\n", "weights", "=", "pi_e_at_position", ",", "\n", "axis", "=", "1", ",", "\n", ")", "\n", "estimated_rewards", "+=", "iw", "*", "(", "reward", "-", "q_hat_factual", ")", "\n", "\n", "return", "estimated_rewards", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators.DoublyRobust.estimate_policy_value": [[998, 1075], ["utils.check_array", "utils.check_array", "utils.check_array", "utils.check_ope_inputs", "estimators.DoublyRobust._estimate_round_rewards().mean", "utils.check_array", "utils.check_array", "numpy.zeros", "estimators.DoublyRobust._estimate_round_rewards"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_ope_inputs", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SelfNormalizedSlateStandardIPS._estimate_round_rewards"], ["", "def", "estimate_policy_value", "(", "\n", "self", ",", "\n", "reward", ":", "np", ".", "ndarray", ",", "\n", "action", ":", "np", ".", "ndarray", ",", "\n", "action_dist", ":", "np", ".", "ndarray", ",", "\n", "estimated_rewards_by_reg_model", ":", "np", ".", "ndarray", ",", "\n", "pscore", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "position", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "estimated_pscore", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "float", ":", "\n", "        ", "\"\"\"Estimate the policy value of evaluation policy.\n\n        Parameters\n        ----------\n        reward: array-like, shape (n_rounds,)\n            Rewards observed for each data in logged bandit data, i.e., :math:`r_i`.\n\n        action: array-like, shape (n_rounds,)\n            Actions sampled by the logging/behavior policy for each data in logged bandit data, i.e., :math:`a_i`.\n\n        action_dist: array-like, shape (n_rounds, n_actions, len_list)\n            Action choice probabilities of the evaluation policy (can be deterministic), i.e., :math:`\\\\pi_e(a_i|x_i)`.\n\n        estimated_rewards_by_reg_model: array-like, shape (n_rounds, n_actions, len_list)\n            Estimated expected rewards given context, action, and position, i.e., :math:`\\\\hat{q}(x_i,a_i)`.\n\n        pscore: array-like, shape (n_rounds,), default=None\n            Action choice probabilities of the logging/behavior policy (propensity scores), i.e., :math:`\\\\pi_b(a_i|x_i)`.\n            If `use_estimated_pscore` is False, `pscore` must be given.\n\n        position: array-like, shape (n_rounds,), default=None\n            Indices to differentiate positions in a recommendation interface where the actions are presented.\n            If None, the effect of position on the reward will be ignored.\n            (If only a single action is chosen for each data, you can just ignore this argument.)\n\n        estimated_pscore: array-like, shape (n_rounds,), default=None\n            Estimated behavior policy (propensity scores), i.e., :math:`\\\\hat{\\\\pi}_b(a_i|x_i)`.\n            If `self.use_estimated_pscore` is True, `estimated_pscore` must be given.\n\n        Returns\n        ----------\n        V_hat: float\n            Estimated policy value of evaluation policy.\n\n        \"\"\"", "\n", "check_array", "(", "\n", "array", "=", "estimated_rewards_by_reg_model", ",", "\n", "name", "=", "\"estimated_rewards_by_reg_model\"", ",", "\n", "expected_dim", "=", "3", ",", "\n", ")", "\n", "check_array", "(", "array", "=", "reward", ",", "name", "=", "\"reward\"", ",", "expected_dim", "=", "1", ")", "\n", "check_array", "(", "array", "=", "action", ",", "name", "=", "\"action\"", ",", "expected_dim", "=", "1", ")", "\n", "if", "self", ".", "use_estimated_pscore", ":", "\n", "            ", "check_array", "(", "array", "=", "estimated_pscore", ",", "name", "=", "\"estimated_pscore\"", ",", "expected_dim", "=", "1", ")", "\n", "pscore_", "=", "estimated_pscore", "\n", "", "else", ":", "\n", "            ", "check_array", "(", "array", "=", "pscore", ",", "name", "=", "\"pscore\"", ",", "expected_dim", "=", "1", ")", "\n", "pscore_", "=", "pscore", "\n", "", "check_ope_inputs", "(", "\n", "action_dist", "=", "action_dist", ",", "\n", "position", "=", "position", ",", "\n", "action", "=", "action", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore", "=", "pscore_", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", ")", "\n", "if", "position", "is", "None", ":", "\n", "            ", "position", "=", "np", ".", "zeros", "(", "action_dist", ".", "shape", "[", "0", "]", ",", "dtype", "=", "int", ")", "\n", "\n", "", "return", "self", ".", "_estimate_round_rewards", "(", "\n", "reward", "=", "reward", ",", "\n", "action", "=", "action", ",", "\n", "position", "=", "position", ",", "\n", "pscore", "=", "pscore_", ",", "\n", "action_dist", "=", "action_dist", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", ")", ".", "mean", "(", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators.DoublyRobust.estimate_interval": [[1077, 1172], ["utils.check_array", "utils.check_array", "utils.check_array", "utils.check_ope_inputs", "estimators.DoublyRobust._estimate_round_rewards", "utils.estimate_confidence_interval_by_bootstrap", "utils.check_array", "utils.check_array", "numpy.zeros"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_ope_inputs", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SelfNormalizedSlateStandardIPS._estimate_round_rewards", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.estimate_confidence_interval_by_bootstrap", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array"], ["", "def", "estimate_interval", "(", "\n", "self", ",", "\n", "reward", ":", "np", ".", "ndarray", ",", "\n", "action", ":", "np", ".", "ndarray", ",", "\n", "action_dist", ":", "np", ".", "ndarray", ",", "\n", "estimated_rewards_by_reg_model", ":", "np", ".", "ndarray", ",", "\n", "pscore", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "position", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "estimated_pscore", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "alpha", ":", "float", "=", "0.05", ",", "\n", "n_bootstrap_samples", ":", "int", "=", "10000", ",", "\n", "random_state", ":", "Optional", "[", "int", "]", "=", "None", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "Dict", "[", "str", ",", "float", "]", ":", "\n", "        ", "\"\"\"Estimate the confidence interval of the policy value using bootstrap.\n\n        Parameters\n        ----------\n        reward: array-like, shape (n_rounds,)\n            Rewards observed for each data in logged bandit data, i.e., :math:`r_i`.\n\n        action: array-like, shape (n_rounds,)\n            Actions sampled by the logging/behavior policy for each data in logged bandit data, i.e., :math:`a_i`.\n\n        action_dist: array-like, shape (n_rounds, n_actions, len_list)\n            Action choice probabilities of the evaluation policy (can be deterministic), i.e., :math:`\\\\pi_e(a_i|x_i)`.\n\n        estimated_rewards_by_reg_model: array-like, shape (n_rounds, n_actions, len_list)\n            Estimated expected rewards given context, action, and position, i.e., :math:`\\\\hat{q}(x_i,a_i)`.\n\n        pscore: array-like, shape (n_rounds,), default=None\n            Action choice probabilities of the logging/behavior policy (propensity scores), i.e., :math:`\\\\pi_b(a_i|x_i)`.\n            If `use_estimated_pscore` is False, `pscore` must be given.\n\n        position: array-like, shape (n_rounds,), default=None\n            Indices to differentiate positions in a recommendation interface where the actions are presented.\n            If None, the effect of position on the reward will be ignored.\n            (If only a single action is chosen for each data, you can just ignore this argument.)\n\n        estimated_pscore: array-like, shape (n_rounds,), default=None\n            Estimated behavior policy (propensity scores), i.e., :math:`\\\\hat{\\\\pi}_b(a_i|x_i)`.\n            If `self.use_estimated_pscore` is True, `estimated_pscore` must be given.\n\n        alpha: float, default=0.05\n            Significance level.\n\n        n_bootstrap_samples: int, default=10000\n            Number of resampling performed in bootstrap sampling.\n\n        random_state: int, default=None\n            Controls the random seed in bootstrap sampling.\n\n        Returns\n        ----------\n        estimated_confidence_interval: Dict[str, float]\n            Dictionary storing the estimated mean and upper-lower confidence bounds.\n\n        \"\"\"", "\n", "check_array", "(", "\n", "array", "=", "estimated_rewards_by_reg_model", ",", "\n", "name", "=", "\"estimated_rewards_by_reg_model\"", ",", "\n", "expected_dim", "=", "3", ",", "\n", ")", "\n", "check_array", "(", "array", "=", "reward", ",", "name", "=", "\"reward\"", ",", "expected_dim", "=", "1", ")", "\n", "check_array", "(", "array", "=", "action", ",", "name", "=", "\"action\"", ",", "expected_dim", "=", "1", ")", "\n", "if", "self", ".", "use_estimated_pscore", ":", "\n", "            ", "check_array", "(", "array", "=", "estimated_pscore", ",", "name", "=", "\"estimated_pscore\"", ",", "expected_dim", "=", "1", ")", "\n", "pscore_", "=", "estimated_pscore", "\n", "", "else", ":", "\n", "            ", "check_array", "(", "array", "=", "pscore", ",", "name", "=", "\"pscore\"", ",", "expected_dim", "=", "1", ")", "\n", "pscore_", "=", "pscore", "\n", "", "check_ope_inputs", "(", "\n", "action_dist", "=", "action_dist", ",", "\n", "position", "=", "position", ",", "\n", "action", "=", "action", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore", "=", "pscore_", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", ")", "\n", "if", "position", "is", "None", ":", "\n", "            ", "position", "=", "np", ".", "zeros", "(", "action_dist", ".", "shape", "[", "0", "]", ",", "dtype", "=", "int", ")", "\n", "\n", "", "estimated_round_rewards", "=", "self", ".", "_estimate_round_rewards", "(", "\n", "reward", "=", "reward", ",", "\n", "action", "=", "action", ",", "\n", "position", "=", "position", ",", "\n", "pscore", "=", "pscore_", ",", "\n", "action_dist", "=", "action_dist", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", ")", "\n", "return", "estimate_confidence_interval_by_bootstrap", "(", "\n", "samples", "=", "estimated_round_rewards", ",", "\n", "alpha", "=", "alpha", ",", "\n", "n_bootstrap_samples", "=", "n_bootstrap_samples", ",", "\n", "random_state", "=", "random_state", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators.DoublyRobust._estimate_mse_score": [[1174, 1259], ["numpy.var", "estimators.DoublyRobust._estimate_round_rewards", "helper.estimate_high_probability_upper_bound_bias", "helper.estimate_bias_in_ope", "numpy.minimum", "numpy.minimum", "numpy.arange", "numpy.arange", "numpy.arange"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SelfNormalizedSlateStandardIPS._estimate_round_rewards", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.helper.estimate_high_probability_upper_bound_bias", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.helper.estimate_bias_in_ope"], ["", "def", "_estimate_mse_score", "(", "\n", "self", ",", "\n", "reward", ":", "np", ".", "ndarray", ",", "\n", "action", ":", "np", ".", "ndarray", ",", "\n", "pscore", ":", "np", ".", "ndarray", ",", "\n", "action_dist", ":", "np", ".", "ndarray", ",", "\n", "estimated_rewards_by_reg_model", ":", "np", ".", "ndarray", ",", "\n", "position", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "use_bias_upper_bound", ":", "bool", "=", "True", ",", "\n", "delta", ":", "float", "=", "0.05", ",", "\n", ")", "->", "float", ":", "\n", "        ", "\"\"\"Estimate the MSE score of a given clipping hyperparameter to conduct hyperparameter tuning.\n\n        Parameters\n        ----------\n        reward: array-like, shape (n_rounds,)\n            Rewards observed for each data in logged bandit data, i.e., :math:`r_i`.\n\n        action: array-like, shape (n_rounds,)\n            Actions sampled by the logging/behavior policy for each data in logged bandit data, i.e., :math:`a_i`.\n\n        pscore: array-like, shape (n_rounds,)\n            Action choice probabilities of the logging/behavior policy (propensity scores), i.e., :math:`\\\\pi_b(a_i|x_i)`.\n\n        action_dist: array-like, shape (n_rounds, n_actions, len_list)\n            Action choice probabilities of the evaluation policy (can be deterministic), i.e., :math:`\\\\pi_e(a_i|x_i)`.\n\n        position: array-like, shape (n_rounds,), default=None\n            Indices to differentiate positions in a recommendation interface where the actions are presented.\n            If None, the effect of position on the reward will be ignored.\n            (If only a single action is chosen for each data, you can just ignore this argument.)\n\n        estimated_rewards_by_reg_model: array-like, shape (n_rounds, n_actions, len_list)\n            Estimated expected rewards given context, action, and position, i.e., :math:`\\\\hat{q}(x_i,a_i)`.\n\n        use_bias_upper_bound: bool, default=True\n            Whether to use a bias upper bound in hyperparameter tuning.\n            If False, the direct bias estimator is used to estimate the MSE. See Su et al.(2020) for details.\n\n        delta: float, default=0.05\n            A confidence delta to construct a high probability upper bound used in SLOPE.\n\n        Returns\n        ----------\n        estimated_mse_score: float\n            Estimated MSE score of a given clipping hyperparameter `lambda_`.\n            MSE score is the sum of (high probability) upper bound of bias and the sample variance.\n            This is estimated using the automatic hyperparameter tuning procedure\n            based on Section 5 of Su et al.(2020).\n\n        \"\"\"", "\n", "n", "=", "reward", ".", "shape", "[", "0", "]", "\n", "# estimate the sample variance of DR with clipping", "\n", "sample_variance", "=", "np", ".", "var", "(", "\n", "self", ".", "_estimate_round_rewards", "(", "\n", "reward", "=", "reward", ",", "\n", "action", "=", "action", ",", "\n", "pscore", "=", "pscore", ",", "\n", "action_dist", "=", "action_dist", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", "position", "=", "position", ",", "\n", ")", "\n", ")", "\n", "sample_variance", "/=", "n", "\n", "\n", "# estimate the (high probability) upper bound of the bias of DR with clipping", "\n", "iw", "=", "action_dist", "[", "np", ".", "arange", "(", "n", ")", ",", "action", ",", "position", "]", "/", "pscore", "\n", "if", "use_bias_upper_bound", ":", "\n", "            ", "bias_term", "=", "estimate_high_probability_upper_bound_bias", "(", "\n", "reward", "=", "reward", ",", "\n", "iw", "=", "iw", ",", "\n", "iw_hat", "=", "np", ".", "minimum", "(", "iw", ",", "self", ".", "lambda_", ")", ",", "\n", "q_hat", "=", "estimated_rewards_by_reg_model", "[", "np", ".", "arange", "(", "n", ")", ",", "action", ",", "position", "]", ",", "\n", "delta", "=", "delta", ",", "\n", ")", "\n", "", "else", ":", "\n", "            ", "bias_term", "=", "estimate_bias_in_ope", "(", "\n", "reward", "=", "reward", ",", "\n", "iw", "=", "iw", ",", "\n", "iw_hat", "=", "np", ".", "minimum", "(", "iw", ",", "self", ".", "lambda_", ")", ",", "\n", "q_hat", "=", "estimated_rewards_by_reg_model", "[", "np", ".", "arange", "(", "n", ")", ",", "action", ",", "position", "]", ",", "\n", ")", "\n", "", "estimated_mse_score", "=", "sample_variance", "+", "(", "bias_term", "**", "2", ")", "\n", "\n", "return", "estimated_mse_score", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators.SelfNormalizedDoublyRobust._estimate_round_rewards": [[1303, 1356], ["numpy.average", "iw.mean", "numpy.arange", "numpy.arange", "numpy.arange", "numpy.arange"], "methods", ["None"], ["def", "_estimate_round_rewards", "(", "\n", "self", ",", "\n", "reward", ":", "np", ".", "ndarray", ",", "\n", "action", ":", "np", ".", "ndarray", ",", "\n", "pscore", ":", "np", ".", "ndarray", ",", "\n", "action_dist", ":", "np", ".", "ndarray", ",", "\n", "estimated_rewards_by_reg_model", ":", "np", ".", "ndarray", ",", "\n", "position", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "np", ".", "ndarray", ":", "\n", "        ", "\"\"\"Estimate round-wise (or sample-wise) rewards.\n\n        Parameters\n        ----------\n        reward: array-like, shape (n_rounds,)\n            Rewards observed for each data in logged bandit data, i.e., :math:`r_i`.\n\n        action: array-like, shape (n_rounds,)\n            Actions sampled by the logging/behavior policy for each data in logged bandit data, i.e., :math:`a_i`.\n\n        pscore: array-like, shape (n_rounds,)\n            Action choice probabilities of the logging/behavior policy (propensity scores), i.e., :math:`\\\\pi_b(a_i|x_i)`.\n\n        action_dist: array-like, shape (n_rounds, n_actions, len_list)\n            Action choice probabilities of the evaluation policy (can be deterministic), i.e., :math:`\\\\pi_e(a_i|x_i)`.\n\n        estimated_rewards_by_reg_model: array-like, shape (n_rounds, n_actions, len_list)\n            Estimated expected rewards given context, action, and position, i.e., :math:`\\\\hat{q}(x_i,a_i)`.\n\n        position: array-like, shape (n_rounds,), default=None\n            Indices to differentiate positions in a recommendation interface where the actions are presented.\n            If None, the effect of position on the reward will be ignored.\n            (If only a single action is chosen for each data, you can just ignore this argument.)\n\n        Returns\n        ----------\n        estimated_rewards: array-like, shape (n_rounds,)\n            Estimated rewards for each observation.\n\n        \"\"\"", "\n", "n", "=", "action", ".", "shape", "[", "0", "]", "\n", "iw", "=", "action_dist", "[", "np", ".", "arange", "(", "n", ")", ",", "action", ",", "position", "]", "/", "pscore", "\n", "q_hat_at_position", "=", "estimated_rewards_by_reg_model", "[", "np", ".", "arange", "(", "n", ")", ",", ":", ",", "position", "]", "\n", "q_hat_factual", "=", "estimated_rewards_by_reg_model", "[", "np", ".", "arange", "(", "n", ")", ",", "action", ",", "position", "]", "\n", "pi_e_at_position", "=", "action_dist", "[", "np", ".", "arange", "(", "n", ")", ",", ":", ",", "position", "]", "\n", "estimated_rewards", "=", "np", ".", "average", "(", "\n", "q_hat_at_position", ",", "\n", "weights", "=", "pi_e_at_position", ",", "\n", "axis", "=", "1", ",", "\n", ")", "\n", "estimated_rewards", "+=", "iw", "*", "(", "reward", "-", "q_hat_factual", ")", "/", "iw", ".", "mean", "(", ")", "\n", "\n", "return", "estimated_rewards", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators.SwitchDoublyRobust.__post_init__": [[1408, 1421], ["sklearn.utils.check_scalar", "ValueError", "isinstance", "TypeError", "type"], "methods", ["None"], ["def", "__post_init__", "(", "self", ")", "->", "None", ":", "\n", "        ", "\"\"\"Initialize Class.\"\"\"", "\n", "check_scalar", "(", "\n", "self", ".", "lambda_", ",", "\n", "name", "=", "\"lambda_\"", ",", "\n", "target_type", "=", "(", "int", ",", "float", ")", ",", "\n", "min_val", "=", "0.0", ",", "\n", ")", "\n", "if", "self", ".", "lambda_", "!=", "self", ".", "lambda_", ":", "\n", "            ", "raise", "ValueError", "(", "\"`lambda_` must not be nan\"", ")", "\n", "", "if", "not", "isinstance", "(", "self", ".", "use_estimated_pscore", ",", "bool", ")", ":", "\n", "            ", "raise", "TypeError", "(", "\n", "f\"`use_estimated_pscore` must be a bool, but {type(self.use_estimated_pscore)} is given\"", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators.SwitchDoublyRobust._estimate_round_rewards": [[1423, 1477], ["numpy.array", "numpy.average", "numpy.arange", "numpy.arange", "numpy.arange", "numpy.arange"], "methods", ["None"], ["", "", "def", "_estimate_round_rewards", "(", "\n", "self", ",", "\n", "reward", ":", "np", ".", "ndarray", ",", "\n", "action", ":", "np", ".", "ndarray", ",", "\n", "pscore", ":", "np", ".", "ndarray", ",", "\n", "action_dist", ":", "np", ".", "ndarray", ",", "\n", "estimated_rewards_by_reg_model", ":", "np", ".", "ndarray", ",", "\n", "position", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "np", ".", "ndarray", ":", "\n", "        ", "\"\"\"Estimate round-wise (or sample-wise) rewards.\n\n        Parameters\n        ----------\n        reward: array-like, shape (n_rounds,)\n            Rewards observed for each data in logged bandit data, i.e., :math:`r_i`.\n\n        action: array-like, shape (n_rounds,)\n            Actions sampled by the logging/behavior policy for each data in logged bandit data, i.e., :math:`a_i`.\n\n        pscore: array-like, shape (n_rounds,)\n            Action choice probabilities of the logging/behavior policy (propensity scores), i.e., :math:`\\\\pi_b(a_i|x_i)`.\n\n        action_dist: array-like, shape (n_rounds, n_actions, len_list)\n            Action choice probabilities of the evaluation policy (can be deterministic), i.e., :math:`\\\\pi_e(a_i|x_i)`.\n\n        estimated_rewards_by_reg_model: array-like, shape (n_rounds, n_actions, len_list)\n            Estimated expected rewards given context, action, and position, i.e., :math:`\\\\hat{q}(x_i,a_i)`.\n\n        position: array-like, shape (n_rounds,), default=None\n            Indices to differentiate positions in a recommendation interface where the actions are presented.\n            If None, the effect of position on the reward will be ignored.\n            (If only a single action is chosen for each data, you can just ignore this argument.)\n\n        Returns\n        ----------\n        estimated_rewards: array-like, shape (n_rounds,)\n            Estimated rewards for each observation.\n\n        \"\"\"", "\n", "n", "=", "action", ".", "shape", "[", "0", "]", "\n", "iw", "=", "action_dist", "[", "np", ".", "arange", "(", "n", ")", ",", "action", ",", "position", "]", "/", "pscore", "\n", "switch_indicator", "=", "np", ".", "array", "(", "iw", "<=", "self", ".", "lambda_", ",", "dtype", "=", "int", ")", "\n", "q_hat_at_position", "=", "estimated_rewards_by_reg_model", "[", "np", ".", "arange", "(", "n", ")", ",", ":", ",", "position", "]", "\n", "q_hat_factual", "=", "estimated_rewards_by_reg_model", "[", "np", ".", "arange", "(", "n", ")", ",", "action", ",", "position", "]", "\n", "pi_e_at_position", "=", "action_dist", "[", "np", ".", "arange", "(", "n", ")", ",", ":", ",", "position", "]", "\n", "estimated_rewards", "=", "np", ".", "average", "(", "\n", "q_hat_at_position", ",", "\n", "weights", "=", "pi_e_at_position", ",", "\n", "axis", "=", "1", ",", "\n", ")", "\n", "estimated_rewards", "+=", "switch_indicator", "*", "iw", "*", "(", "reward", "-", "q_hat_factual", ")", "\n", "\n", "return", "estimated_rewards", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators.SwitchDoublyRobust._estimate_mse_score": [[1478, 1563], ["numpy.var", "estimators.SwitchDoublyRobust._estimate_round_rewards", "helper.estimate_high_probability_upper_bound_bias", "helper.estimate_bias_in_ope", "numpy.arange", "numpy.array", "numpy.array", "numpy.arange", "numpy.arange"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SelfNormalizedSlateStandardIPS._estimate_round_rewards", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.helper.estimate_high_probability_upper_bound_bias", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.helper.estimate_bias_in_ope"], ["", "def", "_estimate_mse_score", "(", "\n", "self", ",", "\n", "reward", ":", "np", ".", "ndarray", ",", "\n", "action", ":", "np", ".", "ndarray", ",", "\n", "pscore", ":", "np", ".", "ndarray", ",", "\n", "action_dist", ":", "np", ".", "ndarray", ",", "\n", "estimated_rewards_by_reg_model", ":", "np", ".", "ndarray", ",", "\n", "position", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "use_bias_upper_bound", ":", "bool", "=", "False", ",", "\n", "delta", ":", "float", "=", "0.05", ",", "\n", ")", "->", "float", ":", "\n", "        ", "\"\"\"Estimate the MSE score of a given switching hyperparameter to conduct hyperparameter tuning.\n\n        Parameters\n        ----------\n        reward: array-like, shape (n_rounds,)\n            Rewards observed for each data in logged bandit data, i.e., :math:`r_i`.\n\n        action: array-like, shape (n_rounds,)\n            Actions sampled by the logging/behavior policy for each data in logged bandit data, i.e., :math:`a_i`.\n\n        pscore: array-like, shape (n_rounds,)\n            Action choice probabilities of the logging/behavior policy (propensity scores), i.e., :math:`\\\\pi_b(a_i|x_i)`.\n\n        action_dist: array-like, shape (n_rounds, n_actions, len_list)\n            Action choice probabilities of the evaluation policy (can be deterministic), i.e., :math:`\\\\pi_e(a_i|x_i)`.\n\n        estimated_rewards_by_reg_model: array-like, shape (n_rounds, n_actions, len_list)\n            Estimated expected rewards given context, action, and position, i.e., :math:`\\\\hat{q}(x_i,a_i)`.\n\n        position: array-like, shape (n_rounds,), default=None\n            Indices to differentiate positions in a recommendation interface where the actions are presented.\n            If None, the effect of position on the reward will be ignored.\n            (If only a single action is chosen for each data, you can just ignore this argument.)\n\n        use_bias_upper_bound: bool, default=True\n            Whether to use a bias upper bound in hyperparameter tuning.\n            If False, the direct bias estimator is used to estimate the MSE. See Su et al.(2020) for details.\n\n        delta: float, default=0.05\n            A confidence delta to construct a high probability upper bound used in SLOPE.\n\n        Returns\n        ----------\n        estimated_mse_score: float\n            Estimated MSE score of a given switching hyperparameter `lambda_`.\n            MSE score is the sum of (high probability) upper bound of bias and the sample variance.\n            This is estimated using the automatic hyperparameter tuning procedure\n            based on Section 5 of Su et al.(2020).\n\n        \"\"\"", "\n", "n", "=", "reward", ".", "shape", "[", "0", "]", "\n", "# estimate the sample variance of Switch-DR (Eq.(8) of Wang et al.(2017))", "\n", "sample_variance", "=", "np", ".", "var", "(", "\n", "self", ".", "_estimate_round_rewards", "(", "\n", "reward", "=", "reward", ",", "\n", "action", "=", "action", ",", "\n", "pscore", "=", "pscore", ",", "\n", "action_dist", "=", "action_dist", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", "position", "=", "position", ",", "\n", ")", "\n", ")", "\n", "sample_variance", "/=", "n", "\n", "\n", "# estimate the (high probability) upper bound of the bias of Switch-DR", "\n", "iw", "=", "action_dist", "[", "np", ".", "arange", "(", "n", ")", ",", "action", ",", "position", "]", "/", "pscore", "\n", "if", "use_bias_upper_bound", ":", "\n", "            ", "bias_term", "=", "estimate_high_probability_upper_bound_bias", "(", "\n", "reward", "=", "reward", ",", "\n", "iw", "=", "iw", ",", "\n", "iw_hat", "=", "iw", "*", "np", ".", "array", "(", "iw", "<=", "self", ".", "lambda_", ",", "dtype", "=", "int", ")", ",", "\n", "q_hat", "=", "estimated_rewards_by_reg_model", "[", "np", ".", "arange", "(", "n", ")", ",", "action", ",", "position", "]", ",", "\n", "delta", "=", "delta", ",", "\n", ")", "\n", "", "else", ":", "\n", "            ", "bias_term", "=", "estimate_bias_in_ope", "(", "\n", "reward", "=", "reward", ",", "\n", "iw", "=", "iw", ",", "\n", "iw_hat", "=", "iw", "*", "np", ".", "array", "(", "iw", "<=", "self", ".", "lambda_", ",", "dtype", "=", "int", ")", ",", "\n", "q_hat", "=", "estimated_rewards_by_reg_model", "[", "np", ".", "arange", "(", "n", ")", ",", "action", ",", "position", "]", ",", "\n", ")", "\n", "", "estimated_mse_score", "=", "sample_variance", "+", "(", "bias_term", "**", "2", ")", "\n", "\n", "return", "estimated_mse_score", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators.DoublyRobustWithShrinkage.__post_init__": [[1613, 1626], ["sklearn.utils.check_scalar", "ValueError", "isinstance", "TypeError", "type"], "methods", ["None"], ["def", "__post_init__", "(", "self", ")", "->", "None", ":", "\n", "        ", "\"\"\"Initialize Class.\"\"\"", "\n", "check_scalar", "(", "\n", "self", ".", "lambda_", ",", "\n", "name", "=", "\"lambda_\"", ",", "\n", "target_type", "=", "(", "int", ",", "float", ")", ",", "\n", "min_val", "=", "0.0", ",", "\n", ")", "\n", "if", "self", ".", "lambda_", "!=", "self", ".", "lambda_", ":", "\n", "            ", "raise", "ValueError", "(", "\"`lambda_` must not be nan\"", ")", "\n", "", "if", "not", "isinstance", "(", "self", ".", "use_estimated_pscore", ",", "bool", ")", ":", "\n", "            ", "raise", "TypeError", "(", "\n", "f\"`use_estimated_pscore` must be a bool, but {type(self.use_estimated_pscore)} is given\"", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators.DoublyRobustWithShrinkage._estimate_round_rewards": [[1628, 1686], ["numpy.average", "numpy.arange", "numpy.arange", "numpy.arange", "numpy.arange"], "methods", ["None"], ["", "", "def", "_estimate_round_rewards", "(", "\n", "self", ",", "\n", "reward", ":", "np", ".", "ndarray", ",", "\n", "action", ":", "np", ".", "ndarray", ",", "\n", "pscore", ":", "np", ".", "ndarray", ",", "\n", "action_dist", ":", "np", ".", "ndarray", ",", "\n", "estimated_rewards_by_reg_model", ":", "np", ".", "ndarray", ",", "\n", "position", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "np", ".", "ndarray", ":", "\n", "        ", "\"\"\"Estimate round-wise (or sample-wise) rewards.\n\n        Parameters\n        ----------\n        reward: array-like, shape (n_rounds,)\n            Rewards observed for each data in logged bandit data, i.e., :math:`r_i`.\n\n        action: array-like, shape (n_rounds,)\n            Actions sampled by the logging/behavior policy for each data in logged bandit data, i.e., :math:`a_i`.\n\n        pscore: array-like, shape (n_rounds,)\n            Action choice probabilities of the logging/behavior policy (propensity scores), i.e., :math:`\\\\pi_b(a_i|x_i)`.\n\n        action_dist: array-like, shape (n_rounds, n_actions, len_list)\n            Action choice probabilities of the evaluation policy (can be deterministic), i.e., :math:`\\\\pi_e(a_i|x_i)`.\n\n        estimated_rewards_by_reg_model: array-like, shape (n_rounds, n_actions, len_list)\n            Estimated expected rewards given context, action, and position, i.e., :math:`\\\\hat{q}(x_i,a_i)`.\n\n        position: array-like, shape (n_rounds,), default=None\n            Indices to differentiate positions in a recommendation interface where the actions are presented.\n            If None, the effect of position on the reward will be ignored.\n            (If only a single action is chosen for each data, you can just ignore this argument.)\n\n        Returns\n        ----------\n        estimated_rewards: array-like, shape (n_rounds,)\n            Estimated rewards for each observation.\n\n        \"\"\"", "\n", "n", "=", "action", ".", "shape", "[", "0", "]", "\n", "iw", "=", "action_dist", "[", "np", ".", "arange", "(", "n", ")", ",", "action", ",", "position", "]", "/", "pscore", "\n", "if", "self", ".", "lambda_", "<", "np", ".", "inf", ":", "\n", "            ", "iw_hat", "=", "(", "self", ".", "lambda_", "*", "iw", ")", "/", "(", "iw", "**", "2", "+", "self", ".", "lambda_", ")", "\n", "", "else", ":", "\n", "            ", "iw_hat", "=", "iw", "\n", "\n", "", "q_hat_at_position", "=", "estimated_rewards_by_reg_model", "[", "np", ".", "arange", "(", "n", ")", ",", ":", ",", "position", "]", "\n", "q_hat_factual", "=", "estimated_rewards_by_reg_model", "[", "np", ".", "arange", "(", "n", ")", ",", "action", ",", "position", "]", "\n", "pi_e_at_position", "=", "action_dist", "[", "np", ".", "arange", "(", "n", ")", ",", ":", ",", "position", "]", "\n", "estimated_rewards", "=", "np", ".", "average", "(", "\n", "q_hat_at_position", ",", "\n", "weights", "=", "pi_e_at_position", ",", "\n", "axis", "=", "1", ",", "\n", ")", "\n", "estimated_rewards", "+=", "iw_hat", "*", "(", "reward", "-", "q_hat_factual", ")", "\n", "\n", "return", "estimated_rewards", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators.DoublyRobustWithShrinkage._estimate_mse_score": [[1687, 1775], ["numpy.var", "estimators.DoublyRobustWithShrinkage._estimate_round_rewards", "helper.estimate_high_probability_upper_bound_bias", "helper.estimate_bias_in_ope", "numpy.arange", "numpy.arange", "numpy.arange"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SelfNormalizedSlateStandardIPS._estimate_round_rewards", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.helper.estimate_high_probability_upper_bound_bias", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.helper.estimate_bias_in_ope"], ["", "def", "_estimate_mse_score", "(", "\n", "self", ",", "\n", "reward", ":", "np", ".", "ndarray", ",", "\n", "action", ":", "np", ".", "ndarray", ",", "\n", "pscore", ":", "np", ".", "ndarray", ",", "\n", "action_dist", ":", "np", ".", "ndarray", ",", "\n", "estimated_rewards_by_reg_model", ":", "np", ".", "ndarray", ",", "\n", "position", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "use_bias_upper_bound", ":", "bool", "=", "False", ",", "\n", "delta", ":", "float", "=", "0.05", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "float", ":", "\n", "        ", "\"\"\"Estimate the MSE score of a given shrinkage hyperparameter to conduct hyperparameter tuning.\n\n        Parameters\n        ----------\n        reward: array-like, shape (n_rounds,)\n            Rewards observed for each data in logged bandit data, i.e., :math:`r_i`.\n\n        action: array-like, shape (n_rounds,)\n            Actions sampled by the logging/behavior policy for each data in logged bandit data, i.e., :math:`a_i`.\n\n        pscore: array-like, shape (n_rounds,)\n            Action choice probabilities of the logging/behavior policy (propensity scores), i.e., :math:`\\\\pi_b(a_i|x_i)`.\n\n        action_dist: array-like, shape (n_rounds, n_actions, len_list)\n            Action choice probabilities of the evaluation policy (can be deterministic), i.e., :math:`\\\\pi_e(a_i|x_i)`.\n\n        estimated_rewards_by_reg_model: array-like, shape (n_rounds, n_actions, len_list)\n            Estimated expected rewards given context, action, and position, i.e., :math:`\\\\hat{q}(x_i,a_i)`.\n\n        position: array-like, shape (n_rounds,), default=None\n            Indices to differentiate positions in a recommendation interface where the actions are presented.\n\n        use_bias_upper_bound: bool, default=True\n            Whether to use a bias upper bound in hyperparameter tuning.\n            If False, the direct bias estimator is used to estimate the MSE. See Su et al.(2020) for details.\n\n        delta: float, default=0.05\n            A confidence delta to construct a high probability upper bound used in SLOPE.\n\n        Returns\n        ----------\n        estimated_mse_score: float\n            Estimated MSE score of a given shrinkage hyperparameter `lambda_`.\n            MSE score is the sum of (high probability) upper bound of bias and the sample variance.\n            This is estimated using the automatic hyperparameter tuning procedure\n            based on Section 5 of Su et al.(2020).\n\n        \"\"\"", "\n", "n", "=", "reward", ".", "shape", "[", "0", "]", "\n", "# estimate the sample variance of DRos", "\n", "sample_variance", "=", "np", ".", "var", "(", "\n", "self", ".", "_estimate_round_rewards", "(", "\n", "reward", "=", "reward", ",", "\n", "action", "=", "action", ",", "\n", "pscore", "=", "pscore", ",", "\n", "action_dist", "=", "action_dist", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", "position", "=", "position", ",", "\n", ")", "\n", ")", "\n", "sample_variance", "/=", "n", "\n", "\n", "# estimate the (high probability) upper bound of the bias of DRos", "\n", "iw", "=", "action_dist", "[", "np", ".", "arange", "(", "n", ")", ",", "action", ",", "position", "]", "/", "pscore", "\n", "if", "self", ".", "lambda_", "<", "np", ".", "inf", ":", "\n", "            ", "iw_hat", "=", "(", "self", ".", "lambda_", "*", "iw", ")", "/", "(", "iw", "**", "2", "+", "self", ".", "lambda_", ")", "\n", "", "else", ":", "\n", "            ", "iw_hat", "=", "iw", "\n", "", "if", "use_bias_upper_bound", ":", "\n", "            ", "bias_term", "=", "estimate_high_probability_upper_bound_bias", "(", "\n", "reward", "=", "reward", ",", "\n", "iw", "=", "iw", ",", "\n", "iw_hat", "=", "iw_hat", ",", "\n", "q_hat", "=", "estimated_rewards_by_reg_model", "[", "np", ".", "arange", "(", "n", ")", ",", "action", ",", "position", "]", ",", "\n", "delta", "=", "delta", ",", "\n", ")", "\n", "", "else", ":", "\n", "            ", "bias_term", "=", "estimate_bias_in_ope", "(", "\n", "reward", "=", "reward", ",", "\n", "iw", "=", "iw", ",", "\n", "iw_hat", "=", "iw_hat", ",", "\n", "q_hat", "=", "estimated_rewards_by_reg_model", "[", "np", ".", "arange", "(", "n", ")", ",", "action", ",", "position", "]", ",", "\n", ")", "\n", "", "estimated_mse_score", "=", "sample_variance", "+", "(", "bias_term", "**", "2", ")", "\n", "\n", "return", "estimated_mse_score", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators.SubGaussianInverseProbabilityWeighting.__post_init__": [[1818, 1832], ["sklearn.utils.check_scalar", "ValueError", "isinstance", "TypeError", "type"], "methods", ["None"], ["def", "__post_init__", "(", "self", ")", "->", "None", ":", "\n", "        ", "\"\"\"Initialize Class.\"\"\"", "\n", "check_scalar", "(", "\n", "self", ".", "lambda_", ",", "\n", "name", "=", "\"lambda_\"", ",", "\n", "target_type", "=", "(", "int", ",", "float", ")", ",", "\n", "min_val", "=", "0.0", ",", "\n", "max_val", "=", "1.0", ",", "\n", ")", "\n", "if", "self", ".", "lambda_", "!=", "self", ".", "lambda_", ":", "\n", "            ", "raise", "ValueError", "(", "\"`lambda_` must not be nan\"", ")", "\n", "", "if", "not", "isinstance", "(", "self", ".", "use_estimated_pscore", ",", "bool", ")", ":", "\n", "            ", "raise", "TypeError", "(", "\n", "f\"`use_estimated_pscore` must be a bool, but {type(self.use_estimated_pscore)} is given\"", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators.SubGaussianInverseProbabilityWeighting._estimate_round_rewards": [[1834, 1875], ["numpy.arange"], "methods", ["None"], ["", "", "def", "_estimate_round_rewards", "(", "\n", "self", ",", "\n", "reward", ":", "np", ".", "ndarray", ",", "\n", "action", ":", "np", ".", "ndarray", ",", "\n", "pscore", ":", "np", ".", "ndarray", ",", "\n", "action_dist", ":", "np", ".", "ndarray", ",", "\n", "position", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "np", ".", "ndarray", ":", "\n", "        ", "\"\"\"Estimate round-wise (or sample-wise) rewards.\n\n        Parameters\n        ----------\n        reward: array-like or Tensor, shape (n_rounds,)\n            Rewards observed for each data in logged bandit data, i.e., :math:`r_i`.\n\n        action: array-like or Tensor, shape (n_rounds,)\n            Actions sampled by the logging/behavior policy for each data in logged bandit data, i.e., :math:`a_i`.\n\n        pscore: array-like or Tensor, shape (n_rounds,)\n            Action choice probabilities of the logging/behavior policy (propensity scores), i.e., :math:`\\\\pi_b(a_i|x_i)`.\n\n        action_dist: array-like or Tensor, shape (n_rounds, n_actions, len_list)\n            Action choice probabilities of the evaluation policy (can be deterministic), i.e., :math:`\\\\pi_e(a_i|x_i)`.\n\n        position: array-like or Tensor, shape (n_rounds,), default=None\n            Indices to differentiate positions in a recommendation interface where the actions are presented.\n            If None, the effect of position on the reward will be ignored.\n            (If only a single action is chosen for each data, you can just ignore this argument.)\n\n        Returns\n        ----------\n        estimated_rewards: array-like or Tensor, shape (n_rounds,)\n            Estimated rewards for each observation.\n\n        \"\"\"", "\n", "iw", "=", "action_dist", "[", "np", ".", "arange", "(", "action", ".", "shape", "[", "0", "]", ")", ",", "action", ",", "position", "]", "/", "pscore", "\n", "iw_hat", "=", "iw", "/", "(", "1", "-", "self", ".", "lambda_", "+", "self", ".", "lambda_", "*", "iw", ")", "\n", "estimated_rewards", "=", "iw_hat", "*", "reward", "\n", "\n", "return", "estimated_rewards", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators.SubGaussianInverseProbabilityWeighting._estimate_mse_score": [[1876, 1957], ["numpy.var", "estimators.SubGaussianInverseProbabilityWeighting._estimate_round_rewards", "helper.estimate_high_probability_upper_bound_bias", "helper.estimate_bias_in_ope", "numpy.arange"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SelfNormalizedSlateStandardIPS._estimate_round_rewards", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.helper.estimate_high_probability_upper_bound_bias", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.helper.estimate_bias_in_ope"], ["", "def", "_estimate_mse_score", "(", "\n", "self", ",", "\n", "reward", ":", "np", ".", "ndarray", ",", "\n", "action", ":", "np", ".", "ndarray", ",", "\n", "pscore", ":", "np", ".", "ndarray", ",", "\n", "action_dist", ":", "np", ".", "ndarray", ",", "\n", "position", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "use_bias_upper_bound", ":", "bool", "=", "False", ",", "\n", "delta", ":", "float", "=", "0.05", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "float", ":", "\n", "        ", "\"\"\"Estimate the MSE score of a given shrinkage hyperparameter to conduct hyperparameter tuning.\n\n        Parameters\n        ----------\n        reward: array-like, shape (n_rounds,)\n            Rewards observed for each data in logged bandit data, i.e., :math:`r_i`.\n\n        action: array-like, shape (n_rounds,)\n            Actions sampled by the logging/behavior policy for each data in logged bandit data, i.e., :math:`a_i`.\n\n        pscore: array-like, shape (n_rounds,)\n            Action choice probabilities of the logging/behavior policy (propensity scores), i.e., :math:`\\\\pi_b(a_i|x_i)`.\n\n        action_dist: array-like, shape (n_rounds, n_actions, len_list)\n            Action choice probabilities of the evaluation policy (can be deterministic), i.e., :math:`\\\\pi_e(a_i|x_i)`.\n\n        estimated_rewards_by_reg_model: array-like, shape (n_rounds, n_actions, len_list)\n            Estimated expected rewards given context, action, and position, i.e., :math:`\\\\hat{q}(x_i,a_i)`.\n\n        position: array-like, shape (n_rounds,), default=None\n            Indices to differentiate positions in a recommendation interface where the actions are presented.\n\n        use_bias_upper_bound: bool, default=True\n            Whether to use a bias upper bound in hyperparameter tuning.\n            If False, the direct bias estimator is used to estimate the MSE. See Su et al.(2020) for details.\n\n        delta: float, default=0.05\n            A confidence delta to construct a high probability upper bound used in SLOPE.\n\n        Returns\n        ----------\n        estimated_mse_score: float\n            Estimated MSE score of a given shrinkage hyperparameter `lambda_`.\n            MSE score is the sum of (high probability) upper bound of bias and the sample variance.\n            This is estimated using the automatic hyperparameter tuning procedure\n            based on Section 5 of Su et al.(2020).\n\n        \"\"\"", "\n", "n", "=", "reward", ".", "shape", "[", "0", "]", "\n", "# estimate the sample variance of DRos", "\n", "sample_variance", "=", "np", ".", "var", "(", "\n", "self", ".", "_estimate_round_rewards", "(", "\n", "reward", "=", "reward", ",", "\n", "action", "=", "action", ",", "\n", "pscore", "=", "pscore", ",", "\n", "action_dist", "=", "action_dist", ",", "\n", "position", "=", "position", ",", "\n", ")", "\n", ")", "\n", "sample_variance", "/=", "n", "\n", "\n", "# estimate the (high probability) upper bound of the bias of SGIPW", "\n", "iw", "=", "action_dist", "[", "np", ".", "arange", "(", "n", ")", ",", "action", ",", "position", "]", "/", "pscore", "\n", "iw_hat", "=", "iw", "/", "(", "1", "-", "self", ".", "lambda_", "+", "self", ".", "lambda_", "*", "iw", ")", "\n", "if", "use_bias_upper_bound", ":", "\n", "            ", "bias_term", "=", "estimate_high_probability_upper_bound_bias", "(", "\n", "reward", "=", "reward", ",", "\n", "iw", "=", "iw", ",", "\n", "iw_hat", "=", "iw_hat", ",", "\n", "delta", "=", "delta", ",", "\n", ")", "\n", "", "else", ":", "\n", "            ", "bias_term", "=", "estimate_bias_in_ope", "(", "\n", "reward", "=", "reward", ",", "\n", "iw", "=", "iw", ",", "\n", "iw_hat", "=", "iw_hat", ",", "\n", ")", "\n", "", "estimated_mse_score", "=", "sample_variance", "+", "(", "bias_term", "**", "2", ")", "\n", "\n", "return", "estimated_mse_score", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators.SubGaussianDoublyRobust.__post_init__": [[2002, 2016], ["sklearn.utils.check_scalar", "ValueError", "isinstance", "TypeError", "type"], "methods", ["None"], ["def", "__post_init__", "(", "self", ")", "->", "None", ":", "\n", "        ", "\"\"\"Initialize Class.\"\"\"", "\n", "check_scalar", "(", "\n", "self", ".", "lambda_", ",", "\n", "name", "=", "\"lambda_\"", ",", "\n", "target_type", "=", "(", "int", ",", "float", ")", ",", "\n", "min_val", "=", "0.0", ",", "\n", "max_val", "=", "1.0", ",", "\n", ")", "\n", "if", "self", ".", "lambda_", "!=", "self", ".", "lambda_", ":", "\n", "            ", "raise", "ValueError", "(", "\"`lambda_` must not be nan\"", ")", "\n", "", "if", "not", "isinstance", "(", "self", ".", "use_estimated_pscore", ",", "bool", ")", ":", "\n", "            ", "raise", "TypeError", "(", "\n", "f\"`use_estimated_pscore` must be a bool, but {type(self.use_estimated_pscore)} is given\"", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators.SubGaussianDoublyRobust._estimate_round_rewards": [[2018, 2073], ["numpy.average", "numpy.arange", "numpy.arange", "numpy.arange", "numpy.arange"], "methods", ["None"], ["", "", "def", "_estimate_round_rewards", "(", "\n", "self", ",", "\n", "reward", ":", "np", ".", "ndarray", ",", "\n", "action", ":", "np", ".", "ndarray", ",", "\n", "pscore", ":", "np", ".", "ndarray", ",", "\n", "action_dist", ":", "np", ".", "ndarray", ",", "\n", "estimated_rewards_by_reg_model", ":", "np", ".", "ndarray", ",", "\n", "position", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "np", ".", "ndarray", ":", "\n", "        ", "\"\"\"Estimate round-wise (or sample-wise) rewards.\n\n        Parameters\n        ----------\n        reward: array-like or Tensor, shape (n_rounds,)\n            Rewards observed for each data in logged bandit data, i.e., :math:`r_i`.\n\n        action: array-like or Tensor, shape (n_rounds,)\n            Actions sampled by the logging/behavior policy for each data in logged bandit data, i.e., :math:`a_i`.\n\n        pscore: array-like or Tensor, shape (n_rounds,)\n            Action choice probabilities of the logging/behavior policy (propensity scores), i.e., :math:`\\\\pi_b(a_i|x_i)`.\n\n        action_dist: array-like or Tensor, shape (n_rounds, n_actions, len_list)\n            Action choice probabilities of the evaluation policy (can be deterministic), i.e., :math:`\\\\pi_e(a_i|x_i)`.\n\n        estimated_rewards_by_reg_model: array-like or Tensor, shape (n_rounds, n_actions, len_list)\n            Estimated expected rewards given context, action, and position, i.e., :math:`\\\\hat{q}(x_i,a_i)`.\n\n        position: array-like or Tensor, shape (n_rounds,), default=None\n            Indices to differentiate positions in a recommendation interface where the actions are presented.\n            If None, the effect of position on the reward will be ignored.\n            (If only a single action is chosen for each data, you can just ignore this argument.)\n\n        Returns\n        ----------\n        estimated_rewards: array-like or Tensor, shape (n_rounds,)\n            Estimated rewards for each observation.\n\n        \"\"\"", "\n", "n", "=", "action", ".", "shape", "[", "0", "]", "\n", "iw", "=", "action_dist", "[", "np", ".", "arange", "(", "n", ")", ",", "action", ",", "position", "]", "/", "pscore", "\n", "iw_hat", "=", "iw", "/", "(", "1", "-", "self", ".", "lambda_", "+", "self", ".", "lambda_", "*", "iw", ")", "\n", "\n", "q_hat_at_position", "=", "estimated_rewards_by_reg_model", "[", "np", ".", "arange", "(", "n", ")", ",", ":", ",", "position", "]", "\n", "q_hat_factual", "=", "estimated_rewards_by_reg_model", "[", "np", ".", "arange", "(", "n", ")", ",", "action", ",", "position", "]", "\n", "pi_e_at_position", "=", "action_dist", "[", "np", ".", "arange", "(", "n", ")", ",", ":", ",", "position", "]", "\n", "estimated_rewards", "=", "np", ".", "average", "(", "\n", "q_hat_at_position", ",", "\n", "weights", "=", "pi_e_at_position", ",", "\n", "axis", "=", "1", ",", "\n", ")", "\n", "estimated_rewards", "+=", "iw_hat", "*", "(", "reward", "-", "q_hat_factual", ")", "\n", "\n", "return", "estimated_rewards", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators.SubGaussianDoublyRobust._estimate_mse_score": [[2074, 2159], ["numpy.var", "estimators.SubGaussianDoublyRobust._estimate_round_rewards", "helper.estimate_high_probability_upper_bound_bias", "helper.estimate_bias_in_ope", "numpy.arange", "numpy.arange", "numpy.arange"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SelfNormalizedSlateStandardIPS._estimate_round_rewards", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.helper.estimate_high_probability_upper_bound_bias", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.helper.estimate_bias_in_ope"], ["", "def", "_estimate_mse_score", "(", "\n", "self", ",", "\n", "reward", ":", "np", ".", "ndarray", ",", "\n", "action", ":", "np", ".", "ndarray", ",", "\n", "pscore", ":", "np", ".", "ndarray", ",", "\n", "action_dist", ":", "np", ".", "ndarray", ",", "\n", "estimated_rewards_by_reg_model", ":", "np", ".", "ndarray", ",", "\n", "position", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "use_bias_upper_bound", ":", "bool", "=", "False", ",", "\n", "delta", ":", "float", "=", "0.05", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "float", ":", "\n", "        ", "\"\"\"Estimate the MSE score of a given shrinkage hyperparameter to conduct hyperparameter tuning.\n\n        Parameters\n        ----------\n        reward: array-like, shape (n_rounds,)\n            Rewards observed for each data in logged bandit data, i.e., :math:`r_i`.\n\n        action: array-like, shape (n_rounds,)\n            Actions sampled by the logging/behavior policy for each data in logged bandit data, i.e., :math:`a_i`.\n\n        pscore: array-like, shape (n_rounds,)\n            Action choice probabilities of the logging/behavior policy (propensity scores), i.e., :math:`\\\\pi_b(a_i|x_i)`.\n\n        action_dist: array-like, shape (n_rounds, n_actions, len_list)\n            Action choice probabilities of the evaluation policy (can be deterministic), i.e., :math:`\\\\pi_e(a_i|x_i)`.\n\n        estimated_rewards_by_reg_model: array-like, shape (n_rounds, n_actions, len_list)\n            Estimated expected rewards given context, action, and position, i.e., :math:`\\\\hat{q}(x_i,a_i)`.\n\n        position: array-like, shape (n_rounds,), default=None\n            Indices to differentiate positions in a recommendation interface where the actions are presented.\n\n        use_bias_upper_bound: bool, default=True\n            Whether to use a bias upper bound in hyperparameter tuning.\n            If False, the direct bias estimator is used to estimate the MSE. See Su et al.(2020) for details.\n\n        delta: float, default=0.05\n            A confidence delta to construct a high probability upper bound used in SLOPE.\n\n        Returns\n        ----------\n        estimated_mse_score: float\n            Estimated MSE score of a given shrinkage hyperparameter `lambda_`.\n            MSE score is the sum of (high probability) upper bound of bias and the sample variance.\n            This is estimated using the automatic hyperparameter tuning procedure\n            based on Section 5 of Su et al.(2020).\n\n        \"\"\"", "\n", "n", "=", "reward", ".", "shape", "[", "0", "]", "\n", "# estimate the sample variance of DRos", "\n", "sample_variance", "=", "np", ".", "var", "(", "\n", "self", ".", "_estimate_round_rewards", "(", "\n", "reward", "=", "reward", ",", "\n", "action", "=", "action", ",", "\n", "pscore", "=", "pscore", ",", "\n", "action_dist", "=", "action_dist", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", "position", "=", "position", ",", "\n", ")", "\n", ")", "\n", "sample_variance", "/=", "n", "\n", "\n", "# estimate the (high probability) upper bound of the bias of SGDR", "\n", "iw", "=", "action_dist", "[", "np", ".", "arange", "(", "n", ")", ",", "action", ",", "position", "]", "/", "pscore", "\n", "iw_hat", "=", "iw", "/", "(", "1", "-", "self", ".", "lambda_", "+", "self", ".", "lambda_", "*", "iw", ")", "\n", "if", "use_bias_upper_bound", ":", "\n", "            ", "bias_term", "=", "estimate_high_probability_upper_bound_bias", "(", "\n", "reward", "=", "reward", ",", "\n", "iw", "=", "iw", ",", "\n", "iw_hat", "=", "iw_hat", ",", "\n", "q_hat", "=", "estimated_rewards_by_reg_model", "[", "np", ".", "arange", "(", "n", ")", ",", "action", ",", "position", "]", ",", "\n", "delta", "=", "delta", ",", "\n", ")", "\n", "", "else", ":", "\n", "            ", "bias_term", "=", "estimate_bias_in_ope", "(", "\n", "reward", "=", "reward", ",", "\n", "iw", "=", "iw", ",", "\n", "iw_hat", "=", "iw_hat", ",", "\n", "q_hat", "=", "estimated_rewards_by_reg_model", "[", "np", ".", "arange", "(", "n", ")", ",", "action", ",", "position", "]", ",", "\n", ")", "\n", "", "estimated_mse_score", "=", "sample_variance", "+", "(", "bias_term", "**", "2", ")", "\n", "\n", "return", "estimated_mse_score", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators.BalancedInverseProbabilityWeighting.__post_init__": [[2210, 2220], ["sklearn.utils.check_scalar", "ValueError"], "methods", ["None"], ["def", "__post_init__", "(", "self", ")", "->", "None", ":", "\n", "        ", "\"\"\"Initialize Class.\"\"\"", "\n", "check_scalar", "(", "\n", "self", ".", "lambda_", ",", "\n", "name", "=", "\"lambda_\"", ",", "\n", "target_type", "=", "(", "int", ",", "float", ")", ",", "\n", "min_val", "=", "0.0", ",", "\n", ")", "\n", "if", "self", ".", "lambda_", "!=", "self", ".", "lambda_", ":", "\n", "            ", "raise", "ValueError", "(", "\"`lambda_` must not be nan\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators.BalancedInverseProbabilityWeighting._estimate_round_rewards": [[2221, 2265], ["isinstance", "numpy.zeros", "numpy.minimum", "numpy.minimum.mean"], "methods", ["None"], ["", "", "def", "_estimate_round_rewards", "(", "\n", "self", ",", "\n", "reward", ":", "np", ".", "ndarray", ",", "\n", "action", ":", "np", ".", "ndarray", ",", "\n", "estimated_importance_weights", ":", "np", ".", "ndarray", ",", "\n", "action_dist", ":", "np", ".", "ndarray", ",", "\n", "position", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "np", ".", "ndarray", ":", "\n", "        ", "\"\"\"Estimate round-wise (or sample-wise) rewards.\n\n        Parameters\n        ----------\n        reward: array-like, shape (n_rounds,)\n            Rewards observed for each data in logged bandit data, i.e., :math:`r_i`.\n\n        action: array-like, shape (n_rounds,)\n            Actions sampled by the logging/behavior policy for each data in logged bandit data, i.e., :math:`a_i`.\n\n        estimated_importance_weights: array-like, shape (n_rounds,)\n            Importance weights estimated via supervised classification using `obp.ope.ImportanceWeightEstimator`.\n\n        action_dist: array-like, shape (n_rounds, n_actions, len_list)\n            Action choice probabilities of the evaluation policy (can be deterministic), i.e., :math:`\\\\pi_e(a_i|x_i)`.\n\n        position: array-like, shape (n_rounds,), default=None\n            Indices to differentiate positions in a recommendation interface where the actions are presented.\n            If None, the effect of position on the reward will be ignored.\n            (If only a single action is chosen for each data, you can just ignore this argument.)\n\n        Returns\n        ----------\n        estimated_rewards: array-like, shape (n_rounds,)\n            Estimated rewards for each observation.\n\n        \"\"\"", "\n", "if", "position", "is", "None", ":", "\n", "            ", "position", "=", "np", ".", "zeros", "(", "action_dist", ".", "shape", "[", "0", "]", ",", "dtype", "=", "int", ")", "\n", "\n", "", "iw", "=", "estimated_importance_weights", "\n", "# weight clipping", "\n", "if", "isinstance", "(", "iw", ",", "np", ".", "ndarray", ")", ":", "\n", "            ", "iw", "=", "np", ".", "minimum", "(", "iw", ",", "self", ".", "lambda_", ")", "\n", "", "return", "reward", "*", "iw", "/", "iw", ".", "mean", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators.BalancedInverseProbabilityWeighting.estimate_policy_value": [[2266, 2324], ["utils.check_array", "utils.check_array", "utils.check_array", "utils.check_ope_inputs", "estimators.BalancedInverseProbabilityWeighting._estimate_round_rewards().mean", "numpy.zeros", "estimators.BalancedInverseProbabilityWeighting._estimate_round_rewards"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_ope_inputs", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SelfNormalizedSlateStandardIPS._estimate_round_rewards"], ["", "def", "estimate_policy_value", "(", "\n", "self", ",", "\n", "reward", ":", "np", ".", "ndarray", ",", "\n", "action", ":", "np", ".", "ndarray", ",", "\n", "action_dist", ":", "np", ".", "ndarray", ",", "\n", "estimated_importance_weights", ":", "np", ".", "ndarray", ",", "\n", "position", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "np", ".", "ndarray", ":", "\n", "        ", "\"\"\"Estimate the policy value of evaluation policy.\n\n        Parameters\n        ----------\n        reward: array-like, shape (n_rounds,)\n            Rewards observed for each data in logged bandit data, i.e., :math:`r_i`.\n\n        action: array-like, shape (n_rounds,)\n            Actions sampled by the logging/behavior policy for each data in logged bandit data, i.e., :math:`a_i`.\n\n        estimated_importance_weights: array-like, shape (n_rounds,)\n            Importance weights estimated via supervised classification using `obp.ope.ImportanceWeightEstimator`.\n\n        action_dist: array-like, shape (n_rounds, n_actions, len_list)\n            Action choice probabilities of the evaluation policy (can be deterministic), i.e., :math:`\\\\pi_e(a_i|x_i)`.\n\n        position: array-like, shape (n_rounds,), default=None\n            Indices to differentiate positions in a recommendation interface where the actions are presented.\n            If None, the effect of position on the reward will be ignored.\n            (If only a single action is chosen for each data, you can just ignore this argument.)\n\n        Returns\n        ----------\n        V_hat: float\n            Estimated policy value of evaluation policy.\n\n        \"\"\"", "\n", "check_array", "(", "array", "=", "reward", ",", "name", "=", "\"reward\"", ",", "expected_dim", "=", "1", ")", "\n", "check_array", "(", "array", "=", "action", ",", "name", "=", "\"action\"", ",", "expected_dim", "=", "1", ")", "\n", "check_array", "(", "\n", "array", "=", "estimated_importance_weights", ",", "\n", "name", "=", "\"estimated_importance_weights\"", ",", "\n", "expected_dim", "=", "1", ",", "\n", ")", "\n", "check_ope_inputs", "(", "\n", "action_dist", "=", "action_dist", ",", "\n", "position", "=", "position", ",", "\n", "action", "=", "action", ",", "\n", "reward", "=", "reward", ",", "\n", "estimated_importance_weights", "=", "estimated_importance_weights", ",", "\n", ")", "\n", "if", "position", "is", "None", ":", "\n", "            ", "position", "=", "np", ".", "zeros", "(", "action_dist", ".", "shape", "[", "0", "]", ",", "dtype", "=", "int", ")", "\n", "", "return", "self", ".", "_estimate_round_rewards", "(", "\n", "reward", "=", "reward", ",", "\n", "action", "=", "action", ",", "\n", "position", "=", "position", ",", "\n", "estimated_importance_weights", "=", "estimated_importance_weights", ",", "\n", "action_dist", "=", "action_dist", ",", "\n", ")", ".", "mean", "(", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators.BalancedInverseProbabilityWeighting.estimate_interval": [[2326, 2403], ["utils.check_array", "utils.check_array", "utils.check_array", "utils.check_ope_inputs", "estimators.BalancedInverseProbabilityWeighting._estimate_round_rewards", "utils.estimate_confidence_interval_by_bootstrap", "numpy.zeros"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_ope_inputs", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SelfNormalizedSlateStandardIPS._estimate_round_rewards", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.estimate_confidence_interval_by_bootstrap"], ["", "def", "estimate_interval", "(", "\n", "self", ",", "\n", "reward", ":", "np", ".", "ndarray", ",", "\n", "action", ":", "np", ".", "ndarray", ",", "\n", "estimated_importance_weights", ":", "np", ".", "ndarray", ",", "\n", "action_dist", ":", "np", ".", "ndarray", ",", "\n", "position", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "alpha", ":", "float", "=", "0.05", ",", "\n", "n_bootstrap_samples", ":", "int", "=", "10000", ",", "\n", "random_state", ":", "Optional", "[", "int", "]", "=", "None", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "Dict", "[", "str", ",", "float", "]", ":", "\n", "        ", "\"\"\"Estimate the confidence interval of the policy value using bootstrap.\n\n        Parameters\n        ----------\n        reward: array-like, shape (n_rounds,)\n            Rewards observed for each data in logged bandit data, i.e., :math:`r_i`.\n\n        action: array-like, shape (n_rounds,)\n            Actions sampled by the logging/behavior policy for each data in logged bandit data, i.e., :math:`a_i`.\n\n        estimated_importance_weights: array-like, shape (n_rounds,)\n            Importance weights estimated via supervised classification using `obp.ope.ImportanceWeightEstimator`.\n\n        action_dist: array-like, shape (n_rounds, n_actions, len_list)\n            Action choice probabilities of the evaluation policy (can be deterministic), i.e., :math:`\\\\pi_e(a_i|x_i)`.\n\n        position: array-like, shape (n_rounds,), default=None\n            Indices to differentiate positions in a recommendation interface where the actions are presented.\n            If None, the effect of position on the reward will be ignored.\n            (If only a single action is chosen for each data, you can just ignore this argument.)\n\n        alpha: float, default=0.05\n            Significance level.\n\n        n_bootstrap_samples: int, default=10000\n            Number of resampling performed in bootstrap sampling.\n\n        random_state: int, default=None\n            Controls the random seed in bootstrap sampling.\n\n        Returns\n        ----------\n        estimated_confidence_interval: Dict[str, float]\n            Dictionary storing the estimated mean and upper-lower confidence bounds.\n\n        \"\"\"", "\n", "check_array", "(", "array", "=", "reward", ",", "name", "=", "\"reward\"", ",", "expected_dim", "=", "1", ")", "\n", "check_array", "(", "array", "=", "action", ",", "name", "=", "\"action\"", ",", "expected_dim", "=", "1", ")", "\n", "check_array", "(", "\n", "array", "=", "estimated_importance_weights", ",", "\n", "name", "=", "\"estimated_importance_weights\"", ",", "\n", "expected_dim", "=", "1", ",", "\n", ")", "\n", "check_ope_inputs", "(", "\n", "action_dist", "=", "action_dist", ",", "\n", "position", "=", "position", ",", "\n", "action", "=", "action", ",", "\n", "reward", "=", "reward", ",", "\n", "estimated_importance_weights", "=", "estimated_importance_weights", ",", "\n", ")", "\n", "if", "position", "is", "None", ":", "\n", "            ", "position", "=", "np", ".", "zeros", "(", "action_dist", ".", "shape", "[", "0", "]", ",", "dtype", "=", "int", ")", "\n", "\n", "", "estimated_round_rewards", "=", "self", ".", "_estimate_round_rewards", "(", "\n", "reward", "=", "reward", ",", "\n", "action", "=", "action", ",", "\n", "position", "=", "position", ",", "\n", "estimated_importance_weights", "=", "estimated_importance_weights", ",", "\n", "action_dist", "=", "action_dist", ",", "\n", ")", "\n", "return", "estimate_confidence_interval_by_bootstrap", "(", "\n", "samples", "=", "estimated_round_rewards", ",", "\n", "alpha", "=", "alpha", ",", "\n", "n_bootstrap_samples", "=", "n_bootstrap_samples", ",", "\n", "random_state", "=", "random_state", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta_multi.MultiLoggersOffPolicyEvaluation.__post_init__": [[99, 115], ["dict", "RuntimeError", "isinstance", "isinstance", "isinstance", "isinstance"], "methods", ["None"], ["def", "__post_init__", "(", "self", ")", "->", "None", ":", "\n", "        ", "\"\"\"Initialize class.\"\"\"", "\n", "for", "key_", "in", "[", "\"action\"", ",", "\"position\"", ",", "\"reward\"", "]", ":", "\n", "            ", "if", "key_", "not", "in", "self", ".", "bandit_feedback", ":", "\n", "                ", "raise", "RuntimeError", "(", "f\"Missing key of {key_} in 'bandit_feedback'.\"", ")", "\n", "", "", "self", ".", "ope_estimators_", "=", "dict", "(", ")", "\n", "self", ".", "is_model_dependent", "=", "False", "\n", "for", "estimator", "in", "self", ".", "ope_estimators", ":", "\n", "            ", "self", ".", "ope_estimators_", "[", "estimator", ".", "estimator_name", "]", "=", "estimator", "\n", "if", "(", "\n", "isinstance", "(", "estimator", ",", "DM", ")", "\n", "or", "isinstance", "(", "estimator", ",", "NaiveDR", ")", "\n", "or", "isinstance", "(", "estimator", ",", "BalDR", ")", "\n", "or", "isinstance", "(", "estimator", ",", "WeightedDR", ")", "\n", ")", ":", "\n", "                ", "self", ".", "is_model_dependent", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta_multi.MultiLoggersOffPolicyEvaluation._create_estimator_inputs": [[116, 200], ["utils.check_array", "isinstance", "meta_multi.MultiLoggersOffPolicyEvaluation._preprocess_model_based_input", "estimated_rewards_by_reg_model.items", "utils.check_array", "isinstance", "utils.check_array", "ValueError", "value_or_dict.items", "utils.check_array", "ValueError", "utils.check_array", "ValueError", "ValueError"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta.OffPolicyEvaluation._preprocess_model_based_input", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array"], ["", "", "", "def", "_create_estimator_inputs", "(", "\n", "self", ",", "\n", "action_dist", ":", "np", ".", "ndarray", ",", "\n", "estimated_rewards_by_reg_model", ":", "Optional", "[", "\n", "Union", "[", "np", ".", "ndarray", ",", "Dict", "[", "str", ",", "np", ".", "ndarray", "]", "]", "\n", "]", "=", "None", ",", "\n", "estimated_pscore", ":", "Optional", "[", "Union", "[", "np", ".", "ndarray", ",", "Dict", "[", "str", ",", "np", ".", "ndarray", "]", "]", "]", "=", "None", ",", "\n", "estimated_pscore_avg", ":", "Optional", "[", "Union", "[", "np", ".", "ndarray", ",", "Dict", "[", "str", ",", "np", ".", "ndarray", "]", "]", "]", "=", "None", ",", "\n", ")", "->", "Dict", "[", "str", ",", "Dict", "[", "str", ",", "np", ".", "ndarray", "]", "]", ":", "\n", "        ", "\"\"\"Create input dictionary to estimate policy value using subclasses of `BaseOffPolicyEstimator`\"\"\"", "\n", "check_array", "(", "array", "=", "action_dist", ",", "name", "=", "\"action_dist\"", ",", "expected_dim", "=", "3", ")", "\n", "if", "estimated_rewards_by_reg_model", "is", "None", ":", "\n", "            ", "pass", "\n", "", "elif", "isinstance", "(", "estimated_rewards_by_reg_model", ",", "dict", ")", ":", "\n", "            ", "for", "estimator_name", ",", "value", "in", "estimated_rewards_by_reg_model", ".", "items", "(", ")", ":", "\n", "                ", "check_array", "(", "\n", "array", "=", "value", ",", "\n", "name", "=", "f\"estimated_rewards_by_reg_model[{estimator_name}]\"", ",", "\n", "expected_dim", "=", "3", ",", "\n", ")", "\n", "if", "value", ".", "shape", "!=", "action_dist", ".", "shape", ":", "\n", "                    ", "raise", "ValueError", "(", "\n", "f\"Expected `estimated_rewards_by_reg_model[{estimator_name}].shape == action_dist.shape`, but found it False.\"", "\n", ")", "\n", "", "", "", "else", ":", "\n", "            ", "check_array", "(", "\n", "array", "=", "estimated_rewards_by_reg_model", ",", "\n", "name", "=", "\"estimated_rewards_by_reg_model\"", ",", "\n", "expected_dim", "=", "3", ",", "\n", ")", "\n", "if", "estimated_rewards_by_reg_model", ".", "shape", "!=", "action_dist", ".", "shape", ":", "\n", "                ", "raise", "ValueError", "(", "\n", "\"Expected `estimated_rewards_by_reg_model.shape == action_dist.shape`, but found it False\"", "\n", ")", "\n", "", "", "for", "var_name", ",", "value_or_dict", "in", "{", "\n", "\"estimated_pscore\"", ":", "estimated_pscore", ",", "\n", "\"estimated_pscore_avg\"", ":", "estimated_pscore_avg", ",", "\n", "}", ".", "items", "(", ")", ":", "\n", "            ", "if", "value_or_dict", "is", "None", ":", "\n", "                ", "pass", "\n", "", "elif", "isinstance", "(", "value_or_dict", ",", "dict", ")", ":", "\n", "                ", "for", "estimator_name", ",", "value", "in", "value_or_dict", ".", "items", "(", ")", ":", "\n", "                    ", "check_array", "(", "\n", "array", "=", "value", ",", "\n", "name", "=", "f\"{var_name}[{estimator_name}]\"", ",", "\n", "expected_dim", "=", "1", ",", "\n", ")", "\n", "if", "value", ".", "shape", "[", "0", "]", "!=", "action_dist", ".", "shape", "[", "0", "]", ":", "\n", "                        ", "raise", "ValueError", "(", "\n", "f\"Expected `{var_name}[{estimator_name}].shape[0] == action_dist.shape[0]`, but found it False\"", "\n", ")", "\n", "", "", "", "else", ":", "\n", "                ", "check_array", "(", "array", "=", "value_or_dict", ",", "name", "=", "var_name", ",", "expected_dim", "=", "1", ")", "\n", "if", "value_or_dict", ".", "shape", "[", "0", "]", "!=", "action_dist", ".", "shape", "[", "0", "]", ":", "\n", "                    ", "raise", "ValueError", "(", "\n", "f\"Expected `{var_name}.shape[0] == action_dist.shape[0]`, but found it False\"", "\n", ")", "\n", "\n", "", "", "", "estimator_inputs", "=", "{", "\n", "estimator_name", ":", "{", "\n", "input_", ":", "self", ".", "bandit_feedback", "[", "input_", "]", "\n", "for", "input_", "in", "[", "\"reward\"", ",", "\"action\"", ",", "\"position\"", "]", "\n", "}", "\n", "for", "estimator_name", "in", "self", ".", "ope_estimators_", "\n", "}", "\n", "for", "estimator_name", "in", "self", ".", "ope_estimators_", ":", "\n", "            ", "for", "input_", "in", "[", "\"stratum_idx\"", ",", "\"pscore\"", ",", "\"pscore_avg\"", "]", ":", "\n", "                ", "if", "input_", "in", "self", ".", "bandit_feedback", ":", "\n", "                    ", "estimator_inputs", "[", "estimator_name", "]", "[", "input_", "]", "=", "self", ".", "bandit_feedback", "[", "\n", "input_", "\n", "]", "\n", "", "else", ":", "\n", "                    ", "estimator_inputs", "[", "estimator_name", "]", "[", "input_", "]", "=", "None", "\n", "", "", "estimator_inputs", "[", "estimator_name", "]", "[", "\"action_dist\"", "]", "=", "action_dist", "\n", "estimator_inputs", "=", "self", ".", "_preprocess_model_based_input", "(", "\n", "estimator_inputs", "=", "estimator_inputs", ",", "\n", "estimator_name", "=", "estimator_name", ",", "\n", "model_based_input", "=", "{", "\n", "\"estimated_rewards_by_reg_model\"", ":", "estimated_rewards_by_reg_model", ",", "\n", "\"estimated_pscore\"", ":", "estimated_pscore", ",", "\n", "\"estimated_pscore_avg\"", ":", "estimated_pscore_avg", ",", "\n", "}", ",", "\n", ")", "\n", "", "return", "estimator_inputs", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta_multi.MultiLoggersOffPolicyEvaluation.estimate_policy_values": [[201, 257], ["dict", "meta_multi.MultiLoggersOffPolicyEvaluation._create_estimator_inputs", "meta_multi.MultiLoggersOffPolicyEvaluation.ope_estimators_.items", "estimator.estimate_policy_value", "ValueError"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta.OffPolicyEvaluation._create_estimator_inputs", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_policy_value"], ["", "def", "estimate_policy_values", "(", "\n", "self", ",", "\n", "action_dist", ":", "np", ".", "ndarray", ",", "\n", "estimated_rewards_by_reg_model", ":", "Optional", "[", "\n", "Union", "[", "np", ".", "ndarray", ",", "Dict", "[", "str", ",", "np", ".", "ndarray", "]", "]", "\n", "]", "=", "None", ",", "\n", "estimated_pscore", ":", "Optional", "[", "Union", "[", "np", ".", "ndarray", ",", "Dict", "[", "str", ",", "np", ".", "ndarray", "]", "]", "]", "=", "None", ",", "\n", "estimated_pscore_avg", ":", "Optional", "[", "Union", "[", "np", ".", "ndarray", ",", "Dict", "[", "str", ",", "np", ".", "ndarray", "]", "]", "]", "=", "None", ",", "\n", ")", "->", "Dict", "[", "str", ",", "float", "]", ":", "\n", "        ", "\"\"\"Estimate the policy value of evaluation policy.\n\n        Parameters\n        ------------\n        action_dist: array-like, shape (n_rounds, n_actions, len_list)\n            Action choice probabilities of the evaluation policy (can be deterministic), i.e., :math:`\\\\pi_e(a_i|x_i)`.\n\n        estimated_rewards_by_reg_model: array-like, shape (n_rounds, n_actions, len_list) or Dict[str, array-like], default=None\n            Expected rewards given each round, action, and position estimated by regression model, i.e., :math:`\\\\hat{q}(x_i,a_i)`.\n            When an array-like is given, all OPE estimators use it.\n            When a dict with an estimator's name as its key is given, the corresponding value is used for the estimator.\n            If None, model-dependent estimators such as DM and DR cannot be used.\n\n        estimated_pscore: array-like, shape (n_rounds,), default=None\n            Estimated behavior policy (propensity scores), i.e., :math:`\\\\hat{\\\\pi}_k(a_i|x_i)`.\n            When an array-like is given, all OPE estimators use it.\n            When a dict with an estimator's name as its key is given, the corresponding value is used for the estimator.\n\n        estimated_pscore_avg: array-like, shape (n_rounds,), default=None\n            Estimated average behavior policy (propensity scores), i.e., :math:`\\\\hat{\\\\pi}_{avg}(a_i|x_i)`.\n            When an array-like is given, all OPE estimators use it.\n            When a dict with an estimator's name as its key is given, the corresponding value is used for the estimator.\n\n        Returns\n        ----------\n        policy_value_dict: Dict[str, float]\n            Dictionary containing the policy values estimated by OPE estimators.\n\n        \"\"\"", "\n", "if", "self", ".", "is_model_dependent", ":", "\n", "            ", "if", "estimated_rewards_by_reg_model", "is", "None", ":", "\n", "                ", "raise", "ValueError", "(", "\n", "\"When model dependent estimators such as DM or DR are used, `estimated_rewards_by_reg_model` must be given\"", "\n", ")", "\n", "\n", "", "", "policy_value_dict", "=", "dict", "(", ")", "\n", "estimator_inputs", "=", "self", ".", "_create_estimator_inputs", "(", "\n", "action_dist", "=", "action_dist", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", "estimated_pscore", "=", "estimated_pscore", ",", "\n", "estimated_pscore_avg", "=", "estimated_pscore_avg", ",", "\n", ")", "\n", "for", "estimator_name", ",", "estimator", "in", "self", ".", "ope_estimators_", ".", "items", "(", ")", ":", "\n", "            ", "policy_value_dict", "[", "estimator_name", "]", "=", "estimator", ".", "estimate_policy_value", "(", "\n", "**", "estimator_inputs", "[", "estimator_name", "]", "\n", ")", "\n", "", "return", "policy_value_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta_multi.MultiLoggersOffPolicyEvaluation.estimate_intervals": [[258, 335], ["utils.check_confidence_interval_arguments", "dict", "meta_multi.MultiLoggersOffPolicyEvaluation._create_estimator_inputs", "meta_multi.MultiLoggersOffPolicyEvaluation.ope_estimators_.items", "estimator.estimate_interval", "ValueError"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_confidence_interval_arguments", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta.OffPolicyEvaluation._create_estimator_inputs", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_interval"], ["", "def", "estimate_intervals", "(", "\n", "self", ",", "\n", "action_dist", ":", "np", ".", "ndarray", ",", "\n", "estimated_rewards_by_reg_model", ":", "Optional", "[", "\n", "Union", "[", "np", ".", "ndarray", ",", "Dict", "[", "str", ",", "np", ".", "ndarray", "]", "]", "\n", "]", "=", "None", ",", "\n", "estimated_pscore", ":", "Optional", "[", "Union", "[", "np", ".", "ndarray", ",", "Dict", "[", "str", ",", "np", ".", "ndarray", "]", "]", "]", "=", "None", ",", "\n", "estimated_pscore_avg", ":", "Optional", "[", "Union", "[", "np", ".", "ndarray", ",", "Dict", "[", "str", ",", "np", ".", "ndarray", "]", "]", "]", "=", "None", ",", "\n", "alpha", ":", "float", "=", "0.05", ",", "\n", "n_bootstrap_samples", ":", "int", "=", "100", ",", "\n", "random_state", ":", "Optional", "[", "int", "]", "=", "None", ",", "\n", ")", "->", "Dict", "[", "str", ",", "Dict", "[", "str", ",", "float", "]", "]", ":", "\n", "        ", "\"\"\"Estimate confidence intervals of policy values using bootstrap.\n\n        Parameters\n        ------------\n        action_dist: array-like, shape (n_rounds, n_actions, len_list)\n            Action choice probabilities of the evaluation policy (can be deterministic), i.e., :math:`\\\\pi_e(a_i|x_i)`.\n\n        estimated_rewards_by_reg_model: array-like, shape (n_rounds, n_actions, len_list) or Dict[str, array-like], default=None\n            Estimated expected rewards given context, action, and position, i.e., :math:`\\\\hat{q}(x_i,a_i)`.\n            When an array-like is given, all OPE estimators use it.\n            When a dict with an estimator's name as its key is given, the corresponding value is used for the estimator.\n            If None, model-dependent estimators such as DM and DR cannot be used.\n\n        estimated_pscore: array-like, shape (n_rounds,), default=None\n            Estimated behavior policy (propensity scores), i.e., :math:`\\\\hat{\\\\pi}_b(a_i|x_i)`.\n            When an array-like is given, all OPE estimators use it.\n            When a dict with an estimator's name as its key is given, the corresponding value is used for the estimator.\n\n        estimated_pscore_avg: array-like, shape (n_rounds,), default=None\n            Estimated average behavior policy (propensity scores), i.e., :math:`\\\\hat{\\\\pi}_{avg}(a_i|x_i)`.\n            When an array-like is given, all OPE estimators use it.\n            When a dict with an estimator's name as its key is given, the corresponding value is used for the estimator.\n\n        alpha: float, default=0.05\n            Significance level.\n\n        n_bootstrap_samples: int, default=100\n            Number of resampling performed in bootstrap sampling.\n\n        random_state: int, default=None\n            Controls the random seed in bootstrap sampling.\n\n        Returns\n        ----------\n        policy_value_interval_dict: Dict[str, Dict[str, float]]\n            Dictionary containing confidence intervals of the estimated policy values.\n\n        \"\"\"", "\n", "if", "self", ".", "is_model_dependent", ":", "\n", "            ", "if", "estimated_rewards_by_reg_model", "is", "None", ":", "\n", "                ", "raise", "ValueError", "(", "\n", "\"When model dependent estimators such as DM or DR are used, `estimated_rewards_by_reg_model` must be given\"", "\n", ")", "\n", "\n", "", "", "check_confidence_interval_arguments", "(", "\n", "alpha", "=", "alpha", ",", "\n", "n_bootstrap_samples", "=", "n_bootstrap_samples", ",", "\n", "random_state", "=", "random_state", ",", "\n", ")", "\n", "policy_value_interval_dict", "=", "dict", "(", ")", "\n", "estimator_inputs", "=", "self", ".", "_create_estimator_inputs", "(", "\n", "action_dist", "=", "action_dist", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", "estimated_pscore", "=", "estimated_pscore", ",", "\n", "estimated_pscore_avg", "=", "estimated_pscore_avg", ",", "\n", ")", "\n", "for", "estimator_name", ",", "estimator", "in", "self", ".", "ope_estimators_", ".", "items", "(", ")", ":", "\n", "            ", "policy_value_interval_dict", "[", "estimator_name", "]", "=", "estimator", ".", "estimate_interval", "(", "\n", "**", "estimator_inputs", "[", "estimator_name", "]", ",", "\n", "alpha", "=", "alpha", ",", "\n", "n_bootstrap_samples", "=", "n_bootstrap_samples", ",", "\n", "random_state", "=", "random_state", ",", "\n", ")", "\n", "\n", "", "return", "policy_value_interval_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta_multi.MultiLoggersOffPolicyEvaluation.summarize_off_policy_estimates": [[336, 418], ["pandas.DataFrame", "pandas.DataFrame", "meta_multi.MultiLoggersOffPolicyEvaluation.bandit_feedback[].mean", "meta_multi.MultiLoggersOffPolicyEvaluation.estimate_policy_values", "meta_multi.MultiLoggersOffPolicyEvaluation.estimate_intervals", "logger.warning"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta.OffPolicyEvaluation.estimate_policy_values", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta.OffPolicyEvaluation.estimate_intervals"], ["", "def", "summarize_off_policy_estimates", "(", "\n", "self", ",", "\n", "action_dist", ":", "np", ".", "ndarray", ",", "\n", "estimated_rewards_by_reg_model", ":", "Optional", "[", "\n", "Union", "[", "np", ".", "ndarray", ",", "Dict", "[", "str", ",", "np", ".", "ndarray", "]", "]", "\n", "]", "=", "None", ",", "\n", "estimated_pscore", ":", "Optional", "[", "Union", "[", "np", ".", "ndarray", ",", "Dict", "[", "str", ",", "np", ".", "ndarray", "]", "]", "]", "=", "None", ",", "\n", "estimated_pscore_avg", ":", "Optional", "[", "Union", "[", "np", ".", "ndarray", ",", "Dict", "[", "str", ",", "np", ".", "ndarray", "]", "]", "]", "=", "None", ",", "\n", "alpha", ":", "float", "=", "0.05", ",", "\n", "n_bootstrap_samples", ":", "int", "=", "100", ",", "\n", "random_state", ":", "Optional", "[", "int", "]", "=", "None", ",", "\n", ")", "->", "Tuple", "[", "DataFrame", ",", "DataFrame", "]", ":", "\n", "        ", "\"\"\"Summarize policy values and their confidence intervals estimated by OPE estimators.\n\n        Parameters\n        ------------\n        action_dist: array-like, shape (n_rounds, n_actions, len_list)\n            Action choice probabilities of the evaluation policy (can be deterministic), i.e., :math:`\\\\pi_e(a_i|x_i)`.\n\n        estimated_rewards_by_reg_model: array-like, shape (n_rounds, n_actions, len_list) or Dict[str, array-like], default=None\n            Expected rewards given each round, action, and position estimated by regression model, i.e., :math:`\\\\hat{q}(x_i,a_i)`.\n            When an array-like is given, all OPE estimators use it.\n            When a dict with an estimator's name as its key is given, the corresponding value is used for the estimator.\n            If None, model-dependent estimators such as DM and DR cannot be used.\n\n        estimated_pscore: array-like, shape (n_rounds,), default=None\n            Estimated behavior policy (propensity scores), i.e., :math:`\\\\hat{\\\\pi}_b(a_i|x_i)`.\n            When an array-like is given, all OPE estimators use it.\n            When a dict with an estimator's name as its key is given, the corresponding value is used for the estimator.\n\n        estimated_pscore_avg: array-like, shape (n_rounds,), default=None\n            Estimated average behavior policy (propensity scores), i.e., :math:`\\\\hat{\\\\pi}_{avg}(a_i|x_i)`.\n            When an array-like is given, all OPE estimators use it.\n            When a dict with an estimator's name as its key is given, the corresponding value is used for the estimator.\n\n        alpha: float, default=0.05\n            Significance level.\n\n        n_bootstrap_samples: int, default=100\n            Number of resampling performed in bootstrap sampling.\n\n        random_state: int, default=None\n            Controls the random seed in bootstrap sampling.\n\n        Returns\n        ----------\n        (policy_value_df, policy_value_interval_df): Tuple[DataFrame, DataFrame]\n            Policy values and their confidence intervals estimated by OPE estimators.\n\n        \"\"\"", "\n", "policy_value_df", "=", "DataFrame", "(", "\n", "self", ".", "estimate_policy_values", "(", "\n", "action_dist", "=", "action_dist", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", "estimated_pscore", "=", "estimated_pscore", ",", "\n", "estimated_pscore_avg", "=", "estimated_pscore_avg", ",", "\n", ")", ",", "\n", "index", "=", "[", "\"estimated_policy_value\"", "]", ",", "\n", ")", "\n", "policy_value_interval_df", "=", "DataFrame", "(", "\n", "self", ".", "estimate_intervals", "(", "\n", "action_dist", "=", "action_dist", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", "estimated_pscore", "=", "estimated_pscore", ",", "\n", "estimated_pscore_avg", "=", "estimated_pscore_avg", ",", "\n", "alpha", "=", "alpha", ",", "\n", "n_bootstrap_samples", "=", "n_bootstrap_samples", ",", "\n", "random_state", "=", "random_state", ",", "\n", ")", "\n", ")", "\n", "policy_value_of_behavior_policy", "=", "self", ".", "bandit_feedback", "[", "\"reward\"", "]", ".", "mean", "(", ")", "\n", "policy_value_df", "=", "policy_value_df", ".", "T", "\n", "if", "policy_value_of_behavior_policy", "<=", "0", ":", "\n", "            ", "logger", ".", "warning", "(", "\n", "f\"Policy value of the behavior policy is {policy_value_of_behavior_policy} (<=0); relative estimated policy value is set to np.nan\"", "\n", ")", "\n", "policy_value_df", "[", "\"relative_estimated_policy_value\"", "]", "=", "np", ".", "nan", "\n", "", "else", ":", "\n", "            ", "policy_value_df", "[", "\"relative_estimated_policy_value\"", "]", "=", "(", "\n", "policy_value_df", ".", "estimated_policy_value", "/", "policy_value_of_behavior_policy", "\n", ")", "\n", "", "return", "policy_value_df", ",", "policy_value_interval_df", ".", "T", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta_multi.MultiLoggersOffPolicyEvaluation.visualize_off_policy_estimates": [[419, 520], ["dict", "meta_multi.MultiLoggersOffPolicyEvaluation._create_estimator_inputs", "meta_multi.MultiLoggersOffPolicyEvaluation.ope_estimators_.items", "pandas.DataFrame", "pandas.DataFrame.rename", "matplotlib.style.use", "matplotlib.subplots", "seaborn.barplot", "matplotlib.xlabel", "matplotlib.ylabel", "matplotlib.yticks", "matplotlib.xticks", "isinstance", "isinstance", "estimator._estimate_round_rewards", "meta_multi.MultiLoggersOffPolicyEvaluation.bandit_feedback[].mean", "fig.savefig", "str", "key.upper", "numpy.int32", "dict.keys", "len"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta.OffPolicyEvaluation._create_estimator_inputs", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SelfNormalizedSlateStandardIPS._estimate_round_rewards"], ["", "def", "visualize_off_policy_estimates", "(", "\n", "self", ",", "\n", "action_dist", ":", "np", ".", "ndarray", ",", "\n", "estimated_rewards_by_reg_model", ":", "Optional", "[", "\n", "Union", "[", "np", ".", "ndarray", ",", "Dict", "[", "str", ",", "np", ".", "ndarray", "]", "]", "\n", "]", "=", "None", ",", "\n", "estimated_pscore", ":", "Optional", "[", "Union", "[", "np", ".", "ndarray", ",", "Dict", "[", "str", ",", "np", ".", "ndarray", "]", "]", "]", "=", "None", ",", "\n", "estimated_pscore_avg", ":", "Optional", "[", "Union", "[", "np", ".", "ndarray", ",", "Dict", "[", "str", ",", "np", ".", "ndarray", "]", "]", "]", "=", "None", ",", "\n", "alpha", ":", "float", "=", "0.05", ",", "\n", "is_relative", ":", "bool", "=", "False", ",", "\n", "n_bootstrap_samples", ":", "int", "=", "100", ",", "\n", "random_state", ":", "Optional", "[", "int", "]", "=", "None", ",", "\n", "fig_dir", ":", "Optional", "[", "Path", "]", "=", "None", ",", "\n", "fig_name", ":", "str", "=", "\"estimated_policy_value.png\"", ",", "\n", ")", "->", "None", ":", "\n", "        ", "\"\"\"Visualize the estimated policy values.\n\n        Parameters\n        ----------\n        action_dist: array-like, shape (n_rounds, n_actions, len_list)\n            Action choice probabilities of the evaluation policy (can be deterministic), i.e., :math:`\\\\pi_e(a_i|x_i)`.\n\n        estimated_rewards_by_reg_model: array-like, shape (n_rounds, n_actions, len_list) or Dict[str, array-like], default=None\n            Estimated expected rewards given context, action, and position, i.e., :math:`\\\\hat{q}(x_i,a_i)`.\n            When an array-like is given, all OPE estimators use it.\n            When a dict with an estimator's name as its key is given, the corresponding value is used for the estimator.\n            If None, model-dependent estimators such as DM and DR cannot be used.\n\n        estimated_pscore: array-like, shape (n_rounds,), default=None\n            Estimated behavior policy (propensity scores), i.e., :math:`\\\\hat{\\\\pi}_b(a_i|x_i)`.\n            When an array-like is given, all OPE estimators use it.\n            When a dict with an estimator's name as its key is given, the corresponding value is used for the estimator.\n\n        estimated_pscore_avg: array-like, shape (n_rounds,), default=None\n            Estimated average behavior policy (propensity scores), i.e., :math:`\\\\hat{\\\\pi}_{avg}(a_i|x_i)`.\n            When an array-like is given, all OPE estimators use it.\n            When a dict with an estimator's name as its key is given, the corresponding value is used for the estimator.\n\n        alpha: float, default=0.05\n            Significance level.\n\n        n_bootstrap_samples: int, default=100\n            Number of resampling performed in bootstrap sampling.\n\n        random_state: int, default=None\n            Controls the random seed in bootstrap sampling.\n\n        is_relative: bool, default=False,\n            If True, the method visualizes the estimated policy values of evaluation policy\n            relative to the ground-truth policy value of behavior policy.\n\n        fig_dir: Path, default=None\n            Path to store the bar figure.\n            If None, the figure will not be saved.\n\n        fig_name: str, default=\"estimated_policy_value.png\"\n            Name of the bar figure.\n\n        \"\"\"", "\n", "if", "fig_dir", "is", "not", "None", ":", "\n", "            ", "assert", "isinstance", "(", "fig_dir", ",", "Path", ")", ",", "\"`fig_dir` must be a Path\"", "\n", "", "if", "fig_name", "is", "not", "None", ":", "\n", "            ", "assert", "isinstance", "(", "fig_name", ",", "str", ")", ",", "\"`fig_dir` must be a string\"", "\n", "\n", "", "estimated_round_rewards_dict", "=", "dict", "(", ")", "\n", "estimator_inputs", "=", "self", ".", "_create_estimator_inputs", "(", "\n", "action_dist", "=", "action_dist", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", "estimated_pscore", "=", "estimated_pscore", ",", "\n", "estimated_pscore_avg", "=", "estimated_pscore_avg", ",", "\n", ")", "\n", "for", "estimator_name", ",", "estimator", "in", "self", ".", "ope_estimators_", ".", "items", "(", ")", ":", "\n", "            ", "estimated_round_rewards_dict", "[", "\n", "estimator_name", "\n", "]", "=", "estimator", ".", "_estimate_round_rewards", "(", "**", "estimator_inputs", "[", "estimator_name", "]", ")", "\n", "", "estimated_round_rewards_df", "=", "DataFrame", "(", "estimated_round_rewards_dict", ")", "\n", "estimated_round_rewards_df", ".", "rename", "(", "\n", "columns", "=", "{", "key", ":", "key", ".", "upper", "(", ")", "for", "key", "in", "estimated_round_rewards_dict", ".", "keys", "(", ")", "}", ",", "\n", "inplace", "=", "True", ",", "\n", ")", "\n", "if", "is_relative", ":", "\n", "            ", "estimated_round_rewards_df", "/=", "self", ".", "bandit_feedback", "[", "\"reward\"", "]", ".", "mean", "(", ")", "\n", "\n", "", "plt", ".", "style", ".", "use", "(", "\"ggplot\"", ")", "\n", "fig", ",", "ax", "=", "plt", ".", "subplots", "(", "figsize", "=", "(", "8", ",", "6", ")", ")", "\n", "sns", ".", "barplot", "(", "\n", "data", "=", "estimated_round_rewards_df", ",", "\n", "ax", "=", "ax", ",", "\n", "ci", "=", "100", "*", "(", "1", "-", "alpha", ")", ",", "\n", "n_boot", "=", "n_bootstrap_samples", ",", "\n", "seed", "=", "random_state", ",", "\n", ")", "\n", "plt", ".", "xlabel", "(", "\"OPE Estimators\"", ",", "fontsize", "=", "25", ")", "\n", "plt", ".", "ylabel", "(", "\n", "f\"Estimated Policy Value (\u00b1 {np.int32(100*(1 - alpha))}% CI)\"", ",", "fontsize", "=", "20", "\n", ")", "\n", "plt", ".", "yticks", "(", "fontsize", "=", "15", ")", "\n", "plt", ".", "xticks", "(", "fontsize", "=", "25", "-", "2", "*", "len", "(", "self", ".", "ope_estimators", ")", ")", "\n", "\n", "if", "fig_dir", ":", "\n", "            ", "fig", ".", "savefig", "(", "str", "(", "fig_dir", "/", "fig_name", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta_multi.MultiLoggersOffPolicyEvaluation.evaluate_performance_of_estimators": [[521, 617], ["sklearn.utils.check_scalar", "dict", "meta_multi.MultiLoggersOffPolicyEvaluation._create_estimator_inputs", "meta_multi.MultiLoggersOffPolicyEvaluation.ope_estimators_.items", "ValueError", "ValueError", "estimator.estimate_policy_value", "numpy.abs"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta.OffPolicyEvaluation._create_estimator_inputs", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_policy_value"], ["", "", "def", "evaluate_performance_of_estimators", "(", "\n", "self", ",", "\n", "ground_truth_policy_value", ":", "float", ",", "\n", "action_dist", ":", "np", ".", "ndarray", ",", "\n", "estimated_rewards_by_reg_model", ":", "Optional", "[", "\n", "Union", "[", "np", ".", "ndarray", ",", "Dict", "[", "str", ",", "np", ".", "ndarray", "]", "]", "\n", "]", "=", "None", ",", "\n", "estimated_pscore", ":", "Optional", "[", "Union", "[", "np", ".", "ndarray", ",", "Dict", "[", "str", ",", "np", ".", "ndarray", "]", "]", "]", "=", "None", ",", "\n", "estimated_pscore_avg", ":", "Optional", "[", "Union", "[", "np", ".", "ndarray", ",", "Dict", "[", "str", ",", "np", ".", "ndarray", "]", "]", "]", "=", "None", ",", "\n", "metric", ":", "str", "=", "\"se\"", ",", "\n", ")", "->", "Dict", "[", "str", ",", "float", "]", ":", "\n", "        ", "\"\"\"Evaluate the accuracy of OPE estimators.\n\n        Note\n        ------\n        Evaluate the estimation performance of OPE estimators with relative estimation error (relative-EE) or squared error (SE):\n\n        .. math ::\n\n            \\\\text{Relative-EE} (\\\\hat{V}; \\\\mathcal{D}) = \\\\left|  \\\\frac{\\\\hat{V}(\\\\pi; \\\\mathcal{D}) - V(\\\\pi)}{V(\\\\pi)} \\\\right|,\n\n        .. math ::\n\n            \\\\text{SE} (\\\\hat{V}; \\\\mathcal{D}) = \\\\left(\\\\hat{V}(\\\\pi; \\\\mathcal{D}) - V(\\\\pi) \\\\right)^2,\n\n        where :math:`V({\\\\pi})` is the ground-truth policy value of the evalation policy :math:`\\\\pi_e` (often estimated using on-policy estimation).\n        :math:`\\\\hat{V}(\\\\pi; \\\\mathcal{D})` is the policy value estimated by an OPE estimator :math:`\\\\hat{V}` and logged bandit feedback :math:`\\\\mathcal{D}`.\n\n        Parameters\n        ----------\n        ground_truth policy value: float\n            Ground_truth policy value of evaluation policy, i.e., :math:`V(\\\\pi_e)`.\n            With Open Bandit Dataset, we use an on-policy estimate of the policy value as its ground-truth.\n\n        action_dist: array-like, shape (n_rounds, n_actions, len_list)\n            Action choice probabilities of the evaluation policy (can be deterministic), i.e., :math:`\\\\pi_e(a_i|x_i)`.\n\n        estimated_rewards_by_reg_model: array-like, shape (n_rounds, n_actions, len_list) or Dict[str, array-like], default=None\n            Estimated expected rewards given context, action, and position, i.e., :math:`\\\\hat{q}(x_i,a_i)`.\n            When an array-like is given, all OPE estimators use it.\n            When a dict with an estimator's name as its key is given, the corresponding value is used for the estimator.\n            If None, model-dependent estimators such as DM and DR cannot be used.\n\n        estimated_pscore: array-like, shape (n_rounds,), default=None\n            Estimated behavior policy (propensity scores), i.e., :math:`\\\\hat{\\\\pi}_b(a_i|x_i)`.\n            When an array-like is given, all OPE estimators use it.\n            When a dict with an estimator's name as its key is given, the corresponding value is used for the estimator.\n\n        estimated_pscore_avg: array-like, shape (n_rounds,), default=None\n            Estimated average behavior policy (propensity scores), i.e., :math:`\\\\hat{\\\\pi}_{avg}(a_i|x_i)`.\n            When an array-like is given, all OPE estimators use it.\n            When a dict with an estimator's name as its key is given, the corresponding value is used for the estimator.\n\n        metric: str, default=\"se\"\n            Evaluation metric used to evaluate and compare the estimation performance of OPE estimators.\n            Must be either \"relative-ee\" or \"se\".\n\n        Returns\n        ----------\n        eval_metric_ope_dict: Dict[str, float]\n            Dictionary containing the value of evaluation metric for the estimation performance of OPE estimators.\n\n        \"\"\"", "\n", "check_scalar", "(", "\n", "ground_truth_policy_value", ",", "\n", "\"ground_truth_policy_value\"", ",", "\n", "float", ",", "\n", ")", "\n", "if", "metric", "not", "in", "[", "\"relative-ee\"", ",", "\"se\"", "]", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "f\"`metric` must be either 'relative-ee' or 'se', but {metric} is given\"", "\n", ")", "\n", "", "if", "metric", "==", "\"relative-ee\"", "and", "ground_truth_policy_value", "==", "0.0", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"`ground_truth_policy_value` must be non-zero when metric is relative-ee\"", "\n", ")", "\n", "\n", "", "eval_metric_ope_dict", "=", "dict", "(", ")", "\n", "estimator_inputs", "=", "self", ".", "_create_estimator_inputs", "(", "\n", "action_dist", "=", "action_dist", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", "estimated_pscore", "=", "estimated_pscore", ",", "\n", "estimated_pscore_avg", "=", "estimated_pscore_avg", ",", "\n", ")", "\n", "for", "estimator_name", ",", "estimator", "in", "self", ".", "ope_estimators_", ".", "items", "(", ")", ":", "\n", "            ", "estimated_policy_value", "=", "estimator", ".", "estimate_policy_value", "(", "\n", "**", "estimator_inputs", "[", "estimator_name", "]", "\n", ")", "\n", "if", "metric", "==", "\"relative-ee\"", ":", "\n", "                ", "relative_ee_", "=", "estimated_policy_value", "-", "ground_truth_policy_value", "\n", "relative_ee_", "/=", "ground_truth_policy_value", "\n", "eval_metric_ope_dict", "[", "estimator_name", "]", "=", "np", ".", "abs", "(", "relative_ee_", ")", "\n", "", "elif", "metric", "==", "\"se\"", ":", "\n", "                ", "se_", "=", "(", "estimated_policy_value", "-", "ground_truth_policy_value", ")", "**", "2", "\n", "eval_metric_ope_dict", "[", "estimator_name", "]", "=", "se_", "\n", "", "", "return", "eval_metric_ope_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta_multi.MultiLoggersOffPolicyEvaluation.summarize_estimators_comparison": [[618, 676], ["pandas.DataFrame", "meta_multi.MultiLoggersOffPolicyEvaluation.evaluate_performance_of_estimators"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta.OffPolicyEvaluation.evaluate_performance_of_estimators"], ["", "def", "summarize_estimators_comparison", "(", "\n", "self", ",", "\n", "ground_truth_policy_value", ":", "float", ",", "\n", "action_dist", ":", "np", ".", "ndarray", ",", "\n", "estimated_rewards_by_reg_model", ":", "Optional", "[", "\n", "Union", "[", "np", ".", "ndarray", ",", "Dict", "[", "str", ",", "np", ".", "ndarray", "]", "]", "\n", "]", "=", "None", ",", "\n", "estimated_pscore", ":", "Optional", "[", "Union", "[", "np", ".", "ndarray", ",", "Dict", "[", "str", ",", "np", ".", "ndarray", "]", "]", "]", "=", "None", ",", "\n", "estimated_pscore_avg", ":", "Optional", "[", "Union", "[", "np", ".", "ndarray", ",", "Dict", "[", "str", ",", "np", ".", "ndarray", "]", "]", "]", "=", "None", ",", "\n", "metric", ":", "str", "=", "\"se\"", ",", "\n", ")", "->", "DataFrame", ":", "\n", "        ", "\"\"\"Summarize the performance comparison among OPE estimators.\n\n        Parameters\n        ----------\n        ground_truth policy value: float\n            Ground_truth policy value of evaluation policy, i.e., :math:`V(\\\\pi_e)`.\n            With Open Bandit Dataset, we use an on-policy estimate of the policy value as ground-truth.\n\n        action_dist: array-like, shape (n_rounds, n_actions, len_list)\n            Action choice probabilities of the evaluation policy (can be deterministic), i.e., :math:`\\\\pi_e(a_i|x_i)`.\n\n        estimated_rewards_by_reg_model: array-like, shape (n_rounds, n_actions, len_list), default=None\n            Estimated expected rewards given context, action, and position, i.e., :math:`\\\\hat{q}(x_i,a_i)`.\n            If None, model-dependent estimators such as DM and DR cannot be used.\n\n        estimated_pscore: array-like, shape (n_rounds,), default=None\n            Estimated behavior policy (propensity scores), i.e., :math:`\\\\hat{\\\\pi}_b(a_i|x_i)`.\n            When an array-like is given, all OPE estimators use it.\n            When a dict with an estimator's name as its key is given, the corresponding value is used for the estimator.\n\n        estimated_pscore_avg: array-like, shape (n_rounds,), default=None\n            Estimated average behavior policy (propensity scores), i.e., :math:`\\\\hat{\\\\pi}_{avg}(a_i|x_i)`.\n            When an array-like is given, all OPE estimators use it.\n            When a dict with an estimator's name as its key is given, the corresponding value is used for the estimator.\n\n        metric: str, default=\"se\"\n            Evaluation metric used to evaluate and compare the estimation performance of OPE estimators.\n            Must be either \"relative-ee\" or \"se\".\n\n        Returns\n        ----------\n        eval_metric_ope_df: DataFrame\n            Results of performance comparison among OPE estimators.\n\n        \"\"\"", "\n", "eval_metric_ope_df", "=", "DataFrame", "(", "\n", "self", ".", "evaluate_performance_of_estimators", "(", "\n", "ground_truth_policy_value", "=", "ground_truth_policy_value", ",", "\n", "action_dist", "=", "action_dist", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", "estimated_pscore", "=", "estimated_pscore", ",", "\n", "estimated_pscore_avg", "=", "estimated_pscore_avg", ",", "\n", "metric", "=", "metric", ",", "\n", ")", ",", "\n", "index", "=", "[", "metric", "]", ",", "\n", ")", "\n", "return", "eval_metric_ope_df", ".", "T", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta_multi.MultiLoggersOffPolicyEvaluation.visualize_off_policy_estimates_of_multiple_policies": [[677, 795], ["zip", "matplotlib.style.use", "matplotlib.figure", "enumerate", "len", "len", "ValueError", "isinstance", "isinstance", "meta_multi.MultiLoggersOffPolicyEvaluation._create_estimator_inputs", "meta_multi.MultiLoggersOffPolicyEvaluation.ope_estimators_.items", "pandas.DataFrame", "matplotlib.figure.add_subplot", "seaborn.barplot", "plt.figure.add_subplot.set_title", "plt.figure.add_subplot.set_ylabel", "matplotlib.yticks", "matplotlib.xticks", "matplotlib.figure.savefig", "estimator._estimate_round_rewards", "meta_multi.MultiLoggersOffPolicyEvaluation.bandit_feedback[].mean", "len", "estimator_name.upper", "str", "len", "numpy.int32", "len"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta.OffPolicyEvaluation._create_estimator_inputs", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SelfNormalizedSlateStandardIPS._estimate_round_rewards"], ["", "def", "visualize_off_policy_estimates_of_multiple_policies", "(", "\n", "self", ",", "\n", "policy_name_list", ":", "List", "[", "str", "]", ",", "\n", "action_dist_list", ":", "List", "[", "np", ".", "ndarray", "]", ",", "\n", "estimated_rewards_by_reg_model", ":", "Optional", "[", "\n", "Union", "[", "np", ".", "ndarray", ",", "Dict", "[", "str", ",", "np", ".", "ndarray", "]", "]", "\n", "]", "=", "None", ",", "\n", "estimated_pscore", ":", "Optional", "[", "Union", "[", "np", ".", "ndarray", ",", "Dict", "[", "str", ",", "np", ".", "ndarray", "]", "]", "]", "=", "None", ",", "\n", "estimated_pscore_avg", ":", "Optional", "[", "Union", "[", "np", ".", "ndarray", ",", "Dict", "[", "str", ",", "np", ".", "ndarray", "]", "]", "]", "=", "None", ",", "\n", "alpha", ":", "float", "=", "0.05", ",", "\n", "is_relative", ":", "bool", "=", "False", ",", "\n", "n_bootstrap_samples", ":", "int", "=", "100", ",", "\n", "random_state", ":", "Optional", "[", "int", "]", "=", "None", ",", "\n", "fig_dir", ":", "Optional", "[", "Path", "]", "=", "None", ",", "\n", "fig_name", ":", "str", "=", "\"estimated_policy_value.png\"", ",", "\n", ")", "->", "None", ":", "\n", "        ", "\"\"\"Visualize the estimated policy values.\n\n        Parameters\n        ----------\n        policy_name_list: List[str]\n            List of the names of evaluation policies.\n\n        action_dist_list: List[array-like, shape (n_rounds, n_actions, len_list)]\n            List of action choice probabilities of the evaluation policies (can be deterministic), i.e., :math:`\\\\pi_e(a_i|x_i)`.\n\n        estimated_rewards_by_reg_model: array-like, shape (n_rounds, n_actions, len_list) or Dict[str, array-like], default=None\n            Estimated expected rewards given context, action, and position, i.e., :math:`\\\\hat{q}(x_i,a_i)`.\n            When an array-like is given, all OPE estimators use it.\n            When a dict is given, if the dict has the name of an estimator as a key, the corresponding value is used.\n            If None, model-dependent estimators such as DM and DR cannot be used.\n\n        estimated_pscore: array-like, shape (n_rounds,), default=None\n            Estimated behavior policy (propensity scores), i.e., :math:`\\\\hat{\\\\pi}_b(a_i|x_i)`.\n            When an array-like is given, all OPE estimators use it.\n            When a dict with an estimator's name as its key is given, the corresponding value is used for the estimator.\n\n        estimated_pscore_avg: array-like, shape (n_rounds,), default=None\n            Estimated average behavior policy (propensity scores), i.e., :math:`\\\\hat{\\\\pi}_{avg}(a_i|x_i)`.\n            When an array-like is given, all OPE estimators use it.\n            When a dict with an estimator's name as its key is given, the corresponding value is used for the estimator.\n\n        alpha: float, default=0.05\n            Significance level.\n\n        n_bootstrap_samples: int, default=100\n            Number of resampling performed in bootstrap sampling.\n\n        random_state: int, default=None\n            Controls the random seed in bootstrap sampling.\n\n        is_relative: bool, default=False,\n            If True, the method visualizes the estimated policy values of evaluation policy\n            relative to the ground-truth policy value of behavior policy.\n\n        fig_dir: Path, default=None\n            Path to store the bar figure.\n            If None, the figure will not be saved.\n\n        fig_name: str, default=\"estimated_policy_value.png\"\n            Name of the bar figure.\n\n        \"\"\"", "\n", "if", "len", "(", "policy_name_list", ")", "!=", "len", "(", "action_dist_list", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"the length of `policy_name_list` must be the same as `action_dist_list`\"", "\n", ")", "\n", "", "if", "fig_dir", "is", "not", "None", ":", "\n", "            ", "assert", "isinstance", "(", "fig_dir", ",", "Path", ")", ",", "\"`fig_dir` must be a Path\"", "\n", "", "if", "fig_name", "is", "not", "None", ":", "\n", "            ", "assert", "isinstance", "(", "fig_name", ",", "str", ")", ",", "\"`fig_dir` must be a string\"", "\n", "\n", "", "estimated_round_rewards_dict", "=", "{", "\n", "estimator_name", ":", "{", "}", "for", "estimator_name", "in", "self", ".", "ope_estimators_", "\n", "}", "\n", "\n", "for", "policy_name", ",", "action_dist", "in", "zip", "(", "policy_name_list", ",", "action_dist_list", ")", ":", "\n", "            ", "estimator_inputs", "=", "self", ".", "_create_estimator_inputs", "(", "\n", "action_dist", "=", "action_dist", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", "estimated_pscore", "=", "estimated_pscore", ",", "\n", "estimated_pscore_avg", "=", "estimated_pscore_avg", ",", "\n", ")", "\n", "for", "estimator_name", ",", "estimator", "in", "self", ".", "ope_estimators_", ".", "items", "(", ")", ":", "\n", "                ", "estimated_round_rewards_dict", "[", "estimator_name", "]", "[", "\n", "policy_name", "\n", "]", "=", "estimator", ".", "_estimate_round_rewards", "(", "\n", "**", "estimator_inputs", "[", "estimator_name", "]", "\n", ")", "\n", "\n", "", "", "plt", ".", "style", ".", "use", "(", "\"ggplot\"", ")", "\n", "fig", "=", "plt", ".", "figure", "(", "figsize", "=", "(", "8", ",", "6.2", "*", "len", "(", "self", ".", "ope_estimators_", ")", ")", ")", "\n", "\n", "for", "i", ",", "estimator_name", "in", "enumerate", "(", "self", ".", "ope_estimators_", ")", ":", "\n", "            ", "estimated_round_rewards_df", "=", "DataFrame", "(", "\n", "estimated_round_rewards_dict", "[", "estimator_name", "]", "\n", ")", "\n", "if", "is_relative", ":", "\n", "                ", "estimated_round_rewards_df", "/=", "self", ".", "bandit_feedback", "[", "\"reward\"", "]", ".", "mean", "(", ")", "\n", "\n", "", "ax", "=", "fig", ".", "add_subplot", "(", "len", "(", "action_dist_list", ")", ",", "1", ",", "i", "+", "1", ")", "\n", "sns", ".", "barplot", "(", "\n", "data", "=", "estimated_round_rewards_df", ",", "\n", "ax", "=", "ax", ",", "\n", "ci", "=", "100", "*", "(", "1", "-", "alpha", ")", ",", "\n", "n_boot", "=", "n_bootstrap_samples", ",", "\n", "seed", "=", "random_state", ",", "\n", ")", "\n", "ax", ".", "set_title", "(", "estimator_name", ".", "upper", "(", ")", ",", "fontsize", "=", "20", ")", "\n", "ax", ".", "set_ylabel", "(", "\n", "f\"Estimated Policy Value (\u00b1 {np.int32(100*(1 - alpha))}% CI)\"", ",", "\n", "fontsize", "=", "20", ",", "\n", ")", "\n", "plt", ".", "yticks", "(", "fontsize", "=", "15", ")", "\n", "plt", ".", "xticks", "(", "fontsize", "=", "25", "-", "2", "*", "len", "(", "policy_name_list", ")", ")", "\n", "\n", "", "if", "fig_dir", ":", "\n", "            ", "fig", ".", "savefig", "(", "str", "(", "fig_dir", "/", "fig_name", ")", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta.OffPolicyEvaluation.__post_init__": [[90, 101], ["dict", "RuntimeError", "isinstance", "isinstance"], "methods", ["None"], ["def", "__post_init__", "(", "self", ")", "->", "None", ":", "\n", "        ", "\"\"\"Initialize class.\"\"\"", "\n", "for", "key_", "in", "[", "\"action\"", ",", "\"position\"", ",", "\"reward\"", ",", "\"context\"", "]", ":", "\n", "            ", "if", "key_", "not", "in", "self", ".", "bandit_feedback", ":", "\n", "                ", "raise", "RuntimeError", "(", "f\"Missing key of {key_} in 'bandit_feedback'.\"", ")", "\n", "", "", "self", ".", "ope_estimators_", "=", "dict", "(", ")", "\n", "self", ".", "is_model_dependent", "=", "False", "\n", "for", "estimator", "in", "self", ".", "ope_estimators", ":", "\n", "            ", "self", ".", "ope_estimators_", "[", "estimator", ".", "estimator_name", "]", "=", "estimator", "\n", "if", "isinstance", "(", "estimator", ",", "DM", ")", "or", "isinstance", "(", "estimator", ",", "DR", ")", ":", "\n", "                ", "self", ".", "is_model_dependent", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta.OffPolicyEvaluation._create_estimator_inputs": [[102, 221], ["utils.check_array", "isinstance", "meta.OffPolicyEvaluation._preprocess_model_based_input", "estimated_rewards_by_reg_model.items", "utils.check_array", "isinstance", "utils.check_array", "ValueError", "value_or_dict.items", "utils.check_array", "ValueError", "utils.check_array", "ValueError", "ValueError", "ValueError", "ValueError"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta.OffPolicyEvaluation._preprocess_model_based_input", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array"], ["", "", "", "def", "_create_estimator_inputs", "(", "\n", "self", ",", "\n", "action_dist", ":", "np", ".", "ndarray", ",", "\n", "estimated_rewards_by_reg_model", ":", "Optional", "[", "\n", "Union", "[", "np", ".", "ndarray", ",", "Dict", "[", "str", ",", "np", ".", "ndarray", "]", "]", "\n", "]", "=", "None", ",", "\n", "estimated_pscore", ":", "Optional", "[", "Union", "[", "np", ".", "ndarray", ",", "Dict", "[", "str", ",", "np", ".", "ndarray", "]", "]", "]", "=", "None", ",", "\n", "estimated_importance_weights", ":", "Optional", "[", "\n", "Union", "[", "np", ".", "ndarray", ",", "Dict", "[", "str", ",", "np", ".", "ndarray", "]", "]", "\n", "]", "=", "None", ",", "\n", "action_embed", ":", "Optional", "[", "Union", "[", "np", ".", "ndarray", ",", "Dict", "[", "str", ",", "np", ".", "ndarray", "]", "]", "]", "=", "None", ",", "\n", "pi_b", ":", "Optional", "[", "Union", "[", "np", ".", "ndarray", ",", "Dict", "[", "str", ",", "np", ".", "ndarray", "]", "]", "]", "=", "None", ",", "\n", "p_e_a", ":", "Optional", "[", "Union", "[", "np", ".", "ndarray", ",", "Dict", "[", "str", ",", "np", ".", "ndarray", "]", "]", "]", "=", "None", ",", "\n", ")", "->", "Dict", "[", "str", ",", "Dict", "[", "str", ",", "np", ".", "ndarray", "]", "]", ":", "\n", "        ", "\"\"\"Create input dictionary to estimate policy value using subclasses of `BaseOffPolicyEstimator`\"\"\"", "\n", "check_array", "(", "array", "=", "action_dist", ",", "name", "=", "\"action_dist\"", ",", "expected_dim", "=", "3", ")", "\n", "if", "estimated_rewards_by_reg_model", "is", "None", ":", "\n", "            ", "pass", "\n", "", "elif", "isinstance", "(", "estimated_rewards_by_reg_model", ",", "dict", ")", ":", "\n", "            ", "for", "estimator_name", ",", "value", "in", "estimated_rewards_by_reg_model", ".", "items", "(", ")", ":", "\n", "                ", "check_array", "(", "\n", "array", "=", "value", ",", "\n", "name", "=", "f\"estimated_rewards_by_reg_model[{estimator_name}]\"", ",", "\n", "expected_dim", "=", "3", ",", "\n", ")", "\n", "if", "value", ".", "shape", "!=", "action_dist", ".", "shape", ":", "\n", "                    ", "raise", "ValueError", "(", "\n", "f\"Expected `estimated_rewards_by_reg_model[{estimator_name}].shape == action_dist.shape`, but found it False.\"", "\n", ")", "\n", "", "", "", "else", ":", "\n", "            ", "check_array", "(", "\n", "array", "=", "estimated_rewards_by_reg_model", ",", "\n", "name", "=", "\"estimated_rewards_by_reg_model\"", ",", "\n", "expected_dim", "=", "3", ",", "\n", ")", "\n", "if", "estimated_rewards_by_reg_model", ".", "shape", "!=", "action_dist", ".", "shape", ":", "\n", "                ", "raise", "ValueError", "(", "\n", "\"Expected `estimated_rewards_by_reg_model.shape == action_dist.shape`, but found it False\"", "\n", ")", "\n", "", "", "for", "var_name", ",", "value_or_dict", "in", "{", "\n", "\"estimated_pscore\"", ":", "estimated_pscore", ",", "\n", "\"estimated_importance_weights\"", ":", "estimated_importance_weights", ",", "\n", "\"action_embed\"", ":", "action_embed", ",", "\n", "\"pi_b\"", ":", "pi_b", ",", "\n", "\"p_e_a\"", ":", "p_e_a", ",", "\n", "}", ".", "items", "(", ")", ":", "\n", "            ", "if", "value_or_dict", "is", "None", ":", "\n", "                ", "pass", "\n", "", "elif", "isinstance", "(", "value_or_dict", ",", "dict", ")", ":", "\n", "                ", "for", "estimator_name", ",", "value", "in", "value_or_dict", ".", "items", "(", ")", ":", "\n", "                    ", "expected_dim", "=", "1", "\n", "if", "var_name", "in", "[", "\"p_e_a\"", ",", "\"pi_b\"", "]", ":", "\n", "                        ", "expected_dim", "=", "3", "\n", "", "elif", "var_name", "in", "[", "\"action_embed\"", "]", ":", "\n", "                        ", "expected_dim", "=", "2", "\n", "", "check_array", "(", "\n", "array", "=", "value", ",", "\n", "name", "=", "f\"{var_name}[{estimator_name}]\"", ",", "\n", "expected_dim", "=", "expected_dim", ",", "\n", ")", "\n", "if", "var_name", "!=", "\"p_e_a\"", ":", "\n", "                        ", "if", "value", ".", "shape", "[", "0", "]", "!=", "action_dist", ".", "shape", "[", "0", "]", ":", "\n", "                            ", "raise", "ValueError", "(", "\n", "f\"Expected `{var_name}[{estimator_name}].shape[0] == action_dist.shape[0]`, but found it False\"", "\n", ")", "\n", "", "", "else", ":", "\n", "                        ", "if", "value", ".", "shape", "[", "0", "]", "!=", "action_dist", ".", "shape", "[", "1", "]", ":", "\n", "                            ", "raise", "ValueError", "(", "\n", "f\"Expected `{var_name}[{estimator_name}].shape[0] == action_dist.shape[1]`, but found it False\"", "\n", ")", "\n", "", "", "", "", "else", ":", "\n", "                ", "expected_dim", "=", "1", "\n", "if", "var_name", "in", "[", "\"p_e_a\"", ",", "\"pi_b\"", "]", ":", "\n", "                    ", "expected_dim", "=", "3", "\n", "", "elif", "var_name", "in", "[", "\"action_embed\"", "]", ":", "\n", "                    ", "expected_dim", "=", "2", "\n", "", "check_array", "(", "\n", "array", "=", "value_or_dict", ",", "name", "=", "var_name", ",", "expected_dim", "=", "expected_dim", "\n", ")", "\n", "if", "var_name", "!=", "\"p_e_a\"", ":", "\n", "                    ", "if", "value_or_dict", ".", "shape", "[", "0", "]", "!=", "action_dist", ".", "shape", "[", "0", "]", ":", "\n", "                        ", "raise", "ValueError", "(", "\n", "f\"Expected `{var_name}.shape[0] == action_dist.shape[0]`, but found it False\"", "\n", ")", "\n", "", "", "else", ":", "\n", "                    ", "if", "value", ".", "shape", "[", "0", "]", "!=", "action_dist", ".", "shape", "[", "1", "]", ":", "\n", "                        ", "raise", "ValueError", "(", "\n", "f\"Expected `{var_name}[{estimator_name}].shape[0] == action_dist.shape[1]`, but found it False\"", "\n", ")", "\n", "\n", "", "", "", "", "estimator_inputs", "=", "{", "\n", "estimator_name", ":", "{", "\n", "input_", ":", "self", ".", "bandit_feedback", "[", "input_", "]", "\n", "for", "input_", "in", "[", "\"action\"", ",", "\"position\"", ",", "\"reward\"", ",", "\"context\"", "]", "\n", "}", "\n", "for", "estimator_name", "in", "self", ".", "ope_estimators_", "\n", "}", "\n", "\n", "for", "estimator_name", "in", "self", ".", "ope_estimators_", ":", "\n", "            ", "if", "\"pscore\"", "in", "self", ".", "bandit_feedback", ":", "\n", "                ", "estimator_inputs", "[", "estimator_name", "]", "[", "\"pscore\"", "]", "=", "self", ".", "bandit_feedback", "[", "\n", "\"pscore\"", "\n", "]", "\n", "", "else", ":", "\n", "                ", "estimator_inputs", "[", "estimator_name", "]", "[", "\"pscore\"", "]", "=", "None", "\n", "", "estimator_inputs", "[", "estimator_name", "]", "[", "\"action_dist\"", "]", "=", "action_dist", "\n", "estimator_inputs", "=", "self", ".", "_preprocess_model_based_input", "(", "\n", "estimator_inputs", "=", "estimator_inputs", ",", "\n", "estimator_name", "=", "estimator_name", ",", "\n", "model_based_input", "=", "{", "\n", "\"estimated_rewards_by_reg_model\"", ":", "estimated_rewards_by_reg_model", ",", "\n", "\"estimated_pscore\"", ":", "estimated_pscore", ",", "\n", "\"estimated_importance_weights\"", ":", "estimated_importance_weights", ",", "\n", "\"action_embed\"", ":", "action_embed", ",", "\n", "\"pi_b\"", ":", "pi_b", ",", "\n", "\"p_e_a\"", ":", "p_e_a", ",", "\n", "}", ",", "\n", ")", "\n", "", "return", "estimator_inputs", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta.OffPolicyEvaluation._preprocess_model_based_input": [[222, 241], ["model_based_input.items", "isinstance"], "methods", ["None"], ["", "def", "_preprocess_model_based_input", "(", "\n", "self", ",", "\n", "estimator_inputs", ":", "Dict", "[", "str", ",", "Optional", "[", "np", ".", "ndarray", "]", "]", ",", "\n", "estimator_name", ":", "str", ",", "\n", "model_based_input", ":", "Dict", "[", "\n", "str", ",", "Optional", "[", "Union", "[", "np", ".", "ndarray", ",", "Dict", "[", "str", ",", "np", ".", "ndarray", "]", "]", "]", "\n", "]", ",", "\n", ")", "->", "Dict", "[", "str", ",", "Optional", "[", "np", ".", "ndarray", "]", "]", ":", "\n", "        ", "for", "var_name", ",", "value_or_dict", "in", "model_based_input", ".", "items", "(", ")", ":", "\n", "            ", "if", "isinstance", "(", "value_or_dict", ",", "dict", ")", ":", "\n", "                ", "if", "estimator_name", "in", "value_or_dict", ":", "\n", "                    ", "estimator_inputs", "[", "estimator_name", "]", "[", "var_name", "]", "=", "value_or_dict", "[", "\n", "estimator_name", "\n", "]", "\n", "", "else", ":", "\n", "                    ", "estimator_inputs", "[", "estimator_name", "]", "[", "var_name", "]", "=", "None", "\n", "", "", "else", ":", "\n", "                ", "estimator_inputs", "[", "estimator_name", "]", "[", "var_name", "]", "=", "value_or_dict", "\n", "", "", "return", "estimator_inputs", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta.OffPolicyEvaluation.estimate_policy_values": [[242, 321], ["dict", "meta.OffPolicyEvaluation._create_estimator_inputs", "meta.OffPolicyEvaluation.ope_estimators_.items", "estimator.estimate_policy_value", "ValueError"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta.OffPolicyEvaluation._create_estimator_inputs", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_policy_value"], ["", "def", "estimate_policy_values", "(", "\n", "self", ",", "\n", "action_dist", ":", "np", ".", "ndarray", ",", "\n", "estimated_rewards_by_reg_model", ":", "Optional", "[", "\n", "Union", "[", "np", ".", "ndarray", ",", "Dict", "[", "str", ",", "np", ".", "ndarray", "]", "]", "\n", "]", "=", "None", ",", "\n", "estimated_pscore", ":", "Optional", "[", "Union", "[", "np", ".", "ndarray", ",", "Dict", "[", "str", ",", "np", ".", "ndarray", "]", "]", "]", "=", "None", ",", "\n", "estimated_importance_weights", ":", "Optional", "[", "\n", "Union", "[", "np", ".", "ndarray", ",", "Dict", "[", "str", ",", "np", ".", "ndarray", "]", "]", "\n", "]", "=", "None", ",", "\n", "action_embed", ":", "Optional", "[", "Union", "[", "np", ".", "ndarray", ",", "Dict", "[", "str", ",", "np", ".", "ndarray", "]", "]", "]", "=", "None", ",", "\n", "pi_b", ":", "Optional", "[", "Union", "[", "np", ".", "ndarray", ",", "Dict", "[", "str", ",", "np", ".", "ndarray", "]", "]", "]", "=", "None", ",", "\n", "p_e_a", ":", "Optional", "[", "Union", "[", "np", ".", "ndarray", ",", "Dict", "[", "str", ",", "np", ".", "ndarray", "]", "]", "]", "=", "None", ",", "\n", ")", "->", "Dict", "[", "str", ",", "float", "]", ":", "\n", "        ", "\"\"\"Estimate the policy value of evaluation policy.\n\n        Parameters\n        ------------\n        action_dist: array-like, shape (n_rounds, n_actions, len_list)\n            Action choice probabilities of the evaluation policy (can be deterministic), i.e., :math:`\\\\pi_e(a_i|x_i)`.\n\n        estimated_rewards_by_reg_model: array-like, shape (n_rounds, n_actions, len_list) or Dict[str, array-like], default=None\n            Expected rewards given each round, action, and position estimated by regression model, i.e., :math:`\\\\hat{q}(x_i,a_i)`.\n            When an array-like is given, all OPE estimators use it.\n            When a dict with an estimator's name as its key is given, the corresponding value is used for the estimator.\n            If None, model-dependent estimators such as DM and DR cannot be used.\n\n        estimated_pscore: array-like, shape (n_rounds,), default=None\n            Estimated behavior policy (propensity scores), i.e., :math:`\\\\hat{\\\\pi}_b(a_i|x_i)`.\n            When an array-like is given, all OPE estimators use it.\n            When a dict with an estimator's name as its key is given, the corresponding value is used for the estimator.\n\n        estimated_importance_weights: array-like, shape (n_rounds,)  or Dict[str, array-like], default=None\n            Importance weights estimated via supervised classification implemented by `obp.ope.ImportanceWeightEstimator`.\n            When an array-like is given, all OPE estimators use it.\n            When a dict with an estimator's name as its key is given, the corresponding value is used for the estimator.\n\n        action_embed: array-like, shape (n_rounds, dim_action_embed)\n            Context vectors characterizing actions or action embeddings such as item category information.\n            This is used to estimate the marginal importance weights.\n\n        pi_b: array-like, shape (n_rounds, n_actions, len_list)\n            Action choice probabilities of the logging/behavior policy, i.e., :math:`\\\\pi_b(a_i|x_i)`.\n\n        p_e_a: array-like, shape (n_actions, n_cat_per_dim, n_cat_dim), default=None\n            Conditional distribution of action embeddings given each action.\n            This distribution is available only when we use synthetic bandit data, i.e.,\n            `obp.dataset.SyntheticBanditDatasetWithActionEmbeds`.\n            See the output of the `obtain_batch_bandit_feedback` argument of this class.\n            If `p_e_a` is given, MIPW uses the true marginal importance weights based on this distribution.\n            The performance of MIPW with the true weights is useful in synthetic experiments of research papers.\n\n        Returns\n        ----------\n        policy_value_dict: Dict[str, float]\n            Dictionary containing the policy values estimated by OPE estimators.\n\n        \"\"\"", "\n", "if", "self", ".", "is_model_dependent", ":", "\n", "            ", "if", "estimated_rewards_by_reg_model", "is", "None", ":", "\n", "                ", "raise", "ValueError", "(", "\n", "\"When model dependent estimators such as DM or DR are used, `estimated_rewards_by_reg_model` must be given\"", "\n", ")", "\n", "\n", "", "", "policy_value_dict", "=", "dict", "(", ")", "\n", "estimator_inputs", "=", "self", ".", "_create_estimator_inputs", "(", "\n", "action_dist", "=", "action_dist", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", "estimated_pscore", "=", "estimated_pscore", ",", "\n", "estimated_importance_weights", "=", "estimated_importance_weights", ",", "\n", "action_embed", "=", "action_embed", ",", "\n", "pi_b", "=", "pi_b", ",", "\n", "p_e_a", "=", "p_e_a", ",", "\n", ")", "\n", "for", "estimator_name", ",", "estimator", "in", "self", ".", "ope_estimators_", ".", "items", "(", ")", ":", "\n", "            ", "policy_value_dict", "[", "estimator_name", "]", "=", "estimator", ".", "estimate_policy_value", "(", "\n", "**", "estimator_inputs", "[", "estimator_name", "]", "\n", ")", "\n", "", "return", "policy_value_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta.OffPolicyEvaluation.estimate_intervals": [[322, 422], ["utils.check_confidence_interval_arguments", "dict", "meta.OffPolicyEvaluation._create_estimator_inputs", "meta.OffPolicyEvaluation.ope_estimators_.items", "estimator.estimate_interval", "ValueError"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_confidence_interval_arguments", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta.OffPolicyEvaluation._create_estimator_inputs", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_interval"], ["", "def", "estimate_intervals", "(", "\n", "self", ",", "\n", "action_dist", ":", "np", ".", "ndarray", ",", "\n", "estimated_rewards_by_reg_model", ":", "Optional", "[", "\n", "Union", "[", "np", ".", "ndarray", ",", "Dict", "[", "str", ",", "np", ".", "ndarray", "]", "]", "\n", "]", "=", "None", ",", "\n", "estimated_pscore", ":", "Optional", "[", "Union", "[", "np", ".", "ndarray", ",", "Dict", "[", "str", ",", "np", ".", "ndarray", "]", "]", "]", "=", "None", ",", "\n", "estimated_importance_weights", ":", "Optional", "[", "\n", "Union", "[", "np", ".", "ndarray", ",", "Dict", "[", "str", ",", "np", ".", "ndarray", "]", "]", "\n", "]", "=", "None", ",", "\n", "action_embed", ":", "Optional", "[", "Union", "[", "np", ".", "ndarray", ",", "Dict", "[", "str", ",", "np", ".", "ndarray", "]", "]", "]", "=", "None", ",", "\n", "pi_b", ":", "Optional", "[", "Union", "[", "np", ".", "ndarray", ",", "Dict", "[", "str", ",", "np", ".", "ndarray", "]", "]", "]", "=", "None", ",", "\n", "p_e_a", ":", "Optional", "[", "Union", "[", "np", ".", "ndarray", ",", "Dict", "[", "str", ",", "np", ".", "ndarray", "]", "]", "]", "=", "None", ",", "\n", "alpha", ":", "float", "=", "0.05", ",", "\n", "n_bootstrap_samples", ":", "int", "=", "100", ",", "\n", "random_state", ":", "Optional", "[", "int", "]", "=", "None", ",", "\n", ")", "->", "Dict", "[", "str", ",", "Dict", "[", "str", ",", "float", "]", "]", ":", "\n", "        ", "\"\"\"Estimate confidence intervals of policy values using bootstrap.\n\n        Parameters\n        ------------\n        action_dist: array-like, shape (n_rounds, n_actions, len_list)\n            Action choice probabilities of the evaluation policy (can be deterministic), i.e., :math:`\\\\pi_e(a_i|x_i)`.\n\n        estimated_rewards_by_reg_model: array-like, shape (n_rounds, n_actions, len_list) or Dict[str, array-like], default=None\n            Estimated expected rewards given context, action, and position, i.e., :math:`\\\\hat{q}(x_i,a_i)`.\n            When an array-like is given, all OPE estimators use it.\n            When a dict with an estimator's name as its key is given, the corresponding value is used for the estimator.\n            If None, model-dependent estimators such as DM and DR cannot be used.\n\n        estimated_pscore: array-like, shape (n_rounds,), default=None\n            Estimated behavior policy (propensity scores), i.e., :math:`\\\\hat{\\\\pi}_b(a_i|x_i)`.\n            When an array-like is given, all OPE estimators use it.\n            When a dict with an estimator's name as its key is given, the corresponding value is used for the estimator.\n\n        estimated_importance_weights: array-like, shape (n_rounds,)  or Dict[str, array-like], default=None\n            Importance weights estimated via supervised classification implemented by `obp.ope.ImportanceWeightEstimator`.\n            When an array-like is given, all OPE estimators use it.\n            When a dict with an estimator's name as its key is given, the corresponding value is used for the estimator.\n\n        action_embed: array-like, shape (n_rounds, dim_action_embed)\n            Context vectors characterizing actions or action embeddings such as item category information.\n            This is used to estimate the marginal importance weights.\n\n        pi_b: array-like, shape (n_rounds, n_actions, len_list)\n            Action choice probabilities of the logging/behavior policy, i.e., :math:`\\\\pi_b(a_i|x_i)`.\n\n        p_e_a: array-like, shape (n_actions, n_cat_per_dim, n_cat_dim), default=None\n            Conditional distribution of action embeddings given each action.\n            This distribution is available only when we use synthetic bandit data, i.e.,\n            `obp.dataset.SyntheticBanditDatasetWithActionEmbeds`.\n            See the output of the `obtain_batch_bandit_feedback` argument of this class.\n            If `p_e_a` is given, MIPW uses the true marginal importance weights based on this distribution.\n            The performance of MIPW with the true weights is useful in synthetic experiments of research papers.\n\n        alpha: float, default=0.05\n            Significance level.\n\n        n_bootstrap_samples: int, default=100\n            Number of resampling performed in bootstrap sampling.\n\n        random_state: int, default=None\n            Controls the random seed in bootstrap sampling.\n\n        Returns\n        ----------\n        policy_value_interval_dict: Dict[str, Dict[str, float]]\n            Dictionary containing confidence intervals of the estimated policy values.\n\n        \"\"\"", "\n", "if", "self", ".", "is_model_dependent", ":", "\n", "            ", "if", "estimated_rewards_by_reg_model", "is", "None", ":", "\n", "                ", "raise", "ValueError", "(", "\n", "\"When model dependent estimators such as DM or DR are used, `estimated_rewards_by_reg_model` must be given\"", "\n", ")", "\n", "\n", "", "", "check_confidence_interval_arguments", "(", "\n", "alpha", "=", "alpha", ",", "\n", "n_bootstrap_samples", "=", "n_bootstrap_samples", ",", "\n", "random_state", "=", "random_state", ",", "\n", ")", "\n", "policy_value_interval_dict", "=", "dict", "(", ")", "\n", "estimator_inputs", "=", "self", ".", "_create_estimator_inputs", "(", "\n", "action_dist", "=", "action_dist", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", "estimated_pscore", "=", "estimated_pscore", ",", "\n", "estimated_importance_weights", "=", "estimated_importance_weights", ",", "\n", "action_embed", "=", "action_embed", ",", "\n", "pi_b", "=", "pi_b", ",", "\n", "p_e_a", "=", "p_e_a", ",", "\n", ")", "\n", "for", "estimator_name", ",", "estimator", "in", "self", ".", "ope_estimators_", ".", "items", "(", ")", ":", "\n", "            ", "policy_value_interval_dict", "[", "estimator_name", "]", "=", "estimator", ".", "estimate_interval", "(", "\n", "**", "estimator_inputs", "[", "estimator_name", "]", ",", "\n", "alpha", "=", "alpha", ",", "\n", "n_bootstrap_samples", "=", "n_bootstrap_samples", ",", "\n", "random_state", "=", "random_state", ",", "\n", ")", "\n", "\n", "", "return", "policy_value_interval_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta.OffPolicyEvaluation.summarize_off_policy_estimates": [[423, 531], ["pandas.DataFrame", "pandas.DataFrame", "meta.OffPolicyEvaluation.bandit_feedback[].mean", "meta.OffPolicyEvaluation.estimate_policy_values", "meta.OffPolicyEvaluation.estimate_intervals", "logger.warning"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta.OffPolicyEvaluation.estimate_policy_values", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta.OffPolicyEvaluation.estimate_intervals"], ["", "def", "summarize_off_policy_estimates", "(", "\n", "self", ",", "\n", "action_dist", ":", "np", ".", "ndarray", ",", "\n", "estimated_rewards_by_reg_model", ":", "Optional", "[", "\n", "Union", "[", "np", ".", "ndarray", ",", "Dict", "[", "str", ",", "np", ".", "ndarray", "]", "]", "\n", "]", "=", "None", ",", "\n", "estimated_pscore", ":", "Optional", "[", "Union", "[", "np", ".", "ndarray", ",", "Dict", "[", "str", ",", "np", ".", "ndarray", "]", "]", "]", "=", "None", ",", "\n", "estimated_importance_weights", ":", "Optional", "[", "\n", "Union", "[", "np", ".", "ndarray", ",", "Dict", "[", "str", ",", "np", ".", "ndarray", "]", "]", "\n", "]", "=", "None", ",", "\n", "action_embed", ":", "Optional", "[", "Union", "[", "np", ".", "ndarray", ",", "Dict", "[", "str", ",", "np", ".", "ndarray", "]", "]", "]", "=", "None", ",", "\n", "pi_b", ":", "Optional", "[", "Union", "[", "np", ".", "ndarray", ",", "Dict", "[", "str", ",", "np", ".", "ndarray", "]", "]", "]", "=", "None", ",", "\n", "p_e_a", ":", "Optional", "[", "Union", "[", "np", ".", "ndarray", ",", "Dict", "[", "str", ",", "np", ".", "ndarray", "]", "]", "]", "=", "None", ",", "\n", "alpha", ":", "float", "=", "0.05", ",", "\n", "n_bootstrap_samples", ":", "int", "=", "100", ",", "\n", "random_state", ":", "Optional", "[", "int", "]", "=", "None", ",", "\n", ")", "->", "Tuple", "[", "DataFrame", ",", "DataFrame", "]", ":", "\n", "        ", "\"\"\"Summarize policy values and their confidence intervals estimated by OPE estimators.\n\n        Parameters\n        ------------\n        action_dist: array-like, shape (n_rounds, n_actions, len_list)\n            Action choice probabilities of the evaluation policy (can be deterministic), i.e., :math:`\\\\pi_e(a_i|x_i)`.\n\n        estimated_rewards_by_reg_model: array-like, shape (n_rounds, n_actions, len_list) or Dict[str, array-like], default=None\n            Expected rewards given each round, action, and position estimated by regression model, i.e., :math:`\\\\hat{q}(x_i,a_i)`.\n            When an array-like is given, all OPE estimators use it.\n            When a dict with an estimator's name as its key is given, the corresponding value is used for the estimator.\n            If None, model-dependent estimators such as DM and DR cannot be used.\n\n        estimated_pscore: array-like, shape (n_rounds,), default=None\n            Estimated behavior policy (propensity scores), i.e., :math:`\\\\hat{\\\\pi}_b(a_i|x_i)`.\n            When an array-like is given, all OPE estimators use it.\n            When a dict with an estimator's name as its key is given, the corresponding value is used for the estimator.\n\n        estimated_importance_weights: array-like, shape (n_rounds,)  or Dict[str, array-like], default=None\n            Importance weights estimated via supervised classification implemented by `obp.ope.ImportanceWeightEstimator`.\n            When an array-like is given, all OPE estimators use it.\n            When a dict with an estimator's name as its key is given, the corresponding value is used for the estimator.\n\n        action_embed: array-like, shape (n_rounds, dim_action_embed)\n            Context vectors characterizing actions or action embeddings such as item category information.\n            This is used to estimate the marginal importance weights.\n\n        pi_b: array-like, shape (n_rounds, n_actions, len_list)\n            Action choice probabilities of the logging/behavior policy, i.e., :math:`\\\\pi_b(a_i|x_i)`.\n\n        p_e_a: array-like, shape (n_actions, n_cat_per_dim, n_cat_dim), default=None\n            Conditional distribution of action embeddings given each action.\n            This distribution is available only when we use synthetic bandit data, i.e.,\n            `obp.dataset.SyntheticBanditDatasetWithActionEmbeds`.\n            See the output of the `obtain_batch_bandit_feedback` argument of this class.\n            If `p_e_a` is given, MIPW uses the true marginal importance weights based on this distribution.\n            The performance of MIPW with the true weights is useful in synthetic experiments of research papers.\n\n        alpha: float, default=0.05\n            Significance level.\n\n        n_bootstrap_samples: int, default=100\n            Number of resampling performed in bootstrap sampling.\n\n        random_state: int, default=None\n            Controls the random seed in bootstrap sampling.\n\n        Returns\n        ----------\n        (policy_value_df, policy_value_interval_df): Tuple[DataFrame, DataFrame]\n            Policy values and their confidence intervals estimated by OPE estimators.\n\n        \"\"\"", "\n", "policy_value_df", "=", "DataFrame", "(", "\n", "self", ".", "estimate_policy_values", "(", "\n", "action_dist", "=", "action_dist", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", "estimated_pscore", "=", "estimated_pscore", ",", "\n", "estimated_importance_weights", "=", "estimated_importance_weights", ",", "\n", "action_embed", "=", "action_embed", ",", "\n", "pi_b", "=", "pi_b", ",", "\n", "p_e_a", "=", "p_e_a", ",", "\n", ")", ",", "\n", "index", "=", "[", "\"estimated_policy_value\"", "]", ",", "\n", ")", "\n", "policy_value_interval_df", "=", "DataFrame", "(", "\n", "self", ".", "estimate_intervals", "(", "\n", "action_dist", "=", "action_dist", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", "estimated_pscore", "=", "estimated_pscore", ",", "\n", "estimated_importance_weights", "=", "estimated_importance_weights", ",", "\n", "action_embed", "=", "action_embed", ",", "\n", "pi_b", "=", "pi_b", ",", "\n", "p_e_a", "=", "p_e_a", ",", "\n", "alpha", "=", "alpha", ",", "\n", "n_bootstrap_samples", "=", "n_bootstrap_samples", ",", "\n", "random_state", "=", "random_state", ",", "\n", ")", "\n", ")", "\n", "policy_value_of_behavior_policy", "=", "self", ".", "bandit_feedback", "[", "\"reward\"", "]", ".", "mean", "(", ")", "\n", "policy_value_df", "=", "policy_value_df", ".", "T", "\n", "if", "policy_value_of_behavior_policy", "<=", "0", ":", "\n", "            ", "logger", ".", "warning", "(", "\n", "f\"Policy value of the behavior policy is {policy_value_of_behavior_policy} (<=0); relative estimated policy value is set to np.nan\"", "\n", ")", "\n", "policy_value_df", "[", "\"relative_estimated_policy_value\"", "]", "=", "np", ".", "nan", "\n", "", "else", ":", "\n", "            ", "policy_value_df", "[", "\"relative_estimated_policy_value\"", "]", "=", "(", "\n", "policy_value_df", ".", "estimated_policy_value", "/", "policy_value_of_behavior_policy", "\n", ")", "\n", "", "return", "policy_value_df", ",", "policy_value_interval_df", ".", "T", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta.OffPolicyEvaluation.visualize_off_policy_estimates": [[532, 656], ["dict", "meta.OffPolicyEvaluation._create_estimator_inputs", "meta.OffPolicyEvaluation.ope_estimators_.items", "pandas.DataFrame", "pandas.DataFrame.rename", "matplotlib.style.use", "matplotlib.subplots", "seaborn.barplot", "matplotlib.xlabel", "matplotlib.ylabel", "matplotlib.yticks", "matplotlib.xticks", "isinstance", "isinstance", "estimator._estimate_round_rewards", "meta.OffPolicyEvaluation.bandit_feedback[].mean", "fig.savefig", "str", "key.upper", "numpy.int32", "dict.keys", "len"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta.OffPolicyEvaluation._create_estimator_inputs", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SelfNormalizedSlateStandardIPS._estimate_round_rewards"], ["", "def", "visualize_off_policy_estimates", "(", "\n", "self", ",", "\n", "action_dist", ":", "np", ".", "ndarray", ",", "\n", "estimated_rewards_by_reg_model", ":", "Optional", "[", "\n", "Union", "[", "np", ".", "ndarray", ",", "Dict", "[", "str", ",", "np", ".", "ndarray", "]", "]", "\n", "]", "=", "None", ",", "\n", "estimated_pscore", ":", "Optional", "[", "Union", "[", "np", ".", "ndarray", ",", "Dict", "[", "str", ",", "np", ".", "ndarray", "]", "]", "]", "=", "None", ",", "\n", "estimated_importance_weights", ":", "Optional", "[", "\n", "Union", "[", "np", ".", "ndarray", ",", "Dict", "[", "str", ",", "np", ".", "ndarray", "]", "]", "\n", "]", "=", "None", ",", "\n", "action_embed", ":", "Optional", "[", "Union", "[", "np", ".", "ndarray", ",", "Dict", "[", "str", ",", "np", ".", "ndarray", "]", "]", "]", "=", "None", ",", "\n", "pi_b", ":", "Optional", "[", "Union", "[", "np", ".", "ndarray", ",", "Dict", "[", "str", ",", "np", ".", "ndarray", "]", "]", "]", "=", "None", ",", "\n", "p_e_a", ":", "Optional", "[", "Union", "[", "np", ".", "ndarray", ",", "Dict", "[", "str", ",", "np", ".", "ndarray", "]", "]", "]", "=", "None", ",", "\n", "alpha", ":", "float", "=", "0.05", ",", "\n", "is_relative", ":", "bool", "=", "False", ",", "\n", "n_bootstrap_samples", ":", "int", "=", "100", ",", "\n", "random_state", ":", "Optional", "[", "int", "]", "=", "None", ",", "\n", "fig_dir", ":", "Optional", "[", "Path", "]", "=", "None", ",", "\n", "fig_name", ":", "str", "=", "\"estimated_policy_value.png\"", ",", "\n", ")", "->", "None", ":", "\n", "        ", "\"\"\"Visualize the estimated policy values.\n\n        Parameters\n        ----------\n        action_dist: array-like, shape (n_rounds, n_actions, len_list)\n            Action choice probabilities of the evaluation policy (can be deterministic), i.e., :math:`\\\\pi_e(a_i|x_i)`.\n\n        estimated_rewards_by_reg_model: array-like, shape (n_rounds, n_actions, len_list) or Dict[str, array-like], default=None\n            Estimated expected rewards given context, action, and position, i.e., :math:`\\\\hat{q}(x_i,a_i)`.\n            When an array-like is given, all OPE estimators use it.\n            When a dict with an estimator's name as its key is given, the corresponding value is used for the estimator.\n            If None, model-dependent estimators such as DM and DR cannot be used.\n\n        estimated_pscore: array-like, shape (n_rounds,), default=None\n            Estimated behavior policy (propensity scores), i.e., :math:`\\\\hat{\\\\pi}_b(a_i|x_i)`.\n            When an array-like is given, all OPE estimators use it.\n            When a dict with an estimator's name as its key is given, the corresponding value is used for the estimator.\n\n        estimated_importance_weights: array-like, shape (n_rounds,)  or Dict[str, array-like], default=None\n            Importance weights estimated via supervised classification implemented by `obp.ope.ImportanceWeightEstimator`.\n            When an array-like is given, all OPE estimators use it.\n            When a dict with an estimator's name as its key is given, the corresponding value is used for the estimator.\n\n        action_embed: array-like, shape (n_rounds, dim_action_embed)\n            Context vectors characterizing actions or action embeddings such as item category information.\n            This is used to estimate the marginal importance weights.\n\n        pi_b: array-like, shape (n_rounds, n_actions, len_list)\n            Action choice probabilities of the logging/behavior policy, i.e., :math:`\\\\pi_b(a_i|x_i)`.\n\n        p_e_a: array-like, shape (n_actions, n_cat_per_dim, n_cat_dim), default=None\n            Conditional distribution of action embeddings given each action.\n            This distribution is available only when we use synthetic bandit data, i.e.,\n            `obp.dataset.SyntheticBanditDatasetWithActionEmbeds`.\n            See the output of the `obtain_batch_bandit_feedback` argument of this class.\n            If `p_e_a` is given, MIPW uses the true marginal importance weights based on this distribution.\n            The performance of MIPW with the true weights is useful in synthetic experiments of research papers.\n\n        alpha: float, default=0.05\n            Significance level.\n\n        n_bootstrap_samples: int, default=100\n            Number of resampling performed in bootstrap sampling.\n\n        random_state: int, default=None\n            Controls the random seed in bootstrap sampling.\n\n        is_relative: bool, default=False,\n            If True, the method visualizes the estimated policy values of evaluation policy\n            relative to the ground-truth policy value of behavior policy.\n\n        fig_dir: Path, default=None\n            Path to store the bar figure.\n            If None, the figure will not be saved.\n\n        fig_name: str, default=\"estimated_policy_value.png\"\n            Name of the bar figure.\n\n        \"\"\"", "\n", "if", "fig_dir", "is", "not", "None", ":", "\n", "            ", "assert", "isinstance", "(", "fig_dir", ",", "Path", ")", ",", "\"`fig_dir` must be a Path\"", "\n", "", "if", "fig_name", "is", "not", "None", ":", "\n", "            ", "assert", "isinstance", "(", "fig_name", ",", "str", ")", ",", "\"`fig_dir` must be a string\"", "\n", "\n", "", "estimated_round_rewards_dict", "=", "dict", "(", ")", "\n", "estimator_inputs", "=", "self", ".", "_create_estimator_inputs", "(", "\n", "action_dist", "=", "action_dist", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", "estimated_pscore", "=", "estimated_pscore", ",", "\n", "estimated_importance_weights", "=", "estimated_importance_weights", ",", "\n", "action_embed", "=", "action_embed", ",", "\n", "pi_b", "=", "pi_b", ",", "\n", "p_e_a", "=", "p_e_a", ",", "\n", ")", "\n", "for", "estimator_name", ",", "estimator", "in", "self", ".", "ope_estimators_", ".", "items", "(", ")", ":", "\n", "            ", "estimated_round_rewards_dict", "[", "\n", "estimator_name", "\n", "]", "=", "estimator", ".", "_estimate_round_rewards", "(", "**", "estimator_inputs", "[", "estimator_name", "]", ")", "\n", "", "estimated_round_rewards_df", "=", "DataFrame", "(", "estimated_round_rewards_dict", ")", "\n", "estimated_round_rewards_df", ".", "rename", "(", "\n", "columns", "=", "{", "key", ":", "key", ".", "upper", "(", ")", "for", "key", "in", "estimated_round_rewards_dict", ".", "keys", "(", ")", "}", ",", "\n", "inplace", "=", "True", ",", "\n", ")", "\n", "if", "is_relative", ":", "\n", "            ", "estimated_round_rewards_df", "/=", "self", ".", "bandit_feedback", "[", "\"reward\"", "]", ".", "mean", "(", ")", "\n", "\n", "", "plt", ".", "style", ".", "use", "(", "\"ggplot\"", ")", "\n", "fig", ",", "ax", "=", "plt", ".", "subplots", "(", "figsize", "=", "(", "8", ",", "6", ")", ")", "\n", "sns", ".", "barplot", "(", "\n", "data", "=", "estimated_round_rewards_df", ",", "\n", "ax", "=", "ax", ",", "\n", "ci", "=", "100", "*", "(", "1", "-", "alpha", ")", ",", "\n", "n_boot", "=", "n_bootstrap_samples", ",", "\n", "seed", "=", "random_state", ",", "\n", ")", "\n", "plt", ".", "xlabel", "(", "\"OPE Estimators\"", ",", "fontsize", "=", "25", ")", "\n", "plt", ".", "ylabel", "(", "\n", "f\"Estimated Policy Value (\u00b1 {np.int32(100*(1 - alpha))}% CI)\"", ",", "fontsize", "=", "20", "\n", ")", "\n", "plt", ".", "yticks", "(", "fontsize", "=", "15", ")", "\n", "plt", ".", "xticks", "(", "fontsize", "=", "25", "-", "2", "*", "len", "(", "self", ".", "ope_estimators", ")", ")", "\n", "\n", "if", "fig_dir", ":", "\n", "            ", "fig", ".", "savefig", "(", "str", "(", "fig_dir", "/", "fig_name", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta.OffPolicyEvaluation.evaluate_performance_of_estimators": [[657, 776], ["sklearn.utils.check_scalar", "dict", "meta.OffPolicyEvaluation._create_estimator_inputs", "meta.OffPolicyEvaluation.ope_estimators_.items", "ValueError", "ValueError", "estimator.estimate_policy_value", "numpy.abs"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta.OffPolicyEvaluation._create_estimator_inputs", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_policy_value"], ["", "", "def", "evaluate_performance_of_estimators", "(", "\n", "self", ",", "\n", "ground_truth_policy_value", ":", "float", ",", "\n", "action_dist", ":", "np", ".", "ndarray", ",", "\n", "estimated_rewards_by_reg_model", ":", "Optional", "[", "\n", "Union", "[", "np", ".", "ndarray", ",", "Dict", "[", "str", ",", "np", ".", "ndarray", "]", "]", "\n", "]", "=", "None", ",", "\n", "estimated_pscore", ":", "Optional", "[", "Union", "[", "np", ".", "ndarray", ",", "Dict", "[", "str", ",", "np", ".", "ndarray", "]", "]", "]", "=", "None", ",", "\n", "estimated_importance_weights", ":", "Optional", "[", "\n", "Union", "[", "np", ".", "ndarray", ",", "Dict", "[", "str", ",", "np", ".", "ndarray", "]", "]", "\n", "]", "=", "None", ",", "\n", "action_embed", ":", "Optional", "[", "Union", "[", "np", ".", "ndarray", ",", "Dict", "[", "str", ",", "np", ".", "ndarray", "]", "]", "]", "=", "None", ",", "\n", "pi_b", ":", "Optional", "[", "Union", "[", "np", ".", "ndarray", ",", "Dict", "[", "str", ",", "np", ".", "ndarray", "]", "]", "]", "=", "None", ",", "\n", "p_e_a", ":", "Optional", "[", "Union", "[", "np", ".", "ndarray", ",", "Dict", "[", "str", ",", "np", ".", "ndarray", "]", "]", "]", "=", "None", ",", "\n", "metric", ":", "str", "=", "\"se\"", ",", "\n", ")", "->", "Dict", "[", "str", ",", "float", "]", ":", "\n", "        ", "\"\"\"Evaluate the accuracy of OPE estimators.\n\n        Note\n        ------\n        Evaluate the estimation performance of OPE estimators with relative estimation error (relative-EE) or squared error (SE):\n\n        .. math ::\n\n            \\\\text{Relative-EE} (\\\\hat{V}; \\\\mathcal{D}) = \\\\left|  \\\\frac{\\\\hat{V}(\\\\pi; \\\\mathcal{D}) - V(\\\\pi)}{V(\\\\pi)} \\\\right|,\n\n        .. math ::\n\n            \\\\text{SE} (\\\\hat{V}; \\\\mathcal{D}) = \\\\left(\\\\hat{V}(\\\\pi; \\\\mathcal{D}) - V(\\\\pi) \\\\right)^2,\n\n        where :math:`V({\\\\pi})` is the ground-truth policy value of the evalation policy :math:`\\\\pi_e` (often estimated using on-policy estimation).\n        :math:`\\\\hat{V}(\\\\pi; \\\\mathcal{D})` is the policy value estimated by an OPE estimator :math:`\\\\hat{V}` and logged bandit feedback :math:`\\\\mathcal{D}`.\n\n        Parameters\n        ----------\n        ground_truth policy value: float\n            Ground_truth policy value of evaluation policy, i.e., :math:`V(\\\\pi_e)`.\n            With Open Bandit Dataset, we use an on-policy estimate of the policy value as its ground-truth.\n\n        action_dist: array-like, shape (n_rounds, n_actions, len_list)\n            Action choice probabilities of the evaluation policy (can be deterministic), i.e., :math:`\\\\pi_e(a_i|x_i)`.\n\n        estimated_rewards_by_reg_model: array-like, shape (n_rounds, n_actions, len_list) or Dict[str, array-like], default=None\n            Estimated expected rewards given context, action, and position, i.e., :math:`\\\\hat{q}(x_i,a_i)`.\n            When an array-like is given, all OPE estimators use it.\n            When a dict with an estimator's name as its key is given, the corresponding value is used for the estimator.\n            If None, model-dependent estimators such as DM and DR cannot be used.\n\n        estimated_pscore: array-like, shape (n_rounds,), default=None\n            Estimated behavior policy (propensity scores), i.e., :math:`\\\\hat{\\\\pi}_b(a_i|x_i)`.\n            When an array-like is given, all OPE estimators use it.\n            When a dict with an estimator's name as its key is given, the corresponding value is used for the estimator.\n\n        estimated_importance_weights: array-like, shape (n_rounds,)  or Dict[str, array-like], default=None\n            Importance weights estimated via supervised classification implemented by `obp.ope.ImportanceWeightEstimator`.\n            When an array-like is given, all OPE estimators use it.\n            When a dict with an estimator's name as its key is given, the corresponding value is used for the estimator.\n\n        action_embed: array-like, shape (n_rounds, dim_action_embed)\n            Context vectors characterizing actions or action embeddings such as item category information.\n            This is used to estimate the marginal importance weights.\n\n        pi_b: array-like, shape (n_rounds, n_actions, len_list)\n            Action choice probabilities of the logging/behavior policy, i.e., :math:`\\\\pi_b(a_i|x_i)`.\n\n        p_e_a: array-like, shape (n_actions, n_cat_per_dim, n_cat_dim), default=None\n            Conditional distribution of action embeddings given each action.\n            This distribution is available only when we use synthetic bandit data, i.e.,\n            `obp.dataset.SyntheticBanditDatasetWithActionEmbeds`.\n            See the output of the `obtain_batch_bandit_feedback` argument of this class.\n            If `p_e_a` is given, MIPW uses the true marginal importance weights based on this distribution.\n            The performance of MIPW with the true weights is useful in synthetic experiments of research papers.\n\n        metric: str, default=\"se\"\n            Evaluation metric used to evaluate and compare the estimation performance of OPE estimators.\n            Must be either \"relative-ee\" or \"se\".\n\n        Returns\n        ----------\n        eval_metric_ope_dict: Dict[str, float]\n            Dictionary containing the value of evaluation metric for the estimation performance of OPE estimators.\n\n        \"\"\"", "\n", "check_scalar", "(", "\n", "ground_truth_policy_value", ",", "\n", "\"ground_truth_policy_value\"", ",", "\n", "float", ",", "\n", ")", "\n", "if", "metric", "not", "in", "[", "\"relative-ee\"", ",", "\"se\"", "]", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "f\"`metric` must be either 'relative-ee' or 'se', but {metric} is given\"", "\n", ")", "\n", "", "if", "metric", "==", "\"relative-ee\"", "and", "ground_truth_policy_value", "==", "0.0", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"`ground_truth_policy_value` must be non-zero when metric is relative-ee\"", "\n", ")", "\n", "\n", "", "eval_metric_ope_dict", "=", "dict", "(", ")", "\n", "estimator_inputs", "=", "self", ".", "_create_estimator_inputs", "(", "\n", "action_dist", "=", "action_dist", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", "estimated_pscore", "=", "estimated_pscore", ",", "\n", "estimated_importance_weights", "=", "estimated_importance_weights", ",", "\n", "action_embed", "=", "action_embed", ",", "\n", "pi_b", "=", "pi_b", ",", "\n", "p_e_a", "=", "p_e_a", ",", "\n", ")", "\n", "for", "estimator_name", ",", "estimator", "in", "self", ".", "ope_estimators_", ".", "items", "(", ")", ":", "\n", "            ", "estimated_policy_value", "=", "estimator", ".", "estimate_policy_value", "(", "\n", "**", "estimator_inputs", "[", "estimator_name", "]", "\n", ")", "\n", "if", "metric", "==", "\"relative-ee\"", ":", "\n", "                ", "relative_ee_", "=", "estimated_policy_value", "-", "ground_truth_policy_value", "\n", "relative_ee_", "/=", "ground_truth_policy_value", "\n", "eval_metric_ope_dict", "[", "estimator_name", "]", "=", "np", ".", "abs", "(", "relative_ee_", ")", "\n", "", "elif", "metric", "==", "\"se\"", ":", "\n", "                ", "se_", "=", "(", "estimated_policy_value", "-", "ground_truth_policy_value", ")", "**", "2", "\n", "eval_metric_ope_dict", "[", "estimator_name", "]", "=", "se_", "\n", "", "", "return", "eval_metric_ope_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta.OffPolicyEvaluation.summarize_estimators_comparison": [[777, 858], ["pandas.DataFrame", "meta.OffPolicyEvaluation.evaluate_performance_of_estimators"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta.OffPolicyEvaluation.evaluate_performance_of_estimators"], ["", "def", "summarize_estimators_comparison", "(", "\n", "self", ",", "\n", "ground_truth_policy_value", ":", "float", ",", "\n", "action_dist", ":", "np", ".", "ndarray", ",", "\n", "estimated_rewards_by_reg_model", ":", "Optional", "[", "\n", "Union", "[", "np", ".", "ndarray", ",", "Dict", "[", "str", ",", "np", ".", "ndarray", "]", "]", "\n", "]", "=", "None", ",", "\n", "estimated_pscore", ":", "Optional", "[", "Union", "[", "np", ".", "ndarray", ",", "Dict", "[", "str", ",", "np", ".", "ndarray", "]", "]", "]", "=", "None", ",", "\n", "estimated_importance_weights", ":", "Optional", "[", "\n", "Union", "[", "np", ".", "ndarray", ",", "Dict", "[", "str", ",", "np", ".", "ndarray", "]", "]", "\n", "]", "=", "None", ",", "\n", "action_embed", ":", "Optional", "[", "Union", "[", "np", ".", "ndarray", ",", "Dict", "[", "str", ",", "np", ".", "ndarray", "]", "]", "]", "=", "None", ",", "\n", "pi_b", ":", "Optional", "[", "Union", "[", "np", ".", "ndarray", ",", "Dict", "[", "str", ",", "np", ".", "ndarray", "]", "]", "]", "=", "None", ",", "\n", "p_e_a", ":", "Optional", "[", "Union", "[", "np", ".", "ndarray", ",", "Dict", "[", "str", ",", "np", ".", "ndarray", "]", "]", "]", "=", "None", ",", "\n", "metric", ":", "str", "=", "\"se\"", ",", "\n", ")", "->", "DataFrame", ":", "\n", "        ", "\"\"\"Summarize the performance comparison among OPE estimators.\n\n        Parameters\n        ----------\n        ground_truth policy value: float\n            Ground_truth policy value of evaluation policy, i.e., :math:`V(\\\\pi_e)`.\n            With Open Bandit Dataset, we use an on-policy estimate of the policy value as ground-truth.\n\n        action_dist: array-like, shape (n_rounds, n_actions, len_list)\n            Action choice probabilities of the evaluation policy (can be deterministic), i.e., :math:`\\\\pi_e(a_i|x_i)`.\n\n        estimated_rewards_by_reg_model: array-like, shape (n_rounds, n_actions, len_list), default=None\n            Estimated expected rewards given context, action, and position, i.e., :math:`\\\\hat{q}(x_i,a_i)`.\n            If None, model-dependent estimators such as DM and DR cannot be used.\n\n        estimated_pscore: array-like, shape (n_rounds,), default=None\n            Estimated behavior policy (propensity scores), i.e., :math:`\\\\hat{\\\\pi}_b(a_i|x_i)`.\n            When an array-like is given, all OPE estimators use it.\n            When a dict with an estimator's name as its key is given, the corresponding value is used for the estimator.\n\n        estimated_importance_weights: array-like, shape (n_rounds,)  or Dict[str, array-like], default=None\n            Importance weights estimated via supervised classification implemented by `obp.ope.ImportanceWeightEstimator`.\n            When an array-like is given, all OPE estimators use it.\n            When a dict with an estimator's name as its key is given, the corresponding value is used for the estimator.\n\n        action_embed: array-like, shape (n_rounds, dim_action_embed)\n            Context vectors characterizing actions or action embeddings such as item category information.\n            This is used to estimate the marginal importance weights.\n\n        pi_b: array-like, shape (n_rounds, n_actions, len_list)\n            Action choice probabilities of the logging/behavior policy, i.e., :math:`\\\\pi_b(a_i|x_i)`.\n\n        p_e_a: array-like, shape (n_actions, n_cat_per_dim, n_cat_dim), default=None\n            Conditional distribution of action embeddings given each action.\n            This distribution is available only when we use synthetic bandit data, i.e.,\n            `obp.dataset.SyntheticBanditDatasetWithActionEmbeds`.\n            See the output of the `obtain_batch_bandit_feedback` argument of this class.\n            If `p_e_a` is given, MIPW uses the true marginal importance weights based on this distribution.\n            The performance of MIPW with the true weights is useful in synthetic experiments of research papers.\n\n        metric: str, default=\"se\"\n            Evaluation metric used to evaluate and compare the estimation performance of OPE estimators.\n            Must be either \"relative-ee\" or \"se\".\n\n        Returns\n        ----------\n        eval_metric_ope_df: DataFrame\n            Results of performance comparison among OPE estimators.\n\n        \"\"\"", "\n", "eval_metric_ope_df", "=", "DataFrame", "(", "\n", "self", ".", "evaluate_performance_of_estimators", "(", "\n", "ground_truth_policy_value", "=", "ground_truth_policy_value", ",", "\n", "action_dist", "=", "action_dist", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", "estimated_pscore", "=", "estimated_pscore", ",", "\n", "estimated_importance_weights", "=", "estimated_importance_weights", ",", "\n", "action_embed", "=", "action_embed", ",", "\n", "pi_b", "=", "pi_b", ",", "\n", "p_e_a", "=", "p_e_a", ",", "\n", "metric", "=", "metric", ",", "\n", ")", ",", "\n", "index", "=", "[", "metric", "]", ",", "\n", ")", "\n", "return", "eval_metric_ope_df", ".", "T", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta.OffPolicyEvaluation.visualize_off_policy_estimates_of_multiple_policies": [[859, 1000], ["zip", "matplotlib.style.use", "matplotlib.figure", "enumerate", "len", "len", "ValueError", "isinstance", "isinstance", "meta.OffPolicyEvaluation._create_estimator_inputs", "meta.OffPolicyEvaluation.ope_estimators_.items", "pandas.DataFrame", "matplotlib.figure.add_subplot", "seaborn.barplot", "plt.figure.add_subplot.set_title", "plt.figure.add_subplot.set_ylabel", "matplotlib.yticks", "matplotlib.xticks", "matplotlib.figure.savefig", "estimator._estimate_round_rewards", "meta.OffPolicyEvaluation.bandit_feedback[].mean", "len", "estimator_name.upper", "str", "len", "numpy.int32", "len"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.meta.OffPolicyEvaluation._create_estimator_inputs", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SelfNormalizedSlateStandardIPS._estimate_round_rewards"], ["", "def", "visualize_off_policy_estimates_of_multiple_policies", "(", "\n", "self", ",", "\n", "policy_name_list", ":", "List", "[", "str", "]", ",", "\n", "action_dist_list", ":", "List", "[", "np", ".", "ndarray", "]", ",", "\n", "estimated_rewards_by_reg_model", ":", "Optional", "[", "\n", "Union", "[", "np", ".", "ndarray", ",", "Dict", "[", "str", ",", "np", ".", "ndarray", "]", "]", "\n", "]", "=", "None", ",", "\n", "estimated_pscore", ":", "Optional", "[", "Union", "[", "np", ".", "ndarray", ",", "Dict", "[", "str", ",", "np", ".", "ndarray", "]", "]", "]", "=", "None", ",", "\n", "estimated_importance_weights", ":", "Optional", "[", "\n", "Union", "[", "np", ".", "ndarray", ",", "Dict", "[", "str", ",", "np", ".", "ndarray", "]", "]", "\n", "]", "=", "None", ",", "\n", "action_embed", ":", "Optional", "[", "Union", "[", "np", ".", "ndarray", ",", "Dict", "[", "str", ",", "np", ".", "ndarray", "]", "]", "]", "=", "None", ",", "\n", "pi_b", ":", "Optional", "[", "Union", "[", "np", ".", "ndarray", ",", "Dict", "[", "str", ",", "np", ".", "ndarray", "]", "]", "]", "=", "None", ",", "\n", "p_e_a", ":", "Optional", "[", "Union", "[", "np", ".", "ndarray", ",", "Dict", "[", "str", ",", "np", ".", "ndarray", "]", "]", "]", "=", "None", ",", "\n", "alpha", ":", "float", "=", "0.05", ",", "\n", "is_relative", ":", "bool", "=", "False", ",", "\n", "n_bootstrap_samples", ":", "int", "=", "100", ",", "\n", "random_state", ":", "Optional", "[", "int", "]", "=", "None", ",", "\n", "fig_dir", ":", "Optional", "[", "Path", "]", "=", "None", ",", "\n", "fig_name", ":", "str", "=", "\"estimated_policy_value.png\"", ",", "\n", ")", "->", "None", ":", "\n", "        ", "\"\"\"Visualize the estimated policy values.\n\n        Parameters\n        ----------\n        policy_name_list: List[str]\n            List of the names of evaluation policies.\n\n        action_dist_list: List[array-like, shape (n_rounds, n_actions, len_list)]\n            List of action choice probabilities of the evaluation policies (can be deterministic), i.e., :math:`\\\\pi_e(a_i|x_i)`.\n\n        estimated_rewards_by_reg_model: array-like, shape (n_rounds, n_actions, len_list) or Dict[str, array-like], default=None\n            Estimated expected rewards given context, action, and position, i.e., :math:`\\\\hat{q}(x_i,a_i)`.\n            When an array-like is given, all OPE estimators use it.\n            When a dict is given, if the dict has the name of an estimator as a key, the corresponding value is used.\n            If None, model-dependent estimators such as DM and DR cannot be used.\n\n        estimated_pscore: array-like, shape (n_rounds,), default=None\n            Estimated behavior policy (propensity scores), i.e., :math:`\\\\hat{\\\\pi}_b(a_i|x_i)`.\n            When an array-like is given, all OPE estimators use it.\n            When a dict with an estimator's name as its key is given, the corresponding value is used for the estimator.\n\n        estimated_importance_weights: array-like, shape (n_rounds,)  or Dict[str, array-like], default=None\n            Importance weights estimated via supervised classification implemented by `obp.ope.ImportanceWeightEstimator`.\n            When an array-like is given, all OPE estimators use it.\n            When a dict with an estimator's name as its key is given, the corresponding value is used for the estimator.\n\n        action_embed: array-like, shape (n_rounds, dim_action_embed)\n            Context vectors characterizing actions or action embeddings such as item category information.\n            This is used to estimate the marginal importance weights.\n\n        pi_b: array-like, shape (n_rounds, n_actions, len_list)\n            Action choice probabilities of the logging/behavior policy, i.e., :math:`\\\\pi_b(a_i|x_i)`.\n\n        p_e_a: array-like, shape (n_actions, n_cat_per_dim, n_cat_dim), default=None\n            Conditional distribution of action embeddings given each action.\n            This distribution is available only when we use synthetic bandit data, i.e.,\n            `obp.dataset.SyntheticBanditDatasetWithActionEmbeds`.\n            See the output of the `obtain_batch_bandit_feedback` argument of this class.\n            If `p_e_a` is given, MIPW uses the true marginal importance weights based on this distribution.\n            The performance of MIPW with the true weights is useful in synthetic experiments of research papers.\n\n        alpha: float, default=0.05\n            Significance level.\n\n        n_bootstrap_samples: int, default=100\n            Number of resampling performed in bootstrap sampling.\n\n        random_state: int, default=None\n            Controls the random seed in bootstrap sampling.\n\n        is_relative: bool, default=False,\n            If True, the method visualizes the estimated policy values of evaluation policy\n            relative to the ground-truth policy value of behavior policy.\n\n        fig_dir: Path, default=None\n            Path to store the bar figure.\n            If None, the figure will not be saved.\n\n        fig_name: str, default=\"estimated_policy_value.png\"\n            Name of the bar figure.\n\n        \"\"\"", "\n", "if", "len", "(", "policy_name_list", ")", "!=", "len", "(", "action_dist_list", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"the length of `policy_name_list` must be the same as `action_dist_list`\"", "\n", ")", "\n", "", "if", "fig_dir", "is", "not", "None", ":", "\n", "            ", "assert", "isinstance", "(", "fig_dir", ",", "Path", ")", ",", "\"`fig_dir` must be a Path\"", "\n", "", "if", "fig_name", "is", "not", "None", ":", "\n", "            ", "assert", "isinstance", "(", "fig_name", ",", "str", ")", ",", "\"`fig_dir` must be a string\"", "\n", "\n", "", "estimated_round_rewards_dict", "=", "{", "\n", "estimator_name", ":", "{", "}", "for", "estimator_name", "in", "self", ".", "ope_estimators_", "\n", "}", "\n", "\n", "for", "policy_name", ",", "action_dist", "in", "zip", "(", "policy_name_list", ",", "action_dist_list", ")", ":", "\n", "            ", "estimator_inputs", "=", "self", ".", "_create_estimator_inputs", "(", "\n", "action_dist", "=", "action_dist", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", "estimated_pscore", "=", "estimated_pscore", ",", "\n", "estimated_importance_weights", "=", "estimated_importance_weights", ",", "\n", "action_embed", "=", "action_embed", ",", "\n", "pi_b", "=", "pi_b", ",", "\n", "p_e_a", "=", "p_e_a", ",", "\n", ")", "\n", "for", "estimator_name", ",", "estimator", "in", "self", ".", "ope_estimators_", ".", "items", "(", ")", ":", "\n", "                ", "estimated_round_rewards_dict", "[", "estimator_name", "]", "[", "\n", "policy_name", "\n", "]", "=", "estimator", ".", "_estimate_round_rewards", "(", "\n", "**", "estimator_inputs", "[", "estimator_name", "]", "\n", ")", "\n", "\n", "", "", "plt", ".", "style", ".", "use", "(", "\"ggplot\"", ")", "\n", "fig", "=", "plt", ".", "figure", "(", "figsize", "=", "(", "8", ",", "6.2", "*", "len", "(", "self", ".", "ope_estimators_", ")", ")", ")", "\n", "\n", "for", "i", ",", "estimator_name", "in", "enumerate", "(", "self", ".", "ope_estimators_", ")", ":", "\n", "            ", "estimated_round_rewards_df", "=", "DataFrame", "(", "\n", "estimated_round_rewards_dict", "[", "estimator_name", "]", "\n", ")", "\n", "if", "is_relative", ":", "\n", "                ", "estimated_round_rewards_df", "/=", "self", ".", "bandit_feedback", "[", "\"reward\"", "]", ".", "mean", "(", ")", "\n", "\n", "", "ax", "=", "fig", ".", "add_subplot", "(", "len", "(", "action_dist_list", ")", ",", "1", ",", "i", "+", "1", ")", "\n", "sns", ".", "barplot", "(", "\n", "data", "=", "estimated_round_rewards_df", ",", "\n", "ax", "=", "ax", ",", "\n", "ci", "=", "100", "*", "(", "1", "-", "alpha", ")", ",", "\n", "n_boot", "=", "n_bootstrap_samples", ",", "\n", "seed", "=", "random_state", ",", "\n", ")", "\n", "ax", ".", "set_title", "(", "estimator_name", ".", "upper", "(", ")", ",", "fontsize", "=", "20", ")", "\n", "ax", ".", "set_ylabel", "(", "\n", "f\"Estimated Policy Value (\u00b1 {np.int32(100*(1 - alpha))}% CI)\"", ",", "\n", "fontsize", "=", "20", ",", "\n", ")", "\n", "plt", ".", "yticks", "(", "fontsize", "=", "15", ")", "\n", "plt", ".", "xticks", "(", "fontsize", "=", "25", "-", "2", "*", "len", "(", "policy_name_list", ")", ")", "\n", "\n", "", "if", "fig_dir", ":", "\n", "            ", "fig", ".", "savefig", "(", "str", "(", "fig_dir", "/", "fig_name", ")", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.helper.estimate_bias_in_ope": [[14, 58], ["numpy.abs", "numpy.zeros", "estimated_bias_arr.mean"], "function", ["None"], ["def", "estimate_bias_in_ope", "(", "\n", "reward", ":", "np", ".", "ndarray", ",", "\n", "iw", ":", "np", ".", "ndarray", ",", "\n", "iw_hat", ":", "np", ".", "ndarray", ",", "\n", "q_hat", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", ")", "->", "float", ":", "\n", "    ", "\"\"\"Helper to estimate a bias in OPE.\n\n    Parameters\n    ----------\n    reward: array-like, shape (n_rounds,)\n        Rewards observed for each data in logged bandit data, i.e., :math:`r_t`.\n\n    iw: array-like, shape (n_rounds,)\n        Importance weight for each data in logged bandit data, i.e., :math:`w(x,a)=\\\\pi_e(a|x)/ \\\\pi_b(a|x)`.\n\n    iw_hat: array-like, shape (n_rounds,)\n        Importance weight (IW) modified by a hyparpareter. How IW is modified depends on the estimator as follows.\n            - clipping: :math:`\\\\hat{w}(x,a) := \\\\min \\\\{ \\\\lambda, w(x,a) \\\\}`\n            - switching: :math:`\\\\hat{w}(x,a) := w(x,a) \\\\cdot \\\\mathbb{I} \\\\{ w(x,a) < \\\\lambda \\\\}`\n            - shrinkage: :math:`\\\\hat{w}(x,a) := (\\\\lambda w(x,a)) / (\\\\lambda + w^2(x,a))`\n        where :math:`\\\\lambda` is a hyperparameter value.\n\n    q_hat: array-like, shape (n_rounds,), default=None\n        Estimated expected reward given context :math:`x_i` and action :math:`a_i`.\n\n    Returns\n    ----------\n    estimated_bias: float\n        Estimated the bias in OPE.\n        This is based on the direct bias estimation stated on page 17 of Su et al.(2020).\n\n    References\n    ----------\n    Yi Su, Maria Dimakopoulou, Akshay Krishnamurthy, and Miroslav Dudik.\n    \"Doubly Robust Off-Policy Evaluation with Shrinkage.\", 2020.\n\n    \"\"\"", "\n", "if", "q_hat", "is", "None", ":", "\n", "        ", "q_hat", "=", "np", ".", "zeros", "(", "reward", ".", "shape", "[", "0", "]", ")", "\n", "", "estimated_bias_arr", "=", "(", "iw", "-", "iw_hat", ")", "*", "(", "reward", "-", "q_hat", ")", "\n", "estimated_bias", "=", "np", ".", "abs", "(", "estimated_bias_arr", ".", "mean", "(", ")", ")", "\n", "\n", "return", "estimated_bias", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.helper.estimate_high_probability_upper_bound_bias": [[60, 117], ["sklearn.utils.check_scalar", "helper.estimate_bias_in_ope", "numpy.sqrt", "numpy.log", "numpy.log", "iw.max"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.helper.estimate_bias_in_ope"], ["", "def", "estimate_high_probability_upper_bound_bias", "(", "\n", "reward", ":", "np", ".", "ndarray", ",", "\n", "iw", ":", "np", ".", "ndarray", ",", "\n", "iw_hat", ":", "np", ".", "ndarray", ",", "\n", "q_hat", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "delta", ":", "float", "=", "0.05", ",", "\n", ")", "->", "float", ":", "\n", "    ", "\"\"\"Helper to estimate a high probability upper bound of bias in OPE.\n\n    Parameters\n    ----------\n    reward: array-like, shape (n_rounds,)\n        Rewards observed for each data in logged bandit data, i.e., :math:`r_t`.\n\n    iw: array-like, shape (n_rounds,)\n        Importance weight for each data in logged bandit data, i.e., :math:`w(x,a)=\\\\pi_e(a|x)/ \\\\pi_b(a|x)`.\n\n    iw_hat: array-like, shape (n_rounds,)\n        Importance weight (IW) modified by a hyparpareter. How IW is modified depends on the estimator as follows.\n            - clipping: :math:`\\\\hat{w}(x,a) := \\\\min \\\\{ \\\\lambda, w(x,a) \\\\}`\n            - switching: :math:`\\\\hat{w}(x,a) := w(x,a) \\\\cdot \\\\mathbb{I} \\\\{ w(x,a) < \\\\lambda \\\\}`\n            - shrinkage: :math:`\\\\hat{w}(x,a) := (\\\\lambda w(x,a)) / (\\\\lambda + w^2(x,a))`\n        where :math:`\\\\lambda` and :math:`\\\\lambda` are hyperparameters.\n\n    q_hat: array-like, shape (n_rounds,), default=None\n        Estimated expected reward given context :math:`x_i` and action :math:`a_i`.\n\n    delta: float, default=0.05\n        A confidence delta to construct a high probability upper bound based on Bernstein inequality.\n\n    Returns\n    ----------\n    bias_upper_bound: float\n        Estimated (high probability) upper bound of the bias.\n        This upper bound is based on the direct bias estimation\n        stated on page 17 of Su et al.(2020).\n\n    References\n    ----------\n    Yi Su, Maria Dimakopoulou, Akshay Krishnamurthy, and Miroslav Dudik.\n    \"Doubly Robust Off-Policy Evaluation with Shrinkage.\", 2020.\n\n    \"\"\"", "\n", "check_scalar", "(", "delta", ",", "\"delta\"", ",", "(", "int", ",", "float", ")", ",", "min_val", "=", "0.0", ",", "max_val", "=", "1.0", ")", "\n", "\n", "estimated_bias", "=", "estimate_bias_in_ope", "(", "\n", "reward", "=", "reward", ",", "\n", "iw", "=", "iw", ",", "\n", "iw_hat", "=", "iw_hat", ",", "\n", "q_hat", "=", "q_hat", ",", "\n", ")", "\n", "n", "=", "reward", ".", "shape", "[", "0", "]", "\n", "bias_upper_bound", "=", "estimated_bias", "\n", "bias_upper_bound", "+=", "sqrt", "(", "(", "2", "*", "(", "iw", "**", "2", ")", ".", "mean", "(", ")", "*", "log", "(", "2", "/", "delta", ")", ")", "/", "n", ")", "\n", "bias_upper_bound", "+=", "(", "2", "*", "iw", ".", "max", "(", ")", "*", "log", "(", "2", "/", "delta", ")", ")", "/", "(", "3", "*", "n", ")", "\n", "\n", "return", "bias_upper_bound", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.helper.estimate_hoeffding_lower_bound": [[119, 159], ["sklearn.utils.check_scalar", "x.max", "sklearn.utils.check_scalar", "numpy.sqrt", "x.mean", "x.max", "numpy.log"], "function", ["None"], ["", "def", "estimate_hoeffding_lower_bound", "(", "\n", "x", ":", "np", ".", "ndarray", ",", "x_max", ":", "Optional", "[", "float", "]", "=", "None", ",", "delta", ":", "float", "=", "0.05", "\n", ")", "->", "float", ":", "\n", "    ", "\"\"\"Estimate a high probability lower bound of mean of random variables by Hoeffding Inequality.\n\n    Parameters\n    ----------\n    x: array-like, shape (n,)\n        Size n of independent real-valued bounded random variables of interest.\n\n    x_max: float, default=None.\n        A maximum value of random variable `x`.\n        If None, this is estimated from the given samples.\n\n    delta: float, default=0.05\n        A confidence delta to construct a high probability lower bound.\n\n    Returns\n    ----------\n    lower_bound_estimate: float\n        A high probability lower bound of mean of random variables `x` estimated by Hoeffding Inequality.\n        See page 3 of Thomas et al.(2015) for details.\n\n    References\n    ----------\n    Philip S. Thomas, Georgios Theocharous, and Mohammad Ghavamzadeh.\n    \"High Confidence Off-Policy Evaluation.\", 2015.\n\n    \"\"\"", "\n", "if", "x_max", "is", "None", ":", "\n", "        ", "x_max", "=", "x", ".", "max", "(", ")", "\n", "", "else", ":", "\n", "        ", "check_scalar", "(", "x_max", ",", "\"x_max\"", ",", "(", "int", ",", "float", ")", ",", "min_val", "=", "x", ".", "max", "(", ")", ")", "\n", "", "check_scalar", "(", "delta", ",", "\"delta\"", ",", "(", "int", ",", "float", ")", ",", "min_val", "=", "0.0", ",", "max_val", "=", "1.0", ")", "\n", "\n", "n", "=", "x", ".", "shape", "[", "0", "]", "\n", "ci", "=", "x_max", "*", "sqrt", "(", "log", "(", "1.0", "/", "delta", ")", "/", "(", "2", "*", "n", ")", ")", "\n", "lower_bound_estimate", "=", "x", ".", "mean", "(", ")", "-", "ci", "\n", "\n", "return", "lower_bound_estimate", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.helper.estimate_bernstein_lower_bound": [[161, 202], ["sklearn.utils.check_scalar", "numpy.sqrt", "x.max", "sklearn.utils.check_scalar", "numpy.log", "x.mean", "x.max", "numpy.var", "numpy.log"], "function", ["None"], ["", "def", "estimate_bernstein_lower_bound", "(", "\n", "x", ":", "np", ".", "ndarray", ",", "x_max", ":", "Optional", "[", "float", "]", ",", "delta", ":", "float", "=", "0.05", "\n", ")", "->", "float", ":", "\n", "    ", "\"\"\"Estimate a high probability lower bound of mean of random variables by empirical Bernstein Inequality.\n\n    Parameters\n    ----------\n    x: array-like, shape (n, )\n        Size n of independent real-valued bounded random variables of interest.\n\n    x_max: float, default=None.\n        A maximum value of random variable `x`.\n        If None, this is estimated from the given samples.\n\n    delta: float, default=0.05\n        A confidence delta to construct a high probability lower bound.\n\n    Returns\n    ----------\n    lower_bound_estimate: float\n        A high probability lower bound of mean of random variables `x` estimated by Hoeffding Inequality.\n        See page 3 of Thomas et al.(2015) for details.\n\n    References\n    ----------\n    Philip S. Thomas, Georgios Theocharous, and Mohammad Ghavamzadeh.\n    \"High Confidence Off-Policy Evaluation.\", 2015.\n\n    \"\"\"", "\n", "if", "x_max", "is", "None", ":", "\n", "        ", "x_max", "=", "x", ".", "max", "(", ")", "\n", "", "else", ":", "\n", "        ", "check_scalar", "(", "x_max", ",", "\"x_max\"", ",", "(", "int", ",", "float", ")", ",", "min_val", "=", "x", ".", "max", "(", ")", ")", "\n", "", "check_scalar", "(", "delta", ",", "\"delta\"", ",", "(", "int", ",", "float", ")", ",", "min_val", "=", "0.0", ",", "max_val", "=", "1.0", ")", "\n", "\n", "n", "=", "x", ".", "shape", "[", "0", "]", "\n", "ci1", "=", "7", "*", "x_max", "*", "log", "(", "2.0", "/", "delta", ")", "/", "(", "3", "*", "(", "n", "-", "1", ")", ")", "\n", "ci2", "=", "sqrt", "(", "2", "*", "log", "(", "2.0", "/", "delta", ")", "*", "var", "(", "x", ")", "/", "(", "n", "-", "1", ")", ")", "\n", "lower_bound_estimate", "=", "x", ".", "mean", "(", ")", "-", "ci1", "-", "ci2", "\n", "\n", "return", "lower_bound_estimate", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.helper.estimate_student_t_lower_bound": [[204, 235], ["sklearn.utils.check_scalar", "numpy.sqrt", "scipy.stats.t().ppf", "x.mean", "numpy.var", "scipy.stats.t"], "function", ["None"], ["", "def", "estimate_student_t_lower_bound", "(", "x", ":", "np", ".", "ndarray", ",", "delta", ":", "float", "=", "0.05", ")", "->", "float", ":", "\n", "    ", "\"\"\"Estimate a high probability lower bound of mean of random variables based on Student t distribution.\n\n    Parameters\n    ----------\n    x: array-like, shape (n, )\n        Size n of independent real-valued bounded random variables of interest.\n\n    delta: float, default=0.05\n        A confidence delta to construct a high probability lower bound.\n\n    Returns\n    ----------\n    lower_bound_estimate: float\n        A high probability lower bound of mean of random variables `x` estimated based on Student t distribution.\n        See Section 2.4 of Thomas et al.(2015) for details.\n\n    References\n    ----------\n    Philip S. Thomas, Georgios Theocharous, and Mohammad Ghavamzadeh.\n    \"High Confidence Off-Policy Improvement.\", 2015.\n\n    \"\"\"", "\n", "check_scalar", "(", "delta", ",", "\"delta\"", ",", "(", "int", ",", "float", ")", ",", "min_val", "=", "0.0", ",", "max_val", "=", "1.0", ")", "\n", "\n", "n", "=", "x", ".", "shape", "[", "0", "]", "\n", "ci", "=", "sqrt", "(", "var", "(", "x", ")", "/", "(", "n", "-", "1", ")", ")", "\n", "ci", "*=", "stats", ".", "t", "(", "n", "-", "1", ")", ".", "ppf", "(", "1.0", "-", "delta", ")", "\n", "lower_bound_estimate", "=", "x", ".", "mean", "(", ")", "-", "ci", "\n", "\n", "return", "lower_bound_estimate", "\n", "", ""]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.classification_model.ImportanceWeightEstimator.__post_init__": [[67, 99], ["sklearn.utils.check_scalar", "sklearn.utils.check_scalar", "sklearn.utils.check_scalar", "ValueError", "isinstance", "ValueError", "numpy.eye", "isinstance", "sklearn.base.clone", "sklearn.base.clone", "sklearn.calibration.CalibratedClassifierCV", "numpy.arange", "numpy.arange"], "methods", ["None"], ["def", "__post_init__", "(", "self", ")", "->", "None", ":", "\n", "        ", "\"\"\"Initialize Class.\"\"\"", "\n", "check_scalar", "(", "self", ".", "n_actions", ",", "\"n_actions\"", ",", "int", ",", "min_val", "=", "2", ")", "\n", "check_scalar", "(", "self", ".", "len_list", ",", "\"len_list\"", ",", "int", ",", "min_val", "=", "1", ")", "\n", "check_scalar", "(", "self", ".", "calibration_cv", ",", "\"calibration_cv\"", ",", "int", ")", "\n", "if", "not", "(", "\n", "isinstance", "(", "self", ".", "fitting_method", ",", "str", ")", "\n", "and", "self", ".", "fitting_method", "in", "[", "\"sample\"", ",", "\"raw\"", "]", "\n", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "f\"`fitting_method` must be either 'sample' or 'raw', but {self.fitting_method} is given\"", "\n", ")", "\n", "", "if", "not", "isinstance", "(", "self", ".", "base_model", ",", "BaseEstimator", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"`base_model` must be BaseEstimator or a child class of BaseEstimator\"", "\n", ")", "\n", "\n", "", "if", "self", ".", "calibration_cv", ">", "1", ":", "\n", "            ", "self", ".", "base_model_list", "=", "[", "\n", "clone", "(", "\n", "CalibratedClassifierCV", "(", "\n", "base_estimator", "=", "self", ".", "base_model", ",", "cv", "=", "self", ".", "calibration_cv", "\n", ")", ",", "\n", ")", "\n", "for", "_", "in", "np", ".", "arange", "(", "self", ".", "len_list", ")", "\n", "]", "\n", "", "else", ":", "\n", "            ", "self", ".", "base_model_list", "=", "[", "\n", "clone", "(", "self", ".", "base_model", ")", "for", "_", "in", "np", ".", "arange", "(", "self", ".", "len_list", ")", "\n", "]", "\n", "", "if", "self", ".", "action_context", "is", "None", "or", "self", ".", "fitting_method", "==", "\"raw\"", ":", "\n", "            ", "self", ".", "action_context", "=", "np", ".", "eye", "(", "self", ".", "n_actions", ",", "dtype", "=", "int", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.classification_model.ImportanceWeightEstimator.fit": [[100, 178], ["utils.check_bandit_feedback_inputs", "utils.check_array", "numpy.zeros", "numpy.arange", "numpy.zeros_like", "utils.check_array", "ValueError", "numpy.allclose", "ValueError", "numpy.arange", "classification_model.ImportanceWeightEstimator._pre_process_for_clf_model", "classification_model.ImportanceWeightEstimator.base_model_list[].fit", "numpy.zeros_like", "numpy.zeros_like.max", "ValueError", "action_dist.sum", "utils.sample_action_fast", "ValueError", "numpy.zeros_like.max"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_bandit_feedback_inputs", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.classification_model.ImportanceWeightEstimator._pre_process_for_clf_model", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.classification_model.PropensityScoreEstimator.fit", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.sample_action_fast"], ["", "", "def", "fit", "(", "\n", "self", ",", "\n", "context", ":", "np", ".", "ndarray", ",", "\n", "action", ":", "np", ".", "ndarray", ",", "\n", "action_dist", ":", "np", ".", "ndarray", ",", "\n", "position", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "random_state", ":", "Optional", "[", "int", "]", "=", "None", ",", "\n", ")", "->", "None", ":", "\n", "        ", "\"\"\"Fit the classification model on given logged bandit data.\n\n        Parameters\n        ----------\n        context: array-like, shape (n_rounds, dim_context)\n            Context vectors observed for each data in logged bandit data, i.e., :math:`x_i`.\n\n        action: array-like, shape (n_rounds,)\n            Actions sampled by the logging/behavior policy for each data in logged bandit data, i.e., :math:`a_i`.\n\n        action_dist: array-like, shape (n_rounds, n_actions, len_list)\n            Action choice probabilities of the evaluation policy (can be deterministic), i.e., :math:`\\\\pi_e(a_i|x_i)`.\n\n        position: array-like, shape (n_rounds,), default=None\n            Indices to differentiate positions in a recommendation interface where the actions are presented.\n            If None, a classification model assumes that there is only a single position in  a recommendation interface.\n            When `len_list` > 1, an array must be given as `position`.\n\n        random_state: int, default=None\n            `random_state` affects the sampling of actions from the evaluation policy.\n\n        \"\"\"", "\n", "check_bandit_feedback_inputs", "(", "\n", "context", "=", "context", ",", "\n", "action", "=", "action", ",", "\n", "reward", "=", "np", ".", "zeros_like", "(", "action", ")", ",", "# use dummy reward", "\n", "position", "=", "position", ",", "\n", "action_context", "=", "self", ".", "action_context", ",", "\n", ")", "\n", "check_array", "(", "array", "=", "action_dist", ",", "name", "=", "\"action_dist\"", ",", "expected_dim", "=", "3", ")", "\n", "n", "=", "context", ".", "shape", "[", "0", "]", "\n", "\n", "if", "position", "is", "None", "or", "self", ".", "len_list", "==", "1", ":", "\n", "            ", "position", "=", "np", ".", "zeros_like", "(", "action", ")", "\n", "", "else", ":", "\n", "            ", "check_array", "(", "array", "=", "position", ",", "name", "=", "\"position\"", ",", "expected_dim", "=", "1", ")", "\n", "if", "position", ".", "max", "(", ")", ">=", "self", ".", "len_list", ":", "\n", "                ", "raise", "ValueError", "(", "\n", "f\"`position` elements must be smaller than `len_list`, but the maximum value is {position.max()} (>= {self.len_list})\"", "\n", ")", "\n", "", "", "if", "action_dist", ".", "shape", "!=", "(", "n", ",", "self", ".", "n_actions", ",", "self", ".", "len_list", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "f\"shape of `action_dist` must be (n_rounds, n_actions, len_list)=({n, self.n_actions, self.len_list}), but is {action_dist.shape}\"", "\n", ")", "\n", "", "if", "not", "np", ".", "allclose", "(", "action_dist", ".", "sum", "(", "axis", "=", "1", ")", ",", "1", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\"`action_dist` must be a probability distribution\"", ")", "\n", "\n", "# If self.fitting_method != \"sample\", `sampled_action` has no information", "\n", "", "sampled_action", "=", "np", ".", "zeros", "(", "n", ",", "dtype", "=", "int", ")", "\n", "if", "self", ".", "fitting_method", "==", "\"sample\"", ":", "\n", "            ", "for", "pos_", "in", "np", ".", "arange", "(", "self", ".", "len_list", ")", ":", "\n", "                ", "idx", "=", "position", "==", "pos_", "\n", "sampled_action_at_position", "=", "sample_action_fast", "(", "\n", "action_dist", "=", "action_dist", "[", "idx", "]", "[", ":", ",", ":", ",", "pos_", "]", ",", "\n", "random_state", "=", "random_state", ",", "\n", ")", "\n", "sampled_action", "[", "idx", "]", "=", "sampled_action_at_position", "\n", "\n", "", "", "for", "pos_", "in", "np", ".", "arange", "(", "self", ".", "len_list", ")", ":", "\n", "            ", "idx", "=", "position", "==", "pos_", "\n", "action_dist_at_pos", "=", "action_dist", "[", "idx", "]", "[", ":", ",", ":", ",", "pos_", "]", "\n", "X", ",", "y", "=", "self", ".", "_pre_process_for_clf_model", "(", "\n", "context", "=", "context", "[", "idx", "]", ",", "\n", "action", "=", "action", "[", "idx", "]", ",", "\n", "action_dist_at_pos", "=", "action_dist_at_pos", ",", "\n", "sampled_action_at_position", "=", "sampled_action", "[", "idx", "]", ",", "\n", ")", "\n", "if", "X", ".", "shape", "[", "0", "]", "==", "0", ":", "\n", "                ", "raise", "ValueError", "(", "f\"No training data at position {pos_}\"", ")", "\n", "", "self", ".", "base_model_list", "[", "pos_", "]", ".", "fit", "(", "X", ",", "y", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.classification_model.ImportanceWeightEstimator.predict": [[179, 216], ["numpy.zeros", "numpy.arange", "classification_model.ImportanceWeightEstimator._pre_process_for_clf_model", "classification_model.ImportanceWeightEstimator.base_model_list[].predict_proba"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.classification_model.ImportanceWeightEstimator._pre_process_for_clf_model", "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.offline.NNPolicyLearner.predict_proba"], ["", "", "def", "predict", "(", "\n", "self", ",", "\n", "context", ":", "np", ".", "ndarray", ",", "\n", "action", ":", "np", ".", "ndarray", ",", "\n", "position", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", ")", "->", "np", ".", "ndarray", ":", "\n", "        ", "\"\"\"Predict the importance weights.\n\n        Parameters\n        ----------\n        context: array-like, shape (n_rounds_of_new_data, dim_context)\n            Context vectors observed for each data in logged bandit data, i.e., :math:`x_i`.\n\n        action: array-like, shape (n_rounds_of_new_data,)\n            Actions sampled by the logging/behavior policy for each data in logged bandit data, i.e., :math:`a_i`.\n\n        position: array-like, shape (n_rounds_of_new_data,), default=None\n            Indices to differentiate positions in a recommendation interface where the actions are presented.\n            If None, a classification model assumes that there is only a single position in  a recommendation interface.\n            When `len_list` > 1, an array must be given as `position`.\n\n        Returns\n        ----------\n        estimated_importance_weights: array-like, shape (n_rounds_of_new_data, )\n            Importance weights estimated via supervised classification, i.e., :math:`\\\\hat{w}(x_t, a_t)`.\n\n        \"\"\"", "\n", "proba_eval_policy", "=", "np", ".", "zeros", "(", "action", ".", "shape", "[", "0", "]", ")", "\n", "for", "pos_", "in", "np", ".", "arange", "(", "self", ".", "len_list", ")", ":", "\n", "            ", "idx", "=", "position", "==", "pos_", "\n", "X", ",", "_", ",", "=", "self", ".", "_pre_process_for_clf_model", "(", "\n", "context", "=", "context", "[", "idx", "]", ",", "\n", "action", "=", "action", "[", "idx", "]", ",", "\n", "is_prediction", "=", "True", ",", "\n", ")", "\n", "proba_eval_policy", "[", "idx", "]", "=", "self", ".", "base_model_list", "[", "pos_", "]", ".", "predict_proba", "(", "X", ")", "[", ":", ",", "1", "]", "\n", "", "return", "proba_eval_policy", "/", "(", "1", "-", "proba_eval_policy", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.classification_model.ImportanceWeightEstimator.fit_predict": [[217, 349], ["utils.check_bandit_feedback_inputs", "utils.check_array", "sklearn.utils.check_scalar", "sklearn.utils.check_random_state", "sklearn.model_selection.KFold", "sklearn.model_selection.KFold.get_n_splits", "sklearn.model_selection.KFold.split", "numpy.zeros_like", "ValueError", "numpy.allclose", "ValueError", "classification_model.ImportanceWeightEstimator.fit", "classification_model.ImportanceWeightEstimator.predict", "numpy.zeros", "classification_model.ImportanceWeightEstimator.fit", "classification_model.ImportanceWeightEstimator.predict", "numpy.zeros_like", "numpy.zeros_like.max", "ValueError", "action_dist.sum", "numpy.zeros", "numpy.arange", "numpy.arange", "classification_model.ImportanceWeightEstimator._pre_process_for_clf_model", "classification_model.ImportanceWeightEstimator.eval_result[].append", "classification_model.ImportanceWeightEstimator.eval_result[].append", "utils.sample_action_fast", "classification_model.ImportanceWeightEstimator.base_model_list[].predict_proba", "numpy.zeros_like.max"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_bandit_feedback_inputs", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.classification_model.PropensityScoreEstimator.fit", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.classification_model.PropensityScoreEstimator.predict", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.classification_model.PropensityScoreEstimator.fit", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.classification_model.PropensityScoreEstimator.predict", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.classification_model.ImportanceWeightEstimator._pre_process_for_clf_model", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.sample_action_fast", "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.offline.NNPolicyLearner.predict_proba"], ["", "def", "fit_predict", "(", "\n", "self", ",", "\n", "context", ":", "np", ".", "ndarray", ",", "\n", "action", ":", "np", ".", "ndarray", ",", "\n", "action_dist", ":", "np", ".", "ndarray", ",", "\n", "position", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "n_folds", ":", "int", "=", "1", ",", "\n", "random_state", ":", "Optional", "[", "int", "]", "=", "None", ",", "\n", "evaluate_model_performance", ":", "bool", "=", "False", ",", "\n", ")", "->", "np", ".", "ndarray", ":", "\n", "        ", "\"\"\"Fit the classification model on given logged bandit data and predict the importance weights on the same data, possibly using cross-fitting to avoid over-fitting.\n\n        Note\n        ------\n        When `n_folds` is larger than 1, the cross-fitting procedure is applied.\n\n        Parameters\n        ----------\n        context: array-like, shape (n_rounds, dim_context)\n            Context vectors observed for each data in logged bandit data, i.e., :math:`x_i`.\n\n        action: array-like, shape (n_rounds,)\n            Actions sampled by the logging/behavior policy for each data in logged bandit data, i.e., :math:`a_i`.\n\n        action_dist: array-like, shape (n_rounds, n_actions, len_list)\n            Action choice probabilities of the evaluation policy (can be deterministic), i.e., :math:`\\\\pi_e(a_i|x_i)`.\n\n        position: array-like, shape (n_rounds,), default=None\n            Indices to differentiate positions in a recommendation interface where the actions are presented.\n            If None, a classification model assumes that there is only a single position in  a recommendation interface.\n            When `len_list` > 1, an array must be given as `position`.\n\n        n_folds: int, default=1\n            Number of folds in the cross-fitting procedure.\n            When 1 is given, the classification model is trained on the whole logged bandit data.\n            Please refer to https://arxiv.org/abs/2002.08536 about the details of the cross-fitting procedure.\n\n        random_state: int, default=None\n            `random_state` affects the ordering of the indices, which controls the randomness of each fold.\n            See https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html for the details.\n\n        evaluate_model_performance: bool, default=False\n            Whether the performance of the classification model is evaluated or not.\n            If True, the predicted probability of the classification model and the true label of each fold is saved in `self.eval_result[fold]`\n\n        Returns\n        -----------\n        estimated_importance_weights: array-like, shape (n_rounds_of_new_data, )\n            Importance weights estimated via supervised classification, i.e., :math:`\\\\hat{w}(x_t, a_t)`.\n\n        \"\"\"", "\n", "check_bandit_feedback_inputs", "(", "\n", "context", "=", "context", ",", "\n", "action", "=", "action", ",", "\n", "reward", "=", "np", ".", "zeros_like", "(", "action", ")", ",", "# use dummy reward", "\n", "position", "=", "position", ",", "\n", "action_context", "=", "self", ".", "action_context", ",", "\n", ")", "\n", "n", "=", "context", ".", "shape", "[", "0", "]", "\n", "\n", "if", "position", "is", "None", "or", "self", ".", "len_list", "==", "1", ":", "\n", "            ", "position", "=", "np", ".", "zeros_like", "(", "action", ")", "\n", "", "else", ":", "\n", "            ", "if", "position", ".", "max", "(", ")", ">=", "self", ".", "len_list", ":", "\n", "                ", "raise", "ValueError", "(", "\n", "f\"`position` elements must be smaller than `len_list`, but the maximum value is {position.max()} (>= {self.len_list})\"", "\n", ")", "\n", "\n", "", "", "check_array", "(", "array", "=", "action_dist", ",", "name", "=", "\"action_dist\"", ",", "expected_dim", "=", "3", ")", "\n", "check_scalar", "(", "n_folds", ",", "\"n_folds\"", ",", "int", ",", "min_val", "=", "1", ")", "\n", "check_random_state", "(", "random_state", ")", "\n", "\n", "if", "action_dist", ".", "shape", "!=", "(", "n", ",", "self", ".", "n_actions", ",", "self", ".", "len_list", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "f\"shape of `action_dist` must be (n_rounds, n_actions, len_list)=({n, self.n_actions, self.len_list}), but is {action_dist.shape}\"", "\n", ")", "\n", "", "if", "not", "np", ".", "allclose", "(", "action_dist", ".", "sum", "(", "axis", "=", "1", ")", ",", "1", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\"`action_dist` must be a probability distribution\"", ")", "\n", "\n", "", "if", "n_folds", "==", "1", ":", "\n", "            ", "self", ".", "fit", "(", "\n", "context", "=", "context", ",", "\n", "action", "=", "action", ",", "\n", "position", "=", "position", ",", "\n", "action_dist", "=", "action_dist", ",", "\n", "random_state", "=", "random_state", ",", "\n", ")", "\n", "return", "self", ".", "predict", "(", "context", "=", "context", ",", "action", "=", "action", ",", "position", "=", "position", ")", "\n", "", "else", ":", "\n", "            ", "estimated_importance_weights", "=", "np", ".", "zeros", "(", "n", ")", "\n", "", "kf", "=", "KFold", "(", "n_splits", "=", "n_folds", ",", "shuffle", "=", "True", ",", "random_state", "=", "random_state", ")", "\n", "kf", ".", "get_n_splits", "(", "context", ")", "\n", "if", "evaluate_model_performance", ":", "\n", "            ", "self", ".", "eval_result", "=", "{", "\"y\"", ":", "[", "]", ",", "\"proba\"", ":", "[", "]", "}", "\n", "", "for", "train_idx", ",", "test_idx", "in", "kf", ".", "split", "(", "context", ")", ":", "\n", "            ", "self", ".", "fit", "(", "\n", "context", "=", "context", "[", "train_idx", "]", ",", "\n", "action", "=", "action", "[", "train_idx", "]", ",", "\n", "position", "=", "position", "[", "train_idx", "]", ",", "\n", "action_dist", "=", "action_dist", "[", "train_idx", "]", ",", "\n", "random_state", "=", "random_state", ",", "\n", ")", "\n", "estimated_importance_weights", "[", "test_idx", "]", "=", "self", ".", "predict", "(", "\n", "context", "=", "context", "[", "test_idx", "]", ",", "\n", "action", "=", "action", "[", "test_idx", "]", ",", "\n", "position", "=", "position", "[", "test_idx", "]", ",", "\n", ")", "\n", "if", "evaluate_model_performance", ":", "\n", "                ", "sampled_action", "=", "np", ".", "zeros", "(", "test_idx", ".", "shape", "[", "0", "]", ",", "dtype", "=", "int", ")", "\n", "if", "self", ".", "fitting_method", "==", "\"sample\"", ":", "\n", "                    ", "for", "pos_", "in", "np", ".", "arange", "(", "self", ".", "len_list", ")", ":", "\n", "                        ", "idx", "=", "position", "[", "test_idx", "]", "==", "pos_", "\n", "sampled_action_at_position", "=", "sample_action_fast", "(", "\n", "action_dist", "=", "action_dist", "[", "test_idx", "]", "[", "idx", "]", "[", ":", ",", ":", ",", "pos_", "]", ",", "\n", "random_state", "=", "random_state", ",", "\n", ")", "\n", "sampled_action", "[", "idx", "]", "=", "sampled_action_at_position", "\n", "", "", "for", "pos_", "in", "np", ".", "arange", "(", "self", ".", "len_list", ")", ":", "\n", "                    ", "idx", "=", "position", "[", "test_idx", "]", "==", "pos_", "\n", "action_dist_at_pos", "=", "action_dist", "[", "test_idx", "]", "[", "idx", "]", "[", ":", ",", ":", ",", "pos_", "]", "\n", "X", ",", "y", "=", "self", ".", "_pre_process_for_clf_model", "(", "\n", "context", "=", "context", "[", "test_idx", "]", "[", "idx", "]", ",", "\n", "action", "=", "action", "[", "test_idx", "]", "[", "idx", "]", ",", "\n", "action_dist_at_pos", "=", "action_dist_at_pos", ",", "\n", "sampled_action_at_position", "=", "sampled_action", "[", "idx", "]", ",", "\n", ")", "\n", "proba_eval_policy", "=", "self", ".", "base_model_list", "[", "pos_", "]", ".", "predict_proba", "(", "X", ")", "[", "\n", ":", ",", "1", "\n", "]", "\n", "self", ".", "eval_result", "[", "\"proba\"", "]", ".", "append", "(", "proba_eval_policy", ")", "\n", "self", ".", "eval_result", "[", "\"y\"", "]", ".", "append", "(", "y", ")", "\n", "", "", "", "return", "estimated_importance_weights", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.classification_model.ImportanceWeightEstimator._pre_process_for_clf_model": [[350, 395], ["numpy.copy", "numpy.zeros", "numpy.ones"], "methods", ["None"], ["", "def", "_pre_process_for_clf_model", "(", "\n", "self", ",", "\n", "context", ":", "np", ".", "ndarray", ",", "\n", "action", ":", "np", ".", "ndarray", ",", "\n", "action_dist_at_pos", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "sampled_action_at_position", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "is_prediction", ":", "bool", "=", "False", ",", "\n", ")", "->", "np", ".", "ndarray", ":", "\n", "        ", "\"\"\"Preprocess feature vectors and output labels to train a classification model.\n\n        Note\n        -----\n        Please override this method if you want to use another feature enginnering\n        for training the classification model.\n\n        Parameters\n        -----------\n        context: array-like, shape (n_rounds,)\n            Context vectors observed for each data in logged bandit data, i.e., :math:`x_i`.\n\n        action: array-like, shape (n_rounds,)\n            Actions sampled by the logging/behavior policy for each data in logged bandit data, i.e., :math:`a_i`.\n\n\n        action_dist_at_pos: array-like, shape (n_rounds, n_actions,)\n            Action choice probabilities of the evaluation policy of each position (can be deterministic), i.e., :math:`\\\\pi_e(a_i|x_i)`.\n\n        sampled_action_at_position: array-like, shape (n_rounds, n_actions,)\n            Actions sampled by evaluation policy for each data at each position.\n\n        \"\"\"", "\n", "behavior_policy_feature", "=", "np", ".", "c_", "[", "context", ",", "self", ".", "action_context", "[", "action", "]", "]", "\n", "if", "is_prediction", ":", "\n", "            ", "return", "behavior_policy_feature", ",", "None", "\n", "", "if", "self", ".", "fitting_method", "==", "\"raw\"", ":", "\n", "            ", "evaluation_policy_feature", "=", "np", ".", "c_", "[", "context", ",", "action_dist_at_pos", "]", "\n", "", "elif", "self", ".", "fitting_method", "==", "\"sample\"", ":", "\n", "            ", "evaluation_policy_feature", "=", "np", ".", "c_", "[", "\n", "context", ",", "self", ".", "action_context", "[", "sampled_action_at_position", "]", "\n", "]", "\n", "", "X", "=", "np", ".", "copy", "(", "behavior_policy_feature", ")", "\n", "y", "=", "np", ".", "zeros", "(", "X", ".", "shape", "[", "0", "]", ",", "dtype", "=", "int", ")", "\n", "X", "=", "np", ".", "r_", "[", "X", ",", "evaluation_policy_feature", "]", "\n", "y", "=", "np", ".", "r_", "[", "y", ",", "np", ".", "ones", "(", "evaluation_policy_feature", ".", "shape", "[", "0", "]", ",", "dtype", "=", "int", ")", "]", "\n", "return", "X", ",", "y", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.classification_model.PropensityScoreEstimator.__post_init__": [[429, 451], ["sklearn.utils.check_scalar", "sklearn.utils.check_scalar", "sklearn.utils.check_scalar", "isinstance", "ValueError", "sklearn.base.clone", "sklearn.base.clone", "sklearn.calibration.CalibratedClassifierCV", "numpy.arange", "numpy.arange"], "methods", ["None"], ["def", "__post_init__", "(", "self", ")", "->", "None", ":", "\n", "        ", "\"\"\"Initialize Class.\"\"\"", "\n", "check_scalar", "(", "self", ".", "n_actions", ",", "\"n_actions\"", ",", "int", ",", "min_val", "=", "2", ")", "\n", "check_scalar", "(", "self", ".", "len_list", ",", "\"len_list\"", ",", "int", ",", "min_val", "=", "1", ")", "\n", "check_scalar", "(", "self", ".", "calibration_cv", ",", "\"calibration_cv\"", ",", "int", ")", "\n", "if", "not", "isinstance", "(", "self", ".", "base_model", ",", "BaseEstimator", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"`base_model` must be BaseEstimator or a child class of BaseEstimator\"", "\n", ")", "\n", "\n", "", "if", "self", ".", "calibration_cv", ">", "1", ":", "\n", "            ", "self", ".", "base_model_list", "=", "[", "\n", "clone", "(", "\n", "CalibratedClassifierCV", "(", "\n", "base_estimator", "=", "self", ".", "base_model", ",", "cv", "=", "self", ".", "calibration_cv", "\n", ")", ",", "\n", ")", "\n", "for", "_", "in", "np", ".", "arange", "(", "self", ".", "len_list", ")", "\n", "]", "\n", "", "else", ":", "\n", "            ", "self", ".", "base_model_list", "=", "[", "\n", "clone", "(", "self", ".", "base_model", ")", "for", "_", "in", "np", ".", "arange", "(", "self", ".", "len_list", ")", "\n", "]", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.classification_model.PropensityScoreEstimator.fit": [[453, 496], ["utils.check_bandit_feedback_inputs", "numpy.arange", "numpy.zeros_like", "classification_model.PropensityScoreEstimator.base_model_list[].fit", "numpy.zeros_like", "numpy.eye", "numpy.zeros_like.max", "ValueError", "ValueError", "numpy.zeros_like.max"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_bandit_feedback_inputs", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.classification_model.PropensityScoreEstimator.fit"], ["", "", "def", "fit", "(", "\n", "self", ",", "\n", "context", ":", "np", ".", "ndarray", ",", "\n", "action", ":", "np", ".", "ndarray", ",", "\n", "position", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", ")", "->", "None", ":", "\n", "        ", "\"\"\"Fit the classification model on given logged bandit data.\n\n        Parameters\n        ----------\n        context: array-like, shape (n_rounds, dim_context)\n            Context vectors observed for each data in logged bandit data, i.e., :math:`x_i`.\n\n        action: array-like, shape (n_rounds,)\n            Actions sampled by the logging/behavior policy for each data in logged bandit data, i.e., :math:`a_i`.\n\n        position: array-like, shape (n_rounds,), default=None\n            Indices to differentiate positions in a recommendation interface where the actions are presented.\n            If None, a classification model assumes that there is only a single position in  a recommendation interface.\n            When `len_list` > 1, an array must be given as `position`.\n\n        \"\"\"", "\n", "check_bandit_feedback_inputs", "(", "\n", "context", "=", "context", ",", "\n", "action", "=", "action", ",", "\n", "reward", "=", "np", ".", "zeros_like", "(", "action", ")", ",", "# use dummy reward", "\n", "position", "=", "position", ",", "\n", "action_context", "=", "np", ".", "eye", "(", "self", ".", "n_actions", ",", "dtype", "=", "int", ")", ",", "\n", ")", "\n", "\n", "if", "position", "is", "None", "or", "self", ".", "len_list", "==", "1", ":", "\n", "            ", "position", "=", "np", ".", "zeros_like", "(", "action", ")", "\n", "", "else", ":", "\n", "            ", "if", "position", ".", "max", "(", ")", ">=", "self", ".", "len_list", ":", "\n", "                ", "raise", "ValueError", "(", "\n", "f\"`position` elements must be smaller than `len_list`, but the maximum value is {position.max()} (>= {self.len_list})\"", "\n", ")", "\n", "\n", "", "", "for", "pos_", "in", "np", ".", "arange", "(", "self", ".", "len_list", ")", ":", "\n", "            ", "idx", "=", "position", "==", "pos_", "\n", "if", "context", "[", "idx", "]", ".", "shape", "[", "0", "]", "==", "0", ":", "\n", "                ", "raise", "ValueError", "(", "f\"No training data at position {pos_}\"", ")", "\n", "", "self", ".", "base_model_list", "[", "pos_", "]", ".", "fit", "(", "context", "[", "idx", "]", ",", "action", "[", "idx", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.classification_model.PropensityScoreEstimator.predict": [[497, 533], ["numpy.zeros", "numpy.arange", "classification_model.PropensityScoreEstimator.base_model_list[].predict_proba", "numpy.arange"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.policy.offline.NNPolicyLearner.predict_proba"], ["", "", "def", "predict", "(", "\n", "self", ",", "\n", "context", ":", "np", ".", "ndarray", ",", "\n", "action", ":", "np", ".", "ndarray", ",", "\n", "position", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", ")", "->", "np", ".", "ndarray", ":", "\n", "        ", "\"\"\"Predict the propensity scores.\n\n        Parameters\n        ----------\n        context: array-like, shape (n_rounds_of_new_data, dim_context)\n            Context vectors observed for each data in logged bandit data, i.e., :math:`x_i`.\n\n        action: array-like, shape (n_rounds_of_new_data,)\n            Actions sampled by the logging/behavior policy for each data in logged bandit data, i.e., :math:`a_i`.\n\n        position: array-like, shape (n_rounds_of_new_data,), default=None\n            Indices to differentiate positions in a recommendation interface where the actions are presented.\n            If None, a classification model assumes that there is only a single position in  a recommendation interface.\n            When `len_list` > 1, an array must be given as `position`.\n\n        Returns\n        ----------\n        estimated_pscore: array-like, shape (n_rounds_of_new_data, )\n            Estimated propensity scores, i.e., :math:`\\\\hat{\\\\pi}_b (a \\\\mid x)`.\n\n        \"\"\"", "\n", "estimated_pscore", "=", "np", ".", "zeros", "(", "action", ".", "shape", "[", "0", "]", ")", "\n", "for", "pos_", "in", "np", ".", "arange", "(", "self", ".", "len_list", ")", ":", "\n", "            ", "idx", "=", "position", "==", "pos_", "\n", "if", "context", "[", "idx", "]", ".", "shape", "[", "0", "]", "==", "0", ":", "\n", "                ", "continue", "\n", "", "estimated_pscore", "[", "idx", "]", "=", "self", ".", "base_model_list", "[", "pos_", "]", ".", "predict_proba", "(", "\n", "context", "[", "idx", "]", "\n", ")", "[", "np", ".", "arange", "(", "action", "[", "idx", "]", ".", "shape", "[", "0", "]", ")", ",", "action", "[", "idx", "]", "]", "\n", "", "return", "estimated_pscore", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.classification_model.PropensityScoreEstimator.fit_predict": [[534, 634], ["utils.check_bandit_feedback_inputs", "sklearn.utils.check_scalar", "sklearn.utils.check_random_state", "sklearn.model_selection.KFold", "sklearn.model_selection.KFold.get_n_splits", "sklearn.model_selection.KFold.split", "numpy.zeros_like", "classification_model.PropensityScoreEstimator.fit", "classification_model.PropensityScoreEstimator.predict", "numpy.zeros", "classification_model.PropensityScoreEstimator.fit", "classification_model.PropensityScoreEstimator.predict", "numpy.zeros_like", "numpy.eye", "numpy.zeros_like.max", "ValueError", "numpy.arange", "classification_model.PropensityScoreEstimator.base_model_list[].predict_proba", "classification_model.PropensityScoreEstimator.eval_result[].append", "classification_model.PropensityScoreEstimator.eval_result[].append", "numpy.zeros_like.max"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_bandit_feedback_inputs", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.classification_model.PropensityScoreEstimator.fit", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.classification_model.PropensityScoreEstimator.predict", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.classification_model.PropensityScoreEstimator.fit", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.classification_model.PropensityScoreEstimator.predict", "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.offline.NNPolicyLearner.predict_proba"], ["", "def", "fit_predict", "(", "\n", "self", ",", "\n", "context", ":", "np", ".", "ndarray", ",", "\n", "action", ":", "np", ".", "ndarray", ",", "\n", "position", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "n_folds", ":", "int", "=", "1", ",", "\n", "random_state", ":", "Optional", "[", "int", "]", "=", "None", ",", "\n", "evaluate_model_performance", ":", "bool", "=", "False", ",", "\n", ")", "->", "np", ".", "ndarray", ":", "\n", "        ", "\"\"\"Fit the classification model on given logged bandit data and predict the propensity score on the same data, possibly using the cross-fitting procedure to avoid over-fitting.\n\n        Note\n        ------\n        When `n_folds` is larger than 1, the cross-fitting procedure is applied.\n\n        Parameters\n        ----------\n        context: array-like, shape (n_rounds, dim_context)\n            Context vectors observed for each data in logged bandit data, i.e., :math:`x_i`.\n\n        action: array-like, shape (n_rounds,)\n            Actions sampled by the logging/behavior policy for each data in logged bandit data, i.e., :math:`a_i`.\n\n        position: array-like, shape (n_rounds,), default=None\n            Indices to differentiate positions in a recommendation interface where the actions are presented.\n            If None, a classification model assumes that there is only a single position.\n            When `len_list` > 1, an array must be given as `position`.\n\n        n_folds: int, default=1\n            Number of folds in the cross-fitting procedure.\n            When 1 is given, the classification model is trained on the whole logged bandit data.\n            Please refer to https://arxiv.org/abs/2002.08536 about the details of the cross-fitting procedure.\n\n        random_state: int, default=None\n            `random_state` affects the ordering of the indices, which controls the randomness of each fold.\n            See https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html for the details.\n\n        evaluate_model_performance: bool, default=False\n            Whether the performance of the classification model is evaluated or not.\n            If True, the predicted probability of the classification model and the true label of each fold is saved in `self.eval_result[fold]`\n\n        Returns\n        -----------\n        estimated_pscore: array-like, shape (n_rounds_of_new_data, )\n            Estimated propensity score, i.e., :math:`\\\\hat{\\\\pi}_b (a \\\\mid x)`.\n\n        \"\"\"", "\n", "check_bandit_feedback_inputs", "(", "\n", "context", "=", "context", ",", "\n", "action", "=", "action", ",", "\n", "reward", "=", "np", ".", "zeros_like", "(", "action", ")", ",", "# use dummy reward", "\n", "position", "=", "position", ",", "\n", "action_context", "=", "np", ".", "eye", "(", "self", ".", "n_actions", ",", "dtype", "=", "int", ")", ",", "\n", ")", "\n", "if", "position", "is", "None", "or", "self", ".", "len_list", "==", "1", ":", "\n", "            ", "position", "=", "np", ".", "zeros_like", "(", "action", ")", "\n", "", "else", ":", "\n", "            ", "if", "position", ".", "max", "(", ")", ">=", "self", ".", "len_list", ":", "\n", "                ", "raise", "ValueError", "(", "\n", "f\"`position` elements must be smaller than `len_list`, but the maximum value is {position.max()} (>= {self.len_list})\"", "\n", ")", "\n", "", "", "check_scalar", "(", "n_folds", ",", "\"n_folds\"", ",", "int", ",", "min_val", "=", "1", ")", "\n", "check_random_state", "(", "random_state", ")", "\n", "\n", "if", "n_folds", "==", "1", ":", "\n", "            ", "self", ".", "fit", "(", "\n", "context", "=", "context", ",", "\n", "action", "=", "action", ",", "\n", "position", "=", "position", ",", "\n", ")", "\n", "return", "self", ".", "predict", "(", "context", "=", "context", ",", "action", "=", "action", ",", "position", "=", "position", ")", "\n", "", "else", ":", "\n", "            ", "estimated_pscore", "=", "np", ".", "zeros", "(", "context", ".", "shape", "[", "0", "]", ")", "\n", "", "kf", "=", "KFold", "(", "n_splits", "=", "n_folds", ",", "shuffle", "=", "True", ",", "random_state", "=", "random_state", ")", "\n", "kf", ".", "get_n_splits", "(", "context", ")", "\n", "if", "evaluate_model_performance", ":", "\n", "            ", "self", ".", "eval_result", "=", "{", "\"y\"", ":", "[", "]", ",", "\"proba\"", ":", "[", "]", "}", "\n", "", "for", "train_idx", ",", "test_idx", "in", "kf", ".", "split", "(", "context", ")", ":", "\n", "            ", "self", ".", "fit", "(", "\n", "context", "=", "context", "[", "train_idx", "]", ",", "\n", "action", "=", "action", "[", "train_idx", "]", ",", "\n", "position", "=", "position", "[", "train_idx", "]", ",", "\n", ")", "\n", "\n", "estimated_pscore", "[", "test_idx", "]", "=", "self", ".", "predict", "(", "\n", "context", "=", "context", "[", "test_idx", "]", ",", "\n", "action", "=", "action", "[", "test_idx", "]", ",", "\n", "position", "=", "position", "[", "test_idx", "]", ",", "\n", ")", "\n", "if", "evaluate_model_performance", ":", "\n", "                ", "for", "pos_", "in", "np", ".", "arange", "(", "self", ".", "len_list", ")", ":", "\n", "                    ", "idx", "=", "position", "[", "test_idx", "]", "==", "pos_", "\n", "if", "context", "[", "test_idx", "]", "[", "idx", "]", ".", "shape", "[", "0", "]", "==", "0", ":", "\n", "                        ", "continue", "\n", "", "proba_eval", "=", "self", ".", "base_model_list", "[", "pos_", "]", ".", "predict_proba", "(", "\n", "context", "[", "test_idx", "]", "[", "idx", "]", "\n", ")", "\n", "self", ".", "eval_result", "[", "\"proba\"", "]", ".", "append", "(", "proba_eval", ")", "\n", "self", ".", "eval_result", "[", "\"y\"", "]", ".", "append", "(", "action", "[", "test_idx", "]", "[", "idx", "]", ")", "\n", "", "", "", "return", "estimated_pscore", "\n", "", "", ""]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_tuning.BaseOffPolicyEstimatorTuning.__new__": [[76, 79], ["dataclasses.dataclass", "super().__new__"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_tuning.BaseOffPolicyEstimatorTuning.__new__"], ["def", "__new__", "(", "cls", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "dataclass", "(", "cls", ")", "\n", "return", "super", "(", ")", ".", "__new__", "(", "cls", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_tuning.BaseOffPolicyEstimatorTuning._check_lambdas": [[80, 97], ["isinstance", "TypeError", "len", "ValueError", "sklearn.utils.check_scalar", "ValueError"], "methods", ["None"], ["", "def", "_check_lambdas", "(", "self", ",", "min_val", ":", "float", "=", "0.0", ",", "max_val", ":", "float", "=", "np", ".", "inf", ")", "->", "None", ":", "\n", "        ", "\"\"\"Check type and value of lambdas.\"\"\"", "\n", "if", "isinstance", "(", "self", ".", "lambdas", ",", "list", ")", ":", "\n", "            ", "if", "len", "(", "self", ".", "lambdas", ")", "==", "0", ":", "\n", "                ", "raise", "ValueError", "(", "\"lambdas must not be empty\"", ")", "\n", "", "for", "hyperparam_", "in", "self", ".", "lambdas", ":", "\n", "                ", "check_scalar", "(", "\n", "hyperparam_", ",", "\n", "name", "=", "\"an element of lambdas\"", ",", "\n", "target_type", "=", "(", "int", ",", "float", ")", ",", "\n", "min_val", "=", "min_val", ",", "\n", "max_val", "=", "max_val", ",", "\n", ")", "\n", "if", "hyperparam_", "!=", "hyperparam_", ":", "\n", "                    ", "raise", "ValueError", "(", "\"an element of lambdas must not be nan\"", ")", "\n", "", "", "", "else", ":", "\n", "            ", "raise", "TypeError", "(", "\"lambdas must be a list\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_tuning.BaseOffPolicyEstimatorTuning._check_init_inputs": [[98, 114], ["sklearn.utils.check_scalar", "ValueError", "isinstance", "TypeError", "isinstance", "TypeError", "type", "type"], "methods", ["None"], ["", "", "def", "_check_init_inputs", "(", "self", ")", "->", "None", ":", "\n", "        ", "\"\"\"Initialize Class.\"\"\"", "\n", "if", "self", ".", "tuning_method", "not", "in", "[", "\"slope\"", ",", "\"mse\"", "]", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"`tuning_method` must be either 'slope' or 'mse'\"", "\n", "f\", but {self.tuning_method} is given\"", "\n", ")", "\n", "", "if", "not", "isinstance", "(", "self", ".", "use_bias_upper_bound", ",", "bool", ")", ":", "\n", "            ", "raise", "TypeError", "(", "\n", "\"`use_bias_upper_bound` must be a bool\"", "\n", "f\", but {type(self.use_bias_upper_bound)} is given\"", "\n", ")", "\n", "", "check_scalar", "(", "self", ".", "delta", ",", "\"delta\"", ",", "(", "float", ")", ",", "min_val", "=", "0.0", ",", "max_val", "=", "1.0", ")", "\n", "if", "not", "isinstance", "(", "self", ".", "use_estimated_pscore", ",", "bool", ")", ":", "\n", "            ", "raise", "TypeError", "(", "\n", "f\"`use_estimated_pscore` must be a bool, but {type(self.use_estimated_pscore)} is given\"", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_tuning.BaseOffPolicyEstimatorTuning._tune_hyperparam_with_mse": [[116, 142], ["dict", "estimators_tuning.BaseOffPolicyEstimatorTuning.base_ope_estimator()._estimate_mse_score", "min", "estimators_tuning.BaseOffPolicyEstimatorTuning.estimated_mse_score_dict.items", "estimators_tuning.BaseOffPolicyEstimatorTuning.base_ope_estimator"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators.SubGaussianDoublyRobust._estimate_mse_score"], ["", "", "def", "_tune_hyperparam_with_mse", "(", "\n", "self", ",", "\n", "reward", ":", "np", ".", "ndarray", ",", "\n", "action", ":", "np", ".", "ndarray", ",", "\n", "pscore", ":", "np", ".", "ndarray", ",", "\n", "action_dist", ":", "np", ".", "ndarray", ",", "\n", "estimated_rewards_by_reg_model", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "position", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", ")", "->", "float", ":", "\n", "        ", "\"\"\"Find the best hyperparameter value from the candidate set by estimating the mse.\"\"\"", "\n", "self", ".", "estimated_mse_score_dict", "=", "dict", "(", ")", "\n", "for", "hyperparam_", "in", "self", ".", "lambdas", ":", "\n", "            ", "estimated_mse_score", "=", "self", ".", "base_ope_estimator", "(", "\n", "lambda_", "=", "hyperparam_", ",", "use_estimated_pscore", "=", "self", ".", "use_estimated_pscore", "\n", ")", ".", "_estimate_mse_score", "(", "\n", "reward", "=", "reward", ",", "\n", "action", "=", "action", ",", "\n", "pscore", "=", "pscore", ",", "\n", "action_dist", "=", "action_dist", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", "position", "=", "position", ",", "\n", "use_bias_upper_bound", "=", "self", ".", "use_bias_upper_bound", ",", "\n", "delta", "=", "self", ".", "delta", ",", "\n", ")", "\n", "self", ".", "estimated_mse_score_dict", "[", "hyperparam_", "]", "=", "estimated_mse_score", "\n", "", "return", "min", "(", "self", ".", "estimated_mse_score_dict", ".", "items", "(", ")", ",", "key", "=", "lambda", "x", ":", "x", "[", "1", "]", ")", "[", "0", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_tuning.BaseOffPolicyEstimatorTuning._tune_hyperparam_with_slope": [[143, 190], ["enumerate", "numpy.sqrt", "estimators_tuning.BaseOffPolicyEstimatorTuning.base_ope_estimator()._estimate_round_rewards", "theta_list_for_sort.append", "estimators_tuning.BaseOffPolicyEstimatorTuning.mean", "helper.estimate_student_t_lower_bound", "cnf_list_for_sort.append", "numpy.argsort", "estimators_tuning.BaseOffPolicyEstimatorTuning.mean", "len", "estimators_tuning.BaseOffPolicyEstimatorTuning.base_ope_estimator", "theta_list.append", "cnf_list.append", "numpy.array", "numpy.array", "theta_list.append", "cnf_list.append", "numpy.abs"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SelfNormalizedSlateStandardIPS._estimate_round_rewards", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.helper.estimate_student_t_lower_bound"], ["", "def", "_tune_hyperparam_with_slope", "(", "\n", "self", ",", "\n", "reward", ":", "np", ".", "ndarray", ",", "\n", "action", ":", "np", ".", "ndarray", ",", "\n", "pscore", ":", "np", ".", "ndarray", ",", "\n", "action_dist", ":", "np", ".", "ndarray", ",", "\n", "estimated_rewards_by_reg_model", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "position", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", ")", "->", "float", ":", "\n", "        ", "\"\"\"Find the best hyperparameter value from the candidate set by SLOPE.\"\"\"", "\n", "C", "=", "np", ".", "sqrt", "(", "6", ")", "-", "1", "\n", "theta_list_for_sort", ",", "cnf_list_for_sort", "=", "[", "]", ",", "[", "]", "\n", "for", "hyperparam_", "in", "self", ".", "lambdas", ":", "\n", "            ", "estimated_round_rewards", "=", "self", ".", "base_ope_estimator", "(", "\n", "hyperparam_", "\n", ")", ".", "_estimate_round_rewards", "(", "\n", "reward", "=", "reward", ",", "\n", "action", "=", "action", ",", "\n", "pscore", "=", "pscore", ",", "\n", "action_dist", "=", "action_dist", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", "position", "=", "position", ",", "\n", ")", "\n", "theta_list_for_sort", ".", "append", "(", "estimated_round_rewards", ".", "mean", "(", ")", ")", "\n", "cnf", "=", "estimated_round_rewards", ".", "mean", "(", ")", "\n", "cnf", "-=", "estimate_student_t_lower_bound", "(", "\n", "x", "=", "estimated_round_rewards", ",", "\n", "delta", "=", "self", ".", "delta", ",", "\n", ")", "\n", "cnf_list_for_sort", ".", "append", "(", "cnf", ")", "\n", "\n", "", "theta_list", ",", "cnf_list", "=", "[", "]", ",", "[", "]", "\n", "sorted_idx_list", "=", "np", ".", "argsort", "(", "cnf_list_for_sort", ")", "[", ":", ":", "-", "1", "]", "\n", "for", "i", ",", "idx", "in", "enumerate", "(", "sorted_idx_list", ")", ":", "\n", "            ", "cnf_i", "=", "cnf_list_for_sort", "[", "idx", "]", "\n", "theta_i", "=", "theta_list_for_sort", "[", "idx", "]", "\n", "if", "len", "(", "theta_list", ")", "<", "1", ":", "\n", "                ", "theta_list", ".", "append", "(", "theta_i", ")", ",", "cnf_list", ".", "append", "(", "cnf_i", ")", "\n", "", "else", ":", "\n", "                ", "theta_j", ",", "cnf_j", "=", "np", ".", "array", "(", "theta_list", ")", ",", "np", ".", "array", "(", "cnf_list", ")", "\n", "if", "(", "np", ".", "abs", "(", "theta_j", "-", "theta_i", ")", "<=", "cnf_i", "+", "C", "*", "cnf_j", ")", ".", "all", "(", ")", ":", "\n", "                    ", "theta_list", ".", "append", "(", "theta_i", ")", ",", "cnf_list", ".", "append", "(", "cnf_i", ")", "\n", "", "else", ":", "\n", "                    ", "best_idx", "=", "sorted_idx_list", "[", "i", "-", "1", "]", "\n", "return", "self", ".", "lambdas", "[", "best_idx", "]", "\n", "\n", "", "", "", "return", "self", ".", "lambdas", "[", "sorted_idx_list", "[", "-", "1", "]", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_tuning.BaseOffPolicyEstimatorTuning.estimate_policy_value_with_tuning": [[191, 273], ["estimators_tuning.BaseOffPolicyEstimatorTuning.base_ope_estimator().estimate_policy_value", "utils.check_array", "utils.check_array", "hasattr", "estimators_tuning.BaseOffPolicyEstimatorTuning._tune_hyperparam_with_mse", "estimators_tuning.BaseOffPolicyEstimatorTuning.base_ope_estimator", "estimators_tuning.BaseOffPolicyEstimatorTuning._tune_hyperparam_with_slope"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_policy_value", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_tuning.BaseOffPolicyEstimatorTuning._tune_hyperparam_with_mse", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_tuning.BaseOffPolicyEstimatorTuning._tune_hyperparam_with_slope"], ["", "def", "estimate_policy_value_with_tuning", "(", "\n", "self", ",", "\n", "reward", ":", "np", ".", "ndarray", ",", "\n", "action", ":", "np", ".", "ndarray", ",", "\n", "action_dist", ":", "np", ".", "ndarray", ",", "\n", "estimated_rewards_by_reg_model", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "pscore", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "position", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "estimated_pscore", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", ")", "->", "float", ":", "\n", "        ", "\"\"\"Estimate the policy value of evaluation policy with a tuned hyperparameter.\n\n        Parameters\n        ----------\n        reward: array-like, shape (n_rounds,)\n            Rewards observed for each data in logged bandit data, i.e., :math:`r_i`.\n\n        action: array-like, shape (n_rounds,)\n            Actions sampled by the logging/behavior policy for each data in logged bandit data, i.e., :math:`a_i`.\n\n        action_dist: array-like, shape (n_rounds, n_actions, len_list)\n            Action choice probabilities of the evaluation policy (can be deterministic), i.e., :math:`\\\\pi_e(a_i|x_i)`.\n\n        estimated_rewards_by_reg_model: array-like, shape (n_rounds, n_actions, len_list), default=None\n            Estimated expected rewards given context, action, and position, i.e., :math:`\\\\hat{q}(x_i,a_i)`.\n\n        pscore: array-like, shape (n_rounds,), default=None\n            Action choice probabilities of the logging/behavior policy (propensity scores), i.e., :math:`\\\\pi_b(a_i|x_i)`.\n            If `use_estimated_pscore` is False, `pscore` must be given.\n\n        position: array-like, shape (n_rounds,), default=None\n            Indices to differentiate positions in a recommendation interface where the actions are presented.\n            If None, the effect of position on the reward will be ignored.\n            (If only a single action is chosen for each data, you can just ignore this argument.)\n\n        estimated_pscore: array-like, shape (n_rounds,), default=None\n            Estimated behavior policy (propensity scores), i.e., :math:`\\\\hat{\\\\pi}_b(a_i|x_i)`.\n            If `self.use_estimated_pscore` is True, `estimated_pscore` must be given.\n\n        Returns\n        ----------\n        V_hat: float\n            Estimated policy value of evaluation policy.\n\n        \"\"\"", "\n", "if", "self", ".", "use_estimated_pscore", ":", "\n", "            ", "check_array", "(", "array", "=", "estimated_pscore", ",", "name", "=", "\"estimated_pscore\"", ",", "expected_dim", "=", "1", ")", "\n", "pscore_", "=", "estimated_pscore", "\n", "", "else", ":", "\n", "            ", "check_array", "(", "array", "=", "pscore", ",", "name", "=", "\"pscore\"", ",", "expected_dim", "=", "1", ")", "\n", "pscore_", "=", "pscore", "\n", "# tune hyperparameter if necessary", "\n", "", "if", "not", "hasattr", "(", "self", ",", "\"best_hyperparam\"", ")", ":", "\n", "            ", "if", "self", ".", "tuning_method", "==", "\"mse\"", ":", "\n", "                ", "self", ".", "best_hyperparam", "=", "self", ".", "_tune_hyperparam_with_mse", "(", "\n", "reward", "=", "reward", ",", "\n", "action", "=", "action", ",", "\n", "pscore", "=", "pscore_", ",", "\n", "action_dist", "=", "action_dist", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", "position", "=", "position", ",", "\n", ")", "\n", "", "elif", "self", ".", "tuning_method", "==", "\"slope\"", ":", "\n", "                ", "self", ".", "best_hyperparam", "=", "self", ".", "_tune_hyperparam_with_slope", "(", "\n", "reward", "=", "reward", ",", "\n", "action", "=", "action", ",", "\n", "pscore", "=", "pscore_", ",", "\n", "action_dist", "=", "action_dist", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", "position", "=", "position", ",", "\n", ")", "\n", "\n", "", "", "return", "self", ".", "base_ope_estimator", "(", "\n", "lambda_", "=", "self", ".", "best_hyperparam", ",", "use_estimated_pscore", "=", "self", ".", "use_estimated_pscore", "\n", ")", ".", "estimate_policy_value", "(", "\n", "reward", "=", "reward", ",", "\n", "action", "=", "action", ",", "\n", "position", "=", "position", ",", "\n", "pscore", "=", "pscore_", ",", "\n", "estimated_pscore", "=", "estimated_pscore", ",", "\n", "action_dist", "=", "action_dist", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_tuning.BaseOffPolicyEstimatorTuning.estimate_interval_with_tuning": [[275, 371], ["estimators_tuning.BaseOffPolicyEstimatorTuning.base_ope_estimator().estimate_interval", "utils.check_array", "utils.check_array", "hasattr", "estimators_tuning.BaseOffPolicyEstimatorTuning._tune_hyperparam_with_mse", "estimators_tuning.BaseOffPolicyEstimatorTuning.base_ope_estimator", "estimators_tuning.BaseOffPolicyEstimatorTuning._tune_hyperparam_with_slope"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_interval", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_tuning.BaseOffPolicyEstimatorTuning._tune_hyperparam_with_mse", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_tuning.BaseOffPolicyEstimatorTuning._tune_hyperparam_with_slope"], ["", "def", "estimate_interval_with_tuning", "(", "\n", "self", ",", "\n", "reward", ":", "np", ".", "ndarray", ",", "\n", "action", ":", "np", ".", "ndarray", ",", "\n", "action_dist", ":", "np", ".", "ndarray", ",", "\n", "estimated_rewards_by_reg_model", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "pscore", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "position", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "estimated_pscore", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "alpha", ":", "float", "=", "0.05", ",", "\n", "n_bootstrap_samples", ":", "int", "=", "10000", ",", "\n", "random_state", ":", "Optional", "[", "int", "]", "=", "None", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "Dict", "[", "str", ",", "float", "]", ":", "\n", "        ", "\"\"\"Estimate the confidence interval of the policy value using bootstrap.\n\n        Parameters\n        ----------\n        reward: array-like, shape (n_rounds,)\n            Rewards observed for each data in logged bandit data, i.e., :math:`r_i`.\n\n        action: array-like, shape (n_rounds,)\n            Actions sampled by the logging/behavior policy for each data in logged bandit data, i.e., :math:`a_i`.\n\n        action_dist: array-like, shape (n_rounds, n_actions, len_list)\n            Action choice probabilities of the evaluation policy (can be deterministic), i.e., :math:`\\\\pi_e(a_i|x_i)`.\n\n        estimated_rewards_by_reg_model: array-like, shape (n_rounds, n_actions, len_list), default=None\n            Estimated expected rewards given context, action, and position, i.e., :math:`\\\\hat{q}(x_i,a_i)`.\n\n        pscore: array-like, shape (n_rounds,), default=None\n            Action choice probabilities of the logging/behavior policy (propensity scores), i.e., :math:`\\\\pi_b(a_i|x_i)`.\n            If `use_estimated_pscore` is False, `pscore` must be given.\n\n        position: array-like, shape (n_rounds,), default=None\n            Indices to differentiate positions in a recommendation interface where the actions are presented.\n            If None, the effect of position on the reward will be ignored.\n            (If only a single action is chosen for each data, you can just ignore this argument.)\n\n        estimated_pscore: array-like, shape (n_rounds,), default=None\n            Estimated behavior policy (propensity scores), i.e., :math:`\\\\hat{\\\\pi}_b(a_i|x_i)`.\n            If `self.use_estimated_pscore` is True, `estimated_pscore` must be given.\n\n        alpha: float, default=0.05\n            Significance level.\n\n        n_bootstrap_samples: int, default=10000\n            Number of resampling performed in bootstrap sampling.\n\n        random_state: int, default=None\n            Controls the random seed in bootstrap sampling.\n\n        Returns\n        ----------\n        estimated_confidence_interval: Dict[str, float]\n            Dictionary storing the estimated mean and upper-lower confidence bounds.\n\n        \"\"\"", "\n", "if", "self", ".", "use_estimated_pscore", ":", "\n", "            ", "check_array", "(", "array", "=", "estimated_pscore", ",", "name", "=", "\"estimated_pscore\"", ",", "expected_dim", "=", "1", ")", "\n", "pscore_", "=", "estimated_pscore", "\n", "", "else", ":", "\n", "            ", "check_array", "(", "array", "=", "pscore", ",", "name", "=", "\"pscore\"", ",", "expected_dim", "=", "1", ")", "\n", "pscore_", "=", "pscore", "\n", "# tune hyperparameter if necessary", "\n", "", "if", "not", "hasattr", "(", "self", ",", "\"best_hyperparam\"", ")", ":", "\n", "            ", "if", "self", ".", "tuning_method", "==", "\"mse\"", ":", "\n", "                ", "self", ".", "best_hyperparam", "=", "self", ".", "_tune_hyperparam_with_mse", "(", "\n", "reward", "=", "reward", ",", "\n", "action", "=", "action", ",", "\n", "pscore", "=", "pscore_", ",", "\n", "action_dist", "=", "action_dist", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", "position", "=", "position", ",", "\n", ")", "\n", "", "elif", "self", ".", "tuning_method", "==", "\"slope\"", ":", "\n", "                ", "self", ".", "best_hyperparam", "=", "self", ".", "_tune_hyperparam_with_slope", "(", "\n", "reward", "=", "reward", ",", "\n", "action", "=", "action", ",", "\n", "pscore", "=", "pscore_", ",", "\n", "action_dist", "=", "action_dist", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", "position", "=", "position", ",", "\n", ")", "\n", "\n", "", "", "return", "self", ".", "base_ope_estimator", "(", "self", ".", "best_hyperparam", ")", ".", "estimate_interval", "(", "\n", "reward", "=", "reward", ",", "\n", "action", "=", "action", ",", "\n", "position", "=", "position", ",", "\n", "pscore", "=", "pscore_", ",", "\n", "estimated_pscore", "=", "estimated_pscore", ",", "\n", "action_dist", "=", "action_dist", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", "alpha", "=", "alpha", ",", "\n", "n_bootstrap_samples", "=", "n_bootstrap_samples", ",", "\n", "random_state", "=", "random_state", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_tuning.InverseProbabilityWeightingTuning.__post_init__": [[417, 423], ["estimators_tuning.BaseOffPolicyEstimatorTuning._check_lambdas", "estimators_tuning.BaseOffPolicyEstimatorTuning._check_init_inputs", "estimators_tuning.InverseProbabilityWeightingTuning.lambdas.sort"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_tuning.BaseOffPolicyEstimatorTuning._check_lambdas", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_tuning.BaseOffPolicyEstimatorTuning._check_init_inputs"], ["def", "__post_init__", "(", "self", ")", "->", "None", ":", "\n", "        ", "\"\"\"Initialize Class.\"\"\"", "\n", "self", ".", "base_ope_estimator", "=", "InverseProbabilityWeighting", "\n", "super", "(", ")", ".", "_check_lambdas", "(", ")", "\n", "super", "(", ")", ".", "_check_init_inputs", "(", ")", "\n", "self", ".", "lambdas", ".", "sort", "(", "reverse", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_tuning.InverseProbabilityWeightingTuning.estimate_policy_value": [[424, 491], ["utils.check_array", "utils.check_array", "utils.check_ope_inputs", "estimators_tuning.BaseOffPolicyEstimatorTuning.estimate_policy_value_with_tuning", "utils.check_array", "utils.check_array", "numpy.zeros"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_ope_inputs", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_tuning.BaseOffPolicyEstimatorTuning.estimate_policy_value_with_tuning", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array"], ["", "def", "estimate_policy_value", "(", "\n", "self", ",", "\n", "reward", ":", "np", ".", "ndarray", ",", "\n", "action", ":", "np", ".", "ndarray", ",", "\n", "action_dist", ":", "np", ".", "ndarray", ",", "\n", "pscore", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "position", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "estimated_pscore", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "np", ".", "ndarray", ":", "\n", "        ", "\"\"\"Estimate the policy value of evaluation policy.\n\n        Parameters\n        ----------\n        reward: array-like, shape (n_rounds,)\n            Rewards observed for each data in logged bandit data, i.e., :math:`r_i`.\n\n        action: array-like, shape (n_rounds,)\n            Actions sampled by the logging/behavior policy for each data in logged bandit data, i.e., :math:`a_i`.\n\n        action_dist: array-like, shape (n_rounds, n_actions, len_list)\n            Action choice probabilities of the evaluation policy (can be deterministic), i.e., :math:`\\\\pi_e(a_i|x_i)`.\n\n        pscore: array-like, shape (n_rounds,), default=None\n            Action choice probabilities of the logging/behavior policy (propensity scores), i.e., :math:`\\\\pi_b(a_i|x_i)`.\n            If `use_estimated_pscore` is False, `pscore` must be given.\n\n        position: array-like, shape (n_rounds,), default=None\n            Indices to differentiate positions in a recommendation interface where the actions are presented.\n            If None, the effect of position on the reward will be ignored.\n            (If only a single action is chosen for each data, you can just ignore this argument.)\n\n        estimated_pscore: array-like, shape (n_rounds,), default=None\n            Estimated behavior policy (propensity scores), i.e., :math:`\\\\hat{\\\\pi}_b(a_i|x_i)`.\n            If `self.use_estimated_pscore` is True, `estimated_pscore` must be given.\n\n        Returns\n        ----------\n        V_hat: float\n            Estimated policy value of evaluation policy.\n\n        \"\"\"", "\n", "check_array", "(", "array", "=", "reward", ",", "name", "=", "\"reward\"", ",", "expected_dim", "=", "1", ")", "\n", "check_array", "(", "array", "=", "action", ",", "name", "=", "\"action\"", ",", "expected_dim", "=", "1", ")", "\n", "if", "self", ".", "use_estimated_pscore", ":", "\n", "            ", "check_array", "(", "array", "=", "estimated_pscore", ",", "name", "=", "\"estimated_pscore\"", ",", "expected_dim", "=", "1", ")", "\n", "pscore_", "=", "estimated_pscore", "\n", "", "else", ":", "\n", "            ", "check_array", "(", "array", "=", "pscore", ",", "name", "=", "\"pscore\"", ",", "expected_dim", "=", "1", ")", "\n", "pscore_", "=", "pscore", "\n", "", "check_ope_inputs", "(", "\n", "action_dist", "=", "action_dist", ",", "\n", "position", "=", "position", ",", "\n", "action", "=", "action", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore", "=", "pscore_", ",", "\n", ")", "\n", "if", "position", "is", "None", ":", "\n", "            ", "position", "=", "np", ".", "zeros", "(", "action_dist", ".", "shape", "[", "0", "]", ",", "dtype", "=", "int", ")", "\n", "\n", "", "return", "super", "(", ")", ".", "estimate_policy_value_with_tuning", "(", "\n", "reward", "=", "reward", ",", "\n", "action", "=", "action", ",", "\n", "position", "=", "position", ",", "\n", "pscore", "=", "pscore_", ",", "\n", "action_dist", "=", "action_dist", ",", "\n", "estimated_pscore", "=", "estimated_pscore", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_tuning.InverseProbabilityWeightingTuning.estimate_interval": [[493, 576], ["utils.check_array", "utils.check_array", "utils.check_ope_inputs", "estimators_tuning.BaseOffPolicyEstimatorTuning.estimate_interval_with_tuning", "utils.check_array", "utils.check_array", "numpy.zeros"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_ope_inputs", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_tuning.BaseOffPolicyEstimatorTuning.estimate_interval_with_tuning", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array"], ["", "def", "estimate_interval", "(", "\n", "self", ",", "\n", "reward", ":", "np", ".", "ndarray", ",", "\n", "action", ":", "np", ".", "ndarray", ",", "\n", "action_dist", ":", "np", ".", "ndarray", ",", "\n", "pscore", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "position", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "estimated_pscore", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "alpha", ":", "float", "=", "0.05", ",", "\n", "n_bootstrap_samples", ":", "int", "=", "10000", ",", "\n", "random_state", ":", "Optional", "[", "int", "]", "=", "None", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "Dict", "[", "str", ",", "float", "]", ":", "\n", "        ", "\"\"\"Estimate the confidence interval of the policy value using bootstrap.\n\n        Parameters\n        ----------\n        reward: array-like, shape (n_rounds,)\n            Rewards observed for each data in logged bandit data, i.e., :math:`r_i`.\n\n        action: array-like, shape (n_rounds,)\n            Actions sampled by the logging/behavior policy for each data in logged bandit data, i.e., :math:`a_i`.\n\n        action_dist: array-like, shape (n_rounds, n_actions, len_list)\n            Action choice probabilities\n            by the evaluation policy (can be deterministic), i.e., :math:`\\\\pi_e(a_i|x_i)`.\n\n        pscore: array-like, shape (n_rounds,), default=None\n            Action choice probabilities of the logging/behavior policy (propensity scores), i.e., :math:`\\\\pi_b(a_i|x_i)`.\n            If `use_estimated_pscore` is False, `pscore` must be given.\n\n        position: array-like, shape (n_rounds,), default=None\n            Indices to differentiate positions in a recommendation interface where the actions are presented.\n            If None, the effect of position on the reward will be ignored.\n           (If only a single action is chosen for each data, you can just ignore this argument.)\n\n        estimated_pscore: array-like, shape (n_rounds,), default=None\n            Estimated behavior policy (propensity scores), i.e., :math:`\\\\hat{\\\\pi}_b(a_i|x_i)`.\n            If `self.use_estimated_pscore` is True, `estimated_pscore` must be given.\n\n        alpha: float, default=0.05\n            Significance level.\n\n        n_bootstrap_samples: int, default=10000\n            Number of resampling performed in bootstrap sampling.\n\n        random_state: int, default=None\n            Controls the random seed in bootstrap sampling.\n\n        Returns\n        ----------\n        estimated_confidence_interval: Dict[str, float]\n            Dictionary storing the estimated mean and upper-lower confidence bounds.\n\n        \"\"\"", "\n", "check_array", "(", "array", "=", "reward", ",", "name", "=", "\"reward\"", ",", "expected_dim", "=", "1", ")", "\n", "check_array", "(", "array", "=", "action", ",", "name", "=", "\"action\"", ",", "expected_dim", "=", "1", ")", "\n", "if", "self", ".", "use_estimated_pscore", ":", "\n", "            ", "check_array", "(", "array", "=", "estimated_pscore", ",", "name", "=", "\"estimated_pscore\"", ",", "expected_dim", "=", "1", ")", "\n", "pscore_", "=", "estimated_pscore", "\n", "", "else", ":", "\n", "            ", "check_array", "(", "array", "=", "pscore", ",", "name", "=", "\"pscore\"", ",", "expected_dim", "=", "1", ")", "\n", "pscore_", "=", "pscore", "\n", "", "check_ope_inputs", "(", "\n", "action_dist", "=", "action_dist", ",", "\n", "position", "=", "position", ",", "\n", "action", "=", "action", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore", "=", "pscore_", ",", "\n", ")", "\n", "if", "position", "is", "None", ":", "\n", "            ", "position", "=", "np", ".", "zeros", "(", "action_dist", ".", "shape", "[", "0", "]", ",", "dtype", "=", "int", ")", "\n", "\n", "", "return", "super", "(", ")", ".", "estimate_interval_with_tuning", "(", "\n", "reward", "=", "reward", ",", "\n", "action", "=", "action", ",", "\n", "position", "=", "position", ",", "\n", "pscore", "=", "pscore_", ",", "\n", "estimated_pscore", "=", "estimated_pscore", ",", "\n", "action_dist", "=", "action_dist", ",", "\n", "alpha", "=", "alpha", ",", "\n", "n_bootstrap_samples", "=", "n_bootstrap_samples", ",", "\n", "random_state", "=", "random_state", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_tuning.DoublyRobustTuning.__post_init__": [[617, 623], ["estimators_tuning.BaseOffPolicyEstimatorTuning._check_lambdas", "estimators_tuning.BaseOffPolicyEstimatorTuning._check_init_inputs", "estimators_tuning.DoublyRobustTuning.lambdas.sort"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_tuning.BaseOffPolicyEstimatorTuning._check_lambdas", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_tuning.BaseOffPolicyEstimatorTuning._check_init_inputs"], ["def", "__post_init__", "(", "self", ")", "->", "None", ":", "\n", "        ", "\"\"\"Initialize Class.\"\"\"", "\n", "self", ".", "base_ope_estimator", "=", "DoublyRobust", "\n", "super", "(", ")", ".", "_check_lambdas", "(", ")", "\n", "super", "(", ")", ".", "_check_init_inputs", "(", ")", "\n", "self", ".", "lambdas", ".", "sort", "(", "reverse", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_tuning.DoublyRobustTuning.estimate_policy_value": [[624, 702], ["utils.check_array", "utils.check_array", "utils.check_array", "utils.check_ope_inputs", "estimators_tuning.BaseOffPolicyEstimatorTuning.estimate_policy_value_with_tuning", "utils.check_array", "utils.check_array", "numpy.zeros"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_ope_inputs", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_tuning.BaseOffPolicyEstimatorTuning.estimate_policy_value_with_tuning", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array"], ["", "def", "estimate_policy_value", "(", "\n", "self", ",", "\n", "reward", ":", "np", ".", "ndarray", ",", "\n", "action", ":", "np", ".", "ndarray", ",", "\n", "action_dist", ":", "np", ".", "ndarray", ",", "\n", "estimated_rewards_by_reg_model", ":", "np", ".", "ndarray", ",", "\n", "pscore", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "position", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "estimated_pscore", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "float", ":", "\n", "        ", "\"\"\"Estimate the policy value of evaluation policy with a tuned hyperparameter.\n\n        Parameters\n        ----------\n        reward: array-like, shape (n_rounds,)\n            Rewards observed for each data in logged bandit data, i.e., :math:`r_i`.\n\n        action: array-like, shape (n_rounds,)\n            Actions sampled by the logging/behavior policy for each data in logged bandit data, i.e., :math:`a_i`.\n\n        action_dist: array-like, shape (n_rounds, n_actions, len_list)\n            Action choice probabilities of the evaluation policy (can be deterministic), i.e., :math:`\\\\pi_e(a_i|x_i)`.\n\n        estimated_rewards_by_reg_model: array-like, shape (n_rounds, n_actions, len_list)\n            Estimated expected rewards given context, action, and position, i.e., :math:`\\\\hat{q}(x_i,a_i)`.\n\n        pscore: array-like, shape (n_rounds,), default=None\n            Action choice probabilities of the logging/behavior policy (propensity scores), i.e., :math:`\\\\pi_b(a_i|x_i)`.\n            If `use_estimated_pscore` is False, `pscore` must be given.\n\n        position: array-like, shape (n_rounds,), default=None\n            Indices to differentiate positions in a recommendation interface where the actions are presented.\n            If None, the effect of position on the reward will be ignored.\n            (If only a single action is chosen for each data, you can just ignore this argument.)\n\n        estimated_pscore: array-like, shape (n_rounds,), default=None\n            Estimated behavior policy (propensity scores), i.e., :math:`\\\\hat{\\\\pi}_b(a_i|x_i)`.\n            If `self.use_estimated_pscore` is True, `estimated_pscore` must be given.\n\n        Returns\n        ----------\n        V_hat: float\n            Estimated policy value of evaluation policy.\n\n        \"\"\"", "\n", "check_array", "(", "\n", "array", "=", "estimated_rewards_by_reg_model", ",", "\n", "name", "=", "\"estimated_rewards_by_reg_model\"", ",", "\n", "expected_dim", "=", "3", ",", "\n", ")", "\n", "check_array", "(", "array", "=", "reward", ",", "name", "=", "\"reward\"", ",", "expected_dim", "=", "1", ")", "\n", "check_array", "(", "array", "=", "action", ",", "name", "=", "\"action\"", ",", "expected_dim", "=", "1", ")", "\n", "if", "self", ".", "use_estimated_pscore", ":", "\n", "            ", "check_array", "(", "array", "=", "estimated_pscore", ",", "name", "=", "\"estimated_pscore\"", ",", "expected_dim", "=", "1", ")", "\n", "pscore_", "=", "estimated_pscore", "\n", "", "else", ":", "\n", "            ", "check_array", "(", "array", "=", "pscore", ",", "name", "=", "\"pscore\"", ",", "expected_dim", "=", "1", ")", "\n", "pscore_", "=", "pscore", "\n", "", "check_ope_inputs", "(", "\n", "action_dist", "=", "action_dist", ",", "\n", "position", "=", "position", ",", "\n", "action", "=", "action", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore", "=", "pscore_", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", ")", "\n", "if", "position", "is", "None", ":", "\n", "            ", "position", "=", "np", ".", "zeros", "(", "action_dist", ".", "shape", "[", "0", "]", ",", "dtype", "=", "int", ")", "\n", "\n", "", "return", "super", "(", ")", ".", "estimate_policy_value_with_tuning", "(", "\n", "reward", "=", "reward", ",", "\n", "action", "=", "action", ",", "\n", "position", "=", "position", ",", "\n", "pscore", "=", "pscore_", ",", "\n", "estimated_pscore", "=", "estimated_pscore", ",", "\n", "action_dist", "=", "action_dist", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_tuning.DoublyRobustTuning.estimate_interval": [[704, 797], ["utils.check_array", "utils.check_array", "utils.check_array", "utils.check_ope_inputs", "estimators_tuning.BaseOffPolicyEstimatorTuning.estimate_interval_with_tuning", "utils.check_array", "utils.check_array", "numpy.zeros"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_ope_inputs", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_tuning.BaseOffPolicyEstimatorTuning.estimate_interval_with_tuning", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array"], ["", "def", "estimate_interval", "(", "\n", "self", ",", "\n", "reward", ":", "np", ".", "ndarray", ",", "\n", "action", ":", "np", ".", "ndarray", ",", "\n", "action_dist", ":", "np", ".", "ndarray", ",", "\n", "estimated_rewards_by_reg_model", ":", "np", ".", "ndarray", ",", "\n", "pscore", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "position", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "estimated_pscore", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "alpha", ":", "float", "=", "0.05", ",", "\n", "n_bootstrap_samples", ":", "int", "=", "10000", ",", "\n", "random_state", ":", "Optional", "[", "int", "]", "=", "None", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "Dict", "[", "str", ",", "float", "]", ":", "\n", "        ", "\"\"\"Estimate the confidence interval of the policy value using bootstrap.\n\n        Parameters\n        ----------\n        reward: array-like, shape (n_rounds,)\n            Rewards observed for each data in logged bandit data, i.e., :math:`r_i`.\n\n        action: array-like, shape (n_rounds,)\n            Actions sampled by the logging/behavior policy for each data in logged bandit data, i.e., :math:`a_i`.\n\n        action_dist: array-like, shape (n_rounds, n_actions, len_list)\n            Action choice probabilities of the evaluation policy (can be deterministic), i.e., :math:`\\\\pi_e(a_i|x_i)`.\n\n        estimated_rewards_by_reg_model: array-like, shape (n_rounds, n_actions, len_list)\n            Estimated expected rewards given context, action, and position, i.e., :math:`\\\\hat{q}(x_i,a_i)`.\n\n        pscore: array-like, shape (n_rounds,), default=None\n            Action choice probabilities of the logging/behavior policy (propensity scores), i.e., :math:`\\\\pi_b(a_i|x_i)`.\n            If `use_estimated_pscore` is False, `pscore` must be given.\n\n        position: array-like, shape (n_rounds,), default=None\n            Indices to differentiate positions in a recommendation interface where the actions are presented.\n            If None, the effect of position on the reward will be ignored.\n            (If only a single action is chosen for each data, you can just ignore this argument.)\n\n        estimated_pscore: array-like, shape (n_rounds,), default=None\n            Estimated behavior policy (propensity scores), i.e., :math:`\\\\hat{\\\\pi}_b(a_i|x_i)`.\n            If `self.use_estimated_pscore` is True, `estimated_pscore` must be given.\n\n        alpha: float, default=0.05\n            Significance level.\n\n        n_bootstrap_samples: int, default=10000\n            Number of resampling performed in bootstrap sampling.\n\n        random_state: int, default=None\n            Controls the random seed in bootstrap sampling.\n\n        Returns\n        ----------\n        estimated_confidence_interval: Dict[str, float]\n            Dictionary storing the estimated mean and upper-lower confidence bounds.\n\n        \"\"\"", "\n", "check_array", "(", "\n", "array", "=", "estimated_rewards_by_reg_model", ",", "\n", "name", "=", "\"estimated_rewards_by_reg_model\"", ",", "\n", "expected_dim", "=", "3", ",", "\n", ")", "\n", "check_array", "(", "array", "=", "reward", ",", "name", "=", "\"reward\"", ",", "expected_dim", "=", "1", ")", "\n", "check_array", "(", "array", "=", "action", ",", "name", "=", "\"action\"", ",", "expected_dim", "=", "1", ")", "\n", "if", "self", ".", "use_estimated_pscore", ":", "\n", "            ", "check_array", "(", "array", "=", "estimated_pscore", ",", "name", "=", "\"estimated_pscore\"", ",", "expected_dim", "=", "1", ")", "\n", "pscore_", "=", "estimated_pscore", "\n", "", "else", ":", "\n", "            ", "check_array", "(", "array", "=", "pscore", ",", "name", "=", "\"pscore\"", ",", "expected_dim", "=", "1", ")", "\n", "pscore_", "=", "pscore", "\n", "", "check_ope_inputs", "(", "\n", "action_dist", "=", "action_dist", ",", "\n", "position", "=", "position", ",", "\n", "action", "=", "action", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore", "=", "pscore_", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", ")", "\n", "if", "position", "is", "None", ":", "\n", "            ", "position", "=", "np", ".", "zeros", "(", "action_dist", ".", "shape", "[", "0", "]", ",", "dtype", "=", "int", ")", "\n", "\n", "", "return", "super", "(", ")", ".", "estimate_interval_with_tuning", "(", "\n", "reward", "=", "reward", ",", "\n", "action", "=", "action", ",", "\n", "position", "=", "position", ",", "\n", "pscore", "=", "pscore_", ",", "\n", "estimated_pscore", "=", "estimated_pscore", ",", "\n", "action_dist", "=", "action_dist", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", "alpha", "=", "alpha", ",", "\n", "n_bootstrap_samples", "=", "n_bootstrap_samples", ",", "\n", "random_state", "=", "random_state", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_tuning.SwitchDoublyRobustTuning.__post_init__": [[837, 843], ["estimators_tuning.BaseOffPolicyEstimatorTuning._check_lambdas", "estimators_tuning.BaseOffPolicyEstimatorTuning._check_init_inputs", "estimators_tuning.SwitchDoublyRobustTuning.lambdas.sort"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_tuning.BaseOffPolicyEstimatorTuning._check_lambdas", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_tuning.BaseOffPolicyEstimatorTuning._check_init_inputs"], ["def", "__post_init__", "(", "self", ")", "->", "None", ":", "\n", "        ", "\"\"\"Initialize Class.\"\"\"", "\n", "self", ".", "base_ope_estimator", "=", "SwitchDoublyRobust", "\n", "super", "(", ")", ".", "_check_lambdas", "(", ")", "\n", "super", "(", ")", ".", "_check_init_inputs", "(", ")", "\n", "self", ".", "lambdas", ".", "sort", "(", "reverse", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_tuning.SwitchDoublyRobustTuning.estimate_policy_value": [[844, 922], ["utils.check_array", "utils.check_array", "utils.check_array", "utils.check_ope_inputs", "estimators_tuning.BaseOffPolicyEstimatorTuning.estimate_policy_value_with_tuning", "utils.check_array", "utils.check_array", "numpy.zeros"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_ope_inputs", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_tuning.BaseOffPolicyEstimatorTuning.estimate_policy_value_with_tuning", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array"], ["", "def", "estimate_policy_value", "(", "\n", "self", ",", "\n", "reward", ":", "np", ".", "ndarray", ",", "\n", "action", ":", "np", ".", "ndarray", ",", "\n", "action_dist", ":", "np", ".", "ndarray", ",", "\n", "estimated_rewards_by_reg_model", ":", "np", ".", "ndarray", ",", "\n", "pscore", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "position", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "estimated_pscore", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "float", ":", "\n", "        ", "\"\"\"Estimate the policy value of evaluation policy with a tuned hyperparameter.\n\n        Parameters\n        ----------\n        reward: array-like, shape (n_rounds,)\n            Rewards observed for each data in logged bandit data, i.e., :math:`r_i`.\n\n        action: array-like, shape (n_rounds,)\n            Actions sampled by the logging/behavior policy for each data in logged bandit data, i.e., :math:`a_i`.\n\n        action_dist: array-like, shape (n_rounds, n_actions, len_list)\n            Action choice probabilities of the evaluation policy (can be deterministic), i.e., :math:`\\\\pi_e(a_i|x_i)`.\n\n        estimated_rewards_by_reg_model: array-like, shape (n_rounds, n_actions, len_list)\n            Estimated expected rewards given context, action, and position, i.e., :math:`\\\\hat{q}(x_i,a_i)`.\n\n        pscore: array-like, shape (n_rounds,), default=None\n            Action choice probabilities of the logging/behavior policy (propensity scores), i.e., :math:`\\\\pi_b(a_i|x_i)`.\n            If `use_estimated_pscore` is False, `pscore` must be given.\n\n        position: array-like, shape (n_rounds,), default=None\n            Indices to differentiate positions in a recommendation interface where the actions are presented.\n            If None, the effect of position on the reward will be ignored.\n            (If only a single action is chosen for each data, you can just ignore this argument.)\n\n        estimated_pscore: array-like, shape (n_rounds,), default=None\n            Estimated behavior policy (propensity scores), i.e., :math:`\\\\hat{\\\\pi}_b(a_i|x_i)`.\n            If `self.use_estimated_pscore` is True, `estimated_pscore` must be given.\n\n        Returns\n        ----------\n        V_hat: float\n            Estimated policy value of evaluation policy.\n\n        \"\"\"", "\n", "check_array", "(", "\n", "array", "=", "estimated_rewards_by_reg_model", ",", "\n", "name", "=", "\"estimated_rewards_by_reg_model\"", ",", "\n", "expected_dim", "=", "3", ",", "\n", ")", "\n", "check_array", "(", "array", "=", "reward", ",", "name", "=", "\"reward\"", ",", "expected_dim", "=", "1", ")", "\n", "check_array", "(", "array", "=", "action", ",", "name", "=", "\"action\"", ",", "expected_dim", "=", "1", ")", "\n", "if", "self", ".", "use_estimated_pscore", ":", "\n", "            ", "check_array", "(", "array", "=", "estimated_pscore", ",", "name", "=", "\"estimated_pscore\"", ",", "expected_dim", "=", "1", ")", "\n", "pscore_", "=", "estimated_pscore", "\n", "", "else", ":", "\n", "            ", "check_array", "(", "array", "=", "pscore", ",", "name", "=", "\"pscore\"", ",", "expected_dim", "=", "1", ")", "\n", "pscore_", "=", "pscore", "\n", "", "check_ope_inputs", "(", "\n", "action_dist", "=", "action_dist", ",", "\n", "position", "=", "position", ",", "\n", "action", "=", "action", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore", "=", "pscore_", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", ")", "\n", "if", "position", "is", "None", ":", "\n", "            ", "position", "=", "np", ".", "zeros", "(", "action_dist", ".", "shape", "[", "0", "]", ",", "dtype", "=", "int", ")", "\n", "\n", "", "return", "super", "(", ")", ".", "estimate_policy_value_with_tuning", "(", "\n", "reward", "=", "reward", ",", "\n", "action", "=", "action", ",", "\n", "position", "=", "position", ",", "\n", "pscore", "=", "pscore_", ",", "\n", "estimated_pscore", "=", "estimated_pscore", ",", "\n", "action_dist", "=", "action_dist", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_tuning.SwitchDoublyRobustTuning.estimate_interval": [[924, 1017], ["utils.check_array", "utils.check_array", "utils.check_array", "utils.check_ope_inputs", "estimators_tuning.BaseOffPolicyEstimatorTuning.estimate_interval_with_tuning", "utils.check_array", "utils.check_array", "numpy.zeros"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_ope_inputs", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_tuning.BaseOffPolicyEstimatorTuning.estimate_interval_with_tuning", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array"], ["", "def", "estimate_interval", "(", "\n", "self", ",", "\n", "reward", ":", "np", ".", "ndarray", ",", "\n", "action", ":", "np", ".", "ndarray", ",", "\n", "action_dist", ":", "np", ".", "ndarray", ",", "\n", "estimated_rewards_by_reg_model", ":", "np", ".", "ndarray", ",", "\n", "pscore", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "position", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "estimated_pscore", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "alpha", ":", "float", "=", "0.05", ",", "\n", "n_bootstrap_samples", ":", "int", "=", "10000", ",", "\n", "random_state", ":", "Optional", "[", "int", "]", "=", "None", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "Dict", "[", "str", ",", "float", "]", ":", "\n", "        ", "\"\"\"Estimate the confidence interval of the policy value using bootstrap.\n\n        Parameters\n        ----------\n        reward: array-like, shape (n_rounds,)\n            Rewards observed for each data in logged bandit data, i.e., :math:`r_i`.\n\n        action: array-like, shape (n_rounds,)\n            Actions sampled by the logging/behavior policy for each data in logged bandit data, i.e., :math:`a_i`.\n\n        action_dist: array-like, shape (n_rounds, n_actions, len_list)\n            Action choice probabilities of the evaluation policy (can be deterministic), i.e., :math:`\\\\pi_e(a_i|x_i)`.\n\n        estimated_rewards_by_reg_model: array-like, shape (n_rounds, n_actions, len_list)\n            Estimated expected rewards given context, action, and position, i.e., :math:`\\\\hat{q}(x_i,a_i)`.\n\n        pscore: array-like, shape (n_rounds,), default=None\n            Action choice probabilities of the logging/behavior policy (propensity scores), i.e., :math:`\\\\pi_b(a_i|x_i)`.\n            If `use_estimated_pscore` is False, `pscore` must be given.\n\n        position: array-like, shape (n_rounds,), default=None\n            Indices to differentiate positions in a recommendation interface where the actions are presented.\n            If None, the effect of position on the reward will be ignored.\n            (If only a single action is chosen for each data, you can just ignore this argument.)\n\n        estimated_pscore: array-like, shape (n_rounds,), default=None\n            Estimated behavior policy (propensity scores), i.e., :math:`\\\\hat{\\\\pi}_b(a_i|x_i)`.\n            If `self.use_estimated_pscore` is True, `estimated_pscore` must be given.\n\n        alpha: float, default=0.05\n            Significance level.\n\n        n_bootstrap_samples: int, default=10000\n            Number of resampling performed in bootstrap sampling.\n\n        random_state: int, default=None\n            Controls the random seed in bootstrap sampling.\n\n        Returns\n        ----------\n        estimated_confidence_interval: Dict[str, float]\n            Dictionary storing the estimated mean and upper-lower confidence bounds.\n\n        \"\"\"", "\n", "check_array", "(", "\n", "array", "=", "estimated_rewards_by_reg_model", ",", "\n", "name", "=", "\"estimated_rewards_by_reg_model\"", ",", "\n", "expected_dim", "=", "3", ",", "\n", ")", "\n", "check_array", "(", "array", "=", "reward", ",", "name", "=", "\"reward\"", ",", "expected_dim", "=", "1", ")", "\n", "check_array", "(", "array", "=", "action", ",", "name", "=", "\"action\"", ",", "expected_dim", "=", "1", ")", "\n", "if", "self", ".", "use_estimated_pscore", ":", "\n", "            ", "check_array", "(", "array", "=", "estimated_pscore", ",", "name", "=", "\"estimated_pscore\"", ",", "expected_dim", "=", "1", ")", "\n", "pscore_", "=", "estimated_pscore", "\n", "", "else", ":", "\n", "            ", "check_array", "(", "array", "=", "pscore", ",", "name", "=", "\"pscore\"", ",", "expected_dim", "=", "1", ")", "\n", "pscore_", "=", "pscore", "\n", "", "check_ope_inputs", "(", "\n", "action_dist", "=", "action_dist", ",", "\n", "position", "=", "position", ",", "\n", "action", "=", "action", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore", "=", "pscore_", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", ")", "\n", "if", "position", "is", "None", ":", "\n", "            ", "position", "=", "np", ".", "zeros", "(", "action_dist", ".", "shape", "[", "0", "]", ",", "dtype", "=", "int", ")", "\n", "\n", "", "return", "super", "(", ")", ".", "estimate_interval_with_tuning", "(", "\n", "reward", "=", "reward", ",", "\n", "action", "=", "action", ",", "\n", "position", "=", "position", ",", "\n", "pscore", "=", "pscore_", ",", "\n", "estimated_pscore", "=", "estimated_pscore", ",", "\n", "action_dist", "=", "action_dist", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", "alpha", "=", "alpha", ",", "\n", "n_bootstrap_samples", "=", "n_bootstrap_samples", ",", "\n", "random_state", "=", "random_state", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_tuning.DoublyRobustWithShrinkageTuning.__post_init__": [[1057, 1063], ["estimators_tuning.BaseOffPolicyEstimatorTuning._check_lambdas", "estimators_tuning.BaseOffPolicyEstimatorTuning._check_init_inputs", "estimators_tuning.DoublyRobustWithShrinkageTuning.lambdas.sort"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_tuning.BaseOffPolicyEstimatorTuning._check_lambdas", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_tuning.BaseOffPolicyEstimatorTuning._check_init_inputs"], ["def", "__post_init__", "(", "self", ")", "->", "None", ":", "\n", "        ", "\"\"\"Initialize Class.\"\"\"", "\n", "self", ".", "base_ope_estimator", "=", "DoublyRobustWithShrinkage", "\n", "super", "(", ")", ".", "_check_lambdas", "(", ")", "\n", "super", "(", ")", ".", "_check_init_inputs", "(", ")", "\n", "self", ".", "lambdas", ".", "sort", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_tuning.DoublyRobustWithShrinkageTuning.estimate_policy_value": [[1064, 1142], ["utils.check_array", "utils.check_array", "utils.check_array", "utils.check_ope_inputs", "estimators_tuning.BaseOffPolicyEstimatorTuning.estimate_policy_value_with_tuning", "utils.check_array", "utils.check_array", "numpy.zeros"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_ope_inputs", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_tuning.BaseOffPolicyEstimatorTuning.estimate_policy_value_with_tuning", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array"], ["", "def", "estimate_policy_value", "(", "\n", "self", ",", "\n", "reward", ":", "np", ".", "ndarray", ",", "\n", "action", ":", "np", ".", "ndarray", ",", "\n", "action_dist", ":", "np", ".", "ndarray", ",", "\n", "estimated_rewards_by_reg_model", ":", "np", ".", "ndarray", ",", "\n", "pscore", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "position", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "estimated_pscore", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "float", ":", "\n", "        ", "\"\"\"Estimate the policy value of evaluation policy with a tuned hyperparameter.\n\n        Parameters\n        ----------\n        reward: array-like, shape (n_rounds,)\n            Rewards observed for each data in logged bandit data, i.e., :math:`r_i`.\n\n        action: array-like, shape (n_rounds,)\n            Actions sampled by the logging/behavior policy for each data in logged bandit data, i.e., :math:`a_i`.\n\n        action_dist: array-like, shape (n_rounds, n_actions, len_list)\n            Action choice probabilities of the evaluation policy (can be deterministic), i.e., :math:`\\\\pi_e(a_i|x_i)`.\n\n        estimated_rewards_by_reg_model: array-like, shape (n_rounds, n_actions, len_list)\n            Estimated expected rewards given context, action, and position, i.e., :math:`\\\\hat{q}(x_i,a_i)`.\n\n        pscore: array-like, shape (n_rounds,), default=None\n            Action choice probabilities of the logging/behavior policy (propensity scores), i.e., :math:`\\\\pi_b(a_i|x_i)`.\n            If `use_estimated_pscore` is False, `pscore` must be given.\n\n        position: array-like, shape (n_rounds,), default=None\n            Indices to differentiate positions in a recommendation interface where the actions are presented.\n            If None, the effect of position on the reward will be ignored.\n            (If only a single action is chosen for each data, you can just ignore this argument.)\n\n        estimated_pscore: array-like, shape (n_rounds,), default=None\n            Estimated behavior policy (propensity scores), i.e., :math:`\\\\hat{\\\\pi}_b(a_i|x_i)`.\n            If `self.use_estimated_pscore` is True, `estimated_pscore` must be given.\n\n        Returns\n        ----------\n        V_hat: float\n            Estimated policy value of evaluation policy.\n\n        \"\"\"", "\n", "check_array", "(", "\n", "array", "=", "estimated_rewards_by_reg_model", ",", "\n", "name", "=", "\"estimated_rewards_by_reg_model\"", ",", "\n", "expected_dim", "=", "3", ",", "\n", ")", "\n", "check_array", "(", "array", "=", "reward", ",", "name", "=", "\"reward\"", ",", "expected_dim", "=", "1", ")", "\n", "check_array", "(", "array", "=", "action", ",", "name", "=", "\"action\"", ",", "expected_dim", "=", "1", ")", "\n", "if", "self", ".", "use_estimated_pscore", ":", "\n", "            ", "check_array", "(", "array", "=", "estimated_pscore", ",", "name", "=", "\"estimated_pscore\"", ",", "expected_dim", "=", "1", ")", "\n", "pscore_", "=", "estimated_pscore", "\n", "", "else", ":", "\n", "            ", "check_array", "(", "array", "=", "pscore", ",", "name", "=", "\"pscore\"", ",", "expected_dim", "=", "1", ")", "\n", "pscore_", "=", "pscore", "\n", "", "check_ope_inputs", "(", "\n", "action_dist", "=", "action_dist", ",", "\n", "position", "=", "position", ",", "\n", "action", "=", "action", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore", "=", "pscore_", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", ")", "\n", "if", "position", "is", "None", ":", "\n", "            ", "position", "=", "np", ".", "zeros", "(", "action_dist", ".", "shape", "[", "0", "]", ",", "dtype", "=", "int", ")", "\n", "\n", "", "return", "super", "(", ")", ".", "estimate_policy_value_with_tuning", "(", "\n", "reward", "=", "reward", ",", "\n", "action", "=", "action", ",", "\n", "position", "=", "position", ",", "\n", "pscore", "=", "pscore_", ",", "\n", "estimated_pscore", "=", "estimated_pscore", ",", "\n", "action_dist", "=", "action_dist", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_tuning.DoublyRobustWithShrinkageTuning.estimate_interval": [[1144, 1237], ["utils.check_array", "utils.check_array", "utils.check_array", "utils.check_ope_inputs", "estimators_tuning.BaseOffPolicyEstimatorTuning.estimate_interval_with_tuning", "utils.check_array", "utils.check_array", "numpy.zeros"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_ope_inputs", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_tuning.BaseOffPolicyEstimatorTuning.estimate_interval_with_tuning", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array"], ["", "def", "estimate_interval", "(", "\n", "self", ",", "\n", "reward", ":", "np", ".", "ndarray", ",", "\n", "action", ":", "np", ".", "ndarray", ",", "\n", "action_dist", ":", "np", ".", "ndarray", ",", "\n", "estimated_rewards_by_reg_model", ":", "np", ".", "ndarray", ",", "\n", "pscore", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "position", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "estimated_pscore", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "alpha", ":", "float", "=", "0.05", ",", "\n", "n_bootstrap_samples", ":", "int", "=", "10000", ",", "\n", "random_state", ":", "Optional", "[", "int", "]", "=", "None", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "Dict", "[", "str", ",", "float", "]", ":", "\n", "        ", "\"\"\"Estimate the confidence interval of the policy value using bootstrap.\n\n        Parameters\n        ----------\n        reward: array-like, shape (n_rounds,)\n            Rewards observed for each data in logged bandit data, i.e., :math:`r_i`.\n\n        action: array-like, shape (n_rounds,)\n            Actions sampled by the logging/behavior policy for each data in logged bandit data, i.e., :math:`a_i`.\n\n        action_dist: array-like, shape (n_rounds, n_actions, len_list)\n            Action choice probabilities of the evaluation policy (can be deterministic), i.e., :math:`\\\\pi_e(a_i|x_i)`.\n\n        estimated_rewards_by_reg_model: array-like, shape (n_rounds, n_actions, len_list)\n            Estimated expected rewards given context, action, and position, i.e., :math:`\\\\hat{q}(x_i,a_i)`.\n\n        pscore: array-like, shape (n_rounds,), default=None\n            Action choice probabilities of the logging/behavior policy (propensity scores), i.e., :math:`\\\\pi_b(a_i|x_i)`.\n            If `use_estimated_pscore` is False, `pscore` must be given.\n\n        position: array-like, shape (n_rounds,), default=None\n            Indices to differentiate positions in a recommendation interface where the actions are presented.\n            If None, the effect of position on the reward will be ignored.\n            (If only a single action is chosen for each data, you can just ignore this argument.)\n\n        estimated_pscore: array-like, shape (n_rounds,), default=None\n            Estimated behavior policy (propensity scores), i.e., :math:`\\\\hat{\\\\pi}_b(a_i|x_i)`.\n            If `self.use_estimated_pscore` is True, `estimated_pscore` must be given.\n\n        alpha: float, default=0.05\n            Significance level.\n\n        n_bootstrap_samples: int, default=10000\n            Number of resampling performed in bootstrap sampling.\n\n        random_state: int, default=None\n            Controls the random seed in bootstrap sampling.\n\n        Returns\n        ----------\n        estimated_confidence_interval: Dict[str, float]\n            Dictionary storing the estimated mean and upper-lower confidence bounds.\n\n        \"\"\"", "\n", "check_array", "(", "\n", "array", "=", "estimated_rewards_by_reg_model", ",", "\n", "name", "=", "\"estimated_rewards_by_reg_model\"", ",", "\n", "expected_dim", "=", "3", ",", "\n", ")", "\n", "check_array", "(", "array", "=", "reward", ",", "name", "=", "\"reward\"", ",", "expected_dim", "=", "1", ")", "\n", "check_array", "(", "array", "=", "action", ",", "name", "=", "\"action\"", ",", "expected_dim", "=", "1", ")", "\n", "if", "self", ".", "use_estimated_pscore", ":", "\n", "            ", "check_array", "(", "array", "=", "estimated_pscore", ",", "name", "=", "\"estimated_pscore\"", ",", "expected_dim", "=", "1", ")", "\n", "pscore_", "=", "estimated_pscore", "\n", "", "else", ":", "\n", "            ", "check_array", "(", "array", "=", "pscore", ",", "name", "=", "\"pscore\"", ",", "expected_dim", "=", "1", ")", "\n", "pscore_", "=", "pscore", "\n", "", "check_ope_inputs", "(", "\n", "action_dist", "=", "action_dist", ",", "\n", "position", "=", "position", ",", "\n", "action", "=", "action", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore", "=", "pscore_", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", ")", "\n", "if", "position", "is", "None", ":", "\n", "            ", "position", "=", "np", ".", "zeros", "(", "action_dist", ".", "shape", "[", "0", "]", ",", "dtype", "=", "int", ")", "\n", "\n", "", "return", "super", "(", ")", ".", "estimate_interval_with_tuning", "(", "\n", "reward", "=", "reward", ",", "\n", "action", "=", "action", ",", "\n", "position", "=", "position", ",", "\n", "pscore", "=", "pscore_", ",", "\n", "estimated_pscore", "=", "estimated_pscore", ",", "\n", "action_dist", "=", "action_dist", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", "alpha", "=", "alpha", ",", "\n", "n_bootstrap_samples", "=", "n_bootstrap_samples", ",", "\n", "random_state", "=", "random_state", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_tuning.SubGaussianInverseProbabilityWeightingTuning.__post_init__": [[1286, 1292], ["estimators_tuning.BaseOffPolicyEstimatorTuning._check_lambdas", "estimators_tuning.BaseOffPolicyEstimatorTuning._check_init_inputs", "estimators_tuning.SubGaussianInverseProbabilityWeightingTuning.lambdas.sort"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_tuning.BaseOffPolicyEstimatorTuning._check_lambdas", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_tuning.BaseOffPolicyEstimatorTuning._check_init_inputs"], ["def", "__post_init__", "(", "self", ")", "->", "None", ":", "\n", "        ", "\"\"\"Initialize Class.\"\"\"", "\n", "self", ".", "base_ope_estimator", "=", "SubGaussianInverseProbabilityWeighting", "\n", "super", "(", ")", ".", "_check_lambdas", "(", "max_val", "=", "1.0", ")", "\n", "super", "(", ")", ".", "_check_init_inputs", "(", ")", "\n", "self", ".", "lambdas", ".", "sort", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_tuning.SubGaussianInverseProbabilityWeightingTuning.estimate_policy_value": [[1293, 1359], ["utils.check_array", "utils.check_array", "utils.check_ope_inputs", "estimators_tuning.BaseOffPolicyEstimatorTuning.estimate_policy_value_with_tuning", "utils.check_array", "utils.check_array", "numpy.zeros"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_ope_inputs", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_tuning.BaseOffPolicyEstimatorTuning.estimate_policy_value_with_tuning", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array"], ["", "def", "estimate_policy_value", "(", "\n", "self", ",", "\n", "reward", ":", "np", ".", "ndarray", ",", "\n", "action", ":", "np", ".", "ndarray", ",", "\n", "action_dist", ":", "np", ".", "ndarray", ",", "\n", "pscore", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "position", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "estimated_pscore", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "float", ":", "\n", "        ", "\"\"\"Estimate the policy value of evaluation policy with a tuned hyperparameter.\n\n        Parameters\n        ----------\n        reward: array-like, shape (n_rounds,)\n            Rewards observed for each data in logged bandit data, i.e., :math:`r_i`.\n\n        action: array-like, shape (n_rounds,)\n            Actions sampled by the logging/behavior policy for each data in logged bandit data, i.e., :math:`a_i`.\n\n        action_dist: array-like, shape (n_rounds, n_actions, len_list)\n            Action choice probabilities of the evaluation policy (can be deterministic), i.e., :math:`\\\\pi_e(a_i|x_i)`.\n\n        pscore: array-like, shape (n_rounds,), default=None\n            Action choice probabilities of the logging/behavior policy (propensity scores), i.e., :math:`\\\\pi_b(a_i|x_i)`.\n            If `use_estimated_pscore` is False, `pscore` must be given.\n\n        position: array-like, shape (n_rounds,), default=None\n            Indices to differentiate positions in a recommendation interface where the actions are presented.\n            If None, the effect of position on the reward will be ignored.\n            (If only a single action is chosen for each data, you can just ignore this argument.)\n\n        estimated_pscore: array-like, shape (n_rounds,), default=None\n            Estimated behavior policy (propensity scores), i.e., :math:`\\\\hat{\\\\pi}_b(a_i|x_i)`.\n            If `self.use_estimated_pscore` is True, `estimated_pscore` must be given.\n\n        Returns\n        ----------\n        V_hat: float\n            Estimated policy value of evaluation policy.\n\n        \"\"\"", "\n", "check_array", "(", "array", "=", "reward", ",", "name", "=", "\"reward\"", ",", "expected_dim", "=", "1", ")", "\n", "check_array", "(", "array", "=", "action", ",", "name", "=", "\"action\"", ",", "expected_dim", "=", "1", ")", "\n", "if", "self", ".", "use_estimated_pscore", ":", "\n", "            ", "check_array", "(", "array", "=", "estimated_pscore", ",", "name", "=", "\"estimated_pscore\"", ",", "expected_dim", "=", "1", ")", "\n", "pscore_", "=", "estimated_pscore", "\n", "", "else", ":", "\n", "            ", "check_array", "(", "array", "=", "pscore", ",", "name", "=", "\"pscore\"", ",", "expected_dim", "=", "1", ")", "\n", "pscore_", "=", "pscore", "\n", "", "check_ope_inputs", "(", "\n", "action_dist", "=", "action_dist", ",", "\n", "position", "=", "position", ",", "\n", "action", "=", "action", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore", "=", "pscore_", ",", "\n", ")", "\n", "if", "position", "is", "None", ":", "\n", "            ", "position", "=", "np", ".", "zeros", "(", "action_dist", ".", "shape", "[", "0", "]", ",", "dtype", "=", "int", ")", "\n", "\n", "", "return", "super", "(", ")", ".", "estimate_policy_value_with_tuning", "(", "\n", "reward", "=", "reward", ",", "\n", "action", "=", "action", ",", "\n", "position", "=", "position", ",", "\n", "pscore", "=", "pscore_", ",", "\n", "action_dist", "=", "action_dist", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_tuning.SubGaussianInverseProbabilityWeightingTuning.estimate_interval": [[1361, 1442], ["utils.check_array", "utils.check_array", "utils.check_ope_inputs", "estimators_tuning.BaseOffPolicyEstimatorTuning.estimate_interval_with_tuning", "utils.check_array", "utils.check_array", "numpy.zeros"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_ope_inputs", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_tuning.BaseOffPolicyEstimatorTuning.estimate_interval_with_tuning", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array"], ["", "def", "estimate_interval", "(", "\n", "self", ",", "\n", "reward", ":", "np", ".", "ndarray", ",", "\n", "action", ":", "np", ".", "ndarray", ",", "\n", "action_dist", ":", "np", ".", "ndarray", ",", "\n", "pscore", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "position", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "estimated_pscore", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "alpha", ":", "float", "=", "0.05", ",", "\n", "n_bootstrap_samples", ":", "int", "=", "10000", ",", "\n", "random_state", ":", "Optional", "[", "int", "]", "=", "None", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "Dict", "[", "str", ",", "float", "]", ":", "\n", "        ", "\"\"\"Estimate the confidence interval of the policy value using bootstrap.\n\n        Parameters\n        ----------\n        reward: array-like, shape (n_rounds,)\n            Rewards observed for each data in logged bandit data, i.e., :math:`r_i`.\n\n        action: array-like, shape (n_rounds,)\n            Actions sampled by the logging/behavior policy for each data in logged bandit data, i.e., :math:`a_i`.\n\n        action_dist: array-like, shape (n_rounds, n_actions, len_list)\n            Action choice probabilities of the evaluation policy (can be deterministic), i.e., :math:`\\\\pi_e(a_i|x_i)`.\n\n        pscore: array-like, shape (n_rounds,), default=None\n            Action choice probabilities of the logging/behavior policy (propensity scores), i.e., :math:`\\\\pi_b(a_i|x_i)`.\n            If `use_estimated_pscore` is False, `pscore` must be given.\n\n        position: array-like, shape (n_rounds,), default=None\n            Indices to differentiate positions in a recommendation interface where the actions are presented.\n            If None, the effect of position on the reward will be ignored.\n            (If only a single action is chosen for each data, you can just ignore this argument.)\n\n        estimated_pscore: array-like, shape (n_rounds,), default=None\n            Estimated behavior policy (propensity scores), i.e., :math:`\\\\hat{\\\\pi}_b(a_i|x_i)`.\n            If `self.use_estimated_pscore` is True, `estimated_pscore` must be given.\n\n        alpha: float, default=0.05\n            Significance level.\n\n        n_bootstrap_samples: int, default=10000\n            Number of resampling performed in bootstrap sampling.\n\n        random_state: int, default=None\n            Controls the random seed in bootstrap sampling.\n\n        Returns\n        ----------\n        estimated_confidence_interval: Dict[str, float]\n            Dictionary storing the estimated mean and upper-lower confidence bounds.\n\n        \"\"\"", "\n", "check_array", "(", "array", "=", "reward", ",", "name", "=", "\"reward\"", ",", "expected_dim", "=", "1", ")", "\n", "check_array", "(", "array", "=", "action", ",", "name", "=", "\"action\"", ",", "expected_dim", "=", "1", ")", "\n", "if", "self", ".", "use_estimated_pscore", ":", "\n", "            ", "check_array", "(", "array", "=", "estimated_pscore", ",", "name", "=", "\"estimated_pscore\"", ",", "expected_dim", "=", "1", ")", "\n", "pscore_", "=", "estimated_pscore", "\n", "", "else", ":", "\n", "            ", "check_array", "(", "array", "=", "pscore", ",", "name", "=", "\"pscore\"", ",", "expected_dim", "=", "1", ")", "\n", "pscore_", "=", "pscore", "\n", "", "check_ope_inputs", "(", "\n", "action_dist", "=", "action_dist", ",", "\n", "position", "=", "position", ",", "\n", "action", "=", "action", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore", "=", "pscore_", ",", "\n", ")", "\n", "if", "position", "is", "None", ":", "\n", "            ", "position", "=", "np", ".", "zeros", "(", "action_dist", ".", "shape", "[", "0", "]", ",", "dtype", "=", "int", ")", "\n", "\n", "", "return", "super", "(", ")", ".", "estimate_interval_with_tuning", "(", "\n", "reward", "=", "reward", ",", "\n", "action", "=", "action", ",", "\n", "position", "=", "position", ",", "\n", "pscore", "=", "pscore_", ",", "\n", "action_dist", "=", "action_dist", ",", "\n", "alpha", "=", "alpha", ",", "\n", "n_bootstrap_samples", "=", "n_bootstrap_samples", ",", "\n", "random_state", "=", "random_state", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_tuning.SubGaussianDoublyRobustTuning.__post_init__": [[1485, 1491], ["estimators_tuning.BaseOffPolicyEstimatorTuning._check_lambdas", "estimators_tuning.BaseOffPolicyEstimatorTuning._check_init_inputs", "estimators_tuning.SubGaussianDoublyRobustTuning.lambdas.sort"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_tuning.BaseOffPolicyEstimatorTuning._check_lambdas", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_tuning.BaseOffPolicyEstimatorTuning._check_init_inputs"], ["def", "__post_init__", "(", "self", ")", "->", "None", ":", "\n", "        ", "\"\"\"Initialize Class.\"\"\"", "\n", "self", ".", "base_ope_estimator", "=", "SubGaussianDoublyRobust", "\n", "super", "(", ")", ".", "_check_lambdas", "(", "max_val", "=", "1.0", ")", "\n", "super", "(", ")", ".", "_check_init_inputs", "(", ")", "\n", "self", ".", "lambdas", ".", "sort", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_tuning.SubGaussianDoublyRobustTuning.estimate_policy_value": [[1492, 1569], ["utils.check_array", "utils.check_array", "utils.check_array", "utils.check_ope_inputs", "estimators_tuning.BaseOffPolicyEstimatorTuning.estimate_policy_value_with_tuning", "utils.check_array", "utils.check_array", "numpy.zeros"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_ope_inputs", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_tuning.BaseOffPolicyEstimatorTuning.estimate_policy_value_with_tuning", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array"], ["", "def", "estimate_policy_value", "(", "\n", "self", ",", "\n", "reward", ":", "np", ".", "ndarray", ",", "\n", "action", ":", "np", ".", "ndarray", ",", "\n", "action_dist", ":", "np", ".", "ndarray", ",", "\n", "estimated_rewards_by_reg_model", ":", "np", ".", "ndarray", ",", "\n", "pscore", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "position", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "estimated_pscore", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "float", ":", "\n", "        ", "\"\"\"Estimate the policy value of evaluation policy with a tuned hyperparameter.\n\n        Parameters\n        ----------\n        reward: array-like, shape (n_rounds,)\n            Rewards observed for each data in logged bandit data, i.e., :math:`r_i`.\n\n        action: array-like, shape (n_rounds,)\n            Actions sampled by the logging/behavior policy for each data in logged bandit data, i.e., :math:`a_i`.\n\n        action_dist: array-like, shape (n_rounds, n_actions, len_list)\n            Action choice probabilities of the evaluation policy (can be deterministic), i.e., :math:`\\\\pi_e(a_i|x_i)`.\n\n        estimated_rewards_by_reg_model: array-like, shape (n_rounds, n_actions, len_list)\n            Estimated expected rewards given context, action, and position, i.e., :math:`\\\\hat{q}(x_i,a_i)`.\n\n        pscore: array-like, shape (n_rounds,), default=None\n            Action choice probabilities of the logging/behavior policy (propensity scores), i.e., :math:`\\\\pi_b(a_i|x_i)`.\n            If `use_estimated_pscore` is False, `pscore` must be given.\n\n        position: array-like, shape (n_rounds,), default=None\n            Indices to differentiate positions in a recommendation interface where the actions are presented.\n            If None, the effect of position on the reward will be ignored.\n            (If only a single action is chosen for each data, you can just ignore this argument.)\n\n        estimated_pscore: array-like, shape (n_rounds,), default=None\n            Estimated behavior policy (propensity scores), i.e., :math:`\\\\hat{\\\\pi}_b(a_i|x_i)`.\n            If `self.use_estimated_pscore` is True, `estimated_pscore` must be given.\n\n        Returns\n        ----------\n        V_hat: float\n            Estimated policy value of evaluation policy.\n\n        \"\"\"", "\n", "check_array", "(", "\n", "array", "=", "estimated_rewards_by_reg_model", ",", "\n", "name", "=", "\"estimated_rewards_by_reg_model\"", ",", "\n", "expected_dim", "=", "3", ",", "\n", ")", "\n", "check_array", "(", "array", "=", "reward", ",", "name", "=", "\"reward\"", ",", "expected_dim", "=", "1", ")", "\n", "check_array", "(", "array", "=", "action", ",", "name", "=", "\"action\"", ",", "expected_dim", "=", "1", ")", "\n", "if", "self", ".", "use_estimated_pscore", ":", "\n", "            ", "check_array", "(", "array", "=", "estimated_pscore", ",", "name", "=", "\"estimated_pscore\"", ",", "expected_dim", "=", "1", ")", "\n", "pscore_", "=", "estimated_pscore", "\n", "", "else", ":", "\n", "            ", "check_array", "(", "array", "=", "pscore", ",", "name", "=", "\"pscore\"", ",", "expected_dim", "=", "1", ")", "\n", "pscore_", "=", "pscore", "\n", "", "check_ope_inputs", "(", "\n", "action_dist", "=", "action_dist", ",", "\n", "position", "=", "position", ",", "\n", "action", "=", "action", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore", "=", "pscore_", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", ")", "\n", "if", "position", "is", "None", ":", "\n", "            ", "position", "=", "np", ".", "zeros", "(", "action_dist", ".", "shape", "[", "0", "]", ",", "dtype", "=", "int", ")", "\n", "\n", "", "return", "super", "(", ")", ".", "estimate_policy_value_with_tuning", "(", "\n", "reward", "=", "reward", ",", "\n", "action", "=", "action", ",", "\n", "position", "=", "position", ",", "\n", "pscore", "=", "pscore_", ",", "\n", "action_dist", "=", "action_dist", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_tuning.SubGaussianDoublyRobustTuning.estimate_interval": [[1571, 1663], ["utils.check_array", "utils.check_array", "utils.check_array", "utils.check_ope_inputs", "estimators_tuning.BaseOffPolicyEstimatorTuning.estimate_interval_with_tuning", "utils.check_array", "utils.check_array", "numpy.zeros"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_ope_inputs", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_tuning.BaseOffPolicyEstimatorTuning.estimate_interval_with_tuning", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array"], ["", "def", "estimate_interval", "(", "\n", "self", ",", "\n", "reward", ":", "np", ".", "ndarray", ",", "\n", "action", ":", "np", ".", "ndarray", ",", "\n", "action_dist", ":", "np", ".", "ndarray", ",", "\n", "estimated_rewards_by_reg_model", ":", "np", ".", "ndarray", ",", "\n", "pscore", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "position", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "estimated_pscore", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "alpha", ":", "float", "=", "0.05", ",", "\n", "n_bootstrap_samples", ":", "int", "=", "10000", ",", "\n", "random_state", ":", "Optional", "[", "int", "]", "=", "None", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "Dict", "[", "str", ",", "float", "]", ":", "\n", "        ", "\"\"\"Estimate the confidence interval of the policy value using bootstrap.\n\n        Parameters\n        ----------\n        reward: array-like, shape (n_rounds,)\n            Rewards observed for each data in logged bandit data, i.e., :math:`r_i`.\n\n        action: array-like, shape (n_rounds,)\n            Actions sampled by the logging/behavior policy for each data in logged bandit data, i.e., :math:`a_i`.\n\n        action_dist: array-like, shape (n_rounds, n_actions, len_list)\n            Action choice probabilities of the evaluation policy (can be deterministic), i.e., :math:`\\\\pi_e(a_i|x_i)`.\n\n        estimated_rewards_by_reg_model: array-like, shape (n_rounds, n_actions, len_list)\n            Estimated expected rewards given context, action, and position, i.e., :math:`\\\\hat{q}(x_i,a_i)`.\n\n        pscore: array-like, shape (n_rounds,), default=None\n            Action choice probabilities of the logging/behavior policy (propensity scores), i.e., :math:`\\\\pi_b(a_i|x_i)`.\n            If `use_estimated_pscore` is False, `pscore` must be given.\n\n        position: array-like, shape (n_rounds,), default=None\n            Indices to differentiate positions in a recommendation interface where the actions are presented.\n            If None, the effect of position on the reward will be ignored.\n            (If only a single action is chosen for each data, you can just ignore this argument.)\n\n        estimated_pscore: array-like, shape (n_rounds,), default=None\n            Estimated behavior policy (propensity scores), i.e., :math:`\\\\hat{\\\\pi}_b(a_i|x_i)`.\n            If `self.use_estimated_pscore` is True, `estimated_pscore` must be given.\n\n        alpha: float, default=0.05\n            Significance level.\n\n        n_bootstrap_samples: int, default=10000\n            Number of resampling performed in bootstrap sampling.\n\n        random_state: int, default=None\n            Controls the random seed in bootstrap sampling.\n\n        Returns\n        ----------\n        estimated_confidence_interval: Dict[str, float]\n            Dictionary storing the estimated mean and upper-lower confidence bounds.\n\n        \"\"\"", "\n", "check_array", "(", "\n", "array", "=", "estimated_rewards_by_reg_model", ",", "\n", "name", "=", "\"estimated_rewards_by_reg_model\"", ",", "\n", "expected_dim", "=", "3", ",", "\n", ")", "\n", "check_array", "(", "array", "=", "reward", ",", "name", "=", "\"reward\"", ",", "expected_dim", "=", "1", ")", "\n", "check_array", "(", "array", "=", "action", ",", "name", "=", "\"action\"", ",", "expected_dim", "=", "1", ")", "\n", "if", "self", ".", "use_estimated_pscore", ":", "\n", "            ", "check_array", "(", "array", "=", "estimated_pscore", ",", "name", "=", "\"estimated_pscore\"", ",", "expected_dim", "=", "1", ")", "\n", "pscore_", "=", "estimated_pscore", "\n", "", "else", ":", "\n", "            ", "check_array", "(", "array", "=", "pscore", ",", "name", "=", "\"pscore\"", ",", "expected_dim", "=", "1", ")", "\n", "pscore_", "=", "pscore", "\n", "", "check_ope_inputs", "(", "\n", "action_dist", "=", "action_dist", ",", "\n", "position", "=", "position", ",", "\n", "action", "=", "action", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore", "=", "pscore_", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", ")", "\n", "if", "position", "is", "None", ":", "\n", "            ", "position", "=", "np", ".", "zeros", "(", "action_dist", ".", "shape", "[", "0", "]", ",", "dtype", "=", "int", ")", "\n", "\n", "", "return", "super", "(", ")", ".", "estimate_interval_with_tuning", "(", "\n", "reward", "=", "reward", ",", "\n", "action", "=", "action", ",", "\n", "position", "=", "position", ",", "\n", "pscore", "=", "pscore_", ",", "\n", "action_dist", "=", "action_dist", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", "alpha", "=", "alpha", ",", "\n", "n_bootstrap_samples", "=", "n_bootstrap_samples", ",", "\n", "random_state", "=", "random_state", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_multi.BaseMultiLoggersOffPolicyEstimator._estimate_round_rewards": [[23, 27], ["None"], "methods", ["None"], ["@", "abstractmethod", "\n", "def", "_estimate_round_rewards", "(", "self", ")", "->", "np", ".", "ndarray", ":", "\n", "        ", "\"\"\"Estimate round-wise (or sample-wise) rewards.\"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_multi.BaseMultiLoggersOffPolicyEstimator.estimate_policy_value": [[28, 32], ["None"], "methods", ["None"], ["", "@", "abstractmethod", "\n", "def", "estimate_policy_value", "(", "self", ")", "->", "float", ":", "\n", "        ", "\"\"\"Estimate the policy value of evaluation policy.\"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_multi.BaseMultiLoggersOffPolicyEstimator.estimate_interval": [[33, 37], ["None"], "methods", ["None"], ["", "@", "abstractmethod", "\n", "def", "estimate_interval", "(", "self", ")", "->", "Dict", "[", "str", ",", "float", "]", ":", "\n", "        ", "\"\"\"Estimate the confidence interval of the policy value using bootstrap.\"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_multi.MultiLoggersNaiveInverseProbabilityWeighting.__post_init__": [[89, 102], ["sklearn.utils.check_scalar", "ValueError", "isinstance", "TypeError", "type"], "methods", ["None"], ["def", "__post_init__", "(", "self", ")", "->", "None", ":", "\n", "        ", "\"\"\"Initialize Class.\"\"\"", "\n", "check_scalar", "(", "\n", "self", ".", "lambda_", ",", "\n", "name", "=", "\"lambda_\"", ",", "\n", "target_type", "=", "(", "int", ",", "float", ")", ",", "\n", "min_val", "=", "0.0", ",", "\n", ")", "\n", "if", "self", ".", "lambda_", "!=", "self", ".", "lambda_", ":", "\n", "            ", "raise", "ValueError", "(", "\"`lambda_` must not be nan\"", ")", "\n", "", "if", "not", "isinstance", "(", "self", ".", "use_estimated_pscore", ",", "bool", ")", ":", "\n", "            ", "raise", "TypeError", "(", "\n", "f\"`use_estimated_pscore` must be a bool, but {type(self.use_estimated_pscore)} is given\"", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_multi.MultiLoggersNaiveInverseProbabilityWeighting._estimate_round_rewards": [[104, 149], ["isinstance", "numpy.zeros", "numpy.minimum", "numpy.arange"], "methods", ["None"], ["", "", "def", "_estimate_round_rewards", "(", "\n", "self", ",", "\n", "reward", ":", "np", ".", "ndarray", ",", "\n", "action", ":", "np", ".", "ndarray", ",", "\n", "pscore", ":", "np", ".", "ndarray", ",", "\n", "action_dist", ":", "np", ".", "ndarray", ",", "\n", "position", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "np", ".", "ndarray", ":", "\n", "        ", "\"\"\"Estimate round-wise (or sample-wise) rewards.\n\n        Parameters\n        ----------\n        reward: array-like, shape (n_rounds,)\n            Rewards observed for each data in logged bandit data, i.e., :math:`r_i`.\n\n        action: array-like, shape (n_rounds,)\n            Actions sampled by the logging/behavior policy for each data in logged bandit data, i.e., :math:`a_i`.\n\n        pscore: array-like, shape (n_rounds,)\n            Action choice probabilities of the logging/behavior policy (propensity scores), i.e., :math:`\\\\pi_k(a_i|x_i)`.\n            If `use_estimated_pscore` is False, `pscore` must be given.\n\n        action_dist: array-like, shape (n_rounds, n_actions, len_list)\n            Action choice probabilities of the evaluation policy (can be deterministic), i.e., :math:`\\\\pi_e(a_i|x_i)`.\n\n        position: array-like, shape (n_rounds,), default=None\n            Indices to differentiate positions in a recommendation interface where the actions are presented.\n            If None, the effect of position on the reward will be ignored.\n            (If only a single action is chosen for each data, you can just ignore this argument.)\n\n        Returns\n        ----------\n        estimated_rewards: array-like, shape (n_rounds,)\n            Estimated rewards for each observation.\n\n        \"\"\"", "\n", "if", "position", "is", "None", ":", "\n", "            ", "position", "=", "np", ".", "zeros", "(", "action_dist", ".", "shape", "[", "0", "]", ",", "dtype", "=", "int", ")", "\n", "\n", "", "iw", "=", "action_dist", "[", "np", ".", "arange", "(", "action", ".", "shape", "[", "0", "]", ")", ",", "action", ",", "position", "]", "/", "pscore", "\n", "# weight clipping", "\n", "if", "isinstance", "(", "iw", ",", "np", ".", "ndarray", ")", ":", "\n", "            ", "iw", "=", "np", ".", "minimum", "(", "iw", ",", "self", ".", "lambda_", ")", "\n", "", "return", "reward", "*", "iw", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_multi.MultiLoggersNaiveInverseProbabilityWeighting.estimate_policy_value": [[150, 217], ["utils.check_array", "utils.check_array", "utils.check_multi_loggers_ope_inputs", "estimators_multi.MultiLoggersNaiveInverseProbabilityWeighting._estimate_round_rewards().mean", "utils.check_array", "utils.check_array", "numpy.zeros", "estimators_multi.MultiLoggersNaiveInverseProbabilityWeighting._estimate_round_rewards"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_multi_loggers_ope_inputs", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SelfNormalizedSlateStandardIPS._estimate_round_rewards"], ["", "def", "estimate_policy_value", "(", "\n", "self", ",", "\n", "reward", ":", "np", ".", "ndarray", ",", "\n", "action", ":", "np", ".", "ndarray", ",", "\n", "action_dist", ":", "np", ".", "ndarray", ",", "\n", "pscore", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "position", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "estimated_pscore", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "np", ".", "ndarray", ":", "\n", "        ", "\"\"\"Estimate the policy value of evaluation policy.\n\n        Parameters\n        ----------\n        reward: array-like, shape (n_rounds,)\n            Rewards observed for each data in logged bandit data, i.e., :math:`r_i`.\n\n        action: array-like, shape (n_rounds,)\n            Actions sampled by the logging/behavior policy for each data in logged bandit data, i.e., :math:`a_i`.\n\n        action_dist: array-like, shape (n_rounds, n_actions, len_list)\n            Action choice probabilities of the evaluation policy (can be deterministic), i.e., :math:`\\\\pi_e(a_i|x_i)`.\n\n        pscore: array-like, shape (n_rounds,), default=None\n            Action choice probabilities of the logging/behavior policy (propensity scores), i.e., :math:`\\\\pi_k(a_i|x_i)`.\n            If `use_estimated_pscore` is False, `pscore` must be given.\n\n        position: array-like, shape (n_rounds,), default=None\n            Indices to differentiate positions in a recommendation interface where the actions are presented.\n            If None, the effect of position on the reward will be ignored.\n            (If only a single action is chosen for each data, you can just ignore this argument.)\n\n        estimated_pscore: array-like, shape (n_rounds,), default=None\n            Estimated behavior policy (propensity scores), i.e., :math:`\\\\hat{\\\\pi}_k(a_i|x_i)`.\n            If `self.use_estimated_pscore` is True, `estimated_pscore` must be given.\n\n        Returns\n        ----------\n        V_hat: float\n            Estimated policy value of evaluation policy.\n\n        \"\"\"", "\n", "check_array", "(", "array", "=", "reward", ",", "name", "=", "\"reward\"", ",", "expected_dim", "=", "1", ")", "\n", "check_array", "(", "array", "=", "action", ",", "name", "=", "\"action\"", ",", "expected_dim", "=", "1", ")", "\n", "if", "self", ".", "use_estimated_pscore", ":", "\n", "            ", "check_array", "(", "array", "=", "estimated_pscore", ",", "name", "=", "\"estimated_pscore\"", ",", "expected_dim", "=", "1", ")", "\n", "pscore_", "=", "estimated_pscore", "\n", "", "else", ":", "\n", "            ", "check_array", "(", "array", "=", "pscore", ",", "name", "=", "\"pscore\"", ",", "expected_dim", "=", "1", ")", "\n", "pscore_", "=", "pscore", "\n", "\n", "", "check_multi_loggers_ope_inputs", "(", "\n", "action_dist", "=", "action_dist", ",", "\n", "position", "=", "position", ",", "\n", "action", "=", "action", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore", "=", "pscore_", ",", "\n", ")", "\n", "if", "position", "is", "None", ":", "\n", "            ", "position", "=", "np", ".", "zeros", "(", "action_dist", ".", "shape", "[", "0", "]", ",", "dtype", "=", "int", ")", "\n", "\n", "", "return", "self", ".", "_estimate_round_rewards", "(", "\n", "reward", "=", "reward", ",", "\n", "action", "=", "action", ",", "\n", "position", "=", "position", ",", "\n", "pscore", "=", "pscore_", ",", "\n", "action_dist", "=", "action_dist", ",", "\n", ")", ".", "mean", "(", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_multi.MultiLoggersNaiveInverseProbabilityWeighting.estimate_interval": [[219, 304], ["utils.check_array", "utils.check_array", "utils.check_multi_loggers_ope_inputs", "estimators_multi.MultiLoggersNaiveInverseProbabilityWeighting._estimate_round_rewards", "utils.estimate_confidence_interval_by_bootstrap", "utils.check_array", "utils.check_array", "numpy.zeros"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_multi_loggers_ope_inputs", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SelfNormalizedSlateStandardIPS._estimate_round_rewards", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.estimate_confidence_interval_by_bootstrap", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array"], ["", "def", "estimate_interval", "(", "\n", "self", ",", "\n", "reward", ":", "np", ".", "ndarray", ",", "\n", "action", ":", "np", ".", "ndarray", ",", "\n", "action_dist", ":", "np", ".", "ndarray", ",", "\n", "pscore", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "position", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "estimated_pscore", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "alpha", ":", "float", "=", "0.05", ",", "\n", "n_bootstrap_samples", ":", "int", "=", "10000", ",", "\n", "random_state", ":", "Optional", "[", "int", "]", "=", "None", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "Dict", "[", "str", ",", "float", "]", ":", "\n", "        ", "\"\"\"Estimate the confidence interval of the policy value using bootstrap.\n\n        Parameters\n        ----------\n        reward: array-like, shape (n_rounds,)\n            Rewards observed for each data in logged bandit data, i.e., :math:`r_i`.\n\n        action: array-like, shape (n_rounds,)\n            Actions sampled by the logging/behavior policy for each data in logged bandit data, i.e., :math:`a_i`.\n\n        action_dist: array-like, shape (n_rounds, n_actions, len_list)\n            Action choice probabilities of the evaluation policy (can be deterministic), i.e., :math:`\\\\pi_e(a_i|x_i)`.\n\n        pscore: array-like, shape (n_rounds,), default=None\n            Action choice probabilities of the logging/behavior policy (propensity scores), i.e., :math:`\\\\pi_k(a_i|x_i)`.\n            If `use_estimated_pscore` is False, `pscore` must be given.\n\n        position: array-like, shape (n_rounds,), default=None\n            Indices to differentiate positions in a recommendation interface where the actions are presented.\n            If None, the effect of position on the reward will be ignored.\n            (If only a single action is chosen for each data, you can just ignore this argument.)\n\n        estimated_pscore: array-like, shape (n_rounds,), default=None\n            Estimated behavior policy (propensity scores), i.e., :math:`\\\\hat{\\\\pi}_b(a_i|x_i)`.\n            If `self.use_estimated_pscore` is True, `estimated_pscore` must be given.\n\n        alpha: float, default=0.05\n            Significance level.\n\n        n_bootstrap_samples: int, default=10000\n            Number of resampling performed in bootstrap sampling.\n\n        random_state: int, default=None\n            Controls the random seed in bootstrap sampling.\n\n        Returns\n        ----------\n        estimated_confidence_interval: Dict[str, float]\n            Dictionary storing the estimated mean and upper-lower confidence bounds.\n\n        \"\"\"", "\n", "check_array", "(", "array", "=", "reward", ",", "name", "=", "\"reward\"", ",", "expected_dim", "=", "1", ")", "\n", "check_array", "(", "array", "=", "action", ",", "name", "=", "\"action\"", ",", "expected_dim", "=", "1", ")", "\n", "if", "self", ".", "use_estimated_pscore", ":", "\n", "            ", "check_array", "(", "array", "=", "estimated_pscore", ",", "name", "=", "\"estimated_pscore\"", ",", "expected_dim", "=", "1", ")", "\n", "pscore_", "=", "estimated_pscore", "\n", "", "else", ":", "\n", "            ", "check_array", "(", "array", "=", "pscore", ",", "name", "=", "\"pscore\"", ",", "expected_dim", "=", "1", ")", "\n", "pscore_", "=", "pscore", "\n", "\n", "", "check_multi_loggers_ope_inputs", "(", "\n", "action_dist", "=", "action_dist", ",", "\n", "position", "=", "position", ",", "\n", "action", "=", "action", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore", "=", "pscore_", ",", "\n", ")", "\n", "if", "position", "is", "None", ":", "\n", "            ", "position", "=", "np", ".", "zeros", "(", "action_dist", ".", "shape", "[", "0", "]", ",", "dtype", "=", "int", ")", "\n", "\n", "", "estimated_round_rewards", "=", "self", ".", "_estimate_round_rewards", "(", "\n", "reward", "=", "reward", ",", "\n", "action", "=", "action", ",", "\n", "position", "=", "position", ",", "\n", "pscore", "=", "pscore_", ",", "\n", "action_dist", "=", "action_dist", ",", "\n", ")", "\n", "return", "estimate_confidence_interval_by_bootstrap", "(", "\n", "samples", "=", "estimated_round_rewards", ",", "\n", "alpha", "=", "alpha", ",", "\n", "n_bootstrap_samples", "=", "n_bootstrap_samples", ",", "\n", "random_state", "=", "random_state", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_multi.MultiLoggersBalancedInverseProbabilityWeighting.__post_init__": [[361, 374], ["sklearn.utils.check_scalar", "ValueError", "isinstance", "TypeError", "type"], "methods", ["None"], ["def", "__post_init__", "(", "self", ")", "->", "None", ":", "\n", "        ", "\"\"\"Initialize Class.\"\"\"", "\n", "check_scalar", "(", "\n", "self", ".", "lambda_", ",", "\n", "name", "=", "\"lambda_\"", ",", "\n", "target_type", "=", "(", "int", ",", "float", ")", ",", "\n", "min_val", "=", "0.0", ",", "\n", ")", "\n", "if", "self", ".", "lambda_", "!=", "self", ".", "lambda_", ":", "\n", "            ", "raise", "ValueError", "(", "\"`lambda_` must not be nan\"", ")", "\n", "", "if", "not", "isinstance", "(", "self", ".", "use_estimated_pscore", ",", "bool", ")", ":", "\n", "            ", "raise", "TypeError", "(", "\n", "f\"`use_estimated_pscore` must be a bool, but {type(self.use_estimated_pscore)} is given\"", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_multi.MultiLoggersBalancedInverseProbabilityWeighting._estimate_round_rewards": [[376, 421], ["isinstance", "numpy.zeros", "numpy.minimum", "numpy.arange"], "methods", ["None"], ["", "", "def", "_estimate_round_rewards", "(", "\n", "self", ",", "\n", "reward", ":", "np", ".", "ndarray", ",", "\n", "action", ":", "np", ".", "ndarray", ",", "\n", "pscore_avg", ":", "np", ".", "ndarray", ",", "\n", "action_dist", ":", "np", ".", "ndarray", ",", "\n", "position", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "np", ".", "ndarray", ":", "\n", "        ", "\"\"\"Estimate round-wise (or sample-wise) rewards.\n\n        Parameters\n        ----------\n        reward: array-like, shape (n_rounds,)\n            Rewards observed for each data in logged bandit data, i.e., :math:`r_i`.\n\n        action: array-like, shape (n_rounds,)\n            Actions sampled by the logging/behavior policy for each data in logged bandit data, i.e., :math:`a_i`.\n\n        pscore_avg: array-like, shape (n_rounds,)\n            Action choice probabilities of the average logging/behavior policy, i.e., :math:`\\\\pi_{avg}(a_i|x_i)`.\n            If `use_estimated_pscore` is False, `pscore_avg` must be given.\n\n        action_dist: array-like, shape (n_rounds, n_actions, len_list)\n            Action choice probabilities of the evaluation policy (can be deterministic), i.e., :math:`\\\\pi_e(a_i|x_i)`.\n\n        position: array-like, shape (n_rounds,), default=None\n            Indices to differentiate positions in a recommendation interface where the actions are presented.\n            If None, the effect of position on the reward will be ignored.\n            (If only a single action is chosen for each data, you can just ignore this argument.)\n\n        Returns\n        ----------\n        estimated_rewards: array-like, shape (n_rounds,)\n            Estimated rewards for each observation.\n\n        \"\"\"", "\n", "if", "position", "is", "None", ":", "\n", "            ", "position", "=", "np", ".", "zeros", "(", "action_dist", ".", "shape", "[", "0", "]", ",", "dtype", "=", "int", ")", "\n", "\n", "", "iw_avg", "=", "action_dist", "[", "np", ".", "arange", "(", "action", ".", "shape", "[", "0", "]", ")", ",", "action", ",", "position", "]", "/", "pscore_avg", "\n", "# weight clipping", "\n", "if", "isinstance", "(", "iw_avg", ",", "np", ".", "ndarray", ")", ":", "\n", "            ", "iw_avg", "=", "np", ".", "minimum", "(", "iw_avg", ",", "self", ".", "lambda_", ")", "\n", "", "return", "reward", "*", "iw_avg", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_multi.MultiLoggersBalancedInverseProbabilityWeighting.estimate_policy_value": [[422, 491], ["utils.check_array", "utils.check_array", "utils.check_multi_loggers_ope_inputs", "estimators_multi.MultiLoggersBalancedInverseProbabilityWeighting._estimate_round_rewards().mean", "utils.check_array", "utils.check_array", "numpy.zeros", "estimators_multi.MultiLoggersBalancedInverseProbabilityWeighting._estimate_round_rewards"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_multi_loggers_ope_inputs", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SelfNormalizedSlateStandardIPS._estimate_round_rewards"], ["", "def", "estimate_policy_value", "(", "\n", "self", ",", "\n", "reward", ":", "np", ".", "ndarray", ",", "\n", "action", ":", "np", ".", "ndarray", ",", "\n", "action_dist", ":", "np", ".", "ndarray", ",", "\n", "pscore_avg", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "position", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "estimated_pscore_avg", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "np", ".", "ndarray", ":", "\n", "        ", "\"\"\"Estimate the policy value of evaluation policy.\n\n        Parameters\n        ----------\n        reward: array-like, shape (n_rounds,)\n            Rewards observed for each data in logged bandit data, i.e., :math:`r_i`.\n\n        action: array-like, shape (n_rounds,)\n            Actions sampled by the logging/behavior policy for each data in logged bandit data, i.e., :math:`a_i`.\n\n        action_dist: array-like, shape (n_rounds, n_actions, len_list)\n            Action choice probabilities of the evaluation policy (can be deterministic), i.e., :math:`\\\\pi_e(a_i|x_i)`.\n\n        pscore_avg: array-like, shape (n_rounds,), default=None\n            Action choice probabilities of the logging/behavior policy (propensity scores), i.e., :math:`\\\\pi_{avg}(a_i|x_i)`.\n            If `use_estimated_pscore` is False, `pscore_avg` must be given.\n\n        position: array-like, shape (n_rounds,), default=None\n            Indices to differentiate positions in a recommendation interface where the actions are presented.\n            If None, the effect of position on the reward will be ignored.\n            (If only a single action is chosen for each data, you can just ignore this argument.)\n\n        estimated_pscore_avg: array-like, shape (n_rounds,), default=None\n            Estimated average logging/behavior policy, i.e., :math:`\\\\hat{\\\\pi}_{avg}(a_i|x_i)`.\n            If `self.use_estimated_pscore` is True, `estimated_pscore` must be given.\n\n        Returns\n        ----------\n        V_hat: float\n            Estimated policy value of evaluation policy.\n\n        \"\"\"", "\n", "check_array", "(", "array", "=", "reward", ",", "name", "=", "\"reward\"", ",", "expected_dim", "=", "1", ")", "\n", "check_array", "(", "array", "=", "action", ",", "name", "=", "\"action\"", ",", "expected_dim", "=", "1", ")", "\n", "if", "self", ".", "use_estimated_pscore", ":", "\n", "            ", "check_array", "(", "\n", "array", "=", "estimated_pscore_avg", ",", "name", "=", "\"estimated_pscore_avg\"", ",", "expected_dim", "=", "1", "\n", ")", "\n", "pscore_", "=", "estimated_pscore_avg", "\n", "", "else", ":", "\n", "            ", "check_array", "(", "array", "=", "pscore_avg", ",", "name", "=", "\"pscore_avg\"", ",", "expected_dim", "=", "1", ")", "\n", "pscore_", "=", "pscore_avg", "\n", "\n", "", "check_multi_loggers_ope_inputs", "(", "\n", "action_dist", "=", "action_dist", ",", "\n", "position", "=", "position", ",", "\n", "action", "=", "action", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore", "=", "pscore_", ",", "\n", ")", "\n", "if", "position", "is", "None", ":", "\n", "            ", "position", "=", "np", ".", "zeros", "(", "action_dist", ".", "shape", "[", "0", "]", ",", "dtype", "=", "int", ")", "\n", "\n", "", "return", "self", ".", "_estimate_round_rewards", "(", "\n", "reward", "=", "reward", ",", "\n", "action", "=", "action", ",", "\n", "position", "=", "position", ",", "\n", "pscore_avg", "=", "pscore_", ",", "\n", "action_dist", "=", "action_dist", ",", "\n", ")", ".", "mean", "(", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_multi.MultiLoggersBalancedInverseProbabilityWeighting.estimate_interval": [[493, 580], ["utils.check_array", "utils.check_array", "utils.check_multi_loggers_ope_inputs", "estimators_multi.MultiLoggersBalancedInverseProbabilityWeighting._estimate_round_rewards", "utils.estimate_confidence_interval_by_bootstrap", "utils.check_array", "utils.check_array", "numpy.zeros"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_multi_loggers_ope_inputs", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SelfNormalizedSlateStandardIPS._estimate_round_rewards", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.estimate_confidence_interval_by_bootstrap", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array"], ["", "def", "estimate_interval", "(", "\n", "self", ",", "\n", "reward", ":", "np", ".", "ndarray", ",", "\n", "action", ":", "np", ".", "ndarray", ",", "\n", "action_dist", ":", "np", ".", "ndarray", ",", "\n", "pscore_avg", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "position", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "estimated_pscore_avg", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "alpha", ":", "float", "=", "0.05", ",", "\n", "n_bootstrap_samples", ":", "int", "=", "10000", ",", "\n", "random_state", ":", "Optional", "[", "int", "]", "=", "None", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "Dict", "[", "str", ",", "float", "]", ":", "\n", "        ", "\"\"\"Estimate the confidence interval of the policy value using bootstrap.\n\n        Parameters\n        ----------\n        reward: array-like, shape (n_rounds,)\n            Rewards observed for each data in logged bandit data, i.e., :math:`r_i`.\n\n        action: array-like, shape (n_rounds,)\n            Actions sampled by the logging/behavior policy for each data in logged bandit data, i.e., :math:`a_i`.\n\n        action_dist: array-like, shape (n_rounds, n_actions, len_list)\n            Action choice probabilities of the evaluation policy (can be deterministic), i.e., :math:`\\\\pi_e(a_i|x_i)`.\n\n        pscore_avg: array-like, shape (n_rounds,), default=None\n            Action choice probabilities of the average logging/behavior policy (propensity scores), i.e., :math:`\\\\pi_{avg}(a_i|x_i)`.\n            If `use_estimated_pscore` is False, `pscore_avg` must be given.\n\n        position: array-like, shape (n_rounds,), default=None\n            Indices to differentiate positions in a recommendation interface where the actions are presented.\n            If None, the effect of position on the reward will be ignored.\n            (If only a single action is chosen for each data, you can just ignore this argument.)\n\n        estimated_pscore: array-like, shape (n_rounds,), default=None\n            Estimated logging/behavior policy, i.e., :math:`\\\\hat{\\\\pi}_b(a_i|x_i)`.\n            If `self.use_estimated_pscore` is True, `estimated_pscore` must be given.\n\n        alpha: float, default=0.05\n            Significance level.\n\n        n_bootstrap_samples: int, default=10000\n            Number of resampling performed in bootstrap sampling.\n\n        random_state: int, default=None\n            Controls the random seed in bootstrap sampling.\n\n        Returns\n        ----------\n        estimated_confidence_interval: Dict[str, float]\n            Dictionary storing the estimated mean and upper-lower confidence bounds.\n\n        \"\"\"", "\n", "check_array", "(", "array", "=", "reward", ",", "name", "=", "\"reward\"", ",", "expected_dim", "=", "1", ")", "\n", "check_array", "(", "array", "=", "action", ",", "name", "=", "\"action\"", ",", "expected_dim", "=", "1", ")", "\n", "if", "self", ".", "use_estimated_pscore", ":", "\n", "            ", "check_array", "(", "\n", "array", "=", "estimated_pscore_avg", ",", "name", "=", "\"estimated_pscore_avg\"", ",", "expected_dim", "=", "1", "\n", ")", "\n", "pscore_", "=", "estimated_pscore_avg", "\n", "", "else", ":", "\n", "            ", "check_array", "(", "array", "=", "pscore_avg", ",", "name", "=", "\"pscore_avg\"", ",", "expected_dim", "=", "1", ")", "\n", "pscore_", "=", "pscore_avg", "\n", "\n", "", "check_multi_loggers_ope_inputs", "(", "\n", "action_dist", "=", "action_dist", ",", "\n", "position", "=", "position", ",", "\n", "action", "=", "action", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore", "=", "pscore_", ",", "\n", ")", "\n", "if", "position", "is", "None", ":", "\n", "            ", "position", "=", "np", ".", "zeros", "(", "action_dist", ".", "shape", "[", "0", "]", ",", "dtype", "=", "int", ")", "\n", "\n", "", "estimated_round_rewards", "=", "self", ".", "_estimate_round_rewards", "(", "\n", "reward", "=", "reward", ",", "\n", "action", "=", "action", ",", "\n", "position", "=", "position", ",", "\n", "pscore", "=", "pscore_", ",", "\n", "action_dist", "=", "action_dist", ",", "\n", ")", "\n", "return", "estimate_confidence_interval_by_bootstrap", "(", "\n", "samples", "=", "estimated_round_rewards", ",", "\n", "alpha", "=", "alpha", ",", "\n", "n_bootstrap_samples", "=", "n_bootstrap_samples", ",", "\n", "random_state", "=", "random_state", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_multi.MultiLoggersWeightedInverseProbabilityWeighting._estimate_round_rewards": [[636, 694], ["isinstance", "numpy.unique", "numpy.zeros", "numpy.zeros", "numpy.minimum", "numpy.var", "numpy.sum", "numpy.arange"], "methods", ["None"], ["def", "_estimate_round_rewards", "(", "\n", "self", ",", "\n", "reward", ":", "np", ".", "ndarray", ",", "\n", "action", ":", "np", ".", "ndarray", ",", "\n", "pscore", ":", "np", ".", "ndarray", ",", "\n", "stratum_idx", ":", "np", ".", "ndarray", ",", "\n", "action_dist", ":", "np", ".", "ndarray", ",", "\n", "position", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "np", ".", "ndarray", ":", "\n", "        ", "\"\"\"Estimate round-wise (or sample-wise) rewards.\n\n        Parameters\n        ----------\n        reward: array-like, shape (n_rounds,)\n            Rewards observed for each data in logged bandit data, i.e., :math:`r_i`.\n\n        action: array-like, shape (n_rounds,)\n            Actions sampled by the logging/behavior policy for each data in logged bandit data, i.e., :math:`a_i`.\n\n        pscore: array-like, shape (n_rounds,)\n            Action choice probabilities of the logging/behavior policy (propensity scores), i.e., :math:`\\\\pi_k(a_i|x_i)`.\n            If `use_estimated_pscore` is False, `pscore` must be given.\n\n        stratum_idx: array-like, shape (n_rounds,)\n            Indices to differentiate the logging/behavior policy that generate each data, i.e., :math:`k`.\n\n        action_dist: array-like, shape (n_rounds, n_actions, len_list)\n            Action choice probabilities of the evaluation policy (can be deterministic), i.e., :math:`\\\\pi_e(a_i|x_i)`.\n\n        position: array-like, shape (n_rounds,), default=None\n            Indices to differentiate positions in a recommendation interface where the actions are presented.\n            If None, the effect of position on the reward will be ignored.\n            (If only a single action is chosen for each data, you can just ignore this argument.)\n\n        Returns\n        ----------\n        estimated_rewards: array-like, shape (n_rounds,)\n            Estimated rewards for each observation.\n\n        \"\"\"", "\n", "if", "position", "is", "None", ":", "\n", "            ", "position", "=", "np", ".", "zeros", "(", "action_dist", ".", "shape", "[", "0", "]", ",", "dtype", "=", "int", ")", "\n", "\n", "", "n", "=", "action", ".", "shape", "[", "0", "]", "\n", "iw", "=", "action_dist", "[", "np", ".", "arange", "(", "n", ")", ",", "action", ",", "position", "]", "/", "pscore", "\n", "# weight clipping", "\n", "if", "isinstance", "(", "iw", ",", "np", ".", "ndarray", ")", ":", "\n", "            ", "iw", "=", "np", ".", "minimum", "(", "iw", ",", "self", ".", "lambda_", ")", "\n", "\n", "", "unique_stratum_idx", ",", "n_data_strata", "=", "np", ".", "unique", "(", "stratum_idx", ",", "return_counts", "=", "True", ")", "\n", "var_k", "=", "np", ".", "zeros", "(", "unique_stratum_idx", ".", "shape", "[", "0", "]", ")", "\n", "for", "k", "in", "unique_stratum_idx", ":", "\n", "            ", "idx_", "=", "stratum_idx", "==", "k", "\n", "var_k", "[", "k", "]", "=", "np", ".", "var", "(", "reward", "[", "idx_", "]", "*", "iw", "[", "idx_", "]", ")", "\n", "", "weight_k", "=", "n", "/", "(", "var_k", "*", "np", ".", "sum", "(", "n_data_strata", "/", "var_k", ")", ")", "\n", "\n", "return", "reward", "*", "iw", "*", "weight_k", "[", "stratum_idx", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_multi.MultiLoggersWeightedInverseProbabilityWeighting.estimate_policy_value": [[695, 769], ["utils.check_array", "utils.check_array", "utils.check_array", "utils.check_multi_loggers_ope_inputs", "estimators_multi.MultiLoggersWeightedInverseProbabilityWeighting._estimate_round_rewards().mean", "utils.check_array", "utils.check_array", "numpy.zeros", "estimators_multi.MultiLoggersWeightedInverseProbabilityWeighting._estimate_round_rewards"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_multi_loggers_ope_inputs", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SelfNormalizedSlateStandardIPS._estimate_round_rewards"], ["", "def", "estimate_policy_value", "(", "\n", "self", ",", "\n", "reward", ":", "np", ".", "ndarray", ",", "\n", "action", ":", "np", ".", "ndarray", ",", "\n", "action_dist", ":", "np", ".", "ndarray", ",", "\n", "stratum_idx", ":", "np", ".", "ndarray", ",", "\n", "pscore", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "position", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "estimated_pscore", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "np", ".", "ndarray", ":", "\n", "        ", "\"\"\"Estimate the policy value of evaluation policy.\n\n        Parameters\n        ----------\n        reward: array-like, shape (n_rounds,)\n            Rewards observed for each data in logged bandit data, i.e., :math:`r_i`.\n\n        action: array-like, shape (n_rounds,)\n            Actions sampled by the logging/behavior policy for each data in logged bandit data, i.e., :math:`a_i`.\n\n        action_dist: array-like, shape (n_rounds, n_actions, len_list)\n            Action choice probabilities of the evaluation policy (can be deterministic), i.e., :math:`\\\\pi_e(a_i|x_i)`.\n\n        stratum_idx: array-like, shape (n_rounds,)\n            Indices to differentiate the logging/behavior policy that generate each data, i.e., :math:`k`.\n\n        pscore: array-like, shape (n_rounds,), default=None\n            Action choice probabilities of the logging/behavior policy (propensity scores), i.e., :math:`\\\\pi_k(a_i|x_i)`.\n            If `use_estimated_pscore` is False, `pscore` must be given.\n\n        position: array-like, shape (n_rounds,), default=None\n            Indices to differentiate positions in a recommendation interface where the actions are presented.\n            If None, the effect of position on the reward will be ignored.\n            (If only a single action is chosen for each data, you can just ignore this argument.)\n\n        estimated_pscore: array-like, shape (n_rounds,), default=None\n            Estimated behavior policy (propensity scores), i.e., :math:`\\\\hat{\\\\pi}_k(a_i|x_i)`.\n            If `self.use_estimated_pscore` is True, `estimated_pscore` must be given.\n\n        Returns\n        ----------\n        V_hat: float\n            Estimated policy value of evaluation policy.\n\n        \"\"\"", "\n", "check_array", "(", "array", "=", "reward", ",", "name", "=", "\"reward\"", ",", "expected_dim", "=", "1", ")", "\n", "check_array", "(", "array", "=", "action", ",", "name", "=", "\"action\"", ",", "expected_dim", "=", "1", ")", "\n", "check_array", "(", "array", "=", "stratum_idx", ",", "name", "=", "\"stratum_idx\"", ",", "expected_dim", "=", "1", ")", "\n", "if", "self", ".", "use_estimated_pscore", ":", "\n", "            ", "check_array", "(", "array", "=", "estimated_pscore", ",", "name", "=", "\"estimated_pscore\"", ",", "expected_dim", "=", "1", ")", "\n", "pscore_", "=", "estimated_pscore", "\n", "", "else", ":", "\n", "            ", "check_array", "(", "array", "=", "pscore", ",", "name", "=", "\"pscore\"", ",", "expected_dim", "=", "1", ")", "\n", "pscore_", "=", "pscore", "\n", "\n", "", "check_multi_loggers_ope_inputs", "(", "\n", "action_dist", "=", "action_dist", ",", "\n", "position", "=", "position", ",", "\n", "action", "=", "action", ",", "\n", "reward", "=", "reward", ",", "\n", "stratum_idx", "=", "stratum_idx", ",", "\n", "pscore", "=", "pscore_", ",", "\n", ")", "\n", "if", "position", "is", "None", ":", "\n", "            ", "position", "=", "np", ".", "zeros", "(", "action_dist", ".", "shape", "[", "0", "]", ",", "dtype", "=", "int", ")", "\n", "\n", "", "return", "self", ".", "_estimate_round_rewards", "(", "\n", "reward", "=", "reward", ",", "\n", "action", "=", "action", ",", "\n", "position", "=", "position", ",", "\n", "pscore", "=", "pscore_", ",", "\n", "stratum_idx", "=", "stratum_idx", ",", "\n", "action_dist", "=", "action_dist", ",", "\n", ")", ".", "mean", "(", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_multi.MultiLoggersWeightedInverseProbabilityWeighting.estimate_interval": [[771, 863], ["utils.check_array", "utils.check_array", "utils.check_array", "utils.check_multi_loggers_ope_inputs", "estimators_multi.MultiLoggersWeightedInverseProbabilityWeighting._estimate_round_rewards", "utils.estimate_confidence_interval_by_bootstrap", "utils.check_array", "utils.check_array", "numpy.zeros"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_multi_loggers_ope_inputs", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SelfNormalizedSlateStandardIPS._estimate_round_rewards", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.estimate_confidence_interval_by_bootstrap", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array"], ["", "def", "estimate_interval", "(", "\n", "self", ",", "\n", "reward", ":", "np", ".", "ndarray", ",", "\n", "action", ":", "np", ".", "ndarray", ",", "\n", "stratum_idx", ":", "np", ".", "ndarray", ",", "\n", "action_dist", ":", "np", ".", "ndarray", ",", "\n", "pscore", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "position", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "estimated_pscore", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "alpha", ":", "float", "=", "0.05", ",", "\n", "n_bootstrap_samples", ":", "int", "=", "10000", ",", "\n", "random_state", ":", "Optional", "[", "int", "]", "=", "None", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "Dict", "[", "str", ",", "float", "]", ":", "\n", "        ", "\"\"\"Estimate the confidence interval of the policy value using bootstrap.\n\n        Parameters\n        ----------\n        reward: array-like, shape (n_rounds,)\n            Rewards observed for each data in logged bandit data, i.e., :math:`r_i`.\n\n        action: array-like, shape (n_rounds,)\n            Actions sampled by the logging/behavior policy for each data in logged bandit data, i.e., :math:`a_i`.\n\n        action_dist: array-like, shape (n_rounds, n_actions, len_list)\n            Action choice probabilities of the evaluation policy (can be deterministic), i.e., :math:`\\\\pi_e(a_i|x_i)`.\n\n        stratum_idx: array-like, shape (n_rounds,)\n            Indices to differentiate the logging/behavior policy that generate each data, i.e., :math:`k_i`.\n\n        pscore: array-like, shape (n_rounds,), default=None\n            Action choice probabilities of the logging/behavior policy (propensity scores), i.e., :math:`\\\\pi_k(a_i|x_i)`.\n            If `use_estimated_pscore` is False, `pscore` must be given.\n\n        position: array-like, shape (n_rounds,), default=None\n            Indices to differentiate positions in a recommendation interface where the actions are presented.\n            If None, the effect of position on the reward will be ignored.\n            (If only a single action is chosen for each data, you can just ignore this argument.)\n\n        estimated_pscore: array-like, shape (n_rounds,), default=None\n            Estimated behavior policy (propensity scores), i.e., :math:`\\\\hat{\\\\pi}_b(a_i|x_i)`.\n            If `self.use_estimated_pscore` is True, `estimated_pscore` must be given.\n\n        alpha: float, default=0.05\n            Significance level.\n\n        n_bootstrap_samples: int, default=10000\n            Number of resampling performed in bootstrap sampling.\n\n        random_state: int, default=None\n            Controls the random seed in bootstrap sampling.\n\n        Returns\n        ----------\n        estimated_confidence_interval: Dict[str, float]\n            Dictionary storing the estimated mean and upper-lower confidence bounds.\n\n        \"\"\"", "\n", "check_array", "(", "array", "=", "reward", ",", "name", "=", "\"reward\"", ",", "expected_dim", "=", "1", ")", "\n", "check_array", "(", "array", "=", "action", ",", "name", "=", "\"action\"", ",", "expected_dim", "=", "1", ")", "\n", "check_array", "(", "array", "=", "stratum_idx", ",", "name", "=", "\"stratum_idx\"", ",", "expected_dim", "=", "1", ")", "\n", "if", "self", ".", "use_estimated_pscore", ":", "\n", "            ", "check_array", "(", "array", "=", "estimated_pscore", ",", "name", "=", "\"estimated_pscore\"", ",", "expected_dim", "=", "1", ")", "\n", "pscore_", "=", "estimated_pscore", "\n", "", "else", ":", "\n", "            ", "check_array", "(", "array", "=", "pscore", ",", "name", "=", "\"pscore\"", ",", "expected_dim", "=", "1", ")", "\n", "pscore_", "=", "pscore", "\n", "\n", "", "check_multi_loggers_ope_inputs", "(", "\n", "action_dist", "=", "action_dist", ",", "\n", "position", "=", "position", ",", "\n", "action", "=", "action", ",", "\n", "reward", "=", "reward", ",", "\n", "stratum_idx", "=", "stratum_idx", ",", "\n", "pscore", "=", "pscore_", ",", "\n", ")", "\n", "if", "position", "is", "None", ":", "\n", "            ", "position", "=", "np", ".", "zeros", "(", "action_dist", ".", "shape", "[", "0", "]", ",", "dtype", "=", "int", ")", "\n", "\n", "", "estimated_round_rewards", "=", "self", ".", "_estimate_round_rewards", "(", "\n", "reward", "=", "reward", ",", "\n", "action", "=", "action", ",", "\n", "position", "=", "position", ",", "\n", "stratum_idx", "=", "stratum_idx", ",", "\n", "pscore", "=", "pscore_", ",", "\n", "action_dist", "=", "action_dist", ",", "\n", ")", "\n", "return", "estimate_confidence_interval_by_bootstrap", "(", "\n", "samples", "=", "estimated_round_rewards", ",", "\n", "alpha", "=", "alpha", ",", "\n", "n_bootstrap_samples", "=", "n_bootstrap_samples", ",", "\n", "random_state", "=", "random_state", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_multi.MultiLoggersNaiveDoublyRobust.__post_init__": [[919, 932], ["sklearn.utils.check_scalar", "ValueError", "isinstance", "TypeError", "type"], "methods", ["None"], ["def", "__post_init__", "(", "self", ")", "->", "None", ":", "\n", "        ", "\"\"\"Initialize Class.\"\"\"", "\n", "check_scalar", "(", "\n", "self", ".", "lambda_", ",", "\n", "name", "=", "\"lambda_\"", ",", "\n", "target_type", "=", "(", "int", ",", "float", ")", ",", "\n", "min_val", "=", "0.0", ",", "\n", ")", "\n", "if", "self", ".", "lambda_", "!=", "self", ".", "lambda_", ":", "\n", "            ", "raise", "ValueError", "(", "\"`lambda_` must not be nan\"", ")", "\n", "", "if", "not", "isinstance", "(", "self", ".", "use_estimated_pscore", ",", "bool", ")", ":", "\n", "            ", "raise", "TypeError", "(", "\n", "f\"`use_estimated_pscore` must be a bool, but {type(self.use_estimated_pscore)} is given\"", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_multi.MultiLoggersNaiveDoublyRobust._estimate_round_rewards": [[934, 995], ["isinstance", "numpy.average", "numpy.zeros", "numpy.minimum", "numpy.arange", "numpy.arange", "numpy.arange", "numpy.arange"], "methods", ["None"], ["", "", "def", "_estimate_round_rewards", "(", "\n", "self", ",", "\n", "reward", ":", "np", ".", "ndarray", ",", "\n", "action", ":", "np", ".", "ndarray", ",", "\n", "pscore", ":", "np", ".", "ndarray", ",", "\n", "action_dist", ":", "np", ".", "ndarray", ",", "\n", "estimated_rewards_by_reg_model", ":", "np", ".", "ndarray", ",", "\n", "position", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "np", ".", "ndarray", ":", "\n", "        ", "\"\"\"Estimate round-wise (or sample-wise) rewards.\n\n        Parameters\n        ----------\n        reward: array-like, shape (n_rounds,)\n            Rewards observed for each data in logged bandit data, i.e., :math:`r_i`.\n\n        action: array-like, shape (n_rounds,)\n            Actions sampled by the logging/behavior policy for each data in logged bandit data, i.e., :math:`a_i`.\n\n        pscore: array-like, shape (n_rounds,)\n            Action choice probabilities of the logging/behavior policy (propensity scores), i.e., :math:`\\\\pi_k(a_i|x_i)`.\n            If `use_estimated_pscore` is False, `pscore` must be given.\n\n        action_dist: array-like, shape (n_rounds, n_actions, len_list)\n            Action choice probabilities of the evaluation policy (can be deterministic), i.e., :math:`\\\\pi_e(a_i|x_i)`.\n\n        estimated_rewards_by_reg_model: array-like, shape (n_rounds, n_actions, len_list)\n            Estimated expected rewards given context, action, and position, i.e., :math:`\\\\hat{q}(x_i,a_i)`.\n\n        position: array-like, shape (n_rounds,), default=None\n            Indices to differentiate positions in a recommendation interface where the actions are presented.\n            If None, the effect of position on the reward will be ignored.\n            (If only a single action is chosen for each data, you can just ignore this argument.)\n\n        Returns\n        ----------\n        estimated_rewards: array-like, shape (n_rounds,)\n            Estimated rewards for each observation.\n\n        \"\"\"", "\n", "if", "position", "is", "None", ":", "\n", "            ", "position", "=", "np", ".", "zeros", "(", "action_dist", ".", "shape", "[", "0", "]", ",", "dtype", "=", "int", ")", "\n", "\n", "", "iw", "=", "action_dist", "[", "np", ".", "arange", "(", "action", ".", "shape", "[", "0", "]", ")", ",", "action", ",", "position", "]", "/", "pscore", "\n", "# weight clipping", "\n", "if", "isinstance", "(", "iw", ",", "np", ".", "ndarray", ")", ":", "\n", "            ", "iw", "=", "np", ".", "minimum", "(", "iw", ",", "self", ".", "lambda_", ")", "\n", "\n", "", "n", "=", "action", ".", "shape", "[", "0", "]", "\n", "q_hat_at_position", "=", "estimated_rewards_by_reg_model", "[", "np", ".", "arange", "(", "n", ")", ",", ":", ",", "position", "]", "\n", "q_hat_factual", "=", "estimated_rewards_by_reg_model", "[", "np", ".", "arange", "(", "n", ")", ",", "action", ",", "position", "]", "\n", "pi_e_at_position", "=", "action_dist", "[", "np", ".", "arange", "(", "n", ")", ",", ":", ",", "position", "]", "\n", "estimated_rewards", "=", "np", ".", "average", "(", "\n", "q_hat_at_position", ",", "\n", "weights", "=", "pi_e_at_position", ",", "\n", "axis", "=", "1", ",", "\n", ")", "\n", "estimated_rewards", "+=", "iw", "*", "(", "reward", "-", "q_hat_factual", ")", "\n", "\n", "return", "estimated_rewards", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_multi.MultiLoggersNaiveDoublyRobust.estimate_policy_value": [[996, 1074], ["utils.check_array", "utils.check_array", "utils.check_array", "utils.check_multi_loggers_ope_inputs", "estimators_multi.MultiLoggersNaiveDoublyRobust._estimate_round_rewards().mean", "utils.check_array", "utils.check_array", "numpy.zeros", "estimators_multi.MultiLoggersNaiveDoublyRobust._estimate_round_rewards"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_multi_loggers_ope_inputs", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SelfNormalizedSlateStandardIPS._estimate_round_rewards"], ["", "def", "estimate_policy_value", "(", "\n", "self", ",", "\n", "reward", ":", "np", ".", "ndarray", ",", "\n", "action", ":", "np", ".", "ndarray", ",", "\n", "action_dist", ":", "np", ".", "ndarray", ",", "\n", "estimated_rewards_by_reg_model", ":", "np", ".", "ndarray", ",", "\n", "pscore", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "position", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "estimated_pscore", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "np", ".", "ndarray", ":", "\n", "        ", "\"\"\"Estimate the policy value of evaluation policy.\n\n        Parameters\n        ----------\n        reward: array-like, shape (n_rounds,)\n            Rewards observed for each data in logged bandit data, i.e., :math:`r_i`.\n\n        action: array-like, shape (n_rounds,)\n            Actions sampled by the logging/behavior policy for each data in logged bandit data, i.e., :math:`a_i`.\n\n        action_dist: array-like, shape (n_rounds, n_actions, len_list)\n            Action choice probabilities of the evaluation policy (can be deterministic), i.e., :math:`\\\\pi_e(a_i|x_i)`.\n\n        estimated_rewards_by_reg_model: array-like, shape (n_rounds, n_actions, len_list)\n            Estimated expected rewards given context, action, and position, i.e., :math:`\\\\hat{q}(x_i,a_i)`.\n\n        pscore: array-like, shape (n_rounds,), default=None\n            Action choice probabilities of the logging/behavior policy (propensity scores), i.e., :math:`\\\\pi_k(a_i|x_i)`.\n            If `use_estimated_pscore` is False, `pscore` must be given.\n\n        position: array-like, shape (n_rounds,), default=None\n            Indices to differentiate positions in a recommendation interface where the actions are presented.\n            If None, the effect of position on the reward will be ignored.\n            (If only a single action is chosen for each data, you can just ignore this argument.)\n\n        estimated_pscore: array-like, shape (n_rounds,), default=None\n            Estimated behavior policy (propensity scores), i.e., :math:`\\\\hat{\\\\pi}_k(a_i|x_i)`.\n            If `self.use_estimated_pscore` is True, `estimated_pscore` must be given.\n\n        Returns\n        ----------\n        V_hat: float\n            Estimated policy value of evaluation policy.\n\n        \"\"\"", "\n", "check_array", "(", "\n", "array", "=", "estimated_rewards_by_reg_model", ",", "\n", "name", "=", "\"estimated_rewards_by_reg_model\"", ",", "\n", "expected_dim", "=", "3", ",", "\n", ")", "\n", "check_array", "(", "array", "=", "reward", ",", "name", "=", "\"reward\"", ",", "expected_dim", "=", "1", ")", "\n", "check_array", "(", "array", "=", "action", ",", "name", "=", "\"action\"", ",", "expected_dim", "=", "1", ")", "\n", "if", "self", ".", "use_estimated_pscore", ":", "\n", "            ", "check_array", "(", "array", "=", "estimated_pscore", ",", "name", "=", "\"estimated_pscore\"", ",", "expected_dim", "=", "1", ")", "\n", "pscore_", "=", "estimated_pscore", "\n", "", "else", ":", "\n", "            ", "check_array", "(", "array", "=", "pscore", ",", "name", "=", "\"pscore\"", ",", "expected_dim", "=", "1", ")", "\n", "pscore_", "=", "pscore", "\n", "\n", "", "check_multi_loggers_ope_inputs", "(", "\n", "action_dist", "=", "action_dist", ",", "\n", "position", "=", "position", ",", "\n", "action", "=", "action", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore", "=", "pscore_", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", ")", "\n", "if", "position", "is", "None", ":", "\n", "            ", "position", "=", "np", ".", "zeros", "(", "action_dist", ".", "shape", "[", "0", "]", ",", "dtype", "=", "int", ")", "\n", "\n", "", "return", "self", ".", "_estimate_round_rewards", "(", "\n", "reward", "=", "reward", ",", "\n", "action", "=", "action", ",", "\n", "position", "=", "position", ",", "\n", "pscore", "=", "pscore_", ",", "\n", "action_dist", "=", "action_dist", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", ")", ".", "mean", "(", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_multi.MultiLoggersNaiveDoublyRobust.estimate_interval": [[1076, 1172], ["utils.check_array", "utils.check_array", "utils.check_array", "utils.check_multi_loggers_ope_inputs", "estimators_multi.MultiLoggersNaiveDoublyRobust._estimate_round_rewards", "utils.estimate_confidence_interval_by_bootstrap", "utils.check_array", "utils.check_array", "numpy.zeros"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_multi_loggers_ope_inputs", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SelfNormalizedSlateStandardIPS._estimate_round_rewards", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.estimate_confidence_interval_by_bootstrap", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array"], ["", "def", "estimate_interval", "(", "\n", "self", ",", "\n", "reward", ":", "np", ".", "ndarray", ",", "\n", "action", ":", "np", ".", "ndarray", ",", "\n", "action_dist", ":", "np", ".", "ndarray", ",", "\n", "estimated_rewards_by_reg_model", ":", "np", ".", "ndarray", ",", "\n", "pscore", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "position", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "estimated_pscore", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "alpha", ":", "float", "=", "0.05", ",", "\n", "n_bootstrap_samples", ":", "int", "=", "10000", ",", "\n", "random_state", ":", "Optional", "[", "int", "]", "=", "None", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "Dict", "[", "str", ",", "float", "]", ":", "\n", "        ", "\"\"\"Estimate the confidence interval of the policy value using bootstrap.\n\n        Parameters\n        ----------\n        reward: array-like, shape (n_rounds,)\n            Rewards observed for each data in logged bandit data, i.e., :math:`r_i`.\n\n        action: array-like, shape (n_rounds,)\n            Actions sampled by the logging/behavior policy for each data in logged bandit data, i.e., :math:`a_i`.\n\n        action_dist: array-like, shape (n_rounds, n_actions, len_list)\n            Action choice probabilities of the evaluation policy (can be deterministic), i.e., :math:`\\\\pi_e(a_i|x_i)`.\n\n        estimated_rewards_by_reg_model: array-like, shape (n_rounds, n_actions, len_list)\n            Estimated expected rewards given context, action, and position, i.e., :math:`\\\\hat{q}(x_i,a_i)`.\n\n        pscore: array-like, shape (n_rounds,), default=None\n            Action choice probabilities of the logging/behavior policy (propensity scores), i.e., :math:`\\\\pi_k(a_i|x_i)`.\n            If `use_estimated_pscore` is False, `pscore` must be given.\n\n        position: array-like, shape (n_rounds,), default=None\n            Indices to differentiate positions in a recommendation interface where the actions are presented.\n            If None, the effect of position on the reward will be ignored.\n            (If only a single action is chosen for each data, you can just ignore this argument.)\n\n        estimated_pscore: array-like, shape (n_rounds,), default=None\n            Estimated behavior policy (propensity scores), i.e., :math:`\\\\hat{\\\\pi}_b(a_i|x_i)`.\n            If `self.use_estimated_pscore` is True, `estimated_pscore` must be given.\n\n        alpha: float, default=0.05\n            Significance level.\n\n        n_bootstrap_samples: int, default=10000\n            Number of resampling performed in bootstrap sampling.\n\n        random_state: int, default=None\n            Controls the random seed in bootstrap sampling.\n\n        Returns\n        ----------\n        estimated_confidence_interval: Dict[str, float]\n            Dictionary storing the estimated mean and upper-lower confidence bounds.\n\n        \"\"\"", "\n", "check_array", "(", "\n", "array", "=", "estimated_rewards_by_reg_model", ",", "\n", "name", "=", "\"estimated_rewards_by_reg_model\"", ",", "\n", "expected_dim", "=", "3", ",", "\n", ")", "\n", "check_array", "(", "array", "=", "reward", ",", "name", "=", "\"reward\"", ",", "expected_dim", "=", "1", ")", "\n", "check_array", "(", "array", "=", "action", ",", "name", "=", "\"action\"", ",", "expected_dim", "=", "1", ")", "\n", "if", "self", ".", "use_estimated_pscore", ":", "\n", "            ", "check_array", "(", "array", "=", "estimated_pscore", ",", "name", "=", "\"estimated_pscore\"", ",", "expected_dim", "=", "1", ")", "\n", "pscore_", "=", "estimated_pscore", "\n", "", "else", ":", "\n", "            ", "check_array", "(", "array", "=", "pscore", ",", "name", "=", "\"pscore\"", ",", "expected_dim", "=", "1", ")", "\n", "pscore_", "=", "pscore", "\n", "\n", "", "check_multi_loggers_ope_inputs", "(", "\n", "action_dist", "=", "action_dist", ",", "\n", "position", "=", "position", ",", "\n", "action", "=", "action", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore", "=", "pscore_", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", ")", "\n", "if", "position", "is", "None", ":", "\n", "            ", "position", "=", "np", ".", "zeros", "(", "action_dist", ".", "shape", "[", "0", "]", ",", "dtype", "=", "int", ")", "\n", "\n", "", "estimated_round_rewards", "=", "self", ".", "_estimate_round_rewards", "(", "\n", "reward", "=", "reward", ",", "\n", "action", "=", "action", ",", "\n", "position", "=", "position", ",", "\n", "pscore", "=", "pscore_", ",", "\n", "action_dist", "=", "action_dist", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", ")", "\n", "return", "estimate_confidence_interval_by_bootstrap", "(", "\n", "samples", "=", "estimated_round_rewards", ",", "\n", "alpha", "=", "alpha", ",", "\n", "n_bootstrap_samples", "=", "n_bootstrap_samples", ",", "\n", "random_state", "=", "random_state", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_multi.MultiLoggersBalancedDoublyRobust.__post_init__": [[1229, 1242], ["sklearn.utils.check_scalar", "ValueError", "isinstance", "TypeError", "type"], "methods", ["None"], ["def", "__post_init__", "(", "self", ")", "->", "None", ":", "\n", "        ", "\"\"\"Initialize Class.\"\"\"", "\n", "check_scalar", "(", "\n", "self", ".", "lambda_", ",", "\n", "name", "=", "\"lambda_\"", ",", "\n", "target_type", "=", "(", "int", ",", "float", ")", ",", "\n", "min_val", "=", "0.0", ",", "\n", ")", "\n", "if", "self", ".", "lambda_", "!=", "self", ".", "lambda_", ":", "\n", "            ", "raise", "ValueError", "(", "\"`lambda_` must not be nan\"", ")", "\n", "", "if", "not", "isinstance", "(", "self", ".", "use_estimated_pscore", ",", "bool", ")", ":", "\n", "            ", "raise", "TypeError", "(", "\n", "f\"`use_estimated_pscore` must be a bool, but {type(self.use_estimated_pscore)} is given\"", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_multi.MultiLoggersBalancedDoublyRobust._estimate_round_rewards": [[1244, 1304], ["isinstance", "numpy.average", "numpy.zeros", "numpy.minimum", "numpy.arange", "numpy.arange", "numpy.arange", "numpy.arange"], "methods", ["None"], ["", "", "def", "_estimate_round_rewards", "(", "\n", "self", ",", "\n", "reward", ":", "np", ".", "ndarray", ",", "\n", "action", ":", "np", ".", "ndarray", ",", "\n", "pscore_avg", ":", "np", ".", "ndarray", ",", "\n", "action_dist", ":", "np", ".", "ndarray", ",", "\n", "estimated_rewards_by_reg_model", ":", "np", ".", "ndarray", ",", "\n", "position", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "np", ".", "ndarray", ":", "\n", "        ", "\"\"\"Estimate round-wise (or sample-wise) rewards.\n\n        Parameters\n        ----------\n        reward: array-like, shape (n_rounds,)\n            Rewards observed for each data in logged bandit data, i.e., :math:`r_i`.\n\n        action: array-like, shape (n_rounds,)\n            Actions sampled by the logging/behavior policy for each data in logged bandit data, i.e., :math:`a_i`.\n\n        pscore_avg: array-like, shape (n_rounds,)\n            Action choice probabilities of the average logging/behavior policy, i.e., :math:`\\\\pi_{avg}(a_i|x_i)`.\n            If `use_estimated_pscore` is False, `pscore_avg` must be given.\n\n        action_dist: array-like, shape (n_rounds, n_actions, len_list)\n            Action choice probabilities of the evaluation policy (can be deterministic), i.e., :math:`\\\\pi_e(a_i|x_i)`.\n\n        estimated_rewards_by_reg_model: array-like, shape (n_rounds, n_actions, len_list)\n            Estimated expected rewards given context, action, and position, i.e., :math:`\\\\hat{q}(x_i,a_i)`.\n\n        position: array-like, shape (n_rounds,), default=None\n            Indices to differentiate positions in a recommendation interface where the actions are presented.\n            If None, the effect of position on the reward will be ignored.\n            (If only a single action is chosen for each data, you can just ignore this argument.)\n\n        Returns\n        ----------\n        estimated_rewards: array-like, shape (n_rounds,)\n            Estimated rewards for each observation.\n\n        \"\"\"", "\n", "if", "position", "is", "None", ":", "\n", "            ", "position", "=", "np", ".", "zeros", "(", "action_dist", ".", "shape", "[", "0", "]", ",", "dtype", "=", "int", ")", "\n", "", "iw_avg", "=", "action_dist", "[", "np", ".", "arange", "(", "action", ".", "shape", "[", "0", "]", ")", ",", "action", ",", "position", "]", "/", "pscore_avg", "\n", "# weight clipping", "\n", "if", "isinstance", "(", "iw_avg", ",", "np", ".", "ndarray", ")", ":", "\n", "            ", "iw_avg", "=", "np", ".", "minimum", "(", "iw_avg", ",", "self", ".", "lambda_", ")", "\n", "\n", "", "n", "=", "action", ".", "shape", "[", "0", "]", "\n", "q_hat_at_position", "=", "estimated_rewards_by_reg_model", "[", "np", ".", "arange", "(", "n", ")", ",", ":", ",", "position", "]", "\n", "q_hat_factual", "=", "estimated_rewards_by_reg_model", "[", "np", ".", "arange", "(", "n", ")", ",", "action", ",", "position", "]", "\n", "pi_e_at_position", "=", "action_dist", "[", "np", ".", "arange", "(", "n", ")", ",", ":", ",", "position", "]", "\n", "estimated_rewards", "=", "np", ".", "average", "(", "\n", "q_hat_at_position", ",", "\n", "weights", "=", "pi_e_at_position", ",", "\n", "axis", "=", "1", ",", "\n", ")", "\n", "estimated_rewards", "+=", "iw_avg", "*", "(", "reward", "-", "q_hat_factual", ")", "\n", "\n", "return", "estimated_rewards", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_multi.MultiLoggersBalancedDoublyRobust.estimate_policy_value": [[1305, 1385], ["utils.check_array", "utils.check_array", "utils.check_array", "utils.check_multi_loggers_ope_inputs", "estimators_multi.MultiLoggersBalancedDoublyRobust._estimate_round_rewards().mean", "utils.check_array", "utils.check_array", "numpy.zeros", "estimators_multi.MultiLoggersBalancedDoublyRobust._estimate_round_rewards"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_multi_loggers_ope_inputs", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SelfNormalizedSlateStandardIPS._estimate_round_rewards"], ["", "def", "estimate_policy_value", "(", "\n", "self", ",", "\n", "reward", ":", "np", ".", "ndarray", ",", "\n", "action", ":", "np", ".", "ndarray", ",", "\n", "action_dist", ":", "np", ".", "ndarray", ",", "\n", "estimated_rewards_by_reg_model", ":", "np", ".", "ndarray", ",", "\n", "pscore_avg", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "position", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "estimated_pscore_avg", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "np", ".", "ndarray", ":", "\n", "        ", "\"\"\"Estimate the policy value of evaluation policy.\n\n        Parameters\n        ----------\n        reward: array-like, shape (n_rounds,)\n            Rewards observed for each data in logged bandit data, i.e., :math:`r_i`.\n\n        action: array-like, shape (n_rounds,)\n            Actions sampled by the logging/behavior policy for each data in logged bandit data, i.e., :math:`a_i`.\n\n        action_dist: array-like, shape (n_rounds, n_actions, len_list)\n            Action choice probabilities of the evaluation policy (can be deterministic), i.e., :math:`\\\\pi_e(a_i|x_i)`.\n\n        estimated_rewards_by_reg_model: array-like, shape (n_rounds, n_actions, len_list)\n            Estimated expected rewards given context, action, and position, i.e., :math:`\\\\hat{q}(x_i,a_i)`.\n\n        pscore_avg: array-like, shape (n_rounds,), default=None\n            Action choice probabilities of the logging/behavior policy (propensity scores), i.e., :math:`\\\\pi_{avg}(a_i|x_i)`.\n            If `use_estimated_pscore` is False, `pscore_avg` must be given.\n\n        position: array-like, shape (n_rounds,), default=None\n            Indices to differentiate positions in a recommendation interface where the actions are presented.\n            If None, the effect of position on the reward will be ignored.\n            (If only a single action is chosen for each data, you can just ignore this argument.)\n\n        estimated_pscore_avg: array-like, shape (n_rounds,), default=None\n            Estimated average logging/behavior policy, i.e., :math:`\\\\hat{\\\\pi}_{avg}(a_i|x_i)`.\n            If `self.use_estimated_pscore` is True, `estimated_pscore` must be given.\n\n        Returns\n        ----------\n        V_hat: float\n            Estimated policy value of evaluation policy.\n\n        \"\"\"", "\n", "check_array", "(", "\n", "array", "=", "estimated_rewards_by_reg_model", ",", "\n", "name", "=", "\"estimated_rewards_by_reg_model\"", ",", "\n", "expected_dim", "=", "3", ",", "\n", ")", "\n", "check_array", "(", "array", "=", "reward", ",", "name", "=", "\"reward\"", ",", "expected_dim", "=", "1", ")", "\n", "check_array", "(", "array", "=", "action", ",", "name", "=", "\"action\"", ",", "expected_dim", "=", "1", ")", "\n", "if", "self", ".", "use_estimated_pscore", ":", "\n", "            ", "check_array", "(", "\n", "array", "=", "estimated_pscore_avg", ",", "name", "=", "\"estimated_pscore_avg\"", ",", "expected_dim", "=", "1", "\n", ")", "\n", "pscore_", "=", "estimated_pscore_avg", "\n", "", "else", ":", "\n", "            ", "check_array", "(", "array", "=", "pscore_avg", ",", "name", "=", "\"pscore_avg\"", ",", "expected_dim", "=", "1", ")", "\n", "pscore_", "=", "pscore_avg", "\n", "\n", "", "check_multi_loggers_ope_inputs", "(", "\n", "action_dist", "=", "action_dist", ",", "\n", "position", "=", "position", ",", "\n", "action", "=", "action", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore", "=", "pscore_", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", ")", "\n", "if", "position", "is", "None", ":", "\n", "            ", "position", "=", "np", ".", "zeros", "(", "action_dist", ".", "shape", "[", "0", "]", ",", "dtype", "=", "int", ")", "\n", "\n", "", "return", "self", ".", "_estimate_round_rewards", "(", "\n", "reward", "=", "reward", ",", "\n", "action", "=", "action", ",", "\n", "position", "=", "position", ",", "\n", "pscore_avg", "=", "pscore_", ",", "\n", "action_dist", "=", "action_dist", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", ")", ".", "mean", "(", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_multi.MultiLoggersBalancedDoublyRobust.estimate_interval": [[1387, 1485], ["utils.check_array", "utils.check_array", "utils.check_array", "utils.check_multi_loggers_ope_inputs", "estimators_multi.MultiLoggersBalancedDoublyRobust._estimate_round_rewards", "utils.estimate_confidence_interval_by_bootstrap", "utils.check_array", "utils.check_array", "numpy.zeros"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_multi_loggers_ope_inputs", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SelfNormalizedSlateStandardIPS._estimate_round_rewards", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.estimate_confidence_interval_by_bootstrap", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array"], ["", "def", "estimate_interval", "(", "\n", "self", ",", "\n", "reward", ":", "np", ".", "ndarray", ",", "\n", "action", ":", "np", ".", "ndarray", ",", "\n", "action_dist", ":", "np", ".", "ndarray", ",", "\n", "estimated_rewards_by_reg_model", ":", "np", ".", "ndarray", ",", "\n", "pscore_avg", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "position", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "estimated_pscore_avg", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "alpha", ":", "float", "=", "0.05", ",", "\n", "n_bootstrap_samples", ":", "int", "=", "10000", ",", "\n", "random_state", ":", "Optional", "[", "int", "]", "=", "None", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "Dict", "[", "str", ",", "float", "]", ":", "\n", "        ", "\"\"\"Estimate the confidence interval of the policy value using bootstrap.\n\n        Parameters\n        ----------\n        reward: array-like, shape (n_rounds,)\n            Rewards observed for each data in logged bandit data, i.e., :math:`r_i`.\n\n        action: array-like, shape (n_rounds,)\n            Actions sampled by the logging/behavior policy for each data in logged bandit data, i.e., :math:`a_i`.\n\n        action_dist: array-like, shape (n_rounds, n_actions, len_list)\n            Action choice probabilities of the evaluation policy (can be deterministic), i.e., :math:`\\\\pi_e(a_i|x_i)`.\n\n        estimated_rewards_by_reg_model: array-like, shape (n_rounds, n_actions, len_list)\n            Estimated expected rewards given context, action, and position, i.e., :math:`\\\\hat{q}(x_i,a_i)`.\n\n        pscore_avg: array-like, shape (n_rounds,), default=None\n            Action choice probabilities of the average logging/behavior policy (propensity scores), i.e., :math:`\\\\pi_{avg}(a_i|x_i)`.\n            If `use_estimated_pscore` is False, `pscore_avg` must be given.\n\n        position: array-like, shape (n_rounds,), default=None\n            Indices to differentiate positions in a recommendation interface where the actions are presented.\n            If None, the effect of position on the reward will be ignored.\n            (If only a single action is chosen for each data, you can just ignore this argument.)\n\n        estimated_pscore: array-like, shape (n_rounds,), default=None\n            Estimated logging/behavior policy, i.e., :math:`\\\\hat{\\\\pi}_b(a_i|x_i)`.\n            If `self.use_estimated_pscore` is True, `estimated_pscore` must be given.\n\n        alpha: float, default=0.05\n            Significance level.\n\n        n_bootstrap_samples: int, default=10000\n            Number of resampling performed in bootstrap sampling.\n\n        random_state: int, default=None\n            Controls the random seed in bootstrap sampling.\n\n        Returns\n        ----------\n        estimated_confidence_interval: Dict[str, float]\n            Dictionary storing the estimated mean and upper-lower confidence bounds.\n\n        \"\"\"", "\n", "check_array", "(", "\n", "array", "=", "estimated_rewards_by_reg_model", ",", "\n", "name", "=", "\"estimated_rewards_by_reg_model\"", ",", "\n", "expected_dim", "=", "3", ",", "\n", ")", "\n", "check_array", "(", "array", "=", "reward", ",", "name", "=", "\"reward\"", ",", "expected_dim", "=", "1", ")", "\n", "check_array", "(", "array", "=", "action", ",", "name", "=", "\"action\"", ",", "expected_dim", "=", "1", ")", "\n", "if", "self", ".", "use_estimated_pscore", ":", "\n", "            ", "check_array", "(", "\n", "array", "=", "estimated_pscore_avg", ",", "name", "=", "\"estimated_pscore_avg\"", ",", "expected_dim", "=", "1", "\n", ")", "\n", "pscore_", "=", "estimated_pscore_avg", "\n", "", "else", ":", "\n", "            ", "check_array", "(", "array", "=", "pscore_avg", ",", "name", "=", "\"pscore_avg\"", ",", "expected_dim", "=", "1", ")", "\n", "pscore_", "=", "pscore_avg", "\n", "\n", "", "check_multi_loggers_ope_inputs", "(", "\n", "action_dist", "=", "action_dist", ",", "\n", "position", "=", "position", ",", "\n", "action", "=", "action", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore", "=", "pscore_", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", ")", "\n", "if", "position", "is", "None", ":", "\n", "            ", "position", "=", "np", ".", "zeros", "(", "action_dist", ".", "shape", "[", "0", "]", ",", "dtype", "=", "int", ")", "\n", "\n", "", "estimated_round_rewards", "=", "self", ".", "_estimate_round_rewards", "(", "\n", "reward", "=", "reward", ",", "\n", "action", "=", "action", ",", "\n", "position", "=", "position", ",", "\n", "pscore", "=", "pscore_", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", "action_dist", "=", "action_dist", ",", "\n", ")", "\n", "return", "estimate_confidence_interval_by_bootstrap", "(", "\n", "samples", "=", "estimated_round_rewards", ",", "\n", "alpha", "=", "alpha", ",", "\n", "n_bootstrap_samples", "=", "n_bootstrap_samples", ",", "\n", "random_state", "=", "random_state", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_multi.MultiLoggersWeightedDoublyRobust._estimate_round_rewards": [[1541, 1615], ["isinstance", "numpy.average", "numpy.unique", "numpy.zeros", "numpy.zeros", "numpy.minimum", "numpy.var", "numpy.sum", "numpy.arange", "numpy.arange", "numpy.arange", "numpy.arange"], "methods", ["None"], ["def", "_estimate_round_rewards", "(", "\n", "self", ",", "\n", "reward", ":", "np", ".", "ndarray", ",", "\n", "action", ":", "np", ".", "ndarray", ",", "\n", "pscore", ":", "np", ".", "ndarray", ",", "\n", "stratum_idx", ":", "np", ".", "ndarray", ",", "\n", "action_dist", ":", "np", ".", "ndarray", ",", "\n", "estimated_rewards_by_reg_model", ":", "np", ".", "ndarray", ",", "\n", "position", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "np", ".", "ndarray", ":", "\n", "        ", "\"\"\"Estimate round-wise (or sample-wise) rewards.\n\n        Parameters\n        ----------\n        reward: array-like, shape (n_rounds,)\n            Rewards observed for each data in logged bandit data, i.e., :math:`r_i`.\n\n        action: array-like, shape (n_rounds,)\n            Actions sampled by the logging/behavior policy for each data in logged bandit data, i.e., :math:`a_i`.\n\n        pscore: array-like, shape (n_rounds,)\n            Action choice probabilities of the logging/behavior policy (propensity scores), i.e., :math:`\\\\pi_k(a_i|x_i)`.\n            If `use_estimated_pscore` is False, `pscore` must be given.\n\n        stratum_idx: array-like, shape (n_rounds,)\n            Indices to differentiate the logging/behavior policy that generate each data, i.e., :math:`k`.\n\n        action_dist: array-like, shape (n_rounds, n_actions, len_list)\n            Action choice probabilities of the evaluation policy (can be deterministic), i.e., :math:`\\\\pi_e(a_i|x_i)`.\n\n        estimated_rewards_by_reg_model: array-like, shape (n_rounds, n_actions, len_list)\n            Estimated expected rewards given context, action, and position, i.e., :math:`\\\\hat{q}(x_i,a_i)`.\n\n        position: array-like, shape (n_rounds,), default=None\n            Indices to differentiate positions in a recommendation interface where the actions are presented.\n            If None, the effect of position on the reward will be ignored.\n            (If only a single action is chosen for each data, you can just ignore this argument.)\n\n        Returns\n        ----------\n        estimated_rewards: array-like, shape (n_rounds,)\n            Estimated rewards for each observation.\n\n        \"\"\"", "\n", "if", "position", "is", "None", ":", "\n", "            ", "position", "=", "np", ".", "zeros", "(", "action_dist", ".", "shape", "[", "0", "]", ",", "dtype", "=", "int", ")", "\n", "", "iw", "=", "action_dist", "[", "np", ".", "arange", "(", "action", ".", "shape", "[", "0", "]", ")", ",", "action", ",", "position", "]", "/", "pscore", "\n", "# weight clipping", "\n", "if", "isinstance", "(", "iw", ",", "np", ".", "ndarray", ")", ":", "\n", "            ", "iw", "=", "np", ".", "minimum", "(", "iw", ",", "self", ".", "lambda_", ")", "\n", "\n", "", "n", "=", "action", ".", "shape", "[", "0", "]", "\n", "q_hat_at_position", "=", "estimated_rewards_by_reg_model", "[", "np", ".", "arange", "(", "n", ")", ",", ":", ",", "position", "]", "\n", "q_hat_factual", "=", "estimated_rewards_by_reg_model", "[", "np", ".", "arange", "(", "n", ")", ",", "action", ",", "position", "]", "\n", "pi_e_at_position", "=", "action_dist", "[", "np", ".", "arange", "(", "n", ")", ",", ":", ",", "position", "]", "\n", "estimated_rewards", "=", "np", ".", "average", "(", "\n", "q_hat_at_position", ",", "\n", "weights", "=", "pi_e_at_position", ",", "\n", "axis", "=", "1", ",", "\n", ")", "\n", "\n", "unique_stratum_idx", ",", "n_data_strata", "=", "np", ".", "unique", "(", "stratum_idx", ",", "return_counts", "=", "True", ")", "\n", "var_k", "=", "np", ".", "zeros", "(", "unique_stratum_idx", ".", "shape", "[", "0", "]", ")", "\n", "for", "k", "in", "unique_stratum_idx", ":", "\n", "            ", "idx_", "=", "stratum_idx", "==", "k", "\n", "var_k", "[", "k", "]", "=", "np", ".", "var", "(", "\n", "estimated_rewards", "[", "idx_", "]", "\n", "+", "iw", "[", "idx_", "]", "*", "(", "reward", "[", "idx_", "]", "-", "q_hat_factual", "[", "idx_", "]", ")", "\n", ")", "\n", "", "weight_k", "=", "n", "/", "(", "var_k", "*", "np", ".", "sum", "(", "n_data_strata", "/", "var_k", ")", ")", "\n", "estimated_rewards", "+=", "iw", "*", "(", "reward", "-", "q_hat_factual", ")", "*", "weight_k", "[", "stratum_idx", "]", "\n", "\n", "return", "estimated_rewards", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_multi.MultiLoggersWeightedDoublyRobust.estimate_policy_value": [[1616, 1701], ["utils.check_array", "utils.check_array", "utils.check_array", "utils.check_array", "utils.check_multi_loggers_ope_inputs", "estimators_multi.MultiLoggersWeightedDoublyRobust._estimate_round_rewards().mean", "utils.check_array", "utils.check_array", "numpy.zeros", "estimators_multi.MultiLoggersWeightedDoublyRobust._estimate_round_rewards"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_multi_loggers_ope_inputs", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SelfNormalizedSlateStandardIPS._estimate_round_rewards"], ["", "def", "estimate_policy_value", "(", "\n", "self", ",", "\n", "reward", ":", "np", ".", "ndarray", ",", "\n", "action", ":", "np", ".", "ndarray", ",", "\n", "action_dist", ":", "np", ".", "ndarray", ",", "\n", "stratum_idx", ":", "np", ".", "ndarray", ",", "\n", "estimated_rewards_by_reg_model", ":", "np", ".", "ndarray", ",", "\n", "pscore", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "position", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "estimated_pscore", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "np", ".", "ndarray", ":", "\n", "        ", "\"\"\"Estimate the policy value of evaluation policy.\n\n        Parameters\n        ----------\n        reward: array-like, shape (n_rounds,)\n            Rewards observed for each data in logged bandit data, i.e., :math:`r_i`.\n\n        action: array-like, shape (n_rounds,)\n            Actions sampled by the logging/behavior policy for each data in logged bandit data, i.e., :math:`a_i`.\n\n        action_dist: array-like, shape (n_rounds, n_actions, len_list)\n            Action choice probabilities of the evaluation policy (can be deterministic), i.e., :math:`\\\\pi_e(a_i|x_i)`.\n\n        stratum_idx: array-like, shape (n_rounds,)\n            Indices to differentiate the logging/behavior policy that generate each data, i.e., :math:`k`.\n\n        estimated_rewards_by_reg_model: array-like, shape (n_rounds, n_actions, len_list)\n            Estimated expected rewards given context, action, and position, i.e., :math:`\\\\hat{q}(x_i,a_i)`.\n\n        pscore: array-like, shape (n_rounds,), default=None\n            Action choice probabilities of the logging/behavior policy (propensity scores), i.e., :math:`\\\\pi_k(a_i|x_i)`.\n            If `use_estimated_pscore` is False, `pscore` must be given.\n\n        position: array-like, shape (n_rounds,), default=None\n            Indices to differentiate positions in a recommendation interface where the actions are presented.\n            If None, the effect of position on the reward will be ignored.\n            (If only a single action is chosen for each data, you can just ignore this argument.)\n\n        estimated_pscore: array-like, shape (n_rounds,), default=None\n            Estimated behavior policy (propensity scores), i.e., :math:`\\\\hat{\\\\pi}_k(a_i|x_i)`.\n            If `self.use_estimated_pscore` is True, `estimated_pscore` must be given.\n\n        Returns\n        ----------\n        V_hat: float\n            Estimated policy value of evaluation policy.\n\n        \"\"\"", "\n", "check_array", "(", "\n", "array", "=", "estimated_rewards_by_reg_model", ",", "\n", "name", "=", "\"estimated_rewards_by_reg_model\"", ",", "\n", "expected_dim", "=", "3", ",", "\n", ")", "\n", "check_array", "(", "array", "=", "reward", ",", "name", "=", "\"reward\"", ",", "expected_dim", "=", "1", ")", "\n", "check_array", "(", "array", "=", "action", ",", "name", "=", "\"action\"", ",", "expected_dim", "=", "1", ")", "\n", "check_array", "(", "array", "=", "stratum_idx", ",", "name", "=", "\"stratum_idx\"", ",", "expected_dim", "=", "1", ")", "\n", "if", "self", ".", "use_estimated_pscore", ":", "\n", "            ", "check_array", "(", "array", "=", "estimated_pscore", ",", "name", "=", "\"estimated_pscore\"", ",", "expected_dim", "=", "1", ")", "\n", "pscore_", "=", "estimated_pscore", "\n", "", "else", ":", "\n", "            ", "check_array", "(", "array", "=", "pscore", ",", "name", "=", "\"pscore\"", ",", "expected_dim", "=", "1", ")", "\n", "pscore_", "=", "pscore", "\n", "\n", "", "check_multi_loggers_ope_inputs", "(", "\n", "action_dist", "=", "action_dist", ",", "\n", "position", "=", "position", ",", "\n", "action", "=", "action", ",", "\n", "reward", "=", "reward", ",", "\n", "stratum_idx", "=", "stratum_idx", ",", "\n", "pscore", "=", "pscore_", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", ")", "\n", "if", "position", "is", "None", ":", "\n", "            ", "position", "=", "np", ".", "zeros", "(", "action_dist", ".", "shape", "[", "0", "]", ",", "dtype", "=", "int", ")", "\n", "\n", "", "return", "self", ".", "_estimate_round_rewards", "(", "\n", "reward", "=", "reward", ",", "\n", "action", "=", "action", ",", "\n", "position", "=", "position", ",", "\n", "pscore", "=", "pscore_", ",", "\n", "stratum_idx", "=", "stratum_idx", ",", "\n", "action_dist", "=", "action_dist", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", ")", ".", "mean", "(", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_multi.MultiLoggersWeightedDoublyRobust.estimate_interval": [[1703, 1806], ["utils.check_array", "utils.check_array", "utils.check_array", "utils.check_array", "utils.check_multi_loggers_ope_inputs", "estimators_multi.MultiLoggersWeightedDoublyRobust._estimate_round_rewards", "utils.estimate_confidence_interval_by_bootstrap", "utils.check_array", "utils.check_array", "numpy.zeros"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_multi_loggers_ope_inputs", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SelfNormalizedSlateStandardIPS._estimate_round_rewards", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.estimate_confidence_interval_by_bootstrap", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array"], ["", "def", "estimate_interval", "(", "\n", "self", ",", "\n", "reward", ":", "np", ".", "ndarray", ",", "\n", "action", ":", "np", ".", "ndarray", ",", "\n", "stratum_idx", ":", "np", ".", "ndarray", ",", "\n", "action_dist", ":", "np", ".", "ndarray", ",", "\n", "estimated_rewards_by_reg_model", ":", "np", ".", "ndarray", ",", "\n", "pscore", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "position", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "estimated_pscore", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "alpha", ":", "float", "=", "0.05", ",", "\n", "n_bootstrap_samples", ":", "int", "=", "10000", ",", "\n", "random_state", ":", "Optional", "[", "int", "]", "=", "None", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "Dict", "[", "str", ",", "float", "]", ":", "\n", "        ", "\"\"\"Estimate the confidence interval of the policy value using bootstrap.\n\n        Parameters\n        ----------\n        reward: array-like, shape (n_rounds,)\n            Rewards observed for each data in logged bandit data, i.e., :math:`r_i`.\n\n        action: array-like, shape (n_rounds,)\n            Actions sampled by the logging/behavior policy for each data in logged bandit data, i.e., :math:`a_i`.\n\n        action_dist: array-like, shape (n_rounds, n_actions, len_list)\n            Action choice probabilities of the evaluation policy (can be deterministic), i.e., :math:`\\\\pi_e(a_i|x_i)`.\n\n        stratum_idx: array-like, shape (n_rounds,)\n            Indices to differentiate the logging/behavior policy that generate each data, i.e., :math:`k_i`.\n\n        estimated_rewards_by_reg_model: array-like, shape (n_rounds, n_actions, len_list)\n            Estimated expected rewards given context, action, and position, i.e., :math:`\\\\hat{q}(x_i,a_i)`.\n\n        pscore: array-like, shape (n_rounds,), default=None\n            Action choice probabilities of the logging/behavior policy (propensity scores), i.e., :math:`\\\\pi_k(a_i|x_i)`.\n            If `use_estimated_pscore` is False, `pscore` must be given.\n\n        position: array-like, shape (n_rounds,), default=None\n            Indices to differentiate positions in a recommendation interface where the actions are presented.\n            If None, the effect of position on the reward will be ignored.\n            (If only a single action is chosen for each data, you can just ignore this argument.)\n\n        estimated_pscore: array-like, shape (n_rounds,), default=None\n            Estimated behavior policy (propensity scores), i.e., :math:`\\\\hat{\\\\pi}_b(a_i|x_i)`.\n            If `self.use_estimated_pscore` is True, `estimated_pscore` must be given.\n\n        alpha: float, default=0.05\n            Significance level.\n\n        n_bootstrap_samples: int, default=10000\n            Number of resampling performed in bootstrap sampling.\n\n        random_state: int, default=None\n            Controls the random seed in bootstrap sampling.\n\n        Returns\n        ----------\n        estimated_confidence_interval: Dict[str, float]\n            Dictionary storing the estimated mean and upper-lower confidence bounds.\n\n        \"\"\"", "\n", "check_array", "(", "\n", "array", "=", "estimated_rewards_by_reg_model", ",", "\n", "name", "=", "\"estimated_rewards_by_reg_model\"", ",", "\n", "expected_dim", "=", "3", ",", "\n", ")", "\n", "check_array", "(", "array", "=", "reward", ",", "name", "=", "\"reward\"", ",", "expected_dim", "=", "1", ")", "\n", "check_array", "(", "array", "=", "action", ",", "name", "=", "\"action\"", ",", "expected_dim", "=", "1", ")", "\n", "check_array", "(", "array", "=", "stratum_idx", ",", "name", "=", "\"stratum_idx\"", ",", "expected_dim", "=", "1", ")", "\n", "if", "self", ".", "use_estimated_pscore", ":", "\n", "            ", "check_array", "(", "array", "=", "estimated_pscore", ",", "name", "=", "\"estimated_pscore\"", ",", "expected_dim", "=", "1", ")", "\n", "pscore_", "=", "estimated_pscore", "\n", "", "else", ":", "\n", "            ", "check_array", "(", "array", "=", "pscore", ",", "name", "=", "\"pscore\"", ",", "expected_dim", "=", "1", ")", "\n", "pscore_", "=", "pscore", "\n", "\n", "", "check_multi_loggers_ope_inputs", "(", "\n", "action_dist", "=", "action_dist", ",", "\n", "position", "=", "position", ",", "\n", "action", "=", "action", ",", "\n", "reward", "=", "reward", ",", "\n", "stratum_idx", "=", "stratum_idx", ",", "\n", "pscore", "=", "pscore_", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", ")", "\n", "if", "position", "is", "None", ":", "\n", "            ", "position", "=", "np", ".", "zeros", "(", "action_dist", ".", "shape", "[", "0", "]", ",", "dtype", "=", "int", ")", "\n", "\n", "", "estimated_round_rewards", "=", "self", ".", "_estimate_round_rewards", "(", "\n", "reward", "=", "reward", ",", "\n", "action", "=", "action", ",", "\n", "position", "=", "position", ",", "\n", "stratum_idx", "=", "stratum_idx", ",", "\n", "pscore", "=", "pscore_", ",", "\n", "action_dist", "=", "action_dist", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", ")", "\n", "return", "estimate_confidence_interval_by_bootstrap", "(", "\n", "samples", "=", "estimated_round_rewards", ",", "\n", "alpha", "=", "alpha", ",", "\n", "n_bootstrap_samples", "=", "n_bootstrap_samples", ",", "\n", "random_state", "=", "random_state", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_embed.MarginalizedInverseProbabilityWeighting.__post_init__": [[84, 108], ["sklearn.utils.check_scalar", "sklearn.utils.check_scalar", "sklearn.utils.check_scalar", "sklearn.base.is_classifier", "ValueError", "ValueError"], "methods", ["None"], ["def", "__post_init__", "(", "self", ")", "->", "None", ":", "\n", "        ", "\"\"\"Initialize Class.\"\"\"", "\n", "check_scalar", "(", "self", ".", "n_actions", ",", "name", "=", "\"n_actions\"", ",", "target_type", "=", "int", ",", "min_val", "=", "2", ")", "\n", "check_scalar", "(", "\n", "self", ".", "min_emb_dim", ",", "\n", "name", "=", "\"min_emb_dim\"", ",", "\n", "target_type", "=", "int", ",", "\n", "min_val", "=", "1", ",", "\n", ")", "\n", "check_scalar", "(", "\n", "self", ".", "delta", ",", "\n", "name", "=", "\"delta\"", ",", "\n", "target_type", "=", "float", ",", "\n", "min_val", "=", "0.0", ",", "\n", "max_val", "=", "1.0", ",", "\n", ")", "\n", "if", "self", ".", "embedding_selection_method", "is", "not", "None", ":", "\n", "            ", "if", "self", ".", "embedding_selection_method", "not", "in", "[", "\"exact\"", ",", "\"greedy\"", "]", ":", "\n", "                ", "raise", "ValueError", "(", "\n", "\"If given, `embedding_selection_method` must be either 'exact' or 'greedy', but\"", "\n", "f\"{self.embedding_selection_method} is given.\"", "\n", ")", "\n", "", "", "if", "not", "is_classifier", "(", "self", ".", "pi_a_x_e_estimator", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\"`pi_a_x_e_estimator` must be a classifier.\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_embed.MarginalizedInverseProbabilityWeighting._estimate_round_rewards": [[109, 194], ["numpy.ones", "numpy.ones", "numpy.arange", "estimators_embed.MarginalizedInverseProbabilityWeighting.max", "estimators_embed.MarginalizedInverseProbabilityWeighting._estimate_w_x_e", "estimators_embed.MarginalizedInverseProbabilityWeighting.max", "numpy.sqrt", "scipy.stats.t.ppf", "r_hat.mean", "numpy.var", "numpy.arange", "numpy.arange", "numpy.arange", "numpy.arange", "numpy.arange", "numpy.arange"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_embed.MarginalizedInverseProbabilityWeighting._estimate_w_x_e"], ["", "", "def", "_estimate_round_rewards", "(", "\n", "self", ",", "\n", "context", ":", "np", ".", "ndarray", ",", "\n", "reward", ":", "np", ".", "ndarray", ",", "\n", "action", ":", "np", ".", "ndarray", ",", "\n", "action_embed", ":", "np", ".", "ndarray", ",", "\n", "pi_b", ":", "np", ".", "ndarray", ",", "\n", "action_dist", ":", "np", ".", "ndarray", ",", "\n", "position", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "p_e_a", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "with_dev", ":", "bool", "=", "False", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "np", ".", "ndarray", ":", "\n", "        ", "\"\"\"Estimate round-wise (or sample-wise) rewards.\n\n        Parameters\n        ----------\n        reward: array-like, shape (n_rounds,)\n            Rewards observed for each data in logged bandit data, i.e., :math:`r_i`.\n\n        action: array-like, shape (n_rounds,)\n            Actions sampled by the logging/behavior policy for each data in logged bandit data, i.e., :math:`a_i`.\n\n        action_embed: array-like, shape (n_rounds, dim_action_embed)\n            Context vectors characterizing actions or action embeddings such as item category information.\n            This is used to estimate the marginal importance weights.\n\n        pi_b: array-like, shape (n_rounds, n_actions, len_list)\n            Action choice probabilities of the logging/behavior policy, i.e., :math:`\\\\pi_b(a_i|x_i)`.\n\n        action_dist: array-like, shape (n_rounds, n_actions, len_list)\n            Action choice probabilities of the evaluation policy (can be deterministic), i.e., :math:`\\\\pi_e(a_i|x_i)`.\n\n        position: array-like, shape (n_rounds,), default=None\n            Indices to differentiate positions in a recommendation interface where the actions are presented.\n            If None, the effect of position on the reward will be ignored.\n            (If only a single action is chosen for each data, you can just ignore this argument.)\n\n        p_e_a: array-like, shape (n_actions, n_cat_per_dim, n_cat_dim), default=None\n            Conditional distribution of action embeddings given each action.\n            This distribution is available only when we use synthetic bandit data, i.e.,\n            `obp.dataset.SyntheticBanditDatasetWithActionEmbeds`.\n            See the output of the `obtain_batch_bandit_feedback` argument of this class.\n            If `p_e_a` is given, MIPW uses the true marginal importance weights based on this distribution.\n            The performance of MIPW with the true weights is useful in synthetic experiments of research papers.\n\n        with_dev: bool, default=False.\n            Whether to output a deviation bound with the estimated sample-wise rewards.\n\n        Returns\n        ----------\n        estimated_rewards: array-like, shape (n_rounds,)\n            Estimated rewards for each observation.\n\n        \"\"\"", "\n", "n", "=", "reward", ".", "shape", "[", "0", "]", "\n", "if", "p_e_a", "is", "not", "None", ":", "\n", "            ", "p_e_pi_b", "=", "np", ".", "ones", "(", "n", ")", "\n", "p_e_pi_e", "=", "np", ".", "ones", "(", "n", ")", "\n", "for", "d", "in", "np", ".", "arange", "(", "p_e_a", ".", "shape", "[", "-", "1", "]", ")", ":", "\n", "                ", "p_e_pi_b_d", "=", "pi_b", "[", "np", ".", "arange", "(", "n", ")", ",", ":", ",", "position", "]", "@", "p_e_a", "[", ":", ",", ":", ",", "d", "]", "\n", "p_e_pi_b", "*=", "p_e_pi_b_d", "[", "np", ".", "arange", "(", "n", ")", ",", "action_embed", "[", ":", ",", "d", "]", "]", "\n", "p_e_pi_e_d", "=", "action_dist", "[", "np", ".", "arange", "(", "n", ")", ",", ":", ",", "position", "]", "@", "p_e_a", "[", ":", ",", ":", ",", "d", "]", "\n", "p_e_pi_e", "*=", "p_e_pi_e_d", "[", "np", ".", "arange", "(", "n", ")", ",", "action_embed", "[", ":", ",", "d", "]", "]", "\n", "", "w_x_e", "=", "p_e_pi_e", "/", "p_e_pi_b", "\n", "self", ".", "max_w_x_e", "=", "w_x_e", ".", "max", "(", ")", "\n", "\n", "", "else", ":", "\n", "            ", "w_x_e", "=", "self", ".", "_estimate_w_x_e", "(", "\n", "context", "=", "context", ",", "\n", "action", "=", "action", ",", "\n", "action_embed", "=", "action_embed", ",", "\n", "pi_e", "=", "action_dist", "[", "np", ".", "arange", "(", "n", ")", ",", ":", ",", "position", "]", ",", "\n", "pi_b", "=", "pi_b", "[", "np", ".", "arange", "(", "n", ")", ",", ":", ",", "position", "]", ",", "\n", ")", "\n", "self", ".", "max_w_x_e", "=", "w_x_e", ".", "max", "(", ")", "\n", "\n", "", "if", "with_dev", ":", "\n", "            ", "r_hat", "=", "reward", "*", "w_x_e", "\n", "cnf", "=", "np", ".", "sqrt", "(", "np", ".", "var", "(", "r_hat", ")", "/", "(", "n", "-", "1", ")", ")", "\n", "cnf", "*=", "stats", ".", "t", ".", "ppf", "(", "1.0", "-", "(", "self", ".", "delta", "/", "2", ")", ",", "n", "-", "1", ")", "\n", "\n", "return", "r_hat", ".", "mean", "(", ")", ",", "cnf", "\n", "\n", "", "return", "reward", "*", "w_x_e", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_embed.MarginalizedInverseProbabilityWeighting._estimate_w_x_e": [[195, 218], ["numpy.where", "sklearn.preprocessing.OneHotEncoder().fit_transform", "numpy.zeros", "estimators_embed.MarginalizedInverseProbabilityWeighting.pi_a_x_e_estimator.fit", "estimators_embed.MarginalizedInverseProbabilityWeighting.pi_a_x_e_estimator.predict_proba", "sklearn.preprocessing.OneHotEncoder", "numpy.unique"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.classification_model.PropensityScoreEstimator.fit", "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.offline.NNPolicyLearner.predict_proba"], ["", "def", "_estimate_w_x_e", "(", "\n", "self", ",", "\n", "context", ":", "np", ".", "ndarray", ",", "\n", "action", ":", "np", ".", "ndarray", ",", "\n", "action_embed", ":", "np", ".", "ndarray", ",", "\n", "pi_b", ":", "np", ".", "ndarray", ",", "\n", "pi_e", ":", "np", ".", "ndarray", ",", "\n", ")", "->", "np", ".", "ndarray", ":", "\n", "        ", "\"\"\"Estimate the marginal importance weights.\"\"\"", "\n", "n", "=", "action", ".", "shape", "[", "0", "]", "\n", "w_x_a", "=", "pi_e", "/", "pi_b", "\n", "w_x_a", "=", "np", ".", "where", "(", "w_x_a", "<", "np", ".", "inf", ",", "w_x_a", ",", "0", ")", "\n", "c", "=", "OneHotEncoder", "(", "\n", "sparse", "=", "False", ",", "\n", "drop", "=", "\"first\"", ",", "\n", ")", ".", "fit_transform", "(", "action_embed", ")", "\n", "x_e", "=", "np", ".", "c_", "[", "context", ",", "c", "]", "\n", "pi_a_x_e", "=", "np", ".", "zeros", "(", "(", "n", ",", "self", ".", "n_actions", ")", ")", "\n", "self", ".", "pi_a_x_e_estimator", ".", "fit", "(", "x_e", ",", "action", ")", "\n", "pi_a_x_e", "[", ":", ",", "np", ".", "unique", "(", "action", ")", "]", "=", "self", ".", "pi_a_x_e_estimator", ".", "predict_proba", "(", "x_e", ")", "\n", "w_x_e", "=", "(", "w_x_a", "*", "pi_a_x_e", ")", ".", "sum", "(", "1", ")", "\n", "\n", "return", "w_x_e", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_embed.MarginalizedInverseProbabilityWeighting.estimate_policy_value": [[219, 329], ["obp.utils.check_array", "obp.utils.check_array", "obp.utils.check_array", "obp.utils.check_array", "obp.utils.check_ope_inputs", "obp.utils.check_ope_inputs", "numpy.zeros", "obp.utils.check_array", "obp.utils.check_array", "estimators_embed.MarginalizedInverseProbabilityWeighting._estimate_round_rewards().mean", "estimators_embed.MarginalizedInverseProbabilityWeighting._estimate_with_exact_pruning", "estimators_embed.MarginalizedInverseProbabilityWeighting._estimate_with_greedy_pruning", "estimators_embed.MarginalizedInverseProbabilityWeighting._estimate_round_rewards"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_ope_inputs", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_ope_inputs", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_embed.MarginalizedInverseProbabilityWeighting._estimate_with_exact_pruning", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_embed.MarginalizedInverseProbabilityWeighting._estimate_with_greedy_pruning", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SelfNormalizedSlateStandardIPS._estimate_round_rewards"], ["", "def", "estimate_policy_value", "(", "\n", "self", ",", "\n", "reward", ":", "np", ".", "ndarray", ",", "\n", "action", ":", "np", ".", "ndarray", ",", "\n", "action_embed", ":", "np", ".", "ndarray", ",", "\n", "pi_b", ":", "np", ".", "ndarray", ",", "\n", "action_dist", ":", "np", ".", "ndarray", ",", "\n", "context", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "p_e_a", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "position", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "np", ".", "ndarray", ":", "\n", "        ", "\"\"\"Estimate round-wise (or sample-wise) rewards.\n\n        Parameters\n        ----------\n        reward: array-like, shape (n_rounds,)\n            Rewards observed for each data in logged bandit data, i.e., :math:`r_i`.\n\n        action: array-like, shape (n_rounds,)\n            Actions sampled by the logging/behavior policy for each data in logged bandit data, i.e., :math:`a_i`.\n\n        action_embed: array-like, shape (n_rounds, dim_action_embed)\n            Context vectors characterizing actions or action embeddings such as item category information.\n            This is used to estimate the marginal importance weights.\n\n        pi_b: array-like, shape (n_rounds, n_actions, len_list)\n            Action choice probabilities of the logging/behavior policy, i.e., :math:`\\\\pi_b(a_i|x_i)`.\n\n        action_dist: array-like, shape (n_rounds, n_actions, len_list)\n            Action choice probabilities of the evaluation policy (can be deterministic), i.e., :math:`\\\\pi_e(a_i|x_i)`.\n\n        position: array-like, shape (n_rounds,), default=None\n            Indices to differentiate positions in a recommendation interface where the actions are presented.\n            If None, the effect of position on the reward will be ignored.\n            (If only a single action is chosen for each data, you can just ignore this argument.)\n\n        context: array-like, shape (n_rounds, dim_context), default=None\n            Context vectors observed for each data in logged bandit data, i.e., :math:`x_i`.\n\n        p_e_a: array-like, shape (n_actions, n_cat_per_dim, n_cat_dim), default=None\n            Conditional distribution of action embeddings given each action.\n            This distribution is available only when we use synthetic bandit data, i.e.,\n            `obp.dataset.SyntheticBanditDatasetWithActionEmbeds`.\n            See the output of the `obtain_batch_bandit_feedback` argument of this class.\n            If `p_e_a` is given, MIPW uses the true marginal importance weights based on this distribution.\n            The performance of MIPW with the true weights is useful in synthetic experiments of research papers.\n\n        Returns\n        ----------\n        V_hat: float\n            Estimated policy value of the evaluation policy.\n\n        \"\"\"", "\n", "check_array", "(", "array", "=", "reward", ",", "name", "=", "\"reward\"", ",", "expected_dim", "=", "1", ")", "\n", "check_array", "(", "array", "=", "action_embed", ",", "name", "=", "\"action_embed\"", ",", "expected_dim", "=", "2", ")", "\n", "check_array", "(", "array", "=", "pi_b", ",", "name", "=", "\"pi_b\"", ",", "expected_dim", "=", "3", ")", "\n", "check_array", "(", "array", "=", "action_dist", ",", "name", "=", "\"action_dist\"", ",", "expected_dim", "=", "3", ")", "\n", "check_ope_inputs", "(", "\n", "action_dist", "=", "pi_b", ",", "\n", "position", "=", "position", ",", "\n", "action", "=", "action", ",", "\n", "reward", "=", "reward", ",", "\n", ")", "\n", "check_ope_inputs", "(", "\n", "action_dist", "=", "action_dist", ",", "\n", "position", "=", "position", ",", "\n", "action", "=", "action", ",", "\n", "reward", "=", "reward", ",", "\n", ")", "\n", "if", "position", "is", "None", ":", "\n", "            ", "position", "=", "np", ".", "zeros", "(", "action_dist", ".", "shape", "[", "0", "]", ",", "dtype", "=", "int", ")", "\n", "", "if", "p_e_a", "is", "not", "None", ":", "\n", "            ", "check_array", "(", "array", "=", "p_e_a", ",", "name", "=", "\"p_e_a\"", ",", "expected_dim", "=", "3", ")", "\n", "", "else", ":", "\n", "            ", "check_array", "(", "array", "=", "context", ",", "name", "=", "\"context\"", ",", "expected_dim", "=", "2", ")", "\n", "\n", "", "if", "action_embed", ".", "shape", "[", "1", "]", ">", "1", "and", "self", ".", "embedding_selection_method", "is", "not", "None", ":", "\n", "            ", "if", "self", ".", "embedding_selection_method", "==", "\"exact\"", ":", "\n", "                ", "return", "self", ".", "_estimate_with_exact_pruning", "(", "\n", "context", "=", "context", ",", "\n", "reward", "=", "reward", ",", "\n", "action", "=", "action", ",", "\n", "action_embed", "=", "action_embed", ",", "\n", "position", "=", "position", ",", "\n", "pi_b", "=", "pi_b", ",", "\n", "action_dist", "=", "action_dist", ",", "\n", "p_e_a", "=", "p_e_a", ",", "\n", ")", "\n", "", "elif", "self", ".", "embedding_selection_method", "==", "\"greedy\"", ":", "\n", "                ", "return", "self", ".", "_estimate_with_greedy_pruning", "(", "\n", "context", "=", "context", ",", "\n", "reward", "=", "reward", ",", "\n", "action", "=", "action", ",", "\n", "action_embed", "=", "action_embed", ",", "\n", "position", "=", "position", ",", "\n", "pi_b", "=", "pi_b", ",", "\n", "action_dist", "=", "action_dist", ",", "\n", "p_e_a", "=", "p_e_a", ",", "\n", ")", "\n", "", "", "else", ":", "\n", "            ", "return", "self", ".", "_estimate_round_rewards", "(", "\n", "context", "=", "context", ",", "\n", "reward", "=", "reward", ",", "\n", "action", "=", "action", ",", "\n", "action_embed", "=", "action_embed", ",", "\n", "position", "=", "position", ",", "\n", "pi_b", "=", "pi_b", ",", "\n", "action_dist", "=", "action_dist", ",", "\n", "p_e_a", "=", "p_e_a", ",", "\n", ")", ".", "mean", "(", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_embed.MarginalizedInverseProbabilityWeighting._estimate_with_exact_pruning": [[331, 389], ["numpy.arange", "numpy.arange", "list", "numpy.sqrt", "itertools.combinations", "numpy.argsort", "estimators_embed.MarginalizedInverseProbabilityWeighting._estimate_round_rewards", "estimators_embed.MarginalizedInverseProbabilityWeighting._estimate_round_rewards", "len", "numpy.array", "numpy.array", "theta_list_.append", "cnf_list_.append", "theta_list.append", "cnf_list.append", "theta_list.append", "cnf_list.append", "numpy.abs"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SelfNormalizedSlateStandardIPS._estimate_round_rewards", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SelfNormalizedSlateStandardIPS._estimate_round_rewards"], ["", "", "def", "_estimate_with_exact_pruning", "(", "\n", "self", ",", "\n", "context", ":", "np", ".", "ndarray", ",", "\n", "reward", ":", "np", ".", "ndarray", ",", "\n", "action", ":", "np", ".", "ndarray", ",", "\n", "action_embed", ":", "np", ".", "ndarray", ",", "\n", "pi_b", ":", "np", ".", "ndarray", ",", "\n", "action_dist", ":", "np", ".", "ndarray", ",", "\n", "position", ":", "np", ".", "ndarray", ",", "\n", "p_e_a", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", ")", "->", "float", ":", "\n", "        ", "\"\"\"Apply an exact version of data-drive action embedding selection.\"\"\"", "\n", "n_emb_dim", "=", "action_embed", ".", "shape", "[", "1", "]", "\n", "theta_list", ",", "cnf_list", "=", "[", "]", ",", "[", "]", "\n", "feat_list", ",", "C", "=", "np", ".", "arange", "(", "n_emb_dim", ")", ",", "np", ".", "sqrt", "(", "6", ")", "-", "1", "\n", "for", "i", "in", "np", ".", "arange", "(", "n_emb_dim", ",", "self", ".", "min_emb_dim", "-", "1", ",", "-", "1", ")", ":", "\n", "            ", "comb_list", "=", "list", "(", "itertools", ".", "combinations", "(", "feat_list", ",", "i", ")", ")", "\n", "theta_list_", ",", "cnf_list_", "=", "[", "]", ",", "[", "]", "\n", "for", "comb", "in", "comb_list", ":", "\n", "                ", "if", "p_e_a", "is", "None", ":", "\n", "                    ", "theta", ",", "cnf", "=", "self", ".", "_estimate_round_rewards", "(", "\n", "context", "=", "context", ",", "\n", "reward", "=", "reward", ",", "\n", "action", "=", "action", ",", "\n", "action_embed", "=", "action_embed", "[", ":", ",", "comb", "]", ",", "\n", "pi_b", "=", "pi_b", ",", "\n", "action_dist", "=", "action_dist", ",", "\n", "position", "=", "position", ",", "\n", "with_dev", "=", "True", ",", "\n", ")", "\n", "", "else", ":", "\n", "                    ", "theta", ",", "cnf", "=", "self", ".", "_estimate_round_rewards", "(", "\n", "context", "=", "context", ",", "\n", "reward", "=", "reward", ",", "\n", "action", "=", "action", ",", "\n", "action_embed", "=", "action_embed", "[", ":", ",", "comb", "]", ",", "\n", "pi_b", "=", "pi_b", ",", "\n", "action_dist", "=", "action_dist", ",", "\n", "position", "=", "position", ",", "\n", "p_e_a", "=", "p_e_a", "[", ":", ",", ":", ",", "comb", "]", ",", "\n", "with_dev", "=", "True", ",", "\n", ")", "\n", "", "if", "len", "(", "theta_list", ")", ">", "0", ":", "\n", "                    ", "theta_list_", ".", "append", "(", "theta", ")", ",", "cnf_list_", ".", "append", "(", "cnf", ")", "\n", "", "else", ":", "\n", "                    ", "theta_list", ".", "append", "(", "theta", ")", ",", "cnf_list", ".", "append", "(", "cnf", ")", "\n", "continue", "\n", "\n", "", "", "idx_list", "=", "np", ".", "argsort", "(", "cnf_list_", ")", "[", ":", ":", "-", "1", "]", "\n", "for", "idx", "in", "idx_list", ":", "\n", "                ", "theta_i", ",", "cnf_i", "=", "theta_list_", "[", "idx", "]", ",", "cnf_list_", "[", "idx", "]", "\n", "theta_j", ",", "cnf_j", "=", "np", ".", "array", "(", "theta_list", ")", ",", "np", ".", "array", "(", "cnf_list", ")", "\n", "if", "(", "np", ".", "abs", "(", "theta_j", "-", "theta_i", ")", "<=", "cnf_i", "+", "C", "*", "cnf_j", ")", ".", "all", "(", ")", ":", "\n", "                    ", "theta_list", ".", "append", "(", "theta_i", ")", ",", "cnf_list", ".", "append", "(", "cnf_i", ")", "\n", "", "else", ":", "\n", "                    ", "return", "theta_j", "[", "-", "1", "]", "\n", "\n", "", "", "", "return", "theta_j", "[", "-", "1", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_embed.MarginalizedInverseProbabilityWeighting._estimate_with_greedy_pruning": [[390, 477], ["numpy.arange", "estimators_embed.MarginalizedInverseProbabilityWeighting._estimate_round_rewards", "estimators_embed.MarginalizedInverseProbabilityWeighting._estimate_round_rewards", "theta_list.append", "cnf_list.append", "numpy.where", "numpy.sqrt", "numpy.where", "d_list_.append", "numpy.argsort", "estimators_embed.MarginalizedInverseProbabilityWeighting._estimate_round_rewards", "estimators_embed.MarginalizedInverseProbabilityWeighting._estimate_round_rewards", "theta_list_.append", "cnf_list_.append", "numpy.array", "numpy.array", "theta_list.append", "cnf_list.append", "numpy.abs"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SelfNormalizedSlateStandardIPS._estimate_round_rewards", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SelfNormalizedSlateStandardIPS._estimate_round_rewards", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SelfNormalizedSlateStandardIPS._estimate_round_rewards", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SelfNormalizedSlateStandardIPS._estimate_round_rewards"], ["", "def", "_estimate_with_greedy_pruning", "(", "\n", "self", ",", "\n", "context", ":", "np", ".", "ndarray", ",", "\n", "reward", ":", "np", ".", "ndarray", ",", "\n", "action", ":", "np", ".", "ndarray", ",", "\n", "action_embed", ":", "np", ".", "ndarray", ",", "\n", "pi_b", ":", "np", ".", "ndarray", ",", "\n", "action_dist", ":", "np", ".", "ndarray", ",", "\n", "position", ":", "np", ".", "ndarray", ",", "\n", "p_e_a", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", ")", "->", "float", ":", "\n", "        ", "\"\"\"Apply a greedy version of data-drive action embedding selection.\"\"\"", "\n", "n_emb_dim", "=", "action_embed", ".", "shape", "[", "1", "]", "\n", "theta_list", ",", "cnf_list", "=", "[", "]", ",", "[", "]", "\n", "current_feat", ",", "C", "=", "np", ".", "arange", "(", "n_emb_dim", ")", ",", "np", ".", "sqrt", "(", "6", ")", "-", "1", "\n", "\n", "# init", "\n", "if", "p_e_a", "is", "None", ":", "\n", "            ", "theta", ",", "cnf", "=", "self", ".", "_estimate_round_rewards", "(", "\n", "context", "=", "context", ",", "\n", "reward", "=", "reward", ",", "\n", "action", "=", "action", ",", "\n", "action_embed", "=", "action_embed", "[", ":", ",", "current_feat", "]", ",", "\n", "pi_b", "=", "pi_b", ",", "\n", "action_dist", "=", "action_dist", ",", "\n", "position", "=", "position", ",", "\n", "with_dev", "=", "True", ",", "\n", ")", "\n", "", "else", ":", "\n", "            ", "theta", ",", "cnf", "=", "self", ".", "_estimate_round_rewards", "(", "\n", "context", "=", "context", ",", "\n", "reward", "=", "reward", ",", "\n", "action", "=", "action", ",", "\n", "action_embed", "=", "action_embed", "[", ":", ",", "current_feat", "]", ",", "\n", "pi_b", "=", "pi_b", ",", "\n", "action_dist", "=", "action_dist", ",", "\n", "position", "=", "position", ",", "\n", "p_e_a", "=", "p_e_a", "[", ":", ",", ":", ",", "current_feat", "]", ",", "\n", "with_dev", "=", "True", ",", "\n", ")", "\n", "", "theta_list", ".", "append", "(", "theta", ")", ",", "cnf_list", ".", "append", "(", "cnf", ")", "\n", "\n", "# iterate", "\n", "while", "current_feat", ".", "shape", "[", "0", "]", ">", "self", ".", "min_emb_dim", ":", "\n", "            ", "theta_list_", ",", "cnf_list_", ",", "d_list_", "=", "[", "]", ",", "[", "]", ",", "[", "]", "\n", "for", "d", "in", "current_feat", ":", "\n", "                ", "idx_without_d", "=", "np", ".", "where", "(", "current_feat", "!=", "d", ",", "True", ",", "False", ")", "\n", "candidate_feat", "=", "current_feat", "[", "idx_without_d", "]", "\n", "if", "p_e_a", "is", "None", ":", "\n", "                    ", "theta", ",", "cnf", "=", "self", ".", "_estimate_round_rewards", "(", "\n", "context", "=", "context", ",", "\n", "reward", "=", "reward", ",", "\n", "action", "=", "action", ",", "\n", "action_embed", "=", "action_embed", "[", ":", ",", "candidate_feat", "]", ",", "\n", "pi_b", "=", "pi_b", ",", "\n", "action_dist", "=", "action_dist", ",", "\n", "position", "=", "position", ",", "\n", "with_dev", "=", "True", ",", "\n", ")", "\n", "", "else", ":", "\n", "                    ", "theta", ",", "cnf", "=", "self", ".", "_estimate_round_rewards", "(", "\n", "context", "=", "context", ",", "\n", "reward", "=", "reward", ",", "\n", "action", "=", "action", ",", "\n", "action_embed", "=", "action_embed", "[", ":", ",", "candidate_feat", "]", ",", "\n", "pi_b", "=", "pi_b", ",", "\n", "action_dist", "=", "action_dist", ",", "\n", "position", "=", "position", ",", "\n", "p_e_a", "=", "p_e_a", "[", ":", ",", ":", ",", "candidate_feat", "]", ",", "\n", "with_dev", "=", "True", ",", "\n", ")", "\n", "", "d_list_", ".", "append", "(", "d", ")", "\n", "theta_list_", ".", "append", "(", "theta", ")", ",", "cnf_list_", ".", "append", "(", "cnf", ")", "\n", "\n", "", "idx_list", "=", "np", ".", "argsort", "(", "cnf_list_", ")", "[", ":", ":", "-", "1", "]", "\n", "for", "idx", "in", "idx_list", ":", "\n", "                ", "excluded_dim", "=", "d_list_", "[", "idx", "]", "\n", "theta_i", ",", "cnf_i", "=", "theta_list_", "[", "idx", "]", ",", "cnf_list_", "[", "idx", "]", "\n", "theta_j", ",", "cnf_j", "=", "np", ".", "array", "(", "theta_list", ")", ",", "np", ".", "array", "(", "cnf_list", ")", "\n", "if", "(", "np", ".", "abs", "(", "theta_j", "-", "theta_i", ")", "<=", "cnf_i", "+", "C", "*", "cnf_j", ")", ".", "all", "(", ")", ":", "\n", "                    ", "theta_list", ".", "append", "(", "theta_i", ")", ",", "cnf_list", ".", "append", "(", "cnf_i", ")", "\n", "", "else", ":", "\n", "                    ", "return", "theta_j", "[", "-", "1", "]", "\n", "", "", "idx_without_d", "=", "np", ".", "where", "(", "current_feat", "!=", "excluded_dim", ",", "True", ",", "False", ")", "\n", "current_feat", "=", "current_feat", "[", "idx_without_d", "]", "\n", "\n", "", "return", "theta_j", "[", "-", "1", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_embed.MarginalizedInverseProbabilityWeighting.estimate_interval": [[478, 582], ["obp.utils.check_array", "obp.utils.check_array", "obp.utils.check_array", "obp.utils.check_array", "obp.utils.check_ope_inputs", "obp.utils.check_ope_inputs", "estimators_embed.MarginalizedInverseProbabilityWeighting._estimate_round_rewards", "utils.estimate_confidence_interval_by_bootstrap", "numpy.zeros", "obp.utils.check_array", "obp.utils.check_array"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_ope_inputs", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_ope_inputs", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SelfNormalizedSlateStandardIPS._estimate_round_rewards", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.estimate_confidence_interval_by_bootstrap", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array"], ["", "def", "estimate_interval", "(", "\n", "self", ",", "\n", "reward", ":", "np", ".", "ndarray", ",", "\n", "action", ":", "np", ".", "ndarray", ",", "\n", "action_embed", ":", "np", ".", "ndarray", ",", "\n", "pi_b", ":", "np", ".", "ndarray", ",", "\n", "action_dist", ":", "np", ".", "ndarray", ",", "\n", "context", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "position", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "p_e_a", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "alpha", ":", "float", "=", "0.05", ",", "\n", "n_bootstrap_samples", ":", "int", "=", "10000", ",", "\n", "random_state", ":", "Optional", "[", "int", "]", "=", "None", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "Dict", "[", "str", ",", "float", "]", ":", "\n", "        ", "\"\"\"Estimate the confidence interval of the policy value using nonparametric bootstrap.\n\n        Parameters\n        ----------\n        reward: array-like, shape (n_rounds,)\n            Rewards observed for each data in logged bandit data, i.e., :math:`r_i`.\n\n        action: array-like, shape (n_rounds,)\n            Actions sampled by the logging/behavior policy for each data in logged bandit data, i.e., :math:`a_i`.\n\n        action_embed: array-like, shape (n_rounds, dim_action_embed)\n            Context vectors characterizing actions or action embeddings such as item category information.\n            This is used to estimate the marginal importance weights.\n\n        pi_b: array-like, shape (n_rounds, n_actions, len_list)\n            Action choice probabilities of the logging/behavior policy, i.e., :math:`\\\\pi_b(a_i|x_i)`.\n\n        action_dist: array-like, shape (n_rounds, n_actions, len_list)\n            Action choice probabilities of the evaluation policy (can be deterministic), i.e., :math:`\\\\pi_e(a_i|x_i)`.\n\n        context: array-like, shape (n_rounds, dim_context), default=None\n            Context vectors observed for each data in logged bandit data, i.e., :math:`x_i`.\n\n        position: array-like, shape (n_rounds,), default=None\n            Indices to differentiate positions in a recommendation interface where the actions are presented.\n            If None, the effect of position on the reward will be ignored.\n            (If only a single action is chosen for each data, you can just ignore this argument.)\n\n        p_e_a: array-like, shape (n_actions, n_cat_per_dim, n_cat_dim), default=None\n            Conditional distribution of action embeddings given each action.\n            This distribution is available only when we use synthetic bandit data, i.e.,\n            `obp.dataset.SyntheticBanditDatasetWithActionEmbeds`.\n            See the output of the `obtain_batch_bandit_feedback` argument of this class.\n            If `p_e_a` is given, MIPW uses the true marginal importance weights based on this distribution.\n            The performance of MIPW with the true weights is useful in synthetic experiments of research papers.\n\n        alpha: float, default=0.05\n            Significance level.\n\n        n_bootstrap_samples: int, default=10000\n            Number of resampling performed in bootstrap sampling.\n\n        random_state: int, default=None\n            Controls the random seed in bootstrap sampling.\n\n        Returns\n        ----------\n        estimated_confidence_interval: Dict[str, float]\n            Dictionary storing the estimated mean and upper-lower confidence bounds.\n\n        \"\"\"", "\n", "check_array", "(", "array", "=", "reward", ",", "name", "=", "\"reward\"", ",", "expected_dim", "=", "1", ")", "\n", "check_array", "(", "array", "=", "action_embed", ",", "name", "=", "\"action_embed\"", ",", "expected_dim", "=", "2", ")", "\n", "check_array", "(", "array", "=", "pi_b", ",", "name", "=", "\"pi_b\"", ",", "expected_dim", "=", "3", ")", "\n", "check_array", "(", "array", "=", "action_dist", ",", "name", "=", "\"action_dist\"", ",", "expected_dim", "=", "3", ")", "\n", "check_ope_inputs", "(", "\n", "action_dist", "=", "pi_b", ",", "\n", "position", "=", "position", ",", "\n", "action", "=", "action", ",", "\n", "reward", "=", "reward", ",", "\n", ")", "\n", "check_ope_inputs", "(", "\n", "action_dist", "=", "action_dist", ",", "\n", "position", "=", "position", ",", "\n", "action", "=", "action", ",", "\n", "reward", "=", "reward", ",", "\n", ")", "\n", "if", "position", "is", "None", ":", "\n", "            ", "position", "=", "np", ".", "zeros", "(", "action_dist", ".", "shape", "[", "0", "]", ",", "dtype", "=", "int", ")", "\n", "", "if", "p_e_a", "is", "not", "None", ":", "\n", "            ", "check_array", "(", "array", "=", "p_e_a", ",", "name", "=", "\"p_e_a\"", ",", "expected_dim", "=", "3", ")", "\n", "", "else", ":", "\n", "            ", "check_array", "(", "array", "=", "context", ",", "name", "=", "\"context\"", ",", "expected_dim", "=", "2", ")", "\n", "\n", "", "estimated_round_rewards", "=", "self", ".", "_estimate_round_rewards", "(", "\n", "context", "=", "context", ",", "\n", "reward", "=", "reward", ",", "\n", "action", "=", "action", ",", "\n", "action_embed", "=", "action_embed", ",", "\n", "position", "=", "position", ",", "\n", "pi_b", "=", "pi_b", ",", "\n", "action_dist", "=", "action_dist", ",", "\n", "p_e_a", "=", "p_e_a", ",", "\n", ")", "\n", "return", "estimate_confidence_interval_by_bootstrap", "(", "\n", "samples", "=", "estimated_round_rewards", ",", "\n", "alpha", "=", "alpha", ",", "\n", "n_bootstrap_samples", "=", "n_bootstrap_samples", ",", "\n", "random_state", "=", "random_state", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_embed.SelfNormalizedMarginalizedInverseProbabilityWeighting._estimate_round_rewards": [[634, 719], ["numpy.ones", "numpy.ones", "numpy.arange", "estimators_embed.SelfNormalizedMarginalizedInverseProbabilityWeighting.max", "estimators_embed.SelfNormalizedMarginalizedInverseProbabilityWeighting._estimate_w_x_e", "estimators_embed.SelfNormalizedMarginalizedInverseProbabilityWeighting.max", "numpy.sqrt", "scipy.stats.t.ppf", "estimators_embed.SelfNormalizedMarginalizedInverseProbabilityWeighting.mean", "r_hat.mean", "numpy.var", "numpy.arange", "numpy.arange", "numpy.arange", "numpy.arange", "numpy.arange", "numpy.arange"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_embed.MarginalizedInverseProbabilityWeighting._estimate_w_x_e"], ["def", "_estimate_round_rewards", "(", "\n", "self", ",", "\n", "context", ":", "np", ".", "ndarray", ",", "\n", "reward", ":", "np", ".", "ndarray", ",", "\n", "action", ":", "np", ".", "ndarray", ",", "\n", "action_embed", ":", "np", ".", "ndarray", ",", "\n", "pi_b", ":", "np", ".", "ndarray", ",", "\n", "action_dist", ":", "np", ".", "ndarray", ",", "\n", "position", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "p_e_a", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "with_dev", ":", "bool", "=", "False", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "np", ".", "ndarray", ":", "\n", "        ", "\"\"\"Estimate round-wise (or sample-wise) rewards.\n\n        Parameters\n        ----------\n        reward: array-like, shape (n_rounds,)\n            Rewards observed for each data in logged bandit data, i.e., :math:`r_i`.\n\n        action: array-like, shape (n_rounds,)\n            Actions sampled by the logging/behavior policy for each data in logged bandit data, i.e., :math:`a_i`.\n\n        action_embed: array-like, shape (n_rounds, dim_action_embed)\n            Context vectors characterizing actions or action embeddings such as item category information.\n            This is used to estimate the marginal importance weights.\n\n        pi_b: array-like, shape (n_rounds, n_actions, len_list)\n            Action choice probabilities of the logging/behavior policy, i.e., :math:`\\\\pi_b(a_i|x_i)`.\n\n        action_dist: array-like, shape (n_rounds, n_actions, len_list)\n            Action choice probabilities of the evaluation policy (can be deterministic), i.e., :math:`\\\\pi_e(a_i|x_i)`.\n\n        position: array-like, shape (n_rounds,), default=None\n            Indices to differentiate positions in a recommendation interface where the actions are presented.\n            If None, the effect of position on the reward will be ignored.\n            (If only a single action is chosen for each data, you can just ignore this argument.)\n\n        p_e_a: array-like, shape (n_actions, n_cat_per_dim, n_cat_dim), default=None\n            Conditional distribution of action embeddings given each action.\n            This distribution is available only when we use synthetic bandit data, i.e.,\n            `obp.dataset.SyntheticBanditDatasetWithActionEmbeds`.\n            See the output of the `obtain_batch_bandit_feedback` argument of this class.\n            If `p_e_a` is given, MIPW uses the true marginal importance weights based on this distribution.\n            The performance of MIPW with the true weights is useful in synthetic experiments of research papers.\n\n        with_dev: bool, default=False.\n            Whether to output a deviation bound with the estimated sample-wise rewards.\n\n        Returns\n        ----------\n        estimated_rewards: array-like, shape (n_rounds,)\n            Estimated rewards for each observation.\n\n        \"\"\"", "\n", "n", "=", "reward", ".", "shape", "[", "0", "]", "\n", "if", "p_e_a", "is", "not", "None", ":", "\n", "            ", "p_e_pi_b", "=", "np", ".", "ones", "(", "n", ")", "\n", "p_e_pi_e", "=", "np", ".", "ones", "(", "n", ")", "\n", "for", "d", "in", "np", ".", "arange", "(", "p_e_a", ".", "shape", "[", "-", "1", "]", ")", ":", "\n", "                ", "p_e_pi_b_d", "=", "pi_b", "[", "np", ".", "arange", "(", "n", ")", ",", ":", ",", "position", "]", "@", "p_e_a", "[", ":", ",", ":", ",", "d", "]", "\n", "p_e_pi_b", "*=", "p_e_pi_b_d", "[", "np", ".", "arange", "(", "n", ")", ",", "action_embed", "[", ":", ",", "d", "]", "]", "\n", "p_e_pi_e_d", "=", "action_dist", "[", "np", ".", "arange", "(", "n", ")", ",", ":", ",", "position", "]", "@", "p_e_a", "[", ":", ",", ":", ",", "d", "]", "\n", "p_e_pi_e", "*=", "p_e_pi_e_d", "[", "np", ".", "arange", "(", "n", ")", ",", "action_embed", "[", ":", ",", "d", "]", "]", "\n", "", "w_x_e", "=", "p_e_pi_e", "/", "p_e_pi_b", "\n", "self", ".", "max_w_x_e", "=", "w_x_e", ".", "max", "(", ")", "\n", "\n", "", "else", ":", "\n", "            ", "w_x_e", "=", "self", ".", "_estimate_w_x_e", "(", "\n", "context", "=", "context", ",", "\n", "action", "=", "action", ",", "\n", "action_embed", "=", "action_embed", ",", "\n", "pi_e", "=", "action_dist", "[", "np", ".", "arange", "(", "n", ")", ",", ":", ",", "position", "]", ",", "\n", "pi_b", "=", "pi_b", "[", "np", ".", "arange", "(", "n", ")", ",", ":", ",", "position", "]", ",", "\n", ")", "\n", "self", ".", "max_w_x_e", "=", "w_x_e", ".", "max", "(", ")", "\n", "\n", "", "if", "with_dev", ":", "\n", "            ", "r_hat", "=", "reward", "*", "w_x_e", "\n", "cnf", "=", "np", ".", "sqrt", "(", "np", ".", "var", "(", "r_hat", ")", "/", "(", "n", "-", "1", ")", ")", "\n", "cnf", "*=", "stats", ".", "t", ".", "ppf", "(", "1.0", "-", "(", "self", ".", "delta", "/", "2", ")", ",", "n", "-", "1", ")", "\n", "\n", "return", "r_hat", ".", "mean", "(", ")", ",", "cnf", "\n", "\n", "", "return", "reward", "*", "w_x_e", "/", "w_x_e", ".", "mean", "(", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.BaseSlateOffPolicyEstimator._estimate_round_rewards": [[25, 29], ["None"], "methods", ["None"], ["@", "abstractmethod", "\n", "def", "_estimate_round_rewards", "(", "self", ")", "->", "np", ".", "ndarray", ":", "\n", "        ", "\"\"\"Estimate round-wise (or sample-wise) rewards.\"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.BaseSlateOffPolicyEstimator.estimate_policy_value": [[30, 34], ["None"], "methods", ["None"], ["", "@", "abstractmethod", "\n", "def", "estimate_policy_value", "(", "self", ")", "->", "float", ":", "\n", "        ", "\"\"\"Estimate the policy value of evaluation policy.\"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.BaseSlateOffPolicyEstimator.estimate_interval": [[35, 39], ["None"], "methods", ["None"], ["", "@", "abstractmethod", "\n", "def", "estimate_interval", "(", "self", ")", "->", "Dict", "[", "str", ",", "float", "]", ":", "\n", "        ", "\"\"\"Estimate the confidence interval of the policy value using bootstrap.\"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.BaseSlateInverseProbabilityWeighting._estimate_round_rewards": [[53, 88], ["None"], "methods", ["None"], ["def", "_estimate_round_rewards", "(", "\n", "self", ",", "\n", "reward", ":", "np", ".", "ndarray", ",", "\n", "position", ":", "np", ".", "ndarray", ",", "\n", "behavior_policy_pscore", ":", "np", ".", "ndarray", ",", "\n", "evaluation_policy_pscore", ":", "np", ".", "ndarray", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "np", ".", "ndarray", ":", "\n", "        ", "\"\"\"Estimate rewards for each slate and slot (position).\n\n        Parameters\n        ----------\n        reward: array-like, shape (<= n_rounds * len_list,)\n            Slot-level rewards, i.e., :math:`r_{i}(l)`.\n\n        position: array-like, shape (<= n_rounds * len_list,)\n            Indices to differentiate slots/positions in a slate/ranking.\n\n        behavior_policy_pscore: array-like, shape (<= n_rounds * len_list,)\n            Marginal probabilities of behavior policy choosing a particular action at each position (slot) or\n            joint probabilities of behavior policy choosing a whole slate/ranking of actions.\n\n        evaluation_policy_pscore: array-like, shape (<= n_rounds * len_list,)\n            Marginal probabilities of evaluation policy choosing a particular action at each slot/position or\n            joint probabilities of evaluation policy choosing a whole slate/ranking of actions.\n\n        Returns\n        ----------\n        estimated_rewards: array-like, shape (<= n_rounds * len_list,)\n            Rewards estimated for each slate and slot (position).\n\n        \"\"\"", "\n", "iw", "=", "evaluation_policy_pscore", "/", "behavior_policy_pscore", "\n", "estimated_rewards", "=", "reward", "*", "iw", "\n", "return", "estimated_rewards", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.BaseSlateInverseProbabilityWeighting._estimate_slate_confidence_interval_by_bootstrap": [[89, 133], ["numpy.unique", "list", "numpy.array", "utils.estimate_confidence_interval_by_bootstrap", "numpy.array.append", "estimated_rewards[].sum"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.estimate_confidence_interval_by_bootstrap"], ["", "def", "_estimate_slate_confidence_interval_by_bootstrap", "(", "\n", "self", ",", "\n", "slate_id", ":", "np", ".", "ndarray", ",", "\n", "estimated_rewards", ":", "np", ".", "ndarray", ",", "\n", "alpha", ":", "float", "=", "0.05", ",", "\n", "n_bootstrap_samples", ":", "int", "=", "10000", ",", "\n", "random_state", ":", "Optional", "[", "int", "]", "=", "None", ",", "\n", ")", "->", "Dict", "[", "str", ",", "float", "]", ":", "\n", "        ", "\"\"\"Estimate the confidence interval of the policy value using bootstrap.\n\n        Parameters\n        ----------\n        slate_id: array-like, shape (<= n_rounds * len_list,)\n            Indices to differentiate slates (i.e., ranking or list of actions)\n\n        estimated_rewards: array-like, shape (<= n_rounds * len_list,)\n            Rewards estimated for each slate and slot (position).\n\n        alpha: float, default=0.05\n            Significance level.\n\n        n_bootstrap_samples: int, default=10000\n            Number of resampling performed in bootstrap sampling.\n\n        random_state: int, default=None\n            Controls the random seed in bootstrap sampling.\n\n        Returns\n        ----------\n        estimated_confidence_interval: Dict[str, float]\n            Dictionary storing the estimated mean and upper-lower confidence bounds.\n\n        \"\"\"", "\n", "unique_slate", "=", "np", ".", "unique", "(", "slate_id", ")", "\n", "# sum estimated_rewards in each slate", "\n", "estimated_round_rewards", "=", "list", "(", ")", "\n", "for", "slate", "in", "unique_slate", ":", "\n", "            ", "estimated_round_rewards", ".", "append", "(", "estimated_rewards", "[", "slate_id", "==", "slate", "]", ".", "sum", "(", ")", ")", "\n", "", "estimated_round_rewards", "=", "np", ".", "array", "(", "estimated_round_rewards", ")", "\n", "return", "estimate_confidence_interval_by_bootstrap", "(", "\n", "samples", "=", "estimated_round_rewards", ",", "\n", "alpha", "=", "alpha", ",", "\n", "n_bootstrap_samples", "=", "n_bootstrap_samples", ",", "\n", "random_state", "=", "random_state", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateStandardIPS.estimate_policy_value": [[160, 211], ["utils.check_sips_inputs", "estimators_slate.SlateStandardIPS._estimate_round_rewards().sum", "estimators_slate.SlateStandardIPS._estimate_round_rewards", "numpy.unique"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_sips_inputs", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SelfNormalizedSlateStandardIPS._estimate_round_rewards"], ["def", "estimate_policy_value", "(", "\n", "self", ",", "\n", "slate_id", ":", "np", ".", "ndarray", ",", "\n", "reward", ":", "np", ".", "ndarray", ",", "\n", "position", ":", "np", ".", "ndarray", ",", "\n", "pscore", ":", "np", ".", "ndarray", ",", "\n", "evaluation_policy_pscore", ":", "np", ".", "ndarray", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "float", ":", "\n", "        ", "\"\"\"Estimate the policy value of evaluation policy.\n\n        Parameters\n        ----------\n        slate_id: array-like, shape (<= n_rounds * len_list,)\n            Indices to differentiate slates (i.e., ranking or list of actions)\n\n        reward: array-like, shape (<= n_rounds * len_list,)\n            Slot-level rewards, i.e., :math:`r_{i}(l)`.\n\n        position: array-like, shape (<= n_rounds * len_list,)\n            Indices to differentiate slots/positions in a slate/ranking.\n\n        pscore: array-like, shape (<= n_rounds * len_list,)\n            Joint probabilities of behavior policy choosing a slate action, i.e., :math:`\\\\pi_b(a_i|x_i)`.\n            This parameter must be unique in each slate.\n\n        evaluation_policy_pscore: array-like, shape (<= n_rounds * len_list,)\n            Joint probabilities of evaluation policy choosing a slate action, i.e., :math:`\\\\pi_e(a_i|x_i)`.\n            This parameter must be unique in each slate.\n\n        Returns\n        ----------\n        V_hat: float\n            Estimated policy value of evaluation policy.\n\n        \"\"\"", "\n", "check_sips_inputs", "(", "\n", "slate_id", "=", "slate_id", ",", "\n", "reward", "=", "reward", ",", "\n", "position", "=", "position", ",", "\n", "pscore", "=", "pscore", ",", "\n", "evaluation_policy_pscore", "=", "evaluation_policy_pscore", ",", "\n", ")", "\n", "return", "(", "\n", "self", ".", "_estimate_round_rewards", "(", "\n", "reward", "=", "reward", ",", "\n", "position", "=", "position", ",", "\n", "behavior_policy_pscore", "=", "pscore", ",", "\n", "evaluation_policy_pscore", "=", "evaluation_policy_pscore", ",", "\n", ")", ".", "sum", "(", ")", "\n", "/", "np", ".", "unique", "(", "slate_id", ")", ".", "shape", "[", "0", "]", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateStandardIPS.estimate_interval": [[213, 280], ["utils.check_sips_inputs", "estimators_slate.SlateStandardIPS._estimate_round_rewards", "estimators_slate.SlateStandardIPS._estimate_slate_confidence_interval_by_bootstrap"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_sips_inputs", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SelfNormalizedSlateStandardIPS._estimate_round_rewards", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust._estimate_slate_confidence_interval_by_bootstrap"], ["", "def", "estimate_interval", "(", "\n", "self", ",", "\n", "slate_id", ":", "np", ".", "ndarray", ",", "\n", "reward", ":", "np", ".", "ndarray", ",", "\n", "position", ":", "np", ".", "ndarray", ",", "\n", "pscore", ":", "np", ".", "ndarray", ",", "\n", "evaluation_policy_pscore", ":", "np", ".", "ndarray", ",", "\n", "alpha", ":", "float", "=", "0.05", ",", "\n", "n_bootstrap_samples", ":", "int", "=", "10000", ",", "\n", "random_state", ":", "Optional", "[", "int", "]", "=", "None", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "Dict", "[", "str", ",", "float", "]", ":", "\n", "        ", "\"\"\"Estimate the confidence interval of the policy value using bootstrap.\n\n        Parameters\n        ----------\n        slate_id: array-like, shape (<= n_rounds * len_list,)\n            Indices to differentiate slates (i.e., ranking or list of actions)\n\n        reward: array-like, shape (<= n_rounds * len_list,)\n            Slot-level rewards, i.e., :math:`r_{i}(l)`.\n\n        position: array-like, shape (<= n_rounds * len_list,)\n            Indices to differentiate slots/positions in a slate/ranking.\n\n        pscore: array-like, shape (<= n_rounds * len_list,)\n            Joint probabilities of behavior policy choosing a slate action, i.e., :math:`\\\\pi_b(a_i|x_i)`.\n            This parameter must be unique in each slate.\n\n        evaluation_policy_pscore: array-like, shape (<= n_rounds * len_list,)\n            Joint probabilities of evaluation policy choosing a slate action, i.e., :math:`\\\\pi_e(a_i|x_i)`.\n            This parameter must be unique in each slate.\n\n        alpha: float, default=0.05\n            Significance level.\n\n        n_bootstrap_samples: int, default=10000\n            Number of resampling performed in bootstrap sampling.\n\n        random_state: int, default=None\n            Controls the random seed in bootstrap sampling.\n\n        Returns\n        ----------\n        estimated_confidence_interval: Dict[str, float]\n            Dictionary storing the estimated mean and upper-lower confidence bounds.\n\n        \"\"\"", "\n", "check_sips_inputs", "(", "\n", "slate_id", "=", "slate_id", ",", "\n", "reward", "=", "reward", ",", "\n", "position", "=", "position", ",", "\n", "pscore", "=", "pscore", ",", "\n", "evaluation_policy_pscore", "=", "evaluation_policy_pscore", ",", "\n", ")", "\n", "estimated_rewards", "=", "self", ".", "_estimate_round_rewards", "(", "\n", "reward", "=", "reward", ",", "\n", "position", "=", "position", ",", "\n", "behavior_policy_pscore", "=", "pscore", ",", "\n", "evaluation_policy_pscore", "=", "evaluation_policy_pscore", ",", "\n", ")", "\n", "return", "self", ".", "_estimate_slate_confidence_interval_by_bootstrap", "(", "\n", "slate_id", "=", "slate_id", ",", "\n", "estimated_rewards", "=", "estimated_rewards", ",", "\n", "alpha", "=", "alpha", ",", "\n", "n_bootstrap_samples", "=", "n_bootstrap_samples", ",", "\n", "random_state", "=", "random_state", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateIndependentIPS.estimate_policy_value": [[310, 361], ["utils.check_iips_inputs", "estimators_slate.SlateIndependentIPS._estimate_round_rewards().sum", "estimators_slate.SlateIndependentIPS._estimate_round_rewards", "numpy.unique"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_iips_inputs", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SelfNormalizedSlateStandardIPS._estimate_round_rewards"], ["def", "estimate_policy_value", "(", "\n", "self", ",", "\n", "slate_id", ":", "np", ".", "ndarray", ",", "\n", "reward", ":", "np", ".", "ndarray", ",", "\n", "position", ":", "np", ".", "ndarray", ",", "\n", "pscore_item_position", ":", "np", ".", "ndarray", ",", "\n", "evaluation_policy_pscore_item_position", ":", "np", ".", "ndarray", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "float", ":", "\n", "        ", "\"\"\"Estimate the policy value of evaluation policy.\n\n        Parameters\n        ----------\n        slate_id: array-like, shape (<= n_rounds * len_list,)\n            Indices to differentiate slates (i.e., ranking or list of actions)\n\n        reward: array-like, shape (<= n_rounds * len_list,)\n            Slot-level rewards, i.e., :math:`r_{i}(l)`.\n\n        position: array-like, shape (<= n_rounds * len_list,)\n            Indices to differentiate slots/positions in a slate/ranking.\n\n        pscore_item_position: array-like, shape (<= n_rounds * len_list,)\n            Marginal probabilities of behavior policy choosing a particular action at each position (slot),\n            i.e., :math:`\\\\pi_b(a_{i}(l) |x_i)`.\n\n        evaluation_policy_pscore_item_position: array-like, shape (<= n_rounds * len_list,)\n            Marginal probabilities of evaluation policy choosing a particular action at each position (slot),\n             i.e., :math:`\\\\pi_e(a_{i}(l) |x_i)`.\n\n        Returns\n        ----------\n        V_hat: float\n            Estimated policy value of evaluation policy.\n\n        \"\"\"", "\n", "check_iips_inputs", "(", "\n", "slate_id", "=", "slate_id", ",", "\n", "reward", "=", "reward", ",", "\n", "position", "=", "position", ",", "\n", "pscore_item_position", "=", "pscore_item_position", ",", "\n", "evaluation_policy_pscore_item_position", "=", "evaluation_policy_pscore_item_position", ",", "\n", ")", "\n", "return", "(", "\n", "self", ".", "_estimate_round_rewards", "(", "\n", "reward", "=", "reward", ",", "\n", "position", "=", "position", ",", "\n", "behavior_policy_pscore", "=", "pscore_item_position", ",", "\n", "evaluation_policy_pscore", "=", "evaluation_policy_pscore_item_position", ",", "\n", ")", ".", "sum", "(", ")", "\n", "/", "np", ".", "unique", "(", "slate_id", ")", ".", "shape", "[", "0", "]", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateIndependentIPS.estimate_interval": [[363, 430], ["utils.check_iips_inputs", "estimators_slate.SlateIndependentIPS._estimate_round_rewards", "estimators_slate.SlateIndependentIPS._estimate_slate_confidence_interval_by_bootstrap"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_iips_inputs", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SelfNormalizedSlateStandardIPS._estimate_round_rewards", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust._estimate_slate_confidence_interval_by_bootstrap"], ["", "def", "estimate_interval", "(", "\n", "self", ",", "\n", "slate_id", ":", "np", ".", "ndarray", ",", "\n", "reward", ":", "np", ".", "ndarray", ",", "\n", "position", ":", "np", ".", "ndarray", ",", "\n", "pscore_item_position", ":", "np", ".", "ndarray", ",", "\n", "evaluation_policy_pscore_item_position", ":", "np", ".", "ndarray", ",", "\n", "alpha", ":", "float", "=", "0.05", ",", "\n", "n_bootstrap_samples", ":", "int", "=", "10000", ",", "\n", "random_state", ":", "Optional", "[", "int", "]", "=", "None", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "Dict", "[", "str", ",", "float", "]", ":", "\n", "        ", "\"\"\"Estimate the confidence interval of the policy value using bootstrap.\n\n        Parameters\n        ----------\n        slate_id: array-like, shape (<= n_rounds * len_list,)\n            Indices to differentiate slates (i.e., ranking or list of actions)\n\n        reward: array-like, shape (<= n_rounds * len_list,)\n            Slot-level rewards, i.e., :math:`r_{i}(l)`.\n\n        position: array-like, shape (<= n_rounds * len_list,)\n            Indices to differentiate slots/positions in a slate/ranking.\n\n        pscore_item_position: array-like, shape (<= n_rounds * len_list,)\n            Marginal probabilities of behavior policy choosing a particular action at each position (slot),\n            i.e., :math:`\\\\pi_b(a_{i}(l) |x_i)`.\n\n        evaluation_policy_pscore_item_position: array-like, shape (<= n_rounds * len_list,)\n            Marginal probabilities of evaluation policy choosing a particular action at each position (slot),\n             i.e., :math:`\\\\pi_e(a_{i}(l) |x_i)`.\n\n        alpha: float, default=0.05\n            Significance level.\n\n        n_bootstrap_samples: int, default=10000\n            Number of resampling performed in bootstrap sampling.\n\n        random_state: int, default=None\n            Controls the random seed in bootstrap sampling.\n\n        Returns\n        ----------\n        estimated_confidence_interval: Dict[str, float]\n            Dictionary storing the estimated mean and upper-lower confidence bounds.\n\n        \"\"\"", "\n", "check_iips_inputs", "(", "\n", "slate_id", "=", "slate_id", ",", "\n", "reward", "=", "reward", ",", "\n", "position", "=", "position", ",", "\n", "pscore_item_position", "=", "pscore_item_position", ",", "\n", "evaluation_policy_pscore_item_position", "=", "evaluation_policy_pscore_item_position", ",", "\n", ")", "\n", "estimated_rewards", "=", "self", ".", "_estimate_round_rewards", "(", "\n", "reward", "=", "reward", ",", "\n", "position", "=", "position", ",", "\n", "behavior_policy_pscore", "=", "pscore_item_position", ",", "\n", "evaluation_policy_pscore", "=", "evaluation_policy_pscore_item_position", ",", "\n", ")", "\n", "return", "self", ".", "_estimate_slate_confidence_interval_by_bootstrap", "(", "\n", "slate_id", "=", "slate_id", ",", "\n", "estimated_rewards", "=", "estimated_rewards", ",", "\n", "alpha", "=", "alpha", ",", "\n", "n_bootstrap_samples", "=", "n_bootstrap_samples", ",", "\n", "random_state", "=", "random_state", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateRewardInteractionIPS.estimate_policy_value": [[457, 507], ["utils.check_rips_inputs", "estimators_slate.SlateRewardInteractionIPS._estimate_round_rewards().sum", "estimators_slate.SlateRewardInteractionIPS._estimate_round_rewards", "numpy.unique"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_rips_inputs", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SelfNormalizedSlateStandardIPS._estimate_round_rewards"], ["def", "estimate_policy_value", "(", "\n", "self", ",", "\n", "slate_id", ":", "np", ".", "ndarray", ",", "\n", "reward", ":", "np", ".", "ndarray", ",", "\n", "position", ":", "np", ".", "ndarray", ",", "\n", "pscore_cascade", ":", "np", ".", "ndarray", ",", "\n", "evaluation_policy_pscore_cascade", ":", "np", ".", "ndarray", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "float", ":", "\n", "        ", "\"\"\"Estimate the policy value of evaluation policy.\n\n        Parameters\n        ----------\n        slate_id: array-like, shape (<= n_rounds * len_list,)\n            Indices to differentiate slates (i.e., ranking or list of actions)\n\n        reward: array-like, shape (<= n_rounds * len_list,)\n            Slot-level rewards, i.e., :math:`r_{i}(l)`.\n\n        position: array-like, shape (<= n_rounds * len_list,)\n            Indices to differentiate slots/positions in a slate/ranking.\n\n        pscore_cascade: array-like, shape (<= n_rounds * len_list,)\n            Joint probabilities of behavior policy choosing a particular sequence of actions from the top position to the :math:`l`-th position (:math:`a_{1:l}`). This type of action choice probabilities corresponds to the cascade model.\n\n        evaluation_policy_pscore_cascade: array-like, shape (<= n_rounds * len_list,)\n            Joint probabilities of behavior policy choosing a particular sequence of actions from the top position to the :math:`l`-th position (:math:`a_{1:l}`). This type of action choice probabilities corresponds to the cascade model.\n\n        Returns\n        ----------\n        V_hat: float\n            Estimated policy value of evaluation policy.\n\n        \"\"\"", "\n", "\n", "check_rips_inputs", "(", "\n", "slate_id", "=", "slate_id", ",", "\n", "reward", "=", "reward", ",", "\n", "position", "=", "position", ",", "\n", "pscore_cascade", "=", "pscore_cascade", ",", "\n", "evaluation_policy_pscore_cascade", "=", "evaluation_policy_pscore_cascade", ",", "\n", ")", "\n", "return", "(", "\n", "self", ".", "_estimate_round_rewards", "(", "\n", "reward", "=", "reward", ",", "\n", "position", "=", "position", ",", "\n", "behavior_policy_pscore", "=", "pscore_cascade", ",", "\n", "evaluation_policy_pscore", "=", "evaluation_policy_pscore_cascade", ",", "\n", ")", ".", "sum", "(", ")", "\n", "/", "np", ".", "unique", "(", "slate_id", ")", ".", "shape", "[", "0", "]", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateRewardInteractionIPS.estimate_interval": [[509, 574], ["utils.check_rips_inputs", "estimators_slate.SlateRewardInteractionIPS._estimate_round_rewards", "estimators_slate.SlateRewardInteractionIPS._estimate_slate_confidence_interval_by_bootstrap"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_rips_inputs", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SelfNormalizedSlateStandardIPS._estimate_round_rewards", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust._estimate_slate_confidence_interval_by_bootstrap"], ["", "def", "estimate_interval", "(", "\n", "self", ",", "\n", "slate_id", ":", "np", ".", "ndarray", ",", "\n", "reward", ":", "np", ".", "ndarray", ",", "\n", "position", ":", "np", ".", "ndarray", ",", "\n", "pscore_cascade", ":", "np", ".", "ndarray", ",", "\n", "evaluation_policy_pscore_cascade", ":", "np", ".", "ndarray", ",", "\n", "alpha", ":", "float", "=", "0.05", ",", "\n", "n_bootstrap_samples", ":", "int", "=", "10000", ",", "\n", "random_state", ":", "Optional", "[", "int", "]", "=", "None", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "Dict", "[", "str", ",", "float", "]", ":", "\n", "        ", "\"\"\"Estimate the confidence interval of the policy value using bootstrap.\n\n        Parameters\n        ----------\n        slate_id: array-like, shape (<= n_rounds * len_list,)\n            Indices to differentiate slates (i.e., ranking or list of actions)\n\n        reward: array-like, shape (<= n_rounds * len_list,)\n            Slot-level rewards, i.e., :math:`r_{i}(l)`.\n\n        position: array-like, shape (<= n_rounds * len_list,)\n            Indices to differentiate slots/positions in a slate/ranking.\n\n        pscore_cascade: array-like, shape (<= n_rounds * len_list,)\n            Joint probabilities of behavior policy choosing a particular sequence of actions from the top position to the :math:`l`-th position (:math:`a_{1:l}`). This type of action choice probabilities corresponds to the cascade model.\n\n        evaluation_policy_pscore_cascade: array-like, shape (<= n_rounds * len_list,)\n            Joint probabilities of behavior policy choosing a particular sequence of actions from the top position to the :math:`l`-th position (:math:`a_{1:l}`). This type of action choice probabilities corresponds to the cascade model.\n\n        alpha: float, default=0.05\n            Significance level.\n\n        n_bootstrap_samples: int, default=10000\n            Number of resampling performed in bootstrap sampling.\n\n        random_state: int, default=None\n            Controls the random seed in bootstrap sampling.\n\n        Returns\n        ----------\n        estimated_confidence_interval: Dict[str, float]\n            Dictionary storing the estimated mean and upper-lower confidence bounds.\n\n        \"\"\"", "\n", "check_rips_inputs", "(", "\n", "slate_id", "=", "slate_id", ",", "\n", "reward", "=", "reward", ",", "\n", "position", "=", "position", ",", "\n", "pscore_cascade", "=", "pscore_cascade", ",", "\n", "evaluation_policy_pscore_cascade", "=", "evaluation_policy_pscore_cascade", ",", "\n", ")", "\n", "estimated_rewards", "=", "self", ".", "_estimate_round_rewards", "(", "\n", "reward", "=", "reward", ",", "\n", "position", "=", "position", ",", "\n", "behavior_policy_pscore", "=", "pscore_cascade", ",", "\n", "evaluation_policy_pscore", "=", "evaluation_policy_pscore_cascade", ",", "\n", ")", "\n", "return", "self", ".", "_estimate_slate_confidence_interval_by_bootstrap", "(", "\n", "slate_id", "=", "slate_id", ",", "\n", "estimated_rewards", "=", "estimated_rewards", ",", "\n", "alpha", "=", "alpha", ",", "\n", "n_bootstrap_samples", "=", "n_bootstrap_samples", ",", "\n", "random_state", "=", "random_state", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.__post_init__": [[611, 614], ["sklearn.utils.check_scalar"], "methods", ["None"], ["def", "__post_init__", "(", "self", ")", ":", "\n", "        ", "\"\"\"Initialize Class.\"\"\"", "\n", "check_scalar", "(", "self", ".", "n_unique_action", ",", "\"n_unique_action\"", ",", "int", ",", "min_val", "=", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust._estimate_round_rewards": [[615, 691], ["q_hat.reshape", "range", "numpy.array", "numpy.roll", "range", "numpy.array.append", "numpy.array", "range"], "methods", ["None"], ["", "def", "_estimate_round_rewards", "(", "\n", "self", ",", "\n", "action", ":", "np", ".", "ndarray", ",", "\n", "reward", ":", "np", ".", "ndarray", ",", "\n", "position", ":", "np", ".", "ndarray", ",", "\n", "behavior_policy_pscore", ":", "np", ".", "ndarray", ",", "\n", "evaluation_policy_pscore", ":", "np", ".", "ndarray", ",", "\n", "q_hat", ":", "np", ".", "ndarray", ",", "\n", "evaluation_policy_action_dist", ":", "np", ".", "ndarray", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "np", ".", "ndarray", ":", "\n", "        ", "\"\"\"Estimate rewards for each slate and slot (position).\n\n        Parameters\n        ----------\n        action: array-like, (n_rounds * len_list,)\n            Actions observed at each slot in a ranking/slate in logged bandit data, i.e., :math:`a_{i}(l)`,\n            which is chosen by the behavior policy :math:`\\\\pi_b`.\n\n        reward: array-like, shape (n_rounds * len_list,)\n            Slot-level rewards observed for each data in logged bandit data, i.e., :math:`r_{i}(l)`.\n\n        position: array-like, shape (n_rounds * len_list,)\n            Indices to differentiate slots/positions in a slate/ranking.\n\n        pscore_cascade: array-like, shape (n_rounds * len_list,)\n            Joint probabilities of behavior policy choosing a particular sequence of actions from the top position to the :math:`l`-th position (:math:`a_{1:l}`). This type of action choice probabilities corresponds to the cascade model.\n\n        evaluation_policy_pscore_cascade: array-like, shape (n_rounds * len_list,)\n            Joint probabilities of evaluation policy choosing a particular sequence of actions from the top position to the :math:`l`-th position (:math:`a_{1:l}`). This type of action choice probabilities corresponds to the cascade model.\n\n        q_hat: array-like (n_rounds * len_list * n_unique_actions, )\n            :math:`\\\\hat{Q}_l` for all unique actions,\n            i.e., :math:`\\\\hat{Q}_{i,l}(x_i, a_i(1), \\\\ldots, a_i(l-1), a_i(l)) \\\\forall a_i(l) \\\\in \\\\mathcal{A}`.\n\n        evaluation_policy_action_dist: array-like (n_rounds * len_list * n_unique_actions, )\n            Plackett-luce style action distribution induced by evaluation policy\n            (action choice probabilities at each slot given previous action choices)\n            , i.e., :math:`\\\\pi_e(a_i(l) | x_i, a_i(1), \\\\ldots, a_i(l-1)) \\\\forall a_i(l) \\\\in \\\\mathcal{A}`.\n\n        Returns\n        ----------\n        estimated_rewards: array-like, shape (n_rounds * len_list,)\n            Rewards rewards for each slate and slot (position).\n\n        \"\"\"", "\n", "# (n_rounds_ * len_list * n_unique_action, ) -> (n_rounds_, len_list, n_unique_action)", "\n", "q_hat_3d", "=", "q_hat", ".", "reshape", "(", "(", "-", "1", ",", "self", ".", "len_list", ",", "self", ".", "n_unique_action", ")", ")", "\n", "# the estimated Q functions for the action taken by the behavior policy", "\n", "# (n_rounds_, len_list, n_unique_action) -> (n_rounds_ * len_list, )", "\n", "q_hat_for_observed_action", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "self", ".", "n_rounds_", ")", ":", "\n", "            ", "for", "pos_", "in", "range", "(", "self", ".", "len_list", ")", ":", "\n", "                ", "q_hat_for_observed_action", ".", "append", "(", "\n", "q_hat_3d", "[", "i", ",", "pos_", ",", "action", "[", "i", "*", "self", ".", "len_list", "+", "pos_", "]", "]", "\n", ")", "\n", "", "", "q_hat_for_observed_action", "=", "np", ".", "array", "(", "q_hat_for_observed_action", ")", "\n", "# the expected Q function under the evaluation policy", "\n", "# (n_rounds_ * len_list * n_unique_action, ) -> (n_rounds_, len_list, n_unique_action) -> (n_rounds_, len_list) -> (n_rounds_ * len_list, )", "\n", "expected_q_hat_under_eval_policy", "=", "(", "\n", "(", "evaluation_policy_action_dist", "*", "q_hat", ")", "\n", ".", "reshape", "(", "(", "-", "1", ",", "self", ".", "len_list", ",", "self", ".", "n_unique_action", ")", ")", "\n", ".", "sum", "(", "axis", "=", "2", ")", "\n", ".", "flatten", "(", ")", "\n", ")", "\n", "# importance weights", "\n", "# (n_rounds * len_list, )", "\n", "iw", "=", "evaluation_policy_pscore", "/", "behavior_policy_pscore", "\n", "iw_prev", "=", "np", ".", "roll", "(", "iw", ",", "1", ")", "\n", "iw_prev", "[", "np", ".", "array", "(", "[", "i", "*", "self", ".", "len_list", "for", "i", "in", "range", "(", "self", ".", "n_rounds_", ")", "]", ")", "]", "=", "1", "\n", "# estimate policy value given each round and slot in a doubly robust manner", "\n", "estimated_rewards", "=", "(", "\n", "iw", "*", "(", "reward", "-", "q_hat_for_observed_action", ")", "\n", "+", "iw_prev", "*", "expected_q_hat_under_eval_policy", "\n", ")", "\n", "return", "estimated_rewards", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_policy_value": [[692, 766], ["utils.check_cascade_dr_inputs", "estimators_slate.SlateCascadeDoublyRobust._estimate_round_rewards().sum", "numpy.unique", "estimators_slate.SlateCascadeDoublyRobust._estimate_round_rewards"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_cascade_dr_inputs", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SelfNormalizedSlateStandardIPS._estimate_round_rewards"], ["", "def", "estimate_policy_value", "(", "\n", "self", ",", "\n", "slate_id", ":", "np", ".", "ndarray", ",", "\n", "action", ":", "np", ".", "ndarray", ",", "\n", "reward", ":", "np", ".", "ndarray", ",", "\n", "position", ":", "np", ".", "ndarray", ",", "\n", "pscore_cascade", ":", "np", ".", "ndarray", ",", "\n", "evaluation_policy_pscore_cascade", ":", "np", ".", "ndarray", ",", "\n", "q_hat", ":", "np", ".", "ndarray", ",", "\n", "evaluation_policy_action_dist", ":", "np", ".", "ndarray", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "float", ":", "\n", "        ", "\"\"\"Estimate the policy value of evaluation policy.\n\n        Parameters\n        ----------\n        slate_id: array-like, shape (n_rounds * len_list,)\n            Indices to differentiate slates (i.e., ranking or list of actions)\n\n        action: array-like, (n_rounds * len_list,)\n            Actions observed at each slot in a ranking/slate in logged bandit data, i.e., :math:`a_{i}(l)`,\n            which is chosen by the behavior policy :math:`\\\\pi_b`.\n\n        reward: array-like, shape (n_rounds * len_list,)\n            Slot-level rewards observed for each data in logged bandit data, i.e., :math:`r_{i}(l)`.\n\n        position: array-like, shape (n_rounds * len_list,)\n            Indices to differentiate slots/positions in a slate/ranking.\n\n        pscore_cascade: array-like, shape (n_rounds * len_list,)\n            Joint probabilities of behavior policy choosing a particular sequence of actions from the top position to the :math:`l`-th position (:math:`a_{1:l}`).\n\n        evaluation_policy_pscore_cascade: array-like, shape (n_rounds * len_list,)\n            Joint probabilities of evaluation policy choosing a particular sequence of actions from the top position to the :math:`l`-th position (:math:`a_{1:l}`). This type of action choice probabilities corresponds to the cascade model.\n\n\n        q_hat: array-like (n_rounds * len_list * n_unique_actions, )\n            :math:`\\\\hat{Q}_l` for all unique actions,\n            i.e., :math:`\\\\hat{Q}_{i,l}(x_i, a_i(1), \\\\ldots, a_i(l-1), a_i(l)) \\\\forall a_i(l) \\\\in \\\\mathcal{A}`.\n\n        evaluation_policy_action_dist: array-like (n_rounds * len_list * n_unique_actions, )\n            Plackett-luce style action distribution induced by evaluation policy\n            (action choice probabilities at each slot given previous action choices)\n            , i.e., :math:`\\\\pi_e(a_i(l) | x_i, a_i(1), \\\\ldots, a_i(l-1)) \\\\forall a_i(l) \\\\in \\\\mathcal{A}`.\n\n        Returns\n        ----------\n        V_hat: array-like, shape (n_rounds * len_list,)\n            Estimated policy value of evaluation policy.\n\n        \"\"\"", "\n", "check_cascade_dr_inputs", "(", "\n", "n_unique_action", "=", "self", ".", "n_unique_action", ",", "\n", "slate_id", "=", "slate_id", ",", "\n", "action", "=", "action", ",", "\n", "reward", "=", "reward", ",", "\n", "position", "=", "position", ",", "\n", "pscore_cascade", "=", "pscore_cascade", ",", "\n", "evaluation_policy_pscore_cascade", "=", "evaluation_policy_pscore_cascade", ",", "\n", "q_hat", "=", "q_hat", ",", "\n", "evaluation_policy_action_dist", "=", "evaluation_policy_action_dist", ",", "\n", ")", "\n", "self", ".", "n_rounds_", "=", "np", ".", "unique", "(", "slate_id", ")", ".", "shape", "[", "0", "]", "\n", "return", "(", "\n", "self", ".", "_estimate_round_rewards", "(", "\n", "action", "=", "action", ",", "\n", "reward", "=", "reward", ",", "\n", "position", "=", "position", ",", "\n", "behavior_policy_pscore", "=", "pscore_cascade", ",", "\n", "evaluation_policy_pscore", "=", "evaluation_policy_pscore_cascade", ",", "\n", "q_hat", "=", "q_hat", ",", "\n", "evaluation_policy_action_dist", "=", "evaluation_policy_action_dist", ",", "\n", ")", ".", "sum", "(", ")", "\n", "/", "self", ".", "n_rounds_", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_interval": [[768, 858], ["utils.check_cascade_dr_inputs", "estimators_slate.SlateCascadeDoublyRobust._estimate_round_rewards", "estimators_slate.SlateCascadeDoublyRobust._estimate_slate_confidence_interval_by_bootstrap", "numpy.unique"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_cascade_dr_inputs", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SelfNormalizedSlateStandardIPS._estimate_round_rewards", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust._estimate_slate_confidence_interval_by_bootstrap"], ["", "def", "estimate_interval", "(", "\n", "self", ",", "\n", "slate_id", ":", "np", ".", "ndarray", ",", "\n", "action", ":", "np", ".", "ndarray", ",", "\n", "reward", ":", "np", ".", "ndarray", ",", "\n", "position", ":", "np", ".", "ndarray", ",", "\n", "pscore_cascade", ":", "np", ".", "ndarray", ",", "\n", "evaluation_policy_pscore_cascade", ":", "np", ".", "ndarray", ",", "\n", "q_hat", ":", "np", ".", "ndarray", ",", "\n", "evaluation_policy_action_dist", ":", "np", ".", "ndarray", ",", "\n", "alpha", ":", "float", "=", "0.05", ",", "\n", "n_bootstrap_samples", ":", "int", "=", "10000", ",", "\n", "random_state", ":", "Optional", "[", "int", "]", "=", "None", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "Dict", "[", "str", ",", "float", "]", ":", "\n", "        ", "\"\"\"Estimate the confidence interval of the policy value using bootstrap.\n\n        Parameters\n        ----------\n        slate_id: array-like, shape (n_rounds * len_list,)\n            Indices to differentiate slates (i.e., ranking or list of actions)\n\n        action: array-like, (n_rounds * len_list,)\n            Actions observed at each slot in a ranking/slate in logged bandit data, i.e., :math:`a_{i}(l)`,\n            which is chosen by the behavior policy :math:`\\\\pi_b`.\n\n        reward: array-like, shape (n_rounds * len_list,)\n            Slot-level rewards observed for each data in logged bandit data, i.e., :math:`r_{i}(l)`.\n\n        position: array-like, shape (n_rounds * len_list,)\n            Indices to differentiate slots/positions in a slate/ranking.\n\n        pscore_cascade: array-like, shape (n_rounds * len_list,)\n            Joint probabilities of behavior policy choosing a particular sequence of actions from the top position to the :math:`l`-th position (:math:`a_{1:l}`).\n\n        evaluation_policy_pscore_cascade: array-like, shape (n_rounds * len_list,)\n            Joint probabilities of evaluation policy choosing a particular sequence of actions from the top position to the :math:`l`-th position (:math:`a_{1:l}`). This type of action choice probabilities corresponds to the cascade model.\n\n\n        q_hat: array-like (n_rounds * len_list * n_unique_actions, )\n            :math:`\\\\hat{Q}_l` for all unique actions,\n            i.e., :math:`\\\\hat{Q}_{i,l}(x_i, a_i(1), \\\\ldots, a_i(l-1), a_i(l)) \\\\forall a_i(l) \\\\in \\\\mathcal{A}`.\n\n        evaluation_policy_action_dist: array-like (n_rounds * len_list * n_unique_actions, )\n            Plackett-luce style action distribution induced by evaluation policy\n            (action choice probabilities at each slot given previous action choices)\n            , i.e., :math:`\\\\pi_e(a_i(l) | x_i, a_i(1), \\\\ldots, a_i(l-1)) \\\\forall a_i(l) \\\\in \\\\mathcal{A}`.\n\n        alpha: float, default=0.05\n            Significance level.\n\n        n_bootstrap_samples: int, default=10000\n            Number of resampling performed in bootstrap sampling.\n\n        random_state: int, default=None\n            Controls the random seed in bootstrap sampling.\n\n        Returns\n        ----------\n        estimated_confidence_interval: Dict[str, float]\n            Dictionary storing the estimated mean and upper-lower confidence bounds.\n\n        \"\"\"", "\n", "check_cascade_dr_inputs", "(", "\n", "n_unique_action", "=", "self", ".", "n_unique_action", ",", "\n", "slate_id", "=", "slate_id", ",", "\n", "action", "=", "action", ",", "\n", "reward", "=", "reward", ",", "\n", "position", "=", "position", ",", "\n", "pscore_cascade", "=", "pscore_cascade", ",", "\n", "evaluation_policy_pscore_cascade", "=", "evaluation_policy_pscore_cascade", ",", "\n", "q_hat", "=", "q_hat", ",", "\n", "evaluation_policy_action_dist", "=", "evaluation_policy_action_dist", ",", "\n", ")", "\n", "self", ".", "n_rounds_", "=", "np", ".", "unique", "(", "slate_id", ")", ".", "shape", "[", "0", "]", "\n", "estimated_rewards", "=", "self", ".", "_estimate_round_rewards", "(", "\n", "action", "=", "action", ",", "\n", "reward", "=", "reward", ",", "\n", "position", "=", "position", ",", "\n", "behavior_policy_pscore", "=", "pscore_cascade", ",", "\n", "evaluation_policy_pscore", "=", "evaluation_policy_pscore_cascade", ",", "\n", "q_hat", "=", "q_hat", ",", "\n", "evaluation_policy_action_dist", "=", "evaluation_policy_action_dist", ",", "\n", ")", "\n", "return", "self", ".", "_estimate_slate_confidence_interval_by_bootstrap", "(", "\n", "slate_id", "=", "slate_id", ",", "\n", "estimated_rewards", "=", "estimated_rewards", ",", "\n", "alpha", "=", "alpha", ",", "\n", "n_bootstrap_samples", "=", "n_bootstrap_samples", ",", "\n", "random_state", "=", "random_state", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust._estimate_slate_confidence_interval_by_bootstrap": [[860, 904], ["numpy.unique", "list", "numpy.array", "utils.estimate_confidence_interval_by_bootstrap", "numpy.array.append", "estimated_rewards[].sum"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.estimate_confidence_interval_by_bootstrap"], ["", "def", "_estimate_slate_confidence_interval_by_bootstrap", "(", "\n", "self", ",", "\n", "slate_id", ":", "np", ".", "ndarray", ",", "\n", "estimated_rewards", ":", "np", ".", "ndarray", ",", "\n", "alpha", ":", "float", "=", "0.05", ",", "\n", "n_bootstrap_samples", ":", "int", "=", "10000", ",", "\n", "random_state", ":", "Optional", "[", "int", "]", "=", "None", ",", "\n", ")", "->", "Dict", "[", "str", ",", "float", "]", ":", "\n", "        ", "\"\"\"Estimate the confidence interval of the policy value using bootstrap.\n\n        Parameters\n        ----------\n        slate_id: array-like, shape (<= n_rounds * len_list,)\n            Indices to differentiate slates (i.e., ranking or list of actions)\n\n        estimated_rewards: array-like, shape (<= n_rounds * len_list,)\n            Rewards estimated for each slate and slot (position).\n\n        alpha: float, default=0.05\n            Significance level.\n\n        n_bootstrap_samples: int, default=10000\n            Number of resampling performed in bootstrap sampling.\n\n        random_state: int, default=None\n            Controls the random seed in bootstrap sampling.\n\n        Returns\n        ----------\n        estimated_confidence_interval: Dict[str, float]\n            Dictionary storing the estimated mean and upper-lower confidence bounds.\n\n        \"\"\"", "\n", "unique_slate", "=", "np", ".", "unique", "(", "slate_id", ")", "\n", "# sum estimated_rewards in each slate", "\n", "estimated_round_rewards", "=", "list", "(", ")", "\n", "for", "slate", "in", "unique_slate", ":", "\n", "            ", "estimated_round_rewards", ".", "append", "(", "estimated_rewards", "[", "slate_id", "==", "slate", "]", ".", "sum", "(", ")", ")", "\n", "", "estimated_round_rewards", "=", "np", ".", "array", "(", "estimated_round_rewards", ")", "\n", "return", "estimate_confidence_interval_by_bootstrap", "(", "\n", "samples", "=", "estimated_round_rewards", ",", "\n", "alpha", "=", "alpha", ",", "\n", "n_bootstrap_samples", "=", "n_bootstrap_samples", ",", "\n", "random_state", "=", "random_state", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.BaseSlateSelfNormalizedInverseProbabilityWeighting._estimate_round_rewards": [[921, 960], ["numpy.zeros_like", "numpy.zeros_like", "range", "iw[].mean"], "methods", ["None"], ["def", "_estimate_round_rewards", "(", "\n", "self", ",", "\n", "reward", ":", "np", ".", "ndarray", ",", "\n", "position", ":", "np", ".", "ndarray", ",", "\n", "behavior_policy_pscore", ":", "np", ".", "ndarray", ",", "\n", "evaluation_policy_pscore", ":", "np", ".", "ndarray", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "np", ".", "ndarray", ":", "\n", "        ", "\"\"\"Self-Normalized estimated rewards for each slate and slot (position).\n\n        Parameters\n        ----------\n        reward: array-like, shape (<= n_rounds * len_list,)\n            Slot-level rewards, i.e., :math:`r_{i}(l)`.\n\n        position: array-like, shape (<= n_rounds * len_list,)\n            Indices to differentiate slots/positions in a slate/ranking.\n\n        behavior_policy_pscore: array-like, shape (<= n_rounds * len_list,)\n            Marginal probabilities of behavior policy choosing a particular action at each position (slot) or\n            joint probabilities of behavior policy choosing a whole slate/ranking of actions.\n\n        evaluation_policy_pscore: array-like, shape (<= n_rounds * len_list,)\n            Marginal probabilities of evaluation policy choosing a particular action at each slot/position or\n            joint probabilities of evaluation policy choosing a whole slate/ranking of actions.\n\n        Returns\n        ----------\n        estimated_rewards: array-like, shape (<= n_rounds * len_list,)\n            Self-Normalized rewards estimated for each slate and slot (position).\n\n        \"\"\"", "\n", "estimated_rewards", "=", "np", ".", "zeros_like", "(", "behavior_policy_pscore", ")", "\n", "iw", "=", "np", ".", "zeros_like", "(", "behavior_policy_pscore", ")", "\n", "for", "pos_", "in", "range", "(", "self", ".", "len_list", ")", ":", "\n", "            ", "idx", "=", "position", "==", "pos_", "\n", "iw", "[", "idx", "]", "=", "evaluation_policy_pscore", "[", "idx", "]", "/", "behavior_policy_pscore", "[", "idx", "]", "\n", "estimated_rewards", "[", "idx", "]", "=", "reward", "[", "idx", "]", "*", "iw", "[", "idx", "]", "/", "iw", "[", "idx", "]", ".", "mean", "(", ")", "\n", "", "return", "estimated_rewards", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SelfNormalizedSlateStandardIPS._estimate_round_rewards": [[999, 1035], ["numpy.zeros_like", "iw.mean"], "methods", ["None"], ["def", "_estimate_round_rewards", "(", "\n", "self", ",", "\n", "reward", ":", "np", ".", "ndarray", ",", "\n", "position", ":", "np", ".", "ndarray", ",", "\n", "behavior_policy_pscore", ":", "np", ".", "ndarray", ",", "\n", "evaluation_policy_pscore", ":", "np", ".", "ndarray", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "np", ".", "ndarray", ":", "\n", "        ", "\"\"\"Estimate rewards for each slate and slot (position).\n\n        Parameters\n        ----------\n        reward: array-like, shape (<= n_rounds * len_list,)\n            Slot-level rewards, i.e., :math:`r_{i}(l)`.\n\n        position: array-like, shape (<= n_rounds * len_list,)\n            Indices to differentiate slots/positions in a slate/ranking.\n\n        pscore: array-like, shape (<= n_rounds * len_list,)\n            Joint probabilities of behavior policy choosing a slate action, i.e., :math:`\\\\pi_b(a_i|x_i)`.\n            This parameter must be unique in each slate.\n\n        evaluation_policy_pscore: array-like, shape (<= n_rounds * len_list,)\n            Joint probabilities of evaluation policy choosing a slate action, i.e., :math:`\\\\pi_e(a_i|x_i)`.\n            This parameter must be unique in each slate.\n\n        Returns\n        ----------\n        estimated_rewards: array-like, shape (<= n_rounds * len_list,)\n            Rewards estimated by the SNSIPS estimator for each slate and slot (position).\n\n        \"\"\"", "\n", "estimated_rewards", "=", "np", ".", "zeros_like", "(", "behavior_policy_pscore", ")", "\n", "iw", "=", "evaluation_policy_pscore", "/", "behavior_policy_pscore", "\n", "estimated_rewards", "=", "reward", "*", "iw", "/", "iw", ".", "mean", "(", ")", "\n", "return", "estimated_rewards", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.benchmark_ope_estimators_hypara.main": [[35, 181], ["hydra.main", "print", "logger.info", "time.time", "logger.info", "list", "numpy.random.seed", "obp.dataset.OpenBanditDataset", "obp.dataset.OpenBanditDataset", "pyieoe.evaluator.InterpretableOPEEvaluator", "logger.info", "pyieoe.evaluator.InterpretableOPEEvaluator.estimate_policy_value", "pyieoe.evaluator.InterpretableOPEEvaluator.calculate_mean", "pyieoe.evaluator.InterpretableOPEEvaluator.calculate_mean", "pathlib.Path", "pathlib.Path.mkdir", "pandas.DataFrame", "list", "list", "list", "pandas.DataFrame.to_csv", "pandas.DataFrame", "pandas.DataFrame().reset_index", "DataFrame().reset_index.rename", "pingouin.pairwise_ttests().round().drop", "pg.pairwise_ttests().round().drop.to_csv", "print", "numpy.round", "logger.info", "obp.dataset.OpenBanditDataset.calc_on_policy_policy_value_estimate", "obp.policy.BernoulliTS", "obp.policy.BernoulliTS.compute_batch_action_dist", "obp.dataset.OpenBanditDataset.calc_on_policy_policy_value_estimate", "obp.policy.Random", "obp.policy.Random.compute_batch_action_dist", "dict", "evaluator.calculate_mean.keys", "evaluator.calculate_mean.values", "evaluator.calculate_mean.values", "pyieoe.evaluator.InterpretableOPEEvaluator.calculate_squared_error", "dict", "obp.ope.DoublyRobustWithShrinkage", "obp.ope.DoublyRobustWithShrinkageTuning", "obp.dataset.OpenBanditDataset.obtain_batch_bandit_feedback", "obp.dataset.OpenBanditDataset.obtain_batch_bandit_feedback", "numpy.arange", "pandas.DataFrame", "pingouin.pairwise_ttests().round", "pathlib.Path().cwd", "obp.dataset.OpenBanditDataset.obtain_batch_bandit_feedback", "obp.dataset.OpenBanditDataset.obtain_batch_bandit_feedback", "DataFrame().reset_index.stack", "time.time", "pathlib.Path().cwd", "pingouin.pairwise_ttests", "pathlib.Path", "pathlib.Path"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.benchmark_ope_estimators.main", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_policy_value", "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.real.OpenBanditDataset.calc_on_policy_policy_value_estimate", "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.contextfree.BernoulliTS.compute_batch_action_dist", "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.real.OpenBanditDataset.calc_on_policy_policy_value_estimate", "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.contextfree.BernoulliTS.compute_batch_action_dist", "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.multiclass.MultiClassToBanditReduction.obtain_batch_bandit_feedback", "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.multiclass.MultiClassToBanditReduction.obtain_batch_bandit_feedback", "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.multiclass.MultiClassToBanditReduction.obtain_batch_bandit_feedback", "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.multiclass.MultiClassToBanditReduction.obtain_batch_bandit_feedback"], ["@", "hydra", ".", "main", "(", "config_path", "=", "\"./conf\"", ",", "config_name", "=", "\"config\"", ")", "\n", "def", "main", "(", "cfg", ":", "DictConfig", ")", "->", "None", ":", "\n", "    ", "print", "(", "cfg", ")", "\n", "logger", ".", "info", "(", "f\"The current working directory is {Path().cwd()}\"", ")", "\n", "start_time", "=", "time", ".", "time", "(", ")", "\n", "logger", ".", "info", "(", "\"initializing experimental condition..\"", ")", "\n", "\n", "# compared ope estimators", "\n", "lambdas", "=", "list", "(", "dict", "(", "cfg", ".", "estimator_hyperparams", ")", "[", "\"lambdas\"", "]", ")", "\n", "ope_estimators", "=", "[", "\n", "DoublyRobustWithShrinkage", "(", "lambda_", "=", "lam_", ",", "estimator_name", "=", "f\"DRos ({lam_})\"", ")", "\n", "for", "lam_", "in", "lambdas", "\n", "]", "+", "[", "\n", "DoublyRobustWithShrinkageTuning", "(", "\n", "lambdas", "=", "lambdas", ",", "estimator_name", "=", "\"DRos (tuning)\"", "\n", ")", ",", "\n", "]", "\n", "\n", "# configurations", "\n", "n_seeds", "=", "cfg", ".", "setting", ".", "n_seeds", "\n", "sample_size", "=", "cfg", ".", "setting", ".", "sample_size", "\n", "reg_model", "=", "cfg", ".", "setting", ".", "reg_model", "\n", "campaign", "=", "cfg", ".", "setting", ".", "campaign", "\n", "behavior_policy", "=", "cfg", ".", "setting", ".", "behavior_policy", "\n", "test_size", "=", "cfg", ".", "setting", ".", "test_size", "\n", "is_timeseries_split", "=", "cfg", ".", "setting", ".", "is_timeseries_split", "\n", "n_folds", "=", "cfg", ".", "setting", ".", "n_folds", "\n", "obd_path", "=", "(", "\n", "Path", "(", ")", ".", "cwd", "(", ")", ".", "parents", "[", "5", "]", "/", "\"open_bandit_dataset\"", "\n", "if", "cfg", ".", "setting", ".", "is_full_obd", "\n", "else", "None", "\n", ")", "\n", "random_state", "=", "cfg", ".", "setting", ".", "random_state", "\n", "np", ".", "random", ".", "seed", "(", "random_state", ")", "\n", "\n", "# define dataset", "\n", "dataset_ts", "=", "OpenBanditDataset", "(", "\n", "behavior_policy", "=", "\"bts\"", ",", "campaign", "=", "campaign", ",", "data_path", "=", "obd_path", "\n", ")", "\n", "dataset_ur", "=", "OpenBanditDataset", "(", "\n", "behavior_policy", "=", "\"random\"", ",", "campaign", "=", "campaign", ",", "data_path", "=", "obd_path", "\n", ")", "\n", "\n", "# prepare logged bandit feedback and evaluation policies", "\n", "if", "behavior_policy", "==", "\"random\"", ":", "\n", "        ", "if", "is_timeseries_split", ":", "\n", "            ", "bandit_feedback_ur", "=", "dataset_ur", ".", "obtain_batch_bandit_feedback", "(", "\n", "test_size", "=", "test_size", ",", "\n", "is_timeseries_split", "=", "True", ",", "\n", ")", "[", "0", "]", "\n", "", "else", ":", "\n", "            ", "bandit_feedback_ur", "=", "dataset_ur", ".", "obtain_batch_bandit_feedback", "(", ")", "\n", "", "bandit_feedbacks", "=", "[", "bandit_feedback_ur", "]", "\n", "# obtain the ground-truth policy value", "\n", "ground_truth_ts", "=", "OpenBanditDataset", ".", "calc_on_policy_policy_value_estimate", "(", "\n", "behavior_policy", "=", "\"bts\"", ",", "\n", "campaign", "=", "campaign", ",", "\n", "data_path", "=", "obd_path", ",", "\n", "test_size", "=", "test_size", ",", "\n", "is_timeseries_split", "=", "is_timeseries_split", ",", "\n", ")", "\n", "# obtain action choice probabilities and define evaluation policies", "\n", "policy_ts", "=", "BernoulliTS", "(", "\n", "n_actions", "=", "dataset_ts", ".", "n_actions", ",", "\n", "len_list", "=", "dataset_ts", ".", "len_list", ",", "\n", "random_state", "=", "random_state", ",", "\n", "is_zozotown_prior", "=", "True", ",", "\n", "campaign", "=", "campaign", ",", "\n", ")", "\n", "action_dist_ts", "=", "policy_ts", ".", "compute_batch_action_dist", "(", "n_rounds", "=", "1000000", ")", "\n", "evaluation_policies", "=", "[", "(", "ground_truth_ts", ",", "action_dist_ts", ")", "]", "\n", "", "else", ":", "\n", "        ", "if", "is_timeseries_split", ":", "\n", "            ", "bandit_feedback_ts", "=", "dataset_ts", ".", "obtain_batch_bandit_feedback", "(", "\n", "test_size", "=", "test_size", ",", "\n", "is_timeseries_split", "=", "True", ",", "\n", ")", "[", "0", "]", "\n", "", "else", ":", "\n", "            ", "bandit_feedback_ts", "=", "dataset_ts", ".", "obtain_batch_bandit_feedback", "(", ")", "\n", "", "bandit_feedbacks", "=", "[", "bandit_feedback_ts", "]", "\n", "# obtain the ground-truth policy value", "\n", "ground_truth_ur", "=", "OpenBanditDataset", ".", "calc_on_policy_policy_value_estimate", "(", "\n", "behavior_policy", "=", "\"random\"", ",", "\n", "campaign", "=", "campaign", ",", "\n", "data_path", "=", "obd_path", ",", "\n", "test_size", "=", "test_size", ",", "\n", "is_timeseries_split", "=", "is_timeseries_split", ",", "\n", ")", "\n", "# obtain action choice probabilities and define evaluation policies", "\n", "policy_ur", "=", "Random", "(", "\n", "n_actions", "=", "dataset_ur", ".", "n_actions", ",", "\n", "len_list", "=", "dataset_ur", ".", "len_list", ",", "\n", "random_state", "=", "random_state", ",", "\n", ")", "\n", "action_dist_ur", "=", "policy_ur", ".", "compute_batch_action_dist", "(", "n_rounds", "=", "1000000", ")", "\n", "evaluation_policies", "=", "[", "(", "ground_truth_ur", ",", "action_dist_ur", ")", "]", "\n", "\n", "# regression models used in ope estimators", "\n", "", "hyperparams", "=", "dict", "(", "cfg", ".", "reg_model_hyperparams", ")", "[", "reg_model", "]", "\n", "regression_models", "=", "[", "reg_model_dict", "[", "reg_model", "]", "(", "**", "hyperparams", ")", "]", "\n", "\n", "# define an evaluator class", "\n", "evaluator", "=", "InterpretableOPEEvaluator", "(", "\n", "random_states", "=", "np", ".", "arange", "(", "n_seeds", ")", ",", "\n", "bandit_feedbacks", "=", "bandit_feedbacks", ",", "\n", "evaluation_policies", "=", "evaluation_policies", ",", "\n", "ope_estimators", "=", "ope_estimators", ",", "\n", "regression_models", "=", "regression_models", ",", "\n", ")", "\n", "\n", "# conduct an evaluation of OPE experiment", "\n", "logger", ".", "info", "(", "\"experiment started\"", ")", "\n", "_", "=", "evaluator", ".", "estimate_policy_value", "(", "sample_size", "=", "sample_size", ",", "n_folds_", "=", "n_folds", ")", "\n", "# calculate statistics", "\n", "mean", "=", "evaluator", ".", "calculate_mean", "(", "root", "=", "True", ")", "\n", "mean_scaled", "=", "evaluator", ".", "calculate_mean", "(", "scale", "=", "True", ",", "root", "=", "True", ")", "\n", "\n", "# save results of the evaluation of off-policy estimators", "\n", "log_path", "=", "Path", "(", "\"./outputs/hypara\"", ")", "\n", "log_path", ".", "mkdir", "(", "exist_ok", "=", "True", ",", "parents", "=", "True", ")", "\n", "# save root mse", "\n", "root_mse_df", "=", "DataFrame", "(", ")", "\n", "root_mse_df", "[", "\"estimator\"", "]", "=", "list", "(", "mean", ".", "keys", "(", ")", ")", "\n", "root_mse_df", "[", "\"mean\"", "]", "=", "list", "(", "mean", ".", "values", "(", ")", ")", "\n", "root_mse_df", "[", "\"mean(scaled)\"", "]", "=", "list", "(", "mean_scaled", ".", "values", "(", ")", ")", "\n", "root_mse_df", ".", "to_csv", "(", "log_path", "/", "\"root_mse.csv\"", ")", "\n", "# conduct pairwise t-tests", "\n", "se_df", "=", "DataFrame", "(", "evaluator", ".", "calculate_squared_error", "(", ")", ")", "\n", "se_df", "=", "DataFrame", "(", "se_df", ".", "stack", "(", ")", ")", ".", "reset_index", "(", "1", ")", "\n", "se_df", ".", "rename", "(", "columns", "=", "{", "\"level_1\"", ":", "\"estimators\"", ",", "0", ":", "\"se\"", "}", ",", "inplace", "=", "True", ")", "\n", "nonparam_ttests", "=", "(", "\n", "pg", ".", "pairwise_ttests", "(", "\n", "data", "=", "se_df", ",", "\n", "dv", "=", "\"se\"", ",", "\n", "parametric", "=", "False", ",", "\n", "between", "=", "\"estimators\"", ",", "\n", ")", "\n", ".", "round", "(", "4", ")", "\n", ".", "drop", "(", "[", "\"Contrast\"", ",", "\"Parametric\"", ",", "\"Paired\"", "]", ",", "axis", "=", "1", ")", "\n", ")", "\n", "nonparam_ttests", ".", "to_csv", "(", "log_path", "/", "\"nonparam_ttests.csv\"", ")", "\n", "# print result", "\n", "print", "(", "root_mse_df", ")", "\n", "experiment", "=", "f\"{campaign}-{behavior_policy}-{sample_size}\"", "\n", "elapsed_time", "=", "np", ".", "round", "(", "(", "time", ".", "time", "(", ")", "-", "start_time", ")", "/", "60", ",", "2", ")", "\n", "logger", ".", "info", "(", "f\"finish experiment {experiment} in {elapsed_time}min\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.benchmark_ope_estimators.main": [[40, 191], ["hydra.main", "print", "logger.info", "time.time", "logger.info", "list", "numpy.random.seed", "obp.dataset.OpenBanditDataset", "obp.dataset.OpenBanditDataset", "pyieoe.evaluator.InterpretableOPEEvaluator", "logger.info", "pyieoe.evaluator.InterpretableOPEEvaluator.estimate_policy_value", "pyieoe.evaluator.InterpretableOPEEvaluator.calculate_mean", "pyieoe.evaluator.InterpretableOPEEvaluator.calculate_mean", "pathlib.Path", "pathlib.Path.mkdir", "pandas.DataFrame", "list", "list", "list", "pandas.DataFrame.to_csv", "pandas.DataFrame", "pandas.DataFrame().reset_index", "DataFrame().reset_index.rename", "pingouin.pairwise_ttests().round().drop", "pg.pairwise_ttests().round().drop.to_csv", "pandas.DataFrame().describe().to_csv", "print", "numpy.round", "logger.info", "obp.ope.InverseProbabilityWeighting", "obp.ope.SelfNormalizedInverseProbabilityWeighting", "obp.ope.DirectMethod", "obp.ope.DoublyRobust", "obp.ope.SelfNormalizedDoublyRobust", "obp.ope.SwitchDoublyRobustTuning", "obp.ope.DoublyRobustWithShrinkageTuning", "obp.dataset.OpenBanditDataset.calc_on_policy_policy_value_estimate", "obp.policy.BernoulliTS", "obp.policy.BernoulliTS.compute_batch_action_dist", "obp.dataset.OpenBanditDataset.calc_on_policy_policy_value_estimate", "obp.policy.Random", "obp.policy.Random.compute_batch_action_dist", "dict", "evaluator.calculate_mean.keys", "evaluator.calculate_mean.values", "evaluator.calculate_mean.values", "pyieoe.evaluator.InterpretableOPEEvaluator.calculate_squared_error", "dict", "obp.dataset.OpenBanditDataset.obtain_batch_bandit_feedback", "obp.dataset.OpenBanditDataset.obtain_batch_bandit_feedback", "numpy.arange", "pandas.DataFrame", "pingouin.pairwise_ttests().round", "pandas.DataFrame().describe", "pathlib.Path().cwd", "obp.dataset.OpenBanditDataset.obtain_batch_bandit_feedback", "obp.dataset.OpenBanditDataset.obtain_batch_bandit_feedback", "DataFrame().reset_index.stack", "time.time", "pathlib.Path().cwd", "pingouin.pairwise_ttests", "pandas.DataFrame", "pathlib.Path", "pathlib.Path"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.ope.benchmark_ope_estimators.main", "home.repos.pwc.inspect_result.st-tech_zr-obp.ope.estimators_slate.SlateCascadeDoublyRobust.estimate_policy_value", "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.real.OpenBanditDataset.calc_on_policy_policy_value_estimate", "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.contextfree.BernoulliTS.compute_batch_action_dist", "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.real.OpenBanditDataset.calc_on_policy_policy_value_estimate", "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.contextfree.BernoulliTS.compute_batch_action_dist", "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.multiclass.MultiClassToBanditReduction.obtain_batch_bandit_feedback", "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.multiclass.MultiClassToBanditReduction.obtain_batch_bandit_feedback", "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.multiclass.MultiClassToBanditReduction.obtain_batch_bandit_feedback", "home.repos.pwc.inspect_result.st-tech_zr-obp.dataset.multiclass.MultiClassToBanditReduction.obtain_batch_bandit_feedback"], ["@", "hydra", ".", "main", "(", "config_path", "=", "\"./conf\"", ",", "config_name", "=", "\"config\"", ")", "\n", "def", "main", "(", "cfg", ":", "DictConfig", ")", "->", "None", ":", "\n", "    ", "print", "(", "cfg", ")", "\n", "logger", ".", "info", "(", "f\"The current working directory is {Path().cwd()}\"", ")", "\n", "start_time", "=", "time", ".", "time", "(", ")", "\n", "logger", ".", "info", "(", "\"initializing experimental condition..\"", ")", "\n", "\n", "# compared ope estimators", "\n", "lambdas", "=", "list", "(", "dict", "(", "cfg", ".", "estimator_hyperparams", ")", "[", "\"lambdas\"", "]", ")", "\n", "ope_estimators", "=", "[", "\n", "InverseProbabilityWeighting", "(", "estimator_name", "=", "\"IPW\"", ")", ",", "\n", "SelfNormalizedInverseProbabilityWeighting", "(", "estimator_name", "=", "\"SNIPW\"", ")", ",", "\n", "DirectMethod", "(", "estimator_name", "=", "\"DM\"", ")", ",", "\n", "DoublyRobust", "(", "estimator_name", "=", "\"DR\"", ")", ",", "\n", "SelfNormalizedDoublyRobust", "(", "estimator_name", "=", "\"SNDR\"", ")", ",", "\n", "SwitchDoublyRobustTuning", "(", "lambdas", "=", "lambdas", ",", "estimator_name", "=", "\"Switch-DR\"", ")", ",", "\n", "DoublyRobustWithShrinkageTuning", "(", "lambdas", "=", "lambdas", ",", "estimator_name", "=", "\"DRos\"", ")", ",", "\n", "]", "\n", "\n", "# configurations", "\n", "n_seeds", "=", "cfg", ".", "setting", ".", "n_seeds", "\n", "sample_size", "=", "cfg", ".", "setting", ".", "sample_size", "\n", "reg_model", "=", "cfg", ".", "setting", ".", "reg_model", "\n", "campaign", "=", "cfg", ".", "setting", ".", "campaign", "\n", "behavior_policy", "=", "cfg", ".", "setting", ".", "behavior_policy", "\n", "test_size", "=", "cfg", ".", "setting", ".", "test_size", "\n", "is_timeseries_split", "=", "cfg", ".", "setting", ".", "is_timeseries_split", "\n", "n_folds", "=", "cfg", ".", "setting", ".", "n_folds", "\n", "obd_path", "=", "(", "\n", "Path", "(", ")", ".", "cwd", "(", ")", ".", "parents", "[", "5", "]", "/", "\"open_bandit_dataset\"", "\n", "if", "cfg", ".", "setting", ".", "is_full_obd", "\n", "else", "None", "\n", ")", "\n", "random_state", "=", "cfg", ".", "setting", ".", "random_state", "\n", "np", ".", "random", ".", "seed", "(", "random_state", ")", "\n", "\n", "# define dataset", "\n", "dataset_ts", "=", "OpenBanditDataset", "(", "\n", "behavior_policy", "=", "\"bts\"", ",", "campaign", "=", "campaign", ",", "data_path", "=", "obd_path", "\n", ")", "\n", "dataset_ur", "=", "OpenBanditDataset", "(", "\n", "behavior_policy", "=", "\"random\"", ",", "campaign", "=", "campaign", ",", "data_path", "=", "obd_path", "\n", ")", "\n", "\n", "# prepare logged bandit feedback and evaluation policies", "\n", "if", "behavior_policy", "==", "\"random\"", ":", "\n", "        ", "if", "is_timeseries_split", ":", "\n", "            ", "bandit_feedback_ur", "=", "dataset_ur", ".", "obtain_batch_bandit_feedback", "(", "\n", "test_size", "=", "test_size", ",", "\n", "is_timeseries_split", "=", "True", ",", "\n", ")", "[", "0", "]", "\n", "", "else", ":", "\n", "            ", "bandit_feedback_ur", "=", "dataset_ur", ".", "obtain_batch_bandit_feedback", "(", ")", "\n", "", "bandit_feedbacks", "=", "[", "bandit_feedback_ur", "]", "\n", "# obtain the ground-truth policy value", "\n", "ground_truth_ts", "=", "OpenBanditDataset", ".", "calc_on_policy_policy_value_estimate", "(", "\n", "behavior_policy", "=", "\"bts\"", ",", "\n", "campaign", "=", "campaign", ",", "\n", "data_path", "=", "obd_path", ",", "\n", "test_size", "=", "test_size", ",", "\n", "is_timeseries_split", "=", "is_timeseries_split", ",", "\n", ")", "\n", "# obtain action choice probabilities and define evaluation policies", "\n", "policy_ts", "=", "BernoulliTS", "(", "\n", "n_actions", "=", "dataset_ts", ".", "n_actions", ",", "\n", "len_list", "=", "dataset_ts", ".", "len_list", ",", "\n", "random_state", "=", "random_state", ",", "\n", "is_zozotown_prior", "=", "True", ",", "\n", "campaign", "=", "campaign", ",", "\n", ")", "\n", "action_dist_ts", "=", "policy_ts", ".", "compute_batch_action_dist", "(", "n_rounds", "=", "1000000", ")", "\n", "evaluation_policies", "=", "[", "(", "ground_truth_ts", ",", "action_dist_ts", ")", "]", "\n", "", "else", ":", "\n", "        ", "if", "is_timeseries_split", ":", "\n", "            ", "bandit_feedback_ts", "=", "dataset_ts", ".", "obtain_batch_bandit_feedback", "(", "\n", "test_size", "=", "test_size", ",", "\n", "is_timeseries_split", "=", "True", ",", "\n", ")", "[", "0", "]", "\n", "", "else", ":", "\n", "            ", "bandit_feedback_ts", "=", "dataset_ts", ".", "obtain_batch_bandit_feedback", "(", ")", "\n", "", "bandit_feedbacks", "=", "[", "bandit_feedback_ts", "]", "\n", "# obtain the ground-truth policy value", "\n", "ground_truth_ur", "=", "OpenBanditDataset", ".", "calc_on_policy_policy_value_estimate", "(", "\n", "behavior_policy", "=", "\"random\"", ",", "\n", "campaign", "=", "campaign", ",", "\n", "data_path", "=", "obd_path", ",", "\n", "test_size", "=", "test_size", ",", "\n", "is_timeseries_split", "=", "is_timeseries_split", ",", "\n", ")", "\n", "# obtain action choice probabilities and define evaluation policies", "\n", "policy_ur", "=", "Random", "(", "\n", "n_actions", "=", "dataset_ur", ".", "n_actions", ",", "\n", "len_list", "=", "dataset_ur", ".", "len_list", ",", "\n", "random_state", "=", "random_state", ",", "\n", ")", "\n", "action_dist_ur", "=", "policy_ur", ".", "compute_batch_action_dist", "(", "n_rounds", "=", "1000000", ")", "\n", "evaluation_policies", "=", "[", "(", "ground_truth_ur", ",", "action_dist_ur", ")", "]", "\n", "\n", "# regression models used in ope estimators", "\n", "", "hyperparams", "=", "dict", "(", "cfg", ".", "reg_model_hyperparams", ")", "[", "reg_model", "]", "\n", "regression_models", "=", "[", "reg_model_dict", "[", "reg_model", "]", "(", "**", "hyperparams", ")", "]", "\n", "\n", "# define an evaluator class", "\n", "evaluator", "=", "InterpretableOPEEvaluator", "(", "\n", "random_states", "=", "np", ".", "arange", "(", "n_seeds", ")", ",", "\n", "bandit_feedbacks", "=", "bandit_feedbacks", ",", "\n", "evaluation_policies", "=", "evaluation_policies", ",", "\n", "ope_estimators", "=", "ope_estimators", ",", "\n", "regression_models", "=", "regression_models", ",", "\n", ")", "\n", "\n", "# conduct an evaluation of OPE experiment", "\n", "logger", ".", "info", "(", "\"experiment started\"", ")", "\n", "_", "=", "evaluator", ".", "estimate_policy_value", "(", "sample_size", "=", "sample_size", ",", "n_folds_", "=", "n_folds", ")", "\n", "# calculate statistics", "\n", "mean", "=", "evaluator", ".", "calculate_mean", "(", "root", "=", "True", ")", "\n", "mean_scaled", "=", "evaluator", ".", "calculate_mean", "(", "scale", "=", "True", ",", "root", "=", "True", ")", "\n", "\n", "# save results of the evaluation of off-policy estimators", "\n", "log_path", "=", "Path", "(", "\"./outputs\"", ")", "\n", "log_path", ".", "mkdir", "(", "exist_ok", "=", "True", ",", "parents", "=", "True", ")", "\n", "# save root mse", "\n", "root_mse_df", "=", "DataFrame", "(", ")", "\n", "root_mse_df", "[", "\"estimator\"", "]", "=", "list", "(", "mean", ".", "keys", "(", ")", ")", "\n", "root_mse_df", "[", "\"mean\"", "]", "=", "list", "(", "mean", ".", "values", "(", ")", ")", "\n", "root_mse_df", "[", "\"mean(scaled)\"", "]", "=", "list", "(", "mean_scaled", ".", "values", "(", ")", ")", "\n", "root_mse_df", ".", "to_csv", "(", "log_path", "/", "\"root_mse.csv\"", ")", "\n", "# conduct pairwise t-tests", "\n", "se_df", "=", "DataFrame", "(", "evaluator", ".", "calculate_squared_error", "(", ")", ")", "\n", "se_df", "=", "DataFrame", "(", "se_df", ".", "stack", "(", ")", ")", ".", "reset_index", "(", "1", ")", "\n", "se_df", ".", "rename", "(", "columns", "=", "{", "\"level_1\"", ":", "\"estimators\"", ",", "0", ":", "\"se\"", "}", ",", "inplace", "=", "True", ")", "\n", "nonparam_ttests", "=", "(", "\n", "pg", ".", "pairwise_ttests", "(", "\n", "data", "=", "se_df", ",", "\n", "dv", "=", "\"se\"", ",", "\n", "parametric", "=", "False", ",", "\n", "between", "=", "\"estimators\"", ",", "\n", ")", "\n", ".", "round", "(", "4", ")", "\n", ".", "drop", "(", "[", "\"Contrast\"", ",", "\"Parametric\"", ",", "\"Paired\"", "]", ",", "axis", "=", "1", ")", "\n", ")", "\n", "nonparam_ttests", ".", "to_csv", "(", "log_path", "/", "\"nonparam_ttests.csv\"", ")", "\n", "# save reg model metrics", "\n", "DataFrame", "(", "evaluator", ".", "reg_model_metrics", ")", ".", "describe", "(", ")", ".", "to_csv", "(", "\n", "log_path", "/", "\"reg_model_metrics.csv\"", "\n", ")", "\n", "# print result", "\n", "print", "(", "root_mse_df", ")", "\n", "experiment", "=", "f\"{campaign}-{behavior_policy}-{sample_size}\"", "\n", "elapsed_time", "=", "np", ".", "round", "(", "(", "time", ".", "time", "(", ")", "-", "start_time", ")", "/", "60", ",", "2", ")", "\n", "logger", ".", "info", "(", "f\"finish experiment {experiment} in {elapsed_time}min\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_confidence_interval_arguments": [[16, 43], ["sklearn.utils.check_random_state", "sklearn.utils.check_scalar", "sklearn.utils.check_scalar"], "function", ["None"], ["def", "check_confidence_interval_arguments", "(", "\n", "alpha", ":", "float", "=", "0.05", ",", "\n", "n_bootstrap_samples", ":", "int", "=", "10000", ",", "\n", "random_state", ":", "Optional", "[", "int", "]", "=", "None", ",", "\n", ")", "->", "Optional", "[", "ValueError", "]", ":", "\n", "    ", "\"\"\"Check confidence interval arguments.\n\n    Parameters\n    ----------\n    alpha: float, default=0.05\n        Significance level.\n\n    n_bootstrap_samples: int, default=10000\n        Number of resampling performed in bootstrap sampling.\n\n    random_state: int, default=None\n        Controls the random seed in bootstrap sampling.\n\n    Returns\n    ----------\n    estimated_confidence_interval: Dict[str, float]\n        Dictionary storing the estimated mean and upper-lower confidence bounds.\n\n    \"\"\"", "\n", "check_random_state", "(", "random_state", ")", "\n", "check_scalar", "(", "alpha", ",", "\"alpha\"", ",", "float", ",", "min_val", "=", "0.0", ",", "max_val", "=", "1.0", ")", "\n", "check_scalar", "(", "n_bootstrap_samples", ",", "\"n_bootstrap_samples\"", ",", "int", ",", "min_val", "=", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.estimate_confidence_interval_by_bootstrap": [[45, 87], ["utils.check_confidence_interval_arguments", "list", "sklearn.utils.check_random_state", "numpy.arange", "numpy.percentile", "numpy.percentile", "list.append", "numpy.mean", "numpy.mean", "sklearn.utils.check_random_state.choice"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_confidence_interval_arguments"], ["", "def", "estimate_confidence_interval_by_bootstrap", "(", "\n", "samples", ":", "np", ".", "ndarray", ",", "\n", "alpha", ":", "float", "=", "0.05", ",", "\n", "n_bootstrap_samples", ":", "int", "=", "10000", ",", "\n", "random_state", ":", "Optional", "[", "int", "]", "=", "None", ",", "\n", ")", "->", "Dict", "[", "str", ",", "float", "]", ":", "\n", "    ", "\"\"\"Estimate confidence interval using bootstrap.\n\n    Parameters\n    ----------\n    samples: array-like\n        Empirical observed samples to be used to estimate cumulative distribution function.\n\n    alpha: float, default=0.05\n        Significance level.\n\n    n_bootstrap_samples: int, default=10000\n        Number of resampling performed in bootstrap sampling.\n\n    random_state: int, default=None\n        Controls the random seed in bootstrap sampling.\n\n    Returns\n    ----------\n    estimated_confidence_interval: Dict[str, float]\n        Dictionary storing the estimated mean and upper-lower confidence bounds.\n\n    \"\"\"", "\n", "check_confidence_interval_arguments", "(", "\n", "alpha", "=", "alpha", ",", "n_bootstrap_samples", "=", "n_bootstrap_samples", ",", "random_state", "=", "random_state", "\n", ")", "\n", "\n", "boot_samples", "=", "list", "(", ")", "\n", "random_", "=", "check_random_state", "(", "random_state", ")", "\n", "for", "_", "in", "np", ".", "arange", "(", "n_bootstrap_samples", ")", ":", "\n", "        ", "boot_samples", ".", "append", "(", "np", ".", "mean", "(", "random_", ".", "choice", "(", "samples", ",", "size", "=", "samples", ".", "shape", "[", "0", "]", ")", ")", ")", "\n", "", "lower_bound", "=", "np", ".", "percentile", "(", "boot_samples", ",", "100", "*", "(", "alpha", "/", "2", ")", ")", "\n", "upper_bound", "=", "np", ".", "percentile", "(", "boot_samples", ",", "100", "*", "(", "1.0", "-", "alpha", "/", "2", ")", ")", "\n", "return", "{", "\n", "\"mean\"", ":", "np", ".", "mean", "(", "boot_samples", ")", ",", "\n", "f\"{100 * (1. - alpha)}% CI (lower)\"", ":", "lower_bound", ",", "\n", "f\"{100 * (1. - alpha)}% CI (upper)\"", ":", "upper_bound", ",", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.sample_action_fast": [[90, 115], ["sklearn.utils.check_random_state", "action_dist.cumsum", "flg.argmax", "sklearn.utils.check_random_state.uniform"], "function", ["None"], ["", "def", "sample_action_fast", "(", "\n", "action_dist", ":", "np", ".", "ndarray", ",", "random_state", ":", "Optional", "[", "int", "]", "=", "None", "\n", ")", "->", "np", ".", "ndarray", ":", "\n", "    ", "\"\"\"Sample actions faster based on a given action distribution.\n\n    Parameters\n    ----------\n    action_dist: array-like, shape (n_rounds, n_actions)\n        Distribution over actions.\n\n    random_state: Optional[int], default=None\n        Controls the random seed in sampling synthetic bandit dataset.\n\n    Returns\n    ---------\n    sampled_action: array-like, shape (n_rounds,)\n        Actions sampled based on `action_dist`.\n\n    \"\"\"", "\n", "random_", "=", "check_random_state", "(", "random_state", ")", "\n", "uniform_rvs", "=", "random_", ".", "uniform", "(", "size", "=", "action_dist", ".", "shape", "[", "0", "]", ")", "[", ":", ",", "np", ".", "newaxis", "]", "\n", "cum_action_dist", "=", "action_dist", ".", "cumsum", "(", "axis", "=", "1", ")", "\n", "flg", "=", "cum_action_dist", ">", "uniform_rvs", "\n", "sampled_action", "=", "flg", ".", "argmax", "(", "axis", "=", "1", ")", "\n", "return", "sampled_action", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.convert_to_action_dist": [[117, 148], ["numpy.zeros", "numpy.arange", "numpy.arange", "numpy.ones"], "function", ["None"], ["", "def", "convert_to_action_dist", "(", "\n", "n_actions", ":", "int", ",", "\n", "selected_actions", ":", "np", ".", "ndarray", ",", "\n", ")", "->", "np", ".", "ndarray", ":", "\n", "    ", "\"\"\"Convert selected actions (output of `run_bandit_simulation`) to distribution over actions.\n\n    Parameters\n    ----------\n    n_actions: int\n        Number of actions.\n\n    selected_actions: array-like, shape (n_rounds, len_list)\n        Sequence of actions selected by evaluation policy\n        at each round in offline bandit simulation.\n\n    Returns\n    ----------\n    action_dist: array-like, shape (n_rounds, n_actions, len_list)\n        Action choice probabilities (can be deterministic).\n\n    \"\"\"", "\n", "n_rounds", ",", "len_list", "=", "selected_actions", ".", "shape", "\n", "action_dist", "=", "np", ".", "zeros", "(", "(", "n_rounds", ",", "n_actions", ",", "len_list", ")", ")", "\n", "for", "pos", "in", "np", ".", "arange", "(", "len_list", ")", ":", "\n", "        ", "selected_actions_", "=", "selected_actions", "[", ":", ",", "pos", "]", "\n", "action_dist", "[", "\n", "np", ".", "arange", "(", "n_rounds", ")", ",", "\n", "selected_actions_", ",", "\n", "pos", "*", "np", ".", "ones", "(", "n_rounds", ",", "int", ")", ",", "\n", "]", "=", "1", "\n", "", "return", "action_dist", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array": [[150, 176], ["isinstance", "ValueError", "ValueError", "type"], "function", ["None"], ["", "def", "check_array", "(", "\n", "array", ":", "np", ".", "ndarray", ",", "\n", "name", ":", "str", ",", "\n", "expected_dim", ":", "int", "=", "1", ",", "\n", ")", "->", "ValueError", ":", "\n", "    ", "\"\"\"Input validation on an array.\n\n    Parameters\n    -------------\n    array: object\n        Input object to check.\n\n    name: str\n        Name of the input array.\n\n    expected_dim: int, default=1\n        Expected dimension of the input array.\n\n    \"\"\"", "\n", "if", "not", "isinstance", "(", "array", ",", "np", ".", "ndarray", ")", ":", "\n", "        ", "raise", "ValueError", "(", "\n", "f\"`{name}` must be {expected_dim}D array, but got {type(array)}\"", "\n", ")", "\n", "", "if", "array", ".", "ndim", "!=", "expected_dim", ":", "\n", "        ", "raise", "ValueError", "(", "\n", "f\"`{name}` must be {expected_dim}D array, but got {array.ndim}D array\"", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_tensor": [[179, 205], ["isinstance", "ValueError", "ValueError", "type"], "function", ["None"], ["", "", "def", "check_tensor", "(", "\n", "tensor", ":", "torch", ".", "tensor", ",", "\n", "name", ":", "str", ",", "\n", "expected_dim", ":", "int", "=", "1", ",", "\n", ")", "->", "ValueError", ":", "\n", "    ", "\"\"\"Input validation on a tensor.\n\n    Parameters\n    -------------\n    array: object\n        Input object to check.\n\n    name: str\n        Name of the input array.\n\n    expected_dim: int, default=1\n        Expected dimension of the input array.\n\n    \"\"\"", "\n", "if", "not", "isinstance", "(", "tensor", ",", "torch", ".", "Tensor", ")", ":", "\n", "        ", "raise", "ValueError", "(", "\n", "f\"`{name}` must be {expected_dim}D tensor, but got {type(tensor)}\"", "\n", ")", "\n", "", "if", "tensor", ".", "ndim", "!=", "expected_dim", ":", "\n", "        ", "raise", "ValueError", "(", "\n", "f\"`{name}` must be {expected_dim}D tensor, but got {tensor.ndim}D tensor\"", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_bandit_feedback_inputs": [[208, 311], ["utils.check_array", "utils.check_array", "utils.check_array", "utils.check_array", "utils.check_array", "numpy.any", "utils.check_array", "utils.check_array", "ValueError", "ValueError", "ValueError", "ValueError", "ValueError", "ValueError", "ValueError", "ValueError", "ValueError", "ValueError", "numpy.issubdtype", "numpy.issubdtype", "numpy.issubdtype", "numpy.issubdtype", "numpy.issubdtype", "action.min", "action.max", "action.min", "position.min", "action.min", "action.max", "action.min"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array"], ["", "", "def", "check_bandit_feedback_inputs", "(", "\n", "context", ":", "np", ".", "ndarray", ",", "\n", "action", ":", "np", ".", "ndarray", ",", "\n", "reward", ":", "np", ".", "ndarray", ",", "\n", "expected_reward", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "position", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "pscore", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "action_context", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", ")", "->", "Optional", "[", "ValueError", "]", ":", "\n", "    ", "\"\"\"Check inputs for bandit learning or simulation.\n\n    Parameters\n    -----------\n    context: array-like, shape (n_rounds, dim_context)\n        Context vectors observed for each data, i.e., :math:`x_i`.\n\n    action: array-like, shape (n_rounds,)\n        Actions sampled by the logging/behavior policy for each data in logged bandit data, i.e., :math:`a_i`.\n\n    reward: array-like, shape (n_rounds,)\n        Rewards observed for each data in logged bandit data, i.e., :math:`r_i`.\n\n    expected_reward: array-like, shape (n_rounds, n_actions), default=None\n        Expected reward of each data, i.e., :math:`\\\\mathbb{E}[r_i|x_i,a_i]`.\n\n    position: array-like, shape (n_rounds,), default=None\n        Indices to differentiate positions in a recommendation interface where the actions are presented.\n\n    pscore: array-like, shape (n_rounds,)\n        Action choice probabilities of the logging/behavior policy (propensity scores), i.e., :math:`\\\\pi_b(a_i|x_i)`.\n\n    action_context: array-like, shape (n_actions, dim_action_context)\n        Context vectors characterizing each action.\n\n    \"\"\"", "\n", "check_array", "(", "array", "=", "context", ",", "name", "=", "\"context\"", ",", "expected_dim", "=", "2", ")", "\n", "check_array", "(", "array", "=", "action", ",", "name", "=", "\"action\"", ",", "expected_dim", "=", "1", ")", "\n", "check_array", "(", "array", "=", "reward", ",", "name", "=", "\"reward\"", ",", "expected_dim", "=", "1", ")", "\n", "if", "expected_reward", "is", "not", "None", ":", "\n", "        ", "check_array", "(", "array", "=", "expected_reward", ",", "name", "=", "\"expected_reward\"", ",", "expected_dim", "=", "2", ")", "\n", "if", "not", "(", "\n", "context", ".", "shape", "[", "0", "]", "\n", "==", "action", ".", "shape", "[", "0", "]", "\n", "==", "reward", ".", "shape", "[", "0", "]", "\n", "==", "expected_reward", ".", "shape", "[", "0", "]", "\n", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"Expected `context.shape[0] == action.shape[0] == reward.shape[0] == expected_reward.shape[0]`\"", "\n", "\", but found it False\"", "\n", ")", "\n", "", "if", "not", "(", "\n", "np", ".", "issubdtype", "(", "action", ".", "dtype", ",", "np", ".", "integer", ")", "\n", "and", "action", ".", "min", "(", ")", ">=", "0", "\n", "and", "action", ".", "max", "(", ")", "<", "expected_reward", ".", "shape", "[", "1", "]", "\n", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"`action` elements must be integers in the range of [0, `expected_reward.shape[1]`)\"", "\n", ")", "\n", "", "", "else", ":", "\n", "        ", "if", "not", "(", "np", ".", "issubdtype", "(", "action", ".", "dtype", ",", "np", ".", "integer", ")", "and", "action", ".", "min", "(", ")", ">=", "0", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\"`action` elements must be non-negative integers\"", ")", "\n", "", "", "if", "pscore", "is", "not", "None", ":", "\n", "        ", "check_array", "(", "array", "=", "pscore", ",", "name", "=", "\"pscore\"", ",", "expected_dim", "=", "1", ")", "\n", "if", "not", "(", "\n", "context", ".", "shape", "[", "0", "]", "==", "action", ".", "shape", "[", "0", "]", "==", "reward", ".", "shape", "[", "0", "]", "==", "pscore", ".", "shape", "[", "0", "]", "\n", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"Expected `context.shape[0] == action.shape[0] == reward.shape[0] == pscore.shape[0]`\"", "\n", "\", but found it False\"", "\n", ")", "\n", "", "if", "np", ".", "any", "(", "pscore", "<=", "0", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\"`pscore` must be positive\"", ")", "\n", "\n", "", "", "if", "position", "is", "not", "None", ":", "\n", "        ", "check_array", "(", "array", "=", "position", ",", "name", "=", "\"position\"", ",", "expected_dim", "=", "1", ")", "\n", "if", "not", "(", "\n", "context", ".", "shape", "[", "0", "]", "==", "action", ".", "shape", "[", "0", "]", "==", "reward", ".", "shape", "[", "0", "]", "==", "position", ".", "shape", "[", "0", "]", "\n", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"Expected `context.shape[0] == action.shape[0] == reward.shape[0] == position.shape[0]`\"", "\n", "\", but found it False\"", "\n", ")", "\n", "", "if", "not", "(", "np", ".", "issubdtype", "(", "position", ".", "dtype", ",", "np", ".", "integer", ")", "and", "position", ".", "min", "(", ")", ">=", "0", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\"`position` elements must be non-negative integers\"", ")", "\n", "", "", "else", ":", "\n", "        ", "if", "not", "(", "context", ".", "shape", "[", "0", "]", "==", "action", ".", "shape", "[", "0", "]", "==", "reward", ".", "shape", "[", "0", "]", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"Expected `context.shape[0] == action.shape[0] == reward.shape[0]`\"", "\n", "\", but found it False\"", "\n", ")", "\n", "", "", "if", "action_context", "is", "not", "None", ":", "\n", "        ", "check_array", "(", "array", "=", "action_context", ",", "name", "=", "\"action_context\"", ",", "expected_dim", "=", "2", ")", "\n", "if", "not", "(", "\n", "np", ".", "issubdtype", "(", "action", ".", "dtype", ",", "np", ".", "integer", ")", "\n", "and", "action", ".", "min", "(", ")", ">=", "0", "\n", "and", "action", ".", "max", "(", ")", "<", "action_context", ".", "shape", "[", "0", "]", "\n", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"`action` elements must be integers in the range of [0, `action_context.shape[0]`)\"", "\n", ")", "\n", "", "", "else", ":", "\n", "        ", "if", "not", "(", "np", ".", "issubdtype", "(", "action", ".", "dtype", ",", "np", ".", "integer", ")", "and", "action", ".", "min", "(", ")", ">=", "0", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\"`action` elements must be non-negative integers\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_ope_inputs": [[313, 413], ["utils.check_array", "numpy.allclose", "ValueError", "utils.check_array", "numpy.any", "utils.check_array", "utils.check_array", "numpy.any", "action_dist.sum", "ValueError", "ValueError", "position.max", "ValueError", "ValueError", "ValueError", "ValueError", "ValueError", "ValueError", "ValueError", "ValueError", "ValueError", "ValueError", "numpy.issubdtype", "numpy.issubdtype", "position.min", "action.min", "action.max"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array"], ["", "", "", "def", "check_ope_inputs", "(", "\n", "action_dist", ":", "np", ".", "ndarray", ",", "\n", "position", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "action", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "reward", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "pscore", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "estimated_rewards_by_reg_model", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "estimated_importance_weights", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", ")", "->", "Optional", "[", "ValueError", "]", ":", "\n", "    ", "\"\"\"Check inputs for ope.\n\n    Parameters\n    -----------\n    action_dist: array-like, shape (n_rounds, n_actions, len_list)\n        Action choice probabilities of the evaluation policy (can be deterministic), i.e., :math:`\\\\pi_e(a_i|x_i)`.\n\n    position: array-like, shape (n_rounds,), default=None\n        Indices to differentiate positions in a recommendation interface where the actions are presented.\n\n    action: array-like, shape (n_rounds,), default=None\n        Actions sampled by the logging/behavior policy for each data in logged bandit data, i.e., :math:`a_i`.\n\n    reward: array-like, shape (n_rounds,), default=None\n        Rewards observed for each data in logged bandit data, i.e., :math:`r_i`.\n\n    pscore: array-like, shape (n_rounds,)\n        Action choice probabilities of the logging/behavior policy (propensity scores), i.e., :math:`\\\\pi_b(a_i|x_i)`.\n\n    estimated_rewards_by_reg_model: array-like, shape (n_rounds, n_actions, len_list), default=None\n        Estimated expected rewards given context, action, and position, i.e., :math:`\\\\hat{q}(x_i,a_i)`.\n\n    estimated_importance_weights: array-like, shape (n_rounds,), default=None\n        Importance weights estimated via supervised classification, i.e., :math:`\\\\hat{w}(x_t, a_t)`.\n\n    \"\"\"", "\n", "# action_dist", "\n", "check_array", "(", "array", "=", "action_dist", ",", "name", "=", "\"action_dist\"", ",", "expected_dim", "=", "3", ")", "\n", "if", "not", "np", ".", "allclose", "(", "action_dist", ".", "sum", "(", "axis", "=", "1", ")", ",", "1", ")", ":", "\n", "        ", "raise", "ValueError", "(", "\"`action_dist` must be a probability distribution\"", ")", "\n", "\n", "# position", "\n", "", "if", "position", "is", "not", "None", ":", "\n", "        ", "check_array", "(", "array", "=", "position", ",", "name", "=", "\"position\"", ",", "expected_dim", "=", "1", ")", "\n", "if", "not", "(", "position", ".", "shape", "[", "0", "]", "==", "action_dist", ".", "shape", "[", "0", "]", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"Expected `position.shape[0] == action_dist.shape[0]`, but found it False\"", "\n", ")", "\n", "", "if", "not", "(", "np", ".", "issubdtype", "(", "position", ".", "dtype", ",", "np", ".", "integer", ")", "and", "position", ".", "min", "(", ")", ">=", "0", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\"`position` elements must be non-negative integers\"", ")", "\n", "", "if", "position", ".", "max", "(", ")", ">=", "action_dist", ".", "shape", "[", "2", "]", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"`position` elements must be smaller than `action_dist.shape[2]`\"", "\n", ")", "\n", "", "", "elif", "action_dist", ".", "shape", "[", "2", "]", ">", "1", ":", "\n", "        ", "raise", "ValueError", "(", "\n", "\"`position` elements must be given when `action_dist.shape[2] > 1`\"", "\n", ")", "\n", "\n", "# estimated_rewards_by_reg_model", "\n", "", "if", "estimated_rewards_by_reg_model", "is", "not", "None", ":", "\n", "        ", "if", "estimated_rewards_by_reg_model", ".", "shape", "!=", "action_dist", ".", "shape", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"Expected `estimated_rewards_by_reg_model.shape == action_dist.shape`, but found it False\"", "\n", ")", "\n", "\n", "", "", "if", "estimated_importance_weights", "is", "not", "None", ":", "\n", "        ", "if", "not", "(", "action", ".", "shape", "[", "0", "]", "==", "estimated_importance_weights", ".", "shape", "[", "0", "]", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"Expected `action.shape[0] == estimated_importance_weights.shape[0]`, but found it False\"", "\n", ")", "\n", "", "if", "np", ".", "any", "(", "estimated_importance_weights", "<", "0", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\"estimated_importance_weights must be non-negative\"", ")", "\n", "\n", "# action, reward", "\n", "", "", "if", "action", "is", "not", "None", "or", "reward", "is", "not", "None", ":", "\n", "        ", "check_array", "(", "array", "=", "action", ",", "name", "=", "\"action\"", ",", "expected_dim", "=", "1", ")", "\n", "check_array", "(", "array", "=", "reward", ",", "name", "=", "\"reward\"", ",", "expected_dim", "=", "1", ")", "\n", "if", "not", "(", "action", ".", "shape", "[", "0", "]", "==", "reward", ".", "shape", "[", "0", "]", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"Expected `action.shape[0] == reward.shape[0]`, but found it False\"", "\n", ")", "\n", "", "if", "not", "(", "\n", "np", ".", "issubdtype", "(", "action", ".", "dtype", ",", "np", ".", "integer", ")", "\n", "and", "action", ".", "min", "(", ")", ">=", "0", "\n", "and", "action", ".", "max", "(", ")", "<", "action_dist", ".", "shape", "[", "1", "]", "\n", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"`action` elements must be integers in the range of [0, `action_dist.shape[1]`)\"", "\n", ")", "\n", "\n", "# pscore", "\n", "", "", "if", "pscore", "is", "not", "None", ":", "\n", "        ", "if", "pscore", ".", "ndim", "!=", "1", ":", "\n", "            ", "raise", "ValueError", "(", "\"pscore must be 1-dimensional\"", ")", "\n", "", "if", "not", "(", "action", ".", "shape", "[", "0", "]", "==", "reward", ".", "shape", "[", "0", "]", "==", "pscore", ".", "shape", "[", "0", "]", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"Expected `action.shape[0] == reward.shape[0] == pscore.shape[0]`, but found it False\"", "\n", ")", "\n", "", "if", "np", ".", "any", "(", "pscore", "<=", "0", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\"`pscore` must be positive\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_multi_loggers_ope_inputs": [[415, 470], ["utils.check_ope_inputs", "utils.check_array", "ValueError", "ValueError", "numpy.issubdtype", "stratum_idx.min"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_ope_inputs", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array"], ["", "", "", "def", "check_multi_loggers_ope_inputs", "(", "\n", "action_dist", ":", "np", ".", "ndarray", ",", "\n", "position", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "action", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "reward", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "stratum_idx", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "pscore", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "estimated_rewards_by_reg_model", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", ")", "->", "Optional", "[", "ValueError", "]", ":", "\n", "    ", "\"\"\"Check inputs for ope with multiple loggers.\n\n    Parameters\n    -----------\n    action_dist: array-like, shape (n_rounds, n_actions, len_list)\n        Action choice probabilities of the evaluation policy (can be deterministic), i.e., :math:`\\\\pi_e(a_i|x_i)`.\n\n    position: array-like, shape (n_rounds,), default=None\n        Indices to differentiate positions in a recommendation interface where the actions are presented.\n\n    action: array-like, shape (n_rounds,), default=None\n        Actions sampled by the logging/behavior policy for each data in logged bandit data, i.e., :math:`a_i`.\n\n    reward: array-like, shape (n_rounds,), default=None\n        Rewards observed for each data in logged bandit data, i.e., :math:`r_i`.\n\n    stratum_idx: array-like, shape (n_rounds,)\n        Indices to differentiate the logging/behavior policy that generate each data, i.e., :math:`k`.\n\n    pscore: array-like, shape (n_rounds,)\n        Action choice probabilities of the logging/behavior policy (propensity scores), i.e., :math:`\\\\pi_b(a_i|x_i)`.\n\n    estimated_rewards_by_reg_model: array-like, shape (n_rounds, n_actions, len_list), default=None\n        Estimated expected rewards given context, action, and position, i.e., :math:`\\\\hat{q}(x_i,a_i)`.\n\n    \"\"\"", "\n", "check_ope_inputs", "(", "\n", "action_dist", "=", "action_dist", ",", "\n", "position", "=", "position", ",", "\n", "action", "=", "action", ",", "\n", "reward", "=", "reward", ",", "\n", "pscore", "=", "pscore", ",", "\n", "estimated_rewards_by_reg_model", "=", "estimated_rewards_by_reg_model", ",", "\n", ")", "\n", "\n", "# stratum idx", "\n", "if", "stratum_idx", "is", "not", "None", ":", "\n", "        ", "check_array", "(", "array", "=", "stratum_idx", ",", "name", "=", "\"stratum_idx\"", ",", "expected_dim", "=", "1", ")", "\n", "if", "not", "(", "action_dist", ".", "shape", "[", "0", "]", "==", "stratum_idx", ".", "shape", "[", "0", "]", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"Expected `action_dist.shape[0] == stratum_idx.shape[0]`, but found it False\"", "\n", ")", "\n", "", "if", "not", "(", "\n", "np", ".", "issubdtype", "(", "stratum_idx", ".", "dtype", ",", "np", ".", "integer", ")", "and", "stratum_idx", ".", "min", "(", ")", ">=", "0", "\n", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\"`stratum_idx` elements must be non-negative integers\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_continuous_bandit_feedback_inputs": [[472, 534], ["utils.check_array", "utils.check_array", "utils.check_array", "utils.check_array", "utils.check_array", "numpy.any", "ValueError", "ValueError", "ValueError"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array"], ["", "", "", "def", "check_continuous_bandit_feedback_inputs", "(", "\n", "context", ":", "np", ".", "ndarray", ",", "\n", "action_by_behavior_policy", ":", "np", ".", "ndarray", ",", "\n", "reward", ":", "np", ".", "ndarray", ",", "\n", "expected_reward", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "pscore", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", ")", "->", "Optional", "[", "ValueError", "]", ":", "\n", "    ", "\"\"\"Check inputs for bandit learning or simulation with continuous actions.\n\n    Parameters\n    -----------\n    context: array-like, shape (n_rounds, dim_context)\n        Context vectors observed for each data, i.e., :math:`x_i`.\n\n    action_by_behavior_policy: array-like, shape (n_rounds,)\n        Continuous action values sampled by the logging/behavior policy for each data in logged bandit data, i.e., :math:`a_i`.\n\n    reward: array-like, shape (n_rounds,)\n        Rewards observed for each data in logged bandit data, i.e., :math:`r_i`.\n\n    expected_reward: array-like, shape (n_rounds, n_actions), default=None\n        Expected reward of each data, i.e., :math:`\\\\mathbb{E}[r_i|x_i,a_i]`.\n\n    pscore: array-like, shape (n_rounds,), default=None\n        Probability densities of the continuous action values sampled by the logging/behavior policy\n        (generalized propensity scores), i.e., :math:`\\\\pi_b(a_i|x_i)`.\n\n    \"\"\"", "\n", "check_array", "(", "array", "=", "context", ",", "name", "=", "\"context\"", ",", "expected_dim", "=", "2", ")", "\n", "check_array", "(", "\n", "array", "=", "action_by_behavior_policy", ",", "\n", "name", "=", "\"action_by_behavior_policy\"", ",", "\n", "expected_dim", "=", "1", ",", "\n", ")", "\n", "check_array", "(", "array", "=", "reward", ",", "name", "=", "\"reward\"", ",", "expected_dim", "=", "1", ")", "\n", "\n", "if", "expected_reward", "is", "not", "None", ":", "\n", "        ", "check_array", "(", "array", "=", "expected_reward", ",", "name", "=", "\"expected_reward\"", ",", "expected_dim", "=", "1", ")", "\n", "if", "not", "(", "\n", "context", ".", "shape", "[", "0", "]", "\n", "==", "action_by_behavior_policy", ".", "shape", "[", "0", "]", "\n", "==", "reward", ".", "shape", "[", "0", "]", "\n", "==", "expected_reward", ".", "shape", "[", "0", "]", "\n", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"Expected `context.shape[0] == action_by_behavior_policy.shape[0]\"", "\n", "\"== reward.shape[0] == expected_reward.shape[0]`, but found it False\"", "\n", ")", "\n", "", "", "if", "pscore", "is", "not", "None", ":", "\n", "        ", "check_array", "(", "array", "=", "pscore", ",", "name", "=", "\"pscore\"", ",", "expected_dim", "=", "1", ")", "\n", "if", "not", "(", "\n", "context", ".", "shape", "[", "0", "]", "\n", "==", "action_by_behavior_policy", ".", "shape", "[", "0", "]", "\n", "==", "reward", ".", "shape", "[", "0", "]", "\n", "==", "pscore", ".", "shape", "[", "0", "]", "\n", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"Expected `context.shape[0] == action_by_behavior_policy.shape[0]\"", "\n", "\"== reward.shape[0] == pscore.shape[0]`, but found it False\"", "\n", ")", "\n", "", "if", "np", ".", "any", "(", "pscore", "<=", "0", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\"`pscore` must be positive\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_continuous_ope_inputs": [[536, 620], ["utils.check_array", "utils.check_array", "utils.check_array", "utils.check_array", "utils.check_array", "numpy.any", "ValueError", "ValueError", "ValueError", "ValueError", "ValueError"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array"], ["", "", "", "def", "check_continuous_ope_inputs", "(", "\n", "action_by_evaluation_policy", ":", "np", ".", "ndarray", ",", "\n", "action_by_behavior_policy", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "reward", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "pscore", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "estimated_rewards_by_reg_model", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", ")", "->", "Optional", "[", "ValueError", "]", ":", "\n", "    ", "\"\"\"Check inputs for OPE with continuous actions.\n\n    Parameters\n    -----------\n    action_by_evaluation_policy: array-like, shape (n_rounds,)\n        Continuous action values given by the evaluation policy (can be deterministic), i.e., :math:`\\\\pi_e(x_t)`.\n\n    action_by_behavior_policy: array-like, shape (n_rounds,), default=None\n        Continuous action values sampled by the logging/behavior policy for each data in logged bandit data, i.e., :math:`a_i`.\n\n    reward: array-like, shape (n_rounds,), default=None\n        Rewards observed for each data in logged bandit data, i.e., :math:`r_i`.\n\n    pscore: array-like, shape (n_rounds,), default=None\n        Probability densities of the continuous action values sampled by the logging/behavior policy\n        (generalized propensity scores), i.e., :math:`\\\\pi_b(a_i|x_i)`.\n\n    estimated_rewards_by_reg_model: array-like, shape (n_rounds,), default=None\n        Expected rewards given context and action estimated by a regression model, i.e., :math:`\\\\hat{q}(x_i,a_i)`.\n\n    \"\"\"", "\n", "# action_by_evaluation_policy", "\n", "check_array", "(", "\n", "array", "=", "action_by_evaluation_policy", ",", "\n", "name", "=", "\"action_by_evaluation_policy\"", ",", "\n", "expected_dim", "=", "1", ",", "\n", ")", "\n", "\n", "# estimated_rewards_by_reg_model", "\n", "if", "estimated_rewards_by_reg_model", "is", "not", "None", ":", "\n", "        ", "check_array", "(", "\n", "array", "=", "estimated_rewards_by_reg_model", ",", "\n", "name", "=", "\"estimated_rewards_by_reg_model\"", ",", "\n", "expected_dim", "=", "1", ",", "\n", ")", "\n", "if", "(", "\n", "estimated_rewards_by_reg_model", ".", "shape", "[", "0", "]", "\n", "!=", "action_by_evaluation_policy", ".", "shape", "[", "0", "]", "\n", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"Expected `estimated_rewards_by_reg_model.shape[0] == action_by_evaluation_policy.shape[0]`\"", "\n", "\", but found if False\"", "\n", ")", "\n", "\n", "# action, reward", "\n", "", "", "if", "action_by_behavior_policy", "is", "not", "None", "or", "reward", "is", "not", "None", ":", "\n", "        ", "check_array", "(", "\n", "array", "=", "action_by_behavior_policy", ",", "\n", "name", "=", "\"action_by_behavior_policy\"", ",", "\n", "expected_dim", "=", "1", ",", "\n", ")", "\n", "check_array", "(", "array", "=", "reward", ",", "name", "=", "\"reward\"", ",", "expected_dim", "=", "1", ")", "\n", "if", "not", "(", "action_by_behavior_policy", ".", "shape", "[", "0", "]", "==", "reward", ".", "shape", "[", "0", "]", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"Expected `action_by_behavior_policy.shape[0] == reward.shape[0]`\"", "\n", "\", but found it False\"", "\n", ")", "\n", "", "if", "not", "(", "\n", "action_by_behavior_policy", ".", "shape", "[", "0", "]", "==", "action_by_evaluation_policy", ".", "shape", "[", "0", "]", "\n", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"Expected `action_by_behavior_policy.shape[0] == action_by_evaluation_policy.shape[0]`\"", "\n", "\", but found it False\"", "\n", ")", "\n", "\n", "# pscore", "\n", "", "", "if", "pscore", "is", "not", "None", ":", "\n", "        ", "check_array", "(", "array", "=", "pscore", ",", "name", "=", "\"pscore\"", ",", "expected_dim", "=", "1", ")", "\n", "if", "not", "(", "\n", "action_by_behavior_policy", ".", "shape", "[", "0", "]", "==", "reward", ".", "shape", "[", "0", "]", "==", "pscore", ".", "shape", "[", "0", "]", "\n", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"Expected `action_by_behavior_policy.shape[0] == reward.shape[0] == pscore.shape[0]`\"", "\n", "\", but found it False\"", "\n", ")", "\n", "", "if", "np", ".", "any", "(", "pscore", "<=", "0", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\"`pscore` must be positive\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils._check_slate_ope_inputs": [[622, 690], ["utils.check_array", "utils.check_array", "utils.check_array", "utils.check_array", "utils.check_array", "ValueError", "numpy.any", "numpy.any", "ValueError", "numpy.any", "numpy.any", "ValueError", "ValueError", "ValueError", "position.min", "slate_id.min"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array"], ["", "", "", "def", "_check_slate_ope_inputs", "(", "\n", "slate_id", ":", "np", ".", "ndarray", ",", "\n", "reward", ":", "np", ".", "ndarray", ",", "\n", "position", ":", "np", ".", "ndarray", ",", "\n", "pscore", ":", "np", ".", "ndarray", ",", "\n", "evaluation_policy_pscore", ":", "np", ".", "ndarray", ",", "\n", "pscore_type", ":", "str", ",", "\n", ")", "->", "Optional", "[", "ValueError", "]", ":", "\n", "    ", "\"\"\"Check inputs of Slate OPE estimators.\n\n    Parameters\n    -----------\n    slate_id: array-like, shape (<= n_rounds * len_list,)\n        Slate id observed for each data in logged bandit data.\n\n    reward: array-like, shape (<= n_rounds * len_list,)\n        Slot-level rewards, i.e., :math:`r_{i}(l)`.\n\n    position: array-like, shape (<= n_rounds * len_list,)\n        Indices to differentiate positions in a recommendation interface where the actions are presented.\n\n    pscore: array-like, shape (<= n_rounds * len_list,)\n        Action choice probabilities of the logging/behavior policy (propensity scores).\n\n    evaluation_policy_pscore: array-like, shape (<= n_rounds * len_list,)\n        Action choice probabilities of the evaluation policy.\n\n    pscore_type: str\n        Either \"pscore\", \"pscore_item_position\", or \"pscore_cascade\".\n\n    \"\"\"", "\n", "# position", "\n", "check_array", "(", "array", "=", "position", ",", "name", "=", "\"position\"", ",", "expected_dim", "=", "1", ")", "\n", "if", "not", "(", "position", ".", "dtype", "==", "int", "and", "position", ".", "min", "(", ")", ">=", "0", ")", ":", "\n", "        ", "raise", "ValueError", "(", "\"`position` elements must be non-negative integers\"", ")", "\n", "\n", "# reward", "\n", "", "check_array", "(", "array", "=", "reward", ",", "name", "=", "\"reward\"", ",", "expected_dim", "=", "1", ")", "\n", "\n", "# pscore", "\n", "check_array", "(", "array", "=", "pscore", ",", "name", "=", "f\"{pscore_type}\"", ",", "expected_dim", "=", "1", ")", "\n", "if", "np", ".", "any", "(", "pscore", "<=", "0", ")", "or", "np", ".", "any", "(", "pscore", ">", "1", ")", ":", "\n", "        ", "raise", "ValueError", "(", "f\"`{pscore_type}` must be in the range of (0, 1]\"", ")", "\n", "\n", "# evaluation_policy_pscore", "\n", "", "check_array", "(", "\n", "array", "=", "evaluation_policy_pscore", ",", "\n", "name", "=", "f\"evaluation_policy_{pscore_type}\"", ",", "\n", "expected_dim", "=", "1", ",", "\n", ")", "\n", "if", "np", ".", "any", "(", "evaluation_policy_pscore", "<", "0", ")", "or", "np", ".", "any", "(", "evaluation_policy_pscore", ">", "1", ")", ":", "\n", "        ", "raise", "ValueError", "(", "\n", "f\"`evaluation_policy_{pscore_type}` must be in the range of [0, 1]\"", "\n", ")", "\n", "\n", "# slate id", "\n", "", "check_array", "(", "array", "=", "slate_id", ",", "name", "=", "\"slate_id\"", ",", "expected_dim", "=", "1", ")", "\n", "if", "not", "(", "slate_id", ".", "dtype", "==", "int", "and", "slate_id", ".", "min", "(", ")", ">=", "0", ")", ":", "\n", "        ", "raise", "ValueError", "(", "\"slate_id elements must be non-negative integers\"", ")", "\n", "", "if", "not", "(", "\n", "slate_id", ".", "shape", "[", "0", "]", "\n", "==", "position", ".", "shape", "[", "0", "]", "\n", "==", "reward", ".", "shape", "[", "0", "]", "\n", "==", "pscore", ".", "shape", "[", "0", "]", "\n", "==", "evaluation_policy_pscore", ".", "shape", "[", "0", "]", "\n", ")", ":", "\n", "        ", "raise", "ValueError", "(", "\n", "f\"`slate_id`, `position`, `reward`, `{pscore_type}`, and `evaluation_policy_{pscore_type}` \"", "\n", "\"must have the same number of samples.\"", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_sips_inputs": [[694, 751], ["utils._check_slate_ope_inputs", "pandas.DataFrame", "pd.DataFrame.groupby().apply", "pd.DataFrame.groupby().apply", "pd.DataFrame.duplicated().sum", "ValueError", "ValueError", "ValueError", "pd.DataFrame.groupby", "pd.DataFrame.groupby", "pd.DataFrame.duplicated", "x[].unique", "x[].unique"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils._check_slate_ope_inputs"], ["", "", "def", "check_sips_inputs", "(", "\n", "slate_id", ":", "np", ".", "ndarray", ",", "\n", "reward", ":", "np", ".", "ndarray", ",", "\n", "position", ":", "np", ".", "ndarray", ",", "\n", "pscore", ":", "np", ".", "ndarray", ",", "\n", "evaluation_policy_pscore", ":", "np", ".", "ndarray", ",", "\n", ")", "->", "Optional", "[", "ValueError", "]", ":", "\n", "    ", "\"\"\"Check inputs of SlateStandardIPS.\n\n    Parameters\n    -----------\n    slate_id: array-like, shape (<= n_rounds * len_list,)\n        Slate id observed for each data in logged bandit data.\n\n    reward: array-like, shape (<= n_rounds * len_list,)\n        Slot-level rewards, i.e., :math:`r_{i}(l)`.\n\n    position: array-like, shape (<= n_rounds * len_list,)\n        Indices to differentiate positions in a recommendation interface where the actions are presented.\n\n    pscore: array-like, shape (<= n_rounds * len_list,)\n        Action choice probabilities of the logging/behavior policy (propensity scores), i.e., :math:`\\\\pi_b(a_i|x_i)`.\n\n    evaluation_policy_pscore: array-like, shape (<= n_rounds * len_list,)\n        Action choice probabilities of the evaluation policy, i.e., :math:`\\\\pi_e(a_i|x_i)`.\n\n    \"\"\"", "\n", "_check_slate_ope_inputs", "(", "\n", "slate_id", "=", "slate_id", ",", "\n", "reward", "=", "reward", ",", "\n", "position", "=", "position", ",", "\n", "pscore", "=", "pscore", ",", "\n", "evaluation_policy_pscore", "=", "evaluation_policy_pscore", ",", "\n", "pscore_type", "=", "\"pscore\"", ",", "\n", ")", "\n", "\n", "bandit_feedback_df", "=", "pd", ".", "DataFrame", "(", ")", "\n", "bandit_feedback_df", "[", "\"slate_id\"", "]", "=", "slate_id", "\n", "bandit_feedback_df", "[", "\"reward\"", "]", "=", "reward", "\n", "bandit_feedback_df", "[", "\"position\"", "]", "=", "position", "\n", "bandit_feedback_df", "[", "\"pscore\"", "]", "=", "pscore", "\n", "bandit_feedback_df", "[", "\"evaluation_policy_pscore\"", "]", "=", "evaluation_policy_pscore", "\n", "# check uniqueness", "\n", "if", "bandit_feedback_df", ".", "duplicated", "(", "[", "\"slate_id\"", ",", "\"position\"", "]", ")", ".", "sum", "(", ")", ">", "0", ":", "\n", "        ", "raise", "ValueError", "(", "\"`position` must not be duplicated in each slate\"", ")", "\n", "# check pscore uniqueness", "\n", "", "distinct_count_pscore_in_slate", "=", "bandit_feedback_df", ".", "groupby", "(", "\"slate_id\"", ")", ".", "apply", "(", "\n", "lambda", "x", ":", "x", "[", "\"pscore\"", "]", ".", "unique", "(", ")", ".", "shape", "[", "0", "]", "\n", ")", "\n", "if", "(", "distinct_count_pscore_in_slate", "!=", "1", ")", ".", "sum", "(", ")", ">", "0", ":", "\n", "        ", "raise", "ValueError", "(", "\"`pscore` must be unique in each slate\"", ")", "\n", "# check pscore uniqueness of evaluation policy", "\n", "", "distinct_count_evaluation_policy_pscore_in_slate", "=", "bandit_feedback_df", ".", "groupby", "(", "\n", "\"slate_id\"", "\n", ")", ".", "apply", "(", "lambda", "x", ":", "x", "[", "\"evaluation_policy_pscore\"", "]", ".", "unique", "(", ")", ".", "shape", "[", "0", "]", ")", "\n", "if", "(", "distinct_count_evaluation_policy_pscore_in_slate", "!=", "1", ")", ".", "sum", "(", ")", ">", "0", ":", "\n", "        ", "raise", "ValueError", "(", "\"`evaluation_policy_pscore` must be unique in each slate\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_iips_inputs": [[753, 795], ["utils._check_slate_ope_inputs", "pandas.DataFrame", "pd.DataFrame.duplicated().sum", "ValueError", "pd.DataFrame.duplicated"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils._check_slate_ope_inputs"], ["", "", "def", "check_iips_inputs", "(", "\n", "slate_id", ":", "np", ".", "ndarray", ",", "\n", "reward", ":", "np", ".", "ndarray", ",", "\n", "position", ":", "np", ".", "ndarray", ",", "\n", "pscore_item_position", ":", "np", ".", "ndarray", ",", "\n", "evaluation_policy_pscore_item_position", ":", "np", ".", "ndarray", ",", "\n", ")", "->", "Optional", "[", "ValueError", "]", ":", "\n", "    ", "\"\"\"Check inputs of SlateIndependentIPS.\n\n    Parameters\n    -----------\n    slate_id: array-like, shape (<= n_rounds * len_list,)\n        Slate id observed for each data in logged bandit data.\n\n    reward: array-like, shape (<= n_rounds * len_list,)\n        Slot-level rewards, i.e., :math:`r_{i}(l)`.\n\n    position: array-like, shape (<= n_rounds * len_list,)\n        Indices to differentiate positions in a recommendation interface where the actions are presented.\n\n    pscore_item_position: array-like, shape (<= n_rounds * len_list,)\n        Marginal action choice probabilities of the slot (:math:`l`) by a behavior policy (propensity scores), i.e., :math:`\\\\pi_b(a_{i}(l) |x_i)`.\n\n    evaluation_policy_pscore_item_position: array-like, shape (<= n_rounds * len_list,)\n        Marginal action choice probabilities of the slot (:math:`l`) by the evaluation policy, i.e., :math:`\\\\pi_e(a_{i}(l) |x_i)`.\n\n    \"\"\"", "\n", "_check_slate_ope_inputs", "(", "\n", "slate_id", "=", "slate_id", ",", "\n", "reward", "=", "reward", ",", "\n", "position", "=", "position", ",", "\n", "pscore", "=", "pscore_item_position", ",", "\n", "evaluation_policy_pscore", "=", "evaluation_policy_pscore_item_position", ",", "\n", "pscore_type", "=", "\"pscore_item_position\"", ",", "\n", ")", "\n", "\n", "bandit_feedback_df", "=", "pd", ".", "DataFrame", "(", ")", "\n", "bandit_feedback_df", "[", "\"slate_id\"", "]", "=", "slate_id", "\n", "bandit_feedback_df", "[", "\"position\"", "]", "=", "position", "\n", "# check uniqueness", "\n", "if", "bandit_feedback_df", ".", "duplicated", "(", "[", "\"slate_id\"", ",", "\"position\"", "]", ")", ".", "sum", "(", ")", ">", "0", ":", "\n", "        ", "raise", "ValueError", "(", "\"`position` must not be duplicated in each slate\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_rips_inputs": [[797, 876], ["utils._check_slate_ope_inputs", "pandas.DataFrame", "bandit_feedback_df.sort_values().reset_index().copy.sort_values().reset_index().copy", "bandit_feedback_df.sort_values().reset_index().copy.duplicated().sum", "ValueError", "[].expanding().min", "ValueError", "[].expanding().min", "ValueError", "bandit_feedback_df.sort_values().reset_index().copy.sort_values().reset_index", "bandit_feedback_df.sort_values().reset_index().copy.duplicated", "[].expanding", "[].expanding", "bandit_feedback_df.sort_values().reset_index().copy.sort_values", "bandit_feedback_df.sort_values().reset_index().copy.groupby", "bandit_feedback_df.sort_values().reset_index().copy.groupby"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils._check_slate_ope_inputs"], ["", "", "def", "check_rips_inputs", "(", "\n", "slate_id", ":", "np", ".", "ndarray", ",", "\n", "reward", ":", "np", ".", "ndarray", ",", "\n", "position", ":", "np", ".", "ndarray", ",", "\n", "pscore_cascade", ":", "np", ".", "ndarray", ",", "\n", "evaluation_policy_pscore_cascade", ":", "np", ".", "ndarray", ",", "\n", ")", "->", "Optional", "[", "ValueError", "]", ":", "\n", "    ", "\"\"\"Check inputs of SlateRewardInteractionIPS.\n\n    Parameters\n    -----------\n    slate_id: array-like, shape (<= n_rounds * len_list,)\n        Slate id observed for each data in logged bandit data.\n\n    reward: array-like, shape (<= n_rounds * len_list,)\n        Slot-level rewards, i.e., :math:`r_{i}(l)`.\n\n    position: array-like, shape (<= n_rounds * len_list,)\n        Indices to differentiate positions in a recommendation interface where the actions are presented.\n\n    pscore_cascade: array-like, shape (<= n_rounds * len_list,)\n        Action choice probabilities of the logging/behavior policy (propensity scores), i.e., :math:`\\\\pi_b(a_i|x_i)`.\n\n    evaluation_policy_pscore_cascade: array-like, shape (<= n_rounds * len_list,)\n        Action choice probabilities above the slot (:math:`l`) by the evaluation policy, i.e., :math:`\\\\pi_e(\\\\{a_{t, j}\\\\}_{j \\\\le k}|x_t)`.\n\n    \"\"\"", "\n", "_check_slate_ope_inputs", "(", "\n", "slate_id", "=", "slate_id", ",", "\n", "reward", "=", "reward", ",", "\n", "position", "=", "position", ",", "\n", "pscore", "=", "pscore_cascade", ",", "\n", "evaluation_policy_pscore", "=", "evaluation_policy_pscore_cascade", ",", "\n", "pscore_type", "=", "\"pscore_cascade\"", ",", "\n", ")", "\n", "\n", "bandit_feedback_df", "=", "pd", ".", "DataFrame", "(", ")", "\n", "bandit_feedback_df", "[", "\"slate_id\"", "]", "=", "slate_id", "\n", "bandit_feedback_df", "[", "\"reward\"", "]", "=", "reward", "\n", "bandit_feedback_df", "[", "\"position\"", "]", "=", "position", "\n", "bandit_feedback_df", "[", "\"pscore_cascade\"", "]", "=", "pscore_cascade", "\n", "bandit_feedback_df", "[", "\n", "\"evaluation_policy_pscore_cascade\"", "\n", "]", "=", "evaluation_policy_pscore_cascade", "\n", "# sort dataframe", "\n", "bandit_feedback_df", "=", "(", "\n", "bandit_feedback_df", ".", "sort_values", "(", "[", "\"slate_id\"", ",", "\"position\"", "]", ")", "\n", ".", "reset_index", "(", "drop", "=", "True", ")", "\n", ".", "copy", "(", ")", "\n", ")", "\n", "# check uniqueness", "\n", "if", "bandit_feedback_df", ".", "duplicated", "(", "[", "\"slate_id\"", ",", "\"position\"", "]", ")", ".", "sum", "(", ")", ">", "0", ":", "\n", "        ", "raise", "ValueError", "(", "\"`position` must not be duplicated in each slate\"", ")", "\n", "# check pscore_cascade structure", "\n", "", "previous_minimum_pscore_cascade", "=", "(", "\n", "bandit_feedback_df", ".", "groupby", "(", "\"slate_id\"", ")", "[", "\"pscore_cascade\"", "]", "\n", ".", "expanding", "(", ")", "\n", ".", "min", "(", ")", "\n", ".", "values", "\n", ")", "\n", "if", "(", "\n", "previous_minimum_pscore_cascade", "<", "bandit_feedback_df", "[", "\"pscore_cascade\"", "]", "\n", ")", ".", "sum", "(", ")", ">", "0", ":", "\n", "        ", "raise", "ValueError", "(", "\n", "\"`pscore_cascade` must be non-increasing sequence in each slate\"", "\n", ")", "\n", "# check pscore_cascade structure of evaluation policy", "\n", "", "previous_minimum_evaluation_policy_pscore_cascade", "=", "(", "\n", "bandit_feedback_df", ".", "groupby", "(", "\"slate_id\"", ")", "[", "\"evaluation_policy_pscore_cascade\"", "]", "\n", ".", "expanding", "(", ")", "\n", ".", "min", "(", ")", "\n", ".", "values", "\n", ")", "\n", "if", "(", "\n", "previous_minimum_evaluation_policy_pscore_cascade", "\n", "<", "bandit_feedback_df", "[", "\"evaluation_policy_pscore_cascade\"", "]", "\n", ")", ".", "sum", "(", ")", ">", "0", ":", "\n", "        ", "raise", "ValueError", "(", "\n", "\"`evaluation_policy_pscore_cascade` must be non-increasing sequence in each slate\"", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_cascade_dr_inputs": [[879, 973], ["utils.check_rips_inputs", "utils.check_array", "utils.check_array", "utils.check_array", "evaluation_policy_action_dist.reshape", "ValueError", "ValueError", "numpy.allclose", "ValueError", "numpy.issubdtype", "numpy.ones", "evaluation_policy_action_dist.reshape.sum", "action.min", "action.max"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_rips_inputs", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_array"], ["", "", "def", "check_cascade_dr_inputs", "(", "\n", "n_unique_action", ":", "int", ",", "\n", "slate_id", ":", "np", ".", "ndarray", ",", "\n", "action", ":", "np", ".", "ndarray", ",", "\n", "reward", ":", "np", ".", "ndarray", ",", "\n", "position", ":", "np", ".", "ndarray", ",", "\n", "pscore_cascade", ":", "np", ".", "ndarray", ",", "\n", "evaluation_policy_pscore_cascade", ":", "np", ".", "ndarray", ",", "\n", "q_hat", ":", "np", ".", "ndarray", ",", "\n", "evaluation_policy_action_dist", ":", "np", ".", "ndarray", ",", "\n", ")", "->", "Optional", "[", "ValueError", "]", ":", "\n", "    ", "\"\"\"Check inputs of SlateCascadeDoublyRobust.\n\n    Parameters\n    -----------\n    n_unique_action: int\n        Number of unique actions.\n\n    slate_id: array-like, shape (<= n_rounds * len_list,)\n        Indices to differentiate slates (i.e., ranking or list of actions)\n\n    action: array-like, (<= n_rounds * len_list,)\n        Actions observed at each slot in a ranking/slate in logged bandit data, i.e., :math:`a_{i}(l)`,\n        which is chosen by the behavior policy :math:`\\\\pi_b`.\n\n    reward: array-like, shape (<= n_rounds * len_list,)\n        Slot-level rewards observed for each data in logged bandit data, i.e., :math:`r_{i}(l)`.\n\n    position: array-like, shape (<= n_rounds * len_list,)\n        Indices to differentiate slots/positions in a slate/ranking.\n\n    pscore_cascade: array-like, shape (<= n_rounds * len_list,)\n        Probabilities of behavior policy selecting action :math:`a` at position (slot) `k` conditional on the previous actions (presented at position `1` to `k-1`)\n        , i.e., :math:`\\\\pi_b(a_i(l) | x_i, a_i(1), \\\\ldots, a_i(l-1))`.\n\n    evaluation_policy_pscore_cascade: array-like, shape (<= n_rounds * len_list,)\n        Probabilities of evaluation policy selecting action :math:`a` at position (slot) `k` conditional on the previous actions (presented at position `1` to `k-1`)\n        , i.e., :math:`\\\\pi_e(a_i(l) | x_i, a_i(1), \\\\ldots, a_i(l-1))`.\n\n    q_hat: array-like (<= n_rounds * len_list * n_unique_actions, )\n        :math:`\\\\hat{Q}_l` used in Cascade-DR.\n        , i.e., :math:`\\\\hat{Q}_{i,l}(x_i, a_i(1), \\\\ldots, a_i(l-1), a_i(l)) \\\\forall a_i(l) \\\\in \\\\mathcal{A}`.\n\n    evaluation_policy_action_dist: array-like (<= n_rounds * len_list * n_unique_actions, )\n        Action choice probabilities of the evaluation policy for all possible actions\n        , i.e., :math:`\\\\pi_e(a_i(l) | x_i, a_i(1), \\\\ldots, a_i(l-1)) \\\\forall a_i(l) \\\\in \\\\mathcal{A}`.\n\n    \"\"\"", "\n", "check_rips_inputs", "(", "\n", "slate_id", "=", "slate_id", ",", "\n", "reward", "=", "reward", ",", "\n", "position", "=", "position", ",", "\n", "pscore_cascade", "=", "pscore_cascade", ",", "\n", "evaluation_policy_pscore_cascade", "=", "evaluation_policy_pscore_cascade", ",", "\n", ")", "\n", "check_array", "(", "array", "=", "action", ",", "name", "=", "\"action\"", ",", "expected_dim", "=", "1", ")", "\n", "check_array", "(", "\n", "array", "=", "q_hat", ",", "\n", "name", "=", "\"q_hat\"", ",", "\n", "expected_dim", "=", "1", ",", "\n", ")", "\n", "check_array", "(", "\n", "array", "=", "evaluation_policy_action_dist", ",", "\n", "name", "=", "\"evaluation_policy_action_dist\"", ",", "\n", "expected_dim", "=", "1", ",", "\n", ")", "\n", "if", "not", "(", "\n", "np", ".", "issubdtype", "(", "action", ".", "dtype", ",", "np", ".", "integer", ")", "\n", "and", "action", ".", "min", "(", ")", ">=", "0", "\n", "and", "action", ".", "max", "(", ")", "<", "n_unique_action", "\n", ")", ":", "\n", "        ", "raise", "ValueError", "(", "\n", "\"`action` elements must be integers in the range of [0, n_unique_action)\"", "\n", ")", "\n", "", "if", "not", "(", "\n", "slate_id", ".", "shape", "[", "0", "]", "\n", "==", "action", ".", "shape", "[", "0", "]", "\n", "==", "q_hat", ".", "shape", "[", "0", "]", "//", "n_unique_action", "\n", "==", "evaluation_policy_action_dist", ".", "shape", "[", "0", "]", "//", "n_unique_action", "\n", ")", ":", "\n", "        ", "raise", "ValueError", "(", "\n", "\"Expected `slate_id.shape[0] == action.shape[0] == \"", "\n", "\"q_hat.shape[0] // n_unique_action == evaluation_policy_action_dist.shape[0] // n_unique_action`, \"", "\n", "\"but found it False\"", "\n", ")", "\n", "", "evaluation_policy_action_dist_", "=", "evaluation_policy_action_dist", ".", "reshape", "(", "\n", "(", "-", "1", ",", "n_unique_action", ")", "\n", ")", "\n", "if", "not", "np", ".", "allclose", "(", "\n", "np", ".", "ones", "(", "evaluation_policy_action_dist_", ".", "shape", "[", "0", "]", ")", ",", "\n", "evaluation_policy_action_dist_", ".", "sum", "(", "axis", "=", "1", ")", ",", "\n", ")", ":", "\n", "        ", "raise", "ValueError", "(", "\n", "\"`evaluation_policy_action_dist[i * n_unique_action : (i+1) * n_unique_action]` \"", "\n", "\"must sum up to one for all i.\"", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.sigmoid": [[977, 980], ["numpy.exp", "numpy.minimum", "numpy.exp", "numpy.abs"], "function", ["None"], ["", "", "def", "sigmoid", "(", "x", ":", "Union", "[", "float", ",", "np", ".", "ndarray", "]", ")", "->", "Union", "[", "float", ",", "np", ".", "ndarray", "]", ":", "\n", "    ", "\"\"\"Calculate sigmoid function.\"\"\"", "\n", "return", "np", ".", "exp", "(", "np", ".", "minimum", "(", "x", ",", "0", ")", ")", "/", "(", "1.0", "+", "np", ".", "exp", "(", "-", "np", ".", "abs", "(", "x", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.softmax": [[982, 988], ["numpy.exp", "numpy.max", "numpy.sum"], "function", ["None"], ["", "def", "softmax", "(", "x", ":", "Union", "[", "float", ",", "np", ".", "ndarray", "]", ")", "->", "Union", "[", "float", ",", "np", ".", "ndarray", "]", ":", "\n", "    ", "\"\"\"Calculate softmax function.\"\"\"", "\n", "b", "=", "np", ".", "max", "(", "x", ",", "axis", "=", "1", ")", "[", ":", ",", "np", ".", "newaxis", "]", "\n", "numerator", "=", "np", ".", "exp", "(", "x", "-", "b", ")", "\n", "denominator", "=", "np", ".", "sum", "(", "numerator", ",", "axis", "=", "1", ")", "[", ":", ",", "np", ".", "newaxis", "]", "\n", "return", "numerator", "/", "denominator", "\n", "", ""]], "home.repos.pwc.inspect_result.st-tech_zr-obp.simulator.simulator.run_bandit_simulation": [[24, 95], ["utils.check_bandit_feedback_inputs", "list", "tqdm.tqdm", "utils.convert_to_action_dist", "numpy.zeros_like", "zip", "list.append", "RuntimeError", "policy_.select_action", "numpy.array", "policy_.select_action", "policy_.update_params", "bandit_feedback[].max", "context_.reshape", "policy_.update_params", "context_.reshape"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_bandit_feedback_inputs", "home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.convert_to_action_dist", "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.linear.LinTS.select_action", "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.linear.LinTS.select_action", "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.linear.BaseLinPolicy.update_params", "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.linear.BaseLinPolicy.update_params"], ["def", "run_bandit_simulation", "(", "\n", "bandit_feedback", ":", "BanditFeedback", ",", "policy", ":", "BanditPolicy", "\n", ")", "->", "np", ".", "ndarray", ":", "\n", "    ", "\"\"\"Run an online bandit algorithm on the given logged bandit feedback data.\n\n    Parameters\n    ----------\n    bandit_feedback: BanditFeedback\n        Logged bandit data used in offline bandit simulation.\n\n    policy: BanditPolicy\n        Online bandit policy to be evaluated in offline bandit simulation (i.e., evaluation policy).\n\n    Returns\n    --------\n    action_dist: array-like, shape (n_rounds, n_actions, len_list)\n        Action choice probabilities (can be deterministic).\n\n    \"\"\"", "\n", "for", "key_", "in", "[", "\"action\"", ",", "\"position\"", ",", "\"reward\"", ",", "\"pscore\"", ",", "\"context\"", "]", ":", "\n", "        ", "if", "key_", "not", "in", "bandit_feedback", ":", "\n", "            ", "raise", "RuntimeError", "(", "f\"Missing key of {key_} in 'bandit_feedback'.\"", ")", "\n", "", "", "check_bandit_feedback_inputs", "(", "\n", "context", "=", "bandit_feedback", "[", "\"context\"", "]", ",", "\n", "action", "=", "bandit_feedback", "[", "\"action\"", "]", ",", "\n", "reward", "=", "bandit_feedback", "[", "\"reward\"", "]", ",", "\n", "position", "=", "bandit_feedback", "[", "\"position\"", "]", ",", "\n", "pscore", "=", "bandit_feedback", "[", "\"pscore\"", "]", ",", "\n", ")", "\n", "\n", "policy_", "=", "policy", "\n", "selected_actions_list", "=", "list", "(", ")", "\n", "dim_context", "=", "bandit_feedback", "[", "\"context\"", "]", ".", "shape", "[", "1", "]", "\n", "if", "bandit_feedback", "[", "\"position\"", "]", "is", "None", ":", "\n", "        ", "bandit_feedback", "[", "\"position\"", "]", "=", "np", ".", "zeros_like", "(", "\n", "bandit_feedback", "[", "\"action\"", "]", ",", "dtype", "=", "int", "\n", ")", "\n", "", "for", "action_", ",", "reward_", ",", "position_", ",", "context_", "in", "tqdm", "(", "\n", "zip", "(", "\n", "bandit_feedback", "[", "\"action\"", "]", ",", "\n", "bandit_feedback", "[", "\"reward\"", "]", ",", "\n", "bandit_feedback", "[", "\"position\"", "]", ",", "\n", "bandit_feedback", "[", "\"context\"", "]", ",", "\n", ")", ",", "\n", "total", "=", "bandit_feedback", "[", "\"n_rounds\"", "]", ",", "\n", ")", ":", "\n", "\n", "# select a list of actions", "\n", "        ", "if", "policy_", ".", "policy_type", "==", "PolicyType", ".", "CONTEXT_FREE", ":", "\n", "            ", "selected_actions", "=", "policy_", ".", "select_action", "(", ")", "\n", "", "elif", "policy_", ".", "policy_type", "==", "PolicyType", ".", "CONTEXTUAL", ":", "\n", "            ", "selected_actions", "=", "policy_", ".", "select_action", "(", "context_", ".", "reshape", "(", "1", ",", "dim_context", ")", ")", "\n", "", "action_match_", "=", "action_", "==", "selected_actions", "[", "position_", "]", "\n", "# update parameters of a bandit policy", "\n", "# only when selected actions&positions are equal to logged actions&positions", "\n", "if", "action_match_", ":", "\n", "            ", "if", "policy_", ".", "policy_type", "==", "PolicyType", ".", "CONTEXT_FREE", ":", "\n", "                ", "policy_", ".", "update_params", "(", "action", "=", "action_", ",", "reward", "=", "reward_", ")", "\n", "", "elif", "policy_", ".", "policy_type", "==", "PolicyType", ".", "CONTEXTUAL", ":", "\n", "                ", "policy_", ".", "update_params", "(", "\n", "action", "=", "action_", ",", "\n", "reward", "=", "reward_", ",", "\n", "context", "=", "context_", ".", "reshape", "(", "1", ",", "dim_context", ")", ",", "\n", ")", "\n", "", "", "selected_actions_list", ".", "append", "(", "selected_actions", ")", "\n", "\n", "", "action_dist", "=", "convert_to_action_dist", "(", "\n", "n_actions", "=", "bandit_feedback", "[", "\"action\"", "]", ".", "max", "(", ")", "+", "1", ",", "\n", "selected_actions", "=", "np", ".", "array", "(", "selected_actions_list", ")", ",", "\n", ")", "\n", "return", "action_dist", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.simulator.simulator.calc_ground_truth_policy_value": [[97, 178], ["utils.check_bandit_feedback_inputs", "tqdm.tqdm", "numpy.arange", "copy.deepcopy", "zip", "RuntimeError", "reward_sampler", "copy.deepcopy.select_action", "context_.reshape", "numpy.array", "copy.deepcopy.update_params", "copy.deepcopy.select_action", "copy.deepcopy.update_params", "context_.reshape", "context_.reshape"], "function", ["home.repos.pwc.inspect_result.st-tech_zr-obp.obp.utils.check_bandit_feedback_inputs", "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.linear.LinTS.select_action", "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.linear.BaseLinPolicy.update_params", "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.linear.LinTS.select_action", "home.repos.pwc.inspect_result.st-tech_zr-obp.policy.linear.BaseLinPolicy.update_params"], ["", "def", "calc_ground_truth_policy_value", "(", "\n", "bandit_feedback", ":", "BanditFeedback", ",", "\n", "reward_sampler", ":", "Callable", "[", "[", "np", ".", "ndarray", ",", "np", ".", "ndarray", "]", ",", "float", "]", ",", "\n", "policy", ":", "BanditPolicy", ",", "\n", "n_sim", ":", "int", "=", "100", ",", "\n", ")", "->", "float", ":", "\n", "    ", "\"\"\"Calculate the ground-truth policy value of a given online bandit algorithm by Monte-Carlo Simulation.\n\n    Parameters\n    ----------\n    bandit_feedback: BanditFeedback\n        Logged bandit data used in offline bandit simulation. Must contain \"expected_rewards\" as a key.\n\n    reward_sampler: Callable[[np.ndarray, np.ndarray], np.ndarray]\n        Function sampling reward for each given action-context pair, i.e., :math:`p(r|x, a)`.\n\n    policy: BanditPolicy\n        Online bandit policy to be evaluated in offline bandit simulation (i.e., evaluation policy).\n\n    n_sim: int, default=100\n        Number of simulations in the Monte Carlo simulation to compute the policy value.\n\n    Returns\n    --------\n    ground_truth_policy_value: float\n        policy value of a given evaluation policy.\n\n    \"\"\"", "\n", "for", "key_", "in", "[", "\n", "\"action\"", ",", "\n", "\"position\"", ",", "\n", "\"reward\"", ",", "\n", "\"expected_reward\"", ",", "\n", "\"context\"", ",", "\n", "]", ":", "\n", "        ", "if", "key_", "not", "in", "bandit_feedback", ":", "\n", "            ", "raise", "RuntimeError", "(", "f\"Missing key of {key_} in 'bandit_feedback'.\"", ")", "\n", "", "", "check_bandit_feedback_inputs", "(", "\n", "context", "=", "bandit_feedback", "[", "\"context\"", "]", ",", "\n", "action", "=", "bandit_feedback", "[", "\"action\"", "]", ",", "\n", "reward", "=", "bandit_feedback", "[", "\"reward\"", "]", ",", "\n", "expected_reward", "=", "bandit_feedback", "[", "\"expected_reward\"", "]", ",", "\n", "position", "=", "bandit_feedback", "[", "\"position\"", "]", ",", "\n", ")", "\n", "\n", "cumulative_reward", "=", "0.0", "\n", "dim_context", "=", "bandit_feedback", "[", "\"context\"", "]", ".", "shape", "[", "1", "]", "\n", "\n", "for", "_", "in", "tqdm", "(", "np", ".", "arange", "(", "n_sim", ")", ",", "total", "=", "n_sim", ")", ":", "\n", "        ", "policy_", "=", "deepcopy", "(", "policy", ")", "\n", "for", "position_", ",", "context_", ",", "expected_reward_", "in", "zip", "(", "\n", "bandit_feedback", "[", "\"position\"", "]", ",", "\n", "bandit_feedback", "[", "\"context\"", "]", ",", "\n", "bandit_feedback", "[", "\"expected_reward\"", "]", ",", "\n", ")", ":", "\n", "\n", "# select a list of actions", "\n", "            ", "if", "policy_", ".", "policy_type", "==", "PolicyType", ".", "CONTEXT_FREE", ":", "\n", "                ", "selected_actions", "=", "policy_", ".", "select_action", "(", ")", "\n", "", "elif", "policy_", ".", "policy_type", "==", "PolicyType", ".", "CONTEXTUAL", ":", "\n", "                ", "selected_actions", "=", "policy_", ".", "select_action", "(", "\n", "context_", ".", "reshape", "(", "1", ",", "dim_context", ")", "\n", ")", "\n", "", "action", "=", "selected_actions", "[", "position_", "]", "\n", "# sample reward", "\n", "reward", "=", "reward_sampler", "(", "\n", "context_", ".", "reshape", "(", "1", ",", "dim_context", ")", ",", "np", ".", "array", "(", "[", "action", "]", ")", "\n", ")", "\n", "cumulative_reward", "+=", "expected_reward_", "[", "action", "]", "\n", "\n", "# update parameters of a bandit policy", "\n", "if", "policy_", ".", "policy_type", "==", "PolicyType", ".", "CONTEXT_FREE", ":", "\n", "                ", "policy_", ".", "update_params", "(", "action", "=", "action", ",", "reward", "=", "reward", ")", "\n", "", "elif", "policy_", ".", "policy_type", "==", "PolicyType", ".", "CONTEXTUAL", ":", "\n", "                ", "policy_", ".", "update_params", "(", "\n", "action", "=", "action", ",", "\n", "reward", "=", "reward", ",", "\n", "context", "=", "context_", ".", "reshape", "(", "1", ",", "dim_context", ")", ",", "\n", ")", "\n", "\n", "", "", "", "return", "cumulative_reward", "/", "(", "n_sim", "*", "bandit_feedback", "[", "\"n_rounds\"", "]", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.st-tech_zr-obp.cf_policy_search.custom_dataset.OBDWithInteractionFeatures.pre_process": [[14, 20], ["super().pre_process", "custom_dataset.OBDWithInteractionFeatures._pre_process_context_set_2"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.cf_policy_search.custom_dataset.OBDWithInteractionFeatures.pre_process", "home.repos.pwc.inspect_result.st-tech_zr-obp.cf_policy_search.custom_dataset.OBDWithInteractionFeatures._pre_process_context_set_2"], ["def", "pre_process", "(", "self", ")", "->", "None", ":", "\n", "\n", "        ", "if", "self", ".", "context_set", "==", "\"1\"", ":", "\n", "            ", "super", "(", ")", ".", "pre_process", "(", ")", "\n", "", "elif", "self", ".", "context_set", "==", "\"2\"", ":", "\n", "            ", "self", ".", "_pre_process_context_set_2", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.cf_policy_search.custom_dataset.OBDWithInteractionFeatures._pre_process_context_set_1": [[21, 27], ["custom_dataset.OBDWithInteractionFeatures.data.columns.str.contains", "pandas.get_dummies"], "methods", ["None"], ["", "", "def", "_pre_process_context_set_1", "(", "self", ")", "->", "None", ":", "\n", "        ", "\"\"\"Create Context Set 1 (c.f., Section 5.2)\"\"\"", "\n", "\n", "user_cols", "=", "self", ".", "data", ".", "columns", ".", "str", ".", "contains", "(", "\"user_feature\"", ")", "\n", "self", ".", "context", "=", "pd", ".", "get_dummies", "(", "\n", "self", ".", "data", ".", "loc", "[", ":", ",", "user_cols", "]", ",", "drop_first", "=", "True", "\n", ")", ".", "values", "\n"]], "home.repos.pwc.inspect_result.st-tech_zr-obp.cf_policy_search.custom_dataset.OBDWithInteractionFeatures._pre_process_context_set_2": [[29, 37], ["super().pre_process", "custom_dataset.OBDWithInteractionFeatures.data.columns.str.contains", "sklearn.decomposition.PCA().fit_transform", "sklearn.decomposition.PCA"], "methods", ["home.repos.pwc.inspect_result.st-tech_zr-obp.cf_policy_search.custom_dataset.OBDWithInteractionFeatures.pre_process"], ["", "def", "_pre_process_context_set_2", "(", "self", ")", "->", "None", ":", "\n", "        ", "\"\"\"Create Context Set 2 (c.f., Section 5.2)\"\"\"", "\n", "\n", "super", "(", ")", ".", "pre_process", "(", ")", "\n", "affinity_cols", "=", "self", ".", "data", ".", "columns", ".", "str", ".", "contains", "(", "\"affinity\"", ")", "\n", "Xaffinity", "=", "self", ".", "data", ".", "loc", "[", ":", ",", "affinity_cols", "]", ".", "values", "\n", "self", ".", "context", "=", "PCA", "(", "n_components", "=", "30", ")", ".", "fit_transform", "(", "\n", "np", ".", "c_", "[", "self", ".", "context", ",", "Xaffinity", "]", "\n", ")", "\n"]]}