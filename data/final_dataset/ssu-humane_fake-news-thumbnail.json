{"home.repos.pwc.inspect_result.ssu-humane_fake-news-thumbnail.None.train.tuplify_with_device": [[15, 18], ["tuple", "batch[].to", "batch[].to", "batch[].to", "batch[].to"], "function", ["None"], ["def", "tuplify_with_device", "(", "batch", ",", "device", ")", ":", "\n", "    ", "return", "tuple", "(", "[", "batch", "[", "0", "]", ".", "to", "(", "device", ",", "dtype", "=", "torch", ".", "long", ")", ",", "batch", "[", "1", "]", ".", "to", "(", "device", ",", "dtype", "=", "torch", ".", "long", ")", ",", "\n", "batch", "[", "2", "]", ".", "to", "(", "device", ",", "dtype", "=", "torch", ".", "float", ")", ",", "batch", "[", "3", "]", ".", "to", "(", "device", ",", "dtype", "=", "torch", ".", "float", ")", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ssu-humane_fake-news-thumbnail.None.train.compute_accuracy": [[19, 23], ["sklearn.metrics.accuracy_score", "sklearn.metrics.roc_auc_score", "y_pred.round"], "function", ["None"], ["", "def", "compute_accuracy", "(", "y_pred", ",", "y_target", ")", ":", "\n", "    ", "correct", "=", "accuracy_score", "(", "y_target", ",", "y_pred", ".", "round", "(", ")", ")", "\n", "def_roc", "=", "roc_auc_score", "(", "y_target", ",", "y_pred", ")", "\n", "return", "correct", ",", "def_roc", "\n", "\n"]], "home.repos.pwc.inspect_result.ssu-humane_fake-news-thumbnail.None.train.set_seed": [[24, 31], ["torch.cuda.device_count", "torch.cuda.device_count", "torch.cuda.device_count", "random.seed", "numpy.random.seed", "torch.manual_seed", "torch.manual_seed", "torch.manual_seed", "torch.cuda.manual_seed_all", "torch.cuda.manual_seed_all", "torch.cuda.manual_seed_all"], "function", ["None"], ["", "def", "set_seed", "(", "seed", ")", ":", "\n", "    ", "n_gpu", "=", "torch", ".", "cuda", ".", "device_count", "(", ")", "\n", "random", ".", "seed", "(", "seed", ")", "\n", "np", ".", "random", ".", "seed", "(", "seed", ")", "\n", "torch", ".", "manual_seed", "(", "seed", ")", "\n", "if", "n_gpu", ">", "0", ":", "\n", "        ", "torch", ".", "cuda", ".", "manual_seed_all", "(", "seed", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ssu-humane_fake-news-thumbnail.None.train.main": [[33, 162], ["torch.device", "torch.device", "torch.device", "torch.cuda.empty_cache", "torch.cuda.empty_cache", "torch.cuda.empty_cache", "train.set_seed", "model.ClassificationModel", "model.ClassificationModel.to", "model.ClassificationModel.clip_freeze", "pandas.read_json", "pandas.read_json", "datasets.Dataset", "torch.utils.data.DataLoader", "datasets.Dataset", "torch.utils.data.DataLoader", "torch.BCEWithLogitsLoss", "torch.AdamW", "torch.lr_scheduler.ReduceLROnPlateau", "range", "print", "print", "print", "torch.save", "torch.save", "torch.save", "model.ClassificationModel.parameters", "print", "print", "model.ClassificationModel.train", "enumerate", "print", "model.ClassificationModel.eval", "enumerate", "optim.lr_scheduler.ReduceLROnPlateau.step", "numpy.concatenate().reshape", "numpy.concatenate().reshape().astype", "train.compute_accuracy", "print", "print", "print", "print", "open", "json.dump", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "tqdm.auto.tqdm", "train.tuplify_with_device", "optim.AdamW.zero_grad", "model.ClassificationModel.", "nn.BCEWithLogitsLoss.", "loss_func.item", "train_loss_set.append", "loss_func.backward", "torch.utils.clip_grad_norm_", "optim.AdamW.step", "train.tuplify_with_device", "nn.BCEWithLogitsLoss.", "loss_func.item", "val_loss_set.append", "torch.sigmoid.detach().cpu().numpy", "b_labels.to().numpy", "np.concatenate().reshape.append", "np.concatenate().reshape().astype.append", "model.ClassificationModel.state_dict", "torch.sigmoid.view", "b_labels.view", "model.ClassificationModel.parameters", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid.view", "b_labels.view", "numpy.concatenate", "numpy.concatenate().reshape", "model.ClassificationModel.", "torch.sigmoid.detach().cpu", "b_labels.to", "numpy.concatenate", "torch.sigmoid.detach"], "function", ["home.repos.pwc.inspect_result.ssu-humane_fake-news-thumbnail.None.train.set_seed", "home.repos.pwc.inspect_result.ssu-humane_fake-news-thumbnail.None.model.ClassificationModel.clip_freeze", "home.repos.pwc.inspect_result.ssu-humane_fake-news-thumbnail.None.train.compute_accuracy", "home.repos.pwc.inspect_result.ssu-humane_fake-news-thumbnail.None.train.tuplify_with_device", "home.repos.pwc.inspect_result.ssu-humane_fake-news-thumbnail.None.train.tuplify_with_device"], ["", "", "def", "main", "(", "args", ",", "train_loss_set", ",", "val_loss_set", ")", ":", "\n", "    ", "train_loss_set", "=", "[", "]", "\n", "argument_path", "=", "f'{args.save}/model{args.sa_num}.json'", "\n", "argument", "=", "{", "'seed'", ":", "args", ".", "seed", ",", "'learning_rate'", ":", "args", ".", "learning_rate", ",", "'batch_size'", ":", "args", ".", "batch_size", ",", "'epoch'", ":", "args", ".", "num_epochs", ",", "'max_grad_norm'", ":", "args", ".", "max_grad_norm", ",", "'factor'", ":", "args", ".", "factor", ",", "'mode'", ":", "args", ".", "sched_mode", ",", "'patience'", ":", "args", ".", "patience", "}", "\n", "\n", "device", "=", "torch", ".", "device", "(", "\"cuda\"", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "else", "\"cpu\"", ")", "\n", "torch", ".", "cuda", ".", "empty_cache", "(", ")", "\n", "set_seed", "(", "args", ".", "seed", ")", "\n", "model", "=", "ClassificationModel", "(", ")", "\n", "model", ".", "to", "(", "device", ")", "\n", "\n", "#clip model freeze    ", "\n", "model", ".", "clip_freeze", "(", ")", "# clip laeyr weight all freeze", "\n", "\n", "#Data load", "\n", "train", "=", "pd", ".", "read_json", "(", "f\"{args.train_path}/train.json\"", ")", "\n", "val", "=", "pd", ".", "read_json", "(", "f\"{args.val_path}/val.json\"", ")", "\n", "\n", "#train data loader", "\n", "train_data", "=", "Dataset", "(", "train", ",", "args", ".", "image_path", ")", "\n", "train_dataloader", "=", "DataLoader", "(", "train_data", ",", "batch_size", "=", "args", ".", "batch_size", ")", "\n", "\n", "#val data loader", "\n", "val_data", "=", "Dataset", "(", "val", ",", "args", ".", "image_path", ")", "\n", "val_dataloader", "=", "DataLoader", "(", "val_data", ",", "batch_size", "=", "args", ".", "batch_size", ")", "\n", "\n", "\n", "loss_func", "=", "nn", ".", "BCEWithLogitsLoss", "(", ")", "\n", "\n", "optimizer", "=", "optim", ".", "AdamW", "(", "model", ".", "parameters", "(", ")", ",", "lr", "=", "args", ".", "learning_rate", ")", "\n", "scheduler", "=", "optim", ".", "lr_scheduler", ".", "ReduceLROnPlateau", "(", "optimizer", "=", "optimizer", ",", "\n", "mode", "=", "args", ".", "sched_mode", ",", "factor", "=", "args", ".", "factor", ",", "\n", "patience", "=", "args", ".", "patience", ",", "verbose", "=", "True", ")", "\n", "\n", "best_ep", "=", "0", "\n", "best_model_state_on_dev", "=", "None", "\n", "best_dev_acc", "=", "0.0", "\n", "best_dev_auroc", "=", "0.0", "\n", "\n", "for", "e_idx", "in", "range", "(", "1", ",", "args", ".", "num_epochs", "+", "1", ")", ":", "\n", "        ", "print", "(", "'Epoch {}/{}'", ".", "format", "(", "e_idx", ",", "args", ".", "num_epochs", ")", ")", "\n", "print", "(", "'----------------------'", ")", "\n", "\n", "tr_loss", "=", "0.0", "\n", "nb_tr_steps", "=", "0", "\n", "\n", "model", ".", "train", "(", ")", "\n", "# train", "\n", "for", "batch_index", ",", "batch", "in", "enumerate", "(", "tqdm", "(", "train_dataloader", ")", ")", ":", "\n", "            ", "batch", "=", "tuplify_with_device", "(", "batch", ",", "device", ")", "\n", "\n", "b_input_ids", ",", "b_attention_mask", ",", "b_pixel_values", ",", "b_labels", "=", "batch", "\n", "\n", "optimizer", ".", "zero_grad", "(", ")", "\n", "\n", "y_pred", "=", "model", "(", "input_ids", "=", "b_input_ids", ",", "attention_mask", "=", "b_attention_mask", ",", "\n", "pixel_values", "=", "b_pixel_values", ")", "\n", "loss", "=", "loss_func", "(", "y_pred", ".", "view", "(", "-", "1", ",", "1", ")", ",", "b_labels", ".", "view", "(", "-", "1", ",", "1", ")", ")", "\n", "loss_item", "=", "loss", ".", "item", "(", ")", "\n", "train_loss_set", ".", "append", "(", "loss_item", ")", "\n", "\n", "\n", "loss", ".", "backward", "(", ")", "\n", "nn", ".", "utils", ".", "clip_grad_norm_", "(", "model", ".", "parameters", "(", ")", ",", "args", ".", "max_grad_norm", ")", "\n", "optimizer", ".", "step", "(", ")", "\n", "\n", "\n", "tr_loss", "+=", "loss_item", "\n", "nb_tr_steps", "+=", "1", "\n", "\n", "", "print", "(", "\"train loss: {:.4f}\"", ".", "format", "(", "tr_loss", "/", "nb_tr_steps", ")", ")", "\n", "\n", "val_loss", "=", "0.0", "\n", "nb_val_steps", "=", "0", "\n", "# validation", "\n", "dev_y_preds", ",", "dev_y_targets", "=", "[", "]", ",", "[", "]", "\n", "model", ".", "eval", "(", ")", "\n", "for", "batch_index", ",", "batch", "in", "enumerate", "(", "val_dataloader", ")", ":", "\n", "            ", "batch", "=", "tuplify_with_device", "(", "batch", ",", "device", ")", "\n", "b_input_ids", ",", "b_attention_mask", ",", "b_pixel_values", ",", "b_labels", "=", "batch", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                ", "y_pred", "=", "torch", ".", "sigmoid", "(", "model", "(", "input_ids", "=", "b_input_ids", ",", "attention_mask", "=", "b_attention_mask", ",", "pixel_values", "=", "b_pixel_values", ")", ")", "\n", "\n", "", "val_loss", "=", "loss_func", "(", "y_pred", ".", "view", "(", "-", "1", ",", "1", ")", ",", "b_labels", ".", "view", "(", "-", "1", ",", "1", ")", ")", "\n", "val_loss_item", "=", "val_loss", ".", "item", "(", ")", "\n", "val_loss_set", ".", "append", "(", "val_loss_item", ")", "\n", "\n", "y_pred", "=", "y_pred", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "label_ids", "=", "b_labels", ".", "to", "(", "'cpu'", ")", ".", "numpy", "(", ")", "\n", "\n", "dev_y_preds", ".", "append", "(", "y_pred", ")", "\n", "dev_y_targets", ".", "append", "(", "label_ids", ")", "\n", "\n", "val_loss", "+=", "val_loss_item", "\n", "nb_val_steps", "+=", "1", "\n", "\n", "", "scheduler", ".", "step", "(", "loss", ")", "\n", "\n", "dev_y_preds", "=", "np", ".", "concatenate", "(", "dev_y_preds", ")", ".", "reshape", "(", "(", "-", "1", ",", ")", ")", "\n", "dev_y_targets", "=", "np", ".", "concatenate", "(", "dev_y_targets", ")", ".", "reshape", "(", "(", "-", "1", ",", ")", ")", ".", "astype", "(", "int", ")", "\n", "dev_acc", ",", "dev_auroc", "=", "compute_accuracy", "(", "dev_y_preds", ",", "dev_y_targets", ")", "\n", "\n", "if", "dev_acc", ">", "best_dev_acc", ":", "\n", "            ", "best_ep", "=", "e_idx", "\n", "best_dev_acc", "=", "dev_acc", "\n", "best_dev_auroc", "=", "dev_auroc", "\n", "best_model_state_on_dev", "=", "model", ".", "state_dict", "(", ")", "\n", "\n", "", "print", "(", "\"val loss: {:.4f}\"", ".", "format", "(", "val_loss", "/", "nb_val_steps", ")", ")", "\n", "print", "(", "\"val acc: {:.4f}\"", ".", "format", "(", "dev_acc", ")", ")", "\n", "print", "(", "\"val auroc: {:.4f}\"", ".", "format", "(", "dev_auroc", ")", ")", "\n", "print", "(", ")", "\n", "\n", "\n", "", "print", "(", "\"best eq: {:.4f}\"", ".", "format", "(", "best_ep", ")", ")", "\n", "print", "(", "\"best_dev_acc: {:.4f}\"", ".", "format", "(", "best_dev_acc", ")", ")", "\n", "print", "(", "\"best_dev_auroc: {:.4f}\"", ".", "format", "(", "best_dev_auroc", ")", ")", "\n", "torch", ".", "save", "(", "best_model_state_on_dev", ",", "f'{args.save}/model{args.sa_num}.pt'", ")", "\n", "\n", "argument", "[", "'train_loss'", "]", "=", "train_loss_set", "\n", "argument", "[", "'best_ep'", "]", "=", "best_ep", "\n", "argument", "[", "'best_dev_acc'", "]", "=", "best_dev_acc", "\n", "argument", "[", "'best_dev_auroc'", "]", "=", "best_dev_auroc", "\n", "with", "open", "(", "argument_path", ",", "'w'", ")", "as", "f", ":", "\n", "        ", "json", ".", "dump", "(", "argument", ",", "f", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ssu-humane_fake-news-thumbnail.None.model.ClassificationModel.__init__": [[5, 13], ["torch.Module.__init__", "transformers.CLIPModel.from_pretrained", "torch.Bilinear", "torch.ReLU", "torch.Linear", "torch.ReLU", "torch.Linear"], "methods", ["home.repos.pwc.inspect_result.ssu-humane_fake-news-thumbnail.None.datasets.Dataset.__init__"], ["    ", "def", "__init__", "(", "self", ",", "pretrained_model", "=", "\"openai/clip-vit-base-patch32\"", ")", ":", "\n", "        ", "super", "(", "ClassificationModel", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "clip", "=", "CLIPModel", ".", "from_pretrained", "(", "pretrained_model", ")", "\n", "self", ".", "bilayer", "=", "nn", ".", "Bilinear", "(", "512", ",", "512", ",", "512", ")", "\n", "self", ".", "relu1", "=", "nn", ".", "ReLU", "(", ")", "\n", "self", ".", "linear1", "=", "nn", ".", "Linear", "(", "512", ",", "512", ")", "\n", "self", ".", "relu2", "=", "nn", ".", "ReLU", "(", ")", "\n", "self", ".", "linear2", "=", "nn", ".", "Linear", "(", "512", ",", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ssu-humane_fake-news-thumbnail.None.model.ClassificationModel.forward": [[14, 21], ["model.ClassificationModel.clip", "model.ClassificationModel.bilayer", "model.ClassificationModel.relu1", "model.ClassificationModel.linear1", "model.ClassificationModel.relu2", "model.ClassificationModel.linear2"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "attention_mask", ",", "pixel_values", ")", ":", "\n", "        ", "clip_layer", "=", "self", ".", "clip", "(", "input_ids", "=", "input_ids", ",", "attention_mask", "=", "attention_mask", ",", "pixel_values", "=", "pixel_values", ")", "\n", "x", "=", "self", ".", "bilayer", "(", "clip_layer", ".", "text_embeds", ",", "clip_layer", ".", "image_embeds", ")", "\n", "x", "=", "self", ".", "relu1", "(", "x", ")", "\n", "x", "=", "self", ".", "linear1", "(", "x", ")", "\n", "x", "=", "self", ".", "relu2", "(", "x", ")", "\n", "return", "self", ".", "linear2", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ssu-humane_fake-news-thumbnail.None.model.ClassificationModel.clip_freeze": [[22, 28], ["model.ClassificationModel.clip.state_dict().keys", "model.ClassificationModel.clip.named_parameters", "model.ClassificationModel.clip.state_dict"], "methods", ["None"], ["", "def", "clip_freeze", "(", "self", ")", ":", "\n", "        ", "model_weight", "=", "self", ".", "clip", ".", "state_dict", "(", ")", ".", "keys", "(", ")", "\n", "model_weight_list", "=", "[", "*", "model_weight", "]", "\n", "for", "name", ",", "param", "in", "self", ".", "clip", ".", "named_parameters", "(", ")", ":", "\n", "            ", "if", "name", "in", "model_weight_list", ":", "\n", "                ", "param", ".", "requires_grad", "=", "False", "\n", "", "", "", "", ""]], "home.repos.pwc.inspect_result.ssu-humane_fake-news-thumbnail.None.datasets.Dataset.__init__": [[15, 20], ["datasets.Dataset.__init__", "transformers.CLIPProcessor.from_pretrained"], "methods", ["home.repos.pwc.inspect_result.ssu-humane_fake-news-thumbnail.None.datasets.Dataset.__init__"], ["    ", "def", "__init__", "(", "self", ",", "df", ",", "images_path", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "df", "=", "df", "\n", "self", ".", "images_path", "=", "images_path", "\n", "self", ".", "processor", "=", "CLIPProcessor", ".", "from_pretrained", "(", "\"openai/clip-vit-base-patch32\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ssu-humane_fake-news-thumbnail.None.datasets.Dataset.__len__": [[21, 23], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "df", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ssu-humane_fake-news-thumbnail.None.datasets.Dataset.__getitem__": [[24, 31], ["PIL.Image.open", "datasets.Dataset.processor", "datasets.context_padding", "torch.from_numpy", "numpy.asarray", "input_ids.squeeze", "attention_mask.squeeze", "datasets.Dataset.pixel_values.squeeze"], "methods", ["home.repos.pwc.inspect_result.ssu-humane_fake-news-thumbnail.None.datasets.context_padding"], ["", "def", "__getitem__", "(", "self", ",", "idx", ")", ":", "\n", "        ", "row", "=", "self", ".", "df", ".", "iloc", "[", "idx", "]", "\n", "image", "=", "Image", ".", "open", "(", "f\"{self.images_path}/{row['link'][-19:]}.jpg\"", ")", "\n", "inputs", "=", "self", ".", "processor", "(", "text", "=", "row", "[", "'title'", "]", ",", "images", "=", "image", ",", "return_tensors", "=", "\"pt\"", ",", "padding", "=", "True", ")", "\n", "input_ids", ",", "attention_mask", "=", "context_padding", "(", "inputs", ")", "\n", "label", "=", "torch", ".", "from_numpy", "(", "np", ".", "asarray", "(", "row", "[", "'label'", "]", ")", ")", "\n", "return", "input_ids", ".", "squeeze", "(", ")", ",", "attention_mask", ".", "squeeze", "(", ")", ",", "inputs", ".", "pixel_values", ".", "squeeze", "(", ")", ",", "label", "", "", "", ""]], "home.repos.pwc.inspect_result.ssu-humane_fake-news-thumbnail.None.datasets.context_padding": [[7, 13], ["torch.zeros", "torch.cat().long", "torch.cat().long", "torch.cat", "torch.cat"], "function", ["None"], ["def", "context_padding", "(", "inputs", ",", "context_length", "=", "77", ")", ":", "\n", "    ", "shape", "=", "(", "1", ",", "context_length", "-", "inputs", ".", "input_ids", ".", "shape", "[", "1", "]", ")", "\n", "x", "=", "torch", ".", "zeros", "(", "shape", ")", "\n", "input_ids", "=", "torch", ".", "cat", "(", "[", "inputs", ".", "input_ids", ",", "x", "]", ",", "dim", "=", "1", ")", ".", "long", "(", ")", "\n", "attention_mask", "=", "torch", ".", "cat", "(", "[", "inputs", ".", "attention_mask", ",", "x", "]", ",", "dim", "=", "1", ")", ".", "long", "(", ")", "\n", "return", "input_ids", ",", "attention_mask", "\n", "\n"]]}