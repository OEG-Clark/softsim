{"home.repos.pwc.inspect_result.Yale-LILY_SummerTime.dataset.dataset_loaders.CnndmDataset.__init__": [[35, 46], ["summertime.dataset.st_dataset.SummDataset.__init__"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.tests.model_selector_test.ToyModel.__init__"], ["def", "__init__", "(", "self", ",", "cache_dir", ":", "Optional", "[", "str", "]", "=", "None", ")", ":", "\n", "        ", "\"\"\"Create dataset information from the huggingface Dataset class\n        :rtype: object\n        :param cache_dir: Optional, a str specifying where to download/load the dataset to/from\n        \"\"\"", "\n", "dataset_args", "=", "(", "\n", "\"cnn_dailymail\"", ",", "\n", "\"3.0.0\"", ",", "\n", ")", "\n", "dataset_kwargs", "=", "{", "\"cache_dir\"", ":", "cache_dir", "}", "\n", "super", "(", ")", ".", "__init__", "(", "dataset_args", "=", "dataset_args", ",", "dataset_kwargs", "=", "dataset_kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.dataset.dataset_loaders.CnndmDataset._process_data": [[47, 61], ["tqdm.tqdm.tqdm", "summertime.dataset.st_dataset.SummInstance"], "methods", ["None"], ["", "def", "_process_data", "(", "self", ",", "data", ":", "Dataset", ")", "->", "Generator", "[", "SummInstance", ",", "None", ",", "None", "]", ":", "\n", "        ", "\"\"\"\n        Overrides the SummDataset '_process_data()' method\n        This method processes the data contained in the dataset\n            and puts each data instance into a SummInstance object\n        :param dataset: a train/validation/test dataset\n        :rtype: a generator yielding SummInstance objects\n        \"\"\"", "\n", "for", "instance", "in", "tqdm", "(", "data", ")", ":", "\n", "            ", "article", ":", "str", "=", "instance", "[", "\"article\"", "]", "\n", "highlights", ":", "str", "=", "instance", "[", "\"highlights\"", "]", "\n", "summ_instance", "=", "SummInstance", "(", "source", "=", "article", ",", "summary", "=", "highlights", ")", "\n", "\n", "yield", "summ_instance", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.dataset.dataset_loaders.MultinewsDataset.__init__": [[78, 86], ["summertime.dataset.st_dataset.SummDataset.__init__"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.tests.model_selector_test.ToyModel.__init__"], ["def", "__init__", "(", "self", ",", "cache_dir", ":", "Optional", "[", "str", "]", "=", "None", ")", ":", "\n", "        ", "\"\"\"Create dataset information from the huggingface Dataset class\n        :rtype: object\n        :param cache_dir: Optional, a str specifying where to download/load the dataset to/from\n        \"\"\"", "\n", "dataset_args", "=", "(", "\"multi_news\"", ",", ")", "\n", "dataset_kwargs", "=", "{", "\"cache_dir\"", ":", "cache_dir", "}", "\n", "super", "(", ")", ".", "__init__", "(", "dataset_args", "=", "dataset_args", ",", "dataset_kwargs", "=", "dataset_kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.dataset.dataset_loaders.MultinewsDataset._process_data": [[87, 105], ["tqdm.tqdm.tqdm", "summertime.dataset.st_dataset.SummInstance", "instance[].split"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.utils.sentence_splitter.PunktSentenceSplitter.split"], ["", "def", "_process_data", "(", "self", ",", "data", ":", "Dataset", ")", "->", "Generator", "[", "SummInstance", ",", "None", ",", "None", "]", ":", "\n", "        ", "\"\"\"\n        Overrides the SummDataset '_process_data()' method\n        This method processes the data contained in the dataset\n            and puts each data instance into a SummInstance object\n        :param dataset: a train/validation/test dataset\n        :rtype: a generator yielding SummInstance objects\n        \"\"\"", "\n", "for", "instance", "in", "tqdm", "(", "data", ")", ":", "\n", "            ", "document", ":", "list", "=", "[", "\n", "doc", "for", "doc", "in", "instance", "[", "\"document\"", "]", ".", "split", "(", "\"|||||\"", ")", "if", "doc", "\n", "]", "# removes the empty string generated", "\n", "# since each doc ends with the delimiting token '|||||'", "\n", "# the final doc creates an empty string", "\n", "summary", ":", "str", "=", "instance", "[", "\"summary\"", "]", "\n", "summ_instance", "=", "SummInstance", "(", "source", "=", "document", ",", "summary", "=", "summary", ")", "\n", "\n", "yield", "summ_instance", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.dataset.dataset_loaders.SamsumDataset.__init__": [[122, 130], ["summertime.dataset.st_dataset.SummDataset.__init__"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.tests.model_selector_test.ToyModel.__init__"], ["def", "__init__", "(", "self", ",", "cache_dir", ":", "Optional", "[", "str", "]", "=", "None", ")", ":", "\n", "        ", "\"\"\"Create dataset information from the huggingface Dataset class\n        :rtype: object\n        :param cache_dir: Optional, a str specifying where to download/load the dataset to/from\n        \"\"\"", "\n", "dataset_args", "=", "(", "\"samsum\"", ",", ")", "\n", "dataset_kwargs", "=", "{", "\"cache_dir\"", ":", "cache_dir", "}", "\n", "super", "(", ")", ".", "__init__", "(", "dataset_args", "=", "dataset_args", ",", "dataset_kwargs", "=", "dataset_kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.dataset.dataset_loaders.SamsumDataset._process_data": [[131, 148], ["tqdm.tqdm.tqdm", "instance[].split", "summertime.dataset.st_dataset.SummInstance", "summertime.dataset.st_dataset.SummInstance.ensure_dialogue_format"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.utils.sentence_splitter.PunktSentenceSplitter.split", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.dataset.st_dataset.SummInstance.ensure_dialogue_format"], ["", "def", "_process_data", "(", "self", ",", "data", ":", "Dataset", ")", "->", "Generator", "[", "SummInstance", ",", "None", ",", "None", "]", ":", "\n", "        ", "\"\"\"\n        Overrides the SummDataset '_process_data()' method\n        This method processes the data contained in the dataset\n            and puts each data instance into a SummInstance object\n        :param dataset: a train/validation/test dataset\n        :rtype: a generator yielding SummInstance objects\n        \"\"\"", "\n", "for", "instance", "in", "tqdm", "(", "data", ")", ":", "\n", "# split each dialogue into a list of strings such as [\"speaker1 : utter..\", \"speaker2 : utter...\"]", "\n", "            ", "dialogue", ":", "List", "=", "instance", "[", "\"dialogue\"", "]", ".", "split", "(", "\"\\r\\n\"", ")", "\n", "\n", "summary", ":", "str", "=", "instance", "[", "\"summary\"", "]", "\n", "summ_instance", "=", "SummInstance", "(", "source", "=", "dialogue", ",", "summary", "=", "summary", ")", "\n", "summ_instance", ".", "ensure_dialogue_format", "(", ")", "\n", "\n", "yield", "summ_instance", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.dataset.dataset_loaders.XsumDataset.__init__": [[165, 173], ["summertime.dataset.st_dataset.SummDataset.__init__"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.tests.model_selector_test.ToyModel.__init__"], ["def", "__init__", "(", "self", ",", "cache_dir", ":", "Optional", "[", "str", "]", "=", "None", ")", ":", "\n", "        ", "\"\"\"Create dataset information from the huggingface Dataset class\n        :rtype: object\n        :param cache_dir: Optional, a str specifying where to download/load the dataset to/from\n        \"\"\"", "\n", "dataset_args", "=", "(", "\"xsum\"", ",", ")", "\n", "dataset_kwargs", "=", "{", "\"cache_dir\"", ":", "cache_dir", "}", "\n", "super", "(", ")", ".", "__init__", "(", "dataset_args", "=", "dataset_args", ",", "dataset_kwargs", "=", "dataset_kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.dataset.dataset_loaders.XsumDataset._process_data": [[174, 188], ["tqdm.tqdm.tqdm", "summertime.dataset.st_dataset.SummInstance"], "methods", ["None"], ["", "def", "_process_data", "(", "self", ",", "data", ":", "Dataset", ")", "->", "Generator", "[", "SummInstance", ",", "None", ",", "None", "]", ":", "\n", "        ", "\"\"\"\n        Overrides the SummDataset '_process_data()' method\n        This method processes the data contained in the dataset\n            and puts each data instance into a SummInstance object\n        :param dataset: a train/validation/test dataset\n        :rtype: a generator yielding SummInstance objects\n        \"\"\"", "\n", "for", "instance", "in", "tqdm", "(", "data", ")", ":", "\n", "            ", "document", ":", "List", "=", "instance", "[", "\"document\"", "]", "\n", "summary", ":", "str", "=", "instance", "[", "\"summary\"", "]", "\n", "summ_instance", "=", "SummInstance", "(", "source", "=", "document", ",", "summary", "=", "summary", ")", "\n", "\n", "yield", "summ_instance", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.dataset.dataset_loaders.PubmedqaDataset.__init__": [[205, 218], ["summertime.dataset.st_dataset.SummDataset.__init__"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.tests.model_selector_test.ToyModel.__init__"], ["def", "__init__", "(", "self", ",", "cache_dir", ":", "Optional", "[", "str", "]", "=", "None", ",", "seed", ":", "int", "=", "None", ")", ":", "\n", "        ", "\"\"\"Create dataset information from the huggingface Dataset class\n        :rtype: object\n        :param cache_dir: Optional, a str specifying where to download/load the dataset to/from\n        :param seed: Optional, a seed for the random generator used for making the train and val splits\n        \"\"\"", "\n", "dataset_args", "=", "(", "\n", "\"pubmed_qa\"", ",", "\n", "\"pqa_artificial\"", ",", "\n", ")", "\n", "dataset_kwargs", "=", "{", "\"cache_dir\"", ":", "cache_dir", "}", "\n", "super", "(", ")", ".", "__init__", "(", "\n", "dataset_args", "=", "dataset_args", ",", "dataset_kwargs", "=", "dataset_kwargs", ",", "splitseed", "=", "seed", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.dataset.dataset_loaders.PubmedqaDataset._process_data": [[220, 235], ["tqdm.tqdm.tqdm", "summertime.dataset.st_dataset.SummInstance"], "methods", ["None"], ["", "def", "_process_data", "(", "self", ",", "data", ":", "Dataset", ")", "->", "Generator", "[", "SummInstance", ",", "None", ",", "None", "]", ":", "\n", "        ", "\"\"\"\n        Overrides the SummDataset '_process_data()' method\n        This method processes the data contained in the dataset\n            and puts each data instance into a SummInstance object\n        :param dataset: a train/validation/test dataset\n        :rtype: a generator yielding SummInstance objects\n        \"\"\"", "\n", "for", "instance", "in", "tqdm", "(", "data", ")", ":", "\n", "            ", "context", ":", "str", "=", "\" \"", ".", "join", "(", "instance", "[", "\"context\"", "]", "[", "\"contexts\"", "]", ")", "\n", "answer", ":", "str", "=", "instance", "[", "\"long_answer\"", "]", "\n", "query", ":", "str", "=", "instance", "[", "\"question\"", "]", "\n", "summ_instance", "=", "SummInstance", "(", "source", "=", "context", ",", "summary", "=", "answer", ",", "query", "=", "query", ")", "\n", "\n", "yield", "summ_instance", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.dataset.dataset_loaders.MlsumDataset.__init__": [[284, 295], ["summertime.dataset.st_dataset.SummDataset.__init__"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.tests.model_selector_test.ToyModel.__init__"], ["def", "__init__", "(", "\n", "self", ",", "languages", ":", "Union", "[", "str", ",", "List", "[", "str", "]", "]", "=", "\"all\"", ",", "cache_dir", ":", "Optional", "[", "str", "]", "=", "None", "\n", ")", ":", "\n", "        ", "\"\"\"Create dataset information from the huggingface Dataset class\n        :rtype: object\n        :param languages: Optional, a str or a list[str] specifying languages to be included\n        :param cache_dir: Optional, a str specifying where to download/load the dataset to/from\n        \"\"\"", "\n", "dataset_args", "=", "(", ")", "\n", "dataset_kwargs", "=", "{", "\"cache_dir\"", ":", "cache_dir", ",", "\"languages\"", ":", "languages", "}", "\n", "super", "(", ")", ".", "__init__", "(", "dataset_args", "=", "dataset_args", ",", "dataset_kwargs", "=", "dataset_kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.dataset.dataset_loaders.MlsumDataset._load_dataset_safe": [[296, 333], ["print", "kwargs.pop", "dataset_loaders.MlsumDataset._concatenate_dataset_dicts", "isinstance", "super()._load_dataset_safe", "language_datasets.append", "dataset_loaders.MlsumDataset.is_supported", "dataset_loaders.MlsumDataset.is_supported"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.dataset.st_dataset.SummDataset._concatenate_dataset_dicts", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.dataset.st_dataset.SummDataset._load_dataset_safe", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.dataset.dataset_loaders.XlsumDataset.is_supported", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.dataset.dataset_loaders.XlsumDataset.is_supported"], ["", "def", "_load_dataset_safe", "(", "self", ",", "args", ",", "kwargs", ")", ":", "\n", "        ", "\"\"\"\n        Overrides the parent class method\n        Method loads multiple datasets of different languages provided in :param languages:\n            It then concatenates these datasets into one combined dataset\n        :rtype: datasetDict containing the combined dataset\n        :param languages: Optional, either a string or list of strings specifying the languages\n            to load\n        \"\"\"", "\n", "print", "(", "MlsumDataset", ".", "mlsum_instantiation_guide", ")", "\n", "\n", "languages", "=", "kwargs", "[", "\"languages\"", "]", "\n", "kwargs", ".", "pop", "(", "\"languages\"", ",", "None", ")", "\n", "\n", "# Choose languages to download articles", "\n", "if", "languages", "==", "\"all\"", ":", "\n", "            ", "selected_languages", "=", "MlsumDataset", ".", "supported_languages", "\n", "", "elif", "isinstance", "(", "languages", ",", "list", ")", ":", "\n", "            ", "for", "language", "in", "languages", ":", "\n", "                ", "assert", "self", ".", "is_supported", "(", "language", ")", "\n", "", "selected_languages", "=", "languages", "\n", "", "else", ":", "\n", "            ", "assert", "self", ".", "is_supported", "(", "languages", ")", "\n", "selected_languages", "=", "[", "languages", "]", "\n", "\n", "# Concatenate selected languaeges into one dataset", "\n", "", "language_datasets", "=", "[", "]", "\n", "for", "language", "in", "selected_languages", ":", "\n", "            ", "dataset_args", "=", "(", "\"mlsum\"", ",", "language", ",", "*", "args", ")", "\n", "dataset_kwargs", "=", "kwargs", "\n", "dataset", "=", "super", "(", ")", ".", "_load_dataset_safe", "(", "dataset_args", ",", "dataset_kwargs", ")", "\n", "\n", "language_datasets", ".", "append", "(", "dataset", ")", "\n", "\n", "", "mlsum_dataset", "=", "self", ".", "_concatenate_dataset_dicts", "(", "language_datasets", ")", "\n", "\n", "return", "mlsum_dataset", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.dataset.dataset_loaders.MlsumDataset._process_data": [[334, 348], ["tqdm.tqdm.tqdm", "summertime.dataset.st_dataset.SummInstance"], "methods", ["None"], ["", "def", "_process_data", "(", "self", ",", "data", ":", "Dataset", ")", "->", "Generator", "[", "SummInstance", ",", "None", ",", "None", "]", ":", "\n", "        ", "\"\"\"\n        Overrides the SummDataset '_process_data()' method\n        This method processes the data contained in the dataset\n            and puts each data instance into a SummInstance object\n        :param dataset: a train/validation/test dataset\n        :rtype: a generator yielding SummInstance objects\n        \"\"\"", "\n", "for", "instance", "in", "tqdm", "(", "data", ")", ":", "\n", "            ", "article", ":", "List", "=", "instance", "[", "\"text\"", "]", "\n", "summary", ":", "str", "=", "instance", "[", "\"summary\"", "]", "\n", "summ_instance", "=", "SummInstance", "(", "source", "=", "article", ",", "summary", "=", "summary", ")", "\n", "\n", "yield", "summ_instance", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.dataset.dataset_loaders.MlsumDataset.is_supported": [[349, 362], ["print", "ValueError"], "methods", ["None"], ["", "", "def", "is_supported", "(", "self", ",", "language", ":", "str", ")", ":", "\n", "        ", "\"\"\"\n        Checks whether the requested langues is supported\n        :param language: string containing the requested language\n        :rtype bool:\n        \"\"\"", "\n", "if", "language", "not", "in", "MlsumDataset", ".", "supported_languages", ":", "\n", "            ", "print", "(", "MlsumDataset", ".", "mlsum_instantiation_guide", ")", "\n", "raise", "ValueError", "(", "\n", "f\"The language(s): '{language}' entered is not supported. See above message for usage info\"", "\n", ")", "\n", "", "else", ":", "\n", "            ", "return", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.dataset.dataset_loaders.XlsumDataset.__init__": [[448, 459], ["summertime.dataset.st_dataset.SummDataset.__init__"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.tests.model_selector_test.ToyModel.__init__"], ["def", "__init__", "(", "\n", "self", ",", "languages", ":", "Union", "[", "str", ",", "List", "[", "str", "]", "]", ",", "cache_dir", ":", "Optional", "[", "str", "]", "=", "None", "\n", ")", ":", "\n", "        ", "\"\"\"Create dataset information from the huggingface Dataset class\n        :rtype: object\n        :param languages: Optional, a str or a list[str] specifying languages to be included\n        :param cache_dir: Optional, a str specifying where to download/load the dataset to/from\n        \"\"\"", "\n", "dataset_args", "=", "(", ")", "\n", "dataset_kwargs", "=", "{", "\"cache_dir\"", ":", "cache_dir", ",", "\"languages\"", ":", "languages", "}", "\n", "super", "(", ")", ".", "__init__", "(", "dataset_args", "=", "dataset_args", ",", "dataset_kwargs", "=", "dataset_kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.dataset.dataset_loaders.XlsumDataset._load_dataset_safe": [[460, 497], ["print", "kwargs.pop", "dataset_loaders.XlsumDataset._concatenate_dataset_dicts", "isinstance", "super()._load_dataset_safe", "language_datasets.append", "dataset_loaders.XlsumDataset.is_supported", "dataset_loaders.XlsumDataset.is_supported"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.dataset.st_dataset.SummDataset._concatenate_dataset_dicts", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.dataset.st_dataset.SummDataset._load_dataset_safe", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.dataset.dataset_loaders.XlsumDataset.is_supported", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.dataset.dataset_loaders.XlsumDataset.is_supported"], ["", "def", "_load_dataset_safe", "(", "self", ",", "args", ",", "kwargs", ")", ":", "\n", "        ", "\"\"\"\n        Overrides the parent class method\n        Method loads multiple datasets of different languages provided in :param languages:\n            It then concatenates these datasets into one combined dataset\n        :rtype: datasetDict containing the combined dataset\n        :param languages: Optional, either a string or list of strings specifying the languages\n            to load\n        \"\"\"", "\n", "print", "(", "self", ".", "instantiation_guide", ")", "\n", "\n", "languages", "=", "kwargs", "[", "\"languages\"", "]", "\n", "kwargs", ".", "pop", "(", "\"languages\"", ",", "None", ")", "\n", "\n", "# Choose languages to load", "\n", "if", "languages", "==", "\"all\"", ":", "\n", "            ", "selected_languages", "=", "self", ".", "supported_languages", "\n", "", "elif", "isinstance", "(", "languages", ",", "list", ")", ":", "\n", "            ", "for", "language", "in", "languages", ":", "\n", "                ", "assert", "self", ".", "is_supported", "(", "language", ")", "\n", "", "selected_languages", "=", "languages", "\n", "", "else", ":", "\n", "            ", "assert", "self", ".", "is_supported", "(", "languages", ")", "\n", "selected_languages", "=", "[", "languages", "]", "\n", "\n", "# Concatenate selected languages into one dataset", "\n", "", "language_datasets", "=", "[", "]", "\n", "for", "language", "in", "selected_languages", ":", "\n", "            ", "dataset_args", "=", "(", "\"csebuetnlp/xlsum\"", ",", "language", ")", "\n", "dataset_kwargs", "=", "kwargs", "\n", "dataset", "=", "super", "(", ")", ".", "_load_dataset_safe", "(", "dataset_args", ",", "dataset_kwargs", ")", "\n", "\n", "language_datasets", ".", "append", "(", "dataset", ")", "\n", "\n", "", "xlsum_dataset", "=", "self", ".", "_concatenate_dataset_dicts", "(", "language_datasets", ")", "\n", "\n", "return", "xlsum_dataset", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.dataset.dataset_loaders.XlsumDataset._process_data": [[498, 512], ["tqdm.tqdm.tqdm", "summertime.dataset.st_dataset.SummInstance"], "methods", ["None"], ["", "def", "_process_data", "(", "self", ",", "data", ":", "Dataset", ")", "->", "Generator", "[", "SummInstance", ",", "None", ",", "None", "]", ":", "\n", "        ", "\"\"\"\n        Overrides the SummDataset '_process_data()' method\n        This method processes the data contained in the dataset\n            and puts each data instance into a SummInstance object\n        :param dataset: a train/validation/test dataset\n        :rtype: a generator yielding SummInstance objects\n        \"\"\"", "\n", "for", "instance", "in", "tqdm", "(", "data", ")", ":", "\n", "            ", "article", ":", "List", "=", "instance", "[", "\"text\"", "]", "\n", "summary", ":", "str", "=", "instance", "[", "\"summary\"", "]", "\n", "summ_instance", "=", "SummInstance", "(", "source", "=", "article", ",", "summary", "=", "summary", ")", "\n", "\n", "yield", "summ_instance", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.dataset.dataset_loaders.XlsumDataset.is_supported": [[513, 526], ["print", "ValueError"], "methods", ["None"], ["", "", "def", "is_supported", "(", "self", ",", "language", ":", "str", ")", ":", "\n", "        ", "\"\"\"\n        Checks whether the requested language is supported\n        :param language: string containing the requested language\n        :rtype bool:\n        \"\"\"", "\n", "if", "language", "not", "in", "self", ".", "supported_languages", ":", "\n", "            ", "print", "(", "self", ".", "instantiation_guide", ")", "\n", "raise", "ValueError", "(", "\n", "f\"The language(s): '{language}' entered is not supported. See above message for usage info\"", "\n", ")", "\n", "", "else", ":", "\n", "            ", "return", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.dataset.dataset_loaders.ScisummnetDataset.__init__": [[550, 558], ["summertime.dataset.st_dataset.SummDataset.__init__"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.tests.model_selector_test.ToyModel.__init__"], ["def", "__init__", "(", "self", ",", "cache_dir", ":", "Optional", "[", "str", "]", "=", "None", ",", "seed", ":", "int", "=", "None", ")", ":", "\n", "        ", "\"\"\"Create dataset information from the huggingface Dataset class\n        :rtype: object\n        :param cache_dir: Optional, a str specifying where to download/load the dataset to/from\n        :param seed: Optional, a seed for the random generator used for making the train and val splits\n        \"\"\"", "\n", "dataset_kwargs", "=", "{", "\"cache_dir\"", ":", "cache_dir", ",", "\"path\"", ":", "self", ".", "builder_script_path", "}", "\n", "super", "(", ")", ".", "__init__", "(", "dataset_kwargs", "=", "dataset_kwargs", ",", "splitseed", "=", "seed", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.dataset.dataset_loaders.ScisummnetDataset._process_data": [[559, 576], ["tqdm.tqdm.tqdm", "summertime.dataset.st_dataset.SummInstance"], "methods", ["None"], ["", "def", "_process_data", "(", "self", ",", "data", ":", "Dataset", ")", "->", "Generator", "[", "SummInstance", ",", "None", ",", "None", "]", ":", "\n", "        ", "\"\"\"\n        Overrides the SummDataset '_process_data()' method\n        This method processes the data contained in the dataset\n            and puts each data instance into a SummInstance object\n        :param dataset: a train/validation/test dataset\n        :rtype: a generator yielding SummInstance objects\n        \"\"\"", "\n", "for", "instance", "in", "tqdm", "(", "data", ")", ":", "\n", "            ", "docs", ":", "List", "=", "[", "\n", "instance", "[", "\"document_xml\"", "]", ",", "\n", "instance", "[", "\"citing_sentences_annotated.json\"", "]", ",", "\n", "]", "\n", "summary", ":", "str", "=", "instance", "[", "\"summary\"", "]", "\n", "summ_instance", "=", "SummInstance", "(", "source", "=", "docs", ",", "summary", "=", "summary", ")", "\n", "\n", "yield", "summ_instance", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.dataset.dataset_loaders.SummscreenDataset.__init__": [[596, 603], ["summertime.dataset.st_dataset.SummDataset.__init__"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.tests.model_selector_test.ToyModel.__init__"], ["def", "__init__", "(", "self", ",", "cache_dir", ":", "Optional", "[", "str", "]", "=", "None", ")", ":", "\n", "        ", "\"\"\"Create dataset information from the huggingface Dataset class\n        :rtype: object\n        :param cache_dir: Optional, a str specifying where to download/load the dataset to/from\n        \"\"\"", "\n", "dataset_kwargs", "=", "{", "\"cache_dir\"", ":", "cache_dir", ",", "\"path\"", ":", "self", ".", "builder_script_path", "}", "\n", "super", "(", ")", ".", "__init__", "(", "dataset_kwargs", "=", "dataset_kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.dataset.dataset_loaders.SummscreenDataset._process_data": [[604, 620], ["tqdm.tqdm.tqdm", "summertime.dataset.st_dataset.SummInstance"], "methods", ["None"], ["", "def", "_process_data", "(", "self", ",", "data", ":", "Dataset", ")", "->", "Generator", "[", "SummInstance", ",", "None", ",", "None", "]", ":", "\n", "        ", "\"\"\"\n        Overrides the SummDataset '_process_data()' method\n        This method processes the data contained in the dataset\n            and puts each data instance into a SummInstance object\n        :param dataset: a train/validation/test dataset\n        :rtype: a generator yielding SummInstance objects\n        \"\"\"", "\n", "for", "instance", "in", "tqdm", "(", "data", ")", ":", "\n", "            ", "transcript", ":", "List", "=", "instance", "[", "\n", "\"transcript\"", "\n", "]", "# convert string into a list of string dialogues", "\n", "recap", ":", "str", "=", "instance", "[", "\"recap\"", "]", "\n", "summ_instance", "=", "SummInstance", "(", "source", "=", "transcript", ",", "summary", "=", "recap", ")", "\n", "\n", "yield", "summ_instance", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.dataset.dataset_loaders.QMsumDataset.__init__": [[641, 648], ["summertime.dataset.st_dataset.SummDataset.__init__"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.tests.model_selector_test.ToyModel.__init__"], ["def", "__init__", "(", "self", ",", "cache_dir", ":", "Optional", "[", "str", "]", "=", "None", ")", ":", "\n", "        ", "\"\"\"Create dataset information from the huggingface Dataset class\n        :rtype: object\n        :param cache_dir: Optional, a str specifying where to download/load the dataset to/from\n        \"\"\"", "\n", "dataset_kwargs", "=", "{", "\"cache_dir\"", ":", "cache_dir", ",", "\"path\"", ":", "self", ".", "builder_script_path", "}", "\n", "super", "(", ")", ".", "__init__", "(", "dataset_kwargs", "=", "dataset_kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.dataset.dataset_loaders.QMsumDataset._process_data": [[649, 672], ["tqdm.tqdm.tqdm", "summertime.dataset.st_dataset.SummInstance"], "methods", ["None"], ["", "def", "_process_data", "(", "self", ",", "data", ":", "Dataset", ")", "->", "Generator", "[", "SummInstance", ",", "None", ",", "None", "]", ":", "\n", "        ", "\"\"\"\n        Overrides the SummDataset '_process_data()' method\n        This method processes the data contained in the dataset\n            and puts each data instance into a SummInstance object\n        :param dataset: a train/validation/test dataset\n        :rtype: a generator yielding SummInstance objects\n        \"\"\"", "\n", "for", "instance", "in", "tqdm", "(", "data", ")", ":", "\n", "            ", "for", "query_set", "in", "(", "\n", "instance", "[", "\"general_query_list\"", "]", "+", "instance", "[", "\"specific_query_list\"", "]", "\n", ")", ":", "\n", "                ", "meeting", ":", "List", "=", "[", "\n", "utterance", "[", "\"speaker\"", "]", "+", "\" : \"", "+", "utterance", "[", "\"content\"", "]", "\n", "for", "utterance", "in", "instance", "[", "\"meeting_transcripts\"", "]", "\n", "]", "\n", "query", ":", "str", "=", "query_set", "[", "\"query\"", "]", "\n", "summary", ":", "str", "=", "query_set", "[", "\"answer\"", "]", "\n", "summ_instance", "=", "SummInstance", "(", "\n", "source", "=", "meeting", ",", "summary", "=", "summary", ",", "query", "=", "query", "\n", ")", "\n", "\n", "", "yield", "summ_instance", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.dataset.dataset_loaders.ArxivDataset.__init__": [[690, 705], ["print", "summertime.dataset.st_dataset.SummDataset.__init__"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.tests.model_selector_test.ToyModel.__init__"], ["def", "__init__", "(", "self", ",", "cache_dir", ":", "Optional", "[", "str", "]", "=", "None", ")", ":", "\n", "        ", "\"\"\"Create dataset information from the huggingface Dataset class\n        :rtype: object\n        :param cache_dir: Optional, a str specifying where to download/load the dataset to/from\n        \"\"\"", "\n", "print", "(", "\n", "\"*****************\"", ",", "\n", "\"***Attention***\"", ",", "\n", "\"This dataset is quite large (approx 5Gb and will need about 15 Gb for the extraction process\"", ",", "\n", "\"Cancel/interrupt the download if size and time constraints will not be met\"", ",", "\n", "\"*****************\"", ",", "\n", "sep", "=", "\"\\n\"", ",", "\n", ")", "\n", "dataset_kwargs", "=", "{", "\"cache_dir\"", ":", "cache_dir", ",", "\"path\"", ":", "self", ".", "builder_script_path", "}", "\n", "super", "(", ")", ".", "__init__", "(", "dataset_kwargs", "=", "dataset_kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.dataset.dataset_loaders.ArxivDataset._process_data": [[706, 720], ["tqdm.tqdm.tqdm", "summertime.dataset.st_dataset.SummInstance"], "methods", ["None"], ["", "def", "_process_data", "(", "self", ",", "data", ":", "Dataset", ")", "->", "Generator", "[", "SummInstance", ",", "None", ",", "None", "]", ":", "\n", "        ", "\"\"\"\n        Overrides the SummDataset '_process_data()' method\n        This method processes the data contained in the dataset\n            and puts each data instance into a SummInstance object\n        :param dataset: a train/validation/test dataset\n        :rtype: a generator yielding SummInstance objects\n        \"\"\"", "\n", "for", "instance", "in", "tqdm", "(", "data", ")", ":", "\n", "            ", "article", ":", "List", "=", "instance", "[", "\"article_text\"", "]", "\n", "abstract", ":", "str", "=", "\" \"", ".", "join", "(", "instance", "[", "\"abstract_text\"", "]", ")", "\n", "summ_instance", "=", "SummInstance", "(", "source", "=", "article", ",", "summary", "=", "abstract", ")", "\n", "\n", "yield", "summ_instance", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.dataset.dataset_loaders.MassivesummDataset.__init__": [[862, 878], ["summertime.dataset.st_dataset.SummDataset.__init__"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.tests.model_selector_test.ToyModel.__init__"], ["def", "__init__", "(", "\n", "self", ",", "languages", ":", "Union", "[", "str", ",", "List", "[", "str", "]", "]", ",", "cache_dir", ":", "Optional", "[", "str", "]", "=", "None", "\n", ")", ":", "\n", "        ", "\"\"\"Create dataset information from the huggingface Dataset class\n        :rtype: object\n        :param languages: Optional, a str or a list[str] specifying languages to be included\n        :param cache_dir: Optional, a str specifying where to download/load the dataset to/from\n        \"\"\"", "\n", "dataset_args", "=", "(", ")", "\n", "dataset_kwargs", "=", "{", "\n", "\"name\"", ":", "languages", ",", "\n", "\"cache_dir\"", ":", "cache_dir", ",", "\n", "\"path\"", ":", "self", ".", "builder_script_path", ",", "\n", "}", "\n", "\n", "super", "(", ")", ".", "__init__", "(", "dataset_args", "=", "dataset_args", ",", "dataset_kwargs", "=", "dataset_kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.dataset.dataset_loaders.MassivesummDataset._process_data": [[879, 893], ["tqdm.tqdm.tqdm", "summertime.dataset.st_dataset.SummInstance"], "methods", ["None"], ["", "def", "_process_data", "(", "self", ",", "data", ":", "Dataset", ")", "->", "Generator", "[", "SummInstance", ",", "None", ",", "None", "]", ":", "\n", "        ", "\"\"\"\n        Overrides the SummDataset '_process_data()' method\n        This method processes the data contained in the dataset\n            and puts each data instance into a SummInstance object\n        :param dataset: a train/validation/test dataset\n        :rtype: a generator yielding SummInstance objects\n        \"\"\"", "\n", "for", "instance", "in", "tqdm", "(", "data", ")", ":", "\n", "            ", "article", ":", "str", "=", "instance", "[", "\"article\"", "]", "\n", "summary", ":", "str", "=", "instance", "[", "\"summary\"", "]", "\n", "summ_instance", "=", "SummInstance", "(", "source", "=", "article", ",", "summary", "=", "summary", ")", "\n", "\n", "yield", "summ_instance", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.dataset.st_dataset.SummInstance.__init__": [[29, 44], ["None"], "methods", ["None"], ["def", "__init__", "(", "\n", "self", ",", "source", ":", "Union", "[", "List", "[", "str", "]", ",", "str", "]", ",", "summary", ":", "str", ",", "query", ":", "Optional", "[", "str", "]", "=", "None", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Create a summarization instance\n        :rtype: object\n        :param source: either `List[str]` or `str`, depending on the dataset itself, string joining may needed to fit\n            into specific models. For example, for the same document, it could be simply `str` or `List[str]` for\n            a list of sentences in the same document\n        :param summary: a string summary that serves as ground truth\n        :param query: Optional, applies when a string query is present\n        \"\"\"", "\n", "self", ".", "source", "=", "source", "\n", "self", ".", "summary", "=", "summary", "\n", "self", ".", "query", "=", "query", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.dataset.st_dataset.SummInstance.__repr__": [[45, 51], ["str"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "instance_dict", "=", "{", "\"source\"", ":", "self", ".", "source", ",", "\"summary\"", ":", "self", ".", "summary", "}", "\n", "if", "self", ".", "query", ":", "\n", "            ", "instance_dict", "[", "\"query\"", "]", "=", "self", ".", "query", "\n", "\n", "", "return", "str", "(", "instance_dict", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.dataset.st_dataset.SummInstance.__str__": [[52, 58], ["pprint.pformat"], "methods", ["None"], ["", "def", "__str__", "(", "self", ")", ":", "\n", "        ", "instance_dict", "=", "{", "\"source\"", ":", "self", ".", "source", ",", "\"summary\"", ":", "self", ".", "summary", "}", "\n", "if", "self", ".", "query", ":", "\n", "            ", "instance_dict", "[", "\"query\"", "]", "=", "self", ".", "query", "\n", "\n", "", "return", "pformat", "(", "instance_dict", ",", "indent", "=", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.dataset.st_dataset.SummInstance.ensure_dialogue_format": [[59, 69], ["re.compile", "isinstance", "range", "len", "re.compile.match"], "methods", ["None"], ["", "def", "ensure_dialogue_format", "(", "self", ")", ":", "\n", "        ", "pattern", "=", "re", ".", "compile", "(", "r\"\\w+\\s:\\s\\w+\"", ")", "\n", "\n", "assert", "isinstance", "(", "\n", "self", ".", "source", ",", "list", "\n", ")", ",", "\"Source should be a list of strings for dialogue\"", "\n", "\n", "for", "i", "in", "range", "(", "len", "(", "self", ".", "source", ")", ")", ":", "\n", "            ", "if", "not", "pattern", ".", "match", "(", "self", ".", "source", "[", "i", "]", ")", ":", "\n", "                ", "self", ".", "source", "[", "i", "]", "=", "f\"None : {self.source[i]}\"", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.dataset.st_dataset.SummDataset.__init__": [[81, 128], ["st_dataset.SummDataset._load_dataset_safe", "st_dataset.SummDataset._get_dataset_info", "st_dataset.SummDataset._process_data", "st_dataset.SummDataset._process_data", "st_dataset.SummDataset._process_data", "st_dataset.SummDataset.remove", "st_dataset.SummDataset._generate_missing_val_test_splits", "st_dataset.SummDataset.remove"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.dataset.st_dataset.SummDataset._load_dataset_safe", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.dataset.st_dataset.SummDataset._get_dataset_info", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.dataset.st_dataset.CustomDataset._process_data", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.dataset.st_dataset.CustomDataset._process_data", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.dataset.st_dataset.CustomDataset._process_data", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.dataset.st_dataset.SummDataset._generate_missing_val_test_splits"], ["def", "__init__", "(", "\n", "self", ",", "\n", "dataset_args", ":", "Optional", "[", "Tuple", "[", "str", "]", "]", "=", "(", ")", ",", "\n", "dataset_kwargs", ":", "Optional", "[", "Tuple", "[", "str", "]", "]", "=", "{", "}", ",", "\n", "splitseed", ":", "Optional", "[", "int", "]", "=", "None", ",", "\n", ")", ":", "\n", "        ", "\"\"\"Create dataset information from the huggingface Dataset class\n        :rtype: object\n        :param dataset_args: a tuple containing arguments to passed on to the 'load_dataset_safe' method.\n            The args are used by the huggingface 'load_dataset' method\n            The arguments for each dataset are different and comprise of a string or multiple strings\n        :param dataset_kwargs: a dictionary containing keyword arguments to be passed on to the 'load_dataset_safe' method\n        :param splitseed: a number to instantiate the random generator used to generate val/test splits\n            for the datasets without them\n        \"\"\"", "\n", "\n", "# Load dataset from huggingface, use default huggingface arguments", "\n", "dataset", "=", "self", ".", "_load_dataset_safe", "(", "dataset_args", ",", "dataset_kwargs", ")", "\n", "\n", "info_set", "=", "self", ".", "_get_dataset_info", "(", "dataset", ")", "\n", "\n", "# Ensure any dataset with a val or dev or validation split is standardised to validation split", "\n", "if", "\"val\"", "in", "dataset", ":", "\n", "            ", "dataset", "[", "\"validation\"", "]", "=", "dataset", "[", "\"val\"", "]", "\n", "dataset", ".", "remove", "(", "\"val\"", ")", "\n", "", "elif", "\"dev\"", "in", "dataset", ":", "\n", "            ", "dataset", "[", "\"validation\"", "]", "=", "dataset", "[", "\"dev\"", "]", "\n", "dataset", ".", "remove", "(", "\"dev\"", ")", "\n", "\n", "# If no splits other other than training, generate them", "\n", "", "assert", "(", "\n", "\"train\"", "in", "dataset", "or", "\"validation\"", "in", "dataset", "or", "\"test\"", "in", "dataset", "\n", ")", ",", "\"At least one of train/validation test needs to be not empty!\"", "\n", "\n", "if", "not", "(", "\"validation\"", "in", "dataset", "or", "\"test\"", "in", "dataset", ")", ":", "\n", "            ", "dataset", "=", "self", ".", "_generate_missing_val_test_splits", "(", "dataset", ",", "splitseed", ")", "\n", "\n", "", "self", ".", "description", "=", "info_set", ".", "description", "\n", "self", ".", "citation", "=", "info_set", ".", "citation", "\n", "self", ".", "homepage", "=", "info_set", ".", "homepage", "\n", "\n", "# Extract the dataset entries from folders and load into dataset", "\n", "self", ".", "_train_set", "=", "self", ".", "_process_data", "(", "dataset", "[", "\"train\"", "]", ")", "\n", "self", ".", "_validation_set", "=", "self", ".", "_process_data", "(", "\n", "dataset", "[", "\"validation\"", "]", "\n", ")", "# Some datasets have a validation split", "\n", "self", ".", "_test_set", "=", "self", ".", "_process_data", "(", "dataset", "[", "\"test\"", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.dataset.st_dataset.SummDataset.train_set": [[129, 138], ["print", "list"], "methods", ["None"], ["", "@", "property", "\n", "def", "train_set", "(", "self", ")", "->", "Union", "[", "Generator", "[", "SummInstance", ",", "None", ",", "None", "]", ",", "List", "]", ":", "\n", "        ", "if", "self", ".", "_train_set", "is", "not", "None", ":", "\n", "            ", "return", "self", ".", "_train_set", "\n", "", "else", ":", "\n", "            ", "print", "(", "\n", "f\"{self.dataset_name} does not contain a train set, empty list returned\"", "\n", ")", "\n", "return", "list", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.dataset.st_dataset.SummDataset.validation_set": [[139, 148], ["print", "list"], "methods", ["None"], ["", "", "@", "property", "\n", "def", "validation_set", "(", "self", ")", "->", "Union", "[", "Generator", "[", "SummInstance", ",", "None", ",", "None", "]", ",", "List", "]", ":", "\n", "        ", "if", "self", ".", "_validation_set", "is", "not", "None", ":", "\n", "            ", "return", "self", ".", "_validation_set", "\n", "", "else", ":", "\n", "            ", "print", "(", "\n", "f\"{self.dataset_name} does not contain a validation set, empty list returned\"", "\n", ")", "\n", "return", "list", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.dataset.st_dataset.SummDataset.test_set": [[149, 158], ["print", "list"], "methods", ["None"], ["", "", "@", "property", "\n", "def", "test_set", "(", "self", ")", "->", "Union", "[", "Generator", "[", "SummInstance", ",", "None", ",", "None", "]", ",", "List", "]", ":", "\n", "        ", "if", "self", ".", "_test_set", "is", "not", "None", ":", "\n", "            ", "return", "self", ".", "_test_set", "\n", "", "else", ":", "\n", "            ", "print", "(", "\n", "f\"{self.dataset_name} does not contain a test set, empty list returned\"", "\n", ")", "\n", "return", "list", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.dataset.st_dataset.SummDataset._load_dataset_safe": [[159, 189], ["range", "datasets.load_dataset", "time.sleep", "RuntimeError"], "methods", ["None"], ["", "", "def", "_load_dataset_safe", "(", "self", ",", "args", "=", "(", ")", ",", "kwargs", "=", "{", "}", ")", "->", "Dataset", ":", "\n", "        ", "\"\"\"\n        This method creates a wrapper around the huggingface 'load_dataset()' function for a more robust download function,\n            the original 'load_dataset()' function occassionally fails when it cannot reach a server especially after multiple requests.\n            This method tackles this problem by attempting the download multiple times with a wait time before each retry\n\n        The wrapper method passes all arguments and keyword arguments to the 'load_dataset' function with no alteration.\n        :rtype: Dataset\n        :param args: non-keyword arguments to passed on to the 'load_dataset' function\n        :param kwargs: keyword arguments to passed on to the 'load_dataset' function\n        \"\"\"", "\n", "\n", "tries", "=", "DEFAULT_NUMBER_OF_RETRIES_ALLOWED", "\n", "wait_time", "=", "DEFAULT_WAIT_SECONDS_BEFORE_RETRY", "\n", "\n", "for", "i", "in", "range", "(", "tries", ")", ":", "\n", "            ", "try", ":", "\n", "                ", "dataset", "=", "load_dataset", "(", "*", "args", ",", "**", "kwargs", ")", "\n", "", "except", "ConnectionError", ":", "\n", "                ", "if", "i", "<", "tries", "-", "1", ":", "# i is zero indexed", "\n", "                    ", "sleep", "(", "wait_time", ")", "\n", "continue", "\n", "", "else", ":", "\n", "                    ", "raise", "RuntimeError", "(", "\n", "\"Wait for a minute and attempt downloading the dataset again. \\\n                         The server hosting the dataset occassionally times out.\"", "\n", ")", "\n", "", "", "break", "\n", "\n", "", "return", "dataset", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.dataset.st_dataset.SummDataset._get_dataset_info": [[190, 198], ["None"], "methods", ["None"], ["", "def", "_get_dataset_info", "(", "self", ",", "data_dict", ":", "DatasetDict", ")", "->", "DatasetInfo", ":", "\n", "        ", "\"\"\"\n        Get the information set from the dataset\n        The information set contains: dataset name, description, version, citation and licence\n        :param data_dict: DatasetDict\n        :rtype: DatasetInfo\n        \"\"\"", "\n", "return", "data_dict", "[", "\"train\"", "]", ".", "info", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.dataset.st_dataset.SummDataset._process_data": [[199, 211], ["None"], "methods", ["None"], ["", "@", "abstractmethod", "\n", "def", "_process_data", "(", "self", ",", "dataset", ":", "Dataset", ")", "->", "Generator", "[", "SummInstance", ",", "None", ",", "None", "]", ":", "\n", "        ", "\"\"\"\n        Abstract class method to process the data contained within each dataset.\n        Each dataset class processes it's own information differently due to the diversity in domains\n        This method processes the data contained in the dataset\n            and puts each data instance into a SummInstance object,\n            the SummInstance has the following properties [source, summary, query[optional]]\n        :param dataset: a train/validation/test dataset\n        :rtype: a generator yielding SummInstance objects\n        \"\"\"", "\n", "return", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.dataset.st_dataset.SummDataset._generate_missing_val_test_splits": [[212, 251], ["dataset_dict[].train_test_split", "dataset_dict[].train_test_split"], "methods", ["None"], ["", "def", "_generate_missing_val_test_splits", "(", "\n", "self", ",", "dataset_dict", ":", "DatasetDict", ",", "seed", ":", "int", "\n", ")", "->", "DatasetDict", ":", "\n", "        ", "\"\"\"\n        Creating the train, val and test splits from a dataset\n            the generated sets are 'train: ~.80', 'validation: ~.10', and 'test: ~10' in size\n            the splits are randomized for each object unless a seed is provided for the random generator\n\n        :param dataset: Arrow Dataset with containing, usually the train set\n        :param seed: seed for the random generator to shuffle the dataset\n        :rtype: Arrow DatasetDict containing the three splits\n        \"\"\"", "\n", "\n", "# Return dataset if no train set available for splitting", "\n", "if", "\"train\"", "not", "in", "dataset_dict", ":", "\n", "            ", "if", "\"validation\"", "not", "in", "dataset_dict", ":", "\n", "                ", "dataset_dict", "[", "\"validation\"", "]", "=", "None", "\n", "", "if", "\"test\"", "not", "in", "dataset_dict", ":", "\n", "                ", "dataset_dict", "[", "\"test\"", "]", "=", "None", "\n", "\n", "", "return", "dataset_dict", "\n", "\n", "# Create a 'test' split from 'train' if no 'test' set is available", "\n", "", "if", "\"test\"", "not", "in", "dataset_dict", ":", "\n", "            ", "dataset_traintest_split", "=", "dataset_dict", "[", "\"train\"", "]", ".", "train_test_split", "(", "\n", "test_size", "=", "TEST_OR_VAL_SPLIT_RATIO", ",", "seed", "=", "seed", "\n", ")", "\n", "dataset_dict", "[", "\"train\"", "]", "=", "dataset_traintest_split", "[", "\"train\"", "]", "\n", "dataset_dict", "[", "\"test\"", "]", "=", "dataset_traintest_split", "[", "\"test\"", "]", "\n", "\n", "# Create a 'validation' split from the remaining 'train' set if no 'validation' set is available", "\n", "", "if", "\"validation\"", "not", "in", "dataset_dict", ":", "\n", "            ", "dataset_trainval_split", "=", "dataset_dict", "[", "\"train\"", "]", ".", "train_test_split", "(", "\n", "test_size", "=", "TEST_OR_VAL_SPLIT_RATIO", ",", "seed", "=", "seed", "\n", ")", "\n", "dataset_dict", "[", "\"train\"", "]", "=", "dataset_trainval_split", "[", "\"train\"", "]", "\n", "dataset_dict", "[", "\"validation\"", "]", "=", "dataset_trainval_split", "[", "\"test\"", "]", "\n", "\n", "", "return", "dataset_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.dataset.st_dataset.SummDataset._concatenate_dataset_dicts": [[252, 273], ["set", "set.pop", "datasets.DatasetDict", "len", "ValueError", "datasets.concatenate_datasets", "tuple", "dataset_dict.keys"], "methods", ["None"], ["", "def", "_concatenate_dataset_dicts", "(", "\n", "self", ",", "dataset_dicts", ":", "List", "[", "DatasetDict", "]", "\n", ")", "->", "DatasetDict", ":", "\n", "        ", "\"\"\"\n        Concatenate two dataset dicts with similar splits and columns tinto one\n        :param dataset_dicts: A list of DatasetDicts\n        :rtype: DatasetDict containing the combined data\n        \"\"\"", "\n", "\n", "# Ensure all dataset dicts have the same splits", "\n", "setsofsplits", "=", "set", "(", "tuple", "(", "dataset_dict", ".", "keys", "(", ")", ")", "for", "dataset_dict", "in", "dataset_dicts", ")", "\n", "if", "len", "(", "setsofsplits", ")", ">", "1", ":", "\n", "            ", "raise", "ValueError", "(", "\"Splits must match for all datasets\"", ")", "\n", "\n", "# Concatenate all datasets into one according to the splits", "\n", "", "temp_dict", "=", "{", "}", "\n", "for", "split", "in", "setsofsplits", ".", "pop", "(", ")", ":", "\n", "            ", "split_set", "=", "[", "dataset_dict", "[", "split", "]", "for", "dataset_dict", "in", "dataset_dicts", "]", "\n", "temp_dict", "[", "split", "]", "=", "concatenate_datasets", "(", "split_set", ")", "\n", "\n", "", "return", "DatasetDict", "(", "temp_dict", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.dataset.st_dataset.SummDataset.generate_basic_description": [[274, 291], ["None"], "methods", ["None"], ["", "@", "classmethod", "\n", "def", "generate_basic_description", "(", "cls", ")", "->", "str", ":", "\n", "        ", "\"\"\"\n        Automatically generate the basic description string based on the attributes\n        :rtype: string containing the description\n        :param cls: class object\n        \"\"\"", "\n", "\n", "basic_description", "=", "(", "\n", "f\": {cls.dataset_name} is a \"", "\n", "f\"{'query-based ' if cls.is_query_based else ''}\"", "\n", "f\"{'dialogue ' if cls.is_dialogue_based else ''}\"", "\n", "f\"{'multi-document' if cls.is_multi_document else 'single-document'} \"", "\n", "f\"summarization dataset.\"", "\n", ")", "\n", "\n", "return", "basic_description", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.dataset.st_dataset.SummDataset.show_description": [[292, 298], ["print"], "methods", ["None"], ["", "@", "classmethod", "\n", "def", "show_description", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Print the description of the dataset.\n        \"\"\"", "\n", "print", "(", "self", ".", "dataset_name", ",", "\":\\n\"", ",", "self", ".", "description", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.dataset.st_dataset.CustomDataset.__init__": [[310, 355], ["st_dataset.CustomDataset._process_data", "st_dataset.CustomDataset._process_data", "st_dataset.CustomDataset._process_data", "ValueError"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.dataset.st_dataset.CustomDataset._process_data", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.dataset.st_dataset.CustomDataset._process_data", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.dataset.st_dataset.CustomDataset._process_data"], ["def", "__init__", "(", "\n", "self", ",", "\n", "train_set", ":", "List", "[", "Dict", "]", "=", "[", "]", ",", "\n", "test_set", ":", "List", "[", "Dict", "]", "=", "[", "]", ",", "\n", "validation_set", ":", "List", "[", "Dict", "]", "=", "[", "]", ",", "\n", "query_based", ":", "bool", "=", "False", ",", "\n", "multi_doc", ":", "bool", "=", "False", ",", "\n", "dialogue_based", ":", "bool", "=", "False", ",", "\n", ")", ":", "\n", "        ", "\"\"\"Create dataset information from the huggingface Dataset class\n        :rtype: dataset object\n        :param train_set: List[Dict], list of dictionaries that contain a data instance.\n            Contains the training examples and is in the form listed below.\n                The dictionary is in the form:\n                    {\"source\": \"source_data\", \"summary\": \"summary_data\", \"query\":\"query_data\"}\n                        * source_data is either of type List[str] or str\n                        * summary_data is of type str\n                        * query_data is of type str\n                The list of dictionaries looks as follows:\n                    [dictionary_instance_1, dictionary_instance_2, ...]\n        :param validation_set: Optional[List[Dict]], similar format to train_set\n            Contains the validation examples\n        :param test_set: Optional[List[Dict]], similar format to train_set\n            Contains the test examples\n        :param query_based: bool, Is the dataset query-based?\n        :param multi_doc: bool, Does each dataset instance source contain multiple documents\n        :param dialogue_based: Is the dataset dialogue-based?\n        \"\"\"", "\n", "\n", "if", "not", "(", "train_set", "or", "test_set", "or", "validation_set", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\"Missing data for the dataset\"", ")", "\n", "\n", "", "self", ".", "is_dialogue_based", "=", "dialogue_based", "\n", "self", ".", "is_multi_document", "=", "multi_doc", "\n", "self", ".", "is_query_based", "=", "query_based", "\n", "\n", "# Load the data into their respective splits", "\n", "self", ".", "_train_set", "=", "self", ".", "_process_data", "(", "train_set", ",", "\"Train\"", ")", "\n", "self", ".", "_validation_set", "=", "self", ".", "_process_data", "(", "validation_set", ",", "\"Validation\"", ")", "\n", "self", ".", "_test_set", "=", "self", ".", "_process_data", "(", "test_set", ",", "\"Test\"", ")", "\n", "\n", "self", ".", "dataset_name", "=", "None", "\n", "self", ".", "version", "=", "None", "\n", "\n", "self", ".", "dataset_name", "=", "\"Custom\"", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.dataset.st_dataset.CustomDataset._process_data": [[356, 393], ["data_instances.append", "TypeError", "st_dataset.SummInstance", "TypeError"], "methods", ["None"], ["", "def", "_process_data", "(", "\n", "self", ",", "data", ":", "Dataset", ",", "split", ":", "str", "\n", ")", "->", "Generator", "[", "SummInstance", ",", "None", ",", "None", "]", ":", "\n", "        ", "\"\"\"\n        This method processes the data contained in the dataset\n            and puts each data instance into a SummInstance object\n        :param dataset: a train/validation/test dataset\n        :rtype: a generator yielding SummInstance objects\n        \"\"\"", "\n", "data_instances", "=", "[", "]", "\n", "\n", "for", "instance", "in", "data", ":", "\n", "            ", "if", "\"source\"", "in", "instance", "and", "instance", "[", "\"source\"", "]", ":", "\n", "                ", "if", "self", ".", "is_dialogue_based", "or", "self", ".", "is_multi_document", ":", "\n", "                    ", "source", ":", "List", "=", "instance", "[", "\"source\"", "]", "\n", "", "else", ":", "\n", "                    ", "source", ":", "str", "=", "instance", "[", "\"source\"", "]", "\n", "", "", "else", ":", "\n", "                ", "raise", "TypeError", "(", "\"Missing source for a dataset instance\"", ")", "\n", "\n", "# TODO: Ensure models can handle datasets with no summaries", "\n", "", "summary", "=", "None", "\n", "if", "\"summary\"", "in", "instance", "and", "instance", "[", "\"summary\"", "]", ":", "\n", "                ", "summary", ":", "str", "=", "instance", "[", "\"summary\"", "]", "\n", "\n", "", "query", "=", "None", "\n", "if", "self", ".", "is_query_based", ":", "\n", "                ", "if", "\"query\"", "in", "instance", "and", "instance", "[", "\"query\"", "]", ":", "\n", "                    ", "query", ":", "str", "=", "instance", "[", "\"query\"", "]", "\n", "", "else", ":", "\n", "                    ", "raise", "TypeError", "(", "\"Missing query for a query-based dataset\"", ")", "\n", "\n", "", "", "data_instances", ".", "append", "(", "\n", "SummInstance", "(", "source", "=", "source", ",", "summary", "=", "summary", ",", "query", "=", "query", ")", "\n", ")", "\n", "\n", "", "return", "(", "data", "for", "data", "in", "data_instances", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.dataset.__init__.list_all_datasets": [[34, 42], ["ds.generate_basic_description", "all_datasets.append"], "function", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.multi_doc.multi_doc_joint_model.MultiDocJointModel.generate_basic_description"], ["def", "list_all_datasets", "(", ")", ":", "\n", "    ", "all_datasets", "=", "[", "]", "\n", "for", "ds", "in", "SUPPORTED_SUMM_DATASETS", ":", "\n", "        ", "dataset_description", "=", "ds", ".", "generate_basic_description", "(", ")", "\n", "\n", "all_datasets", ".", "append", "(", "(", "ds", ".", "dataset_name", ",", "dataset_description", ")", ")", "\n", "\n", "", "return", "all_datasets", "\n", "", ""]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.non_huggingface_datasets_builders.scisummnet.SummertimeScisummnet._info": [[45, 61], ["datasets.Features", "datasets.DatasetInfo", "datasets.Value", "datasets.Value", "datasets.Value", "datasets.Value"], "methods", ["None"], ["def", "_info", "(", "self", ")", ":", "\n", "        ", "features", "=", "datasets", ".", "Features", "(", "\n", "{", "\n", "\"entry_number\"", ":", "datasets", ".", "Value", "(", "\"string\"", ")", ",", "\n", "\"document_xml\"", ":", "datasets", ".", "Value", "(", "\"string\"", ")", ",", "\n", "\"citing_sentences_annotated.json\"", ":", "datasets", ".", "Value", "(", "\"string\"", ")", ",", "\n", "\"summary\"", ":", "datasets", ".", "Value", "(", "\"string\"", ")", ",", "\n", "}", "\n", ")", "\n", "return", "datasets", ".", "DatasetInfo", "(", "\n", "description", "=", "_DESCRIPTION", ",", "\n", "features", "=", "features", ",", "\n", "supervised_keys", "=", "None", ",", "\n", "homepage", "=", "_HOMEPAGE", ",", "\n", "license", "=", "_LICENSE", ",", "\n", "citation", "=", "_CITATION", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.non_huggingface_datasets_builders.scisummnet.SummertimeScisummnet._split_generators": [[63, 75], ["dl_manager.download_and_extract", "os.path.join", "datasets.SplitGenerator"], "methods", ["None"], ["", "def", "_split_generators", "(", "self", ",", "dl_manager", ")", ":", "\n", "        ", "\"\"\"Returns SplitGenerators.\"\"\"", "\n", "my_urls", "=", "_URLs", "\n", "path", "=", "dl_manager", ".", "download_and_extract", "(", "my_urls", ")", "\n", "trainpath", "=", "os", ".", "path", ".", "join", "(", "\n", "path", ",", "\"scisummnet_release1.1__20190413\"", ",", "\"top1000_complete\"", "\n", ")", "\n", "return", "[", "\n", "datasets", ".", "SplitGenerator", "(", "\n", "name", "=", "datasets", ".", "Split", ".", "TRAIN", ",", "\n", "# These kwargs will be passed to _generate_examples", "\n", "gen_kwargs", "=", "{", "\"extraction_path\"", ":", "trainpath", ",", "\"split\"", ":", "\"train\"", "}", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.non_huggingface_datasets_builders.scisummnet.SummertimeScisummnet._generate_examples": [[78, 106], ["os.listdir", "os.path.join", "os.path.join", "os.path.join", "open", "f.read", "open", "f.read", "open", "f.read"], "methods", ["None"], ["", "def", "_generate_examples", "(", "self", ",", "extraction_path", ",", "split", ")", ":", "\n", "        ", "\"\"\"Yields examples.\"\"\"", "\n", "\n", "for", "folder", "in", "os", ".", "listdir", "(", "extraction_path", ")", ":", "\n", "\n", "            ", "entry", "=", "{", "}", "\n", "\n", "entry", "[", "\"entry_number\"", "]", "=", "folder", "\n", "\n", "doc_xml_path", "=", "os", ".", "path", ".", "join", "(", "\n", "extraction_path", ",", "folder", ",", "\"Documents_xml\"", ",", "folder", "+", "\".xml\"", "\n", ")", "\n", "with", "open", "(", "doc_xml_path", ",", "\"r\"", ",", "encoding", "=", "\"utf-8\"", ")", "as", "f", ":", "\n", "                ", "entry", "[", "\"document_xml\"", "]", "=", "f", ".", "read", "(", ")", "\n", "\n", "", "cite_annot_path", "=", "os", ".", "path", ".", "join", "(", "\n", "extraction_path", ",", "folder", ",", "\"citing_sentences_annotated.json\"", "\n", ")", "\n", "with", "open", "(", "cite_annot_path", ",", "\"r\"", ",", "encoding", "=", "\"utf-8\"", ")", "as", "f", ":", "\n", "                ", "entry", "[", "\"citing_sentences_annotated.json\"", "]", "=", "f", ".", "read", "(", ")", "\n", "\n", "", "summary_path", "=", "os", ".", "path", ".", "join", "(", "\n", "extraction_path", ",", "folder", ",", "\"summary\"", ",", "folder", "+", "\".gold.txt\"", "\n", ")", "\n", "with", "open", "(", "summary_path", ",", "\"r\"", ",", "encoding", "=", "\"utf-8\"", ")", "as", "f", ":", "\n", "                ", "entry", "[", "\"summary\"", "]", "=", "f", ".", "read", "(", ")", "\n", "\n", "", "yield", "entry", "[", "\"entry_number\"", "]", ",", "entry", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.non_huggingface_datasets_builders.qmsum.SummertimeQmsum._info": [[42, 74], ["datasets.Features", "datasets.DatasetInfo", "datasets.Value", "datasets.Value", "datasets.Value", "datasets.Value", "datasets.Value", "datasets.Value", "datasets.Value", "datasets.Value"], "methods", ["None"], ["def", "_info", "(", "self", ")", ":", "\n", "        ", "features", "=", "datasets", ".", "Features", "(", "\n", "{", "\n", "\"entry_number\"", ":", "datasets", ".", "Value", "(", "\"string\"", ")", ",", "\n", "\"meeting_transcripts\"", ":", "[", "\n", "{", "\n", "\"speaker\"", ":", "datasets", ".", "Value", "(", "\"string\"", ")", ",", "\n", "\"content\"", ":", "datasets", ".", "Value", "(", "\"string\"", ")", ",", "\n", "}", "\n", "]", ",", "\n", "\"general_query_list\"", ":", "[", "\n", "{", "\n", "\"query\"", ":", "datasets", ".", "Value", "(", "\"string\"", ")", ",", "\n", "\"answer\"", ":", "datasets", ".", "Value", "(", "\"string\"", ")", ",", "\n", "}", "\n", "]", ",", "\n", "\"specific_query_list\"", ":", "[", "\n", "{", "\n", "\"query\"", ":", "datasets", ".", "Value", "(", "\"string\"", ")", ",", "\n", "\"answer\"", ":", "datasets", ".", "Value", "(", "\"string\"", ")", ",", "\n", "\"relevant_text_span\"", ":", "[", "[", "datasets", ".", "Value", "(", "\"string\"", ")", "]", "]", ",", "\n", "}", "\n", "]", ",", "\n", "}", "\n", ")", "\n", "return", "datasets", ".", "DatasetInfo", "(", "\n", "description", "=", "_DESCRIPTION", ",", "\n", "features", "=", "features", ",", "\n", "supervised_keys", "=", "None", ",", "\n", "homepage", "=", "_HOMEPAGE", ",", "\n", "license", "=", "None", ",", "\n", "citation", "=", "_CITATION", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.non_huggingface_datasets_builders.qmsum.SummertimeQmsum._split_generators": [[76, 100], ["dl_manager.download_and_extract", "datasets.SplitGenerator", "datasets.SplitGenerator", "datasets.SplitGenerator"], "methods", ["None"], ["", "def", "_split_generators", "(", "self", ",", "dl_manager", ")", ":", "\n", "        ", "\"\"\"Returns SplitGenerators.\"\"\"", "\n", "my_urls", "=", "_URLs", "\n", "downloaded_files", "=", "dl_manager", ".", "download_and_extract", "(", "my_urls", ")", "\n", "\n", "trainpath", "=", "downloaded_files", "[", "\"train\"", "]", "\n", "valpath", "=", "downloaded_files", "[", "\"val\"", "]", "\n", "testpath", "=", "downloaded_files", "[", "\"test\"", "]", "\n", "\n", "return", "[", "\n", "datasets", ".", "SplitGenerator", "(", "\n", "name", "=", "datasets", ".", "Split", ".", "TRAIN", ",", "\n", "# These kwargs will be passed to _generate_examples", "\n", "gen_kwargs", "=", "{", "\"filepath\"", ":", "trainpath", ",", "\"split\"", ":", "\"train\"", "}", ",", "\n", ")", ",", "\n", "datasets", ".", "SplitGenerator", "(", "\n", "name", "=", "datasets", ".", "Split", ".", "VALIDATION", ",", "\n", "# These kwargs will be passed to _generate_examples", "\n", "gen_kwargs", "=", "{", "\"filepath\"", ":", "valpath", ",", "\"split\"", ":", "\"val\"", "}", ",", "\n", ")", ",", "\n", "datasets", ".", "SplitGenerator", "(", "\n", "name", "=", "datasets", ".", "Split", ".", "TEST", ",", "\n", "# These kwargs will be passed to _generate_examples", "\n", "gen_kwargs", "=", "{", "\"filepath\"", ":", "testpath", ",", "\"split\"", ":", "\"test\"", "}", ",", "\n", ")", ",", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.non_huggingface_datasets_builders.qmsum.SummertimeQmsum._generate_examples": [[103, 120], ["os.path.join", "open", "enumerate", "json.loads", "str"], "methods", ["None"], ["", "def", "_generate_examples", "(", "self", ",", "filepath", ",", "split", ")", ":", "\n", "        ", "\"\"\"Yields examples.\"\"\"", "\n", "\n", "extraction_path", "=", "os", ".", "path", ".", "join", "(", "filepath", ")", "\n", "\n", "with", "open", "(", "extraction_path", ")", "as", "f", ":", "\n", "            ", "for", "i", ",", "line", "in", "enumerate", "(", "f", ")", ":", "\n", "\n", "                ", "instance", "=", "json", ".", "loads", "(", "line", ")", "\n", "\n", "entry", "=", "{", "}", "\n", "entry", "[", "\"entry_number\"", "]", "=", "split", "+", "\"_\"", "+", "str", "(", "i", ")", "\n", "entry", "[", "\"meeting_transcripts\"", "]", "=", "instance", "[", "\"meeting_transcripts\"", "]", "\n", "entry", "[", "\"general_query_list\"", "]", "=", "instance", "[", "\"general_query_list\"", "]", "\n", "entry", "[", "\"specific_query_list\"", "]", "=", "instance", "[", "\"specific_query_list\"", "]", "\n", "\n", "yield", "entry", "[", "\"entry_number\"", "]", ",", "entry", "\n", "", "", "", "", ""]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.non_huggingface_datasets_builders.massivesumm.SummertimeMassivesumm._info": [[149, 164], ["datasets.Features", "datasets.DatasetInfo", "datasets.Value", "datasets.Value", "datasets.Value"], "methods", ["None"], ["def", "_info", "(", "self", ")", ":", "\n", "        ", "features", "=", "datasets", ".", "Features", "(", "\n", "{", "\n", "\"article_url\"", ":", "datasets", ".", "Value", "(", "\"string\"", ")", ",", "\n", "\"article\"", ":", "datasets", ".", "Value", "(", "\"string\"", ")", ",", "\n", "\"summary\"", ":", "datasets", ".", "Value", "(", "\"string\"", ")", ",", "\n", "}", "\n", ")", "\n", "return", "datasets", ".", "DatasetInfo", "(", "\n", "description", "=", "_DESCRIPTION", ",", "\n", "features", "=", "features", ",", "\n", "supervised_keys", "=", "None", ",", "\n", "homepage", "=", "_HOMEPAGE", ",", "\n", "license", "=", "_LICENSE", ",", "\n", "citation", "=", "_CITATION", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.non_huggingface_datasets_builders.massivesumm.SummertimeMassivesumm._split_generators": [[166, 193], ["gdown.cached_download", "summertime.util.massivesumm_utils.massivesumm_extract_from_url", "dl_manager.download", "open", "datasets.SplitGenerator", "f.write"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.util.massivesumm_utils.massivesumm_extract_from_url"], ["", "def", "_split_generators", "(", "self", ",", "dl_manager", ")", ":", "\n", "        ", "\"\"\"Returns SplitGenerators.\"\"\"", "\n", "# get google drive file id", "\n", "drive_id", "=", "_URL_IDs", "[", "self", ".", "config", ".", "name", "]", "\n", "# get download url for the file", "\n", "url", "=", "f\"https://drive.google.com/uc?id={drive_id}&export=download\"", "\n", "\n", "path", "=", "gdown", ".", "cached_download", "(", "url", ")", "\n", "\n", "# download webpages and scrape summaries into json format", "\n", "data", "=", "massivesumm_extract_from_url", "(", "path", ")", "\n", "\n", "# get a modifiable cached file by attempting a download", "\n", "data_dir", "=", "dl_manager", ".", "download", "(", "url", ")", "\n", "\n", "# save the extracted data to the data_dir", "\n", "with", "open", "(", "data_dir", ",", "\"w\"", ",", "encoding", "=", "\"utf-8\"", ")", "as", "f", ":", "\n", "            ", "for", "line", "in", "data", ":", "\n", "                ", "f", ".", "write", "(", "line", "+", "\"\\n\"", ")", "\n", "\n", "", "", "return", "[", "\n", "datasets", ".", "SplitGenerator", "(", "\n", "name", "=", "datasets", ".", "Split", ".", "TRAIN", ",", "\n", "# These kwargs will be passed to _generate_examples", "\n", "gen_kwargs", "=", "{", "\n", "\"filepath\"", ":", "data_dir", ",", "\n", "\"split\"", ":", "\"train\"", ",", "\n", "}", ",", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.non_huggingface_datasets_builders.massivesumm.SummertimeMassivesumm._generate_examples": [[197, 211], ["open", "json.loads", "data[].replace"], "methods", ["None"], ["", "def", "_generate_examples", "(", "self", ",", "filepath", ",", "split", ")", ":", "\n", "        ", "\"\"\"Yields examples.\"\"\"", "\n", "\n", "with", "open", "(", "filepath", ",", "encoding", "=", "\"utf-8\"", ")", "as", "f", ":", "\n", "            ", "for", "row", "in", "f", ":", "\n", "                ", "data", "=", "json", ".", "loads", "(", "row", ")", "\n", "\n", "entry", "=", "{", "\n", "\"article_url\"", ":", "data", "[", "\"url\"", "]", ",", "\n", "# split article by newlines", "\n", "\"article\"", ":", "data", "[", "\"text\"", "]", ".", "replace", "(", "\"\\n\"", ",", "\" \"", ")", ",", "\n", "\"summary\"", ":", "data", "[", "\"summary\"", "]", ",", "\n", "}", "\n", "yield", "entry", "[", "\"article_url\"", "]", ",", "entry", "\n", "", "", "", "", ""]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.non_huggingface_datasets_builders.arxiv_longsummarization.SummertimeArxiv._info": [[46, 61], ["datasets.Features", "datasets.DatasetInfo", "datasets.Value", "datasets.Value", "datasets.Value"], "methods", ["None"], ["def", "_info", "(", "self", ")", ":", "\n", "        ", "features", "=", "datasets", ".", "Features", "(", "\n", "{", "\n", "\"article_id\"", ":", "datasets", ".", "Value", "(", "\"string\"", ")", ",", "\n", "\"article_text\"", ":", "[", "datasets", ".", "Value", "(", "\"string\"", ")", "]", ",", "\n", "\"abstract_text\"", ":", "[", "datasets", ".", "Value", "(", "\"string\"", ")", "]", ",", "\n", "}", "\n", ")", "\n", "return", "datasets", ".", "DatasetInfo", "(", "\n", "description", "=", "_DESCRIPTION", ",", "\n", "features", "=", "features", ",", "\n", "supervised_keys", "=", "None", ",", "\n", "homepage", "=", "_HOMEPAGE", ",", "\n", "license", "=", "_LICENSE", ",", "\n", "citation", "=", "_CITATION", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.non_huggingface_datasets_builders.arxiv_longsummarization.SummertimeArxiv._split_generators": [[63, 88], ["dl_manager.download_and_extract", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "datasets.SplitGenerator", "datasets.SplitGenerator", "datasets.SplitGenerator"], "methods", ["None"], ["", "def", "_split_generators", "(", "self", ",", "dl_manager", ")", ":", "\n", "        ", "\"\"\"Returns SplitGenerators.\"\"\"", "\n", "my_urls", "=", "_URL", "\n", "path", "=", "dl_manager", ".", "download_and_extract", "(", "my_urls", ")", "\n", "path", "=", "os", ".", "path", ".", "join", "(", "path", ",", "\"arxiv-dataset\"", ")", "\n", "\n", "trainpath", "=", "os", ".", "path", ".", "join", "(", "path", ",", "\"train.txt\"", ")", "\n", "valpath", "=", "os", ".", "path", ".", "join", "(", "path", ",", "\"val.txt\"", ")", "\n", "testpath", "=", "os", ".", "path", ".", "join", "(", "path", ",", "\"test.txt\"", ")", "\n", "\n", "return", "[", "\n", "datasets", ".", "SplitGenerator", "(", "\n", "name", "=", "datasets", ".", "Split", ".", "TRAIN", ",", "\n", "# These kwargs will be passed to _generate_examples", "\n", "gen_kwargs", "=", "{", "\"filepath\"", ":", "trainpath", ",", "\"split\"", ":", "\"train\"", "}", ",", "\n", ")", ",", "\n", "datasets", ".", "SplitGenerator", "(", "\n", "name", "=", "datasets", ".", "Split", ".", "VALIDATION", ",", "\n", "# These kwargs will be passed to _generate_examples", "\n", "gen_kwargs", "=", "{", "\"filepath\"", ":", "valpath", ",", "\"split\"", ":", "\"val\"", "}", ",", "\n", ")", ",", "\n", "datasets", ".", "SplitGenerator", "(", "\n", "name", "=", "datasets", ".", "Split", ".", "TEST", ",", "\n", "# These kwargs will be passed to _generate_examples", "\n", "gen_kwargs", "=", "{", "\"filepath\"", ":", "testpath", ",", "\"split\"", ":", "\"test\"", "}", ",", "\n", ")", ",", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.non_huggingface_datasets_builders.arxiv_longsummarization.SummertimeArxiv._generate_examples": [[91, 105], ["open", "json.loads"], "methods", ["None"], ["", "def", "_generate_examples", "(", "self", ",", "filepath", ",", "split", ")", ":", "\n", "        ", "\"\"\"Yields examples.\"\"\"", "\n", "\n", "with", "open", "(", "filepath", ",", "\"r\"", ")", "as", "f", ":", "\n", "            ", "for", "line", "in", "f", ":", "\n", "\n", "                ", "instance", "=", "json", ".", "loads", "(", "line", ")", "\n", "\n", "entry", "=", "{", "}", "\n", "entry", "[", "\"article_id\"", "]", "=", "instance", "[", "\"article_id\"", "]", "\n", "entry", "[", "\"article_text\"", "]", "=", "instance", "[", "\"article_text\"", "]", "\n", "entry", "[", "\"abstract_text\"", "]", "=", "instance", "[", "\"abstract_text\"", "]", "\n", "\n", "yield", "entry", "[", "\"article_id\"", "]", ",", "entry", "\n", "", "", "", "", ""]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.non_huggingface_datasets_builders.summscreen.SummertimeSummscreen._info": [[50, 65], ["datasets.Features", "datasets.DatasetInfo", "datasets.Value", "datasets.features.Sequence", "datasets.Value", "datasets.Value"], "methods", ["None"], ["def", "_info", "(", "self", ")", ":", "\n", "        ", "features", "=", "datasets", ".", "Features", "(", "\n", "{", "\n", "\"entry_number\"", ":", "datasets", ".", "Value", "(", "\"string\"", ")", ",", "\n", "\"transcript\"", ":", "datasets", ".", "features", ".", "Sequence", "(", "datasets", ".", "Value", "(", "\"string\"", ")", ")", ",", "\n", "\"recap\"", ":", "datasets", ".", "Value", "(", "\"string\"", ")", ",", "\n", "}", "\n", ")", "\n", "return", "datasets", ".", "DatasetInfo", "(", "\n", "description", "=", "_DESCRIPTION", ",", "\n", "features", "=", "features", ",", "\n", "supervised_keys", "=", "None", ",", "\n", "homepage", "=", "_HOMEPAGE", ",", "\n", "license", "=", "_LICENSE", ",", "\n", "citation", "=", "_CITATION", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.non_huggingface_datasets_builders.summscreen.SummertimeSummscreen._split_generators": [[67, 100], ["dl_manager.download_and_extract", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "datasets.SplitGenerator", "datasets.SplitGenerator", "datasets.SplitGenerator"], "methods", ["None"], ["", "def", "_split_generators", "(", "self", ",", "dl_manager", ")", ":", "\n", "        ", "\"\"\"Returns SplitGenerators.\"\"\"", "\n", "my_urls", "=", "_URLs", "\n", "path", "=", "dl_manager", ".", "download_and_extract", "(", "my_urls", ")", "\n", "path", "=", "os", ".", "path", ".", "join", "(", "path", ",", "\"SummScreen\"", ")", "\n", "\n", "trainpath_fd", "=", "os", ".", "path", ".", "join", "(", "\"ForeverDreaming\"", ",", "\"fd_train.json\"", ")", "\n", "trainpath_tms", "=", "os", ".", "path", ".", "join", "(", "\"TVMegaSite\"", ",", "\"tms_train.json\"", ")", "\n", "trainpaths", "=", "[", "trainpath_fd", ",", "trainpath_tms", "]", "\n", "\n", "devpath_fd", "=", "os", ".", "path", ".", "join", "(", "\"ForeverDreaming\"", ",", "\"fd_dev.json\"", ")", "\n", "devpath_tms", "=", "os", ".", "path", ".", "join", "(", "\"TVMegaSite\"", ",", "\"tms_dev.json\"", ")", "\n", "devpaths", "=", "[", "devpath_fd", ",", "devpath_tms", "]", "\n", "\n", "testpath_fd", "=", "os", ".", "path", ".", "join", "(", "\"ForeverDreaming\"", ",", "\"fd_test.json\"", ")", "\n", "testpath_tms", "=", "os", ".", "path", ".", "join", "(", "\"TVMegaSite\"", ",", "\"tms_test.json\"", ")", "\n", "testpaths", "=", "[", "testpath_fd", ",", "testpath_tms", "]", "\n", "\n", "return", "[", "\n", "datasets", ".", "SplitGenerator", "(", "\n", "name", "=", "datasets", ".", "Split", ".", "TRAIN", ",", "\n", "# These kwargs will be passed to _generate_examples", "\n", "gen_kwargs", "=", "{", "\"filepaths\"", ":", "(", "path", ",", "trainpaths", ")", ",", "\"split\"", ":", "\"train\"", "}", ",", "\n", ")", ",", "\n", "datasets", ".", "SplitGenerator", "(", "\n", "name", "=", "datasets", ".", "Split", ".", "VALIDATION", ",", "\n", "# These kwargs will be passed to _generate_examples", "\n", "gen_kwargs", "=", "{", "\"filepaths\"", ":", "(", "path", ",", "devpaths", ")", ",", "\"split\"", ":", "\"dev\"", "}", ",", "\n", ")", ",", "\n", "datasets", ".", "SplitGenerator", "(", "\n", "name", "=", "datasets", ".", "Split", ".", "TEST", ",", "\n", "# These kwargs will be passed to _generate_examples", "\n", "gen_kwargs", "=", "{", "\"filepaths\"", ":", "(", "path", ",", "testpaths", ")", ",", "\"split\"", ":", "\"test\"", "}", ",", "\n", ")", ",", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.non_huggingface_datasets_builders.summscreen.SummertimeSummscreen._generate_examples": [[103, 124], ["os.path.join", "open", "line.replace", "json.loads"], "methods", ["None"], ["", "def", "_generate_examples", "(", "self", ",", "filepaths", ",", "split", ")", ":", "\n", "        ", "\"\"\"Yields examples.\"\"\"", "\n", "\n", "path", ",", "relative_filepaths", "=", "filepaths", "\n", "for", "filepath", "in", "relative_filepaths", ":", "\n", "\n", "            ", "extraction_path", "=", "os", ".", "path", ".", "join", "(", "path", ",", "filepath", ")", "\n", "\n", "with", "open", "(", "extraction_path", ",", "\"r\"", ")", "as", "f", ":", "\n", "                ", "for", "line", "in", "f", ":", "\n", "                    ", "processed_line", "=", "line", ".", "replace", "(", "\"@@ \"", ",", "\"\"", ")", "\n", "instance", "=", "json", ".", "loads", "(", "processed_line", ")", "\n", "\n", "entry", "=", "{", "}", "\n", "entry", "[", "\"entry_number\"", "]", "=", "instance", "[", "\"filename\"", "]", "\n", "entry", "[", "\"transcript\"", "]", "=", "instance", "[", "\"Transcript\"", "]", "\n", "entry", "[", "\"recap\"", "]", "=", "instance", "[", "\"Recap\"", "]", "[", "\n", "0", "\n", "]", "# Recap is a single string in list", "\n", "\n", "yield", "entry", "[", "\"entry_number\"", "]", ",", "entry", "\n", "", "", "", "", "", ""]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.model.defaults.summarizer.__init__": [[5, 7], ["single_doc.PegasusModel.__init__"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.tests.model_selector_test.ToyModel.__init__"], ["    ", "def", "__init__", "(", "self", ",", "device", "=", "\"cpu\"", ")", ":", "\n", "        ", "super", "(", "summarizer", ",", "self", ")", ".", "__init__", "(", "device", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.model.defaults.summarizer.show_capability": [[8, 11], ["print", "super().show_capability"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.multi_doc.multi_doc_joint_model.MultiDocJointModel.show_capability"], ["", "def", "show_capability", "(", "self", ")", ":", "\n", "        ", "print", "(", "\"Pegasus is the default single-document summarization model.\"", ")", "\n", "super", "(", "summarizer", ",", "self", ")", ".", "show_capability", "(", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.model.__init__.list_all_models": [[36, 44], ["model_class.generate_basic_description", "all_model_tuples.append"], "function", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.multi_doc.multi_doc_joint_model.MultiDocJointModel.generate_basic_description"], ["for", "ds", "in", "SUPPORTED_SUMM_DATASETS", ":", "\n", "        ", "dataset_description", "=", "ds", ".", "generate_basic_description", "(", ")", "\n", "\n", "all_datasets", ".", "append", "(", "(", "ds", ".", "dataset_name", ",", "dataset_description", ")", ")", "\n", "\n", "", "return", "all_datasets", "\n", "", ""]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.model.base_model.SummModel.__init__": [[18, 27], ["None"], "methods", ["None"], ["def", "__init__", "(", "\n", "self", ",", "\n", "trained_domain", ":", "str", "=", "None", ",", "\n", "max_input_length", ":", "int", "=", "None", ",", "\n", "max_output_length", ":", "int", "=", "None", ",", "\n", ")", ":", "\n", "        ", "self", ".", "trained_domain", "=", "trained_domain", "\n", "self", ".", "max_input_length", "=", "max_input_length", "\n", "self", ".", "max_output_length", "=", "max_output_length", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.model.base_model.SummModel.summarize": [[28, 41], ["NotImplementedError"], "methods", ["None"], ["", "def", "summarize", "(", "\n", "self", ",", "corpus", ":", "Union", "[", "List", "[", "str", "]", ",", "List", "[", "List", "[", "str", "]", "]", "]", ",", "queries", ":", "List", "[", "str", "]", "=", "None", "\n", ")", "->", "List", "[", "str", "]", ":", "\n", "        ", "\"\"\"\n        All summarization models should have this function\n\n        :param corpus: each string in the list is a source document to be summarized; if the model is multi-document or\n            dialogue summarization model, then each instance contains a list of documents/utterances\n        :param queries: a list of queries if this is a query-based model\n        :return: a list of generated summaries\n        \"\"\"", "\n", "raise", "NotImplementedError", "(", "\n", "\"The base class for models shouldn't be instantiated!\"", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.model.base_model.SummModel.assert_summ_input_type": [[43, 57], ["NotImplementedError"], "methods", ["None"], ["", "@", "classmethod", "\n", "def", "assert_summ_input_type", "(", "\n", "cls", ",", "corpus", ":", "Union", "[", "List", "[", "str", "]", ",", "List", "[", "List", "[", "str", "]", "]", "]", ",", "queries", ":", "Union", "[", "List", "[", "str", "]", ",", "None", "]", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Verifies that type of input corpus or queries for summarization align with the model type.\n\n        For single-doc models: also verifies that language of input corpus\n        and queries for summarization align with the model type.\n\n        Returns the ISO-639 language tag of the input corpus as a string (for single-doc models).\n        \"\"\"", "\n", "raise", "NotImplementedError", "(", "\n", "\"The base class for models shouldn't be instantiated!\"", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.model.base_model.SummModel.show_capability": [[59, 66], ["NotImplementedError"], "methods", ["None"], ["", "@", "classmethod", "\n", "def", "show_capability", "(", "cls", ")", "->", "None", ":", "\n", "        ", "\"\"\"\n        Use concise language to show the strength and weakness for each model. Try not to use NLP terminologies\n        \"\"\"", "\n", "raise", "NotImplementedError", "(", "\n", "\"The base class for models shouldn't be instantiated!\"", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.model.base_model.SummModel.generate_basic_description": [[68, 88], ["None"], "methods", ["None"], ["", "@", "classmethod", "\n", "def", "generate_basic_description", "(", "cls", ")", "->", "str", ":", "\n", "        ", "\"\"\"\n        Automatically generate the basic description string based on the attributes\n        \"\"\"", "\n", "extractive_abstractive", "=", "\"extractive\"", "if", "cls", ".", "is_extractive", "else", "\"abstractive\"", "\n", "neural", "=", "\"neural\"", "if", "cls", ".", "is_neural", "else", "\"non-neural\"", "\n", "\n", "basic_description", "=", "(", "\n", "f\"{cls.model_name} is a\"", "\n", "f\"{'query-based' if cls.is_query_based else ''} \"", "\n", "f\"{extractive_abstractive}, {neural} model for summarization.\"", "\n", ")", "\n", "if", "cls", ".", "is_multi_document", "or", "cls", ".", "is_dialogue_based", ":", "\n", "            ", "basic_description", "+=", "(", "\n", "f\"It can handle {'multi-document' if cls.is_multi_document else ''} \"", "\n", "f\"{'dialogue' if cls.is_dialogue_based else ''} textual data.\"", "\n", ")", "\n", "\n", "", "return", "basic_description", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.model.base_model.SummPipeline.__init__": [[106, 113], ["base_model.SummModel.__init__"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.tests.model_selector_test.ToyModel.__init__"], ["def", "__init__", "(", "\n", "self", ",", "trained_domain", ":", "str", ",", "max_input_length", ":", "int", ",", "max_output_length", ":", "int", "\n", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "\n", "trained_domain", "=", "trained_domain", ",", "\n", "max_input_length", "=", "max_input_length", ",", "\n", "max_output_length", "=", "max_output_length", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_transfo_xl.TransfoXLTokenizer.__init__": [[87, 156], ["tokenization_utils.PreTrainedTokenizer.__init__", "collections.Counter", "re.compile", "tokenization_transfo_xl.TransfoXLTokenizer._compile_space_around_punctuation_pattern", "tokenization_transfo_xl.TransfoXLTokenizer.build_vocab", "torch.load", "torch.load.items", "tokenization_transfo_xl.TransfoXLTokenizer.build_vocab", "ValueError"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.tests.model_selector_test.ToyModel.__init__", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_transfo_xl.TransfoXLTokenizer._compile_space_around_punctuation_pattern", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_transfo_xl.TransfoXLTokenizer.build_vocab", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.BaseTrainer.BaseTrainer.load", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_transfo_xl.TransfoXLTokenizer.build_vocab"], ["def", "__init__", "(", "\n", "self", ",", "\n", "special", "=", "None", ",", "\n", "min_freq", "=", "0", ",", "\n", "max_size", "=", "None", ",", "\n", "lower_case", "=", "False", ",", "\n", "delimiter", "=", "None", ",", "\n", "vocab_file", "=", "None", ",", "\n", "pretrained_vocab_file", "=", "None", ",", "\n", "never_split", "=", "None", ",", "\n", "unk_token", "=", "\"<unk>\"", ",", "\n", "eos_token", "=", "\"<eos>\"", ",", "\n", "additional_special_tokens", "=", "[", "\"<formula>\"", "]", ",", "\n", "**", "kwargs", "\n", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "\n", "unk_token", "=", "unk_token", ",", "\n", "eos_token", "=", "eos_token", ",", "\n", "additional_special_tokens", "=", "additional_special_tokens", ",", "\n", "**", "kwargs", ",", "\n", ")", "\n", "\n", "self", ".", "max_len_single_sentence", "=", "(", "\n", "self", ".", "max_len", "\n", ")", "# no default special tokens - you can update this value if you add special tokens", "\n", "self", ".", "max_len_sentences_pair", "=", "(", "\n", "self", ".", "max_len", "\n", ")", "# no default special tokens - you can update this value if you add special tokens", "\n", "\n", "if", "never_split", "is", "None", ":", "\n", "            ", "never_split", "=", "self", ".", "all_special_tokens", "\n", "", "if", "special", "is", "None", ":", "\n", "            ", "special", "=", "[", "]", "\n", "", "self", ".", "counter", "=", "Counter", "(", ")", "\n", "self", ".", "special", "=", "special", "\n", "self", ".", "min_freq", "=", "min_freq", "\n", "self", ".", "max_size", "=", "max_size", "\n", "self", ".", "lower_case", "=", "lower_case", "\n", "self", ".", "delimiter", "=", "delimiter", "\n", "self", ".", "vocab_file", "=", "vocab_file", "\n", "self", ".", "never_split", "=", "never_split", "\n", "self", ".", "punctuation_symbols", "=", "'!\"#$%&()*+,-./\\:;<=>?@[\\\\]^_`{|}~'", "# noqa: W605", "\n", "self", ".", "punction_without_space_before_pattern", "=", "re", ".", "compile", "(", "\n", "r\"[^\\s][{}]\"", ".", "format", "(", "self", ".", "punctuation_symbols", ")", "\n", ")", "\n", "self", ".", "punctuation_with_space_around_pattern", "=", "(", "\n", "self", ".", "_compile_space_around_punctuation_pattern", "(", ")", "\n", ")", "\n", "\n", "try", ":", "\n", "            ", "if", "pretrained_vocab_file", "is", "not", "None", ":", "\n", "# Hack because, honestly this tokenizer was not made to be used", "\n", "# in a library like ours, at all.", "\n", "                ", "vocab_dict", "=", "torch", ".", "load", "(", "pretrained_vocab_file", ")", "\n", "for", "key", ",", "value", "in", "vocab_dict", ".", "items", "(", ")", ":", "\n", "                    ", "if", "key", "not", "in", "self", ".", "__dict__", ":", "\n", "                        ", "self", ".", "__dict__", "[", "key", "]", "=", "value", "\n", "\n", "", "", "", "if", "vocab_file", "is", "not", "None", ":", "\n", "                ", "self", ".", "build_vocab", "(", ")", "\n", "", "", "except", "Exception", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"Unable to parse file {}. Unknown format. \"", "\n", "\"If you tried to load a model saved through TransfoXLTokenizerFast,\"", "\n", "\"please note they are not compatible.\"", ".", "format", "(", "pretrained_vocab_file", ")", "\n", ")", "\n", "\n", "", "if", "vocab_file", "is", "not", "None", ":", "\n", "            ", "self", ".", "build_vocab", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_transfo_xl.TransfoXLTokenizer._compile_space_around_punctuation_pattern": [[157, 162], ["re.compile"], "methods", ["None"], ["", "", "def", "_compile_space_around_punctuation_pattern", "(", "self", ")", ":", "\n", "        ", "look_ahead_for_special_token", "=", "\"(?=[{}])\"", ".", "format", "(", "self", ".", "punctuation_symbols", ")", "\n", "look_ahead_to_match_all_except_space", "=", "\"(?=[^\\s])\"", "# noqa: W605", "\n", "return", "re", ".", "compile", "(", "\n", "r\"\"", "+", "look_ahead_for_special_token", "+", "look_ahead_to_match_all_except_space", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_transfo_xl.TransfoXLTokenizer.count_file": [[164, 179], ["os.path.exists", "logger.info", "open", "enumerate", "tokenization_transfo_xl.TransfoXLTokenizer.tokenize", "tokenization_transfo_xl.TransfoXLTokenizer.counter.update", "sents.append", "logger.info"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizerFast.tokenize", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.HMNetTrainer.HMNetTrainer.update"], ["", "def", "count_file", "(", "self", ",", "path", ",", "verbose", "=", "False", ",", "add_eos", "=", "False", ")", ":", "\n", "        ", "if", "verbose", ":", "\n", "            ", "logger", ".", "info", "(", "\"counting file {} ...\"", ".", "format", "(", "path", ")", ")", "\n", "", "assert", "os", ".", "path", ".", "exists", "(", "path", ")", "\n", "\n", "sents", "=", "[", "]", "\n", "with", "open", "(", "path", ",", "\"r\"", ",", "encoding", "=", "\"utf-8\"", ")", "as", "f", ":", "\n", "            ", "for", "idx", ",", "line", "in", "enumerate", "(", "f", ")", ":", "\n", "                ", "if", "verbose", "and", "idx", ">", "0", "and", "idx", "%", "500000", "==", "0", ":", "\n", "                    ", "logger", ".", "info", "(", "\"    line {}\"", ".", "format", "(", "idx", ")", ")", "\n", "", "symbols", "=", "self", ".", "tokenize", "(", "line", ",", "add_eos", "=", "add_eos", ")", "\n", "self", ".", "counter", ".", "update", "(", "symbols", ")", "\n", "sents", ".", "append", "(", "symbols", ")", "\n", "\n", "", "", "return", "sents", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_transfo_xl.TransfoXLTokenizer.count_sents": [[180, 190], ["enumerate", "logger.info", "tokenization_transfo_xl.TransfoXLTokenizer.counter.update", "logger.info", "len"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.HMNetTrainer.HMNetTrainer.update"], ["", "def", "count_sents", "(", "self", ",", "sents", ",", "verbose", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        sents : a list of sentences, each a list of tokenized symbols\n        \"\"\"", "\n", "if", "verbose", ":", "\n", "            ", "logger", ".", "info", "(", "\"counting {} sents ...\"", ".", "format", "(", "len", "(", "sents", ")", ")", ")", "\n", "", "for", "idx", ",", "symbols", "in", "enumerate", "(", "sents", ")", ":", "\n", "            ", "if", "verbose", "and", "idx", ">", "0", "and", "idx", "%", "500000", "==", "0", ":", "\n", "                ", "logger", ".", "info", "(", "\"    line {}\"", ".", "format", "(", "idx", ")", ")", "\n", "", "self", ".", "counter", ".", "update", "(", "symbols", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_transfo_xl.TransfoXLTokenizer._build_from_file": [[191, 205], ["collections.OrderedDict", "open", "tokenization_transfo_xl.TransfoXLTokenizer.add_symbol", "ValueError", "line.strip().split", "line.strip"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_transfo_xl.TransfoXLTokenizer.add_symbol", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.utils.sentence_splitter.PunktSentenceSplitter.split"], ["", "", "def", "_build_from_file", "(", "self", ",", "vocab_file", ")", ":", "\n", "        ", "self", ".", "idx2sym", "=", "[", "]", "\n", "self", ".", "sym2idx", "=", "OrderedDict", "(", ")", "\n", "\n", "with", "open", "(", "vocab_file", ",", "\"r\"", ",", "encoding", "=", "\"utf-8\"", ")", "as", "f", ":", "\n", "            ", "for", "line", "in", "f", ":", "\n", "                ", "symb", "=", "line", ".", "strip", "(", ")", ".", "split", "(", ")", "[", "0", "]", "\n", "self", ".", "add_symbol", "(", "symb", ")", "\n", "", "", "if", "\"<UNK>\"", "in", "self", ".", "sym2idx", ":", "\n", "            ", "self", ".", "unk_idx", "=", "self", ".", "sym2idx", "[", "\"<UNK>\"", "]", "\n", "", "elif", "\"<unk>\"", "in", "self", ".", "sym2idx", ":", "\n", "            ", "self", ".", "unk_idx", "=", "self", ".", "sym2idx", "[", "\"<unk>\"", "]", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"No <unkown> token in vocabulary\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_transfo_xl.TransfoXLTokenizer.save_vocabulary": [[206, 231], ["logger.warning", "os.path.isdir", "torch.save", "os.path.join"], "methods", ["None"], ["", "", "def", "save_vocabulary", "(", "self", ",", "vocab_path", ")", ":", "\n", "        ", "\"\"\"\n        Save the vocabulary and special tokens file to a directory.\n\n        Args:\n            vocab_path (:obj:`str`):\n                The directory in which to save the vocabulary.\n\n        Returns:\n            :obj:`Tuple(str)`: Paths to the files saved.\n        \"\"\"", "\n", "\n", "logger", ".", "warning", "(", "\n", "\"Please note you will not be able to load the save vocabulary in\"", "\n", "\" Rust-based TransfoXLTokenizerFast as they don't share the same structure.\"", "\n", ")", "\n", "\n", "if", "os", ".", "path", ".", "isdir", "(", "vocab_path", ")", ":", "\n", "            ", "vocab_file", "=", "os", ".", "path", ".", "join", "(", "\n", "vocab_path", ",", "VOCAB_FILES_NAMES", "[", "\"pretrained_vocab_file\"", "]", "\n", ")", "\n", "", "else", ":", "\n", "            ", "vocab_file", "=", "vocab_path", "\n", "", "torch", ".", "save", "(", "self", ".", "__dict__", ",", "vocab_file", ")", "\n", "return", "(", "vocab_file", ",", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_transfo_xl.TransfoXLTokenizer.build_vocab": [[232, 257], ["logger.info", "tokenization_transfo_xl.TransfoXLTokenizer._build_from_file", "logger.info", "logger.info", "collections.OrderedDict", "tokenization_transfo_xl.TransfoXLTokenizer.counter.most_common", "logger.info", "tokenization_transfo_xl.TransfoXLTokenizer.add_special", "tokenization_transfo_xl.TransfoXLTokenizer.add_symbol", "len", "len", "len"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_transfo_xl.TransfoXLTokenizer._build_from_file", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_transfo_xl.TransfoXLTokenizer.add_special", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_transfo_xl.TransfoXLTokenizer.add_symbol"], ["", "def", "build_vocab", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "vocab_file", ":", "\n", "            ", "logger", ".", "info", "(", "\"building vocab from {}\"", ".", "format", "(", "self", ".", "vocab_file", ")", ")", "\n", "self", ".", "_build_from_file", "(", "self", ".", "vocab_file", ")", "\n", "logger", ".", "info", "(", "\"final vocab size {}\"", ".", "format", "(", "len", "(", "self", ")", ")", ")", "\n", "", "else", ":", "\n", "            ", "logger", ".", "info", "(", "\n", "\"building vocab with min_freq={}, max_size={}\"", ".", "format", "(", "\n", "self", ".", "min_freq", ",", "self", ".", "max_size", "\n", ")", "\n", ")", "\n", "self", ".", "idx2sym", "=", "[", "]", "\n", "self", ".", "sym2idx", "=", "OrderedDict", "(", ")", "\n", "\n", "for", "sym", "in", "self", ".", "special", ":", "\n", "                ", "self", ".", "add_special", "(", "sym", ")", "\n", "\n", "", "for", "sym", ",", "cnt", "in", "self", ".", "counter", ".", "most_common", "(", "self", ".", "max_size", ")", ":", "\n", "                ", "if", "cnt", "<", "self", ".", "min_freq", ":", "\n", "                    ", "break", "\n", "", "self", ".", "add_symbol", "(", "sym", ")", "\n", "\n", "", "logger", ".", "info", "(", "\n", "\"final vocab size {} from {} unique tokens\"", ".", "format", "(", "\n", "len", "(", "self", ")", ",", "len", "(", "self", ".", "counter", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_transfo_xl.TransfoXLTokenizer.encode_file": [[260, 280], ["os.path.exists", "logger.info", "open", "enumerate", "torch.cat", "tokenization_transfo_xl.TransfoXLTokenizer.tokenize", "torch.cat.append", "logger.info", "tokenization_transfo_xl.TransfoXLTokenizer.convert_to_tensor"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizerFast.tokenize", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_transfo_xl.TransfoXLTokenizer.convert_to_tensor"], ["", "", "def", "encode_file", "(", "\n", "self", ",", "path", ",", "ordered", "=", "False", ",", "verbose", "=", "False", ",", "add_eos", "=", "True", ",", "add_double_eos", "=", "False", "\n", ")", ":", "\n", "        ", "if", "verbose", ":", "\n", "            ", "logger", ".", "info", "(", "\"encoding file {} ...\"", ".", "format", "(", "path", ")", ")", "\n", "", "assert", "os", ".", "path", ".", "exists", "(", "path", ")", "\n", "encoded", "=", "[", "]", "\n", "with", "open", "(", "path", ",", "\"r\"", ",", "encoding", "=", "\"utf-8\"", ")", "as", "f", ":", "\n", "            ", "for", "idx", ",", "line", "in", "enumerate", "(", "f", ")", ":", "\n", "                ", "if", "verbose", "and", "idx", ">", "0", "and", "idx", "%", "500000", "==", "0", ":", "\n", "                    ", "logger", ".", "info", "(", "\"    line {}\"", ".", "format", "(", "idx", ")", ")", "\n", "", "symbols", "=", "self", ".", "tokenize", "(", "\n", "line", ",", "add_eos", "=", "add_eos", ",", "add_double_eos", "=", "add_double_eos", "\n", ")", "\n", "encoded", ".", "append", "(", "self", ".", "convert_to_tensor", "(", "symbols", ")", ")", "\n", "\n", "", "", "if", "ordered", ":", "\n", "            ", "encoded", "=", "torch", ".", "cat", "(", "encoded", ")", "\n", "\n", "", "return", "encoded", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_transfo_xl.TransfoXLTokenizer.encode_sents": [[281, 294], ["enumerate", "logger.info", "torch.cat.append", "torch.cat", "logger.info", "tokenization_transfo_xl.TransfoXLTokenizer.convert_to_tensor", "len"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_transfo_xl.TransfoXLTokenizer.convert_to_tensor"], ["", "def", "encode_sents", "(", "self", ",", "sents", ",", "ordered", "=", "False", ",", "verbose", "=", "False", ")", ":", "\n", "        ", "if", "verbose", ":", "\n", "            ", "logger", ".", "info", "(", "\"encoding {} sents ...\"", ".", "format", "(", "len", "(", "sents", ")", ")", ")", "\n", "", "encoded", "=", "[", "]", "\n", "for", "idx", ",", "symbols", "in", "enumerate", "(", "sents", ")", ":", "\n", "            ", "if", "verbose", "and", "idx", ">", "0", "and", "idx", "%", "500000", "==", "0", ":", "\n", "                ", "logger", ".", "info", "(", "\"    line {}\"", ".", "format", "(", "idx", ")", ")", "\n", "", "encoded", ".", "append", "(", "self", ".", "convert_to_tensor", "(", "symbols", ")", ")", "\n", "\n", "", "if", "ordered", ":", "\n", "            ", "encoded", "=", "torch", ".", "cat", "(", "encoded", ")", "\n", "\n", "", "return", "encoded", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_transfo_xl.TransfoXLTokenizer.add_special": [[295, 300], ["tokenization_transfo_xl.TransfoXLTokenizer.idx2sym.append", "setattr", "len", "sym.strip"], "methods", ["None"], ["", "def", "add_special", "(", "self", ",", "sym", ")", ":", "\n", "        ", "if", "sym", "not", "in", "self", ".", "sym2idx", ":", "\n", "            ", "self", ".", "idx2sym", ".", "append", "(", "sym", ")", "\n", "self", ".", "sym2idx", "[", "sym", "]", "=", "len", "(", "self", ".", "idx2sym", ")", "-", "1", "\n", "setattr", "(", "self", ",", "\"{}_idx\"", ".", "format", "(", "sym", ".", "strip", "(", "\"<>\"", ")", ")", ",", "self", ".", "sym2idx", "[", "sym", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_transfo_xl.TransfoXLTokenizer.add_symbol": [[301, 305], ["tokenization_transfo_xl.TransfoXLTokenizer.idx2sym.append", "len"], "methods", ["None"], ["", "", "def", "add_symbol", "(", "self", ",", "sym", ")", ":", "\n", "        ", "if", "sym", "not", "in", "self", ".", "sym2idx", ":", "\n", "            ", "self", ".", "idx2sym", ".", "append", "(", "sym", ")", "\n", "self", ".", "sym2idx", "[", "sym", "]", "=", "len", "(", "self", ".", "idx2sym", ")", "-", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_transfo_xl.TransfoXLTokenizer._convert_id_to_token": [[306, 310], ["len"], "methods", ["None"], ["", "", "def", "_convert_id_to_token", "(", "self", ",", "idx", ")", ":", "\n", "        ", "\"\"\"Converts an id in a token (BPE) using the vocab.\"\"\"", "\n", "assert", "0", "<=", "idx", "<", "len", "(", "self", ")", ",", "\"Index {} out of vocabulary range\"", ".", "format", "(", "idx", ")", "\n", "return", "self", ".", "idx2sym", "[", "idx", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_transfo_xl.TransfoXLTokenizer._convert_token_to_id": [[311, 328], ["hasattr", "tokenization_transfo_xl.TransfoXLTokenizer.sym2idx.get", "ValueError"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.closablequeue.ClosableQueue.get"], ["", "def", "_convert_token_to_id", "(", "self", ",", "sym", ")", ":", "\n", "        ", "\"\"\"Converts a token (str) in an id using the vocab.\"\"\"", "\n", "if", "sym", "in", "self", ".", "sym2idx", ":", "\n", "            ", "return", "self", ".", "sym2idx", "[", "sym", "]", "\n", "", "else", ":", "\n", "# logger.info('encounter unk {}'.format(sym))", "\n", "# assert '<eos>' not in sym", "\n", "            ", "if", "hasattr", "(", "self", ",", "\"unk_idx\"", ")", ":", "\n", "                ", "return", "self", ".", "sym2idx", ".", "get", "(", "sym", ",", "self", ".", "unk_idx", ")", "\n", "# Backward compatibility with pre-trained models", "\n", "", "elif", "\"<unk>\"", "in", "self", ".", "sym2idx", ":", "\n", "                ", "return", "self", ".", "sym2idx", "[", "\"<unk>\"", "]", "\n", "", "elif", "\"<UNK>\"", "in", "self", ".", "sym2idx", ":", "\n", "                ", "return", "self", ".", "sym2idx", "[", "\"<UNK>\"", "]", "\n", "", "else", ":", "\n", "                ", "raise", "ValueError", "(", "\n", "\"Token not in vocabulary and no <unk> token in vocabulary for replacement\"", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_transfo_xl.TransfoXLTokenizer.convert_tokens_to_string": [[330, 334], ["None"], "methods", ["None"], ["", "", "", "def", "convert_tokens_to_string", "(", "self", ",", "tokens", ")", ":", "\n", "        ", "\"\"\"Converts a sequence of tokens (string) in a single string.\"\"\"", "\n", "out_string", "=", "\" \"", ".", "join", "(", "tokens", ")", ".", "strip", "(", ")", "\n", "return", "out_string", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_transfo_xl.TransfoXLTokenizer.convert_to_tensor": [[335, 337], ["torch.LongTensor", "tokenization_transfo_xl.TransfoXLTokenizer.convert_tokens_to_ids"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizer.convert_tokens_to_ids"], ["", "def", "convert_to_tensor", "(", "self", ",", "symbols", ")", ":", "\n", "        ", "return", "torch", ".", "LongTensor", "(", "self", ".", "convert_tokens_to_ids", "(", "symbols", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_transfo_xl.TransfoXLTokenizer.vocab_size": [[338, 341], ["len"], "methods", ["None"], ["", "@", "property", "\n", "def", "vocab_size", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "idx2sym", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_transfo_xl.TransfoXLTokenizer.get_vocab": [[342, 344], ["dict"], "methods", ["None"], ["", "def", "get_vocab", "(", "self", ")", ":", "\n", "        ", "return", "dict", "(", "self", ".", "sym2idx", ",", "**", "self", ".", "added_tokens_encoder", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_transfo_xl.TransfoXLTokenizer._tokenize": [[345, 363], ["line.lower.lower.strip", "line.lower.lower.lower", "line.lower.lower.split"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.utils.sentence_splitter.PunktSentenceSplitter.split"], ["", "def", "_tokenize", "(", "self", ",", "line", ",", "add_eos", "=", "False", ",", "add_double_eos", "=", "False", ")", ":", "\n", "        ", "line", "=", "line", ".", "strip", "(", ")", "\n", "# convert to lower case", "\n", "if", "self", ".", "lower_case", ":", "\n", "            ", "line", "=", "line", ".", "lower", "(", ")", "\n", "\n", "# empty delimiter '' will evaluate False", "\n", "", "if", "self", ".", "delimiter", "==", "\"\"", ":", "\n", "            ", "symbols", "=", "line", "\n", "", "else", ":", "\n", "            ", "symbols", "=", "line", ".", "split", "(", "self", ".", "delimiter", ")", "\n", "\n", "", "if", "add_double_eos", ":", "# lm1b", "\n", "            ", "return", "[", "\"<S>\"", "]", "+", "symbols", "+", "[", "\"<S>\"", "]", "\n", "", "elif", "add_eos", ":", "\n", "            ", "return", "symbols", "+", "[", "\"<eos>\"", "]", "\n", "", "else", ":", "\n", "            ", "return", "symbols", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_transfo_xl.TransfoXLTokenizer.prepare_for_tokenization": [[364, 377], ["tokenization_transfo_xl.TransfoXLTokenizer.punctuation_with_space_around_pattern.sub"], "methods", ["None"], ["", "", "def", "prepare_for_tokenization", "(", "self", ",", "text", ",", "**", "kwargs", ")", ":", "\n", "# add spaces before punctuation symbols as should be done in transfo-xl", "\n", "        ", "text", "=", "self", ".", "punctuation_with_space_around_pattern", ".", "sub", "(", "r\" \"", ",", "text", ")", "\n", "\n", "# if \"add_space_before_punct_symbol\" in kwargs and kwargs[\"add_space_before_punct_symbol\"]:", "\n", "#     text = self.punctuation_with_space_around_pattern.sub(r\" \", text)", "\n", "# elif self.punction_without_space_before_pattern.search(text):", "\n", "#     # searches until the first occurence of a punctuation symbol without surrounding spaces", "\n", "#     logger.warning(", "\n", "#         \"You might want to consider setting `add_space_before_punct_symbol=True` as an argument to the `tokenizer.encode()` to avoid tokenizing words with punctuation symbols to the `<unk>` token\"", "\n", "#     )", "\n", "\n", "return", "text", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_transfo_xl._TransfoXLDelimiterLookupTokenizer.__init__": [[380, 440], ["tokenizers.implementations.BaseTokenizer.__init__", "tokenizers.models.WordLevel.from_files", "tokenizers.Tokenizer", "len", "tokenizers.pre_tokenizers.CharDelimiterSplit", "tokenizers.pre_tokenizers.WhitespaceSplit", "tokenizers.processors.BertProcessing", "ValueError", "tokenizers.normalizers.unicode_normalizer_from_str", "tokenizers.normalizers.Lowercase", "tokenizers.normalizers.Sequence", "len", "tokenizers.Tokenizer.token_to_id", "tokenizers.Tokenizer.token_to_id"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.tests.model_selector_test.ToyModel.__init__"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "vocab_file", ",", "\n", "delimiter", ",", "\n", "lowercase", ",", "\n", "unk_token", ",", "\n", "eos_token", ",", "\n", "add_eos", "=", "False", ",", "\n", "add_double_eos", "=", "False", ",", "\n", "normalization", ":", "Optional", "[", "str", "]", "=", "None", ",", "\n", ")", ":", "\n", "\n", "        ", "try", ":", "\n", "            ", "tokenizer", "=", "WordLevel", ".", "from_files", "(", "vocab_file", ",", "unk_token", "=", "unk_token", ")", "\n", "tokenizer", "=", "Tokenizer", "(", "tokenizer", ")", "\n", "", "except", "Exception", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"Unable to parse file {}. Unknown format. \"", "\n", "\"If you tried to load a model saved through TransfoXLTokenizer,\"", "\n", "\"please note they are not compatible.\"", ".", "format", "(", "vocab_file", ")", "\n", ")", "\n", "\n", "# Create the correct normalization path", "\n", "", "normalizer", "=", "[", "]", "\n", "\n", "# Include unicode normalization", "\n", "if", "normalization", ":", "\n", "            ", "normalizer", "+=", "[", "unicode_normalizer_from_str", "(", "normalization", ")", "]", "\n", "\n", "# Include case normalization", "\n", "", "if", "lowercase", ":", "\n", "            ", "normalizer", "+=", "[", "Lowercase", "(", ")", "]", "\n", "\n", "", "if", "len", "(", "normalizer", ")", ">", "0", ":", "\n", "            ", "tokenizer", ".", "normalizer", "=", "(", "\n", "Sequence", "(", "normalizer", ")", "if", "len", "(", "normalizer", ")", ">", "1", "else", "normalizer", "[", "0", "]", "\n", ")", "\n", "\n", "# Setup the splitter", "\n", "", "tokenizer", ".", "pre_tokenizer", "=", "(", "\n", "CharDelimiterSplit", "(", "delimiter", ")", "if", "delimiter", "else", "WhitespaceSplit", "(", ")", "\n", ")", "\n", "\n", "if", "add_double_eos", ":", "\n", "            ", "tokenizer", ".", "post_processor", "=", "BertProcessing", "(", "\n", "(", "eos_token", ",", "tokenizer", ".", "token_to_id", "(", "eos_token", ")", ")", ",", "\n", "(", "eos_token", ",", "tokenizer", ".", "token_to_id", "(", "eos_token", ")", ")", ",", "\n", ")", "\n", "\n", "", "parameters", "=", "{", "\n", "\"model\"", ":", "\"TransfoXLModel\"", ",", "\n", "\"add_eos\"", ":", "add_eos", ",", "\n", "\"add_double_eos\"", ":", "add_double_eos", ",", "\n", "\"unk_token\"", ":", "unk_token", ",", "\n", "\"eos_token\"", ":", "eos_token", ",", "\n", "\"delimiter\"", ":", "delimiter", ",", "\n", "\"lowercase\"", ":", "lowercase", ",", "\n", "}", "\n", "\n", "super", "(", ")", ".", "__init__", "(", "tokenizer", ",", "parameters", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_transfo_xl._TransfoXLDelimiterLookupTokenizer.encode_batch": [[441, 450], ["super().encode_batch", "isinstance", "seq.strip", "seq[].strip", "seq[].strip"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_transfo_xl._TransfoXLDelimiterLookupTokenizer.encode_batch"], ["", "def", "encode_batch", "(", "\n", "self", ",", "sequences", ":", "List", "[", "Union", "[", "str", ",", "Tuple", "[", "str", ",", "str", "]", "]", "]", "\n", ")", "->", "List", "[", "Encoding", "]", ":", "\n", "        ", "return", "super", "(", ")", ".", "encode_batch", "(", "\n", "[", "\n", "seq", ".", "strip", "(", ")", "\n", "if", "isinstance", "(", "seq", ",", "str", ")", "\n", "else", "(", "seq", "[", "0", "]", ".", "strip", "(", ")", ",", "seq", "[", "1", "]", ".", "strip", "(", ")", ")", "\n", "for", "seq", "in", "sequences", "\n", "]", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_transfo_xl._TransfoXLDelimiterLookupTokenizer.encode": [[453, 455], ["tokenizers.implementations.BaseTokenizer.encode", "sequence.strip", "pair.strip"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizer.encode"], ["", "def", "encode", "(", "self", ",", "sequence", ":", "str", ",", "pair", ":", "Optional", "[", "str", "]", "=", "None", ")", "->", "Encoding", ":", "\n", "        ", "return", "super", "(", ")", ".", "encode", "(", "sequence", ".", "strip", "(", ")", ",", "pair", ".", "strip", "(", ")", "if", "pair", "else", "pair", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_transfo_xl.TransfoXLTokenizerFast.__init__": [[463, 497], ["tokenization_utils.PreTrainedTokenizerFast.__init__", "tokenization_transfo_xl._TransfoXLDelimiterLookupTokenizer"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.tests.model_selector_test.ToyModel.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "special", "=", "None", ",", "\n", "min_freq", "=", "0", ",", "\n", "max_size", "=", "None", ",", "\n", "lower_case", "=", "False", ",", "\n", "delimiter", "=", "None", ",", "\n", "vocab_file", "=", "None", ",", "\n", "pretrained_vocab_file", "=", "None", ",", "\n", "never_split", "=", "None", ",", "\n", "unk_token", "=", "\"<unk>\"", ",", "\n", "eos_token", "=", "\"<eos>\"", ",", "\n", "additional_special_tokens", "=", "[", "\"<formula>\"", "]", ",", "\n", "add_eos", "=", "False", ",", "\n", "add_double_eos", "=", "False", ",", "\n", "normalization", "=", "None", ",", "\n", "**", "kwargs", "\n", ")", ":", "\n", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "\n", "_TransfoXLDelimiterLookupTokenizer", "(", "\n", "vocab_file", "=", "vocab_file", "or", "pretrained_vocab_file", ",", "\n", "delimiter", "=", "delimiter", ",", "\n", "lowercase", "=", "lower_case", ",", "\n", "unk_token", "=", "unk_token", ",", "\n", "eos_token", "=", "eos_token", ",", "\n", "add_eos", "=", "add_eos", ",", "\n", "add_double_eos", "=", "add_double_eos", ",", "\n", "normalization", "=", "normalization", ",", "\n", ")", ",", "\n", "unk_token", "=", "unk_token", ",", "\n", "eos_token", "=", "eos_token", ",", "\n", "additional_special_tokens", "=", "additional_special_tokens", ",", "\n", "**", "kwargs", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_transfo_xl.TransfoXLTokenizerFast.save_pretrained": [[499, 506], ["logger.warning", "super().save_pretrained"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.MeetingNet_Transformer.MeetingNet_Transformer.save_pretrained"], ["", "def", "save_pretrained", "(", "self", ",", "save_directory", ")", ":", "\n", "        ", "logger", ".", "warning", "(", "\n", "\"Please note you will not be able to load the vocabulary in\"", "\n", "\" Python-based TransfoXLTokenizer as they don't share the same structure.\"", "\n", ")", "\n", "\n", "return", "super", "(", ")", ".", "save_pretrained", "(", "save_directory", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_transfo_xl.LMOrderedIterator.__init__": [[509, 530], ["data.narrow.narrow.narrow", "data.narrow.narrow.view().t().contiguous().to", "data.narrow.narrow.size", "data.narrow.narrow.view().t().contiguous", "data.narrow.narrow.view().t", "data.narrow.narrow.view"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "data", ",", "bsz", ",", "bptt", ",", "device", "=", "\"cpu\"", ",", "ext_len", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        data -- LongTensor -- the LongTensor is strictly ordered\n        \"\"\"", "\n", "self", ".", "bsz", "=", "bsz", "\n", "self", ".", "bptt", "=", "bptt", "\n", "self", ".", "ext_len", "=", "ext_len", "if", "ext_len", "is", "not", "None", "else", "0", "\n", "\n", "self", ".", "device", "=", "device", "\n", "\n", "# Work out how cleanly we can divide the dataset into bsz parts.", "\n", "self", ".", "n_step", "=", "data", ".", "size", "(", "0", ")", "//", "bsz", "\n", "\n", "# Trim off any extra elements that wouldn't cleanly fit (remainders).", "\n", "data", "=", "data", ".", "narrow", "(", "0", ",", "0", ",", "self", ".", "n_step", "*", "bsz", ")", "\n", "\n", "# Evenly divide the data across the bsz batches.", "\n", "self", ".", "data", "=", "data", ".", "view", "(", "bsz", ",", "-", "1", ")", ".", "t", "(", ")", ".", "contiguous", "(", ")", ".", "to", "(", "device", ")", "\n", "\n", "# Number of mini-batches", "\n", "self", ".", "n_batch", "=", "(", "self", ".", "n_step", "+", "self", ".", "bptt", "-", "1", ")", "//", "self", ".", "bptt", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_transfo_xl.LMOrderedIterator.get_batch": [[531, 546], ["min", "max", "data.transpose().contiguous().to", "target.transpose().contiguous().to", "data.transpose().contiguous", "target.transpose().contiguous", "tokenization_transfo_xl.LMOrderedIterator.data.size", "data.transpose", "target.transpose"], "methods", ["None"], ["", "def", "get_batch", "(", "self", ",", "i", ",", "bptt", "=", "None", ")", ":", "\n", "        ", "if", "bptt", "is", "None", ":", "\n", "            ", "bptt", "=", "self", ".", "bptt", "\n", "", "seq_len", "=", "min", "(", "bptt", ",", "self", ".", "data", ".", "size", "(", "0", ")", "-", "1", "-", "i", ")", "\n", "\n", "end_idx", "=", "i", "+", "seq_len", "\n", "beg_idx", "=", "max", "(", "0", ",", "i", "-", "self", ".", "ext_len", ")", "\n", "\n", "data", "=", "self", ".", "data", "[", "beg_idx", ":", "end_idx", "]", "\n", "target", "=", "self", ".", "data", "[", "i", "+", "1", ":", "i", "+", "1", "+", "seq_len", "]", "\n", "\n", "data_out", "=", "data", ".", "transpose", "(", "0", ",", "1", ")", ".", "contiguous", "(", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "target_out", "=", "target", ".", "transpose", "(", "0", ",", "1", ")", ".", "contiguous", "(", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "\n", "return", "data_out", ",", "target_out", ",", "seq_len", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_transfo_xl.LMOrderedIterator.get_fixlen_iter": [[547, 550], ["range", "tokenization_transfo_xl.LMOrderedIterator.data.size", "tokenization_transfo_xl.LMOrderedIterator.get_batch"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_transfo_xl.LMOrderedIterator.get_batch"], ["", "def", "get_fixlen_iter", "(", "self", ",", "start", "=", "0", ")", ":", "\n", "        ", "for", "i", "in", "range", "(", "start", ",", "self", ".", "data", ".", "size", "(", "0", ")", "-", "1", ",", "self", ".", "bptt", ")", ":", "\n", "            ", "yield", "self", ".", "get_batch", "(", "i", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_transfo_xl.LMOrderedIterator.get_varlen_iter": [[551, 562], ["min", "tokenization_transfo_xl.LMOrderedIterator.get_batch", "max", "numpy.random.random", "int", "tokenization_transfo_xl.LMOrderedIterator.data.size", "numpy.random.normal"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_transfo_xl.LMOrderedIterator.get_batch"], ["", "", "def", "get_varlen_iter", "(", "self", ",", "start", "=", "0", ",", "std", "=", "5", ",", "min_len", "=", "5", ",", "max_deviation", "=", "3", ")", ":", "\n", "        ", "max_len", "=", "self", ".", "bptt", "+", "max_deviation", "*", "std", "\n", "i", "=", "start", "\n", "while", "True", ":", "\n", "            ", "bptt", "=", "self", ".", "bptt", "if", "np", ".", "random", ".", "random", "(", ")", "<", "0.95", "else", "self", ".", "bptt", "/", "2.0", "\n", "bptt", "=", "min", "(", "max_len", ",", "max", "(", "min_len", ",", "int", "(", "np", ".", "random", ".", "normal", "(", "bptt", ",", "std", ")", ")", ")", ")", "\n", "data", ",", "target", ",", "seq_len", "=", "self", ".", "get_batch", "(", "i", ",", "bptt", ")", "\n", "i", "+=", "seq_len", "\n", "yield", "data", ",", "target", ",", "seq_len", "\n", "if", "i", ">=", "self", ".", "data", ".", "size", "(", "0", ")", "-", "2", ":", "\n", "                ", "break", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_transfo_xl.LMOrderedIterator.__iter__": [[563, 565], ["tokenization_transfo_xl.LMOrderedIterator.get_fixlen_iter"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_transfo_xl.LMOrderedIterator.get_fixlen_iter"], ["", "", "", "def", "__iter__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "get_fixlen_iter", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_transfo_xl.LMShuffledIterator.__init__": [[568, 580], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "data", ",", "bsz", ",", "bptt", ",", "device", "=", "\"cpu\"", ",", "ext_len", "=", "None", ",", "shuffle", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        data -- list[LongTensor] -- there is no order among the LongTensors\n        \"\"\"", "\n", "self", ".", "data", "=", "data", "\n", "\n", "self", ".", "bsz", "=", "bsz", "\n", "self", ".", "bptt", "=", "bptt", "\n", "self", ".", "ext_len", "=", "ext_len", "if", "ext_len", "is", "not", "None", "else", "0", "\n", "\n", "self", ".", "device", "=", "device", "\n", "self", ".", "shuffle", "=", "shuffle", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_transfo_xl.LMShuffledIterator.get_sent_stream": [[581, 592], ["numpy.random.permutation", "numpy.array", "len", "range", "len"], "methods", ["None"], ["", "def", "get_sent_stream", "(", "self", ")", ":", "\n", "# index iterator", "\n", "        ", "epoch_indices", "=", "(", "\n", "np", ".", "random", ".", "permutation", "(", "len", "(", "self", ".", "data", ")", ")", "\n", "if", "self", ".", "shuffle", "\n", "else", "np", ".", "array", "(", "range", "(", "len", "(", "self", ".", "data", ")", ")", ")", "\n", ")", "\n", "\n", "# sentence iterator", "\n", "for", "idx", "in", "epoch_indices", ":", "\n", "            ", "yield", "self", ".", "data", "[", "idx", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_transfo_xl.LMShuffledIterator.stream_iterator": [[593, 643], ["torch.LongTensor", "torch.LongTensor", "data[].fill_", "torch.LongTensor.fill_", "range", "torch.LongTensor.transpose().contiguous().to", "torch.LongTensor.transpose().contiguous().to", "min", "torch.LongTensor.resize_", "torch.LongTensor.size", "torch.LongTensor.size", "torch.LongTensor.transpose().contiguous", "torch.LongTensor.transpose().contiguous", "min", "next", "torch.LongTensor.transpose", "torch.LongTensor.transpose", "len", "len"], "methods", ["None"], ["", "", "def", "stream_iterator", "(", "self", ",", "sent_stream", ")", ":", "\n", "# streams for each data in the batch", "\n", "        ", "streams", "=", "[", "None", "]", "*", "self", ".", "bsz", "\n", "\n", "data", "=", "torch", ".", "LongTensor", "(", "self", ".", "bptt", ",", "self", ".", "bsz", ")", "\n", "target", "=", "torch", ".", "LongTensor", "(", "self", ".", "bptt", ",", "self", ".", "bsz", ")", "\n", "\n", "n_retain", "=", "0", "\n", "\n", "while", "True", ":", "\n", "# data   : [n_retain+bptt x bsz]", "\n", "# target : [bptt x bsz]", "\n", "            ", "data", "[", "n_retain", ":", "]", ".", "fill_", "(", "-", "1", ")", "\n", "target", ".", "fill_", "(", "-", "1", ")", "\n", "\n", "valid_batch", "=", "True", "\n", "\n", "for", "i", "in", "range", "(", "self", ".", "bsz", ")", ":", "\n", "                ", "n_filled", "=", "0", "\n", "try", ":", "\n", "                    ", "while", "n_filled", "<", "self", ".", "bptt", ":", "\n", "                        ", "if", "streams", "[", "i", "]", "is", "None", "or", "len", "(", "streams", "[", "i", "]", ")", "<=", "1", ":", "\n", "                            ", "streams", "[", "i", "]", "=", "next", "(", "sent_stream", ")", "\n", "# number of new tokens to fill in", "\n", "", "n_new", "=", "min", "(", "len", "(", "streams", "[", "i", "]", ")", "-", "1", ",", "self", ".", "bptt", "-", "n_filled", ")", "\n", "# first n_retain tokens are retained from last batch", "\n", "data", "[", "\n", "n_retain", "+", "n_filled", ":", "n_retain", "+", "n_filled", "+", "n_new", ",", "i", "\n", "]", "=", "streams", "[", "i", "]", "[", ":", "n_new", "]", "\n", "target", "[", "n_filled", ":", "n_filled", "+", "n_new", ",", "i", "]", "=", "streams", "[", "i", "]", "[", "\n", "1", ":", "n_new", "+", "1", "\n", "]", "\n", "streams", "[", "i", "]", "=", "streams", "[", "i", "]", "[", "n_new", ":", "]", "\n", "n_filled", "+=", "n_new", "\n", "", "", "except", "StopIteration", ":", "\n", "                    ", "valid_batch", "=", "False", "\n", "break", "\n", "\n", "", "", "if", "not", "valid_batch", ":", "\n", "                ", "return", "\n", "\n", "", "data_out", "=", "data", ".", "transpose", "(", "0", ",", "1", ")", ".", "contiguous", "(", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "target_out", "=", "target", ".", "transpose", "(", "0", ",", "1", ")", ".", "contiguous", "(", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "\n", "yield", "data_out", ",", "target_out", ",", "self", ".", "bptt", "\n", "\n", "n_retain", "=", "min", "(", "data", ".", "size", "(", "0", ")", ",", "self", ".", "ext_len", ")", "\n", "if", "n_retain", ">", "0", ":", "\n", "                ", "data", "[", ":", "n_retain", "]", "=", "data", "[", "-", "n_retain", ":", "]", "\n", "", "data", ".", "resize_", "(", "n_retain", "+", "self", ".", "bptt", ",", "data", ".", "size", "(", "1", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_transfo_xl.LMShuffledIterator.__iter__": [[644, 650], ["tokenization_transfo_xl.LMShuffledIterator.get_sent_stream", "tokenization_transfo_xl.LMShuffledIterator.stream_iterator"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_transfo_xl.LMMultiFileIterator.get_sent_stream", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_transfo_xl.LMShuffledIterator.stream_iterator"], ["", "", "def", "__iter__", "(", "self", ")", ":", "\n", "# sent_stream is an iterator", "\n", "        ", "sent_stream", "=", "self", ".", "get_sent_stream", "(", ")", "\n", "\n", "for", "batch", "in", "self", ".", "stream_iterator", "(", "sent_stream", ")", ":", "\n", "            ", "yield", "batch", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_transfo_xl.LMMultiFileIterator.__init__": [[653, 666], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "\n", "self", ",", "paths", ",", "vocab", ",", "bsz", ",", "bptt", ",", "device", "=", "\"cpu\"", ",", "ext_len", "=", "None", ",", "shuffle", "=", "False", "\n", ")", ":", "\n", "\n", "        ", "self", ".", "paths", "=", "paths", "\n", "self", ".", "vocab", "=", "vocab", "\n", "\n", "self", ".", "bsz", "=", "bsz", "\n", "self", ".", "bptt", "=", "bptt", "\n", "self", ".", "ext_len", "=", "ext_len", "if", "ext_len", "is", "not", "None", "else", "0", "\n", "\n", "self", ".", "device", "=", "device", "\n", "self", ".", "shuffle", "=", "shuffle", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_transfo_xl.LMMultiFileIterator.get_sent_stream": [[667, 674], ["tokenization_transfo_xl.LMMultiFileIterator.vocab.encode_file", "iter", "numpy.random.shuffle"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_transfo_xl.TransfoXLTokenizer.encode_file"], ["", "def", "get_sent_stream", "(", "self", ",", "path", ")", ":", "\n", "        ", "sents", "=", "self", ".", "vocab", ".", "encode_file", "(", "path", ",", "add_double_eos", "=", "True", ")", "\n", "if", "self", ".", "shuffle", ":", "\n", "            ", "np", ".", "random", ".", "shuffle", "(", "sents", ")", "\n", "", "sent_stream", "=", "iter", "(", "sents", ")", "\n", "\n", "return", "sent_stream", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_transfo_xl.LMMultiFileIterator.__iter__": [[675, 684], ["numpy.random.shuffle", "tokenization_transfo_xl.LMMultiFileIterator.get_sent_stream", "tokenization_transfo_xl.LMMultiFileIterator.stream_iterator"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_transfo_xl.LMMultiFileIterator.get_sent_stream", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_transfo_xl.LMShuffledIterator.stream_iterator"], ["", "def", "__iter__", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "shuffle", ":", "\n", "            ", "np", ".", "random", ".", "shuffle", "(", "self", ".", "paths", ")", "\n", "\n", "", "for", "path", "in", "self", ".", "paths", ":", "\n", "# sent_stream is an iterator", "\n", "            ", "sent_stream", "=", "self", ".", "get_sent_stream", "(", "path", ")", "\n", "for", "batch", "in", "self", ".", "stream_iterator", "(", "sent_stream", ")", ":", "\n", "                ", "yield", "batch", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_transfo_xl.TransfoXLCorpus.from_pretrained": [[687, 738], ["TransfoXLTokenizer.from_pretrained", "cls", "torch.load", "torch.load.items", "os.path.join", "file_utils.cached_path", "logger.info", "logger.info", "torch.tensor", "torch.tensor", "torch.tensor", "logger.error", "PRETRAINED_CORPUS_ARCHIVE_MAP.keys"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.MeetingNet_Transformer.MeetingNet_Transformer.from_pretrained", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.BaseTrainer.BaseTrainer.load", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.file_utils.cached_path"], ["    ", "@", "classmethod", "\n", "def", "from_pretrained", "(", "\n", "cls", ",", "pretrained_model_name_or_path", ",", "cache_dir", "=", "None", ",", "*", "inputs", ",", "**", "kwargs", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Instantiate a pre-processed corpus.\n        \"\"\"", "\n", "vocab", "=", "TransfoXLTokenizer", ".", "from_pretrained", "(", "\n", "pretrained_model_name_or_path", ",", "*", "inputs", ",", "**", "kwargs", "\n", ")", "\n", "if", "pretrained_model_name_or_path", "in", "PRETRAINED_CORPUS_ARCHIVE_MAP", ":", "\n", "            ", "corpus_file", "=", "PRETRAINED_CORPUS_ARCHIVE_MAP", "[", "pretrained_model_name_or_path", "]", "\n", "", "else", ":", "\n", "            ", "corpus_file", "=", "os", ".", "path", ".", "join", "(", "pretrained_model_name_or_path", ",", "CORPUS_NAME", ")", "\n", "# redirect to the cache, if necessary", "\n", "", "try", ":", "\n", "            ", "resolved_corpus_file", "=", "cached_path", "(", "corpus_file", ",", "cache_dir", "=", "cache_dir", ")", "\n", "", "except", "EnvironmentError", ":", "\n", "            ", "logger", ".", "error", "(", "\n", "\"Corpus '{}' was not found in corpus list ({}). \"", "\n", "\"We assumed '{}' was a path or url but couldn't find files {} \"", "\n", "\"at this path or url.\"", ".", "format", "(", "\n", "pretrained_model_name_or_path", ",", "\n", "\", \"", ".", "join", "(", "PRETRAINED_CORPUS_ARCHIVE_MAP", ".", "keys", "(", ")", ")", ",", "\n", "pretrained_model_name_or_path", ",", "\n", "corpus_file", ",", "\n", ")", "\n", ")", "\n", "return", "None", "\n", "", "if", "resolved_corpus_file", "==", "corpus_file", ":", "\n", "            ", "logger", ".", "info", "(", "\"loading corpus file {}\"", ".", "format", "(", "corpus_file", ")", ")", "\n", "", "else", ":", "\n", "            ", "logger", ".", "info", "(", "\n", "\"loading corpus file {} from cache at {}\"", ".", "format", "(", "\n", "corpus_file", ",", "resolved_corpus_file", "\n", ")", "\n", ")", "\n", "\n", "# Instantiate tokenizer.", "\n", "", "corpus", "=", "cls", "(", "*", "inputs", ",", "**", "kwargs", ")", "\n", "corpus_dict", "=", "torch", ".", "load", "(", "resolved_corpus_file", ")", "\n", "for", "key", ",", "value", "in", "corpus_dict", ".", "items", "(", ")", ":", "\n", "            ", "corpus", ".", "__dict__", "[", "key", "]", "=", "value", "\n", "", "corpus", ".", "vocab", "=", "vocab", "\n", "if", "corpus", ".", "train", "is", "not", "None", ":", "\n", "            ", "corpus", ".", "train", "=", "torch", ".", "tensor", "(", "corpus", ".", "train", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "", "if", "corpus", ".", "valid", "is", "not", "None", ":", "\n", "            ", "corpus", ".", "valid", "=", "torch", ".", "tensor", "(", "corpus", ".", "valid", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "", "if", "corpus", ".", "test", "is", "not", "None", ":", "\n", "            ", "corpus", ".", "test", "=", "torch", ".", "tensor", "(", "corpus", ".", "test", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "", "return", "corpus", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_transfo_xl.TransfoXLCorpus.__init__": [[739, 745], ["tokenization_transfo_xl.TransfoXLTokenizer"], "methods", ["None"], ["", "def", "__init__", "(", "self", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "self", ".", "vocab", "=", "TransfoXLTokenizer", "(", "*", "args", ",", "**", "kwargs", ")", "\n", "self", ".", "dataset", "=", "None", "\n", "self", ".", "train", "=", "None", "\n", "self", ".", "valid", "=", "None", "\n", "self", ".", "test", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_transfo_xl.TransfoXLCorpus.build_corpus": [[746, 794], ["tokenization_transfo_xl.TransfoXLCorpus.vocab.build_vocab", "tokenization_transfo_xl.TransfoXLCorpus.vocab.count_file", "tokenization_transfo_xl.TransfoXLCorpus.vocab.count_file", "tokenization_transfo_xl.TransfoXLCorpus.vocab.count_file", "tokenization_transfo_xl.TransfoXLCorpus.vocab.encode_file", "tokenization_transfo_xl.TransfoXLCorpus.vocab.encode_file", "tokenization_transfo_xl.TransfoXLCorpus.vocab.encode_file", "os.path.join", "os.path.join", "os.path.join", "tokenization_transfo_xl.TransfoXLCorpus.vocab.count_file", "os.path.join", "os.path.join", "os.path.join", "tokenization_transfo_xl.TransfoXLCorpus.vocab.encode_file", "tokenization_transfo_xl.TransfoXLCorpus.vocab.encode_file", "tokenization_transfo_xl.TransfoXLCorpus.vocab.encode_file", "os.path.join", "os.path.join", "glob.glob", "os.path.join", "os.path.join", "os.path.join", "tokenization_transfo_xl.TransfoXLCorpus.vocab.encode_file", "tokenization_transfo_xl.TransfoXLCorpus.vocab.encode_file", "os.path.join", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_transfo_xl.TransfoXLTokenizer.build_vocab", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_transfo_xl.TransfoXLTokenizer.count_file", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_transfo_xl.TransfoXLTokenizer.count_file", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_transfo_xl.TransfoXLTokenizer.count_file", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_transfo_xl.TransfoXLTokenizer.encode_file", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_transfo_xl.TransfoXLTokenizer.encode_file", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_transfo_xl.TransfoXLTokenizer.encode_file", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_transfo_xl.TransfoXLTokenizer.count_file", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_transfo_xl.TransfoXLTokenizer.encode_file", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_transfo_xl.TransfoXLTokenizer.encode_file", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_transfo_xl.TransfoXLTokenizer.encode_file", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_transfo_xl.TransfoXLTokenizer.encode_file", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_transfo_xl.TransfoXLTokenizer.encode_file"], ["", "def", "build_corpus", "(", "self", ",", "path", ",", "dataset", ")", ":", "\n", "        ", "self", ".", "dataset", "=", "dataset", "\n", "\n", "if", "self", ".", "dataset", "in", "[", "\"ptb\"", ",", "\"wt2\"", ",", "\"enwik8\"", ",", "\"text8\"", "]", ":", "\n", "            ", "self", ".", "vocab", ".", "count_file", "(", "os", ".", "path", ".", "join", "(", "path", ",", "\"train.txt\"", ")", ")", "\n", "self", ".", "vocab", ".", "count_file", "(", "os", ".", "path", ".", "join", "(", "path", ",", "\"valid.txt\"", ")", ")", "\n", "self", ".", "vocab", ".", "count_file", "(", "os", ".", "path", ".", "join", "(", "path", ",", "\"test.txt\"", ")", ")", "\n", "", "elif", "self", ".", "dataset", "==", "\"wt103\"", ":", "\n", "            ", "self", ".", "vocab", ".", "count_file", "(", "os", ".", "path", ".", "join", "(", "path", ",", "\"train.txt\"", ")", ")", "\n", "", "elif", "self", ".", "dataset", "==", "\"lm1b\"", ":", "\n", "            ", "train_path_pattern", "=", "os", ".", "path", ".", "join", "(", "\n", "path", ",", "\n", "\"1-billion-word-language-modeling-benchmark-r13output\"", ",", "\n", "\"training-monolingual.tokenized.shuffled\"", ",", "\n", "\"news.en-*\"", ",", "\n", ")", "\n", "train_paths", "=", "glob", ".", "glob", "(", "train_path_pattern", ")", "\n", "# the vocab will load from file when build_vocab() is called", "\n", "\n", "", "self", ".", "vocab", ".", "build_vocab", "(", ")", "\n", "\n", "if", "self", ".", "dataset", "in", "[", "\"ptb\"", ",", "\"wt2\"", ",", "\"wt103\"", "]", ":", "\n", "            ", "self", ".", "train", "=", "self", ".", "vocab", ".", "encode_file", "(", "\n", "os", ".", "path", ".", "join", "(", "path", ",", "\"train.txt\"", ")", ",", "ordered", "=", "True", "\n", ")", "\n", "self", ".", "valid", "=", "self", ".", "vocab", ".", "encode_file", "(", "\n", "os", ".", "path", ".", "join", "(", "path", ",", "\"valid.txt\"", ")", ",", "ordered", "=", "True", "\n", ")", "\n", "self", ".", "test", "=", "self", ".", "vocab", ".", "encode_file", "(", "\n", "os", ".", "path", ".", "join", "(", "path", ",", "\"test.txt\"", ")", ",", "ordered", "=", "True", "\n", ")", "\n", "", "elif", "self", ".", "dataset", "in", "[", "\"enwik8\"", ",", "\"text8\"", "]", ":", "\n", "            ", "self", ".", "train", "=", "self", ".", "vocab", ".", "encode_file", "(", "\n", "os", ".", "path", ".", "join", "(", "path", ",", "\"train.txt\"", ")", ",", "ordered", "=", "True", ",", "add_eos", "=", "False", "\n", ")", "\n", "self", ".", "valid", "=", "self", ".", "vocab", ".", "encode_file", "(", "\n", "os", ".", "path", ".", "join", "(", "path", ",", "\"valid.txt\"", ")", ",", "ordered", "=", "True", ",", "add_eos", "=", "False", "\n", ")", "\n", "self", ".", "test", "=", "self", ".", "vocab", ".", "encode_file", "(", "\n", "os", ".", "path", ".", "join", "(", "path", ",", "\"test.txt\"", ")", ",", "ordered", "=", "True", ",", "add_eos", "=", "False", "\n", ")", "\n", "", "elif", "self", ".", "dataset", "==", "\"lm1b\"", ":", "\n", "            ", "self", ".", "train", "=", "train_paths", "\n", "self", ".", "valid", "=", "self", ".", "vocab", ".", "encode_file", "(", "\n", "os", ".", "path", ".", "join", "(", "path", ",", "\"valid.txt\"", ")", ",", "ordered", "=", "False", ",", "add_double_eos", "=", "True", "\n", ")", "\n", "self", ".", "test", "=", "self", ".", "vocab", ".", "encode_file", "(", "\n", "os", ".", "path", ".", "join", "(", "path", ",", "\"test.txt\"", ")", ",", "ordered", "=", "False", ",", "add_double_eos", "=", "True", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_transfo_xl.TransfoXLCorpus.get_iterator": [[796, 811], ["tokenization_transfo_xl.LMOrderedIterator", "tokenization_transfo_xl.LMMultiFileIterator", "tokenization_transfo_xl.LMOrderedIterator", "tokenization_transfo_xl.LMShuffledIterator"], "methods", ["None"], ["", "", "def", "get_iterator", "(", "self", ",", "split", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "if", "split", "==", "\"train\"", ":", "\n", "            ", "if", "self", ".", "dataset", "in", "[", "\"ptb\"", ",", "\"wt2\"", ",", "\"wt103\"", ",", "\"enwik8\"", ",", "\"text8\"", "]", ":", "\n", "                ", "data_iter", "=", "LMOrderedIterator", "(", "self", ".", "train", ",", "*", "args", ",", "**", "kwargs", ")", "\n", "", "elif", "self", ".", "dataset", "==", "\"lm1b\"", ":", "\n", "                ", "kwargs", "[", "\"shuffle\"", "]", "=", "True", "\n", "data_iter", "=", "LMMultiFileIterator", "(", "self", ".", "train", ",", "self", ".", "vocab", ",", "*", "args", ",", "**", "kwargs", ")", "\n", "", "", "elif", "split", "in", "[", "\"valid\"", ",", "\"test\"", "]", ":", "\n", "            ", "data", "=", "self", ".", "valid", "if", "split", "==", "\"valid\"", "else", "self", ".", "test", "\n", "if", "self", ".", "dataset", "in", "[", "\"ptb\"", ",", "\"wt2\"", ",", "\"wt103\"", ",", "\"enwik8\"", ",", "\"text8\"", "]", ":", "\n", "                ", "data_iter", "=", "LMOrderedIterator", "(", "data", ",", "*", "args", ",", "**", "kwargs", ")", "\n", "", "elif", "self", ".", "dataset", "==", "\"lm1b\"", ":", "\n", "                ", "data_iter", "=", "LMShuffledIterator", "(", "data", ",", "*", "args", ",", "**", "kwargs", ")", "\n", "\n", "", "", "return", "data_iter", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_transfo_xl.get_lm_corpus": [[813, 843], ["os.path.join", "os.path.join", "os.path.exists", "logger.info", "torch.load", "os.path.exists", "logger.info", "logger.info", "tokenization_transfo_xl.TransfoXLCorpus", "torch.save", "open", "pickle.load", "os.path.join"], "function", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.BaseTrainer.BaseTrainer.load", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.BaseTrainer.BaseTrainer.load"], ["", "", "def", "get_lm_corpus", "(", "datadir", ",", "dataset", ")", ":", "\n", "    ", "fn", "=", "os", ".", "path", ".", "join", "(", "datadir", ",", "\"cache.pt\"", ")", "\n", "fn_pickle", "=", "os", ".", "path", ".", "join", "(", "datadir", ",", "\"cache.pkl\"", ")", "\n", "if", "os", ".", "path", ".", "exists", "(", "fn", ")", ":", "\n", "        ", "logger", ".", "info", "(", "\"Loading cached dataset...\"", ")", "\n", "corpus", "=", "torch", ".", "load", "(", "fn_pickle", ")", "\n", "", "elif", "os", ".", "path", ".", "exists", "(", "fn", ")", ":", "\n", "        ", "logger", ".", "info", "(", "\"Loading cached dataset from pickle...\"", ")", "\n", "with", "open", "(", "fn", ",", "\"rb\"", ")", "as", "fp", ":", "\n", "            ", "corpus", "=", "pickle", ".", "load", "(", "fp", ")", "\n", "", "", "else", ":", "\n", "        ", "logger", ".", "info", "(", "\"Producing dataset {}...\"", ".", "format", "(", "dataset", ")", ")", "\n", "kwargs", "=", "{", "}", "\n", "if", "dataset", "in", "[", "\"wt103\"", ",", "\"wt2\"", "]", ":", "\n", "            ", "kwargs", "[", "\"special\"", "]", "=", "[", "\"<eos>\"", "]", "\n", "kwargs", "[", "\"lower_case\"", "]", "=", "False", "\n", "", "elif", "dataset", "==", "\"ptb\"", ":", "\n", "            ", "kwargs", "[", "\"special\"", "]", "=", "[", "\"<eos>\"", "]", "\n", "kwargs", "[", "\"lower_case\"", "]", "=", "True", "\n", "", "elif", "dataset", "==", "\"lm1b\"", ":", "\n", "            ", "kwargs", "[", "\"special\"", "]", "=", "[", "]", "\n", "kwargs", "[", "\"lower_case\"", "]", "=", "False", "\n", "kwargs", "[", "\"vocab_file\"", "]", "=", "os", ".", "path", ".", "join", "(", "datadir", ",", "\"1b_word_vocab.txt\"", ")", "\n", "", "elif", "dataset", "in", "[", "\"enwik8\"", ",", "\"text8\"", "]", ":", "\n", "            ", "pass", "\n", "\n", "", "corpus", "=", "TransfoXLCorpus", "(", "datadir", ",", "dataset", ",", "**", "kwargs", ")", "\n", "torch", ".", "save", "(", "corpus", ",", "fn", ")", "\n", "\n", "", "return", "corpus", "\n", "", ""]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.modeling_encoder_decoder.PreTrainedEncoderDecoder.__init__": [[39, 46], ["torch.nn.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.tests.model_selector_test.ToyModel.__init__"], ["def", "__init__", "(", "self", ",", "encoder", ",", "decoder", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "encoder", "=", "encoder", "\n", "self", ".", "decoder", "=", "decoder", "\n", "# manually set the self.config", "\n", "self", ".", "config", "=", "decoder", ".", "config", "\n", "self", ".", "config", ".", "is_encoder_decoder", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.modeling_encoder_decoder.PreTrainedEncoderDecoder.from_pretrained": [[47, 166], ["kwargs_common.copy", "kwargs_common.copy", "kwargs_common.copy.update", "kwargs_common.copy.update", "kwargs_common.copy.pop", "kwargs_common.copy.pop", "cls", "AutoModel.from_pretrained", "AutoModelWithLMHead.from_pretrained", "kwargs.items", "kwargs.items", "argument.startswith", "kwargs.items", "argument.startswith", "argument.startswith", "argument.startswith", "len", "len"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.HMNetTrainer.HMNetTrainer.update", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.HMNetTrainer.HMNetTrainer.update", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.MeetingNet_Transformer.MeetingNet_Transformer.from_pretrained", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.MeetingNet_Transformer.MeetingNet_Transformer.from_pretrained"], ["", "@", "classmethod", "\n", "def", "from_pretrained", "(", "\n", "cls", ",", "\n", "encoder_pretrained_model_name_or_path", "=", "None", ",", "\n", "decoder_pretrained_model_name_or_path", "=", "None", ",", "\n", "*", "model_args", ",", "\n", "**", "kwargs", ",", "\n", ")", ":", "\n", "        ", "r\"\"\"Instantiates an encoder and a decoder from one or two base classes of the library from pre-trained model checkpoints.\n\n\n        The model is set in evaluation mode by default using `model.eval()` (Dropout modules are deactivated)\n        To train the model, you need to first set it back in training mode with `model.train()`\n\n        Params:\n            encoder_pretrained_model_name_or_path: information necessary to initiate the encoder. Either:\n\n                - a string with the `shortcut name` of a pre-trained model to load from cache or download, e.g.: ``bert-base-uncased``.\n                - a string with the `identifier name` of a pre-trained model that was user-uploaded to our S3, e.g.: ``dbmdz/bert-base-german-cased``.\n                - a path to a `directory` containing model weights saved using :func:`~transformers.PreTrainedModel.save_pretrained`, e.g.: ``./my_model_directory/encoder``.\n                - a path or url to a `tensorflow index checkpoint file` (e.g. `./tf_model/model.ckpt.index`). In this case, ``from_tf`` should be set to True and a configuration object should be provided as ``config`` argument. This loading path is slower than converting the TensorFlow checkpoint in a PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.\n\n            decoder_pretrained_model_name_or_path: information necessary to initiate the decoder. Either:\n\n                - a string with the `shortcut name` of a pre-trained model to load from cache or download, e.g.: ``bert-base-uncased``.\n                - a string with the `identifier name` of a pre-trained model that was user-uploaded to our S3, e.g.: ``dbmdz/bert-base-german-cased``.\n                - a path to a `directory` containing model weights saved using :func:`~transformers.PreTrainedModel.save_pretrained`, e.g.: ``./my_model_directory/decoder``.\n                - a path or url to a `tensorflow index checkpoint file` (e.g. `./tf_model/model.ckpt.index`). In this case, ``from_tf`` should be set to True and a configuration object should be provided as ``config`` argument. This loading path is slower than converting the TensorFlow checkpoint in a PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.\n\n            model_args: (`optional`) Sequence of positional arguments:\n                All remaning positional arguments will be passed to the underlying model's ``__init__`` method\n\n            config: (`optional`) instance of a class derived from :class:`~transformers.PretrainedConfig`:\n                Configuration for the model to use instead of an automatically loaded configuation. Configuration can be automatically loaded when:\n\n                - the model is a model provided by the library (loaded with the ``shortcut-name`` string of a pretrained model), or\n                - the model was saved using :func:`~transformers.PreTrainedModel.save_pretrained` and is reloaded by suppling the save directory.\n                - the model is loaded by suppling a local directory as ``pretrained_model_name_or_path`` and a configuration JSON file named `config.json` is found in the directory.\n\n            state_dict: (`optional`) dict:\n                an optional state dictionnary for the model to use instead of a state dictionary loaded from saved weights file.\n                This option can be used if you want to create a model from a pretrained configuration but load your own weights.\n                In this case though, you should check if using :func:`~transformers.PreTrainedModel.save_pretrained` and :func:`~transformers.PreTrainedModel.from_pretrained` is not a simpler option.\n\n            cache_dir: (`optional`) string:\n                Path to a directory in which a downloaded pre-trained model\n                configuration should be cached if the standard cache should not be used.\n\n            force_download: (`optional`) boolean, default False:\n                Force to (re-)download the model weights and configuration files and override the cached versions if they exists.\n\n            proxies: (`optional`) dict, default None:\n                A dictionary of proxy servers to use by protocol or endpoint, e.g.: {'http': 'foo.bar:3128', 'http://hostname': 'foo.bar:4012'}.\n                The proxies are used on each request.\n\n            output_loading_info: (`optional`) boolean:\n                Set to ``True`` to also return a dictionnary containing missing keys, unexpected keys and error messages.\n\n            kwargs: (`optional`) Remaining dictionary of keyword arguments.\n                Can be used to update the configuration object (after it being loaded) and initiate the model. (e.g. ``output_attention=True``). Behave differently depending on whether a `config` is provided or automatically loaded:\n\n                - If a configuration is provided with ``config``, ``**kwargs`` will be directly passed to the underlying model's ``__init__`` method (we assume all relevant updates to the configuration have already been done)\n                - If a configuration is not provided, ``kwargs`` will be first passed to the configuration class initialization function (:func:`~transformers.PretrainedConfig.from_pretrained`). Each key of ``kwargs`` that corresponds to a configuration attribute will be used to override said attribute with the supplied ``kwargs`` value. Remaining keys that do not correspond to any configuration attribute will be passed to the underlying model's ``__init__`` function.\n\n                You can specify kwargs sepcific for the encoder and decoder by prefixing the key with `encoder_` and `decoder_` respectively. (e.g. ``decoder_output_attention=True``). The remaining kwargs will be passed to both encoders and decoders.\n\n        Examples::\n\n            # For example purposes. Not runnable.\n            model = PreTrainedEncoderDecoder.from_pretrained('bert-base-uncased', 'bert-base-uncased') # initialize Bert2Bert\n        \"\"\"", "\n", "\n", "# keyword arguments come in 3 flavors: encoder-specific (prefixed by", "\n", "# `encoder_`), decoder-specific (prefixed by `decoder_`) and those", "\n", "# that apply to the model as a whole.", "\n", "# We let the specific kwargs override the common ones in case of conflict.", "\n", "kwargs_common", "=", "{", "\n", "argument", ":", "value", "\n", "for", "argument", ",", "value", "in", "kwargs", ".", "items", "(", ")", "\n", "if", "not", "argument", ".", "startswith", "(", "\"encoder_\"", ")", "\n", "and", "not", "argument", ".", "startswith", "(", "\"decoder_\"", ")", "\n", "}", "\n", "kwargs_decoder", "=", "kwargs_common", ".", "copy", "(", ")", "\n", "kwargs_encoder", "=", "kwargs_common", ".", "copy", "(", ")", "\n", "kwargs_encoder", ".", "update", "(", "\n", "{", "\n", "argument", "[", "len", "(", "\"encoder_\"", ")", ":", "]", ":", "value", "\n", "for", "argument", ",", "value", "in", "kwargs", ".", "items", "(", ")", "\n", "if", "argument", ".", "startswith", "(", "\"encoder_\"", ")", "\n", "}", "\n", ")", "\n", "kwargs_decoder", ".", "update", "(", "\n", "{", "\n", "argument", "[", "len", "(", "\"decoder_\"", ")", ":", "]", ":", "value", "\n", "for", "argument", ",", "value", "in", "kwargs", ".", "items", "(", ")", "\n", "if", "argument", ".", "startswith", "(", "\"decoder_\"", ")", "\n", "}", "\n", ")", "\n", "\n", "# Load and initialize the encoder and decoder", "\n", "# The distinction between encoder and decoder at the model level is made", "\n", "# by the value of the flag `is_decoder` that we need to set correctly.", "\n", "encoder", "=", "kwargs_encoder", ".", "pop", "(", "\"model\"", ",", "None", ")", "\n", "if", "encoder", "is", "None", ":", "\n", "            ", "encoder", "=", "AutoModel", ".", "from_pretrained", "(", "\n", "encoder_pretrained_model_name_or_path", ",", "*", "model_args", ",", "**", "kwargs_encoder", "\n", ")", "\n", "", "encoder", ".", "config", ".", "is_decoder", "=", "False", "\n", "\n", "decoder", "=", "kwargs_decoder", ".", "pop", "(", "\"model\"", ",", "None", ")", "\n", "if", "decoder", "is", "None", ":", "\n", "            ", "decoder", "=", "AutoModelWithLMHead", ".", "from_pretrained", "(", "\n", "decoder_pretrained_model_name_or_path", ",", "**", "kwargs_decoder", "\n", ")", "\n", "", "decoder", ".", "config", ".", "is_decoder", "=", "True", "\n", "\n", "model", "=", "cls", "(", "encoder", ",", "decoder", ")", "\n", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.modeling_encoder_decoder.PreTrainedEncoderDecoder.save_pretrained": [[167, 218], ["modeling_encoder_decoder.PreTrainedEncoderDecoder.encoder.save_pretrained", "modeling_encoder_decoder.PreTrainedEncoderDecoder.decoder.save_pretrained", "os.path.exists", "os.mkdir", "len", "os.path.exists", "os.mkdir", "os.path.join", "os.path.exists", "os.mkdir", "os.path.join", "os.listdir", "os.path.isdir", "print", "os.listdir", "os.rmdir", "len", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.remove", "os.path.join", "os.listdir", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.MeetingNet_Transformer.MeetingNet_Transformer.save_pretrained", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.MeetingNet_Transformer.MeetingNet_Transformer.save_pretrained"], ["", "def", "save_pretrained", "(", "self", ",", "save_directory", ")", ":", "\n", "        ", "\"\"\"Save a Seq2Seq model and its configuration file in a format such\n        that it can be loaded using `:func:`~transformers.PreTrainedEncoderDecoder.from_pretrained`\n\n        We save the encoder' and decoder's parameters in two separate directories.\n        \"\"\"", "\n", "\n", "# If the root output directory does not exist, create it", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "save_directory", ")", ":", "\n", "            ", "os", ".", "mkdir", "(", "save_directory", ")", "\n", "\n", "# Check whether the output directory is empty or not", "\n", "", "sub_directories", "=", "[", "\n", "directory", "\n", "for", "directory", "in", "os", ".", "listdir", "(", "save_directory", ")", "\n", "if", "os", ".", "path", ".", "isdir", "(", "os", ".", "path", ".", "join", "(", "save_directory", ",", "directory", ")", ")", "\n", "]", "\n", "\n", "if", "len", "(", "sub_directories", ")", ">", "0", ":", "\n", "            ", "if", "\"encoder\"", "in", "sub_directories", "and", "\"decoder\"", "in", "sub_directories", ":", "\n", "                ", "print", "(", "\n", "\"WARNING: there is an older version of encoder-decoder saved in\"", "\n", "+", "\" the output directory. The default behaviour is to overwrite them.\"", "\n", ")", "\n", "\n", "# Empty the output directory", "\n", "", "for", "directory_to_remove", "in", "sub_directories", ":", "\n", "# Remove all files into the subdirectory", "\n", "                ", "files_to_remove", "=", "os", ".", "listdir", "(", "\n", "os", ".", "path", ".", "join", "(", "save_directory", ",", "directory_to_remove", ")", "\n", ")", "\n", "for", "file_to_remove", "in", "files_to_remove", ":", "\n", "                    ", "os", ".", "remove", "(", "\n", "os", ".", "path", ".", "join", "(", "\n", "save_directory", ",", "directory_to_remove", ",", "file_to_remove", "\n", ")", "\n", ")", "\n", "# Remove the subdirectory itself", "\n", "", "os", ".", "rmdir", "(", "os", ".", "path", ".", "join", "(", "save_directory", ",", "directory_to_remove", ")", ")", "\n", "\n", "", "assert", "len", "(", "os", ".", "listdir", "(", "save_directory", ")", ")", "==", "0", "# sanity check", "\n", "\n", "# Create the \"encoder\" directory inside the output directory and save the encoder into it", "\n", "", "if", "not", "os", ".", "path", ".", "exists", "(", "os", ".", "path", ".", "join", "(", "save_directory", ",", "\"encoder\"", ")", ")", ":", "\n", "            ", "os", ".", "mkdir", "(", "os", ".", "path", ".", "join", "(", "save_directory", ",", "\"encoder\"", ")", ")", "\n", "", "self", ".", "encoder", ".", "save_pretrained", "(", "os", ".", "path", ".", "join", "(", "save_directory", ",", "\"encoder\"", ")", ")", "\n", "\n", "# Create the \"encoder\" directory inside the output directory and save the decoder into it", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "os", ".", "path", ".", "join", "(", "save_directory", ",", "\"decoder\"", ")", ")", ":", "\n", "            ", "os", ".", "mkdir", "(", "os", ".", "path", ".", "join", "(", "save_directory", ",", "\"decoder\"", ")", ")", "\n", "", "self", ".", "decoder", ".", "save_pretrained", "(", "os", ".", "path", ".", "join", "(", "save_directory", ",", "\"decoder\"", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.modeling_encoder_decoder.PreTrainedEncoderDecoder.prepare_model_kwargs": [[219, 255], ["kwargs_common.copy", "kwargs_common.copy", "kwargs_common.copy.update", "kwargs_common.copy.update", "kwargs_common.copy.get", "kwargs.items", "kwargs.items", "argument.startswith", "kwargs.items", "argument.startswith", "argument.startswith", "argument.startswith", "len", "len"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.HMNetTrainer.HMNetTrainer.update", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.HMNetTrainer.HMNetTrainer.update", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.closablequeue.ClosableQueue.get"], ["", "@", "staticmethod", "\n", "def", "prepare_model_kwargs", "(", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"Prepare the encoder and decoder's keyword arguments.\n        Keyword arguments come in 3 flavors:\n        - encoder-specific (prefixed by `encoder_`)\n        - decoder-specific (prefixed by `decoder_`)\n        - those that apply to the model as whole.\n        We let the specific kwargs override the common ones in case of\n        conflict.\n        \"\"\"", "\n", "kwargs_common", "=", "{", "\n", "argument", ":", "value", "\n", "for", "argument", ",", "value", "in", "kwargs", ".", "items", "(", ")", "\n", "if", "not", "argument", ".", "startswith", "(", "\"encoder_\"", ")", "\n", "and", "not", "argument", ".", "startswith", "(", "\"decoder_\"", ")", "\n", "}", "\n", "decoder_kwargs", "=", "kwargs_common", ".", "copy", "(", ")", "\n", "encoder_kwargs", "=", "kwargs_common", ".", "copy", "(", ")", "\n", "encoder_kwargs", ".", "update", "(", "\n", "{", "\n", "argument", "[", "len", "(", "\"encoder_\"", ")", ":", "]", ":", "value", "\n", "for", "argument", ",", "value", "in", "kwargs", ".", "items", "(", ")", "\n", "if", "argument", ".", "startswith", "(", "\"encoder_\"", ")", "\n", "}", "\n", ")", "\n", "decoder_kwargs", ".", "update", "(", "\n", "{", "\n", "argument", "[", "len", "(", "\"decoder_\"", ")", ":", "]", ":", "value", "\n", "for", "argument", ",", "value", "in", "kwargs", ".", "items", "(", ")", "\n", "if", "argument", ".", "startswith", "(", "\"decoder_\"", ")", "\n", "}", "\n", ")", "\n", "decoder_kwargs", "[", "\"encoder_attention_mask\"", "]", "=", "encoder_kwargs", ".", "get", "(", "\n", "\"attention_mask\"", ",", "None", "\n", ")", "\n", "return", "encoder_kwargs", ",", "decoder_kwargs", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.modeling_encoder_decoder.PreTrainedEncoderDecoder.forward": [[256, 289], ["modeling_encoder_decoder.PreTrainedEncoderDecoder.prepare_model_kwargs", "kwargs_encoder.pop", "modeling_encoder_decoder.PreTrainedEncoderDecoder.decoder", "modeling_encoder_decoder.PreTrainedEncoderDecoder.encoder"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.modeling_encoder_decoder.PreTrainedEncoderDecoder.prepare_model_kwargs", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizerFast.decoder"], ["", "def", "forward", "(", "self", ",", "encoder_input_ids", "=", "None", ",", "decoder_input_ids", "=", "None", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"The forward pass on a seq2eq depends what we are performing:\n\n        - During training we perform one forward pass through both the encoder\n          and decoder;\n        - During prediction, we perform one forward pass through the encoder,\n          and then perform several forward passes with the encoder's hidden\n          state through the decoder to decode a full sequence.\n\n        Therefore, we skip the forward pass on the encoder if an argument named\n        `encoder_hidden_state` is passed to this function.\n\n        Params:\n            encoder_input_ids: ``torch.LongTensor`` of shape ``(batch_size, sequence_length)``\n                Indices of encoder input sequence tokens in the vocabulary.\n            decoder_input_ids: ``torch.LongTensor`` of shape ``(batch_size, sequence_length)``\n                Indices of decoder input sequence tokens in the vocabulary.\n            kwargs: (`optional`) Remaining dictionary of keyword arguments.\n        \"\"\"", "\n", "kwargs_encoder", ",", "kwargs_decoder", "=", "self", ".", "prepare_model_kwargs", "(", "**", "kwargs", ")", "\n", "\n", "# Encode if needed (training, first prediction pass)", "\n", "encoder_hidden_states", "=", "kwargs_encoder", ".", "pop", "(", "\"hidden_states\"", ",", "None", ")", "\n", "if", "encoder_hidden_states", "is", "None", ":", "\n", "            ", "encoder_outputs", "=", "self", ".", "encoder", "(", "encoder_input_ids", ",", "**", "kwargs_encoder", ")", "\n", "encoder_hidden_states", "=", "encoder_outputs", "[", "0", "]", "\n", "", "else", ":", "\n", "            ", "encoder_outputs", "=", "(", ")", "\n", "\n", "", "kwargs_decoder", "[", "\"encoder_hidden_states\"", "]", "=", "encoder_hidden_states", "\n", "decoder_outputs", "=", "self", ".", "decoder", "(", "decoder_input_ids", ",", "**", "kwargs_decoder", ")", "\n", "\n", "return", "decoder_outputs", "+", "encoder_outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.modeling_encoder_decoder.PreTrainedEncoderDecoder.prepare_inputs_for_generation": [[290, 304], ["type"], "methods", ["None"], ["", "def", "prepare_inputs_for_generation", "(", "self", ",", "input_ids", ",", "past", ",", "attention_mask", ",", "**", "kwargs", ")", ":", "\n", "        ", "assert", "past", "is", "not", "None", ",", "\"past has to be defined for encoder_outputs\"", "\n", "\n", "# first step", "\n", "if", "type", "(", "past", ")", "is", "tuple", ":", "\n", "            ", "encoder_outputs", "=", "past", "\n", "", "else", ":", "\n", "            ", "encoder_outputs", "=", "(", "past", ",", ")", "\n", "\n", "", "return", "{", "\n", "\"decoder_input_ids\"", ":", "input_ids", ",", "\n", "\"encoder_outputs\"", ":", "encoder_outputs", ",", "\n", "\"encoder_hidden_states\"", ":", "encoder_outputs", "[", "0", "]", ",", "\n", "\"decoder_attention_mask\"", ":", "None", ",", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.modeling_encoder_decoder.PreTrainedEncoderDecoder.prepare_scores_for_generation": [[306, 308], ["None"], "methods", ["None"], ["", "def", "prepare_scores_for_generation", "(", "self", ",", "scores", ",", "**", "kwargs", ")", ":", "\n", "        ", "return", "scores", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.modeling_encoder_decoder.PreTrainedEncoderDecoder._do_output_past": [[309, 318], ["getattr", "getattr", "len"], "methods", ["None"], ["", "def", "_do_output_past", "(", "self", ",", "outputs", ")", ":", "\n", "        ", "\"\"\"During generation, decide whether to pass the `past` variable to the next forward pass.\"\"\"", "\n", "has_output_past", "=", "getattr", "(", "self", ".", "config", ",", "\"output_past\"", ",", "False", ")", "\n", "mem_len", "=", "getattr", "(", "self", ".", "config", ",", "\"mem_len\"", ",", "0", ")", "\n", "if", "len", "(", "outputs", ")", "<=", "1", ":", "\n", "            ", "return", "False", "\n", "", "if", "mem_len", ">", "0", "or", "has_output_past", ":", "\n", "            ", "return", "True", "\n", "", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.modeling_encoder_decoder.PreTrainedEncoderDecoder.enforce_repetition_penalty_": [[319, 330], ["range", "set", "prev_output_tokens[].tolist"], "methods", ["None"], ["", "def", "enforce_repetition_penalty_", "(", "\n", "self", ",", "lprobs", ",", "batch_size", ",", "num_beams", ",", "prev_output_tokens", ",", "repetition_penalty", "\n", ")", ":", "\n", "        ", "\"\"\"repetition penalty (from CTRL paper https://arxiv.org/abs/1909.05858).\"\"\"", "\n", "for", "i", "in", "range", "(", "batch_size", "*", "num_beams", ")", ":", "\n", "            ", "for", "previous_token", "in", "set", "(", "prev_output_tokens", "[", "i", "]", ".", "tolist", "(", ")", ")", ":", "\n", "# if score < 0 then repetition penalty has to multiplied to reduce the previous token probability", "\n", "                ", "if", "lprobs", "[", "i", ",", "previous_token", "]", "<", "0", ":", "\n", "                    ", "lprobs", "[", "i", ",", "previous_token", "]", "*=", "repetition_penalty", "\n", "", "else", ":", "\n", "                    ", "lprobs", "[", "i", ",", "previous_token", "]", "/=", "repetition_penalty", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.modeling_encoder_decoder.PreTrainedEncoderDecoder.get_encoder": [[331, 333], ["None"], "methods", ["None"], ["", "", "", "", "def", "get_encoder", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "encoder", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.modeling_encoder_decoder.PreTrainedEncoderDecoder.get_decoder": [[334, 336], ["None"], "methods", ["None"], ["", "def", "get_decoder", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "decoder", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.modeling_encoder_decoder.PreTrainedEncoderDecoder.get_output_embeddings": [[337, 339], ["modeling_encoder_decoder.PreTrainedEncoderDecoder.decoder.get_output_embeddings"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.modeling_encoder_decoder.PreTrainedEncoderDecoder.get_output_embeddings"], ["", "def", "get_output_embeddings", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "decoder", ".", "get_output_embeddings", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.modeling_encoder_decoder.PreTrainedEncoderDecoder.generate": [[340, 757], ["torch.no_grad", "isinstance", "isinstance", "modeling_encoder_decoder.PreTrainedEncoderDecoder.get_output_embeddings", "AttributeError", "isinstance", "isinstance", "isinstance", "isinstance", "isinstance", "isinstance", "torch.full", "torch.full.ne().long", "logger.warning", "hasattr", "callable", "modeling_encoder_decoder.PreTrainedEncoderDecoder.get_encoder", "modeling_encoder_decoder.PreTrainedEncoderDecoder.", "torch.full.unsqueeze().expand", "torch.full.new_ones.unsqueeze().expand", "torch.full.contiguous().view", "torch.full.new_ones.contiguous().view", "torch.full", "torch.arange().view().repeat().view().to", "modeling_encoder_decoder.PreTrainedEncoderDecoder._generate_beam_search", "modeling_encoder_decoder.PreTrainedEncoderDecoder._generate_no_beam_search", "isinstance", "isinstance", "isinstance", "isinstance", "isinstance", "isinstance", "torch.full.dim", "torch.full.new_ones", "encoder_outputs[].index_select", "torch.full.ne", "torch.full.unsqueeze", "torch.full.new_ones.unsqueeze", "torch.full.contiguous", "torch.full.new_ones.contiguous", "torch.arange().view().repeat().view", "next", "next", "modeling_encoder_decoder.PreTrainedEncoderDecoder.parameters", "modeling_encoder_decoder.PreTrainedEncoderDecoder.parameters", "torch.arange().view().repeat", "torch.arange().view", "torch.arange"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.modeling_encoder_decoder.PreTrainedEncoderDecoder.get_output_embeddings", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.modeling_encoder_decoder.PreTrainedEncoderDecoder.get_encoder", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.MeetingNet_Transformer.MeetingNet_Transformer._generate_beam_search", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.MeetingNet_Transformer.MeetingNet_Transformer._generate_no_beam_search"], ["", "@", "torch", ".", "no_grad", "(", ")", "\n", "def", "generate", "(", "\n", "self", ",", "\n", "input_ids", "=", "None", ",", "\n", "max_length", "=", "None", ",", "\n", "min_length", "=", "None", ",", "\n", "do_sample", "=", "None", ",", "\n", "early_stopping", "=", "None", ",", "\n", "num_beams", "=", "None", ",", "\n", "temperature", "=", "None", ",", "\n", "top_k", "=", "None", ",", "\n", "top_p", "=", "None", ",", "\n", "repetition_penalty", "=", "None", ",", "\n", "bad_words_ids", "=", "None", ",", "\n", "bos_token_id", "=", "None", ",", "\n", "pad_token_id", "=", "None", ",", "\n", "eos_token_id", "=", "None", ",", "\n", "length_penalty", "=", "None", ",", "\n", "no_repeat_ngram_size", "=", "None", ",", "\n", "num_return_sequences", "=", "None", ",", "\n", "attention_mask", "=", "None", ",", "\n", "decoder_start_token_id", "=", "None", ",", "\n", ")", ":", "\n", "        ", "r\"\"\"Generates sequences for models with a LM head. The method currently supports greedy decoding, beam-search decoding, sampling with temperature, sampling with top-k or nucleus sampling.\n\n        Adapted in part from `Facebook's XLM beam search code`_.\n\n        .. _`Facebook's XLM beam search code`:\n           https://github.com/facebookresearch/XLM/blob/9e6f6814d17be4fe5b15f2e6c43eb2b2d76daeb4/src/model/transformer.py#L529\n\n\n        Parameters:\n\n            input_ids: (`optional`) `torch.LongTensor` of shape `(batch_size, sequence_length)`\n                The sequence used as a prompt for the generation. If `None` the method initializes\n                it as an empty `torch.LongTensor` of shape `(1,)`.\n\n            max_length: (`optional`) int\n                The max length of the sequence to be generated.  Between `min_length` and infinity. Default to 20.\n\n            min_length: (`optional`) int\n                The min length of the sequence to be generated.  Between 0 and infinity. Default to 0.\n\n            do_sample: (`optional`) bool\n                If set to `False` greedy decoding is used. Otherwise sampling is used. Defaults to `False` as defined in `configuration_utils.PretrainedConfig`.\n\n            early_stopping: (`optional`) bool\n                if set to `True` beam search is stopped when at least `num_beams` sentences finished per batch. Defaults to `False` as defined in `configuration_utils.PretrainedConfig`.\n\n            num_beams: (`optional`) int\n                Number of beams for beam search. Must be between 1 and infinity. 1 means no beam search. Default to 1.\n\n            temperature: (`optional`) float\n                The value used to module the next token probabilities. Must be strictly positive. Default to 1.0.\n\n            top_k: (`optional`) int\n                The number of highest probability vocabulary tokens to keep for top-k-filtering. Between 1 and infinity. Default to 50.\n\n            top_p: (`optional`) float\n                The cumulative probability of parameter highest probability vocabulary tokens to keep for nucleus sampling. Must be between 0 and 1. Default to 1.\n\n            repetition_penalty: (`optional`) float\n                The parameter for repetition penalty. Between 1.0 and infinity. 1.0 means no penalty. Default to 1.0.\n\n            pad_token_id: (`optional`) int\n                Padding token. Default to specicic model pad_token_id or None if it does not exist.\n\n            bos_token_id: (`optional`) int\n                BOS token. Defaults to `bos_token_id` as defined in the models config.\n\n            eos_token_id: (`optional`) int\n                EOS token. Defaults to `eos_token_id` as defined in the models config.\n\n            length_penalty: (`optional`) float\n                Exponential penalty to the length. Default to 1.\n\n            no_repeat_ngram_size: (`optional`) int\n                If set to int > 0, all ngrams of size `no_repeat_ngram_size` can only occur once.\n            bad_words_ids: (`optional`) list of lists of int\n                `bad_words_ids` contains tokens that are not allowed to be generated. In order to get the tokens of the words that should not appear in the generated text, use `tokenizer.encode(bad_word, add_prefix_space=True)`.\n\n            num_return_sequences: (`optional`) int\n                The number of independently computed returned sequences for each element in the batch. Default to 1.\n\n            attention_mask (`optional`) obj: `torch.LongTensor` of same shape as `input_ids`\n                Mask to avoid performing attention on padding token indices.\n                Mask values selected in ``[0, 1]``:\n                ``1`` for tokens that are NOT MASKED, ``0`` for MASKED tokens.\n                Defaults to `None`.\n\n            `What are attention masks? <../glossary.html#attention-mask>`__\n\n            decoder_start_token_id=None: (`optional`) int\n                If an encoder-decoder model starts decoding with a different token than BOS.\n                Defaults to `None` and is changed to `BOS` later.\n\n        Return:\n\n            output: `torch.LongTensor` of shape `(batch_size * num_return_sequences, sequence_length)`\n                sequence_length is either equal to max_length or shorter if all batches finished early due to the `eos_token_id`\n\n        Examples::\n\n            tokenizer = AutoTokenizer.from_pretrained('distilgpt2')   # Initialize tokenizer\n            model = AutoModelWithLMHead.from_pretrained('distilgpt2')    # Download model and configuration from S3 and cache.\n            outputs = model.generate(max_length=40)  # do greedy decoding\n            print('Generated: {}'.format(tokenizer.decode(outputs[0], skip_special_tokens=True)))\n\n            tokenizer = AutoTokenizer.from_pretrained('openai-gpt')   # Initialize tokenizer\n            model = AutoModelWithLMHead.from_pretrained('openai-gpt')    # Download model and configuration from S3 and cache.\n            input_context = 'The dog'\n            input_ids = tokenizer.encode(input_context, return_tensors='pt')  # encode input context\n            outputs = model.generate(input_ids=input_ids, num_beams=5, num_return_sequences=3, temperature=1.5)  # generate 3 independent sequences using beam search decoding (5 beams) with sampling from initial context 'The dog'\n            for i in range(3): #  3 output sequences were generated\n                print('Generated {}: {}'.format(i, tokenizer.decode(outputs[i], skip_special_tokens=True)))\n\n            tokenizer = AutoTokenizer.from_pretrained('distilgpt2')   # Initialize tokenizer\n            model = AutoModelWithLMHead.from_pretrained('distilgpt2')    # Download model and configuration from S3 and cache.\n            input_context = 'The dog'\n            input_ids = tokenizer.encode(input_context, return_tensors='pt')  # encode input context\n            outputs = model.generate(input_ids=input_ids, max_length=40, temperature=0.7, num_return_sequences=3)  # 3 generate sequences using by sampling\n            for i in range(3): #  3 output sequences were generated\n                print('Generated {}: {}'.format(i, tokenizer.decode(outputs[i], skip_special_tokens=True)))\n\n            tokenizer = AutoTokenizer.from_pretrained('ctrl')   # Initialize tokenizer\n            model = AutoModelWithLMHead.from_pretrained('ctrl')    # Download model and configuration from S3 and cache.\n            input_context = 'Legal My neighbor is'  # \"Legal\" is one of the control codes for ctrl\n            input_ids = tokenizer.encode(input_context, return_tensors='pt')  # encode input context\n            outputs = model.generate(input_ids=input_ids, max_length=50, temperature=0.7, repetition_penalty=1.2)  # generate sequences\n            print('Generated: {}'.format(tokenizer.decode(outputs[0], skip_special_tokens=True)))\n\n            tokenizer = AutoTokenizer.from_pretrained('gpt2')   # Initialize tokenizer\n            model = AutoModelWithLMHead.from_pretrained('gpt2')    # Download model and configuration from S3 and cache.\n            input_context = 'My cute dog'  # \"Legal\" is one of the control codes for ctrl\n            bad_words_ids = [tokenizer.encode(bad_word, add_prefix_space=True) for bad_word in ['idiot', 'stupid', 'shut up']]\n            input_ids = tokenizer.encode(input_context, return_tensors='pt')  # encode input context\n            outputs = model.generate(input_ids=input_ids, max_length=100, do_sample=True, bad_words_ids=bad_words_ids)  # generate sequences without allowing bad_words to be generated\n        \"\"\"", "\n", "\n", "# We cannot generate if the model does not have a LM head", "\n", "if", "self", ".", "get_output_embeddings", "(", ")", "is", "None", ":", "\n", "            ", "raise", "AttributeError", "(", "\n", "\"You tried to generate sequences with a model that does not have a LM Head.\"", "\n", "\"Please use another model class (e.g. `OpenAIGPTLMHeadModel`, `XLNetLMHeadModel`, `GPT2LMHeadModel`, `CTRLLMHeadModel`, `T5WithLMHeadModel`, `TransfoXLLMHeadModel`, `XLMWithLMHeadModel`, `BartForConditionalGeneration` )\"", "\n", ")", "\n", "\n", "", "max_length", "=", "max_length", "if", "max_length", "is", "not", "None", "else", "self", ".", "config", ".", "max_length", "\n", "min_length", "=", "min_length", "if", "min_length", "is", "not", "None", "else", "self", ".", "config", ".", "min_length", "\n", "do_sample", "=", "do_sample", "if", "do_sample", "is", "not", "None", "else", "self", ".", "config", ".", "do_sample", "\n", "early_stopping", "=", "(", "\n", "early_stopping", "if", "early_stopping", "is", "not", "None", "else", "self", ".", "config", ".", "early_stopping", "\n", ")", "\n", "num_beams", "=", "num_beams", "if", "num_beams", "is", "not", "None", "else", "self", ".", "config", ".", "num_beams", "\n", "temperature", "=", "(", "\n", "temperature", "if", "temperature", "is", "not", "None", "else", "self", ".", "config", ".", "temperature", "\n", ")", "\n", "top_k", "=", "top_k", "if", "top_k", "is", "not", "None", "else", "self", ".", "config", ".", "top_k", "\n", "top_p", "=", "top_p", "if", "top_p", "is", "not", "None", "else", "self", ".", "config", ".", "top_p", "\n", "repetition_penalty", "=", "(", "\n", "repetition_penalty", "\n", "if", "repetition_penalty", "is", "not", "None", "\n", "else", "self", ".", "config", ".", "repetition_penalty", "\n", ")", "\n", "bos_token_id", "=", "(", "\n", "bos_token_id", "if", "bos_token_id", "is", "not", "None", "else", "self", ".", "config", ".", "bos_token_id", "\n", ")", "\n", "pad_token_id", "=", "(", "\n", "pad_token_id", "if", "pad_token_id", "is", "not", "None", "else", "self", ".", "config", ".", "pad_token_id", "\n", ")", "\n", "eos_token_id", "=", "(", "\n", "eos_token_id", "if", "eos_token_id", "is", "not", "None", "else", "self", ".", "config", ".", "eos_token_id", "\n", ")", "\n", "length_penalty", "=", "(", "\n", "length_penalty", "if", "length_penalty", "is", "not", "None", "else", "self", ".", "config", ".", "length_penalty", "\n", ")", "\n", "no_repeat_ngram_size", "=", "(", "\n", "no_repeat_ngram_size", "\n", "if", "no_repeat_ngram_size", "is", "not", "None", "\n", "else", "self", ".", "config", ".", "no_repeat_ngram_size", "\n", ")", "\n", "bad_words_ids", "=", "(", "\n", "bad_words_ids", "if", "bad_words_ids", "is", "not", "None", "else", "self", ".", "config", ".", "bad_words_ids", "\n", ")", "\n", "num_return_sequences", "=", "(", "\n", "num_return_sequences", "\n", "if", "num_return_sequences", "is", "not", "None", "\n", "else", "self", ".", "config", ".", "num_return_sequences", "\n", ")", "\n", "decoder_start_token_id", "=", "(", "\n", "decoder_start_token_id", "\n", "if", "decoder_start_token_id", "is", "not", "None", "\n", "else", "self", ".", "config", ".", "decoder_start_token_id", "\n", ")", "\n", "\n", "if", "input_ids", "is", "not", "None", ":", "\n", "            ", "batch_size", "=", "input_ids", ".", "shape", "[", "0", "]", "# overriden by the input batch_size", "\n", "", "else", ":", "\n", "            ", "batch_size", "=", "1", "\n", "\n", "", "assert", "(", "\n", "isinstance", "(", "max_length", ",", "int", ")", "and", "max_length", ">", "0", "\n", ")", ",", "\"`max_length` should be a strictly positive integer.\"", "\n", "assert", "(", "\n", "isinstance", "(", "min_length", ",", "int", ")", "and", "min_length", ">=", "0", "\n", ")", ",", "\"`min_length` should be a positive integer.\"", "\n", "assert", "isinstance", "(", "do_sample", ",", "bool", ")", ",", "\"`do_sample` should be a boolean.\"", "\n", "assert", "isinstance", "(", "early_stopping", ",", "bool", ")", ",", "\"`early_stopping` should be a boolean.\"", "\n", "assert", "(", "\n", "isinstance", "(", "num_beams", ",", "int", ")", "and", "num_beams", ">", "0", "\n", ")", ",", "\"`num_beams` should be a strictly positive integer.\"", "\n", "assert", "temperature", ">", "0", ",", "\"`temperature` should be strictly positive.\"", "\n", "assert", "(", "\n", "isinstance", "(", "top_k", ",", "int", ")", "and", "top_k", ">=", "0", "\n", ")", ",", "\"`top_k` should be a positive integer.\"", "\n", "assert", "0", "<=", "top_p", "<=", "1", ",", "\"`top_p` should be between 0 and 1.\"", "\n", "assert", "repetition_penalty", ">=", "1.0", ",", "\"`repetition_penalty` should be >= 1.\"", "\n", "assert", "input_ids", "is", "not", "None", "or", "(", "\n", "isinstance", "(", "bos_token_id", ",", "int", ")", "and", "bos_token_id", ">=", "0", "\n", ")", ",", "\"If input_ids is not defined, `bos_token_id` should be a positive integer.\"", "\n", "assert", "pad_token_id", "is", "None", "or", "(", "\n", "isinstance", "(", "pad_token_id", ",", "int", ")", "and", "(", "pad_token_id", ">=", "0", ")", "\n", ")", ",", "\"`pad_token_id` should be a positive integer.\"", "\n", "assert", "(", "eos_token_id", "is", "None", ")", "or", "(", "\n", "isinstance", "(", "eos_token_id", ",", "int", ")", "and", "(", "eos_token_id", ">=", "0", ")", "\n", ")", ",", "\"`eos_token_id` should be a positive integer.\"", "\n", "assert", "length_penalty", ">", "0", ",", "\"`length_penalty` should be strictly positive.\"", "\n", "assert", "(", "\n", "isinstance", "(", "no_repeat_ngram_size", ",", "int", ")", "and", "no_repeat_ngram_size", ">=", "0", "\n", ")", ",", "\"`no_repeat_ngram_size` should be a positive integer.\"", "\n", "assert", "(", "\n", "isinstance", "(", "num_return_sequences", ",", "int", ")", "and", "num_return_sequences", ">", "0", "\n", ")", ",", "\"`num_return_sequences` should be a strictly positive integer.\"", "\n", "assert", "(", "\n", "bad_words_ids", "is", "None", "\n", "or", "isinstance", "(", "bad_words_ids", ",", "list", ")", "\n", "and", "isinstance", "(", "bad_words_ids", "[", "0", "]", ",", "list", ")", "\n", ")", ",", "\"`bad_words_ids` is either `None` or a list of lists of tokens that should not be generated\"", "\n", "\n", "if", "input_ids", "is", "None", ":", "\n", "            ", "assert", "isinstance", "(", "bos_token_id", ",", "int", ")", "and", "bos_token_id", ">=", "0", ",", "(", "\n", "\"you should either supply a context to complete as `input_ids` input \"", "\n", "\"or a `bos_token_id` (integer >= 0) as a first token to start the generation.\"", "\n", ")", "\n", "input_ids", "=", "torch", ".", "full", "(", "\n", "(", "batch_size", ",", "1", ")", ",", "\n", "bos_token_id", ",", "\n", "dtype", "=", "torch", ".", "long", ",", "\n", "device", "=", "next", "(", "self", ".", "parameters", "(", ")", ")", ".", "device", ",", "\n", ")", "\n", "", "else", ":", "\n", "            ", "assert", "(", "\n", "input_ids", ".", "dim", "(", ")", "==", "2", "\n", ")", ",", "\"Input prompt should be of shape (batch_size, sequence length).\"", "\n", "\n", "# not allow to duplicate outputs when greedy decoding", "\n", "", "if", "do_sample", "is", "False", ":", "\n", "            ", "if", "num_beams", "==", "1", ":", "\n", "# no_beam_search greedy generation conditions", "\n", "                ", "assert", "(", "\n", "num_return_sequences", "==", "1", "\n", ")", ",", "\"Greedy decoding will always produce the same output for num_beams == 1 and num_return_sequences > 1. Please set num_return_sequences = 1\"", "\n", "\n", "", "else", ":", "\n", "# beam_search greedy generation conditions", "\n", "                ", "assert", "(", "\n", "num_beams", ">=", "num_return_sequences", "\n", ")", ",", "\"Greedy beam search decoding cannot return more sequences than it has beams. Please set num_beams >= num_return_sequences\"", "\n", "\n", "# create attention mask if necessary", "\n", "# TODO (PVP): this should later be handled by the forward fn() in each model in the future see PR 3140", "\n", "", "", "if", "(", "\n", "(", "attention_mask", "is", "None", ")", "\n", "and", "(", "pad_token_id", "is", "not", "None", ")", "\n", "and", "(", "pad_token_id", "in", "input_ids", ")", "\n", ")", ":", "\n", "            ", "attention_mask", "=", "input_ids", ".", "ne", "(", "pad_token_id", ")", ".", "long", "(", ")", "\n", "", "elif", "attention_mask", "is", "None", ":", "\n", "            ", "attention_mask", "=", "input_ids", ".", "new_ones", "(", "input_ids", ".", "shape", ")", "\n", "\n", "# set pad_token_id to eos_token_id if not set. Important that this is done after", "\n", "# attention_mask is created", "\n", "", "if", "pad_token_id", "is", "None", "and", "eos_token_id", "is", "not", "None", ":", "\n", "            ", "logger", ".", "warning", "(", "\n", "\"Setting `pad_token_id` to {} (first `eos_token_id`) to generate sequence\"", ".", "format", "(", "\n", "eos_token_id", "\n", ")", "\n", ")", "\n", "pad_token_id", "=", "eos_token_id", "\n", "\n", "# current position and vocab size", "\n", "", "vocab_size", "=", "self", ".", "config", ".", "vocab_size", "\n", "\n", "# set effective batch size and effective batch multiplier according to do_sample", "\n", "if", "do_sample", ":", "\n", "            ", "effective_batch_size", "=", "batch_size", "*", "num_return_sequences", "\n", "effective_batch_mult", "=", "num_return_sequences", "\n", "", "else", ":", "\n", "            ", "effective_batch_size", "=", "batch_size", "\n", "effective_batch_mult", "=", "1", "\n", "\n", "", "if", "self", ".", "config", ".", "is_encoder_decoder", ":", "\n", "            ", "if", "decoder_start_token_id", "is", "None", ":", "\n", "                ", "decoder_start_token_id", "=", "bos_token_id", "\n", "\n", "", "assert", "(", "\n", "decoder_start_token_id", "is", "not", "None", "\n", ")", ",", "\"decoder_start_token_id or bos_token_id has to be defined for encoder-decoder generation\"", "\n", "assert", "hasattr", "(", "\n", "self", ",", "\"get_encoder\"", "\n", ")", ",", "\"{} should have a 'get_encoder' function defined\"", ".", "format", "(", "self", ")", "\n", "assert", "callable", "(", "self", ".", "get_encoder", ")", ",", "\"{} should be a method\"", ".", "format", "(", "\n", "self", ".", "get_encoder", "\n", ")", "\n", "\n", "# get encoder and store encoder outputs", "\n", "encoder", "=", "self", ".", "get_encoder", "(", ")", "\n", "\n", "encoder_outputs", "=", "encoder", "(", "input_ids", ",", "attention_mask", "=", "attention_mask", ")", "\n", "\n", "# Expand input ids if num_beams > 1 or num_return_sequences > 1", "\n", "", "if", "num_return_sequences", ">", "1", "or", "num_beams", ">", "1", ":", "\n", "            ", "input_ids_len", "=", "input_ids", ".", "shape", "[", "-", "1", "]", "\n", "input_ids", "=", "input_ids", ".", "unsqueeze", "(", "1", ")", ".", "expand", "(", "\n", "batch_size", ",", "effective_batch_mult", "*", "num_beams", ",", "input_ids_len", "\n", ")", "\n", "attention_mask", "=", "attention_mask", ".", "unsqueeze", "(", "1", ")", ".", "expand", "(", "\n", "batch_size", ",", "effective_batch_mult", "*", "num_beams", ",", "input_ids_len", "\n", ")", "\n", "\n", "input_ids", "=", "input_ids", ".", "contiguous", "(", ")", ".", "view", "(", "\n", "effective_batch_size", "*", "num_beams", ",", "input_ids_len", "\n", ")", "# shape: (batch_size * num_return_sequences * num_beams, cur_len)", "\n", "attention_mask", "=", "attention_mask", ".", "contiguous", "(", ")", ".", "view", "(", "\n", "effective_batch_size", "*", "num_beams", ",", "input_ids_len", "\n", ")", "# shape: (batch_size * num_return_sequences * num_beams, cur_len)", "\n", "\n", "", "if", "self", ".", "config", ".", "is_encoder_decoder", ":", "\n", "# create empty decoder_input_ids", "\n", "            ", "input_ids", "=", "torch", ".", "full", "(", "\n", "(", "effective_batch_size", "*", "num_beams", ",", "1", ")", ",", "\n", "decoder_start_token_id", ",", "\n", "dtype", "=", "torch", ".", "long", ",", "\n", "device", "=", "next", "(", "self", ".", "parameters", "(", ")", ")", ".", "device", ",", "\n", ")", "\n", "cur_len", "=", "1", "\n", "\n", "assert", "(", "\n", "batch_size", "==", "encoder_outputs", "[", "0", "]", ".", "shape", "[", "0", "]", "\n", ")", ",", "f\"expected encoder_outputs[0] to have 1st dimension bs={batch_size}, got {encoder_outputs[0].shape[0]} \"", "\n", "\n", "# expand batch_idx to assign correct encoder output for expanded input_ids (due to num_beams > 1 and num_return_sequences > 1)", "\n", "expanded_batch_idxs", "=", "(", "\n", "torch", ".", "arange", "(", "batch_size", ")", "\n", ".", "view", "(", "-", "1", ",", "1", ")", "\n", ".", "repeat", "(", "1", ",", "num_beams", "*", "effective_batch_mult", ")", "\n", ".", "view", "(", "-", "1", ")", "\n", ".", "to", "(", "input_ids", ".", "device", ")", "\n", ")", "\n", "# expand encoder_outputs", "\n", "encoder_outputs", "=", "(", "\n", "encoder_outputs", "[", "0", "]", ".", "index_select", "(", "0", ",", "expanded_batch_idxs", ")", ",", "\n", "*", "encoder_outputs", "[", "1", ":", "]", ",", "\n", ")", "\n", "\n", "", "else", ":", "\n", "            ", "encoder_outputs", "=", "None", "\n", "cur_len", "=", "input_ids", ".", "shape", "[", "-", "1", "]", "\n", "\n", "", "if", "num_beams", ">", "1", ":", "\n", "            ", "output", "=", "self", ".", "_generate_beam_search", "(", "\n", "input_ids", ",", "\n", "cur_len", "=", "cur_len", ",", "\n", "max_length", "=", "max_length", ",", "\n", "min_length", "=", "min_length", ",", "\n", "do_sample", "=", "do_sample", ",", "\n", "early_stopping", "=", "early_stopping", ",", "\n", "temperature", "=", "temperature", ",", "\n", "top_k", "=", "top_k", ",", "\n", "top_p", "=", "top_p", ",", "\n", "repetition_penalty", "=", "repetition_penalty", ",", "\n", "no_repeat_ngram_size", "=", "no_repeat_ngram_size", ",", "\n", "bad_words_ids", "=", "bad_words_ids", ",", "\n", "bos_token_id", "=", "bos_token_id", ",", "\n", "pad_token_id", "=", "pad_token_id", ",", "\n", "decoder_start_token_id", "=", "decoder_start_token_id", ",", "\n", "eos_token_id", "=", "eos_token_id", ",", "\n", "batch_size", "=", "effective_batch_size", ",", "\n", "num_return_sequences", "=", "num_return_sequences", ",", "\n", "length_penalty", "=", "length_penalty", ",", "\n", "num_beams", "=", "num_beams", ",", "\n", "vocab_size", "=", "vocab_size", ",", "\n", "encoder_outputs", "=", "encoder_outputs", ",", "\n", "attention_mask", "=", "attention_mask", ",", "\n", ")", "\n", "", "else", ":", "\n", "            ", "output", "=", "self", ".", "_generate_no_beam_search", "(", "\n", "input_ids", ",", "\n", "cur_len", "=", "cur_len", ",", "\n", "max_length", "=", "max_length", ",", "\n", "min_length", "=", "min_length", ",", "\n", "do_sample", "=", "do_sample", ",", "\n", "temperature", "=", "temperature", ",", "\n", "top_k", "=", "top_k", ",", "\n", "top_p", "=", "top_p", ",", "\n", "repetition_penalty", "=", "repetition_penalty", ",", "\n", "no_repeat_ngram_size", "=", "no_repeat_ngram_size", ",", "\n", "bad_words_ids", "=", "bad_words_ids", ",", "\n", "bos_token_id", "=", "bos_token_id", ",", "\n", "pad_token_id", "=", "pad_token_id", ",", "\n", "decoder_start_token_id", "=", "decoder_start_token_id", ",", "\n", "eos_token_id", "=", "eos_token_id", ",", "\n", "batch_size", "=", "effective_batch_size", ",", "\n", "encoder_outputs", "=", "encoder_outputs", ",", "\n", "attention_mask", "=", "attention_mask", ",", "\n", ")", "\n", "\n", "", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.modeling_encoder_decoder.PreTrainedEncoderDecoder._generate_no_beam_search": [[758, 900], ["torch.cat.new().fill_", "torch.cat.new().fill_", "enumerate", "modeling_encoder_decoder.PreTrainedEncoderDecoder.prepare_inputs_for_generation", "modeling_encoder_decoder.PreTrainedEncoderDecoder.", "modeling_encoder_decoder.PreTrainedEncoderDecoder._do_output_past", "torch.cat", "torch.cat.new().fill_.min().item", "torch.cat.new().fill_.max().item", "torch.cat.new().fill_", "torch.cat.new", "torch.cat.new", "modeling_encoder_decoder.PreTrainedEncoderDecoder.enforce_repetition_penalty_", "modeling_encoder_decoder.calc_banned_ngram_tokens", "range", "modeling_encoder_decoder.calc_banned_bad_words_ids", "range", "modeling_encoder_decoder.top_k_top_p_filtering", "torch.nn.functional.softmax", "torch.multinomial().squeeze", "torch.argmax", "torch.cat.new().fill_.mul().bool", "torch.cat.new().fill_.masked_fill_", "torch.cat.new().fill_.mul_", "torch.cat.new().fill_.max", "torch.cat", "float", "tokens_to_add.unsqueeze", "torch.cat.new().fill_.min", "torch.cat.new().fill_.max", "torch.cat.new", "float", "float", "torch.multinomial", "torch.cat.new().fill_.mul", "torch.cat.new_ones", "torch.cat.new().fill_.max().item", "eos_in_sents.long", "torch.cat.new().fill_.max"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.MeetingNet_Transformer.MeetingNet_Transformer.prepare_inputs_for_generation", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.modeling_encoder_decoder.PreTrainedEncoderDecoder._do_output_past", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.MeetingNet_Transformer.MeetingNet_Transformer.enforce_repetition_penalty_", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.modeling_encoder_decoder.calc_banned_ngram_tokens", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.modeling_encoder_decoder.calc_banned_bad_words_ids", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.modeling_encoder_decoder.top_k_top_p_filtering"], ["", "def", "_generate_no_beam_search", "(", "\n", "self", ",", "\n", "input_ids", ",", "\n", "cur_len", ",", "\n", "max_length", ",", "\n", "min_length", ",", "\n", "do_sample", ",", "\n", "temperature", ",", "\n", "top_k", ",", "\n", "top_p", ",", "\n", "repetition_penalty", ",", "\n", "no_repeat_ngram_size", ",", "\n", "bad_words_ids", ",", "\n", "bos_token_id", ",", "\n", "pad_token_id", ",", "\n", "eos_token_id", ",", "\n", "decoder_start_token_id", ",", "\n", "batch_size", ",", "\n", "encoder_outputs", ",", "\n", "attention_mask", ",", "\n", ")", ":", "\n", "        ", "\"\"\"Generate sequences for each example without beam search (num_beams == 1).\n        All returned sequence are generated independantly.\n        \"\"\"", "\n", "# length of generated sentences / unfinished sentences", "\n", "unfinished_sents", "=", "input_ids", ".", "new", "(", "batch_size", ")", ".", "fill_", "(", "1", ")", "\n", "sent_lengths", "=", "input_ids", ".", "new", "(", "batch_size", ")", ".", "fill_", "(", "max_length", ")", "\n", "\n", "past", "=", "encoder_outputs", "# defined for encoder-decoder models, None for decoder-only models", "\n", "\n", "while", "cur_len", "<", "max_length", ":", "\n", "            ", "model_inputs", "=", "self", ".", "prepare_inputs_for_generation", "(", "\n", "input_ids", ",", "past", "=", "past", ",", "attention_mask", "=", "attention_mask", "\n", ")", "\n", "\n", "outputs", "=", "self", "(", "**", "model_inputs", ")", "\n", "next_token_logits", "=", "outputs", "[", "0", "]", "[", ":", ",", "-", "1", ",", ":", "]", "\n", "\n", "# if model has past, then set the past variable to speed up decoding", "\n", "if", "self", ".", "_do_output_past", "(", "outputs", ")", ":", "\n", "                ", "past", "=", "outputs", "[", "1", "]", "\n", "\n", "# repetition penalty from CTRL paper (https://arxiv.org/abs/1909.05858)", "\n", "", "if", "repetition_penalty", "!=", "1.0", ":", "\n", "                ", "self", ".", "enforce_repetition_penalty_", "(", "\n", "next_token_logits", ",", "batch_size", ",", "1", ",", "input_ids", ",", "repetition_penalty", "\n", ")", "\n", "\n", "", "if", "no_repeat_ngram_size", ">", "0", ":", "\n", "# calculate a list of banned tokens to prevent repetitively generating the same ngrams", "\n", "# from fairseq: https://github.com/pytorch/fairseq/blob/a07cb6f40480928c9e0548b737aadd36ee66ac76/fairseq/sequence_generator.py#L345", "\n", "                ", "banned_tokens", "=", "calc_banned_ngram_tokens", "(", "\n", "input_ids", ",", "batch_size", ",", "no_repeat_ngram_size", ",", "cur_len", "\n", ")", "\n", "for", "batch_idx", "in", "range", "(", "batch_size", ")", ":", "\n", "                    ", "next_token_logits", "[", "batch_idx", ",", "banned_tokens", "[", "batch_idx", "]", "]", "=", "-", "float", "(", "\n", "\"inf\"", "\n", ")", "\n", "\n", "", "", "if", "bad_words_ids", "is", "not", "None", ":", "\n", "# calculate a list of banned tokens according to bad words", "\n", "                ", "banned_tokens", "=", "calc_banned_bad_words_ids", "(", "input_ids", ",", "bad_words_ids", ")", "\n", "\n", "for", "batch_idx", "in", "range", "(", "batch_size", ")", ":", "\n", "                    ", "next_token_logits", "[", "batch_idx", ",", "banned_tokens", "[", "batch_idx", "]", "]", "=", "-", "float", "(", "\n", "\"inf\"", "\n", ")", "\n", "\n", "# set eos token prob to zero if min_length is not reached", "\n", "", "", "if", "eos_token_id", "is", "not", "None", "and", "cur_len", "<", "min_length", ":", "\n", "                ", "next_token_logits", "[", ":", ",", "eos_token_id", "]", "=", "-", "float", "(", "\"inf\"", ")", "\n", "\n", "", "if", "do_sample", ":", "\n", "# Temperature (higher temperature => more likely to sample low probability tokens)", "\n", "                ", "if", "temperature", "!=", "1.0", ":", "\n", "                    ", "next_token_logits", "=", "next_token_logits", "/", "temperature", "\n", "# Top-p/top-k filtering", "\n", "", "next_token_logits", "=", "top_k_top_p_filtering", "(", "\n", "next_token_logits", ",", "top_k", "=", "top_k", ",", "top_p", "=", "top_p", "\n", ")", "\n", "# Sample", "\n", "probs", "=", "F", ".", "softmax", "(", "next_token_logits", ",", "dim", "=", "-", "1", ")", "\n", "next_token", "=", "torch", ".", "multinomial", "(", "probs", ",", "num_samples", "=", "1", ")", ".", "squeeze", "(", "1", ")", "\n", "", "else", ":", "\n", "# Greedy decoding", "\n", "                ", "next_token", "=", "torch", ".", "argmax", "(", "next_token_logits", ",", "dim", "=", "-", "1", ")", "\n", "\n", "# update generations and finished sentences", "\n", "", "if", "eos_token_id", "is", "not", "None", ":", "\n", "# pad finished sentences if eos_token_id exist", "\n", "                ", "tokens_to_add", "=", "next_token", "*", "unfinished_sents", "+", "(", "pad_token_id", ")", "*", "(", "\n", "1", "-", "unfinished_sents", "\n", ")", "\n", "", "else", ":", "\n", "                ", "tokens_to_add", "=", "next_token", "\n", "\n", "", "input_ids", "=", "torch", ".", "cat", "(", "[", "input_ids", ",", "tokens_to_add", ".", "unsqueeze", "(", "-", "1", ")", "]", ",", "dim", "=", "-", "1", ")", "\n", "\n", "if", "eos_token_id", "is", "not", "None", ":", "\n", "                ", "eos_in_sents", "=", "tokens_to_add", "==", "eos_token_id", "\n", "# if sentence is unfinished and the token to add is eos, sent_lengths is filled with current length", "\n", "is_sents_unfinished_and_token_to_add_is_eos", "=", "unfinished_sents", ".", "mul", "(", "\n", "eos_in_sents", ".", "long", "(", ")", "\n", ")", ".", "bool", "(", ")", "\n", "sent_lengths", ".", "masked_fill_", "(", "\n", "is_sents_unfinished_and_token_to_add_is_eos", ",", "cur_len", "+", "1", "\n", ")", "\n", "# unfinished_sents is set to zero if eos in sentence", "\n", "unfinished_sents", ".", "mul_", "(", "(", "~", "eos_in_sents", ")", ".", "long", "(", ")", ")", "\n", "\n", "# stop when there is a </s> in each sentence, or if we exceed the maximul length", "\n", "", "if", "unfinished_sents", ".", "max", "(", ")", "==", "0", ":", "\n", "                ", "break", "\n", "\n", "# extend attention_mask for new generated input if only decoder", "\n", "", "if", "self", ".", "config", ".", "is_encoder_decoder", "is", "False", ":", "\n", "                ", "attention_mask", "=", "torch", ".", "cat", "(", "\n", "[", "\n", "attention_mask", ",", "\n", "attention_mask", ".", "new_ones", "(", "(", "attention_mask", ".", "shape", "[", "0", "]", ",", "1", ")", ")", ",", "\n", "]", ",", "\n", "dim", "=", "-", "1", ",", "\n", ")", "\n", "\n", "", "cur_len", "=", "cur_len", "+", "1", "\n", "\n", "# if there are different sentences lengths in the batch, some batches have to be padded", "\n", "", "if", "sent_lengths", ".", "min", "(", ")", ".", "item", "(", ")", "!=", "sent_lengths", ".", "max", "(", ")", ".", "item", "(", ")", ":", "\n", "            ", "assert", "(", "\n", "pad_token_id", "is", "not", "None", "\n", ")", ",", "\"`Pad_token_id` has to be defined if batches have different lengths\"", "\n", "# finished sents are filled with pad_token", "\n", "decoded", "=", "input_ids", ".", "new", "(", "batch_size", ",", "sent_lengths", ".", "max", "(", ")", ".", "item", "(", ")", ")", ".", "fill_", "(", "\n", "pad_token_id", "\n", ")", "\n", "", "else", ":", "\n", "            ", "decoded", "=", "input_ids", "\n", "\n", "", "for", "hypo_idx", ",", "hypo", "in", "enumerate", "(", "input_ids", ")", ":", "\n", "            ", "decoded", "[", "hypo_idx", ",", ":", "sent_lengths", "[", "hypo_idx", "]", "]", "=", "hypo", "[", ":", "sent_lengths", "[", "hypo_idx", "]", "]", "\n", "\n", "", "return", "decoded", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.modeling_encoder_decoder.PreTrainedEncoderDecoder._generate_beam_search": [[901, 1224], ["torch.zeros", "beam_scores.new.new.view", "range", "torch.cat.new", "enumerate", "modeling_encoder_decoder.BeamHypotheses", "modeling_encoder_decoder.PreTrainedEncoderDecoder.prepare_inputs_for_generation", "modeling_encoder_decoder.PreTrainedEncoderDecoder.", "modeling_encoder_decoder.PreTrainedEncoderDecoder._do_output_past", "torch.nn.functional.log_softmax", "range", "all", "beam_scores.new.new.new", "torch.cat.new", "torch.cat.new", "torch.cat", "range", "sorted", "range", "torch.cat.new.min().item", "torch.cat.new.max().item", "min", "torch.cat.new().fill_", "enumerate", "torch.stack().type().to", "range", "range", "modeling_encoder_decoder.PreTrainedEncoderDecoder.enforce_repetition_penalty_", "modeling_encoder_decoder.PreTrainedEncoderDecoder.prepare_scores_for_generation", "modeling_encoder_decoder.calc_banned_ngram_tokens", "enumerate", "modeling_encoder_decoder.calc_banned_bad_words_ids", "enumerate", "modeling_encoder_decoder.top_k_top_p_filtering", "_scores.contiguous().view.contiguous().view.contiguous().view", "torch.nn.functional.softmax", "torch.multinomial", "torch.gather", "torch.sort", "torch.gather", "next_scores.view.view.view", "torch.topk", "next_scores.view.view.size", "torch.gather.size", "enumerate", "next_batch_beam.extend", "len", "modeling_encoder_decoder.PreTrainedEncoderDecoder._reorder_cache", "torch.cat", "all", "torch.all", "beam_scores[].item", "generated_hyps[].add", "len", "best.append", "float", "beam_scores[].expand_as", "beam_scores[].expand_as", "next_batch_beam.extend", "zip", "generated_hyps[].is_done", "len", "len", "torch.cat.new.unsqueeze", "sorted.pop", "torch.cat.new.min", "torch.cat.new.max", "torch.cat.new.max().item", "torch.cat.new", "len", "torch.stack().type", "next", "float", "float", "_scores.contiguous().view.contiguous().view.contiguous", "len", "generated_hyps[].add", "next_sent_beam.append", "len", "next_scores[].max().item", "torch.cat.new_ones", "beam_scores.new.new.view", "modeling_encoder_decoder.PreTrainedEncoderDecoder.parameters", "token_id.item", "input_ids[].clone", "beam_token_score.item", "beam_scores.new.new.view", "torch.cat.new.max", "torch.stack", "next_scores[].max"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.MeetingNet_Transformer.MeetingNet_Transformer.prepare_inputs_for_generation", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.modeling_encoder_decoder.PreTrainedEncoderDecoder._do_output_past", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.MeetingNet_Transformer.MeetingNet_Transformer.enforce_repetition_penalty_", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.MeetingNet_Transformer.MeetingNet_Transformer.prepare_scores_for_generation", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.modeling_encoder_decoder.calc_banned_ngram_tokens", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.modeling_encoder_decoder.calc_banned_bad_words_ids", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.modeling_encoder_decoder.top_k_top_p_filtering", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.Transformer.TransformerBeam.topk", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.MeetingNet_Transformer.MeetingNet_Transformer._reorder_cache", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.modeling_encoder_decoder.BeamHypotheses.add", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.modeling_encoder_decoder.BeamHypotheses.is_done", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.modeling_encoder_decoder.BeamHypotheses.add"], ["", "def", "_generate_beam_search", "(", "\n", "self", ",", "\n", "input_ids", ",", "\n", "cur_len", ",", "\n", "max_length", ",", "\n", "min_length", ",", "\n", "do_sample", ",", "\n", "early_stopping", ",", "\n", "temperature", ",", "\n", "top_k", ",", "\n", "top_p", ",", "\n", "repetition_penalty", ",", "\n", "no_repeat_ngram_size", ",", "\n", "bad_words_ids", ",", "\n", "bos_token_id", ",", "\n", "pad_token_id", ",", "\n", "eos_token_id", ",", "\n", "decoder_start_token_id", ",", "\n", "batch_size", ",", "\n", "num_return_sequences", ",", "\n", "length_penalty", ",", "\n", "num_beams", ",", "\n", "vocab_size", ",", "\n", "encoder_outputs", ",", "\n", "attention_mask", ",", "\n", ")", ":", "\n", "        ", "\"\"\"Generate sequences for each example with beam search.\"\"\"", "\n", "\n", "# generated hypotheses", "\n", "generated_hyps", "=", "[", "\n", "BeamHypotheses", "(", "\n", "num_beams", ",", "max_length", ",", "length_penalty", ",", "early_stopping", "=", "early_stopping", "\n", ")", "\n", "for", "_", "in", "range", "(", "batch_size", ")", "\n", "]", "\n", "\n", "# scores for each sentence in the beam", "\n", "beam_scores", "=", "torch", ".", "zeros", "(", "\n", "(", "batch_size", ",", "num_beams", ")", ",", "dtype", "=", "torch", ".", "float", ",", "device", "=", "input_ids", ".", "device", "\n", ")", "\n", "\n", "# for greedy decoding it is made sure that only tokens of the first beam are considered to avoid sampling the exact same tokens three times", "\n", "if", "do_sample", "is", "False", ":", "\n", "            ", "beam_scores", "[", ":", ",", "1", ":", "]", "=", "-", "1e9", "\n", "", "beam_scores", "=", "beam_scores", ".", "view", "(", "-", "1", ")", "# shape (batch_size * num_beams,)", "\n", "\n", "# cache compute states", "\n", "past", "=", "encoder_outputs", "# defined for encoder-decoder models, None for decoder-only models", "\n", "\n", "# done sentences", "\n", "done", "=", "[", "False", "for", "_", "in", "range", "(", "batch_size", ")", "]", "\n", "\n", "while", "cur_len", "<", "max_length", ":", "\n", "            ", "model_inputs", "=", "self", ".", "prepare_inputs_for_generation", "(", "\n", "input_ids", ",", "past", "=", "past", ",", "attention_mask", "=", "attention_mask", "\n", ")", "\n", "outputs", "=", "self", "(", "\n", "**", "model_inputs", "\n", ")", "# (batch_size * num_beams, cur_len, vocab_size)", "\n", "next_token_logits", "=", "outputs", "[", "0", "]", "[", "\n", ":", ",", "-", "1", ",", ":", "\n", "]", "# (batch_size * num_beams, vocab_size)", "\n", "\n", "# if model has past, then set the past variable to speed up decoding", "\n", "if", "self", ".", "_do_output_past", "(", "outputs", ")", ":", "\n", "                ", "past", "=", "outputs", "[", "1", "]", "\n", "\n", "# repetition penalty (from CTRL paper https://arxiv.org/abs/1909.05858)", "\n", "", "if", "repetition_penalty", "!=", "1.0", ":", "\n", "                ", "self", ".", "enforce_repetition_penalty_", "(", "\n", "next_token_logits", ",", "\n", "batch_size", ",", "\n", "num_beams", ",", "\n", "input_ids", ",", "\n", "repetition_penalty", ",", "\n", ")", "\n", "\n", "", "if", "temperature", "!=", "1.0", ":", "\n", "                ", "next_token_logits", "=", "next_token_logits", "/", "temperature", "\n", "\n", "", "scores", "=", "F", ".", "log_softmax", "(", "\n", "next_token_logits", ",", "dim", "=", "-", "1", "\n", ")", "# (batch_size * num_beams, vocab_size)", "\n", "if", "self", ".", "config", ".", "is_encoder_decoder", "and", "do_sample", "is", "False", ":", "\n", "# TODO (PVP) still a bit hacky here - there might be a better solutino", "\n", "                ", "scores", "=", "self", ".", "prepare_scores_for_generation", "(", "\n", "scores", ",", "cur_len", "=", "cur_len", ",", "max_length", "=", "max_length", "\n", ")", "\n", "\n", "# set eos token prob to zero if min_length is not reached", "\n", "", "if", "eos_token_id", "is", "not", "None", "and", "cur_len", "<", "min_length", ":", "\n", "                ", "scores", "[", ":", ",", "eos_token_id", "]", "=", "-", "float", "(", "\"inf\"", ")", "\n", "\n", "", "if", "no_repeat_ngram_size", ">", "0", ":", "\n", "# calculate a list of banned tokens to prevent repetitively generating the same ngrams", "\n", "                ", "num_batch_hypotheses", "=", "batch_size", "*", "num_beams", "\n", "# from fairseq: https://github.com/pytorch/fairseq/blob/a07cb6f40480928c9e0548b737aadd36ee66ac76/fairseq/sequence_generator.py#L345", "\n", "banned_batch_tokens", "=", "calc_banned_ngram_tokens", "(", "\n", "input_ids", ",", "num_batch_hypotheses", ",", "no_repeat_ngram_size", ",", "cur_len", "\n", ")", "\n", "for", "i", ",", "banned_tokens", "in", "enumerate", "(", "banned_batch_tokens", ")", ":", "\n", "                    ", "scores", "[", "i", ",", "banned_tokens", "]", "=", "-", "float", "(", "\"inf\"", ")", "\n", "\n", "", "", "if", "bad_words_ids", "is", "not", "None", ":", "\n", "# calculate a list of banned tokens according to bad words", "\n", "                ", "banned_tokens", "=", "calc_banned_bad_words_ids", "(", "input_ids", ",", "bad_words_ids", ")", "\n", "\n", "for", "i", ",", "banned_tokens", "in", "enumerate", "(", "banned_tokens", ")", ":", "\n", "                    ", "scores", "[", "i", ",", "banned_tokens", "]", "=", "-", "float", "(", "\"inf\"", ")", "\n", "\n", "", "", "assert", "scores", ".", "shape", "==", "(", "\n", "batch_size", "*", "num_beams", ",", "\n", "vocab_size", ",", "\n", ")", ",", "\"Shapes of scores: {} != {}\"", ".", "format", "(", "\n", "scores", ".", "shape", ",", "(", "batch_size", "*", "num_beams", ",", "vocab_size", ")", "\n", ")", "\n", "\n", "if", "do_sample", ":", "\n", "                ", "_scores", "=", "scores", "+", "beam_scores", "[", ":", ",", "None", "]", ".", "expand_as", "(", "\n", "scores", "\n", ")", "# (batch_size * num_beams, vocab_size)", "\n", "# Top-p/top-k filtering", "\n", "_scores", "=", "top_k_top_p_filtering", "(", "\n", "_scores", ",", "top_k", "=", "top_k", ",", "top_p", "=", "top_p", ",", "min_tokens_to_keep", "=", "2", "\n", ")", "# (batch_size * num_beams, vocab_size)", "\n", "# re-organize to group the beam together to sample from all beam_idxs", "\n", "_scores", "=", "_scores", ".", "contiguous", "(", ")", ".", "view", "(", "\n", "batch_size", ",", "num_beams", "*", "vocab_size", "\n", ")", "# (batch_size, num_beams * vocab_size)", "\n", "\n", "# Sample 2 next tokens for each beam (so we have some spare tokens and match output of greedy beam search)", "\n", "probs", "=", "F", ".", "softmax", "(", "_scores", ",", "dim", "=", "-", "1", ")", "\n", "next_tokens", "=", "torch", ".", "multinomial", "(", "\n", "probs", ",", "num_samples", "=", "2", "*", "num_beams", "\n", ")", "# (batch_size, num_beams * 2)", "\n", "# Compute next scores", "\n", "next_scores", "=", "torch", ".", "gather", "(", "\n", "_scores", ",", "-", "1", ",", "next_tokens", "\n", ")", "# (batch_size, num_beams * 2)", "\n", "# sort the sampled vector to make sure that the first num_beams samples are the best", "\n", "next_scores", ",", "next_scores_indices", "=", "torch", ".", "sort", "(", "\n", "next_scores", ",", "descending", "=", "True", ",", "dim", "=", "1", "\n", ")", "\n", "next_tokens", "=", "torch", ".", "gather", "(", "\n", "next_tokens", ",", "-", "1", ",", "next_scores_indices", "\n", ")", "# (batch_size, num_beams * 2)", "\n", "\n", "", "else", ":", "\n", "                ", "next_scores", "=", "scores", "+", "beam_scores", "[", ":", ",", "None", "]", ".", "expand_as", "(", "\n", "scores", "\n", ")", "# (batch_size * num_beams, vocab_size)", "\n", "\n", "# re-organize to group the beam together (we are keeping top hypothesis accross beams)", "\n", "next_scores", "=", "next_scores", ".", "view", "(", "\n", "batch_size", ",", "num_beams", "*", "vocab_size", "\n", ")", "# (batch_size, num_beams * vocab_size)", "\n", "\n", "next_scores", ",", "next_tokens", "=", "torch", ".", "topk", "(", "\n", "next_scores", ",", "2", "*", "num_beams", ",", "dim", "=", "1", ",", "largest", "=", "True", ",", "sorted", "=", "True", "\n", ")", "\n", "\n", "", "assert", "(", "\n", "next_scores", ".", "size", "(", ")", "==", "next_tokens", ".", "size", "(", ")", "==", "(", "batch_size", ",", "2", "*", "num_beams", ")", "\n", ")", "\n", "\n", "# next batch beam content", "\n", "next_batch_beam", "=", "[", "]", "\n", "\n", "# for each sentence", "\n", "for", "batch_idx", "in", "range", "(", "batch_size", ")", ":", "\n", "\n", "# if we are done with this sentence", "\n", "                ", "if", "done", "[", "batch_idx", "]", ":", "\n", "                    ", "assert", "(", "\n", "len", "(", "generated_hyps", "[", "batch_idx", "]", ")", ">=", "num_beams", "\n", ")", ",", "\"Batch can only be done if at least {} beams have been generated\"", ".", "format", "(", "\n", "num_beams", "\n", ")", "\n", "assert", "(", "\n", "eos_token_id", "is", "not", "None", "and", "pad_token_id", "is", "not", "None", "\n", ")", ",", "\"generated beams >= num_beams -> eos_token_id and pad_token have to be defined\"", "\n", "next_batch_beam", ".", "extend", "(", "\n", "[", "(", "0", ",", "pad_token_id", ",", "0", ")", "]", "*", "num_beams", "\n", ")", "# pad the batch", "\n", "continue", "\n", "\n", "# next sentence beam content", "\n", "", "next_sent_beam", "=", "[", "]", "\n", "\n", "# next tokens for this sentence", "\n", "for", "beam_token_rank", ",", "(", "beam_token_id", ",", "beam_token_score", ")", "in", "enumerate", "(", "\n", "zip", "(", "next_tokens", "[", "batch_idx", "]", ",", "next_scores", "[", "batch_idx", "]", ")", "\n", ")", ":", "\n", "# get beam and token IDs", "\n", "                    ", "beam_id", "=", "beam_token_id", "//", "vocab_size", "\n", "token_id", "=", "beam_token_id", "%", "vocab_size", "\n", "\n", "effective_beam_id", "=", "batch_idx", "*", "num_beams", "+", "beam_id", "\n", "# add to generated hypotheses if end of sentence or last iteration", "\n", "if", "(", "eos_token_id", "is", "not", "None", ")", "and", "(", "token_id", ".", "item", "(", ")", "==", "eos_token_id", ")", ":", "\n", "# if beam_token does not belong to top num_beams tokens, it should not be added", "\n", "                        ", "is_beam_token_worse_than_top_num_beams", "=", "(", "\n", "beam_token_rank", ">=", "num_beams", "\n", ")", "\n", "if", "is_beam_token_worse_than_top_num_beams", ":", "\n", "                            ", "continue", "\n", "", "generated_hyps", "[", "batch_idx", "]", ".", "add", "(", "\n", "input_ids", "[", "effective_beam_id", "]", ".", "clone", "(", ")", ",", "\n", "beam_token_score", ".", "item", "(", ")", ",", "\n", ")", "\n", "", "else", ":", "\n", "# add next predicted token if it is not eos_token", "\n", "                        ", "next_sent_beam", ".", "append", "(", "\n", "(", "beam_token_score", ",", "token_id", ",", "effective_beam_id", ")", "\n", ")", "\n", "\n", "# the beam for next step is full", "\n", "", "if", "len", "(", "next_sent_beam", ")", "==", "num_beams", ":", "\n", "                        ", "break", "\n", "\n", "# Check if were done so that we can save a pad step if all(done)", "\n", "", "", "done", "[", "batch_idx", "]", "=", "done", "[", "batch_idx", "]", "or", "generated_hyps", "[", "batch_idx", "]", ".", "is_done", "(", "\n", "next_scores", "[", "batch_idx", "]", ".", "max", "(", ")", ".", "item", "(", ")", ",", "cur_len", "=", "cur_len", "\n", ")", "\n", "\n", "# update next beam content", "\n", "assert", "len", "(", "next_sent_beam", ")", "==", "num_beams", ",", "\"Beam should always be full\"", "\n", "next_batch_beam", ".", "extend", "(", "next_sent_beam", ")", "\n", "assert", "len", "(", "next_batch_beam", ")", "==", "num_beams", "*", "(", "batch_idx", "+", "1", ")", "\n", "\n", "# stop when we are done with each sentence", "\n", "", "if", "all", "(", "done", ")", ":", "\n", "                ", "break", "\n", "\n", "# sanity check / prepare next batch", "\n", "", "assert", "len", "(", "next_batch_beam", ")", "==", "batch_size", "*", "num_beams", "\n", "beam_scores", "=", "beam_scores", ".", "new", "(", "[", "x", "[", "0", "]", "for", "x", "in", "next_batch_beam", "]", ")", "\n", "beam_tokens", "=", "input_ids", ".", "new", "(", "[", "x", "[", "1", "]", "for", "x", "in", "next_batch_beam", "]", ")", "\n", "beam_idx", "=", "input_ids", ".", "new", "(", "[", "x", "[", "2", "]", "for", "x", "in", "next_batch_beam", "]", ")", "\n", "\n", "# re-order batch", "\n", "input_ids", "=", "input_ids", "[", "beam_idx", ",", ":", "]", "\n", "input_ids", "=", "torch", ".", "cat", "(", "[", "input_ids", ",", "beam_tokens", ".", "unsqueeze", "(", "1", ")", "]", ",", "dim", "=", "-", "1", ")", "\n", "# re-order internal states", "\n", "if", "past", "is", "not", "None", ":", "\n", "                ", "past", "=", "self", ".", "_reorder_cache", "(", "past", ",", "beam_idx", ")", "\n", "\n", "# extend attention_mask for new generated input if only decoder", "\n", "", "if", "self", ".", "config", ".", "is_encoder_decoder", "is", "False", ":", "\n", "                ", "attention_mask", "=", "torch", ".", "cat", "(", "\n", "[", "\n", "attention_mask", ",", "\n", "attention_mask", ".", "new_ones", "(", "(", "attention_mask", ".", "shape", "[", "0", "]", ",", "1", ")", ")", ",", "\n", "]", ",", "\n", "dim", "=", "-", "1", ",", "\n", ")", "\n", "\n", "# update current length", "\n", "", "cur_len", "=", "cur_len", "+", "1", "\n", "\n", "# finalize all open beam hypotheses and end to generated hypotheses", "\n", "", "for", "batch_idx", "in", "range", "(", "batch_size", ")", ":", "\n", "            ", "if", "done", "[", "batch_idx", "]", ":", "\n", "                ", "continue", "\n", "\n", "# test that beam scores match previously calculated scores if not eos and batch_idx not done", "\n", "", "if", "eos_token_id", "is", "not", "None", "and", "all", "(", "\n", "(", "token_id", "%", "vocab_size", ")", ".", "item", "(", ")", "is", "not", "eos_token_id", "\n", "for", "token_id", "in", "next_tokens", "[", "batch_idx", "]", "\n", ")", ":", "\n", "                ", "assert", "torch", ".", "all", "(", "\n", "next_scores", "[", "batch_idx", ",", ":", "num_beams", "]", "\n", "==", "beam_scores", ".", "view", "(", "batch_size", ",", "num_beams", ")", "[", "batch_idx", "]", "\n", ")", ",", "\"If batch_idx is not done, final next scores: {} have to equal to accumulated beam_scores: {}\"", ".", "format", "(", "\n", "next_scores", "[", ":", ",", ":", "num_beams", "]", "[", "batch_idx", "]", ",", "\n", "beam_scores", ".", "view", "(", "batch_size", ",", "num_beams", ")", "[", "batch_idx", "]", ",", "\n", ")", "\n", "\n", "# need to add best num_beams hypotheses to generated hyps", "\n", "", "for", "beam_id", "in", "range", "(", "num_beams", ")", ":", "\n", "                ", "effective_beam_id", "=", "batch_idx", "*", "num_beams", "+", "beam_id", "\n", "final_score", "=", "beam_scores", "[", "effective_beam_id", "]", ".", "item", "(", ")", "\n", "final_tokens", "=", "input_ids", "[", "effective_beam_id", "]", "\n", "generated_hyps", "[", "batch_idx", "]", ".", "add", "(", "final_tokens", ",", "final_score", ")", "\n", "\n", "# depending on whether greedy generation is wanted or not define different output_batch_size and output_num_return_sequences_per_batch", "\n", "", "", "output_batch_size", "=", "(", "\n", "batch_size", "if", "do_sample", "else", "batch_size", "*", "num_return_sequences", "\n", ")", "\n", "output_num_return_sequences_per_batch", "=", "1", "if", "do_sample", "else", "num_return_sequences", "\n", "\n", "# select the best hypotheses", "\n", "sent_lengths", "=", "input_ids", ".", "new", "(", "output_batch_size", ")", "\n", "best", "=", "[", "]", "\n", "\n", "# retrieve best hypotheses", "\n", "for", "i", ",", "hypotheses", "in", "enumerate", "(", "generated_hyps", ")", ":", "\n", "            ", "sorted_hyps", "=", "sorted", "(", "hypotheses", ".", "beams", ",", "key", "=", "lambda", "x", ":", "x", "[", "0", "]", ")", "\n", "for", "j", "in", "range", "(", "output_num_return_sequences_per_batch", ")", ":", "\n", "                ", "effective_batch_idx", "=", "output_num_return_sequences_per_batch", "*", "i", "+", "j", "\n", "best_hyp", "=", "sorted_hyps", ".", "pop", "(", ")", "[", "1", "]", "\n", "sent_lengths", "[", "effective_batch_idx", "]", "=", "len", "(", "best_hyp", ")", "\n", "best", ".", "append", "(", "best_hyp", ")", "\n", "\n", "# shorter batches are filled with pad_token", "\n", "", "", "if", "sent_lengths", ".", "min", "(", ")", ".", "item", "(", ")", "!=", "sent_lengths", ".", "max", "(", ")", ".", "item", "(", ")", ":", "\n", "            ", "assert", "pad_token_id", "is", "not", "None", ",", "\"`Pad_token_id` has to be defined\"", "\n", "sent_max_len", "=", "min", "(", "sent_lengths", ".", "max", "(", ")", ".", "item", "(", ")", "+", "1", ",", "max_length", ")", "\n", "decoded", "=", "input_ids", ".", "new", "(", "output_batch_size", ",", "sent_max_len", ")", ".", "fill_", "(", "pad_token_id", ")", "\n", "\n", "# fill with hypothesis and eos_token_id if necessary", "\n", "for", "i", ",", "hypo", "in", "enumerate", "(", "best", ")", ":", "\n", "                ", "decoded", "[", "i", ",", ":", "sent_lengths", "[", "i", "]", "]", "=", "hypo", "\n", "if", "sent_lengths", "[", "i", "]", "<", "max_length", ":", "\n", "                    ", "decoded", "[", "i", ",", "sent_lengths", "[", "i", "]", "]", "=", "eos_token_id", "\n", "", "", "", "else", ":", "\n", "# none of the hypotheses have an eos_token", "\n", "            ", "assert", "(", "len", "(", "hypo", ")", "==", "max_length", "for", "hypo", "in", "best", ")", "\n", "decoded", "=", "(", "\n", "torch", ".", "stack", "(", "best", ")", ".", "type", "(", "torch", ".", "long", ")", ".", "to", "(", "next", "(", "self", ".", "parameters", "(", ")", ")", ".", "device", ")", "\n", ")", "\n", "\n", "", "return", "decoded", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.modeling_encoder_decoder.PreTrainedEncoderDecoder._force_token_ids_generation": [[1226, 1238], ["isinstance", "torch.tensor", "len", "float", "range", "next", "modeling_encoder_decoder.PreTrainedEncoderDecoder.parameters"], "methods", ["None"], ["", "def", "_force_token_ids_generation", "(", "self", ",", "scores", ",", "token_ids", ")", ":", "\n", "        ", "if", "isinstance", "(", "token_ids", ",", "int", ")", ":", "\n", "            ", "token_ids", "=", "[", "token_ids", "]", "\n", "", "all_but_token_ids_mask", "=", "torch", ".", "tensor", "(", "\n", "[", "x", "for", "x", "in", "range", "(", "self", ".", "config", ".", "vocab_size", ")", "if", "x", "not", "in", "token_ids", "]", ",", "\n", "dtype", "=", "torch", ".", "long", ",", "\n", "device", "=", "next", "(", "self", ".", "parameters", "(", ")", ")", ".", "device", ",", "\n", ")", "\n", "assert", "(", "\n", "len", "(", "scores", ".", "shape", ")", "==", "2", "\n", ")", ",", "\"scores should be of rank 2 with shape: [batch_size, vocab_size]\"", "\n", "scores", "[", ":", ",", "all_but_token_ids_mask", "]", "=", "-", "float", "(", "\"inf\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.modeling_encoder_decoder.PreTrainedEncoderDecoder._reorder_cache": [[1239, 1254], ["tuple", "torch.cat", "reordered_past.append", "layer_past[].unsqueeze().clone().detach", "layer_past[].unsqueeze().clone", "layer_past[].unsqueeze"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "_reorder_cache", "(", "past", ",", "beam_idx", ")", ":", "\n", "        ", "reordered_past", "=", "[", "]", "\n", "for", "layer_past", "in", "past", ":", "\n", "# get the correct batch idx from layer past batch dim", "\n", "# batch dim of `past` and `mems` is at 2nd position", "\n", "            ", "reordered_layer_past", "=", "[", "\n", "layer_past", "[", ":", ",", "i", "]", ".", "unsqueeze", "(", "1", ")", ".", "clone", "(", ")", ".", "detach", "(", ")", "for", "i", "in", "beam_idx", "\n", "]", "\n", "reordered_layer_past", "=", "torch", ".", "cat", "(", "reordered_layer_past", ",", "dim", "=", "1", ")", "\n", "# check that shape matches", "\n", "assert", "reordered_layer_past", ".", "shape", "==", "layer_past", ".", "shape", "\n", "reordered_past", ".", "append", "(", "reordered_layer_past", ")", "\n", "", "past", "=", "tuple", "(", "reordered_past", ")", "\n", "return", "past", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.modeling_encoder_decoder.BeamHypotheses.__init__": [[1362, 1372], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "num_beams", ",", "max_length", ",", "length_penalty", ",", "early_stopping", ")", ":", "\n", "        ", "\"\"\"\n        Initialize n-best list of hypotheses.\n        \"\"\"", "\n", "self", ".", "max_length", "=", "max_length", "-", "1", "# ignoring bos_token", "\n", "self", ".", "length_penalty", "=", "length_penalty", "\n", "self", ".", "early_stopping", "=", "early_stopping", "\n", "self", ".", "num_beams", "=", "num_beams", "\n", "self", ".", "beams", "=", "[", "]", "\n", "self", ".", "worst_score", "=", "1e9", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.modeling_encoder_decoder.BeamHypotheses.__len__": [[1373, 1378], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Number of hypotheses in the list.\n        \"\"\"", "\n", "return", "len", "(", "self", ".", "beams", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.modeling_encoder_decoder.BeamHypotheses.add": [[1379, 1394], ["modeling_encoder_decoder.BeamHypotheses.beams.append", "len", "len", "len", "sorted", "min", "enumerate"], "methods", ["None"], ["", "def", "add", "(", "self", ",", "hyp", ",", "sum_logprobs", ")", ":", "\n", "        ", "\"\"\"\n        Add a new hypothesis to the list.\n        \"\"\"", "\n", "score", "=", "sum_logprobs", "/", "len", "(", "hyp", ")", "**", "self", ".", "length_penalty", "\n", "if", "len", "(", "self", ")", "<", "self", ".", "num_beams", "or", "score", ">", "self", ".", "worst_score", ":", "\n", "            ", "self", ".", "beams", ".", "append", "(", "(", "score", ",", "hyp", ")", ")", "\n", "if", "len", "(", "self", ")", ">", "self", ".", "num_beams", ":", "\n", "                ", "sorted_scores", "=", "sorted", "(", "\n", "[", "(", "s", ",", "idx", ")", "for", "idx", ",", "(", "s", ",", "_", ")", "in", "enumerate", "(", "self", ".", "beams", ")", "]", "\n", ")", "\n", "del", "self", ".", "beams", "[", "sorted_scores", "[", "0", "]", "[", "1", "]", "]", "\n", "self", ".", "worst_score", "=", "sorted_scores", "[", "1", "]", "[", "0", "]", "\n", "", "else", ":", "\n", "                ", "self", ".", "worst_score", "=", "min", "(", "score", ",", "self", ".", "worst_score", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.modeling_encoder_decoder.BeamHypotheses.is_done": [[1395, 1411], ["len"], "methods", ["None"], ["", "", "", "def", "is_done", "(", "self", ",", "best_sum_logprobs", ",", "cur_len", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        If there are enough hypotheses and that none of the hypotheses being generated\n        can become better than the worst one in the heap, then we are done with this sentence.\n        \"\"\"", "\n", "\n", "if", "len", "(", "self", ")", "<", "self", ".", "num_beams", ":", "\n", "            ", "return", "False", "\n", "", "elif", "self", ".", "early_stopping", ":", "\n", "            ", "return", "True", "\n", "", "else", ":", "\n", "            ", "if", "cur_len", "is", "None", ":", "\n", "                ", "cur_len", "=", "self", ".", "max_length", "\n", "", "cur_score", "=", "best_sum_logprobs", "/", "cur_len", "**", "self", ".", "length_penalty", "\n", "ret", "=", "self", ".", "worst_score", ">=", "cur_score", "\n", "return", "ret", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.modeling_encoder_decoder.calc_banned_ngram_tokens": [[1256, 1279], ["range", "prev_input_ids[].tolist", "zip", "tuple", "generated_ngrams[].get", "modeling_encoder_decoder.calc_banned_ngram_tokens._get_generated_ngrams"], "function", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.closablequeue.ClosableQueue.get"], ["", "", "def", "calc_banned_ngram_tokens", "(", "prev_input_ids", ",", "num_hypos", ",", "no_repeat_ngram_size", ",", "cur_len", ")", ":", "\n", "# Copied from fairseq for no_repeat_ngram in beam_search\"\"\"", "\n", "    ", "if", "cur_len", "+", "1", "<", "no_repeat_ngram_size", ":", "\n", "# return no banned tokens if we haven't generated no_repeat_ngram_size tokens yet", "\n", "        ", "return", "[", "[", "]", "for", "_", "in", "range", "(", "num_hypos", ")", "]", "\n", "", "generated_ngrams", "=", "[", "{", "}", "for", "_", "in", "range", "(", "num_hypos", ")", "]", "\n", "for", "idx", "in", "range", "(", "num_hypos", ")", ":", "\n", "        ", "gen_tokens", "=", "prev_input_ids", "[", "idx", "]", ".", "tolist", "(", ")", "\n", "generated_ngram", "=", "generated_ngrams", "[", "idx", "]", "\n", "for", "ngram", "in", "zip", "(", "*", "[", "gen_tokens", "[", "i", ":", "]", "for", "i", "in", "range", "(", "no_repeat_ngram_size", ")", "]", ")", ":", "\n", "            ", "prev_ngram_tuple", "=", "tuple", "(", "ngram", "[", ":", "-", "1", "]", ")", "\n", "generated_ngram", "[", "prev_ngram_tuple", "]", "=", "generated_ngram", ".", "get", "(", "\n", "prev_ngram_tuple", ",", "[", "]", "\n", ")", "+", "[", "ngram", "[", "-", "1", "]", "]", "\n", "\n", "", "", "def", "_get_generated_ngrams", "(", "hypo_idx", ")", ":", "\n", "# Before decoding the next token, prevent decoding of ngrams that have already appeared", "\n", "        ", "start_idx", "=", "cur_len", "+", "1", "-", "no_repeat_ngram_size", "\n", "ngram_idx", "=", "tuple", "(", "prev_input_ids", "[", "hypo_idx", ",", "start_idx", ":", "cur_len", "]", ".", "tolist", "(", ")", ")", "\n", "return", "generated_ngrams", "[", "hypo_idx", "]", ".", "get", "(", "ngram_idx", ",", "[", "]", ")", "\n", "\n", "", "banned_tokens", "=", "[", "_get_generated_ngrams", "(", "hypo_idx", ")", "for", "hypo_idx", "in", "range", "(", "num_hypos", ")", "]", "\n", "return", "banned_tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.modeling_encoder_decoder.calc_banned_bad_words_ids": [[1281, 1320], ["banned_tokens.append", "len", "len", "len", "banned_tokens_slice.append", "len", "modeling_encoder_decoder.calc_banned_bad_words_ids._tokens_match"], "function", ["None"], ["", "def", "calc_banned_bad_words_ids", "(", "prev_input_ids", ",", "bad_words_ids", ")", ":", "\n", "    ", "banned_tokens", "=", "[", "]", "\n", "\n", "def", "_tokens_match", "(", "prev_tokens", ",", "tokens", ")", ":", "\n", "        ", "if", "len", "(", "tokens", ")", "==", "0", ":", "\n", "# if bad word tokens is just one token always ban it", "\n", "            ", "return", "True", "\n", "", "if", "len", "(", "tokens", ")", ">", "len", "(", "prev_input_ids", ")", ":", "\n", "# if bad word tokens are longer then prev input_ids they can't be equal", "\n", "            ", "return", "False", "\n", "\n", "", "if", "prev_tokens", "[", "-", "len", "(", "tokens", ")", ":", "]", "==", "tokens", ":", "\n", "# if tokens match", "\n", "            ", "return", "True", "\n", "", "else", ":", "\n", "            ", "return", "False", "\n", "\n", "", "", "for", "prev_input_ids_slice", "in", "prev_input_ids", ":", "\n", "        ", "banned_tokens_slice", "=", "[", "]", "\n", "\n", "for", "banned_token_seq", "in", "bad_words_ids", ":", "\n", "            ", "assert", "(", "\n", "len", "(", "banned_token_seq", ")", ">", "0", "\n", ")", ",", "\"Banned words token sequences {} cannot have an empty list\"", ".", "format", "(", "\n", "bad_words_ids", "\n", ")", "\n", "\n", "if", "(", "\n", "_tokens_match", "(", "prev_input_ids_slice", ".", "tolist", "(", ")", ",", "banned_token_seq", "[", ":", "-", "1", "]", ")", "\n", "is", "False", "\n", ")", ":", "\n", "# if tokens do not match continue", "\n", "                ", "continue", "\n", "\n", "", "banned_tokens_slice", ".", "append", "(", "banned_token_seq", "[", "-", "1", "]", ")", "\n", "\n", "", "banned_tokens", ".", "append", "(", "banned_tokens_slice", ")", "\n", "\n", "", "return", "banned_tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.modeling_encoder_decoder.top_k_top_p_filtering": [[1322, 1359], ["float", "min", "torch.sort", "torch.cumsum", "sorted_indices_to_remove[].clone", "sorted_indices_to_remove.scatter", "max", "logits.size", "torch.nn.functional.softmax", "torch.topk"], "function", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.evaluation.error_viz.scatter", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.Transformer.TransformerBeam.topk"], ["", "def", "top_k_top_p_filtering", "(", "\n", "logits", ",", "top_k", "=", "0", ",", "top_p", "=", "1.0", ",", "filter_value", "=", "-", "float", "(", "\"Inf\"", ")", ",", "min_tokens_to_keep", "=", "1", "\n", ")", ":", "\n", "    ", "\"\"\"Filter a distribution of logits using top-k and/or nucleus (top-p) filtering\n    Args:\n        logits: logits distribution shape (batch size, vocabulary size)\n        if top_k > 0: keep only top k tokens with highest probability (top-k filtering).\n        if top_p < 1.0: keep the top tokens with cumulative probability >= top_p (nucleus filtering).\n            Nucleus filtering is described in Holtzman et al. (http://arxiv.org/abs/1904.09751)\n        Make sure we keep at least min_tokens_to_keep per batch example in the output\n    From: https://gist.github.com/thomwolf/1a5a29f6962089e871b94cbd09daf317\n    \"\"\"", "\n", "if", "top_k", ">", "0", ":", "\n", "        ", "top_k", "=", "min", "(", "max", "(", "top_k", ",", "min_tokens_to_keep", ")", ",", "logits", ".", "size", "(", "-", "1", ")", ")", "# Safety check", "\n", "# Remove all tokens with a probability less than the last token of the top-k", "\n", "indices_to_remove", "=", "logits", "<", "torch", ".", "topk", "(", "logits", ",", "top_k", ")", "[", "0", "]", "[", "...", ",", "-", "1", ",", "None", "]", "\n", "logits", "[", "indices_to_remove", "]", "=", "filter_value", "\n", "\n", "", "if", "top_p", "<", "1.0", ":", "\n", "        ", "sorted_logits", ",", "sorted_indices", "=", "torch", ".", "sort", "(", "logits", ",", "descending", "=", "True", ")", "\n", "cumulative_probs", "=", "torch", ".", "cumsum", "(", "F", ".", "softmax", "(", "sorted_logits", ",", "dim", "=", "-", "1", ")", ",", "dim", "=", "-", "1", ")", "\n", "\n", "# Remove tokens with cumulative probability above the threshold (token with 0 are kept)", "\n", "sorted_indices_to_remove", "=", "cumulative_probs", ">", "top_p", "\n", "if", "min_tokens_to_keep", ">", "1", ":", "\n", "# Keep at least min_tokens_to_keep (set to min_tokens_to_keep-1 because we add the first one below)", "\n", "            ", "sorted_indices_to_remove", "[", "...", ",", ":", "min_tokens_to_keep", "]", "=", "0", "\n", "# Shift the indices to the right to keep also the first token above the threshold", "\n", "", "sorted_indices_to_remove", "[", "...", ",", "1", ":", "]", "=", "sorted_indices_to_remove", "[", "...", ",", ":", "-", "1", "]", ".", "clone", "(", ")", "\n", "sorted_indices_to_remove", "[", "...", ",", "0", "]", "=", "0", "\n", "\n", "# scatter sorted tensors to original indexing", "\n", "indices_to_remove", "=", "sorted_indices_to_remove", ".", "scatter", "(", "\n", "1", ",", "sorted_indices", ",", "sorted_indices_to_remove", "\n", ")", "\n", "logits", "[", "indices_to_remove", "]", "=", "filter_value", "\n", "", "return", "logits", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.file_utils.is_torch_available": [[108, 110], ["None"], "function", ["None"], ["def", "is_torch_available", "(", ")", ":", "\n", "    ", "return", "_torch_available", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.file_utils.is_tf_available": [[112, 114], ["None"], "function", ["None"], ["", "def", "is_tf_available", "(", ")", ":", "\n", "    ", "return", "_tf_available", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.file_utils.add_start_docstrings": [[116, 122], ["None"], "function", ["None"], ["", "def", "add_start_docstrings", "(", "*", "docstr", ")", ":", "\n", "    ", "def", "docstring_decorator", "(", "fn", ")", ":", "\n", "        ", "fn", ".", "__doc__", "=", "\"\"", ".", "join", "(", "docstr", ")", "+", "(", "fn", ".", "__doc__", "if", "fn", ".", "__doc__", "is", "not", "None", "else", "\"\"", ")", "\n", "return", "fn", "\n", "\n", "", "return", "docstring_decorator", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.file_utils.add_start_docstrings_to_callable": [[124, 147], ["fn.__qualname__.split"], "function", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.utils.sentence_splitter.PunktSentenceSplitter.split"], ["", "def", "add_start_docstrings_to_callable", "(", "*", "docstr", ")", ":", "\n", "    ", "def", "docstring_decorator", "(", "fn", ")", ":", "\n", "        ", "class_name", "=", "\":class:`~transformers.{}`\"", ".", "format", "(", "fn", ".", "__qualname__", ".", "split", "(", "\".\"", ")", "[", "0", "]", ")", "\n", "intro", "=", "\"   The {} forward method, overrides the :func:`__call__` special method.\"", ".", "format", "(", "\n", "class_name", "\n", ")", "\n", "note", "=", "r\"\"\"\n\n    .. note::\n        Although the recipe for forward pass needs to be defined within\n        this function, one should call the :class:`Module` instance afterwards\n        instead of this since the former takes care of running the\n        pre and post processing steps while the latter silently ignores them.\n        \"\"\"", "\n", "fn", ".", "__doc__", "=", "(", "\n", "intro", "\n", "+", "note", "\n", "+", "\"\"", ".", "join", "(", "docstr", ")", "\n", "+", "(", "fn", ".", "__doc__", "if", "fn", ".", "__doc__", "is", "not", "None", "else", "\"\"", ")", "\n", ")", "\n", "return", "fn", "\n", "\n", "", "return", "docstring_decorator", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.file_utils.add_end_docstrings": [[149, 155], ["None"], "function", ["None"], ["", "def", "add_end_docstrings", "(", "*", "docstr", ")", ":", "\n", "    ", "def", "docstring_decorator", "(", "fn", ")", ":", "\n", "        ", "fn", ".", "__doc__", "=", "fn", ".", "__doc__", "+", "\"\"", ".", "join", "(", "docstr", ")", "\n", "return", "fn", "\n", "\n", "", "return", "docstring_decorator", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.file_utils.is_remote_url": [[157, 160], ["urllib.parse.urlparse"], "function", ["None"], ["", "def", "is_remote_url", "(", "url_or_filename", ")", ":", "\n", "    ", "parsed", "=", "urlparse", "(", "url_or_filename", ")", "\n", "return", "parsed", ".", "scheme", "in", "(", "\"http\"", ",", "\"https\"", ",", "\"s3\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.file_utils.hf_bucket_url": [[162, 168], ["None"], "function", ["None"], ["", "def", "hf_bucket_url", "(", "identifier", ",", "postfix", "=", "None", ",", "cdn", "=", "False", ")", "->", "str", ":", "\n", "    ", "endpoint", "=", "CLOUDFRONT_DISTRIB_PREFIX", "if", "cdn", "else", "S3_BUCKET_PREFIX", "\n", "if", "postfix", "is", "None", ":", "\n", "        ", "return", "\"/\"", ".", "join", "(", "(", "endpoint", ",", "identifier", ")", ")", "\n", "", "else", ":", "\n", "        ", "return", "\"/\"", ".", "join", "(", "(", "endpoint", ",", "identifier", ",", "postfix", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.file_utils.url_to_filename": [[170, 192], ["url.encode", "hashlib.sha256", "hashlib.sha256.hexdigest", "url.endswith", "etag.encode", "hashlib.sha256", "hashlib.sha256.hexdigest"], "function", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizer.encode", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizer.encode"], ["", "", "def", "url_to_filename", "(", "url", ",", "etag", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    Convert `url` into a hashed filename in a repeatable way.\n    If `etag` is specified, append its hash to the url's, delimited\n    by a period.\n    If the url ends with .h5 (Keras HDF5 weights) adds '.h5' to the name\n    so that TF 2.0 can identify it as a HDF5 file\n    (see https://github.com/tensorflow/tensorflow/blob/00fad90125b18b80fe054de1055770cfb8fe4ba3/tensorflow/python/keras/engine/network.py#L1380)\n    \"\"\"", "\n", "url_bytes", "=", "url", ".", "encode", "(", "\"utf-8\"", ")", "\n", "url_hash", "=", "sha256", "(", "url_bytes", ")", "\n", "filename", "=", "url_hash", ".", "hexdigest", "(", ")", "\n", "\n", "if", "etag", ":", "\n", "        ", "etag_bytes", "=", "etag", ".", "encode", "(", "\"utf-8\"", ")", "\n", "etag_hash", "=", "sha256", "(", "etag_bytes", ")", "\n", "filename", "+=", "\".\"", "+", "etag_hash", ".", "hexdigest", "(", ")", "\n", "\n", "", "if", "url", ".", "endswith", "(", "\".h5\"", ")", ":", "\n", "        ", "filename", "+=", "\".h5\"", "\n", "\n", "", "return", "filename", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.file_utils.filename_to_url": [[194, 218], ["isinstance", "os.path.join", "str", "os.path.exists", "EnvironmentError", "os.path.exists", "EnvironmentError", "open", "json.load"], "function", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.BaseTrainer.BaseTrainer.load"], ["", "def", "filename_to_url", "(", "filename", ",", "cache_dir", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    Return the url and etag (which may be ``None``) stored for `filename`.\n    Raise ``EnvironmentError`` if `filename` or its stored metadata do not exist.\n    \"\"\"", "\n", "if", "cache_dir", "is", "None", ":", "\n", "        ", "cache_dir", "=", "TRANSFORMERS_CACHE", "\n", "", "if", "isinstance", "(", "cache_dir", ",", "Path", ")", ":", "\n", "        ", "cache_dir", "=", "str", "(", "cache_dir", ")", "\n", "\n", "", "cache_path", "=", "os", ".", "path", ".", "join", "(", "cache_dir", ",", "filename", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "cache_path", ")", ":", "\n", "        ", "raise", "EnvironmentError", "(", "\"file {} not found\"", ".", "format", "(", "cache_path", ")", ")", "\n", "\n", "", "meta_path", "=", "cache_path", "+", "\".json\"", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "meta_path", ")", ":", "\n", "        ", "raise", "EnvironmentError", "(", "\"file {} not found\"", ".", "format", "(", "meta_path", ")", ")", "\n", "\n", "", "with", "open", "(", "meta_path", ",", "encoding", "=", "\"utf-8\"", ")", "as", "meta_file", ":", "\n", "        ", "metadata", "=", "json", ".", "load", "(", "meta_file", ")", "\n", "", "url", "=", "metadata", "[", "\"url\"", "]", "\n", "etag", "=", "metadata", "[", "\"etag\"", "]", "\n", "\n", "return", "url", ",", "etag", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.file_utils.cached_path": [[220, 318], ["isinstance", "isinstance", "file_utils.is_remote_url", "str", "str", "file_utils.get_from_cache", "os.path.exists", "os.path.split", "os.path.join", "output_file.replace", "os.path.isdir", "os.listdir", "filelock.FileLock", "shutil.rmtree", "os.makedirs", "zipfile.is_zipfile", "EnvironmentError", "ValueError", "zipfile.is_zipfile", "tarfile.is_tarfile", "tarfile.is_tarfile", "urllib.parse.urlparse", "zipfile.ZipFile", "zip_file.extractall", "zip_file.close", "tarfile.open", "tarfile.open.extractall", "tarfile.open.close", "EnvironmentError"], "function", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.file_utils.is_remote_url", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.file_utils.get_from_cache", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.utils.sentence_splitter.PunktSentenceSplitter.split", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.closablequeue.ClosableQueue.close", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.closablequeue.ClosableQueue.close"], ["", "def", "cached_path", "(", "\n", "url_or_filename", ",", "\n", "cache_dir", "=", "None", ",", "\n", "force_download", "=", "False", ",", "\n", "proxies", "=", "None", ",", "\n", "resume_download", "=", "False", ",", "\n", "user_agent", "=", "None", ",", "\n", "extract_compressed_file", "=", "False", ",", "\n", "force_extract", "=", "False", ",", "\n", "local_files_only", "=", "False", ",", "\n", ")", "->", "Optional", "[", "str", "]", ":", "\n", "    ", "\"\"\"\n    Given something that might be a URL (or might be a local path),\n    determine which. If it's a URL, download the file and cache it, and\n    return the path to the cached file. If it's already a local path,\n    make sure the file exists and then return the path.\n    Args:\n        cache_dir: specify a cache directory to save the file to (overwrite the default cache dir).\n        force_download: if True, re-dowload the file even if it's already cached in the cache dir.\n        resume_download: if True, resume the download if incompletly recieved file is found.\n        user_agent: Optional string or dict that will be appended to the user-agent on remote requests.\n        extract_compressed_file: if True and the path point to a zip or tar file, extract the compressed\n            file in a folder along the archive.\n        force_extract: if True when extract_compressed_file is True and the archive was already extracted,\n            re-extract the archive and overide the folder where it was extracted.\n\n    Return:\n        None in case of non-recoverable file (non-existent or inaccessible url + no cache on disk).\n        Local path (string) otherwise\n    \"\"\"", "\n", "if", "cache_dir", "is", "None", ":", "\n", "        ", "cache_dir", "=", "TRANSFORMERS_CACHE", "\n", "", "if", "isinstance", "(", "url_or_filename", ",", "Path", ")", ":", "\n", "        ", "url_or_filename", "=", "str", "(", "url_or_filename", ")", "\n", "", "if", "isinstance", "(", "cache_dir", ",", "Path", ")", ":", "\n", "        ", "cache_dir", "=", "str", "(", "cache_dir", ")", "\n", "\n", "", "if", "is_remote_url", "(", "url_or_filename", ")", ":", "\n", "# URL, so get it from the cache (downloading if necessary)", "\n", "        ", "output_path", "=", "get_from_cache", "(", "\n", "url_or_filename", ",", "\n", "cache_dir", "=", "cache_dir", ",", "\n", "force_download", "=", "force_download", ",", "\n", "proxies", "=", "proxies", ",", "\n", "resume_download", "=", "resume_download", ",", "\n", "user_agent", "=", "user_agent", ",", "\n", "local_files_only", "=", "local_files_only", ",", "\n", ")", "\n", "", "elif", "os", ".", "path", ".", "exists", "(", "url_or_filename", ")", ":", "\n", "# File, and it exists.", "\n", "        ", "output_path", "=", "url_or_filename", "\n", "", "elif", "urlparse", "(", "url_or_filename", ")", ".", "scheme", "==", "\"\"", ":", "\n", "# File, but it doesn't exist.", "\n", "        ", "raise", "EnvironmentError", "(", "\"file {} not found\"", ".", "format", "(", "url_or_filename", ")", ")", "\n", "", "else", ":", "\n", "# Something unknown", "\n", "        ", "raise", "ValueError", "(", "\n", "\"unable to parse {} as a URL or as a local path\"", ".", "format", "(", "url_or_filename", ")", "\n", ")", "\n", "\n", "", "if", "extract_compressed_file", ":", "\n", "        ", "if", "not", "is_zipfile", "(", "output_path", ")", "and", "not", "tarfile", ".", "is_tarfile", "(", "output_path", ")", ":", "\n", "            ", "return", "output_path", "\n", "\n", "# Path where we extract compressed archives", "\n", "# We avoid '.' in dir name and add \"-extracted\" at the end: \"./model.zip\" => \"./model-zip-extracted/\"", "\n", "", "output_dir", ",", "output_file", "=", "os", ".", "path", ".", "split", "(", "output_path", ")", "\n", "output_extract_dir_name", "=", "output_file", ".", "replace", "(", "\".\"", ",", "\"-\"", ")", "+", "\"-extracted\"", "\n", "output_path_extracted", "=", "os", ".", "path", ".", "join", "(", "output_dir", ",", "output_extract_dir_name", ")", "\n", "\n", "if", "(", "\n", "os", ".", "path", ".", "isdir", "(", "output_path_extracted", ")", "\n", "and", "os", ".", "listdir", "(", "output_path_extracted", ")", "\n", "and", "not", "force_extract", "\n", ")", ":", "\n", "            ", "return", "output_path_extracted", "\n", "\n", "# Prevent parallel extractions", "\n", "", "lock_path", "=", "output_path", "+", "\".lock\"", "\n", "with", "FileLock", "(", "lock_path", ")", ":", "\n", "            ", "shutil", ".", "rmtree", "(", "output_path_extracted", ",", "ignore_errors", "=", "True", ")", "\n", "os", ".", "makedirs", "(", "output_path_extracted", ")", "\n", "if", "is_zipfile", "(", "output_path", ")", ":", "\n", "                ", "with", "ZipFile", "(", "output_path", ",", "\"r\"", ")", "as", "zip_file", ":", "\n", "                    ", "zip_file", ".", "extractall", "(", "output_path_extracted", ")", "\n", "zip_file", ".", "close", "(", ")", "\n", "", "", "elif", "tarfile", ".", "is_tarfile", "(", "output_path", ")", ":", "\n", "                ", "tar_file", "=", "tarfile", ".", "open", "(", "output_path", ")", "\n", "tar_file", ".", "extractall", "(", "output_path_extracted", ")", "\n", "tar_file", ".", "close", "(", ")", "\n", "", "else", ":", "\n", "                ", "raise", "EnvironmentError", "(", "\n", "\"Archive format of {} could not be identified\"", ".", "format", "(", "output_path", ")", "\n", ")", "\n", "\n", "", "", "return", "output_path_extracted", "\n", "\n", "", "return", "output_path", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.file_utils.split_s3_path": [[320, 331], ["urllib.parse.urlparse", "s3_path.startswith", "ValueError"], "function", ["None"], ["", "def", "split_s3_path", "(", "url", ")", ":", "\n", "    ", "\"\"\"Split a full s3 path into the bucket name and path.\"\"\"", "\n", "parsed", "=", "urlparse", "(", "url", ")", "\n", "if", "not", "parsed", ".", "netloc", "or", "not", "parsed", ".", "path", ":", "\n", "        ", "raise", "ValueError", "(", "\"bad s3 path {}\"", ".", "format", "(", "url", ")", ")", "\n", "", "bucket_name", "=", "parsed", ".", "netloc", "\n", "s3_path", "=", "parsed", ".", "path", "\n", "# Remove '/' at beginning of path.", "\n", "if", "s3_path", ".", "startswith", "(", "\"/\"", ")", ":", "\n", "        ", "s3_path", "=", "s3_path", "[", "1", ":", "]", "\n", "", "return", "bucket_name", ",", "s3_path", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.file_utils.s3_request": [[333, 350], ["functools.wraps", "func", "int", "EnvironmentError"], "function", ["None"], ["", "def", "s3_request", "(", "func", ")", ":", "\n", "    ", "\"\"\"\n    Wrapper function for s3 requests in order to create more helpful error\n    messages.\n    \"\"\"", "\n", "\n", "@", "wraps", "(", "func", ")", "\n", "def", "wrapper", "(", "url", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "try", ":", "\n", "            ", "return", "func", "(", "url", ",", "*", "args", ",", "**", "kwargs", ")", "\n", "", "except", "ClientError", "as", "exc", ":", "\n", "            ", "if", "int", "(", "exc", ".", "response", "[", "\"Error\"", "]", "[", "\"Code\"", "]", ")", "==", "404", ":", "\n", "                ", "raise", "EnvironmentError", "(", "\"file {} not found\"", ".", "format", "(", "url", ")", ")", "\n", "", "else", ":", "\n", "                ", "raise", "\n", "\n", "", "", "", "return", "wrapper", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.file_utils.s3_etag": [[352, 359], ["boto3.resource", "file_utils.split_s3_path", "boto3.resource.Object", "botocore.config.Config"], "function", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.file_utils.split_s3_path"], ["", "@", "s3_request", "\n", "def", "s3_etag", "(", "url", ",", "proxies", "=", "None", ")", ":", "\n", "    ", "\"\"\"Check ETag on S3 object.\"\"\"", "\n", "s3_resource", "=", "boto3", ".", "resource", "(", "\"s3\"", ",", "config", "=", "Config", "(", "proxies", "=", "proxies", ")", ")", "\n", "bucket_name", ",", "s3_path", "=", "split_s3_path", "(", "url", ")", "\n", "s3_object", "=", "s3_resource", ".", "Object", "(", "bucket_name", ",", "s3_path", ")", "\n", "return", "s3_object", ".", "e_tag", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.file_utils.s3_get": [[361, 367], ["boto3.resource", "file_utils.split_s3_path", "boto3.resource.Bucket().download_fileobj", "botocore.config.Config", "boto3.resource.Bucket"], "function", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.file_utils.split_s3_path"], ["", "@", "s3_request", "\n", "def", "s3_get", "(", "url", ",", "temp_file", ",", "proxies", "=", "None", ")", ":", "\n", "    ", "\"\"\"Pull a file directly from S3.\"\"\"", "\n", "s3_resource", "=", "boto3", ".", "resource", "(", "\"s3\"", ",", "config", "=", "Config", "(", "proxies", "=", "proxies", ")", ")", "\n", "bucket_name", ",", "s3_path", "=", "split_s3_path", "(", "url", ")", "\n", "s3_resource", ".", "Bucket", "(", "bucket_name", ")", ".", "download_fileobj", "(", "s3_path", ",", "temp_file", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.file_utils.http_get": [[369, 400], ["file_utils.is_torch_available", "file_utils.is_tf_available", "isinstance", "requests.get", "requests.get.headers.get", "tqdm.auto.tqdm", "requests.get.iter_content", "tqdm.auto.tqdm.close", "isinstance", "sys.version.split", "int", "bool", "tqdm.auto.tqdm.update", "temp_file.write", "len", "logger.getEffectiveLevel", "user_agent.items"], "function", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.file_utils.is_torch_available", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.file_utils.is_tf_available", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.closablequeue.ClosableQueue.get", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.closablequeue.ClosableQueue.get", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.closablequeue.ClosableQueue.close", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.utils.sentence_splitter.PunktSentenceSplitter.split", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.HMNetTrainer.HMNetTrainer.update"], ["", "def", "http_get", "(", "url", ",", "temp_file", ",", "proxies", "=", "None", ",", "resume_size", "=", "0", ",", "user_agent", "=", "None", ")", ":", "\n", "    ", "ua", "=", "\"transformers/{}; python/{}\"", ".", "format", "(", "__version__", ",", "sys", ".", "version", ".", "split", "(", ")", "[", "0", "]", ")", "\n", "if", "is_torch_available", "(", ")", ":", "\n", "        ", "ua", "+=", "\"; torch/{}\"", ".", "format", "(", "torch", ".", "__version__", ")", "\n", "", "if", "is_tf_available", "(", ")", ":", "\n", "        ", "ua", "+=", "\"; tensorflow/{}\"", ".", "format", "(", "tf", ".", "__version__", ")", "\n", "", "if", "isinstance", "(", "user_agent", ",", "dict", ")", ":", "\n", "        ", "ua", "+=", "\"; \"", "+", "\"; \"", ".", "join", "(", "\"{}/{}\"", ".", "format", "(", "k", ",", "v", ")", "for", "k", ",", "v", "in", "user_agent", ".", "items", "(", ")", ")", "\n", "", "elif", "isinstance", "(", "user_agent", ",", "str", ")", ":", "\n", "        ", "ua", "+=", "\"; \"", "+", "user_agent", "\n", "", "headers", "=", "{", "\"user-agent\"", ":", "ua", "}", "\n", "if", "resume_size", ">", "0", ":", "\n", "        ", "headers", "[", "\"Range\"", "]", "=", "\"bytes=%d-\"", "%", "(", "resume_size", ",", ")", "\n", "", "response", "=", "requests", ".", "get", "(", "url", ",", "stream", "=", "True", ",", "proxies", "=", "proxies", ",", "headers", "=", "headers", ")", "\n", "if", "response", ".", "status_code", "==", "416", ":", "# Range not satisfiable", "\n", "        ", "return", "\n", "", "content_length", "=", "response", ".", "headers", ".", "get", "(", "\"Content-Length\"", ")", "\n", "total", "=", "resume_size", "+", "int", "(", "content_length", ")", "if", "content_length", "is", "not", "None", "else", "None", "\n", "progress", "=", "tqdm", "(", "\n", "unit", "=", "\"B\"", ",", "\n", "unit_scale", "=", "True", ",", "\n", "total", "=", "total", ",", "\n", "initial", "=", "resume_size", ",", "\n", "desc", "=", "\"Downloading\"", ",", "\n", "disable", "=", "bool", "(", "logger", ".", "getEffectiveLevel", "(", ")", "==", "logging", ".", "NOTSET", ")", ",", "\n", ")", "\n", "for", "chunk", "in", "response", ".", "iter_content", "(", "chunk_size", "=", "1024", ")", ":", "\n", "        ", "if", "chunk", ":", "# filter out keep-alive new chunks", "\n", "            ", "progress", ".", "update", "(", "len", "(", "chunk", ")", ")", "\n", "temp_file", ".", "write", "(", "chunk", ")", "\n", "", "", "progress", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.file_utils.get_from_cache": [[402, 535], ["isinstance", "os.makedirs", "file_utils.url_to_filename", "os.path.join", "str", "url.startswith", "os.path.exists", "os.path.exists", "filelock.FileLock", "logger.info", "os.rename", "logger.info", "file_utils.s3_etag", "os.path.exists", "functools.partial", "functools.partial.", "logger.info", "url.startswith", "open", "json.dump", "requests.head", "len", "os.path.join", "file_utils.s3_get", "file_utils.http_get", "requests.head.headers.get", "fnmatch.filter", "ValueError", "open", "os.stat", "logger.warn", "os.listdir", "file.endswith", "file.endswith"], "function", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.file_utils.url_to_filename", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.file_utils.s3_etag", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.file_utils.s3_get", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.file_utils.http_get", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.closablequeue.ClosableQueue.get"], ["", "def", "get_from_cache", "(", "\n", "url", ",", "\n", "cache_dir", "=", "None", ",", "\n", "force_download", "=", "False", ",", "\n", "proxies", "=", "None", ",", "\n", "etag_timeout", "=", "10", ",", "\n", "resume_download", "=", "False", ",", "\n", "user_agent", "=", "None", ",", "\n", "local_files_only", "=", "False", ",", "\n", ")", "->", "Optional", "[", "str", "]", ":", "\n", "    ", "\"\"\"\n    Given a URL, look for the corresponding file in the local cache.\n    If it's not there, download it. Then return the path to the cached file.\n\n    Return:\n        None in case of non-recoverable file (non-existent or inaccessible url + no cache on disk).\n        Local path (string) otherwise\n    \"\"\"", "\n", "if", "cache_dir", "is", "None", ":", "\n", "        ", "cache_dir", "=", "TRANSFORMERS_CACHE", "\n", "", "if", "isinstance", "(", "cache_dir", ",", "Path", ")", ":", "\n", "        ", "cache_dir", "=", "str", "(", "cache_dir", ")", "\n", "\n", "", "os", ".", "makedirs", "(", "cache_dir", ",", "exist_ok", "=", "True", ")", "\n", "\n", "etag", "=", "None", "\n", "if", "not", "local_files_only", ":", "\n", "# Get eTag to add to filename, if it exists.", "\n", "        ", "if", "url", ".", "startswith", "(", "\"s3://\"", ")", ":", "\n", "            ", "etag", "=", "s3_etag", "(", "url", ",", "proxies", "=", "proxies", ")", "\n", "", "else", ":", "\n", "            ", "try", ":", "\n", "                ", "response", "=", "requests", ".", "head", "(", "\n", "url", ",", "allow_redirects", "=", "True", ",", "proxies", "=", "proxies", ",", "timeout", "=", "etag_timeout", "\n", ")", "\n", "if", "response", ".", "status_code", "==", "200", ":", "\n", "                    ", "etag", "=", "response", ".", "headers", ".", "get", "(", "\"ETag\"", ")", "\n", "", "", "except", "(", "EnvironmentError", ",", "requests", ".", "exceptions", ".", "Timeout", ")", ":", "\n", "# etag is already None", "\n", "                ", "pass", "\n", "\n", "", "", "", "filename", "=", "url_to_filename", "(", "url", ",", "etag", ")", "\n", "\n", "# get cache path to put the file", "\n", "cache_path", "=", "os", ".", "path", ".", "join", "(", "cache_dir", ",", "filename", ")", "\n", "\n", "# etag is None = we don't have a connection, or url doesn't exist, or is otherwise inaccessible.", "\n", "# try to get the last downloaded one", "\n", "if", "etag", "is", "None", ":", "\n", "        ", "if", "os", ".", "path", ".", "exists", "(", "cache_path", ")", ":", "\n", "            ", "return", "cache_path", "\n", "", "else", ":", "\n", "            ", "matching_files", "=", "[", "\n", "file", "\n", "for", "file", "in", "fnmatch", ".", "filter", "(", "os", ".", "listdir", "(", "cache_dir", ")", ",", "filename", "+", "\".*\"", ")", "\n", "if", "not", "file", ".", "endswith", "(", "\".json\"", ")", "and", "not", "file", ".", "endswith", "(", "\".lock\"", ")", "\n", "]", "\n", "if", "len", "(", "matching_files", ")", ">", "0", ":", "\n", "                ", "return", "os", ".", "path", ".", "join", "(", "cache_dir", ",", "matching_files", "[", "-", "1", "]", ")", "\n", "", "else", ":", "\n", "# If files cannot be found and local_files_only=True,", "\n", "# the models might've been found if local_files_only=False", "\n", "# Notify the user about that", "\n", "                ", "if", "local_files_only", ":", "\n", "                    ", "raise", "ValueError", "(", "\n", "\"Cannot find the requested files in the cached path and outgoing traffic has been\"", "\n", "\" disabled. To enable model look-ups and downloads online, set 'local_files_only'\"", "\n", "\" to False.\"", "\n", ")", "\n", "", "return", "None", "\n", "\n", "# From now on, etag is not None.", "\n", "", "", "", "if", "os", ".", "path", ".", "exists", "(", "cache_path", ")", "and", "not", "force_download", ":", "\n", "        ", "return", "cache_path", "\n", "\n", "# Prevent parallel downloads of the same file with a lock.", "\n", "", "lock_path", "=", "cache_path", "+", "\".lock\"", "\n", "with", "FileLock", "(", "lock_path", ")", ":", "\n", "\n", "        ", "if", "resume_download", ":", "\n", "            ", "incomplete_path", "=", "cache_path", "+", "\".incomplete\"", "\n", "\n", "@", "contextmanager", "\n", "def", "_resumable_file_manager", "(", ")", ":", "\n", "                ", "with", "open", "(", "incomplete_path", ",", "\"a+b\"", ")", "as", "f", ":", "\n", "                    ", "yield", "f", "\n", "\n", "", "", "temp_file_manager", "=", "_resumable_file_manager", "\n", "if", "os", ".", "path", ".", "exists", "(", "incomplete_path", ")", ":", "\n", "                ", "resume_size", "=", "os", ".", "stat", "(", "incomplete_path", ")", ".", "st_size", "\n", "", "else", ":", "\n", "                ", "resume_size", "=", "0", "\n", "", "", "else", ":", "\n", "            ", "temp_file_manager", "=", "partial", "(", "\n", "tempfile", ".", "NamedTemporaryFile", ",", "dir", "=", "cache_dir", ",", "delete", "=", "False", "\n", ")", "\n", "resume_size", "=", "0", "\n", "\n", "# Download to temporary file, then copy to cache dir once finished.", "\n", "# Otherwise you get corrupt cache entries if the download gets interrupted.", "\n", "", "with", "temp_file_manager", "(", ")", "as", "temp_file", ":", "\n", "            ", "logger", ".", "info", "(", "\n", "\"%s not found in cache or force_download set to True, downloading to %s\"", ",", "\n", "url", ",", "\n", "temp_file", ".", "name", ",", "\n", ")", "\n", "\n", "# GET file object", "\n", "if", "url", ".", "startswith", "(", "\"s3://\"", ")", ":", "\n", "                ", "if", "resume_download", ":", "\n", "                    ", "logger", ".", "warn", "(", "\n", "'Warning: resumable downloads are not implemented for \"s3://\" urls'", "\n", ")", "\n", "", "s3_get", "(", "url", ",", "temp_file", ",", "proxies", "=", "proxies", ")", "\n", "", "else", ":", "\n", "                ", "http_get", "(", "\n", "url", ",", "\n", "temp_file", ",", "\n", "proxies", "=", "proxies", ",", "\n", "resume_size", "=", "resume_size", ",", "\n", "user_agent", "=", "user_agent", ",", "\n", ")", "\n", "\n", "", "", "logger", ".", "info", "(", "\"storing %s in cache at %s\"", ",", "url", ",", "cache_path", ")", "\n", "os", ".", "rename", "(", "temp_file", ".", "name", ",", "cache_path", ")", "\n", "\n", "logger", ".", "info", "(", "\"creating metadata file for %s\"", ",", "cache_path", ")", "\n", "meta", "=", "{", "\"url\"", ":", "url", ",", "\"etag\"", ":", "etag", "}", "\n", "meta_path", "=", "cache_path", "+", "\".json\"", "\n", "with", "open", "(", "meta_path", ",", "\"w\"", ")", "as", "meta_file", ":", "\n", "            ", "json", ".", "dump", "(", "meta", ",", "meta_file", ")", "\n", "\n", "", "", "return", "cache_path", "\n", "", ""]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizer.bos_token": [[231, 234], ["None"], "methods", ["None"], ["", "@", "bos_token", ".", "setter", "\n", "def", "bos_token", "(", "self", ",", "value", ")", ":", "\n", "        ", "self", ".", "_bos_token", "=", "value", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizer.eos_token": [[235, 238], ["None"], "methods", ["None"], ["", "@", "eos_token", ".", "setter", "\n", "def", "eos_token", "(", "self", ",", "value", ")", ":", "\n", "        ", "self", ".", "_eos_token", "=", "value", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizer.unk_token": [[239, 242], ["None"], "methods", ["None"], ["", "@", "unk_token", ".", "setter", "\n", "def", "unk_token", "(", "self", ",", "value", ")", ":", "\n", "        ", "self", ".", "_unk_token", "=", "value", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizer.sep_token": [[243, 246], ["None"], "methods", ["None"], ["", "@", "sep_token", ".", "setter", "\n", "def", "sep_token", "(", "self", ",", "value", ")", ":", "\n", "        ", "self", ".", "_sep_token", "=", "value", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizer.pad_token": [[247, 250], ["None"], "methods", ["None"], ["", "@", "pad_token", ".", "setter", "\n", "def", "pad_token", "(", "self", ",", "value", ")", ":", "\n", "        ", "self", ".", "_pad_token", "=", "value", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizer.cls_token": [[251, 254], ["None"], "methods", ["None"], ["", "@", "cls_token", ".", "setter", "\n", "def", "cls_token", "(", "self", ",", "value", ")", ":", "\n", "        ", "self", ".", "_cls_token", "=", "value", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizer.mask_token": [[255, 258], ["None"], "methods", ["None"], ["", "@", "mask_token", ".", "setter", "\n", "def", "mask_token", "(", "self", ",", "value", ")", ":", "\n", "        ", "self", ".", "_mask_token", "=", "value", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizer.additional_special_tokens": [[259, 262], ["None"], "methods", ["None"], ["", "@", "additional_special_tokens", ".", "setter", "\n", "def", "additional_special_tokens", "(", "self", ",", "value", ")", ":", "\n", "        ", "self", ".", "_additional_special_tokens", "=", "value", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizer.bos_token_id": [[263, 267], ["tokenization_utils.PreTrainedTokenizer.convert_tokens_to_ids"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizer.convert_tokens_to_ids"], ["", "@", "property", "\n", "def", "bos_token_id", "(", "self", ")", ":", "\n", "        ", "\"\"\"Id of the beginning of sentence token in the vocabulary. Log an error if used while not having been set.\"\"\"", "\n", "return", "self", ".", "convert_tokens_to_ids", "(", "self", ".", "bos_token", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizer.eos_token_id": [[268, 272], ["tokenization_utils.PreTrainedTokenizer.convert_tokens_to_ids"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizer.convert_tokens_to_ids"], ["", "@", "property", "\n", "def", "eos_token_id", "(", "self", ")", ":", "\n", "        ", "\"\"\"Id of the end of sentence token in the vocabulary. Log an error if used while not having been set.\"\"\"", "\n", "return", "self", ".", "convert_tokens_to_ids", "(", "self", ".", "eos_token", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizer.unk_token_id": [[273, 277], ["tokenization_utils.PreTrainedTokenizer.convert_tokens_to_ids"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizer.convert_tokens_to_ids"], ["", "@", "property", "\n", "def", "unk_token_id", "(", "self", ")", ":", "\n", "        ", "\"\"\"Id of the unknown token in the vocabulary. Log an error if used while not having been set.\"\"\"", "\n", "return", "self", ".", "convert_tokens_to_ids", "(", "self", ".", "unk_token", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizer.sep_token_id": [[278, 282], ["tokenization_utils.PreTrainedTokenizer.convert_tokens_to_ids"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizer.convert_tokens_to_ids"], ["", "@", "property", "\n", "def", "sep_token_id", "(", "self", ")", ":", "\n", "        ", "\"\"\"Id of the separation token in the vocabulary. E.g. separate context and query in an input sequence. Log an error if used while not having been set.\"\"\"", "\n", "return", "self", ".", "convert_tokens_to_ids", "(", "self", ".", "sep_token", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizer.pad_token_id": [[283, 287], ["tokenization_utils.PreTrainedTokenizer.convert_tokens_to_ids"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizer.convert_tokens_to_ids"], ["", "@", "property", "\n", "def", "pad_token_id", "(", "self", ")", ":", "\n", "        ", "\"\"\"Id of the padding token in the vocabulary. Log an error if used while not having been set.\"\"\"", "\n", "return", "self", ".", "convert_tokens_to_ids", "(", "self", ".", "pad_token", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizer.pad_token_type_id": [[288, 292], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "pad_token_type_id", "(", "self", ")", ":", "\n", "        ", "\"\"\"Id of the padding token type in the vocabulary.\"\"\"", "\n", "return", "self", ".", "_pad_token_type_id", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizer.cls_token_id": [[293, 297], ["tokenization_utils.PreTrainedTokenizer.convert_tokens_to_ids"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizer.convert_tokens_to_ids"], ["", "@", "property", "\n", "def", "cls_token_id", "(", "self", ")", ":", "\n", "        ", "\"\"\"Id of the classification token in the vocabulary. E.g. to extract a summary of an input sequence leveraging self-attention along the full depth of the model. Log an error if used while not having been set.\"\"\"", "\n", "return", "self", ".", "convert_tokens_to_ids", "(", "self", ".", "cls_token", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizer.mask_token_id": [[298, 302], ["tokenization_utils.PreTrainedTokenizer.convert_tokens_to_ids"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizer.convert_tokens_to_ids"], ["", "@", "property", "\n", "def", "mask_token_id", "(", "self", ")", ":", "\n", "        ", "\"\"\"Id of the mask token in the vocabulary. E.g. when training a model with masked-language modeling. Log an error if used while not having been set.\"\"\"", "\n", "return", "self", ".", "convert_tokens_to_ids", "(", "self", ".", "mask_token", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizer.additional_special_tokens_ids": [[303, 307], ["tokenization_utils.PreTrainedTokenizer.convert_tokens_to_ids"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizer.convert_tokens_to_ids"], ["", "@", "property", "\n", "def", "additional_special_tokens_ids", "(", "self", ")", ":", "\n", "        ", "\"\"\"Ids of all the additional special tokens in the vocabulary (list of integers). Log an error if used while not having been set.\"\"\"", "\n", "return", "self", ".", "convert_tokens_to_ids", "(", "self", ".", "additional_special_tokens", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizer.get_vocab": [[308, 311], ["NotImplementedError"], "methods", ["None"], ["", "def", "get_vocab", "(", "self", ")", ":", "\n", "        ", "\"\"\"Returns the vocabulary as a dict of {token: index} pairs. `tokenizer.get_vocab()[token]` is equivalent to `tokenizer.convert_tokens_to_ids(token)` when `token` is in the vocab.\"\"\"", "\n", "raise", "NotImplementedError", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizer.__init__": [[312, 347], ["kwargs.pop", "kwargs.pop", "set", "kwargs.items", "int", "setattr", "isinstance", "isinstance", "all", "isinstance"], "methods", ["None"], ["", "def", "__init__", "(", "self", ",", "max_len", "=", "None", ",", "**", "kwargs", ")", ":", "\n", "        ", "self", ".", "_bos_token", "=", "None", "\n", "self", ".", "_eos_token", "=", "None", "\n", "self", ".", "_unk_token", "=", "None", "\n", "self", ".", "_sep_token", "=", "None", "\n", "self", ".", "_pad_token", "=", "None", "\n", "self", ".", "_cls_token", "=", "None", "\n", "self", ".", "_mask_token", "=", "None", "\n", "self", ".", "_pad_token_type_id", "=", "0", "\n", "self", ".", "_additional_special_tokens", "=", "[", "]", "\n", "\n", "self", ".", "max_len", "=", "max_len", "if", "max_len", "is", "not", "None", "else", "int", "(", "1e12", ")", "\n", "\n", "# Padding side is right by default and over-riden in subclasses. If specified in the kwargs, it is changed.", "\n", "self", ".", "padding_side", "=", "kwargs", ".", "pop", "(", "\"padding_side\"", ",", "self", ".", "padding_side", ")", "\n", "self", ".", "model_input_names", "=", "kwargs", ".", "pop", "(", "\"model_input_names\"", ",", "self", ".", "model_input_names", ")", "\n", "\n", "# Added tokens", "\n", "self", ".", "added_tokens_encoder", "=", "{", "}", "\n", "self", ".", "unique_added_tokens_encoder", "=", "set", "(", ")", "\n", "self", ".", "added_tokens_decoder", "=", "{", "}", "\n", "\n", "# inputs and kwargs for saving and re-loading (see ``from_pretrained`` and ``save_pretrained``)", "\n", "self", ".", "init_inputs", "=", "(", ")", "\n", "self", ".", "init_kwargs", "=", "{", "}", "\n", "\n", "for", "key", ",", "value", "in", "kwargs", ".", "items", "(", ")", ":", "\n", "            ", "if", "key", "in", "self", ".", "SPECIAL_TOKENS_ATTRIBUTES", ":", "\n", "                ", "if", "key", "==", "\"additional_special_tokens\"", ":", "\n", "                    ", "assert", "isinstance", "(", "value", ",", "(", "list", ",", "tuple", ")", ")", "and", "all", "(", "\n", "isinstance", "(", "t", ",", "str", ")", "for", "t", "in", "value", "\n", ")", "\n", "", "else", ":", "\n", "                    ", "assert", "isinstance", "(", "value", ",", "str", ")", "\n", "", "setattr", "(", "self", ",", "key", ",", "value", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizer.from_pretrained": [[348, 402], ["cls._from_pretrained"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizer._from_pretrained"], ["", "", "", "@", "classmethod", "\n", "def", "from_pretrained", "(", "cls", ",", "*", "inputs", ",", "**", "kwargs", ")", ":", "\n", "        ", "r\"\"\"\n        Instantiate a :class:`~transformers.PreTrainedTokenizer` (or a derived class) from a predefined tokenizer.\n\n        Args:\n            pretrained_model_name_or_path: either:\n\n                - a string with the `shortcut name` of a predefined tokenizer to load from cache or download, e.g.: ``bert-base-uncased``.\n                - a string with the `identifier name` of a predefined tokenizer that was user-uploaded to our S3, e.g.: ``dbmdz/bert-base-german-cased``.\n                - a path to a `directory` containing vocabulary files required by the tokenizer, for instance saved using the :func:`~transformers.PreTrainedTokenizer.save_pretrained` method, e.g.: ``./my_model_directory/``.\n                - (not applicable to all derived classes, deprecated) a path or url to a single saved vocabulary file if and only if the tokenizer only requires a single vocabulary file (e.g. Bert, XLNet), e.g.: ``./my_model_directory/vocab.txt``.\n\n            cache_dir: (`optional`) string:\n                Path to a directory in which a downloaded predefined tokenizer vocabulary files should be cached if the standard cache should not be used.\n\n            force_download: (`optional`) boolean, default False:\n                Force to (re-)download the vocabulary files and override the cached versions if they exists.\n\n            resume_download: (`optional`) boolean, default False:\n                Do not delete incompletely recieved file. Attempt to resume the download if such a file exists.\n\n            proxies: (`optional`) dict, default None:\n                A dictionary of proxy servers to use by protocol or endpoint, e.g.: {'http': 'foo.bar:3128', 'http://hostname': 'foo.bar:4012'}.\n                The proxies are used on each request.\n\n            inputs: (`optional`) positional arguments: will be passed to the Tokenizer ``__init__`` method.\n\n            kwargs: (`optional`) keyword arguments: will be passed to the Tokenizer ``__init__`` method. Can be used to set special tokens like ``bos_token``, ``eos_token``, ``unk_token``, ``sep_token``, ``pad_token``, ``cls_token``, ``mask_token``, ``additional_special_tokens``. See parameters in the doc string of :class:`~transformers.PreTrainedTokenizer` for details.\n\n        Examples::\n\n            # We can't instantiate directly the base class `PreTrainedTokenizer` so let's show our examples on a derived class: BertTokenizer\n\n            # Download vocabulary from S3 and cache.\n            tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\n            # Download vocabulary from S3 (user-uploaded) and cache.\n            tokenizer = BertTokenizer.from_pretrained('dbmdz/bert-base-german-cased')\n\n            # If vocabulary files are in a directory (e.g. tokenizer was saved using `save_pretrained('./test/saved_model/')`)\n            tokenizer = BertTokenizer.from_pretrained('./test/saved_model/')\n\n            # If the tokenizer uses a single vocabulary file, you can point directly to this file\n            tokenizer = BertTokenizer.from_pretrained('./test/saved_model/my_vocab.txt')\n\n            # You can link tokens to special vocabulary when instantiating\n            tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', unk_token='<unk>')\n            # You should be sure '<unk>' is in the vocabulary when doing that.\n            # Otherwise use tokenizer.add_special_tokens({'unk_token': '<unk>'}) instead)\n            assert tokenizer.unk_token == '<unk>'\n\n        \"\"\"", "\n", "return", "cls", ".", "_from_pretrained", "(", "*", "inputs", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizer._from_pretrained": [[403, 611], ["kwargs.pop", "kwargs.pop", "kwargs.pop", "kwargs.pop", "kwargs.pop", "list", "all", "vocab_files.items", "resolved_vocab_files.pop", "json.load.update", "resolved_vocab_files.pop", "resolved_vocab_files.pop", "resolved_vocab_files.items", "cls.unique_added_tokens_encoder.update", "cls.max_model_input_sizes.keys", "cls.pretrained_vocab_files_map.items", "logger.info", "vocab_files.items", "EnvironmentError", "json.load.pop", "json.load.items", "cls", "set", "cls.added_tokens_encoder.update", "cls.added_tokens_decoder.update", "cls.unique_added_tokens_encoder.update", "cls.pretrained_init_configuration[].copy", "os.path.isfile", "file_utils.is_remote_url", "logger.warning", "EnvironmentError", "logger.info", "logger.info", "open", "json.load", "isinstance", "min", "open", "json.load", "OSError", "open", "json.load", "set", "len", "ValueError", "list", "os.path.isdir", "file_utils.cached_path", "resolved_vocab_files.values", "list", "json.load.get", "json.load.items", "cls.added_tokens_encoder.keys", "cls.vocab_files_names.keys", "os.path.join", "file_utils.hf_bucket_url", "list", "cls.vocab_files_names.values", "int", "os.path.exists", "logger.info", "cls.vocab_files_names.values"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.HMNetTrainer.HMNetTrainer.update", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.HMNetTrainer.HMNetTrainer.update", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.HMNetTrainer.HMNetTrainer.update", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.HMNetTrainer.HMNetTrainer.update", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.HMNetTrainer.HMNetTrainer.update", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.file_utils.is_remote_url", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.BaseTrainer.BaseTrainer.load", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.BaseTrainer.BaseTrainer.load", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.BaseTrainer.BaseTrainer.load", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.file_utils.cached_path", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.closablequeue.ClosableQueue.get", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.file_utils.hf_bucket_url"], ["", "@", "classmethod", "\n", "def", "_from_pretrained", "(", "cls", ",", "pretrained_model_name_or_path", ",", "*", "init_inputs", ",", "**", "kwargs", ")", ":", "\n", "        ", "cache_dir", "=", "kwargs", ".", "pop", "(", "\"cache_dir\"", ",", "None", ")", "\n", "force_download", "=", "kwargs", ".", "pop", "(", "\"force_download\"", ",", "False", ")", "\n", "resume_download", "=", "kwargs", ".", "pop", "(", "\"resume_download\"", ",", "False", ")", "\n", "proxies", "=", "kwargs", ".", "pop", "(", "\"proxies\"", ",", "None", ")", "\n", "local_files_only", "=", "kwargs", ".", "pop", "(", "\"local_files_only\"", ",", "False", ")", "\n", "\n", "s3_models", "=", "list", "(", "cls", ".", "max_model_input_sizes", ".", "keys", "(", ")", ")", "\n", "vocab_files", "=", "{", "}", "\n", "init_configuration", "=", "{", "}", "\n", "if", "pretrained_model_name_or_path", "in", "s3_models", ":", "\n", "# Get the vocabulary from AWS S3 bucket", "\n", "            ", "for", "file_id", ",", "map_list", "in", "cls", ".", "pretrained_vocab_files_map", ".", "items", "(", ")", ":", "\n", "                ", "vocab_files", "[", "file_id", "]", "=", "map_list", "[", "pretrained_model_name_or_path", "]", "\n", "", "if", "(", "\n", "cls", ".", "pretrained_init_configuration", "\n", "and", "pretrained_model_name_or_path", "in", "cls", ".", "pretrained_init_configuration", "\n", ")", ":", "\n", "                ", "init_configuration", "=", "cls", ".", "pretrained_init_configuration", "[", "\n", "pretrained_model_name_or_path", "\n", "]", ".", "copy", "(", ")", "\n", "", "", "else", ":", "\n", "# Get the vocabulary from local files", "\n", "            ", "logger", ".", "info", "(", "\n", "\"Model name '{}' not found in model shortcut name list ({}). \"", "\n", "\"Assuming '{}' is a path, a model identifier, or url to a directory containing tokenizer files.\"", ".", "format", "(", "\n", "pretrained_model_name_or_path", ",", "\n", "\", \"", ".", "join", "(", "s3_models", ")", ",", "\n", "pretrained_model_name_or_path", ",", "\n", ")", "\n", ")", "\n", "\n", "if", "os", ".", "path", ".", "isfile", "(", "pretrained_model_name_or_path", ")", "or", "is_remote_url", "(", "\n", "pretrained_model_name_or_path", "\n", ")", ":", "\n", "                ", "if", "len", "(", "cls", ".", "vocab_files_names", ")", ">", "1", ":", "\n", "                    ", "raise", "ValueError", "(", "\n", "\"Calling {}.from_pretrained() with the path to a single file or url is not supported.\"", "\n", "\"Use a model identifier or the path to a directory instead.\"", ".", "format", "(", "\n", "cls", ".", "__name__", "\n", ")", "\n", ")", "\n", "", "logger", ".", "warning", "(", "\n", "\"Calling {}.from_pretrained() with the path to a single file or url is deprecated\"", ".", "format", "(", "\n", "cls", ".", "__name__", "\n", ")", "\n", ")", "\n", "file_id", "=", "list", "(", "cls", ".", "vocab_files_names", ".", "keys", "(", ")", ")", "[", "0", "]", "\n", "vocab_files", "[", "file_id", "]", "=", "pretrained_model_name_or_path", "\n", "", "else", ":", "\n", "# At this point pretrained_model_name_or_path is either a directory or a model identifier name", "\n", "                ", "additional_files_names", "=", "{", "\n", "\"added_tokens_file\"", ":", "ADDED_TOKENS_FILE", ",", "\n", "\"special_tokens_map_file\"", ":", "SPECIAL_TOKENS_MAP_FILE", ",", "\n", "\"tokenizer_config_file\"", ":", "TOKENIZER_CONFIG_FILE", ",", "\n", "}", "\n", "# Look for the tokenizer main vocabulary files + the additional tokens files", "\n", "for", "file_id", ",", "file_name", "in", "{", "\n", "**", "cls", ".", "vocab_files_names", ",", "\n", "**", "additional_files_names", ",", "\n", "}", ".", "items", "(", ")", ":", "\n", "                    ", "if", "os", ".", "path", ".", "isdir", "(", "pretrained_model_name_or_path", ")", ":", "\n", "                        ", "full_file_name", "=", "os", ".", "path", ".", "join", "(", "\n", "pretrained_model_name_or_path", ",", "file_name", "\n", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "full_file_name", ")", ":", "\n", "                            ", "logger", ".", "info", "(", "\n", "\"Didn't find file {}. We won't load it.\"", ".", "format", "(", "\n", "full_file_name", "\n", ")", "\n", ")", "\n", "full_file_name", "=", "None", "\n", "", "", "else", ":", "\n", "                        ", "full_file_name", "=", "hf_bucket_url", "(", "\n", "pretrained_model_name_or_path", ",", "postfix", "=", "file_name", "\n", ")", "\n", "\n", "", "vocab_files", "[", "file_id", "]", "=", "full_file_name", "\n", "\n", "# Get files from url, cache, or disk depending on the case", "\n", "", "", "", "try", ":", "\n", "            ", "resolved_vocab_files", "=", "{", "}", "\n", "for", "file_id", ",", "file_path", "in", "vocab_files", ".", "items", "(", ")", ":", "\n", "                ", "if", "file_path", "is", "None", ":", "\n", "                    ", "resolved_vocab_files", "[", "file_id", "]", "=", "None", "\n", "", "else", ":", "\n", "                    ", "resolved_vocab_files", "[", "file_id", "]", "=", "cached_path", "(", "\n", "file_path", ",", "\n", "cache_dir", "=", "cache_dir", ",", "\n", "force_download", "=", "force_download", ",", "\n", "proxies", "=", "proxies", ",", "\n", "resume_download", "=", "resume_download", ",", "\n", "local_files_only", "=", "local_files_only", ",", "\n", ")", "\n", "", "", "", "except", "EnvironmentError", ":", "\n", "            ", "if", "pretrained_model_name_or_path", "in", "s3_models", ":", "\n", "                ", "msg", "=", "\"Couldn't reach server at '{}' to download vocabulary files.\"", "\n", "", "else", ":", "\n", "                ", "msg", "=", "(", "\n", "\"Model name '{}' was not found in tokenizers model name list ({}). \"", "\n", "\"We assumed '{}' was a path or url to a directory containing vocabulary files \"", "\n", "\"named {}, but couldn't find such vocabulary files at this path or url.\"", ".", "format", "(", "\n", "pretrained_model_name_or_path", ",", "\n", "\", \"", ".", "join", "(", "s3_models", ")", ",", "\n", "pretrained_model_name_or_path", ",", "\n", "list", "(", "cls", ".", "vocab_files_names", ".", "values", "(", ")", ")", ",", "\n", ")", "\n", ")", "\n", "\n", "", "raise", "EnvironmentError", "(", "msg", ")", "\n", "\n", "", "if", "all", "(", "\n", "full_file_name", "is", "None", "for", "full_file_name", "in", "resolved_vocab_files", ".", "values", "(", ")", "\n", ")", ":", "\n", "            ", "raise", "EnvironmentError", "(", "\n", "\"Model name '{}' was not found in tokenizers model name list ({}). \"", "\n", "\"We assumed '{}' was a path, a model identifier, or url to a directory containing vocabulary files \"", "\n", "\"named {} but couldn't find such vocabulary files at this path or url.\"", ".", "format", "(", "\n", "pretrained_model_name_or_path", ",", "\n", "\", \"", ".", "join", "(", "s3_models", ")", ",", "\n", "pretrained_model_name_or_path", ",", "\n", "list", "(", "cls", ".", "vocab_files_names", ".", "values", "(", ")", ")", ",", "\n", ")", "\n", ")", "\n", "\n", "", "for", "file_id", ",", "file_path", "in", "vocab_files", ".", "items", "(", ")", ":", "\n", "            ", "if", "file_path", "==", "resolved_vocab_files", "[", "file_id", "]", ":", "\n", "                ", "logger", ".", "info", "(", "\"loading file {}\"", ".", "format", "(", "file_path", ")", ")", "\n", "", "else", ":", "\n", "                ", "logger", ".", "info", "(", "\n", "\"loading file {} from cache at {}\"", ".", "format", "(", "\n", "file_path", ",", "resolved_vocab_files", "[", "file_id", "]", "\n", ")", "\n", ")", "\n", "\n", "# Prepare tokenizer initialization kwargs", "\n", "# Did we saved some inputs and kwargs to reload ?", "\n", "", "", "tokenizer_config_file", "=", "resolved_vocab_files", ".", "pop", "(", "\"tokenizer_config_file\"", ",", "None", ")", "\n", "if", "tokenizer_config_file", "is", "not", "None", ":", "\n", "            ", "with", "open", "(", "\n", "tokenizer_config_file", ",", "encoding", "=", "\"utf-8\"", "\n", ")", "as", "tokenizer_config_handle", ":", "\n", "                ", "init_kwargs", "=", "json", ".", "load", "(", "tokenizer_config_handle", ")", "\n", "", "saved_init_inputs", "=", "init_kwargs", ".", "pop", "(", "\"init_inputs\"", ",", "(", ")", ")", "\n", "if", "not", "init_inputs", ":", "\n", "                ", "init_inputs", "=", "saved_init_inputs", "\n", "", "", "else", ":", "\n", "            ", "init_kwargs", "=", "init_configuration", "\n", "\n", "# Update with newly provided kwargs", "\n", "", "init_kwargs", ".", "update", "(", "kwargs", ")", "\n", "\n", "# Set max length if needed", "\n", "if", "pretrained_model_name_or_path", "in", "cls", ".", "max_model_input_sizes", ":", "\n", "# if we're using a pretrained model, ensure the tokenizer", "\n", "# wont index sequences longer than the number of positional embeddings", "\n", "            ", "max_len", "=", "cls", ".", "max_model_input_sizes", "[", "pretrained_model_name_or_path", "]", "\n", "if", "max_len", "is", "not", "None", "and", "isinstance", "(", "max_len", ",", "(", "int", ",", "float", ")", ")", ":", "\n", "                ", "init_kwargs", "[", "\"max_len\"", "]", "=", "min", "(", "\n", "init_kwargs", ".", "get", "(", "\"max_len\"", ",", "int", "(", "1e12", ")", ")", ",", "max_len", "\n", ")", "\n", "\n", "# Merge resolved_vocab_files arguments in init_kwargs.", "\n", "", "", "added_tokens_file", "=", "resolved_vocab_files", ".", "pop", "(", "\"added_tokens_file\"", ",", "None", ")", "\n", "special_tokens_map_file", "=", "resolved_vocab_files", ".", "pop", "(", "\n", "\"special_tokens_map_file\"", ",", "None", "\n", ")", "\n", "for", "args_name", ",", "file_path", "in", "resolved_vocab_files", ".", "items", "(", ")", ":", "\n", "            ", "if", "args_name", "not", "in", "init_kwargs", ":", "\n", "                ", "init_kwargs", "[", "args_name", "]", "=", "file_path", "\n", "", "", "if", "special_tokens_map_file", "is", "not", "None", ":", "\n", "            ", "with", "open", "(", "\n", "special_tokens_map_file", ",", "encoding", "=", "\"utf-8\"", "\n", ")", "as", "special_tokens_map_handle", ":", "\n", "                ", "special_tokens_map", "=", "json", ".", "load", "(", "special_tokens_map_handle", ")", "\n", "", "for", "key", ",", "value", "in", "special_tokens_map", ".", "items", "(", ")", ":", "\n", "                ", "if", "key", "not", "in", "init_kwargs", ":", "\n", "                    ", "init_kwargs", "[", "key", "]", "=", "value", "\n", "\n", "# Instantiate tokenizer.", "\n", "", "", "", "try", ":", "\n", "            ", "tokenizer", "=", "cls", "(", "*", "init_inputs", ",", "**", "init_kwargs", ")", "\n", "", "except", "OSError", ":", "\n", "            ", "raise", "OSError", "(", "\n", "\"Unable to load vocabulary from file. \"", "\n", "\"Please check that the provided vocabulary is accessible and not corrupted.\"", "\n", ")", "\n", "\n", "# Save inputs and kwargs for saving and re-loading with ``save_pretrained``", "\n", "", "tokenizer", ".", "init_inputs", "=", "init_inputs", "\n", "tokenizer", ".", "init_kwargs", "=", "init_kwargs", "\n", "\n", "# update unique_added_tokens_encoder with special tokens for correct tokenization", "\n", "tokenizer", ".", "unique_added_tokens_encoder", ".", "update", "(", "set", "(", "tokenizer", ".", "all_special_tokens", ")", ")", "\n", "\n", "# Add supplementary tokens.", "\n", "if", "added_tokens_file", "is", "not", "None", ":", "\n", "            ", "with", "open", "(", "added_tokens_file", ",", "encoding", "=", "\"utf-8\"", ")", "as", "added_tokens_handle", ":", "\n", "                ", "added_tok_encoder", "=", "json", ".", "load", "(", "added_tokens_handle", ")", "\n", "", "added_tok_decoder", "=", "{", "v", ":", "k", "for", "k", ",", "v", "in", "added_tok_encoder", ".", "items", "(", ")", "}", "\n", "tokenizer", ".", "added_tokens_encoder", ".", "update", "(", "added_tok_encoder", ")", "\n", "tokenizer", ".", "added_tokens_decoder", ".", "update", "(", "added_tok_decoder", ")", "\n", "tokenizer", ".", "unique_added_tokens_encoder", ".", "update", "(", "\n", "set", "(", "tokenizer", ".", "added_tokens_encoder", ".", "keys", "(", ")", ")", "\n", ")", "\n", "\n", "", "return", "tokenizer", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizer.save_pretrained": [[612, 653], ["os.path.join", "os.path.join", "os.path.join", "copy.deepcopy", "tokenization_utils.PreTrainedTokenizer.vocab_files_names.keys", "tokenization_utils.PreTrainedTokenizer.save_vocabulary", "os.path.isdir", "logger.error", "len", "copy.deepcopy", "copy.deepcopy.pop", "open", "f.write", "open", "f.write", "len", "json.dumps", "json.dumps", "open", "json.dumps", "f.write"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizerFast.save_vocabulary"], ["", "def", "save_pretrained", "(", "self", ",", "save_directory", ")", ":", "\n", "        ", "\"\"\"Save the tokenizer vocabulary files together with:\n            - added tokens,\n            - special-tokens-to-class-attributes-mapping,\n            - tokenizer instantiation positional and keywords inputs (e.g. do_lower_case for Bert).\n\n        This won't save modifications other than (added tokens and special token mapping) you may have\n        applied to the tokenizer after the instantiation (e.g. modifying tokenizer.do_lower_case after creation).\n\n        This method make sure the full tokenizer can then be re-loaded using the :func:`~transformers.PreTrainedTokenizer.from_pretrained` class method.\n        \"\"\"", "\n", "if", "not", "os", ".", "path", ".", "isdir", "(", "save_directory", ")", ":", "\n", "            ", "logger", ".", "error", "(", "\n", "\"Saving directory ({}) should be a directory\"", ".", "format", "(", "save_directory", ")", "\n", ")", "\n", "return", "\n", "\n", "", "special_tokens_map_file", "=", "os", ".", "path", ".", "join", "(", "save_directory", ",", "SPECIAL_TOKENS_MAP_FILE", ")", "\n", "added_tokens_file", "=", "os", ".", "path", ".", "join", "(", "save_directory", ",", "ADDED_TOKENS_FILE", ")", "\n", "tokenizer_config_file", "=", "os", ".", "path", ".", "join", "(", "save_directory", ",", "TOKENIZER_CONFIG_FILE", ")", "\n", "\n", "tokenizer_config", "=", "copy", ".", "deepcopy", "(", "self", ".", "init_kwargs", ")", "\n", "if", "len", "(", "self", ".", "init_inputs", ")", ">", "0", ":", "\n", "            ", "tokenizer_config", "[", "\"init_inputs\"", "]", "=", "copy", ".", "deepcopy", "(", "self", ".", "init_inputs", ")", "\n", "", "for", "file_id", "in", "self", ".", "vocab_files_names", ".", "keys", "(", ")", ":", "\n", "            ", "tokenizer_config", ".", "pop", "(", "file_id", ",", "None", ")", "\n", "\n", "", "with", "open", "(", "tokenizer_config_file", ",", "\"w\"", ",", "encoding", "=", "\"utf-8\"", ")", "as", "f", ":", "\n", "            ", "f", ".", "write", "(", "json", ".", "dumps", "(", "tokenizer_config", ",", "ensure_ascii", "=", "False", ")", ")", "\n", "\n", "", "with", "open", "(", "special_tokens_map_file", ",", "\"w\"", ",", "encoding", "=", "\"utf-8\"", ")", "as", "f", ":", "\n", "            ", "f", ".", "write", "(", "json", ".", "dumps", "(", "self", ".", "special_tokens_map", ",", "ensure_ascii", "=", "False", ")", ")", "\n", "\n", "", "if", "len", "(", "self", ".", "added_tokens_encoder", ")", ">", "0", ":", "\n", "            ", "with", "open", "(", "added_tokens_file", ",", "\"w\"", ",", "encoding", "=", "\"utf-8\"", ")", "as", "f", ":", "\n", "                ", "out_str", "=", "json", ".", "dumps", "(", "self", ".", "added_tokens_encoder", ",", "ensure_ascii", "=", "False", ")", "\n", "f", ".", "write", "(", "out_str", ")", "\n", "\n", "", "", "vocab_files", "=", "self", ".", "save_vocabulary", "(", "save_directory", ")", "\n", "\n", "return", "vocab_files", "+", "(", "special_tokens_map_file", ",", "added_tokens_file", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizer.save_vocabulary": [[654, 661], ["None"], "methods", ["None"], ["", "def", "save_vocabulary", "(", "self", ",", "save_directory", ")", ":", "\n", "        ", "\"\"\"Save the tokenizer vocabulary to a directory. This method does *NOT* save added tokens\n        and special token mappings.\n\n        Please use :func:`~transformers.PreTrainedTokenizer.save_pretrained` `()` to save the full Tokenizer state if you want to reload it using the :func:`~transformers.PreTrainedTokenizer.from_pretrained` class method.\n        \"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizer.vocab_size": [[662, 665], ["None"], "methods", ["None"], ["", "def", "vocab_size", "(", "self", ")", ":", "\n", "        ", "\"\"\"Size of the base vocabulary (without the added tokens)\"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizer.__len__": [[666, 669], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "\"\"\"Size of the full vocabulary with the added tokens\"\"\"", "\n", "return", "self", ".", "vocab_size", "+", "len", "(", "self", ".", "added_tokens_encoder", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizer.add_tokens": [[670, 725], ["dict", "tokenization_utils.PreTrainedTokenizer.added_tokens_encoder.update", "set().union", "tokenization_utils.PreTrainedTokenizer.added_tokens_decoder.update", "len", "isinstance", "isinstance", "set", "tokenization_utils.PreTrainedTokenizer.init_kwargs.get", "token.lower.lower.lower", "to_add_tokens.append", "logger.info", "dict.items", "set", "tokenization_utils.PreTrainedTokenizer.convert_tokens_to_ids", "tokenization_utils.PreTrainedTokenizer.convert_tokens_to_ids", "enumerate", "tokenization_utils.PreTrainedTokenizer.added_tokens_encoder.keys", "len"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.HMNetTrainer.HMNetTrainer.update", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.HMNetTrainer.HMNetTrainer.update", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.closablequeue.ClosableQueue.get", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizer.convert_tokens_to_ids", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizer.convert_tokens_to_ids"], ["", "def", "add_tokens", "(", "self", ",", "new_tokens", ")", ":", "\n", "        ", "\"\"\"\n        Add a list of new tokens to the tokenizer class. If the new tokens are not in the\n        vocabulary, they are added to it with indices starting from length of the current vocabulary.\n\n        Args:\n            new_tokens: string or list of string. Each string is a token to add. Tokens are only added if they are not already in the vocabulary (tested by checking if the tokenizer assign the index of the ``unk_token`` to them).\n\n        Returns:\n            Number of tokens added to the vocabulary.\n\n        Examples::\n\n            # Let's see how to increase the vocabulary of Bert model and tokenizer\n            tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n            model = BertModel.from_pretrained('bert-base-uncased')\n\n            num_added_toks = tokenizer.add_tokens(['new_tok1', 'my_new-tok2'])\n            print('We have added', num_added_toks, 'tokens')\n            model.resize_token_embeddings(len(tokenizer))  # Notice: resize_token_embeddings expect to receive the full size of the new vocabulary, i.e. the length of the tokenizer.\n        \"\"\"", "\n", "if", "not", "new_tokens", ":", "\n", "            ", "return", "0", "\n", "\n", "", "if", "not", "isinstance", "(", "new_tokens", ",", "list", ")", ":", "\n", "            ", "new_tokens", "=", "[", "new_tokens", "]", "\n", "\n", "", "to_add_tokens", "=", "[", "]", "\n", "for", "token", "in", "new_tokens", ":", "\n", "            ", "assert", "isinstance", "(", "token", ",", "str", ")", "\n", "if", "(", "\n", "self", ".", "init_kwargs", ".", "get", "(", "\"do_lower_case\"", ",", "False", ")", "\n", "and", "token", "not", "in", "self", ".", "all_special_tokens", "\n", ")", ":", "\n", "                ", "token", "=", "token", ".", "lower", "(", ")", "\n", "", "if", "(", "\n", "token", "!=", "self", ".", "unk_token", "\n", "and", "self", ".", "convert_tokens_to_ids", "(", "token", ")", "\n", "==", "self", ".", "convert_tokens_to_ids", "(", "self", ".", "unk_token", ")", "\n", "and", "token", "not", "in", "to_add_tokens", "\n", ")", ":", "\n", "                ", "to_add_tokens", ".", "append", "(", "token", ")", "\n", "logger", ".", "info", "(", "\"Adding %s to the vocabulary\"", ",", "token", ")", "\n", "\n", "", "", "added_tok_encoder", "=", "dict", "(", "\n", "(", "tok", ",", "len", "(", "self", ")", "+", "i", ")", "for", "i", ",", "tok", "in", "enumerate", "(", "to_add_tokens", ")", "\n", ")", "\n", "added_tok_decoder", "=", "{", "v", ":", "k", "for", "k", ",", "v", "in", "added_tok_encoder", ".", "items", "(", ")", "}", "\n", "self", ".", "added_tokens_encoder", ".", "update", "(", "added_tok_encoder", ")", "\n", "self", ".", "unique_added_tokens_encoder", "=", "set", "(", "self", ".", "added_tokens_encoder", ".", "keys", "(", ")", ")", ".", "union", "(", "\n", "set", "(", "self", ".", "all_special_tokens", ")", "\n", ")", "\n", "self", ".", "added_tokens_decoder", ".", "update", "(", "added_tok_decoder", ")", "\n", "\n", "return", "len", "(", "to_add_tokens", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizer.num_added_tokens": [[726, 746], ["len", "tokenization_utils.PreTrainedTokenizer.build_inputs_with_special_tokens"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizerFast.build_inputs_with_special_tokens"], ["", "def", "num_added_tokens", "(", "self", ",", "pair", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        Returns the number of added tokens when encoding a sequence with special tokens.\n\n        Note:\n            This encodes inputs and checks the number of added tokens, and is therefore not efficient. Do not put this\n            inside your training loop.\n\n        Args:\n            pair: Returns the number of added tokens in the case of a sequence pair if set to True, returns the\n                number of added tokens in the case of a single sequence if set to False.\n\n        Returns:\n            Number of tokens added to sequences\n        \"\"\"", "\n", "token_ids_0", "=", "[", "]", "\n", "token_ids_1", "=", "[", "]", "\n", "return", "len", "(", "\n", "self", ".", "build_inputs_with_special_tokens", "(", "\n", "token_ids_0", ",", "token_ids_1", "if", "pair", "else", "None", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizer.add_special_tokens": [[749, 804], ["special_tokens_dict.items", "logger.info", "setattr", "tokenization_utils.PreTrainedTokenizer.add_tokens", "isinstance", "tokenization_utils.PreTrainedTokenizer.add_tokens", "isinstance", "all", "isinstance"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizerFast.add_tokens", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizerFast.add_tokens"], ["", "def", "add_special_tokens", "(", "self", ",", "special_tokens_dict", ")", ":", "\n", "        ", "\"\"\"\n        Add a dictionary of special tokens (eos, pad, cls...) to the encoder and link them\n        to class attributes. If special tokens are NOT in the vocabulary, they are added\n        to it (indexed starting from the last index of the current vocabulary).\n\n        Using `add_special_tokens` will ensure your special tokens can be used in several ways:\n\n        - special tokens are carefully handled by the tokenizer (they are never split)\n        - you can easily refer to special tokens using tokenizer class attributes like `tokenizer.cls_token`. This makes it easy to develop model-agnostic training and fine-tuning scripts.\n\n        When possible, special tokens are already registered for provided pretrained models (ex: BertTokenizer cls_token is already registered to be '[CLS]' and XLM's one is also registered to be '</s>')\n\n        Args:\n            special_tokens_dict: dict of string. Keys should be in the list of predefined special attributes:\n                [``bos_token``, ``eos_token``, ``unk_token``, ``sep_token``, ``pad_token``, ``cls_token``, ``mask_token``,\n                ``additional_special_tokens``].\n\n                Tokens are only added if they are not already in the vocabulary (tested by checking if the tokenizer assign the index of the ``unk_token`` to them).\n\n        Returns:\n            Number of tokens added to the vocabulary.\n\n        Examples::\n\n            # Let's see how to add a new classification token to GPT-2\n            tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n            model = GPT2Model.from_pretrained('gpt2')\n\n            special_tokens_dict = {'cls_token': '<CLS>'}\n\n            num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n            print('We have added', num_added_toks, 'tokens')\n            model.resize_token_embeddings(len(tokenizer))  # Notice: resize_token_embeddings expect to receive the full size of the new vocabulary, i.e. the length of the tokenizer.\n\n            assert tokenizer.cls_token == '<CLS>'\n        \"\"\"", "\n", "if", "not", "special_tokens_dict", ":", "\n", "            ", "return", "0", "\n", "\n", "", "added_tokens", "=", "0", "\n", "for", "key", ",", "value", "in", "special_tokens_dict", ".", "items", "(", ")", ":", "\n", "            ", "assert", "key", "in", "self", ".", "SPECIAL_TOKENS_ATTRIBUTES", "\n", "if", "key", "==", "\"additional_special_tokens\"", ":", "\n", "                ", "assert", "isinstance", "(", "value", ",", "(", "list", ",", "tuple", ")", ")", "and", "all", "(", "\n", "isinstance", "(", "t", ",", "str", ")", "for", "t", "in", "value", "\n", ")", "\n", "added_tokens", "+=", "self", ".", "add_tokens", "(", "value", ")", "\n", "", "else", ":", "\n", "                ", "assert", "isinstance", "(", "value", ",", "str", ")", "\n", "added_tokens", "+=", "self", ".", "add_tokens", "(", "[", "value", "]", ")", "\n", "", "logger", ".", "info", "(", "\"Assigning %s to the %s key of the tokenizer\"", ",", "value", ",", "key", ")", "\n", "setattr", "(", "self", ",", "key", ",", "value", ")", "\n", "\n", "", "return", "added_tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizer.tokenize": [[805, 878], ["tokenization_utils.PreTrainedTokenizer.prepare_for_tokenization", "tokenization_utils.PreTrainedTokenizer.init_kwargs.get", "tokenization_utils.PreTrainedTokenizer.tokenize.split_on_tokens"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizer.prepare_for_tokenization", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.closablequeue.ClosableQueue.get"], ["", "def", "tokenize", "(", "self", ",", "text", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"Converts a string in a sequence of tokens (string), using the tokenizer.\n        Split in words for word-based vocabulary or sub-words for sub-word-based\n        vocabularies (BPE/SentencePieces/WordPieces).\n\n        Take care of added tokens.\n\n        text: The sequence to be encoded.\n        add_prefix_space: Only applies to GPT-2 and RoBERTa tokenizers. When `True`, this ensures that the sequence\n            begins with an empty space. False by default except for when using RoBERTa with `add_special_tokens=True`.\n        **kwargs: passed to the `prepare_for_tokenization` preprocessing method.\n        \"\"\"", "\n", "all_special_tokens", "=", "self", ".", "all_special_tokens", "\n", "text", "=", "self", ".", "prepare_for_tokenization", "(", "text", ",", "**", "kwargs", ")", "\n", "\n", "def", "lowercase_text", "(", "t", ")", ":", "\n", "# convert non-special tokens to lowercase", "\n", "            ", "escaped_special_toks", "=", "[", "re", ".", "escape", "(", "s_tok", ")", "for", "s_tok", "in", "all_special_tokens", "]", "\n", "pattern", "=", "r\"(\"", "+", "r\"|\"", ".", "join", "(", "escaped_special_toks", ")", "+", "r\")|\"", "+", "r\"(.+?)\"", "\n", "return", "re", ".", "sub", "(", "pattern", ",", "lambda", "m", ":", "m", ".", "groups", "(", ")", "[", "0", "]", "or", "m", ".", "groups", "(", ")", "[", "1", "]", ".", "lower", "(", ")", ",", "t", ")", "\n", "\n", "", "if", "self", ".", "init_kwargs", ".", "get", "(", "\"do_lower_case\"", ",", "False", ")", ":", "\n", "            ", "text", "=", "lowercase_text", "(", "text", ")", "\n", "\n", "", "def", "split_on_token", "(", "tok", ",", "text", ")", ":", "\n", "            ", "result", "=", "[", "]", "\n", "split_text", "=", "text", ".", "split", "(", "tok", ")", "\n", "for", "i", ",", "sub_text", "in", "enumerate", "(", "split_text", ")", ":", "\n", "                ", "sub_text", "=", "sub_text", ".", "rstrip", "(", ")", "\n", "if", "i", "==", "0", "and", "not", "sub_text", ":", "\n", "                    ", "result", "+=", "[", "tok", "]", "\n", "", "elif", "i", "==", "len", "(", "split_text", ")", "-", "1", ":", "\n", "                    ", "if", "sub_text", ":", "\n", "                        ", "result", "+=", "[", "sub_text", "]", "\n", "", "else", ":", "\n", "                        ", "pass", "\n", "", "", "else", ":", "\n", "                    ", "if", "sub_text", ":", "\n", "                        ", "result", "+=", "[", "sub_text", "]", "\n", "", "result", "+=", "[", "tok", "]", "\n", "", "", "return", "result", "\n", "\n", "", "def", "split_on_tokens", "(", "tok_list", ",", "text", ")", ":", "\n", "            ", "if", "not", "text", ".", "strip", "(", ")", ":", "\n", "                ", "return", "[", "]", "\n", "", "if", "not", "tok_list", ":", "\n", "                ", "return", "self", ".", "_tokenize", "(", "text", ")", "\n", "\n", "", "tokenized_text", "=", "[", "]", "\n", "text_list", "=", "[", "text", "]", "\n", "for", "tok", "in", "tok_list", ":", "\n", "                ", "tokenized_text", "=", "[", "]", "\n", "for", "sub_text", "in", "text_list", ":", "\n", "                    ", "if", "sub_text", "not", "in", "self", ".", "unique_added_tokens_encoder", ":", "\n", "                        ", "tokenized_text", "+=", "split_on_token", "(", "tok", ",", "sub_text", ")", "\n", "", "else", ":", "\n", "                        ", "tokenized_text", "+=", "[", "sub_text", "]", "\n", "", "", "text_list", "=", "tokenized_text", "\n", "\n", "", "return", "list", "(", "\n", "itertools", ".", "chain", ".", "from_iterable", "(", "\n", "(", "\n", "self", ".", "_tokenize", "(", "token", ")", "\n", "if", "token", "not", "in", "self", ".", "unique_added_tokens_encoder", "\n", "else", "[", "token", "]", "\n", "for", "token", "in", "tokenized_text", "\n", ")", "\n", ")", "\n", ")", "\n", "\n", "", "added_tokens", "=", "self", ".", "unique_added_tokens_encoder", "\n", "tokenized_text", "=", "split_on_tokens", "(", "added_tokens", ",", "text", ")", "\n", "return", "tokenized_text", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizer._tokenize": [[879, 887], ["None"], "methods", ["None"], ["", "def", "_tokenize", "(", "self", ",", "text", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"Converts a string in a sequence of tokens (string), using the tokenizer.\n        Split in words for word-based vocabulary or sub-words for sub-word-based\n        vocabularies (BPE/SentencePieces/WordPieces).\n\n        Do NOT take care of added tokens.\n        \"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizer.convert_tokens_to_ids": [[888, 902], ["isinstance", "tokenization_utils.PreTrainedTokenizer._convert_token_to_id_with_added_voc", "ids.append", "tokenization_utils.PreTrainedTokenizer._convert_token_to_id_with_added_voc"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizerFast._convert_token_to_id_with_added_voc", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizerFast._convert_token_to_id_with_added_voc"], ["", "def", "convert_tokens_to_ids", "(", "self", ",", "tokens", ")", ":", "\n", "        ", "\"\"\"Converts a single token, or a sequence of tokens, (str) in a single integer id\n        (resp. a sequence of ids), using the vocabulary.\n        \"\"\"", "\n", "if", "tokens", "is", "None", ":", "\n", "            ", "return", "None", "\n", "\n", "", "if", "isinstance", "(", "tokens", ",", "str", ")", ":", "\n", "            ", "return", "self", ".", "_convert_token_to_id_with_added_voc", "(", "tokens", ")", "\n", "\n", "", "ids", "=", "[", "]", "\n", "for", "token", "in", "tokens", ":", "\n", "            ", "ids", ".", "append", "(", "self", ".", "_convert_token_to_id_with_added_voc", "(", "token", ")", ")", "\n", "", "return", "ids", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizer._convert_token_to_id_with_added_voc": [[903, 910], ["tokenization_utils.PreTrainedTokenizer._convert_token_to_id"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizer._convert_token_to_id"], ["", "def", "_convert_token_to_id_with_added_voc", "(", "self", ",", "token", ")", ":", "\n", "        ", "if", "token", "is", "None", ":", "\n", "            ", "return", "None", "\n", "\n", "", "if", "token", "in", "self", ".", "added_tokens_encoder", ":", "\n", "            ", "return", "self", ".", "added_tokens_encoder", "[", "token", "]", "\n", "", "return", "self", ".", "_convert_token_to_id", "(", "token", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizer._convert_token_to_id": [[911, 913], ["None"], "methods", ["None"], ["", "def", "_convert_token_to_id", "(", "self", ",", "token", ")", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizer.encode": [[914, 984], ["tokenization_utils.PreTrainedTokenizer.encode_plus"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizerFast.encode_plus"], ["", "def", "encode", "(", "\n", "self", ",", "\n", "text", ":", "str", ",", "\n", "text_pair", ":", "Optional", "[", "str", "]", "=", "None", ",", "\n", "add_special_tokens", ":", "bool", "=", "True", ",", "\n", "max_length", ":", "Optional", "[", "int", "]", "=", "None", ",", "\n", "stride", ":", "int", "=", "0", ",", "\n", "truncation_strategy", ":", "str", "=", "\"longest_first\"", ",", "\n", "pad_to_max_length", ":", "bool", "=", "False", ",", "\n", "return_tensors", ":", "Optional", "[", "str", "]", "=", "None", ",", "\n", "**", "kwargs", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Converts a string in a sequence of ids (integer), using the tokenizer and vocabulary.\n\n        Same as doing ``self.convert_tokens_to_ids(self.tokenize(text))``.\n\n        Args:\n            text (:obj:`str` or :obj:`List[str]`):\n                The first sequence to be encoded. This can be a string, a list of strings (tokenized string using\n                the `tokenize` method) or a list of integers (tokenized string ids using the `convert_tokens_to_ids`\n                method)\n            text_pair (:obj:`str` or :obj:`List[str]`, `optional`, defaults to :obj:`None`):\n                Optional second sequence to be encoded. This can be a string, a list of strings (tokenized\n                string using the `tokenize` method) or a list of integers (tokenized string ids using the\n                `convert_tokens_to_ids` method)\n            add_special_tokens (:obj:`bool`, `optional`, defaults to :obj:`True`):\n                If set to ``True``, the sequences will be encoded with the special tokens relative\n                to their model.\n            max_length (:obj:`int`, `optional`, defaults to :obj:`None`):\n                If set to a number, will limit the total sequence returned so that it has a maximum length.\n                If there are overflowing tokens, those will be added to the returned dictionary\n            stride (:obj:`int`, `optional`, defaults to ``0``):\n                If set to a number along with max_length, the overflowing tokens returned will contain some tokens\n                from the main sequence returned. The value of this argument defines the number of additional tokens.\n            truncation_strategy (:obj:`str`, `optional`, defaults to `longest_first`):\n                String selected in the following options:\n\n                - 'longest_first' (default) Iteratively reduce the inputs sequence until the input is under max_length\n                  starting from the longest one at each token (when there is a pair of input sequences)\n                - 'only_first': Only truncate the first sequence\n                - 'only_second': Only truncate the second sequence\n                - 'do_not_truncate': Does not truncate (raise an error if the input sequence is longer than max_length)\n            pad_to_max_length (:obj:`bool`, `optional`, defaults to :obj:`False`):\n                If set to True, the returned sequences will be padded according to the model's padding side and\n                padding index, up to their max length. If no max length is specified, the padding is done up to the\n                model's max length. The tokenizer padding sides are handled by the class attribute `padding_side`\n                which can be set to the following strings:\n\n                - 'left': pads on the left of the sequences\n                - 'right': pads on the right of the sequences\n                Defaults to False: no padding.\n            return_tensors (:obj:`str`, `optional`, defaults to :obj:`None`):\n                Can be set to 'tf' or 'pt' to return respectively TensorFlow :obj:`tf.constant`\n                or PyTorch :obj:`torch.Tensor` instead of a list of python integers.\n            **kwargs: passed to the `self.tokenize()` method\n        \"\"\"", "\n", "encoded_inputs", "=", "self", ".", "encode_plus", "(", "\n", "text", ",", "\n", "text_pair", "=", "text_pair", ",", "\n", "max_length", "=", "max_length", ",", "\n", "add_special_tokens", "=", "add_special_tokens", ",", "\n", "stride", "=", "stride", ",", "\n", "truncation_strategy", "=", "truncation_strategy", ",", "\n", "pad_to_max_length", "=", "pad_to_max_length", ",", "\n", "return_tensors", "=", "return_tensors", ",", "\n", "**", "kwargs", ",", "\n", ")", "\n", "\n", "return", "encoded_inputs", "[", "\"input_ids\"", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizer.encode_plus": [[985, 1141], ["tokenization_utils.PreTrainedTokenizer.encode_plus.get_input_ids"], "methods", ["None"], ["", "def", "encode_plus", "(", "\n", "self", ",", "\n", "text", ":", "str", ",", "\n", "text_pair", ":", "Optional", "[", "str", "]", "=", "None", ",", "\n", "add_special_tokens", ":", "bool", "=", "True", ",", "\n", "max_length", ":", "Optional", "[", "int", "]", "=", "None", ",", "\n", "stride", ":", "int", "=", "0", ",", "\n", "truncation_strategy", ":", "str", "=", "\"longest_first\"", ",", "\n", "pad_to_max_length", ":", "bool", "=", "False", ",", "\n", "return_tensors", ":", "Optional", "[", "str", "]", "=", "None", ",", "\n", "return_token_type_ids", ":", "Optional", "[", "bool", "]", "=", "None", ",", "\n", "return_attention_mask", ":", "Optional", "[", "bool", "]", "=", "None", ",", "\n", "return_overflowing_tokens", ":", "bool", "=", "False", ",", "\n", "return_special_tokens_mask", ":", "bool", "=", "False", ",", "\n", "return_offsets_mapping", ":", "bool", "=", "False", ",", "\n", "**", "kwargs", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Returns a dictionary containing the encoded sequence or sequence pair and additional information:\n        the mask for sequence classification and the overflowing elements if a ``max_length`` is specified.\n\n        Args:\n            text (:obj:`str` or :obj:`List[str]`):\n                The first sequence to be encoded. This can be a string, a list of strings (tokenized string using\n                the `tokenize` method) or a list of integers (tokenized string ids using the `convert_tokens_to_ids`\n                method)\n            text_pair (:obj:`str` or :obj:`List[str]`, `optional`, defaults to :obj:`None`):\n                Optional second sequence to be encoded. This can be a string, a list of strings (tokenized\n                string using the `tokenize` method) or a list of integers (tokenized string ids using the\n                `convert_tokens_to_ids` method)\n            add_special_tokens (:obj:`bool`, `optional`, defaults to :obj:`True`):\n                If set to ``True``, the sequences will be encoded with the special tokens relative\n                to their model.\n            max_length (:obj:`int`, `optional`, defaults to :obj:`None`):\n                If set to a number, will limit the total sequence returned so that it has a maximum length.\n                If there are overflowing tokens, those will be added to the returned dictionary\n            stride (:obj:`int`, `optional`, defaults to ``0``):\n                If set to a number along with max_length, the overflowing tokens returned will contain some tokens\n                from the main sequence returned. The value of this argument defines the number of additional tokens.\n            truncation_strategy (:obj:`str`, `optional`, defaults to `longest_first`):\n                String selected in the following options:\n\n                - 'longest_first' (default) Iteratively reduce the inputs sequence until the input is under max_length\n                  starting from the longest one at each token (when there is a pair of input sequences)\n                - 'only_first': Only truncate the first sequence\n                - 'only_second': Only truncate the second sequence\n                - 'do_not_truncate': Does not truncate (raise an error if the input sequence is longer than max_length)\n            pad_to_max_length (:obj:`bool`, `optional`, defaults to :obj:`False`):\n                If set to True, the returned sequences will be padded according to the model's padding side and\n                padding index, up to their max length. If no max length is specified, the padding is done up to the\n                model's max length. The tokenizer padding sides are handled by the class attribute `padding_side`\n                which can be set to the following strings:\n\n                - 'left': pads on the left of the sequences\n                - 'right': pads on the right of the sequences\n                Defaults to False: no padding.\n            return_tensors (:obj:`str`, `optional`, defaults to :obj:`None`):\n                Can be set to 'tf' or 'pt' to return respectively TensorFlow :obj:`tf.constant`\n                or PyTorch :obj:`torch.Tensor` instead of a list of python integers.\n            return_token_type_ids (:obj:`bool`, `optional`, defaults to :obj:`None`):\n                Whether to return token type IDs. If left to the default, will return the token type IDs according\n                to the specific tokenizer's default, defined by the :obj:`return_outputs` attribute.\n\n                `What are token type IDs? <../glossary.html#token-type-ids>`_\n            return_attention_mask (:obj:`bool`, `optional`, defaults to :obj:`none`):\n                Whether to return the attention mask. If left to the default, will return the attention mask according\n                to the specific tokenizer's default, defined by the :obj:`return_outputs` attribute.\n\n                `What are attention masks? <../glossary.html#attention-mask>`__\n            return_overflowing_tokens (:obj:`bool`, `optional`, defaults to :obj:`False`):\n                Set to True to return overflowing token information (default False).\n            return_special_tokens_mask (:obj:`bool`, `optional`, defaults to :obj:`False`):\n                Set to True to return special tokens mask information (default False).\n            return_offsets_mapping (:obj:`bool`, `optional`, defaults to :obj:`False`):\n                Set to True to return (char_start, char_end) for each token (default False).\n                If using Python's tokenizer, this method will raise NotImplementedError. This one is only available on\n                Rust-based tokenizers inheriting from PreTrainedTokenizerFast.\n            **kwargs: passed to the `self.tokenize()` method\n\n        Return:\n            A Dictionary of shape::\n\n                {\n                    input_ids: list[int],\n                    token_type_ids: list[int] if return_token_type_ids is True (default)\n                    attention_mask: list[int] if return_attention_mask is True (default)\n                    overflowing_tokens: list[int] if a ``max_length`` is specified and return_overflowing_tokens is True\n                    num_truncated_tokens: int if a ``max_length`` is specified and return_overflowing_tokens is True\n                    special_tokens_mask: list[int] if ``add_special_tokens`` if set to ``True`` and return_special_tokens_mask is True\n                }\n\n            With the fields:\n\n            - ``input_ids``: list of token ids to be fed to a model\n            - ``token_type_ids``: list of token type ids to be fed to a model\n            - ``attention_mask``: list of indices specifying which tokens should be attended to by the model\n            - ``overflowing_tokens``: list of overflowing tokens if a max length is specified.\n            - ``num_truncated_tokens``: number of overflowing tokens a ``max_length`` is specified\n            - ``special_tokens_mask``: if adding special tokens, this is a list of [0, 1], with 0 specifying special added\n              tokens and 1 specifying sequence tokens.\n        \"\"\"", "\n", "\n", "def", "get_input_ids", "(", "text", ")", ":", "\n", "            ", "if", "isinstance", "(", "text", ",", "str", ")", ":", "\n", "                ", "tokens", "=", "self", ".", "tokenize", "(", "\n", "text", ",", "add_special_tokens", "=", "add_special_tokens", ",", "**", "kwargs", "\n", ")", "\n", "return", "self", ".", "convert_tokens_to_ids", "(", "tokens", ")", "\n", "", "elif", "(", "\n", "isinstance", "(", "text", ",", "(", "list", ",", "tuple", ")", ")", "\n", "and", "len", "(", "text", ")", ">", "0", "\n", "and", "isinstance", "(", "text", "[", "0", "]", ",", "str", ")", "\n", ")", ":", "\n", "                ", "return", "self", ".", "convert_tokens_to_ids", "(", "text", ")", "\n", "", "elif", "(", "\n", "isinstance", "(", "text", ",", "(", "list", ",", "tuple", ")", ")", "\n", "and", "len", "(", "text", ")", ">", "0", "\n", "and", "isinstance", "(", "text", "[", "0", "]", ",", "int", ")", "\n", ")", ":", "\n", "                ", "return", "text", "\n", "", "else", ":", "\n", "                ", "raise", "ValueError", "(", "\n", "\"Input is not valid. Should be a string, a list/tuple of strings or a list/tuple of integers.\"", "\n", ")", "\n", "\n", "", "", "if", "return_offsets_mapping", ":", "\n", "            ", "raise", "NotImplementedError", "(", "\n", "\"return_offset_mapping is not available when using Python tokenizers.\"", "\n", "\"To use this feature, change your tokenizer to one deriving from \"", "\n", "\"transformers.PreTrainedTokenizerFast.\"", "\n", "\"More information on available tokenizers at \"", "\n", "\"https://github.com/huggingface/transformers/pull/2674\"", "\n", ")", "\n", "\n", "# Throw an error if we can pad because there is no padding token", "\n", "", "if", "pad_to_max_length", "and", "self", ".", "pad_token_id", "is", "None", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"Unable to set proper padding strategy as the tokenizer does not have a padding token. In this case please set the `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` or add a new pad token via the function add_special_tokens if you want to use a padding strategy\"", "\n", ")", "\n", "\n", "", "first_ids", "=", "get_input_ids", "(", "text", ")", "\n", "second_ids", "=", "get_input_ids", "(", "text_pair", ")", "if", "text_pair", "is", "not", "None", "else", "None", "\n", "\n", "return", "self", ".", "prepare_for_model", "(", "\n", "first_ids", ",", "\n", "pair_ids", "=", "second_ids", ",", "\n", "max_length", "=", "max_length", ",", "\n", "pad_to_max_length", "=", "pad_to_max_length", ",", "\n", "add_special_tokens", "=", "add_special_tokens", ",", "\n", "stride", "=", "stride", ",", "\n", "truncation_strategy", "=", "truncation_strategy", ",", "\n", "return_tensors", "=", "return_tensors", ",", "\n", "return_attention_mask", "=", "return_attention_mask", ",", "\n", "return_token_type_ids", "=", "return_token_type_ids", ",", "\n", "return_overflowing_tokens", "=", "return_overflowing_tokens", ",", "\n", "return_special_tokens_mask", "=", "return_special_tokens_mask", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizer.batch_encode_plus": [[1143, 1362], ["isinstance", "ValueError", "NotImplementedError", "tokenization_utils.PreTrainedTokenizer.encode_plus.get_input_ids"], "methods", ["None"], ["", "def", "batch_encode_plus", "(", "\n", "self", ",", "\n", "batch_text_or_text_pairs", ":", "Union", "[", "str", ",", "List", "[", "str", "]", "]", ",", "\n", "add_special_tokens", ":", "bool", "=", "True", ",", "\n", "max_length", ":", "Optional", "[", "int", "]", "=", "None", ",", "\n", "stride", ":", "int", "=", "0", ",", "\n", "truncation_strategy", ":", "str", "=", "\"longest_first\"", ",", "\n", "pad_to_max_length", ":", "bool", "=", "False", ",", "\n", "return_tensors", ":", "Optional", "[", "str", "]", "=", "None", ",", "\n", "return_token_type_ids", ":", "Optional", "[", "bool", "]", "=", "None", ",", "\n", "return_attention_masks", ":", "Optional", "[", "bool", "]", "=", "None", ",", "\n", "return_overflowing_tokens", ":", "bool", "=", "False", ",", "\n", "return_special_tokens_masks", ":", "bool", "=", "False", ",", "\n", "return_offsets_mapping", ":", "bool", "=", "False", ",", "\n", "return_input_lengths", ":", "bool", "=", "False", ",", "\n", "**", "kwargs", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Returns a dictionary containing the encoded sequence or sequence pair and additional information:\n        the mask for sequence classification and the overflowing elements if a ``max_length`` is specified.\n\n        Args:\n            batch_text_or_text_pairs (:obj:`List[str]` or :obj:`List[List[str]]`):\n                Batch of sequences or pair of sequences to be encoded.\n                This can be a list of string/string-sequences/int-sequences or a list of pair of\n                string/string-sequences/int-sequence (see details in encode_plus)\n            add_special_tokens (:obj:`bool`, `optional`, defaults to :obj:`True`):\n                If set to ``True``, the sequences will be encoded with the special tokens relative\n                to their model.\n            max_length (:obj:`int`, `optional`, defaults to :obj:`None`):\n                If set to a number, will limit the total sequence returned so that it has a maximum length.\n                If there are overflowing tokens, those will be added to the returned dictionary\n            stride (:obj:`int`, `optional`, defaults to ``0``):\n                If set to a number along with max_length, the overflowing tokens returned will contain some tokens\n                from the main sequence returned. The value of this argument defines the number of additional tokens.\n            truncation_strategy (:obj:`str`, `optional`, defaults to `longest_first`):\n                String selected in the following options:\n\n                - 'longest_first' (default) Iteratively reduce the inputs sequence until the input is under max_length\n                  starting from the longest one at each token (when there is a pair of input sequences)\n                - 'only_first': Only truncate the first sequence\n                - 'only_second': Only truncate the second sequence\n                - 'do_not_truncate': Does not truncate (raise an error if the input sequence is longer than max_length)\n            pad_to_max_length (:obj:`bool`, `optional`, defaults to :obj:`False`):\n                If set to True, the returned sequences will be padded according to the model's padding side and\n                padding index, up to their max length. If no max length is specified, the padding is done up to the\n                model's max length. The tokenizer padding sides are handled by the class attribute `padding_side`\n                which can be set to the following strings:\n\n                - 'left': pads on the left of the sequences\n                - 'right': pads on the right of the sequences\n                Defaults to False: no padding.\n            return_tensors (:obj:`str`, `optional`, defaults to :obj:`None`):\n                Can be set to 'tf' or 'pt' to return respectively TensorFlow :obj:`tf.constant`\n                or PyTorch :obj:`torch.Tensor` instead of a list of python integers.\n            return_token_type_ids (:obj:`bool`, `optional`, defaults to :obj:`None`):\n                Whether to return token type IDs. If left to the default, will return the token type IDs according\n                to the specific tokenizer's default, defined by the :obj:`return_outputs` attribute.\n\n                `What are token type IDs? <../glossary.html#token-type-ids>`_\n            return_attention_masks (:obj:`bool`, `optional`, defaults to :obj:`none`):\n                Whether to return the attention mask. If left to the default, will return the attention mask according\n                to the specific tokenizer's default, defined by the :obj:`return_outputs` attribute.\n\n                `What are attention masks? <../glossary.html#attention-mask>`__\n            return_overflowing_tokens (:obj:`bool`, `optional`, defaults to :obj:`False`):\n                Set to True to return overflowing token information (default False).\n            return_special_tokens_masks (:obj:`bool`, `optional`, defaults to :obj:`False`):\n                Set to True to return special tokens mask information (default False).\n            return_offsets_mapping (:obj:`bool`, `optional`, defaults to :obj:`False`):\n                Set to True to return (char_start, char_end) for each token (default False).\n                If using Python's tokenizer, this method will raise NotImplementedError. This one is only available on\n                Rust-based tokenizers inheriting from PreTrainedTokenizerFast.\n            return_input_lengths (:obj:`bool`, `optional`, defaults to :obj:`False`):\n                If set the resulting dictionary will include the length of each sample\n            **kwargs: passed to the `self.tokenize()` method\n\n        Return:\n            A Dictionary of shape::\n\n                {\n                    input_ids: list[List[int]],\n                    token_type_ids: list[List[int]] if return_token_type_ids is True (default)\n                    attention_mask: list[List[int]] if return_attention_mask is True (default)\n                    overflowing_tokens: list[List[int]] if a ``max_length`` is specified and return_overflowing_tokens is True\n                    num_truncated_tokens: List[int] if a ``max_length`` is specified and return_overflowing_tokens is True\n                    special_tokens_mask: list[List[int]] if ``add_special_tokens`` if set to ``True`` and return_special_tokens_mask is True\n                }\n\n            With the fields:\n\n            - ``input_ids``: list of token ids to be fed to a model\n            - ``token_type_ids``: list of token type ids to be fed to a model\n            - ``attention_mask``: list of indices specifying which tokens should be attended to by the model\n            - ``overflowing_tokens``: list of overflowing tokens if a max length is specified.\n            - ``num_truncated_tokens``: number of overflowing tokens a ``max_length`` is specified\n            - ``special_tokens_mask``: if adding special tokens, this is a list of [0, 1], with 0 specifying special added\n              tokens and 1 specifying sequence tokens.\n        \"\"\"", "\n", "\n", "def", "get_input_ids", "(", "text", ")", ":", "\n", "            ", "if", "isinstance", "(", "text", ",", "str", ")", ":", "\n", "                ", "tokens", "=", "self", ".", "tokenize", "(", "\n", "text", ",", "add_special_tokens", "=", "add_special_tokens", ",", "**", "kwargs", "\n", ")", "\n", "return", "self", ".", "convert_tokens_to_ids", "(", "tokens", ")", "\n", "", "elif", "(", "\n", "isinstance", "(", "text", ",", "(", "list", ",", "tuple", ")", ")", "\n", "and", "len", "(", "text", ")", ">", "0", "\n", "and", "isinstance", "(", "text", "[", "0", "]", ",", "str", ")", "\n", ")", ":", "\n", "                ", "return", "self", ".", "convert_tokens_to_ids", "(", "text", ")", "\n", "", "elif", "(", "\n", "isinstance", "(", "text", ",", "(", "list", ",", "tuple", ")", ")", "\n", "and", "len", "(", "text", ")", ">", "0", "\n", "and", "isinstance", "(", "text", "[", "0", "]", ",", "int", ")", "\n", ")", ":", "\n", "                ", "return", "text", "\n", "", "else", ":", "\n", "                ", "raise", "ValueError", "(", "\n", "\"Input is not valid. Should be a string, a list/tuple of strings or a list/tuple of integers.\"", "\n", ")", "\n", "\n", "# Throw an error if we can pad because there is no padding token", "\n", "", "", "if", "pad_to_max_length", "and", "self", ".", "pad_token_id", "is", "None", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"Unable to set proper padding strategy as the tokenizer does not have a padding token. In this case please set the `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` or add a new pad token via the function add_special_tokens if you want to use a padding strategy\"", "\n", ")", "\n", "\n", "", "if", "return_offsets_mapping", ":", "\n", "            ", "raise", "NotImplementedError", "(", "\n", "\"return_offset_mapping is not available when using Python tokenizers.\"", "\n", "\"To use this feature, change your tokenizer to one deriving from \"", "\n", "\"transformers.PreTrainedTokenizerFast.\"", "\n", "\"More information on available tokenizers at \"", "\n", "\"https://github.com/huggingface/transformers/pull/2674\"", "\n", ")", "\n", "\n", "", "input_ids", "=", "[", "]", "\n", "for", "ids_or_pair_ids", "in", "batch_text_or_text_pairs", ":", "\n", "            ", "if", "isinstance", "(", "ids_or_pair_ids", ",", "(", "list", ",", "tuple", ")", ")", "and", "len", "(", "ids_or_pair_ids", ")", "==", "2", ":", "\n", "                ", "ids", ",", "pair_ids", "=", "ids_or_pair_ids", "\n", "", "else", ":", "\n", "                ", "ids", ",", "pair_ids", "=", "ids_or_pair_ids", ",", "None", "\n", "\n", "", "first_ids", "=", "get_input_ids", "(", "ids", ")", "\n", "second_ids", "=", "get_input_ids", "(", "pair_ids", ")", "if", "pair_ids", "is", "not", "None", "else", "None", "\n", "input_ids", ".", "append", "(", "(", "first_ids", ",", "second_ids", ")", ")", "\n", "\n", "", "if", "max_length", "is", "None", "and", "pad_to_max_length", ":", "\n", "\n", "            ", "def", "total_sequence_length", "(", "input_pairs", ")", ":", "\n", "                ", "first_ids", ",", "second_ids", "=", "input_pairs", "\n", "return", "len", "(", "first_ids", ")", "+", "(", "\n", "self", ".", "num_added_tokens", "(", ")", "\n", "if", "second_ids", "is", "None", "\n", "else", "(", "len", "(", "second_ids", ")", "+", "self", ".", "num_added_tokens", "(", "pair", "=", "True", ")", ")", "\n", ")", "\n", "\n", "", "max_length", "=", "max", "(", "[", "total_sequence_length", "(", "ids", ")", "for", "ids", "in", "input_ids", "]", ")", "\n", "\n", "", "batch_outputs", "=", "{", "}", "\n", "for", "first_ids", ",", "second_ids", "in", "input_ids", ":", "\n", "# Prepares a sequence of input id, or a pair of sequences of inputs ids so that it can be used by", "\n", "# the model. It adds special tokens, truncates sequences if overflowing while taking into account", "\n", "# the special tokens and manages a window stride for overflowing tokens", "\n", "            ", "outputs", "=", "self", ".", "prepare_for_model", "(", "\n", "first_ids", ",", "\n", "pair_ids", "=", "second_ids", ",", "\n", "max_length", "=", "max_length", ",", "\n", "pad_to_max_length", "=", "pad_to_max_length", ",", "\n", "add_special_tokens", "=", "add_special_tokens", ",", "\n", "stride", "=", "stride", ",", "\n", "truncation_strategy", "=", "truncation_strategy", ",", "\n", "return_attention_mask", "=", "return_attention_masks", ",", "\n", "return_token_type_ids", "=", "return_token_type_ids", ",", "\n", "return_overflowing_tokens", "=", "return_overflowing_tokens", ",", "\n", "return_special_tokens_mask", "=", "return_special_tokens_masks", ",", "\n", ")", "\n", "\n", "# Append the non-padded length to the output", "\n", "if", "return_input_lengths", ":", "\n", "                ", "outputs", "[", "\"input_len\"", "]", "=", "len", "(", "outputs", "[", "\"input_ids\"", "]", ")", "\n", "\n", "", "for", "key", ",", "value", "in", "outputs", ".", "items", "(", ")", ":", "\n", "                ", "if", "key", "not", "in", "batch_outputs", ":", "\n", "                    ", "batch_outputs", "[", "key", "]", "=", "[", "]", "\n", "", "batch_outputs", "[", "key", "]", ".", "append", "(", "value", ")", "\n", "\n", "", "", "if", "return_tensors", "is", "not", "None", ":", "\n", "\n", "# Do the tensor conversion in batch", "\n", "            ", "for", "key", ",", "value", "in", "batch_outputs", ".", "items", "(", ")", ":", "\n", "                ", "if", "return_tensors", "==", "\"tf\"", "and", "is_tf_available", "(", ")", ":", "\n", "                    ", "try", ":", "\n", "                        ", "batch_outputs", "[", "key", "]", "=", "tf", ".", "constant", "(", "value", ")", "\n", "", "except", "ValueError", ":", "\n", "                        ", "if", "None", "in", "[", "item", "for", "sequence", "in", "value", "for", "item", "in", "sequence", "]", ":", "\n", "                            ", "raise", "ValueError", "(", "self", ".", "NO_PAD_TOKEN_FOR_BATCH_MSG", ")", "\n", "", "else", ":", "\n", "                            ", "raise", "ValueError", "(", "self", ".", "UNEVEN_SEQUENCES_FOR_BATCH_MSG", ")", "\n", "", "", "", "elif", "return_tensors", "==", "\"pt\"", "and", "is_torch_available", "(", ")", ":", "\n", "                    ", "try", ":", "\n", "                        ", "batch_outputs", "[", "key", "]", "=", "torch", ".", "tensor", "(", "value", ")", "\n", "", "except", "ValueError", ":", "\n", "                        ", "raise", "ValueError", "(", "self", ".", "UNEVEN_SEQUENCES_FOR_BATCH_MSG", ")", "\n", "", "except", "RuntimeError", ":", "\n", "                        ", "if", "None", "in", "[", "item", "for", "sequence", "in", "value", "for", "item", "in", "sequence", "]", ":", "\n", "                            ", "raise", "ValueError", "(", "self", ".", "NO_PAD_TOKEN_FOR_BATCH_MSG", ")", "\n", "", "else", ":", "\n", "                            ", "raise", "\n", "", "", "", "elif", "return_tensors", "is", "not", "None", ":", "\n", "                    ", "logger", ".", "warning", "(", "\n", "\"Unable to convert output to tensors format {}, PyTorch or TensorFlow is not available.\"", ".", "format", "(", "\n", "return_tensors", "\n", ")", "\n", ")", "\n", "\n", "", "", "", "return", "batch_outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizer.prepare_for_model": [[1363, 1592], ["bool", "len", "len", "tokenization_utils.PreTrainedTokenizer.truncate_sequences", "tokenization_utils.PreTrainedTokenizer.build_inputs_with_special_tokens", "tokenization_utils.PreTrainedTokenizer.create_token_type_ids_from_sequences", "logger.warning", "logger.warning", "file_utils.is_tf_available", "tf.constant", "tokenization_utils.PreTrainedTokenizer.num_added_tokens", "tokenization_utils.PreTrainedTokenizer.get_special_tokens_mask", "len", "len", "len", "tf.constant", "tf.constant", "file_utils.is_torch_available", "torch.tensor", "len", "len", "len", "ValueError", "len", "torch.tensor", "torch.tensor", "logger.warning", "len", "len", "len", "len", "str", "len"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizer.truncate_sequences", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizerFast.build_inputs_with_special_tokens", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizer.create_token_type_ids_from_sequences", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.file_utils.is_tf_available", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizerFast.num_added_tokens", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizer.get_special_tokens_mask", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.file_utils.is_torch_available"], ["", "def", "prepare_for_model", "(", "\n", "self", ",", "\n", "ids", ":", "List", "[", "int", "]", ",", "\n", "pair_ids", ":", "Optional", "[", "List", "[", "int", "]", "]", "=", "None", ",", "\n", "max_length", ":", "Optional", "[", "int", "]", "=", "None", ",", "\n", "add_special_tokens", ":", "bool", "=", "True", ",", "\n", "stride", ":", "int", "=", "0", ",", "\n", "truncation_strategy", ":", "str", "=", "\"longest_first\"", ",", "\n", "pad_to_max_length", ":", "bool", "=", "False", ",", "\n", "return_tensors", ":", "Optional", "[", "str", "]", "=", "None", ",", "\n", "return_token_type_ids", ":", "Optional", "[", "bool", "]", "=", "None", ",", "\n", "return_attention_mask", ":", "Optional", "[", "bool", "]", "=", "None", ",", "\n", "return_overflowing_tokens", ":", "bool", "=", "False", ",", "\n", "return_special_tokens_mask", ":", "bool", "=", "False", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Prepares a sequence of input id, or a pair of sequences of inputs ids so that it can be used by the model.\n        It adds special tokens, truncates\n        sequences if overflowing while taking into account the special tokens and manages a window stride for\n        overflowing tokens\n\n        Args:\n            ids: list of tokenized input ids. Can be obtained from a string by chaining the\n                `tokenize` and `convert_tokens_to_ids` methods.\n            pair_ids: Optional second list of input ids. Can be obtained from a string by chaining the\n                `tokenize` and `convert_tokens_to_ids` methods.\n            max_length: maximum length of the returned list. Will truncate by taking into account the special tokens.\n            add_special_tokens: if set to ``True``, the sequences will be encoded with the special tokens relative\n                to their model.\n            stride: window stride for overflowing tokens. Can be useful for edge effect removal when using sequential\n                list of inputs.\n            truncation_strategy: string selected in the following options:\n                - 'longest_first' (default) Iteratively reduce the inputs sequence until the input is under max_length\n                    starting from the longest one at each token (when there is a pair of input sequences)\n                - 'only_first': Only truncate the first sequence\n                - 'only_second': Only truncate the second sequence\n                - 'do_not_truncate': Does not truncate (raise an error if the input sequence is longer than max_length)\n            pad_to_max_length: if set to True, the returned sequences will be padded according to the model's padding side and\n                padding index, up to their max length. If no max length is specified, the padding is done up to the model's max length.\n                The tokenizer padding sides are handled by the following strings:\n                - 'left': pads on the left of the sequences\n                - 'right': pads on the right of the sequences\n                Defaults to False: no padding.\n            return_tensors: (optional) can be set to 'tf' or 'pt' to return respectively TensorFlow tf.constant\n                or PyTorch torch.Tensor instead of a list of python integers.\n            return_token_type_ids: (optional) Set to False to avoid returning token_type_ids (default True).\n            return_attention_mask: (optional) Set to False to avoid returning attention mask (default True)\n            return_overflowing_tokens: (optional) Set to True to return overflowing token information (default False).\n            return_special_tokens_mask: (optional) Set to True to return special tokens mask information (default False).\n\n        Return:\n            A Dictionary of shape::\n\n                {\n                    input_ids: list[int],\n                    token_type_ids: list[int] if return_token_type_ids is True (default)\n                    overflowing_tokens: list[int] if a ``max_length`` is specified and return_overflowing_tokens is True\n                    num_truncated_tokens: int if a ``max_length`` is specified and return_overflowing_tokens is True\n                    special_tokens_mask: list[int] if ``add_special_tokens`` if set to ``True`` and return_special_tokens_mask is True\n                }\n\n            With the fields:\n                ``input_ids``: list of token ids to be fed to a model\n                ``token_type_ids``: list of token type ids to be fed to a model\n\n                ``overflowing_tokens``: list of overflowing tokens if a max length is specified.\n                ``num_truncated_tokens``: number of overflowing tokens a ``max_length`` is specified\n                ``special_tokens_mask``: if adding special tokens, this is a list of [0, 1], with 0 specifying special added\n                tokens and 1 specifying sequence tokens.\n        \"\"\"", "\n", "pair", "=", "bool", "(", "pair_ids", "is", "not", "None", ")", "\n", "len_ids", "=", "len", "(", "ids", ")", "\n", "len_pair_ids", "=", "len", "(", "pair_ids", ")", "if", "pair", "else", "0", "\n", "\n", "if", "return_token_type_ids", "is", "None", ":", "\n", "            ", "return_token_type_ids", "=", "\"token_type_ids\"", "in", "self", ".", "model_input_names", "\n", "", "if", "return_attention_mask", "is", "None", ":", "\n", "            ", "return_attention_mask", "=", "\"attention_mask\"", "in", "self", ".", "model_input_names", "\n", "\n", "", "encoded_inputs", "=", "{", "}", "\n", "\n", "# Handle max sequence length", "\n", "total_len", "=", "(", "\n", "len_ids", "\n", "+", "len_pair_ids", "\n", "+", "(", "self", ".", "num_added_tokens", "(", "pair", "=", "pair", ")", "if", "add_special_tokens", "else", "0", ")", "\n", ")", "\n", "if", "max_length", "and", "total_len", ">", "max_length", ":", "\n", "            ", "ids", ",", "pair_ids", ",", "overflowing_tokens", "=", "self", ".", "truncate_sequences", "(", "\n", "ids", ",", "\n", "pair_ids", "=", "pair_ids", ",", "\n", "num_tokens_to_remove", "=", "total_len", "-", "max_length", ",", "\n", "truncation_strategy", "=", "truncation_strategy", ",", "\n", "stride", "=", "stride", ",", "\n", ")", "\n", "if", "return_overflowing_tokens", ":", "\n", "                ", "encoded_inputs", "[", "\"overflowing_tokens\"", "]", "=", "overflowing_tokens", "\n", "encoded_inputs", "[", "\"num_truncated_tokens\"", "]", "=", "total_len", "-", "max_length", "\n", "\n", "# Handle special_tokens", "\n", "", "", "if", "add_special_tokens", ":", "\n", "            ", "sequence", "=", "self", ".", "build_inputs_with_special_tokens", "(", "ids", ",", "pair_ids", ")", "\n", "token_type_ids", "=", "self", ".", "create_token_type_ids_from_sequences", "(", "ids", ",", "pair_ids", ")", "\n", "", "else", ":", "\n", "            ", "sequence", "=", "ids", "+", "pair_ids", "if", "pair", "else", "ids", "\n", "token_type_ids", "=", "[", "0", "]", "*", "len", "(", "ids", ")", "+", "(", "[", "1", "]", "*", "len", "(", "pair_ids", ")", "if", "pair", "else", "[", "]", ")", "\n", "\n", "", "if", "return_special_tokens_mask", ":", "\n", "            ", "if", "add_special_tokens", ":", "\n", "                ", "encoded_inputs", "[", "\"special_tokens_mask\"", "]", "=", "self", ".", "get_special_tokens_mask", "(", "\n", "ids", ",", "pair_ids", "\n", ")", "\n", "", "else", ":", "\n", "                ", "encoded_inputs", "[", "\"special_tokens_mask\"", "]", "=", "[", "0", "]", "*", "len", "(", "sequence", ")", "\n", "\n", "", "", "encoded_inputs", "[", "\"input_ids\"", "]", "=", "sequence", "\n", "if", "return_token_type_ids", ":", "\n", "            ", "encoded_inputs", "[", "\"token_type_ids\"", "]", "=", "token_type_ids", "\n", "\n", "", "if", "max_length", "and", "len", "(", "encoded_inputs", "[", "\"input_ids\"", "]", ")", ">", "max_length", ":", "\n", "            ", "encoded_inputs", "[", "\"input_ids\"", "]", "=", "encoded_inputs", "[", "\"input_ids\"", "]", "[", ":", "max_length", "]", "\n", "if", "return_token_type_ids", ":", "\n", "                ", "encoded_inputs", "[", "\"token_type_ids\"", "]", "=", "encoded_inputs", "[", "\"token_type_ids\"", "]", "[", "\n", ":", "max_length", "\n", "]", "\n", "", "if", "return_special_tokens_mask", ":", "\n", "                ", "encoded_inputs", "[", "\"special_tokens_mask\"", "]", "=", "encoded_inputs", "[", "\n", "\"special_tokens_mask\"", "\n", "]", "[", ":", "max_length", "]", "\n", "\n", "", "", "if", "max_length", "is", "None", "and", "len", "(", "encoded_inputs", "[", "\"input_ids\"", "]", ")", ">", "self", ".", "max_len", ":", "\n", "            ", "logger", ".", "warning", "(", "\n", "\"Token indices sequence length is longer than the specified maximum sequence length \"", "\n", "\"for this model ({} > {}). Running this sequence through the model will result in \"", "\n", "\"indexing errors\"", ".", "format", "(", "len", "(", "ids", ")", ",", "self", ".", "max_len", ")", "\n", ")", "\n", "\n", "", "needs_to_be_padded", "=", "pad_to_max_length", "and", "(", "\n", "max_length", "\n", "and", "len", "(", "encoded_inputs", "[", "\"input_ids\"", "]", ")", "<", "max_length", "\n", "or", "max_length", "is", "None", "\n", "and", "len", "(", "encoded_inputs", "[", "\"input_ids\"", "]", ")", "<", "self", ".", "max_len", "\n", "and", "self", ".", "max_len", "<=", "10000", "\n", ")", "\n", "\n", "if", "pad_to_max_length", "and", "max_length", "is", "None", "and", "self", ".", "max_len", ">", "10000", ":", "\n", "            ", "logger", ".", "warning", "(", "\n", "\"Sequence can't be padded as no maximum length is specified and the model maximum length is too high.\"", "\n", ")", "\n", "\n", "", "if", "needs_to_be_padded", ":", "\n", "            ", "difference", "=", "(", "max_length", "if", "max_length", "is", "not", "None", "else", "self", ".", "max_len", ")", "-", "len", "(", "\n", "encoded_inputs", "[", "\"input_ids\"", "]", "\n", ")", "\n", "\n", "if", "self", ".", "padding_side", "==", "\"right\"", ":", "\n", "                ", "if", "return_attention_mask", ":", "\n", "                    ", "encoded_inputs", "[", "\"attention_mask\"", "]", "=", "[", "1", "]", "*", "len", "(", "\n", "encoded_inputs", "[", "\"input_ids\"", "]", "\n", ")", "+", "[", "0", "]", "*", "difference", "\n", "", "if", "return_token_type_ids", ":", "\n", "                    ", "encoded_inputs", "[", "\"token_type_ids\"", "]", "=", "(", "\n", "encoded_inputs", "[", "\"token_type_ids\"", "]", "\n", "+", "[", "self", ".", "pad_token_type_id", "]", "*", "difference", "\n", ")", "\n", "", "if", "return_special_tokens_mask", ":", "\n", "                    ", "encoded_inputs", "[", "\"special_tokens_mask\"", "]", "=", "(", "\n", "encoded_inputs", "[", "\"special_tokens_mask\"", "]", "+", "[", "1", "]", "*", "difference", "\n", ")", "\n", "", "encoded_inputs", "[", "\"input_ids\"", "]", "=", "(", "\n", "encoded_inputs", "[", "\"input_ids\"", "]", "+", "[", "self", ".", "pad_token_id", "]", "*", "difference", "\n", ")", "\n", "", "elif", "self", ".", "padding_side", "==", "\"left\"", ":", "\n", "                ", "if", "return_attention_mask", ":", "\n", "                    ", "encoded_inputs", "[", "\"attention_mask\"", "]", "=", "[", "0", "]", "*", "difference", "+", "[", "1", "]", "*", "len", "(", "\n", "encoded_inputs", "[", "\"input_ids\"", "]", "\n", ")", "\n", "", "if", "return_token_type_ids", ":", "\n", "                    ", "encoded_inputs", "[", "\"token_type_ids\"", "]", "=", "[", "\n", "self", ".", "pad_token_type_id", "\n", "]", "*", "difference", "+", "encoded_inputs", "[", "\"token_type_ids\"", "]", "\n", "", "if", "return_special_tokens_mask", ":", "\n", "                    ", "encoded_inputs", "[", "\"special_tokens_mask\"", "]", "=", "[", "\n", "1", "\n", "]", "*", "difference", "+", "encoded_inputs", "[", "\"special_tokens_mask\"", "]", "\n", "", "encoded_inputs", "[", "\"input_ids\"", "]", "=", "[", "\n", "self", ".", "pad_token_id", "\n", "]", "*", "difference", "+", "encoded_inputs", "[", "\"input_ids\"", "]", "\n", "\n", "", "else", ":", "\n", "                ", "raise", "ValueError", "(", "\"Invalid padding strategy:\"", "+", "str", "(", "self", ".", "padding_side", ")", ")", "\n", "\n", "", "", "elif", "return_attention_mask", ":", "\n", "            ", "encoded_inputs", "[", "\"attention_mask\"", "]", "=", "[", "1", "]", "*", "len", "(", "encoded_inputs", "[", "\"input_ids\"", "]", ")", "\n", "\n", "# Prepare inputs as tensors if asked", "\n", "", "if", "return_tensors", "==", "\"tf\"", "and", "is_tf_available", "(", ")", ":", "\n", "            ", "encoded_inputs", "[", "\"input_ids\"", "]", "=", "tf", ".", "constant", "(", "[", "encoded_inputs", "[", "\"input_ids\"", "]", "]", ")", "\n", "\n", "if", "\"token_type_ids\"", "in", "encoded_inputs", ":", "\n", "                ", "encoded_inputs", "[", "\"token_type_ids\"", "]", "=", "tf", ".", "constant", "(", "\n", "[", "encoded_inputs", "[", "\"token_type_ids\"", "]", "]", "\n", ")", "\n", "\n", "", "if", "\"attention_mask\"", "in", "encoded_inputs", ":", "\n", "                ", "encoded_inputs", "[", "\"attention_mask\"", "]", "=", "tf", ".", "constant", "(", "\n", "[", "encoded_inputs", "[", "\"attention_mask\"", "]", "]", "\n", ")", "\n", "\n", "", "", "elif", "return_tensors", "==", "\"pt\"", "and", "is_torch_available", "(", ")", ":", "\n", "            ", "encoded_inputs", "[", "\"input_ids\"", "]", "=", "torch", ".", "tensor", "(", "[", "encoded_inputs", "[", "\"input_ids\"", "]", "]", ")", "\n", "\n", "if", "\"token_type_ids\"", "in", "encoded_inputs", ":", "\n", "                ", "encoded_inputs", "[", "\"token_type_ids\"", "]", "=", "torch", ".", "tensor", "(", "\n", "[", "encoded_inputs", "[", "\"token_type_ids\"", "]", "]", "\n", ")", "\n", "\n", "", "if", "\"attention_mask\"", "in", "encoded_inputs", ":", "\n", "                ", "encoded_inputs", "[", "\"attention_mask\"", "]", "=", "torch", ".", "tensor", "(", "\n", "[", "encoded_inputs", "[", "\"attention_mask\"", "]", "]", "\n", ")", "\n", "", "", "elif", "return_tensors", "is", "not", "None", ":", "\n", "            ", "logger", ".", "warning", "(", "\n", "\"Unable to convert output to tensors format {}, PyTorch or TensorFlow is not available.\"", ".", "format", "(", "\n", "return_tensors", "\n", ")", "\n", ")", "\n", "\n", "", "return", "encoded_inputs", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizer.prepare_for_tokenization": [[1593, 1596], ["None"], "methods", ["None"], ["", "def", "prepare_for_tokenization", "(", "self", ",", "text", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"Performs any necessary transformations before tokenization\"\"\"", "\n", "return", "text", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizer.truncate_sequences": [[1597, 1647], ["range", "min", "len", "min", "len", "len", "min", "len", "len", "len", "ValueError", "ValueError", "len"], "methods", ["None"], ["", "def", "truncate_sequences", "(", "\n", "self", ",", "\n", "ids", ",", "\n", "pair_ids", "=", "None", ",", "\n", "num_tokens_to_remove", "=", "0", ",", "\n", "truncation_strategy", "=", "\"longest_first\"", ",", "\n", "stride", "=", "0", ",", "\n", ")", ":", "\n", "        ", "\"\"\"Truncates a sequence pair in place to the maximum length.\n        truncation_strategy: string selected in the following options:\n            - 'longest_first' (default) Iteratively reduce the inputs sequence until the input is under max_length\n                starting from the longest one at each token (when there is a pair of input sequences).\n                Overflowing tokens only contains overflow from the first sequence.\n            - 'only_first': Only truncate the first sequence. raise an error if the first sequence is shorter or equal to than num_tokens_to_remove.\n            - 'only_second': Only truncate the second sequence\n            - 'do_not_truncate': Does not truncate (raise an error if the input sequence is longer than max_length)\n        \"\"\"", "\n", "if", "num_tokens_to_remove", "<=", "0", ":", "\n", "            ", "return", "ids", ",", "pair_ids", ",", "[", "]", "\n", "\n", "", "if", "truncation_strategy", "==", "\"longest_first\"", ":", "\n", "            ", "overflowing_tokens", "=", "[", "]", "\n", "for", "_", "in", "range", "(", "num_tokens_to_remove", ")", ":", "\n", "                ", "if", "pair_ids", "is", "None", "or", "len", "(", "ids", ")", ">", "len", "(", "pair_ids", ")", ":", "\n", "                    ", "overflowing_tokens", "=", "[", "ids", "[", "-", "1", "]", "]", "+", "overflowing_tokens", "\n", "ids", "=", "ids", "[", ":", "-", "1", "]", "\n", "", "else", ":", "\n", "                    ", "pair_ids", "=", "pair_ids", "[", ":", "-", "1", "]", "\n", "", "", "window_len", "=", "min", "(", "len", "(", "ids", ")", ",", "stride", ")", "\n", "if", "window_len", ">", "0", ":", "\n", "                ", "overflowing_tokens", "=", "ids", "[", "-", "window_len", ":", "]", "+", "overflowing_tokens", "\n", "", "", "elif", "truncation_strategy", "==", "\"only_first\"", ":", "\n", "            ", "assert", "len", "(", "ids", ")", ">", "num_tokens_to_remove", "\n", "window_len", "=", "min", "(", "len", "(", "ids", ")", ",", "stride", "+", "num_tokens_to_remove", ")", "\n", "overflowing_tokens", "=", "ids", "[", "-", "window_len", ":", "]", "\n", "ids", "=", "ids", "[", ":", "-", "num_tokens_to_remove", "]", "\n", "", "elif", "truncation_strategy", "==", "\"only_second\"", ":", "\n", "            ", "assert", "pair_ids", "is", "not", "None", "and", "len", "(", "pair_ids", ")", ">", "num_tokens_to_remove", "\n", "window_len", "=", "min", "(", "len", "(", "pair_ids", ")", ",", "stride", "+", "num_tokens_to_remove", ")", "\n", "overflowing_tokens", "=", "pair_ids", "[", "-", "window_len", ":", "]", "\n", "pair_ids", "=", "pair_ids", "[", ":", "-", "num_tokens_to_remove", "]", "\n", "", "elif", "truncation_strategy", "==", "\"do_not_truncate\"", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"Input sequence are too long for max_length. Please select a truncation strategy.\"", "\n", ")", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"Truncation_strategy should be selected in ['longest_first', 'only_first', 'only_second', 'do_not_truncate']\"", "\n", ")", "\n", "", "return", "(", "ids", ",", "pair_ids", ",", "overflowing_tokens", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizer.create_token_type_ids_from_sequences": [[1648, 1652], ["len", "len", "len"], "methods", ["None"], ["", "def", "create_token_type_ids_from_sequences", "(", "self", ",", "token_ids_0", ",", "token_ids_1", "=", "None", ")", ":", "\n", "        ", "if", "token_ids_1", "is", "None", ":", "\n", "            ", "return", "len", "(", "token_ids_0", ")", "*", "[", "0", "]", "\n", "", "return", "[", "0", "]", "*", "len", "(", "token_ids_0", ")", "+", "[", "1", "]", "*", "len", "(", "token_ids_1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizer.build_inputs_with_special_tokens": [[1653, 1664], ["None"], "methods", ["None"], ["", "def", "build_inputs_with_special_tokens", "(", "self", ",", "token_ids_0", ",", "token_ids_1", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Build model inputs from a sequence or a pair of sequence for sequence classification tasks\n        by concatenating and adding special tokens.\n        A RoBERTa sequence has the following format:\n            single sequence: <s> X </s>\n            pair of sequences: <s> A </s></s> B </s>\n        \"\"\"", "\n", "if", "token_ids_1", "is", "None", ":", "\n", "            ", "return", "token_ids_0", "\n", "", "return", "token_ids_0", "+", "token_ids_1", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizer.get_special_tokens_mask": [[1665, 1683], ["len", "len"], "methods", ["None"], ["", "def", "get_special_tokens_mask", "(", "\n", "self", ",", "token_ids_0", ",", "token_ids_1", "=", "None", ",", "already_has_special_tokens", "=", "False", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Retrieves sequence ids from a token list that has no special tokens added. This method is called when adding\n        special tokens using the tokenizer ``prepare_for_model`` or ``encode_plus`` methods.\n\n        Args:\n            token_ids_0: list of ids (must not contain special tokens)\n            token_ids_1: Optional list of ids (must not contain special tokens), necessary when fetching sequence ids\n                for sequence pairs\n            already_has_special_tokens: (default False) Set to True if the token list is already formated with\n                special tokens for the model\n\n        Returns:\n            A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.\n        \"\"\"", "\n", "return", "[", "0", "]", "*", "(", "(", "len", "(", "token_ids_1", ")", "if", "token_ids_1", "else", "0", ")", "+", "len", "(", "token_ids_0", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizer.convert_ids_to_tokens": [[1684, 1706], ["isinstance", "int", "tokenization_utils.PreTrainedTokenizer._convert_id_to_token", "tokens.append", "tokens.append", "tokenization_utils.PreTrainedTokenizer._convert_id_to_token"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizerFast._convert_id_to_token", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizerFast._convert_id_to_token"], ["", "def", "convert_ids_to_tokens", "(", "self", ",", "ids", ",", "skip_special_tokens", "=", "False", ")", ":", "\n", "        ", "\"\"\"Converts a single index or a sequence of indices (integers) in a token \"\n        (resp.) a sequence of tokens (str), using the vocabulary and added tokens.\n\n        Args:\n            skip_special_tokens: Don't decode special tokens (self.all_special_tokens). Default: False\n        \"\"\"", "\n", "if", "isinstance", "(", "ids", ",", "int", ")", ":", "\n", "            ", "if", "ids", "in", "self", ".", "added_tokens_decoder", ":", "\n", "                ", "return", "self", ".", "added_tokens_decoder", "[", "ids", "]", "\n", "", "else", ":", "\n", "                ", "return", "self", ".", "_convert_id_to_token", "(", "ids", ")", "\n", "", "", "tokens", "=", "[", "]", "\n", "for", "index", "in", "ids", ":", "\n", "            ", "index", "=", "int", "(", "index", ")", "\n", "if", "skip_special_tokens", "and", "index", "in", "self", ".", "all_special_ids", ":", "\n", "                ", "continue", "\n", "", "if", "index", "in", "self", ".", "added_tokens_decoder", ":", "\n", "                ", "tokens", ".", "append", "(", "self", ".", "added_tokens_decoder", "[", "index", "]", ")", "\n", "", "else", ":", "\n", "                ", "tokens", ".", "append", "(", "self", ".", "_convert_id_to_token", "(", "index", ")", ")", "\n", "", "", "return", "tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizer._convert_id_to_token": [[1707, 1709], ["None"], "methods", ["None"], ["", "def", "_convert_id_to_token", "(", "self", ",", "index", ")", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizer.convert_tokens_to_string": [[1710, 1716], ["tokenization_utils.PreTrainedTokenizer.convert_ids_to_tokens"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizer.convert_ids_to_tokens"], ["", "def", "convert_tokens_to_string", "(", "self", ",", "tokens", ")", ":", "\n", "        ", "\"\"\"Converts a sequence of tokens (string) in a single string.\n        The most simple way to do it is ' '.join(self.convert_ids_to_tokens(token_ids))\n        but we often want to remove sub-word tokenization artifacts at the same time.\n        \"\"\"", "\n", "return", "\" \"", ".", "join", "(", "self", ".", "convert_ids_to_tokens", "(", "tokens", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizer.decode": [[1717, 1758], ["tokenization_utils.PreTrainedTokenizer.convert_ids_to_tokens", "sub_texts.append", "tokenization_utils.PreTrainedTokenizer.clean_up_tokenization", "sub_texts.append", "current_sub_text.append", "tokenization_utils.PreTrainedTokenizer.convert_tokens_to_string", "sub_texts.append", "tokenization_utils.PreTrainedTokenizer.convert_tokens_to_string"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizer.convert_ids_to_tokens", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizer.clean_up_tokenization", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizerFast.convert_tokens_to_string", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizerFast.convert_tokens_to_string"], ["", "def", "decode", "(", "\n", "self", ",", "token_ids", ",", "skip_special_tokens", "=", "False", ",", "clean_up_tokenization_spaces", "=", "True", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Converts a sequence of ids (integer) in a string, using the tokenizer and vocabulary\n        with options to remove special tokens and clean up tokenization spaces.\n        Similar to doing ``self.convert_tokens_to_string(self.convert_ids_to_tokens(token_ids))``.\n\n        Args:\n            token_ids: list of tokenized input ids. Can be obtained using the `encode` or `encode_plus` methods.\n            skip_special_tokens: if set to True, will replace special tokens.\n            clean_up_tokenization_spaces: if set to True, will clean up the tokenization spaces.\n        \"\"\"", "\n", "filtered_tokens", "=", "self", ".", "convert_ids_to_tokens", "(", "\n", "token_ids", ",", "skip_special_tokens", "=", "skip_special_tokens", "\n", ")", "\n", "\n", "# To avoid mixing byte-level and unicode for byte-level BPT", "\n", "# we need to build string separatly for added tokens and byte-level tokens", "\n", "# cf. https://github.com/huggingface/transformers/issues/1133", "\n", "sub_texts", "=", "[", "]", "\n", "current_sub_text", "=", "[", "]", "\n", "for", "token", "in", "filtered_tokens", ":", "\n", "            ", "if", "skip_special_tokens", "and", "token", "in", "self", ".", "all_special_ids", ":", "\n", "                ", "continue", "\n", "", "if", "token", "in", "self", ".", "added_tokens_encoder", ":", "\n", "                ", "if", "current_sub_text", ":", "\n", "                    ", "sub_texts", ".", "append", "(", "self", ".", "convert_tokens_to_string", "(", "current_sub_text", ")", ")", "\n", "current_sub_text", "=", "[", "]", "\n", "", "sub_texts", ".", "append", "(", "token", ")", "\n", "", "else", ":", "\n", "                ", "current_sub_text", ".", "append", "(", "token", ")", "\n", "", "", "if", "current_sub_text", ":", "\n", "            ", "sub_texts", ".", "append", "(", "self", ".", "convert_tokens_to_string", "(", "current_sub_text", ")", ")", "\n", "", "text", "=", "\" \"", ".", "join", "(", "sub_texts", ")", "\n", "\n", "if", "clean_up_tokenization_spaces", ":", "\n", "            ", "clean_text", "=", "self", ".", "clean_up_tokenization", "(", "text", ")", "\n", "return", "clean_text", "\n", "", "else", ":", "\n", "            ", "return", "text", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizer.special_tokens_map": [[1759, 1770], ["getattr"], "methods", ["None"], ["", "", "@", "property", "\n", "def", "special_tokens_map", "(", "self", ")", ":", "\n", "        ", "\"\"\"A dictionary mapping special token class attribute (cls_token, unk_token...) to their\n        values ('<unk>', '<cls>'...)\n        \"\"\"", "\n", "set_attr", "=", "{", "}", "\n", "for", "attr", "in", "self", ".", "SPECIAL_TOKENS_ATTRIBUTES", ":", "\n", "            ", "attr_value", "=", "getattr", "(", "self", ",", "\"_\"", "+", "attr", ")", "\n", "if", "attr_value", ":", "\n", "                ", "set_attr", "[", "attr", "]", "=", "attr_value", "\n", "", "", "return", "set_attr", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizer.all_special_tokens": [[1771, 1786], ["set_attr.values", "list", "set", "isinstance", "list"], "methods", ["None"], ["", "@", "property", "\n", "def", "all_special_tokens", "(", "self", ")", ":", "\n", "        ", "\"\"\"List all the special tokens ('<unk>', '<cls>'...) mapped to class attributes\n        (cls_token, unk_token...).\n        \"\"\"", "\n", "all_toks", "=", "[", "]", "\n", "set_attr", "=", "self", ".", "special_tokens_map", "\n", "for", "attr_value", "in", "set_attr", ".", "values", "(", ")", ":", "\n", "            ", "all_toks", "=", "all_toks", "+", "(", "\n", "list", "(", "attr_value", ")", "\n", "if", "isinstance", "(", "attr_value", ",", "(", "list", ",", "tuple", ")", ")", "\n", "else", "[", "attr_value", "]", "\n", ")", "\n", "", "all_toks", "=", "list", "(", "set", "(", "all_toks", ")", ")", "\n", "return", "all_toks", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizer.all_special_ids": [[1787, 1795], ["tokenization_utils.PreTrainedTokenizer.convert_tokens_to_ids"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizer.convert_tokens_to_ids"], ["", "@", "property", "\n", "def", "all_special_ids", "(", "self", ")", ":", "\n", "        ", "\"\"\"List the vocabulary indices of the special tokens ('<unk>', '<cls>'...) mapped to\n        class attributes (cls_token, unk_token...).\n        \"\"\"", "\n", "all_toks", "=", "self", ".", "all_special_tokens", "\n", "all_ids", "=", "self", ".", "convert_tokens_to_ids", "(", "all_toks", ")", "\n", "return", "all_ids", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizer.clean_up_tokenization": [[1796, 1813], ["out_string.replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace.replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace.replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace", "out_string.replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace.replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace.replace().replace().replace().replace().replace().replace().replace().replace().replace().replace", "out_string.replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace.replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace.replace().replace().replace().replace().replace().replace().replace().replace().replace", "out_string.replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace.replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace.replace().replace().replace().replace().replace().replace().replace().replace", "out_string.replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace.replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace.replace().replace().replace().replace().replace().replace().replace", "out_string.replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace.replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace.replace().replace().replace().replace().replace().replace", "out_string.replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace.replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace.replace().replace().replace().replace().replace", "out_string.replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace.replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace.replace().replace().replace().replace", "out_string.replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace.replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace.replace().replace().replace", "out_string.replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace.replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace.replace().replace", "out_string.replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace.replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace.replace"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "clean_up_tokenization", "(", "out_string", ")", ":", "\n", "        ", "\"\"\"Clean up a list of simple English tokenization artifacts like spaces before punctuations and abreviated forms.\"\"\"", "\n", "out_string", "=", "(", "\n", "out_string", ".", "replace", "(", "\" .\"", ",", "\".\"", ")", "\n", ".", "replace", "(", "\" ?\"", ",", "\"?\"", ")", "\n", ".", "replace", "(", "\" !\"", ",", "\"!\"", ")", "\n", ".", "replace", "(", "\" ,\"", ",", "\",\"", ")", "\n", ".", "replace", "(", "\" ' \"", ",", "\"'\"", ")", "\n", ".", "replace", "(", "\" n't\"", ",", "\"n't\"", ")", "\n", ".", "replace", "(", "\" 'm\"", ",", "\"'m\"", ")", "\n", ".", "replace", "(", "\" do not\"", ",", "\" don't\"", ")", "\n", ".", "replace", "(", "\" 's\"", ",", "\"'s\"", ")", "\n", ".", "replace", "(", "\" 've\"", ",", "\"'ve\"", ")", "\n", ".", "replace", "(", "\" 're\"", ",", "\"'re\"", ")", "\n", ")", "\n", "return", "out_string", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizerFast.__init__": [[1819, 1830], ["tokenization_utils.PreTrainedTokenizer.__init__", "ValueError", "tokenization_utils.PreTrainedTokenizerFast.num_added_tokens", "tokenization_utils.PreTrainedTokenizerFast.num_added_tokens"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.tests.model_selector_test.ToyModel.__init__", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizerFast.num_added_tokens", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizerFast.num_added_tokens"], ["def", "__init__", "(", "self", ",", "tokenizer", ":", "BaseTokenizer", ",", "**", "kwargs", ")", ":", "\n", "        ", "if", "tokenizer", "is", "None", ":", "\n", "            ", "raise", "ValueError", "(", "\"Provided tokenizer cannot be None\"", ")", "\n", "", "self", ".", "_tokenizer", "=", "tokenizer", "\n", "\n", "super", "(", ")", ".", "__init__", "(", "**", "kwargs", ")", "\n", "self", ".", "max_len_single_sentence", "=", "self", ".", "max_len", "-", "self", ".", "num_added_tokens", "(", "\n", "False", "\n", ")", "# take into account special tokens", "\n", "self", ".", "max_len_sentences_pair", "=", "self", ".", "max_len", "-", "self", ".", "num_added_tokens", "(", "\n", "True", "\n", ")", "# take into account special tokens", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizerFast.tokenizer": [[1832, 1835], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "tokenizer", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "_tokenizer", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizerFast.decoder": [[1836, 1839], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "decoder", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "_tokenizer", ".", "_tokenizer", ".", "decoder", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizerFast.vocab_size": [[1840, 1843], ["tokenization_utils.PreTrainedTokenizerFast._tokenizer.get_vocab_size"], "methods", ["None"], ["", "@", "property", "\n", "def", "vocab_size", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "_tokenizer", ".", "get_vocab_size", "(", "with_added_tokens", "=", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizerFast.__len__": [[1844, 1846], ["tokenization_utils.PreTrainedTokenizerFast._tokenizer.get_vocab_size"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "_tokenizer", ".", "get_vocab_size", "(", "with_added_tokens", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizerFast.bos_token": [[1847, 1851], ["tokenization_utils.PreTrainedTokenizerFast._update_special_tokens"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizerFast._update_special_tokens"], ["", "@", "PreTrainedTokenizer", ".", "bos_token", ".", "setter", "\n", "def", "bos_token", "(", "self", ",", "value", ")", ":", "\n", "        ", "self", ".", "_bos_token", "=", "value", "\n", "self", ".", "_update_special_tokens", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizerFast.eos_token": [[1852, 1856], ["tokenization_utils.PreTrainedTokenizerFast._update_special_tokens"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizerFast._update_special_tokens"], ["", "@", "PreTrainedTokenizer", ".", "eos_token", ".", "setter", "\n", "def", "eos_token", "(", "self", ",", "value", ")", ":", "\n", "        ", "self", ".", "_eos_token", "=", "value", "\n", "self", ".", "_update_special_tokens", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizerFast.unk_token": [[1857, 1861], ["tokenization_utils.PreTrainedTokenizerFast._update_special_tokens"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizerFast._update_special_tokens"], ["", "@", "PreTrainedTokenizer", ".", "unk_token", ".", "setter", "\n", "def", "unk_token", "(", "self", ",", "value", ")", ":", "\n", "        ", "self", ".", "_unk_token", "=", "value", "\n", "self", ".", "_update_special_tokens", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizerFast.sep_token": [[1862, 1866], ["tokenization_utils.PreTrainedTokenizerFast._update_special_tokens"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizerFast._update_special_tokens"], ["", "@", "PreTrainedTokenizer", ".", "sep_token", ".", "setter", "\n", "def", "sep_token", "(", "self", ",", "value", ")", ":", "\n", "        ", "self", ".", "_sep_token", "=", "value", "\n", "self", ".", "_update_special_tokens", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizerFast.pad_token": [[1867, 1871], ["tokenization_utils.PreTrainedTokenizerFast._update_special_tokens"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizerFast._update_special_tokens"], ["", "@", "PreTrainedTokenizer", ".", "pad_token", ".", "setter", "\n", "def", "pad_token", "(", "self", ",", "value", ")", ":", "\n", "        ", "self", ".", "_pad_token", "=", "value", "\n", "self", ".", "_update_special_tokens", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizerFast.cls_token": [[1872, 1876], ["tokenization_utils.PreTrainedTokenizerFast._update_special_tokens"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizerFast._update_special_tokens"], ["", "@", "PreTrainedTokenizer", ".", "cls_token", ".", "setter", "\n", "def", "cls_token", "(", "self", ",", "value", ")", ":", "\n", "        ", "self", ".", "_cls_token", "=", "value", "\n", "self", ".", "_update_special_tokens", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizerFast.mask_token": [[1877, 1881], ["tokenization_utils.PreTrainedTokenizerFast._update_special_tokens"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizerFast._update_special_tokens"], ["", "@", "PreTrainedTokenizer", ".", "mask_token", ".", "setter", "\n", "def", "mask_token", "(", "self", ",", "value", ")", ":", "\n", "        ", "self", ".", "_mask_token", "=", "value", "\n", "self", ".", "_update_special_tokens", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizerFast.additional_special_tokens": [[1882, 1886], ["tokenization_utils.PreTrainedTokenizerFast._update_special_tokens"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizerFast._update_special_tokens"], ["", "@", "PreTrainedTokenizer", ".", "additional_special_tokens", ".", "setter", "\n", "def", "additional_special_tokens", "(", "self", ",", "value", ")", ":", "\n", "        ", "self", ".", "_additional_special_tokens", "=", "value", "\n", "self", ".", "_update_special_tokens", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizerFast._update_special_tokens": [[1887, 1890], ["tokenization_utils.PreTrainedTokenizerFast._tokenizer.add_special_tokens"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizerFast.add_special_tokens"], ["", "def", "_update_special_tokens", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "_tokenizer", "is", "not", "None", ":", "\n", "            ", "self", ".", "_tokenizer", ".", "add_special_tokens", "(", "self", ".", "all_special_tokens", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizerFast._convert_encoding": [[1891, 1958], ["collections.defaultdict", "encoding_dict[].append", "file_utils.is_tf_available", "tf.constant", "encoding_dict[].append", "encoding_dict[].append", "encoding_dict[].append", "encoding_dict[].append", "tf.constant", "tf.constant", "file_utils.is_torch_available", "torch.tensor", "torch.tensor", "torch.tensor", "logger.warning", "e.original_str.offsets"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.file_utils.is_tf_available", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.file_utils.is_torch_available"], ["", "", "def", "_convert_encoding", "(", "\n", "self", ",", "\n", "encoding", ",", "\n", "return_tensors", "=", "None", ",", "\n", "return_token_type_ids", "=", "None", ",", "\n", "return_attention_mask", "=", "None", ",", "\n", "return_overflowing_tokens", "=", "False", ",", "\n", "return_special_tokens_mask", "=", "False", ",", "\n", "return_offsets_mapping", "=", "False", ",", "\n", ")", ":", "\n", "        ", "if", "return_token_type_ids", "is", "None", ":", "\n", "            ", "return_token_type_ids", "=", "\"token_type_ids\"", "in", "self", ".", "model_input_names", "\n", "", "if", "return_attention_mask", "is", "None", ":", "\n", "            ", "return_attention_mask", "=", "\"attention_mask\"", "in", "self", ".", "model_input_names", "\n", "\n", "", "if", "return_overflowing_tokens", "and", "encoding", ".", "overflowing", "is", "not", "None", ":", "\n", "            ", "encodings", "=", "[", "encoding", "]", "+", "encoding", ".", "overflowing", "\n", "", "else", ":", "\n", "            ", "encodings", "=", "[", "encoding", "]", "\n", "\n", "", "encoding_dict", "=", "defaultdict", "(", "list", ")", "\n", "for", "e", "in", "encodings", ":", "\n", "            ", "encoding_dict", "[", "\"input_ids\"", "]", ".", "append", "(", "e", ".", "ids", ")", "\n", "\n", "if", "return_token_type_ids", ":", "\n", "                ", "encoding_dict", "[", "\"token_type_ids\"", "]", ".", "append", "(", "e", ".", "type_ids", ")", "\n", "", "if", "return_attention_mask", ":", "\n", "                ", "encoding_dict", "[", "\"attention_mask\"", "]", ".", "append", "(", "e", ".", "attention_mask", ")", "\n", "", "if", "return_special_tokens_mask", ":", "\n", "                ", "encoding_dict", "[", "\"special_tokens_mask\"", "]", ".", "append", "(", "e", ".", "special_tokens_mask", ")", "\n", "", "if", "return_offsets_mapping", ":", "\n", "                ", "encoding_dict", "[", "\"offset_mapping\"", "]", ".", "append", "(", "\n", "[", "e", ".", "original_str", ".", "offsets", "(", "o", ")", "for", "o", "in", "e", ".", "offsets", "]", "\n", ")", "\n", "\n", "# Prepare inputs as tensors if asked", "\n", "", "", "if", "return_tensors", "==", "\"tf\"", "and", "is_tf_available", "(", ")", ":", "\n", "            ", "encoding_dict", "[", "\"input_ids\"", "]", "=", "tf", ".", "constant", "(", "encoding_dict", "[", "\"input_ids\"", "]", ")", "\n", "if", "\"token_type_ids\"", "in", "encoding_dict", ":", "\n", "                ", "encoding_dict", "[", "\"token_type_ids\"", "]", "=", "tf", ".", "constant", "(", "\n", "encoding_dict", "[", "\"token_type_ids\"", "]", "\n", ")", "\n", "\n", "", "if", "\"attention_mask\"", "in", "encoding_dict", ":", "\n", "                ", "encoding_dict", "[", "\"attention_mask\"", "]", "=", "tf", ".", "constant", "(", "\n", "encoding_dict", "[", "\"attention_mask\"", "]", "\n", ")", "\n", "\n", "", "", "elif", "return_tensors", "==", "\"pt\"", "and", "is_torch_available", "(", ")", ":", "\n", "            ", "encoding_dict", "[", "\"input_ids\"", "]", "=", "torch", ".", "tensor", "(", "encoding_dict", "[", "\"input_ids\"", "]", ")", "\n", "if", "\"token_type_ids\"", "in", "encoding_dict", ":", "\n", "                ", "encoding_dict", "[", "\"token_type_ids\"", "]", "=", "torch", ".", "tensor", "(", "\n", "encoding_dict", "[", "\"token_type_ids\"", "]", "\n", ")", "\n", "\n", "", "if", "\"attention_mask\"", "in", "encoding_dict", ":", "\n", "                ", "encoding_dict", "[", "\"attention_mask\"", "]", "=", "torch", ".", "tensor", "(", "\n", "encoding_dict", "[", "\"attention_mask\"", "]", "\n", ")", "\n", "", "", "elif", "return_tensors", "is", "not", "None", ":", "\n", "            ", "logger", ".", "warning", "(", "\n", "\"Unable to convert output to tensors format {}, PyTorch or TensorFlow is not available.\"", ".", "format", "(", "\n", "return_tensors", "\n", ")", "\n", ")", "\n", "\n", "", "return", "encoding_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizerFast._convert_token_to_id_with_added_voc": [[1959, 1964], ["tokenization_utils.PreTrainedTokenizerFast._tokenizer.token_to_id"], "methods", ["None"], ["", "def", "_convert_token_to_id_with_added_voc", "(", "self", ",", "token", ")", ":", "\n", "        ", "id", "=", "self", ".", "_tokenizer", ".", "token_to_id", "(", "token", ")", "\n", "if", "id", "is", "None", ":", "\n", "            ", "return", "self", ".", "unk_token_id", "\n", "", "return", "id", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizerFast._convert_id_to_token": [[1965, 1967], ["tokenization_utils.PreTrainedTokenizerFast._tokenizer.id_to_token", "int"], "methods", ["None"], ["", "def", "_convert_id_to_token", "(", "self", ",", "index", ")", ":", "\n", "        ", "return", "self", ".", "_tokenizer", ".", "id_to_token", "(", "int", "(", "index", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizerFast.convert_tokens_to_string": [[1968, 1970], ["tokenization_utils.PreTrainedTokenizerFast._tokenizer.decode"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizerFast.decode"], ["", "def", "convert_tokens_to_string", "(", "self", ",", "tokens", ")", ":", "\n", "        ", "return", "self", ".", "_tokenizer", ".", "decode", "(", "tokens", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizerFast.add_tokens": [[1971, 1975], ["isinstance", "tokenization_utils.PreTrainedTokenizerFast._tokenizer.add_tokens"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizerFast.add_tokens"], ["", "def", "add_tokens", "(", "self", ",", "new_tokens", ")", ":", "\n", "        ", "if", "isinstance", "(", "new_tokens", ",", "str", ")", ":", "\n", "            ", "new_tokens", "=", "[", "new_tokens", "]", "\n", "", "return", "self", ".", "_tokenizer", ".", "add_tokens", "(", "new_tokens", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizerFast.add_special_tokens": [[1976, 1980], ["tokenization_utils.PreTrainedTokenizer.add_special_tokens", "tokenization_utils.PreTrainedTokenizerFast._update_special_tokens"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizerFast.add_special_tokens", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizerFast._update_special_tokens"], ["", "def", "add_special_tokens", "(", "self", ",", "special_tokens_dict", ")", ":", "\n", "        ", "added", "=", "super", "(", ")", ".", "add_special_tokens", "(", "special_tokens_dict", ")", "\n", "self", ".", "_update_special_tokens", "(", ")", "\n", "return", "added", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizerFast.build_inputs_with_special_tokens": [[1981, 1986], ["None"], "methods", ["None"], ["", "def", "build_inputs_with_special_tokens", "(", "self", ",", "token_ids_0", ",", "token_ids_1", "=", "None", ")", ":", "\n", "        ", "if", "token_ids_1", "is", "None", ":", "\n", "            ", "return", "token_ids_0", "\n", "", "else", ":", "\n", "            ", "return", "token_ids_0", "+", "token_ids_1", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizerFast.num_added_tokens": [[1987, 1989], ["tokenization_utils.PreTrainedTokenizerFast.tokenizer.num_special_tokens_to_add"], "methods", ["None"], ["", "", "def", "num_added_tokens", "(", "self", ",", "pair", "=", "False", ")", ":", "\n", "        ", "return", "self", ".", "tokenizer", ".", "num_special_tokens_to_add", "(", "pair", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizerFast.tokenize": [[1990, 1992], ["tokenization_utils.PreTrainedTokenizerFast.tokenizer.encode"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizer.encode"], ["", "def", "tokenize", "(", "self", ",", "text", ",", "**", "kwargs", ")", ":", "\n", "        ", "return", "self", ".", "tokenizer", ".", "encode", "(", "text", ")", ".", "tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizerFast.batch_encode_plus": [[1993, 2091], ["tokens[].keys", "logger.warning", "ValueError", "tokenization_utils.truncate_and_pad", "tokenization_utils.PreTrainedTokenizerFast._convert_encoding", "isinstance", "TypeError", "len", "isinstance", "tokenization_utils.PreTrainedTokenizerFast._tokenizer.encode_batch", "tf.stack", "tokenization_utils.PreTrainedTokenizerFast._tokenizer.encode", "tokenization_utils.PreTrainedTokenizerFast._tokenizer.encode", "torch.stack", "enumerate", "type", "len", "len", "len"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.truncate_and_pad", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizerFast._convert_encoding", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_transfo_xl._TransfoXLDelimiterLookupTokenizer.encode_batch", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizer.encode", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizer.encode"], ["", "def", "batch_encode_plus", "(", "\n", "self", ",", "\n", "batch_text_or_text_pairs", ":", "Optional", "[", "Union", "[", "List", "[", "str", "]", ",", "List", "[", "Tuple", "[", "str", "]", "]", "]", "]", "=", "None", ",", "\n", "add_special_tokens", ":", "bool", "=", "True", ",", "\n", "max_length", ":", "Optional", "[", "int", "]", "=", "None", ",", "\n", "stride", ":", "int", "=", "0", ",", "\n", "truncation_strategy", ":", "str", "=", "\"longest_first\"", ",", "\n", "pad_to_max_length", ":", "bool", "=", "False", ",", "\n", "return_tensors", ":", "Optional", "[", "str", "]", "=", "None", ",", "\n", "return_token_type_ids", ":", "Optional", "[", "bool", "]", "=", "None", ",", "\n", "return_attention_mask", ":", "Optional", "[", "bool", "]", "=", "None", ",", "\n", "return_overflowing_tokens", ":", "bool", "=", "False", ",", "\n", "return_special_tokens_mask", ":", "bool", "=", "False", ",", "\n", "return_offsets_mapping", ":", "bool", "=", "False", ",", "\n", "**", "kwargs", "\n", ")", ":", "\n", "        ", "if", "not", "add_special_tokens", ":", "\n", "            ", "logger", ".", "warning", "(", "\n", "\"Fast tokenizers add special tokens by default. To remove special tokens, please specify\"", "\n", "\"`add_special_tokens=False` during the initialisation rather than when calling `encode`,\"", "\n", "\"`encode_plus` or `batch_encode_plus`.\"", "\n", ")", "\n", "\n", "# Needed if we have to return a tensor", "\n", "", "pad_to_max_length", "=", "pad_to_max_length", "or", "(", "return_tensors", "is", "not", "None", ")", "\n", "\n", "# Throw an error if we can pad because there is no padding token", "\n", "if", "pad_to_max_length", "and", "self", ".", "pad_token_id", "is", "None", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"Unable to set proper padding strategy as the tokenizer does not have a padding token\"", "\n", ")", "\n", "\n", "# Set the truncation and padding strategy and restore the initial configuration", "\n", "", "with", "truncate_and_pad", "(", "\n", "tokenizer", "=", "self", ".", "_tokenizer", ",", "\n", "max_length", "=", "max_length", ",", "\n", "stride", "=", "stride", ",", "\n", "strategy", "=", "truncation_strategy", ",", "\n", "pad_to_max_length", "=", "pad_to_max_length", ",", "\n", "padding_side", "=", "self", ".", "padding_side", ",", "\n", "pad_token_id", "=", "self", ".", "pad_token_id", ",", "\n", "pad_token_type_id", "=", "self", ".", "pad_token_type_id", ",", "\n", "pad_token", "=", "self", ".", "_pad_token", ",", "\n", ")", ":", "\n", "\n", "            ", "if", "not", "isinstance", "(", "batch_text_or_text_pairs", ",", "list", ")", ":", "\n", "                ", "raise", "TypeError", "(", "\n", "\"batch_text_or_text_pairs has to be a list (got {})\"", ".", "format", "(", "\n", "type", "(", "batch_text_or_text_pairs", ")", "\n", ")", "\n", ")", "\n", "\n", "# Avoid thread overhead if only one example.", "\n", "", "if", "len", "(", "batch_text_or_text_pairs", ")", "==", "1", ":", "\n", "                ", "if", "isinstance", "(", "batch_text_or_text_pairs", "[", "0", "]", ",", "(", "tuple", ",", "list", ")", ")", ":", "\n", "                    ", "tokens", "=", "self", ".", "_tokenizer", ".", "encode", "(", "*", "batch_text_or_text_pairs", "[", "0", "]", ")", "\n", "", "else", ":", "\n", "                    ", "tokens", "=", "self", ".", "_tokenizer", ".", "encode", "(", "batch_text_or_text_pairs", "[", "0", "]", ")", "\n", "", "tokens", "=", "[", "tokens", "]", "\n", "", "else", ":", "\n", "                ", "tokens", "=", "self", ".", "_tokenizer", ".", "encode_batch", "(", "batch_text_or_text_pairs", ")", "\n", "\n", "# Convert encoding to dict", "\n", "", "", "tokens", "=", "[", "\n", "self", ".", "_convert_encoding", "(", "\n", "encoding", "=", "encoding", ",", "\n", "return_tensors", "=", "return_tensors", ",", "\n", "return_token_type_ids", "=", "return_token_type_ids", ",", "\n", "return_attention_mask", "=", "return_attention_mask", ",", "\n", "return_overflowing_tokens", "=", "return_overflowing_tokens", ",", "\n", "return_special_tokens_mask", "=", "return_special_tokens_mask", ",", "\n", "return_offsets_mapping", "=", "return_offsets_mapping", ",", "\n", ")", "\n", "for", "encoding", "in", "tokens", "\n", "]", "\n", "\n", "# Sanitize the output to have dict[list] from list[dict]", "\n", "sanitized", "=", "{", "}", "\n", "for", "key", "in", "tokens", "[", "0", "]", ".", "keys", "(", ")", ":", "\n", "            ", "stack", "=", "[", "e", "for", "item", "in", "tokens", "for", "e", "in", "item", "[", "key", "]", "]", "\n", "if", "return_tensors", "==", "\"tf\"", ":", "\n", "                ", "stack", "=", "tf", ".", "stack", "(", "stack", ",", "axis", "=", "0", ")", "\n", "", "elif", "return_tensors", "==", "\"pt\"", ":", "\n", "                ", "stack", "=", "torch", ".", "stack", "(", "stack", ",", "dim", "=", "0", ")", "\n", "", "elif", "not", "return_tensors", "and", "len", "(", "stack", ")", "==", "1", ":", "\n", "                ", "stack", "=", "stack", "[", "0", "]", "\n", "\n", "", "sanitized", "[", "key", "]", "=", "stack", "\n", "\n", "# If returning overflowing tokens, we need to return a mapping", "\n", "# from the batch idx to the original sample", "\n", "", "if", "return_overflowing_tokens", ":", "\n", "            ", "overflow_to_sample_mapping", "=", "[", "\n", "i", "if", "len", "(", "item", "[", "\"input_ids\"", "]", ")", "==", "1", "else", "[", "i", "]", "*", "len", "(", "item", "[", "\"input_ids\"", "]", ")", "\n", "for", "i", ",", "item", "in", "enumerate", "(", "tokens", ")", "\n", "]", "\n", "sanitized", "[", "\"overflow_to_sample_mapping\"", "]", "=", "overflow_to_sample_mapping", "\n", "", "return", "sanitized", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizerFast.encode_plus": [[2092, 2134], ["tokenization_utils.PreTrainedTokenizerFast.batch_encode_plus", "isinstance", "tokenization_utils.PreTrainedTokenizerFast.items"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizerFast.batch_encode_plus"], ["", "def", "encode_plus", "(", "\n", "self", ",", "\n", "text", ":", "str", ",", "\n", "text_pair", ":", "Optional", "[", "str", "]", "=", "None", ",", "\n", "add_special_tokens", ":", "bool", "=", "False", ",", "\n", "max_length", ":", "Optional", "[", "int", "]", "=", "None", ",", "\n", "pad_to_max_length", ":", "bool", "=", "False", ",", "\n", "stride", ":", "int", "=", "0", ",", "\n", "truncation_strategy", ":", "str", "=", "\"longest_first\"", ",", "\n", "return_tensors", ":", "Optional", "[", "bool", "]", "=", "None", ",", "\n", "return_token_type_ids", ":", "Optional", "[", "bool", "]", "=", "None", ",", "\n", "return_attention_mask", ":", "Optional", "[", "bool", "]", "=", "None", ",", "\n", "return_overflowing_tokens", ":", "bool", "=", "False", ",", "\n", "return_special_tokens_mask", ":", "bool", "=", "False", ",", "\n", "return_offsets_mapping", ":", "bool", "=", "False", ",", "\n", "**", "kwargs", "\n", ")", ":", "\n", "        ", "batched_input", "=", "[", "(", "text", ",", "text_pair", ")", "]", "if", "text_pair", "else", "[", "text", "]", "\n", "batched_output", "=", "self", ".", "batch_encode_plus", "(", "\n", "batched_input", ",", "\n", "add_special_tokens", "=", "add_special_tokens", ",", "\n", "max_length", "=", "max_length", ",", "\n", "stride", "=", "stride", ",", "\n", "truncation_strategy", "=", "truncation_strategy", ",", "\n", "return_tensors", "=", "return_tensors", ",", "\n", "return_token_type_ids", "=", "return_token_type_ids", ",", "\n", "return_attention_mask", "=", "return_attention_mask", ",", "\n", "return_overflowing_tokens", "=", "return_overflowing_tokens", ",", "\n", "return_special_tokens_mask", "=", "return_special_tokens_mask", ",", "\n", "return_offsets_mapping", "=", "return_offsets_mapping", ",", "\n", "pad_to_max_length", "=", "pad_to_max_length", ",", "\n", "**", "kwargs", ",", "\n", ")", "\n", "\n", "# Return tensor is None, then we can remove the leading batch axis", "\n", "if", "not", "return_tensors", ":", "\n", "            ", "return", "{", "\n", "key", ":", "value", "[", "0", "]", "if", "isinstance", "(", "value", "[", "0", "]", ",", "list", ")", "else", "value", "\n", "for", "key", ",", "value", "in", "batched_output", ".", "items", "(", ")", "\n", "}", "\n", "", "else", ":", "\n", "            ", "return", "batched_output", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizerFast.decode": [[2135, 2145], ["tokenization_utils.PreTrainedTokenizerFast.tokenizer.decode", "tokenization_utils.PreTrainedTokenizerFast.clean_up_tokenization"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizerFast.decode", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizer.clean_up_tokenization"], ["", "", "def", "decode", "(", "\n", "self", ",", "token_ids", ",", "skip_special_tokens", "=", "False", ",", "clean_up_tokenization_spaces", "=", "True", "\n", ")", ":", "\n", "        ", "text", "=", "self", ".", "tokenizer", ".", "decode", "(", "token_ids", ",", "skip_special_tokens", ")", "\n", "\n", "if", "clean_up_tokenization_spaces", ":", "\n", "            ", "clean_text", "=", "self", ".", "clean_up_tokenization", "(", "text", ")", "\n", "return", "clean_text", "\n", "", "else", ":", "\n", "            ", "return", "text", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizerFast.save_vocabulary": [[2146, 2154], ["os.path.isdir", "tuple", "tokenization_utils.PreTrainedTokenizerFast._tokenizer.save", "os.path.split", "tokenization_utils.PreTrainedTokenizerFast._tokenizer.save", "os.path.abspath"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.utils.sentence_splitter.PunktSentenceSplitter.split"], ["", "", "def", "save_vocabulary", "(", "self", ",", "save_directory", ")", ":", "\n", "        ", "if", "os", ".", "path", ".", "isdir", "(", "save_directory", ")", ":", "\n", "            ", "files", "=", "self", ".", "_tokenizer", ".", "save", "(", "save_directory", ")", "\n", "", "else", ":", "\n", "            ", "folder", ",", "file", "=", "os", ".", "path", ".", "split", "(", "os", ".", "path", ".", "abspath", "(", "save_directory", ")", ")", "\n", "files", "=", "self", ".", "_tokenizer", ".", "save", "(", "folder", ",", "name", "=", "file", ")", "\n", "\n", "", "return", "tuple", "(", "files", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.truncate_and_pad": [[50, 110], ["tokenizer.enable_truncation", "tokenizer.enable_padding", "tokenizer.no_truncation", "tokenizer.no_padding", "logger.warning"], "function", ["None"], ["@", "contextmanager", "\n", "def", "truncate_and_pad", "(", "\n", "tokenizer", ":", "BaseTokenizer", ",", "\n", "max_length", ":", "int", ",", "\n", "stride", ":", "int", ",", "\n", "strategy", ":", "str", ",", "\n", "pad_to_max_length", ":", "bool", ",", "\n", "padding_side", ":", "str", ",", "\n", "pad_token_id", ":", "int", ",", "\n", "pad_token_type_id", ":", "int", ",", "\n", "pad_token", ":", "str", ",", "\n", ")", ":", "\n", "    ", "\"\"\"\n    This contextmanager is in charge of defining the truncation and the padding strategies and then\n    restore the tokenizer settings afterwards.\n\n    This contextmanager assumes the provider tokenizer has no padding / truncation strategy\n    before the managed section. If your tokenizer set a padding / truncation strategy before,\n    then it will be reset to no padding/truncation when exiting the managed section.\n\n    :param tokenizer:\n    :param max_length:\n    :param stride:\n    :param strategy:\n    :param pad_to_max_length:\n    :param padding_side:\n    :param pad_token_id:\n    :param pad_token_type_id:\n    :param pad_token:\n    :return:\n    \"\"\"", "\n", "\n", "# Handle all the truncation and padding stuff", "\n", "if", "max_length", "is", "not", "None", ":", "\n", "        ", "tokenizer", ".", "enable_truncation", "(", "max_length", ",", "stride", "=", "stride", ",", "strategy", "=", "strategy", ")", "\n", "\n", "", "if", "pad_to_max_length", "and", "(", "pad_token", "and", "pad_token_id", ">=", "0", ")", ":", "\n", "        ", "tokenizer", ".", "enable_padding", "(", "\n", "max_length", "=", "max_length", ",", "\n", "direction", "=", "padding_side", ",", "\n", "pad_id", "=", "pad_token_id", ",", "\n", "pad_type_id", "=", "pad_token_type_id", ",", "\n", "pad_token", "=", "pad_token", ",", "\n", ")", "\n", "", "elif", "pad_to_max_length", ":", "\n", "        ", "logger", ".", "warning", "(", "\n", "\"Disabled padding because no padding token set (pad_token: {}, pad_token_id: {}).\\n\"", "\n", "\"To remove this error, you can add a new pad token and then resize model embedding:\\n\"", "\n", "\"\\ttokenizer.pad_token = '<PAD>'\\n\\tmodel.resize_token_embeddings(len(tokenizer))\"", ".", "format", "(", "\n", "pad_token", ",", "pad_token_id", "\n", ")", "\n", ")", "\n", "\n", "", "yield", "\n", "\n", "if", "max_length", "is", "not", "None", ":", "\n", "        ", "tokenizer", ".", "no_truncation", "(", ")", "\n", "\n", "", "if", "pad_to_max_length", "and", "(", "pad_token", "and", "pad_token_id", ">=", "0", ")", ":", "\n", "        ", "tokenizer", ".", "no_padding", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.trim_batch": [[2156, 2167], ["input_ids.ne().any", "input_ids.ne"], "function", ["None"], ["", "", "def", "trim_batch", "(", "\n", "input_ids", ",", "\n", "pad_token_id", ",", "\n", "attention_mask", "=", "None", ",", "\n", ")", ":", "\n", "    ", "\"\"\"Remove columns that are populated exclusively by pad_token_id\"\"\"", "\n", "keep_column_mask", "=", "input_ids", ".", "ne", "(", "pad_token_id", ")", ".", "any", "(", "dim", "=", "0", ")", "\n", "if", "attention_mask", "is", "None", ":", "\n", "        ", "return", "input_ids", "[", ":", ",", "keep_column_mask", "]", "\n", "", "else", ":", "\n", "        ", "return", "(", "input_ids", "[", ":", ",", "keep_column_mask", "]", ",", "attention_mask", "[", ":", ",", "keep_column_mask", "]", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.pyrouge.Rouge155.Rouge155.__init__": [[74, 95], ["Rouge155.Rouge155.__set_dir_properties", "Rouge155.Rouge155.__get_config_path", "Rouge155.Rouge155.__set_rouge_dir", "Rouge155.Rouge155.__clean_rouge_args", "utils.log.get_global_console_logger", "utils.log.get_global_console_logger"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.pyrouge.Rouge155.Rouge155.__set_dir_properties", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.pyrouge.Rouge155.Rouge155.__get_config_path", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.pyrouge.Rouge155.Rouge155.__set_rouge_dir", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.pyrouge.Rouge155.Rouge155.__clean_rouge_args", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.utils.log.get_global_console_logger", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.utils.log.get_global_console_logger"], ["def", "__init__", "(", "self", ",", "rouge_dir", "=", "None", ",", "rouge_args", "=", "None", ",", "log_level", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Create a Rouge155 object.\n\n            rouge_dir:  Directory containing Rouge-1.5.5.pl\n            rouge_args: Arguments to pass through to ROUGE if you\n                        don't want to use the default pyrouge\n                        arguments.\n\n        \"\"\"", "\n", "if", "log_level", "is", "None", ":", "\n", "            ", "self", ".", "log", "=", "log", ".", "get_global_console_logger", "(", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "log", "=", "log", ".", "get_global_console_logger", "(", "log_level", ")", "\n", "", "self", ".", "__set_dir_properties", "(", ")", "\n", "self", ".", "_config_file", "=", "None", "\n", "self", ".", "_settings_file", "=", "self", ".", "__get_config_path", "(", ")", "\n", "self", ".", "__set_rouge_dir", "(", "rouge_dir", ")", "\n", "self", ".", "args", "=", "self", ".", "__clean_rouge_args", "(", "rouge_args", ")", "\n", "self", ".", "_system_filename_pattern", "=", "None", "\n", "self", ".", "_model_filename_pattern", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.pyrouge.Rouge155.Rouge155.save_home_dir": [[96, 104], ["ConfigParser", "ConfigParser.add_section", "ConfigParser.set", "Rouge155.Rouge155.log.info", "open", "ConfigParser.write"], "methods", ["None"], ["", "def", "save_home_dir", "(", "self", ")", ":", "\n", "        ", "config", "=", "ConfigParser", "(", ")", "\n", "section", "=", "\"pyrouge settings\"", "\n", "config", ".", "add_section", "(", "section", ")", "\n", "config", ".", "set", "(", "section", ",", "\"home_dir\"", ",", "self", ".", "_home_dir", ")", "\n", "with", "open", "(", "self", ".", "_settings_file", ",", "\"w\"", ")", "as", "f", ":", "\n", "            ", "config", ".", "write", "(", "f", ")", "\n", "", "self", ".", "log", ".", "info", "(", "\"Set ROUGE home directory to {}.\"", ".", "format", "(", "self", ".", "_home_dir", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.pyrouge.Rouge155.Rouge155.settings_file": [[105, 112], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "settings_file", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Path of the setttings file, which stores the ROUGE home dir.\n\n        \"\"\"", "\n", "return", "self", ".", "_settings_file", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.pyrouge.Rouge155.Rouge155.bin_path": [[113, 126], ["Exception"], "methods", ["None"], ["", "@", "property", "\n", "def", "bin_path", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        The full path of the ROUGE binary (although it's technically\n        a script), i.e. rouge_home_dir/ROUGE-1.5.5.pl\n\n        \"\"\"", "\n", "if", "self", ".", "_bin_path", "is", "None", ":", "\n", "            ", "raise", "Exception", "(", "\n", "\"ROUGE path not set. Please set the ROUGE home directory \"", "\n", "\"and ensure that ROUGE-1.5.5.pl exists in it.\"", "\n", ")", "\n", "", "return", "self", ".", "_bin_path", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.pyrouge.Rouge155.Rouge155.system_filename_pattern": [[142, 145], ["None"], "methods", ["None"], ["", "@", "system_filename_pattern", ".", "setter", "\n", "def", "system_filename_pattern", "(", "self", ",", "pattern", ")", ":", "\n", "        ", "self", ".", "_system_filename_pattern", "=", "pattern", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.pyrouge.Rouge155.Rouge155.model_filename_pattern": [[165, 168], ["None"], "methods", ["None"], ["", "@", "model_filename_pattern", ".", "setter", "\n", "def", "model_filename_pattern", "(", "self", ",", "pattern", ")", ":", "\n", "        ", "self", ".", "_model_filename_pattern", "=", "pattern", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.pyrouge.Rouge155.Rouge155.config_file": [[173, 178], ["os.path.split", "utils.file_utils.verify_dir"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.utils.sentence_splitter.PunktSentenceSplitter.split", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.utils.file_utils.verify_dir"], ["", "@", "config_file", ".", "setter", "\n", "def", "config_file", "(", "self", ",", "path", ")", ":", "\n", "        ", "config_dir", ",", "_", "=", "os", ".", "path", ".", "split", "(", "path", ")", "\n", "verify_dir", "(", "config_dir", ",", "\"configuration file\"", ")", "\n", "self", ".", "_config_file", "=", "path", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.pyrouge.Rouge155.Rouge155.split_sentences": [[179, 194], ["Rouge155.Rouge155.log.info", "PunktSentenceSplitter", "functools.partial", "Rouge155.Rouge155.__process_summaries", "PunktSentenceSplitter.split"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.pyrouge.Rouge155.Rouge155.__process_summaries", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.utils.sentence_splitter.PunktSentenceSplitter.split"], ["", "def", "split_sentences", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        ROUGE requires texts split into sentences. In case the texts\n        are not already split, this method can be used.\n\n        \"\"\"", "\n", "from", "pyrouge", ".", "utils", ".", "sentence_splitter", "import", "PunktSentenceSplitter", "\n", "\n", "self", ".", "log", ".", "info", "(", "\"Splitting sentences.\"", ")", "\n", "ss", "=", "PunktSentenceSplitter", "(", ")", "\n", "sent_split_to_string", "=", "lambda", "s", ":", "\"\\n\"", ".", "join", "(", "ss", ".", "split", "(", "s", ")", ")", "\n", "process_func", "=", "partial", "(", "\n", "DirectoryProcessor", ".", "process", ",", "function", "=", "sent_split_to_string", "\n", ")", "\n", "self", ".", "__process_summaries", "(", "process_func", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.pyrouge.Rouge155.Rouge155.convert_summaries_to_rouge_format": [[195, 209], ["utils.file_utils.DirectoryProcessor.process"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.util.massivesumm_utils.Article.process"], ["", "@", "staticmethod", "\n", "def", "convert_summaries_to_rouge_format", "(", "input_dir", ",", "output_dir", ")", ":", "\n", "        ", "\"\"\"\n        Convert all files in input_dir into a format ROUGE understands\n        and saves the files to output_dir. The input files are assumed\n        to be plain text with one sentence per line.\n\n            input_dir:  Path of directory containing the input files.\n            output_dir: Path of directory in which the converted files\n                        will be saved.\n\n        \"\"\"", "\n", "DirectoryProcessor", ".", "process", "(", "\n", "input_dir", ",", "output_dir", ",", "Rouge155", ".", "convert_text_to_rouge_format", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.pyrouge.Rouge155.Rouge155.convert_text_to_rouge_format": [[211, 243], ["text.split", "enumerate"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.utils.sentence_splitter.PunktSentenceSplitter.split"], ["", "@", "staticmethod", "\n", "def", "convert_text_to_rouge_format", "(", "text", ",", "title", "=", "\"dummy title\"", ")", ":", "\n", "        ", "\"\"\"\n        Convert a text to a format ROUGE understands. The text is\n        assumed to contain one sentence per line.\n\n            text:   The text to convert, containg one sentence per line.\n            title:  Optional title for the text. The title will appear\n                    in the converted file, but doesn't seem to have\n                    any other relevance.\n\n        Returns: The converted text as string.\n\n        \"\"\"", "\n", "sentences", "=", "text", ".", "split", "(", "\"\\n\"", ")", "\n", "sent_elems", "=", "[", "\n", "'<a name=\"{i}\">[{i}]</a> <a href=\"#{i}\" id={i}>'", "\n", "\"{text}</a>\"", ".", "format", "(", "i", "=", "i", ",", "text", "=", "sent", ")", "\n", "for", "i", ",", "sent", "in", "enumerate", "(", "sentences", ",", "start", "=", "1", ")", "\n", "]", "\n", "html", "=", "\"\"\"<html>\n<head>\n<title>{title}</title>\n</head>\n<body bgcolor=\"white\">\n{elems}\n</body>\n</html>\"\"\"", ".", "format", "(", "\n", "title", "=", "title", ",", "elems", "=", "\"\\n\"", ".", "join", "(", "sent_elems", ")", "\n", ")", "\n", "\n", "return", "html", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.pyrouge.Rouge155.Rouge155.write_config_static": [[244, 312], ["re.compile", "sorted", "re.compile.match", "Exception", "codecs.open", "f.write", "enumerate", "f.write", "os.listdir", "Rouge155.__get_model_filenames_for_id", "system_models_tuples.append", "Rouge155.__get_eval_string", "f.write", "re.compile.match.groups", "sorted"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.pyrouge.Rouge155.Rouge155.__get_model_filenames_for_id", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.pyrouge.Rouge155.Rouge155.__get_eval_string"], ["", "@", "staticmethod", "\n", "def", "write_config_static", "(", "\n", "system_dir", ",", "\n", "system_filename_pattern", ",", "\n", "model_dir", ",", "\n", "model_filename_pattern", ",", "\n", "config_file_path", ",", "\n", "system_id", "=", "None", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Write the ROUGE configuration file, which is basically a list\n        of system summary files and their corresponding model summary\n        files.\n\n        pyrouge uses regular expressions to automatically find the\n        matching model summary files for a given system summary file\n        (cf. docstrings for system_filename_pattern and\n        model_filename_pattern).\n\n            system_dir:                 Path of directory containing\n                                        system summaries.\n            system_filename_pattern:    Regex string for matching\n                                        system summary filenames.\n            model_dir:                  Path of directory containing\n                                        model summaries.\n            model_filename_pattern:     Regex string for matching model\n                                        summary filenames.\n            config_file_path:           Path of the configuration file.\n            system_id:                  Optional system ID string which\n                                        will appear in the ROUGE output.\n\n        \"\"\"", "\n", "system_filenames", "=", "[", "f", "for", "f", "in", "os", ".", "listdir", "(", "system_dir", ")", "]", "\n", "system_models_tuples", "=", "[", "]", "\n", "\n", "system_filename_pattern", "=", "re", ".", "compile", "(", "system_filename_pattern", ")", "\n", "for", "system_filename", "in", "sorted", "(", "system_filenames", ")", ":", "\n", "            ", "match", "=", "system_filename_pattern", ".", "match", "(", "system_filename", ")", "\n", "if", "match", ":", "\n", "                ", "id", "=", "match", ".", "groups", "(", "0", ")", "[", "0", "]", "\n", "model_filenames", "=", "Rouge155", ".", "__get_model_filenames_for_id", "(", "\n", "id", ",", "model_dir", ",", "model_filename_pattern", "\n", ")", "\n", "system_models_tuples", ".", "append", "(", "(", "system_filename", ",", "sorted", "(", "model_filenames", ")", ")", ")", "\n", "", "", "if", "not", "system_models_tuples", ":", "\n", "            ", "raise", "Exception", "(", "\n", "\"Did not find any files matching the pattern {} \"", "\n", "\"in the system summaries directory {}.\"", ".", "format", "(", "\n", "system_filename_pattern", ".", "pattern", ",", "system_dir", "\n", ")", "\n", ")", "\n", "\n", "", "with", "codecs", ".", "open", "(", "config_file_path", ",", "\"w\"", ",", "encoding", "=", "\"utf-8\"", ")", "as", "f", ":", "\n", "            ", "f", ".", "write", "(", "'<ROUGE-EVAL version=\"1.55\">'", ")", "\n", "for", "task_id", ",", "(", "system_filename", ",", "model_filenames", ")", "in", "enumerate", "(", "\n", "system_models_tuples", ",", "start", "=", "1", "\n", ")", ":", "\n", "\n", "                ", "eval_string", "=", "Rouge155", ".", "__get_eval_string", "(", "\n", "task_id", ",", "\n", "system_id", ",", "\n", "system_dir", ",", "\n", "system_filename", ",", "\n", "model_dir", ",", "\n", "model_filenames", ",", "\n", ")", "\n", "f", ".", "write", "(", "eval_string", ")", "\n", "", "f", ".", "write", "(", "\"</ROUGE-EVAL>\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.pyrouge.Rouge155.Rouge155.write_config": [[313, 343], ["os.path.join", "Rouge155.write_config_static", "Rouge155.Rouge155.log.info", "tempfile.mkdtemp", "os.path.split", "utils.file_utils.verify_dir"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.pyrouge.Rouge155.Rouge155.write_config_static", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.utils.sentence_splitter.PunktSentenceSplitter.split", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.utils.file_utils.verify_dir"], ["", "", "def", "write_config", "(", "self", ",", "config_file_path", "=", "None", ",", "system_id", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Write the ROUGE configuration file, which is basically a list\n        of system summary files and their matching model summary files.\n\n        This is a non-static version of write_config_file_static().\n\n            config_file_path:   Path of the configuration file.\n            system_id:          Optional system ID string which will\n                                appear in the ROUGE output.\n\n        \"\"\"", "\n", "if", "not", "system_id", ":", "\n", "            ", "system_id", "=", "1", "\n", "", "if", "(", "not", "config_file_path", ")", "or", "(", "not", "self", ".", "_config_dir", ")", ":", "\n", "            ", "self", ".", "_config_dir", "=", "mkdtemp", "(", ")", "\n", "config_filename", "=", "\"rouge_conf.xml\"", "\n", "", "else", ":", "\n", "            ", "config_dir", ",", "config_filename", "=", "os", ".", "path", ".", "split", "(", "config_file_path", ")", "\n", "verify_dir", "(", "config_dir", ",", "\"configuration file\"", ")", "\n", "", "self", ".", "_config_file", "=", "os", ".", "path", ".", "join", "(", "self", ".", "_config_dir", ",", "config_filename", ")", "\n", "Rouge155", ".", "write_config_static", "(", "\n", "self", ".", "_system_dir", ",", "\n", "self", ".", "_system_filename_pattern", ",", "\n", "self", ".", "_model_dir", ",", "\n", "self", ".", "_model_filename_pattern", ",", "\n", "self", ".", "_config_file", ",", "\n", "system_id", ",", "\n", ")", "\n", "self", ".", "log", ".", "info", "(", "\"Written ROUGE configuration to {}\"", ".", "format", "(", "self", ".", "_config_file", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.pyrouge.Rouge155.Rouge155.evaluate": [[344, 365], ["Rouge155.Rouge155.write_config", "Rouge155.Rouge155.__get_options", "os.environ.copy", "Rouge155.Rouge155.log.info", "subprocess.check_output().decode", "hasattr", "subprocess.check_output"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.pyrouge.Rouge155.Rouge155.write_config", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.pyrouge.Rouge155.Rouge155.__get_options", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizerFast.decode"], ["", "def", "evaluate", "(", "self", ",", "system_id", "=", "1", ",", "rouge_args", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Run ROUGE to evaluate the system summaries in system_dir against\n        the model summaries in model_dir. The summaries are assumed to\n        be in the one-sentence-per-line HTML format ROUGE understands.\n\n            system_id:  Optional system ID which will be printed in\n                        ROUGE's output.\n\n        Returns: Rouge output as string.\n\n        \"\"\"", "\n", "self", ".", "write_config", "(", "system_id", "=", "system_id", ")", "\n", "options", "=", "self", ".", "__get_options", "(", "rouge_args", ")", "\n", "command", "=", "[", "self", ".", "_bin_path", "]", "+", "options", "\n", "env", "=", "os", ".", "environ", ".", "copy", "(", ")", "\n", "if", "hasattr", "(", "self", ",", "\"_home_dir\"", ")", "and", "self", ".", "_home_dir", ":", "\n", "            ", "env", "[", "\"ROUGE_EVAL_HOME\"", "]", "=", "self", ".", "_home_dir", "\n", "", "self", ".", "log", ".", "info", "(", "\"Running ROUGE with command {}\"", ".", "format", "(", "\" \"", ".", "join", "(", "command", ")", ")", ")", "\n", "rouge_output", "=", "check_output", "(", "command", ",", "env", "=", "env", ")", ".", "decode", "(", "\"UTF-8\"", ")", "\n", "return", "rouge_output", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.pyrouge.Rouge155.Rouge155.convert_and_evaluate": [[366, 389], ["Rouge155.Rouge155.__write_summaries", "Rouge155.Rouge155.evaluate", "Rouge155.Rouge155.split_sentences"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.pyrouge.Rouge155.Rouge155.__write_summaries", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.evaluation.meteor_metric.Meteor.evaluate", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.pyrouge.Rouge155.Rouge155.split_sentences"], ["", "def", "convert_and_evaluate", "(", "self", ",", "system_id", "=", "1", ",", "split_sentences", "=", "False", ",", "rouge_args", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Convert plain text summaries to ROUGE format and run ROUGE to\n        evaluate the system summaries in system_dir against the model\n        summaries in model_dir. Optionally split texts into sentences\n        in case they aren't already.\n\n        This is just a convenience method combining\n        convert_summaries_to_rouge_format() and evaluate().\n\n            split_sentences:    Optional argument specifying if\n                                sentences should be split.\n            system_id:          Optional system ID which will be printed\n                                in ROUGE's output.\n\n        Returns: ROUGE output as string.\n\n        \"\"\"", "\n", "if", "split_sentences", ":", "\n", "            ", "self", ".", "split_sentences", "(", ")", "\n", "", "self", ".", "__write_summaries", "(", ")", "\n", "rouge_output", "=", "self", ".", "evaluate", "(", "system_id", ",", "rouge_args", ")", "\n", "return", "rouge_output", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.pyrouge.Rouge155.Rouge155.output_to_dict": [[390, 424], ["re.compile", "output.split", "re.compile.match", "re.compile.match.groups", "rouge_type.lower().replace.lower().replace.lower().replace", "float", "float", "float", "rouge_type.lower().replace.lower().replace.lower"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.utils.sentence_splitter.PunktSentenceSplitter.split"], ["", "def", "output_to_dict", "(", "self", ",", "output", ")", ":", "\n", "        ", "\"\"\"\n        Convert the ROUGE output into python dictionary for further\n        processing.\n\n        \"\"\"", "\n", "# 0 ROUGE-1 Average_R: 0.02632 (95%-conf.int. 0.02632 - 0.02632)", "\n", "pattern", "=", "re", ".", "compile", "(", "\n", "r\"(\\d+) (ROUGE-\\S+) (Average_\\w): (\\d.\\d+) \"", "\n", "r\"\\(95%-conf.int. (\\d.\\d+) - (\\d.\\d+)\\)\"", "\n", ")", "\n", "results", "=", "{", "}", "\n", "for", "line", "in", "output", ".", "split", "(", "\"\\n\"", ")", ":", "\n", "            ", "match", "=", "pattern", ".", "match", "(", "line", ")", "\n", "if", "match", ":", "\n", "                ", "(", "\n", "sys_id", ",", "\n", "rouge_type", ",", "\n", "measure", ",", "\n", "result", ",", "\n", "conf_begin", ",", "\n", "conf_end", ",", "\n", ")", "=", "match", ".", "groups", "(", ")", "\n", "measure", "=", "{", "\n", "\"Average_R\"", ":", "\"recall\"", ",", "\n", "\"Average_P\"", ":", "\"precision\"", ",", "\n", "\"Average_F\"", ":", "\"f_score\"", ",", "\n", "}", "[", "measure", "]", "\n", "rouge_type", "=", "rouge_type", ".", "lower", "(", ")", ".", "replace", "(", "\"-\"", ",", "\"_\"", ")", "\n", "key", "=", "\"{}_{}\"", ".", "format", "(", "rouge_type", ",", "measure", ")", "\n", "results", "[", "key", "]", "=", "float", "(", "result", ")", "\n", "results", "[", "\"{}_cb\"", ".", "format", "(", "key", ")", "]", "=", "float", "(", "conf_begin", ")", "\n", "results", "[", "\"{}_ce\"", ".", "format", "(", "key", ")", "]", "=", "float", "(", "conf_end", ")", "\n", "", "", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.pyrouge.Rouge155.Rouge155.__set_rouge_dir": [[428, 446], ["os.path.join", "os.path.join", "Rouge155.Rouge155.__get_rouge_home_dir_from_settings", "Rouge155.Rouge155.save_home_dir", "os.path.exists", "Exception"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.pyrouge.Rouge155.Rouge155.__get_rouge_home_dir_from_settings", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.pyrouge.Rouge155.Rouge155.save_home_dir"], ["", "def", "__set_rouge_dir", "(", "self", ",", "home_dir", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Verfify presence of ROUGE-1.5.5.pl and data folder, and set\n        those paths.\n\n        \"\"\"", "\n", "if", "not", "home_dir", ":", "\n", "            ", "self", ".", "_home_dir", "=", "self", ".", "__get_rouge_home_dir_from_settings", "(", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "_home_dir", "=", "home_dir", "\n", "self", ".", "save_home_dir", "(", ")", "\n", "", "self", ".", "_bin_path", "=", "os", ".", "path", ".", "join", "(", "self", ".", "_home_dir", ",", "\"ROUGE-1.5.5.pl\"", ")", "\n", "self", ".", "data_dir", "=", "os", ".", "path", ".", "join", "(", "self", ".", "_home_dir", ",", "\"data\"", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "self", ".", "_bin_path", ")", ":", "\n", "            ", "raise", "Exception", "(", "\n", "\"ROUGE binary not found at {}. Please set the \"", "\n", "\"correct path by running pyrouge_set_rouge_path \"", "\n", "\"/path/to/rouge/home.\"", ".", "format", "(", "self", ".", "_bin_path", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.pyrouge.Rouge155.Rouge155.__get_rouge_home_dir_from_settings": [[448, 458], ["ConfigParser", "ConfigParser.get", "open", "hasattr", "ConfigParser.read_file", "ConfigParser.readfp"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.closablequeue.ClosableQueue.get"], ["", "", "def", "__get_rouge_home_dir_from_settings", "(", "self", ")", ":", "\n", "        ", "config", "=", "ConfigParser", "(", ")", "\n", "with", "open", "(", "self", ".", "_settings_file", ")", "as", "f", ":", "\n", "            ", "if", "hasattr", "(", "config", ",", "\"read_file\"", ")", ":", "\n", "                ", "config", ".", "read_file", "(", "f", ")", "\n", "", "else", ":", "\n", "# use deprecated python 2.x method", "\n", "                ", "config", ".", "readfp", "(", "f", ")", "\n", "", "", "rouge_home_dir", "=", "config", ".", "get", "(", "\"pyrouge settings\"", ",", "\"home_dir\"", ")", "\n", "return", "rouge_home_dir", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.pyrouge.Rouge155.Rouge155.__get_eval_string": [[459, 504], ["enumerate", "chr"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "__get_eval_string", "(", "\n", "task_id", ",", "system_id", ",", "system_dir", ",", "system_filename", ",", "model_dir", ",", "model_filenames", "\n", ")", ":", "\n", "        ", "\"\"\"\n        ROUGE can evaluate several system summaries for a given text\n        against several model summaries, i.e. there is an m-to-n\n        relation between system and model summaries. The system\n        summaries are listed in the <PEERS> tag and the model summaries\n        in the <MODELS> tag. pyrouge currently only supports one system\n        summary per text, i.e. it assumes a 1-to-n relation between\n        system and model summaries.\n\n        \"\"\"", "\n", "peer_elems", "=", "'<P ID=\"{id}\">{name}</P>'", ".", "format", "(", "\n", "id", "=", "system_id", ",", "name", "=", "system_filename", "\n", ")", "\n", "\n", "model_elems", "=", "[", "\n", "'<M ID=\"{id}\">{name}</M>'", ".", "format", "(", "id", "=", "chr", "(", "65", "+", "i", ")", ",", "name", "=", "name", ")", "\n", "for", "i", ",", "name", "in", "enumerate", "(", "model_filenames", ")", "\n", "]", "\n", "\n", "model_elems", "=", "\"\\n\\t\\t\\t\"", ".", "join", "(", "model_elems", ")", "\n", "eval_string", "=", "\"\"\"\n    <EVAL ID=\"{task_id}\">\n        <MODEL-ROOT>{model_root}</MODEL-ROOT>\n        <PEER-ROOT>{peer_root}</PEER-ROOT>\n        <INPUT-FORMAT TYPE=\"SEE\">\n        </INPUT-FORMAT>\n        <PEERS>\n            {peer_elems}\n        </PEERS>\n        <MODELS>\n            {model_elems}\n        </MODELS>\n    </EVAL>\n\"\"\"", ".", "format", "(", "\n", "task_id", "=", "task_id", ",", "\n", "model_root", "=", "model_dir", ",", "\n", "model_elems", "=", "model_elems", ",", "\n", "peer_root", "=", "system_dir", ",", "\n", "peer_elems", "=", "peer_elems", ",", "\n", ")", "\n", "return", "eval_string", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.pyrouge.Rouge155.Rouge155.__process_summaries": [[505, 525], ["tempfile.mkdtemp", "os.path.join", "os.mkdir", "os.path.join", "os.mkdir", "Rouge155.Rouge155.log.info", "process_func", "process_func"], "methods", ["None"], ["", "def", "__process_summaries", "(", "self", ",", "process_func", ")", ":", "\n", "        ", "\"\"\"\n        Helper method that applies process_func to the files in the\n        system and model folders and saves the resulting files to new\n        system and model folders.\n\n        \"\"\"", "\n", "temp_dir", "=", "mkdtemp", "(", ")", "\n", "new_system_dir", "=", "os", ".", "path", ".", "join", "(", "temp_dir", ",", "\"system\"", ")", "\n", "os", ".", "mkdir", "(", "new_system_dir", ")", "\n", "new_model_dir", "=", "os", ".", "path", ".", "join", "(", "temp_dir", ",", "\"model\"", ")", "\n", "os", ".", "mkdir", "(", "new_model_dir", ")", "\n", "self", ".", "log", ".", "info", "(", "\n", "\"Processing summaries. Saving system files to {} and \"", "\n", "\"model files to {}.\"", ".", "format", "(", "new_system_dir", ",", "new_model_dir", ")", "\n", ")", "\n", "process_func", "(", "self", ".", "_system_dir", ",", "new_system_dir", ")", "\n", "process_func", "(", "self", ".", "_model_dir", ",", "new_model_dir", ")", "\n", "self", ".", "_system_dir", "=", "new_system_dir", "\n", "self", ".", "_model_dir", "=", "new_model_dir", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.pyrouge.Rouge155.Rouge155.__write_summaries": [[526, 529], ["Rouge155.Rouge155.log.info", "Rouge155.Rouge155.__process_summaries"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.pyrouge.Rouge155.Rouge155.__process_summaries"], ["", "def", "__write_summaries", "(", "self", ")", ":", "\n", "        ", "self", ".", "log", ".", "info", "(", "\"Writing summaries.\"", ")", "\n", "self", ".", "__process_summaries", "(", "self", ".", "convert_summaries_to_rouge_format", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.pyrouge.Rouge155.Rouge155.__get_model_filenames_for_id": [[530, 541], ["re.compile", "model_filenames_pattern.replace", "Exception", "os.listdir", "re.compile.match"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "__get_model_filenames_for_id", "(", "id", ",", "model_dir", ",", "model_filenames_pattern", ")", ":", "\n", "        ", "pattern", "=", "re", ".", "compile", "(", "model_filenames_pattern", ".", "replace", "(", "\"#ID#\"", ",", "id", ")", ")", "\n", "model_filenames", "=", "[", "f", "for", "f", "in", "os", ".", "listdir", "(", "model_dir", ")", "if", "pattern", ".", "match", "(", "f", ")", "]", "\n", "if", "not", "model_filenames", ":", "\n", "            ", "raise", "Exception", "(", "\n", "\"Could not find any model summaries for the system\"", "\n", "\" summary with ID {}. Specified model filename pattern was: \"", "\n", "\"{}\"", ".", "format", "(", "id", ",", "model_filenames_pattern", ")", "\n", ")", "\n", "", "return", "model_filenames", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.pyrouge.Rouge155.Rouge155.__get_options": [[542, 573], ["Rouge155.Rouge155.__add_config_option", "Rouge155.Rouge155.args.split", "rouge_args.split", "list", "map"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.pyrouge.Rouge155.Rouge155.__add_config_option", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.utils.sentence_splitter.PunktSentenceSplitter.split", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.utils.sentence_splitter.PunktSentenceSplitter.split"], ["", "def", "__get_options", "(", "self", ",", "rouge_args", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Get supplied command line arguments for ROUGE or use default\n        ones.\n\n        \"\"\"", "\n", "if", "self", ".", "args", ":", "\n", "            ", "options", "=", "self", ".", "args", ".", "split", "(", ")", "\n", "", "elif", "rouge_args", ":", "\n", "            ", "options", "=", "rouge_args", ".", "split", "(", ")", "\n", "", "else", ":", "\n", "            ", "options", "=", "[", "\n", "\"-e\"", ",", "\n", "self", ".", "_data_dir", ",", "\n", "\"-c\"", ",", "\n", "95", ",", "\n", "\"-2\"", ",", "\n", "\"-1\"", ",", "\n", "\"-U\"", ",", "\n", "\"-r\"", ",", "\n", "1000", ",", "\n", "\"-n\"", ",", "\n", "4", ",", "\n", "\"-w\"", ",", "\n", "1.2", ",", "\n", "\"-a\"", ",", "\n", "]", "\n", "options", "=", "list", "(", "map", "(", "str", ",", "options", ")", ")", "\n", "\n", "", "options", "=", "self", ".", "__add_config_option", "(", "options", ")", "\n", "return", "options", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.pyrouge.Rouge155.Rouge155.__create_dir_property": [[574, 592], ["setattr", "property", "setattr", "getattr", "utils.file_utils.verify_dir", "setattr"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.utils.file_utils.verify_dir"], ["", "def", "__create_dir_property", "(", "self", ",", "dir_name", ",", "docstring", ")", ":", "\n", "        ", "\"\"\"\n        Generate getter and setter for a directory property.\n\n        \"\"\"", "\n", "property_name", "=", "\"{}_dir\"", ".", "format", "(", "dir_name", ")", "\n", "private_name", "=", "\"_\"", "+", "property_name", "\n", "setattr", "(", "self", ",", "private_name", ",", "None", ")", "\n", "\n", "def", "fget", "(", "self", ")", ":", "\n", "            ", "return", "getattr", "(", "self", ",", "private_name", ")", "\n", "\n", "", "def", "fset", "(", "self", ",", "path", ")", ":", "\n", "            ", "verify_dir", "(", "path", ",", "dir_name", ")", "\n", "setattr", "(", "self", ",", "private_name", ",", "path", ")", "\n", "\n", "", "p", "=", "property", "(", "fget", "=", "fget", ",", "fset", "=", "fset", ",", "doc", "=", "docstring", ")", "\n", "setattr", "(", "self", ".", "__class__", ",", "property_name", ",", "p", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.pyrouge.Rouge155.Rouge155.__set_dir_properties": [[593, 606], ["Rouge155.Rouge155.__create_dir_property"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.pyrouge.Rouge155.Rouge155.__create_dir_property"], ["", "def", "__set_dir_properties", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Automatically generate the properties for directories.\n\n        \"\"\"", "\n", "directories", "=", "[", "\n", "(", "\"home\"", ",", "\"The ROUGE home directory.\"", ")", ",", "\n", "(", "\"data\"", ",", "\"The path of the ROUGE 'data' directory.\"", ")", ",", "\n", "(", "\"system\"", ",", "\"Path of the directory containing system summaries.\"", ")", ",", "\n", "(", "\"model\"", ",", "\"Path of the directory containing model summaries.\"", ")", ",", "\n", "]", "\n", "for", "(", "dirname", ",", "docstring", ")", "in", "directories", ":", "\n", "            ", "self", ".", "__create_dir_property", "(", "dirname", ",", "docstring", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.pyrouge.Rouge155.Rouge155.__clean_rouge_args": [[607, 621], ["re.compile", "re.compile.match", "re.compile.match.group"], "methods", ["None"], ["", "", "def", "__clean_rouge_args", "(", "self", ",", "rouge_args", ")", ":", "\n", "        ", "\"\"\"\n        Remove enclosing quotation marks, if any.\n\n        \"\"\"", "\n", "if", "not", "rouge_args", ":", "\n", "            ", "return", "\n", "", "quot_mark_pattern", "=", "re", ".", "compile", "(", "'\"(.+)\"'", ")", "\n", "match", "=", "quot_mark_pattern", ".", "match", "(", "rouge_args", ")", "\n", "if", "match", ":", "\n", "            ", "cleaned_args", "=", "match", ".", "group", "(", "1", ")", "\n", "return", "cleaned_args", "\n", "", "else", ":", "\n", "            ", "return", "rouge_args", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.pyrouge.Rouge155.Rouge155.__add_config_option": [[622, 624], ["None"], "methods", ["None"], ["", "", "def", "__add_config_option", "(", "self", ",", "options", ")", ":", "\n", "        ", "return", "options", "+", "[", "\"-m\"", "]", "+", "[", "self", ".", "_config_file", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.pyrouge.Rouge155.Rouge155.__get_config_path": [[625, 639], ["os.path.join", "os.path.join", "platform.system", "os.getenv", "os.path.exists", "os.makedirs", "os.path.expanduser", "os.path.dirname"], "methods", ["None"], ["", "def", "__get_config_path", "(", "self", ")", ":", "\n", "        ", "if", "platform", ".", "system", "(", ")", "==", "\"Windows\"", ":", "\n", "            ", "parent_dir", "=", "os", ".", "getenv", "(", "\"APPDATA\"", ")", "\n", "config_dir_name", "=", "\"pyrouge\"", "\n", "", "elif", "os", ".", "name", "==", "\"posix\"", ":", "\n", "            ", "parent_dir", "=", "os", ".", "path", ".", "expanduser", "(", "\"~\"", ")", "\n", "config_dir_name", "=", "\".pyrouge\"", "\n", "", "else", ":", "\n", "            ", "parent_dir", "=", "os", ".", "path", ".", "dirname", "(", "__file__", ")", "\n", "config_dir_name", "=", "\"\"", "\n", "", "config_dir", "=", "os", ".", "path", ".", "join", "(", "parent_dir", ",", "config_dir_name", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "config_dir", ")", ":", "\n", "            ", "os", ".", "makedirs", "(", "config_dir", ")", "\n", "", "return", "os", ".", "path", ".", "join", "(", "config_dir", ",", "\"settings.ini\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.utils.file_utils.DirectoryProcessor.process": [[10, 27], ["pyrouge.get_global_console_logger", "pyrouge.get_global_console_logger.info", "os.listdir", "pyrouge.get_global_console_logger.info", "os.path.exists", "os.makedirs", "pyrouge.get_global_console_logger.info", "os.path.join", "function", "os.path.join", "codecs.open", "f.read", "codecs.open", "f.write"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.utils.log.get_global_console_logger"], ["import", "os", "\n", "import", "shutil", "\n", "import", "sys", "\n", "import", "tarfile", "\n", "import", "tempfile", "\n", "from", "contextlib", "import", "contextmanager", "\n", "from", "functools", "import", "partial", ",", "wraps", "\n", "from", "hashlib", "import", "sha256", "\n", "from", "typing", "import", "Optional", "\n", "from", "urllib", ".", "parse", "import", "urlparse", "\n", "from", "zipfile", "import", "ZipFile", ",", "is_zipfile", "\n", "\n", "import", "boto3", "\n", "import", "requests", "\n", "from", "botocore", ".", "config", "import", "Config", "\n", "from", "botocore", ".", "exceptions", "import", "ClientError", "\n", "from", "filelock", "import", "FileLock", "\n", "from", "tqdm", ".", "auto", "import", "tqdm", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.utils.file_utils.str_from_file": [[28, 32], ["open", "f.read().strip", "f.read"], "function", ["None"], ["\n", "logger", "=", "logging", ".", "getLogger", "(", "__name__", ")", "# pylint: disable=invalid-name", "\n", "\n", "try", ":", "\n", "    ", "USE_TF", "=", "os", ".", "environ", ".", "get", "(", "\"USE_TF\"", ",", "\"AUTO\"", ")", ".", "upper", "(", ")", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.utils.file_utils.xml_equal": [[46, 63], ["xml.tostring().decode", "re.sub", "re.sub", "file_utils.xml_equal.canonical"], "function", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizerFast.decode"], ["    ", "USE_TF", "=", "os", ".", "environ", ".", "get", "(", "\"USE_TF\"", ",", "\"AUTO\"", ")", ".", "upper", "(", ")", "\n", "USE_TORCH", "=", "os", ".", "environ", ".", "get", "(", "\"USE_TORCH\"", ",", "\"AUTO\"", ")", ".", "upper", "(", ")", "\n", "\n", "if", "USE_TF", "in", "(", "\"1\"", ",", "\"ON\"", ",", "\"YES\"", ",", "\"AUTO\"", ")", "and", "USE_TORCH", "not", "in", "(", "\"1\"", ",", "\"ON\"", ",", "\"YES\"", ")", ":", "\n", "        ", "import", "tensorflow", "as", "tf", "\n", "\n", "assert", "hasattr", "(", "tf", ",", "\"__version__\"", ")", "and", "int", "(", "tf", ".", "__version__", "[", "0", "]", ")", ">=", "2", "\n", "_tf_available", "=", "True", "# pylint: disable=invalid-name", "\n", "logger", ".", "info", "(", "\"TensorFlow version {} available.\"", ".", "format", "(", "tf", ".", "__version__", ")", ")", "\n", "", "else", ":", "\n", "        ", "logger", ".", "info", "(", "\"Disabling Tensorflow because USE_TORCH is set\"", ")", "\n", "_tf_available", "=", "False", "\n", "", "", "except", "(", "ImportError", ",", "AssertionError", ")", ":", "\n", "    ", "_tf_available", "=", "False", "# pylint: disable=invalid-name", "\n", "\n", "", "try", ":", "\n", "    ", "from", "torch", ".", "hub", "import", "_get_torch_home", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.utils.file_utils.list_files": [[65, 78], ["os.walk", "os.path.join", "os.path.join", "file_list.extend", "file_utils.list_files"], "function", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.utils.file_utils.list_files"], ["", "except", "ImportError", ":", "\n", "    ", "torch_cache_home", "=", "os", ".", "path", ".", "expanduser", "(", "\n", "os", ".", "getenv", "(", "\n", "\"TORCH_HOME\"", ",", "os", ".", "path", ".", "join", "(", "os", ".", "getenv", "(", "\"XDG_CACHE_HOME\"", ",", "\"~/.cache\"", ")", ",", "\"torch\"", ")", "\n", ")", "\n", ")", "\n", "", "default_cache_path", "=", "os", ".", "path", ".", "join", "(", "torch_cache_home", ",", "\"transformers\"", ")", "\n", "\n", "try", ":", "\n", "    ", "from", "pathlib", "import", "Path", "\n", "\n", "PYTORCH_PRETRAINED_BERT_CACHE", "=", "Path", "(", "\n", "os", ".", "getenv", "(", "\n", "\"PYTORCH_TRANSFORMERS_CACHE\"", ",", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.utils.file_utils.verify_dir": [[80, 88], ["os.path.exists", "Exception"], "function", ["None"], [")", "\n", ")", "\n", "", "except", "(", "AttributeError", ",", "ImportError", ")", ":", "\n", "    ", "PYTORCH_PRETRAINED_BERT_CACHE", "=", "os", ".", "getenv", "(", "\n", "\"PYTORCH_TRANSFORMERS_CACHE\"", ",", "\n", "os", ".", "getenv", "(", "\"PYTORCH_PRETRAINED_BERT_CACHE\"", ",", "default_cache_path", ")", ",", "\n", ")", "\n", "\n", "", "PYTORCH_TRANSFORMERS_CACHE", "=", "(", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.utils.sentence_splitter.PunktSentenceSplitter.__init__": [[16, 38], ["log.get_global_console_logger", "nltk.data.load", "sentence_splitter.PunktSentenceSplitter.log.error", "sentence_splitter.PunktSentenceSplitter.log.error", "sentence_splitter.PunktSentenceSplitter.log.error"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.utils.log.get_global_console_logger", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.BaseTrainer.BaseTrainer.load"], ["def", "__init__", "(", "self", ",", "language", "=", "\"en\"", ",", "punkt_data_path", "=", "None", ")", ":", "\n", "        ", "self", ".", "lang2datapath", "=", "{", "\"en\"", ":", "\"tokenizers/punkt/english.pickle\"", "}", "\n", "self", ".", "log", "=", "log", ".", "get_global_console_logger", "(", ")", "\n", "try", ":", "\n", "            ", "import", "nltk", ".", "data", "\n", "", "except", "ImportError", ":", "\n", "            ", "self", ".", "log", ".", "error", "(", "\n", "\"Cannot import NLTK data for the sentence splitter. Please \"", "\n", "\"check if the 'punkt' NLTK-package is installed correctly.\"", "\n", ")", "\n", "", "try", ":", "\n", "            ", "if", "not", "punkt_data_path", ":", "\n", "                ", "punkt_data_path", "=", "self", ".", "lang2datapath", "[", "language", "]", "\n", "", "self", ".", "sent_detector", "=", "nltk", ".", "data", ".", "load", "(", "punkt_data_path", ")", "\n", "", "except", "KeyError", ":", "\n", "            ", "self", ".", "log", ".", "error", "(", "\n", "\"No sentence splitter data for language {}.\"", ".", "format", "(", "language", ")", "\n", ")", "\n", "", "except", ":", "\n", "            ", "self", ".", "log", ".", "error", "(", "\n", "\"Could not load sentence splitter data: {}\"", ".", "format", "(", "\n", "self", ".", "lang2datapath", "[", "language", "]", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.utils.sentence_splitter.PunktSentenceSplitter.split": [[41, 45], ["string_utils.cleanup", "sentence_splitter.PunktSentenceSplitter.sent_detector.tokenize", "string_utils.cleanup.strip"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.utils.string_utils.cleanup", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizerFast.tokenize"], ["", "", "def", "split", "(", "self", ",", "text", ")", ":", "\n", "        ", "\"\"\"Splits text and returns a list of the resulting sentences.\"\"\"", "\n", "text", "=", "cleanup", "(", "text", ")", "\n", "return", "self", ".", "sent_detector", ".", "tokenize", "(", "text", ".", "strip", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.utils.sentence_splitter.PunktSentenceSplitter.split_files": [[46, 50], ["sentence_splitter.PunktSentenceSplitter", "file_utils.DirectoryProcessor.process"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.util.massivesumm_utils.Article.process"], ["", "@", "staticmethod", "\n", "def", "split_files", "(", "input_dir", ",", "output_dir", ",", "lang", "=", "\"en\"", ",", "punkt_data_path", "=", "None", ")", ":", "\n", "        ", "ss", "=", "PunktSentenceSplitter", "(", "lang", ",", "punkt_data_path", ")", "\n", "DirectoryProcessor", ".", "process", "(", "input_dir", ",", "output_dir", ",", "ss", ".", "split", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.utils.log.get_console_logger": [[4, 6], ["logging.getLogger"], "function", ["None"], ["def", "get_console_logger", "(", "name", ",", "level", "=", "logging", ".", "WARNING", ")", ":", "\n", "    ", "return", "logging", ".", "getLogger", "(", "\"pyrouge\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.utils.log.get_global_console_logger": [[8, 10], ["logging.getLogger"], "function", ["None"], ["", "def", "get_global_console_logger", "(", "level", "=", "logging", ".", "WARNING", ")", ":", "\n", "    ", "return", "logging", ".", "getLogger", "(", "\"pyrouge\"", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.utils.string_utils.remove_newlines": [[6, 11], ["re.compile", "re.sub", "string_utils.remove_extraneous_whitespace"], "function", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.utils.string_utils.remove_extraneous_whitespace"], ["def", "remove_newlines", "(", "s", ")", ":", "\n", "    ", "p", "=", "re", ".", "compile", "(", "\"[\\n|\\r\\n|\\n\\r]\"", ")", "\n", "s", "=", "re", ".", "sub", "(", "p", ",", "\" \"", ",", "s", ")", "\n", "s", "=", "remove_extraneous_whitespace", "(", "s", ")", "\n", "return", "s", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.utils.string_utils.remove_extraneous_whitespace": [[13, 17], ["re.compile", "re.sub"], "function", ["None"], ["", "def", "remove_extraneous_whitespace", "(", "s", ")", ":", "\n", "    ", "p", "=", "re", ".", "compile", "(", "\"(\\s+)\"", ")", "\n", "s", "=", "re", ".", "sub", "(", "p", ",", "\" \"", ",", "s", ")", "\n", "return", "s", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.utils.string_utils.cleanup": [[19, 21], ["string_utils.remove_newlines"], "function", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.utils.string_utils.remove_newlines"], ["", "def", "cleanup", "(", "s", ")", ":", "\n", "    ", "return", "remove_newlines", "(", "s", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Evaluation.OldROUGEEval.OldROUGEEval.__init__": [[420, 422], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Evaluation.OldROUGEEval.OldROUGEEval.make_html_safe": [[423, 427], ["s.replace", "s.replace"], "methods", ["None"], ["", "def", "make_html_safe", "(", "self", ",", "s", ")", ":", "\n", "        ", "s", ".", "replace", "(", "\"<\"", ",", "\"&lt;\"", ")", "\n", "s", ".", "replace", "(", "\">\"", ",", "\"&gt;\"", ")", "\n", "return", "s", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Evaluation.OldROUGEEval.OldROUGEEval.eval": [[428, 433], ["OldROUGEEval.rouge", "OldROUGEEval.OldROUGEEval.make_html_safe", "OldROUGEEval.OldROUGEEval.make_html_safe"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Evaluation.OldROUGEEval.rouge", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Evaluation.ROUGEEval.ROUGEEval.make_html_safe", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Evaluation.ROUGEEval.ROUGEEval.make_html_safe"], ["", "def", "eval", "(", "self", ",", "predictions", ",", "groundtruths", ")", ":", "\n", "        ", "predictions", "=", "[", "self", ".", "make_html_safe", "(", "w", ")", "for", "w", "in", "predictions", "]", "\n", "groundtruths", "=", "[", "self", ".", "make_html_safe", "(", "w", ")", "for", "w", "in", "groundtruths", "]", "\n", "results", "=", "rouge", "(", "predictions", ",", "groundtruths", ")", "\n", "return", "results", "\n", "", "", ""]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Evaluation.OldROUGEEval._get_ngrams": [[21, 40], ["len", "range"], "function", ["None"], ["def", "_get_ngrams", "(", "n", ",", "text", ")", ":", "\n", "    ", "\"\"\"Calcualtes n-grams.\n\n    Args:\n      n: which n-grams to calculate\n      text: An array of tokens\n\n    Returns:\n      A set of n-grams\n    \"\"\"", "\n", "ngram_set", "=", "{", "}", "\n", "text_length", "=", "len", "(", "text", ")", "\n", "max_index_ngram_start", "=", "text_length", "-", "n", "\n", "for", "i", "in", "range", "(", "max_index_ngram_start", "+", "1", ")", ":", "\n", "        ", "k", "=", "\" \"", ".", "join", "(", "text", "[", "i", ":", "i", "+", "n", "]", ")", "\n", "if", "k", "not", "in", "ngram_set", ":", "\n", "            ", "ngram_set", "[", "k", "]", "=", "0", "\n", "", "ngram_set", "[", "k", "]", "+=", "1", "\n", "", "return", "ngram_set", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Evaluation.OldROUGEEval._get_su": [[42, 67], ["len", "range", "range"], "function", ["None"], ["", "def", "_get_su", "(", "dist", ",", "text", ")", ":", "\n", "    ", "\"\"\"Calcualtes skip-grams and unigram\n\n    Args:\n      n: which n-grams to calculate\n      text: An array of tokens\n\n    Returns:\n      A set of n-grams\n    \"\"\"", "\n", "su_set", "=", "{", "}", "\n", "text_length", "=", "len", "(", "text", ")", "\n", "for", "i", "in", "range", "(", "text_length", ")", ":", "\n", "        ", "k", "=", "text", "[", "i", "]", "\n", "if", "k", "not", "in", "su_set", ":", "\n", "            ", "su_set", "[", "k", "]", "=", "0", "\n", "", "su_set", "[", "k", "]", "+=", "1", "\n", "for", "j", "in", "range", "(", "i", "+", "1", ",", "text_length", ")", ":", "\n", "            ", "if", "j", "-", "i", "-", "1", ">", "dist", ":", "\n", "                ", "break", "\n", "", "k", "=", "text", "[", "i", "]", "+", "\" \"", "+", "text", "[", "j", "]", "\n", "if", "k", "not", "in", "su_set", ":", "\n", "                ", "su_set", "[", "k", "]", "=", "0", "\n", "", "su_set", "[", "k", "]", "+=", "1", "\n", "", "", "return", "su_set", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Evaluation.OldROUGEEval._split_into_words": [[69, 72], ["list", "itertools.chain", "_.split"], "function", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.utils.sentence_splitter.PunktSentenceSplitter.split"], ["", "def", "_split_into_words", "(", "sentences", ")", ":", "\n", "    ", "\"\"\"Splits multiple sentences into words and flattens the result\"\"\"", "\n", "return", "list", "(", "itertools", ".", "chain", "(", "*", "[", "_", ".", "split", "(", "\" \"", ")", "for", "_", "in", "sentences", "]", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Evaluation.OldROUGEEval._get_word_ngrams": [[74, 81], ["OldROUGEEval._split_into_words", "OldROUGEEval._get_ngrams", "len"], "function", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Evaluation.OldROUGEEval._split_into_words", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Evaluation.OldROUGEEval._get_ngrams"], ["", "def", "_get_word_ngrams", "(", "n", ",", "sentences", ")", ":", "\n", "    ", "\"\"\"Calculates word n-grams for multiple sentences.\"\"\"", "\n", "assert", "len", "(", "sentences", ")", ">", "0", "\n", "assert", "n", ">", "0", "\n", "\n", "words", "=", "_split_into_words", "(", "sentences", ")", "\n", "return", "_get_ngrams", "(", "n", ",", "words", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Evaluation.OldROUGEEval._get_word_su": [[83, 90], ["OldROUGEEval._split_into_words", "OldROUGEEval._get_su", "len"], "function", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Evaluation.OldROUGEEval._split_into_words", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Evaluation.OldROUGEEval._get_su"], ["", "def", "_get_word_su", "(", "dist", ",", "sentences", ")", ":", "\n", "    ", "\"\"\"Calculates word skip-dist-grams for multiple sentences.\"\"\"", "\n", "assert", "len", "(", "sentences", ")", ">", "0", "\n", "assert", "dist", ">", "0", "\n", "\n", "words", "=", "_split_into_words", "(", "sentences", ")", "\n", "return", "_get_su", "(", "dist", ",", "words", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Evaluation.OldROUGEEval._len_lcs": [[92, 108], ["OldROUGEEval._lcs", "len", "len"], "function", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Evaluation.OldROUGEEval._lcs"], ["", "def", "_len_lcs", "(", "x", ",", "y", ")", ":", "\n", "    ", "\"\"\"\n    Returns the length of the Longest Common Subsequence between sequences x\n    and y.\n    Source: http://www.algorithmist.com/index.php/Longest_Common_Subsequence\n\n    Args:\n      x: sequence of words\n      y: sequence of words\n\n    Returns\n      integer: Length of LCS between x and y\n    \"\"\"", "\n", "table", "=", "_lcs", "(", "x", ",", "y", ")", "\n", "n", ",", "m", "=", "len", "(", "x", ")", ",", "len", "(", "y", ")", "\n", "return", "table", "[", "n", ",", "m", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Evaluation.OldROUGEEval._lcs": [[110, 135], ["dict", "range", "len", "len", "range", "max"], "function", ["None"], ["", "def", "_lcs", "(", "x", ",", "y", ")", ":", "\n", "    ", "\"\"\"\n    Computes the length of the longest common subsequence (lcs) between two\n    strings. The implementation below uses a DP programming algorithm and runs\n    in O(nm) time where n = len(x) and m = len(y).\n    Source: http://www.algorithmist.com/index.php/Longest_Common_Subsequence\n\n    Args:\n      x: collection of words\n      y: collection of words\n\n    Returns:\n      Table of dictionary of coord and len lcs\n    \"\"\"", "\n", "n", ",", "m", "=", "len", "(", "x", ")", ",", "len", "(", "y", ")", "\n", "table", "=", "dict", "(", ")", "\n", "for", "i", "in", "range", "(", "n", "+", "1", ")", ":", "\n", "        ", "for", "j", "in", "range", "(", "m", "+", "1", ")", ":", "\n", "            ", "if", "i", "==", "0", "or", "j", "==", "0", ":", "\n", "                ", "table", "[", "i", ",", "j", "]", "=", "0", "\n", "", "elif", "x", "[", "i", "-", "1", "]", "==", "y", "[", "j", "-", "1", "]", ":", "\n", "                ", "table", "[", "i", ",", "j", "]", "=", "table", "[", "i", "-", "1", ",", "j", "-", "1", "]", "+", "1", "\n", "", "else", ":", "\n", "                ", "table", "[", "i", ",", "j", "]", "=", "max", "(", "table", "[", "i", "-", "1", ",", "j", "]", ",", "table", "[", "i", ",", "j", "-", "1", "]", ")", "\n", "", "", "", "return", "table", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Evaluation.OldROUGEEval._recon_lcs": [[137, 165], ["OldROUGEEval._lcs", "tuple", "len", "len", "map", "OldROUGEEval._recon_lcs._recon"], "function", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Evaluation.OldROUGEEval._lcs"], ["", "def", "_recon_lcs", "(", "x", ",", "y", ")", ":", "\n", "    ", "\"\"\"\n    Returns the Longest Subsequence between x and y.\n    Source: http://www.algorithmist.com/index.php/Longest_Common_Subsequence\n\n    Args:\n      x: sequence of words\n      y: sequence of words\n\n    Returns:\n      sequence: LCS of x and y\n    \"\"\"", "\n", "i", ",", "j", "=", "len", "(", "x", ")", ",", "len", "(", "y", ")", "\n", "table", "=", "_lcs", "(", "x", ",", "y", ")", "\n", "\n", "def", "_recon", "(", "i", ",", "j", ")", ":", "\n", "        ", "\"\"\"private recon calculation\"\"\"", "\n", "if", "i", "==", "0", "or", "j", "==", "0", ":", "\n", "            ", "return", "[", "]", "\n", "", "elif", "x", "[", "i", "-", "1", "]", "==", "y", "[", "j", "-", "1", "]", ":", "\n", "            ", "return", "_recon", "(", "i", "-", "1", ",", "j", "-", "1", ")", "+", "[", "(", "x", "[", "i", "-", "1", "]", ",", "i", ")", "]", "\n", "", "elif", "table", "[", "i", "-", "1", ",", "j", "]", ">", "table", "[", "i", ",", "j", "-", "1", "]", ":", "\n", "            ", "return", "_recon", "(", "i", "-", "1", ",", "j", ")", "\n", "", "else", ":", "\n", "            ", "return", "_recon", "(", "i", ",", "j", "-", "1", ")", "\n", "\n", "", "", "recon_tuple", "=", "tuple", "(", "map", "(", "lambda", "x", ":", "x", "[", "0", "]", ",", "_recon", "(", "i", ",", "j", ")", ")", ")", "\n", "return", "recon_tuple", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Evaluation.OldROUGEEval.rouge_su": [[167, 185], ["OldROUGEEval.rouge_n"], "function", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Evaluation.OldROUGEEval.rouge_n"], ["", "def", "rouge_su", "(", "evaluated_sentences", ",", "reference_sentences", ",", "dist", "=", "4", ")", ":", "\n", "    ", "\"\"\"\n    Computes ROUGE-SU_dist of two text collections of sentences.\n    Sourece: http://research.microsoft.com/en-us/um/people/cyl/download/\n    papers/rouge-working-note-v1.3.1.pdf\n\n    Args:\n      evaluated_sentences: The sentences that have been picked by the summarizer\n      reference_sentences: The sentences from the referene set\n      n: maximum distance between two tokens.  Defaults to 4.\n\n    Returns:\n      A tuple (f1, precision, recall) for ROUGE-SU4\n\n    Raises:\n      ValueError: raises exception if a param has len <= 0\n    \"\"\"", "\n", "return", "rouge_n", "(", "evaluated_sentences", ",", "reference_sentences", ",", "dist", "=", "dist", ",", "su", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Evaluation.OldROUGEEval.rouge_n": [[187, 242], ["sum", "sum", "_get_word_ngrams.items", "ValueError", "OldROUGEEval._get_word_su", "OldROUGEEval._get_word_su", "OldROUGEEval._get_word_ngrams", "OldROUGEEval._get_word_ngrams", "len", "len", "_get_word_ngrams.items", "_get_word_ngrams.items"], "function", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Evaluation.OldROUGEEval._get_word_su", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Evaluation.OldROUGEEval._get_word_su", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Evaluation.OldROUGEEval._get_word_ngrams", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Evaluation.OldROUGEEval._get_word_ngrams"], ["", "def", "rouge_n", "(", "evaluated_sentences", ",", "reference_sentences", ",", "n", "=", "2", ",", "dist", "=", "4", ",", "su", "=", "False", ")", ":", "\n", "    ", "\"\"\"\n    Computes ROUGE-N of two text collections of sentences.\n    Sourece: http://research.microsoft.com/en-us/um/people/cyl/download/\n    papers/rouge-working-note-v1.3.1.pdf\n\n    Args:\n      evaluated_sentences: The sentences that have been picked by the summarizer\n      reference_sentences: The sentences from the referene set\n      n: Size of ngram.  Defaults to 2.\n      su: if true, we are computing rouge_su\n\n    Returns:\n      A tuple (f1, precision, recall) for ROUGE-N\n\n    Raises:\n      ValueError: raises exception if a param has len <= 0\n    \"\"\"", "\n", "if", "len", "(", "evaluated_sentences", ")", "<=", "0", "or", "len", "(", "reference_sentences", ")", "<=", "0", ":", "\n", "        ", "raise", "ValueError", "(", "\"Collections must contain at least 1 sentence.\"", ")", "\n", "\n", "", "if", "su", "==", "True", ":", "\n", "        ", "evaluated_ngrams", "=", "_get_word_su", "(", "dist", ",", "evaluated_sentences", ")", "\n", "reference_ngrams", "=", "_get_word_su", "(", "dist", ",", "reference_sentences", ")", "\n", "", "else", ":", "\n", "        ", "evaluated_ngrams", "=", "_get_word_ngrams", "(", "n", ",", "evaluated_sentences", ")", "\n", "reference_ngrams", "=", "_get_word_ngrams", "(", "n", ",", "reference_sentences", ")", "\n", "\n", "", "reference_count", "=", "sum", "(", "[", "v", "for", "k", ",", "v", "in", "reference_ngrams", ".", "items", "(", ")", "]", ")", "\n", "evaluated_count", "=", "sum", "(", "[", "v", "for", "k", ",", "v", "in", "evaluated_ngrams", ".", "items", "(", ")", "]", ")", "\n", "\n", "# Gets the overlapping ngrams between evaluated and reference", "\n", "overlapping_count", "=", "0", "\n", "for", "k", ",", "v", "in", "reference_ngrams", ".", "items", "(", ")", ":", "\n", "        ", "if", "k", "in", "evaluated_ngrams", ":", "\n", "            ", "if", "evaluated_ngrams", "[", "k", "]", "<", "v", ":", "\n", "                ", "overlapping_count", "+=", "evaluated_ngrams", "[", "k", "]", "\n", "", "else", ":", "\n", "                ", "overlapping_count", "+=", "v", "\n", "\n", "# Handle edge case. This isn't mathematically correct, but it's good enough", "\n", "", "", "", "if", "evaluated_count", "==", "0", ":", "\n", "        ", "precision", "=", "0.0", "\n", "", "else", ":", "\n", "        ", "precision", "=", "overlapping_count", "/", "evaluated_count", "\n", "\n", "", "if", "reference_count", "==", "0", ":", "\n", "        ", "recall", "=", "0.0", "\n", "", "else", ":", "\n", "        ", "recall", "=", "overlapping_count", "/", "reference_count", "\n", "\n", "", "f1_score", "=", "2.0", "*", "(", "(", "precision", "*", "recall", ")", "/", "(", "precision", "+", "recall", "+", "1e-8", ")", ")", "\n", "\n", "# return overlapping_count / reference_count", "\n", "return", "f1_score", ",", "precision", ",", "recall", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Evaluation.OldROUGEEval._f_p_r_lcs": [[244, 265], ["None"], "function", ["None"], ["", "def", "_f_p_r_lcs", "(", "llcs", ",", "m", ",", "n", ")", ":", "\n", "    ", "\"\"\"\n    Computes the LCS-based F-measure score\n    Source: http://research.microsoft.com/en-us/um/people/cyl/download/papers/\n    rouge-working-note-v1.3.1.pdf\n\n    Args:\n      llcs: Length of LCS\n      m: number of words in reference summary\n      n: number of words in candidate summary\n\n    Returns:\n      Float. LCS-based F-measure score\n    \"\"\"", "\n", "r_lcs", "=", "llcs", "/", "m", "\n", "p_lcs", "=", "llcs", "/", "n", "\n", "beta", "=", "p_lcs", "/", "(", "r_lcs", "+", "1e-12", ")", "\n", "num", "=", "(", "1", "+", "(", "beta", "**", "2", ")", ")", "*", "r_lcs", "*", "p_lcs", "\n", "denom", "=", "r_lcs", "+", "(", "(", "beta", "**", "2", ")", "*", "p_lcs", ")", "\n", "f_lcs", "=", "num", "/", "(", "denom", "+", "1e-12", ")", "\n", "return", "f_lcs", ",", "p_lcs", ",", "r_lcs", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Evaluation.OldROUGEEval.rouge_l_sentence_level": [[267, 302], ["OldROUGEEval._split_into_words", "OldROUGEEval._split_into_words", "len", "len", "OldROUGEEval._len_lcs", "OldROUGEEval._f_p_r_lcs", "ValueError", "len", "len"], "function", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Evaluation.OldROUGEEval._split_into_words", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Evaluation.OldROUGEEval._split_into_words", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Evaluation.OldROUGEEval._len_lcs", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Evaluation.OldROUGEEval._f_p_r_lcs"], ["", "def", "rouge_l_sentence_level", "(", "evaluated_sentences", ",", "reference_sentences", ")", ":", "\n", "    ", "\"\"\"\n    Computes ROUGE-L (sentence level) of two text collections of sentences.\n    http://research.microsoft.com/en-us/um/people/cyl/download/papers/\n    rouge-working-note-v1.3.1.pdf\n\n    Calculated according to:\n    R_lcs = LCS(X,Y)/m\n    P_lcs = LCS(X,Y)/n\n    F_lcs = ((1 + beta^2)*R_lcs*P_lcs) / (R_lcs + (beta^2) * P_lcs)\n\n    where:\n    X = reference summary\n    Y = Candidate summary\n    m = length of reference summary\n    n = length of candidate summary\n\n    Args:\n      evaluated_sentences: The sentences that have been picked by the summarizer\n      reference_sentences: The sentences from the referene set\n\n    Returns:\n      A float: F_lcs\n\n    Raises:\n      ValueError: raises exception if a param has len <= 0\n    \"\"\"", "\n", "if", "len", "(", "evaluated_sentences", ")", "<=", "0", "or", "len", "(", "reference_sentences", ")", "<=", "0", ":", "\n", "        ", "raise", "ValueError", "(", "\"Collections must contain at least 1 sentence.\"", ")", "\n", "", "reference_words", "=", "_split_into_words", "(", "reference_sentences", ")", "\n", "evaluated_words", "=", "_split_into_words", "(", "evaluated_sentences", ")", "\n", "m", "=", "len", "(", "reference_words", ")", "\n", "n", "=", "len", "(", "evaluated_words", ")", "\n", "lcs", "=", "_len_lcs", "(", "evaluated_words", ",", "reference_words", ")", "\n", "return", "_f_p_r_lcs", "(", "lcs", ",", "m", ",", "n", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Evaluation.OldROUGEEval._union_lcs": [[304, 339], ["set", "OldROUGEEval._split_into_words", "len", "len", "ValueError", "OldROUGEEval._split_into_words", "set", "len", "lcs_union.union.union", "OldROUGEEval._recon_lcs"], "function", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Evaluation.OldROUGEEval._split_into_words", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Evaluation.OldROUGEEval._split_into_words", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Evaluation.OldROUGEEval._recon_lcs"], ["", "def", "_union_lcs", "(", "evaluated_sentences", ",", "reference_sentence", ")", ":", "\n", "    ", "\"\"\"\n    Returns LCS_u(r_i, C) which is the LCS score of the union longest common\n    subsequence between reference sentence ri and candidate summary C. For example\n    if r_i= w1 w2 w3 w4 w5, and C contains two sentences: c1 = w1 w2 w6 w7 w8 and\n    c2 = w1 w3 w8 w9 w5, then the longest common subsequence of r_i and c1 is\n    \u201cw1 w2\u201d and the longest common subsequence of r_i and c2 is \u201cw1 w3 w5\u201d. The\n    union longest common subsequence of r_i, c1, and c2 is \u201cw1 w2 w3 w5\u201d and\n    LCS_u(r_i, C) = 4/5.\n\n    Args:\n      evaluated_sentences: The sentences that have been picked by the summarizer\n      reference_sentence: One of the sentences in the reference summaries\n\n    Returns:\n      float: LCS_u(r_i, C)\n\n    ValueError:\n      Raises exception if a param has len <= 0\n    \"\"\"", "\n", "if", "len", "(", "evaluated_sentences", ")", "<=", "0", ":", "\n", "        ", "raise", "ValueError", "(", "\"Collections must contain at least 1 sentence.\"", ")", "\n", "\n", "", "lcs_union", "=", "set", "(", ")", "\n", "reference_words", "=", "_split_into_words", "(", "[", "reference_sentence", "]", ")", "\n", "combined_lcs_length", "=", "0", "\n", "for", "eval_s", "in", "evaluated_sentences", ":", "\n", "        ", "evaluated_words", "=", "_split_into_words", "(", "[", "eval_s", "]", ")", "\n", "lcs", "=", "set", "(", "_recon_lcs", "(", "reference_words", ",", "evaluated_words", ")", ")", "\n", "combined_lcs_length", "+=", "len", "(", "lcs", ")", "\n", "lcs_union", "=", "lcs_union", ".", "union", "(", "lcs", ")", "\n", "\n", "", "union_lcs_count", "=", "len", "(", "lcs_union", ")", "\n", "union_lcs_value", "=", "union_lcs_count", "/", "combined_lcs_length", "\n", "return", "union_lcs_value", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Evaluation.OldROUGEEval.rouge_l_summary_level": [[341, 382], ["len", "len", "OldROUGEEval._f_p_r_lcs", "ValueError", "OldROUGEEval._split_into_words", "OldROUGEEval._split_into_words", "OldROUGEEval._union_lcs", "len", "len"], "function", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Evaluation.OldROUGEEval._f_p_r_lcs", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Evaluation.OldROUGEEval._split_into_words", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Evaluation.OldROUGEEval._split_into_words", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Evaluation.OldROUGEEval._union_lcs"], ["", "def", "rouge_l_summary_level", "(", "evaluated_sentences", ",", "reference_sentences", ")", ":", "\n", "    ", "\"\"\"\n    Computes ROUGE-L (summary level) of two text collections of sentences.\n    http://research.microsoft.com/en-us/um/people/cyl/download/papers/\n    rouge-working-note-v1.3.1.pdf\n\n    Calculated according to:\n    R_lcs = SUM(1, u)[LCS<union>(r_i,C)]/m\n    P_lcs = SUM(1, u)[LCS<union>(r_i,C)]/n\n    F_lcs = ((1 + beta^2)*R_lcs*P_lcs) / (R_lcs + (beta^2) * P_lcs)\n\n    where:\n    SUM(i,u) = SUM from i through u\n    u = number of sentences in reference summary\n    C = Candidate summary made up of v sentences\n    m = number of words in reference summary\n    n = number of words in candidate summary\n\n    Args:\n      evaluated_sentences: The sentences that have been picked by the summarizer\n      reference_sentence: One of the sentences in the reference summaries\n\n    Returns:\n      A float: F_lcs\n\n    Raises:\n      ValueError: raises exception if a param has len <= 0\n    \"\"\"", "\n", "if", "len", "(", "evaluated_sentences", ")", "<=", "0", "or", "len", "(", "reference_sentences", ")", "<=", "0", ":", "\n", "        ", "raise", "ValueError", "(", "\"Collections must contain at least 1 sentence.\"", ")", "\n", "\n", "# total number of words in reference sentences", "\n", "", "m", "=", "len", "(", "_split_into_words", "(", "reference_sentences", ")", ")", "\n", "\n", "# total number of words in evaluated sentences", "\n", "n", "=", "len", "(", "_split_into_words", "(", "evaluated_sentences", ")", ")", "\n", "\n", "union_lcs_sum_across_all_references", "=", "0", "\n", "for", "ref_s", "in", "reference_sentences", ":", "\n", "        ", "union_lcs_sum_across_all_references", "+=", "_union_lcs", "(", "evaluated_sentences", ",", "ref_s", ")", "\n", "", "return", "_f_p_r_lcs", "(", "union_lcs_sum_across_all_references", ",", "m", ",", "n", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Evaluation.OldROUGEEval.rouge": [[384, 416], ["map", "map", "map", "map", "OldROUGEEval.rouge_n", "zip", "OldROUGEEval.rouge_n", "zip", "OldROUGEEval.rouge_su", "zip", "OldROUGEEval.rouge_l_sentence_level", "zip", "zip", "zip", "zip", "zip"], "function", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Evaluation.OldROUGEEval.rouge_n", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Evaluation.OldROUGEEval.rouge_n", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Evaluation.OldROUGEEval.rouge_su", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Evaluation.OldROUGEEval.rouge_l_sentence_level"], ["", "def", "rouge", "(", "hypotheses", ",", "references", ")", ":", "\n", "    ", "\"\"\"Calculates average rouge scores for a list of hypotheses and\n    references\"\"\"", "\n", "\n", "# Filter out hyps that are of 0 length", "\n", "# hyps_and_refs = zip(hypotheses, references)", "\n", "# hyps_and_refs = [_ for _ in hyps_and_refs if len(_[0]) > 0]", "\n", "# hypotheses, references = zip(*hyps_and_refs)", "\n", "\n", "# Calculate ROUGE-1 F1, precision, recall scores", "\n", "rouge_1", "=", "[", "rouge_n", "(", "[", "hyp", "]", ",", "[", "ref", "]", ",", "1", ")", "for", "hyp", ",", "ref", "in", "zip", "(", "hypotheses", ",", "references", ")", "]", "\n", "rouge_1_f", ",", "rouge_1_p", ",", "rouge_1_r", "=", "map", "(", "np", ".", "mean", ",", "zip", "(", "*", "rouge_1", ")", ")", "\n", "\n", "# Calculate ROUGE-2 F1, precision, recall scores", "\n", "rouge_2", "=", "[", "rouge_n", "(", "[", "hyp", "]", ",", "[", "ref", "]", ",", "2", ")", "for", "hyp", ",", "ref", "in", "zip", "(", "hypotheses", ",", "references", ")", "]", "\n", "rouge_2_f", ",", "rouge_2_p", ",", "rouge_2_r", "=", "map", "(", "np", ".", "mean", ",", "zip", "(", "*", "rouge_2", ")", ")", "\n", "\n", "# Calculate ROUGE-SU4 F1, precision, recall scores", "\n", "rouge_su4", "=", "[", "rouge_su", "(", "[", "hyp", "]", ",", "[", "ref", "]", ",", "4", ")", "for", "hyp", ",", "ref", "in", "zip", "(", "hypotheses", ",", "references", ")", "]", "\n", "rouge_su4_f", ",", "rouge_su4_p", ",", "rouge_su4_r", "=", "map", "(", "np", ".", "mean", ",", "zip", "(", "*", "rouge_su4", ")", ")", "\n", "\n", "# Calculate ROUGE-L F1, precision, recall scores", "\n", "rouge_l", "=", "[", "\n", "rouge_l_sentence_level", "(", "[", "hyp", "]", ",", "[", "ref", "]", ")", "for", "hyp", ",", "ref", "in", "zip", "(", "hypotheses", ",", "references", ")", "\n", "]", "\n", "rouge_l_f", ",", "rouge_l_p", ",", "rouge_l_r", "=", "map", "(", "np", ".", "mean", ",", "zip", "(", "*", "rouge_l", ")", ")", "\n", "\n", "return", "{", "\n", "\"rouge_1_f_score\"", ":", "rouge_1_f", ",", "\n", "\"rouge_2_f_score\"", ":", "rouge_2_f", ",", "\n", "\"rouge_su4_f_score\"", ":", "rouge_su4_f", ",", "\n", "\"rouge_l_f_score\"", ":", "rouge_l_f", ",", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Evaluation.ROUGEEval.ROUGEEval.__init__": [[60, 73], ["os.path.join", "ROUGEEval.ROUGEEval.opt.get", "os.path.dirname", "float", "float"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.closablequeue.ClosableQueue.get"], ["def", "__init__", "(", "self", ",", "run_dir", ",", "save_dir", ",", "opt", ")", ":", "\n", "        ", "self", ".", "run_dir", "=", "run_dir", "\n", "self", ".", "save_dir", "=", "save_dir", "\n", "self", ".", "opt", "=", "opt", "\n", "\n", "# use relative path to make it work on Philly", "\n", "self", ".", "pyrouge_dir", "=", "os", ".", "path", ".", "join", "(", "\n", "os", ".", "path", ".", "dirname", "(", "__file__", ")", ",", "\"../ThirdParty/ROUGE/ROUGE-1.5.5/\"", "\n", ")", "\n", "\n", "self", ".", "eval_batches_num", "=", "self", ".", "opt", ".", "get", "(", "\"EVAL_BATCHES_NUM\"", ",", "float", "(", "\"Inf\"", ")", ")", "\n", "self", ".", "best_score", "=", "-", "float", "(", "\"Inf\"", ")", "\n", "self", ".", "best_res", "=", "{", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Evaluation.ROUGEEval.ROUGEEval.reset_best_score": [[74, 79], ["float", "float"], "methods", ["None"], ["", "def", "reset_best_score", "(", "self", ",", "set_high", "=", "False", ")", ":", "\n", "        ", "if", "set_high", ":", "\n", "            ", "self", ".", "best_score", "=", "float", "(", "\"Inf\"", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "best_score", "=", "-", "float", "(", "\"Inf\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Evaluation.ROUGEEval.ROUGEEval.make_html_safe": [[80, 84], ["s.replace.replace.replace", "s.replace.replace.replace"], "methods", ["None"], ["", "", "def", "make_html_safe", "(", "self", ",", "s", ")", ":", "\n", "        ", "s", "=", "s", ".", "replace", "(", "\"<\"", ",", "\"&lt;\"", ")", "\n", "s", "=", "s", ".", "replace", "(", "\">\"", ",", "\"&gt;\"", ")", "\n", "return", "s", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Evaluation.ROUGEEval.ROUGEEval.print_to_rouge_dir": [[85, 106], ["enumerate", "os.path.join", "open", "re.split", "enumerate", "f.write", "re.finditer", "sent.replace.replace.encode", "sent.replace.replace.replace", "len", "sent.replace.replace.encode", "len", "x.group", "x.group", "x.group", "x.group"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.utils.sentence_splitter.PunktSentenceSplitter.split", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizer.encode", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizer.encode"], ["", "def", "print_to_rouge_dir", "(", "\n", "self", ",", "summaries", ",", "dir", ",", "suffix", ",", "split_chars", ",", "special_char_dict", "=", "None", "\n", ")", ":", "\n", "        ", "for", "idx", ",", "summary", "in", "enumerate", "(", "summaries", ")", ":", "\n", "            ", "fname", "=", "os", ".", "path", ".", "join", "(", "dir", ",", "\"%06d_%s.txt\"", "%", "(", "idx", ",", "suffix", ")", ")", "\n", "with", "open", "(", "fname", ",", "\"wb\"", ")", "as", "f", ":", "\n", "                ", "sents", "=", "re", ".", "split", "(", "r\"(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s\"", ",", "summary", ")", "\n", "for", "i", ",", "sent", "in", "enumerate", "(", "sents", ")", ":", "\n", "                    ", "if", "split_chars", ":", "\n", "# sent = re.sub(r'([\\u4e00-\\u9fff])', r' \\1 ', sent)", "\n", "                        ", "for", "x", "in", "re", ".", "finditer", "(", "r\"([\\u4e00-\\u9fff])\"", ",", "sent", ")", ":", "\n", "                            ", "if", "not", "x", ".", "group", "(", "1", ")", "in", "special_char_dict", ":", "\n", "                                ", "special_char_dict", "[", "x", ".", "group", "(", "1", ")", "]", "=", "len", "(", "special_char_dict", ")", "\n", "", "sent", "=", "sent", ".", "replace", "(", "\n", "x", ".", "group", "(", "1", ")", ",", "\" {} \"", ".", "format", "(", "special_char_dict", "[", "x", ".", "group", "(", "1", ")", "]", ")", "\n", ")", "\n", "", "", "if", "i", "==", "len", "(", "sents", ")", "-", "1", ":", "\n", "                        ", "to_print", "=", "sent", ".", "encode", "(", "\"utf-8\"", ")", "\n", "", "else", ":", "\n", "                        ", "to_print", "=", "sent", ".", "encode", "(", "\"utf-8\"", ")", "+", "\"\\n\"", ".", "encode", "(", "\"utf-8\"", ")", "\n", "", "f", ".", "write", "(", "to_print", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Evaluation.ROUGEEval.ROUGEEval.print_to_rouge_dir_gt": [[107, 137], ["enumerate", "enumerate", "summary.split", "os.path.join", "open", "re.split", "enumerate", "f.write", "re.finditer", "sent.replace.replace.encode", "sent.replace.replace.replace", "len", "sent.replace.replace.encode", "len", "x.group", "x.group", "x.group", "x.group"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.utils.sentence_splitter.PunktSentenceSplitter.split", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.utils.sentence_splitter.PunktSentenceSplitter.split", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizer.encode", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizer.encode"], ["", "", "", "", "def", "print_to_rouge_dir_gt", "(", "self", ",", "summaries", ",", "dir", ",", "suffix", ",", "split_chars", ")", ":", "\n", "        ", "if", "split_chars", ":", "\n", "            ", "char_dict", "=", "{", "}", "\n", "\n", "", "for", "idx", ",", "summary", "in", "enumerate", "(", "summaries", ")", ":", "\n", "            ", "for", "ref_idx", ",", "sub_summary", "in", "enumerate", "(", "summary", ".", "split", "(", "\" ||| \"", ")", ")", ":", "\n", "                ", "fname", "=", "os", ".", "path", ".", "join", "(", "\n", "dir", ",", "\"%s.%06d_%s.txt\"", "%", "(", "ascii_uppercase", "[", "ref_idx", "]", ",", "idx", ",", "suffix", ")", "\n", ")", "\n", "with", "open", "(", "fname", ",", "\"wb\"", ")", "as", "f", ":", "\n", "                    ", "sents", "=", "re", ".", "split", "(", "\n", "r\"(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s\"", ",", "sub_summary", "\n", ")", "\n", "for", "i", ",", "sent", "in", "enumerate", "(", "sents", ")", ":", "\n", "                        ", "if", "split_chars", ":", "\n", "                            ", "for", "x", "in", "re", ".", "finditer", "(", "r\"([\\u4e00-\\u9fff])\"", ",", "sent", ")", ":", "\n", "                                ", "if", "not", "x", ".", "group", "(", "1", ")", "in", "char_dict", ":", "\n", "                                    ", "char_dict", "[", "x", ".", "group", "(", "1", ")", "]", "=", "len", "(", "char_dict", ")", "\n", "", "sent", "=", "sent", ".", "replace", "(", "\n", "x", ".", "group", "(", "1", ")", ",", "\" {} \"", ".", "format", "(", "char_dict", "[", "x", ".", "group", "(", "1", ")", "]", ")", "\n", ")", "\n", "\n", "", "", "if", "i", "==", "len", "(", "sents", ")", "-", "1", ":", "\n", "                            ", "to_print", "=", "sent", ".", "encode", "(", "\"utf-8\"", ")", "\n", "", "else", ":", "\n", "                            ", "to_print", "=", "sent", ".", "encode", "(", "\"utf-8\"", ")", "+", "\"\\n\"", ".", "encode", "(", "\"utf-8\"", ")", "\n", "", "f", ".", "write", "(", "to_print", ")", "\n", "\n", "", "", "", "", "if", "split_chars", ":", "\n", "            ", "return", "char_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Evaluation.ROUGEEval.ROUGEEval._convert_tokens_to_string": [[152, 162], ["tokenizer.decode", "t.lower", "tokenizer.convert_tokens_to_ids"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizerFast.decode", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizer.convert_tokens_to_ids"], ["", "", "def", "_convert_tokens_to_string", "(", "self", ",", "tokenizer", ",", "tokens", ")", ":", "\n", "        ", "if", "\"EVAL_TOKENIZED\"", "in", "self", ".", "opt", ":", "\n", "            ", "tokens", "=", "[", "t", "for", "t", "in", "tokens", "if", "t", "not", "in", "tokenizer", ".", "all_special_tokens", "]", "\n", "", "if", "\"EVAL_LOWERCASE\"", "in", "self", ".", "opt", ":", "\n", "            ", "tokens", "=", "[", "t", ".", "lower", "(", ")", "for", "t", "in", "tokens", "]", "\n", "", "if", "\"EVAL_TOKENIZED\"", "in", "self", ".", "opt", ":", "\n", "            ", "return", "\" \"", ".", "join", "(", "tokens", ")", "\n", "", "else", ":", "\n", "            ", "return", "tokenizer", ".", "decode", "(", "\n", "tokenizer", ".", "convert_tokens_to_ids", "(", "tokens", ")", ",", "skip_special_tokens", "=", "True", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Evaluation.ROUGEEval.ROUGEEval.eval_batches": [[164, 323], ["int", "logger.info", "isinstance", "torch.no_grad", "enumerate", "len", "len", "len", "len", "len", "module", "predictions.extend", "gts.extend", "x_tokens.extend", "y_tokens.extend", "len", "len", "len", "len", "len", "len", "len", "len", "os.path.isdir", "os.makedirs", "open", "ROUGEEval.write_json_res", "ROUGEEval.ROUGEEval.eval", "summertime.model.third_party.HMNet.Evaluation.OldROUGEEval.rouge", "float", "len", "torch.is_tensor", "os.path.join", "os.path.join", "os.path.join", "logger.exception", "shutil.copyfile", "dev_batch[].to", "ROUGEEval.ROUGEEval._convert_tokens_to_string", "float", "os.path.join", "os.path.join", "len", "ROUGEEval.ROUGEEval._convert_tokens_to_string"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Evaluation.ROUGEEval.write_json_res", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.Transformer.BeamSearchNode.eval", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Evaluation.OldROUGEEval.rouge", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.dialogue.hmnet_model.HMNetModel._convert_tokens_to_string", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.dialogue.hmnet_model.HMNetModel._convert_tokens_to_string"], ["", "", "def", "eval_batches", "(", "self", ",", "module", ",", "dev_batches", ",", "save_folder", ",", "label", "=", "\"\"", ")", ":", "\n", "        ", "max_sent_len", "=", "int", "(", "self", ".", "opt", "[", "\"MAX_GEN_LENGTH\"", "]", ")", "\n", "\n", "logger", ".", "info", "(", "\n", "\"Decoding current model ... \\nSaving folder is {}\"", ".", "format", "(", "save_folder", ")", "\n", ")", "\n", "\n", "predictions", "=", "[", "]", "# prediction of tokens from model", "\n", "x_tokens", "=", "[", "]", "# input tokens", "\n", "y_tokens", "=", "[", "]", "# groundtruths tokens", "\n", "x_ids", "=", "[", "]", "# input token ids", "\n", "y_ids", "=", "[", "]", "# groundtruths token ids", "\n", "gts", "=", "[", "]", "# groundtruths string", "\n", "got_better_score", "=", "False", "\n", "# err = 0", "\n", "if", "not", "isinstance", "(", "module", ".", "tokenizer", ",", "list", ")", ":", "\n", "            ", "encoder_tokenizer", "=", "module", ".", "tokenizer", "\n", "decoder_tokenizer", "=", "module", ".", "tokenizer", "\n", "", "elif", "len", "(", "module", ".", "tokenizer", ")", "==", "1", ":", "\n", "            ", "encoder_tokenizer", "=", "module", ".", "tokenizer", "[", "0", "]", "\n", "decoder_tokenizer", "=", "module", ".", "tokenizer", "[", "0", "]", "\n", "", "elif", "len", "(", "module", ".", "tokenizer", ")", "==", "2", ":", "\n", "            ", "encoder_tokenizer", "=", "module", ".", "tokenizer", "[", "0", "]", "\n", "decoder_tokenizer", "=", "module", ".", "tokenizer", "[", "1", "]", "\n", "", "else", ":", "\n", "            ", "assert", "False", ",", "f\"len(module.tokenizer) > 2\"", "\n", "\n", "", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "for", "j", ",", "dev_batch", "in", "enumerate", "(", "dev_batches", ")", ":", "\n", "                ", "for", "b", "in", "dev_batch", ":", "\n", "                    ", "if", "torch", ".", "is_tensor", "(", "dev_batch", "[", "b", "]", ")", ":", "\n", "                        ", "dev_batch", "[", "b", "]", "=", "dev_batch", "[", "b", "]", ".", "to", "(", "self", ".", "opt", "[", "\"device\"", "]", ")", "\n", "\n", "", "", "beam_search_res", "=", "module", "(", "\n", "dev_batch", ",", "beam_search", "=", "True", ",", "max_sent_len", "=", "max_sent_len", "\n", ")", "\n", "pred", "=", "[", "\n", "[", "t", "[", "0", "]", "for", "t", "in", "x", "]", "if", "len", "(", "x", ")", ">", "0", "else", "[", "[", "]", "]", "for", "x", "in", "beam_search_res", "\n", "]", "\n", "predictions", ".", "extend", "(", "\n", "[", "\n", "[", "\n", "self", ".", "_convert_tokens_to_string", "(", "decoder_tokenizer", ",", "tt", ")", "\n", "for", "tt", "in", "t", "\n", "]", "\n", "for", "t", "in", "pred", "\n", "]", "\n", ")", "\n", "\n", "gts", ".", "extend", "(", "\n", "[", "\n", "self", ".", "_convert_tokens_to_string", "(", "decoder_tokenizer", ",", "t", ")", "\n", "for", "t", "in", "dev_batch", "[", "\"decoder_tokens\"", "]", "\n", "]", "\n", ")", "\n", "x_tokens", ".", "extend", "(", "dev_batch", "[", "\"encoder_tokens\"", "]", ")", "\n", "y_tokens", ".", "extend", "(", "dev_batch", "[", "\"decoder_tokens\"", "]", ")", "\n", "\n", "if", "(", "\"DEBUG\"", "in", "self", ".", "opt", "and", "j", ">=", "10", ")", "or", "j", ">=", "self", ".", "eval_batches_num", ":", "\n", "# in debug mode (decode first 10 batches) ortherwise decode first self.eval_batches_num bathes", "\n", "                    ", "break", "\n", "\n", "# use MPI to gather results from all processes / GPUs", "\n", "# the result of the gather operation is a list of sublists", "\n", "# each sublist corresponds to the list created on one of the MPI processes (or GPUs, respectively)", "\n", "# we flatten this list into a \"simple\" list", "\n", "", "", "", "assert", "len", "(", "predictions", ")", "==", "len", "(", "\n", "gts", "\n", ")", ",", "\"len(predictions): {0}, len(gts): {1}\"", ".", "format", "(", "len", "(", "predictions", ")", ",", "len", "(", "gts", ")", ")", "\n", "# comm = MPI.COMM_WORLD", "\n", "# predictions = comm.gather(predictions, root=0)", "\n", "# x_tokens = comm.gather(x_tokens, root=0)", "\n", "# y_tokens = comm.gather(y_tokens, root=0)", "\n", "# if GPU numbers are high (>=8), passing x_ids, y_ids to a rank 0 will cause out of memory", "\n", "# x_ids = comm.gather(x_ids, root=0)", "\n", "# y_ids = comm.gather(y_ids, root=0)", "\n", "# gts = comm.gather(gts, root=0)", "\n", "if", "self", ".", "opt", "[", "\"rank\"", "]", "==", "0", ":", "\n", "# flatten lists", "\n", "            ", "predictions", "=", "[", "item", "for", "sublist", "in", "predictions", "for", "item", "in", "sublist", "]", "\n", "y_tokens", "=", "[", "item", "for", "sublist", "in", "y_tokens", "for", "item", "in", "sublist", "]", "\n", "x_tokens", "=", "[", "item", "for", "sublist", "in", "x_tokens", "for", "item", "in", "sublist", "]", "\n", "# x_ids = [item for sublist in x_ids for item in sublist]", "\n", "# y_ids = [item for sublist in y_ids for item in sublist]", "\n", "gts", "=", "[", "item", "for", "sublist", "in", "gts", "for", "item", "in", "sublist", "]", "\n", "# import pdb; pdb.set_trace()", "\n", "assert", "(", "\n", "len", "(", "predictions", ")", "==", "len", "(", "y_tokens", ")", "==", "len", "(", "x_tokens", ")", "==", "len", "(", "gts", ")", "\n", ")", ",", "\"len(predictions): {0}, len(y_tokens): {1}, len(x_tokens): {2}, len(gts): {3}\"", ".", "format", "(", "\n", "len", "(", "predictions", ")", ",", "len", "(", "y_tokens", ")", ",", "len", "(", "x_tokens", ")", ",", "len", "(", "gts", ")", "\n", ")", "\n", "\n", "# write intermediate results only on rank 0", "\n", "if", "not", "os", ".", "path", ".", "isdir", "(", "os", ".", "path", ".", "join", "(", "save_folder", ",", "\"intermediate_results\"", ")", ")", ":", "\n", "                ", "os", ".", "makedirs", "(", "os", ".", "path", ".", "join", "(", "save_folder", ",", "\"intermediate_results\"", ")", ")", "\n", "", "top_1_predictions", "=", "[", "pred", "[", "0", "]", "for", "pred", "in", "predictions", "]", "\n", "with", "open", "(", "\n", "os", ".", "path", ".", "join", "(", "\n", "save_folder", ",", "\"intermediate_results\"", ",", "\"res_\"", "+", "label", "+", "\".json\"", "\n", ")", ",", "\n", "\"w\"", ",", "\n", "encoding", "=", "\"utf-8\"", ",", "\n", ")", "as", "output_file", ":", "\n", "                ", "write_json_res", "(", "\n", "output_file", ",", "\n", "[", "encoder_tokenizer", ",", "decoder_tokenizer", "]", ",", "\n", "x_ids", ",", "\n", "y_ids", ",", "\n", "x_tokens", ",", "\n", "y_tokens", ",", "\n", "predictions", ",", "\n", "gts", ",", "\n", ")", "\n", "", "try", ":", "\n", "                ", "result", "=", "self", ".", "eval", "(", "top_1_predictions", ",", "gts", ")", "\n", "", "except", "Exception", "as", "e", ":", "\n", "                ", "logger", ".", "exception", "(", "\"ROUGE Eval ERROR\"", ")", "\n", "result", "=", "{", "}", "\n", "score", "=", "-", "float", "(", "\"Inf\"", ")", "\n", "pass", "# this happens when no overlapping between pred and gts", "\n", "", "else", ":", "\n", "                ", "rouge_su4", "=", "rouge", "(", "top_1_predictions", ",", "gts", ")", "# f, prec, recall", "\n", "result", "=", "{", "\n", "\"ROUGE_1\"", ":", "result", "[", "\"rouge_1_f_score\"", "]", "*", "100.0", ",", "\n", "\"ROUGE_1_Prc\"", ":", "result", "[", "\"rouge_1_precision\"", "]", "*", "100.0", ",", "\n", "\"ROUGE_1_Rcl\"", ":", "result", "[", "\"rouge_1_recall\"", "]", "*", "100.0", ",", "\n", "\"ROUGE_2\"", ":", "result", "[", "\"rouge_2_f_score\"", "]", "*", "100.0", ",", "\n", "\"ROUGE_2_Prc\"", ":", "result", "[", "\"rouge_2_precision\"", "]", "*", "100.0", ",", "\n", "\"ROUGE_2_Rcl\"", ":", "result", "[", "\"rouge_2_recall\"", "]", "*", "100.0", ",", "\n", "\"ROUGE_L\"", ":", "result", "[", "\"rouge_l_f_score\"", "]", "*", "100.0", ",", "\n", "\"ROUGE_L_Prc\"", ":", "result", "[", "\"rouge_l_precision\"", "]", "*", "100.0", ",", "\n", "\"ROUGE_L_Rcl\"", ":", "result", "[", "\"rouge_l_recall\"", "]", "*", "100.0", ",", "\n", "\"ROUGE_SU4\"", ":", "rouge_su4", "[", "\"rouge_su4_f_score\"", "]", "*", "100.0", ",", "\n", "}", "\n", "\n", "score", "=", "result", "[", "\"ROUGE_1\"", "]", "\n", "if", "score", ">", "self", ".", "best_score", ":", "\n", "                    ", "copyfile", "(", "\n", "os", ".", "path", ".", "join", "(", "\n", "save_folder", ",", "\n", "\"intermediate_results\"", ",", "\n", "\"res_\"", "+", "label", "+", "\".json\"", ",", "\n", ")", ",", "\n", "os", ".", "path", ".", "join", "(", "\n", "save_folder", ",", "\n", "\"intermediate_results\"", ",", "\n", "\"res_\"", "+", "label", "+", "\".best.json\"", ",", "\n", ")", ",", "\n", ")", "\n", "self", ".", "best_score", "=", "score", "\n", "self", ".", "best_res", "=", "result", "\n", "got_better_score", "=", "True", "\n", "\n", "", "", "", "else", ":", "\n", "            ", "result", "=", "{", "}", "\n", "score", "=", "-", "float", "(", "\"Inf\"", ")", "\n", "got_better_score", "=", "False", "\n", "\n", "", "return", "result", ",", "score", ",", "got_better_score", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Evaluation.ROUGEEval.ROUGEEval.eval": [[324, 356], ["os.path.join", "os.path.exists", "os.makedirs", "os.path.join", "os.path.exists", "os.makedirs", "ROUGEEval.ROUGEEval.print_to_rouge_dir_gt", "ROUGEEval.ROUGEEval.print_to_rouge_dir", "summertime.model.third_party.HMNet.ThirdParty.ROUGE.pyrouge.Rouge155", "summertime.model.third_party.HMNet.ThirdParty.ROUGE.pyrouge.Rouge155.output_to_dict", "ROUGEEval.ROUGEEval.make_html_safe", "ROUGEEval.ROUGEEval.make_html_safe", "shutil.rmtree", "shutil.rmtree", "summertime.model.third_party.HMNet.ThirdParty.ROUGE.pyrouge.Rouge155.convert_and_evaluate"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Evaluation.ROUGEEval.ROUGEEval.print_to_rouge_dir_gt", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Evaluation.ROUGEEval.ROUGEEval.print_to_rouge_dir", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.pyrouge.Rouge155.Rouge155.output_to_dict", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Evaluation.ROUGEEval.ROUGEEval.make_html_safe", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Evaluation.ROUGEEval.ROUGEEval.make_html_safe", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.pyrouge.Rouge155.Rouge155.convert_and_evaluate"], ["", "def", "eval", "(", "self", ",", "predictions", ",", "groundtruths", ")", ":", "\n", "# predictions, groundtruths = self.filter_empty(predictions, groundtruths)", "\n", "        ", "predictions", "=", "[", "self", ".", "make_html_safe", "(", "w", ")", "for", "w", "in", "predictions", "]", "\n", "groundtruths", "=", "[", "self", ".", "make_html_safe", "(", "w", ")", "for", "w", "in", "groundtruths", "]", "\n", "pred_dir", "=", "os", ".", "path", ".", "join", "(", "self", ".", "save_dir", ",", "\"predictions\"", ")", "\n", "if", "os", ".", "path", ".", "exists", "(", "pred_dir", ")", ":", "\n", "            ", "shutil", ".", "rmtree", "(", "pred_dir", ")", "\n", "", "os", ".", "makedirs", "(", "pred_dir", ")", "\n", "\n", "gt_dir", "=", "os", ".", "path", ".", "join", "(", "self", ".", "save_dir", ",", "\"groundtruths\"", ")", "\n", "if", "os", ".", "path", ".", "exists", "(", "gt_dir", ")", ":", "\n", "            ", "shutil", ".", "rmtree", "(", "gt_dir", ")", "\n", "", "os", ".", "makedirs", "(", "gt_dir", ")", "\n", "\n", "special_char_dict", "=", "self", ".", "print_to_rouge_dir_gt", "(", "\n", "groundtruths", ",", "gt_dir", ",", "\"gt\"", ",", "\"SPLIT_CHARS_FOR_EVAL\"", "in", "self", ".", "opt", "\n", ")", "\n", "self", ".", "print_to_rouge_dir", "(", "\n", "predictions", ",", "\n", "pred_dir", ",", "\n", "\"pred\"", ",", "\n", "\"SPLIT_CHARS_FOR_EVAL\"", "in", "self", ".", "opt", ",", "\n", "special_char_dict", ",", "\n", ")", "\n", "\n", "r", "=", "pyrouge", ".", "Rouge155", "(", "self", ".", "pyrouge_dir", ")", "\n", "r", ".", "system_dir", "=", "pred_dir", "\n", "r", ".", "model_dir", "=", "gt_dir", "\n", "r", ".", "system_filename_pattern", "=", "\"(\\d+)_pred.txt\"", "\n", "r", ".", "model_filename_pattern", "=", "\"[A-Z].#ID#_gt.txt\"", "\n", "results", "=", "r", ".", "output_to_dict", "(", "r", ".", "convert_and_evaluate", "(", ")", ")", "\n", "return", "results", "\n", "", "", ""]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Evaluation.ROUGEEval.write_json_res": [[19, 40], ["zip", "json.dump", "data.append", "isinstance", "isinstance"], "function", ["None"], ["def", "write_json_res", "(", "\n", "output_file", ",", "tokenizers", ",", "x_ids", ",", "y_ids", ",", "x_tokens", ",", "y_tokens", ",", "predictions", ",", "gts", "\n", ")", ":", "\n", "    ", "data", "=", "[", "]", "\n", "\n", "# for x_id, y_id, x_token, y_token, preds, gt in zip(x_ids, y_ids, x_tokens, y_tokens, predictions, gts):", "\n", "# x_id = tokenizers[0].decode(x_id, skip_special_tokens=False) if x_id.dim() == 1 else tokenizers[0].convert_tokens_to_string(x_token)", "\n", "# y_id = tokenizers[1].decode(y_id, skip_special_tokens=False) if y_id.dim() == 1 else tokenizers[1].convert_tokens_to_string(y_token)", "\n", "for", "x_token", ",", "y_token", ",", "preds", ",", "gt", "in", "zip", "(", "x_tokens", ",", "y_tokens", ",", "predictions", ",", "gts", ")", ":", "\n", "        ", "data", ".", "append", "(", "\n", "{", "\n", "# 'x_ids': x_id,", "\n", "# 'y_ids': y_id,", "\n", "\"x_tokens\"", ":", "x_token", "if", "isinstance", "(", "x_token", ",", "str", ")", "else", "\" \"", ".", "join", "(", "x_token", ")", ",", "\n", "\"y_tokens\"", ":", "y_token", "if", "isinstance", "(", "y_token", ",", "str", ")", "else", "\" \"", ".", "join", "(", "y_token", ")", ",", "\n", "\"predictions\"", ":", "preds", ",", "\n", "\"gt\"", ":", "gt", ",", "\n", "}", "\n", ")", "\n", "\n", "", "json", ".", "dump", "(", "data", ",", "output_file", ",", "indent", "=", "4", ",", "ensure_ascii", "=", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Utils.GeneralUtils.ObjectView.__init__": [[20, 22], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "d", ")", ":", "\n", "        ", "self", ".", "__dict__", "=", "d", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Utils.GeneralUtils.AverageMeter.__init__": [[27, 29], ["GeneralUtils.AverageMeter.reset"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Utils.GeneralUtils.AverageMeter.reset"], ["def", "__init__", "(", "self", ")", ":", "\n", "        ", "self", ".", "reset", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Utils.GeneralUtils.AverageMeter.reset": [[30, 35], ["None"], "methods", ["None"], ["", "def", "reset", "(", "self", ")", ":", "\n", "        ", "self", ".", "val", "=", "0", "\n", "self", ".", "avg", "=", "0", "\n", "self", ".", "sum", "=", "0", "\n", "self", ".", "count", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Utils.GeneralUtils.AverageMeter.update": [[36, 46], ["math.exp"], "methods", ["None"], ["", "def", "update", "(", "self", ",", "val", ",", "n", "=", "1", ",", "decay", "=", "0", ")", ":", "\n", "        ", "self", ".", "val", "=", "val", "\n", "if", "decay", ":", "\n", "            ", "alpha", "=", "math", ".", "exp", "(", "-", "n", "/", "decay", ")", "# exponential decay over 100 updates", "\n", "self", ".", "sum", "=", "alpha", "*", "self", ".", "sum", "+", "(", "1", "-", "alpha", ")", "*", "val", "*", "n", "\n", "self", ".", "count", "=", "alpha", "*", "self", ".", "count", "+", "(", "1", "-", "alpha", ")", "*", "n", "\n", "", "else", ":", "\n", "            ", "self", ".", "sum", "+=", "val", "*", "n", "\n", "self", ".", "count", "+=", "n", "\n", "", "self", ".", "avg", "=", "self", ".", "sum", "/", "self", ".", "count", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Utils.GeneralUtils.BaseBatchGen.__init__": [[59, 89], ["None"], "methods", ["None"], ["def", "__init__", "(", "\n", "self", ",", "\n", "task_args", ",", "\n", "dataset_label", ",", "\n", "model_config", "=", "None", ",", "\n", "tokenizer", "=", "None", ",", "\n", "world_size", "=", "1", ",", "\n", "rank", "=", "0", ",", "\n", "seed", "=", "None", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            task_args (dict): dictionary records arguments\n            dataset_label (str): 'train', 'dev' or 'test'\n            model_config: config of the model\n            tokenizer: tokenizer used to process text\n            world_size (int): total number of GPUs\n            rank (int): order of current GPU\n            seed (int): random seed\n        \"\"\"", "\n", "self", ".", "opt", "=", "task_args", "\n", "self", ".", "dataset_label", "=", "dataset_label", "\n", "self", ".", "model_config", "=", "model_config", "\n", "self", ".", "tokenizer", "=", "tokenizer", "\n", "self", ".", "world_size", "=", "world_size", "\n", "self", ".", "rank", "=", "rank", "\n", "self", ".", "seed", "=", "seed", "\n", "self", ".", "evaluation", "=", "dataset_label", "in", "[", "\"dev\"", ",", "\"test\"", "]", "\n", "\n", "self", ".", "_iter", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Utils.GeneralUtils.BaseBatchGen._build_iter": [[90, 95], ["NotImplementedError"], "methods", ["None"], ["", "def", "_build_iter", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Build infinibatch iterator and assign to self._iter\n        \"\"\"", "\n", "raise", "NotImplementedError", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Utils.GeneralUtils.BaseBatchGen.iterator": [[96, 101], ["NotImplementedError"], "methods", ["None"], ["", "@", "property", "\n", "def", "iterator", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "_iter", "is", "None", ":", "\n", "            ", "raise", "NotImplementedError", "(", "\"_build_iter() must called first\"", ")", "\n", "", "return", "self", ".", "_iter", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Utils.GeneralUtils.BaseBatchGen.__iter__": [[102, 106], ["NotImplementedError"], "methods", ["None"], ["", "def", "__iter__", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "_iter", "is", "None", ":", "\n", "            ", "raise", "NotImplementedError", "(", "\"_build_iter() must called first\"", ")", "\n", "", "return", "self", ".", "_iter", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Utils.GeneralUtils.BaseBatchGen.__next__": [[107, 109], ["next"], "methods", ["None"], ["", "def", "__next__", "(", "self", ")", ":", "\n", "        ", "return", "next", "(", "self", ".", "_iter", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Utils.GeneralUtils.move_batch_to_device": [[111, 139], ["torch.is_tensor", "torch.is_tensor", "batch.to", "isinstance", "isinstance", "GeneralUtils.move_batch_to_device", "tuple", "isinstance", "logger.debug", "GeneralUtils.move_batch_to_device", "GeneralUtils.move_batch_to_device", "type"], "function", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Utils.GeneralUtils.move_batch_to_device", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Utils.GeneralUtils.move_batch_to_device", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Utils.GeneralUtils.move_batch_to_device"], ["", "", "def", "move_batch_to_device", "(", "batch", ",", "device", ")", ":", "\n", "    ", "\"\"\"\n    Move the batch to the device.\n    It should be called before feeding the batch to the model.\n\n    Args:\n        batch (torch.tensor or container of torch.tensor): input batch\n        device (torch.device): device to move the batch to\n    Returns:\n        return_batch: same type as the input batch with internal tensors moved to device\n    \"\"\"", "\n", "if", "torch", ".", "is_tensor", "(", "batch", ")", ":", "\n", "        ", "return_batch", "=", "batch", ".", "to", "(", "device", ")", "\n", "", "elif", "isinstance", "(", "batch", ",", "list", ")", ":", "\n", "        ", "return_batch", "=", "[", "move_batch_to_device", "(", "t", ",", "device", ")", "for", "t", "in", "batch", "]", "\n", "", "elif", "isinstance", "(", "batch", ",", "tuple", ")", ":", "\n", "        ", "return_batch", "=", "tuple", "(", "move_batch_to_device", "(", "t", ",", "device", ")", "for", "t", "in", "batch", ")", "\n", "", "elif", "isinstance", "(", "batch", ",", "dict", ")", ":", "\n", "        ", "return_batch", "=", "{", "}", "\n", "for", "k", "in", "batch", ":", "\n", "            ", "return_batch", "[", "k", "]", "=", "move_batch_to_device", "(", "batch", "[", "k", "]", ",", "device", ")", "\n", "", "", "else", ":", "\n", "        ", "logger", ".", "debug", "(", "\n", "f\"Can not move type {type(batch)} to device. Skipping it in the batch.\"", "\n", ")", "\n", "return_batch", "=", "batch", "\n", "\n", "", "return", "return_batch", "\n", "", ""]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Utils.distributed.distributed": [[8, 46], ["torch.device", "torch.cuda.set_device", "torch.device", "torch.cuda.is_available", "int", "int", "int", "int", "torch.distributed.is_available", "str", "str", "torch.distributed.init_process_group", "print"], "function", ["None"], ["def", "distributed", "(", "opt", ",", "is_nocuda", ")", ":", "\n", "    ", "cluster", "=", "opt", "[", "\"cluster\"", "]", "\n", "world_size", "=", "1", "\n", "local_size", "=", "1", "\n", "rank", "=", "0", "\n", "local_rank", "=", "0", "\n", "is_master", "=", "True", "\n", "run", "=", "None", "\n", "\n", "if", "is_nocuda", "or", "not", "torch", ".", "cuda", ".", "is_available", "(", ")", ":", "\n", "        ", "device", "=", "torch", ".", "device", "(", "\"cpu\"", ")", "\n", "n_gpu", "=", "0", "\n", "", "else", ":", "\n", "        ", "if", "\"OMPI_COMM_WORLD_SIZE\"", "in", "os", ".", "environ", ":", "\n", "            ", "world_size", "=", "int", "(", "os", ".", "environ", "[", "\"OMPI_COMM_WORLD_SIZE\"", "]", ")", "\n", "local_size", "=", "int", "(", "os", ".", "environ", "[", "\"OMPI_COMM_WORLD_LOCAL_SIZE\"", "]", ")", "\n", "rank", "=", "int", "(", "os", ".", "environ", "[", "\"OMPI_COMM_WORLD_RANK\"", "]", ")", "\n", "local_rank", "=", "int", "(", "os", ".", "environ", "[", "\"OMPI_COMM_WORLD_LOCAL_RANK\"", "]", ")", "\n", "is_master", "=", "rank", "==", "0", "\n", "\n", "", "torch", ".", "cuda", ".", "set_device", "(", "local_rank", ")", "\n", "device", "=", "torch", ".", "device", "(", "\"cuda\"", ",", "local_rank", ")", "\n", "n_gpu", "=", "1", "\n", "# the following assumes that all processes run on a single node", "\n", "if", "torch", ".", "distributed", ".", "is_available", "(", ")", "and", "world_size", ">", "1", ":", "\n", "# Initializes the distributed backend which will take care of sychronizing nodes/GPUs", "\n", "            ", "os", ".", "environ", "[", "\"WORLD_SIZE\"", "]", "=", "str", "(", "world_size", ")", "\n", "os", ".", "environ", "[", "\"RANK\"", "]", "=", "str", "(", "rank", ")", "\n", "os", ".", "environ", "[", "\"MASTER_ADDR\"", "]", "=", "\"127.0.0.1\"", "\n", "os", ".", "environ", "[", "\"MASTER_PORT\"", "]", "=", "(", "\n", "opt", "[", "\"master_port\"", "]", "if", "\"master_port\"", "in", "opt", "else", "\"35551\"", "\n", ")", "\n", "torch", ".", "distributed", ".", "init_process_group", "(", "\n", "backend", "=", "\"nccl\"", "\n", ")", "# using environment variable initialization", "\n", "print", "(", "\"Distributed package is available. Process group initialized.\"", ")", "\n", "\n", "", "", "return", "device", ",", "n_gpu", ",", "world_size", ",", "local_size", ",", "rank", ",", "local_rank", ",", "is_master", ",", "run", "\n", "", ""]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Utils.Serialization.NumpyJSONEncoder.default": [[9, 18], ["isinstance", "int", "isinstance", "float", "isinstance", "obj.tolist", "super().default"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Utils.Serialization.NumpyJSONEncoder.default"], ["    ", "def", "default", "(", "self", ",", "obj", ")", ":", "\n", "        ", "if", "isinstance", "(", "obj", ",", "np", ".", "integer", ")", ":", "\n", "            ", "return", "int", "(", "obj", ")", "\n", "", "elif", "isinstance", "(", "obj", ",", "np", ".", "floating", ")", ":", "\n", "            ", "return", "float", "(", "obj", ")", "\n", "", "elif", "isinstance", "(", "obj", ",", "np", ".", "ndarray", ")", ":", "\n", "            ", "return", "obj", ".", "tolist", "(", ")", "\n", "", "else", ":", "\n", "            ", "return", "super", "(", "NumpyJSONEncoder", ",", "self", ")", ".", "default", "(", "obj", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Utils.Arguments.Arguments.__init__": [[8, 12], ["os.path.exists", "Exception"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "confFile", ")", ":", "\n", "        ", "if", "not", "os", ".", "path", ".", "exists", "(", "confFile", ")", ":", "\n", "            ", "raise", "Exception", "(", "\"The argument file does not exist: \"", "+", "confFile", ")", "\n", "", "self", ".", "confFile", "=", "confFile", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Utils.Arguments.Arguments.is_int": [[13, 19], ["int"], "methods", ["None"], ["", "def", "is_int", "(", "self", ",", "s", ")", ":", "\n", "        ", "try", ":", "\n", "            ", "int", "(", "s", ")", "\n", "return", "True", "\n", "", "except", "ValueError", ":", "\n", "            ", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Utils.Arguments.Arguments.is_float": [[20, 26], ["float"], "methods", ["None"], ["", "", "def", "is_float", "(", "self", ",", "s", ")", ":", "\n", "        ", "try", ":", "\n", "            ", "float", "(", "s", ")", "\n", "return", "True", "\n", "", "except", "ValueError", ":", "\n", "            ", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Utils.Arguments.Arguments.is_bool": [[27, 29], ["s.lower", "s.lower"], "methods", ["None"], ["", "", "def", "is_bool", "(", "self", ",", "s", ")", ":", "\n", "        ", "return", "s", ".", "lower", "(", ")", "==", "\"true\"", "or", "s", ".", "lower", "(", ")", "==", "\"false\"", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Utils.Arguments.Arguments.add_opt": [[42, 53], ["Arguments.Arguments.is_int", "print", "int", "Arguments.Arguments.is_float", "float", "Arguments.Arguments.is_bool", "value.lower"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Utils.Arguments.Arguments.is_int", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Utils.Arguments.Arguments.is_float", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Utils.Arguments.Arguments.is_bool"], ["", "def", "add_opt", "(", "self", ",", "opt", ",", "key", ",", "value", ",", "force_override", "=", "False", ")", ":", "\n", "        ", "if", "not", "key", "in", "opt", "or", "force_override", ":", "\n", "            ", "opt", "[", "key", "]", "=", "value", "\n", "if", "self", ".", "is_int", "(", "value", ")", ":", "\n", "                ", "opt", "[", "key", "]", "=", "int", "(", "value", ")", "\n", "", "elif", "self", ".", "is_float", "(", "value", ")", ":", "\n", "                ", "opt", "[", "key", "]", "=", "float", "(", "value", ")", "\n", "", "elif", "self", ".", "is_bool", "(", "value", ")", ":", "\n", "                ", "opt", "[", "key", "]", "=", "value", ".", "lower", "(", ")", "==", "\"true\"", "\n", "", "", "else", ":", "\n", "            ", "print", "(", "\"Warning: Option key %s already exists\"", "%", "key", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Utils.Arguments.Arguments.readArguments": [[54, 92], ["open", "line.endswith", "line.replace", "line.replace.find", "line.replace.split", "line.strip", "len", "Arguments.Arguments.add_opt"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.utils.sentence_splitter.PunktSentenceSplitter.split", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Utils.Arguments.Arguments.add_opt"], ["", "", "def", "readArguments", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Parse config file.\n\n        Supported syntax:\n         - general form: var WHITESPACE val, with WHITESPACE=space or TAB\n         - whole-line or line-end comments begin with #\n         - lines that end with backslash are continuation lines\n         - multiple values are white-space separated, hence no spaces allowed in keys or values\n        \"\"\"", "\n", "opt", "=", "{", "}", "\n", "with", "open", "(", "self", ".", "confFile", ",", "encoding", "=", "\"utf-8\"", ")", "as", "f", ":", "\n", "            ", "prev_line", "=", "\"\"", "# allow multi-line arguments", "\n", "for", "line", "in", "f", ":", "\n", "# concatenate previous line if it ended in backslash", "\n", "                ", "line", "=", "prev_line", "+", "line", ".", "strip", "(", ")", "\n", "if", "line", ".", "endswith", "(", "\"\\\\\"", ")", ":", "\n", "                    ", "prev_line", "=", "line", "[", ":", "-", "1", "]", "+", "\" \"", "\n", "continue", "\n", "", "prev_line", "=", "\"\"", "\n", "l", "=", "line", ".", "replace", "(", "\"\\t\"", ",", "\" \"", ")", "\n", "# strip comments", "\n", "pos", "=", "l", ".", "find", "(", "\"#\"", ")", "\n", "if", "pos", ">=", "0", ":", "\n", "                    ", "l", "=", "l", "[", ":", "pos", "]", "\n", "", "parts", "=", "l", ".", "split", "(", ")", "\n", "if", "not", "parts", ":", "\n", "                    ", "continue", "# empty line or line comment", "\n", "", "elif", "len", "(", "parts", ")", "==", "1", ":", "\n", "                    ", "key", "=", "parts", "[", "0", "]", "\n", "if", "not", "key", "in", "opt", ":", "\n", "                        ", "opt", "[", "key", "]", "=", "True", "\n", "", "", "else", ":", "\n", "                    ", "key", "=", "parts", "[", "0", "]", "\n", "value", "=", "\" \"", ".", "join", "(", "parts", "[", "1", ":", "]", ")", "\n", "self", ".", "add_opt", "(", "opt", ",", "key", ",", "value", ")", "\n", "", "", "assert", "not", "prev_line", ",", "\"Config file must not end with a backslash\"", "\n", "", "return", "opt", "\n", "", "", ""]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.HMNet.InfinibatchLoader._bump_seed": [[24, 29], ["None"], "function", ["None"], ["def", "_bump_seed", "(", "seed", ")", ":", "\n", "    ", "\"\"\"\n    Helper to bump a random seed if not None.\n    \"\"\"", "\n", "return", "None", "if", "seed", "is", "None", "else", "seed", "+", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.HMNet.InfinibatchLoader.HMNetBatchGen": [[31, 689], ["os.path.join", "json.load", "task_args.get", "os.path.join", "task_args.get", "task_args.get", "task_args.get", "task_args.get", "task_args.get", "task_args.get", "task_args.get", "task_args.get", "json.load", "enumerate", "max", "max", "max", "max", "enumerate", "enumerate", "summertime.model.third_party.HMNet.DataLoader.iterators.ZipIterator", "summertime.model.third_party.HMNet.DataLoader.iterators.SelectManyIterator", "InfinibatchLoader._bump_seed", "summertime.model.third_party.HMNet.DataLoader.iterators.BufferedShuffleIterator", "summertime.model.third_party.HMNet.DataLoader.iterators.SelectManyIterator", "open", "open", "source_chunk_files.sort", "datasets_chunks.sort", "enumerate", "datasets_chunks[].sort", "summertime.model.third_party.HMNet.DataLoader.iterators.ChunkedSourceIterator", "datasets_doc_samples.append", "enumerate", "timeit.default_timer", "timeit.default_timer", "InfinibatchLoader._bump_seed", "summertime.model.third_party.HMNet.DataLoader.iterators.SamplingRandomMapIterator", "enumerate", "datasets_samples.append", "max", "InfinibatchLoader._bump_seed", "datasets_batches.append", "max", "max", "max", "max", "len", "torch.LongTensor().fill_", "torch.LongTensor().fill_", "torch.LongTensor().fill_", "torch.LongTensor().fill_", "torch.LongTensor().fill_", "range", "InfinibatchLoader.HMNetBatchGen._pad_batch"], "function", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.BaseTrainer.BaseTrainer.load", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.closablequeue.ClosableQueue.get", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.closablequeue.ClosableQueue.get", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.closablequeue.ClosableQueue.get", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.closablequeue.ClosableQueue.get", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.closablequeue.ClosableQueue.get", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.closablequeue.ClosableQueue.get", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.closablequeue.ClosableQueue.get", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.closablequeue.ClosableQueue.get", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.closablequeue.ClosableQueue.get", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.BaseTrainer.BaseTrainer.load", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.HMNet.InfinibatchLoader._bump_seed", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.iterators.ChunkedSourceIterator", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.HMNet.InfinibatchLoader._bump_seed", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.iterators.SamplingRandomMapIterator", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.HMNet.InfinibatchLoader._bump_seed"], ["", "def", "HMNetBatchGen", "(", "\n", "task_args", ",", "\n", "dataset_label", ",", "\n", "model_config", "=", "None", ",", "\n", "tokenizer", "=", "None", ",", "\n", "world_size", "=", "None", ",", "\n", "rank", "=", "None", ",", "\n", "seed", "=", "None", ",", "\n", ")", ":", "\n", "    ", "\"\"\"\n    This example batch generater creates simple MLM training batches\n    It take paths to the dataset directories, and produce final iterator that yields tensors of a batch\n    It performs file reading, shuffling, tokenization, masking, batching, collating by nesting the iterators in the DataLoader infinibatch library\n    arguments:\n        task_args: a dict containing parameters for the task\n        dataset_label: train, dev, or test\n        model_config: model architecture config\n        tokenizer: a list of tokenizers\n        world_size, rank: GPU world size and rank for distributed training\n    Note: this batch generator does not move the batches to the GPU. The caller must do that as desired.\n    \"\"\"", "\n", "\n", "role_dict_file", "=", "os", ".", "path", ".", "join", "(", "task_args", "[", "\"datadir\"", "]", ",", "task_args", "[", "\"ROLE_DICT_FILE\"", "]", ")", "\n", "role_dict", "=", "json", ".", "load", "(", "open", "(", "role_dict_file", ")", ")", "\n", "inv_role_dict", "=", "{", "v", ":", "k", "for", "k", ",", "v", "in", "role_dict", ".", "items", "(", ")", "}", "\n", "anon_roles", "=", "task_args", ".", "get", "(", "\n", "\"ANONYMOUS_ROLES\"", ",", "False", "\n", ")", "# whether to convert all speakers to speaker-0, speaker-1, ...", "\n", "\n", "dataset_file", "=", "os", ".", "path", ".", "join", "(", "\n", "task_args", "[", "\"datadir\"", "]", ",", "task_args", "[", "\"{}_FILE\"", ".", "format", "(", "dataset_label", ".", "upper", "(", ")", ")", "]", "\n", ")", "\n", "is_train", "=", "dataset_label", "==", "\"train\"", "\n", "tokens_per_batch", "=", "task_args", "[", "\"MINI_BATCH\"", "]", "*", "task_args", "[", "\"MAX_TRANSCRIPT_WORD\"", "]", "\n", "batch_read_ahead", "=", "task_args", "[", "\"BATCH_READ_AHEAD\"", "]", "\n", "doc_shuffle_buffer_size", "=", "task_args", "[", "\"DOC_SHUFFLE_BUF_SIZE\"", "]", "\n", "sample_shuffle_buffer_size", "=", "task_args", "[", "\"SAMPLE_SHUFFLE_BUFFER_SIZE\"", "]", "\n", "batch_shuffle_buffer_size", "=", "task_args", "[", "\"BATCH_SHUFFLE_BUFFER_SIZE\"", "]", "\n", "\n", "max_padding_ratio", "=", "task_args", ".", "get", "(", "\"MAX_PADDING_RATIO\"", ",", "1.0", ")", "\n", "\n", "max_gen_length", "=", "task_args", ".", "get", "(", "\"MAX_GEN_LENGTH\"", ",", "200", ")", "\n", "max_transcript_len", "=", "task_args", ".", "get", "(", "\"MAX_TRANSCRIPT_WORD\"", ",", "8300", ")", "\n", "max_sentence_len", "=", "task_args", ".", "get", "(", "\"MAX_SENT_LEN\"", ",", "30", ")", "\n", "max_sentence_num", "=", "task_args", ".", "get", "(", "\"MAX_SENT_NUM\"", ",", "400", ")", "\n", "\n", "merge_summary_buffer_size", "=", "task_args", ".", "get", "(", "\"MERGE_SUMMARY_BUFFER_SIZE\"", ",", "24", ")", "\n", "merge_summary_num", "=", "task_args", ".", "get", "(", "\"MERGE_SUMMARY_NUM\"", ",", "1", ")", "\n", "merge_summary_shuffle", "=", "task_args", ".", "get", "(", "\"MERGE_SUMMARY_SHUFFLE\"", ",", "False", ")", "\n", "\n", "###############################", "\n", "# set up rank-aware chunk file path iterator", "\n", "# this part can be used as is in all tasks", "\n", "###############################", "\n", "# dataset_file is the path to a json file containing dataset information", "\n", "data_sets", "=", "json", ".", "load", "(", "open", "(", "dataset_file", ",", "encoding", "=", "\"utf-8\"", ")", ")", "\n", "\n", "# get paths to all the chunk files in the source and target dataset dirs", "\n", "datasets_chunks", "=", "[", "]", "\n", "for", "i", ",", "data_set", "in", "enumerate", "(", "data_sets", ")", ":", "\n", "        ", "task", "=", "data_set", "[", "\"task\"", "]", "\n", "dataset_name", "=", "data_set", "[", "\"name\"", "]", "\n", "source", "=", "data_set", "[", "\"source\"", "]", "\n", "# to determine if use relative path to load dataset", "\n", "if", "\"USE_REL_DATA_PATH\"", "in", "task_args", ":", "\n", "            ", "source", "[", "\"dataset\"", "]", "=", "os", ".", "path", ".", "join", "(", "task_args", "[", "\"datadir\"", "]", ",", "source", "[", "\"dataset\"", "]", ")", "\n", "", "source_chunk_files", "=", "[", "\n", "x", "for", "x", "in", "os", ".", "scandir", "(", "source", "[", "\"dataset\"", "]", ")", "if", "x", ".", "name", ".", "endswith", "(", "\".gz\"", ")", "\n", "]", "# enumerate all .gz files in the given paths", "\n", "source_chunk_files", ".", "sort", "(", "key", "=", "lambda", "x", ":", "x", ".", "name", ")", "\n", "if", "\"target\"", "in", "data_set", ":", "\n", "            ", "target", "=", "data_set", "[", "\"target\"", "]", "\n", "if", "\"USE_REL_DATA_PATH\"", "in", "task_args", ":", "\n", "                ", "target", "[", "\"dataset\"", "]", "=", "os", ".", "path", ".", "join", "(", "\n", "task_args", "[", "\"datadir\"", "]", ",", "target", "[", "\"dataset\"", "]", "\n", ")", "\n", "\n", "", "target_chunk_files", "=", "[", "\n", "x", "for", "x", "in", "os", ".", "scandir", "(", "target", "[", "\"dataset\"", "]", ")", "if", "x", ".", "name", ".", "endswith", "(", "\".gz\"", ")", "\n", "]", "# enumerate all .gz files in the given paths", "\n", "target_chunk_files", ".", "sort", "(", "key", "=", "lambda", "x", ":", "x", ".", "name", ")", "\n", "assert", "len", "(", "source_chunk_files", ")", "==", "len", "(", "\n", "target_chunk_files", "\n", ")", ",", "f\"Number of chunk files should be the same in source ({len(source_chunk_files)}) and target ({len(target_chunk_files)}) datasets.\"", "\n", "assert", "all", "(", "\n", "[", "\n", "s", ".", "name", "==", "t", ".", "name", "\n", "for", "s", ",", "t", "in", "zip", "(", "source_chunk_files", ",", "target_chunk_files", ")", "\n", "]", "\n", ")", "\n", "\n", "datasets_chunks", ".", "append", "(", "\n", "[", "\n", "{", "\n", "\"source\"", ":", "{", "\"dataset\"", ":", "os", ".", "path", ".", "join", "(", "source", "[", "\"dataset\"", "]", ",", "s", ".", "name", ")", "}", ",", "\n", "\"target\"", ":", "{", "\n", "\"dataset\"", ":", "os", ".", "path", ".", "join", "(", "target", "[", "\"dataset\"", "]", ",", "t", ".", "name", ")", "\n", "if", "target", "[", "\"dataset\"", "]", "\n", "else", "None", "\n", "}", ",", "\n", "\"task\"", ":", "task", ",", "\n", "\"cid\"", ":", "i", ",", "# corpus id for corpus based metric computation during evaluation", "\n", "\"name\"", ":", "dataset_name", ",", "\n", "}", "\n", "for", "s", ",", "t", "in", "zip", "(", "source_chunk_files", ",", "target_chunk_files", ")", "\n", "]", "\n", ")", "\n", "", "else", ":", "\n", "            ", "datasets_chunks", ".", "append", "(", "\n", "[", "\n", "{", "\n", "\"source\"", ":", "{", "\"dataset\"", ":", "os", ".", "path", ".", "join", "(", "source", "[", "\"dataset\"", "]", ",", "s", ".", "name", ")", "}", ",", "\n", "\"task\"", ":", "task", ",", "\n", "\"cid\"", ":", "i", ",", "# corpus id for corpus based metric computation during evaluation", "\n", "\"name\"", ":", "dataset_name", ",", "\n", "}", "\n", "for", "s", "in", "source_chunk_files", "\n", "]", "\n", ")", "\n", "\n", "# create an iterator to iterate the chunk file paths in each dataset", "\n", "", "", "if", "is_train", ":", "\n", "        ", "for", "dataset_chunks", "in", "datasets_chunks", ":", "\n", "            ", "dataset_chunks", ".", "sort", "(", "\n", "key", "=", "lambda", "x", ":", "x", "[", "\"source\"", "]", "[", "\"dataset\"", "]", "\n", ")", "# make sure file order is always the same, independent of OS", "\n", "", "datasets_chunks", ".", "sort", "(", "\n", "key", "=", "lambda", "x", ":", "x", "[", "0", "]", "[", "\"source\"", "]", "[", "\"dataset\"", "]", "\n", ")", "# make sure file order is always the same, independent of OS", "\n", "\n", "for", "i", ",", "dataset_chunks", "in", "enumerate", "(", "datasets_chunks", ")", ":", "\n", "            ", "datasets_chunks", "[", "i", "]", "=", "iterators", ".", "InfinitePermutationSourceIterator", "(", "\n", "dataset_chunks", ",", "\n", "seed", ",", "\n", "shuffle", "=", "True", ",", "\n", "num_instances", "=", "world_size", ",", "\n", "instance_rank", "=", "rank", ",", "\n", ")", "\n", "", "", "else", ":", "\n", "        ", "datasets_chunks", "=", "[", "\n", "[", "chunk", "for", "dataset_chunks", "in", "datasets_chunks", "for", "chunk", "in", "dataset_chunks", "]", "\n", "]", "# flatten the datasets", "\n", "datasets_chunks", "[", "0", "]", ".", "sort", "(", "\n", "key", "=", "lambda", "x", ":", "x", "[", "\"source\"", "]", "[", "\"dataset\"", "]", "\n", ")", "# make sure file order is always the same, independent of OS", "\n", "datasets_chunks", "[", "0", "]", "=", "iterators", ".", "ChunkedSourceIterator", "(", "\n", "datasets_chunks", "[", "0", "]", ",", "num_instances", "=", "world_size", ",", "instance_rank", "=", "rank", "\n", ")", "# in evaluation mode, the files are iterated once without shuffling, but still with parallelization", "\n", "###############################", "\n", "\n", "", "dataset_batch_read_ahead", "=", "max", "(", "1", ",", "batch_read_ahead", "//", "len", "(", "datasets_chunks", ")", ")", "\n", "dataset_doc_shuffle_buffer_size", "=", "max", "(", "\n", "1", ",", "doc_shuffle_buffer_size", "//", "len", "(", "datasets_chunks", ")", "\n", ")", "\n", "dataset_sample_shuffle_buffer_size", "=", "max", "(", "\n", "1", ",", "sample_shuffle_buffer_size", "//", "len", "(", "datasets_chunks", ")", "\n", ")", "\n", "dataset_batch_shuffle_buffer_size", "=", "max", "(", "\n", "1", ",", "batch_shuffle_buffer_size", "//", "len", "(", "datasets_chunks", ")", "\n", ")", "\n", "\n", "###############################", "\n", "# set up document iterator from chunk file iterator", "\n", "###############################", "\n", "# use SelectManyIterator to split each chunk file into multiple documents", "\n", "def", "read_docs_from_chunk", "(", "chunk", ")", ":", "\n", "# this function is provided to the SelectManyIterator constructor as a callback", "\n", "# it takes one item from the source iterator as input (one chunk in this case), and return an iterable (a list of documents), each item in the returned iterable will be yielded by the SelectManyIterator", "\n", "        ", "docs", "=", "[", "]", "\n", "doc", "=", "[", "]", "\n", "cid", "=", "chunk", "[", "\"cid\"", "]", "\n", "task", "=", "chunk", "[", "\"task\"", "]", "\n", "source", "=", "chunk", "[", "\"source\"", "]", "\n", "name", "=", "chunk", "[", "\"name\"", "]", "\n", "with", "gzip", ".", "open", "(", "source", "[", "\"dataset\"", "]", ",", "\"rt\"", ",", "encoding", "=", "\"utf-8\"", ")", "as", "fs", ":", "\n", "            ", "if", "\"target\"", "in", "chunk", ":", "\n", "                ", "target", "=", "chunk", "[", "\"target\"", "]", "\n", "if", "target", "[", "\"dataset\"", "]", ":", "\n", "                    ", "with", "gzip", ".", "open", "(", "target", "[", "\"dataset\"", "]", ",", "\"rt\"", ",", "encoding", "=", "\"utf-8\"", ")", "as", "ft", ":", "\n", "                        ", "for", "line_s", ",", "line_t", "in", "zip", "(", "fs", ",", "ft", ")", ":", "\n", "                            ", "line_s", ",", "line_t", "=", "line_s", ".", "strip", "(", ")", ",", "line_t", ".", "strip", "(", ")", "\n", "if", "line_s", "!=", "\"\"", ":", "\n", "# take care of multiple reference, assume line_t is splitted by \" ||| \"", "\n", "                                ", "if", "is_train", ":", "\n", "# for train, split references to multiple pairs", "\n", "                                    ", "line_t_list", "=", "line_t", ".", "split", "(", "\" ||| \"", ")", "\n", "", "else", ":", "\n", "# for valid and test, not split", "\n", "                                    ", "line_t_list", "=", "[", "line_t", "]", "\n", "\n", "", "for", "sub_line_t", "in", "line_t_list", ":", "\n", "                                    ", "if", "(", "\n", "task", "==", "\"sum\"", "\n", "and", "len", "(", "doc", ")", ">=", "merge_summary_buffer_size", "\n", ")", ":", "\n", "                                        ", "docs", ".", "append", "(", "doc", ")", "\n", "doc", "=", "[", "]", "\n", "", "elif", "(", "not", "task", "==", "\"sum\"", ")", "and", "len", "(", "doc", ")", ">", "0", ":", "\n", "                                        ", "docs", ".", "append", "(", "doc", ")", "\n", "doc", "=", "[", "]", "\n", "", "doc", ".", "append", "(", "\n", "{", "\n", "\"source\"", ":", "{", "\"sequence\"", ":", "line_s", "}", ",", "\n", "\"target\"", ":", "{", "\"sequence\"", ":", "sub_line_t", "}", ",", "\n", "\"task\"", ":", "task", ",", "\n", "\"cid\"", ":", "cid", ",", "\n", "\"name\"", ":", "name", ",", "\n", "}", "\n", ")", "\n", "\n", "", "", "", "", "", "", "else", ":", "\n", "                ", "for", "line", "in", "fs", ":", "\n", "                    ", "line", "=", "line", ".", "strip", "(", ")", "\n", "if", "len", "(", "doc", ")", ">", "0", ":", "\n", "                        ", "docs", ".", "append", "(", "doc", ")", "\n", "doc", "=", "[", "]", "\n", "", "if", "line", "!=", "\"\"", ":", "\n", "                        ", "doc", ".", "append", "(", "\n", "{", "\n", "\"source\"", ":", "{", "\"sequence\"", ":", "line", "}", ",", "\n", "\"task\"", ":", "task", ",", "\n", "\"cid\"", ":", "cid", ",", "\n", "\"name\"", ":", "name", ",", "\n", "}", "\n", ")", "\n", "\n", "", "", "", "", "if", "len", "(", "doc", ")", ">", "0", ":", "\n", "            ", "docs", ".", "append", "(", "doc", ")", "\n", "", "return", "(", "\n", "docs", "# each doc in the docs list will be yielded by the SelectManyIterator", "\n", ")", "\n", "\n", "", "datasets_doc_samples", "=", "[", "]", "\n", "for", "dataset_chunks", "in", "datasets_chunks", ":", "\n", "        ", "datasets_doc_samples", ".", "append", "(", "\n", "iterators", ".", "SelectManyIterator", "(", "dataset_chunks", ",", "read_docs_from_chunk", ")", "\n", ")", "\n", "###############################", "\n", "\n", "###############################", "\n", "# set up the doc randomizer", "\n", "###############################", "\n", "# use BufferedShuffleIterator to shuffle the items from the source iterator", "\n", "# We shuffle before the next steps since at startup, shuffling needs to fill a large buffers. Doing expensive operations afterwards will reduce startup time.", "\n", "# the principle that determines a proper shuffle_buffer_size is: shuffle_buffer_size >> chunk_size", "\n", "", "if", "is_train", ":", "\n", "        ", "for", "i", ",", "doc_samples", "in", "enumerate", "(", "datasets_doc_samples", ")", ":", "\n", "            ", "seed", "=", "_bump_seed", "(", "seed", ")", "\n", "datasets_doc_samples", "[", "i", "]", "=", "iterators", ".", "BufferedShuffleIterator", "(", "\n", "doc_samples", ",", "dataset_doc_shuffle_buffer_size", ",", "seed", "\n", ")", "\n", "###############################", "\n", "\n", "", "", "def", "_parse_tags", "(", "parsed_text", ")", ":", "\n", "        ", "output", "=", "{", "\"word\"", ":", "[", "]", ",", "\"pos_id\"", ":", "[", "]", ",", "\"ent_id\"", ":", "[", "]", "}", "\n", "\n", "for", "token", "in", "parsed_text", ":", "\n", "# [(token.text,token.idx) for token in parsed_sentence]", "\n", "            ", "output", "[", "\"word\"", "]", ".", "append", "(", "_str", "(", "token", ".", "text", ")", ")", "\n", "pos", "=", "token", ".", "tag_", "\n", "output", "[", "\"pos_id\"", "]", ".", "append", "(", "POS", "[", "pos", "]", "if", "pos", "in", "POS", "else", "0", ")", "\n", "\n", "ent", "=", "(", "\n", "\"O\"", "\n", "if", "token", ".", "ent_iob_", "==", "\"O\"", "\n", "else", "(", "token", ".", "ent_iob_", "+", "\"-\"", "+", "token", ".", "ent_type_", ")", "\n", ")", "\n", "output", "[", "\"ent_id\"", "]", ".", "append", "(", "ENT", "[", "ent", "]", "if", "ent", "in", "ENT", "else", "0", ")", "\n", "\n", "", "word_idx", "=", "0", "\n", "for", "sent", "in", "parsed_text", ".", "sents", ":", "\n", "# output['sentences'].append((word_idx, word_idx + len(sent)))", "\n", "            ", "word_idx", "+=", "len", "(", "sent", ")", "\n", "\n", "", "assert", "word_idx", "==", "len", "(", "output", "[", "\"word\"", "]", ")", "\n", "assert", "len", "(", "output", "[", "\"word\"", "]", ")", ">", "0", "\n", "\n", "return", "output", "\n", "\n", "", "def", "_str", "(", "s", ")", ":", "\n", "        ", "\"\"\"Convert PTB tokens to normal tokens\"\"\"", "\n", "if", "s", ".", "lower", "(", ")", "==", "\"-lrb-\"", ":", "\n", "            ", "s", "=", "\"(\"", "\n", "", "elif", "s", ".", "lower", "(", ")", "==", "\"-rrb-\"", ":", "\n", "            ", "s", "=", "\")\"", "\n", "", "elif", "s", ".", "lower", "(", ")", "==", "\"-lsb-\"", ":", "\n", "            ", "s", "=", "\"[\"", "\n", "", "elif", "s", ".", "lower", "(", ")", "==", "\"-rsb-\"", ":", "\n", "            ", "s", "=", "\"]\"", "\n", "", "elif", "s", ".", "lower", "(", ")", "==", "\"-lcb-\"", ":", "\n", "            ", "s", "=", "\"{\"", "\n", "", "elif", "s", ".", "lower", "(", ")", "==", "\"-rcb-\"", ":", "\n", "            ", "s", "=", "\"}\"", "\n", "", "return", "s", "\n", "\n", "###############################", "\n", "# tokenize all sentences in a doc", "\n", "###############################", "\n", "# use SamplingRandomMapIterator because it applies one-to-one mapping (new iterator take one document from source iterator, apply transform, and output it) with checkpointed random state", "\n", "", "def", "tokenize", "(", "rand", ":", "Random", ",", "doc", ")", ":", "\n", "# this function is provided to the SamplingRandomMapIterator constructor as a callback", "\n", "# it takes one item from the source iterator as input, and returns one processed item", "\n", "# use the provided Random object for all random operations in the transform, because that random object is checkpointed.", "\n", "        ", "start", "=", "timer", "(", ")", "\n", "for", "sample", "in", "doc", ":", "\n", "            ", "if", "anon_roles", ":", "\n", "                ", "sample_role_dict", "=", "{", "}", "\n", "\n", "", "source", "=", "sample", "[", "\"source\"", "]", "\n", "if", "sample", "[", "\"task\"", "]", "==", "\"sum\"", ":", "\n", "# make pseduo meetings", "\n", "                ", "turns", "=", "json", ".", "loads", "(", "source", "[", "\"sequence\"", "]", ")", "\n", "source", "[", "\"sequence\"", "]", "=", "[", "]", "\n", "sample", "[", "\"meeting\"", "]", "=", "[", "]", "\n", "for", "turn", "in", "turns", ":", "\n", "                    ", "turn", "[", "\"role\"", "]", "=", "role_dict", ".", "get", "(", "sample", "[", "\"name\"", "]", ",", "0", ")", "\n", "sample", "[", "\"meeting\"", "]", ".", "append", "(", "turn", ")", "\n", "source", "[", "\"sequence\"", "]", ".", "extend", "(", "turn", "[", "\"utt\"", "]", "[", "\"word\"", "]", ")", "\n", "\n", "", "target", "=", "sample", "[", "\"target\"", "]", "\n", "target", "[", "\"sequence\"", "]", "=", "tokenizer", ".", "tokenize", "(", "target", "[", "\"sequence\"", "]", ")", "\n", "\n", "", "elif", "sample", "[", "\"task\"", "]", "==", "\"meeting\"", ":", "\n", "                ", "data", "=", "json", ".", "loads", "(", "source", "[", "\"sequence\"", "]", ")", "\n", "sample", "[", "\"meeting\"", "]", "=", "[", "]", "\n", "source", "[", "\"sequence\"", "]", "=", "[", "]", "\n", "\n", "for", "turn", "in", "data", "[", "\"meeting\"", "]", ":", "\n", "                    ", "if", "anon_roles", ":", "\n", "                        ", "if", "turn", "[", "\"role\"", "]", "not", "in", "sample_role_dict", ":", "\n", "                            ", "sample_role_dict", "[", "turn", "[", "\"role\"", "]", "]", "=", "len", "(", "sample_role_dict", ")", "\n", "", "turn", "[", "\"role\"", "]", "=", "role_dict", ".", "get", "(", "\n", "\"<speaker {}>\"", ".", "format", "(", "sample_role_dict", "[", "turn", "[", "\"role\"", "]", "]", ")", ",", "0", "\n", ")", "\n", "", "else", ":", "\n", "                        ", "turn", "[", "\"role\"", "]", "=", "role_dict", ".", "get", "(", "turn", "[", "\"role\"", "]", ",", "0", ")", "\n", "", "sample", "[", "\"meeting\"", "]", ".", "append", "(", "turn", ")", "\n", "assert", "isinstance", "(", "turn", "[", "\"utt\"", "]", ",", "dict", ")", ",", "turn", "[", "\"utt\"", "]", "\n", "source", "[", "\"sequence\"", "]", ".", "extend", "(", "turn", "[", "\"utt\"", "]", "[", "\"word\"", "]", ")", "\n", "\n", "", "sample", "[", "\"target\"", "]", "=", "{", "}", "\n", "summary_str", "=", "\" \"", ".", "join", "(", "data", "[", "\"summary\"", "]", ")", "\n", "if", "anon_roles", ":", "\n", "                    ", "for", "role", "in", "sample_role_dict", ":", "\n", "                        ", "summary_str", "=", "summary_str", ".", "replace", "(", "\n", "role", ",", "\"<speaker {}>\"", ".", "format", "(", "sample_role_dict", "[", "role", "]", ")", "\n", ")", "\n", "", "", "sample", "[", "\"target\"", "]", "[", "\"sequence\"", "]", "=", "tokenizer", ".", "tokenize", "(", "summary_str", ")", "\n", "\n", "", "else", ":", "\n", "                ", "assert", "False", ",", "f\"Undefined Task {sample['task']}\"", "\n", "\n", "", "", "doc", "=", "[", "\n", "sample", "\n", "for", "sample", "in", "doc", "\n", "if", "len", "(", "sample", "[", "\"source\"", "]", "[", "\"sequence\"", "]", ")", ">", "0", "\n", "and", "(", "\n", "\"target\"", "not", "in", "sample", "\n", "or", "sample", "[", "\"target\"", "]", "[", "\"sequence\"", "]", "is", "None", "\n", "or", "len", "(", "sample", "[", "\"target\"", "]", "[", "\"sequence\"", "]", ")", ">", "0", "\n", ")", "\n", "]", "\n", "end", "=", "timer", "(", ")", "\n", "# print('Tokenize takes {:06.2f} seconds'.format(end-start))", "\n", "return", "doc", "\n", "\n", "", "for", "i", ",", "doc_samples", "in", "enumerate", "(", "datasets_doc_samples", ")", ":", "\n", "        ", "seed", "=", "_bump_seed", "(", "seed", ")", "\n", "datasets_doc_samples", "[", "i", "]", "=", "iterators", ".", "SamplingRandomMapIterator", "(", "\n", "doc_samples", ",", "transform", "=", "tokenize", ",", "seed", "=", "seed", "\n", ")", "\n", "###############################", "\n", "\n", "###############################", "\n", "# shuffle samples from documents again", "\n", "###############################", "\n", "", "if", "is_train", ":", "\n", "        ", "for", "i", ",", "samples", "in", "enumerate", "(", "datasets_doc_samples", ")", ":", "\n", "            ", "seed", "=", "_bump_seed", "(", "seed", ")", "\n", "datasets_doc_samples", "[", "i", "]", "=", "iterators", ".", "BufferedShuffleIterator", "(", "\n", "samples", ",", "dataset_sample_shuffle_buffer_size", ",", "seed", "\n", ")", "\n", "###############################", "\n", "\n", "", "", "def", "concat_samples_in_doc", "(", "doc", ")", ":", "\n", "        ", "if", "len", "(", "doc", ")", "==", "1", ":", "\n", "# return for all meeting dataset and article dataset with one article per sample", "\n", "            ", "return", "doc", "\n", "\n", "", "concat_sample", "=", "{", "}", "\n", "concat_sample", "[", "\"source\"", "]", "=", "{", "\"sequence\"", ":", "[", "]", "}", "\n", "concat_sample", "[", "\"target\"", "]", "=", "{", "\"sequence\"", ":", "[", "]", "}", "\n", "concat_sample", "[", "\"meeting\"", "]", "=", "[", "]", "\n", "\n", "ret_sample_list", "=", "[", "]", "\n", "\n", "count", "=", "0", "\n", "for", "sample", "in", "doc", ":", "\n", "            ", "for", "turn", "in", "sample", "[", "\"meeting\"", "]", ":", "\n", "# take the role add append '-n' for the n-th document", "\n", "                ", "turn", "[", "\"role\"", "]", "=", "role_dict", ".", "get", "(", "\n", "inv_role_dict", "[", "turn", "[", "\"role\"", "]", "]", "+", "\"-{}\"", ".", "format", "(", "count", ")", ",", "0", "\n", ")", "\n", "concat_sample", "[", "\"meeting\"", "]", ".", "append", "(", "turn", ")", "\n", "\n", "", "concat_sample", "[", "\"source\"", "]", "[", "\"sequence\"", "]", ".", "extend", "(", "sample", "[", "\"source\"", "]", "[", "\"sequence\"", "]", ")", "\n", "concat_sample", "[", "\"target\"", "]", "[", "\"sequence\"", "]", ".", "extend", "(", "sample", "[", "\"target\"", "]", "[", "\"sequence\"", "]", ")", "\n", "\n", "count", "+=", "1", "\n", "\n", "if", "count", ">=", "merge_summary_num", ":", "\n", "                ", "if", "merge_summary_shuffle", "and", "count", ">", "1", "and", "is_train", ":", "\n", "                    ", "shuffle", "(", "concat_sample", "[", "\"meeting\"", "]", ")", "\n", "", "ret_sample_list", ".", "append", "(", "concat_sample", ")", "\n", "concat_sample", "=", "{", "}", "\n", "concat_sample", "[", "\"source\"", "]", "=", "{", "\"sequence\"", ":", "[", "]", "}", "\n", "concat_sample", "[", "\"target\"", "]", "=", "{", "\"sequence\"", ":", "[", "]", "}", "\n", "concat_sample", "[", "\"meeting\"", "]", "=", "[", "]", "\n", "count", "=", "0", "\n", "\n", "", "", "return", "ret_sample_list", "\n", "\n", "", "datasets_samples", "=", "[", "]", "\n", "for", "doc_samples", "in", "datasets_doc_samples", ":", "\n", "        ", "datasets_samples", ".", "append", "(", "\n", "iterators", ".", "SelectManyIterator", "(", "doc_samples", ",", "concat_samples_in_doc", ")", "\n", ")", "\n", "\n", "###############################", "\n", "# batching with dynamic batch size depending on the task", "\n", "###############################", "\n", "", "def", "dynamic_batch_size", "(", "sample", ")", ":", "\n", "        ", "if", "is_train", ":", "\n", "            ", "batch_size", "=", "tokens_per_batch", "//", "(", "\n", "len", "(", "sample", "[", "\"source\"", "]", "[", "\"sequence\"", "]", ")", "\n", "+", "len", "(", "sample", "[", "\"target\"", "]", "[", "\"sequence\"", "]", ")", "\n", "+", "1", "\n", ")", "\n", "", "else", ":", "\n", "            ", "batch_size", "=", "tokens_per_batch", "//", "(", "\n", "len", "(", "sample", "[", "\"source\"", "]", "[", "\"sequence\"", "]", ")", "+", "max_gen_length", "+", "1", "\n", ")", "\n", "", "return", "max", "(", "1", ",", "batch_size", ")", "\n", "\n", "", "datasets_batches", "=", "[", "]", "\n", "for", "i", ",", "samples", "in", "enumerate", "(", "datasets_samples", ")", ":", "\n", "        ", "seed", "=", "_bump_seed", "(", "seed", ")", "\n", "datasets_batches", ".", "append", "(", "\n", "iterators", ".", "BucketedReadaheadBatchIterator", "(", "\n", "samples", ",", "\n", "read_ahead", "=", "dataset_batch_read_ahead", ",", "\n", "key", "=", "lambda", "x", ":", "len", "(", "x", "[", "\"source\"", "]", "[", "\"sequence\"", "]", ")", ",", "\n", "batch_size", "=", "dynamic_batch_size", ",", "\n", "shuffle", "=", "is_train", ",", "\n", "seed", "=", "seed", ",", "\n", ")", "\n", ")", "\n", "###############################", "\n", "\n", "###############################", "\n", "# create a zip iterator on all datasets", "\n", "###############################", "\n", "# Use ZipIterator to zip datasets from different datasets. This is to make dataset-dependent tasks distributed evenly", "\n", "", "datasets_batches_zip", "=", "iterators", ".", "ZipIterator", "(", "*", "tuple", "(", "datasets_batches", ")", ")", "\n", "###############################", "\n", "\n", "###############################", "\n", "# unzip batches from all datasets", "\n", "###############################", "\n", "def", "unzip", "(", "datasets_batche", ")", ":", "\n", "        ", "return", "[", "batche", "for", "batche", "in", "datasets_batche", "]", "\n", "\n", "", "batches", "=", "iterators", ".", "SelectManyIterator", "(", "datasets_batches_zip", ",", "unzip", ")", "\n", "###############################", "\n", "\n", "###############################", "\n", "# set up the batch randomizer", "\n", "###############################", "\n", "seed", "=", "_bump_seed", "(", "seed", ")", "\n", "batches", "=", "iterators", ".", "BufferedShuffleIterator", "(", "\n", "batches", ",", "batch_shuffle_buffer_size", ",", "seed", "\n", ")", "\n", "###############################", "\n", "\n", "def", "_pad_batch", "(", "batch", ")", ":", "\n", "# padding and generate final batch", "\n", "        ", "x_sent_batch", "=", "[", "]", "\n", "x_role_batch", "=", "[", "]", "\n", "x_pos_batch", "=", "[", "]", "\n", "x_ent_batch", "=", "[", "]", "\n", "y_sent_batch", "=", "[", "]", "\n", "\n", "encoder_tokens", ",", "decoder_tokens", "=", "[", "]", ",", "[", "]", "\n", "\n", "for", "datum", "in", "batch", ":", "\n", "            ", "x_sent", "=", "[", "]", "\n", "x_role", "=", "[", "]", "\n", "x_pos", "=", "[", "]", "\n", "x_ent", "=", "[", "]", "\n", "\n", "sample_input_tokens", "=", "[", "]", "\n", "\n", "total_word_len", "=", "0", "\n", "total_sent_len", "=", "0", "\n", "\n", "assert", "len", "(", "datum", "[", "\"meeting\"", "]", ")", ">", "0", "\n", "for", "m", "in", "datum", "[", "\"meeting\"", "]", ":", "# each m is actually a turn", "\n", "                ", "words", "=", "m", "[", "\"utt\"", "]", "[", "\"word\"", "]", "\n", "pos", "=", "m", "[", "\"utt\"", "]", "[", "\"pos_id\"", "]", "\n", "ent", "=", "m", "[", "\"utt\"", "]", "[", "\"ent_id\"", "]", "\n", "L", "=", "len", "(", "words", ")", "\n", "# assert L < max_transcript_len, \"a turn {} is longer than max_transcript_len\".format(' '.join(words))", "\n", "if", "L", ">", "max_transcript_len", ":", "\n", "# this is rarely happpened when a turn is super long", "\n", "# in this case we just skip it to save memory", "\n", "                    ", "continue", "\n", "", "if", "(", "\n", "total_word_len", "+", "L", ">", "max_transcript_len", "\n", "or", "total_sent_len", "+", "1", ">", "max_sentence_num", "\n", ")", ":", "\n", "                    ", "break", "\n", "\n", "", "sample_input_tokens", ".", "extend", "(", "words", ")", "\n", "\n", "for", "i", "in", "range", "(", "math", ".", "ceil", "(", "L", "/", "max_sentence_len", ")", ")", ":", "\n", "                    ", "x_role", ".", "append", "(", "m", "[", "\"role\"", "]", ")", "\n", "sub_words", "=", "words", "[", "\n", "i", "*", "max_sentence_len", ":", "min", "(", "(", "i", "+", "1", ")", "*", "max_sentence_len", ",", "L", ")", "\n", "]", "\n", "x_sent", ".", "append", "(", "\n", "[", "tokenizer", ".", "bos_token", "]", "+", "sub_words", "+", "[", "tokenizer", ".", "eos_token", "]", "\n", ")", "\n", "x_pos", ".", "append", "(", "\n", "[", "0", "]", "\n", "+", "pos", "[", "i", "*", "max_sentence_len", ":", "min", "(", "(", "i", "+", "1", ")", "*", "max_sentence_len", ",", "L", ")", "]", "\n", "+", "[", "0", "]", "\n", ")", "\n", "x_ent", ".", "append", "(", "\n", "[", "0", "]", "\n", "+", "ent", "[", "i", "*", "max_sentence_len", ":", "min", "(", "(", "i", "+", "1", ")", "*", "max_sentence_len", ",", "L", ")", "]", "\n", "+", "[", "0", "]", "\n", ")", "\n", "\n", "total_sent_len", "+=", "1", "\n", "\n", "", "total_word_len", "+=", "L", "\n", "\n", "", "if", "is_train", ":", "# training", "\n", "                ", "y_sent", "=", "(", "\n", "[", "tokenizer", ".", "bos_token", "]", "\n", "+", "datum", "[", "\"target\"", "]", "[", "\"sequence\"", "]", "[", ":", "max_gen_length", "]", "\n", "+", "[", "tokenizer", ".", "eos_token", "]", "\n", ")", "\n", "", "else", ":", "\n", "                ", "y_sent", "=", "(", "\n", "[", "tokenizer", ".", "bos_token", "]", "\n", "+", "datum", "[", "\"target\"", "]", "[", "\"sequence\"", "]", "\n", "+", "[", "tokenizer", ".", "eos_token", "]", "\n", ")", "\n", "\n", "", "if", "len", "(", "x_sent", ")", ">", "0", ":", "\n", "# this could be false when there is a single but very long turn", "\n", "                ", "x_sent_batch", ".", "append", "(", "x_sent", ")", "\n", "x_role_batch", ".", "append", "(", "x_role", ")", "\n", "x_pos_batch", ".", "append", "(", "x_pos", ")", "\n", "x_ent_batch", ".", "append", "(", "x_ent", ")", "\n", "y_sent_batch", ".", "append", "(", "y_sent", ")", "\n", "\n", "encoder_tokens", ".", "append", "(", "sample_input_tokens", ")", "\n", "decoder_tokens", ".", "append", "(", "y_sent", ")", "\n", "\n", "", "", "if", "len", "(", "x_sent_batch", ")", "==", "0", ":", "\n", "# this could happen when there is a single but very long turn", "\n", "# leading the whole batch with all instances filtered", "\n", "            ", "return", "None", "\n", "\n", "# count max length", "\n", "", "x_max_doc_len", "=", "max", "(", "[", "len", "(", "s", ")", "for", "s", "in", "x_sent_batch", "]", ")", "\n", "x_max_sent_len", "=", "max", "(", "[", "max", "(", "[", "len", "(", "t", ")", "for", "t", "in", "s", "]", ")", "for", "s", "in", "x_sent_batch", "]", ")", "\n", "y_max_len", "=", "max", "(", "[", "len", "(", "s", ")", "for", "s", "in", "y_sent_batch", "]", ")", "\n", "x_role_max_len", "=", "max", "(", "[", "len", "(", "s", ")", "for", "s", "in", "x_role_batch", "]", ")", "\n", "actual_size", "=", "len", "(", "x_sent_batch", ")", "\n", "\n", "actual_tokens_per_batch", "=", "actual_size", "*", "(", "\n", "x_max_doc_len", "*", "x_max_sent_len", "+", "y_max_len", "\n", ")", "\n", "\n", "# if the actual batch size is too larger than expected because of skewed length", "\n", "if", "(", "actual_tokens_per_batch", "/", "tokens_per_batch", ")", ">", "(", "\n", "max_padding_ratio", "+", "1", "\n", ")", "and", "is_train", ":", "\n", "            ", "return", "None", "\n", "\n", "# create tensors", "\n", "", "x_tensor", "=", "torch", ".", "LongTensor", "(", "actual_size", ",", "x_max_doc_len", ",", "x_max_sent_len", ")", ".", "fill_", "(", "\n", "tokenizer", ".", "pad_token_id", "\n", ")", "\n", "x_pos_tensor", "=", "torch", ".", "LongTensor", "(", "\n", "actual_size", ",", "x_max_doc_len", ",", "x_max_sent_len", "\n", ")", ".", "fill_", "(", "0", ")", "\n", "x_ent_tensor", "=", "torch", ".", "LongTensor", "(", "\n", "actual_size", ",", "x_max_doc_len", ",", "x_max_sent_len", "\n", ")", ".", "fill_", "(", "0", ")", "\n", "x_role_tensor", "=", "torch", ".", "LongTensor", "(", "actual_size", ",", "x_role_max_len", ")", ".", "fill_", "(", "0", ")", "\n", "y_tensor", "=", "torch", ".", "LongTensor", "(", "actual_size", ",", "y_max_len", ")", ".", "fill_", "(", "\n", "tokenizer", ".", "pad_token_id", "\n", ")", "\n", "\n", "for", "i", "in", "range", "(", "len", "(", "x_sent_batch", ")", ")", ":", "\n", "            ", "for", "j", "in", "range", "(", "len", "(", "x_sent_batch", "[", "i", "]", ")", ")", ":", "\n", "                ", "x_tensor", "[", "i", ",", "j", ",", ":", "len", "(", "x_sent_batch", "[", "i", "]", "[", "j", "]", ")", "]", "=", "torch", ".", "LongTensor", "(", "\n", "tokenizer", ".", "convert_tokens_to_ids", "(", "x_sent_batch", "[", "i", "]", "[", "j", "]", ")", "\n", ")", "\n", "y_tensor", "[", "i", ",", ":", "len", "(", "y_sent_batch", "[", "i", "]", ")", "]", "=", "torch", ".", "LongTensor", "(", "\n", "tokenizer", ".", "convert_tokens_to_ids", "(", "y_sent_batch", "[", "i", "]", ")", "\n", ")", "\n", "\n", "", "for", "j", "in", "range", "(", "len", "(", "x_pos_batch", "[", "i", "]", ")", ")", ":", "\n", "                ", "x_pos_tensor", "[", "i", ",", "j", ",", ":", "len", "(", "x_pos_batch", "[", "i", "]", "[", "j", "]", ")", "]", "=", "torch", ".", "LongTensor", "(", "\n", "x_pos_batch", "[", "i", "]", "[", "j", "]", "\n", ")", "\n", "", "for", "j", "in", "range", "(", "len", "(", "x_ent_batch", "[", "i", "]", ")", ")", ":", "\n", "                ", "x_ent_tensor", "[", "i", ",", "j", ",", ":", "len", "(", "x_ent_batch", "[", "i", "]", "[", "j", "]", ")", "]", "=", "torch", ".", "LongTensor", "(", "\n", "x_ent_batch", "[", "i", "]", "[", "j", "]", "\n", ")", "\n", "\n", "", "x_role_tensor", "[", "i", ",", ":", "len", "(", "x_role_batch", "[", "i", "]", ")", "]", "=", "torch", ".", "LongTensor", "(", "x_role_batch", "[", "i", "]", ")", "\n", "\n", "", "return", "{", "\n", "\"encoder_input_ids\"", ":", "x_tensor", ",", "\n", "\"encoder_input_roles\"", ":", "x_role_tensor", ",", "\n", "\"encoder_input_pos\"", ":", "x_pos_tensor", ",", "\n", "\"encoder_input_ent\"", ":", "x_ent_tensor", ",", "\n", "\"decoder_input_ids\"", ":", "y_tensor", ",", "\n", "\"encoder_tokens\"", ":", "encoder_tokens", ",", "\n", "\"decoder_tokens\"", ":", "decoder_tokens", ",", "\n", "}", "\n", "\n", "###############################", "\n", "# collate samples into padded rectangular tensors", "\n", "###############################", "\n", "", "def", "collate", "(", "batch", ")", ":", "\n", "        ", "batch", "=", "_pad_batch", "(", "batch", ")", "\n", "\n", "if", "batch", "is", "None", ":", "\n", "            ", "ret_batches", "=", "[", "]", "\n", "", "else", ":", "\n", "            ", "ret_batches", "=", "[", "batch", "]", "\n", "\n", "", "return", "ret_batches", "\n", "\n", "###############################", "\n", "# collate samples into padded rectangular tensors", "\n", "###############################", "\n", "", "batches", "=", "iterators", ".", "SelectManyIterator", "(", "batches", ",", "collate", ")", "\n", "###############################", "\n", "\n", "return", "batches", "\n", "", ""]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.BaseTrainer.BaseTrainer.__init__": [[8, 22], ["print", "print"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "opt", ")", ":", "\n", "        ", "self", ".", "opt", "=", "opt", "\n", "if", "self", ".", "opt", "[", "\"cuda\"", "]", "==", "True", ":", "\n", "            ", "self", ".", "use_cuda", "=", "True", "\n", "print", "(", "\"Using Cuda\\n\"", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "use_cuda", "=", "False", "\n", "print", "(", "\"Using CPU\\n\"", ")", "\n", "\n", "", "self", ".", "is_official", "=", "\"OFFICIAL\"", "in", "self", ".", "opt", "\n", "self", ".", "opt", "[", "\"logFile\"", "]", "=", "\"log.txt\"", "\n", "self", ".", "saveFolder", "=", "None", "\n", "self", ".", "logFileHandle", "=", "None", "\n", "self", ".", "tb_writer", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.BaseTrainer.BaseTrainer.log": [[23, 36], ["print", "BaseTrainer.BaseTrainer.logFileHandle.write", "open", "print", "os.path.join"], "methods", ["None"], ["", "def", "log", "(", "self", ",", "s", ")", ":", "\n", "# In official case, the program does not output logs", "\n", "        ", "if", "self", ".", "is_official", ":", "\n", "            ", "return", "\n", "", "try", ":", "\n", "            ", "if", "self", ".", "logFileHandle", "is", "None", ":", "\n", "                ", "self", ".", "logFileHandle", "=", "open", "(", "\n", "os", ".", "path", ".", "join", "(", "self", ".", "saveFolder", ",", "self", ".", "opt", "[", "\"logFile\"", "]", ")", ",", "\"a\"", "\n", ")", "\n", "", "self", ".", "logFileHandle", ".", "write", "(", "s", "+", "\"\\n\"", ")", "\n", "", "except", "Exception", "as", "e", ":", "\n", "            ", "print", "(", "\"ERROR while writing log file:\"", ",", "e", ")", "\n", "", "print", "(", "s", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.BaseTrainer.BaseTrainer.getSaveFolder": [[37, 51], ["os.path.join", "os.path.exists", "os.makedirs", "print", "str"], "methods", ["None"], ["", "def", "getSaveFolder", "(", "self", ")", ":", "\n", "        ", "runid", "=", "1", "\n", "while", "True", ":", "\n", "            ", "saveFolder", "=", "os", ".", "path", ".", "join", "(", "\n", "self", ".", "opt", "[", "\"datadir\"", "]", ",", "\n", "self", ".", "opt", "[", "\"basename\"", "]", "+", "\"_conf~\"", ",", "\n", "\"run_\"", "+", "str", "(", "runid", ")", ",", "\n", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "saveFolder", ")", ":", "\n", "                ", "self", ".", "saveFolder", "=", "saveFolder", "\n", "os", ".", "makedirs", "(", "self", ".", "saveFolder", ")", "\n", "print", "(", "\"Saving logs, model and evaluation in \"", "+", "self", ".", "saveFolder", ")", "\n", "return", "\n", "", "runid", "=", "runid", "+", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.BaseTrainer.BaseTrainer.saveConf": [[53, 63], ["open", "os.path.join", "fw.write"], "methods", ["None"], ["", "", "def", "saveConf", "(", "self", ")", ":", "\n", "# with open(self.opt['confFile'], encoding='utf-8') as f:", "\n", "#     with open(os.path.join(self.saveFolder, 'conf_copy.tsv'), 'w', encoding='utf-8') as fw:", "\n", "#         for line in f:", "\n", "#             fw.write(line)", "\n", "        ", "with", "open", "(", "\n", "os", ".", "path", ".", "join", "(", "self", ".", "saveFolder", ",", "\"conf_copy.tsv\"", ")", ",", "\"w\"", ",", "encoding", "=", "\"utf-8\"", "\n", ")", "as", "fw", ":", "\n", "            ", "for", "k", "in", "self", ".", "opt", ":", "\n", "                ", "fw", ".", "write", "(", "\"{0}\\t{1}\\n\"", ".", "format", "(", "k", ",", "self", ".", "opt", "[", "k", "]", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.BaseTrainer.BaseTrainer.train": [[64, 66], ["None"], "methods", ["None"], ["", "", "", "def", "train", "(", "self", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.BaseTrainer.BaseTrainer.load": [[67, 69], ["None"], "methods", ["None"], ["", "def", "load", "(", "self", ")", ":", "\n", "        ", "pass", "\n", "", "", ""]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.DistributedTrainer.DistributedTrainer.__init__": [[17, 99], ["summertime.model.third_party.HMNet.Models.Trainers.BaseTrainer.BaseTrainer.__init__", "random.seed", "numpy.random.seed", "torch.manual_seed", "summertime.model.third_party.HMNet.Utils.distributed.distributed", "DistributedTrainer.DistributedTrainer.getSaveFolder", "DistributedTrainer.DistributedTrainer.saveConf", "int", "pkg_resources.parse_version", "pkg_resources.parse_version", "print", "torch.cuda.manual_seed", "print", "print", "int", "print", "print"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.tests.model_selector_test.ToyModel.__init__", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Utils.distributed.distributed", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.DistributedTrainer.DistributedTrainer.getSaveFolder", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.DistributedTrainer.DistributedTrainer.saveConf"], ["    ", "def", "__init__", "(", "self", ",", "opt", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "opt", ")", "\n", "\n", "self", ".", "seed", "=", "int", "(", "self", ".", "opt", "[", "\"SEED\"", "]", ")", "if", "\"SEED\"", "in", "self", ".", "opt", "else", "0", "\n", "\n", "random", ".", "seed", "(", "self", ".", "seed", ")", "\n", "np", ".", "random", ".", "seed", "(", "self", ".", "seed", ")", "\n", "torch", ".", "manual_seed", "(", "self", ".", "seed", ")", "\n", "\n", "(", "\n", "self", ".", "opt", "[", "\"device\"", "]", ",", "\n", "_", ",", "\n", "self", ".", "opt", "[", "\"world_size\"", "]", ",", "\n", "self", ".", "opt", "[", "\"local_size\"", "]", ",", "\n", "self", ".", "opt", "[", "\"rank\"", "]", ",", "\n", "self", ".", "opt", "[", "\"local_rank\"", "]", ",", "\n", "_", ",", "\n", "self", ".", "opt", "[", "\"run\"", "]", ",", "\n", ")", "=", "distributed", "(", "opt", ",", "not", "self", ".", "use_cuda", ")", "\n", "\n", "self", ".", "getSaveFolder", "(", ")", "\n", "self", ".", "opt", "[", "\"logFile\"", "]", "=", "f\"log_{self.opt['rank']}.txt\"", "\n", "self", ".", "saveConf", "(", ")", "\n", "\n", "self", ".", "high_pytorch_version", "=", "parse_version", "(", "torch", ".", "__version__", ")", ">=", "parse_version", "(", "\n", "\"1.2.0\"", "\n", ")", "\n", "if", "self", ".", "opt", "[", "\"rank\"", "]", "==", "0", ":", "\n", "            ", "print", "(", "\n", "bcolors", ".", "OKGREEN", ",", "\n", "torch", ".", "__version__", ",", "\n", "bcolors", ".", "ENDC", ",", "\n", "\"is\"", ",", "\n", "\"high\"", "if", "self", ".", "high_pytorch_version", "else", "\"low\"", ",", "\n", ")", "\n", "\n", "", "if", "self", ".", "use_cuda", ":", "\n", "# torch.cuda.manual_seed_all(self.seed)", "\n", "# ddp: only set seed on GPU associated with this process", "\n", "            ", "torch", ".", "cuda", ".", "manual_seed", "(", "self", ".", "seed", ")", "\n", "\n", "# ddp: print stats and update learning rate", "\n", "", "if", "self", ".", "opt", "[", "\"rank\"", "]", "==", "0", ":", "\n", "            ", "print", "(", "\n", "\"Number of GPUs is\"", ",", "\n", "bcolors", ".", "OKGREEN", ",", "\n", "self", ".", "opt", "[", "\"world_size\"", "]", ",", "\n", "bcolors", ".", "ENDC", ",", "\n", ")", "\n", "# print('Boost learning rate from', bcolors.OKGREEN, self.opt['START_LEARNING_RATE'], bcolors.ENDC, 'to',", "\n", "#     bcolors.OKGREEN, self.opt['START_LEARNING_RATE'] * self.opt['world_size'], bcolors.ENDC)", "\n", "print", "(", "\n", "\"Effective batch size is increased from\"", ",", "\n", "bcolors", ".", "OKGREEN", ",", "\n", "self", ".", "opt", "[", "\"MINI_BATCH\"", "]", ",", "\n", "bcolors", ".", "ENDC", ",", "\n", "\"to\"", ",", "\n", "bcolors", ".", "OKGREEN", ",", "\n", "self", ".", "opt", "[", "\"MINI_BATCH\"", "]", "*", "self", ".", "opt", "[", "\"world_size\"", "]", ",", "\n", "bcolors", ".", "ENDC", ",", "\n", ")", "\n", "\n", "", "self", ".", "grad_acc_steps", "=", "1", "\n", "if", "\"GRADIENT_ACCUMULATE_STEP\"", "in", "self", ".", "opt", ":", "\n", "            ", "if", "self", ".", "opt", "[", "\"rank\"", "]", "==", "0", ":", "\n", "                ", "print", "(", "\n", "\"Gradient accumulation steps =\"", ",", "\n", "bcolors", ".", "OKGREEN", ",", "\n", "self", ".", "opt", "[", "\"GRADIENT_ACCUMULATE_STEP\"", "]", ",", "\n", "bcolors", ".", "ENDC", ",", "\n", ")", "\n", "# print('Boost learning rate from', bcolors.OKGREEN, self.opt['START_LEARNING_RATE'], bcolors.ENDC, 'to',", "\n", "# bcolors.OKGREEN, self.opt['START_LEARNING_RATE'] * self.opt['world_size'] * self.opt['GRADIENT_ACCUMULATE_STEP'], bcolors.ENDC)", "\n", "print", "(", "\n", "\"Effective batch size =\"", ",", "\n", "bcolors", ".", "OKGREEN", ",", "\n", "self", ".", "opt", "[", "\"MINI_BATCH\"", "]", "\n", "*", "self", ".", "opt", "[", "\"world_size\"", "]", "\n", "*", "self", ".", "opt", "[", "\"GRADIENT_ACCUMULATE_STEP\"", "]", ",", "\n", "bcolors", ".", "ENDC", ",", "\n", ")", "\n", "", "self", ".", "grad_acc_steps", "=", "int", "(", "self", ".", "opt", "[", "\"GRADIENT_ACCUMULATE_STEP\"", "]", ")", "\n", "# self.opt['START_LEARNING_RATE'] *= self.opt['world_size'] * self.grad_acc_steps", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.DistributedTrainer.DistributedTrainer.tb_log_scalar": [[101, 108], ["DistributedTrainer.DistributedTrainer.tb_writer.add_scalar", "torch.utils.tensorboard.SummaryWriter", "os.path.join"], "methods", ["None"], ["", "", "def", "tb_log_scalar", "(", "self", ",", "name", ",", "value", ",", "step", ")", ":", "\n", "        ", "if", "self", ".", "opt", "[", "\"rank\"", "]", "==", "0", ":", "\n", "            ", "if", "self", ".", "tb_writer", "is", "None", ":", "\n", "                ", "self", ".", "tb_writer", "=", "SummaryWriter", "(", "\n", "os", ".", "path", ".", "join", "(", "self", ".", "saveFolder", ",", "\"tensorboard\"", ")", "\n", ")", "\n", "", "self", ".", "tb_writer", ".", "add_scalar", "(", "name", ",", "value", ",", "step", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.DistributedTrainer.DistributedTrainer.log": [[109, 122], ["print", "DistributedTrainer.DistributedTrainer.logFileHandle.write", "open", "print", "os.path.join"], "methods", ["None"], ["", "", "def", "log", "(", "self", ",", "s", ")", ":", "\n", "# When 'OFFICIAL' flag is set in the config file, the program does not output logs", "\n", "        ", "if", "self", ".", "is_official", ":", "\n", "            ", "return", "\n", "", "try", ":", "\n", "            ", "if", "self", ".", "logFileHandle", "is", "None", ":", "\n", "                ", "self", ".", "logFileHandle", "=", "open", "(", "\n", "os", ".", "path", ".", "join", "(", "self", ".", "saveFolder", ",", "self", ".", "opt", "[", "\"logFile\"", "]", ")", ",", "\"a\"", "\n", ")", "\n", "", "self", ".", "logFileHandle", ".", "write", "(", "s", "+", "\"\\n\"", ")", "\n", "", "except", "Exception", "as", "e", ":", "\n", "            ", "print", "(", "\"ERROR while writing log file:\"", ",", "e", ")", "\n", "", "print", "(", "s", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.DistributedTrainer.DistributedTrainer.getSaveFolder": [[123, 145], ["os.path.join", "os.path.isdir", "print", "str", "torch.distributed.barrier", "os.makedirs", "torch.distributed.barrier"], "methods", ["None"], ["", "def", "getSaveFolder", "(", "self", ")", ":", "\n", "        ", "runid", "=", "1", "\n", "while", "True", ":", "\n", "            ", "saveFolder", "=", "os", ".", "path", ".", "join", "(", "\n", "self", ".", "opt", "[", "\"datadir\"", "]", ",", "\n", "self", ".", "opt", "[", "\"basename\"", "]", "+", "\"_conf~\"", ",", "\n", "\"run_\"", "+", "str", "(", "runid", ")", ",", "\n", ")", "\n", "if", "not", "os", ".", "path", ".", "isdir", "(", "saveFolder", ")", ":", "\n", "                ", "if", "self", ".", "opt", "[", "\"world_size\"", "]", ">", "1", ":", "\n", "                    ", "torch", ".", "distributed", ".", "barrier", "(", ")", "\n", "", "if", "self", ".", "opt", "[", "\"rank\"", "]", "==", "0", ":", "\n", "                    ", "os", ".", "makedirs", "(", "saveFolder", ")", "\n", "", "self", ".", "saveFolder", "=", "saveFolder", "\n", "if", "self", ".", "opt", "[", "\"world_size\"", "]", ">", "1", ":", "\n", "                    ", "torch", ".", "distributed", ".", "barrier", "(", ")", "\n", "", "print", "(", "\n", "\"Saving logs, model, checkpoint, and evaluation in \"", "\n", "+", "self", ".", "saveFolder", "\n", ")", "\n", "return", "\n", "", "runid", "=", "runid", "+", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.DistributedTrainer.DistributedTrainer.saveConf": [[146, 149], ["super().saveConf"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.DistributedTrainer.DistributedTrainer.saveConf"], ["", "", "def", "saveConf", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "opt", "[", "\"rank\"", "]", "==", "0", ":", "\n", "            ", "super", "(", ")", ".", "saveConf", "(", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.HMNetTrainer.ObjectView.__init__": [[33, 35], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "d", ")", ":", "\n", "        ", "self", ".", "__dict__", "=", "d", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.HMNetTrainer.WrappedModel.__init__": [[38, 42], ["torch.Module.__init__", "HMNetTrainer.WrappedModel.add_module", "HMNetTrainer.WrappedModel.add_module"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.tests.model_selector_test.ToyModel.__init__"], ["    ", "def", "__init__", "(", "self", ",", "model", ",", "criterion", ")", ":", "\n", "        ", "super", "(", "WrappedModel", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "add_module", "(", "\"model\"", ",", "model", ")", "\n", "self", ".", "add_module", "(", "\"criterion\"", ",", "criterion", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.HMNetTrainer.WrappedModel.forward": [[43, 47], ["HMNetTrainer.WrappedModel.model", "HMNetTrainer.WrappedModel.criterion"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "batch", ")", ":", "\n", "        ", "output", "=", "self", ".", "model", "(", "batch", ")", "\n", "loss", "=", "self", ".", "criterion", "(", "output", ",", "batch", ")", "\n", "return", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.HMNetTrainer.HMNetTrainer.__init__": [[60, 63], ["summertime.model.third_party.HMNet.Models.Trainers.DistributedTrainer.DistributedTrainer.__init__", "summertime.model.third_party.HMNet.Models.Trainers.Tasks.Task.setup_task"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.tests.model_selector_test.ToyModel.__init__", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.Tasks.Task.setup_task"], ["def", "__init__", "(", "self", ",", "opt", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "opt", ")", "\n", "self", ".", "task", "=", "Task", ".", "setup_task", "(", "self", ".", "opt", "[", "\"TASK\"", "]", ",", "self", ".", "opt", ",", "self", ".", "saveFolder", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.HMNetTrainer.HMNetTrainer.is_gradient_accumulation_boundary": [[64, 66], ["None"], "methods", ["None"], ["", "def", "is_gradient_accumulation_boundary", "(", "self", ")", ":", "\n", "        ", "return", "(", "self", ".", "updates", "+", "1", ")", "%", "self", ".", "grad_acc_steps", "==", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.HMNetTrainer.HMNetTrainer.get_batch_generator": [[67, 83], ["HMNetTrainer.HMNetTrainer.task.batch_gen", "isinstance", "HMNetTrainer.HMNetTrainer.log"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.DistributedTrainer.DistributedTrainer.log"], ["", "def", "get_batch_generator", "(", "self", ",", "dataset_label", ")", ":", "\n", "        ", "batch_generator", "=", "self", ".", "task", ".", "batch_gen", "(", "\n", "self", ".", "opt", ",", "\n", "dataset_label", "=", "dataset_label", ",", "\n", "model_config", "=", "self", ".", "module", ".", "config", ",", "\n", "tokenizer", "=", "self", ".", "module", ".", "tokenizer", ",", "\n", "world_size", "=", "self", ".", "opt", "[", "\"world_size\"", "]", ",", "\n", "rank", "=", "self", ".", "opt", "[", "\"rank\"", "]", ",", "\n", "seed", "=", "self", ".", "seed", ",", "\n", ")", "\n", "if", "isinstance", "(", "batch_generator", ",", "BaseBatchGen", ")", ":", "\n", "# If it is a wrapper class of an infinibatch iterator,", "\n", "# get the internal infnitibatch iterator.", "\n", "            ", "batch_generator", "=", "batch_generator", ".", "iterator", "\n", "", "self", ".", "log", "(", "f\"Loaded data on rank {self.opt['rank']}.\"", ")", "\n", "return", "batch_generator", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.HMNetTrainer.HMNetTrainer.set_up_model": [[84, 118], ["sum", "HMNetTrainer.HMNetTrainer.log", "HMNetTrainer.HMNetTrainer.module.to", "importlib.import_module", "getattr", "getattr.", "importlib.import_module", "getattr", "getattr.", "HMNetTrainer.HMNetTrainer.log", "HMNetTrainer.HMNetTrainer.log", "p.numel", "HMNetTrainer.HMNetTrainer.log", "HMNetTrainer.HMNetTrainer.log", "HMNetTrainer.HMNetTrainer.module.parameters"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.DistributedTrainer.DistributedTrainer.log", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.DistributedTrainer.DistributedTrainer.log", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.DistributedTrainer.DistributedTrainer.log", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.DistributedTrainer.DistributedTrainer.log", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.DistributedTrainer.DistributedTrainer.log"], ["", "def", "set_up_model", "(", "self", ")", ":", "\n", "# instantiate module (tokenizer should be contained in module as self.module.tokenizer)", "\n", "        ", "try", ":", "\n", "            ", "model_module", "=", "importlib", ".", "import_module", "(", "\n", "\"summertime.model.third_party.HMNet.Models.Networks.\"", "\n", "+", "self", ".", "opt", "[", "\"MODEL\"", "]", "\n", ")", "\n", "model_class", "=", "getattr", "(", "model_module", ",", "self", ".", "opt", "[", "\"MODEL\"", "]", ")", "\n", "self", ".", "module", "=", "model_class", "(", "self", ".", "opt", ")", "\n", "", "except", "Exception", "as", "e", ":", "\n", "            ", "self", ".", "log", "(", "e", ")", "\n", "self", ".", "log", "(", "\"ERROR: Model {} is unknown\"", ".", "format", "(", "self", ".", "opt", "[", "\"MODEL\"", "]", ")", ")", "\n", "assert", "False", "\n", "\n", "# calculate total trainable parameters", "\n", "", "pytorch_total_params", "=", "sum", "(", "\n", "p", ".", "numel", "(", ")", "for", "p", "in", "self", ".", "module", ".", "parameters", "(", ")", "if", "p", ".", "requires_grad", "\n", ")", "\n", "self", ".", "log", "(", "\"Total trainable parameters: {}\"", ".", "format", "(", "pytorch_total_params", ")", ")", "\n", "\n", "# instantiate criterion", "\n", "try", ":", "\n", "            ", "criterion_module", "=", "importlib", ".", "import_module", "(", "\n", "\"summertime.model.third_party.HMNet.Models.Criteria.\"", "\n", "+", "self", ".", "opt", "[", "\"CRITERION\"", "]", "\n", ")", "\n", "criterion_class", "=", "getattr", "(", "criterion_module", ",", "self", ".", "opt", "[", "\"CRITERION\"", "]", ")", "\n", "self", ".", "criterion", "=", "criterion_class", "(", "self", ".", "opt", ",", "self", ".", "module", ")", "\n", "", "except", "Exception", "as", "e", ":", "\n", "            ", "self", ".", "log", "(", "e", ")", "\n", "self", ".", "log", "(", "\"ERROR: Criterion {} is unknown\"", ".", "format", "(", "self", ".", "opt", "[", "\"CRITERION\"", "]", ")", ")", "\n", "assert", "False", "\n", "\n", "", "self", ".", "module", ".", "to", "(", "self", ".", "opt", "[", "\"device\"", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.HMNetTrainer.HMNetTrainer.get_optimizer_params_config": [[119, 128], ["inspect.signature", "inspect.signature.parameters.keys", "param_name.upper", "param_name.upper"], "methods", ["None"], ["", "def", "get_optimizer_params_config", "(", "self", ",", "optimizer_class", ")", ":", "\n", "        ", "optimizer_parameters", "=", "{", "}", "\n", "sig", "=", "inspect", ".", "signature", "(", "optimizer_class", ")", "\n", "for", "param_name", "in", "sig", ".", "parameters", ".", "keys", "(", ")", ":", "\n", "            ", "if", "param_name", "==", "\"lr\"", ":", "\n", "                ", "optimizer_parameters", "[", "param_name", "]", "=", "self", ".", "opt", "[", "\"START_LEARNING_RATE\"", "]", "\n", "", "if", "param_name", "not", "in", "[", "\"params\"", ",", "\"lr\"", "]", "and", "param_name", ".", "upper", "(", ")", "in", "self", ".", "opt", ":", "\n", "                ", "optimizer_parameters", "[", "param_name", "]", "=", "self", ".", "opt", "[", "param_name", ".", "upper", "(", ")", "]", "\n", "", "", "return", "optimizer_parameters", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.HMNetTrainer.HMNetTrainer.get_lr_scheduler_params_config": [[129, 136], ["inspect.signature", "inspect.signature.parameters.keys", "param_name.upper", "param_name.upper"], "methods", ["None"], ["", "def", "get_lr_scheduler_params_config", "(", "self", ",", "lr_scheduler_class", ")", ":", "\n", "        ", "lr_scheduler_parameters", "=", "{", "}", "\n", "sig", "=", "inspect", ".", "signature", "(", "lr_scheduler_class", ")", "\n", "for", "param_name", "in", "sig", ".", "parameters", ".", "keys", "(", ")", ":", "\n", "            ", "if", "param_name", "not", "in", "[", "\"optimizer\"", "]", "and", "param_name", ".", "upper", "(", ")", "in", "self", ".", "opt", ":", "\n", "                ", "lr_scheduler_parameters", "[", "param_name", "]", "=", "self", ".", "opt", "[", "param_name", ".", "upper", "(", ")", "]", "\n", "", "", "return", "lr_scheduler_parameters", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.HMNetTrainer.HMNetTrainer.set_up_optimizer_and_lr_scheduler": [[137, 196], ["HMNetTrainer.HMNetTrainer.module.get_training_parameters", "HMNetTrainer.HMNetTrainer.get_optimizer_params_config", "HMNetTrainer.HMNetTrainer.log", "getattr.", "HMNetTrainer.HMNetTrainer.optimizer.zero_grad", "HMNetTrainer.HMNetTrainer.get_lr_scheduler_params_config", "HMNetTrainer.HMNetTrainer.log", "getattr.", "getattr", "HMNetTrainer.HMNetTrainer.log", "getattr", "HMNetTrainer.HMNetTrainer.log", "importlib.import_module", "getattr", "HMNetTrainer.HMNetTrainer.log", "importlib.import_module", "getattr", "HMNetTrainer.HMNetTrainer.log", "HMNetTrainer.HMNetTrainer.log", "HMNetTrainer.HMNetTrainer.log", "HMNetTrainer.HMNetTrainer.log", "HMNetTrainer.HMNetTrainer.log"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.MeetingNet_Transformer.MeetingNet_Transformer.get_training_parameters", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.HMNetTrainer.HMNetTrainer.get_optimizer_params_config", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.DistributedTrainer.DistributedTrainer.log", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.HMNetTrainer.HMNetTrainer.get_lr_scheduler_params_config", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.DistributedTrainer.DistributedTrainer.log", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.DistributedTrainer.DistributedTrainer.log", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.DistributedTrainer.DistributedTrainer.log", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.DistributedTrainer.DistributedTrainer.log", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.DistributedTrainer.DistributedTrainer.log", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.DistributedTrainer.DistributedTrainer.log", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.DistributedTrainer.DistributedTrainer.log", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.DistributedTrainer.DistributedTrainer.log", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.DistributedTrainer.DistributedTrainer.log"], ["", "def", "set_up_optimizer_and_lr_scheduler", "(", "self", ")", ":", "\n", "\n", "        ", "parameters", "=", "self", ".", "module", ".", "get_training_parameters", "(", ")", "\n", "\n", "# instantiate optimizer", "\n", "try", ":", "# first try pytorch native optimizer", "\n", "            ", "optimizer_class", "=", "getattr", "(", "optim", ",", "self", ".", "opt", "[", "\"OPTIMIZER\"", "]", ")", "\n", "self", ".", "log", "(", "\n", "\"Using pytorch native optimizier: {}\"", ".", "format", "(", "self", ".", "opt", "[", "\"OPTIMIZER\"", "]", ")", "\n", ")", "\n", "", "except", ":", "\n", "            ", "try", ":", "# then try custom optimizer inside Models.Optimizers", "\n", "                ", "optimizer_module", "=", "importlib", ".", "import_module", "(", "\n", "\"summertime.model.third_party.HMNet.Models.Optimizers.\"", "\n", "+", "self", ".", "opt", "[", "\"OPTIMIZER\"", "]", "\n", ")", "\n", "optimizer_class", "=", "getattr", "(", "optimizer_module", ",", "self", ".", "opt", "[", "\"OPTIMIZER\"", "]", ")", "\n", "self", ".", "log", "(", "\"Using custom optimizer: {}\"", ".", "format", "(", "self", ".", "opt", "[", "\"OPTIMIZER\"", "]", ")", ")", "\n", "", "except", "Exception", "as", "e", ":", "\n", "                ", "self", ".", "log", "(", "e", ")", "\n", "self", ".", "log", "(", "\"ERROR: Optimizer {} is unknown\"", ".", "format", "(", "self", ".", "opt", "[", "\"OPTIMIZER\"", "]", ")", ")", "\n", "assert", "False", "\n", "\n", "", "", "optimizer_parameters", "=", "self", ".", "get_optimizer_params_config", "(", "optimizer_class", ")", "\n", "self", ".", "log", "(", "f\"Optimizer parameters: {optimizer_parameters}\"", ")", "\n", "self", ".", "optimizer", "=", "optimizer_class", "(", "parameters", ",", "**", "optimizer_parameters", ")", "\n", "self", ".", "optimizer", ".", "zero_grad", "(", ")", "\n", "\n", "# instantiate lr scheduler", "\n", "try", ":", "# first look for pytorch native lr scheduler", "\n", "            ", "lr_scheduler_class", "=", "getattr", "(", "lr_scheduler", ",", "self", ".", "opt", "[", "\"LR_SCHEDULER\"", "]", ")", "\n", "self", ".", "log", "(", "\n", "\"Using pytorch native lr scheduler: {}\"", ".", "format", "(", "self", ".", "opt", "[", "\"LR_SCHEDULER\"", "]", ")", "\n", ")", "\n", "", "except", ":", "\n", "            ", "try", ":", "# then look for custom lr scheduler inside Models.Optimizers", "\n", "                ", "lr_scheduler_module", "=", "importlib", ".", "import_module", "(", "\n", "\"summertime.model.third_party.HMNet.Models.Optimizers.\"", "\n", "+", "self", ".", "opt", "[", "\"LR_SCHEDULER\"", "]", "\n", ")", "\n", "lr_scheduler_class", "=", "getattr", "(", "\n", "lr_scheduler_module", ",", "self", ".", "opt", "[", "\"LR_SCHEDULER\"", "]", "\n", ")", "\n", "self", ".", "log", "(", "\n", "\"Using custom lr scheduler: {}\"", ".", "format", "(", "self", ".", "opt", "[", "\"LR_SCHEDULER\"", "]", ")", "\n", ")", "\n", "", "except", "Exception", "as", "e", ":", "\n", "                ", "self", ".", "log", "(", "e", ")", "\n", "self", ".", "log", "(", "\n", "\"ERROR: LR Scheduler {} is unknown\"", ".", "format", "(", "self", ".", "opt", "[", "\"LR_SCHEDULER\"", "]", ")", "\n", ")", "\n", "assert", "False", "\n", "\n", "", "", "lr_scheduler_parameters", "=", "self", ".", "get_lr_scheduler_params_config", "(", "\n", "lr_scheduler_class", "\n", ")", "\n", "self", ".", "log", "(", "f\"Lr scheduler parameters: {lr_scheduler_parameters}\"", ")", "\n", "self", ".", "lr_scheduler", "=", "lr_scheduler_class", "(", "\n", "self", ".", "optimizer", ",", "**", "lr_scheduler_parameters", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.HMNetTrainer.HMNetTrainer.initialize_fp16_DDP": [[198, 225], ["HMNetTrainer.WrappedModel", "HMNetTrainer.HMNetTrainer.network.to", "amp.initialize", "torch.nn.parallel.DistributedDataParallel", "torch.nn.parallel.DistributedDataParallel", "torch.nn.parallel.DistributedDataParallel", "torch.nn.parallel.DistributedDataParallel", "torch.nn.parallel.DistributedDataParallel", "torch.nn.parallel.DistributedDataParallel", "torch.nn.parallel.DistributedDataParallel", "torch.nn.parallel.DistributedDataParallel", "torch.nn.parallel.DistributedDataParallel", "torch.nn.parallel.DistributedDataParallel", "torch.nn.parallel.DistributedDataParallel", "torch.nn.parallel.DistributedDataParallel", "torch.nn.parallel.DistributedDataParallel", "torch.nn.parallel.DistributedDataParallel", "torch.nn.parallel.DistributedDataParallel", "torch.nn.parallel.DistributedDataParallel", "torch.nn.parallel.DistributedDataParallel", "torch.nn.parallel.DistributedDataParallel", "torch.nn.parallel.DistributedDataParallel", "torch.nn.parallel.DistributedDataParallel", "torch.nn.parallel.DistributedDataParallel", "torch.nn.parallel.DistributedDataParallel", "torch.nn.parallel.DistributedDataParallel", "torch.nn.parallel.DistributedDataParallel", "torch.nn.parallel.DistributedDataParallel", "HMNetTrainer.HMNetTrainer.log"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.DistributedTrainer.DistributedTrainer.log"], ["", "def", "initialize_fp16_DDP", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Wrap the module and criterion to a single network, then depending on the settings,\n        wrap the network with apex amp module for fp16 training, and wrap the network with\n        pytorch DDP module for distributed data parallel training\n        \"\"\"", "\n", "self", ".", "network", "=", "WrappedModel", "(", "self", ".", "module", ",", "self", ".", "criterion", ")", "\n", "self", ".", "network", ".", "to", "(", "self", ".", "opt", "[", "\"device\"", "]", ")", "\n", "\n", "if", "self", ".", "opt", "[", "\"fp16\"", "]", ":", "\n", "            ", "from", "apex", "import", "amp", "\n", "\n", "self", ".", "network", ",", "self", ".", "optimizer", "=", "amp", ".", "initialize", "(", "\n", "self", ".", "network", ",", "self", ".", "optimizer", ",", "opt_level", "=", "self", ".", "opt", "[", "\"fp16_opt_level\"", "]", "\n", ")", "\n", "\n", "", "if", "self", ".", "opt", "[", "\"world_size\"", "]", ">", "1", ":", "\n", "            ", "self", ".", "network", "=", "torch", ".", "nn", ".", "parallel", ".", "DistributedDataParallel", "(", "\n", "self", ".", "network", ",", "\n", "device_ids", "=", "[", "self", ".", "opt", "[", "\"local_rank\"", "]", "]", ",", "\n", "output_device", "=", "self", ".", "opt", "[", "\"local_rank\"", "]", ",", "\n", "find_unused_parameters", "=", "True", ",", "\n", ")", "\n", "self", ".", "log", "(", "f\"Wrapped model with DDP on rank {self.opt['rank']}.\"", ")", "\n", "assert", "self", ".", "module", "is", "self", ".", "network", ".", "module", ".", "model", "\n", "", "else", ":", "\n", "            ", "assert", "self", ".", "module", "is", "self", ".", "network", ".", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.HMNetTrainer.HMNetTrainer.eval": [[226, 241], ["HMNetTrainer.HMNetTrainer.set_up_model", "HMNetTrainer.HMNetTrainer.log", "HMNetTrainer.HMNetTrainer.log", "HMNetTrainer.HMNetTrainer.get_batch_generator", "HMNetTrainer.HMNetTrainer.task.evaluator.reset_best_score", "HMNetTrainer.HMNetTrainer.task.evaluator.eval_batches", "HMNetTrainer.HMNetTrainer.log"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.HMNetTrainer.HMNetTrainer.set_up_model", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.DistributedTrainer.DistributedTrainer.log", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.DistributedTrainer.DistributedTrainer.log", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.HMNetTrainer.HMNetTrainer.get_batch_generator", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Evaluation.ROUGEEval.ROUGEEval.reset_best_score", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Evaluation.ROUGEEval.ROUGEEval.eval_batches", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.DistributedTrainer.DistributedTrainer.log"], ["", "", "def", "eval", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "opt", "[", "\"rank\"", "]", "==", "0", ":", "\n", "            ", "self", ".", "log", "(", "\"-----------------------------------------------\"", ")", "\n", "self", ".", "log", "(", "\"Evaluating model ... \"", ")", "\n", "", "self", ".", "set_up_model", "(", ")", "\n", "\n", "for", "eval_dataset", "in", "[", "\"dev\"", ",", "\"test\"", "]", ":", "\n", "            ", "batch_generator_eval", "=", "self", ".", "get_batch_generator", "(", "eval_dataset", ")", "\n", "\n", "self", ".", "task", ".", "evaluator", ".", "reset_best_score", "(", "set_high", "=", "True", ")", "\n", "result", ",", "score", ",", "got_better_score", "=", "self", ".", "task", ".", "evaluator", ".", "eval_batches", "(", "\n", "self", ".", "module", ",", "batch_generator_eval", ",", "self", ".", "saveFolder", ",", "eval_dataset", "\n", ")", "\n", "if", "self", ".", "opt", "[", "\"rank\"", "]", "==", "0", ":", "\n", "                ", "self", ".", "log", "(", "\"{0} results breakdown\\n{1}\"", ".", "format", "(", "eval_dataset", ",", "result", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.HMNetTrainer.HMNetTrainer.eval_return_results": [[242, 258], ["HMNetTrainer.HMNetTrainer.set_up_model", "HMNetTrainer.HMNetTrainer.log", "HMNetTrainer.HMNetTrainer.log", "HMNetTrainer.HMNetTrainer.get_batch_generator", "HMNetTrainer.HMNetTrainer.task.evaluator.reset_best_score", "HMNetTrainer.HMNetTrainer.task.evaluator.eval_batches", "HMNetTrainer.HMNetTrainer.log"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.HMNetTrainer.HMNetTrainer.set_up_model", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.DistributedTrainer.DistributedTrainer.log", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.DistributedTrainer.DistributedTrainer.log", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.HMNetTrainer.HMNetTrainer.get_batch_generator", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Evaluation.ROUGEEval.ROUGEEval.reset_best_score", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Evaluation.ROUGEEval.ROUGEEval.eval_batches", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.DistributedTrainer.DistributedTrainer.log"], ["", "", "", "def", "eval_return_results", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "opt", "[", "\"rank\"", "]", "==", "0", ":", "\n", "            ", "self", ".", "log", "(", "\"-----------------------------------------------\"", ")", "\n", "self", ".", "log", "(", "\"Evaluating model ... \"", ")", "\n", "", "self", ".", "set_up_model", "(", ")", "\n", "\n", "for", "eval_dataset", "in", "[", "\"test\"", "]", ":", "\n", "            ", "batch_generator_eval", "=", "self", ".", "get_batch_generator", "(", "eval_dataset", ")", "\n", "\n", "self", ".", "task", ".", "evaluator", ".", "reset_best_score", "(", "set_high", "=", "True", ")", "\n", "result", ",", "score", ",", "got_better_score", "=", "self", ".", "task", ".", "evaluator", ".", "eval_batches", "(", "\n", "self", ".", "module", ",", "batch_generator_eval", ",", "self", ".", "saveFolder", ",", "eval_dataset", "\n", ")", "\n", "if", "self", ".", "opt", "[", "\"rank\"", "]", "==", "0", ":", "\n", "                ", "self", ".", "log", "(", "\"{0} results breakdown\\n{1}\"", ".", "format", "(", "eval_dataset", ",", "result", ")", ")", "\n", "", "", "return", "result", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.HMNetTrainer.HMNetTrainer.train": [[259, 412], ["HMNetTrainer.HMNetTrainer.log", "HMNetTrainer.HMNetTrainer.set_up_model", "HMNetTrainer.HMNetTrainer.get_batch_generator", "isinstance", "HMNetTrainer.HMNetTrainer.set_up_optimizer_and_lr_scheduler", "HMNetTrainer.HMNetTrainer.initialize_fp16_DDP", "summertime.model.third_party.HMNet.Utils.GeneralUtils.AverageMeter", "range", "HMNetTrainer.HMNetTrainer.log", "HMNetTrainer.HMNetTrainer.log", "len", "HMNetTrainer.HMNetTrainer.load_checkpoint", "HMNetTrainer.HMNetTrainer.log", "datetime.datetime.datetime.now", "enumerate", "HMNetTrainer.HMNetTrainer.log", "HMNetTrainer.HMNetTrainer.log", "HMNetTrainer.HMNetTrainer.log", "HMNetTrainer.HMNetTrainer.update", "isinstance", "HMNetTrainer.HMNetTrainer.is_gradient_accumulation_boundary", "HMNetTrainer.HMNetTrainer.save_checkpoint", "isinstance", "str", "HMNetTrainer.HMNetTrainer.get_batch_generator", "HMNetTrainer.HMNetTrainer.task.evaluator.eval_batches", "HMNetTrainer.HMNetTrainer.tb_log_scalar", "HMNetTrainer.HMNetTrainer.log", "HMNetTrainer.HMNetTrainer.log", "HMNetTrainer.HMNetTrainer.log", "HMNetTrainer.HMNetTrainer.log", "HMNetTrainer.HMNetTrainer.tb_log_scalar", "HMNetTrainer.HMNetTrainer.tb_log_scalar", "HMNetTrainer.HMNetTrainer.tb_log_scalar", "str", "HMNetTrainer.HMNetTrainer.log", "datetime.datetime.datetime.now", "HMNetTrainer.HMNetTrainer.lr_scheduler.get_lr", "HMNetTrainer.HMNetTrainer.lr_scheduler.get_lr", "str().split", "str", "datetime.datetime.datetime.now"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.DistributedTrainer.DistributedTrainer.log", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.HMNetTrainer.HMNetTrainer.set_up_model", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.HMNetTrainer.HMNetTrainer.get_batch_generator", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.HMNetTrainer.HMNetTrainer.set_up_optimizer_and_lr_scheduler", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.HMNetTrainer.HMNetTrainer.initialize_fp16_DDP", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.DistributedTrainer.DistributedTrainer.log", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.DistributedTrainer.DistributedTrainer.log", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.HMNetTrainer.HMNetTrainer.load_checkpoint", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.DistributedTrainer.DistributedTrainer.log", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.DistributedTrainer.DistributedTrainer.log", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.DistributedTrainer.DistributedTrainer.log", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.DistributedTrainer.DistributedTrainer.log", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.HMNetTrainer.HMNetTrainer.update", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.HMNetTrainer.HMNetTrainer.is_gradient_accumulation_boundary", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.HMNetTrainer.HMNetTrainer.save_checkpoint", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.HMNetTrainer.HMNetTrainer.get_batch_generator", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Evaluation.ROUGEEval.ROUGEEval.eval_batches", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.DistributedTrainer.DistributedTrainer.tb_log_scalar", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.DistributedTrainer.DistributedTrainer.log", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.DistributedTrainer.DistributedTrainer.log", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.DistributedTrainer.DistributedTrainer.log", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.DistributedTrainer.DistributedTrainer.log", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.DistributedTrainer.DistributedTrainer.tb_log_scalar", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.DistributedTrainer.DistributedTrainer.tb_log_scalar", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.DistributedTrainer.DistributedTrainer.tb_log_scalar", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.DistributedTrainer.DistributedTrainer.log", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.utils.sentence_splitter.PunktSentenceSplitter.split"], ["", "def", "train", "(", "self", ")", ":", "\n", "        ", "self", ".", "log", "(", "f\"train on rank {self.opt['rank']}\"", ")", "\n", "if", "self", ".", "opt", "[", "\"rank\"", "]", "==", "0", ":", "\n", "            ", "self", ".", "log", "(", "\"-----------------------------------------------\"", ")", "\n", "self", ".", "log", "(", "\"Initializing model...\"", ")", "\n", "\n", "", "self", ".", "set_up_model", "(", ")", "# setup self.module as original model", "\n", "self", ".", "network", "=", "None", "\n", "self", ".", "train_batch_generator", "=", "self", ".", "get_batch_generator", "(", "\"train\"", ")", "\n", "if", "isinstance", "(", "self", ".", "train_batch_generator", ",", "iterators", ".", "CheckpointableIterator", ")", ":", "\n", "# training batch generator is infinite", "\n", "            ", "self", ".", "updates_per_epoch", "=", "self", ".", "opt", "[", "\"UPDATES_PER_EPOCH\"", "]", "\n", "", "else", ":", "\n", "            ", "self", ".", "updates_per_epoch", "=", "len", "(", "self", ".", "train_batch_generator", ")", "\n", "", "self", ".", "updates", "=", "0", "\n", "self", ".", "optim_steps", "=", "0", "\n", "self", ".", "start_epoch_idx", "=", "0", "\n", "self", ".", "start_batch_idx", "=", "0", "\n", "\n", "self", ".", "set_up_optimizer_and_lr_scheduler", "(", ")", "\n", "self", ".", "initialize_fp16_DDP", "(", ")", "\n", "if", "\"RESUME\"", "in", "self", ".", "opt", ":", "\n", "# Resume complete training states, including optimizer, lr_scheduler, train batch generator, and updates count", "\n", "# from the checkpoint location indicated in a .json file", "\n", "            ", "self", ".", "load_checkpoint", "(", ")", "\n", "\n", "######################", "\n", "# Start the main loop", "\n", "######################", "\n", "\n", "", "numEpochs", "=", "self", ".", "opt", "[", "\"MAX_NUM_EPOCHS\"", "]", "\n", "self", ".", "train_loss", "=", "AverageMeter", "(", ")", "# track the average training loss", "\n", "self", ".", "acc_loss", "=", "0.0", "\n", "# after every 'SAVE_PER_UPDATE_NUM' updates, it will save a checkpoint by setting save_a_checkpoint to True temporarily", "\n", "save_a_checkpoint", "=", "False", "\n", "for", "epoch", "in", "range", "(", "self", ".", "start_epoch_idx", ",", "numEpochs", ")", ":", "\n", "            ", "self", ".", "current_epoch_idx", "=", "epoch", "\n", "self", ".", "log", "(", "\"Epoch {}\"", ".", "format", "(", "epoch", ")", ")", "\n", "\n", "startTime", "=", "datetime", ".", "now", "(", ")", "\n", "\n", "for", "batch_idx", ",", "batch", "in", "enumerate", "(", "self", ".", "train_batch_generator", ")", ":", "\n", "                ", "if", "self", ".", "current_epoch_idx", "==", "self", ".", "start_epoch_idx", ":", "\n", "                    ", "if", "isinstance", "(", "\n", "self", ".", "train_batch_generator", ",", "iterators", ".", "CheckpointableIterator", "\n", ")", ":", "\n", "                        ", "batch_idx", "+=", "self", ".", "start_batch_idx", "\n", "", "elif", "batch_idx", "<", "self", ".", "start_batch_idx", ":", "\n", "                        ", "continue", "\n", "", "", "self", ".", "current_batch_idx", "=", "batch_idx", "\n", "\n", "# after every 'SAVE_PER_UPDATE_NUM' updates, save a checkpoint", "\n", "if", "(", "\"SAVE_PER_UPDATE_NUM\"", "in", "self", ".", "opt", ")", "and", "(", "\n", "self", ".", "updates", "+", "1", "\n", ")", "%", "self", ".", "opt", "[", "\"SAVE_PER_UPDATE_NUM\"", "]", "==", "0", ":", "\n", "# Make sure the next update is going to update the weights and zero the gradients, then we can checkpoint", "\n", "                    ", "assert", "self", ".", "is_gradient_accumulation_boundary", "(", ")", "\n", "save_a_checkpoint", "=", "True", "\n", "\n", "# update", "\n", "", "self", ".", "update", "(", "batch", ")", "\n", "\n", "if", "save_a_checkpoint", ":", "\n", "# evaluate at the checkpointed moment, and log the results", "\n", "                    ", "if", "self", ".", "task", ".", "evaluator", "is", "not", "None", ":", "\n", "                        ", "evaluate_label", "=", "\"update_\"", "+", "str", "(", "self", ".", "updates", ")", "\n", "eval_dataset", "=", "\"dev\"", "\n", "batches", "=", "self", ".", "get_batch_generator", "(", "eval_dataset", ")", "\n", "(", "\n", "result", ",", "\n", "score", ",", "\n", "got_better_score", ",", "\n", ")", "=", "self", ".", "task", ".", "evaluator", ".", "eval_batches", "(", "\n", "self", ".", "module", ",", "batches", ",", "self", ".", "saveFolder", ",", "evaluate_label", "\n", ")", "\n", "self", ".", "tb_log_scalar", "(", "\"Eval/score\"", ",", "score", ",", "self", ".", "updates", ")", "\n", "if", "got_better_score", ":", "\n", "                            ", "self", ".", "log", "(", "\n", "\"Got new better score on rank-{0} evaluator, at updates {1}\"", ".", "format", "(", "\n", "self", ".", "opt", "[", "\"rank\"", "]", ",", "self", ".", "updates", "\n", ")", "\n", ")", "\n", "", "self", ".", "log", "(", "\n", "\"Updates {0} - {1}: Current Score: {2:.3f} (best Score: {3:.3f})\"", ".", "format", "(", "\n", "self", ".", "updates", ",", "\n", "eval_dataset", ",", "\n", "score", ",", "\n", "self", ".", "task", ".", "evaluator", ".", "best_score", ",", "\n", ")", "\n", ")", "\n", "self", ".", "log", "(", "\"Current results breakdown\\n{0}\"", ".", "format", "(", "result", ")", ")", "\n", "self", ".", "log", "(", "\n", "\"Best results breakdown\\n{0}\"", ".", "format", "(", "\n", "self", ".", "task", ".", "evaluator", ".", "best_res", "\n", ")", "\n", ")", "\n", "# save complete training states, including model weights, optimizer, lr_scheduler, batch generator, and updates count", "\n", "", "self", ".", "save_checkpoint", "(", "self", ".", "updates", ")", "\n", "save_a_checkpoint", "=", "False", "\n", "\n", "# logging", "\n", "", "if", "(", "\n", "(", "batch_idx", "%", "10", "==", "0", ")", "\n", "or", "(", "epoch", "==", "0", "and", "batch_idx", "<=", "50", ")", "\n", "or", "\"DEBUG\"", "in", "self", ".", "opt", "\n", ")", ":", "\n", "                    ", "if", "self", ".", "opt", "[", "\"rank\"", "]", "==", "0", ":", "\n", "                        ", "batch_size", "=", "batch", "[", "\"encoder_input_ids\"", "]", ".", "shape", "[", "0", "]", "\n", "self", ".", "log", "(", "\n", "\"epochs[{0:6}] updates[{1:6}] bsz[{2:d}] train loss[{3:.5f}] avg train loss[{4:.5f}] learning rate[{5:.5e}] remaining[{6}]\"", ".", "format", "(", "\n", "epoch", ",", "\n", "self", ".", "updates", ",", "\n", "batch_size", ",", "\n", "self", ".", "train_loss", ".", "val", ",", "\n", "self", ".", "train_loss", ".", "avg", ",", "\n", "self", ".", "lr_scheduler", ".", "get_lr", "(", ")", "[", "0", "]", ",", "\n", "str", "(", "\n", "(", "datetime", ".", "now", "(", ")", "-", "startTime", ")", "\n", "/", "(", "batch_idx", "+", "1", ")", "\n", "*", "(", "self", ".", "updates_per_epoch", "-", "batch_idx", "-", "1", ")", "\n", ")", ".", "split", "(", "\".\"", ")", "[", "0", "]", ",", "\n", ")", "\n", ")", "\n", "\n", "self", ".", "tb_log_scalar", "(", "\n", "\"Loss/train_val\"", ",", "self", ".", "train_loss", ".", "val", ",", "self", ".", "updates", "\n", ")", "\n", "self", ".", "tb_log_scalar", "(", "\n", "\"Loss/train_avg\"", ",", "self", ".", "train_loss", ".", "avg", ",", "self", ".", "updates", "\n", ")", "\n", "self", ".", "tb_log_scalar", "(", "\n", "\"Learning Rate/lr\"", ",", "\n", "self", ".", "lr_scheduler", ".", "get_lr", "(", ")", "[", "0", "]", ",", "\n", "self", ".", "updates", ",", "\n", ")", "\n", "\n", "# if \"DEBUG\" in self.opt and batch_idx > 200:  # exist early for DEBUG mode", "\n", "#     break", "\n", "\n", "", "", "if", "(", "\n", "isinstance", "(", "\n", "self", ".", "train_batch_generator", ",", "iterators", ".", "CheckpointableIterator", "\n", ")", "\n", "and", "batch_idx", "+", "1", "==", "self", ".", "updates_per_epoch", "\n", ")", ":", "\n", "                    ", "break", "\n", "\n", "", "", "self", ".", "log", "(", "\"This epoch takes\"", "+", "str", "(", "datetime", ".", "now", "(", ")", "-", "startTime", ")", ")", "\n", "self", ".", "log", "(", "\"PROGRESS: {0:.2f}%\"", ".", "format", "(", "100.0", "*", "(", "epoch", "+", "1", ")", "/", "numEpochs", ")", ")", "\n", "self", ".", "log", "(", "\"Config file is at \"", "+", "self", ".", "opt", "[", "\"confFile\"", "]", ")", "\n", "\n", "if", "\"DEBUG\"", "in", "self", ".", "opt", ":", "# exist early for DEBUG mode", "\n", "                ", "break", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.HMNetTrainer.HMNetTrainer.update": [[413, 505], ["HMNetTrainer.HMNetTrainer.network.train", "isinstance", "HMNetTrainer.HMNetTrainer.is_gradient_accumulation_boundary", "tuple", "isinstance", "HMNetTrainer.HMNetTrainer.network", "HMNetTrainer.HMNetTrainer.update.backward"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.HMNetTrainer.HMNetTrainer.train", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.HMNetTrainer.HMNetTrainer.is_gradient_accumulation_boundary"], ["", "", "", "def", "update", "(", "self", ",", "batch", ")", ":", "\n", "# forward loss, backward propagation, model update, and one step of optimization and lr scheduler", "\n", "        ", "self", ".", "network", ".", "train", "(", ")", "\n", "# put the batch to the device", "\n", "# @TODO make this more general, maybe have a self.task.move_batch(batch, device)", "\n", "# so the trainer decides when and where to move batches, and task tells how", "\n", "if", "isinstance", "(", "batch", ",", "tuple", ")", ":", "\n", "            ", "batch", "=", "tuple", "(", "t", ".", "to", "(", "self", ".", "opt", "[", "\"device\"", "]", ")", "for", "t", "in", "batch", ")", "\n", "", "elif", "isinstance", "(", "batch", ",", "list", ")", ":", "\n", "            ", "batch", "=", "[", "t", ".", "to", "(", "self", ".", "opt", "[", "\"device\"", "]", ")", "for", "t", "in", "batch", "]", "\n", "", "elif", "isinstance", "(", "batch", ",", "dict", ")", ":", "\n", "            ", "for", "k", "in", "batch", ":", "\n", "                ", "if", "torch", ".", "is_tensor", "(", "batch", "[", "k", "]", ")", ":", "\n", "                    ", "batch", "[", "k", "]", "=", "batch", "[", "k", "]", ".", "to", "(", "self", ".", "opt", "[", "\"device\"", "]", ")", "\n", "", "", "", "else", ":", "\n", "            ", "assert", "torch", ".", "is_tensor", "(", "batch", ")", "\n", "batch", "=", "batch", ".", "to", "(", "self", ".", "opt", "[", "\"device\"", "]", ")", "\n", "\n", "# determine whether gradient sync can be skiped or not for this update", "\n", "", "skip_gradient_sync", "=", "False", "\n", "if", "self", ".", "opt", "[", "\"world_size\"", "]", ">", "1", "and", "not", "self", ".", "is_gradient_accumulation_boundary", "(", ")", ":", "\n", "            ", "if", "not", "self", ".", "opt", "[", "\"fp16\"", "]", ":", "\n", "# https://krishansubudhi.github.io/deeplearning/2020/02/06/apex-gradient-accumulation.html", "\n", "# When using fp16, if we skip grad sync during grad accumulation, the grad sync at the", "\n", "# grad accumulation boundary cannot properly sync the whole accumulated grad.", "\n", "# So with fp16 on, we have to sync even if it's not grad accumulation boundary.", "\n", "                ", "if", "self", ".", "high_pytorch_version", ":", "\n", "                    ", "skip_gradient_sync", "=", "True", "\n", "\n", "# forward", "\n", "", "", "", "if", "skip_gradient_sync", ":", "\n", "            ", "with", "self", ".", "network", ".", "no_sync", "(", ")", ":", "\n", "                ", "loss", "=", "self", ".", "network", "(", "batch", ")", "\n", "", "", "else", ":", "\n", "            ", "loss", "=", "self", ".", "network", "(", "batch", ")", "\n", "", "if", "self", ".", "grad_acc_steps", ">", "1", ":", "\n", "            ", "loss", "=", "loss", "/", "self", ".", "grad_acc_steps", "\n", "", "self", ".", "acc_loss", "+=", "loss", "\n", "# self.log(f\"forward() done on rank {self.opt['rank']}\")", "\n", "# print(loss.item())", "\n", "\n", "# backward", "\n", "def", "backward", "(", "loss_tensor", ")", ":", "\n", "            ", "if", "self", ".", "opt", "[", "\"fp16\"", "]", ":", "\n", "                ", "from", "apex", "import", "amp", "\n", "\n", "with", "amp", ".", "scale_loss", "(", "loss_tensor", ",", "self", ".", "optimizer", ")", "as", "scaled_loss", ":", "\n", "                    ", "scaled_loss", ".", "backward", "(", ")", "\n", "", "", "else", ":", "\n", "                ", "loss_tensor", ".", "backward", "(", ")", "\n", "\n", "", "", "if", "skip_gradient_sync", ":", "\n", "            ", "with", "self", ".", "network", ".", "no_sync", "(", ")", ":", "\n", "                ", "backward", "(", "loss", ")", "\n", "", "", "else", ":", "\n", "            ", "if", "\"DEBUG\"", "in", "self", ".", "opt", "and", "self", ".", "opt", "[", "\"rank\"", "]", "==", "0", ":", "\n", "                ", "self", ".", "log", "(", "\n", "\"Performing synchronized backward at step {0}\"", ".", "format", "(", "\n", "self", ".", "optim_steps", "\n", ")", "\n", ")", "\n", "", "backward", "(", "loss", ")", "\n", "# self.log(f\"backward() done on rank {self.opt['rank']}\")", "\n", "\n", "# step", "\n", "", "if", "self", ".", "is_gradient_accumulation_boundary", "(", ")", ":", "\n", "            ", "if", "self", ".", "opt", "[", "\"world_size\"", "]", ">", "1", ":", "\n", "# ddp: use all_reduce to sum up values of self.acc_loss over all processes", "\n", "# the operations happens in place (i.e., the value of self.acc_loss is replaced) and all processes received the updated value", "\n", "                ", "torch", ".", "distributed", ".", "all_reduce", "(", "\n", "self", ".", "acc_loss", ",", "torch", ".", "distributed", ".", "ReduceOp", ".", "SUM", "\n", ")", "\n", "self", ".", "acc_loss", "/=", "self", ".", "opt", "[", "\"world_size\"", "]", "\n", "", "self", ".", "train_loss", ".", "update", "(", "self", ".", "acc_loss", ".", "data", ",", "1", ")", "\n", "self", ".", "acc_loss", "=", "0.0", "\n", "if", "\"GRAD_CLIPPING\"", "in", "self", ".", "opt", ":", "\n", "                ", "if", "self", ".", "opt", "[", "\"fp16\"", "]", ":", "\n", "                    ", "from", "apex", "import", "amp", "\n", "\n", "torch", ".", "nn", ".", "utils", ".", "clip_grad_norm_", "(", "\n", "amp", ".", "master_params", "(", "self", ".", "optimizer", ")", ",", "self", ".", "opt", "[", "\"GRAD_CLIPPING\"", "]", "\n", ")", "\n", "", "else", ":", "\n", "                    ", "torch", ".", "nn", ".", "utils", ".", "clip_grad_norm_", "(", "\n", "self", ".", "network", ".", "parameters", "(", ")", ",", "self", ".", "opt", "[", "\"GRAD_CLIPPING\"", "]", "\n", ")", "\n", "", "", "self", ".", "optim_steps", "+=", "1", "\n", "self", ".", "optimizer", ".", "step", "(", ")", "\n", "self", ".", "optimizer", ".", "zero_grad", "(", ")", "\n", "self", ".", "lr_scheduler", ".", "step", "(", ")", "\n", "\n", "", "self", ".", "updates", "+=", "1", "\n", "# self.log(f\"step() done on rank {self.opt['rank']}\")", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.HMNetTrainer.HMNetTrainer.save_checkpoint": [[507, 593], ["HMNetTrainer.HMNetTrainer.log", "os.path.join", "os.path.isdir", "os.path.join", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "isinstance", "HMNetTrainer.HMNetTrainer.log", "os.path.join", "os.makedirs", "os.path.join", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.distributed.barrier", "torch.distributed.barrier", "torch.distributed.barrier", "torch.distributed.barrier", "torch.distributed.barrier", "torch.distributed.barrier", "torch.distributed.barrier", "torch.distributed.barrier", "torch.distributed.barrier", "torch.distributed.barrier", "torch.distributed.barrier", "torch.distributed.barrier", "torch.distributed.barrier", "torch.distributed.barrier", "torch.distributed.barrier", "torch.distributed.barrier", "torch.distributed.barrier", "torch.distributed.barrier", "torch.distributed.barrier", "torch.distributed.barrier", "torch.distributed.barrier", "torch.distributed.barrier", "torch.distributed.barrier", "torch.distributed.barrier", "torch.distributed.barrier", "str", "random.getstate", "numpy.random.get_state", "torch.get_rng_state", "torch.get_rng_state", "torch.get_rng_state", "torch.get_rng_state", "torch.get_rng_state", "torch.get_rng_state", "torch.get_rng_state", "torch.get_rng_state", "torch.get_rng_state", "torch.get_rng_state", "torch.get_rng_state", "torch.get_rng_state", "torch.get_rng_state", "torch.get_rng_state", "torch.get_rng_state", "torch.get_rng_state", "torch.get_rng_state", "torch.get_rng_state", "torch.get_rng_state", "torch.get_rng_state", "torch.get_rng_state", "torch.get_rng_state", "torch.get_rng_state", "torch.get_rng_state", "torch.get_rng_state", "os.path.join", "HMNetTrainer.HMNetTrainer.train_batch_generator.getstate", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "HMNetTrainer.HMNetTrainer.log", "HMNetTrainer.HMNetTrainer.module.save_pretrained", "json.dump", "str", "HMNetTrainer.HMNetTrainer.network.state_dict", "HMNetTrainer.HMNetTrainer.optimizer.state_dict", "HMNetTrainer.HMNetTrainer.lr_scheduler.state_dict", "torch.cuda.get_rng_state", "torch.cuda.get_rng_state", "torch.cuda.get_rng_state", "torch.cuda.get_rng_state", "torch.cuda.get_rng_state", "torch.cuda.get_rng_state", "torch.cuda.get_rng_state", "torch.cuda.get_rng_state", "torch.cuda.get_rng_state", "torch.cuda.get_rng_state", "torch.cuda.get_rng_state", "torch.cuda.get_rng_state", "torch.cuda.get_rng_state", "torch.cuda.get_rng_state", "torch.cuda.get_rng_state", "torch.cuda.get_rng_state", "torch.cuda.get_rng_state", "torch.cuda.get_rng_state", "torch.cuda.get_rng_state", "torch.cuda.get_rng_state", "torch.cuda.get_rng_state", "torch.cuda.get_rng_state", "torch.cuda.get_rng_state", "torch.cuda.get_rng_state", "torch.cuda.get_rng_state", "str", "os.path.relpath", "open", "amp.state_dict", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.DistributedTrainer.DistributedTrainer.log", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.DistributedTrainer.DistributedTrainer.log", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.iterators.BucketedReadaheadBatchIterator.getstate", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.iterators.BucketedReadaheadBatchIterator.getstate", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.DistributedTrainer.DistributedTrainer.log", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.MeetingNet_Transformer.MeetingNet_Transformer.save_pretrained"], ["", "def", "save_checkpoint", "(", "self", ",", "tag", ")", ":", "\n", "        ", "\"\"\"\n        Save complete training states, including model weights, optimizer, lr_scheduler,\n        fp16 loss scaler, random state, batch generator, and updates count\n        Also save a model with save_pretrained API for model transfer\n        \"\"\"", "\n", "self", ".", "log", "(", "\"Saving checkpoint...\"", ")", "\n", "resume_epoch_idx", "=", "self", ".", "current_epoch_idx", "\n", "resume_batch_idx", "=", "self", ".", "current_batch_idx", "+", "1", "\n", "if", "resume_batch_idx", "==", "self", ".", "updates_per_epoch", ":", "\n", "            ", "resume_batch_idx", "=", "0", "\n", "resume_epoch_idx", "+=", "1", "\n", "\n", "", "if", "self", ".", "opt", "[", "\"fp16\"", "]", ":", "\n", "            ", "from", "apex", "import", "amp", "\n", "", "if", "self", ".", "opt", "[", "\"rank\"", "]", "==", "0", ":", "\n", "            ", "save_dir", "=", "os", ".", "path", ".", "join", "(", "self", ".", "saveFolder", ",", "str", "(", "tag", ")", ")", "\n", "os", ".", "makedirs", "(", "save_dir", ")", "\n", "save_path", "=", "os", ".", "path", ".", "join", "(", "save_dir", ",", "\"training_states.pt\"", ")", "\n", "state", "=", "{", "\n", "\"network\"", ":", "self", ".", "network", ".", "state_dict", "(", ")", ",", "\n", "\"optimizer\"", ":", "self", ".", "optimizer", ".", "state_dict", "(", ")", ",", "\n", "\"lr_scheduler\"", ":", "self", ".", "lr_scheduler", ".", "state_dict", "(", ")", ",", "\n", "\"amp\"", ":", "amp", ".", "state_dict", "(", ")", "if", "self", ".", "opt", "[", "\"fp16\"", "]", "else", "None", ",", "\n", "\"optim_steps\"", ":", "self", ".", "optim_steps", ",", "\n", "\"updates\"", ":", "self", ".", "updates", ",", "\n", "\"updates_per_epoch\"", ":", "self", ".", "updates_per_epoch", ",", "\n", "\"start_epoch_idx\"", ":", "resume_epoch_idx", ",", "\n", "\"start_batch_idx\"", ":", "resume_batch_idx", ",", "\n", "}", "\n", "\n", "torch", ".", "save", "(", "state", ",", "save_path", ")", "\n", "", "if", "self", ".", "opt", "[", "\"world_size\"", "]", ">", "1", ":", "\n", "            ", "torch", ".", "distributed", ".", "barrier", "(", ")", "\n", "", "save_dir", "=", "os", ".", "path", ".", "join", "(", "self", ".", "saveFolder", ",", "str", "(", "tag", ")", ")", "\n", "assert", "os", ".", "path", ".", "isdir", "(", "save_dir", ")", "\n", "\n", "random_state_path", "=", "os", ".", "path", ".", "join", "(", "\n", "save_dir", ",", "\"random_state_rank_{:04d}\"", ".", "format", "(", "self", ".", "opt", "[", "\"rank\"", "]", ")", "\n", ")", "\n", "random_state", "=", "{", "\n", "\"random\"", ":", "random", ".", "getstate", "(", ")", ",", "\n", "\"numpy_random\"", ":", "np", ".", "random", ".", "get_state", "(", ")", ",", "\n", "\"torch_random\"", ":", "torch", ".", "get_rng_state", "(", ")", ",", "\n", "\"torch_cuda_random\"", ":", "torch", ".", "cuda", ".", "get_rng_state", "(", "device", "=", "self", ".", "opt", "[", "\"device\"", "]", ")", "\n", "if", "self", ".", "use_cuda", "\n", "else", "None", ",", "\n", "}", "\n", "torch", ".", "save", "(", "random_state", ",", "random_state_path", ")", "\n", "\n", "if", "isinstance", "(", "self", ".", "train_batch_generator", ",", "iterators", ".", "CheckpointableIterator", ")", ":", "\n", "# save batch generators for all ranks", "\n", "            ", "batch_generator_file_path", "=", "os", ".", "path", ".", "join", "(", "\n", "save_dir", ",", "\n", "\"batch_generator_checkpoint_rank_{:04d}\"", ".", "format", "(", "self", ".", "opt", "[", "\"rank\"", "]", ")", ",", "\n", ")", "\n", "batch_generator_state", "=", "self", ".", "train_batch_generator", ".", "getstate", "(", ")", "\n", "torch", ".", "save", "(", "batch_generator_state", ",", "batch_generator_file_path", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "log", "(", "\n", "\"Batch generator is not checkpointable. Cannot save to checkpoint.\"", "\n", ")", "\n", "\n", "", "if", "self", ".", "opt", "[", "\"rank\"", "]", "==", "0", ":", "\n", "            ", "self", ".", "module", ".", "save_pretrained", "(", "save_dir", ")", "\n", "\n", "", "if", "self", ".", "opt", "[", "\"rank\"", "]", "==", "0", ":", "\n", "# save the latest checkpoint location to json file", "\n", "            ", "checkpoint_location", "=", "{", "\n", "\"checkpoint_tag\"", ":", "str", "(", "tag", ")", ",", "\n", "\"checkpoint_path\"", ":", "os", ".", "path", ".", "relpath", "(", "\n", "self", ".", "saveFolder", ",", "start", "=", "self", ".", "opt", "[", "\"datadir\"", "]", "\n", ")", ",", "\n", "}", "\n", "json", ".", "dump", "(", "\n", "checkpoint_location", ",", "\n", "open", "(", "\n", "os", ".", "path", ".", "join", "(", "\n", "self", ".", "opt", "[", "\"datadir\"", "]", ",", "\n", "self", ".", "opt", "[", "\"basename\"", "]", "+", "\"_resume_checkpoint.json\"", ",", "\n", ")", ",", "\n", "\"w\"", ",", "\n", "encoding", "=", "\"utf-8\"", ",", "\n", ")", ",", "\n", ")", "\n", "", "self", ".", "log", "(", "f\"Finished saving checkpoint and model to {save_dir}.\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.HMNetTrainer.HMNetTrainer.load_model": [[594, 598], ["HMNetTrainer.HMNetTrainer.module.from_pretrained", "HMNetTrainer.HMNetTrainer.module.to"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.MeetingNet_Transformer.MeetingNet_Transformer.from_pretrained"], ["", "def", "load_model", "(", "self", ",", "model_path", ")", ":", "\n", "# Load the model only, without any training states, using the from_pretrained API", "\n", "        ", "self", ".", "module", "=", "self", ".", "module", ".", "from_pretrained", "(", "model_path", ")", "\n", "self", ".", "module", ".", "to", "(", "self", ".", "opt", "[", "\"device\"", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.HMNetTrainer.HMNetTrainer.load_checkpoint": [[599, 693], ["HMNetTrainer.HMNetTrainer.log", "os.path.join", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "HMNetTrainer.HMNetTrainer.network.load_state_dict", "HMNetTrainer.HMNetTrainer.optimizer.load_state_dict", "HMNetTrainer.HMNetTrainer.lr_scheduler.load_state_dict", "os.path.join", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "random.setstate", "numpy.random.set_state", "torch.set_rng_state", "torch.set_rng_state", "torch.set_rng_state", "torch.set_rng_state", "torch.set_rng_state", "torch.set_rng_state", "torch.set_rng_state", "torch.set_rng_state", "torch.set_rng_state", "torch.set_rng_state", "torch.set_rng_state", "torch.set_rng_state", "torch.set_rng_state", "torch.set_rng_state", "torch.set_rng_state", "torch.set_rng_state", "torch.set_rng_state", "torch.set_rng_state", "torch.set_rng_state", "torch.set_rng_state", "torch.set_rng_state", "torch.set_rng_state", "torch.set_rng_state", "torch.set_rng_state", "torch.set_rng_state", "HMNetTrainer.HMNetTrainer.log", "json.load", "os.path.join", "json.dump", "amp.load_state_dict", "torch.cuda.set_rng_state", "torch.cuda.set_rng_state", "torch.cuda.set_rng_state", "torch.cuda.set_rng_state", "torch.cuda.set_rng_state", "torch.cuda.set_rng_state", "torch.cuda.set_rng_state", "torch.cuda.set_rng_state", "torch.cuda.set_rng_state", "torch.cuda.set_rng_state", "torch.cuda.set_rng_state", "torch.cuda.set_rng_state", "torch.cuda.set_rng_state", "torch.cuda.set_rng_state", "torch.cuda.set_rng_state", "torch.cuda.set_rng_state", "torch.cuda.set_rng_state", "torch.cuda.set_rng_state", "torch.cuda.set_rng_state", "torch.cuda.set_rng_state", "torch.cuda.set_rng_state", "torch.cuda.set_rng_state", "torch.cuda.set_rng_state", "torch.cuda.set_rng_state", "torch.cuda.set_rng_state", "isinstance", "os.path.join", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "HMNetTrainer.HMNetTrainer.train_batch_generator.setstate", "HMNetTrainer.HMNetTrainer.log", "open", "os.path.isdir", "open", "os.path.join", "HMNetTrainer.HMNetTrainer.log", "HMNetTrainer.HMNetTrainer.log", "os.path.join", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.DistributedTrainer.DistributedTrainer.log", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.BaseTrainer.BaseTrainer.load", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.BaseTrainer.BaseTrainer.load", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.BaseTrainer.BaseTrainer.load", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.BaseTrainer.BaseTrainer.load", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.BaseTrainer.BaseTrainer.load", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.BaseTrainer.BaseTrainer.load", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.BaseTrainer.BaseTrainer.load", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.BaseTrainer.BaseTrainer.load", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.BaseTrainer.BaseTrainer.load", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.BaseTrainer.BaseTrainer.load", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.BaseTrainer.BaseTrainer.load", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.BaseTrainer.BaseTrainer.load", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.BaseTrainer.BaseTrainer.load", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.BaseTrainer.BaseTrainer.load", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.BaseTrainer.BaseTrainer.load", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.BaseTrainer.BaseTrainer.load", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.BaseTrainer.BaseTrainer.load", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.BaseTrainer.BaseTrainer.load", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.BaseTrainer.BaseTrainer.load", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.BaseTrainer.BaseTrainer.load", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.BaseTrainer.BaseTrainer.load", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.BaseTrainer.BaseTrainer.load", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.BaseTrainer.BaseTrainer.load", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.BaseTrainer.BaseTrainer.load", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.BaseTrainer.BaseTrainer.load", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.BaseTrainer.BaseTrainer.load", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.BaseTrainer.BaseTrainer.load", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.BaseTrainer.BaseTrainer.load", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.BaseTrainer.BaseTrainer.load", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.BaseTrainer.BaseTrainer.load", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.BaseTrainer.BaseTrainer.load", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.BaseTrainer.BaseTrainer.load", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.BaseTrainer.BaseTrainer.load", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.BaseTrainer.BaseTrainer.load", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.BaseTrainer.BaseTrainer.load", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.BaseTrainer.BaseTrainer.load", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.BaseTrainer.BaseTrainer.load", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.BaseTrainer.BaseTrainer.load", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.BaseTrainer.BaseTrainer.load", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.BaseTrainer.BaseTrainer.load", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.BaseTrainer.BaseTrainer.load", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.BaseTrainer.BaseTrainer.load", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.BaseTrainer.BaseTrainer.load", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.BaseTrainer.BaseTrainer.load", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.BaseTrainer.BaseTrainer.load", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.BaseTrainer.BaseTrainer.load", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.BaseTrainer.BaseTrainer.load", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.BaseTrainer.BaseTrainer.load", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.BaseTrainer.BaseTrainer.load", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.BaseTrainer.BaseTrainer.load", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.iterators.BucketedReadaheadBatchIterator.setstate", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.DistributedTrainer.DistributedTrainer.log", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.BaseTrainer.BaseTrainer.load", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.BaseTrainer.BaseTrainer.load", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.BaseTrainer.BaseTrainer.load", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.BaseTrainer.BaseTrainer.load", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.BaseTrainer.BaseTrainer.load", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.BaseTrainer.BaseTrainer.load", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.BaseTrainer.BaseTrainer.load", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.BaseTrainer.BaseTrainer.load", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.BaseTrainer.BaseTrainer.load", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.BaseTrainer.BaseTrainer.load", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.BaseTrainer.BaseTrainer.load", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.BaseTrainer.BaseTrainer.load", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.BaseTrainer.BaseTrainer.load", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.BaseTrainer.BaseTrainer.load", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.BaseTrainer.BaseTrainer.load", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.BaseTrainer.BaseTrainer.load", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.BaseTrainer.BaseTrainer.load", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.BaseTrainer.BaseTrainer.load", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.BaseTrainer.BaseTrainer.load", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.BaseTrainer.BaseTrainer.load", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.BaseTrainer.BaseTrainer.load", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.BaseTrainer.BaseTrainer.load", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.BaseTrainer.BaseTrainer.load", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.BaseTrainer.BaseTrainer.load", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.BaseTrainer.BaseTrainer.load", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.BaseTrainer.BaseTrainer.load", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.iterators.BucketedReadaheadBatchIterator.setstate", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.DistributedTrainer.DistributedTrainer.log", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.DistributedTrainer.DistributedTrainer.log", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.DistributedTrainer.DistributedTrainer.log"], ["", "def", "load_checkpoint", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Load complete training states, including model weights, optimizer, lr_scheduler,\n        fp16 loss scaler, random state, batch generator, and updates count\n        \"\"\"", "\n", "try", ":", "\n", "# load the checkpoint location from json file", "\n", "            ", "checkpoint_location", "=", "json", ".", "load", "(", "\n", "open", "(", "\n", "os", ".", "path", ".", "join", "(", "\n", "self", ".", "opt", "[", "\"datadir\"", "]", ",", "\n", "self", ".", "opt", "[", "\"basename\"", "]", "+", "\"_resume_checkpoint.json\"", ",", "\n", ")", ",", "\n", "encoding", "=", "\"utf-8\"", ",", "\n", ")", "\n", ")", "\n", "checkpoint_path", "=", "os", ".", "path", ".", "join", "(", "\n", "self", ".", "opt", "[", "\"datadir\"", "]", ",", "\n", "checkpoint_location", "[", "\"checkpoint_path\"", "]", ",", "\n", "checkpoint_location", "[", "\"checkpoint_tag\"", "]", ",", "\n", ")", "\n", "tag", "=", "checkpoint_location", "[", "\"checkpoint_tag\"", "]", "\n", "if", "not", "os", ".", "path", ".", "isdir", "(", "checkpoint_path", ")", ":", "\n", "                ", "if", "self", ".", "opt", "[", "\"rank\"", "]", "==", "0", ":", "\n", "                    ", "self", ".", "log", "(", "\n", "\"Checkpoint path {} not exist. Continue without loading checkpoint\"", ".", "format", "(", "\n", "checkpoint_path", "\n", ")", "\n", ")", "\n", "", "return", "\n", "", "", "except", ":", "\n", "            ", "if", "self", ".", "opt", "[", "\"rank\"", "]", "==", "0", ":", "\n", "                ", "self", ".", "log", "(", "\n", "f\"Cannot find checkpoint path from {self.opt['basename']+'_resume_checkpoint.json'}.\\n\"", "\n", "f\"Make sure {os.path.join(self.opt['datadir'], self.opt['basename']+'_resume_checkpoint.json')} exists.\\n\"", "\n", "f\"Continue without loading checkpoint\"", "\n", ")", "\n", "", "return", "\n", "# save a copy of the resumed checkpoint location in the save folder of current run", "\n", "", "if", "self", ".", "opt", "[", "\"rank\"", "]", "==", "0", ":", "\n", "            ", "json", ".", "dump", "(", "\n", "checkpoint_location", ",", "\n", "open", "(", "\n", "os", ".", "path", ".", "join", "(", "self", ".", "saveFolder", ",", "\"resumed_checkpoint.json\"", ")", ",", "\n", "\"w\"", ",", "\n", "encoding", "=", "\"utf-8\"", ",", "\n", ")", ",", "\n", ")", "\n", "\n", "", "self", ".", "log", "(", "f\"Loading checkpoint from {checkpoint_path}...\"", ")", "\n", "load_path", "=", "os", ".", "path", ".", "join", "(", "checkpoint_path", ",", "\"training_states.pt\"", ")", "\n", "state", "=", "torch", ".", "load", "(", "load_path", ",", "map_location", "=", "self", ".", "opt", "[", "\"device\"", "]", ")", "\n", "self", ".", "network", ".", "load_state_dict", "(", "state", "[", "\"network\"", "]", ")", "\n", "self", ".", "optimizer", ".", "load_state_dict", "(", "state", "[", "\"optimizer\"", "]", ")", "\n", "self", ".", "lr_scheduler", ".", "load_state_dict", "(", "state", "[", "\"lr_scheduler\"", "]", ")", "\n", "if", "self", ".", "opt", "[", "\"fp16\"", "]", ":", "\n", "            ", "from", "apex", "import", "amp", "\n", "\n", "amp", ".", "load_state_dict", "(", "state", "[", "\"amp\"", "]", ")", "\n", "", "self", ".", "optim_steps", "=", "state", "[", "\"optim_steps\"", "]", "\n", "self", ".", "updates", "=", "state", "[", "\"updates\"", "]", "\n", "self", ".", "start_epoch_idx", "=", "state", "[", "\"start_epoch_idx\"", "]", "\n", "self", ".", "start_batch_idx", "=", "state", "[", "\"start_batch_idx\"", "]", "\n", "assert", "self", ".", "updates_per_epoch", "==", "state", "[", "\"updates_per_epoch\"", "]", "\n", "assert", "self", ".", "start_batch_idx", "<", "self", ".", "updates_per_epoch", "\n", "\n", "random_state_path", "=", "os", ".", "path", ".", "join", "(", "\n", "checkpoint_path", ",", "\"random_state_rank_{:04d}\"", ".", "format", "(", "self", ".", "opt", "[", "\"rank\"", "]", ")", "\n", ")", "\n", "random_state", "=", "torch", ".", "load", "(", "random_state_path", ",", "map_location", "=", "\"cpu\"", ")", "\n", "random", ".", "setstate", "(", "random_state", "[", "\"random\"", "]", ")", "\n", "np", ".", "random", ".", "set_state", "(", "random_state", "[", "\"numpy_random\"", "]", ")", "\n", "torch", ".", "set_rng_state", "(", "random_state", "[", "\"torch_random\"", "]", ")", "\n", "if", "self", ".", "use_cuda", ":", "\n", "            ", "torch", ".", "cuda", ".", "set_rng_state", "(", "\n", "random_state", "[", "\"torch_cuda_random\"", "]", ",", "device", "=", "self", ".", "opt", "[", "\"device\"", "]", "\n", ")", "\n", "\n", "", "if", "\"RESET_DATA_LOADER\"", "not", "in", "self", ".", "opt", "and", "isinstance", "(", "\n", "self", ".", "train_batch_generator", ",", "iterators", ".", "CheckpointableIterator", "\n", ")", ":", "\n", "            ", "batch_generator_file_path", "=", "os", ".", "path", ".", "join", "(", "\n", "checkpoint_path", ",", "\n", "\"batch_generator_checkpoint_rank_{:04d}\"", ".", "format", "(", "self", ".", "opt", "[", "\"rank\"", "]", ")", ",", "\n", ")", "\n", "batch_generator_state", "=", "torch", ".", "load", "(", "\n", "batch_generator_file_path", ",", "map_location", "=", "\"cpu\"", "\n", ")", "\n", "self", ".", "train_batch_generator", ".", "setstate", "(", "batch_generator_state", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "log", "(", "\n", "\"No need to resume batch generator or batch generator is not checkpointable. Didn't load from checkpoint.\"", "\n", ")", "\n", "", "self", ".", "log", "(", "f\"Finished loading checkpoint from {checkpoint_path}.\"", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.Tasks.Task.__init__": [[12, 15], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "batch_gen", ",", "evaluator", ")", ":", "\n", "        ", "self", ".", "batch_gen", "=", "batch_gen", "\n", "self", ".", "evaluator", "=", "evaluator", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.Tasks.Task.setup_task": [[16, 35], ["cls", "ROUGEEval", "print"], "methods", ["None"], ["", "@", "classmethod", "\n", "def", "setup_task", "(", "cls", ",", "task_name", ",", "opt", ",", "save_dir", ")", ":", "\n", "\n", "        ", "if", "task_name", "==", "\"HMNet\"", ":", "\n", "            ", "from", "summertime", ".", "model", ".", "third_party", ".", "HMNet", ".", "Utils", ".", "HMNet", ".", "InfinibatchLoader", "import", "(", "\n", "HMNetBatchGen", ",", "\n", ")", "\n", "\n", "batch_gen", "=", "HMNetBatchGen", "\n", "from", "summertime", ".", "model", ".", "third_party", ".", "HMNet", ".", "Evaluation", ".", "ROUGEEval", "import", "(", "\n", "ROUGEEval", ",", "\n", ")", "\n", "\n", "evaluator", "=", "ROUGEEval", "(", "opt", "[", "\"datadir\"", "]", ",", "save_dir", ",", "opt", ")", "\n", "", "else", ":", "\n", "            ", "assert", "False", "\n", "print", "(", "\"ERROR: Task {} not defined\"", ".", "format", "(", "task_name", ")", ")", "\n", "\n", "", "return", "cls", "(", "batch_gen", ",", "evaluator", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Optimizers.RAdam.RAdam.__init__": [[19, 23], ["dict", "torch.optim.optimizer.Optimizer.__init__", "range"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.tests.model_selector_test.ToyModel.__init__"], ["def", "__init__", "(", "self", ",", "params", ",", "lr", "=", "1e-3", ",", "betas", "=", "(", "0.9", ",", "0.999", ")", ",", "eps", "=", "1e-8", ",", "weight_decay", "=", "0", ")", ":", "\n", "        ", "defaults", "=", "dict", "(", "lr", "=", "lr", ",", "betas", "=", "betas", ",", "eps", "=", "eps", ",", "weight_decay", "=", "weight_decay", ")", "\n", "self", ".", "buffer", "=", "[", "[", "None", ",", "None", ",", "None", "]", "for", "ind", "in", "range", "(", "10", ")", "]", "\n", "super", "(", "RAdam", ",", "self", ")", ".", "__init__", "(", "params", ",", "defaults", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Optimizers.RAdam.RAdam.__setstate__": [[24, 26], ["super().__setstate__"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.iterators.CheckpointableIterator.__setstate__"], ["", "def", "__setstate__", "(", "self", ",", "state", ")", ":", "\n", "        ", "super", "(", "RAdam", ",", "self", ")", ".", "__setstate__", "(", "state", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Optimizers.RAdam.RAdam.step": [[27, 103], ["closure", "p.grad.data.float", "p.data.float", "exp_avg_sq.mul_().addcmul_", "exp_avg.mul_().add_", "p.data.copy_", "RuntimeError", "len", "torch.zeros_like", "torch.zeros_like", "state[].type_as", "state[].type_as", "p.data.float.add_", "exp_avg_sq.sqrt().add_", "p.data.float.addcdiv_", "p.data.float.add_", "exp_avg_sq.mul_", "exp_avg.mul_", "int", "exp_avg_sq.sqrt", "math.sqrt"], "methods", ["None"], ["", "def", "step", "(", "self", ",", "closure", "=", "None", ")", ":", "\n", "\n", "        ", "loss", "=", "None", "\n", "if", "closure", "is", "not", "None", ":", "\n", "            ", "loss", "=", "closure", "(", ")", "\n", "\n", "", "for", "group", "in", "self", ".", "param_groups", ":", "\n", "\n", "            ", "for", "p", "in", "group", "[", "\"params\"", "]", ":", "\n", "                ", "if", "p", ".", "grad", "is", "None", ":", "\n", "                    ", "continue", "\n", "", "grad", "=", "p", ".", "grad", ".", "data", ".", "float", "(", ")", "\n", "if", "grad", ".", "is_sparse", ":", "\n", "                    ", "raise", "RuntimeError", "(", "\"RAdam does not support sparse gradients\"", ")", "\n", "\n", "", "p_data_fp32", "=", "p", ".", "data", ".", "float", "(", ")", "\n", "\n", "state", "=", "self", ".", "state", "[", "p", "]", "\n", "\n", "if", "len", "(", "state", ")", "==", "0", ":", "\n", "                    ", "state", "[", "\"step\"", "]", "=", "0", "\n", "state", "[", "\"exp_avg\"", "]", "=", "torch", ".", "zeros_like", "(", "p_data_fp32", ")", "\n", "state", "[", "\"exp_avg_sq\"", "]", "=", "torch", ".", "zeros_like", "(", "p_data_fp32", ")", "\n", "", "else", ":", "\n", "                    ", "state", "[", "\"exp_avg\"", "]", "=", "state", "[", "\"exp_avg\"", "]", ".", "type_as", "(", "p_data_fp32", ")", "\n", "state", "[", "\"exp_avg_sq\"", "]", "=", "state", "[", "\"exp_avg_sq\"", "]", ".", "type_as", "(", "p_data_fp32", ")", "\n", "\n", "", "exp_avg", ",", "exp_avg_sq", "=", "state", "[", "\"exp_avg\"", "]", ",", "state", "[", "\"exp_avg_sq\"", "]", "\n", "beta1", ",", "beta2", "=", "group", "[", "\"betas\"", "]", "\n", "\n", "exp_avg_sq", ".", "mul_", "(", "beta2", ")", ".", "addcmul_", "(", "1", "-", "beta2", ",", "grad", ",", "grad", ")", "\n", "exp_avg", ".", "mul_", "(", "beta1", ")", ".", "add_", "(", "1", "-", "beta1", ",", "grad", ")", "\n", "\n", "state", "[", "\"step\"", "]", "+=", "1", "\n", "buffered", "=", "self", ".", "buffer", "[", "int", "(", "state", "[", "\"step\"", "]", "%", "10", ")", "]", "\n", "if", "state", "[", "\"step\"", "]", "==", "buffered", "[", "0", "]", ":", "\n", "                    ", "N_sma", ",", "step_size", "=", "buffered", "[", "1", "]", ",", "buffered", "[", "2", "]", "\n", "", "else", ":", "\n", "                    ", "buffered", "[", "0", "]", "=", "state", "[", "\"step\"", "]", "\n", "beta2_t", "=", "beta2", "**", "state", "[", "\"step\"", "]", "\n", "N_sma_max", "=", "2", "/", "(", "1", "-", "beta2", ")", "-", "1", "\n", "N_sma", "=", "N_sma_max", "-", "2", "*", "state", "[", "\"step\"", "]", "*", "beta2_t", "/", "(", "1", "-", "beta2_t", ")", "\n", "buffered", "[", "1", "]", "=", "N_sma", "\n", "\n", "# more conservative since it's an approximated value", "\n", "if", "N_sma", ">=", "5", ":", "\n", "                        ", "step_size", "=", "(", "\n", "group", "[", "\"lr\"", "]", "\n", "*", "math", ".", "sqrt", "(", "\n", "(", "1", "-", "beta2_t", ")", "\n", "*", "(", "N_sma", "-", "4", ")", "\n", "/", "(", "N_sma_max", "-", "4", ")", "\n", "*", "(", "N_sma", "-", "2", ")", "\n", "/", "N_sma", "\n", "*", "N_sma_max", "\n", "/", "(", "N_sma_max", "-", "2", ")", "\n", ")", "\n", "/", "(", "1", "-", "beta1", "**", "state", "[", "\"step\"", "]", ")", "\n", ")", "\n", "", "else", ":", "\n", "                        ", "step_size", "=", "group", "[", "\"lr\"", "]", "/", "(", "1", "-", "beta1", "**", "state", "[", "\"step\"", "]", ")", "\n", "", "buffered", "[", "2", "]", "=", "step_size", "\n", "\n", "", "if", "group", "[", "\"weight_decay\"", "]", "!=", "0", ":", "\n", "                    ", "p_data_fp32", ".", "add_", "(", "-", "group", "[", "\"weight_decay\"", "]", "*", "group", "[", "\"lr\"", "]", ",", "p_data_fp32", ")", "\n", "\n", "# more conservative since it's an approximated value", "\n", "", "if", "N_sma", ">=", "5", ":", "\n", "                    ", "denom", "=", "exp_avg_sq", ".", "sqrt", "(", ")", ".", "add_", "(", "group", "[", "\"eps\"", "]", ")", "\n", "p_data_fp32", ".", "addcdiv_", "(", "-", "step_size", ",", "exp_avg", ",", "denom", ")", "\n", "", "else", ":", "\n", "                    ", "p_data_fp32", ".", "add_", "(", "-", "step_size", ",", "exp_avg", ")", "\n", "\n", "", "p", ".", "data", ".", "copy_", "(", "p_data_fp32", ")", "\n", "\n", "", "", "return", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Optimizers.RAdam.PlainRAdam.__init__": [[106, 110], ["dict", "torch.optim.optimizer.Optimizer.__init__"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.tests.model_selector_test.ToyModel.__init__"], ["    ", "def", "__init__", "(", "self", ",", "params", ",", "lr", "=", "1e-3", ",", "betas", "=", "(", "0.9", ",", "0.999", ")", ",", "eps", "=", "1e-8", ",", "weight_decay", "=", "0", ")", ":", "\n", "        ", "defaults", "=", "dict", "(", "lr", "=", "lr", ",", "betas", "=", "betas", ",", "eps", "=", "eps", ",", "weight_decay", "=", "weight_decay", ")", "\n", "\n", "super", "(", "PlainRAdam", ",", "self", ")", ".", "__init__", "(", "params", ",", "defaults", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Optimizers.RAdam.PlainRAdam.__setstate__": [[111, 113], ["super().__setstate__"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.iterators.CheckpointableIterator.__setstate__"], ["", "def", "__setstate__", "(", "self", ",", "state", ")", ":", "\n", "        ", "super", "(", "PlainRAdam", ",", "self", ")", ".", "__setstate__", "(", "state", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Optimizers.RAdam.PlainRAdam.step": [[114, 179], ["closure", "p.grad.data.float", "p.data.float", "exp_avg_sq.mul_().addcmul_", "exp_avg.mul_().add_", "p.data.copy_", "RuntimeError", "len", "torch.zeros_like", "torch.zeros_like", "state[].type_as", "state[].type_as", "p.data.float.add_", "exp_avg_sq.sqrt().add_", "p.data.float.addcdiv_", "p.data.float.add_", "exp_avg_sq.mul_", "exp_avg.mul_", "math.sqrt", "exp_avg_sq.sqrt"], "methods", ["None"], ["", "def", "step", "(", "self", ",", "closure", "=", "None", ")", ":", "\n", "\n", "        ", "loss", "=", "None", "\n", "if", "closure", "is", "not", "None", ":", "\n", "            ", "loss", "=", "closure", "(", ")", "\n", "\n", "", "for", "group", "in", "self", ".", "param_groups", ":", "\n", "\n", "            ", "for", "p", "in", "group", "[", "\"params\"", "]", ":", "\n", "                ", "if", "p", ".", "grad", "is", "None", ":", "\n", "                    ", "continue", "\n", "", "grad", "=", "p", ".", "grad", ".", "data", ".", "float", "(", ")", "\n", "if", "grad", ".", "is_sparse", ":", "\n", "                    ", "raise", "RuntimeError", "(", "\"RAdam does not support sparse gradients\"", ")", "\n", "\n", "", "p_data_fp32", "=", "p", ".", "data", ".", "float", "(", ")", "\n", "\n", "state", "=", "self", ".", "state", "[", "p", "]", "\n", "\n", "if", "len", "(", "state", ")", "==", "0", ":", "\n", "                    ", "state", "[", "\"step\"", "]", "=", "0", "\n", "state", "[", "\"exp_avg\"", "]", "=", "torch", ".", "zeros_like", "(", "p_data_fp32", ")", "\n", "state", "[", "\"exp_avg_sq\"", "]", "=", "torch", ".", "zeros_like", "(", "p_data_fp32", ")", "\n", "", "else", ":", "\n", "                    ", "state", "[", "\"exp_avg\"", "]", "=", "state", "[", "\"exp_avg\"", "]", ".", "type_as", "(", "p_data_fp32", ")", "\n", "state", "[", "\"exp_avg_sq\"", "]", "=", "state", "[", "\"exp_avg_sq\"", "]", ".", "type_as", "(", "p_data_fp32", ")", "\n", "\n", "", "exp_avg", ",", "exp_avg_sq", "=", "state", "[", "\"exp_avg\"", "]", ",", "state", "[", "\"exp_avg_sq\"", "]", "\n", "beta1", ",", "beta2", "=", "group", "[", "\"betas\"", "]", "\n", "\n", "exp_avg_sq", ".", "mul_", "(", "beta2", ")", ".", "addcmul_", "(", "1", "-", "beta2", ",", "grad", ",", "grad", ")", "\n", "exp_avg", ".", "mul_", "(", "beta1", ")", ".", "add_", "(", "1", "-", "beta1", ",", "grad", ")", "\n", "\n", "state", "[", "\"step\"", "]", "+=", "1", "\n", "beta2_t", "=", "beta2", "**", "state", "[", "\"step\"", "]", "\n", "N_sma_max", "=", "2", "/", "(", "1", "-", "beta2", ")", "-", "1", "\n", "N_sma", "=", "N_sma_max", "-", "2", "*", "state", "[", "\"step\"", "]", "*", "beta2_t", "/", "(", "1", "-", "beta2_t", ")", "\n", "\n", "if", "group", "[", "\"weight_decay\"", "]", "!=", "0", ":", "\n", "                    ", "p_data_fp32", ".", "add_", "(", "-", "group", "[", "\"weight_decay\"", "]", "*", "group", "[", "\"lr\"", "]", ",", "p_data_fp32", ")", "\n", "\n", "# more conservative since it's an approximated value", "\n", "", "if", "N_sma", ">=", "5", ":", "\n", "                    ", "step_size", "=", "(", "\n", "group", "[", "\"lr\"", "]", "\n", "*", "math", ".", "sqrt", "(", "\n", "(", "1", "-", "beta2_t", ")", "\n", "*", "(", "N_sma", "-", "4", ")", "\n", "/", "(", "N_sma_max", "-", "4", ")", "\n", "*", "(", "N_sma", "-", "2", ")", "\n", "/", "N_sma", "\n", "*", "N_sma_max", "\n", "/", "(", "N_sma_max", "-", "2", ")", "\n", ")", "\n", "/", "(", "1", "-", "beta1", "**", "state", "[", "\"step\"", "]", ")", "\n", ")", "\n", "denom", "=", "exp_avg_sq", ".", "sqrt", "(", ")", ".", "add_", "(", "group", "[", "\"eps\"", "]", ")", "\n", "p_data_fp32", ".", "addcdiv_", "(", "-", "step_size", ",", "exp_avg", ",", "denom", ")", "\n", "", "else", ":", "\n", "                    ", "step_size", "=", "group", "[", "\"lr\"", "]", "/", "(", "1", "-", "beta1", "**", "state", "[", "\"step\"", "]", ")", "\n", "p_data_fp32", ".", "add_", "(", "-", "step_size", ",", "exp_avg", ")", "\n", "\n", "", "p", ".", "data", ".", "copy_", "(", "p_data_fp32", ")", "\n", "\n", "", "", "return", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Optimizers.RAdam.AdamW.__init__": [[182, 189], ["dict", "torch.optim.optimizer.Optimizer.__init__"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.tests.model_selector_test.ToyModel.__init__"], ["    ", "def", "__init__", "(", "\n", "self", ",", "params", ",", "lr", "=", "1e-3", ",", "betas", "=", "(", "0.9", ",", "0.999", ")", ",", "eps", "=", "1e-8", ",", "weight_decay", "=", "0", ",", "warmup", "=", "0", "\n", ")", ":", "\n", "        ", "defaults", "=", "dict", "(", "\n", "lr", "=", "lr", ",", "betas", "=", "betas", ",", "eps", "=", "eps", ",", "weight_decay", "=", "weight_decay", ",", "warmup", "=", "warmup", "\n", ")", "\n", "super", "(", "AdamW", ",", "self", ")", ".", "__init__", "(", "params", ",", "defaults", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Optimizers.RAdam.AdamW.__setstate__": [[190, 192], ["super().__setstate__"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.iterators.CheckpointableIterator.__setstate__"], ["", "def", "__setstate__", "(", "self", ",", "state", ")", ":", "\n", "        ", "super", "(", "AdamW", ",", "self", ")", ".", "__setstate__", "(", "state", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Optimizers.RAdam.AdamW.step": [[193, 248], ["closure", "p.grad.data.float", "p.data.float", "exp_avg_sq.mul_().addcmul_", "exp_avg.mul_().add_", "exp_avg_sq.sqrt().add_", "p.data.float.addcdiv_", "p.data.copy_", "RuntimeError", "len", "torch.zeros_like", "torch.zeros_like", "state[].type_as", "state[].type_as", "p.data.float.add_", "exp_avg_sq.mul_", "exp_avg.mul_", "exp_avg_sq.sqrt", "math.sqrt"], "methods", ["None"], ["", "def", "step", "(", "self", ",", "closure", "=", "None", ")", ":", "\n", "        ", "loss", "=", "None", "\n", "if", "closure", "is", "not", "None", ":", "\n", "            ", "loss", "=", "closure", "(", ")", "\n", "\n", "", "for", "group", "in", "self", ".", "param_groups", ":", "\n", "\n", "            ", "for", "p", "in", "group", "[", "\"params\"", "]", ":", "\n", "                ", "if", "p", ".", "grad", "is", "None", ":", "\n", "                    ", "continue", "\n", "", "grad", "=", "p", ".", "grad", ".", "data", ".", "float", "(", ")", "\n", "if", "grad", ".", "is_sparse", ":", "\n", "                    ", "raise", "RuntimeError", "(", "\n", "\"Adam does not support sparse gradients, please consider SparseAdam instead\"", "\n", ")", "\n", "\n", "", "p_data_fp32", "=", "p", ".", "data", ".", "float", "(", ")", "\n", "\n", "state", "=", "self", ".", "state", "[", "p", "]", "\n", "\n", "if", "len", "(", "state", ")", "==", "0", ":", "\n", "                    ", "state", "[", "\"step\"", "]", "=", "0", "\n", "state", "[", "\"exp_avg\"", "]", "=", "torch", ".", "zeros_like", "(", "p_data_fp32", ")", "\n", "state", "[", "\"exp_avg_sq\"", "]", "=", "torch", ".", "zeros_like", "(", "p_data_fp32", ")", "\n", "", "else", ":", "\n", "                    ", "state", "[", "\"exp_avg\"", "]", "=", "state", "[", "\"exp_avg\"", "]", ".", "type_as", "(", "p_data_fp32", ")", "\n", "state", "[", "\"exp_avg_sq\"", "]", "=", "state", "[", "\"exp_avg_sq\"", "]", ".", "type_as", "(", "p_data_fp32", ")", "\n", "\n", "", "exp_avg", ",", "exp_avg_sq", "=", "state", "[", "\"exp_avg\"", "]", ",", "state", "[", "\"exp_avg_sq\"", "]", "\n", "beta1", ",", "beta2", "=", "group", "[", "\"betas\"", "]", "\n", "\n", "state", "[", "\"step\"", "]", "+=", "1", "\n", "\n", "exp_avg_sq", ".", "mul_", "(", "beta2", ")", ".", "addcmul_", "(", "1", "-", "beta2", ",", "grad", ",", "grad", ")", "\n", "exp_avg", ".", "mul_", "(", "beta1", ")", ".", "add_", "(", "1", "-", "beta1", ",", "grad", ")", "\n", "\n", "denom", "=", "exp_avg_sq", ".", "sqrt", "(", ")", ".", "add_", "(", "group", "[", "\"eps\"", "]", ")", "\n", "bias_correction1", "=", "1", "-", "beta1", "**", "state", "[", "\"step\"", "]", "\n", "bias_correction2", "=", "1", "-", "beta2", "**", "state", "[", "\"step\"", "]", "\n", "\n", "if", "group", "[", "\"warmup\"", "]", ">", "state", "[", "\"step\"", "]", ":", "\n", "                    ", "scheduled_lr", "=", "1e-8", "+", "state", "[", "\"step\"", "]", "*", "group", "[", "\"lr\"", "]", "/", "group", "[", "\"warmup\"", "]", "\n", "", "else", ":", "\n", "                    ", "scheduled_lr", "=", "group", "[", "\"lr\"", "]", "\n", "\n", "", "step_size", "=", "group", "[", "\"lr\"", "]", "*", "math", ".", "sqrt", "(", "bias_correction2", ")", "/", "bias_correction1", "\n", "\n", "if", "group", "[", "\"weight_decay\"", "]", "!=", "0", ":", "\n", "                    ", "p_data_fp32", ".", "add_", "(", "-", "group", "[", "\"weight_decay\"", "]", "*", "scheduled_lr", ",", "p_data_fp32", ")", "\n", "\n", "", "p_data_fp32", ".", "addcdiv_", "(", "-", "step_size", ",", "exp_avg", ",", "denom", ")", "\n", "\n", "p", ".", "data", ".", "copy_", "(", "p_data_fp32", ")", "\n", "\n", "", "", "return", "loss", "\n", "", "", ""]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Optimizers.LnrWrmpInvSqRtDcyScheduler.LnrWrmpInvSqRtDcyScheduler.__init__": [[11, 18], ["torch.optim.lr_scheduler.LambdaLR.__init__"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.tests.model_selector_test.ToyModel.__init__"], ["def", "__init__", "(", "self", ",", "optimizer", ",", "warmup_steps", ",", "warmup_init_lr", ",", "warmup_end_lr", ")", ":", "\n", "        ", "self", ".", "warmup_steps", "=", "warmup_steps", "\n", "self", ".", "warmup_init_lr", "=", "warmup_init_lr", "\n", "self", ".", "warmup_end_lr", "=", "warmup_end_lr", "\n", "self", ".", "lr_step", "=", "(", "warmup_end_lr", "-", "warmup_init_lr", ")", "/", "warmup_steps", "\n", "super", "(", "LnrWrmpInvSqRtDcyScheduler", ",", "self", ")", ".", "__init__", "(", "\n", "optimizer", ",", "self", ".", "lr_lambda", ",", "last_epoch", "=", "-", "1", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Optimizers.LnrWrmpInvSqRtDcyScheduler.LnrWrmpInvSqRtDcyScheduler.lr_lambda": [[20, 25], ["float", "math.sqrt", "float"], "methods", ["None"], ["", "def", "lr_lambda", "(", "self", ",", "step", ")", ":", "\n", "        ", "if", "step", "<", "self", ".", "warmup_steps", ":", "\n", "            ", "return", "(", "self", ".", "warmup_init_lr", "+", "step", "*", "self", ".", "lr_step", ")", "/", "self", ".", "warmup_end_lr", "\n", "", "else", ":", "\n", "            ", "return", "1.0", "/", "float", "(", "math", ".", "sqrt", "(", "step", "/", "float", "(", "self", ".", "warmup_steps", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Optimizers.LnrWrmpInvSqRtDcyScheduler.LnrWrmpInvSqRtDcyScheduler.get_last_lr": [[26, 28], ["LnrWrmpInvSqRtDcyScheduler.LnrWrmpInvSqRtDcyScheduler.get_lr"], "methods", ["None"], ["", "", "def", "get_last_lr", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "get_lr", "(", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.Layers.set_dropout_prob": [[16, 19], ["None"], "function", ["None"], ["def", "set_dropout_prob", "(", "p", ")", ":", "\n", "    ", "global", "dropout_p", "\n", "dropout_p", "=", "p", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.Layers.set_seq_dropout": [[21, 24], ["None"], "function", ["None"], ["", "def", "set_seq_dropout", "(", "option", ")", ":", "# option = True or False", "\n", "    ", "global", "do_seq_dropout", "\n", "do_seq_dropout", "=", "option", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.Layers.seq_dropout": [[26, 39], ["torch.autograd.Variable", "torch.autograd.Variable.unsqueeze().expand_as", "torch.bernoulli", "torch.bernoulli", "torch.bernoulli", "torch.bernoulli", "torch.autograd.Variable.unsqueeze", "x.data.new().zero_", "x.data.new", "x.size", "x.size"], "function", ["None"], ["", "def", "seq_dropout", "(", "x", ",", "p", "=", "0", ",", "training", "=", "False", ")", ":", "\n", "    ", "\"\"\"\n    x: batch * len * input_size\n    \"\"\"", "\n", "if", "training", "==", "False", "or", "p", "==", "0", ":", "\n", "        ", "return", "x", "\n", "", "dropout_mask", "=", "Variable", "(", "\n", "1.0", "\n", "/", "(", "1", "-", "p", ")", "\n", "*", "torch", ".", "bernoulli", "(", "(", "1", "-", "p", ")", "*", "(", "x", ".", "data", ".", "new", "(", "x", ".", "size", "(", "0", ")", ",", "x", ".", "size", "(", "2", ")", ")", ".", "zero_", "(", ")", "+", "1", ")", ")", ",", "\n", "requires_grad", "=", "False", ",", "\n", ")", "\n", "return", "dropout_mask", ".", "unsqueeze", "(", "1", ")", ".", "expand_as", "(", "x", ")", "*", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.Layers.dropout": [[41, 49], ["Layers.seq_dropout", "torch.dropout", "len", "x.size"], "function", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.Layers.seq_dropout", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.Layers.dropout"], ["", "def", "dropout", "(", "x", ",", "p", "=", "0", ",", "training", "=", "False", ")", ":", "\n", "    ", "\"\"\"\n    x: (batch * len * input_size) or (any other shape)\n    \"\"\"", "\n", "if", "do_seq_dropout", "and", "len", "(", "x", ".", "size", "(", ")", ")", "==", "3", ":", "# if x is (batch * len * input_size)", "\n", "        ", "return", "seq_dropout", "(", "x", ",", "p", "=", "p", ",", "training", "=", "training", ")", "\n", "", "else", ":", "\n", "        ", "return", "F", ".", "dropout", "(", "x", ",", "p", "=", "p", ",", "training", "=", "training", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.Transformer.LayerNorm.__init__": [[32, 37], ["torch.Module.__init__", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.tests.model_selector_test.ToyModel.__init__"], ["def", "__init__", "(", "self", ",", "n_state", ",", "e", "=", "1e-5", ")", ":", "\n", "        ", "super", "(", "LayerNorm", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "g", "=", "nn", ".", "Parameter", "(", "torch", ".", "ones", "(", "n_state", ")", ")", "\n", "self", ".", "b", "=", "nn", ".", "Parameter", "(", "torch", ".", "zeros", "(", "n_state", ")", ")", "\n", "self", ".", "e", "=", "e", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.Transformer.LayerNorm.forward": [[45, 50], ["x.mean", "torch.sqrt", "torch.sqrt", "torch.sqrt", "torch.sqrt", "torch.sqrt", "torch.sqrt", "torch.sqrt", "torch.sqrt", "torch.sqrt"], "methods", ["None"], ["def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "u", "=", "x", ".", "mean", "(", "-", "1", ",", "keepdim", "=", "True", ")", "\n", "s", "=", "(", "x", "-", "u", ")", ".", "pow", "(", "2", ")", ".", "mean", "(", "-", "1", ",", "keepdim", "=", "True", ")", "\n", "x", "=", "(", "x", "-", "u", ")", "/", "torch", ".", "sqrt", "(", "s", "+", "self", ".", "e", ")", "\n", "return", "self", ".", "g", "*", "x", "+", "self", ".", "b", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.Transformer.Conv1D.__init__": [[60, 67], ["torch.Module.__init__", "torch.empty", "torch.empty", "torch.empty", "torch.empty", "torch.empty", "torch.empty", "torch.empty", "torch.empty", "torch.empty", "torch.init.normal_", "torch.init.normal_", "torch.init.normal_", "torch.nn.parameter.Parameter", "torch.nn.parameter.Parameter", "torch.nn.parameter.Parameter", "torch.nn.parameter.Parameter", "torch.nn.parameter.Parameter", "torch.nn.parameter.Parameter", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.tests.model_selector_test.ToyModel.__init__"], ["    ", "def", "__init__", "(", "self", ",", "nf", ",", "nx", ")", ":", "\n", "        ", "super", "(", "Conv1D", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "nf", "=", "nf", "\n", "w", "=", "torch", ".", "empty", "(", "nx", ",", "nf", ")", "\n", "nn", ".", "init", ".", "normal_", "(", "w", ",", "std", "=", "0.02", ")", "\n", "self", ".", "w", "=", "Parameter", "(", "w", ")", "\n", "self", ".", "b", "=", "Parameter", "(", "torch", ".", "zeros", "(", "nf", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.Transformer.Conv1D.forward": [[75, 80], ["torch.addmm", "torch.addmm", "torch.addmm", "torch.addmm", "torch.addmm", "torch.addmm", "torch.addmm", "torch.addmm", "torch.addmm", "x.view.view.view", "x.view.view.view", "x.view.view.size", "x.view.view.size"], "methods", ["None"], ["def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "size_out", "=", "x", ".", "size", "(", ")", "[", ":", "-", "1", "]", "+", "(", "self", ".", "nf", ",", ")", "\n", "x", "=", "torch", ".", "addmm", "(", "self", ".", "b", ",", "x", ".", "view", "(", "-", "1", ",", "x", ".", "size", "(", "-", "1", ")", ")", ",", "self", ".", "w", ")", "\n", "x", "=", "x", ".", "view", "(", "*", "size_out", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.Transformer.PositionalEmbedding.__init__": [[83, 89], ["torch.Module.__init__", "float", "Transformer.PositionalEmbedding.register_buffer", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.tests.model_selector_test.ToyModel.__init__"], ["    ", "def", "__init__", "(", "self", ",", "opt", ",", "demb", ")", ":", "\n", "        ", "super", "(", "PositionalEmbedding", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "demb", "=", "demb", "\n", "inv_freq", "=", "1", "/", "(", "10000", "**", "(", "torch", ".", "arange", "(", "0.0", ",", "demb", ",", "2.0", ")", "/", "demb", ")", ")", "\n", "self", ".", "pos_discount", "=", "float", "(", "opt", "[", "\"TRANSFORMER_POS_DISCOUNT\"", "]", ")", "\n", "self", ".", "register_buffer", "(", "\"inv_freq\"", ",", "inv_freq", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.Transformer.PositionalEmbedding.forward": [[97, 104], ["torch.ger", "torch.ger", "torch.ger", "torch.ger", "torch.ger", "torch.ger", "torch.ger", "torch.ger", "torch.ger", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.ger.sin", "torch.ger.sin", "torch.ger.sin", "torch.ger.cos", "torch.ger.cos", "torch.ger.cos"], "methods", ["None"], ["def", "forward", "(", "self", ",", "pos_seq", ")", ":", "\n", "        ", "sinusoid_inp", "=", "torch", ".", "ger", "(", "pos_seq", ",", "self", ".", "inv_freq", ")", "\n", "pos_emb", "=", "(", "\n", "torch", ".", "cat", "(", "[", "sinusoid_inp", ".", "sin", "(", ")", ",", "sinusoid_inp", ".", "cos", "(", ")", "]", ",", "dim", "=", "-", "1", ")", "\n", "/", "self", ".", "pos_discount", "\n", ")", "\n", "return", "pos_emb", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.Transformer.Splitter.__init__": [[112, 116], ["torch.Module.__init__", "Transformer.Conv1D"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.tests.model_selector_test.ToyModel.__init__"], ["    ", "def", "__init__", "(", "self", ",", "nx", ")", ":", "\n", "        ", "super", "(", "Splitter", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "nx", "=", "nx", "\n", "self", ".", "augmenter", "=", "Conv1D", "(", "nx", "*", "3", ",", "nx", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.Transformer.Splitter.forward": [[124, 132], ["Transformer.Splitter.augmenter", "Transformer.Splitter.split"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.utils.sentence_splitter.PunktSentenceSplitter.split"], ["def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "augmenter", "(", "x", ")", "\n", "# x: batch x len x (3 x nx)", "\n", "\n", "query", ",", "key", ",", "value", "=", "x", ".", "split", "(", "self", ".", "nx", ",", "dim", "=", "2", ")", "\n", "# query,key,value: batch x len x nx", "\n", "\n", "return", "query", ",", "key", ",", "value", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.Transformer.Attention.__init__": [[144, 176], ["torch.Module.__init__", "int", "Transformer.Conv1D", "torch.Dropout", "torch.Dropout", "torch.Dropout", "torch.Dropout", "torch.Dropout", "torch.Dropout", "torch.autograd.Variable().cuda", "torch.autograd.Variable().cuda", "torch.autograd.Variable().cuda", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.tril().view", "torch.tril().view", "torch.tril().view", "torch.tril().view", "torch.tril().view", "torch.tril().view", "torch.tril().view", "torch.tril().view", "torch.tril().view", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.tril().view", "torch.tril().view", "torch.tril().view", "torch.tril().view", "torch.tril().view", "torch.tril().view", "torch.tril().view", "torch.tril().view", "torch.tril().view", "torch.tril", "torch.tril", "torch.tril", "torch.tril", "torch.tril", "torch.tril", "torch.tril", "torch.tril", "torch.tril", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.tril", "torch.tril", "torch.tril", "torch.tril", "torch.tril", "torch.tril", "torch.tril", "torch.tril", "torch.tril", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.tests.model_selector_test.ToyModel.__init__"], ["def", "__init__", "(", "self", ",", "nx", ",", "opt", ")", ":", "\n", "        ", "super", "(", "Attention", ",", "self", ")", ".", "__init__", "(", ")", "\n", "n_state", "=", "nx", "# in Attention: n_state=768 (nx=n_embd)", "\n", "# [switch nx => n_state from Block to Attention to keep identical to TF implem]", "\n", "n_head", "=", "int", "(", "opt", "[", "\"TRANSFORMER_HEAD\"", "]", ")", "\n", "resid_pdrop", "=", "opt", "[", "\"TRANSFORMER_RESIDUAL_DROPOUT\"", "]", "\n", "attn_pdrop", "=", "opt", "[", "\"TRANSFORMER_ATTENTION_DROPOUT\"", "]", "\n", "use_cuda", "=", "opt", "[", "\"cuda\"", "]", "\n", "\n", "assert", "n_state", "%", "n_head", "==", "0", "\n", "# if mask is needed, uncomment this", "\n", "self", ".", "maxlen", "=", "2048", "# beyond this scale", "\n", "self", ".", "mask", "=", "(", "\n", "Variable", "(", "\n", "torch", ".", "tril", "(", "torch", ".", "ones", "(", "self", ".", "maxlen", ",", "self", ".", "maxlen", ")", ")", ".", "view", "(", "\n", "1", ",", "1", ",", "self", ".", "maxlen", ",", "self", ".", "maxlen", "\n", ")", ",", "\n", "requires_grad", "=", "False", ",", "\n", ")", ".", "cuda", "(", ")", "\n", "if", "use_cuda", "\n", "else", "Variable", "(", "\n", "torch", ".", "tril", "(", "torch", ".", "ones", "(", "self", ".", "maxlen", ",", "self", ".", "maxlen", ")", ")", ".", "view", "(", "\n", "1", ",", "1", ",", "self", ".", "maxlen", ",", "self", ".", "maxlen", "\n", ")", ",", "\n", "requires_grad", "=", "False", ",", "\n", ")", "\n", ")", "\n", "self", ".", "n_head", "=", "n_head", "\n", "self", ".", "c_proj", "=", "Conv1D", "(", "n_state", ",", "nx", ")", "\n", "self", ".", "attn_dropout", "=", "nn", ".", "Dropout", "(", "attn_pdrop", ")", "\n", "self", ".", "resid_dropout", "=", "nn", ".", "Dropout", "(", "resid_pdrop", ")", "\n", "self", ".", "use_cuda", "=", "use_cuda", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.Transformer.Attention._attn": [[190, 233], ["torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "Transformer.Attention.attn_dropout", "math.sqrt", "x_mask.unsqueeze().unsqueeze().expand_as().float", "torch.Softmax", "torch.Softmax", "torch.Softmax", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "v.size", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul.size", "torch.matmul.size", "torch.matmul.size", "torch.matmul.size", "torch.matmul.size", "torch.matmul.size", "Transformer.Attention.mask[].cuda", "torch.autograd.Variable().cuda", "torch.autograd.Variable().cuda", "torch.autograd.Variable().cuda", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "x_mask.unsqueeze().unsqueeze().expand_as", "torch.tril().view", "torch.tril().view", "torch.tril().view", "torch.tril().view", "torch.tril().view", "torch.tril().view", "torch.tril().view", "torch.tril().view", "torch.tril().view", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.matmul.size", "torch.matmul.size", "torch.matmul.size", "torch.matmul.size", "torch.matmul.size", "torch.matmul.size", "x_mask.unsqueeze().unsqueeze", "torch.matmul.size", "torch.matmul.size", "torch.matmul.size", "torch.matmul.size", "torch.matmul.size", "torch.matmul.size", "torch.tril().view", "torch.tril().view", "torch.tril().view", "torch.tril().view", "torch.tril().view", "torch.tril().view", "torch.tril().view", "torch.tril().view", "torch.tril().view", "torch.tril", "torch.tril", "torch.tril", "torch.tril", "torch.tril", "torch.tril", "torch.tril", "torch.tril", "torch.tril", "torch.matmul.size", "torch.matmul.size", "torch.matmul.size", "torch.matmul.size", "torch.matmul.size", "torch.matmul.size", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "x_mask.unsqueeze", "torch.matmul.size", "torch.matmul.size", "torch.matmul.size", "torch.matmul.size", "torch.matmul.size", "torch.matmul.size", "torch.tril", "torch.tril", "torch.tril", "torch.tril", "torch.tril", "torch.tril", "torch.tril", "torch.tril", "torch.tril", "torch.matmul.size", "torch.matmul.size", "torch.matmul.size", "torch.matmul.size", "torch.matmul.size", "torch.matmul.size", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.matmul.size", "torch.matmul.size", "torch.matmul.size", "torch.matmul.size", "torch.matmul.size", "torch.matmul.size"], "methods", ["None"], ["def", "_attn", "(", "self", ",", "q", ",", "k", ",", "v", ",", "x_mask", ",", "one_dir_visible", ",", "return_attn_weight", ")", ":", "\n", "        ", "w", "=", "torch", ".", "matmul", "(", "q", ",", "k", ")", "\n", "# batch x n_head x len x kv_len", "\n", "w", "=", "w", "/", "math", ".", "sqrt", "(", "v", ".", "size", "(", "-", "1", ")", ")", "\n", "\n", "mask", "=", "None", "\n", "if", "one_dir_visible", ":", "# mask \"seeing the future\"", "\n", "            ", "if", "w", ".", "size", "(", "-", "2", ")", "<=", "self", ".", "maxlen", "and", "w", ".", "size", "(", "-", "1", ")", "<=", "self", ".", "maxlen", ":", "\n", "                ", "mask", "=", "(", "\n", "self", ".", "mask", "[", ":", ",", ":", ",", ":", "w", ".", "size", "(", "-", "2", ")", ",", ":", "w", ".", "size", "(", "-", "1", ")", "]", ".", "cuda", "(", ")", "\n", "if", "self", ".", "use_cuda", "\n", "else", "self", ".", "mask", "[", ":", ",", ":", ",", ":", "w", ".", "size", "(", "-", "2", ")", ",", ":", "w", ".", "size", "(", "-", "1", ")", "]", "\n", ")", "\n", "", "else", ":", "\n", "                ", "mask", "=", "(", "\n", "Variable", "(", "\n", "torch", ".", "tril", "(", "torch", ".", "ones", "(", "w", ".", "size", "(", "-", "2", ")", ",", "w", ".", "size", "(", "-", "1", ")", ")", ")", ".", "view", "(", "\n", "1", ",", "1", ",", "w", ".", "size", "(", "-", "2", ")", ",", "w", ".", "size", "(", "-", "1", ")", "\n", ")", ",", "\n", "requires_grad", "=", "False", ",", "\n", ")", ".", "cuda", "(", ")", "\n", "if", "self", ".", "use_cuda", "\n", "else", "Variable", "(", "\n", "torch", ".", "tril", "(", "torch", ".", "ones", "(", "w", ".", "size", "(", "-", "2", ")", ",", "w", ".", "size", "(", "-", "1", ")", ")", ")", ".", "view", "(", "\n", "1", ",", "1", ",", "w", ".", "size", "(", "-", "2", ")", ",", "w", ".", "size", "(", "-", "1", ")", "\n", ")", ",", "\n", "requires_grad", "=", "False", ",", "\n", ")", "\n", ")", "\n", "\n", "", "", "if", "x_mask", "is", "not", "None", ":", "\n", "            ", "mask", "=", "x_mask", ".", "unsqueeze", "(", "1", ")", ".", "unsqueeze", "(", "1", ")", ".", "expand_as", "(", "w", ")", ".", "float", "(", ")", "\n", "# batch x n_head x len x kv_len", "\n", "\n", "", "if", "mask", "is", "not", "None", ":", "\n", "            ", "w", "=", "w", "*", "mask", "+", "-", "1e9", "*", "(", "1", "-", "mask", ")", "\n", "\n", "", "w_prob", "=", "nn", ".", "Softmax", "(", "dim", "=", "-", "1", ")", "(", "w", ")", "\n", "w_prob", "=", "self", ".", "attn_dropout", "(", "w_prob", ")", "\n", "if", "return_attn_weight", ":", "\n", "            ", "return", "torch", ".", "matmul", "(", "w_prob", ",", "v", ")", ",", "w", "\n", "", "else", ":", "\n", "            ", "return", "torch", ".", "matmul", "(", "w_prob", ",", "v", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.Transformer.Attention.merge_heads": [[234, 238], ["x.permute().contiguous.permute().contiguous.permute().contiguous", "x.permute().contiguous.permute().contiguous.view", "x.permute().contiguous.permute().contiguous.permute", "x.permute().contiguous.permute().contiguous.size", "x.permute().contiguous.permute().contiguous.size", "x.permute().contiguous.permute().contiguous.size"], "methods", ["None"], ["", "", "def", "merge_heads", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "x", ".", "permute", "(", "0", ",", "2", ",", "1", ",", "3", ")", ".", "contiguous", "(", ")", "\n", "new_x_shape", "=", "x", ".", "size", "(", ")", "[", ":", "-", "2", "]", "+", "(", "x", ".", "size", "(", "-", "2", ")", "*", "x", ".", "size", "(", "-", "1", ")", ",", ")", "\n", "return", "x", ".", "view", "(", "*", "new_x_shape", ")", "# in Tensorflow implem: fct merge_states", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.Transformer.Attention.split_heads": [[247, 254], ["x.view.view.view", "x.view.view.permute", "x.view.view.permute", "x.view.view.size", "x.view.view.size"], "methods", ["None"], ["def", "split_heads", "(", "self", ",", "x", ",", "k", "=", "False", ")", ":", "\n", "        ", "new_x_shape", "=", "x", ".", "size", "(", ")", "[", ":", "-", "1", "]", "+", "(", "self", ".", "n_head", ",", "x", ".", "size", "(", "-", "1", ")", "//", "self", ".", "n_head", ")", "\n", "x", "=", "x", ".", "view", "(", "*", "new_x_shape", ")", "# in Tensorflow implem: fct split_states", "\n", "if", "k", ":", "\n", "            ", "return", "x", ".", "permute", "(", "0", ",", "2", ",", "3", ",", "1", ")", "\n", "", "else", ":", "\n", "            ", "return", "x", ".", "permute", "(", "0", ",", "2", ",", "1", ",", "3", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.Transformer.Attention.forward": [[267, 306], ["Transformer.Attention.split_heads", "Transformer.Attention.split_heads", "Transformer.Attention.split_heads", "Transformer.Attention._attn", "Transformer.Attention.merge_heads", "Transformer.Attention.c_proj", "Transformer.Attention.resid_dropout", "torch.sum.permute().contiguous", "torch.sum.permute().contiguous", "torch.sum.permute().contiguous", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum.permute", "torch.sum.permute", "torch.sum.permute"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.Transformer.Attention.split_heads", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.Transformer.Attention.split_heads", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.Transformer.Attention.split_heads", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.Transformer.Attention._attn", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.Transformer.Attention.merge_heads"], ["def", "forward", "(", "\n", "self", ",", "query", ",", "key", ",", "value", ",", "x_mask", ",", "one_dir_visible", "=", "False", ",", "return_attn_weight", "=", "False", "\n", ")", ":", "\n", "        ", "query", "=", "self", ".", "split_heads", "(", "query", ")", "\n", "# batch x n_head x len x (n_state/n_head)", "\n", "\n", "key", "=", "self", ".", "split_heads", "(", "key", ",", "k", "=", "True", ")", "\n", "# batch x n_head x (n_state/n_head) x kv_len", "\n", "\n", "value", "=", "self", ".", "split_heads", "(", "value", ")", "\n", "# batch x n_head x kv_len x (n_state/n_head)", "\n", "\n", "out", "=", "self", ".", "_attn", "(", "query", ",", "key", ",", "value", ",", "x_mask", ",", "one_dir_visible", ",", "return_attn_weight", ")", "\n", "\n", "if", "return_attn_weight", ":", "\n", "            ", "a", ",", "attn_weight", "=", "out", "\n", "# a: batch x n_head x len x (n_state/n_head)", "\n", "# attn_weight: batch x n_head x len x kv_len", "\n", "attn_weight", "=", "attn_weight", ".", "permute", "(", "0", ",", "2", ",", "3", ",", "1", ")", ".", "contiguous", "(", ")", "\n", "# batch x len x kv_len x n_head", "\n", "attn_weight", "=", "torch", ".", "sum", "(", "attn_weight", ",", "dim", "=", "3", ")", "\n", "# batch x len x kv_len", "\n", "", "else", ":", "\n", "            ", "a", "=", "out", "\n", "# batch x n_head x len x (n_state/n_head)", "\n", "\n", "", "a", "=", "self", ".", "merge_heads", "(", "a", ")", "\n", "# batch x len x n_state", "\n", "\n", "a", "=", "self", ".", "c_proj", "(", "a", ")", "\n", "# batch x len x n_state", "\n", "\n", "a", "=", "self", ".", "resid_dropout", "(", "a", ")", "\n", "# batch x len x n_state", "\n", "\n", "if", "return_attn_weight", ":", "\n", "            ", "return", "a", ",", "attn_weight", "\n", "", "else", ":", "\n", "            ", "return", "a", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.Transformer.MLP.__init__": [[319, 326], ["torch.Module.__init__", "int", "Transformer.Conv1D", "Transformer.Conv1D", "torch.Dropout", "torch.Dropout", "torch.Dropout"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.tests.model_selector_test.ToyModel.__init__"], ["def", "__init__", "(", "self", ",", "n_state", ",", "opt", ")", ":", "# in MLP: n_state=3072 (4 * n_embd)", "\n", "        ", "super", "(", "MLP", ",", "self", ")", ".", "__init__", "(", ")", "\n", "nx", "=", "int", "(", "opt", "[", "\"transformer_embed_dim\"", "]", ")", "\n", "resid_pdrop", "=", "opt", "[", "\"TRANSFORMER_RESIDUAL_DROPOUT\"", "]", "\n", "self", ".", "c_fc", "=", "Conv1D", "(", "n_state", ",", "nx", ")", "\n", "self", ".", "c_proj", "=", "Conv1D", "(", "nx", ",", "n_state", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "resid_pdrop", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.Transformer.MLP.forward": [[333, 337], ["torch.relu", "torch.relu", "torch.relu", "Transformer.MLP.c_proj", "Transformer.MLP.dropout", "Transformer.MLP.c_fc"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.Layers.dropout"], ["def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "h", "=", "F", ".", "relu", "(", "self", ".", "c_fc", "(", "x", ")", ")", "\n", "h2", "=", "self", ".", "c_proj", "(", "h", ")", "\n", "return", "self", ".", "dropout", "(", "h2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.Transformer.EncoderBlock.__init__": [[345, 356], ["torch.Module.__init__", "int", "Transformer.Splitter", "Transformer.Attention", "Transformer.LayerNorm", "Transformer.MLP", "Transformer.LayerNorm"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.tests.model_selector_test.ToyModel.__init__"], ["    ", "def", "__init__", "(", "self", ",", "opt", ")", ":", "\n", "        ", "super", "(", "EncoderBlock", ",", "self", ")", ".", "__init__", "(", ")", "\n", "nx", "=", "int", "(", "opt", "[", "\"transformer_embed_dim\"", "]", ")", "\n", "self", ".", "one_dir_visible", "=", "False", "\n", "if", "\"transformer_encoder_one_dir_visible\"", "in", "opt", ":", "\n", "            ", "self", ".", "one_dir_visible", "=", "opt", "[", "\"transformer_encoder_one_dir_visible\"", "]", "\n", "", "self", ".", "splitter", "=", "Splitter", "(", "nx", ")", "\n", "self", ".", "attn", "=", "Attention", "(", "nx", ",", "opt", ")", "\n", "self", ".", "ln_1", "=", "LayerNorm", "(", "nx", ")", "\n", "self", ".", "mlp", "=", "MLP", "(", "4", "*", "nx", ",", "opt", ")", "\n", "self", ".", "ln_2", "=", "LayerNorm", "(", "nx", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.Transformer.EncoderBlock.forward": [[365, 378], ["Transformer.EncoderBlock.splitter", "Transformer.EncoderBlock.ln_1", "Transformer.EncoderBlock.mlp", "Transformer.EncoderBlock.ln_2", "Transformer.EncoderBlock.attn", "Transformer.EncoderBlock.attn"], "methods", ["None"], ["def", "forward", "(", "self", ",", "x", ",", "x_mask", ")", ":", "\n", "        ", "query", ",", "key", ",", "value", "=", "self", ".", "splitter", "(", "x", ")", "\n", "if", "self", ".", "one_dir_visible", ":", "\n", "# in this case, use triangle masking, as it's one_direction", "\n", "            ", "a", "=", "self", ".", "attn", "(", "query", ",", "key", ",", "value", ",", "None", ",", "one_dir_visible", "=", "True", ")", "\n", "", "else", ":", "\n", "# in this case, use x_mask for attention masking", "\n", "            ", "a", "=", "self", ".", "attn", "(", "query", ",", "key", ",", "value", ",", "x_mask", ",", "one_dir_visible", "=", "False", ")", "\n", "\n", "", "n", "=", "self", ".", "ln_1", "(", "x", "+", "a", ")", "# residual", "\n", "m", "=", "self", ".", "mlp", "(", "n", ")", "\n", "h", "=", "self", ".", "ln_2", "(", "n", "+", "m", ")", "\n", "return", "h", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.Transformer.DecoderBlock.__init__": [[386, 396], ["torch.Module.__init__", "int", "Transformer.Splitter", "Transformer.Attention", "Transformer.Attention", "Transformer.LayerNorm", "Transformer.LayerNorm", "Transformer.MLP", "Transformer.LayerNorm"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.tests.model_selector_test.ToyModel.__init__"], ["    ", "def", "__init__", "(", "self", ",", "opt", ")", ":", "\n", "        ", "super", "(", "DecoderBlock", ",", "self", ")", ".", "__init__", "(", ")", "\n", "nx", "=", "int", "(", "opt", "[", "\"transformer_embed_dim\"", "]", ")", "\n", "self", ".", "decoder_splitter", "=", "Splitter", "(", "nx", ")", "\n", "self", ".", "self_attn", "=", "Attention", "(", "nx", ",", "opt", ")", "\n", "self", ".", "cross_attn", "=", "Attention", "(", "nx", ",", "opt", ")", "\n", "self", ".", "ln_1", "=", "LayerNorm", "(", "nx", ")", "\n", "self", ".", "ln_2", "=", "LayerNorm", "(", "nx", ")", "\n", "self", ".", "mlp", "=", "MLP", "(", "4", "*", "nx", ",", "opt", ")", "\n", "self", ".", "ln_3", "=", "LayerNorm", "(", "nx", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.Transformer.DecoderBlock.forward": [[408, 430], ["Transformer.DecoderBlock.decoder_splitter", "Transformer.DecoderBlock.self_attn", "Transformer.DecoderBlock.ln_1", "Transformer.DecoderBlock.mlp", "Transformer.DecoderBlock.ln_3", "Transformer.DecoderBlock.cross_attn", "Transformer.DecoderBlock.ln_2"], "methods", ["None"], ["def", "forward", "(", "self", ",", "x_mask", ",", "y", ",", "enc_key", ",", "enc_value", ",", "lang_model", "=", "False", ")", ":", "\n", "        ", "query", ",", "key", ",", "value", "=", "self", ".", "decoder_splitter", "(", "y", ")", "\n", "# batch x len x n_state", "\n", "\n", "# self-attention", "\n", "a", "=", "self", ".", "self_attn", "(", "query", ",", "key", ",", "value", ",", "None", ",", "one_dir_visible", "=", "True", ")", "\n", "# batch x len x n_state", "\n", "\n", "n", "=", "self", ".", "ln_1", "(", "y", "+", "a", ")", "# residual", "\n", "\n", "# seq2seq", "\n", "if", "not", "lang_model", ":", "\n", "# src-tgt attention", "\n", "            ", "o", "=", "self", ".", "cross_attn", "(", "n", ",", "enc_key", ",", "enc_value", ",", "x_mask", ")", "\n", "p", "=", "self", ".", "ln_2", "(", "n", "+", "o", ")", "# residual", "\n", "# batch x len x n_state", "\n", "", "else", ":", "# language model", "\n", "            ", "p", "=", "n", "\n", "\n", "", "m", "=", "self", ".", "mlp", "(", "p", ")", "\n", "h", "=", "self", ".", "ln_3", "(", "p", "+", "m", ")", "\n", "return", "h", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.Transformer.Embedder.__init__": [[443, 455], ["torch.Module.__init__", "int", "torch.Dropout", "torch.Dropout", "torch.Dropout", "Transformer.PositionalEmbedding", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.init.normal_", "torch.init.normal_", "torch.init.normal_"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.tests.model_selector_test.ToyModel.__init__"], ["def", "__init__", "(", "self", ",", "opt", ",", "embed", "=", "None", ")", ":", "\n", "        ", "super", "(", "Embedder", ",", "self", ")", ".", "__init__", "(", ")", "\n", "n_state", "=", "int", "(", "opt", "[", "\"transformer_embed_dim\"", "]", ")", "# n_state", "\n", "embed_dropout_rate", "=", "opt", "[", "\"TRANSFORMER_EMBED_DROPOUT\"", "]", "\n", "if", "embed", "is", "None", ":", "\n", "            ", "self", ".", "embed", "=", "nn", ".", "Embedding", "(", "opt", "[", "\"vocab_size\"", "]", ",", "n_state", ")", "\n", "nn", ".", "init", ".", "normal_", "(", "self", ".", "embed", ".", "weight", ",", "std", "=", "0.02", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "embed", "=", "embed", "\n", "", "self", ".", "drop", "=", "nn", ".", "Dropout", "(", "embed_dropout_rate", ")", "\n", "self", ".", "pos_emb", "=", "PositionalEmbedding", "(", "opt", ",", "n_state", ")", "\n", "self", ".", "use_cuda", "=", "opt", "[", "\"cuda\"", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.Transformer.Embedder.forward": [[463, 484], ["Transformer.Embedder.embed", "Transformer.Embedder.pos_emb", "Transformer.Embedder.drop", "torch.arange().type", "torch.arange().type", "torch.arange().type", "torch.arange().type", "torch.arange().type", "torch.arange().type", "torch.arange().type", "torch.arange().type", "torch.arange().type", "torch.autograd.Variable().cuda", "torch.autograd.Variable().cuda", "torch.autograd.Variable().cuda", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "Transformer.Embedder.unsqueeze().repeat", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "Transformer.Embedder.unsqueeze().repeat", "Transformer.Embedder.unsqueeze", "Transformer.Embedder.unsqueeze"], "methods", ["None"], ["def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x_emb", "=", "self", ".", "embed", "(", "x", ")", "\n", "batch_size", "=", "x", ".", "shape", "[", "0", "]", "\n", "x_len", "=", "x", ".", "shape", "[", "1", "]", "\n", "x_pos", "=", "self", ".", "pos_emb", "(", "\n", "torch", ".", "arange", "(", "x_len", ")", ".", "type", "(", "\n", "torch", ".", "cuda", ".", "FloatTensor", "if", "self", ".", "use_cuda", "else", "torch", ".", "FloatTensor", "\n", ")", "\n", ")", "# len x n_state", "\n", "x_pos", "=", "(", "\n", "Variable", "(", "\n", "x_pos", ".", "unsqueeze", "(", "0", ")", ".", "repeat", "(", "batch_size", ",", "1", ",", "1", ")", ",", "requires_grad", "=", "False", "\n", ")", ".", "cuda", "(", ")", "\n", "if", "self", ".", "use_cuda", "\n", "else", "Variable", "(", "\n", "x_pos", ".", "unsqueeze", "(", "0", ")", ".", "repeat", "(", "batch_size", ",", "1", ",", "1", ")", ",", "requires_grad", "=", "False", "\n", ")", "\n", ")", "\n", "x_input", "=", "x_emb", "+", "x_pos", "\n", "h", "=", "self", ".", "drop", "(", "x_input", ")", "\n", "return", "h", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.Transformer.TransformerEncoder.__init__": [[497, 509], ["torch.Module.__init__", "int", "int", "int", "Transformer.Embedder", "Transformer.EncoderBlock", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "float", "copy.deepcopy", "range"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.tests.model_selector_test.ToyModel.__init__"], ["def", "__init__", "(", "self", ",", "opt", ",", "embed", "=", "None", ")", ":", "\n", "        ", "super", "(", "TransformerEncoder", ",", "self", ")", ".", "__init__", "(", ")", "\n", "vocab", "=", "int", "(", "opt", "[", "\"vocab_size\"", "]", ")", "\n", "n_state", "=", "int", "(", "opt", "[", "\"transformer_embed_dim\"", "]", ")", "\n", "n_layer", "=", "int", "(", "opt", "[", "\"TRANSFORMER_LAYER\"", "]", ")", "\n", "if", "\"vae_z_scale_factor\"", "in", "opt", ":", "\n", "            ", "self", ".", "vae_z_scale_factor", "=", "float", "(", "opt", "[", "\"vae_z_scale_factor\"", "]", ")", "\n", "\n", "", "self", ".", "embedder", "=", "Embedder", "(", "opt", ",", "embed", ")", "\n", "block", "=", "EncoderBlock", "(", "opt", ")", "\n", "self", ".", "blocks", "=", "nn", ".", "ModuleList", "(", "[", "copy", ".", "deepcopy", "(", "block", ")", "for", "_", "in", "range", "(", "n_layer", ")", "]", ")", "\n", "self", ".", "use_cuda", "=", "opt", "[", "\"cuda\"", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.Transformer.TransformerEncoder.forward": [[518, 532], ["x_mask.type.type.type", "Transformer.TransformerEncoder.embedder", "x.eq", "block"], "methods", ["None"], ["def", "forward", "(", "self", ",", "x", ",", "z", "=", "None", ")", ":", "\n", "        ", "x_mask", "=", "~", "x", ".", "eq", "(", "0", ")", "# 1 is PAD_id", "\n", "x_mask", "=", "x_mask", ".", "type", "(", "\n", "torch", ".", "cuda", ".", "FloatTensor", "if", "self", ".", "use_cuda", "else", "torch", ".", "FloatTensor", "\n", ")", "\n", "\n", "h", "=", "self", ".", "embedder", "(", "x", ")", "\n", "if", "z", "is", "not", "None", ":", "\n", "            ", "z", "*=", "self", ".", "vae_z_scale_factor", "\n", "h", "+=", "z", "\n", "\n", "", "for", "block", "in", "self", ".", "blocks", ":", "\n", "            ", "h", "=", "block", "(", "h", ",", "x_mask", ")", "\n", "", "return", "h", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.Transformer.TransformerDecoder.__init__": [[545, 564], ["torch.Module.__init__", "int", "int", "int", "Transformer.Embedder", "Transformer.Splitter", "Transformer.DecoderBlock", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "Transformer.Conv1D", "torch.Linear", "torch.Linear", "torch.Linear", "copy.deepcopy", "range"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.tests.model_selector_test.ToyModel.__init__"], ["def", "__init__", "(", "self", ",", "opt", ",", "embed", "=", "None", ")", ":", "\n", "        ", "super", "(", "TransformerDecoder", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "opt", "=", "opt", "\n", "vocab_size", "=", "int", "(", "opt", "[", "\"vocab_size\"", "]", ")", "\n", "n_state", "=", "int", "(", "opt", "[", "\"transformer_embed_dim\"", "]", ")", "# n_state", "\n", "n_layer", "=", "int", "(", "opt", "[", "\"TRANSFORMER_LAYER\"", "]", ")", "\n", "self", ".", "embedder", "=", "Embedder", "(", "opt", ",", "embed", ")", "\n", "self", ".", "encoder_splitter", "=", "Splitter", "(", "n_state", ")", "\n", "block", "=", "DecoderBlock", "(", "opt", ")", "\n", "self", ".", "blocks", "=", "nn", ".", "ModuleList", "(", "[", "copy", ".", "deepcopy", "(", "block", ")", "for", "_", "in", "range", "(", "n_layer", ")", "]", ")", "\n", "if", "embed", "is", "None", ":", "\n", "            ", "self", ".", "linear", "=", "Conv1D", "(", "vocab_size", ",", "n_state", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "linear", "=", "nn", ".", "Linear", "(", "n_state", ",", "vocab_size", ",", "bias", "=", "False", ")", "\n", "if", "(", "\n", "\"FINETUNE_RETRAIN_SOFTMAX\"", "not", "in", "opt", "\n", ")", ":", "# if FINETUNE_RETRAIN_SOFTMAX, linear needs to be seperately trained", "\n", "                ", "self", ".", "linear", ".", "weight", "=", "embed", ".", "weight", "# share weight", "\n", "", "", "self", ".", "use_coda", "=", "opt", "[", "\"cuda\"", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.Transformer.TransformerDecoder.forward": [[575, 596], ["Transformer.TransformerDecoder.embedder", "torch.softmax", "torch.softmax", "torch.softmax", "Transformer.TransformerDecoder.encoder_splitter", "x_mask.type.type.type", "block", "Transformer.TransformerDecoder.linear", "x.eq"], "methods", ["None"], ["def", "forward", "(", "self", ",", "x", ",", "x_out", ",", "y", ",", "lang_model", "=", "False", ")", ":", "\n", "# seq2seq", "\n", "        ", "if", "not", "lang_model", ":", "\n", "            ", "_", ",", "enc_key", ",", "enc_value", "=", "self", ".", "encoder_splitter", "(", "x_out", ")", "\n", "# enc_key: batch x encoder_len x n_state", "\n", "# enc_value: batch x encoder_len x n_state", "\n", "\n", "x_mask", "=", "~", "x", ".", "eq", "(", "0", ")", "# 1 is PAD_id", "\n", "x_mask", "=", "x_mask", ".", "type", "(", "\n", "torch", ".", "cuda", ".", "FloatTensor", "if", "self", ".", "use_cuda", "else", "torch", ".", "FloatTensor", "\n", ")", "\n", "", "else", ":", "\n", "            ", "enc_key", "=", "None", "\n", "enc_value", "=", "None", "\n", "x_mask", "=", "None", "\n", "\n", "", "h", "=", "self", ".", "embedder", "(", "y", ")", "\n", "for", "block", "in", "self", ".", "blocks", ":", "\n", "            ", "h", "=", "block", "(", "x_mask", ",", "h", ",", "enc_key", ",", "enc_value", ",", "lang_model", ")", "\n", "", "prob", "=", "F", ".", "softmax", "(", "self", ".", "linear", "(", "h", ")", ",", "dim", "=", "-", "1", ")", "\n", "return", "prob", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.Transformer.TransformerBeam.__init__": [[607, 616], ["int", "int"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "opt", ",", "encoder", ",", "decoder", ",", "begin_id", ",", "vocab", ")", ":", "\n", "        ", "self", ".", "encoder", "=", "encoder", "\n", "self", ".", "decoder", "=", "decoder", "\n", "self", ".", "opt", "=", "opt", "\n", "self", ".", "max_sent_len", "=", "int", "(", "opt", "[", "\"max_sent_len\"", "]", ")", "\n", "self", ".", "begin_id", "=", "begin_id", "\n", "self", ".", "vocab", "=", "vocab", "\n", "self", ".", "beam_width", "=", "int", "(", "opt", "[", "\"beam_width\"", "]", ")", "\n", "self", ".", "use_cuda", "=", "opt", "[", "\"cuda\"", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.Transformer.TransformerBeam.merge_candidates": [[618, 631], ["len", "len", "C.append", "C.append"], "methods", ["None"], ["", "def", "merge_candidates", "(", "self", ",", "cand_A", ",", "cand_B", ")", ":", "\n", "        ", "C", "=", "[", "]", "\n", "pA", ",", "lA", ",", "pB", ",", "lB", "=", "0", ",", "len", "(", "cand_A", ")", ",", "0", ",", "len", "(", "cand_B", ")", "\n", "lC", "=", "0", "\n", "while", "(", "pA", "<", "lA", "or", "pB", "<", "lB", ")", "and", "(", "lC", "<", "self", ".", "beam_width", ")", ":", "\n", "            ", "if", "pA", "<", "lA", "and", "(", "pB", ">=", "lB", "or", "cand_A", "[", "pA", "]", "[", "1", "]", ">", "cand_B", "[", "pB", "]", "[", "1", "]", ")", ":", "\n", "                ", "C", ".", "append", "(", "cand_A", "[", "pA", "]", ")", "\n", "pA", "+=", "1", "\n", "", "else", ":", "\n", "                ", "C", ".", "append", "(", "cand_B", "[", "pB", "]", ")", "\n", "pB", "+=", "1", "\n", "", "lC", "+=", "1", "\n", "", "return", "C", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.Transformer.TransformerBeam.topk": [[640, 692], ["Transformer.TransformerBeam.encoder", "range", "range", "range", "sent_ids.append", "int", "Transformer.TransformerBeam.decoder", "range", "torch.multinomial", "torch.multinomial", "torch.multinomial", "torch.multinomial", "torch.multinomial", "torch.multinomial", "torch.multinomial", "torch.multinomial", "torch.multinomial", "range", "range", "sents.append", "torch.autograd.Variable().cuda", "torch.autograd.Variable().cuda", "torch.autograd.Variable().cuda", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.topk", "torch.topk", "torch.topk", "torch.topk", "torch.topk", "torch.topk", "torch.topk", "torch.topk", "torch.topk", "float", "len", "utt.append", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "sent_ids[].append", "sent_ids[].append", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "int", "int", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "int"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizerFast.decoder", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.Transformer.TransformerBeam.topk", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.Transformer.TransformerBeam.topk", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.Transformer.TransformerBeam.topk", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.Transformer.TransformerBeam.topk", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.Transformer.TransformerBeam.topk", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.Transformer.TransformerBeam.topk", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.Transformer.TransformerBeam.topk", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.Transformer.TransformerBeam.topk", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.Transformer.TransformerBeam.topk"], ["def", "topk", "(", "self", ",", "x", ",", "k", ")", ":", "\n", "        ", "batch_size", "=", "x", ".", "shape", "[", "0", "]", "\n", "x_len", "=", "x", ".", "shape", "[", "1", "]", "\n", "x_out", "=", "self", ".", "encoder", "(", "x", ")", "\n", "# x_out: batch x encoder_len x n_state", "\n", "\n", "# sent_ids is the words for each of the batch_size sentences", "\n", "sent_ids", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "batch_size", ")", ":", "\n", "            ", "sent_ids", ".", "append", "(", "[", "self", ".", "begin_id", "]", ")", "\n", "\n", "", "topk", "=", "1", "\n", "MIN_GEN_LENGTH", "=", "45", "\n", "if", "\"MIN_GEN_LENGTH\"", "in", "self", ".", "opt", ":", "\n", "            ", "MIN_GEN_LENGTH", "=", "int", "(", "self", ".", "opt", "[", "\"MIN_GEN_LENGTH\"", "]", ")", "\n", "", "for", "l", "in", "range", "(", "self", ".", "max_sent_len", ")", ":", "\n", "            ", "y", "=", "(", "\n", "Variable", "(", "torch", ".", "LongTensor", "(", "sent_ids", ")", ")", ".", "cuda", "(", ")", "\n", "if", "self", ".", "use_cuda", "\n", "else", "Variable", "(", "torch", ".", "LongTensor", "(", "sent_ids", ")", ")", "\n", ")", "# batch_size x l", "\n", "decoder_outputs", "=", "self", ".", "decoder", "(", "x", ",", "x_out", ",", "y", ")", "\n", "probs", "=", "decoder_outputs", "[", "\n", ":", ",", "-", "1", ",", ":", "\n", "]", "# batch_size x vocab_size (only take the last output)", "\n", "for", "i", "in", "range", "(", "batch_size", ")", ":", "\n", "                ", "topk_probs", ",", "_", "=", "torch", ".", "topk", "(", "probs", "[", "i", "]", ",", "k", ")", "\n", "threshold", "=", "float", "(", "topk_probs", "[", "-", "1", "]", ")", "\n", "probs", "[", "i", "]", "[", "probs", "[", "i", "]", "<", "threshold", "]", "=", "0.0", "\n", "\n", "", "samples", "=", "torch", ".", "multinomial", "(", "\n", "probs", ",", "2", "\n", ")", "# sample 2 since the first one may be <END>", "\n", "for", "i", "in", "range", "(", "batch_size", ")", ":", "\n", "                ", "if", "l", "<", "MIN_GEN_LENGTH", "and", "self", ".", "vocab", "[", "int", "(", "samples", "[", "i", ",", "0", "]", ")", "]", "==", "\"<END>\"", ":", "\n", "                    ", "sent_ids", "[", "i", "]", ".", "append", "(", "int", "(", "samples", "[", "i", ",", "1", "]", ")", ")", "\n", "", "else", ":", "\n", "                    ", "sent_ids", "[", "i", "]", ".", "append", "(", "int", "(", "samples", "[", "i", ",", "0", "]", ")", ")", "\n", "\n", "", "", "", "sents", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "batch_size", ")", ":", "\n", "            ", "utt", "=", "[", "]", "\n", "for", "j", "in", "range", "(", "len", "(", "sent_ids", "[", "i", "]", ")", ")", ":", "\n", "                ", "w", "=", "self", ".", "vocab", "[", "sent_ids", "[", "i", "]", "[", "j", "]", "]", "\n", "if", "w", "==", "\"<BEGIN>\"", ":", "\n", "                    ", "continue", "\n", "", "if", "w", "==", "\"<END>\"", ":", "\n", "                    ", "break", "\n", "", "utt", ".", "append", "(", "w", ")", "\n", "", "sents", ".", "append", "(", "[", "(", "utt", ",", "0", ")", "]", ")", "\n", "\n", "", "return", "sents", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.Transformer.TransformerBeam.beam_search": [[700, 817], ["Transformer.TransformerBeam.encoder", "range", "range", "range", "Transformer.BeamSearchNode", "range", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "Transformer.TransformerBeam.decoder", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "history_nodes.append", "range", "int", "len", "end_nodes[].sort", "sents.append", "sum", "ys.extend", "torch.cat.extend", "torch.cat.extend", "torch.cat.extend", "torch.cat.extend", "torch.cat.extend", "torch.cat.extend", "torch.autograd.Variable().cuda", "torch.autograd.Variable().cuda", "torch.autograd.Variable().cuda", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "len", "range", "set", "range", "range", "Transformer.TransformerBeam.append", "len", "Transformer.TransformerBeam.append", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.topk", "torch.topk", "torch.topk", "torch.topk", "torch.topk", "torch.topk", "torch.topk", "torch.topk", "torch.topk", "Transformer.TransformerBeam.merge_candidates", "len", "float", "int", "Transformer.BeamSearchNode", "len", "x_out[].unsqueeze", "x[].unsqueeze", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "set.add", "sum", "last_nodes.items", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "end_nodes[].append", "[].append", "end_nodes[].append", "zip", "Transformer.BeamSearchNode.eval", "node.eval"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizerFast.decoder", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.DistributedTrainer.DistributedTrainer.log", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.DistributedTrainer.DistributedTrainer.log", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.DistributedTrainer.DistributedTrainer.log", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.DistributedTrainer.DistributedTrainer.log", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.DistributedTrainer.DistributedTrainer.log", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.DistributedTrainer.DistributedTrainer.log", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.DistributedTrainer.DistributedTrainer.log", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.DistributedTrainer.DistributedTrainer.log", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.DistributedTrainer.DistributedTrainer.log", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.Transformer.TransformerBeam.topk", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.Transformer.TransformerBeam.topk", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.Transformer.TransformerBeam.topk", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.Transformer.TransformerBeam.topk", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.Transformer.TransformerBeam.topk", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.Transformer.TransformerBeam.topk", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.Transformer.TransformerBeam.topk", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.Transformer.TransformerBeam.topk", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.Transformer.TransformerBeam.topk", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.Transformer.TransformerBeam.merge_candidates", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.modeling_encoder_decoder.BeamHypotheses.add", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.Transformer.BeamSearchNode.eval", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.Transformer.BeamSearchNode.eval"], ["def", "beam_search", "(", "self", ",", "x", ")", ":", "\n", "        ", "batch_size", "=", "x", ".", "shape", "[", "0", "]", "\n", "x_len", "=", "x", ".", "shape", "[", "1", "]", "\n", "x_out", "=", "self", ".", "encoder", "(", "x", ")", "\n", "# x_out: batch x encoder_len x n_state", "\n", "\n", "sents", "=", "[", "]", "\n", "topk", "=", "1", "\n", "history_nodes", "=", "[", "{", "}", "]", "\n", "end_nodes", "=", "{", "}", "\n", "for", "idx", "in", "range", "(", "batch_size", ")", ":", "\n", "            ", "start_node", "=", "BeamSearchNode", "(", "[", "self", ".", "begin_id", "]", ",", "0", ",", "1", ")", "\n", "history_nodes", "[", "0", "]", "[", "idx", "]", "=", "[", "start_node", "]", "\n", "end_nodes", "[", "idx", "]", "=", "[", "]", "\n", "\n", "", "for", "l", "in", "range", "(", "self", ".", "max_sent_len", ")", ":", "\n", "            ", "last_nodes", "=", "history_nodes", "[", "-", "1", "]", "\n", "if", "sum", "(", "[", "len", "(", "l", ")", "for", "i", ",", "l", "in", "last_nodes", ".", "items", "(", ")", "]", ")", "==", "0", ":", "# no nodes left", "\n", "                ", "break", "\n", "", "ys", "=", "[", "]", "\n", "x_outs", "=", "[", "]", "\n", "xs", "=", "[", "]", "\n", "for", "idx", "in", "range", "(", "batch_size", ")", ":", "\n", "                ", "ys", ".", "extend", "(", "[", "node", ".", "word_ids", "for", "node", "in", "last_nodes", "[", "idx", "]", "]", ")", "\n", "x_outs", ".", "extend", "(", "\n", "[", "x_out", "[", "idx", ",", ":", ",", ":", "]", ".", "unsqueeze", "(", "0", ")", "for", "node", "in", "last_nodes", "[", "idx", "]", "]", "\n", ")", "\n", "xs", ".", "extend", "(", "[", "x", "[", "idx", ",", ":", "]", ".", "unsqueeze", "(", "0", ")", "for", "node", "in", "last_nodes", "[", "idx", "]", "]", ")", "\n", "\n", "", "ys", "=", "(", "\n", "Variable", "(", "torch", ".", "LongTensor", "(", "ys", ")", ")", ".", "cuda", "(", ")", "\n", "if", "self", ".", "use_cuda", "\n", "else", "Variable", "(", "torch", ".", "LongTensor", "(", "ys", ")", ")", "\n", ")", "# N x l", "\n", "x_outs", "=", "torch", ".", "cat", "(", "x_outs", ",", "dim", "=", "0", ")", "# N x x_len x n_state", "\n", "xs", "=", "torch", ".", "cat", "(", "xs", ",", "dim", "=", "0", ")", "# N x x_len", "\n", "probs", "=", "self", ".", "decoder", "(", "xs", ",", "x_outs", ",", "ys", ")", "\n", "log_probs", "=", "torch", ".", "log", "(", "\n", "probs", "[", ":", ",", "-", "1", ",", ":", "]", "+", "1e-15", "\n", ")", "# N x vocab_size (only take the last output)", "\n", "\n", "history_nodes", ".", "append", "(", "{", "}", ")", "\n", "p", "=", "0", "\n", "for", "idx", "in", "range", "(", "batch_size", ")", ":", "\n", "                ", "history_nodes", "[", "-", "1", "]", "[", "idx", "]", "=", "[", "]", "\n", "N", "=", "len", "(", "last_nodes", "[", "idx", "]", ")", "\n", "if", "N", "==", "0", ":", "\n", "                    ", "continue", "\n", "", "log_prob", "=", "log_probs", "[", "p", ":", "p", "+", "N", "]", "\n", "p", "+=", "N", "\n", "# log_prob = N x extended_vocab_size", "\n", "\n", "# generate", "\n", "candidates", "=", "[", "]", "\n", "for", "k", "in", "range", "(", "N", ")", ":", "\n", "                    ", "logprobs", ",", "ids", "=", "torch", ".", "topk", "(", "log_prob", "[", "k", "]", ",", "self", ".", "beam_width", ")", "\n", "candidates", "=", "self", ".", "merge_candidates", "(", "\n", "candidates", ",", "[", "(", "k", ",", "p", ",", "d", ")", "for", "p", ",", "d", "in", "zip", "(", "logprobs", ",", "ids", ")", "]", "\n", ")", "\n", "\n", "", "candidates", "=", "candidates", "[", ":", "self", ".", "beam_width", "]", "\n", "extended_nodes_in_last_nodes", "=", "set", "(", ")", "\n", "for", "k", "in", "range", "(", "len", "(", "candidates", ")", ")", ":", "\n", "                    ", "h", ",", "logp", ",", "next_word_id", "=", "candidates", "[", "\n", "k", "\n", "]", "# h means \"the h-th node in last_nodes\"", "\n", "logp", "=", "float", "(", "logp", ")", "\n", "next_word_id", "=", "int", "(", "next_word_id", ")", "\n", "prev_node", "=", "last_nodes", "[", "idx", "]", "[", "h", "]", "\n", "next_wordids", "=", "prev_node", ".", "word_ids", "+", "[", "next_word_id", "]", "\n", "next_word", "=", "self", ".", "vocab", "[", "next_word_id", "]", "\n", "\n", "next_node", "=", "BeamSearchNode", "(", "\n", "next_wordids", ",", "prev_node", ".", "log_prob", "+", "logp", ",", "prev_node", ".", "length", "+", "1", "\n", ")", "\n", "if", "next_node", ".", "duplicate", "==", "False", ":", "# no duplicate trigram generated", "\n", "                        ", "extended_nodes_in_last_nodes", ".", "add", "(", "h", ")", "\n", "if", "next_word", "==", "\"<END>\"", "or", "l", "==", "self", ".", "max_sent_len", "-", "1", ":", "\n", "                            ", "end_nodes", "[", "idx", "]", ".", "append", "(", "(", "next_node", ".", "eval", "(", ")", ",", "next_node", ")", ")", "\n", "", "else", ":", "\n", "                            ", "history_nodes", "[", "-", "1", "]", "[", "idx", "]", ".", "append", "(", "next_node", ")", "\n", "\n", "", "", "", "special_words", "=", "[", "\"<PAD>\"", ",", "\"<UNK>\"", ",", "\"<s>\"", ",", "\"</s>\"", ",", "\"<BEGIN>\"", ",", "\"<END>\"", "]", "\n", "for", "k", "in", "range", "(", "N", ")", ":", "\n", "                    ", "if", "k", "not", "in", "extended_nodes_in_last_nodes", ":", "\n", "                        ", "node", "=", "last_nodes", "[", "idx", "]", "[", "k", "]", "\n", "effective_word_count", "=", "sum", "(", "\n", "[", "\n", "1", "\n", "for", "x", "in", "node", ".", "word_ids", "\n", "if", "self", ".", "vocab", "[", "x", "]", "not", "in", "special_words", "\n", "]", "\n", ")", "\n", "if", "effective_word_count", ">=", "5", ":", "\n", "                            ", "end_nodes", "[", "idx", "]", ".", "append", "(", "(", "node", ".", "eval", "(", ")", ",", "node", ")", ")", "\n", "\n", "", "", "", "", "", "MIN_GEN_LENGTH", "=", "45", "\n", "if", "\"MIN_GEN_LENGTH\"", "in", "self", ".", "opt", ":", "\n", "            ", "MIN_GEN_LENGTH", "=", "int", "(", "self", ".", "opt", "[", "\"MIN_GEN_LENGTH\"", "]", ")", "\n", "", "for", "idx", "in", "range", "(", "batch_size", ")", ":", "\n", "            ", "t", "=", "len", "(", "[", "w", "for", "w", "in", "end_nodes", "[", "idx", "]", "if", "w", "[", "1", "]", ".", "length", ">", "MIN_GEN_LENGTH", "]", ")", "\n", "if", "t", ">", "0", ":", "\n", "                ", "end_nodes", "[", "idx", "]", "=", "[", "\n", "w", "for", "w", "in", "end_nodes", "[", "idx", "]", "if", "w", "[", "1", "]", ".", "length", ">", "MIN_GEN_LENGTH", "\n", "]", "\n", "\n", "", "end_nodes", "[", "idx", "]", ".", "sort", "(", "key", "=", "lambda", "tup", ":", "tup", "[", "0", "]", ",", "reverse", "=", "True", ")", "\n", "candidates", "=", "[", "]", "\n", "for", "score", ",", "node", "in", "end_nodes", "[", "idx", "]", "[", ":", "topk", "]", ":", "\n", "                ", "utt", "=", "[", "self", ".", "vocab", "[", "x", "]", "for", "x", "in", "node", ".", "word_ids", "]", "\n", "utt", "=", "[", "x", "for", "x", "in", "utt", "if", "x", "not", "in", "[", "\"<BEGIN>\"", ",", "\"<END>\"", "]", "]", "\n", "candidates", ".", "append", "(", "(", "utt", ",", "score", ")", ")", "\n", "", "if", "len", "(", "candidates", ")", "==", "0", ":", "\n", "                ", "candidates", ".", "append", "(", "(", "\"\"", ",", "0", ")", ")", "\n", "", "sents", ".", "append", "(", "candidates", ")", "\n", "\n", "", "return", "sents", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.Transformer.BeamSearchNode.__init__": [[820, 840], ["set", "range", "len", "set.add", "str", "str", "str"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.modeling_encoder_decoder.BeamHypotheses.add"], ["    ", "def", "__init__", "(", "self", ",", "word_ids", ",", "log_prob", ",", "length", ")", ":", "\n", "        ", "self", ".", "word_ids", "=", "word_ids", "\n", "self", ".", "log_prob", "=", "log_prob", "\n", "self", ".", "length", "=", "length", "\n", "\n", "trigram_set", "=", "set", "(", ")", "\n", "self", ".", "duplicate", "=", "False", "\n", "\n", "for", "i", "in", "range", "(", "2", ",", "len", "(", "word_ids", ")", ")", ":", "\n", "            ", "trigram", "=", "(", "\n", "str", "(", "word_ids", "[", "i", "-", "2", "]", ")", "\n", "+", "\" \"", "\n", "+", "str", "(", "word_ids", "[", "i", "-", "1", "]", ")", "\n", "+", "\" \"", "\n", "+", "str", "(", "word_ids", "[", "i", "]", ")", "\n", ")", "\n", "if", "trigram", "in", "trigram_set", ":", "\n", "                ", "self", ".", "duplicate", "=", "True", "\n", "break", "\n", "", "trigram_set", ".", "add", "(", "trigram", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.Transformer.BeamSearchNode.eval": [[841, 843], ["float"], "methods", ["None"], ["", "", "def", "eval", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "log_prob", "/", "float", "(", "self", ".", "length", "-", "1.0", "+", "1e-6", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.Transformer.BeamSearchNode.__lt__": [[844, 846], ["None"], "methods", ["None"], ["", "def", "__lt__", "(", "self", ",", "other", ")", ":", "\n", "        ", "return", "self", ".", "length", "<", "other", ".", "length", "\n", "", "", ""]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.Transformer.gelu": [[17, 22], ["torch.tanh", "torch.tanh", "torch.tanh", "math.sqrt", "torch.pow", "torch.pow", "torch.pow"], "function", ["None"], ["def", "gelu", "(", "x", ")", ":", "\n", "    ", "return", "(", "\n", "0.5", "\n", "*", "x", "\n", "*", "(", "1", "+", "torch", ".", "tanh", "(", "math", ".", "sqrt", "(", "2", "/", "math", ".", "pi", ")", "*", "(", "x", "+", "0.044715", "*", "torch", ".", "pow", "(", "x", ",", "3", ")", ")", ")", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.Transformer.swish": [[25, 27], ["torch.sigmoid", "torch.sigmoid", "torch.sigmoid"], "function", ["None"], ["", "def", "swish", "(", "x", ")", ":", "\n", "    ", "return", "x", "*", "torch", ".", "sigmoid", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.MeetingNet_Transformer.MeetingNet_Transformer.__init__": [[173, 249], ["torch.Module.__init__", "getattr", "os.path.join", "MeetingNet_Transformer.MeetingNet_Transformer.tokenizer_class.from_pretrained", "int", "int", "int", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.init.normal_", "torch.init.normal_", "torch.init.normal_", "torch.init.normal_", "summertime.model.third_party.HMNet.Models.Networks.Transformer.Embedder", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.Embedding", "MeetingNet_Transformer.Encoder", "MeetingNet_Transformer.Decoder", "os.path.isdir", "print", "MeetingNet_Transformer.MeetingNet_Transformer.from_pretrained", "getattr", "setattr", "MeetingNet_Transformer.MeetingNet_Transformer.tokenizer.convert_ids_to_tokens", "os.path.join", "MeetingNet_Transformer.MeetingNet_Transformer.tokenizer.convert_ids_to_tokens", "len", "len", "len"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.tests.model_selector_test.ToyModel.__init__", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.MeetingNet_Transformer.MeetingNet_Transformer.from_pretrained", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.MeetingNet_Transformer.MeetingNet_Transformer.from_pretrained", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizer.convert_ids_to_tokens", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizer.convert_ids_to_tokens"], ["    ", "def", "__init__", "(", "self", ",", "opt", ")", ":", "\n", "        ", "super", "(", "MeetingNet_Transformer", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "opt", "=", "opt", "\n", "self", ".", "use_cuda", "=", "self", ".", "opt", "[", "\"cuda\"", "]", "==", "True", "\n", "self", ".", "config", "=", "{", "}", "\n", "\n", "# load tokenizer", "\n", "self", ".", "tokenizer_class", "=", "getattr", "(", "tokenization_transfo_xl", ",", "opt", "[", "\"PRE_TOKENIZER\"", "]", ")", "\n", "self", ".", "pretrained_tokenizer_path", "=", "os", ".", "path", ".", "join", "(", "\n", "opt", "[", "\"datadir\"", "]", ",", "opt", "[", "\"PRE_TOKENIZER_PATH\"", "]", "\n", ")", "\n", "if", "not", "os", ".", "path", ".", "isdir", "(", "self", ".", "pretrained_tokenizer_path", ")", ":", "\n", "            ", "\"\"\"\n            This if-else statement makes sure the pre-trained tokenizer exists\n            If it does not exist, it assumes the input string is the HuggingFace tokenizer name,\n            and downloads it from their website.\n            \"\"\"", "\n", "self", ".", "pretrained_tokenizer_path", "=", "opt", "[", "\"PRE_TOKENIZER_PATH\"", "]", "\n", "", "else", ":", "\n", "            ", "print", "(", "\"Loading Tokenizer from {}...\"", ".", "format", "(", "self", ".", "pretrained_tokenizer_path", ")", ")", "\n", "\n", "# here is a simple workaround to make sure all special tokens are not None", "\n", "", "self", ".", "tokenizer", "=", "self", ".", "tokenizer_class", ".", "from_pretrained", "(", "\n", "self", ".", "pretrained_tokenizer_path", "\n", ")", "\n", "special_tokens_tuple_list", "=", "[", "\n", "(", "\"eos_token\"", ",", "128", ")", ",", "\n", "(", "\"unk_token\"", ",", "129", ")", ",", "\n", "(", "\"pad_token\"", ",", "130", ")", ",", "\n", "(", "\"bos_token\"", ",", "131", ")", ",", "\n", "]", "\n", "\n", "for", "special_token_name", ",", "special_token_id_offset", "in", "special_tokens_tuple_list", ":", "\n", "            ", "if", "getattr", "(", "self", ".", "tokenizer", ",", "special_token_name", ")", "==", "None", ":", "\n", "                ", "setattr", "(", "\n", "self", ".", "tokenizer", ",", "\n", "special_token_name", ",", "\n", "self", ".", "tokenizer", ".", "convert_ids_to_tokens", "(", "\n", "len", "(", "self", ".", "tokenizer", ")", "-", "special_token_id_offset", "\n", ")", ",", "\n", ")", "\n", "self", ".", "config", "[", "special_token_name", "]", "=", "self", ".", "tokenizer", ".", "convert_ids_to_tokens", "(", "\n", "len", "(", "self", ".", "tokenizer", ")", "-", "special_token_id_offset", "\n", ")", "\n", "self", ".", "config", "[", "special_token_name", "+", "\"_id\"", "]", "=", "(", "\n", "len", "(", "self", ".", "tokenizer", ")", "-", "special_token_id_offset", "\n", ")", "\n", "\n", "", "", "self", ".", "vocab_size", "=", "self", ".", "tokenizer", ".", "vocab_size", "\n", "opt", "[", "\"vocab_size\"", "]", "=", "self", ".", "vocab_size", "\n", "self", ".", "role_size", "=", "int", "(", "opt", "[", "\"ROLE_SIZE\"", "]", ")", "\n", "vocab_dim", "=", "int", "(", "opt", "[", "\"VOCAB_DIM\"", "]", ")", "\n", "role_dim", "=", "int", "(", "opt", "[", "\"ROLE_DIM\"", "]", ")", "\n", "opt", "[", "\"transformer_embed_dim\"", "]", "=", "vocab_dim", "\n", "embed", "=", "nn", ".", "Embedding", "(", "\n", "self", ".", "vocab_size", ",", "vocab_dim", ",", "padding_idx", "=", "self", ".", "tokenizer", ".", "pad_token_id", "\n", ")", "\n", "nn", ".", "init", ".", "normal_", "(", "embed", ".", "weight", ",", "std", "=", "0.02", ")", "\n", "embedder", "=", "Embedder", "(", "opt", ",", "embed", ")", "\n", "role_embed", "=", "nn", ".", "Embedding", "(", "self", ".", "role_size", ",", "role_dim", ",", "padding_idx", "=", "0", ")", "\n", "\n", "self", ".", "encoder", "=", "Encoder", "(", "\n", "opt", ",", "self", ".", "vocab_size", ",", "vocab_dim", ",", "role_dim", ",", "embedder", ",", "role_embed", "\n", ")", "\n", "self", ".", "decoder", "=", "Decoder", "(", "\n", "opt", ",", "\n", "vocab_dim", ",", "\n", "self", ".", "vocab_size", ",", "\n", "embedder", ",", "\n", "self", ".", "encoder", ".", "token_transformer_dim", ",", "\n", "self", ".", "encoder", ".", "sent_transformer_dim", ",", "\n", ")", "\n", "\n", "if", "\"PYLEARN_MODEL\"", "in", "self", ".", "opt", ":", "\n", "            ", "self", ".", "from_pretrained", "(", "os", ".", "path", ".", "join", "(", "opt", "[", "\"datadir\"", "]", ",", "opt", "[", "\"PYLEARN_MODEL\"", "]", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.MeetingNet_Transformer.MeetingNet_Transformer.save_pretrained": [[250, 257], ["dict", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "os.path.join", "MeetingNet_Transformer.MeetingNet_Transformer.state_dict().items", "MeetingNet_Transformer.MeetingNet_Transformer.state_dict"], "methods", ["None"], ["", "", "def", "save_pretrained", "(", "self", ",", "save_dir", ")", ":", "\n", "        ", "network_state", "=", "dict", "(", "[", "(", "k", ",", "v", ")", "for", "k", ",", "v", "in", "self", ".", "state_dict", "(", ")", ".", "items", "(", ")", "]", ")", "\n", "params", "=", "{", "\n", "\"state_dict\"", ":", "{", "\"network\"", ":", "network_state", "}", ",", "\n", "\"config\"", ":", "self", ".", "opt", ",", "\n", "}", "\n", "torch", ".", "save", "(", "params", ",", "os", ".", "path", ".", "join", "(", "save_dir", ",", "\"model.pt\"", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.MeetingNet_Transformer.MeetingNet_Transformer.from_pretrained": [[258, 270], ["torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "MeetingNet_Transformer.MeetingNet_Transformer.load_state_dict", "os.path.join", "torch.device", "torch.device", "torch.device", "torch.device", "torch.device", "torch.device", "torch.device", "torch.device", "torch.device", "torch.device", "torch.device", "torch.device", "torch.device", "torch.device", "torch.device", "torch.device"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.BaseTrainer.BaseTrainer.load", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.BaseTrainer.BaseTrainer.load", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.BaseTrainer.BaseTrainer.load", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.BaseTrainer.BaseTrainer.load", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.BaseTrainer.BaseTrainer.load", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.BaseTrainer.BaseTrainer.load", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.BaseTrainer.BaseTrainer.load", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.BaseTrainer.BaseTrainer.load", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.BaseTrainer.BaseTrainer.load", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.BaseTrainer.BaseTrainer.load", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.BaseTrainer.BaseTrainer.load", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.BaseTrainer.BaseTrainer.load", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.BaseTrainer.BaseTrainer.load", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.BaseTrainer.BaseTrainer.load", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.BaseTrainer.BaseTrainer.load", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.BaseTrainer.BaseTrainer.load"], ["", "def", "from_pretrained", "(", "self", ",", "load_dir", ")", ":", "\n", "        ", "checkpoint", "=", "torch", ".", "load", "(", "\n", "os", ".", "path", ".", "join", "(", "load_dir", ",", "\"model.pt\"", ")", ",", "\n", "map_location", "=", "torch", ".", "device", "(", "\"cuda\"", ",", "self", ".", "opt", "[", "\"local_rank\"", "]", ")", "\n", "if", "self", ".", "use_cuda", "\n", "else", "\"cpu\"", ",", "\n", ")", "\n", "state_dict", "=", "checkpoint", "[", "\"state_dict\"", "]", "\n", "\n", "self", ".", "load_state_dict", "(", "state_dict", "[", "\"network\"", "]", ")", "\n", "\n", "return", "self", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.MeetingNet_Transformer.MeetingNet_Transformer.get_training_parameters": [[271, 273], ["MeetingNet_Transformer.MeetingNet_Transformer.parameters"], "methods", ["None"], ["", "def", "get_training_parameters", "(", "self", ")", ":", "\n", "        ", "return", "[", "p", "for", "p", "in", "self", ".", "parameters", "(", ")", "if", "p", ".", "requires_grad", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.MeetingNet_Transformer.MeetingNet_Transformer.forward": [[274, 285], ["MeetingNet_Transformer.MeetingNet_Transformer._forward", "MeetingNet_Transformer.MeetingNet_Transformer.generate"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.MeetingNet_Transformer.MeetingNet_Transformer._forward", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.MeetingNet_Transformer.MeetingNet_Transformer.generate"], ["", "def", "forward", "(", "self", ",", "batch", ",", "beam_search", "=", "False", ",", "max_sent_len", "=", "None", ")", ":", "\n", "        ", "if", "beam_search", ":", "\n", "# return self.beam_search(batch, max_sent_len)", "\n", "            ", "return", "self", ".", "generate", "(", "batch", ",", "max_sent_len", ")", "\n", "\n", "", "outputs", "=", "self", ".", "_forward", "(", "**", "batch", ")", "\n", "vocab_logprob", "=", "outputs", "[", "0", "]", "\n", "\n", "# assume all encoder-decoder model input has BOS and EOS", "\n", "# otherwise the loss will be ill-defined", "\n", "return", "vocab_logprob", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.MeetingNet_Transformer.MeetingNet_Transformer._forward": [[297, 312], ["kwargs.pop", "kwargs.pop", "kwargs.pop", "kwargs.pop", "kwargs.pop", "MeetingNet_Transformer.MeetingNet_Transformer.encoder", "MeetingNet_Transformer.MeetingNet_Transformer.decoder"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizerFast.decoder"], ["def", "_forward", "(", "self", ",", "**", "kwargs", ")", ":", "\n", "\n", "        ", "encoder_input_ids", "=", "kwargs", ".", "pop", "(", "\"encoder_input_ids\"", ")", "\n", "encoder_input_roles", "=", "kwargs", ".", "pop", "(", "\"encoder_input_roles\"", ")", "\n", "encoder_input_pos", "=", "kwargs", ".", "pop", "(", "\"encoder_input_pos\"", ")", "\n", "encoder_input_ent", "=", "kwargs", ".", "pop", "(", "\"encoder_input_ent\"", ")", "\n", "decoder_input_ids", "=", "kwargs", ".", "pop", "(", "\"decoder_input_ids\"", ")", "\n", "\n", "token_encoder_outputs", ",", "sent_encoder_outputs", "=", "self", ".", "encoder", "(", "\n", "encoder_input_ids", ",", "encoder_input_roles", ",", "encoder_input_pos", ",", "encoder_input_ent", "\n", ")", "\n", "vocab_logprob", "=", "self", ".", "decoder", "(", "\n", "token_encoder_outputs", ",", "sent_encoder_outputs", ",", "decoder_input_ids", "\n", ")", "\n", "return", "vocab_logprob", ",", "(", "token_encoder_outputs", ",", "sent_encoder_outputs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.MeetingNet_Transformer.MeetingNet_Transformer.generate": [[313, 359], ["MeetingNet_Transformer.MeetingNet_Transformer.eval", "int", "MeetingNet_Transformer.MeetingNet_Transformer.opt.get", "MeetingNet_Transformer.MeetingNet_Transformer._generate", "outputs.view.view.view", "range", "sents.append", "MeetingNet_Transformer.MeetingNet_Transformer.opt.get", "MeetingNet_Transformer.MeetingNet_Transformer.opt.get", "MeetingNet_Transformer.MeetingNet_Transformer.opt.get", "MeetingNet_Transformer.MeetingNet_Transformer.opt.get", "MeetingNet_Transformer.MeetingNet_Transformer.opt.get", "MeetingNet_Transformer.MeetingNet_Transformer.opt.get", "MeetingNet_Transformer.MeetingNet_Transformer.opt.get", "MeetingNet_Transformer.MeetingNet_Transformer.tokenizer.convert_ids_to_tokens", "range"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.Transformer.BeamSearchNode.eval", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.closablequeue.ClosableQueue.get", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.iterators.FixedBatchIterator._generate", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.closablequeue.ClosableQueue.get", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.closablequeue.ClosableQueue.get", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.closablequeue.ClosableQueue.get", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.closablequeue.ClosableQueue.get", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.closablequeue.ClosableQueue.get", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.closablequeue.ClosableQueue.get", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.closablequeue.ClosableQueue.get", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizer.convert_ids_to_tokens"], ["", "def", "generate", "(", "self", ",", "batch", ",", "max_sent_len", ")", ":", "\n", "        ", "self", ".", "eval", "(", ")", "\n", "self", ".", "beam_width", "=", "int", "(", "self", ".", "opt", "[", "\"BEAM_WIDTH\"", "]", ")", "\n", "\n", "input_ids", "=", "batch", "[", "\"encoder_input_ids\"", "]", "\n", "input_roles", "=", "batch", "[", "\"encoder_input_roles\"", "]", "\n", "input_pos", "=", "batch", "[", "\"encoder_input_pos\"", "]", "\n", "input_ent", "=", "batch", "[", "\"encoder_input_ent\"", "]", "\n", "\n", "batch_size", "=", "input_ids", ".", "shape", "[", "0", "]", "\n", "\n", "num_return_sequences", "=", "self", ".", "opt", ".", "get", "(", "\"NUM_RETURN_SEQUENCES\"", ",", "1", ")", "\n", "outputs", "=", "self", ".", "_generate", "(", "\n", "input_ids", "=", "input_ids", ",", "\n", "input_roles", "=", "input_roles", ",", "\n", "input_pos", "=", "input_pos", ",", "\n", "input_ent", "=", "input_ent", ",", "\n", "min_length", "=", "self", ".", "opt", ".", "get", "(", "\"MIN_GEN_LENGTH\"", ",", "None", ")", ",", "\n", "max_length", "=", "max_sent_len", ",", "\n", "num_beams", "=", "self", ".", "beam_width", ",", "\n", "bad_words_ids", "=", "None", ",", "\n", "bos_token_id", "=", "self", ".", "tokenizer", ".", "bos_token_id", ",", "\n", "decoder_start_token_id", "=", "self", ".", "tokenizer", ".", "bos_token_id", ",", "\n", "eos_token_id", "=", "self", ".", "tokenizer", ".", "eos_token_id", ",", "\n", "pad_token_id", "=", "self", ".", "tokenizer", ".", "pad_token_id", ",", "\n", "do_sample", "=", "self", ".", "opt", ".", "get", "(", "\"DO_SAMPLE\"", ",", "False", ")", ",", "\n", "top_k", "=", "self", ".", "opt", ".", "get", "(", "\"TOP_K\"", ",", "50", ")", ",", "\n", "top_p", "=", "self", ".", "opt", ".", "get", "(", "\"TOP_P\"", ",", "1", ")", ",", "\n", "repetition_penalty", "=", "self", ".", "opt", ".", "get", "(", "\"REPETITION_PENALTY\"", ",", "1.0", ")", ",", "\n", "length_penalty", "=", "self", ".", "opt", ".", "get", "(", "\"LENGTH_PENALTY\"", ",", "1.0", ")", ",", "\n", "no_repeat_ngram_size", "=", "self", ".", "opt", ".", "get", "(", "\"NO_REPEAT_NGRAM_SIZE\"", ",", "3", ")", ",", "\n", "num_return_sequences", "=", "num_return_sequences", ",", "\n", ")", "\n", "\n", "sents", "=", "[", "]", "\n", "outputs", "=", "outputs", ".", "view", "(", "outputs", ".", "shape", "[", "0", "]", ",", "num_return_sequences", ",", "-", "1", ")", "\n", "\n", "for", "idx", "in", "range", "(", "batch_size", ")", ":", "\n", "# TODO: use real inference scores", "\n", "            ", "candidates", "=", "[", "\n", "(", "self", ".", "tokenizer", ".", "convert_ids_to_tokens", "(", "outputs", "[", "idx", ",", "i", ",", ":", "]", ")", ",", "0.0", ")", "\n", "for", "i", "in", "range", "(", "num_return_sequences", ")", "\n", "]", "\n", "sents", ".", "append", "(", "candidates", ")", "\n", "\n", "", "return", "sents", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.MeetingNet_Transformer.MeetingNet_Transformer.prepare_inputs_for_generation": [[360, 373], ["type"], "methods", ["None"], ["", "def", "prepare_inputs_for_generation", "(", "self", ",", "input_ids", ",", "past", ",", "attention_mask", ",", "**", "kwargs", ")", ":", "\n", "        ", "assert", "past", "is", "not", "None", ",", "\"past has to be defined for encoder_outputs\"", "\n", "\n", "# first step", "\n", "if", "type", "(", "past", ")", "is", "tuple", ":", "\n", "            ", "encoder_outputs", "=", "past", "\n", "", "else", ":", "\n", "            ", "encoder_outputs", "=", "(", "past", ",", ")", "\n", "\n", "", "return", "{", "\n", "\"decoder_input_ids\"", ":", "input_ids", ",", "\n", "\"token_encoder_outputs\"", ":", "encoder_outputs", "[", "0", "]", ",", "\n", "\"sent_encoder_outputs\"", ":", "encoder_outputs", "[", "1", "]", ",", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.MeetingNet_Transformer.MeetingNet_Transformer.prepare_scores_for_generation": [[375, 377], ["None"], "methods", ["None"], ["", "def", "prepare_scores_for_generation", "(", "self", ",", "scores", ",", "**", "kwargs", ")", ":", "\n", "        ", "return", "scores", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.MeetingNet_Transformer.MeetingNet_Transformer.enforce_repetition_penalty_": [[378, 389], ["range", "set", "prev_output_tokens[].tolist"], "methods", ["None"], ["", "def", "enforce_repetition_penalty_", "(", "\n", "self", ",", "lprobs", ",", "batch_size", ",", "num_beams", ",", "prev_output_tokens", ",", "repetition_penalty", "\n", ")", ":", "\n", "        ", "\"\"\"repetition penalty (from CTRL paper https://arxiv.org/abs/1909.05858).\"\"\"", "\n", "for", "i", "in", "range", "(", "batch_size", "*", "num_beams", ")", ":", "\n", "            ", "for", "previous_token", "in", "set", "(", "prev_output_tokens", "[", "i", "]", ".", "tolist", "(", ")", ")", ":", "\n", "# if score < 0 then repetition penalty has to multiplied to reduce the previous token probability", "\n", "                ", "if", "lprobs", "[", "i", ",", "previous_token", "]", "<", "0", ":", "\n", "                    ", "lprobs", "[", "i", ",", "previous_token", "]", "*=", "repetition_penalty", "\n", "", "else", ":", "\n", "                    ", "lprobs", "[", "i", ",", "previous_token", "]", "/=", "repetition_penalty", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.MeetingNet_Transformer.MeetingNet_Transformer._generate": [[390, 785], ["torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "isinstance", "isinstance", "MeetingNet_Transformer.MeetingNet_Transformer.encoder", "torch.full", "torch.full", "torch.full", "torch.full", "torch.full", "torch.full", "torch.full", "torch.full", "torch.full", "torch.full", "torch.full", "torch.full", "torch.full", "torch.full", "torch.full", "torch.full", "torch.arange().view().repeat().view().to", "torch.arange().view().repeat().view().to", "torch.arange().view().repeat().view().to", "torch.arange().view().repeat().view().to", "torch.arange().view().repeat().view().to", "torch.arange().view().repeat().view().to", "torch.arange().view().repeat().view().to", "torch.arange().view().repeat().view().to", "torch.arange().view().repeat().view().to", "torch.arange().view().repeat().view().to", "torch.arange().view().repeat().view().to", "torch.arange().view().repeat().view().to", "torch.arange().view().repeat().view().to", "torch.arange().view().repeat().view().to", "torch.arange().view().repeat().view().to", "torch.arange().view().repeat().view().to", "isinstance", "isinstance", "isinstance", "isinstance", "isinstance", "isinstance", "torch.full", "torch.full", "torch.full", "torch.full", "torch.full", "torch.full", "torch.full", "torch.full", "torch.full", "torch.full", "torch.full", "torch.full", "torch.full", "torch.full", "torch.full", "torch.full", "torch.full.ne().long", "torch.full.ne().long", "torch.full.ne().long", "torch.full.ne().long", "logger.warning", "encoder_outputs[].index_select", "encoder_outputs[].index_select", "MeetingNet_Transformer.MeetingNet_Transformer._generate_beam_search", "MeetingNet_Transformer.MeetingNet_Transformer._generate_no_beam_search", "isinstance", "isinstance", "isinstance", "isinstance", "isinstance", "isinstance", "torch.full.dim", "torch.full.dim", "torch.full.dim", "torch.full.dim", "torch.full.new_ones", "torch.full.new_ones", "torch.full.new_ones", "torch.full.new_ones", "torch.arange().view().repeat().view", "torch.arange().view().repeat().view", "torch.arange().view().repeat().view", "torch.arange().view().repeat().view", "torch.arange().view().repeat().view", "torch.arange().view().repeat().view", "torch.arange().view().repeat().view", "torch.arange().view().repeat().view", "torch.arange().view().repeat().view", "torch.arange().view().repeat().view", "torch.arange().view().repeat().view", "torch.arange().view().repeat().view", "torch.arange().view().repeat().view", "torch.arange().view().repeat().view", "torch.arange().view().repeat().view", "torch.arange().view().repeat().view", "torch.full.ne", "torch.full.ne", "torch.full.ne", "torch.full.ne", "next", "next", "MeetingNet_Transformer.MeetingNet_Transformer.parameters", "torch.arange().view().repeat", "torch.arange().view().repeat", "torch.arange().view().repeat", "torch.arange().view().repeat", "torch.arange().view().repeat", "torch.arange().view().repeat", "torch.arange().view().repeat", "torch.arange().view().repeat", "torch.arange().view().repeat", "torch.arange().view().repeat", "torch.arange().view().repeat", "torch.arange().view().repeat", "torch.arange().view().repeat", "torch.arange().view().repeat", "torch.arange().view().repeat", "torch.arange().view().repeat", "MeetingNet_Transformer.MeetingNet_Transformer.parameters", "torch.arange().view", "torch.arange().view", "torch.arange().view", "torch.arange().view", "torch.arange().view", "torch.arange().view", "torch.arange().view", "torch.arange().view", "torch.arange().view", "torch.arange().view", "torch.arange().view", "torch.arange().view", "torch.arange().view", "torch.arange().view", "torch.arange().view", "torch.arange().view", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.MeetingNet_Transformer.MeetingNet_Transformer._generate_beam_search", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.MeetingNet_Transformer.MeetingNet_Transformer._generate_no_beam_search"], ["", "", "", "", "@", "torch", ".", "no_grad", "(", ")", "\n", "def", "_generate", "(", "\n", "self", ",", "\n", "input_ids", "=", "None", ",", "\n", "input_roles", "=", "None", ",", "\n", "input_pos", "=", "None", ",", "\n", "input_ent", "=", "None", ",", "\n", "max_length", "=", "None", ",", "\n", "min_length", "=", "None", ",", "\n", "do_sample", "=", "None", ",", "\n", "early_stopping", "=", "False", ",", "\n", "num_beams", "=", "None", ",", "\n", "temperature", "=", "1.0", ",", "\n", "top_k", "=", "None", ",", "\n", "top_p", "=", "None", ",", "\n", "repetition_penalty", "=", "None", ",", "\n", "bad_words_ids", "=", "None", ",", "\n", "bos_token_id", "=", "None", ",", "\n", "pad_token_id", "=", "None", ",", "\n", "eos_token_id", "=", "None", ",", "\n", "length_penalty", "=", "None", ",", "\n", "no_repeat_ngram_size", "=", "None", ",", "\n", "num_return_sequences", "=", "None", ",", "\n", "attention_mask", "=", "None", ",", "\n", "decoder_start_token_id", "=", "None", ",", "\n", ")", ":", "\n", "        ", "r\"\"\"Generates sequences for models with a LM head. The method currently supports greedy decoding, beam-search decoding, sampling with temperature, sampling with top-k or nucleus sampling.\n\n        Adapted in part from `Facebook's XLM beam search code`_.\n\n        .. _`Facebook's XLM beam search code`:\n           https://github.com/facebookresearch/XLM/blob/9e6f6814d17be4fe5b15f2e6c43eb2b2d76daeb4/src/model/transformer.py#L529\n\n\n        Parameters:\n\n            input_ids: (`optional`) `torch.LongTensor` of shape `(batch_size, sequence_length)`\n                The sequence used as a prompt for the generation. If `None` the method initializes\n                it as an empty `torch.LongTensor` of shape `(1,)`.\n\n            max_length: (`optional`) int\n                The max length of the sequence to be generated.  Between `min_length` and infinity. Default to 20.\n\n            min_length: (`optional`) int\n                The min length of the sequence to be generated.  Between 0 and infinity. Default to 0.\n\n            do_sample: (`optional`) bool\n                If set to `False` greedy decoding is used. Otherwise sampling is used. Defaults to `False` as defined in `configuration_utils.PretrainedConfig`.\n\n            early_stopping: (`optional`) bool\n                if set to `True` beam search is stopped when at least `num_beams` sentences finished per batch. Defaults to `False` as defined in `configuration_utils.PretrainedConfig`.\n\n            num_beams: (`optional`) int\n                Number of beams for beam search. Must be between 1 and infinity. 1 means no beam search. Default to 1.\n\n            temperature: (`optional`) float\n                The value used to module the next token probabilities. Must be strictly positive. Default to 1.0.\n\n            top_k: (`optional`) int\n                The number of highest probability vocabulary tokens to keep for top-k-filtering. Between 1 and infinity. Default to 50.\n\n            top_p: (`optional`) float\n                The cumulative probability of parameter highest probability vocabulary tokens to keep for nucleus sampling. Must be between 0 and 1. Default to 1.\n\n            repetition_penalty: (`optional`) float\n                The parameter for repetition penalty. Between 1.0 and infinity. 1.0 means no penalty. Default to 1.0.\n\n            pad_token_id: (`optional`) int\n                Padding token. Default to specicic model pad_token_id or None if it does not exist.\n\n            bos_token_id: (`optional`) int\n                BOS token. Defaults to `bos_token_id` as defined in the models config.\n\n            eos_token_id: (`optional`) int\n                EOS token. Defaults to `eos_token_id` as defined in the models config.\n\n            length_penalty: (`optional`) float\n                Exponential penalty to the length. Default to 1.\n\n            no_repeat_ngram_size: (`optional`) int\n                If set to int > 0, all ngrams of size `no_repeat_ngram_size` can only occur once.\n            bad_words_ids: (`optional`) list of lists of int\n                `bad_words_ids` contains tokens that are not allowed to be generated. In order to get the tokens of the words that should not appear in the generated text, use `tokenizer.encode(bad_word, add_prefix_space=True)`.\n\n            num_return_sequences: (`optional`) int\n                The number of independently computed returned sequences for each element in the batch. Default to 1.\n\n            attention_mask (`optional`) obj: `torch.LongTensor` of same shape as `input_ids`\n                Mask to avoid performing attention on padding token indices.\n                Mask values selected in ``[0, 1]``:\n                ``1`` for tokens that are NOT MASKED, ``0`` for MASKED tokens.\n                Defaults to `None`.\n\n            `What are attention masks? <../glossary.html#attention-mask>`__\n\n            decoder_start_token_id=None: (`optional`) int\n                If an encoder-decoder model starts decoding with a different token than BOS.\n                Defaults to `None` and is changed to `BOS` later.\n\n        Return:\n\n            output: `torch.LongTensor` of shape `(batch_size * num_return_sequences, sequence_length)`\n                sequence_length is either equal to max_length or shorter if all batches finished early due to the `eos_token_id`\n\n        Examples::\n\n            tokenizer = AutoTokenizer.from_pretrained('distilgpt2')   # Initialize tokenizer\n            model = AutoModelWithLMHead.from_pretrained('distilgpt2')    # Download model and configuration from S3 and cache.\n            outputs = model.generate(max_length=40)  # do greedy decoding\n            print('Generated: {}'.format(tokenizer.decode(outputs[0], skip_special_tokens=True)))\n\n            tokenizer = AutoTokenizer.from_pretrained('openai-gpt')   # Initialize tokenizer\n            model = AutoModelWithLMHead.from_pretrained('openai-gpt')    # Download model and configuration from S3 and cache.\n            input_context = 'The dog'\n            input_ids = tokenizer.encode(input_context, return_tensors='pt')  # encode input context\n            outputs = model.generate(input_ids=input_ids, num_beams=5, num_return_sequences=3, temperature=1.5)  # generate 3 independent sequences using beam search decoding (5 beams) with sampling from initial context 'The dog'\n            for i in range(3): #  3 output sequences were generated\n                print('Generated {}: {}'.format(i, tokenizer.decode(outputs[i], skip_special_tokens=True)))\n\n            tokenizer = AutoTokenizer.from_pretrained('distilgpt2')   # Initialize tokenizer\n            model = AutoModelWithLMHead.from_pretrained('distilgpt2')    # Download model and configuration from S3 and cache.\n            input_context = 'The dog'\n            input_ids = tokenizer.encode(input_context, return_tensors='pt')  # encode input context\n            outputs = model.generate(input_ids=input_ids, max_length=40, temperature=0.7, num_return_sequences=3)  # 3 generate sequences using by sampling\n            for i in range(3): #  3 output sequences were generated\n                print('Generated {}: {}'.format(i, tokenizer.decode(outputs[i], skip_special_tokens=True)))\n\n            tokenizer = AutoTokenizer.from_pretrained('ctrl')   # Initialize tokenizer\n            model = AutoModelWithLMHead.from_pretrained('ctrl')    # Download model and configuration from S3 and cache.\n            input_context = 'Legal My neighbor is'  # \"Legal\" is one of the control codes for ctrl\n            input_ids = tokenizer.encode(input_context, return_tensors='pt')  # encode input context\n            outputs = model.generate(input_ids=input_ids, max_length=50, temperature=0.7, repetition_penalty=1.2)  # generate sequences\n            print('Generated: {}'.format(tokenizer.decode(outputs[0], skip_special_tokens=True)))\n\n            tokenizer = AutoTokenizer.from_pretrained('gpt2')   # Initialize tokenizer\n            model = AutoModelWithLMHead.from_pretrained('gpt2')    # Download model and configuration from S3 and cache.\n            input_context = 'My cute dog'  # \"Legal\" is one of the control codes for ctrl\n            bad_words_ids = [tokenizer.encode(bad_word, add_prefix_space=True) for bad_word in ['idiot', 'stupid', 'shut up']]\n            input_ids = tokenizer.encode(input_context, return_tensors='pt')  # encode input context\n            outputs = model.generate(input_ids=input_ids, max_length=100, do_sample=True, bad_words_ids=bad_words_ids)  # generate sequences without allowing bad_words to be generated\n        \"\"\"", "\n", "\n", "max_length", "=", "max_length", "if", "max_length", "is", "not", "None", "else", "self", ".", "config", ".", "max_length", "\n", "min_length", "=", "min_length", "if", "min_length", "is", "not", "None", "else", "self", ".", "config", ".", "min_length", "\n", "do_sample", "=", "do_sample", "if", "do_sample", "is", "not", "None", "else", "self", ".", "config", ".", "do_sample", "\n", "early_stopping", "=", "(", "\n", "early_stopping", "if", "early_stopping", "is", "not", "None", "else", "self", ".", "config", ".", "early_stopping", "\n", ")", "\n", "num_beams", "=", "num_beams", "if", "num_beams", "is", "not", "None", "else", "self", ".", "config", ".", "num_beams", "\n", "temperature", "=", "(", "\n", "temperature", "if", "temperature", "is", "not", "None", "else", "self", ".", "config", ".", "temperature", "\n", ")", "\n", "top_k", "=", "top_k", "if", "top_k", "is", "not", "None", "else", "self", ".", "config", ".", "top_k", "\n", "top_p", "=", "top_p", "if", "top_p", "is", "not", "None", "else", "self", ".", "config", ".", "top_p", "\n", "repetition_penalty", "=", "(", "\n", "repetition_penalty", "\n", "if", "repetition_penalty", "is", "not", "None", "\n", "else", "self", ".", "config", ".", "repetition_penalty", "\n", ")", "\n", "bos_token_id", "=", "(", "\n", "bos_token_id", "if", "bos_token_id", "is", "not", "None", "else", "self", ".", "config", ".", "bos_token_id", "\n", ")", "\n", "pad_token_id", "=", "(", "\n", "pad_token_id", "if", "pad_token_id", "is", "not", "None", "else", "self", ".", "config", ".", "pad_token_id", "\n", ")", "\n", "eos_token_id", "=", "(", "\n", "eos_token_id", "if", "eos_token_id", "is", "not", "None", "else", "self", ".", "config", ".", "eos_token_id", "\n", ")", "\n", "length_penalty", "=", "(", "\n", "length_penalty", "if", "length_penalty", "is", "not", "None", "else", "self", ".", "config", ".", "length_penalty", "\n", ")", "\n", "no_repeat_ngram_size", "=", "(", "\n", "no_repeat_ngram_size", "\n", "if", "no_repeat_ngram_size", "is", "not", "None", "\n", "else", "self", ".", "config", ".", "no_repeat_ngram_size", "\n", ")", "\n", "bad_words_ids", "=", "bad_words_ids", "\n", "num_return_sequences", "=", "(", "\n", "num_return_sequences", "\n", "if", "num_return_sequences", "is", "not", "None", "\n", "else", "self", ".", "config", ".", "num_return_sequences", "\n", ")", "\n", "decoder_start_token_id", "=", "(", "\n", "decoder_start_token_id", "\n", "if", "decoder_start_token_id", "is", "not", "None", "\n", "else", "self", ".", "config", ".", "decoder_start_token_id", "\n", ")", "\n", "\n", "if", "input_ids", "is", "not", "None", ":", "\n", "            ", "batch_size", "=", "input_ids", ".", "shape", "[", "0", "]", "# overriden by the input batch_size", "\n", "", "else", ":", "\n", "            ", "batch_size", "=", "1", "\n", "\n", "", "assert", "(", "\n", "isinstance", "(", "max_length", ",", "int", ")", "and", "max_length", ">", "0", "\n", ")", ",", "\"`max_length` should be a strictly positive integer.\"", "\n", "assert", "(", "\n", "isinstance", "(", "min_length", ",", "int", ")", "and", "min_length", ">=", "0", "\n", ")", ",", "\"`min_length` should be a positive integer.\"", "\n", "assert", "isinstance", "(", "do_sample", ",", "bool", ")", ",", "\"`do_sample` should be a boolean.\"", "\n", "assert", "isinstance", "(", "early_stopping", ",", "bool", ")", ",", "\"`early_stopping` should be a boolean.\"", "\n", "assert", "(", "\n", "isinstance", "(", "num_beams", ",", "int", ")", "and", "num_beams", ">", "0", "\n", ")", ",", "\"`num_beams` should be a strictly positive integer.\"", "\n", "assert", "temperature", ">", "0", ",", "\"`temperature` should be strictly positive.\"", "\n", "assert", "(", "\n", "isinstance", "(", "top_k", ",", "int", ")", "and", "top_k", ">=", "0", "\n", ")", ",", "\"`top_k` should be a positive integer.\"", "\n", "assert", "0", "<=", "top_p", "<=", "1", ",", "\"`top_p` should be between 0 and 1.\"", "\n", "assert", "repetition_penalty", ">=", "1.0", ",", "\"`repetition_penalty` should be >= 1.\"", "\n", "assert", "input_ids", "is", "not", "None", "or", "(", "\n", "isinstance", "(", "bos_token_id", ",", "int", ")", "and", "bos_token_id", ">=", "0", "\n", ")", ",", "\"If input_ids is not defined, `bos_token_id` should be a positive integer.\"", "\n", "assert", "pad_token_id", "is", "None", "or", "(", "\n", "isinstance", "(", "pad_token_id", ",", "int", ")", "and", "(", "pad_token_id", ">=", "0", ")", "\n", ")", ",", "\"`pad_token_id` should be a positive integer.\"", "\n", "assert", "(", "eos_token_id", "is", "None", ")", "or", "(", "\n", "isinstance", "(", "eos_token_id", ",", "int", ")", "and", "(", "eos_token_id", ">=", "0", ")", "\n", ")", ",", "\"`eos_token_id` should be a positive integer.\"", "\n", "assert", "length_penalty", ">", "0", ",", "\"`length_penalty` should be strictly positive.\"", "\n", "assert", "(", "\n", "isinstance", "(", "no_repeat_ngram_size", ",", "int", ")", "and", "no_repeat_ngram_size", ">=", "0", "\n", ")", ",", "\"`no_repeat_ngram_size` should be a positive integer.\"", "\n", "assert", "(", "\n", "isinstance", "(", "num_return_sequences", ",", "int", ")", "and", "num_return_sequences", ">", "0", "\n", ")", ",", "\"`num_return_sequences` should be a strictly positive integer.\"", "\n", "assert", "(", "\n", "bad_words_ids", "is", "None", "\n", "or", "isinstance", "(", "bad_words_ids", ",", "list", ")", "\n", "and", "isinstance", "(", "bad_words_ids", "[", "0", "]", ",", "list", ")", "\n", ")", ",", "\"`bad_words_ids` is either `None` or a list of lists of tokens that should not be generated\"", "\n", "\n", "if", "input_ids", "is", "None", ":", "\n", "            ", "assert", "isinstance", "(", "bos_token_id", ",", "int", ")", "and", "bos_token_id", ">=", "0", ",", "(", "\n", "\"you should either supply a context to complete as `input_ids` input \"", "\n", "\"or a `bos_token_id` (integer >= 0) as a first token to start the generation.\"", "\n", ")", "\n", "input_ids", "=", "torch", ".", "full", "(", "\n", "(", "batch_size", ",", "1", ")", ",", "\n", "bos_token_id", ",", "\n", "dtype", "=", "torch", ".", "long", ",", "\n", "device", "=", "next", "(", "self", ".", "parameters", "(", ")", ")", ".", "device", ",", "\n", ")", "\n", "", "else", ":", "\n", "            ", "assert", "(", "\n", "input_ids", ".", "dim", "(", ")", "==", "3", "\n", ")", ",", "\"Input prompt should be of shape (batch_size, sequence length).\"", "\n", "\n", "# not allow to duplicate outputs when greedy decoding", "\n", "", "if", "do_sample", "is", "False", ":", "\n", "            ", "if", "num_beams", "==", "1", ":", "\n", "# no_beam_search greedy generation conditions", "\n", "                ", "assert", "(", "\n", "num_return_sequences", "==", "1", "\n", ")", ",", "\"Greedy decoding will always produce the same output for num_beams == 1 and num_return_sequences > 1. Please set num_return_sequences = 1\"", "\n", "\n", "", "else", ":", "\n", "# beam_search greedy generation conditions", "\n", "                ", "assert", "(", "\n", "num_beams", ">=", "num_return_sequences", "\n", ")", ",", "\"Greedy beam search decoding cannot return more sequences than it has beams. Please set num_beams >= num_return_sequences\"", "\n", "\n", "# create attention mask if necessary", "\n", "# TODO (PVP): this should later be handled by the forward fn() in each model in the future see PR 3140", "\n", "", "", "if", "(", "\n", "(", "attention_mask", "is", "None", ")", "\n", "and", "(", "pad_token_id", "is", "not", "None", ")", "\n", "and", "(", "pad_token_id", "in", "input_ids", ")", "\n", ")", ":", "\n", "            ", "attention_mask", "=", "input_ids", ".", "ne", "(", "pad_token_id", ")", ".", "long", "(", ")", "\n", "", "elif", "attention_mask", "is", "None", ":", "\n", "            ", "attention_mask", "=", "input_ids", ".", "new_ones", "(", "input_ids", ".", "shape", ")", "\n", "\n", "# set pad_token_id to eos_token_id if not set. Important that this is done after", "\n", "# attention_mask is created", "\n", "", "if", "pad_token_id", "is", "None", "and", "eos_token_id", "is", "not", "None", ":", "\n", "            ", "logger", ".", "warning", "(", "\n", "\"Setting `pad_token_id` to {} (first `eos_token_id`) to generate sequence\"", ".", "format", "(", "\n", "eos_token_id", "\n", ")", "\n", ")", "\n", "pad_token_id", "=", "eos_token_id", "\n", "\n", "# current position and vocab size", "\n", "", "vocab_size", "=", "self", ".", "vocab_size", "\n", "\n", "# set effective batch size and effective batch multiplier according to do_sample", "\n", "if", "do_sample", ":", "\n", "            ", "effective_batch_size", "=", "batch_size", "*", "num_return_sequences", "\n", "effective_batch_mult", "=", "num_return_sequences", "\n", "", "else", ":", "\n", "            ", "effective_batch_size", "=", "batch_size", "\n", "effective_batch_mult", "=", "1", "\n", "\n", "", "if", "decoder_start_token_id", "is", "None", ":", "\n", "            ", "decoder_start_token_id", "=", "bos_token_id", "\n", "\n", "", "assert", "(", "\n", "decoder_start_token_id", "is", "not", "None", "\n", ")", ",", "\"decoder_start_token_id or bos_token_id has to be defined for encoder-decoder generation\"", "\n", "\n", "encoder_outputs", "=", "self", ".", "encoder", "(", "input_ids", ",", "input_roles", ",", "input_pos", ",", "input_ent", ")", "\n", "\n", "# # Expand input ids if num_beams > 1 or num_return_sequences > 1", "\n", "# if num_return_sequences > 1 or num_beams > 1:", "\n", "#     input_sent_len = input_ids.shape[2]", "\n", "#     input_word_len = input_ids.shape[3]", "\n", "#     input_ids = input_ids.unsqueeze(1).expand(batch_size, effective_batch_mult * num_beams, input_sent_len, input_word_len)", "\n", "#     attention_mask = attention_mask.unsqueeze(1).expand(", "\n", "#         batch_size, effective_batch_mult * num_beams, input_sent_len, input_word_len", "\n", "#     )", "\n", "\n", "#     input_ids = input_ids.contiguous().view(", "\n", "#         effective_batch_size * num_beams, input_sent_len, input_word_len", "\n", "#     )  # shape: (batch_size * num_return_sequences * num_beams, input_sent_len, input_word_len)", "\n", "#     attention_mask = attention_mask.contiguous().view(", "\n", "#         effective_batch_size * num_beams, input_sent_len, input_word_len", "\n", "#     )  # shape: (batch_size * num_return_sequences * num_beams, input_sent_len, input_word_len)", "\n", "\n", "# create empty decoder_input_ids", "\n", "input_ids", "=", "torch", ".", "full", "(", "\n", "(", "effective_batch_size", "*", "num_beams", ",", "1", ")", ",", "\n", "decoder_start_token_id", ",", "\n", "dtype", "=", "torch", ".", "long", ",", "\n", "device", "=", "next", "(", "self", ".", "parameters", "(", ")", ")", ".", "device", ",", "\n", ")", "\n", "cur_len", "=", "1", "\n", "\n", "assert", "(", "\n", "batch_size", "==", "encoder_outputs", "[", "0", "]", ".", "shape", "[", "0", "]", "\n", ")", ",", "f\"expected encoder_outputs[0] to have 1st dimension bs={batch_size}, got {encoder_outputs[0].shape[0]} \"", "\n", "\n", "# expand batch_idx to assign correct encoder output for expanded input_ids (due to num_beams > 1 and num_return_sequences > 1)", "\n", "expanded_batch_idxs", "=", "(", "\n", "torch", ".", "arange", "(", "batch_size", ")", "\n", ".", "view", "(", "-", "1", ",", "1", ")", "\n", ".", "repeat", "(", "1", ",", "num_beams", "*", "effective_batch_mult", ")", "\n", ".", "view", "(", "-", "1", ")", "\n", ".", "to", "(", "input_ids", ".", "device", ")", "\n", ")", "\n", "# expand encoder_outputs", "\n", "encoder_outputs", "=", "(", "\n", "encoder_outputs", "[", "0", "]", ".", "index_select", "(", "0", ",", "expanded_batch_idxs", ")", ",", "\n", "encoder_outputs", "[", "1", "]", ".", "index_select", "(", "0", ",", "expanded_batch_idxs", ")", ",", "\n", ")", "\n", "\n", "if", "num_beams", ">", "1", ":", "\n", "            ", "output", "=", "self", ".", "_generate_beam_search", "(", "\n", "input_ids", ",", "\n", "cur_len", "=", "cur_len", ",", "\n", "max_length", "=", "max_length", ",", "\n", "min_length", "=", "min_length", ",", "\n", "do_sample", "=", "do_sample", ",", "\n", "early_stopping", "=", "early_stopping", ",", "\n", "temperature", "=", "temperature", ",", "\n", "top_k", "=", "top_k", ",", "\n", "top_p", "=", "top_p", ",", "\n", "repetition_penalty", "=", "repetition_penalty", ",", "\n", "no_repeat_ngram_size", "=", "no_repeat_ngram_size", ",", "\n", "bad_words_ids", "=", "bad_words_ids", ",", "\n", "bos_token_id", "=", "bos_token_id", ",", "\n", "pad_token_id", "=", "pad_token_id", ",", "\n", "decoder_start_token_id", "=", "decoder_start_token_id", ",", "\n", "eos_token_id", "=", "eos_token_id", ",", "\n", "batch_size", "=", "effective_batch_size", ",", "\n", "num_return_sequences", "=", "num_return_sequences", ",", "\n", "length_penalty", "=", "length_penalty", ",", "\n", "num_beams", "=", "num_beams", ",", "\n", "vocab_size", "=", "vocab_size", ",", "\n", "encoder_outputs", "=", "encoder_outputs", ",", "\n", "attention_mask", "=", "attention_mask", ",", "\n", ")", "\n", "", "else", ":", "\n", "            ", "output", "=", "self", ".", "_generate_no_beam_search", "(", "\n", "input_ids", ",", "\n", "cur_len", "=", "cur_len", ",", "\n", "max_length", "=", "max_length", ",", "\n", "min_length", "=", "min_length", ",", "\n", "do_sample", "=", "do_sample", ",", "\n", "temperature", "=", "temperature", ",", "\n", "top_k", "=", "top_k", ",", "\n", "top_p", "=", "top_p", ",", "\n", "repetition_penalty", "=", "repetition_penalty", ",", "\n", "no_repeat_ngram_size", "=", "no_repeat_ngram_size", ",", "\n", "bad_words_ids", "=", "bad_words_ids", ",", "\n", "bos_token_id", "=", "bos_token_id", ",", "\n", "pad_token_id", "=", "pad_token_id", ",", "\n", "decoder_start_token_id", "=", "decoder_start_token_id", ",", "\n", "eos_token_id", "=", "eos_token_id", ",", "\n", "batch_size", "=", "effective_batch_size", ",", "\n", "encoder_outputs", "=", "encoder_outputs", ",", "\n", "attention_mask", "=", "attention_mask", ",", "\n", ")", "\n", "\n", "", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.MeetingNet_Transformer.MeetingNet_Transformer._generate_no_beam_search": [[786, 914], ["torch.cat.new().fill_", "torch.cat.new().fill_", "torch.cat.new().fill_", "torch.cat.new().fill_", "torch.cat.new().fill_", "torch.cat.new().fill_", "torch.cat.new().fill_", "torch.cat.new().fill_", "enumerate", "MeetingNet_Transformer.MeetingNet_Transformer.prepare_inputs_for_generation", "MeetingNet_Transformer.MeetingNet_Transformer.decoder", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat.new().fill_.min().item", "torch.cat.new().fill_.max().item", "torch.cat.new().fill_", "torch.cat.new().fill_", "torch.cat.new().fill_", "torch.cat.new().fill_", "torch.cat.new", "torch.cat.new", "torch.cat.new", "torch.cat.new", "torch.cat.new", "torch.cat.new", "torch.cat.new", "torch.cat.new", "MeetingNet_Transformer.MeetingNet_Transformer.enforce_repetition_penalty_", "summertime.model.third_party.HMNet.ThirdParty.Huggingface.Transformers.src.transformers.modeling_encoder_decoder.calc_banned_ngram_tokens", "range", "summertime.model.third_party.HMNet.ThirdParty.Huggingface.Transformers.src.transformers.modeling_encoder_decoder.calc_banned_bad_words_ids", "range", "summertime.model.third_party.HMNet.ThirdParty.Huggingface.Transformers.src.transformers.modeling_encoder_decoder.top_k_top_p_filtering", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.multinomial().squeeze", "torch.multinomial().squeeze", "torch.multinomial().squeeze", "torch.multinomial().squeeze", "torch.multinomial().squeeze", "torch.multinomial().squeeze", "torch.multinomial().squeeze", "torch.multinomial().squeeze", "torch.multinomial().squeeze", "torch.multinomial().squeeze", "torch.multinomial().squeeze", "torch.multinomial().squeeze", "torch.multinomial().squeeze", "torch.multinomial().squeeze", "torch.multinomial().squeeze", "torch.multinomial().squeeze", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.cat.new().fill_.mul().bool", "torch.cat.new().fill_.masked_fill_", "torch.cat.new().fill_.mul_", "torch.cat.new().fill_.max", "float", "tokens_to_add.unsqueeze", "torch.cat.new().fill_.min", "torch.cat.new().fill_.max", "torch.cat.new", "torch.cat.new", "torch.cat.new", "torch.cat.new", "float", "float", "torch.multinomial", "torch.multinomial", "torch.multinomial", "torch.multinomial", "torch.multinomial", "torch.multinomial", "torch.multinomial", "torch.multinomial", "torch.multinomial", "torch.multinomial", "torch.multinomial", "torch.multinomial", "torch.multinomial", "torch.multinomial", "torch.multinomial", "torch.multinomial", "torch.cat.new().fill_.mul", "torch.cat.new().fill_.max().item", "eos_in_sents.long", "torch.cat.new().fill_.max"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.MeetingNet_Transformer.MeetingNet_Transformer.prepare_inputs_for_generation", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizerFast.decoder", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.MeetingNet_Transformer.MeetingNet_Transformer.enforce_repetition_penalty_", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.modeling_encoder_decoder.calc_banned_ngram_tokens", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.modeling_encoder_decoder.calc_banned_bad_words_ids", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.modeling_encoder_decoder.top_k_top_p_filtering"], ["", "def", "_generate_no_beam_search", "(", "\n", "self", ",", "\n", "input_ids", ",", "\n", "cur_len", ",", "\n", "max_length", ",", "\n", "min_length", ",", "\n", "do_sample", ",", "\n", "temperature", ",", "\n", "top_k", ",", "\n", "top_p", ",", "\n", "repetition_penalty", ",", "\n", "no_repeat_ngram_size", ",", "\n", "bad_words_ids", ",", "\n", "bos_token_id", ",", "\n", "pad_token_id", ",", "\n", "eos_token_id", ",", "\n", "decoder_start_token_id", ",", "\n", "batch_size", ",", "\n", "encoder_outputs", ",", "\n", "attention_mask", ",", "\n", ")", ":", "\n", "        ", "\"\"\"Generate sequences for each example without beam search (num_beams == 1).\n        All returned sequence are generated independantly.\n        \"\"\"", "\n", "# length of generated sentences / unfinished sentences", "\n", "unfinished_sents", "=", "input_ids", ".", "new", "(", "batch_size", ")", ".", "fill_", "(", "1", ")", "\n", "sent_lengths", "=", "input_ids", ".", "new", "(", "batch_size", ")", ".", "fill_", "(", "max_length", ")", "\n", "\n", "past", "=", "encoder_outputs", "# defined for encoder-decoder models, None for decoder-only models", "\n", "\n", "while", "cur_len", "<", "max_length", ":", "\n", "            ", "model_inputs", "=", "self", ".", "prepare_inputs_for_generation", "(", "\n", "input_ids", ",", "past", "=", "past", ",", "attention_mask", "=", "attention_mask", "\n", ")", "\n", "\n", "outputs", "=", "self", ".", "decoder", "(", "**", "model_inputs", ")", "\n", "next_token_logits", "=", "outputs", "[", ":", ",", "-", "1", ",", ":", "]", "\n", "\n", "# repetition penalty from CTRL paper (https://arxiv.org/abs/1909.05858)", "\n", "if", "repetition_penalty", "!=", "1.0", ":", "\n", "                ", "self", ".", "enforce_repetition_penalty_", "(", "\n", "next_token_logits", ",", "batch_size", ",", "1", ",", "input_ids", ",", "repetition_penalty", "\n", ")", "\n", "\n", "", "if", "no_repeat_ngram_size", ">", "0", ":", "\n", "# calculate a list of banned tokens to prevent repetitively generating the same ngrams", "\n", "# from fairseq: https://github.com/pytorch/fairseq/blob/a07cb6f40480928c9e0548b737aadd36ee66ac76/fairseq/sequence_generator.py#L345", "\n", "                ", "banned_tokens", "=", "calc_banned_ngram_tokens", "(", "\n", "input_ids", ",", "batch_size", ",", "no_repeat_ngram_size", ",", "cur_len", "\n", ")", "\n", "for", "batch_idx", "in", "range", "(", "batch_size", ")", ":", "\n", "                    ", "next_token_logits", "[", "batch_idx", ",", "banned_tokens", "[", "batch_idx", "]", "]", "=", "-", "float", "(", "\n", "\"inf\"", "\n", ")", "\n", "\n", "", "", "if", "bad_words_ids", "is", "not", "None", ":", "\n", "# calculate a list of banned tokens according to bad words", "\n", "                ", "banned_tokens", "=", "calc_banned_bad_words_ids", "(", "input_ids", ",", "bad_words_ids", ")", "\n", "\n", "for", "batch_idx", "in", "range", "(", "batch_size", ")", ":", "\n", "                    ", "next_token_logits", "[", "batch_idx", ",", "banned_tokens", "[", "batch_idx", "]", "]", "=", "-", "float", "(", "\n", "\"inf\"", "\n", ")", "\n", "\n", "# set eos token prob to zero if min_length is not reached", "\n", "", "", "if", "eos_token_id", "is", "not", "None", "and", "cur_len", "<", "min_length", ":", "\n", "                ", "next_token_logits", "[", ":", ",", "eos_token_id", "]", "=", "-", "float", "(", "\"inf\"", ")", "\n", "\n", "", "if", "do_sample", ":", "\n", "# Temperature (higher temperature => more likely to sample low probability tokens)", "\n", "                ", "if", "temperature", "!=", "1.0", ":", "\n", "                    ", "next_token_logits", "=", "next_token_logits", "/", "temperature", "\n", "# Top-p/top-k filtering", "\n", "", "next_token_logits", "=", "top_k_top_p_filtering", "(", "\n", "next_token_logits", ",", "top_k", "=", "top_k", ",", "top_p", "=", "top_p", "\n", ")", "\n", "# Sample", "\n", "probs", "=", "F", ".", "softmax", "(", "next_token_logits", ",", "dim", "=", "-", "1", ")", "\n", "next_token", "=", "torch", ".", "multinomial", "(", "probs", ",", "num_samples", "=", "1", ")", ".", "squeeze", "(", "1", ")", "\n", "", "else", ":", "\n", "# Greedy decoding", "\n", "                ", "next_token", "=", "torch", ".", "argmax", "(", "next_token_logits", ",", "dim", "=", "-", "1", ")", "\n", "\n", "# update generations and finished sentences", "\n", "", "if", "eos_token_id", "is", "not", "None", ":", "\n", "# pad finished sentences if eos_token_id exist", "\n", "                ", "tokens_to_add", "=", "next_token", "*", "unfinished_sents", "+", "(", "pad_token_id", ")", "*", "(", "\n", "1", "-", "unfinished_sents", "\n", ")", "\n", "", "else", ":", "\n", "                ", "tokens_to_add", "=", "next_token", "\n", "\n", "", "input_ids", "=", "torch", ".", "cat", "(", "[", "input_ids", ",", "tokens_to_add", ".", "unsqueeze", "(", "-", "1", ")", "]", ",", "dim", "=", "-", "1", ")", "\n", "\n", "if", "eos_token_id", "is", "not", "None", ":", "\n", "                ", "eos_in_sents", "=", "tokens_to_add", "==", "eos_token_id", "\n", "# if sentence is unfinished and the token to add is eos, sent_lengths is filled with current length", "\n", "is_sents_unfinished_and_token_to_add_is_eos", "=", "unfinished_sents", ".", "mul", "(", "\n", "eos_in_sents", ".", "long", "(", ")", "\n", ")", ".", "bool", "(", ")", "\n", "sent_lengths", ".", "masked_fill_", "(", "\n", "is_sents_unfinished_and_token_to_add_is_eos", ",", "cur_len", "+", "1", "\n", ")", "\n", "# unfinished_sents is set to zero if eos in sentence", "\n", "unfinished_sents", ".", "mul_", "(", "(", "~", "eos_in_sents", ")", ".", "long", "(", ")", ")", "\n", "\n", "# stop when there is a </s> in each sentence, or if we exceed the maximul length", "\n", "", "if", "unfinished_sents", ".", "max", "(", ")", "==", "0", ":", "\n", "                ", "break", "\n", "\n", "", "cur_len", "=", "cur_len", "+", "1", "\n", "\n", "# if there are different sentences lengths in the batch, some batches have to be padded", "\n", "", "if", "sent_lengths", ".", "min", "(", ")", ".", "item", "(", ")", "!=", "sent_lengths", ".", "max", "(", ")", ".", "item", "(", ")", ":", "\n", "            ", "assert", "(", "\n", "pad_token_id", "is", "not", "None", "\n", ")", ",", "\"`Pad_token_id` has to be defined if batches have different lengths\"", "\n", "# finished sents are filled with pad_token", "\n", "decoded", "=", "input_ids", ".", "new", "(", "batch_size", ",", "sent_lengths", ".", "max", "(", ")", ".", "item", "(", ")", ")", ".", "fill_", "(", "\n", "pad_token_id", "\n", ")", "\n", "", "else", ":", "\n", "            ", "decoded", "=", "input_ids", "\n", "\n", "", "for", "hypo_idx", ",", "hypo", "in", "enumerate", "(", "input_ids", ")", ":", "\n", "            ", "decoded", "[", "hypo_idx", ",", ":", "sent_lengths", "[", "hypo_idx", "]", "]", "=", "hypo", "[", ":", "sent_lengths", "[", "hypo_idx", "]", "]", "\n", "\n", "", "return", "decoded", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.MeetingNet_Transformer.MeetingNet_Transformer._generate_beam_search": [[915, 1224], ["torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "beam_scores.new.new.view", "range", "torch.cat.new", "torch.cat.new", "torch.cat.new", "torch.cat.new", "enumerate", "summertime.model.third_party.HMNet.ThirdParty.Huggingface.Transformers.src.transformers.modeling_encoder_decoder.BeamHypotheses", "MeetingNet_Transformer.MeetingNet_Transformer.prepare_inputs_for_generation", "MeetingNet_Transformer.MeetingNet_Transformer.decoder", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "range", "all", "beam_scores.new.new.new", "torch.cat.new", "torch.cat.new", "torch.cat.new", "torch.cat.new", "torch.cat.new", "torch.cat.new", "torch.cat.new", "torch.cat.new", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "range", "sorted", "range", "torch.cat.new.min().item", "torch.cat.new.max().item", "min", "torch.cat.new().fill_", "torch.cat.new().fill_", "torch.cat.new().fill_", "torch.cat.new().fill_", "enumerate", "torch.stack().type().to", "torch.stack().type().to", "torch.stack().type().to", "torch.stack().type().to", "torch.stack().type().to", "torch.stack().type().to", "torch.stack().type().to", "torch.stack().type().to", "torch.stack().type().to", "torch.stack().type().to", "torch.stack().type().to", "torch.stack().type().to", "torch.stack().type().to", "torch.stack().type().to", "torch.stack().type().to", "torch.stack().type().to", "range", "range", "MeetingNet_Transformer.MeetingNet_Transformer.enforce_repetition_penalty_", "MeetingNet_Transformer.MeetingNet_Transformer.prepare_scores_for_generation", "summertime.model.third_party.HMNet.ThirdParty.Huggingface.Transformers.src.transformers.modeling_encoder_decoder.calc_banned_ngram_tokens", "enumerate", "summertime.model.third_party.HMNet.ThirdParty.Huggingface.Transformers.src.transformers.modeling_encoder_decoder.calc_banned_bad_words_ids", "enumerate", "summertime.model.third_party.HMNet.ThirdParty.Huggingface.Transformers.src.transformers.modeling_encoder_decoder.top_k_top_p_filtering", "_scores.contiguous().view.contiguous().view.contiguous().view", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.multinomial", "torch.multinomial", "torch.multinomial", "torch.multinomial", "torch.multinomial", "torch.multinomial", "torch.multinomial", "torch.multinomial", "torch.multinomial", "torch.multinomial", "torch.multinomial", "torch.multinomial", "torch.multinomial", "torch.multinomial", "torch.multinomial", "torch.multinomial", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.sort", "torch.sort", "torch.sort", "torch.sort", "torch.sort", "torch.sort", "torch.sort", "torch.sort", "torch.sort", "torch.sort", "torch.sort", "torch.sort", "torch.sort", "torch.sort", "torch.sort", "torch.sort", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "next_scores.view.view.view", "torch.topk", "torch.topk", "torch.topk", "torch.topk", "torch.topk", "torch.topk", "torch.topk", "torch.topk", "torch.topk", "torch.topk", "torch.topk", "torch.topk", "torch.topk", "torch.topk", "torch.topk", "torch.topk", "next_scores.view.view.size", "torch.gather.size", "torch.gather.size", "torch.gather.size", "torch.gather.size", "enumerate", "next_batch_beam.extend", "len", "MeetingNet_Transformer.MeetingNet_Transformer._reorder_cache", "all", "torch.all", "torch.all", "torch.all", "torch.all", "torch.all", "torch.all", "torch.all", "torch.all", "torch.all", "torch.all", "torch.all", "torch.all", "torch.all", "torch.all", "torch.all", "torch.all", "beam_scores[].item", "generated_hyps[].add", "len", "best.append", "float", "beam_scores[].expand_as", "beam_scores[].expand_as", "next_batch_beam.extend", "zip", "generated_hyps[].is_done", "len", "len", "torch.cat.new.unsqueeze", "sorted.pop", "torch.cat.new.min", "torch.cat.new.max", "torch.cat.new.max().item", "torch.cat.new", "torch.cat.new", "torch.cat.new", "torch.cat.new", "len", "torch.stack().type", "torch.stack().type", "torch.stack().type", "torch.stack().type", "torch.stack().type", "torch.stack().type", "torch.stack().type", "torch.stack().type", "torch.stack().type", "torch.stack().type", "torch.stack().type", "torch.stack().type", "torch.stack().type", "torch.stack().type", "torch.stack().type", "torch.stack().type", "next", "float", "float", "_scores.contiguous().view.contiguous().view.contiguous", "len", "generated_hyps[].add", "next_sent_beam.append", "len", "next_scores[].max().item", "beam_scores.new.new.view", "MeetingNet_Transformer.MeetingNet_Transformer.parameters", "token_id.item", "input_ids[].clone", "beam_token_score.item", "beam_scores.new.new.view", "torch.cat.new.max", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "next_scores[].max"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.MeetingNet_Transformer.MeetingNet_Transformer.prepare_inputs_for_generation", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizerFast.decoder", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.MeetingNet_Transformer.MeetingNet_Transformer.enforce_repetition_penalty_", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.MeetingNet_Transformer.MeetingNet_Transformer.prepare_scores_for_generation", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.modeling_encoder_decoder.calc_banned_ngram_tokens", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.modeling_encoder_decoder.calc_banned_bad_words_ids", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.modeling_encoder_decoder.top_k_top_p_filtering", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.Transformer.TransformerBeam.topk", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.Transformer.TransformerBeam.topk", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.Transformer.TransformerBeam.topk", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.Transformer.TransformerBeam.topk", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.Transformer.TransformerBeam.topk", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.Transformer.TransformerBeam.topk", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.Transformer.TransformerBeam.topk", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.Transformer.TransformerBeam.topk", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.Transformer.TransformerBeam.topk", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.Transformer.TransformerBeam.topk", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.Transformer.TransformerBeam.topk", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.Transformer.TransformerBeam.topk", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.Transformer.TransformerBeam.topk", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.Transformer.TransformerBeam.topk", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.Transformer.TransformerBeam.topk", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.Transformer.TransformerBeam.topk", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.MeetingNet_Transformer.MeetingNet_Transformer._reorder_cache", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.modeling_encoder_decoder.BeamHypotheses.add", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.modeling_encoder_decoder.BeamHypotheses.is_done", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.modeling_encoder_decoder.BeamHypotheses.add"], ["", "def", "_generate_beam_search", "(", "\n", "self", ",", "\n", "input_ids", ",", "\n", "cur_len", ",", "\n", "max_length", ",", "\n", "min_length", ",", "\n", "do_sample", ",", "\n", "early_stopping", ",", "\n", "temperature", ",", "\n", "top_k", ",", "\n", "top_p", ",", "\n", "repetition_penalty", ",", "\n", "no_repeat_ngram_size", ",", "\n", "bad_words_ids", ",", "\n", "bos_token_id", ",", "\n", "pad_token_id", ",", "\n", "eos_token_id", ",", "\n", "decoder_start_token_id", ",", "\n", "batch_size", ",", "\n", "num_return_sequences", ",", "\n", "length_penalty", ",", "\n", "num_beams", ",", "\n", "vocab_size", ",", "\n", "encoder_outputs", ",", "\n", "attention_mask", ",", "\n", ")", ":", "\n", "        ", "\"\"\"Generate sequences for each example with beam search.\"\"\"", "\n", "\n", "# generated hypotheses", "\n", "generated_hyps", "=", "[", "\n", "BeamHypotheses", "(", "\n", "num_beams", ",", "max_length", ",", "length_penalty", ",", "early_stopping", "=", "early_stopping", "\n", ")", "\n", "for", "_", "in", "range", "(", "batch_size", ")", "\n", "]", "\n", "\n", "# scores for each sentence in the beam", "\n", "beam_scores", "=", "torch", ".", "zeros", "(", "\n", "(", "batch_size", ",", "num_beams", ")", ",", "dtype", "=", "torch", ".", "float", ",", "device", "=", "input_ids", ".", "device", "\n", ")", "\n", "\n", "# for greedy decoding it is made sure that only tokens of the first beam are considered to avoid sampling the exact same tokens three times", "\n", "if", "do_sample", "is", "False", ":", "\n", "            ", "beam_scores", "[", ":", ",", "1", ":", "]", "=", "-", "1e9", "\n", "", "beam_scores", "=", "beam_scores", ".", "view", "(", "-", "1", ")", "# shape (batch_size * num_beams,)", "\n", "\n", "# cache compute states", "\n", "past", "=", "encoder_outputs", "# defined for encoder-decoder models, None for decoder-only models", "\n", "\n", "# done sentences", "\n", "done", "=", "[", "False", "for", "_", "in", "range", "(", "batch_size", ")", "]", "\n", "\n", "while", "cur_len", "<", "max_length", ":", "\n", "            ", "model_inputs", "=", "self", ".", "prepare_inputs_for_generation", "(", "\n", "input_ids", ",", "past", "=", "past", ",", "attention_mask", "=", "attention_mask", "\n", ")", "\n", "outputs", "=", "self", ".", "decoder", "(", "\n", "**", "model_inputs", "\n", ")", "# (batch_size * num_beams, cur_len, vocab_size)", "\n", "next_token_logits", "=", "outputs", "[", "\n", ":", ",", "-", "1", ",", ":", "\n", "]", "# (batch_size * num_beams, vocab_size)", "\n", "\n", "# repetition penalty (from CTRL paper https://arxiv.org/abs/1909.05858)", "\n", "if", "repetition_penalty", "!=", "1.0", ":", "\n", "                ", "self", ".", "enforce_repetition_penalty_", "(", "\n", "next_token_logits", ",", "\n", "batch_size", ",", "\n", "num_beams", ",", "\n", "input_ids", ",", "\n", "repetition_penalty", ",", "\n", ")", "\n", "\n", "", "if", "temperature", "!=", "1.0", ":", "\n", "                ", "next_token_logits", "=", "next_token_logits", "/", "temperature", "\n", "\n", "", "scores", "=", "F", ".", "log_softmax", "(", "\n", "next_token_logits", ",", "dim", "=", "-", "1", "\n", ")", "# (batch_size * num_beams, vocab_size)", "\n", "if", "do_sample", "is", "False", ":", "\n", "# TODO (PVP) still a bit hacky here - there might be a better solution", "\n", "                ", "scores", "=", "self", ".", "prepare_scores_for_generation", "(", "\n", "scores", ",", "cur_len", "=", "cur_len", ",", "max_length", "=", "max_length", "\n", ")", "\n", "\n", "# set eos token prob to zero if min_length is not reached", "\n", "", "if", "eos_token_id", "is", "not", "None", "and", "cur_len", "<", "min_length", ":", "\n", "                ", "scores", "[", ":", ",", "eos_token_id", "]", "=", "-", "float", "(", "\"inf\"", ")", "\n", "\n", "", "if", "no_repeat_ngram_size", ">", "0", ":", "\n", "# calculate a list of banned tokens to prevent repetitively generating the same ngrams", "\n", "                ", "num_batch_hypotheses", "=", "batch_size", "*", "num_beams", "\n", "# from fairseq: https://github.com/pytorch/fairseq/blob/a07cb6f40480928c9e0548b737aadd36ee66ac76/fairseq/sequence_generator.py#L345", "\n", "banned_batch_tokens", "=", "calc_banned_ngram_tokens", "(", "\n", "input_ids", ",", "num_batch_hypotheses", ",", "no_repeat_ngram_size", ",", "cur_len", "\n", ")", "\n", "for", "i", ",", "banned_tokens", "in", "enumerate", "(", "banned_batch_tokens", ")", ":", "\n", "                    ", "scores", "[", "i", ",", "banned_tokens", "]", "=", "-", "float", "(", "\"inf\"", ")", "\n", "\n", "", "", "if", "bad_words_ids", "is", "not", "None", ":", "\n", "# calculate a list of banned tokens according to bad words", "\n", "                ", "banned_tokens", "=", "calc_banned_bad_words_ids", "(", "input_ids", ",", "bad_words_ids", ")", "\n", "\n", "for", "i", ",", "banned_tokens", "in", "enumerate", "(", "banned_tokens", ")", ":", "\n", "                    ", "scores", "[", "i", ",", "banned_tokens", "]", "=", "-", "float", "(", "\"inf\"", ")", "\n", "\n", "", "", "assert", "scores", ".", "shape", "==", "(", "\n", "batch_size", "*", "num_beams", ",", "\n", "vocab_size", ",", "\n", ")", ",", "\"Shapes of scores: {} != {}\"", ".", "format", "(", "\n", "scores", ".", "shape", ",", "(", "batch_size", "*", "num_beams", ",", "vocab_size", ")", "\n", ")", "\n", "\n", "if", "do_sample", ":", "\n", "                ", "_scores", "=", "scores", "+", "beam_scores", "[", ":", ",", "None", "]", ".", "expand_as", "(", "\n", "scores", "\n", ")", "# (batch_size * num_beams, vocab_size)", "\n", "# Top-p/top-k filtering", "\n", "_scores", "=", "top_k_top_p_filtering", "(", "\n", "_scores", ",", "top_k", "=", "top_k", ",", "top_p", "=", "top_p", ",", "min_tokens_to_keep", "=", "2", "\n", ")", "# (batch_size * num_beams, vocab_size)", "\n", "# re-organize to group the beam together to sample from all beam_idxs", "\n", "_scores", "=", "_scores", ".", "contiguous", "(", ")", ".", "view", "(", "\n", "batch_size", ",", "num_beams", "*", "vocab_size", "\n", ")", "# (batch_size, num_beams * vocab_size)", "\n", "\n", "# Sample 2 next tokens for each beam (so we have some spare tokens and match output of greedy beam search)", "\n", "probs", "=", "F", ".", "softmax", "(", "_scores", ",", "dim", "=", "-", "1", ")", "\n", "next_tokens", "=", "torch", ".", "multinomial", "(", "\n", "probs", ",", "num_samples", "=", "2", "*", "num_beams", "\n", ")", "# (batch_size, num_beams * 2)", "\n", "# Compute next scores", "\n", "next_scores", "=", "torch", ".", "gather", "(", "\n", "_scores", ",", "-", "1", ",", "next_tokens", "\n", ")", "# (batch_size, num_beams * 2)", "\n", "# sort the sampled vector to make sure that the first num_beams samples are the best", "\n", "next_scores", ",", "next_scores_indices", "=", "torch", ".", "sort", "(", "\n", "next_scores", ",", "descending", "=", "True", ",", "dim", "=", "1", "\n", ")", "\n", "next_tokens", "=", "torch", ".", "gather", "(", "\n", "next_tokens", ",", "-", "1", ",", "next_scores_indices", "\n", ")", "# (batch_size, num_beams * 2)", "\n", "\n", "", "else", ":", "\n", "                ", "next_scores", "=", "scores", "+", "beam_scores", "[", ":", ",", "None", "]", ".", "expand_as", "(", "\n", "scores", "\n", ")", "# (batch_size * num_beams, vocab_size)", "\n", "\n", "# re-organize to group the beam together (we are keeping top hypothesis accross beams)", "\n", "next_scores", "=", "next_scores", ".", "view", "(", "\n", "batch_size", ",", "num_beams", "*", "vocab_size", "\n", ")", "# (batch_size, num_beams * vocab_size)", "\n", "\n", "next_scores", ",", "next_tokens", "=", "torch", ".", "topk", "(", "\n", "next_scores", ",", "2", "*", "num_beams", ",", "dim", "=", "1", ",", "largest", "=", "True", ",", "sorted", "=", "True", "\n", ")", "\n", "\n", "", "assert", "(", "\n", "next_scores", ".", "size", "(", ")", "==", "next_tokens", ".", "size", "(", ")", "==", "(", "batch_size", ",", "2", "*", "num_beams", ")", "\n", ")", "\n", "\n", "# next batch beam content", "\n", "next_batch_beam", "=", "[", "]", "\n", "\n", "# for each sentence", "\n", "for", "batch_idx", "in", "range", "(", "batch_size", ")", ":", "\n", "\n", "# if we are done with this sentence", "\n", "                ", "if", "done", "[", "batch_idx", "]", ":", "\n", "                    ", "assert", "(", "\n", "len", "(", "generated_hyps", "[", "batch_idx", "]", ")", ">=", "num_beams", "\n", ")", ",", "\"Batch can only be done if at least {} beams have been generated\"", ".", "format", "(", "\n", "num_beams", "\n", ")", "\n", "assert", "(", "\n", "eos_token_id", "is", "not", "None", "and", "pad_token_id", "is", "not", "None", "\n", ")", ",", "\"generated beams >= num_beams -> eos_token_id and pad_token have to be defined\"", "\n", "next_batch_beam", ".", "extend", "(", "\n", "[", "(", "0", ",", "pad_token_id", ",", "0", ")", "]", "*", "num_beams", "\n", ")", "# pad the batch", "\n", "continue", "\n", "\n", "# next sentence beam content", "\n", "", "next_sent_beam", "=", "[", "]", "\n", "\n", "# next tokens for this sentence", "\n", "for", "beam_token_rank", ",", "(", "beam_token_id", ",", "beam_token_score", ")", "in", "enumerate", "(", "\n", "zip", "(", "next_tokens", "[", "batch_idx", "]", ",", "next_scores", "[", "batch_idx", "]", ")", "\n", ")", ":", "\n", "# get beam and token IDs", "\n", "                    ", "beam_id", "=", "beam_token_id", "//", "vocab_size", "\n", "token_id", "=", "beam_token_id", "%", "vocab_size", "\n", "\n", "effective_beam_id", "=", "batch_idx", "*", "num_beams", "+", "beam_id", "\n", "# add to generated hypotheses if end of sentence or last iteration", "\n", "if", "(", "eos_token_id", "is", "not", "None", ")", "and", "(", "token_id", ".", "item", "(", ")", "==", "eos_token_id", ")", ":", "\n", "# if beam_token does not belong to top num_beams tokens, it should not be added", "\n", "                        ", "is_beam_token_worse_than_top_num_beams", "=", "(", "\n", "beam_token_rank", ">=", "num_beams", "\n", ")", "\n", "if", "is_beam_token_worse_than_top_num_beams", ":", "\n", "                            ", "continue", "\n", "", "generated_hyps", "[", "batch_idx", "]", ".", "add", "(", "\n", "input_ids", "[", "effective_beam_id", "]", ".", "clone", "(", ")", ",", "\n", "beam_token_score", ".", "item", "(", ")", ",", "\n", ")", "\n", "", "else", ":", "\n", "# add next predicted token if it is not eos_token", "\n", "                        ", "next_sent_beam", ".", "append", "(", "\n", "(", "beam_token_score", ",", "token_id", ",", "effective_beam_id", ")", "\n", ")", "\n", "\n", "# the beam for next step is full", "\n", "", "if", "len", "(", "next_sent_beam", ")", "==", "num_beams", ":", "\n", "                        ", "break", "\n", "\n", "# Check if were done so that we can save a pad step if all(done)", "\n", "", "", "done", "[", "batch_idx", "]", "=", "done", "[", "batch_idx", "]", "or", "generated_hyps", "[", "batch_idx", "]", ".", "is_done", "(", "\n", "next_scores", "[", "batch_idx", "]", ".", "max", "(", ")", ".", "item", "(", ")", ",", "cur_len", "=", "cur_len", "\n", ")", "\n", "\n", "# update next beam content", "\n", "assert", "len", "(", "next_sent_beam", ")", "==", "num_beams", ",", "\"Beam should always be full\"", "\n", "next_batch_beam", ".", "extend", "(", "next_sent_beam", ")", "\n", "assert", "len", "(", "next_batch_beam", ")", "==", "num_beams", "*", "(", "batch_idx", "+", "1", ")", "\n", "\n", "# stop when we are done with each sentence", "\n", "", "if", "all", "(", "done", ")", ":", "\n", "                ", "break", "\n", "\n", "# sanity check / prepare next batch", "\n", "", "assert", "len", "(", "next_batch_beam", ")", "==", "batch_size", "*", "num_beams", "\n", "beam_scores", "=", "beam_scores", ".", "new", "(", "[", "x", "[", "0", "]", "for", "x", "in", "next_batch_beam", "]", ")", "\n", "beam_tokens", "=", "input_ids", ".", "new", "(", "[", "x", "[", "1", "]", "for", "x", "in", "next_batch_beam", "]", ")", "\n", "beam_idx", "=", "input_ids", ".", "new", "(", "[", "x", "[", "2", "]", "for", "x", "in", "next_batch_beam", "]", ")", "\n", "\n", "# re-order batch", "\n", "input_ids", "=", "input_ids", "[", "beam_idx", ",", ":", "]", "\n", "input_ids", "=", "torch", ".", "cat", "(", "[", "input_ids", ",", "beam_tokens", ".", "unsqueeze", "(", "1", ")", "]", ",", "dim", "=", "-", "1", ")", "\n", "# re-order internal states", "\n", "if", "past", "is", "not", "None", ":", "\n", "                ", "past", "=", "self", ".", "_reorder_cache", "(", "past", ",", "beam_idx", ")", "\n", "\n", "# update current length", "\n", "", "cur_len", "=", "cur_len", "+", "1", "\n", "\n", "# finalize all open beam hypotheses and end to generated hypotheses", "\n", "", "for", "batch_idx", "in", "range", "(", "batch_size", ")", ":", "\n", "            ", "if", "done", "[", "batch_idx", "]", ":", "\n", "                ", "continue", "\n", "\n", "# test that beam scores match previously calculated scores if not eos and batch_idx not done", "\n", "", "if", "eos_token_id", "is", "not", "None", "and", "all", "(", "\n", "(", "token_id", "%", "vocab_size", ")", ".", "item", "(", ")", "is", "not", "eos_token_id", "\n", "for", "token_id", "in", "next_tokens", "[", "batch_idx", "]", "\n", ")", ":", "\n", "                ", "assert", "torch", ".", "all", "(", "\n", "next_scores", "[", "batch_idx", ",", ":", "num_beams", "]", "\n", "==", "beam_scores", ".", "view", "(", "batch_size", ",", "num_beams", ")", "[", "batch_idx", "]", "\n", ")", ",", "\"If batch_idx is not done, final next scores: {} have to equal to accumulated beam_scores: {}\"", ".", "format", "(", "\n", "next_scores", "[", ":", ",", ":", "num_beams", "]", "[", "batch_idx", "]", ",", "\n", "beam_scores", ".", "view", "(", "batch_size", ",", "num_beams", ")", "[", "batch_idx", "]", ",", "\n", ")", "\n", "\n", "# need to add best num_beams hypotheses to generated hyps", "\n", "", "for", "beam_id", "in", "range", "(", "num_beams", ")", ":", "\n", "                ", "effective_beam_id", "=", "batch_idx", "*", "num_beams", "+", "beam_id", "\n", "final_score", "=", "beam_scores", "[", "effective_beam_id", "]", ".", "item", "(", ")", "\n", "final_tokens", "=", "input_ids", "[", "effective_beam_id", "]", "\n", "generated_hyps", "[", "batch_idx", "]", ".", "add", "(", "final_tokens", ",", "final_score", ")", "\n", "\n", "# depending on whether greedy generation is wanted or not define different output_batch_size and output_num_return_sequences_per_batch", "\n", "", "", "output_batch_size", "=", "(", "\n", "batch_size", "if", "do_sample", "else", "batch_size", "*", "num_return_sequences", "\n", ")", "\n", "output_num_return_sequences_per_batch", "=", "1", "if", "do_sample", "else", "num_return_sequences", "\n", "\n", "# select the best hypotheses", "\n", "sent_lengths", "=", "input_ids", ".", "new", "(", "output_batch_size", ")", "\n", "best", "=", "[", "]", "\n", "\n", "# retrieve best hypotheses", "\n", "for", "i", ",", "hypotheses", "in", "enumerate", "(", "generated_hyps", ")", ":", "\n", "            ", "sorted_hyps", "=", "sorted", "(", "hypotheses", ".", "beams", ",", "key", "=", "lambda", "x", ":", "x", "[", "0", "]", ")", "\n", "for", "j", "in", "range", "(", "output_num_return_sequences_per_batch", ")", ":", "\n", "                ", "effective_batch_idx", "=", "output_num_return_sequences_per_batch", "*", "i", "+", "j", "\n", "best_hyp", "=", "sorted_hyps", ".", "pop", "(", ")", "[", "1", "]", "\n", "sent_lengths", "[", "effective_batch_idx", "]", "=", "len", "(", "best_hyp", ")", "\n", "best", ".", "append", "(", "best_hyp", ")", "\n", "\n", "# shorter batches are filled with pad_token", "\n", "", "", "if", "sent_lengths", ".", "min", "(", ")", ".", "item", "(", ")", "!=", "sent_lengths", ".", "max", "(", ")", ".", "item", "(", ")", ":", "\n", "            ", "assert", "pad_token_id", "is", "not", "None", ",", "\"`Pad_token_id` has to be defined\"", "\n", "sent_max_len", "=", "min", "(", "sent_lengths", ".", "max", "(", ")", ".", "item", "(", ")", "+", "1", ",", "max_length", ")", "\n", "decoded", "=", "input_ids", ".", "new", "(", "output_batch_size", ",", "sent_max_len", ")", ".", "fill_", "(", "pad_token_id", ")", "\n", "\n", "# fill with hypothesis and eos_token_id if necessary", "\n", "for", "i", ",", "hypo", "in", "enumerate", "(", "best", ")", ":", "\n", "                ", "decoded", "[", "i", ",", ":", "sent_lengths", "[", "i", "]", "]", "=", "hypo", "\n", "if", "sent_lengths", "[", "i", "]", "<", "max_length", ":", "\n", "                    ", "decoded", "[", "i", ",", "sent_lengths", "[", "i", "]", "]", "=", "eos_token_id", "\n", "", "", "", "else", ":", "\n", "# none of the hypotheses have an eos_token", "\n", "            ", "assert", "(", "len", "(", "hypo", ")", "==", "max_length", "for", "hypo", "in", "best", ")", "\n", "decoded", "=", "(", "\n", "torch", ".", "stack", "(", "best", ")", ".", "type", "(", "torch", ".", "long", ")", ".", "to", "(", "next", "(", "self", ".", "parameters", "(", ")", ")", ".", "device", ")", "\n", ")", "\n", "\n", "", "return", "decoded", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.MeetingNet_Transformer.MeetingNet_Transformer._force_token_ids_generation": [[1226, 1238], ["isinstance", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "len", "float", "range", "next", "MeetingNet_Transformer.MeetingNet_Transformer.parameters"], "methods", ["None"], ["", "def", "_force_token_ids_generation", "(", "self", ",", "scores", ",", "token_ids", ")", ":", "\n", "        ", "if", "isinstance", "(", "token_ids", ",", "int", ")", ":", "\n", "            ", "token_ids", "=", "[", "token_ids", "]", "\n", "", "all_but_token_ids_mask", "=", "torch", ".", "tensor", "(", "\n", "[", "x", "for", "x", "in", "range", "(", "self", ".", "vocab_size", ")", "if", "x", "not", "in", "token_ids", "]", ",", "\n", "dtype", "=", "torch", ".", "long", ",", "\n", "device", "=", "next", "(", "self", ".", "parameters", "(", ")", ")", ".", "device", ",", "\n", ")", "\n", "assert", "(", "\n", "len", "(", "scores", ".", "shape", ")", "==", "2", "\n", ")", ",", "\"scores should be of rank 2 with shape: [batch_size, vocab_size]\"", "\n", "scores", "[", ":", ",", "all_but_token_ids_mask", "]", "=", "-", "float", "(", "\"inf\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.MeetingNet_Transformer.MeetingNet_Transformer._reorder_cache": [[1239, 1254], ["tuple", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "reordered_past.append", "layer_past[].unsqueeze().clone().detach", "layer_past[].unsqueeze().clone", "layer_past[].unsqueeze"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "_reorder_cache", "(", "past", ",", "beam_idx", ")", ":", "\n", "        ", "reordered_past", "=", "[", "]", "\n", "for", "layer_past", "in", "past", ":", "\n", "# get the correct batch idx from layer past batch dim", "\n", "# batch dim of `past` and `mems` is at 2nd position", "\n", "            ", "reordered_layer_past", "=", "[", "\n", "layer_past", "[", "i", ",", ":", "]", ".", "unsqueeze", "(", "0", ")", ".", "clone", "(", ")", ".", "detach", "(", ")", "for", "i", "in", "beam_idx", "\n", "]", "\n", "reordered_layer_past", "=", "torch", ".", "cat", "(", "reordered_layer_past", ",", "dim", "=", "0", ")", "\n", "# check that shape matches", "\n", "assert", "reordered_layer_past", ".", "shape", "==", "layer_past", ".", "shape", "\n", "reordered_past", ".", "append", "(", "reordered_layer_past", ")", "\n", "", "past", "=", "tuple", "(", "reordered_past", ")", "\n", "return", "past", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.MeetingNet_Transformer.MeetingTransformerEncoder.__init__": [[1267, 1274], ["torch.Module.__init__", "int", "int", "summertime.model.third_party.HMNet.Models.Networks.Transformer.EncoderBlock", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "copy.deepcopy", "range"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.tests.model_selector_test.ToyModel.__init__"], ["def", "__init__", "(", "self", ",", "opt", ",", "transformer_embed_dim", ")", ":", "\n", "        ", "super", "(", "MeetingTransformerEncoder", ",", "self", ")", ".", "__init__", "(", ")", "\n", "vocab", "=", "int", "(", "opt", "[", "\"vocab_size\"", "]", ")", "\n", "n_layer", "=", "int", "(", "opt", "[", "\"TRANSFORMER_LAYER\"", "]", ")", "\n", "opt", "[", "\"transformer_embed_dim\"", "]", "=", "transformer_embed_dim", "\n", "block", "=", "EncoderBlock", "(", "opt", ")", "\n", "self", ".", "blocks", "=", "nn", ".", "ModuleList", "(", "[", "copy", ".", "deepcopy", "(", "block", ")", "for", "_", "in", "range", "(", "n_layer", ")", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.MeetingNet_Transformer.MeetingTransformerEncoder.forward": [[1282, 1287], ["block"], "methods", ["None"], ["def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "h", "=", "x", "\n", "for", "block", "in", "self", ".", "blocks", ":", "\n", "            ", "h", "=", "block", "(", "h", ",", "None", ")", "\n", "", "return", "h", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.MeetingNet_Transformer.MeetingDecoderBlock.__init__": [[1295, 1308], ["torch.Module.__init__", "summertime.model.third_party.HMNet.Models.Networks.Transformer.Splitter", "summertime.model.third_party.HMNet.Models.Networks.Transformer.Attention", "summertime.model.third_party.HMNet.Models.Networks.Transformer.Attention", "summertime.model.third_party.HMNet.Models.Networks.Transformer.Attention", "summertime.model.third_party.HMNet.Models.Networks.Transformer.LayerNorm", "summertime.model.third_party.HMNet.Models.Networks.Transformer.LayerNorm", "summertime.model.third_party.HMNet.Models.Networks.Transformer.MLP", "summertime.model.third_party.HMNet.Models.Networks.Transformer.LayerNorm", "summertime.model.third_party.HMNet.Models.Networks.Transformer.LayerNorm"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.tests.model_selector_test.ToyModel.__init__"], ["    ", "def", "__init__", "(", "self", ",", "opt", ",", "n_state", ")", ":", "\n", "        ", "super", "(", "MeetingDecoderBlock", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "opt", "=", "opt", "\n", "self", ".", "decoder_splitter", "=", "Splitter", "(", "n_state", ")", "\n", "self", ".", "attn", "=", "Attention", "(", "n_state", ",", "opt", ")", "\n", "self", ".", "token_attn", "=", "Attention", "(", "n_state", ",", "opt", ")", "\n", "self", ".", "sent_attn", "=", "Attention", "(", "n_state", ",", "opt", ")", "\n", "self", ".", "ln_1", "=", "LayerNorm", "(", "n_state", ")", "\n", "self", ".", "ln_2", "=", "LayerNorm", "(", "n_state", ")", "\n", "opt", "[", "\"transformer_embed_dim\"", "]", "=", "n_state", "\n", "self", ".", "mlp", "=", "MLP", "(", "4", "*", "n_state", ",", "opt", ")", "\n", "self", ".", "ln_3", "=", "LayerNorm", "(", "n_state", ")", "\n", "self", ".", "ln_4", "=", "LayerNorm", "(", "n_state", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.MeetingNet_Transformer.MeetingDecoderBlock.forward": [[1320, 1347], ["MeetingNet_Transformer.MeetingDecoderBlock.decoder_splitter", "MeetingNet_Transformer.MeetingDecoderBlock.attn", "MeetingNet_Transformer.MeetingDecoderBlock.ln_1", "MeetingNet_Transformer.MeetingDecoderBlock.token_attn", "MeetingNet_Transformer.MeetingDecoderBlock.ln_2", "MeetingNet_Transformer.MeetingDecoderBlock.mlp", "MeetingNet_Transformer.MeetingDecoderBlock.ln_4", "MeetingNet_Transformer.MeetingDecoderBlock.sent_attn", "MeetingNet_Transformer.MeetingDecoderBlock.ln_3"], "methods", ["None"], ["def", "forward", "(", "self", ",", "y", ",", "token_enc_key", ",", "token_enc_value", ",", "sent_enc_key", ",", "sent_enc_value", ")", ":", "\n", "        ", "query", ",", "key", ",", "value", "=", "self", ".", "decoder_splitter", "(", "y", ")", "\n", "# batch x len x n_state", "\n", "\n", "# self-attention", "\n", "a", "=", "self", ".", "attn", "(", "query", ",", "key", ",", "value", ",", "None", ",", "one_dir_visible", "=", "True", ")", "\n", "# batch x len x n_state", "\n", "\n", "n", "=", "self", ".", "ln_1", "(", "y", "+", "a", ")", "# residual", "\n", "\n", "if", "\"NO_HIERARCHY\"", "in", "self", ".", "opt", ":", "\n", "            ", "q", "=", "y", "\n", "r", "=", "n", "\n", "", "else", ":", "\n", "# src-tgt attention on sentences", "\n", "            ", "q", "=", "self", ".", "sent_attn", "(", "n", ",", "sent_enc_key", ",", "sent_enc_value", ",", "None", ")", "\n", "r", "=", "self", ".", "ln_3", "(", "n", "+", "q", ")", "# residual", "\n", "# batch x len x n_state", "\n", "\n", "# src-tgt attention on tokens", "\n", "", "o", "=", "self", ".", "token_attn", "(", "r", ",", "token_enc_key", ",", "token_enc_value", ",", "None", ")", "\n", "p", "=", "self", ".", "ln_2", "(", "r", "+", "o", ")", "# residual", "\n", "# batch x len x n_state", "\n", "\n", "m", "=", "self", ".", "mlp", "(", "p", ")", "\n", "h", "=", "self", ".", "ln_4", "(", "p", "+", "m", ")", "\n", "return", "h", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.MeetingNet_Transformer.MeetingTransformerDecoder.__init__": [[1362, 1374], ["torch.Module.__init__", "int", "int", "summertime.model.third_party.HMNet.Models.Networks.Transformer.Splitter", "MeetingNet_Transformer.MeetingDecoderBlock", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "copy.deepcopy", "range"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.tests.model_selector_test.ToyModel.__init__"], ["def", "__init__", "(", "self", ",", "opt", ",", "embedder", ",", "embed_size", ",", "token_dim", ",", "sent_dim", ")", ":", "\n", "        ", "super", "(", "MeetingTransformerDecoder", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "fp16", "=", "\"FP16\"", "in", "opt", "\n", "vocab_size", "=", "int", "(", "opt", "[", "\"vocab_size\"", "]", ")", "\n", "n_layer", "=", "int", "(", "opt", "[", "\"TRANSFORMER_LAYER\"", "]", ")", "\n", "self", ".", "encoder_splitter", "=", "Splitter", "(", "embed_size", ")", "\n", "block", "=", "MeetingDecoderBlock", "(", "opt", ",", "embed_size", ")", "\n", "self", ".", "token_linear", "=", "nn", ".", "Linear", "(", "token_dim", ",", "embed_size", ")", "\n", "self", ".", "sent_linear", "=", "nn", ".", "Linear", "(", "sent_dim", ",", "embed_size", ")", "\n", "self", ".", "blocks", "=", "nn", ".", "ModuleList", "(", "[", "copy", ".", "deepcopy", "(", "block", ")", "for", "_", "in", "range", "(", "n_layer", ")", "]", ")", "\n", "self", ".", "linear", "=", "nn", ".", "Linear", "(", "embed_size", ",", "vocab_size", ",", "bias", "=", "False", ")", "\n", "self", ".", "linear", ".", "weight", "=", "embedder", ".", "embed", ".", "weight", "# share weight", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.MeetingNet_Transformer.MeetingTransformerDecoder.forward": [[1384, 1402], ["MeetingNet_Transformer.MeetingTransformerDecoder.encoder_splitter", "MeetingNet_Transformer.MeetingTransformerDecoder.encoder_splitter", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "MeetingNet_Transformer.MeetingTransformerDecoder.token_linear", "MeetingNet_Transformer.MeetingTransformerDecoder.sent_linear", "block", "MeetingNet_Transformer.MeetingTransformerDecoder.linear"], "methods", ["None"], ["def", "forward", "(", "self", ",", "token_encoder_inputs", ",", "sent_encoder_inputs", ",", "decoder_input_ids", ")", ":", "\n", "        ", "_", ",", "token_enc_key", ",", "token_enc_value", "=", "self", ".", "encoder_splitter", "(", "\n", "self", ".", "token_linear", "(", "token_encoder_inputs", ")", "\n", ")", "\n", "# token_enc_key: batch x encoder_len x n_state", "\n", "# token_enc_value: batch x encoder_len x n_state", "\n", "\n", "_", ",", "sent_enc_key", ",", "sent_enc_value", "=", "self", ".", "encoder_splitter", "(", "\n", "self", ".", "sent_linear", "(", "sent_encoder_inputs", ")", "\n", ")", "\n", "# sent_enc_key: batch x encoder_len x n_state", "\n", "# sent_enc_value: batch x encoder_len x n_state", "\n", "\n", "h", "=", "decoder_input_ids", "\n", "for", "block", "in", "self", ".", "blocks", ":", "\n", "            ", "h", "=", "block", "(", "h", ",", "token_enc_key", ",", "token_enc_value", ",", "sent_enc_key", ",", "sent_enc_value", ")", "\n", "", "prob", "=", "F", ".", "softmax", "(", "self", ".", "linear", "(", "h", ")", ",", "dim", "=", "-", "1", ")", "\n", "return", "prob", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.MeetingNet_Transformer.Encoder.__init__": [[1413, 1441], ["torch.Module.__init__", "summertime.model.third_party.HMNet.Models.Networks.Layers.set_seq_dropout", "MeetingNet_Transformer.MeetingTransformerEncoder", "MeetingNet_Transformer.MeetingTransformerEncoder", "print", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.Embedding", "print", "len", "len"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.tests.model_selector_test.ToyModel.__init__", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.Layers.set_seq_dropout"], ["def", "__init__", "(", "self", ",", "opt", ",", "vocab_size", ",", "embed_size", ",", "role_dim", ",", "embedder", ",", "role_embed", ")", ":", "\n", "        ", "super", "(", "Encoder", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "opt", "=", "opt", "\n", "self", ".", "vocab_size", "=", "vocab_size", "\n", "\n", "set_seq_dropout", "(", "\"VARIATIONAL_DROPOUT\"", "in", "self", ".", "opt", ")", "\n", "\n", "self", ".", "embed_size", "=", "embed_size", "\n", "self", ".", "embedder", "=", "embedder", "\n", "self", ".", "role_embed", "=", "role_embed", "\n", "\n", "self", ".", "token_transformer_dim", "=", "embed_size", "\n", "if", "\"USE_POSENT\"", "in", "opt", ":", "\n", "            ", "print", "(", "\"Use POS and ENT\"", ")", "\n", "pos_dim", "=", "opt", "[", "\"POS_DIM\"", "]", "\n", "ent_dim", "=", "opt", "[", "\"ENT_DIM\"", "]", "\n", "self", ".", "pos_embed", "=", "nn", ".", "Embedding", "(", "len", "(", "POS", ")", ",", "pos_dim", ")", "\n", "self", ".", "ent_embed", "=", "nn", ".", "Embedding", "(", "len", "(", "ENT", ")", ",", "ent_dim", ")", "\n", "self", ".", "token_transformer_dim", "+=", "pos_dim", "+", "ent_dim", "\n", "\n", "", "self", ".", "sent_transformer_dim", "=", "self", ".", "token_transformer_dim", "\n", "if", "\"USE_ROLE\"", "in", "opt", ":", "\n", "            ", "print", "(", "\"USE_ROLE\"", ")", "\n", "role_dim", "=", "opt", "[", "\"ROLE_DIM\"", "]", "\n", "self", ".", "sent_transformer_dim", "+=", "role_dim", "\n", "\n", "", "self", ".", "token_encoder", "=", "MeetingTransformerEncoder", "(", "opt", ",", "self", ".", "token_transformer_dim", ")", "\n", "self", ".", "sent_encoder", "=", "MeetingTransformerEncoder", "(", "opt", ",", "self", ".", "sent_transformer_dim", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.MeetingNet_Transformer.Encoder.forward": [[1452, 1499], ["x.size", "x.size", "x.size", "x.clone", "MeetingNet_Transformer.Encoder.embedder", "torch.cat.view", "torch.cat.view", "torch.cat.view", "torch.cat.view", "torch.cat.size", "torch.cat.size", "torch.cat.size", "torch.cat.size", "MeetingNet_Transformer.Encoder.token_encoder", "token_transformer_output.view.view.size", "token_transformer_output.view.view.view", "MeetingNet_Transformer.Encoder.sent_encoder", "token_transformer_output.view.view.view", "x.clone.view", "MeetingNet_Transformer.Encoder.role_embed", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat.view", "torch.cat.view", "torch.cat.view", "torch.cat.view", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "MeetingNet_Transformer.Encoder.pos_embed", "MeetingNet_Transformer.Encoder.ent_embed"], "methods", ["None"], ["def", "forward", "(", "self", ",", "x", ",", "x_role", ",", "x_pos", ",", "x_ent", ")", ":", "\n", "        ", "batch_size", "=", "x", ".", "size", "(", "0", ")", "\n", "sent_num", "=", "x", ".", "size", "(", "1", ")", "\n", "x_len", "=", "x", ".", "size", "(", "2", ")", "\n", "\n", "# x contains word id >= vocab_size", "\n", "vocab_x", "=", "x", ".", "clone", "(", ")", "\n", "vocab_x", "[", "vocab_x", ">=", "self", ".", "vocab_size", "]", "=", "1", "# UNK", "\n", "embedded", "=", "self", ".", "embedder", "(", "vocab_x", ".", "view", "(", "batch_size", ",", "-", "1", ")", ")", "\n", "# embedded = 1 x sent_num * x_len x embed_size", "\n", "embedded", "=", "embedded", ".", "view", "(", "batch_size", ",", "sent_num", ",", "x_len", ",", "-", "1", ")", "\n", "# embedded = 1 x sent_num x x_len x embed_size", "\n", "\n", "if", "\"USE_ROLE\"", "in", "self", ".", "opt", ":", "\n", "            ", "role_embed", "=", "self", ".", "role_embed", "(", "x_role", ")", "# 1 x sent_num x role_dim", "\n", "\n", "", "if", "\"USE_POSENT\"", "in", "self", ".", "opt", ":", "\n", "            ", "embedded", "=", "torch", ".", "cat", "(", "\n", "[", "embedded", ",", "self", ".", "pos_embed", "(", "x_pos", ")", ",", "self", ".", "ent_embed", "(", "x_ent", ")", "]", ",", "dim", "=", "3", "\n", ")", "\n", "# 1 x sent_num x x_len x (embed_size + pos_dim + ent_dim )", "\n", "\n", "", "feat_dim", "=", "embedded", ".", "size", "(", "3", ")", "\n", "\n", "token_transformer_output", "=", "self", ".", "token_encoder", "(", "\n", "embedded", ".", "view", "(", "-", "1", ",", "x_len", ",", "feat_dim", ")", "\n", ")", "\n", "token_transformer_dim", "=", "token_transformer_output", ".", "size", "(", "2", ")", "\n", "token_transformer_output", "=", "token_transformer_output", ".", "view", "(", "\n", "batch_size", ",", "sent_num", ",", "x_len", ",", "token_transformer_dim", "\n", ")", "\n", "# 1 x sent_num x x_len x token_transformer_dim", "\n", "\n", "sent_encoder_inputs", "=", "token_transformer_output", "[", "\n", ":", ",", ":", ",", "0", ",", ":", "\n", "]", "# 1 x sent_num x token_transformer_dim", "\n", "if", "\"USE_ROLE\"", "in", "self", ".", "opt", ":", "\n", "            ", "sent_encoder_inputs", "=", "torch", ".", "cat", "(", "[", "sent_encoder_inputs", ",", "role_embed", "]", ",", "dim", "=", "2", ")", "\n", "", "sent_encoder_outputs", "=", "self", ".", "sent_encoder", "(", "\n", "sent_encoder_inputs", "\n", ")", "# 1 x sent_num x sent_transformer_dim", "\n", "\n", "token_transformer_output", "=", "token_transformer_output", ".", "view", "(", "\n", "batch_size", ",", "-", "1", ",", "token_transformer_dim", "\n", ")", "\n", "\n", "return", "token_transformer_output", ",", "sent_encoder_outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.MeetingNet_Transformer.Decoder.__init__": [[1502, 1518], ["torch.Module.__init__", "MeetingNet_Transformer.MeetingTransformerDecoder"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.tests.model_selector_test.ToyModel.__init__"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "opt", ",", "\n", "embed_size", ",", "\n", "vocab_size", ",", "\n", "embedder", ",", "\n", "token_transformer_dim", ",", "\n", "sent_transformer_dim", ",", "\n", ")", ":", "\n", "        ", "super", "(", "Decoder", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "opt", "=", "opt", "\n", "self", ".", "embed_size", "=", "embed_size", "\n", "self", ".", "vocab_size", "=", "vocab_size", "\n", "self", ".", "embedder", "=", "embedder", "\n", "self", ".", "sent_decoder", "=", "MeetingTransformerDecoder", "(", "\n", "opt", ",", "embedder", ",", "embed_size", ",", "token_transformer_dim", ",", "sent_transformer_dim", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.MeetingNet_Transformer.Decoder.forward": [[1520, 1532], ["decoder_input_ids.clone", "MeetingNet_Transformer.Decoder.embedder", "MeetingNet_Transformer.Decoder.sent_decoder", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.DistributedTrainer.DistributedTrainer.log", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.DistributedTrainer.DistributedTrainer.log", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.DistributedTrainer.DistributedTrainer.log", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.DistributedTrainer.DistributedTrainer.log", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.DistributedTrainer.DistributedTrainer.log", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.DistributedTrainer.DistributedTrainer.log", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.DistributedTrainer.DistributedTrainer.log", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.DistributedTrainer.DistributedTrainer.log", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.DistributedTrainer.DistributedTrainer.log", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.DistributedTrainer.DistributedTrainer.log", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.DistributedTrainer.DistributedTrainer.log", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.DistributedTrainer.DistributedTrainer.log", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.DistributedTrainer.DistributedTrainer.log", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.DistributedTrainer.DistributedTrainer.log", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.DistributedTrainer.DistributedTrainer.log", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.DistributedTrainer.DistributedTrainer.log"], ["", "def", "forward", "(", "self", ",", "token_encoder_outputs", ",", "sent_encoder_outputs", ",", "decoder_input_ids", ")", ":", "\n", "        ", "vocab_y", "=", "decoder_input_ids", ".", "clone", "(", ")", "\n", "vocab_y", "[", "vocab_y", ">=", "self", ".", "vocab_size", "]", "=", "1", "# UNK", "\n", "embedded", "=", "self", ".", "embedder", "(", "vocab_y", ")", "\n", "\n", "vocab_prob", "=", "self", ".", "sent_decoder", "(", "\n", "token_encoder_outputs", ",", "sent_encoder_outputs", ",", "embedded", "\n", ")", "\n", "# vocab_prob: batch x y_len x vocab_size", "\n", "\n", "vocab_logprob", "=", "torch", ".", "log", "(", "vocab_prob", "+", "1e-15", ")", "\n", "return", "vocab_logprob", "\n", "", "", ""]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Criteria.MLECriterion.MLECriterion.__init__": [[15, 22], ["torch.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.tests.model_selector_test.ToyModel.__init__"], ["def", "__init__", "(", "self", ",", "opt", ",", "module", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "opt", "=", "opt", "\n", "self", ".", "ignore_index", "=", "(", "\n", "self", ".", "opt", "[", "\"IGNORE_INDEX\"", "]", "\n", "if", "\"IGNORE_INDEX\"", "in", "self", ".", "opt", "\n", "else", "module", ".", "tokenizer", ".", "pad_token_id", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Criteria.MLECriterion.MLECriterion.forward": [[24, 41], ["torch.nll_loss", "torch.nll_loss", "torch.nll_loss", "vocab_logprob.contiguous().view", "y.contiguous().view", "vocab_logprob.contiguous", "y.contiguous"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "vocab_logprob", ",", "batch", ")", ":", "\n", "        ", "extended_vocab_size", "=", "vocab_logprob", ".", "shape", "[", "2", "]", "\n", "y", "=", "batch", "[", "\"decoder_input_ids\"", "]", "\n", "\n", "if", "\"USE_BOS_TOKEN\"", "in", "self", ".", "opt", ":", "\n", "            ", "y", "=", "y", "[", ":", ",", "1", ":", "]", "\n", "\n", "", "if", "\"USE_EOS_TOKEN\"", "in", "self", ".", "opt", ":", "\n", "            ", "vocab_logprob", "=", "vocab_logprob", "[", ":", ",", ":", "-", "1", ",", ":", "]", "\n", "\n", "", "loss", "=", "F", ".", "nll_loss", "(", "\n", "vocab_logprob", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ",", "extended_vocab_size", ")", ",", "\n", "y", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ")", ",", "\n", "ignore_index", "=", "self", ".", "ignore_index", ",", "\n", ")", "\n", "\n", "return", "loss", "\n", "", "", ""]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.iterators.CheckpointableIterator.__iter__": [[263, 265], ["None"], "methods", ["None"], ["def", "__iter__", "(", "self", ")", ":", "\n", "        ", "return", "self", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.iterators.CheckpointableIterator.getstate": [[266, 280], ["None"], "methods", ["None"], ["", "@", "abstractmethod", "\n", "def", "getstate", "(", "self", ")", "->", "Dict", ":", "\n", "        ", "\"\"\"\n        Get checkpoint of current state of iterator\n\n        In a pipeline of iterators, this function __recursively__ calls itself on the preceeding iterator\n        and includes the gathered information in the returned checkpoint.\n        Thereby, to obtain a checkpoint of the state of an entire pipeline of iterators\n        you only have to call this function on the __last__ iterator in the pipeline.\n        A checkpoint is represented as a `dict`,\n        but the caller should treat a checkpoint as an opaque object\n        and not make any assumptions about the existence or meaning of the `dict` entries.\n        \"\"\"", "\n", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.iterators.CheckpointableIterator.setstate": [[281, 296], ["None"], "methods", ["None"], ["", "@", "abstractmethod", "\n", "def", "setstate", "(", "self", ",", "checkpoint", ":", "Optional", "[", "Dict", "]", ")", ":", "\n", "        ", "\"\"\"\n        Set state of iterator to given checkpoint\n\n        In a pipeline of iterators, this function __recursively__ calls itself on the preceeding iterator.\n        Thereby, to set the state of an entire pipeline of iterators to a given checkpoint\n        you only have to call this function on the __last__ iterator in the pipeline.\n\n        Args:\n            checkpoint: Checkpoint that should be used to reset the state of the iterator (or pipeline).\n                        If this is __None__, the state of the iterator (or pipeline) is reset to the initial\n                        state immediately after construction.\n        \"\"\"", "\n", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.iterators.CheckpointableIterator.__getstate__": [[297, 299], ["iterators.CheckpointableIterator.getstate"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.iterators.BucketedReadaheadBatchIterator.getstate"], ["", "def", "__getstate__", "(", "self", ")", "->", "Dict", ":", "# implementation of pickle Protocol", "\n", "        ", "return", "self", ".", "getstate", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.iterators.CheckpointableIterator.__setstate__": [[300, 302], ["iterators.CheckpointableIterator.setstate"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.iterators.BucketedReadaheadBatchIterator.setstate"], ["", "def", "__setstate__", "(", "self", ",", "checkpoint", ":", "Optional", "[", "Dict", "]", ")", ":", "\n", "        ", "self", ".", "setstate", "(", "checkpoint", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.iterators.CheckpointableIterator.__next__": [[303, 306], ["None"], "methods", ["None"], ["", "@", "abstractmethod", "\n", "def", "__next__", "(", "self", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.iterators.NativeCheckpointableIterator.__init__": [[318, 328], ["iterators.NativeCheckpointableIterator.setstate", "iter", "ValueError"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.iterators.BucketedReadaheadBatchIterator.setstate"], ["def", "__init__", "(", "self", ",", "iterable", ":", "Iterable", ")", ":", "\n", "# check whether iterable is iterable or iterator:", "\n", "# if the variable iterable contains an iterator, the function __iter__ returns self", "\n", "# if the variable iterable is an actual iterator, it should not return self", "\n", "        ", "if", "iter", "(", "iterable", ")", "is", "iterable", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"It looks like you are passing an iterator instead of an iterable. This is not supported and can cause undefined behavior when used with checkpointing.\"", "\n", ")", "\n", "", "self", ".", "_input_iterable", "=", "iterable", "\n", "self", ".", "setstate", "(", "None", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.iterators.NativeCheckpointableIterator.getstate": [[329, 331], ["None"], "methods", ["None"], ["", "def", "getstate", "(", "self", ")", "->", "Dict", ":", "\n", "        ", "return", "{", "\"num_items_yielded\"", ":", "self", ".", "_num_items_yielded", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.iterators.NativeCheckpointableIterator.setstate": [[332, 338], ["iter", "iterators._advance_iterator"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.iterators._advance_iterator"], ["", "def", "setstate", "(", "self", ",", "checkpoint", ":", "Optional", "[", "Dict", "]", ")", ":", "\n", "        ", "self", ".", "_iterator", "=", "iter", "(", "self", ".", "_input_iterable", ")", "\n", "self", ".", "_num_items_yielded", "=", "(", "\n", "_advance_iterator", "(", "self", ".", "_iterator", ",", "checkpoint", "[", "\"num_items_yielded\"", "]", ")", "\n", "if", "checkpoint", "is", "not", "None", "\n", "else", "0", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.iterators.NativeCheckpointableIterator.__next__": [[340, 346], ["next"], "methods", ["None"], ["", "def", "__next__", "(", "self", ")", ":", "\n", "        ", "item", "=", "next", "(", "\n", "self", ".", "_iterator", "\n", ")", "# call this before increasing _num_items_yielded to correctly handle the case when a StopIteration exception is thrown", "\n", "self", ".", "_num_items_yielded", "+=", "1", "\n", "return", "item", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.iterators.InfinitePermutationSourceIterator.__init__": [[408, 432], ["iterators.InfinitePermutationSourceIterator.setstate", "ValueError"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.iterators.BucketedReadaheadBatchIterator.setstate"], ["def", "__init__", "(", "\n", "self", ",", "\n", "source_items", ":", "List", ",", "\n", "seed", ":", "Optional", "[", "int", "]", "=", "None", ",", "\n", "shuffle", ":", "bool", "=", "True", ",", "\n", "num_instances", ":", "int", "=", "1", ",", "\n", "instance_rank", ":", "int", "=", "0", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            source_items: input list, must not be empty and must be small enough to fit into RAM entirely, ownership of the list and the data goes to the iterator, do not modify it!\n            seed: random seed used for shuffling (or None)\n            shuffle: set False to bypass the shuffling. Then this is just a checkpointed version of itertools.cycle(). (Default: True)\n            num_instances: number of instances of this iterator. Meant for use with multi-process data loading, e.g., in distributed training.\n            instance_rank: rank of this instance of the iterator. Meant for use with multi-process data loading, e.g., in distributed training.\n        \"\"\"", "\n", "self", ".", "_source_items", "=", "source_items", "\n", "if", "not", "self", ".", "_source_items", ":", "\n", "            ", "raise", "ValueError", "(", "\"InfinitePermutationIterator: source must not be empty\"", ")", "\n", "", "self", ".", "_shuffle", "=", "shuffle", "\n", "self", ".", "_seed", "=", "seed", "\n", "self", ".", "_num_instances", "=", "num_instances", "\n", "self", ".", "_instance_rank", "=", "instance_rank", "\n", "self", ".", "setstate", "(", "None", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.iterators.InfinitePermutationSourceIterator.getstate": [[433, 437], ["None"], "methods", ["None"], ["", "def", "getstate", "(", "self", ")", "->", "Dict", ":", "\n", "        ", "return", "{", "\n", "\"random_state\"", ":", "self", ".", "_random_state", ",", "# state of random generator before generating the current shuffling of the sequence", "\n", "\"num_items_yielded\"", ":", "self", ".", "_num_items_yielded", ",", "\n", "}", "# how many items have already been iterated over in the current shuffling", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.iterators.InfinitePermutationSourceIterator.setstate": [[439, 483], ["iterators.InfinitePermutationSourceIterator.setstate._generate"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.iterators.FixedBatchIterator._generate"], ["", "def", "setstate", "(", "self", ",", "checkpoint", ":", "Optional", "[", "Dict", "]", ")", ":", "\n", "# set iteration state. Do this outside the generator below in case getstate() is called before ever iterating", "\n", "        ", "self", ".", "_random_state", "=", "checkpoint", "[", "\"random_state\"", "]", "if", "checkpoint", "else", "None", "\n", "self", ".", "_num_items_yielded", "=", "checkpoint", "[", "\"num_items_yielded\"", "]", "if", "checkpoint", "else", "0", "\n", "# We define the iteration itself as a generator for ease of implementation.", "\n", "# We could as well just have used an explicit state machine represented by class members.", "\n", "def", "_generate", "(", ")", "->", "Iterator", ":", "\n", "# create and reset random generator", "\n", "            ", "random", "=", "Random", "(", "self", ".", "_seed", ")", "\n", "if", "self", ".", "_random_state", "is", "not", "None", ":", "# restore the random generator's state", "\n", "                ", "random", ".", "setstate", "(", "self", ".", "_random_state", ")", "\n", "", "skip_to_checkpoint", "=", "(", "\n", "self", ".", "_num_items_yielded", "\n", ")", "# items to skip in order to advance to checkpoint", "\n", "# main outer loop for infinite passes over items (reshuffle before each pass)", "\n", "while", "True", ":", "\n", "# (re-)shuffle all items", "\n", "                ", "self", ".", "_random_state", "=", "(", "\n", "random", ".", "getstate", "(", ")", "\n", ")", "# remember random state before shuffling", "\n", "self", ".", "_num_items_yielded", "=", "0", "\n", "shuffled_items", "=", "self", ".", "_source_items", "[", "\n", ":", "\n", "]", "# note: if underlying iterator is checkpointable, use setstate(checkpoint['nested_state']) on it", "\n", "if", "self", ".", "_shuffle", ":", "\n", "                    ", "random", ".", "shuffle", "(", "shuffled_items", ")", "\n", "", "shuffled_iterator", "=", "iter", "(", "shuffled_items", ")", "\n", "# skip initial items when restarting from checkpoint", "\n", "if", "(", "\n", "skip_to_checkpoint", "\n", ")", ":", "# @TODO: find a way to abstract this more, so that we can plug it into the 'for' statement directly", "\n", "                    ", "self", ".", "_num_items_yielded", "+=", "_advance_iterator", "(", "\n", "shuffled_iterator", ",", "skip_to_checkpoint", "\n", ")", "\n", "skip_to_checkpoint", "=", "0", "# done skipping", "\n", "# main inner loop over items", "\n", "", "for", "item", "in", "shuffled_iterator", ":", "\n", "                    ", "self", ".", "_num_items_yielded", "+=", "1", "# record how many items we have iterated over in this pass over the items", "\n", "if", "(", "\n", "self", ".", "_num_items_yielded", "-", "1", "\n", ")", "%", "self", ".", "_num_instances", "==", "self", ".", "_instance_rank", ":", "# build-in islice facility", "\n", "                        ", "yield", "item", "\n", "\n", "", "", "", "", "self", ".", "_iterator", "=", "_generate", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.iterators.InfinitePermutationSourceIterator.__next__": [[484, 486], ["next"], "methods", ["None"], ["", "def", "__next__", "(", "self", ")", ":", "\n", "        ", "return", "next", "(", "self", ".", "_iterator", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.iterators.SelectManyIterator.__init__": [[493, 513], ["iterators.SelectManyIterator.setstate", "isinstance", "ValueError"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.iterators.BucketedReadaheadBatchIterator.setstate"], ["def", "__init__", "(", "\n", "self", ",", "\n", "source_iterator", ":", "CheckpointableIterator", ",", "\n", "collection_selector", ":", "Optional", "[", "Callable", "[", "[", "Any", "]", ",", "Iterator", "]", "]", "=", "None", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            source_iterator: iterator over the items to pass to collection_selector()\n            collection_selector: user callback that maps an item into an Iterable, whose items will be yielded.\n                                 The returned Iterator is used only once. Hence, it is also allowed to\n                                 return self-iterables, such as iterators and generator expressions.\n                                 If None is given, no callback is applied.\n        \"\"\"", "\n", "if", "not", "isinstance", "(", "source_iterator", ",", "CheckpointableIterator", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\"source_iterator has to be a CheckpointableIterator\"", ")", "\n", "", "self", ".", "_source_iterator", "=", "source_iterator", "# type: CheckpointableIterator", "\n", "self", ".", "_collection_selector", "=", "(", "\n", "collection_selector", "\n", ")", "# type: Callable[[Any], Iterator]", "\n", "self", ".", "setstate", "(", "None", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.iterators.SelectManyIterator.getstate": [[514, 518], ["None"], "methods", ["None"], ["", "def", "getstate", "(", "self", ")", "->", "Dict", ":", "\n", "        ", "return", "{", "\n", "\"source_state\"", ":", "self", ".", "_source_state", ",", "\n", "\"flattened_items_yielded\"", ":", "self", ".", "_flattened_items_yielded", ",", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.iterators.SelectManyIterator.setstate": [[520, 549], ["iterators.SelectManyIterator._source_iterator.setstate", "iterators.SelectManyIterator.setstate._generate"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.iterators.BucketedReadaheadBatchIterator.setstate", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.iterators.FixedBatchIterator._generate"], ["", "def", "setstate", "(", "self", ",", "checkpoint", ":", "Optional", "[", "Dict", "]", ")", ":", "\n", "        ", "self", ".", "_source_state", "=", "checkpoint", "[", "\"source_state\"", "]", "if", "checkpoint", "else", "None", "\n", "self", ".", "_flattened_items_yielded", "=", "(", "\n", "checkpoint", "[", "\"flattened_items_yielded\"", "]", "if", "checkpoint", "else", "0", "\n", ")", "\n", "self", ".", "_source_iterator", ".", "setstate", "(", "self", ".", "_source_state", ")", "\n", "\n", "def", "_generate", "(", ")", ":", "\n", "            ", "skip_to_checkpoint", "=", "self", ".", "_flattened_items_yielded", "\n", "# main loop over source source_items", "\n", "for", "source_item", "in", "self", ".", "_source_iterator", ":", "\n", "                ", "if", "self", ".", "_collection_selector", "is", "not", "None", ":", "\n", "                    ", "data", "=", "iter", "(", "self", ".", "_collection_selector", "(", "source_item", ")", ")", "\n", "", "else", ":", "\n", "                    ", "data", "=", "iter", "(", "source_item", ")", "\n", "", "self", ".", "_flattened_items_yielded", "=", "0", "\n", "if", "skip_to_checkpoint", ":", "\n", "# print(\"Skipping to index\", skip_to_checkpoint, file=sys.stderr)", "\n", "                    ", "self", ".", "_flattened_items_yielded", "+=", "_advance_iterator", "(", "\n", "data", ",", "skip_to_checkpoint", "\n", ")", "\n", "skip_to_checkpoint", "=", "0", "\n", "# main loop over lines", "\n", "", "for", "item", "in", "data", ":", "\n", "                    ", "self", ".", "_flattened_items_yielded", "+=", "1", "\n", "yield", "item", "\n", "", "self", ".", "_source_state", "=", "self", ".", "_source_iterator", ".", "getstate", "(", ")", "\n", "\n", "", "", "self", ".", "_iterator", "=", "_generate", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.iterators.SelectManyIterator.__next__": [[550, 552], ["next"], "methods", ["None"], ["", "def", "__next__", "(", "self", ")", ":", "\n", "        ", "return", "next", "(", "self", ".", "_iterator", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.iterators.BufferedShuffleIterator.__init__": [[559, 576], ["random.Random", "iterators.BufferedShuffleIterator.setstate", "isinstance", "ValueError", "range"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.iterators.BucketedReadaheadBatchIterator.setstate"], ["def", "__init__", "(", "\n", "self", ",", "source_iterator", ":", "CheckpointableIterator", ",", "buffer_size", ":", "int", ",", "seed", ":", "int", "=", "0", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            source_iterator: checkpointable iterator or restartable iterable over input items to shuffle\n            buffer_size: size of the buffer in number of items used for shuffling\n            seed: random seed used for shuffling (or None)\n        \"\"\"", "\n", "if", "not", "isinstance", "(", "source_iterator", ",", "CheckpointableIterator", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\"source_iterator has to be a CheckpointableIterator\"", ")", "\n", "", "self", ".", "_source_iterator", "=", "source_iterator", "\n", "self", ".", "_buffer", "=", "[", "\n", "None", "for", "_", "in", "range", "(", "buffer_size", ")", "\n", "]", "# maybe do this lazily?   --Yes, since user may set state immediately, then this is not needed here", "\n", "self", ".", "_random", "=", "Random", "(", "seed", ")", "\n", "self", ".", "setstate", "(", "None", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.iterators.BufferedShuffleIterator.getstate": [[577, 582], ["iterators.BufferedShuffleIterator._source_iterator.getstate", "copy.deepcopy", "iterators.BufferedShuffleIterator._random.getstate"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.iterators.BucketedReadaheadBatchIterator.getstate", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.iterators.BucketedReadaheadBatchIterator.getstate"], ["", "def", "getstate", "(", "self", ")", "->", "Dict", ":", "\n", "        ", "return", "{", "\n", "\"source_state\"", ":", "self", ".", "_source_iterator", ".", "getstate", "(", ")", ",", "\n", "\"buffer\"", ":", "copy", ".", "deepcopy", "(", "self", ".", "_buffer", ")", ",", "\n", "\"random_state\"", ":", "self", ".", "_random", ".", "getstate", "(", ")", ",", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.iterators.BufferedShuffleIterator.setstate": [[584, 593], ["iterators.BufferedShuffleIterator._generate", "iterators.BufferedShuffleIterator._source_iterator.setstate", "iterators.BufferedShuffleIterator._random.setstate", "iterators.BufferedShuffleIterator._source_iterator.setstate"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.iterators.FixedBatchIterator._generate", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.iterators.BucketedReadaheadBatchIterator.setstate", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.iterators.BucketedReadaheadBatchIterator.setstate", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.iterators.BucketedReadaheadBatchIterator.setstate"], ["", "def", "setstate", "(", "self", ",", "checkpoint", ":", "Optional", "[", "Dict", "]", ")", ":", "\n", "        ", "if", "checkpoint", ":", "\n", "            ", "self", ".", "_source_iterator", ".", "setstate", "(", "checkpoint", "[", "\"source_state\"", "]", ")", "\n", "self", ".", "_buffer", "=", "checkpoint", "[", "\"buffer\"", "]", "\n", "self", ".", "_random", ".", "setstate", "(", "checkpoint", "[", "\"random_state\"", "]", ")", "\n", "# @TODO: Can we add a comment how the flush part is handled?", "\n", "", "else", ":", "\n", "            ", "self", ".", "_source_iterator", ".", "setstate", "(", "None", ")", "\n", "", "self", ".", "_iterator", "=", "self", ".", "_generate", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.iterators.BufferedShuffleIterator._generate": [[594, 616], ["iterators.BufferedShuffleIterator._random.randrange", "iterators.BufferedShuffleIterator._buffer.pop", "len"], "methods", ["None"], ["", "def", "_generate", "(", "self", ")", "->", "Iterator", ":", "\n", "# shuffle data with a buffer:", "\n", "# this is similar to what the Fisher-Yates shuffle does,", "\n", "# but modified to run with a constant-size buffer", "\n", "# see https://en.wikipedia.org/wiki/Fisher%E2%80%93Yates_shuffle", "\n", "# this was inspired by an algorithm implemented in Kaldi", "\n", "# see https://kaldi-asr.org/doc/nnet-shuffle-egs_8cc.html", "\n", "        ", "for", "item", "in", "self", ".", "_source_iterator", ":", "\n", "            ", "index", "=", "self", ".", "_random", ".", "randrange", "(", "0", ",", "len", "(", "self", ".", "_buffer", ")", ")", "\n", "result", "=", "None", "\n", "if", "self", ".", "_buffer", "[", "index", "]", "is", "not", "None", ":", "\n", "                ", "result", "=", "self", ".", "_buffer", "[", "index", "]", "\n", "", "self", ".", "_buffer", "[", "index", "]", "=", "item", "\n", "# only yield value once buffer is updated to allow for correct checkpointing!", "\n", "if", "result", "is", "not", "None", ":", "\n", "                ", "yield", "result", "\n", "\n", "# flush buffer", "\n", "", "", "while", "self", ".", "_buffer", ":", "\n", "            ", "item", "=", "self", ".", "_buffer", ".", "pop", "(", ")", "\n", "if", "item", "is", "not", "None", ":", "\n", "                ", "yield", "item", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.iterators.BufferedShuffleIterator.__next__": [[617, 619], ["next"], "methods", ["None"], ["", "", "", "def", "__next__", "(", "self", ")", ":", "\n", "        ", "return", "next", "(", "self", ".", "_iterator", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.iterators.MapIterator.__init__": [[626, 638], ["isinstance", "ValueError"], "methods", ["None"], ["def", "__init__", "(", "\n", "self", ",", "source_iterator", ":", "CheckpointableIterator", ",", "transform", ":", "Callable", "[", "[", "str", "]", ",", "Any", "]", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            source_iterator: checkpointable iterator\n            transform: function to be applied to each data item\n        \"\"\"", "\n", "if", "not", "isinstance", "(", "source_iterator", ",", "CheckpointableIterator", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\"source_iterator has to be a CheckpointableIterator\"", ")", "\n", "", "self", ".", "_source_iterator", "=", "source_iterator", "\n", "self", ".", "_transform", "=", "transform", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.iterators.MapIterator.getstate": [[639, 641], ["iterators.MapIterator._source_iterator.getstate"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.iterators.BucketedReadaheadBatchIterator.getstate"], ["", "def", "getstate", "(", "self", ")", "->", "Dict", ":", "\n", "        ", "return", "self", ".", "_source_iterator", ".", "getstate", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.iterators.MapIterator.setstate": [[642, 644], ["iterators.MapIterator._source_iterator.setstate"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.iterators.BucketedReadaheadBatchIterator.setstate"], ["", "def", "setstate", "(", "self", ",", "checkpoint", ":", "Optional", "[", "Dict", "]", ")", ":", "\n", "        ", "self", ".", "_source_iterator", ".", "setstate", "(", "checkpoint", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.iterators.MapIterator.__next__": [[645, 647], ["iterators.MapIterator._transform", "next"], "methods", ["None"], ["", "def", "__next__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "_transform", "(", "next", "(", "self", ".", "_source_iterator", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.iterators.ZipIterator.__init__": [[694, 705], ["isinstance", "ValueError"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "*", "source_iterators", ":", "CheckpointableIterator", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            source_iterators: list of iterators to zip, item by item\n        \"\"\"", "\n", "for", "source_iterator", "in", "source_iterators", ":", "\n", "            ", "if", "not", "isinstance", "(", "source_iterator", ",", "CheckpointableIterator", ")", ":", "\n", "                ", "raise", "ValueError", "(", "\n", "\"all iterators in source_iterators have to be CheckpointableIterator\"", "\n", ")", "\n", "", "", "self", ".", "_source_iterators", "=", "source_iterators", "# type: List[CheckpointableIterator]", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.iterators.ZipIterator.getstate": [[706, 710], ["tuple", "iterator.getstate"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.iterators.BucketedReadaheadBatchIterator.getstate"], ["", "def", "getstate", "(", "self", ")", "->", "Dict", ":", "\n", "        ", "return", "{", "\n", "\"input_states\"", ":", "tuple", "(", "\n", "iterator", ".", "getstate", "(", ")", "for", "iterator", "in", "self", ".", "_source_iterators", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.iterators.ZipIterator.setstate": [[713, 722], ["zip", "iterator.setstate", "iterator.setstate"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.iterators.BucketedReadaheadBatchIterator.setstate", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.iterators.BucketedReadaheadBatchIterator.setstate"], ["", "def", "setstate", "(", "self", ",", "checkpoint", ":", "Optional", "[", "Dict", "]", ")", ":", "\n", "        ", "if", "checkpoint", "is", "None", ":", "\n", "            ", "for", "iterator", "in", "self", ".", "_source_iterators", ":", "\n", "                ", "iterator", ".", "setstate", "(", "None", ")", "\n", "", "", "else", ":", "\n", "            ", "for", "iterator", ",", "state", "in", "zip", "(", "\n", "self", ".", "_source_iterators", ",", "checkpoint", "[", "\"input_states\"", "]", "\n", ")", ":", "\n", "                ", "iterator", ".", "setstate", "(", "state", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.iterators.ZipIterator.__next__": [[723, 730], ["tuple", "res.append", "next"], "methods", ["None"], ["", "", "", "def", "__next__", "(", "self", ")", ":", "\n", "        ", "res", "=", "(", "\n", "[", "]", "\n", ")", "# (note: can't use a generator expression, as it gets confused when a next() call raises StopIteration)", "\n", "for", "iterator", "in", "self", ".", "_source_iterators", ":", "\n", "            ", "res", ".", "append", "(", "next", "(", "iterator", ")", ")", "\n", "", "return", "tuple", "(", "res", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.iterators.WindowedIterator.__init__": [[743, 753], ["iterators.WindowedIterator.setstate", "isinstance", "ValueError"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.iterators.BucketedReadaheadBatchIterator.setstate"], ["def", "__init__", "(", "self", ",", "source_iterator", ":", "CheckpointableIterator", ",", "width", ":", "int", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            source_iterator: checkpointable input iterators\n        \"\"\"", "\n", "if", "not", "isinstance", "(", "source_iterator", ",", "CheckpointableIterator", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\"source_iterator has to be a CheckpointableIterator\"", ")", "\n", "", "self", ".", "_source_iterator", "=", "source_iterator", "# type: CheckpointableIterator", "\n", "self", ".", "_width", "=", "width", "# type: int", "\n", "self", ".", "setstate", "(", "None", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.iterators.WindowedIterator.getstate": [[754, 758], ["None"], "methods", ["None"], ["", "def", "getstate", "(", "self", ")", "->", "Dict", ":", "\n", "        ", "return", "{", "\n", "\"source_state\"", ":", "self", ".", "_source_state", ",", "# state for first item in FIFO", "\n", "\"item_index\"", ":", "self", ".", "_item_index", ",", "\n", "}", "# index of next item to serve", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.iterators.WindowedIterator.setstate": [[760, 765], ["iterators.WindowedIterator._source_iterator.setstate", "iterators.WindowedIterator._generate"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.iterators.BucketedReadaheadBatchIterator.setstate", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.iterators.FixedBatchIterator._generate"], ["", "def", "setstate", "(", "self", ",", "checkpoint", ":", "Optional", "[", "Dict", "]", ")", ":", "\n", "        ", "self", ".", "_source_state", "=", "checkpoint", "[", "\"source_state\"", "]", "if", "checkpoint", "else", "None", "\n", "self", ".", "_item_index", "=", "checkpoint", "[", "\"item_index\"", "]", "if", "checkpoint", "else", "0", "\n", "self", ".", "_source_iterator", ".", "setstate", "(", "self", ".", "_source_state", ")", "\n", "self", ".", "_iterator", "=", "self", ".", "_generate", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.iterators.WindowedIterator._fifo_slice": [[766, 769], ["tuple"], "methods", ["None"], ["", "def", "_fifo_slice", "(", "self", ",", "i", ")", ":", "# returns a window into the FIFO beginning at i", "\n", "# @TODO: for efficiency, make this a slice view", "\n", "        ", "return", "tuple", "(", "self", ".", "_fifo", "[", "i", ":", "i", "+", "self", ".", "_width", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.iterators.WindowedIterator._generate": [[770, 792], ["iterators.WindowedIterator._source_iterator.getstate", "list", "itertools.islice", "len", "iterators.WindowedIterator._source_iterator.getstate", "iterators.WindowedIterator._fifo.extend", "min", "itertools.islice", "iterators.WindowedIterator._fifo_slice", "len"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.iterators.BucketedReadaheadBatchIterator.getstate", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.iterators.BucketedReadaheadBatchIterator.getstate", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.iterators.WindowedIterator._fifo_slice"], ["", "def", "_generate", "(", "self", ")", "->", "Iterator", ":", "\n", "        ", "self", ".", "_source_state", "=", "self", ".", "_source_iterator", ".", "getstate", "(", ")", "\n", "self", ".", "_fifo", "=", "list", "(", "islice", "(", "self", ".", "_source_iterator", ",", "self", ".", "_width", ")", ")", "\n", "# we do this in overlapping blocks of length 2*width, for easier checkpointing and potential efficiency", "\n", "while", "len", "(", "self", ".", "_fifo", ")", "==", "self", ".", "_width", ":", "\n", "# we got 'width' items; append another 'width' (or less if at end)", "\n", "            ", "next_input_state", "=", "self", ".", "_source_iterator", ".", "getstate", "(", ")", "\n", "self", ".", "_fifo", ".", "extend", "(", "islice", "(", "self", ".", "_source_iterator", ",", "self", ".", "_width", ")", ")", "\n", "# now serve all positions in first half (last = width - 1). If at end, then limit accordingly.", "\n", "last", "=", "min", "(", "self", ".", "_width", "-", "1", ",", "len", "(", "self", ".", "_fifo", ")", "-", "self", ".", "_width", ")", "\n", "while", "self", ".", "_item_index", "<=", "last", ":", "\n", "                ", "window", "=", "self", ".", "_fifo_slice", "(", "self", ".", "_item_index", ")", "\n", "self", ".", "_item_index", "+=", "1", "\n", "yield", "window", "\n", "# drop all we just served; if < width left, we have hit the end", "\n", "", "self", ".", "_fifo", "=", "self", ".", "_fifo", "[", "\n", "last", "+", "1", ":", "\n", "]", "# Note: This must be a new list, since the old might still be in a slice view.", "\n", "self", ".", "_source_state", "=", "(", "\n", "next_input_state", "# this reflects now the first element in the FIFO", "\n", ")", "\n", "self", ".", "_item_index", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.iterators.WindowedIterator.__next__": [[793, 795], ["next"], "methods", ["None"], ["", "", "def", "__next__", "(", "self", ")", ":", "\n", "        ", "return", "next", "(", "self", ".", "_iterator", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.iterators.FixedBatchIterator.__init__": [[806, 817], ["iterators.FixedBatchIterator.setstate", "isinstance", "ValueError"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.iterators.BucketedReadaheadBatchIterator.setstate"], ["def", "__init__", "(", "self", ",", "source_iterator", ":", "CheckpointableIterator", ",", "batch_size", ":", "int", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            source_iterator: checkpointable input iterators\n            batch_size: number of items per batch\n        \"\"\"", "\n", "if", "not", "isinstance", "(", "source_iterator", ",", "CheckpointableIterator", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\"source_iterator has to be a CheckpointableIterator\"", ")", "\n", "", "self", ".", "_source_iterator", "=", "source_iterator", "# type: CheckpointableIterator", "\n", "self", ".", "_batch_size", "=", "batch_size", "# type: int", "\n", "self", ".", "setstate", "(", "None", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.iterators.FixedBatchIterator.getstate": [[818, 821], ["iterators.FixedBatchIterator._source_iterator.getstate"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.iterators.BucketedReadaheadBatchIterator.getstate"], ["", "def", "getstate", "(", "self", ")", "->", "Dict", ":", "\n", "        ", "return", "{", "\n", "\"source_state\"", ":", "self", ".", "_source_iterator", ".", "getstate", "(", ")", "\n", "}", "# state for first item in next batch", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.iterators.FixedBatchIterator.setstate": [[823, 827], ["iterators.FixedBatchIterator._source_iterator.setstate", "iterators.FixedBatchIterator._generate"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.iterators.BucketedReadaheadBatchIterator.setstate", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.iterators.FixedBatchIterator._generate"], ["", "def", "setstate", "(", "self", ",", "checkpoint", ":", "Optional", "[", "Dict", "]", ")", ":", "\n", "        ", "self", ".", "_source_state", "=", "checkpoint", "[", "\"source_state\"", "]", "if", "checkpoint", "else", "None", "\n", "self", ".", "_source_iterator", ".", "setstate", "(", "self", ".", "_source_state", ")", "\n", "self", ".", "_iterator", "=", "self", ".", "_generate", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.iterators.FixedBatchIterator._generate": [[828, 834], ["list", "itertools.islice"], "methods", ["None"], ["", "def", "_generate", "(", "self", ")", "->", "Iterator", ":", "\n", "        ", "while", "True", ":", "\n", "            ", "batch", "=", "list", "(", "islice", "(", "self", ".", "_source_iterator", ",", "self", ".", "_batch_size", ")", ")", "\n", "if", "not", "batch", ":", "\n", "                ", "break", "\n", "", "yield", "batch", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.iterators.FixedBatchIterator.__next__": [[835, 837], ["next"], "methods", ["None"], ["", "", "def", "__next__", "(", "self", ")", ":", "\n", "        ", "return", "next", "(", "self", ".", "_iterator", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.iterators.RandomIterator.__init__": [[846, 854], ["random.Random", "iterators.RandomIterator._random.seed"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "seed", ":", "Optional", "[", "int", "]", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            seed: Random seed.\n        \"\"\"", "\n", "self", ".", "_random", "=", "Random", "(", ")", "# type: Random", "\n", "if", "seed", "is", "not", "None", ":", "\n", "            ", "self", ".", "_random", ".", "seed", "(", "seed", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.iterators.RandomIterator.getstate": [[855, 857], ["iterators.RandomIterator._random.getstate"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.iterators.BucketedReadaheadBatchIterator.getstate"], ["", "", "def", "getstate", "(", "self", ")", "->", "Dict", ":", "\n", "        ", "return", "{", "\"random_state\"", ":", "self", ".", "_random", ".", "getstate", "(", ")", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.iterators.RandomIterator.setstate": [[858, 860], ["iterators.RandomIterator._random.setstate"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.iterators.BucketedReadaheadBatchIterator.setstate"], ["", "def", "setstate", "(", "self", ",", "checkpoint", ":", "Optional", "[", "Dict", "]", ")", ":", "\n", "        ", "self", ".", "_random", ".", "setstate", "(", "checkpoint", "[", "\"random_state\"", "]", "if", "checkpoint", "else", "None", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.iterators.RandomIterator.__next__": [[861, 863], ["iterators.RandomIterator._random.random"], "methods", ["None"], ["", "def", "__next__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "_random", ".", "random", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.iterators.RecurrentIterator.__init__": [[871, 889], ["iterators.RecurrentIterator.setstate", "isinstance", "ValueError"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.iterators.BucketedReadaheadBatchIterator.setstate"], ["def", "__init__", "(", "\n", "self", ",", "\n", "source_iterator", ":", "CheckpointableIterator", ",", "\n", "step_function", ":", "Callable", "[", "[", "Any", ",", "Any", "]", ",", "Tuple", "[", "Any", ",", "Any", "]", "]", ",", "\n", "initial_state", ":", "Any", "=", "None", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            source_iterator: checkpointable iterator to recur over\n            step_function: user-supplied function with signature step_function(state, item) -> (new_state, output)\n            initial_state: initial state to be passed to the step_function upon first invocation\n        \"\"\"", "\n", "if", "not", "isinstance", "(", "source_iterator", ",", "CheckpointableIterator", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\"source_iterator has to be a CheckpointableIterator\"", ")", "\n", "", "self", ".", "_source_iterator", "=", "source_iterator", "# type: CheckpointableIterator", "\n", "self", ".", "_step_function", "=", "step_function", "# type: Callable[[Any,Any], Tuple[Any,Any]]", "\n", "self", ".", "_initial_state", "=", "initial_state", "# type: Any", "\n", "self", ".", "setstate", "(", "None", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.iterators.RecurrentIterator.getstate": [[890, 894], ["iterators.RecurrentIterator._source_iterator.getstate"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.iterators.BucketedReadaheadBatchIterator.getstate"], ["", "def", "getstate", "(", "self", ")", ":", "\n", "        ", "return", "{", "\n", "\"recurrent_state\"", ":", "self", ".", "_recurrent_state", ",", "\n", "\"source_state\"", ":", "self", ".", "_source_iterator", ".", "getstate", "(", ")", ",", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.iterators.RecurrentIterator.setstate": [[896, 912], ["iterators.RecurrentIterator._source_iterator.setstate", "iterators.RecurrentIterator.setstate._generate"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.iterators.BucketedReadaheadBatchIterator.setstate", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.iterators.FixedBatchIterator._generate"], ["", "def", "setstate", "(", "self", ",", "checkpoint", ")", ":", "\n", "        ", "self", ".", "_recurrent_state", "=", "(", "\n", "checkpoint", "[", "\"recurrent_state\"", "]", "if", "checkpoint", "else", "self", ".", "_initial_state", "\n", ")", "\n", "self", ".", "_source_iterator", ".", "setstate", "(", "\n", "checkpoint", "[", "\"source_state\"", "]", "if", "checkpoint", "else", "None", "\n", ")", "\n", "\n", "def", "_generate", "(", ")", ":", "\n", "            ", "for", "item", "in", "self", ".", "_source_iterator", ":", "\n", "                ", "self", ".", "_recurrent_state", ",", "output", "=", "self", ".", "_step_function", "(", "\n", "self", ".", "_recurrent_state", ",", "item", "\n", ")", "\n", "yield", "output", "\n", "\n", "", "", "self", ".", "_iterator", "=", "_generate", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.iterators.RecurrentIterator.__next__": [[913, 915], ["next"], "methods", ["None"], ["", "def", "__next__", "(", "self", ")", ":", "\n", "        ", "return", "next", "(", "self", ".", "_iterator", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.iterators.PrefetchIterator.__init__": [[987, 997], ["iterators.PrefetchIterator.setstate", "isinstance", "ValueError"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.iterators.BucketedReadaheadBatchIterator.setstate"], ["def", "__init__", "(", "\n", "self", ",", "source_iterator", ":", "CheckpointableIterator", ",", "buffer_size", ":", "int", "=", "1000", "\n", ")", ":", "\n", "        ", "if", "not", "isinstance", "(", "source_iterator", ",", "CheckpointableIterator", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\"source_iterator has to be a CheckpointableIterator\"", ")", "\n", "", "self", ".", "_source_iterator", "=", "source_iterator", "# type:CheckpointableIterator", "\n", "self", ".", "_buffer_size", "=", "buffer_size", "# type: int", "\n", "self", ".", "_queue", "=", "None", "# type: Optional[ClosableQueue]", "\n", "self", ".", "_thread", "=", "None", "# type: Optional[Thread]", "\n", "self", ".", "setstate", "(", "None", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.iterators.PrefetchIterator.getstate": [[998, 1000], ["None"], "methods", ["None"], ["", "def", "getstate", "(", "self", ")", "->", "Dict", ":", "\n", "        ", "return", "{", "\"source_state\"", ":", "self", ".", "_source_state", ",", "\"item_offset\"", ":", "self", ".", "_item_offset", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.iterators.PrefetchIterator.setstate": [[1001, 1029], ["iterators.PrefetchIterator._source_iterator.setstate", "closablequeue.ClosableQueue", "threading.Thread", "iterators.PrefetchIterator._thread.start", "iterators.PrefetchIterator._queue.close", "iterators.PrefetchIterator._thread.join"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.iterators.BucketedReadaheadBatchIterator.setstate", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.closablequeue.ClosableQueue.close"], ["", "def", "setstate", "(", "self", ",", "checkpoint", ":", "Optional", "[", "Dict", "]", ")", ":", "\n", "        ", "if", "(", "\n", "self", ".", "_thread", "is", "not", "None", "\n", ")", ":", "# if there is a prefetching thread running, close the queue and wait for the thread to terminate", "\n", "            ", "assert", "self", ".", "_queue", "is", "not", "None", "\n", "self", ".", "_queue", ".", "close", "(", ")", "\n", "self", ".", "_thread", ".", "join", "(", ")", "\n", "\n", "", "self", ".", "_source_state", "=", "(", "\n", "checkpoint", "[", "\"source_state\"", "]", "if", "checkpoint", "is", "not", "None", "else", "None", "\n", ")", "\n", "self", ".", "_item_offset", "=", "checkpoint", "[", "\"item_offset\"", "]", "if", "checkpoint", "is", "not", "None", "else", "0", "\n", "\n", "self", ".", "_source_iterator", ".", "setstate", "(", "self", ".", "_source_state", ")", "\n", "\n", "self", ".", "_queue", "=", "ClosableQueue", "(", "maxsize", "=", "self", ".", "_buffer_size", ")", "# clear queue", "\n", "# make thread daemonic so it is killed when the main program terminates", "\n", "self", ".", "_thread", "=", "Thread", "(", "\n", "target", "=", "self", ".", "_prefetch_thread_fn", ",", "\n", "args", "=", "(", "\n", "self", ".", "_source_iterator", ",", "\n", "self", ".", "_item_offset", ",", "\n", "self", ".", "_buffer_size", ",", "\n", "self", ".", "_queue", ",", "\n", ")", ",", "\n", "daemon", "=", "True", ",", "\n", ")", "\n", "self", ".", "_thread", ".", "start", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.iterators.PrefetchIterator._prefetch_thread_fn": [[1030, 1059], ["iterators._advance_iterator", "next", "source.getstate", "queue.put", "queue.close"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.iterators._advance_iterator", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.iterators.BucketedReadaheadBatchIterator.getstate", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.closablequeue.ClosableQueue.put", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.closablequeue.ClosableQueue.close"], ["", "@", "staticmethod", "\n", "def", "_prefetch_thread_fn", "(", "\n", "source", ",", "item_offset", ",", "buffer_size", ",", "queue", "\n", ")", ":", "# behavior of the prefetching thread, only call from that thread!", "\n", "        ", "_advance_iterator", "(", "source", ",", "item_offset", ")", "# skip to checkpoint", "\n", "\n", "while", "True", ":", "\n", "            ", "try", ":", "\n", "                ", "item", "=", "next", "(", "source", ")", "\n", "", "except", "StopIteration", ":", "\n", "                ", "queue", ".", "close", "(", ")", "\n", "return", "\n", "\n", "", "if", "(", "\n", "item_offset", "==", "buffer_size", "-", "1", "\n", ")", ":", "# send a new source state a the END of each window of length _buffer_size", "\n", "                ", "source_state", "=", "(", "\n", "source", ".", "getstate", "(", ")", "\n", ")", "# this is the state for retrieving the NEXT element, i.e. the first element of the next buffer", "\n", "item_offset", "=", "0", "\n", "", "else", ":", "\n", "                ", "source_state", "=", "None", "\n", "item_offset", "+=", "1", "\n", "", "msg", "=", "(", "item", ",", "source_state", ")", "\n", "\n", "try", ":", "\n", "                ", "queue", ".", "put", "(", "msg", ")", "\n", "", "except", "ClosedException", ":", "\n", "                ", "return", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.iterators.PrefetchIterator.__next__": [[1060, 1077], ["iterators.PrefetchIterator._queue.get"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.closablequeue.ClosableQueue.get"], ["", "", "", "def", "__next__", "(", "self", ")", ":", "\n", "        ", "try", ":", "\n", "            ", "msg", "=", "self", ".", "_queue", ".", "get", "(", ")", "\n", "", "except", "ClosedException", ":", "\n", "            ", "raise", "StopIteration", "\n", "\n", "", "item", ",", "prefetch_source_state", "=", "msg", "\n", "if", "prefetch_source_state", "is", "not", "None", ":", "\n", "            ", "assert", "(", "\n", "self", ".", "_item_offset", "==", "self", ".", "_buffer_size", "-", "1", "\n", ")", "# we expect a new source state at then END of each window of length _buffer_size", "\n", "self", ".", "_source_state", "=", "prefetch_source_state", "\n", "self", ".", "_item_offset", "=", "0", "\n", "", "else", ":", "\n", "            ", "self", ".", "_item_offset", "=", "self", ".", "_item_offset", "+", "1", "\n", "assert", "self", ".", "_item_offset", "<", "self", ".", "_buffer_size", "\n", "", "return", "item", "# for debugging, its useful to return msg instead of item", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.iterators.PrefetchIterator.__del__": [[1078, 1088], ["iterators.PrefetchIterator._queue.close", "iterators.PrefetchIterator._thread.join"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.closablequeue.ClosableQueue.close"], ["", "def", "__del__", "(", "\n", "self", ",", "\n", ")", ":", "# note: this is often not called. If you really need it, gc.collect() will do the trick.", "\n", "        ", "if", "self", ".", "_thread", "is", "not", "None", ":", "\n", "            ", "assert", "self", ".", "_queue", "is", "not", "None", "\n", "self", ".", "_queue", ".", "close", "(", ")", "\n", "try", ":", "\n", "                ", "self", ".", "_thread", ".", "join", "(", ")", "\n", "", "except", ":", "\n", "                ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.iterators.BucketedReadaheadBatchIterator.__init__": [[1102, 1134], ["iter", "iterators.BucketedReadaheadBatchIterator.setstate", "isinstance", "ValueError", "random.Random", "iterators.BucketedReadaheadBatchIterator._random.seed"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.iterators.BucketedReadaheadBatchIterator.setstate"], ["def", "__init__", "(", "\n", "self", ",", "\n", "source_iterator", ":", "CheckpointableIterator", ",", "\n", "read_ahead", ":", "int", ",", "\n", "key", ":", "Callable", "[", "[", "Any", "]", ",", "Any", "]", ",", "\n", "batch_size", ":", "Union", "[", "int", ",", "Callable", "[", "[", "Any", "]", ",", "int", "]", "]", ",", "\n", "shuffle", ":", "bool", "=", "True", ",", "\n", "seed", ":", "Optional", "[", "int", "]", "=", "None", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            source_iterator: The data set that is read from. Typically this is an infinite source.\n            read_ahead: Number of items to fetch ahead for grouping purposes.\n            key: User-provided callback to define how data is sorted for purpose of batching.\n            batch_size: Batch size in number of items. Either an integer or a callback to determine batch size for a given first batch item.\n            shuffle: Pass False to not randomize the batches. (default: True)\n            seed: Random seed for batch shuffling.\n        \"\"\"", "\n", "if", "not", "isinstance", "(", "source_iterator", ",", "CheckpointableIterator", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\"source_iterator has to be a CheckpointableIterator\"", ")", "\n", "# keep arguments", "\n", "", "self", ".", "_key", "=", "key", "# type: Callable[[Any], Any]", "\n", "self", ".", "_batch_size", "=", "batch_size", "# type: Union[int,Callable[[Any], int]]", "\n", "self", ".", "_read_ahead", "=", "read_ahead", "# type: int", "\n", "# initialize state", "\n", "self", ".", "_random", "=", "None", "\n", "if", "shuffle", ":", "\n", "            ", "self", ".", "_random", "=", "Random", "(", ")", "# type: Random", "\n", "if", "seed", "is", "not", "None", ":", "\n", "                ", "self", ".", "_random", ".", "seed", "(", "seed", ")", "\n", "", "", "self", ".", "_source_iterator", "=", "iter", "(", "source_iterator", ")", "# type: CheckpointableIterator", "\n", "self", ".", "setstate", "(", "None", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.iterators.BucketedReadaheadBatchIterator.getstate": [[1135, 1140], ["None"], "methods", ["None"], ["", "def", "getstate", "(", "self", ")", ":", "\n", "        ", "return", "{", "\n", "\"source_state\"", ":", "self", ".", "_source_state", ",", "\n", "\"random_state\"", ":", "self", ".", "_random_state", ",", "\n", "\"num_served\"", ":", "self", ".", "_num_batches_yielded", ",", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.iterators.BucketedReadaheadBatchIterator.setstate": [[1142, 1187], ["iterators.BucketedReadaheadBatchIterator._source_iterator.setstate", "iterators.BucketedReadaheadBatchIterator.setstate._generate"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.iterators.BucketedReadaheadBatchIterator.setstate", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.iterators.FixedBatchIterator._generate"], ["", "def", "setstate", "(", "self", ",", "checkpoint", ":", "Optional", "[", "Dict", "]", ")", ":", "\n", "        ", "self", ".", "_source_state", "=", "(", "\n", "checkpoint", "[", "\"source_state\"", "]", "if", "checkpoint", "else", "None", "\n", ")", "# type: Dict  -- state of input before reading the current set of batches", "\n", "self", ".", "_random_state", "=", "(", "\n", "checkpoint", "[", "\"random_state\"", "]", "if", "checkpoint", "else", "None", "\n", ")", "# type: Any   -- state of random generator at _source_state", "\n", "self", ".", "_num_batches_yielded", "=", "(", "\n", "checkpoint", "[", "\"num_served\"", "]", "if", "checkpoint", "else", "0", "\n", ")", "# type: int   -- number of batches served from the current set of batches", "\n", "# checkpointing: restore to start of current set of batches", "\n", "self", ".", "_source_iterator", ".", "setstate", "(", "self", ".", "_source_state", ")", "\n", "if", "self", ".", "_random_state", ":", "\n", "            ", "self", ".", "_random", ".", "setstate", "(", "self", ".", "_random_state", ")", "\n", "", "self", ".", "_source_exhausted", "=", "(", "\n", "False", "\n", ")", "# type: bool  -- set to True once we hit StopIteration on source", "\n", "\n", "def", "_generate", "(", ")", ":", "\n", "            ", "skip_to_checkpoint", "=", "self", ".", "_num_batches_yielded", "\n", "source_exhausted", "=", "False", "\n", "while", "not", "source_exhausted", ":", "\n", "# prefetch the readahead buffer", "\n", "                ", "self", ".", "_source_state", "=", "self", ".", "_source_iterator", ".", "getstate", "(", ")", "\n", "self", ".", "_random_state", "=", "self", ".", "_random", ".", "getstate", "(", ")", "if", "self", ".", "_random", "else", "None", "\n", "items", "=", "list", "(", "islice", "(", "self", ".", "_source_iterator", ",", "self", ".", "_read_ahead", ")", ")", "\n", "source_exhausted", "=", "len", "(", "items", ")", "<", "self", ".", "_read_ahead", "\n", "# create batches", "\n", "batches", "=", "self", ".", "_create_batches", "(", "items", ")", "\n", "# shuffle the batches", "\n", "if", "self", ".", "_random", ":", "\n", "                    ", "self", ".", "_random", ".", "shuffle", "(", "batches", ")", "\n", "# on first loop iteration, restore iterator inside batches from checkpoint", "\n", "", "batches", "=", "iter", "(", "batches", ")", "\n", "self", ".", "_num_batches_yielded", "=", "_advance_iterator", "(", "\n", "batches", ",", "skip_to_checkpoint", "\n", ")", "\n", "skip_to_checkpoint", "=", "0", "\n", "# main loop over batches in current read-ahead section", "\n", "for", "batch", "in", "batches", ":", "\n", "                    ", "self", ".", "_num_batches_yielded", "+=", "1", "\n", "yield", "batch", "\n", "\n", "", "", "", "self", ".", "_iterator", "=", "(", "\n", "_generate", "(", ")", "\n", ")", "# type: Iterator  -- iterator into current set of batches", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.iterators.BucketedReadaheadBatchIterator._create_batches": [[1189, 1215], ["items.sort", "cur_batch.append", "batches.append", "len", "batches.append", "isinstance", "iterators.BucketedReadaheadBatchIterator._batch_size"], "methods", ["None"], ["", "def", "_create_batches", "(", "\n", "self", ",", "items", ":", "List", "[", "Any", "]", "\n", ")", "->", "List", "[", "List", "[", "Any", "]", "]", ":", "# helper to form batches from a list of items", "\n", "# sort by length, longest first", "\n", "        ", "if", "self", ".", "_key", ":", "\n", "            ", "items", ".", "sort", "(", "\n", "key", "=", "self", ".", "_key", ",", "reverse", "=", "True", "\n", ")", "# note: sort() is stable, so we won't undo any randomization besides the bucketing", "\n", "# group into batches", "\n", "", "cur_batch", "=", "None", "\n", "batches", "=", "[", "]", "\n", "for", "item", "in", "items", ":", "\n", "            ", "if", "not", "cur_batch", ":", "\n", "                ", "batch_size", "=", "(", "\n", "self", ".", "_batch_size", "\n", "if", "isinstance", "(", "self", ".", "_batch_size", ",", "int", ")", "\n", "else", "self", ".", "_batch_size", "(", "item", ")", "\n", ")", "\n", "cur_batch", "=", "[", "]", "\n", "", "cur_batch", ".", "append", "(", "item", ")", "\n", "if", "len", "(", "cur_batch", ")", ">=", "batch_size", ":", "# this batch is full", "\n", "                ", "batches", ".", "append", "(", "cur_batch", ")", "\n", "cur_batch", "=", "None", "\n", "", "", "if", "cur_batch", ":", "\n", "            ", "batches", ".", "append", "(", "cur_batch", ")", "\n", "", "return", "batches", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.iterators.BucketedReadaheadBatchIterator.__next__": [[1216, 1218], ["next"], "methods", ["None"], ["", "def", "__next__", "(", "self", ")", ":", "\n", "        ", "return", "next", "(", "self", ".", "_iterator", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.iterators._advance_iterator": [[249, 254], ["range", "next"], "function", ["None"], ["def", "_advance_iterator", "(", "iterator", ":", "Iterator", ",", "n", ":", "int", ")", ":", "\n", "    ", "\"\"\"Little helper to advance an iterator by n items\"\"\"", "\n", "for", "_", "in", "range", "(", "n", ")", ":", "\n", "        ", "next", "(", "iterator", ")", "\n", "", "return", "n", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.iterators.create_source_iterator": [[348, 369], ["ValueError", "iterators.InfinitePermutationSourceIterator", "iterators.ChunkedSourceIterator"], "function", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.iterators.ChunkedSourceIterator"], ["", "", "def", "create_source_iterator", "(", "\n", "source_items", ":", "List", ",", "\n", "train", ":", "bool", "=", "True", ",", "\n", "seed", ":", "Optional", "[", "int", "]", "=", "None", ",", "\n", "shuffle", ":", "bool", "=", "True", ",", "\n", "num_instances", ":", "int", "=", "1", ",", "\n", "instance_rank", ":", "int", "=", "0", ",", "\n", ")", ":", "\n", "    ", "if", "not", "train", "and", "shuffle", ":", "\n", "        ", "raise", "ValueError", "(", "\"shuffling is not supported when train=False\"", ")", "\n", "", "if", "train", ":", "\n", "        ", "return", "InfinitePermutationSourceIterator", "(", "\n", "source_items", ",", "\n", "seed", "=", "seed", ",", "\n", "shuffle", "=", "shuffle", ",", "\n", "num_instances", "=", "num_instances", ",", "\n", "instance_rank", "=", "instance_rank", ",", "\n", ")", "\n", "", "else", ":", "\n", "        ", "return", "ChunkedSourceIterator", "(", "\n", "source_items", ",", "num_instances", "=", "num_instances", ",", "instance_rank", "=", "instance_rank", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.iterators.ChunkedSourceIterator": [[372, 394], ["math.ceil", "iterators.NativeCheckpointableIterator", "len"], "function", ["None"], ["", "", "def", "ChunkedSourceIterator", "(", "\n", "source_items", ":", "List", ",", "num_instances", ":", "int", "=", "1", ",", "instance_rank", ":", "int", "=", "0", "\n", ")", ":", "\n", "    ", "\"\"\"\n    Cuts source list into chunks, one per instance, and serves out items in chunk corresponding to instance_rank\n\n    This is a source iterator:\n    It is meant to be used at the beginning of a data loading pipeline.\n    As such, it takes a list as its source and not a CheckpointableIterator.\n\n    Args:\n        source_items: input list, must not be empty and must be small enough to fit into RAM entirely, ownership of the list and the data goes to the iterator, do not modify it!\n        num_instances: number of instances of this iterator. Meant for use with multi-process data loading, e.g., in distributed training.\n        instance_rank: rank of this instance of the iterator. Meant for use with multi-process data loading, e.g., in distributed training.\n    \"\"\"", "\n", "# heuristic: assuming blocks are all of the same size, math.ceil should give us the shortest makespan", "\n", "chunk_size", "=", "math", ".", "ceil", "(", "len", "(", "source_items", ")", "/", "num_instances", ")", "\n", "# this does not cause any out-of-bounds issues:", "\n", "# a slice with a start-index beyong the end of the list is empty,", "\n", "# and an end-index of a slice is capped at the end of the list", "\n", "chunk", "=", "source_items", "[", "instance_rank", "*", "chunk_size", ":", "(", "instance_rank", "+", "1", ")", "*", "chunk_size", "]", "\n", "return", "NativeCheckpointableIterator", "(", "chunk", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.iterators.ParallelMapIterator": [[649, 685], ["iterators.FixedBatchIterator", "multiprocessing.Pool", "iterators.MapIterator", "iterators.SelectManyIterator", "multiprocessing.Pool.map"], "function", ["None"], ["", "", "def", "ParallelMapIterator", "(", "\n", "source_iterator", ":", "CheckpointableIterator", ",", "\n", "transform", ":", "Callable", "[", "[", "str", "]", ",", "Any", "]", ",", "\n", "num_processes", ":", "int", ",", "\n", "num_items_per_process", ":", "int", ",", "\n", ")", ":", "\n", "    ", "\"\"\"\n    Applies given transform to each data item\n\n    Behaves the same as MapIterator, but applies transform in parallel using multiple processes in a parallel map operation.\n\n    Warning:\n    The transform function has to be pickleable because it is sent across process boundaries.\n    To achieve this, transform should be a top-level function.\n\n    Args:\n        source_iterator: checkpointable iterator\n        transform: function to be applied to each data item, has to be pickleable, see above\n        num_processes: number of processes to use for parallel map\n        num_items_per_process: number of data items each process operates on\n    \"\"\"", "\n", "# divide stream of data items into batches", "\n", "batched_samples", "=", "FixedBatchIterator", "(", "\n", "source_iterator", ",", "num_processes", "*", "num_items_per_process", "\n", ")", "\n", "# create process pool and capture it in closure that performs parallel map", "\n", "p", "=", "Pool", "(", "num_processes", ")", "\n", "\n", "def", "parallel_map_transform", "(", "buffer", ")", ":", "\n", "        ", "return", "p", ".", "map", "(", "transform", ",", "buffer", ")", "\n", "\n", "# apply transform in parallel to data items in a batch", "\n", "", "batched_transformed_samples", "=", "MapIterator", "(", "batched_samples", ",", "parallel_map_transform", ")", "\n", "# unpack batches to go back to stream of (now transformed) data items", "\n", "transformed_samples", "=", "SelectManyIterator", "(", "batched_transformed_samples", ")", "\n", "return", "transformed_samples", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.iterators.SamplingRandomMapIterator": [[917, 942], ["random.Random", "iterators.RecurrentIterator", "random.Random.seed", "random.Random.setstate", "transform", "random.Random.getstate", "random.Random.getstate"], "function", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.iterators.BucketedReadaheadBatchIterator.setstate", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.iterators.BucketedReadaheadBatchIterator.getstate", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.iterators.BucketedReadaheadBatchIterator.getstate"], ["", "", "def", "SamplingRandomMapIterator", "(", "\n", "source_iterator", ":", "CheckpointableIterator", ",", "\n", "transform", ":", "Callable", "[", "[", "Random", ",", "Any", "]", ",", "Any", "]", ",", "\n", "seed", ":", "Optional", "[", "int", "]", "=", "None", ",", "\n", ")", ":", "\n", "    ", "\"\"\"\n    An iterator that calls a transform function on each item, while also passing a checkpointed\n    random generator.\n\n    Args:\n        source_iterator: checkpointable iterator to recur over\n        step_function: user-supplied function with signature step_function(random, item) -> result_item\n        seed: random seed\n    \"\"\"", "\n", "_random", "=", "Random", "(", ")", "\n", "if", "seed", "is", "not", "None", ":", "\n", "        ", "_random", ".", "seed", "(", "seed", ")", "\n", "\n", "", "def", "_step_function", "(", "state", ",", "item", ")", ":", "\n", "        ", "_random", ".", "setstate", "(", "state", ")", "\n", "output", "=", "transform", "(", "_random", ",", "item", ")", "\n", "return", "_random", ".", "getstate", "(", ")", ",", "output", "\n", "\n", "", "return", "RecurrentIterator", "(", "\n", "source_iterator", ",", "_step_function", ",", "initial_state", "=", "_random", ".", "getstate", "(", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.iterators.BlockwiseShuffleIterator": [[945, 976], ["iterators.FixedBatchIterator", "iterators.SamplingRandomMapIterator", "iterators.SelectManyIterator", "random.shuffle", "iter"], "function", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.iterators.SamplingRandomMapIterator"], ["", "def", "BlockwiseShuffleIterator", "(", "\n", "source_iterator", ":", "CheckpointableIterator", ",", "block_size", ":", "int", ",", "seed", ":", "int", "=", "0", "\n", ")", ":", "\n", "    ", "\"\"\"\n    Shuffles a sequence of items by grouping consecutive items in blocks of fixed size, shuffling\n    each block, and yielding the shuffled items of all blocks as a flat sequence.\n\n    E.g. [1, 2, 3, 4, 5, 6, 7, 8] with block_size = 3 may yield [3, 1, 2, 4, 6, 5, 8, 7].\n\n    Args:\n        source_iterator: checkpointable iterator or restartable iterable over input items to shuffle\n        block_size: size of the buffer in number of items used for shuffling\n        seed: random seed used for shuffling (or None)\n    \"\"\"", "\n", "# This is implemented as a pipeline:", "\n", "#  - group N consecutive items together", "\n", "#  - shuffle them", "\n", "#  - flatten the result", "\n", "blocks", "=", "FixedBatchIterator", "(", "source_iterator", ",", "batch_size", "=", "block_size", ")", "\n", "\n", "def", "shuffle_block_fn", "(", "random", ":", "Random", ",", "block", ":", "List", ")", ":", "\n", "        ", "random", ".", "shuffle", "(", "block", ")", "\n", "return", "block", "\n", "\n", "", "shuffled_blocks", "=", "SamplingRandomMapIterator", "(", "\n", "blocks", ",", "transform", "=", "shuffle_block_fn", ",", "seed", "=", "seed", "\n", ")", "\n", "samples", "=", "SelectManyIterator", "(", "\n", "shuffled_blocks", ",", "collection_selector", "=", "lambda", "shuffled_block", ":", "iter", "(", "shuffled_block", ")", "\n", ")", "\n", "return", "samples", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.closablequeue.ClosableQueue.__init__": [[25, 32], ["collections.deque", "threading.Lock", "threading.Condition", "threading.Condition"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "maxsize", ":", "int", "=", "1000", ")", ":", "\n", "        ", "self", ".", "_maxsize", "=", "maxsize", "\n", "self", ".", "_queue", "=", "deque", "(", ")", "\n", "self", ".", "_mutex", "=", "Lock", "(", ")", "\n", "self", ".", "_not_empty", "=", "Condition", "(", "self", ".", "_mutex", ")", "\n", "self", ".", "_not_full", "=", "Condition", "(", "self", ".", "_mutex", ")", "\n", "self", ".", "_closed", "=", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.closablequeue.ClosableQueue.put": [[33, 47], ["closablequeue.ClosableQueue._queue.append", "closablequeue.ClosableQueue._not_empty.notify", "closablequeue.ClosedException", "len", "closablequeue.ClosableQueue._not_full.wait", "closablequeue.ClosedException"], "methods", ["None"], ["", "def", "put", "(", "self", ",", "item", ")", ":", "\n", "        ", "with", "self", ".", "_not_full", ":", "\n", "            ", "if", "self", ".", "_closed", ":", "\n", "                ", "raise", "ClosedException", "(", "\n", "\"This queue has been closed, no more items can be added.\"", "\n", ")", "\n", "", "while", "len", "(", "self", ".", "_queue", ")", ">=", "self", ".", "_maxsize", ":", "\n", "                ", "self", ".", "_not_full", ".", "wait", "(", ")", "\n", "if", "self", ".", "_closed", ":", "\n", "                    ", "raise", "ClosedException", "(", "\n", "\"This queue has been closed, no more items can be added.\"", "\n", ")", "\n", "", "", "self", ".", "_queue", ".", "append", "(", "item", ")", "\n", "self", ".", "_not_empty", ".", "notify", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.closablequeue.ClosableQueue.get": [[48, 63], ["closablequeue.ClosableQueue._queue.popleft", "closablequeue.ClosableQueue._not_full.notify", "closablequeue.ClosedException", "len", "closablequeue.ClosableQueue._not_empty.wait", "len", "closablequeue.ClosedException", "len"], "methods", ["None"], ["", "", "def", "get", "(", "self", ")", ":", "\n", "        ", "with", "self", ".", "_not_empty", ":", "\n", "            ", "if", "self", ".", "_closed", "and", "len", "(", "self", ".", "_queue", ")", "==", "0", ":", "\n", "                ", "raise", "ClosedException", "(", "\n", "\"This queue has been closed and is empty, no more items can be retrieved.\"", "\n", ")", "\n", "", "while", "len", "(", "self", ".", "_queue", ")", "==", "0", ":", "\n", "                ", "self", ".", "_not_empty", ".", "wait", "(", ")", "\n", "if", "self", ".", "_closed", "and", "len", "(", "self", ".", "_queue", ")", "==", "0", ":", "\n", "                    ", "raise", "ClosedException", "(", "\n", "\"This queue has been closed and is empty, no more items can be retrieved.\"", "\n", ")", "\n", "", "", "item", "=", "self", ".", "_queue", ".", "popleft", "(", ")", "\n", "self", ".", "_not_full", ".", "notify", "(", ")", "\n", "", "return", "item", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.closablequeue.ClosableQueue.close": [[64, 69], ["closablequeue.ClosableQueue._not_empty.notify_all", "closablequeue.ClosableQueue._not_full.notify_all"], "methods", ["None"], ["", "def", "close", "(", "self", ")", ":", "\n", "        ", "with", "self", ".", "_mutex", ":", "\n", "            ", "self", ".", "_closed", "=", "True", "\n", "self", ".", "_not_empty", ".", "notify_all", "(", ")", "\n", "self", ".", "_not_full", ".", "notify_all", "(", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.datasets.bump_seed": [[20, 25], ["None"], "function", ["None"], ["def", "bump_seed", "(", "seed", ":", "Optional", "[", "int", "]", ",", "step", "=", "1", ")", ":", "\n", "    ", "\"\"\"\n    Helper to bump a random seed if not None.\n    \"\"\"", "\n", "return", "None", "if", "seed", "is", "None", "else", "seed", "+", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.datasets.chunked_dataset_iterator": [[27, 93], ["iterators.create_source_iterator", "iterators.SelectManyIterator", "ValueError", "iterators.PrefetchIterator", "iterators.MapIterator", "iterators.BufferedShuffleIterator", "iterators.BlockwiseShuffleIterator", "datasets.bump_seed", "datasets.bump_seed"], "function", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.iterators.create_source_iterator", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.iterators.BlockwiseShuffleIterator", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.datasets.bump_seed", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.datasets.bump_seed"], ["", "def", "chunked_dataset_iterator", "(", "\n", "chunk_refs", ":", "List", ",", "\n", "read_chunk_fn", ":", "Callable", "[", "[", "Any", "]", ",", "Iterator", "]", ",", "\n", "buffer_size", ":", "int", ",", "\n", "train", ":", "bool", "=", "True", ",", "\n", "seed", ":", "Optional", "[", "int", "]", "=", "None", ",", "\n", "shuffle", ":", "bool", "=", "True", ",", "\n", "use_windowed", ":", "bool", "=", "False", ",", "\n", "transform", ":", "Callable", "[", "[", "Any", "]", ",", "Any", "]", "=", "None", ",", "\n", "prefetch", ":", "bool", "=", "True", ",", "\n", "num_instances", ":", "int", "=", "1", ",", "\n", "instance_rank", ":", "int", "=", "0", ",", "\n", ")", ":", "\n", "    ", "\"\"\"\n    Dataset reading data from gzipped chunks.\n\n    If train=True, this chunks are strided assigned to instances in strides and the data is infinitely repeated in permutations.\n    Otherwise, the chunks are split among the instances in consecutive blocks and the data is not repeated.\n    This way, when using this dataset for inference on multiple GPUs, to order the outputs in a way that corresponds\n    to the original order of the data items in the dataset, one simply has to collect the lists of outputs from each GPU\n    and then concatenate these lists in order of increasing rank.\n    When using MPI, this can be achieved by a gather-operation to get a list of lists of outputs, one list per GPU,\n    followed by flattening the lists back into a single list.\n\n    Args:\n        chunk_refs: references (such as path names) to chunk files\n        read_chunk_fn: function(chunk_ref) -> Iterator to read a chunk's content into an iterator over its items, e.g. read a file and split into text lines\n        train: see above\n        shuffle: if true, the data is shuffled. If train is False then shuffle must be False as well.\n        buffer_size: size of the buffer in number of samples / data items used for shuffling (default: 2**20)\n        transform: transform to be applied to each data item (transform(Any) -> Any)\n        prefetch: if True, insert a prefetch iterator with buffer_size\n        seed: random seed (or None)\n        num_instances: number of instances of this dataset. Meant for use with multi-process data loading, e.g., in distributed training.\n        instance_rank: rank of this instance of the dataset. Meant for use with multi-process data loading, e.g., in distributed training.\n        use_windowed: temporary option to switch back to the WindowedShuffleIterator (default False). Will go away once shown that we don't need it anymore.\n    \"\"\"", "\n", "if", "not", "train", "and", "shuffle", ":", "\n", "        ", "raise", "ValueError", "(", "\"shuffling is not supported when train=False\"", ")", "\n", "# set up the chunk reader", "\n", "", "chunk_refs", "=", "create_source_iterator", "(", "\n", "chunk_refs", ",", "\n", "train", "=", "train", ",", "\n", "seed", "=", "seed", ",", "\n", "shuffle", "=", "shuffle", ",", "\n", "num_instances", "=", "num_instances", ",", "\n", "instance_rank", "=", "instance_rank", ",", "\n", ")", "\n", "# set up the item reader", "\n", "samples", "=", "SelectManyIterator", "(", "\n", "source_iterator", "=", "chunk_refs", ",", "collection_selector", "=", "read_chunk_fn", "\n", ")", "\n", "# wrap the I/O operation in a prefetch iterator", "\n", "if", "prefetch", ":", "\n", "        ", "samples", "=", "PrefetchIterator", "(", "samples", ",", "buffer_size", ")", "\n", "# set up the item randomizer", "\n", "", "if", "shuffle", ":", "\n", "        ", "if", "use_windowed", ":", "\n", "            ", "samples", "=", "BufferedShuffleIterator", "(", "samples", ",", "buffer_size", ",", "bump_seed", "(", "seed", ",", "1", ")", ")", "\n", "", "else", ":", "\n", "            ", "samples", "=", "BlockwiseShuffleIterator", "(", "samples", ",", "buffer_size", ",", "bump_seed", "(", "seed", ",", "1", ")", ")", "\n", "# apply transform, if given", "\n", "", "", "if", "transform", "is", "not", "None", ":", "\n", "        ", "samples", "=", "MapIterator", "(", "samples", ",", "transform", ")", "\n", "# this is what we are serving out", "\n", "", "return", "samples", "\n", "", ""]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.test.test_closablequeue.TestClosableQueue.setUp": [[11, 13], ["infinibatch.closablequeue.ClosableQueue"], "methods", ["None"], ["    ", "def", "setUp", "(", "self", ")", ":", "\n", "        ", "self", ".", "queue", "=", "ClosableQueue", "(", "maxsize", "=", "10", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.test.test_closablequeue.TestClosableQueue.put_items": [[14, 19], ["test_closablequeue.TestClosableQueue.queue.put", "test_closablequeue.TestClosableQueue.queue.close"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.closablequeue.ClosableQueue.put", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.closablequeue.ClosableQueue.close"], ["", "def", "put_items", "(", "self", ",", "items", ",", "close", "=", "False", ")", ":", "\n", "        ", "for", "item", "in", "items", ":", "\n", "            ", "self", ".", "queue", ".", "put", "(", "item", ")", "\n", "", "if", "close", ":", "\n", "            ", "self", ".", "queue", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.test.test_closablequeue.TestClosableQueue.get_items": [[20, 22], ["test_closablequeue.TestClosableQueue.queue.get", "range"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.closablequeue.ClosableQueue.get"], ["", "", "def", "get_items", "(", "self", ",", "num_items", ")", ":", "\n", "        ", "return", "[", "self", ".", "queue", ".", "get", "(", ")", "for", "_", "in", "range", "(", "num_items", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.test.test_closablequeue.TestClosableQueue.test_basic": [[23, 26], ["test_closablequeue.TestClosableQueue.put_items", "test_closablequeue.TestClosableQueue.assertListEqual", "range", "test_closablequeue.TestClosableQueue.get_items", "list", "range"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.test.test_closablequeue.TestClosableQueue.put_items", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.test.test_closablequeue.TestClosableQueue.get_items"], ["", "def", "test_basic", "(", "self", ")", ":", "\n", "        ", "self", ".", "put_items", "(", "range", "(", "10", ")", ")", "\n", "self", ".", "assertListEqual", "(", "self", ".", "get_items", "(", "10", ")", ",", "list", "(", "range", "(", "10", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.test.test_closablequeue.TestClosableQueue.test_closed_put": [[27, 30], ["test_closablequeue.TestClosableQueue.queue.close", "test_closablequeue.TestClosableQueue.assertRaises"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.closablequeue.ClosableQueue.close"], ["", "def", "test_closed_put", "(", "self", ")", ":", "\n", "        ", "self", ".", "queue", ".", "close", "(", ")", "\n", "self", ".", "assertRaises", "(", "ClosedException", ",", "self", ".", "queue", ".", "put", ",", "42", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.test.test_closablequeue.TestClosableQueue.test_closed_get": [[31, 36], ["test_closablequeue.TestClosableQueue.put_items", "test_closablequeue.TestClosableQueue.queue.close", "test_closablequeue.TestClosableQueue.assertListEqual", "test_closablequeue.TestClosableQueue.assertRaises", "range", "test_closablequeue.TestClosableQueue.get_items", "list", "range"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.test.test_closablequeue.TestClosableQueue.put_items", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.closablequeue.ClosableQueue.close", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.test.test_closablequeue.TestClosableQueue.get_items"], ["", "def", "test_closed_get", "(", "self", ")", ":", "\n", "        ", "self", ".", "put_items", "(", "range", "(", "10", ")", ")", "\n", "self", ".", "queue", ".", "close", "(", ")", "\n", "self", ".", "assertListEqual", "(", "self", ".", "get_items", "(", "10", ")", ",", "list", "(", "range", "(", "10", ")", ")", ")", "\n", "self", ".", "assertRaises", "(", "ClosedException", ",", "self", ".", "queue", ".", "get", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.test.test_closablequeue.TestClosableQueue.test_basic_two_threads": [[37, 43], ["threading.Thread", "threading.Thread.start", "test_closablequeue.TestClosableQueue.get_items", "threading.Thread.join", "test_closablequeue.TestClosableQueue.assertListEqual", "list", "range", "range"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.test.test_closablequeue.TestClosableQueue.get_items"], ["", "def", "test_basic_two_threads", "(", "self", ")", ":", "\n", "        ", "thread", "=", "Thread", "(", "target", "=", "self", ".", "put_items", ",", "args", "=", "(", "range", "(", "20", ")", ",", ")", ")", "\n", "thread", ".", "start", "(", ")", "\n", "result", "=", "self", ".", "get_items", "(", "20", ")", "\n", "thread", ".", "join", "(", ")", "\n", "self", ".", "assertListEqual", "(", "result", ",", "list", "(", "range", "(", "20", ")", ")", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.test.test_iterators.TestCheckpointableIterator.test_basic": [[52, 54], ["test_iterators.TestCheckpointableIterator.assertListEqual", "list"], "methods", ["None"], ["def", "test_basic", "(", "self", ")", ":", "\n", "        ", "self", ".", "assertListEqual", "(", "list", "(", "self", ".", "iterator", ")", ",", "self", ".", "expected_result", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.test.test_iterators.TestCheckpointableIterator.test_checkpointing_from_start": [[55, 60], ["range", "test_iterators.TestCheckpointableIterator.iterator.setstate", "test_iterators.TestCheckpointableIterator.assertListEqual", "len", "next", "list"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.iterators.BucketedReadaheadBatchIterator.setstate"], ["", "def", "test_checkpointing_from_start", "(", "self", ")", ":", "\n", "        ", "for", "_", "in", "range", "(", "len", "(", "self", ".", "expected_result", ")", ")", ":", "\n", "            ", "next", "(", "self", ".", "iterator", ")", "\n", "", "self", ".", "iterator", ".", "setstate", "(", "None", ")", "\n", "self", ".", "assertListEqual", "(", "list", "(", "self", ".", "iterator", ")", ",", "self", ".", "expected_result", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.test.test_iterators.TestCheckpointableIterator.test_checkpointing_in_middle": [[61, 66], ["test_iterators.TestCheckpointableIterator.iterator.setstate", "test_iterators.TestCheckpointableIterator.assertListEqual", "next", "test_iterators.TestCheckpointableIterator.iterator.getstate", "range", "len"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.iterators.BucketedReadaheadBatchIterator.setstate", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.iterators.BucketedReadaheadBatchIterator.getstate"], ["", "def", "test_checkpointing_in_middle", "(", "self", ")", ":", "\n", "        ", "result", "=", "[", "next", "(", "self", ".", "iterator", ")", "for", "_", "in", "range", "(", "len", "(", "self", ".", "expected_result", ")", "//", "3", ")", "]", "\n", "self", ".", "iterator", ".", "setstate", "(", "self", ".", "iterator", ".", "getstate", "(", ")", ")", "\n", "result", "+=", "[", "item", "for", "item", "in", "self", ".", "iterator", "]", "\n", "self", ".", "assertListEqual", "(", "result", ",", "self", ".", "expected_result", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.test.test_iterators.TestCheckpointableIterator.test_checkpointing_at_end": [[67, 72], ["range", "test_iterators.TestCheckpointableIterator.iterator.setstate", "test_iterators.TestCheckpointableIterator.assertRaises", "len", "next", "test_iterators.TestCheckpointableIterator.iterator.getstate"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.iterators.BucketedReadaheadBatchIterator.setstate", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.iterators.BucketedReadaheadBatchIterator.getstate"], ["", "def", "test_checkpointing_at_end", "(", "self", ")", ":", "\n", "        ", "for", "_", "in", "range", "(", "len", "(", "self", ".", "expected_result", ")", ")", ":", "\n", "            ", "next", "(", "self", ".", "iterator", ")", "\n", "", "self", ".", "iterator", ".", "setstate", "(", "self", ".", "iterator", ".", "getstate", "(", ")", ")", "\n", "self", ".", "assertRaises", "(", "StopIteration", ",", "self", ".", "iterator", ".", "__next__", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.test.test_iterators.TestBase.setUp": [[75, 114], ["tempfile.mkdtemp", "enumerate", "os.path.join", "test_iterators.TestBase.chunk_file_paths.append", "test_iterators.TestBase.flattened_test_data.append", "gzip.open", "f.write", "str().zfill", "str"], "methods", ["None"], ["    ", "def", "setUp", "(", "self", ")", ":", "\n", "        ", "self", ".", "test_data", "=", "[", "\n", "[", "\n", "\"item number one\"", ",", "\n", "\"item number two\"", ",", "\n", "\"item number three\"", ",", "\n", "\"item number four\"", ",", "\n", "]", ",", "\n", "[", "\"item number five\"", "]", ",", "\n", "[", "\n", "\"item number six\"", ",", "\n", "\"item number seven\"", ",", "\n", "\"item number eight\"", ",", "\n", "\"item number nine\"", ",", "\n", "\"item number ten\"", ",", "\n", "\"item number eleven\"", ",", "\n", "]", ",", "\n", "[", "\n", "\"item number twelve\"", ",", "\n", "\"item number thirteen\"", ",", "\n", "\"item number fourteen\"", ",", "\n", "]", ",", "\n", "]", "\n", "\n", "self", ".", "flattened_test_data", "=", "[", "]", "\n", "for", "chunk", "in", "self", ".", "test_data", ":", "\n", "            ", "for", "item", "in", "chunk", ":", "\n", "                ", "self", ".", "flattened_test_data", ".", "append", "(", "item", ")", "\n", "\n", "", "", "self", ".", "data_dir", "=", "tempfile", ".", "mkdtemp", "(", ")", "\n", "self", ".", "chunk_file_paths", "=", "[", "]", "\n", "for", "chunk_id", ",", "chunk", "in", "enumerate", "(", "self", ".", "test_data", ")", ":", "\n", "            ", "file_name", "=", "os", ".", "path", ".", "join", "(", "\n", "self", ".", "data_dir", ",", "\"chunk_\"", "+", "str", "(", "chunk_id", ")", ".", "zfill", "(", "10", ")", "+", "\".gz\"", "\n", ")", "\n", "self", ".", "chunk_file_paths", ".", "append", "(", "file_name", ")", "\n", "file_content", "=", "\"\\n\"", ".", "join", "(", "chunk", ")", "\n", "with", "gzip", ".", "open", "(", "file_name", ",", "\"wt\"", ",", "encoding", "=", "\"utf-8\"", ")", "as", "f", ":", "\n", "                ", "f", ".", "write", "(", "file_content", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.test.test_iterators.TestBase.read_chunk": [[115, 121], ["gzip.open", "iter", "f.read().splitlines", "f.read"], "methods", ["None"], ["", "", "", "@", "staticmethod", "\n", "def", "read_chunk", "(", "\n", "textfile_path", ":", "str", ",", "\n", ")", "->", "Iterator", "[", "str", "]", ":", "# read_chunk_fn for chunked_dataset_iterator", "\n", "        ", "with", "gzip", ".", "open", "(", "textfile_path", ",", "\"rt\"", ",", "encoding", "=", "\"utf-8\"", ")", "as", "f", ":", "\n", "            ", "return", "iter", "(", "f", ".", "read", "(", ")", ".", "splitlines", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.test.test_iterators.TestBase.tearDown": [[122, 125], ["gc.collect", "shutil.rmtree"], "methods", ["None"], ["", "", "def", "tearDown", "(", "self", ")", ":", "\n", "        ", "gc", ".", "collect", "(", ")", "# this will get the pre-fetch terminated in some tests, which otherwise may still want to read these files", "\n", "shutil", ".", "rmtree", "(", "self", ".", "data_dir", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.test.test_iterators.TestBase.assertMultisetEqual": [[126, 129], ["test_iterators.TestBase.assertEqual", "test_iterators.TestBase.assertSetEqual", "len", "len", "set", "set"], "methods", ["None"], ["", "def", "assertMultisetEqual", "(", "self", ",", "a", ",", "b", ")", ":", "\n", "        ", "self", ".", "assertEqual", "(", "len", "(", "a", ")", ",", "len", "(", "b", ")", ")", "\n", "self", ".", "assertSetEqual", "(", "set", "(", "a", ")", ",", "set", "(", "b", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.test.test_iterators.TestSourceIterator.test_exception": [[132, 135], ["test_iterators.TestSourceIterator.assertRaises"], "methods", ["None"], ["    ", "def", "test_exception", "(", "self", ")", ":", "\n", "        ", "self", ".", "assertRaises", "(", "\n", "ValueError", ",", "create_source_iterator", ",", "[", "1", "]", ",", "train", "=", "False", ",", "shuffle", "=", "True", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.test.test_iterators.TestChunkedSourceIterator.setUp": [[139, 142], ["list", "infinibatch.iterators.ChunkedSourceIterator", "range"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.iterators.ChunkedSourceIterator"], ["    ", "def", "setUp", "(", "self", ")", ":", "\n", "        ", "self", ".", "expected_result", "=", "list", "(", "range", "(", "53", ")", ")", "\n", "self", ".", "iterator", "=", "ChunkedSourceIterator", "(", "self", ".", "expected_result", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.test.test_iterators.TestChunkedSourceIterator.test_multiple_instance": [[143, 154], ["range", "range", "test_iterators.TestChunkedSourceIterator.assertListEqual", "infinibatch.iterators.ChunkedSourceIterator", "items.extend", "list"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.iterators.ChunkedSourceIterator"], ["", "def", "test_multiple_instance", "(", "self", ")", ":", "\n", "        ", "for", "num_instances", "in", "range", "(", "2", ",", "17", ")", ":", "\n", "            ", "items", "=", "[", "]", "\n", "for", "rank", "in", "range", "(", "num_instances", ")", ":", "\n", "                ", "iterator", "=", "ChunkedSourceIterator", "(", "\n", "self", ".", "expected_result", ",", "\n", "num_instances", "=", "num_instances", ",", "\n", "instance_rank", "=", "rank", ",", "\n", ")", "\n", "items", ".", "extend", "(", "list", "(", "iterator", ")", ")", "\n", "", "self", ".", "assertListEqual", "(", "items", ",", "self", ".", "expected_result", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.test.test_iterators.TestInfinitePermutationSourceIterator.test_repeat_once": [[157, 164], ["iter", "list", "list", "test_iterators.TestInfinitePermutationSourceIterator.assertMultisetEqual", "test_iterators.TestInfinitePermutationSourceIterator.assertTrue", "infinibatch.iterators.InfinitePermutationSourceIterator", "itertools.islice", "itertools.islice", "any", "len", "len", "zip"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.test.test_iterators.TestBase.assertMultisetEqual"], ["    ", "def", "test_repeat_once", "(", "self", ")", ":", "\n", "# This tests that two consecutive iterations through the test data yields differently ordered sequences.", "\n", "        ", "reader", "=", "iter", "(", "InfinitePermutationSourceIterator", "(", "self", ".", "flattened_test_data", ",", "42", ")", ")", "\n", "items0", "=", "list", "(", "itertools", ".", "islice", "(", "reader", ",", "len", "(", "self", ".", "flattened_test_data", ")", ")", ")", "\n", "items1", "=", "list", "(", "itertools", ".", "islice", "(", "reader", ",", "len", "(", "self", ".", "flattened_test_data", ")", ")", ")", "\n", "self", ".", "assertMultisetEqual", "(", "items0", "+", "items1", ",", "self", ".", "flattened_test_data", "*", "2", ")", "\n", "self", ".", "assertTrue", "(", "any", "(", "item0", "!=", "item1", "for", "item0", ",", "item1", "in", "zip", "(", "items0", ",", "items1", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.test.test_iterators.TestInfinitePermutationSourceIterator.test_reiter_once": [[165, 174], ["infinibatch.iterators.InfinitePermutationSourceIterator", "infinibatch.iterators.InfinitePermutationSourceIterator.getstate", "list", "infinibatch.iterators.InfinitePermutationSourceIterator.setstate", "list", "test_iterators.TestInfinitePermutationSourceIterator.assertMultisetEqual", "test_iterators.TestInfinitePermutationSourceIterator.assertSequenceEqual", "itertools.islice", "itertools.islice", "len", "len"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.iterators.BucketedReadaheadBatchIterator.getstate", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.iterators.BucketedReadaheadBatchIterator.setstate", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.test.test_iterators.TestBase.assertMultisetEqual"], ["", "def", "test_reiter_once", "(", "self", ")", ":", "\n", "# This differs from test_repeat_once in that we use checkpoints.", "\n", "        ", "reader", "=", "InfinitePermutationSourceIterator", "(", "self", ".", "flattened_test_data", ",", "42", ")", "\n", "checkpoint", "=", "reader", ".", "getstate", "(", ")", "\n", "items0", "=", "list", "(", "itertools", ".", "islice", "(", "reader", ",", "len", "(", "self", ".", "flattened_test_data", ")", ")", ")", "\n", "reader", ".", "setstate", "(", "checkpoint", ")", "\n", "items1", "=", "list", "(", "itertools", ".", "islice", "(", "reader", ",", "len", "(", "self", ".", "flattened_test_data", ")", ")", ")", "\n", "self", ".", "assertMultisetEqual", "(", "items0", "+", "items1", ",", "self", ".", "flattened_test_data", "*", "2", ")", "\n", "self", ".", "assertSequenceEqual", "(", "items0", ",", "items1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.test.test_iterators.TestInfinitePermutationSourceIterator.test_checkpointing": [[175, 201], ["random.Random.Random", "range", "random.Random.Random.randrange", "random.Random.Random.randrange", "random.Random.Random.randrange", "list", "infinibatch.iterators.InfinitePermutationSourceIterator", "list", "infinibatch.iterators.InfinitePermutationSourceIterator.getstate", "list", "infinibatch.iterators.InfinitePermutationSourceIterator.setstate", "list", "pickle.dumps", "pickle.loads", "infinibatch.iterators.InfinitePermutationSourceIterator.setstate", "list", "test_iterators.TestInfinitePermutationSourceIterator.assertTrue", "test_iterators.TestInfinitePermutationSourceIterator.assertTrue", "range", "itertools.islice", "itertools.islice", "itertools.islice", "itertools.islice"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.iterators.BucketedReadaheadBatchIterator.getstate", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.iterators.BucketedReadaheadBatchIterator.setstate", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.iterators.BucketedReadaheadBatchIterator.setstate"], ["", "def", "test_checkpointing", "(", "self", ")", ":", "\n", "        ", "random", "=", "Random", "(", ")", "\n", "for", "i", "in", "range", "(", "5", ")", ":", "\n", "# random sequence lengths to for testing different configurations", "\n", "            ", "test_source_length", "=", "random", ".", "randrange", "(", "5", ",", "25", ")", "\n", "test_first_output_length", "=", "random", ".", "randrange", "(", "5", ",", "25", ")", "\n", "test_second_output_length", "=", "random", ".", "randrange", "(", "5", ",", "25", ")", "\n", "# source", "\n", "test_source", "=", "list", "(", "range", "(", "test_source_length", ")", ")", "\n", "reader", "=", "InfinitePermutationSourceIterator", "(", "test_source", ",", "seed", "=", "i", ")", "\n", "# fetch a first sequence", "\n", "_", "=", "list", "(", "itertools", ".", "islice", "(", "reader", ",", "test_first_output_length", ")", ")", "\n", "# fetch a second sequence", "\n", "checkpoint", "=", "reader", ".", "getstate", "(", ")", "\n", "items1a", "=", "list", "(", "itertools", ".", "islice", "(", "reader", ",", "test_second_output_length", ")", ")", "\n", "# fetch that second sequence again via checkpointing", "\n", "reader", ".", "setstate", "(", "checkpoint", ")", "\n", "items1b", "=", "list", "(", "itertools", ".", "islice", "(", "reader", ",", "test_second_output_length", ")", ")", "\n", "# and again with serialized checkpoint", "\n", "as_json", "=", "pickle", ".", "dumps", "(", "checkpoint", ")", "\n", "checkpoint2", "=", "pickle", ".", "loads", "(", "as_json", ")", "\n", "reader", ".", "setstate", "(", "checkpoint2", ")", "\n", "items1c", "=", "list", "(", "itertools", ".", "islice", "(", "reader", ",", "test_second_output_length", ")", ")", "\n", "# must be the same", "\n", "self", ".", "assertTrue", "(", "items1a", "==", "items1b", ")", "\n", "self", ".", "assertTrue", "(", "items1a", "==", "items1c", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.test.test_iterators.TestNativeCheckpointableIterator.setUp": [[204, 207], ["list", "infinibatch.iterators.NativeCheckpointableIterator", "range"], "methods", ["None"], ["    ", "def", "setUp", "(", "self", ")", ":", "\n", "        ", "self", ".", "expected_result", "=", "list", "(", "range", "(", "53", ")", ")", "\n", "self", ".", "iterator", "=", "NativeCheckpointableIterator", "(", "self", ".", "expected_result", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.test.test_iterators.TestNativeCheckpointableIterator.test_iterator_exception": [[208, 210], ["test_iterators.TestNativeCheckpointableIterator.assertRaises", "iter", "range"], "methods", ["None"], ["", "def", "test_iterator_exception", "(", "self", ")", ":", "\n", "        ", "self", ".", "assertRaises", "(", "ValueError", ",", "NativeCheckpointableIterator", ",", "iter", "(", "range", "(", "10", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.test.test_iterators.TestRecurrentIterator.setUp": [[213, 227], ["list", "infinibatch.iterators.RecurrentIterator", "range", "test_iterators.TestRecurrentIterator.expected_result.append", "infinibatch.iterators.NativeCheckpointableIterator"], "methods", ["None"], ["    ", "def", "setUp", "(", "self", ")", ":", "\n", "        ", "data", "=", "list", "(", "range", "(", "53", ")", ")", "\n", "\n", "self", ".", "expected_result", "=", "[", "0", "]", "\n", "for", "i", "in", "data", "[", "1", ":", "]", ":", "\n", "            ", "self", ".", "expected_result", ".", "append", "(", "self", ".", "expected_result", "[", "-", "1", "]", "+", "i", ")", "\n", "\n", "", "def", "step_function", "(", "prev_state", ",", "item", ")", ":", "\n", "            ", "output", "=", "item", "+", "prev_state", "\n", "new_state", "=", "output", "\n", "return", "new_state", ",", "output", "\n", "\n", "", "self", ".", "iterator", "=", "RecurrentIterator", "(", "\n", "NativeCheckpointableIterator", "(", "data", ")", ",", "step_function", ",", "initial_state", "=", "0", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.test.test_iterators.TestSamplingRandomMapIterator.setUp": [[231, 244], ["list", "random.Random.Random", "random.Random.Random.seed", "infinibatch.iterators.SamplingRandomMapIterator", "range", "infinibatch.iterators.NativeCheckpointableIterator", "random.Random.Random.random", "random.Random.Random.random"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.iterators.SamplingRandomMapIterator"], ["    ", "def", "setUp", "(", "self", ")", ":", "\n", "        ", "data", "=", "list", "(", "range", "(", "53", ")", ")", "\n", "\n", "def", "transform", "(", "random", ":", "Random", ",", "item", ":", "int", ")", ":", "\n", "            ", "return", "item", "+", "random", ".", "random", "(", ")", "\n", "\n", "", "seed", "=", "1", "\n", "random", "=", "Random", "(", ")", "\n", "random", ".", "seed", "(", "seed", ")", "\n", "self", ".", "expected_result", "=", "[", "n", "+", "random", ".", "random", "(", ")", "for", "n", "in", "data", "]", "\n", "\n", "self", ".", "iterator", "=", "SamplingRandomMapIterator", "(", "\n", "NativeCheckpointableIterator", "(", "data", ")", ",", "transform", "=", "transform", ",", "seed", "=", "seed", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.test.test_iterators.TestFixedBatchIterator.setUp": [[248, 256], ["list", "infinibatch.iterators.FixedBatchIterator", "range", "infinibatch.iterators.NativeCheckpointableIterator"], "methods", ["None"], ["    ", "def", "setUp", "(", "self", ")", ":", "\n", "        ", "data", "=", "list", "(", "range", "(", "5", ")", ")", "\n", "\n", "batch_size", "=", "3", "\n", "self", ".", "expected_result", "=", "[", "data", "[", "0", ":", "3", "]", ",", "data", "[", "3", ":", "]", "]", "\n", "\n", "self", ".", "iterator", "=", "FixedBatchIterator", "(", "\n", "NativeCheckpointableIterator", "(", "data", ")", ",", "batch_size", "=", "batch_size", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.test.test_iterators.TestSelectManyIterator._select_many_from_chunks": [[261, 265], ["infinibatch.iterators.SelectManyIterator"], "methods", ["None"], ["    ", "@", "staticmethod", "\n", "def", "_select_many_from_chunks", "(", "chunk_file_paths", ")", ":", "\n", "        ", "return", "SelectManyIterator", "(", "\n", "source_iterator", "=", "chunk_file_paths", ",", "collection_selector", "=", "TestBase", ".", "read_chunk", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.test.test_iterators.TestSelectManyIterator.test": [[267, 274], ["list", "test_iterators.TestSelectManyIterator.assertListEqual", "test_iterators.TestSelectManyIterator._select_many_from_chunks", "infinibatch.iterators.NativeCheckpointableIterator"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.test.test_iterators.TestSelectManyIterator._select_many_from_chunks"], ["", "def", "test", "(", "self", ")", ":", "\n", "        ", "items", "=", "list", "(", "\n", "self", ".", "_select_many_from_chunks", "(", "\n", "NativeCheckpointableIterator", "(", "self", ".", "chunk_file_paths", ")", "\n", ")", "\n", ")", "\n", "self", ".", "assertListEqual", "(", "items", ",", "self", ".", "flattened_test_data", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.test.test_iterators.TestSelectManyIterator.test_no_selector": [[275, 280], ["list", "list", "test_iterators.TestSelectManyIterator.assertListEqual", "range", "infinibatch.iterators.SelectManyIterator", "infinibatch.iterators.NativeCheckpointableIterator"], "methods", ["None"], ["", "def", "test_no_selector", "(", "self", ")", ":", "\n", "        ", "data", "=", "list", "(", "range", "(", "100", ")", ")", "\n", "sublists", "=", "[", "data", "[", ":", "10", "]", ",", "data", "[", "10", ":", "42", "]", ",", "data", "[", "42", ":", "87", "]", ",", "data", "[", "87", ":", "]", "]", "\n", "result", "=", "list", "(", "SelectManyIterator", "(", "NativeCheckpointableIterator", "(", "sublists", ")", ")", ")", "\n", "self", ".", "assertListEqual", "(", "result", ",", "data", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.test.test_iterators.TestSelectManyIterator.test_different_line_endings": [[281, 304], ["tempfile.mkdtemp", "os.path.join", "tempfile.mkdtemp", "os.path.join", "list", "list", "test_iterators.TestSelectManyIterator.assertListEqual", "shutil.rmtree", "shutil.rmtree", "gzip.open", "f.write", "gzip.open", "f.write", "test_iterators.TestSelectManyIterator._select_many_from_chunks", "test_iterators.TestSelectManyIterator._select_many_from_chunks", "infinibatch.iterators.NativeCheckpointableIterator", "infinibatch.iterators.NativeCheckpointableIterator"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.test.test_iterators.TestSelectManyIterator._select_many_from_chunks", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.test.test_iterators.TestSelectManyIterator._select_many_from_chunks"], ["", "def", "test_different_line_endings", "(", "self", ")", ":", "\n", "# write data in binary mode with LF line endings", "\n", "        ", "lf_dir", "=", "tempfile", ".", "mkdtemp", "(", ")", "\n", "lf_file", "=", "os", ".", "path", ".", "join", "(", "lf_dir", ",", "\"test.gz\"", ")", "\n", "with", "gzip", ".", "open", "(", "lf_file", ",", "\"w\"", ")", "as", "f", ":", "\n", "            ", "f", ".", "write", "(", "\"\\n\"", ".", "join", "(", "self", ".", "flattened_test_data", ")", ".", "encode", "(", "\"utf-8\"", ")", ")", "\n", "\n", "# write data in binary mode with CRLF line endings", "\n", "", "crlf_dir", "=", "tempfile", ".", "mkdtemp", "(", ")", "\n", "crlf_file", "=", "os", ".", "path", ".", "join", "(", "crlf_dir", ",", "\"test.gz\"", ")", "\n", "with", "gzip", ".", "open", "(", "crlf_file", ",", "\"w\"", ")", "as", "f", ":", "\n", "            ", "f", ".", "write", "(", "\"\\r\\n\"", ".", "join", "(", "self", ".", "flattened_test_data", ")", ".", "encode", "(", "\"utf-8\"", ")", ")", "\n", "\n", "", "lf_data", "=", "list", "(", "\n", "self", ".", "_select_many_from_chunks", "(", "NativeCheckpointableIterator", "(", "[", "lf_file", "]", ")", ")", "\n", ")", "\n", "crlf_dat", "=", "list", "(", "\n", "self", ".", "_select_many_from_chunks", "(", "NativeCheckpointableIterator", "(", "[", "crlf_file", "]", ")", ")", "\n", ")", "\n", "self", ".", "assertListEqual", "(", "lf_data", ",", "crlf_dat", ")", "\n", "\n", "shutil", ".", "rmtree", "(", "lf_dir", ")", "\n", "shutil", ".", "rmtree", "(", "crlf_dir", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.test.test_iterators.TestSelectManyIterator.test_checkpointing": [[305, 328], ["infinibatch.iterators.InfinitePermutationSourceIterator", "random.Random.Random", "range", "os.path.join", "random.Random.Random.randrange", "random.Random.Random.randrange", "test_iterators.TestSelectManyIterator._select_many_from_chunks", "range", "test_iterators.TestSelectManyIterator.getstate", "list", "test_iterators.TestSelectManyIterator.setstate", "list", "test_iterators.TestSelectManyIterator.assertListEqual", "os.scandir", "next", "itertools.islice", "itertools.islice", "subpath.is_file", "subpath.name.endswith"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.test.test_iterators.TestSelectManyIterator._select_many_from_chunks", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.iterators.BucketedReadaheadBatchIterator.getstate", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.iterators.BucketedReadaheadBatchIterator.setstate"], ["", "def", "test_checkpointing", "(", "self", ")", ":", "\n", "        ", "chunk_file_paths", "=", "[", "\n", "os", ".", "path", ".", "join", "(", "self", ".", "data_dir", ",", "subpath", ".", "name", ")", "\n", "for", "subpath", "in", "os", ".", "scandir", "(", "self", ".", "data_dir", ")", "\n", "if", "subpath", ".", "is_file", "(", ")", "and", "subpath", ".", "name", ".", "endswith", "(", "\".gz\"", ")", "\n", "]", "\n", "chunk_file_paths", "=", "InfinitePermutationSourceIterator", "(", "\n", "chunk_file_paths", ",", "shuffle", "=", "False", "\n", ")", "# using this as checkpointed cycle()", "\n", "random", "=", "Random", "(", "1", ")", "\n", "for", "_", "in", "range", "(", "5", ")", ":", "\n", "            ", "first_length", "=", "random", ".", "randrange", "(", "11", ",", "31", ")", "\n", "extra_length", "=", "random", ".", "randrange", "(", "11", ",", "33", ")", "\n", "dataset", "=", "self", ".", "_select_many_from_chunks", "(", "chunk_file_paths", ")", "\n", "for", "_", "in", "range", "(", "first_length", ")", ":", "\n", "                ", "next", "(", "dataset", ")", "\n", "", "checkpoint", "=", "dataset", ".", "getstate", "(", ")", "\n", "items0", "=", "list", "(", "itertools", ".", "islice", "(", "dataset", ",", "extra_length", ")", ")", "\n", "# print(len(items0))", "\n", "dataset", ".", "setstate", "(", "checkpoint", ")", "\n", "items1", "=", "list", "(", "itertools", ".", "islice", "(", "dataset", ",", "extra_length", ")", ")", "\n", "# print(len(items1))", "\n", "self", ".", "assertListEqual", "(", "items0", ",", "items1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.test.test_iterators.TestBufferedShuffleIterator.test_shuffle": [[331, 339], ["list", "test_iterators.TestBufferedShuffleIterator.assertMultisetEqual", "infinibatch.iterators.BufferedShuffleIterator", "infinibatch.iterators.NativeCheckpointableIterator", "test_iterators.TestBufferedShuffleIterator.flattened_test_data.copy"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.test.test_iterators.TestBase.assertMultisetEqual"], ["    ", "def", "test_shuffle", "(", "self", ")", ":", "\n", "# work on copy of data in case data is modified by class", "\n", "        ", "items", "=", "list", "(", "\n", "BufferedShuffleIterator", "(", "\n", "NativeCheckpointableIterator", "(", "self", ".", "flattened_test_data", ".", "copy", "(", ")", ")", ",", "971", ",", "42", "\n", ")", "\n", ")", "\n", "self", ".", "assertMultisetEqual", "(", "items", ",", "self", ".", "flattened_test_data", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.test.test_iterators.TestBufferedShuffleIterator.test_shuffle_buffer_size_one": [[340, 348], ["list", "test_iterators.TestBufferedShuffleIterator.assertListEqual", "infinibatch.iterators.BufferedShuffleIterator", "infinibatch.iterators.NativeCheckpointableIterator", "test_iterators.TestBufferedShuffleIterator.flattened_test_data.copy"], "methods", ["None"], ["", "def", "test_shuffle_buffer_size_one", "(", "self", ")", ":", "\n", "# work on copy of data in case data is modified by class", "\n", "        ", "items", "=", "list", "(", "\n", "BufferedShuffleIterator", "(", "\n", "NativeCheckpointableIterator", "(", "self", ".", "flattened_test_data", ".", "copy", "(", ")", ")", ",", "1", ",", "42", "\n", ")", "\n", ")", "\n", "self", ".", "assertListEqual", "(", "items", ",", "self", ".", "flattened_test_data", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.test.test_iterators.TestBlockwiseShuffleIterator.test_shuffle": [[352, 360], ["list", "test_iterators.TestBlockwiseShuffleIterator.assertMultisetEqual", "infinibatch.iterators.BlockwiseShuffleIterator", "infinibatch.iterators.NativeCheckpointableIterator", "test_iterators.TestBlockwiseShuffleIterator.flattened_test_data.copy"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.test.test_iterators.TestBase.assertMultisetEqual", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.iterators.BlockwiseShuffleIterator"], ["    ", "def", "test_shuffle", "(", "self", ")", ":", "\n", "# work on copy of data in case data is modified by class", "\n", "        ", "items", "=", "list", "(", "\n", "BlockwiseShuffleIterator", "(", "\n", "NativeCheckpointableIterator", "(", "self", ".", "flattened_test_data", ".", "copy", "(", ")", ")", ",", "971", ",", "42", "\n", ")", "\n", ")", "\n", "self", ".", "assertMultisetEqual", "(", "items", ",", "self", ".", "flattened_test_data", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.test.test_iterators.TestBlockwiseShuffleIterator.test_shuffle_buffer_size_one": [[361, 369], ["list", "test_iterators.TestBlockwiseShuffleIterator.assertListEqual", "infinibatch.iterators.BlockwiseShuffleIterator", "infinibatch.iterators.NativeCheckpointableIterator", "test_iterators.TestBlockwiseShuffleIterator.flattened_test_data.copy"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.iterators.BlockwiseShuffleIterator"], ["", "def", "test_shuffle_buffer_size_one", "(", "self", ")", ":", "\n", "# work on copy of data in case data is modified by class", "\n", "        ", "items", "=", "list", "(", "\n", "BlockwiseShuffleIterator", "(", "\n", "NativeCheckpointableIterator", "(", "self", ".", "flattened_test_data", ".", "copy", "(", ")", ")", ",", "1", ",", "42", "\n", ")", "\n", ")", "\n", "self", ".", "assertListEqual", "(", "items", ",", "self", ".", "flattened_test_data", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.test.test_iterators.TestMapIterator.setUp": [[376, 380], ["list", "infinibatch.iterators.MapIterator", "range", "test_iterators.map_fun", "infinibatch.iterators.NativeCheckpointableIterator"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.test.test_iterators.map_fun"], ["    ", "def", "setUp", "(", "self", ")", ":", "\n", "        ", "data", "=", "list", "(", "range", "(", "53", ")", ")", "\n", "self", ".", "expected_result", "=", "[", "map_fun", "(", "n", ")", "for", "n", "in", "data", "]", "\n", "self", ".", "iterator", "=", "MapIterator", "(", "NativeCheckpointableIterator", "(", "data", ")", ",", "map_fun", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.test.test_iterators.TestParallelMapIterator.setUp": [[383, 388], ["list", "infinibatch.iterators.ParallelMapIterator", "range", "test_iterators.map_fun", "infinibatch.iterators.NativeCheckpointableIterator"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.iterators.ParallelMapIterator", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.test.test_iterators.map_fun"], ["    ", "def", "setUp", "(", "self", ")", ":", "\n", "        ", "data", "=", "list", "(", "range", "(", "53", ")", ")", "\n", "self", ".", "expected_result", "=", "[", "map_fun", "(", "n", ")", "for", "n", "in", "data", "]", "\n", "self", ".", "iterator", "=", "ParallelMapIterator", "(", "\n", "NativeCheckpointableIterator", "(", "data", ")", ",", "map_fun", ",", "5", ",", "7", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.test.test_iterators.TestZipIterator.setUp": [[392, 398], ["list", "list", "infinibatch.iterators.ZipIterator", "range", "zip", "infinibatch.iterators.NativeCheckpointableIterator", "infinibatch.iterators.NativeCheckpointableIterator"], "methods", ["None"], ["    ", "def", "setUp", "(", "self", ")", ":", "\n", "        ", "data1", "=", "list", "(", "range", "(", "53", ")", ")", "\n", "data2", "=", "[", "n", "*", "n", "for", "n", "in", "data1", "]", "\n", "self", ".", "expected_result", "=", "list", "(", "zip", "(", "data1", ",", "data2", ")", ")", "\n", "self", ".", "iterator", "=", "ZipIterator", "(", "\n", "NativeCheckpointableIterator", "(", "data1", ")", ",", "NativeCheckpointableIterator", "(", "data2", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.test.test_iterators.TestWindowedIterator.test": [[402, 417], ["list", "infinibatch.iterators.WindowedIterator", "list", "infinibatch.iterators.WindowedIterator.getstate", "list", "infinibatch.iterators.WindowedIterator.setstate", "list", "list", "test_iterators.TestWindowedIterator.assertListEqual", "test_iterators.TestWindowedIterator.assertListEqual", "range", "infinibatch.iterators.NativeCheckpointableIterator", "itertools.islice", "zip", "itertools.islice", "itertools.islice"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.iterators.BucketedReadaheadBatchIterator.getstate", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.iterators.BucketedReadaheadBatchIterator.setstate"], ["    ", "def", "test", "(", "self", ")", ":", "\n", "        ", "for", "n", "in", "[", "0", ",", "2", ",", "3", ",", "8", ",", "9", ",", "10", ",", "11", ",", "12", "]", ":", "# cover various boundary conditions", "\n", "            ", "seq", "=", "list", "(", "range", "(", "n", ")", ")", "\n", "it", "=", "WindowedIterator", "(", "NativeCheckpointableIterator", "(", "seq", ")", ",", "3", ")", "\n", "actual0", "=", "list", "(", "itertools", ".", "islice", "(", "it", ",", "n", "*", "3", "//", "10", ")", ")", "\n", "checkpoint", "=", "it", ".", "getstate", "(", ")", "\n", "actual1a", "=", "list", "(", "it", ")", "\n", "it", ".", "setstate", "(", "checkpoint", ")", "\n", "actual1b", "=", "list", "(", "it", ")", "\n", "actual", "=", "actual0", "+", "actual1a", "\n", "expected", "=", "list", "(", "\n", "zip", "(", "seq", ",", "itertools", ".", "islice", "(", "seq", ",", "1", ",", "None", ")", ",", "itertools", ".", "islice", "(", "seq", ",", "2", ",", "None", ")", ")", "\n", ")", "\n", "self", ".", "assertListEqual", "(", "actual", ",", "expected", ")", "# basic operation", "\n", "self", ".", "assertListEqual", "(", "actual1a", ",", "actual1b", ")", "# checkpointing", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.test.test_iterators.TestRandomIterator.test": [[420, 429], ["infinibatch.iterators.RandomIterator", "list", "infinibatch.iterators.RandomIterator.getstate", "list", "infinibatch.iterators.RandomIterator.setstate", "list", "test_iterators.TestRandomIterator.assertListEqual", "itertools.islice", "itertools.islice", "itertools.islice"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.iterators.BucketedReadaheadBatchIterator.getstate", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.iterators.BucketedReadaheadBatchIterator.setstate"], ["    ", "def", "test", "(", "self", ")", ":", "\n", "        ", "n", "=", "100", "\n", "it", "=", "RandomIterator", "(", "seed", "=", "1", ")", "\n", "_", "=", "list", "(", "itertools", ".", "islice", "(", "it", ",", "n", "*", "3", "//", "10", ")", ")", "\n", "checkpoint", "=", "it", ".", "getstate", "(", ")", "\n", "items1a", "=", "list", "(", "itertools", ".", "islice", "(", "it", ",", "n", "*", "7", "//", "10", ")", ")", "\n", "it", ".", "setstate", "(", "checkpoint", ")", "\n", "items1b", "=", "list", "(", "itertools", ".", "islice", "(", "it", ",", "n", "*", "7", "//", "10", ")", ")", "\n", "self", ".", "assertListEqual", "(", "items1a", ",", "items1b", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.test.test_iterators.TestPrefetchIterator.setUp": [[432, 436], ["list", "infinibatch.iterators.NativeCheckpointableIterator", "infinibatch.iterators.PrefetchIterator", "range"], "methods", ["None"], ["    ", "def", "setUp", "(", "self", ")", ":", "\n", "        ", "self", ".", "expected_result", "=", "list", "(", "range", "(", "53", ")", ")", "\n", "source_iterator", "=", "NativeCheckpointableIterator", "(", "self", ".", "expected_result", ")", "\n", "self", ".", "iterator", "=", "PrefetchIterator", "(", "source_iterator", ",", "buffer_size", "=", "13", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.test.test_iterators.Test_chunked_dataset_iterator.test_no_shuffle": [[439, 452], ["list", "test_iterators.Test_chunked_dataset_iterator.assertListEqual", "itertools.islice", "infinibatch.datasets.chunked_dataset_iterator", "len"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.datasets.chunked_dataset_iterator"], ["    ", "def", "test_no_shuffle", "(", "self", ")", ":", "\n", "        ", "items", "=", "list", "(", "\n", "itertools", ".", "islice", "(", "\n", "chunked_dataset_iterator", "(", "\n", "self", ".", "chunk_file_paths", ",", "\n", "self", ".", "read_chunk", ",", "\n", "shuffle", "=", "False", ",", "\n", "buffer_size", "=", "1000", ",", "\n", ")", ",", "\n", "len", "(", "self", ".", "flattened_test_data", ")", ",", "\n", ")", "\n", ")", "\n", "self", ".", "assertListEqual", "(", "items", ",", "self", ".", "flattened_test_data", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.test.test_iterators.Test_chunked_dataset_iterator.test_other_files_present": [[453, 468], ["list", "test_iterators.Test_chunked_dataset_iterator.assertListEqual", "open", "f.write", "itertools.islice", "os.path.join", "infinibatch.datasets.chunked_dataset_iterator", "len"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.datasets.chunked_dataset_iterator"], ["", "def", "test_other_files_present", "(", "self", ")", ":", "\n", "        ", "with", "open", "(", "os", ".", "path", ".", "join", "(", "self", ".", "data_dir", ",", "\"i_do_not_belong_here.txt\"", ")", ",", "\"w\"", ")", "as", "f", ":", "\n", "            ", "f", ".", "write", "(", "\"really ...\"", ")", "\n", "", "items", "=", "list", "(", "\n", "itertools", ".", "islice", "(", "\n", "chunked_dataset_iterator", "(", "\n", "self", ".", "chunk_file_paths", ",", "\n", "self", ".", "read_chunk", ",", "\n", "shuffle", "=", "False", ",", "\n", "buffer_size", "=", "1000", ",", "\n", ")", ",", "\n", "len", "(", "self", ".", "flattened_test_data", ")", ",", "\n", ")", "\n", ")", "\n", "self", ".", "assertListEqual", "(", "items", ",", "self", ".", "flattened_test_data", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.test.test_iterators.Test_chunked_dataset_iterator.test_transform": [[469, 485], ["list", "test_iterators.Test_chunked_dataset_iterator.assertListEqual", "transform", "itertools.islice", "infinibatch.datasets.chunked_dataset_iterator", "len"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.datasets.chunked_dataset_iterator"], ["", "def", "test_transform", "(", "self", ")", ":", "\n", "        ", "transform", "=", "lambda", "s", ":", "s", "+", "\"!\"", "\n", "modified_test_data", "=", "[", "transform", "(", "s", ")", "for", "s", "in", "self", ".", "flattened_test_data", "]", "\n", "items", "=", "list", "(", "\n", "itertools", ".", "islice", "(", "\n", "chunked_dataset_iterator", "(", "\n", "self", ".", "chunk_file_paths", ",", "\n", "self", ".", "read_chunk", ",", "\n", "shuffle", "=", "False", ",", "\n", "buffer_size", "=", "1000", ",", "\n", "transform", "=", "transform", ",", "\n", ")", ",", "\n", "len", "(", "self", ".", "flattened_test_data", ")", ",", "\n", ")", "\n", ")", "\n", "self", ".", "assertListEqual", "(", "items", ",", "modified_test_data", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.test.test_iterators.Test_chunked_dataset_iterator.test_two_instances": [[486, 510], ["infinibatch.datasets.chunked_dataset_iterator", "infinibatch.datasets.chunked_dataset_iterator", "list", "list", "test_iterators.Test_chunked_dataset_iterator.assertMultisetEqual", "itertools.islice", "itertools.islice", "set", "len", "len", "len", "len"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.datasets.chunked_dataset_iterator", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.datasets.chunked_dataset_iterator", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.test.test_iterators.TestBase.assertMultisetEqual"], ["", "def", "test_two_instances", "(", "self", ")", ":", "\n", "        ", "dataset0", "=", "chunked_dataset_iterator", "(", "\n", "self", ".", "chunk_file_paths", ",", "\n", "self", ".", "read_chunk", ",", "\n", "shuffle", "=", "False", ",", "\n", "buffer_size", "=", "1000", ",", "\n", "num_instances", "=", "2", ",", "\n", "instance_rank", "=", "0", ",", "\n", ")", "\n", "dataset1", "=", "chunked_dataset_iterator", "(", "\n", "self", ".", "chunk_file_paths", ",", "\n", "self", ".", "read_chunk", ",", "\n", "shuffle", "=", "False", ",", "\n", "buffer_size", "=", "1000", ",", "\n", "num_instances", "=", "2", ",", "\n", "instance_rank", "=", "1", ",", "\n", ")", "\n", "items0", "=", "list", "(", "\n", "itertools", ".", "islice", "(", "dataset0", ",", "len", "(", "self", ".", "test_data", "[", "0", "]", ")", "+", "len", "(", "self", ".", "test_data", "[", "2", "]", ")", ")", "\n", ")", "\n", "items1", "=", "list", "(", "\n", "itertools", ".", "islice", "(", "dataset1", ",", "len", "(", "self", ".", "test_data", "[", "1", "]", ")", "+", "len", "(", "self", ".", "test_data", "[", "3", "]", ")", ")", "\n", ")", "\n", "self", ".", "assertMultisetEqual", "(", "set", "(", "items0", "+", "items1", ")", ",", "self", ".", "flattened_test_data", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.test.test_iterators.Test_chunked_dataset_iterator.test_checkpointing": [[511, 534], ["random.Random.Random", "range", "random.Random.Random.randrange", "random.Random.Random.randrange", "infinibatch.datasets.chunked_dataset_iterator", "range", "infinibatch.datasets.chunked_dataset_iterator.getstate", "list", "infinibatch.datasets.chunked_dataset_iterator.setstate", "list", "test_iterators.Test_chunked_dataset_iterator.assertListEqual", "next", "itertools.islice", "itertools.islice"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.datasets.chunked_dataset_iterator", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.iterators.BucketedReadaheadBatchIterator.getstate", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.iterators.BucketedReadaheadBatchIterator.setstate"], ["", "def", "test_checkpointing", "(", "self", ")", ":", "\n", "        ", "random", "=", "Random", "(", "1", ")", "\n", "for", "use_windowed", "in", "(", "True", ",", "False", ")", ":", "\n", "            ", "for", "i", "in", "range", "(", "2", ")", ":", "\n", "                ", "first_length", "=", "random", ".", "randrange", "(", "11", ",", "21", ")", "\n", "extra_length", "=", "random", ".", "randrange", "(", "11", ",", "21", ")", "\n", "dataset", "=", "chunked_dataset_iterator", "(", "\n", "self", ".", "chunk_file_paths", ",", "\n", "self", ".", "read_chunk", ",", "\n", "shuffle", "=", "(", "i", "%", "2", "==", "0", ")", ",", "\n", "buffer_size", "=", "1000", ",", "\n", "seed", "=", "i", ",", "\n", "num_instances", "=", "2", ",", "\n", "instance_rank", "=", "0", ",", "\n", "use_windowed", "=", "use_windowed", ",", "\n", ")", "\n", "for", "_", "in", "range", "(", "first_length", ")", ":", "\n", "                    ", "next", "(", "dataset", ")", "\n", "", "checkpoint", "=", "dataset", ".", "getstate", "(", ")", "\n", "items1", "=", "list", "(", "itertools", ".", "islice", "(", "dataset", ",", "extra_length", ")", ")", "\n", "dataset", ".", "setstate", "(", "checkpoint", ")", "\n", "items2", "=", "list", "(", "itertools", ".", "islice", "(", "dataset", ",", "extra_length", ")", ")", "\n", "self", ".", "assertListEqual", "(", "items1", ",", "items2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.test.test_iterators.TestBucketedReadaheadBatchIterator.txest_basic_functionality": [[537, 574], ["infinibatch.iterators.BucketedReadaheadBatchIterator", "list", "infinibatch.iterators.BucketedReadaheadBatchIterator", "list", "print", "test_iterators.TestBucketedReadaheadBatchIterator.assertListEqual", "infinibatch.datasets.chunked_dataset_iterator", "itertools.islice", "infinibatch.datasets.chunked_dataset_iterator", "itertools.islice", "len", "len", "len", "len", "len", "len"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.datasets.chunked_dataset_iterator", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.datasets.chunked_dataset_iterator"], ["    ", "def", "txest_basic_functionality", "(", "self", ")", ":", "\n", "        ", "num_batches", "=", "13", "\n", "batch_labels", "=", "(", "\n", "75", "# note: these settings imply a few iterations through the chunks", "\n", ")", "\n", "# basic operation, should not crash", "\n", "bg", "=", "BucketedReadaheadBatchIterator", "(", "\n", "chunked_dataset_iterator", "(", "\n", "self", ".", "chunk_file_paths", ",", "\n", "self", ".", "read_chunk", ",", "\n", "shuffle", "=", "True", ",", "\n", "buffer_size", "=", "1000", ",", "\n", "seed", "=", "1", ",", "\n", ")", ",", "\n", "read_ahead", "=", "100", ",", "\n", "seed", "=", "1", ",", "\n", "key", "=", "lambda", "line", ":", "len", "(", "line", ")", ",", "\n", "batch_size", "=", "lambda", "line", ":", "batch_labels", "//", "(", "1", "+", "len", "(", "line", ")", ")", ",", "\n", ")", "\n", "batches1", "=", "list", "(", "itertools", ".", "islice", "(", "bg", ",", "num_batches", ")", ")", "\n", "# verify determinism", "\n", "bg", "=", "BucketedReadaheadBatchIterator", "(", "\n", "chunked_dataset_iterator", "(", "\n", "self", ".", "chunk_file_paths", ",", "\n", "self", ".", "read_chunk", ",", "\n", "shuffle", "=", "True", ",", "\n", "buffer_size", "=", "1000", ",", "\n", "seed", "=", "1", ",", "\n", ")", ",", "\n", "read_ahead", "=", "100", ",", "\n", "seed", "=", "1", ",", "\n", "key", "=", "lambda", "line", ":", "len", "(", "line", ")", ",", "\n", "batch_size", "=", "lambda", "line", ":", "batch_labels", "//", "(", "1", "+", "len", "(", "line", ")", ")", ",", "\n", ")", "\n", "batches2", "=", "list", "(", "itertools", ".", "islice", "(", "bg", ",", "num_batches", ")", ")", "\n", "print", "(", "[", "(", "len", "(", "batch", "[", "0", "]", ")", ",", "len", "(", "batch", ")", ")", "for", "batch", "in", "batches1", "]", ")", "\n", "self", ".", "assertListEqual", "(", "batches1", ",", "batches2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.test.test_iterators.TestBucketedReadaheadBatchIterator.test_checkpointing": [[575, 598], ["infinibatch.iterators.BucketedReadaheadBatchIterator", "list", "infinibatch.iterators.BucketedReadaheadBatchIterator.getstate", "list", "infinibatch.iterators.BucketedReadaheadBatchIterator.setstate", "list", "test_iterators.TestBucketedReadaheadBatchIterator.assertListEqual", "infinibatch.datasets.chunked_dataset_iterator", "itertools.islice", "itertools.islice", "itertools.islice", "len", "len"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.iterators.BucketedReadaheadBatchIterator.getstate", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.iterators.BucketedReadaheadBatchIterator.setstate", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.datasets.chunked_dataset_iterator"], ["", "def", "test_checkpointing", "(", "self", ")", ":", "\n", "        ", "first_batches", "=", "12", "\n", "extra_batches", "=", "7", "\n", "batch_labels", "=", "123", "\n", "bg", "=", "BucketedReadaheadBatchIterator", "(", "\n", "chunked_dataset_iterator", "(", "\n", "self", ".", "chunk_file_paths", ",", "\n", "self", ".", "read_chunk", ",", "\n", "shuffle", "=", "True", ",", "\n", "buffer_size", "=", "1000", ",", "\n", "seed", "=", "1", ",", "\n", ")", ",", "\n", "read_ahead", "=", "100", ",", "\n", "seed", "=", "1", ",", "\n", "key", "=", "lambda", "line", ":", "len", "(", "line", ")", ",", "\n", "batch_size", "=", "lambda", "line", ":", "batch_labels", "//", "(", "1", "+", "len", "(", "line", ")", ")", ",", "\n", ")", "\n", "_", "=", "list", "(", "itertools", ".", "islice", "(", "bg", ",", "first_batches", ")", ")", "\n", "checkpoint", "=", "bg", ".", "getstate", "(", ")", "\n", "batches1", "=", "list", "(", "itertools", ".", "islice", "(", "bg", ",", "extra_batches", ")", ")", "\n", "bg", ".", "setstate", "(", "checkpoint", ")", "\n", "batches2", "=", "list", "(", "itertools", ".", "islice", "(", "bg", ",", "extra_batches", ")", ")", "\n", "self", ".", "assertListEqual", "(", "batches1", ",", "batches2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.test.test_iterators.map_fun": [[371, 373], ["None"], "function", ["None"], ["", "", "def", "map_fun", "(", "n", ")", ":", "\n", "    ", "return", "n", "+", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.test.test_doctests.load_tests": [[15, 18], ["tests.addTests", "doctest.DocTestSuite"], "function", ["None"], ["def", "load_tests", "(", "loader", ",", "tests", ",", "ignore", ")", ":", "\n", "    ", "tests", ".", "addTests", "(", "doctest", ".", "DocTestSuite", "(", "infinibatch", ".", "iterators", ")", ")", "\n", "return", "tests", "\n", "", ""]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.bin.block_randomize._try_parse_azure_blob_uri": [[29, 40], ["re.compile().match", "re.compile().match.group", "re.compile().match.group", "re.compile().match.group", "re.compile"], "function", ["None"], ["def", "_try_parse_azure_blob_uri", "(", "path", ":", "str", ")", ":", "\n", "    ", "try", ":", "\n", "        ", "m", "=", "re", ".", "compile", "(", "\"https://([a-z0-9]*).blob.core.windows.net/([^/]*)/(.*)\"", ")", ".", "match", "(", "\n", "path", "\n", ")", "\n", "# print (m.group(1))", "\n", "# print (m.group(2))", "\n", "# print (m.group(3))", "\n", "return", "(", "m", ".", "group", "(", "1", ")", ",", "m", ".", "group", "(", "2", ")", ",", "m", ".", "group", "(", "3", ")", ")", "\n", "", "except", ":", "\n", "        ", "return", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.bin.block_randomize._get_azure_key": [[42, 51], ["isinstance"], "function", ["None"], ["", "", "def", "_get_azure_key", "(", "\n", "storage_account", ":", "str", ",", "credentials", ":", "Optional", "[", "Union", "[", "str", ",", "Dict", "[", "str", ",", "str", "]", "]", "]", "\n", ")", ":", "\n", "    ", "if", "not", "credentials", ":", "\n", "        ", "return", "None", "\n", "", "elif", "isinstance", "(", "credentials", ",", "str", ")", ":", "\n", "        ", "return", "credentials", "\n", "", "else", ":", "\n", "        ", "return", "credentials", "[", "storage_account", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.bin.block_randomize.read_utf8_file": [[53, 84], ["block_randomize._try_parse_azure_blob_uri", "path.endswith", "iter", "BlobClient.from_blob_url().download_blob().readall", "gzip.decompress", "f.read.decode().splitlines", "open", "f.read", "print", "BlobClient.from_blob_url().download_blob", "f.read.decode", "BlobClient.from_blob_url", "block_randomize._get_azure_key"], "function", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.bin.block_randomize._try_parse_azure_blob_uri", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizerFast.decode", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.bin.block_randomize._get_azure_key"], ["", "", "def", "read_utf8_file", "(", "\n", "path", ":", "str", ",", "credentials", ":", "Optional", "[", "Union", "[", "str", ",", "Dict", "[", "str", ",", "str", "]", "]", "]", "\n", ")", "->", "Iterator", "[", "str", "]", ":", "\n", "    ", "blob_data", "=", "_try_parse_azure_blob_uri", "(", "path", ")", "\n", "if", "blob_data", "is", "None", ":", "\n", "        ", "with", "open", "(", "path", ",", "\"rb\"", ")", "as", "f", ":", "\n", "            ", "data", "=", "f", ".", "read", "(", ")", "\n", "", "", "else", ":", "\n", "        ", "try", ":", "\n", "# pip install azure-storage-blob", "\n", "            ", "from", "azure", ".", "storage", ".", "blob", "import", "BlobClient", "\n", "", "except", ":", "\n", "            ", "print", "(", "\n", "\"Failed to import azure.storage.blob. Please pip install azure-storage-blob\"", ",", "\n", "file", "=", "sys", ".", "stderr", ",", "\n", ")", "\n", "raise", "\n", "", "data", "=", "(", "\n", "BlobClient", ".", "from_blob_url", "(", "\n", "path", ",", "\n", "credential", "=", "_get_azure_key", "(", "\n", "storage_account", "=", "blob_data", "[", "0", "]", ",", "credentials", "=", "credentials", "\n", ")", ",", "\n", ")", "\n", ".", "download_blob", "(", ")", "\n", ".", "readall", "(", ")", "\n", ")", "\n", "", "if", "path", ".", "endswith", "(", "\".gz\"", ")", ":", "\n", "        ", "data", "=", "gzip", ".", "decompress", "(", "data", ")", "\n", "# @TODO: auto-detect UCS-2 by BOM", "\n", "", "return", "iter", "(", "data", ".", "decode", "(", "encoding", "=", "\"utf-8\"", ")", ".", "splitlines", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.bin.block_randomize.enumerate_files": [[86, 131], ["block_randomize._try_parse_azure_blob_uri", "print", "ContainerClient.from_container_url", "print", "os.path.join", "os.path.join", "blob_path.endswith", "len", "print", "os.scandir", "os.scandir", "print", "block_randomize._get_azure_key", "ContainerClient.from_container_url.walk_blobs", "path.is_file", "blob[].endswith", "path.name.endswith"], "function", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.bin.block_randomize._try_parse_azure_blob_uri", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.bin.block_randomize._get_azure_key"], ["", "def", "enumerate_files", "(", "\n", "dir", ":", "str", ",", "ext", ":", "str", ",", "credentials", ":", "Optional", "[", "Union", "[", "str", ",", "Dict", "[", "str", ",", "str", "]", "]", "]", "\n", ")", ":", "\n", "    ", "blob_data", "=", "_try_parse_azure_blob_uri", "(", "dir", ")", "\n", "if", "blob_data", "is", "None", ":", "\n", "        ", "return", "[", "\n", "os", ".", "path", ".", "join", "(", "dir", ",", "path", ".", "name", ")", "\n", "for", "path", "in", "os", ".", "scandir", "(", "dir", ")", "\n", "if", "path", ".", "is_file", "(", ")", "and", "(", "ext", "is", "None", "or", "path", ".", "name", ".", "endswith", "(", "ext", ")", ")", "\n", "]", "\n", "", "else", ":", "\n", "        ", "try", ":", "\n", "# pip install azure-storage-blob", "\n", "            ", "from", "azure", ".", "storage", ".", "blob", "import", "ContainerClient", "\n", "", "except", ":", "\n", "            ", "print", "(", "\n", "\"Failed to import azure.storage.blob. Please pip install azure-storage-blob\"", ",", "\n", "file", "=", "sys", ".", "stderr", ",", "\n", ")", "\n", "raise", "\n", "", "account", ",", "container", ",", "blob_path", "=", "blob_data", "\n", "\n", "print", "(", "\"enumerate_files: enumerating blobs in\"", ",", "dir", ",", "file", "=", "sys", ".", "stderr", ",", "flush", "=", "True", ")", "\n", "# @BUGBUG: The prefix does not seem to have to start; seems it can also be a substring", "\n", "container_uri", "=", "\"https://\"", "+", "account", "+", "\".blob.core.windows.net/\"", "+", "container", "\n", "container_client", "=", "ContainerClient", ".", "from_container_url", "(", "\n", "container_uri", ",", "credential", "=", "_get_azure_key", "(", "account", ",", "credentials", ")", "\n", ")", "\n", "if", "not", "blob_path", ".", "endswith", "(", "\"/\"", ")", ":", "\n", "            ", "blob_path", "+=", "\"/\"", "\n", "", "blob_uris", "=", "[", "\n", "container_uri", "+", "\"/\"", "+", "blob", "[", "\"name\"", "]", "\n", "for", "blob", "in", "container_client", ".", "walk_blobs", "(", "blob_path", ",", "delimiter", "=", "\"\"", ")", "\n", "if", "(", "ext", "is", "None", "or", "blob", "[", "\"name\"", "]", ".", "endswith", "(", "ext", ")", ")", "\n", "]", "\n", "print", "(", "\n", "\"enumerate_files:\"", ",", "\n", "len", "(", "blob_uris", ")", ",", "\n", "\"blobs found\"", ",", "\n", "file", "=", "sys", ".", "stderr", ",", "\n", "flush", "=", "True", ",", "\n", ")", "\n", "for", "blob_name", "in", "blob_uris", "[", ":", "10", "]", ":", "\n", "            ", "print", "(", "blob_name", ",", "file", "=", "sys", ".", "stderr", ",", "flush", "=", "True", ")", "\n", "", "return", "blob_uris", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.torch.data.IterableCheckpointedDataset.__init__": [[17, 20], ["super().__init__"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.tests.model_selector_test.ToyModel.__init__"], ["def", "__init__", "(", "self", ",", "source", ":", "CheckpointableIterator", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "_source", "=", "source", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.torch.data.IterableCheckpointedDataset.__iter__": [[21, 27], ["torch.utils.data.get_worker_info", "iter"], "methods", ["None"], ["", "def", "__iter__", "(", "self", ")", ":", "# this is called in the forked clone", "\n", "        ", "worker_info", "=", "torch", ".", "utils", ".", "data", ".", "get_worker_info", "(", ")", "\n", "assert", "(", "\n", "worker_info", "is", "None", "or", "worker_info", ".", "num_workers", "==", "1", "\n", ")", "# not supported since we can't get at the checkpoint for each worker", "\n", "return", "iter", "(", "self", ".", "_source", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.torch.data.IterableChunkedDataset.__init__": [[31, 54], ["super().__init__", "infinibatch.datasets.chunked_dataset_iterator"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.tests.model_selector_test.ToyModel.__init__", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.datasets.chunked_dataset_iterator"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "paths", ":", "Union", "[", "str", ",", "Iterable", "[", "str", "]", "]", ",", "\n", "shuffle", ":", "bool", "=", "True", ",", "\n", "buffer_size", ":", "int", "=", "2", "**", "20", ",", "\n", "transform", "=", "None", ",", "\n", "seed", ":", "int", "=", "None", ",", "\n", "world_size", ":", "int", "=", "1", ",", "\n", "rank", ":", "int", "=", "0", ",", "\n", "num_workers_per_rank", ":", "int", "=", "1", ",", "\n", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "rank", "=", "rank", "\n", "self", ".", "num_workers_per_rank", "=", "num_workers_per_rank", "\n", "# instance_rank is set assuming that num_workers_per_rank = 1 and adapted dynamically in __iter__", "\n", "self", ".", "dataset", "=", "chunked_dataset_iterator", "(", "\n", "paths", ",", "\n", "shuffle", "=", "shuffle", ",", "\n", "buffer_size", "=", "buffer_size", ",", "\n", "transform", "=", "transform", ",", "\n", "seed", "=", "seed", ",", "\n", "num_instances", "=", "world_size", "*", "num_workers_per_rank", ",", "\n", "instance_rank", "=", "rank", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.torch.data.IterableChunkedDataset.__iter__": [[56, 66], ["torch.utils.data.get_worker_info", "iter"], "methods", ["None"], ["", "def", "__iter__", "(", "self", ")", ":", "\n", "        ", "worker_info", "=", "torch", ".", "utils", ".", "data", ".", "get_worker_info", "(", ")", "\n", "if", "worker_info", "is", "None", ":", "# single-process data loading", "\n", "            ", "self", ".", "dataset", ".", "_instance_rank", "=", "self", ".", "rank", "\n", "", "else", ":", "\n", "            ", "assert", "worker_info", ".", "num_workers", "==", "self", ".", "num_workers_per_rank", "\n", "self", ".", "dataset", ".", "_instance_rank", "=", "(", "\n", "self", ".", "rank", "*", "self", ".", "num_workers_per_rank", "+", "worker_info", ".", "id", "\n", ")", "\n", "", "return", "iter", "(", "self", ".", "dataset", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.query_based.base_query_based_model.QueryBasedSummModel.__init__": [[14, 32], ["summertime.model.base_model.SummModel.__init__", "model_backend"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.tests.model_selector_test.ToyModel.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "trained_domain", ":", "str", "=", "None", ",", "\n", "max_input_length", ":", "int", "=", "None", ",", "\n", "max_output_length", ":", "int", "=", "None", ",", "\n", "model_backend", ":", "SummModel", "=", "TextRankModel", ",", "\n", "retrieval_ratio", ":", "float", "=", "0.5", ",", "\n", "preprocess", ":", "bool", "=", "True", ",", "\n", "**", "kwargs", ",", "\n", ")", ":", "\n", "        ", "super", "(", "QueryBasedSummModel", ",", "self", ")", ".", "__init__", "(", "\n", "trained_domain", "=", "trained_domain", ",", "\n", "max_input_length", "=", "max_input_length", ",", "\n", "max_output_length", "=", "max_output_length", ",", "\n", ")", "\n", "self", ".", "model", "=", "model_backend", "(", "**", "kwargs", ")", "\n", "self", ".", "retrieval_ratio", "=", "retrieval_ratio", "\n", "self", ".", "preprocess", "=", "preprocess", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.query_based.base_query_based_model.QueryBasedSummModel._retrieve": [[33, 35], ["NotImplementedError"], "methods", ["None"], ["", "def", "_retrieve", "(", "self", ",", "instance", ":", "List", "[", "str", "]", ",", "query", ":", "List", "[", "str", "]", ",", "n_best", ")", "->", "List", "[", "str", "]", ":", "\n", "        ", "raise", "NotImplementedError", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.query_based.base_query_based_model.QueryBasedSummModel.summarize": [[36, 69], ["base_query_based_model.QueryBasedSummModel.assert_summ_input_type", "zip", "base_query_based_model.QueryBasedSummModel.model.summarize", "isinstance", "max", "base_query_based_model.QueryBasedSummModel._retrieve", "retrieval_output.append", "nltk.sent_tokenize", "base_query_based_model.Preprocessor", "base_query_based_model.Preprocessor.preprocess", "base_query_based_model.Preprocessor.preprocess", "int", "len"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.multi_doc.base_multi_doc_model.MultiDocSummModel.assert_summ_input_type", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.tests.model_selector_test.ToyModel.summarize", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.query_based.bm25_model.BM25SummModel._retrieve", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.query_based.base_query_based_model.Preprocessor.preprocess", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.query_based.base_query_based_model.Preprocessor.preprocess"], ["", "def", "summarize", "(", "\n", "self", ",", "\n", "corpus", ":", "Union", "[", "List", "[", "str", "]", ",", "List", "[", "List", "[", "str", "]", "]", "]", ",", "\n", "queries", ":", "List", "[", "str", "]", "=", "None", ",", "\n", ")", "->", "List", "[", "str", "]", ":", "\n", "        ", "self", ".", "assert_summ_input_type", "(", "corpus", ",", "queries", ")", "\n", "\n", "retrieval_output", "=", "[", "]", "# List[str]", "\n", "for", "instance", ",", "query", "in", "zip", "(", "corpus", ",", "queries", ")", ":", "\n", "            ", "if", "isinstance", "(", "instance", ",", "str", ")", ":", "\n", "                ", "is_dialogue", "=", "False", "\n", "instance", "=", "sent_tokenize", "(", "instance", ")", "\n", "", "else", ":", "\n", "                ", "is_dialogue", "=", "True", "\n", "", "query", "=", "[", "query", "]", "\n", "\n", "# instance & query now are List[str] for sure", "\n", "if", "self", ".", "preprocess", ":", "\n", "                ", "preprocessor", "=", "Preprocessor", "(", ")", "\n", "instance", "=", "preprocessor", ".", "preprocess", "(", "instance", ")", "\n", "query", "=", "preprocessor", ".", "preprocess", "(", "query", ")", "\n", "\n", "", "n_best", "=", "max", "(", "int", "(", "len", "(", "instance", ")", "*", "self", ".", "retrieval_ratio", ")", ",", "1", ")", "\n", "top_n_sent", "=", "self", ".", "_retrieve", "(", "instance", ",", "query", ",", "n_best", ")", "\n", "\n", "if", "not", "is_dialogue", ":", "\n", "                ", "top_n_sent", "=", "\" \"", ".", "join", "(", "top_n_sent", ")", "# str", "\n", "", "retrieval_output", ".", "append", "(", "top_n_sent", ")", "\n", "\n", "", "summaries", "=", "self", ".", "model", ".", "summarize", "(", "\n", "retrieval_output", "\n", ")", "# List[str] or List[List[str]]", "\n", "return", "summaries", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.query_based.base_query_based_model.QueryBasedSummModel.generate_specific_description": [[70, 87], ["None"], "methods", ["None"], ["", "def", "generate_specific_description", "(", "self", ")", ":", "\n", "        ", "is_neural", "=", "self", ".", "model", ".", "is_neural", "&", "self", ".", "is_neural", "\n", "is_extractive", "=", "self", ".", "model", ".", "is_extractive", "|", "self", ".", "is_extractive", "\n", "model_name", "=", "\"Pipeline with retriever: {}, summarizer: {}\"", ".", "format", "(", "\n", "self", ".", "model_name", ",", "self", ".", "model", ".", "model_name", "\n", ")", "\n", "\n", "extractive_abstractive", "=", "\"extractive\"", "if", "is_extractive", "else", "\"abstractive\"", "\n", "neural", "=", "\"neural\"", "if", "is_neural", "else", "\"non-neural\"", "\n", "\n", "basic_description", "=", "(", "\n", "f\"{model_name} is a \"", "\n", "f\"{'query-based' if self.is_query_based else ''} \"", "\n", "f\"{extractive_abstractive}, {neural} model for summarization.\"", "\n", ")", "\n", "\n", "return", "basic_description", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.query_based.base_query_based_model.QueryBasedSummModel.assert_summ_input_type": [[88, 102], ["TypeError", "isinstance", "TypeError", "all", "TypeError", "isinstance"], "methods", ["None"], ["", "@", "classmethod", "\n", "def", "assert_summ_input_type", "(", "cls", ",", "corpus", ",", "query", ")", ":", "\n", "        ", "if", "query", "is", "None", ":", "\n", "            ", "raise", "TypeError", "(", "\n", "\"Query-based summarization models summarize instances of query-text pairs\uff0c however, query is missing.\"", "\n", ")", "\n", "\n", "", "if", "not", "isinstance", "(", "query", ",", "list", ")", ":", "\n", "            ", "raise", "TypeError", "(", "\n", "\"Query-based single-document summarization requires query of `List[str]`.\"", "\n", ")", "\n", "", "if", "not", "all", "(", "[", "isinstance", "(", "q", ",", "str", ")", "for", "q", "in", "query", "]", ")", ":", "\n", "            ", "raise", "TypeError", "(", "\n", "\"Query-based single-document summarization requires query of `List[str]`.\"", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.query_based.base_query_based_model.QueryBasedSummModel.generate_basic_description": [[104, 112], ["None"], "methods", ["None"], ["", "", "@", "classmethod", "\n", "def", "generate_basic_description", "(", "cls", ")", "->", "str", ":", "\n", "        ", "basic_description", "=", "(", "\n", "\"QueryBasedSummModel performs query-based summarization. Given a query-text pair,\"", "\n", "\"the model will first extract the most relevant sentences in articles or turns in \"", "\n", "\"dialogues, then use the single document summarization model to generate the summary\"", "\n", ")", "\n", "return", "basic_description", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.query_based.base_query_based_model.QueryBasedSummModel.show_capability": [[113, 124], ["cls.generate_basic_description", "print"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.multi_doc.multi_doc_joint_model.MultiDocJointModel.generate_basic_description"], ["", "@", "classmethod", "\n", "def", "show_capability", "(", "cls", ")", ":", "\n", "        ", "basic_description", "=", "cls", ".", "generate_basic_description", "(", ")", "\n", "more_details", "=", "(", "\n", "\"A query-based summarization model.\"", "\n", "\" Allows for custom model backend selection at initialization.\"", "\n", "\" Retrieve relevant turns and then summarize the retrieved turns\\n\"", "\n", "\"Strengths: \\n - Allows for control of backend model.\\n\"", "\n", "\"Weaknesses: \\n - Heavily depends on the performance of both retriever and summarizer.\\n\"", "\n", ")", "\n", "print", "(", "f\"{basic_description}\\n{'#' * 20}\\n{more_details}\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.query_based.base_query_based_model.Preprocessor.__init__": [[127, 133], ["nltk.corpus.stopwords.words", "nltk.stem.PorterStemmer"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "remove_stopwords", "=", "True", ",", "lower_case", "=", "True", ",", "stem", "=", "False", ")", ":", "\n", "        ", "self", ".", "sw", "=", "stopwords", ".", "words", "(", "\"english\"", ")", "\n", "self", ".", "stemmer", "=", "PorterStemmer", "(", ")", "\n", "self", ".", "remove_stopwords", "=", "remove_stopwords", "\n", "self", ".", "lower_case", "=", "lower_case", "\n", "self", ".", "stem", "=", "stem", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.query_based.base_query_based_model.Preprocessor.preprocess": [[134, 148], ["nltk.word_tokenize", "sent.lower", "base_query_based_model.Preprocessor.stemmer.stem"], "methods", ["None"], ["", "def", "preprocess", "(", "self", ",", "corpus", ":", "List", "[", "str", "]", ")", "->", "List", "[", "str", "]", ":", "\n", "        ", "if", "self", ".", "lower_case", ":", "\n", "            ", "corpus", "=", "[", "sent", ".", "lower", "(", ")", "for", "sent", "in", "corpus", "]", "\n", "", "tokenized_corpus", "=", "[", "word_tokenize", "(", "sent", ")", "for", "sent", "in", "corpus", "]", "\n", "if", "self", ".", "remove_stopwords", ":", "\n", "            ", "tokenized_corpus", "=", "[", "\n", "[", "word", "for", "word", "in", "sent", "if", "word", "not", "in", "self", ".", "sw", "]", "\n", "for", "sent", "in", "tokenized_corpus", "\n", "]", "\n", "", "if", "self", ".", "stem", ":", "\n", "            ", "tokenized_corpus", "=", "[", "\n", "[", "self", ".", "stemmer", ".", "stem", "(", "word", ")", "for", "word", "in", "sent", "]", "for", "sent", "in", "tokenized_corpus", "\n", "]", "\n", "", "return", "[", "\" \"", ".", "join", "(", "sent", ")", "for", "sent", "in", "tokenized_corpus", "]", "\n", "", "", ""]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.query_based.tf_idf_model.TFIDFSummModel.__init__": [[18, 38], ["base_query_based_model.QueryBasedSummModel.__init__", "sklearn.feature_extraction.text.TfidfVectorizer"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.tests.model_selector_test.ToyModel.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "trained_domain", ":", "str", "=", "None", ",", "\n", "max_input_length", ":", "int", "=", "None", ",", "\n", "max_output_length", ":", "int", "=", "None", ",", "\n", "model_backend", ":", "SummModel", "=", "TextRankModel", ",", "\n", "retrieval_ratio", ":", "float", "=", "0.5", ",", "\n", "preprocess", ":", "bool", "=", "True", ",", "\n", "**", "kwargs", "\n", ")", ":", "\n", "        ", "super", "(", "TFIDFSummModel", ",", "self", ")", ".", "__init__", "(", "\n", "trained_domain", "=", "trained_domain", ",", "\n", "max_input_length", "=", "max_input_length", ",", "\n", "max_output_length", "=", "max_output_length", ",", "\n", "model_backend", "=", "model_backend", ",", "\n", "retrieval_ratio", "=", "retrieval_ratio", ",", "\n", "preprocess", "=", "preprocess", ",", "\n", "**", "kwargs", "\n", ")", "\n", "self", ".", "vectorizer", "=", "TfidfVectorizer", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.query_based.tf_idf_model.TFIDFSummModel._retrieve": [[39, 47], ["tf_idf_model.TFIDFSummModel.vectorizer.fit_transform", "tf_idf_model.TFIDFSummModel.vectorizer.transform", "sklearn.metrics.pairwise.cosine_similarity().squeeze", "sklearn.metrics.pairwise.cosine_similarity", "sklearn.metrics.pairwise.cosine_similarity().squeeze.argsort"], "methods", ["None"], ["", "def", "_retrieve", "(", "self", ",", "instance", ":", "List", "[", "str", "]", ",", "query", ":", "List", "[", "str", "]", ",", "n_best", ")", ":", "\n", "        ", "instance_vectors", "=", "self", ".", "vectorizer", ".", "fit_transform", "(", "instance", ")", "\n", "query_vector", "=", "self", ".", "vectorizer", ".", "transform", "(", "query", ")", "\n", "\n", "similarities", "=", "cosine_similarity", "(", "query_vector", ",", "instance_vectors", ")", ".", "squeeze", "(", ")", "\n", "top_n_index", "=", "similarities", ".", "argsort", "(", ")", "[", ":", ":", "-", "1", "]", "[", "0", ":", "n_best", "]", "\n", "top_n_sent", "=", "[", "instance", "[", "ind", "]", "for", "ind", "in", "top_n_index", "]", "# List[str]", "\n", "return", "top_n_sent", "\n", "", "", ""]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.query_based.bm25_model.BM25SummModel.__init__": [[18, 36], ["base_query_based_model.QueryBasedSummModel.__init__"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.tests.model_selector_test.ToyModel.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "trained_domain", ":", "str", "=", "None", ",", "\n", "max_input_length", ":", "int", "=", "None", ",", "\n", "max_output_length", ":", "int", "=", "None", ",", "\n", "model_backend", ":", "SummModel", "=", "TextRankModel", ",", "\n", "retrieval_ratio", ":", "float", "=", "0.5", ",", "\n", "preprocess", ":", "bool", "=", "True", ",", "\n", "**", "kwargs", "\n", ")", ":", "\n", "        ", "super", "(", "BM25SummModel", ",", "self", ")", ".", "__init__", "(", "\n", "trained_domain", "=", "trained_domain", ",", "\n", "max_input_length", "=", "max_input_length", ",", "\n", "max_output_length", "=", "max_output_length", ",", "\n", "model_backend", "=", "model_backend", ",", "\n", "retrieval_ratio", "=", "retrieval_ratio", ",", "\n", "preprocess", "=", "preprocess", ",", "\n", "**", "kwargs", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.query_based.bm25_model.BM25SummModel._retrieve": [[38, 46], ["gensim.summarization.bm25.BM25", "gensim.summarization.bm25.BM25.get_scores", "sorted", "nltk.word_tokenize", "range", "sorted", "len"], "methods", ["None"], ["", "def", "_retrieve", "(", "self", ",", "instance", ":", "List", "[", "str", "]", ",", "query", ":", "List", "[", "str", "]", ",", "n_best", ")", ":", "\n", "        ", "bm25", "=", "BM25", "(", "word_tokenize", "(", "s", ")", "for", "s", "in", "instance", ")", "\n", "scores", "=", "bm25", ".", "get_scores", "(", "query", ")", "\n", "best_sent_ind", "=", "sorted", "(", "\n", "range", "(", "len", "(", "scores", ")", ")", ",", "key", "=", "lambda", "i", ":", "scores", "[", "i", "]", ",", "reverse", "=", "True", "\n", ")", "[", ":", "n_best", "]", "\n", "top_n_sent", "=", "[", "instance", "[", "ind", "]", "for", "ind", "in", "sorted", "(", "best_sent_ind", ")", "]", "\n", "return", "top_n_sent", "\n", "", "", ""]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.single_doc.textrank_model.TextRankModel.__init__": [[15, 24], ["base_single_doc_model.SingleDocSummModel.__init__", "spacy.load", "textrank_model.TextRankModel.nlp.add_pipe"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.tests.model_selector_test.ToyModel.__init__", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.BaseTrainer.BaseTrainer.load"], ["def", "__init__", "(", "self", ",", "num_sentences", "=", "1", ")", ":", "\n", "        ", "super", "(", "TextRankModel", ",", "self", ")", ".", "__init__", "(", "\n", "trained_domain", "=", "\"not_trained\"", ",", "max_input_length", "=", "None", ",", "max_output_length", "=", "None", "\n", ")", "\n", "\n", "self", ".", "num_sentences", "=", "num_sentences", "\n", "# load a spaCy model, depending on language, scale, etc.", "\n", "self", ".", "nlp", "=", "spacy", ".", "load", "(", "\"en_core_web_sm\"", ")", "\n", "self", ".", "nlp", ".", "add_pipe", "(", "\"textrank\"", ",", "last", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.single_doc.textrank_model.TextRankModel.summarize": [[25, 31], ["textrank_model.TextRankModel.assert_summ_input_type", "list", "map", "textrank_model.TextRankModel.summarize_single"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.multi_doc.base_multi_doc_model.MultiDocSummModel.assert_summ_input_type", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.single_doc.longformer_model.LongformerModel.summarize_single"], ["", "def", "summarize", "(", "\n", "self", ",", "corpus", ":", "Union", "[", "List", "[", "str", "]", ",", "List", "[", "List", "[", "str", "]", "]", "]", ",", "queries", ":", "List", "[", "str", "]", "=", "None", "\n", ")", "->", "List", "[", "str", "]", ":", "\n", "        ", "self", ".", "assert_summ_input_type", "(", "corpus", ",", "queries", ")", "\n", "\n", "return", "list", "(", "map", "(", "lambda", "x", ":", "\" \"", ".", "join", "(", "self", ".", "summarize_single", "(", "x", ")", ")", ",", "corpus", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.single_doc.textrank_model.TextRankModel.summarize_single": [[32, 82], ["textrank_model.TextRankModel.nlp", "sum", "sorted", "sorted", "unit_vector.append", "range", "math.sqrt", "sent_rank.items", "sent_rank.items", "summary_sentences.append", "set", "len", "operator.itemgetter", "operator.itemgetter", "sent_vector.add"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.modeling_encoder_decoder.BeamHypotheses.add"], ["", "def", "summarize_single", "(", "self", ",", "corpus", ")", "->", "List", "[", "str", "]", ":", "\n", "# add PyTextRank to the spaCy pipeline", "\n", "        ", "doc", "=", "self", ".", "nlp", "(", "corpus", ")", "\n", "sent_bounds", "=", "[", "[", "s", ".", "start", ",", "s", ".", "end", ",", "set", "(", "[", "]", ")", "]", "for", "s", "in", "doc", ".", "sents", "]", "\n", "\n", "limit_phrases", "=", "self", ".", "num_sentences", "\n", "phrase_id", "=", "0", "\n", "unit_vector", "=", "[", "]", "\n", "for", "p", "in", "doc", ".", "_", ".", "phrases", ":", "\n", "            ", "unit_vector", ".", "append", "(", "p", ".", "rank", ")", "\n", "for", "chunk", "in", "p", ".", "chunks", ":", "\n", "                ", "for", "sent_start", ",", "sent_end", ",", "sent_vector", "in", "sent_bounds", ":", "\n", "                    ", "if", "chunk", ".", "start", ">=", "sent_start", "and", "chunk", ".", "end", "<=", "sent_end", ":", "\n", "                        ", "sent_vector", ".", "add", "(", "phrase_id", ")", "\n", "break", "\n", "", "", "", "phrase_id", "+=", "1", "\n", "if", "phrase_id", "==", "limit_phrases", ":", "\n", "                ", "break", "\n", "\n", "", "", "sum_ranks", "=", "sum", "(", "unit_vector", ")", "\n", "\n", "unit_vector", "=", "[", "rank", "/", "sum_ranks", "for", "rank", "in", "unit_vector", "]", "\n", "\n", "sent_rank", "=", "{", "}", "\n", "sent_id", "=", "0", "\n", "for", "sent_start", ",", "sent_end", ",", "sent_vector", "in", "sent_bounds", ":", "\n", "            ", "sum_sq", "=", "0.0", "\n", "for", "phrase_id", "in", "range", "(", "len", "(", "unit_vector", ")", ")", ":", "\n", "                ", "if", "phrase_id", "not", "in", "sent_vector", ":", "\n", "                    ", "sum_sq", "+=", "unit_vector", "[", "phrase_id", "]", "**", "2.0", "\n", "", "", "sent_rank", "[", "sent_id", "]", "=", "sqrt", "(", "sum_sq", ")", "\n", "sent_id", "+=", "1", "\n", "\n", "", "sorted", "(", "sent_rank", ".", "items", "(", ")", ",", "key", "=", "itemgetter", "(", "1", ")", ")", "\n", "\n", "sent_text", "=", "{", "}", "\n", "sent_id", "=", "0", "\n", "limit_sentences", "=", "self", ".", "num_sentences", "\n", "summary_sentences", "=", "[", "]", "\n", "for", "sent", "in", "doc", ".", "sents", ":", "\n", "            ", "sent_text", "[", "sent_id", "]", "=", "sent", ".", "text", "\n", "sent_id", "+=", "1", "\n", "", "num_sent", "=", "0", "\n", "for", "sent_id", ",", "rank", "in", "sorted", "(", "sent_rank", ".", "items", "(", ")", ",", "key", "=", "itemgetter", "(", "1", ")", ")", ":", "\n", "            ", "summary_sentences", ".", "append", "(", "sent_text", "[", "sent_id", "]", ")", "\n", "num_sent", "+=", "1", "\n", "if", "num_sent", "==", "limit_sentences", ":", "\n", "                ", "break", "\n", "\n", "", "", "return", "summary_sentences", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.single_doc.textrank_model.TextRankModel.show_capability": [[83, 92], ["cls.generate_basic_description", "print"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.multi_doc.multi_doc_joint_model.MultiDocJointModel.generate_basic_description"], ["", "@", "classmethod", "\n", "def", "show_capability", "(", "cls", ")", ":", "\n", "        ", "basic_description", "=", "cls", ".", "generate_basic_description", "(", ")", "\n", "more_details", "=", "(", "\n", "\"A graphbased ranking model for text processing. Extractive sentence summarization. \\n \"", "\n", "\"Strengths: \\n - Fast with low memory usage \\n - Allows for control of summary length \\n \"", "\n", "\"Weaknesses: \\n - Not as accurate as neural methods.\"", "\n", ")", "\n", "print", "(", "f\"{basic_description} \\n {'#'*20} \\n {more_details}\"", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.single_doc.bart_model.BartModel.__init__": [[12, 21], ["base_single_doc_model.SingleDocSummModel.__init__", "transformers.BartTokenizer.from_pretrained", "transformers.BartForConditionalGeneration.from_pretrained().to", "transformers.BartForConditionalGeneration.from_pretrained"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.tests.model_selector_test.ToyModel.__init__", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.MeetingNet_Transformer.MeetingNet_Transformer.from_pretrained", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.MeetingNet_Transformer.MeetingNet_Transformer.from_pretrained"], ["def", "__init__", "(", "self", ",", "device", "=", "\"cpu\"", ")", ":", "\n", "        ", "super", "(", "BartModel", ",", "self", ")", ".", "__init__", "(", "\n", "trained_domain", "=", "\"News\"", ",", "max_input_length", "=", "1024", ",", "max_output_length", "=", "None", "\n", ")", "\n", "\n", "self", ".", "device", "=", "device", "\n", "model_name", "=", "\"facebook/bart-large-cnn\"", "\n", "self", ".", "tokenizer", "=", "BartTokenizer", ".", "from_pretrained", "(", "model_name", ")", "\n", "self", ".", "model", "=", "BartForConditionalGeneration", ".", "from_pretrained", "(", "model_name", ")", ".", "to", "(", "device", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.single_doc.bart_model.BartModel.summarize": [[22, 34], ["bart_model.BartModel.assert_summ_input_type", "bart_model.BartModel.tokenizer().to", "bart_model.BartModel.model.generate", "bart_model.BartModel.tokenizer.batch_decode", "bart_model.BartModel.tokenizer"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.multi_doc.base_multi_doc_model.MultiDocSummModel.assert_summ_input_type", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.MeetingNet_Transformer.MeetingNet_Transformer.generate", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizerFast.tokenizer"], ["", "def", "summarize", "(", "self", ",", "corpus", ",", "queries", "=", "None", ")", ":", "\n", "        ", "self", ".", "assert_summ_input_type", "(", "corpus", ",", "queries", ")", "\n", "\n", "batch", "=", "self", ".", "tokenizer", "(", "\n", "corpus", ",", "truncation", "=", "True", ",", "padding", "=", "\"longest\"", ",", "return_tensors", "=", "\"pt\"", "\n", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "encoded_summaries", "=", "self", ".", "model", ".", "generate", "(", "**", "batch", ")", "\n", "summaries", "=", "self", ".", "tokenizer", ".", "batch_decode", "(", "\n", "encoded_summaries", ",", "skip_special_tokens", "=", "True", "\n", ")", "\n", "\n", "return", "summaries", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.single_doc.bart_model.BartModel.show_capability": [[35, 39], ["print", "cls.generate_basic_description"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.multi_doc.multi_doc_joint_model.MultiDocJointModel.generate_basic_description"], ["", "@", "classmethod", "\n", "def", "show_capability", "(", "cls", ")", "->", "None", ":", "\n", "# TODO zhangir: add the show capability function for BART", "\n", "        ", "print", "(", "cls", ".", "generate_basic_description", "(", ")", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.single_doc.lexrank_model.LexRankModel.__init__": [[14, 26], ["base_single_doc_model.SingleDocSummModel.__init__", "nltk.download", "lexrank.LexRank", "nltk.sent_tokenize"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.tests.model_selector_test.ToyModel.__init__"], ["def", "__init__", "(", "self", ",", "data", ",", "summary_length", "=", "2", ",", "threshold", "=", "0.1", ")", ":", "\n", "        ", "super", "(", "LexRankModel", ",", "self", ")", ".", "__init__", "(", "\n", "trained_domain", "=", "\"not_trained\"", ",", "\n", "max_input_length", "=", "None", ",", "\n", "max_output_length", "=", "None", ",", "\n", ")", "\n", "\n", "nltk", ".", "download", "(", "\"punkt\"", ",", "quiet", "=", "True", ")", "\n", "corpus", "=", "[", "nltk", ".", "sent_tokenize", "(", "example", ")", "for", "example", "in", "data", "]", "\n", "self", ".", "lxr", "=", "LR", "(", "corpus", ",", "stopwords", "=", "STOPWORDS", "[", "\"en\"", "]", ")", "\n", "self", ".", "summary_length", "=", "summary_length", "\n", "self", ".", "threshold", "=", "threshold", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.single_doc.lexrank_model.LexRankModel.summarize": [[27, 41], ["lexrank_model.LexRankModel.assert_summ_input_type", "nltk.sent_tokenize", "lexrank_model.LexRankModel.lxr.get_summary"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.multi_doc.base_multi_doc_model.MultiDocSummModel.assert_summ_input_type"], ["", "def", "summarize", "(", "self", ",", "corpus", ",", "queries", "=", "None", ")", ":", "\n", "        ", "self", ".", "assert_summ_input_type", "(", "corpus", ",", "queries", ")", "\n", "\n", "documents", "=", "[", "nltk", ".", "sent_tokenize", "(", "document", ")", "for", "document", "in", "corpus", "]", "\n", "summaries", "=", "[", "\n", "\" \"", ".", "join", "(", "\n", "self", ".", "lxr", ".", "get_summary", "(", "\n", "document", ",", "summary_size", "=", "self", ".", "summary_length", ",", "threshold", "=", "self", ".", "threshold", "\n", ")", "\n", ")", "\n", "for", "document", "in", "documents", "\n", "]", "\n", "\n", "return", "summaries", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.single_doc.lexrank_model.LexRankModel.show_capability": [[42, 55], ["cls.generate_basic_description", "print"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.multi_doc.multi_doc_joint_model.MultiDocJointModel.generate_basic_description"], ["", "@", "classmethod", "\n", "def", "show_capability", "(", "cls", ")", ":", "\n", "        ", "basic_description", "=", "cls", ".", "generate_basic_description", "(", ")", "\n", "more_details", "=", "(", "\n", "\"Works by using a graph-based method to identify the most salient sentences in the document. \\n\"", "\n", "\"Strengths: \\n - Fast with low memory usage \\n - Allows for control of summary length \\n \"", "\n", "\"Weaknesses: \\n - Not as accurate as neural methods. \\n \"", "\n", "\"Initialization arguments: \\n \"", "\n", "\"- `corpus`: Unlabelled corpus of documents. ` \\n \"", "\n", "\"- `summary_length`: sentence length of summaries \\n \"", "\n", "\"- `threshold`: Level of salience required for sentence to be included in summary.\"", "\n", ")", "\n", "print", "(", "f\"{basic_description} \\n {'#'*20} \\n {more_details}\"", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.single_doc.t5_model.T5Model.__init__": [[12, 21], ["base_single_doc_model.SingleDocSummModel.__init__", "transformers.T5Tokenizer.from_pretrained", "transformers.T5ForConditionalGeneration.from_pretrained().to", "transformers.T5ForConditionalGeneration.from_pretrained"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.tests.model_selector_test.ToyModel.__init__", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.MeetingNet_Transformer.MeetingNet_Transformer.from_pretrained", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.MeetingNet_Transformer.MeetingNet_Transformer.from_pretrained"], ["def", "__init__", "(", "self", ",", "device", "=", "\"cpu\"", ")", ":", "\n", "        ", "super", "(", "T5Model", ",", "self", ")", ".", "__init__", "(", "\n", "trained_domain", "=", "\"Web Crawl\"", ",", "max_input_length", "=", "1024", ",", "max_output_length", "=", "None", "\n", ")", "\n", "\n", "self", ".", "device", "=", "device", "\n", "model_name", "=", "\"t5-large\"", "\n", "self", ".", "tokenizer", "=", "T5Tokenizer", ".", "from_pretrained", "(", "model_name", ")", "\n", "self", ".", "model", "=", "T5ForConditionalGeneration", ".", "from_pretrained", "(", "model_name", ")", ".", "to", "(", "device", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.single_doc.t5_model.T5Model.summarize": [[22, 34], ["t5_model.T5Model.assert_summ_input_type", "t5_model.T5Model.tokenizer().to", "t5_model.T5Model.model.generate", "t5_model.T5Model.tokenizer.batch_decode", "t5_model.T5Model.tokenizer"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.multi_doc.base_multi_doc_model.MultiDocSummModel.assert_summ_input_type", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.MeetingNet_Transformer.MeetingNet_Transformer.generate", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizerFast.tokenizer"], ["", "def", "summarize", "(", "self", ",", "corpus", ",", "queries", "=", "None", ")", ":", "\n", "        ", "self", ".", "assert_summ_input_type", "(", "corpus", ",", "queries", ")", "\n", "\n", "batch", "=", "self", ".", "tokenizer", "(", "\n", "corpus", ",", "truncation", "=", "True", ",", "padding", "=", "\"longest\"", ",", "return_tensors", "=", "\"pt\"", "\n", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "encoded_summaries", "=", "self", ".", "model", ".", "generate", "(", "**", "batch", ")", "\n", "summaries", "=", "self", ".", "tokenizer", ".", "batch_decode", "(", "\n", "encoded_summaries", ",", "skip_special_tokens", "=", "True", "\n", ")", "\n", "\n", "return", "summaries", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.single_doc.t5_model.T5Model.show_capability": [[35, 48], ["cls.generate_basic_description", "print"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.multi_doc.multi_doc_joint_model.MultiDocJointModel.generate_basic_description"], ["", "@", "classmethod", "\n", "def", "show_capability", "(", "cls", ")", "->", "None", ":", "\n", "        ", "basic_description", "=", "cls", ".", "generate_basic_description", "(", ")", "\n", "more_details", "=", "(", "\n", "\"Introduced in 2020, T5 is a large pretrained language model trained on web crawl using \"", "\n", "\"transfer learning approaches and teacher forcing.\\n \"", "\n", "\"Strengths: \\n - High accuracy \\n \"", "\n", "\"Weaknesses: \\n - High memory usage \\n \"", "\n", "\"Initialization arguments: \\n \"", "\n", "\"- `device = 'cpu'` specifies the device the model is stored on and uses for computation. \"", "\n", "\"Use `device='gpu'` to run on an Nvidia GPU.\"", "\n", ")", "\n", "print", "(", "f\"{basic_description} \\n {'#'*20} \\n {more_details}\"", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.single_doc.longformer_model.LongformerModel.__init__": [[12, 23], ["base_single_doc_model.SingleDocSummModel.__init__", "transformers.EncoderDecoderModel.from_pretrained().to", "transformers.LongformerTokenizer.from_pretrained", "transformers.EncoderDecoderModel.from_pretrained"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.tests.model_selector_test.ToyModel.__init__", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.MeetingNet_Transformer.MeetingNet_Transformer.from_pretrained", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.MeetingNet_Transformer.MeetingNet_Transformer.from_pretrained"], ["def", "__init__", "(", "self", ",", "device", "=", "\"cpu\"", ")", ":", "\n", "        ", "super", "(", "LongformerModel", ",", "self", ")", ".", "__init__", "(", "\n", "trained_domain", "=", "\"News\"", ",", "max_input_length", "=", "4096", ",", "max_output_length", "=", "None", "\n", ")", "\n", "\n", "self", ".", "device", "=", "device", "\n", "self", ".", "model", "=", "EncoderDecoderModel", ".", "from_pretrained", "(", "\n", "\"patrickvonplaten/longformer2roberta-cnn_dailymail-fp16\"", "\n", ")", ".", "to", "(", "device", ")", "\n", "self", ".", "tokenizer", "=", "LongformerTokenizer", ".", "from_pretrained", "(", "\n", "\"allenai/longformer-base-4096\"", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.single_doc.longformer_model.LongformerModel.summarize": [[25, 31], ["longformer_model.LongformerModel.assert_summ_input_type", "list", "map", "longformer_model.LongformerModel.summarize_single"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.multi_doc.base_multi_doc_model.MultiDocSummModel.assert_summ_input_type", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.single_doc.longformer_model.LongformerModel.summarize_single"], ["", "def", "summarize", "(", "self", ",", "corpus", ",", "queries", "=", "None", ")", ":", "\n", "        ", "self", ".", "assert_summ_input_type", "(", "corpus", ",", "queries", ")", "\n", "\n", "summaries", "=", "list", "(", "map", "(", "lambda", "doc", ":", "self", ".", "summarize_single", "(", "doc", ")", ",", "corpus", ")", ")", "\n", "\n", "return", "summaries", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.single_doc.longformer_model.LongformerModel.summarize_single": [[32, 49], ["longformer_model.LongformerModel.tokenizer().to", "print", "longformer_model.LongformerModel.model.generate", "longformer_model.LongformerModel.tokenizer.decode", "longformer_model.LongformerModel.tokenizer"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.MeetingNet_Transformer.MeetingNet_Transformer.generate", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizerFast.decode", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizerFast.tokenizer"], ["", "def", "summarize_single", "(", "self", ",", "document", ")", ":", "\n", "# Tokenizes document and returns PyTorch torch.Tensor object with length attribute", "\n", "        ", "tokenized_sequence", "=", "self", ".", "tokenizer", "(", "\n", "document", ",", "\n", "return_tensors", "=", "\"pt\"", ",", "\n", "return_length", "=", "True", ",", "\n", "truncation", "=", "True", ",", "\n", "max_length", "=", "4096", ",", "\n", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "print", "(", "\n", "f\"Longformer model: processing document of {tokenized_sequence.length} tokens\"", "\n", ")", "\n", "input_ids", "=", "tokenized_sequence", ".", "input_ids", "\n", "# output_ids is tensor with one layer: output_ids[0] extracts tensor layer for decoding", "\n", "output_ids", "=", "self", ".", "model", ".", "generate", "(", "input_ids", ")", "\n", "\n", "return", "self", ".", "tokenizer", ".", "decode", "(", "output_ids", "[", "0", "]", ",", "skip_special_tokens", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.single_doc.longformer_model.LongformerModel.show_capability": [[50, 61], ["cls.generate_basic_description", "print"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.multi_doc.multi_doc_joint_model.MultiDocJointModel.generate_basic_description"], ["", "@", "classmethod", "\n", "def", "show_capability", "(", "cls", ")", "->", "None", ":", "\n", "        ", "basic_description", "=", "cls", ".", "generate_basic_description", "(", ")", "\n", "more_details", "=", "(", "\n", "\"A Longformer2Roberta model finetuned on CNN-DM dataset for summarization.\\n\\n\"", "\n", "\"Strengths:\\n - Correctly handles longer (> 2000 tokens) corpus.\\n\\n\"", "\n", "\"Weaknesses:\\n - Less accurate on contexts outside training domain.\\n\\n\"", "\n", "\"Initialization arguments:\\n \"", "\n", "' - device: use `device=\"cuda\"` to load onto an NVIDIA GPU.\\n'", "\n", ")", "\n", "print", "(", "f\"{basic_description} \\n {'#'*20} \\n {more_details}\"", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.single_doc.base_single_doc_model.SingleDocSummModel.__init__": [[5, 15], ["summertime.model.base_model.SummModel.__init__"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.tests.model_selector_test.ToyModel.__init__"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "trained_domain", ":", "str", "=", "None", ",", "\n", "max_input_length", ":", "int", "=", "None", ",", "\n", "max_output_length", ":", "int", "=", "None", ",", "\n", ")", ":", "\n", "        ", "super", "(", "SingleDocSummModel", ",", "self", ")", ".", "__init__", "(", "\n", "trained_domain", "=", "trained_domain", ",", "\n", "max_input_length", "=", "max_input_length", ",", "\n", "max_output_length", "=", "max_output_length", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.single_doc.base_single_doc_model.SingleDocSummModel.assert_summ_input_type": [[17, 54], ["isinstance", "TypeError", "all", "TypeError", "all", "isinstance", "TypeError", "all", "TypeError", "isinstance", "print", "isinstance", "isinstance", "ins.encode", "isinstance", "ins.encode"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizer.encode", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizer.encode"], ["", "@", "classmethod", "\n", "def", "assert_summ_input_type", "(", "cls", ",", "corpus", ",", "query", ")", ":", "\n", "        ", "if", "not", "isinstance", "(", "corpus", ",", "list", ")", ":", "\n", "            ", "raise", "TypeError", "(", "\n", "\"Single-document summarization requires corpus of `List[str]`.\"", "\n", ")", "\n", "", "if", "not", "all", "(", "[", "isinstance", "(", "ins", ",", "str", ")", "for", "ins", "in", "corpus", "]", ")", ":", "\n", "            ", "raise", "TypeError", "(", "\n", "\"Single-document summarization requires corpus of `List[str]`.\"", "\n", ")", "\n", "\n", "", "if", "query", "is", "not", "None", ":", "\n", "            ", "if", "not", "isinstance", "(", "query", ",", "list", ")", ":", "\n", "                ", "raise", "TypeError", "(", "\n", "\"Query-based single-document summarization requires query of `List[str]`.\"", "\n", ")", "\n", "", "if", "not", "all", "(", "[", "isinstance", "(", "q", ",", "str", ")", "for", "q", "in", "query", "]", ")", ":", "\n", "                ", "raise", "TypeError", "(", "\n", "\"Query-based single-document summarization requires query of `List[str]`.\"", "\n", ")", "\n", "\n", "", "", "warning", "=", "\"Warning: non-ASCII input corpus detected!\\n\\\nIf this is not English, consider using \\\none of our multilingual models such as summertime.model.multilingual.MBartModel .\"", "\n", "\n", "# python 3.6 does not have string.ascii() functionality, so we use this instead", "\n", "try", ":", "\n", "            ", "if", "all", "(", "[", "isinstance", "(", "ins", ",", "list", ")", "for", "ins", "in", "corpus", "]", ")", ":", "\n", "                ", "[", "ins", ".", "encode", "(", "\"ascii\"", ")", "for", "batch", "in", "corpus", "for", "ins", "in", "batch", "]", "\n", "\n", "", "elif", "isinstance", "(", "corpus", ",", "list", ")", ":", "\n", "                ", "[", "ins", ".", "encode", "(", "\"ascii\"", ")", "for", "ins", "in", "corpus", "]", "\n", "\n", "", "", "except", "UnicodeEncodeError", ":", "\n", "            ", "print", "(", "warning", ")", "\n", "\n", "", "return", "\"en\"", "# ISO-639-1 code for English", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.single_doc.pegasus_model.PegasusModel.__init__": [[11, 25], ["base_single_doc_model.SingleDocSummModel.__init__", "print", "transformers.PegasusTokenizer.from_pretrained", "print", "transformers.PegasusForConditionalGeneration.from_pretrained().to", "transformers.PegasusForConditionalGeneration.from_pretrained"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.tests.model_selector_test.ToyModel.__init__", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.MeetingNet_Transformer.MeetingNet_Transformer.from_pretrained", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.MeetingNet_Transformer.MeetingNet_Transformer.from_pretrained"], ["def", "__init__", "(", "self", ",", "device", "=", "\"cpu\"", ")", ":", "\n", "        ", "super", "(", "PegasusModel", ",", "self", ")", ".", "__init__", "(", "\n", "trained_domain", "=", "\"News\"", ",", "max_input_length", "=", "1024", ",", "max_output_length", "=", "None", "\n", ")", "\n", "\n", "self", ".", "device", "=", "device", "\n", "\n", "model_name", "=", "\"google/pegasus-xsum\"", "\n", "print", "(", "\"init load pretrained tokenizer\"", ")", "\n", "self", ".", "tokenizer", "=", "PegasusTokenizer", ".", "from_pretrained", "(", "model_name", ")", "\n", "print", "(", "\"init load pretrained model with tokenizer on \"", "+", "device", ")", "\n", "# self.model = PegasusForConditionalGeneration.from_pretrained(model_name).to(device)", "\n", "self", ".", "model", "=", "PegasusForConditionalGeneration", ".", "from_pretrained", "(", "model_name", ")", ".", "to", "(", "\n", "device", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.single_doc.pegasus_model.PegasusModel.summarize": [[27, 43], ["pegasus_model.PegasusModel.assert_summ_input_type", "print", "pegasus_model.PegasusModel.tokenizer().to", "print", "pegasus_model.PegasusModel.model.generate", "print", "pegasus_model.PegasusModel.tokenizer.decode", "pegasus_model.PegasusModel.tokenizer"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.multi_doc.base_multi_doc_model.MultiDocSummModel.assert_summ_input_type", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.MeetingNet_Transformer.MeetingNet_Transformer.generate", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizerFast.decode", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizerFast.tokenizer"], ["", "def", "summarize", "(", "self", ",", "corpus", ",", "queries", "=", "None", ")", ":", "\n", "        ", "self", ".", "assert_summ_input_type", "(", "corpus", ",", "queries", ")", "\n", "\n", "print", "(", "\"batching\"", ")", "\n", "# batch = self.tokenizer(corpus, truncation=True, padding='longest', return_tensors=\"pt\").to(self.device)", "\n", "batch", "=", "self", ".", "tokenizer", "(", "corpus", ",", "truncation", "=", "True", ",", "return_tensors", "=", "\"pt\"", ")", ".", "to", "(", "\n", "self", ".", "device", "\n", ")", "\n", "print", "(", "\"encoding batches\"", ")", "\n", "# encoded_summaries = self.model.generate(**batch, max_length=40, max_time=120)", "\n", "encoded_summaries", "=", "self", ".", "model", ".", "generate", "(", "batch", "[", "\"input_ids\"", "]", ",", "max_time", "=", "1024", ")", "\n", "print", "(", "\"decoding batches\"", ")", "\n", "# summaries = self.tokenizer.batch_decode(encoded_summaries, skip_special_tokens=True)", "\n", "summaries", "=", "[", "self", ".", "tokenizer", ".", "decode", "(", "encoded_summaries", "[", "0", "]", ")", "]", "\n", "\n", "return", "summaries", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.single_doc.pegasus_model.PegasusModel.show_capability": [[44, 58], ["cls.generate_basic_description", "print"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.multi_doc.multi_doc_joint_model.MultiDocJointModel.generate_basic_description"], ["", "@", "classmethod", "\n", "def", "show_capability", "(", "cls", ")", ":", "\n", "        ", "basic_description", "=", "cls", ".", "generate_basic_description", "(", ")", "\n", "more_details", "=", "(", "\n", "\"Introduced in 2019, a large neural abstractive summarization model trained on web crawl and \"", "\n", "\"news data.\\n \"", "\n", "\"Strengths: \\n - High accuracy \\n - Performs well on almost all kinds of non-literary written \"", "\n", "\"text \\n \"", "\n", "\"Weaknesses: \\n - High memory usage \\n \"", "\n", "\"Initialization arguments: \\n \"", "\n", "\"- `device = 'cpu'` specifies the device the model is stored on and uses for computation. \"", "\n", "\"Use `device='cuda'` to run on an Nvidia GPU.\"", "\n", ")", "\n", "print", "(", "f\"{basic_description} \\n {'#'*20} \\n {more_details}\"", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.multilingual.mt5_model.MT5Model.__init__": [[128, 141], ["base_multilingual_model.MultilingualSummModel.__init__", "transformers.MT5Tokenizer.from_pretrained", "transformers.MT5ForConditionalGeneration.from_pretrained().to", "transformers.MT5ForConditionalGeneration.from_pretrained"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.tests.model_selector_test.ToyModel.__init__", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.MeetingNet_Transformer.MeetingNet_Transformer.from_pretrained", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.MeetingNet_Transformer.MeetingNet_Transformer.from_pretrained"], ["def", "__init__", "(", "self", ",", "device", "=", "\"cpu\"", ")", ":", "\n", "\n", "        ", "super", "(", "MT5Model", ",", "self", ")", ".", "__init__", "(", "\n", "trained_domain", "=", "\"News\"", ",", "\n", "max_input_length", "=", "512", ",", "\n", "max_output_length", "=", "None", ",", "\n", ")", "\n", "\n", "self", ".", "device", "=", "device", "\n", "\n", "model_name", "=", "\"csebuetnlp/mT5_multilingual_XLSum\"", "\n", "self", ".", "tokenizer", "=", "MT5Tokenizer", ".", "from_pretrained", "(", "model_name", ")", "\n", "self", ".", "model", "=", "MT5ForConditionalGeneration", ".", "from_pretrained", "(", "model_name", ")", ".", "to", "(", "device", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.multilingual.mt5_model.MT5Model.summarize": [[142, 163], ["mt5_model.MT5Model.assert_summ_input_type", "mt5_model.MT5Model.model.generate", "mt5_model.MT5Model.tokenizer.batch_decode", "mt5_model.MT5Model.tokenizer.as_target_tokenizer", "mt5_model.MT5Model.tokenizer().to", "mt5_model.MT5Model.tokenizer"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.multi_doc.base_multi_doc_model.MultiDocSummModel.assert_summ_input_type", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.MeetingNet_Transformer.MeetingNet_Transformer.generate", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizerFast.tokenizer"], ["", "def", "summarize", "(", "self", ",", "corpus", ",", "queries", "=", "None", ")", ":", "\n", "        ", "self", ".", "assert_summ_input_type", "(", "corpus", ",", "queries", ")", "\n", "\n", "with", "self", ".", "tokenizer", ".", "as_target_tokenizer", "(", ")", ":", "\n", "            ", "batch", "=", "self", ".", "tokenizer", "(", "\n", "corpus", ",", "\n", "truncation", "=", "True", ",", "\n", "padding", "=", "\"longest\"", ",", "\n", "max_length", "=", "self", ".", "max_input_length", ",", "\n", "return_tensors", "=", "\"pt\"", ",", "\n", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "\n", "", "encoded_summaries", "=", "self", ".", "model", ".", "generate", "(", "\n", "**", "batch", ",", "num_beams", "=", "4", ",", "length_penalty", "=", "1.0", ",", "early_stopping", "=", "True", "\n", ")", "\n", "\n", "summaries", "=", "self", ".", "tokenizer", ".", "batch_decode", "(", "\n", "encoded_summaries", ",", "skip_special_tokens", "=", "True", "\n", ")", "\n", "\n", "return", "summaries", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.multilingual.mt5_model.MT5Model.show_capability": [[164, 177], ["cls.generate_basic_description", "print"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.multi_doc.multi_doc_joint_model.MultiDocJointModel.generate_basic_description"], ["", "@", "classmethod", "\n", "def", "show_capability", "(", "cls", ")", "->", "None", ":", "\n", "        ", "basic_description", "=", "cls", ".", "generate_basic_description", "(", ")", "\n", "more_details", "=", "(", "\n", "\"Introduced in ____, a massively multilingual variant of Google's T5, a large neural model. \"", "\n", "\"Trained on web crawled data and fine-tuned on XLSum, a 45-language multilingual news dataset.\\n\"", "\n", "\"Strengths: \\n - Massively multilingual: supports 101 different languages\\n\"", "\n", "\"Weaknesses: \\n - High memory usage\\n - Lower max input length (512)\"", "\n", "\"Initialization arguments: \\n \"", "\n", "\"- `device = 'cpu'` specifies the device the model is stored on and uses for computation. \"", "\n", "\"Use `device='cuda'` to run on an Nvidia GPU.\"", "\n", ")", "\n", "print", "(", "f\"{basic_description} \\n {'#'*20} \\n {more_details}\"", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.multilingual.translation_pipeline_model.TranslationPipelineModel.__init__": [[97, 110], ["model_backend", "base_multilingual_model.MultilingualSummModel.__init__", "easynmt.EasyNMT"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.tests.model_selector_test.ToyModel.__init__"], ["def", "__init__", "(", "self", ",", "model_backend", ":", "SummModel", "=", "BartModel", ",", "**", "kwargs", ")", ":", "\n", "\n", "        ", "model", ":", "SummModel", "=", "model_backend", "(", "**", "kwargs", ")", "\n", "self", ".", "model", "=", "model", "\n", "\n", "super", "(", "TranslationPipelineModel", ",", "self", ")", ".", "__init__", "(", "\n", "trained_domain", "=", "self", ".", "model", ".", "trained_domain", ",", "\n", "max_input_length", "=", "self", ".", "model", ".", "max_input_length", ",", "\n", "max_output_length", "=", "self", ".", "model", ".", "max_output_length", ",", "\n", ")", "\n", "\n", "# translation module", "\n", "self", ".", "translator", "=", "EasyNMT", "(", "\"opus-mt\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.multilingual.translation_pipeline_model.TranslationPipelineModel.summarize": [[111, 131], ["translation_pipeline_model.TranslationPipelineModel.assert_summ_input_type", "base_multilingual_model.fasttext_predict", "translation_pipeline_model.TranslationPipelineModel.translator.translate", "translation_pipeline_model.TranslationPipelineModel.model.summarize", "translation_pipeline_model.TranslationPipelineModel.translator.translate", "translation_pipeline_model.TranslationPipelineModel.translator.translate"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.multi_doc.base_multi_doc_model.MultiDocSummModel.assert_summ_input_type", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.multilingual.base_multilingual_model.fasttext_predict", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.tests.model_selector_test.ToyModel.summarize"], ["", "def", "summarize", "(", "self", ",", "corpus", ",", "queries", "=", "None", ")", ":", "\n", "        ", "self", ".", "assert_summ_input_type", "(", "corpus", ",", "queries", ")", "\n", "\n", "src_lang", "=", "fasttext_predict", "(", "corpus", ")", "\n", "# translate to English", "\n", "corpus", "=", "self", ".", "translator", ".", "translate", "(", "\n", "corpus", ",", "source_lang", "=", "src_lang", ",", "target_lang", "=", "\"en\"", ",", "beam_size", "=", "4", "\n", ")", "\n", "# TODO: translate each doc separately if provided multiple docs in corpus?", "\n", "if", "queries", ":", "\n", "            ", "queries", "=", "self", ".", "translator", ".", "translate", "(", "queries", ",", "target_lang", "=", "\"en\"", ",", "beam_size", "=", "4", ")", "\n", "\n", "# summarize in English", "\n", "", "english_summaries", "=", "self", ".", "model", ".", "summarize", "(", "corpus", ",", "queries", ")", "\n", "\n", "summaries", "=", "self", ".", "translator", ".", "translate", "(", "\n", "english_summaries", ",", "source_lang", "=", "\"en\"", ",", "target_lang", "=", "src_lang", ",", "beam_size", "=", "4", "\n", ")", "\n", "\n", "return", "summaries", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.multilingual.translation_pipeline_model.TranslationPipelineModel.show_capability": [[132, 149], ["cls.generate_basic_description", "print"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.multi_doc.multi_doc_joint_model.MultiDocJointModel.generate_basic_description"], ["", "@", "classmethod", "\n", "def", "show_capability", "(", "cls", ")", "->", "None", ":", "\n", "        ", "basic_description", "=", "cls", ".", "generate_basic_description", "(", ")", "\n", "more_details", "=", "(", "\n", "\"A simple pipeline model for multilingual translation. \"", "\n", "\"Uses machine translation to translate input into English, \"", "\n", "\"then performs summarization in English before translating results \"", "\n", "\"back to the original language.\\n\"", "\n", "\"Strengths: \\n - Massively multilingual: supports ~150 languages\\n\"", "\n", "\"Weaknesses: \\n - Information loss from translation to and from English\"", "\n", "\"Initialization arguments: \\n \"", "\n", "\" - model_backend: the monolingual model to use for summarization. Defaults to BART\"", "\n", "# TODO: if change to Pegasus, change this to reflect that!!", "\n", "\"- `device = 'cpu'` specifies the device the model is stored on and uses for computation. \"", "\n", "\"Use `device='cuda'` to run on an Nvidia GPU.\"", "\n", ")", "\n", "print", "(", "f\"{basic_description} \\n {'#'*20} \\n {more_details}\"", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.multilingual.mbart_model.MBartModel.__init__": [[65, 81], ["base_multilingual_model.MultilingualSummModel.__init__", "transformers.MBart50Tokenizer.from_pretrained", "transformers.MBartForConditionalGeneration.from_pretrained().to", "transformers.MBartForConditionalGeneration.from_pretrained"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.tests.model_selector_test.ToyModel.__init__", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.MeetingNet_Transformer.MeetingNet_Transformer.from_pretrained", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.MeetingNet_Transformer.MeetingNet_Transformer.from_pretrained"], ["def", "__init__", "(", "self", ",", "device", "=", "\"cpu\"", ")", ":", "\n", "        ", "super", "(", "MBartModel", ",", "self", ")", ".", "__init__", "(", "\n", "# TODO: trained domain not news (at least not exclusively)", "\n", "trained_domain", "=", "\"News\"", ",", "\n", "max_input_length", "=", "1024", ",", "\n", "max_output_length", "=", "None", ",", "\n", ")", "\n", "\n", "self", ".", "device", "=", "device", "\n", "\n", "model_name", "=", "\"facebook/mbart-large-50\"", "\n", "self", ".", "tokenizer", "=", "MBart50Tokenizer", ".", "from_pretrained", "(", "\n", "model_name", ",", "src_lang", "=", "\"en_XX\"", ",", "tgt_lang", "=", "\"en_XX\"", "\n", ")", "\n", "self", ".", "model", "=", "MBartForConditionalGeneration", ".", "from_pretrained", "(", "model_name", ")", ".", "to", "(", "\n", "device", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.multilingual.mbart_model.MBartModel.summarize": [[83, 107], ["mbart_model.MBartModel.assert_summ_input_type", "mbart_model.MBartModel.model.generate", "mbart_model.MBartModel.tokenizer.batch_decode", "mbart_model.MBartModel.tokenizer.as_target_tokenizer", "mbart_model.MBartModel.tokenizer().to", "mbart_model.MBartModel.tokenizer"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.multi_doc.base_multi_doc_model.MultiDocSummModel.assert_summ_input_type", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Networks.MeetingNet_Transformer.MeetingNet_Transformer.generate", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizerFast.tokenizer"], ["", "def", "summarize", "(", "self", ",", "corpus", ",", "queries", "=", "None", ")", ":", "\n", "        ", "lang_code", "=", "self", ".", "assert_summ_input_type", "(", "corpus", ",", "queries", ")", "\n", "\n", "self", ".", "tokenizer", ".", "src_lang", "=", "lang_code", "\n", "self", ".", "tokenizer", ".", "tgt_lang", "=", "lang_code", "\n", "\n", "with", "self", ".", "tokenizer", ".", "as_target_tokenizer", "(", ")", ":", "\n", "            ", "batch", "=", "self", ".", "tokenizer", "(", "\n", "corpus", ",", "truncation", "=", "True", ",", "padding", "=", "\"longest\"", ",", "return_tensors", "=", "\"pt\"", "\n", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "\n", "", "encoded_summaries", "=", "self", ".", "model", ".", "generate", "(", "\n", "**", "batch", ",", "\n", "decoder_start_token_id", "=", "self", ".", "tokenizer", ".", "lang_code_to_id", "[", "lang_code", "]", ",", "\n", "length_penalty", "=", "1.0", ",", "\n", "num_beams", "=", "4", ",", "\n", "early_stopping", "=", "True", ",", "\n", ")", "\n", "\n", "summaries", "=", "self", ".", "tokenizer", ".", "batch_decode", "(", "\n", "encoded_summaries", ",", "skip_special_tokens", "=", "True", "\n", ")", "\n", "\n", "return", "summaries", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.multilingual.mbart_model.MBartModel.show_capability": [[108, 122], ["cls.generate_basic_description", "print"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.multi_doc.multi_doc_joint_model.MultiDocJointModel.generate_basic_description"], ["", "@", "classmethod", "\n", "def", "show_capability", "(", "cls", ")", "->", "None", ":", "\n", "        ", "basic_description", "=", "cls", ".", "generate_basic_description", "(", ")", "\n", "more_details", "=", "(", "\n", "\"Introduced in 2020, a multilingual variant of BART (a large neural model) \"", "\n", "\"trained on web crawl data.\\n\"", "\n", "\"Strengths: \\n - Multilinguality: supports 50 different languages\\n\"", "\n", "\" - Higher max input length than mT5 (1024)\"", "\n", "\"Weaknesses: \\n - High memory usage\"", "\n", "\"Initialization arguments: \\n \"", "\n", "\"- `device = 'cpu'` specifies the device the model is stored on and uses for computation. \"", "\n", "\"Use `device='cuda'` to run on an Nvidia GPU.\"", "\n", ")", "\n", "print", "(", "f\"{basic_description} \\n {'#'*20} \\n {more_details}\"", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.multilingual.base_multilingual_model.MultilingualSummModel.__init__": [[48, 58], ["summertime.model.single_doc.base_single_doc_model.SingleDocSummModel.__init__"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.tests.model_selector_test.ToyModel.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "trained_domain", ":", "str", "=", "None", ",", "\n", "max_input_length", ":", "int", "=", "None", ",", "\n", "max_output_length", ":", "int", "=", "None", ",", "\n", ")", ":", "\n", "        ", "super", "(", "MultilingualSummModel", ",", "self", ")", ".", "__init__", "(", "\n", "trained_domain", "=", "trained_domain", ",", "\n", "max_input_length", "=", "max_input_length", ",", "\n", "max_output_length", "=", "max_output_length", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.multilingual.base_multilingual_model.MultilingualSummModel.assert_summ_input_type": [[60, 76], ["super().assert_summ_input_type", "base_multilingual_model.fasttext_predict", "print", "ValueError"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.multi_doc.base_multi_doc_model.MultiDocSummModel.assert_summ_input_type", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.multilingual.base_multilingual_model.fasttext_predict"], ["", "@", "classmethod", "\n", "def", "assert_summ_input_type", "(", "cls", ",", "corpus", ",", "query", ")", ":", "\n", "\n", "        ", "super", "(", ")", ".", "assert_summ_input_type", "(", "corpus", ",", "query", ")", "\n", "\n", "label", "=", "fasttext_predict", "(", "corpus", ")", "\n", "\n", "# check if language code is in the supported language dictionary", "\n", "if", "label", "in", "cls", ".", "lang_tag_dict", ":", "\n", "            ", "print", "(", "f\"Supported language '{label}' detected.\"", ")", "\n", "return", "cls", ".", "lang_tag_dict", "[", "label", "]", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "f\"Unsupported language '{label}' detected! \\\nTry checking if another of our multilingual models \\\nsupports this language.\"", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.multilingual.base_multilingual_model.fasttext_predict": [[9, 39], ["summertime.util.download_utils.get_cached_file_path", "fasttext.load_model", "all", "label.replace.replace", "str", "fasttext.load_model.predict", "isinstance", "isinstance", "fasttext.load_model.predict"], "function", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.util.download_utils.get_cached_file_path", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.HMNetTrainer.HMNetTrainer.load_model"], ["def", "fasttext_predict", "(", "corpus", ":", "Union", "[", "List", "[", "str", "]", ",", "List", "[", "List", "[", "str", "]", "]", "]", ")", ":", "\n", "    ", "\"\"\"\n    Utility function to predict the language of input text\n    using fasttext classifier.\n    \"\"\"", "\n", "url", "=", "\"https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.ftz\"", "\n", "\n", "filepath", "=", "get_cached_file_path", "(", "\"fasttext\"", ",", "\"lid.176.ftz\"", ",", "url", ")", "\n", "\n", "fasttext", ".", "FastText", ".", "eprint", "=", "lambda", "x", ":", "None", "\n", "classifier", "=", "fasttext", ".", "load_model", "(", "str", "(", "filepath", ")", ")", "\n", "\n", "# fasttext returns a tuple of 2 lists:", "\n", "# the first list contains a list of predicted language labels", "\n", "# of the form {__label__<lang_code>}", "\n", "# and the second list contains the corresponding probabilities", "\n", "prediction", ":", "Tuple", "[", "List", "[", "List", "[", "str", "]", "]", ",", "List", "]", "=", "None", "\n", "if", "all", "(", "[", "isinstance", "(", "ins", ",", "list", ")", "for", "ins", "in", "corpus", "]", ")", ":", "\n", "        ", "prediction", "=", "classifier", ".", "predict", "(", "corpus", "[", "0", "]", ")", "\n", "\n", "", "elif", "isinstance", "(", "corpus", ",", "list", ")", ":", "\n", "        ", "prediction", "=", "classifier", ".", "predict", "(", "corpus", ")", "\n", "\n", "# access the first (most likely) predicted language label", "\n", "", "label", "=", "prediction", "[", "0", "]", "[", "0", "]", "[", "0", "]", "\n", "\n", "# remove prefix from label string to get language code", "\n", "label", "=", "label", ".", "replace", "(", "\"__label__\"", ",", "\"\"", ")", "\n", "\n", "return", "label", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.dialogue.flatten_dialogue_model.FlattenDialogueModel.__init__": [[12, 19], ["model_backend", "summertime.model.dialogue.base_dialogue_model.DialogueSummModel.__init__"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.tests.model_selector_test.ToyModel.__init__"], ["def", "__init__", "(", "self", ",", "model_backend", ":", "SummModel", "=", "BartModel", ",", "**", "kwargs", ")", ":", "\n", "        ", "self", ".", "model", ":", "SummModel", "=", "model_backend", "(", "**", "kwargs", ")", "\n", "\n", "super", "(", "DialogueSummModel", ",", "self", ")", ".", "__init__", "(", "\n", "trained_domain", "=", "self", ".", "model", ".", "trained_domain", ",", "\n", "max_input_length", "=", "self", ".", "model", ".", "max_input_length", ",", "\n", "max_output_length", "=", "self", ".", "model", ".", "max_output_length", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.dialogue.flatten_dialogue_model.FlattenDialogueModel.summarize": [[21, 34], ["flatten_dialogue_model.FlattenDialogueModel.assert_summ_input_type", "flatten_dialogue_model.FlattenDialogueModel.model.summarize", "joint_corpus.append"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.multi_doc.base_multi_doc_model.MultiDocSummModel.assert_summ_input_type", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.tests.model_selector_test.ToyModel.summarize"], ["", "def", "summarize", "(", "\n", "self", ",", "\n", "corpus", ":", "Union", "[", "List", "[", "str", "]", ",", "List", "[", "List", "[", "str", "]", "]", "]", ",", "\n", "query", ":", "Union", "[", "List", "[", "str", "]", ",", "List", "[", "List", "[", "str", "]", "]", "]", "=", "None", ",", "\n", ")", "->", "List", "[", "str", "]", ":", "\n", "        ", "self", ".", "assert_summ_input_type", "(", "corpus", ",", "None", ")", "\n", "joint_corpus", "=", "[", "]", "\n", "for", "instance", "in", "corpus", ":", "\n", "            ", "joint_corpus", ".", "append", "(", "\" \"", ".", "join", "(", "instance", ")", ")", "\n", "\n", "", "summaries", "=", "self", ".", "model", ".", "summarize", "(", "joint_corpus", ")", "\n", "\n", "return", "summaries", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.dialogue.flatten_dialogue_model.FlattenDialogueModel.generate_basic_description": [[35, 44], ["None"], "methods", ["None"], ["", "@", "classmethod", "\n", "def", "generate_basic_description", "(", "cls", ")", "->", "str", ":", "\n", "        ", "basic_description", "=", "(", "\n", "\"FlattenDialogueModel performs multi-document summarization by\"", "\n", "\" first concatenating all dialogue utterances,\"", "\n", "\" and then treat the concatenated text as a single document and use\"", "\n", "\" single document models to solve it.\"", "\n", ")", "\n", "return", "basic_description", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.dialogue.flatten_dialogue_model.FlattenDialogueModel.show_capability": [[45, 56], ["cls.generate_basic_description", "print"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.multi_doc.multi_doc_joint_model.MultiDocJointModel.generate_basic_description"], ["", "@", "classmethod", "\n", "def", "show_capability", "(", "cls", ")", ":", "\n", "        ", "basic_description", "=", "cls", ".", "generate_basic_description", "(", ")", "\n", "more_details", "=", "(", "\n", "\"A dialogue summarization model.\"", "\n", "\" Allows for custom model backend selection at initialization.\"", "\n", "\" Concatenates the utterances in the dialogue and returns single-document summarization of joint corpus.\\n\"", "\n", "\"Strengths: \\n - Allows for control of backend model.\\n\"", "\n", "\"Weaknesses: \\n - Disregards the dialogue structure.\\n\"", "\n", ")", "\n", "print", "(", "f\"{basic_description}\\n{'#' * 20}\\n{more_details}\"", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.dialogue.hmnet_model.HMNetModel.__init__": [[169, 215], ["summertime.model.dialogue.base_dialogue_model.DialogueSummModel.__init__", "hmnet_model.HMNetModel._parse_args", "summertime.model.third_party.HMNet.Models.Trainers.HMNetTrainer.HMNetTrainer", "pathlib.Path().resolve", "summertime.util.download_utils.get_cached_file_path", "pathlib.Path"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.tests.model_selector_test.ToyModel.__init__", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.dialogue.hmnet_model.HMNetModel._parse_args", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.util.download_utils.get_cached_file_path"], ["def", "__init__", "(", "\n", "self", ",", "\n", "min_gen_length", ":", "int", "=", "10", ",", "\n", "max_gen_length", ":", "int", "=", "300", ",", "\n", "beam_width", ":", "int", "=", "6", ",", "\n", "**", "kwargs", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Create a summarization model with HMNet backbone. In the default setting, the inference speed will be\n        10s/sample (on one GPU), however, if one can tune these three parameters properly, e.g. min_gen_length=10,\n        max_gen_length=100, and beam_width=2, the inference speed will increase to 2s/sample (on one GPU).\n\n        Args:\n          min_gen_length (int): minimum generation length of the decoder\n          max_gen_length (int): maximum generation length of the decoder\n          beam_width (int): width of the beam when doing beam search in the decoding process\n          kwargs: the other valid parameters. The valid parameters can be found in\n              model/dialogue/hmnet/config/dialogue.conf . You can use either lower case or upper case for parameter\n              name. The valid parameter name is one of the following args, however, we do not encourage you to modify\n               them, since some unexpected, untested errors might be triggered:\n              ['MODEL', 'TASK', 'CRITERION', 'SEED', 'MAX_NUM_EPOCHS', 'EVAL_PER_UPDATE_NUM'\n              , 'UPDATES_PER_EPOCH', 'OPTIMIZER', 'START_LEARNING_RATE', 'LR_SCHEDULER', 'WARMUP_STEPS',\n              'WARMUP_INIT_LR', 'WARMUP_END_LR', 'GRADIENT_ACCUMULATE_STEP', 'GRAD_CLIPPING', 'USE_REL_DATA_PATH',\n              'TRAIN_FILE', 'DEV_FILE', 'TEST_FILE', 'ROLE_DICT_FILE', 'MINI_BATCH', 'MAX_PADDING_RATIO',\n              'BATCH_READ_AHEAD', 'DOC_SHUFFLE_BUF_SIZE', 'SAMPLE_SHUFFLE_BUFFER_SIZE', 'BATCH_SHUFFLE_BUFFER_SIZE',\n              'MAX_TRANSCRIPT_WORD', 'MAX_SENT_LEN', 'MAX_SENT_NUM', 'DROPOUT', 'VOCAB_DIM', 'ROLE_SIZE', 'ROLE_DIM',\n              'POS_DIM', 'ENT_DIM', 'USE_ROLE', 'USE_POSENT', 'USE_BOS_TOKEN', 'USE_EOS_TOKEN',\n              'TRANSFORMER_EMBED_DROPOUT', 'TRANSFORMER_RESIDUAL_DROPOUT', 'TRANSFORMER_ATTENTION_DROPOUT',\n              'TRANSFORMER_LAYER', 'TRANSFORMER_HEAD', 'TRANSFORMER_POS_DISCOUNT', 'PRE_TOKENIZER',\n              'PRE_TOKENIZER_PATH', 'PYLEARN_MODEL', 'EXTRA_IDS', 'BEAM_WIDTH', 'EVAL_TOKENIZED', 'EVAL_LOWERCASE',\n              'MAX_GEN_LENGTH', 'MIN_GEN_LENGTH', 'NO_REPEAT_NGRAM_SIZE']\n\n        Return an instance of HMNet model for dialogue summarization.\n        \"\"\"", "\n", "super", "(", "HMNetModel", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "root_path", "=", "Path", "(", "__file__", ")", ".", "resolve", "(", ")", ".", "parent", "\n", "\n", "# we leave the most influential params with prompt and the others as hidden kwargs", "\n", "kwargs", "[", "\"MIN_GEN_LENGTH\"", "]", "=", "min_gen_length", "\n", "kwargs", "[", "\"MAX_GEN_LENGTH\"", "]", "=", "max_gen_length", "\n", "kwargs", "[", "\"BEAM_WIDTH\"", "]", "=", "beam_width", "\n", "kwargs", "[", "\"PYLEARN_MODEL\"", "]", "=", "get_cached_file_path", "(", "\n", "\"hmnet\"", ",", "\"model.pt\"", ",", "PRETRAINED_MODEL_DOWNLOAD_LINK", "\n", ")", ".", "parent", "\n", "self", ".", "opt", "=", "self", ".", "_parse_args", "(", "kwargs", ")", "\n", "self", ".", "model", "=", "HMNetTrainer", "(", "self", ".", "opt", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.dialogue.hmnet_model.HMNetModel._get_root": [[216, 222], ["os.getcwd", "os.path.join", "os.listdir", "os.path.dirname"], "methods", ["None"], ["", "def", "_get_root", "(", "self", ")", ":", "\n", "        ", "root_path", "=", "os", ".", "getcwd", "(", ")", "\n", "while", "\"model\"", "not", "in", "os", ".", "listdir", "(", "root_path", ")", ":", "\n", "            ", "root_path", "=", "os", ".", "path", ".", "dirname", "(", "root_path", ")", "\n", "", "root_path", "=", "os", ".", "path", ".", "join", "(", "root_path", ",", "\"model/dialogue\"", ")", "\n", "return", "root_path", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.dialogue.hmnet_model.HMNetModel._parse_args": [[223, 309], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "summertime.model.third_party.HMNet.Utils.Arguments.Arguments", "summertime.model.third_party.HMNet.Utils.Arguments.Arguments.readArguments", "os.path.basename", "argparse.ArgumentParser.parse_args.__dict__.items", "kwargs.items", "argparse.ArgumentParser.parse_args.config_overrides.split", "torch.cuda.is_available", "os.path.dirname", "os.path.join", "config_override.strip.strip.strip", "print", "key.upper", "print", "print", "config_override.strip.strip.split", "summertime.model.third_party.HMNet.Utils.Arguments.Arguments.add_opt", "summertime.model.third_party.HMNet.Utils.Arguments.Arguments.readArguments.keys", "len", "x.upper", "key.upper"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Utils.Arguments.Arguments.readArguments", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.utils.sentence_splitter.PunktSentenceSplitter.split", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.utils.sentence_splitter.PunktSentenceSplitter.split", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Utils.Arguments.Arguments.add_opt"], ["", "def", "_parse_args", "(", "self", ",", "kwargs", ")", ":", "\n", "        ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "\n", "description", "=", "\"HMNet: Pretrain or fine-tune models for HMNet model.\"", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--command\"", ",", "default", "=", "\"evaluate\"", ",", "help", "=", "\"Command: train/evaluate\"", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--conf_file\"", ",", "\n", "default", "=", "os", ".", "path", ".", "join", "(", "self", ".", "root_path", ",", "\"hmnet/config/dialogue.conf\"", ")", ",", "\n", "help", "=", "\"Path to the BigLearn conf file.\"", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--PYLEARN_MODEL\"", ",", "help", "=", "\"Overrides this option from the conf file.\"", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--master_port\"", ",", "help", "=", "\"Overrides this option default\"", ",", "default", "=", "None", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\"--cluster\"", ",", "help", "=", "\"local, philly or aml\"", ",", "default", "=", "\"local\"", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--dist_init_path\"", ",", "help", "=", "\"Distributed init path for AML\"", ",", "default", "=", "\"./tmp\"", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--fp16\"", ",", "\n", "action", "=", "\"store_true\"", ",", "\n", "help", "=", "\"Whether to use 16-bit float precision instead of 32-bit\"", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--fp16_opt_level\"", ",", "\n", "type", "=", "str", ",", "\n", "default", "=", "\"O1\"", ",", "\n", "help", "=", "\"For fp16: Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3'].\"", "\n", "\"See details at https://nvidia.github.io/apex/amp.html\"", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\"--no_cuda\"", ",", "action", "=", "\"store_true\"", ",", "help", "=", "\"Disable cuda.\"", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--config_overrides\"", ",", "\n", "help", "=", "\"Override parameters on config, VAR=val;VAR=val;...\"", ",", "\n", ")", "\n", "\n", "cmdline_args", "=", "parser", ".", "parse_args", "(", "[", "]", ")", "\n", "command", "=", "cmdline_args", ".", "command", "\n", "conf_file", "=", "cmdline_args", ".", "conf_file", "\n", "conf_args", "=", "Arguments", "(", "conf_file", ")", "\n", "opt", "=", "conf_args", ".", "readArguments", "(", ")", "\n", "\n", "if", "cmdline_args", ".", "config_overrides", ":", "\n", "            ", "for", "config_override", "in", "cmdline_args", ".", "config_overrides", ".", "split", "(", "\";\"", ")", ":", "\n", "                ", "config_override", "=", "config_override", ".", "strip", "(", ")", "\n", "if", "config_override", ":", "\n", "                    ", "var_val", "=", "config_override", ".", "split", "(", "\"=\"", ")", "\n", "assert", "(", "\n", "len", "(", "var_val", ")", "==", "2", "\n", ")", ",", "f\"Config override '{var_val}' does not have the form 'VAR=val'\"", "\n", "conf_args", ".", "add_opt", "(", "opt", ",", "var_val", "[", "0", "]", ",", "var_val", "[", "1", "]", ",", "force_override", "=", "True", ")", "\n", "\n", "", "", "", "opt", "[", "\"cuda\"", "]", "=", "torch", ".", "cuda", ".", "is_available", "(", ")", "and", "not", "cmdline_args", ".", "no_cuda", "\n", "opt", "[", "\"confFile\"", "]", "=", "conf_file", "\n", "if", "\"datadir\"", "not", "in", "opt", ":", "\n", "            ", "opt", "[", "\"datadir\"", "]", "=", "os", ".", "path", ".", "dirname", "(", "\n", "conf_file", "\n", ")", "# conf_file specifies where the data folder is", "\n", "", "opt", "[", "\"basename\"", "]", "=", "os", ".", "path", ".", "basename", "(", "\n", "conf_file", "\n", ")", "# conf_file specifies where the name of save folder is", "\n", "opt", "[", "\"command\"", "]", "=", "command", "\n", "\n", "# combine cmdline_args into opt dictionary", "\n", "for", "key", ",", "val", "in", "cmdline_args", ".", "__dict__", ".", "items", "(", ")", ":", "\n", "# if val is not None and key not in ['command', 'conf_file']:", "\n", "            ", "if", "val", "is", "not", "None", ":", "\n", "                ", "opt", "[", "key", "]", "=", "val", "\n", "\n", "# combine kwargs into opt dictionary (we allow lower case)", "\n", "", "", "for", "key", ",", "val", "in", "kwargs", ".", "items", "(", ")", ":", "\n", "            ", "valid_keys", "=", "[", "x", "for", "x", "in", "opt", ".", "keys", "(", ")", "if", "x", ".", "upper", "(", ")", "==", "x", "]", "\n", "if", "key", "==", "\"PYLEARN_MODEL\"", ":", "\n", "                ", "print", "(", "f\"Using model from location {val}/model.pt\"", ")", "\n", "", "if", "key", ".", "upper", "(", ")", "not", "in", "valid_keys", ":", "\n", "                ", "print", "(", "\"WARNING: {} is not a valid key in HMNet.\"", ".", "format", "(", "key", ")", ")", "\n", "print", "(", "\"The valid keys are:\"", ",", "valid_keys", ")", "\n", "continue", "\n", "", "if", "val", "is", "not", "None", ":", "\n", "                ", "opt", "[", "key", ".", "upper", "(", ")", "]", "=", "val", "\n", "\n", "", "", "return", "opt", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.dialogue.hmnet_model.HMNetModel.summarize": [[310, 328], ["hmnet_model.HMNetModel.assert_summ_input_type", "print", "os.path.join", "hmnet_model.HMNetModel._create_datafolder", "hmnet_model.HMNetModel._preprocess", "hmnet_model.HMNetModel._evaluate", "os.path.dirname", "corpus.__len__"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.multi_doc.base_multi_doc_model.MultiDocSummModel.assert_summ_input_type", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.dialogue.hmnet_model.HMNetModel._create_datafolder", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.dialogue.hmnet_model.HMNetModel._preprocess", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.dialogue.hmnet_model.HMNetModel._evaluate", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizerFast.__len__"], ["", "def", "summarize", "(", "self", ",", "corpus", ":", "List", "[", "List", "[", "str", "]", "]", ",", "queries", ":", "List", "[", "str", "]", "=", "None", ")", ":", "\n", "        ", "self", ".", "assert_summ_input_type", "(", "corpus", ",", "queries", ")", "\n", "\n", "print", "(", "f\"HMNet model: processing document of {corpus.__len__()} samples\"", ")", "\n", "# transform the original dataset to \"dialogue\" input", "\n", "# we only use test set path for evaluation", "\n", "data_folder", "=", "os", ".", "path", ".", "join", "(", "\n", "os", ".", "path", ".", "dirname", "(", "self", ".", "opt", "[", "\"datadir\"", "]", ")", ",", "\n", "\"ExampleRawData/meeting_summarization/AMI_proprec/test\"", ",", "\n", ")", "\n", "\n", "self", ".", "_create_datafolder", "(", "data_folder", ")", "\n", "self", ".", "_preprocess", "(", "corpus", ",", "data_folder", ")", "\n", "\n", "# return self.model.eval()", "\n", "results", "=", "self", ".", "_evaluate", "(", ")", "\n", "\n", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.dialogue.hmnet_model.HMNetModel._evaluate": [[329, 343], ["hmnet_model.HMNetModel.model.set_up_model", "hmnet_model.HMNetModel.model.get_batch_generator", "hmnet_model.HMNetModel._eval_batches", "hmnet_model.HMNetModel.model.log", "hmnet_model.HMNetModel.model.log"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.HMNetTrainer.HMNetTrainer.set_up_model", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.HMNetTrainer.HMNetTrainer.get_batch_generator", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.dialogue.hmnet_model.HMNetModel._eval_batches", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.DistributedTrainer.DistributedTrainer.log", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.DistributedTrainer.DistributedTrainer.log"], ["", "def", "_evaluate", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "opt", "[", "\"rank\"", "]", "==", "0", ":", "\n", "            ", "self", ".", "model", ".", "log", "(", "\"-----------------------------------------------\"", ")", "\n", "self", ".", "model", ".", "log", "(", "\"Evaluating model ... \"", ")", "\n", "\n", "", "self", ".", "model", ".", "set_up_model", "(", ")", "\n", "\n", "eval_dataset", "=", "\"test\"", "\n", "batch_generator_eval", "=", "self", ".", "model", ".", "get_batch_generator", "(", "eval_dataset", ")", "\n", "predictions", "=", "self", ".", "_eval_batches", "(", "\n", "self", ".", "model", ".", "module", ",", "batch_generator_eval", ",", "self", ".", "model", ".", "saveFolder", ",", "eval_dataset", "\n", ")", "\n", "\n", "return", "predictions", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.dialogue.hmnet_model.HMNetModel._eval_batches": [[344, 394], ["int", "print", "print", "time.time", "print", "isinstance", "torch.no_grad", "enumerate", "len", "module", "predictions.extend", "time.time", "len", "torch.is_tensor", "dev_batch[].to", "len", "hmnet_model.HMNetModel._convert_tokens_to_string"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.dialogue.hmnet_model.HMNetModel._convert_tokens_to_string"], ["", "def", "_eval_batches", "(", "self", ",", "module", ",", "dev_batches", ",", "save_folder", ",", "label", "=", "\"\"", ")", ":", "\n", "        ", "max_sent_len", "=", "int", "(", "self", ".", "opt", "[", "\"MAX_GEN_LENGTH\"", "]", ")", "\n", "\n", "print", "(", "\"Decoding current model ... \\nSaving folder is {}\"", ".", "format", "(", "save_folder", ")", ")", "\n", "print", "(", "\"Each sample will cost about 10 second.\"", ")", "\n", "import", "time", "\n", "\n", "start_time", "=", "time", ".", "time", "(", ")", "\n", "predictions", "=", "[", "]", "# prediction of tokens from model", "\n", "if", "not", "isinstance", "(", "module", ".", "tokenizer", ",", "list", ")", ":", "\n", "            ", "decoder_tokenizer", "=", "module", ".", "tokenizer", "\n", "", "elif", "len", "(", "module", ".", "tokenizer", ")", "==", "1", ":", "\n", "            ", "decoder_tokenizer", "=", "module", ".", "tokenizer", "[", "0", "]", "\n", "", "elif", "len", "(", "module", ".", "tokenizer", ")", "==", "2", ":", "\n", "            ", "decoder_tokenizer", "=", "module", ".", "tokenizer", "[", "1", "]", "\n", "", "else", ":", "\n", "            ", "assert", "False", ",", "\"len(module.tokenizer) > 2\"", "\n", "\n", "", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "for", "j", ",", "dev_batch", "in", "enumerate", "(", "dev_batches", ")", ":", "\n", "                ", "for", "b", "in", "dev_batch", ":", "\n", "                    ", "if", "torch", ".", "is_tensor", "(", "dev_batch", "[", "b", "]", ")", ":", "\n", "                        ", "dev_batch", "[", "b", "]", "=", "dev_batch", "[", "b", "]", ".", "to", "(", "self", ".", "opt", "[", "\"device\"", "]", ")", "\n", "\n", "", "", "beam_search_res", "=", "module", "(", "\n", "dev_batch", ",", "beam_search", "=", "True", ",", "max_sent_len", "=", "max_sent_len", "\n", ")", "\n", "pred", "=", "[", "\n", "[", "t", "[", "0", "]", "for", "t", "in", "x", "]", "if", "len", "(", "x", ")", ">", "0", "else", "[", "[", "]", "]", "for", "x", "in", "beam_search_res", "\n", "]", "\n", "predictions", ".", "extend", "(", "\n", "[", "\n", "[", "\n", "self", ".", "_convert_tokens_to_string", "(", "decoder_tokenizer", ",", "tt", ")", "\n", "for", "tt", "in", "t", "\n", "]", "\n", "for", "t", "in", "pred", "\n", "]", "\n", ")", "\n", "\n", "if", "(", "\n", "\"DEBUG\"", "in", "self", ".", "opt", "and", "j", ">=", "10", "\n", ")", "or", "j", ">=", "self", ".", "model", ".", "task", ".", "evaluator", ".", "eval_batches_num", ":", "\n", "# in debug mode (decode first 10 batches) ortherwise decode first self.eval_batches_num bathes", "\n", "                    ", "break", "\n", "\n", "", "", "", "top1_predictions", "=", "[", "x", "[", "0", "]", "for", "x", "in", "predictions", "]", "\n", "\n", "print", "(", "\"Total time for inference:\"", ",", "time", ".", "time", "(", ")", "-", "start_time", ")", "\n", "return", "top1_predictions", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.dialogue.hmnet_model.HMNetModel._convert_tokens_to_string": [[395, 405], ["tokenizer.decode", "t.lower", "tokenizer.convert_tokens_to_ids"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizerFast.decode", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizer.convert_tokens_to_ids"], ["", "def", "_convert_tokens_to_string", "(", "self", ",", "tokenizer", ",", "tokens", ")", ":", "\n", "        ", "if", "\"EVAL_TOKENIZED\"", "in", "self", ".", "opt", ":", "\n", "            ", "tokens", "=", "[", "t", "for", "t", "in", "tokens", "if", "t", "not", "in", "tokenizer", ".", "all_special_tokens", "]", "\n", "", "if", "\"EVAL_LOWERCASE\"", "in", "self", ".", "opt", ":", "\n", "            ", "tokens", "=", "[", "t", ".", "lower", "(", ")", "for", "t", "in", "tokens", "]", "\n", "", "if", "\"EVAL_TOKENIZED\"", "in", "self", ".", "opt", ":", "\n", "            ", "return", "\" \"", ".", "join", "(", "tokens", ")", "\n", "", "else", ":", "\n", "            ", "return", "tokenizer", ".", "decode", "(", "\n", "tokenizer", ".", "convert_tokens_to_ids", "(", "tokens", ")", ",", "skip_special_tokens", "=", "True", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.dialogue.hmnet_model.HMNetModel._preprocess": [[407, 455], ["enumerate", "isinstance", "new_sample[].append", "samples.append", "os.path.join", "RuntimeError", "nlp", "new_sample[].append", "gzip.open", "file.write", "x.strip", "len", "ent_id.append", "pos_id.append", "json.dumps", "turn.split", "str"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.utils.sentence_splitter.PunktSentenceSplitter.split"], ["", "", "def", "_preprocess", "(", "self", ",", "corpus", ",", "test_path", ")", ":", "\n", "        ", "samples", "=", "[", "]", "\n", "for", "i", ",", "sample", "in", "enumerate", "(", "corpus", ")", ":", "\n", "            ", "new_sample", "=", "{", "\"id\"", ":", "i", ",", "\"meeting\"", ":", "[", "]", ",", "\"summary\"", ":", "[", "]", "}", "\n", "if", "isinstance", "(", "sample", ",", "str", ")", ":", "\n", "                ", "raise", "RuntimeError", "(", "\n", "\"Error: the input of HMNet should be dialogues, rather than documents.\"", "\n", ")", "\n", "\n", "# add all the turns one by one", "\n", "", "for", "turn", "in", "sample", ":", "\n", "                ", "turn", "=", "[", "x", ".", "strip", "(", ")", "for", "x", "in", "turn", ".", "split", "(", "\":\"", ")", "]", "\n", "if", "len", "(", "turn", ")", "<", "2", ":", "\n", "                    ", "continue", "\n", "", "tokenized_turn", "=", "nlp", "(", "turn", "[", "1", "]", ")", "\n", "# In case we can't find proper entity in move_names", "\n", "ent_id", "=", "[", "]", "\n", "pos_id", "=", "[", "]", "\n", "for", "token", "in", "tokenized_turn", ":", "\n", "                    ", "ent", "=", "(", "\n", "token", ".", "ent_iob_", "+", "\"-\"", "+", "token", ".", "ent_type_", "\n", "if", "token", ".", "ent_iob_", "!=", "\"O\"", "\n", "else", "\"O\"", "\n", ")", "\n", "ent_id", ".", "append", "(", "ENT", "[", "ent", "]", "if", "ent", "in", "ENT", "else", "ENT", "[", "\"\"", "]", ")", "\n", "\n", "pos", "=", "token", ".", "tag_", "\n", "pos_id", ".", "append", "(", "POS", "[", "pos", "]", "if", "pos", "in", "POS", "else", "POS", "[", "\"\"", "]", ")", "\n", "\n", "", "new_sample", "[", "\"meeting\"", "]", ".", "append", "(", "\n", "{", "\n", "\"speaker\"", ":", "turn", "[", "0", "]", ",", "\n", "\"role\"", ":", "\"\"", ",", "\n", "\"utt\"", ":", "{", "\n", "\"word\"", ":", "[", "str", "(", "token", ")", "for", "token", "in", "tokenized_turn", "]", ",", "\n", "\"pos_id\"", ":", "pos_id", ",", "\n", "\"ent_id\"", ":", "ent_id", ",", "\n", "}", ",", "\n", "}", "\n", ")", "\n", "", "new_sample", "[", "\"summary\"", "]", ".", "append", "(", "\n", "\"This is a dummy summary. HMNet will filter out the sample w/o summary!\"", "\n", ")", "\n", "samples", ".", "append", "(", "new_sample", ")", "\n", "# save to the gzip", "\n", "file_path", "=", "os", ".", "path", ".", "join", "(", "test_path", ",", "\"split_{}.jsonl.gz\"", ".", "format", "(", "i", ")", ")", "\n", "with", "gzip", ".", "open", "(", "file_path", ",", "\"wt\"", ",", "encoding", "=", "\"utf-8\"", ")", "as", "file", ":", "\n", "                ", "file", ".", "write", "(", "json", ".", "dumps", "(", "new_sample", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.dialogue.hmnet_model.HMNetModel._clean_datafolder": [[456, 461], ["os.listdir", "os.path.join", "os.remove"], "methods", ["None"], ["", "", "", "def", "_clean_datafolder", "(", "self", ",", "data_folder", ")", ":", "\n", "        ", "for", "name", "in", "os", ".", "listdir", "(", "data_folder", ")", ":", "\n", "            ", "name", "=", "os", ".", "path", ".", "join", "(", "data_folder", ",", "name", ")", "\n", "if", "\".gz\"", "in", "name", ":", "\n", "                ", "os", ".", "remove", "(", "name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.dialogue.hmnet_model.HMNetModel._create_datafolder": [[462, 492], ["os.path.exists", "hmnet_model.HMNetModel._clean_datafolder", "os.makedirs", "open", "json.dump", "open", "json.dump", "os.path.join", "os.path.join", "os.path.dirname", "os.path.dirname", "os.path.dirname"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.dialogue.hmnet_model.HMNetModel._clean_datafolder"], ["", "", "", "def", "_create_datafolder", "(", "self", ",", "data_folder", ")", ":", "\n", "        ", "if", "os", ".", "path", ".", "exists", "(", "data_folder", ")", ":", "\n", "            ", "self", ".", "_clean_datafolder", "(", "data_folder", ")", "\n", "", "else", ":", "\n", "            ", "os", ".", "makedirs", "(", "data_folder", ")", "\n", "", "with", "open", "(", "\n", "os", ".", "path", ".", "join", "(", "os", ".", "path", ".", "dirname", "(", "data_folder", ")", ",", "\"test_ami.json\"", ")", ",", "\n", "\"w\"", ",", "\n", "encoding", "=", "\"utf-8\"", ",", "\n", ")", "as", "file", ":", "\n", "            ", "json", ".", "dump", "(", "\n", "[", "\n", "{", "\n", "\"source\"", ":", "{", "\n", "\"dataset\"", ":", "\"../ExampleRawData/meeting_summarization/AMI_proprec/test/\"", "\n", "}", ",", "\n", "\"task\"", ":", "\"meeting\"", ",", "\n", "\"name\"", ":", "\"ami\"", ",", "\n", "}", "\n", "]", ",", "\n", "file", ",", "\n", ")", "\n", "\n", "", "with", "open", "(", "\n", "os", ".", "path", ".", "join", "(", "\n", "os", ".", "path", ".", "dirname", "(", "os", ".", "path", ".", "dirname", "(", "data_folder", ")", ")", ",", "\"role_dict_ext.json\"", "\n", ")", ",", "\n", "\"w\"", ",", "\n", ")", "as", "file", ":", "\n", "            ", "json", ".", "dump", "(", "{", "}", ",", "file", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.dialogue.hmnet_model.HMNetModel.show_capability": [[493, 504], ["cls.generate_basic_description", "print"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.multi_doc.multi_doc_joint_model.MultiDocJointModel.generate_basic_description"], ["", "", "@", "classmethod", "\n", "def", "show_capability", "(", "cls", ")", "->", "None", ":", "\n", "        ", "basic_description", "=", "cls", ".", "generate_basic_description", "(", ")", "\n", "more_details", "=", "(", "\n", "\"A HMNet model finetuned on CNN-DM dataset for summarization.\\n\\n\"", "\n", "\"Strengths:\\n - High performance on dialogue summarization task.\\n\\n\"", "\n", "\"Weaknesses:\\n - Not suitable for datasets other than dialogues.\\n\\n\"", "\n", "\"Initialization arguments:\\n \"", "\n", "\" - `corpus`: Unlabelled corpus of documents.\\n\"", "\n", ")", "\n", "print", "(", "f\"{basic_description} \\n {'#' * 20} \\n {more_details}\"", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.dialogue.base_dialogue_model.DialogueSummModel.__init__": [[12, 22], ["summertime.model.base_model.SummModel.__init__"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.tests.model_selector_test.ToyModel.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "trained_domain", ":", "str", "=", "None", ",", "\n", "max_input_length", ":", "int", "=", "None", ",", "\n", "max_output_length", ":", "int", "=", "None", ",", "\n", ")", ":", "\n", "        ", "super", "(", "DialogueSummModel", ",", "self", ")", ".", "__init__", "(", "\n", "trained_domain", "=", "trained_domain", ",", "\n", "max_input_length", "=", "max_input_length", ",", "\n", "max_output_length", "=", "max_output_length", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.dialogue.base_dialogue_model.DialogueSummModel.assert_summ_input_type": [[24, 35], ["all", "re.compile", "all", "isinstance", "re.compile.match", "itertools.chain.from_iterable"], "methods", ["None"], ["", "@", "classmethod", "\n", "def", "assert_summ_input_type", "(", "\n", "cls", ",", "corpus", ":", "Union", "[", "List", "[", "str", "]", ",", "List", "[", "List", "[", "str", "]", "]", "]", ",", "queries", ":", "Union", "[", "List", "[", "str", "]", ",", "None", "]", "\n", ")", ":", "\n", "        ", "\"\"\"each instance must be a list of \\\"speaker : utterance\\\" \"\"\"", "\n", "assert", "all", "(", "[", "isinstance", "(", "instance", ",", "list", ")", "for", "instance", "in", "corpus", "]", ")", "\n", "\n", "pattern", "=", "re", ".", "compile", "(", "r\"\\w+\\s:\\s\\w+\"", ")", "\n", "assert", "all", "(", "\n", "[", "pattern", ".", "match", "(", "instance", ")", "for", "instance", "in", "chain", ".", "from_iterable", "(", "corpus", ")", "]", "\n", ")", ",", "'each instance must be a list of \"[speaker] : [utterance]\", the \":\" is essential'", "\n", "", "", ""]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.multi_doc.base_multi_doc_model.MultiDocSummModel.__init__": [[8, 18], ["summertime.model.base_model.SummModel.__init__"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.tests.model_selector_test.ToyModel.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "trained_domain", ":", "str", "=", "None", ",", "\n", "max_input_length", ":", "int", "=", "None", ",", "\n", "max_output_length", ":", "int", "=", "None", ",", "\n", ")", ":", "\n", "        ", "super", "(", "MultiDocSummModel", ",", "self", ")", ".", "__init__", "(", "\n", "trained_domain", "=", "trained_domain", ",", "\n", "max_input_length", "=", "max_input_length", ",", "\n", "max_output_length", "=", "max_output_length", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.multi_doc.base_multi_doc_model.MultiDocSummModel.assert_summ_input_type": [[20, 40], ["all", "TypeError", "isinstance", "TypeError", "all", "TypeError", "isinstance", "all", "isinstance", "isinstance"], "methods", ["None"], ["", "@", "classmethod", "\n", "def", "assert_summ_input_type", "(", "cls", ",", "corpus", ",", "query", ")", ":", "\n", "        ", "if", "not", "all", "(", "\n", "[", "\n", "isinstance", "(", "ins", ",", "list", ")", "and", "all", "(", "[", "isinstance", "(", "doc", ",", "str", ")", "for", "doc", "in", "ins", "]", ")", "\n", "for", "ins", "in", "corpus", "\n", "]", "\n", ")", ":", "\n", "            ", "raise", "TypeError", "(", "\n", "\"Multi-document summarization models summarize instances of multiple documents (`List[List[str]]`).\"", "\n", ")", "\n", "\n", "", "if", "query", "is", "not", "None", ":", "\n", "            ", "if", "not", "isinstance", "(", "query", ",", "list", ")", ":", "\n", "                ", "raise", "TypeError", "(", "\n", "\"Query-based single-document summarization requires query of `List[str]`.\"", "\n", ")", "\n", "", "if", "not", "all", "(", "[", "isinstance", "(", "q", ",", "str", ")", "for", "q", "in", "query", "]", ")", ":", "\n", "                ", "raise", "TypeError", "(", "\n", "\"Query-based single-document summarization requires query of `List[str]`.\"", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.multi_doc.multi_doc_separate_model.MultiDocSeparateModel.__init__": [[12, 20], ["model_backend", "base_multi_doc_model.MultiDocSummModel.__init__"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.tests.model_selector_test.ToyModel.__init__"], ["def", "__init__", "(", "self", ",", "model_backend", ":", "SummModel", "=", "TextRankModel", ",", "**", "kwargs", ")", ":", "\n", "        ", "model", ":", "SummModel", "=", "model_backend", "(", "**", "kwargs", ")", "\n", "self", ".", "model", "=", "model", "\n", "\n", "super", "(", "MultiDocSeparateModel", ",", "self", ")", ".", "__init__", "(", "\n", "trained_domain", "=", "self", ".", "model", ".", "trained_domain", ",", "\n", "max_input_length", "=", "self", ".", "model", ".", "max_input_length", ",", "\n", "max_output_length", "=", "self", ".", "model", ".", "max_output_length", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.multi_doc.multi_doc_separate_model.MultiDocSeparateModel.summarize": [[22, 34], ["multi_doc_separate_model.MultiDocSeparateModel.assert_summ_input_type", "multi_doc_separate_model.MultiDocSeparateModel.model.summarize", "summaries.append"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.multi_doc.base_multi_doc_model.MultiDocSummModel.assert_summ_input_type", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.tests.model_selector_test.ToyModel.summarize"], ["", "def", "summarize", "(", "\n", "self", ",", "\n", "corpus", ":", "Union", "[", "List", "[", "str", "]", ",", "List", "[", "List", "[", "str", "]", "]", "]", ",", "\n", "query", ":", "Union", "[", "List", "[", "str", "]", ",", "List", "[", "List", "[", "str", "]", "]", "]", "=", "None", ",", "\n", ")", "->", "List", "[", "str", "]", ":", "\n", "        ", "self", ".", "assert_summ_input_type", "(", "corpus", ",", "None", ")", "\n", "summaries", "=", "[", "]", "\n", "for", "instance", "in", "corpus", ":", "\n", "            ", "instance_summaries", "=", "self", ".", "model", ".", "summarize", "(", "instance", ")", "\n", "summaries", ".", "append", "(", "\" \"", ".", "join", "(", "instance_summaries", ")", ")", "\n", "\n", "", "return", "summaries", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.multi_doc.multi_doc_separate_model.MultiDocSeparateModel.generate_basic_description": [[35, 43], ["None"], "methods", ["None"], ["", "@", "classmethod", "\n", "def", "generate_basic_description", "(", "cls", ")", "->", "str", ":", "\n", "        ", "basic_description", "=", "(", "\n", "\"MultiDocSeparateModel performs multi-document summarization by\"", "\n", "\" first performing single-document summarization on each document,\"", "\n", "\" and then concatenating the results.\"", "\n", ")", "\n", "return", "basic_description", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.multi_doc.multi_doc_separate_model.MultiDocSeparateModel.show_capability": [[44, 55], ["cls.generate_basic_description", "print"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.multi_doc.multi_doc_joint_model.MultiDocJointModel.generate_basic_description"], ["", "@", "classmethod", "\n", "def", "show_capability", "(", "cls", ")", ":", "\n", "        ", "basic_description", "=", "cls", ".", "generate_basic_description", "(", ")", "\n", "more_details", "=", "(", "\n", "\"A multi-document summarization model.\"", "\n", "\" Allows for custom model backend selection at initialization.\"", "\n", "\" Performs single-document summarization on each document in corpus and returns concatenated result.\\n\"", "\n", "\"Strengths: \\n - Allows for control of backend model.\\n\"", "\n", "\"Weaknesses: \\n - Assumes all documents are equally weighted.\\n - May produce redundant information for similar documents.\\n\"", "\n", ")", "\n", "print", "(", "f\"{basic_description}\\n{'#' * 20}\\n{more_details}\"", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.multi_doc.multi_doc_joint_model.MultiDocJointModel.__init__": [[12, 20], ["model_backend", "base_multi_doc_model.MultiDocSummModel.__init__"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.tests.model_selector_test.ToyModel.__init__"], ["def", "__init__", "(", "self", ",", "model_backend", ":", "SummModel", "=", "TextRankModel", ",", "**", "kwargs", ")", ":", "\n", "        ", "model", ":", "SummModel", "=", "model_backend", "(", "**", "kwargs", ")", "\n", "self", ".", "model", "=", "model", "\n", "\n", "super", "(", "MultiDocJointModel", ",", "self", ")", ".", "__init__", "(", "\n", "trained_domain", "=", "self", ".", "model", ".", "trained_domain", ",", "\n", "max_input_length", "=", "self", ".", "model", ".", "max_input_length", ",", "\n", "max_output_length", "=", "self", ".", "model", ".", "max_output_length", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.multi_doc.multi_doc_joint_model.MultiDocJointModel.summarize": [[22, 35], ["multi_doc_joint_model.MultiDocJointModel.assert_summ_input_type", "multi_doc_joint_model.MultiDocJointModel.model.summarize", "joint_corpus.append"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.multi_doc.base_multi_doc_model.MultiDocSummModel.assert_summ_input_type", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.tests.model_selector_test.ToyModel.summarize"], ["", "def", "summarize", "(", "\n", "self", ",", "\n", "corpus", ":", "Union", "[", "List", "[", "str", "]", ",", "List", "[", "List", "[", "str", "]", "]", "]", ",", "\n", "query", ":", "Union", "[", "List", "[", "str", "]", ",", "List", "[", "List", "[", "str", "]", "]", "]", "=", "None", ",", "\n", ")", "->", "List", "[", "str", "]", ":", "\n", "        ", "self", ".", "assert_summ_input_type", "(", "corpus", ",", "None", ")", "\n", "joint_corpus", "=", "[", "]", "\n", "for", "instance", "in", "corpus", ":", "\n", "            ", "joint_corpus", ".", "append", "(", "\" \"", ".", "join", "(", "instance", ")", ")", "\n", "\n", "", "summaries", "=", "self", ".", "model", ".", "summarize", "(", "joint_corpus", ")", "\n", "\n", "return", "summaries", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.multi_doc.multi_doc_joint_model.MultiDocJointModel.generate_basic_description": [[36, 44], ["None"], "methods", ["None"], ["", "@", "classmethod", "\n", "def", "generate_basic_description", "(", "cls", ")", "->", "str", ":", "\n", "        ", "basic_description", "=", "(", "\n", "\"MultiDocJointModel performs multi-document summarization by\"", "\n", "\" first concatenating all documents,\"", "\n", "\" and then performing single-document summarization on the concatenation.\"", "\n", ")", "\n", "return", "basic_description", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.multi_doc.multi_doc_joint_model.MultiDocJointModel.show_capability": [[45, 57], ["cls.generate_basic_description", "print"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.multi_doc.multi_doc_joint_model.MultiDocJointModel.generate_basic_description"], ["", "@", "classmethod", "\n", "def", "show_capability", "(", "cls", ")", ":", "\n", "        ", "basic_description", "=", "cls", ".", "generate_basic_description", "(", ")", "\n", "more_details", "=", "(", "\n", "\"A multi-document summarization model.\"", "\n", "\" Allows for custom model backend selection at initialization.\"", "\n", "\" Concatenates each document in corpus and returns single-document summarization of joint corpus.\\n\"", "\n", "\"Strengths: \\n - Allows for control of backend model.\\n\"", "\n", "\"Weaknesses: \\n - Assumes all documents are equally weighted.\\n\"", "\n", "\" - May fail to extract information from certain documents.\\n\"", "\n", ")", "\n", "print", "(", "f\"{basic_description}\\n{'#' * 20}\\n{more_details}\"", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.pipeline.__init__.get_lxr_train_set": [[11, 32], ["range", "list", "subset.append", "map", "next", "iter", "isinstance"], "function", ["None"], ["QMsumDataset", ",", "\n", "ArxivDataset", ",", "\n", "MassivesummDataset", ",", "\n", ")", "\n", "\n", "from", "summertime", ".", "dataset", ".", "st_dataset", "import", "CustomDataset", "\n", "\n", "SUPPORTED_SUMM_DATASETS", "=", "[", "\n", "CnndmDataset", ",", "\n", "MultinewsDataset", ",", "\n", "SamsumDataset", ",", "\n", "XsumDataset", ",", "\n", "PubmedqaDataset", ",", "\n", "MlsumDataset", ",", "\n", "XlsumDataset", ",", "\n", "ScisummnetDataset", ",", "\n", "SummscreenDataset", ",", "\n", "QMsumDataset", ",", "\n", "ArxivDataset", ",", "\n", "MassivesummDataset", ",", "\n", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.pipeline.__init__.assemble_model_pipeline": [[34, 143], ["list", "list", "list", "list", "list", "isinstance", "dataset", "filter", "filter", "filter", "filter", "list", "map", "model_cls", "model_cls", "model_cls", "map", "__init__.get_lxr_train_set", "matching_models.append", "query_model_cls", "matching_models.append", "matching_models.append", "multi_doc_model_cls", "multi_doc_model_cls", "query_model_cls", "query_model_cls", "__init__.get_lxr_train_set", "__init__.get_lxr_train_set"], "function", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.pipeline.__init__.get_lxr_train_set", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.pipeline.__init__.get_lxr_train_set", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.pipeline.__init__.get_lxr_train_set"], ["def", "list_all_datasets", "(", ")", ":", "\n", "    ", "all_datasets", "=", "[", "]", "\n", "for", "ds", "in", "SUPPORTED_SUMM_DATASETS", ":", "\n", "        ", "dataset_description", "=", "ds", ".", "generate_basic_description", "(", ")", "\n", "\n", "all_datasets", ".", "append", "(", "(", "ds", ".", "dataset_name", ",", "dataset_description", ")", ")", "\n", "\n", "", "return", "all_datasets", "\n", "", ""]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.util.download_utils.get_cached_file_path": [[11, 32], ["pathlib.Path", "pathlib.Path", "pathlib.Path.exists", "pathlib.Path.mkdir", "pathlib.Path.exists", "ValueError", "download_utils.download_with_progressbar", "pathlib.Path.exists"], "function", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.util.download_utils.download_with_progressbar"], ["def", "get_cached_file_path", "(", "\n", "sub_dir", ":", "str", ",", "file_name", ":", "str", ",", "url", ":", "str", "=", "None", ",", "force_download", ":", "bool", "=", "False", "\n", ")", ":", "\n", "    ", "\"\"\"get the file from cached location, optionally, if not cached, then download it\"\"\"", "\n", "cache_dir", "=", "Path", "(", "HF_CACHE_LOCATION", ",", "sub_dir", ")", "\n", "\n", "if", "not", "cache_dir", ".", "exists", "(", ")", ":", "\n", "        ", "cache_dir", ".", "mkdir", "(", ")", "\n", "\n", "", "file_path", "=", "Path", "(", "cache_dir", ",", "file_name", ")", "\n", "if", "file_path", ".", "exists", "(", ")", "and", "not", "force_download", ":", "\n", "        ", "return", "file_path", "\n", "", "elif", "not", "file_path", ".", "exists", "(", ")", "and", "not", "url", ":", "\n", "        ", "raise", "ValueError", "(", "\n", "\"file {} does not exist and url for downloading is not provided\"", ".", "format", "(", "\n", "file_path", "\n", ")", "\n", ")", "\n", "", "else", ":", "\n", "        ", "download_with_progressbar", "(", "url", ",", "file_path", ")", "\n", "return", "file_path", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.util.download_utils.download_with_progressbar": [[37, 53], ["print", "urllib.request.urlretrieve", "progressbar.ProgressBar", "progressbar.ProgressBar.start", "progressbar.ProgressBar.update", "progressbar.ProgressBar.finish"], "function", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.Trainers.HMNetTrainer.HMNetTrainer.update"], ["def", "download_with_progressbar", "(", "url", ":", "str", ",", "file_path", ":", "str", ")", ":", "\n", "    ", "def", "show_progress", "(", "block_num", ",", "block_size", ",", "total_size", ")", ":", "\n", "        ", "global", "pbar", "\n", "if", "pbar", "is", "None", ":", "\n", "            ", "pbar", "=", "progressbar", ".", "ProgressBar", "(", "maxval", "=", "total_size", ")", "\n", "pbar", ".", "start", "(", ")", "\n", "\n", "", "downloaded", "=", "block_num", "*", "block_size", "\n", "if", "downloaded", "<", "total_size", ":", "\n", "            ", "pbar", ".", "update", "(", "downloaded", ")", "\n", "", "else", ":", "\n", "            ", "pbar", ".", "finish", "(", ")", "\n", "pbar", "=", "None", "\n", "\n", "", "", "print", "(", "f\"Start downloading {file_path} from {url}...\"", ")", "\n", "urllib", ".", "request", ".", "urlretrieve", "(", "url", ",", "file_path", ",", "show_progress", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.util.massivesumm_utils.Article.__init__": [[58, 68], ["massivesumm_utils.Article._parse_html"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.util.massivesumm_utils.Article._parse_html"], ["def", "__init__", "(", "self", ",", "archive", ",", "html", ")", ":", "\n", "\n", "        ", "self", ".", "archive", "=", "archive", "\n", "self", ".", "html", "=", "html", "if", "html", "is", "not", "None", "else", "\"\"", "\n", "\n", "# @djam my doing", "\n", "self", ".", "url", "=", "archive", "\n", "self", ".", "date", "=", "None", "\n", "# self._parse_archive()", "\n", "self", ".", "_parse_html", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.util.massivesumm_utils.Article._parse_archive": [[69, 76], ["massivesumm_utils.Article.archive.split", "splits[].split", "massivesumm_utils.Article.normalize_url"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.utils.sentence_splitter.PunktSentenceSplitter.split", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.utils.sentence_splitter.PunktSentenceSplitter.split", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.util.massivesumm_utils.Article.normalize_url"], ["", "def", "_parse_archive", "(", "self", ")", ":", "\n", "\n", "        ", "*", "splits", ",", "url", "=", "self", ".", "archive", ".", "split", "(", "\"id_/\"", ")", "\n", "*", "_", ",", "date", "=", "splits", "[", "0", "]", ".", "split", "(", "\"/\"", ")", "\n", "\n", "self", ".", "url", "=", "self", ".", "normalize_url", "(", "url", ")", "\n", "self", ".", "date", "=", "date", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.util.massivesumm_utils.Article._parse_html": [[77, 84], ["massivesumm_utils.Article._load_html", "massivesumm_utils.Article._find_canonical_url", "massivesumm_utils.Article._extract_text", "massivesumm_utils.Article._extract_summary"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.util.massivesumm_utils.Article._load_html", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.util.massivesumm_utils.Article._find_canonical_url", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.util.massivesumm_utils.Article._extract_text", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.util.massivesumm_utils.Article._extract_summary"], ["", "def", "_parse_html", "(", "self", ")", ":", "\n", "\n", "        ", "self", ".", "_load_html", "(", ")", "\n", "self", ".", "_find_canonical_url", "(", ")", "\n", "\n", "self", ".", "_extract_text", "(", ")", "\n", "self", ".", "_extract_summary", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.util.massivesumm_utils.Article._extract_summary": [[85, 121], ["massivesumm_utils.Article.soup.findAll", "meta.attrs.items", "len", "sorted", "meta.get().strip", "meta.get"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.closablequeue.ClosableQueue.get"], ["", "def", "_extract_summary", "(", "self", ")", ":", "\n", "\n", "        ", "self", ".", "all_summaries", "=", "{", "}", "\n", "\n", "for", "meta", "in", "self", ".", "soup", ".", "findAll", "(", "\"meta\"", ")", ":", "\n", "            ", "for", "attr", ",", "value", "in", "meta", ".", "attrs", ".", "items", "(", ")", ":", "\n", "\n", "                ", "if", "attr", "in", "(", "\"name\"", ",", "\"property\"", ")", "and", "\"description\"", "in", "value", ":", "\n", "\n", "# Extract the tag content. If we can't find anything,", "\n", "# ignore it and move onto the next tag.", "\n", "\n", "                    ", "try", ":", "\n", "\n", "                        ", "self", ".", "all_summaries", "[", "value", "]", "=", "meta", ".", "get", "(", "\"content\"", ")", ".", "strip", "(", ")", "\n", "\n", "", "except", "Exception", ":", "\n", "\n", "                        ", "continue", "\n", "\n", "", "", "", "", "if", "len", "(", "self", ".", "all_summaries", ")", "==", "0", ":", "\n", "\n", "            ", "self", ".", "summary", "=", "None", "\n", "return", "\n", "\n", "", "for", "kind", "in", "(", "\"og:description\"", ",", "\"twitter:description\"", ",", "\"description\"", ")", ":", "\n", "\n", "            ", "if", "kind", "in", "self", ".", "all_summaries", ":", "\n", "\n", "                ", "self", ".", "summary", "=", "self", ".", "all_summaries", "[", "kind", "]", "\n", "break", "\n", "\n", "", "", "else", ":", "\n", "\n", "            ", "random_pick", "=", "sorted", "(", "self", ".", "all_summaries", ")", "[", "0", "]", "\n", "self", ".", "summary", "=", "self", ".", "all_summaries", "[", "random_pick", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.util.massivesumm_utils.Article._extract_text": [[122, 157], ["bs4.BeautifulSoup", "bs4.BeautifulSoup.findAll", "massivesumm_utils.Article.readability.short_title", "massivesumm_utils.Article.readability.summary", "len", "_whitespace.sub().strip", "paragraph_text.append", "paragraph.text.split", "_whitespace.sub"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.utils.sentence_splitter.PunktSentenceSplitter.split"], ["", "", "def", "_extract_text", "(", "self", ")", ":", "\n", "\n", "        ", "\"\"\"\n        Uses Readability to extract the body text and titles of the articles.\n        \"\"\"", "\n", "\n", "# Confusingly, the Readability package calls the body text of the article", "\n", "# its \"summary.\" We want to create a plain text document from the body text,", "\n", "# so we need to extract the text from Readability's HTML version.", "\n", "\n", "body_soup", "=", "BeautifulSoup", "(", "self", ".", "readability", ".", "summary", "(", ")", ",", "\"lxml\"", ")", "\n", "\n", "# Now go through and extract each paragraph (in order).", "\n", "\n", "paragraph_text", "=", "[", "]", "\n", "for", "paragraph", "in", "body_soup", ".", "findAll", "(", "\"p\"", ")", ":", "\n", "\n", "# Very short pieces of text tend not to be article body text, but", "\n", "# captions, attributions, and advertising. It seems that excluding", "\n", "# paragraphs shorter than five words removes most of this.", "\n", "\n", "            ", "if", "len", "(", "paragraph", ".", "text", ".", "split", "(", ")", ")", ">=", "5", ":", "\n", "\n", "                ", "paragraph_body", "=", "_whitespace", ".", "sub", "(", "\" \"", ",", "paragraph", ".", "text", ")", ".", "strip", "(", ")", "\n", "paragraph_text", ".", "append", "(", "paragraph_body", ")", "\n", "\n", "# We join the plain text paragraphs of the article with double new lines.", "\n", "\n", "", "", "self", ".", "text", "=", "\"\\n\\n\"", ".", "join", "(", "paragraph_text", ")", "\n", "\n", "# \"Short title\" uses in-page heuristics to remove cruft from <title>; e.g.:", "\n", "# .title():       American Recalls Moment Leg Broken by Truck in Nice - ABC News", "\n", "# .short_title(): American Recalls Moment Leg Broken by Truck in Nice", "\n", "\n", "self", ".", "title", "=", "self", ".", "readability", ".", "short_title", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.util.massivesumm_utils.Article._load_html": [[158, 172], ["readability.Document", "bs4.BeautifulSoup", "massivesumm_utils.Article.html.strip", "Exception"], "methods", ["None"], ["", "def", "_load_html", "(", "self", ")", ":", "\n", "\n", "# Readability crashes if it encounters empty pages.", "\n", "\n", "        ", "if", "self", ".", "html", ".", "strip", "(", ")", "==", "\"\"", ":", "\n", "\n", "            ", "raise", "Exception", "(", "\"No page content?\"", ")", "\n", "\n", "# The document has content. Create:", "\n", "# - A Readability parse object to extract the text", "\n", "# - A full-page BeautifulSoup object to extract summaries.", "\n", "\n", "", "self", ".", "readability", "=", "Document", "(", "self", ".", "html", ")", "\n", "self", ".", "soup", "=", "BeautifulSoup", "(", "self", ".", "html", ",", "\"lxml\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.util.massivesumm_utils.Article._find_canonical_url": [[173, 210], ["massivesumm_utils.Article.soup.find().get", "urllib.parse.urljoin", "massivesumm_utils.Article.normalize_url", "massivesumm_utils.Article.same_domain", "massivesumm_utils.Article.soup.find"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.closablequeue.ClosableQueue.get", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.util.massivesumm_utils.Article.normalize_url", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.util.massivesumm_utils.Article.same_domain"], ["", "def", "_find_canonical_url", "(", "self", ")", ":", "\n", "\n", "# Start out by normalizing the URL as we know it. Without reading the", "\n", "# page yet, this is our best guess of the article's canonical URL.", "\n", "\n", "        ", "self", ".", "original_url", "=", "self", ".", "url", "\n", "\n", "try", ":", "\n", "\n", "# Try to extract the page's canonical URL, if it has one. If it doesn't,", "\n", "# BeautifulSoup will raise an exception, and we will give up, sticking", "\n", "# with the normalized URL as the best URL.", "\n", "\n", "            ", "rel_canon", "=", "self", ".", "soup", ".", "find", "(", "\"link\"", ",", "{", "\"rel\"", ":", "\"canonical\"", "}", ")", ".", "get", "(", "\"href\"", ")", "\n", "\n", "# I've sometimes seen the canonical URL be relative to the current page.", "\n", "# Although this is rare, we can handle this using our best knowledge of", "\n", "# the page's URL so far. Just in case, we'll normalize this too.", "\n", "\n", "abs_canon_url", "=", "urljoin", "(", "self", ".", "url", ",", "rel_canon", ")", "\n", "norm_canon_url", "=", "self", ".", "normalize_url", "(", "abs_canon_url", ")", "\n", "\n", "# Sometimes, the canonical URL will be on a completely different domain.", "\n", "# I'm not sure why. But as a sanity check, make sure it's on the same", "\n", "# domain before using it.", "\n", "\n", "if", "self", ".", "same_domain", "(", "self", ".", "url", ",", "norm_canon_url", ")", ":", "\n", "\n", "                ", "self", ".", "url", "=", "self", ".", "norm_canon_url", "\n", "\n", "", "", "except", "Exception", ":", "\n", "\n", "# If we've failed at some point (most likely because the page doesn't", "\n", "# use the canonical tag), set the canonical and normalized canonical", "\n", "# URLs to None so that the user is aware of this.", "\n", "\n", "            ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.util.massivesumm_utils.Article.serialize": [[211, 224], ["None"], "methods", ["None"], ["", "", "def", "serialize", "(", "self", ")", ":", "\n", "\n", "        ", "\"\"\"\n        Return simple page object to JSONify and write to file.\n        \"\"\"", "\n", "\n", "return", "{", "\n", "\"url\"", ":", "self", ".", "url", ",", "\n", "\"archive\"", ":", "self", ".", "archive", ",", "\n", "\"title\"", ":", "self", ".", "title", ",", "\n", "\"date\"", ":", "self", ".", "date", ",", "\n", "\"text\"", ":", "self", ".", "text", ",", "\n", "\"summary\"", ":", "self", ".", "summary", ",", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.util.massivesumm_utils.Article.process": [[226, 239], ["page.get", "page.get", "page.get", "massivesumm_utils.Article.serialize", "print", "massivesumm_utils.Article"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.closablequeue.ClosableQueue.get", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.closablequeue.ClosableQueue.get", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.closablequeue.ClosableQueue.get", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.util.massivesumm_utils.Article.serialize"], ["", "@", "staticmethod", "\n", "def", "process", "(", "page", ")", ":", "\n", "\n", "        ", "url", "=", "page", ".", "get", "(", "\"archive\"", ",", "page", ".", "get", "(", "\"url\"", ")", ")", "\n", "html", "=", "page", ".", "get", "(", "\"html\"", ",", "\"\"", ")", "\n", "if", "html", "is", "None", ":", "\n", "            ", "html", "=", "\"\"", "\n", "\n", "", "try", ":", "\n", "            ", "return", "Article", "(", "url", ",", "html", ")", ".", "serialize", "(", ")", "\n", "", "except", "Exception", ":", "\n", "            ", "print", "(", "\"FAILING TO PROCESS HTML\"", ")", "\n", "return", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.util.massivesumm_utils.Article.same_domain": [[240, 249], ["urllib.parse.urlparse", "urllib.parse.urlparse"], "methods", ["None"], ["", "", "@", "staticmethod", "\n", "def", "same_domain", "(", "url1", ",", "url2", ")", ":", "\n", "\n", "        ", "\"\"\"\n        Check if two URLs share the same domain (urlparse netloc).\n        This is used primarily in evaluating canonical URLs.\n        \"\"\"", "\n", "\n", "return", "urlparse", "(", "url1", ")", ".", "netloc", "==", "urlparse", "(", "url2", ")", ".", "netloc", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.util.massivesumm_utils.Article.normalize_url": [[250, 273], ["url.replace().replace().replace", "urllib.parse.urlparse", "parsed._replace._replace._replace", "parsed._replace._replace.geturl", "url.replace().replace", "urllib.parse.quote", "parsed._replace._replace.netloc.replace", "url.replace"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "normalize_url", "(", "url", ")", ":", "\n", "\n", "        ", "\"\"\"\n        Remove fragments, ports, and other junk from Archive.org scrapes.\n        This is to detect duplicate pages, and prettify URLs.\n        \"\"\"", "\n", "\n", "# Multiple forward slashes should be replaced with just one.", "\n", "\n", "cleaned", "=", "url", ".", "replace", "(", "\"://\"", ",", "\"\\0\"", ")", ".", "replace", "(", "\"//\"", ",", "\"/\"", ")", ".", "replace", "(", "\"\\0\"", ",", "\"://\"", ")", "\n", "\n", "# Removing fragments and query parameters.", "\n", "\n", "parsed", "=", "urlparse", "(", "cleaned", ")", "\n", "parsed", "=", "parsed", ".", "_replace", "(", "\n", "path", "=", "quote", "(", "parsed", ".", "path", ",", "safe", "=", "\"%/\"", ")", ",", "\n", "netloc", "=", "parsed", ".", "netloc", ".", "replace", "(", "\":80\"", ",", "\"\"", ")", ",", "\n", "query", "=", "\"\"", ",", "\n", "fragment", "=", "\"\"", ",", "\n", ")", "\n", "\n", "return", "parsed", ".", "geturl", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.util.massivesumm_utils.load_samples": [[275, 282], ["gzip.open", "tqdm.tqdm", "json.loads", "samples.append"], "function", ["None"], ["", "", "def", "load_samples", "(", "filename", ":", "str", ")", "->", "list", ":", "\n", "    ", "samples", "=", "[", "]", "\n", "with", "gzip", ".", "open", "(", "filename", ")", "as", "fh_in", ":", "\n", "        ", "for", "row", "in", "tqdm", "(", "fh_in", ")", ":", "\n", "            ", "sample", "=", "json", ".", "loads", "(", "row", ")", "\n", "samples", ".", "append", "(", "sample", ")", "\n", "", "", "return", "samples", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.util.massivesumm_utils.download_sample": [[284, 309], ["int", "int", "requests.get", "io.BytesIO", "gzip.GzipFile", "gzip.GzipFile.read().decode", "decompressed_file.read().decode.strip().split", "gzip.GzipFile.read", "decompressed_file.read().decode.strip", "open", "err_log.write", "json.dumps"], "function", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.closablequeue.ClosableQueue.get", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.tokenization_utils.PreTrainedTokenizerFast.decode", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.utils.sentence_splitter.PunktSentenceSplitter.split"], ["", "def", "download_sample", "(", "sample", ":", "dict", ")", "->", "dict", ":", "\n", "    ", "filename", "=", "sample", "[", "\"filename\"", "]", "\n", "length", "=", "int", "(", "sample", "[", "\"length\"", "]", ")", "\n", "offset", "=", "int", "(", "sample", "[", "\"offset\"", "]", ")", "\n", "\n", "offset_end", "=", "offset", "+", "length", "-", "1", "\n", "# We'll get the file via HTTPS so we don't need to worry about S3 credentials", "\n", "# Getting the file on S3 is equivalent however - you can request a Range", "\n", "prefix", "=", "\"https://commoncrawl.s3.amazonaws.com/\"", "\n", "# We can then use the Range header to ask for just this set of bytes", "\n", "try", ":", "\n", "        ", "resp", "=", "requests", ".", "get", "(", "\n", "prefix", "+", "filename", ",", "\n", "headers", "=", "{", "\"Range\"", ":", "\"bytes={}-{}\"", ".", "format", "(", "offset", ",", "offset_end", ")", "}", ",", "\n", ")", "\n", "\n", "compressed_file", "=", "io", ".", "BytesIO", "(", "resp", ".", "content", ")", "\n", "decompressed_file", "=", "gzip", ".", "GzipFile", "(", "fileobj", "=", "compressed_file", ")", "\n", "data", "=", "decompressed_file", ".", "read", "(", ")", ".", "decode", "(", ")", "\n", "warc", ",", "header", ",", "response", "=", "data", ".", "strip", "(", ")", ".", "split", "(", "\"\\r\\n\\r\\n\"", ",", "2", ")", "\n", "return", "{", "\"html\"", ":", "response", ",", "**", "sample", "}", "\n", "", "except", "Exception", ":", "\n", "        ", "with", "open", "(", "\"error.log\"", ",", "\"at\"", ")", "as", "err_log", ":", "\n", "            ", "err_log", ".", "write", "(", "json", ".", "dumps", "(", "sample", ")", "+", "\"\\n\"", ")", "\n", "", "return", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.util.massivesumm_utils.download_list": [[311, 316], ["multiprocessing.Pool", "pool.imap_unordered"], "function", ["None"], ["", "", "def", "download_list", "(", "samples", ",", "n_processes", ":", "int", ")", ":", "\n", "    ", "with", "Pool", "(", "n_processes", ")", "as", "pool", ":", "\n", "        ", "for", "sample", "in", "pool", ".", "imap_unordered", "(", "download_sample", ",", "samples", ")", ":", "\n", "            ", "if", "sample", ":", "# don't yield failing samples", "\n", "                ", "yield", "sample", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.util.massivesumm_utils.run": [[318, 331], ["multiprocessing.cpu_count", "massivesumm_utils.load_samples", "tqdm.tqdm", "random.sample", "massivesumm_utils.download_list", "downloaded.append", "len", "json.dumps"], "function", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.util.massivesumm_utils.load_samples", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.util.massivesumm_utils.download_list"], ["", "", "", "", "def", "run", "(", "url_file", ":", "str", ")", ":", "\n", "    ", "n_proc", "=", "cpu_count", "(", ")", "\n", "limit", "=", "-", "1", "\n", "\n", "samples", "=", "load_samples", "(", "url_file", ")", "\n", "if", "limit", ">", "0", ":", "\n", "        ", "samples", "=", "random", ".", "sample", "(", "samples", ",", "limit", ")", "\n", "\n", "", "downloaded", "=", "[", "]", "\n", "for", "sample", "in", "tqdm", "(", "download_list", "(", "samples", ",", "n_processes", "=", "n_proc", ")", ",", "total", "=", "len", "(", "samples", ")", ")", ":", "\n", "        ", "downloaded", ".", "append", "(", "json", ".", "dumps", "(", "sample", ")", ")", "\n", "\n", "", "return", "downloaded", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.util.massivesumm_utils.extract": [[333, 383], ["multiprocessing.cpu_count", "set", "print", "print", "print", "orjson.loads", "orjson.loads.get", "set.add", "len", "tqdm.tqdm", "massivesumm_utils.extract.process_batch"], "function", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.infinibatch.closablequeue.ClosableQueue.get", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.transformers.modeling_encoder_decoder.BeamHypotheses.add"], ["", "def", "extract", "(", "archive", ")", ":", "\n", "    ", "n_proc", "=", "cpu_count", "(", ")", "\n", "batch_size", "=", "n_proc", "*", "20", "\n", "\n", "todo", "=", "set", "(", ")", "\n", "\n", "print", "(", "\"Loading downloaded summaries: \"", ",", "end", "=", "\"\"", ")", "\n", "\n", "for", "article", "in", "archive", ":", "\n", "        ", "article", "=", "orjson", ".", "loads", "(", "article", ")", "\n", "url", "=", "article", ".", "get", "(", "\"archive\"", ",", "article", ".", "get", "(", "\"url\"", ")", ")", "\n", "todo", ".", "add", "(", "url", ")", "\n", "\n", "", "print", "(", "\"found\"", ",", "len", "(", "todo", ")", ",", "\"new summaries to extract.\\n\"", ")", "\n", "dataset", "=", "[", "]", "\n", "\n", "with", "tqdm", "(", "total", "=", "len", "(", "todo", ")", ",", "desc", "=", "\"Extracting Summaries\"", ")", "as", "progress", ":", "\n", "\n", "        ", "chunk", "=", "[", "]", "\n", "\n", "def", "process_batch", "(", ")", ":", "\n", "\n", "            ", "with", "Pool", "(", "n_proc", ")", "as", "ex", ":", "\n", "                ", "results", "=", "list", "(", "ex", ".", "map", "(", "Article", ".", "process", ",", "chunk", ")", ")", "\n", "results", "=", "[", "r", "for", "r", "in", "results", "if", "r", "is", "not", "None", "]", "\n", "\n", "for", "result", "in", "results", ":", "\n", "                    ", "if", "result", "[", "\"text\"", "]", "is", "None", "or", "result", "[", "\"summary\"", "]", "is", "None", ":", "\n", "                        ", "continue", "\n", "", "else", ":", "\n", "                        ", "dataset", ".", "append", "(", "json", ".", "dumps", "(", "result", ")", ")", "\n", "\n", "", "", "progress", ".", "update", "(", "len", "(", "results", ")", ")", "\n", "\n", "", "", "for", "article", "in", "archive", ":", "\n", "            ", "article", "=", "orjson", ".", "loads", "(", "article", ")", "\n", "url", "=", "article", ".", "get", "(", "\"archive\"", ",", "article", ".", "get", "(", "\"url\"", ")", ")", "\n", "if", "url", "not", "in", "todo", ":", "\n", "                ", "continue", "\n", "\n", "", "chunk", ".", "append", "(", "article", ")", "\n", "\n", "if", "len", "(", "chunk", ")", ">=", "batch_size", ":", "\n", "                ", "process_batch", "(", ")", "\n", "chunk", "=", "[", "]", "\n", "\n", "", "", "process_batch", "(", ")", "\n", "\n", "", "print", "(", "\"\\nExtraction complete.\"", ")", "\n", "return", "dataset", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.util.massivesumm_utils.massivesumm_extract_from_url": [[385, 391], ["massivesumm_utils.run", "massivesumm_utils.extract"], "function", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.evaluation.model_selector.ModelSelector.run", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.util.massivesumm_utils.extract"], ["", "def", "massivesumm_extract_from_url", "(", "urls", ")", ":", "\n", "\n", "    ", "archive", "=", "run", "(", "urls", ")", "\n", "dataset", "=", "extract", "(", "archive", ")", "\n", "\n", "return", "dataset", "\n", "", ""]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.evaluation.bleu_metric.Bleu.__init__": [[12, 15], ["summ_eval.bleu_metric.BleuMetric", "summertime.evaluation.summeval_metric.SummEvalMetric.__init__"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.tests.model_selector_test.ToyModel.__init__"], ["def", "__init__", "(", "self", ")", ":", "\n", "        ", "se_metric", "=", "BleuMetric", "(", ")", "\n", "super", "(", "Bleu", ",", "self", ")", ".", "__init__", "(", "se_metric", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.evaluation.bleu_metric.Bleu.evaluate": [[16, 25], ["super().evaluate"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.evaluation.meteor_metric.Meteor.evaluate"], ["", "def", "evaluate", "(", "\n", "self", ",", "inputs", ":", "List", "[", "str", "]", ",", "targets", ":", "List", "[", "str", "]", ",", "keys", ":", "List", "[", "str", "]", "=", "[", "\"bleu\"", "]", "\n", ")", "->", "Dict", "[", "str", ",", "float", "]", ":", "\n", "# TODO zhangir: potentially update when dataset api is merged.", "\n", "        ", "result_dict", "=", "super", "(", "Bleu", ",", "self", ")", ".", "evaluate", "(", "inputs", ",", "targets", ",", "keys", ")", "\n", "# making this normalization to make sure everything metric is 0-1", "\n", "result_dict", "[", "\"bleu\"", "]", "=", "result_dict", "[", "\"bleu\"", "]", "/", "100", "\n", "\n", "return", "result_dict", "\n", "", "", ""]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.evaluation.summeval_metric.SummEvalMetric.__init__": [[11, 13], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "se_metric", ":", "SEMetric", ")", ":", "\n", "        ", "self", ".", "se_metric", "=", "se_metric", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.evaluation.summeval_metric.SummEvalMetric.evaluate": [[14, 19], ["summeval_metric.SummEvalMetric.se_metric.evaluate_batch"], "methods", ["None"], ["", "def", "evaluate", "(", "\n", "self", ",", "inputs", ":", "List", "[", "str", "]", ",", "targets", ":", "List", "[", "str", "]", ",", "keys", ":", "List", "[", "str", "]", "\n", ")", "->", "Dict", "[", "str", ",", "float", "]", ":", "\n", "        ", "score_dict", "=", "self", ".", "se_metric", ".", "evaluate_batch", "(", "inputs", ",", "targets", ")", "\n", "return", "{", "key", ":", "score_dict", "[", "key", "]", "if", "key", "in", "score_dict", "else", "None", "for", "key", "in", "keys", "}", "\n", "", "", ""]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.evaluation.bertscore_metric.BertScore.__init__": [[12, 15], ["summ_eval.bert_score_metric.BertScoreMetric", "summertime.evaluation.summeval_metric.SummEvalMetric.__init__"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.tests.model_selector_test.ToyModel.__init__"], ["def", "__init__", "(", "self", ")", ":", "\n", "        ", "se_metric", "=", "BertScoreMetric", "(", ")", "\n", "super", "(", "BertScore", ",", "self", ")", ".", "__init__", "(", "se_metric", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.evaluation.bertscore_metric.BertScore.evaluate": [[16, 21], ["super().evaluate"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.evaluation.meteor_metric.Meteor.evaluate"], ["", "def", "evaluate", "(", "\n", "self", ",", "inputs", ":", "List", "[", "str", "]", ",", "targets", ":", "List", "[", "str", "]", ",", "keys", ":", "List", "[", "str", "]", "=", "[", "\"bert_score_f1\"", "]", "\n", ")", "->", "Dict", "[", "str", ",", "float", "]", ":", "\n", "# TODO zhangir: update when datasets api is merged", "\n", "        ", "return", "super", "(", "BertScore", ",", "self", ")", ".", "evaluate", "(", "inputs", ",", "targets", ",", "keys", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.evaluation.base_metric.SummMetric.evaluate": [[10, 27], ["NotImplementedError"], "methods", ["None"], ["def", "evaluate", "(", "\n", "self", ",", "\n", "# TODO zhangir: integrate with dataset api", "\n", "inputs", ":", "List", "[", "str", "]", ",", "\n", "targets", ":", "List", "[", "str", "]", ",", "\n", "keys", ":", "List", "[", "str", "]", ",", "\n", ")", "->", "Dict", "[", "str", ",", "float", "]", ":", "\n", "        ", "\"\"\"\n        All metrics should have this function.\n        :input: A list of summaries.\n        :target: A list of target summaries corresponding to each entry of input.\n        :keys: Which metrics to return,\n        e.g, ['rouge_1_f_score', 'rouge_2_f_score']\n        :return: A dictionary with keys metrics and values scores.\n        \"\"\"", "\n", "raise", "NotImplementedError", "(", "\n", "\"the base class for metrics shouldn't be instantiated!\"", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.evaluation.model_selector.EvaluationTable.__init__": [[13, 15], ["dict.__init__"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.tests.model_selector_test.ToyModel.__init__"], ["    ", "def", "__init__", "(", "self", ",", "*", "args", ",", "**", "kw", ")", ":", "\n", "        ", "super", "(", "EvaluationTable", ",", "self", ")", ".", "__init__", "(", "*", "args", ",", "**", "kw", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.evaluation.model_selector.EvaluationTable.__str__": [[16, 25], ["prettytable.PrettyTable", "list", "prettytable.PrettyTable.__str__", "model_selector.EvaluationTable.keys", "prettytable.PrettyTable.add_row", "list", "model_selector.EvaluationTable.keys"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.evaluation.model_selector.EvaluationTable.__str__"], ["", "def", "__str__", "(", "self", ")", ":", "\n", "        ", "out", "=", "PrettyTable", "(", ")", "\n", "metrics", "=", "list", "(", "self", "[", "list", "(", "self", ".", "keys", "(", ")", ")", "[", "0", "]", "]", ".", "keys", "(", ")", ")", "\n", "out", ".", "field_names", "=", "[", "\"Model\"", "]", "+", "metrics", "\n", "for", "model_name", "in", "self", ":", "\n", "            ", "to_add", "=", "[", "model_name", "]", "+", "[", "self", "[", "model_name", "]", "[", "metric", "]", "for", "metric", "in", "metrics", "]", "\n", "out", ".", "add_row", "(", "to_add", ")", "\n", "", "out", ".", "float_format", "=", "\".3\"", "\n", "return", "out", ".", "__str__", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.evaluation.model_selector.EvaluationTable.__repr__": [[26, 28], ["model_selector.EvaluationTable.__str__"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.evaluation.model_selector.EvaluationTable.__str__"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "__str__", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.evaluation.model_selector.ModelSelector.__init__": [[31, 47], ["itertools.islice"], "methods", ["None"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "models", ":", "List", "[", "SummModel", "]", ",", "\n", "generator", ":", "Generator", "[", "SummInstance", ",", "None", ",", "None", "]", ",", "\n", "metrics", ":", "List", "[", "SummMetric", "]", ",", "\n", "max_instances", ":", "int", "=", "-", "1", ",", "\n", ")", ":", "\n", "\n", "        ", "self", ".", "models", "=", "models", "\n", "\n", "if", "max_instances", "==", "-", "1", ":", "\n", "            ", "self", ".", "generator", "=", "generator", "\n", "", "else", ":", "\n", "            ", "self", ".", "generator", "=", "itertools", ".", "islice", "(", "generator", ",", "max_instances", ")", "\n", "\n", "", "self", ".", "metrics", "=", "metrics", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.evaluation.model_selector.ModelSelector.run": [[48, 85], ["model_selector.EvaluationTable", "list", "itertools.tee", "metric.evaluate", "list.pop", "len", "len", "model.summarize", "metric.evaluate"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.evaluation.meteor_metric.Meteor.evaluate", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.tests.model_selector_test.ToyModel.summarize", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.evaluation.meteor_metric.Meteor.evaluate"], ["", "def", "run", "(", "self", ")", "->", "EvaluationTable", ":", "\n", "        ", "\"\"\"Evaluates every model on every metric, returning an EvaluationTable\"\"\"", "\n", "store_data", "=", "EvaluationTable", "(", ")", "\n", "\n", "tiny_generators", "=", "list", "(", "\n", "itertools", ".", "tee", "(", "self", ".", "generator", ",", "len", "(", "self", ".", "models", ")", "*", "len", "(", "self", ".", "metrics", ")", ")", "\n", ")", "\n", "\n", "for", "model", "in", "self", ".", "models", ":", "\n", "            ", "store_data", "[", "model", ".", "model_name", "]", "=", "{", "}", "\n", "\n", "for", "metric", "in", "self", ".", "metrics", ":", "\n", "# TODO: make default keys a class variable", "\n", "                ", "get_keys", "=", "metric", ".", "evaluate", "(", "[", "\"test\"", "]", ",", "[", "\"test\"", "]", ")", "\n", "# used for averaging metric across examples", "\n", "sum_score_dict", "=", "{", "key", ":", "0", "for", "key", "in", "get_keys", "}", "\n", "num_instances", "=", "0", "\n", "\n", "current_generator", "=", "tiny_generators", ".", "pop", "(", ")", "\n", "for", "instance", "in", "current_generator", ":", "\n", "                    ", "input", "=", "model", ".", "summarize", "(", "[", "instance", ".", "source", "]", ")", "\n", "score_dict", "=", "metric", ".", "evaluate", "(", "input", ",", "[", "instance", ".", "summary", "]", ")", "\n", "sum_score_dict", "=", "{", "\n", "key", ":", "sum_score_dict", "[", "key", "]", "+", "score_dict", "[", "key", "]", "\n", "for", "key", "in", "sum_score_dict", "\n", "}", "\n", "\n", "num_instances", "+=", "1", "\n", "\n", "", "avg_score_dict", "=", "{", "\n", "key", ":", "sum_score_dict", "[", "key", "]", "/", "num_instances", "for", "key", "in", "sum_score_dict", "\n", "}", "\n", "\n", "for", "key", "in", "avg_score_dict", ":", "\n", "                    ", "store_data", "[", "model", ".", "model_name", "]", "[", "key", "]", "=", "avg_score_dict", "[", "key", "]", "\n", "\n", "", "", "", "return", "store_data", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.evaluation.model_selector.ModelSelector.run_halving": [[86, 115], ["itertools.islice", "model_selector.ModelSelector", "model_selector.ModelSelector.run", "model_selector._remove_bad_model", "len", "itertools.islice", "model_selector.ModelSelector", "model_selector.ModelSelector.run", "model_selector._update_table", "model_selector._remove_bad_model"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.evaluation.model_selector.ModelSelector.run", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.evaluation.model_selector._remove_bad_model", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.evaluation.model_selector.ModelSelector.run", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.evaluation.model_selector._update_table", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.evaluation.model_selector._remove_bad_model"], ["", "def", "run_halving", "(", "self", ",", "min_instances", ":", "int", ",", "factor", ":", "int", "=", "3", ")", "->", "EvaluationTable", ":", "\n", "        ", "models", "=", "self", ".", "models", "\n", "\n", "total_instances", "=", "0", "\n", "# first run with min_instances instances", "\n", "num_instances", "=", "min_instances", "\n", "tiny_generator", "=", "itertools", ".", "islice", "(", "self", ".", "generator", ",", "min_instances", ")", "\n", "\n", "temp_selector", "=", "ModelSelector", "(", "self", ".", "models", ",", "tiny_generator", ",", "self", ".", "metrics", ")", "\n", "table", "=", "temp_selector", ".", "run", "(", ")", "\n", "\n", "models", "=", "_remove_bad_model", "(", "self", ".", "models", ",", "table", ")", "\n", "\n", "total_instances", "+=", "num_instances", "\n", "\n", "num_instances", "=", "num_instances", "*", "factor", "\n", "\n", "while", "len", "(", "models", ")", ">", "1", ":", "\n", "            ", "tiny_generator", "=", "itertools", ".", "islice", "(", "self", ".", "generator", ",", "num_instances", ")", "\n", "temp_selector", "=", "ModelSelector", "(", "models", ",", "tiny_generator", ",", "self", ".", "metrics", ")", "\n", "new_table", "=", "temp_selector", ".", "run", "(", ")", "\n", "table", "=", "_update_table", "(", "table", ",", "new_table", ",", "total_instances", ",", "num_instances", ")", "\n", "\n", "models", "=", "_remove_bad_model", "(", "models", ",", "new_table", ")", "\n", "\n", "total_instances", "+=", "num_instances", "\n", "num_instances", "=", "num_instances", "*", "factor", "\n", "\n", "", "return", "table", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.evaluation.model_selector.ModelSelector.visualize": [[116, 129], ["list", "data.append", "data.append", "plotutils.radar.make_radar_plot", "output[].keys", "rows.append", "row_names.append", "list", "output.keys"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.plotutils.radar.make_radar_plot"], ["", "def", "visualize", "(", "self", ",", "output", ":", "EvaluationTable", ")", ":", "\n", "# Preprocesses data.", "\n", "        ", "data", "=", "[", "]", "\n", "metrics", "=", "list", "(", "output", "[", "list", "(", "output", ".", "keys", "(", ")", ")", "[", "0", "]", "]", ".", "keys", "(", ")", ")", "\n", "data", ".", "append", "(", "metrics", ")", "\n", "rows", "=", "[", "]", "\n", "row_names", "=", "[", "]", "\n", "for", "model", "in", "output", ":", "\n", "            ", "rows", ".", "append", "(", "[", "output", "[", "model", "]", "[", "metric", "]", "for", "metric", "in", "metrics", "]", ")", "\n", "row_names", ".", "append", "(", "model", ")", "\n", "", "data", ".", "append", "(", "rows", ")", "\n", "\n", "return", "make_radar_plot", "(", "data", ",", "row_names", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.evaluation.model_selector._update_table": [[131, 146], ["None"], "function", ["None"], ["", "", "def", "_update_table", "(", "\n", "table", ":", "EvaluationTable", ",", "\n", "new_table", ":", "EvaluationTable", ",", "\n", "total_instances", ":", "int", ",", "\n", "num_instances", ":", "int", ",", "\n", ")", "->", "EvaluationTable", ":", "\n", "    ", "\"\"\"Merges df1 and df2\"\"\"", "\n", "for", "model", "in", "new_table", ":", "\n", "        ", "for", "metric", "in", "new_table", "[", "model", "]", ":", "\n", "            ", "denom", "=", "total_instances", "+", "num_instances", "\n", "table", "[", "model", "]", "[", "metric", "]", "=", "(", "\n", "total_instances", "/", "denom", "*", "table", "[", "model", "]", "[", "metric", "]", "\n", "+", "num_instances", "/", "denom", "*", "new_table", "[", "model", "]", "[", "metric", "]", "\n", ")", "\n", "", "", "return", "table", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.evaluation.model_selector._remove_bad_model": [[148, 172], ["range", "functools.reduce", "len", "models.pop"], "function", ["None"], ["", "def", "_remove_bad_model", "(", "models", ":", "List", "[", "SummModel", "]", ",", "table", ":", "EvaluationTable", ")", ":", "\n", "    ", "\"\"\"Removes a model's row from the dataframe if it is worse than every other model\n    on every metric\"\"\"", "\n", "name", "=", "None", "\n", "for", "model", "in", "table", ":", "\n", "        ", "cumulative_and", "=", "1", "\n", "for", "other", "in", "table", ":", "\n", "            ", "cumulative_and", "*=", "reduce", "(", "\n", "operator", ".", "mul", ",", "\n", "[", "\n", "1", "if", "table", "[", "model", "]", "[", "metric", "]", "<=", "table", "[", "other", "]", "[", "metric", "]", "else", "0", "\n", "for", "metric", "in", "table", "[", "model", "]", "\n", "]", ",", "\n", "1", ",", "\n", ")", "\n", "", "if", "cumulative_and", "==", "1", ":", "\n", "            ", "name", "=", "model", "\n", "", "", "if", "name", ":", "\n", "        ", "for", "i", "in", "range", "(", "len", "(", "models", ")", ")", ":", "\n", "            ", "if", "models", "[", "i", "]", ".", "model_name", "==", "name", ":", "\n", "                ", "models", ".", "pop", "(", "i", ")", "\n", "return", "models", "\n", "", "", "", "else", ":", "\n", "        ", "return", "models", "\n", "", "", ""]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.evaluation.rougewe_metric.RougeWe.__init__": [[13, 19], ["nltk.download", "RougeWeMetric", "summertime.evaluation.summeval_metric.SummEvalMetric.__init__"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.tests.model_selector_test.ToyModel.__init__"], ["def", "__init__", "(", "self", ")", ":", "\n", "        ", "from", "summ_eval", ".", "rouge_we_metric", "import", "RougeWeMetric", "\n", "\n", "nltk", ".", "download", "(", "\"stopwords\"", ")", "\n", "se_metric", "=", "RougeWeMetric", "(", ")", "\n", "super", "(", "RougeWe", ",", "self", ")", ".", "__init__", "(", "se_metric", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.evaluation.rougewe_metric.RougeWe.evaluate": [[20, 25], ["super().evaluate"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.evaluation.meteor_metric.Meteor.evaluate"], ["", "def", "evaluate", "(", "\n", "self", ",", "inputs", ":", "List", "[", "str", "]", ",", "targets", ":", "List", "[", "str", "]", ",", "keys", ":", "List", "[", "str", "]", "=", "[", "\"rouge_we_3_f\"", "]", "\n", ")", "->", "Dict", "[", "str", ",", "float", "]", ":", "\n", "# TODO zhangir: update when dataset api is merged.", "\n", "        ", "return", "super", "(", "RougeWe", ",", "self", ")", ".", "evaluate", "(", "inputs", ",", "targets", ",", "keys", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.evaluation.rouge_metric.Rouge.__init__": [[12, 15], ["summ_eval.rouge_metric.RougeMetric", "summertime.evaluation.summeval_metric.SummEvalMetric.__init__"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.tests.model_selector_test.ToyModel.__init__"], ["def", "__init__", "(", "self", ")", ":", "\n", "        ", "se_metric", "=", "RougeMetric", "(", ")", "\n", "super", "(", "Rouge", ",", "self", ")", ".", "__init__", "(", "se_metric", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.evaluation.rouge_metric.Rouge.evaluate": [[16, 24], ["rouge_metric.Rouge.se_metric.evaluate_batch"], "methods", ["None"], ["", "def", "evaluate", "(", "\n", "self", ",", "\n", "inputs", ":", "List", "[", "str", "]", ",", "\n", "targets", ":", "List", "[", "str", "]", ",", "\n", "keys", ":", "List", "[", "str", "]", "=", "[", "\"rouge_1_f_score\"", ",", "\"rouge_2_f_score\"", ",", "\"rouge_l_f_score\"", "]", ",", "\n", ")", "->", "Dict", "[", "str", ",", "float", "]", ":", "\n", "        ", "score_dict", "=", "self", ".", "se_metric", ".", "evaluate_batch", "(", "inputs", ",", "targets", ")", "\n", "return", "{", "key", ":", "score_dict", "[", "\"rouge\"", "]", "[", "key", "]", "for", "key", "in", "keys", "}", "\n", "", "", ""]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.evaluation.error_viz.scatter": [[9, 87], ["itertools.islice", "matplotlib.figure", "plt.figure.add_axes", "fig.add_axes.set_xlim", "fig.add_axes.set_ylim", "fig.add_axes.spines[].set_visible", "fig.add_axes.spines[].set_visible", "fig.add_axes.spines[].set_visible", "fig.add_axes.spines[].set_visible", "fig.add_axes.scatter", "fig.add_axes.scatter", "fig.add_axes.legend", "matplotlib.xlabel", "matplotlib.ylabel", "fig.add_axes.text", "fig.add_axes.text", "fig.add_axes.text", "fig.add_axes.text", "matplotlib.savefig", "matplotlib.show", "models[].summarize", "models[].summarize", "model0_lexical.append", "model1_lexical.append", "model0_semantic.append", "model1_semantic.append", "lexical_metric.evaluate", "lexical_metric.evaluate", "semantic_metric.evaluate", "semantic_metric.evaluate"], "function", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.evaluation.error_viz.scatter", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.evaluation.error_viz.scatter", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.tests.model_selector_test.ToyModel.summarize", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.tests.model_selector_test.ToyModel.summarize", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.evaluation.meteor_metric.Meteor.evaluate", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.evaluation.meteor_metric.Meteor.evaluate", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.evaluation.meteor_metric.Meteor.evaluate", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.evaluation.meteor_metric.Meteor.evaluate"], ["def", "scatter", "(", "\n", "models", ":", "Tuple", "[", "SummModel", ",", "SummModel", "]", ",", "\n", "generator", ":", "Generator", "[", "SummInstance", ",", "None", ",", "None", "]", ",", "\n", "metrics", ":", "Tuple", "[", "SummMetric", ",", "SummMetric", "]", ",", "\n", "keys", ":", "Tuple", "[", "str", ",", "str", "]", ",", "\n", "max_instances", ":", "int", "=", "-", "1", ",", "\n", ")", ":", "\n", "    ", "\"\"\"\n    Scatter plot that compares the qualitative nature of the errors two models\n    are making.\n    models: Tuple[SummModel, SummModel]\n    generator: Generator[SummInstance]\n    metrics: Tuple[SummMetric, SummMetric]\n    keys: Tuple[str, str]\n    max_instances: Defaults to -1, in which case entire dataset is used.\n    \"\"\"", "\n", "\n", "lexical_metric", "=", "metrics", "[", "0", "]", "\n", "semantic_metric", "=", "metrics", "[", "1", "]", "\n", "lexical_key", "=", "keys", "[", "0", "]", "\n", "semantic_key", "=", "keys", "[", "1", "]", "\n", "\n", "tiny_generator", "=", "itertools", ".", "islice", "(", "generator", ",", "max_instances", ")", "\n", "\n", "model0_lexical", "=", "[", "]", "\n", "model1_lexical", "=", "[", "]", "\n", "model0_semantic", "=", "[", "]", "\n", "model1_semantic", "=", "[", "]", "\n", "for", "instance", "in", "tiny_generator", ":", "\n", "        ", "model0_summ", "=", "models", "[", "0", "]", ".", "summarize", "(", "[", "instance", ".", "source", "]", ")", "\n", "model1_summ", "=", "models", "[", "1", "]", ".", "summarize", "(", "[", "instance", ".", "source", "]", ")", "\n", "\n", "summary", "=", "[", "instance", ".", "summary", "]", "\n", "\n", "model0_lexical", ".", "append", "(", "\n", "lexical_metric", ".", "evaluate", "(", "model0_summ", ",", "summary", ")", "[", "lexical_key", "]", "\n", ")", "\n", "\n", "model1_lexical", ".", "append", "(", "\n", "lexical_metric", ".", "evaluate", "(", "model1_summ", ",", "summary", ")", "[", "lexical_key", "]", "\n", ")", "\n", "\n", "model0_semantic", ".", "append", "(", "\n", "semantic_metric", ".", "evaluate", "(", "model0_summ", ",", "summary", ")", "[", "semantic_key", "]", "\n", ")", "\n", "\n", "model1_semantic", ".", "append", "(", "\n", "semantic_metric", ".", "evaluate", "(", "model1_summ", ",", "summary", ")", "[", "semantic_key", "]", "\n", ")", "\n", "\n", "", "fig", "=", "plt", ".", "figure", "(", "frameon", "=", "False", ")", "\n", "ax", "=", "fig", ".", "add_axes", "(", "[", "0", ",", "0", ",", "1", ",", "1", "]", ")", "\n", "ax", ".", "set_xlim", "(", "[", "0", ",", "1", "]", ")", "\n", "ax", ".", "set_ylim", "(", "[", "0", ",", "1", "]", ")", "\n", "ax", ".", "spines", "[", "\"top\"", "]", ".", "set_visible", "(", "False", ")", "\n", "ax", ".", "spines", "[", "\"right\"", "]", ".", "set_visible", "(", "False", ")", "\n", "ax", ".", "spines", "[", "\"left\"", "]", ".", "set_visible", "(", "False", ")", "\n", "ax", ".", "spines", "[", "\"bottom\"", "]", ".", "set_visible", "(", "False", ")", "\n", "\n", "ax", ".", "scatter", "(", "model0_lexical", ",", "model0_semantic", ",", "label", "=", "models", "[", "0", "]", ".", "model_name", ")", "\n", "ax", ".", "scatter", "(", "model1_lexical", ",", "model1_semantic", ",", "label", "=", "models", "[", "1", "]", ".", "model_name", ")", "\n", "ax", ".", "legend", "(", "loc", "=", "(", "1.2", ",", "0.5", ")", ")", "\n", "\n", "plt", ".", "xlabel", "(", "\n", "\"Lexical ({})\"", ".", "format", "(", "lexical_metric", ".", "metric_name", ")", ",", "fontsize", "=", "12", ",", "color", "=", "\"grey\"", "\n", ")", "\n", "plt", ".", "ylabel", "(", "\n", "\"Semantic ({})\"", ".", "format", "(", "semantic_metric", ".", "metric_name", ")", ",", "fontsize", "=", "12", ",", "color", "=", "\"grey\"", "\n", ")", "\n", "\n", "ax", ".", "text", "(", "-", "0.3", ",", "-", "0.2", ",", "\"Hallucination\"", ",", "fontsize", "=", "15", ")", "\n", "ax", ".", "text", "(", "-", "0.3", ",", "1.2", ",", "\"Abstraction\"", ",", "fontsize", "=", "15", ")", "\n", "ax", ".", "text", "(", "1", ",", "1.2", ",", "\"Extraction\"", ",", "fontsize", "=", "15", ")", "\n", "ax", ".", "text", "(", "1", ",", "-", "0.2", ",", "\"Misinterpretation\"", ",", "fontsize", "=", "15", ")", "\n", "\n", "plt", ".", "savefig", "(", "\"scatter.pdf\"", ",", "bbox_inches", "=", "\"tight\"", ")", "\n", "\n", "plt", ".", "show", "(", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.evaluation.meteor_metric.Meteor.__init__": [[14, 16], ["nltk.download"], "methods", ["None"], ["def", "__init__", "(", "self", ")", ":", "\n", "        ", "nltk", ".", "download", "(", "\"wordnet\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.evaluation.meteor_metric.Meteor.evaluate": [[17, 32], ["statistics.mean", "nltk.translate.meteor_score.meteor_score", "KeyError", "zip"], "methods", ["None"], ["", "def", "evaluate", "(", "\n", "self", ",", "inputs", ":", "List", "[", "str", "]", ",", "targets", ":", "List", "[", "str", "]", ",", "keys", "=", "[", "\"meteor\"", "]", "\n", ")", "->", "Dict", "[", "str", ",", "float", "]", ":", "\n", "\n", "        ", "for", "key", "in", "keys", ":", "\n", "            ", "if", "key", "!=", "\"meteor\"", ":", "\n", "                ", "raise", "KeyError", "(", "key", ",", "\"is not a valid key\"", ")", "\n", "\n", "", "", "meteor_scores", "=", "[", "\n", "nltk_meteor", ".", "meteor_score", "(", "[", "input", "]", ",", "target", ")", "\n", "for", "input", ",", "target", "in", "zip", "(", "inputs", ",", "targets", ")", "\n", "]", "\n", "meteor_score", "=", "statistics", ".", "mean", "(", "meteor_scores", ")", "\n", "\n", "return", "{", "key", ":", "meteor_score", "for", "key", "in", "keys", "}", "\n", "", "", ""]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.plotutils.radar.radar_factory": [[12, 94], ["numpy.linspace", "matplotlib.projections.register_projection", "super().__init__", "radar..set_theta_zero_location", "super().fill", "super().plot", "line.get_data", "radar..set_thetagrids", "radar.._close_line", "numpy.append", "numpy.append", "line.set_data", "numpy.degrees", "matplotlib.patches.Circle", "super()._gen_axes_spines", "matplotlib.patches.RegularPolygon", "ValueError", "matplotlib.spines.Spine", "matplotlib.spines.Spine.set_transform", "ValueError", "matplotlib.path.Path.unit_regular_polygon", "matplotlib.transforms.Affine2D().scale().translate", "matplotlib.transforms.Affine2D().scale", "matplotlib.transforms.Affine2D"], "function", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.tests.model_selector_test.ToyModel.__init__"], ["def", "radar_factory", "(", "num_vars", ",", "frame", "=", "\"circle\"", ")", ":", "\n", "    ", "\"\"\"\n    Create a radar chart with `num_vars` axes.\n\n    This function creates a RadarAxes projection and registers it.\n\n    Parameters\n    ----------\n    num_vars : int\n        Number of variables for radar chart.\n    frame : {'circle', 'polygon'}\n        Shape of frame surrounding axes.\n\n    Adapted from https://matplotlib.org/stable/gallery/specialty_plots/radar_chart.html\n\n    \"\"\"", "\n", "# calculate evenly-spaced axis angles", "\n", "theta", "=", "np", ".", "linspace", "(", "0", ",", "2", "*", "np", ".", "pi", ",", "num_vars", ",", "endpoint", "=", "False", ")", "\n", "\n", "class", "RadarAxes", "(", "PolarAxes", ")", ":", "\n", "\n", "        ", "name", "=", "\"radar\"", "\n", "# use 1 line segment to connect specified points", "\n", "RESOLUTION", "=", "1", "\n", "\n", "def", "__init__", "(", "self", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "            ", "super", "(", ")", ".", "__init__", "(", "*", "args", ",", "**", "kwargs", ")", "\n", "# rotate plot such that the first axis is at the top", "\n", "self", ".", "set_theta_zero_location", "(", "\"N\"", ")", "\n", "\n", "", "def", "fill", "(", "self", ",", "*", "args", ",", "closed", "=", "True", ",", "**", "kwargs", ")", ":", "\n", "            ", "\"\"\"Override fill so that line is closed by default\"\"\"", "\n", "return", "super", "(", ")", ".", "fill", "(", "closed", "=", "closed", ",", "*", "args", ",", "**", "kwargs", ")", "\n", "\n", "", "def", "plot", "(", "self", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "            ", "\"\"\"Override plot so that line is closed by default\"\"\"", "\n", "lines", "=", "super", "(", ")", ".", "plot", "(", "*", "args", ",", "**", "kwargs", ")", "\n", "for", "line", "in", "lines", ":", "\n", "                ", "self", ".", "_close_line", "(", "line", ")", "\n", "\n", "", "", "def", "_close_line", "(", "self", ",", "line", ")", ":", "\n", "            ", "x", ",", "y", "=", "line", ".", "get_data", "(", ")", "\n", "if", "x", "[", "0", "]", "!=", "x", "[", "-", "1", "]", ":", "\n", "                ", "x", "=", "np", ".", "append", "(", "x", ",", "x", "[", "0", "]", ")", "\n", "y", "=", "np", ".", "append", "(", "y", ",", "y", "[", "0", "]", ")", "\n", "line", ".", "set_data", "(", "x", ",", "y", ")", "\n", "\n", "", "", "def", "set_varlabels", "(", "self", ",", "labels", ")", ":", "\n", "            ", "self", ".", "set_thetagrids", "(", "np", ".", "degrees", "(", "theta", ")", ",", "labels", ")", "\n", "\n", "", "def", "_gen_axes_patch", "(", "self", ")", ":", "\n", "# The Axes patch must be centered at (0.5, 0.5) and of radius 0.5", "\n", "# in axes coordinates.", "\n", "            ", "if", "frame", "==", "\"circle\"", ":", "\n", "                ", "return", "Circle", "(", "(", "0.5", ",", "0.5", ")", ",", "0.5", ")", "\n", "", "elif", "frame", "==", "\"polygon\"", ":", "\n", "                ", "return", "RegularPolygon", "(", "(", "0.5", ",", "0.5", ")", ",", "num_vars", ",", "radius", "=", "0.5", ",", "edgecolor", "=", "\"k\"", ")", "\n", "", "else", ":", "\n", "                ", "raise", "ValueError", "(", "\"Unknown value for 'frame': %s\"", "%", "frame", ")", "\n", "\n", "", "", "def", "_gen_axes_spines", "(", "self", ")", ":", "\n", "            ", "if", "frame", "==", "\"circle\"", ":", "\n", "                ", "return", "super", "(", ")", ".", "_gen_axes_spines", "(", ")", "\n", "", "elif", "frame", "==", "\"polygon\"", ":", "\n", "# spine_type must be 'left'/'right'/'top'/'bottom'/'circle'.", "\n", "                ", "spine", "=", "Spine", "(", "\n", "axes", "=", "self", ",", "\n", "spine_type", "=", "\"circle\"", ",", "\n", "path", "=", "Path", ".", "unit_regular_polygon", "(", "num_vars", ")", ",", "\n", ")", "\n", "# unit_regular_polygon gives a polygon of radius 1 centered at", "\n", "# (0, 0) but we want a polygon of radius 0.5 centered at (0.5,", "\n", "# 0.5) in axes coordinates.", "\n", "spine", ".", "set_transform", "(", "\n", "Affine2D", "(", ")", ".", "scale", "(", "0.5", ")", ".", "translate", "(", "0.5", ",", "0.5", ")", "+", "self", ".", "transAxes", "\n", ")", "\n", "return", "{", "\"polar\"", ":", "spine", "}", "\n", "", "else", ":", "\n", "                ", "raise", "ValueError", "(", "\"Unknown value for 'frame': %s\"", "%", "frame", ")", "\n", "\n", "", "", "", "register_projection", "(", "RadarAxes", ")", "\n", "return", "theta", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.plotutils.radar.make_radar_plot": [[96, 124], ["len", "radar.radar_factory", "data.pop", "matplotlib.subplots", "fig.subplots_adjust", "axs.set_rgrids", "zip", "axs.set_varlabels", "tuple", "axs.legend", "axs.plot", "axs.fill", "dict"], "function", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.plotutils.radar.radar_factory"], ["", "def", "make_radar_plot", "(", "data", ",", "row_names", ")", ":", "\n", "    ", "\"\"\"\n    Format data as a two-item list, consisting of a list of metric names\n    and a list of entries for each metric.\n    \"\"\"", "\n", "N", "=", "len", "(", "data", "[", "0", "]", ")", "\n", "theta", "=", "radar_factory", "(", "N", ",", "frame", "=", "\"polygon\"", ")", "\n", "\n", "spoke_labels", "=", "data", ".", "pop", "(", "0", ")", "\n", "\n", "fig", ",", "axs", "=", "plt", ".", "subplots", "(", "\n", "figsize", "=", "(", "9", ",", "9", ")", ",", "nrows", "=", "1", ",", "ncols", "=", "1", ",", "subplot_kw", "=", "dict", "(", "projection", "=", "\"radar\"", ")", "\n", ")", "\n", "fig", ".", "subplots_adjust", "(", "wspace", "=", "0.25", ",", "hspace", "=", "0.20", ",", "top", "=", "0.85", ",", "bottom", "=", "0.05", ")", "\n", "\n", "colors", "=", "[", "\"b\"", ",", "\"r\"", ",", "\"g\"", ",", "\"m\"", ",", "\"y\"", "]", "*", "(", "N", "%", "5", ")", "\n", "\n", "case_data", "=", "data", "[", "0", "]", "\n", "axs", ".", "set_rgrids", "(", "[", "0.2", ",", "0.4", ",", "0.6", ",", "0.8", "]", ")", "\n", "for", "d", ",", "color", "in", "zip", "(", "case_data", ",", "colors", ")", ":", "\n", "        ", "axs", ".", "plot", "(", "theta", ",", "d", ",", "color", "=", "color", ")", "\n", "axs", ".", "fill", "(", "theta", ",", "d", ",", "facecolor", "=", "color", ",", "alpha", "=", "0.25", ")", "\n", "", "axs", ".", "set_varlabels", "(", "spoke_labels", ")", "\n", "\n", "labels", "=", "tuple", "(", "row_names", ")", "\n", "axs", ".", "legend", "(", "labels", ",", "loc", "=", "(", "0.9", ",", "0.95", ")", ",", "labelspacing", "=", "0.1", ",", "fontsize", "=", "\"large\"", ")", "\n", "\n", "return", "fig", "\n", "", ""]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.tests.dataset_test.TestDatasets._test_instance": [[45, 58], ["dataset_test.TestDatasets.assertTrue", "dataset_test.TestDatasets.assertTrue", "dataset_test.TestDatasets.assertTrue", "isinstance", "isinstance", "isinstance", "isinstance"], "methods", ["None"], ["    ", "def", "_test_instance", "(", "\n", "self", ",", "\n", "ins", ":", "SummInstance", ",", "\n", "is_query", ":", "bool", "=", "False", ",", "\n", "is_multi_document", ":", "bool", "=", "False", ",", "\n", "is_dialogue", ":", "bool", "=", "False", ",", "\n", ")", ":", "\n", "        ", "if", "is_multi_document", "or", "is_dialogue", ":", "\n", "            ", "self", ".", "assertTrue", "(", "isinstance", "(", "ins", ".", "source", ",", "list", ")", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "assertTrue", "(", "isinstance", "(", "ins", ".", "source", ",", "list", ")", "or", "isinstance", "(", "ins", ".", "source", ",", "str", ")", ")", "\n", "", "if", "is_query", ":", "\n", "            ", "self", ".", "assertTrue", "(", "isinstance", "(", "ins", ".", "query", ",", "str", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.tests.dataset_test.TestDatasets.test_all_datasets": [[59, 160], ["helpers.print_with_color", "summertime.dataset.st_dataset.CustomDataset", "helpers.print_with_color", "print", "helpers.print_with_color", "summertime.dataset.list_all_datasets", "helpers.print_with_color", "dataset_test.get_dummy_data_with_query", "dataset_test.get_dummy_data_with_query", "dataset_test.get_dummy_data_with_query", "list", "print", "dataset_test.TestDatasets._test_instance", "list", "print", "dataset_test.TestDatasets._test_instance", "list", "print", "dataset_test.TestDatasets._test_instance", "helpers.print_with_color", "ds_cls", "isinstance", "ds_cls", "helpers.print_with_color", "ds_cls", "ds_cls.show_description", "len", "len", "len"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.tests.helpers.print_with_color", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.tests.helpers.print_with_color", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.tests.helpers.print_with_color", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.dataset.__init__.list_all_datasets", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.tests.helpers.print_with_color", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.tests.dataset_test.get_dummy_data_with_query", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.tests.dataset_test.get_dummy_data_with_query", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.tests.dataset_test.get_dummy_data_with_query", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.tests.dataset_test.TestDatasets._test_instance", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.tests.dataset_test.TestDatasets._test_instance", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.tests.dataset_test.TestDatasets._test_instance", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.tests.helpers.print_with_color", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.tests.helpers.print_with_color", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.dataset.st_dataset.SummDataset.show_description"], ["", "", "def", "test_all_datasets", "(", "self", ")", ":", "\n", "\n", "# Test custom dataset", "\n", "        ", "print_with_color", "(", "f\"{'#' * 10} Loading custom dataset... {'#' * 10}\\n\\n\"", ",", "\"35\"", ")", "\n", "\n", "train_set", "=", "[", "\n", "{", "\n", "\"source\"", ":", "instance", "[", "0", "]", ",", "\n", "\"summary\"", ":", "instance", "[", "1", "]", ",", "\n", "\"query\"", ":", "instance", "[", "2", "]", ",", "\n", "}", "\n", "for", "instance", "in", "get_dummy_data_with_query", "(", "NUM_DUMMY_DATA_INSTANCES", ")", "\n", "]", "\n", "validation_set", "=", "[", "\n", "{", "\n", "\"source\"", ":", "instance", "[", "0", "]", ",", "\n", "\"summary\"", ":", "None", ",", "\n", "\"query\"", ":", "instance", "[", "2", "]", ",", "\n", "}", "\n", "for", "instance", "in", "get_dummy_data_with_query", "(", "NUM_DUMMY_DATA_INSTANCES", ")", "\n", "]", "\n", "test_set", "=", "[", "\n", "{", "\n", "\"source\"", ":", "instance", "[", "0", "]", ",", "\n", "\"summary\"", ":", "instance", "[", "1", "]", ",", "\n", "\"query\"", ":", "instance", "[", "2", "]", ",", "\n", "}", "\n", "for", "instance", "in", "get_dummy_data_with_query", "(", "NUM_DUMMY_DATA_INSTANCES", ")", "\n", "]", "\n", "\n", "custom_dataset", "=", "CustomDataset", "(", "\n", "train_set", "=", "train_set", ",", "\n", "validation_set", "=", "validation_set", ",", "\n", "test_set", "=", "test_set", ",", "\n", "query_based", "=", "True", ",", "\n", "multi_doc", "=", "False", ",", "\n", ")", "\n", "\n", "test_datasets", "=", "[", "custom_dataset", "]", "+", "SUPPORTED_SUMM_DATASETS", "\n", "\n", "# Test pre-loaded SummerTime datasets", "\n", "print_with_color", "(", "f\"{'#' * 10} Testing all datasets... {'#' * 10}\\n\\n\"", ",", "\"35\"", ")", "\n", "print", "(", "list_all_datasets", "(", ")", ")", "\n", "\n", "num_datasets", "=", "0", "\n", "\n", "for", "ds_cls", "in", "test_datasets", ":", "\n", "\n", "# TODO: Temporarily skipping Arxiv (size/time), > 30min download time for Travis-CI", "\n", "            ", "if", "ds_cls", "in", "[", "ArxivDataset", "]", ":", "\n", "                ", "continue", "\n", "", "elif", "ds_cls", "in", "[", "MassivesummDataset", "]", ":", "\n", "                ", "print_with_color", "(", "f\"Testing {ds_cls} dataset...\"", ",", "\"35\"", ")", "\n", "ds", "=", "ds_cls", "(", "\"danish\"", ")", "\n", "\n", "", "elif", "isinstance", "(", "ds_cls", ",", "CustomDataset", ")", ":", "\n", "                ", "ds", "=", "ds_cls", "\n", "", "elif", "ds_cls", "in", "[", "XlsumDataset", "]", ":", "\n", "                ", "ds", "=", "ds_cls", "(", "[", "\"yoruba\"", ",", "\"vietnamese\"", "]", ")", "\n", "", "else", ":", "\n", "                ", "print_with_color", "(", "f\"Testing {ds_cls} dataset...\"", ",", "\"35\"", ")", "\n", "ds", ":", "SummDataset", "=", "ds_cls", "(", ")", "\n", "\n", "ds", ".", "show_description", "(", ")", "\n", "\n", "# must have at least one of train/dev/test set", "\n", "", "assert", "ds", ".", "train_set", "or", "ds", ".", "validation_set", "or", "ds", ".", "test_set", "\n", "\n", "if", "ds", ".", "train_set", "is", "not", "None", ":", "\n", "                ", "train_set", "=", "list", "(", "ds", ".", "train_set", ")", "\n", "print", "(", "f\"{ds_cls} has a training set of {len(train_set)} examples\"", ")", "\n", "self", ".", "_test_instance", "(", "\n", "train_set", "[", "0", "]", ",", "\n", "is_multi_document", "=", "ds", ".", "is_multi_document", ",", "\n", "is_dialogue", "=", "ds", ".", "is_dialogue_based", ",", "\n", ")", "\n", "\n", "", "if", "ds", ".", "validation_set", "is", "not", "None", ":", "\n", "                ", "val_set", "=", "list", "(", "ds", ".", "validation_set", ")", "\n", "print", "(", "f\"{ds_cls} has a validation set of {len(val_set)} examples\"", ")", "\n", "self", ".", "_test_instance", "(", "\n", "val_set", "[", "0", "]", ",", "\n", "is_multi_document", "=", "ds", ".", "is_multi_document", ",", "\n", "is_dialogue", "=", "ds", ".", "is_dialogue_based", ",", "\n", ")", "\n", "\n", "", "if", "ds", ".", "test_set", "is", "not", "None", ":", "\n", "                ", "test_set", "=", "list", "(", "ds", ".", "test_set", ")", "\n", "print", "(", "f\"{ds_cls} has a test set of {len(test_set)} examples\"", ")", "\n", "self", ".", "_test_instance", "(", "\n", "test_set", "[", "0", "]", ",", "\n", "is_multi_document", "=", "ds", ".", "is_multi_document", ",", "\n", "is_dialogue", "=", "ds", ".", "is_dialogue_based", ",", "\n", ")", "\n", "\n", "", "print_with_color", "(", "f\"{ds.dataset_name} dataset test complete\\n\"", ",", "\"32\"", ")", "\n", "num_datasets", "+=", "1", "\n", "\n", "", "print_with_color", "(", "\n", "f\"{'#' * 10} test_all_datasets {__name__} complete ({num_datasets} datasets) {'#' * 10}\"", ",", "\n", "\"32\"", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.tests.dataset_test.get_dummy_data_with_query": [[36, 38], ["None"], "function", ["None"], ["def", "get_dummy_data_with_query", "(", "n", ":", "int", ")", ":", "\n", "    ", "return", "[", "DUMMY_DATA_WITH_QUERY", "]", "*", "n", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.tests.dataset_test.get_dummy_data_without_query": [[40, 42], ["None"], "function", ["None"], ["", "def", "get_dummy_data_without_query", "(", "n", ":", "int", ")", ":", "\n", "    ", "return", "[", "DUMMY_DATA_WITHOUT_QUERY", "]", "*", "n", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.tests.integration_test.IntegrationTests.get_prediction": [[25, 48], ["model.summarize", "isinstance"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.tests.model_selector_test.ToyModel.summarize"], ["    ", "def", "get_prediction", "(", "\n", "self", ",", "model", ":", "SummModel", ",", "dataset", ":", "SummDataset", ",", "test_instances", ":", "List", "[", "SummInstance", "]", "\n", ")", "->", "Tuple", "[", "Union", "[", "List", "[", "str", "]", ",", "List", "[", "List", "[", "str", "]", "]", "]", ",", "Union", "[", "List", "[", "str", "]", ",", "List", "[", "List", "[", "str", "]", "]", "]", "]", ":", "\n", "        ", "\"\"\"\n        Get summary prediction given model and dataset instances.\n\n        :param SummModel `model`: Model for summarization task.\n        :param SummDataset `dataset`: Dataset for summarization task.\n        :param List[SummInstance] `test_instances`: Instances from `dataset` to summarize.\n        :returns Tuple containing summary list of summary predictions and targets corresponding to each instance in `test_instances`.\n        \"\"\"", "\n", "\n", "src", "=", "(", "\n", "[", "ins", ".", "source", "[", "0", "]", "for", "ins", "in", "test_instances", "]", "\n", "if", "isinstance", "(", "dataset", ",", "ScisummnetDataset", ")", "\n", "else", "[", "ins", ".", "source", "for", "ins", "in", "test_instances", "]", "\n", ")", "\n", "tgt", "=", "[", "ins", ".", "summary", "for", "ins", "in", "test_instances", "]", "\n", "query", "=", "(", "\n", "[", "ins", ".", "query", "for", "ins", "in", "test_instances", "]", "if", "dataset", ".", "is_query_based", "else", "None", "\n", ")", "\n", "prediction", "=", "model", ".", "summarize", "(", "src", ",", "query", ")", "\n", "return", "prediction", ",", "tgt", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.tests.integration_test.IntegrationTests.get_eval_dict": [[49, 59], ["metric.evaluate"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.evaluation.meteor_metric.Meteor.evaluate"], ["", "def", "get_eval_dict", "(", "self", ",", "metric", ":", "SummMetric", ",", "prediction", ":", "List", "[", "str", "]", ",", "tgt", ":", "List", "[", "str", "]", ")", ":", "\n", "        ", "\"\"\"\n        Run evaluation metric on summary prediction.\n\n        :param SummMetric `metric`: Evaluation metric.\n        :param List[str] `prediction`: Summary prediction instances.\n        :param List[str] `tgt`: Target prediction instances from dataset.\n        \"\"\"", "\n", "score_dict", "=", "metric", ".", "evaluate", "(", "prediction", ",", "tgt", ")", "\n", "return", "score_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.tests.integration_test.IntegrationTests.test_all": [[60, 113], ["helpers.print_with_color", "helpers.print_with_color", "print", "evaluation_metrics.append", "dataset_cls", "eval_cls", "list", "print", "helpers.print_with_color", "summertime.pipeline.assemble_model_pipeline", "helpers.retrieve_random_test_instances", "helpers.print_with_color", "integration_test.IntegrationTests.get_prediction", "print", "helpers.print_with_color", "helpers.print_with_color", "integration_test.IntegrationTests.get_eval_dict", "print", "len"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.tests.helpers.print_with_color", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.tests.helpers.print_with_color", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.tests.helpers.print_with_color", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.pipeline.__init__.assemble_model_pipeline", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.tests.helpers.retrieve_random_test_instances", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.tests.helpers.print_with_color", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.tests.integration_test.IntegrationTests.get_prediction", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.tests.helpers.print_with_color", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.tests.helpers.print_with_color", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.tests.integration_test.IntegrationTests.get_eval_dict"], ["", "def", "test_all", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Runs integration test on all compatible dataset + model + evaluation metric pipelines supported by SummerTime.\n        \"\"\"", "\n", "\n", "print_with_color", "(", "\"\\nInitializing all evaluation metrics...\"", ",", "\"35\"", ")", "\n", "evaluation_metrics", "=", "[", "]", "\n", "for", "eval_cls", "in", "SUPPORTED_EVALUATION_METRICS", ":", "\n", "# # TODO: Temporarily skipping Rouge/RougeWE metrics to avoid local bug.", "\n", "# if eval_cls in [Rouge, RougeWe]:", "\n", "#     continue", "\n", "            ", "print", "(", "eval_cls", ")", "\n", "evaluation_metrics", ".", "append", "(", "eval_cls", "(", ")", ")", "\n", "\n", "", "print_with_color", "(", "\"\\n\\nBeginning integration tests...\"", ",", "\"35\"", ")", "\n", "for", "dataset_cls", "in", "SUPPORTED_SUMM_DATASETS", ":", "\n", "# TODO: Temporarily skipping Arxiv (size/time)", "\n", "            ", "if", "dataset_cls", "in", "[", "ArxivDataset", "]", ":", "\n", "                ", "continue", "\n", "", "dataset", "=", "dataset_cls", "(", ")", "\n", "if", "dataset", ".", "train_set", "is", "not", "None", ":", "\n", "                ", "dataset_instances", "=", "list", "(", "dataset", ".", "train_set", ")", "\n", "print", "(", "\n", "f\"\\n{dataset.dataset_name} has a training set of {len(dataset_instances)} examples\"", "\n", ")", "\n", "print_with_color", "(", "\n", "f\"Initializing all matching model pipelines for {dataset.dataset_name} dataset...\"", ",", "\n", "\"35\"", ",", "\n", ")", "\n", "# matching_model_instances = assemble_model_pipeline(dataset_cls, list(filter(lambda m: m != PegasusModel, SUPPORTED_SUMM_MODELS)))", "\n", "matching_model_instances", "=", "assemble_model_pipeline", "(", "\n", "dataset_cls", ",", "SUPPORTED_SUMM_MODELS", "\n", ")", "\n", "for", "model", ",", "model_name", "in", "matching_model_instances", ":", "\n", "                    ", "test_instances", "=", "retrieve_random_test_instances", "(", "\n", "dataset_instances", "=", "dataset_instances", ",", "num_instances", "=", "1", "\n", ")", "\n", "print_with_color", "(", "\n", "f\"{'#' * 20} Testing: {dataset.dataset_name} dataset, {model_name} model {'#' * 20}\"", ",", "\n", "\"35\"", ",", "\n", ")", "\n", "prediction", ",", "tgt", "=", "self", ".", "get_prediction", "(", "\n", "model", ",", "dataset", ",", "test_instances", "\n", ")", "\n", "print", "(", "f\"Prediction: {prediction}\\nTarget: {tgt}\\n\"", ")", "\n", "for", "metric", "in", "evaluation_metrics", ":", "\n", "                        ", "print_with_color", "(", "f\"{metric.metric_name} metric\"", ",", "\"35\"", ")", "\n", "score_dict", "=", "self", ".", "get_eval_dict", "(", "metric", ",", "prediction", ",", "tgt", ")", "\n", "print", "(", "score_dict", ")", "\n", "\n", "", "print_with_color", "(", "\n", "f\"{'#' * 20} Test for {dataset.dataset_name} dataset, {model_name} model COMPLETE {'#' * 20}\\n\\n\"", ",", "\n", "\"32\"", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.tests.model_test.TestIndividualModels.test_hmnet_model": [[57, 65], ["model_test.get_dummy_dialogue_instances", "summertime.model.dialogue.HMNetModel", "summertime.model.dialogue.HMNetModel.summarize", "all", "isinstance"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.tests.model_test.get_dummy_dialogue_instances", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.tests.model_selector_test.ToyModel.summarize"], ["def", "test_hmnet_model", "(", "self", ")", ":", "\n", "        ", "from", "summertime", ".", "model", ".", "dialogue", ".", "hmnet_model", "import", "HMNetModel", "\n", "\n", "dummy_corpus", "=", "get_dummy_dialogue_instances", "(", "2", ")", "\n", "model", "=", "HMNetModel", "(", "min_gen_length", "=", "10", ",", "max_gen_length", "=", "30", ",", "beam_width", "=", "2", ")", "\n", "result", "=", "model", ".", "summarize", "(", "dummy_corpus", ")", "\n", "\n", "assert", "all", "(", "[", "isinstance", "(", "r", ",", "str", ")", "for", "r", "in", "result", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.tests.model_test.TestModels.test_list_models": [[68, 77], ["helpers.print_with_color", "summertime.model.list_all_models", "model_test.TestModels.assertEqual", "helpers.print_with_color", "print", "model_test.TestModels.assertTrue", "len", "len"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.tests.helpers.print_with_color", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.model.__init__.list_all_models", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.tests.helpers.print_with_color"], ["    ", "def", "test_list_models", "(", "self", ")", ":", "\n", "        ", "print_with_color", "(", "f\"{'#'*10} Testing test_list_models... {'#'*10}\\n\"", ",", "\"35\"", ")", "\n", "all_models", "=", "list_all_models", "(", ")", "\n", "for", "model_class", ",", "model_description", "in", "all_models", ":", "\n", "            ", "print", "(", "f\"{model_class} : {model_description}\"", ")", "\n", "self", ".", "assertTrue", "(", "True", ")", "\n", "", "self", ".", "assertEqual", "(", "len", "(", "all_models", ")", ",", "len", "(", "SUPPORTED_SUMM_MODELS", ")", ")", "\n", "print_with_color", "(", "\n", "f\"{'#'*10} test_list_models {__name__} test complete {'#'*10}\\n\\n\"", ",", "\"32\"", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.tests.model_test.TestModels.validate_prediction": [[79, 87], ["model_test.TestModels.assertTrue", "model_test.TestModels.assertTrue", "model_test.TestModels.assertTrue", "print", "isinstance", "all", "len", "len", "isinstance"], "methods", ["None"], ["", "def", "validate_prediction", "(", "self", ",", "prediction", ":", "List", "[", "str", "]", ",", "src", ":", "List", ")", ":", "\n", "        ", "\"\"\"\n        Verify that prediction instances match source instances.\n        \"\"\"", "\n", "self", ".", "assertTrue", "(", "isinstance", "(", "prediction", ",", "list", ")", ")", "\n", "self", ".", "assertTrue", "(", "all", "(", "[", "isinstance", "(", "ins", ",", "str", ")", "for", "ins", "in", "prediction", "]", ")", ")", "\n", "self", ".", "assertTrue", "(", "len", "(", "prediction", ")", "==", "len", "(", "src", ")", ")", "\n", "print", "(", "\"Prediction typing and length matches source instances!\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.tests.model_test.TestModels.test_model_summarize": [[88, 159], ["summertime.dataset.dataset_loaders.CnndmDataset", "summertime.dataset.dataset_loaders.MlsumDataset", "summertime.dataset.dataset_loaders.MultinewsDataset", "summertime.dataset.dataset_loaders.PubmedqaDataset", "summertime.dataset.dataset_loaders.SamsumDataset", "helpers.print_with_color", "summertime.model.list_all_models", "helpers.print_with_color", "helpers.print_with_color", "helpers.print_with_color", "summertime.model.dialogue.HMNetModel", "helpers.get_summarization_set", "model_class", "model_class", "helpers.get_query_based_summarization_set", "model_class.summarize", "print", "helpers.get_summarization_set", "model_class.summarize", "print", "model_test.TestModels.validate_prediction", "helpers.get_summarization_set", "model_class.summarize", "print", "model_test.TestModels.validate_prediction", "helpers.get_summarization_set", "model_class.summarize", "print", "model_test.TestModels.validate_prediction", "helpers.get_summarization_set", "model_class.summarize", "print", "model_test.TestModels.validate_prediction"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.tests.helpers.print_with_color", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.model.__init__.list_all_models", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.tests.helpers.print_with_color", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.tests.helpers.print_with_color", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.tests.helpers.print_with_color", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.tests.helpers.get_summarization_set", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.tests.helpers.get_query_based_summarization_set", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.tests.model_selector_test.ToyModel.summarize", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.tests.helpers.get_summarization_set", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.tests.model_selector_test.ToyModel.summarize", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.tests.model_test.TestModels.validate_prediction", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.tests.helpers.get_summarization_set", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.tests.model_selector_test.ToyModel.summarize", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.tests.model_test.TestModels.validate_prediction", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.tests.helpers.get_summarization_set", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.tests.model_selector_test.ToyModel.summarize", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.tests.model_test.TestModels.validate_prediction", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.tests.helpers.get_summarization_set", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.tests.model_selector_test.ToyModel.summarize", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.tests.model_test.TestModels.validate_prediction"], ["", "def", "test_model_summarize", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Test all supported models on instances from datasets.\n        \"\"\"", "\n", "\n", "single_doc_dataset", "=", "CnndmDataset", "(", ")", "\n", "multiling_dataset", "=", "MlsumDataset", "(", "[", "\"es\"", "]", ")", "\n", "multi_doc_dataset", "=", "MultinewsDataset", "(", ")", "\n", "query_based_dataset", "=", "PubmedqaDataset", "(", ")", "\n", "dialogue_based_dataset", "=", "SamsumDataset", "(", ")", "\n", "\n", "print_with_color", "(", "f\"{'#'*10} Testing all models... {'#'*10}\\n\"", ",", "\"35\"", ")", "\n", "\n", "num_models", "=", "0", "\n", "all_models", "=", "list_all_models", "(", ")", "\n", "\n", "for", "model_class", ",", "_", "in", "all_models", ":", "\n", "            ", "print_with_color", "(", "f\"Testing {model_class.model_name} model...\"", ",", "\"35\"", ")", "\n", "\n", "if", "model_class", "in", "[", "HMNetModel", "]", ":", "\n", "                ", "model", "=", "HMNetModel", "(", "min_gen_length", "=", "10", ",", "max_gen_length", "=", "100", ",", "beam_width", "=", "2", ")", "\n", "\n", "", "if", "model_class", "==", "LexRankModel", ":", "\n", "# current LexRankModel requires a training set", "\n", "                ", "training_src", ",", "training_tgt", "=", "get_summarization_set", "(", "\n", "single_doc_dataset", ",", "100", "\n", ")", "\n", "model", "=", "model_class", "(", "training_src", ")", "\n", "", "else", ":", "\n", "                ", "model", "=", "model_class", "(", ")", "\n", "\n", "", "if", "model", ".", "is_query_based", ":", "\n", "                ", "test_src", ",", "test_tgt", ",", "test_query", "=", "get_query_based_summarization_set", "(", "\n", "query_based_dataset", ",", "1", "\n", ")", "\n", "prediction", "=", "model", ".", "summarize", "(", "test_src", ",", "test_query", ")", "\n", "print", "(", "\n", "f\"Query: {test_query}\\nGold summary: {test_tgt}\\nPredicted summary: {prediction}\"", "\n", ")", "\n", "", "elif", "model", ".", "is_multi_document", ":", "\n", "                ", "test_src", ",", "test_tgt", "=", "get_summarization_set", "(", "multi_doc_dataset", ",", "1", ")", "\n", "prediction", "=", "model", ".", "summarize", "(", "test_src", ")", "\n", "print", "(", "f\"Gold summary: {test_tgt} \\nPredicted summary: {prediction}\"", ")", "\n", "self", ".", "validate_prediction", "(", "prediction", ",", "test_src", ")", "\n", "", "elif", "model", ".", "is_dialogue_based", ":", "\n", "                ", "test_src", ",", "test_tgt", "=", "get_summarization_set", "(", "dialogue_based_dataset", ",", "1", ")", "\n", "prediction", "=", "model", ".", "summarize", "(", "test_src", ")", "\n", "print", "(", "f\"Gold summary: {test_tgt}\\nPredicted summary: {prediction}\"", ")", "\n", "self", ".", "validate_prediction", "(", "prediction", ",", "test_src", ")", "\n", "", "elif", "model", ".", "is_multilingual", ":", "\n", "                ", "test_src", ",", "test_tgt", "=", "get_summarization_set", "(", "multiling_dataset", ",", "1", ")", "\n", "prediction", "=", "model", ".", "summarize", "(", "test_src", ")", "\n", "print", "(", "f\"Gold summary: {test_tgt} \\nPredicted summary: {prediction}\"", ")", "\n", "self", ".", "validate_prediction", "(", "prediction", ",", "test_src", ")", "\n", "", "else", ":", "\n", "                ", "test_src", ",", "test_tgt", "=", "get_summarization_set", "(", "single_doc_dataset", ",", "1", ")", "\n", "prediction", "=", "model", ".", "summarize", "(", "\n", "[", "test_src", "[", "0", "]", "*", "5", "]", "if", "model_class", "==", "LongformerModel", "else", "test_src", "\n", ")", "\n", "print", "(", "f\"Gold summary: {test_tgt} \\nPredicted summary: {prediction}\"", ")", "\n", "self", ".", "validate_prediction", "(", "\n", "prediction", ",", "\n", "[", "test_src", "[", "0", "]", "*", "5", "]", "if", "model_class", "==", "LongformerModel", "else", "test_src", ",", "\n", ")", "\n", "\n", "", "print_with_color", "(", "f\"{model_class.model_name} model test complete\\n\"", ",", "\"32\"", ")", "\n", "num_models", "+=", "1", "\n", "\n", "", "print_with_color", "(", "\n", "f\"{'#'*10} test_model_summarize complete ({num_models} models) {'#'*10}\\n\"", ",", "\n", "\"32\"", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.tests.model_test.get_dummy_single_doc_instances": [[38, 40], ["None"], "function", ["None"], ["def", "get_dummy_single_doc_instances", "(", "n", ":", "int", ")", ":", "\n", "    ", "return", "[", "DUMMY_DOC_INPUT", "]", "*", "n", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.tests.model_test.get_dummy_multi_doc_instances": [[42, 44], ["range"], "function", ["None"], ["", "def", "get_dummy_multi_doc_instances", "(", "n", ":", "int", ",", "m", ":", "int", "=", "5", ")", ":", "\n", "    ", "return", "[", "[", "DUMMY_DOC_INPUT", "]", "*", "m", "for", "_", "in", "range", "(", "n", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.tests.model_test.get_dummy_query_based_instances": [[46, 48], ["None"], "function", ["None"], ["", "def", "get_dummy_query_based_instances", "(", "n", ":", "int", ")", ":", "\n", "    ", "return", "[", "DUMMY_DOC_INPUT", "]", "*", "n", ",", "[", "DUMMY_QUERY_INPUT", "]", "*", "n", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.tests.model_test.get_dummy_dialogue_instances": [[50, 52], ["range"], "function", ["None"], ["", "def", "get_dummy_dialogue_instances", "(", "n", ":", "int", ")", ":", "\n", "    ", "return", "[", "DUMMY_DIALOGUE_INPUT", "for", "_", "in", "range", "(", "n", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.tests.model_selector_test.ToyModel.__init__": [[9, 12], ["summertime.model.base_model.SummModel.__init__", "str"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.tests.model_selector_test.ToyModel.__init__"], ["    ", "def", "__init__", "(", "self", ",", "num", ":", "int", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "ToyModel", ",", "self", ")", "\n", "self", ".", "model_name", "=", "str", "(", "num", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.tests.model_selector_test.ToyModel.summarize": [[13, 23], ["len"], "methods", ["None"], ["", "def", "summarize", "(", "self", ",", "corpus", ")", ":", "\n", "        ", "return", "(", "\n", "[", "\n", "\"\"\"\n        Glowing letters that had been hanging above\n        the Yankee stadium from 1976 to 2008 were placed for auction at\n        Sotheby\u2019s on Wednesday, but were not sold, The current owner\n        of the sign is Reggie Jackson, a Yankee hall-of-famer.\"\"\"", "\n", "]", "\n", "*", "len", "(", "corpus", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.tests.model_selector_test.TestModelSelector.test_model_selector": [[27, 46], ["print", "model_selector_test.ToyModel", "model_selector_test.ToyModel", "iter", "iter", "summertime.evaluation.model_selector.ModelSelector", "summertime.evaluation.model_selector.ModelSelector.run", "print", "summertime.evaluation.model_selector.ModelSelector", "summertime.evaluation.model_selector.ModelSelector.run_halving", "print", "print", "metric", "summertime.dataset.st_dataset.SummInstance", "summertime.dataset.st_dataset.SummInstance"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.evaluation.model_selector.ModelSelector.run", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.evaluation.model_selector.ModelSelector.run_halving"], ["    ", "def", "test_model_selector", "(", "self", ")", ":", "\n", "        ", "print", "(", "f\"{'#'*10} model_selector_evaluate STARTS {'#'*10}\"", ")", "\n", "\n", "model_1", "=", "ToyModel", "(", "1", ")", "\n", "model_2", "=", "ToyModel", "(", "2", ")", "\n", "models", "=", "[", "model_1", ",", "model_2", "]", "\n", "generator1", "=", "iter", "(", "[", "SummInstance", "(", "\"A context.\"", ",", "\"A summary.\"", ")", "]", "*", "10", ")", "\n", "generator2", "=", "iter", "(", "[", "SummInstance", "(", "\"A context.\"", ",", "\"A summary.\"", ")", "]", "*", "10", ")", "\n", "metrics", "=", "[", "metric", "(", ")", "for", "metric", "in", "SUPPORTED_EVALUATION_METRICS", "]", "\n", "\n", "selector", "=", "ModelSelector", "(", "models", ",", "generator1", ",", "metrics", ")", "\n", "table", "=", "selector", ".", "run", "(", ")", "\n", "print", "(", "table", ")", "\n", "\n", "new_selector", "=", "ModelSelector", "(", "models", ",", "generator2", ",", "metrics", ")", "\n", "smart_table", "=", "new_selector", ".", "run_halving", "(", "min_instances", "=", "2", ",", "factor", "=", "2", ")", "\n", "print", "(", "smart_table", ")", "\n", "\n", "print", "(", "f\"{'#'*10} model_selector_evaluate ENDS {'#'*10}\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.tests.evaluation_test.TestEvaluationMetrics.get_summary_pairs": [[10, 33], ["None"], "methods", ["None"], ["    ", "def", "get_summary_pairs", "(", "self", ",", "size", ":", "int", "=", "1", ")", "->", "Tuple", "[", "List", "[", "str", "]", "]", ":", "\n", "        ", "test_output", "=", "(", "\n", "[", "\n", "\"\"\"\n        Glowing letters that had been hanging above\n        the Yankee stadium from 1976 to 2008 were placed for auction at\n        Sotheby\u2019s on Wednesday, but were not sold, The current owner\n        of the sign is Reggie Jackson, a Yankee hall-of-famer.\"\"\"", "\n", "]", "\n", "*", "size", "\n", ")", "\n", "test_target", "=", "(", "\n", "[", "\n", "\"\"\"\n        An auction for the lights from Yankee Stadium failed to\n        produce any bids on Wednesday at Sotheby\u2019s. The lights,\n        currently owned by former Yankees player Reggie Jackson,\n        lit the stadium from 1976 until 2008.\"\"\"", "\n", "]", "\n", "*", "size", "\n", ")", "\n", "\n", "return", "test_output", ",", "test_target", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.tests.evaluation_test.TestEvaluationMetrics.test_evaluate": [[34, 67], ["helpers.print_with_color", "helpers.print_with_color", "helpers.print_with_color", "metric_class", "evaluation_test.TestEvaluationMetrics.get_summary_pairs", "metric_class.evaluate", "print", "print", "evaluation_test.TestEvaluationMetrics.assertTrue", "evaluation_test.TestEvaluationMetrics.assertNotEqual", "metric_class.evaluate.items", "helpers.print_with_color", "isinstance", "evaluation_test.TestEvaluationMetrics.assertTrue", "isinstance", "isinstance"], "methods", ["home.repos.pwc.inspect_result.Yale-LILY_SummerTime.tests.helpers.print_with_color", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.tests.helpers.print_with_color", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.tests.helpers.print_with_color", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.tests.evaluation_test.TestEvaluationMetrics.get_summary_pairs", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.evaluation.meteor_metric.Meteor.evaluate", "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.tests.helpers.print_with_color"], ["", "def", "test_evaluate", "(", "self", ")", ":", "\n", "        ", "print_with_color", "(", "f\"{'#'*10} Testing all evaluation metrics... {'#'*10}\\n\"", ",", "\"35\"", ")", "\n", "\n", "num_eval_metrics", "=", "0", "\n", "\n", "for", "metric_class", "in", "SUPPORTED_EVALUATION_METRICS", ":", "\n", "# if metric_class in [Rouge, RougeWe]:", "\n", "#     # TODO: Temporarily skipping Rouge/RougeWE metrics to avoid local bug.", "\n", "#     continue", "\n", "\n", "            ", "print_with_color", "(", "f\"Testing {metric_class.metric_name}...\"", ",", "\"35\"", ")", "\n", "\n", "metric", "=", "metric_class", "(", ")", "\n", "\n", "test_output", ",", "test_target", "=", "self", ".", "get_summary_pairs", "(", ")", "\n", "score_dict", "=", "metric", ".", "evaluate", "(", "test_output", ",", "test_target", ")", "\n", "print", "(", "f\"{metric_class} output dictionary\"", ")", "\n", "print", "(", "score_dict", ")", "\n", "self", ".", "assertTrue", "(", "isinstance", "(", "score_dict", ",", "Dict", ")", ")", "\n", "self", ".", "assertNotEqual", "(", "score_dict", ",", "{", "}", ")", "\n", "\n", "for", "k", ",", "v", "in", "score_dict", ".", "items", "(", ")", ":", "\n", "                ", "self", ".", "assertTrue", "(", "isinstance", "(", "k", ",", "str", ")", "and", "isinstance", "(", "v", ",", "float", ")", ")", "\n", "# # TODO: add metric score range assertions", "\n", "# self.assertTrue(self.range[0] <= score_dict[k])", "\n", "# self.assertTrue(score_dict[k] <= self.range[1])", "\n", "\n", "", "print_with_color", "(", "f\"{metric_class.metric_name} test complete\\n\"", ",", "\"32\"", ")", "\n", "num_eval_metrics", "+=", "1", "\n", "\n", "", "print_with_color", "(", "\n", "f\"{'#'*10} Evaluation metrics test complete ({num_eval_metrics} metrics) {'#'*10}\"", ",", "\n", "\"32\"", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.tests.helpers.print_with_color": [[7, 18], ["print"], "function", ["None"], ["def", "print_with_color", "(", "s", ":", "str", ",", "color", ":", "str", ")", ":", "\n", "    ", "\"\"\"\n    Print formatted string.\n\n    :param str `s`: String to print.\n    :param str `color`: ANSI color code.\n\n    :see https://gist.github.com/RabaDabaDoba/145049536f815903c79944599c6f952a\n    \"\"\"", "\n", "\n", "print", "(", "f\"\\033[{color}m{s}\\033[0m\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.tests.helpers.retrieve_random_test_instances": [[20, 37], ["range", "test_instances.append", "random.randint", "len"], "function", ["None"], ["", "def", "retrieve_random_test_instances", "(", "\n", "dataset_instances", ":", "List", "[", "SummInstance", "]", ",", "num_instances", "=", "3", "\n", ")", "->", "List", "[", "SummInstance", "]", ":", "\n", "    ", "\"\"\"\n    Retrieve random test instances from a dataset training set.\n\n    :param List[SummInstance] `dataset_instances`: Instances from a dataset `train_set` to pull random examples from.\n    :param int `num_instances`: Number of random instances to pull. Defaults to `3`.\n    :return List of SummInstance to summarize.\n    \"\"\"", "\n", "\n", "test_instances", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "num_instances", ")", ":", "\n", "        ", "test_instances", ".", "append", "(", "\n", "dataset_instances", "[", "random", ".", "randint", "(", "0", ",", "len", "(", "dataset_instances", ")", "-", "1", ")", "]", "\n", ")", "\n", "", "return", "test_instances", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.tests.helpers.get_summarization_set": [[39, 50], ["range", "zip", "subset.append", "list", "list", "next", "list", "map"], "function", ["None"], ["", "def", "get_summarization_set", "(", "dataset", ":", "SummDataset", ",", "size", "=", "1", ")", "->", "Tuple", "[", "List", ",", "List", "]", ":", "\n", "    ", "\"\"\"\n    Return instances from given summarization dataset, in the format of (sources, targets).\n    \"\"\"", "\n", "subset", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "size", ")", ":", "\n", "        ", "subset", ".", "append", "(", "next", "(", "dataset", ".", "train_set", ")", ")", "\n", "\n", "", "src", ",", "tgt", "=", "zip", "(", "*", "(", "list", "(", "map", "(", "lambda", "x", ":", "(", "x", ".", "source", ",", "x", ".", "summary", ")", ",", "subset", ")", ")", ")", ")", "\n", "\n", "return", "list", "(", "src", ")", ",", "list", "(", "tgt", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Yale-LILY_SummerTime.tests.helpers.get_query_based_summarization_set": [[52, 67], ["range", "zip", "subset.append", "list", "list", "list", "next", "list", "map"], "function", ["None"], ["", "def", "get_query_based_summarization_set", "(", "\n", "dataset", ":", "SummDataset", ",", "size", "=", "1", "\n", ")", "->", "Tuple", "[", "List", ",", "List", ",", "List", "]", ":", "\n", "    ", "\"\"\"\n    Return instances from given query-based summarization dataset, in the format of (sources, targets, queries).\n    \"\"\"", "\n", "subset", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "size", ")", ":", "\n", "        ", "subset", ".", "append", "(", "next", "(", "dataset", ".", "train_set", ")", ")", "\n", "\n", "", "src", ",", "tgt", ",", "queries", "=", "zip", "(", "\n", "*", "(", "list", "(", "map", "(", "lambda", "x", ":", "(", "x", ".", "source", ",", "x", ".", "summary", ",", "x", ".", "query", ")", ",", "subset", ")", ")", ")", "\n", ")", "\n", "\n", "return", "list", "(", "src", ")", ",", "list", "(", "tgt", ")", ",", "list", "(", "queries", ")", "\n", "", ""]]}