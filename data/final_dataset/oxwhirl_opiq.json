{"home.repos.pwc.inspect_result.oxwhirl_opiq.src.main.save_results": [[53, 62], ["config_dict.get", "ex.observers.append", "logging.critical", "sacred.observers.FileStorageObserver.create"], "function", ["None"], ["def", "save_results", "(", "config_dict", ")", ":", "\n", "    ", "save", "=", "config_dict", ".", "get", "(", "\"save\"", ",", "False", ")", "\n", "\n", "# Sorry I removed the mongodb stuff!", "\n", "\n", "# Saving results to disk", "\n", "if", "save", ":", "\n", "        ", "ex", ".", "observers", ".", "append", "(", "FileStorageObserver", ".", "create", "(", "\"results/sacred\"", ")", ")", "\n", "logging", ".", "critical", "(", "\"Saving sacred to file\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.src.main.main": [[66, 443], ["utils.dict2namedtuple.convert", "logging.getLogger", "utils.logging.configure_stats_logging", "utils.logging.get_stats", "logging.getLogger.critical", "gym.make", "config._replace._replace", "config._replace._replace", "envs.env_wrapper.EnvWrapper", "sorted", "logging.getLogger.critical", "torch.device", "logging.getLogger.critical", "target_agent.load_state_dict", "agent.to", "target_agent.to", "getattr", "getattr", "buffer.buffer.ReplayBuffer", "buffer.buffer.ReplayBuffer", "envs.env_wrapper.EnvWrapper", "set", "set", "time.time", "logging.critical", "envs.env_wrapper.EnvWrapper.reset", "range", "envs.env_wrapper.EnvWrapper.reset", "logging.critical", "logging.getLogger.critical", "envs.env_wrapper.EnvWrapper.close", "envs.env_wrapper.EnvWrapper.close", "logging.getLogger.critical", "logging.getLogger.critical", "logging.disable", "config._replace._asdict().items", "agent.specifier.get_model", "agent.specifier.get_model", "agent.state_dict", "action.eps_greedy.EpsGreedy", "buffer.buffer.ReplayBuffer", "trainer.dqn_train.DQNTrainer", "buffer.buffer.ReplayBuffer.store_frame", "buffer.buffer.ReplayBuffer.encode_recent_observation", "torch.tensor().unsqueeze", "numpy.random.randint", "envs.env_wrapper.EnvWrapper.step", "buffer.buffer.ReplayBuffer.store_effect", "numpy.random.randint", "bsp_action.BSPAction.update_k", "buffer.buffer.ReplayBuffer.store_frame", "buffer.buffer.ReplayBuffer.encode_recent_observation", "bsp_action.BSPAction.select_actions", "envs.env_wrapper.EnvWrapper.step", "utils.logging.get_stats.update_t", "utils.logging.get_stats.update_stats", "buffer.buffer.ReplayBuffer.store_effect", "range", "[].numpy", "envs.env_wrapper.EnvWrapper.log_visitation", "set.add", "set.add", "utils.logging.get_stats.update_stats", "utils.logging.get_stats.update_stats", "utils.logging.get_stats.update_stats", "info.items", "logging.getLogger.critical", "client.close", "logging.getLogger.critical", "torch.cuda.is_available", "count.atari_count.AtariCount", "action.optimistic_action.OptimisticAction", "gym.make", "count.rnd_count.ApproxRndCount.visit", "getattr", "envs.env_wrapper.EnvWrapper.reset", "logging.getLogger.warning", "buffer.buffer.ReplayBuffer.store_frame", "torch.no_grad", "torch.tensor().unsqueeze", "agent", "logging.getLogger.warning", "count.rnd_count.ApproxRndCount.visit", "getattr", "debug_info.update", "envs.env_wrapper.EnvWrapper.render", "buffer.buffer.ReplayBuffer.store_effect", "logging.getLogger.warning", "envs.env_wrapper.EnvWrapper.reset", "max", "utils.logging.get_stats.update_stats", "utils.logging.get_stats.update_stats", "utils.logging.get_stats.update_stats", "utils.logging.get_stats.update_stats", "utils.logging.get_stats.update_stats", "buffer.buffer.ReplayBuffer.can_sample", "trainer.dqn_train.DQNTrainer.update_target_agent", "range", "utils.logging.get_stats.update_stats", "len", "len", "torch.no_grad", "utils.logging.get_stats.print_stats", "str", "config._replace._asdict", "count.rnd_network_count.RndNetworkDistill", "action.bsp_action.BSPAction", "Exception", "torch.tensor", "buffer.buffer.ReplayBuffer.store_frame", "buffer.buffer.ReplayBuffer.store_effect", "numpy.random.randint", "bsp_action.BSPAction.update_k", "buffer.buffer.ReplayBuffer.sample", "trainer.dqn_train.DQNTrainer.train", "buffer.buffer.ReplayBuffer.can_sample", "count.rnd_count.ApproxRndCount.update_target_agent", "utils.logging.get_stats.update_stats", "numpy.mean", "utils.logging.get_stats.update_stats", "envs.env_wrapper.EnvWrapper.count_state_action_space", "envs.env_wrapper.EnvWrapper.state_counts", "envs.env_wrapper.EnvWrapper.q_value_estimates", "logging.getLogger.error", "time.time", "count.dora_count.DoraCount", "count.rnd_count.ApproxRndCount", "math.sqrt", "torch.tensor", "math.sqrt", "buffer.buffer.ReplayBuffer.store_frame", "buffer.buffer.ReplayBuffer.store_effect", "buffer.buffer.ReplayBuffer.sample", "count.rnd_count.ApproxRndCount.train", "agent.detach().cpu", "utils.logging.save_image", "utils.logging.save_sa_count_vals", "utils.logging.save_actual_counts", "utils.logging.save_image", "utils.logging.save_q_vals", "range", "logging.getLogger.error", "utils.logging.get_stats.update_stats", "utils.logging.get_stats.update_stats", "envs.env_wrapper.EnvWrapper.reset", "utils.timehelper.time_left", "utils.timehelper.time_str", "agent.detach", "buffer.buffer.ReplayBuffer.store_frame", "buffer.buffer.ReplayBuffer.encode_recent_observation", "torch.tensor().unsqueeze", "agent", "envs.env_wrapper.EnvWrapper.step", "buffer.buffer.ReplayBuffer.store_effect", "bsp_action.BSPAction.select_actions", "action.testing.get_test_action", "envs.env_wrapper.EnvWrapper.render", "time.time", "torch.tensor"], "function", ["home.repos.pwc.inspect_result.oxwhirl_opiq.utils.dict2namedtuple.convert", "home.repos.pwc.inspect_result.oxwhirl_opiq.utils.logging.configure_stats_logging", "home.repos.pwc.inspect_result.oxwhirl_opiq.utils.logging.get_stats", "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.gridworld.GridWorld.reset", "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.gridworld.GridWorld.reset", "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.montezuma.Montezuma.close", "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.montezuma.Montezuma.close", "home.repos.pwc.inspect_result.oxwhirl_opiq.agent.specifier.get_model", "home.repos.pwc.inspect_result.oxwhirl_opiq.agent.specifier.get_model", "home.repos.pwc.inspect_result.oxwhirl_opiq.buffer.buffer.ReplayBuffer.store_frame", "home.repos.pwc.inspect_result.oxwhirl_opiq.buffer.buffer.ReplayBuffer.encode_recent_observation", "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.gridworld.GridWorld.step", "home.repos.pwc.inspect_result.oxwhirl_opiq.buffer.buffer.ReplayBuffer.store_effect", "home.repos.pwc.inspect_result.oxwhirl_opiq.action.bsp_action.BSPAction.update_k", "home.repos.pwc.inspect_result.oxwhirl_opiq.buffer.buffer.ReplayBuffer.store_frame", "home.repos.pwc.inspect_result.oxwhirl_opiq.buffer.buffer.ReplayBuffer.encode_recent_observation", "home.repos.pwc.inspect_result.oxwhirl_opiq.action.count_action.CountBonus.select_actions", "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.gridworld.GridWorld.step", "home.repos.pwc.inspect_result.oxwhirl_opiq.utils.logging.Stats.update_t", "home.repos.pwc.inspect_result.oxwhirl_opiq.utils.logging.Stats.update_stats", "home.repos.pwc.inspect_result.oxwhirl_opiq.buffer.buffer.ReplayBuffer.store_effect", "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.env_wrapper.EnvWrapper.log_visitation", "home.repos.pwc.inspect_result.oxwhirl_opiq.utils.logging.Stats.update_stats", "home.repos.pwc.inspect_result.oxwhirl_opiq.utils.logging.Stats.update_stats", "home.repos.pwc.inspect_result.oxwhirl_opiq.utils.logging.Stats.update_stats", "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.montezuma.Montezuma.close", "home.repos.pwc.inspect_result.oxwhirl_opiq.count.dora_count.DoraCount.visit", "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.gridworld.GridWorld.reset", "home.repos.pwc.inspect_result.oxwhirl_opiq.buffer.buffer.ReplayBuffer.store_frame", "home.repos.pwc.inspect_result.oxwhirl_opiq.count.dora_count.DoraCount.visit", "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.gridworld.GridWorld.render", "home.repos.pwc.inspect_result.oxwhirl_opiq.buffer.buffer.ReplayBuffer.store_effect", "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.gridworld.GridWorld.reset", "home.repos.pwc.inspect_result.oxwhirl_opiq.utils.logging.Stats.update_stats", "home.repos.pwc.inspect_result.oxwhirl_opiq.utils.logging.Stats.update_stats", "home.repos.pwc.inspect_result.oxwhirl_opiq.utils.logging.Stats.update_stats", "home.repos.pwc.inspect_result.oxwhirl_opiq.utils.logging.Stats.update_stats", "home.repos.pwc.inspect_result.oxwhirl_opiq.utils.logging.Stats.update_stats", "home.repos.pwc.inspect_result.oxwhirl_opiq.buffer.buffer.ReplayBuffer.can_sample", "home.repos.pwc.inspect_result.oxwhirl_opiq.trainer.dqn_train.DQNTrainer.update_target_agent", "home.repos.pwc.inspect_result.oxwhirl_opiq.utils.logging.Stats.update_stats", "home.repos.pwc.inspect_result.oxwhirl_opiq.utils.logging.Stats.print_stats", "home.repos.pwc.inspect_result.oxwhirl_opiq.buffer.buffer.ReplayBuffer.store_frame", "home.repos.pwc.inspect_result.oxwhirl_opiq.buffer.buffer.ReplayBuffer.store_effect", "home.repos.pwc.inspect_result.oxwhirl_opiq.action.bsp_action.BSPAction.update_k", "home.repos.pwc.inspect_result.oxwhirl_opiq.buffer.buffer.ReplayBuffer.sample", "home.repos.pwc.inspect_result.oxwhirl_opiq.trainer.dqn_train.DQNTrainer.train", "home.repos.pwc.inspect_result.oxwhirl_opiq.buffer.buffer.ReplayBuffer.can_sample", "home.repos.pwc.inspect_result.oxwhirl_opiq.trainer.dqn_train.DQNTrainer.update_target_agent", "home.repos.pwc.inspect_result.oxwhirl_opiq.utils.logging.Stats.update_stats", "home.repos.pwc.inspect_result.oxwhirl_opiq.utils.logging.Stats.update_stats", "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.gridworld.GridWorld.count_state_action_space", "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.env_wrapper.EnvWrapper.state_counts", "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.gridworld.GridWorld.q_value_estimates", "home.repos.pwc.inspect_result.oxwhirl_opiq.buffer.buffer.ReplayBuffer.store_frame", "home.repos.pwc.inspect_result.oxwhirl_opiq.buffer.buffer.ReplayBuffer.store_effect", "home.repos.pwc.inspect_result.oxwhirl_opiq.buffer.buffer.ReplayBuffer.sample", "home.repos.pwc.inspect_result.oxwhirl_opiq.trainer.dqn_train.DQNTrainer.train", "home.repos.pwc.inspect_result.oxwhirl_opiq.utils.logging.save_image", "home.repos.pwc.inspect_result.oxwhirl_opiq.utils.logging.save_sa_count_vals", "home.repos.pwc.inspect_result.oxwhirl_opiq.utils.logging.save_actual_counts", "home.repos.pwc.inspect_result.oxwhirl_opiq.utils.logging.save_image", "home.repos.pwc.inspect_result.oxwhirl_opiq.utils.logging.save_q_vals", "home.repos.pwc.inspect_result.oxwhirl_opiq.utils.logging.Stats.update_stats", "home.repos.pwc.inspect_result.oxwhirl_opiq.utils.logging.Stats.update_stats", "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.gridworld.GridWorld.reset", "home.repos.pwc.inspect_result.oxwhirl_opiq.utils.timehelper.time_left", "home.repos.pwc.inspect_result.oxwhirl_opiq.utils.timehelper.time_str", "home.repos.pwc.inspect_result.oxwhirl_opiq.buffer.buffer.ReplayBuffer.store_frame", "home.repos.pwc.inspect_result.oxwhirl_opiq.buffer.buffer.ReplayBuffer.encode_recent_observation", "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.gridworld.GridWorld.step", "home.repos.pwc.inspect_result.oxwhirl_opiq.buffer.buffer.ReplayBuffer.store_effect", "home.repos.pwc.inspect_result.oxwhirl_opiq.action.count_action.CountBonus.select_actions", "home.repos.pwc.inspect_result.oxwhirl_opiq.action.testing.get_test_action", "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.gridworld.GridWorld.render"], ["@", "ex", ".", "automain", "\n", "def", "main", "(", "_config", ",", "_run", ")", ":", "\n", "    ", "config", "=", "convert", "(", "_config", ")", "\n", "_id", "=", "_run", ".", "_id", "\n", "\n", "# Logging stuff", "\n", "logger", "=", "logging", ".", "getLogger", "(", "\"Main\"", ")", "\n", "if", "config", ".", "mongo", ":", "\n", "        ", "logging", ".", "disable", "(", "logging", ".", "WARNING", ")", "\n", "", "configure_stats_logging", "(", "\n", "str", "(", "_id", ")", "+", "\"_\"", "+", "config", ".", "name", ",", "\n", "log_interval", "=", "config", ".", "log_interval", ",", "\n", "sacred_info", "=", "_run", ".", "info", ",", "\n", "use_tb", "=", "config", ".", "tb", ",", "\n", ")", "\n", "stats", "=", "get_stats", "(", ")", "\n", "\n", "logger", ".", "critical", "(", "\"ID: {}\"", ".", "format", "(", "_id", ")", ")", "\n", "# Update config with environment specific information", "\n", "env", "=", "gym", ".", "make", "(", "config", ".", "env", ")", "\n", "num_actions", "=", "env", ".", "action_space", ".", "n", "\n", "config", "=", "config", ".", "_replace", "(", "num_actions", "=", "num_actions", ")", "\n", "state_shape", "=", "env", ".", "observation_space", ".", "shape", "\n", "config", "=", "config", ".", "_replace", "(", "state_shape", "=", "state_shape", ")", "\n", "# Wrap env", "\n", "env", "=", "EnvWrapper", "(", "env", ",", "debug", "=", "True", ",", "args", "=", "config", ")", "\n", "\n", "# Log the config", "\n", "config_str", "=", "\"Config:\\n\\n\"", "\n", "for", "k", ",", "v", "in", "sorted", "(", "config", ".", "_asdict", "(", ")", ".", "items", "(", ")", ")", ":", "\n", "        ", "config_str", "+=", "\"     {}: {}\\n\"", ".", "format", "(", "k", ",", "v", ")", "\n", "", "logger", ".", "critical", "(", "config_str", ")", "\n", "\n", "device", "=", "torch", ".", "device", "(", "\"cuda\"", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "else", "\"cpu\"", ")", "\n", "logger", ".", "critical", "(", "\"Device: {}\"", ".", "format", "(", "device", ".", "type", ")", ")", "\n", "\n", "# Make agent and target agent", "\n", "agent", "=", "get_model", "(", "config", ".", "agent", ")", "(", "config", ")", "\n", "target_agent", "=", "get_model", "(", "config", ".", "agent", ")", "(", "config", ")", "\n", "target_agent", ".", "load_state_dict", "(", "agent", ".", "state_dict", "(", ")", ")", "\n", "agent", ".", "to", "(", "device", ")", "\n", "target_agent", ".", "to", "(", "device", ")", "\n", "\n", "# Pseudocount stuff", "\n", "count_model", "=", "None", "\n", "if", "config", ".", "count_rewards", ":", "\n", "        ", "if", "config", ".", "atari_count", ":", "\n", "            ", "count_model", "=", "AtariCount", "(", "config", ")", "\n", "", "elif", "config", ".", "rnd_net_count", ":", "\n", "# assert config.count_state_only_rewards", "\n", "            ", "count_model", "=", "RndNetworkDistill", "(", "config", ",", "device", ")", "\n", "", "elif", "config", ".", "dora_count", ":", "\n", "            ", "count_model", "=", "DoraCount", "(", "config", ",", "device", ")", "\n", "", "else", ":", "\n", "            ", "count_model", "=", "PseudoCount", "(", "config", ")", "\n", "\n", "# Make action selector", "\n", "", "", "action_selector", "=", "None", "\n", "if", "config", ".", "action_selector", "==", "\"eps_greedy\"", ":", "\n", "        ", "action_selector", "=", "eps_greedy", ".", "EpsGreedy", "(", "config", ")", "\n", "", "elif", "config", ".", "action_selector", "==", "\"optimistic_action\"", ":", "\n", "        ", "action_selector", "=", "optimistic_action", ".", "OptimisticAction", "(", "count_model", ",", "config", ")", "\n", "", "elif", "config", ".", "action_selector", "==", "\"bsp\"", ":", "\n", "        ", "action_selector", "=", "bsp_action", ".", "BSPAction", "(", "config", ")", "\n", "", "else", ":", "\n", "        ", "raise", "Exception", "(", "\"{} is not an Action Selector!\"", ".", "format", "(", "config", ".", "action_selector", ")", ")", "\n", "\n", "# Make replay buffer", "\n", "# Check if the obs dtype of the environment is an int", "\n", "", "obs_dtype", "=", "getattr", "(", "env", ".", "wrapped_env", ",", "\"obs_dtype\"", ",", "np", ".", "float32", ")", "\n", "obs_scaling", "=", "getattr", "(", "env", ".", "wrapped_env", ",", "\"obs_scaling\"", ",", "1", ")", "\n", "replay_buffer", "=", "ReplayBuffer", "(", "\n", "size", "=", "config", ".", "buffer_size", ",", "frame_history_len", "=", "config", ".", "past_frames_input", ",", "obs_dtype", "=", "obs_dtype", ",", "obs_scaling", "=", "obs_scaling", ",", "args", "=", "config", "\n", ")", "\n", "\n", "if", "config", ".", "dora_count", ":", "\n", "        ", "dora_buffer", "=", "ReplayBuffer", "(", "\n", "size", "=", "config", ".", "batch_size", "*", "4", ",", "frame_history_len", "=", "config", ".", "past_frames_input", ",", "obs_dtype", "=", "obs_dtype", ",", "obs_scaling", "=", "obs_scaling", ",", "args", "=", "config", "\n", ")", "\n", "\n", "# Make trainer", "\n", "", "trainer", "=", "None", "\n", "if", "config", ".", "trainer", "==", "\"DQN\"", ":", "\n", "        ", "trainer", "=", "DQNTrainer", "(", "agent", "=", "agent", ",", "target_agent", "=", "target_agent", ",", "args", "=", "config", ",", "count_model", "=", "count_model", ",", "buffer", "=", "replay_buffer", ")", "\n", "", "else", ":", "\n", "        ", "raise", "Exception", "\n", "", "testing_buffer", "=", "ReplayBuffer", "(", "\n", "size", "=", "(", "config", ".", "past_frames_input", "+", "1", ")", ",", "frame_history_len", "=", "config", ".", "past_frames_input", ",", "args", "=", "config", "\n", ")", "\n", "\n", "# Testing stuff", "\n", "testing_env", "=", "EnvWrapper", "(", "env", "=", "gym", ".", "make", "(", "config", ".", "env", ")", ",", "debug", "=", "True", ",", "args", "=", "config", ")", "\n", "if", "config", ".", "test_augmented", ":", "\n", "        ", "assert", "config", ".", "action_selector", "==", "\"optimistic_action\"", "\n", "\n", "# Player Positions", "\n", "", "positions", "=", "set", "(", ")", "\n", "action_positions", "=", "set", "(", ")", "\n", "\n", "T", "=", "0", "\n", "start_time", "=", "time", ".", "time", "(", ")", "\n", "last_time", "=", "start_time", "\n", "\n", "# Lots of code duplication :(", "\n", "logging", ".", "critical", "(", "\"Filling buffer with {:,} random experiences.\"", ".", "format", "(", "config", ".", "buffer_burn_in", ")", ")", "\n", "state", "=", "env", ".", "reset", "(", ")", "\n", "assert", "config", ".", "buffer_burn_in", "==", "0", "\n", "for", "t", "in", "range", "(", "config", ".", "buffer_burn_in", ")", ":", "\n", "        ", "buffer_idx", "=", "replay_buffer", ".", "store_frame", "(", "state", ")", "\n", "stacked_states", "=", "replay_buffer", ".", "encode_recent_observation", "(", ")", "\n", "tensor_state", "=", "torch", ".", "tensor", "(", "stacked_states", ",", "device", "=", "device", ")", ".", "unsqueeze", "(", "0", ")", "\n", "action", "=", "np", ".", "random", ".", "randint", "(", "config", ".", "num_actions", ")", "\n", "next_state", ",", "reward", ",", "terminated", ",", "info", "=", "env", ".", "step", "(", "action", ")", "\n", "terminal_to_store", "=", "terminated", "\n", "if", "\"Steps_Termination\"", "in", "info", "and", "info", "[", "\"Steps_Termination\"", "]", ":", "\n", "            ", "terminal_to_store", "=", "False", "\n", "\n", "", "intrinsic_reward", "=", "0", "\n", "pseudo_count", "=", "0", "\n", "if", "config", ".", "count_rewards", ":", "\n", "            ", "pseudo_count", "=", "count_model", ".", "visit", "(", "tensor_state", ",", "action", ")", "\n", "if", "getattr", "(", "count_model", ",", "\"reward_directly\"", ",", "False", ")", ":", "\n", "                ", "intrinsic_reward", "=", "pseudo_count", "\n", "", "else", ":", "\n", "                ", "count_bonus", "=", "config", ".", "count_beta", "/", "sqrt", "(", "pseudo_count", ")", "\n", "intrinsic_reward", "=", "count_bonus", "\n", "\n", "", "", "replay_buffer", ".", "store_effect", "(", "buffer_idx", ",", "action", ",", "reward", "-", "config", ".", "reward_baseline", ",", "intrinsic_reward", ",", "terminal_to_store", ",", "pseudo_count", ")", "\n", "state", "=", "next_state", "\n", "if", "terminated", ":", "\n", "            ", "state", "=", "env", ".", "reset", "(", ")", "\n", "logger", ".", "warning", "(", "\"Random action burn in t: {:,}\"", ".", "format", "(", "t", ")", ")", "\n", "\n", "", "", "state", "=", "env", ".", "reset", "(", ")", "\n", "episode", "=", "0", "\n", "episode_reward", "=", "0", "\n", "intrinsic_episode_reward", "=", "0", "\n", "episode_length", "=", "0", "\n", "env_positive_reward", "=", "0", "\n", "max_episode_reward", "=", "0", "\n", "if", "config", ".", "bsp", ":", "\n", "        ", "bsp_k", "=", "np", ".", "random", ".", "randint", "(", "config", ".", "bsp_k", ")", "\n", "action_selector", ".", "update_k", "(", "bsp_k", ")", "\n", "\n", "", "logging", ".", "critical", "(", "\"Beginning training.\"", ")", "\n", "\n", "while", "T", "<", "config", ".", "t_max", ":", "\n", "\n", "# Store the current state", "\n", "        ", "buffer_idx", "=", "replay_buffer", ".", "store_frame", "(", "state", ")", "\n", "if", "config", ".", "dora_count", ":", "\n", "            ", "dora_idx", "=", "dora_buffer", ".", "store_frame", "(", "state", ")", "\n", "\n", "# Get the stacked input vector", "\n", "", "stacked_states", "=", "replay_buffer", ".", "encode_recent_observation", "(", ")", "\n", "\n", "# Get output from agent", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "tensor_state", "=", "torch", ".", "tensor", "(", "stacked_states", ",", "device", "=", "device", ")", ".", "unsqueeze", "(", "0", ")", "\n", "agent_output", "=", "agent", "(", "tensor_state", ")", "\n", "# agent_output = agent(torch.Tensor(stacked_states).unsqueeze(0))", "\n", "\n", "# Select action", "\n", "", "action", ",", "action_info", "=", "action_selector", ".", "select_actions", "(", "\n", "agent_output", ",", "T", ",", "info", "=", "{", "\"state\"", ":", "tensor_state", "}", "\n", ")", "\n", "\n", "# Take an environment step", "\n", "next_state", ",", "reward", ",", "terminated", ",", "info", "=", "env", ".", "step", "(", "action", ")", "\n", "T", "+=", "1", "\n", "stats", ".", "update_t", "(", "T", ")", "\n", "episode_reward", "+=", "reward", "\n", "episode_length", "+=", "1", "\n", "terminal_to_store", "=", "terminated", "\n", "if", "\"Steps_Termination\"", "in", "info", "and", "info", "[", "\"Steps_Termination\"", "]", ":", "\n", "            ", "logger", ".", "warning", "(", "\"Terminating because of episode limit\"", ")", "\n", "terminal_to_store", "=", "False", "\n", "\n", "# Log if a positive reward was ever received from environment. ~Finding goal", "\n", "", "if", "reward", ">", "0.1", ":", "\n", "            ", "env_positive_reward", "=", "1", "\n", "", "stats", ".", "update_stats", "(", "\"Positive_Reward\"", ",", "env_positive_reward", ")", "\n", "\n", "# Calculate count based intrinsic motivation", "\n", "intrinsic_reward", "=", "0", "\n", "pseudo_count", "=", "0", "\n", "if", "config", ".", "count_rewards", ":", "\n", "            ", "pseudo_count", "=", "count_model", ".", "visit", "(", "tensor_state", ",", "action", ")", "\n", "if", "getattr", "(", "count_model", ",", "\"reward_directly\"", ",", "False", ")", ":", "\n", "# The count-model is giving us the intrinsic reward directly", "\n", "                ", "intrinsic_reward", "=", "pseudo_count", "[", "0", "]", "\n", "", "else", ":", "\n", "# Count-model is giving us the pseudo-count", "\n", "                ", "count_bonus", "=", "config", ".", "count_beta", "/", "sqrt", "(", "pseudo_count", ")", "\n", "intrinsic_reward", "=", "count_bonus", "\n", "", "intrinsic_episode_reward", "+=", "intrinsic_reward", "\n", "\n", "# Render training", "\n", "", "if", "config", ".", "render_train_env", ":", "\n", "            ", "debug_info", "=", "{", "}", "\n", "debug_info", ".", "update", "(", "action_info", ")", "\n", "env", ".", "render", "(", "debug_info", "=", "debug_info", ")", "\n", "\n", "# Add what happened to the buffer", "\n", "", "replay_buffer", ".", "store_effect", "(", "buffer_idx", ",", "action", ",", "reward", "-", "config", ".", "reward_baseline", ",", "intrinsic_reward", ",", "terminal_to_store", ",", "pseudo_count", ")", "\n", "if", "config", ".", "dora_count", ":", "\n", "            ", "dora_buffer", ".", "store_effect", "(", "dora_idx", ",", "action", ",", "reward", "-", "config", ".", "reward_baseline", ",", "intrinsic_reward", ",", "terminal_to_store", ",", "pseudo_count", ")", "\n", "\n", "# Update state", "\n", "", "state", "=", "next_state", "\n", "\n", "# If terminated", "\n", "if", "terminated", ":", "\n", "# If we terminated due to episode limit, we need to add the current state in", "\n", "            ", "if", "\"Steps_Termination\"", "in", "info", "and", "info", "[", "\"Steps_Termination\"", "]", ":", "\n", "                ", "buffer_idx", "=", "replay_buffer", ".", "store_frame", "(", "state", ")", "\n", "replay_buffer", ".", "store_effect", "(", "buffer_idx", ",", "0", ",", "0", ",", "0", ",", "True", ",", "0", ",", "dont_sample", "=", "True", ")", "\n", "if", "config", ".", "dora_count", ":", "\n", "                    ", "dora_idx", "=", "dora_buffer", ".", "store_frame", "(", "state", ")", "\n", "dora_buffer", ".", "store_effect", "(", "dora_idx", ",", "0", ",", "0", ",", "0", ",", "True", ",", "0", ",", "dont_sample", "=", "True", ")", "\n", "\n", "", "", "logger", ".", "warning", "(", "\"T: {:,}, Episode Reward: {:.2f}\"", ".", "format", "(", "T", ",", "episode_reward", ")", ")", "\n", "state", "=", "env", ".", "reset", "(", ")", "\n", "max_episode_reward", "=", "max", "(", "max_episode_reward", ",", "episode_reward", ")", "\n", "stats", ".", "update_stats", "(", "\"Episode Reward\"", ",", "episode_reward", ")", "\n", "stats", ".", "update_stats", "(", "\"Max Episode Reward\"", ",", "max_episode_reward", ")", "\n", "stats", ".", "update_stats", "(", "\"Episode Length\"", ",", "episode_length", ")", "\n", "stats", ".", "update_stats", "(", "\"Intrin Eps Reward\"", ",", "intrinsic_episode_reward", ")", "\n", "episode_reward", "=", "0", "\n", "episode_length", "=", "0", "\n", "intrinsic_episode_reward", "=", "0", "\n", "episode", "+=", "1", "\n", "stats", ".", "update_stats", "(", "\"Episode\"", ",", "episode", ")", "\n", "if", "config", ".", "bsp", ":", "\n", "                ", "bsp_k", "=", "np", ".", "random", ".", "randint", "(", "config", ".", "bsp_k", ")", "\n", "action_selector", ".", "update_k", "(", "bsp_k", ")", "\n", "\n", "# Train if possible", "\n", "", "", "for", "_", "in", "range", "(", "config", ".", "training_iters", ")", ":", "\n", "            ", "sampled_batch", "=", "None", "\n", "\n", "if", "T", "%", "config", ".", "update_freq", "!=", "0", ":", "\n", "# Only train every update_freq timesteps", "\n", "                ", "continue", "\n", "", "if", "replay_buffer", ".", "can_sample", "(", "config", ".", "batch_size", ")", ":", "\n", "                ", "sampled_batch", "=", "replay_buffer", ".", "sample", "(", "config", ".", "batch_size", ",", "nstep", "=", "config", ".", "n_step", ")", "\n", "\n", "", "if", "sampled_batch", "is", "not", "None", ":", "\n", "                ", "trainer", ".", "train", "(", "sampled_batch", ")", "\n", "\n", "", "if", "config", ".", "dora_count", ":", "\n", "                ", "if", "dora_buffer", ".", "can_sample", "(", "config", ".", "batch_size", ")", ":", "\n", "                    ", "sampled_batch", "=", "replay_buffer", ".", "sample", "(", "config", ".", "batch_size", ",", "nstep", "=", "config", ".", "n_step", ")", "\n", "", "if", "sampled_batch", "is", "not", "None", ":", "\n", "                    ", "count_model", ".", "train", "(", "sampled_batch", ")", "\n", "\n", "# Update target networks if necessary", "\n", "", "", "", "if", "T", "%", "config", ".", "target_update_interval", "==", "0", ":", "\n", "            ", "trainer", ".", "update_target_agent", "(", ")", "\n", "if", "config", ".", "dora_count", ":", "\n", "                ", "count_model", ".", "update_target_agent", "(", ")", "\n", "\n", "# Logging", "\n", "", "", "if", "config", ".", "bsp", ":", "\n", "            ", "agent_output", "=", "agent_output", "[", ":", ",", ":", ",", "bsp_k", "]", "\n", "", "q_vals_numpy", "=", "agent_output", ".", "detach", "(", ")", ".", "cpu", "(", ")", "[", "0", "]", ".", "numpy", "(", ")", "\n", "if", "num_actions", "<", "20", ":", "\n", "            ", "for", "action_id", "in", "range", "(", "config", ".", "num_actions", ")", ":", "\n", "                ", "stats", ".", "update_stats", "(", "\"Q-Value_{}\"", ".", "format", "(", "action_id", ")", ",", "q_vals_numpy", "[", "action_id", "]", ")", "\n", "", "", "else", ":", "\n", "            ", "stats", ".", "update_stats", "(", "\"Q-Value_Mean\"", ",", "np", ".", "mean", "(", "q_vals_numpy", ")", ")", "\n", "", "player_pos", "=", "env", ".", "log_visitation", "(", ")", "\n", "positions", ".", "add", "(", "player_pos", ")", "\n", "action_positions", ".", "add", "(", "(", "player_pos", ",", "action", ")", ")", "\n", "stats", ".", "update_stats", "(", "\"States Visited\"", ",", "len", "(", "positions", ")", ")", "\n", "stats", ".", "update_stats", "(", "\"State_Actions Visited\"", ",", "len", "(", "action_positions", ")", ")", "\n", "stats", ".", "update_stats", "(", "\"Player Position\"", ",", "player_pos", ")", "\n", "# Log all env stats returned", "\n", "for", "k", ",", "v", "in", "info", ".", "items", "(", ")", ":", "\n", "            ", "if", "k", "!=", "\"Steps_Termination\"", ":", "\n", "                ", "stats", ".", "update_stats", "(", "k", ",", "v", ")", "\n", "\n", "", "", "if", "config", ".", "save_count_gifs", ">", "0", "and", "T", "%", "config", ".", "save_count_gifs", "==", "0", ":", "\n", "            ", "if", "count_model", "is", "not", "None", ":", "\n", "                ", "state_action_counts", ",", "count_nums", "=", "env", ".", "count_state_action_space", "(", "count_model", ")", "\n", "if", "state_action_counts", "is", "not", "None", ":", "\n", "                    ", "save_image", "(", "state_action_counts", ",", "image_name", "=", "\"SA_Counts__{}_Size__{}_T\"", ".", "format", "(", "config", ".", "count_size", ",", "T", ")", ",", "direc_name", "=", "\"State_Action_Counts\"", ")", "\n", "save_sa_count_vals", "(", "count_nums", ",", "name", "=", "\"SA_PCounts__{}_Size__{}_T\"", ".", "format", "(", "config", ".", "count_size", ",", "T", ")", ",", "direc_name", "=", "\"Sa_Count_Estimates\"", ")", "\n", "\n", "", "actual_counts", "=", "env", ".", "state_counts", "(", ")", "\n", "if", "actual_counts", "is", "not", "None", ":", "\n", "                    ", "save_actual_counts", "(", "actual_counts", ",", "name", "=", "\"Counts__{}_T\"", ".", "format", "(", "T", ")", ",", "direc_name", "=", "\"Actual_Counts\"", ")", "\n", "\n", "", "q_val_img", ",", "q_vals", "=", "env", ".", "q_value_estimates", "(", "count_model", ",", "agent", ")", "\n", "if", "q_val_img", "is", "not", "None", ":", "\n", "                    ", "save_image", "(", "q_val_img", ",", "image_name", "=", "\"Q_Vals__{}_Size__{}_T\"", ".", "format", "(", "config", ".", "count_size", ",", "T", ")", ",", "direc_name", "=", "\"Q_Value_Estimates\"", ")", "\n", "", "if", "q_vals", "is", "not", "None", ":", "\n", "                    ", "save_q_vals", "(", "q_vals", ",", "name", "=", "\"Q_Vals__{}_Size__{}_T\"", ".", "format", "(", "config", ".", "count_size", ",", "T", ")", ",", "direc_name", "=", "\"Q_Value_Estimates\"", ")", "\n", "\n", "# Testing", "\n", "", "", "", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "if", "T", "%", "config", ".", "testing_interval", "==", "0", ":", "\n", "\n", "                ", "prefixes", "=", "[", "\"\"", "]", "\n", "if", "config", ".", "test_augmented", ":", "\n", "                    ", "prefixes", "+=", "[", "\"Aug_\"", "]", "\n", "\n", "", "for", "prefix", "in", "prefixes", ":", "\n", "                    ", "total_test_reward", "=", "0", "\n", "total_test_length", "=", "0", "\n", "for", "_", "in", "range", "(", "config", ".", "test_episodes", ")", ":", "\n", "                        ", "test_episode_reward", "=", "0", "\n", "test_episode_length", "=", "0", "\n", "test_state", "=", "testing_env", ".", "reset", "(", ")", "\n", "test_env_terminated", "=", "False", "\n", "\n", "while", "not", "test_env_terminated", ":", "\n", "                            ", "test_buffer_idx", "=", "testing_buffer", ".", "store_frame", "(", "test_state", ")", "\n", "stacked_test_states", "=", "testing_buffer", ".", "encode_recent_observation", "(", ")", "\n", "test_tensor_state", "=", "torch", ".", "tensor", "(", "stacked_test_states", ",", "device", "=", "device", ")", ".", "unsqueeze", "(", "0", ")", "\n", "testing_agent_output", "=", "agent", "(", "test_tensor_state", ")", "\n", "\n", "if", "prefix", "==", "\"Aug_\"", "or", "config", ".", "bsp", ":", "\n", "                                ", "test_action", ",", "_", "=", "action_selector", ".", "select_actions", "(", "testing_agent_output", ",", "T", ",", "info", "=", "{", "\"state\"", ":", "test_tensor_state", "}", ",", "testing", "=", "True", ")", "\n", "", "else", ":", "\n", "                                ", "test_action", "=", "get_test_action", "(", "testing_agent_output", ",", "config", ")", "\n", "\n", "", "next_test_state", ",", "test_reward", ",", "test_env_terminated", ",", "_", "=", "testing_env", ".", "step", "(", "test_action", ")", "\n", "if", "config", ".", "render_test_env", ":", "\n", "                                ", "testing_env", ".", "render", "(", ")", "\n", "\n", "", "test_episode_length", "+=", "1", "\n", "test_episode_reward", "+=", "test_reward", "\n", "\n", "testing_buffer", ".", "store_effect", "(", "\n", "test_buffer_idx", ",", "test_action", ",", "test_reward", ",", "0", ",", "test_env_terminated", ",", "0", "\n", ")", "\n", "\n", "test_state", "=", "next_test_state", "\n", "\n", "", "total_test_length", "+=", "test_episode_length", "\n", "total_test_reward", "+=", "test_episode_reward", "\n", "\n", "", "mean_test_reward", "=", "total_test_reward", "/", "config", ".", "test_episodes", "\n", "mean_test_length", "=", "total_test_length", "/", "config", ".", "test_episodes", "\n", "\n", "logger", ".", "error", "(", "\n", "\"{}Testing -- T: {:,}/{:,}, Test Reward: {:.2f}, Test Length: {:,}\"", ".", "format", "(", "\n", "prefix", ",", "T", ",", "config", ".", "t_max", ",", "mean_test_reward", ",", "mean_test_length", "\n", ")", "\n", ")", "\n", "\n", "stats", ".", "update_stats", "(", "\"{}Test Reward\"", ".", "format", "(", "prefix", ")", ",", "mean_test_reward", ",", "always_log", "=", "True", ")", "\n", "stats", ".", "update_stats", "(", "\n", "\"{}Test Episode Length\"", ".", "format", "(", "prefix", ")", ",", "mean_test_length", ",", "always_log", "=", "True", "\n", ")", "\n", "\n", "", "logger", ".", "error", "(", "\"Estimated time left: {}. Time passed: {}\"", ".", "format", "(", "\n", "time_left", "(", "last_time", ",", "T", "-", "config", ".", "testing_interval", ",", "T", ",", "config", ".", "t_max", ")", ",", "\n", "time_str", "(", "time", ".", "time", "(", ")", "-", "start_time", ")", ")", ")", "\n", "last_time", "=", "time", ".", "time", "(", ")", "\n", "\n", "", "", "if", "T", "%", "(", "config", ".", "log_interval", "*", "4", ")", "==", "0", ":", "\n", "            ", "stats", ".", "print_stats", "(", ")", "\n", "\n", "", "", "logger", ".", "critical", "(", "\"Closing envs\"", ")", "\n", "env", ".", "close", "(", ")", "\n", "testing_env", ".", "close", "(", ")", "\n", "\n", "logger", ".", "critical", "(", "\"Finished training.\"", ")", "\n", "\n", "if", "client", "is", "not", "None", ":", "\n", "        ", "logger", ".", "critical", "(", "\"Attempting to close pymongo client\"", ")", "\n", "client", ".", "close", "(", ")", "\n", "logger", ".", "critical", "(", "\"Pymongo client closed\"", ")", "\n", "\n", "", "logger", ".", "critical", "(", "\"Exiting\"", ")", "", "", ""]], "home.repos.pwc.inspect_result.oxwhirl_opiq.utils.logging.Stats.__init__": [[15, 36], ["logging.getLogger", "collections.defaultdict", "collections.defaultdict", "collections.defaultdict", "logging.Stats.logger.error", "logging.Stats.stats[].append", "logging.Stats.stats[].append", "collections.deque", "collections.deque"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "log_interval", ",", "sacred_info", ",", "use_tb", ")", ":", "\n", "        ", "self", ".", "T", "=", "0", "\n", "self", ".", "logger", "=", "logging", ".", "getLogger", "(", "\"stats\"", ")", "\n", "\n", "# Stats", "\n", "self", ".", "stats", "=", "defaultdict", "(", "lambda", ":", "deque", "(", "maxlen", "=", "20", ")", ")", "\n", "self", ".", "stats_ts", "=", "defaultdict", "(", "lambda", ":", "deque", "(", "maxlen", "=", "20", ")", ")", "\n", "\n", "self", ".", "log_t", "=", "defaultdict", "(", "lambda", ":", "0", ")", "\n", "self", ".", "log_interval", "=", "log_interval", "\n", "self", ".", "logger", ".", "error", "(", "\"Logging every {} steps\"", ".", "format", "(", "log_interval", ")", ")", "\n", "\n", "# TB", "\n", "self", ".", "tb", "=", "use_tb", "\n", "\n", "# Sacred", "\n", "self", ".", "sacred_info", "=", "sacred_info", "\n", "\n", "# Fill in defaults for Episode", "\n", "self", ".", "stats", "[", "\"Episode\"", "]", ".", "append", "(", "0", ")", "\n", "self", ".", "stats", "[", "\"Episode_T\"", "]", ".", "append", "(", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.utils.logging.Stats.update_stats": [[37, 60], ["logging.Stats.stats[].append", "logging.Stats.stats_ts[].append", "logging.Stats.sacred_info[].append", "logging.Stats.sacred_info[].append", "tensorboard_logger.log_value", "type"], "methods", ["None"], ["", "def", "update_stats", "(", "self", ",", "key", ",", "value", ",", "always_log", "=", "False", ")", ":", "\n", "\n", "        ", "if", "value", "is", "None", ":", "\n", "            ", "return", "\n", "\n", "", "if", "(", "not", "always_log", "and", "self", ".", "T", "-", "self", ".", "log_t", "[", "key", "]", ">=", "self", ".", "log_interval", ")", "or", "always_log", ":", "\n", "# Tb", "\n", "            ", "if", "self", ".", "tb", ":", "\n", "                ", "if", "not", "type", "(", "value", ")", "is", "tuple", ":", "\n", "                    ", "log_value", "(", "name", "=", "key", ",", "value", "=", "value", ",", "step", "=", "self", ".", "T", ")", "\n", "\n", "# Sacred", "\n", "", "", "if", "key", "in", "self", ".", "sacred_info", ":", "\n", "                ", "self", ".", "sacred_info", "[", "key", "+", "\"_T\"", "]", ".", "append", "(", "self", ".", "T", ")", "\n", "self", ".", "sacred_info", "[", "key", "]", ".", "append", "(", "value", ")", "\n", "", "else", ":", "\n", "                ", "self", ".", "sacred_info", "[", "key", "]", "=", "[", "value", "]", "\n", "self", ".", "sacred_info", "[", "key", "+", "\"_T\"", "]", "=", "[", "self", ".", "T", "]", "\n", "\n", "", "self", ".", "log_t", "[", "key", "]", "=", "self", ".", "T", "\n", "\n", "", "self", ".", "stats", "[", "key", "]", ".", "append", "(", "value", ")", "\n", "self", ".", "stats_ts", "[", "key", "]", ".", "append", "(", "self", ".", "T", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.utils.logging.Stats.update_t": [[61, 63], ["None"], "methods", ["None"], ["", "def", "update_t", "(", "self", ",", "T", ")", ":", "\n", "        ", "self", ".", "T", "=", "T", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.utils.logging.Stats.print_stats": [[64, 76], ["sorted", "logging.Stats.logger.error", "logging.Stats.stats.items", "numpy.mean"], "methods", ["None"], ["", "def", "print_stats", "(", "self", ")", ":", "\n", "        ", "log_str", "=", "\"Recent Stats | T: {:,} | Episode: {:,}\\n\"", ".", "format", "(", "self", ".", "T", ",", "self", ".", "stats", "[", "\"Episode\"", "]", "[", "-", "1", "]", ")", "\n", "i", "=", "0", "\n", "for", "(", "k", ",", "v", ")", "in", "sorted", "(", "self", ".", "stats", ".", "items", "(", ")", ")", ":", "\n", "            ", "if", "k", "==", "\"Episode\"", "or", "k", "==", "\"Player Position\"", ":", "\n", "                ", "continue", "\n", "", "i", "+=", "1", "\n", "# window = 5 if k != \"epsilon\" else 1", "\n", "item", "=", "\"{:.4f}\"", ".", "format", "(", "np", ".", "mean", "(", "[", "x", "for", "x", "in", "self", ".", "stats", "[", "k", "]", "]", ")", ")", "\n", "log_str", "+=", "\"{:<25}{:>8}\"", ".", "format", "(", "k", "+", "\":\"", ",", "item", ")", "\n", "log_str", "+=", "\"\\n\"", "if", "i", "%", "4", "==", "0", "else", "\"\\t\"", "\n", "", "self", ".", "logger", ".", "error", "(", "log_str", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.utils.logging.configure_stats_logging": [[83, 91], ["logging.Stats", "datetime.datetime.now().strftime", "tensorboard_logger.configure", "datetime.datetime.now"], "function", ["None"], ["def", "configure_stats_logging", "(", "name", ",", "log_interval", ",", "sacred_info", ",", "use_tb", ")", "->", "None", ":", "\n", "    ", "global", "stats", "\n", "stats", "=", "Stats", "(", "log_interval", "=", "log_interval", ",", "sacred_info", "=", "sacred_info", ",", "use_tb", "=", "use_tb", ")", "\n", "date", "=", "\"{}\"", ".", "format", "(", "datetime", ".", "datetime", ".", "now", "(", ")", ".", "strftime", "(", "\"%Y-%m-%d_%H-%M\"", ")", ")", "\n", "global", "run_results_name", "\n", "run_results_name", "=", "\"{}__{}\"", ".", "format", "(", "name", ",", "date", ")", "\n", "if", "use_tb", ":", "\n", "        ", "configure", "(", "\"results/tb/{}\"", ".", "format", "(", "run_results_name", ")", ",", "flush_secs", "=", "10", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.utils.logging.save_image": [[93, 102], ["stats.logger.critical", "os.makedirs", "PIL.Image.fromarray", "Image.fromarray.save", "image.astype"], "function", ["None"], ["", "", "def", "save_image", "(", "image", ",", "image_name", ",", "direc_name", "=", "\"all\"", ",", ")", ":", "\n", "    ", "global", "run_results_name", "\n", "global", "stats", "\n", "stats", ".", "logger", ".", "critical", "(", "\"Saving state-action counts: {} in directory {}\"", ".", "format", "(", "image_name", ",", "direc_name", ")", ")", "\n", "results_directory", "=", "\"results/images/{}/{}\"", ".", "format", "(", "run_results_name", ",", "direc_name", ")", "\n", "os", ".", "makedirs", "(", "results_directory", ",", "exist_ok", "=", "True", ")", "\n", "\n", "img", "=", "Image", ".", "fromarray", "(", "image", ".", "astype", "(", "np", ".", "uint8", ")", ")", "\n", "img", ".", "save", "(", "\"{}/{}.png\"", ".", "format", "(", "results_directory", ",", "image_name", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.utils.logging.save_q_vals": [[104, 112], ["stats.logger.critical", "os.makedirs", "numpy.savetxt"], "function", ["None"], ["", "def", "save_q_vals", "(", "q_vals", ",", "name", ",", "direc_name", "=", "\"all\"", ")", ":", "\n", "    ", "global", "run_results_name", "\n", "global", "stats", "\n", "stats", ".", "logger", ".", "critical", "(", "\"Saving Q-Values: {} in directory {}\"", ".", "format", "(", "name", ",", "direc_name", ")", ")", "\n", "results_directory", "=", "\"results/q_values/{}/{}\"", ".", "format", "(", "run_results_name", ",", "direc_name", ")", "\n", "os", ".", "makedirs", "(", "results_directory", ",", "exist_ok", "=", "True", ")", "\n", "\n", "np", ".", "savetxt", "(", "\"{}/{}.txt\"", ".", "format", "(", "results_directory", ",", "name", ")", ",", "q_vals", "[", ":", ",", ":", ",", "0", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.utils.logging.save_actual_counts": [[114, 122], ["stats.logger.critical", "os.makedirs", "numpy.savetxt"], "function", ["None"], ["", "def", "save_actual_counts", "(", "counts", ",", "name", ",", "direc_name", "=", "\"all\"", ")", ":", "\n", "    ", "global", "run_results_name", "\n", "global", "stats", "\n", "stats", ".", "logger", ".", "critical", "(", "\"Saving Counts: {} in directory {}\"", ".", "format", "(", "name", ",", "direc_name", ")", ")", "\n", "results_directory", "=", "\"results/actual_counts/{}/{}\"", ".", "format", "(", "run_results_name", ",", "direc_name", ")", "\n", "os", ".", "makedirs", "(", "results_directory", ",", "exist_ok", "=", "True", ")", "\n", "\n", "np", ".", "savetxt", "(", "\"{}/{}.txt\"", ".", "format", "(", "results_directory", ",", "name", ")", ",", "counts", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.utils.logging.save_sa_count_vals": [[124, 132], ["stats.logger.critical", "os.makedirs", "numpy.savetxt"], "function", ["None"], ["", "def", "save_sa_count_vals", "(", "c_vals", ",", "name", ",", "direc_name", "=", "\"all\"", ")", ":", "\n", "    ", "global", "run_results_name", "\n", "global", "stats", "\n", "stats", ".", "logger", ".", "critical", "(", "\"Saving SA PCounts: {} in directory {}\"", ".", "format", "(", "name", ",", "direc_name", ")", ")", "\n", "results_directory", "=", "\"results/sa_count_p_values/{}/{}\"", ".", "format", "(", "run_results_name", ",", "direc_name", ")", "\n", "os", ".", "makedirs", "(", "results_directory", ",", "exist_ok", "=", "True", ")", "\n", "\n", "np", ".", "savetxt", "(", "\"{}/{}.txt\"", ".", "format", "(", "results_directory", ",", "name", ")", ",", "c_vals", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.utils.logging.get_stats": [[134, 137], ["None"], "function", ["None"], ["", "def", "get_stats", "(", ")", "->", "Stats", ":", "\n", "    ", "global", "stats", "\n", "return", "stats", "\n", "", ""]], "home.repos.pwc.inspect_result.oxwhirl_opiq.utils.dict2namedtuple.convert": [[4, 6], ["collections.namedtuple", "dictionary.keys"], "function", ["None"], ["def", "convert", "(", "dictionary", ")", ":", "\n", "    ", "return", "namedtuple", "(", "\"GenericDict\"", ",", "dictionary", ".", "keys", "(", ")", ")", "(", "**", "dictionary", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.oxwhirl_opiq.utils.timehelper.print_time": [[5, 15], ["max", "min", "print", "time.time", "len", "numpy.mean", "timehelper.time_str", "timehelper.time_str"], "function", ["home.repos.pwc.inspect_result.oxwhirl_opiq.utils.timehelper.time_str", "home.repos.pwc.inspect_result.oxwhirl_opiq.utils.timehelper.time_str"], ["def", "print_time", "(", "start_time", ",", "T", ",", "t_max", ",", "episode", ",", "episode_rewards", ")", ":", "\n", "    ", "time_elapsed", "=", "time", ".", "time", "(", ")", "-", "start_time", "\n", "T", "=", "max", "(", "1", ",", "T", ")", "\n", "time_left", "=", "time_elapsed", "*", "(", "t_max", "-", "T", ")", "/", "T", "\n", "# Just in case its over 100 days", "\n", "time_left", "=", "min", "(", "time_left", ",", "60", "*", "60", "*", "24", "*", "100", ")", "\n", "last_reward", "=", "\"N\\A\"", "\n", "if", "len", "(", "episode_rewards", ")", ">", "5", ":", "\n", "        ", "last_reward", "=", "\"{:.2f}\"", ".", "format", "(", "np", ".", "mean", "(", "episode_rewards", "[", "-", "50", ":", "]", ")", ")", "\n", "", "print", "(", "\"\\033[F\\033[F\\x1b[KEp: {:,}, T: {:,}/{:,}, Reward: {}, \\n\\x1b[KElapsed: {}, Left: {}\\n\"", ".", "format", "(", "episode", ",", "T", ",", "t_max", ",", "last_reward", ",", "time_str", "(", "time_elapsed", ")", ",", "time_str", "(", "time_left", ")", ")", ",", "\" \"", "*", "10", ",", "end", "=", "\"\\r\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.utils.timehelper.time_left": [[16, 25], ["max", "min", "timehelper.time_str", "time.time"], "function", ["home.repos.pwc.inspect_result.oxwhirl_opiq.utils.timehelper.time_str"], ["", "def", "time_left", "(", "start_time", ",", "t_start", ",", "t_current", ",", "t_max", ")", ":", "\n", "    ", "if", "t_current", ">=", "t_max", ":", "\n", "        ", "return", "\"-\"", "\n", "", "time_elapsed", "=", "time", ".", "time", "(", ")", "-", "start_time", "\n", "t_current", "=", "max", "(", "1", ",", "t_current", ")", "\n", "time_left", "=", "time_elapsed", "*", "(", "t_max", "-", "t_current", ")", "/", "(", "t_current", "-", "t_start", ")", "\n", "# Just in case its over 100 days", "\n", "time_left", "=", "min", "(", "time_left", ",", "60", "*", "60", "*", "24", "*", "100", ")", "\n", "return", "time_str", "(", "time_left", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.utils.timehelper.time_str": [[26, 42], ["divmod", "divmod", "divmod", "int", "int", "int", "int", "timehelper.time_left", "timehelper.time_left"], "function", ["home.repos.pwc.inspect_result.oxwhirl_opiq.utils.timehelper.time_left", "home.repos.pwc.inspect_result.oxwhirl_opiq.utils.timehelper.time_left"], ["", "def", "time_str", "(", "s", ")", ":", "\n", "    ", "\"\"\"\n    Convert seconds to a nicer string showing days, hours, minutes and seconds\n    \"\"\"", "\n", "days", ",", "remainder", "=", "divmod", "(", "s", ",", "60", "*", "60", "*", "24", ")", "\n", "hours", ",", "remainder", "=", "divmod", "(", "remainder", ",", "60", "*", "60", ")", "\n", "minutes", ",", "seconds", "=", "divmod", "(", "remainder", ",", "60", ")", "\n", "string", "=", "\"\"", "\n", "if", "days", ">", "0", ":", "\n", "        ", "string", "+=", "\"{:d} days, \"", ".", "format", "(", "int", "(", "days", ")", ")", "\n", "", "if", "hours", ">", "0", ":", "\n", "        ", "string", "+=", "\"{:d} hours, \"", ".", "format", "(", "int", "(", "hours", ")", ")", "\n", "", "if", "minutes", ">", "0", ":", "\n", "        ", "string", "+=", "\"{:d} minutes, \"", ".", "format", "(", "int", "(", "minutes", ")", ")", "\n", "", "string", "+=", "\"{:d} seconds\"", ".", "format", "(", "int", "(", "seconds", ")", ")", "\n", "return", "string", "\n", "", ""]], "home.repos.pwc.inspect_result.oxwhirl_opiq.count.HashCount.HashingBonusEvaluator.__init__": [[16, 39], ["numpy.asarray", "torch.tensor().transpose().to().float", "numpy.zeros", "torch.normal().to", "range", "mods_list.append", "mods.append", "torch.tensor().transpose().to", "len", "numpy.max", "torch.normal", "torch.tensor().transpose", "torch.zeros", "torch.ones", "torch.tensor"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "dim_key", "=", "128", ",", "obs_processed_flat_dim", "=", "None", ",", "bucket_sizes", "=", "None", ",", "actions", "=", "1", ")", ":", "\n", "# Hashing function: SimHash", "\n", "        ", "if", "bucket_sizes", "is", "None", ":", "\n", "# Large prime numbers", "\n", "# bucket_sizes = [999931, 999953, 999959, 999961, 999979, 999983]", "\n", "# Smaller prime numbers to (hopefully) have less precision errors when batching", "\n", "            ", "bucket_sizes", "=", "[", "911", ",", "919", ",", "929", ",", "937", ",", "941", ",", "947", "]", "\n", "", "mods_list", "=", "[", "]", "\n", "for", "bucket_size", "in", "bucket_sizes", ":", "\n", "            ", "mod", "=", "1", "\n", "mods", "=", "[", "]", "\n", "for", "_", "in", "range", "(", "dim_key", ")", ":", "\n", "                ", "mods", ".", "append", "(", "mod", ")", "\n", "mod", "=", "(", "mod", "*", "2", ")", "%", "bucket_size", "\n", "", "mods_list", ".", "append", "(", "mods", ")", "\n", "", "self", ".", "bucket_sizes", "=", "np", ".", "asarray", "(", "bucket_sizes", ")", "\n", "# self.mods_list = np.asarray(mods_list).T", "\n", "self", ".", "mods_list", "=", "torch", ".", "tensor", "(", "mods_list", ")", ".", "transpose", "(", "1", ",", "0", ")", ".", "to", "(", "device", ")", ".", "float", "(", ")", "\n", "self", ".", "tables", "=", "np", ".", "zeros", "(", "(", "actions", ",", "len", "(", "bucket_sizes", ")", ",", "np", ".", "max", "(", "bucket_sizes", ")", ")", ")", "\n", "\n", "# self.projection_matrix = np.random.normal(size=(obs_processed_flat_dim, dim_key))", "\n", "self", ".", "projection_matrix", "=", "torch", ".", "normal", "(", "mean", "=", "torch", ".", "zeros", "(", "size", "=", "(", "obs_processed_flat_dim", ",", "dim_key", ")", ")", ",", "\n", "std", "=", "torch", ".", "ones", "(", "size", "=", "(", "1", ",", "obs_processed_flat_dim", ",", "dim_key", ")", ")", ")", ".", "to", "(", "device", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.count.HashCount.HashingBonusEvaluator.project": [[40, 42], ["torch.sign().float", "torch.sign"], "methods", ["None"], ["", "def", "project", "(", "self", ",", "obss", ")", ":", "\n", "        ", "return", "torch", ".", "sign", "(", "obss", "@", "self", ".", "projection_matrix", ")", ".", "float", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.count.HashCount.HashingBonusEvaluator.compute_keys": [[43, 49], ["torch.sign().float", "torch.sign"], "methods", ["None"], ["", "def", "compute_keys", "(", "self", ",", "obss", ")", ":", "\n", "        ", "binaries", "=", "torch", ".", "sign", "(", "obss", "@", "self", ".", "projection_matrix", ")", ".", "float", "(", ")", "\n", "# binaries = np.sign(np.asarray(obss).dot(self.projection_matrix))", "\n", "keys", "=", "np", ".", "cast", "[", "'int'", "]", "(", "(", "binaries", "@", "self", ".", "mods_list", ")", ".", "cpu", "(", ")", ")", "%", "self", ".", "bucket_sizes", "\n", "# keys = np.cast['int'](binaries.dot(self.mods_list)) % self.bucket_sizes", "\n", "return", "keys", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.count.HashCount.HashingBonusEvaluator.inc_hash": [[50, 54], ["HashCount.HashingBonusEvaluator.compute_keys", "range", "len", "numpy.add.at"], "methods", ["home.repos.pwc.inspect_result.oxwhirl_opiq.count.HashCount.HashingBonusEvaluator.compute_keys"], ["", "def", "inc_hash", "(", "self", ",", "obss", ",", "action", ")", ":", "\n", "        ", "keys", "=", "self", ".", "compute_keys", "(", "obss", ")", "\n", "for", "idx", "in", "range", "(", "len", "(", "self", ".", "bucket_sizes", ")", ")", ":", "\n", "            ", "np", ".", "add", ".", "at", "(", "self", ".", "tables", "[", "action", ",", "idx", "]", ",", "keys", "[", ":", ",", "idx", "]", ",", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.count.HashCount.HashingBonusEvaluator.query_hash": [[55, 61], ["HashCount.HashingBonusEvaluator.compute_keys", "range", "numpy.asarray().min", "len", "all_counts.append", "numpy.asarray"], "methods", ["home.repos.pwc.inspect_result.oxwhirl_opiq.count.HashCount.HashingBonusEvaluator.compute_keys"], ["", "", "def", "query_hash", "(", "self", ",", "obss", ",", "action", ")", ":", "\n", "        ", "keys", "=", "self", ".", "compute_keys", "(", "obss", ")", "\n", "all_counts", "=", "[", "]", "\n", "for", "idx", "in", "range", "(", "len", "(", "self", ".", "bucket_sizes", ")", ")", ":", "\n", "            ", "all_counts", ".", "append", "(", "self", ".", "tables", "[", "action", ",", "idx", ",", "keys", "[", ":", ",", "idx", "]", "]", ")", "\n", "", "return", "np", ".", "asarray", "(", "all_counts", ")", ".", "min", "(", "axis", "=", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.count.HashCount.HashingBonusEvaluator.query_all_actions": [[62, 68], ["HashCount.HashingBonusEvaluator.compute_keys", "range", "numpy.asarray().min", "len", "all_counts.append", "numpy.asarray"], "methods", ["home.repos.pwc.inspect_result.oxwhirl_opiq.count.HashCount.HashingBonusEvaluator.compute_keys"], ["", "def", "query_all_actions", "(", "self", ",", "obss", ")", ":", "\n", "        ", "keys", "=", "self", ".", "compute_keys", "(", "obss", ")", "\n", "all_counts", "=", "[", "]", "\n", "for", "idx", "in", "range", "(", "len", "(", "self", ".", "bucket_sizes", ")", ")", ":", "\n", "            ", "all_counts", ".", "append", "(", "self", ".", "tables", "[", ":", ",", "idx", ",", "keys", "[", ":", ",", "idx", "]", "]", ")", "\n", "", "return", "np", ".", "asarray", "(", "all_counts", ")", ".", "min", "(", "axis", "=", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.count.rnd_count.RndCount.__init__": [[8, 18], ["numpy.random.normal", "collections.defaultdict", "numpy.prod", "range"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "self", ".", "config", "=", "config", "\n", "\n", "self", ".", "num_actions", "=", "self", ".", "config", ".", "num_actions", "\n", "\n", "self", ".", "state_action_counters", "=", "[", "defaultdict", "(", "lambda", ":", "0", ")", "for", "_", "in", "range", "(", "self", ".", "num_actions", ")", "]", "\n", "\n", "self", ".", "count_embedding", "=", "config", ".", "count_size", "\n", "matrix_size", "=", "(", "np", ".", "prod", "(", "self", ".", "config", ".", "state_shape", ")", ",", "self", ".", "count_embedding", ")", "\n", "self", ".", "rnd_matrix", "=", "np", ".", "random", ".", "normal", "(", "0", ",", "1", ",", "size", "=", "matrix_size", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.count.rnd_count.RndCount.get_count": [[19, 32], ["numpy.sign().astype", "tuple", "numpy.sign", "state.flatten().dot", "state.flatten"], "methods", ["None"], ["", "def", "get_count", "(", "self", ",", "state", ",", "action", "=", "0", ",", "visit", "=", "False", ")", ":", "\n", "        ", "if", "not", "self", ".", "config", ".", "count_state_action", ":", "\n", "            ", "action", "=", "0", "\n", "\n", "", "features", "=", "np", ".", "sign", "(", "state", ".", "flatten", "(", ")", ".", "dot", "(", "self", ".", "rnd_matrix", ")", ")", ".", "astype", "(", "np", ".", "int16", ")", "\n", "features_key", "=", "tuple", "(", "features", ")", "\n", "pseudo_count", "=", "self", ".", "state_action_counters", "[", "action", "]", "[", "features_key", "]", "\n", "\n", "if", "visit", ":", "\n", "            ", "self", ".", "state_action_counters", "[", "action", "]", "[", "features_key", "]", "=", "pseudo_count", "+", "1", "\n", "pseudo_count", "+=", "1", "\n", "\n", "", "return", "pseudo_count", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.count.rnd_count.RndCount.visit": [[33, 35], ["rnd_count.RndCount.get_count"], "methods", ["home.repos.pwc.inspect_result.oxwhirl_opiq.count.dora_count.DoraCount.get_count"], ["", "def", "visit", "(", "self", ",", "state", ",", "action", "=", "0", ")", ":", "\n", "        ", "return", "self", ".", "get_count", "(", "state", ",", "action", ",", "visit", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.count.rnd_count.ApproxRndCount.__init__": [[39, 48], ["HashCount.HashingBonusEvaluator", "HashCount.HashingBonusEvaluator", "utils.logging.get_stats", "numpy.prod", "numpy.prod", "len"], "methods", ["home.repos.pwc.inspect_result.oxwhirl_opiq.utils.logging.get_stats"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "# self.hash_counts = [HashingBonusEvaluator(dim_key=config.count_size, obs_processed_flat_dim=np.prod(config.state_shape)) for _ in range(config.num_actions)]", "\n", "        ", "self", ".", "hash_counts", "=", "HashingBonusEvaluator", "(", "dim_key", "=", "config", ".", "count_size", ",", "obs_processed_flat_dim", "=", "np", ".", "prod", "(", "config", ".", "state_shape", ")", ",", "actions", "=", "config", ".", "num_actions", ")", "\n", "self", ".", "hash_state_count", "=", "HashingBonusEvaluator", "(", "dim_key", "=", "config", ".", "count_size", ",", "obs_processed_flat_dim", "=", "np", ".", "prod", "(", "config", ".", "state_shape", ")", ",", "actions", "=", "1", ")", "\n", "self", ".", "config", "=", "config", "\n", "\n", "self", ".", "flatten", "=", "True", "if", "len", "(", "config", ".", "state_shape", ")", ">", "1", "else", "False", "\n", "\n", "self", ".", "stats", "=", "get_stats", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.count.rnd_count.ApproxRndCount.get_count": [[49, 73], ["state.reshape.reshape.reshape", "rnd_count.ApproxRndCount.hash_counts.inc_hash", "rnd_count.ApproxRndCount.hash_state_count.query_hash", "rnd_count.ApproxRndCount.hash_counts.query_hash", "len", "rnd_count.ApproxRndCount.hash_state_count.inc_hash"], "methods", ["home.repos.pwc.inspect_result.oxwhirl_opiq.count.HashCount.HashingBonusEvaluator.inc_hash", "home.repos.pwc.inspect_result.oxwhirl_opiq.count.HashCount.HashingBonusEvaluator.query_hash", "home.repos.pwc.inspect_result.oxwhirl_opiq.count.HashCount.HashingBonusEvaluator.query_hash", "home.repos.pwc.inspect_result.oxwhirl_opiq.count.HashCount.HashingBonusEvaluator.inc_hash"], ["", "def", "get_count", "(", "self", ",", "state", ",", "action", "=", "0", ",", "visit", "=", "False", ")", ":", "\n", "        ", "if", "not", "self", ".", "config", ".", "count_state_action", ":", "\n", "            ", "action", "=", "0", "\n", "\n", "", "if", "len", "(", "state", ".", "shape", ")", ">=", "3", "and", "state", ".", "shape", "[", "1", "]", ">", "1", ":", "\n", "            ", "state", "=", "state", "[", ":", ",", "-", "1", "]", "\n", "\n", "# Must do hashing and key computation on the cpu in order to ensure we don't incur errors leading to wildly wrong counts", "\n", "# state = state.to(\"cpu\")", "\n", "# if len(state.shape) == 1:", "\n", "#     state = [state]", "\n", "", "if", "self", ".", "flatten", ":", "\n", "# state = state.flatten()", "\n", "# state = np.asarray(state)", "\n", "            ", "state", "=", "state", ".", "reshape", "(", "state", ".", "shape", "[", "0", "]", ",", "-", "1", ")", "\n", "", "if", "visit", ":", "\n", "            ", "self", ".", "hash_counts", ".", "inc_hash", "(", "state", ",", "action", ")", "\n", "if", "self", ".", "config", ".", "count_state_only_rewards", ":", "\n", "                ", "self", ".", "hash_state_count", ".", "inc_hash", "(", "state", ",", "0", ")", "\n", "\n", "", "", "if", "self", ".", "config", ".", "count_state_only_rewards", ":", "\n", "            ", "return", "self", ".", "hash_state_count", ".", "query_hash", "(", "state", ",", "0", ")", "\n", "", "else", ":", "\n", "            ", "return", "self", ".", "hash_counts", ".", "query_hash", "(", "state", ",", "action", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.count.rnd_count.ApproxRndCount.get_all_action_counts": [[74, 81], ["rnd_count.ApproxRndCount.hash_counts.query_all_actions", "state.reshape.reshape.reshape", "len"], "methods", ["home.repos.pwc.inspect_result.oxwhirl_opiq.count.HashCount.HashingBonusEvaluator.query_all_actions"], ["", "", "def", "get_all_action_counts", "(", "self", ",", "state", ")", ":", "\n", "        ", "if", "len", "(", "state", ".", "shape", ")", ">=", "3", "and", "state", ".", "shape", "[", "1", "]", ">", "1", ":", "\n", "            ", "state", "=", "state", "[", ":", ",", "-", "1", "]", "\n", "", "if", "self", ".", "flatten", ":", "\n", "# state = np.asarray(state)", "\n", "            ", "state", "=", "state", ".", "reshape", "(", "state", ".", "shape", "[", "0", "]", ",", "-", "1", ")", "\n", "", "return", "self", ".", "hash_counts", ".", "query_all_actions", "(", "state", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.count.rnd_count.ApproxRndCount.visit": [[82, 86], ["rnd_count.ApproxRndCount.get_count", "rnd_count.ApproxRndCount.stats.update_stats"], "methods", ["home.repos.pwc.inspect_result.oxwhirl_opiq.count.dora_count.DoraCount.get_count", "home.repos.pwc.inspect_result.oxwhirl_opiq.utils.logging.Stats.update_stats"], ["", "def", "visit", "(", "self", ",", "state", ",", "action", "=", "0", ")", ":", "\n", "        ", "visit_counts", "=", "self", ".", "get_count", "(", "state", ",", "action", ",", "visit", "=", "True", ")", "\n", "self", ".", "stats", ".", "update_stats", "(", "\"Pseudo Counts\"", ",", "visit_counts", "[", "0", "]", ")", "\n", "return", "visit_counts", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.count.atari_count.AtariCount.__init__": [[8, 21], ["utils.logging.get_stats", "collections.defaultdict", "range"], "methods", ["home.repos.pwc.inspect_result.oxwhirl_opiq.utils.logging.get_stats"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "self", ".", "config", "=", "config", "\n", "\n", "self", ".", "num_actions", "=", "self", ".", "config", ".", "num_actions", "\n", "\n", "self", ".", "state_action_counters", "=", "[", "defaultdict", "(", "lambda", ":", "0", ")", "for", "_", "in", "range", "(", "self", ".", "num_actions", ")", "]", "\n", "\n", "self", ".", "target_shape", "=", "(", "config", ".", "atari_target_x", ",", "config", ".", "atari_target_y", ")", "\n", "self", ".", "max_pix_value", "=", "255", "\n", "\n", "self", ".", "stats", "=", "get_stats", "(", ")", "\n", "\n", "self", ".", "hash_vector", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.count.atari_count.AtariCount.get_count": [[22, 38], ["atari_count.AtariCount.tensor_hash().item", "atari_count.AtariCount.convert_state", "len", "atari_count.AtariCount.tensor_hash"], "methods", ["home.repos.pwc.inspect_result.oxwhirl_opiq.count.atari_count.AtariCount.convert_state", "home.repos.pwc.inspect_result.oxwhirl_opiq.count.atari_count.AtariCount.tensor_hash"], ["", "def", "get_count", "(", "self", ",", "state", ",", "action", "=", "0", ",", "visit", "=", "False", ")", ":", "\n", "        ", "if", "not", "self", ".", "config", ".", "count_state_action", ":", "\n", "            ", "action", "=", "0", "\n", "\n", "", "if", "len", "(", "state", ".", "shape", ")", ">=", "4", "and", "state", ".", "shape", "[", "1", "]", ">", "1", ":", "\n", "            ", "state", "=", "state", "[", ":", ",", "-", "1", "]", "\n", "\n", "", "abs_state", "=", "self", ".", "convert_state", "(", "state", ")", "[", "0", "]", "\n", "features_key", "=", "self", ".", "tensor_hash", "(", "abs_state", ")", ".", "item", "(", ")", "\n", "pseudo_count", "=", "self", ".", "state_action_counters", "[", "action", "]", "[", "features_key", "]", "\n", "\n", "if", "visit", ":", "\n", "            ", "self", ".", "state_action_counters", "[", "action", "]", "[", "features_key", "]", "=", "pseudo_count", "+", "1", "\n", "pseudo_count", "+=", "1", "\n", "\n", "", "return", "pseudo_count", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.count.atari_count.AtariCount.get_all_action_counts": [[39, 53], ["numpy.zeros", "atari_count.AtariCount.convert_state", "atari_count.AtariCount.tensor_hash().tolist", "enumerate", "range", "len", "atari_count.AtariCount.tensor_hash"], "methods", ["home.repos.pwc.inspect_result.oxwhirl_opiq.count.atari_count.AtariCount.convert_state", "home.repos.pwc.inspect_result.oxwhirl_opiq.count.atari_count.AtariCount.tensor_hash"], ["", "def", "get_all_action_counts", "(", "self", ",", "states", ")", ":", "\n", "        ", "counts", "=", "np", ".", "zeros", "(", "shape", "=", "(", "self", ".", "num_actions", ",", "states", ".", "shape", "[", "0", "]", ")", ")", "\n", "if", "len", "(", "states", ".", "shape", ")", ">=", "4", "and", "states", ".", "shape", "[", "1", "]", ">", "1", ":", "\n", "            ", "states", "=", "states", "[", ":", ",", "-", "1", "]", "\n", "# Get features for all states", "\n", "", "abs_states", "=", "self", ".", "convert_state", "(", "states", ")", "\n", "hashed_states", "=", "self", ".", "tensor_hash", "(", "abs_states", ")", ".", "tolist", "(", ")", "\n", "for", "si", ",", "s", "in", "enumerate", "(", "hashed_states", ")", ":", "\n", "            ", "for", "a", "in", "range", "(", "self", ".", "num_actions", ")", ":", "\n", "# features_key = s.item()", "\n", "                ", "features_key", "=", "s", "\n", "pseudo_count", "=", "self", ".", "state_action_counters", "[", "a", "]", "[", "features_key", "]", "\n", "counts", "[", "a", ",", "si", "]", "=", "pseudo_count", "\n", "", "", "return", "counts", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.count.atari_count.AtariCount.visit": [[54, 58], ["atari_count.AtariCount.get_count", "atari_count.AtariCount.stats.update_stats"], "methods", ["home.repos.pwc.inspect_result.oxwhirl_opiq.count.dora_count.DoraCount.get_count", "home.repos.pwc.inspect_result.oxwhirl_opiq.utils.logging.Stats.update_stats"], ["", "def", "visit", "(", "self", ",", "state", ",", "action", "=", "0", ")", ":", "\n", "        ", "visit_count", "=", "self", ".", "get_count", "(", "state", ",", "action", ",", "visit", "=", "True", ")", "\n", "self", ".", "stats", ".", "update_stats", "(", "\"Pseudo Counts\"", ",", "visit_count", ")", "\n", "return", "visit_count", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.count.atari_count.AtariCount.convert_state": [[59, 65], ["torch.nn.functional.interpolate", "state.detach().unsqueeze", "state.detach"], "methods", ["None"], ["", "def", "convert_state", "(", "self", ",", "state", ")", ":", "\n", "# State is already grayscale", "\n", "# Do this on the cpu if things get weird", "\n", "        ", "resized", "=", "torch", ".", "nn", ".", "functional", ".", "interpolate", "(", "state", ".", "detach", "(", ")", ".", "unsqueeze", "(", "1", ")", ",", "size", "=", "self", ".", "target_shape", ",", "mode", "=", "\"area\"", ")", "[", ":", ",", "0", "]", "\n", "intensity", "=", "(", "resized", "*", "255", "/", "32", ")", ".", "int", "(", ")", "\n", "return", "intensity", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.count.atari_count.AtariCount.tensor_hash": [[66, 78], ["hashes.cpu", "len", "x.size", "torch.randint_like"], "methods", ["None"], ["", "def", "tensor_hash", "(", "self", ",", "x", ")", ":", "\n", "# Hash function to use for content-based hashing of tensors", "\n", "        ", "if", "self", ".", "hash_vector", "is", "None", ":", "\n", "            ", "self", ".", "hash_vector", "=", "(", "torch", ".", "randint_like", "(", "x", ",", "57", ")", "*", "317", ")", ".", "int", "(", ")", "\n", "\n", "", "hashes", "=", "None", "\n", "if", "len", "(", "x", ".", "size", "(", ")", ")", "<", "3", ":", "\n", "            ", "hashes", "=", "(", "self", ".", "hash_vector", "*", "x", ")", ".", "sum", "(", ")", "\n", "", "else", ":", "\n", "            ", "hashes", "=", "(", "self", ".", "hash_vector", "*", "x", ")", ".", "sum", "(", "dim", "=", "1", ")", ".", "sum", "(", "dim", "=", "1", ")", "\n", "\n", "", "return", "hashes", ".", "cpu", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.count.rnd_network_count.RndNetworkDistill.__init__": [[9, 55], ["utils.logging.get_stats", "torch.optim.RMSprop", "rnd_network_count.RndNetworkDistill.predictors[].state_dict", "rnd_network_count.RndNetworkDistill.targets[].state_dict", "n.parameters", "rnd_network_count.RndNetworkDistill.state_predictor.parameters", "range", "range", "range", "range", "p.load_state_dict", "t.load_state_dict", "rnd_network_count.RndNetworkDistill.state_predictor.load_state_dict", "rnd_network_count.RndNetworkDistill.state_target.load_state_dict", "agent.rnd_net.specifier.get_pred", "agent.rnd_net.specifier.get_target", "agent.rnd_net.specifier.get_pred", "agent.rnd_net.specifier.get_target"], "methods", ["home.repos.pwc.inspect_result.oxwhirl_opiq.utils.logging.get_stats", "home.repos.pwc.inspect_result.oxwhirl_opiq.rnd_net.specifier.get_pred", "home.repos.pwc.inspect_result.oxwhirl_opiq.rnd_net.specifier.get_target", "home.repos.pwc.inspect_result.oxwhirl_opiq.rnd_net.specifier.get_pred", "home.repos.pwc.inspect_result.oxwhirl_opiq.rnd_net.specifier.get_target"], ["    ", "def", "__init__", "(", "self", ",", "config", ",", "device", ")", ":", "\n", "        ", "self", ".", "config", "=", "config", "\n", "\n", "# assert self.config.count_state_action # For now", "\n", "num_actions", "=", "config", ".", "num_actions", "\n", "if", "not", "self", ".", "config", ".", "count_state_action", ":", "\n", "            ", "num_actions", "=", "1", "\n", "\n", "", "self", ".", "num_actions", "=", "num_actions", "\n", "\n", "# Maybe the predictors should all start the same and have the same target", "\n", "self", ".", "predictors", "=", "[", "rnd_specifier", ".", "get_pred", "(", "config", ".", "rnd_net_name", ")", "(", "config", ")", ".", "to", "(", "device", ")", "for", "_", "in", "range", "(", "num_actions", ")", "]", "\n", "self", ".", "targets", "=", "[", "rnd_specifier", ".", "get_target", "(", "config", ".", "rnd_net_name", ")", "(", "config", ")", ".", "to", "(", "device", ")", "for", "_", "in", "range", "(", "num_actions", ")", "]", "\n", "\n", "self", ".", "states", "=", "[", "None", "for", "_", "in", "range", "(", "num_actions", ")", "]", "\n", "self", ".", "states_idx", "=", "[", "0", "for", "_", "in", "range", "(", "num_actions", ")", "]", "\n", "\n", "if", "self", ".", "config", ".", "rnd_same_starts", ":", "\n", "            ", "p_dict", "=", "self", ".", "predictors", "[", "0", "]", ".", "state_dict", "(", ")", "\n", "t_dict", "=", "self", ".", "targets", "[", "0", "]", ".", "state_dict", "(", ")", "\n", "for", "p", "in", "self", ".", "predictors", ":", "\n", "                ", "p", ".", "load_state_dict", "(", "p_dict", ")", "\n", "", "for", "t", "in", "self", ".", "targets", ":", "\n", "                ", "t", ".", "load_state_dict", "(", "t_dict", ")", "\n", "\n", "", "", "if", "self", ".", "config", ".", "count_state_only_rewards", ":", "\n", "            ", "self", ".", "state_predictor", "=", "rnd_specifier", ".", "get_pred", "(", "config", ".", "rnd_net_name", ")", "(", "config", ")", ".", "to", "(", "device", ")", "\n", "self", ".", "state_target", "=", "rnd_specifier", ".", "get_target", "(", "config", ".", "rnd_net_name", ")", "(", "config", ")", ".", "to", "(", "device", ")", "\n", "if", "self", ".", "config", ".", "rnd_same_starts", ":", "\n", "                ", "self", ".", "state_predictor", ".", "load_state_dict", "(", "p_dict", ")", "\n", "self", ".", "state_target", ".", "load_state_dict", "(", "t_dict", ")", "\n", "", "self", ".", "state_states", "=", "None", "\n", "self", ".", "state_states_idx", "=", "0", "\n", "\n", "", "self", ".", "stats", "=", "get_stats", "(", ")", "\n", "self", ".", "reward_directly", "=", "True", "\n", "\n", "# Training stuff", "\n", "self", ".", "net_parameters", "=", "[", "]", "\n", "for", "n", "in", "self", ".", "predictors", ":", "\n", "            ", "self", ".", "net_parameters", "+=", "n", ".", "parameters", "(", ")", "\n", "", "if", "self", ".", "config", ".", "count_state_only_rewards", ":", "\n", "            ", "self", ".", "net_parameters", "+=", "self", ".", "state_predictor", ".", "parameters", "(", ")", "\n", "", "self", ".", "optimiser", "=", "torch", ".", "optim", ".", "RMSprop", "(", "params", "=", "self", ".", "net_parameters", ",", "lr", "=", "self", ".", "config", ".", "lr", ")", "\n", "\n", "self", ".", "train_times", "=", "self", ".", "config", ".", "rnd_train_times", "if", "self", ".", "config", ".", "rnd_train_times", ">", "0", "else", "self", ".", "config", ".", "rnd_batch_size", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.count.rnd_network_count.RndNetworkDistill.get_count": [[56, 111], ["reward.to().numpy", "reward.to().numpy", "torch.no_grad", "torch.no_grad", "rnd_network_count.RndNetworkDistill.state_target", "rnd_network_count.RndNetworkDistill.state_predictor", "len", "numpy.random.random", "reward.to", "len", "numpy.random.random", "reward.to", "len", "torch.zeros", "rnd_network_count.RndNetworkDistill.train", "len", "torch.zeros", "rnd_network_count.RndNetworkDistill.train"], "methods", ["home.repos.pwc.inspect_result.oxwhirl_opiq.trainer.dqn_train.DQNTrainer.train", "home.repos.pwc.inspect_result.oxwhirl_opiq.trainer.dqn_train.DQNTrainer.train"], ["", "def", "get_count", "(", "self", ",", "state", ",", "action", "=", "-", "1", ",", "visit", "=", "False", ")", ":", "\n", "        ", "if", "self", ".", "config", ".", "count_state_only_rewards", "and", "action", "==", "-", "1", ":", "\n", "# Lots of code duplication :(", "\n", "            ", "if", "len", "(", "state", ".", "shape", ")", ">=", "4", "and", "state", ".", "shape", "[", "1", "]", ">", "1", ":", "\n", "                ", "state", "=", "state", "[", ":", ",", "-", "1", ":", "]", "\n", "\n", "", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                ", "target_x", "=", "self", ".", "state_target", "(", "state", ")", "\n", "pred_x", "=", "self", ".", "state_predictor", "(", "state", ")", "\n", "\n", "reward", "=", "(", "target_x", "-", "pred_x", ")", ".", "pow", "(", "2", ")", ".", "sum", "(", "dim", "=", "1", ")", "*", "self", ".", "config", ".", "rnd_net_scaler", "\n", "\n", "", "if", "visit", ":", "\n", "                ", "if", "np", ".", "random", ".", "random", "(", ")", "<", "self", ".", "config", ".", "rnd_train_p", ":", "\n", "                    ", "if", "len", "(", "state", ".", "shape", ")", ">=", "4", ":", "\n", "# Remove the batch dimension", "\n", "                        ", "state", "=", "state", "[", "0", "]", "\n", "", "if", "self", ".", "state_states", "is", "None", ":", "\n", "                        ", "self", ".", "state_states", "=", "torch", ".", "zeros", "(", "(", "self", ".", "config", ".", "rnd_batch_size", ",", ")", "+", "state", ".", "shape", ",", "dtype", "=", "torch", ".", "float32", ",", "device", "=", "state", ".", "device", ")", "\n", "", "self", ".", "state_states", "[", "self", ".", "state_states_idx", "]", "=", "state", "\n", "self", ".", "state_states_idx", "+=", "1", "\n", "if", "self", ".", "state_states_idx", "%", "self", ".", "train_times", "==", "0", ":", "\n", "                        ", "self", ".", "train", "(", "action", "=", "-", "1", ")", "\n", "if", "self", ".", "state_states_idx", ">=", "self", ".", "config", ".", "rnd_batch_size", ":", "\n", "                            ", "self", ".", "state_states_idx", "=", "0", "\n", "\n", "", "", "", "", "return", "reward", ".", "to", "(", "\"cpu\"", ")", ".", "numpy", "(", ")", "\n", "\n", "", "if", "not", "self", ".", "config", ".", "count_state_action", ":", "\n", "            ", "action", "=", "0", "\n", "\n", "", "if", "len", "(", "state", ".", "shape", ")", ">=", "4", "and", "state", ".", "shape", "[", "1", "]", ">", "1", ":", "\n", "            ", "state", "=", "state", "[", ":", ",", "-", "1", ":", "]", "\n", "\n", "", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "target_x", "=", "self", ".", "targets", "[", "action", "]", "(", "state", ")", "\n", "pred_x", "=", "self", ".", "predictors", "[", "action", "]", "(", "state", ")", "\n", "\n", "reward", "=", "(", "target_x", "-", "pred_x", ")", ".", "pow", "(", "2", ")", ".", "sum", "(", "dim", "=", "1", ")", "*", "self", ".", "config", ".", "rnd_net_scaler", "\n", "\n", "", "if", "visit", ":", "\n", "            ", "if", "np", ".", "random", ".", "random", "(", ")", "<", "self", ".", "config", ".", "rnd_train_p", ":", "\n", "                ", "if", "len", "(", "state", ".", "shape", ")", ">=", "4", ":", "\n", "# Remove the batch dimension", "\n", "                    ", "state", "=", "state", "[", "0", "]", "\n", "", "if", "self", ".", "states", "[", "action", "]", "is", "None", ":", "\n", "                    ", "self", ".", "states", "[", "action", "]", "=", "torch", ".", "zeros", "(", "(", "self", ".", "config", ".", "rnd_batch_size", ",", ")", "+", "state", ".", "shape", ",", "dtype", "=", "torch", ".", "float32", ",", "device", "=", "state", ".", "device", ")", "\n", "", "self", ".", "states", "[", "action", "]", "[", "self", ".", "states_idx", "[", "action", "]", "]", "=", "state", "\n", "self", ".", "states_idx", "[", "action", "]", "+=", "1", "\n", "if", "self", ".", "states_idx", "[", "action", "]", "%", "self", ".", "train_times", "==", "0", ":", "\n", "                    ", "self", ".", "train", "(", "action", ")", "\n", "if", "self", ".", "states_idx", "[", "action", "]", ">=", "self", ".", "config", ".", "rnd_batch_size", ":", "\n", "                        ", "self", ".", "states_idx", "[", "action", "]", "=", "0", "\n", "\n", "", "", "", "", "return", "reward", ".", "to", "(", "\"cpu\"", ")", ".", "numpy", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.count.rnd_network_count.RndNetworkDistill.train": [[112, 130], ["rnd_network_count.RndNetworkDistill.optimiser.zero_grad", "loss.backward", "torch.nn.utils.clip_grad_norm_", "rnd_network_count.RndNetworkDistill.optimiser.step", "rnd_network_count.RndNetworkDistill.stats.update_stats", "rnd_network_count.RndNetworkDistill.state_predictor", "rnd_network_count.RndNetworkDistill.state_target", "loss.to().item", "loss.to", "rnd_network_count.RndNetworkDistill.detach"], "methods", ["home.repos.pwc.inspect_result.oxwhirl_opiq.envs.gridworld.GridWorld.step", "home.repos.pwc.inspect_result.oxwhirl_opiq.utils.logging.Stats.update_stats"], ["", "def", "train", "(", "self", ",", "action", ")", ":", "\n", "        ", "if", "action", "==", "-", "1", ":", "\n", "            ", "states", "=", "self", ".", "state_states", "\n", "prediction", "=", "self", ".", "state_predictor", "(", "states", ")", "\n", "targets", "=", "self", ".", "state_target", "(", "states", ")", "\n", "", "else", ":", "\n", "            ", "states", "=", "self", ".", "states", "[", "action", "]", "\n", "prediction", "=", "self", ".", "predictors", "[", "action", "]", "(", "states", ")", "\n", "targets", "=", "self", ".", "targets", "[", "action", "]", "(", "states", ")", "\n", "\n", "", "loss", "=", "(", "prediction", "-", "targets", ".", "detach", "(", ")", ")", ".", "pow", "(", "2", ")", ".", "mean", "(", ")", "\n", "\n", "self", ".", "optimiser", ".", "zero_grad", "(", ")", "\n", "loss", ".", "backward", "(", ")", "\n", "grad_norm", "=", "torch", ".", "nn", ".", "utils", ".", "clip_grad_norm_", "(", "self", ".", "net_parameters", ",", "self", ".", "config", ".", "clip_grad_norm", ")", "\n", "self", ".", "optimiser", ".", "step", "(", ")", "\n", "\n", "self", ".", "stats", ".", "update_stats", "(", "\"rnd_loss_{}\"", ".", "format", "(", "action", ")", ",", "loss", ".", "to", "(", "\"cpu\"", ")", ".", "item", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.count.rnd_network_count.RndNetworkDistill.get_all_action_counts": [[131, 138], ["numpy.zeros", "range", "rnd_network_count.RndNetworkDistill.get_count"], "methods", ["home.repos.pwc.inspect_result.oxwhirl_opiq.count.dora_count.DoraCount.get_count"], ["", "def", "get_all_action_counts", "(", "self", ",", "states", ")", ":", "\n", "# Could be sped up", "\n", "        ", "counts", "=", "np", ".", "zeros", "(", "shape", "=", "(", "self", ".", "num_actions", ",", "states", ".", "shape", "[", "0", "]", ")", ")", "\n", "for", "a", "in", "range", "(", "self", ".", "num_actions", ")", ":", "\n", "            ", "sa_counts", "=", "self", ".", "get_count", "(", "states", ",", "a", ",", "visit", "=", "False", ")", "\n", "counts", "[", "a", "]", "=", "sa_counts", "\n", "", "return", "counts", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.count.rnd_network_count.RndNetworkDistill.visit": [[139, 145], ["rnd_network_count.RndNetworkDistill.get_count", "rnd_network_count.RndNetworkDistill.get_count"], "methods", ["home.repos.pwc.inspect_result.oxwhirl_opiq.count.dora_count.DoraCount.get_count", "home.repos.pwc.inspect_result.oxwhirl_opiq.count.dora_count.DoraCount.get_count"], ["", "def", "visit", "(", "self", ",", "state", ",", "action", "=", "0", ")", ":", "\n", "        ", "if", "self", ".", "config", ".", "count_state_only_rewards", "and", "self", ".", "config", ".", "count_state_action", ":", "\n", "# Need to update the count for this action", "\n", "            ", "self", ".", "get_count", "(", "state", ",", "action", ",", "visit", "=", "True", ")", "\n", "action", "=", "-", "1", "\n", "", "return", "self", ".", "get_count", "(", "state", ",", "action", ",", "visit", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.count.dora_count.DoraCount.__init__": [[10, 34], ["dora_count.DoraCount.target_net.load_state_dict", "utils.logging.get_stats", "dora_count.DoraCount.net.parameters", "torch.optim.RMSprop", "dora_count.DoraCount.net.state_dict", "agent.dora.specifier.get_net", "agent.dora.specifier.get_net"], "methods", ["home.repos.pwc.inspect_result.oxwhirl_opiq.utils.logging.get_stats", "home.repos.pwc.inspect_result.oxwhirl_opiq.dora.specifier.get_net", "home.repos.pwc.inspect_result.oxwhirl_opiq.dora.specifier.get_net"], ["    ", "def", "__init__", "(", "self", ",", "config", ",", "device", ")", ":", "\n", "        ", "self", ".", "config", "=", "config", "\n", "self", ".", "args", "=", "config", "# V. Lazy coding", "\n", "\n", "num_actions", "=", "config", ".", "num_actions", "\n", "if", "not", "self", ".", "config", ".", "count_state_action", ":", "\n", "            ", "raise", "Exception", "\n", "\n", "", "self", ".", "num_actions", "=", "num_actions", "\n", "\n", "self", ".", "net", "=", "dora_specifier", "(", "config", ".", "dora_name", ")", "(", "config", ")", ".", "to", "(", "device", ")", "\n", "self", ".", "target_net", "=", "dora_specifier", "(", "config", ".", "dora_name", ")", "(", "config", ")", ".", "to", "(", "device", ")", "\n", "self", ".", "target_net", ".", "load_state_dict", "(", "self", ".", "net", ".", "state_dict", "(", ")", ")", "\n", "\n", "self", ".", "stats", "=", "get_stats", "(", ")", "\n", "self", ".", "reward_directly", "=", "True", "\n", "\n", "self", ".", "train_times", "=", "config", ".", "batch_size", "\n", "self", ".", "states", "=", "None", "\n", "self", ".", "states_idx", "=", "0", "\n", "\n", "self", ".", "device", "=", "device", "\n", "self", ".", "agent_parameters", "=", "self", ".", "net", ".", "parameters", "(", ")", "\n", "self", ".", "optimiser", "=", "torch", ".", "optim", ".", "RMSprop", "(", "params", "=", "self", ".", "agent_parameters", ",", "lr", "=", "self", ".", "config", ".", "lr", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.count.dora_count.DoraCount.update_target_agent": [[35, 37], ["dora_count.DoraCount.target_net.load_state_dict", "dora_count.DoraCount.net.state_dict"], "methods", ["None"], ["", "def", "update_target_agent", "(", "self", ")", ":", "\n", "        ", "self", ".", "target_net", ".", "load_state_dict", "(", "self", ".", "net", ".", "state_dict", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.count.dora_count.DoraCount.get_count": [[38, 44], ["reward.numpy", "torch.no_grad", "dora_count.DoraCount.net", "torch.sqrt().to", "torch.sqrt", "torch.log"], "methods", ["None"], ["", "def", "get_count", "(", "self", ",", "state", ",", "action", "=", "-", "1", ",", "visit", "=", "False", ")", ":", "\n", "        ", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "e_values", "=", "self", ".", "net", "(", "state", ")", "\n", "e_value", "=", "e_values", "[", ":", ",", "action", "]", "\n", "reward", "=", "self", ".", "config", ".", "dora_beta", "/", "torch", ".", "sqrt", "(", "-", "torch", ".", "log", "(", "e_value", ")", ")", ".", "to", "(", "\"cpu\"", ")", "\n", "", "return", "reward", ".", "numpy", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.count.dora_count.DoraCount.train": [[45, 97], ["torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "states.type.type.to", "actions.type.type.to", "terminations.to.to.to", "steps.to.to.to", "n_last_states.type.type.to", "next_actions.type.type.to", "states.type.type.type", "n_last_states.type.type.type", "actions.type.type.type", "next_actions.type.type.type", "dora_count.DoraCount.net", "dora_count.DoraCount.target_net", "dora_count.DoraCount.gather().squeeze", "dora_count.DoraCount.gather().squeeze", "td_error.pow().mean", "dora_count.DoraCount.optimiser.zero_grad", "loss.to.to.backward", "torch.nn.utils.clip_grad_norm_", "dora_count.DoraCount.optimiser.step", "loss.to.to.to", "dora_count.DoraCount.stats.update_stats", "targets.detach", "loss.to.to.item", "dora_count.DoraCount.gather", "dora_count.DoraCount.gather", "td_error.pow", "steps.to.to.float", "actions.type.type.unsqueeze", "next_actions.type.type.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.oxwhirl_opiq.envs.gridworld.GridWorld.step", "home.repos.pwc.inspect_result.oxwhirl_opiq.utils.logging.Stats.update_stats"], ["", "def", "train", "(", "self", ",", "batch", ")", ":", "\n", "        ", "states", ",", "actions", ",", "rewards", ",", "intrinsic_rewards", ",", "next_states", ",", "terminations", ",", "extra_info", "=", "batch", "\n", "\n", "states", "=", "torch", ".", "from_numpy", "(", "states", ")", "\n", "actions", "=", "torch", ".", "from_numpy", "(", "actions", ")", "\n", "n_last_states", "=", "torch", ".", "from_numpy", "(", "extra_info", "[", "\"last_states\"", "]", ")", "\n", "terminations", "=", "torch", ".", "from_numpy", "(", "extra_info", "[", "\"dones\"", "]", ")", "\n", "steps", "=", "torch", ".", "from_numpy", "(", "extra_info", "[", "\"steps\"", "]", ")", "\n", "next_actions", "=", "torch", ".", "from_numpy", "(", "extra_info", "[", "\"sarsa_actions\"", "]", ")", "\n", "\n", "device", "=", "self", ".", "device", "\n", "\n", "# Move them over", "\n", "states", "=", "states", ".", "to", "(", "device", ")", "\n", "actions", "=", "actions", ".", "to", "(", "device", ")", "\n", "terminations", "=", "terminations", ".", "to", "(", "device", ")", "\n", "steps", "=", "steps", ".", "to", "(", "device", ")", "\n", "n_last_states", "=", "n_last_states", ".", "to", "(", "device", ")", "\n", "next_actions", "=", "next_actions", ".", "to", "(", "device", ")", "\n", "\n", "# Change dtypes", "\n", "states", "=", "states", ".", "type", "(", "torch", ".", "float32", ")", "\n", "n_last_states", "=", "n_last_states", ".", "type", "(", "torch", ".", "float32", ")", "\n", "actions", "=", "actions", ".", "type", "(", "torch", ".", "long", ")", "\n", "next_actions", "=", "next_actions", ".", "type", "(", "torch", ".", "long", ")", "\n", "\n", "terminations", "=", "terminations", "[", ":", ",", "0", "]", "\n", "steps", "=", "steps", "[", ":", ",", "0", "]", "\n", "\n", "states", ".", "requires_grad", "=", "True", "\n", "q_values", "=", "self", ".", "net", "(", "states", ")", "\n", "target_agent_q_values", "=", "self", ".", "target_net", "(", "n_last_states", ")", "\n", "\n", "taken_q_value", "=", "q_values", ".", "gather", "(", "dim", "=", "1", ",", "index", "=", "actions", ".", "unsqueeze", "(", "1", ")", ")", ".", "squeeze", "(", "1", ")", "\n", "taken_target_q_value", "=", "target_agent_q_values", ".", "gather", "(", "dim", "=", "1", ",", "index", "=", "next_actions", ".", "unsqueeze", "(", "1", ")", ")", ".", "squeeze", "(", "1", ")", "\n", "\n", "# n-step Q-Learning target", "\n", "targets", "=", "(", "self", ".", "args", ".", "gamma", "**", "steps", ".", "float", "(", ")", ")", "*", "(", "1", "-", "terminations", ")", "*", "taken_target_q_value", "\n", "\n", "td_error", "=", "taken_q_value", "-", "targets", ".", "detach", "(", ")", "\n", "\n", "loss", "=", "td_error", ".", "pow", "(", "2", ")", ".", "mean", "(", ")", "\n", "\n", "self", ".", "optimiser", ".", "zero_grad", "(", ")", "\n", "loss", ".", "backward", "(", ")", "\n", "grad_norm", "=", "torch", ".", "nn", ".", "utils", ".", "clip_grad_norm_", "(", "self", ".", "agent_parameters", ",", "self", ".", "args", ".", "clip_grad_norm", ")", "\n", "self", ".", "optimiser", ".", "step", "(", ")", "\n", "\n", "# Move stuff back to cpu for logging", "\n", "loss", "=", "loss", ".", "to", "(", "\"cpu\"", ")", "\n", "\n", "self", ".", "stats", ".", "update_stats", "(", "\"Dora_Loss\"", ",", "loss", ".", "item", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.count.dora_count.DoraCount.get_all_action_counts": [[98, 102], ["dora_count.DoraCount.net", "final_values.transpose", "torch.sqrt", "torch.log().clamp_", "torch.log"], "methods", ["None"], ["", "def", "get_all_action_counts", "(", "self", ",", "states", ")", ":", "\n", "        ", "e_values", "=", "self", ".", "net", "(", "states", ")", "\n", "final_values", "=", "self", ".", "config", ".", "dora_beta", "/", "torch", ".", "sqrt", "(", "torch", ".", "log", "(", "e_values", ")", ".", "clamp_", "(", "0.00001", ",", "0.99999", ")", ")", "\n", "return", "final_values", ".", "transpose", "(", "0", ",", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.count.dora_count.DoraCount.visit": [[103, 105], ["dora_count.DoraCount.get_count"], "methods", ["home.repos.pwc.inspect_result.oxwhirl_opiq.count.dora_count.DoraCount.get_count"], ["", "def", "visit", "(", "self", ",", "state", ",", "action", "=", "0", ")", ":", "\n", "        ", "return", "self", ".", "get_count", "(", "state", ",", "action", ",", "visit", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.buffer.buffer.ReplayBuffer.__init__": [[36, 82], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "size", ",", "frame_history_len", ",", "obs_dtype", "=", "np", ".", "float32", ",", "obs_scaling", "=", "1.0", ",", "args", "=", "None", ")", ":", "\n", "        ", "\"\"\"This is a memory efficient implementation of the replay buffer.\n        The sepecific memory optimizations use here are:\n            - only store each frame once rather than k times\n              even if every observation normally consists of k last frames\n            - store frames as np.uint8 (actually it is most time-performance\n              to cast them back to float32 on GPU to minimize memory transfer\n              time)\n            - store frame_t and frame_(t+1) in the same buffer.\n        For the tipical use case in Atari Deep RL buffer with 1M frames the total\n        memory footprint of this buffer is 10^6 * 84 * 84 bytes ~= 7 gigabytes\n        Warning! Assumes that returning frame of zeros at the beginning\n        of the episode, when there is less frames than `frame_history_len`,\n        is acceptable.\n        Parameters\n        ----------\n        size: int\n            Max number of transitions to store in the buffer. When the buffer\n            overflows the old memories are dropped.\n        frame_history_len: int\n            Number of memories to be retried for each observation.\n        \"\"\"", "\n", "self", ".", "size", "=", "size", "\n", "self", ".", "frame_history_len", "=", "frame_history_len", "\n", "self", ".", "args", "=", "args", "\n", "\n", "self", ".", "next_idx", "=", "0", "\n", "self", ".", "num_in_buffer", "=", "0", "\n", "\n", "self", ".", "obs", "=", "None", "\n", "self", ".", "action", "=", "None", "\n", "self", ".", "reward", "=", "None", "\n", "self", ".", "intrinsic_reward", "=", "None", "\n", "self", ".", "done", "=", "None", "\n", "self", ".", "dont_sample", "=", "None", "\n", "\n", "self", ".", "obs_dtype", "=", "obs_dtype", "\n", "self", ".", "obs_scaling", "=", "obs_scaling", "\n", "\n", "self", ".", "bsp", "=", "args", ".", "bsp", "\n", "if", "self", ".", "bsp", ":", "\n", "            ", "self", ".", "bsp_w", "=", "None", "\n", "\n", "", "self", ".", "mmc", "=", "args", ".", "mmc", "\n", "if", "self", ".", "mmc", ":", "\n", "            ", "self", ".", "mmc_v", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.buffer.buffer.ReplayBuffer.can_sample": [[83, 86], ["None"], "methods", ["None"], ["", "", "def", "can_sample", "(", "self", ",", "batch_size", ")", ":", "\n", "        ", "\"\"\"Returns true if `batch_size` different transitions can be sampled from the buffer.\"\"\"", "\n", "return", "batch_size", "+", "1", "<=", "self", ".", "num_in_buffer", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.buffer.buffer.ReplayBuffer._encode_sample": [[87, 164], ["numpy.concatenate", "len", "numpy.concatenate", "numpy.array", "numpy.zeros", "numpy.ones", "numpy.zeros", "numpy.zeros", "numpy.zeros", "numpy.zeros", "numpy.zeros", "numpy.zeros", "enumerate", "numpy.zeros", "numpy.zeros", "range", "buffer.ReplayBuffer._encode_observation", "buffer.ReplayBuffer._encode_observation", "buffer.ReplayBuffer._encode_observation"], "methods", ["home.repos.pwc.inspect_result.oxwhirl_opiq.buffer.buffer.ReplayBuffer._encode_observation", "home.repos.pwc.inspect_result.oxwhirl_opiq.buffer.buffer.ReplayBuffer._encode_observation", "home.repos.pwc.inspect_result.oxwhirl_opiq.buffer.buffer.ReplayBuffer._encode_observation"], ["", "def", "_encode_sample", "(", "self", ",", "idxes", ",", "nstep", "=", "1", ")", ":", "\n", "        ", "obs_batch", "=", "np", ".", "concatenate", "(", "\n", "[", "self", ".", "_encode_observation", "(", "idx", ")", "[", "None", "]", "for", "idx", "in", "idxes", "]", ",", "0", "\n", ")", "\n", "act_batch", "=", "self", ".", "action", "[", "idxes", "]", "\n", "rew_batch", "=", "self", ".", "reward", "[", "idxes", "]", "\n", "int_rew_batch", "=", "self", ".", "intrinsic_reward", "[", "idxes", "]", "\n", "\n", "if", "nstep", "==", "1", ":", "\n", "            ", "next_obs_batch", "=", "np", ".", "concatenate", "(", "\n", "[", "self", ".", "_encode_observation", "(", "idx", "+", "1", ")", "[", "None", "]", "for", "idx", "in", "idxes", "]", ",", "0", "\n", ")", "\n", "done_mask", "=", "np", ".", "array", "(", "\n", "[", "1.0", "if", "self", ".", "done", "[", "idx", "]", "else", "0.0", "for", "idx", "in", "idxes", "]", ",", "dtype", "=", "np", ".", "float32", "\n", ")", "\n", "", "else", ":", "\n", "            ", "next_obs_batch", "=", "None", "\n", "done_mask", "=", "None", "\n", "\n", "", "bs", "=", "len", "(", "idxes", ")", "\n", "extra_info", "=", "{", "}", "\n", "if", "nstep", ">", "1", ":", "\n", "            ", "steps", "=", "np", ".", "zeros", "(", "shape", "=", "(", "bs", ",", "1", ")", ",", "dtype", "=", "np", ".", "uint8", ")", "\n", "done_nstep", "=", "np", ".", "ones", "(", "shape", "=", "(", "bs", ",", "1", ")", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "reward_nstep", "=", "np", ".", "zeros", "(", "shape", "=", "(", "bs", ",", "nstep", ")", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "intrin_reward_nstep", "=", "np", ".", "zeros", "(", "shape", "=", "(", "bs", ",", "nstep", ")", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "\n", "next_state_shape", "=", "(", "bs", ",", "nstep", ",", ")", "+", "self", ".", "state_shape", "\n", "next_state_nstep", "=", "np", ".", "zeros", "(", "shape", "=", "next_state_shape", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "last_states", "=", "np", ".", "zeros", "(", "shape", "=", "(", "bs", ",", ")", "+", "self", ".", "state_shape", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "next_actions_nstep", "=", "np", ".", "zeros", "(", "shape", "=", "(", "bs", ",", "nstep", ")", ",", "dtype", "=", "np", ".", "uint8", ")", "\n", "sarsa_actions", "=", "np", ".", "zeros", "(", "shape", "=", "(", "bs", ",", ")", ",", "dtype", "=", "np", ".", "uint8", ")", "\n", "if", "self", ".", "bsp", ":", "\n", "                ", "bsp_ws", "=", "np", ".", "zeros", "(", "shape", "=", "(", "bs", ",", "self", ".", "args", ".", "bsp_k", ")", ")", "\n", "", "if", "self", ".", "mmc", ":", "\n", "                ", "mmc_vs", "=", "np", ".", "zeros", "(", "shape", "=", "(", "bs", ",", ")", ")", "\n", "\n", "", "for", "e", ",", "idx", "in", "enumerate", "(", "idxes", ")", ":", "\n", "\n", "                ", "steps_to_use", "=", "1", "\n", "\n", "while", "idx", "+", "steps_to_use", "<=", "self", ".", "num_in_buffer", "-", "2", "and", "not", "self", ".", "done", "[", "idx", "+", "steps_to_use", "-", "1", "]", "and", "not", "self", ".", "dont_sample", "[", "(", "idx", "+", "steps_to_use", ")", "%", "self", ".", "size", "]", "and", "steps_to_use", "<", "nstep", ":", "\n", "                    ", "steps_to_use", "+=", "1", "\n", "\n", "", "reward_nstep", "[", "e", ",", ":", "steps_to_use", "]", "=", "self", ".", "reward", "[", "idx", ":", "idx", "+", "steps_to_use", "]", "\n", "intrin_reward_nstep", "[", "e", ",", ":", "steps_to_use", "]", "=", "self", ".", "intrinsic_reward", "[", "idx", ":", "idx", "+", "steps_to_use", "]", "\n", "next_actions_nstep", "[", "e", ",", ":", "steps_to_use", "]", "=", "self", ".", "action", "[", "idx", ":", "idx", "+", "steps_to_use", "]", "\n", "sarsa_actions", "[", "e", "]", "=", "self", ".", "action", "[", "idx", "+", "steps_to_use", "]", "\n", "encoded_states", "=", "self", ".", "_encode_observation", "(", "(", "idx", "+", "1", ")", "%", "self", ".", "size", ",", "nstep", "=", "steps_to_use", ")", "[", "None", "]", "\n", "for", "j", "in", "range", "(", "steps_to_use", ")", ":", "\n", "                    ", "next_state_nstep", "[", "e", ",", "j", "]", "=", "encoded_states", "[", "0", ",", "j", ":", "j", "+", "self", ".", "args", ".", "past_frames_input", "]", "\n", "# next_state_nstep[e, j - idx] = self._encode_observation((j + 1) % self.size)[None]", "\n", "\n", "", "steps", "[", "e", "]", "=", "steps_to_use", "\n", "done_nstep", "[", "e", "]", "=", "self", ".", "done", "[", "idx", "+", "steps_to_use", "-", "1", "]", "\n", "last_states", "[", "e", "]", "=", "next_state_nstep", "[", "e", ",", "steps_to_use", "-", "1", "]", "\n", "\n", "if", "self", ".", "bsp", ":", "\n", "                    ", "bsp_ws", "[", "e", "]", "=", "self", ".", "bsp_w", "[", "idx", "]", "\n", "", "if", "self", ".", "mmc", ":", "\n", "                    ", "mmc_vs", "[", "e", "]", "=", "self", ".", "mmc_v", "[", "idx", "]", "\n", "\n", "", "", "extra_info", "[", "\"steps\"", "]", "=", "steps", "\n", "extra_info", "[", "\"dones\"", "]", "=", "done_nstep", "\n", "extra_info", "[", "\"rewards\"", "]", "=", "reward_nstep", "\n", "extra_info", "[", "\"intrin_rewards\"", "]", "=", "intrin_reward_nstep", "\n", "extra_info", "[", "\"next_states\"", "]", "=", "next_state_nstep", "\n", "extra_info", "[", "\"last_states\"", "]", "=", "last_states", "\n", "extra_info", "[", "\"next_actions\"", "]", "=", "next_actions_nstep", "\n", "extra_info", "[", "\"sarsa_actions\"", "]", "=", "sarsa_actions", "\n", "\n", "if", "self", ".", "bsp", ":", "\n", "                ", "extra_info", "[", "\"bsp_w\"", "]", "=", "bsp_ws", "\n", "", "if", "self", ".", "mmc", ":", "\n", "                ", "extra_info", "[", "\"mmc\"", "]", "=", "mmc_vs", "\n", "\n", "", "", "return", "obs_batch", ",", "act_batch", ",", "rew_batch", ",", "int_rew_batch", ",", "next_obs_batch", ",", "done_mask", ",", "extra_info", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.buffer.buffer.ReplayBuffer.sample": [[166, 202], ["buffer.ReplayBuffer.can_sample", "buffer.sample_n_unique", "buffer.ReplayBuffer._encode_sample", "functools.partial"], "methods", ["home.repos.pwc.inspect_result.oxwhirl_opiq.buffer.buffer.ReplayBuffer.can_sample", "home.repos.pwc.inspect_result.oxwhirl_opiq.buffer.buffer.sample_n_unique", "home.repos.pwc.inspect_result.oxwhirl_opiq.buffer.buffer.ReplayBuffer._encode_sample"], ["", "def", "sample", "(", "self", ",", "batch_size", ",", "nstep", "=", "1", ")", ":", "\n", "        ", "\"\"\"Sample `batch_size` different transitions.\n        i-th sample transition is the following:\n        when observing `obs_batch[i]`, action `act_batch[i]` was taken,\n        after which reward `rew_batch[i]` was received and subsequent\n        observation  next_obs_batch[i] was observed, unless the epsiode\n        was done which is represented by `done_mask[i]` which is equal\n        to 1 if episode has ended as a result of that action.\n        Parameters\n        ----------\n        batch_size: int\n            How many transitions to sample.\n        Returns\n        -------\n        obs_batch: np.array\n            Array of shape\n            (batch_size, img_c * frame_history_len, img_h, img_w)\n            and dtype np.uint8\n        act_batch: np.array\n            Array of shape (batch_size,) and dtype np.int32\n        rew_batch: np.array\n            Array of shape (batch_size,) and dtype np.float32\n        next_obs_batch: np.array\n            Array of shape\n            (batch_size, img_c * frame_history_len, img_h, img_w)\n            and dtype np.uint8\n        done_mask: np.array\n            Array of shape (batch_size,) and dtype np.float32\n        \"\"\"", "\n", "assert", "self", ".", "can_sample", "(", "batch_size", ")", "\n", "idxes", "=", "sample_n_unique", "(", "\n", "# lambda: random.randint(0, self.num_in_buffer - 2), batch_size", "\n", "partial", "(", "get_good_idx", ",", "self", ".", "dont_sample", ",", "self", ".", "num_in_buffer", "-", "2", ")", ",", "batch_size", "\n", ")", "\n", "# assert all(self.dont_sample[idxes] == False)", "\n", "return", "self", ".", "_encode_sample", "(", "idxes", ",", "nstep", "=", "nstep", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.buffer.buffer.ReplayBuffer.encode_recent_observation": [[203, 214], ["buffer.ReplayBuffer._encode_observation"], "methods", ["home.repos.pwc.inspect_result.oxwhirl_opiq.buffer.buffer.ReplayBuffer._encode_observation"], ["", "def", "encode_recent_observation", "(", "self", ")", ":", "\n", "        ", "\"\"\"Return the most recent `frame_history_len` frames.\n        Returns\n        -------\n        observation: np.array\n            Array of shape (img_c * frame_history_len, img_h, img_w)\n            and dtype np.uint8, where observation[i*img_c:(i+1)*img_c, :, :]\n            encodes frame at time `t - frame_history_len + i`\n        \"\"\"", "\n", "assert", "self", ".", "num_in_buffer", ">", "0", "\n", "return", "self", ".", "_encode_observation", "(", "(", "self", ".", "next_idx", "-", "1", ")", "%", "self", ".", "size", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.buffer.buffer.ReplayBuffer._encode_observation": [[215, 244], ["range", "range", "len", "buffer.ReplayBuffer.obs[].astype", "numpy.zeros_like", "frames.append", "numpy.concatenate().astype", "len", "range", "numpy.concatenate().astype", "buffer.ReplayBuffer.obs[].reshape().astype", "numpy.concatenate", "numpy.concatenate", "buffer.ReplayBuffer.obs[].reshape"], "methods", ["None"], ["", "def", "_encode_observation", "(", "self", ",", "idx", ",", "nstep", "=", "1", ")", ":", "\n", "        ", "end_idx", "=", "idx", "+", "nstep", "# make noninclusive", "\n", "start_idx", "=", "end_idx", "-", "(", "nstep", "-", "1", ")", "-", "self", ".", "frame_history_len", "\n", "# this checks if we are using low-dimensional observations, such as RAM", "\n", "# state, in which case we just directly return the latest RAM.", "\n", "if", "len", "(", "self", ".", "obs", ".", "shape", ")", "==", "2", "and", "nstep", "==", "1", "and", "self", ".", "frame_history_len", "==", "1", ":", "\n", "            ", "return", "self", ".", "obs", "[", "end_idx", "-", "1", "]", ".", "astype", "(", "np", ".", "float32", ")", "*", "self", ".", "obs_scaling", "\n", "# if there weren't enough frames ever in the buffer for context", "\n", "", "if", "start_idx", "<", "0", "and", "self", ".", "num_in_buffer", "!=", "self", ".", "size", ":", "\n", "            ", "start_idx", "=", "0", "\n", "", "for", "idx", "in", "range", "(", "start_idx", ",", "end_idx", "-", "1", ")", ":", "\n", "            ", "if", "self", ".", "done", "[", "idx", "%", "self", ".", "size", "]", ":", "\n", "                ", "start_idx", "=", "idx", "+", "1", "\n", "", "", "missing_context", "=", "self", ".", "frame_history_len", "+", "(", "nstep", "-", "1", ")", "-", "(", "end_idx", "-", "start_idx", ")", "\n", "# if zero padding is needed for missing context", "\n", "# or we are on the boundry of the buffer", "\n", "if", "start_idx", "<", "0", "or", "missing_context", ">", "0", ":", "\n", "            ", "frames", "=", "[", "np", ".", "zeros_like", "(", "self", ".", "obs", "[", "0", "]", ")", "for", "_", "in", "range", "(", "missing_context", ")", "]", "\n", "for", "idx", "in", "range", "(", "start_idx", ",", "end_idx", ")", ":", "\n", "                ", "frames", ".", "append", "(", "self", ".", "obs", "[", "idx", "%", "self", ".", "size", "]", ")", "\n", "", "return", "np", ".", "concatenate", "(", "frames", ",", "0", ")", ".", "astype", "(", "np", ".", "float32", ")", "*", "self", ".", "obs_scaling", "# c, h, w instead of h, w c", "\n", "", "else", ":", "\n", "# this optimization has potential to saves about 30% compute time \\o/", "\n", "# c, h, w instead of h, w c", "\n", "            ", "if", "len", "(", "self", ".", "obs", ".", "shape", ")", "==", "3", ":", "\n", "                ", "return", "np", ".", "concatenate", "(", "self", ".", "obs", "[", "start_idx", ":", "end_idx", "]", ",", "0", ")", ".", "astype", "(", "np", ".", "float32", ")", "*", "self", ".", "obs_scaling", "\n", "", "else", ":", "\n", "                ", "img_h", ",", "img_w", "=", "self", ".", "obs", ".", "shape", "[", "2", "]", ",", "self", ".", "obs", ".", "shape", "[", "3", "]", "\n", "return", "self", ".", "obs", "[", "start_idx", ":", "end_idx", "]", ".", "reshape", "(", "-", "1", ",", "img_h", ",", "img_w", ")", ".", "astype", "(", "np", ".", "float32", ")", "*", "self", ".", "obs_scaling", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.buffer.buffer.ReplayBuffer.store_frame": [[245, 290], ["min", "len", "frame.transpose.transpose.transpose", "numpy.empty", "numpy.empty", "numpy.empty", "numpy.empty", "numpy.empty", "numpy.empty", "numpy.empty", "len", "frame.transpose.transpose.transpose", "numpy.empty", "numpy.empty", "list"], "methods", ["None"], ["", "", "", "def", "store_frame", "(", "self", ",", "frame", ")", ":", "\n", "        ", "\"\"\"Store a single frame in the buffer at the next available index, overwriting\n        old frames if necessary.\n        Parameters\n        ----------\n        frame: np.array\n            Array of shape (img_h, img_w, img_c) and dtype np.uint8\n            the frame to be stored\n        Returns\n        -------\n        idx: int\n            Index at which the frame is stored. To be used for `store_effect` later.\n        \"\"\"", "\n", "# if observation is an image...", "\n", "if", "len", "(", "frame", ".", "shape", ")", ">", "2", ":", "\n", "# transpose image frame into c, h, w instead of h, w, c", "\n", "            ", "frame", "=", "frame", ".", "transpose", "(", "2", ",", "0", ",", "1", ")", "\n", "", "elif", "len", "(", "frame", ".", "shape", ")", ">", "1", ":", "\n", "            ", "frame", "=", "frame", ".", "transpose", "(", "0", ",", "1", ")", "\n", "\n", "", "if", "self", ".", "obs", "is", "None", ":", "\n", "            ", "self", ".", "obs", "=", "np", ".", "empty", "(", "[", "self", ".", "size", "]", "+", "list", "(", "frame", ".", "shape", ")", ",", "dtype", "=", "self", ".", "obs_dtype", ")", "\n", "self", ".", "action", "=", "np", ".", "empty", "(", "[", "self", ".", "size", "]", ",", "dtype", "=", "np", ".", "int32", ")", "\n", "self", ".", "reward", "=", "np", ".", "empty", "(", "[", "self", ".", "size", "]", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "self", ".", "intrinsic_reward", "=", "np", ".", "empty", "(", "[", "self", ".", "size", "]", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "self", ".", "pseudo_count", "=", "np", ".", "empty", "(", "[", "self", ".", "size", "]", ",", "dtype", "=", "np", ".", "int32", ")", "\n", "self", ".", "done", "=", "np", ".", "empty", "(", "[", "self", ".", "size", "]", ",", "dtype", "=", "np", ".", "bool", ")", "\n", "self", ".", "dont_sample", "=", "np", ".", "empty", "(", "[", "self", ".", "size", "]", ",", "dtype", "=", "np", ".", "bool", ")", "\n", "if", "self", ".", "bsp", ":", "\n", "                ", "self", ".", "bsp_w", "=", "np", ".", "empty", "(", "[", "self", ".", "size", ",", "self", ".", "args", ".", "bsp_k", "]", ",", "dtype", "=", "np", ".", "int32", ")", "\n", "", "if", "self", ".", "mmc", ":", "\n", "                ", "self", ".", "mmc_v", "=", "np", ".", "empty", "(", "[", "self", ".", "size", "]", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "", "", "self", ".", "obs", "[", "self", ".", "next_idx", "]", "=", "frame", "\n", "\n", "ret", "=", "self", ".", "next_idx", "\n", "self", ".", "next_idx", "=", "(", "self", ".", "next_idx", "+", "1", ")", "%", "self", ".", "size", "\n", "self", ".", "num_in_buffer", "=", "min", "(", "self", ".", "size", ",", "self", ".", "num_in_buffer", "+", "1", ")", "\n", "\n", "# State shape including stacked frames", "\n", "self", ".", "state_shape", "=", "frame", ".", "shape", "\n", "# Kind of hacky but whatever", "\n", "if", "self", ".", "frame_history_len", ">", "1", ":", "\n", "            ", "self", ".", "state_shape", "=", "(", "self", ".", "frame_history_len", ",", "*", "self", ".", "state_shape", "[", "1", ":", "]", ")", "\n", "\n", "", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.buffer.buffer.ReplayBuffer.store_effect": [[291, 330], ["numpy.random.binomial", "numpy.clip", "numpy.clip"], "methods", ["None"], ["", "def", "store_effect", "(", "self", ",", "idx", ",", "action", ",", "reward", ",", "intrinsic_reward", ",", "done", ",", "pseudo_count", ",", "dont_sample", "=", "False", ")", ":", "\n", "        ", "\"\"\"Store effects of action taken after obeserving frame stored\n        at index idx. The reason `store_frame` and `store_effect` is broken\n        up into two functions is so that once can call `encode_recent_observation`\n        in between.\n        Paramters\n        ---------\n        idx: int\n            Index in buffer of recently observed frame (returned by `store_frame`).\n        action: int\n            Action that was performed upon observing this frame.\n        reward: float\n            Reward that was received when the actions was performed.\n        done: bool\n            True if episode was finished after performing that action.\n        \"\"\"", "\n", "self", ".", "action", "[", "idx", "]", "=", "action", "\n", "self", ".", "reward", "[", "idx", "]", "=", "reward", "\n", "self", ".", "intrinsic_reward", "[", "idx", "]", "=", "intrinsic_reward", "\n", "self", ".", "done", "[", "idx", "]", "=", "done", "\n", "self", ".", "dont_sample", "[", "idx", "]", "=", "dont_sample", "\n", "self", ".", "pseudo_count", "[", "idx", "]", "=", "pseudo_count", "\n", "\n", "if", "self", ".", "bsp", ":", "\n", "            ", "self", ".", "bsp_w", "[", "idx", "]", "=", "np", ".", "random", ".", "binomial", "(", "1", ",", "self", ".", "args", ".", "bsp_p", ",", "size", "=", "self", ".", "args", ".", "bsp_k", ")", "\n", "\n", "", "if", "self", ".", "mmc", ":", "\n", "            ", "self", ".", "mmc_v", "[", "idx", "]", "=", "0", "\n", "if", "done", ":", "\n", "# Go back and fill in the MMC", "\n", "                ", "eps_reward", "=", "np", ".", "clip", "(", "reward", ",", "-", "1", ",", "+", "1", ")", "if", "self", ".", "args", ".", "reward_clipping", "else", "reward", "\n", "self", ".", "mmc_v", "[", "idx", "]", "=", "eps_reward", "\n", "end_idx", "=", "(", "idx", "-", "1", ")", "%", "self", ".", "num_in_buffer", "\n", "while", "not", "self", ".", "done", "[", "end_idx", "]", ":", "\n", "                    ", "reward_to_use", "=", "np", ".", "clip", "(", "self", ".", "reward", "[", "end_idx", "]", ",", "-", "1", ",", "1", ")", "if", "self", ".", "args", ".", "reward_clipping", "else", "self", ".", "reward", "[", "end_idx", "]", "\n", "eps_reward", "=", "self", ".", "args", ".", "gamma", "*", "eps_reward", "+", "reward_to_use", "\n", "self", ".", "mmc_v", "[", "end_idx", "]", "=", "eps_reward", "\n", "end_idx", "-=", "1", "\n", "end_idx", "%=", "self", ".", "num_in_buffer", "\n", "", "", "", "", "", ""]], "home.repos.pwc.inspect_result.oxwhirl_opiq.buffer.buffer.sample_n_unique": [[12, 25], ["sampling_f", "len", "res.append"], "function", ["None"], ["def", "sample_n_unique", "(", "sampling_f", ",", "n", ")", ":", "\n", "    ", "\"\"\"Helper function. Given a function `sampling_f` that returns\n    comparable objects, sample n such unique objects.\n    \"\"\"", "\n", "res", "=", "[", "]", "\n", "times_tried", "=", "0", "\n", "while", "len", "(", "res", ")", "<", "n", "and", "times_tried", "<", "100", ":", "\n", "        ", "candidate", "=", "sampling_f", "(", ")", "\n", "if", "candidate", "not", "in", "res", ":", "\n", "            ", "res", ".", "append", "(", "candidate", ")", "\n", "", "else", ":", "\n", "            ", "times_tried", "+=", "1", "\n", "", "", "return", "res", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.buffer.buffer.get_good_idx": [[27, 32], ["random.randint", "random.randint"], "function", ["None"], ["", "def", "get_good_idx", "(", "dont_sample", ",", "ceiling", ")", ":", "\n", "    ", "idx", "=", "random", ".", "randint", "(", "0", ",", "ceiling", ")", "\n", "while", "dont_sample", "[", "idx", "]", ":", "\n", "        ", "idx", "=", "random", ".", "randint", "(", "0", ",", "ceiling", ")", "\n", "", "return", "idx", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.env_wrapper.EnvWrapper.__init__": [[9, 22], ["env_wrapper.EnvWrapper.wrapped_env.unwrapped.update_limit", "print"], "methods", ["home.repos.pwc.inspect_result.oxwhirl_opiq.envs.gridworld.GridWorld.update_limit"], ["    ", "def", "__init__", "(", "self", ",", "env", ",", "debug", ",", "args", ")", ":", "\n", "        ", "self", ".", "wrapped_env", "=", "env", "\n", "self", ".", "metadata", "=", "env", ".", "metadata", "\n", "self", ".", "made_screen", "=", "False", "\n", "self", ".", "debug", "=", "debug", "\n", "self", ".", "scaling", "=", "args", ".", "render_scaling", "\n", "self", ".", "args", "=", "args", "\n", "\n", "if", "self", ".", "args", ".", "episode_limit", ">", "0", ":", "\n", "            ", "try", ":", "\n", "                ", "self", ".", "wrapped_env", ".", "unwrapped", ".", "update_limit", "(", "self", ".", "args", ".", "episode_limit", ")", "\n", "", "except", "AttributeError", ":", "\n", "                ", "print", "(", "\"Cant update the episode limit for this env\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.env_wrapper.EnvWrapper.step": [[23, 25], ["env_wrapper.EnvWrapper.wrapped_env.step"], "methods", ["home.repos.pwc.inspect_result.oxwhirl_opiq.envs.gridworld.GridWorld.step"], ["", "", "", "def", "step", "(", "self", ",", "a", ")", ":", "\n", "        ", "return", "self", ".", "wrapped_env", ".", "step", "(", "a", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.env_wrapper.EnvWrapper.reset": [[26, 28], ["env_wrapper.EnvWrapper.wrapped_env.reset"], "methods", ["home.repos.pwc.inspect_result.oxwhirl_opiq.envs.gridworld.GridWorld.reset"], ["", "def", "reset", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "wrapped_env", ".", "reset", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.env_wrapper.EnvWrapper.render": [[29, 31], ["env_wrapper.EnvWrapper.debug_render"], "methods", ["home.repos.pwc.inspect_result.oxwhirl_opiq.envs.env_wrapper.EnvWrapper.debug_render"], ["", "def", "render", "(", "self", ",", "mode", "=", "\"human\"", ",", "close", "=", "False", ",", "debug_info", "=", "{", "}", ")", ":", "\n", "        ", "return", "self", ".", "debug_render", "(", "mode", "=", "mode", ",", "close", "=", "close", ",", "debug_info", "=", "debug_info", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.env_wrapper.EnvWrapper.visits_and_frontier_overlayed": [[32, 38], ["env_wrapper.EnvWrapper.wrapped_env.unwrapped.visits_and_frontier_states", "print"], "methods", ["home.repos.pwc.inspect_result.oxwhirl_opiq.envs.gridworld.GridWorld.visits_and_frontier_states"], ["", "def", "visits_and_frontier_overlayed", "(", "self", ",", "log", "=", "False", ")", ":", "\n", "        ", "try", ":", "\n", "            ", "return", "self", ".", "wrapped_env", ".", "unwrapped", ".", "visits_and_frontier_states", "(", ")", "\n", "", "except", "AttributeError", ":", "\n", "            ", "if", "log", ":", "\n", "                ", "print", "(", "\"No visits and frontier vis here\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.env_wrapper.EnvWrapper.xp_and_frontier_overlayed": [[39, 45], ["env_wrapper.EnvWrapper.wrapped_env.unwrapped.xp_and_frontier_states", "print"], "methods", ["home.repos.pwc.inspect_result.oxwhirl_opiq.envs.gridworld.GridWorld.xp_and_frontier_states"], ["", "", "", "def", "xp_and_frontier_overlayed", "(", "self", ",", "log", "=", "False", ")", ":", "\n", "        ", "try", ":", "\n", "            ", "return", "self", ".", "wrapped_env", ".", "unwrapped", ".", "xp_and_frontier_states", "(", ")", "\n", "", "except", "AttributeError", ":", "\n", "            ", "if", "log", ":", "\n", "                ", "print", "(", "\"No xp and frontier vis here\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.env_wrapper.EnvWrapper.bonus_xp_and_frontier_overlayed": [[46, 52], ["env_wrapper.EnvWrapper.wrapped_env.unwrapped.bonus_xp_and_frontier_states", "print"], "methods", ["home.repos.pwc.inspect_result.oxwhirl_opiq.envs.gridworld.GridWorld.bonus_xp_and_frontier_states"], ["", "", "", "def", "bonus_xp_and_frontier_overlayed", "(", "self", ",", "log", "=", "False", ")", ":", "\n", "        ", "try", ":", "\n", "            ", "return", "self", ".", "wrapped_env", ".", "unwrapped", ".", "bonus_xp_and_frontier_states", "(", ")", "\n", "", "except", "AttributeError", ":", "\n", "            ", "if", "log", ":", "\n", "                ", "print", "(", "\"No bonus_xp and frontier vis here\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.env_wrapper.EnvWrapper.trained_on_states": [[53, 59], ["env_wrapper.EnvWrapper.wrapped_env.unwrapped.trained_on_states", "print"], "methods", ["home.repos.pwc.inspect_result.oxwhirl_opiq.envs.gridworld.GridWorld.trained_on_states"], ["", "", "", "def", "trained_on_states", "(", "self", ",", "states", ",", "log", "=", "False", ")", ":", "\n", "        ", "try", ":", "\n", "            ", "return", "self", ".", "wrapped_env", ".", "unwrapped", ".", "trained_on_states", "(", "states", ",", "self", ".", "args", ")", "\n", "", "except", "AttributeError", ":", "\n", "            ", "if", "log", ":", "\n", "                ", "print", "(", "\"No trained on states vis for this environment\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.env_wrapper.EnvWrapper.xp_replay_states": [[60, 68], ["env_wrapper.EnvWrapper.wrapped_env.unwrapped.xp_replay_states", "print"], "methods", ["home.repos.pwc.inspect_result.oxwhirl_opiq.envs.gridworld.GridWorld.xp_replay_states"], ["", "", "", "def", "xp_replay_states", "(", "self", ",", "player_positions", ",", "log", "=", "False", ",", "bonus_replay", "=", "False", ")", ":", "\n", "        ", "try", ":", "\n", "            ", "return", "self", ".", "wrapped_env", ".", "unwrapped", ".", "xp_replay_states", "(", "\n", "player_positions", ",", "self", ".", "args", ",", "bonus_replay", "=", "bonus_replay", "\n", ")", "\n", "", "except", "AttributeError", ":", "\n", "            ", "if", "log", ":", "\n", "                ", "print", "(", "\"No xp replay states for this environment\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.env_wrapper.EnvWrapper.visitations": [[69, 75], ["env_wrapper.EnvWrapper.wrapped_env.unwrapped.player_visits", "print"], "methods", ["home.repos.pwc.inspect_result.oxwhirl_opiq.envs.gridworld.GridWorld.player_visits"], ["", "", "", "def", "visitations", "(", "self", ",", "player_positions", ",", "log", "=", "False", ")", ":", "\n", "        ", "try", ":", "\n", "            ", "return", "self", ".", "wrapped_env", ".", "unwrapped", ".", "player_visits", "(", "player_positions", ",", "self", ".", "args", ")", "\n", "", "except", "AttributeError", ":", "\n", "            ", "if", "log", ":", "\n", "                ", "print", "(", "\"No visitations for this environment\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.env_wrapper.EnvWrapper.frontier": [[76, 82], ["env_wrapper.EnvWrapper.wrapped_env.unwrapped.frontier", "print"], "methods", ["home.repos.pwc.inspect_result.oxwhirl_opiq.envs.montezuma.Montezuma.frontier"], ["", "", "", "def", "frontier", "(", "self", ",", "exp_model", ",", "max_bonus", ",", "log", "=", "False", ")", ":", "\n", "        ", "try", ":", "\n", "            ", "return", "self", ".", "wrapped_env", ".", "unwrapped", ".", "frontier", "(", "exp_model", ",", "self", ".", "args", ",", "max_bonus", ")", "\n", "", "except", "AttributeError", ":", "\n", "            ", "if", "log", ":", "\n", "                ", "print", "(", "\"No frontier for this environment\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.env_wrapper.EnvWrapper.explorations": [[83, 91], ["env_wrapper.EnvWrapper.wrapped_env.unwrapped.bonus_landscape", "print"], "methods", ["home.repos.pwc.inspect_result.oxwhirl_opiq.envs.gridworld.GridWorld.bonus_landscape"], ["", "", "", "def", "explorations", "(", "self", ",", "player_positions", ",", "exploration_bonuses", ",", "max_bonus", ",", "log", "=", "False", ")", ":", "\n", "        ", "try", ":", "\n", "            ", "return", "self", ".", "wrapped_env", ".", "unwrapped", ".", "bonus_landscape", "(", "\n", "player_positions", ",", "exploration_bonuses", ",", "max_bonus", ",", "self", ".", "args", "\n", ")", "\n", "", "except", "AttributeError", ":", "\n", "            ", "if", "log", ":", "\n", "                ", "print", "(", "\"No bonus landscape for this environment\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.env_wrapper.EnvWrapper.log_visitation": [[92, 97], ["env_wrapper.EnvWrapper.wrapped_env.unwrapped.log_player_pos"], "methods", ["home.repos.pwc.inspect_result.oxwhirl_opiq.envs.gridworld.GridWorld.log_player_pos"], ["", "", "", "def", "log_visitation", "(", "self", ")", ":", "\n", "        ", "try", ":", "\n", "            ", "return", "self", ".", "wrapped_env", ".", "unwrapped", ".", "log_player_pos", "(", ")", "\n", "", "except", "AttributeError", ":", "\n", "            ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.env_wrapper.EnvWrapper.state_to_player_pos": [[98, 103], ["env_wrapper.EnvWrapper.wrapped_env.unwrapped.state_to_player_pos"], "methods", ["home.repos.pwc.inspect_result.oxwhirl_opiq.envs.gridworld.GridWorld.state_to_player_pos"], ["", "", "def", "state_to_player_pos", "(", "self", ",", "state", ")", ":", "\n", "        ", "try", ":", "\n", "            ", "return", "self", ".", "wrapped_env", ".", "unwrapped", ".", "state_to_player_pos", "(", "state", ")", "\n", "", "except", "AttributeError", ":", "\n", "            ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.env_wrapper.EnvWrapper.state_to_image": [[104, 109], ["env_wrapper.EnvWrapper.wrapped_env.unwrapped.state_to_image", "env_wrapper.EnvWrapper.debug_render"], "methods", ["home.repos.pwc.inspect_result.oxwhirl_opiq.envs.gridworld.GridWorld.state_to_image", "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.env_wrapper.EnvWrapper.debug_render"], ["", "", "def", "state_to_image", "(", "self", ",", "state", ")", ":", "\n", "        ", "try", ":", "\n", "            ", "return", "self", ".", "wrapped_env", ".", "unwrapped", ".", "state_to_image", "(", "state", ")", "\n", "", "except", "AttributeError", ":", "\n", "            ", "return", "self", ".", "debug_render", "(", "mode", "=", "\"rgb_array\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.env_wrapper.EnvWrapper.count_state_action_space": [[110, 115], ["env_wrapper.EnvWrapper.wrapped_env.unwrapped.count_state_action_space"], "methods", ["home.repos.pwc.inspect_result.oxwhirl_opiq.envs.gridworld.GridWorld.count_state_action_space"], ["", "", "def", "count_state_action_space", "(", "self", ",", "count_model", ")", ":", "\n", "        ", "try", ":", "\n", "            ", "return", "self", ".", "wrapped_env", ".", "unwrapped", ".", "count_state_action_space", "(", "count_model", ",", "self", ".", "args", ")", "\n", "", "except", "AttributeError", ":", "\n", "            ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.env_wrapper.EnvWrapper.q_value_estimates": [[116, 121], ["env_wrapper.EnvWrapper.wrapped_env.unwrapped.q_value_estimates"], "methods", ["home.repos.pwc.inspect_result.oxwhirl_opiq.envs.gridworld.GridWorld.q_value_estimates"], ["", "", "def", "q_value_estimates", "(", "self", ",", "count_model", ",", "nn", ")", ":", "\n", "        ", "try", ":", "\n", "            ", "return", "self", ".", "wrapped_env", ".", "unwrapped", ".", "q_value_estimates", "(", "count_model", ",", "nn", ",", "self", ".", "args", ")", "\n", "", "except", "AttributeError", ":", "\n", "            ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.env_wrapper.EnvWrapper.state_counts": [[122, 127], ["None"], "methods", ["None"], ["", "", "def", "state_counts", "(", "self", ")", ":", "\n", "        ", "try", ":", "\n", "            ", "return", "self", ".", "wrapped_env", ".", "unwrapped", ".", "counts", "\n", "", "except", "Exception", ":", "\n", "            ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.env_wrapper.EnvWrapper.debug_render": [[128, 207], ["env_wrapper.EnvWrapper.wrapped_env.render", "env_wrapper.EnvWrapper.debug_render", "env_wrapper.EnvWrapper.screen.fill", "range", "pygame.display.update", "pygame.quit", "env_wrapper.EnvWrapper.wrapped_env.render", "pygame.init", "pygame.display.set_mode", "range", "env_wrapper.EnvWrapper.wrapped_env.render", "numpy.swapaxes", "numpy.zeros", "numpy.swapaxes", "env_wrapper.EnvWrapper.draw_q_values", "env_wrapper.EnvWrapper.draw_count", "numpy.all", "pygame.draw.rect"], "methods", ["home.repos.pwc.inspect_result.oxwhirl_opiq.envs.gridworld.GridWorld.render", "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.env_wrapper.EnvWrapper.debug_render", "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.gridworld.GridWorld.render", "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.gridworld.GridWorld.render", "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.env_wrapper.EnvWrapper.draw_q_values", "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.env_wrapper.EnvWrapper.draw_count"], ["", "", "def", "debug_render", "(", "self", ",", "debug_info", "=", "{", "}", ",", "mode", "=", "\"human\"", ",", "close", "=", "False", ")", ":", "\n", "        ", "if", "self", ".", "debug", ":", "\n", "\n", "            ", "if", "mode", "==", "\"human\"", ":", "\n", "                ", "if", "close", ":", "\n", "                    ", "pygame", ".", "quit", "(", ")", "\n", "return", "\n", "", "if", "\"human\"", "in", "self", ".", "wrapped_env", ".", "metadata", "[", "\"render.modes\"", "]", ":", "\n", "                    ", "self", ".", "wrapped_env", ".", "render", "(", "mode", "=", "\"human\"", ")", "\n", "", "rgb_array", "=", "self", ".", "debug_render", "(", "debug_info", ",", "mode", "=", "\"rgb_array\"", ")", "\n", "if", "not", "self", ".", "made_screen", ":", "\n", "                    ", "pygame", ".", "init", "(", ")", "\n", "screen_size", "=", "(", "\n", "rgb_array", ".", "shape", "[", "1", "]", "*", "self", ".", "scaling", ",", "\n", "rgb_array", ".", "shape", "[", "0", "]", "*", "self", ".", "scaling", ",", "\n", ")", "\n", "screen", "=", "pygame", ".", "display", ".", "set_mode", "(", "screen_size", ")", "\n", "self", ".", "screen", "=", "screen", "\n", "self", ".", "made_screen", "=", "True", "\n", "", "self", ".", "screen", ".", "fill", "(", "(", "0", ",", "0", ",", "0", ")", ")", "\n", "for", "x", "in", "range", "(", "rgb_array", ".", "shape", "[", "0", "]", ")", ":", "\n", "                    ", "for", "y", "in", "range", "(", "rgb_array", ".", "shape", "[", "1", "]", ")", ":", "\n", "                        ", "if", "not", "np", ".", "all", "(", "rgb_array", "[", "x", ",", "y", ",", ":", "]", "==", "(", "0", ",", "0", ",", "0", ")", ")", ":", "\n", "                            ", "pygame", ".", "draw", ".", "rect", "(", "\n", "self", ".", "screen", ",", "\n", "rgb_array", "[", "x", ",", "y", "]", ",", "\n", "(", "\n", "y", "*", "self", ".", "scaling", ",", "\n", "x", "*", "self", ".", "scaling", ",", "\n", "self", ".", "scaling", ",", "\n", "self", ".", "scaling", ",", "\n", ")", ",", "\n", ")", "\n", "", "", "", "pygame", ".", "display", ".", "update", "(", ")", "\n", "\n", "", "elif", "mode", "==", "\"rgb_array\"", ":", "\n", "                ", "env_image", "=", "self", ".", "wrapped_env", ".", "render", "(", "mode", "=", "\"rgb_array\"", ")", "\n", "env_image", "=", "np", ".", "swapaxes", "(", "env_image", ",", "0", ",", "1", ")", "\n", "image_x", "=", "env_image", ".", "shape", "[", "0", "]", "\n", "image_y", "=", "env_image", ".", "shape", "[", "1", "]", "+", "1", "\n", "\n", "if", "\"CTS_State\"", "in", "debug_info", ":", "\n", "                    ", "image_x", "+=", "5", "+", "debug_info", "[", "\"CTS_State\"", "]", ".", "shape", "[", "0", "]", "\n", "", "if", "\"Q_Values\"", "in", "debug_info", ":", "\n", "                    ", "image_y", "+=", "50", "\n", "\n", "", "image", "=", "np", ".", "zeros", "(", "shape", "=", "(", "image_x", ",", "image_y", ",", "3", ")", ")", "\n", "image", "[", ":", "env_image", ".", "shape", "[", "0", "]", ",", ":", "env_image", ".", "shape", "[", "1", "]", ",", ":", "]", "=", "env_image", "\n", "\n", "if", "\"Action_Bonus\"", "in", "debug_info", ":", "\n", "# Add the action bonus stuff to the q values", "\n", "                    ", "debug_info", "[", "\"Q_Values\"", "]", "=", "(", "\n", "debug_info", "[", "\"Q_Values\"", "]", "+", "debug_info", "[", "\"Action_Bonus\"", "]", "\n", ")", "\n", "\n", "# Draw the Q-Values", "\n", "", "if", "\"Q_Values\"", "in", "debug_info", ":", "\n", "                    ", "q_vals_image", "=", "self", ".", "draw_q_values", "(", "\n", "debug_info", ",", "env_image", ".", "shape", "[", "0", "]", "-", "1", ",", "48", "\n", ")", "\n", "image", "[", "\n", "1", ":", "env_image", ".", "shape", "[", "0", "]", ",", "\n", "env_image", ".", "shape", "[", "1", "]", "+", "2", ":", "env_image", ".", "shape", "[", "1", "]", "+", "50", ",", "\n", ":", ",", "\n", "]", "=", "q_vals_image", "\n", "\n", "# Draw the Pseudo-Count stuff", "\n", "", "if", "\"CTS_State\"", "in", "debug_info", ":", "\n", "                    ", "count_image", "=", "self", ".", "draw_count", "(", "\n", "debug_info", ",", "\n", "5", "-", "1", "+", "debug_info", "[", "\"CTS_State\"", "]", ".", "shape", "[", "0", "]", ",", "\n", "image_y", "-", "1", ",", "\n", ")", "\n", "image", "[", "env_image", ".", "shape", "[", "0", "]", "+", "1", ":", ",", ":", "-", "1", ",", ":", "]", "=", "count_image", "\n", "\n", "", "image", "=", "np", ".", "swapaxes", "(", "image", ",", "0", ",", "1", ")", "\n", "return", "image", "\n", "", "", "else", ":", "\n", "            ", "return", "self", ".", "wrapped_env", ".", "render", "(", "mode", ",", "close", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.env_wrapper.EnvWrapper.draw_q_values": [[208, 292], ["numpy.zeros", "numpy.argmax", "len", "int", "enumerate", "int", "skimage.draw.polygon", "skimage.draw.set_color", "numpy.fliplr", "skimage.draw.polygon", "skimage.draw.set_color", "enumerate", "skimage.draw.polygon", "skimage.draw.set_color", "int", "int"], "methods", ["None"], ["", "", "def", "draw_q_values", "(", "self", ",", "info", ",", "width", ",", "height", ")", ":", "\n", "        ", "image", "=", "np", ".", "zeros", "(", "(", "width", ",", "height", ",", "3", ")", ")", "\n", "red", "=", "(", "255", ",", "0", ",", "0", ")", "\n", "yellow", "=", "(", "255", ",", "255", ",", "0", ")", "\n", "green", "=", "(", "0", ",", "255", ",", "0", ")", "\n", "blue", "=", "(", "0", ",", "0", ",", "255", ")", "\n", "purple", "=", "(", "255", ",", "0", ",", "255", ")", "\n", "orange", "=", "(", "255", ",", "165", ",", "0", ")", "\n", "\n", "q_values", "=", "info", "[", "\"Q_Values\"", "]", "\n", "max_q_value", "=", "info", "[", "\"Max_Q_Value\"", "]", "\n", "min_q_value", "=", "info", "[", "\"Min_Q_Value\"", "]", "\n", "chosen_action", "=", "info", "[", "\"Action\"", "]", "\n", "epsilon", "=", "info", "[", "\"Epsilon\"", "]", "\n", "forced_action", "=", "-", "1", "\n", "if", "\"Forced_Action\"", "in", "info", ":", "\n", "            ", "forced_action", "=", "info", "[", "\"Forced_Action\"", "]", "\n", "\n", "# Hack", "\n", "", "if", "max_q_value", "==", "min_q_value", ":", "\n", "            ", "q_val_sizes", "=", "[", "(", "height", "-", "4", ")", "for", "_", "in", "q_values", "]", "\n", "", "else", ":", "\n", "            ", "q_val_sizes", "=", "[", "\n", "int", "(", "(", "q_val", "-", "min_q_value", ")", "/", "(", "max_q_value", "-", "min_q_value", ")", "*", "(", "height", "-", "4", ")", ")", "\n", "+", "4", "\n", "for", "q_val", "in", "q_values", "\n", "]", "\n", "", "greedy_action", "=", "np", ".", "argmax", "(", "q_values", ")", "\n", "actions", "=", "len", "(", "q_values", ")", "\n", "bar_width", "=", "int", "(", "width", "/", "actions", ")", "\n", "\n", "# Draw the Q-Values", "\n", "for", "i", ",", "q_size", "in", "enumerate", "(", "q_val_sizes", ")", ":", "\n", "            ", "if", "i", "==", "forced_action", ":", "\n", "                ", "q_color", "=", "red", "\n", "", "elif", "i", "==", "greedy_action", ":", "\n", "                ", "q_color", "=", "yellow", "\n", "", "elif", "i", "==", "chosen_action", ":", "\n", "                ", "q_color", "=", "green", "\n", "", "else", ":", "\n", "                ", "q_color", "=", "blue", "\n", "", "rect_coords", "=", "[", "\n", "(", "i", "*", "bar_width", ",", "4", ")", ",", "\n", "(", "i", "*", "bar_width", ",", "q_size", ")", ",", "\n", "(", "(", "i", "+", "1", ")", "*", "bar_width", ",", "q_size", ")", ",", "\n", "(", "(", "i", "+", "1", ")", "*", "bar_width", ",", "4", ")", ",", "\n", "]", "\n", "rect_row", "=", "[", "r", "[", "0", "]", "for", "r", "in", "rect_coords", "]", "\n", "rect_col", "=", "[", "r", "[", "1", "]", "for", "r", "in", "rect_coords", "]", "\n", "rect_array_coords", "=", "draw", ".", "polygon", "(", "rect_row", ",", "rect_col", ")", "\n", "draw", ".", "set_color", "(", "image", ",", "rect_array_coords", ",", "q_color", ")", "\n", "\n", "# Draw the action bonus stuff if it is there", "\n", "", "if", "\"Action_Bonus\"", "in", "info", ":", "\n", "            ", "q_val_bonuses", "=", "info", "[", "\"Action_Bonus\"", "]", "\n", "q_val_bonus_sizes", "=", "[", "\n", "int", "(", "\n", "(", "q_bonus", "-", "min_q_value", ")", "/", "(", "max_q_value", "-", "min_q_value", ")", "*", "(", "height", "-", "4", ")", "\n", ")", "\n", "+", "4", "\n", "for", "q_bonus", "in", "q_val_bonuses", "\n", "]", "\n", "for", "i", ",", "q_size", "in", "enumerate", "(", "q_val_bonus_sizes", ")", ":", "\n", "                ", "q_color", "=", "orange", "\n", "rect_coords", "=", "[", "\n", "(", "i", "*", "bar_width", ",", "4", ")", ",", "\n", "(", "i", "*", "bar_width", ",", "q_size", ")", ",", "\n", "(", "(", "i", "+", "1", ")", "*", "bar_width", ",", "q_size", ")", ",", "\n", "(", "(", "i", "+", "1", ")", "*", "bar_width", ",", "4", ")", ",", "\n", "]", "\n", "rect_row", "=", "[", "r", "[", "0", "]", "for", "r", "in", "rect_coords", "]", "\n", "rect_col", "=", "[", "r", "[", "1", "]", "for", "r", "in", "rect_coords", "]", "\n", "rect_array_coords", "=", "draw", ".", "polygon", "(", "rect_row", ",", "rect_col", ")", "\n", "draw", ".", "set_color", "(", "image", ",", "rect_array_coords", ",", "q_color", ")", "\n", "\n", "# Epsilon", "\n", "", "", "bar_width", "=", "int", "(", "width", "*", "epsilon", ")", "\n", "bonus_rect_coords", "=", "[", "(", "0", ",", "0", ")", ",", "(", "0", ",", "3", ")", ",", "(", "bar_width", ",", "3", ")", ",", "(", "bar_width", ",", "0", ")", "]", "\n", "rect_row", "=", "[", "r", "[", "0", "]", "for", "r", "in", "bonus_rect_coords", "]", "\n", "rect_col", "=", "[", "r", "[", "1", "]", "for", "r", "in", "bonus_rect_coords", "]", "\n", "rect_array_coords", "=", "draw", ".", "polygon", "(", "rect_row", ",", "rect_col", ")", "\n", "draw", ".", "set_color", "(", "image", ",", "rect_array_coords", ",", "purple", ")", "\n", "\n", "return", "np", ".", "fliplr", "(", "image", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.env_wrapper.EnvWrapper.draw_count": [[293, 334], ["numpy.zeros", "int", "skimage.draw.polygon", "skimage.draw.set_color", "numpy.concatenate", "numpy.empty_like", "range", "numpy.stack", "numpy.fliplr", "numpy.fliplr", "range", "numpy.swapaxes", "numpy.abs", "range", "max", "min", "numpy.clip", "range", "int", "int"], "methods", ["None"], ["", "def", "draw_count", "(", "self", ",", "info", ",", "width", ",", "height", ")", ":", "\n", "        ", "image", "=", "np", ".", "zeros", "(", "(", "width", ",", "height", ",", "3", ")", ")", "\n", "\n", "red", "=", "(", "255", ",", "0", ",", "0", ")", "\n", "\n", "bonus", "=", "info", "[", "\"Bonus\"", "]", "\n", "max_bonus", "=", "info", "[", "\"Max_Bonus\"", "]", "\n", "cts_image", "=", "info", "[", "\"CTS_State\"", "]", "\n", "cts_pg", "=", "info", "[", "\"Pixel_PG\"", "]", "\n", "\n", "if", "max_bonus", "==", "0", ":", "\n", "            ", "max_bonus", "=", "0.000001", "\n", "# Bar", "\n", "", "bar_height", "=", "int", "(", "height", "*", "bonus", "/", "max_bonus", ")", "\n", "bonus_rect_coords", "=", "[", "(", "0", ",", "0", ")", ",", "(", "0", ",", "bar_height", ")", ",", "(", "3", ",", "bar_height", ")", ",", "(", "3", ",", "0", ")", "]", "\n", "rect_row", "=", "[", "r", "[", "0", "]", "for", "r", "in", "bonus_rect_coords", "]", "\n", "rect_col", "=", "[", "r", "[", "1", "]", "for", "r", "in", "bonus_rect_coords", "]", "\n", "rect_array_coords", "=", "draw", ".", "polygon", "(", "rect_row", ",", "rect_col", ")", "\n", "draw", ".", "set_color", "(", "image", ",", "rect_array_coords", ",", "red", ")", "\n", "\n", "# PG per pixel", "\n", "cts_gray", "=", "np", ".", "concatenate", "(", "[", "cts_image", "for", "_", "in", "range", "(", "3", ")", "]", ",", "axis", "=", "2", ")", "\n", "cts_pg_image", "=", "np", ".", "empty_like", "(", "cts_gray", ")", "\n", "for", "x", "in", "range", "(", "cts_image", ".", "shape", "[", "0", "]", ")", ":", "\n", "            ", "for", "y", "in", "range", "(", "cts_image", ".", "shape", "[", "1", "]", ")", ":", "\n", "                ", "pg", "=", "cts_pg", "[", "x", ",", "y", "]", "\n", "if", "pg", "<", "0", ":", "\n", "# Blue", "\n", "                    ", "pg", "=", "max", "(", "-", "1", ",", "pg", ")", "\n", "cts_pg_image", "[", "x", ",", "y", ",", ":", "]", "=", "(", "0", ",", "0", ",", "int", "(", "-", "pg", "*", "255", ")", ")", "\n", "", "else", ":", "\n", "# Red", "\n", "                    ", "pg", "=", "min", "(", "pg", ",", "1", ")", "\n", "cts_pg_image", "[", "x", ",", "y", ",", ":", "]", "=", "(", "int", "(", "pg", "*", "255", ")", ",", "0", ",", "0", ")", "\n", "", "", "", "cts_alpha", "=", "np", ".", "stack", "(", "[", "np", ".", "abs", "(", "np", ".", "clip", "(", "cts_pg", ",", "-", "1", ",", "1", ")", ")", "for", "_", "in", "range", "(", "3", ")", "]", ",", "axis", "=", "2", ")", "\n", "cts_colour_image", "=", "cts_alpha", "*", "cts_pg_image", "+", "(", "1", "-", "cts_alpha", ")", "*", "cts_gray", "\n", "image", "[", "4", ":", ",", "-", "cts_image", ".", "shape", "[", "1", "]", ":", ",", ":", "]", "=", "np", ".", "fliplr", "(", "\n", "np", ".", "swapaxes", "(", "cts_colour_image", ",", "0", ",", "1", ")", "\n", ")", "\n", "\n", "return", "np", ".", "fliplr", "(", "image", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.atari.AtariEnv.__init__": [[12, 28], ["gym.make", "OpenAI_AtariWrapper.make_atari", "utils.logging.get_stats"], "methods", ["home.repos.pwc.inspect_result.oxwhirl_opiq.envs.OpenAI_AtariWrapper.make_atari", "home.repos.pwc.inspect_result.oxwhirl_opiq.utils.logging.get_stats"], ["def", "__init__", "(", "self", ",", "game", ")", ":", "\n", "        ", "self", ".", "mont_env", "=", "gym", ".", "make", "(", "\"{}NoFrameskip-v4\"", ".", "format", "(", "game", ")", ")", "\n", "# self.mario_env = modewrapper(self.mario_env)", "\n", "self", ".", "max_timesteps", "=", "4500", "\n", "self", ".", "mont_env", "=", "make_atari", "(", "self", ".", "mont_env", ",", "max_episode_steps", "=", "self", ".", "max_timesteps", ")", "\n", "\n", "self", ".", "steps", "=", "0", "\n", "\n", "self", ".", "_seed", "=", "56", "\n", "self", ".", "observation_space", "=", "self", ".", "mont_env", ".", "observation_space", "\n", "self", ".", "action_space", "=", "self", ".", "mont_env", ".", "action_space", "\n", "\n", "self", ".", "stats", "=", "get_stats", "(", ")", "\n", "\n", "self", ".", "obs_dtype", "=", "np", ".", "uint8", "\n", "self", ".", "obs_scaling", "=", "1", "/", "255.0", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.atari.AtariEnv._step": [[29, 36], ["atari.AtariEnv.mont_env.step"], "methods", ["home.repos.pwc.inspect_result.oxwhirl_opiq.envs.gridworld.GridWorld.step"], ["", "def", "_step", "(", "self", ",", "a", ")", ":", "\n", "        ", "self", ".", "steps", "+=", "1", "\n", "s", ",", "r", ",", "finished", ",", "info", "=", "self", ".", "mont_env", ".", "step", "(", "a", ")", "\n", "if", "self", ".", "steps", ">=", "self", ".", "max_timesteps", "and", "not", "finished", ":", "\n", "            ", "finished", "=", "True", "\n", "info", "[", "\"Steps_Termination\"", "]", "=", "True", "\n", "", "return", "s", ",", "r", ",", "finished", ",", "info", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.atari.AtariEnv._reset": [[37, 40], ["atari.AtariEnv.mont_env.reset"], "methods", ["home.repos.pwc.inspect_result.oxwhirl_opiq.envs.gridworld.GridWorld.reset"], ["", "def", "_reset", "(", "self", ")", ":", "\n", "        ", "self", ".", "steps", "=", "0", "\n", "return", "self", ".", "mont_env", ".", "reset", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.atari.AtariEnv._render": [[41, 43], ["atari.AtariEnv.mont_env.render"], "methods", ["home.repos.pwc.inspect_result.oxwhirl_opiq.envs.gridworld.GridWorld.render"], ["", "def", "_render", "(", "self", ",", "mode", "=", "\"rgb_array\"", ",", "close", "=", "False", ")", ":", "\n", "        ", "return", "self", ".", "mont_env", ".", "render", "(", "mode", "=", "mode", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.atari.AtariEnv._close": [[44, 46], ["atari.AtariEnv.mont_env.close"], "methods", ["home.repos.pwc.inspect_result.oxwhirl_opiq.envs.montezuma.Montezuma.close"], ["", "def", "_close", "(", "self", ")", ":", "\n", "        ", "self", ".", "mont_env", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.atari.AtariEnv.log_player_pos": [[47, 49], ["None"], "methods", ["None"], ["", "def", "log_player_pos", "(", "self", ")", ":", "\n", "        ", "return", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.atari.AtariEnv.trained_on_states": [[50, 52], ["None"], "methods", ["None"], ["", "def", "trained_on_states", "(", "self", ",", "player_visits", ",", "args", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.atari.AtariEnv.xp_and_frontier_states": [[53, 55], ["None"], "methods", ["None"], ["", "def", "xp_and_frontier_states", "(", "self", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.atari.AtariEnv.bonus_xp_and_frontier_states": [[56, 58], ["None"], "methods", ["None"], ["", "def", "bonus_xp_and_frontier_states", "(", "self", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.atari.AtariEnv.visits_and_frontier_states": [[59, 61], ["None"], "methods", ["None"], ["", "def", "visits_and_frontier_states", "(", "self", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.atari.AtariEnv.xp_replay_states": [[62, 64], ["None"], "methods", ["None"], ["", "def", "xp_replay_states", "(", "self", ",", "player_visits", ",", "args", ",", "bonus_replay", "=", "False", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.atari.AtariEnv.player_visits": [[65, 67], ["None"], "methods", ["None"], ["", "def", "player_visits", "(", "self", ",", "player_visits", ",", "args", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.atari.AtariEnv.bonus_landscape": [[68, 70], ["None"], "methods", ["None"], ["", "def", "bonus_landscape", "(", "self", ",", "player_visits", ",", "exploration_bonuses", ",", "max_bonus", ",", "args", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.atari.AtariEnv.frontier": [[71, 73], ["None"], "methods", ["None"], ["", "def", "frontier", "(", "self", ",", "exp_model", ",", "args", ",", "max_bonus", "=", "None", ")", ":", "\n", "        ", "pass", "", "", "", ""]], "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.maze.SnakingMaze.__init__": [[9, 31], ["logging.getLogger", "gridworld.GridWorld.__init__", "maze.SnakingMaze.logger.critical", "range", "maze.SnakingMaze.logger.critical", "range"], "methods", ["home.repos.pwc.inspect_result.oxwhirl_opiq.action.count_action.CountBonus.__init__"], ["    ", "def", "__init__", "(", "self", ",", "size", ",", "rnd_seed", "=", "13", ",", "corridor_width", "=", "10", ",", "neg_reward", "=", "True", ",", "randomise", "=", "False", ",", "num_actions", "=", "4", ",", "danger", "=", "False", ")", ":", "\n", "        ", "self", ".", "size", "=", "size", "\n", "self", ".", "rnd_seed", "=", "rnd_seed", "\n", "self", ".", "_seed", "=", "self", ".", "rnd_seed", "\n", "self", ".", "corridor_width", "=", "corridor_width", "\n", "self", ".", "logger", "=", "logging", ".", "getLogger", "(", "\"Maze\"", ")", "\n", "super", "(", ")", ".", "__init__", "(", "randomise", "=", "randomise", ",", "num_actions", "=", "num_actions", ",", "danger", "=", "danger", ")", "\n", "self", ".", "logger", ".", "critical", "(", "\"Maze of size: {}\"", ".", "format", "(", "self", ".", "grid", ".", "shape", ")", ")", "\n", "\n", "# Count number of states", "\n", "num_states", "=", "0", "\n", "for", "x", "in", "range", "(", "self", ".", "grid", ".", "shape", "[", "0", "]", ")", ":", "\n", "            ", "for", "y", "in", "range", "(", "self", ".", "grid", ".", "shape", "[", "1", "]", ")", ":", "\n", "                ", "if", "self", ".", "grid", "[", "x", ",", "y", "]", "!=", "1", ":", "\n", "                    ", "num_states", "+=", "1", "\n", "\n", "", "", "", "self", ".", "logger", ".", "critical", "(", "\"Number of states: {}\"", ".", "format", "(", "num_states", ")", ")", "\n", "\n", "# No negative reward at each timestep", "\n", "if", "not", "neg_reward", ":", "\n", "            ", "self", ".", "negative_reward", "=", "0", "\n", "", "self", ".", "positive_reward", "=", "+", "10", "# Clipped to +1 if we are reward clipping", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.maze.SnakingMaze.create_grid": [[32, 187], ["numpy.ones", "int", "int", "random.seed", "numpy.zeros", "numpy.zeros", "range", "len", "range", "check.append", "check.append", "check.append", "check.append", "len", "len", "history.append", "random.choice", "history.pop", "range", "int", "int", "range", "range", "range", "range", "range"], "methods", ["None"], ["", "def", "create_grid", "(", "self", ")", ":", "\n", "        ", "self", ".", "grid", "=", "np", ".", "ones", "(", "shape", "=", "(", "self", ".", "size", "*", "10", ",", "self", ".", "size", "*", "10", ")", ")", "\n", "self", ".", "grid", "[", "1", ":", "-", "1", ",", "1", ":", "-", "1", "]", "=", "0", "\n", "# Goal", "\n", "self", ".", "grid", "[", "1", ",", "-", "2", "]", "=", "2", "\n", "# Player", "\n", "self", ".", "grid", "[", "-", "2", ",", "1", "]", "=", "3", "\n", "\n", "# Copied from https://en.wikipedia.org/wiki/Maze_generation_algorithm#Python_code_examples", "\n", "num_rows", "=", "int", "(", "self", ".", "size", ")", "# number of rows", "\n", "num_cols", "=", "int", "(", "self", ".", "size", ")", "# number of columns", "\n", "corridor_width", "=", "self", ".", "corridor_width", "\n", "if", "self", ".", "size", "==", "18", ":", "\n", "# Hack", "\n", "            ", "self", ".", "rnd_seed", "=", "2", "\n", "", "random", ".", "seed", "(", "self", ".", "rnd_seed", ")", "\n", "# The array M is going to hold the array information for each cell.", "\n", "# The first four coordinates tell if walls exist on those sides", "\n", "# and the fifth indicates if the cell has been visited in the search.", "\n", "# M(LEFT, UP, RIGHT, DOWN, CHECK_IF_VISITED)", "\n", "M", "=", "np", ".", "zeros", "(", "(", "num_rows", ",", "num_cols", ",", "5", ")", ",", "dtype", "=", "np", ".", "uint8", ")", "\n", "\n", "# The array image is going to be the output image to display", "\n", "image", "=", "np", ".", "zeros", "(", "\n", "(", "num_rows", "*", "corridor_width", ",", "num_cols", "*", "corridor_width", ")", ",", "dtype", "=", "np", ".", "uint8", "\n", ")", "\n", "\n", "# Set starting row and column", "\n", "r", "=", "0", "\n", "c", "=", "0", "\n", "history", "=", "[", "(", "r", ",", "c", ")", "]", "# The history is the stack of visited locations", "\n", "\n", "max_history_len", "=", "0", "\n", "max_pair", "=", "(", "0", ",", "0", ")", "\n", "offset", "=", "(", "0", ",", "0", ")", "\n", "prev", "=", "(", "0", ",", "0", ")", "\n", "prev_max", "=", "(", "0", ",", "0", ")", "\n", "# Trace a path though the cells of the maze and open walls along the path.", "\n", "# We do this with a while loop, repeating the loop until there is no history,", "\n", "# which would mean we backtracked to the initial start.", "\n", "while", "history", ":", "\n", "            ", "M", "[", "r", ",", "c", ",", "4", "]", "=", "1", "# designate this location as visited", "\n", "# check if the adjacent cells are valid for moving to", "\n", "check", "=", "[", "]", "\n", "if", "c", ">", "0", "and", "M", "[", "r", ",", "c", "-", "1", ",", "4", "]", "==", "0", ":", "\n", "                ", "check", ".", "append", "(", "\"L\"", ")", "\n", "", "if", "r", ">", "0", "and", "M", "[", "r", "-", "1", ",", "c", ",", "4", "]", "==", "0", ":", "\n", "                ", "check", ".", "append", "(", "\"U\"", ")", "\n", "", "if", "c", "<", "num_cols", "-", "1", "and", "M", "[", "r", ",", "c", "+", "1", ",", "4", "]", "==", "0", ":", "\n", "                ", "check", ".", "append", "(", "\"R\"", ")", "\n", "", "if", "r", "<", "num_rows", "-", "1", "and", "M", "[", "r", "+", "1", ",", "c", ",", "4", "]", "==", "0", ":", "\n", "                ", "check", ".", "append", "(", "\"D\"", ")", "\n", "\n", "", "if", "len", "(", "history", ")", ">", "max_history_len", ":", "\n", "                ", "max_pair", "=", "(", "r", ",", "c", ")", "\n", "prev_max", "=", "(", "prev", "[", "0", "]", ",", "prev", "[", "1", "]", ")", "\n", "max_history_len", "=", "len", "(", "history", ")", "\n", "", "if", "len", "(", "check", ")", ":", "# If there is a valid cell to move to.", "\n", "# Mark the walls between cells as open if we move", "\n", "                ", "prev", "=", "(", "r", ",", "c", ")", "\n", "history", ".", "append", "(", "[", "r", ",", "c", "]", ")", "\n", "move_direction", "=", "random", ".", "choice", "(", "check", ")", "\n", "if", "move_direction", "==", "\"L\"", ":", "\n", "                    ", "M", "[", "r", ",", "c", ",", "0", "]", "=", "1", "\n", "c", "=", "c", "-", "1", "\n", "M", "[", "r", ",", "c", ",", "2", "]", "=", "1", "\n", "", "if", "move_direction", "==", "\"U\"", ":", "\n", "                    ", "M", "[", "r", ",", "c", ",", "1", "]", "=", "1", "\n", "r", "=", "r", "-", "1", "\n", "M", "[", "r", ",", "c", ",", "3", "]", "=", "1", "\n", "", "if", "move_direction", "==", "\"R\"", ":", "\n", "                    ", "M", "[", "r", ",", "c", ",", "2", "]", "=", "1", "\n", "c", "=", "c", "+", "1", "\n", "M", "[", "r", ",", "c", ",", "0", "]", "=", "1", "\n", "", "if", "move_direction", "==", "\"D\"", ":", "\n", "                    ", "M", "[", "r", ",", "c", ",", "3", "]", "=", "1", "\n", "r", "=", "r", "+", "1", "\n", "M", "[", "r", ",", "c", ",", "1", "]", "=", "1", "\n", "", "", "else", ":", "# If there are no valid cells to move to.", "\n", "# retrace one step back in history if no move is possible", "\n", "                ", "r", ",", "c", "=", "history", ".", "pop", "(", ")", "\n", "\n", "# Open the walls at the start and finish", "\n", "# M[0,0,0] = 1", "\n", "# M[num_rows-1,num_cols-1,2] = 1", "\n", "# print(M)", "\n", "\n", "# Generate the image for display", "\n", "", "", "for", "row", "in", "range", "(", "0", ",", "num_rows", ")", ":", "\n", "            ", "for", "col", "in", "range", "(", "0", ",", "num_cols", ")", ":", "\n", "                ", "cell_data", "=", "M", "[", "row", ",", "col", "]", "\n", "for", "i", "in", "range", "(", "\n", "corridor_width", "*", "row", "+", "1", ",", "corridor_width", "*", "row", "+", "corridor_width", "\n", ")", ":", "\n", "                    ", "image", "[", "\n", "i", ",", "\n", "range", "(", "\n", "corridor_width", "*", "col", "+", "1", ",", "\n", "corridor_width", "*", "col", "+", "corridor_width", ",", "\n", ")", ",", "\n", "]", "=", "1", "\n", "if", "cell_data", "[", "0", "]", "==", "1", ":", "\n", "                        ", "image", "[", "\n", "range", "(", "\n", "corridor_width", "*", "row", "+", "1", ",", "\n", "corridor_width", "*", "row", "+", "corridor_width", ",", "\n", ")", ",", "\n", "corridor_width", "*", "col", ",", "\n", "]", "=", "1", "\n", "", "if", "cell_data", "[", "1", "]", "==", "1", ":", "\n", "                        ", "image", "[", "\n", "corridor_width", "*", "row", ",", "\n", "range", "(", "\n", "corridor_width", "*", "col", "+", "1", ",", "\n", "corridor_width", "*", "col", "+", "corridor_width", ",", "\n", ")", ",", "\n", "]", "=", "1", "\n", "", "if", "cell_data", "[", "2", "]", "==", "1", ":", "\n", "                        ", "image", "[", "\n", "range", "(", "\n", "corridor_width", "*", "row", "+", "1", ",", "\n", "corridor_width", "*", "row", "+", "corridor_width", ",", "\n", ")", ",", "\n", "corridor_width", "*", "col", "+", "corridor_width", "-", "1", ",", "\n", "]", "=", "1", "\n", "", "if", "cell_data", "[", "3", "]", "==", "1", ":", "\n", "                        ", "image", "[", "\n", "corridor_width", "*", "row", "+", "corridor_width", "-", "1", ",", "\n", "range", "(", "\n", "corridor_width", "*", "col", "+", "1", ",", "\n", "corridor_width", "*", "col", "+", "corridor_width", ",", "\n", ")", ",", "\n", "]", "=", "1", "\n", "\n", "", "", "", "", "image", "=", "1", "-", "image", "\n", "\n", "# Fill in the bottom and right with walls", "\n", "image", "[", "-", "1", ",", ":", "]", "=", "1", "\n", "image", "[", ":", ",", "-", "1", "]", "=", "1", "\n", "\n", "image", "[", "1", ",", "1", "]", "=", "3", "\n", "# print(max_pair, prev_max)", "\n", "rr", ",", "cc", "=", "max_pair", "\n", "xx", "=", "0", "\n", "if", "rr", ">", "prev_max", "[", "0", "]", ":", "\n", "            ", "xx", "=", "1", "\n", "", "yy", "=", "0", "\n", "if", "cc", ">", "prev_max", "[", "1", "]", ":", "\n", "            ", "yy", "=", "1", "\n", "", "image", "[", "\n", "rr", "*", "corridor_width", "+", "int", "(", "(", "corridor_width", ")", "*", "xx", ")", "+", "(", "-", "1", ")", "**", "xx", ",", "\n", "cc", "*", "corridor_width", "+", "int", "(", "(", "corridor_width", "-", "0", ")", "*", "yy", ")", "+", "(", "-", "1", ")", "**", "yy", ",", "\n", "]", "=", "2", "\n", "# image[-2, -2] = 2", "\n", "self", ".", "grid", "=", "image", "\n", "", "", ""]], "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.nchain.NChain.__init__": [[11, 29], ["gym.spaces.Discrete", "numpy.random.seed", "gym.spaces.Box", "numpy.random.randint", "range"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "n", "=", "10", ",", "p", "=", "1", ",", "back", "=", "False", ")", ":", "\n", "\n", "        ", "self", ".", "n", "=", "n", "# Size of NChain", "\n", "self", ".", "p", "=", "p", "# Probability of succesfully transitioning right", "\n", "self", ".", "back", "=", "back", "# If we unsuccesfully transition right, do we go back a state", "\n", "\n", "self", ".", "action_space", "=", "spaces", ".", "Discrete", "(", "2", ")", "\n", "\n", "self", ".", "limit", "=", "n", "+", "9", "\n", "# Increase the limit if p < 1", "\n", "# self.limit *= 1 / p", "\n", "self", ".", "steps", "=", "0", "\n", "\n", "self", ".", "_seed", "=", "56", "\n", "np", ".", "random", ".", "seed", "(", "self", ".", "_seed", ")", "\n", "self", ".", "actions", "=", "[", "np", ".", "random", ".", "randint", "(", "2", ")", "for", "_", "in", "range", "(", "n", ")", "]", "# If we should reverse the actions at this state", "\n", "\n", "self", ".", "observation_space", "=", "spaces", ".", "Box", "(", "low", "=", "0", ",", "high", "=", "1", ",", "shape", "=", "(", "n", ",", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.nchain.NChain.update_limit": [[30, 33], ["print"], "methods", ["None"], ["", "def", "update_limit", "(", "self", ",", "new_limit", ")", ":", "\n", "        ", "self", ".", "limit", "=", "new_limit", "\n", "print", "(", "\"New Limit:\"", ",", "self", ".", "limit", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.nchain.NChain._step": [[34, 71], ["nchain.NChain._get_therm", "max", "numpy.random.random", "min", "max"], "methods", ["home.repos.pwc.inspect_result.oxwhirl_opiq.envs.nchain.NChain._get_therm"], ["", "def", "_step", "(", "self", ",", "a", ")", ":", "\n", "        ", "self", ".", "steps", "+=", "1", "\n", "\n", "a_r", "=", "(", "a", "+", "self", ".", "actions", "[", "self", ".", "pos", "]", ")", "%", "2", "\n", "\n", "r", "=", "0", "\n", "if", "self", ".", "pos", "==", "0", "and", "a_r", "==", "0", ":", "\n", "            ", "r", "=", "0.001", "\n", "", "if", "self", ".", "pos", "==", "self", ".", "n", "-", "1", "and", "a_r", "==", "1", ":", "\n", "            ", "r", "=", "1", "\n", "\n", "", "new_pos", "=", "self", ".", "pos", "\n", "if", "a_r", "==", "0", ":", "\n", "            ", "new_pos", "=", "max", "(", "0", ",", "self", ".", "pos", "-", "1", ")", "\n", "", "elif", "a_r", "==", "1", ":", "\n", "            ", "if", "np", ".", "random", ".", "random", "(", ")", "<", "self", ".", "p", ":", "\n", "# Succesfully transition right", "\n", "                ", "new_pos", "=", "min", "(", "self", ".", "n", "-", "1", ",", "self", ".", "pos", "+", "1", ")", "\n", "", "else", ":", "\n", "                ", "if", "self", ".", "back", ":", "\n", "# Go back", "\n", "                    ", "new_pos", "=", "max", "(", "0", ",", "self", ".", "pos", "-", "1", ")", "\n", "", "else", ":", "\n", "# Stay put", "\n", "                    ", "new_pos", "=", "self", ".", "pos", "\n", "\n", "", "", "", "self", ".", "pos", "=", "new_pos", "\n", "\n", "s", "=", "self", ".", "_get_therm", "(", ")", "\n", "\n", "info", "=", "{", "}", "\n", "finished", "=", "False", "\n", "if", "self", ".", "steps", "==", "self", ".", "limit", ":", "\n", "            ", "finished", "=", "True", "\n", "info", "[", "\"Steps_Termination\"", "]", "=", "True", "\n", "\n", "", "return", "s", ",", "r", ",", "finished", ",", "info", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.nchain.NChain._get_therm": [[72, 76], ["numpy.zeros"], "methods", ["None"], ["", "def", "_get_therm", "(", "self", ")", ":", "\n", "        ", "s", "=", "np", ".", "zeros", "(", "shape", "=", "(", "self", ".", "n", ",", ")", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "s", "[", ":", "self", ".", "pos", "+", "1", "]", "=", "1", "\n", "return", "s", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.nchain.NChain._reset": [[77, 81], ["nchain.NChain._get_therm"], "methods", ["home.repos.pwc.inspect_result.oxwhirl_opiq.envs.nchain.NChain._get_therm"], ["", "def", "_reset", "(", "self", ")", ":", "\n", "        ", "self", ".", "steps", "=", "0", "\n", "self", ".", "pos", "=", "1", "\n", "return", "self", ".", "_get_therm", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.nchain.NChain._render": [[82, 84], ["None"], "methods", ["None"], ["", "def", "_render", "(", "self", ",", "mode", "=", "\"rgb_array\"", ",", "close", "=", "False", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.nchain.NChain._close": [[85, 87], ["None"], "methods", ["None"], ["", "def", "_close", "(", "self", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.nchain.NChain.log_player_pos": [[88, 90], ["None"], "methods", ["None"], ["", "def", "log_player_pos", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "pos", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.nchain.NChain.trained_on_states": [[91, 93], ["None"], "methods", ["None"], ["", "def", "trained_on_states", "(", "self", ",", "player_visits", ",", "args", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.nchain.NChain.xp_and_frontier_states": [[94, 96], ["None"], "methods", ["None"], ["", "def", "xp_and_frontier_states", "(", "self", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.nchain.NChain.bonus_xp_and_frontier_states": [[97, 99], ["None"], "methods", ["None"], ["", "def", "bonus_xp_and_frontier_states", "(", "self", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.nchain.NChain.visits_and_frontier_states": [[100, 102], ["None"], "methods", ["None"], ["", "def", "visits_and_frontier_states", "(", "self", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.nchain.NChain.xp_replay_states": [[103, 105], ["None"], "methods", ["None"], ["", "def", "xp_replay_states", "(", "self", ",", "player_visits", ",", "args", ",", "bonus_replay", "=", "False", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.nchain.NChain.player_visits": [[106, 108], ["None"], "methods", ["None"], ["", "def", "player_visits", "(", "self", ",", "player_visits", ",", "args", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.nchain.NChain.bonus_landscape": [[109, 111], ["None"], "methods", ["None"], ["", "def", "bonus_landscape", "(", "self", ",", "player_visits", ",", "exploration_bonuses", ",", "max_bonus", ",", "args", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.nchain.NChain.frontier": [[112, 114], ["None"], "methods", ["None"], ["", "def", "frontier", "(", "self", ",", "exp_model", ",", "args", ",", "max_bonus", "=", "None", ")", ":", "\n", "        ", "pass", "", "", "", ""]], "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.OpenAI_AtariWrapper.NoopResetEnv.__init__": [[14, 22], ["gym.Wrapper.__init__", "env.unwrapped.get_action_meanings"], "methods", ["home.repos.pwc.inspect_result.oxwhirl_opiq.action.count_action.CountBonus.__init__"], ["    ", "def", "__init__", "(", "self", ",", "env", ",", "noop_max", "=", "30", ")", ":", "\n", "        ", "\"\"\"Sample initial states by taking random number of no-ops on reset.\n        No-op is assumed to be action 0.\n        \"\"\"", "\n", "gym", ".", "Wrapper", ".", "__init__", "(", "self", ",", "env", ")", "\n", "self", ".", "noop_max", "=", "noop_max", "\n", "self", ".", "override_num_noops", "=", "None", "\n", "assert", "env", ".", "unwrapped", ".", "get_action_meanings", "(", ")", "[", "0", "]", "==", "'NOOP'", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.OpenAI_AtariWrapper.NoopResetEnv._reset": [[23, 37], ["OpenAI_AtariWrapper.NoopResetEnv.env.reset", "range", "OpenAI_AtariWrapper.NoopResetEnv.unwrapped.np_random.randint", "OpenAI_AtariWrapper.NoopResetEnv.env.step", "OpenAI_AtariWrapper.NoopResetEnv.env.reset"], "methods", ["home.repos.pwc.inspect_result.oxwhirl_opiq.envs.gridworld.GridWorld.reset", "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.gridworld.GridWorld.step", "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.gridworld.GridWorld.reset"], ["", "def", "_reset", "(", "self", ")", ":", "\n", "        ", "\"\"\" Do no-op action for a number of steps in [1, noop_max].\"\"\"", "\n", "self", ".", "env", ".", "reset", "(", ")", "\n", "if", "self", ".", "override_num_noops", "is", "not", "None", ":", "\n", "            ", "noops", "=", "self", ".", "override_num_noops", "\n", "", "else", ":", "\n", "            ", "noops", "=", "self", ".", "unwrapped", ".", "np_random", ".", "randint", "(", "1", ",", "self", ".", "noop_max", "+", "1", ")", "# pylint: disable=E1101", "\n", "", "assert", "noops", ">", "0", "\n", "obs", "=", "None", "\n", "for", "_", "in", "range", "(", "noops", ")", ":", "\n", "            ", "obs", ",", "_", ",", "done", ",", "_", "=", "self", ".", "env", ".", "step", "(", "0", ")", "\n", "if", "done", ":", "\n", "                ", "obs", "=", "self", ".", "env", ".", "reset", "(", ")", "\n", "", "", "return", "obs", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.OpenAI_AtariWrapper.FireResetEnv.__init__": [[40, 45], ["gym.Wrapper.__init__", "len", "env.unwrapped.get_action_meanings", "env.unwrapped.get_action_meanings"], "methods", ["home.repos.pwc.inspect_result.oxwhirl_opiq.action.count_action.CountBonus.__init__"], ["    ", "def", "__init__", "(", "self", ",", "env", ")", ":", "\n", "        ", "\"\"\"Take action on reset for environments that are fixed until firing.\"\"\"", "\n", "gym", ".", "Wrapper", ".", "__init__", "(", "self", ",", "env", ")", "\n", "assert", "env", ".", "unwrapped", ".", "get_action_meanings", "(", ")", "[", "1", "]", "==", "'FIRE'", "\n", "assert", "len", "(", "env", ".", "unwrapped", ".", "get_action_meanings", "(", ")", ")", ">=", "3", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.OpenAI_AtariWrapper.FireResetEnv._reset": [[46, 55], ["OpenAI_AtariWrapper.FireResetEnv.env.reset", "OpenAI_AtariWrapper.FireResetEnv.env.step", "OpenAI_AtariWrapper.FireResetEnv.env.step", "OpenAI_AtariWrapper.FireResetEnv.env.reset", "OpenAI_AtariWrapper.FireResetEnv.env.reset"], "methods", ["home.repos.pwc.inspect_result.oxwhirl_opiq.envs.gridworld.GridWorld.reset", "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.gridworld.GridWorld.step", "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.gridworld.GridWorld.step", "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.gridworld.GridWorld.reset", "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.gridworld.GridWorld.reset"], ["", "def", "_reset", "(", "self", ")", ":", "\n", "        ", "self", ".", "env", ".", "reset", "(", ")", "\n", "obs", ",", "_", ",", "done", ",", "_", "=", "self", ".", "env", ".", "step", "(", "1", ")", "\n", "if", "done", ":", "\n", "            ", "self", ".", "env", ".", "reset", "(", ")", "\n", "", "obs", ",", "_", ",", "done", ",", "_", "=", "self", ".", "env", ".", "step", "(", "2", ")", "\n", "if", "done", ":", "\n", "            ", "self", ".", "env", ".", "reset", "(", ")", "\n", "", "return", "obs", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.OpenAI_AtariWrapper.EpisodicLifeEnv.__init__": [[58, 65], ["gym.Wrapper.__init__"], "methods", ["home.repos.pwc.inspect_result.oxwhirl_opiq.action.count_action.CountBonus.__init__"], ["    ", "def", "__init__", "(", "self", ",", "env", ")", ":", "\n", "        ", "\"\"\"Make end-of-life == end-of-episode, but only reset on true game over.\n        Done by DeepMind for the DQN and co. since it helps value estimation.\n        \"\"\"", "\n", "gym", ".", "Wrapper", ".", "__init__", "(", "self", ",", "env", ")", "\n", "self", ".", "lives", "=", "0", "\n", "self", ".", "was_real_done", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.OpenAI_AtariWrapper.EpisodicLifeEnv._step": [[66, 79], ["OpenAI_AtariWrapper.EpisodicLifeEnv.env.step", "OpenAI_AtariWrapper.EpisodicLifeEnv.env.unwrapped.ale.lives"], "methods", ["home.repos.pwc.inspect_result.oxwhirl_opiq.envs.gridworld.GridWorld.step"], ["", "def", "_step", "(", "self", ",", "action", ")", ":", "\n", "        ", "obs", ",", "reward", ",", "done", ",", "info", "=", "self", ".", "env", ".", "step", "(", "action", ")", "\n", "self", ".", "was_real_done", "=", "done", "\n", "# check current lives, make loss of life terminal,", "\n", "# then update lives to handle bonus lives", "\n", "lives", "=", "self", ".", "env", ".", "unwrapped", ".", "ale", ".", "lives", "(", ")", "\n", "if", "lives", "<", "self", ".", "lives", "and", "lives", ">", "0", ":", "\n", "# for Qbert somtimes we stay in lives == 0 condtion for a few frames", "\n", "# so its important to keep lives > 0, so that we only reset once", "\n", "# the environment advertises done.", "\n", "            ", "done", "=", "True", "\n", "", "self", ".", "lives", "=", "lives", "\n", "return", "obs", ",", "reward", ",", "done", ",", "info", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.OpenAI_AtariWrapper.EpisodicLifeEnv._reset": [[80, 92], ["OpenAI_AtariWrapper.EpisodicLifeEnv.env.unwrapped.ale.lives", "OpenAI_AtariWrapper.EpisodicLifeEnv.env.reset", "OpenAI_AtariWrapper.EpisodicLifeEnv.env.step"], "methods", ["home.repos.pwc.inspect_result.oxwhirl_opiq.envs.gridworld.GridWorld.reset", "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.gridworld.GridWorld.step"], ["", "def", "_reset", "(", "self", ")", ":", "\n", "        ", "\"\"\"Reset only when lives are exhausted.\n        This way all states are still reachable even though lives are episodic,\n        and the learner need not know about any of this behind-the-scenes.\n        \"\"\"", "\n", "if", "self", ".", "was_real_done", ":", "\n", "            ", "obs", "=", "self", ".", "env", ".", "reset", "(", ")", "\n", "", "else", ":", "\n", "# no-op step to advance from terminal/lost life state", "\n", "            ", "obs", ",", "_", ",", "_", ",", "_", "=", "self", ".", "env", ".", "step", "(", "0", ")", "\n", "", "self", ".", "lives", "=", "self", ".", "env", ".", "unwrapped", ".", "ale", ".", "lives", "(", ")", "\n", "return", "obs", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.OpenAI_AtariWrapper.MaxAndSkipEnv.__init__": [[95, 101], ["gym.Wrapper.__init__", "collections.deque"], "methods", ["home.repos.pwc.inspect_result.oxwhirl_opiq.action.count_action.CountBonus.__init__"], ["    ", "def", "__init__", "(", "self", ",", "env", ",", "skip", "=", "4", ",", "max_over", "=", "2", ")", ":", "\n", "        ", "\"\"\"Return only every `skip`-th frame\"\"\"", "\n", "gym", ".", "Wrapper", ".", "__init__", "(", "self", ",", "env", ")", "\n", "# most recent raw observations (for max pooling across time steps)", "\n", "self", ".", "_obs_buffer", "=", "deque", "(", "maxlen", "=", "max_over", ")", "\n", "self", ".", "_skip", "=", "skip", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.OpenAI_AtariWrapper.MaxAndSkipEnv._step": [[102, 115], ["range", "numpy.max", "OpenAI_AtariWrapper.MaxAndSkipEnv.env.step", "OpenAI_AtariWrapper.MaxAndSkipEnv._obs_buffer.append", "numpy.stack"], "methods", ["home.repos.pwc.inspect_result.oxwhirl_opiq.envs.gridworld.GridWorld.step"], ["", "def", "_step", "(", "self", ",", "action", ")", ":", "\n", "        ", "\"\"\"Repeat action, sum reward, and max over last observations.\"\"\"", "\n", "total_reward", "=", "0.0", "\n", "done", "=", "None", "\n", "for", "_", "in", "range", "(", "self", ".", "_skip", ")", ":", "\n", "            ", "obs", ",", "reward", ",", "done", ",", "info", "=", "self", ".", "env", ".", "step", "(", "action", ")", "\n", "self", ".", "_obs_buffer", ".", "append", "(", "obs", ")", "\n", "total_reward", "+=", "reward", "\n", "if", "done", ":", "\n", "                ", "break", "\n", "", "", "max_frame", "=", "np", ".", "max", "(", "np", ".", "stack", "(", "self", ".", "_obs_buffer", ")", ",", "axis", "=", "0", ")", "\n", "\n", "return", "max_frame", ",", "total_reward", ",", "done", ",", "info", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.OpenAI_AtariWrapper.MaxAndSkipEnv._reset": [[116, 122], ["OpenAI_AtariWrapper.MaxAndSkipEnv._obs_buffer.clear", "OpenAI_AtariWrapper.MaxAndSkipEnv.env.reset", "OpenAI_AtariWrapper.MaxAndSkipEnv._obs_buffer.append"], "methods", ["home.repos.pwc.inspect_result.oxwhirl_opiq.envs.gridworld.GridWorld.reset"], ["", "def", "_reset", "(", "self", ")", ":", "\n", "        ", "\"\"\"Clear past frame buffer and init. to first obs. from inner env.\"\"\"", "\n", "self", ".", "_obs_buffer", ".", "clear", "(", ")", "\n", "obs", "=", "self", ".", "env", ".", "reset", "(", ")", "\n", "self", ".", "_obs_buffer", ".", "append", "(", "obs", ")", "\n", "return", "obs", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.OpenAI_AtariWrapper.ClipRewardEnv._reward": [[125, 128], ["numpy.sign"], "methods", ["None"], ["    ", "def", "_reward", "(", "self", ",", "reward", ")", ":", "\n", "        ", "\"\"\"Bin reward to {+1, 0, -1} by its sign.\"\"\"", "\n", "return", "np", ".", "sign", "(", "reward", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.OpenAI_AtariWrapper.ClipNegativeRewardEnv._reward": [[131, 137], ["None"], "methods", ["None"], ["    ", "def", "_reward", "(", "self", ",", "reward", ")", ":", "\n", "        ", "\"\"\"Bin reward to {+1, 0, -1} by its sign.\"\"\"", "\n", "if", "reward", ">=", "0", ":", "\n", "            ", "return", "reward", "\n", "", "else", ":", "\n", "            ", "return", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.OpenAI_AtariWrapper.NoRewardEnv._reward": [[140, 142], ["None"], "methods", ["None"], ["    ", "def", "_reward", "(", "self", ",", "reward", ")", ":", "\n", "        ", "return", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.OpenAI_AtariWrapper.WarpFrame.__init__": [[145, 150], ["gym.ObservationWrapper.__init__", "gym.spaces.Box"], "methods", ["home.repos.pwc.inspect_result.oxwhirl_opiq.action.count_action.CountBonus.__init__"], ["    ", "def", "__init__", "(", "self", ",", "env", ",", "res", "=", "84", ")", ":", "\n", "        ", "\"\"\"Warp frames to 84x84 as done in the Nature paper and later work.\"\"\"", "\n", "gym", ".", "ObservationWrapper", ".", "__init__", "(", "self", ",", "env", ")", "\n", "self", ".", "res", "=", "res", "\n", "self", ".", "observation_space", "=", "spaces", ".", "Box", "(", "low", "=", "0", ",", "high", "=", "255", ",", "shape", "=", "(", "self", ".", "res", ",", "self", ".", "res", ",", "1", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.OpenAI_AtariWrapper.WarpFrame._observation": [[151, 156], ["numpy.dot", "numpy.array", "obs.astype", "numpy.array", "PIL.Image.fromarray().resize", "numpy.array.reshape", "PIL.Image.fromarray"], "methods", ["None"], ["", "def", "_observation", "(", "self", ",", "obs", ")", ":", "\n", "        ", "frame", "=", "np", ".", "dot", "(", "obs", ".", "astype", "(", "'float32'", ")", ",", "np", ".", "array", "(", "[", "0.299", ",", "0.587", ",", "0.114", "]", ",", "'float32'", ")", ")", "\n", "frame", "=", "np", ".", "array", "(", "Image", ".", "fromarray", "(", "frame", ")", ".", "resize", "(", "(", "self", ".", "res", ",", "self", ".", "res", ")", ",", "\n", "resample", "=", "Image", ".", "BILINEAR", ")", ",", "dtype", "=", "np", ".", "uint8", ")", "\n", "return", "frame", ".", "reshape", "(", "(", "self", ".", "res", ",", "self", ".", "res", ",", "1", ")", ")", "/", "255.0", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.OpenAI_AtariWrapper.ResizeFrame.__init__": [[159, 164], ["gym.ObservationWrapper.__init__", "gym.spaces.Box"], "methods", ["home.repos.pwc.inspect_result.oxwhirl_opiq.action.count_action.CountBonus.__init__"], ["    ", "def", "__init__", "(", "self", ",", "env", ",", "res", "=", "40", ")", ":", "\n", "        ", "\"\"\"Warp frames to 84x84 as done in the Nature paper and later work.\"\"\"", "\n", "gym", ".", "ObservationWrapper", ".", "__init__", "(", "self", ",", "env", ")", "\n", "self", ".", "res", "=", "res", "\n", "self", ".", "observation_space", "=", "spaces", ".", "Box", "(", "low", "=", "0", ",", "high", "=", "1", ",", "shape", "=", "(", "self", ".", "res", ",", "self", ".", "res", ",", "1", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.OpenAI_AtariWrapper.ResizeFrame._observation": [[165, 174], ["numpy.array", "obs.astype", "PIL.Image.fromarray().resize", "numpy.array.reshape", "PIL.Image.fromarray"], "methods", ["None"], ["", "def", "_observation", "(", "self", ",", "obs", ")", ":", "\n", "        ", "frame", "=", "obs", ".", "astype", "(", "\"float32\"", ")", "*", "255", "\n", "frame", "=", "frame", "[", ":", ",", ":", ",", "0", "]", "\n", "# frame = np.concatenate([frame, frame, frame], axis=2)", "\n", "# frame = np.dot(frame, np.array([0.299, 0.587, 0.114], 'float32'))", "\n", "\n", "frame", "=", "np", ".", "array", "(", "Image", ".", "fromarray", "(", "frame", ")", ".", "resize", "(", "(", "self", ".", "res", ",", "self", ".", "res", ")", ",", "\n", "resample", "=", "Image", ".", "BILINEAR", ")", ",", "dtype", "=", "np", ".", "uint8", ")", "\n", "return", "frame", ".", "reshape", "(", "(", "self", ".", "res", ",", "self", ".", "res", ",", "1", ")", ")", "/", "255.0", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.OpenAI_AtariWrapper.FrameStack.__init__": [[177, 185], ["gym.Wrapper.__init__", "collections.deque", "gym.spaces.Box"], "methods", ["home.repos.pwc.inspect_result.oxwhirl_opiq.action.count_action.CountBonus.__init__"], ["    ", "def", "__init__", "(", "self", ",", "env", ",", "k", ")", ":", "\n", "        ", "\"\"\"Buffer observations and stack across channels (last axis).\"\"\"", "\n", "gym", ".", "Wrapper", ".", "__init__", "(", "self", ",", "env", ")", "\n", "self", ".", "k", "=", "k", "\n", "self", ".", "frames", "=", "deque", "(", "[", "]", ",", "maxlen", "=", "k", ")", "\n", "shp", "=", "env", ".", "observation_space", ".", "shape", "\n", "assert", "shp", "[", "2", "]", "==", "1", "# can only stack 1-channel frames", "\n", "self", ".", "observation_space", "=", "spaces", ".", "Box", "(", "low", "=", "0", ",", "high", "=", "255", ",", "shape", "=", "(", "shp", "[", "0", "]", ",", "shp", "[", "1", "]", ",", "k", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.OpenAI_AtariWrapper.FrameStack._reset": [[186, 191], ["OpenAI_AtariWrapper.FrameStack.env.reset", "range", "OpenAI_AtariWrapper.FrameStack._observation", "OpenAI_AtariWrapper.FrameStack.frames.append"], "methods", ["home.repos.pwc.inspect_result.oxwhirl_opiq.envs.gridworld.GridWorld.reset", "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.OpenAI_AtariWrapper.FrameStack._observation"], ["", "def", "_reset", "(", "self", ")", ":", "\n", "        ", "\"\"\"Clear buffer and re-fill by duplicating the first observation.\"\"\"", "\n", "ob", "=", "self", ".", "env", ".", "reset", "(", ")", "\n", "for", "_", "in", "range", "(", "self", ".", "k", ")", ":", "self", ".", "frames", ".", "append", "(", "ob", ")", "\n", "return", "self", ".", "_observation", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.OpenAI_AtariWrapper.FrameStack._step": [[192, 196], ["OpenAI_AtariWrapper.FrameStack.env.step", "OpenAI_AtariWrapper.FrameStack.frames.append", "OpenAI_AtariWrapper.FrameStack._observation"], "methods", ["home.repos.pwc.inspect_result.oxwhirl_opiq.envs.gridworld.GridWorld.step", "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.OpenAI_AtariWrapper.FrameStack._observation"], ["", "def", "_step", "(", "self", ",", "action", ")", ":", "\n", "        ", "ob", ",", "reward", ",", "done", ",", "info", "=", "self", ".", "env", ".", "step", "(", "action", ")", "\n", "self", ".", "frames", ".", "append", "(", "ob", ")", "\n", "return", "self", ".", "_observation", "(", ")", ",", "reward", ",", "done", ",", "info", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.OpenAI_AtariWrapper.FrameStack._observation": [[197, 200], ["numpy.concatenate", "len"], "methods", ["None"], ["", "def", "_observation", "(", "self", ")", ":", "\n", "        ", "assert", "len", "(", "self", ".", "frames", ")", "==", "self", ".", "k", "\n", "return", "np", ".", "concatenate", "(", "self", ".", "frames", ",", "axis", "=", "2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.OpenAI_AtariWrapper.GreyscaleRender.__init__": [[203, 206], ["gym.Wrapper.__init__"], "methods", ["home.repos.pwc.inspect_result.oxwhirl_opiq.action.count_action.CountBonus.__init__"], ["    ", "def", "__init__", "(", "self", ",", "env", ")", ":", "\n", "        ", "gym", ".", "Wrapper", ".", "__init__", "(", "self", ",", "env", ")", "\n", "self", ".", "latest_obs", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.OpenAI_AtariWrapper.GreyscaleRender._step": [[207, 211], ["OpenAI_AtariWrapper.GreyscaleRender.env.step"], "methods", ["home.repos.pwc.inspect_result.oxwhirl_opiq.envs.gridworld.GridWorld.step"], ["", "def", "_step", "(", "self", ",", "action", ")", ":", "\n", "        ", "stuff", "=", "self", ".", "env", ".", "step", "(", "action", ")", "\n", "self", ".", "latest_obs", "=", "stuff", "[", "0", "]", "\n", "return", "stuff", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.OpenAI_AtariWrapper.GreyscaleRender.render": [[212, 222], ["OpenAI_AtariWrapper.GreyscaleRender.unwrapped._render", "numpy.stack", "range"], "methods", ["home.repos.pwc.inspect_result.oxwhirl_opiq.envs.nchain.NChain._render"], ["", "def", "render", "(", "self", ",", "mode", "=", "\"human\"", ",", "close", "=", "False", ")", ":", "\n", "        ", "if", "mode", "==", "\"human\"", ":", "\n", "            ", "self", ".", "unwrapped", ".", "_render", "(", "mode", "=", "mode", ",", "close", "=", "close", ")", "\n", "", "else", ":", "\n", "# print(self.unwrapped)", "\n", "# grid = self.env._observation()", "\n", "            ", "grid", "=", "self", ".", "latest_obs", "\n", "grid", "=", "grid", "[", ":", ",", ":", ",", "-", "1", "]", "\n", "grid", "=", "np", ".", "stack", "(", "[", "grid", "for", "_", "in", "range", "(", "3", ")", "]", ",", "axis", "=", "2", ")", "\n", "return", "grid", "*", "255", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.OpenAI_AtariWrapper.DiscreteToMultiDiscrete.__init__": [[335, 371], ["range", "isinstance", "enumerate", "isinstance", "range", "len", "len", "len", "Error", "range", "options.keys"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "multi_discrete", ",", "options", "=", "None", ")", ":", "\n", "# assert isinstance(multi_discrete, MultiDiscrete)", "\n", "        ", "self", ".", "multi_discrete", "=", "multi_discrete", "\n", "self", ".", "num_discrete_space", "=", "self", ".", "multi_discrete", ".", "n", "\n", "\n", "# Config 1", "\n", "if", "options", "is", "None", ":", "\n", "            ", "self", ".", "n", "=", "self", ".", "num_discrete_space", "+", "1", "# +1 for NOOP at beginning", "\n", "self", ".", "mapping", "=", "{", "i", ":", "[", "0", "]", "*", "self", ".", "num_discrete_space", "for", "i", "in", "range", "(", "self", ".", "n", ")", "}", "\n", "for", "i", "in", "range", "(", "self", ".", "num_discrete_space", ")", ":", "\n", "                ", "self", ".", "mapping", "[", "i", "+", "1", "]", "[", "i", "]", "=", "self", ".", "multi_discrete", ".", "high", "[", "i", "]", "\n", "\n", "# Config 2", "\n", "", "", "elif", "isinstance", "(", "options", ",", "list", ")", ":", "\n", "            ", "assert", "len", "(", "options", ")", "<=", "self", ".", "num_discrete_space", "\n", "self", ".", "n", "=", "len", "(", "options", ")", "+", "1", "# +1 for NOOP at beginning", "\n", "self", ".", "mapping", "=", "{", "i", ":", "[", "0", "]", "*", "self", ".", "num_discrete_space", "for", "i", "in", "range", "(", "self", ".", "n", ")", "}", "\n", "for", "i", ",", "disc_num", "in", "enumerate", "(", "options", ")", ":", "\n", "                ", "assert", "disc_num", "<", "self", ".", "num_discrete_space", "\n", "self", ".", "mapping", "[", "i", "+", "1", "]", "[", "disc_num", "]", "=", "self", ".", "multi_discrete", ".", "high", "[", "disc_num", "]", "\n", "\n", "# Config 3", "\n", "", "", "elif", "isinstance", "(", "options", ",", "dict", ")", ":", "\n", "            ", "self", ".", "n", "=", "len", "(", "options", ".", "keys", "(", ")", ")", "\n", "self", ".", "mapping", "=", "options", "\n", "# for i, key in enumerate(options.keys()):", "\n", "#     if i != key:", "\n", "#         raise Error('DiscreteToMultiDiscrete must contain ordered keys. ' \\", "\n", "#                     'Item {0} should have a key of \"{0}\", but key \"{1}\" found instead.'.format(i, key))", "\n", "#     if not self.multi_discrete.contains(options[key]):", "\n", "#         raise Error('DiscreteToMultiDiscrete mapping for key {0} is ' \\", "\n", "#                     'not contained in the underlying MultiDiscrete action space. ' \\", "\n", "#                     'Invalid mapping: {1}'.format(key, options[key]))", "\n", "# Unknown parameter provided", "\n", "", "else", ":", "\n", "            ", "raise", "Error", "(", "'DiscreteToMultiDiscrete - Invalid parameter provided.'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.OpenAI_AtariWrapper.DiscreteToMultiDiscrete.__call__": [[372, 374], ["None"], "methods", ["None"], ["", "", "def", "__call__", "(", "self", ",", "discrete_action", ")", ":", "\n", "        ", "return", "self", ".", "mapping", "[", "discrete_action", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.OpenAI_AtariWrapper.LimitedDiscreteActions.__init__": [[504, 519], ["gym.ActionWrapper.__init__", "len", "gym.spaces.Discrete", "len", "range", "zip", "itertools.combinations", "range", "zip", "itertools.permutations", "len", "len"], "methods", ["home.repos.pwc.inspect_result.oxwhirl_opiq.action.count_action.CountBonus.__init__"], ["def", "__init__", "(", "self", ",", "env", ",", "all_buttons", ",", "whitelist", "=", "KNOWN_BUTTONS", "|", "KNOWN_SHOULDERS", ")", ":", "\n", "        ", "gym", ".", "ActionWrapper", ".", "__init__", "(", "self", ",", "env", ")", "\n", "\n", "self", ".", "_num_buttons", "=", "len", "(", "all_buttons", ")", "\n", "button_keys", "=", "{", "i", "for", "i", "in", "range", "(", "len", "(", "all_buttons", ")", ")", "if", "all_buttons", "[", "i", "]", "in", "whitelist", "&", "self", ".", "KNOWN_BUTTONS", "}", "\n", "buttons", "=", "[", "(", ")", ",", "*", "zip", "(", "button_keys", ")", ",", "*", "itertools", ".", "combinations", "(", "button_keys", ",", "2", ")", "]", "\n", "shoulder_keys", "=", "{", "i", "for", "i", "in", "range", "(", "len", "(", "all_buttons", ")", ")", "if", "all_buttons", "[", "i", "]", "in", "whitelist", "&", "self", ".", "KNOWN_SHOULDERS", "}", "\n", "shoulders", "=", "[", "(", ")", ",", "*", "zip", "(", "shoulder_keys", ")", ",", "*", "itertools", ".", "permutations", "(", "shoulder_keys", ",", "2", ")", "]", "\n", "arrows", "=", "[", "(", ")", ",", "(", "4", ",", ")", ",", "(", "5", ",", ")", ",", "(", "6", ",", ")", ",", "(", "7", ",", ")", "]", "# (), up, down, left, right", "\n", "acts", "=", "[", "]", "\n", "acts", "+=", "arrows", "\n", "acts", "+=", "buttons", "[", "1", ":", "]", "\n", "acts", "+=", "[", "a", "+", "b", "for", "a", "in", "arrows", "[", "-", "2", ":", "]", "for", "b", "in", "buttons", "[", "1", ":", "]", "]", "\n", "self", ".", "_actions", "=", "acts", "\n", "self", ".", "action_space", "=", "gym", ".", "spaces", ".", "Discrete", "(", "len", "(", "self", ".", "_actions", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.OpenAI_AtariWrapper.LimitedDiscreteActions.action": [[520, 525], ["numpy.zeros"], "methods", ["None"], ["", "def", "action", "(", "self", ",", "a", ")", ":", "\n", "        ", "mask", "=", "np", ".", "zeros", "(", "self", ".", "_num_buttons", ")", "\n", "for", "i", "in", "self", ".", "_actions", "[", "a", "]", ":", "\n", "            ", "mask", "[", "i", "]", "=", "1", "\n", "", "return", "mask", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.OpenAI_AtariWrapper.MarioEnv.__init__": [[528, 539], ["gym.Wrapper.__init__"], "methods", ["home.repos.pwc.inspect_result.oxwhirl_opiq.action.count_action.CountBonus.__init__"], ["    ", "def", "__init__", "(", "self", ",", "env", "=", "None", ",", "tilesEnv", "=", "False", ")", ":", "\n", "        ", "\"\"\"Reset mario environment without actually restarting fceux everytime.\n        This speeds up unrolling by approximately 10 times.\n        \"\"\"", "\n", "super", "(", "MarioEnv", ",", "self", ")", ".", "__init__", "(", "env", ")", "\n", "self", ".", "resetCount", "=", "-", "1", "\n", "# reward is distance travelled. So normalize it with total distance", "\n", "# https://github.com/ppaquette/gym-super-mario/blob/master/ppaquette_gym_super_mario/lua/super-mario-bros.lua", "\n", "# However, we will not use this reward at all. It is only for completion.", "\n", "self", ".", "maxDistance", "=", "3000.0", "\n", "self", ".", "tilesEnv", "=", "tilesEnv", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.OpenAI_AtariWrapper.MarioEnv._reset": [[540, 555], ["OpenAI_AtariWrapper.MarioEnv.env.step", "info.get", "info.get", "print", "sys.stdout.flush", "OpenAI_AtariWrapper.MarioEnv.env.reset", "time.sleep", "OpenAI_AtariWrapper.MarioEnv.env.close", "OpenAI_AtariWrapper.MarioEnv._reset"], "methods", ["home.repos.pwc.inspect_result.oxwhirl_opiq.envs.gridworld.GridWorld.step", "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.gridworld.GridWorld.reset", "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.montezuma.Montezuma.close", "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.OpenAI_AtariWrapper.MarioEnv._reset"], ["", "def", "_reset", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "resetCount", "<", "0", ":", "\n", "            ", "print", "(", "'\\nDoing hard mario fceux reset (4 seconds wait) !'", ")", "\n", "sys", ".", "stdout", ".", "flush", "(", ")", "\n", "self", ".", "env", ".", "reset", "(", ")", "\n", "time", ".", "sleep", "(", "4", ")", "\n", "", "obs", ",", "_", ",", "_", ",", "info", "=", "self", ".", "env", ".", "step", "(", "7", ")", "# take right once to start game", "\n", "if", "info", ".", "get", "(", "'ignore'", ",", "False", ")", ":", "# assuming this happens only in beginning", "\n", "            ", "self", ".", "resetCount", "=", "-", "1", "\n", "self", ".", "env", ".", "close", "(", ")", "\n", "return", "self", ".", "_reset", "(", ")", "\n", "", "self", ".", "resetCount", "=", "info", ".", "get", "(", "'iteration'", ",", "-", "1", ")", "\n", "if", "self", ".", "tilesEnv", ":", "\n", "            ", "return", "obs", "\n", "", "return", "obs", "[", "24", ":", "-", "12", ",", "8", ":", "-", "8", ",", ":", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.OpenAI_AtariWrapper.MarioEnv._step": [[556, 566], ["OpenAI_AtariWrapper.MarioEnv.env.step", "info.get", "float", "OpenAI_AtariWrapper.MarioEnv.reset"], "methods", ["home.repos.pwc.inspect_result.oxwhirl_opiq.envs.gridworld.GridWorld.step", "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.gridworld.GridWorld.reset"], ["", "def", "_step", "(", "self", ",", "action", ")", ":", "\n", "        ", "obs", ",", "reward", ",", "done", ",", "info", "=", "self", ".", "env", ".", "step", "(", "action", ")", "\n", "if", "info", ".", "get", "(", "'ignore'", ",", "True", ")", ":", "\n", "            ", "return", "self", ".", "reset", "(", ")", ",", "0", ",", "False", ",", "info", "\n", "# print('info:', info)", "\n", "", "done", "=", "info", "[", "'iteration'", "]", ">", "self", ".", "resetCount", "\n", "reward", "=", "float", "(", "reward", ")", "/", "self", ".", "maxDistance", "# note: we do not use this rewards at all.", "\n", "if", "self", ".", "tilesEnv", ":", "\n", "            ", "return", "obs", ",", "reward", ",", "done", ",", "info", "\n", "", "return", "obs", "[", "24", ":", "-", "12", ",", "8", ":", "-", "8", ",", ":", "]", ",", "reward", ",", "done", ",", "info", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.OpenAI_AtariWrapper.MarioEnv._close": [[567, 570], ["OpenAI_AtariWrapper.MarioEnv.env.close"], "methods", ["home.repos.pwc.inspect_result.oxwhirl_opiq.envs.montezuma.Montezuma.close"], ["", "def", "_close", "(", "self", ")", ":", "\n", "        ", "self", ".", "resetCount", "=", "-", "1", "\n", "return", "self", ".", "env", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.OpenAI_AtariWrapper.MontezumaInfoWrapper.__init__": [[589, 593], ["gym.Wrapper.__init__", "set"], "methods", ["home.repos.pwc.inspect_result.oxwhirl_opiq.action.count_action.CountBonus.__init__"], ["    ", "def", "__init__", "(", "self", ",", "env", ",", "room_address", ")", ":", "\n", "        ", "super", "(", "MontezumaInfoWrapper", ",", "self", ")", ".", "__init__", "(", "env", ")", "\n", "self", ".", "room_address", "=", "room_address", "\n", "self", ".", "visited_rooms", "=", "set", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.OpenAI_AtariWrapper.MontezumaInfoWrapper.get_current_room": [[594, 598], ["unwrap().ale.getRAM", "int", "len", "OpenAI_AtariWrapper.unwrap"], "methods", ["home.repos.pwc.inspect_result.oxwhirl_opiq.envs.OpenAI_AtariWrapper.unwrap"], ["", "def", "get_current_room", "(", "self", ")", ":", "\n", "        ", "ram", "=", "unwrap", "(", "self", ".", "env", ")", ".", "ale", ".", "getRAM", "(", ")", "\n", "assert", "len", "(", "ram", ")", "==", "128", "\n", "return", "int", "(", "ram", "[", "self", ".", "room_address", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.OpenAI_AtariWrapper.MontezumaInfoWrapper.step": [[599, 608], ["OpenAI_AtariWrapper.MontezumaInfoWrapper.env.step", "OpenAI_AtariWrapper.MontezumaInfoWrapper.visited_rooms.add", "OpenAI_AtariWrapper.MontezumaInfoWrapper.get_current_room", "info[].update", "OpenAI_AtariWrapper.MontezumaInfoWrapper.visited_rooms.clear", "OpenAI_AtariWrapper.MontezumaInfoWrapper.visited_rooms.copy"], "methods", ["home.repos.pwc.inspect_result.oxwhirl_opiq.envs.gridworld.GridWorld.step", "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.OpenAI_AtariWrapper.MontezumaInfoWrapper.get_current_room"], ["", "def", "step", "(", "self", ",", "action", ")", ":", "\n", "        ", "obs", ",", "rew", ",", "done", ",", "info", "=", "self", ".", "env", ".", "step", "(", "action", ")", "\n", "self", ".", "visited_rooms", ".", "add", "(", "self", ".", "get_current_room", "(", ")", ")", "\n", "if", "done", ":", "\n", "            ", "if", "'episode'", "not", "in", "info", ":", "\n", "                ", "info", "[", "'episode'", "]", "=", "{", "}", "\n", "", "info", "[", "'episode'", "]", ".", "update", "(", "visited_rooms", "=", "self", ".", "visited_rooms", ".", "copy", "(", ")", ")", "\n", "self", ".", "visited_rooms", ".", "clear", "(", ")", "\n", "", "return", "obs", ",", "rew", ",", "done", ",", "info", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.OpenAI_AtariWrapper.StickyActionEnv.__init__": [[611, 615], ["gym.Wrapper.__init__"], "methods", ["home.repos.pwc.inspect_result.oxwhirl_opiq.action.count_action.CountBonus.__init__"], ["    ", "def", "__init__", "(", "self", ",", "env", ",", "p", "=", "0.25", ")", ":", "\n", "        ", "super", "(", "StickyActionEnv", ",", "self", ")", ".", "__init__", "(", "env", ")", "\n", "self", ".", "p", "=", "p", "\n", "self", ".", "last_action", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.OpenAI_AtariWrapper.StickyActionEnv.reset": [[616, 619], ["OpenAI_AtariWrapper.StickyActionEnv.env.reset"], "methods", ["home.repos.pwc.inspect_result.oxwhirl_opiq.envs.gridworld.GridWorld.reset"], ["", "def", "reset", "(", "self", ")", ":", "\n", "        ", "self", ".", "last_action", "=", "0", "\n", "return", "self", ".", "env", ".", "reset", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.OpenAI_AtariWrapper.StickyActionEnv.step": [[620, 626], ["OpenAI_AtariWrapper.StickyActionEnv.env.step", "OpenAI_AtariWrapper.StickyActionEnv.unwrapped.np_random.uniform"], "methods", ["home.repos.pwc.inspect_result.oxwhirl_opiq.envs.gridworld.GridWorld.step"], ["", "def", "step", "(", "self", ",", "action", ")", ":", "\n", "        ", "if", "self", ".", "unwrapped", ".", "np_random", ".", "uniform", "(", ")", "<", "self", ".", "p", ":", "\n", "            ", "action", "=", "self", ".", "last_action", "\n", "", "self", ".", "last_action", "=", "action", "\n", "obs", ",", "reward", ",", "done", ",", "info", "=", "self", ".", "env", ".", "step", "(", "action", ")", "\n", "return", "obs", ",", "reward", ",", "done", ",", "info", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.OpenAI_AtariWrapper.WarpFrame2.__init__": [[638, 645], ["gym.ObservationWrapper.__init__", "gym.spaces.Box"], "methods", ["home.repos.pwc.inspect_result.oxwhirl_opiq.action.count_action.CountBonus.__init__"], ["    ", "def", "__init__", "(", "self", ",", "env", ",", "res", "=", "84", ")", ":", "\n", "        ", "\"\"\"Warp frames to 84x84 as done in the Nature paper and later work.\"\"\"", "\n", "gym", ".", "ObservationWrapper", ".", "__init__", "(", "self", ",", "env", ")", "\n", "self", ".", "width", "=", "res", "\n", "self", ".", "height", "=", "res", "\n", "self", ".", "observation_space", "=", "spaces", ".", "Box", "(", "low", "=", "0", ",", "high", "=", "255", ",", "\n", "shape", "=", "(", "self", ".", "height", ",", "self", ".", "width", ",", "1", ")", ",", "dtype", "=", "np", ".", "uint8", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.OpenAI_AtariWrapper.WarpFrame2.observation": [[646, 650], ["cv2.cvtColor", "cv2.resize"], "methods", ["None"], ["", "def", "observation", "(", "self", ",", "frame", ")", ":", "\n", "        ", "frame", "=", "cv2", ".", "cvtColor", "(", "frame", ",", "cv2", ".", "COLOR_RGB2GRAY", ")", "\n", "frame", "=", "cv2", ".", "resize", "(", "frame", ",", "(", "self", ".", "width", ",", "self", ".", "height", ")", ",", "interpolation", "=", "cv2", ".", "INTER_AREA", ")", "\n", "return", "frame", "[", ":", ",", ":", ",", "None", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.OpenAI_AtariWrapper.wrap_maze": [[224, 231], ["OpenAI_AtariWrapper.ResizeFrame", "OpenAI_AtariWrapper.FrameStack", "OpenAI_AtariWrapper.GreyscaleRender", "print"], "function", ["None"], ["", "", "", "def", "wrap_maze", "(", "env", ")", ":", "\n", "# Change the size of the maze to be (40, 40)", "\n", "    ", "env", "=", "ResizeFrame", "(", "env", ",", "res", "=", "40", ")", "\n", "env", "=", "FrameStack", "(", "env", ",", "1", ")", "\n", "env", "=", "GreyscaleRender", "(", "env", ")", "\n", "print", "(", "\"Wrapping maze to be (40, 40)\"", ")", "\n", "return", "env", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.OpenAI_AtariWrapper.wrap_deepmind": [[233, 252], ["OpenAI_AtariWrapper.MaxAndSkipEnv", "OpenAI_AtariWrapper.WarpFrame", "print", "OpenAI_AtariWrapper.GreyscaleRender", "OpenAI_AtariWrapper.EpisodicLifeEnv", "FrameStack.unwrapped.get_action_meanings", "OpenAI_AtariWrapper.FireResetEnv", "OpenAI_AtariWrapper.ClipRewardEnv", "OpenAI_AtariWrapper.FrameStack"], "function", ["None"], ["", "def", "wrap_deepmind", "(", "env", ",", "episode_life", "=", "True", ",", "clip_rewards", "=", "True", ",", "stack", "=", "4", ")", ":", "\n", "    ", "\"\"\"Configure environment for DeepMind-style Atari.\n\n    Note: this does not include frame stacking!\"\"\"", "\n", "assert", "'NoFrameskip'", "in", "env", ".", "spec", ".", "id", "# required for DeepMind-style skip", "\n", "if", "episode_life", ":", "\n", "        ", "env", "=", "EpisodicLifeEnv", "(", "env", ")", "\n", "# env = NoopResetEnv(env, noop_max=30)", "\n", "", "env", "=", "MaxAndSkipEnv", "(", "env", ",", "skip", "=", "4", ")", "\n", "if", "'FIRE'", "in", "env", ".", "unwrapped", ".", "get_action_meanings", "(", ")", ":", "\n", "        ", "env", "=", "FireResetEnv", "(", "env", ")", "\n", "", "env", "=", "WarpFrame", "(", "env", ",", "res", "=", "42", ")", "\n", "if", "clip_rewards", ":", "\n", "        ", "env", "=", "ClipRewardEnv", "(", "env", ")", "\n", "", "if", "stack", ">", "1", ":", "\n", "        ", "env", "=", "FrameStack", "(", "env", ",", "4", ")", "\n", "", "print", "(", "\"Wrapping environment with Deepmind-style setttings but 42 x 42.\"", ")", "\n", "env", "=", "GreyscaleRender", "(", "env", ")", "\n", "return", "env", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.OpenAI_AtariWrapper.SetResolution": [[263, 284], ["super().__init__", "target_resolution.lower().split", "int", "int", "gym.spaces.Box", "gym.error.Error", "target_resolution.lower"], "function", ["home.repos.pwc.inspect_result.oxwhirl_opiq.action.count_action.CountBonus.__init__"], ["def", "SetResolution", "(", "target_resolution", ")", ":", "\n", "    ", "class", "SetResolutionWrapper", "(", "gym", ".", "Wrapper", ")", ":", "\n", "        ", "\"\"\"\n            Doom wrapper to change screen resolution\n        \"\"\"", "\n", "\n", "def", "__init__", "(", "self", ",", "env", ")", ":", "\n", "            ", "super", "(", "SetResolutionWrapper", ",", "self", ")", ".", "__init__", "(", "env", ")", "\n", "if", "target_resolution", "not", "in", "resolutions", ":", "\n", "                ", "raise", "gym", ".", "error", ".", "Error", "(", "\n", "'Error - The specified resolution \"{}\" is not supported by Vizdoom.'", ".", "format", "(", "target_resolution", ")", ")", "\n", "", "parts", "=", "target_resolution", ".", "lower", "(", ")", ".", "split", "(", "'x'", ")", "\n", "width", "=", "int", "(", "parts", "[", "0", "]", ")", "\n", "height", "=", "int", "(", "parts", "[", "1", "]", ")", "\n", "screen_res", "=", "target_resolution", "\n", "self", ".", "screen_width", ",", "self", ".", "screen_height", ",", "self", ".", "unwrapped", ".", "screen_resolution", "=", "width", ",", "height", ",", "screen_res", "\n", "self", ".", "unwrapped", ".", "observation_space", "=", "gym", ".", "spaces", ".", "Box", "(", "low", "=", "0", ",", "high", "=", "255", ",", "\n", "shape", "=", "(", "self", ".", "screen_height", ",", "self", ".", "screen_width", ",", "3", ")", ")", "\n", "self", ".", "observation_space", "=", "self", ".", "unwrapped", ".", "observation_space", "\n", "\n", "", "", "return", "SetResolutionWrapper", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.OpenAI_AtariWrapper.ToDiscrete": [[401, 447], ["super().__init__", "OpenAI_AtariWrapper.DiscreteToMultiDiscrete", "OpenAI_AtariWrapper..env._step", "OpenAI_AtariWrapper..action_space", "gym.error.Error"], "function", ["home.repos.pwc.inspect_result.oxwhirl_opiq.action.count_action.CountBonus.__init__", "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.OpenAI_AtariWrapper.MarioEnv._step"], ["def", "ToDiscrete", "(", "config", ")", ":", "\n", "# Config can be 'minimal', 'constant-7', 'constant-17', 'full'", "\n", "\n", "    ", "class", "ToDiscreteWrapper", "(", "gym", ".", "Wrapper", ")", ":", "\n", "        ", "\"\"\"\n            Doom wrapper to convert MultiDiscrete action space to Discrete\n            config:\n                - minimal - Will only use the levels' allowed actions (+ NOOP)\n                - constant-7 - Will use the 7 minimum actions (+NOOP) to complete all levels\n                - constant-17 - Will use the 17 most common actions (+NOOP) to complete all levels\n                - full - Will use all available actions (+ NOOP)\n            list of commands:\n                - minimal:\n                    Basic:              NOOP, ATTACK, MOVE_RIGHT, MOVE_LEFT\n                    Corridor:           NOOP, ATTACK, MOVE_RIGHT, MOVE_LEFT, MOVE_FORWARD, TURN_RIGHT, TURN_LEFT\n                    DefendCenter        NOOP, ATTACK, TURN_RIGHT, TURN_LEFT\n                    DefendLine:         NOOP, ATTACK, TURN_RIGHT, TURN_LEFT\n                    HealthGathering:    NOOP, MOVE_FORWARD, TURN_RIGHT, TURN_LEFT\n                    MyWayHome:          NOOP, MOVE_FORWARD, TURN_RIGHT, TURN_LEFT\n                    PredictPosition:    NOOP, ATTACK, TURN_RIGHT, TURN_LEFT\n                    TakeCover:          NOOP, MOVE_RIGHT, MOVE_LEFT\n                    Deathmatch:         NOOP, ALL COMMANDS (Deltas are limited to [0,1] range and will not work properly)\n                - constant-7: NOOP, ATTACK, MOVE_RIGHT, MOVE_LEFT, MOVE_FORWARD, TURN_RIGHT, TURN_LEFT, SELECT_NEXT_WEAPON\n                - constant-17: NOOP, ATTACK, JUMP, CROUCH, TURN180, RELOAD, SPEED, STRAFE, MOVE_RIGHT, MOVE_LEFT, MOVE_BACKWARD\n                                MOVE_FORWARD, TURN_RIGHT, TURN_LEFT, LOOK_UP, LOOK_DOWN, SELECT_NEXT_WEAPON, SELECT_PREV_WEAPON\n        \"\"\"", "\n", "\n", "def", "__init__", "(", "self", ",", "env", ")", ":", "\n", "            ", "super", "(", "ToDiscreteWrapper", ",", "self", ")", ".", "__init__", "(", "env", ")", "\n", "if", "config", "==", "'minimal'", ":", "\n", "                ", "allowed_actions", "=", "ALLOWED_ACTIONS", "[", "self", ".", "unwrapped", ".", "level", "]", "\n", "", "elif", "config", "==", "'constant-7'", ":", "\n", "                ", "allowed_actions", "=", "[", "0", ",", "10", ",", "11", ",", "13", ",", "14", ",", "15", ",", "31", "]", "\n", "", "elif", "config", "==", "'constant-17'", ":", "\n", "                ", "allowed_actions", "=", "[", "0", ",", "2", ",", "3", ",", "4", ",", "6", ",", "8", ",", "9", ",", "10", ",", "11", ",", "12", ",", "13", ",", "14", ",", "15", ",", "16", ",", "17", ",", "31", ",", "32", "]", "\n", "", "elif", "config", "==", "'full'", ":", "\n", "                ", "allowed_actions", "=", "None", "\n", "", "else", ":", "\n", "                ", "raise", "gym", ".", "error", ".", "Error", "(", "\n", "'Invalid configuration. Valid options are \"minimal\", \"constant-7\", \"constant-17\", \"full\"'", ")", "\n", "", "self", ".", "action_space", "=", "DiscreteToMultiDiscrete", "(", "self", ".", "action_space", ",", "allowed_actions", ")", "\n", "\n", "", "def", "_step", "(", "self", ",", "action", ")", ":", "\n", "            ", "return", "self", ".", "env", ".", "_step", "(", "self", ".", "action_space", "(", "action", ")", ")", "\n", "\n", "", "", "return", "ToDiscreteWrapper", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.OpenAI_AtariWrapper.wrap_vizdoom": [[449, 459], ["OpenAI_AtariWrapper.WarpFrame2", "OpenAI_AtariWrapper.ClipNegativeRewardEnv", "OpenAI_AtariWrapper.MaxAndSkipEnv"], "function", ["None"], ["", "def", "wrap_vizdoom", "(", "env", ",", "stack", "=", "4", ",", "action_repeat", "=", "4", ")", ":", "\n", "# resolution_wrapper = SetResolution(\"160x120\")", "\n", "# env = resolution_wrapper(env)", "\n", "    ", "if", "action_repeat", ">", "1", ":", "\n", "        ", "env", "=", "MaxAndSkipEnv", "(", "env", ",", "skip", "=", "action_repeat", ",", "max_over", "=", "1", ")", "\n", "", "env", "=", "WarpFrame2", "(", "env", ",", "res", "=", "42", ")", "\n", "env", "=", "ClipNegativeRewardEnv", "(", "env", ")", "\n", "# env = FrameStack(env, 4)", "\n", "# env = GreyscaleRender(env)", "\n", "return", "env", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.OpenAI_AtariWrapper.ToDiscreteMario": [[461, 492], ["super().__init__", "OpenAI_AtariWrapper.DiscreteToMultiDiscrete", "OpenAI_AtariWrapper..env.step", "OpenAI_AtariWrapper..action_space"], "function", ["home.repos.pwc.inspect_result.oxwhirl_opiq.action.count_action.CountBonus.__init__", "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.gridworld.GridWorld.step"], ["", "def", "ToDiscreteMario", "(", ")", ":", "\n", "    ", "class", "ToDiscreteWrapper", "(", "gym", ".", "Wrapper", ")", ":", "\n", "        ", "\"\"\"\n            Wrapper to convert MultiDiscrete action space to Discrete\n            Only supports one config, which maps to the most logical discrete space possible\n        \"\"\"", "\n", "\n", "def", "__init__", "(", "self", ",", "env", ")", ":", "\n", "            ", "super", "(", "ToDiscreteWrapper", ",", "self", ")", ".", "__init__", "(", "env", ")", "\n", "mapping", "=", "{", "\n", "0", ":", "[", "0", ",", "0", ",", "0", ",", "0", ",", "0", ",", "0", "]", ",", "# NOOP", "\n", "1", ":", "[", "1", ",", "0", ",", "0", ",", "0", ",", "0", ",", "0", "]", ",", "# Up", "\n", "2", ":", "[", "0", ",", "0", ",", "1", ",", "0", ",", "0", ",", "0", "]", ",", "# Down", "\n", "3", ":", "[", "0", ",", "1", ",", "0", ",", "0", ",", "0", ",", "0", "]", ",", "# Left", "\n", "4", ":", "[", "0", ",", "1", ",", "0", ",", "0", ",", "1", ",", "0", "]", ",", "# Left + A", "\n", "5", ":", "[", "0", ",", "1", ",", "0", ",", "0", ",", "0", ",", "1", "]", ",", "# Left + B", "\n", "6", ":", "[", "0", ",", "1", ",", "0", ",", "0", ",", "1", ",", "1", "]", ",", "# Left + A + B", "\n", "7", ":", "[", "0", ",", "0", ",", "0", ",", "1", ",", "0", ",", "0", "]", ",", "# Right", "\n", "8", ":", "[", "0", ",", "0", ",", "0", ",", "1", ",", "1", ",", "0", "]", ",", "# Right + A", "\n", "9", ":", "[", "0", ",", "0", ",", "0", ",", "1", ",", "0", ",", "1", "]", ",", "# Right + B", "\n", "10", ":", "[", "0", ",", "0", ",", "0", ",", "1", ",", "1", ",", "1", "]", ",", "# Right + A + B", "\n", "11", ":", "[", "0", ",", "0", ",", "0", ",", "0", ",", "1", ",", "0", "]", ",", "# A", "\n", "12", ":", "[", "0", ",", "0", ",", "0", ",", "0", ",", "0", ",", "1", "]", ",", "# B", "\n", "13", ":", "[", "0", ",", "0", ",", "0", ",", "0", ",", "1", ",", "1", "]", ",", "# A + B", "\n", "}", "\n", "self", ".", "action_space", "=", "DiscreteToMultiDiscrete", "(", "self", ".", "action_space", ",", "mapping", ")", "\n", "\n", "", "def", "_step", "(", "self", ",", "action", ")", ":", "\n", "            ", "return", "self", ".", "env", ".", "step", "(", "self", ".", "action_space", "(", "action", ")", ")", "\n", "\n", "", "", "return", "ToDiscreteWrapper", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.OpenAI_AtariWrapper.wrap_mario": [[572, 586], ["OpenAI_AtariWrapper.MaxAndSkipEnv", "OpenAI_AtariWrapper.WarpFrame", "OpenAI_AtariWrapper.GreyscaleRender", "OpenAI_AtariWrapper.LimitedDiscreteActions", "OpenAI_AtariWrapper.NoRewardEnv", "print"], "function", ["None"], ["", "", "def", "wrap_mario", "(", "env", ",", "stack", "=", "4", ",", "buttons", "=", "None", ")", ":", "\n", "# env = MarioEnv(env)", "\n", "# buttons = env.BUTTONS", "\n", "    ", "env", "=", "MaxAndSkipEnv", "(", "env", ",", "skip", "=", "4", ",", "max_over", "=", "1", ")", "\n", "env", "=", "WarpFrame", "(", "env", ",", "res", "=", "42", ")", "\n", "# env = FrameStack(env, 4)", "\n", "env", "=", "GreyscaleRender", "(", "env", ")", "\n", "# discrete_action_wrapper = ToDiscreteMario()", "\n", "env", "=", "LimitedDiscreteActions", "(", "env", ",", "buttons", ")", "\n", "# env = discrete_action_wrapper(env)", "\n", "# No reward", "\n", "env", "=", "NoRewardEnv", "(", "env", ")", "\n", "print", "(", "\"Wrapping mario env\"", ")", "\n", "return", "env", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.OpenAI_AtariWrapper.unwrap": [[627, 636], ["hasattr", "hasattr", "OpenAI_AtariWrapper.unwrap", "hasattr", "OpenAI_AtariWrapper.unwrap"], "function", ["home.repos.pwc.inspect_result.oxwhirl_opiq.envs.OpenAI_AtariWrapper.unwrap", "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.OpenAI_AtariWrapper.unwrap"], ["", "", "def", "unwrap", "(", "env", ")", ":", "\n", "    ", "if", "hasattr", "(", "env", ",", "\"unwrapped\"", ")", ":", "\n", "        ", "return", "env", ".", "unwrapped", "\n", "", "elif", "hasattr", "(", "env", ",", "\"env\"", ")", ":", "\n", "        ", "return", "unwrap", "(", "env", ".", "env", ")", "\n", "", "elif", "hasattr", "(", "env", ",", "\"leg_env\"", ")", ":", "\n", "        ", "return", "unwrap", "(", "env", ".", "leg_env", ")", "\n", "", "else", ":", "\n", "        ", "return", "env", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.OpenAI_AtariWrapper.make_montezuma": [[651, 660], ["OpenAI_AtariWrapper.StickyActionEnv", "OpenAI_AtariWrapper.MaxAndSkipEnv", "OpenAI_AtariWrapper.MontezumaInfoWrapper", "OpenAI_AtariWrapper.WarpFrame2"], "function", ["None"], ["", "", "def", "make_montezuma", "(", "env", ",", "max_episode_steps", "=", "4500", ")", ":", "\n", "# env = gym.make(env_id)", "\n", "        ", "env", ".", "_max_episode_steps", "=", "max_episode_steps", "*", "4", "\n", "assert", "'NoFrameskip'", "in", "env", ".", "spec", ".", "id", "\n", "env", "=", "StickyActionEnv", "(", "env", ")", "\n", "env", "=", "MaxAndSkipEnv", "(", "env", ",", "skip", "=", "4", ")", "\n", "env", "=", "MontezumaInfoWrapper", "(", "env", ",", "room_address", "=", "3", ")", "\n", "env", "=", "WarpFrame2", "(", "env", ")", "\n", "return", "env", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.OpenAI_AtariWrapper.make_montezuma_ram": [[661, 670], ["OpenAI_AtariWrapper.StickyActionEnv", "OpenAI_AtariWrapper.MaxAndSkipEnv", "OpenAI_AtariWrapper.MontezumaInfoWrapper"], "function", ["None"], ["", "def", "make_montezuma_ram", "(", "env", ",", "max_episode_steps", "=", "4500", ")", ":", "\n", "# env = gym.make(env_id)", "\n", "        ", "env", ".", "_max_episode_steps", "=", "max_episode_steps", "*", "4", "\n", "assert", "'NoFrameskip'", "in", "env", ".", "spec", ".", "id", "\n", "env", "=", "StickyActionEnv", "(", "env", ")", "\n", "env", "=", "MaxAndSkipEnv", "(", "env", ",", "skip", "=", "4", ")", "\n", "env", "=", "MontezumaInfoWrapper", "(", "env", ",", "room_address", "=", "3", ")", "\n", "# env = WarpFrame2(env)", "\n", "return", "env", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.OpenAI_AtariWrapper.make_atari": [[671, 680], ["OpenAI_AtariWrapper.StickyActionEnv", "OpenAI_AtariWrapper.MaxAndSkipEnv", "OpenAI_AtariWrapper.WarpFrame2"], "function", ["None"], ["", "def", "make_atari", "(", "env", ",", "max_episode_steps", "=", "4500", ")", ":", "\n", "# env = gym.make(env_id)", "\n", "    ", "env", ".", "_max_episode_steps", "=", "max_episode_steps", "*", "4", "\n", "assert", "'NoFrameskip'", "in", "env", ".", "spec", ".", "id", "\n", "env", "=", "StickyActionEnv", "(", "env", ")", "\n", "env", "=", "MaxAndSkipEnv", "(", "env", ",", "skip", "=", "4", ")", "\n", "# env = MontezumaInfoWrapper(env, room_address=3)", "\n", "env", "=", "WarpFrame2", "(", "env", ")", "\n", "return", "env", "\n", "", ""]], "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.montezuma.Montezuma.__init__": [[12, 28], ["gym.make", "OpenAI_AtariWrapper.make_montezuma", "utils.logging.get_stats"], "methods", ["home.repos.pwc.inspect_result.oxwhirl_opiq.envs.OpenAI_AtariWrapper.make_montezuma", "home.repos.pwc.inspect_result.oxwhirl_opiq.utils.logging.get_stats"], ["def", "__init__", "(", "self", ")", ":", "\n", "        ", "self", ".", "mont_env", "=", "gym", ".", "make", "(", "\"MontezumaRevengeNoFrameskip-v4\"", ")", "\n", "# self.mario_env = modewrapper(self.mario_env)", "\n", "self", ".", "max_timesteps", "=", "4500", "\n", "self", ".", "mont_env", "=", "make_montezuma", "(", "self", ".", "mont_env", ",", "max_episode_steps", "=", "self", ".", "max_timesteps", ")", "\n", "\n", "self", ".", "steps", "=", "0", "\n", "\n", "self", ".", "_seed", "=", "56", "\n", "self", ".", "observation_space", "=", "self", ".", "mont_env", ".", "observation_space", "\n", "self", ".", "action_space", "=", "self", ".", "mont_env", ".", "action_space", "\n", "\n", "self", ".", "stats", "=", "get_stats", "(", ")", "\n", "\n", "self", ".", "obs_dtype", "=", "np", ".", "uint8", "\n", "self", ".", "obs_scaling", "=", "1", "/", "255.0", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.montezuma.Montezuma.step": [[29, 44], ["montezuma.Montezuma.mont_env.step", "montezuma.Montezuma.mont_env.env.get_current_room"], "methods", ["home.repos.pwc.inspect_result.oxwhirl_opiq.envs.gridworld.GridWorld.step", "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.OpenAI_AtariWrapper.MontezumaInfoWrapper.get_current_room"], ["", "def", "step", "(", "self", ",", "a", ")", ":", "\n", "        ", "self", ".", "steps", "+=", "1", "\n", "s", ",", "r", ",", "finished", ",", "info", "=", "self", ".", "mont_env", ".", "step", "(", "a", ")", "\n", "if", "self", ".", "steps", ">=", "self", ".", "max_timesteps", "and", "not", "finished", ":", "\n", "            ", "finished", "=", "True", "\n", "info", "[", "\"Steps_Termination\"", "]", "=", "True", "\n", "", "self", ".", "current_room", "=", "self", ".", "mont_env", ".", "env", ".", "get_current_room", "(", ")", "\n", "# if \"episode\" in info and \"visited_rooms\" in info[\"episode\"]:", "\n", "#     visited_rooms = info[\"episode\"][\"visited_rooms\"]", "\n", "#     for room in range(1,25):", "\n", "#         self.stats.update_stats(\"Visited_Room_{}\".format(room), room in visited_rooms)", "\n", "if", "\"episode\"", "in", "info", ":", "\n", "# We are already logging the useful info from it", "\n", "            ", "del", "info", "[", "\"episode\"", "]", "\n", "", "return", "s", ",", "r", ",", "finished", ",", "info", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.montezuma.Montezuma.reset": [[45, 48], ["montezuma.Montezuma.mont_env.reset"], "methods", ["home.repos.pwc.inspect_result.oxwhirl_opiq.envs.gridworld.GridWorld.reset"], ["", "def", "reset", "(", "self", ")", ":", "\n", "        ", "self", ".", "steps", "=", "0", "\n", "return", "self", ".", "mont_env", ".", "reset", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.montezuma.Montezuma.render": [[49, 51], ["montezuma.Montezuma.mont_env.render"], "methods", ["home.repos.pwc.inspect_result.oxwhirl_opiq.envs.gridworld.GridWorld.render"], ["", "def", "render", "(", "self", ",", "mode", "=", "\"rgb_array\"", ",", "close", "=", "False", ")", ":", "\n", "        ", "return", "self", ".", "mont_env", ".", "render", "(", "mode", "=", "mode", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.montezuma.Montezuma.close": [[52, 54], ["montezuma.Montezuma.mont_env.close"], "methods", ["home.repos.pwc.inspect_result.oxwhirl_opiq.envs.montezuma.Montezuma.close"], ["", "def", "close", "(", "self", ")", ":", "\n", "        ", "self", ".", "mont_env", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.montezuma.Montezuma.log_player_pos": [[55, 57], ["None"], "methods", ["None"], ["", "def", "log_player_pos", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "current_room", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.montezuma.Montezuma.trained_on_states": [[58, 60], ["None"], "methods", ["None"], ["", "def", "trained_on_states", "(", "self", ",", "player_visits", ",", "args", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.montezuma.Montezuma.xp_and_frontier_states": [[61, 63], ["None"], "methods", ["None"], ["", "def", "xp_and_frontier_states", "(", "self", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.montezuma.Montezuma.bonus_xp_and_frontier_states": [[64, 66], ["None"], "methods", ["None"], ["", "def", "bonus_xp_and_frontier_states", "(", "self", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.montezuma.Montezuma.visits_and_frontier_states": [[67, 69], ["None"], "methods", ["None"], ["", "def", "visits_and_frontier_states", "(", "self", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.montezuma.Montezuma.xp_replay_states": [[70, 72], ["None"], "methods", ["None"], ["", "def", "xp_replay_states", "(", "self", ",", "player_visits", ",", "args", ",", "bonus_replay", "=", "False", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.montezuma.Montezuma.player_visits": [[73, 75], ["None"], "methods", ["None"], ["", "def", "player_visits", "(", "self", ",", "player_visits", ",", "args", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.montezuma.Montezuma.bonus_landscape": [[76, 78], ["None"], "methods", ["None"], ["", "def", "bonus_landscape", "(", "self", ",", "player_visits", ",", "exploration_bonuses", ",", "max_bonus", ",", "args", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.montezuma.Montezuma.frontier": [[79, 81], ["None"], "methods", ["None"], ["", "def", "frontier", "(", "self", ",", "exp_model", ",", "args", ",", "max_bonus", "=", "None", ")", ":", "\n", "        ", "pass", "", "", "", ""]], "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.gridworld.GridWorld.__init__": [[19, 43], ["gridworld.GridWorld.reset", "print", "gym.spaces.Discrete", "gym.spaces.Box", "numpy.empty_like"], "methods", ["home.repos.pwc.inspect_result.oxwhirl_opiq.envs.gridworld.GridWorld.reset"], ["def", "__init__", "(", "self", ",", "randomise", "=", "False", ",", "num_actions", "=", "4", ",", "danger", "=", "False", ")", ":", "\n", "        ", "self", ".", "reset", "(", ")", "\n", "self", ".", "actions", "=", "[", "(", "0", ",", "-", "1", ")", ",", "(", "1", ",", "0", ")", ",", "(", "0", ",", "1", ")", ",", "(", "-", "1", ",", "0", ")", "]", "\n", "if", "num_actions", ">", "4", ":", "\n", "            ", "self", ".", "actions", "=", "[", "(", "0", ",", "-", "1", ")", ",", "(", "1", ",", "0", ")", ",", "(", "0", ",", "1", ")", ",", "(", "-", "1", ",", "0", ")", ",", "(", "0", ",", "0", ")", "]", "\n", "# Time-limit on the environment, 5 is arbitrary", "\n", "", "self", ".", "limit", "=", "self", ".", "grid", ".", "size", "*", "10", "\n", "print", "(", "\"Limit:\"", ",", "self", ".", "limit", ")", "\n", "self", ".", "positive_reward", "=", "+", "1", "\n", "self", ".", "negative_reward", "=", "-", "0.005", "\n", "self", ".", "danger", "=", "danger", "\n", "\n", "# Gym Stuff", "\n", "self", ".", "action_space", "=", "spaces", ".", "Discrete", "(", "num_actions", ")", "\n", "self", ".", "observation_space", "=", "spaces", ".", "Box", "(", "low", "=", "0", ",", "high", "=", "1", ",", "shape", "=", "self", ".", "grid", ".", "shape", ")", "\n", "self", ".", "reward_range", "=", "(", "self", ".", "negative_reward", ",", "self", ".", "positive_reward", ")", "\n", "\n", "# Counting stuff", "\n", "self", ".", "counts", "=", "np", ".", "empty_like", "(", "self", ".", "grid", ")", "\n", "self", ".", "found_goal", "=", "False", "\n", "\n", "# Randomise actions", "\n", "self", ".", "total_actions", "=", "num_actions", "\n", "self", ".", "randomise", "=", "randomise", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.gridworld.GridWorld.update_limit": [[44, 47], ["print"], "methods", ["None"], ["", "def", "update_limit", "(", "self", ",", "new_limit", ")", ":", "\n", "        ", "self", ".", "limit", "=", "new_limit", "\n", "print", "(", "\"New Limit:\"", ",", "self", ".", "limit", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.gridworld.GridWorld.get_randomised_action": [[48, 63], ["list", "range", "list.index", "min", "range"], "methods", ["None"], ["", "def", "get_randomised_action", "(", "self", ",", "a", ")", ":", "\n", "        ", "state_seed", "=", "(", "937", "*", "self", ".", "player_pos", "[", "0", "]", "+", "79", "*", "self", ".", "player_pos", "[", "1", "]", ")", "\n", "all_actions", "=", "list", "(", "range", "(", "self", ".", "total_actions", ")", ")", "\n", "for", "i", "in", "range", "(", "self", ".", "total_actions", "-", "1", ")", ":", "\n", "            ", "state_seed", "=", "(", "state_seed", "*", "29", ")", "%", "137", "\n", "to_swap", "=", "state_seed", "%", "(", "self", ".", "total_actions", "-", "i", ")", "\n", "to_swap", "+=", "i", "\n", "temp", "=", "all_actions", "[", "i", "]", "\n", "all_actions", "[", "i", "]", "=", "all_actions", "[", "to_swap", "]", "\n", "all_actions", "[", "to_swap", "]", "=", "temp", "\n", "\n", "", "action_to_take", "=", "all_actions", ".", "index", "(", "a", ")", "\n", "action_to_take", "=", "min", "(", "action_to_take", ",", "4", "if", "self", ".", "total_actions", ">", "4", "else", "3", ")", "\n", "\n", "return", "action_to_take", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.gridworld.GridWorld.step": [[64, 143], ["numpy.array", "gridworld.GridWorld.get_randomised_action", "action_counts.append"], "methods", ["home.repos.pwc.inspect_result.oxwhirl_opiq.envs.gridworld.GridWorld.get_randomised_action"], ["", "def", "step", "(", "self", ",", "a", ")", ":", "\n", "\n", "        ", "if", "self", ".", "randomise", ":", "\n", "            ", "a", "=", "self", ".", "get_randomised_action", "(", "a", ")", "\n", "\n", "", "info_dict", "=", "{", "}", "\n", "\n", "# Update counts", "\n", "self", ".", "counts", "[", "self", ".", "player_pos", "]", "+=", "1", "\n", "current_count", "=", "self", ".", "counts", "[", "self", ".", "player_pos", "]", "\n", "action_counts", "=", "[", "]", "\n", "for", "aa", "in", "self", ".", "actions", ":", "\n", "            ", "new_player_pos", "=", "(", "self", ".", "player_pos", "[", "0", "]", "+", "aa", "[", "0", "]", ",", "self", ".", "player_pos", "[", "1", "]", "+", "aa", "[", "1", "]", ")", "\n", "# Clip", "\n", "if", "(", "\n", "new_player_pos", "[", "0", "]", "<", "0", "\n", "or", "new_player_pos", "[", "0", "]", ">=", "self", ".", "grid", ".", "shape", "[", "0", "]", "\n", "or", "new_player_pos", "[", "1", "]", "<", "0", "\n", "or", "new_player_pos", "[", "1", "]", ">=", "self", ".", "grid", ".", "shape", "[", "1", "]", "\n", ")", ":", "\n", "                ", "new_player_pos", "=", "self", ".", "player_pos", "\n", "\n", "# Into a wall", "\n", "", "if", "self", ".", "grid", "[", "new_player_pos", "]", "==", "1", ":", "\n", "                ", "new_player_pos", "=", "self", ".", "player_pos", "\n", "\n", "", "action_counts", ".", "append", "(", "self", ".", "counts", "[", "new_player_pos", "]", ")", "\n", "\n", "", "self", ".", "steps", "+=", "1", "\n", "new_player_pos", "=", "(", "\n", "self", ".", "player_pos", "[", "0", "]", "+", "self", ".", "actions", "[", "a", "]", "[", "0", "]", ",", "\n", "self", ".", "player_pos", "[", "1", "]", "+", "self", ".", "actions", "[", "a", "]", "[", "1", "]", ",", "\n", ")", "\n", "# Clip", "\n", "if", "(", "\n", "new_player_pos", "[", "0", "]", "<", "0", "\n", "or", "new_player_pos", "[", "0", "]", ">=", "self", ".", "grid", ".", "shape", "[", "0", "]", "\n", "or", "new_player_pos", "[", "1", "]", "<", "0", "\n", "or", "new_player_pos", "[", "1", "]", ">=", "self", ".", "grid", ".", "shape", "[", "1", "]", "\n", ")", ":", "\n", "            ", "new_player_pos", "=", "self", ".", "player_pos", "\n", "\n", "", "r", "=", "self", ".", "negative_reward", "\n", "\n", "finished", "=", "False", "\n", "\n", "# Into a wall", "\n", "if", "self", ".", "grid", "[", "new_player_pos", "]", "==", "1", ":", "\n", "            ", "new_player_pos", "=", "self", ".", "player_pos", "\n", "if", "self", ".", "danger", ":", "\n", "                ", "finished", "=", "True", "\n", "# Into a goal", "\n", "", "", "elif", "self", ".", "grid", "[", "new_player_pos", "]", "==", "2", ":", "\n", "            ", "r", "+=", "self", ".", "positive_reward", "\n", "self", ".", "found_goal", "=", "True", "\n", "self", ".", "goals", "-=", "1", "\n", "if", "self", ".", "goals", "==", "0", ":", "\n", "                ", "finished", "=", "True", "\n", "\n", "", "", "self", ".", "grid", "[", "self", ".", "player_pos", "]", "=", "0", "\n", "self", ".", "grid", "[", "new_player_pos", "]", "=", "3", "\n", "self", ".", "player_pos", "=", "new_player_pos", "\n", "\n", "if", "self", ".", "danger", "and", "finished", ":", "\n", "# Gone into wall", "\n", "            ", "r", "=", "-", "1", "\n", "\n", "", "if", "self", ".", "steps", ">=", "self", ".", "limit", "and", "not", "finished", ":", "\n", "            ", "finished", "=", "True", "\n", "info_dict", "[", "\"Steps_Termination\"", "]", "=", "True", "\n", "\n", "# Fill in info dict with the action selection statistics", "\n", "", "new_state_count", "=", "self", ".", "counts", "[", "new_player_pos", "]", "\n", "count_list", "=", "[", "current_count", "]", "+", "action_counts", "+", "[", "new_state_count", "]", "\n", "info_dict", "[", "\"Action_Counts\"", "]", "=", "np", ".", "array", "(", "count_list", ")", "\n", "\n", "info_dict", "[", "\"Found Goal\"", "]", "=", "self", ".", "found_goal", "\n", "\n", "return", "self", ".", "grid", "[", ":", ",", ":", ",", "np", ".", "newaxis", "]", "/", "3", ",", "r", ",", "finished", ",", "info_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.gridworld.GridWorld.reset": [[144, 154], ["gridworld.GridWorld.create_grid", "numpy.argwhere", "numpy.argwhere"], "methods", ["home.repos.pwc.inspect_result.oxwhirl_opiq.envs.gridworld.GridWorld.create_grid"], ["", "def", "reset", "(", "self", ")", ":", "\n", "        ", "self", ".", "steps", "=", "0", "\n", "self", ".", "create_grid", "(", ")", "\n", "player_pos_np", "=", "np", ".", "argwhere", "(", "self", ".", "grid", "==", "3", ")", "[", "0", "]", "\n", "self", ".", "player_pos", "=", "(", "player_pos_np", "[", "0", "]", ",", "player_pos_np", "[", "1", "]", ")", "\n", "self", ".", "goals", "=", "(", "self", ".", "grid", "==", "2", ")", ".", "sum", "(", ")", "\n", "self", ".", "num_goals", "=", "self", ".", "goals", "\n", "self", ".", "goals_order", "=", "np", ".", "argwhere", "(", "self", ".", "grid", "==", "2", ")", "\n", "# print(self.goals_order)", "\n", "return", "self", ".", "grid", "[", ":", ",", ":", ",", "np", ".", "newaxis", "]", "/", "3", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.gridworld.GridWorld.render": [[155, 170], ["numpy.zeros", "range", "range"], "methods", ["None"], ["", "def", "render", "(", "self", ",", "mode", "=", "\"rgb_array\"", ",", "close", "=", "False", ")", ":", "\n", "        ", "if", "mode", "==", "\"rgb_array\"", ":", "\n", "            ", "grid", "=", "self", ".", "grid", "\n", "image", "=", "np", ".", "zeros", "(", "shape", "=", "(", "grid", ".", "shape", "[", "0", "]", ",", "grid", ".", "shape", "[", "1", "]", ",", "3", ")", ")", "\n", "for", "x", "in", "range", "(", "grid", ".", "shape", "[", "0", "]", ")", ":", "\n", "                ", "for", "y", "in", "range", "(", "grid", ".", "shape", "[", "1", "]", ")", ":", "\n", "                    ", "if", "grid", "[", "x", ",", "y", "]", "!=", "0", ":", "\n", "                        ", "image", "[", "x", ",", "y", "]", "=", "(", "\n", "255", "*", "grid", "[", "x", ",", "y", "]", "/", "3", ",", "\n", "255", "*", "grid", "[", "x", ",", "y", "]", "/", "3", ",", "\n", "255", "*", "grid", "[", "x", ",", "y", "]", "/", "3", ",", "\n", ")", "\n", "", "", "", "return", "image", "\n", "", "else", ":", "\n", "            ", "pass", "\n", "# raise Exception(\"Cannot do human rendering\")", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.gridworld.GridWorld.state_to_image": [[172, 184], ["numpy.zeros", "range", "range"], "methods", ["None"], ["", "", "def", "state_to_image", "(", "self", ",", "state", ")", ":", "\n", "        ", "grid", "=", "state", "\n", "image", "=", "np", ".", "zeros", "(", "shape", "=", "(", "grid", ".", "shape", "[", "0", "]", ",", "grid", ".", "shape", "[", "1", "]", ",", "3", ")", ")", "\n", "for", "x", "in", "range", "(", "grid", ".", "shape", "[", "0", "]", ")", ":", "\n", "            ", "for", "y", "in", "range", "(", "grid", ".", "shape", "[", "1", "]", ")", ":", "\n", "                ", "if", "grid", "[", "x", ",", "y", "]", "!=", "0", ":", "\n", "                    ", "image", "[", "x", ",", "y", "]", "=", "(", "\n", "255", "*", "grid", "[", "x", ",", "y", "]", "/", "3", ",", "\n", "255", "*", "grid", "[", "x", ",", "y", "]", "/", "3", ",", "\n", "255", "*", "grid", "[", "x", ",", "y", "]", "/", "3", ",", "\n", ")", "\n", "", "", "", "return", "image", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.gridworld.GridWorld.create_grid": [[185, 187], ["numpy.array"], "methods", ["None"], ["", "def", "create_grid", "(", "self", ")", ":", "\n", "        ", "self", ".", "grid", "=", "np", ".", "array", "(", "[", "[", "3", ",", "0", "]", ",", "[", "1", ",", "2", "]", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.gridworld.GridWorld.log_player_pos": [[188, 195], ["list", "tuple"], "methods", ["None"], ["", "def", "log_player_pos", "(", "self", ")", ":", "\n", "        ", "goals_list", "=", "[", "self", ".", "grid", "[", "g", "[", "0", "]", ",", "g", "[", "1", "]", "]", "==", "2", "for", "g", "in", "self", ".", "goals_order", "]", "\n", "# print(goals_list)", "\n", "player_pos", "=", "list", "(", "self", ".", "player_pos", ")", "\n", "joint", "=", "player_pos", "+", "goals_list", "\n", "# print(tuple(joint))", "\n", "return", "tuple", "(", "joint", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.gridworld.GridWorld.state_to_player_pos": [[196, 207], ["list", "tuple", "numpy.argwhere"], "methods", ["None"], ["", "def", "state_to_player_pos", "(", "self", ",", "state", ")", ":", "\n", "        ", "internal_state", "=", "state", "[", ":", ",", ":", ",", "0", "]", "\n", "goals_list", "=", "[", "\n", "internal_state", "[", "g", "[", "0", "]", ",", "g", "[", "1", "]", "]", ">", "0.6", "and", "internal_state", "[", "g", "[", "0", "]", ",", "g", "[", "1", "]", "]", "<", "0.7", "\n", "for", "g", "in", "self", ".", "goals_order", "\n", "]", "\n", "# print(goals_list)", "\n", "player_pos", "=", "list", "(", "np", ".", "argwhere", "(", "internal_state", ">", "0.9", ")", "[", "0", "]", ")", "\n", "joint", "=", "player_pos", "+", "goals_list", "\n", "# print(tuple(joint))", "\n", "return", "tuple", "(", "joint", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.gridworld.GridWorld.trained_on_states": [[208, 295], ["numpy.zeros", "range", "numpy.insert", "numpy.insert", "colour_images.append", "Exception", "numpy.array", "numpy.max", "numpy.max", "range", "range", "range", "numpy.clip", "colour_maze.astype", "enumerate", "enumerate", "enumerate", "numpy.argwhere", "range", "range", "range", "numpy.argwhere", "range"], "methods", ["None"], ["", "def", "trained_on_states", "(", "self", ",", "player_visits", ",", "args", ")", ":", "\n", "\n", "# interval = args.exp_replay_size", "\n", "\n", "        ", "if", "self", ".", "num_goals", ">", "3", ":", "\n", "            ", "raise", "Exception", "(", "\"Cant do trained on states for >3 goals atm\"", ")", "\n", "\n", "# We want to show visualisations for the agent depending on which goals they've visited as well", "\n", "# Keep it seperate from the other one", "\n", "", "colour_images", "=", "[", "]", "\n", "# Works for num_goals <= 3", "\n", "# print(self.grid.shape)", "\n", "canvas", "=", "np", ".", "zeros", "(", "\n", "(", "\n", "self", ".", "grid", ".", "shape", "[", "0", "]", "*", "self", ".", "num_goals", ",", "\n", "self", ".", "grid", ".", "shape", "[", "1", "]", "*", "self", ".", "num_goals", ",", "\n", "3", ",", "\n", ")", "\n", ")", "\n", "grid_x", "=", "self", ".", "grid", ".", "shape", "[", "0", "]", "\n", "grid_y", "=", "self", ".", "grid", ".", "shape", "[", "1", "]", "\n", "\n", "# end_t = int(args.t_max * i / 100) * args.batch_size", "\n", "# start_t = int(args.t_max * (i - 1) / 100) * args.batch_size", "\n", "# print(\"\\n\\n\\n\\n\",start_t, end_t)", "\n", "\n", "for", "visit", "in", "player_visits", ":", "\n", "# print(visit)", "\n", "            ", "px", "=", "visit", "[", "0", "]", "\n", "py", "=", "visit", "[", "1", "]", "\n", "\n", "np_goals", "=", "np", ".", "array", "(", "visit", "[", "2", ":", "]", ")", "\n", "goal_colours", "=", "[", "ig", "for", "ig", ",", "c", "in", "enumerate", "(", "visit", "[", "2", ":", "]", ")", "if", "c", "==", "True", "]", "\n", "if", "goal_colours", "==", "[", "]", ":", "\n", "                ", "goal_colours", "=", "[", "0", ",", "1", ",", "2", "]", "\n", "", "x_place", "=", "px", "+", "grid_x", "*", "(", "np_goals", "==", "False", ")", ".", "sum", "(", ")", "\n", "yy", "=", "0", "\n", "if", "(", "np_goals", "==", "False", ")", ".", "sum", "(", ")", "==", "1", ":", "\n", "                ", "yy", "=", "np", ".", "argwhere", "(", "np_goals", "==", "False", ")", "[", "0", "]", "\n", "", "elif", "(", "np_goals", "==", "False", ")", ".", "sum", "(", ")", "==", "2", ":", "\n", "                ", "yy", "=", "np", ".", "argwhere", "(", "np_goals", "==", "True", ")", "[", "0", "]", "\n", "", "y_place", "=", "py", "+", "grid_y", "*", "yy", "\n", "\n", "# print(x_place, y_place, goal_colours, canvas.shape)", "\n", "canvas", "[", "x_place", ",", "y_place", ",", "goal_colours", "]", "+=", "1", "\n", "\n", "", "if", "np", ".", "max", "(", "canvas", ")", "==", "0", ":", "\n", "            ", "return", "\n", "", "canvas", "=", "canvas", "/", "np", ".", "max", "(", "canvas", ")", "\n", "\n", "# TODO: Colour the unvisited goals", "\n", "for", "goal", "in", "self", ".", "goals_order", ":", "\n", "            ", "canvas", "[", "goal", "[", "0", "]", ",", "goal", "[", "1", "]", ",", ":", "]", "=", "2", "/", "3", "\n", "", "if", "self", ".", "num_goals", ">=", "2", ":", "\n", "            ", "for", "g", "in", "range", "(", "self", ".", "num_goals", ")", ":", "\n", "                ", "for", "go_i", ",", "goal", "in", "enumerate", "(", "self", ".", "goals_order", ")", ":", "\n", "                    ", "if", "go_i", "!=", "g", ":", "\n", "                        ", "canvas", "[", "goal", "[", "0", "]", "+", "grid_x", ",", "goal", "[", "1", "]", "+", "g", "*", "grid_y", ",", ":", "]", "=", "2", "/", "3", "\n", "", "", "", "", "if", "self", ".", "num_goals", ">=", "3", ":", "\n", "            ", "for", "g", "in", "range", "(", "self", ".", "num_goals", ")", ":", "\n", "                ", "for", "go_i", ",", "goal", "in", "enumerate", "(", "self", ".", "goals_order", ")", ":", "\n", "                    ", "if", "go_i", "==", "g", ":", "\n", "                        ", "canvas", "[", "goal", "[", "0", "]", "+", "2", "*", "grid_x", ",", "goal", "[", "1", "]", "+", "g", "*", "grid_y", ",", ":", "]", "=", "2", "/", "3", "\n", "\n", "# The walls", "\n", "", "", "", "", "for", "x", "in", "range", "(", "grid_x", ")", ":", "\n", "            ", "for", "y", "in", "range", "(", "grid_y", ")", ":", "\n", "                ", "if", "self", ".", "grid", "[", "x", ",", "y", "]", "==", "1", ":", "\n", "                    ", "canvas", "[", "x", ",", "y", ",", ":", "]", "=", "1", "/", "3", "\n", "for", "zx", "in", "range", "(", "1", ",", "self", ".", "num_goals", ")", ":", "\n", "                        ", "for", "zy", "in", "range", "(", "self", ".", "num_goals", ")", ":", "\n", "                            ", "canvas", "[", "zx", "*", "grid_x", "+", "x", ",", "zy", "*", "grid_y", "+", "y", ",", ":", "]", "=", "1", "/", "3", "\n", "\n", "# Seperate the mazes", "\n", "", "", "", "", "", "canvas", "=", "np", ".", "insert", "(", "\n", "canvas", ",", "[", "(", "z", "+", "1", ")", "*", "grid_x", "for", "z", "in", "range", "(", "self", ".", "num_goals", "-", "1", ")", "]", ",", "1", ",", "axis", "=", "0", "\n", ")", "\n", "canvas", "=", "np", ".", "insert", "(", "\n", "canvas", ",", "[", "(", "z", "+", "1", ")", "*", "grid_x", "for", "z", "in", "range", "(", "self", ".", "num_goals", "-", "1", ")", "]", ",", "1", ",", "axis", "=", "1", "\n", ")", "\n", "canvas", "[", "0", ":", "grid_x", ",", "grid_y", "+", "1", ":", ",", ":", "]", "=", "0", "\n", "colour_maze", "=", "canvas", "\n", "\n", "colour_maze", "=", "np", ".", "clip", "(", "colour_maze", ",", "0", ",", "1", ")", "*", "255", "\n", "# colour_maze = np.swapaxes(colour_maze, 0, 1)", "\n", "colour_images", ".", "append", "(", "colour_maze", ".", "astype", "(", "np", ".", "uint8", ")", ")", "\n", "return", "colour_images", "[", "0", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.gridworld.GridWorld.xp_and_frontier_states": [[296, 315], ["numpy.empty_like", "range"], "methods", ["None"], ["", "def", "xp_and_frontier_states", "(", "self", ")", ":", "\n", "# We should have already computed the xp replay and frontier images", "\n", "        ", "xp_replay_image", "=", "self", ".", "xp_replay_image", "\n", "frontier_colours", "=", "self", ".", "frontier_image", "\n", "\n", "if", "frontier_colours", ".", "shape", "[", "1", "]", "!=", "xp_replay_image", ".", "shape", "[", "1", "]", ":", "\n", "            ", "tiled_xp_replay_image", "=", "np", ".", "empty_like", "(", "frontier_colours", ")", "\n", "times_to_tile", "=", "frontier_colours", ".", "shape", "[", "1", "]", "//", "xp_replay_image", ".", "shape", "[", "1", "]", "\n", "for", "i", "in", "range", "(", "times_to_tile", ")", ":", "\n", "                ", "tiled_xp_replay_image", "[", "\n", ":", ",", "\n", "i", "*", "xp_replay_image", ".", "shape", "[", "1", "]", ":", "(", "i", "+", "1", ")", "*", "xp_replay_image", ".", "shape", "[", "1", "]", ",", "\n", ":", ",", "\n", "]", "=", "xp_replay_image", "\n", "", "xp_replay_image", "=", "tiled_xp_replay_image", "\n", "\n", "", "overlayed_image", "=", "xp_replay_image", "+", "frontier_colours", "\n", "\n", "return", "overlayed_image", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.gridworld.GridWorld.bonus_xp_and_frontier_states": [[316, 335], ["numpy.empty_like", "range"], "methods", ["None"], ["", "def", "bonus_xp_and_frontier_states", "(", "self", ")", ":", "\n", "# We should have already computer the xp replay and frontier images", "\n", "        ", "xp_replay_image", "=", "self", ".", "bonus_replay_image", "\n", "frontier_colours", "=", "self", ".", "frontier_image", "\n", "\n", "if", "frontier_colours", ".", "shape", "[", "1", "]", "!=", "xp_replay_image", ".", "shape", "[", "1", "]", ":", "\n", "            ", "tiled_xp_replay_image", "=", "np", ".", "empty_like", "(", "frontier_colours", ")", "\n", "times_to_tile", "=", "frontier_colours", ".", "shape", "[", "1", "]", "//", "xp_replay_image", ".", "shape", "[", "1", "]", "\n", "for", "i", "in", "range", "(", "times_to_tile", ")", ":", "\n", "                ", "tiled_xp_replay_image", "[", "\n", ":", ",", "\n", "i", "*", "xp_replay_image", ".", "shape", "[", "1", "]", ":", "(", "i", "+", "1", ")", "*", "xp_replay_image", ".", "shape", "[", "1", "]", ",", "\n", ":", ",", "\n", "]", "=", "xp_replay_image", "\n", "", "xp_replay_image", "=", "tiled_xp_replay_image", "\n", "\n", "", "overlayed_image", "=", "xp_replay_image", "+", "frontier_colours", "\n", "\n", "return", "overlayed_image", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.gridworld.GridWorld.visits_and_frontier_states": [[336, 352], ["numpy.empty_like", "range"], "methods", ["None"], ["", "def", "visits_and_frontier_states", "(", "self", ")", ":", "\n", "        ", "visits_image", "=", "self", ".", "player_visits_image", "\n", "frontier_colours", "=", "self", ".", "frontier_image", "\n", "\n", "if", "frontier_colours", ".", "shape", "[", "1", "]", "!=", "visits_image", ".", "shape", "[", "1", "]", ":", "\n", "            ", "tiled_xp_replay_image", "=", "np", ".", "empty_like", "(", "frontier_colours", ")", "\n", "times_to_tile", "=", "frontier_colours", ".", "shape", "[", "1", "]", "//", "visits_image", ".", "shape", "[", "1", "]", "\n", "for", "i", "in", "range", "(", "times_to_tile", ")", ":", "\n", "                ", "tiled_xp_replay_image", "[", "\n", ":", ",", "i", "*", "visits_image", ".", "shape", "[", "1", "]", ":", "(", "i", "+", "1", ")", "*", "visits_image", ".", "shape", "[", "1", "]", ",", ":", "\n", "]", "=", "visits_image", "\n", "", "visits_image", "=", "tiled_xp_replay_image", "\n", "\n", "", "overlayed_image", "=", "visits_image", "+", "frontier_colours", "\n", "\n", "return", "overlayed_image", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.gridworld.GridWorld.xp_replay_states": [[353, 461], ["numpy.zeros", "range", "numpy.insert", "numpy.insert", "colour_images.append", "Exception", "numpy.array", "numpy.max", "range", "range", "range", "numpy.copy", "numpy.clip", "colour_maze.astype", "numpy.copy", "numpy.copy", "print", "enumerate", "enumerate", "enumerate", "numpy.argwhere", "range", "range", "range", "numpy.argwhere", "range"], "methods", ["None"], ["", "def", "xp_replay_states", "(", "self", ",", "player_visits", ",", "args", ",", "bonus_replay", "=", "False", ")", ":", "\n", "\n", "# interval = args.exp_replay_size", "\n", "\n", "        ", "if", "self", ".", "num_goals", ">", "1", ":", "\n", "            ", "raise", "Exception", "(", "\"Cant do xp replay states for >1 goals atm\"", ")", "\n", "\n", "# We want to show visualisations for the agent depending on which goals they've visited as well", "\n", "# Keep it seperate from the other one", "\n", "", "colour_images", "=", "[", "]", "\n", "# Works for num_goals <= 3", "\n", "# print(self.grid.shape)", "\n", "canvas", "=", "np", ".", "zeros", "(", "\n", "(", "\n", "self", ".", "grid", ".", "shape", "[", "0", "]", "*", "self", ".", "num_goals", ",", "\n", "self", ".", "grid", ".", "shape", "[", "1", "]", "*", "self", ".", "num_goals", ",", "\n", "3", ",", "\n", ")", "\n", ")", "\n", "grid_x", "=", "self", ".", "grid", ".", "shape", "[", "0", "]", "\n", "grid_y", "=", "self", ".", "grid", ".", "shape", "[", "1", "]", "\n", "\n", "# end_t = int(args.t_max * i / 100)", "\n", "# start_t = max(0, end_t - args.exp_replay_size)", "\n", "\n", "# print(\"\\n\\n\")", "\n", "# print(self.num_goals)", "\n", "# print(self.goals_order)", "\n", "# print(canvas.shape)", "\n", "# print(\"\\n\\n\")", "\n", "\n", "for", "visit", "in", "player_visits", ":", "\n", "# print(visit)", "\n", "            ", "px", "=", "visit", "[", "0", "]", "\n", "py", "=", "visit", "[", "1", "]", "\n", "\n", "np_goals", "=", "np", ".", "array", "(", "visit", "[", "2", ":", "]", ")", "\n", "goal_colours", "=", "[", "ig", "for", "ig", ",", "c", "in", "enumerate", "(", "visit", "[", "2", ":", "]", ")", "if", "c", "==", "True", "]", "\n", "if", "goal_colours", "==", "[", "]", ":", "\n", "                ", "goal_colours", "=", "[", "0", ",", "1", ",", "2", "]", "\n", "", "x_place", "=", "px", "#", "\n", "yy", "=", "0", "\n", "if", "(", "np_goals", "==", "False", ")", ".", "sum", "(", ")", "==", "1", ":", "\n", "                ", "yy", "=", "np", ".", "argwhere", "(", "np_goals", "==", "False", ")", "[", "0", "]", "\n", "", "elif", "(", "np_goals", "==", "False", ")", ".", "sum", "(", ")", "==", "2", ":", "\n", "                ", "yy", "=", "np", ".", "argwhere", "(", "np_goals", "==", "True", ")", "[", "0", "]", "\n", "", "y_place", "=", "py", "#", "\n", "\n", "# print(x_place, y_place, goal_colours, canvas.shape)", "\n", "if", "x_place", ">=", "grid_x", "or", "y_place", ">=", "grid_y", ":", "\n", "                ", "print", "(", "px", ",", "py", ",", "np_goals", ",", "goal_colours", ",", "x_place", ",", "y_place", ")", "\n", "", "canvas", "[", "x_place", ",", "y_place", ",", "goal_colours", "]", "=", "1", "\n", "\n", "", "if", "np", ".", "max", "(", "canvas", ")", "==", "0", ":", "\n", "            ", "return", "\n", "# canvas = canvas / (np.max(canvas) / scaling)", "\n", "\n", "# TODO: Colour the unvisited goals", "\n", "", "for", "goal", "in", "self", ".", "goals_order", ":", "\n", "            ", "canvas", "[", "goal", "[", "0", "]", ",", "goal", "[", "1", "]", ",", ":", "]", "=", "2", "/", "3", "\n", "", "if", "self", ".", "num_goals", ">=", "2", ":", "\n", "            ", "for", "g", "in", "range", "(", "self", ".", "num_goals", ")", ":", "\n", "                ", "for", "go_i", ",", "goal", "in", "enumerate", "(", "self", ".", "goals_order", ")", ":", "\n", "                    ", "if", "go_i", "!=", "g", ":", "\n", "                        ", "canvas", "[", "goal", "[", "0", "]", "+", "grid_x", ",", "goal", "[", "1", "]", "+", "g", "*", "grid_y", ",", ":", "]", "=", "2", "/", "3", "\n", "", "", "", "", "if", "self", ".", "num_goals", ">=", "3", ":", "\n", "            ", "for", "g", "in", "range", "(", "self", ".", "num_goals", ")", ":", "\n", "                ", "for", "go_i", ",", "goal", "in", "enumerate", "(", "self", ".", "goals_order", ")", ":", "\n", "                    ", "if", "go_i", "==", "g", ":", "\n", "                        ", "canvas", "[", "goal", "[", "0", "]", "+", "2", "*", "grid_x", ",", "goal", "[", "1", "]", "+", "g", "*", "grid_y", ",", ":", "]", "=", "2", "/", "3", "\n", "\n", "# The walls", "\n", "", "", "", "", "for", "x", "in", "range", "(", "grid_x", ")", ":", "\n", "            ", "for", "y", "in", "range", "(", "grid_y", ")", ":", "\n", "                ", "if", "self", ".", "grid", "[", "x", ",", "y", "]", "==", "1", ":", "\n", "                    ", "canvas", "[", "x", ",", "y", ",", ":", "]", "=", "1", "/", "3", "\n", "for", "zx", "in", "range", "(", "1", ",", "self", ".", "num_goals", ")", ":", "\n", "                        ", "for", "zy", "in", "range", "(", "self", ".", "num_goals", ")", ":", "\n", "                            ", "canvas", "[", "zx", "*", "grid_x", "+", "x", ",", "zy", "*", "grid_y", "+", "y", ",", ":", "]", "=", "1", "/", "3", "\n", "\n", "# Seperate the mazes", "\n", "", "", "", "", "", "canvas", "=", "np", ".", "insert", "(", "\n", "canvas", ",", "[", "(", "z", "+", "1", ")", "*", "grid_x", "for", "z", "in", "range", "(", "self", ".", "num_goals", "-", "1", ")", "]", ",", "1", ",", "axis", "=", "0", "\n", ")", "\n", "canvas", "=", "np", ".", "insert", "(", "\n", "canvas", ",", "[", "(", "z", "+", "1", ")", "*", "grid_x", "for", "z", "in", "range", "(", "self", ".", "num_goals", "-", "1", ")", "]", ",", "1", ",", "axis", "=", "1", "\n", ")", "\n", "canvas", "[", "0", ":", "grid_x", ",", "grid_y", "+", "1", ":", ",", ":", "]", "=", "0", "\n", "\n", "# Flip red and blue to make the bonus replay states blue instead of red", "\n", "if", "bonus_replay", ":", "\n", "# Blue instead of red", "\n", "            ", "red_canvas", "=", "np", ".", "copy", "(", "canvas", "[", ":", ",", ":", ",", "0", "]", ")", "\n", "canvas", "[", ":", ",", ":", ",", "0", "]", "=", "canvas", "[", ":", ",", ":", ",", "2", "]", "\n", "canvas", "[", ":", ",", ":", ",", "2", "]", "=", "red_canvas", "\n", "\n", "", "colour_maze", "=", "canvas", "\n", "\n", "colour_maze", "=", "np", ".", "clip", "(", "colour_maze", ",", "0", ",", "1", ")", "*", "255", "\n", "# colour_maze = np.swapaxes(colour_maze, 0, 1)", "\n", "colour_images", ".", "append", "(", "colour_maze", ".", "astype", "(", "np", ".", "uint8", ")", ")", "\n", "\n", "if", "not", "bonus_replay", ":", "\n", "            ", "self", ".", "xp_replay_image", "=", "np", ".", "copy", "(", "colour_images", "[", "0", "]", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "bonus_replay_image", "=", "np", ".", "copy", "(", "colour_images", "[", "0", "]", ")", "\n", "\n", "", "return", "colour_images", "[", "0", "]", "\n", "# save_video(\"{}/visitations/Goal_Visits__Interval_{}__T_{}\".format(LOGDIR, interval_size, T), colour_images)", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.gridworld.GridWorld.player_visits": [[463, 551], ["numpy.zeros", "range", "numpy.insert", "numpy.insert", "colour_images.append", "numpy.copy", "Exception", "numpy.array", "numpy.max", "range", "range", "range", "numpy.clip", "colour_maze.astype", "numpy.max", "enumerate", "enumerate", "enumerate", "numpy.argwhere", "range", "range", "range", "numpy.argwhere", "range"], "methods", ["None"], ["", "def", "player_visits", "(", "self", ",", "player_visits", ",", "args", ")", ":", "\n", "# Log the visitations", "\n", "# with open(\"{}/logs/Player_Positions.txt\".format(args.log_path), \"a\") as file:", "\n", "#     file.write('\\n'.join(\" \".join(str(x) for x in t) for t in player_visits))", "\n", "\n", "        ", "scaling", "=", "2", "\n", "\n", "if", "self", ".", "num_goals", ">", "3", ":", "\n", "            ", "raise", "Exception", "(", "\"Cant do state visitations for >3 goals atm\"", ")", "\n", "\n", "# We want to show visualisations for the agent depending on which goals they've visited as well", "\n", "# Keep it seperate from the other one", "\n", "", "colour_images", "=", "[", "]", "\n", "# Works for num_goals <= 3", "\n", "# print(self.grid.shape)", "\n", "canvas", "=", "np", ".", "zeros", "(", "\n", "(", "\n", "self", ".", "grid", ".", "shape", "[", "0", "]", "*", "self", ".", "num_goals", ",", "\n", "self", ".", "grid", ".", "shape", "[", "1", "]", "*", "self", ".", "num_goals", ",", "\n", "3", ",", "\n", ")", "\n", ")", "\n", "grid_x", "=", "self", ".", "grid", ".", "shape", "[", "0", "]", "\n", "grid_y", "=", "self", ".", "grid", ".", "shape", "[", "1", "]", "\n", "\n", "for", "visit", "in", "player_visits", ":", "\n", "            ", "px", "=", "visit", "[", "0", "]", "\n", "py", "=", "visit", "[", "1", "]", "\n", "\n", "np_goals", "=", "np", ".", "array", "(", "visit", "[", "2", ":", "]", ")", "\n", "goal_colours", "=", "[", "ig", "for", "ig", ",", "c", "in", "enumerate", "(", "visit", "[", "2", ":", "]", ")", "if", "c", "==", "True", "]", "\n", "if", "goal_colours", "==", "[", "]", ":", "\n", "                ", "goal_colours", "=", "[", "0", ",", "1", ",", "2", "]", "\n", "", "x_place", "=", "px", "+", "grid_x", "*", "(", "np_goals", "==", "False", ")", ".", "sum", "(", ")", "\n", "yy", "=", "0", "\n", "if", "(", "np_goals", "==", "False", ")", ".", "sum", "(", ")", "==", "1", ":", "\n", "                ", "yy", "=", "np", ".", "argwhere", "(", "np_goals", "==", "False", ")", "[", "0", "]", "\n", "", "elif", "(", "np_goals", "==", "False", ")", ".", "sum", "(", ")", "==", "2", ":", "\n", "                ", "yy", "=", "np", ".", "argwhere", "(", "np_goals", "==", "True", ")", "[", "0", "]", "\n", "", "y_place", "=", "py", "+", "grid_y", "*", "yy", "\n", "\n", "# print(x_place, y_place, goal_colours, canvas.shape)", "\n", "canvas", "[", "x_place", ",", "y_place", ",", "goal_colours", "]", "+=", "1", "\n", "\n", "", "if", "np", ".", "max", "(", "canvas", ")", "==", "0", ":", "\n", "            ", "return", "\n", "", "canvas", "=", "canvas", "/", "(", "np", ".", "max", "(", "canvas", ")", "/", "scaling", ")", "\n", "\n", "# TODO: Colour the unvisited goals", "\n", "for", "goal", "in", "self", ".", "goals_order", ":", "\n", "            ", "canvas", "[", "goal", "[", "0", "]", ",", "goal", "[", "1", "]", ",", ":", "]", "=", "2", "/", "3", "\n", "", "if", "self", ".", "num_goals", ">=", "2", ":", "\n", "            ", "for", "g", "in", "range", "(", "self", ".", "num_goals", ")", ":", "\n", "                ", "for", "go_i", ",", "goal", "in", "enumerate", "(", "self", ".", "goals_order", ")", ":", "\n", "                    ", "if", "go_i", "!=", "g", ":", "\n", "                        ", "canvas", "[", "goal", "[", "0", "]", "+", "grid_x", ",", "goal", "[", "1", "]", "+", "g", "*", "grid_y", ",", ":", "]", "=", "2", "/", "3", "\n", "", "", "", "", "if", "self", ".", "num_goals", ">=", "3", ":", "\n", "            ", "for", "g", "in", "range", "(", "self", ".", "num_goals", ")", ":", "\n", "                ", "for", "go_i", ",", "goal", "in", "enumerate", "(", "self", ".", "goals_order", ")", ":", "\n", "                    ", "if", "go_i", "==", "g", ":", "\n", "                        ", "canvas", "[", "goal", "[", "0", "]", "+", "2", "*", "grid_x", ",", "goal", "[", "1", "]", "+", "g", "*", "grid_y", ",", ":", "]", "=", "2", "/", "3", "\n", "\n", "# The walls", "\n", "", "", "", "", "for", "x", "in", "range", "(", "grid_x", ")", ":", "\n", "            ", "for", "y", "in", "range", "(", "grid_y", ")", ":", "\n", "                ", "if", "self", ".", "grid", "[", "x", ",", "y", "]", "==", "1", ":", "\n", "                    ", "canvas", "[", "x", ",", "y", ",", ":", "]", "=", "1", "/", "3", "\n", "for", "zx", "in", "range", "(", "1", ",", "self", ".", "num_goals", ")", ":", "\n", "                        ", "for", "zy", "in", "range", "(", "self", ".", "num_goals", ")", ":", "\n", "                            ", "canvas", "[", "zx", "*", "grid_x", "+", "x", ",", "zy", "*", "grid_y", "+", "y", ",", ":", "]", "=", "1", "/", "3", "\n", "\n", "# Seperate the mazes", "\n", "", "", "", "", "", "canvas", "=", "np", ".", "insert", "(", "\n", "canvas", ",", "[", "(", "z", "+", "1", ")", "*", "grid_x", "for", "z", "in", "range", "(", "self", ".", "num_goals", "-", "1", ")", "]", ",", "1", ",", "axis", "=", "0", "\n", ")", "\n", "canvas", "=", "np", ".", "insert", "(", "\n", "canvas", ",", "[", "(", "z", "+", "1", ")", "*", "grid_x", "for", "z", "in", "range", "(", "self", ".", "num_goals", "-", "1", ")", "]", ",", "1", ",", "axis", "=", "1", "\n", ")", "\n", "canvas", "[", "0", ":", "grid_x", ",", "grid_y", "+", "1", ":", ",", ":", "]", "=", "0", "\n", "colour_maze", "=", "canvas", "\n", "\n", "colour_maze", "=", "np", ".", "clip", "(", "colour_maze", ",", "0", ",", "1", ")", "*", "255", "\n", "# colour_maze = np.swapaxes(colour_maze, 0, 1)", "\n", "colour_images", ".", "append", "(", "colour_maze", ".", "astype", "(", "np", ".", "uint8", ")", ")", "\n", "\n", "self", ".", "player_visits_image", "=", "np", ".", "copy", "(", "colour_images", "[", "0", "]", ")", "\n", "\n", "return", "colour_images", "[", "0", "]", "\n", "# save_video(\"{}/visitations/Goal_Visits__Interval_{}__T_{}\".format(LOGDIR, interval_size, T), colour_images)", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.gridworld.GridWorld.bonus_landscape": [[553, 639], ["numpy.zeros", "zip", "range", "numpy.insert", "numpy.insert", "colour_images.append", "Exception", "numpy.array", "max", "numpy.max", "range", "range", "range", "numpy.clip", "colour_maze.astype", "enumerate", "enumerate", "enumerate", "numpy.argwhere", "range", "range", "range", "numpy.argwhere", "range"], "methods", ["None"], ["", "def", "bonus_landscape", "(", "self", ",", "player_visits", ",", "exploration_bonuses", ",", "max_bonus", ",", "args", ")", ":", "\n", "# interval = int(args.t_max / args.interval_size)", "\n", "# scaling = 2", "\n", "\n", "        ", "if", "self", ".", "num_goals", ">", "3", ":", "\n", "            ", "raise", "Exception", "(", "\"Cant do bonus landscape for >3 goals atm\"", ")", "\n", "\n", "# We want to show visualisations for the agent depending on which goals they've visited as well", "\n", "# Keep it seperate from the other one", "\n", "", "colour_images", "=", "[", "]", "\n", "# for i in range(0, args.t_max, interval // 10):", "\n", "# Works for num_goals <= 3", "\n", "# print(self.grid.shape)", "\n", "canvas", "=", "np", ".", "zeros", "(", "\n", "(", "\n", "self", ".", "grid", ".", "shape", "[", "0", "]", "*", "self", ".", "num_goals", ",", "\n", "self", ".", "grid", ".", "shape", "[", "1", "]", "*", "self", ".", "num_goals", ",", "\n", "3", ",", "\n", ")", "\n", ")", "\n", "grid_x", "=", "self", ".", "grid", ".", "shape", "[", "0", "]", "\n", "grid_y", "=", "self", ".", "grid", ".", "shape", "[", "1", "]", "\n", "\n", "for", "visit", ",", "bonus", "in", "zip", "(", "player_visits", ",", "exploration_bonuses", ")", ":", "\n", "            ", "relative_bonus", "=", "bonus", "/", "max_bonus", "\n", "px", "=", "visit", "[", "0", "]", "\n", "py", "=", "visit", "[", "1", "]", "\n", "\n", "np_goals", "=", "np", ".", "array", "(", "visit", "[", "2", ":", "]", ")", "\n", "goal_colours", "=", "[", "ig", "for", "ig", ",", "c", "in", "enumerate", "(", "visit", "[", "2", ":", "]", ")", "if", "c", "==", "True", "]", "\n", "if", "goal_colours", "==", "[", "]", ":", "\n", "                ", "goal_colours", "=", "[", "0", ",", "1", ",", "2", "]", "\n", "", "x_place", "=", "px", "+", "grid_x", "*", "(", "np_goals", "==", "False", ")", ".", "sum", "(", ")", "\n", "yy", "=", "0", "\n", "if", "(", "np_goals", "==", "False", ")", ".", "sum", "(", ")", "==", "1", ":", "\n", "                ", "yy", "=", "np", ".", "argwhere", "(", "np_goals", "==", "False", ")", "[", "0", "]", "\n", "", "elif", "(", "np_goals", "==", "False", ")", ".", "sum", "(", ")", "==", "2", ":", "\n", "                ", "yy", "=", "np", ".", "argwhere", "(", "np_goals", "==", "True", ")", "[", "0", "]", "\n", "", "y_place", "=", "py", "+", "grid_y", "*", "yy", "\n", "\n", "# print(x_place, y_place, goal_colours, canvas.shape)", "\n", "canvas", "[", "x_place", ",", "y_place", ",", "goal_colours", "]", "=", "max", "(", "\n", "relative_bonus", ",", "canvas", "[", "x_place", ",", "y_place", ",", "goal_colours", "[", "0", "]", "]", "\n", ")", "\n", "\n", "", "if", "np", ".", "max", "(", "canvas", ")", "==", "0", ":", "\n", "            ", "return", "\n", "# canvas = canvas / np.max(canvas)", "\n", "\n", "# TODO: Colour the unvisited goals", "\n", "", "for", "goal", "in", "self", ".", "goals_order", ":", "\n", "            ", "canvas", "[", "goal", "[", "0", "]", ",", "goal", "[", "1", "]", ",", ":", "]", "=", "2", "/", "3", "\n", "", "if", "self", ".", "num_goals", ">=", "2", ":", "\n", "            ", "for", "g", "in", "range", "(", "self", ".", "num_goals", ")", ":", "\n", "                ", "for", "go_i", ",", "goal", "in", "enumerate", "(", "self", ".", "goals_order", ")", ":", "\n", "                    ", "if", "go_i", "!=", "g", ":", "\n", "                        ", "canvas", "[", "goal", "[", "0", "]", "+", "grid_x", ",", "goal", "[", "1", "]", "+", "g", "*", "grid_y", ",", ":", "]", "=", "2", "/", "3", "\n", "", "", "", "", "if", "self", ".", "num_goals", ">=", "3", ":", "\n", "            ", "for", "g", "in", "range", "(", "self", ".", "num_goals", ")", ":", "\n", "                ", "for", "go_i", ",", "goal", "in", "enumerate", "(", "self", ".", "goals_order", ")", ":", "\n", "                    ", "if", "go_i", "==", "g", ":", "\n", "                        ", "canvas", "[", "goal", "[", "0", "]", "+", "2", "*", "grid_x", ",", "goal", "[", "1", "]", "+", "g", "*", "grid_y", ",", ":", "]", "=", "2", "/", "3", "\n", "\n", "# The walls", "\n", "", "", "", "", "for", "x", "in", "range", "(", "grid_x", ")", ":", "\n", "            ", "for", "y", "in", "range", "(", "grid_y", ")", ":", "\n", "                ", "if", "self", ".", "grid", "[", "x", ",", "y", "]", "==", "1", ":", "\n", "                    ", "canvas", "[", "x", ",", "y", ",", ":", "]", "=", "1", "/", "3", "\n", "for", "zx", "in", "range", "(", "1", ",", "self", ".", "num_goals", ")", ":", "\n", "                        ", "for", "zy", "in", "range", "(", "self", ".", "num_goals", ")", ":", "\n", "                            ", "canvas", "[", "zx", "*", "grid_x", "+", "x", ",", "zy", "*", "grid_y", "+", "y", ",", ":", "]", "=", "1", "/", "3", "\n", "\n", "# Seperate the mazes", "\n", "", "", "", "", "", "canvas", "=", "np", ".", "insert", "(", "\n", "canvas", ",", "[", "(", "z", "+", "1", ")", "*", "grid_x", "for", "z", "in", "range", "(", "self", ".", "num_goals", "-", "1", ")", "]", ",", "1", ",", "axis", "=", "0", "\n", ")", "\n", "canvas", "=", "np", ".", "insert", "(", "\n", "canvas", ",", "[", "(", "z", "+", "1", ")", "*", "grid_x", "for", "z", "in", "range", "(", "self", ".", "num_goals", "-", "1", ")", "]", ",", "1", ",", "axis", "=", "1", "\n", ")", "\n", "canvas", "[", "0", ":", "grid_x", ",", "grid_y", "+", "1", ":", ",", ":", "]", "=", "0", "\n", "colour_maze", "=", "canvas", "\n", "\n", "colour_maze", "=", "np", ".", "clip", "(", "colour_maze", ",", "0", ",", "1", ")", "*", "255", "\n", "# colour_maze = np.swapaxes(colour_maze, 0, 1)", "\n", "colour_images", ".", "append", "(", "colour_maze", ".", "astype", "(", "np", ".", "uint8", ")", ")", "\n", "return", "colour_images", "[", "0", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.gridworld.GridWorld.count_state_action_space": [[640, 691], ["numpy.zeros", "numpy.zeros", "range", "canvas.clip.clip.clip", "range", "range", "range", "numpy.clip", "range", "range", "numpy.copy", "count_model.get_count", "torch.tensor"], "methods", ["home.repos.pwc.inspect_result.oxwhirl_opiq.count.dora_count.DoraCount.get_count"], ["", "def", "count_state_action_space", "(", "self", ",", "count_model", ",", "args", ")", ":", "\n", "\n", "        ", "actions", "=", "args", ".", "num_actions", "\n", "canvas", "=", "np", ".", "zeros", "(", "\n", "(", "\n", "self", ".", "grid", ".", "shape", "[", "0", "]", "*", "self", ".", "num_goals", ",", "\n", "self", ".", "grid", ".", "shape", "[", "1", "]", "*", "self", ".", "num_goals", "*", "actions", ",", "\n", "3", ",", "\n", ")", "\n", ")", "\n", "grid_x", "=", "self", ".", "grid", ".", "shape", "[", "0", "]", "\n", "grid_y", "=", "self", ".", "grid", ".", "shape", "[", "1", "]", "\n", "grid", "=", "self", ".", "grid", "\n", "\n", "counts", "=", "np", ".", "zeros", "(", "(", "self", ".", "grid", ".", "shape", "[", "0", "]", "*", "self", ".", "num_goals", ",", "self", ".", "grid", ".", "shape", "[", "1", "]", "*", "self", ".", "num_goals", "*", "actions", ")", ")", "\n", "\n", "# TODO: Batch the states if we need more efficiency", "\n", "for", "a", "in", "range", "(", "actions", ")", ":", "\n", "            ", "for", "x", "in", "range", "(", "grid_x", ")", ":", "\n", "                ", "for", "y", "in", "range", "(", "grid_y", ")", ":", "\n", "\n", "                    ", "if", "grid", "[", "x", ",", "y", "]", "==", "1", "or", "grid", "[", "x", ",", "y", "]", "==", "2", ":", "\n", "# If the position is a wall the player cannot ever be there", "\n", "                        ", "continue", "\n", "\n", "", "state_copy", "=", "np", ".", "copy", "(", "self", ".", "grid", ")", "\n", "state_copy", "[", "self", ".", "player_pos", "]", "=", "0", "\n", "state_copy", "[", "x", ",", "y", "]", "=", "3", "\n", "# print(state_copy)", "\n", "state_copy", "=", "state_copy", "[", "np", ".", "newaxis", ",", ":", ",", ":", "]", "/", "3", "\n", "\n", "count", "=", "count_model", ".", "get_count", "(", "torch", ".", "tensor", "(", "[", "state_copy", "]", ",", "device", "=", "device", ",", "dtype", "=", "torch", ".", "float32", ")", ",", "action", "=", "a", ")", "\n", "# print(x,y,bonus)", "\n", "canvas", "[", "x", ",", "y", "+", "(", "grid_y", "*", "a", ")", ",", "1", "]", "=", "count", "\n", "counts", "[", "x", ",", "y", "+", "(", "grid_y", "*", "a", ")", "]", "=", "count", "\n", "\n", "# canvas /= np.max(canvas)", "\n", "", "", "", "max_count", "=", "500", "\n", "canvas", "=", "canvas", ".", "clip", "(", "min", "=", "0", ",", "max", "=", "max_count", ")", "\n", "canvas", "/=", "max_count", "\n", "\n", "# Walls", "\n", "for", "a", "in", "range", "(", "actions", ")", ":", "\n", "            ", "for", "x", "in", "range", "(", "grid_x", ")", ":", "\n", "                ", "for", "y", "in", "range", "(", "grid_y", ")", ":", "\n", "                    ", "if", "grid", "[", "x", ",", "y", "]", "==", "1", "or", "grid", "[", "x", ",", "y", "]", "==", "2", ":", "\n", "                        ", "canvas", "[", "x", ",", "y", "+", "(", "grid_y", "*", "a", ")", ",", ":", "]", "=", "grid", "[", "x", ",", "y", "]", "/", "3", "\n", "\n", "", "", "", "", "canvas", "=", "np", ".", "clip", "(", "canvas", ",", "0", ",", "1", ")", "*", "255", "\n", "\n", "return", "canvas", ",", "counts", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.gridworld.GridWorld.np_q_vals": [[692, 731], ["numpy.zeros", "range", "range", "numpy.copy", "torch.tensor", "range", "nn", "count_model.get_count"], "methods", ["home.repos.pwc.inspect_result.oxwhirl_opiq.count.dora_count.DoraCount.get_count"], ["", "def", "np_q_vals", "(", "self", ",", "count_model", ",", "nn", ",", "args", ")", ":", "\n", "        ", "actions", "=", "args", ".", "num_actions", "\n", "canvas", "=", "np", ".", "zeros", "(", "\n", "(", "\n", "self", ".", "grid", ".", "shape", "[", "0", "]", "*", "self", ".", "num_goals", ",", "\n", "self", ".", "grid", ".", "shape", "[", "1", "]", "*", "self", ".", "num_goals", "*", "actions", ",", "\n", "3", ",", "\n", ")", "\n", ")", "\n", "grid_x", "=", "self", ".", "grid", ".", "shape", "[", "0", "]", "\n", "grid_y", "=", "self", ".", "grid", ".", "shape", "[", "1", "]", "\n", "grid", "=", "self", ".", "grid", "\n", "\n", "# TODO: Batch the states if we need more efficiency", "\n", "for", "x", "in", "range", "(", "grid_x", ")", ":", "\n", "            ", "for", "y", "in", "range", "(", "grid_y", ")", ":", "\n", "\n", "                ", "if", "grid", "[", "x", ",", "y", "]", "==", "1", "or", "grid", "[", "x", ",", "y", "]", "==", "2", ":", "\n", "# If the position is a wall the player cannot ever be there", "\n", "                    ", "continue", "\n", "\n", "", "state_copy", "=", "np", ".", "copy", "(", "self", ".", "grid", ")", "\n", "state_copy", "[", "self", ".", "player_pos", "]", "=", "0", "\n", "state_copy", "[", "x", ",", "y", "]", "=", "3", "\n", "# print(state_copy)", "\n", "state_copy", "=", "state_copy", "[", "np", ".", "newaxis", ",", ":", ",", ":", "]", "/", "3", "\n", "\n", "state_tensor", "=", "torch", ".", "tensor", "(", "[", "state_copy", "]", ",", "device", "=", "device", ",", "dtype", "=", "torch", ".", "float32", ")", "\n", "# state_tensor = state_tensor.transpose(1,3).transpose(2,3)", "\n", "q_vals", "=", "nn", "(", "state_tensor", ")", "[", "0", "]", "\n", "# Inefficient", "\n", "for", "a", "in", "range", "(", "actions", ")", ":", "\n", "                    ", "canvas", "[", "x", ",", "y", "+", "(", "grid_y", "*", "a", ")", ",", "0", "]", "=", "q_vals", "[", "a", "]", "\n", "\n", "if", "args", ".", "optim_bootstrap", ":", "\n", "                        ", "count", "=", "count_model", ".", "get_count", "(", "state_tensor", ",", "action", "=", "a", ")", "\n", "# print(x,y,bonus)", "\n", "canvas", "[", "x", ",", "y", "+", "(", "grid_y", "*", "a", ")", ",", "0", "]", "+=", "args", ".", "optim_bootstrap_tau", "/", "(", "(", "count", "+", "1.0", ")", "**", "args", ".", "optim_m", ")", "\n", "", "", "", "", "return", "canvas", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.gridworld.GridWorld.q_value_estimates": [[732, 756], ["gridworld.GridWorld.np_q_vals", "numpy.copy", "canvas.clip.clip.clip", "range", "range", "numpy.clip", "range"], "methods", ["home.repos.pwc.inspect_result.oxwhirl_opiq.envs.gridworld.GridWorld.np_q_vals"], ["", "def", "q_value_estimates", "(", "self", ",", "count_model", ",", "nn", ",", "args", ")", ":", "\n", "        ", "q_vals", "=", "self", ".", "np_q_vals", "(", "count_model", ",", "nn", ",", "args", ")", "\n", "canvas", "=", "np", ".", "copy", "(", "q_vals", ")", "\n", "actions", "=", "args", ".", "num_actions", "\n", "grid_x", "=", "self", ".", "grid", ".", "shape", "[", "0", "]", "\n", "grid_y", "=", "self", ".", "grid", ".", "shape", "[", "1", "]", "\n", "grid", "=", "self", ".", "grid", "\n", "\n", "# canvas /= np.max(canvas)", "\n", "max_count", "=", "self", ".", "positive_reward", "if", "args", ".", "reward_clipping", "is", "False", "else", "+", "1", "\n", "max_count", "*=", "1.5", "\n", "canvas", "=", "canvas", ".", "clip", "(", "min", "=", "0", ",", "max", "=", "max_count", ")", "\n", "canvas", "/=", "max_count", "\n", "\n", "# Walls", "\n", "for", "a", "in", "range", "(", "actions", ")", ":", "\n", "            ", "for", "x", "in", "range", "(", "grid_x", ")", ":", "\n", "                ", "for", "y", "in", "range", "(", "grid_y", ")", ":", "\n", "                    ", "if", "grid", "[", "x", ",", "y", "]", "==", "1", "or", "grid", "[", "x", ",", "y", "]", "==", "2", ":", "\n", "                        ", "canvas", "[", "x", ",", "y", "+", "(", "grid_y", "*", "a", ")", ",", ":", "]", "=", "grid", "[", "x", ",", "y", "]", "/", "3", "\n", "\n", "", "", "", "", "canvas", "=", "np", ".", "clip", "(", "canvas", ",", "0", ",", "1", ")", "*", "255", "\n", "\n", "return", "canvas", ",", "q_vals", "\n", "", "", ""]], "home.repos.pwc.inspect_result.oxwhirl_opiq.agent.conv_dqn.DQN.__init__": [[11, 64], ["torch.Module.__init__", "logging.getLogger", "conv_dqn.DQN.logger.critical", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "range", "conv_dqn.DQN.logger.critical", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "conv_dqn.DQN.logger.critical", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "int", "conv_dqn.DQN.logger.critical", "int", "conv_dqn.DQN.logger.critical", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.Parameter", "torch.Parameter", "torch.Parameter", "range"], "methods", ["home.repos.pwc.inspect_result.oxwhirl_opiq.action.count_action.CountBonus.__init__"], ["    ", "def", "__init__", "(", "self", ",", "args", ")", ":", "\n", "        ", "super", "(", "DQN", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "logger", "=", "logging", ".", "getLogger", "(", "\"Maze_DQN\"", ")", "\n", "self", ".", "input_frames", "=", "args", ".", "past_frames_input", "\n", "\n", "maze_size", "=", "args", ".", "state_shape", "[", "0", "]", "\n", "\n", "image_size", "=", "maze_size", "\n", "self", ".", "logger", ".", "critical", "(", "\n", "\"Maze image input: ({1},{0},{0})\"", ".", "format", "(", "image_size", ",", "self", ".", "input_frames", ")", "\n", ")", "\n", "\n", "stride", "=", "2", "\n", "stride_third", "=", "1", "\n", "if", "maze_size", "<=", "9", ":", "\n", "            ", "stride", "=", "1", "\n", "", "if", "maze_size", ">", "40", ":", "\n", "            ", "stride_third", "=", "2", "\n", "\n", "", "channels", "=", "32", "\n", "if", "image_size", "<", "25", ":", "\n", "            ", "channels", "=", "16", "\n", "\n", "", "convs", "=", "2", "\n", "if", "image_size", ">", "26", ":", "\n", "            ", "convs", "=", "3", "\n", "", "self", ".", "convs", "=", "convs", "\n", "\n", "self", ".", "conv1", "=", "nn", ".", "Conv2d", "(", "self", ".", "input_frames", ",", "channels", ",", "3", ",", "stride", "=", "stride", ")", "\n", "self", ".", "conv2", "=", "nn", ".", "Conv2d", "(", "channels", ",", "channels", ",", "3", ",", "stride", "=", "stride", ")", "\n", "if", "convs", ">", "2", ":", "\n", "            ", "self", ".", "conv3", "=", "nn", ".", "Conv2d", "(", "channels", ",", "channels", ",", "3", ",", "stride", "=", "stride_third", ")", "\n", "\n", "", "for", "_", "in", "range", "(", "2", ")", ":", "\n", "            ", "image_size", "=", "int", "(", "(", "image_size", "+", "2", "*", "0", "-", "3", ")", "/", "stride", "+", "1", ")", "\n", "self", ".", "logger", ".", "critical", "(", "\"After Conv: ({},{},{})\"", ".", "format", "(", "channels", ",", "image_size", ",", "image_size", ")", ")", "\n", "\n", "", "if", "convs", ">", "2", ":", "\n", "            ", "image_size", "=", "int", "(", "(", "image_size", "+", "2", "*", "0", "-", "3", ")", "/", "stride_third", "+", "1", ")", "\n", "self", ".", "logger", ".", "critical", "(", "\"After Conv: ({},{},{})\"", ".", "format", "(", "channels", ",", "image_size", ",", "image_size", ")", ")", "\n", "\n", "", "self", ".", "fc_size", "=", "image_size", "*", "image_size", "*", "channels", "\n", "self", ".", "fc_size_half", "=", "image_size", "*", "image_size", "*", "(", "channels", "//", "2", ")", "\n", "self", ".", "logger", ".", "critical", "(", "\"FC {} -> {}\"", ".", "format", "(", "self", ".", "fc_size", ",", "self", ".", "fc_size_half", ")", ")", "\n", "self", ".", "fc1", "=", "nn", ".", "Linear", "(", "self", ".", "fc_size", ",", "self", ".", "fc_size_half", ")", "\n", "\n", "self", ".", "qvals", "=", "nn", ".", "Linear", "(", "self", ".", "fc_size_half", ",", "args", ".", "num_actions", ")", "\n", "self", ".", "logger", ".", "critical", "(", "\"FC {} -> {} (Q-Values)\"", ".", "format", "(", "self", ".", "fc_size_half", ",", "args", ".", "num_actions", ")", ")", "\n", "\n", "if", "args", ".", "final_layer_bias", ">", "-", "1", ":", "\n", "            ", "optimistic_bias", "=", "torch", ".", "tensor", "(", "[", "args", ".", "final_layer_bias", "for", "_", "in", "range", "(", "args", ".", "num_actions", ")", "]", ",", "dtype", "=", "torch", ".", "float", ")", "\n", "self", ".", "qvals", ".", "bias", "=", "nn", ".", "Parameter", "(", "optimistic_bias", ")", "\n", "# self.to(device)", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.agent.conv_dqn.DQN.forward": [[66, 82], ["torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.relu.view", "torch.relu", "torch.relu", "torch.relu", "conv_dqn.DQN.qvals", "conv_dqn.DQN.conv1", "conv_dqn.DQN.conv2", "torch.relu", "torch.relu", "torch.relu", "conv_dqn.DQN.fc1", "conv_dqn.DQN.conv3"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "# x = x.to(device)", "\n", "\n", "        ", "x", "=", "F", ".", "relu", "(", "self", ".", "conv1", "(", "x", ")", ")", "\n", "x", "=", "F", ".", "relu", "(", "self", ".", "conv2", "(", "x", ")", ")", "\n", "if", "self", ".", "convs", ">", "2", ":", "\n", "            ", "x", "=", "F", ".", "relu", "(", "self", ".", "conv3", "(", "x", ")", ")", "\n", "\n", "# Flatten", "\n", "", "x", "=", "x", ".", "view", "(", "-", "1", ",", "self", ".", "fc_size", ")", "\n", "\n", "x", "=", "F", ".", "relu", "(", "self", ".", "fc1", "(", "x", ")", ")", "\n", "\n", "q", "=", "self", ".", "qvals", "(", "x", ")", "\n", "\n", "return", "q", "\n", "", "", ""]], "home.repos.pwc.inspect_result.oxwhirl_opiq.agent.fc_atari_dqn.DQN.__init__": [[9, 25], ["torch.Module.__init__", "torch.Sequential", "torch.Sequential", "torch.Linear", "torch.Linear", "torch.ReLU", "torch.ReLU", "torch.Linear", "torch.Linear", "torch.ReLU", "torch.ReLU", "torch.Linear", "torch.Linear", "torch.ReLU", "torch.ReLU", "torch.Linear", "torch.Linear", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.Parameter", "torch.Parameter", "range"], "methods", ["home.repos.pwc.inspect_result.oxwhirl_opiq.action.count_action.CountBonus.__init__"], ["    ", "def", "__init__", "(", "self", ",", "args", ")", ":", "\n", "        ", "super", "(", "DQN", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "input_dim", "=", "args", ".", "state_shape", "[", "0", "]", "\n", "num_actions", "=", "args", ".", "num_actions", "\n", "\n", "self", ".", "model", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Linear", "(", "input_dim", ",", "512", ")", ",", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Linear", "(", "512", ",", "512", ")", ",", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Linear", "(", "512", ",", "256", ")", ",", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Linear", "(", "256", ",", "num_actions", ")", "\n", ")", "\n", "\n", "if", "args", ".", "final_layer_bias", ">", "-", "1", ":", "\n", "            ", "optimistic_bias", "=", "torch", ".", "tensor", "(", "[", "args", ".", "final_layer_bias", "for", "_", "in", "range", "(", "num_actions", ")", "]", ",", "dtype", "=", "torch", ".", "float", ")", "\n", "self", ".", "model", "[", "-", "1", "]", ".", "bias", "=", "nn", ".", "Parameter", "(", "optimistic_bias", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.agent.fc_atari_dqn.DQN.forward": [[27, 31], ["fc_atari_dqn.DQN.model"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "# model_input = x.to(device)", "\n", "\n", "        ", "return", "self", ".", "model", "(", "x", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.oxwhirl_opiq.agent.atari_qrdqn.QRDQN.__init__": [[9, 37], ["torch.Module.__init__", "logging.getLogger", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Conv2d", "torch.Conv2d", "torch.ReLU", "torch.ReLU", "torch.Conv2d", "torch.Conv2d", "torch.ReLU", "torch.ReLU", "torch.Conv2d", "torch.Conv2d", "torch.ReLU", "torch.ReLU", "torch.Linear", "torch.Linear", "torch.ReLU", "torch.ReLU", "torch.Linear", "torch.Linear", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.Parameter", "torch.Parameter", "atari_qrdqn.QRDQN.feature_size", "range"], "methods", ["home.repos.pwc.inspect_result.oxwhirl_opiq.action.count_action.CountBonus.__init__", "home.repos.pwc.inspect_result.oxwhirl_opiq.rnd_net.atari_conv_net.Predictor_RND.feature_size"], ["    ", "def", "__init__", "(", "self", ",", "args", ")", ":", "\n", "        ", "super", "(", "QRDQN", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "logger", "=", "logging", ".", "getLogger", "(", "\"Atari_DQN\"", ")", "\n", "self", ".", "input_frames", "=", "args", ".", "past_frames_input", "\n", "\n", "self", ".", "input_shape", "=", "(", "args", ".", "past_frames_input", ",", "*", "args", ".", "state_shape", "[", ":", "2", "]", ")", "\n", "self", ".", "num_actions", "=", "args", ".", "num_actions", "\n", "self", ".", "quantiles", "=", "args", ".", "quantiles", "\n", "\n", "self", ".", "features", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Conv2d", "(", "self", ".", "input_shape", "[", "0", "]", ",", "32", ",", "kernel_size", "=", "8", ",", "stride", "=", "4", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Conv2d", "(", "32", ",", "64", ",", "kernel_size", "=", "4", ",", "stride", "=", "2", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Conv2d", "(", "64", ",", "64", ",", "kernel_size", "=", "3", ",", "stride", "=", "1", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", "\n", ")", "\n", "\n", "self", ".", "fc", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Linear", "(", "self", ".", "feature_size", "(", ")", ",", "512", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Linear", "(", "512", ",", "self", ".", "num_actions", "*", "self", ".", "quantiles", ")", "\n", ")", "\n", "\n", "if", "args", ".", "final_layer_bias", ">", "-", "1", ":", "\n", "            ", "optimistic_bias", "=", "torch", ".", "tensor", "(", "[", "args", ".", "final_layer_bias", "for", "_", "in", "range", "(", "args", ".", "num_actions", ")", "]", ",", "dtype", "=", "torch", ".", "float", ")", "\n", "self", ".", "fc", "[", "-", "1", "]", ".", "bias", "=", "nn", ".", "Parameter", "(", "optimistic_bias", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.agent.atari_qrdqn.QRDQN.forward": [[38, 40], ["atari_qrdqn.QRDQN.get_quantiles().mean", "atari_qrdqn.QRDQN.get_quantiles"], "methods", ["home.repos.pwc.inspect_result.oxwhirl_opiq.agent.atari_qrdqn.QRDQN.get_quantiles"], ["", "", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "return", "self", ".", "get_quantiles", "(", "x", ")", ".", "mean", "(", "dim", "=", "2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.agent.atari_qrdqn.QRDQN.get_quantiles": [[41, 46], ["atari_qrdqn.QRDQN.features", "x.view.view.view", "atari_qrdqn.QRDQN.fc", "atari_qrdqn.QRDQN.view", "x.view.view.size"], "methods", ["None"], ["", "def", "get_quantiles", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "features", "(", "x", ")", "\n", "x", "=", "x", ".", "view", "(", "x", ".", "size", "(", "0", ")", ",", "-", "1", ")", "\n", "q", "=", "self", ".", "fc", "(", "x", ")", "\n", "return", "q", ".", "view", "(", "-", "1", ",", "self", ".", "num_actions", ",", "self", ".", "quantiles", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.agent.atari_qrdqn.QRDQN.feature_size": [[47, 49], ["atari_qrdqn.QRDQN.features().view().size", "atari_qrdqn.QRDQN.features().view", "atari_qrdqn.QRDQN.features", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros"], "methods", ["None"], ["", "def", "feature_size", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "features", "(", "torch", ".", "zeros", "(", "1", ",", "*", "self", ".", "input_shape", ")", ")", ".", "view", "(", "1", ",", "-", "1", ")", ".", "size", "(", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.agent.bsp_atari_conv.BSPDQN.__init__": [[11, 57], ["torch.Module.__init__", "logging.getLogger", "bsp_atari_conv.BSPDQN.logger.critical", "torch.Sequential", "torch.Sequential", "torch.Sequential", "bsp_atari_conv.BSPDQN.conv.parameters", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "bsp_atari_conv.BSPDQN.feature_size", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Linear", "torch.Linear", "torch.Linear", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.Linear", "torch.Linear", "torch.Linear", "range", "torch.Linear", "torch.Linear", "torch.Linear", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.Linear", "torch.Linear", "torch.Linear", "range", "bsp_atari_conv.BSPDQN.feature_size", "bsp_atari_conv.BSPDQN.feature_size"], "methods", ["home.repos.pwc.inspect_result.oxwhirl_opiq.action.count_action.CountBonus.__init__", "home.repos.pwc.inspect_result.oxwhirl_opiq.rnd_net.atari_conv_net.Predictor_RND.feature_size", "home.repos.pwc.inspect_result.oxwhirl_opiq.rnd_net.atari_conv_net.Predictor_RND.feature_size", "home.repos.pwc.inspect_result.oxwhirl_opiq.rnd_net.atari_conv_net.Predictor_RND.feature_size"], ["    ", "def", "__init__", "(", "self", ",", "args", ")", ":", "\n", "        ", "super", "(", "BSPDQN", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "logger", "=", "logging", ".", "getLogger", "(", "\"Atari_BSP_DQN\"", ")", "\n", "self", ".", "input_frames", "=", "args", ".", "past_frames_input", "\n", "self", ".", "input_shape", "=", "(", "args", ".", "past_frames_input", ",", "*", "args", ".", "state_shape", "[", ":", "2", "]", ")", "\n", "self", ".", "num_actions", "=", "args", ".", "num_actions", "\n", "\n", "maze_size", "=", "args", ".", "state_shape", "[", "0", "]", "\n", "\n", "image_size", "=", "maze_size", "\n", "self", ".", "logger", ".", "critical", "(", "\n", "\"Atari image input: ({1},{0},{0})\"", ".", "format", "(", "image_size", ",", "self", ".", "input_frames", ")", "\n", ")", "\n", "self", ".", "bsp_k", "=", "args", ".", "bsp_k", "\n", "self", ".", "conv", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Conv2d", "(", "self", ".", "input_shape", "[", "0", "]", ",", "32", ",", "kernel_size", "=", "8", ",", "stride", "=", "4", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Conv2d", "(", "32", ",", "64", ",", "kernel_size", "=", "4", ",", "stride", "=", "2", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Conv2d", "(", "64", ",", "64", ",", "kernel_size", "=", "3", ",", "stride", "=", "1", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", "\n", ")", "\n", "self", ".", "conv_params", "=", "self", ".", "conv", ".", "parameters", "(", ")", "\n", "self", ".", "heads", "=", "nn", ".", "ModuleList", "(", "[", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Linear", "(", "self", ".", "feature_size", "(", ")", ",", "512", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Linear", "(", "512", ",", "self", ".", "num_actions", ")", "\n", ")", "for", "_", "in", "range", "(", "self", ".", "bsp_k", ")", "]", ")", "\n", "\n", "self", ".", "prior_conv", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Conv2d", "(", "self", ".", "input_shape", "[", "0", "]", ",", "32", ",", "kernel_size", "=", "8", ",", "stride", "=", "4", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Conv2d", "(", "32", ",", "64", ",", "kernel_size", "=", "4", ",", "stride", "=", "2", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Conv2d", "(", "64", ",", "64", ",", "kernel_size", "=", "3", ",", "stride", "=", "1", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", "\n", ")", "\n", "self", ".", "prior_heads", "=", "nn", ".", "ModuleList", "(", "[", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Linear", "(", "self", ".", "feature_size", "(", ")", ",", "512", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Linear", "(", "512", ",", "self", ".", "num_actions", ")", "\n", ")", "for", "_", "in", "range", "(", "self", ".", "bsp_k", ")", "]", ")", "\n", "self", ".", "prior_heads", ".", "requires_grad", "=", "False", "\n", "self", ".", "prior_beta", "=", "args", ".", "bsp_beta", "\n", "self", ".", "fc_size", "=", "self", ".", "feature_size", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.agent.bsp_atari_conv.BSPDQN.feature_size": [[58, 60], ["bsp_atari_conv.BSPDQN.conv().view().size", "bsp_atari_conv.BSPDQN.conv().view", "bsp_atari_conv.BSPDQN.conv", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros"], "methods", ["None"], ["", "def", "feature_size", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "conv", "(", "torch", ".", "zeros", "(", "1", ",", "*", "self", ".", "input_shape", ")", ")", ".", "view", "(", "1", ",", "-", "1", ")", ".", "size", "(", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.agent.bsp_atari_conv.BSPDQN.scale_gradients": [[62, 65], ["None"], "methods", ["None"], ["", "def", "scale_gradients", "(", "self", ")", ":", "\n", "        ", "for", "c", "in", "self", ".", "conv_params", ":", "\n", "            ", "c", ".", "grad", "*=", "1", "/", "self", ".", "bsp_k", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.agent.bsp_atari_conv.BSPDQN.forward": [[66, 81], ["bsp_atari_conv.BSPDQN.conv", "bsp_atari_conv.BSPDQN.prior_conv", "c.view.view.view", "pc.view.view.view", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "range", "range"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "# x = x.to(device)", "\n", "        ", "c", "=", "self", ".", "conv", "(", "x", ")", "\n", "pc", "=", "self", ".", "prior_conv", "(", "x", ")", "\n", "\n", "c", "=", "c", ".", "view", "(", "-", "1", ",", "self", ".", "fc_size", ")", "\n", "pc", "=", "pc", ".", "view", "(", "-", "1", ",", "self", ".", "fc_size", ")", "\n", "\n", "qs", "=", "[", "self", ".", "heads", "[", "i", "]", "(", "c", ")", "for", "i", "in", "range", "(", "self", ".", "bsp_k", ")", "]", "\n", "prior_qs", "=", "[", "self", ".", "prior_heads", "[", "i", "]", "(", "pc", ")", "for", "i", "in", "range", "(", "self", ".", "bsp_k", ")", "]", "\n", "\n", "torch_qs", "=", "torch", ".", "stack", "(", "qs", ",", "dim", "=", "2", ")", "\n", "torch_prior_qs", "=", "torch", ".", "stack", "(", "prior_qs", ",", "dim", "=", "2", ")", "\n", "\n", "return", "torch_qs", "+", "self", ".", "prior_beta", "*", "torch_prior_qs", "\n", "", "", ""]], "home.repos.pwc.inspect_result.oxwhirl_opiq.agent.atari_dqn.DQN.__init__": [[9, 36], ["torch.Module.__init__", "logging.getLogger", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Conv2d", "torch.Conv2d", "torch.ReLU", "torch.ReLU", "torch.Conv2d", "torch.Conv2d", "torch.ReLU", "torch.ReLU", "torch.Conv2d", "torch.Conv2d", "torch.ReLU", "torch.ReLU", "torch.Linear", "torch.Linear", "torch.ReLU", "torch.ReLU", "torch.Linear", "torch.Linear", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.Parameter", "torch.Parameter", "atari_dqn.DQN.feature_size", "range"], "methods", ["home.repos.pwc.inspect_result.oxwhirl_opiq.action.count_action.CountBonus.__init__", "home.repos.pwc.inspect_result.oxwhirl_opiq.rnd_net.atari_conv_net.Predictor_RND.feature_size"], ["    ", "def", "__init__", "(", "self", ",", "args", ")", ":", "\n", "        ", "super", "(", "DQN", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "logger", "=", "logging", ".", "getLogger", "(", "\"Atari_DQN\"", ")", "\n", "self", ".", "input_frames", "=", "args", ".", "past_frames_input", "\n", "\n", "self", ".", "input_shape", "=", "(", "args", ".", "past_frames_input", ",", "*", "args", ".", "state_shape", "[", ":", "2", "]", ")", "\n", "self", ".", "num_actions", "=", "args", ".", "num_actions", "\n", "\n", "self", ".", "features", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Conv2d", "(", "self", ".", "input_shape", "[", "0", "]", ",", "32", ",", "kernel_size", "=", "8", ",", "stride", "=", "4", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Conv2d", "(", "32", ",", "64", ",", "kernel_size", "=", "4", ",", "stride", "=", "2", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Conv2d", "(", "64", ",", "64", ",", "kernel_size", "=", "3", ",", "stride", "=", "1", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", "\n", ")", "\n", "\n", "self", ".", "fc", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Linear", "(", "self", ".", "feature_size", "(", ")", ",", "512", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Linear", "(", "512", ",", "self", ".", "num_actions", ")", "\n", ")", "\n", "\n", "if", "args", ".", "final_layer_bias", ">", "-", "1", ":", "\n", "            ", "optimistic_bias", "=", "torch", ".", "tensor", "(", "[", "args", ".", "final_layer_bias", "for", "_", "in", "range", "(", "args", ".", "num_actions", ")", "]", ",", "dtype", "=", "torch", ".", "float", ")", "\n", "self", ".", "fc", "[", "-", "1", "]", ".", "bias", "=", "nn", ".", "Parameter", "(", "optimistic_bias", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.agent.atari_dqn.DQN.forward": [[37, 42], ["atari_dqn.DQN.features", "x.view.view.view", "atari_dqn.DQN.fc", "x.view.view.size"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "features", "(", "x", ")", "\n", "x", "=", "x", ".", "view", "(", "x", ".", "size", "(", "0", ")", ",", "-", "1", ")", "\n", "q", "=", "self", ".", "fc", "(", "x", ")", "\n", "return", "q", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.agent.atari_dqn.DQN.feature_size": [[43, 45], ["atari_dqn.DQN.features().view().size", "atari_dqn.DQN.features().view", "atari_dqn.DQN.features", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros"], "methods", ["None"], ["", "def", "feature_size", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "features", "(", "torch", ".", "zeros", "(", "1", ",", "*", "self", ".", "input_shape", ")", ")", ".", "view", "(", "1", ",", "-", "1", ")", ".", "size", "(", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.agent.conv_dqn_bigger.DQN.__init__": [[11, 64], ["torch.Module.__init__", "logging.getLogger", "conv_dqn_bigger.DQN.logger.critical", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "range", "conv_dqn_bigger.DQN.logger.critical", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "conv_dqn_bigger.DQN.logger.critical", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "int", "conv_dqn_bigger.DQN.logger.critical", "int", "conv_dqn_bigger.DQN.logger.critical", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.Parameter", "torch.Parameter", "torch.Parameter", "range"], "methods", ["home.repos.pwc.inspect_result.oxwhirl_opiq.action.count_action.CountBonus.__init__"], ["    ", "def", "__init__", "(", "self", ",", "args", ")", ":", "\n", "        ", "super", "(", "DQN", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "logger", "=", "logging", ".", "getLogger", "(", "\"Maze_DQN\"", ")", "\n", "self", ".", "input_frames", "=", "args", ".", "past_frames_input", "\n", "\n", "maze_size", "=", "args", ".", "state_shape", "[", "0", "]", "\n", "\n", "image_size", "=", "maze_size", "\n", "self", ".", "logger", ".", "critical", "(", "\n", "\"Maze image input: ({1},{0},{0})\"", ".", "format", "(", "image_size", ",", "self", ".", "input_frames", ")", "\n", ")", "\n", "\n", "stride", "=", "2", "\n", "stride_third", "=", "1", "\n", "if", "maze_size", "<=", "9", ":", "\n", "            ", "stride", "=", "1", "\n", "", "if", "maze_size", ">", "40", ":", "\n", "            ", "stride_third", "=", "2", "\n", "\n", "", "channels", "=", "32", "\n", "# if image_size < 25:", "\n", "#     channels = 16", "\n", "\n", "convs", "=", "2", "\n", "if", "image_size", ">", "26", ":", "\n", "            ", "convs", "=", "3", "\n", "", "self", ".", "convs", "=", "convs", "\n", "\n", "self", ".", "conv1", "=", "nn", ".", "Conv2d", "(", "self", ".", "input_frames", ",", "channels", ",", "3", ",", "stride", "=", "stride", ")", "\n", "self", ".", "conv2", "=", "nn", ".", "Conv2d", "(", "channels", ",", "channels", ",", "3", ",", "stride", "=", "stride", ")", "\n", "if", "convs", ">", "2", ":", "\n", "            ", "self", ".", "conv3", "=", "nn", ".", "Conv2d", "(", "channels", ",", "channels", ",", "3", ",", "stride", "=", "stride_third", ")", "\n", "\n", "", "for", "_", "in", "range", "(", "2", ")", ":", "\n", "            ", "image_size", "=", "int", "(", "(", "image_size", "+", "2", "*", "0", "-", "3", ")", "/", "stride", "+", "1", ")", "\n", "self", ".", "logger", ".", "critical", "(", "\"After Conv: ({},{},{})\"", ".", "format", "(", "channels", ",", "image_size", ",", "image_size", ")", ")", "\n", "\n", "", "if", "convs", ">", "2", ":", "\n", "            ", "image_size", "=", "int", "(", "(", "image_size", "+", "2", "*", "0", "-", "3", ")", "/", "stride_third", "+", "1", ")", "\n", "self", ".", "logger", ".", "critical", "(", "\"After Conv: ({},{},{})\"", ".", "format", "(", "channels", ",", "image_size", ",", "image_size", ")", ")", "\n", "\n", "", "self", ".", "fc_size", "=", "image_size", "*", "image_size", "*", "channels", "\n", "self", ".", "fc_size_half", "=", "image_size", "*", "image_size", "*", "(", "channels", "//", "2", ")", "\n", "self", ".", "logger", ".", "critical", "(", "\"FC {} -> {}\"", ".", "format", "(", "self", ".", "fc_size", ",", "self", ".", "fc_size_half", ")", ")", "\n", "self", ".", "fc1", "=", "nn", ".", "Linear", "(", "self", ".", "fc_size", ",", "self", ".", "fc_size_half", ")", "\n", "\n", "self", ".", "qvals", "=", "nn", ".", "Linear", "(", "self", ".", "fc_size_half", ",", "args", ".", "num_actions", ")", "\n", "self", ".", "logger", ".", "critical", "(", "\"FC {} -> {} (Q-Values)\"", ".", "format", "(", "self", ".", "fc_size_half", ",", "args", ".", "num_actions", ")", ")", "\n", "\n", "if", "args", ".", "final_layer_bias", ">", "-", "1", ":", "\n", "            ", "optimistic_bias", "=", "torch", ".", "tensor", "(", "[", "args", ".", "final_layer_bias", "for", "_", "in", "range", "(", "args", ".", "num_actions", ")", "]", ",", "dtype", "=", "torch", ".", "float", ")", "\n", "self", ".", "qvals", ".", "bias", "=", "nn", ".", "Parameter", "(", "optimistic_bias", ")", "\n", "# self.to(device)", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.agent.conv_dqn_bigger.DQN.forward": [[66, 82], ["torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.relu.view", "torch.relu", "torch.relu", "torch.relu", "conv_dqn_bigger.DQN.qvals", "conv_dqn_bigger.DQN.conv1", "conv_dqn_bigger.DQN.conv2", "torch.relu", "torch.relu", "torch.relu", "conv_dqn_bigger.DQN.fc1", "conv_dqn_bigger.DQN.conv3"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "# x = x.to(device)", "\n", "\n", "        ", "x", "=", "F", ".", "relu", "(", "self", ".", "conv1", "(", "x", ")", ")", "\n", "x", "=", "F", ".", "relu", "(", "self", ".", "conv2", "(", "x", ")", ")", "\n", "if", "self", ".", "convs", ">", "2", ":", "\n", "            ", "x", "=", "F", ".", "relu", "(", "self", ".", "conv3", "(", "x", ")", ")", "\n", "\n", "# Flatten", "\n", "", "x", "=", "x", ".", "view", "(", "-", "1", ",", "self", ".", "fc_size", ")", "\n", "\n", "x", "=", "F", ".", "relu", "(", "self", ".", "fc1", "(", "x", ")", ")", "\n", "\n", "q", "=", "self", ".", "qvals", "(", "x", ")", "\n", "\n", "return", "q", "\n", "", "", ""]], "home.repos.pwc.inspect_result.oxwhirl_opiq.agent.tiny_dqn.DQN.__init__": [[9, 17], ["torch.Module.__init__", "torch.Sequential", "torch.Sequential", "torch.Linear", "torch.Linear", "torch.Tanh", "torch.Tanh", "torch.Linear", "torch.Linear"], "methods", ["home.repos.pwc.inspect_result.oxwhirl_opiq.action.count_action.CountBonus.__init__"], ["    ", "def", "__init__", "(", "self", ",", "args", ")", ":", "\n", "        ", "super", "(", "DQN", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "input_dim", "=", "args", ".", "state_shape", "[", "0", "]", "\n", "num_actions", "=", "args", ".", "num_actions", "\n", "\n", "self", ".", "model", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Linear", "(", "input_dim", ",", "32", ")", ",", "nn", ".", "Tanh", "(", ")", ",", "nn", ".", "Linear", "(", "32", ",", "num_actions", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.agent.tiny_dqn.DQN.forward": [[20, 24], ["tiny_dqn.DQN.model"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "# model_input = x.to(device)", "\n", "\n", "        ", "return", "self", ".", "model", "(", "x", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.oxwhirl_opiq.agent.specifier.get_model": [[25, 27], ["None"], "function", ["None"], ["def", "get_model", "(", "name", ")", ":", "\n", "    ", "return", "names_to_models", "[", "name", "]", "\n", "", ""]], "home.repos.pwc.inspect_result.oxwhirl_opiq.agent.fc_dqn.DQN.__init__": [[9, 22], ["torch.Module.__init__", "torch.Sequential", "torch.Sequential", "torch.Linear", "torch.Linear", "torch.ReLU", "torch.ReLU", "torch.Linear", "torch.Linear", "torch.ReLU", "torch.ReLU", "torch.Linear", "torch.Linear", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.Parameter", "torch.Parameter", "range"], "methods", ["home.repos.pwc.inspect_result.oxwhirl_opiq.action.count_action.CountBonus.__init__"], ["    ", "def", "__init__", "(", "self", ",", "args", ")", ":", "\n", "        ", "super", "(", "DQN", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "input_dim", "=", "args", ".", "state_shape", "[", "0", "]", "\n", "num_actions", "=", "args", ".", "num_actions", "\n", "\n", "self", ".", "model", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Linear", "(", "input_dim", ",", "256", ")", ",", "nn", ".", "ReLU", "(", ")", ",", "nn", ".", "Linear", "(", "256", ",", "256", ")", ",", "nn", ".", "ReLU", "(", ")", ",", "nn", ".", "Linear", "(", "256", ",", "num_actions", ")", "\n", ")", "\n", "\n", "if", "args", ".", "final_layer_bias", ">", "-", "1", ":", "\n", "            ", "optimistic_bias", "=", "torch", ".", "tensor", "(", "[", "args", ".", "final_layer_bias", "for", "_", "in", "range", "(", "num_actions", ")", "]", ",", "dtype", "=", "torch", ".", "float", ")", "\n", "self", ".", "model", "[", "-", "1", "]", ".", "bias", "=", "nn", ".", "Parameter", "(", "optimistic_bias", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.agent.fc_dqn.DQN.forward": [[24, 28], ["fc_dqn.DQN.model"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "# model_input = x.to(device)", "\n", "\n", "        ", "return", "self", ".", "model", "(", "x", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.oxwhirl_opiq.agent.bsp_maze_conv.BSPDQN.__init__": [[11, 51], ["torch.Module.__init__", "logging.getLogger", "bsp_maze_conv.BSPDQN.logger.critical", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "range", "torch.Sequential", "torch.Sequential", "torch.Sequential", "bsp_maze_conv.BSPDQN.conv.parameters", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "int", "bsp_maze_conv.BSPDQN.logger.critical", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Linear", "torch.Linear", "torch.Linear", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.Linear", "torch.Linear", "torch.Linear", "range", "torch.Linear", "torch.Linear", "torch.Linear", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.Linear", "torch.Linear", "torch.Linear", "range"], "methods", ["home.repos.pwc.inspect_result.oxwhirl_opiq.action.count_action.CountBonus.__init__"], ["    ", "def", "__init__", "(", "self", ",", "args", ")", ":", "\n", "        ", "super", "(", "BSPDQN", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "logger", "=", "logging", ".", "getLogger", "(", "\"Maze_BSP_DQN\"", ")", "\n", "self", ".", "input_frames", "=", "args", ".", "past_frames_input", "\n", "\n", "maze_size", "=", "args", ".", "state_shape", "[", "0", "]", "\n", "\n", "image_size", "=", "maze_size", "\n", "self", ".", "logger", ".", "critical", "(", "\n", "\"Maze image input: ({1},{0},{0})\"", ".", "format", "(", "image_size", ",", "self", ".", "input_frames", ")", "\n", ")", "\n", "stride", "=", "2", "\n", "channels", "=", "16", "\n", "self", ".", "conv1", "=", "nn", ".", "Conv2d", "(", "self", ".", "input_frames", ",", "channels", ",", "3", ",", "stride", "=", "stride", ")", "\n", "self", ".", "conv2", "=", "nn", ".", "Conv2d", "(", "channels", ",", "channels", ",", "3", ",", "stride", "=", "stride", ")", "\n", "\n", "for", "_", "in", "range", "(", "2", ")", ":", "\n", "            ", "image_size", "=", "int", "(", "(", "image_size", "+", "2", "*", "0", "-", "3", ")", "/", "stride", "+", "1", ")", "\n", "self", ".", "logger", ".", "critical", "(", "\"After Conv: ({},{},{})\"", ".", "format", "(", "channels", ",", "image_size", ",", "image_size", ")", ")", "\n", "\n", "", "self", ".", "fc_size", "=", "400", "\n", "\n", "self", ".", "bsp_k", "=", "args", ".", "bsp_k", "\n", "self", ".", "conv", "=", "nn", ".", "Sequential", "(", "nn", ".", "Conv2d", "(", "self", ".", "input_frames", ",", "channels", ",", "3", ",", "stride", "=", "stride", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Conv2d", "(", "channels", ",", "channels", ",", "3", ",", "stride", "=", "stride", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ")", "\n", "self", ".", "conv_params", "=", "self", ".", "conv", ".", "parameters", "(", ")", "\n", "self", ".", "heads", "=", "nn", ".", "ModuleList", "(", "[", "nn", ".", "Sequential", "(", "nn", ".", "Linear", "(", "400", ",", "200", ")", ",", "nn", ".", "ReLU", "(", ")", ",", "nn", ".", "Linear", "(", "200", ",", "args", ".", "num_actions", ")", ")", "for", "_", "in", "range", "(", "self", ".", "bsp_k", ")", "]", ")", "\n", "\n", "self", ".", "prior_conv", "=", "nn", ".", "Sequential", "(", "nn", ".", "Conv2d", "(", "self", ".", "input_frames", ",", "channels", ",", "3", ",", "stride", "=", "stride", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Conv2d", "(", "channels", ",", "channels", ",", "3", ",", "stride", "=", "stride", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ")", "\n", "self", ".", "prior_heads", "=", "nn", ".", "ModuleList", "(", "[", "nn", ".", "Sequential", "(", "nn", ".", "Linear", "(", "400", ",", "200", ")", ",", "nn", ".", "ReLU", "(", ")", ",", "nn", ".", "Linear", "(", "200", ",", "args", ".", "num_actions", ")", ")", "for", "_", "in", "range", "(", "self", ".", "bsp_k", ")", "]", ")", "\n", "self", ".", "prior_conv", ".", "requires_grad", "=", "False", "\n", "self", ".", "prior_heads", ".", "requires_grad", "=", "False", "\n", "\n", "self", ".", "prior_beta", "=", "args", ".", "bsp_beta", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.agent.bsp_maze_conv.BSPDQN.scale_gradients": [[53, 56], ["None"], "methods", ["None"], ["", "def", "scale_gradients", "(", "self", ")", ":", "\n", "        ", "for", "c", "in", "self", ".", "conv_params", ":", "\n", "            ", "c", ".", "grad", "*=", "1", "/", "self", ".", "bsp_k", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.agent.bsp_maze_conv.BSPDQN.forward": [[57, 72], ["bsp_maze_conv.BSPDQN.conv", "bsp_maze_conv.BSPDQN.prior_conv", "c.view.view.view", "pc.view.view.view", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "range", "range"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "# x = x.to(device)", "\n", "        ", "c", "=", "self", ".", "conv", "(", "x", ")", "\n", "pc", "=", "self", ".", "prior_conv", "(", "x", ")", "\n", "\n", "c", "=", "c", ".", "view", "(", "-", "1", ",", "self", ".", "fc_size", ")", "\n", "pc", "=", "pc", ".", "view", "(", "-", "1", ",", "self", ".", "fc_size", ")", "\n", "\n", "qs", "=", "[", "self", ".", "heads", "[", "i", "]", "(", "c", ")", "for", "i", "in", "range", "(", "self", ".", "bsp_k", ")", "]", "\n", "prior_qs", "=", "[", "self", ".", "prior_heads", "[", "i", "]", "(", "pc", ")", "for", "i", "in", "range", "(", "self", ".", "bsp_k", ")", "]", "\n", "\n", "torch_qs", "=", "torch", ".", "stack", "(", "qs", ",", "dim", "=", "2", ")", "\n", "torch_prior_qs", "=", "torch", ".", "stack", "(", "prior_qs", ",", "dim", "=", "2", ")", "\n", "\n", "return", "torch_qs", "+", "self", ".", "prior_beta", "*", "torch_prior_qs", "\n", "", "", ""]], "home.repos.pwc.inspect_result.oxwhirl_opiq.agent.maze_conv.DQN.__init__": [[11, 44], ["torch.Module.__init__", "logging.getLogger", "maze_conv.DQN.logger.critical", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "range", "maze_conv.DQN.logger.critical", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "maze_conv.DQN.logger.critical", "int", "maze_conv.DQN.logger.critical", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.Parameter", "torch.Parameter", "torch.Parameter", "range"], "methods", ["home.repos.pwc.inspect_result.oxwhirl_opiq.action.count_action.CountBonus.__init__"], ["    ", "def", "__init__", "(", "self", ",", "args", ")", ":", "\n", "        ", "super", "(", "DQN", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "logger", "=", "logging", ".", "getLogger", "(", "\"Maze_DQN\"", ")", "\n", "self", ".", "input_frames", "=", "args", ".", "past_frames_input", "\n", "\n", "maze_size", "=", "args", ".", "state_shape", "[", "0", "]", "\n", "\n", "image_size", "=", "maze_size", "\n", "self", ".", "logger", ".", "critical", "(", "\n", "\"Maze image input: ({1},{0},{0})\"", ".", "format", "(", "image_size", ",", "self", ".", "input_frames", ")", "\n", ")", "\n", "stride", "=", "2", "\n", "channels", "=", "16", "\n", "self", ".", "conv1", "=", "nn", ".", "Conv2d", "(", "self", ".", "input_frames", ",", "channels", ",", "3", ",", "stride", "=", "stride", ")", "\n", "self", ".", "conv2", "=", "nn", ".", "Conv2d", "(", "channels", ",", "channels", ",", "3", ",", "stride", "=", "stride", ")", "\n", "\n", "for", "_", "in", "range", "(", "2", ")", ":", "\n", "            ", "image_size", "=", "int", "(", "(", "image_size", "+", "2", "*", "0", "-", "3", ")", "/", "stride", "+", "1", ")", "\n", "self", ".", "logger", ".", "critical", "(", "\"After Conv: ({},{},{})\"", ".", "format", "(", "channels", ",", "image_size", ",", "image_size", ")", ")", "\n", "\n", "", "self", ".", "fc_size", "=", "image_size", "*", "image_size", "*", "channels", "\n", "assert", "self", ".", "fc_size", "==", "400", "\n", "self", ".", "fc_size_half", "=", "image_size", "*", "image_size", "*", "(", "channels", "//", "2", ")", "\n", "self", ".", "logger", ".", "critical", "(", "\"FC {} -> {}\"", ".", "format", "(", "self", ".", "fc_size", ",", "self", ".", "fc_size_half", ")", ")", "\n", "self", ".", "fc1", "=", "nn", ".", "Linear", "(", "self", ".", "fc_size", ",", "self", ".", "fc_size_half", ")", "\n", "\n", "self", ".", "qvals", "=", "nn", ".", "Linear", "(", "self", ".", "fc_size_half", ",", "args", ".", "num_actions", ")", "\n", "self", ".", "logger", ".", "critical", "(", "\"FC {} -> {} (Q-Values)\"", ".", "format", "(", "self", ".", "fc_size_half", ",", "args", ".", "num_actions", ")", ")", "\n", "\n", "if", "args", ".", "final_layer_bias", ">", "-", "1", ":", "\n", "            ", "optimistic_bias", "=", "torch", ".", "tensor", "(", "[", "args", ".", "final_layer_bias", "for", "_", "in", "range", "(", "args", ".", "num_actions", ")", "]", ",", "dtype", "=", "torch", ".", "float", ")", "\n", "self", ".", "qvals", ".", "bias", "=", "nn", ".", "Parameter", "(", "optimistic_bias", ")", "\n", "# self.to(device)", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.agent.maze_conv.DQN.forward": [[46, 58], ["torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.relu.view", "torch.relu", "torch.relu", "torch.relu", "maze_conv.DQN.qvals", "maze_conv.DQN.conv1", "maze_conv.DQN.conv2", "maze_conv.DQN.fc1"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "# x = x.to(device)", "\n", "\n", "        ", "x", "=", "F", ".", "relu", "(", "self", ".", "conv1", "(", "x", ")", ")", "\n", "x", "=", "F", ".", "relu", "(", "self", ".", "conv2", "(", "x", ")", ")", "\n", "\n", "# Flatten", "\n", "x", "=", "x", ".", "view", "(", "-", "1", ",", "self", ".", "fc_size", ")", "\n", "x", "=", "F", ".", "relu", "(", "self", ".", "fc1", "(", "x", ")", ")", "\n", "q", "=", "self", ".", "qvals", "(", "x", ")", "\n", "\n", "return", "q", "\n", "", "", ""]], "home.repos.pwc.inspect_result.oxwhirl_opiq.rnd_net.bigger_conv_net.Target_RND.__init__": [[10, 29], ["torch.Module.__init__", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "range", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "int"], "methods", ["home.repos.pwc.inspect_result.oxwhirl_opiq.action.count_action.CountBonus.__init__"], ["    ", "def", "__init__", "(", "self", ",", "args", ")", ":", "\n", "        ", "super", "(", "Target_RND", ",", "self", ")", ".", "__init__", "(", ")", "\n", "maze_size", "=", "args", ".", "state_shape", "[", "0", "]", "\n", "\n", "image_size", "=", "maze_size", "\n", "stride", "=", "2", "\n", "channels", "=", "16", "\n", "self", ".", "conv1", "=", "nn", ".", "Conv2d", "(", "1", ",", "channels", ",", "3", ",", "stride", "=", "stride", ")", "\n", "self", ".", "conv2", "=", "nn", ".", "Conv2d", "(", "channels", ",", "channels", ",", "3", ",", "stride", "=", "stride", ")", "\n", "\n", "for", "_", "in", "range", "(", "2", ")", ":", "\n", "            ", "image_size", "=", "int", "(", "(", "image_size", "+", "2", "*", "0", "-", "3", ")", "/", "stride", "+", "1", ")", "\n", "\n", "", "self", ".", "fc_size", "=", "image_size", "*", "image_size", "*", "channels", "\n", "assert", "self", ".", "fc_size", "==", "400", "\n", "self", ".", "fc_size_half", "=", "image_size", "*", "image_size", "*", "(", "channels", "//", "2", ")", "\n", "self", ".", "fc1", "=", "nn", ".", "Linear", "(", "self", ".", "fc_size", ",", "self", ".", "fc_size_half", ")", "\n", "\n", "self", ".", "out", "=", "nn", ".", "Linear", "(", "self", ".", "fc_size_half", ",", "args", ".", "rnd_rep_size", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.rnd_net.bigger_conv_net.Target_RND.forward": [[30, 38], ["torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.relu.view", "torch.relu", "torch.relu", "torch.relu", "bigger_conv_net.Target_RND.out", "bigger_conv_net.Target_RND.conv1", "bigger_conv_net.Target_RND.conv2", "bigger_conv_net.Target_RND.fc1"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "F", ".", "relu", "(", "self", ".", "conv1", "(", "x", ")", ")", "\n", "x", "=", "F", ".", "relu", "(", "self", ".", "conv2", "(", "x", ")", ")", "\n", "# Flatten", "\n", "x", "=", "x", ".", "view", "(", "-", "1", ",", "self", ".", "fc_size", ")", "\n", "x", "=", "F", ".", "relu", "(", "self", ".", "fc1", "(", "x", ")", ")", "\n", "out", "=", "self", ".", "out", "(", "x", ")", "\n", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.rnd_net.bigger_conv_net.Predictor_RND.__init__": [[42, 63], ["torch.Module.__init__", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "range", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "int"], "methods", ["home.repos.pwc.inspect_result.oxwhirl_opiq.action.count_action.CountBonus.__init__"], ["    ", "def", "__init__", "(", "self", ",", "args", ")", ":", "\n", "        ", "super", "(", "Predictor_RND", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "# This predictor is intentionally bigger than the target network", "\n", "maze_size", "=", "args", ".", "state_shape", "[", "0", "]", "\n", "\n", "image_size", "=", "maze_size", "\n", "stride", "=", "2", "\n", "channels", "=", "24", "\n", "self", ".", "conv1", "=", "nn", ".", "Conv2d", "(", "1", ",", "channels", ",", "3", ",", "stride", "=", "stride", ")", "\n", "self", ".", "conv2", "=", "nn", ".", "Conv2d", "(", "channels", ",", "channels", ",", "3", ",", "stride", "=", "stride", ")", "\n", "\n", "for", "_", "in", "range", "(", "2", ")", ":", "\n", "            ", "image_size", "=", "int", "(", "(", "image_size", "+", "2", "*", "0", "-", "3", ")", "/", "stride", "+", "1", ")", "\n", "\n", "", "self", ".", "fc_size", "=", "image_size", "*", "image_size", "*", "channels", "\n", "assert", "self", ".", "fc_size", "==", "600", "\n", "self", ".", "fc_size_half", "=", "image_size", "*", "image_size", "*", "(", "channels", "//", "2", ")", "\n", "self", ".", "fc1", "=", "nn", ".", "Linear", "(", "self", ".", "fc_size", ",", "self", ".", "fc_size_half", ")", "\n", "\n", "self", ".", "out", "=", "nn", ".", "Linear", "(", "self", ".", "fc_size_half", ",", "args", ".", "rnd_rep_size", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.rnd_net.bigger_conv_net.Predictor_RND.forward": [[64, 72], ["torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.relu.view", "torch.relu", "torch.relu", "torch.relu", "bigger_conv_net.Predictor_RND.out", "bigger_conv_net.Predictor_RND.conv1", "bigger_conv_net.Predictor_RND.conv2", "bigger_conv_net.Predictor_RND.fc1"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "F", ".", "relu", "(", "self", ".", "conv1", "(", "x", ")", ")", "\n", "x", "=", "F", ".", "relu", "(", "self", ".", "conv2", "(", "x", ")", ")", "\n", "# Flatten", "\n", "x", "=", "x", ".", "view", "(", "-", "1", ",", "self", ".", "fc_size", ")", "\n", "x", "=", "F", ".", "relu", "(", "self", ".", "fc1", "(", "x", ")", ")", "\n", "out", "=", "self", ".", "out", "(", "x", ")", "\n", "return", "out", "\n", "", "", ""]], "home.repos.pwc.inspect_result.oxwhirl_opiq.rnd_net.atari_conv_net.Target_RND.__init__": [[11, 27], ["torch.Module.__init__", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.Linear", "torch.Linear", "torch.Linear", "atari_conv_net.Target_RND.feature_size"], "methods", ["home.repos.pwc.inspect_result.oxwhirl_opiq.action.count_action.CountBonus.__init__", "home.repos.pwc.inspect_result.oxwhirl_opiq.rnd_net.atari_conv_net.Predictor_RND.feature_size"], ["    ", "def", "__init__", "(", "self", ",", "args", ")", ":", "\n", "        ", "super", "(", "Target_RND", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "input_shape", "=", "(", "1", ",", "*", "args", ".", "state_shape", "[", ":", "2", "]", ")", "\n", "\n", "self", ".", "features", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Conv2d", "(", "self", ".", "input_shape", "[", "0", "]", ",", "32", ",", "kernel_size", "=", "8", ",", "stride", "=", "4", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Conv2d", "(", "32", ",", "64", ",", "kernel_size", "=", "4", ",", "stride", "=", "2", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Conv2d", "(", "64", ",", "64", ",", "kernel_size", "=", "3", ",", "stride", "=", "1", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", "\n", ")", "\n", "\n", "self", ".", "fc", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Linear", "(", "self", ".", "feature_size", "(", ")", ",", "args", ".", "rnd_rep_size", ")", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.rnd_net.atari_conv_net.Target_RND.feature_size": [[29, 31], ["atari_conv_net.Target_RND.features().view().size", "atari_conv_net.Target_RND.features().view", "atari_conv_net.Target_RND.features", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros"], "methods", ["None"], ["", "def", "feature_size", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "features", "(", "torch", ".", "zeros", "(", "1", ",", "*", "self", ".", "input_shape", ")", ")", ".", "view", "(", "1", ",", "-", "1", ")", ".", "size", "(", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.rnd_net.atari_conv_net.Target_RND.forward": [[32, 37], ["atari_conv_net.Target_RND.features", "x.view.view.view", "atari_conv_net.Target_RND.fc", "x.view.view.size"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "features", "(", "x", ")", "\n", "x", "=", "x", ".", "view", "(", "x", ".", "size", "(", "0", ")", ",", "-", "1", ")", "\n", "out", "=", "self", ".", "fc", "(", "x", ")", "\n", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.rnd_net.atari_conv_net.Predictor_RND.__init__": [[41, 60], ["torch.Module.__init__", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.Linear", "torch.Linear", "torch.Linear", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.Linear", "torch.Linear", "torch.Linear", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.Linear", "torch.Linear", "torch.Linear", "atari_conv_net.Predictor_RND.feature_size"], "methods", ["home.repos.pwc.inspect_result.oxwhirl_opiq.action.count_action.CountBonus.__init__", "home.repos.pwc.inspect_result.oxwhirl_opiq.rnd_net.atari_conv_net.Predictor_RND.feature_size"], ["    ", "def", "__init__", "(", "self", ",", "args", ")", ":", "\n", "        ", "super", "(", "Predictor_RND", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "input_shape", "=", "(", "1", ",", "*", "args", ".", "state_shape", "[", ":", "2", "]", ")", "\n", "\n", "self", ".", "features", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Conv2d", "(", "self", ".", "input_shape", "[", "0", "]", ",", "32", ",", "kernel_size", "=", "8", ",", "stride", "=", "4", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Conv2d", "(", "32", ",", "64", ",", "kernel_size", "=", "4", ",", "stride", "=", "2", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Conv2d", "(", "64", ",", "64", ",", "kernel_size", "=", "3", ",", "stride", "=", "1", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", "\n", ")", "\n", "\n", "self", ".", "fc", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Linear", "(", "self", ".", "feature_size", "(", ")", ",", "512", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Linear", "(", "512", ",", "512", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Linear", "(", "512", ",", "args", ".", "rnd_rep_size", ")", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.rnd_net.atari_conv_net.Predictor_RND.feature_size": [[62, 64], ["atari_conv_net.Predictor_RND.features().view().size", "atari_conv_net.Predictor_RND.features().view", "atari_conv_net.Predictor_RND.features", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros"], "methods", ["None"], ["", "def", "feature_size", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "features", "(", "torch", ".", "zeros", "(", "1", ",", "*", "self", ".", "input_shape", ")", ")", ".", "view", "(", "1", ",", "-", "1", ")", ".", "size", "(", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.rnd_net.atari_conv_net.Predictor_RND.forward": [[65, 70], ["atari_conv_net.Predictor_RND.features", "x.view.view.view", "atari_conv_net.Predictor_RND.fc", "x.view.view.size"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "features", "(", "x", ")", "\n", "x", "=", "x", ".", "view", "(", "x", ".", "size", "(", "0", ")", ",", "-", "1", ")", "\n", "out", "=", "self", ".", "fc", "(", "x", ")", "\n", "return", "out", "\n", "", "", ""]], "home.repos.pwc.inspect_result.oxwhirl_opiq.rnd_net.conv_net.Target_RND.__init__": [[10, 29], ["torch.Module.__init__", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "range", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "int"], "methods", ["home.repos.pwc.inspect_result.oxwhirl_opiq.action.count_action.CountBonus.__init__"], ["    ", "def", "__init__", "(", "self", ",", "args", ")", ":", "\n", "        ", "super", "(", "Target_RND", ",", "self", ")", ".", "__init__", "(", ")", "\n", "maze_size", "=", "args", ".", "state_shape", "[", "0", "]", "\n", "\n", "image_size", "=", "maze_size", "\n", "stride", "=", "2", "\n", "channels", "=", "16", "\n", "self", ".", "conv1", "=", "nn", ".", "Conv2d", "(", "1", ",", "channels", ",", "3", ",", "stride", "=", "stride", ")", "\n", "self", ".", "conv2", "=", "nn", ".", "Conv2d", "(", "channels", ",", "channels", ",", "3", ",", "stride", "=", "stride", ")", "\n", "\n", "for", "_", "in", "range", "(", "2", ")", ":", "\n", "            ", "image_size", "=", "int", "(", "(", "image_size", "+", "2", "*", "0", "-", "3", ")", "/", "stride", "+", "1", ")", "\n", "\n", "", "self", ".", "fc_size", "=", "image_size", "*", "image_size", "*", "channels", "\n", "assert", "self", ".", "fc_size", "==", "400", "\n", "self", ".", "fc_size_half", "=", "image_size", "*", "image_size", "*", "(", "channels", "//", "2", ")", "\n", "self", ".", "fc1", "=", "nn", ".", "Linear", "(", "self", ".", "fc_size", ",", "self", ".", "fc_size_half", ")", "\n", "\n", "self", ".", "out", "=", "nn", ".", "Linear", "(", "self", ".", "fc_size_half", ",", "args", ".", "rnd_rep_size", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.rnd_net.conv_net.Target_RND.forward": [[30, 38], ["torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.relu.view", "torch.relu", "torch.relu", "torch.relu", "conv_net.Target_RND.out", "conv_net.Target_RND.conv1", "conv_net.Target_RND.conv2", "conv_net.Target_RND.fc1"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "F", ".", "relu", "(", "self", ".", "conv1", "(", "x", ")", ")", "\n", "x", "=", "F", ".", "relu", "(", "self", ".", "conv2", "(", "x", ")", ")", "\n", "# Flatten", "\n", "x", "=", "x", ".", "view", "(", "-", "1", ",", "self", ".", "fc_size", ")", "\n", "x", "=", "F", ".", "relu", "(", "self", ".", "fc1", "(", "x", ")", ")", "\n", "out", "=", "self", ".", "out", "(", "x", ")", "\n", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.rnd_net.conv_net.Predictor_RND.__init__": [[42, 61], ["torch.Module.__init__", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "range", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "int"], "methods", ["home.repos.pwc.inspect_result.oxwhirl_opiq.action.count_action.CountBonus.__init__"], ["    ", "def", "__init__", "(", "self", ",", "args", ")", ":", "\n", "        ", "super", "(", "Predictor_RND", ",", "self", ")", ".", "__init__", "(", ")", "\n", "maze_size", "=", "args", ".", "state_shape", "[", "0", "]", "\n", "\n", "image_size", "=", "maze_size", "\n", "stride", "=", "2", "\n", "channels", "=", "16", "\n", "self", ".", "conv1", "=", "nn", ".", "Conv2d", "(", "1", ",", "channels", ",", "3", ",", "stride", "=", "stride", ")", "\n", "self", ".", "conv2", "=", "nn", ".", "Conv2d", "(", "channels", ",", "channels", ",", "3", ",", "stride", "=", "stride", ")", "\n", "\n", "for", "_", "in", "range", "(", "2", ")", ":", "\n", "            ", "image_size", "=", "int", "(", "(", "image_size", "+", "2", "*", "0", "-", "3", ")", "/", "stride", "+", "1", ")", "\n", "\n", "", "self", ".", "fc_size", "=", "image_size", "*", "image_size", "*", "channels", "\n", "assert", "self", ".", "fc_size", "==", "400", "\n", "self", ".", "fc_size_half", "=", "image_size", "*", "image_size", "*", "(", "channels", "//", "2", ")", "\n", "self", ".", "fc1", "=", "nn", ".", "Linear", "(", "self", ".", "fc_size", ",", "self", ".", "fc_size_half", ")", "\n", "\n", "self", ".", "out", "=", "nn", ".", "Linear", "(", "self", ".", "fc_size_half", ",", "args", ".", "rnd_rep_size", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.rnd_net.conv_net.Predictor_RND.forward": [[62, 70], ["torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.relu.view", "torch.relu", "torch.relu", "torch.relu", "conv_net.Predictor_RND.out", "conv_net.Predictor_RND.conv1", "conv_net.Predictor_RND.conv2", "conv_net.Predictor_RND.fc1"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "F", ".", "relu", "(", "self", ".", "conv1", "(", "x", ")", ")", "\n", "x", "=", "F", ".", "relu", "(", "self", ".", "conv2", "(", "x", ")", ")", "\n", "# Flatten", "\n", "x", "=", "x", ".", "view", "(", "-", "1", ",", "self", ".", "fc_size", ")", "\n", "x", "=", "F", ".", "relu", "(", "self", ".", "fc1", "(", "x", ")", ")", "\n", "out", "=", "self", ".", "out", "(", "x", ")", "\n", "return", "out", "\n", "", "", ""]], "home.repos.pwc.inspect_result.oxwhirl_opiq.rnd_net.specifier.get_target": [[16, 25], ["None"], "function", ["None"], ["\"2conv\"", ":", "conv_dqn", ",", "\n", "\"2convbigger\"", ":", "conv_dqn_bigger", ",", "\n", "\"atari\"", ":", "atari_dqn", ",", "\n", "\"maze\"", ":", "maze_dqn", ",", "\n", "\"maze_bsp\"", ":", "maze_bsp_dqn", ",", "\n", "\"atari_bsp\"", ":", "atari_bsp_dqn", ",", "\n", "}", "\n", "\n", "\n", "def", "get_model", "(", "name", ")", ":", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.rnd_net.specifier.get_pred": [[26, 35], ["None"], "function", ["None"], ["    ", "return", "names_to_models", "[", "name", "]", "\n", "", ""]], "home.repos.pwc.inspect_result.oxwhirl_opiq.rnd_net.smaller_conv_net.Target_RND.__init__": [[10, 29], ["torch.Module.__init__", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "range", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "int"], "methods", ["home.repos.pwc.inspect_result.oxwhirl_opiq.action.count_action.CountBonus.__init__"], ["    ", "def", "__init__", "(", "self", ",", "args", ")", ":", "\n", "        ", "super", "(", "Target_RND", ",", "self", ")", ".", "__init__", "(", ")", "\n", "maze_size", "=", "args", ".", "state_shape", "[", "0", "]", "\n", "\n", "image_size", "=", "maze_size", "\n", "stride", "=", "2", "\n", "channels", "=", "16", "\n", "self", ".", "conv1", "=", "nn", ".", "Conv2d", "(", "1", ",", "channels", ",", "3", ",", "stride", "=", "stride", ")", "\n", "self", ".", "conv2", "=", "nn", ".", "Conv2d", "(", "channels", ",", "channels", ",", "3", ",", "stride", "=", "stride", ")", "\n", "\n", "for", "_", "in", "range", "(", "2", ")", ":", "\n", "            ", "image_size", "=", "int", "(", "(", "image_size", "+", "2", "*", "0", "-", "3", ")", "/", "stride", "+", "1", ")", "\n", "\n", "", "self", ".", "fc_size", "=", "image_size", "*", "image_size", "*", "channels", "\n", "assert", "self", ".", "fc_size", "==", "400", "\n", "self", ".", "fc_size_half", "=", "image_size", "*", "image_size", "*", "(", "channels", "//", "2", ")", "\n", "self", ".", "fc1", "=", "nn", ".", "Linear", "(", "self", ".", "fc_size", ",", "self", ".", "fc_size_half", ")", "\n", "\n", "self", ".", "out", "=", "nn", ".", "Linear", "(", "self", ".", "fc_size_half", ",", "args", ".", "rnd_rep_size", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.rnd_net.smaller_conv_net.Target_RND.forward": [[30, 38], ["torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.relu.view", "torch.relu", "torch.relu", "torch.relu", "smaller_conv_net.Target_RND.out", "smaller_conv_net.Target_RND.conv1", "smaller_conv_net.Target_RND.conv2", "smaller_conv_net.Target_RND.fc1"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "F", ".", "relu", "(", "self", ".", "conv1", "(", "x", ")", ")", "\n", "x", "=", "F", ".", "relu", "(", "self", ".", "conv2", "(", "x", ")", ")", "\n", "# Flatten", "\n", "x", "=", "x", ".", "view", "(", "-", "1", ",", "self", ".", "fc_size", ")", "\n", "x", "=", "F", ".", "relu", "(", "self", ".", "fc1", "(", "x", ")", ")", "\n", "out", "=", "self", ".", "out", "(", "x", ")", "\n", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.rnd_net.smaller_conv_net.Predictor_RND.__init__": [[42, 64], ["torch.Module.__init__", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "range", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "int"], "methods", ["home.repos.pwc.inspect_result.oxwhirl_opiq.action.count_action.CountBonus.__init__"], ["    ", "def", "__init__", "(", "self", ",", "args", ")", ":", "\n", "        ", "super", "(", "Predictor_RND", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "# This predictor is intentionally smaller/differ than the target network", "\n", "\n", "maze_size", "=", "args", ".", "state_shape", "[", "0", "]", "\n", "\n", "image_size", "=", "maze_size", "\n", "stride", "=", "2", "\n", "channels", "=", "8", "\n", "self", ".", "conv1", "=", "nn", ".", "Conv2d", "(", "1", ",", "channels", ",", "3", ",", "stride", "=", "stride", ")", "\n", "self", ".", "conv2", "=", "nn", ".", "Conv2d", "(", "channels", ",", "channels", ",", "3", ",", "stride", "=", "stride", ")", "\n", "\n", "for", "_", "in", "range", "(", "2", ")", ":", "\n", "            ", "image_size", "=", "int", "(", "(", "image_size", "+", "2", "*", "0", "-", "3", ")", "/", "stride", "+", "1", ")", "\n", "\n", "", "self", ".", "fc_size", "=", "image_size", "*", "image_size", "*", "channels", "\n", "assert", "self", ".", "fc_size", "==", "200", "\n", "self", ".", "fc_size_half", "=", "image_size", "*", "image_size", "*", "(", "channels", "//", "2", ")", "\n", "self", ".", "fc1", "=", "nn", ".", "Linear", "(", "self", ".", "fc_size", ",", "self", ".", "fc_size_half", ")", "\n", "\n", "self", ".", "out", "=", "nn", ".", "Linear", "(", "self", ".", "fc_size_half", ",", "args", ".", "rnd_rep_size", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.rnd_net.smaller_conv_net.Predictor_RND.forward": [[65, 73], ["torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.relu.view", "torch.relu", "torch.relu", "torch.relu", "smaller_conv_net.Predictor_RND.out", "smaller_conv_net.Predictor_RND.conv1", "smaller_conv_net.Predictor_RND.conv2", "smaller_conv_net.Predictor_RND.fc1"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "F", ".", "relu", "(", "self", ".", "conv1", "(", "x", ")", ")", "\n", "x", "=", "F", ".", "relu", "(", "self", ".", "conv2", "(", "x", ")", ")", "\n", "# Flatten", "\n", "x", "=", "x", ".", "view", "(", "-", "1", ",", "self", ".", "fc_size", ")", "\n", "x", "=", "F", ".", "relu", "(", "self", ".", "fc1", "(", "x", ")", ")", "\n", "out", "=", "self", ".", "out", "(", "x", ")", "\n", "return", "out", "\n", "", "", ""]], "home.repos.pwc.inspect_result.oxwhirl_opiq.rnd_net.fc_net.Target_RND.__init__": [[9, 18], ["torch.Module.__init__", "torch.Sequential", "torch.Sequential", "torch.Linear", "torch.Linear", "torch.ReLU", "torch.ReLU", "torch.Linear", "torch.Linear"], "methods", ["home.repos.pwc.inspect_result.oxwhirl_opiq.action.count_action.CountBonus.__init__"], ["    ", "def", "__init__", "(", "self", ",", "args", ")", ":", "\n", "        ", "super", "(", "Target_RND", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "input_dim", "=", "args", ".", "state_shape", "[", "0", "]", "\n", "rep_size", "=", "args", ".", "rnd_rep_size", "\n", "\n", "self", ".", "model", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Linear", "(", "input_dim", ",", "256", ")", ",", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Linear", "(", "256", ",", "rep_size", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.rnd_net.fc_net.Target_RND.forward": [[21, 23], ["fc_net.Target_RND.model"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "return", "self", ".", "model", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.rnd_net.fc_net.Predictor_RND.__init__": [[27, 36], ["torch.Module.__init__", "torch.Sequential", "torch.Sequential", "torch.Linear", "torch.Linear", "torch.ReLU", "torch.ReLU", "torch.Linear", "torch.Linear"], "methods", ["home.repos.pwc.inspect_result.oxwhirl_opiq.action.count_action.CountBonus.__init__"], ["    ", "def", "__init__", "(", "self", ",", "args", ")", ":", "\n", "        ", "super", "(", "Predictor_RND", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "input_dim", "=", "args", ".", "state_shape", "[", "0", "]", "\n", "rep_size", "=", "args", ".", "rnd_rep_size", "\n", "\n", "self", ".", "model", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Linear", "(", "input_dim", ",", "256", ")", ",", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Linear", "(", "256", ",", "rep_size", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.rnd_net.fc_net.Predictor_RND.forward": [[38, 40], ["fc_net.Predictor_RND.model"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "return", "self", ".", "model", "(", "x", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.oxwhirl_opiq.dora.specifier.get_net": [[3, 8], ["None"], "function", ["None"], ["from", ".", "fc_dqn", "import", "DQN", "as", "fc_dqn", "\n", "from", ".", "atari_dqn", "import", "DQN", "as", "atari_dqn", "\n", "from", ".", "conv_dqn_bigger", "import", "DQN", "as", "conv_dqn_bigger", "\n", "from", ".", "maze_conv", "import", "DQN", "as", "maze_dqn", "\n", "from", ".", "fc_atari_dqn", "import", "DQN", "as", "fc_atari_dqn", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.dora.maze_conv.DQN.__init__": [[11, 42], ["torch.Module.__init__", "logging.getLogger", "maze_conv.DQN.logger.critical", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "range", "maze_conv.DQN.logger.critical", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "maze_conv.DQN.logger.critical", "maze_conv.DQN.qvals.weight.data.fill_", "int", "maze_conv.DQN.logger.critical"], "methods", ["home.repos.pwc.inspect_result.oxwhirl_opiq.action.count_action.CountBonus.__init__"], ["    ", "def", "__init__", "(", "self", ",", "args", ")", ":", "\n", "        ", "super", "(", "DQN", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "logger", "=", "logging", ".", "getLogger", "(", "\"Maze_DQN\"", ")", "\n", "self", ".", "input_frames", "=", "args", ".", "past_frames_input", "\n", "\n", "maze_size", "=", "args", ".", "state_shape", "[", "0", "]", "\n", "\n", "image_size", "=", "maze_size", "\n", "self", ".", "logger", ".", "critical", "(", "\n", "\"Maze image input: ({1},{0},{0})\"", ".", "format", "(", "image_size", ",", "self", ".", "input_frames", ")", "\n", ")", "\n", "stride", "=", "2", "\n", "channels", "=", "16", "\n", "self", ".", "conv1", "=", "nn", ".", "Conv2d", "(", "self", ".", "input_frames", ",", "channels", ",", "3", ",", "stride", "=", "stride", ")", "\n", "self", ".", "conv2", "=", "nn", ".", "Conv2d", "(", "channels", ",", "channels", ",", "3", ",", "stride", "=", "stride", ")", "\n", "\n", "for", "_", "in", "range", "(", "2", ")", ":", "\n", "            ", "image_size", "=", "int", "(", "(", "image_size", "+", "2", "*", "0", "-", "3", ")", "/", "stride", "+", "1", ")", "\n", "self", ".", "logger", ".", "critical", "(", "\"After Conv: ({},{},{})\"", ".", "format", "(", "channels", ",", "image_size", ",", "image_size", ")", ")", "\n", "\n", "", "self", ".", "fc_size", "=", "image_size", "*", "image_size", "*", "channels", "\n", "assert", "self", ".", "fc_size", "==", "400", "\n", "self", ".", "fc_size_half", "=", "image_size", "*", "image_size", "*", "(", "channels", "//", "2", ")", "\n", "self", ".", "logger", ".", "critical", "(", "\"FC {} -> {}\"", ".", "format", "(", "self", ".", "fc_size", ",", "self", ".", "fc_size_half", ")", ")", "\n", "self", ".", "fc1", "=", "nn", ".", "Linear", "(", "self", ".", "fc_size", ",", "self", ".", "fc_size_half", ")", "\n", "\n", "self", ".", "qvals", "=", "nn", ".", "Linear", "(", "self", ".", "fc_size_half", ",", "args", ".", "num_actions", ")", "\n", "self", ".", "logger", ".", "critical", "(", "\"FC {} -> {} (Q-Values)\"", ".", "format", "(", "self", ".", "fc_size_half", ",", "args", ".", "num_actions", ")", ")", "\n", "\n", "if", "args", ".", "final_layer_bias", ">", "-", "1", ":", "\n", "            ", "optimistic_bias", "=", "torch", ".", "tensor", "(", "[", "args", ".", "final_layer_bias", "for", "_", "in", "range", "(", "args", ".", "num_actions", ")", "]", ",", "dtype", "=", "torch", ".", "float", ")", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.dora.maze_conv.DQN.forward": [[45, 57], ["torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.relu.view", "torch.relu", "torch.relu", "torch.relu", "maze_conv.DQN.qvals", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "maze_conv.DQN.conv1", "maze_conv.DQN.conv2", "maze_conv.DQN.fc1"], "methods", ["None"], ["\n", "", "", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "# x = x.to(device)", "\n", "\n", "        ", "x", "=", "F", ".", "relu", "(", "self", ".", "conv1", "(", "x", ")", ")", "\n", "x", "=", "F", ".", "relu", "(", "self", ".", "conv2", "(", "x", ")", ")", "\n", "\n", "# Flatten", "\n", "x", "=", "x", ".", "view", "(", "-", "1", ",", "self", ".", "fc_size", ")", "\n", "x", "=", "F", ".", "relu", "(", "self", ".", "fc1", "(", "x", ")", ")", "\n", "q", "=", "self", ".", "qvals", "(", "x", ")", "\n", "\n", "return", "q", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.trainer.dqn_train.DQNTrainer.__init__": [[12, 33], ["list", "logging.getLogger", "utils.logging.get_stats", "dqn_train.DQNTrainer.agent.parameters", "torch.optim.RMSprop", "torch.optim.RMSprop"], "methods", ["home.repos.pwc.inspect_result.oxwhirl_opiq.utils.logging.get_stats"], ["    ", "def", "__init__", "(", "self", ",", "agent", ",", "target_agent", ",", "args", ",", "count_model", "=", "None", ",", "buffer", "=", "None", ")", ":", "\n", "        ", "self", ".", "args", "=", "args", "\n", "self", ".", "agent", "=", "agent", "\n", "self", ".", "target_agent", "=", "target_agent", "\n", "\n", "# self.parameters = self.agent.parameters()", "\n", "self", ".", "agent_parameters", "=", "list", "(", "self", ".", "agent", ".", "parameters", "(", ")", ")", "\n", "if", "args", ".", "atari_rms", ":", "\n", "            ", "self", ".", "optimiser", "=", "torch", ".", "optim", ".", "RMSprop", "(", "params", "=", "self", ".", "agent_parameters", ",", "lr", "=", "self", ".", "args", ".", "lr", ",", "alpha", "=", "0.95", ",", "eps", "=", "0.00001", ",", "centered", "=", "True", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "optimiser", "=", "torch", ".", "optim", ".", "RMSprop", "(", "params", "=", "self", ".", "agent_parameters", ",", "lr", "=", "self", ".", "args", ".", "lr", ")", "\n", "\n", "", "self", ".", "logger", "=", "logging", ".", "getLogger", "(", "\"DQNTrainer\"", ")", "\n", "self", ".", "stats", "=", "get_stats", "(", ")", "\n", "\n", "self", ".", "count_model", "=", "count_model", "\n", "\n", "self", ".", "nstep", "=", "args", ".", "n_step", ">", "1", "\n", "self", ".", "goal_samples", "=", "0", "\n", "\n", "self", ".", "buffer", "=", "buffer", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.trainer.dqn_train.DQNTrainer.train": [[34, 142], ["torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "states.type.type.to", "actions.type.type.to", "rewards.clamp.clamp.to", "intrinsic_rewards.clamp.clamp.to", "next_states.type.type.to", "terminations.to.to.to", "states.type.type.type", "next_states.type.type.type", "actions.type.type.type", "dqn_train.DQNTrainer.target_agent", "dqn_train.DQNTrainer.gather().squeeze", "td_error.pow", "loss.to.to.mean", "dqn_train.DQNTrainer.optimiser.zero_grad", "loss.to.to.backward", "torch.nn.utils.clip_grad_norm_", "dqn_train.DQNTrainer.optimiser.step", "loss.to.to.to", "dqn_train.DQNTrainer.logger.debug", "dqn_train.DQNTrainer.stats.update_stats", "dqn_train.DQNTrainer.stats.update_stats", "dqn_train.DQNTrainer.stats.update_stats", "dqn_train.DQNTrainer.stats.update_stats", "dqn_train.DQNTrainer.train_nstep", "rewards.clamp.clamp.clamp", "dqn_train.DQNTrainer.agent", "dqn_train.DQNTrainer.agent", "dqn_train.DQNTrainer.count_model.get_all_action_counts", "torch.tensor", "dqn_train.DQNTrainer.gather().squeeze", "getattr", "dqn_train.DQNTrainer.stats.update_stats", "dqn_train.DQNTrainer.stats.update_stats", "dqn_train.DQNTrainer.stats.update_stats", "torch.min", "intrinsic_rewards.clamp.clamp.clamp", "targets.detach", "td_error.mean().to().item", "loss.to.to.item", "targets.mean().to().item", "torch.cat", "next_states.type.type.detach", "dqn_train.DQNTrainer.transpose", "dqn_train.DQNTrainer.gather", "q_values_next_states.max", "dqn_train.DQNTrainer.max", "torch.tensor", "dqn_train.DQNTrainer.count_model.get_all_action_counts", "torch.tensor", "torch.tensor.gather().squeeze", "dqn_train.DQNTrainer.stats.update_stats", "taken_action_counts.clamp_.clamp_.clamp_", "intrinsic_rewards.clamp.clamp.mean().to().item", "taken_action_intrinsic_rewards.mean().to().item", "dqn_train.DQNTrainer.gather", "dqn_train.DQNTrainer.count_model.get_count", "states.type.type.detach", "dqn_train.DQNTrainer.transpose", "taken_action_counts.clamp_.clamp_.pow", "td_error.mean().to", "targets.mean().to", "actions.type.type.unsqueeze", "states.type.type.detach", "torch.tensor.gather", "intrinsic_rewards.clamp.clamp.mean().to", "taken_action_intrinsic_rewards.mean().to", "max_next_state_actions.unsqueeze", "td_error.mean", "targets.mean", "actions.type.type.unsqueeze", "intrinsic_rewards.clamp.clamp.mean", "taken_action_intrinsic_rewards.mean"], "methods", ["home.repos.pwc.inspect_result.oxwhirl_opiq.envs.gridworld.GridWorld.step", "home.repos.pwc.inspect_result.oxwhirl_opiq.utils.logging.Stats.update_stats", "home.repos.pwc.inspect_result.oxwhirl_opiq.utils.logging.Stats.update_stats", "home.repos.pwc.inspect_result.oxwhirl_opiq.utils.logging.Stats.update_stats", "home.repos.pwc.inspect_result.oxwhirl_opiq.utils.logging.Stats.update_stats", "home.repos.pwc.inspect_result.oxwhirl_opiq.trainer.dqn_train.DQNTrainer.train_nstep", "home.repos.pwc.inspect_result.oxwhirl_opiq.count.dora_count.DoraCount.get_all_action_counts", "home.repos.pwc.inspect_result.oxwhirl_opiq.utils.logging.Stats.update_stats", "home.repos.pwc.inspect_result.oxwhirl_opiq.utils.logging.Stats.update_stats", "home.repos.pwc.inspect_result.oxwhirl_opiq.utils.logging.Stats.update_stats", "home.repos.pwc.inspect_result.oxwhirl_opiq.count.dora_count.DoraCount.get_all_action_counts", "home.repos.pwc.inspect_result.oxwhirl_opiq.utils.logging.Stats.update_stats", "home.repos.pwc.inspect_result.oxwhirl_opiq.count.dora_count.DoraCount.get_count"], ["", "def", "train", "(", "self", ",", "batch", ")", ":", "\n", "        ", "if", "self", ".", "nstep", ":", "\n", "# Bad coding with a lot of duplicated code.", "\n", "            ", "return", "self", ".", "train_nstep", "(", "batch", ")", "\n", "\n", "", "states", ",", "actions", ",", "rewards", ",", "intrinsic_rewards", ",", "next_states", ",", "terminations", ",", "extra_info", "=", "batch", "\n", "\n", "states", "=", "torch", ".", "from_numpy", "(", "states", ")", "\n", "actions", "=", "torch", ".", "from_numpy", "(", "actions", ")", "\n", "rewards", "=", "torch", ".", "from_numpy", "(", "rewards", ")", "\n", "intrinsic_rewards", "=", "torch", ".", "from_numpy", "(", "intrinsic_rewards", ")", "\n", "next_states", "=", "torch", ".", "from_numpy", "(", "next_states", ")", "\n", "terminations", "=", "torch", ".", "from_numpy", "(", "terminations", ")", "\n", "\n", "# Clip Rewards to [-1, 1]", "\n", "if", "self", ".", "args", ".", "reward_clipping", ":", "\n", "            ", "rewards", "=", "rewards", ".", "clamp", "(", "min", "=", "-", "1", ",", "max", "=", "+", "1", ")", "\n", "\n", "# Move them over", "\n", "", "states", "=", "states", ".", "to", "(", "device", ")", "\n", "actions", "=", "actions", ".", "to", "(", "device", ")", "\n", "rewards", "=", "rewards", ".", "to", "(", "device", ")", "\n", "intrinsic_rewards", "=", "intrinsic_rewards", ".", "to", "(", "device", ")", "\n", "next_states", "=", "next_states", ".", "to", "(", "device", ")", "\n", "terminations", "=", "terminations", ".", "to", "(", "device", ")", "\n", "\n", "# Change dtypes", "\n", "states", "=", "states", ".", "type", "(", "torch", ".", "float32", ")", "\n", "next_states", "=", "next_states", ".", "type", "(", "torch", ".", "float32", ")", "\n", "actions", "=", "actions", ".", "type", "(", "torch", ".", "long", ")", "\n", "\n", "states", ".", "requires_grad", "=", "True", "\n", "if", "self", ".", "args", ".", "double_q", ":", "\n", "            ", "state_next_state_q_vals", "=", "self", ".", "agent", "(", "torch", ".", "cat", "(", "[", "states", ",", "next_states", "]", ",", "dim", "=", "0", ")", ")", "\n", "num_states", "=", "states", ".", "shape", "[", "0", "]", "\n", "q_values", "=", "state_next_state_q_vals", "[", ":", "num_states", "]", "\n", "q_values_next_states", "=", "state_next_state_q_vals", "[", "num_states", ":", "]", "\n", "", "else", ":", "\n", "            ", "q_values", "=", "self", ".", "agent", "(", "states", ")", "\n", "", "target_agent_q_values", "=", "self", ".", "target_agent", "(", "next_states", ")", "\n", "\n", "if", "self", ".", "args", ".", "optim_bootstrap", ":", "\n", "            ", "assert", "self", ".", "args", ".", "double_q", "is", "False", "\n", "# Get the counts for the next states", "\n", "state_action_counts", "=", "self", ".", "count_model", ".", "get_all_action_counts", "(", "next_states", ".", "detach", "(", ")", ")", "\n", "counts_tensor", "=", "torch", ".", "tensor", "(", "state_action_counts", ".", "transpose", "(", "1", ",", "0", ")", ",", "device", "=", "next_states", ".", "device", ",", "dtype", "=", "torch", ".", "float32", ")", "\n", "optims", "=", "self", ".", "args", ".", "optim_bootstrap_tau", "/", "(", "counts_tensor", "+", "1.0", ")", ".", "pow", "(", "self", ".", "args", ".", "optim_m", ")", "\n", "target_agent_q_values", "=", "target_agent_q_values", "+", "optims", "\n", "if", "self", ".", "args", ".", "double_q", ":", "\n", "                ", "q_values_next_states", "=", "q_values_next_states", "+", "optims", "\n", "\n", "", "", "taken_q_value", "=", "q_values", ".", "gather", "(", "dim", "=", "1", ",", "index", "=", "actions", ".", "unsqueeze", "(", "1", ")", ")", ".", "squeeze", "(", "1", ")", "\n", "\n", "if", "self", ".", "args", ".", "double_q", ":", "\n", "            ", "max_next_state_actions", "=", "q_values_next_states", ".", "max", "(", "dim", "=", "1", ")", "[", "1", "]", "\n", "max_target_q_values", "=", "target_agent_q_values", ".", "gather", "(", "dim", "=", "1", ",", "index", "=", "max_next_state_actions", ".", "unsqueeze", "(", "1", ")", ")", ".", "squeeze", "(", "1", ")", "\n", "", "else", ":", "\n", "            ", "max_target_q_values", "=", "target_agent_q_values", ".", "max", "(", "dim", "=", "1", ")", "[", "0", "]", "\n", "\n", "", "if", "self", ".", "args", ".", "recompute_count_rewards", "and", "self", ".", "args", ".", "count_rewards", ":", "\n", "            ", "if", "self", ".", "args", ".", "count_state_only_rewards", ":", "\n", "# Counts for just the state", "\n", "                ", "taken_action_counts", "=", "torch", ".", "tensor", "(", "self", ".", "count_model", ".", "get_count", "(", "states", ".", "detach", "(", ")", ")", ",", "device", "=", "next_states", ".", "device", ",", "dtype", "=", "torch", ".", "float32", ")", "\n", "", "else", ":", "\n", "# Get the counts for the current states for all actions", "\n", "                ", "state_action_counts", "=", "self", ".", "count_model", ".", "get_all_action_counts", "(", "states", ".", "detach", "(", ")", ")", "\n", "counts_tensor", "=", "torch", ".", "tensor", "(", "state_action_counts", ".", "transpose", "(", "1", ",", "0", ")", ",", "device", "=", "next_states", ".", "device", ",", "dtype", "=", "torch", ".", "float32", ")", "\n", "taken_action_counts", "=", "counts_tensor", ".", "gather", "(", "dim", "=", "1", ",", "index", "=", "actions", ".", "unsqueeze", "(", "1", ")", ")", ".", "squeeze", "(", "1", ")", "\n", "zero_counts", "=", "(", "taken_action_counts", "<", "1", ")", ".", "sum", "(", ")", ".", "item", "(", ")", "\n", "self", ".", "stats", ".", "update_stats", "(", "\"0_Counts\"", ",", "zero_counts", ")", "\n", "", "if", "getattr", "(", "self", ".", "count_model", ",", "\"reward_directly\"", ",", "False", ")", ":", "\n", "                ", "taken_action_intrinsic_rewards", "=", "taken_action_counts", "\n", "", "else", ":", "\n", "                ", "taken_action_counts", "=", "taken_action_counts", ".", "clamp_", "(", "min", "=", "1", ",", "max", "=", "10000000", ")", "\n", "taken_action_intrinsic_rewards", "=", "self", ".", "args", ".", "count_beta", "/", "taken_action_counts", ".", "pow", "(", "0.5", ")", "\n", "", "self", ".", "stats", ".", "update_stats", "(", "\"intrinsic rewards\"", ",", "intrinsic_rewards", ".", "mean", "(", ")", ".", "to", "(", "cpu_device", ")", ".", "item", "(", ")", ")", "\n", "self", ".", "stats", ".", "update_stats", "(", "\"intrinsic rewards recalc\"", ",", "taken_action_intrinsic_rewards", ".", "mean", "(", ")", ".", "to", "(", "cpu_device", ")", ".", "item", "(", ")", ")", "\n", "self", ".", "stats", ".", "update_stats", "(", "\"intrinsic rewards diff\"", ",", "(", "intrinsic_rewards", "-", "taken_action_intrinsic_rewards", ")", ".", "mean", "(", ")", ".", "to", "(", "cpu_device", ")", ".", "item", "(", ")", ")", "\n", "intrinsic_rewards", "=", "torch", ".", "min", "(", "intrinsic_rewards", ",", "taken_action_intrinsic_rewards", ")", "# To ensure", "\n", "\n", "", "if", "self", ".", "args", ".", "reward_clipping", ":", "\n", "            ", "intrinsic_rewards", "=", "intrinsic_rewards", ".", "clamp", "(", "min", "=", "-", "1", ",", "max", "=", "+", "1", ")", "\n", "\n", "# 1-step Q-Learning target", "\n", "", "targets", "=", "rewards", "+", "intrinsic_rewards", "+", "self", ".", "args", ".", "gamma", "*", "max_target_q_values", "*", "(", "1", "-", "terminations", ")", "\n", "\n", "td_error", "=", "taken_q_value", "-", "targets", ".", "detach", "(", ")", "\n", "\n", "# Loss is td-error^2 for each sample. Take the mean over the batch", "\n", "loss", "=", "td_error", ".", "pow", "(", "2", ")", "\n", "\n", "loss", "=", "loss", ".", "mean", "(", ")", "\n", "\n", "self", ".", "optimiser", ".", "zero_grad", "(", ")", "\n", "loss", ".", "backward", "(", ")", "\n", "grad_norm", "=", "torch", ".", "nn", ".", "utils", ".", "clip_grad_norm_", "(", "self", ".", "agent_parameters", ",", "self", ".", "args", ".", "clip_grad_norm", ")", "\n", "self", ".", "optimiser", ".", "step", "(", ")", "\n", "\n", "# Move stuff back to cpu for logging", "\n", "loss", "=", "loss", ".", "to", "(", "cpu_device", ")", "\n", "\n", "self", ".", "logger", ".", "debug", "(", "\"Loss: {:.4f}, GradNorm: {:.4f}\"", ".", "format", "(", "loss", ",", "grad_norm", ")", ")", "\n", "# self.logger.debug(\"Q-Values: {}\".format(q_values))", "\n", "\n", "self", ".", "stats", ".", "update_stats", "(", "\"td_error\"", ",", "td_error", ".", "mean", "(", ")", ".", "to", "(", "cpu_device", ")", ".", "item", "(", ")", ")", "\n", "self", ".", "stats", ".", "update_stats", "(", "\"Loss\"", ",", "loss", ".", "item", "(", ")", ")", "\n", "self", ".", "stats", ".", "update_stats", "(", "\"GradNorm\"", ",", "grad_norm", ")", "\n", "self", ".", "stats", ".", "update_stats", "(", "\"targets\"", ",", "targets", ".", "mean", "(", ")", ".", "to", "(", "cpu_device", ")", ".", "item", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.trainer.dqn_train.DQNTrainer.train_nstep": [[143, 335], ["torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "states.type.type.to", "actions.type.type.to", "n_rewards.clamp.clamp.to", "n_intrinsic_rewards.clamp.clamp.to", "n_next_states.type.type.to", "terminations.to.to.to", "steps.to.to.to", "n_last_states.type.type.to", "next_actions.type.type.to", "states.type.type.type", "n_next_states.type.type.type", "n_last_states.type.type.type", "actions.type.type.type", "next_actions.type.type.type", "dqn_train.DQNTrainer.target_agent", "torch.tensor", "gamma_tensor.repeat.repeat.repeat", "enumerate", "discounted_rewards.sum", "any", "dqn_train.DQNTrainer.stats.update_stats", "td_error.pow", "loss.to.to.mean", "dqn_train.DQNTrainer.optimiser.zero_grad", "loss.to.to.backward", "torch.nn.utils.clip_grad_norm_", "dqn_train.DQNTrainer.optimiser.step", "loss.to.to.to", "dqn_train.DQNTrainer.logger.debug", "dqn_train.DQNTrainer.stats.update_stats", "dqn_train.DQNTrainer.stats.update_stats", "dqn_train.DQNTrainer.stats.update_stats", "dqn_train.DQNTrainer.stats.update_stats", "torch.from_numpy().to().type", "torch.from_numpy().to().type", "dqn_train.DQNTrainer.agent", "dqn_train.DQNTrainer.agent", "dqn_train.DQNTrainer.count_model.get_all_action_counts", "torch.tensor", "getattr", "dqn_train.DQNTrainer.gather().squeeze", "dqn_train.DQNTrainer.gather().squeeze", "torch.cat", "torch.cat.reshape", "getattr", "dqn_train.DQNTrainer.stats.update_stats", "dqn_train.DQNTrainer.stats.update_stats", "intrinsic_rewards.view", "dqn_train.DQNTrainer.stats.update_stats", "torch.min", "discounted_i_rewards.sum", "discounted_sum_i_rewards.sum.sum.sum", "n_rewards.clamp.clamp.clamp", "targets.detach", "dqn_train.DQNTrainer.agent.scale_gradients", "td_error.mean().to().item", "targets.mean().to().item", "loss.to.to.item", "torch.cat", "n_last_states.type.type.detach", "dqn_train.DQNTrainer.transpose", "q_values_next_states.max", "dqn_train.DQNTrainer.max", "torch.tensor", "dqn_train.DQNTrainer.count_model.get_all_action_counts", "torch.tensor", "next_actions.type.type.reshape", "torch.tensor.gather", "dqn_train.DQNTrainer.stats.update_stats", "taken_action_counts.clamp_.clamp_.clamp_", "n_intrinsic_rewards.clamp.clamp.mean().to().item", "intrinsic_rewards.mean().to().item", "discounted_diff_rewards.mean().to().item", "reshaped_i_rewards.clamp.clamp.clamp", "n_intrinsic_rewards.clamp.clamp.clamp", "torch.from_numpy().to().type.float", "torch.from_numpy().to", "torch.from_numpy().to", "dqn_train.DQNTrainer.gather", "dqn_train.DQNTrainer.gather", "range", "states.type.type.unsqueeze", "dqn_train.DQNTrainer.count_model.get_count", "torch.cat.reshape.detach", "dqn_train.DQNTrainer.transpose", "taken_action_counts.clamp_.clamp_.pow", "td_error.mean().to", "targets.mean().to", "torch.arange", "torch.cat.reshape.detach", "n_intrinsic_rewards.clamp.clamp.mean().to", "intrinsic_rewards.mean().to", "discounted_diff_rewards.mean().to", "torch.from_numpy().to().type.float", "torch.from_numpy", "torch.from_numpy", "actions.type.type.unsqueeze", "max_next_state_actions.unsqueeze", "torch.from_numpy().to().type.unsqueeze().expand_as", "steps.to.to.float", "td_error.mean", "targets.mean", "n_intrinsic_rewards.clamp.clamp.mean", "intrinsic_rewards.mean", "discounted_diff_rewards.mean", "steps.to.to.float", "torch.from_numpy().to().type.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.oxwhirl_opiq.utils.logging.Stats.update_stats", "home.repos.pwc.inspect_result.oxwhirl_opiq.envs.gridworld.GridWorld.step", "home.repos.pwc.inspect_result.oxwhirl_opiq.utils.logging.Stats.update_stats", "home.repos.pwc.inspect_result.oxwhirl_opiq.utils.logging.Stats.update_stats", "home.repos.pwc.inspect_result.oxwhirl_opiq.utils.logging.Stats.update_stats", "home.repos.pwc.inspect_result.oxwhirl_opiq.utils.logging.Stats.update_stats", "home.repos.pwc.inspect_result.oxwhirl_opiq.count.dora_count.DoraCount.get_all_action_counts", "home.repos.pwc.inspect_result.oxwhirl_opiq.utils.logging.Stats.update_stats", "home.repos.pwc.inspect_result.oxwhirl_opiq.utils.logging.Stats.update_stats", "home.repos.pwc.inspect_result.oxwhirl_opiq.utils.logging.Stats.update_stats", "home.repos.pwc.inspect_result.oxwhirl_opiq.agent.bsp_maze_conv.BSPDQN.scale_gradients", "home.repos.pwc.inspect_result.oxwhirl_opiq.count.dora_count.DoraCount.get_all_action_counts", "home.repos.pwc.inspect_result.oxwhirl_opiq.utils.logging.Stats.update_stats", "home.repos.pwc.inspect_result.oxwhirl_opiq.count.dora_count.DoraCount.get_count"], ["", "def", "train_nstep", "(", "self", ",", "batch", ")", ":", "\n", "        ", "states", ",", "actions", ",", "rewards", ",", "intrinsic_rewards", ",", "next_states", ",", "terminations", ",", "extra_info", "=", "batch", "\n", "\n", "states", "=", "torch", ".", "from_numpy", "(", "states", ")", "\n", "actions", "=", "torch", ".", "from_numpy", "(", "actions", ")", "\n", "n_rewards", "=", "torch", ".", "from_numpy", "(", "extra_info", "[", "\"rewards\"", "]", ")", "\n", "n_intrinsic_rewards", "=", "torch", ".", "from_numpy", "(", "extra_info", "[", "\"intrin_rewards\"", "]", ")", "\n", "n_next_states", "=", "torch", ".", "from_numpy", "(", "extra_info", "[", "\"next_states\"", "]", ")", "\n", "n_last_states", "=", "torch", ".", "from_numpy", "(", "extra_info", "[", "\"last_states\"", "]", ")", "\n", "terminations", "=", "torch", ".", "from_numpy", "(", "extra_info", "[", "\"dones\"", "]", ")", "\n", "steps", "=", "torch", ".", "from_numpy", "(", "extra_info", "[", "\"steps\"", "]", ")", "\n", "next_actions", "=", "torch", ".", "from_numpy", "(", "extra_info", "[", "\"next_actions\"", "]", ")", "\n", "\n", "# Clip Rewards to [-1, 1]", "\n", "# n_rewards = n_rewards.clamp(min=-1, max=+1)", "\n", "\n", "# Move them over", "\n", "states", "=", "states", ".", "to", "(", "device", ")", "\n", "actions", "=", "actions", ".", "to", "(", "device", ")", "\n", "n_rewards", "=", "n_rewards", ".", "to", "(", "device", ")", "\n", "n_intrinsic_rewards", "=", "n_intrinsic_rewards", ".", "to", "(", "device", ")", "\n", "n_next_states", "=", "n_next_states", ".", "to", "(", "device", ")", "\n", "terminations", "=", "terminations", ".", "to", "(", "device", ")", "\n", "steps", "=", "steps", ".", "to", "(", "device", ")", "\n", "n_last_states", "=", "n_last_states", ".", "to", "(", "device", ")", "\n", "next_actions", "=", "next_actions", ".", "to", "(", "device", ")", "\n", "\n", "# Change dtypes", "\n", "states", "=", "states", ".", "type", "(", "torch", ".", "float32", ")", "\n", "n_next_states", "=", "n_next_states", ".", "type", "(", "torch", ".", "float32", ")", "\n", "n_last_states", "=", "n_last_states", ".", "type", "(", "torch", ".", "float32", ")", "\n", "actions", "=", "actions", ".", "type", "(", "torch", ".", "long", ")", "\n", "next_actions", "=", "next_actions", ".", "type", "(", "torch", ".", "long", ")", "\n", "\n", "terminations", "=", "terminations", "[", ":", ",", "0", "]", "\n", "steps", "=", "steps", "[", ":", ",", "0", "]", "\n", "\n", "if", "self", ".", "args", ".", "bsp", ":", "\n", "            ", "bsp_w", "=", "torch", ".", "from_numpy", "(", "extra_info", "[", "\"bsp_w\"", "]", ")", ".", "to", "(", "device", ")", ".", "type", "(", "torch", ".", "long", ")", "\n", "\n", "", "if", "self", ".", "args", ".", "mmc", ":", "\n", "            ", "mmc_v", "=", "torch", ".", "from_numpy", "(", "extra_info", "[", "\"mmc\"", "]", ")", ".", "to", "(", "device", ")", ".", "type", "(", "torch", ".", "float", ")", "\n", "\n", "", "states", ".", "requires_grad", "=", "True", "\n", "# assert not self.args.double_q", "\n", "if", "self", ".", "args", ".", "double_q", ":", "\n", "            ", "state_next_state_q_vals", "=", "self", ".", "agent", "(", "torch", ".", "cat", "(", "[", "states", ",", "n_last_states", "]", ",", "dim", "=", "0", ")", ")", "\n", "num_states", "=", "states", ".", "shape", "[", "0", "]", "\n", "q_values", "=", "state_next_state_q_vals", "[", ":", "num_states", "]", "\n", "q_values_next_states", "=", "state_next_state_q_vals", "[", "num_states", ":", "]", "\n", "", "else", ":", "\n", "            ", "q_values", "=", "self", ".", "agent", "(", "states", ")", "\n", "# next_states_to_use = n_next_states.index_select(dim=1, index=(steps - 1).long())", "\n", "", "target_agent_q_values", "=", "self", ".", "target_agent", "(", "n_last_states", ")", "\n", "\n", "if", "self", ".", "args", ".", "optim_bootstrap", ":", "\n", "# assert self.args.double_q is False", "\n", "# Get the counts for the next states", "\n", "            ", "state_action_counts", "=", "self", ".", "count_model", ".", "get_all_action_counts", "(", "n_last_states", ".", "detach", "(", ")", ")", "\n", "counts_tensor", "=", "torch", ".", "tensor", "(", "state_action_counts", ".", "transpose", "(", "1", ",", "0", ")", ",", "device", "=", "n_next_states", ".", "device", ",", "dtype", "=", "torch", ".", "float32", ")", "\n", "if", "getattr", "(", "self", ".", "count_model", ",", "\"reward_directly\"", ",", "False", ")", ":", "\n", "                ", "optims", "=", "counts_tensor", "\n", "# Hyperparameter scaling", "\n", "optims", "=", "optims", "/", "self", ".", "args", ".", "rnd_net_scaler", "\n", "optims", "=", "optims", "**", "self", ".", "args", ".", "rnd_optim_m", "\n", "optims", "=", "optims", "*", "self", ".", "args", ".", "optim_bootstrap_tau", "\n", "", "else", ":", "\n", "                ", "optims", "=", "self", ".", "args", ".", "optim_bootstrap_tau", "/", "(", "counts_tensor", "+", "1.0", ")", ".", "pow", "(", "self", ".", "args", ".", "optim_m", ")", "\n", "", "target_agent_q_values", "=", "target_agent_q_values", "+", "optims", "\n", "if", "self", ".", "args", ".", "optim_interpolation", ":", "\n", "                ", "w", "=", "1", "/", "(", "counts_tensor", "+", "1.0", ")", ".", "pow", "(", "self", ".", "args", ".", "optim_m", ")", "\n", "target_agent_q_values", "=", "target_agent_q_values", "*", "(", "1", "-", "w", ")", "+", "self", ".", "args", ".", "optim_bootstrap_tau", "*", "(", "w", ")", "\n", "", "if", "self", ".", "args", ".", "double_q", ":", "\n", "                ", "q_values_next_states", "=", "q_values_next_states", "+", "optims", "\n", "\n", "", "", "if", "self", ".", "args", ".", "bsp", ":", "\n", "            ", "actions", "=", "actions", "\n", "# Pytorch weirdness?", "\n", "bs", "=", "states", ".", "shape", "[", "0", "]", "\n", "taken_q_value", "=", "q_values", "[", "torch", ".", "arange", "(", "bs", ")", ",", "actions", ",", ":", "]", "\n", "", "else", ":", "\n", "            ", "taken_q_value", "=", "q_values", ".", "gather", "(", "dim", "=", "1", ",", "index", "=", "actions", ".", "unsqueeze", "(", "1", ")", ")", ".", "squeeze", "(", "1", ")", "\n", "\n", "", "if", "self", ".", "args", ".", "double_q", ":", "\n", "            ", "max_next_state_actions", "=", "q_values_next_states", ".", "max", "(", "dim", "=", "1", ")", "[", "1", "]", "\n", "max_target_q_values", "=", "target_agent_q_values", ".", "gather", "(", "dim", "=", "1", ",", "index", "=", "max_next_state_actions", ".", "unsqueeze", "(", "1", ")", ")", ".", "squeeze", "(", "1", ")", "\n", "", "else", ":", "\n", "            ", "max_target_q_values", "=", "target_agent_q_values", ".", "max", "(", "dim", "=", "1", ")", "[", "0", "]", "\n", "\n", "", "gamma_tensor", "=", "torch", ".", "tensor", "(", "[", "self", ".", "args", ".", "gamma", "**", "i", "for", "i", "in", "range", "(", "self", ".", "args", ".", "n_step", ")", "]", ",", "device", "=", "n_next_states", ".", "device", ")", "\n", "batch_size", "=", "states", ".", "shape", "[", "0", "]", "\n", "gamma_tensor", "=", "gamma_tensor", ".", "repeat", "(", "batch_size", ",", "1", ")", "\n", "# Slow", "\n", "for", "idx", ",", "step", "in", "enumerate", "(", "steps", ")", ":", "\n", "            ", "gamma_tensor", "[", "idx", ",", "step", ":", "]", "=", "0", "\n", "", "if", "self", ".", "args", ".", "recompute_count_rewards", "and", "self", ".", "args", ".", "count_rewards", ":", "\n", "# assert self.args.count_state_only_rewards", "\n", "            ", "batch_size", "=", "states", ".", "shape", "[", "0", "]", "\n", "shape_to_use", "=", "(", "batch_size", "*", "(", "self", ".", "args", ".", "n_step", ")", ",", ")", "+", "n_next_states", ".", "shape", "[", "2", ":", "]", "\n", "states_to_count", "=", "torch", ".", "cat", "(", "[", "states", ".", "unsqueeze", "(", "1", ")", ",", "n_next_states", "[", ":", ",", ":", "-", "1", "]", "]", ",", "dim", "=", "1", ")", "\n", "states_to_count_flattened", "=", "states_to_count", ".", "reshape", "(", "shape_to_use", ")", "\n", "# Counts for just the state", "\n", "if", "self", ".", "args", ".", "count_state_only_rewards", ":", "\n", "                ", "taken_action_counts", "=", "torch", ".", "tensor", "(", "self", ".", "count_model", ".", "get_count", "(", "states_to_count_flattened", ".", "detach", "(", ")", ",", "action", "=", "-", "1", ")", ",", "device", "=", "n_next_states", ".", "device", ",", "dtype", "=", "torch", ".", "float32", ")", "\n", "", "else", ":", "\n", "# Get the counts for the current states for all actions", "\n", "                ", "state_action_counts", "=", "self", ".", "count_model", ".", "get_all_action_counts", "(", "states_to_count_flattened", ".", "detach", "(", ")", ")", "\n", "counts_tensor", "=", "torch", ".", "tensor", "(", "state_action_counts", ".", "transpose", "(", "1", ",", "0", ")", ",", "device", "=", "n_next_states", ".", "device", ",", "dtype", "=", "torch", ".", "float32", ")", "\n", "nn", "=", "next_actions", ".", "reshape", "(", "counts_tensor", ".", "shape", "[", "0", "]", ",", "1", ")", "\n", "taken_action_counts", "=", "counts_tensor", ".", "gather", "(", "dim", "=", "1", ",", "index", "=", "nn", ")", "\n", "zero_counts", "=", "(", "taken_action_counts", "<", "1", ")", ".", "sum", "(", ")", ".", "item", "(", ")", "\n", "self", ".", "stats", ".", "update_stats", "(", "\"0_Counts\"", ",", "zero_counts", ")", "\n", "#", "\n", "# # Slow count to ensure it works correctly on the gpu", "\n", "# scs = torch.ones(size=(batch_size, self.args.n_step), device=n_next_states.device, dtype=torch.float32)", "\n", "# for b_idx in range(batch_size):", "\n", "#     rc = self.count_model.get_count(states[b_idx].detach())[0]", "\n", "#     scs[b_idx, 0] = rc", "\n", "#     for ss in range(steps[b_idx] - 1):", "\n", "#         rc_n = self.count_model.get_count(n_next_states[b_idx, ss].detach())[0]", "\n", "#         scs[b_idx, ss + 1] = rc_n", "\n", "# taken_action_counts = scs", "\n", "\n", "", "if", "getattr", "(", "self", ".", "count_model", ",", "\"reward_directly\"", ",", "False", ")", ":", "\n", "                ", "taken_action_intrinsic_rewards", "=", "taken_action_counts", "\n", "", "else", ":", "\n", "                ", "taken_action_counts", "=", "taken_action_counts", ".", "clamp_", "(", "min", "=", "1", ",", "max", "=", "10000000", ")", "\n", "taken_action_intrinsic_rewards", "=", "self", ".", "args", ".", "count_beta", "/", "taken_action_counts", ".", "pow", "(", "0.5", ")", "\n", "", "self", ".", "stats", ".", "update_stats", "(", "\"intrinsic rewards\"", ",", "n_intrinsic_rewards", ".", "mean", "(", ")", ".", "to", "(", "cpu_device", ")", ".", "item", "(", ")", ")", "\n", "intrinsic_rewards", "=", "taken_action_intrinsic_rewards", "\n", "self", ".", "stats", ".", "update_stats", "(", "\"intrin rewards recalc\"", ",", "intrinsic_rewards", ".", "mean", "(", ")", ".", "to", "(", "cpu_device", ")", ".", "item", "(", ")", ")", "\n", "reshaped_i_rewards", "=", "intrinsic_rewards", ".", "view", "(", "batch_size", ",", "self", ".", "args", ".", "n_step", ")", "\n", "# reshaped_i_rewards = taken_action_intrinsic_rewards", "\n", "discounted_diff_rewards", "=", "(", "n_intrinsic_rewards", "-", "reshaped_i_rewards", ")", "*", "gamma_tensor", "\n", "self", ".", "stats", ".", "update_stats", "(", "\"intrin rewards diff\"", ",", "discounted_diff_rewards", ".", "mean", "(", ")", ".", "to", "(", "cpu_device", ")", ".", "item", "(", ")", ")", "\n", "reshaped_i_rewards", "=", "torch", ".", "min", "(", "reshaped_i_rewards", ",", "n_intrinsic_rewards", ")", "# To ensure our recomputed rewards aren't > originals. Due to errors introduced into the keys when batching", "\n", "if", "self", ".", "args", ".", "reward_clipping", ":", "\n", "                ", "reshaped_i_rewards", "=", "reshaped_i_rewards", ".", "clamp", "(", "min", "=", "-", "1", ",", "max", "=", "+", "1", ")", "\n", "", "discounted_i_rewards", "=", "reshaped_i_rewards", "*", "gamma_tensor", "\n", "discounted_sum_i_rewards", "=", "discounted_i_rewards", ".", "sum", "(", "dim", "=", "1", ")", "\n", "", "else", ":", "\n", "            ", "if", "self", ".", "args", ".", "reward_clipping", ":", "\n", "                ", "n_intrinsic_rewards", "=", "n_intrinsic_rewards", ".", "clamp", "(", "min", "=", "-", "1", ",", "max", "=", "+", "1", ")", "\n", "", "discounted_sum_i_rewards", "=", "n_intrinsic_rewards", "*", "gamma_tensor", "\n", "discounted_sum_i_rewards", "=", "discounted_sum_i_rewards", ".", "sum", "(", "dim", "=", "1", ")", "\n", "\n", "", "if", "self", ".", "args", ".", "reward_clipping", ":", "\n", "            ", "n_rewards", "=", "n_rewards", ".", "clamp", "(", "min", "=", "-", "1", ",", "max", "=", "+", "1", ")", "\n", "", "discounted_rewards", "=", "n_rewards", "*", "gamma_tensor", "\n", "discounted_sum_rewards", "=", "discounted_rewards", ".", "sum", "(", "dim", "=", "1", ")", "\n", "\n", "# n-step Q-Learning target", "\n", "# targets = rewards + intrinsic_rewards + self.args.gamma * max_target_q_values * (1 - terminations)", "\n", "if", "self", ".", "args", ".", "bsp", ":", "\n", "            ", "targets", "=", "(", "discounted_sum_rewards", "+", "discounted_sum_i_rewards", ")", ".", "unsqueeze", "(", "1", ")", "+", "(", "(", "self", ".", "args", ".", "gamma", "**", "steps", ".", "float", "(", ")", ")", "*", "(", "1", "-", "terminations", ")", ")", ".", "unsqueeze", "(", "1", ")", "*", "max_target_q_values", "\n", "targets", "=", "targets", "*", "bsp_w", ".", "float", "(", ")", "\n", "if", "self", ".", "args", ".", "mmc", ":", "\n", "                ", "targets", "=", "(", "1", "-", "self", ".", "args", ".", "mmc_beta", ")", "*", "targets", "+", "(", "self", ".", "args", ".", "mmc_beta", ")", "*", "mmc_v", ".", "unsqueeze", "(", "1", ")", ".", "expand_as", "(", "targets", ")", "*", "bsp_w", ".", "float", "(", ")", "\n", "", "", "else", ":", "\n", "            ", "targets", "=", "discounted_sum_rewards", "+", "discounted_sum_i_rewards", "+", "(", "self", ".", "args", ".", "gamma", "**", "steps", ".", "float", "(", ")", ")", "*", "(", "1", "-", "terminations", ")", "*", "max_target_q_values", "\n", "\n", "if", "self", ".", "args", ".", "mmc", ":", "\n", "                ", "targets", "=", "(", "1", "-", "self", ".", "args", ".", "mmc_beta", ")", "*", "targets", "+", "(", "self", ".", "args", ".", "mmc_beta", ")", "*", "mmc_v", "\n", "\n", "", "", "if", "any", "(", "discounted_sum_rewards", ">", "0.9", ")", ":", "\n", "            ", "self", ".", "goal_samples", "+=", "1", "\n", "", "self", ".", "stats", ".", "update_stats", "(", "\"goal_samples\"", ",", "self", ".", "goal_samples", ")", "\n", "\n", "td_error", "=", "taken_q_value", "-", "targets", ".", "detach", "(", ")", "\n", "\n", "# Loss is td-error^2 for each sample. Take the mean over the batch", "\n", "loss", "=", "td_error", ".", "pow", "(", "2", ")", "\n", "\n", "loss", "=", "loss", ".", "mean", "(", ")", "\n", "\n", "self", ".", "optimiser", ".", "zero_grad", "(", ")", "\n", "loss", ".", "backward", "(", ")", "\n", "grad_norm", "=", "torch", ".", "nn", ".", "utils", ".", "clip_grad_norm_", "(", "self", ".", "agent_parameters", ",", "self", ".", "args", ".", "clip_grad_norm", ")", "\n", "if", "self", ".", "args", ".", "bsp", "and", "self", ".", "args", ".", "bsp_grad_norm", ":", "\n", "            ", "self", ".", "agent", ".", "scale_gradients", "(", ")", "\n", "", "self", ".", "optimiser", ".", "step", "(", ")", "\n", "\n", "# Move stuff back to cpu for logging", "\n", "loss", "=", "loss", ".", "to", "(", "cpu_device", ")", "\n", "\n", "self", ".", "logger", ".", "debug", "(", "\"Loss: {:.4f}, GradNorm: {:.4f}\"", ".", "format", "(", "loss", ",", "grad_norm", ")", ")", "\n", "# self.logger.debug(\"Q-Values: {}\".format(q_values))", "\n", "\n", "self", ".", "stats", ".", "update_stats", "(", "\"td_error\"", ",", "td_error", ".", "mean", "(", ")", ".", "to", "(", "cpu_device", ")", ".", "item", "(", ")", ")", "\n", "self", ".", "stats", ".", "update_stats", "(", "\"targets\"", ",", "targets", ".", "mean", "(", ")", ".", "to", "(", "cpu_device", ")", ".", "item", "(", ")", ")", "\n", "self", ".", "stats", ".", "update_stats", "(", "\"Loss\"", ",", "loss", ".", "item", "(", ")", ")", "\n", "self", ".", "stats", ".", "update_stats", "(", "\"GradNorm\"", ",", "grad_norm", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.trainer.dqn_train.DQNTrainer.update_target_agent": [[336, 338], ["dqn_train.DQNTrainer.target_agent.load_state_dict", "dqn_train.DQNTrainer.agent.state_dict"], "methods", ["None"], ["", "def", "update_target_agent", "(", "self", ")", ":", "\n", "        ", "self", ".", "target_agent", ".", "load_state_dict", "(", "self", ".", "agent", ".", "state_dict", "(", ")", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.oxwhirl_opiq.action.eps_greedy.EpsGreedy.__init__": [[8, 17], ["utils.logging.get_stats", "logging.getLogger"], "methods", ["home.repos.pwc.inspect_result.oxwhirl_opiq.utils.logging.get_stats"], ["    ", "def", "__init__", "(", "self", ",", "args", ")", ":", "\n", "\n", "        ", "self", ".", "epsilon_start", "=", "args", ".", "epsilon_start", "\n", "self", ".", "epsilon_finish", "=", "args", ".", "epsilon_finish", "\n", "self", ".", "epsilon_time_length", "=", "args", ".", "epsilon_time_length", "\n", "\n", "self", ".", "num_actions", "=", "args", ".", "num_actions", "\n", "self", ".", "stats", "=", "get_stats", "(", ")", "\n", "self", ".", "logger", "=", "logging", ".", "getLogger", "(", "\"EpsGreedy\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.action.eps_greedy.EpsGreedy.select_actions": [[18, 41], ["max", "eps_greedy.EpsGreedy.logger.debug", "eps_greedy.EpsGreedy.log_stats", "eps_greedy.EpsGreedy.logger.info", "numpy.random.random", "numpy.random.randint", "eps_greedy.EpsGreedy.logger.debug", "q_values.argmax().cpu().item", "eps_greedy.EpsGreedy.logger.debug", "q_values.argmax().cpu", "q_values.argmax"], "methods", ["home.repos.pwc.inspect_result.oxwhirl_opiq.action.count_action.CountBonus.log_stats"], ["", "def", "select_actions", "(", "self", ",", "q_values", ",", "t", ",", "info", "=", "{", "}", ")", ":", "\n", "\n", "        ", "epsilon", "=", "max", "(", "\n", "self", ".", "epsilon_finish", ",", "\n", "self", ".", "epsilon_start", "\n", "-", "(", "t", "/", "self", ".", "epsilon_time_length", ")", "\n", "*", "(", "self", ".", "epsilon_start", "-", "self", ".", "epsilon_finish", ")", ",", "\n", ")", "\n", "self", ".", "logger", ".", "debug", "(", "\"Epsilon: {:.2f}\"", ".", "format", "(", "epsilon", ")", ")", "\n", "if", "t", "%", "1000", "==", "0", ":", "\n", "            ", "self", ".", "logger", ".", "info", "(", "\"Epsilon: {:.2f}\"", ".", "format", "(", "epsilon", ")", ")", "\n", "", "self", ".", "log_stats", "(", "epsilon", ")", "\n", "\n", "if", "np", ".", "random", ".", "random", "(", ")", "<", "epsilon", ":", "\n", "# Random action", "\n", "            ", "action", "=", "np", ".", "random", ".", "randint", "(", "self", ".", "num_actions", ")", "\n", "self", ".", "logger", ".", "debug", "(", "\"Random action selected\"", ")", "\n", "", "else", ":", "\n", "# Argmax over Q-Values", "\n", "            ", "action", "=", "q_values", ".", "argmax", "(", ")", ".", "cpu", "(", ")", ".", "item", "(", ")", "\n", "self", ".", "logger", ".", "debug", "(", "\"Argmax action over Q-Values selected\"", ")", "\n", "\n", "", "return", "action", ",", "{", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.action.eps_greedy.EpsGreedy.log_stats": [[42, 44], ["eps_greedy.EpsGreedy.stats.update_stats"], "methods", ["home.repos.pwc.inspect_result.oxwhirl_opiq.utils.logging.Stats.update_stats"], ["", "def", "log_stats", "(", "self", ",", "epsilon", ")", ":", "\n", "        ", "self", ".", "stats", ".", "update_stats", "(", "\"Epsilon\"", ",", "epsilon", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.oxwhirl_opiq.action.optimistic_action.OptimisticAction.__init__": [[9, 24], ["utils.logging.get_stats", "logging.getLogger"], "methods", ["home.repos.pwc.inspect_result.oxwhirl_opiq.utils.logging.get_stats"], ["    ", "def", "__init__", "(", "self", ",", "count_model", ",", "args", ")", ":", "\n", "\n", "        ", "self", ".", "epsilon_start", "=", "args", ".", "epsilon_start", "\n", "self", ".", "epsilon_finish", "=", "args", ".", "epsilon_finish", "\n", "self", ".", "epsilon_time_length", "=", "args", ".", "epsilon_time_length", "\n", "\n", "self", ".", "num_actions", "=", "args", ".", "num_actions", "\n", "self", ".", "stats", "=", "get_stats", "(", ")", "\n", "self", ".", "logger", "=", "logging", ".", "getLogger", "(", "\"OptimisticAction\"", ")", "\n", "\n", "self", ".", "count_model", "=", "count_model", "\n", "self", ".", "m", "=", "args", ".", "optim_m", "\n", "self", ".", "tau", "=", "args", ".", "optim_action_tau", "\n", "\n", "self", ".", "config", "=", "args", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.action.optimistic_action.OptimisticAction.select_actions": [[25, 76], ["max", "optimistic_action.OptimisticAction.stats.update_stats", "optimistic_action.OptimisticAction.logger.debug", "numpy.random.randint", "optimistic_action.OptimisticAction.logger.debug", "[].cpu().numpy", "getattr", "numpy.argmax", "optimistic_action.OptimisticAction.logger.info", "numpy.random.random", "optimistic_action.OptimisticAction.count_model.get_all_action_counts", "optimistic_action.OptimisticAction.logger.debug", "[].cpu", "optims.detach().cpu().numpy.detach().cpu().numpy.detach().cpu().numpy", "range", "optimistic_action.OptimisticAction.stats.update_stats", "optimistic_action.OptimisticAction.stats.update_stats", "numpy.mean", "optims.detach().cpu().numpy.detach().cpu().numpy.detach().cpu", "optims.detach().cpu().numpy.detach().cpu().numpy.detach"], "methods", ["home.repos.pwc.inspect_result.oxwhirl_opiq.utils.logging.Stats.update_stats", "home.repos.pwc.inspect_result.oxwhirl_opiq.count.dora_count.DoraCount.get_all_action_counts", "home.repos.pwc.inspect_result.oxwhirl_opiq.utils.logging.Stats.update_stats", "home.repos.pwc.inspect_result.oxwhirl_opiq.utils.logging.Stats.update_stats"], ["", "def", "select_actions", "(", "self", ",", "q_values", ",", "t", ",", "info", "=", "{", "}", ",", "testing", "=", "False", ")", ":", "\n", "\n", "        ", "state", "=", "info", "[", "\"state\"", "]", "\n", "\n", "epsilon", "=", "max", "(", "\n", "self", ".", "epsilon_finish", ",", "\n", "self", ".", "epsilon_start", "\n", "-", "(", "t", "/", "self", ".", "epsilon_time_length", ")", "\n", "*", "(", "self", ".", "epsilon_start", "-", "self", ".", "epsilon_finish", ")", ",", "\n", ")", "\n", "if", "not", "testing", ":", "\n", "            ", "self", ".", "logger", ".", "debug", "(", "\"Epsilon: {:.2f}\"", ".", "format", "(", "epsilon", ")", ")", "\n", "if", "t", "%", "1000", "==", "0", ":", "\n", "                ", "self", ".", "logger", ".", "info", "(", "\"Epsilon: {:.2f}\"", ".", "format", "(", "epsilon", ")", ")", "\n", "\n", "", "", "if", "not", "testing", "and", "np", ".", "random", ".", "random", "(", ")", "<", "epsilon", ":", "\n", "# Random action", "\n", "            ", "action", "=", "np", ".", "random", ".", "randint", "(", "self", ".", "num_actions", ")", "\n", "self", ".", "logger", ".", "debug", "(", "\"Random action selected\"", ")", "\n", "", "else", ":", "\n", "            ", "q_vals_copy", "=", "(", "q_values", "+", "0", ")", ".", "detach", "(", ")", "[", "0", "]", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "state_action_counts", "=", "self", ".", "count_model", ".", "get_all_action_counts", "(", "state", ")", "[", ":", ",", "0", "]", "\n", "if", "getattr", "(", "self", ".", "count_model", ",", "\"reward_directly\"", ",", "False", ")", ":", "\n", "                ", "if", "self", ".", "config", ".", "dora_count", ":", "\n", "                    ", "optims", "=", "state_action_counts", "\n", "optims", "=", "optims", "*", "self", ".", "config", ".", "dora_action", "/", "self", ".", "config", ".", "dora_beta", "\n", "optims", "=", "optims", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "", "elif", "self", ".", "config", ".", "rnd_net_count", ":", "\n", "                    ", "optims", "=", "state_action_counts", "\n", "optims", "=", "optims", "*", "self", ".", "tau", "/", "self", ".", "config", ".", "rnd_net_scaler", "\n", "", "", "else", ":", "\n", "                ", "optims", "=", "self", ".", "tau", "/", "(", "(", "state_action_counts", "+", "1.0", ")", "**", "self", ".", "m", ")", "\n", "# optims = np.concatenate([self.tau / np.power(self.count_model.get_count(state, a) + 1, self.m) for a in range(self.num_actions)])", "\n", "# np_optims = np.array(optims)", "\n", "\n", "", "optim_q_vals", "=", "q_vals_copy", "+", "optims", "\n", "if", "self", ".", "config", ".", "optim_interpolation", ":", "\n", "                ", "w", "=", "1", "/", "(", "(", "state_action_counts", "+", "1.0", ")", "**", "self", ".", "m", ")", "\n", "optim_q_vals", "=", "q_vals_copy", "*", "(", "1", "-", "w", ")", "+", "self", ".", "tau", "*", "(", "w", ")", "\n", "", "action", "=", "np", ".", "argmax", "(", "optim_q_vals", ")", "\n", "if", "not", "testing", ":", "\n", "                ", "self", ".", "logger", ".", "debug", "(", "\"Argmax action over Q-Values selected\"", ")", "\n", "if", "self", ".", "num_actions", "<", "20", ":", "\n", "                    ", "for", "a", "in", "range", "(", "self", ".", "num_actions", ")", ":", "\n", "                        ", "self", ".", "stats", ".", "update_stats", "(", "\"Optim_Q_Value_{}\"", ".", "format", "(", "a", ")", ",", "optim_q_vals", "[", "a", "]", ")", "\n", "", "", "else", ":", "\n", "                    ", "self", ".", "stats", ".", "update_stats", "(", "\"Optim_Mean_Q_Value\"", ",", "np", ".", "mean", "(", "optim_q_vals", ")", ")", "\n", "\n", "", "", "", "self", ".", "stats", ".", "update_stats", "(", "\"Epsilon\"", ",", "epsilon", ")", "\n", "\n", "return", "action", ",", "{", "}", "\n", "", "", ""]], "home.repos.pwc.inspect_result.oxwhirl_opiq.action.bsp_action.BSPAction.__init__": [[9, 22], ["utils.logging.get_stats", "logging.getLogger"], "methods", ["home.repos.pwc.inspect_result.oxwhirl_opiq.utils.logging.get_stats"], ["    ", "def", "__init__", "(", "self", ",", "args", ")", ":", "\n", "\n", "        ", "self", ".", "epsilon_start", "=", "args", ".", "epsilon_start", "\n", "self", ".", "epsilon_finish", "=", "args", ".", "epsilon_finish", "\n", "self", ".", "epsilon_time_length", "=", "args", ".", "epsilon_time_length", "\n", "\n", "self", ".", "num_actions", "=", "args", ".", "num_actions", "\n", "self", ".", "stats", "=", "get_stats", "(", ")", "\n", "self", ".", "logger", "=", "logging", ".", "getLogger", "(", "\"BSPAction\"", ")", "\n", "\n", "self", ".", "config", "=", "args", "\n", "\n", "self", ".", "current_k", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.action.bsp_action.BSPAction.update_k": [[23, 25], ["None"], "methods", ["None"], ["", "def", "update_k", "(", "self", ",", "k", ")", ":", "\n", "        ", "self", ".", "current_k", "=", "k", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.action.bsp_action.BSPAction.select_actions": [[26, 67], ["max", "bsp_action.BSPAction.stats.update_stats", "bsp_action.BSPAction.logger.debug", "numpy.random.randint", "bsp_action.BSPAction.logger.debug", "bsp_action.BSPAction.logger.info", "numpy.random.random", "numpy.array", "range", "numpy.random.choice", "[].cpu().numpy", "numpy.argmax", "numpy.argmax", "numpy.flatnonzero", "bsp_action.BSPAction.logger.debug", "[].cpu().numpy", "[].cpu", "range", "bsp_action.BSPAction.stats.update_stats", "range", "numpy.array.max", "bsp_action.BSPAction.stats.update_stats", "numpy.mean", "[].cpu", "q_values[].detach", "q_values.detach"], "methods", ["home.repos.pwc.inspect_result.oxwhirl_opiq.utils.logging.Stats.update_stats", "home.repos.pwc.inspect_result.oxwhirl_opiq.utils.logging.Stats.update_stats", "home.repos.pwc.inspect_result.oxwhirl_opiq.utils.logging.Stats.update_stats"], ["", "def", "select_actions", "(", "self", ",", "q_values", ",", "t", ",", "info", "=", "{", "}", ",", "testing", "=", "False", ")", ":", "\n", "\n", "        ", "state", "=", "info", "[", "\"state\"", "]", "\n", "\n", "epsilon", "=", "max", "(", "\n", "self", ".", "epsilon_finish", ",", "\n", "self", ".", "epsilon_start", "\n", "-", "(", "t", "/", "self", ".", "epsilon_time_length", ")", "\n", "*", "(", "self", ".", "epsilon_start", "-", "self", ".", "epsilon_finish", ")", ",", "\n", ")", "\n", "if", "not", "testing", ":", "\n", "            ", "self", ".", "logger", ".", "debug", "(", "\"Epsilon: {:.2f}\"", ".", "format", "(", "epsilon", ")", ")", "\n", "if", "t", "%", "1000", "==", "0", ":", "\n", "                ", "self", ".", "logger", ".", "info", "(", "\"Epsilon: {:.2f}\"", ".", "format", "(", "epsilon", ")", ")", "\n", "\n", "", "", "if", "not", "testing", "and", "np", ".", "random", ".", "random", "(", ")", "<", "epsilon", ":", "\n", "# Random action", "\n", "            ", "action", "=", "np", ".", "random", ".", "randint", "(", "self", ".", "num_actions", ")", "\n", "self", ".", "logger", ".", "debug", "(", "\"Random action selected\"", ")", "\n", "", "elif", "testing", ":", "\n", "# Majority vote", "\n", "            ", "action_votes", "=", "np", ".", "array", "(", "[", "0", "for", "_", "in", "range", "(", "self", ".", "num_actions", ")", "]", ")", "\n", "for", "k", "in", "range", "(", "self", ".", "config", ".", "bsp_k", ")", ":", "\n", "                ", "argmax_action", "=", "np", ".", "argmax", "(", "q_values", ".", "detach", "(", ")", "[", "0", ",", ":", ",", "k", "]", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ")", "\n", "action_votes", "[", "argmax_action", "]", "+=", "1", "\n", "# Random tie breaking", "\n", "", "action", "=", "np", ".", "random", ".", "choice", "(", "np", ".", "flatnonzero", "(", "action_votes", "==", "action_votes", ".", "max", "(", ")", ")", ")", "\n", "", "else", ":", "\n", "            ", "q_vals", "=", "q_values", "[", ":", ",", ":", ",", "self", ".", "current_k", "]", ".", "detach", "(", ")", "[", "0", "]", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "action", "=", "np", ".", "argmax", "(", "q_vals", ")", "\n", "if", "not", "testing", ":", "\n", "                ", "self", ".", "logger", ".", "debug", "(", "\"Argmax action over Q-Values selected\"", ")", "\n", "if", "self", ".", "num_actions", "<", "20", ":", "\n", "                    ", "for", "a", "in", "range", "(", "self", ".", "num_actions", ")", ":", "\n", "                        ", "self", ".", "stats", ".", "update_stats", "(", "\"BSP_Q_Value_{}\"", ".", "format", "(", "a", ")", ",", "q_vals", "[", "a", "]", ")", "\n", "", "", "else", ":", "\n", "                    ", "self", ".", "stats", ".", "update_stats", "(", "\"BSP_Mean_Q_Value\"", ",", "np", ".", "mean", "(", "q_vals", ")", ")", "\n", "\n", "", "", "", "self", ".", "stats", ".", "update_stats", "(", "\"Epsilon\"", ",", "epsilon", ")", "\n", "\n", "return", "action", ",", "{", "}", "\n", "", "", ""]], "home.repos.pwc.inspect_result.oxwhirl_opiq.action.count_action.CountBonus.__init__": [[8, 23], ["utils.logging.get_stats", "logging.getLogger"], "methods", ["home.repos.pwc.inspect_result.oxwhirl_opiq.utils.logging.get_stats"], ["    ", "def", "__init__", "(", "self", ",", "count_model", ",", "args", ")", ":", "\n", "\n", "        ", "self", ".", "epsilon_start", "=", "args", ".", "epsilon_start", "\n", "self", ".", "epsilon_finish", "=", "args", ".", "epsilon_finish", "\n", "self", ".", "epsilon_time_length", "=", "args", ".", "epsilon_time_length", "\n", "\n", "self", ".", "num_actions", "=", "args", ".", "num_actions", "\n", "self", ".", "stats", "=", "get_stats", "(", ")", "\n", "self", ".", "logger", "=", "logging", ".", "getLogger", "(", "\"CountBonus\"", ")", "\n", "\n", "self", ".", "min_q", "=", "1000", "\n", "self", ".", "max_q", "=", "-", "1000", "\n", "\n", "self", ".", "count_model", "=", "count_model", "\n", "self", ".", "args", "=", "args", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.action.count_action.CountBonus.select_actions": [[24, 71], ["max", "count_action.CountBonus.logger.debug", "count_action.CountBonus.log_stats", "q_values[].detach().numpy", "max", "min", "count_action.CountBonus.logger.info", "info[].max", "info[].min", "numpy.random.random", "numpy.random.randint", "count_action.CountBonus.logger.debug", "range", "q_values.argmax().cpu().item", "count_action.CountBonus.logger.debug", "q_values[].detach", "count_action.CountBonus.count_model.bonus", "action_bonuses.append", "numpy.sqrt", "q_values.argmax().cpu", "q_values.argmax"], "methods", ["home.repos.pwc.inspect_result.oxwhirl_opiq.action.count_action.CountBonus.log_stats"], ["", "def", "select_actions", "(", "self", ",", "q_values", ",", "t", ",", "info", ")", ":", "\n", "\n", "        ", "state", "=", "info", "[", "\"state\"", "]", "\n", "\n", "epsilon", "=", "max", "(", "\n", "self", ".", "epsilon_finish", ",", "\n", "self", ".", "epsilon_start", "\n", "-", "(", "t", "/", "self", ".", "epsilon_time_length", ")", "\n", "*", "(", "self", ".", "epsilon_start", "-", "self", ".", "epsilon_finish", ")", ",", "\n", ")", "\n", "self", ".", "logger", ".", "debug", "(", "\"Epsilon: {:.2f}\"", ".", "format", "(", "epsilon", ")", ")", "\n", "if", "t", "%", "1000", "==", "0", ":", "\n", "            ", "self", ".", "logger", ".", "info", "(", "\"Epsilon: {:.2f}\"", ".", "format", "(", "epsilon", ")", ")", "\n", "", "self", ".", "log_stats", "(", "epsilon", ")", "\n", "\n", "info", "=", "{", "}", "\n", "info", "[", "\"Q_Values\"", "]", "=", "q_values", "[", "0", "]", ".", "detach", "(", ")", ".", "numpy", "(", ")", "\n", "self", ".", "max_q", "=", "max", "(", "self", ".", "max_q", "*", "0.99999999", ",", "info", "[", "\"Q_Values\"", "]", ".", "max", "(", ")", ")", "\n", "self", ".", "min_q", "=", "min", "(", "self", ".", "min_q", "*", "0.99999999", ",", "info", "[", "\"Q_Values\"", "]", ".", "min", "(", ")", ")", "\n", "info", "[", "\"Max_Q_Value\"", "]", "=", "self", ".", "max_q", "\n", "info", "[", "\"Min_Q_Value\"", "]", "=", "self", ".", "min_q", "\n", "\n", "if", "np", ".", "random", ".", "random", "(", ")", "<", "epsilon", ":", "\n", "# Random action", "\n", "            ", "action", "=", "np", ".", "random", ".", "randint", "(", "self", ".", "num_actions", ")", "\n", "self", ".", "logger", ".", "debug", "(", "\"Random action selected\"", ")", "\n", "", "else", ":", "\n", "            ", "action_bonuses", "=", "[", "]", "\n", "# Argmax over Q-Values with the count bonus added", "\n", "for", "a", "in", "range", "(", "self", ".", "args", ".", "num_actions", ")", ":", "\n", "                ", "_", ",", "action_info", "=", "self", ".", "count_model", ".", "bonus", "(", "state", ",", "a", ",", "dont_remember", "=", "True", ")", "\n", "pseudo_count", "=", "action_info", "[", "\"Pseudo_Count\"", "]", "\n", "\n", "action_bonus", "=", "self", ".", "args", ".", "action_bonus_scaler", "/", "np", ".", "sqrt", "(", "\n", "pseudo_count", "+", "0.01", "\n", ")", "\n", "q_values", "[", "0", ",", "a", "]", "+=", "action_bonus", "\n", "action_bonuses", ".", "append", "(", "action_bonus", ")", "\n", "\n", "", "action", "=", "q_values", ".", "argmax", "(", ")", ".", "cpu", "(", ")", ".", "item", "(", ")", "\n", "self", ".", "logger", ".", "debug", "(", "\"Argmax action over Q-Values selected\"", ")", "\n", "info", "[", "\"Action_Bonus\"", "]", "=", "action_bonuses", "\n", "\n", "", "info", "[", "\"Action\"", "]", "=", "action", "\n", "info", "[", "\"Epsilon\"", "]", "=", "epsilon", "\n", "\n", "return", "action", ",", "info", "\n", "\n"]], "home.repos.pwc.inspect_result.oxwhirl_opiq.action.count_action.CountBonus.log_stats": [[72, 74], ["count_action.CountBonus.stats.update_stats"], "methods", ["home.repos.pwc.inspect_result.oxwhirl_opiq.utils.logging.Stats.update_stats"], ["", "def", "log_stats", "(", "self", ",", "epsilon", ")", ":", "\n", "        ", "self", ".", "stats", ".", "update_stats", "(", "\"Epsilon\"", ",", "epsilon", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.oxwhirl_opiq.action.testing.get_test_action": [[4, 11], ["numpy.random.random", "numpy.random.randint", "agent_q_values.argmax().cpu().item", "agent_q_values.argmax().cpu", "agent_q_values.argmax"], "function", ["None"], ["def", "get_test_action", "(", "agent_q_values", ",", "args", ")", ":", "\n", "    ", "action", "=", "None", "\n", "if", "np", ".", "random", ".", "random", "(", ")", "<", "args", ".", "test_epsilon", ":", "\n", "        ", "action", "=", "np", ".", "random", ".", "randint", "(", "args", ".", "num_actions", ")", "\n", "", "else", ":", "\n", "        ", "action", "=", "agent_q_values", ".", "argmax", "(", ")", ".", "cpu", "(", ")", ".", "item", "(", ")", "\n", "", "return", "action", "\n", "", ""]]}