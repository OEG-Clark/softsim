{"home.repos.pwc.inspect_result.noc-lab_clinical_concept_extraction.clinical_concept_extraction.bilm.Vocabulary.__init__": [[23, 56], ["open", "line.strip", "bilm.Vocabulary._id_to_word.append", "ValueError"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "filename", ",", "validate_file", "=", "False", ")", ":", "\n", "        ", "'''\n        filename = the vocabulary file.  It is a flat text file with one\n            (normalized) token per line.  In addition, the file should also\n            contain the special tokens <S>, </S>, <UNK> (case sensitive).\n        '''", "\n", "self", ".", "_id_to_word", "=", "[", "]", "\n", "self", ".", "_word_to_id", "=", "{", "}", "\n", "self", ".", "_unk", "=", "-", "1", "\n", "self", ".", "_bos", "=", "-", "1", "\n", "self", ".", "_eos", "=", "-", "1", "\n", "\n", "with", "open", "(", "filename", ")", "as", "f", ":", "\n", "            ", "idx", "=", "0", "\n", "for", "line", "in", "f", ":", "\n", "                ", "word_name", "=", "line", ".", "strip", "(", ")", "\n", "if", "word_name", "==", "'<S>'", ":", "\n", "                    ", "self", ".", "_bos", "=", "idx", "\n", "", "elif", "word_name", "==", "'</S>'", ":", "\n", "                    ", "self", ".", "_eos", "=", "idx", "\n", "", "elif", "word_name", "==", "'<UNK>'", ":", "\n", "                    ", "self", ".", "_unk", "=", "idx", "\n", "", "if", "word_name", "==", "'!!!MAXTERMID'", ":", "\n", "                    ", "continue", "\n", "\n", "", "self", ".", "_id_to_word", ".", "append", "(", "word_name", ")", "\n", "self", ".", "_word_to_id", "[", "word_name", "]", "=", "idx", "\n", "idx", "+=", "1", "\n", "\n", "# check to ensure file has special tokens", "\n", "", "", "if", "validate_file", ":", "\n", "            ", "if", "self", ".", "_bos", "==", "-", "1", "or", "self", ".", "_eos", "==", "-", "1", "or", "self", ".", "_unk", "==", "-", "1", ":", "\n", "                ", "raise", "ValueError", "(", "\"Ensure the vocabulary file has \"", "\n", "\"<S>, </S>, <UNK> tokens\"", ")", "\n"]], "home.repos.pwc.inspect_result.noc-lab_clinical_concept_extraction.clinical_concept_extraction.bilm.Vocabulary.bos": [[58, 61], ["None"], "methods", ["None"], ["", "", "", "@", "property", "\n", "def", "bos", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "_bos", "\n", "\n"]], "home.repos.pwc.inspect_result.noc-lab_clinical_concept_extraction.clinical_concept_extraction.bilm.Vocabulary.eos": [[62, 65], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "eos", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "_eos", "\n", "\n"]], "home.repos.pwc.inspect_result.noc-lab_clinical_concept_extraction.clinical_concept_extraction.bilm.Vocabulary.unk": [[66, 69], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "unk", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "_unk", "\n", "\n"]], "home.repos.pwc.inspect_result.noc-lab_clinical_concept_extraction.clinical_concept_extraction.bilm.Vocabulary.size": [[70, 73], ["len"], "methods", ["None"], ["", "@", "property", "\n", "def", "size", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "_id_to_word", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.noc-lab_clinical_concept_extraction.clinical_concept_extraction.bilm.Vocabulary.word_to_id": [[74, 78], ["None"], "methods", ["None"], ["", "def", "word_to_id", "(", "self", ",", "word", ")", ":", "\n", "        ", "if", "word", "in", "self", ".", "_word_to_id", ":", "\n", "            ", "return", "self", ".", "_word_to_id", "[", "word", "]", "\n", "", "return", "self", ".", "unk", "\n", "\n"]], "home.repos.pwc.inspect_result.noc-lab_clinical_concept_extraction.clinical_concept_extraction.bilm.Vocabulary.id_to_word": [[79, 81], ["None"], "methods", ["None"], ["", "def", "id_to_word", "(", "self", ",", "cur_id", ")", ":", "\n", "        ", "return", "self", ".", "_id_to_word", "[", "cur_id", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.noc-lab_clinical_concept_extraction.clinical_concept_extraction.bilm.Vocabulary.decode": [[82, 85], ["bilm.Vocabulary.id_to_word"], "methods", ["home.repos.pwc.inspect_result.noc-lab_clinical_concept_extraction.clinical_concept_extraction.bilm.Vocabulary.id_to_word"], ["", "def", "decode", "(", "self", ",", "cur_ids", ")", ":", "\n", "        ", "\"\"\"Convert a list of ids to a sentence, with space inserted.\"\"\"", "\n", "return", "' '", ".", "join", "(", "[", "self", ".", "id_to_word", "(", "cur_id", ")", "for", "cur_id", "in", "cur_ids", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.noc-lab_clinical_concept_extraction.clinical_concept_extraction.bilm.Vocabulary.encode": [[86, 104], ["numpy.array", "numpy.array", "bilm.Vocabulary.word_to_id", "bilm.Vocabulary.word_to_id", "sentence.split"], "methods", ["home.repos.pwc.inspect_result.noc-lab_clinical_concept_extraction.clinical_concept_extraction.bilm.Vocabulary.word_to_id", "home.repos.pwc.inspect_result.noc-lab_clinical_concept_extraction.clinical_concept_extraction.bilm.Vocabulary.word_to_id"], ["", "def", "encode", "(", "self", ",", "sentence", ",", "reverse", "=", "False", ",", "split", "=", "True", ")", ":", "\n", "        ", "\"\"\"Convert a sentence to a list of ids, with special tokens added.\n        Sentence is a single string with tokens separated by whitespace.\n\n        If reverse, then the sentence is assumed to be reversed, and\n            this method will swap the BOS/EOS tokens appropriately.\"\"\"", "\n", "\n", "if", "split", ":", "\n", "            ", "word_ids", "=", "[", "\n", "self", ".", "word_to_id", "(", "cur_word", ")", "for", "cur_word", "in", "sentence", ".", "split", "(", ")", "\n", "]", "\n", "", "else", ":", "\n", "            ", "word_ids", "=", "[", "self", ".", "word_to_id", "(", "cur_word", ")", "for", "cur_word", "in", "sentence", "]", "\n", "\n", "", "if", "reverse", ":", "\n", "            ", "return", "np", ".", "array", "(", "[", "self", ".", "eos", "]", "+", "word_ids", "+", "[", "self", ".", "bos", "]", ",", "dtype", "=", "np", ".", "int32", ")", "\n", "", "else", ":", "\n", "            ", "return", "np", ".", "array", "(", "[", "self", ".", "bos", "]", "+", "word_ids", "+", "[", "self", ".", "eos", "]", ",", "dtype", "=", "np", ".", "int32", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.noc-lab_clinical_concept_extraction.clinical_concept_extraction.bilm.UnicodeCharsVocabulary.__init__": [[125, 159], ["bilm.Vocabulary.__init__", "len", "numpy.zeros", "bilm.UnicodeCharsVocabulary.__init__._make_bos_eos"], "methods", ["home.repos.pwc.inspect_result.noc-lab_clinical_concept_extraction.clinical_concept_extraction.elmo_vector.ELMO_MIMIC.__init__"], ["def", "__init__", "(", "self", ",", "filename", ",", "max_word_length", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", "UnicodeCharsVocabulary", ",", "self", ")", ".", "__init__", "(", "filename", ",", "**", "kwargs", ")", "\n", "self", ".", "_max_word_length", "=", "max_word_length", "\n", "\n", "# char ids 0-255 come from utf-8 encoding bytes", "\n", "# assign 256-300 to special chars", "\n", "self", ".", "bos_char", "=", "256", "# <begin sentence>", "\n", "self", ".", "eos_char", "=", "257", "# <end sentence>", "\n", "self", ".", "bow_char", "=", "258", "# <begin word>", "\n", "self", ".", "eow_char", "=", "259", "# <end word>", "\n", "self", ".", "pad_char", "=", "260", "# <padding>", "\n", "\n", "num_words", "=", "len", "(", "self", ".", "_id_to_word", ")", "\n", "\n", "self", ".", "_word_char_ids", "=", "np", ".", "zeros", "(", "[", "num_words", ",", "max_word_length", "]", ",", "\n", "dtype", "=", "np", ".", "int32", ")", "\n", "\n", "# the charcter representation of the begin/end of sentence characters", "\n", "def", "_make_bos_eos", "(", "c", ")", ":", "\n", "            ", "r", "=", "np", ".", "zeros", "(", "[", "self", ".", "max_word_length", "]", ",", "dtype", "=", "np", ".", "int32", ")", "\n", "r", "[", ":", "]", "=", "self", ".", "pad_char", "\n", "r", "[", "0", "]", "=", "self", ".", "bow_char", "\n", "r", "[", "1", "]", "=", "c", "\n", "r", "[", "2", "]", "=", "self", ".", "eow_char", "\n", "return", "r", "\n", "\n", "", "self", ".", "bos_chars", "=", "_make_bos_eos", "(", "self", ".", "bos_char", ")", "\n", "self", ".", "eos_chars", "=", "_make_bos_eos", "(", "self", ".", "eos_char", ")", "\n", "\n", "for", "i", ",", "word", "in", "enumerate", "(", "self", ".", "_id_to_word", ")", ":", "\n", "            ", "self", ".", "_word_char_ids", "[", "i", "]", "=", "self", ".", "_convert_word_to_char_ids", "(", "word", ")", "\n", "\n", "", "self", ".", "_word_char_ids", "[", "self", ".", "bos", "]", "=", "self", ".", "bos_chars", "\n", "self", ".", "_word_char_ids", "[", "self", ".", "eos", "]", "=", "self", ".", "eos_chars", "\n", "# TODO: properly handle <UNK>", "\n"]], "home.repos.pwc.inspect_result.noc-lab_clinical_concept_extraction.clinical_concept_extraction.bilm.UnicodeCharsVocabulary.word_char_ids": [[161, 164], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "word_char_ids", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "_word_char_ids", "\n", "\n"]], "home.repos.pwc.inspect_result.noc-lab_clinical_concept_extraction.clinical_concept_extraction.bilm.UnicodeCharsVocabulary.max_word_length": [[165, 168], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "max_word_length", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "_max_word_length", "\n", "\n"]], "home.repos.pwc.inspect_result.noc-lab_clinical_concept_extraction.clinical_concept_extraction.bilm.UnicodeCharsVocabulary._convert_word_to_char_ids": [[169, 180], ["numpy.zeros", "enumerate", "word.encode", "len"], "methods", ["home.repos.pwc.inspect_result.noc-lab_clinical_concept_extraction.clinical_concept_extraction.bilm.Vocabulary.encode"], ["", "def", "_convert_word_to_char_ids", "(", "self", ",", "word", ")", ":", "\n", "        ", "code", "=", "np", ".", "zeros", "(", "[", "self", ".", "max_word_length", "]", ",", "dtype", "=", "np", ".", "int32", ")", "\n", "code", "[", ":", "]", "=", "self", ".", "pad_char", "\n", "\n", "word_encoded", "=", "word", ".", "encode", "(", "'utf-8'", ",", "'ignore'", ")", "[", ":", "(", "self", ".", "max_word_length", "-", "2", ")", "]", "\n", "code", "[", "0", "]", "=", "self", ".", "bow_char", "\n", "for", "k", ",", "chr_id", "in", "enumerate", "(", "word_encoded", ",", "start", "=", "1", ")", ":", "\n", "            ", "code", "[", "k", "]", "=", "chr_id", "\n", "", "code", "[", "len", "(", "word_encoded", ")", "+", "1", "]", "=", "self", ".", "eow_char", "\n", "\n", "return", "code", "\n", "\n"]], "home.repos.pwc.inspect_result.noc-lab_clinical_concept_extraction.clinical_concept_extraction.bilm.UnicodeCharsVocabulary.word_to_char_ids": [[181, 186], ["bilm.UnicodeCharsVocabulary._convert_word_to_char_ids"], "methods", ["home.repos.pwc.inspect_result.noc-lab_clinical_concept_extraction.clinical_concept_extraction.bilm.UnicodeCharsVocabulary._convert_word_to_char_ids"], ["", "def", "word_to_char_ids", "(", "self", ",", "word", ")", ":", "\n", "        ", "if", "word", "in", "self", ".", "_word_to_id", ":", "\n", "            ", "return", "self", ".", "_word_char_ids", "[", "self", ".", "_word_to_id", "[", "word", "]", "]", "\n", "", "else", ":", "\n", "            ", "return", "self", ".", "_convert_word_to_char_ids", "(", "word", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.noc-lab_clinical_concept_extraction.clinical_concept_extraction.bilm.UnicodeCharsVocabulary.encode_chars": [[187, 201], ["numpy.vstack", "numpy.vstack", "bilm.UnicodeCharsVocabulary.word_to_char_ids", "bilm.UnicodeCharsVocabulary.word_to_char_ids", "sentence.split"], "methods", ["home.repos.pwc.inspect_result.noc-lab_clinical_concept_extraction.clinical_concept_extraction.bilm.UnicodeCharsVocabulary.word_to_char_ids", "home.repos.pwc.inspect_result.noc-lab_clinical_concept_extraction.clinical_concept_extraction.bilm.UnicodeCharsVocabulary.word_to_char_ids"], ["", "", "def", "encode_chars", "(", "self", ",", "sentence", ",", "reverse", "=", "False", ",", "split", "=", "True", ")", ":", "\n", "        ", "'''\n        Encode the sentence as a white space delimited string of tokens.\n        '''", "\n", "if", "split", ":", "\n", "            ", "chars_ids", "=", "[", "self", ".", "word_to_char_ids", "(", "cur_word", ")", "\n", "for", "cur_word", "in", "sentence", ".", "split", "(", ")", "]", "\n", "", "else", ":", "\n", "            ", "chars_ids", "=", "[", "self", ".", "word_to_char_ids", "(", "cur_word", ")", "\n", "for", "cur_word", "in", "sentence", "]", "\n", "", "if", "reverse", ":", "\n", "            ", "return", "np", ".", "vstack", "(", "[", "self", ".", "eos_chars", "]", "+", "chars_ids", "+", "[", "self", ".", "bos_chars", "]", ")", "\n", "", "else", ":", "\n", "            ", "return", "np", ".", "vstack", "(", "[", "self", ".", "bos_chars", "]", "+", "chars_ids", "+", "[", "self", ".", "eos_chars", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.noc-lab_clinical_concept_extraction.clinical_concept_extraction.bilm.Batcher.__init__": [[208, 218], ["bilm.UnicodeCharsVocabulary"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "lm_vocab_file", ":", "str", ",", "max_token_length", ":", "int", ")", ":", "\n", "        ", "'''\n        lm_vocab_file = the language model vocabulary file (one line per\n            token)\n        max_token_length = the maximum number of characters in each token\n        '''", "\n", "self", ".", "_lm_vocab", "=", "UnicodeCharsVocabulary", "(", "\n", "lm_vocab_file", ",", "max_token_length", "\n", ")", "\n", "self", ".", "_max_token_length", "=", "max_token_length", "\n", "\n"]], "home.repos.pwc.inspect_result.noc-lab_clinical_concept_extraction.clinical_concept_extraction.bilm.Batcher.batch_sentences": [[219, 241], ["len", "numpy.zeros", "enumerate", "max", "bilm.Batcher._lm_vocab.encode_chars", "len", "len"], "methods", ["home.repos.pwc.inspect_result.noc-lab_clinical_concept_extraction.clinical_concept_extraction.bilm.UnicodeCharsVocabulary.encode_chars"], ["", "def", "batch_sentences", "(", "self", ",", "sentences", ":", "List", "[", "List", "[", "str", "]", "]", ")", ":", "\n", "        ", "'''\n        Batch the sentences as character ids\n        Each sentence is a list of tokens without <s> or </s>, e.g.\n        [['The', 'first', 'sentence', '.'], ['Second', '.']]\n        '''", "\n", "n_sentences", "=", "len", "(", "sentences", ")", "\n", "max_length", "=", "max", "(", "len", "(", "sentence", ")", "for", "sentence", "in", "sentences", ")", "+", "2", "\n", "\n", "X_char_ids", "=", "np", ".", "zeros", "(", "\n", "(", "n_sentences", ",", "max_length", ",", "self", ".", "_max_token_length", ")", ",", "\n", "dtype", "=", "np", ".", "int64", "\n", ")", "\n", "\n", "for", "k", ",", "sent", "in", "enumerate", "(", "sentences", ")", ":", "\n", "            ", "length", "=", "len", "(", "sent", ")", "+", "2", "\n", "char_ids_without_mask", "=", "self", ".", "_lm_vocab", ".", "encode_chars", "(", "\n", "sent", ",", "split", "=", "False", ")", "\n", "# add one so that 0 is the mask value", "\n", "X_char_ids", "[", "k", ",", ":", "length", ",", ":", "]", "=", "char_ids_without_mask", "+", "1", "\n", "\n", "", "return", "X_char_ids", "\n", "\n"]], "home.repos.pwc.inspect_result.noc-lab_clinical_concept_extraction.clinical_concept_extraction.bilm.TokenBatcher.__init__": [[248, 254], ["bilm.Vocabulary"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "lm_vocab_file", ":", "str", ")", ":", "\n", "        ", "'''\n        lm_vocab_file = the language model vocabulary file (one line per\n            token)\n        '''", "\n", "self", ".", "_lm_vocab", "=", "Vocabulary", "(", "lm_vocab_file", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.noc-lab_clinical_concept_extraction.clinical_concept_extraction.bilm.TokenBatcher.batch_sentences": [[255, 273], ["len", "numpy.zeros", "enumerate", "max", "bilm.TokenBatcher._lm_vocab.encode", "len", "len"], "methods", ["home.repos.pwc.inspect_result.noc-lab_clinical_concept_extraction.clinical_concept_extraction.bilm.Vocabulary.encode"], ["", "def", "batch_sentences", "(", "self", ",", "sentences", ":", "List", "[", "List", "[", "str", "]", "]", ")", ":", "\n", "        ", "'''\n        Batch the sentences as character ids\n        Each sentence is a list of tokens without <s> or </s>, e.g.\n        [['The', 'first', 'sentence', '.'], ['Second', '.']]\n        '''", "\n", "n_sentences", "=", "len", "(", "sentences", ")", "\n", "max_length", "=", "max", "(", "len", "(", "sentence", ")", "for", "sentence", "in", "sentences", ")", "+", "2", "\n", "\n", "X_ids", "=", "np", ".", "zeros", "(", "(", "n_sentences", ",", "max_length", ")", ",", "dtype", "=", "np", ".", "int64", ")", "\n", "\n", "for", "k", ",", "sent", "in", "enumerate", "(", "sentences", ")", ":", "\n", "            ", "length", "=", "len", "(", "sent", ")", "+", "2", "\n", "ids_without_mask", "=", "self", ".", "_lm_vocab", ".", "encode", "(", "sent", ",", "split", "=", "False", ")", "\n", "# add one so that 0 is the mask value", "\n", "X_ids", "[", "k", ",", ":", "length", "]", "=", "ids_without_mask", "+", "1", "\n", "\n", "", "return", "X_ids", "\n", "\n"]], "home.repos.pwc.inspect_result.noc-lab_clinical_concept_extraction.clinical_concept_extraction.bilm.LMDataset.__init__": [[336, 357], ["glob.glob", "print", "hasattr", "bilm.LMDataset._load_random_shard", "len"], "methods", ["home.repos.pwc.inspect_result.noc-lab_clinical_concept_extraction.clinical_concept_extraction.bilm.LMDataset._load_random_shard"], ["def", "__init__", "(", "self", ",", "filepattern", ",", "vocab", ",", "reverse", "=", "False", ",", "test", "=", "False", ",", "\n", "shuffle_on_load", "=", "False", ")", ":", "\n", "        ", "'''\n        filepattern = a glob string that specifies the list of files.\n        vocab = an instance of Vocabulary or UnicodeCharsVocabulary\n        reverse = if True, then iterate over tokens in each sentence in reverse\n        test = if True, then iterate through all data once then stop.\n            Otherwise, iterate forever.\n        shuffle_on_load = if True, then shuffle the sentences after loading.\n        '''", "\n", "self", ".", "_vocab", "=", "vocab", "\n", "self", ".", "_all_shards", "=", "glob", ".", "glob", "(", "filepattern", ")", "\n", "print", "(", "'Found %d shards at %s'", "%", "(", "len", "(", "self", ".", "_all_shards", ")", ",", "filepattern", ")", ")", "\n", "self", ".", "_shards_to_choose", "=", "[", "]", "\n", "\n", "self", ".", "_reverse", "=", "reverse", "\n", "self", ".", "_test", "=", "test", "\n", "self", ".", "_shuffle_on_load", "=", "shuffle_on_load", "\n", "self", ".", "_use_char_inputs", "=", "hasattr", "(", "vocab", ",", "'encode_chars'", ")", "\n", "\n", "self", ".", "_ids", "=", "self", ".", "_load_random_shard", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.noc-lab_clinical_concept_extraction.clinical_concept_extraction.bilm.LMDataset._choose_random_shard": [[358, 364], ["bilm.LMDataset._shards_to_choose.pop", "len", "list", "random.shuffle"], "methods", ["None"], ["", "def", "_choose_random_shard", "(", "self", ")", ":", "\n", "        ", "if", "len", "(", "self", ".", "_shards_to_choose", ")", "==", "0", ":", "\n", "            ", "self", ".", "_shards_to_choose", "=", "list", "(", "self", ".", "_all_shards", ")", "\n", "random", ".", "shuffle", "(", "self", ".", "_shards_to_choose", ")", "\n", "", "shard_name", "=", "self", ".", "_shards_to_choose", ".", "pop", "(", ")", "\n", "return", "shard_name", "\n", "\n"]], "home.repos.pwc.inspect_result.noc-lab_clinical_concept_extraction.clinical_concept_extraction.bilm.LMDataset._load_random_shard": [[365, 383], ["bilm.LMDataset._load_shard", "len", "bilm.LMDataset._choose_random_shard", "len", "bilm.LMDataset._all_shards.pop"], "methods", ["home.repos.pwc.inspect_result.noc-lab_clinical_concept_extraction.clinical_concept_extraction.bilm.LMDataset._load_shard", "home.repos.pwc.inspect_result.noc-lab_clinical_concept_extraction.clinical_concept_extraction.bilm.LMDataset._choose_random_shard"], ["", "def", "_load_random_shard", "(", "self", ")", ":", "\n", "        ", "\"\"\"Randomly select a file and read it.\"\"\"", "\n", "if", "self", ".", "_test", ":", "\n", "            ", "if", "len", "(", "self", ".", "_all_shards", ")", "==", "0", ":", "\n", "# we've loaded all the data", "\n", "# this will propogate up to the generator in get_batch", "\n", "# and stop iterating", "\n", "                ", "raise", "StopIteration", "\n", "", "else", ":", "\n", "                ", "shard_name", "=", "self", ".", "_all_shards", ".", "pop", "(", ")", "\n", "", "", "else", ":", "\n", "# just pick a random shard", "\n", "            ", "shard_name", "=", "self", ".", "_choose_random_shard", "(", ")", "\n", "\n", "", "ids", "=", "self", ".", "_load_shard", "(", "shard_name", ")", "\n", "self", ".", "_i", "=", "0", "\n", "self", ".", "_nids", "=", "len", "(", "ids", ")", "\n", "return", "ids", "\n", "\n"]], "home.repos.pwc.inspect_result.noc-lab_clinical_concept_extraction.clinical_concept_extraction.bilm.LMDataset._load_shard": [[384, 420], ["print", "print", "print", "list", "open", "f.readlines", "random.shuffle", "bilm.LMDataset.vocab.encode", "zip", "sentence.split", "sentence.split.reverse", "sentences.append", "bilm.LMDataset.vocab.encode_chars", "len", "len"], "methods", ["home.repos.pwc.inspect_result.noc-lab_clinical_concept_extraction.clinical_concept_extraction.bilm.Vocabulary.encode", "home.repos.pwc.inspect_result.noc-lab_clinical_concept_extraction.clinical_concept_extraction.bilm.UnicodeCharsVocabulary.encode_chars"], ["", "def", "_load_shard", "(", "self", ",", "shard_name", ")", ":", "\n", "        ", "\"\"\"Read one file and convert to ids.\n\n        Args:\n            shard_name: file path.\n\n        Returns:\n            list of (id, char_id) tuples.\n        \"\"\"", "\n", "print", "(", "'Loading data from: %s'", "%", "shard_name", ")", "\n", "with", "open", "(", "shard_name", ")", "as", "f", ":", "\n", "            ", "sentences_raw", "=", "f", ".", "readlines", "(", ")", "\n", "\n", "", "if", "self", ".", "_reverse", ":", "\n", "            ", "sentences", "=", "[", "]", "\n", "for", "sentence", "in", "sentences_raw", ":", "\n", "                ", "splitted", "=", "sentence", ".", "split", "(", ")", "\n", "splitted", ".", "reverse", "(", ")", "\n", "sentences", ".", "append", "(", "' '", ".", "join", "(", "splitted", ")", ")", "\n", "", "", "else", ":", "\n", "            ", "sentences", "=", "sentences_raw", "\n", "\n", "", "if", "self", ".", "_shuffle_on_load", ":", "\n", "            ", "random", ".", "shuffle", "(", "sentences", ")", "\n", "\n", "", "ids", "=", "[", "self", ".", "vocab", ".", "encode", "(", "sentence", ",", "self", ".", "_reverse", ")", "\n", "for", "sentence", "in", "sentences", "]", "\n", "if", "self", ".", "_use_char_inputs", ":", "\n", "            ", "chars_ids", "=", "[", "self", ".", "vocab", ".", "encode_chars", "(", "sentence", ",", "self", ".", "_reverse", ")", "\n", "for", "sentence", "in", "sentences", "]", "\n", "", "else", ":", "\n", "            ", "chars_ids", "=", "[", "None", "]", "*", "len", "(", "ids", ")", "\n", "\n", "", "print", "(", "'Loaded %d sentences.'", "%", "len", "(", "ids", ")", ")", "\n", "print", "(", "'Finished loading'", ")", "\n", "return", "list", "(", "zip", "(", "ids", ",", "chars_ids", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.noc-lab_clinical_concept_extraction.clinical_concept_extraction.bilm.LMDataset.get_sentence": [[421, 428], ["bilm.LMDataset._load_random_shard"], "methods", ["home.repos.pwc.inspect_result.noc-lab_clinical_concept_extraction.clinical_concept_extraction.bilm.LMDataset._load_random_shard"], ["", "def", "get_sentence", "(", "self", ")", ":", "\n", "        ", "while", "True", ":", "\n", "            ", "if", "self", ".", "_i", "==", "self", ".", "_nids", ":", "\n", "                ", "self", ".", "_ids", "=", "self", ".", "_load_random_shard", "(", ")", "\n", "", "ret", "=", "self", ".", "_ids", "[", "self", ".", "_i", "]", "\n", "self", ".", "_i", "+=", "1", "\n", "yield", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.noc-lab_clinical_concept_extraction.clinical_concept_extraction.bilm.LMDataset.max_word_length": [[429, 435], ["None"], "methods", ["None"], ["", "", "@", "property", "\n", "def", "max_word_length", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "_use_char_inputs", ":", "\n", "            ", "return", "self", ".", "_vocab", ".", "max_word_length", "\n", "", "else", ":", "\n", "            ", "return", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.noc-lab_clinical_concept_extraction.clinical_concept_extraction.bilm.LMDataset.iter_batches": [[436, 443], ["bilm._get_batch", "bilm.LMDataset.get_sentence"], "methods", ["home.repos.pwc.inspect_result.noc-lab_clinical_concept_extraction.clinical_concept_extraction.bilm._get_batch", "home.repos.pwc.inspect_result.noc-lab_clinical_concept_extraction.clinical_concept_extraction.bilm.LMDataset.get_sentence"], ["", "", "def", "iter_batches", "(", "self", ",", "batch_size", ",", "num_steps", ")", ":", "\n", "        ", "for", "X", "in", "_get_batch", "(", "self", ".", "get_sentence", "(", ")", ",", "batch_size", ",", "num_steps", ",", "\n", "self", ".", "max_word_length", ")", ":", "\n", "# token_ids = (batch_size, num_steps)", "\n", "# char_inputs = (batch_size, num_steps, 50) of character ids", "\n", "# targets = word ID of next word (batch_size, num_steps)", "\n", "            ", "yield", "X", "\n", "\n"]], "home.repos.pwc.inspect_result.noc-lab_clinical_concept_extraction.clinical_concept_extraction.bilm.LMDataset.vocab": [[444, 447], ["None"], "methods", ["None"], ["", "", "@", "property", "\n", "def", "vocab", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "_vocab", "\n", "\n"]], "home.repos.pwc.inspect_result.noc-lab_clinical_concept_extraction.clinical_concept_extraction.bilm.BidirectionalLMDataset.__init__": [[450, 460], ["bilm.LMDataset", "bilm.LMDataset"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "filepattern", ",", "vocab", ",", "test", "=", "False", ",", "shuffle_on_load", "=", "False", ")", ":", "\n", "        ", "'''\n        bidirectional version of LMDataset\n        '''", "\n", "self", ".", "_data_forward", "=", "LMDataset", "(", "\n", "filepattern", ",", "vocab", ",", "reverse", "=", "False", ",", "test", "=", "test", ",", "\n", "shuffle_on_load", "=", "shuffle_on_load", ")", "\n", "self", ".", "_data_reverse", "=", "LMDataset", "(", "\n", "filepattern", ",", "vocab", ",", "reverse", "=", "True", ",", "test", "=", "test", ",", "\n", "shuffle_on_load", "=", "shuffle_on_load", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.noc-lab_clinical_concept_extraction.clinical_concept_extraction.bilm.BidirectionalLMDataset.iter_batches": [[461, 475], ["zip", "bilm._get_batch", "bilm._get_batch", "Xr.items", "bilm.BidirectionalLMDataset._data_forward.get_sentence", "bilm.BidirectionalLMDataset._data_reverse.get_sentence"], "methods", ["home.repos.pwc.inspect_result.noc-lab_clinical_concept_extraction.clinical_concept_extraction.bilm._get_batch", "home.repos.pwc.inspect_result.noc-lab_clinical_concept_extraction.clinical_concept_extraction.bilm._get_batch", "home.repos.pwc.inspect_result.noc-lab_clinical_concept_extraction.clinical_concept_extraction.bilm.LMDataset.get_sentence", "home.repos.pwc.inspect_result.noc-lab_clinical_concept_extraction.clinical_concept_extraction.bilm.LMDataset.get_sentence"], ["", "def", "iter_batches", "(", "self", ",", "batch_size", ",", "num_steps", ")", ":", "\n", "        ", "max_word_length", "=", "self", ".", "_data_forward", ".", "max_word_length", "\n", "\n", "for", "X", ",", "Xr", "in", "zip", "(", "\n", "_get_batch", "(", "self", ".", "_data_forward", ".", "get_sentence", "(", ")", ",", "batch_size", ",", "\n", "num_steps", ",", "max_word_length", ")", ",", "\n", "_get_batch", "(", "self", ".", "_data_reverse", ".", "get_sentence", "(", ")", ",", "batch_size", ",", "\n", "num_steps", ",", "max_word_length", ")", "\n", ")", ":", "\n", "\n", "            ", "for", "k", ",", "v", "in", "Xr", ".", "items", "(", ")", ":", "\n", "                ", "X", "[", "k", "+", "'_reverse'", "]", "=", "v", "\n", "\n", "", "yield", "X", "\n", "\n"]], "home.repos.pwc.inspect_result.noc-lab_clinical_concept_extraction.clinical_concept_extraction.bilm.BidirectionalLanguageModel.__init__": [[482, 528], ["open", "json.load", "ValueError"], "methods", ["None"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "options_file", ":", "str", ",", "\n", "weight_file", ":", "str", ",", "\n", "use_character_inputs", "=", "True", ",", "\n", "embedding_weight_file", "=", "None", ",", "\n", "max_batch_size", "=", "128", ",", "\n", ")", ":", "\n", "        ", "'''\n        Creates the language model computational graph and loads weights\n\n        Two options for input type:\n            (1) To use character inputs (paired with Batcher)\n                pass use_character_inputs=True, and ids_placeholder\n                of shape (None, None, max_characters_per_token)\n                to __call__\n            (2) To use token ids as input (paired with TokenBatcher),\n                pass use_character_inputs=False and ids_placeholder\n                of shape (None, None) to __call__.\n                In this case, embedding_weight_file is also required input\n\n        options_file: location of the json formatted file with\n                      LM hyperparameters\n        weight_file: location of the hdf5 file with LM weights\n        use_character_inputs: if True, then use character ids as input,\n            otherwise use token ids\n        max_batch_size: the maximum allowable batch size\n        '''", "\n", "with", "open", "(", "options_file", ",", "'r'", ")", "as", "fin", ":", "\n", "            ", "options", "=", "json", ".", "load", "(", "fin", ")", "\n", "\n", "", "if", "not", "use_character_inputs", ":", "\n", "            ", "if", "embedding_weight_file", "is", "None", ":", "\n", "                ", "raise", "ValueError", "(", "\n", "\"embedding_weight_file is required input with \"", "\n", "\"not use_character_inputs\"", "\n", ")", "\n", "\n", "", "", "self", ".", "_options", "=", "options", "\n", "self", ".", "_weight_file", "=", "weight_file", "\n", "self", ".", "_embedding_weight_file", "=", "embedding_weight_file", "\n", "self", ".", "_use_character_inputs", "=", "use_character_inputs", "\n", "self", ".", "_max_batch_size", "=", "max_batch_size", "\n", "\n", "self", ".", "_ops", "=", "{", "}", "\n", "self", ".", "_graphs", "=", "{", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.noc-lab_clinical_concept_extraction.clinical_concept_extraction.bilm.BidirectionalLanguageModel.__call__": [[529, 581], ["bilm.BidirectionalLanguageModel._build_ops", "len", "bilm.BidirectionalLanguageModelGraph", "tensorflow.variable_scope", "bilm.BidirectionalLanguageModelGraph"], "methods", ["home.repos.pwc.inspect_result.noc-lab_clinical_concept_extraction.clinical_concept_extraction.bilm.BidirectionalLanguageModel._build_ops"], ["", "def", "__call__", "(", "self", ",", "ids_placeholder", ")", ":", "\n", "        ", "'''\n        Given the input character ids (or token ids), returns a dictionary\n            with tensorflow ops:\n\n            {'lm_embeddings': embedding_op,\n             'lengths': sequence_lengths_op,\n             'mask': op to compute mask}\n\n        embedding_op computes the LM embeddings and is shape\n            (None, 3, None, 1024)\n        lengths_op computes the sequence lengths and is shape (None, )\n        mask computes the sequence mask and is shape (None, None)\n\n        ids_placeholder: a tf.placeholder of type int32.\n            If use_character_inputs=True, it is shape\n                (None, None, max_characters_per_token) and holds the input\n                character ids for a batch\n            If use_character_input=False, it is shape (None, None) and\n                holds the input token ids for a batch\n        '''", "\n", "if", "ids_placeholder", "in", "self", ".", "_ops", ":", "\n", "# have already created ops for this placeholder, just return them", "\n", "            ", "ret", "=", "self", ".", "_ops", "[", "ids_placeholder", "]", "\n", "\n", "", "else", ":", "\n", "# need to create the graph", "\n", "            ", "if", "len", "(", "self", ".", "_ops", ")", "==", "0", ":", "\n", "# first time creating the graph, don't reuse variables", "\n", "                ", "lm_graph", "=", "BidirectionalLanguageModelGraph", "(", "\n", "self", ".", "_options", ",", "\n", "self", ".", "_weight_file", ",", "\n", "ids_placeholder", ",", "\n", "embedding_weight_file", "=", "self", ".", "_embedding_weight_file", ",", "\n", "use_character_inputs", "=", "self", ".", "_use_character_inputs", ",", "\n", "max_batch_size", "=", "self", ".", "_max_batch_size", ")", "\n", "", "else", ":", "\n", "                ", "with", "tf", ".", "variable_scope", "(", "''", ",", "reuse", "=", "True", ")", ":", "\n", "                    ", "lm_graph", "=", "BidirectionalLanguageModelGraph", "(", "\n", "self", ".", "_options", ",", "\n", "self", ".", "_weight_file", ",", "\n", "ids_placeholder", ",", "\n", "embedding_weight_file", "=", "self", ".", "_embedding_weight_file", ",", "\n", "use_character_inputs", "=", "self", ".", "_use_character_inputs", ",", "\n", "max_batch_size", "=", "self", ".", "_max_batch_size", ")", "\n", "\n", "", "", "ops", "=", "self", ".", "_build_ops", "(", "lm_graph", ")", "\n", "self", ".", "_ops", "[", "ids_placeholder", "]", "=", "ops", "\n", "self", ".", "_graphs", "[", "ids_placeholder", "]", "=", "lm_graph", "\n", "ret", "=", "ops", "\n", "\n", "", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.noc-lab_clinical_concept_extraction.clinical_concept_extraction.bilm.BidirectionalLanguageModel._build_ops": [[582, 650], ["tensorflow.control_dependencies", "len", "range", "tensorflow.concat", "tensorflow.cast", "tensorflow.reverse_sequence", "tensorflow.reverse_sequence", "tensorflow.cast", "tensorflow.concat", "layers.append", "tensorflow.reverse_sequence", "tensorflow.reverse_sequence", "layers_without_bos_eos.append", "tensorflow.concat", "tensorflow.expand_dims"], "methods", ["None"], ["", "def", "_build_ops", "(", "self", ",", "lm_graph", ")", ":", "\n", "        ", "with", "tf", ".", "control_dependencies", "(", "[", "lm_graph", ".", "update_state_op", "]", ")", ":", "\n", "# get the LM embeddings", "\n", "            ", "token_embeddings", "=", "lm_graph", ".", "embedding", "\n", "layers", "=", "[", "\n", "tf", ".", "concat", "(", "[", "token_embeddings", ",", "token_embeddings", "]", ",", "axis", "=", "2", ")", "\n", "]", "\n", "\n", "n_lm_layers", "=", "len", "(", "lm_graph", ".", "lstm_outputs", "[", "'forward'", "]", ")", "\n", "for", "i", "in", "range", "(", "n_lm_layers", ")", ":", "\n", "                ", "layers", ".", "append", "(", "\n", "tf", ".", "concat", "(", "\n", "[", "lm_graph", ".", "lstm_outputs", "[", "'forward'", "]", "[", "i", "]", ",", "\n", "lm_graph", ".", "lstm_outputs", "[", "'backward'", "]", "[", "i", "]", "]", ",", "\n", "axis", "=", "-", "1", "\n", ")", "\n", ")", "\n", "\n", "# The layers include the BOS/EOS tokens.  Remove them", "\n", "", "sequence_length_wo_bos_eos", "=", "lm_graph", ".", "sequence_lengths", "-", "2", "\n", "layers_without_bos_eos", "=", "[", "]", "\n", "for", "layer", "in", "layers", ":", "\n", "                ", "layer_wo_bos_eos", "=", "layer", "[", ":", ",", "1", ":", ",", ":", "]", "\n", "layer_wo_bos_eos", "=", "tf", ".", "reverse_sequence", "(", "\n", "layer_wo_bos_eos", ",", "\n", "lm_graph", ".", "sequence_lengths", "-", "1", ",", "\n", "seq_axis", "=", "1", ",", "\n", "batch_axis", "=", "0", ",", "\n", ")", "\n", "layer_wo_bos_eos", "=", "layer_wo_bos_eos", "[", ":", ",", "1", ":", ",", ":", "]", "\n", "layer_wo_bos_eos", "=", "tf", ".", "reverse_sequence", "(", "\n", "layer_wo_bos_eos", ",", "\n", "sequence_length_wo_bos_eos", ",", "\n", "seq_axis", "=", "1", ",", "\n", "batch_axis", "=", "0", ",", "\n", ")", "\n", "layers_without_bos_eos", ".", "append", "(", "layer_wo_bos_eos", ")", "\n", "\n", "# concatenate the layers", "\n", "", "lm_embeddings", "=", "tf", ".", "concat", "(", "\n", "[", "tf", ".", "expand_dims", "(", "t", ",", "axis", "=", "1", ")", "for", "t", "in", "layers_without_bos_eos", "]", ",", "\n", "axis", "=", "1", "\n", ")", "\n", "\n", "# get the mask op without bos/eos.", "\n", "# tf doesn't support reversing boolean tensors, so cast", "\n", "# to int then back", "\n", "mask_wo_bos_eos", "=", "tf", ".", "cast", "(", "lm_graph", ".", "mask", "[", ":", ",", "1", ":", "]", ",", "'int32'", ")", "\n", "mask_wo_bos_eos", "=", "tf", ".", "reverse_sequence", "(", "\n", "mask_wo_bos_eos", ",", "\n", "lm_graph", ".", "sequence_lengths", "-", "1", ",", "\n", "seq_axis", "=", "1", ",", "\n", "batch_axis", "=", "0", ",", "\n", ")", "\n", "mask_wo_bos_eos", "=", "mask_wo_bos_eos", "[", ":", ",", "1", ":", "]", "\n", "mask_wo_bos_eos", "=", "tf", ".", "reverse_sequence", "(", "\n", "mask_wo_bos_eos", ",", "\n", "sequence_length_wo_bos_eos", ",", "\n", "seq_axis", "=", "1", ",", "\n", "batch_axis", "=", "0", ",", "\n", ")", "\n", "mask_wo_bos_eos", "=", "tf", ".", "cast", "(", "mask_wo_bos_eos", ",", "'bool'", ")", "\n", "\n", "", "return", "{", "\n", "'lm_embeddings'", ":", "lm_embeddings", ",", "\n", "'lengths'", ":", "sequence_length_wo_bos_eos", ",", "\n", "'token_embeddings'", ":", "lm_graph", ".", "embedding", ",", "\n", "'mask'", ":", "mask_wo_bos_eos", ",", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.noc-lab_clinical_concept_extraction.clinical_concept_extraction.bilm.BidirectionalLanguageModelGraph.__init__": [[718, 746], ["bilm._pretrained_initializer", "getter", "tensorflow.variable_scope", "bilm.BidirectionalLanguageModelGraph._build", "h5py.File"], "methods", ["home.repos.pwc.inspect_result.noc-lab_clinical_concept_extraction.clinical_concept_extraction.bilm._pretrained_initializer", "home.repos.pwc.inspect_result.noc-lab_clinical_concept_extraction.clinical_concept_extraction.bilm.BidirectionalLanguageModelGraph._build"], ["def", "__init__", "(", "self", ",", "options", ",", "weight_file", ",", "ids_placeholder", ",", "\n", "use_character_inputs", "=", "True", ",", "embedding_weight_file", "=", "None", ",", "\n", "max_batch_size", "=", "128", ")", ":", "\n", "\n", "        ", "self", ".", "options", "=", "options", "\n", "self", ".", "_max_batch_size", "=", "max_batch_size", "\n", "self", ".", "ids_placeholder", "=", "ids_placeholder", "\n", "self", ".", "use_character_inputs", "=", "use_character_inputs", "\n", "\n", "# this custom_getter will make all variables not trainable and", "\n", "# override the default initializer", "\n", "def", "custom_getter", "(", "getter", ",", "name", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "            ", "kwargs", "[", "'trainable'", "]", "=", "False", "\n", "kwargs", "[", "'initializer'", "]", "=", "_pretrained_initializer", "(", "\n", "name", ",", "weight_file", ",", "embedding_weight_file", "\n", ")", "\n", "return", "getter", "(", "name", ",", "*", "args", ",", "**", "kwargs", ")", "\n", "\n", "", "if", "embedding_weight_file", "is", "not", "None", ":", "\n", "# get the vocab size", "\n", "            ", "with", "h5py", ".", "File", "(", "embedding_weight_file", ",", "'r'", ")", "as", "fin", ":", "\n", "# +1 for padding", "\n", "                ", "self", ".", "_n_tokens_vocab", "=", "fin", "[", "'embedding'", "]", ".", "shape", "[", "0", "]", "+", "1", "\n", "", "", "else", ":", "\n", "            ", "self", ".", "_n_tokens_vocab", "=", "None", "\n", "\n", "", "with", "tf", ".", "variable_scope", "(", "'bilm'", ",", "custom_getter", "=", "custom_getter", ")", ":", "\n", "            ", "self", ".", "_build", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.noc-lab_clinical_concept_extraction.clinical_concept_extraction.bilm.BidirectionalLanguageModelGraph._build": [[747, 753], ["bilm.BidirectionalLanguageModelGraph._build_lstms", "bilm.BidirectionalLanguageModelGraph._build_word_char_embeddings", "bilm.BidirectionalLanguageModelGraph._build_word_embeddings"], "methods", ["home.repos.pwc.inspect_result.noc-lab_clinical_concept_extraction.clinical_concept_extraction.bilm.BidirectionalLanguageModelGraph._build_lstms", "home.repos.pwc.inspect_result.noc-lab_clinical_concept_extraction.clinical_concept_extraction.bilm.BidirectionalLanguageModelGraph._build_word_char_embeddings", "home.repos.pwc.inspect_result.noc-lab_clinical_concept_extraction.clinical_concept_extraction.bilm.BidirectionalLanguageModelGraph._build_word_embeddings"], ["", "", "def", "_build", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "use_character_inputs", ":", "\n", "            ", "self", ".", "_build_word_char_embeddings", "(", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "_build_word_embeddings", "(", ")", "\n", "", "self", ".", "_build_lstms", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.noc-lab_clinical_concept_extraction.clinical_concept_extraction.bilm.BidirectionalLanguageModelGraph._build_word_char_embeddings": [[754, 929], ["sum", "bilm.BidirectionalLanguageModelGraph._build_word_char_embeddings.make_convolutions"], "methods", ["None"], ["", "def", "_build_word_char_embeddings", "(", "self", ")", ":", "\n", "        ", "'''\n        options contains key 'char_cnn': {\n\n        'n_characters': 262,\n\n        # includes the start / end characters\n        'max_characters_per_token': 50,\n\n        'filters': [\n            [1, 32],\n            [2, 32],\n            [3, 64],\n            [4, 128],\n            [5, 256],\n            [6, 512],\n            [7, 512]\n        ],\n        'activation': 'tanh',\n\n        # for the character embedding\n        'embedding': {'dim': 16}\n\n        # for highway layers\n        # if omitted, then no highway layers\n        'n_highway': 2,\n        }\n        '''", "\n", "projection_dim", "=", "self", ".", "options", "[", "'lstm'", "]", "[", "'projection_dim'", "]", "\n", "\n", "cnn_options", "=", "self", ".", "options", "[", "'char_cnn'", "]", "\n", "filters", "=", "cnn_options", "[", "'filters'", "]", "\n", "n_filters", "=", "sum", "(", "f", "[", "1", "]", "for", "f", "in", "filters", ")", "\n", "max_chars", "=", "cnn_options", "[", "'max_characters_per_token'", "]", "\n", "char_embed_dim", "=", "cnn_options", "[", "'embedding'", "]", "[", "'dim'", "]", "\n", "n_chars", "=", "cnn_options", "[", "'n_characters'", "]", "\n", "if", "n_chars", "!=", "262", ":", "\n", "            ", "raise", "InvalidNumberOfCharacters", "(", "\n", "\"Set n_characters=262 after training see the README.md\"", "\n", ")", "\n", "", "if", "cnn_options", "[", "'activation'", "]", "==", "'tanh'", ":", "\n", "            ", "activation", "=", "tf", ".", "nn", ".", "tanh", "\n", "", "elif", "cnn_options", "[", "'activation'", "]", "==", "'relu'", ":", "\n", "            ", "activation", "=", "tf", ".", "nn", ".", "relu", "\n", "\n", "# the character embeddings", "\n", "", "with", "tf", ".", "device", "(", "\"/cpu:0\"", ")", ":", "\n", "            ", "self", ".", "embedding_weights", "=", "tf", ".", "get_variable", "(", "\n", "\"char_embed\"", ",", "[", "n_chars", ",", "char_embed_dim", "]", ",", "\n", "dtype", "=", "DTYPE", ",", "\n", "initializer", "=", "tf", ".", "random_uniform_initializer", "(", "-", "1.0", ",", "1.0", ")", "\n", ")", "\n", "# shape (batch_size, unroll_steps, max_chars, embed_dim)", "\n", "self", ".", "char_embedding", "=", "tf", ".", "nn", ".", "embedding_lookup", "(", "self", ".", "embedding_weights", ",", "\n", "self", ".", "ids_placeholder", ")", "\n", "\n", "# the convolutions", "\n", "", "def", "make_convolutions", "(", "inp", ")", ":", "\n", "            ", "with", "tf", ".", "variable_scope", "(", "'CNN'", ")", "as", "scope", ":", "\n", "                ", "convolutions", "=", "[", "]", "\n", "for", "i", ",", "(", "width", ",", "num", ")", "in", "enumerate", "(", "filters", ")", ":", "\n", "                    ", "if", "cnn_options", "[", "'activation'", "]", "==", "'relu'", ":", "\n", "# He initialization for ReLU activation", "\n", "# with char embeddings init between -1 and 1", "\n", "# w_init = tf.random_normal_initializer(", "\n", "#    mean=0.0,", "\n", "#    stddev=np.sqrt(2.0 / (width * char_embed_dim))", "\n", "# )", "\n", "\n", "# Kim et al 2015, +/- 0.05", "\n", "                        ", "w_init", "=", "tf", ".", "random_uniform_initializer", "(", "\n", "minval", "=", "-", "0.05", ",", "maxval", "=", "0.05", ")", "\n", "", "elif", "cnn_options", "[", "'activation'", "]", "==", "'tanh'", ":", "\n", "# glorot init", "\n", "                        ", "w_init", "=", "tf", ".", "random_normal_initializer", "(", "\n", "mean", "=", "0.0", ",", "\n", "stddev", "=", "np", ".", "sqrt", "(", "1.0", "/", "(", "width", "*", "char_embed_dim", ")", ")", "\n", ")", "\n", "", "w", "=", "tf", ".", "get_variable", "(", "\n", "\"W_cnn_%s\"", "%", "i", ",", "\n", "[", "1", ",", "width", ",", "char_embed_dim", ",", "num", "]", ",", "\n", "initializer", "=", "w_init", ",", "\n", "dtype", "=", "DTYPE", ")", "\n", "b", "=", "tf", ".", "get_variable", "(", "\n", "\"b_cnn_%s\"", "%", "i", ",", "[", "num", "]", ",", "dtype", "=", "DTYPE", ",", "\n", "initializer", "=", "tf", ".", "constant_initializer", "(", "0.0", ")", ")", "\n", "\n", "conv", "=", "tf", ".", "nn", ".", "conv2d", "(", "\n", "inp", ",", "w", ",", "\n", "strides", "=", "[", "1", ",", "1", ",", "1", ",", "1", "]", ",", "\n", "padding", "=", "\"VALID\"", ")", "+", "b", "\n", "# now max pool", "\n", "conv", "=", "tf", ".", "nn", ".", "max_pool", "(", "\n", "conv", ",", "[", "1", ",", "1", ",", "max_chars", "-", "width", "+", "1", ",", "1", "]", ",", "\n", "[", "1", ",", "1", ",", "1", ",", "1", "]", ",", "'VALID'", ")", "\n", "\n", "# activation", "\n", "conv", "=", "activation", "(", "conv", ")", "\n", "conv", "=", "tf", ".", "squeeze", "(", "conv", ",", "squeeze_dims", "=", "[", "2", "]", ")", "\n", "\n", "convolutions", ".", "append", "(", "conv", ")", "\n", "\n", "", "", "return", "tf", ".", "concat", "(", "convolutions", ",", "2", ")", "\n", "\n", "", "embedding", "=", "make_convolutions", "(", "self", ".", "char_embedding", ")", "\n", "\n", "# for highway and projection layers", "\n", "n_highway", "=", "cnn_options", ".", "get", "(", "'n_highway'", ")", "\n", "use_highway", "=", "n_highway", "is", "not", "None", "and", "n_highway", ">", "0", "\n", "use_proj", "=", "n_filters", "!=", "projection_dim", "\n", "\n", "if", "use_highway", "or", "use_proj", ":", "\n", "#   reshape from (batch_size, n_tokens, dim) to (-1, dim)", "\n", "            ", "batch_size_n_tokens", "=", "tf", ".", "shape", "(", "embedding", ")", "[", "0", ":", "2", "]", "\n", "embedding", "=", "tf", ".", "reshape", "(", "embedding", ",", "[", "-", "1", ",", "n_filters", "]", ")", "\n", "\n", "# set up weights for projection", "\n", "", "if", "use_proj", ":", "\n", "            ", "assert", "n_filters", ">", "projection_dim", "\n", "with", "tf", ".", "variable_scope", "(", "'CNN_proj'", ")", "as", "scope", ":", "\n", "                ", "W_proj_cnn", "=", "tf", ".", "get_variable", "(", "\n", "\"W_proj\"", ",", "[", "n_filters", ",", "projection_dim", "]", ",", "\n", "initializer", "=", "tf", ".", "random_normal_initializer", "(", "\n", "mean", "=", "0.0", ",", "stddev", "=", "np", ".", "sqrt", "(", "1.0", "/", "n_filters", ")", ")", ",", "\n", "dtype", "=", "DTYPE", ")", "\n", "b_proj_cnn", "=", "tf", ".", "get_variable", "(", "\n", "\"b_proj\"", ",", "[", "projection_dim", "]", ",", "\n", "initializer", "=", "tf", ".", "constant_initializer", "(", "0.0", ")", ",", "\n", "dtype", "=", "DTYPE", ")", "\n", "\n", "# apply highways layers", "\n", "", "", "def", "high", "(", "x", ",", "ww_carry", ",", "bb_carry", ",", "ww_tr", ",", "bb_tr", ")", ":", "\n", "            ", "carry_gate", "=", "tf", ".", "nn", ".", "sigmoid", "(", "tf", ".", "matmul", "(", "x", ",", "ww_carry", ")", "+", "bb_carry", ")", "\n", "transform_gate", "=", "tf", ".", "nn", ".", "relu", "(", "tf", ".", "matmul", "(", "x", ",", "ww_tr", ")", "+", "bb_tr", ")", "\n", "return", "carry_gate", "*", "transform_gate", "+", "(", "1.0", "-", "carry_gate", ")", "*", "x", "\n", "\n", "", "if", "use_highway", ":", "\n", "            ", "highway_dim", "=", "n_filters", "\n", "\n", "for", "i", "in", "range", "(", "n_highway", ")", ":", "\n", "                ", "with", "tf", ".", "variable_scope", "(", "'CNN_high_%s'", "%", "i", ")", "as", "scope", ":", "\n", "                    ", "W_carry", "=", "tf", ".", "get_variable", "(", "\n", "'W_carry'", ",", "[", "highway_dim", ",", "highway_dim", "]", ",", "\n", "# glorit init", "\n", "initializer", "=", "tf", ".", "random_normal_initializer", "(", "\n", "mean", "=", "0.0", ",", "stddev", "=", "np", ".", "sqrt", "(", "1.0", "/", "highway_dim", ")", ")", ",", "\n", "dtype", "=", "DTYPE", ")", "\n", "b_carry", "=", "tf", ".", "get_variable", "(", "\n", "'b_carry'", ",", "[", "highway_dim", "]", ",", "\n", "initializer", "=", "tf", ".", "constant_initializer", "(", "-", "2.0", ")", ",", "\n", "dtype", "=", "DTYPE", ")", "\n", "W_transform", "=", "tf", ".", "get_variable", "(", "\n", "'W_transform'", ",", "[", "highway_dim", ",", "highway_dim", "]", ",", "\n", "initializer", "=", "tf", ".", "random_normal_initializer", "(", "\n", "mean", "=", "0.0", ",", "stddev", "=", "np", ".", "sqrt", "(", "1.0", "/", "highway_dim", ")", ")", ",", "\n", "dtype", "=", "DTYPE", ")", "\n", "b_transform", "=", "tf", ".", "get_variable", "(", "\n", "'b_transform'", ",", "[", "highway_dim", "]", ",", "\n", "initializer", "=", "tf", ".", "constant_initializer", "(", "0.0", ")", ",", "\n", "dtype", "=", "DTYPE", ")", "\n", "\n", "", "embedding", "=", "high", "(", "embedding", ",", "W_carry", ",", "b_carry", ",", "\n", "W_transform", ",", "b_transform", ")", "\n", "\n", "# finally project down if needed", "\n", "", "", "if", "use_proj", ":", "\n", "            ", "embedding", "=", "tf", ".", "matmul", "(", "embedding", ",", "W_proj_cnn", ")", "+", "b_proj_cnn", "\n", "\n", "# reshape back to (batch_size, tokens, dim)", "\n", "", "if", "use_highway", "or", "use_proj", ":", "\n", "            ", "shp", "=", "tf", ".", "concat", "(", "[", "batch_size_n_tokens", ",", "[", "projection_dim", "]", "]", ",", "axis", "=", "0", ")", "\n", "embedding", "=", "tf", ".", "reshape", "(", "embedding", ",", "shp", ")", "\n", "\n", "# at last assign attributes for remainder of the model", "\n", "", "self", ".", "embedding", "=", "embedding", "\n", "\n"]], "home.repos.pwc.inspect_result.noc-lab_clinical_concept_extraction.clinical_concept_extraction.bilm.BidirectionalLanguageModelGraph._build_word_embeddings": [[930, 941], ["tensorflow.device", "tensorflow.get_variable", "tensorflow.nn.embedding_lookup"], "methods", ["None"], ["", "def", "_build_word_embeddings", "(", "self", ")", ":", "\n", "        ", "projection_dim", "=", "self", ".", "options", "[", "'lstm'", "]", "[", "'projection_dim'", "]", "\n", "\n", "# the word embeddings", "\n", "with", "tf", ".", "device", "(", "\"/cpu:0\"", ")", ":", "\n", "            ", "self", ".", "embedding_weights", "=", "tf", ".", "get_variable", "(", "\n", "\"embedding\"", ",", "[", "self", ".", "_n_tokens_vocab", ",", "projection_dim", "]", ",", "\n", "dtype", "=", "DTYPE", ",", "\n", ")", "\n", "self", ".", "embedding", "=", "tf", ".", "nn", ".", "embedding_lookup", "(", "self", ".", "embedding_weights", ",", "\n", "self", ".", "ids_placeholder", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.noc-lab_clinical_concept_extraction.clinical_concept_extraction.bilm.BidirectionalLanguageModelGraph._build_lstms": [[942, 1067], ["bilm.BidirectionalLanguageModelGraph.options[].get", "bilm.BidirectionalLanguageModelGraph.options[].get", "bilm.BidirectionalLanguageModelGraph.options[].get", "tensorflow.reduce_sum", "tensorflow.group", "tensorflow.reduce_any", "tensorflow.cast", "tensorflow.shape", "range", "tensorflow.reverse_sequence", "bilm.BidirectionalLanguageModelGraph.lstm_state_sizes[].append", "bilm.BidirectionalLanguageModelGraph.lstm_init_states[].append", "bilm.BidirectionalLanguageModelGraph.lstm_final_states[].append", "tensorflow.nn.rnn_cell.LSTMCell", "tensorflow.nn.rnn_cell.LSTMCell", "tensorflow.Variable", "tensorflow.variable_scope", "tensorflow.nn.dynamic_rnn", "bilm.BidirectionalLanguageModelGraph.lstm_outputs[].append", "bilm.BidirectionalLanguageModelGraph.lstm_outputs[].append", "tensorflow.control_dependencies", "range", "tensorflow.nn.rnn_cell.ResidualWrapper", "tensorflow.zeros", "tensorflow.reverse_sequence", "tensorflow.concat", "tensorflow.assign", "update_ops.append", "tensorflow.nn.rnn_cell.LSTMStateTuple"], "methods", ["None"], ["", "", "def", "_build_lstms", "(", "self", ")", ":", "\n", "# now the LSTMs", "\n", "# these will collect the initial states for the forward", "\n", "#   (and reverse LSTMs if we are doing bidirectional)", "\n", "\n", "# parse the options", "\n", "        ", "lstm_dim", "=", "self", ".", "options", "[", "'lstm'", "]", "[", "'dim'", "]", "\n", "projection_dim", "=", "self", ".", "options", "[", "'lstm'", "]", "[", "'projection_dim'", "]", "\n", "n_lstm_layers", "=", "self", ".", "options", "[", "'lstm'", "]", ".", "get", "(", "'n_layers'", ",", "1", ")", "\n", "cell_clip", "=", "self", ".", "options", "[", "'lstm'", "]", ".", "get", "(", "'cell_clip'", ")", "\n", "proj_clip", "=", "self", ".", "options", "[", "'lstm'", "]", ".", "get", "(", "'proj_clip'", ")", "\n", "use_skip_connections", "=", "self", ".", "options", "[", "'lstm'", "]", "[", "'use_skip_connections'", "]", "\n", "# if use_skip_connections:", "\n", "#     print(\"USING SKIP CONNECTIONS\")", "\n", "# else:", "\n", "#     print(\"NOT USING SKIP CONNECTIONS\")", "\n", "\n", "# the sequence lengths from input mask", "\n", "if", "self", ".", "use_character_inputs", ":", "\n", "            ", "mask", "=", "tf", ".", "reduce_any", "(", "self", ".", "ids_placeholder", ">", "0", ",", "axis", "=", "2", ")", "\n", "", "else", ":", "\n", "            ", "mask", "=", "self", ".", "ids_placeholder", ">", "0", "\n", "", "sequence_lengths", "=", "tf", ".", "reduce_sum", "(", "tf", ".", "cast", "(", "mask", ",", "tf", ".", "int32", ")", ",", "axis", "=", "1", ")", "\n", "batch_size", "=", "tf", ".", "shape", "(", "sequence_lengths", ")", "[", "0", "]", "\n", "\n", "# for each direction, we'll store tensors for each layer", "\n", "self", ".", "lstm_outputs", "=", "{", "'forward'", ":", "[", "]", ",", "'backward'", ":", "[", "]", "}", "\n", "self", ".", "lstm_state_sizes", "=", "{", "'forward'", ":", "[", "]", ",", "'backward'", ":", "[", "]", "}", "\n", "self", ".", "lstm_init_states", "=", "{", "'forward'", ":", "[", "]", ",", "'backward'", ":", "[", "]", "}", "\n", "self", ".", "lstm_final_states", "=", "{", "'forward'", ":", "[", "]", ",", "'backward'", ":", "[", "]", "}", "\n", "\n", "update_ops", "=", "[", "]", "\n", "for", "direction", "in", "[", "'forward'", ",", "'backward'", "]", ":", "\n", "            ", "if", "direction", "==", "'forward'", ":", "\n", "                ", "layer_input", "=", "self", ".", "embedding", "\n", "", "else", ":", "\n", "                ", "layer_input", "=", "tf", ".", "reverse_sequence", "(", "\n", "self", ".", "embedding", ",", "\n", "sequence_lengths", ",", "\n", "seq_axis", "=", "1", ",", "\n", "batch_axis", "=", "0", "\n", ")", "\n", "\n", "", "for", "i", "in", "range", "(", "n_lstm_layers", ")", ":", "\n", "                ", "if", "projection_dim", "<", "lstm_dim", ":", "\n", "# are projecting down output", "\n", "                    ", "lstm_cell", "=", "tf", ".", "nn", ".", "rnn_cell", ".", "LSTMCell", "(", "\n", "lstm_dim", ",", "num_proj", "=", "projection_dim", ",", "\n", "cell_clip", "=", "cell_clip", ",", "proj_clip", "=", "proj_clip", ")", "\n", "", "else", ":", "\n", "                    ", "lstm_cell", "=", "tf", ".", "nn", ".", "rnn_cell", ".", "LSTMCell", "(", "\n", "lstm_dim", ",", "\n", "cell_clip", "=", "cell_clip", ",", "proj_clip", "=", "proj_clip", ")", "\n", "\n", "", "if", "use_skip_connections", ":", "\n", "# ResidualWrapper adds inputs to outputs", "\n", "                    ", "if", "i", "==", "0", ":", "\n", "# don't add skip connection from token embedding to", "\n", "# 1st layer output", "\n", "                        ", "pass", "\n", "", "else", ":", "\n", "# add a skip connection", "\n", "                        ", "lstm_cell", "=", "tf", ".", "nn", ".", "rnn_cell", ".", "ResidualWrapper", "(", "lstm_cell", ")", "\n", "\n", "# collect the input state, run the dynamic rnn, collect", "\n", "# the output", "\n", "", "", "state_size", "=", "lstm_cell", ".", "state_size", "\n", "# the LSTMs are stateful.  To support multiple batch sizes,", "\n", "# we'll allocate size for states up to max_batch_size,", "\n", "# then use the first batch_size entries for each batch", "\n", "init_states", "=", "[", "\n", "tf", ".", "Variable", "(", "\n", "tf", ".", "zeros", "(", "[", "self", ".", "_max_batch_size", ",", "dim", "]", ")", ",", "\n", "trainable", "=", "False", "\n", ")", "\n", "for", "dim", "in", "lstm_cell", ".", "state_size", "\n", "]", "\n", "batch_init_states", "=", "[", "\n", "state", "[", ":", "batch_size", ",", ":", "]", "for", "state", "in", "init_states", "\n", "]", "\n", "\n", "if", "direction", "==", "'forward'", ":", "\n", "                    ", "i_direction", "=", "0", "\n", "", "else", ":", "\n", "                    ", "i_direction", "=", "1", "\n", "", "variable_scope_name", "=", "'RNN_{0}/RNN/MultiRNNCell/Cell{1}'", ".", "format", "(", "\n", "i_direction", ",", "i", ")", "\n", "with", "tf", ".", "variable_scope", "(", "variable_scope_name", ")", ":", "\n", "                    ", "layer_output", ",", "final_state", "=", "tf", ".", "nn", ".", "dynamic_rnn", "(", "\n", "lstm_cell", ",", "\n", "layer_input", ",", "\n", "sequence_length", "=", "sequence_lengths", ",", "\n", "initial_state", "=", "tf", ".", "nn", ".", "rnn_cell", ".", "LSTMStateTuple", "(", "\n", "*", "batch_init_states", ")", ",", "\n", ")", "\n", "\n", "", "self", ".", "lstm_state_sizes", "[", "direction", "]", ".", "append", "(", "lstm_cell", ".", "state_size", ")", "\n", "self", ".", "lstm_init_states", "[", "direction", "]", ".", "append", "(", "init_states", ")", "\n", "self", ".", "lstm_final_states", "[", "direction", "]", ".", "append", "(", "final_state", ")", "\n", "if", "direction", "==", "'forward'", ":", "\n", "                    ", "self", ".", "lstm_outputs", "[", "direction", "]", ".", "append", "(", "layer_output", ")", "\n", "", "else", ":", "\n", "                    ", "self", ".", "lstm_outputs", "[", "direction", "]", ".", "append", "(", "\n", "tf", ".", "reverse_sequence", "(", "\n", "layer_output", ",", "\n", "sequence_lengths", ",", "\n", "seq_axis", "=", "1", ",", "\n", "batch_axis", "=", "0", "\n", ")", "\n", ")", "\n", "\n", "", "with", "tf", ".", "control_dependencies", "(", "[", "layer_output", "]", ")", ":", "\n", "# update the initial states", "\n", "                    ", "for", "i", "in", "range", "(", "2", ")", ":", "\n", "                        ", "new_state", "=", "tf", ".", "concat", "(", "\n", "[", "final_state", "[", "i", "]", "[", ":", "batch_size", ",", ":", "]", ",", "\n", "init_states", "[", "i", "]", "[", "batch_size", ":", ",", ":", "]", "]", ",", "axis", "=", "0", ")", "\n", "state_update_op", "=", "tf", ".", "assign", "(", "init_states", "[", "i", "]", ",", "new_state", ")", "\n", "update_ops", ".", "append", "(", "state_update_op", ")", "\n", "\n", "", "", "layer_input", "=", "layer_output", "\n", "\n", "", "", "self", ".", "mask", "=", "mask", "\n", "self", ".", "sequence_lengths", "=", "sequence_lengths", "\n", "self", ".", "update_state_op", "=", "tf", ".", "group", "(", "*", "update_ops", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.noc-lab_clinical_concept_extraction.clinical_concept_extraction.bilm._get_batch": [[276, 326], ["numpy.zeros", "numpy.zeros", "range", "numpy.zeros", "min", "len", "list", "len", "next"], "function", ["None"], ["", "", "def", "_get_batch", "(", "generator", ",", "batch_size", ",", "num_steps", ",", "max_word_length", ")", ":", "\n", "    ", "\"\"\"Read batches of input.\"\"\"", "\n", "cur_stream", "=", "[", "None", "]", "*", "batch_size", "\n", "\n", "no_more_data", "=", "False", "\n", "while", "True", ":", "\n", "        ", "inputs", "=", "np", ".", "zeros", "(", "[", "batch_size", ",", "num_steps", "]", ",", "np", ".", "int32", ")", "\n", "if", "max_word_length", "is", "not", "None", ":", "\n", "            ", "char_inputs", "=", "np", ".", "zeros", "(", "[", "batch_size", ",", "num_steps", ",", "max_word_length", "]", ",", "\n", "np", ".", "int32", ")", "\n", "", "else", ":", "\n", "            ", "char_inputs", "=", "None", "\n", "", "targets", "=", "np", ".", "zeros", "(", "[", "batch_size", ",", "num_steps", "]", ",", "np", ".", "int32", ")", "\n", "\n", "for", "i", "in", "range", "(", "batch_size", ")", ":", "\n", "            ", "cur_pos", "=", "0", "\n", "\n", "while", "cur_pos", "<", "num_steps", ":", "\n", "                ", "if", "cur_stream", "[", "i", "]", "is", "None", "or", "len", "(", "cur_stream", "[", "i", "]", "[", "0", "]", ")", "<=", "1", ":", "\n", "                    ", "try", ":", "\n", "                        ", "cur_stream", "[", "i", "]", "=", "list", "(", "next", "(", "generator", ")", ")", "\n", "", "except", "StopIteration", ":", "\n", "# No more data, exhaust current streams and quit", "\n", "                        ", "no_more_data", "=", "True", "\n", "break", "\n", "\n", "", "", "how_many", "=", "min", "(", "len", "(", "cur_stream", "[", "i", "]", "[", "0", "]", ")", "-", "1", ",", "num_steps", "-", "cur_pos", ")", "\n", "next_pos", "=", "cur_pos", "+", "how_many", "\n", "\n", "inputs", "[", "i", ",", "cur_pos", ":", "next_pos", "]", "=", "cur_stream", "[", "i", "]", "[", "0", "]", "[", ":", "how_many", "]", "\n", "if", "max_word_length", "is", "not", "None", ":", "\n", "                    ", "char_inputs", "[", "i", ",", "cur_pos", ":", "next_pos", "]", "=", "cur_stream", "[", "i", "]", "[", "1", "]", "[", "\n", ":", "how_many", "]", "\n", "", "targets", "[", "i", ",", "cur_pos", ":", "next_pos", "]", "=", "cur_stream", "[", "i", "]", "[", "0", "]", "[", "1", ":", "how_many", "+", "1", "]", "\n", "\n", "cur_pos", "=", "next_pos", "\n", "\n", "cur_stream", "[", "i", "]", "[", "0", "]", "=", "cur_stream", "[", "i", "]", "[", "0", "]", "[", "how_many", ":", "]", "\n", "if", "max_word_length", "is", "not", "None", ":", "\n", "                    ", "cur_stream", "[", "i", "]", "[", "1", "]", "=", "cur_stream", "[", "i", "]", "[", "1", "]", "[", "how_many", ":", "]", "\n", "\n", "", "", "", "if", "no_more_data", ":", "\n", "# There is no more data.  Note: this will not return data", "\n", "# for the incomplete batch", "\n", "            ", "break", "\n", "\n", "", "X", "=", "{", "'token_ids'", ":", "inputs", ",", "'tokens_characters'", ":", "char_inputs", ",", "\n", "'next_token_id'", ":", "targets", "}", "\n", "\n", "yield", "X", "\n", "\n"]], "home.repos.pwc.inspect_result.noc-lab_clinical_concept_extraction.clinical_concept_extraction.bilm._pretrained_initializer": [[653, 710], ["range", "varname_in_file.startswith", "range", "h5py.File", "numpy.zeros", "h5py.File", "list", "list", "ValueError", "numpy.zeros"], "function", ["None"], ["", "", "def", "_pretrained_initializer", "(", "varname", ",", "weight_file", ",", "embedding_weight_file", "=", "None", ")", ":", "\n", "    ", "'''\n    We'll stub out all the initializers in the pretrained LM with\n    a function that loads the weights from the file\n    '''", "\n", "weight_name_map", "=", "{", "}", "\n", "for", "i", "in", "range", "(", "2", ")", ":", "\n", "        ", "for", "j", "in", "range", "(", "8", ")", ":", "# if we decide to add more layers", "\n", "            ", "root", "=", "'RNN_{}/RNN/MultiRNNCell/Cell{}'", ".", "format", "(", "i", ",", "j", ")", "\n", "weight_name_map", "[", "root", "+", "'/rnn/lstm_cell/kernel'", "]", "=", "root", "+", "'/LSTMCell/W_0'", "\n", "weight_name_map", "[", "root", "+", "'/rnn/lstm_cell/bias'", "]", "=", "root", "+", "'/LSTMCell/B'", "\n", "weight_name_map", "[", "root", "+", "'/rnn/lstm_cell/projection/kernel'", "]", "=", "root", "+", "'/LSTMCell/W_P_0'", "\n", "\n", "# convert the graph name to that in the checkpoint", "\n", "", "", "varname_in_file", "=", "varname", "[", "5", ":", "]", "\n", "if", "varname_in_file", ".", "startswith", "(", "'RNN'", ")", ":", "\n", "        ", "varname_in_file", "=", "weight_name_map", "[", "varname_in_file", "]", "\n", "\n", "", "if", "varname_in_file", "==", "'embedding'", ":", "\n", "        ", "with", "h5py", ".", "File", "(", "embedding_weight_file", ",", "'r'", ")", "as", "fin", ":", "\n", "# Have added a special 0 index for padding not present", "\n", "# in the original model.", "\n", "            ", "embed_weights", "=", "fin", "[", "varname_in_file", "]", "[", "...", "]", "\n", "weights", "=", "np", ".", "zeros", "(", "\n", "(", "embed_weights", ".", "shape", "[", "0", "]", "+", "1", ",", "embed_weights", ".", "shape", "[", "1", "]", ")", ",", "\n", "dtype", "=", "DTYPE", "\n", ")", "\n", "weights", "[", "1", ":", ",", ":", "]", "=", "embed_weights", "\n", "", "", "else", ":", "\n", "        ", "with", "h5py", ".", "File", "(", "weight_file", ",", "'r'", ")", "as", "fin", ":", "\n", "            ", "if", "varname_in_file", "==", "'char_embed'", ":", "\n", "# Have added a special 0 index for padding not present", "\n", "# in the original model.", "\n", "                ", "char_embed_weights", "=", "fin", "[", "varname_in_file", "]", "[", "...", "]", "\n", "weights", "=", "np", ".", "zeros", "(", "\n", "(", "char_embed_weights", ".", "shape", "[", "0", "]", "+", "1", ",", "\n", "char_embed_weights", ".", "shape", "[", "1", "]", ")", ",", "\n", "dtype", "=", "DTYPE", "\n", ")", "\n", "weights", "[", "1", ":", ",", ":", "]", "=", "char_embed_weights", "\n", "", "else", ":", "\n", "                ", "weights", "=", "fin", "[", "varname_in_file", "]", "[", "...", "]", "\n", "\n", "# Tensorflow initializers are callables that accept a shape parameter", "\n", "# and some optional kwargs", "\n", "", "", "", "def", "ret", "(", "shape", ",", "**", "kwargs", ")", ":", "\n", "        ", "if", "list", "(", "shape", ")", "!=", "list", "(", "weights", ".", "shape", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"Invalid shape initializing {0}, got {1}, expected {2}\"", ".", "format", "(", "\n", "varname_in_file", ",", "shape", ",", "weights", ".", "shape", ")", "\n", ")", "\n", "", "return", "weights", "\n", "\n", "", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.noc-lab_clinical_concept_extraction.clinical_concept_extraction.bilm.dump_token_embeddings": [[1069, 1108], ["bilm.UnicodeCharsVocabulary", "bilm.Batcher", "tensorflow.placeholder", "bilm.BidirectionalLanguageModel", "int", "numpy.zeros", "tensorflow.ConfigProto", "open", "json.load", "BidirectionalLanguageModel.", "tensorflow.Session", "sess.run", "range", "h5py.File", "fout.create_dataset", "tensorflow.global_variables_initializer", "bilm.Vocabulary.id_to_word", "[].reshape", "sess.run", "bilm.Batcher.batch_sentences"], "function", ["home.repos.pwc.inspect_result.noc-lab_clinical_concept_extraction.clinical_concept_extraction.bilm.Vocabulary.id_to_word", "home.repos.pwc.inspect_result.noc-lab_clinical_concept_extraction.clinical_concept_extraction.bilm.TokenBatcher.batch_sentences"], ["", "", "def", "dump_token_embeddings", "(", "vocab_file", ",", "options_file", ",", "weight_file", ",", "outfile", ")", ":", "\n", "    ", "'''\n    Given an input vocabulary file, dump all the token embeddings to the\n    outfile.  The result can be used as the embedding_weight_file when\n    constructing a BidirectionalLanguageModel.\n    '''", "\n", "with", "open", "(", "options_file", ",", "'r'", ")", "as", "fin", ":", "\n", "        ", "options", "=", "json", ".", "load", "(", "fin", ")", "\n", "", "max_word_length", "=", "options", "[", "'char_cnn'", "]", "[", "'max_characters_per_token'", "]", "\n", "\n", "vocab", "=", "UnicodeCharsVocabulary", "(", "vocab_file", ",", "max_word_length", ")", "\n", "batcher", "=", "Batcher", "(", "vocab_file", ",", "max_word_length", ")", "\n", "\n", "ids_placeholder", "=", "tf", ".", "placeholder", "(", "'int32'", ",", "\n", "shape", "=", "(", "None", ",", "None", ",", "max_word_length", ")", "\n", ")", "\n", "model", "=", "BidirectionalLanguageModel", "(", "options_file", ",", "weight_file", ")", "\n", "embedding_op", "=", "model", "(", "ids_placeholder", ")", "[", "'token_embeddings'", "]", "\n", "\n", "n_tokens", "=", "vocab", ".", "size", "\n", "embed_dim", "=", "int", "(", "embedding_op", ".", "shape", "[", "2", "]", ")", "\n", "\n", "embeddings", "=", "np", ".", "zeros", "(", "(", "n_tokens", ",", "embed_dim", ")", ",", "dtype", "=", "DTYPE", ")", "\n", "\n", "config", "=", "tf", ".", "ConfigProto", "(", "allow_soft_placement", "=", "True", ")", "\n", "config", ".", "gpu_options", ".", "allow_growth", "=", "True", "\n", "with", "tf", ".", "Session", "(", "config", "=", "config", ")", "as", "sess", ":", "\n", "        ", "sess", ".", "run", "(", "tf", ".", "global_variables_initializer", "(", ")", ")", "\n", "for", "k", "in", "range", "(", "n_tokens", ")", ":", "\n", "            ", "token", "=", "vocab", ".", "id_to_word", "(", "k", ")", "\n", "char_ids", "=", "batcher", ".", "batch_sentences", "(", "[", "[", "token", "]", "]", ")", "[", "0", ",", "1", ",", ":", "]", ".", "reshape", "(", "\n", "1", ",", "1", ",", "-", "1", ")", "\n", "embeddings", "[", "k", ",", ":", "]", "=", "sess", ".", "run", "(", "\n", "embedding_op", ",", "feed_dict", "=", "{", "ids_placeholder", ":", "char_ids", "}", "\n", ")", "\n", "\n", "", "", "with", "h5py", ".", "File", "(", "outfile", ",", "'w'", ")", "as", "fout", ":", "\n", "        ", "ds", "=", "fout", ".", "create_dataset", "(", "\n", "'embedding'", ",", "embeddings", ".", "shape", ",", "dtype", "=", "'float32'", ",", "data", "=", "embeddings", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.noc-lab_clinical_concept_extraction.clinical_concept_extraction.bilm.dump_bilm_embeddings": [[1111, 1145], ["bilm.UnicodeCharsVocabulary", "bilm.Batcher", "tensorflow.placeholder", "bilm.BidirectionalLanguageModel", "BidirectionalLanguageModel.", "tensorflow.ConfigProto", "open", "json.load", "tensorflow.Session", "sess.run", "tensorflow.global_variables_initializer", "open", "h5py.File", "line.strip().split", "bilm.Batcher.batch_sentences", "sess.run", "fout.create_dataset", "line.strip"], "function", ["home.repos.pwc.inspect_result.noc-lab_clinical_concept_extraction.clinical_concept_extraction.bilm.TokenBatcher.batch_sentences"], ["", "", "def", "dump_bilm_embeddings", "(", "vocab_file", ",", "dataset_file", ",", "options_file", ",", "\n", "weight_file", ",", "outfile", ")", ":", "\n", "    ", "with", "open", "(", "options_file", ",", "'r'", ")", "as", "fin", ":", "\n", "        ", "options", "=", "json", ".", "load", "(", "fin", ")", "\n", "", "max_word_length", "=", "options", "[", "'char_cnn'", "]", "[", "'max_characters_per_token'", "]", "\n", "\n", "vocab", "=", "UnicodeCharsVocabulary", "(", "vocab_file", ",", "max_word_length", ")", "\n", "batcher", "=", "Batcher", "(", "vocab_file", ",", "max_word_length", ")", "\n", "\n", "ids_placeholder", "=", "tf", ".", "placeholder", "(", "'int32'", ",", "\n", "shape", "=", "(", "None", ",", "None", ",", "max_word_length", ")", "\n", ")", "\n", "model", "=", "BidirectionalLanguageModel", "(", "options_file", ",", "weight_file", ")", "\n", "ops", "=", "model", "(", "ids_placeholder", ")", "\n", "\n", "config", "=", "tf", ".", "ConfigProto", "(", "allow_soft_placement", "=", "True", ")", "\n", "config", ".", "gpu_options", ".", "allow_growth", "=", "True", "\n", "with", "tf", ".", "Session", "(", "config", "=", "config", ")", "as", "sess", ":", "\n", "        ", "sess", ".", "run", "(", "tf", ".", "global_variables_initializer", "(", ")", ")", "\n", "sentence_id", "=", "0", "\n", "with", "open", "(", "dataset_file", ",", "'r'", ")", "as", "fin", ",", "h5py", ".", "File", "(", "outfile", ",", "'w'", ")", "as", "fout", ":", "\n", "            ", "for", "line", "in", "fin", ":", "\n", "                ", "sentence", "=", "line", ".", "strip", "(", ")", ".", "split", "(", ")", "\n", "char_ids", "=", "batcher", ".", "batch_sentences", "(", "[", "sentence", "]", ")", "\n", "embeddings", "=", "sess", ".", "run", "(", "\n", "ops", "[", "'lm_embeddings'", "]", ",", "feed_dict", "=", "{", "ids_placeholder", ":", "char_ids", "}", "\n", ")", "\n", "ds", "=", "fout", ".", "create_dataset", "(", "\n", "'{}'", ".", "format", "(", "sentence_id", ")", ",", "\n", "embeddings", ".", "shape", "[", "1", ":", "]", ",", "dtype", "=", "'float32'", ",", "\n", "data", "=", "embeddings", "[", "0", ",", ":", ",", ":", ",", ":", "]", "\n", ")", "\n", "\n", "sentence_id", "+=", "1", "\n", "", "", "", "", ""]], "home.repos.pwc.inspect_result.noc-lab_clinical_concept_extraction.clinical_concept_extraction.pipeline.build_inputs": [[10, 23], ["clinical_concept_extraction.elmo_vector.ELMO_MIMIC", "clinical_concept_extraction.elmo_vector.ELMO_MIMIC.close_session", "clinical_concept_extraction.elmo_vector.ELMO_MIMIC.get_embeddings", "all_x.append", "all_l.append", "len"], "function", ["home.repos.pwc.inspect_result.noc-lab_clinical_concept_extraction.clinical_concept_extraction.elmo_vector.ELMO_MIMIC.close_session", "home.repos.pwc.inspect_result.noc-lab_clinical_concept_extraction.clinical_concept_extraction.elmo_vector.ELMO_MIMIC.get_embeddings"], ["def", "build_inputs", "(", "all_sentences", ")", ":", "\n", "    ", "all_x", "=", "[", "]", "\n", "all_l", "=", "[", "]", "\n", "\n", "elmo_models", "=", "ELMO_MIMIC", "(", ")", "\n", "\n", "for", "sentence", "in", "all_sentences", ":", "\n", "        ", "embeddings", "=", "elmo_models", ".", "get_embeddings", "(", "sentence", ")", "\n", "all_x", ".", "append", "(", "embeddings", ")", "\n", "all_l", ".", "append", "(", "len", "(", "embeddings", ")", ")", "\n", "", "elmo_models", ".", "close_session", "(", ")", "\n", "\n", "return", "all_x", ",", "all_l", "\n", "\n"]], "home.repos.pwc.inspect_result.noc-lab_clinical_concept_extraction.clinical_concept_extraction.pipeline.build_generator": [[25, 31], ["zip"], "function", ["None"], ["", "def", "build_generator", "(", "x", ",", "l", ")", ":", "\n", "    ", "def", "generator", "(", ")", ":", "\n", "        ", "for", "x_", ",", "l_", "in", "zip", "(", "x", ",", "l", ")", ":", "\n", "            ", "yield", "x_", ",", "l_", "\n", "\n", "", "", "return", "generator", "\n", "\n"]], "home.repos.pwc.inspect_result.noc-lab_clinical_concept_extraction.clinical_concept_extraction.pipeline.get_annotation": [[33, 78], ["os.path.join", "pipeline.build_inputs", "tensorflow.reset_default_graph", "range", "os.path.isdir", "tensorflow.Graph().as_default", "tensorflow.data.Dataset().from_generator", "dataset.padded_batch.padded_batch", "dataset.padded_batch.make_initializable_iterator", "dataset.make_initializable_iterator.get_next", "clinical_concept_extraction.model.annotation_ensemble", "tensorflow.train.Saver", "tensorflow.ConfigProto", "len", "scipy.stats.mode", "best_v.reshape", "all_y_ens.append", "pipeline.build_generator", "tensorflow.Session", "tf.train.Saver.restore", "sess.run", "tensorflow.Graph", "tensorflow.data.Dataset", "tensorflow.TensorShape", "tensorflow.TensorShape", "os.path.join", "tensorflow.Dimension", "tensorflow.Dimension", "tensorflow.Dimension", "list", "sess.run"], "function", ["home.repos.pwc.inspect_result.noc-lab_clinical_concept_extraction.clinical_concept_extraction.pipeline.build_inputs", "home.repos.pwc.inspect_result.noc-lab_clinical_concept_extraction.clinical_concept_extraction.model.annotation_ensemble", "home.repos.pwc.inspect_result.noc-lab_clinical_concept_extraction.clinical_concept_extraction.pipeline.build_generator"], ["", "def", "get_annotation", "(", "all_sentences", ",", "batch_size", "=", "2", ")", ":", "\n", "    ", "model_path", "=", "os", ".", "path", ".", "join", "(", "os", ".", "environ", "[", "'CCE_ASSETS'", "]", ",", "'blstm'", ")", "\n", "if", "not", "os", ".", "path", ".", "isdir", "(", "model_path", ")", ":", "\n", "        ", "raise", "FileNotFoundError", "\n", "\n", "", "x", ",", "l", "=", "build_inputs", "(", "all_sentences", ")", "\n", "\n", "tf", ".", "reset_default_graph", "(", ")", "\n", "with", "tf", ".", "Graph", "(", ")", ".", "as_default", "(", ")", ":", "\n", "        ", "dataset", "=", "tf", ".", "data", ".", "Dataset", "(", ")", ".", "from_generator", "(", "\n", "build_generator", "(", "x", ",", "l", ")", ",", "\n", "(", "tf", ".", "float32", ",", "tf", ".", "int64", ")", ",", "\n", "(", "tf", ".", "TensorShape", "(", "[", "None", ",", "1024", ",", "3", "]", ")", ",", "tf", ".", "TensorShape", "(", "[", "]", ")", ")", "\n", ")", "\n", "\n", "dataset", "=", "dataset", ".", "padded_batch", "(", "batch_size", ",", "(", "[", "tf", ".", "Dimension", "(", "None", ")", ",", "tf", ".", "Dimension", "(", "1024", ")", ",", "tf", ".", "Dimension", "(", "3", ")", "]", ",", "[", "]", ")", ")", "\n", "\n", "iterator", "=", "dataset", ".", "make_initializable_iterator", "(", ")", "\n", "x_", ",", "l_", "=", "iterator", ".", "get_next", "(", ")", "\n", "\n", "y", "=", "annotation_ensemble", "(", "x_", ",", "l_", ")", "\n", "\n", "saver", "=", "tf", ".", "train", ".", "Saver", "(", ")", "\n", "all_y", "=", "[", "]", "\n", "\n", "config", "=", "tf", ".", "ConfigProto", "(", ")", "\n", "config", ".", "gpu_options", ".", "allow_growth", "=", "True", "\n", "\n", "with", "tf", ".", "Session", "(", "config", "=", "config", ")", "as", "sess", ":", "\n", "            ", "saver", ".", "restore", "(", "sess", ",", "os", ".", "path", ".", "join", "(", "model_path", ",", "'model'", ")", ")", "\n", "sess", ".", "run", "(", "iterator", ".", "initializer", ")", "\n", "while", "True", ":", "\n", "                ", "try", ":", "\n", "                    ", "all_y", "+=", "list", "(", "sess", ".", "run", "(", "[", "y", "]", ")", "[", "0", "]", "[", "0", "]", ")", "\n", "", "except", "tf", ".", "errors", ".", "OutOfRangeError", ":", "\n", "                    ", "break", "\n", "\n", "", "", "", "", "all_y_ens", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "l", ")", ")", ":", "\n", "        ", "best_v", ",", "_", "=", "mode", "(", "all_y", "[", "i", "]", "[", ":", "l", "[", "i", "]", "]", ",", "axis", "=", "1", ")", "\n", "ann_ids", "=", "best_v", ".", "reshape", "(", "-", "1", ")", "\n", "ann", "=", "[", "all_concept", "[", "i", "]", "for", "i", "in", "ann_ids", "]", "\n", "all_y_ens", ".", "append", "(", "ann", ")", "\n", "\n", "", "return", "all_y_ens", "\n", "", ""]], "home.repos.pwc.inspect_result.noc-lab_clinical_concept_extraction.clinical_concept_extraction.model.bidirectional_rnn_func": [[15, 49], ["range", "tensorflow.nn.rnn_cell.MultiRNNCell", "tensorflow.nn.rnn_cell.MultiRNNCell", "tensorflow.nn.bidirectional_dynamic_rnn", "tensorflow.concat", "rnn_type.lower", "rnn_func", "rnn_func", "rnn_type.lower", "tensorflow.nn.rnn_cell.DropoutWrapper", "tensorflow.nn.rnn_cell.DropoutWrapper", "all_fw_cells.append", "all_bw_cells.append", "all_fw_cells.append", "all_bw_cells.append", "rnn_type.lower"], "function", ["None"], ["def", "bidirectional_rnn_func", "(", "x", ",", "l", ",", "train", "=", "True", ")", ":", "\n", "    ", "rnn_type", "=", "FLAGS", ".", "rnn_cell_type", "\n", "if", "rnn_type", ".", "lower", "(", ")", "==", "'lstm'", ":", "\n", "        ", "rnn_func", "=", "tf", ".", "nn", ".", "rnn_cell", ".", "BasicLSTMCell", "\n", "", "elif", "rnn_type", ".", "lower", "(", ")", "==", "'simplernn'", ":", "\n", "        ", "rnn_func", "=", "tf", ".", "nn", ".", "rnn_cell", ".", "BasicRNNCell", "\n", "", "elif", "rnn_type", ".", "lower", "(", ")", "==", "'gru'", ":", "\n", "        ", "rnn_func", "=", "tf", ".", "nn", ".", "rnn_cell", ".", "GRUCell", "\n", "", "else", ":", "\n", "        ", "raise", "TypeError", "\n", "\n", "", "all_fw_cells", "=", "[", "]", "\n", "all_bw_cells", "=", "[", "]", "\n", "for", "_", "in", "range", "(", "FLAGS", ".", "depth", ")", ":", "\n", "        ", "fw_cell", "=", "rnn_func", "(", "num_units", "=", "FLAGS", ".", "hidden_state", ")", "\n", "bw_cell", "=", "rnn_func", "(", "num_units", "=", "FLAGS", ".", "hidden_state", ")", "\n", "if", "train", ":", "\n", "            ", "fw_cell", "=", "tf", ".", "nn", ".", "rnn_cell", ".", "DropoutWrapper", "(", "fw_cell", ",", "state_keep_prob", "=", "FLAGS", ".", "dropout_rate", ")", "\n", "bw_cell", "=", "tf", ".", "nn", ".", "rnn_cell", ".", "DropoutWrapper", "(", "bw_cell", ",", "state_keep_prob", "=", "FLAGS", ".", "dropout_rate", ")", "\n", "all_fw_cells", ".", "append", "(", "fw_cell", ")", "\n", "all_bw_cells", ".", "append", "(", "bw_cell", ")", "\n", "", "else", ":", "\n", "            ", "all_fw_cells", ".", "append", "(", "fw_cell", ")", "\n", "all_bw_cells", ".", "append", "(", "bw_cell", ")", "\n", "\n", "", "", "rnn_fw_cells", "=", "tf", ".", "nn", ".", "rnn_cell", ".", "MultiRNNCell", "(", "all_fw_cells", ")", "\n", "rnn_bw_cells", "=", "tf", ".", "nn", ".", "rnn_cell", ".", "MultiRNNCell", "(", "all_bw_cells", ")", "\n", "\n", "rnn_layer", ",", "_", "=", "tf", ".", "nn", ".", "bidirectional_dynamic_rnn", "(", "\n", "rnn_fw_cells", ",", "rnn_bw_cells", ",", "x", ",", "sequence_length", "=", "l", ",", "dtype", "=", "tf", ".", "float32", ")", "\n", "\n", "rnn_output", "=", "tf", ".", "concat", "(", "rnn_layer", ",", "axis", "=", "-", "1", ")", "\n", "\n", "return", "rnn_output", "\n", "\n"]], "home.repos.pwc.inspect_result.noc-lab_clinical_concept_extraction.clinical_concept_extraction.model.bidirectional_lstm_func_freeze": [[51, 70], ["range", "tensorflow.nn.rnn_cell.MultiRNNCell", "tensorflow.nn.rnn_cell.MultiRNNCell", "tensorflow.nn.bidirectional_dynamic_rnn", "tensorflow.concat", "tensorflow.nn.rnn_cell.BasicLSTMCell", "tensorflow.nn.rnn_cell.BasicLSTMCell", "all_fw_cells.append", "all_bw_cells.append"], "function", ["None"], ["", "def", "bidirectional_lstm_func_freeze", "(", "x", ",", "l", ")", ":", "\n", "    ", "all_fw_cells", "=", "[", "]", "\n", "all_bw_cells", "=", "[", "]", "\n", "for", "_", "in", "range", "(", "2", ")", ":", "\n", "        ", "fw_cell", "=", "tf", ".", "nn", ".", "rnn_cell", ".", "BasicLSTMCell", "(", "num_units", "=", "256", ")", "\n", "bw_cell", "=", "tf", ".", "nn", ".", "rnn_cell", ".", "BasicLSTMCell", "(", "num_units", "=", "256", ")", "\n", "\n", "all_fw_cells", ".", "append", "(", "fw_cell", ")", "\n", "all_bw_cells", ".", "append", "(", "bw_cell", ")", "\n", "\n", "", "rnn_fw_cells", "=", "tf", ".", "nn", ".", "rnn_cell", ".", "MultiRNNCell", "(", "all_fw_cells", ")", "\n", "rnn_bw_cells", "=", "tf", ".", "nn", ".", "rnn_cell", ".", "MultiRNNCell", "(", "all_bw_cells", ")", "\n", "\n", "rnn_layer", ",", "_", "=", "tf", ".", "nn", ".", "bidirectional_dynamic_rnn", "(", "\n", "rnn_fw_cells", ",", "rnn_bw_cells", ",", "x", ",", "sequence_length", "=", "l", ",", "dtype", "=", "tf", ".", "float32", ")", "\n", "\n", "rnn_output", "=", "tf", ".", "concat", "(", "rnn_layer", ",", "axis", "=", "-", "1", ")", "\n", "\n", "return", "rnn_output", "\n", "\n"]], "home.repos.pwc.inspect_result.noc-lab_clinical_concept_extraction.clinical_concept_extraction.model.annotation_ensemble": [[72, 98], ["tensorflow.variable_scope", "tensorflow.cast", "range", "tensorflow.stack", "tensorflow.variable_scope", "tensorflow.get_variable", "tensorflow.nn.softmax", "tensorflow.get_variable", "tensorflow.tensordot", "model.bidirectional_lstm_func_freeze", "tensorflow.layers.dense", "tensorflow.get_variable", "tensorflow.contrib.crf.crf_decode", "tensorflow.cast", "tf.stack.append", "tensorflow.constant_initializer", "tensorflow.constant_initializer", "tensorflow.squeeze", "str"], "function", ["home.repos.pwc.inspect_result.noc-lab_clinical_concept_extraction.clinical_concept_extraction.model.bidirectional_lstm_func_freeze"], ["", "def", "annotation_ensemble", "(", "x", ",", "l", ",", "scope", "=", "'clinical_concept_extraction'", ")", ":", "\n", "    ", "with", "tf", ".", "variable_scope", "(", "scope", ")", ":", "\n", "        ", "l", "=", "tf", ".", "cast", "(", "l", ",", "tf", ".", "int32", ")", "\n", "all_prediction", "=", "[", "]", "\n", "\n", "for", "model_id", "in", "range", "(", "10", ")", ":", "\n", "            ", "with", "tf", ".", "variable_scope", "(", "'copy_'", "+", "str", "(", "model_id", ")", ")", ":", "\n", "                ", "weight", "=", "tf", ".", "get_variable", "(", "'weight'", ",", "[", "3", ",", "1", "]", ",", "tf", ".", "float32", ",", "tf", ".", "constant_initializer", "(", "1", ")", ")", "\n", "n_weight", "=", "tf", ".", "nn", ".", "softmax", "(", "weight", ",", "axis", "=", "0", ")", "\n", "gamma", "=", "tf", ".", "get_variable", "(", "'gamma'", ",", "[", "]", ",", "tf", ".", "float32", ",", "tf", ".", "constant_initializer", "(", "1", ")", ")", "\n", "token_embedding", "=", "tf", ".", "tensordot", "(", "x", ",", "n_weight", ",", "[", "[", "-", "1", "]", ",", "[", "0", "]", "]", ")", "\n", "token_embedding", "=", "gamma", "*", "tf", ".", "squeeze", "(", "token_embedding", ",", "axis", "=", "-", "1", ")", "\n", "\n", "lstm_output", "=", "bidirectional_lstm_func_freeze", "(", "token_embedding", ",", "l", ")", "\n", "\n", "logits", "=", "tf", ".", "layers", ".", "dense", "(", "lstm_output", ",", "7", ",", "activation", "=", "None", ")", "\n", "\n", "transition", "=", "tf", ".", "get_variable", "(", "'transitions'", ",", "shape", "=", "[", "7", ",", "7", "]", ",", "dtype", "=", "tf", ".", "float32", ")", "\n", "\n", "viterbi_sequence", ",", "viterbi_score", "=", "tf", ".", "contrib", ".", "crf", ".", "crf_decode", "(", "logits", ",", "transition", ",", "l", ")", "\n", "prediction", "=", "tf", ".", "cast", "(", "viterbi_sequence", ",", "tf", ".", "int32", ")", "\n", "all_prediction", ".", "append", "(", "prediction", ")", "\n", "\n", "", "", "all_prediction", "=", "tf", ".", "stack", "(", "all_prediction", ",", "axis", "=", "-", "1", ")", "\n", "\n", "return", "all_prediction", ",", "", "", "", ""]], "home.repos.pwc.inspect_result.noc-lab_clinical_concept_extraction.clinical_concept_extraction.elmo_vector.ELMO_MIMIC.__init__": [[7, 26], ["os.path.join", "os.path.join", "os.path.join", "os.path.join", "clinical_concept_extraction.bilm.Batcher", "tensorflow.placeholder", "clinical_concept_extraction.bilm.BidirectionalLanguageModel", "elmo_vector.ELMO_MIMIC.model", "tensorflow.ConfigProto", "tensorflow.Session", "elmo_vector.ELMO_MIMIC.session.run", "os.path.isdir", "tensorflow.global_variables_initializer"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "base_path", "=", "os", ".", "path", ".", "join", "(", "os", ".", "environ", "[", "'CCE_ASSETS'", "]", ",", "'elmo'", ")", "\n", "if", "not", "os", ".", "path", ".", "isdir", "(", "base_path", ")", ":", "\n", "            ", "raise", "FileNotFoundError", "\n", "\n", "", "vocab_file", "=", "os", ".", "path", ".", "join", "(", "base_path", ",", "'vocab.txt'", ")", "\n", "options_file", "=", "os", ".", "path", ".", "join", "(", "base_path", ",", "'options.json'", ")", "\n", "weight_file", "=", "os", ".", "path", ".", "join", "(", "base_path", ",", "'mimic_wiki.hdf5'", ")", "\n", "\n", "self", ".", "batcher", "=", "Batcher", "(", "vocab_file", ",", "50", ")", "\n", "self", ".", "input", "=", "tf", ".", "placeholder", "(", "'int32'", ",", "shape", "=", "(", "None", ",", "None", ",", "50", ")", ")", "\n", "self", ".", "model", "=", "BidirectionalLanguageModel", "(", "options_file", ",", "weight_file", ")", "\n", "self", ".", "output", "=", "self", ".", "model", "(", "self", ".", "input", ")", "\n", "\n", "config", "=", "tf", ".", "ConfigProto", "(", ")", "\n", "config", ".", "gpu_options", ".", "allow_growth", "=", "True", "\n", "\n", "self", ".", "session", "=", "tf", ".", "Session", "(", "config", "=", "config", ")", "\n", "self", ".", "session", ".", "run", "(", "tf", ".", "global_variables_initializer", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.noc-lab_clinical_concept_extraction.clinical_concept_extraction.elmo_vector.ELMO_MIMIC.get_embeddings": [[27, 33], ["elmo_vector.ELMO_MIMIC.batcher.batch_sentences", "elmo_vector.ELMO_MIMIC.session.run", "numpy.transpose"], "methods", ["home.repos.pwc.inspect_result.noc-lab_clinical_concept_extraction.clinical_concept_extraction.bilm.TokenBatcher.batch_sentences"], ["", "def", "get_embeddings", "(", "self", ",", "sentence", ")", ":", "\n", "        ", "sentence_ids", "=", "self", ".", "batcher", ".", "batch_sentences", "(", "[", "sentence", "]", ")", "\n", "embedding", "=", "self", ".", "session", ".", "run", "(", "self", ".", "output", "[", "'lm_embeddings'", "]", ",", "feed_dict", "=", "{", "self", ".", "input", ":", "sentence_ids", "}", ")", "\n", "embedding", "=", "np", ".", "transpose", "(", "embedding", "[", "0", "]", ",", "[", "1", ",", "2", ",", "0", "]", ")", "\n", "\n", "return", "embedding", "\n", "\n"]], "home.repos.pwc.inspect_result.noc-lab_clinical_concept_extraction.clinical_concept_extraction.elmo_vector.ELMO_MIMIC.close_session": [[34, 36], ["elmo_vector.ELMO_MIMIC.session.close"], "methods", ["None"], ["", "def", "close_session", "(", "self", ")", ":", "\n", "        ", "self", ".", "session", ".", "close", "(", ")", "", "", "", ""]], "home.repos.pwc.inspect_result.noc-lab_clinical_concept_extraction.training_scripts.dump_models.main": [[14, 37], ["tensorflow.Graph().as_default", "tensorflow.placeholder", "tensorflow.placeholder", "clinical_concept_extraction.model.annotation_ensemble", "range", "tensorflow.train.Saver", "tensorflow.ConfigProto", "tensorflow.global_variables", "savers.append", "tensorflow.Session", "range", "tf.train.Saver.save", "tensorflow.Graph", "str", "tensorflow.train.Saver", "savers[].restore", "str"], "function", ["home.repos.pwc.inspect_result.noc-lab_clinical_concept_extraction.clinical_concept_extraction.model.annotation_ensemble"], ["def", "main", "(", ")", ":", "\n", "    ", "with", "tf", ".", "Graph", "(", ")", ".", "as_default", "(", ")", ":", "\n", "        ", "x", "=", "tf", ".", "placeholder", "(", "tf", ".", "float32", ",", "[", "None", ",", "100", ",", "1024", ",", "3", "]", ")", "\n", "l", "=", "tf", ".", "placeholder", "(", "tf", ".", "int64", ",", "[", "None", "]", ")", "\n", "\n", "y_", "=", "annotation_ensemble", "(", "x", ",", "l", ")", "\n", "\n", "savers", "=", "[", "]", "\n", "for", "model_id", "in", "range", "(", "10", ")", ":", "\n", "            ", "scope", "=", "'clinical_concept_extraction/copy_'", "+", "str", "(", "model_id", ")", "\n", "variables", "=", "tf", ".", "global_variables", "(", "scope", ")", "\n", "savers", ".", "append", "(", "tf", ".", "train", ".", "Saver", "(", "variables", ")", ")", "\n", "\n", "", "final_saver", "=", "tf", ".", "train", ".", "Saver", "(", ")", "\n", "\n", "config", "=", "tf", ".", "ConfigProto", "(", ")", "\n", "config", ".", "gpu_options", ".", "allow_growth", "=", "True", "\n", "with", "tf", ".", "Session", "(", "config", "=", "config", ")", "as", "sess", ":", "\n", "            ", "for", "model_id", "in", "range", "(", "10", ")", ":", "\n", "                ", "savers", "[", "model_id", "]", ".", "restore", "(", "\n", "sess", ",", "'../ckpt/bilstm_crf_concept/model_'", "+", "str", "(", "model_id", ")", "+", "'/final_model'", ")", "\n", "\n", "", "final_saver", ".", "save", "(", "sess", ",", "'../ckpt/ensemble/model'", ",", "write_meta_graph", "=", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.noc-lab_clinical_concept_extraction.training_scripts.evaluate.space_tokenizer": [[23, 38], ["re.split", "spans.append", "re.search", "this_span.span.span", "spans.append", "re.escape"], "function", ["None"], ["def", "space_tokenizer", "(", "text", ")", ":", "\n", "    ", "tokens", "=", "re", ".", "split", "(", "'\\ +'", ",", "text", ")", "\n", "spans", "=", "[", "]", "\n", "start", "=", "0", "\n", "for", "token", "in", "tokens", ":", "\n", "        ", "if", "token", "==", "''", ":", "\n", "            ", "spans", ".", "append", "(", "(", "start", ",", "start", ")", ")", "\n", "", "else", ":", "\n", "            ", "this_span", "=", "re", ".", "search", "(", "re", ".", "escape", "(", "token", ")", ",", "text", "[", "start", ":", "]", ")", "\n", "assert", "this_span", "is", "not", "None", "\n", "this_span", "=", "this_span", ".", "span", "(", ")", "\n", "spans", ".", "append", "(", "(", "this_span", "[", "0", "]", "+", "start", ",", "this_span", "[", "1", "]", "+", "start", ")", ")", "\n", "start", "+=", "this_span", "[", "1", "]", "\n", "\n", "", "", "return", "tokens", ",", "spans", "\n", "\n"]], "home.repos.pwc.inspect_result.noc-lab_clinical_concept_extraction.training_scripts.evaluate.convert_i2b2_format": [[40, 115], ["tempfile.mkdtemp", "os.listdir", "os.listdir.sort", "open().read", "open().read.split", "evaluate.space_tokenizer", "all_y_pred.pop", "list", "zip", "open", "open", "range", "all_annotation.append", "re.sub().lower", "writer.write", "len", "len", "len", "len", "list.pop", "str", "re.sub", "all_annotation.append", "str", "str", "str"], "function", ["home.repos.pwc.inspect_result.noc-lab_clinical_concept_extraction.training_scripts.evaluate.space_tokenizer"], ["", "def", "convert_i2b2_format", "(", "all_y_pred", ",", "all_l", ")", ":", "\n", "    ", "test_text_path", "=", "'../data/raw/test/txt/'", "\n", "all_concept", "=", "[", "''", ",", "'problem'", ",", "'treatment'", ",", "'test'", "]", "\n", "\n", "tag_map", "=", "{", "0", ":", "0", ",", "1", ":", "1", ",", "2", ":", "2", ",", "3", ":", "3", ",", "4", ":", "1", ",", "5", ":", "2", ",", "6", ":", "3", "}", "\n", "\n", "output_path", "=", "tempfile", ".", "mkdtemp", "(", ")", "\n", "output_path", "+=", "'/'", "# add slash to avoid a bug for jar", "\n", "\n", "all_txt_files", "=", "os", ".", "listdir", "(", "test_text_path", ")", "\n", "all_txt_files", "=", "[", "item", "for", "item", "in", "all_txt_files", "if", "item", "[", "-", "3", ":", "]", "==", "'txt'", "]", "\n", "all_txt_files", ".", "sort", "(", ")", "\n", "\n", "for", "txt_filename", "in", "all_txt_files", ":", "\n", "        ", "text", "=", "open", "(", "test_text_path", "+", "txt_filename", ",", "'r'", ",", "encoding", "=", "'utf-8-sig'", ")", ".", "read", "(", ")", "\n", "\n", "all_sentences", "=", "text", ".", "split", "(", "'\\n'", ")", "\n", "\n", "token_list", "=", "[", "space_tokenizer", "(", "sentence", ")", "for", "sentence", "in", "all_sentences", "]", "\n", "useful_sentences", "=", "[", "all_sentences", "[", "i", "]", "for", "i", "in", "range", "(", "len", "(", "all_sentences", ")", ")", "if", "len", "(", "token_list", "[", "i", "]", "[", "0", "]", ")", ">", "0", "]", "\n", "token_list", "=", "[", "sentence", "for", "sentence", "in", "token_list", "if", "len", "(", "sentence", "[", "0", "]", ")", ">", "0", "]", "\n", "\n", "sent_id", "=", "0", "\n", "all_annotation", "=", "[", "]", "\n", "\n", "for", "sentence", "in", "token_list", ":", "\n", "            ", "y", "=", "all_y_pred", ".", "pop", "(", "0", ")", "\n", "y", "=", "list", "(", "y", ")", "\n", "\n", "last_y", "=", "0", "\n", "last_span_start", "=", "0", "\n", "last_span_end", "=", "0", "\n", "last_id_start", "=", "0", "\n", "last_id_end", "=", "0", "\n", "\n", "token_id", "=", "0", "\n", "\n", "for", "token", ",", "span", "in", "zip", "(", "sentence", "[", "0", "]", ",", "sentence", "[", "1", "]", ")", ":", "\n", "                ", "if", "len", "(", "token", ")", ">", "0", ":", "\n", "                    ", "y_ins", "=", "y", ".", "pop", "(", "0", ")", "\n", "\n", "if", "last_y", "!=", "y_ins", ":", "\n", "                        ", "if", "last_y", "!=", "0", ":", "\n", "                            ", "all_annotation", ".", "append", "(", "\n", "[", "sent_id", ",", "last_span_start", ",", "last_span_end", ",", "last_id_start", ",", "last_id_end", ",", "last_y", "]", ")", "\n", "", "last_span_start", "=", "span", "[", "0", "]", "\n", "last_span_end", "=", "span", "[", "1", "]", "\n", "last_id_start", "=", "token_id", "\n", "last_id_end", "=", "token_id", "\n", "last_y", "=", "tag_map", "[", "y_ins", "]", "\n", "", "else", ":", "\n", "                        ", "last_span_end", "=", "span", "[", "1", "]", "\n", "last_id_end", "=", "token_id", "\n", "\n", "", "", "token_id", "+=", "1", "\n", "\n", "", "if", "last_y", "!=", "0", ":", "\n", "                ", "all_annotation", ".", "append", "(", "[", "sent_id", ",", "last_span_start", ",", "last_span_end", ",", "last_id_start", ",", "last_id_end", ",", "last_y", "]", ")", "\n", "\n", "", "sent_id", "+=", "1", "\n", "\n", "", "with", "open", "(", "output_path", "+", "txt_filename", "[", ":", "-", "3", "]", "+", "'con'", ",", "'w'", ")", "as", "writer", ":", "\n", "            ", "for", "ann", "in", "all_annotation", ":", "\n", "                ", "text", "=", "'c=\"'", "\n", "text", "+=", "re", ".", "sub", "(", "'\\ +'", ",", "' '", ",", "useful_sentences", "[", "ann", "[", "0", "]", "]", "[", "ann", "[", "1", "]", ":", "ann", "[", "2", "]", "]", ")", ".", "lower", "(", ")", "\n", "text", "+=", "'\" '", "\n", "text", "+=", "str", "(", "ann", "[", "0", "]", "+", "1", ")", "+", "':'", "+", "str", "(", "ann", "[", "3", "]", ")", "+", "' '", "+", "str", "(", "ann", "[", "0", "]", "+", "1", ")", "+", "':'", "+", "str", "(", "ann", "[", "4", "]", ")", "\n", "text", "+=", "'||t=\"'", "\n", "text", "+=", "all_concept", "[", "ann", "[", "5", "]", "]", "\n", "text", "+=", "'\"\\n'", "\n", "\n", "# print(text)", "\n", "writer", ".", "write", "(", "text", ")", "\n", "\n", "", "", "", "return", "output_path", "\n", "\n"]], "home.repos.pwc.inspect_result.noc-lab_clinical_concept_extraction.training_scripts.evaluate.eval": [[117, 134], ["subprocess.run", "subprocess.run.stdout.decode", "shutil.rmtree", "print", "print", "status.stdout.decode.split"], "function", ["home.repos.pwc.inspect_result.noc-lab_clinical_concept_extraction.clinical_concept_extraction.bilm.Vocabulary.decode"], ["", "def", "eval", "(", "pred_dir", ")", ":", "\n", "    ", "\"\"\"\n    courtesy of https://github.com/text-machine-lab/CliNER\n    \"\"\"", "\n", "test_text_path", "=", "'../data/raw/test/concept/'", "\n", "\n", "eval_jar", "=", "'./i2b2va-eval.jar'", "\n", "\n", "cmd", "=", "'java -jar %s -rcp %s -scp %s -ft con -ex all'", "%", "(", "eval_jar", ",", "test_text_path", ",", "pred_dir", ")", "\n", "status", "=", "subprocess", ".", "run", "(", "cmd", ",", "shell", "=", "True", ",", "stdout", "=", "subprocess", ".", "PIPE", ")", "\n", "result", "=", "status", ".", "stdout", ".", "decode", "(", ")", "\n", "if", "args", ".", "full_report", ":", "\n", "        ", "print", "(", "result", ")", "\n", "", "else", ":", "\n", "        ", "print", "(", "result", ".", "split", "(", "'\\n'", ")", "[", "7", "]", ")", "\n", "\n", "", "shutil", ".", "rmtree", "(", "pred_dir", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.noc-lab_clinical_concept_extraction.training_scripts.evaluate.main": [[136, 152], ["evaluate.convert_i2b2_format", "evaluate.eval", "glob.glob", "range", "pickle.load", "pickle.load", "all_y_pred_list.append", "len", "scipy.stats.mode", "all_y_pred.append", "open", "open", "range", "len"], "function", ["home.repos.pwc.inspect_result.noc-lab_clinical_concept_extraction.training_scripts.evaluate.convert_i2b2_format", "home.repos.pwc.inspect_result.noc-lab_clinical_concept_extraction.training_scripts.evaluate.eval"], ["", "def", "main", "(", ")", ":", "\n", "    ", "if", "args", ".", "ensemble", ":", "\n", "        ", "all_y_pred_list", "=", "[", "]", "\n", "for", "file", "in", "glob", "(", "args", ".", "save_path", "+", "'*.pkl'", ")", ":", "\n", "            ", "all_y_pred", ",", "all_l", "=", "pickle", ".", "load", "(", "open", "(", "file", ",", "'rb'", ")", ")", "\n", "all_y_pred_list", ".", "append", "(", "all_y_pred", ")", "\n", "# build ensemble model", "\n", "", "all_y_pred", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "all_l", ")", ")", ":", "\n", "            ", "best_v", ",", "_", "=", "mode", "(", "[", "all_y_pred_list", "[", "cv", "]", "[", "i", "]", "for", "cv", "in", "range", "(", "len", "(", "all_y_pred_list", ")", ")", "]", ",", "axis", "=", "0", ")", "\n", "all_y_pred", ".", "append", "(", "best_v", "[", "0", "]", ")", "\n", "\n", "", "", "else", ":", "\n", "        ", "all_y_pred", ",", "all_l", "=", "pickle", ".", "load", "(", "open", "(", "args", ".", "pickle_file", ",", "'rb'", ")", ")", "\n", "", "pred_dir", "=", "convert_i2b2_format", "(", "all_y_pred", ",", "all_l", ")", "\n", "eval", "(", "pred_dir", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.noc-lab_clinical_concept_extraction.training_scripts.preprocess_data.unzip_files": [[9, 26], ["tarfile.open", "tarfile.open.extractall", "tarfile.open.close", "tarfile.open", "tarfile.open.extractall", "tarfile.open.close", "tarfile.open", "tarfile.open.extractall", "tarfile.open.close", "shutil.move", "shutil.move", "shutil.move", "shutil.move"], "function", ["None"], ["def", "unzip_files", "(", ")", ":", "\n", "    ", "tar", "=", "tarfile", ".", "open", "(", "'../data/raw/concept_assertion_relation_training_data.tar.gz'", ",", "\"r:gz\"", ")", "\n", "tar", ".", "extractall", "(", "'../data/raw/'", ")", "\n", "tar", ".", "close", "(", ")", "\n", "\n", "tar", "=", "tarfile", ".", "open", "(", "'../data/raw/reference_standard_for_test_data.tar.gz'", ",", "\"r:gz\"", ")", "\n", "tar", ".", "extractall", "(", "'../data/raw/'", ")", "\n", "tar", ".", "close", "(", ")", "\n", "\n", "tar", "=", "tarfile", ".", "open", "(", "'../data/raw/test_data.tar.gz'", ",", "\"r:gz\"", ")", "\n", "tar", ".", "extractall", "(", "'../data/raw/'", ")", "\n", "tar", ".", "close", "(", ")", "\n", "\n", "shutil", ".", "move", "(", "'../data/raw/concept_assertion_relation_training_data'", ",", "'../data/raw/train'", ")", "\n", "shutil", ".", "move", "(", "'../data/raw/reference_standard_for_test_data'", ",", "'../data/raw/test'", ")", "\n", "shutil", ".", "move", "(", "'../data/raw/test_data'", ",", "'../data/raw/test/txt'", ")", "\n", "shutil", ".", "move", "(", "'../data/raw/test/concepts'", ",", "'../data/raw/test/concept'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.noc-lab_clinical_concept_extraction.training_scripts.preprocess_data.fix_typos": [[28, 40], ["subprocess.call", "subprocess.call"], "function", ["None"], ["", "def", "fix_typos", "(", ")", ":", "\n", "    ", "\"\"\"\n    credit from https://github.com/wboag/awecm/blob/master/code/eval/concept_extraction/readme.txt\n    :return:\n    \"\"\"", "\n", "subprocess", ".", "call", "(", "[", "'sed'", ",", "'-i'", ",", "\n", "'/c=\"bun\" 30:6 30:6||t=\"test\"/d'", ",", "\n", "'../data/raw/train/partners/concept/920798564.con'", "]", ")", "\n", "\n", "subprocess", ".", "call", "(", "[", "'sed'", ",", "'-i'", ",", "\n", "'/c=\"patient\" 140:1 140:1||t=\"test\"/d'", ",", "\n", "'../data/raw/train/beth/concept/record-124.con'", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.noc-lab_clinical_concept_extraction.training_scripts.preprocess_data.parse_dir": [[42, 100], ["os.listdir", "os.listdir.sort", "open().read", "open().read", "concepts.split.split", "re.split", "concept_span_string.split", "span_1.split", "span_2.split", "re.sub", "range", "open", "open().read.split", "open", "len", "re.findall", "re.findall", "re.findall", "int", "int", "int", "original_text.lower", "print", "len", "len"], "function", ["None"], ["", "def", "parse_dir", "(", "base_path", ")", ":", "\n", "    ", "base_txt_path", "=", "base_path", "+", "'txt/'", "\n", "base_con_path", "=", "base_path", "+", "'concept/'", "\n", "\n", "all_txt_files", "=", "os", ".", "listdir", "(", "base_txt_path", ")", "\n", "all_txt_files", "=", "[", "item", "for", "item", "in", "all_txt_files", "if", "item", "[", "-", "3", ":", "]", "==", "'txt'", "]", "\n", "all_txt_files", ".", "sort", "(", ")", "\n", "\n", "all_tokens", "=", "[", "]", "\n", "all_concepts", "=", "[", "]", "\n", "\n", "for", "txt_filename", "in", "all_txt_files", ":", "\n", "# read text file", "\n", "        ", "text", "=", "open", "(", "base_txt_path", "+", "txt_filename", ",", "'r'", ",", "encoding", "=", "'utf-8-sig'", ")", ".", "read", "(", ")", "\n", "token_list", "=", "[", "re", ".", "split", "(", "'\\ +'", ",", "sentence", ")", "for", "sentence", "in", "text", ".", "split", "(", "'\\n'", ")", "]", "\n", "token_list", "=", "[", "sentence", "for", "sentence", "in", "token_list", "if", "len", "(", "sentence", ")", ">", "0", "]", "\n", "\n", "# read concept file", "\n", "concepts", "=", "open", "(", "base_con_path", "+", "txt_filename", "[", ":", "-", "3", "]", "+", "'con'", ",", "'r'", ",", "encoding", "=", "'utf-8-sig'", ")", ".", "read", "(", ")", "\n", "concepts", "=", "concepts", ".", "split", "(", "'\\n'", ")", "\n", "concepts", "=", "[", "concept_item", "for", "concept_item", "in", "concepts", "if", "len", "(", "concept_item", ")", ">", "1", "]", "\n", "\n", "# build annotation", "\n", "concepts_list", "=", "[", "[", "''", "]", "*", "len", "(", "sentence", ")", "for", "sentence", "in", "token_list", "]", "\n", "\n", "for", "concept_item", "in", "concepts", ":", "\n", "            ", "concept_name", "=", "re", ".", "findall", "(", "r'c=\"(.*?)\" \\d'", ",", "concept_item", ")", "[", "0", "]", "\n", "concept_tag", "=", "re", ".", "findall", "(", "r't=\"(.*?)\"$'", ",", "concept_item", ")", "[", "0", "]", "\n", "\n", "concept_span_string", "=", "re", ".", "findall", "(", "r'(\\d+:\\d+\\ \\d+:\\d+)'", ",", "concept_item", ")", "[", "0", "]", "\n", "\n", "span_1", ",", "span_2", "=", "concept_span_string", ".", "split", "(", "' '", ")", "\n", "line1", ",", "start", "=", "span_1", ".", "split", "(", "':'", ")", "\n", "line2", ",", "end", "=", "span_2", ".", "split", "(", "':'", ")", "\n", "\n", "assert", "line1", "==", "line2", "\n", "\n", "line1", ",", "start", ",", "end", "=", "int", "(", "line1", ")", ",", "int", "(", "start", ")", ",", "int", "(", "end", ")", "\n", "\n", "concept_name", "=", "re", ".", "sub", "(", "r'\\ +'", ",", "' '", ",", "concept_name", ")", "\n", "original_text", "=", "' '", ".", "join", "(", "token_list", "[", "line1", "-", "1", "]", "[", "start", ":", "end", "+", "1", "]", ")", "\n", "\n", "if", "concept_name", "!=", "original_text", ".", "lower", "(", ")", ":", "\n", "                ", "print", "(", "concept_name", ",", "original_text", ")", "\n", "raise", "RuntimeError", "\n", "\n", "", "first", "=", "True", "\n", "for", "start_id", "in", "range", "(", "start", ",", "end", "+", "1", ")", ":", "\n", "                ", "if", "first", ":", "\n", "                    ", "concepts_list", "[", "line1", "-", "1", "]", "[", "start_id", "]", "=", "'B-'", "+", "concept_tag", "\n", "first", "=", "False", "\n", "", "else", ":", "\n", "                    ", "concepts_list", "[", "line1", "-", "1", "]", "[", "start_id", "]", "=", "concept_tag", "\n", "\n", "", "", "", "all_tokens", "+=", "(", "token_list", ")", "\n", "all_concepts", "+=", "(", "concepts_list", ")", "\n", "\n", "", "return", "all_tokens", ",", "all_concepts", "\n", "\n"]], "home.repos.pwc.inspect_result.noc-lab_clinical_concept_extraction.training_scripts.preprocess_data.main": [[102, 123], ["preprocess_data.unzip_files", "preprocess_data.fix_typos", "preprocess_data.parse_dir", "pickle.dump", "preprocess_data.parse_dir", "pickle.dump", "preprocess_data.parse_dir", "pickle.dump", "os.path.isdir", "os.makedirs", "open", "open", "open"], "function", ["home.repos.pwc.inspect_result.noc-lab_clinical_concept_extraction.training_scripts.preprocess_data.unzip_files", "home.repos.pwc.inspect_result.noc-lab_clinical_concept_extraction.training_scripts.preprocess_data.fix_typos", "home.repos.pwc.inspect_result.noc-lab_clinical_concept_extraction.training_scripts.preprocess_data.parse_dir", "home.repos.pwc.inspect_result.noc-lab_clinical_concept_extraction.training_scripts.preprocess_data.parse_dir", "home.repos.pwc.inspect_result.noc-lab_clinical_concept_extraction.training_scripts.preprocess_data.parse_dir"], ["", "def", "main", "(", ")", ":", "\n", "    ", "unzip_files", "(", ")", "\n", "fix_typos", "(", ")", "\n", "\n", "save_dir", "=", "'../data/preprocessed/pkl/'", "\n", "\n", "if", "not", "os", ".", "path", ".", "isdir", "(", "save_dir", ")", ":", "\n", "        ", "os", ".", "makedirs", "(", "save_dir", ")", "\n", "\n", "", "beth_path", "=", "'../data/raw/train/beth/'", "\n", "partners_path", "=", "'../data/raw/train/partners/'", "\n", "text_path", "=", "'../data/raw/test/'", "\n", "\n", "all_tokens", ",", "all_concepts", "=", "parse_dir", "(", "beth_path", ")", "\n", "pickle", ".", "dump", "(", "[", "all_tokens", ",", "all_concepts", "]", ",", "open", "(", "save_dir", "+", "'beth.pkl'", ",", "'wb'", ")", ")", "\n", "\n", "all_tokens", ",", "all_concepts", "=", "parse_dir", "(", "partners_path", ")", "\n", "pickle", ".", "dump", "(", "[", "all_tokens", ",", "all_concepts", "]", ",", "open", "(", "save_dir", "+", "'partners.pkl'", ",", "'wb'", ")", ")", "\n", "\n", "all_tokens", ",", "all_concepts", "=", "parse_dir", "(", "text_path", ")", "\n", "pickle", ".", "dump", "(", "[", "all_tokens", ",", "all_concepts", "]", ",", "open", "(", "save_dir", "+", "'text.pkl'", ",", "'wb'", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.noc-lab_clinical_concept_extraction.training_scripts.write_tfrecords.clean_text": [[11, 24], ["zip", "len", "new_t.append", "new_c.append", "html.unescape"], "function", ["None"], ["def", "clean_text", "(", "t", ",", "c", ")", ":", "\n", "    ", "\"\"\"\n    There will be some empty token that should be clean in the first place\n    \"\"\"", "\n", "new_t", "=", "[", "]", "\n", "new_c", "=", "[", "]", "\n", "\n", "for", "t_", ",", "c_", "in", "zip", "(", "t", ",", "c", ")", ":", "\n", "        ", "if", "len", "(", "t_", ")", ">", "0", ":", "\n", "            ", "new_t", ".", "append", "(", "unescape", "(", "t_", ")", ")", "\n", "new_c", ".", "append", "(", "c_", ")", "\n", "\n", "", "", "return", "new_t", ",", "new_c", "\n", "\n"]], "home.repos.pwc.inspect_result.noc-lab_clinical_concept_extraction.training_scripts.write_tfrecords.write_tf_records": [[26, 60], ["tensorflow.python_io.TFRecordWriter", "tqdm.tqdm", "tf.python_io.TFRecordWriter.close", "zip", "write_tfrecords.clean_text", "len", "elmo_model.get_embeddings", "tensorflow.train.Features", "tensorflow.train.FeatureLists", "tensorflow.train.SequenceExample", "tf.python_io.TFRecordWriter.write", "enumerate", "len", "tensorflow.train.Feature", "tensorflow.train.Feature", "tensorflow.train.FeatureList", "tensorflow.train.FeatureList", "tf.train.SequenceExample.SerializeToString", "tensorflow.train.Feature", "tensorflow.train.FloatList", "tensorflow.train.Int64List", "tensorflow.train.Int64List", "embedding.reshape"], "function", ["home.repos.pwc.inspect_result.noc-lab_clinical_concept_extraction.training_scripts.write_tfrecords.clean_text", "home.repos.pwc.inspect_result.noc-lab_clinical_concept_extraction.clinical_concept_extraction.elmo_vector.ELMO_MIMIC.get_embeddings"], ["", "def", "write_tf_records", "(", "list_t", ",", "list_c", ",", "output_filename", ",", "all_c", ",", "elmo_model", ")", ":", "\n", "    ", "concept2id", "=", "{", "concept", ":", "concept_id", "for", "concept_id", ",", "concept", "in", "enumerate", "(", "all_c", ")", "}", "\n", "\n", "writer", "=", "tf", ".", "python_io", ".", "TFRecordWriter", "(", "output_filename", ")", "\n", "\n", "sent_id", "=", "0", "\n", "\n", "for", "t", ",", "c", "in", "tqdm", "(", "zip", "(", "list_t", ",", "list_c", ")", ",", "total", "=", "len", "(", "list_t", ")", ")", ":", "\n", "        ", "t", ",", "c", "=", "clean_text", "(", "t", ",", "c", ")", "\n", "c", "=", "[", "concept2id", "[", "item", "]", "for", "item", "in", "c", "]", "\n", "l", "=", "len", "(", "c", ")", "\n", "\n", "embeddings", "=", "elmo_model", ".", "get_embeddings", "(", "t", "[", ":", "l", "]", ")", "\n", "\n", "context", "=", "tf", ".", "train", ".", "Features", "(", "feature", "=", "{", "# Non-serial data uses Feature", "\n", "\"length\"", ":", "tf", ".", "train", ".", "Feature", "(", "int64_list", "=", "tf", ".", "train", ".", "Int64List", "(", "value", "=", "[", "l", "]", ")", ")", ",", "\n", "}", ")", "\n", "\n", "token_features", "=", "[", "tf", ".", "train", ".", "Feature", "(", "float_list", "=", "tf", ".", "train", ".", "FloatList", "(", "value", "=", "embedding", ".", "reshape", "(", "-", "1", ")", ")", ")", "for", "embedding", "\n", "in", "\n", "embeddings", "]", "\n", "concept_features", "=", "[", "tf", ".", "train", ".", "Feature", "(", "int64_list", "=", "tf", ".", "train", ".", "Int64List", "(", "value", "=", "[", "c_", "]", ")", ")", "for", "c_", "in", "c", "]", "\n", "\n", "feature_list", "=", "{", "\n", "'token'", ":", "tf", ".", "train", ".", "FeatureList", "(", "feature", "=", "token_features", ")", ",", "\n", "'concept'", ":", "tf", ".", "train", ".", "FeatureList", "(", "feature", "=", "concept_features", ")", ",", "\n", "}", "\n", "\n", "feature_lists", "=", "tf", ".", "train", ".", "FeatureLists", "(", "feature_list", "=", "feature_list", ")", "\n", "ex", "=", "tf", ".", "train", ".", "SequenceExample", "(", "feature_lists", "=", "feature_lists", ",", "context", "=", "context", ")", "\n", "writer", ".", "write", "(", "ex", ".", "SerializeToString", "(", ")", ")", "\n", "\n", "sent_id", "+=", "1", "\n", "", "writer", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.noc-lab_clinical_concept_extraction.training_scripts.write_tfrecords.main": [[62, 97], ["pickle.load", "pickle.load", "pickle.load", "clinical_concept_extraction.elmo_vector.ELMO_MIMIC", "sklearn.model_selection.KFold", "sklearn.model_selection.KFold.split", "write_tfrecords.write_tf_records", "os.path.isdir", "os.makedirs", "open", "open", "open", "write_tfrecords.write_tf_records", "str"], "function", ["home.repos.pwc.inspect_result.noc-lab_clinical_concept_extraction.training_scripts.write_tfrecords.write_tf_records", "home.repos.pwc.inspect_result.noc-lab_clinical_concept_extraction.training_scripts.write_tfrecords.write_tf_records"], ["", "def", "main", "(", ")", ":", "\n", "    ", "all_concept", "=", "[", "''", ",", "'problem'", ",", "'treatment'", ",", "'test'", ",", "'B-problem'", ",", "'B-treatment'", ",", "'B-test'", "]", "\n", "\n", "save_dir", "=", "'../data/preprocessed/tfrecords/'", "\n", "\n", "if", "not", "os", ".", "path", ".", "isdir", "(", "save_dir", ")", ":", "\n", "        ", "os", ".", "makedirs", "(", "save_dir", ")", "\n", "\n", "", "beth_t", ",", "beth_c", "=", "pickle", ".", "load", "(", "open", "(", "'../data/preprocessed/pkl/beth.pkl'", ",", "'rb'", ")", ")", "\n", "partners_t", ",", "partners_c", "=", "pickle", ".", "load", "(", "open", "(", "'../data/preprocessed/pkl/partners.pkl'", ",", "'rb'", ")", ")", "\n", "test_t", ",", "test_c", "=", "pickle", ".", "load", "(", "open", "(", "'../data/preprocessed/pkl/text.pkl'", ",", "'rb'", ")", ")", "\n", "\n", "train_t", "=", "beth_t", "+", "partners_t", "\n", "train_c", "=", "beth_c", "+", "partners_c", "\n", "\n", "elmo_model", "=", "ELMO_MIMIC", "(", ")", "\n", "\n", "# not for cv, just to break to 10 shards", "\n", "cv", "=", "KFold", "(", "n_splits", "=", "10", ",", "random_state", "=", "0", ",", "shuffle", "=", "True", ")", "\n", "\n", "split_num", "=", "0", "\n", "\n", "for", "_", ",", "valid_set", "in", "cv", ".", "split", "(", "train_c", ")", ":", "\n", "        ", "valid_t", "=", "[", "train_t", "[", "i", "]", "for", "i", "in", "valid_set", "]", "\n", "valid_c", "=", "[", "train_c", "[", "i", "]", "for", "i", "in", "valid_set", "]", "\n", "\n", "output_filename", "=", "save_dir", "+", "'train_cv'", "+", "str", "(", "split_num", ")", "+", "'.tfrecords'", "\n", "\n", "write_tf_records", "(", "valid_t", ",", "valid_c", ",", "output_filename", ",", "all_concept", ",", "elmo_model", ")", "\n", "\n", "split_num", "+=", "1", "\n", "\n", "", "output_filename", "=", "save_dir", "+", "'test.tfrecords'", "\n", "\n", "write_tf_records", "(", "test_t", ",", "test_c", ",", "output_filename", ",", "all_concept", ",", "elmo_model", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.noc-lab_clinical_concept_extraction.training_scripts.training.annotation_func_train": [[35, 64], ["tensorflow.variable_scope", "tensorflow.cast", "tensorflow.cast", "tensorflow.reduce_mean", "tensorflow.variable_scope", "tensorflow.get_variable", "tensorflow.nn.softmax", "tensorflow.get_variable", "tensorflow.tensordot", "clinical_concept_extraction.model.bidirectional_rnn_func", "tensorflow.layers.dense", "tensorflow.contrib.crf.crf_log_likelihood", "tensorflow.contrib.crf.crf_decode", "tensorflow.train.AdamOptimizer().minimize", "tensorflow.constant_initializer", "tensorflow.constant_initializer", "tensorflow.squeeze", "str", "tensorflow.train.AdamOptimizer"], "function", ["home.repos.pwc.inspect_result.noc-lab_clinical_concept_extraction.clinical_concept_extraction.model.bidirectional_rnn_func"], ["def", "annotation_func_train", "(", "x", ",", "y", ",", "l", ",", "train", ",", "reuse", ",", "scope", "=", "'clinical_concept_extraction'", ")", ":", "\n", "    ", "with", "tf", ".", "variable_scope", "(", "scope", ",", "reuse", "=", "reuse", ")", ":", "\n", "        ", "l", "=", "tf", ".", "cast", "(", "l", ",", "tf", ".", "int32", ")", "\n", "# find logit", "\n", "with", "tf", ".", "variable_scope", "(", "'copy_'", "+", "str", "(", "FLAGS", ".", "random_seed", ")", ")", ":", "\n", "            ", "weight", "=", "tf", ".", "get_variable", "(", "'weight'", ",", "[", "3", ",", "1", "]", ",", "tf", ".", "float32", ",", "tf", ".", "constant_initializer", "(", "1", ")", ")", "\n", "n_weight", "=", "tf", ".", "nn", ".", "softmax", "(", "weight", ",", "axis", "=", "0", ")", "\n", "gamma", "=", "tf", ".", "get_variable", "(", "'gamma'", ",", "[", "]", ",", "tf", ".", "float32", ",", "tf", ".", "constant_initializer", "(", "1", ")", ")", "\n", "token_embedding", "=", "tf", ".", "tensordot", "(", "x", ",", "n_weight", ",", "[", "[", "-", "1", "]", ",", "[", "0", "]", "]", ")", "\n", "token_embedding", "=", "gamma", "*", "tf", ".", "squeeze", "(", "token_embedding", ",", "axis", "=", "-", "1", ")", "\n", "\n", "lstm_output", "=", "bidirectional_rnn_func", "(", "token_embedding", ",", "l", ",", "train", ")", "\n", "\n", "logits", "=", "tf", ".", "layers", ".", "dense", "(", "lstm_output", ",", "7", ",", "activation", "=", "None", ")", "\n", "\n", "log_likelihood", ",", "transition_params", "=", "tf", ".", "contrib", ".", "crf", ".", "crf_log_likelihood", "(", "logits", ",", "y", ",", "l", ")", "\n", "viterbi_sequence", ",", "viterbi_score", "=", "tf", ".", "contrib", ".", "crf", ".", "crf_decode", "(", "logits", ",", "transition_params", ",", "l", ")", "\n", "\n", "", "prediction", "=", "tf", ".", "cast", "(", "viterbi_sequence", ",", "tf", ".", "int32", ")", "\n", "\n", "step_loss", "=", "tf", ".", "reduce_mean", "(", "-", "log_likelihood", ")", "\n", "\n", "if", "train", ":", "\n", "            ", "train_step", "=", "tf", ".", "train", ".", "AdamOptimizer", "(", "FLAGS", ".", "learning_rate", ")", ".", "minimize", "(", "step_loss", ")", "\n", "return", "step_loss", ",", "transition_params", ",", "train_step", ",", "prediction", "\n", "", "else", ":", "\n", "            ", "return", "step_loss", ",", "transition_params", ",", "prediction", "\n", "\n"]], "home.repos.pwc.inspect_result.noc-lab_clinical_concept_extraction.training_scripts.training.annotation_func_test": [[66, 88], ["tensorflow.variable_scope", "tensorflow.cast", "tensorflow.cast", "tensorflow.variable_scope", "tensorflow.get_variable", "tensorflow.nn.softmax", "tensorflow.get_variable", "tensorflow.tensordot", "clinical_concept_extraction.model.bidirectional_rnn_func", "tensorflow.layers.dense", "tensorflow.get_variable", "tensorflow.contrib.crf.crf_decode", "tensorflow.constant_initializer", "tensorflow.constant_initializer", "tensorflow.squeeze", "str"], "function", ["home.repos.pwc.inspect_result.noc-lab_clinical_concept_extraction.clinical_concept_extraction.model.bidirectional_rnn_func"], ["", "", "", "def", "annotation_func_test", "(", "x", ",", "l", ",", "reuse", ",", "scope", "=", "'clinical_concept_extraction'", ")", ":", "\n", "    ", "with", "tf", ".", "variable_scope", "(", "scope", ",", "reuse", "=", "reuse", ")", ":", "\n", "        ", "l", "=", "tf", ".", "cast", "(", "l", ",", "tf", ".", "int32", ")", "\n", "# project to some low dimension", "\n", "# find logit", "\n", "with", "tf", ".", "variable_scope", "(", "'copy_'", "+", "str", "(", "FLAGS", ".", "random_seed", ")", ")", ":", "\n", "            ", "weight", "=", "tf", ".", "get_variable", "(", "'weight'", ",", "[", "3", ",", "1", "]", ",", "tf", ".", "float32", ",", "tf", ".", "constant_initializer", "(", "1", ")", ")", "\n", "n_weight", "=", "tf", ".", "nn", ".", "softmax", "(", "weight", ",", "axis", "=", "0", ")", "\n", "gamma", "=", "tf", ".", "get_variable", "(", "'gamma'", ",", "[", "]", ",", "tf", ".", "float32", ",", "tf", ".", "constant_initializer", "(", "1", ")", ")", "\n", "token_embedding", "=", "tf", ".", "tensordot", "(", "x", ",", "n_weight", ",", "[", "[", "-", "1", "]", ",", "[", "0", "]", "]", ")", "\n", "token_embedding", "=", "gamma", "*", "tf", ".", "squeeze", "(", "token_embedding", ",", "axis", "=", "-", "1", ")", "\n", "\n", "lstm_output", "=", "bidirectional_rnn_func", "(", "token_embedding", ",", "l", ",", "False", ")", "\n", "\n", "logits", "=", "tf", ".", "layers", ".", "dense", "(", "lstm_output", ",", "7", ",", "activation", "=", "None", ")", "\n", "\n", "transition", "=", "tf", ".", "get_variable", "(", "'transitions'", ",", "shape", "=", "[", "7", ",", "7", "]", ",", "dtype", "=", "tf", ".", "float32", ")", "\n", "\n", "viterbi_sequence", ",", "viterbi_score", "=", "tf", ".", "contrib", ".", "crf", ".", "crf_decode", "(", "logits", ",", "transition", ",", "l", ")", "\n", "", "prediction", "=", "tf", ".", "cast", "(", "viterbi_sequence", ",", "tf", ".", "int32", ")", "\n", "\n", "return", "prediction", ",", "\n", "\n"]], "home.repos.pwc.inspect_result.noc-lab_clinical_concept_extraction.training_scripts.training._parse_function": [[90, 105], ["tensorflow.parse_single_sequence_example", "tensorflow.reshape", "tensorflow.FixedLenFeature", "tensorflow.FixedLenSequenceFeature", "tensorflow.FixedLenSequenceFeature"], "function", ["None"], ["", "", "def", "_parse_function", "(", "example_proto", ")", ":", "\n", "    ", "contexts", ",", "features", "=", "tf", ".", "parse_single_sequence_example", "(", "\n", "example_proto", ",", "\n", "context_features", "=", "{", "\n", "\"length\"", ":", "tf", ".", "FixedLenFeature", "(", "[", "]", ",", "dtype", "=", "tf", ".", "int64", ")", ",", "\n", "}", ",", "\n", "sequence_features", "=", "{", "\n", "\"token\"", ":", "tf", ".", "FixedLenSequenceFeature", "(", "[", "1024", "*", "3", "]", ",", "dtype", "=", "tf", ".", "float32", ")", ",", "\n", "\"concept\"", ":", "tf", ".", "FixedLenSequenceFeature", "(", "[", "]", ",", "dtype", "=", "tf", ".", "int64", ")", ",", "\n", "}", "\n", ")", "\n", "\n", "token", "=", "tf", ".", "reshape", "(", "features", "[", "\"token\"", "]", ",", "(", "-", "1", ",", "1024", ",", "3", ")", ")", "\n", "\n", "return", "token", ",", "features", "[", "\"concept\"", "]", ",", "contexts", "[", "\"length\"", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.noc-lab_clinical_concept_extraction.training_scripts.training.find_marco_f1": [[107, 143], ["numpy.zeros", "zip", "numpy.zeros", "range", "range", "numpy.sum", "numpy.sum", "numpy.sum", "numpy.sum", "numpy.sum", "numpy.sum", "range"], "function", ["None"], ["", "def", "find_marco_f1", "(", "all_y_pred", ",", "all_y", ",", "all_l", ")", ":", "\n", "    ", "\"\"\"\n    Note this is NOT the official evaluation script.\n    This is a estimation for token-level marco f1 during training\n    \"\"\"", "\n", "tag_map", "=", "{", "0", ":", "0", ",", "1", ":", "1", ",", "2", ":", "2", ",", "3", ":", "3", ",", "4", ":", "1", ",", "5", ":", "2", ",", "6", ":", "3", "}", "\n", "confusion_matrix", "=", "np", ".", "zeros", "(", "(", "4", ",", "4", ")", ",", "dtype", "=", "int", ")", "\n", "for", "y_pred_ins", ",", "y_ins", ",", "l_ins", "in", "zip", "(", "all_y_pred", ",", "all_y", ",", "all_l", ")", ":", "\n", "        ", "for", "i", "in", "range", "(", "1", ",", "l_ins", ")", ":", "\n", "            ", "confusion_matrix", "[", "tag_map", "[", "y_ins", "[", "i", "]", "]", ",", "tag_map", "[", "y_pred_ins", "[", "i", "]", "]", "]", "+=", "1", "\n", "\n", "", "", "small_conf_mat", "=", "np", ".", "zeros", "(", "[", "2", ",", "2", "]", ")", "\n", "for", "class_id", "in", "range", "(", "1", ",", "4", ")", ":", "\n", "        ", "if", "np", ".", "sum", "(", "confusion_matrix", "[", ":", ",", "class_id", "]", ")", ">", "0", ":", "\n", "            ", "non_class_id", "=", "[", "i", "for", "i", "in", "range", "(", "4", ")", "if", "i", "!=", "class_id", "]", "\n", "small_conf_mat", "[", "0", ",", "0", "]", "+=", "confusion_matrix", "[", "class_id", ",", "class_id", "]", "\n", "small_conf_mat", "[", "1", ",", "0", "]", "+=", "np", ".", "sum", "(", "confusion_matrix", "[", "non_class_id", ",", "class_id", "]", ")", "\n", "small_conf_mat", "[", "0", ",", "1", "]", "+=", "np", ".", "sum", "(", "confusion_matrix", "[", "class_id", ",", "non_class_id", "]", ")", "\n", "small_conf_mat", "[", "1", ",", "1", "]", "+=", "np", ".", "sum", "(", "confusion_matrix", "[", "non_class_id", ",", "non_class_id", "]", ")", "\n", "\n", "", "", "try", ":", "\n", "        ", "micro_r", "=", "small_conf_mat", "[", "0", ",", "0", "]", "/", "(", "np", ".", "sum", "(", "small_conf_mat", "[", "0", ",", ":", "]", ")", ")", "\n", "", "except", ":", "\n", "        ", "micro_r", "=", "np", ".", "nan", "\n", "\n", "", "try", ":", "\n", "        ", "micro_p", "=", "small_conf_mat", "[", "0", ",", "0", "]", "/", "(", "np", ".", "sum", "(", "small_conf_mat", "[", ":", ",", "0", "]", ")", ")", "\n", "", "except", ":", "\n", "        ", "micro_p", "=", "np", ".", "nan", "\n", "\n", "", "try", ":", "\n", "        ", "micro_f", "=", "2", "*", "(", "micro_p", "*", "micro_r", ")", "/", "(", "micro_p", "+", "micro_r", ")", "\n", "", "except", ":", "\n", "        ", "micro_f", "=", "np", ".", "nan", "\n", "\n", "", "return", "micro_p", ",", "micro_r", ",", "micro_f", "\n", "\n"]], "home.repos.pwc.inspect_result.noc-lab_clinical_concept_extraction.training_scripts.training.generate_iterator_ops": [[145, 170], ["tensorflow.data.TFRecordDataset", "dataset.shuffle.map", "dataset.shuffle.padded_batch", "dataset.shuffle.make_initializable_iterator", "dataset.make_initializable_iterator.get_next", "list", "annotation_func_test.append", "annotation_func_test.append", "dataset.shuffle.shuffle", "training.annotation_func_train", "training.annotation_func_test", "tensorflow.Dimension", "tensorflow.Dimension", "tensorflow.Dimension", "tensorflow.Dimension"], "function", ["home.repos.pwc.inspect_result.noc-lab_clinical_concept_extraction.training_scripts.training.annotation_func_train", "home.repos.pwc.inspect_result.noc-lab_clinical_concept_extraction.training_scripts.training.annotation_func_test"], ["", "def", "generate_iterator_ops", "(", "filenames", ",", "train", "=", "True", ",", "reuse", "=", "False", ")", ":", "\n", "    ", "dataset", "=", "tf", ".", "data", ".", "TFRecordDataset", "(", "filenames", ")", "\n", "dataset", "=", "dataset", ".", "map", "(", "_parse_function", ")", "\n", "\n", "if", "train", ":", "\n", "        ", "dataset", "=", "dataset", ".", "shuffle", "(", "buffer_size", "=", "2", "*", "FLAGS", ".", "batch_size", ")", "\n", "\n", "", "dataset", "=", "dataset", ".", "padded_batch", "(", "\n", "FLAGS", ".", "batch_size", ",", "\n", "(", "[", "tf", ".", "Dimension", "(", "None", ")", ",", "tf", ".", "Dimension", "(", "1024", ")", ",", "tf", ".", "Dimension", "(", "3", ")", "]", ",", "\n", "[", "tf", ".", "Dimension", "(", "None", ")", "]", ",", "[", "]", ")", "\n", ")", "\n", "data_iterator", "=", "dataset", ".", "make_initializable_iterator", "(", ")", "\n", "next_x", ",", "next_y", ",", "next_l", "=", "data_iterator", ".", "get_next", "(", ")", "\n", "\n", "if", "train", ":", "\n", "        ", "ops", "=", "annotation_func_train", "(", "next_x", ",", "next_y", ",", "next_l", ",", "train", "=", "train", ",", "reuse", "=", "reuse", ")", "\n", "", "else", ":", "\n", "        ", "ops", "=", "annotation_func_test", "(", "next_x", ",", "next_l", ",", "reuse", "=", "reuse", ")", "\n", "\n", "", "ops", "=", "list", "(", "ops", ")", "\n", "ops", ".", "append", "(", "next_y", ")", "\n", "ops", ".", "append", "(", "next_l", ")", "\n", "\n", "return", "data_iterator", ",", "ops", "\n", "\n"]], "home.repos.pwc.inspect_result.noc-lab_clinical_concept_extraction.training_scripts.training.run_one_epoch_train": [[172, 194], ["sess.run", "training.find_marco_f1", "numpy.mean", "sess.run", "all_loss.append", "list", "list", "list"], "function", ["home.repos.pwc.inspect_result.noc-lab_clinical_concept_extraction.training_scripts.training.find_marco_f1"], ["", "def", "run_one_epoch_train", "(", "sess", ",", "iterator", ",", "ops", ")", ":", "\n", "    ", "\"\"\"Proceed a epoch of training/validation\"\"\"", "\n", "all_loss", "=", "[", "]", "\n", "all_y_pred", "=", "[", "]", "\n", "all_y", "=", "[", "]", "\n", "all_l", "=", "[", "]", "\n", "sess", ".", "run", "(", "iterator", ".", "initializer", ")", "\n", "\n", "while", "True", ":", "\n", "        ", "try", ":", "\n", "            ", "results", "=", "sess", ".", "run", "(", "ops", ")", "\n", "all_loss", ".", "append", "(", "results", "[", "0", "]", ")", "\n", "\n", "all_y_pred", "+=", "list", "(", "results", "[", "-", "3", "]", ")", "\n", "all_y", "+=", "list", "(", "results", "[", "-", "2", "]", ")", "\n", "all_l", "+=", "list", "(", "results", "[", "-", "1", "]", ")", "\n", "", "except", "tf", ".", "errors", ".", "OutOfRangeError", ":", "\n", "            ", "break", "\n", "\n", "", "", "f1", "=", "find_marco_f1", "(", "all_y_pred", ",", "all_y", ",", "all_l", ")", "\n", "\n", "return", "np", ".", "mean", "(", "all_loss", ")", ",", "f1", "[", "2", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.noc-lab_clinical_concept_extraction.training_scripts.training.run_one_epoch_test": [[196, 215], ["sess.run", "training.find_marco_f1", "sess.run", "list", "list", "list"], "function", ["home.repos.pwc.inspect_result.noc-lab_clinical_concept_extraction.training_scripts.training.find_marco_f1"], ["", "def", "run_one_epoch_test", "(", "sess", ",", "iterator", ",", "ops", ")", ":", "\n", "    ", "\"\"\"Proceed a epoch of training/validation\"\"\"", "\n", "all_y_pred", "=", "[", "]", "\n", "all_y", "=", "[", "]", "\n", "all_l", "=", "[", "]", "\n", "sess", ".", "run", "(", "iterator", ".", "initializer", ")", "\n", "\n", "while", "True", ":", "\n", "        ", "try", ":", "\n", "            ", "results", "=", "sess", ".", "run", "(", "ops", ")", "\n", "all_y_pred", "+=", "list", "(", "results", "[", "-", "3", "]", ")", "\n", "all_y", "+=", "list", "(", "results", "[", "-", "2", "]", ")", "\n", "all_l", "+=", "list", "(", "results", "[", "-", "1", "]", ")", "\n", "", "except", "tf", ".", "errors", ".", "OutOfRangeError", ":", "\n", "            ", "break", "\n", "\n", "", "", "f1", "=", "find_marco_f1", "(", "all_y_pred", ",", "all_y", ",", "all_l", ")", "\n", "\n", "return", "f1", ",", "all_y_pred", ",", "all_y", ",", "all_l", "\n", "\n"]], "home.repos.pwc.inspect_result.noc-lab_clinical_concept_extraction.training_scripts.training.train": [[217, 257], ["os.listdir", "tensorflow.Graph().as_default", "tensorflow.set_random_seed", "training.generate_iterator_ops", "tensorflow.ConfigProto", "tensorflow.train.Saver", "tensorflow.gfile.MakeDirs", "tensorflow.Session", "sess.run", "range", "pandas.DataFrame", "pd.DataFrame.to_csv", "tensorflow.Graph", "tensorflow.global_variables_initializer", "time.time", "training.run_one_epoch_train", "print", "pd.DataFrame.append", "tf.train.Saver.save", "time.time"], "function", ["home.repos.pwc.inspect_result.noc-lab_clinical_concept_extraction.training_scripts.training.generate_iterator_ops", "home.repos.pwc.inspect_result.noc-lab_clinical_concept_extraction.training_scripts.training.run_one_epoch_train"], ["", "def", "train", "(", ")", ":", "\n", "    ", "all_files", "=", "os", ".", "listdir", "(", "FLAGS", ".", "tfrecord_dir", ")", "\n", "all_files", "=", "[", "item", "for", "item", "in", "all_files", "if", "item", "[", ":", "2", "]", "==", "'tr'", "]", "\n", "\n", "train_files", "=", "[", "FLAGS", ".", "tfrecord_dir", "+", "item", "for", "item", "in", "all_files", "]", "\n", "\n", "with", "tf", ".", "Graph", "(", ")", ".", "as_default", "(", ")", ":", "\n", "        ", "tf", ".", "set_random_seed", "(", "FLAGS", ".", "random_seed", ")", "\n", "\n", "train_iter", ",", "train_op", "=", "generate_iterator_ops", "(", "\n", "train_files", ",", "train", "=", "True", ",", "reuse", "=", "False", ")", "\n", "\n", "config", "=", "tf", ".", "ConfigProto", "(", ")", "\n", "config", ".", "gpu_options", ".", "allow_growth", "=", "True", "\n", "\n", "saver", "=", "tf", ".", "train", ".", "Saver", "(", ")", "\n", "\n", "tf", ".", "gfile", ".", "MakeDirs", "(", "FLAGS", ".", "save_model_dir", ")", "\n", "\n", "table", "=", "[", "]", "\n", "\n", "with", "tf", ".", "Session", "(", "config", "=", "config", ")", "as", "sess", ":", "\n", "            ", "sess", ".", "run", "(", "tf", ".", "global_variables_initializer", "(", ")", ")", "\n", "for", "epoch", "in", "range", "(", "FLAGS", ".", "num_epochs", ")", ":", "\n", "                ", "tic", "=", "time", ".", "time", "(", ")", "\n", "train_loss", ",", "f1_estimate", "=", "run_one_epoch_train", "(", "\n", "sess", ",", "train_iter", ",", "train_op", ")", "\n", "toc", "=", "time", ".", "time", "(", ")", "-", "tic", "\n", "\n", "print", "(", "\"Epoch %d: train loss %.4f, F1 %.4f, elapsed time %.1f s\"", "%", "\n", "(", "epoch", ",", "train_loss", ",", "f1_estimate", ",", "toc", ")", ")", "\n", "\n", "table", ".", "append", "(", "[", "epoch", ",", "train_loss", ",", "f1_estimate", "]", ")", "\n", "\n", "saver", ".", "save", "(", "sess", ",", "FLAGS", ".", "save_model_dir", "+", "'final_model'", ",", "\n", "write_state", "=", "False", ",", "write_meta_graph", "=", "False", ")", "\n", "\n", "", "table", "=", "pd", ".", "DataFrame", "(", "table", ")", "\n", "table", ".", "columns", "=", "[", "'epoch'", ",", "'loss:train'", ",", "'f1:train'", "]", "\n", "table", ".", "to_csv", "(", "FLAGS", ".", "save_model_dir", "+", "'progress.csv'", ",", "index", "=", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.noc-lab_clinical_concept_extraction.training_scripts.training.evaluate": [[259, 282], ["tensorflow.gfile.MakeDirs", "tensorflow.reset_default_graph", "pickle.dump", "tensorflow.Graph().as_default", "training.generate_iterator_ops", "tensorflow.ConfigProto", "tensorflow.train.Saver", "open", "tensorflow.Session", "sess.run", "tf.train.Saver.restore", "training.run_one_epoch_test", "print", "tensorflow.Graph", "tensorflow.global_variables_initializer", "str"], "function", ["home.repos.pwc.inspect_result.noc-lab_clinical_concept_extraction.training_scripts.training.generate_iterator_ops", "home.repos.pwc.inspect_result.noc-lab_clinical_concept_extraction.training_scripts.training.run_one_epoch_test"], ["", "", "", "def", "evaluate", "(", ")", ":", "\n", "    ", "tf", ".", "gfile", ".", "MakeDirs", "(", "FLAGS", ".", "eval_dir", ")", "\n", "\n", "tf", ".", "reset_default_graph", "(", ")", "\n", "with", "tf", ".", "Graph", "(", ")", ".", "as_default", "(", ")", ":", "\n", "        ", "valid_iter", ",", "valid_op", "=", "generate_iterator_ops", "(", "\n", "[", "FLAGS", ".", "tfrecord_dir", "+", "'test.tfrecords'", "]", ",", "\n", "train", "=", "False", ",", "reuse", "=", "False", "\n", ")", "\n", "\n", "config", "=", "tf", ".", "ConfigProto", "(", ")", "\n", "config", ".", "gpu_options", ".", "allow_growth", "=", "True", "\n", "\n", "saver", "=", "tf", ".", "train", ".", "Saver", "(", ")", "\n", "\n", "with", "tf", ".", "Session", "(", "config", "=", "config", ")", "as", "sess", ":", "\n", "            ", "sess", ".", "run", "(", "tf", ".", "global_variables_initializer", "(", ")", ")", "\n", "saver", ".", "restore", "(", "sess", ",", "FLAGS", ".", "save_model_dir", "+", "'final_model'", ")", "\n", "\n", "f1_estimate", ",", "logits", ",", "all_y", ",", "all_l", "=", "run_one_epoch_test", "(", "sess", ",", "valid_iter", ",", "valid_op", ")", "\n", "\n", "print", "(", "f1_estimate", ")", "\n", "", "", "pickle", ".", "dump", "(", "[", "logits", ",", "all_l", "]", ",", "open", "(", "FLAGS", ".", "eval_dir", "+", "str", "(", "FLAGS", ".", "random_seed", ")", "+", "'.pkl'", ",", "'wb'", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.noc-lab_clinical_concept_extraction.training_scripts.training.main": [[284, 289], ["training.train", "training.evaluate"], "function", ["home.repos.pwc.inspect_result.noc-lab_clinical_concept_extraction.training_scripts.training.train", "home.repos.pwc.inspect_result.noc-lab_clinical_concept_extraction.training_scripts.training.evaluate"], ["", "def", "main", "(", "_", ")", ":", "\n", "    ", "if", "FLAGS", ".", "train", ":", "\n", "        ", "train", "(", ")", "\n", "", "else", ":", "\n", "        ", "evaluate", "(", ")", "\n", "\n"]]}