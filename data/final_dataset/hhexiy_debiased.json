{"home.repos.pwc.inspect_result.hhexiy_debiased.src.options.add_default_arguments": [[1, 8], ["parser.add_argument", "parser.add_argument", "parser.add_argument"], "function", ["None"], ["def", "add_default_arguments", "(", "parser", ")", ":", "\n", "    ", "parser", ".", "add_argument", "(", "'--gpu-id'", ",", "type", "=", "int", ",", "default", "=", "0", ",", "\n", "help", "=", "'GPU id (-1 means CPU)'", ")", "\n", "parser", ".", "add_argument", "(", "'--seed'", ",", "type", "=", "int", ",", "default", "=", "2", ",", "\n", "help", "=", "'random seed'", ")", "\n", "parser", ".", "add_argument", "(", "'--mode'", ",", "choices", "=", "[", "'train'", ",", "'test'", "]", ",", "default", "=", "'train'", ",", "\n", "help", "=", "'train or test'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.options.add_data_arguments": [[9, 33], ["parser.add_argument_group", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument"], "function", ["None"], ["", "def", "add_data_arguments", "(", "parser", ")", ":", "\n", "    ", "group", "=", "parser", ".", "add_argument_group", "(", "'Data'", ")", "\n", "#group.add_argument('--train-file', default='snli_1.0/snli_1.0_train.txt',", "\n", "#                    help='training set file')", "\n", "#group.add_argument('--test-file', default='snli_1.0/snli_1.0_dev.txt',", "\n", "#                    help='validation set file')", "\n", "group", ".", "add_argument", "(", "'--train-split'", ",", "default", "=", "'train'", ",", "\n", "help", "=", "'training data split name'", ")", "\n", "group", ".", "add_argument", "(", "'--test-split'", ",", "default", "=", "'dev'", ",", "\n", "help", "=", "'test data split name'", ")", "\n", "group", ".", "add_argument", "(", "'--superficial'", ",", "choices", "=", "[", "'hypothesis'", ",", "'handcrafted'", "]", ",", "default", "=", "None", ",", "\n", "help", "=", "'only use superficial features'", ")", "\n", "group", ".", "add_argument", "(", "'--additive'", ",", "nargs", "=", "'*'", ",", "default", "=", "None", ",", "\n", "help", "=", "'path to models to be added to the additive model'", ")", "\n", "group", ".", "add_argument", "(", "'--remove'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'remove examples that are predicted correctly by previous models'", ")", "\n", "group", ".", "add_argument", "(", "'--remove-cheat'", ",", "choices", "=", "[", "'True'", ",", "'False'", "]", ",", "default", "=", "'False'", ",", "\n", "help", "=", "'remove examples where cheating features are enabled'", ")", "\n", "group", ".", "add_argument", "(", "'--cheat'", ",", "type", "=", "float", ",", "default", "=", "-", "1", ",", "\n", "help", "=", "'percentage of training data where value of the cheating feature is the groundtruth label. -1 means no cheating features is added at all.'", ")", "\n", "group", ".", "add_argument", "(", "'--task-name'", ",", "required", "=", "True", ",", "\n", "help", "=", "'The name of the task to fine-tune.(MRPC,...)'", ")", "\n", "group", ".", "add_argument", "(", "'--max-num-examples'", ",", "type", "=", "int", ",", "default", "=", "-", "1", ",", "\n", "help", "=", "'maximum number of examples to read, -1 means all.'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.options.add_logging_arguments": [[34, 42], ["parser.add_argument_group", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument"], "function", ["None"], ["", "def", "add_logging_arguments", "(", "parser", ")", ":", "\n", "    ", "group", "=", "parser", ".", "add_argument_group", "(", "'Logging'", ")", "\n", "group", ".", "add_argument", "(", "'--exp-id'", ",", "default", "=", "None", ",", "\n", "help", "=", "'experiment ID'", ")", "\n", "group", ".", "add_argument", "(", "'--output-dir'", ",", "default", "=", "'.'", ",", "\n", "help", "=", "'output directory'", ")", "\n", "group", ".", "add_argument", "(", "'--log-interval'", ",", "type", "=", "int", ",", "default", "=", "20", ",", "\n", "help", "=", "'the interval of two print'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.options.add_model_arguments": [[43, 73], ["parser.add_argument_group", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument"], "function", ["None"], ["", "def", "add_model_arguments", "(", "parser", ")", ":", "\n", "    ", "group", "=", "parser", ".", "add_argument_group", "(", "'Model'", ")", "\n", "#group.add_argument('--num-classes', type=int, default=2,", "\n", "#                    help='number of classes')", "\n", "group", ".", "add_argument", "(", "'--model-type'", ",", "choices", "=", "[", "'cbow'", ",", "'bert'", ",", "'da'", ",", "'esim'", "]", ",", "default", "=", "'bert'", ",", "\n", "help", "=", "'core classifier type'", ")", "\n", "#group.add_argument('--embedding', default='glove',", "\n", "#                    help='word embedding type')", "\n", "group", ".", "add_argument", "(", "'--embedding-source'", ",", "default", "=", "'glove.840B.300d'", ",", "\n", "help", "=", "'embedding file source'", ")", "\n", "group", ".", "add_argument", "(", "'--embedding-size'", ",", "type", "=", "int", ",", "default", "=", "300", ",", "\n", "help", "=", "'size of pretrained word embedding'", ")", "\n", "group", ".", "add_argument", "(", "'--hidden-size'", ",", "type", "=", "int", ",", "default", "=", "200", ",", "\n", "help", "=", "'hidden layer size'", ")", "\n", "group", ".", "add_argument", "(", "'--num-layers'", ",", "type", "=", "int", ",", "default", "=", "1", ",", "\n", "help", "=", "'number of hidden layers'", ")", "\n", "#group.add_argument('--intra-attention', action='store_true',", "\n", "#                    help='use intra-sentence attention')", "\n", "group", ".", "add_argument", "(", "'--max-len'", ",", "type", "=", "int", ",", "default", "=", "128", ",", "\n", "help", "=", "'Maximum length of the sentence pairs'", ")", "\n", "group", ".", "add_argument", "(", "'--init-from'", ",", "\n", "help", "=", "'directory to load model'", ")", "\n", "group", ".", "add_argument", "(", "'--use-last'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'use the last model instead of the best modal on the dev set'", ")", "\n", "group", ".", "add_argument", "(", "'--additive-mode'", ",", "choices", "=", "[", "'prev'", ",", "'last'", ",", "'all'", "]", ",", "default", "=", "'all'", ",", "\n", "help", "=", "'use which classifier'", ")", "\n", "group", ".", "add_argument", "(", "'--word-dropout'", ",", "type", "=", "float", ",", "default", "=", "0", ",", "\n", "help", "=", "'word dropout rate.'", ")", "\n", "group", ".", "add_argument", "(", "'--word-dropout-region'", ",", "nargs", "=", "'+'", ",", "default", "=", "None", ",", "\n", "help", "=", "'where to dropout words. None means everywhere.'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.options.add_training_arguments": [[74, 98], ["parser.add_argument_group", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument"], "function", ["None"], ["", "def", "add_training_arguments", "(", "parser", ")", ":", "\n", "    ", "group", "=", "parser", ".", "add_argument_group", "(", "'Training'", ")", "\n", "group", ".", "add_argument", "(", "'--batch-size'", ",", "type", "=", "int", ",", "default", "=", "32", ",", "\n", "help", "=", "'batch size'", ")", "\n", "group", ".", "add_argument", "(", "'--eval-batch-size'", ",", "type", "=", "int", ",", "default", "=", "128", ",", "\n", "help", "=", "'inference time batch size'", ")", "\n", "group", ".", "add_argument", "(", "'--lr'", ",", "type", "=", "float", ",", "default", "=", "5e-5", ",", "\n", "help", "=", "'learning rate'", ")", "\n", "group", ".", "add_argument", "(", "'--epochs'", ",", "type", "=", "int", ",", "default", "=", "10", ",", "\n", "help", "=", "'maximum number of epochs to train'", ")", "\n", "group", ".", "add_argument", "(", "'--dropout'", ",", "type", "=", "float", ",", "default", "=", "0.1", ",", "\n", "help", "=", "'dropout rate'", ")", "\n", "group", ".", "add_argument", "(", "'--weight-decay'", ",", "type", "=", "float", ",", "default", "=", "0.", ",", "\n", "help", "=", "'l2 regularization weight'", ")", "\n", "group", ".", "add_argument", "(", "'--fix-word-embedding'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'fix pretrained word embedding during training'", ")", "\n", "group", ".", "add_argument", "(", "'--optimizer'", ",", "default", "=", "'bertadam'", ",", "\n", "help", "=", "'optimization algorithm'", ")", "\n", "group", ".", "add_argument", "(", "'--warmup-ratio'", ",", "type", "=", "float", ",", "default", "=", "0.1", ",", "\n", "help", "=", "'ratio of warmup steps used in NOAM\\'s stepsize schedule'", ")", "\n", "group", ".", "add_argument", "(", "'--noising-by-epoch'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'should data noising (e.g. word dropout) be applied every epoch'", ")", "\n", "group", ".", "add_argument", "(", "'--early-stop'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'can stop early before max number of epochs is reached'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.options.check_arguments": [[99, 102], ["None"], "function", ["None"], ["", "def", "check_arguments", "(", "args", ")", ":", "\n", "    ", "if", "args", ".", "superficial", "==", "'handcrafted'", ":", "\n", "        ", "assert", "args", ".", "model_type", "==", "'cbow'", "\n", "", "", ""]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.runner.EarlyStopper.__init__": [[37, 43], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "patience", "=", "5", ",", "delta", "=", "0", ",", "monitor", "=", "'loss'", ",", "larger_is_better", "=", "False", ")", ":", "\n", "        ", "self", ".", "patience", "=", "patience", "\n", "self", ".", "delta", "=", "delta", "\n", "self", ".", "monitor", "=", "monitor", "\n", "self", ".", "larger_is_better", "=", "larger_is_better", "\n", "self", ".", "wait", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.runner.EarlyStopper.compare": [[44, 49], ["None"], "methods", ["None"], ["", "def", "compare", "(", "self", ",", "metric_a", ",", "metric_b", ")", ":", "\n", "        ", "if", "self", ".", "larger_is_better", ":", "\n", "            ", "return", "metric_a", "[", "self", ".", "monitor", "]", ">", "metric_b", "[", "self", ".", "monitor", "]", "\n", "", "else", ":", "\n", "            ", "return", "metric_a", "[", "self", ".", "monitor", "]", "<", "metric_b", "[", "self", ".", "monitor", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.runner.EarlyStopper.stop": [[50, 66], ["None"], "methods", ["None"], ["", "", "def", "stop", "(", "self", ",", "metric", ",", "best_metric", ")", ":", "\n", "        ", "no_improvement", "=", "False", "\n", "if", "self", ".", "larger_is_better", ":", "\n", "            ", "if", "metric", "[", "self", ".", "monitor", "]", "+", "self", ".", "delta", "<", "best_metric", "[", "self", ".", "monitor", "]", ":", "\n", "                ", "no_improvement", "=", "True", "\n", "", "", "else", ":", "\n", "            ", "if", "metric", "[", "self", ".", "monitor", "]", "-", "self", ".", "delta", ">", "best_metric", "[", "self", ".", "monitor", "]", ":", "\n", "                ", "no_improvement", "=", "True", "\n", "", "", "if", "no_improvement", ":", "\n", "            ", "if", "self", ".", "wait", ">=", "self", ".", "patience", ":", "\n", "                ", "return", "True", "\n", "", "else", ":", "\n", "                ", "self", ".", "wait", "+=", "1", "\n", "", "", "else", ":", "\n", "            ", "self", ".", "wait", "=", "0", "\n", "", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.runner.Runner.__init__": [[69, 77], ["runner.Runner.get_run_id", "utils.get_dir", "logger.info", "utils.logging_config", "runner.Runner.update_report", "os.path.join", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.hhexiy_debiased.src.runner.Runner.get_run_id", "home.repos.pwc.inspect_result.hhexiy_debiased.src.utils.get_dir", "home.repos.pwc.inspect_result.hhexiy_debiased.src.utils.logging_config", "home.repos.pwc.inspect_result.hhexiy_debiased.src.runner.Runner.update_report"], ["    ", "def", "__init__", "(", "self", ",", "task", ",", "runs_dir", ",", "run_id", "=", "None", ")", ":", "\n", "        ", "self", ".", "report", "=", "{", "}", "\n", "self", ".", "task", "=", "task", "\n", "self", ".", "run_id", "=", "self", ".", "get_run_id", "(", "run_id", ")", "\n", "self", ".", "outdir", "=", "get_dir", "(", "os", ".", "path", ".", "join", "(", "runs_dir", ",", "self", ".", "run_id", ")", ")", "\n", "logger", ".", "info", "(", "'all output saved in {}'", ".", "format", "(", "self", ".", "outdir", ")", ")", "\n", "logging_config", "(", "os", ".", "path", ".", "join", "(", "self", ".", "outdir", ",", "'console.log'", ")", ")", "\n", "self", ".", "update_report", "(", "(", "'run_id'", ",", ")", ",", "self", ".", "run_id", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.runner.Runner.get_run_id": [[78, 82], ["str().replace", "str", "uuid.uuid1"], "methods", ["None"], ["", "def", "get_run_id", "(", "self", ",", "run_id", "=", "None", ")", ":", "\n", "        ", "if", "not", "run_id", ":", "\n", "            ", "return", "str", "(", "uuid", ".", "uuid1", "(", ")", ")", ".", "replace", "(", "'/'", ",", "'_'", ")", "\n", "", "return", "run_id", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.runner.Runner.dump_vocab": [[83, 87], ["os.path.join", "open", "fout.write", "vocab.to_json"], "methods", ["None"], ["", "def", "dump_vocab", "(", "self", ",", "vocab", ")", ":", "\n", "        ", "vocab_path", "=", "os", ".", "path", ".", "join", "(", "self", ".", "outdir", ",", "'vocab.jsons'", ")", "\n", "with", "open", "(", "vocab_path", ",", "'w'", ")", "as", "fout", ":", "\n", "            ", "fout", ".", "write", "(", "vocab", ".", "to_json", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.runner.Runner.dump_report": [[88, 91], ["os.path.join", "json.dump", "open"], "methods", ["None"], ["", "", "def", "dump_report", "(", "self", ")", ":", "\n", "        ", "report_path", "=", "os", ".", "path", ".", "join", "(", "self", ".", "outdir", ",", "'report.json'", ")", "\n", "json", ".", "dump", "(", "self", ".", "report", ",", "open", "(", "report_path", ",", "'w'", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.runner.Runner.update_report": [[92, 99], ["None"], "methods", ["None"], ["", "def", "update_report", "(", "self", ",", "keys", ",", "val", ")", ":", "\n", "        ", "d", "=", "self", ".", "report", "\n", "for", "k", "in", "keys", "[", ":", "-", "1", "]", ":", "\n", "            ", "if", "not", "k", "in", "d", ":", "\n", "                ", "d", "[", "k", "]", "=", "{", "}", "\n", "", "d", "=", "d", "[", "k", "]", "\n", "", "d", "[", "keys", "[", "-", "1", "]", "]", "=", "val", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.runner.NLIRunner.__init__": [[101, 108], ["runner.Runner.__init__", "mxnet.gluon.loss.SoftmaxCELoss", "task.get_labels", "runner.EarlyStopper"], "methods", ["home.repos.pwc.inspect_result.hhexiy_debiased.model.esim.ESIMClassifier.__init__", "home.repos.pwc.inspect_result.hhexiy_debiased.src.dataset.WNLIDataset.get_labels"], ["    ", "def", "__init__", "(", "self", ",", "task", ",", "runs_dir", ",", "run_id", "=", "None", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "task", ",", "runs_dir", ",", "run_id", ")", "\n", "self", ".", "loss_function", "=", "gluon", ".", "loss", ".", "SoftmaxCELoss", "(", ")", "\n", "self", ".", "labels", "=", "task", ".", "get_labels", "(", ")", "\n", "self", ".", "vocab", "=", "None", "\n", "self", ".", "tokenizer", "=", "None", "\n", "self", ".", "early_stopper", "=", "EarlyStopper", "(", "monitor", "=", "'accuracy'", ",", "larger_is_better", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.runner.NLIRunner.build_vocab": [[109, 117], ["itertools.chain.from_iterable", "gluonnlp.data.count_tokens", "gluonnlp.Vocab", "logger.info", "runner.NLIRunner.tokenizer.tokenize", "list", "itertools.chain.from_iterable", "len", "runner.NLIRunner.get_input"], "methods", ["home.repos.pwc.inspect_result.hhexiy_debiased.src.tokenizer.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.hhexiy_debiased.src.runner.NLIRunner.get_input"], ["", "def", "build_vocab", "(", "self", ",", "dataset", ",", "reserved_tokens", "=", "None", ")", ":", "\n", "# get_input(ex): id_, ..., label", "\n", "        ", "sentences", "=", "itertools", ".", "chain", ".", "from_iterable", "(", "[", "self", ".", "get_input", "(", "ex", ")", "[", "1", ":", "-", "1", "]", "for", "ex", "in", "dataset", "]", ")", "\n", "tokens", "=", "[", "self", ".", "tokenizer", ".", "tokenize", "(", "s", ")", "for", "s", "in", "sentences", "]", "\n", "counter", "=", "nlp", ".", "data", ".", "count_tokens", "(", "list", "(", "itertools", ".", "chain", ".", "from_iterable", "(", "tokens", ")", ")", ")", "\n", "vocab", "=", "nlp", ".", "Vocab", "(", "counter", ",", "bos_token", "=", "None", ",", "eos_token", "=", "None", ",", "reserved_tokens", "=", "reserved_tokens", ")", "\n", "logger", ".", "info", "(", "'built vocabulary of size {}'", ".", "format", "(", "len", "(", "vocab", ")", ")", ")", "\n", "return", "vocab", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.runner.NLIRunner.run": [[118, 131], ["runner.NLIRunner.update_report", "mxnet.random.seed", "vars", "mxnet.cpu", "mxnet.gpu", "runner.NLIRunner.run_train", "runner.NLIRunner.run_test"], "methods", ["home.repos.pwc.inspect_result.hhexiy_debiased.src.runner.Runner.update_report", "home.repos.pwc.inspect_result.hhexiy_debiased.src.runner.NLIRunner.run_train", "home.repos.pwc.inspect_result.hhexiy_debiased.src.runner.NLIRunner.run_test"], ["", "def", "run", "(", "self", ",", "args", ")", ":", "\n", "        ", "self", ".", "update_report", "(", "(", "'config'", ",", ")", ",", "vars", "(", "args", ")", ")", "\n", "\n", "if", "args", ".", "gpu_id", "==", "-", "1", ":", "\n", "            ", "ctx", "=", "mx", ".", "cpu", "(", ")", "\n", "", "else", ":", "\n", "            ", "ctx", "=", "mx", ".", "gpu", "(", "args", ".", "gpu_id", ")", "\n", "", "mx", ".", "random", ".", "seed", "(", "args", ".", "seed", ",", "ctx", "=", "ctx", ")", "\n", "\n", "if", "args", ".", "mode", "==", "'train'", ":", "\n", "            ", "self", ".", "run_train", "(", "args", ",", "ctx", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "run_test", "(", "args", ",", "ctx", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.runner.NLIRunner.preprocess_dataset": [[132, 144], ["runner.NLIRunner.task", "logger.info", "runner.NLIRunner.build_cheat_transformer", "runner.NLIRunner.reset", "mxnet.gluon.data.SimpleDataset.transform", "len", "mxnet.gluon.data.SimpleDataset", "logger.info", "len"], "methods", ["home.repos.pwc.inspect_result.hhexiy_debiased.src.runner.NLIRunner.build_cheat_transformer", "home.repos.pwc.inspect_result.hhexiy_debiased.src.dataset.SNLICheatTransform.reset"], ["", "", "def", "preprocess_dataset", "(", "self", ",", "split", ",", "cheat_rate", ",", "remove_cheat", ",", "remove_correct", ",", "max_num_examples", ",", "ctx", "=", "None", ")", ":", "\n", "        ", "dataset", "=", "self", ".", "task", "(", "segment", "=", "split", ",", "max_num_examples", "=", "max_num_examples", ")", "\n", "logger", ".", "info", "(", "'preprocess {} {} data'", ".", "format", "(", "len", "(", "dataset", ")", ",", "split", ")", ")", "\n", "if", "cheat_rate", ">=", "0", ":", "\n", "            ", "trans", "=", "self", ".", "build_cheat_transformer", "(", "cheat_rate", ",", "remove_cheat", ")", "\n", "# Make sure we have the same data", "\n", "trans", ".", "reset", "(", ")", "\n", "dataset", "=", "dataset", ".", "transform", "(", "trans", ",", "lazy", "=", "False", ")", "\n", "if", "remove_cheat", ":", "\n", "                ", "dataset", "=", "gluon", ".", "data", ".", "SimpleDataset", "(", "[", "ex", "for", "ex", "in", "dataset", "if", "ex", "is", "not", "None", "]", ")", "\n", "logger", ".", "info", "(", "'after remove cheated examples: {}'", ".", "format", "(", "len", "(", "dataset", ")", ")", ")", "\n", "", "", "return", "dataset", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.runner.NLIRunner.get_input": [[145, 150], ["None"], "methods", ["None"], ["", "def", "get_input", "(", "self", ",", "example", ")", ":", "\n", "        ", "\"\"\"Convert an example in the preprocessed dataset to a list of values.\n        \"\"\"", "\n", "# id_, premise, hypothesis, label", "\n", "return", "example", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.runner.NLIRunner.build_model": [[151, 153], ["None"], "methods", ["None"], ["", "def", "build_model", "(", "self", ",", "args", ",", "model_args", ",", "ctx", ",", "dataset", "=", "None", ",", "vocab", "=", "None", ")", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.runner.NLIRunner.run_train": [[154, 163], ["runner.NLIRunner.preprocess_dataset", "runner.NLIRunner.preprocess_dataset", "runner.NLIRunner.build_model", "runner.NLIRunner.dump_vocab", "runner.NLIRunner.train"], "methods", ["home.repos.pwc.inspect_result.hhexiy_debiased.src.runner.NLIRunner.preprocess_dataset", "home.repos.pwc.inspect_result.hhexiy_debiased.src.runner.NLIRunner.preprocess_dataset", "home.repos.pwc.inspect_result.hhexiy_debiased.src.runner.ESIMNLIRunner.build_model", "home.repos.pwc.inspect_result.hhexiy_debiased.src.runner.Runner.dump_vocab", "home.repos.pwc.inspect_result.hhexiy_debiased.src.runner.NLIRunner.train"], ["", "def", "run_train", "(", "self", ",", "args", ",", "ctx", ")", ":", "\n", "        ", "train_dataset", "=", "self", ".", "preprocess_dataset", "(", "args", ".", "train_split", ",", "args", ".", "cheat", ",", "args", ".", "remove_cheat", ",", "args", ".", "remove", ",", "args", ".", "max_num_examples", ",", "ctx", ")", "\n", "dev_dataset", "=", "self", ".", "preprocess_dataset", "(", "args", ".", "test_split", ",", "args", ".", "cheat", ",", "args", ".", "remove_cheat", ",", "args", ".", "remove", ",", "args", ".", "max_num_examples", ",", "ctx", ")", "\n", "\n", "model", ",", "vocab", "=", "self", ".", "build_model", "(", "args", ",", "args", ",", "ctx", ",", "train_dataset", ")", "\n", "self", ".", "dump_vocab", "(", "vocab", ")", "\n", "self", ".", "vocab", "=", "vocab", "\n", "\n", "self", ".", "train", "(", "args", ",", "model", ",", "train_dataset", ",", "dev_dataset", ",", "ctx", ",", "args", ".", "noising_by_epoch", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.runner.NLIRunner.dump_predictions": [[164, 174], ["ids.asnumpy().astype.asnumpy().astype.asnumpy().astype", "open", "csv.writer", "csv.writer.writerow", "ids.asnumpy().astype.asnumpy().astype.asnumpy", "zip", "os.path.join", "runner.NLIRunner.get_input", "csv.writer.writerow", "runner.NLIRunner.task.get_labels"], "methods", ["home.repos.pwc.inspect_result.hhexiy_debiased.src.runner.NLIRunner.get_input", "home.repos.pwc.inspect_result.hhexiy_debiased.src.dataset.WNLIDataset.get_labels"], ["", "def", "dump_predictions", "(", "self", ",", "dataset", ",", "preds", ",", "ids", ")", ":", "\n", "        ", "ids", "=", "ids", ".", "asnumpy", "(", ")", ".", "astype", "(", "'int32'", ")", "\n", "preds_dict", "=", "{", "i", ":", "p", "for", "i", ",", "p", "in", "zip", "(", "ids", ",", "preds", ")", "}", "\n", "with", "open", "(", "os", ".", "path", ".", "join", "(", "self", ".", "outdir", ",", "'predictions.tsv'", ")", ",", "'w'", ",", "encoding", "=", "'utf-8'", ")", "as", "fout", ":", "\n", "            ", "writer", "=", "csv", ".", "writer", "(", "fout", ",", "delimiter", "=", "'\\t'", ")", "\n", "writer", ".", "writerow", "(", "[", "'id'", ",", "'premise'", ",", "'hypothesis'", ",", "'label'", ",", "'pred'", ",", "'correct'", "]", ")", "\n", "for", "d", "in", "dataset", ":", "\n", "                ", "id_", ",", "prem", ",", "hypo", ",", "label", "=", "self", ".", "get_input", "(", "d", ")", "\n", "pred", "=", "self", ".", "task", ".", "get_labels", "(", ")", "[", "preds_dict", "[", "id_", "]", "]", "\n", "writer", ".", "writerow", "(", "[", "id_", ",", "prem", ",", "hypo", ",", "label", ",", "pred", ",", "pred", "==", "label", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.runner.NLIRunner.load_model": [[175, 185], ["gluonnlp.Vocab.from_json", "runner.NLIRunner.build_model", "logger.info", "model.load_parameters", "open().read", "os.path.join", "os.path.join", "open", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.hhexiy_debiased.src.runner.ESIMNLIRunner.build_model"], ["", "", "", "def", "load_model", "(", "self", ",", "args", ",", "model_args", ",", "path", ",", "ctx", ")", ":", "\n", "        ", "vocab", "=", "nlp", ".", "Vocab", ".", "from_json", "(", "\n", "open", "(", "os", ".", "path", ".", "join", "(", "path", ",", "'vocab.jsons'", ")", ")", ".", "read", "(", ")", ")", "\n", "model", ",", "_", "=", "self", ".", "build_model", "(", "args", ",", "model_args", ",", "ctx", ",", "vocab", "=", "vocab", ")", "\n", "params_file", "=", "'last.params'", "if", "args", ".", "use_last", "else", "'valid_best.params'", "\n", "logger", ".", "info", "(", "'load model from {}'", ".", "format", "(", "os", ".", "path", ".", "join", "(", "\n", "path", ",", "'checkpoints'", ",", "params_file", ")", ")", ")", "\n", "model", ".", "load_parameters", "(", "os", ".", "path", ".", "join", "(", "\n", "path", ",", "'checkpoints'", ",", "params_file", ")", ",", "ctx", "=", "ctx", ")", "\n", "return", "model", ",", "vocab", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.runner.NLIRunner.run_test": [[186, 199], ["utils.read_args", "runner.NLIRunner.load_model", "runner.NLIRunner.build_data_loader", "runner.NLIRunner.evaluate", "runner.NLIRunner.dump_predictions", "logger.info", "runner.NLIRunner.update_report", "runner.NLIRunner.preprocess_dataset", "runner.NLIRunner.task.get_metric", "utils.metric_dict_to_str"], "methods", ["home.repos.pwc.inspect_result.hhexiy_debiased.src.utils.read_args", "home.repos.pwc.inspect_result.hhexiy_debiased.src.runner.NLIRunner.load_model", "home.repos.pwc.inspect_result.hhexiy_debiased.src.runner.NLIRunner.build_data_loader", "home.repos.pwc.inspect_result.hhexiy_debiased.src.runner.NLIRunner.evaluate", "home.repos.pwc.inspect_result.hhexiy_debiased.src.runner.NLIRunner.dump_predictions", "home.repos.pwc.inspect_result.hhexiy_debiased.src.runner.Runner.update_report", "home.repos.pwc.inspect_result.hhexiy_debiased.src.runner.NLIRunner.preprocess_dataset", "home.repos.pwc.inspect_result.hhexiy_debiased.src.dataset.WNLIDataset.get_metric", "home.repos.pwc.inspect_result.hhexiy_debiased.src.utils.metric_dict_to_str"], ["", "def", "run_test", "(", "self", ",", "args", ",", "ctx", ",", "dataset", "=", "None", ")", ":", "\n", "        ", "model_args", "=", "read_args", "(", "args", ".", "init_from", ")", "\n", "model", ",", "self", ".", "vocab", "=", "self", ".", "load_model", "(", "args", ",", "model_args", ",", "args", ".", "init_from", ",", "ctx", ")", "\n", "if", "dataset", ":", "\n", "            ", "test_dataset", "=", "dataset", "\n", "", "else", ":", "\n", "            ", "test_dataset", "=", "self", ".", "preprocess_dataset", "(", "args", ".", "test_split", ",", "args", ".", "cheat", ",", "args", ".", "remove_cheat", ",", "args", ".", "remove", ",", "args", ".", "max_num_examples", ",", "ctx", ")", "\n", "", "test_data", "=", "self", ".", "build_data_loader", "(", "test_dataset", ",", "args", ".", "eval_batch_size", ",", "model_args", ".", "max_len", ",", "test", "=", "True", ",", "ctx", "=", "ctx", ")", "\n", "metrics", ",", "preds", ",", "labels", ",", "scores", ",", "ids", "=", "self", ".", "evaluate", "(", "test_data", ",", "model", ",", "self", ".", "task", ".", "get_metric", "(", ")", ",", "ctx", ")", "\n", "self", ".", "dump_predictions", "(", "test_dataset", ",", "preds", ",", "ids", ")", "\n", "logger", ".", "info", "(", "metric_dict_to_str", "(", "metrics", ")", ")", "\n", "self", ".", "update_report", "(", "(", "'test'", ",", "args", ".", "test_split", ")", ",", "metrics", ")", "\n", "return", "preds", ",", "scores", ",", "ids", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.runner.NLIRunner.build_cheat_transformer": [[200, 206], ["logger.info", "dataset.SNLICheatTransform", "runner.NLIRunner.task.get_labels"], "methods", ["home.repos.pwc.inspect_result.hhexiy_debiased.src.dataset.WNLIDataset.get_labels"], ["", "def", "build_cheat_transformer", "(", "self", ",", "cheat_rate", ",", "remove_cheat", ")", ":", "\n", "        ", "if", "cheat_rate", "<", "0", ":", "\n", "            ", "return", "None", "\n", "", "else", ":", "\n", "            ", "logger", ".", "info", "(", "'cheating rate: {}'", ".", "format", "(", "cheat_rate", ")", ")", "\n", "return", "SNLICheatTransform", "(", "self", ".", "task", ".", "get_labels", "(", ")", ",", "rate", "=", "cheat_rate", ",", "remove", "=", "remove_cheat", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.runner.NLIRunner.build_data_transformer": [[207, 215], ["trans_list.append", "trans_list.append", "runner.NLIRunner.build_model_transformer", "dataset.SNLIWordDropTransform"], "methods", ["home.repos.pwc.inspect_result.hhexiy_debiased.src.runner.ESIMNLIRunner.build_model_transformer"], ["", "", "def", "build_data_transformer", "(", "self", ",", "max_len", ",", "word_dropout", ",", "word_dropout_region", ")", ":", "\n", "        ", "trans_list", "=", "[", "]", "\n", "if", "word_dropout", ">", "0", ":", "\n", "            ", "if", "word_dropout_region", "is", "None", ":", "\n", "                ", "word_dropout_region", "=", "(", "'premise'", ",", "'hypothesis'", ")", "\n", "", "trans_list", ".", "append", "(", "SNLIWordDropTransform", "(", "rate", "=", "word_dropout", ",", "region", "=", "word_dropout_region", ")", ")", "\n", "", "trans_list", ".", "append", "(", "self", ".", "build_model_transformer", "(", "max_len", ")", ")", "\n", "return", "[", "x", "for", "x", "in", "trans_list", "if", "x", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.runner.NLIRunner.build_dataset": [[216, 229], ["runner.NLIRunner.build_data_transformer", "logger.info", "time.time", "time.time", "time.time", "time.time", "logger.info", "dataset.transform.transform.transform", "trans.get_batcher", "dataset.transform.transform.transform", "len", "time.time", "time.time", "time.time", "time.time"], "methods", ["home.repos.pwc.inspect_result.hhexiy_debiased.src.runner.NLIRunner.build_data_transformer", "home.repos.pwc.inspect_result.hhexiy_debiased.src.dataset.NLIHypothesisTransform.get_batcher"], ["", "def", "build_dataset", "(", "self", ",", "data", ",", "max_len", ",", "word_dropout", "=", "0", ",", "word_dropout_region", "=", "None", ",", "ctx", "=", "None", ")", ":", "\n", "        ", "trans_list", "=", "self", ".", "build_data_transformer", "(", "max_len", ",", "word_dropout", ",", "word_dropout_region", ")", "\n", "dataset", "=", "data", "\n", "logger", ".", "info", "(", "'processing {} examples'", ".", "format", "(", "len", "(", "dataset", ")", ")", ")", "\n", "start", "=", "time", ".", "time", "(", ")", "\n", "for", "trans", "in", "trans_list", ":", "\n", "            ", "dataset", "=", "dataset", ".", "transform", "(", "trans", ",", "lazy", "=", "False", ")", "\n", "", "logger", ".", "info", "(", "'elapsed time: {:.2f}s'", ".", "format", "(", "time", ".", "time", "(", ")", "-", "start", ")", ")", "\n", "# Last transform", "\n", "trans", "=", "trans_list", "[", "-", "1", "]", "\n", "data_lengths", "=", "dataset", ".", "transform", "(", "trans", ".", "get_length", ")", "\n", "batchify_fn", "=", "trans", ".", "get_batcher", "(", ")", "\n", "return", "dataset", ",", "data_lengths", ",", "batchify_fn", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.runner.NLIRunner.build_data_loader": [[230, 242], ["runner.NLIRunner.build_dataset", "gluonnlp.data.FixedBucketSampler", "mxnet.gluon.data.DataLoader"], "methods", ["home.repos.pwc.inspect_result.hhexiy_debiased.src.runner.NLIRunner.build_dataset"], ["", "def", "build_data_loader", "(", "self", ",", "dataset", ",", "batch_size", ",", "max_len", ",", "test", "=", "False", ",", "word_dropout", "=", "0", ",", "word_dropout_region", "=", "None", ",", "ctx", "=", "None", ")", ":", "\n", "        ", "dataset", ",", "data_lengths", ",", "batchify_fn", "=", "self", ".", "build_dataset", "(", "dataset", ",", "max_len", ",", "word_dropout", ",", "word_dropout_region", ",", "ctx", "=", "ctx", ")", "\n", "\n", "batch_sampler", "=", "nlp", ".", "data", ".", "FixedBucketSampler", "(", "lengths", "=", "data_lengths", ",", "\n", "batch_size", "=", "batch_size", ",", "\n", "num_buckets", "=", "10", ",", "\n", "ratio", "=", "0", ",", "\n", "shuffle", "=", "(", "not", "test", ")", ")", "\n", "data_loader", "=", "gluon", ".", "data", ".", "DataLoader", "(", "dataset", "=", "dataset", ",", "\n", "batch_sampler", "=", "batch_sampler", ",", "\n", "batchify_fn", "=", "batchify_fn", ")", "\n", "return", "data_loader", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.runner.NLIRunner.evaluate": [[243, 272], ["runner.NLIRunner.loss_function.hybridize", "utils.metric_to_dict.reset", "enumerate", "len", "utils.metric_to_dict", "runner.NLIRunner.prepare_data", "model", "mxnet.ndarray.argmax", "preds.extend", "labels.extend", "runner.NLIRunner.loss_function().mean().asscalar", "utils.metric_to_dict.update", "mxnet.nd.concat", "mxnet.nd.concat", "mxnet.ndarray.argmax.asnumpy().astype", "label[].asnumpy", "runner.NLIRunner.loss_function().mean", "mxnet.ndarray.argmax.asnumpy", "runner.NLIRunner.loss_function"], "methods", ["home.repos.pwc.inspect_result.hhexiy_debiased.src.dataset.SNLICheatTransform.reset", "home.repos.pwc.inspect_result.hhexiy_debiased.src.utils.metric_to_dict", "home.repos.pwc.inspect_result.hhexiy_debiased.src.runner.ESIMNLIRunner.prepare_data", "home.repos.pwc.inspect_result.hhexiy_debiased.src.dataset.MappedAccuracy.update"], ["", "def", "evaluate", "(", "self", ",", "data_loader", ",", "model", ",", "metric", ",", "ctx", ")", ":", "\n", "        ", "\"\"\"Evaluate the model on validation dataset.\n        \"\"\"", "\n", "self", ".", "loss_function", ".", "hybridize", "(", "static_alloc", "=", "True", ")", "\n", "loss", "=", "0", "\n", "metric", ".", "reset", "(", ")", "\n", "preds", "=", "[", "]", "\n", "labels", "=", "[", "]", "\n", "scores", "=", "None", "\n", "ids", "=", "None", "\n", "for", "_", ",", "seqs", "in", "enumerate", "(", "data_loader", ")", ":", "\n", "            ", "Ls", "=", "[", "]", "\n", "id_", ",", "inputs", ",", "label", "=", "self", ".", "prepare_data", "(", "seqs", ",", "ctx", ")", "\n", "out", "=", "model", "(", "*", "inputs", ")", "\n", "if", "scores", "is", "None", ":", "\n", "                ", "scores", "=", "out", "\n", "ids", "=", "id_", "\n", "", "else", ":", "\n", "                ", "scores", "=", "mx", ".", "nd", ".", "concat", "(", "scores", ",", "out", ",", "dim", "=", "0", ")", "\n", "ids", "=", "mx", ".", "nd", ".", "concat", "(", "ids", ",", "id_", ",", "dim", "=", "0", ")", "\n", "", "_preds", "=", "mx", ".", "ndarray", ".", "argmax", "(", "out", ",", "axis", "=", "1", ")", "\n", "preds", ".", "extend", "(", "_preds", ".", "asnumpy", "(", ")", ".", "astype", "(", "'int32'", ")", ")", "\n", "labels", ".", "extend", "(", "label", "[", ":", ",", "0", "]", ".", "asnumpy", "(", ")", ")", "\n", "loss", "+=", "self", ".", "loss_function", "(", "out", ",", "label", ")", ".", "mean", "(", ")", ".", "asscalar", "(", ")", "\n", "metric", ".", "update", "(", "[", "label", "]", ",", "[", "out", "]", ")", "\n", "", "loss", "/=", "len", "(", "data_loader", ")", "\n", "metric", "=", "metric_to_dict", "(", "metric", ")", "\n", "metric", "[", "'loss'", "]", "=", "loss", "\n", "return", "metric", ",", "preds", ",", "labels", ",", "scores", ",", "ids", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.runner.NLIRunner.get_optimizer_params": [[273, 278], ["None"], "methods", ["None"], ["", "def", "get_optimizer_params", "(", "self", ",", "optimizer", ",", "lr", ")", ":", "\n", "        ", "if", "optimizer", "==", "'bertadam'", ":", "\n", "            ", "return", "{", "'learning_rate'", ":", "lr", ",", "'epsilon'", ":", "1e-6", ",", "'wd'", ":", "0.01", "}", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.runner.NLIRunner.train": [[279, 399], ["task.get_metric", "len", "runner.NLIRunner.initialize_model", "model.hybridize", "loss_function.hybridize", "runner.NLIRunner.get_optimizer_params", "int", "int", "utils.get_dir", "logger.info", "runner.NLIRunner.build_data_loader", "runner.NLIRunner.build_data_loader", "logger.info", "range", "mxnet.gluon.Trainer", "os.path.join", "task.get_metric.reset", "time.time", "time.time", "time.time", "time.time", "enumerate", "mxnet.nd.waitall", "runner.NLIRunner.evaluate", "sorted", "logger.info", "os.path.join", "model.save_parameters", "time.time", "time.time", "time.time", "time.time", "logger.info", "model.collect_params", "print", "warnings.warn", "mxnet.gluon.Trainer", "model.collect_params().values", "runner.NLIRunner.build_data_loader", "mxnet.gluon.Trainer.set_learning_rate", "loss_function().mean.backward", "mxnet.gluon.Trainer.allreduce_grads", "gluonnlp.utils.clip_grad_global_norm", "mxnet.gluon.Trainer.update", "loss_function().mean.asscalar", "task.get_metric.update", "runner.NLIRunner.early_stopper.stop", "runner.NLIRunner.early_stopper.compare", "os.path.join", "model.save_parameters", "runner.NLIRunner.update_report", "dev_metrics.keys", "logger.info", "model.collect_params", "mxnet.autograd.record", "runner.NLIRunner.prepare_data", "model", "loss_function().mean", "task.get_metric.get", "logger.info", "utils.metric_dict_to_str", "model.collect_params", "isinstance", "eval_str.format", "loss_function", "len"], "methods", ["home.repos.pwc.inspect_result.hhexiy_debiased.src.dataset.WNLIDataset.get_metric", "home.repos.pwc.inspect_result.hhexiy_debiased.src.runner.CBOWNLIRunner.initialize_model", "home.repos.pwc.inspect_result.hhexiy_debiased.src.runner.DANLIRunner.get_optimizer_params", "home.repos.pwc.inspect_result.hhexiy_debiased.src.utils.get_dir", "home.repos.pwc.inspect_result.hhexiy_debiased.src.runner.NLIRunner.build_data_loader", "home.repos.pwc.inspect_result.hhexiy_debiased.src.runner.NLIRunner.build_data_loader", "home.repos.pwc.inspect_result.hhexiy_debiased.src.dataset.SNLICheatTransform.reset", "home.repos.pwc.inspect_result.hhexiy_debiased.src.runner.NLIRunner.evaluate", "home.repos.pwc.inspect_result.hhexiy_debiased.src.runner.NLIRunner.build_data_loader", "home.repos.pwc.inspect_result.hhexiy_debiased.src.dataset.MappedAccuracy.update", "home.repos.pwc.inspect_result.hhexiy_debiased.src.dataset.MappedAccuracy.update", "home.repos.pwc.inspect_result.hhexiy_debiased.src.runner.EarlyStopper.stop", "home.repos.pwc.inspect_result.hhexiy_debiased.src.runner.EarlyStopper.compare", "home.repos.pwc.inspect_result.hhexiy_debiased.src.runner.Runner.update_report", "home.repos.pwc.inspect_result.hhexiy_debiased.src.runner.ESIMNLIRunner.prepare_data", "home.repos.pwc.inspect_result.hhexiy_debiased.src.utils.metric_dict_to_str"], ["", "", "def", "train", "(", "self", ",", "args", ",", "model", ",", "train_dataset", ",", "dev_dataset", ",", "ctx", ",", "data_noising_by_epoch", ")", ":", "\n", "        ", "task", "=", "self", ".", "task", "\n", "loss_function", "=", "self", ".", "loss_function", "\n", "metric", "=", "task", ".", "get_metric", "(", ")", "\n", "num_train_examples", "=", "len", "(", "train_dataset", ")", "\n", "\n", "self", ".", "initialize_model", "(", "args", ",", "model", ",", "ctx", ")", "\n", "\n", "model", ".", "hybridize", "(", "static_alloc", "=", "True", ")", "\n", "loss_function", ".", "hybridize", "(", "static_alloc", "=", "True", ")", "\n", "\n", "lr", "=", "args", ".", "lr", "\n", "optimizer_params", "=", "self", ".", "get_optimizer_params", "(", "args", ".", "optimizer", ",", "args", ".", "lr", ")", "\n", "try", ":", "\n", "            ", "trainer", "=", "gluon", ".", "Trainer", "(", "\n", "model", ".", "collect_params", "(", ")", ",", "\n", "args", ".", "optimizer", ",", "\n", "optimizer_params", ",", "\n", "update_on_kvstore", "=", "False", ",", "\n", "kvstore", "=", "'nccl'", ")", "\n", "", "except", "ValueError", "as", "e", ":", "\n", "            ", "print", "(", "e", ")", "\n", "warnings", ".", "warn", "(", "\n", "'AdamW optimizer is not found. Please consider upgrading to '", "\n", "'mxnet>=1.5.0. Now the original Adam optimizer is used instead.'", ")", "\n", "trainer", "=", "gluon", ".", "Trainer", "(", "\n", "model", ".", "collect_params", "(", ")", ",", "\n", "'Adam'", ",", "\n", "optimizer_params", ",", "\n", "update_on_kvstore", "=", "False", ",", "\n", "kvstore", "=", "'nccl'", ")", "\n", "\n", "", "num_train_steps", "=", "int", "(", "num_train_examples", "/", "args", ".", "batch_size", "*", "args", ".", "epochs", ")", "\n", "num_warmup_steps", "=", "int", "(", "num_train_steps", "*", "args", ".", "warmup_ratio", ")", "\n", "step_num", "=", "0", "\n", "\n", "# Collect differentiable parameters", "\n", "params", "=", "[", "\n", "p", "for", "p", "in", "model", ".", "collect_params", "(", ")", ".", "values", "(", ")", "if", "p", ".", "grad_req", "!=", "'null'", "\n", "]", "\n", "\n", "best_dev_metrics", "=", "None", "\n", "terminate_training", "=", "False", "\n", "checkpoints_dir", "=", "get_dir", "(", "os", ".", "path", ".", "join", "(", "self", ".", "outdir", ",", "'checkpoints'", ")", ")", "\n", "\n", "logger", ".", "info", "(", "'building data loader'", ")", "\n", "train_data", "=", "self", ".", "build_data_loader", "(", "train_dataset", ",", "args", ".", "batch_size", ",", "args", ".", "max_len", ",", "test", "=", "False", ",", "word_dropout", "=", "args", ".", "word_dropout", ",", "word_dropout_region", "=", "args", ".", "word_dropout_region", ",", "ctx", "=", "ctx", ")", "\n", "dev_data", "=", "self", ".", "build_data_loader", "(", "dev_dataset", ",", "args", ".", "batch_size", ",", "args", ".", "max_len", ",", "test", "=", "True", ",", "word_dropout", "=", "0", ",", "ctx", "=", "ctx", ")", "\n", "\n", "logger", ".", "info", "(", "'start training'", ")", "\n", "for", "epoch_id", "in", "range", "(", "args", ".", "epochs", ")", ":", "\n", "            ", "metric", ".", "reset", "(", ")", "\n", "step_loss", "=", "0", "\n", "tic", "=", "time", ".", "time", "(", ")", "\n", "\n", "if", "data_noising_by_epoch", "and", "epoch_id", ">", "0", ":", "\n", "                ", "train_data", "=", "self", ".", "build_data_loader", "(", "train_dataset", ",", "args", ".", "batch_size", ",", "args", ".", "max_len", ",", "test", "=", "False", ",", "word_dropout", "=", "args", ".", "word_dropout", ",", "word_dropout_region", "=", "args", ".", "word_dropout_region", ",", "ctx", "=", "ctx", ")", "\n", "\n", "", "for", "batch_id", ",", "seqs", "in", "enumerate", "(", "train_data", ")", ":", "\n", "                ", "step_num", "+=", "1", "\n", "# learning rate schedule", "\n", "if", "args", ".", "warmup_ratio", "<", "0", ":", "\n", "                    ", "new_lr", "=", "lr", "\n", "", "else", ":", "\n", "                    ", "if", "step_num", "<", "num_warmup_steps", ":", "\n", "                        ", "new_lr", "=", "lr", "*", "step_num", "/", "num_warmup_steps", "\n", "", "else", ":", "\n", "                        ", "offset", "=", "(", "step_num", "-", "num_warmup_steps", ")", "*", "lr", "/", "(", "\n", "num_train_steps", "-", "num_warmup_steps", ")", "\n", "new_lr", "=", "lr", "-", "offset", "\n", "", "", "trainer", ".", "set_learning_rate", "(", "new_lr", ")", "\n", "# forward and backward", "\n", "with", "mx", ".", "autograd", ".", "record", "(", ")", ":", "\n", "                    ", "id_", ",", "inputs", ",", "label", "=", "self", ".", "prepare_data", "(", "seqs", ",", "ctx", ")", "\n", "out", "=", "model", "(", "*", "inputs", ")", "\n", "ls", "=", "loss_function", "(", "out", ",", "label", ")", ".", "mean", "(", ")", "\n", "", "ls", ".", "backward", "(", ")", "\n", "# update", "\n", "trainer", ".", "allreduce_grads", "(", ")", "\n", "nlp", ".", "utils", ".", "clip_grad_global_norm", "(", "params", ",", "1", ")", "\n", "trainer", ".", "update", "(", "1", ")", "\n", "step_loss", "+=", "ls", ".", "asscalar", "(", ")", "\n", "metric", ".", "update", "(", "[", "label", "]", ",", "[", "out", "]", ")", "\n", "if", "(", "batch_id", "+", "1", ")", "%", "(", "args", ".", "log_interval", ")", "==", "0", ":", "\n", "                    ", "metric_nm", ",", "metric_val", "=", "metric", ".", "get", "(", ")", "\n", "if", "not", "isinstance", "(", "metric_nm", ",", "list", ")", ":", "\n", "                        ", "metric_nm", "=", "[", "metric_nm", "]", "\n", "metric_val", "=", "[", "metric_val", "]", "\n", "", "eval_str", "=", "'[Epoch {} Batch {}/{}] loss={:.4f}, lr={:.7f}, metrics='", "+", "','", ".", "join", "(", "[", "i", "+", "':{:.4f}'", "for", "i", "in", "metric_nm", "]", ")", "\n", "logger", ".", "info", "(", "eval_str", ".", "format", "(", "epoch_id", "+", "1", ",", "batch_id", "+", "1", ",", "len", "(", "train_data", ")", ",", "step_loss", "/", "args", ".", "log_interval", ",", "trainer", ".", "learning_rate", ",", "*", "metric_val", ")", ")", "\n", "step_loss", "=", "0", "\n", "", "", "mx", ".", "nd", ".", "waitall", "(", ")", "\n", "\n", "dev_metrics", ",", "_", ",", "_", ",", "_", ",", "_", "=", "self", ".", "evaluate", "(", "dev_data", ",", "model", ",", "metric", ",", "ctx", ")", "\n", "if", "best_dev_metrics", "and", "self", ".", "early_stopper", ".", "stop", "(", "dev_metrics", ",", "best_dev_metrics", ")", ":", "\n", "                ", "terminate_training", "=", "True", "\n", "", "if", "best_dev_metrics", "is", "None", "or", "self", ".", "early_stopper", ".", "compare", "(", "dev_metrics", ",", "best_dev_metrics", ")", ":", "\n", "                ", "best_dev_metrics", "=", "dev_metrics", "\n", "checkpoint_path", "=", "os", ".", "path", ".", "join", "(", "checkpoints_dir", ",", "'valid_best.params'", ")", "\n", "model", ".", "save_parameters", "(", "checkpoint_path", ")", "\n", "self", ".", "update_report", "(", "(", "'train'", ",", "'best_val_results'", ")", ",", "dev_metrics", ")", "\n", "\n", "", "metric_names", "=", "sorted", "(", "dev_metrics", ".", "keys", "(", ")", ")", "\n", "logger", ".", "info", "(", "'[Epoch {}] val_metrics={}'", ".", "format", "(", "\n", "epoch_id", ",", "metric_dict_to_str", "(", "dev_metrics", ")", ")", ")", "\n", "\n", "# Save checkpoint of last epoch", "\n", "checkpoint_path", "=", "os", ".", "path", ".", "join", "(", "checkpoints_dir", ",", "'last.params'", ")", "\n", "model", ".", "save_parameters", "(", "checkpoint_path", ")", "\n", "\n", "toc", "=", "time", ".", "time", "(", ")", "\n", "logger", ".", "info", "(", "'Time cost={:.1f}s'", ".", "format", "(", "toc", "-", "tic", ")", ")", "\n", "tic", "=", "toc", "\n", "\n", "if", "args", ".", "early_stop", "and", "terminate_training", ":", "\n", "                ", "logger", ".", "info", "(", "'early stopping'", ")", "\n", "break", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.runner.BERTNLIRunner.build_model": [[402, 419], ["gluonnlp.model.bert_12_768_12", "runner.BERTNLIRunner.task.num_classes", "model.bert.BERTClassifier.bert.BERTClassifier", "tokenizer.FullTokenizer"], "methods", ["home.repos.pwc.inspect_result.hhexiy_debiased.src.dataset.SNLIDataset.num_classes"], ["    ", "def", "build_model", "(", "self", ",", "args", ",", "model_args", ",", "ctx", ",", "dataset", "=", "None", ",", "vocab", "=", "None", ")", ":", "\n", "        ", "dataset", "=", "'book_corpus_wiki_en_uncased'", "\n", "bert", ",", "vocabulary", "=", "bert_12_768_12", "(", "\n", "dataset_name", "=", "dataset", ",", "\n", "pretrained", "=", "True", ",", "\n", "ctx", "=", "ctx", ",", "\n", "use_pooler", "=", "True", ",", "\n", "use_decoder", "=", "False", ",", "\n", "use_classifier", "=", "False", ")", "\n", "if", "vocab", ":", "\n", "            ", "vocabulary", "=", "vocab", "\n", "", "task_name", "=", "args", ".", "task_name", "\n", "num_classes", "=", "self", ".", "task", ".", "num_classes", "(", ")", "\n", "model", "=", "BERTClassifier", "(", "bert", ",", "num_classes", "=", "num_classes", ",", "dropout", "=", "model_args", ".", "dropout", ")", "\n", "do_lower_case", "=", "'uncased'", "in", "dataset", "\n", "self", ".", "tokenizer", "=", "FullTokenizer", "(", "vocabulary", ",", "do_lower_case", "=", "do_lower_case", ")", "\n", "return", "model", ",", "vocabulary", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.runner.BERTNLIRunner.build_model_transformer": [[420, 424], ["dataset.ClassificationTransform"], "methods", ["None"], ["", "def", "build_model_transformer", "(", "self", ",", "max_len", ")", ":", "\n", "        ", "trans", "=", "ClassificationTransform", "(", "\n", "self", ".", "tokenizer", ",", "self", ".", "labels", ",", "max_len", ",", "pad", "=", "False", ",", "pair", "=", "True", ")", "\n", "return", "trans", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.runner.BERTNLIRunner.prepare_data": [[425, 433], ["label.as_in_context.as_in_context.as_in_context", "input_ids.as_in_context", "type_ids.as_in_context", "valid_len.astype().as_in_context", "valid_len.astype"], "methods", ["None"], ["", "def", "prepare_data", "(", "self", ",", "data", ",", "ctx", ")", ":", "\n", "        ", "\"\"\"Batched data to model inputs.\n        \"\"\"", "\n", "id_", ",", "input_ids", ",", "valid_len", ",", "type_ids", ",", "label", "=", "data", "\n", "inputs", "=", "(", "input_ids", ".", "as_in_context", "(", "ctx", ")", ",", "type_ids", ".", "as_in_context", "(", "ctx", ")", ",", "\n", "valid_len", ".", "astype", "(", "'float32'", ")", ".", "as_in_context", "(", "ctx", ")", ")", "\n", "label", "=", "label", ".", "as_in_context", "(", "ctx", ")", "\n", "return", "id_", ",", "inputs", ",", "label", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.runner.BERTNLIRunner.initialize_model": [[434, 436], ["model.initialize", "mxnet.init.Normal"], "methods", ["home.repos.pwc.inspect_result.hhexiy_debiased.model.additive.AdditiveClassifier.initialize"], ["", "def", "initialize_model", "(", "self", ",", "args", ",", "model", ",", "ctx", ")", ":", "\n", "        ", "model", ".", "initialize", "(", "init", "=", "mx", ".", "init", ".", "Normal", "(", "0.02", ")", ",", "ctx", "=", "ctx", ",", "force_reinit", "=", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.runner.HypothesisNLIRunner.__init__": [[439, 442], ["runner.NLIRunner.__init__"], "methods", ["home.repos.pwc.inspect_result.hhexiy_debiased.model.esim.ESIMClassifier.__init__"], ["    ", "def", "__init__", "(", "self", ",", "task", ",", "runs_dir", ",", "run_id", "=", "None", ",", "feature", "=", "'hypothesis'", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "task", ",", "runs_dir", ",", "run_id", ")", "\n", "self", ".", "feature", "=", "feature", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.runner.HypothesisNLIRunner.build_model_transformer": [[443, 447], ["dataset.NLIHypothesisTransform"], "methods", ["None"], ["", "def", "build_model_transformer", "(", "self", ",", "max_len", ")", ":", "\n", "        ", "trans", "=", "NLIHypothesisTransform", "(", "\n", "self", ".", "tokenizer", ",", "self", ".", "labels", ",", "max_len", ",", "pad", "=", "False", ")", "\n", "return", "trans", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.runner.CBOWNLIRunner.__init__": [[449, 452], ["runner.NLIRunner.__init__", "tokenizer.BasicTokenizer"], "methods", ["home.repos.pwc.inspect_result.hhexiy_debiased.model.esim.ESIMClassifier.__init__"], ["    ", "def", "__init__", "(", "self", ",", "task", ",", "runs_dir", ",", "run_id", "=", "None", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "task", ",", "runs_dir", ",", "run_id", ")", "\n", "self", ".", "tokenizer", "=", "BasicTokenizer", "(", "do_lower_case", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.runner.CBOWNLIRunner.build_model": [[453, 460], ["runner.CBOWNLIRunner.task.num_classes", "model.cbow.NLICBOWClassifier.cbow.NLICBOWClassifier", "runner.CBOWNLIRunner.build_vocab", "len"], "methods", ["home.repos.pwc.inspect_result.hhexiy_debiased.src.dataset.SNLIDataset.num_classes", "home.repos.pwc.inspect_result.hhexiy_debiased.src.runner.NLIRunner.build_vocab"], ["", "def", "build_model", "(", "self", ",", "args", ",", "model_args", ",", "ctx", ",", "dataset", "=", "None", ",", "vocab", "=", "None", ")", ":", "\n", "        ", "if", "vocab", "is", "None", ":", "\n", "            ", "vocab", "=", "self", ".", "build_vocab", "(", "dataset", ")", "\n", "", "num_classes", "=", "self", ".", "task", ".", "num_classes", "(", ")", "\n", "\n", "model", "=", "NLICBOWClassifier", "(", "len", "(", "vocab", ")", ",", "num_classes", ",", "model_args", ".", "embedding_size", ",", "model_args", ".", "hidden_size", ",", "model_args", ".", "num_layers", ",", "dropout", "=", "model_args", ".", "dropout", ")", "\n", "return", "model", ",", "vocab", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.runner.CBOWNLIRunner.build_model_transformer": [[461, 464], ["dataset.CBOWTransform"], "methods", ["None"], ["", "def", "build_model_transformer", "(", "self", ",", "max_len", ")", ":", "\n", "        ", "trans", "=", "CBOWTransform", "(", "self", ".", "labels", ",", "self", ".", "tokenizer", ",", "self", ".", "vocab", ",", "num_input_sentences", "=", "2", ")", "\n", "return", "trans", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.runner.CBOWNLIRunner.prepare_data": [[465, 473], ["label.as_in_context.as_in_context.as_in_context", "x.as_in_context", "x.astype().as_in_context", "x.astype"], "methods", ["None"], ["", "def", "prepare_data", "(", "self", ",", "data", ",", "ctx", ")", ":", "\n", "        ", "\"\"\"Batched data to model inputs.\n        \"\"\"", "\n", "id_", ",", "input_ids", ",", "valid_len", ",", "label", "=", "data", "\n", "inputs", "=", "(", "[", "x", ".", "as_in_context", "(", "ctx", ")", "for", "x", "in", "input_ids", "]", ",", "\n", "[", "x", ".", "astype", "(", "'float32'", ")", ".", "as_in_context", "(", "ctx", ")", "for", "x", "in", "valid_len", "]", ")", "\n", "label", "=", "label", ".", "as_in_context", "(", "ctx", ")", "\n", "return", "id_", ",", "inputs", ",", "label", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.runner.CBOWNLIRunner.initialize_model": [[474, 487], ["model.initialize", "gluonnlp.embedding.create", "runner.CBOWNLIRunner.vocab.set_embedding", "numpy.random.normal", "model.embedding.weight.set_data", "mxnet.init.Normal"], "methods", ["home.repos.pwc.inspect_result.hhexiy_debiased.model.additive.AdditiveClassifier.initialize"], ["", "def", "initialize_model", "(", "self", ",", "args", ",", "model", ",", "ctx", ")", ":", "\n", "        ", "model", ".", "initialize", "(", "init", "=", "mx", ".", "init", ".", "Normal", "(", "0.01", ")", ",", "ctx", "=", "ctx", ",", "force_reinit", "=", "False", ")", "\n", "# Initialize word embeddings", "\n", "if", "args", ".", "embedding_source", ":", "\n", "            ", "glove", "=", "nlp", ".", "embedding", ".", "create", "(", "'glove'", ",", "source", "=", "args", ".", "embedding_source", ")", "\n", "self", ".", "vocab", ".", "set_embedding", "(", "glove", ")", "\n", "unk_idx", "=", "self", ".", "vocab", "[", "self", ".", "vocab", ".", "unknown_token", "]", "\n", "pad_idx", "=", "self", ".", "vocab", "[", "self", ".", "vocab", ".", "padding_token", "]", "\n", "self", ".", "vocab", ".", "embedding", ".", "idx_to_vec", "[", "unk_idx", "]", "=", "np", ".", "random", ".", "normal", "(", "size", "=", "args", ".", "embedding_size", ")", "\n", "self", ".", "vocab", ".", "embedding", ".", "idx_to_vec", "[", "pad_idx", "]", "=", "0.", "\n", "model", ".", "embedding", ".", "weight", ".", "set_data", "(", "self", ".", "vocab", ".", "embedding", ".", "idx_to_vec", ")", "\n", "if", "args", ".", "fix_word_embedding", ":", "\n", "                ", "model", ".", "embedding", ".", "weight", ".", "req_grad", "=", "'null'", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.runner.HandcraftedNLIRunner.build_model": [[490, 498], ["runner.HandcraftedNLIRunner.task.num_classes", "model.cbow.NLIHandcraftedClassifier.cbow.NLIHandcraftedClassifier", "runner.HandcraftedNLIRunner.build_vocab", "len"], "methods", ["home.repos.pwc.inspect_result.hhexiy_debiased.src.dataset.SNLIDataset.num_classes", "home.repos.pwc.inspect_result.hhexiy_debiased.src.runner.NLIRunner.build_vocab"], ["    ", "def", "build_model", "(", "self", ",", "args", ",", "model_args", ",", "ctx", ",", "dataset", "=", "None", ",", "vocab", "=", "None", ")", ":", "\n", "# empty overlap / non-overlap tokens", "\n", "        ", "reserved_tokens", "=", "[", "'<empty>'", "]", "\n", "if", "vocab", "is", "None", ":", "\n", "            ", "vocab", "=", "self", ".", "build_vocab", "(", "dataset", ",", "reserved_tokens", "=", "reserved_tokens", ")", "\n", "", "num_classes", "=", "self", ".", "task", ".", "num_classes", "(", ")", "\n", "model", "=", "NLIHandcraftedClassifier", "(", "len", "(", "vocab", ")", ",", "num_classes", ",", "model_args", ".", "embedding_size", ",", "model_args", ".", "hidden_size", ",", "model_args", ".", "num_layers", ",", "dropout", "=", "model_args", ".", "dropout", ")", "\n", "return", "model", ",", "vocab", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.runner.HandcraftedNLIRunner.build_model_transformer": [[499, 502], ["dataset.NLIHandcraftedTransform"], "methods", ["None"], ["", "def", "build_model_transformer", "(", "self", ",", "max_len", ")", ":", "\n", "        ", "trans", "=", "NLIHandcraftedTransform", "(", "self", ".", "labels", ",", "self", ".", "tokenizer", ",", "self", ".", "vocab", ")", "\n", "return", "trans", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.runner.HandcraftedNLIRunner.prepare_data": [[503, 513], ["label.as_in_context.as_in_context.as_in_context", "dense_features.astype().as_in_context", "overlap_token_ids.as_in_context", "non_overlap_token_ids.as_in_context", "dense_features.astype"], "methods", ["None"], ["", "def", "prepare_data", "(", "self", ",", "data", ",", "ctx", ")", ":", "\n", "        ", "\"\"\"Batched data to model inputs.\n        \"\"\"", "\n", "id_", ",", "dense_features", ",", "overlap_token_ids", ",", "non_overlap_token_ids", ",", "label", "=", "data", "\n", "inputs", "=", "(", "dense_features", ".", "astype", "(", "'float32'", ")", ".", "as_in_context", "(", "ctx", ")", ",", "\n", "overlap_token_ids", ".", "as_in_context", "(", "ctx", ")", ",", "\n", "non_overlap_token_ids", ".", "as_in_context", "(", "ctx", ")", ",", "\n", ")", "\n", "label", "=", "label", ".", "as_in_context", "(", "ctx", ")", "\n", "return", "id_", ",", "inputs", ",", "label", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.runner.DANLIRunner.build_model": [[516, 524], ["runner.DANLIRunner.task.num_classes", "model.decomposable_attention.DecomposableAttentionClassifier.decomposable_attention.DecomposableAttentionClassifier", "runner.DANLIRunner.build_vocab", "len"], "methods", ["home.repos.pwc.inspect_result.hhexiy_debiased.src.dataset.SNLIDataset.num_classes", "home.repos.pwc.inspect_result.hhexiy_debiased.src.runner.NLIRunner.build_vocab"], ["    ", "def", "build_model", "(", "self", ",", "args", ",", "model_args", ",", "ctx", ",", "dataset", "=", "None", ",", "vocab", "=", "None", ")", ":", "\n", "        ", "reserved_tokens", "=", "[", "'NULL'", "]", "\n", "if", "vocab", "is", "None", ":", "\n", "            ", "vocab", "=", "self", ".", "build_vocab", "(", "dataset", ",", "reserved_tokens", "=", "reserved_tokens", ")", "\n", "", "num_classes", "=", "self", ".", "task", ".", "num_classes", "(", ")", "\n", "\n", "model", "=", "DecomposableAttentionClassifier", "(", "len", "(", "vocab", ")", ",", "num_classes", ",", "model_args", ".", "embedding_size", ",", "model_args", ".", "hidden_size", ",", "dropout", "=", "model_args", ".", "dropout", ")", "\n", "return", "model", ",", "vocab", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.runner.DANLIRunner.get_optimizer_params": [[525, 534], ["None"], "methods", ["None"], ["", "def", "get_optimizer_params", "(", "self", ",", "optimizer", ",", "lr", ")", ":", "\n", "        ", "if", "optimizer", "==", "'bertadam'", ":", "\n", "            ", "return", "{", "'learning_rate'", ":", "lr", ",", "'epsilon'", ":", "1e-6", ",", "'wd'", ":", "0.01", "}", "\n", "", "elif", "optimizer", "==", "'adagrad'", ":", "\n", "            ", "return", "{", "'learning_rate'", ":", "lr", ",", "'wd'", ":", "1e-5", ",", "'clip_gradient'", ":", "5", "}", "\n", "", "elif", "optimizer", "==", "'adam'", ":", "\n", "            ", "return", "{", "'learning_rate'", ":", "lr", "}", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.runner.DANLIRunner.build_model_transformer": [[535, 538], ["dataset.DATransform"], "methods", ["None"], ["", "", "def", "build_model_transformer", "(", "self", ",", "max_len", ")", ":", "\n", "        ", "trans", "=", "DATransform", "(", "self", ".", "labels", ",", "self", ".", "tokenizer", ",", "self", ".", "vocab", ")", "\n", "return", "trans", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.runner.DANLIRunner.prepare_data": [[539, 550], ["label.as_in_context.as_in_context.as_in_context", "input_ids[].as_in_context", "input_ids[].as_in_context", "valid_len[].astype().as_in_context", "valid_len[].astype().as_in_context", "valid_len[].astype", "valid_len[].astype"], "methods", ["None"], ["", "def", "prepare_data", "(", "self", ",", "data", ",", "ctx", ")", ":", "\n", "        ", "\"\"\"Batched data to model inputs.\n        \"\"\"", "\n", "id_", ",", "input_ids", ",", "valid_len", ",", "label", "=", "data", "\n", "inputs", "=", "(", "input_ids", "[", "0", "]", ".", "as_in_context", "(", "ctx", ")", ",", "\n", "input_ids", "[", "1", "]", ".", "as_in_context", "(", "ctx", ")", ",", "\n", "valid_len", "[", "0", "]", ".", "astype", "(", "'float32'", ")", ".", "as_in_context", "(", "ctx", ")", ",", "\n", "valid_len", "[", "1", "]", ".", "astype", "(", "'float32'", ")", ".", "as_in_context", "(", "ctx", ")", ",", "\n", ")", "\n", "label", "=", "label", ".", "as_in_context", "(", "ctx", ")", "\n", "return", "id_", ",", "inputs", ",", "label", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.runner.ESIMNLIRunner.build_model": [[553, 561], ["runner.ESIMNLIRunner.task.num_classes", "model.esim.ESIMClassifier.esim.ESIMClassifier", "runner.ESIMNLIRunner.build_vocab", "len"], "methods", ["home.repos.pwc.inspect_result.hhexiy_debiased.src.dataset.SNLIDataset.num_classes", "home.repos.pwc.inspect_result.hhexiy_debiased.src.runner.NLIRunner.build_vocab"], ["    ", "def", "build_model", "(", "self", ",", "args", ",", "model_args", ",", "ctx", ",", "dataset", "=", "None", ",", "vocab", "=", "None", ")", ":", "\n", "        ", "reserved_tokens", "=", "None", "\n", "if", "vocab", "is", "None", ":", "\n", "            ", "vocab", "=", "self", ".", "build_vocab", "(", "dataset", ",", "reserved_tokens", "=", "reserved_tokens", ")", "\n", "", "num_classes", "=", "self", ".", "task", ".", "num_classes", "(", ")", "\n", "\n", "model", "=", "ESIMClassifier", "(", "len", "(", "vocab", ")", ",", "num_classes", ",", "model_args", ".", "embedding_size", ",", "model_args", ".", "hidden_size", ",", "model_args", ".", "hidden_size", ",", "dropout", "=", "model_args", ".", "dropout", ")", "\n", "return", "model", ",", "vocab", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.runner.ESIMNLIRunner.build_model_transformer": [[562, 565], ["dataset.ESIMTransform"], "methods", ["None"], ["", "def", "build_model_transformer", "(", "self", ",", "max_len", ")", ":", "\n", "        ", "trans", "=", "ESIMTransform", "(", "self", ".", "labels", ",", "self", ".", "tokenizer", ",", "self", ".", "vocab", ",", "max_len", ")", "\n", "return", "trans", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.runner.ESIMNLIRunner.prepare_data": [[566, 577], ["label.as_in_context.as_in_context.as_in_context", "input_ids[].as_in_context", "input_ids[].as_in_context"], "methods", ["None"], ["", "def", "prepare_data", "(", "self", ",", "data", ",", "ctx", ")", ":", "\n", "        ", "\"\"\"Batched data to model inputs.\n        \"\"\"", "\n", "id_", ",", "input_ids", ",", "valid_len", ",", "label", "=", "data", "\n", "inputs", "=", "(", "input_ids", "[", "0", "]", ".", "as_in_context", "(", "ctx", ")", ",", "\n", "input_ids", "[", "1", "]", ".", "as_in_context", "(", "ctx", ")", ",", "\n", "#valid_len[0].astype('float32').as_in_context(ctx),", "\n", "#valid_len[1].astype('float32').as_in_context(ctx),", "\n", ")", "\n", "label", "=", "label", ".", "as_in_context", "(", "ctx", ")", "\n", "return", "id_", ",", "inputs", ",", "label", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.runner.get_additive_runner": [[579, 707], ["super().__init__", "super().build_model", "model.additive.AdditiveClassifier", "logger.info", "runner.run_test", "mxnet.nd.stack", "prev_scores.astype().as_in_context.asnumpy", "super().preprocess_dataset", "zip", "mxnet.gluon.data.ArrayDataset", "runner..build_data_transformer", "mxnet.gluon.data.SimpleDataset", "logger.info", "time.time", "time.time", "logger.info", "dataset.transform.transform", "trans.get_batcher", "gluonnlp.data.batchify.Tuple", "mxnet.gluon.data.ArrayDataset", "prev_scores.astype().as_in_context.astype().as_in_context", "super().prepare_data", "utils.metric_to_dict", "super().preprocess_dataset", "super().build_model", "super().evaluate", "len", "len", "id_.asscalar", "reordered_prev_scores.append", "runner..run_prev_model", "dataset.transform.transform", "gluonnlp.data.batchify.Stack", "logger.info", "metric.reset", "super().evaluate", "_metric_dict.items", "enumerate", "logger.info", "mxnet.gluon.data.ArrayDataset", "ValueError", "enumerate", "len", "prev_scores.astype().as_in_context.astype", "utils.metric_to_dict.update", "time.time", "time.time", "numpy.argmax", "_dataset.append", "len"], "function", ["home.repos.pwc.inspect_result.hhexiy_debiased.model.esim.ESIMClassifier.__init__", "home.repos.pwc.inspect_result.hhexiy_debiased.src.runner.ESIMNLIRunner.build_model", "home.repos.pwc.inspect_result.hhexiy_debiased.src.runner.NLIRunner.run_test", "home.repos.pwc.inspect_result.hhexiy_debiased.src.runner.NLIRunner.preprocess_dataset", "home.repos.pwc.inspect_result.hhexiy_debiased.src.runner.NLIRunner.build_data_transformer", "home.repos.pwc.inspect_result.hhexiy_debiased.src.dataset.NLIHypothesisTransform.get_batcher", "home.repos.pwc.inspect_result.hhexiy_debiased.src.runner.ESIMNLIRunner.prepare_data", "home.repos.pwc.inspect_result.hhexiy_debiased.src.utils.metric_to_dict", "home.repos.pwc.inspect_result.hhexiy_debiased.src.runner.NLIRunner.preprocess_dataset", "home.repos.pwc.inspect_result.hhexiy_debiased.src.runner.ESIMNLIRunner.build_model", "home.repos.pwc.inspect_result.hhexiy_debiased.src.runner.NLIRunner.evaluate", "home.repos.pwc.inspect_result.hhexiy_debiased.src.dataset.SNLICheatTransform.reset", "home.repos.pwc.inspect_result.hhexiy_debiased.src.runner.NLIRunner.evaluate", "home.repos.pwc.inspect_result.hhexiy_debiased.src.dataset.MappedAccuracy.update"], ["", "", "def", "get_additive_runner", "(", "base", ",", "remove", "=", "False", ")", ":", "\n", "    ", "class", "AdditiveNLIRunner", "(", "base", ")", ":", "\n", "        ", "\"\"\"Additive model of a superficial classifier and a normal classifier.\n        \"\"\"", "\n", "def", "__init__", "(", "self", ",", "task", ",", "runs_dir", ",", "prev_runners", ",", "prev_args", ",", "run_id", "=", "None", ")", ":", "\n", "            ", "super", "(", ")", ".", "__init__", "(", "task", ",", "runs_dir", ",", "run_id", ")", "\n", "# Runner for the previous model", "\n", "self", ".", "prev_runners", "=", "prev_runners", "\n", "self", ".", "prev_args", "=", "prev_args", "\n", "\n", "", "def", "build_model", "(", "self", ",", "args", ",", "model_args", ",", "ctx", ",", "dataset", "=", "None", ",", "vocab", "=", "None", ")", ":", "\n", "            ", "model", ",", "vocabulary", "=", "super", "(", ")", ".", "build_model", "(", "args", ",", "model_args", ",", "ctx", ",", "dataset", "=", "dataset", ",", "vocab", "=", "vocab", ")", "\n", "model", "=", "AdditiveClassifier", "(", "model", ",", "mode", "=", "args", ".", "additive_mode", ")", "\n", "return", "model", ",", "vocabulary", "\n", "\n", "", "def", "run_prev_model", "(", "self", ",", "dataset", ",", "runner", ",", "args", ",", "ctx", ")", ":", "\n", "            ", "logger", ".", "info", "(", "'running previous model on preprocessed dataset'", ")", "\n", "_", ",", "prev_scores", ",", "ids", "=", "runner", ".", "run_test", "(", "args", ",", "ctx", ",", "dataset", ")", "\n", "assert", "len", "(", "dataset", ")", "==", "len", "(", "prev_scores", ")", "\n", "\n", "# Reorder scores by example id", "\n", "prev_scores", "=", "{", "id_", ".", "asscalar", "(", ")", ":", "prev_scores", "[", "i", "]", "for", "i", ",", "id_", "in", "enumerate", "(", "ids", ")", "}", "\n", "reordered_prev_scores", "=", "[", "]", "\n", "for", "data", "in", "dataset", ":", "\n", "                ", "id_", "=", "data", "[", "0", "]", "\n", "reordered_prev_scores", ".", "append", "(", "prev_scores", "[", "id_", "]", ")", "\n", "", "prev_scores", "=", "mx", ".", "nd", ".", "stack", "(", "*", "reordered_prev_scores", ",", "axis", "=", "0", ")", "\n", "prev_scores", "=", "prev_scores", ".", "asnumpy", "(", ")", "\n", "\n", "return", "prev_scores", "\n", "\n", "", "def", "preprocess_dataset", "(", "self", ",", "split", ",", "cheat_rate", ",", "remove_cheat", ",", "remove_correct", ",", "max_num_examples", ",", "ctx", "=", "None", ")", ":", "\n", "            ", "\"\"\"Add scores from previous classifiers.\n            \"\"\"", "\n", "dataset", "=", "super", "(", ")", ".", "preprocess_dataset", "(", "split", ",", "cheat_rate", ",", "remove_cheat", ",", "remove_correct", ",", "max_num_examples", ")", "\n", "\n", "prev_scores", "=", "0.", "\n", "for", "_prev_runner", ",", "_prev_args", "in", "zip", "(", "self", ".", "prev_runners", ",", "self", ".", "prev_args", ")", ":", "\n", "                ", "_prev_scores", "=", "self", ".", "run_prev_model", "(", "dataset", ",", "_prev_runner", ",", "_prev_args", ",", "ctx", ")", "\n", "prev_scores", "+=", "_prev_scores", "\n", "\n", "", "return", "gluon", ".", "data", ".", "ArrayDataset", "(", "prev_scores", ",", "dataset", ")", "\n", "\n", "", "def", "get_input", "(", "self", ",", "example", ")", ":", "\n", "            ", "\"\"\"Convert an example in the preprocessed dataset to a list of values.\n            \"\"\"", "\n", "scores", ",", "example", "=", "example", "\n", "# id_, premise, hypothesis, label", "\n", "return", "example", "\n", "\n", "", "def", "build_dataset", "(", "self", ",", "data", ",", "max_len", ",", "word_dropout", "=", "0", ",", "word_dropout_region", "=", "None", ",", "ctx", "=", "None", ")", ":", "\n", "            ", "trans_list", "=", "self", ".", "build_data_transformer", "(", "max_len", ",", "word_dropout", ",", "word_dropout_region", ")", "\n", "prev_scores", "=", "[", "x", "[", "0", "]", "for", "x", "in", "data", "]", "\n", "dataset", "=", "gluon", ".", "data", ".", "SimpleDataset", "(", "[", "x", "[", "1", "]", "for", "x", "in", "data", "]", ")", "\n", "logger", ".", "info", "(", "'processing {} examples'", ".", "format", "(", "len", "(", "dataset", ")", ")", ")", "\n", "start", "=", "time", ".", "time", "(", ")", "\n", "for", "trans", "in", "trans_list", ":", "\n", "                ", "dataset", "=", "dataset", ".", "transform", "(", "trans", ",", "lazy", "=", "False", ")", "\n", "", "logger", ".", "info", "(", "'elapsed time: {:.2f}s'", ".", "format", "(", "time", ".", "time", "(", ")", "-", "start", ")", ")", "\n", "# Last transform", "\n", "trans", "=", "trans_list", "[", "-", "1", "]", "\n", "data_lengths", "=", "dataset", ".", "transform", "(", "trans", ".", "get_length", ")", "\n", "batchify_fn", "=", "trans", ".", "get_batcher", "(", ")", "\n", "# Combine with prev_scores", "\n", "batchify_fn", "=", "nlp", ".", "data", ".", "batchify", ".", "Tuple", "(", "nlp", ".", "data", ".", "batchify", ".", "Stack", "(", ")", ",", "batchify_fn", ")", "\n", "dataset", "=", "gluon", ".", "data", ".", "ArrayDataset", "(", "prev_scores", ",", "dataset", ")", "\n", "return", "dataset", ",", "data_lengths", ",", "batchify_fn", "\n", "\n", "", "def", "prepare_data", "(", "self", ",", "data", ",", "ctx", ")", ":", "\n", "            ", "prev_scores", ",", "model_data", "=", "data", "\n", "prev_scores", "=", "prev_scores", ".", "astype", "(", "'float32'", ")", ".", "as_in_context", "(", "ctx", ")", "\n", "id_", ",", "inputs", ",", "label", "=", "super", "(", ")", ".", "prepare_data", "(", "model_data", ",", "ctx", ")", "\n", "return", "id_", ",", "[", "prev_scores", ",", "inputs", "]", ",", "label", "\n", "\n", "", "def", "evaluate", "(", "self", ",", "data_loader", ",", "model", ",", "metric", ",", "ctx", ")", ":", "\n", "            ", "original_mode", "=", "model", ".", "mode", "\n", "metric_dict", "=", "metric_to_dict", "(", "metric", ")", "\n", "results", "=", "{", "}", "\n", "for", "mode", "in", "(", "'all'", ",", "'prev'", ",", "'last'", ")", ":", "\n", "                ", "logger", ".", "info", "(", "'evaluating additive model with mode={}'", ".", "format", "(", "mode", ")", ")", "\n", "model", ".", "mode", "=", "mode", "\n", "metric", ".", "reset", "(", ")", "\n", "_metric_dict", ",", "preds", ",", "labels", ",", "scores", ",", "ids", "=", "super", "(", ")", ".", "evaluate", "(", "data_loader", ",", "model", ",", "metric", ",", "ctx", ")", "\n", "results", "[", "mode", "]", "=", "(", "_metric_dict", ",", "preds", ",", "labels", ",", "scores", ",", "ids", ")", "\n", "for", "k", ",", "v", "in", "_metric_dict", ".", "items", "(", ")", ":", "\n", "                    ", "metric_dict", "[", "'{}_{}'", ".", "format", "(", "model", ".", "mode", ",", "k", ")", "]", "=", "v", "\n", "# The original_mode result will be used for model selection", "\n", "", "if", "mode", "==", "original_mode", ":", "\n", "                    ", "metric_dict", ".", "update", "(", "_metric_dict", ")", "\n", "", "", "model", ".", "mode", "=", "original_mode", "\n", "_", ",", "preds", ",", "labels", ",", "scores", ",", "ids", "=", "results", "[", "original_mode", "]", "\n", "return", "metric_dict", ",", "preds", ",", "labels", ",", "scores", ",", "ids", "\n", "\n", "", "", "class", "RemoveNLIRunner", "(", "AdditiveNLIRunner", ")", ":", "\n", "        ", "def", "preprocess_dataset", "(", "self", ",", "split", ",", "cheat_rate", ",", "remove_cheat", ",", "remove_correct", ",", "max_num_examples", ",", "ctx", "=", "None", ")", ":", "\n", "            ", "\"\"\"Remove examples that are classified correctly by previous models.\n            \"\"\"", "\n", "dataset", "=", "super", "(", ")", ".", "preprocess_dataset", "(", "split", ",", "cheat_rate", ",", "remove_cheat", ",", "remove_correct", ",", "max_num_examples", ",", "ctx", ")", "\n", "if", "remove_correct", ":", "\n", "                ", "_dataset", "=", "[", "]", "\n", "# TODO: move label_map to runner", "\n", "_label_map", "=", "{", "}", "\n", "for", "(", "i", ",", "label", ")", "in", "enumerate", "(", "self", ".", "labels", ")", ":", "\n", "                    ", "_label_map", "[", "label", "]", "=", "i", "\n", "", "for", "d", "in", "dataset", ":", "\n", "                    ", "score", "=", "d", "[", "0", "]", "\n", "# d[1] = id_, premise, hypothesis, label", "\n", "label", "=", "d", "[", "1", "]", "[", "-", "1", "]", "\n", "if", "np", ".", "argmax", "(", "score", ")", "!=", "_label_map", "[", "label", "]", ":", "\n", "                        ", "_dataset", ".", "append", "(", "d", ")", "\n", "", "", "logger", ".", "info", "(", "'after remove correct examples: {}'", ".", "format", "(", "len", "(", "_dataset", ")", ")", ")", "\n", "return", "gluon", ".", "data", ".", "ArrayDataset", "(", "[", "d", "[", "0", "]", "for", "d", "in", "_dataset", "]", ",", "[", "d", "[", "1", "]", "for", "d", "in", "_dataset", "]", ")", "\n", "", "else", ":", "\n", "                ", "return", "dataset", "\n", "\n", "", "", "def", "build_model", "(", "self", ",", "args", ",", "model_args", ",", "ctx", ",", "dataset", "=", "None", ",", "vocab", "=", "None", ")", ":", "\n", "# Just use the base model", "\n", "            ", "if", "args", ".", "additive_mode", "!=", "'last'", ":", "\n", "                ", "raise", "ValueError", "(", "'Remove method only uses the base model.'", ")", "\n", "", "return", "super", "(", ")", ".", "build_model", "(", "args", ",", "model_args", ",", "ctx", ",", "dataset", "=", "dataset", ",", "vocab", "=", "vocab", ")", "\n", "\n", "", "def", "evaluate", "(", "self", ",", "data_loader", ",", "model", ",", "metric", ",", "ctx", ")", ":", "\n", "# Don't need to eval all modes", "\n", "            ", "return", "super", "(", "AdditiveNLIRunner", ",", "self", ")", ".", "evaluate", "(", "data_loader", ",", "model", ",", "metric", ",", "ctx", ")", "\n", "\n", "", "", "if", "remove", ":", "\n", "        ", "return", "RemoveNLIRunner", "\n", "", "return", "AdditiveNLIRunner", "\n", "", ""]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.main.parse_args": [[56, 70], ["argparse.ArgumentParser", "options.add_default_arguments", "options.add_data_arguments", "options.add_logging_arguments", "options.add_model_arguments", "options.add_training_arguments", "argparse.ArgumentParser.parse_args", "eval", "options.check_arguments"], "function", ["home.repos.pwc.inspect_result.hhexiy_debiased.src.options.add_default_arguments", "home.repos.pwc.inspect_result.hhexiy_debiased.src.options.add_data_arguments", "home.repos.pwc.inspect_result.hhexiy_debiased.src.options.add_logging_arguments", "home.repos.pwc.inspect_result.hhexiy_debiased.src.options.add_model_arguments", "home.repos.pwc.inspect_result.hhexiy_debiased.src.options.add_training_arguments", "home.repos.pwc.inspect_result.hhexiy_debiased.scripts.error_analysis.parse_args", "home.repos.pwc.inspect_result.hhexiy_debiased.src.options.check_arguments"], ["def", "parse_args", "(", ")", ":", "\n", "    ", "\"\"\"\n    Parse arguments.\n    \"\"\"", "\n", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "add_default_arguments", "(", "parser", ")", "\n", "add_data_arguments", "(", "parser", ")", "\n", "add_logging_arguments", "(", "parser", ")", "\n", "add_model_arguments", "(", "parser", ")", "\n", "add_training_arguments", "(", "parser", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "args", ".", "remove_cheat", "=", "eval", "(", "args", ".", "remove_cheat", ")", "\n", "check_arguments", "(", "args", ")", "\n", "return", "args", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.main.make_args_compatible": [[71, 80], ["hasattr", "hasattr"], "function", ["None"], ["", "def", "make_args_compatible", "(", "args", ")", ":", "\n", "# Be compatible with previously trained models when we added new options", "\n", "    ", "if", "args", ".", "superficial", "==", "True", ":", "\n", "        ", "args", ".", "superficial", "==", "'hypothesis'", "\n", "", "if", "not", "hasattr", "(", "args", ",", "'remove'", ")", ":", "\n", "        ", "args", ".", "remove", "=", "False", "\n", "", "if", "not", "hasattr", "(", "args", ",", "'remove_cheat'", ")", ":", "\n", "        ", "args", ".", "remove_cheat", "=", "'False'", "\n", "", "return", "args", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.main.get_runner": [[81, 110], ["main.make_args_compatible", "runner.HypothesisNLIRunner", "runner.HandcraftedNLIRunner", "enumerate", "utils.read_args", "main.get_runner", "prev_runners.append", "prev_args.append", "runner.get_additive_runner"], "function", ["home.repos.pwc.inspect_result.hhexiy_debiased.src.main.make_args_compatible", "home.repos.pwc.inspect_result.hhexiy_debiased.src.utils.read_args", "home.repos.pwc.inspect_result.hhexiy_debiased.src.main.get_runner", "home.repos.pwc.inspect_result.hhexiy_debiased.src.runner.get_additive_runner"], ["", "def", "get_runner", "(", "args", ",", "model_args", ",", "task", ",", "output_dir", "=", "None", ")", ":", "\n", "    ", "core_runner", "=", "{", "\n", "'cbow'", ":", "CBOWNLIRunner", ",", "\n", "'bert'", ":", "BERTNLIRunner", ",", "\n", "'da'", ":", "DANLIRunner", ",", "\n", "'esim'", ":", "ESIMNLIRunner", ",", "\n", "}", "\n", "if", "not", "output_dir", ":", "\n", "        ", "output_dir", "=", "args", ".", "output_dir", "\n", "", "model_args", "=", "make_args_compatible", "(", "model_args", ")", "\n", "if", "model_args", ".", "superficial", "==", "'hypothesis'", ":", "\n", "        ", "runner", "=", "HypothesisNLIRunner", "(", "task", ",", "output_dir", ",", "args", ".", "exp_id", ")", "\n", "", "elif", "model_args", ".", "superficial", "==", "'handcrafted'", ":", "\n", "        ", "runner", "=", "HandcraftedNLIRunner", "(", "task", ",", "output_dir", ",", "args", ".", "exp_id", ")", "\n", "", "elif", "model_args", ".", "additive", ":", "\n", "        ", "prev_runners", "=", "[", "]", "\n", "prev_args", "=", "[", "]", "\n", "for", "i", ",", "path", "in", "enumerate", "(", "model_args", ".", "additive", ")", ":", "\n", "            ", "_prev_args", "=", "read_args", "(", "path", ")", "\n", "# Change to inference model", "\n", "_prev_args", ".", "init_from", "=", "path", "\n", "_prev_args", ".", "dropout", "=", "0.0", "\n", "_prev_runner", "=", "get_runner", "(", "args", ",", "_prev_args", ",", "task", ",", "output_dir", "=", "'/tmp'", ")", "\n", "prev_runners", ".", "append", "(", "_prev_runner", ")", "\n", "prev_args", ".", "append", "(", "_prev_args", ")", "\n", "", "runner", "=", "get_additive_runner", "(", "core_runner", "[", "model_args", ".", "model_type", "]", ",", "remove", "=", "model_args", ".", "remove", ")", "(", "task", ",", "output_dir", ",", "prev_runners", ",", "prev_args", ",", "args", ".", "exp_id", ")", "\n", "", "else", ":", "\n", "        ", "runner", "=", "core_runner", "[", "model_args", ".", "model_type", "]", "(", "task", ",", "output_dir", ",", "args", ".", "exp_id", ")", "\n", "", "return", "runner", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.main.main": [[111, 138], ["numpy.random.seed", "random.seed", "main.get_runner", "os.environ.get", "utils.read_args", "os.environ.get.lower", "logger.info", "get_runner.run", "get_runner.dump_report", "print", "logger.info", "get_runner.dump_report"], "function", ["home.repos.pwc.inspect_result.hhexiy_debiased.src.main.get_runner", "home.repos.pwc.inspect_result.hhexiy_debiased.src.utils.read_args", "home.repos.pwc.inspect_result.hhexiy_debiased.src.runner.NLIRunner.run", "home.repos.pwc.inspect_result.hhexiy_debiased.src.runner.Runner.dump_report", "home.repos.pwc.inspect_result.hhexiy_debiased.src.runner.Runner.dump_report"], ["", "def", "main", "(", "args", ")", ":", "\n", "    ", "np", ".", "random", ".", "seed", "(", "args", ".", "seed", ")", "\n", "random", ".", "seed", "(", "args", ".", "seed", ")", "\n", "\n", "if", "args", ".", "mode", "==", "'test'", ":", "\n", "        ", "model_args", "=", "read_args", "(", "args", ".", "init_from", ")", "\n", "", "else", ":", "\n", "        ", "model_args", "=", "args", "\n", "\n", "", "task", "=", "tasks", "[", "args", ".", "task_name", "]", "\n", "\n", "runner", "=", "get_runner", "(", "args", ",", "model_args", ",", "task", ")", "\n", "\n", "pool_type", "=", "os", ".", "environ", ".", "get", "(", "'MXNET_GPU_MEM_POOL_TYPE'", ",", "''", ")", "\n", "if", "pool_type", ".", "lower", "(", ")", "==", "'round'", ":", "\n", "        ", "logger", ".", "info", "(", "\n", "'Setting MXNET_GPU_MEM_POOL_TYPE=\"Round\" may lead to higher memory '", "\n", "'usage and faster speed. If you encounter OOM errors, please unset '", "\n", "'this environment variable.'", ")", "\n", "\n", "", "try", ":", "\n", "        ", "runner", ".", "run", "(", "args", ")", "\n", "runner", ".", "dump_report", "(", ")", "\n", "", "except", "KeyboardInterrupt", "as", "e", ":", "\n", "        ", "print", "(", "e", ")", "\n", "logger", ".", "info", "(", "'Terminated. Dumping report.'", ")", "\n", "runner", ".", "dump_report", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.tokenizer.SNLITokenizer.__init__": [[104, 106], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "do_lower_case", ")", ":", "\n", "        ", "self", ".", "do_lower_case", "=", "do_lower_case", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.tokenizer.SNLITokenizer.tokenize": [[107, 112], ["re.sub().split", "text.lower.lower.lower", "re.sub"], "methods", ["None"], ["", "def", "tokenize", "(", "self", ",", "text", ")", ":", "\n", "        ", "if", "self", ".", "do_lower_case", ":", "\n", "            ", "text", "=", "text", ".", "lower", "(", ")", "\n", "", "tokens", "=", "re", ".", "sub", "(", "r'\\(|\\)'", ",", "''", ",", "text", ")", ".", "split", "(", ")", "\n", "return", "tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.tokenizer.FullTokenizer.__init__": [[125, 129], ["tokenizer.BasicTokenizer", "tokenizer.WordpieceTokenizer"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "vocab", ",", "do_lower_case", "=", "True", ")", ":", "\n", "        ", "self", ".", "vocab", "=", "vocab", "\n", "self", ".", "basic_tokenizer", "=", "BasicTokenizer", "(", "do_lower_case", "=", "do_lower_case", ")", "\n", "self", ".", "wordpiece_tokenizer", "=", "WordpieceTokenizer", "(", "vocab", "=", "self", ".", "vocab", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.tokenizer.FullTokenizer.tokenize": [[130, 137], ["tokenizer.FullTokenizer.basic_tokenizer.tokenize", "tokenizer.FullTokenizer.wordpiece_tokenizer.tokenize", "split_tokens.append"], "methods", ["home.repos.pwc.inspect_result.hhexiy_debiased.src.tokenizer.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.hhexiy_debiased.src.tokenizer.WordpieceTokenizer.tokenize"], ["", "def", "tokenize", "(", "self", ",", "text", ")", ":", "\n", "        ", "split_tokens", "=", "[", "]", "\n", "for", "token", "in", "self", ".", "basic_tokenizer", ".", "tokenize", "(", "text", ")", ":", "\n", "            ", "for", "sub_token", "in", "self", ".", "wordpiece_tokenizer", ".", "tokenize", "(", "token", ")", ":", "\n", "                ", "split_tokens", ".", "append", "(", "sub_token", ")", "\n", "\n", "", "", "return", "split_tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.tokenizer.FullTokenizer.convert_tokens_to_ids": [[138, 140], ["tokenizer.FullTokenizer.convert_tokens_to_ids"], "methods", ["home.repos.pwc.inspect_result.hhexiy_debiased.src.tokenizer.convert_tokens_to_ids"], ["", "def", "convert_tokens_to_ids", "(", "self", ",", "tokens", ")", ":", "\n", "        ", "return", "convert_tokens_to_ids", "(", "self", ".", "vocab", ",", "tokens", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.tokenizer.BasicTokenizer.__init__": [[145, 152], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "do_lower_case", "=", "True", ")", ":", "\n", "        ", "\"\"\"Constructs a BasicTokenizer.\n\n        Args:\n          do_lower_case: Whether to lower case the input.\n        \"\"\"", "\n", "self", ".", "do_lower_case", "=", "do_lower_case", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.tokenizer.BasicTokenizer.tokenize": [[153, 175], ["tokenizer.convert_to_unicode", "tokenizer.BasicTokenizer._clean_text", "tokenizer.BasicTokenizer._tokenize_chinese_chars", "tokenizer.whitespace_tokenize", "tokenizer.whitespace_tokenize", "split_tokens.extend", "tokenizer.BasicTokenizer.lower", "tokenizer.BasicTokenizer._run_strip_accents", "tokenizer.BasicTokenizer._run_split_on_punc"], "methods", ["home.repos.pwc.inspect_result.hhexiy_debiased.src.tokenizer.convert_to_unicode", "home.repos.pwc.inspect_result.hhexiy_debiased.src.tokenizer.BasicTokenizer._clean_text", "home.repos.pwc.inspect_result.hhexiy_debiased.src.tokenizer.BasicTokenizer._tokenize_chinese_chars", "home.repos.pwc.inspect_result.hhexiy_debiased.src.tokenizer.whitespace_tokenize", "home.repos.pwc.inspect_result.hhexiy_debiased.src.tokenizer.whitespace_tokenize", "home.repos.pwc.inspect_result.hhexiy_debiased.src.tokenizer.BasicTokenizer._run_strip_accents", "home.repos.pwc.inspect_result.hhexiy_debiased.src.tokenizer.BasicTokenizer._run_split_on_punc"], ["", "def", "tokenize", "(", "self", ",", "text", ")", ":", "\n", "        ", "\"\"\"Tokenizes a piece of text.\"\"\"", "\n", "text", "=", "convert_to_unicode", "(", "text", ")", "\n", "text", "=", "self", ".", "_clean_text", "(", "text", ")", "\n", "\n", "# This was added on November 1st, 2018 for the multilingual and Chinese", "\n", "# models. This is also applied to the English models now, but it doesn't", "\n", "# matter since the English models were not trained on any Chinese data", "\n", "# and generally don't have any Chinese data in them (there are Chinese", "\n", "# characters in the vocabulary because Wikipedia does have some Chinese", "\n", "# words in the English Wikipedia.).", "\n", "text", "=", "self", ".", "_tokenize_chinese_chars", "(", "text", ")", "\n", "orig_tokens", "=", "whitespace_tokenize", "(", "text", ")", "\n", "split_tokens", "=", "[", "]", "\n", "for", "token", "in", "orig_tokens", ":", "\n", "            ", "if", "self", ".", "do_lower_case", ":", "\n", "                ", "token", "=", "token", ".", "lower", "(", ")", "\n", "token", "=", "self", ".", "_run_strip_accents", "(", "token", ")", "\n", "", "split_tokens", ".", "extend", "(", "self", ".", "_run_split_on_punc", "(", "token", ")", ")", "\n", "\n", "", "output_tokens", "=", "whitespace_tokenize", "(", "' '", ".", "join", "(", "split_tokens", ")", ")", "\n", "return", "output_tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.tokenizer.BasicTokenizer._run_strip_accents": [[176, 186], ["unicodedata.normalize", "unicodedata.category", "output.append"], "methods", ["None"], ["", "def", "_run_strip_accents", "(", "self", ",", "text", ")", ":", "\n", "        ", "\"\"\"Strips accents from a piece of text.\"\"\"", "\n", "text", "=", "unicodedata", ".", "normalize", "(", "'NFD'", ",", "text", ")", "\n", "output", "=", "[", "]", "\n", "for", "char", "in", "text", ":", "\n", "            ", "cat", "=", "unicodedata", ".", "category", "(", "char", ")", "\n", "if", "cat", "==", "'Mn'", ":", "\n", "                ", "continue", "\n", "", "output", ".", "append", "(", "char", ")", "\n", "", "return", "''", ".", "join", "(", "output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.tokenizer.BasicTokenizer._run_split_on_punc": [[187, 206], ["list", "len", "tokenizer._is_punctuation", "output.append", "output[].append", "output.append"], "methods", ["home.repos.pwc.inspect_result.hhexiy_debiased.src.tokenizer._is_punctuation"], ["", "def", "_run_split_on_punc", "(", "self", ",", "text", ")", ":", "\n", "        ", "\"\"\"Splits punctuation on a piece of text.\"\"\"", "\n", "chars", "=", "list", "(", "text", ")", "\n", "i", "=", "0", "\n", "start_new_word", "=", "True", "\n", "output", "=", "[", "]", "\n", "while", "i", "<", "len", "(", "chars", ")", ":", "\n", "            ", "char", "=", "chars", "[", "i", "]", "\n", "if", "_is_punctuation", "(", "char", ")", ":", "\n", "                ", "output", ".", "append", "(", "[", "char", "]", ")", "\n", "start_new_word", "=", "True", "\n", "", "else", ":", "\n", "                ", "if", "start_new_word", ":", "\n", "                    ", "output", ".", "append", "(", "[", "]", ")", "\n", "", "start_new_word", "=", "False", "\n", "output", "[", "-", "1", "]", ".", "append", "(", "char", ")", "\n", "", "i", "+=", "1", "\n", "\n", "", "return", "[", "''", ".", "join", "(", "x", ")", "for", "x", "in", "output", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.tokenizer.BasicTokenizer._tokenize_chinese_chars": [[207, 219], ["ord", "tokenizer.BasicTokenizer._is_chinese_char", "output.append", "output.append", "output.append", "output.append"], "methods", ["home.repos.pwc.inspect_result.hhexiy_debiased.src.tokenizer.BasicTokenizer._is_chinese_char"], ["", "def", "_tokenize_chinese_chars", "(", "self", ",", "text", ")", ":", "\n", "        ", "\"\"\"Adds whitespace around any CJK character.\"\"\"", "\n", "output", "=", "[", "]", "\n", "for", "char", "in", "text", ":", "\n", "            ", "cp", "=", "ord", "(", "char", ")", "\n", "if", "self", ".", "_is_chinese_char", "(", "cp", ")", ":", "\n", "                ", "output", ".", "append", "(", "' '", ")", "\n", "output", ".", "append", "(", "char", ")", "\n", "output", ".", "append", "(", "' '", ")", "\n", "", "else", ":", "\n", "                ", "output", ".", "append", "(", "char", ")", "\n", "", "", "return", "''", ".", "join", "(", "output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.tokenizer.BasicTokenizer._is_chinese_char": [[220, 241], ["None"], "methods", ["None"], ["", "def", "_is_chinese_char", "(", "self", ",", "cp", ")", ":", "\n", "        ", "\"\"\"Checks whether CP is the codepoint of a CJK character.\"\"\"", "\n", "# This defines a \"chinese character\" as anything in the CJK Unicode block:", "\n", "#   https://en.wikipedia.org/wiki/CJK_Unified_Ideographs_(Unicode_block)", "\n", "#", "\n", "# Note that the CJK Unicode block is NOT all Japanese and Korean characters,", "\n", "# despite its name. The modern Korean Hangul alphabet is a different block,", "\n", "# as is Japanese Hiragana and Katakana. Those alphabets are used to write", "\n", "# space-separated words, so they are not treated specially and handled", "\n", "# like the all of the other languages.", "\n", "if", "(", "(", "cp", ">=", "0x4E00", "and", "cp", "<=", "0x9FFF", ")", "\n", "or", "(", "cp", ">=", "0x3400", "and", "cp", "<=", "0x4DBF", ")", "\n", "or", "(", "cp", ">=", "0x20000", "and", "cp", "<=", "0x2A6DF", ")", "\n", "or", "(", "cp", ">=", "0x2A700", "and", "cp", "<=", "0x2B73F", ")", "\n", "or", "(", "cp", ">=", "0x2B740", "and", "cp", "<=", "0x2B81F", ")", "\n", "or", "(", "cp", ">=", "0x2B820", "and", "cp", "<=", "0x2CEAF", ")", "\n", "or", "(", "cp", ">=", "0xF900", "and", "cp", "<=", "0xFAFF", ")", "\n", "or", "(", "cp", ">=", "0x2F800", "and", "cp", "<=", "0x2FA1F", ")", ")", ":", "\n", "            ", "return", "True", "\n", "\n", "", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.tokenizer.BasicTokenizer._clean_text": [[242, 254], ["ord", "tokenizer._is_whitespace", "tokenizer._is_control", "output.append", "output.append"], "methods", ["home.repos.pwc.inspect_result.hhexiy_debiased.src.tokenizer._is_whitespace", "home.repos.pwc.inspect_result.hhexiy_debiased.src.tokenizer._is_control"], ["", "def", "_clean_text", "(", "self", ",", "text", ")", ":", "\n", "        ", "\"\"\"Performs invalid character removal and whitespace cleanup on text.\"\"\"", "\n", "output", "=", "[", "]", "\n", "for", "char", "in", "text", ":", "\n", "            ", "cp", "=", "ord", "(", "char", ")", "\n", "if", "cp", "==", "0", "or", "cp", "==", "0xfffd", "or", "_is_control", "(", "char", ")", ":", "\n", "                ", "continue", "\n", "", "if", "_is_whitespace", "(", "char", ")", ":", "\n", "                ", "output", ".", "append", "(", "' '", ")", "\n", "", "else", ":", "\n", "                ", "output", ".", "append", "(", "char", ")", "\n", "", "", "return", "''", ".", "join", "(", "output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.tokenizer.WordpieceTokenizer.__init__": [[259, 263], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "vocab", ",", "unk_token", "=", "'[UNK]'", ",", "max_input_chars_per_word", "=", "100", ")", ":", "\n", "        ", "self", ".", "vocab", "=", "vocab", "\n", "self", ".", "unk_token", "=", "unk_token", "\n", "self", ".", "max_input_chars_per_word", "=", "max_input_chars_per_word", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.tokenizer.WordpieceTokenizer.tokenize": [[264, 316], ["tokenizer.convert_to_unicode", "tokenizer.whitespace_tokenize", "list", "len", "output_tokens.append", "len", "len", "sub_tokens.append", "output_tokens.append", "output_tokens.extend"], "methods", ["home.repos.pwc.inspect_result.hhexiy_debiased.src.tokenizer.convert_to_unicode", "home.repos.pwc.inspect_result.hhexiy_debiased.src.tokenizer.whitespace_tokenize"], ["", "def", "tokenize", "(", "self", ",", "text", ")", ":", "\n", "        ", "\"\"\"Tokenizes a piece of text into its word pieces.\n\n        This uses a greedy longest-match-first algorithm to perform tokenization\n        using the given vocabulary.\n\n        For example:\n          input = \"unaffable\"\n          output = [\"un\", \"##aff\", \"##able\"]\n\n        Args:\n          text: A single token or whitespace separated tokens. This should have\n            already been passed through `BasicTokenizer.\n\n        Returns:\n          A list of wordpiece tokens.\n        \"\"\"", "\n", "\n", "text", "=", "convert_to_unicode", "(", "text", ")", "\n", "\n", "output_tokens", "=", "[", "]", "\n", "for", "token", "in", "whitespace_tokenize", "(", "text", ")", ":", "\n", "            ", "chars", "=", "list", "(", "token", ")", "\n", "if", "len", "(", "chars", ")", ">", "self", ".", "max_input_chars_per_word", ":", "\n", "                ", "output_tokens", ".", "append", "(", "self", ".", "unk_token", ")", "\n", "continue", "\n", "\n", "", "is_bad", "=", "False", "\n", "start", "=", "0", "\n", "sub_tokens", "=", "[", "]", "\n", "while", "start", "<", "len", "(", "chars", ")", ":", "\n", "                ", "end", "=", "len", "(", "chars", ")", "\n", "cur_substr", "=", "None", "\n", "while", "start", "<", "end", ":", "\n", "                    ", "substr", "=", "''", ".", "join", "(", "chars", "[", "start", ":", "end", "]", ")", "\n", "if", "start", ">", "0", ":", "\n", "                        ", "substr", "=", "'##'", "+", "substr", "\n", "", "if", "substr", "in", "self", ".", "vocab", ":", "\n", "                        ", "cur_substr", "=", "substr", "\n", "break", "\n", "", "end", "-=", "1", "\n", "", "if", "cur_substr", "is", "None", ":", "\n", "                    ", "is_bad", "=", "True", "\n", "break", "\n", "", "sub_tokens", ".", "append", "(", "cur_substr", ")", "\n", "start", "=", "end", "\n", "\n", "", "if", "is_bad", ":", "\n", "                ", "output_tokens", ".", "append", "(", "self", ".", "unk_token", ")", "\n", "", "else", ":", "\n", "                ", "output_tokens", ".", "extend", "(", "sub_tokens", ")", "\n", "", "", "return", "output_tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.tokenizer.convert_to_unicode": [[27, 45], ["isinstance", "isinstance", "isinstance", "ValueError", "text.decode", "ValueError", "text.decode", "isinstance", "ValueError", "type", "type"], "function", ["None"], ["def", "convert_to_unicode", "(", "text", ")", ":", "\n", "    ", "\"\"\"Converts `text` to Unicode (if it's not already), assuming utf-8 input.\"\"\"", "\n", "if", "six", ".", "PY3", ":", "\n", "        ", "if", "isinstance", "(", "text", ",", "str", ")", ":", "\n", "            ", "return", "text", "\n", "", "elif", "isinstance", "(", "text", ",", "bytes", ")", ":", "\n", "            ", "return", "text", ".", "decode", "(", "'utf-8'", ",", "'ignore'", ")", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "'Unsupported string type: %s'", "%", "(", "type", "(", "text", ")", ")", ")", "\n", "", "", "elif", "six", ".", "PY2", ":", "\n", "        ", "if", "isinstance", "(", "text", ",", "str", ")", ":", "\n", "            ", "return", "text", ".", "decode", "(", "'utf-8'", ",", "'ignore'", ")", "\n", "", "elif", "isinstance", "(", "text", ",", "unicode", ")", ":", "# noqa: F821", "\n", "            ", "return", "text", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "'Unsupported string type: %s'", "%", "(", "type", "(", "text", ")", ")", ")", "\n", "", "", "else", ":", "\n", "        ", "raise", "ValueError", "(", "'Not running on Python2 or Python 3?'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.tokenizer.printable_text": [[47, 67], ["isinstance", "isinstance", "isinstance", "ValueError", "text.decode", "ValueError", "isinstance", "text.encode", "ValueError", "type", "type"], "function", ["None"], ["", "", "def", "printable_text", "(", "text", ")", ":", "\n", "    ", "\"\"\"Returns text encoded in a way suitable for print.\"\"\"", "\n", "# These functions want `str` for both Python2 and Python3, but in one case", "\n", "# it's a Unicode string and in the other it's a byte string.", "\n", "if", "six", ".", "PY3", ":", "\n", "        ", "if", "isinstance", "(", "text", ",", "str", ")", ":", "\n", "            ", "return", "text", "\n", "", "elif", "isinstance", "(", "text", ",", "bytes", ")", ":", "\n", "            ", "return", "text", ".", "decode", "(", "'utf-8'", ",", "'ignore'", ")", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "'Unsupported string type: %s'", "%", "(", "type", "(", "text", ")", ")", ")", "\n", "", "", "elif", "six", ".", "PY2", ":", "\n", "        ", "if", "isinstance", "(", "text", ",", "str", ")", ":", "\n", "            ", "return", "text", "\n", "", "elif", "isinstance", "(", "text", ",", "unicode", ")", ":", "# noqa: F821", "\n", "            ", "return", "text", ".", "encode", "(", "'utf-8'", ")", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "'Unsupported string type: %s'", "%", "(", "type", "(", "text", ")", ")", ")", "\n", "", "", "else", ":", "\n", "        ", "raise", "ValueError", "(", "'Not running on Python2 or Python 3?'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.tokenizer.load_vocab": [[69, 82], ["collections.OrderedDict", "io.open", "tokenizer.convert_to_unicode", "token.strip.strip", "reader.readline"], "function", ["home.repos.pwc.inspect_result.hhexiy_debiased.src.tokenizer.convert_to_unicode"], ["", "", "def", "load_vocab", "(", "vocab_file", ")", ":", "\n", "    ", "\"\"\"Loads a vocabulary file into a dictionary.\"\"\"", "\n", "vocab", "=", "collections", ".", "OrderedDict", "(", ")", "\n", "index", "=", "0", "\n", "with", "io", ".", "open", "(", "vocab_file", ",", "'r'", ")", "as", "reader", ":", "\n", "        ", "while", "True", ":", "\n", "            ", "token", "=", "convert_to_unicode", "(", "reader", ".", "readline", "(", ")", ")", "\n", "if", "not", "token", ":", "\n", "                ", "break", "\n", "", "token", "=", "token", ".", "strip", "(", ")", "\n", "vocab", "[", "token", "]", "=", "index", "\n", "index", "+=", "1", "\n", "", "", "return", "vocab", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.tokenizer.convert_tokens_to_ids": [[84, 90], ["ids.append"], "function", ["None"], ["", "def", "convert_tokens_to_ids", "(", "vocab", ",", "tokens", ")", ":", "\n", "    ", "\"\"\"Converts a sequence of tokens into ids using the vocab.\"\"\"", "\n", "ids", "=", "[", "]", "\n", "for", "token", "in", "tokens", ":", "\n", "        ", "ids", ".", "append", "(", "vocab", "[", "token", "]", ")", "\n", "", "return", "ids", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.tokenizer.whitespace_tokenize": [[92, 99], ["text.strip.strip", "text.strip.split"], "function", ["None"], ["", "def", "whitespace_tokenize", "(", "text", ")", ":", "\n", "    ", "\"\"\"Runs basic whitespace cleaning and splitting on a piece of text.\"\"\"", "\n", "text", "=", "text", ".", "strip", "(", ")", "\n", "if", "not", "text", ":", "\n", "        ", "return", "[", "]", "\n", "", "tokens", "=", "text", ".", "split", "(", ")", "\n", "return", "tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.tokenizer._is_whitespace": [[318, 328], ["unicodedata.category"], "function", ["None"], ["", "", "def", "_is_whitespace", "(", "char", ")", ":", "\n", "    ", "\"\"\"Checks whether `chars` is a whitespace character.\"\"\"", "\n", "# \\t, \\n, and \\r are technically control characters but we treat them", "\n", "# as whitespace since they are generally considered as such.", "\n", "if", "char", "in", "[", "' '", ",", "'\\t'", ",", "'\\n'", ",", "'\\r'", "]", ":", "\n", "        ", "return", "True", "\n", "", "cat", "=", "unicodedata", ".", "category", "(", "char", ")", "\n", "if", "cat", "==", "'Zs'", ":", "\n", "        ", "return", "True", "\n", "", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.tokenizer._is_control": [[330, 340], ["unicodedata.category", "unicodedata.category.startswith"], "function", ["None"], ["", "def", "_is_control", "(", "char", ")", ":", "\n", "    ", "\"\"\"Checks whether `chars` is a control character.\"\"\"", "\n", "# These are technically control characters but we count them as whitespace", "\n", "# characters.", "\n", "if", "char", "in", "[", "'\\t'", ",", "'\\n'", ",", "'\\r'", "]", ":", "\n", "        ", "return", "False", "\n", "", "cat", "=", "unicodedata", ".", "category", "(", "char", ")", "\n", "if", "cat", ".", "startswith", "(", "'C'", ")", ":", "\n", "        ", "return", "True", "\n", "", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.tokenizer._is_punctuation": [[342, 359], ["ord", "unicodedata.category", "unicodedata.category.startswith"], "function", ["None"], ["", "def", "_is_punctuation", "(", "char", ")", ":", "\n", "    ", "\"\"\"Checks whether `chars` is a punctuation character.\"\"\"", "\n", "cp", "=", "ord", "(", "char", ")", "\n", "# We treat all non-letter/number ASCII as punctuation.", "\n", "# Characters such as \"^\", \"$\", and \"`\" are not in the Unicode", "\n", "# Punctuation class but we treat them as punctuation anyways, for", "\n", "# consistency.", "\n", "group0", "=", "cp", ">=", "33", "and", "cp", "<=", "47", "\n", "group1", "=", "cp", ">=", "58", "and", "cp", "<=", "64", "\n", "group2", "=", "cp", ">=", "91", "and", "cp", "<=", "96", "\n", "group3", "=", "cp", ">=", "123", "and", "cp", "<=", "126", "\n", "if", "(", "group0", "or", "group1", "or", "group2", "or", "group3", ")", ":", "\n", "        ", "return", "True", "\n", "", "cat", "=", "unicodedata", ".", "category", "(", "char", ")", "\n", "if", "cat", ".", "startswith", "(", "'P'", ")", ":", "\n", "        ", "return", "True", "\n", "", "return", "False", "\n", "", ""]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.dataset.MappedAccuracy.__init__": [[43, 50], ["mxnet.metric.Accuracy.__init__"], "methods", ["home.repos.pwc.inspect_result.hhexiy_debiased.model.esim.ESIMClassifier.__init__"], ["    ", "def", "__init__", "(", "self", ",", "axis", "=", "1", ",", "name", "=", "'mapped-accuracy'", ",", "\n", "output_names", "=", "None", ",", "label_names", "=", "None", ",", "\n", "label_map", "=", "None", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "\n", "axis", "=", "axis", ",", "name", "=", "name", ",", "\n", "output_names", "=", "output_names", ",", "label_names", "=", "label_names", ")", "\n", "self", ".", "label_map", "=", "label_map", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.dataset.MappedAccuracy.update": [[51, 63], ["zip", "super().update", "mxnet.nd.argmax", "_labels.copy", "mxnet.nd.argmax.copy", "dataset.MappedAccuracy.label_map.items", "mapped_labels.append", "mapped_preds.append", "mxnet.nd.where", "mxnet.nd.where", "mxnet.nd.ones_like", "mxnet.nd.ones_like"], "methods", ["home.repos.pwc.inspect_result.hhexiy_debiased.src.dataset.MappedAccuracy.update"], ["", "def", "update", "(", "self", ",", "labels", ",", "preds", ")", ":", "\n", "        ", "mapped_labels", ",", "mapped_preds", "=", "[", "]", ",", "[", "]", "\n", "for", "_labels", ",", "_preds", "in", "zip", "(", "labels", ",", "preds", ")", ":", "\n", "            ", "_preds", "=", "mx", ".", "nd", ".", "argmax", "(", "_preds", ",", "axis", "=", "self", ".", "axis", ",", "keepdims", "=", "True", ")", "\n", "_mapped_labels", "=", "_labels", ".", "copy", "(", ")", "\n", "_mapped_preds", "=", "_preds", ".", "copy", "(", ")", "\n", "for", "k", ",", "v", "in", "self", ".", "label_map", ".", "items", "(", ")", ":", "\n", "                ", "_mapped_labels", "=", "mx", ".", "nd", ".", "where", "(", "_mapped_labels", "==", "k", ",", "mx", ".", "nd", ".", "ones_like", "(", "_labels", ")", "*", "v", ",", "_mapped_labels", ")", "\n", "_mapped_preds", "=", "mx", ".", "nd", ".", "where", "(", "_mapped_preds", "==", "k", ",", "mx", ".", "nd", ".", "ones_like", "(", "_preds", ")", "*", "v", ",", "_mapped_preds", ")", "\n", "", "mapped_labels", ".", "append", "(", "_mapped_labels", ")", "\n", "mapped_preds", ".", "append", "(", "_mapped_preds", ")", "\n", "", "super", "(", ")", ".", "update", "(", "mapped_labels", ",", "mapped_preds", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.dataset.MRPCDataset.__init__": [[78, 89], ["os.path.join", "os.path.join", "gluonnlp.data.TSVDataset.__init__", "os.getenv"], "methods", ["home.repos.pwc.inspect_result.hhexiy_debiased.model.esim.ESIMClassifier.__init__"], ["def", "__init__", "(", "self", ",", "\n", "segment", "=", "'train'", ",", "\n", "root", "=", "os", ".", "path", ".", "join", "(", "os", ".", "getenv", "(", "'GLUE_DIR'", ",", "'glue_data'", ")", ",", "\n", "'MRPC'", ")", ")", ":", "\n", "        ", "self", ".", "_supported_segments", "=", "[", "'train'", ",", "'dev'", ",", "'test'", "]", "\n", "assert", "segment", "in", "self", ".", "_supported_segments", ",", "'Unsupported segment: %s'", "%", "segment", "\n", "path", "=", "os", ".", "path", ".", "join", "(", "root", ",", "'%s.tsv'", "%", "segment", ")", "\n", "A_IDX", ",", "B_IDX", ",", "LABEL_IDX", "=", "3", ",", "4", ",", "0", "\n", "fields", "=", "[", "A_IDX", ",", "B_IDX", ",", "LABEL_IDX", "]", "\n", "super", "(", "MRPCDataset", ",", "self", ")", ".", "__init__", "(", "\n", "path", ",", "num_discard_samples", "=", "1", ",", "field_indices", "=", "fields", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.dataset.MRPCDataset.get_labels": [[90, 94], ["None"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "get_labels", "(", ")", ":", "\n", "        ", "\"\"\"Get classification label ids of the dataset.\"\"\"", "\n", "return", "[", "'0'", ",", "'1'", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.dataset.MRPCDataset.get_metric": [[95, 102], ["mxnet.metric.CompositeEvalMetric", "mxnet.metric.Accuracy", "mxnet.metric.F1", "mxnet.metric.CompositeEvalMetric.add"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "get_metric", "(", ")", ":", "\n", "        ", "\"\"\"Get metrics Accuracy and F1\"\"\"", "\n", "metric", "=", "CompositeEvalMetric", "(", ")", "\n", "for", "child_metric", "in", "[", "Accuracy", "(", ")", ",", "F1", "(", ")", "]", ":", "\n", "            ", "metric", ".", "add", "(", "child_metric", ")", "\n", "", "return", "metric", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.dataset.GLUEDataset.__init__": [[107, 113], ["gluonnlp.data.TSVDataset.__init__"], "methods", ["home.repos.pwc.inspect_result.hhexiy_debiased.model.esim.ESIMClassifier.__init__"], ["def", "__init__", "(", "self", ",", "path", ",", "num_discard_samples", ",", "fields", ",", "label_field", "=", "None", ",", "max_num_examples", "=", "-", "1", ")", ":", "\n", "        ", "self", ".", "fields", "=", "fields", "\n", "self", ".", "max_num_examples", "=", "max_num_examples", "\n", "self", ".", "label_field", "=", "label_field", "\n", "super", "(", "GLUEDataset", ",", "self", ")", ".", "__init__", "(", "\n", "path", ",", "num_discard_samples", "=", "num_discard_samples", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.dataset.GLUEDataset._read": [[114, 130], ["super()._read", "logger.info", "max", "logger.info", "logger.info", "len", "enumerate", "len", "len", "len", "dataset.GLUEDataset.get_labels"], "methods", ["home.repos.pwc.inspect_result.hhexiy_debiased.src.dataset.GLUEDataset._read", "home.repos.pwc.inspect_result.hhexiy_debiased.src.dataset.WNLIDataset.get_labels"], ["", "def", "_read", "(", "self", ")", ":", "\n", "        ", "all_samples", "=", "super", "(", "GLUEDataset", ",", "self", ")", ".", "_read", "(", ")", "\n", "logger", ".", "info", "(", "'read {} examples'", ".", "format", "(", "len", "(", "all_samples", ")", ")", ")", "\n", "largest_field", "=", "max", "(", "self", ".", "fields", ")", "\n", "# filter out error records", "\n", "final_samples", "=", "[", "[", "id_", "]", "+", "[", "s", "[", "f", "]", "for", "f", "in", "self", ".", "fields", "]", "for", "id_", ",", "s", "in", "enumerate", "(", "all_samples", ")", "\n", "if", "len", "(", "s", ")", ">=", "largest_field", "+", "1", "]", "\n", "logger", ".", "info", "(", "'{} examples after filtering by number of fields'", ".", "format", "(", "len", "(", "final_samples", ")", ")", ")", "\n", "# filter wrong labels", "\n", "label_field", "=", "self", ".", "label_field", "+", "1", "# we inserted id_ before fields", "\n", "if", "self", ".", "label_field", "is", "not", "None", ":", "\n", "            ", "final_samples", "=", "[", "s", "for", "s", "in", "final_samples", "if", "s", "[", "label_field", "]", "in", "self", ".", "get_labels", "(", ")", "]", "\n", "logger", ".", "info", "(", "'{} examples after filtering by valid labels'", ".", "format", "(", "len", "(", "final_samples", ")", ")", ")", "\n", "", "if", "self", ".", "max_num_examples", ">", "0", ":", "\n", "            ", "return", "final_samples", "[", ":", "self", ".", "max_num_examples", "]", "\n", "", "return", "final_samples", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.dataset.QQPDataset.__init__": [[146, 160], ["os.path.join", "os.path.join", "dataset.GLUEDataset.__init__", "os.getenv"], "methods", ["home.repos.pwc.inspect_result.hhexiy_debiased.model.esim.ESIMClassifier.__init__"], ["def", "__init__", "(", "self", ",", "\n", "segment", "=", "'train'", ",", "\n", "root", "=", "os", ".", "path", ".", "join", "(", "os", ".", "getenv", "(", "'GLUE_DIR'", ",", "'glue_data'", ")", ",", "'QQP'", ")", ")", ":", "\n", "        ", "self", ".", "_supported_segments", "=", "[", "'train'", ",", "'dev'", ",", "'test'", "]", "\n", "assert", "segment", "in", "self", ".", "_supported_segments", ",", "'Unsupported segment: %s'", "%", "segment", "\n", "path", "=", "os", ".", "path", ".", "join", "(", "root", ",", "'%s.tsv'", "%", "segment", ")", "\n", "if", "segment", "in", "[", "'train'", ",", "'dev'", "]", ":", "\n", "            ", "A_IDX", ",", "B_IDX", ",", "LABEL_IDX", "=", "3", ",", "4", ",", "5", "\n", "fields", "=", "[", "A_IDX", ",", "B_IDX", ",", "LABEL_IDX", "]", "\n", "", "elif", "segment", "==", "'test'", ":", "\n", "            ", "A_IDX", ",", "B_IDX", "=", "1", ",", "2", "\n", "fields", "=", "[", "A_IDX", ",", "B_IDX", "]", "\n", "", "super", "(", "QQPDataset", ",", "self", ")", ".", "__init__", "(", "\n", "path", ",", "num_discard_samples", "=", "1", ",", "fields", "=", "fields", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.dataset.QQPDataset.get_labels": [[161, 165], ["None"], "methods", ["None"], ["", "@", "classmethod", "\n", "def", "get_labels", "(", ")", ":", "\n", "        ", "\"\"\"Get classification label ids of the dataset.\"\"\"", "\n", "return", "[", "'0'", ",", "'1'", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.dataset.QQPDataset.get_metric": [[166, 173], ["mxnet.metric.CompositeEvalMetric", "mxnet.metric.Accuracy", "mxnet.metric.F1", "mxnet.metric.CompositeEvalMetric.add"], "methods", ["None"], ["", "@", "classmethod", "\n", "def", "get_metric", "(", ")", ":", "\n", "        ", "\"\"\"Get metrics Accuracy and F1\"\"\"", "\n", "metric", "=", "CompositeEvalMetric", "(", ")", "\n", "for", "child_metric", "in", "[", "Accuracy", "(", ")", ",", "F1", "(", ")", "]", ":", "\n", "            ", "metric", ".", "add", "(", "child_metric", ")", "\n", "", "return", "metric", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.dataset.QQPWangDataset.__init__": [[179, 190], ["os.path.join", "os.path.join", "dataset.QQPDataset.__init__", "os.getenv"], "methods", ["home.repos.pwc.inspect_result.hhexiy_debiased.model.esim.ESIMClassifier.__init__"], ["def", "__init__", "(", "self", ",", "\n", "segment", "=", "'train'", ",", "\n", "root", "=", "os", ".", "path", ".", "join", "(", "os", ".", "getenv", "(", "'GLUE_DIR'", ",", "'glue_data'", ")", ",", "'QQP-wang'", ")", ")", ":", "\n", "        ", "self", ".", "_supported_segments", "=", "[", "'train'", ",", "'dev'", ",", "'test'", "]", "\n", "assert", "segment", "in", "self", ".", "_supported_segments", ",", "'Unsupported segment: %s'", "%", "segment", "\n", "path", "=", "os", ".", "path", ".", "join", "(", "root", ",", "'%s.tsv'", "%", "segment", ")", "\n", "if", "segment", "in", "[", "'train'", ",", "'dev'", ",", "'test'", "]", ":", "\n", "            ", "A_IDX", ",", "B_IDX", ",", "LABEL_IDX", "=", "3", ",", "4", ",", "5", "\n", "fields", "=", "[", "A_IDX", ",", "B_IDX", ",", "LABEL_IDX", "]", "\n", "", "super", "(", "QQPDataset", ",", "self", ")", ".", "__init__", "(", "\n", "path", ",", "num_discard_samples", "=", "1", ",", "fields", "=", "fields", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.dataset.RTEDataset.__init__": [[206, 220], ["os.path.join", "os.path.join", "dataset.GLUEDataset.__init__", "os.getenv"], "methods", ["home.repos.pwc.inspect_result.hhexiy_debiased.model.esim.ESIMClassifier.__init__"], ["def", "__init__", "(", "self", ",", "\n", "segment", "=", "'train'", ",", "\n", "root", "=", "os", ".", "path", ".", "join", "(", "os", ".", "getenv", "(", "'GLUE_DIR'", ",", "'glue_data'", ")", ",", "'RTE'", ")", ")", ":", "\n", "        ", "self", ".", "_supported_segments", "=", "[", "'train'", ",", "'dev'", ",", "'test'", "]", "\n", "assert", "segment", "in", "self", ".", "_supported_segments", ",", "'Unsupported segment: %s'", "%", "segment", "\n", "path", "=", "os", ".", "path", ".", "join", "(", "root", ",", "'%s.tsv'", "%", "segment", ")", "\n", "if", "segment", "in", "[", "'train'", ",", "'dev'", "]", ":", "\n", "            ", "A_IDX", ",", "B_IDX", ",", "LABEL_IDX", "=", "1", ",", "2", ",", "3", "\n", "fields", "=", "[", "A_IDX", ",", "B_IDX", ",", "LABEL_IDX", "]", "\n", "", "elif", "segment", "==", "'test'", ":", "\n", "            ", "A_IDX", ",", "B_IDX", "=", "1", ",", "2", "\n", "fields", "=", "[", "A_IDX", ",", "B_IDX", "]", "\n", "", "super", "(", "RTEDataset", ",", "self", ")", ".", "__init__", "(", "\n", "path", ",", "num_discard_samples", "=", "1", ",", "fields", "=", "fields", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.dataset.RTEDataset.get_labels": [[221, 225], ["None"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "get_labels", "(", ")", ":", "\n", "        ", "\"\"\"Get classification label ids of the dataset.\"\"\"", "\n", "return", "[", "'not_entailment'", ",", "'entailment'", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.dataset.RTEDataset.get_metric": [[226, 230], ["mxnet.metric.Accuracy"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "get_metric", "(", ")", ":", "\n", "        ", "\"\"\"Get metrics Accuracy\"\"\"", "\n", "return", "Accuracy", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.dataset.QNLIDataset.__init__": [[246, 260], ["os.path.join", "os.path.join", "dataset.GLUEDataset.__init__", "os.getenv"], "methods", ["home.repos.pwc.inspect_result.hhexiy_debiased.model.esim.ESIMClassifier.__init__"], ["def", "__init__", "(", "self", ",", "\n", "segment", "=", "'train'", ",", "\n", "root", "=", "os", ".", "path", ".", "join", "(", "os", ".", "getenv", "(", "'GLUE_DIR'", ",", "'glue_data'", ")", ",", "'RTE'", ")", ")", ":", "\n", "        ", "self", ".", "_supported_segments", "=", "[", "'train'", ",", "'dev'", ",", "'test'", "]", "\n", "assert", "segment", "in", "self", ".", "_supported_segments", ",", "'Unsupported segment: %s'", "%", "segment", "\n", "path", "=", "os", ".", "path", ".", "join", "(", "root", ",", "'%s.tsv'", "%", "segment", ")", "\n", "if", "segment", "in", "[", "'train'", ",", "'dev'", "]", ":", "\n", "            ", "A_IDX", ",", "B_IDX", ",", "LABEL_IDX", "=", "1", ",", "2", ",", "3", "\n", "fields", "=", "[", "A_IDX", ",", "B_IDX", ",", "LABEL_IDX", "]", "\n", "", "elif", "segment", "==", "'test'", ":", "\n", "            ", "A_IDX", ",", "B_IDX", "=", "1", ",", "2", "\n", "fields", "=", "[", "A_IDX", ",", "B_IDX", "]", "\n", "", "super", "(", "QNLIDataset", ",", "self", ")", ".", "__init__", "(", "\n", "path", ",", "num_discard_samples", "=", "1", ",", "fields", "=", "fields", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.dataset.QNLIDataset.get_labels": [[261, 265], ["None"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "get_labels", "(", ")", ":", "\n", "        ", "\"\"\"Get classification label ids of the dataset.\"\"\"", "\n", "return", "[", "'not_entailment'", ",", "'entailment'", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.dataset.QNLIDataset.get_metric": [[266, 270], ["mxnet.metric.Accuracy"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "get_metric", "(", ")", ":", "\n", "        ", "\"\"\"Get metrics Accuracy\"\"\"", "\n", "return", "Accuracy", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.dataset.STSBDataset.__init__": [[286, 301], ["os.path.join", "os.path.join", "dataset.GLUEDataset.__init__", "os.getenv"], "methods", ["home.repos.pwc.inspect_result.hhexiy_debiased.model.esim.ESIMClassifier.__init__"], ["def", "__init__", "(", "self", ",", "\n", "segment", "=", "'train'", ",", "\n", "root", "=", "os", ".", "path", ".", "join", "(", "\n", "os", ".", "getenv", "(", "'GLUE_DIR'", ",", "'glue_data'", ")", ",", "'STS-B'", ")", ")", ":", "\n", "        ", "self", ".", "_supported_segments", "=", "[", "'train'", ",", "'dev'", ",", "'test'", "]", "\n", "assert", "segment", "in", "self", ".", "_supported_segments", ",", "'Unsupported segment: %s'", "%", "segment", "\n", "path", "=", "os", ".", "path", ".", "join", "(", "root", ",", "'%s.tsv'", "%", "segment", ")", "\n", "if", "segment", "in", "[", "'train'", ",", "'dev'", "]", ":", "\n", "            ", "A_IDX", ",", "B_IDX", ",", "LABEL_IDX", "=", "7", ",", "8", ",", "9", "\n", "fields", "=", "[", "A_IDX", ",", "B_IDX", ",", "LABEL_IDX", "]", "\n", "", "elif", "segment", "==", "'test'", ":", "\n", "            ", "A_IDX", ",", "B_IDX", "=", "7", ",", "8", "\n", "fields", "=", "[", "A_IDX", ",", "B_IDX", "]", "\n", "", "super", "(", "STSBDataset", ",", "self", ")", ".", "__init__", "(", "\n", "path", ",", "num_discard_samples", "=", "1", ",", "fields", "=", "fields", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.dataset.STSBDataset.get_metric": [[302, 308], ["mxnet.metric.PearsonCorrelation"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "get_metric", "(", ")", ":", "\n", "        ", "\"\"\"\n        Get metrics Accuracy\n        \"\"\"", "\n", "return", "PearsonCorrelation", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.dataset.COLADataset.__init__": [[324, 341], ["os.path.join", "os.path.join", "os.getenv", "dataset.GLUEDataset.__init__", "dataset.GLUEDataset.__init__"], "methods", ["home.repos.pwc.inspect_result.hhexiy_debiased.model.esim.ESIMClassifier.__init__", "home.repos.pwc.inspect_result.hhexiy_debiased.model.esim.ESIMClassifier.__init__"], ["def", "__init__", "(", "self", ",", "\n", "segment", "=", "'train'", ",", "\n", "root", "=", "os", ".", "path", ".", "join", "(", "os", ".", "getenv", "(", "'GLUE_DIR'", ",", "'glue_data'", ")", ",", "\n", "'CoLA'", ")", ")", ":", "\n", "        ", "self", ".", "_supported_segments", "=", "[", "'train'", ",", "'dev'", ",", "'test'", "]", "\n", "assert", "segment", "in", "self", ".", "_supported_segments", ",", "'Unsupported segment: %s'", "%", "segment", "\n", "path", "=", "os", ".", "path", ".", "join", "(", "root", ",", "'%s.tsv'", "%", "segment", ")", "\n", "if", "segment", "in", "[", "'train'", ",", "'dev'", "]", ":", "\n", "            ", "A_IDX", ",", "LABEL_IDX", "=", "3", ",", "1", "\n", "fields", "=", "[", "A_IDX", ",", "LABEL_IDX", "]", "\n", "super", "(", "COLADataset", ",", "self", ")", ".", "__init__", "(", "\n", "path", ",", "num_discard_samples", "=", "0", ",", "fields", "=", "fields", ")", "\n", "", "elif", "segment", "==", "'test'", ":", "\n", "            ", "A_IDX", "=", "3", "\n", "fields", "=", "[", "A_IDX", "]", "\n", "super", "(", "COLADataset", ",", "self", ")", ".", "__init__", "(", "\n", "path", ",", "num_discard_samples", "=", "1", ",", "fields", "=", "fields", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.dataset.COLADataset.get_metric": [[342, 346], ["mxnet.metric.MCC"], "methods", ["None"], ["", "", "@", "staticmethod", "\n", "def", "get_metric", "(", ")", ":", "\n", "        ", "\"\"\"Get metrics  Matthews Correlation Coefficient\"\"\"", "\n", "return", "MCC", "(", "average", "=", "'micro'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.dataset.COLADataset.get_labels": [[347, 351], ["None"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "get_labels", "(", ")", ":", "\n", "        ", "\"\"\"Get classification label ids of the dataset.\"\"\"", "\n", "return", "[", "'0'", ",", "'1'", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.dataset.SSTDataset.__init__": [[367, 382], ["os.path.join", "os.path.join", "dataset.GLUEDataset.__init__", "os.getenv"], "methods", ["home.repos.pwc.inspect_result.hhexiy_debiased.model.esim.ESIMClassifier.__init__"], ["def", "__init__", "(", "self", ",", "\n", "segment", "=", "'train'", ",", "\n", "root", "=", "os", ".", "path", ".", "join", "(", "os", ".", "getenv", "(", "'GLUE_DIR'", ",", "'glue_data'", ")", ",", "\n", "'CoLA'", ")", ")", ":", "\n", "        ", "self", ".", "_supported_segments", "=", "[", "'train'", ",", "'dev'", ",", "'test'", "]", "\n", "assert", "segment", "in", "self", ".", "_supported_segments", ",", "'Unsupported segment: %s'", "%", "segment", "\n", "path", "=", "os", ".", "path", ".", "join", "(", "root", ",", "'%s.tsv'", "%", "segment", ")", "\n", "if", "segment", "in", "[", "'train'", ",", "'dev'", "]", ":", "\n", "            ", "A_IDX", ",", "LABEL_IDX", "=", "0", ",", "1", "\n", "fields", "=", "[", "A_IDX", ",", "LABEL_IDX", "]", "\n", "", "elif", "segment", "==", "'test'", ":", "\n", "            ", "A_IDX", "=", "1", "\n", "fields", "=", "[", "A_IDX", "]", "\n", "", "super", "(", "SSTDataset", ",", "self", ")", ".", "__init__", "(", "\n", "path", ",", "num_discard_samples", "=", "1", ",", "fields", "=", "fields", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.dataset.SSTDataset.get_metric": [[383, 387], ["mxnet.metric.Accuracy"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "get_metric", "(", ")", ":", "\n", "        ", "\"\"\"Get metrics Accuracy\"\"\"", "\n", "return", "Accuracy", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.dataset.SSTDataset.get_labels": [[388, 392], ["None"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "get_labels", "(", ")", ":", "\n", "        ", "\"\"\"Get classification label ids of the dataset.\"\"\"", "\n", "return", "[", "'0'", ",", "'1'", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.dataset.MNLIDataset.__init__": [[412, 437], ["os.path.join", "os.path.join", "dataset.GLUEDataset.__init__", "os.getenv"], "methods", ["home.repos.pwc.inspect_result.hhexiy_debiased.model.esim.ESIMClassifier.__init__"], ["def", "__init__", "(", "self", ",", "\n", "segment", "=", "'dev_matched'", ",", "\n", "root", "=", "os", ".", "path", ".", "join", "(", "os", ".", "getenv", "(", "'GLUE_DIR'", ",", "'glue_data'", ")", ",", "\n", "'MNLI'", ")", ",", "\n", "max_num_examples", "=", "-", "1", ")", ":", "#pylint: disable=c0330", "\n", "        ", "self", ".", "_supported_segments", "=", "[", "\n", "'dev_matched'", ",", "'dev_mismatched'", ",", "'test_matched'", ",", "'test_mismatched'", ",", "\n", "'train'", "\n", "]", "\n", "assert", "segment", "in", "self", ".", "_supported_segments", ",", "'Unsupported segment: %s'", "%", "segment", "\n", "path", "=", "os", ".", "path", ".", "join", "(", "root", ",", "'%s.tsv'", "%", "segment", ")", "\n", "A_IDX", ",", "B_IDX", "=", "8", ",", "9", "\n", "if", "segment", "in", "[", "'dev_matched'", ",", "'dev_mismatched'", "]", ":", "\n", "            ", "LABEL_IDX", "=", "15", "\n", "fields", "=", "[", "A_IDX", ",", "B_IDX", ",", "LABEL_IDX", "]", "\n", "label_field", "=", "2", "\n", "", "elif", "segment", "in", "[", "'test_matched'", ",", "'test_mismatched'", "]", ":", "\n", "            ", "fields", "=", "[", "A_IDX", ",", "B_IDX", "]", "\n", "label_field", "=", "None", "\n", "", "elif", "segment", "==", "'train'", ":", "\n", "            ", "LABEL_IDX", "=", "11", "\n", "fields", "=", "[", "A_IDX", ",", "B_IDX", ",", "LABEL_IDX", "]", "\n", "label_field", "=", "2", "\n", "", "super", "(", "MNLIDataset", ",", "self", ")", ".", "__init__", "(", "\n", "path", ",", "num_discard_samples", "=", "1", ",", "fields", "=", "fields", ",", "label_field", "=", "label_field", ",", "max_num_examples", "=", "max_num_examples", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.dataset.MNLIDataset.num_classes": [[438, 441], ["None"], "methods", ["None"], ["", "@", "classmethod", "\n", "def", "num_classes", "(", "cls", ")", ":", "\n", "        ", "return", "3", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.dataset.MNLIDataset.get_labels": [[442, 446], ["None"], "methods", ["None"], ["", "@", "classmethod", "\n", "def", "get_labels", "(", "cls", ")", ":", "\n", "        ", "\"\"\"Get classification label ids of the dataset.\"\"\"", "\n", "return", "[", "'neutral'", ",", "'entailment'", ",", "'contradiction'", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.dataset.MNLIDataset.get_metric": [[447, 451], ["mxnet.metric.Accuracy"], "methods", ["None"], ["", "@", "classmethod", "\n", "def", "get_metric", "(", "cls", ")", ":", "\n", "        ", "\"\"\"Get metrics Accuracy\"\"\"", "\n", "return", "Accuracy", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.dataset.SNLIDataset.__init__": [[467, 480], ["os.path.join", "os.path.join", "dataset.GLUEDataset.__init__", "os.getenv"], "methods", ["home.repos.pwc.inspect_result.hhexiy_debiased.model.esim.ESIMClassifier.__init__"], ["def", "__init__", "(", "self", ",", "\n", "segment", "=", "'train'", ",", "\n", "root", "=", "os", ".", "path", ".", "join", "(", "os", ".", "getenv", "(", "'GLUE_DIR'", ",", "'glue_data'", ")", ",", "\n", "'SNLI'", ")", ",", "\n", "max_num_examples", "=", "-", "1", ")", ":", "#pylint: disable=c0330", "\n", "        ", "self", ".", "_supported_segments", "=", "[", "'train'", ",", "'dev'", ",", "'test'", "]", "\n", "assert", "segment", "in", "self", ".", "_supported_segments", ",", "'Unsupported segment: %s'", "%", "segment", "\n", "# NOTE: number of examples in .tsv files is different than original/*.txt", "\n", "path", "=", "os", ".", "path", ".", "join", "(", "root", ",", "'original'", ",", "'snli_1.0_%s.txt'", "%", "segment", ")", "\n", "A_IDX", ",", "B_IDX", ",", "LABEL_IDX", "=", "5", ",", "6", ",", "0", "\n", "fields", "=", "[", "A_IDX", ",", "B_IDX", ",", "LABEL_IDX", "]", "\n", "super", "(", "SNLIDataset", ",", "self", ")", ".", "__init__", "(", "\n", "path", ",", "num_discard_samples", "=", "1", ",", "fields", "=", "fields", ",", "label_field", "=", "2", ",", "max_num_examples", "=", "max_num_examples", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.dataset.SNLIDataset.num_classes": [[481, 484], ["None"], "methods", ["None"], ["", "@", "classmethod", "\n", "def", "num_classes", "(", "cls", ")", ":", "\n", "        ", "return", "3", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.dataset.SNLIDataset.get_labels": [[485, 489], ["None"], "methods", ["None"], ["", "@", "classmethod", "\n", "def", "get_labels", "(", "cls", ")", ":", "\n", "        ", "\"\"\"Get classification label ids of the dataset.\"\"\"", "\n", "return", "[", "'neutral'", ",", "'entailment'", ",", "'contradiction'", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.dataset.SNLIDataset.get_metric": [[490, 494], ["mxnet.metric.Accuracy"], "methods", ["None"], ["", "@", "classmethod", "\n", "def", "get_metric", "(", "cls", ")", ":", "\n", "        ", "\"\"\"Get metrics Accuracy\"\"\"", "\n", "return", "Accuracy", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.dataset.SNLIBreakDataset.__init__": [[502, 515], ["os.path.join", "os.path.join", "dataset.SNLIDataset.__init__", "os.getenv"], "methods", ["home.repos.pwc.inspect_result.hhexiy_debiased.model.esim.ESIMClassifier.__init__"], ["def", "__init__", "(", "self", ",", "\n", "segment", "=", "'test'", ",", "\n", "root", "=", "os", ".", "path", ".", "join", "(", "os", ".", "getenv", "(", "'GLUE_DIR'", ",", "'glue_data'", ")", ",", "\n", "'SNLI-break'", ")", ",", "\n", "max_num_examples", "=", "-", "1", ")", ":", "#pylint: disable=c0330", "\n", "        ", "self", ".", "_supported_segments", "=", "[", "'test'", "]", "\n", "assert", "segment", "in", "self", ".", "_supported_segments", ",", "'Unsupported segment: %s'", "%", "segment", "\n", "# NOTE: number of examples in .tsv files is different than original/*.txt", "\n", "path", "=", "os", ".", "path", ".", "join", "(", "root", ",", "'%s.tsv'", "%", "segment", ")", "\n", "A_IDX", ",", "B_IDX", ",", "LABEL_IDX", "=", "7", ",", "8", ",", "14", "\n", "fields", "=", "[", "A_IDX", ",", "B_IDX", ",", "LABEL_IDX", "]", "\n", "super", "(", "SNLIDataset", ",", "self", ")", ".", "__init__", "(", "\n", "path", ",", "num_discard_samples", "=", "1", ",", "fields", "=", "fields", ",", "label_field", "=", "2", ",", "max_num_examples", "=", "max_num_examples", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.dataset.SNLISwapDataset.__init__": [[518, 531], ["os.path.join", "os.path.join", "dataset.SNLIDataset.__init__", "os.getenv"], "methods", ["home.repos.pwc.inspect_result.hhexiy_debiased.model.esim.ESIMClassifier.__init__"], ["    ", "def", "__init__", "(", "self", ",", "\n", "segment", "=", "'test'", ",", "\n", "root", "=", "os", ".", "path", ".", "join", "(", "os", ".", "getenv", "(", "'GLUE_DIR'", ",", "'glue_data'", ")", ",", "\n", "'SNLI-swap'", ")", ",", "\n", "max_num_examples", "=", "-", "1", ")", ":", "#pylint: disable=c0330", "\n", "        ", "self", ".", "_supported_segments", "=", "[", "'test'", "]", "\n", "assert", "segment", "in", "self", ".", "_supported_segments", ",", "'Unsupported segment: %s'", "%", "segment", "\n", "# NOTE: number of examples in .tsv files is different than original/*.txt", "\n", "path", "=", "os", ".", "path", ".", "join", "(", "root", ",", "'%s.tsv'", "%", "segment", ")", "\n", "A_IDX", ",", "B_IDX", ",", "LABEL_IDX", "=", "5", ",", "6", ",", "0", "\n", "fields", "=", "[", "A_IDX", ",", "B_IDX", ",", "LABEL_IDX", "]", "\n", "super", "(", "SNLIDataset", ",", "self", ")", ".", "__init__", "(", "\n", "path", ",", "num_discard_samples", "=", "1", ",", "fields", "=", "fields", ",", "label_field", "=", "2", ",", "max_num_examples", "=", "max_num_examples", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.dataset.SNLISwapDataset.get_labels": [[532, 538], ["dataset.SNLIDataset.get_labels", "super().get_labels.append"], "methods", ["home.repos.pwc.inspect_result.hhexiy_debiased.src.dataset.WNLIDataset.get_labels"], ["", "@", "classmethod", "\n", "def", "get_labels", "(", "cls", ")", ":", "\n", "        ", "\"\"\"Get classification label ids of the dataset.\"\"\"", "\n", "labels", "=", "super", "(", ")", ".", "get_labels", "(", ")", "\n", "labels", ".", "append", "(", "'non-contradiction'", ")", "\n", "return", "labels", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.dataset.SNLISwapDataset.get_metric": [[539, 544], ["dataset.MappedAccuracy"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "get_metric", "(", ")", ":", "\n", "        ", "\"\"\"Get metrics Accuracy\"\"\"", "\n", "# 0, 1, 2, 3 = ['neutral', 'entailment', 'contradiction', 'non-contradiction']", "\n", "return", "MappedAccuracy", "(", "label_map", "=", "{", "0", ":", "3", ",", "1", ":", "3", "}", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.dataset.MNLISwapDataset.__init__": [[547, 555], ["os.path.join", "dataset.MNLIDataset.__init__", "os.getenv"], "methods", ["home.repos.pwc.inspect_result.hhexiy_debiased.model.esim.ESIMClassifier.__init__"], ["    ", "def", "__init__", "(", "self", ",", "\n", "segment", "=", "'train'", ",", "\n", "root", "=", "os", ".", "path", ".", "join", "(", "os", ".", "getenv", "(", "'GLUE_DIR'", ",", "'glue_data'", ")", ",", "\n", "'MNLI-swap'", ")", ",", "\n", "max_num_examples", "=", "-", "1", ")", ":", "#pylint: disable=c0330", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "segment", ",", "root", ",", "max_num_examples", ")", "\n", "self", ".", "_supported_segments", "=", "[", "'dev_matched'", ",", "'dev_mismatched'", "]", "\n", "assert", "segment", "in", "self", ".", "_supported_segments", ",", "'Unsupported segment: %s'", "%", "segment", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.dataset.MNLISwapDataset.get_labels": [[556, 562], ["dataset.MNLIDataset.get_labels", "super().get_labels.append"], "methods", ["home.repos.pwc.inspect_result.hhexiy_debiased.src.dataset.WNLIDataset.get_labels"], ["", "@", "classmethod", "\n", "def", "get_labels", "(", "cls", ")", ":", "\n", "        ", "\"\"\"Get classification label ids of the dataset.\"\"\"", "\n", "labels", "=", "super", "(", ")", ".", "get_labels", "(", ")", "\n", "labels", ".", "append", "(", "'non-contradiction'", ")", "\n", "return", "labels", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.dataset.MNLISwapDataset.get_metric": [[563, 568], ["dataset.MappedAccuracy"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "get_metric", "(", ")", ":", "\n", "        ", "\"\"\"Get metrics Accuracy\"\"\"", "\n", "# 0, 1, 2, 3 = ['neutral', 'entailment', 'contradiction', 'non-contradiction']", "\n", "return", "MappedAccuracy", "(", "label_map", "=", "{", "0", ":", "3", ",", "1", ":", "3", "}", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.dataset.MNLIHansDataset.__init__": [[577, 591], ["os.path.join", "glob.glob", "dataset.MNLIDataset.__init__", "os.getenv", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.hhexiy_debiased.model.esim.ESIMClassifier.__init__"], ["def", "__init__", "(", "self", ",", "\n", "segment", "=", "'lexical_overlap'", ",", "\n", "root", "=", "os", ".", "path", ".", "join", "(", "os", ".", "getenv", "(", "'GLUE_DIR'", ",", "'glue_data'", ")", ",", "\n", "'MNLI-hans'", ")", ",", "\n", "max_num_examples", "=", "-", "1", ")", ":", "#pylint: disable=c0330", "\n", "        ", "self", ".", "_supported_segments", "=", "[", "segment", "for", "segment", "in", "(", "'lexical_overlap'", ",", "'constituent'", ",", "'subsequence'", ")", "]", "\n", "assert", "segment", "in", "self", ".", "_supported_segments", ",", "'Unsupported segment: %s'", "%", "segment", "\n", "\n", "path", "=", "glob", ".", "glob", "(", "os", ".", "path", ".", "join", "(", "root", ",", "'{}.tsv'", ".", "format", "(", "segment", ")", ")", ")", "\n", "A_IDX", ",", "B_IDX", ",", "LABEL_IDX", "=", "5", ",", "6", ",", "0", "\n", "\n", "fields", "=", "[", "A_IDX", ",", "B_IDX", ",", "LABEL_IDX", "]", "\n", "super", "(", "MNLIDataset", ",", "self", ")", ".", "__init__", "(", "\n", "path", ",", "num_discard_samples", "=", "1", ",", "fields", "=", "fields", ",", "label_field", "=", "2", ",", "max_num_examples", "=", "max_num_examples", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.dataset.MNLIHansDataset.get_labels": [[592, 598], ["dataset.MNLIDataset.get_labels", "super().get_labels.append"], "methods", ["home.repos.pwc.inspect_result.hhexiy_debiased.src.dataset.WNLIDataset.get_labels"], ["", "@", "classmethod", "\n", "def", "get_labels", "(", "cls", ")", ":", "\n", "        ", "\"\"\"Get classification label ids of the dataset.\"\"\"", "\n", "labels", "=", "super", "(", ")", ".", "get_labels", "(", ")", "\n", "labels", ".", "append", "(", "'non-entailment'", ")", "\n", "return", "labels", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.dataset.MNLIHansDataset.get_metric": [[599, 604], ["dataset.MappedAccuracy"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "get_metric", "(", ")", ":", "\n", "        ", "\"\"\"Get metrics Accuracy\"\"\"", "\n", "# 0, 1, 2, 3 = ['neutral', 'entailment', 'contradiction', 'non-entailment']", "\n", "return", "MappedAccuracy", "(", "label_map", "=", "{", "0", ":", "3", ",", "2", ":", "3", "}", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.dataset.MNLIStressTestDataset.__init__": [[607, 626], ["os.path.join", "segment.split", "dataset.MNLIDataset.__init__", "os.getenv", "glob.glob", "glob.glob", "os.path.join", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.hhexiy_debiased.model.esim.ESIMClassifier.__init__"], ["    ", "def", "__init__", "(", "self", ",", "\n", "segment", "=", "'Antonym,matched'", ",", "\n", "root", "=", "os", ".", "path", ".", "join", "(", "os", ".", "getenv", "(", "'GLUE_DIR'", ",", "'glue_data'", ")", ",", "\n", "'MNLI-stress'", ")", ",", "\n", "max_num_examples", "=", "-", "1", ")", ":", "#pylint: disable=c0330", "\n", "        ", "self", ".", "_supported_segments", "=", "[", "'{},{}'", ".", "format", "(", "segment", ",", "type_", ")", "for", "segment", "in", "(", "'Antonym'", ",", "'Length_Mismatch'", ",", "'Negation'", ",", "'Numerical_Reasoning'", ",", "'Spelling_Error'", ",", "'Word_Overlap'", ")", "for", "type_", "in", "(", "'matched'", ",", "'mismatched'", ")", "]", "\n", "assert", "segment", "in", "self", ".", "_supported_segments", ",", "'Unsupported segment: %s'", "%", "segment", "\n", "segment", ",", "type_", "=", "segment", ".", "split", "(", "','", ")", "\n", "\n", "if", "segment", "==", "'Numerical_Reasoning'", ":", "\n", "            ", "path", "=", "glob", ".", "glob", "(", "os", ".", "path", ".", "join", "(", "root", ",", "segment", ",", "'*_.tsv'", ")", ")", "\n", "A_IDX", ",", "B_IDX", ",", "LABEL_IDX", "=", "1", ",", "2", ",", "0", "\n", "", "else", ":", "\n", "            ", "path", "=", "glob", ".", "glob", "(", "os", ".", "path", ".", "join", "(", "root", ",", "segment", ",", "'*_{}.tsv'", ".", "format", "(", "type_", ")", ")", ")", "\n", "A_IDX", ",", "B_IDX", ",", "LABEL_IDX", "=", "5", ",", "6", ",", "0", "\n", "\n", "", "fields", "=", "[", "A_IDX", ",", "B_IDX", ",", "LABEL_IDX", "]", "\n", "super", "(", "MNLIDataset", ",", "self", ")", ".", "__init__", "(", "\n", "path", ",", "num_discard_samples", "=", "1", ",", "fields", "=", "fields", ",", "label_field", "=", "2", ",", "max_num_examples", "=", "max_num_examples", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.dataset.SNLIHaohanDataset.__init__": [[629, 642], ["os.path.join", "os.path.join", "dataset.SNLIDataset.__init__", "os.getenv"], "methods", ["home.repos.pwc.inspect_result.hhexiy_debiased.model.esim.ESIMClassifier.__init__"], ["    ", "def", "__init__", "(", "self", ",", "\n", "segment", "=", "'train'", ",", "\n", "root", "=", "os", ".", "path", ".", "join", "(", "os", ".", "getenv", "(", "'GLUE_DIR'", ",", "'glue_data'", ")", ",", "\n", "'SNLI-haohan'", ")", ",", "\n", "max_num_examples", "=", "-", "1", ")", ":", "#pylint: disable=c0330", "\n", "        ", "self", ".", "_supported_segments", "=", "[", "'train'", ",", "'dev'", ",", "'test'", "]", "\n", "assert", "segment", "in", "self", ".", "_supported_segments", ",", "'Unsupported segment: %s'", "%", "segment", "\n", "# NOTE: number of examples in .tsv files is different than original/*.txt", "\n", "path", "=", "os", ".", "path", ".", "join", "(", "root", ",", "'%s.tsv'", "%", "segment", ")", "\n", "A_IDX", ",", "B_IDX", ",", "LABEL_IDX", "=", "7", ",", "8", ",", "14", "\n", "fields", "=", "[", "A_IDX", ",", "B_IDX", ",", "LABEL_IDX", "]", "\n", "super", "(", ")", ".", "__init__", "(", "\n", "path", ",", "num_discard_samples", "=", "1", ",", "fields", "=", "fields", ",", "label_field", "=", "2", ",", "max_num_examples", "=", "max_num_examples", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.dataset.WNLIDataset.__init__": [[658, 673], ["os.path.join", "os.path.join", "dataset.GLUEDataset.__init__", "os.getenv"], "methods", ["home.repos.pwc.inspect_result.hhexiy_debiased.model.esim.ESIMClassifier.__init__"], ["def", "__init__", "(", "self", ",", "\n", "segment", "=", "'train'", ",", "\n", "root", "=", "os", ".", "path", ".", "join", "(", "os", ".", "getenv", "(", "'GLUE_DIR'", ",", "'glue_data'", ")", ",", "\n", "'WNLI'", ")", ")", ":", "\n", "        ", "self", ".", "_supported_segments", "=", "[", "'train'", ",", "'dev'", ",", "'test'", "]", "\n", "assert", "segment", "in", "self", ".", "_supported_segments", ",", "'Unsupported segment: %s'", "%", "segment", "\n", "path", "=", "os", ".", "path", ".", "join", "(", "root", ",", "'%s.tsv'", "%", "segment", ")", "\n", "if", "segment", "in", "[", "'train'", ",", "'dev'", "]", ":", "\n", "            ", "A_IDX", ",", "B_IDX", ",", "LABEL_IDX", "=", "1", ",", "2", ",", "3", "\n", "fields", "=", "[", "A_IDX", ",", "B_IDX", ",", "LABEL_IDX", "]", "\n", "", "elif", "segment", "==", "'test'", ":", "\n", "            ", "A_IDX", ",", "B_IDX", "=", "1", ",", "2", "\n", "fields", "=", "[", "A_IDX", ",", "B_IDX", "]", "\n", "", "super", "(", "WNLIDataset", ",", "self", ")", ".", "__init__", "(", "\n", "path", ",", "num_discard_samples", "=", "1", ",", "fields", "=", "fields", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.dataset.WNLIDataset.get_labels": [[674, 678], ["None"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "get_labels", "(", ")", ":", "\n", "        ", "\"\"\"Get classification label ids of the dataset.\"\"\"", "\n", "return", "[", "'0'", ",", "'1'", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.dataset.WNLIDataset.get_metric": [[679, 683], ["mxnet.metric.Accuracy"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "get_metric", "(", ")", ":", "\n", "        ", "\"\"\"Get metrics Accuracy\"\"\"", "\n", "return", "Accuracy", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.dataset.BERTTransform.__init__": [[716, 721], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "tokenizer", ",", "max_seq_length", ",", "pad", "=", "True", ",", "pair", "=", "True", ")", ":", "\n", "        ", "self", ".", "_tokenizer", "=", "tokenizer", "\n", "self", ".", "_max_seq_length", "=", "max_seq_length", "\n", "self", ".", "_pad", "=", "pad", "\n", "self", ".", "_pair", "=", "pair", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.dataset.BERTTransform.__call__": [[722, 833], ["convert_to_unicode", "dataset.BERTTransform._tokenizer.tokenize", "tokens.append", "segment_ids.append", "tokens.append", "segment_ids.append", "dataset.BERTTransform._tokenizer.convert_tokens_to_ids", "len", "convert_to_unicode", "dataset.BERTTransform._tokenizer.tokenize", "dataset._truncate_seq_pair", "tokens.append", "segment_ids.append", "tokens.append", "segment_ids.append", "dataset.BERTTransform.extend", "segment_ids.extend", "numpy.array", "numpy.array", "numpy.array", "len", "len", "tokens.append", "segment_ids.append"], "methods", ["home.repos.pwc.inspect_result.hhexiy_debiased.src.tokenizer.convert_to_unicode", "home.repos.pwc.inspect_result.hhexiy_debiased.src.tokenizer.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.hhexiy_debiased.src.tokenizer.convert_tokens_to_ids", "home.repos.pwc.inspect_result.hhexiy_debiased.src.tokenizer.convert_to_unicode", "home.repos.pwc.inspect_result.hhexiy_debiased.src.tokenizer.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.hhexiy_debiased.src.dataset._truncate_seq_pair"], ["", "def", "__call__", "(", "self", ",", "line", ")", ":", "\n", "        ", "\"\"\"Perform transformation for sequence pairs or single sequences.\n\n        The transformation is processed in the following steps:\n        - tokenize the input sequences\n        - insert [CLS], [SEP] as necessary\n        - generate type ids to indicate whether a token belongs to the first\n          sequence or the second sequence.\n        - generate valid length\n\n        For sequence pairs, the input is a tuple of 2 strings:\n        text_a, text_b.\n\n        Inputs:\n            text_a: 'is this jacksonville ?'\n            text_b: 'no it is not'\n        Tokenization:\n            text_a: 'is this jack ##son ##ville ?'\n            text_b: 'no it is not .'\n        Processed:\n            tokens:  '[CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]'\n            type_ids: 0     0  0    0    0     0       0 0     1  1  1  1   1 1\n            valid_length: 14\n\n        For single sequences, the input is a tuple of single string: text_a.\n        Inputs:\n            text_a: 'the dog is hairy .'\n        Tokenization:\n            text_a: 'the dog is hairy .'\n        Processed:\n            text_a:  '[CLS] the dog is hairy . [SEP]'\n            type_ids: 0     0   0   0  0     0 0\n            valid_length: 7\n\n        Parameters\n        ----------\n        line: tuple of str\n            Input strings. For sequence pairs, the input is a tuple of 3 strings:\n            (text_a, text_b). For single sequences, the input is a tuple of single\n            string: (text_a,).\n\n        Returns\n        -------\n        np.array: input token ids in 'int32', shape (batch_size, seq_length)\n        np.array: valid length in 'int32', shape (batch_size,)\n        np.array: input token type ids in 'int32', shape (batch_size, seq_length)\n        \"\"\"", "\n", "# convert to unicode", "\n", "text_a", "=", "line", "[", "0", "]", "\n", "text_a", "=", "convert_to_unicode", "(", "text_a", ")", "\n", "if", "self", ".", "_pair", ":", "\n", "            ", "assert", "len", "(", "line", ")", "==", "2", "\n", "text_b", "=", "line", "[", "1", "]", "\n", "text_b", "=", "convert_to_unicode", "(", "text_b", ")", "\n", "\n", "", "tokens_a", "=", "self", ".", "_tokenizer", ".", "tokenize", "(", "text_a", ")", "\n", "tokens_b", "=", "None", "\n", "\n", "if", "self", ".", "_pair", ":", "\n", "            ", "tokens_b", "=", "self", ".", "_tokenizer", ".", "tokenize", "(", "text_b", ")", "\n", "\n", "", "if", "tokens_b", ":", "\n", "# Modifies `tokens_a` and `tokens_b` in place so that the total", "\n", "# length is less than the specified length.", "\n", "# Account for [CLS], [SEP], [SEP] with \"- 3\"", "\n", "            ", "_truncate_seq_pair", "(", "tokens_a", ",", "tokens_b", ",", "self", ".", "_max_seq_length", "-", "3", ")", "\n", "", "else", ":", "\n", "# Account for [CLS] and [SEP] with \"- 2\"", "\n", "            ", "if", "len", "(", "tokens_a", ")", ">", "self", ".", "_max_seq_length", "-", "2", ":", "\n", "                ", "tokens_a", "=", "tokens_a", "[", "0", ":", "(", "self", ".", "_max_seq_length", "-", "2", ")", "]", "\n", "\n", "# The embedding vectors for `type=0` and `type=1` were learned during", "\n", "# pre-training and are added to the wordpiece embedding vector", "\n", "# (and position vector). This is not *strictly* necessary since", "\n", "# the [SEP] token unambiguously separates the sequences, but it makes", "\n", "# it easier for the model to learn the concept of sequences.", "\n", "\n", "# For classification tasks, the first vector (corresponding to [CLS]) is", "\n", "# used as as the \"sentence vector\". Note that this only makes sense because", "\n", "# the entire model is fine-tuned.", "\n", "", "", "tokens", "=", "[", "]", "\n", "segment_ids", "=", "[", "]", "\n", "tokens", ".", "append", "(", "'[CLS]'", ")", "\n", "segment_ids", ".", "append", "(", "0", ")", "\n", "for", "token", "in", "tokens_a", ":", "\n", "            ", "tokens", ".", "append", "(", "token", ")", "\n", "segment_ids", ".", "append", "(", "0", ")", "\n", "", "tokens", ".", "append", "(", "'[SEP]'", ")", "\n", "segment_ids", ".", "append", "(", "0", ")", "\n", "\n", "if", "tokens_b", ":", "\n", "            ", "for", "token", "in", "tokens_b", ":", "\n", "                ", "tokens", ".", "append", "(", "token", ")", "\n", "segment_ids", ".", "append", "(", "1", ")", "\n", "", "tokens", ".", "append", "(", "'[SEP]'", ")", "\n", "segment_ids", ".", "append", "(", "1", ")", "\n", "\n", "", "input_ids", "=", "self", ".", "_tokenizer", ".", "convert_tokens_to_ids", "(", "tokens", ")", "\n", "\n", "# The valid length of sentences. Only real  tokens are attended to.", "\n", "valid_length", "=", "len", "(", "input_ids", ")", "\n", "\n", "if", "self", ".", "_pad", ":", "\n", "# Zero-pad up to the sequence length.", "\n", "            ", "padding_length", "=", "self", ".", "_max_seq_length", "-", "valid_length", "\n", "# use padding tokens for the rest", "\n", "input_ids", ".", "extend", "(", "[", "self", ".", "_tokenizer", ".", "vocab", "[", "'[PAD]'", "]", "]", "*", "padding_length", ")", "\n", "segment_ids", ".", "extend", "(", "[", "self", ".", "_tokenizer", ".", "vocab", "[", "'[PAD]'", "]", "]", "*", "padding_length", ")", "\n", "\n", "", "return", "np", ".", "array", "(", "input_ids", ",", "dtype", "=", "'int32'", ")", ",", "np", ".", "array", "(", "valid_length", ",", "dtype", "=", "'int32'", ")", ",", "np", ".", "array", "(", "segment_ids", ",", "dtype", "=", "'int32'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.dataset.CBOWTransform.__init__": [[838, 845], ["enumerate"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "labels", ",", "tokenizer", ",", "vocab", ",", "num_input_sentences", "=", "2", ")", ":", "\n", "        ", "self", ".", "_label_map", "=", "{", "}", "\n", "for", "(", "i", ",", "label", ")", "in", "enumerate", "(", "labels", ")", ":", "\n", "            ", "self", ".", "_label_map", "[", "label", "]", "=", "i", "\n", "", "self", ".", "_vocab", "=", "vocab", "\n", "self", ".", "_tokenizer", "=", "tokenizer", "\n", "self", ".", "num_input_sentences", "=", "num_input_sentences", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.dataset.CBOWTransform.__call__": [[846, 856], ["convert_to_unicode", "numpy.array", "dataset.CBOWTransform._vocab", "len", "dataset.CBOWTransform._tokenizer.tokenize"], "methods", ["home.repos.pwc.inspect_result.hhexiy_debiased.src.tokenizer.convert_to_unicode", "home.repos.pwc.inspect_result.hhexiy_debiased.src.tokenizer.WordpieceTokenizer.tokenize"], ["", "def", "__call__", "(", "self", ",", "line", ")", ":", "\n", "        ", "id_", "=", "line", "[", "0", "]", "\n", "inputs", "=", "line", "[", "1", ":", "-", "1", "]", "# list of text strings", "\n", "label", "=", "line", "[", "-", "1", "]", "\n", "label", "=", "convert_to_unicode", "(", "label", ")", "\n", "label_id", "=", "self", ".", "_label_map", "[", "label", "]", "\n", "label_id", "=", "np", ".", "array", "(", "[", "label_id", "]", ",", "dtype", "=", "'int32'", ")", "\n", "input_ids", "=", "[", "self", ".", "_vocab", "(", "self", ".", "_tokenizer", ".", "tokenize", "(", "s", ")", ")", "for", "s", "in", "inputs", "]", "\n", "valid_lengths", "=", "[", "len", "(", "s", ")", "for", "s", "in", "input_ids", "]", "\n", "return", "id_", ",", "input_ids", ",", "valid_lengths", ",", "label_id", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.dataset.CBOWTransform.get_length": [[857, 859], ["max"], "methods", ["None"], ["", "def", "get_length", "(", "self", ",", "*", "data", ")", ":", "\n", "        ", "return", "max", "(", "data", "[", "2", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.dataset.CBOWTransform.get_batcher": [[860, 867], ["gluonnlp.data.batchify.Tuple", "gluonnlp.data.batchify.Stack", "gluonnlp.data.batchify.Tuple", "gluonnlp.data.batchify.Tuple", "gluonnlp.data.batchify.Stack", "gluonnlp.data.batchify.Pad", "gluonnlp.data.batchify.Stack", "range", "range"], "methods", ["None"], ["", "def", "get_batcher", "(", "self", ")", ":", "\n", "        ", "batchify_fn", "=", "nlp", ".", "data", ".", "batchify", ".", "Tuple", "(", "\n", "nlp", ".", "data", ".", "batchify", ".", "Stack", "(", ")", ",", "\n", "nlp", ".", "data", ".", "batchify", ".", "Tuple", "(", "*", "[", "nlp", ".", "data", ".", "batchify", ".", "Pad", "(", "axis", "=", "0", ",", "pad_val", "=", "self", ".", "_vocab", "[", "self", ".", "_vocab", ".", "padding_token", "]", ")", "for", "_", "in", "range", "(", "self", ".", "num_input_sentences", ")", "]", ")", ",", "\n", "nlp", ".", "data", ".", "batchify", ".", "Tuple", "(", "*", "[", "nlp", ".", "data", ".", "batchify", ".", "Stack", "(", ")", "for", "_", "in", "range", "(", "self", ".", "num_input_sentences", ")", "]", ")", ",", "\n", "nlp", ".", "data", ".", "batchify", ".", "Stack", "(", ")", ")", "\n", "return", "batchify_fn", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.dataset.DATransform.__call__": [[872, 882], ["convert_to_unicode", "numpy.array", "dataset.DATransform._vocab", "len", "dataset.DATransform._tokenizer.tokenize"], "methods", ["home.repos.pwc.inspect_result.hhexiy_debiased.src.tokenizer.convert_to_unicode", "home.repos.pwc.inspect_result.hhexiy_debiased.src.tokenizer.WordpieceTokenizer.tokenize"], ["def", "__call__", "(", "self", ",", "line", ")", ":", "\n", "        ", "id_", "=", "line", "[", "0", "]", "\n", "inputs", "=", "line", "[", "1", ":", "-", "1", "]", "# list of text strings", "\n", "label", "=", "line", "[", "-", "1", "]", "\n", "label", "=", "convert_to_unicode", "(", "label", ")", "\n", "label_id", "=", "self", ".", "_label_map", "[", "label", "]", "\n", "label_id", "=", "np", ".", "array", "(", "[", "label_id", "]", ",", "dtype", "=", "'int32'", ")", "\n", "input_ids", "=", "[", "self", ".", "_vocab", "(", "[", "'NULL'", "]", "+", "self", ".", "_tokenizer", ".", "tokenize", "(", "s", ")", ")", "for", "s", "in", "inputs", "]", "\n", "valid_lengths", "=", "[", "len", "(", "s", ")", "for", "s", "in", "input_ids", "]", "\n", "return", "id_", ",", "input_ids", ",", "valid_lengths", ",", "label_id", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.dataset.ESIMTransform.__init__": [[885, 888], ["dataset.CBOWTransform.__init__"], "methods", ["home.repos.pwc.inspect_result.hhexiy_debiased.model.esim.ESIMClassifier.__init__"], ["    ", "def", "__init__", "(", "self", ",", "labels", ",", "tokenizer", ",", "vocab", ",", "max_len", "=", "60", ",", "num_input_sentences", "=", "2", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "labels", ",", "tokenizer", ",", "vocab", ",", "num_input_sentences", ")", "\n", "self", ".", "max_seq_length", "=", "max_len", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.dataset.ESIMTransform.__call__": [[889, 899], ["convert_to_unicode", "numpy.array", "dataset.ESIMTransform._vocab", "len", "dataset.ESIMTransform._tokenizer.tokenize"], "methods", ["home.repos.pwc.inspect_result.hhexiy_debiased.src.tokenizer.convert_to_unicode", "home.repos.pwc.inspect_result.hhexiy_debiased.src.tokenizer.WordpieceTokenizer.tokenize"], ["", "def", "__call__", "(", "self", ",", "line", ")", ":", "\n", "        ", "id_", "=", "line", "[", "0", "]", "\n", "inputs", "=", "line", "[", "1", ":", "-", "1", "]", "# list of text strings", "\n", "label", "=", "line", "[", "-", "1", "]", "\n", "label", "=", "convert_to_unicode", "(", "label", ")", "\n", "label_id", "=", "self", ".", "_label_map", "[", "label", "]", "\n", "label_id", "=", "np", ".", "array", "(", "[", "label_id", "]", ",", "dtype", "=", "'int32'", ")", "\n", "input_ids", "=", "[", "self", ".", "_vocab", "(", "self", ".", "_tokenizer", ".", "tokenize", "(", "s", ")", "[", ":", "self", ".", "max_seq_length", "]", ")", "for", "s", "in", "inputs", "]", "\n", "valid_lengths", "=", "[", "len", "(", "s", ")", "for", "s", "in", "input_ids", "]", "\n", "return", "id_", ",", "input_ids", ",", "valid_lengths", ",", "label_id", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.dataset.ClassificationTransform.__init__": [[918, 924], ["enumerate", "dataset.BERTTransform"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "tokenizer", ",", "labels", ",", "max_seq_length", ",", "pad", "=", "True", ",", "pair", "=", "True", ")", ":", "\n", "        ", "self", ".", "_label_map", "=", "{", "}", "\n", "for", "(", "i", ",", "label", ")", "in", "enumerate", "(", "labels", ")", ":", "\n", "            ", "self", ".", "_label_map", "[", "label", "]", "=", "i", "\n", "", "self", ".", "_bert_xform", "=", "BERTTransform", "(", "\n", "tokenizer", ",", "max_seq_length", ",", "pad", "=", "pad", ",", "pair", "=", "pair", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.dataset.ClassificationTransform.__call__": [[925, 985], ["convert_to_unicode", "numpy.array", "dataset.ClassificationTransform._bert_xform"], "methods", ["home.repos.pwc.inspect_result.hhexiy_debiased.src.tokenizer.convert_to_unicode"], ["", "def", "__call__", "(", "self", ",", "line", ")", ":", "\n", "        ", "\"\"\"Perform transformation for sequence pairs or single sequences.\n\n        The transformation is processed in the following steps:\n        - tokenize the input sequences\n        - insert [CLS], [SEP] as necessary\n        - generate type ids to indicate whether a token belongs to the first\n          sequence or the second sequence.\n        - generate valid length\n\n        For sequence pairs, the input is a tuple of 3 strings:\n        text_a, text_b and label.\n\n        Inputs:\n            text_a: 'is this jacksonville ?'\n            text_b: 'no it is not'\n            label: '0'\n        Tokenization:\n            text_a: 'is this jack ##son ##ville ?'\n            text_b: 'no it is not .'\n        Processed:\n            tokens:  '[CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]'\n            type_ids: 0     0  0    0    0     0       0 0     1  1  1  1   1 1\n            valid_length: 14\n            label: 0\n\n        For single sequences, the input is a tuple of 2 strings: text_a and label.\n        Inputs:\n            text_a: 'the dog is hairy .'\n            label: '1'\n        Tokenization:\n            text_a: 'the dog is hairy .'\n        Processed:\n            text_a:  '[CLS] the dog is hairy . [SEP]'\n            type_ids: 0     0   0   0  0     0 0\n            valid_length: 7\n            label: 1\n\n        Parameters\n        ----------\n        line: tuple of str\n            Input strings. For sequence pairs, the input is a tuple of 3 strings:\n            (text_a, text_b, label). For single sequences, the input is a tuple\n            of 2 strings: (text_a, label).\n\n        Returns\n        -------\n        np.array: input token ids in 'int32', shape (batch_size, seq_length)\n        np.array: valid length in 'int32', shape (batch_size,)\n        np.array: input token type ids in 'int32', shape (batch_size, seq_length)\n        np.array: label id in 'int32', shape (batch_size, 1)\n        \"\"\"", "\n", "id_", "=", "line", "[", "0", "]", "\n", "line", "=", "line", "[", "1", ":", "]", "\n", "label", "=", "line", "[", "-", "1", "]", "\n", "label", "=", "convert_to_unicode", "(", "label", ")", "\n", "label_id", "=", "self", ".", "_label_map", "[", "label", "]", "\n", "label_id", "=", "np", ".", "array", "(", "[", "label_id", "]", ",", "dtype", "=", "'int32'", ")", "\n", "input_ids", ",", "valid_length", ",", "segment_ids", "=", "self", ".", "_bert_xform", "(", "line", "[", ":", "-", "1", "]", ")", "\n", "return", "id_", ",", "input_ids", ",", "valid_length", ",", "segment_ids", ",", "label_id", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.dataset.ClassificationTransform.get_length": [[986, 988], ["None"], "methods", ["None"], ["", "def", "get_length", "(", "self", ",", "*", "data", ")", ":", "\n", "        ", "return", "data", "[", "2", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.dataset.ClassificationTransform.get_batcher": [[989, 995], ["gluonnlp.data.batchify.Tuple", "gluonnlp.data.batchify.Stack", "gluonnlp.data.batchify.Pad", "gluonnlp.data.batchify.Stack", "gluonnlp.data.batchify.Pad", "gluonnlp.data.batchify.Stack"], "methods", ["None"], ["", "def", "get_batcher", "(", "self", ")", ":", "\n", "        ", "batchify_fn", "=", "nlp", ".", "data", ".", "batchify", ".", "Tuple", "(", "\n", "nlp", ".", "data", ".", "batchify", ".", "Stack", "(", ")", ",", "\n", "nlp", ".", "data", ".", "batchify", ".", "Pad", "(", "axis", "=", "0", ")", ",", "nlp", ".", "data", ".", "batchify", ".", "Stack", "(", ")", ",", "\n", "nlp", ".", "data", ".", "batchify", ".", "Pad", "(", "axis", "=", "0", ")", ",", "nlp", ".", "data", ".", "batchify", ".", "Stack", "(", ")", ")", "\n", "return", "batchify_fn", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.dataset.SNLICheatTransform.__init__": [[998, 1003], ["random.Random"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "labels", ",", "rate", "=", "1.", ",", "remove", "=", "False", ")", ":", "\n", "        ", "self", ".", "rate", "=", "rate", "\n", "self", ".", "labels", "=", "labels", "\n", "self", ".", "rng", "=", "random", ".", "Random", "(", "42", ")", "\n", "self", ".", "remove", "=", "remove", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.dataset.SNLICheatTransform.__call__": [[1004, 1014], ["dataset.SNLICheatTransform.rng.random", "dataset.SNLICheatTransform.rng.choice"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "line", ")", ":", "\n", "        ", "id_", ",", "premise", ",", "hypothesis", ",", "label", "=", "line", "[", "0", "]", ",", "line", "[", "1", "]", ",", "line", "[", "2", "]", ",", "line", "[", "3", "]", "\n", "if", "self", ".", "rng", ".", "random", "(", ")", "<", "self", ".", "rate", ":", "\n", "            ", "label", "=", "label", "\n", "if", "self", ".", "remove", ":", "\n", "                ", "return", "None", "\n", "", "", "else", ":", "\n", "            ", "label", "=", "self", ".", "rng", ".", "choice", "(", "self", ".", "labels", ")", "\n", "", "line", "[", "2", "]", "=", "'{} and {}'", ".", "format", "(", "label", ",", "hypothesis", ")", "\n", "return", "line", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.dataset.SNLICheatTransform.reset": [[1015, 1017], ["dataset.SNLICheatTransform.rng.seed"], "methods", ["None"], ["", "def", "reset", "(", "self", ")", ":", "\n", "        ", "self", ".", "rng", ".", "seed", "(", "42", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.dataset.SNLIWordDropTransform.__init__": [[1020, 1024], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "rate", "=", "0.", ",", "region", "=", "(", "'premise'", ",", "'hypothesis'", ")", ",", "tokenizer", "=", "str", ".", "split", ")", ":", "\n", "        ", "self", ".", "rate", "=", "rate", "\n", "self", ".", "region", "=", "region", "\n", "self", ".", "tokenizer", "=", "tokenizer", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.dataset.SNLIWordDropTransform.dropout": [[1025, 1029], ["numpy.random.binomial", "len", "zip"], "methods", ["None"], ["", "def", "dropout", "(", "self", ",", "seq", ")", ":", "\n", "        ", "mask", "=", "np", ".", "random", ".", "binomial", "(", "n", "=", "1", ",", "p", "=", "1", "-", "self", ".", "rate", ",", "size", "=", "len", "(", "seq", ")", ")", "\n", "seq", "=", "[", "s", "for", "m", ",", "s", "in", "zip", "(", "mask", ",", "seq", ")", "if", "m", "==", "1", "]", "\n", "return", "seq", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.dataset.SNLIWordDropTransform.__call__": [[1030, 1038], ["dataset.SNLIWordDropTransform.dropout", "dataset.SNLIWordDropTransform.dropout", "dataset.SNLIWordDropTransform.tokenizer", "dataset.SNLIWordDropTransform.tokenizer"], "methods", ["home.repos.pwc.inspect_result.hhexiy_debiased.src.dataset.SNLIWordDropTransform.dropout", "home.repos.pwc.inspect_result.hhexiy_debiased.src.dataset.SNLIWordDropTransform.dropout"], ["", "def", "__call__", "(", "self", ",", "line", ")", ":", "\n", "        ", "idx", ",", "premise", ",", "hypothesis", ",", "label", "=", "line", "[", "0", "]", ",", "line", "[", "1", "]", ",", "line", "[", "2", "]", ",", "line", "[", "3", "]", "\n", "if", "'premise'", "in", "self", ".", "region", ":", "\n", "            ", "premise", "=", "' '", ".", "join", "(", "self", ".", "dropout", "(", "self", ".", "tokenizer", "(", "premise", ")", ")", ")", "\n", "", "if", "'hypothesis'", "in", "self", ".", "region", ":", "\n", "            ", "hypothesis", "=", "' '", ".", "join", "(", "self", ".", "dropout", "(", "self", ".", "tokenizer", "(", "hypothesis", ")", ")", ")", "\n", "", "line", "=", "[", "idx", ",", "premise", ",", "hypothesis", ",", "label", "]", "\n", "return", "line", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.dataset.NLIHandcraftedTransform.__init__": [[1043, 1049], ["enumerate"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "labels", ",", "tokenizer", ",", "vocab", ")", ":", "\n", "        ", "self", ".", "_label_map", "=", "{", "}", "\n", "for", "(", "i", ",", "label", ")", "in", "enumerate", "(", "labels", ")", ":", "\n", "            ", "self", ".", "_label_map", "[", "label", "]", "=", "i", "\n", "", "self", ".", "_vocab", "=", "vocab", "\n", "self", ".", "_tokenizer", "=", "tokenizer", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.dataset.NLIHandcraftedTransform.__call__": [[1050, 1074], ["convert_to_unicode", "numpy.array", "len", "dataset.NLIHandcraftedTransform._tokenizer.tokenize", "abs", "len", "float", "dataset.NLIHandcraftedTransform._vocab", "dataset.NLIHandcraftedTransform._vocab", "len", "len", "set().intersection", "len", "len", "len", "set", "set().union", "set", "set", "set"], "methods", ["home.repos.pwc.inspect_result.hhexiy_debiased.src.tokenizer.convert_to_unicode", "home.repos.pwc.inspect_result.hhexiy_debiased.src.tokenizer.WordpieceTokenizer.tokenize"], ["", "def", "__call__", "(", "self", ",", "line", ")", ":", "\n", "        ", "id_", "=", "line", "[", "0", "]", "\n", "inputs", "=", "line", "[", "1", ":", "-", "1", "]", "# list of text strings", "\n", "label", "=", "line", "[", "-", "1", "]", "\n", "label", "=", "convert_to_unicode", "(", "label", ")", "\n", "label_id", "=", "self", ".", "_label_map", "[", "label", "]", "\n", "label_id", "=", "np", ".", "array", "(", "[", "label_id", "]", ",", "dtype", "=", "'int32'", ")", "\n", "\n", "assert", "len", "(", "inputs", ")", "==", "2", "\n", "prem", ",", "hypo", "=", "[", "self", ".", "_tokenizer", ".", "tokenize", "(", "s", ")", "for", "s", "in", "inputs", "]", "\n", "len_diff", "=", "abs", "(", "len", "(", "hypo", ")", "-", "len", "(", "prem", ")", ")", "/", "(", "len", "(", "hypo", ")", "+", "len", "(", "prem", ")", ")", "\n", "negation", "=", "1", "if", "(", "'not'", "in", "hypo", "or", "\"n't\"", "in", "hypo", ")", "else", "0", "\n", "jaccard_sim", "=", "len", "(", "set", "(", "prem", ")", ".", "intersection", "(", "set", "(", "hypo", ")", ")", ")", "/", "float", "(", "len", "(", "set", "(", "prem", ")", ".", "union", "(", "set", "(", "hypo", ")", ")", ")", ")", "\n", "overlap_tokens", "=", "[", "w", "for", "w", "in", "hypo", "if", "w", "in", "prem", "]", "\n", "non_overlap_tokens", "=", "[", "w", "for", "w", "in", "hypo", "if", "not", "w", "in", "prem", "]", "\n", "if", "not", "overlap_tokens", ":", "\n", "            ", "overlap_tokens", "=", "[", "'<empty>'", "]", "\n", "", "if", "not", "non_overlap_tokens", ":", "\n", "            ", "non_overlap_tokens", "=", "[", "'<empty>'", "]", "\n", "", "overlap_token_ids", "=", "[", "self", ".", "_vocab", "(", "w", ")", "for", "w", "in", "overlap_tokens", "]", "\n", "non_overlap_token_ids", "=", "[", "self", ".", "_vocab", "(", "w", ")", "for", "w", "in", "non_overlap_tokens", "]", "\n", "dense_features", "=", "[", "len_diff", ",", "negation", ",", "jaccard_sim", "]", "\n", "\n", "return", "id_", ",", "dense_features", ",", "overlap_token_ids", ",", "non_overlap_token_ids", ",", "label_id", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.dataset.NLIHandcraftedTransform.get_length": [[1075, 1077], ["len", "len", "len"], "methods", ["None"], ["", "def", "get_length", "(", "self", ",", "*", "data", ")", ":", "\n", "        ", "return", "len", "(", "data", "[", "1", "]", ")", "+", "len", "(", "data", "[", "2", "]", ")", "+", "len", "(", "data", "[", "3", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.dataset.NLIHandcraftedTransform.get_batcher": [[1078, 1086], ["gluonnlp.data.batchify.Tuple", "gluonnlp.data.batchify.Stack", "gluonnlp.data.batchify.Stack", "gluonnlp.data.batchify.Pad", "gluonnlp.data.batchify.Pad", "gluonnlp.data.batchify.Stack"], "methods", ["None"], ["", "def", "get_batcher", "(", "self", ")", ":", "\n", "        ", "pad_val", "=", "self", ".", "_vocab", "[", "self", ".", "_vocab", ".", "padding_token", "]", "\n", "batchify_fn", "=", "nlp", ".", "data", ".", "batchify", ".", "Tuple", "(", "\n", "nlp", ".", "data", ".", "batchify", ".", "Stack", "(", ")", ",", "nlp", ".", "data", ".", "batchify", ".", "Stack", "(", ")", ",", "\n", "nlp", ".", "data", ".", "batchify", ".", "Pad", "(", "axis", "=", "0", ",", "pad_val", "=", "pad_val", ")", ",", "\n", "nlp", ".", "data", ".", "batchify", ".", "Pad", "(", "axis", "=", "0", ",", "pad_val", "=", "pad_val", ")", ",", "\n", "nlp", ".", "data", ".", "batchify", ".", "Stack", "(", ")", ")", "\n", "return", "batchify_fn", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.dataset.NLIHypothesisTransform.__init__": [[1089, 1095], ["enumerate", "dataset.BERTTransform"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "tokenizer", ",", "labels", ",", "max_seq_length", ",", "pad", "=", "True", ")", ":", "\n", "        ", "self", ".", "_label_map", "=", "{", "}", "\n", "for", "(", "i", ",", "label", ")", "in", "enumerate", "(", "labels", ")", ":", "\n", "            ", "self", ".", "_label_map", "[", "label", "]", "=", "i", "\n", "", "self", ".", "_bert_xform", "=", "BERTTransform", "(", "\n", "tokenizer", ",", "max_seq_length", ",", "pad", "=", "pad", ",", "pair", "=", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.dataset.NLIHypothesisTransform.__call__": [[1096, 1107], ["convert_to_unicode", "numpy.array", "dataset.NLIHypothesisTransform._bert_xform"], "methods", ["home.repos.pwc.inspect_result.hhexiy_debiased.src.tokenizer.convert_to_unicode"], ["", "def", "__call__", "(", "self", ",", "line", ")", ":", "\n", "        ", "id_", "=", "line", "[", "0", "]", "\n", "line", "=", "line", "[", "1", ":", "]", "\n", "# Ignore premise (sentence 0)", "\n", "line", "=", "line", "[", "1", ":", "]", "\n", "label", "=", "line", "[", "-", "1", "]", "\n", "label", "=", "convert_to_unicode", "(", "label", ")", "\n", "label_id", "=", "self", ".", "_label_map", "[", "label", "]", "\n", "label_id", "=", "np", ".", "array", "(", "[", "label_id", "]", ",", "dtype", "=", "'int32'", ")", "\n", "input_ids", ",", "valid_length", ",", "segment_ids", "=", "self", ".", "_bert_xform", "(", "line", "[", ":", "-", "1", "]", ")", "\n", "return", "id_", ",", "input_ids", ",", "valid_length", ",", "segment_ids", ",", "label_id", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.dataset.NLIHypothesisTransform.get_length": [[1108, 1110], ["None"], "methods", ["None"], ["", "def", "get_length", "(", "self", ",", "*", "data", ")", ":", "\n", "        ", "return", "data", "[", "2", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.dataset.NLIHypothesisTransform.get_batcher": [[1111, 1117], ["gluonnlp.data.batchify.Tuple", "gluonnlp.data.batchify.Stack", "gluonnlp.data.batchify.Pad", "gluonnlp.data.batchify.Stack", "gluonnlp.data.batchify.Pad", "gluonnlp.data.batchify.Stack"], "methods", ["None"], ["", "def", "get_batcher", "(", "self", ")", ":", "\n", "        ", "batchify_fn", "=", "nlp", ".", "data", ".", "batchify", ".", "Tuple", "(", "\n", "nlp", ".", "data", ".", "batchify", ".", "Stack", "(", ")", ",", "\n", "nlp", ".", "data", ".", "batchify", ".", "Pad", "(", "axis", "=", "0", ")", ",", "nlp", ".", "data", ".", "batchify", ".", "Stack", "(", ")", ",", "\n", "nlp", ".", "data", ".", "batchify", ".", "Pad", "(", "axis", "=", "0", ")", ",", "nlp", ".", "data", ".", "batchify", ".", "Stack", "(", ")", ")", "\n", "return", "batchify_fn", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.dataset.RegressionTransform.__init__": [[1134, 1137], ["dataset.BERTTransform"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "tokenizer", ",", "max_seq_length", ",", "pad", "=", "True", ",", "pair", "=", "True", ")", ":", "\n", "        ", "self", ".", "_bert_xform", "=", "BERTTransform", "(", "\n", "tokenizer", ",", "max_seq_length", ",", "pad", "=", "pad", ",", "pair", "=", "pair", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.dataset.RegressionTransform.__call__": [[1138, 1194], ["numpy.array", "dataset.RegressionTransform._bert_xform"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "line", ")", ":", "\n", "        ", "\"\"\"Perform transformation for sequence pairs or single sequences.\n\n        The transformation is processed in the following steps:\n        - tokenize the input sequences\n        - insert [CLS], [SEP] as necessary\n        - generate type ids to indicate whether a token belongs to the first\n          sequence or the second sequence.\n        - generate valid length\n\n        For sequence pairs, the input is a tuple of 3 strings:\n        text_a, text_b and label.\n\n        Inputs:\n            text_a: 'is this jacksonville ?'\n            text_b: 'no it is not'\n            label: '0'\n        Tokenization:\n            text_a: 'is this jack ##son ##ville ?'\n            text_b: 'no it is not .'\n        Processed:\n            tokens:  '[CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]'\n            type_ids: 0     0  0    0    0     0       0 0     1  1  1  1   1 1\n            valid_length: 14\n            label: 0\n\n        For single sequences, the input is a tuple of 2 strings: text_a and label.\n        Inputs:\n            text_a: 'the dog is hairy .'\n            label: '1'\n        Tokenization:\n            text_a: 'the dog is hairy .'\n        Processed:\n            text_a:  '[CLS] the dog is hairy . [SEP]'\n            type_ids: 0     0   0   0  0     0 0\n            valid_length: 7\n            label: 1\n\n        Parameters\n        ----------\n        line: tuple of str\n            Input strings. For sequence pairs, the input is a tuple of 3 strings:\n            (text_a, text_b, score). For single sequences, the input is a tuple\n            of 2 strings: (text_a, score).\n\n        Returns\n        -------\n        np.array: input token ids in 'int32', shape (batch_size, seq_length)\n        np.array: valid length in 'int32', shape (batch_size,)\n        np.array: input token type ids in 'int32', shape (batch_size, seq_length)\n        np.array: score in 'float32', shape (batch_size, 1)\n        \"\"\"", "\n", "score", "=", "line", "[", "-", "1", "]", "\n", "scroe_np", "=", "np", ".", "array", "(", "[", "score", "]", ",", "dtype", "=", "'float32'", ")", "\n", "input_ids", ",", "valid_length", ",", "segment_ids", "=", "self", ".", "_bert_xform", "(", "line", "[", ":", "-", "1", "]", ")", "\n", "return", "input_ids", ",", "valid_length", ",", "segment_ids", ",", "scroe_np", "\n", "", "", ""]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.dataset._truncate_seq_pair": [[685, 699], ["len", "len", "len", "len", "tokens_a.pop", "tokens_b.pop"], "function", ["None"], ["", "", "def", "_truncate_seq_pair", "(", "tokens_a", ",", "tokens_b", ",", "max_length", ")", ":", "\n", "    ", "\"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"", "\n", "# This is a simple heuristic which will always truncate the longer sequence", "\n", "# one token at a time. This makes more sense than truncating an equal percent", "\n", "# of tokens from each, since if one sequence is very short then each token", "\n", "# that's truncated likely contains more information than a longer sequence.", "\n", "while", "True", ":", "\n", "        ", "total_length", "=", "len", "(", "tokens_a", ")", "+", "len", "(", "tokens_b", ")", "\n", "if", "total_length", "<=", "max_length", ":", "\n", "            ", "break", "\n", "", "if", "len", "(", "tokens_a", ")", ">", "len", "(", "tokens_b", ")", ":", "\n", "            ", "tokens_a", ".", "pop", "(", ")", "\n", "", "else", ":", "\n", "            ", "tokens_b", ".", "pop", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.utils.logging_config": [[30, 60], ["logging.getLogger", "logging.getLogger.setLevel", "logging.Formatter", "logging.getLogger.removeHandler", "print", "logging.FileHandler", "logging.FileHandler.setLevel", "logging.FileHandler.setFormatter", "logging.getLogger.addHandler", "logging.StreamHandler", "logging.StreamHandler.setLevel", "logging.StreamHandler.setFormatter", "logging.getLogger.addHandler"], "function", ["None"], ["def", "logging_config", "(", "logpath", "=", "None", ",", "\n", "level", "=", "logging", ".", "DEBUG", ",", "\n", "console_level", "=", "logging", ".", "INFO", ",", "\n", "no_console", "=", "False", ")", ":", "\n", "    ", "\"\"\"\n    Config the logging.\n    \"\"\"", "\n", "logger", "=", "logging", ".", "getLogger", "(", "'nli'", ")", "\n", "# Remove all the current handlers", "\n", "for", "handler", "in", "logger", ".", "handlers", ":", "\n", "        ", "logger", ".", "removeHandler", "(", "handler", ")", "\n", "", "logger", ".", "handlers", "=", "[", "]", "\n", "logger", ".", "propagate", "=", "False", "\n", "logger", ".", "setLevel", "(", "logging", ".", "DEBUG", ")", "\n", "\n", "formatter", "=", "logging", ".", "Formatter", "(", "'%(filename)s:%(funcName)s: %(message)s'", ")", "\n", "\n", "if", "logpath", "is", "not", "None", ":", "\n", "        ", "print", "(", "'All Logs will be saved to {}'", ".", "format", "(", "logpath", ")", ")", "\n", "logfile", "=", "logging", ".", "FileHandler", "(", "logpath", ",", "mode", "=", "'w'", ")", "\n", "logfile", ".", "setLevel", "(", "level", ")", "\n", "logfile", ".", "setFormatter", "(", "formatter", ")", "\n", "logger", ".", "addHandler", "(", "logfile", ")", "\n", "\n", "", "if", "not", "no_console", ":", "\n", "# Initialze the console logging", "\n", "        ", "logconsole", "=", "logging", ".", "StreamHandler", "(", ")", "\n", "logconsole", ".", "setLevel", "(", "console_level", ")", "\n", "logconsole", ".", "setFormatter", "(", "formatter", ")", "\n", "logger", ".", "addHandler", "(", "logconsole", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.utils.get_dir": [[61, 65], ["os.path.exists", "os.makedirs"], "function", ["None"], ["", "", "def", "get_dir", "(", "path", ")", ":", "\n", "    ", "if", "not", "os", ".", "path", ".", "exists", "(", "path", ")", ":", "\n", "        ", "os", ".", "makedirs", "(", "path", ")", "\n", "", "return", "path", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.utils.metric_to_dict": [[66, 72], ["metric.get", "isinstance", "zip"], "function", ["None"], ["", "def", "metric_to_dict", "(", "metric", ")", ":", "\n", "    ", "metric_name", ",", "metric_val", "=", "metric", ".", "get", "(", ")", "\n", "if", "not", "isinstance", "(", "metric_name", ",", "list", ")", ":", "\n", "        ", "metric_name", "=", "[", "metric_name", "]", "\n", "metric_val", "=", "[", "metric_val", "]", "\n", "", "return", "{", "name", ":", "val", "for", "name", ",", "val", "in", "zip", "(", "metric_name", ",", "metric_val", ")", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.utils.metric_dict_to_str": [[73, 78], ["sorted", "metrics.keys"], "function", ["None"], ["", "def", "metric_dict_to_str", "(", "metrics", ")", ":", "\n", "    ", "metric_names", "=", "sorted", "(", "metrics", ".", "keys", "(", ")", ")", "\n", "s", "=", "','", ".", "join", "(", "[", "i", "+", "':{:.4f}'", "for", "i", "in", "metric_names", "]", ")", ".", "format", "(", "\n", "*", "(", "metrics", "[", "name", "]", "for", "name", "in", "metric_names", ")", ")", "\n", "return", "s", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.src.utils.read_args": [[79, 83], ["argparse.Namespace", "json.load", "open", "os.path.join"], "function", ["None"], ["", "def", "read_args", "(", "path", ")", ":", "\n", "    ", "config", "=", "json", ".", "load", "(", "open", "(", "os", ".", "path", ".", "join", "(", "path", ",", "'report.json'", ")", ")", ")", "[", "'config'", "]", "\n", "args", "=", "argparse", ".", "Namespace", "(", "**", "config", ")", "\n", "return", "args", "\n", "", ""]], "home.repos.pwc.inspect_result.hhexiy_debiased.model.decomposable_attention.DecomposableAttentionClassifier.__init__": [[38, 56], ["mxnet.gluon.HybridBlock.__init__", "decomposable_attention.DecomposableAttentionClassifier.name_scope", "mxnet.gluon.nn.Dropout", "mxnet.gluon.nn.Embedding", "decomposable_attention.DecomposableAttention", "decomposable_attention.IntraSentenceAttention"], "methods", ["home.repos.pwc.inspect_result.hhexiy_debiased.model.esim.ESIMClassifier.__init__"], ["def", "__init__", "(", "self", ",", "vocab_size", ",", "num_classes", ",", "word_embed_size", ",", "hidden_size", ",", "\n", "dropout", "=", "0.", ",", "intra_attention", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "**", "kwargs", ")", "\n", "self", ".", "word_embed_size", "=", "word_embed_size", "\n", "self", ".", "hidden_size", "=", "hidden_size", "\n", "self", ".", "use_intra_attention", "=", "intra_attention", "\n", "with", "self", ".", "name_scope", "(", ")", ":", "\n", "            ", "self", ".", "dropout_layer", "=", "nn", ".", "Dropout", "(", "dropout", ")", "\n", "self", ".", "embedding", "=", "nn", ".", "Embedding", "(", "vocab_size", ",", "word_embed_size", ")", "\n", "#self.lin_proj = nn.Dense(hidden_size, in_units=word_embed_size,", "\n", "#                         flatten=False, use_bias=False)", "\n", "if", "self", ".", "use_intra_attention", ":", "\n", "                ", "self", ".", "intra_attention", "=", "IntraSentenceAttention", "(", "hidden_size", ",", "hidden_size", ",", "dropout", ")", "\n", "input_size", "=", "hidden_size", "*", "2", "\n", "", "else", ":", "\n", "                ", "self", ".", "intra_attention", "=", "None", "\n", "input_size", "=", "hidden_size", "\n", "", "self", ".", "model", "=", "DecomposableAttention", "(", "input_size", ",", "hidden_size", ",", "num_classes", ",", "dropout", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.model.decomposable_attention.DecomposableAttentionClassifier.hybrid_forward": [[57, 83], ["decomposable_attention.DecomposableAttentionClassifier.embedding", "decomposable_attention.DecomposableAttentionClassifier.embedding", "decomposable_attention.DecomposableAttentionClassifier.model", "F.concat", "F.concat", "decomposable_attention.DecomposableAttentionClassifier.intra_attention", "decomposable_attention.DecomposableAttentionClassifier.intra_attention"], "methods", ["None"], ["", "", "def", "hybrid_forward", "(", "self", ",", "F", ",", "sentence1", ",", "sentence2", ",", "valid_length1", ",", "valid_length2", ")", ":", "\n", "        ", "\"\"\"\n        Predict the relation of two sentences.\n\n        Parameters\n        ----------\n        sentence1 : NDArray\n            Shape (batch_size, length)\n        sentence2 : NDArray\n            Shape (batch_size, length)\n\n        Returns\n        -------\n        pred : NDArray\n            Shape (batch_size, num_classes). num_classes == 3.\n\n        \"\"\"", "\n", "#feature1 = self.lin_proj(self.embedding(sentence1))", "\n", "#feature2 = self.lin_proj(self.embedding(sentence2))", "\n", "feature1", "=", "self", ".", "embedding", "(", "sentence1", ")", "\n", "feature2", "=", "self", ".", "embedding", "(", "sentence2", ")", "\n", "if", "self", ".", "use_intra_attention", ":", "\n", "            ", "feature1", "=", "F", ".", "concat", "(", "feature1", ",", "self", ".", "intra_attention", "(", "feature1", ")", ",", "dim", "=", "-", "1", ")", "\n", "feature2", "=", "F", ".", "concat", "(", "feature2", ",", "self", ".", "intra_attention", "(", "feature2", ")", ",", "dim", "=", "-", "1", ")", "\n", "", "pred", "=", "self", ".", "model", "(", "feature1", ",", "feature2", ",", "valid_length1", ",", "valid_length2", ")", "\n", "return", "pred", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.model.decomposable_attention.IntraSentenceAttention.__init__": [[88, 101], ["mxnet.gluon.HybridBlock.__init__", "decomposable_attention.IntraSentenceAttention.name_scope", "mxnet.gluon.nn.Dropout", "mxnet.gluon.nn.HybridSequential", "decomposable_attention.IntraSentenceAttention.intra_attn_emb.add", "decomposable_attention.IntraSentenceAttention.intra_attn_emb.add", "decomposable_attention.IntraSentenceAttention.intra_attn_emb.add", "decomposable_attention.IntraSentenceAttention.intra_attn_emb.add", "mxnet.gluon.nn.Dense", "mxnet.gluon.nn.Dense"], "methods", ["home.repos.pwc.inspect_result.hhexiy_debiased.model.esim.ESIMClassifier.__init__"], ["def", "__init__", "(", "self", ",", "inp_size", ",", "hidden_size", ",", "dropout", "=", "0.", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "**", "kwargs", ")", "\n", "self", ".", "hidden_size", "=", "hidden_size", "\n", "with", "self", ".", "name_scope", "(", ")", ":", "\n", "            ", "self", ".", "dropout_layer", "=", "nn", ".", "Dropout", "(", "dropout", ")", "\n", "# F_intra in the paper", "\n", "self", ".", "intra_attn_emb", "=", "nn", ".", "HybridSequential", "(", ")", "\n", "self", ".", "intra_attn_emb", ".", "add", "(", "self", ".", "dropout_layer", ")", "\n", "self", ".", "intra_attn_emb", ".", "add", "(", "nn", ".", "Dense", "(", "hidden_size", ",", "in_units", "=", "inp_size", ",", "\n", "activation", "=", "'relu'", ",", "flatten", "=", "False", ")", ")", "\n", "self", ".", "intra_attn_emb", ".", "add", "(", "self", ".", "dropout_layer", ")", "\n", "self", ".", "intra_attn_emb", ".", "add", "(", "nn", ".", "Dense", "(", "hidden_size", ",", "in_units", "=", "hidden_size", ",", "\n", "activation", "=", "'relu'", ",", "flatten", "=", "False", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.model.decomposable_attention.IntraSentenceAttention.hybrid_forward": [[102, 120], ["decomposable_attention.IntraSentenceAttention.intra_attn_emb", "F.batch_dot", "F.batch_dot", "F.batch_dot.softmax"], "methods", ["None"], ["", "", "def", "hybrid_forward", "(", "self", ",", "F", ",", "feature_a", ")", ":", "\n", "        ", "\"\"\"\n        Compute intra-sentence attention given embedded words.\n\n        Parameters\n        ----------\n        feature_a : NDArray\n            Shape (batch_size, length, hidden_size)\n\n        Returns\n        -------\n        alpha : NDArray\n            Shape (batch_size, length, hidden_size)\n        \"\"\"", "\n", "tilde_a", "=", "self", ".", "intra_attn_emb", "(", "feature_a", ")", "\n", "e_matrix", "=", "F", ".", "batch_dot", "(", "tilde_a", ",", "tilde_a", ",", "transpose_b", "=", "True", ")", "\n", "alpha", "=", "F", ".", "batch_dot", "(", "e_matrix", ".", "softmax", "(", ")", ",", "tilde_a", ")", "\n", "return", "alpha", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.model.decomposable_attention.DecomposableAttention.__init__": [[125, 139], ["mxnet.gluon.HybridBlock.__init__", "decomposable_attention.DecomposableAttention.name_scope", "mxnet.gluon.nn.Dropout", "decomposable_attention.DecomposableAttention._ff_layer", "decomposable_attention.DecomposableAttention._ff_layer", "decomposable_attention.DecomposableAttention.h.add", "mxnet.gluon.nn.Dense"], "methods", ["home.repos.pwc.inspect_result.hhexiy_debiased.model.esim.ESIMClassifier.__init__", "home.repos.pwc.inspect_result.hhexiy_debiased.model.decomposable_attention.DecomposableAttention._ff_layer", "home.repos.pwc.inspect_result.hhexiy_debiased.model.decomposable_attention.DecomposableAttention._ff_layer"], ["def", "__init__", "(", "self", ",", "inp_size", ",", "hidden_size", ",", "num_class", ",", "dropout", "=", "0.", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "**", "kwargs", ")", "\n", "with", "self", ".", "name_scope", "(", ")", ":", "\n", "            ", "self", ".", "dropout_layer", "=", "nn", ".", "Dropout", "(", "dropout", ")", "\n", "# attention function", "\n", "#self.f = self._ff_layer(in_units=inp_size, out_units=hidden_size, flatten=False)", "\n", "# compare function", "\n", "self", ".", "g", "=", "self", ".", "_ff_layer", "(", "in_units", "=", "hidden_size", "*", "2", ",", "out_units", "=", "hidden_size", ",", "flatten", "=", "False", ")", "\n", "# predictor", "\n", "self", ".", "h", "=", "self", ".", "_ff_layer", "(", "in_units", "=", "hidden_size", "*", "2", ",", "out_units", "=", "hidden_size", ",", "flatten", "=", "True", ")", "\n", "self", ".", "h", ".", "add", "(", "nn", ".", "Dense", "(", "num_class", ",", "in_units", "=", "hidden_size", ")", ")", "\n", "# extract features", "\n", "", "self", ".", "hidden_size", "=", "hidden_size", "\n", "self", ".", "inp_size", "=", "inp_size", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.model.decomposable_attention.DecomposableAttention._ff_layer": [[140, 147], ["mxnet.gluon.nn.HybridSequential", "mxnet.gluon.nn.HybridSequential.add", "mxnet.gluon.nn.HybridSequential.add", "mxnet.gluon.nn.HybridSequential.add", "mxnet.gluon.nn.HybridSequential.add", "mxnet.gluon.nn.Dense", "mxnet.gluon.nn.Dense"], "methods", ["None"], ["", "def", "_ff_layer", "(", "self", ",", "in_units", ",", "out_units", ",", "flatten", "=", "True", ")", ":", "\n", "        ", "m", "=", "nn", ".", "HybridSequential", "(", ")", "\n", "m", ".", "add", "(", "self", ".", "dropout_layer", ")", "\n", "m", ".", "add", "(", "nn", ".", "Dense", "(", "out_units", ",", "in_units", "=", "in_units", ",", "activation", "=", "'relu'", ",", "flatten", "=", "flatten", ")", ")", "\n", "m", ".", "add", "(", "self", ".", "dropout_layer", ")", "\n", "m", ".", "add", "(", "nn", ".", "Dense", "(", "out_units", ",", "in_units", "=", "out_units", ",", "activation", "=", "'relu'", ",", "flatten", "=", "flatten", ")", ")", "\n", "return", "m", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.model.decomposable_attention.DecomposableAttention.hybrid_forward": [[148, 177], ["F.batch_dot", "F.batch_dot", "F.batch_dot", "decomposable_attention.DecomposableAttention.g", "decomposable_attention.DecomposableAttention.g", "feature1.sum.sum.sum", "feature2.sum.sum.sum", "decomposable_attention.DecomposableAttention.h", "F.batch_dot.softmax", "F.batch_dot.transpose().softmax", "F.concat", "F.concat", "F.concat", "F.batch_dot.transpose"], "methods", ["None"], ["", "def", "hybrid_forward", "(", "self", ",", "F", ",", "a", ",", "b", ",", "len_a", ",", "len_b", ")", ":", "\n", "        ", "\"\"\"\n        Forward of Decomposable Attention layer\n        \"\"\"", "\n", "# a.shape = [B, L1, H]", "\n", "# b.shape = [B, L2, H]", "\n", "# extract features", "\n", "#tilde_a = self.f(a)  # shape = [B, L1, H]", "\n", "#tilde_b = self.f(b)  # shape = [B, L2, H]", "\n", "tilde_a", "=", "a", "# shape = [B, L1, H]", "\n", "tilde_b", "=", "b", "# shape = [B, L2, H]", "\n", "# attention", "\n", "# e.shape = [B, L1, L2]", "\n", "e", "=", "F", ".", "batch_dot", "(", "tilde_a", ",", "tilde_b", ",", "transpose_b", "=", "True", ")", "\n", "## masking", "\n", "#e_mask_a = F.SequenceMask(e, sequence_length=len_a, use_sequence_length=True, axis=1, value=EPS)", "\n", "#e_mask_a_b = F.SequenceMask(e_mask_a.transpose([0, 2, 1]), sequence_length=len_b, use_sequence_length=True, axis=1, value=EPS).transpose([0, 2, 1])", "\n", "#e = e_mask_a_b", "\n", "# beta: b align to a, [B, L1, H]", "\n", "beta", "=", "F", ".", "batch_dot", "(", "e", ".", "softmax", "(", ")", ",", "tilde_b", ")", "\n", "# alpha: a align to b, [B, L2, H]", "\n", "alpha", "=", "F", ".", "batch_dot", "(", "e", ".", "transpose", "(", "[", "0", ",", "2", ",", "1", "]", ")", ".", "softmax", "(", ")", ",", "tilde_a", ")", "\n", "# compare", "\n", "feature1", "=", "self", ".", "g", "(", "F", ".", "concat", "(", "tilde_a", ",", "beta", ",", "dim", "=", "2", ")", ")", "\n", "feature2", "=", "self", ".", "g", "(", "F", ".", "concat", "(", "tilde_b", ",", "alpha", ",", "dim", "=", "2", ")", ")", "\n", "feature1", "=", "feature1", ".", "sum", "(", "axis", "=", "1", ")", "\n", "feature2", "=", "feature2", ".", "sum", "(", "axis", "=", "1", ")", "\n", "yhat", "=", "self", ".", "h", "(", "F", ".", "concat", "(", "feature1", ",", "feature2", ",", "dim", "=", "1", ")", ")", "\n", "return", "yhat", "\n", "", "", ""]], "home.repos.pwc.inspect_result.hhexiy_debiased.model.bert.BERTRegression.__init__": [[45, 53], ["mxnet.gluon.Block.__init__", "bert.BERTRegression.name_scope", "mxnet.gluon.nn.HybridSequential", "bert.BERTRegression.regression.add", "bert.BERTRegression.regression.add", "mxnet.gluon.nn.Dense", "mxnet.gluon.nn.Dropout"], "methods", ["home.repos.pwc.inspect_result.hhexiy_debiased.model.esim.ESIMClassifier.__init__"], ["def", "__init__", "(", "self", ",", "bert", ",", "dropout", "=", "0.0", ",", "prefix", "=", "None", ",", "params", "=", "None", ")", ":", "\n", "        ", "super", "(", "BERTRegression", ",", "self", ")", ".", "__init__", "(", "prefix", "=", "prefix", ",", "params", "=", "params", ")", "\n", "self", ".", "bert", "=", "bert", "\n", "with", "self", ".", "name_scope", "(", ")", ":", "\n", "            ", "self", ".", "regression", "=", "nn", ".", "HybridSequential", "(", "prefix", "=", "prefix", ")", "\n", "if", "dropout", ":", "\n", "                ", "self", ".", "regression", ".", "add", "(", "nn", ".", "Dropout", "(", "rate", "=", "dropout", ")", ")", "\n", "", "self", ".", "regression", ".", "add", "(", "nn", ".", "Dense", "(", "1", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.model.bert.BERTRegression.forward": [[54, 74], ["bert.BERTRegression.bert", "bert.BERTRegression.regression"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "inputs", ",", "token_types", ",", "valid_length", "=", "None", ")", ":", "# pylint: disable=arguments-differ", "\n", "        ", "\"\"\"Generate the unnormalized score for the given the input sequences.\n\n        Parameters\n        ----------\n        inputs : NDArray, shape (batch_size, seq_length)\n            Input words for the sequences.\n        token_types : NDArray, shape (batch_size, seq_length)\n            Token types for the sequences, used to indicate whether the word belongs to the\n            first sentence or the second one.\n        valid_length : NDArray or None, shape (batch_size)\n            Valid length of the sequence. This is used to mask the padded tokens.\n\n        Returns\n        -------\n        outputs : NDArray\n            Shape (batch_size, num_classes)\n        \"\"\"", "\n", "_", ",", "pooler_out", "=", "self", ".", "bert", "(", "inputs", ",", "token_types", ",", "valid_length", ")", "\n", "return", "self", ".", "regression", "(", "pooler_out", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.model.bert.BERTClassifier.__init__": [[96, 104], ["mxnet.gluon.Block.__init__", "bert.BERTClassifier.name_scope", "mxnet.gluon.nn.HybridSequential", "bert.BERTClassifier.classifier.add", "bert.BERTClassifier.classifier.add", "mxnet.gluon.nn.Dense", "mxnet.gluon.nn.Dropout"], "methods", ["home.repos.pwc.inspect_result.hhexiy_debiased.model.esim.ESIMClassifier.__init__"], ["def", "__init__", "(", "self", ",", "bert", ",", "num_classes", "=", "2", ",", "dropout", "=", "0.0", ",", "prefix", "=", "None", ",", "params", "=", "None", ")", ":", "\n", "        ", "super", "(", "BERTClassifier", ",", "self", ")", ".", "__init__", "(", "prefix", "=", "prefix", ",", "params", "=", "params", ")", "\n", "self", ".", "bert", "=", "bert", "\n", "with", "self", ".", "name_scope", "(", ")", ":", "\n", "            ", "self", ".", "classifier", "=", "nn", ".", "HybridSequential", "(", "prefix", "=", "prefix", ")", "\n", "if", "dropout", ":", "\n", "                ", "self", ".", "classifier", ".", "add", "(", "nn", ".", "Dropout", "(", "rate", "=", "dropout", ")", ")", "\n", "", "self", ".", "classifier", ".", "add", "(", "nn", ".", "Dense", "(", "units", "=", "num_classes", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.model.bert.BERTClassifier.initialize": [[105, 107], ["bert.BERTClassifier.classifier.initialize"], "methods", ["home.repos.pwc.inspect_result.hhexiy_debiased.model.additive.AdditiveClassifier.initialize"], ["", "", "def", "initialize", "(", "self", ",", "**", "kwargs", ")", ":", "\n", "        ", "self", ".", "classifier", ".", "initialize", "(", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.model.bert.BERTClassifier.forward": [[108, 128], ["bert.BERTClassifier.bert", "bert.BERTClassifier.classifier"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "inputs", ",", "token_types", ",", "valid_length", "=", "None", ")", ":", "# pylint: disable=arguments-differ", "\n", "        ", "\"\"\"Generate the unnormalized score for the given the input sequences.\n\n        Parameters\n        ----------\n        inputs : NDArray, shape (batch_size, seq_length)\n            Input words for the sequences.\n        token_types : NDArray, shape (batch_size, seq_length)\n            Token types for the sequences, used to indicate whether the word belongs to the\n            first sentence or the second one.\n        valid_length : NDArray or None, shape (batch_size)\n            Valid length of the sequence. This is used to mask the padded tokens.\n\n        Returns\n        -------\n        outputs : NDArray\n            Shape (batch_size, num_classes)\n        \"\"\"", "\n", "_", ",", "pooler_out", "=", "self", ".", "bert", "(", "inputs", ",", "token_types", ",", "valid_length", ")", "\n", "return", "self", ".", "classifier", "(", "pooler_out", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.hhexiy_debiased.model.additive.AdditiveClassifier.__init__": [[8, 14], ["mxnet.gluon.Block.__init__", "hasattr"], "methods", ["home.repos.pwc.inspect_result.hhexiy_debiased.model.esim.ESIMClassifier.__init__"], ["    ", "def", "__init__", "(", "self", ",", "classifier", ",", "mode", "=", "'all'", ",", "prefix", "=", "None", ",", "params", "=", "None", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "prefix", "=", "None", ",", "params", "=", "None", ")", "\n", "self", ".", "classifier", "=", "classifier", "\n", "self", ".", "mode", "=", "mode", "\n", "if", "hasattr", "(", "classifier", ",", "'embedding'", ")", ":", "\n", "            ", "self", ".", "embedding", "=", "classifier", ".", "embedding", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.model.additive.AdditiveClassifier.initialize": [[15, 17], ["additive.AdditiveClassifier.classifier.initialize"], "methods", ["home.repos.pwc.inspect_result.hhexiy_debiased.model.additive.AdditiveClassifier.initialize"], ["", "", "def", "initialize", "(", "self", ",", "**", "kwargs", ")", ":", "\n", "        ", "self", ".", "classifier", ".", "initialize", "(", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.model.additive.AdditiveClassifier.forward": [[18, 27], ["additive.AdditiveClassifier.classifier", "mxnet.nd.add_n"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "prev_scores", ",", "classifier_inputs", ")", ":", "\n", "        ", "scores", "=", "self", ".", "classifier", "(", "*", "classifier_inputs", ")", "\n", "if", "self", ".", "mode", "==", "'all'", ":", "\n", "            ", "outputs", "=", "mx", ".", "nd", ".", "add_n", "(", "prev_scores", ",", "scores", ")", "\n", "", "elif", "self", ".", "mode", "==", "'prev'", ":", "\n", "            ", "outputs", "=", "prev_scores", "\n", "", "else", ":", "\n", "            ", "outputs", "=", "scores", "\n", "", "return", "outputs", "\n", "", "", ""]], "home.repos.pwc.inspect_result.hhexiy_debiased.model.cbow.CBOWClassifier.__init__": [[7, 19], ["mxnet.gluon.Block.__init__", "cbow.CBOWClassifier.name_scope", "mxnet.gluon.nn.Embedding", "mxnet.gluon.nn.HybridSequential", "range", "cbow.CBOWClassifier.classifier.add", "cbow.CBOWClassifier.classifier.add", "cbow.CBOWClassifier.classifier.add", "mxnet.gluon.nn.Dense", "mxnet.gluon.nn.Dropout", "mxnet.gluon.nn.Dense", "cbow.CBOWClassifier.classifier.add", "mxnet.gluon.nn.Dropout"], "methods", ["home.repos.pwc.inspect_result.hhexiy_debiased.model.esim.ESIMClassifier.__init__"], ["    ", "def", "__init__", "(", "self", ",", "vocab_size", ",", "num_classes", ",", "embedding_dim", ",", "hid_dim", ",", "num_layers", ",", "dropout", "=", "0.0", ",", "prefix", "=", "None", ",", "params", "=", "None", ")", ":", "\n", "        ", "super", "(", "CBOWClassifier", ",", "self", ")", ".", "__init__", "(", "prefix", "=", "prefix", ",", "params", "=", "params", ")", "\n", "with", "self", ".", "name_scope", "(", ")", ":", "\n", "            ", "self", ".", "embedding", "=", "nn", ".", "Embedding", "(", "vocab_size", ",", "embedding_dim", ")", "\n", "self", ".", "classifier", "=", "nn", ".", "HybridSequential", "(", "prefix", "=", "prefix", ")", "\n", "if", "dropout", ":", "\n", "                ", "self", ".", "classifier", ".", "add", "(", "nn", ".", "Dropout", "(", "rate", "=", "dropout", ")", ")", "\n", "", "for", "_", "in", "range", "(", "num_layers", ")", ":", "\n", "                ", "self", ".", "classifier", ".", "add", "(", "nn", ".", "Dense", "(", "units", "=", "hid_dim", ",", "activation", "=", "'relu'", ")", ")", "\n", "if", "dropout", ":", "\n", "                    ", "self", ".", "classifier", ".", "add", "(", "nn", ".", "Dropout", "(", "rate", "=", "dropout", ")", ")", "\n", "", "", "self", ".", "classifier", ".", "add", "(", "nn", ".", "Dense", "(", "units", "=", "num_classes", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.model.cbow.CBOWClassifier.forward": [[20, 35], ["None"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "sentences", ",", "valid_lengths", "=", "None", ")", ":", "# pylint: disable=arguments-differ", "\n", "        ", "\"\"\"Classify sentences using sum of word embedding representations.\n\n        Parameters\n        ----------\n        sentences : list of NDArray, shape [(batch_size, seq_length)]\n        valid_length : list of NDArray, shape [(batch_size)]\n            Valid length of the sequence. This is used to mask the padded tokens.\n\n        Returns\n        -------\n        outputs : NDArray\n            Shape (batch_size, num_classes)\n        \"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.model.cbow.NLICBOWClassifier.forward": [[37, 51], ["zip", "mxnet.nd.concat", "cbow.NLICBOWClassifier.classifier", "cbow.NLICBOWClassifier.embedding", "sentence_embeddings.append", "cbow.NLICBOWClassifier.sum"], "methods", ["None"], ["    ", "def", "forward", "(", "self", ",", "sentences", ",", "valid_lengths", "=", "None", ")", ":", "# pylint: disable=arguments-differ", "\n", "        ", "sentence_embeddings", "=", "[", "]", "\n", "for", "sentence", ",", "valid_length", "in", "zip", "(", "sentences", ",", "valid_lengths", ")", ":", "\n", "            ", "emb", "=", "self", ".", "embedding", "(", "sentence", ")", "\n", "# NOTE: masking doesn't seem to make a difference", "\n", "#emb = mx.nd.SequenceMask(emb, sequence_length=valid_length, use_sequence_length=True, axis=1)", "\n", "sentence_embeddings", ".", "append", "(", "emb", ".", "sum", "(", "axis", "=", "1", ")", ")", "\n", "\n", "", "premise", ",", "hypothesis", "=", "sentence_embeddings", "\n", "diff", "=", "premise", "-", "hypothesis", "\n", "prod", "=", "premise", "*", "hypothesis", "\n", "feature", "=", "mx", ".", "nd", ".", "concat", "(", "premise", ",", "hypothesis", ",", "diff", ",", "prod", ",", "dim", "=", "1", ")", "\n", "\n", "return", "self", ".", "classifier", "(", "feature", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.model.cbow.NLIHandcraftedClassifier.forward": [[53, 58], ["cbow.NLIHandcraftedClassifier.embedding().sum", "cbow.NLIHandcraftedClassifier.embedding().sum", "mxnet.nd.concat", "cbow.NLIHandcraftedClassifier.classifier", "cbow.NLIHandcraftedClassifier.embedding", "cbow.NLIHandcraftedClassifier.embedding"], "methods", ["None"], ["    ", "def", "forward", "(", "self", ",", "dense_features", ",", "overlap_tokens", ",", "non_overlap_tokens", ")", ":", "\n", "        ", "overlap_emb", "=", "self", ".", "embedding", "(", "overlap_tokens", ")", ".", "sum", "(", "axis", "=", "1", ")", "\n", "non_overlap_emb", "=", "self", ".", "embedding", "(", "non_overlap_tokens", ")", ".", "sum", "(", "axis", "=", "1", ")", "\n", "feature", "=", "mx", ".", "nd", ".", "concat", "(", "dense_features", ",", "overlap_emb", ",", "non_overlap_emb", ")", "\n", "return", "self", ".", "classifier", "(", "feature", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.hhexiy_debiased.model.esim.ESIMClassifier.__init__": [[51, 68], ["mxnet.gluon.nn.HybridBlock.__init__", "esim.ESIMClassifier.name_scope", "mxnet.gluon.nn.Embedding", "mxnet.gluon.nn.Dropout", "mxnet.gluon.rnn.LSTM", "mxnet.gluon.nn.Dense", "mxnet.gluon.rnn.LSTM", "mxnet.gluon.nn.HybridSequential", "esim.ESIMClassifier.classifier.add", "esim.ESIMClassifier.classifier.add", "esim.ESIMClassifier.classifier.add", "mxnet.gluon.nn.Dense", "esim.ESIMClassifier.classifier.add", "mxnet.gluon.nn.Dense", "mxnet.gluon.nn.Dropout", "mxnet.gluon.nn.Dropout"], "methods", ["home.repos.pwc.inspect_result.hhexiy_debiased.model.esim.ESIMClassifier.__init__"], ["def", "__init__", "(", "self", ",", "vocab_size", ",", "num_classes", ",", "word_embed_size", ",", "hidden_size", ",", "dense_size", ",", "\n", "dropout", "=", "0.", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "**", "kwargs", ")", "\n", "with", "self", ".", "name_scope", "(", ")", ":", "\n", "            ", "self", ".", "embedding", "=", "nn", ".", "Embedding", "(", "vocab_size", ",", "word_embed_size", ")", "\n", "self", ".", "embedding_dropout", "=", "nn", ".", "Dropout", "(", "dropout", ",", "axes", "=", "1", ")", "\n", "self", ".", "lstm_encoder1", "=", "rnn", ".", "LSTM", "(", "hidden_size", ",", "input_size", "=", "word_embed_size", ",", "bidirectional", "=", "True", ",", "layout", "=", "'NTC'", ")", "\n", "self", ".", "ff_proj", "=", "nn", ".", "Dense", "(", "hidden_size", ",", "in_units", "=", "hidden_size", "*", "2", "*", "4", ",", "flatten", "=", "False", ",", "activation", "=", "'relu'", ")", "\n", "self", ".", "lstm_encoder2", "=", "rnn", ".", "LSTM", "(", "hidden_size", ",", "input_size", "=", "hidden_size", ",", "bidirectional", "=", "True", ",", "layout", "=", "'NTC'", ")", "\n", "\n", "self", ".", "classifier", "=", "nn", ".", "HybridSequential", "(", ")", "\n", "if", "dropout", ":", "\n", "                ", "self", ".", "classifier", ".", "add", "(", "nn", ".", "Dropout", "(", "rate", "=", "dropout", ")", ")", "\n", "", "self", ".", "classifier", ".", "add", "(", "nn", ".", "Dense", "(", "units", "=", "hidden_size", ",", "activation", "=", "'relu'", ")", ")", "\n", "if", "dropout", ":", "\n", "                ", "self", ".", "classifier", ".", "add", "(", "nn", ".", "Dropout", "(", "rate", "=", "dropout", ")", ")", "\n", "", "self", ".", "classifier", ".", "add", "(", "nn", ".", "Dense", "(", "units", "=", "num_classes", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.model.esim.ESIMClassifier._soft_attention_align": [[69, 82], ["F.batch_dot", "F.batch_dot", "F.batch_dot", "F.batch_dot.softmax", "F.batch_dot.transpose().softmax", "F.batch_dot.transpose"], "methods", ["None"], ["", "", "def", "_soft_attention_align", "(", "self", ",", "F", ",", "x1", ",", "x2", ")", ":", "#valid_len1, valid_len2):", "\n", "# attention shape: (batch, x1_seq_len, x2_seq_len)", "\n", "        ", "attention", "=", "F", ".", "batch_dot", "(", "x1", ",", "x2", ",", "transpose_b", "=", "True", ")", "\n", "\n", "## masking", "\n", "#a_mask_1 = F.SequenceMask(attention, sequence_length=valid_len1, use_sequence_length=True, axis=1, value=EPS)", "\n", "#a_mask_1_2 = F.SequenceMask(a_mask_1.transpose([0, 2, 1]), sequence_length=valid_len2, use_sequence_length=True, axis=1, value=EPS).transpose([0, 2, 1])", "\n", "#attention = a_mask_1_2", "\n", "\n", "x1_align", "=", "F", ".", "batch_dot", "(", "attention", ".", "softmax", "(", ")", ",", "x2", ")", "\n", "x2_align", "=", "F", ".", "batch_dot", "(", "attention", ".", "transpose", "(", "[", "0", ",", "2", ",", "1", "]", ")", ".", "softmax", "(", ")", ",", "x1", ")", "\n", "\n", "return", "x1_align", ",", "x2_align", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.model.esim.ESIMClassifier._submul": [[83, 88], ["F.concat"], "methods", ["None"], ["", "def", "_submul", "(", "self", ",", "F", ",", "x1", ",", "x2", ")", ":", "\n", "        ", "mul", "=", "x1", "*", "x2", "\n", "sub", "=", "x1", "-", "x2", "\n", "\n", "return", "F", ".", "concat", "(", "mul", ",", "sub", ",", "dim", "=", "-", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.model.esim.ESIMClassifier._pool": [[89, 94], ["x.mean", "x.max", "F.concat"], "methods", ["None"], ["", "def", "_pool", "(", "self", ",", "F", ",", "x", ")", ":", "\n", "        ", "p1", "=", "x", ".", "mean", "(", "axis", "=", "1", ")", "\n", "p2", "=", "x", ".", "max", "(", "axis", "=", "1", ")", "\n", "\n", "return", "F", ".", "concat", "(", "p1", ",", "p2", ",", "dim", "=", "-", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.model.esim.ESIMClassifier.hybrid_forward": [[95, 124], ["esim.ESIMClassifier.embedding_dropout", "esim.ESIMClassifier.embedding_dropout", "esim.ESIMClassifier.lstm_encoder1", "esim.ESIMClassifier.lstm_encoder1", "esim.ESIMClassifier._soft_attention_align", "F.concat", "F.concat", "esim.ESIMClassifier.lstm_encoder2", "esim.ESIMClassifier.lstm_encoder2", "esim.ESIMClassifier._pool", "esim.ESIMClassifier._pool", "esim.ESIMClassifier.classifier", "esim.ESIMClassifier.embedding", "esim.ESIMClassifier.embedding", "esim.ESIMClassifier._submul", "esim.ESIMClassifier._submul", "esim.ESIMClassifier.ff_proj", "esim.ESIMClassifier.ff_proj", "F.concat"], "methods", ["home.repos.pwc.inspect_result.hhexiy_debiased.model.esim.ESIMClassifier._soft_attention_align", "home.repos.pwc.inspect_result.hhexiy_debiased.model.esim.ESIMClassifier._pool", "home.repos.pwc.inspect_result.hhexiy_debiased.model.esim.ESIMClassifier._pool", "home.repos.pwc.inspect_result.hhexiy_debiased.model.esim.ESIMClassifier._submul", "home.repos.pwc.inspect_result.hhexiy_debiased.model.esim.ESIMClassifier._submul"], ["", "def", "hybrid_forward", "(", "self", ",", "F", ",", "x1", ",", "x2", ")", ":", "#, valid_len1, valid_len2):  # pylint: disable=arguments-differ", "\n", "# x1_embed x2_embed shape: (batch, seq_len, word_embed_size)", "\n", "        ", "x1_embed", "=", "self", ".", "embedding_dropout", "(", "self", ".", "embedding", "(", "x1", ")", ")", "\n", "x2_embed", "=", "self", ".", "embedding_dropout", "(", "self", ".", "embedding", "(", "x2", ")", ")", "\n", "\n", "x1_lstm_encode", "=", "self", ".", "lstm_encoder1", "(", "x1_embed", ")", "\n", "x2_lstm_encode", "=", "self", ".", "lstm_encoder1", "(", "x2_embed", ")", "\n", "\n", "# attention", "\n", "x1_algin", ",", "x2_algin", "=", "self", ".", "_soft_attention_align", "(", "F", ",", "x1_lstm_encode", ",", "x2_lstm_encode", ")", "\n", "#valid_len1, valid_len2)", "\n", "\n", "# compose", "\n", "x1_combined", "=", "F", ".", "concat", "(", "x1_lstm_encode", ",", "x1_algin", ",", "\n", "self", ".", "_submul", "(", "F", ",", "x1_lstm_encode", ",", "x1_algin", ")", ",", "dim", "=", "-", "1", ")", "\n", "x2_combined", "=", "F", ".", "concat", "(", "x2_lstm_encode", ",", "x2_algin", ",", "\n", "self", ".", "_submul", "(", "F", ",", "x2_lstm_encode", ",", "x2_algin", ")", ",", "dim", "=", "-", "1", ")", "\n", "\n", "x1_compose", "=", "self", ".", "lstm_encoder2", "(", "self", ".", "ff_proj", "(", "x1_combined", ")", ")", "\n", "x2_compose", "=", "self", ".", "lstm_encoder2", "(", "self", ".", "ff_proj", "(", "x2_combined", ")", ")", "\n", "\n", "# aggregate", "\n", "x1_agg", "=", "self", ".", "_pool", "(", "F", ",", "x1_compose", ")", "\n", "x2_agg", "=", "self", ".", "_pool", "(", "F", ",", "x2_compose", ")", "\n", "\n", "# fully connection", "\n", "output", "=", "self", ".", "classifier", "(", "F", ".", "concat", "(", "x1_agg", ",", "x2_agg", ",", "dim", "=", "-", "1", ")", ")", "\n", "\n", "return", "output", "\n", "", "", ""]], "home.repos.pwc.inspect_result.hhexiy_debiased.scripts.summarize_results.parse_args": [[11, 18], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args"], "function", ["home.repos.pwc.inspect_result.hhexiy_debiased.scripts.error_analysis.parse_args"], ["def", "parse_args", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "parser", ".", "add_argument", "(", "'--runs-dir'", ",", "nargs", "=", "'+'", ")", "\n", "parser", ".", "add_argument", "(", "'--output-json'", ")", "\n", "parser", ".", "add_argument", "(", "'--error-analysis'", ",", "default", "=", "None", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "return", "args", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.scripts.summarize_results.get_model_config": [[19, 22], ["json.load", "json.load", "open"], "function", ["None"], ["", "def", "get_model_config", "(", "model_path", ")", ":", "\n", "    ", "res", "=", "json", ".", "load", "(", "open", "(", "'{}/report.json'", ".", "format", "(", "model_path", ")", ")", ")", "[", "'config'", "]", "\n", "return", "res", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.scripts.summarize_results.analyze": [[23, 37], ["os.path.join", "sklearn.metrics.classification_report", "open", "csv.DictReader", "os.path.dirname", "preds.append", "labels.append", "preds.append", "labels.append"], "function", ["None"], ["", "def", "analyze", "(", "path", ",", "data", ")", ":", "\n", "    ", "pred_file", "=", "os", ".", "path", ".", "join", "(", "'{}/predictions.tsv'", ".", "format", "(", "os", ".", "path", ".", "dirname", "(", "path", ")", ")", ")", "\n", "with", "open", "(", "pred_file", ")", "as", "fin", ":", "\n", "        ", "reader", "=", "csv", ".", "DictReader", "(", "fin", ",", "delimiter", "=", "'\\t'", ")", "\n", "preds", ",", "labels", "=", "[", "]", ",", "[", "]", "\n", "for", "row", "in", "reader", ":", "\n", "            ", "if", "data", "==", "'hans'", ":", "\n", "                ", "preds", ".", "append", "(", "'non-entailment'", "if", "row", "[", "'pred'", "]", "!=", "'entailment'", "else", "'entailment'", ")", "\n", "labels", ".", "append", "(", "row", "[", "'label'", "]", ")", "\n", "", "else", ":", "\n", "                ", "preds", ".", "append", "(", "row", "[", "'pred'", "]", ")", "\n", "labels", ".", "append", "(", "row", "[", "'label'", "]", ")", "\n", "", "", "", "report", "=", "classification_report", "(", "labels", ",", "preds", ",", "output_dict", "=", "True", ")", "\n", "return", "report", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.scripts.summarize_results.parse_file": [[38, 151], ["int", "json.load", "json.load", "summarize_results.get_model_config", "config[].split", "float", "float", "float", "int", "test_data.startswith", "get_model_config.get", "model.upper", "open", "get_model_config.get", "get_model_config.get", "len", "traceback.print_exc", "print", "r[].startswith", "r[].startswith", "c", "summarize_results.analyze", "summarize_results.get_model_config", "os.path.dirname", "report.update", "report.update", "traceback.print_exc", "print", "prev_models.append", "os.path.dirname", "prev_models.append", "prev_models.append"], "function", ["home.repos.pwc.inspect_result.hhexiy_debiased.scripts.summarize_results.get_model_config", "home.repos.pwc.inspect_result.hhexiy_debiased.scripts.summarize_results.analyze", "home.repos.pwc.inspect_result.hhexiy_debiased.scripts.summarize_results.get_model_config", "home.repos.pwc.inspect_result.hhexiy_debiased.src.dataset.MappedAccuracy.update", "home.repos.pwc.inspect_result.hhexiy_debiased.src.dataset.MappedAccuracy.update"], ["", "def", "parse_file", "(", "path", ",", "error_analysis", ")", ":", "\n", "#print('parsing {}'.format(path))", "\n", "    ", "try", ":", "\n", "        ", "res", "=", "json", ".", "load", "(", "open", "(", "path", ")", ")", "\n", "config", "=", "res", "[", "'config'", "]", "\n", "model_config", "=", "get_model_config", "(", "config", "[", "'init_from'", "]", ")", "\n", "test_data", "=", "'{}-{}'", ".", "format", "(", "config", "[", "'task_name'", "]", ",", "config", "[", "'test_split'", "]", ")", "\n", "test_split", "=", "config", "[", "'test_split'", "]", "\n", "train_data", "=", "'{}-{}'", ".", "format", "(", "model_config", "[", "'task_name'", "]", ",", "model_config", "[", "'test_split'", "]", ")", "\n", "model_path", "=", "config", "[", "'init_from'", "]", ".", "split", "(", "'/'", ")", "\n", "model_path", "=", "'/'", ".", "join", "(", "model_path", "[", ":", "-", "1", "]", "+", "[", "model_path", "[", "-", "1", "]", "[", ":", "5", "]", "]", ")", "\n", "model", "=", "model_config", "[", "'model_type'", "]", "\n", "\n", "model_cheat", "=", "float", "(", "model_config", "[", "'cheat'", "]", ")", "\n", "test_cheat", "=", "float", "(", "config", "[", "'cheat'", "]", ")", "\n", "rm_cheat", "=", "float", "(", "model_config", ".", "get", "(", "'remove_cheat'", ",", "False", ")", ")", "\n", "model", "=", "model_config", ".", "get", "(", "'model_type'", ",", "'bert'", ")", "or", "'bert'", "\n", "superficial", "=", "model_config", "[", "'superficial'", "]", "if", "model_config", "[", "'superficial'", "]", "else", "'-1'", "\n", "additive", "=", "len", "(", "model_config", "[", "'additive'", "]", ")", "if", "model_config", "[", "'additive'", "]", "else", "0", "\n", "last", "=", "int", "(", "config", "[", "'use_last'", "]", ")", "\n", "metrics", "=", "res", "[", "'test'", "]", "[", "test_split", "]", "\n", "if", "test_data", ".", "startswith", "(", "'MNLI-hans'", ")", ":", "\n", "            ", "metric_name", "=", "'mapped-accuracy'", "\n", "", "else", ":", "\n", "            ", "metric_name", "=", "'accuracy'", "\n", "", "if", "additive", "==", "0", ":", "\n", "            ", "acc", "=", "metrics", "[", "metric_name", "]", "\n", "additive", "=", "'0'", "\n", "if", "model_config", "[", "'superficial'", "]", "is", "True", "or", "model_config", "[", "'superficial'", "]", "==", "'hypo'", ":", "\n", "                ", "model", "=", "'hypo'", "\n", "", "elif", "model_config", "[", "'superficial'", "]", "==", "'handcrafted'", ":", "\n", "                ", "model", "=", "'hand'", "\n", "", "", "else", ":", "\n", "            ", "if", "config", "[", "'additive_mode'", "]", "==", "'all'", ":", "\n", "                ", "acc", "=", "metrics", "[", "'last_{}'", ".", "format", "(", "metric_name", ")", "]", "\n", "", "elif", "config", "[", "'additive_mode'", "]", "==", "'last'", ":", "\n", "                ", "acc", "=", "metrics", "[", "'{}'", ".", "format", "(", "metric_name", ")", "]", "\n", "", "else", ":", "\n", "                ", "raise", "ValueError", "\n", "", "prev_models", "=", "[", "]", "\n", "for", "prev", "in", "model_config", "[", "'additive'", "]", ":", "\n", "                ", "prev_config", "=", "get_model_config", "(", "prev", ")", "\n", "if", "prev_config", "[", "'superficial'", "]", "==", "'handcrafted'", ":", "\n", "                    ", "prev_models", ".", "append", "(", "'hand'", ")", "\n", "", "elif", "prev_config", "[", "'superficial'", "]", ":", "\n", "                    ", "prev_models", ".", "append", "(", "'hypo'", ")", "\n", "", "else", ":", "\n", "                    ", "prev_models", ".", "append", "(", "'cbow'", ")", "\n", "", "", "additive", "=", "','", ".", "join", "(", "prev_models", ")", "\n", "", "", "except", "Exception", "as", "e", ":", "\n", "        ", "traceback", ".", "print_exc", "(", ")", "\n", "print", "(", "os", ".", "path", ".", "dirname", "(", "path", ")", ")", "\n", "return", "{", "\n", "'status'", ":", "'failed'", ",", "\n", "'eval_path'", ":", "path", ",", "\n", "}", "\n", "", "remove", "=", "int", "(", "model_config", ".", "get", "(", "'remove'", ",", "False", ")", ")", "\n", "if", "remove", "==", "1", ":", "\n", "        ", "assert", "not", "config", "[", "'remove'", "]", "\n", "", "report", "=", "{", "\n", "'status'", ":", "'success'", ",", "\n", "'train_data'", ":", "train_data", ",", "\n", "'test_data'", ":", "test_data", ",", "\n", "'last'", ":", "last", ",", "\n", "'mch'", ":", "model_cheat", ",", "\n", "'tch'", ":", "test_cheat", ",", "\n", "'rm_ch'", ":", "rm_cheat", ",", "\n", "'sup'", ":", "superficial", ",", "\n", "'add'", ":", "additive", ",", "\n", "'rm'", ":", "remove", ",", "\n", "'model'", ":", "model", ".", "upper", "(", ")", ",", "\n", "'acc'", ":", "acc", ",", "\n", "'model_path'", ":", "model_path", ",", "\n", "'eval_path'", ":", "path", ",", "\n", "}", "\n", "constraints", "=", "{", "\n", "lambda", "r", ":", "r", "[", "'add'", "]", "in", "(", "'hand'", ",", "'hypo'", ",", "'cbow'", ",", "'0'", ")", ",", "\n", "lambda", "r", ":", "r", "[", "'test_data'", "]", ".", "startswith", "(", "'MNLI-hans'", ")", ",", "\n", "lambda", "r", ":", "r", "[", "'train_data'", "]", ".", "startswith", "(", "'MNLI'", ")", ",", "\n", "lambda", "r", ":", "r", "[", "'model'", "]", "in", "(", "'BERT'", ",", "'DA'", ",", "'ESIM'", ")", ",", "\n", "}", "\n", "for", "c", "in", "constraints", ":", "\n", "        ", "if", "not", "c", "(", "report", ")", ":", "\n", "            ", "return", "{", "\n", "'status'", ":", "'filtered'", ",", "\n", "'eval_path'", ":", "path", ",", "\n", "}", "\n", "", "", "if", "error_analysis", "is", "not", "None", ":", "\n", "        ", "try", ":", "\n", "            ", "acc_report", "=", "analyze", "(", "path", ",", "error_analysis", ")", "\n", "if", "error_analysis", "==", "'hans'", ":", "\n", "                ", "report", ".", "update", "(", "{", "\n", "'ent'", ":", "acc_report", "[", "'entailment'", "]", "[", "'f1-score'", "]", ",", "\n", "'n-ent'", ":", "acc_report", "[", "'non-entailment'", "]", "[", "'f1-score'", "]", ",", "\n", "'avg'", ":", "acc_report", "[", "'macro avg'", "]", "[", "'f1-score'", "]", ",", "\n", "'acc_report'", ":", "acc_report", ",", "\n", "}", ")", "\n", "", "else", ":", "\n", "                ", "report", ".", "update", "(", "{", "\n", "'ent'", ":", "acc_report", "[", "'entailment'", "]", "[", "'f1-score'", "]", ",", "\n", "'con'", ":", "acc_report", "[", "'contradiction'", "]", "[", "'f1-score'", "]", ",", "\n", "'neu'", ":", "acc_report", "[", "'neutral'", "]", "[", "'f1-score'", "]", ",", "\n", "'avg'", ":", "acc_report", "[", "'macro avg'", "]", "[", "'f1-score'", "]", ",", "\n", "'acc_report'", ":", "acc_report", ",", "\n", "}", ")", "\n", "", "", "except", "Exception", "as", "e", ":", "\n", "            ", "traceback", ".", "print_exc", "(", ")", "\n", "print", "(", "os", ".", "path", ".", "dirname", "(", "path", ")", ")", "\n", "return", "{", "\n", "'status'", ":", "'failed'", ",", "\n", "'eval_path'", ":", "path", ",", "\n", "}", "\n", "", "", "return", "report", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.scripts.summarize_results.main": [[152, 211], ["header.format.format", "sorted", "print", "files.extend", "summarize_results.parse_file", "print", "input", "columns.extend", "len", "print", "print", "glob.glob", "print", "print", "print", "columns.extend", "row_format.format", "open", "json.dump", "json.dump", "shutil.rmtree", "os.path.dirname", "len"], "function", ["home.repos.pwc.inspect_result.hhexiy_debiased.scripts.summarize_results.parse_file"], ["", "def", "main", "(", "args", ")", ":", "\n", "    ", "files", "=", "[", "]", "\n", "for", "d", "in", "args", ".", "runs_dir", ":", "\n", "        ", "files", ".", "extend", "(", "glob", ".", "glob", "(", "'{}/*/report.json'", ".", "format", "(", "d", ")", ")", ")", "\n", "", "all_res", "=", "[", "parse_file", "(", "f", ",", "args", ".", "error_analysis", ")", "for", "f", "in", "files", "]", "\n", "failed_paths", "=", "[", "r", "[", "'eval_path'", "]", "for", "r", "in", "all_res", "if", "r", "[", "'status'", "]", "==", "'failed'", "]", "\n", "if", "failed_paths", ":", "\n", "        ", "print", "(", "'failed paths:'", ")", "\n", "for", "f", "in", "failed_paths", ":", "\n", "            ", "print", "(", "f", ")", "\n", "", "ans", "=", "input", "(", "'remove failed paths? [Y/N]'", ")", "\n", "if", "ans", "==", "'Y'", ":", "\n", "            ", "for", "f", "in", "failed_paths", ":", "\n", "                ", "shutil", ".", "rmtree", "(", "os", ".", "path", ".", "dirname", "(", "f", ")", ")", "\n", "", "print", "(", "'removed {} dirs'", ".", "format", "(", "len", "(", "failed_paths", ")", ")", ")", "\n", "", "else", ":", "\n", "            ", "print", "(", "'ignore failed paths. continue'", ")", "\n", "\n", "", "", "all_res", "=", "[", "r", "for", "r", "in", "all_res", "if", "r", "[", "'status'", "]", "==", "'success'", "]", "\n", "\n", "columns", "=", "[", "\n", "(", "'train_data'", ",", "20", ",", "'s'", ")", ",", "\n", "(", "'test_data'", ",", "50", ",", "'s'", ")", ",", "\n", "(", "'model'", ",", "7", ",", "'s'", ")", ",", "\n", "(", "'rm'", ",", "5", ",", "'d'", ")", ",", "\n", "(", "'add'", ",", "7", ",", "'s'", ")", ",", "\n", "(", "'acc'", ",", "10", ",", "'.3f'", ")", ",", "\n", "]", "\n", "if", "args", ".", "error_analysis", "==", "'hans'", ":", "\n", "        ", "columns", ".", "extend", "(", "[", "\n", "(", "'ent'", ",", "10", ",", "'.3f'", ")", ",", "\n", "(", "'n-ent'", ",", "10", ",", "'.3f'", ")", ",", "\n", "(", "'avg'", ",", "10", ",", "'.3f'", ")", ",", "\n", "]", ")", "\n", "", "elif", "args", ".", "error_analysis", "is", "not", "None", ":", "\n", "        ", "columns", ".", "extend", "(", "[", "\n", "(", "'ent'", ",", "10", ",", "'.3f'", ")", ",", "\n", "(", "'con'", ",", "10", ",", "'.3f'", ")", ",", "\n", "(", "'neu'", ",", "10", ",", "'.3f'", ")", ",", "\n", "(", "'avg'", ",", "10", ",", "'.3f'", ")", ",", "\n", "]", ")", "\n", "#columns.append(('eval_path', 10, 's'))", "\n", "", "if", "len", "(", "all_res", ")", "==", "0", ":", "\n", "        ", "print", "(", "'no results found'", ")", "\n", "return", "\n", "", "header", "=", "''", ".", "join", "(", "[", "'{{:<{w}s}}'", ".", "format", "(", "w", "=", "width", ")", "\n", "for", "_", ",", "width", ",", "_", "in", "columns", "]", ")", "\n", "header", "=", "header", ".", "format", "(", "*", "[", "c", "[", "0", "]", "for", "c", "in", "columns", "]", ")", "\n", "row_format", "=", "''", ".", "join", "(", "[", "'{{{c}:<{w}{f}}}'", ".", "format", "(", "c", "=", "name", ",", "w", "=", "width", ",", "f", "=", "form", ")", "\n", "for", "name", ",", "width", ",", "form", "in", "columns", "]", ")", "\n", "all_res", "=", "sorted", "(", "all_res", ",", "key", "=", "lambda", "x", ":", "[", "x", "[", "c", "[", "0", "]", "]", "for", "c", "in", "columns", "]", ")", "\n", "\n", "print", "(", "header", ")", "\n", "for", "res", "in", "all_res", ":", "\n", "        ", "print", "(", "row_format", ".", "format", "(", "**", "res", ")", ")", "\n", "\n", "", "if", "args", ".", "output_json", ":", "\n", "        ", "with", "open", "(", "args", ".", "output_json", ",", "'w'", ")", "as", "fout", ":", "\n", "            ", "json", ".", "dump", "(", "all_res", ",", "fout", ",", "indent", "=", "2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.scripts.download_glue_data.download_and_extract": [[44, 52], ["print", "urllib.request.urlretrieve", "os.remove", "print", "zipfile.ZipFile", "zip_ref.extractall"], "function", ["None"], ["def", "download_and_extract", "(", "task", ",", "data_dir", ")", ":", "\n", "    ", "print", "(", "\"Downloading and extracting %s...\"", "%", "task", ")", "\n", "data_file", "=", "\"%s.zip\"", "%", "task", "\n", "urllib", ".", "request", ".", "urlretrieve", "(", "TASK2PATH", "[", "task", "]", ",", "data_file", ")", "\n", "with", "zipfile", ".", "ZipFile", "(", "data_file", ")", "as", "zip_ref", ":", "\n", "        ", "zip_ref", ".", "extractall", "(", "data_dir", ")", "\n", "", "os", ".", "remove", "(", "data_file", ")", "\n", "print", "(", "\"\\tCompleted!\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.scripts.download_glue_data.format_mrpc": [[53, 97], ["print", "os.path.join", "os.path.isfile", "os.path.isfile", "urllib.request.urlretrieve", "print", "os.path.isdir", "os.mkdir", "os.path.join", "os.path.join", "print", "os.path.join", "os.path.join", "urllib.request.urlretrieve", "urllib.request.urlretrieve", "os.path.join", "open", "open", "open", "open", "data_fh.readline", "train_fh.write", "dev_fh.write", "open", "open", "data_fh.readline", "test_fh.write", "enumerate", "os.path.join", "dev_ids.append", "os.path.join", "os.path.join", "row.strip().split", "os.path.join", "row.strip().split", "test_fh.write", "row.strip().split", "dev_fh.write", "train_fh.write", "row.strip", "row.strip", "row.strip"], "function", ["None"], ["", "def", "format_mrpc", "(", "data_dir", ",", "path_to_data", ")", ":", "\n", "    ", "print", "(", "\"Processing MRPC...\"", ")", "\n", "mrpc_dir", "=", "os", ".", "path", ".", "join", "(", "data_dir", ",", "\"MRPC\"", ")", "\n", "if", "not", "os", ".", "path", ".", "isdir", "(", "mrpc_dir", ")", ":", "\n", "        ", "os", ".", "mkdir", "(", "mrpc_dir", ")", "\n", "", "if", "path_to_data", ":", "\n", "        ", "mrpc_train_file", "=", "os", ".", "path", ".", "join", "(", "path_to_data", ",", "\"msr_paraphrase_train.txt\"", ")", "\n", "mrpc_test_file", "=", "os", ".", "path", ".", "join", "(", "path_to_data", ",", "\"msr_paraphrase_test.txt\"", ")", "\n", "", "else", ":", "\n", "        ", "print", "(", "\"Local MRPC data not specified, downloading data from %s\"", "%", "MRPC_TRAIN", ")", "\n", "mrpc_train_file", "=", "os", ".", "path", ".", "join", "(", "mrpc_dir", ",", "\"msr_paraphrase_train.txt\"", ")", "\n", "mrpc_test_file", "=", "os", ".", "path", ".", "join", "(", "mrpc_dir", ",", "\"msr_paraphrase_test.txt\"", ")", "\n", "urllib", ".", "request", ".", "urlretrieve", "(", "MRPC_TRAIN", ",", "mrpc_train_file", ")", "\n", "urllib", ".", "request", ".", "urlretrieve", "(", "MRPC_TEST", ",", "mrpc_test_file", ")", "\n", "", "assert", "os", ".", "path", ".", "isfile", "(", "mrpc_train_file", ")", ",", "\"Train data not found at %s\"", "%", "mrpc_train_file", "\n", "assert", "os", ".", "path", ".", "isfile", "(", "mrpc_test_file", ")", ",", "\"Test data not found at %s\"", "%", "mrpc_test_file", "\n", "urllib", ".", "request", ".", "urlretrieve", "(", "TASK2PATH", "[", "\"MRPC\"", "]", ",", "os", ".", "path", ".", "join", "(", "mrpc_dir", ",", "\"dev_ids.tsv\"", ")", ")", "\n", "\n", "dev_ids", "=", "[", "]", "\n", "with", "open", "(", "os", ".", "path", ".", "join", "(", "mrpc_dir", ",", "\"dev_ids.tsv\"", ")", ",", "encoding", "=", "\"utf8\"", ")", "as", "ids_fh", ":", "\n", "        ", "for", "row", "in", "ids_fh", ":", "\n", "            ", "dev_ids", ".", "append", "(", "row", ".", "strip", "(", ")", ".", "split", "(", "'\\t'", ")", ")", "\n", "\n", "", "", "with", "open", "(", "mrpc_train_file", ",", "encoding", "=", "\"utf8\"", ")", "as", "data_fh", ",", "open", "(", "os", ".", "path", ".", "join", "(", "mrpc_dir", ",", "\"train.tsv\"", ")", ",", "'w'", ",", "encoding", "=", "\"utf8\"", ")", "as", "train_fh", ",", "open", "(", "os", ".", "path", ".", "join", "(", "mrpc_dir", ",", "\"dev.tsv\"", ")", ",", "'w'", ",", "encoding", "=", "\"utf8\"", ")", "as", "dev_fh", ":", "\n", "        ", "header", "=", "data_fh", ".", "readline", "(", ")", "\n", "train_fh", ".", "write", "(", "header", ")", "\n", "dev_fh", ".", "write", "(", "header", ")", "\n", "for", "row", "in", "data_fh", ":", "\n", "            ", "label", ",", "id1", ",", "id2", ",", "s1", ",", "s2", "=", "row", ".", "strip", "(", ")", ".", "split", "(", "'\\t'", ")", "\n", "if", "[", "id1", ",", "id2", "]", "in", "dev_ids", ":", "\n", "                ", "dev_fh", ".", "write", "(", "\"%s\\t%s\\t%s\\t%s\\t%s\\n\"", "%", "(", "label", ",", "id1", ",", "id2", ",", "s1", ",", "s2", ")", ")", "\n", "", "else", ":", "\n", "                ", "train_fh", ".", "write", "(", "\"%s\\t%s\\t%s\\t%s\\t%s\\n\"", "%", "(", "label", ",", "id1", ",", "id2", ",", "s1", ",", "s2", ")", ")", "\n", "\n", "", "", "", "with", "open", "(", "mrpc_test_file", ",", "encoding", "=", "\"utf8\"", ")", "as", "data_fh", ",", "open", "(", "os", ".", "path", ".", "join", "(", "mrpc_dir", ",", "\"test.tsv\"", ")", ",", "'w'", ",", "encoding", "=", "\"utf8\"", ")", "as", "test_fh", ":", "\n", "        ", "header", "=", "data_fh", ".", "readline", "(", ")", "\n", "test_fh", ".", "write", "(", "\"index\\t#1 ID\\t#2 ID\\t#1 String\\t#2 String\\n\"", ")", "\n", "for", "idx", ",", "row", "in", "enumerate", "(", "data_fh", ")", ":", "\n", "            ", "label", ",", "id1", ",", "id2", ",", "s1", ",", "s2", "=", "row", ".", "strip", "(", ")", ".", "split", "(", "'\\t'", ")", "\n", "test_fh", ".", "write", "(", "\"%d\\t%s\\t%s\\t%s\\t%s\\n\"", "%", "(", "idx", ",", "id1", ",", "id2", ",", "s1", ",", "s2", ")", ")", "\n", "", "", "print", "(", "\"\\tCompleted!\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.scripts.download_glue_data.download_diagnostic": [[98, 106], ["print", "os.path.join", "urllib.request.urlretrieve", "print", "os.path.isdir", "os.mkdir", "os.path.join", "os.path.join"], "function", ["None"], ["", "def", "download_diagnostic", "(", "data_dir", ")", ":", "\n", "    ", "print", "(", "\"Downloading and extracting diagnostic...\"", ")", "\n", "if", "not", "os", ".", "path", ".", "isdir", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "\"diagnostic\"", ")", ")", ":", "\n", "        ", "os", ".", "mkdir", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "\"diagnostic\"", ")", ")", "\n", "", "data_file", "=", "os", ".", "path", ".", "join", "(", "data_dir", ",", "\"diagnostic\"", ",", "\"diagnostic.tsv\"", ")", "\n", "urllib", ".", "request", ".", "urlretrieve", "(", "TASK2PATH", "[", "\"diagnostic\"", "]", ",", "data_file", ")", "\n", "print", "(", "\"\\tCompleted!\"", ")", "\n", "return", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.scripts.download_glue_data.get_tasks": [[107, 117], ["task_names.split.split", "tasks.append"], "function", ["None"], ["", "def", "get_tasks", "(", "task_names", ")", ":", "\n", "    ", "task_names", "=", "task_names", ".", "split", "(", "','", ")", "\n", "if", "\"all\"", "in", "task_names", ":", "\n", "        ", "tasks", "=", "TASKS", "\n", "", "else", ":", "\n", "        ", "tasks", "=", "[", "]", "\n", "for", "task_name", "in", "task_names", ":", "\n", "            ", "assert", "task_name", "in", "TASKS", ",", "\"Task %s not found!\"", "%", "task_name", "\n", "tasks", ".", "append", "(", "task_name", ")", "\n", "", "", "return", "tasks", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.scripts.download_glue_data.main": [[118, 138], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "download_glue_data.get_tasks", "os.path.isdir", "os.mkdir", "download_glue_data.format_mrpc", "download_glue_data.download_diagnostic", "download_glue_data.download_and_extract"], "function", ["home.repos.pwc.inspect_result.hhexiy_debiased.scripts.error_analysis.parse_args", "home.repos.pwc.inspect_result.hhexiy_debiased.scripts.download_glue_data.get_tasks", "home.repos.pwc.inspect_result.hhexiy_debiased.scripts.download_glue_data.format_mrpc", "home.repos.pwc.inspect_result.hhexiy_debiased.scripts.download_glue_data.download_diagnostic", "home.repos.pwc.inspect_result.hhexiy_debiased.scripts.download_glue_data.download_and_extract"], ["", "def", "main", "(", "arguments", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "parser", ".", "add_argument", "(", "'--data_dir'", ",", "help", "=", "'directory to save data to'", ",", "type", "=", "str", ",", "default", "=", "'glue_data'", ")", "\n", "parser", ".", "add_argument", "(", "'--tasks'", ",", "help", "=", "'tasks to download data for as a comma separated string'", ",", "\n", "type", "=", "str", ",", "default", "=", "'all'", ")", "\n", "parser", ".", "add_argument", "(", "'--path_to_mrpc'", ",", "help", "=", "'path to directory containing extracted MRPC data, msr_paraphrase_train.txt and msr_paraphrase_text.txt'", ",", "\n", "type", "=", "str", ",", "default", "=", "''", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", "arguments", ")", "\n", "\n", "if", "not", "os", ".", "path", ".", "isdir", "(", "args", ".", "data_dir", ")", ":", "\n", "        ", "os", ".", "mkdir", "(", "args", ".", "data_dir", ")", "\n", "", "tasks", "=", "get_tasks", "(", "args", ".", "tasks", ")", "\n", "\n", "for", "task", "in", "tasks", ":", "\n", "        ", "if", "task", "==", "'MRPC'", ":", "\n", "            ", "format_mrpc", "(", "args", ".", "data_dir", ",", "args", ".", "path_to_mrpc", ")", "\n", "", "elif", "task", "==", "'diagnostic'", ":", "\n", "            ", "download_diagnostic", "(", "args", ".", "data_dir", ")", "\n", "", "else", ":", "\n", "            ", "download_and_extract", "(", "task", ",", "args", ".", "data_dir", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.scripts.error_analysis.print_cm": [[5, 36], ["max", "print", "print", "enumerate", "len", "len", "print", "print", "range", "print", "len", "print", "len", "len", "len", "float"], "function", ["None"], ["def", "print_cm", "(", "cm", ",", "labels", ",", "hide_zeroes", "=", "False", ",", "hide_diagonal", "=", "False", ",", "hide_threshold", "=", "None", ")", ":", "\n", "    ", "\"\"\"pretty print for confusion matrixes\"\"\"", "\n", "columnwidth", "=", "max", "(", "[", "len", "(", "x", ")", "for", "x", "in", "labels", "]", "+", "[", "5", "]", ")", "# 5 is value length", "\n", "empty_cell", "=", "\" \"", "*", "columnwidth", "\n", "\n", "# Begin CHANGES", "\n", "fst_empty_cell", "=", "(", "columnwidth", "-", "3", ")", "//", "2", "*", "\" \"", "+", "\"t/p\"", "+", "(", "columnwidth", "-", "3", ")", "//", "2", "*", "\" \"", "\n", "\n", "if", "len", "(", "fst_empty_cell", ")", "<", "len", "(", "empty_cell", ")", ":", "\n", "        ", "fst_empty_cell", "=", "\" \"", "*", "(", "len", "(", "empty_cell", ")", "-", "len", "(", "fst_empty_cell", ")", ")", "+", "fst_empty_cell", "\n", "# Print header", "\n", "", "print", "(", "\"    \"", "+", "fst_empty_cell", ",", "end", "=", "\" \"", ")", "\n", "# End CHANGES", "\n", "\n", "for", "label", "in", "labels", ":", "\n", "        ", "print", "(", "\"%{0}s\"", ".", "format", "(", "columnwidth", ")", "%", "label", ",", "end", "=", "\" \"", ")", "\n", "\n", "", "print", "(", ")", "\n", "# Print rows", "\n", "for", "i", ",", "label1", "in", "enumerate", "(", "labels", ")", ":", "\n", "        ", "print", "(", "\"    %{0}s\"", ".", "format", "(", "columnwidth", ")", "%", "label1", ",", "end", "=", "\" \"", ")", "\n", "for", "j", "in", "range", "(", "len", "(", "labels", ")", ")", ":", "\n", "            ", "cell", "=", "\"%{0}.1f\"", ".", "format", "(", "columnwidth", ")", "%", "cm", "[", "i", ",", "j", "]", "\n", "if", "hide_zeroes", ":", "\n", "                ", "cell", "=", "cell", "if", "float", "(", "cm", "[", "i", ",", "j", "]", ")", "!=", "0", "else", "empty_cell", "\n", "", "if", "hide_diagonal", ":", "\n", "                ", "cell", "=", "cell", "if", "i", "!=", "j", "else", "empty_cell", "\n", "", "if", "hide_threshold", ":", "\n", "                ", "cell", "=", "cell", "if", "cm", "[", "i", ",", "j", "]", ">", "hide_threshold", "else", "empty_cell", "\n", "", "print", "(", "cell", ",", "end", "=", "\" \"", ")", "\n", "", "print", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.scripts.error_analysis.parse_args": [[37, 42], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args"], "function", ["home.repos.pwc.inspect_result.hhexiy_debiased.scripts.error_analysis.parse_args"], ["", "", "def", "parse_args", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "parser", ".", "add_argument", "(", "'--pred-file'", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "return", "args", "\n", "\n"]], "home.repos.pwc.inspect_result.hhexiy_debiased.scripts.error_analysis.main": [[43, 58], ["sklearn.metrics.classification_report", "print", "sklearn.metrics.confusion_matrix", "sorted", "error_analysis.print_cm", "open", "csv.DictReader", "list", "preds.append", "sorted.append", "set"], "function", ["home.repos.pwc.inspect_result.hhexiy_debiased.scripts.error_analysis.print_cm"], ["", "def", "main", "(", "args", ")", ":", "\n", "    ", "with", "open", "(", "args", ".", "pred_file", ")", "as", "fin", ":", "\n", "        ", "reader", "=", "csv", ".", "DictReader", "(", "fin", ",", "delimiter", "=", "'\\t'", ")", "\n", "preds", ",", "labels", "=", "[", "]", ",", "[", "]", "\n", "for", "row", "in", "reader", ":", "\n", "#preds.append('non-entailment' if row['pred'] != 'entailment' else 'entailment')", "\n", "            ", "preds", ".", "append", "(", "'non-contradiction'", "if", "row", "[", "'pred'", "]", "!=", "'contradiction'", "else", "'contradiction'", ")", "\n", "labels", ".", "append", "(", "row", "[", "'label'", "]", ")", "\n", "\n", "", "", "report", "=", "classification_report", "(", "labels", ",", "preds", ")", "\n", "print", "(", "report", ")", "\n", "\n", "conf", "=", "confusion_matrix", "(", "labels", ",", "preds", ")", "\n", "labels", "=", "sorted", "(", "list", "(", "set", "(", "labels", ")", ")", ")", "\n", "print_cm", "(", "conf", ",", "labels", "=", "labels", ")", "\n", "\n"]]}