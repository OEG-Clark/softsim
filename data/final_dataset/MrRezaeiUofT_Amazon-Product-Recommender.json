{"home.repos.pwc.inspect_result.MrRezaeiUofT_Amazon-Product-Recommender.None.Part1_utils.Text_cleaner": [[22, 29], ["range", "HoleText.append", "separator.join", "tensorflow.keras.preprocessing.text.text_to_word_sequence", "str"], "function", ["None"], ["def", "Text_cleaner", "(", "Data", ")", ":", "\n", "    ", "HoleText", "=", "[", "]", "\n", "separator", "=", "' '", "\n", "for", "i", "in", "range", "(", "Data", ".", "shape", "[", "0", "]", ")", ":", "\n", "        ", "HoleText", ".", "append", "(", "separator", ".", "join", "(", "text_to_word_sequence", "(", "str", "(", "Data", "[", "i", "]", ")", ")", ")", ")", "\n", "", "x_c", "=", "HoleText", "\n", "return", "x_c", "\n", "\n"]], "home.repos.pwc.inspect_result.MrRezaeiUofT_Amazon-Product-Recommender.None.Part1_utils.Text_length": [[34, 42], ["range", "HoleText.append", "len", "tensorflow.keras.preprocessing.text.text_to_word_sequence", "range", "str", "len"], "function", ["None"], ["", "def", "Text_length", "(", "Data", ")", ":", "\n", "\n", "    ", "HoleText", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "Data", ".", "shape", "[", "0", "]", ")", ":", "\n", "        ", "HoleText", ".", "append", "(", "text_to_word_sequence", "(", "str", "(", "Data", "[", "i", "]", ")", ")", ")", "\n", "", "x_c", "=", "HoleText", "\n", "x_c_l", "=", "[", "len", "(", "x_c", "[", "i", "]", ")", "for", "i", "in", "range", "(", "len", "(", "x_c", ")", ")", "]", "\n", "return", "x_c_l", "\n", "\n"]], "home.repos.pwc.inspect_result.MrRezaeiUofT_Amazon-Product-Recommender.None.Part1_utils.weighted_categorical_crossentropy": [[43, 69], ["tensorflow.variable", "tensorflow.sum", "tensorflow.clip", "tensorflow.epsilon", "tensorflow.sum", "tensorflow.epsilon", "tensorflow.log"], "function", ["None"], ["", "def", "weighted_categorical_crossentropy", "(", "weights", ")", ":", "\n", "    ", "\"\"\"\n    A weighted version of keras.objectives.categorical_crossentropy\n\n    Variables:\n        weights: numpy array of shape (C,) where C is the number of classes\n\n    Usage:\n        weights = np.array([0.5,2,10]) # Class one at 0.5, class 2 twice the normal weights, class 3 10x.\n        loss = weighted_categorical_crossentropy(weights)\n        model.compile(loss=loss,optimizer='adam')\n    \"\"\"", "\n", "\n", "weights", "=", "K", ".", "variable", "(", "weights", ")", "\n", "\n", "def", "loss", "(", "y_true", ",", "y_pred", ")", ":", "\n", "# scale predictions so that the class probas of each sample sum to 1", "\n", "        ", "y_pred", "/=", "K", ".", "sum", "(", "y_pred", ",", "axis", "=", "-", "1", ",", "keepdims", "=", "True", ")", "\n", "# clip to prevent NaN's and Inf's", "\n", "y_pred", "=", "K", ".", "clip", "(", "y_pred", ",", "K", ".", "epsilon", "(", ")", ",", "1", "-", "K", ".", "epsilon", "(", ")", ")", "\n", "# calc", "\n", "loss", "=", "y_true", "*", "K", ".", "log", "(", "y_pred", ")", "*", "weights", "\n", "loss", "=", "-", "K", ".", "sum", "(", "loss", ",", "-", "1", ")", "\n", "return", "loss", "\n", "\n", "", "return", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.MrRezaeiUofT_Amazon-Product-Recommender.None.Part1_utils.weighted_mse": [[71, 96], ["tensorflow.variable", "tensorflow.sum", "tensorflow.reduce_logsumexp", "tensorflow.pow"], "function", ["None"], ["", "def", "weighted_mse", "(", "weights", ")", ":", "\n", "    ", "\"\"\"\n    A weighted version of MSE\n    Variables:\n        weights: numpy array of shape (C,) where C is the number of classes\n\n    Usage:\n        weights = np.array([0.5,2,10]) # Class one at 0.5, class 2 twice the normal weights, class 3 10x.\n\n        model.compile(loss=loss,optimizer='adam')\n    \"\"\"", "\n", "\n", "weights", "=", "K", ".", "variable", "(", "weights", ")", "\n", "\n", "def", "loss", "(", "y_true", ",", "y_pred", ")", ":", "\n", "# scale predictions so that the class probas of each sample sum to 1", "\n", "        ", "y_pred", "/=", "K", ".", "sum", "(", "y_pred", ",", "axis", "=", "-", "1", ",", "keepdims", "=", "True", ")", "\n", "# clip to prevent NaN's and Inf's", "\n", "# y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())", "\n", "# calc", "\n", "loss", "=", "tf", ".", "reduce_logsumexp", "(", "tf", ".", "pow", "(", "y_true", "-", "y_pred", ",", "2", ")", "*", "weights", ")", "\n", "\n", "return", "loss", "\n", "\n", "", "return", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.MrRezaeiUofT_Amazon-Product-Recommender.None.Part1_utils.clean_text": [[98, 147], ["text.split.translate", "text.split.lower().split", "set", "re.sub", "re.sub", "re.sub", "re.sub", "re.sub", "re.sub", "re.sub", "re.sub", "re.sub", "re.sub", "re.sub", "re.sub", "re.sub", "re.sub", "re.sub", "re.sub", "re.sub", "re.sub", "re.sub", "re.sub", "re.sub", "re.sub", "re.sub", "re.sub", "re.sub", "re.sub", "re.sub", "re.sub", "text.split.split", "nltk.stem.SnowballStemmer", "nltk.corpus.stopwords.words", "nltk.stem.SnowballStemmer.stem", "text.split.lower", "len"], "function", ["None"], ["", "def", "clean_text", "(", "text", ")", ":", "\n", "## Remove puncuation", "\n", "    ", "text", "=", "text", ".", "translate", "(", "string", ".", "punctuation", ")", "\n", "\n", "## Convert words to lower case and split them", "\n", "text", "=", "text", ".", "lower", "(", ")", ".", "split", "(", ")", "\n", "\n", "## Remove stop words", "\n", "stops", "=", "set", "(", "stopwords", ".", "words", "(", "\"english\"", ")", ")", "\n", "text", "=", "[", "w", "for", "w", "in", "text", "if", "not", "w", "in", "stops", "and", "len", "(", "w", ")", ">=", "3", "]", "\n", "\n", "text", "=", "\" \"", ".", "join", "(", "text", ")", "\n", "## Clean the text", "\n", "text", "=", "re", ".", "sub", "(", "r\"[^A-Za-z0-9^,!.\\/'+-=]\"", ",", "\" \"", ",", "text", ")", "\n", "text", "=", "re", ".", "sub", "(", "r\"what's\"", ",", "\"what is \"", ",", "text", ")", "\n", "text", "=", "re", ".", "sub", "(", "r\"\\'s\"", ",", "\" \"", ",", "text", ")", "\n", "text", "=", "re", ".", "sub", "(", "r\"\\'ve\"", ",", "\" have \"", ",", "text", ")", "\n", "text", "=", "re", ".", "sub", "(", "r\"n't\"", ",", "\" not \"", ",", "text", ")", "\n", "text", "=", "re", ".", "sub", "(", "r\"i'm\"", ",", "\"i am \"", ",", "text", ")", "\n", "text", "=", "re", ".", "sub", "(", "r\"\\'re\"", ",", "\" are \"", ",", "text", ")", "\n", "text", "=", "re", ".", "sub", "(", "r\"\\'d\"", ",", "\" would \"", ",", "text", ")", "\n", "text", "=", "re", ".", "sub", "(", "r\"\\'ll\"", ",", "\" will \"", ",", "text", ")", "\n", "text", "=", "re", ".", "sub", "(", "r\",\"", ",", "\" \"", ",", "text", ")", "\n", "text", "=", "re", ".", "sub", "(", "r\"\\.\"", ",", "\" \"", ",", "text", ")", "\n", "text", "=", "re", ".", "sub", "(", "r\"!\"", ",", "\" ! \"", ",", "text", ")", "\n", "text", "=", "re", ".", "sub", "(", "r\"\\/\"", ",", "\" \"", ",", "text", ")", "\n", "text", "=", "re", ".", "sub", "(", "r\"\\^\"", ",", "\" ^ \"", ",", "text", ")", "\n", "text", "=", "re", ".", "sub", "(", "r\"\\+\"", ",", "\" + \"", ",", "text", ")", "\n", "text", "=", "re", ".", "sub", "(", "r\"\\-\"", ",", "\" - \"", ",", "text", ")", "\n", "text", "=", "re", ".", "sub", "(", "r\"\\=\"", ",", "\" = \"", ",", "text", ")", "\n", "text", "=", "re", ".", "sub", "(", "r\"'\"", ",", "\" \"", ",", "text", ")", "\n", "text", "=", "re", ".", "sub", "(", "r\"(\\d+)(k)\"", ",", "r\"\\g<1>000\"", ",", "text", ")", "\n", "text", "=", "re", ".", "sub", "(", "r\":\"", ",", "\" : \"", ",", "text", ")", "\n", "text", "=", "re", ".", "sub", "(", "r\" e g \"", ",", "\" eg \"", ",", "text", ")", "\n", "text", "=", "re", ".", "sub", "(", "r\" b g \"", ",", "\" bg \"", ",", "text", ")", "\n", "text", "=", "re", ".", "sub", "(", "r\" u s \"", ",", "\" american \"", ",", "text", ")", "\n", "text", "=", "re", ".", "sub", "(", "r\"\\0s\"", ",", "\"0\"", ",", "text", ")", "\n", "text", "=", "re", ".", "sub", "(", "r\" 9 11 \"", ",", "\"911\"", ",", "text", ")", "\n", "text", "=", "re", ".", "sub", "(", "r\"e - mail\"", ",", "\"email\"", ",", "text", ")", "\n", "text", "=", "re", ".", "sub", "(", "r\"j k\"", ",", "\"jk\"", ",", "text", ")", "\n", "text", "=", "re", ".", "sub", "(", "r\"\\s{2,}\"", ",", "\" \"", ",", "text", ")", "\n", "# Stemming", "\n", "text", "=", "text", ".", "split", "(", ")", "\n", "stemmer", "=", "nltk", ".", "stem", ".", "SnowballStemmer", "(", "'english'", ")", "\n", "stemmed_words", "=", "[", "stemmer", ".", "stem", "(", "word", ")", "for", "word", "in", "text", "]", "\n", "text", "=", "\" \"", ".", "join", "(", "stemmed_words", ")", "\n", "\n", "\n", "return", "text", "", "", ""]]}